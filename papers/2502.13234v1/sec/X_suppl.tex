\clearpage
\appendix
\setcounter{page}{1}
\maketitlesupplementary

%-------------------------------------------------------------------------
\section{Extended derivations}
\label{sec:derivation}

Below is the derivation of Eq. (2). We apply the generalized formula in DDIM~\cite{ddim} to compute the less noisy video at timestep $t-1$ (denoted as $v_t$), using the noisy video $z_t$ at timestep $t$ along with the predicted noise $\epsilon$:
\begin{align}
    v_{t}(\epsilon,z_t)=
&\sqrt{\bar\alpha_{t-1}} \underbrace{\left(\frac{z_t-\sqrt{1-\bar\alpha_t}\epsilon}{\sqrt{\bar\alpha_t}}\right)}_{\text{``predicted } z_0 \text{''}}\notag\\
&+\underbrace{\sqrt{1-\bar\alpha_{t-1}-\sigma_t^2}\cdot\epsilon}_{\text{``direction pointing to } z_t \text{''}}\notag\\
&+\underbrace{\sigma_t\epsilon_t}_{\text{random noise}}
  \label{eq:derivation}
\end{align}
where $\bar\alpha_t:=\prod_{s=1}^t\alpha_s$ are variance-scaling coefficients~\cite{ddpm}, $\epsilon_t\sim\mathcal N(\bf{0},\bf{I})$ is Gaussian noise, and $\sigma$ is a hyperparamter controlling the stochasticity of the sampling process.

We observe that reducing randomness (\ie using a lower value of $\sigma_t$) improves feature extraction. Thus, following DDIM, we set $\sigma_t=0$. This simplifies the equation to:
\begin{align}
    v_{t}=
\sqrt{\bar\alpha_{t-1}} \left(\frac{z_t-\sqrt{1-\bar\alpha_t}\epsilon}{\sqrt{\bar\alpha_t}}\right)
+\sqrt{1-\bar\alpha_{t-1}}\cdot\epsilon
\end{align}
which can be further simplified as:
\begin{align}
    v_{t}=
&\frac 1 {\sqrt{\alpha_{t}}}z_t+\left(-\frac {\sqrt{1-\bar\alpha_t}} {\sqrt{\alpha_{t}}}+\sqrt{1-\bar\alpha_{t-1}}\right)\epsilon
\end{align}
Next, the DDPM objective can be reformulated to compare the previous noised videos $z_{t-1}$:
\begin{align}
    L=&\E\left[w_t\left\|
\epsilon-\epsilon_{\theta}(z_t, t, c)
\right\|^2\right]\\
=&\E\left[w'_t\left\|
v_t(z_t,\epsilon)-v_t(z_t,\epsilon_{\theta}(z_t, t, c))
\right\|^2\right]
\end{align}
where:
\begin{align}
w'_t=\left(-\frac {\sqrt{1-\bar\alpha_t}} {\sqrt{\alpha_{t}}}+\sqrt{1-\bar\alpha_{t-1}}\right)^{-1}w_t
\end{align}
The time-dependent weight $w_t$ is commonly set to 1. However, we employ a different weighting, where $w'_t$ is 1 for the first 500 steps and to 0 for the last 500 steps. This weighting approach prioritizes the early stages, which are crucial for deciding video motion.

%-------------------------------------------------------------------------
\section{Limitations}
\label{sec:limitations}

One limitation of {\ours} is that it requires a feature extractor to compute the objective, which introduces additional latency and results in longer training time (15 minutes) compared to pixel-level fine-tuning approaches~\cite{md,vmc} (8 minuets) on an NVIDIA GeForce RTX 4090. Furthermore, since {\ours} relies on pre-trained T2V diffusion models, it struggles to synthesize videos that fall outside the generative prior of these models. However, we believe that this challenge can be mitigated as more advanced T2V diffusion models are developed in the future.

Like other existing approaches, another limitation of {\ours} lies in its reliance on DDIM-inverted noise (See \cref{sec:noise} for details), which introduces a potential risk of content leakage from the reference video. As this issue is common among most existing approaches, addressing it will be an important direction for future research.

%-------------------------------------------------------------------------
\section{Analysis of motion features}
\label{sec:supp_retrieval}

We conduct a simple retrieval experiment to verify that our motion feature extractor is capturing motion information from noisy videos. From the SVW dataset~\cite{svw}, we draw 139 javelin video clips with diverse motion trajectories and camera movements and randomly trim each clip to 16 frames. We obtain their motion features by adding noise to each video $z$ and feeding them into our motion feature extractor as follows:
\begin{equation}
\mathcal \mfe(\sqrt{\bar{\alpha}_t}z+\sqrt{1-\bar{\alpha}_t}\epsilon)
\label{eq:retrieval},
\end{equation}
where $\mfe$ denotes our motion feature extractor, and the time step $t$ is set to $500$ for this experiment. After getting the motion features of all videos, we randomly select a query video and retrieve the most similar video from the dataset based on these motion features.

As shown in \cref{fig:retrieval}, the video with the most similar motion features shares the same motion despite having different appearances. In contrast, the video that is most similar in latent space has a nearly identical appearance but opposite motion, while the video with the most similar residual frames contain unrelated motion.

To compute the retrieval accuracy statistically, we label the videos with the top 10\% smallest motion discrepancy values with the query video as positive samples and the rest 90\% of the videos as negative samples. Next, we compute the average precisions (AP) for each retrieval methods to assess their retrieval accuracy. As presented in \cref{tab:retrieval_quant}, our motion features yield the highest accuracy, indicating that they have the strongest correlation with actual motion. These results verify that our motion features capture rich motion information, rather than irrelevant details about visual appearance.

\begin{table}[h]
  \centering
\begin{tabular}{ccccc}
  \toprule
  &\textbf{Ours} & DDPM & VMC & Random \\ \midrule
  AP & \textbf{32.78\%} & 8.20\% & 8.85\% &10.71\% \\
  \bottomrule
\end{tabular}

  \caption{{\bf Retrieval accuracy.} Using our motion features to extract videos with similar motion yields the highest average precision (AP) than directly using latent videos (DDPM~\cite{ddpm}) or their residual frames (VMC~\cite{vmc}).}
  \label{tab:retrieval_quant}
\end{table}

\input{figures/retrieval}

\section{Additional qualitative results}
\label{sec:supp_qualitative}

We present additional qualitative comparisons in \cref{fig:supp_comparisons}, detailed qualitative results in \cref{fig:supp_qualitative}, and further samples generated using CogVideoX~\cite{cogvideo} as the base model in \cref{fig:cogvideo}.

%-------------------------------------------------------------------------
\section{Must motion be learned at feature level?}
\label{sec:why}

Analyzing video motion requires the ability to identify (1) scene composition and (2) the patterns of changes across frames (\ie zooming, rotation, and displacement). Both of them are high-level concepts. The high-level nature of motion is also evident in optical flow estimation, a longstanding focus of research in video motion analysis. Early efforts in this domain primarily relies on rule-based algorithms that use handcrafted rules to model motion~\cite{algo1, algo2, algo3, algo4}. However, such methods often struggle with complex motion, such as large displacements, non-rigid movements, and motion in low-texture regions, all due to their lack of high-level understanding of videos.

With advances in machine learning, recent studies on optical flow estimation have shifted towards data-driven methods that learn motion patterns from large datasets~\cite{deep1, deep2, deep3, deep4, deep5}. These approaches have significantly improved motion estimation by leveraging deep neural networks to understand motion at the feature level, highlighting the importance of a high-level understanding of motion. 

In the context of motion customization, given that motion is inherently a high-level concept, pixel-level objectives, such as frame-difference matching~\cite{md,vmc,sma}, are insufficient for capturing motion. These objectives often fail to capture complex motion, facing the same challenge as early research on optical flow estimation. In contrast, our method precisely extracts motion information with the assistance of a deep neural network. By leveraging a large pre-trained model, our method can understand at a high level and captures key information such as scene composition and patterns of changes.

%-------------------------------------------------------------------------
\section{Implementation details}
\label{sec:rationale}

\paragraph{Training}
\label{sec:supp_training}
To fine-tune the diffusion model, we add LoRAs to all self-attention and feed forward layers, and set the rank to 32. Since motion is mainly determined in early stages~\cite{mc,dmt}, we set the time-dependent weights $w'_t$ in the objective function to 1 for the first 500 timesteps and 0 for the last 500 timesteps. The LoRA~\cite{lora} are optimized for 400 steps at a learning rate of 0.0005, which takes approximately 15 minutes on an NVIDIA GeForce RTX 4090. All videos in the experiments consist of 16 frames at 8 fps and are generated at a resolution of $384\times 384$.
  
\paragraph{Feature extraction}
We extract cross-attention maps and temporal-self attention maps from down\_block.2 at a $12\times 12$ resolution. Both $\mca$ and $\mtsa$ represent the average of all extracted attention maps across heads and layers, which we omit in all equations for conciseness.

\paragraph{Initial noise}
\label{sec:noise}
Following previous work on motion customization~\cite{md, vmc, sma, dmt}, we utilize DDIM inversion to obtain the initial noise $z_T$ for better motion alignment. In our work, the initial noise $z_T$ is computed as in MotionDirector's implementation:
\begin{equation}
    z_T=\sqrt{\beta}\epsilon_{\rm inv}+\sqrt{1-\beta}\epsilon
  \label{eq:ddpm_in}
\end{equation}
where $\epsilon\sim\mathcal N(\bf{0},\bf{I})$ is Gaussian noise, and $\epsilon_{\rm inv}$ represents the inverted noise of the reference video, derived via DDIM inversion~\cite{ddim}. The square root terms in the equation ensure that the variance of $z_T$ remains consistent across all values of $\beta$. In quantitative experiments and human user study, we set a fix value of $\beta=0.3$. In other experiments, $\beta$ varies between the range of $0.0$ to $0.3$.

%-------------------------------------------------------------------------
\section{Evaluation details}

\paragraph{Dataset}
We collect a dataset of 42 video-text pairs, including 14 unique reference videos from DAVIS~\cite{davis} and LOVEU-TGVE~\cite{loveu}, many of which are also used in prior work. For each reference video, we provide exactly 3 target text prompts that describe scenes distinct from the original one and ensure that they are compatible with the motion in the reference video.

\paragraph{Quantitative evaluation}
To evaluate each method, we generate 5 videos per video-text pair, and calculate the average scores across all generated videos.

\paragraph{Human user study}
In the human user study, we employ the same set of videos generated in the quantitative experiments. Each survey consists of 32 tasks. In each task, the survey respondents are presented with a video-text pair, a video generated by our method, and a video generated by one of the four competing methods (\cref{fig:ui}). The video-text pair and videos for each task are randomly selected on the fly, resulting in a total of $4\times 42\times 5\times 5=4200$ different tasks. To assess motion alignment, text alignment, and video quality, the participants are asked three questions: "Which video better matches the motion of the following video?", "Which video better matches the following text?", and "Which video has better video quality (i.e., more realistic and visually appealing)?". To ensure a fair comparison, the order of the choices is randomized.

\clearpage

\begin{figure*}
  \centering
  \includegraphics[width=1.0\linewidth]{figures/supp_qualitative.jpg}
  \caption{{\bf Additional qualitative results.} The results demonstrate {\ours}'s capability to transfer both object movements and camera movements to new scenes.}
  \label{fig:supp_qualitative}
\end{figure*}

\clearpage

\begin{figure*}
  \centering
  \includegraphics[width=1.0\linewidth]{figures/supp_cogvideo.jpg}
  \caption{{\bf More samples generated using CogVideoX~\cite{cogvideo} as the base model.} The results demonstrate the generality of {\ours}. Even with T2V diffusion models that employ full attentions, we can still extract cues for objects movement from attention weights computed between frames and cues for camera framing from attention weights computed between words and patch tokens.}
  \label{fig:cogvideo}
\end{figure*}

\clearpage

\begin{figure*}
  \centering
  \includegraphics[width=1.0\linewidth]{figures/supp_comparisons.jpg}
  \caption{{\bf Additional qualitative comparisons.} The results demonstrate {\ours}'s superiority over existing motion customization methods in terms of video quality, text alignment, and motion alignment.}
  \label{fig:supp_comparisons}
\end{figure*}

\clearpage

\begin{figure*}
  \centering
  \includegraphics[width=0.7\linewidth]{figures/supp_ui.png}
  \caption{{\bf User interface of an evaluation task.} Each task includes three questions, each assessing a key aspect of motion customization.}
  \label{fig:ui}
\end{figure*}

% \clearpage

% {
%     \small
%     \bibliographystyle{ieeenat_fullname}
%     \bibliography{main}
% }