\section{Introduction}
Monte Carlo Tree Search (MCTS) has demonstrated state-of-the-art performance in solving many challenging planning tasks, from playing the game of go~\cite{silver2016mastering} and chess to logistic planning~\cite{silver2017mastering}. It performs look-ahead searches based on Monte Carlo sampling of the actions to balance efficient exploration and optimized exploitation in the large search space. Recent efforts have focused on developing MCTS algorithms for real-world domains that require the elimination of certain standard assumptions. Examples include MuZero~\cite{schrittwieser2020mastering} that leverages the decoding of hidden states to avoid requiring the knowledge of the game dynamics; and MAzero~\cite{liu2024efficient} that performs multi-agent search through decentralized execution. However, existing work have not considered lifelong non-stationarity of task dynamics, which may manifest itself in many open world domains, as the task environment can often vary over time or across scenarios. It requires novel MCTS algorithms that can adapt in response, accumulate, and exploit knowledge throughout the learning process. 

We consider MCTS-based lifelong planning under non-stationarity. An agent faces a series of changing planning tasks -- e.g., with varying transition probabilities and rewards -- which are drawn sequentially throughout the operational lifetime. Transferring knowledge from prior experience to continually adapt Monte Carlo sampling of the actions and thus speed up searches in new tasks is a key question in this setting. We note that although continual and lifelong planning has been studied in reinforcement learning (RL) context, e.g., learning models of the non-stationary task environment~\cite{xie2020deep}, identifying reusable skills~\cite{lu2020reset}, or estimating Bayesian sampling posteriors~\cite{fu2022model}, such prior work are not applicable to MCTS. Monte Carlo action sampling in MCTS relies on Upper Confidence Tree (UCT) or polynomial Upper Confidence Tree (pUCT)~\cite{auger2013continuous,matsuzaki2018empirical} to balance exploration and exploitation in large search spaces. To the best of our knowledge, there has been not existing work analyzing the transfer of knowledge from past MCTS searches to new tasks, thus enabling adaptive the UCT/pUCT rules in lifelong MCTS.


This paper proposes LiZero for Lipschitz lifelong planning using MCTS. We quantify a novel concept that the amount of knowledge transferable from a source task to the UCT/pUCT rule of a new task depends on both the similarity between the tasks as well as the confidence of the knowledge. More precisely, by defining a distance metric between two MDPs, we refine the concentration argument and drive a new adaptive UCT bound (denoted as aUCT in this paper) for lifelong MCTS. The aUCT is shown to consist of two components -- relating to (i) the Lipschitz continuity between the two tasks and (ii) the confidence of knowledge due to the numbers of samples in Monte Carlo action sampling. Our results enable the development a novel LiZero algorithm that makes use of prior experience to run an adaptive MCTS by simulating/traversing from the root node and selecting actions according to the aUCT rule, until reaching a leaf node. We also analyze aUCT's acceleration factor in terms of improved sampling efficiency due to cross-task transfer. It is shown that smaller task distance and higher confidence can both lead to higher acceleration in aUCT.

To support practical deployment of LiZero in lifelong planning, we need efficient solutions to compute aUCT in an online fashion. To this end, we develop practical algorithms to estimate various terms in aUCT and especially the distance metric between two MDPs, from either available state-action samples using a data-driven approach or a parameterized distance using a model-based (deep learning) approach. We provide rigorous analysis on the sampling complexity of the data-driven approach, to ensure arbitrarily small error with high probability, by modeling a non-stationary policy update process by a filtration -- i.e., an increasing sequence of $\sigma$-algebras. For the model-based approach, we obtain an upper bound using a parameterized distance of the neural network models. These results enable effective LiZero application to open world tasks. %{\color{green} Discuss the evaluation.}
We evaluate LiZero on a series of learning tasks with varying transition probabilities and rewards. It is shown that LiZero significantly outperforms MCTS and lifelong RL baselines (e.g., ~\cite{winands2024monte,kocsis2006bandit,chengspeculative,Schrittwieser_2020,brafman2002r,lecarpentier2021lipschitz}) in terms of 
%better knowledge transfer and 
faster convergence to higher optimal rewards. Utilizing the knowledge of only a few source tasks, LiZero achieves 3$\sim$4x speedup with about $31\%$ higher early reward in the first half of the learning process.


Our key contributions are as follows. First, we study theoretically the transfer of past experience in MCTS and develop a novel aUCT rule, depending on both Lipschitz continuity between tasks and the confidence of knowledge in Monte Carlo action sampling. It is proven to provide positive acceleration in MCTS due to cross-task transfer. Second, we develop LiZero for lifelong MCTS planning, with efficient methods for online estimation of aUCT and analytical error bounds. Finally, LiZero achieves significant speed-up over MCTS and lifelong RL baselines in lifelong planning.