\section{Our Proposed Solution}

\subsection{Deriving adaptive Upper Confidence Bound (aUCT)}

To derive the proposed aUCT rule, we consider set of $m$ past known MDPs $\mathcal{M}_1,\ldots,\mathcal{M}_m$ and their leaned search policies $\pi_1,\ldots\pi_m$. Let $S$ and $A$ be their state and action spaces, respectively\footnote{Without loss of generality, we assume that the MDPs have the same state and action spaces. Otherwise, we can consider the extended MDPs defined on the union of their state and action spaces.}, $N_i(s,a)$ be the visit count of MPD $\mathcal{M}_i$ to state-action pair $(s\in S,a\in A)$, $W(s,a)$ to denote its sampled return, and $Q^{N_i}_{\mathcal{M}_i}(s,a)=W_i(s,a)/N_i(s,a)$ be the learned estimate for Q-value of MDP $\mathcal{M}_i$. Our goal is to apply these knowledge toward learning a new MDP, denoted by $\mathcal{M}$. To this end, we derive a new Lipschitz upper confidence bound for $\mathcal{M}$, which utilizes and transfers the knowledge from past MDPs $\mathcal{M}_1,\ldots,\mathcal{M}_N$, thus obtaining an improved Monte Carlo action sampling strategy that limits the tree search on $\mathcal{M}$ to a smaller subsets of sampled actions. We use $N(s,a)$ to denote the visit count of the new MDP to $(s\in S,a\in A)$, $W(s,a)$ to denote the sampled return, and thus $Q^N_{\mathcal{M}}(s,a)=W(s,a)/N(s,a)$ to denote its current Q-value estimate. 

Our key idea in this paper is that an improved upper confidence bound for the new MDP $\mathcal{M}$ can be obtained by (i) analyzing the Lipschitz
continuity between the past and new MDPs with respect to the upper confidence bounds and (ii) taking into account the confidence and aleatory uncertainty of the learned Q-value estimates to determine to what extent the learned knowledge from each $\mathcal{M}_i$ is pertinent. Intuitively, the more similar $\mathcal{M}$ and $\mathcal{M}_i$ are and the more samples (and thus higher confidence) we have in the learned Q-value estimates, the less exploration we would need to perform for solving $\mathcal{M}$ through MCTS. Our analysis will lead to an improved upper confidence bound that guides the MCTS on the new MDP $\mathcal{M}$ over a much smaller subset of action samples, thus significantly improving the search performance. We start with introducing a definition of the distance between any two given MDPs, $\mathcal{M}=\langle R,P\rangle, \ {\mathcal{M}}^{\prime} = \langle {R}^{\prime},{P}^{\prime}\rangle$, with reward functions $R,R'$ and state transitions $P,P'$, respectively. We choose a positive scaling factor $\kappa>0$ to combine the distances with respect to transition probabilities and rewards. Proofs of all theorems and corollaries are presented in the appendix.


\begin{definition}
\label{def:gobal}
Give two MDPs $\mathcal{M}=\langle R,P\rangle, \ {\mathcal{M}}^{\prime} = \langle {R}^{\prime},{P}^{\prime}\rangle$, and a distribution for sampling the state transitions $\mathcal{U}:\mathcal{S}\times \mathcal{A} \times \mathcal{S}' \rightarrow[0,1]$,
we define the pseudometric between the MDPs
as:
\begin{equation}
\begin{aligned}
d(\mathcal{M},\mathcal{M}^{\prime}) &= \Delta R+ \kappa\cdot \Delta P \\
&= %\sum_{s\in\mathcal{S}}\sum_{a\in\mathcal{A}}
\mathbb{E}_{(s,a,s')\sim\mathcal{U}}
\left[|R_s^a-{R}'^{a}_s| + \kappa
|P_{ss^{\prime}}^{a}-{P'_{ss^{\prime}}}^{a} |\right].
\end{aligned} \nonumber
\end{equation}
\end{definition}
Here $d(\mathcal{M},\mathcal{M}^{\prime})$ is our definition of distance between two MDPs, $\mathcal{M}$ and $\mathcal{M}'$. We choose $\mathcal{U}$ to be uniform distribution for sampling the state transitions in this paper. In Section~\ref{sec:distance}, we discuss practical algorithms to estimate the distance metric between two MDPs, from either available state-action samples using a data-driven approach or a parameterized distance using a model-based (deep learning) approach. The sampling complexity and error bounds are also analyzed. 


Next, we prove the main result of this paper and show that the upper confidence bounds of $\mathcal{M}$ and $\mathcal{M}'$ is Lipschitz continuous with respect to distance $d(\mathcal{M},\mathcal{M}^{\prime})$. We obtain a new upper confidence bound for $\mathcal{M}$, by transfer the knowledge from the learned Q-value estimates $Q^{N'}_{\mathcal{M}'}(s,a)=W'(s,a)/N'(s,a)$ of MDP $\mathcal{M}'$. Obviously, the bound also depends on the confidence of learned Q-value estimates, relating to the visit counts $N(s,a)$ and $N'(s,a)$.



\begin{theorem}[Lipschitz aUCT Rule]
\label{Optimal_Q_Lipschitz}
Consider two MDPs \( M \) and \({M}'\) with visit count $N,N'$ and corresponding estimate Q-values $Q_M^{N}(s,a), Q_{M^\prime}^{N'}(s,a)$, respectively. With probability at least $(1-\delta)$ for some positive $\delta>0$, we have
\begin{equation}
\label{eqn:aUCT}
\begin{aligned}
    \left|Q_{\mathcal{M}}^{N}(s,a)-Q_{\mathcal{M}^\prime}^{N'}(s,a)\right|\leq L\cdot d(\mathcal{M},\mathcal{M}') + P(N,N')
\end{aligned} 
\end{equation}
where $L={1}/({1-\gamma})$ is a Lipschitz constant, $d(\mathcal{M},\mathcal{M}')$ is the distance between MDPs, and $P(N,N')$ is given by
\begin{equation}
\label{eqn:aUCT1}
P(N,N') = \frac{2R_{\max}}{1-\gamma}\sqrt{\frac{\ln(2/\delta)}{2\cdot {\rm min}(N,N')}}
\end{equation}
\end{theorem}
In the theorem above, we show that the estimate Q-values between two MDPs are bounded by two terms, i.e., a Lipschitz continuity term depending on the distance $d(\mathcal{M},\mathcal{M}')$ between the two environments and a confidence term depending on the number $N, N'$ of samples used to estimate the Q-values. The Lipschitz continuity term measures how much the learned knowledge of source MDP $\mathcal{M}$ is pertinent to the new MDP $\mathcal{M}'$, while the confidence terms $P(N,N')$ quantifies the sampling bias arising from statistical uncertainty due to limited sampling in MCTS. We note that as the number of samples $N$ goes to infinity, we have $Q_{\mathcal{M}}^{N}(s,a)\rightarrow Q_{\mathcal{M}}^{*}(s,a)$ in Theorem~3.2, approaching the true Q-value $Q_{\mathcal{M}}^{*}(s,a)$ of the new MDP. Our theorem effectively provides an upper confidence bound for the true  Q-value of the new MDP, based on knowledge transfer from the source MDP. We also note that as both numbers $N,N'$ goes to infinity, the confidence term becomes $P(N,N')\rightarrow 0$. Our theorem recovers the Lipschitz lifelong RL~\cite{lecarpentier2021lipschitzlifelongreinforcementlearning} as a special case of our results, with respect to the true Q-values of the two MDPs. 

We apply Theorem~3.2 to MCTS-based lifelong planning with a non-stationary series of $m$ tasks, $\mathcal{M}_1,\ldots,\mathcal{M}_m$. Our goal is to obtain an improved bound on the true Q-value of the new task $\mathcal{M}$ based on knowledge transfer. To this end, we independently apply the knowledge from each past MDP, i.e., $Q^{N_i}_{\mathcal{M}_i}(s,a)=W_i(s,a)/N_i(s,a)$, to the new MDP. By taking the minimum of these bounds and making $N\rightarrow \infty$, it provides a tightest upper bound on the true Q-value $Q_{\mathcal{M}}^{*}(s,a)$ of the new MDP, which is defined as our aUCT bound, as it adaptively transfers knowledge from past tasks to the new tasks in MCTS-based lifelong planning. The result is summarized in the following corollary.

\begin{corollary}[aUCT bound in lifelong planning]
\label{cor:MDPS} 
Given MDPs $\mathcal{M}_1,\ldots,\mathcal{M}_m$, the new MDP's true Q-value is bounded by $Q_{\mathcal{M}}^{*}(s,a)\le U_{\rm aUCT}$ with probability at least $(1-\delta)$. The aUCT bound $U_{\rm aUCT}$ is given by 
\begin{equation}
\begin{aligned}   
U_{\rm aUCT}(s,a) \triangleq \min_{1\leq i\leq m}  \Bigg[ Q_{M_i}^{N_i}(s,a) + L\cdot d(\mathcal{M},\mathcal{M}_i)  +  \frac{2R_{\max}}{1-\gamma} \sqrt{\frac{\ln(2/\delta)}{2N_i(s,a)}} \Bigg]
\end{aligned}
\end{equation}
\end{corollary}
Obtaining this corollary is straightforward from Theorem~3.2 by taking $N\rightarrow \infty$ and considering the tightest bound of all knowledge transfers. In the context of MCTS-based lifelong planning, the more knowledge we have from solving past tasks, the more likely we can easily plan a new task, as the aUCT bound $U_{\rm aUCT}(s,a)$ is taken over the minimum of all past tasks. The confidence of past knowledge, i.e., the statistical uncertainty due to sampling number $N_i$, also affects the knowledge transfer to the new task.


\subsection{Our Proposed LiZero Algorithm Using aUCT}

We use the derived aUCT to design a highly efficient LiZero algorithm for MCTS-based lifelong planning. The LiZero algorithm transfers knowledge from past known tasks by computing $U_{\rm aUCT}(s,a)$ in Corollary~3.3. It requires efficient estimate of the distance $d(\mathcal{M},\mathcal{M}_i)$ (as defined in Definition~3.1) between the source MDPs and the new (target) MDP. We will present practical algorithms for such distance estimate in the next section and present analysis on the sampling complexity and error bounds. We will first introduce our LiZero algorithm in this section. We note that, during MCTS, direct exploration/search in the new task $\mathcal{M}$ also produces new knowledge and leads to improved UCT bound of $\mathcal{M}$. Therefore, our proposed LiZero combines both knowledge transfer through $U_{\rm aUCT}(s,a)$ and knowledge from direct exploration/search in $\mathcal{M}$. 




The search in our proposed LiZero algorithm is divided into three stages, repeated for a certain number of simulations. First, each simulation starts from the internal root state and finishes when the simulation reaches a leaf node. Let $Q^N_{\mathcal{M}}(s,a)=W(s,a)/N(s,a)$ be the current estimate of the new MDP and $N(s)=\sum_{a\in \mathcal{A}} N(s,a)$ be the visit count to state $s\in\mathcal{S}$. For each simulated time-step, LiZero chooses an action $a$ by maximizing a combined upper confidence bound based on aUCT, i.e.,
\begin{equation}
a={\rm arg} \max_a \min \left[ \frac{W(s,a)}{N(s,a)} + C\sqrt{\frac{\ln N(s)}{N(s,a)}}, U_{\rm aUCT}(s,a)\right] \nonumber 
\end{equation}
In practice, we can also use the maximum possible return $R_{\max}/(1-\gamma)$ as an initial value of the search. Next, at the final time-step of the simulation, the reward and state are computed by a dynamics function. A new node, corresponding to the leaf state, is then added to the search tree. Finally, at the end of the simulation, the statistics along the trajectory are updated. Let $G$ be the accumulative (discounted) reward for state-action $(s,a)$ from the simulation. We update the statistics by:
\begin{eqnarray}
& & Q^{N+1}_{\mathcal{M}}(s,a) \coloneq \frac{N(s,a)\cdot Q^{N}_{\mathcal{M}}(s,a)+G}{N(s,a)+1}, \nonumber \\
& & N(s,a) \coloneq  N(s,a) +1. \nonumber
\end{eqnarray}


Intuitively, at the start of task $\mathcal{M}$'s MCTS, there are not sufficient samples available, and thus $U_{\rm aUCT}(s,a)$ serves as a tighter upper confidence bound than that resulted from the Monte Carlo actions sampling in $\mathcal{M}$. As more samples are obtained during the search process, the standard UCT bound is expected to become tighter than $U_{\rm aUCT}(s,a)$. The use of both bounds will ensure both efficient knowledge transfer and task-specific search. The pseudo-code of LiZero is provided in Appendix A.2.


For the proposed LiZero algorithm, we prove that it can result in accelerated convergence in MCTS. More precisely, we analyze the sampling complexity for the learned Q-value estimate $Q^N_{\mathcal{M}}(s,a)$ to converge to the true value $Q^{*}_{\mathcal{M}}(s,a)$, and demonstrate a strictly positive acceleration factor, compared to the standard UCT. The results are summarized in the following theorem.


\begin{theorem}
\label{the:converage}
To ensure the convergence in a finite state-action space, $\max_{(s,a)}|Q^{N}_{\mathcal{M}}(s,a)-Q_{\mathcal{M}}^{*}(s,a)|\leq \epsilon$ with probability \(1-\delta\), the number of samples required by standard UCT is 
\begin{equation}
\begin{aligned}
\tilde{O}\left(\frac{|\mathcal{S}|\cdot|\mathcal{A}|}{(1-\gamma)^3\epsilon^2}\ln\frac{1}{\delta}\right),
\end{aligned}
\end{equation}
while the proposed LiZero algorithm requires:
\begin{equation}
\begin{aligned}
    \tilde{O}\left(\frac{1}{\Gamma} \cdot \frac{|\mathcal{S}|\cdot|\mathcal{A}|}{(1-\gamma)^3\epsilon^2}\ln\frac{1}{\delta}\right),
\end{aligned}
\end{equation}
where $\Gamma> 1$ is an acceleration factor given by
\begin{equation}
\begin{aligned}
\Gamma =\frac
{\sum_{(s,a)\in \mathcal{S}_1\cup \mathcal{S}_0 } \frac{1}{(\Delta^{\mathcal{M}}_{(s,a)})^2}}
{\sum_{(s,a)\in\mathcal{S}_1} (1) + 
\sum_{(s,a)\in\mathcal{S}_{0}} \frac{1}{(\Delta^{\mathcal{M}}_{(s,a)})^2}},
\end{aligned}
\end{equation}
and \( \mathcal{S}_1 = \{(s, a) \mid \exists i : U_{\rm aUCT}(s, a) < Q^{*}_{\mathcal{M}}(s, a^{*})\} \) is a state-action set where $U_{\rm aUCT}$ of action $a$ is lower than the optimal return of $a^{*}$ in state $s$;
and $\Delta^{\mathcal{M}}_{(s,a)} \propto [Q_{\mathcal{M}}^{*}(s,a^{*}) - Q_{\mathcal{M}}^{*}(s,a)]$ is a normalized advantage in the range of $[0, 1]$.
\end{theorem}


The theorem shows that LiZero achieves a strictly improved acceleration $\Gamma>1$ with a reduced sampling complexity (by $1/\Gamma$), in terms of ensuring convergence to the optimal estimates, i.e., $\max_{(s,a)}|Q^{N}_{\mathcal{M}}(s,a)-Q_{\mathcal{M}}^{*}(s,a)|\leq \epsilon$ with probability \(1-\delta\). Since the normalized advantage $\Delta^{\mathcal{M}}_{(s,a)}$ is in $[0,1]$, we have $1/\Delta^{\mathcal{M}}_{(s,a)}\ge 1$. It is then easy to see that the value of $\Gamma$ depends on the cardinality $|\mathcal{S}_1|$ and the normalized advantage $\Delta^{\mathcal{M}}_{(s,a)}$. More precisely, LiZero achieves higher acceleration when (i) our $aUCT$ makes more actions $a$ less favorable, as $U_{\rm aUCT}(s, a) < Q^{*}_{\mathcal{M}}(s, a^{*})$ implies that the sub-optimality of action $a$ in $s$ can be more easily determined due to aUCT; or (ii) $aUCT$ helps establish tighter bounds in cases with a smaller advantage, which naturally requires more samples to distinguish the optimal actions -- since $\Gamma$ increases as the normalized advantage becomes smaller for $(s,a)\in \mathcal{S}_1$, while being larger for $(s,a)\in \mathcal{S}_0$. These explain LiZero's ability to achieve much higher acceleration and lower sampling complexity, resulted from significantly reduced search spaces. We will evaluate this acceleration/speedup through experiments in Section~\ref{sec:eval}.




\section{Estimaing aUCT in Practice}
\label{sec:distance}

To deploy LiZero in practice, we need to estimate aUCT, and in particular, the distance $d_{\mathcal{M}, \mathcal{M}_i}$ between two MDPS. Sampling all transitions based on a uniform distribution $\mathcal{U}$, as defined in Definition~3.1, is clearly too expensive. Thus, we develop efficient algorithms to estimate the distance metric, from either available state-action samples using a data-driven approach or a parameterized distance using a model-based (deep learning) approach. In this section, we also provide rigorous analysis on the sampling complexity and error bounds of the proposed algorithms for distance estimate. The results allow us to readily implement LiZero in practical environments. We will late evaluate the performance of different distance estimaters in Section~\ref{sec:eval} and present the numerical results.

More precisely, we first propose an algorithm to estimate the distance between two MDPs, $\mathcal{M}$ and $\mathcal{M}'$, using trajectory samples drawn from their search policies during MCTS and then making the use of importance sampling to mitigate the bias. We will start with analyzing a stationary search policy and then extend the results to a non-stationary policy update process, by modeling it as a filtration – i.e., an increasing sequence of $\sigma$-algebra. Next, since many practical problems are faced with extremely large or even continuous action and state spaces (i.e., $\mathcal{A}$ and $\mathcal{S}$), we further consider a model-based approach by learning neural network approximations of the MDPs -- denoted by parameter sets $\phi$ and $\phi'$, respectively -- and then computing an upper bound on the distance using a parameterized distance of the neural network models. Analysis on sampling complexity and error bounds are provided as theorems in this section. 



\subsection{Sample-based Distance Estimate}

During MCTS, transition samples are collected from the search to train a search policy $\pi$. It is easy to see that we can leverage these transition samples to estimate distance $d(\mathcal{M},\mathcal{M}')$ between two MDPs, as long as we address the bias arising from gap between search policy $\pi$ and desired sampling distribution $\mathcal{U}$ in the distance definition $d(\mathcal{M},\mathcal{M}')$. It also allows us to obtain a consistent estimate of MDP distance, without depending on the search policy that is updated during training. We note that this bias can be addressed by importance sampling. 

Let $\Delta X(s, a) = \Delta R_{s}^a + \kappa \Delta P_{s}^a$ be the distance metric for a given state-action pair $(s,a)$. We can rewrite the distance as $d(\mathcal{M},\mathcal{M}')=\mathbb{E}_{(s,a)\sim \mathcal{U}}[ \Delta X(s, a)]$. We denote $p_\mathcal{U}(s,a)$ as the probability (or density) of sampling $(s, a)$ according to distribution $\mathcal{U}$. Importance sampling implies:
\begin{equation}
\begin{aligned}
    \mathbb{E}_{(s,a)\sim \mathcal{U}} [\Delta X(s, a)] = \mathbb{E}_{(s,a)\sim \pi} \left[\frac{p_\mathcal{U}(s,a)}{\pi(s,a)}\cdot \Delta X(s, a)\right],
\end{aligned}
\end{equation}
which can be readily computed from the collected transition samples, following the search policy $\pi(s,a)$. Therefore, for a given set of samples $\{(s_i,a_i),\forall i=1,\ldots,n\}$ collected from a search policy $\pi(s,a)$, we can estimate the distance by the empirical mean:
\begin{equation}
\begin{aligned}
    \hat{d}_{1} = \frac{1}{n}\sum_{i=1}^{n} w_i \Delta X(s_i,a_i), \ {\rm with} \ w_i = \frac{\mathcal{U}(s_i,a_i)}{\pi(s_i,a_i)}
\end{aligned}
\end{equation}
where $w_i$ is the importance sampling weight.


As long as the state-action pairs with $\pi(s, a) > 0$ cover the support of $\mathcal{U}$, this estimator satisfies $\mathbb{E}[\hat{d}_{\mathcal{1}}] = d(\mathcal{M}, \mathcal{M}^{\prime})$, meaning it is unbiased.
Let $\alpha$ be the "coverage" of policy $\pi(s, a)$, i.e., $\pi(s, a) \geq \alpha > 0$, and $p_\mathcal{U}^{\max}$ be the maximum desired sampling probability.
We summarize this result in the following theorem and state the sampling complexity for estimator $\hat{d}_{1}$ to $\epsilon$-converge to $d(\mathcal{M}, \mathcal{M}^{\prime})$.




\begin{theorem}[Sampling Complexity under Stationarity]
\label{the:err_signal_policy}
Assume that for any $(s, a)$, the reward plus transition difference is bounded, i.e., $\Delta X(s, a) \in [0, b]$, and that there exists $\alpha$ such that $\pi(s, a) \geq \alpha > 0$.
When $n$ independent samples are used to estimate $\hat{d}_{1}$, we have
\begin{equation}
\begin{aligned}
\text{Pr}\{|\hat{d}_{1}-d(\mathcal{M},\mathcal{M}^{\prime})|\leq \epsilon\} \geq 1-\delta
\end{aligned}
\end{equation}
\end{theorem}
for any $\delta \in (0, 1)$, if the number of samples satisfy
\begin{equation}
\begin{aligned}
    n \geq \frac{1}{2\epsilon^2} b^2\left(\frac{p_\mathcal{U}^{\max}}{\alpha}\right)^2 \cdot \ln\left(\frac{2}{\delta}\right).
\end{aligned}
\end{equation}
Thus, we obtain a convergence guarantee in the sense of arbitrarily high probability $1-\delta$ and arbitrarily small error $\epsilon$, for estimating $d(\mathcal{M},\mathcal{M}^{\prime})$ using $\hat{d}_{1}$. $\hat{d}_{1}$ is unbiased and ensures convergence to the true distance as the number of samples is sufficiently large.

We note that in many practical settings, the search policy $\pi$ would not stick to a stationary distribution. In contrast, it is continuously updated in each iteration, resulting in a non-stationary sequence of policies over time, i.e., $\pi_1, \pi_2, \dots, \pi_k$. Thus, the transition samples $(s_k, a_k)$'s we obtain at each step $k$ for estimating the distance $d(\mathcal{M},\mathcal{M}^{\prime})$ are indeed drawn from a different $\pi_k$. We cannot assume that the samples follow a stationary distribution (nor that $\{\Delta X^w_k\}$ are i.i.d.) in importance sampling. To address this problem, we model the non-stationary process of policy updates as a filtration – i.e., an increasing sequence of $\sigma$-algebra. In particular, we make the following assumption: at the $k$-th sampling step, the environment is forcibly reset to a predetermined policy $\pi_k$ or independently draws a state from an external memory. This assumption is reasonable because, in many episodic learning scenarios, the environment is inherently divided into episodes: at the beginning of each episode, the state is reset to some initial distribution (e.g., the opening state in Atari games or the initial pose in MuJoCo). This naturally results in the ``reset" assumption. 

In this setup, the policy $\pi_{k}$ at step $k$ is determined by information at step $k-1$ or earlier. Consequently, once $\pi_k$ is fixed, the distribution (marginal) of $\Delta X^w_k = \frac{p_\mathcal{U}(s_k, a_k)}{\pi_{k}(s_k, a_k)}\Delta X(s_k, a_k)$ is also fixed. Therefore, we can establish the filtration $\{\mathcal{F}_k, k=1,2,\ldots\}$ as follows: 
\begin{equation}
\begin{aligned}
    \mathcal{F}_{k-1} = \sigma\{\pi_1,...,\pi_k,(s_1,a_1),...,(s_{k-1},a_{k-1})\},
\end{aligned}
\end{equation}
where $\sigma\{\cdot\}$ denotes the smallest $\sigma$-algebra generated by the random elements. Thus, we obtain: 
\begin{equation}
\label{eqn:Martingale}
\begin{aligned}
    \mathbb{E}[\Delta X_k|\mathcal{F}_{k-1}] &= \mathbb{E}_{(s_k,a_k)\sim \pi_k} \left[\frac{p_\mathcal{U}(s_k,a_k)}{\pi_k(s_k,a_k)}\cdot \Delta X(s_k, a_k)\right]\\
    & = \mathbb{E}_{(s_k,a_k)\sim \mathcal{U}} [\Delta X(s,a)] \\
    & = d(\mathcal{M},\mathcal{M}^{\prime})
\end{aligned}
\end{equation}
This allows us to obtain another empirical estimator $\hat{d}_{2}$ using the filtration model. We analyze the sampling complexity of $\hat{d}_{2}$ and summarise the results in the following theorem.
\begin{theorem}[Sampling Complexity under Non-Stationarity]
\label{the:err_multi_policy}
Under the same conditions as Theorem~\ref{the:err_signal_policy}
when $n$ independent samples are used to estimate $\hat{d}_{2}$, we have
\begin{equation}
\begin{aligned}
\text{Pr}\{|\hat{d}_{2}-d(\mathcal{M},\mathcal{M}^{\prime})|\leq \epsilon\} \geq 1-\delta
\end{aligned}
\end{equation}
for any $\delta \in (0, 1)$, if the number of samples satisfy
\begin{equation}
\begin{aligned}
    n \geq \frac{2}{\epsilon^2} b^2 \left(\frac{p_\mathcal{U}^{\max}}{\alpha}\right)^2\cdot \ln \left(\frac{2}{\delta}\right).
\end{aligned}
\end{equation}
\end{theorem}
It implies that more samples are needed considering the non-stationarity of policy update process for distance estimate.




\subsection{Model-based Distance Estimate}
When the action and state spaces, $\mathcal{A}$ and $\mathcal{S}$ are very large or even continuous, employing the sample based method will become increasingly expensive. Therefore, we propose a model-based approach to first approximate the dynamics of MDPs $\mathcal{M}$ and $\mathcal{M}'$ using two neural networks and then estimate $d(\mathcal{M},\mathcal{M}^{\prime})$ based on the parameterized distance between the neural networks. 


To this end, we need to establish a bound on $d(\mathcal{M},\mathcal{M}^{\prime})$ using the distance between their neural network parameters. 
We use a neural network $\Psi_{\phi}: \mathcal{S} \times \mathcal{A} \rightarrow \Delta(\mathcal{S})$ to model the MDP dynamics.
Many model-based learning algorithms, such as %{\color{green} cite some model-based algos}
PILCO~\cite{deisenroth2011pilco},MBPO~\cite{janner2019trust},PETS~\cite{chua2018deep},MuZero~\cite{schrittwieser2020mastering}, can be employed to learn the models of $\mathcal{M}$ and $\mathcal{M}'$.
Let $\phi$ be the neural network parameters of MDP $\mathcal{M}$ and $\phi'$ be the neural network parameters of MDP $\mathcal{M}'$. We define a distance in the parameter space:
\begin{equation}
\begin{aligned}
   \hat{d}_{para}  =  \rho(\phi,\phi') \geq 0,
\end{aligned}
\end{equation}
where \( \rho \) is a distance or divergence measure in the parameter space, such as the \( \ell_2 \)-norm, \( \ell_1 \)-norm, or certain kernel distances.
Intuitively, if $\phi$ and $\phi'$ are very close, it indicates that the two neural networks are similar in fitting the dynamics of the respective MDPs. It suggests that the two MDPs should have a small distance.
To provide a more rigorous characterization of this concept, we present the following theorem, which demonstrates that under proper assumptions, the distance $\hat{d}_{para}$ based on neural network parameters can serve as an upper bound for the desired $d(\mathcal{M},\mathcal{M}^{\prime})$. Let $\kappa={R_{\max}\gamma}/({1-\gamma})$ be a constant.


\begin{theorem}
\label{the:network}
If the neural networks modeling $\mathcal{M}$ and $\mathcal{M}'$ satisfy the Lipschitz condition, i.e., there exists a constant $L > 0$ such that $\forall (s, a)$,  
\( ||\Psi_{\phi}(s, a) - \Psi_{\phi'}(s, a)||_1 \leq L \cdot \rho(\phi, \phi'), \)
then we have:
\begin{equation}
\begin{aligned}
   d(\mathcal{M},\mathcal{M}^{\prime}) \le (1+\kappa)L \hat{d}_{\text{para}}.
\end{aligned}
\end{equation}
\end{theorem}
The theorem indicates that by learning neural networks to model the MDP dynamics, we can estimate the distance $d(\mathcal{M},\mathcal{M}^{\prime})$ by estimating the distance between the neural network parameters. This parameterized distance can be computed for event continuous action and state spaces. 















