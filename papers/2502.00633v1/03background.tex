\section{Background}



Monte Carlo Tree Search (MCTS)~\cite{kocsis2006bandit,silver2016mastering,schrittwieser2020mastering} is a heuristic search algorithm often applied to problems modeled as MDPs to handle exploration and exploitation dynamically.
MCTS builds a search tree by exploring actions from the current state, simulating outcomes, and using those results to update estimated values for the selected actions. 
Normally, the problems solved by MCTS can be modeled using a Markov Decision Process (MDP)~\cite{RL},
which is formally defined as a tuple: $\langle \mathcal{S}, \mathcal{A}, R, P \rangle$, where $\mathcal{S}$ is the state space, and $\mathcal{A}$ is the action space, $R_s^a$ is the reward of taking action $a$ in state $s$ and $P$ is the transition probability matrix.

In the MCTS framework, the Upper Confidence Bound for Trees (UCT)~\cite{coulom2006efficient} and its variant, polynomial Upper Confidence Trees (pUCT)~\cite{matsuzaki2018empirical,auger2013continuous}, are among the most commonly used selection strategies for balancing exploration and exploitation during node selection.
Although these bounds are theoretically grounded and have achieved great empirical success, they are based on static environment assumptions and do not consider dynamic, non-stationary environments~\cite{pourshamsaei2024predictive,hernandez2017survey,goldberg2003maximizing}, where state transitions and reward distributions may change over time, thus requiring the transfer of past knowledge to exploration/exploitation of new tasks. In this paper, we consider MCTS-
based lifelong planning, where an agent faces a
non-stationary series of tasks â€“ e.g., with varying transition probabilities and reward -- and requires the development of new  adaptive UCT bounds.


Lifelong reinforcement learning~\cite{lecarpentier2021lipschitzlifelongreinforcementlearning,xie2020deep,fu2022model,lu2020reset,auger2013continuous,zou2024distributed,zhang2024distributed} is the process of learning a series of problems from unknown MDPs (Markov Decision Processes) online.
Each time an MDP is sampled, it is treated as a separate RL (Reinforcement Learning) problem, where the agent can understand the environment and adjust its own policy~\cite{da2018autonomously,hawasly2013lifelong,abel2018policy,zhang2024collaborative,zhang2024modeling}. The goal is for the agent to interact with the environment using policy $\pi$ to achieve the maximum expected reward.
We can reasonably believe that the knowledge gained in similar MDPs can be reused.
We note that despite recent work on continual and lifelong RL context, e.g., learning models of the non-stationary task environment~\cite{xie2020deep}, identifying reusable skills~\cite{lu2020reset}, or estimating Bayesian sampling posteriors~\cite{fu2022model}, these prior work are not applicable to MCTS in lifelong learning settings. It requires to re-examine and derive adaptive UCT bounds, for knowledge transfer.
 
