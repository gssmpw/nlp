\section{Evaluations}
\label{sec:eval}
Our experiments evaluate LiZero on series of ten learning tasks with varying transition probabilities and rewards. We demonstrate LiZero's ability to transfer past knowledge in MCTS-based planning, resulting in significant convergence speedup (3$\sim$4x) and early reward improvement (about 31\% average improvement during the first half of learning process) in lifelong planning problems.
All experiments are conducted on a Linux machine with AMD EPYC 7513 32-Core Processor CPU and an NVIDIA RTX A6000 GPU, implemented in python3.
All source codes are made available in the supplementary material.


\begin{figure*}[h]
\centering

  \subfigure[Task 1]{\includegraphics[width=0.23\textwidth]{data/task/task1.eps}
  }\hfill
  \subfigure[Task 2]{\includegraphics[width=0.23\textwidth]{data/task/task2.eps}
  }\hfill
  \subfigure[Task 6]{\includegraphics[width=0.23\textwidth]{data/task/task6.eps}
  }\hfill
  \subfigure[Task 10]{\includegraphics[width=0.23\textwidth]{data/task/task10.eps}
  }\hfill
  \vspace{-0.15in}
  \caption{
  Comparing LiZero with MCTS and lifelong RL baselines. We demonstrate the convergence of different algorithms on representatives Tasks 1, 2, 6, and 10, in a non-stationary sequence of ten tasks. In Task~1, since no prior knowledge is yet available, our LiZero and other MCTS baselines show similar convergence speed and optimal rewards. From Task~2 to Task~10, as more knowledge from past tasks gets transferred to the new task by LiZero, it outperforms all baselines with more significantly improved convergence speed. In Task~10 with maximum past knowledge, LiZero demonstrates the largest improvement in convergence speed and optimal reward.
  }
  \label{fig:task}
\end{figure*}



\input{table}



\begin{figure*}[h]
\centering
    \subfigure[]{\includegraphics[width=0.45\textwidth]{data/performance.eps}
  \label{fig:per}
  }\hfill
  \subfigure[ ]{\includegraphics[width=0.45\textwidth]{data/distance.eps}
  \label{fig:ab}
  }\hfill
  \vspace{-0.15in}
  \caption{
  In Figure~\ref{fig:per}, LiZero shows a comfortable speedup of 3$\sim$4x, compared with MCTS and lifelong RL baselines, in terms of achieving the same level of optimal rewards with higher sample efficiency. In Figure~\ref{fig:ab}, Our ablation study comparing different distance estimators in LiZero-U, LiZero-P, and LiZero-N, while MCTS-R can be viewed as a baseline without distance estimator. The relevant performance of these algorithms are provided in Table~\ref{tab:task} and Figure~\ref{fig:per} and thus not repeated here.The superior performance of LiZero is indeed resulted from the use of aUCT in MCTS. The tighter aUCT bounds we use, the higher performance we can obtain.
  }
\end{figure*}




In the evaluation, we consider some state-of-the-art baselines using MCTS and lifelong RL. In particular, we consider two versions of MCTS algorithms that leverage  UCT~\cite{winands2024monte,kocsis2006bandit,chengspeculative}: MCTS-R denotes a version that restarts the search from scratch for each new task, and MCTS-O denotes a version that is oblivious to the non-stationary task dynamics and continues to build upon the search tree from the past. We also consider state-of-the-art MCTS using pUCT, similar to MuZero and related algorithms~\cite{Schrittwieser_2020}. We have two lifelong RL algorithms: RMax~\cite{brafman2002r} and LRMax~\cite{lecarpentier2021lipschitz}, which exploits a similar Lipschitz continuity in RL, but do not consider MCTS using upper confidence bounds. We evaluate three versions of LiZero using different methods for estimating aUCT by computing the task distances, as presented in Section~\ref{sec:distance}. LiZero-U employs a direct distance estimate based on Definition~\ref{def:gobal}; LiZero-P is the data-driven distance estimater $\hat{d}_2$ using samples following the search policy; and LiZero-N is the neural-network based estimator $\hat{d}_{para}$  using parameter distances. 







The experimental environment we used is a variation of the "tight" task by Abel et al.~\cite{abel2018policy}. It generates a non-stationary sequence of ten learning tasks. Each task consists of a $25\times 25$ grid world, with the initial state located at the center, and four possible actions: up, down, left, and right.
The three cells in the top-right corner and one cell in the bottom-left corner are designated as goal cells. For each task, the reward for the goal cells is randomly chosen from the range [0.9, 1].
The remaining cells will randomly generate interference rewards within the range [0, 0.1].
Its state transition matrix selects its own slip probability (performing an action different from the chosen one) within the range [0, 0.1].
This ensures that the sequence of tasks have varying reward and transition probabilities.
Each task for 1,000 epochs. These operations are repeated multiple times to narrow the confidence interval.




Figure~\ref{fig:task} shows the convergence of different algorithms on representatives Tasks 1, 2, 6, and 10, in a non-stationary series of ten task. As tasks are drawn sequentially, LiZero-U, LiZero-P, and LiZero-N algorithms converge more rapidly than the MCTS and lifelong RL baselines. This speedup becomes evident as early as the second task (Task~2) -- while similar convergences are observed in Task~1 as no prior knowledge is yet available. From Task~2 to Task~10, as more knowledge from past tasks gets transferred to the new task by LiZero, it outperforms all baselines in more significantly improved convergence speed. In Task~10 with maximum past knowledge, LiZero outperforms all baselines in convergence speed and optimal reward. MCTS-O (which is oblivious to changing task dynamics) exhibits increased deficiency, as tasks further evolve, and perform worse than MCTS-R (which restarts the search from scratch).





In Table~\ref{tab:task}, we summarize the average rewards (and their standard deviations) obtained in sequential tasks by different algorithms during their first 500 epochs (i.e., first half of the learning process). LiZero algorithms achieves about 31\% early reward improvement on average. As for MCTS baselines with UCT, MCTS-R shows similar reward across different tasks, while MCTS-O demonstrates higher volatility -- due to its reliance on how task dynamics evolve. pUCT achieves higher performance due to the use of improved probabilistic UCT similar to MuZero. All MCTS baselines show better results than lifelong RL algorithms (i.e., RMax and LRMax), which are known to be less sample efficient and require more epochs for exploration/exploitation. With more accurate distance estimates – i.e., from Lizero-N to
LiZero-P and to LiZerio-U – we observe further improved results due to better knowledge transfer that comes with more accurate aUCT calculations.





To evaluate the speedup of LiZero, Figure~\ref{fig:per} shows the average number of epochs needed by different algorithms to achieve 60\%, 70\%, and 80\% of the optimal reward, respectively. We note that LiZero shows a comfortable speedup of 3$\sim$4x, compared with MCTS and lifelong RL baselines, while RL baselines are clearly much less sample efficient that MCTS-based planning in general. We do not go beyond 80\% in this plot since some baselines are never able to achieve more than 80\% of the optimal reward that LiZero obtains. The results provide numerical examples of the acceleration factor $\Gamma$ characterized in Theorem~\ref{the:converage}. A more accurate aUCT bound (like in LiZero-U) generally means further acceleration/speedup.



\textbf{Ablation Study.} Our ablation study considers the impact of distance estimator on performance. Figure~\ref{fig:ab} shows the distance estimators in LiZero-U, LiZero-P, and LiZero-N (each with decreasing accuracy) across the sequence of tasks, while for the purpose of ablation study, MCTS-R can be viewed as an algorithm without distance estimator. Comparing the performance of these algorithms in Table~\ref{tab:task} and Figure~\ref{fig:per}, we see that the superior performance of LiZero is
indeed resulted from the use of aUCT in MCTS -- The tighter aUCT bounds we use, the higher performance we can achieve. Using no distance estimator and thus only UCT (in MCTS-R) leads to the lowest performance. Further, as tasks are drawn, the distance estimates decrease quickly, and by the third task, its is already very small, implying accurate aUCT calculation for knowledge transfer.


