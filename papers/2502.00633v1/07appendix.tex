%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Appendix / supplemental material}

\subsection{Proof of Theorem~\ref{Optimal_Q_Lipschitz}}

\begin{proof}{Proof of Theorem~\ref{Optimal_Q_Lipschitz}}
Since in the MCTS UCB algorithm, the estimated Q-values are obtained through multiple simulations, we need to analyze how the differences in simulation results between two MDPs affect the estimated Q-values.

However, due to the randomness involved in the simulation process of the two MDPs:
\begin{itemize}
    \item \textbf{Transition randomness: }Due to different transition probabilities, the two MDPs may move to different next states even when starting from the same state and action.
    \item \textbf{Action selection randomness: }When using the UCB algorithm, action selection depends on the current statistical information, which in turn relies on the past simulation results.
\end{itemize}
The randomness mentioned above makes it impossible for us to compare two independent random simulation processes directly~\cite{qiao2024br,gao2024cooperative,riis2024mastering,chen2024survey,zhang2025network,yin2025predefined}.

To eliminate the impact of randomness, we need to construct a coupled simulation process for the two MDPs in the same probability space, allowing for a direct comparison between them.
Then we will incorporate the additional errors caused by randomness into the analysis as error terms.
For this purpose, we present the following assumptions.
\begin{assumption}
Let us temporarily assume that the actions selected in each simulation are the same for the two MDPs.
\begin{itemize}
    \item \textbf{Initial action consistency:} The simulation starts from the same state$s$
    \item  \textbf{Action selection consistency:} The same action $a$ is chosen in each state.
\end{itemize}
\end{assumption}
Note: This is a strong assumption and may not hold in practice. We will discuss its impact later.

Thus, we can obtain the difference in cumulative rewards between the two MDPs in a single simulation as:
\begin{equation}
\begin{aligned}
\Delta G = G_M - G_{M^{\prime}} = \sum\limits_{t=0}^{T}\gamma^{t}(R(s_t^{M},a_t)-R^{\prime}(s_t^{M^{\prime}},a_t))
\end{aligned}
\end{equation}
Where \(s_t^{M}\) and \(s_t^{M^{\prime}}\) are the states of the two MDPs at step \(t\), and \(a_t\) is the action selected at step \(t\).

So we can get
\begin{equation}
\left|Q_M^{n_1}(s,a)-Q_{M^\prime}^{n_2}(s,a)\right| = \left|\frac{1}{n_1}\sum_{i=1}^{n_1}G_{M,i} - \frac{1}{n_2}\sum_{i=1}^{n_2}G_{M,i}\right|\leq \bar{\Delta G} =\left|\frac{1}{n} \sum_{i=1}^{n}\Delta G_i\right|
\end{equation}
where $n = \min\{n_1,n_2\}$
To estimate the expectation and variance of \(\Delta G\), we need to analyze how the differences in the state sequences affect the cumulative rewards.

We present several settings for the state differences.
\begin{itemize}
    \item \textbf{Probability of state difference:} At each time step \(t\), the probability that the states of the two MDPs differ is denoted as \(p_t\).
    \item \textbf{Initial state is the same: }\(p_0 = 0\).
    \item \textbf{State difference propagation:} Due to differences in transition probabilities, state differences may accumulate in subsequent time steps.
\end{itemize}
Since the probability of state differences occurring at each step is difficult to calculate precisely, we can use the total variation distance to estimate the probability of transitioning to different states.
We present the definition of the total variation distance between the transition probabilities of the two MDPs and a recursive method for calculating the probability of state differences.

\begin{definition}
Under action \(a_t\), starting from state \(s_t\), the total variation distance between the transition probabilities of the two MDPs is:
\begin{equation}
\begin{aligned}
D_{TV}(P,P^{\prime}) = \frac{1}{2}\sum\limits_{s^{\prime}}|P(s^{\prime}|s_t,a_t)-P^{\prime}(s^{\prime}|s_t,a_t)|
\end{aligned}
\end{equation}
\end{definition}
Thus, starting from the same state \(s_t\) and action \(a_t\), the probability that the two MDPs transition to different next states is at most \(D_{TV}(P, P^{\prime}) \leq \frac{\Delta P}{2}\).

Thus, the probability of state differences occurring can be recursively expressed as:
\begin{equation}
\begin{aligned}
p_{t+1} \leq p_{t} + (1-p_{t})\cdot D_{TV}(P,P^{\prime}) \leq p_t + \frac{\Delta P}{2}
\end{aligned}
\end{equation}
So
\begin{equation}
\begin{aligned}
p_t \leq t \cdot \frac{\Delta P}{2}
\end{aligned}
\end{equation}

Thus, at each time step \(t\), the expected difference in cumulative rewards is:
\begin{equation}
\begin{aligned}
\mathbb{E}[|\Delta G|] 
&=\mathbb{E}[\sum_{t=0}^T\gamma^{t}(R(s_t^{M},a_t)-R^{\prime}(s_t^{M^{\prime}},a_t))] \\
& = \sum_{t=0}^T\gamma^{t}(\underbrace{\mathbb{E}[R(s_t^{M},a_t)-R^{\prime}(s_t^{M},a_t)]}_{\text{The impact of reward function differences}}+ \underbrace{\mathbb{E}[R^{\prime}(s_t^{M},a_t)-R^{\prime}(s_t^{M^{\prime}},a_t)]}_{\text{Reward differences caused by state differences}}) \\
& \leq \sum_{t=0}^T\gamma^{t}(\Delta R + 2R_{\max}\cdot p_{t})\\
& = \frac{\Delta R}{1-\gamma} + \sum_{t=0}^T\gamma^{t}\cdot 2R_{\max}\cdot t \cdot \frac{\Delta P}{2}\\
& = \frac{\Delta R}{1-\gamma} + R_{\max}\Delta P\sum_{t=0}^{T}t\gamma^t\\
& = \frac{\Delta R}{1-\gamma} + R_{\max}\Delta P\cdot \frac{\gamma}{(1-\gamma)^2}
\end{aligned}
\end{equation}

To estimate the variance of the cumulative reward difference, since the cumulative reward is bounded, its variance is also finite.
We can easily obtain
\begin{equation}
\begin{aligned}
|\Delta G| \leq G_{\max} = \frac{2R_{\max}}{1-\gamma}
\end{aligned}
\end{equation}

According to Hoeffding:

\begin{equation}
\begin{aligned}
P(|\bar{\Delta G} - \mathbb{E}[\bar{\Delta G}]|\geq \epsilon) \leq 2\exp(-\frac{2n\epsilon^2}{G_{\max}^2})
\end{aligned}
\end{equation}

Thus, with probability at least \(1 - \delta\), we have:
\begin{equation}
\begin{aligned}
    |\hat{Q}_M^n(s,a)-\hat{Q}_{M^\prime}^n(s,a)|
    &\leq \mathbb{E}[|\Delta\bar{G}|] + G_{\max}\sqrt{\frac{\ln(2/\delta)}{2n}}\\
    & = \frac{\Delta R}{1-\gamma} + R_{\max}\Delta P\cdot \frac{\gamma}{(1-\gamma)^2} + \frac{2R_{\max}}{1-\gamma}\sqrt{\frac{\ln(2/\delta)}{2n}}\\
    & = \frac{1}{1-\gamma}(\Delta R + \frac{R_{\max}\gamma}{1-\gamma}\Delta P) +     \frac{2R_{\max}}{1-\gamma}\sqrt{\frac{\ln(2/\delta)}{2n}}\\
    & = L(\Delta R + \kappa \Delta P) + L_2
\end{aligned}
\end{equation}
    
\end{proof}






\subsection{Proof of Theorem~\ref{the:converage}}

\begin{proof}{Proof of Theorem~\ref{the:converage}}
First, we consider the case of a single MDP and assume that we have a "universal" upper bound \( U(s, a) \geq Q_{M}^{*}(s, a) \).


\begin{lemma}
Since \( U(s, a) \geq Q_{M}^{*} \) holds for all \( (s, a) \), and initially \( Q(s, a) \leq U(s, a) \), for any update, \( Q(s, a) \) maintains \( Q(s, a) \leq U(s, a) \) and \( Q(s, a) \geq (\text{a non-negative expected estimate}) \).
\end{lemma}

The above two points illustrate
Since we update using \( Q(s, a) = \min\{\hat{Q}(s, a), U(s, a)\} \)
And since \( U(s, a) \geq Q^{*}(s, a) \), during all sampling processes, if \( \hat{Q}(s, a) \) overestimates \( Q^{*}(s, a) \) significantly, it will still be truncated by \( U(s, a) \), ensuring that \( Q(s, a) \leq U(s, a) \).
When \( \hat{Q}(s, a) \) gradually approaches \( Q^{*}(s, a) \), it will no longer be truncated. This does not hinder the convergence of \( Q \) to \( Q^{*} \).


\begin{theorem}[Convergence in a Single MDP]
If there are infinitely many samples for each state \(s\) and its available actions \(a\) (i.e., every branch in the MCTS search tree is "continuously" expanded), then the \(Q(s, a)\) generated by the above update formula almost surely converges to \(Q_{M}^{*}(s, a)\).
\end{theorem}




Now we aim to demonstrate that after completing certain MDPs (tasks) \(\bar{M}_1, \bar{M}_2, \dots, \bar{M}_m\), and then switching to a new MDP \(M\), the algorithm achieves faster convergence.

First, we analyze the classic scenario without upper bounds. In a finite state-action space, to achieve the desired outcome with high probability \(1-\delta\): 
\begin{equation}
\begin{aligned}
    \max_{(s,a)\in\mathcal{S}\times \mathcal{A}}|Q_{n}(s,a) - Q_{M}^{*}(s,a)| \leq \epsilon
\end{aligned}
\end{equation}

The standard UCT/UCB theory typically provides a time complexity of \( \tilde{O}\left(\frac{|\mathcal{S}||\mathcal{A}|}{(1-\gamma)^3 \epsilon^2} \ln\frac{1}{\delta}\right) \). To prove this theorem, we just need to analyze the acceleration factor $\Gamma$, comparing the sampling complexity of our aUCT and standard UCT.

More specifically, if we examine each specific \((s, a)\), the analysis often resembles that of multi-armed bandits: for "suboptimal" \((s, a)\), approximately \(\tilde{O}\left(\frac{1}{(\Delta^{M}_{(s,a)})^2}\ln\frac{1}{\delta}\right)\) samples are required.
Where \(\Delta^{M}_{(s,a)} = Q_{M}^{*}(s,a^{*}) - Q_{M}^{*}(s,a)\) is the value gap between the action and the optimal action. Summing up the exploration costs for all state-action pairs gives a total magnitude of \(\sum_{(s,a)} \frac{1}{(\Delta^{M}_{(s,a)})^2}\).

Now we introduce the case with upper bounds and analyze how to reduce the number of samples across different MDPs.

To quantitatively represent this acceleration, we divide the state-action pairs \((s, a)\) into two groups:
\begin{itemize}
    \item $\mathcal{S}_{1}:$ Upper bounds are sufficiently tight and are truncated to be lower than the optimal action from the very beginning.
    \begin{equation}
    \begin{aligned}
        \mathcal{S}_1 = \left\{ (s,a)|\exists i: U_{\bar{M}_i}(s,a)< Q_{M}^{0}(s,a)\right\}
    \end{aligned}
    \end{equation}
    \item $\mathcal{S}_{0}:$ The upper bounds are not "tight enough," i.e.,
    \begin{equation}
    \begin{aligned}
        \mathcal{S}_0 = \text{remaining actions}
    \end{aligned}
    \end{equation}
\end{itemize}


For \((s,a) \in \mathcal{S}_1\):

We treat each sampling as a multi-armed bandit. Let the true mean of the optimal arm be \(\mu^{*}\). For a certain arm \(j\), its true mean is known to satisfy \(\mu_j \leq U_j < \mu^{*}\).

Even if we truncate \(\hat{\mu}_n(j)\) at \(U_j\), the UCB algorithm's "optimistic estimate" for this arm at step \(n\) is still:
\begin{equation}
\begin{aligned}
    Q_{n}(j) = \min \left\{ \hat{\mu}_{n}(j), U_j \right\} + c\sqrt{\frac{\ln(n)}{N_j(n)}}
\end{aligned}
\end{equation}


\begin{equation}
\begin{aligned}
    U_j + c\sqrt{\frac{\ln(n)}{N_j(n)}} < \mu^{*}
\end{aligned}
\end{equation}

Let \(\Delta = \mu^* - U_j\). As long as:
\begin{equation}
\begin{aligned}
    \sqrt{\frac{\ln(n)}{N_j(n)}}\leq \frac{\Delta}{2c}
\end{aligned}
\end{equation}
From the above, it can be ensured that \(Q_n(j)\) cannot exceed \(\mu^{*} - \Delta/2\).
So
\begin{equation}
\begin{aligned}
    N_j(n) \geq \frac{4c^2\ln(n)}{\Delta^2}
\end{aligned}
\end{equation}
Where we obtain a sampling time complexity of \(\tilde{O}(\ln n)\).



For \((s,a) \in \mathcal{S}_0\), these \((s,a)\) cannot be pruned by "truncation." They still require multiple samples, as in classic UCT, to determine whether they are truly optimal. For any \((s,a) \in \mathcal{S}_0\), we still need approximately \( O\left(\frac{1}{(\Delta^{M}_{(s,a)})^2} \ln\frac{1}{\delta}\right) \) samples to distinguish that it is not as good as \((s,a^{*})\).
Thus, the sampling complexity of our algorithm is:  
\begin{eqnarray}
   X_{\rm aUCT} = \sum_{(s,a)\in \mathcal{S}_{1}}\tilde{O}\left(\ln n\right) + \sum_{(s,a)\in \mathcal{S}_{0}}\tilde{O}\left(\frac{1}{(\Delta^{M}_{(s,a)})^2} \ln\frac{1}{\delta}\right),
\end{eqnarray}
Using the fact that \( \tilde{O}(\ln n) \sim \tilde{O}(\ln\frac{1}{\delta}) \), we can rewrite this as
\begin{eqnarray}
\label{eq:aUCT}
   X_{\rm aUCT} = \sum_{(s,a)\in \mathcal{S}_{1}}\tilde{O}\left( \ln\frac{1}{\delta} \right) + \sum_{(s,a)\in \mathcal{S}_{0}}\tilde{O}\left(\frac{1}{(\Delta^{M}_{(s,a)})^2} \ln\frac{1}{\delta}\right).
\end{eqnarray}
In contrast, the sampling complexity of the standard UCT can be obtained using the same analysis, i.e.,
\begin{eqnarray}
\label{eq:UCT}
 X_{\rm UCT} =  \sum_{(s,a)\in \mathcal{S}_0 \cup \mathcal{S}_1}\tilde{O}\left(\frac{1}{(\Delta^{M}_{(s,a)})^2} \ln\frac{1}{\delta}\right).
\end{eqnarray}
Comparing the order bounds from Equation~(\ref{eq:UCT}) and Equation~(\ref{eq:aUCT}), we can find the acceleration factor $\Gamma$ as
\begin{equation}
\begin{aligned}
\Gamma =\frac
{\sum_{(s,a)\in \mathcal{S}_1\cup \mathcal{S}_0 } \frac{1}{(\Delta^{\mathcal{M}}_{(s,a)})^2}}
{\sum_{(s,a)\in\mathcal{S}_1} (1) + 
\sum_{(s,a)\in\mathcal{S}_{0}} \frac{1}{(\Delta^{\mathcal{M}}_{(s,a)})^2}},
\end{aligned}
\end{equation}
which is the desired result in the theorem.


\end{proof}





\subsection{Proof of Theorem~\ref{the:err_signal_policy}}

\begin{proof}{Proof of Theorem~\ref{the:err_signal_policy}}
First, we need to establish unbiasedness and boundedness.
For unbiasedness, we can derive:  
\begin{equation}
\begin{aligned}
    \mathbb{E}[X_i] =\mathbb{E}_{(s,a)\sim \pi} [\frac{\mathcal{U}(s,a)}{\pi(s,a)}\cdot \Delta X(s, a)]= \mathbb{E}_{(s,a)\sim \mathcal{U}} [\Delta X(s, a)] = d(M,M^{\prime})
\end{aligned}
\end{equation}
Therefore, $\mathbb{E}[\hat{d}_{\mathcal{U}}] = d(M, M^{\prime})$, meaning $\hat{d}_{\mathcal{U}}$ is an unbiased estimator.

\begin{equation}
\begin{aligned}
    w_i = \frac{\mathcal{U}(s_i,a_i)}{\pi(s_i,a_i)}\leq \frac{\mathcal{U}_{\max}}{\alpha}
\end{aligned}
\end{equation}
Where $\mathcal{U}{\max} = \max_{(s, a)} \mathcal{U}(s, a) = \frac{1}{|\mathcal{S}| \cdot |\mathcal{A}|}$.
So we can get:

\begin{equation}
\begin{aligned}
    X_i = w_i\Delta X(s_i,a_i)\leq (\frac{\mathcal{U}_{\max}}{\alpha}) b
\end{aligned}
\end{equation}

So we can get $X_i\in[0,C]$ where $C = \frac{\mathcal{U}_{\max}}{\alpha} b$.

Based on the above analysis, we have $\bar{X}_{N} = \frac{1}{N} \sum_{i=1}^{N} X_i = \hat{d}_{\mathcal{U}}$, $\mu = \mathbb{E}[X_i] = d(M,M^{\prime})$. According to Hoeffding's inequality, for $\bar{X}_{N} \in [0, C]$, we have:

\begin{equation}
\begin{aligned}
    \text{Pr}\{|\bar{X}_{N} - \mu|\geq \epsilon \} \leq 2\exp(-\frac{2 N \epsilon^2}{C^2})
\end{aligned}
\end{equation}
To achieve a confidence level of $\delta$, it requires:
\begin{equation}
\begin{aligned}
    2\exp(-\frac{2 N \epsilon^2}{C^2}) \leq \delta \Leftrightarrow \exp(-\frac{2 N \epsilon^2}{C^2}) \leq \frac{\delta}{2} \Leftrightarrow -\frac{2 N \epsilon^2}{C^2} \leq \ln \frac{\delta}{2}\Leftrightarrow \frac{2 N \epsilon^2}{C^2} \geq \ln \frac{2}{\delta} \Leftrightarrow N \geq \frac{C^2}{2\epsilon^2} \ln \frac{2}{\delta}
\end{aligned}
\end{equation}

We get if fulfilled:
\begin{equation}
\begin{aligned}
    N\geq \frac{1}{2\epsilon^2} (\frac{\mathcal{U}_{\max}}{\alpha} b)^2\ln \frac{2}{\delta}
\end{aligned}
\end{equation}
There is then a high probability error upper bound:
\begin{equation}
\begin{aligned}
    \text{Pr}\{|\hat{d}_{\mathcal{U}} - d(M,M^{\prime})|\leq \epsilon\}\geq 1-\delta
\end{aligned}
\end{equation}
\end{proof}



\subsection{Proof of Theorem~\ref{the:err_multi_policy}}


\begin{proof}{Proof of Theorem~\ref{the:err_multi_policy}}
Constructing a martingale difference, let:
\begin{equation}
\begin{aligned}
    S_n := \sum_{k=1}^{n}(X_k - d(M,M^{\prime})), Y_k := X_k-\mathbb{E}[X_k|\mathcal{F}_{k-1}]
\end{aligned}
\end{equation}
According to the martingale condition in formula~\ref{eqn:Martingale}, we know that \( Y_k = X_k - d(M, M^{\prime}) \), and \( S_n = \sum_{k=1}^n Y_k \) satisfies \( \mathbb{E}[Y_k | \mathcal{F}_{k-1}] = 0 \).
Thus, \(\{S_n, \mathcal{F}_n\}\) is a martingale process.

Since \(\pi_{k}(s, a) \geq \alpha \Rightarrow w_k \leq \frac{\mathcal{U}_{\max}}{\alpha}\), and \(\Delta X(s, a) \leq b \Rightarrow X_k = w_k \Delta X(s_k, a_k) \leq \frac{\mathcal{U}_{\max}}{\alpha} b =: C\). Therefore, we have:
\begin{equation}
\begin{aligned}
    |Y_k|\leq \max\{X_k,d(M,M^{\prime})\}\leq C
\end{aligned}
\end{equation}
According to the Azuma-Hoeffding inequality for bounded martingale differences, we have:
\begin{equation}
\begin{aligned}
    \text{Pr}\{|S_n|\geq t\}\leq 2\exp(-\frac{t^2}{2NC^2})
\end{aligned}
\end{equation}
Let \( t = N\epsilon \), then \( |S_n| \geq t \) is equivalent to \( \left|\sum_{k=1}^n X_k - N d(M, M^{\prime})\right| \geq N\epsilon \), that is:
\begin{equation}
\begin{aligned}
    |\hat{d}_{\mathcal{U}}^{(n)}-d(M,M^{\prime})|\geq \epsilon
\end{aligned}
\end{equation}
So:
\begin{equation}
\begin{aligned}
    \text{Pr}\{|\hat{d}_{\mathcal{U}}^{(N)}-d(M,M^{\prime})|\geq \epsilon\}\leq 2\exp(-\frac{N\epsilon^2}{2C^2})
\end{aligned}
\end{equation}
Thus, as long as \( N \geq \frac{2C^2}{\epsilon^2} \ln \frac{2}{\delta} \), we have \( \text{Pr}\{|\hat{d}_{\mathcal{U}}^{(N)} - d(M, M^{\prime})| \geq \epsilon\} \leq \delta \).
\end{proof}


\subsection{{Proof of Theorem~\ref{the:network}}}

\begin{proof}{Proof of Theorem~\ref{the:network}}
We decompose $d_{\mathcal{U}}$.
\begin{equation}
\begin{aligned}
    d_{\mathcal{U}}(M,M_i) & = \mathbb{E}_{(s,a)\sim \mathcal{U}}[\underbrace{|R_s^a-R_s^{a,(i)}|}_{\text{Reward difference}} + \underbrace{\kappa \sum_{s^{\prime}}|P_{ss^{\prime}}^a - P_{ss^{\prime}}^{a,(i)}|}_{\text{transition  difference}}]\\
    & \simeq \mathbb{E}_{(s,a)\sim \mathcal{U}}[|R_s^a-R_s^{a,(i)}| + \kappa ||\Psi_{\phi}(s,a) - \Psi_{\phi_i}(s,a)||_1]\\
    & \leq \mathbb{E}_{(s,a)\sim \mathcal{U}}[L_3\rho(\phi,\phi_i)  + \kappa L_3\rho(\phi,\phi_i)]\\
    &\leq L_3\rho(\phi,\phi_i)+ \kappa L_3\rho(\phi,\phi_i)\\
    & = (1+\kappa) L_3 \rho(\phi,\phi_i)\\
    & = (1+\kappa) L_3 \hat{d}_{para}(M,M_i)
\end{aligned}
\end{equation}
\end{proof}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Pseudo-code}

\begin{algorithm}[ht]
\caption{UMCTS}
\label{alg:umcts}
\begin{algorithmic}[1]
\REQUIRE $\{\mathcal{M}_1,\dots,\mathcal{M}_M\}, \mathcal{U}, \kappa, L, L_2^{(i)}, \gamma, R_{\max}, C, T$
\FOR{$i = 1$ to $M$}
    \STATE Repeat sampling \( (s, a) \) from the uniform distribution \( \mathcal{U} \) to update \( R \) and \( P \).
   \FOR{$j = 1$ to $M$}
      \STATE $d(\mathcal{M}_i,\mathcal{M}_j)
      \gets \mathbb{E}_{(s,a,s') \sim \mathcal{U}}
      \bigl[\,
         |R_s^a - \overline{R}_s^a|
         + \kappa \,|P_{ss'}^a - \overline{P}_{ss'}^a|
      \bigr]$
   \ENDFOR

\vspace{5pt}
\STATE Initialize root node $s_0$, set $N(\cdot), N(\cdot,\cdot), W(\cdot,\cdot)$ to $0$
\FOR{$t = 1$ to $T$}
  \STATE \textbf{Selection}:
  \STATE \quad Set current node $s \leftarrow s_0$
  \WHILE{\text{child nodes of $s$ are fully expanded}}
    \STATE Choose $a = \underset{a}{\mathrm{argmax}}\;
    \bigl(Q(s,a)\bigr)$ \quad \text{// using Eq.\,(*) below}
    \STATE $s \leftarrow \text{child node after action $a$}$
  \ENDWHILE

  \STATE \textbf{Expansion}:
  \STATE \quad Expand one non-visited action $a_{\mathrm{new}}$ at $s$, 
    sample $s'$ from environment or model
  \STATE \quad Create new child node $s'$, set $N(s',\cdot)=0$, $W(s',\cdot)=0$

  \STATE \textbf{Simulation}:
  \STATE \quad Perform a (light) rollout or default policy from $s'$ to terminal or horizon
  \STATE \quad Receive cumulative reward $G$

  \STATE \textbf{Backpropagation}:
  \STATE \quad \text{Traverse back from $s'$ to $s_0$ along visited path}
  \FORALL{\text{visited state-action pairs } $(\tilde{s}, \tilde{a})$}
    \STATE $N(\tilde{s}) \,\leftarrow\, N(\tilde{s})+1$
    \STATE $N(\tilde{s},\tilde{a}) \,\leftarrow\, N(\tilde{s},\tilde{a})+1$
    \STATE $W(\tilde{s},\tilde{a}) \,\leftarrow\, W(\tilde{s},\tilde{a}) + G$
    \STATE \text{// Update $Q(\tilde{s},\tilde{a})$ with UMCTS bound:}
    \STATE $U_{\bar{\mathcal{M}}}(\tilde{s},\tilde{a}) \gets 
      Q_{\bar{\mathcal{M}}}^{*}(\tilde{s},\tilde{a}) 
      + L \cdot d(\mathcal{M},\bar{\mathcal{M}}) 
      + L_2^{(i)}$
    \STATE $U(\tilde{s},\tilde{a}) \gets 
      \min\bigl\{\frac{R_{\max}}{1-\gamma}\,,\,
                 U_{\bar{\mathcal{M}}}(\tilde{s},\tilde{a}),\,\dots\bigr\}$
    \STATE $Q(\tilde{s},\tilde{a}) \gets 
      \min\!\Bigl\{
        \dfrac{W(\tilde{s},\tilde{a})}{N(\tilde{s},\tilde{a})}
        + C\,\sqrt{\dfrac{\ln N(\tilde{s})}{N(\tilde{s},\tilde{a})}},\;
        U(\tilde{s},\tilde{a})
      \Bigr\}\quad (*)$
  \ENDFOR
\ENDFOR

\ENDFOR
\end{algorithmic}
\end{algorithm}






\begin{algorithm}[ht]
\caption{UMCTS with Importance Sampling}
\label{alg:umcts_is}
\begin{algorithmic}[1]
\REQUIRE Tasks $\{\mathcal{M}_1,\dots,\mathcal{M}_M\}$, each partially known; Uniform distribution $\mathcal{U}(s,a)$;Lipschitz constants $L, L_2^{(i)}$; Discount factor $\gamma$, maximum reward $R_{\max}$; Exploration constant $C$; Number of search iterations $T$;A (default) policy $\pi$ used in Simulation for importance sampling; 

\STATE \textbf{Function}~{Distance($\mathcal{M}, \bar{\mathcal{M}}, \pi$)}
\STATE ~~~~$\displaystyle \Delta X(s,a) \;\triangleq\; \Delta R_{s}^a \;+\;\kappa\,\Delta P_{s}^a$
\STATE \textbf{return} $\displaystyle
  \mathbb{E}_{(s,a)\sim \pi}
  \Bigl[
    \frac{\mathcal{U}(s,a)}{\pi(s,a)}
    \cdot
    \Delta X(s,a)
  \Bigr]$

\STATE \textbf{// For each task } $\mathcal{M}_i$
\FOR{$i = 1$ to $M$}
  \STATE Initialize root node $s_0$, set $N(\cdot)=0,\, N(\cdot,\cdot)=0,\,W(\cdot,\cdot)=0$
  \STATE \text{(Optionally maintain a buffer } $\mathcal{D}_i$ \text{ for storing samples }(s,a)\text{)}

  \FOR{$t = 1$ to $T$}
    %------------------------------------
    \STATE \textbf{Selection:}
    \STATE \quad $s \;\leftarrow\; s_0$
    \WHILE{all actions from $s$ are fully expanded \textbf{and} $s$ not terminal}
      \STATE $a \;\leftarrow\; \underset{a}{\mathrm{argmax}}\;\bigl(Q(s,a)\bigr)$ 
          \quad // UCB or UMCTS criterion
      \STATE $s \;\leftarrow\; \text{child node after action }a$
    \ENDWHILE

    %------------------------------------
    \STATE \textbf{Expansion:}
    \IF{\text{$s$ not terminal}}
      \STATE Choose one unvisited action $a_{\mathrm{new}}$ at $s$
      \STATE Sample next state $s' \sim P_i(\cdot \mid s,a_{\mathrm{new}})$  // from environment or model
      \STATE Create child node $s'$, set $N(s',\cdot)=0,\,W(s',\cdot)=0$
    \ENDIF

    %------------------------------------
    \STATE \textbf{Simulation:}
    \STATE \quad Initialize cumulative reward $G \leftarrow 0$
    \STATE \quad $s_{\mathrm{sim}} \leftarrow s'$
    \WHILE{$s_{\mathrm{sim}}$ is not terminal}
      \STATE Pick action $a_{\mathrm{sim}}$ by policy $\pi(\cdot \mid s_{\mathrm{sim}})$
      \STATE Observe reward $r_{\mathrm{sim}} = R_i(s_{\mathrm{sim}}, a_{\mathrm{sim}})$
      \STATE Observe next state $s_{\mathrm{next}} \sim P_i(\cdot \mid s_{\mathrm{sim}}, a_{\mathrm{sim}})$
      \STATE $G \leftarrow G + r_{\mathrm{sim}}$
      \STATE \text{// Update or record increments for } $ R_s^a,\,P_{s,s'}^a$
      \STATE \quad \(\Delta R_{s_{\mathrm{sim}}}^a \), \(\Delta P_{s_{\mathrm{sim}}}^a\) \(\leftarrow\) 
             (computed from new sample)
      \STATE \text{// Optionally store $(s_{\mathrm{sim}}, a_{\mathrm{sim}})$ in $\mathcal{D}_i$ for importance sampling}
      \STATE $s_{\mathrm{sim}} \leftarrow s_{\mathrm{next}}$
    \ENDWHILE

    %------------------------------------
    \STATE \textbf{Backpropagation:}
    \STATE \quad \text{Traverse from $s'$ back to $s_0$ along visited path}
    \FORALL{\text{visited pairs } $(\tilde{s}, \tilde{a})$}
      \STATE $N(\tilde{s}) \;\leftarrow\; N(\tilde{s}) + 1$
      \STATE $N(\tilde{s}, \tilde{a}) \;\leftarrow\; N(\tilde{s}, \tilde{a}) + 1$
      \STATE $W(\tilde{s}, \tilde{a}) \;\leftarrow\; W(\tilde{s}, \tilde{a}) + G$
      \STATE \text{/* Use the Lipschitz bound with distance estimation */}
      \STATE $d(\mathcal{M}_i,\bar{\mathcal{M}}) 
        \;\gets\; \mathrm{Distance}\bigl(\mathcal{M}_i,\bar{\mathcal{M}},\pi\bigr)$
      \STATE $U_{\bar{\mathcal{M}}}(\tilde{s}, \tilde{a})
         \;\gets\;Q_{\bar{\mathcal{M}}}^{*}(\tilde{s},\tilde{a})
         \;+\;L \cdot d(\mathcal{M}_i,\bar{\mathcal{M}})
         \;+\;L_2^{(i)}$
      \STATE $U(\tilde{s},\tilde{a})
         \;\gets\;\min\Bigl\{
           \dfrac{R_{\max}}{1-\gamma},\,
           U_{\bar{\mathcal{M}}}(\tilde{s},\tilde{a}),\dots
         \Bigr\}$
      \STATE \text{/* UMCTS update rule */}
      \STATE $Q(\tilde{s},\tilde{a})
         \;\gets\;\min\Bigl\{
           \dfrac{W(\tilde{s},\tilde{a})}{N(\tilde{s},\tilde{a})}
           + C\,\sqrt{\dfrac{\ln N(\tilde{s})}{N(\tilde{s},\tilde{a})}},
           \;U(\tilde{s},\tilde{a})
         \Bigr\} \quad (*)$
    \ENDFOR
  \ENDFOR
\ENDFOR

\end{algorithmic}
\end{algorithm}











\begin{algorithm}[ht]
\caption{UMCTS with Neural Network Environment Model}
\label{alg:umcts_nn}
\begin{algorithmic}[1]
\REQUIRE  MDPs $\{\mathcal{M}_1,\dots,\mathcal{M}_M\}$, each with trained neural network parameters $\{\phi_1,\dots,\phi_M\}$; A new MDP $M$ (partially known), with neural network $\Psi_{\phi}: \mathcal{S}\times \mathcal{A}\to \Delta(\mathcal{S})$; A distance function $\rho(\phi,\phi_i)\ge 0$ on parameter space (e.g., $\ell_2$-norm); Define $\hat{d}_{para}(M,M_i) = \rho(\phi,\phi_i)$; Lipschitz constants $L, L_2^{(i)}$, discount factor $\gamma$, $R_{\max}$, exploration constant $C$, iterations $T$; A default (simulation) policy $\pi$ for rollouts

\vspace{5pt}
\STATE \textbf{// For each task $M$ (with parameter $\phi$) run UMCTS}
\STATE Initialize root node $s_0$, counters $N(\cdot)=0,\,N(\cdot,\cdot)=0,\,W(\cdot,\cdot)=0$
\FOR{$t = 1$ to $T$}
  %------------------------------------------------
  \STATE \textbf{Selection}:
  \STATE \quad $s \leftarrow s_0$
  \WHILE{\text{all actions from } s \text{ are expanded \textbf{and} } s \text{ not terminal}}
    \STATE $a \;\leftarrow\;\underset{a}{\mathrm{argmax}}\;\bigl(Q(s,a)\bigr)$
    \STATE $s \;\leftarrow\;\text{child node after action }a$
  \ENDWHILE

  %------------------------------------------------
  \STATE \textbf{Expansion}:
  \IF{$s$ not terminal}
    \STATE \text{choose an unvisited action } $a_{\mathrm{new}}$
    \STATE \text{sample } $s' \sim \Psi_{\phi}(\cdot \mid s,a_{\mathrm{new}})$ \quad \text{// neural net predicts next state distribution}
    \STATE \text{create child node } s'
    \STATE $N(s',\cdot)\leftarrow 0,\;W(s',\cdot)\leftarrow 0$
  \ENDIF

  %------------------------------------------------
  \STATE \textbf{Simulation}:
  \STATE \quad $G \leftarrow 0$
  \STATE \quad $s_{\mathrm{sim}} \leftarrow s'$
  \WHILE{$s_{\mathrm{sim}}$ \text{ not terminal}}
    \STATE $a_{\mathrm{sim}} \leftarrow \text{sample from } \pi(\cdot \mid s_{\mathrm{sim}})$
    \STATE \text{// observe reward (possibly from real env or approximated by a learned reward model)}
    \STATE $r_{\mathrm{sim}} = R(s_{\mathrm{sim}}, a_{\mathrm{sim}})$
    \STATE $s_{\mathrm{next}} \sim \Psi_{\phi}(\cdot \mid s_{\mathrm{sim}}, a_{\mathrm{sim}})$

    \STATE $G \;\leftarrow\; G + r_{\mathrm{sim}}$

    \STATE \text{/* update $\phi$ via gradient (e.g. supervised/unsupervised RL objective) */}
    \STATE \quad $\phi \;\leftarrow\; \phi - \eta\,\nabla_{\phi} \mathcal{L}\bigl(\phi;(s_{\mathrm{sim}},a_{\mathrm{sim}},s_{\mathrm{next}})\bigr)$

    \STATE $s_{\mathrm{sim}} \;\leftarrow\; s_{\mathrm{next}}$
  \ENDWHILE

  %------------------------------------------------
  \STATE \textbf{Backpropagation}:
  \STATE \quad \text{traverse from $s'$ back to $s_0$}
  \FORALL{\text{visited state-action pairs } $(\tilde{s}, \tilde{a})$}
    \STATE $N(\tilde{s}) \;\leftarrow\; N(\tilde{s}) + 1$
    \STATE $N(\tilde{s},\tilde{a}) \;\leftarrow\; N(\tilde{s},\tilde{a}) + 1$
    \STATE $W(\tilde{s},\tilde{a}) \;\leftarrow\; W(\tilde{s},\tilde{a}) + G$

    \STATE \text{// parametric distance to previously trained model $\phi_i$}
    \STATE $\hat{d}_{para}(M,M_i) 
      \;\triangleq\;\rho(\phi,\phi_i)$

    \STATE \text{// Lipschitz-based upper bound}
    \STATE $U_{\bar{\mathcal{M}}}(\tilde{s},\tilde{a})
      \;\leftarrow\; 
      Q_{\bar{\mathcal{M}}}^{*}(\tilde{s},\tilde{a})
      \;+\;L\cdot \hat{d}_{para}(M,\bar{\mathcal{M}})
      \;+\;L_2^{(i)}$

    \STATE $U(\tilde{s},\tilde{a})
      \;\leftarrow\;\min\Bigl\{
        \frac{R_{\max}}{1-\gamma},\,
        U_{\bar{\mathcal{M}}}(\tilde{s},\tilde{a}),\dots
      \Bigr\}$

    \STATE \text{// UMCTS update rule}
    \STATE $Q(\tilde{s},\tilde{a})
      \;\leftarrow\;
      \min\Bigl\{
        \dfrac{W(\tilde{s},\tilde{a})}{N(\tilde{s},\tilde{a})}
        + C\sqrt{\dfrac{\ln N(\tilde{s})}{N(\tilde{s},\tilde{a})}},
        \;U(\tilde{s},\tilde{a})
      \Bigr\}
      \quad (*)$
  \ENDFOR

\ENDFOR
\end{algorithmic}
\end{algorithm}