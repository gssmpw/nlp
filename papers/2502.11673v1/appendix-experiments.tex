\section{Further Experimental Evaluations} \label{app:further-experiments}

In this section, we provide further details regarding our experimental evaluations in \cref{sec:experiments}.

\noindent \textbf{All vs Exploitable Strategies.} In addition to \cref{sec:experiments}, we compare the performance of Min-Max, OMD and \cref{algo:phased-aggression-efg-bandit} against a couple of other exploitable strategies strategies. We consider the following constant strategies:

\begin{itemize}[leftmargin=*]
    \setlength{\itemsep}{0.3em}
    \setlength{\parskip}{0pt}
    \item \emph{RaiseK}: Bob raises/calls if and only he has a King, and checks/folds otherwise.
    \item \emph{RandMinMax($\alpha$)}: Bob plays a perturbed version of the Min-Max strategy: In every round, with a small probability $\alpha$, he will play the uniform strategy, and otherwise the Min-Max strategy. 
\end{itemize}

In \cref{fig:all-vs-x2}, we present the amount of money that each of Min-Max, OMD, and Algorithm~\ref{algo:phased-aggression-efg-bandit} extract with respect to the aforementioned exploitable strategies. Specifically, \cref{fig:all-vs-x2} reveals the following. 

\begin{figure*}[ht]
    \centering

    \begin{minipage}{1.0\textwidth}
    \begin{minipage}{0.05\textwidth}
        
    \end{minipage}
    \begin{minipage}{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/All-vs-Rand=0.05.png}
        \caption*{All vs RandMinMax($0.05$)}
    \end{minipage}
    %\hfill
    \begin{minipage}{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/All-vs-Rand=0.1.png}
        \caption*{All vs RandMinMax($0.1$)}
    \end{minipage}
    \begin{minipage}{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/All-vs-Rand=0.15.png}
        \caption*{All vs RandMinMax($0.15$)}
    \end{minipage}
    \begin{minipage}{0.05\textwidth}
        
    \end{minipage}
    \begin{minipage}{1.0\textwidth}
    \centering
    $\;$
    \end{minipage}
    \end{minipage}
    
    \vspace{0.4cm}
    
    \centering
    \begin{minipage}{1.0\textwidth}
    \begin{minipage}{0.05\textwidth}
        
    \end{minipage}
    \begin{minipage}{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/All-vs-Rand=0.3.png}
        \caption*{All vs RandMinMax($0.3$)}
    \end{minipage}
    %\hfill
    \begin{minipage}{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/All-vs-Rand=0.5.png}
        \caption*{All vs RandMinMax($0.5$)}
    \end{minipage}
    \begin{minipage}{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/All-vs-King.png}
        \caption*{All vs RaiseK}
    \end{minipage}
    \begin{minipage}{0.05\textwidth}
        
    \end{minipage}
    \begin{minipage}{1.0\textwidth}
    \centering
    \vspace{0.4cm}
    \caption{All vs Bob comparison for $T=1000$ rounds. The x-axis displays the round $t$, and the y-axis displays how much {\color{mblue}Min-Max}, {\color{mred}OMD}, and {\color{mgreen}\cref{algo:phased-aggression-efg-bandit}} gained from the second algorithm so far. The y-axes have varying scales for readability.}\label{fig:all-vs-x2} \vspace{-0.4cm}
    \end{minipage}
    \end{minipage}
\end{figure*}

\emph{All vs RandMinMax($\alpha$)}: In all plots, our \cref{algo:phased-aggression-efg-bandit} achieves at least the gain of the min-max equilibrium and in fact always improves slightly over it. For small values of $\alpha$ (e.g. $\alpha=0.05$), meaning that Bob plays a (reasonable) strategy very close to the min-max equilibrium, OMD always loses money while \cref{algo:phased-aggression-efg-bandit} wins linearly. For larger values of $\alpha$ (e.g. $\alpha=0.1, 0.15,0.3$), OMD loses an initial amount but slowly starts catching up towards a total positive gain for very large $T$. Finally, when $\alpha$ is large (e,g, $\alpha=0.5$), meaning that Bob plays a highly suboptimal (and not exploitative) strategy, OMD is able to obtain a positive gain much quicker and eventually surpasses our \cref{algo:phased-aggression-efg-bandit} (as it is not restricted to the support of the min-max equilibrium, which in this case is of advantage). 

\emph{All vs RaiseK}: Notice that min-max equilibrium does not exploit RaiseK at all. At the same time, OMD exploits it linearly right away, extracting a near-optimal gain from the opponent. Our \cref{algo:phased-aggression-efg-bandit} also exploits \emph{RaiseK} linearly at a comparable slope, however starting exploitation somewhat delayed due to the risk-averse nature of the algorithm. However, our algorithm consistently exploits weak opponents significantly better than the min-max strategy in all cases, and unlike OMD does so while not risking to lose essentially any money.

In summary, our experimental evaluations reveal the following insights that are in accordance with our theoretical findings: If Alice plays \cref{algo:phased-aggression-efg-bandit}, she secures at least the gain of the min-max strategy, thus not losing against any opponent. Yet, she is able to better exploit strategies that deviate from the min-max strategy, at a level often comparable to standard no-regret algorithms.

\textbf{Implementation Details.} In all experiments, we average $n=5$ runs of repeated play (plotting Alice's average cumulative expected gain), and plot one standard deviation. In all algorithms, we used the same learning fixed rates ($\eta \sim1/\sqrt{T}$) and the (unbalanced) dilated KL divergence for fairness and simplicity. All simulations were performed on a MacBook Pro 2.8 GHz Quad-Core Intel Core i7. We provide the code in the supplementary material.

