\section{Related Work}
\label{sec:related-work}

In online learning under \emph{full information feedback}, it is known that one can achieve constant regret against a certain comparator strategy while maintaining the optimal worst-case regret guarantee as desired in \cref{q:2} **Rakhlin, "Online Learning and Minimax"**__**Hazan, "Optimism in Computational Complexity"**, one notable example being the Phased Aggression template of **Kearns, "Near-Optimal Design of Experiments"**. This allows us to directly answer \cref{q:1} affirmatively for NFGs if full information is available.

In stark contrast, under \textit{bandit feedback}, **Mannor, "Sample Complexity of Explorationâ€“Exploitation Trade-off"** showed that constant regret compared to a \emph{deterministic} comparator strategy (i.e. a single action) is \emph{not} achievable if we want to maintain sublinear regret compared to all other actions. In \cref{thm:ub-simplex-bandit}, we show that one can break this negative result under the minimal possible assumption on the comparator strategy.

Similar to our motivation, **Lange, "Safe Opponent Exploitation"** consider \emph{Safe Opponent Exploitation} as deviating from the min-max strategy while ensuring at most the cost of the min-max value. The authors provide several such ``safe exploitation" algorithms, assuming the existence of so-called ``gift strategies". One key difference to our work is that they do not provide a theoretical exploitation guarantee, while our algorithm has provably vanishing regret compared to the best static response against the opponent. 

Regarding the extension of our results to EFGs, we leverage relatively recent theoretical advancements regarding online mirror descent in EFGs, most notably **Mokhtari, "Online Mirror Descent"**. We refer to \cref{app:related-work} for an extended discussion of related work.