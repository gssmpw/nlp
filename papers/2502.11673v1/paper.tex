\documentclass[11pt]{article}
\usepackage[top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm]{geometry}
\input{header}

\title{\bf \Large Best of Both Worlds:\\Regret Minimization versus Minimax Play}


\author[*,1]{\normalsize Adrian Müller}
\author[*,2]{Jon Schneider}
\author[*,3]{Stratis Skoulakis}
\author[*,4]{Luca Viano}
\author[4]{Volkan Cevher}

\affil[1]{ETH Zürich}
\affil[2]{Google Research}
\affil[3]{Aarhus University}
\affil[4]{EPFL}

\date{}

\singlespacing

\begin{document}


\maketitle

\begin{abstract}
    \noindent In this paper, we investigate the existence of online learning algorithms with bandit feedback that simultaneously guarantee $O(1)$ regret compared to a given comparator strategy, and $O(\sqrt{T})$ regret compared to the best strategy in hindsight, where $T$ is the number of rounds. We provide the first affirmative answer to this question. In the context of symmetric zero-sum games, both in normal- and extensive form, we show that our results allow us to guarantee to risk at most $O(1)$ loss while being able to gain $\Omega(T)$ from exploitable opponents, thereby combining the benefits of both no-regret algorithms and minimax play.
\end{abstract}


{\def\thefootnote{*}\footnotetext{Equal contribution. Correspondence to \texttt{\href{mailto:admuell@ethz.ch}{admuell@ethz.ch}}.}} 

\setlength\parindent{0pt}

\section{Introduction} \label{sec:intro}

Two-player zero-sum games form one of the most fundamental classes studied in game theory, capturing direct competition between two opposing agents. In a zero-sum game, Alice and Bob choose mixed strategies $\xb \in \Xb$ and $\yb\in\Yb$, respectively, for some strategy polytope $\Xb$. Their expected payoffs are specified by a function $V$. Alice aims to minimize $V(\mu,\nu)$, whereas Bob aims to maximize it. This definition subsumes the classical \emph{normal-form zero-sum games} \citep[e.g., Rock-Paper-Scissors]{von2007theory} as well as the more complex $\textit{extensive-form zero-sum games}$ \citep[e.g., Heads-up Poker]{osborne1994course}. A zero-sum game is symmetric if interchanging the actions of the players interchanges payoffs, i.e. $V(\mu,\nu) = -V(\nu,\mu)$.

Now suppose Alice repeatedly plays a symmetric zero-sum game against Bob for $T$ consecutive rounds. In each round, she chooses her next strategy based on her previous observations, and Bob does likewise. There are two popular lines of thought on how Alice could minimize her overall cost over the $T$ rounds of play:

\textbf{1) Min-Max Equilibrium:} In every round $t$, Alice selects $\mu^t = \mu^\star \in \arg\min_{\mu 
\in \Xb}\max_{\nu \in \Yb} V(\mu,\nu)$ \citep{von2007theory}. She then loses at most $V^\star := \min_{\mu \in \Xb}\max_{\nu \in \Yb} V(\mu,\nu)$ money. For symmetric zero-sum games, we have $V^\star = 0$, meaning that:

\smallskip
\smallskip
\begin{center}
    \textit{Alice is guaranteed not to lose any money, but might not win money\footnote{E.g., she never wins money if $\mu^\star$ is full-support \citep{braggion2020strong}, and even otherwise may not win anything.} even if Bob plays poorly.}
\end{center}
\smallskip
\smallskip

\textbf{2) Regret Minimization:} Alice selects $\mu^t \in \Xb$ according to a \emph{no-regret algorithm} \cite{cesa2006prediction}. Then, no matter Bob's strategies
$\nu^1,\ldots,\nu^T \in \Yb$, the \textit{regret} compared to any fixed strategy $\mu$ satisfies
\begin{equation*}
    \sum_{t=1}^T V(\mu^t,\nu^t) - \sum_{t=1}^T V(\mu,\nu^t)\leq O(\sqrt{T}).
\end{equation*}
In symmetric zero-sum games, plugging in the equilibrium $\mu=\mu^\star$, we have $V(\mu,\nu^t)\leq V^\star = 0$. This means that Alice might lose up to $O(\sqrt{T})$ 
money\footnote{There are cases where she does since there is a matching regret lower bound.}. However, if Bob plays sub-optimally, it may be the case that $\min_{\mu \in \Xb} \sum_{t=1}^T V(\mu,\nu^t) = -\Theta(T)$, meaning that Alice wins $\Theta(T)$ money. As a result:

\smallskip
\smallskip
\begin{center}
    \textit{Alice risks losing $O(\sqrt{T})$ money, but can win up to $\Theta(T)$ money if Bob plays sub-optimally.} 
\end{center}
\smallskip
\smallskip

\noindent Whether Alice will choose to play 1) a min-max equilibrium or 2) according to a no-regret algorithm depends on how \emph{risk-averse} Alice is --- how willing Alice is to risk $O(\sqrt{T})$ money in the hope of winning $\Theta(T)$. This naturally raises the question of whether we can have the best of both worlds:
\begin{question}\label{q:1}
    In a symmetric zero-sum game, can Alice risk losing at most $O(1)$ amount of money, but still be able to win up to $\Theta(T)$ amount of money if Bob plays sub-optimally?
\end{question}

In this paper, we answer this question in the affirmative by resolving the following fairly \emph{more general question} from online learning with adversarial linear costs. We explain the reduction in \cref{sec:preliminaries}.
\begin{question}\label{q:2}
    Is it possible to guarantee $O(1)$ regret compared to a specific strategy while maintaining $\OTilde(\sqrt{T})$ regret compared to the best strategy in hindsight?
\end{question}
Question \ref{q:2} is known to admit a relatively simple positive answer in the so-called full-information case (\cref{sec:related-work}). Crucially, in this work we are interested in the \emph{bandit feedback} setting, modeling the fact that Alice only observes the realized cost and not the cost for all actions she could have taken instead. We formalize this learning goal in \cref{sec:reduction,sec:efg-reduction}.

We present our results in the context of symmetric zero-sum games. However, they hold far beyond symmetric, zero-sum, or even two-player games (\cref{q:2}): for any (sufficiently explorative) comparator strategy, one can guarantee constant regret compared to it while still having rate-optimal regret compared to the best strategy in hindsight, even under bandit feedback.

\textbf{Contributions.} Our main contributions are the following:
\begin{itemize}[leftmargin=*] 
    \item We first devise an algorithm for normal-form games (NFGs) under bandit feedback that interpolates between playing the min-max equilibrium and no-regret learning. We prove that if the min-max equilibrium is supported on the whole action space\footnote{This assumption is also necessary, but can easily be relaxed, at the cost of slightly weaker guarantees on when Alice can take advantage of sub-optimal play by Bob. See \cref{rmk:restrict}.}, then our algorithm indeed satisfies the desiderata of our main question (\cref{sec:nfg-upper}). To the best of our knowledge, this is the first result of its kind under bandit feedback.
    \item We complement this regret guarantee with a lower bound for NFGs, showing that the regret bound cannot be improved significantly (\cref{sec:nfg-lower}). This implies that our algorithm is close to optimally exploiting weak strategies, as desired.
    \item We then transfer our insights to the more challenging framework of extensive-form games (EFGs). This is specifically relevant since in stateful games, it is essential to consider bandit feedback. By proposing a corresponding algorithm for EFGs, we show that even in such interactive games with imperfect information, we can answer our main question in the affirmative (\cref{sec:efg-upper}). We generalize our lower bound to this setting, too (\cref{sec:efg-lower}). 
\end{itemize}

Finally, we numerically evaluate our algorithm in simple EFG environments (\cref{sec:experiments}), showing that our results are not merely of theoretical interest. Indeed, our findings confirm our theoretical insights and demonstrate strong results even when the min-max equilibrium is not full-support.

\subsection{Related Work} \label{sec:related-work}

In online learning under \emph{full information feedback}, it is known that one can achieve constant regret against a certain comparator strategy while maintaining the optimal worst-case regret guarantee as desired in \cref{q:2} \citep{hutter2005adaptive,even2008regret,kapralov2011prediction,koolen2013pareto,sani2014exploiting,orabona2016coin,cutkosky2018black,orabona2019modern}, one notable example being the Phased Aggression template of \citet{even2008regret}. This allows us to directly answer \cref{q:1} affirmatively for NFGs if full information is available.

In stark contrast, under \textit{bandit feedback}, \citet{lattimore2015pareto} showed that constant regret compared to a \emph{deterministic} comparator strategy (i.e. a single action) is \emph{not} achievable if we want to maintain sublinear regret compared to all other actions. In \cref{thm:ub-simplex-bandit}, we show that one can break this negative result under the minimal possible assumption on the comparator strategy.

Similar to our motivation, \citet{ganzfried2015safe} consider \emph{Safe Opponent Exploitation} as deviating from the min-max strategy while ensuring at most the cost of the min-max value. The authors provide several such ``safe exploitation" algorithms, assuming the existence of so-called ``gift strategies". One key difference to our work is that they do not provide a theoretical exploitation guarantee, while our algorithm has provably vanishing regret compared to the best static response against the opponent. 

Regarding the extension of our results to EFGs, we leverage relatively recent theoretical advancements regarding online mirror descent in EFGs, most notably \citet{kozuno2021model,bai2022near}. We refer to \cref{app:related-work} for an extended discussion of related work. 

\section{Preliminaries} \label{sec:preliminaries}

In this section, we introduce the relevant notation and explain how Question \ref{q:2} answers Question \ref{q:1}. 

\textbf{General Notation.} As usual, $O$-notation expresses asymptotic behavior, and $\tilde{O}$-notation hides poly-logarithmic factors. The $n$-dimensional simplex is $\Delta_n$, and we let $[n]:=\{1,\dots,n\}$. Moreover, $e_i$ is the $i$-th the standard basis vector of $\R^n$, and $\inner{\cdot}{\cdot}$ the Euclidean inner product. Finally, $\mathbbm{1}_E$ denotes the indicator function of an event $E$.

\textbf{(Safe) Online Linear Minimization.} In Protocol \ref{prot:olm}, we introduce the framework of \emph{online linear minimization} \citep[OLM]{H17} with adversarial costs. In addition to this standard framework, Alice receives a special (``safe") \emph{comparator strategy} $\mu^c\in\Xb$ as input, compared to which Alice would like to be essentially at least as good.
\begin{figure}[H]
\centering
\begin{minipage}{1.0\textwidth}
\begin{protocol}[H]
    \normalsize
    \caption{(Safe) Online Linear Minimization} 
    \label{prot:olm}
    \centering
    \begin{algorithmic}
        \Require{Special comparator $\mu^c\in\Xb$.}
        \For{round $t = 1,\dots,T$}
            \State \textbf{Alice} chooses her next $\mu^t \in \Xb$, \textbf{Bob} chooses the cost vector $c_t$.
            \State \textbf{Alice} suffers expected cost $\inner{\mu^t}{c^t}$.
        \EndFor
        \hspace{-0.33cm}\textbf{Goal:} $\mathcal{R}(\mu^c) \leq O(1)$ and $\max_{\mu\in\Xb}\mathcal{R}(\mu)\leq \OTilde(\sqrt{T}).$
    \end{algorithmic}
\end{protocol}
\end{minipage}
\end{figure}

We define Alice's \emph{expected regret} compared to a strategy $\mu\in\Xb$ by
\begin{align*}
    \mathcal{R}(\mu) := \sum_{t=1}^T \exptn\rectangular{\inner{\mu^t-\mu}{c^t}}.
\end{align*}
The worst-case expected regret $\max_{\mu} \mathcal{R}(\mu)=\sum_{t=1}^T \exptn\rectangular{\inner{\mu^t}{c^t}}-\min_{\mu}\sum_{t=1}^T \exptn\rectangular{\inner{\mu}{c^t}}$ then measures the regret compared to the best fixed strategy $\mu$ in hindsight. Under \emph{safe OLM} (Question \ref{q:2}), we understand the problem of simultaneously guaranteeing 
\begin{align}
    \mathcal{R}(\mu^c) \leq O(1), \quad\text{and}\quad \max_{\mu\in\Xb}\mathcal{R}(\mu)\leq \OTilde(\sqrt{T}). \tag{OLM}\label{eq:safe-olm}
\end{align}
\textbf{Question \ref{q:2} Answers Question \ref{q:1}.} Now suppose Alice was able to guarantee (\ref{eq:safe-olm}). As we explain in \cref{sec:reduction,sec:efg-reduction}, both for NFGs and EFGs, we can write the expected cost in round $t$ as a linear function of the strategy, i.e.
\begin{align*}
    V(\mu,\nu^t)=\inner{\mu}{c^t}
\end{align*}
for some cost vector $c^t$. Alice can now set $\mu^c=\mu^\star=\arg\min_{\mu}\max_{\nu} V(\mu,\nu)$ to be a min-max equilibrium. Since $V(\mu^c,\nu)\leq V^\star=0$ for symmetric zero-sum games, the first part of (\ref{eq:safe-olm}) implies
\begin{align*}
    \sum_{t=1}^T \exptn\rectangular{V(\mu^t,\nu^t)} \leq \sum_{t=1}^T \exptn\rectangular{V(\mu^c,\nu^t)} + O(1) \leq O(1),
\end{align*}
no matter Bob's play. Alice will thus lose at most a constant amount in expectation. Furthermore, if (for example) Bob plays a fixed strategy $\nu^t=\nu\in\Xb$ that is suboptimal in the sense that $\min_{\mu}V(\mu,\nu) = -c < 0$,\footnote{More generally, by \emph{exploitable} we mean that Bob plays an oblivious sequence of strategies $\nu^t$ with $\min_{\mu} \sum_t V(\mu,\nu^t) = -\Theta(T)$. We briefly discuss the adaptive case in \cref{app:related-work}.} then the second part in (\ref{eq:safe-olm}) shows
\begin{align*}
    \sum_{t=1}^T \exptn\rectangular{V(\mu^t,\nu^t)} \leq \min_{\mu} \sum_{t=1}^T V(\mu,\nu) + \OTilde(\sqrt{T}) \leq -\Theta(T),
\end{align*}
and Alice will linearly exploit Bob. We will thus state our results in terms of safe OLM, keeping in mind that the above reduction will automatically answer our initial \cref{q:1}.

\section{Normal-Form Games} \label{sec:simplex}

Suppose Alice and Bob repeatedly play a (symmetric zero-sum) \emph{normal-form game} for $T$ rounds, which means the following. In each round $t$, they simultaneously submit actions $a^t,~b^t\in[A]$ by sampling from mixed strategies $\mu^t,~\nu^t\in\Delta_A$, respectively. Alice receives cost $U_{a^t,b^t}=\inner{e_{a^t}}{U e_{b^t}}$, for some fixed cost matrix $U\in\R^{A\times A}$ with entries in $[0,1]$. We consider \emph{bandit feedback}, meaning that Alice only observes her cost $U_{a_t,b_t}$ and not the cost of actions she could have taken instead. 

\subsection{From NFGs to Online Linear Minimization} \label{sec:reduction}
By defining Alice's \emph{cost function} as
\begin{align*}
    c^t := U e_{b^t}\in\R^A,    
\end{align*}
we see that Alice's expected cost is $V(\mu^t,\nu^t):=\exptn\rectangular{U_{a_t,b_t}}=\exptn\rectangular{\inner{\mu}{c^t}}$, as $a^t\sim \mu^t$. We are thus in the setting of OLM (Protocol \ref{prot:olm}) over $\Xb=\Delta_A$. Notably, Alice does not observe the full cost function $c^t$ but only its entry $c^t(a^t)=U_{a^t,b^t}$ at the chosen action (bandit feedback). We formally consider Protocol \ref{prot:bandit-simplex} for \emph{any} adversarially picked cost functions $c^t$. From \cref{sec:preliminaries} we know that it is now sufficient to set $\mu^c=\mu^\star$ and guarantee (\ref{eq:safe-olm}).
\begin{figure}[H]
\centering
\begin{minipage}{1.0\textwidth}
\begin{protocol}[H]
    \caption{Bandit Feedback over the Simplex (NFGs)} 
    \label{prot:bandit-simplex}
    \centering
    \begin{algorithmic}
        \Require{Special comparator $\mu^c\in\Delta_A$.}
        \For{round $t = 1,\dots, T$}
            \State \textbf{Alice} chooses her next action $a^t \sim \mu^t \in \Delta_A$, \textbf{Bob} chooses the cost $c^t \in \R^A$. \Comment{\color{blue} NFG: $c^t=U e_{b^t}$} 
            \State \textbf{Alice} suffers and observes cost $c^t(a^t)$. \Comment{\color{blue} NFG: $U_{a^t,b^t}$}	
        \EndFor
    \end{algorithmic}
\end{protocol}
\end{minipage}
\end{figure}

\subsection{Upper Bound} \label{sec:nfg-upper}

Our first main result shows that if the special comparator strategy lies in the interior of the simplex, we are able to guarantee constant regret while maintaining low worst-case regret at the optimal rate. 
\begin{restatable}{theorem}{thmUbSimplexBandit}\label{thm:ub-simplex-bandit}
    Let $\delta\in(0,1/A]$. Consider any mixed strategy $\mu^c\in \Delta_A$ such that $\mu^c(a)\geq \delta$ for all $a\in[A]$. Under bandit feedback (Protocol \ref{prot:bandit-simplex}), for any $c^t\in[0,1]^A$, \cref{algo:phased-aggression-nfg} achieves 
    \begin{align*}
        \mathcal{R}(\mu^c) \leq 1, \quad \text{and} \quad \max_{\mu\in\Delta_A} \mathcal{R}(\mu) \leq \OTilde\round{\delta^{-1}\sqrt{T}}.
    \end{align*} 
\end{restatable}
For the case of NFGs, this means that if the min-max strategy $\mu^\star$ is full-support, then in expectation: Alice will lose at most $O(1)$ money while winning $\Omega(T)$ money if Bob plays (oblivious) strategies that are linearly exploitable.

\citet{lattimore2015pareto}'s result implies that the assumption on $\mu^c$ is also necessary. In addition, we show in \cref{thm:lower-nfg} that a multiplicative dependence on $\delta^{-1}$ is unavoidable. We remark that min-max strategies $\mu^\star$ of various zero-sum games are $\delta$-bounded away from zero. For example in Rock-Paper-Scissors $\mu^\star= (1/3,1/3,1/3)$. More importantly, even when this is not the case, we remark the following.
\begin{remark} \label{rmk:restrict}
    Alice can apply the result even in zero-sum games with min-max strategies $\mu^\star \in \Delta_A$ that are not full-support. Indeed, she can consider the subset of actions $\Acal':=\{a \in [A] \colon \mu^\star(a) > 0\}$. Then, our algorithm run on $\Acal'$ still guarantees $\mathcal{R}(\mu^\star)\leq O(1)$, meaning that Alice can lose at most $O(1)$ in symmetric zero-sum games. At the same time, our algorithm guarantees that $\sum_{t=1}^T \exptn\rectangular{V(\mu^t,\nu^t)} \leq \min_{\mu\in\Delta'} \sum_{t=1}^T \exptn\rectangular{V(\mu,\nu^t)} + \OTilde(\sqrt{T})$, where $\Delta'$ is the simplex restricted to $\Acal'$. This means that if Bob plays suboptimally, Alice can still guarantee to win $\Theta(T)$ whenever these actions allow her to do so (while $\mu^\star$ itself does not guarantee this).
\end{remark}


\begin{figure*}[t]
\centering
\begin{minipage}{1.0\textwidth}
\begin{algorithm}[H]
    \caption{Phased Aggression with Importance-Weighting} \label{algo:phased-aggression-nfg}
    \begin{algorithmic}[1]
        \Require{Number of rounds $T$, comparator margin $\delta$, regret upper bound $R\gets\delta^{-1} \sqrt{2T\log(A)}$, OMD learning rates $\eta \gets \sqrt{\delta^2\log(A)/(2T)}$, $\tau\gets \sqrt{2\log(A)/(AT)}$.}
        \vspace{1em}
        \State Initialize $\muHat^{1}(a) \gets \frac{1}{A}$ for all $a\in [A]$, initialize $\alpha \gets 1/R$, $\text{start}\gets 1$, $k \gets 1$ (counts phase).
        \For{round $t =1, \ldots, T$}
            
            \State \textbf{Alice} chooses $\mu^t \in \Delta_{A}$, \textbf{Bob} selects cost $c^t$. \Comment{{\color{blue}in NFGs: $c^t=Ue_{b^t}$}}
            
            \State \textbf{Alice} suffers and observes cost $c^t(a^t)$ for $a^t\sim \mu^t$.\label{line:sample-nfg}
            \State \textbf{Alice} builds cost estimator $\cHat^t(a) \gets \frac{c^t(a^t)}{\mu^t(a)}\indicator{a^t=a}$. \label{line:loss-nfg}
            
            \If {$\max_{\mu\in\Delta_{A}}\sum^t_{j = \text{start}} \inner{\cHat^j}{\mu^c - \mu} > 2 R$ \textbf{ and } $\alpha < 1$} \label{line:new-phase-nfg}
                \State $k \gets k + 1$, $\text{start} \gets t+1$. \Comment{If comparator performs poorly, new phase}\label{line:new-k}
                \State $\muHat^{t+1}(a) \gets \frac{1}{A}$ for all $a\in [A]$.
                \Comment{Re-initialize OMD}\label{line:reset-omd-nfg}
                \State Update $\alpha \gets \min\curly{2^{k-1}/R,~ 1}$. \Comment{Increase $\alpha$ for upcoming phase} \label{line:new-phase2-nfg}
        
        \Else \Comment{OMD update}
            \State $\muHat_x^{t+1} \gets \arg\min_{\mu \in \Delta_A} \round{\eta' \inner{\mu}{\cHat^t} + \Dkl(\mu || \muHat^t)}$, with $\eta' = \eta$ if $\alpha < 1$, and $\eta'=\tau$ if $\alpha = 1$. \label{line:omds-nfg}
            
        \EndIf   
            \State $\mu^{t+1} \gets \alpha \muHat^{t+1} + \left( 1 - \alpha \right) \mu^c$. \Comment{Play shifted OMD to $\mu^c$ by $1-\alpha$} \label{line:combine-bandit-nfg}
        \EndFor
    \end{algorithmic}
\end{algorithm}
\end{minipage}
\end{figure*}

\textbf{Our Algorithm.} In this section, we present \cref{algo:phased-aggression-nfg} and explain its key steps. Our algorithm is inspired by the \emph{Phased Aggression} algorithm, originally proposed by \citet{even2008regret} for the \textit{full-information setting}. We briefly note that a direct application of existing full-information algorithms is not possible. This is because, in the bandit setting, Alice only observes her realized cost and not the cost of the other possible actions she could have chosen. We will thus combine the phasing idea of \citet{even2008regret} with appropriately importance-weighted estimators of the full cost function.\footnote{The same adaptation would not yield our result for full-information algorithms other than Phased Aggression.} 

We now give an outline of \cref{algo:phased-aggression-nfg}. In every round $t$, the Phased Aggression algorithm plays a convex combination between the comparator strategy $\mu^c$ and the strategy $\muHat^t$ chosen by a no-regret algorithm (which runs in parallel). That is, the played strategy is $\mu^t=\alpha\muHat^t+(1-\alpha)\mu^c$ for some $\alpha \in (0,1]$. Whenever the algorithm estimates that the comparator $\mu^c$ is a poor choice, it increases $\alpha$ by a factor of two (so that it puts less weight on $\mu^c$ and more on the no-regret iterates) and restarts the no-regret algorithm. We group all rounds according to these restarts and call them \emph{phases} $k=1,2,\dots$. During each phase, $\alpha$ is constant.

Within this phasing scheme, the specifics of our algorithm are as follows. The no-regret algorithm of our choice is standard online mirror descent\footnote{As regularizer, we use the standard KL divergence $\Dkl(\mu||\mu'):=\sum_a \mu(a)\log(\mu(a)/\mu'(a))$.} \citep[OMD]{H17}. In every round $t$, the algorithm plays its current action $a^t \sim \mu^t$ and observes its cost (Line \ref{line:sample-nfg}). It uses this to construct an importance-weighted estimator $\cHat^t$ of the (unobserved) full cost function (Line \ref{line:loss-nfg}). The algorithm then performs one iteration of OMD with the estimated costs (Line \ref{line:omds-nfg}). This procedure is repeated until a new phase is started (Line \ref{line:new-phase-nfg}), which happens if the comparator $\mu^c$ is performing poorly under the estimated $\cHat^t$'s of the current phase.

Regarding computation, the OMD update can be implemented in closed form as $\muHat^{t+1}(a)\propto \muHat^{t}(a)\exp(-\eta' \cHat^t(a))$. We can check the if-condition in Line \ref{line:new-phase-nfg} by directly computing the maximum in $O(A)$ time.

\textbf{Regret Analysis.} In this section we
provide a proof sketch of Theorem \ref{algo:phased-aggression-nfg}. We defer the full proof to \cref{app:nfg-upper}.

We first introduce some notation. We index the variables by their respective phase $k\geq 1$: Phase $k$ lasts from $\start_k$ to $\start_{k+1}-1$ and uses linear combinations with $\alpha^k = \min\{1,2^{k-1}/R\}$ (Lines \ref{line:new-k}, \ref{line:new-phase2-nfg}). By design, there are at most $1+\lceil\log_2(R)\rceil$ phases, where $R$ is a known regret upper bound for OMD input to the algorithm. The overall regret is at most the sum of regrets across all phases, and we will thus analyze each phase separately. To this end, let 
\begin{align*}
    \RHat^k(\mu):= \sum_{t=\start_k}^{\start_{k+1}-1} \inner{\cHat^t}{\mu^t-\mu} 
\end{align*}
denote the \emph{estimated regret} during phase $k$. The following lemma bounds this estimated regret for phases with $\alpha^k<1$. 
\begin{restatable}[During normal phases]{lemma}{lemmaDuringNfg} \label{lemma:during-nfg}
    Let $k$ be such that $\alpha^k<1$. Then for all $\mu\in\Delta_A$,
    \begin{align*}
        \RHat^k(\mu) \leq 2R+2= 2\delta^{-1} \sqrt{2T\log(A)}+2,
    \end{align*}
    and for the special comparator $\RHat^k(\mu^c) \leq 2^{k-1}$.
\end{restatable}
\noindent The first part of the theorem establishes a worst-case bound on the estimated regret. Such a bound would normally not be possible for importance-weighted cost estimators. In our case, during phases with $\alpha^k<1$, we put constant weight on the comparator strategy $\mu^c$, which in turn is lower bounded by $\delta>0$. Our estimated costs (Line \ref{line:loss-nfg}) will thus be upper bounded, which is a key step in the proof. The second part of the theorem easily follows using the definition of $\alpha^k$.

\noindent Next, suppose the algorithm exits a phase $k$ as the if-condition in Line \ref{line:new-phase} holds. The following lemma establishes that exiting the phase is justified in the sense that we perform sufficiently well compared to the special comparator, according to the estimated costs. 
\begin{restatable}[Exiting a phase]{lemma}{lemmaExitNfg} \label{lemma:exit-nfg}
    Let $k$ be such that $\alpha^k<1$. If \cref{algo:phased-aggression-nfg} exits phase $k$, then $\RHat^k(\mu^c)\leq -2^{k-1}$.
\end{restatable}
We are now ready to prove \cref{thm:ub-simplex-bandit}. First, consider the case that $\alpha=1$ is never reached. 
Note that our cost estimates are unbiased, i.e. $\exptn\rectangular{\cHat^t(a)} = c^t(a)$. It is thus sufficient if we can bound $\RHat^k$. As there are $O(\log R)$ phases, \cref{lemma:during-nfg} implies $\max_{\mu} \mathcal{R}(\mu) \leq O(R\log R)$. Moreover, the previous two lemmas geometrically balance the regret compared to $\mu^c$ to be at most $1$, and we conclude. Second, suppose now that $\alpha=1$ is reached. The final phase will then simply be OMD with standard importance-weighting (a.k.a. Exp3), as we put no weight on the special comparator $\mu^c$. While we cannot apply \cref{lemma:during-nfg}, we can directly bound the remaining \emph{expected} regret of Exp3 \citep{orabona2019modern}. We can thus use the same argument as before, with one additional phase.

\subsection{Lower Bound} \label{sec:nfg-lower}

We will now show that regarding the guarantee we provided in \cref{thm:ub-simplex-bandit}, a multiplicative dependence on the inverse of the ``exploration gap" $\delta$ is indeed unavoidable.\footnote{In fact, if the cost functions are stochastic rather than adversarial, we can match this lower bound, see \cref{app:nfg-stochastic}.}
\begin{restatable}{theorem}{thmLowerNfg} \label{thm:lower-nfg}
    Let $\delta\in(0,1/A]$. There is a comparator $\mu^c\in\Delta_A$ with all $\mu^c(a) \geq \delta$ such that for any algorithm for Protocol \ref{prot:bandit-simplex} there is a sequence $c^1,\dots,c^T\in[0,1]^A$ such that: If $\mathcal{R}(\mu^c) \leq O(1)$, then  
    \begin{align*}
        \max_{\mu\in\Delta_A} \mathcal{R}(\mu) \geq \Omega(\sqrt{\delta^{-1}T}-\delta^{-3/4}T^{1/4}).
    \end{align*}
\end{restatable}
In the context of symmetric zero-sum games, our lower bound establishes that in expectation: If Alice is willing to lose at most $O(1)$ money, the best she can hope to gain from any (fixed) exploitable strategy is $\Theta(T) - \Omega(\delta^{-1/2}\sqrt{T})$.

The key idea of our proof is that any algorithm with low regret compared to $\mu^c=(1-\delta,\delta)$ for $A=2$ actions will need to play action 1 most of the time if one can information-theoretically not detect that action 2 is, in fact, minimally better. We defer the proof to \cref{app:nfg-lower}.


\section{Extensive-Form Games} \label{sec:efg}

In this section, we present our results for EFGs. We start by giving the definition of EFGs that appears in \citet{kozuno2021model,bai2022near,fiegel2023adapting,fiegel2023local}, see \cref{app:efg-background} for a brief discussion. For clarity, we present the \textit{two-player, symmetric zero-sum case}, although our results readily generalize to arbitrary EFGs.

\begin{definition}\label{d:efg}
    An \emph{EFG} is a tuple $(H,\Scal,\Xcal,\Acal,P,x,y,u)$, where 
    \begin{itemize}[leftmargin=*]
        \item there are $2$ players, Alice and Bob. $\Acal=[A]$ denotes the set of possible actions for both players.
        \item $\Scal$ denotes the set of states of the game. $H \in \mathbb{N}$ is the horizon of the game. At stage $h \in [H]$, $\Scal_h \subseteq \Scal$ denotes the possible states. 
        \item $P:=(p_0,p)$ is the transition kernel; the game's state is sampled according to $s_{h+1} \sim p(\cdot | s_h, a_h , b_{h})$ upon actions $a_h, b_h \in \Acal \times \Acal$ in state $s_h \in \Scal$. The initial state is sampled according to $s_1\sim p_0\in\Simplex{\Scal}$.
        \item $u(s,a,b) \in[-1,1]$ is Alice's random \emph{cost} (Bob's reward) for actions $(a,b) \in \Acal\times\Acal$ chosen in state $s \in \Scal$, with mean $\bar{u}(s,a,b)$.
        \item Alice (resp. Bob) observes information sets (infosets) from $\Xcal$ ($|\Xcal|=X$). Infosets are described by a surjective function $x \colon \Scal \to \Xcal$ (resp. $y\colon \Scal \to \Xcal$).
    \end{itemize}
\end{definition}

\noindent The idea behind infosets is that Alice (resp. Bob) has imperfect information about the state of the game: she cannot differentiate between sates $s,s' \in \Scal$ that belong at the same infoset, i.e. when $x(s) = x(s')$. This is reflected in the definition of her policy set. 

\begin{definition}\label{def:strategy}
    A \emph{policy} is a mapping $\pi \colon \Xcal \to \Delta_A$. We denote the set of all such policies by $\Pi$. 
\end{definition}
We let $\pi(a|x)$ denote the probability of playing action $a \in \mathcal{A}$ in states $s \in \Scal$ from infoset $x=x(s) \in \mathcal{X}$. As Alice cannot differentiate between states $s,s' \in \mathcal{S}$ from the same infoset, she must act the same way if $x(s) = x(s')$.
\begin{definition}
    Given policies $(\pi_A,\pi_B) \in \Pi \times \Pi$, the \emph{expected total cost} for Alice equals 
    \begin{align*}
        V(\pi_A,\pi_B) := \exptn\rectangular{\sum_{h=1}^H u(s_h,a_h,b_h)},
    \end{align*}
    where $(s_h,a_h,b_h)$ are the state and actions at stage $h \in [H]$ via $a_h\sim\pi_A(\cdot|x(s_h))$, $b_h\sim\pi_B(\cdot|y(s_h))$, and $s_{h+1} \sim p(\cdot | s_h,a_h,b_h)$.
\end{definition}
For the remainder of the section, we make the following assumptions, which are standard in the EFG literature \citep{kozuno2021model,bai2022near,fiegel2023adapting,fiegel2023local}.
\begin{assumption}\label{a:1}
    \begin{itemize}[leftmargin=*]
        \item \textbf{Tree structure:} For any state $s_h \in \Scal_h$, there exists a unique sequence $(s_1,a_1,b_1, \dots,\allowbreak s_{h-1},a_{h-1},b_{h-1})$ leading to $s_h$.
        \item \textbf{Perfect recall:} Let $s,s'$ such that $x(s) = x(s')$. Then:
        \begin{itemize}[leftmargin=*]
            \item There exists $h \in [H]$ such that $s,s' \in \Scal_h$.
            \item Let $(s_1,a_1,\dots,s_{h-1},a_{h-1})$ be the unique path leading to $s$ and $(s'_1,a'_1,\dots,s'_{h-1},a'_{h-1})$ the unique path leading to $s'$. Then for all $k \in [h-1] \colon x(s_k) = x(s'_k)$ and $a_k = a'_k$.
        \end{itemize}
    \end{itemize}
\end{assumption}
\textit{Tree structure} states that the game proceeds in rounds during which the players cannot loop back to a previous state. \textit{Perfect recall} establishes that the players never forget the history of play. They can only consider two states as the same infoset if the observations so far have been the same \cite{HGPS10}. The latter implies that infosets are partitioned along the horizon, i.e. $\mathcal{X} = \bigcupdot_{h\in [H]}\mathcal{X}_h$, and the same holds for the states.

\textbf{Online Learning in EFGs.} Now suppose Alice and Bob repeatedly play an EFG for $T$ consecutive rounds. In each round $t \in [T]$, Alice and Bob select a pair of policies $(\pi_A^t,\pi_B^t) \in \Pi\times\Pi$. Then a trajectory $(s_1^t,a_1^t,b_1^t,u_1^t,\dots,s_H^t, a_H^t, b_H^t, u_H^t)$ is sampled according to the policies $(\pi_A^t,\pi_B^t)$ and Alice suffers cost $\sum_{h=1}^H u_h^t$, as summarized in Protocol \ref{prot:EFGs-bandit}. 

\begin{figure}[H]
\centering
\begin{minipage}{1.0\textwidth}
\begin{protocol}[H]
    \caption{Bandit Feedback over Policies (EFGs)}
    \label{prot:EFGs-bandit}
    \centering
    \begin{algorithmic}
        \Require{A comparator policy $\pi^c\in\Pi$}.
        \For{round $t=1, \dots, T$}
            \State \textbf{Alice} selects $\pi_A^{t}\in\Pi$, \textbf{Bob} selects $\pi_{B}^{t}\in\Pi$.
            \State \textbf{Alice} obtains costs $\sum_{h=1}^H u_h^t$ and observes trajectory $(x_1^t,a_1^t,u_1^t,\dots,x_H^t, a_H^t, u_H^t)$.
        \EndFor
    \end{algorithmic}
\end{protocol}
\end{minipage}
\end{figure}

We remark in EFGs, we are naturally in the \textit{bandit feedback} setting as Alice only observes the trajectory $(x_1^t,a_1^t,u_1^t,\dots,x_H^t, a_H^t, u_H^t)$. Under \textit{full-information feedback}, Alice would observe Bob's actual policy $\pi_B^t \in \Pi$. 
\begin{remark}[Importance of bandit feedback in EFGs]
    In EFGs, bandit feedback is considerably more natural than full-information feedback. This is due to the fact that when playing against Bob, the realized samples are only observed along \emph{one single trajectory} in the game tree. Observing full information would thus mean knowing Bob's counterfactual policy in states that have never been visited during play, which is not realistic.
\end{remark}

\subsection{From EFGs to Online Linear Minimization} \label{sec:efg-reduction}

As mentioned, we once more resort to the more general OLM problem. Yet this time, our strategy polytope will be the so-called treeplex $\mathcal{P}=\Tcal$ rather than the simplex. The following definition provides an equivalent characterization of a policy. It will allow us to view the expected cost $V(\pi^t_A,\pi^t_B)$ as a (bi-)linear function \citep{HGPS10}.
\begin{definition}\label{d:treeplex}
    A vector $\mu \in \mathbb{R}^{X\cdot A}$
    belongs to the \emph{treeplex} $\Tcal$ iff for all $x_h\in \Xcal_h$ and $a\in\Acal$,
    \begin{align}
        \begin{cases}
            \mu(x_h,a) \geq 0, \label{eq:pseudo}\\
            \sum_{a_h\in\Acal} \mu(x_h,a_h) = \mu(x_{h-1},a_{h-1}) ,
        \end{cases}
    \end{align}
    where $(x_{h-1},a_{h-1})$ is the unique predecessor pair reaching $x_h$. We consider $\mu(x_0,a_0)=1$ for the root by convention.
\end{definition}

\begin{remark}
    There is the following equivalence between \cref{def:strategy,d:treeplex}. Given a policy $\pi\in\Pi$, we can define a unique $\mu_{\pi} \in \Tcal$ by $\mu_{\pi}(x_h,a_h) = \prod_{h'=1}^h \pi(a_{h'}|x_{h'})$, where the $(x_{h'},a_{h'})$ form the unique path to $(x_h,a_h)$. Vice-versa, given $\mu \in \Tcal$, we can recover the corresponding policy via $\pi_{\mu}(a|x)=\mu(x,a)/\sum_{a'}\mu(x,a')$.    
\end{remark}
By convention, we thus identify policies $(\pi_A^t,\pi_B^t)$ with their corresponding treeplex strategies $(\mu^t,\nu^t)$ and write $V(\mu^t,\nu^t)$ for Alice's expected cost. The following lemma shows that this definition indeed allows us to view Protocol \ref{prot:EFGs-bandit} as a (safe) OLM problem (Protocol \ref{prot:olm}).
\begin{lemma}
    For any state $s\in\Scal_h$, infoset $x=x(s) \in \Xcal_h$ and action $a \in \Acal$, let $(s_{1},a_{1},b_{1}, \dots ,s_{h-1},a_{h-1},b_{h-1})$ be the unique path leading to $s$. 
    Let $p(s):=p_0(s_1)\prod_{1\leq h'\leq h-1} p(s_{h'+1} | s_{h'},a_{h'},b_{h'})$, and consider
    \begin{align}
        c^t(x,a) := \sum_{\substack{s\colon x(s)=x,\\ b\in\Acal}} p(s)\cdot \nu^t(y(s),b) \cdot \bar{u}(s,a,b). \label{eq:def-loss}
    \end{align}
    Then $V(\mu,\nu^t) = \inner{\mu}{c^t}$ for all $\mu\in\Tcal$.
\end{lemma}
Alice does not observe the full cost function $c^t$, as we are in the bandit feedback setting. Yet, this lemma establishes that Protocol \ref{prot:olm} over the treeplex $\Xb = \Tcal$ covers EFGs. Thus, it is sufficient to solve the safe OLM problem (\ref{eq:safe-olm}).

\subsection{Upper Bound} \label{sec:efg-upper}

As in the simplex case, our Algorithm \ref{algo:phased-aggression-efg-bandit} guarantees \cref{eq:safe-olm} for any policy $\mu^c\in \Tcal$ that is $\delta$-bounded away from the boundary of the strategy polytope. Once more, we can resort to a restricted action set to relax this assumption (\cref{rmk:restrict}).

\begin{restatable}{theorem}{thmUbInteriorEfg} \label{thm:ub-interior-efg}
    Let $\delta \in (0,1/A]$. For any special comparator $\mu^c\in\Tcal$ such that $\mu^c(x,a) \geq \delta$ for all $x$, $a$, \cref{algo:phased-aggression-efg-bandit} achieves (for any $c^t$'s from \cref{eq:def-loss})
    \begin{align*}
        \mathcal{R}(\mu^c) \leq 1, \quad \text{and} \quad \max_{\mu\in\Tcal} \mathcal{R}(\mu) \leq \tilde{O}\round{\delta^{-1}\sqrt{XH^3T}}.
    \end{align*}
\end{restatable}

\begin{figure*}[ht]
\centering
\begin{minipage}{1.0\textwidth}
\begin{algorithm}[H]
    \caption{Phased Aggression with Importance-Weighting for EFGs} \label{algo:phased-aggression-efg-bandit}
    \begin{algorithmic}[1]
        \Require{Number of rounds $T$, comparator margin $\delta$, regret bound $R\gets\delta^{-1}\sqrt{8XH^3\log(A) T}$, learning rate $\eta \gets \sqrt{\delta^{2}X\log(A)/(8H^2T)}$, balanced learning rate $\tau \gets \sqrt{XA\log(A)/(H^3T)}$.}

        \vspace{1em}
        \State Initialize $\muHat^{1}(x_h,a) \gets \frac{1}{A^h}$ ($h\in[H]$, $x_h\in\Xcal_h$, $a\in \Acal$), initialize $\alpha \gets 1/R$, $\text{start}\gets 1$, $k \gets 1$ (counts phase).
        %\vspace{0.1cm}
        \For{round $t =1, \dots, T$}
            
            \State \textbf{Alice} chooses $\mu^t \in \Tcal$, \textbf{Bob} selects strategy $\nu^t\in\Tcal$. \Comment{{\color{blue}and thus cost $c^t$ via \cref{eq:def-loss}}}
            
            \State \textbf{Alice} obtains costs $\sum_{h=1}^H u^t_h$, observes $(x_1^t,a_1^t,u_1^t,\dots,x_H^t, a_H^t, u_H^t)$. \Comment{{\color{blue}$V(\mu^t,\nu^t)$ in exptn.}}\label{line:sample}
            
            \State \textbf{Alice} builds cost estimator $\cHat^t(x_{h},a) \gets \frac{\mathbbm{1}\{(x_{h}^t,a_{h}^t)=(x_{h},a)\}u^t_h}{\mu^t(x_{h},a)}$. \label{line:loss}
            
            \If {$\max_{\mu\in\Tcal}\sum^t_{j = \text{start}} \inner{\cHat^j}{\mu^c - \mu} > 2 R$ \textbf{ and } $\alpha < 1$} \label{line:new-phase}
            
                \State $\text{start} \gets t+1$, $k\gets k+1$. \Comment{If comparator performs poorly, next phase}
                \State $\muHat^{t+1}(x_h, a) \gets \frac{1}{A^h}$ ($h \in[H]$, $x_h\in \Xcal_h$, $a\in \Acal$).  
                \Comment{Initialize to uniform policy}\label{line:reset-omd}
                \State Update $\alpha \gets \min\curly{2^{k-1}/R,~ 1}$. \Comment{Increase $\alpha$ for upcoming phase} \label{line:new-phase2}
        %\vspace{0.3cm}
        \Else \Comment{OMD update}
            \State \label{line:omds}
            \begin{align}
                \hspace{-2.9cm}\muHat^{t+1} \gets \begin{cases}
                \arg\min_{\mu \in \Tcal} \round{\eta \inner{\mu}{\cHat^t} + D(\mu || \muHat^t)} \quad\quad &\text{(if $\alpha<1$)},\\
                \arg\min_{\mu \in \Tcal} \round{\tau \inner{\mu}{\cHat^t} + \Dbal(\mu || \muHat^t)} \quad\quad &\text{(if $\alpha=1$)}.
                \end{cases}\label{line:omd}
            \end{align}
        \EndIf
            \State $\mu^{t+1} \gets \alpha  \muHat^{t+1} + \left( 1 - \alpha \right) \mu^c$. \Comment{Play shifted OMD to $\mu^c$ by $1-\alpha$} \label{line:combine-bandit}
        \EndFor
    \end{algorithmic}
\end{algorithm}
\end{minipage}
\end{figure*}

\begin{remark}The dependence on $X$ is as good as desired in the sense that there is a $\sqrt{XAT}$ lower bound in the unconstrained case. The dependence on $H$ is less crucial for many relevant EFGs, as we often have $X\simeq A^H$ and so $H$ is a logarithmic factor. See \citet{bai2022near}.
\end{remark}

\textbf{Our Algorithm.} \cref{algo:phased-aggression-efg-bandit} is similar to our algorithm for the simplex. It combines the Phased Aggression scheme with importance-weighted OMD. However, in the EFG case, we have to generalize these notions to the treeplex. 

In particular, we use OMD with the so-called \emph{dilated} KL divergence as regularizer (Line \ref{line:omds}). As we will see in the regret analysis, to this end it is crucial that we use an \emph{unbalanced} dilated KL divergence $D$ \citep{kozuno2021model} in the phases with $\alpha<1$ and a \emph{balanced} KL divergence $\Dbal$ \citep{bai2022near} if $\alpha=1$ is reached. In \cref{app:efg-kl}, we formally define the divergences and confirm that they allow for an efficient closed-form implementation. This is crucial as we want to avoid costly projections onto the treeplex by any means. Moreover, we can efficiently check Line \ref{line:new-phase} via standard dynamic programming over the set of policies.

\textbf{Regret Analysis.} Our analysis follows a similar argument as in \cref{sec:simplex} and we defer the proofs to \cref{app:efg-upper}. The main technical challenge is to transfer the regret bounds for importance-weighted OMD from the simplex (with KL) to the treeplex $\Tcal$ (with dilated KL).

In addition, we now require a careful analysis to obtain the best possible dependence on the number of infosets $X$ and actions $A$, in the following sense. First, when upper bounding the estimated regret in analogy to \cref{lemma:during-nfg} ($\alpha<1$), we analyze OMD with the unbalanced dilated KL divergence by adapting the argument of \citet{kozuno2021model} to our importance-weighting. Using the (more sophisticated) balanced KL here would introduce an additional undesired factor of $\sqrt{A}$. Second, once $\alpha=1$ in the final phase, we analyze the expected regret of \emph{balanced} OMD instead, by adapting the argument of \citet{bai2022near} to our cost estimators. Using the unbalanced divergence would introduce an extra factor of $\sqrt{X}$, which can be prohibitively large.

\subsection{Lower Bound} \label{sec:efg-lower}

As in the case of NFGs, we show that our guarantees for \cref{algo:phased-aggression-efg-bandit} are close to being tight for EFGs of arbitrary depth. Our proof reduces an EFG of depth $H$ to the simplex case from \cref{thm:lower-nfg}. See \cref{app:efg-lower} for the proof.  
\begin{restatable}{theorem}{thmEfgLower}\label{thm:lb-efg-bandit}
    Let $A\geq 2$, $H \geq 1$, and $\delta \in (0,1)$. There exists an EFG of depth $H$ with $X=\Theta(A^H)$ such that for any $\mu^c\in\mathcal{T}$ with $\min_{x,a} \mu^c(x,a) = \delta$, there is an adversary such that for any algorithm: If $\mathcal{R}(\mu^c) \leq O(1)$, then
    \begin{align*}
         \max_{\mu\in\Tcal} \mathcal{R}(\mu) \geq \Omega(\sqrt{\delta^{-1}T} - \delta^{-3/4}T^{1/4}).
    \end{align*}
\end{restatable}

\section{Experimental Evaluations} \label{sec:experiments}
We experimentally compare our \cref{algo:phased-aggression-efg-bandit} for EFGs to the standard OMD algorithm with dilated KL \citep{kozuno2021model} as well as to minimax play. Our evaluations confirm our theoretical findings, revealing that \cref{algo:phased-aggression-efg-bandit} achieves the best of both no-regret and minimax play. We provide further details in \cref{app:further-experiments}.

\textbf{Kuhn Poker.} We consider \emph{Kuhn poker} \citep{kuhn1950simplified}, which serves as a simple yet fundamental example of two-player zero-sum imperfect information EFGs. Kuhn poker is a common $3$-card simplification of standard poker, where each player selects one card from the deck $\{$Jack, Queen, King$\}$ without replacement and initially bets one dollar.\footnote{\url{https://en.wikipedia.org/wiki/Kuhn_poker}} 
\begin{remark}
    The min-max equilibrium of Kuhn Poker is not full-support ($\delta =0$ in \cref{thm:ub-interior-efg}). As seen in \cref{rmk:restrict}, we can easily circumvent this issue by considering only the actions in the support of the equilibrium. For Kuhn Poker, this results in $\delta = 1/3$. \cref{algo:phased-aggression-efg-bandit} is then still guaranteed not to lose any money while being able to compete with the best response within the support of the equilibrium.
\end{remark}
We consider the following baseline algorithms Alice could play over $T$ rounds of Kuhn poker: 1) play the Min-Max equilibrium $\pi^\star$ in every round; or 2) run OMD with dilated KL; or 3) run \cref{algo:phased-aggression-efg-bandit} with comparator policy $\pi^\star$.

We consider two types of experiments: First, we run the three algorithms against each other to check which of the algorithms risks losing money. Second, we evaluate how well each algorithm allows Alice to exploit exploitable strategies. We repeat each experiment $5$ times.

\begin{figure*}[ht]
    \centering
    \begin{minipage}{1.0\textwidth}
    \begin{minipage}{0.05\textwidth}
        
    \end{minipage}
    \begin{minipage}{0.25\textwidth}
        \centering
        \captionsetup{labelformat=empty}
        \includegraphics[width=\linewidth]{plots/PA-Nash.png}
        \caption{Alg.~\ref{algo:phased-aggression-efg-bandit} vs Min-Max}
    \end{minipage}
    %\hfill
    \begin{minipage}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/Exp3-Nash.png}
        \caption*{{OMD} vs {Min-Max}}
    \end{minipage}
    %\hfill
    %\hspace{-0.5cm}
    \begin{minipage}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/PA-Exp3.png}
        \caption*{{Alg.~\ref{algo:phased-aggression-efg-bandit}} vs {OMD}}
    \end{minipage}
    %\hfill
    \begin{minipage}{0.05\textwidth}
        
    \end{minipage}
    \begin{minipage}{1.0\textwidth}
    \centering
    \caption{All vs all comparison for $T=1000$ rounds. The x-axis displays the round $t$, and the y-axis displays how much the respective algorithm ({\color{mblue}Min-Max}, {\color{mred}OMD}, {\color{mgreen}\cref{algo:phased-aggression-efg-bandit}}) \emph{gained} from the other.}\label{fig:all-vs-all}
    \end{minipage}
    \end{minipage}
\end{figure*}
\begin{figure*}[ht]
    \centering
    \begin{minipage}{1.0\textwidth}
    \begin{minipage}{0.05\textwidth}
        
    \end{minipage}
    \begin{minipage}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/All-vs-bluffJ.png}
        \caption*{All vs BluffJ}
    \end{minipage}
    %\hfill
    \begin{minipage}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/All-vs-KingQueen.png}
        \caption*{All vs RaiseKQ}
    \end{minipage}
    \begin{minipage}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/All-vs-Rand=0.2.png}
        \caption*{All vs RandMinMax}
    \end{minipage}
    \begin{minipage}{0.05\textwidth}
        
    \end{minipage}
    \begin{minipage}{1.0\textwidth}
    \centering
    \caption{All vs Bob comparison for $T=1000$ rounds. The x-axis displays the round $t$, and the y-axis displays how much {\color{mblue}Min-Max}, {\color{mred}OMD}, and {\color{mgreen}\cref{algo:phased-aggression-efg-bandit}} \emph{gained} from the second algorithm so far. The y-axes have varying scales for readability.}\label{fig:all-vs-x}
    \end{minipage}
    \end{minipage}
\end{figure*}

\textbf{All vs All.} In \cref{fig:all-vs-all} we plot the total amount of money each algorithm \emph{wins}. As \cref{fig:all-vs-all} shows, both Min-Max and \cref{algo:phased-aggression-efg-bandit} never incur losses while both gain a significant amount of money against OMD. Indeed, as (symmetrized) Kuhn poker is a symmetric zero-sum game, the min-max equilibrium is guaranteed not to lose. The same holds for our \cref{algo:phased-aggression-efg-bandit}. In contrast, a no-regret algorithm such as OMD can lose up to $O(\sqrt{T})$ amount of money. Interestingly, it does lose a similar amount against our \cref{algo:phased-aggression-efg-bandit}.

\textbf{All vs Exploitable Strategies.} We now compare the performance of Min-Max, OMD and \cref{algo:phased-aggression-efg-bandit} against the following reasonable but suboptimal strategies. The goal is to understand their ability to exploit weak strategies. We consider a) \emph{BluffJ}: Bob plays the min-max equilibrium, except that he bets (bluffs) when he has a Jack; b) \emph{RaiseKQ}: Bob raises/calls if and only if he has a King or a Queen, and checks/folds otherwise; c) \emph{RandMinMax}: Each round, with probability $0.2$, he plays the uniform strategy, and else the min-max one.

In \cref{fig:all-vs-x} we present the amount of money each algorithm \emph{wins} against these exploitable strategies. We first consider \emph{All vs BluffJ \& All vs RaiseKQ}. Algorithm~\ref{algo:phased-aggression-efg-bandit} plays conservatively and gains an amount similar to Min-Max until it takes off and starts exploiting Bob near-optimally, as OMD would. OMD, in turn, first loses a certain amount of money and only matches the gain of Min-Max after exploring sufficiently, then having the same slope as \cref{algo:phased-aggression-efg-bandit}. The min-max equilibrium itself does not exploit \textit{BluffJ} at all and exploits \textit{RaiseKQ} sub-optimally. Our algorithm suffers neither of the two drawbacks of losing money or not exploiting the weak strategy. Finally, in \emph{All vs RandMinMax}, our \cref{algo:phased-aggression-efg-bandit} improves slightly over Min-Max. OMD gains at the same rate after losing an initial amount. 


\section{Conclusion}

In this paper, we showed how to provably exploit suboptimal strategies with essentially no expected risk in repeated zero-sum games by combining regret minimization and minimax play. More generally, we believe that our novel results for adversarial bandits leading to these guarantees may be of independent interest. We hope that our work inspires future research on safe online learning, including settings like convex-concave games, learning with feedback graphs, and establishing no-swap-regret guarantees.

\section*{Acknowledgments}

This work is funded (in part) through a PhD fellowship of the Swiss Data Science Center, a joint venture between EPFL and ETH Zurich. This work was supported by Hasler Foundation Program: Hasler Responsible AI (project number 21043). Research was sponsored by the Army Research Office and was accomplished under Grant Number W911NF-24-1-0048. This work was supported by the Swiss National Science Foundation (SNSF) under grant number 200021\_205011.

\bibliographystyle{plainnat}
\bibliography{refs}


\newpage

\appendix

\input{appendix-literature}

\input{appendix-nfg}

\input{appendix-efg}

\input{appendix-experiments}


\end{document}