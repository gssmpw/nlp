\section{Further Related Work} \label{app:related-work}

\textbf{Safe Opponent Exploitation.} While there have been some approaches to safe learning in games \citep{ponsen2011computing,farina2019online,zhang2021subgame,bernasconi2021exploiting,bernasconi2022safe,ge2024safe}, all these works are fairly different from our learning problem. Related to \citet{ganzfried2015safe,ganzfried2018bayesian}, the works of \citet{damer2017safely,liu2022safe} provide algorithms that interpolate between being safe and exploitive through a specific parameter. However, these algorithms may incur up to $\Omega(T)$ regret compared to the best fixed strategy in hindsight. Recently, \citet{maiti2023logarithmic} proved the first \emph{instance-dependent} poly-logarithmic regret bound for noisy $2\times2$ NFGs, which naturally relates to our desired regret bound. However, such bounds become vacuous when the game matrix does not have pairwise distinct entries and assume to observe the opponent's action (which corresponds to full information in our feedback model).

\textbf{Exploiting Adaptive Opponents.} If Bob is oblivious and plays a fixed sequence of (mixed) strategies, then any regret Alice incurs is potential utility she could gain by playing a no-regret strategy (e.g., the best-of-both-worlds strategy we present). However, if Bob is adaptive, switching to a no-regret strategy does not necessarily allow Alice to recover additional utility (Bob could, for example, react to this by playing his minimax strategy). There is a line of recent work \citep{DSSstrat, MMSSbayesian, kolumbus2022auctions, kolumbus2022and, brown2023is, cai2023selling, chen2023persuading, haghtalab2024calibrated, ananthakrishnan2024knowledge, guruganesh2024contracting, arunachaleswaran2024pareto} on how to play against sub-optimal adaptive strategies (e.g. other learning algorithms) in various settings, although almost all of this work only pertains to general sum games. It is an interesting open question to understand to what extent we can obtain similar best-of-both-worlds results for adaptive opponents in zero-sum games.

\textbf{Comparator-Adaptive OL with Full Information.} In OL under full information feedback, \citet{hutter2005adaptive,even2008regret,kapralov2011prediction,koolen2013pareto,sani2014exploiting} establish (with various emphases) that safe OLM over the simplex in the sense of \cref{eq:safe-olm} is possible. Using so-called parameter-free methods from the online convex optimization literature instead \citep[e.g.]{orabona2016coin,cutkosky2018black,orabona2019modern}, one can (after a simple shifting argument) achieve similar guarantees in the full information setting. For our purposes, the most notable of the above algorithms is the Phased Aggression template of \citet{even2008regret}, as it is the only one we were able to adapt to the bandit feedback setting while maintaining the rate-optimal regret guarantee. While the application of the above type of algorithms to symmetric zero-sum (normal-form) games is direct (\cref{sec:preliminaries}), we are not aware of any prior work making this connection, even under full-information feedback.

\textbf{Comparator-Adaptive OL with Bandit Feedback.} \citet{lattimore2015pareto} establishes a sharp separation between full information and bandit feedback. The author shows that $O(1)$ regret compared to a single comparator action implies a worst-case regret of $\Omega(AT)$ for some other action. This rules out algorithms that resolve our question even in the simple normal-form case under bandit feedback. The key to this lower bound is that the algorithm has to play the special comparator essentially every time, thereby not exploring any other options (as the comparator strategy is deterministic) and thus not knowing whether it is safe to switch the arm. The minimal assumption we can make on the comparator strategy is thus that it plays every action with a non-zero probability. In addition to the mentioned works from the online convex optimization literature, \citet{van2020comparator} remarkably analyzes bandit convex optimization algorithms that adapt to the comparator. However, unlike in the full information case, it is not possible to turn them into an algorithm for safe OLM (as the shifting argument one can use for full-information parameter-free methods like \citet{orabona2016coin,cutkosky2018black,orabona2019modern} does no longer work under bandit feedback).

\noindent\textbf{Relation to Safe Reinforcement Learning.} A closely related line of work is that of \emph{conservative bandits} \citep{wu2016conservative} and \emph{conservative RL} \citep{garcelon2020conservative}. In conservative exploration, algorithms are designed to obtain at least a $(1-\alpha)$-fraction of the return of a comparator, which in our motivating example, however, means that the algorithm may suffer a linear loss $\alpha T$ in the worst case. We thus believe that independently of our motivation from a game-theoretic viewpoint, our results nicely complement existing OL literature. In constrained (or safe) reinforcement learning \citep{badanidiyuru2018bandits,efroni2020exploration}, both the regret and the cumulative violation of a constraint are considered. However, even in the stochastic case the goal of constant regret compared to some known strategy can only be realized if there exists a strategy with a strictly larger return \citep{liu2021learning} for the environment, and in the adversarial case even this reduction fails.

\textbf{Online Learning in (Extensive-Form) Games.} While online learning (OL) in NFGs can readily be reduced to the problem of learning from experts \citep{cesa2006prediction} (full information) or multi-armed bandits \citep{lattimore2020bandit}, it becomes more difficult in the case of EFGs \citep{osborne1994course} due to the presence of (imperfectly observed) states and transitions. State-of-the-art algorithms for no-regret learning in EFGs are based on online mirror descent (OMD) over the treeplex, which leads to near-optimal regret bounds in the full information setting \citep{farina2021bandit,fan2024optimality} and the bandit setting \citep{farina2021bandit,kozuno2021model,bai2022near,fiegel2023adapting}. Alternative approaches are based on counterfactual regret minimization \citep{zinkevich2007regret,lanctot2009monte}, which however do not guarantee a bound on the actual regret (see \citet[Theorem 7]{bai2022near}).