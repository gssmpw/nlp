\section{Deferred Proofs for Extensive-Form Games} \label{app:efg}

\subsection{EFG Background} \label{app:efg-background}

The following remark clarifies that the Markov game we defined in \cref{sec:efg} (which is more common in the machine learning literature) indeed covers the case of imperfect information EFGs (which are more common in the game theory literature).
\begin{remark}
    The notion of EFG in \cref{d:efg} from \cref{sec:efg} is usually referred to as a \emph{\underline{t}ree-structured \underline{p}erfect-recall \underline{p}artially-\underline{o}bservable \underline{M}arkov \underline{g}ame} (TP-POMG). This also covers the notion of \emph{\underline{p}erfect-recall \underline{i}mperfect \underline{i}nformation \underline{e}xtensive-\underline{f}orm \underline{g}ames} (P-IIEFG) \citep{osborne1994course} that satisfy the \emph{timeability condition} \citet{jakobsen2016timeability}. In fact, a more careful look reveals that the results directly generalize to \emph{any} P-IIEFG without timeability (see \citet{bai2022near} for this brief discussion).
\end{remark}

For further clarification, we remark that usually, both the cost function $u$, the transition probabilities $p$ and the policies $\pi$ (and treeplex strategies $\mu$) may be non-stationary in the sense that they explicitly vary across the stages $h\in[H]$ of the EFG. However, as we assume tree structure and perfect recall, the state space and infoset space are partitioned along the stages anyway, which is why WLOG we omit the explicit dependence of the above functions on the stage $h$. Finally, to be precise, our algorithm assumes to know the tree structure of the game (but not necessarily the transitions), an assumption that can be removed \citep{fiegel2023adapting}.

\subsection{OMD over the Treeplex} \label{app:efg-kl}

The unbalanced and balanced dilated KL divergence are defined as follows:
\begin{align*}
    D(\mu || \mu') :=& \sum_{\substack{x\in\Xcal,\\a\in\Acal}} \mu(x,a) \log\round{\frac{\pi_{\mu}(a|x)}{\pi_{\mu'}(a|x)}},\\ \quad \Dbal(\mu || \mu') :=& \sum_{h=1}^H\sum_{\substack{x_h \in \Xcal_h,\\ a\in \Acal}} \frac{\mu(x_h,a)}{\mu^{h,\text{bal}}(x_h,a)} \log\round{\frac{\pi_{\mu}(a|x_h)}{\pi_{\mu'}(a|x_h)}},
\end{align*}
where $\pi_{\mu}$ is the policy corresponding to the treeplex strategy $\mu$, and $\mu^{h,\text{bal}}$ is the unique strategy corresponding to the \emph{balanced exploration policy}
\begin{align*}
        \pi^{h,\text{bal}}(a | x_{h'}) := \begin{cases}
        \frac{|\Ccal_h(x_{h'},a)|}{|\Ccal_h (x_{h'})|}&\quad (h'\in\{1,\dots,h-1\}),\\
        \frac{1}{A} &\quad (h'\in\{h,\dots,H\}),
    \end{cases}
\end{align*}
with $\Ccal_h(x_{h'},a) \subset \Xcal_h$ being set of infosets at step $h$ reachable from $(x_{h'},a)$ (i.e. the unique path to such an infoset goes through $(x_{h'},a)$), and $|\Ccal_h (x_{h'})| := \bigcup_{a\in \Acal} \Ccal_h(x_{h'},a)$.

\paragraph{Computation of Unbalanced OMD.}

\noindent For completeness, we restate the closed-form implementation of case one in \cref{line:omd} with the \emph{unbalanced} dilated divergence $D$ from \citet[Appendix B]{kozuno2021model}. In the setup of \cref{line:omd}, let $\piHat^t\in\Pi$ be the policy corresponding to $\muHat^t$. Then we have a closed-form
\begin{align*}
    \piHat^{t+1}(a_h|x_h^t) =& \piHat^t(a_h|x_h^t) \exp\round{\indicator{a_h^t=a_h}(-\eta\cHat^t(x_h^t,a_h)+\log(Z^t_{h+1}))-\log(Z^t_{h})},
\end{align*}
and $\piHat^{t+1}(\cdot|x_h)=\piHat^{t}(\cdot|x_h)$ for all other $x_h\neq x_h^t$. Here, $Z_h^t$ is
\begin{align*}
    Z_h^t := 1-\piHat^t(a_h^t|x_h^t)+\piHat^t(a_h^t|x_h^t)\exp\round{-\eta\cHat^t(x_h^t,a_h^t)+\log(Z^t_{h+1})},
\end{align*}
and $Z_{H+1}^t := 1$.

\paragraph{Computation of Balanced OMD.}

\noindent For completeness, we also restate the closed-form implementation of case two in \cref{line:omd} with the \emph{balanced} dilated divergence $\Dbal$ from \citet[Algorithm 5]{bai2022near}. Once more, let $\piHat^t\in\Pi$ be the policy corresponding to $\muHat^t$. Then we have a closed form for the next iterate, namely
\begin{align*}
    &\piHat^{t+1}(a_h|x_h^t)\\
    &=\piHat^t(a_h|x_h^t) \exp\round{ \indicator{a_h=a_h^t} \round{-\tau \mu^{\text{bal},h}(x_h^t,a_h^t) \cHat^t(x_h^t,a_h^t) + \frac{\mu^{\text{bal},h}(x_h^t,a_h^t) \log(Z_{h+1}^t)}{\mu^{\text{bal},h+1}(x_{h+1}^t,a_{h+1}^t)}} - \log(Z_h^t) },
\end{align*}
and in the other infosets $\piHat^{t+1}(a_h|x_h) = \piHat^{t}(a_h|x_h)$. Here,
\begin{align*}
    Z_h^t := 1-\piHat(a_h^t|x_h^t) + \piHat(a_h^t|x_h^t) \exp\round{ -\tau \mu^{\text{bal},h}(x_h^t,a_h^t) \cHat^t(x_h^t,a_h^t) + \frac{\mu^{\text{bal},h}(x_h^t,a_h^t) \log(Z_{h+1}^t)}{\mu^{\text{bal},h+1}(x_{h+1}^t,a_{h+1}^t)}},
\end{align*}
and $Z_{H+1}^t=1$.


\subsection{Upper Bound} \label{app:efg-upper}

First, note that due to the importance-weighting by the rollout policies the cost estimators are unbiased \citep{kozuno2021model}: $\exptn\rectangular{\cHat^t(x,a)} = \exptn\rectangular{ c^t(x,a) }$, and $\exptn\rectangular{ \inner{\cHat^t}{\mu^t} } = \exptn\rectangular{\exptn\rectangular{ \inner{\cHat^t}{\mu^t} \mid \Fcal_{t-1} }} = \exptn\rectangular{ \inner{c^t}{\mu^t} }$, where $\Fcal_{t-1}$ is the $\sigma$-algebra induced by all random variables prior to sampling the trajectory $(s_1^t,a_1^t,b_1^t,u_1^t\dots,s_H^t,a_H^t,b_H^t,u_H^t)$. Further, WLOG we assume that the costs $u(s,a,b)$ used to define the cost function $c^t$ in \cref{eq:def-loss} are bounded in $[0,1]$. While in EFGs we assumed $u(s,a,b)\in[-1,1]$, we can simply replace them by $(1+u(s,a,b))/2$ without changing the regret bound. With this, we can prove the desired upper bound by resorting to the estimated regret
\begin{align}
    \RHat^k(\mu):=& \sum_{t=\start_k}^{\start_{k+1}-1} \inner{\cHat^t}{\mu^t-\mu} = \alpha^k \sum_{j=\start_k}^{\start_{k+1}-1} \inner{\cHat^t}{\muHat^t-\mu} + (1-\alpha^k) \sum_{t=\start_{k}}^{\start_{k+1}-1} \inner{\cHat^t}{\mu^c-\mu}. \label{eq:phase-reg-efg}
\end{align}
By convention $\start_{k+1}:=T+1$ if $k$ is the last phase. 

\thmUbInteriorEfg*

\begin{proof}
    \emph{Case 1: $\alpha=1$ is not reached.} Suppose first the algorithm ends in phase $k$ with $\alpha^k<1$ at time step $T$. By \cref{lemma:during-efg}, w.r.t. any comparator 
    \begin{align*}
        \sum_{t=1}^T \inner{\cHat^t}{\mu^t-\mu} \leq (2R+2H) \cdot k \leq O(R \log(R)).
    \end{align*}
    All previous phases must have been exited, so by \cref{lemma:during-efg,lemma:exit-efg} we have 
    \begin{align*}
        \sum_{t=1}^T \inner{\cHat^t}{\mu^t-\mu^c} \leq 2^{k-1} - \sum_{i=1}^{k-1} 2^{i-1} = 2^{k-1} - (2^{k-1}-1) = 1.
    \end{align*}
    Taking expectation yields the claim.\\

    \noindent\emph{Case 2: $\alpha=1$ is reached.} Next, suppose $\alpha^k=1$ was reached. Then balanced mirror descent was run in the final phase $k$. As before
    \begin{align*}
        \sum_{t=1}^{\start_{k}-1} \inner{\cHat^t}{\mu^t-\mu} \leq (2R+2H) \cdot k \leq O(R \log(R)).
    \end{align*}
    For the final phase, note that the algorithm runs balanced OMD with importance weights and uniform initialization for $\leq T$ rounds. Thus by \cref{lem:regret-to-init}, this phase has expected regret 
    \begin{align}
        \exptn\rectangular{\sum_{t=\start_{k}}^T \inner{c^t}{\mu^t-\mu}} \leq& \tau H^3T + \frac{1}{\tau}\Dbal(\mu || \mu^{\start_{k}}) \nonumber\\
        \leq& \tau H^3T + \frac{XA\log(A)}{\tau} \tag{by \citet[Lemma C.7]{bai2022near}}\nonumber\\
        \leq& \sqrt{XAH^3\log(A)T} \tag{since $\tau=\sqrt{\frac{XA\log(A)}{2H^3T}}$}\nonumber\\
        \leq& R, \label{eq:exp3-efg}
    \end{align}
    using $\delta\leq1/A$. Thus for any comparator, we have 
    \begin{align*}
        \exptn\rectangular{\sum_{t=1}^T \inner{c^t}{\mu^t - \mu}} \leq O(R\log(R)) + R = O(R\log(R)).
    \end{align*}
    Finally, for the special comparator, we note that all phases $k'$ with $\alpha^{k'}<1$ have been left and thus by \cref{lemma:exit-efg,eq:exp3-efg}
    \begin{align*}
        \exptn\rectangular{\sum_{t=1}^T \inner{c^t}{\mu^t - \mu^c}} \leq R - \sum_{k'=1}^{k-1} 2^{k'-1}= R - (2^{k-1}-1) \leq 1,
    \end{align*}
    where the last step used that $\alpha^k=\min\{1,2^{k-1}/R\}=1$ and thus $R\leq2^{k-1}$.
\end{proof}

The following lemma establishes the statement from \cref{lemma:during-nfg}, generalized to EFGs. The second part of the lemma is essentially the same. Once more, the fact that $\mu^c$ is lower bounded comes into play when upper bounding the estimated cost functions.

\begin{restatable}[During normal phases]{lemma}{lemmaDuringEfg} \label{lemma:during-efg}
    Let $k$ be such that $\alpha^k<1$. Then for all $\mu\in\Tcal$, almost surely
    \begin{align*}
        \RHat^k(\mu) \leq 2R+2H=2\delta^{-1}\sqrt{8XH^3\log(A) T}+2H,
    \end{align*}
    and for the special comparator almost surely $\RHat^k(\mu^c) \leq 2^{k-1}$.
\end{restatable}

\begin{proof}
    WLOG suppose that $R=2^r$ is a power of $2$, else we can run the algorithm for $T$ such that $R$ is the next largest power of two and pay a constant factor in the regret. For the first term in \cref{eq:phase-reg-efg}, we analyze unbalanced OMD to bound $\sum_{t=\start_{k}}^{\start_{k+1}-1} \inner{\cHat^t}{\muHat^t-\mu}$ almost surely, making use of the fact that $\cHat^t$ is bounded. Recall 
    \begin{align*}
        \cHat^t(x_{h},a) = \frac{\indicator{(x_{h}^t,a_{h}^t)=(x_{h},a)}u^t_h}{\mu^t(x_{h},a)} \leq \frac{1}{\mu^t(x_{h},a)}.
    \end{align*}
    Now since $R=2^r$ is a power of $2$, we have $\alpha^k = 2^{k-1}/R \leq 2^{\log_2(R)-1}/R=1/2$, so 
    \begin{align*}
        \cHat^t(x_h,a) \leq \frac{1}{\mu^t(x_{h},a)}=\frac{1}{\alpha\mu^t(x_{h},a)+(1-\alpha)\mu^c(x_{h},a)}\leq \frac{1}{\frac{1}{2}\mu^c(x_{h},a)} \leq\frac{2}{\delta}.
    \end{align*}
    Moreover, $\cHat^t$ is zero outside the visited $((x_h^t,a_h^t))_h$. Thus, by \cref{lem:bandit-omd-bounded}, for the first term in \cref{eq:phase-reg-efg} almost surely
    \begin{align}
        \sum_{t=\start_{k}}^{\start_{k+1}-1} \inner{\cHat^t}{\muHat^t-\mu} \leq \frac{X\log(A)}{\eta} + \frac{4 \eta TH(H+1)}{\delta^2} \leq \delta^{-1}\sqrt{8XH^2\log(A) T} \leq R. \label{eq:omd-regret1-efg}
    \end{align}

    \noindent For the second term in \cref{eq:phase-reg-efg}, note that since the if may only hold at $t':=\start_{k+1}-1$,
    \begin{align}
        \sum_{t=\start_{k}}^{\start_{k+1}-1} \inner{\cHat^t}{\mu^c-\mu} \leq 2R+\inner{\cHat^{t'}}{\mu^c}\leq 2R+2H. \label{eq:while-cond1-efg}
    \end{align}

    \noindent Linearly combining \cref{eq:omd-regret1-efg,eq:while-cond1-efg},
    \begin{align*}
        \RHat^k(\mu) = \alpha^k \sum_{t=\start_{k}}^{\start_{k+1}-1} \inner{\cHat^t}{\muHat^t-\mu} + (1-\alpha^k) \sum_{t=\start_k}^{\start_{k+1}-1} \inner{\cHat^t}{\mu^c-\mu} \leq 2R +2H
    \end{align*}
    for any $\mu$, and for the special comparator $\mu^c$ we have by \cref{eq:omd-regret1-efg}
    \begin{align*}
        R^k(\mu^c)= \alpha^k \sum_{t=\start_{k}}^{\start_{k+1}-1} \inner{\cHat^t}{\muHat^t-\mu^c} + (1-\alpha^k) \sum_{t=\start_{k}}^{\start_{k+1}-1} \inner{\cHat^t}{\mu^c-\mu^c} \leq (2^{k-1}/R)R = 2^{k-1}. 
    \end{align*}
\end{proof}

\noindent Now suppose the algorithm exits a phase $k$. The following result mimics \cref{lemma:exit-nfg} for the case of EFGs, and we resort to essentially the same proof.
\begin{restatable}[Exiting a phase]{lemma}{lemmaExitEfg} \label{lemma:exit-efg}
    Let $k$ be such that $\alpha^k<1$ and suppose \cref{algo:phased-aggression-efg-bandit} exits phase $k$ at time step $\start_{k+1}-1$. Then almost surely $\RHat^k(\mu^c)\leq -2^{k-1}$.
\end{restatable}

\begin{proof}
    The if condition implies $\max_{\mu\in\Tcal}\sum_{t = \start_{k}}^{\start_{k+1}-1} \inner{\cHat^t}{\mu^c - \mu} > 2 R$, so when we let $\mu^{\star}$ be a maximizer, we find
    \begin{align*}
        \sum_{t=\start_{k}}^{\start_{k+1}-1} \inner{\cHat^t}{\mu^t-\mu^c} =& \alpha^k \sum_{t=\start_{k}}^{\start_{k+1}-1} \inner{\cHat^t}{\muHat^t-\mu^c} \nonumber\\
        =& \alpha^k \sum_{t=\start_{k}}^{\start_{k+1}-1} \inner{\cHat^t}{\muHat^t-\mu^{\star}} +\alpha^k \sum_{t=\start_{k}}^{\start_{k+1}-1} \inner{\cHat^t}{\mu^{\star}-\mu^c} \nonumber\\
        \leq& \alpha^k R + \alpha^k(-2R)\nonumber\\
        =& -2^{k-1},
    \end{align*}
    using \cref{eq:omd-regret1-efg} in the last inequality.
\end{proof}

\newpage

\subsection{Auxiliary Lemmas: OMD on the EFG Tree}

\paragraph{Unbalanced OMD Lemmas.}

\begin{lemma}[Bandit OMD with bounded surrogate costs] \label{lem:bandit-omd-bounded}
    Let $\eta >0$, and $L>0$. Let $(\cHat^t)_t$ be cost functions such that for all $t$, $0\leq \cHat^t(x_h,a)\leq L$ (for all $x_h$, $a$), and moreover $\cHat^t(x_h,a) = 0$ if $(x_h,a)\neq(x_h^t,a_h^t)$, where $x_h^t$, $a_h^t$ are arbitrary. Set $\muHat^1(x_h,a) = 1/A^h$ and consider the scheme 
    \begin{align*}
        \muHat^{t+1} =& \arg\min_{\mu \in \Tcal} \inner{\mu}{\cHat^t} + \frac{1}{\eta} D(\mu || \muHat^t)
    \end{align*}
    for $t\leq T'$. Then we have for all $\muHat\in\Tcal$
    \begin{align*}
        \sum_{t=1}^{T'} \inner{\muHat^t-\muHat}{\cHat^t} \leq \frac{X\log(A)}{\eta} + \eta H(H+1)L^2 T'.
    \end{align*}
\end{lemma}

\begin{proof}
    By \cref{lem:KL-diff-bandit}, 
    \begin{align*}
        D(\muHat||\muHat^t) - D(\muHat||\muHat^{t+1}) + D(\muHat^t||\muHat^{t+1}) =& -(D(\muHat||\muHat^{t+1})-D(\muHat||\muHat^t)) + (D(\muHat^t||\muHat^{t+1})-D(\muHat^t||\muHat^t))\\
        =& \eta \inner{\muHat^t-\muHat}{\cHat^t}.
    \end{align*}
    Thus (using $D\geq 0$), we have a regret bound of
    \begin{align*}
        \sum_{t=1}^{T'} \inner{\muHat^t-\muHat}{\cHat^t} \leq \frac{1}{\eta} \round{D(\muHat||\muHat^1) + \sum_{t=1}^T D(\muHat^t||\muHat^{t+1})}.
    \end{align*}
     For the first term we easily have $D(\muHat||\muHat^1) \leq X\log(A)$ \citep[Lemma 6]{kozuno2021model}. For the second term, by \cref{lem:KL-diff-bandit}, we have
     \begin{align*}
         D(\muHat^t||\muHat^{t+1}) = D(\muHat^t||\muHat^{t+1}) - D(\muHat^t||\muHat^t)
         \leq \eta\inner{\muHat^t}{\cHat^t} + \log(Z_1^t)= \eta\sum_{h=1}^H\muHat^t(x_h^t,a_h^t)\cHat^t(x_h^t,a_h^t) + \log(Z_1^t),
     \end{align*}
     using that $\cHat^t$ is zero outside $((x_h^t,a_h^t))_h$. By \cref{eq:Z-closed-form} and $\log(1+x)\leq x$,
     \begin{align*}
         \log(Z_1^t) \leq& \sum_{h=1}^H \muHat^t(x_{h}^t,a_{h}^t)\exp\round{-\eta \sum_{h'=1}^{h-1} \cHat^t(x_{h'}^t,a_{h'}^t)} \round{\exp\round{-\eta\cHat^t(x_{h}^t,a_{h}^t)} - 1} \\
         \leq& \sum_{h=1}^H \muHat^t(x_{h}^t,a_{h}^t)\exp\round{-\eta \sum_{h'=1}^{h-1} \cHat^t(x_{h'}^t,a_{h'}^t)} \round{-\eta\cHat^t(x_{h}^t,a_{h}^t)+\eta^2\cHat^t(x_{h}^t,a_{h}^t)^2},
     \end{align*}
     where we used $\exp(-y)\leq 1-y+y^2$ for $y\geq 0$. We thus find, using $\cHat^t\geq0$ throughout,
     \begin{align*}
         D(\muHat^t||\muHat^{t+1})\leq& \eta\sum_{h=1}^H\muHat^t(x_h^t,a_h^t)\cHat^t(x_h^t,a_h^t) + \log(Z_1^t)\\
         \leq& \eta\sum_{h=1}^H\muHat^t(x_h^t,a_h^t)\cHat^t(x_h^t,a_h^t) \\
         &+\sum_{h=1}^H \muHat^t(x_{h}^t,a_{h}^t)\exp\round{-\eta \sum_{h'=1}^{h-1} \cHat^t(x_{h'}^t,a_{h'}^t)} \round{-\eta\cHat^t(x_{h}^t,a_{h}^t)+\eta^2\cHat^t(x_{h}^t,a_{h}^t)^2}\\
         =& \eta\sum_{h=1}^H\muHat^t(x_h^t,a_h^t)\cHat^t_h(x_h^t,a_h^t)\round{1-\exp\round{-\eta \sum_{h'=1}^{h-1} \cHat^t(x_{h'}^t,a_{h'}^t)}} \\
         &+ \eta^2\sum_{h=1}^H \muHat^t(x_{h}^t,a_{h}^t)\exp\round{-\eta \sum_{h'=1}^{h-1} \cHat^t(x_{h'}^t,a_{h'}^t)} \cHat^t(x_{h}^t,a_{h}^t)^2\\
         \leq& \eta\sum_{h=1}^H\muHat^t(x_h^t,a_h^t)\cHat^t(x_h^t,a_h^t)\round{1-\exp\round{-\eta \sum_{h'=1}^{h-1} \cHat^t(x_{h'}^t,a_{h'}^t)}} \\
         &+ \eta^2\sum_{h=1}^H \muHat^t(x_{h}^t,a_{h}^t) \cHat^t(x_{h}^t,a_{h}^t)^2\\
         \leq& \eta\sum_{h=1}^H\muHat^t(x_h^t,a_h^t)\cHat^t(x_h^t,a_h^t)\round{\eta \sum_{h'=1}^{h-1} \cHat^t(x_{h'}^t,a_{h'}^t)} + \eta^2\sum_{h=1}^H \muHat^t(x_{h}^t,a_{h}^t) \cHat^t(x_{h}^t,a_{h}^t)^2,
     \end{align*}
     where we used $1-\exp(-x)\leq x$ in the last step. Finally, using the bound on the cost functions and the fact that all $\muHat^t(x_h^t,a_h^t) \leq 1$, we find
     \begin{align*}
         D(\muHat^t||\muHat^{t+1})\leq& \eta^2 H^2L^2 + \eta^2H L^2\leq \eta^2H(H+1) L^2.
     \end{align*}
     Summing over $t$ concludes the proof.
\end{proof}

\noindent In the setup of \cref{lem:bandit-omd-bounded}, let $\piHat^t\in\Pi$ be the policy corresponding to $\muHat^t$ and recall (\cref{app:efg-background})
\begin{align*}
    Z_{H+1}^t =& 1,\\
    Z_h^t =& \sum_{a_h} \piHat^t(a_h|x_h^t) \exp\round{\indicator{a_h^t=a_h}(-\eta\cHat^t(x_h^t,a_h)+\log(Z_{h+1}^t))}\\
    =& 1-\piHat^t(a_h^t|x_h^t)+\piHat^t(a_h^t|x_h^t)\exp\round{-\eta\cHat^t(x_h^t,a_h^t)+\log(Z^t_{h+1})},
\end{align*}
\noindent The following lemma is a slight generalization of \citet{kozuno2021model}. Indeed, the proof only uses that $\cHat^t$ is zero outside of the visited $((x_h^t,a_h^t))_h$, not whether we normalize by $\muHat^t$ or $\mu^t$ or from which policy the trajectory $(x_h^t,a_h^t)_h$ is sampled from. The same holds for the following closed form of $Z_1^t$ \citep[c.f. Lemma 6]{kozuno2021model}:
\begin{align}
    Z_1^t = 1 + \sum_{h'=1}^H \muHat^t(x_{h'}^t,a_{h'}^t)\exp\round{-\eta \sum_{h''=1}^{h'-1} \cHat^t(x_{h''}^t,a_{h''}^t)} \round{\exp\round{-\eta\cHat^t(x_{h'}^t,a_{h'}^t)} - 1}. \label{eq:Z-closed-form}
\end{align}

\begin{lemma}[\citet{kozuno2021model}, Lemma 7]\label{lem:KL-diff-bandit}
    In the setup of \cref{lem:bandit-omd-bounded}, we have 
    \begin{align*}
        D(\mu||\muHat^{t+1})-D(\mu||\muHat^t)=\eta\inner{\mu}{\cHat^t}+\log(Z_1^t)
    \end{align*}
    a.s. for all $t\leq T'$, $\mu\in\Tcal$.
\end{lemma}

\paragraph{Balanced OMD Lemmas.}

\noindent Recall the definition of $c^t$ from \cref{eq:def-loss}, for which $\cHat^t$ is an unbiased estimator. Again, recall that we WLOG replaced assume $u(s,a,b)\in[0,1]$ (by rescaling via $(1+u(s,a,b))/2$) for simplicity, without changing the regret bound. 

\begin{lemma} \label{lem:regret-to-init}
    Let $\tau >0$. Set $\muHat^1(x_h,a) = 1/A^h$ and with costs from \cref{eq:def-loss} for Protocol \ref{prot:EFGs-bandit} consider the scheme 
    \begin{align*}
        \cHat^t(x_h,a) =& \frac{\indicator{(x_h,a)=(x_h^t,a_h^t)}u_h^t}{\muHat^t(x_h,a)},\\
        \muHat^{t+1} =& \arg\min_{\mu \in \Tcal} \round{\inner{\mu}{\cHat^t} + \frac{1}{\tau} \Dbal(\mu || \muHat^t)}
    \end{align*}
    for $t\leq T'$. Then for all $\muHat\in\Tcal$
    \begin{align*}
        \exptn\rectangular{\sum_{t=1}^{T'} \inner{\muHat^t-\muHat}{c^t}} \leq \frac{\tau}{2} H^3T' + \frac{1}{\tau} \Dbal(\mu || \mu^1).
    \end{align*}
\end{lemma}

\begin{proof}
    By \cref{lem:KL-to-linear}, we have 
    \begin{align*}
        \frac{1}{\tau} \round{\Dbal(\muHat || \muHat^{t+1}) - \Dbal(\muHat || \muHat^t)} = \inner{\muHat}{\cHat^t} + \Xi_1^t.
    \end{align*}
    Thus,
    \begin{align*}
        \frac{1}{\tau} \exptn\rectangular{\Dbal(\muHat || \muHat^{T'}) - \Dbal(\muHat || \muHat^1)} =& \exptn\rectangular{\sum_{t=1}^{T'} \inner{\muHat}{\cHat^t} + \sum_{t=1}^{T'} \Xi_1^t}\\
        \leq& \exptn\rectangular{\sum_{t=1}^{T'} \inner{\muHat-\muHat^t}{\cHat^t}} + \frac{\tau H^3}{2} T'\tag{by \cref{lem:sum-xi}}\\
        =& \exptn\rectangular{\sum_{t=1}^{T'} \inner{\muHat-\muHat^t}{c^t}} + \frac{\tau H^3}{2} T',
    \end{align*}
    as $\exptn\rectangular{\cHat^t(x,a) \mid \mathcal{F}_{t-1}} = c^t(x,a)$. Using $\Dbal \geq 0$, we conclude
    \begin{align*}
        \exptn\rectangular{ \sum_{t=1}^{T'} \inner{\muHat^t-\muHat}{c^t} } \leq& \frac{1}{\tau} \Dbal(\muHat||\muHat^1) + \frac{\tau H^3}{2} T'.
    \end{align*}
\end{proof}

As before, the following lemma from \citet[Lemma D.7]{bai2022near} does not use the specific form of the cost estimates but only the update rules.
\begin{lemma} \label{lem:KL-to-linear}
    In the setup of \cref{lem:regret-to-init}, for all $\mu \in \Tcal$, we have
    \begin{align*}
        \Dbal(\mu||\muHat^{t+1}) - \Dbal(\mu||\muHat^t) = \tau \inner{\mu}{\cHat^t} + \frac{\log(Z^t_1)}{\mu^{\text{bal},1}(x_1^t,a_1^t)}=\tau \inner{\mu}{\cHat^t} + \tau \Xi_1^t.
    \end{align*}
\end{lemma}

\noindent We introduce some extra notation for convenience: Let $\piHat^t\in\Pi$ be the policy corresponding to $\muHat^t$ and set
\begin{align*}
    \beta_h^t := \tau \mu^{\text{bal},h}(x_h^t,a_h^t), \quad
    \piHat^t_h := \piHat^t(a_h^t|x_h^t), \quad 
    \cHat^t_h := \cHat^t(x_h^t,a_h^t),
\end{align*}
and consider the functions 
\begin{align*}
    \Xi_H^t(\cHat) :=& \Xi_H^t(\cHat_{H}) := \log\round{1-\piHat_H^t + \piHat_H^t\exp(-\beta_H^t\cHat_H)} / \beta_H^t,\\
    \Xi_h^t(\cHat) :=& \Xi_h^t(\cHat_{h:H}) := \log\round{1-\piHat_h^t + \piHat_h^t\exp(\beta_h^t(\Xi_{h+1}^t(\cHat_{h+1:H})-\cHat_h))} / \beta_H^t \quad (h<H),
\end{align*}
and the values
\begin{align*}
    \Xi_h^t :=& \Xi_h^t(\cHat^t) = \frac{1}{\beta_h^t} \log(Z^t_h) = \frac{1}{\beta_h^t} \log\round{ 1-\piHat_h^t + \piHat_h^t\exp(\beta_h^t(\Xi_{h+1}^t - \cHat^t_h))) } \quad (h\in[H])
\end{align*}
for the input $\cHat^t$. The following lemma now lets us bound the remaining term in the proof of \cref{lem:regret-to-init}.

\begin{lemma} \label{lem:sum-xi}
    In the setup of \cref{lem:regret-to-init}, we have 
    \begin{align*}
        \sum_{t=1}^T \exptn\rectangular{\Xi_1^t} \leq -\sum_{t=1}^{T'} \exptn\rectangular{\inner{\muHat^t}{\cHat^t}} + \frac{\tau}{2} H^3 {T'}.
    \end{align*}
\end{lemma}

\begin{proof}
    By \cref{lem:log-to-linear} and as $\cHat^t$ is unbiased,
    \begin{align*}
        \sum_{t=1}^{T'} \exptn\rectangular{\Xi_1^t} \leq& -\sum_{t=1}^{T'} \exptn\rectangular{\inner{\muHat^t}{\cHat^t}} + \frac{\tau H}{2} \sum_{t=1}^{T'} \sum_{h=1}^{H} \sum_{h'=h}^H \sum_{x_{h'},a_{h'}} \exptn\rectangular{\mu^{\text{bal},h}_{1:h}(x_{h},a_{h}) \muHat^{t}_{h+1:h'}(x_{h'},a_{h'}) \cHat^t(x_{h'},a_{h'})}\\
        =& -\sum_{t=1}^{T'} \exptn\rectangular{\inner{\muHat^t}{c^t}} + \frac{\tau H}{2} \sum_{t=1}^{T'} \sum_{h=1}^{H} \sum_{h'=h}^H \underbrace{\sum_{x_{h'},a_{h'}} \exptn\rectangular{\mu^{\text{bal},h}(x_{h},a_{h}) \muHat^{t}_{h+1:h'}(x_{h'},a_{h'}) c^t(x_{h'},a_{h'})}}_{\leq 1}\\ 
        \leq& -\sum_{t=1}^{T'} \exptn\rectangular{\inner{\muHat^t}{c^t}} + \frac{\tau H^3}{2} T'. 
    \end{align*}
\end{proof}

\begin{lemma}[\citet{bai2022near}, Lemma D.11]\label{lem:log-to-linear}
    We have
    \begin{align*}
        \Xi^t_1 \leq -\inner{\muHat^t}{\cHat^t} + \frac{\tau H}{2} \sum_{h=1}^{H} \sum_{h'=h}^H \sum_{x_{h'},a_{h'}} \mu^{\text{bal},h}(x_{h'},a_{h'}) \muHat^{t}_{h+1:h'}(x_{h'},a_{h'})\cHat^t(x_{h'},a_{h'}),
    \end{align*}
    where $\muHat^t_{h+1:h'}(x_{h'}^t,a_{h'}^t):=\prod_{h''=h+1}^{h'}\piHat^t(a_{h''}|x_{h''})$ along the unique path $(x_{h''},a_{h''})_{h''}$ leading from step $h+1$ to $(x_{h'}^t,a_{h'}^t)$.
\end{lemma}

\noindent The proof is the same as in \citet{bai2022near}.\footnote{There, in (ii) we still have $\muHat^t(x_h,a_h) \cHat^t(x_h,a_h) \leq 1$. All other properties used in the proof hold for general $\cHat \geq 0$ (in particular Lemma D.9 and D.10, although stated for $\lTilde \in [0,1]^H$).} 

\subsection{Lower Bound} \label{app:efg-lower}


\thmEfgLower*

\begin{proof}
    Consider an $A$-nary tree with $X=\Theta(A^H)$ leaves and where each infoset corresponds to a unique state. As for the transitions, the learner is deterministically sent to a leaf $s=(a_1,\dots,a_A)$ upon playing $a_h$ in each step $h$. Since $\delta=\min_{x,a}\mu^c(x,a)$, there also exists a leaf information set $x=x(s)$ and and an action $a$ such that $\mu^c(x,a)=\delta$. Now consider two environments in which all state-action triples have cost one, except for the cost in leaf $s$, which is either sampling according to the $(+)$ or $(-)$ environment from \cref{thm:lower-nfg}. We are thus effectively simulating a two-armed bandit with comparator $(1-\delta,\delta)$ with the same construction as in the simplex case. The derivation in \cref{thm:lower-nfg} thus concludes the proof. 
\end{proof}
