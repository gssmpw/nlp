\section{Related Work}
% \textbf{Imitation learning from demo.} 

\textbf{Diffusion based policies.} 
Diffusion-based policies have shown recent success in robotics and decision-making applications. In a pioneering work, “Diffuser”~\cite{janner2022planning}, a planning algorithm
with diffusion models for offline reinforcement learning.
This framework is extended to other tasks in the context of
offline reinforcement learning~\cite{wang2022diffusion}, where the
training dataset includes reward values.
% For example, Ajay
% et al. (2022) propose to model policies as conditional diffusion models. 
Most typically, diffusion based policies are trained from human demonstrations through a supervised objective, and enjoy both high training stability and strong performance in modeling complex and multi-modal trajectory distributions. 
The application of DDPM~\cite{ddpm} and DDIM~\cite{ddim} on visuomotor policy learning for physical robots~\cite{dp} outperforms counterparts like Behavioral Cloning.
While these techniques effectively learn
from multi-modal data, they often create models that are non-trivial to fine-tune using RL. Even if they were compatible with RL, the fine-tuning process can be computationally prohibitive due to the large number of parameters in modern policy models. 

\textbf{Training diffusion models with reinforcement learning.} As demonstration data are often limited, there have been many approaches proposed to improve the performance of diffusion-based policies. 
% One popular approach has been to guide the diffusion denoising process using objectives such as reward signal or goal conditioning. 
% More recent work has explored the possibilities of techniques including Q-learning and weighted regression, either from purely offline estimation or with online interaction.
One straightforward approach \cite{DBLP:conf/iclr/BlackJDKL24, fan2024reinforcement} involves framing diffusion denoising as a Markov Decision Process (MDP), which facilitates preference-aligned generation with policy gradient reinforcement learning. However, this approach often suffers from instability, limiting its practical applicability. \cite{dppo} introduced policy gradient loss on a two-layer MDP for direct diffusion policy fine-tuning, which mitigates this instability, but the method is architecture-specific and does not introduce closed-loop control. 
Alternative approaches to integrating diffusion architectures with reinforcement learning (RL) include leveraging Q-function-based importance sampling~\cite{idql}, employing advantage-weighted regression~\cite{goo2022know}, or reformulating the objective as a supervised learning problem with return conditioning~\cite{chen2021decision,janner2022planning,ajayconditional}.
Additionally, researchers have explored enhancing the denoising training objective by incorporating Q-function maximization~\cite{wang2022diffusion} and iteratively refining the dataset using Q-functions~\cite{yang2023policy}. 
Another promising direction involves augmenting a frozen, chunked diffusion policy model with a residual policy trained through online RL, enabling improved performance without modifying the pre-trained diffusion model~\cite{ankile2024imitation}.



% Recent work developed techniques for training diffusion policies from scratch~\cite{li2024learning}, leveraging unsupervised clustering and Q-learning ensembles to encourage multi-modal behavior discovery.