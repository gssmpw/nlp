\section{Related Work}
% \textbf{Imitation learning from demo.} 

\textbf{Diffusion based policies.} 
Diffusion-based policies have shown recent success in robotics and decision-making applications. In a pioneering work, “Diffuser”____, a planning algorithm
with diffusion models for offline reinforcement learning.
This framework is extended to other tasks in the context of
offline reinforcement learning____, where the
training dataset includes reward values.
% For example, Ajay
% et al. (2022) propose to model policies as conditional diffusion models. 
Most typically, diffusion based policies are trained from human demonstrations through a supervised objective, and enjoy both high training stability and strong performance in modeling complex and multi-modal trajectory distributions. 
The application of DDPM____ and DDIM____ on visuomotor policy learning for physical robots____ outperforms counterparts like Behavioral Cloning.
While these techniques effectively learn
from multi-modal data, they often create models that are non-trivial to fine-tune using RL. Even if they were compatible with RL, the fine-tuning process can be computationally prohibitive due to the large number of parameters in modern policy models. 

\textbf{Training diffusion models with reinforcement learning.} As demonstration data are often limited, there have been many approaches proposed to improve the performance of diffusion-based policies. 
% One popular approach has been to guide the diffusion denoising process using objectives such as reward signal or goal conditioning. 
% More recent work has explored the possibilities of techniques including Q-learning and weighted regression, either from purely offline estimation or with online interaction.
One straightforward approach ____ involves framing diffusion denoising as a Markov Decision Process (MDP), which facilitates preference-aligned generation with policy gradient reinforcement learning. However, this approach often suffers from instability, limiting its practical applicability. ____ introduced policy gradient loss on a two-layer MDP for direct diffusion policy fine-tuning, which mitigates this instability, but the method is architecture-specific and does not introduce closed-loop control. 
Alternative approaches to integrating diffusion architectures with reinforcement learning (RL) include leveraging Q-function-based importance sampling____, employing advantage-weighted regression____, or reformulating the objective as a supervised learning problem with return conditioning____.
Additionally, researchers have explored enhancing the denoising training objective by incorporating Q-function maximization____ and iteratively refining the dataset using Q-functions____. 
Another promising direction involves augmenting a frozen, chunked diffusion policy model with a residual policy trained through online RL, enabling improved performance without modifying the pre-trained diffusion model____.



% Recent work developed techniques for training diffusion policies from scratch____, leveraging unsupervised clustering and Q-learning ensembles to encourage multi-modal behavior discovery.