%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{multirow}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{mathrsfs}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Score-Based Diffusion Policy Compatible with Reinforcement Learning
via Optimal Transport}

\begin{document}

\twocolumn[
\icmltitle{Score-Based Diffusion Policy \\Compatible with Reinforcement Learning
via Optimal Transport}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Mingyang Sun}{Zju,West}
\icmlauthor{Pengxiang Ding}{Zju,West}
\icmlauthor{Weinan Zhang}{SJTU}
\icmlauthor{Donglin Wang}{West}
\end{icmlauthorlist}


\icmlaffiliation{Zju}{Zhejiang University, Hangzhou, China}
\icmlaffiliation{West}{Westlake University, Hangzhou, China}
\icmlaffiliation{SJTU}{Shanghai Jiao Tong University, Shanghai, China}

\icmlcorrespondingauthor{}{sunmingyang, wangdonglin@westlake.edu.cn}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Diffusion policies have shown promise in learning complex behaviors from demonstrations, particularly for tasks requiring precise control and long-term planning. However, they face challenges in robustness when encountering distribution shifts. This paper explores improving diffusion-based imitation learning models through online interactions with the environment. We propose OTPR (Optimal Transport-guided score-based diffusion Policy for Reinforcement learning fine-tuning), a novel method that integrates diffusion policies with RL using optimal transport theory. OTPR leverages the Q-function as a transport cost and views the policy as an optimal transport map, enabling efficient and stable fine-tuning. Moreover, we introduce masked optimal transport to guide state-action matching using expert keypoints and a compatibility-based resampling strategy to enhance training stability. Experiments on three simulation tasks demonstrate OTPR's superior performance and robustness compared to existing methods, especially in complex and sparse-reward environments. In sum, OTPR provides an effective framework for combining IL and RL, achieving versatile and reliable policy learning. The code will be released at \url{https://github.com/Sunmmyy/OTPR.git}.

\end{abstract}

\section{Introduction}
Robotic manipulation is an intricate endeavor, where the delicate interplay of long-term planning and instantaneous control poses a captivating challenge - the quest to develop policies that can seamlessly navigate this balance lies at the forefront of modern robotics~\cite{DBLP:journals/corr/abs-2305-12821,DBLP:conf/nips/MuLXYLLTHJ021,chen2023diffusion}. Tasks demand not only the ability to execute complex sequences but also the adaptability to handle uncertainties and disturbances. Imitation Learning (IL) has emerged as a popular data-driven approach for training robots by imitating demonstration data, with advancements of Behavior Cloning (BC) like diffusion models \cite{dp, ajayconditional} and action chunking \cite{DBLP:conf/rss/ZhaoKLF23} enhancing its ability to learn complex, long-horizon behaviors. 
Notably, Diffusion Policy (DP)~\cite{dp} has shown promise due to the capacity to handle multi-modal action distributions, excel in high-dimensional spaces, and achieve stable training through techniques like denoising and score matching.
% Its efficiency in real-time control further highlights its potential for responsive tasks.  
However, these advancements still fail to address the fundamental flaws of BC, which remains highly susceptible to distributional shifts, where the policy encounters states outside its training data, leading to compounding errors~\cite{ross2010efficient}. 
% The open-loop nature of DP also renders it incapable of reacting to noise and disturbances, making it unreliable in dynamic or uncertain environments. 
% Moreover, BC is hampered by biases in demonstration datasets, overfitting, generalization issues, and occasional training instability.However, despite these advancements, BC methods, including Diffusion Policy, face persistent limitations. Vulnerability to distributional shifts often results in cascading errors when encountering states outside the training data, while their open-loop execution limits responsiveness to noise and disturbances. Furthermore, challenges such as dataset biases, overfitting, and generalization issues—especially in dynamic environments—continue to hinder BC’s scalability and reliability in real-world applications.
% These limitations emphasize the urgent need for new approaches that can transcend BC’s shortcomings and achieve robust, adaptable performance in complex real-world scenarios.


Reinforcement Learning (RL) offers a powerful framework for autonomous learning through trial-and-error interactions guided by reward signals, making it particularly effective in training reactive controllers that adapt to noise, disturbances, and unforeseen states~\cite{kober2013reinforcement}. 
RL learns corrective behaviors directly from experience, enabling policies to recover from errors and handle states beyond the training distribution. Its ability to optimize over long time horizons can also refine action sequences, enhancing robustness and precision.
Unlike IL, which benefits from leveraging demonstration data to jump start learning, RL enhances generalization by exploring diverse scenarios and adapting dynamically to environmental changes.
% By learning corrective behaviors directly from experience, RL enables policies to recover from errors and navigate states that deviate from the training distribution. Furthermore, RL's ability to optimize over long time horizons can complement DP's open-loop execution by refining action sequences for improved robustness and precision.  
However, RL also faces significant challenges, including the need for carefully designed reward functions and  vast interaction data, which is costly to collect, particularly in real-world settings~\cite{DBLP:conf/icml/ParkM024}.

These strengths and weaknesses suggest that an integrated approach, combining RL’s adaptability with DP’s demonstration-driven learning, holds promise for achieving reliable, scalable, and versatile robotic manipulation. 
% While combining BC and RL has a long history, 
% The most straightforward way to use demonstration data in RL is to initialize the RL replay buffer with demonstrations and oversample those demonstrations during training. This approach does not leverage the fact that IL policies trained on the demonstrations can indeed provide more useful information – they can output actions that may not be good enough to solve unseen scenarios, but can still provide some “lower bound” on the action quality when the initial RL actions are highly suboptimal.
The most common approach is to pretrain a imitation policy with human data and then finetune it with RL~\cite{DBLP:conf/iclr/BlackJDKL24}. Some methods apply additional regularization~\cite{DBLP:conf/rss/RajeswaranKGVST18} or seperated policy network~\cite{ankile2024imitation} to ensure that the knowledge from demonstrations does not get washed out quickly by the randomly initialized critics, which may suffer from hyper-parameter tuning issue. 
Additionally, the structure of diffusion models (iterative refinement) inherently complicate the application of standard RL algorithms, often leading to low-efficiency, instability or requiring significant architectural modifications~\cite{dppo,mark2024policy}. 
% , the recent advancements in BC architectures present new challenges for fine-tuning the learned policy with RL.



% We present a way to leverage the strengths of BC and RL that reduces human effort in obtaining reliable policies by building on recent advances in BC methods and simto-real RL training.
% RL augments the BC-learned planner with a closed-loop
% controller capable of fine-grained corrections that mitigate
% the limitations of open-loop action chunk execution and
% collects on-policy data to ensure reliable plan execution.
% By utilizing the Q-function as a transport cost and the policy as an optimal transport map, we formalize policy learning as a Maximin OT optimization problem.

In this paper, we integrate insights from the optimal transport theory~\cite{gu2023optimal,montesuma2024recent} to refine the diffusion policy optimization process, leveraging knowledge gained from expert trajectories to improve learning efficiency and policy performance in subsequent reinforcement learning tasks. 
By utilizing the Q-function as a transport cost and viewing the policy as an optimal transport map, we establish an equivalent relationship between the optimal transport map and the optimal policy, which opens avenues for applying recent advantages of RL to diffusion policy directly. 
Our key contributions are as follows: 
\begin{itemize}
    \item We proposed an \textbf{O}ptimal \textbf{T}ransport guided score-based diffusion \textbf{P}olicy for \textbf{R}einforcement Learning fine-tuning (\textbf{OTPR}), which is the first work to systematically combine optimal transport theory with diffusion policies for reinforcement learning fine-tuning. OTPR's core lies in solving the $L_2$-regularized OT dual problem, thereby deriving a compatibility function that establishes a soft coupling relationship between states and actions, effectively integrating imitation learning and reinforcement learning.
    % By employing a dual stochastic gradient algorithm to solve $L_2$-regularized optimal transport, a compatibility function is derived, establishing a soft coupling relationship between states and actions. 
    
    \item To enhance efficiency and accuracy, we introduce the \textbf{Masked Optimal Transport} to leverage the paired state-action from expert data as keypoints to guide the matching of the other state-action points from replay buffer, which seamlessly integrates imitation learning and reinforcement learning objective. 
    \item To address the sub-optimal performance of the conditional score-based model when trained with standard algorithms on mini-batch data, we introduce a \textbf{Compatibility-based Resampling} strategy to selects action with high compatibility scores to guide the training process, thereby enhancing  performance.
\end{itemize}
% Our main contribution is: 
We conduct extensive experiments on 3 simulation tasks spanning various difficulty levels. The results demonstrate that OTPR consistently matches or outperforms existing state-of-the-art methods in all tasks, with particularly notable improvements in more challenging scenarios.

\begin{figure*}[ht] 
\vskip 0.1in
    \centering
    \includegraphics[width=\textwidth]{figures/overview.pdf} 
    \caption{\textbf{Overview of OTPR.} (a) Estimation Optimal Transport Plan: A stochastic dual approach with two parametrized dual variables is introduced to estimate the optimal transport plan with $Q$-cost from state distribution and action distribution.  (b) Training: OTPR pre-trains a diffusion model from the expert’s data. It then iteratively performs RL to optimize a $Q$-function and trains diffusion models by score matching. (c) Online Interaction: In the inference step, the policy makes action inference by iteratively denoising a random noise, conditioned on current state. OTPR then employs the compatibility function $H$ to reweight each action before resampling one.} 
    \label{fig:overview}
    \vskip -0.2in
\end{figure*}
% \begin{figure*}[ht]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=\columnwidth]{figures/overview.pdf}}
% \caption{xxx}
% \label{fig:overview}
% \end{center}
% \vskip -0.2in
% \end{figure*}

\section{Related Work}

% \textbf{Imitation learning from demo.} 

\textbf{Diffusion based policies.} 
Diffusion-based policies have shown recent success in robotics and decision-making applications. In a pioneering work, “Diffuser”~\cite{janner2022planning}, a planning algorithm
with diffusion models for offline reinforcement learning.
This framework is extended to other tasks in the context of
offline reinforcement learning~\cite{wang2022diffusion}, where the
training dataset includes reward values.
% For example, Ajay
% et al. (2022) propose to model policies as conditional diffusion models. 
Most typically, diffusion based policies are trained from human demonstrations through a supervised objective, and enjoy both high training stability and strong performance in modeling complex and multi-modal trajectory distributions. 
The application of DDPM~\cite{ddpm} and DDIM~\cite{ddim} on visuomotor policy learning for physical robots~\cite{dp} outperforms counterparts like Behavioral Cloning.
While these techniques effectively learn
from multi-modal data, they often create models that are non-trivial to fine-tune using RL. Even if they were compatible with RL, the fine-tuning process can be computationally prohibitive due to the large number of parameters in modern policy models. 

\textbf{Training diffusion models with reinforcement learning.} As demonstration data are often limited, there have been many approaches proposed to improve the performance of diffusion-based policies. 
% One popular approach has been to guide the diffusion denoising process using objectives such as reward signal or goal conditioning. 
% More recent work has explored the possibilities of techniques including Q-learning and weighted regression, either from purely offline estimation or with online interaction.
One straightforward approach \cite{DBLP:conf/iclr/BlackJDKL24, fan2024reinforcement} involves framing diffusion denoising as a Markov Decision Process (MDP), which facilitates preference-aligned generation with policy gradient reinforcement learning. However, this approach often suffers from instability, limiting its practical applicability. \cite{dppo} introduced policy gradient loss on a two-layer MDP for direct diffusion policy fine-tuning, which mitigates this instability, but the method is architecture-specific and does not introduce closed-loop control. 
Alternative approaches to integrating diffusion architectures with reinforcement learning (RL) include leveraging Q-function-based importance sampling~\cite{idql}, employing advantage-weighted regression~\cite{goo2022know}, or reformulating the objective as a supervised learning problem with return conditioning~\cite{chen2021decision,janner2022planning,ajayconditional}.
Additionally, researchers have explored enhancing the denoising training objective by incorporating Q-function maximization~\cite{wang2022diffusion} and iteratively refining the dataset using Q-functions~\cite{yang2023policy}. 
Another promising direction involves augmenting a frozen, chunked diffusion policy model with a residual policy trained through online RL, enabling improved performance without modifying the pre-trained diffusion model~\cite{ankile2024imitation}.



% Recent work developed techniques for training diffusion policies from scratch~\cite{li2024learning}, leveraging unsupervised clustering and Q-learning ensembles to encourage multi-modal behavior discovery. 




\section{Background}


In this section, we offer fundamental definitions and theories to lay the groundwork for our framework, which will be thoroughly analyzed afterwards.  

\subsection{Optimal Transport}
% Optimal transport theory has been continuously developed since Monge first presented this problem \cite{monge} and is currently in connection with Riemannian geometry, partial differential equations, gradient flow, etc. 
Given two probability spaces \( (\mathcal{X}, \mu) \), \( (\mathcal{Y}, \nu) \) and a cost function \( c : \mathcal{X} \times \mathcal{Y} \rightarrow \mathbb{R}\), the Monge problem~\cite{villani2009optimal} is solving optimal map \( \mathcal{T} : \mathcal{X} \rightarrow \mathcal{Y} \) such that  
\begin{equation}  \label{eq:mon}
    \inf \left\{ M(\mathcal{T}) := \mathbb{E}_{\boldsymbol{x}\sim\mu} \left[c(\boldsymbol{x}, \mathcal{T}(\boldsymbol{x}))\right] \; \Big| \; \mathcal{T}_\#\mu = \nu \right\}
\end{equation}  
where the random variables $\boldsymbol{x} \sim \mu$ and \( \mathcal{T}_\#\mu \) is push forward of \( \mu \) subject to \( (\mathcal{T}_\#\mu)(\mathcal{Y}') := \mu(\mathcal{T}^{-1}(\mathcal{Y}')) \) for any measurable set \( \mathcal{Y}' \subset \mathcal{Y} \). Instead of finding the map \( \mathcal{T} \) in the original Monge problem, the relaxed Kantorovich optimal scheme \( K(\gamma) \) is obtained by \( \gamma \) realizing  
\begin{equation} \label{eq:kav} 
    \inf \left\{ K(\gamma) := \mathbb{E}_{\boldsymbol{x} \times \boldsymbol{y}\sim\gamma} [c(\boldsymbol{x}, \boldsymbol{y})] \; \Big| \; \gamma \in \Gamma(\mu, \nu) \right\},
\end{equation}  
where \( \Gamma(\mu, \nu) \) is the space composed of all joint probability measures \( \gamma \) on \( \mathcal{X} \times \mathcal{Y} \) with marginals \( \mu \) and \( \nu \). 

% \textbf{Unsupervised OT.} We consider a source distribution \( p \) and a target distribution \( q \). The unsupervised OT aims to find the optimal coupling/transport plan \( \pi \), i.e., a joint distribution of \( p \) and \( q \), such that the transport cost is minimized, formulated as the following optimization problem:  

% \[  
% \min_{\pi \in \Gamma} \mathbb{E}_{(x,y) \sim \pi} \, c(x,y), \quad \text{s.t. } \Gamma = \{ \pi : T^\#_x \pi = p, T^\#_y \pi = q \},  
% \]  

% where \( c \) is the cost function. \( T^\#_x \pi \) is the marginal distribution of \( \pi \) w.r.t. random variable \( x \). \( T^\#_x \pi = p \) means \( \int \pi(x,y) \, dy = p(x), \, \forall x \in \mathcal{X} \). Similarly, \( T^\#_y \pi = q \) indicates \( \int \pi(x,y) \, dx = q(y), \, \forall y \in \mathcal{Y} \).


\textbf{Regularized OT} Regularization was introduced in \cite{cuturi2013sinkhorn} to speed up the computation of OT problem, which is achieved by incorporating a negative-entropy penalty \( R \) to the primal variable \( \gamma \) of Problem~\ref{eq:kav},  
\begin{equation} \label{eq:kavR} 
    \inf \left\{ K_\lambda(\gamma) := \mathbb{E}_{\boldsymbol{x} \times \boldsymbol{y}\sim\gamma} [c(\boldsymbol{x}, \boldsymbol{y})] + \lambda R(\gamma) \; \Big| \; \gamma \in \Gamma(\mu, \nu) \right\},
\end{equation} 
% Besides efficient computation through the Sinkhorn algorithm, regularization also makes the OT distance differentiable everywhere w.r.t. the weights of the input measures (Blondel et al., 2018), whereas OT is differentiable only almost everywhere.
As highlighted by~\cite{daniels2021score}, adding a regularization term with $\alpha$-strong convexity (such as entropy or squared \( L_2 \) norm) to the problem~\ref{eq:kavR} is a sufficient condition for $\lambda\alpha$-strong convexity of $K_\lambda(\gamma)$ in $L_1$-norm, which makes the dual problem an unconstrained maximization problem. 

In this work, we consider the \( L_2 \) regularization introduced by \cite{dessein2018regularized}, whose computation is found to be more stable since there is no exponential term causing overflow. For all $x \in \mathcal{X} \ \text{and} \ y \in \mathcal{Y}$, 
\begin{equation}
R_{L^2}(\gamma) \overset{\text{def.}}{=} \int_{\mathcal{X} \times \mathcal{Y}} \left( \frac{\mathrm{d}\gamma(x,y)}{\mathrm{d}\mu(x) \mathrm{d}\nu(y)} \right)^{2} \mathrm{d}\mu(x) \mathrm{d}\nu(y).
\end{equation}
where $\frac{\mathrm{d}\gamma(x,y)}{\mathrm{d}\mu(x) \mathrm{d}\nu(y)}$ is the density, i.e., the Radon-Nikodym derivative of $ \gamma \text{ w.r.t. } \mu \times \nu$.

% We use this dual formulation in the next section to propose an efficient stochastic gradient algorithm.

\textbf{Regularized OT Dual} We refer to the objective $K_\lambda(\gamma)$ as the primal objective, and we will use $\mathcal{J}_{\lambda}(u,v)$ to refer to the associated dual objective, with dual variables $u,v$. The dual of the regularized OT problems
can be obtained through the Fenchel-Rockafellar’s duality theorem,
% \begin{align}  
%     & \sup_{u,v} \mathbb{E}_{(X,Y) \sim \mu \times \nu} \left[ u(X) + v(Y) + F_{\varepsilon}(u(X), v(Y)) \right], \tag{6} \\
%     & \text{where } F_{\varepsilon}(u(x), v(y)) = \left\{   
%     \begin{array}{ll}   
%         -\varepsilon e^{-\frac{1}{\varepsilon}(u(x) + v(y) - c(x,y))} & \text{(entropy reg.)} \\   
%         -\frac{1}{4\varepsilon}(u(x) + v(y) - c(x,y))^{2}_{+} & \text{(L}^2 \text{ reg.)}   
%     \end{array}   
%     \right. \tag{7}  
% \end{align}
\begin{align}  \label{eq:kavRdual}
    & \sup_{u,v} \mathbb{E}_{(\boldsymbol{x},\boldsymbol{y}) \sim \mu \times \nu} \left[ u(\boldsymbol{x}) + v(\boldsymbol{y}) + F_{\lambda}(u(\boldsymbol{x}), v(\boldsymbol{y})) \right], \\
    & \text{where } F_{\lambda}(u(\boldsymbol{x}), v(\boldsymbol{y})) =  -\frac{1}{4\lambda}(u(\boldsymbol{x}) + v(\boldsymbol{y}) - c(\boldsymbol{x},\boldsymbol{y}))^{2}_{+} \notag
\end{align}
is concave w.r.t. $(u,v)$ and $a_{+} = \max(a, 0)$. In order to recover the solution \(\gamma_{\lambda}\) of the regularized primal problem~\ref{eq:kavR}, we can use the first-order optimality conditions of the Fenchel-Rockafellar's duality theorem,  
\begin{align}\label{eq:kavRplan}
& \mathrm{d}\gamma_{\lambda}(\boldsymbol{x}, \boldsymbol{y}) = H_{\lambda}(\boldsymbol{x}, \boldsymbol{y})\mathrm{d}\mu(\boldsymbol{x})\mathrm{d}\nu(\boldsymbol{y}) \\
&\text{where} \ H_{\lambda}(\boldsymbol{x}, \boldsymbol{y}) = \frac{1}{2\lambda}(u(\boldsymbol{x}) + v(\boldsymbol{y}) - c(\boldsymbol{x},\boldsymbol{y}))_{+}. \notag
\end{align}
$H$ is called compatibility function. 








\subsection{Reinfrocement Learning and Imitation Learning}\label{sec:ILRL}
\textbf{Reinfrocement Learning} We consider a standard Markov decision process (MDP) consisting of state space \( \boldsymbol{s} \in \mathcal{S} \), continuous action space \( \boldsymbol{a} \in \mathcal{A}\), deterministic state transition function \( \mathcal{P} : \mathcal{S} \times \mathcal{A} \to \mathcal{S} \), reward function \( r : \mathcal{S} \to \mathbb{R} \) and discount factor \( \kappa \). 
$\tau \sim \pi$ denotes the distribution of trajectory $ (s_0, a_0, s_1, a_1, \ldots)$ given the policy $\pi(\boldsymbol{a}|\boldsymbol{s})$.  
% The state-value function  $V_{\pi}(s) = \mathbb{E}_{\tau \sim \pi} \left[ \sum_{t=0}^{\infty} \kappa^t r_t | s_0 = s \right] $ represents the expected reward that the agent can obtain under the policy $\pi$ in the state $s$.
The action-state value function is $Q_{\pi}(s, a) = \mathbb{E}_{\tau \sim \pi} \left[ \sum_{t=0}^{\infty} \kappa^t r_t | a_0 = a, s_0 = s \right] $. 
The goal of RL is to learn the policy $\pi$ that maximizes the discounted expected cumulative reward over a trajectory $\tau$, defined as $\mathcal{J}_{\text{RL}}(\pi) = \mathbb{E}_{\tau\sim\pi}[\sum_{k=0}\kappa^kr_k]$.



\textbf{Imitation Learning} We assume access to a dataset \( \mathcal{D} \) of demonstrations collected by expert human operators (often assumed to be optimal). Each trajectory \( \tau \in \mathcal{D} \) consists of a sequence of transitions $\{(s_0, a_0), \ldots, (s_K, a_K)\}$.  
The most common IL method is behavior cloning (BC) which trains a parameterized policy \(\pi_{\theta}\) to minimize the negative log-likelihood of data, i.e.,  
$ L(\theta) = -\mathbb{E}_{(\boldsymbol{s},\boldsymbol{a}) \sim \mathcal{D}}[\log \pi_{\theta}(\boldsymbol{a}|\boldsymbol{s})]. $ 
In this work, we assume \(\pi_{\theta}\) follows an isotropic Gaussian as its action distribution for simplicity. 
% We note that our framework can easily accommodate more powerful IL methods such as BC-RNN with a Gaussian mixture model \cite{21}. 
With the isotropic assumption, the BC training objective can be formulated as the following squared loss: $L^{IL}(\theta) = \mathbb{E}_{(\boldsymbol{s},\boldsymbol{a}) \sim \mathcal{D}} \left\| \pi_{\theta}(\boldsymbol{s}) - \boldsymbol{a} \right\|_2^2$. 

% \textbf{Reinforcement Learning.} 
% Our method builds on value based RL methods as they can easily consume demonstration data generated by humans. 
% Deep RL methods for continuous action spaces jointly learn a policy (actor) \( \pi_{\theta} \) and one or multiple value functions (critic) \( Q_{\phi} \) parameterized by neural networks \( \theta \) and \( \phi \) respectively. The value functions \( Q_{\phi} \) are trained to minimize TD-error \( L(\phi) = [r_t + \gamma Q_{\phi}(s_{t+1}, \pi_{\theta}(s_{t+1})) - Q_{\phi}(s_t, a_t)]^2 \) while the policy is trained to output actions with high Q-values with \( L^{RL}(\theta) = -Q_{\phi}(s, \pi_{\theta}(s)) \). 
% *******************
% \( \pi_{\theta} \) and \( Q_{\phi}' \) are target networks whose parameters \( \theta' \), \( \phi' \) are exponential moving averages of \( \theta, \phi \) respectively.  

\subsection{Conditional Score Based Diffusion Policy}~\label{sec:CSDP}
The conditional Score Based Diffusion Models (SBDMs) \cite{song2021maximum,batzolis2021conditional} aim to generate a target sample $\boldsymbol{y}$ from the distribution $\mu$ of target training data given a condition data $\boldsymbol{x}$. 
% For the labeled dataset, each target training sample $\boldsymbol{y}$ is paired with a condition data $\boldsymbol{x}_{\text{cond}}(\boldsymbol{y})$. 
In imitation learning, diffusion policy regard state $\boldsymbol{s}$ as condition $\boldsymbol{x}$ and use a forward stochastic differential equation (SDE) to add Gaussian noises to the target training data $\boldsymbol{a}$ for training the conditional score-based model. The forward SDE is \( \mathrm{d}\boldsymbol{a}_t = f(\boldsymbol{a}_t, t) \mathrm{d}t + g(t) \mathrm{d}\mathbf{w} \) with \( \boldsymbol{a}_0 \sim \nu \), where \( \mathbf{w} \in \mathbb{R}^D \) is a standard Wiener process, \( f(\cdot, t) : \mathbb{R}^D \to \mathbb{R}^D \) is the drift coefficient, and \(g(t) \in \mathbb{R}\) is the diffusion coefficient. 
Let \(\nu_{t|0}\) be the conditional distribution of \(\boldsymbol{a}_t\) given the initial state \(\boldsymbol{a}_0\), and \(\nu_t\) be the marginal distribution of \(\boldsymbol{a}_t\). 
% We can choose the \(f(y, t)\), \(g(t)\), and \(T\) such that \(y_t\) approaches some analytically tractable prior distribution \(p_{\text{prior}}(y_T)\) at time \(t=T\), i.e., \(p_T(y_T) \approx p_{\text{prior}}(y_T)\). 
% We take \(f, g, T\), and \(p_{\text{prior}}\) from two popular SDEs, i.e., VE-SDE and VP-SDE \cite{3} (please refer to Appendix A for model details). 
The conditional score-based model is trained by denoising score-matching loss:  
\begin{align}\label{eq:DSM}
\mathcal{J}_{\text{DSM}}&(\theta) = \mathbb{E}_t w_t \mathbb{E}_{\boldsymbol{a}_0 \sim \nu} \mathbb{E}_{\boldsymbol{a}_t \sim \nu_{t | 0} (\boldsymbol{a}_t | \boldsymbol{a}_0)}  \\ 
&\left\| s_{\theta}(\boldsymbol{a}_t; \boldsymbol{s}_{\text{cond}}(\boldsymbol{a}_0), t) - \nabla_{\boldsymbol{a}_t} \log \nu_{t|0}(\boldsymbol{a}_t | \boldsymbol{a}_0) \right\|^2_2, \notag
\end{align}

where \(w_t\) is the weight for time \(t\). In this paper, \(t\) is uniformly sampled from \([0, T]\), i.e., \(t \sim U([0, T])\). With the trained \(s_{\hat{\theta}}(\boldsymbol{a}; \boldsymbol{s}, t)\), given a condition data \(\boldsymbol{s}\), the target sample \(\boldsymbol{a}_0\) is generated by the reverse SDE as \(\mathrm{d}\boldsymbol{a}_t = [f(\boldsymbol{a}_t, t) - g(t)^2 s_{\hat{\theta}}(\boldsymbol{a}_t; \boldsymbol{s}, t)] \mathrm{d}t + g(t) \mathrm{d}\bar{\mathbf{w}}\), where \(\bar{\mathbf{w}}\) is a standard Wiener process in the reverse-time direction. 
% This process starts from a noise sample \(y_T\) and ends at \(t = 0\).


\section{Method}

% In this section, we introduce our method, OTPR. 

\subsection{An Optimal Transport View of Policy Learning}
We approach the policy optimization problem from the perspective of optimal transport. Considering Eq.~\ref{eq:mon}, by substituting the cost function \(c(\boldsymbol{x}, \boldsymbol{a})\) with the critic \(Q(\boldsymbol{s}, \boldsymbol{a})\) and viewing our policy \(\pi\) as a map that moves mass from the state distribution \(\mu(\boldsymbol{s})\), to the corresponding distribution of actions $\nu(\boldsymbol{a})$ given by an optimal behavior policy \(\pi^\beta(\cdot|\boldsymbol{s})\), we formulate the following primal state-conditioned Monge OT problem:  
\begin{equation}  \label{eq:monRL}
    \inf \left\{ M(\mathcal{\pi}) := \mathbb{E}_{\boldsymbol{s}\sim\mu} \left[-Q^\beta(\boldsymbol{s}, \mathcal{\pi}(\boldsymbol{s}))\right] \; \Big| \; \mathcal{\pi}_\#\mu = \nu \right\}
\end{equation}  


The objective is to minimize the expectation of the negative critic function \(Q^{\beta}\) while mapping exclusively to the distribution of actions given by the behavior policy \(\pi^\beta\), a concept also explored in offline RL work~\cite{asadulaev2025rethinking}.


\begin{proposition}\label{propOTRL}
Given an optimal behavior policy $\pi^\beta$ and a critic-based cost function $c = -Q^{\beta}$, let $\pi^*$ is the solution to Eq.~\ref{eq:monRL} with the $Q^{\beta}$ cost function. Then it holds that: $\mathcal{J}_{\text{RL}}(\pi^*) = \mathcal{J}_{\text{RL}}(\pi^\beta)$.
\end{proposition}





The proof is given in Appendix~\ref{app:ProofP1}. Proposition~\ref{propOTRL} offers valuable insights into the connection between optimal transport theory and RL by establishing an equivalent relationship between the optimal transport map and the optimal policy. Meanwhile, given the paired state-action data derived from an expert policy, the IL problem can be reframed as achieving  a conditional optimal transport map (i.e., diffusion policy). This link indicates that the transformations defined by the optimal transport map can effectively integrate RL with IL. We will next demonstrate how to use an estimated optimal transport plan (as intuitively illustrated in Fig.~\ref{fig:toys} of Appendix) to serve as a guide to utilize reinforcement learning to optimize the pre-trained diffusion policy via imitation learning.

% The paired state-action data in imitation learning originates from an expert policy
% According to the setting of Sec~\ref{sec:ILRL}, the paired state-action data in imitation learning originates from an expert policy, which is assumed to be the optimal policy. Thus, the imitation learning problem can be replaced by the estimation problem of the optimal transmission map described in Eq.~\ref{eq:monRL}.

\subsection{OT-Guided Conditional Denoising Score Matching}
% We provide a reformulation for the conditional score-based model with paired training dataset discussed in Sect. 2.1, and this formulation motivates us to extend the conditional SBDM to unpaired and partially paired settings in Sect. 3.2. 
% Let \( q \) and \( p \) respectively denote the distributions of target data and condition data. 
In IL setting, we denote the condition data as \( \boldsymbol{s}_{\text{cond}}(\boldsymbol{a}) \) for a target action \( \boldsymbol{a} \), and \( \mu \) is the measure by push-forwarding \( \nu \) using \( \boldsymbol{s}_{\text{cond}} \), i.e.,  $  
\mu(\boldsymbol{s}) = \sum_{\{\boldsymbol{a} : \boldsymbol{s}_{\text{cond}}(\boldsymbol{a}) = \boldsymbol{s}\}} \nu(\boldsymbol{a})  $
over the paired training dataset $\mathcal{D}$. Section~\ref{sec:CSDP} provides a explicit reformulation for the conditional score-based diffusion policy with the paired training data.

\begin{proposition} \label{prop2}
Let \( \mathcal{C}(\boldsymbol{s}, \boldsymbol{a}) = \frac{1}{\mu(\boldsymbol{s})} \delta(\boldsymbol{s} - \boldsymbol{s}_{\text{cond}}(\boldsymbol{a})) \) where \( \delta \) is the Dirac delta function, then \( \mathcal{J}_{\text{DSM}}(\theta) \) in Eq.~\ref{eq:DSM} can be reformulated as  
\begin{align}\label{eq:DSMCC}
\mathcal{J}_{\text{CDSM}}(\theta) = & \mathbb{E}_{t} w_{t} \mathbb{E}_{\boldsymbol{s} \sim \mu} \mathbb{E}_{\boldsymbol{a} \sim \nu} \mathcal{C}(\boldsymbol{s},\boldsymbol{a}) \mathbb{E}_{\boldsymbol{a}_{t} \sim \nu_{t}|0}(\boldsymbol{a}_{t}|\boldsymbol{a}) \notag \\
&\left\| s_{\theta}(\boldsymbol{a}_{t}; \boldsymbol{s}, t) - \nabla_{\boldsymbol{a}_{t}} \log \nu_{t|0}(\boldsymbol{a}_{t}|\boldsymbol{a}) \right\|^{2}_{2}. 
\end{align}
Furthermore, \( \upsilon(\boldsymbol{s},\boldsymbol{a}) = \mathcal{C}(\boldsymbol{s},\boldsymbol{a}) \mu(\boldsymbol{s}) \nu(\boldsymbol{a}) \) is a joint distribution for marginal distributions \( \mu \) and \( \nu \).  
\end{proposition} 


The proof is given in Appendix~\ref{app:ProofP2}. In Proposition~\ref{prop2}, the coupling relationship of condition state and target action is explicitly modeled in \( \mathcal{C}(\boldsymbol{s},\boldsymbol{a}) \). 
Nevertheless, in contrast to IL, the definition of \( \mathcal{C}(\boldsymbol{s},\boldsymbol{a}) \) in RL is not explicit due to the absence of an optimal paired relationship between \( \boldsymbol{s}, \boldsymbol{a} \).
Fortunately, the joint distribution \( \upsilon \) exhibits a similar formulation to the transport plan \( \gamma \) in Eq.~\ref{eq:kavRplan}. We therefore use \( L_2 \)-regularized OT to model the coupling relationship between state \( \boldsymbol{s} \) and action \( \boldsymbol{a} \) for unpaired settings. Specifically, given a $Q$ network learnt by RL, the \( L_2 \)-regularized OT is applied to the distributions \( \mu, \nu \) to approximately construct a conditional transport plan $\hat{\gamma}(\boldsymbol{a}|\boldsymbol{s}) = H(\boldsymbol{s}, \boldsymbol{a})\nu(\boldsymbol{a})$, and the coupling relationship of the condition data \( \boldsymbol{s} \) and target data \( \boldsymbol{a} \) is built by the compatibility function \( H(\boldsymbol{s},\boldsymbol{a}) \). We then extend the formulation for paired setting in Eq.~\ref{eq:DSMCC} by replacing \( \mathcal{C} \) with \( H \) to develop the training objective for unpaired setting, which is given by  
\begin{align}\label{eq:HDSM}
\mathcal{J}_{\text{HDSM}}(\theta) = & \mathbb{E}_{t} w_{t} \mathbb{E}_{\boldsymbol{s} \sim \mu} \mathbb{E}_{\boldsymbol{a} \sim \nu} H(\boldsymbol{s},\boldsymbol{a}) \mathbb{E}_{\boldsymbol{a}_{t} \sim \nu_{t}|0}(\boldsymbol{a}_{t}|\boldsymbol{a}) \notag \\
&\left\| s_{\theta}(\boldsymbol{a}_{t}; \boldsymbol{s}, t) - \nabla_{\boldsymbol{a}_{t}} \log \nu_{t|0}(\boldsymbol{a}_{t}|\boldsymbol{a}) \right\|^{2}_{2}. 
\end{align}

% Equation~\ref{eq:HDSM} is dubbed ``OT-guided conditional denoising score matching''.
In Eq.~\ref{eq:HDSM}, \( H \) is a ``soft" coupling relationship of state data and action data, because there may exist multiple \( a \) satisfying \( H(\boldsymbol{s},\boldsymbol{a}) > 0 \) for each \( s \). 
% While Eq. (8) assumes "hard" coupling relationship, i.e., there is only one condition data \( x \) for each \( y \) satisfying \( C(x,y) > 0 \). 
We minimize \( \mathcal{J}_{\text{HDSM}}(\theta) \) to train the conditional score-based model \( s_{\theta}(\boldsymbol{a}_t; \boldsymbol{s}, t) \). 
% We will theoretically analyze that our formulation in Eq. (9) is still a diffusion model in Sect. 


\begin{theorem}\label{theorem1}
  For \( \boldsymbol{s} \sim \mu \), consider the forward SDE   
\( \mathrm{d}\boldsymbol{a}_t = f(\boldsymbol{a}_t, t) \mathrm{d}t + g(t) \mathrm{d}\mathbf{w} \) with \( \boldsymbol{a}_0 \sim \hat{\gamma}(\cdot | \boldsymbol{s}) \) and \( t \in [0, T] \). Let \( \nu_t(\boldsymbol{a}_t|\boldsymbol{s}) \) be the distribution of \( \boldsymbol{a}_t \) and   
\(\mathcal{J}_{\text{CSM}}(\theta) = \mathbb{E}_t w_t \mathbb{E}_{\boldsymbol{s} \sim \mu} \mathbb{E}_{\boldsymbol{a}_t \sim \nu_t(\boldsymbol{a}_t|\boldsymbol{s})} \| s_\theta(\boldsymbol{a}_t; \boldsymbol{s}, t) - \nabla_{\boldsymbol{a}_t} \log \nu_t(\boldsymbol{a}_t|\boldsymbol{s}) \|^2_2,  \)
then we have  \(\nabla_\theta \mathcal{J}_{\text{HDSM}}(\theta) = \nabla_\theta \mathcal{J}_{\text{CSM}}(\theta).  \)
\end{theorem} 
We give the proof in Appendix~\ref{app:ProofT1}. Theorem~\ref{theorem1} indicates that the trained \( s_\theta(\boldsymbol{a}_t; \boldsymbol{s}, t) \) using Eq.~\ref{eq:HDSM} approximates \( \nabla_{\boldsymbol{a}_t} \log \nu_t(\boldsymbol{a}_t|\boldsymbol{s}) \). Based on this, we can interpret our approach as follows. Given a condition data \( \boldsymbol{s} \), we sample action data \( \boldsymbol{a}_0 \) from the conditional transport plan \( \hat{\gamma}(\boldsymbol{a}_0|\boldsymbol{s}) \), produce \( \boldsymbol{a}_t \) by the forward SDE solver (examples given in Appendix~\ref{app:TFNoise}), and train \( s_\theta(\boldsymbol{a}_t; \boldsymbol{s}, t) \) to approximate \( \nabla_{\boldsymbol{a}_t} \log \nu_t(\boldsymbol{a}_t|\boldsymbol{s}) \). 

\textbf{Sample Generation} We denote the trained conditional score-based model as $s_{\hat{\theta}}(\boldsymbol{a};\boldsymbol{s}, t)$ where $\hat{\theta}$ is the value of $\theta$ after training. Given the condition state $\boldsymbol{s}$, we generate action samples by the following SDE:
\begin{equation}\label{eq:sampleactionSDE}
    \mathrm{d}\boldsymbol{a}_t = \left[f(\boldsymbol{a})_t, t) - g(t)^2 s_{\hat{\theta}}(\boldsymbol{a};\boldsymbol{s}, t)\right]\mathrm{d}t + g(t)\mathrm{d}\bar{\mathbf{w}}.
\end{equation}
Numerical solvers such as the Euler-Maruyama method, DDIM, or DPM-Solver can be employed to efficiently solve this reverse SDE, enabling the generation of high-quality action samples~\cite{ddim, lu2022dpm}.


\subsection{Expert Data Masked Optimal Transport}
For the computation of $H$, a value-based reinforcement learning can provide an estimated Q-network, while optimizing the optimal tranport problem gives $u, v$, which is often computationally challenging because OT needs transport all the mass of state to exactly match the mass of action distribution, which presents computational challenges.
Fortunately, in imitation learning, expert demonstrations $\mathcal{D}^\beta$ have provide matched pairs of state and action data points (called ``keypoints'') \( \mathcal{K} = \{(\boldsymbol{s}_i, \boldsymbol{a}_i)\}_{i=1}^{N} \). 
These keypoints are not only valuable but also crucial for investigating how to leverage them to guide the correct matching in OT. 
% It is reasonable to annotate some paired keypoints across state and action spaces for guiding the matching in OT. 
We introduce masked OT~\cite{gu2022keypoint} to leverage the given matched keypoints to guide the correct transport in OT by preserving the relation of each data point to the keypoints:
\begin{align}\label{eq:kavmask}
&\inf \left\{ K(\tilde{\gamma}) := \mathbb{E}_{\boldsymbol{s} \times \boldsymbol{a}\sim\tilde{\gamma}} [g(\boldsymbol{s}, \boldsymbol{a})] \; \Big| \; \tilde{\gamma} \in \tilde{\Gamma}(\mu, \nu; \mathbf{m}) \right\}
    % &\min_{\tilde{\gamma} \in {\tilde{\Gamma}}} \mathbb{E}_{(\boldsymbol{s},\boldsymbol{a}) \sim m \otimes \tilde{\gamma}} g(x,y),\\ 
    % &s.t. \quad \tilde{\Gamma}= \{ \hat{\pi} : T_x^\#(m \otimes \hat{\pi}) = p, T_y^\#(m \otimes \hat{\pi}) = q \} \notag 
\end{align}
where the transport plan \( \mathbf{m} \odot \tilde{\gamma} \) is \( (\mathbf{m} \odot \tilde{\gamma})(\boldsymbol{s},\boldsymbol{a}) = \mathbf{m}(s,a)\tilde{\gamma}(\boldsymbol{s},\boldsymbol{a}) \), and \( \mathbf{m} \) is a binary mask function. Given a pair of keypoints \( (\boldsymbol{s}_i, \boldsymbol{a}_i) \in \mathcal{K} \), then \( \mathbf{m}(\boldsymbol{s}_{i}, \boldsymbol{a}_{i}) = 1, \mathbf{m}(\boldsymbol{s}_{i}, \boldsymbol{a}) = 0 \)  and \( \mathbf{m}(\boldsymbol{s},\boldsymbol{a}) = 1 \) if \( \boldsymbol{s}, \boldsymbol{a} \) do not coincide with any keypoint. 
The mask-based modeling of the transport plan ensures that the keypoint pairs are always matched in the derived transport plan. \( g \) in Eq.~\ref{eq:kavmask} is defined as \( g(\boldsymbol{s},\boldsymbol{a}) = d(R_{\boldsymbol{s}}^s, R_{\boldsymbol{a}}^t) \), where \( R_{\boldsymbol{s}}^s, R_{\boldsymbol{a}}^t \in (0, 1)^N \) model the vector of relation of \( \boldsymbol{s}, \boldsymbol{a} \) to each of the paired keypoints in state and action space respectively, and \( d \) is the Jensen–Shannon divergence. The \( i \)-th elements of \( R_{\boldsymbol{s}} \) and \( R_{\boldsymbol{a}} \) are respectively defined by  
\begin{equation}
\left\{
\begin{aligned}
R_{\boldsymbol{s},i}^s = \frac{\exp(-c(\boldsymbol{s},s_k)/\rho)}{\sum_{j=1}^{N} \exp(-c(\boldsymbol{s},s_j)/\rho)}, \\
R_{\boldsymbol{a},i}^t = \frac{\exp(-c(\boldsymbol{a},a_k)/\rho)}{\sum_{j=1}^{N} \exp(-c(\boldsymbol{a},a_j)/\rho)},  
\end{aligned}
\right.
\end{equation}
where $\rho$ is a commonly used temperature in the softmax function. Further, the masked matrix is introduced into the duality of the $L^2$ reguarized OT problem, and the penalty term $F_{\lambda}$ is updated as:
\begin{align}  \label{eq:kavRdualK}
    & \sup_{u,v} \mathbb{E}_{(\boldsymbol{s},\boldsymbol{a}) \sim \mu \times \nu} \left[ u(\boldsymbol{s}) + v(\boldsymbol{a}) + \tilde{F}_{\lambda}(u(\boldsymbol{s}), v(\boldsymbol{a})) \right], \\
    &\tilde{F}_{\lambda}(u(\boldsymbol{s}), v(\boldsymbol{a})) =  -\frac{1}{4\lambda}\mathbf{m}(\boldsymbol{s},\boldsymbol{a})(u(\boldsymbol{s}) + v(\boldsymbol{a}) - g(\boldsymbol{s},\boldsymbol{a}))^{2}_{+}. \notag
\end{align}
The dual~\ref{eq:kavRdual} and \ref{eq:kavRdualK} are unconstrained concave, which can be maximized through stochastic gradient methods by sampling batches from $\mu \times \nu$. Following~\cite{seguy2018large}, we use deep neural networks for their ability to approximate $u_\omega, v_\omega$ with the parameters $\omega$ and the estimate of OT plan is
\begin{align}\label{eq:kavRplanK}
& \hat{\gamma}(\boldsymbol{s}, \boldsymbol{a}) = H(\boldsymbol{s}, \boldsymbol{a})\mathrm{d}\mu(\boldsymbol{s})\mathrm{d}\nu(\boldsymbol{a}), \\
&\text{where} \ H(\boldsymbol{s}, \boldsymbol{a}) = \frac{1}{2\lambda}(u_\omega(\boldsymbol{s}) + v_\omega(\boldsymbol{a}) - g(\boldsymbol{s},\boldsymbol{a}))_{+}. \notag
\end{align}
The pseudo-codeis given in Appendix~\ref{app:alOT}.

\begin{algorithm}[tb]
   \caption{Online Score-Based Diffusion Policy Training}
   \label{alg:traingpolicybrief}
\begin{algorithmic}
   \STATE {\bfseries Input:} The pre-trained imitation policy, expert demonstrations $\mathcal{D}$, initialzed $Q$-network and replay buffer $\mathcal{B}$.
   \STATE {\bfseries Output: } Trained conditional score-based policy $s_\theta$.
   \FOR{iteration = $1, 2, \dots$}
   \STATE Learn $u_\omega, v_\omega$ by optimizing the dual problem~\ref{eq:kavRdualK}.
   \WHILE{ no done with episode}
   \STATE Observe current state $s$;
   \STATE Sample $a_l$ by the SDE~\ref{eq:sampleactionSDE}, $l = 1, \dots, L$;
   \STATE Compute $H(s, a_l)$ using Eq.~\ref{eq:kavRplanK};
   \STATE Normalize: $p_l = \frac{H(s,a_l)}{\sum_j H(s, a_j)}$;
   \STATE Select $a $ as a categorical from $p_l$;
   \STATE Store transition in $\mathcal{B}$.
   % \FOR{$i=1$ {\bfseries to} $m-1$}
   % \IF{$x_i > x_{i+1}$}
   % \STATE Swap $x_i$ and $x_{i+1}$
   % \STATE 
   % \ENDIF
   % \ENDFOR
   \ENDWHILE
   \STATE Learn $Q$-network by employed RL algorithm.
   \STATE Update $\theta$ of score model $s_\theta$ by fitting noise.
   \ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{OT-Guided Training}
To implement \( \mathcal{J}_{\text{HDSM}}(\theta) \) in Eq.~\ref{eq:HDSM} using training samples to optimize \( \theta \), we can sample mini-batch data from replay buffer, and then compute \( H(\boldsymbol{s}, \boldsymbol{a}) \) and \( \mathcal{J}_{\boldsymbol{s},\boldsymbol{a}} = \mathbb{E}_t w_t \mathbb{E}_{\boldsymbol{a}_t \sim \nu_{t|0}(\boldsymbol{a}_t|)} \| s_\theta(\boldsymbol{a}_t; \boldsymbol{s}, t) - \nabla_{\boldsymbol{a}_t} \log \nu_{t|0}(\boldsymbol{a}_t|\boldsymbol{a}) \|^2_2 \) over the pairs of \( (\boldsymbol{s}, \boldsymbol{a}) \) in \( \mathcal{S} \) and \( \mathcal{A} \). However, such a strategy is sub-optimal. This is because given a mini-batch of samples \( \boldsymbol{s} \) and \( \boldsymbol{a} \), for each source sample \( s \), there may not exist a target sample \( a \) in the mini-batch with a higher value of \( H(s,a) \) that matches condition data \( s \). Therefore, few or even no samples in a mini-batch contribute to the loss function, leading to a large bias of the computed loss and instability of the training. To tackle this challenge, we generate $L$ samples from policy model, and then use the compatibility function to reweight these actions, ultimately forming the intended policy when resampled. This approach is summarized in Algorithm~\ref{alg:traingpolicybrief}. 
% \textbf{Resampling-by-compatibility.} To implement the loss in Eq. (9), we perform the following steps:  
% \begin{itemize}  
%     \item Sample \( x \) from \( p \) and sample \( Y_x = \{y^l\}_{l=1}^L \) from \( q \);  
%     \item Resample a \( y \) from \( Y_x \) with the probability proportional to \( H(x, y^l) \);  
%     \item Compute the training loss \( \mathcal{J}_{x,y} \) in the above paragraph on the sampled pair \( (x,y) \).  
% \end{itemize}  
In implementation, our approach can be used to replace the policy improvement step in multiple RL algorithms, while keeping the critic training as is. At evaluation time, we simply taking the action by setting $L=1$ to reduce computational requirements.





\section{Analysis}
% OTPR is proposed to learn the conditional score based diffusion policy guided by OT. 
The proposed OTPR essentially aims to develop a conditional score-based diffusion policy for data transport from state space to action space in OT. To generate samples from conditional OT plan $\gamma^*(\cdot|\boldsymbol{s})$, the algorithm involves two key module learning: the dual term $(u_\omega, v_\omega)$ and the score model $s_\theta$. In this section, we will provide an analysis from the perspective of optimal transport, illustrating how the two aforementioned processes establish the upper bound of the distance between the distribution $\nu^{\text{SDE}}(\boldsymbol{a}|\boldsymbol{s})$ of generated samples and the conditional optimal transport plan $\gamma^*(\boldsymbol{a}|\boldsymbol{s})$.

 



To be specific, we investigate the upper bound of the expected Wasserstein distance $\mathbb{E}_{\boldsymbol{s}\sim\mu}W_2(\nu^{\text{SDE}}(\cdot|\boldsymbol{s}), \gamma^*(\cdot|\boldsymbol{s}))$. 
Since $W_2(\cdot|\cdot)$ is a proper metric, we can conveniently leverage the triangle inequality to derive an upper bound for this expectation: 
$
\mathbb{E}_{\boldsymbol{s}\sim\mu}W_2(\nu^{\text{SDE}}(\cdot|\boldsymbol{s}), \gamma^*(\cdot|\boldsymbol{s})) \leq  
\mathbb{E}_{\boldsymbol{s}\sim\mu}W_2(\hat{\gamma}(\cdot|\boldsymbol{s}), \gamma^*(\cdot|\boldsymbol{s}) ) + \mathbb{E}_{\boldsymbol{s}\sim\mu}W_2(\nu^{\text{SDE}}(\cdot|\boldsymbol{s}), \hat{\gamma}(\cdot|\boldsymbol{s})),$
% \begin{align}
% &\mathbb{E}_{\boldsymbol{s}\sim\mu}W_2(\nu^{\text{SDE}}(\cdot|\boldsymbol{s}), \gamma^*(\cdot|\boldsymbol{s})) \leq \\ 
% &\mathbb{E}_{\boldsymbol{s}\sim\mu}W_2(\nu^{\text{SDE}}(\cdot|\boldsymbol{s}), \hat{\gamma}(\cdot|\boldsymbol{s})) + \mathbb{E}_{\boldsymbol{s}\sim\mu}W_2(\hat{\gamma}(\cdot|\boldsymbol{s}), \gamma^*(\cdot|\boldsymbol{s}) )\notag
% \end{align}
where $\hat{\gamma}$ is the estimated OT plan depending on $u_{\hat{\omega}}, v_{\hat{\omega}}$. This inequality provides a means to assess the upper bound by breaking it down into two more manageable comparisons.

To bound the first term, we denote the Lagrange function for $L_2$-regularized OTs in Eq.~\ref{eq:kavRdual} as $\mathcal{L}(\gamma, u, v)$ with dual variables $u, v$ as follows:
\begin{align}
\mathcal{L}(\gamma, u, v) &= \int \left( \gamma(\boldsymbol{s}, \boldsymbol{a}) + \lambda \frac{\gamma(\boldsymbol{s}, \boldsymbol{a})^2}{\mu(\boldsymbol{s}) \nu(\boldsymbol{s})} \right) \mathrm{d}\boldsymbol{s} \, \mathrm{d}\boldsymbol{a}  \notag \\
&+ \int u(\boldsymbol{s}) \left( \int \gamma(\boldsymbol{s}, \boldsymbol{a}) \mathrm{d}\boldsymbol{a} - \mu(\boldsymbol{s}) \right) \mathrm{d}\boldsymbol{s}   \\ 
&+ \int v(\boldsymbol{a}) \left( \int \gamma(\boldsymbol{s}, \boldsymbol{a}) \mathrm{d}\boldsymbol{s} - \nu(\boldsymbol{a}) \right) \mathrm{d}\boldsymbol{a}.\notag  
\end{align}
% We follow \cite{daniels2021score} to assume that \( \mathcal{L}(\gamma, u, v) \) is \( \kappa \)-strongly convex in \( L_1 \)-norm w.r.t. \(\gamma\).
Because $\mathcal{L}(\gamma, u, v)$ is a sum of $K_\lambda(\gamma)$ and linear terms, the Lagrangian inherits $\lambda$-strong convexity in $L_1$-norm.
Given the trained $u_{\hat{\omega}}$ and $v_{\hat{\omega}}$ which are $\epsilon$-approximate maximizers of $\mathcal{J}_{\lambda}(u,v)$, the pseudo-plan $\hat{\gamma} = H(\boldsymbol{s},\boldsymbol{a};u_{\hat{\omega}},v_{\hat{\omega}})\mu(\boldsymbol{s})\nu(\boldsymbol{a})$ satisfies:
\begin{equation}
    \frac{\lambda}{2}\|\hat{\gamma}-\gamma^*\|^2_1 \leq \mathcal{L}(\hat{\gamma}, u_{\hat{\omega}}, v_{\hat{\omega}}) - \mathcal{L}(\gamma^*, u^*, v^*) \leq \epsilon
\end{equation}
Since the strong convexity of $\mathcal{L}$ implies a Polyak-Łojasiewicz (PL)  inequality, we have,
\begin{equation}
    \| \hat{\gamma} - \gamma^* \|_1 \leq \frac{1}{\lambda} \left\|\nabla_{\hat{\gamma}} L(\hat{\gamma}, u_{\hat{\omega}}, v_{\hat{\omega}}) \right\|_1 
\end{equation}
Consequently, we can derive an upper bound for the expected Wasserstein distance as follows: 
\begin{equation}
\mathbb{E}_{\boldsymbol{s}\sim\mu}W_2(\hat{\gamma}(\cdot|\boldsymbol{s}),\gamma^*(\cdot|\boldsymbol{s})) \leq \frac{\eta}{\lambda}\left\|\nabla_{\hat{\gamma}} L(\hat{\gamma}, u_{\hat{\omega}}, v_{\hat{\omega}}) \right\|_1,
\end{equation}
where $\eta = \max_{a,a'\in \mathcal{A}}\{\|a-a'\|_2\}$.

For the bound of $\mathbb{E}_{\boldsymbol{s}\sim\mu}W_2(\nu^{\text{SDE}}(\cdot|\boldsymbol{s}), \hat{\gamma}(\cdot|\boldsymbol{s}))$, it is difficult to get without explicit $f$ and $g$ given, but from the existing convergence guarantees for a general class of score-based generative models, we get $\mathbb{E}_{\boldsymbol{s}\sim\mu}W_2(\nu^{\text{SDE}}(\cdot|\boldsymbol{s}), \hat{\gamma}(\cdot|\boldsymbol{s})) \leq \epsilon$, which can be easily interpreted as two terms (1) the initialization of the algorithm at $\hat{\nu}_T(\cdot|\boldsymbol{s})$ instead of $\hat{\gamma}_T((\cdot|\boldsymbol{s}))$, (2) the discretization and score-matching errors in running the algorithm~\cite{DBLP:conf/nips/KwonFL22, DBLP:journals/corr/abs-2311-11003}.








% \textbf{toy experiments}

\begin{figure*}[!htb]
\vskip 0.1in
	\begin{center}
		\subfigure{\includegraphics[trim = 4mm 0mm 0mm 0mm,  clip,width=0.31\linewidth ]{figures/Exp1_kc.pdf}}
		\subfigure{\includegraphics[trim = 4mm 0mm 0mm 0mm,  clip,width=0.31\linewidth ]{figures/Exp1_km.pdf}}
		\subfigure{\includegraphics[trim = 4mm 0mm 0mm 0mm,  clip,width=0.31\linewidth ]{figures/Exp1_kp.pdf}}
		\subfigure{\includegraphics[trim = 4mm 0mm 0mm 0mm,  clip,width=0.31\linewidth ]{figures/Exp1_CALVIN.pdf}}
		\subfigure{\includegraphics[trim = 4mm 0mm 0mm 0mm,  clip,width=0.31\linewidth ]{figures/Exp1_can.pdf}}
		\subfigure{\includegraphics[trim = 4mm 0mm 0mm 0mm,  clip,width=0.31\linewidth ]{figures/Exp1_square.pdf}}
		\caption{\textbf{Learning curves of online fine-tuning with various methods.} Observe that OTPR largely always dominates or attains similar performance to the next best method. Other methods for fine-tuning diffusion policies (IDQL, DQL, DPPO) are a bit unstable, and perform substantially worse.}
		\label{fig:comparisons}
	\end{center}
    \vskip -0.3in
\end{figure*}



\section{Experiments}
In this section, we evaluate OTPR and several prior approaches, in a number of benchmark domains that require learning policies from static offline expert data and then fine-tune them with limited online interaction in the MDP (offline-to-online fine-tuning). We also study the hybrid RL problem setting (i.e., online RL with offline data put in the replay buffer) for some experiments. Finally, we perform ablation experiments to understand the utility of different components of OTPR. 

\subsection{Experimental Setup}

\textbf{Environments and tasks.} We study: (1) Robomimic tasks~\cite{mandlekar2021matters}, which is a commonly used benchmark designed to study imitation learning for robot
manipulation.The evaluation score represents the success rate. (2) Franka-Kitchen tasks~\cite{gupta2019relay}, which require solving a sequence of four manipulation tasks in a kitchen environment with a 9-Dof Franka robot; and (3) the CALVIN benchmark~\cite{mees2022calvin}, an evaluation benchmark designed for long-horizon, language-conditioned manipulation, which requires solving a sequence of four manipulation tasks in a tabletop environment. The evaluation score for a trajectory is the maximum number of sub-tasks completed simultaneously at any single point in the trajectory. 
% All of these tasks present long horizons; the Franka-Kitchen and CALVIN tasks require chaining different skills into a coherent long episode. That said, 
The CALVIN task is significantly challenging, as policies must be learned directly from pixels using offline play data obtained through human teleoperation.
% This offline data presents fairly low action coverage but pretty high coverage over different modes of semantic behavior. 
% Due to the diversity of offline data, we believe that CALVIN should stress test the ability of any approach to effectively utilize the multi-modal nature of diffusion policies for improving efficiency of fine-tuning.

\textbf{Implementation details.} 
We provide a detailed list of hyper-parameters and best practices for running OTPR in Appendix. We instantiate OTPR using the popular IQL with keeping the critic training as is. For the image-based domain, we use a ResNet 18 encoder and store features in the replay buffer to facilitate the estimation of the dual terms.

% Table generated by Excel2LaTeX from sheet 'Sheet8'
\begin{table*}[htbp]
  \centering
  \caption{Comparison of OTPR with other demo-augmented RL algorithms. OTPR outperforms every other approach, both in terms of the offline performance (left of $\rightarrow$) and performance after fine-tuning (right of $\rightarrow$). }
  \vskip 0.1in
    \begin{tabular}{cccccc}
    \toprule
    \toprule
    \multirow{2}[2]{*}{} & \multicolumn{3}{c}{Franka-Kitchen} & \multicolumn{2}{c}{RoboMimic} \\
          & Kitchen-Complete-v0 & Kitchen-Mixed-v0 & Kitchen-Partial-v0 & Can-State   & Square-State \\
    \midrule
    RLPD  &   $0 \rightarrow 18$    &   $0 \rightarrow 14$    &  $0 \rightarrow 34$     &  $0 \rightarrow 0$     & $0 \rightarrow 3$ \\
    \quad Cal-QL \quad \ & $19 \rightarrow 57$ & $37 \rightarrow 72$ & $59 \rightarrow 84$ & $ 0 \rightarrow 0 $      & $0 \rightarrow 0$ \\
 %    IQL   & $54 \rightarrow 60$ & $48 
 % \rightarrow 48$ & $57 \rightarrow 50$ & $ 34 \rightarrow 30$      & $28 \rightarrow 15 $ \\
    IBRL  &  $0 \rightarrow 25$     &   $0 \rightarrow 13$    &  $0 \rightarrow 15$    & $0 \rightarrow 64$    & $0 \rightarrow 50$ \\
    \midrule
    % OTPR-U &  $61 \rightarrow $     & $59 \rightarrow 79$      &   $42 \rightarrow 93$    & $63 \rightarrow 99$   &  $40 \rightarrow 98$\\
    OTPR  &  $61 \rightarrow 92$     & $59 \rightarrow 79$      &   $42 \rightarrow 93$    & $63 \rightarrow 99$   &  $40 \rightarrow 98$\\
    \bottomrule
    \bottomrule
    \end{tabular}%
  \label{tab:exp1RL}%
  \vskip -0.1in
\end{table*}%



\subsection{Results}


\noindent\textbf{Comparisons with Other Online Fine-Tuning Methods.} We conduct a comprehensive comparison of OTPR against a range of reinforcement learning (RL) methods designed for fine-tuning diffusion-based policies. Specifically, we evaluate the following approaches: (1) Implicit Diffusion Q-Learning (IDQL)~\cite{idql}, which extends Implicit Q-Learning (IQL) to incorporate diffusion policies through critic-based re-ranking; (2) Diffusion Policy Optimization (DPPO)~\cite{dppo}, which fine-tunes diffusion policies initially learned via imitation learning by optimizing a two-layer Markov Decision Process (MDP) loss; and (3) Diffusion Q-Learning (DQL)~\cite{wang2022diffusion}, which trains diffusion policies using a reparameterized policy gradient estimator similar to the Soft Actor-Critic (SAC) framework~\cite{haarnoja2018soft}.

Overall, OTPR performs consistently and significantly improves fine-tuning efficiency and asymptotic performance of diffusion policies. Notably, OTPR consistently maintains high normalized scores in the kitchen-complete-v0, CALVIN and Can task, while other methods exhibit relative instability, especially DQL and IDQL, which show considerable fluctuations in performance across different interaction steps. 
This may be attributed to both DQL and IDQL performing off-policy updates and propagating gradients from the imperfect Q function to the actor, which results in even greater training instability in sparse-reward tasks given the continuous action space and large action chunk sizes. In contrast, OTPR can quickly mitigate the adverse effects brought about by this issue by leveraging the guidance of the compatible function.
This analysis suggests that OTPR is a robust and effective approach for online fine-tuning in diffusion policy tasks, consistently outperforming the other methods in terms of stability and overall performance.
% These experimental results highlight that OTPR demonstrates better training stability and superior overall performance across different environments.



\noindent\textbf{Comparisons with demo-augmented RL.} Next, we compare OTPR with recently proposed RL methods for training robot policies (not necessarily diffusion based) leveraging offline data, including RLPD~\cite{ball2023efficient}, Cal-QL~\cite{nakamoto2024cal}, and IBRL~\cite{hu2023imitation}. These methods add expert data in the replay buffer and performs off-policy updates. We evaluate these methods on Franka-Kitchen and RoboMimic environents.
IBRL and Cal-QL are also pretrained with behavior cloning and offline RL objectives, respectively.
All of results are shown on Table~\ref{tab:exp1RL}. In the Franka-Kitchen domains, while Cal-QL demonstrates competitive performance, OTPR shows more impressive score improvements, rising from 61 to 92 in Kitchen-Complete-v0 and from 59 to 79 in Kitchen-Mixed-v0. In contrast, other methods such as RLPD, IQL, and IBRL perform significantly worse, particularly in the Kitchen-Partial-v0 task, where OTPR leads with a final score of 93.
In the RoboMimic environment, OTPR continues to excel, achieving high scores of 99 in Can-State and 98 in Square-State, showcasing its robustness across diverse scenarios. Although IBRL performs the best among the competitors, there remains a significant gap in performance.



\subsection{Ablation Experiments}
% Finally, we present some experiments to understand the importance of each component of OTPR. 

\begin{figure}[t]
 \vskip 0.1in
	\begin{center}
    \subfigure{\includegraphics[trim = 2mm 2mm 2mm 2mm,  clip,width=0.485\columnwidth ]{figures/Exp1_can_HQA.pdf}}
		\subfigure{\includegraphics[trim = 2mm 2mm 2mm 2mm,  clip,width=0.485\columnwidth ]{figures/Exp1_can_a.pdf}}
		\caption{(left) Comparison between
OTPR with different guidance (H, Q and A). (right) Comparison between
OTPR with (OTPR-M) and without (OTPR-U) the expert data mask. }
		\label{fig:ab2}
	\end{center}
     \vskip -0.2in
\end{figure}

\noindent\textbf{Effect of the compatibility function.} 
In the previous section, we have already demonstrated the advantages of OTPR over other diffusion-based fine-tuning methods that rely on Q-values. Now, to spotlight the pivotal role of our method’s core component—the guidance from the compatibility function $H$, we replace it with $Q$ and advantages $A$ within the same training framework. The experimental results on the Robomimic-Can task are illustrated in Fig.~\ref{fig:ab2}(left).
Clearly, compared to using Q-value and advantage, OT-guided training demonstrates significantly faster convergence and superior evaluation performance.


\noindent\textbf{Effect of the masked OT.} OTPR incorporates masked Optimal Transport (OT) to utilize expert data as keypoints, guiding accurate distribution transport. As depicted in Fig.~\ref{fig:ab2}, OTPR-U, which lacks the mask matrix, exhibits instability and reduced efficiency, despite outperforming other mainstream methods. Notably, even without the mask, OTPR can still operate as a fully functional offline RL algorithm by leveraging the compatibility function without reward. 
% We detail its performance on the Offline RL benchmark in the appendix.

\section{Conclusion}
This paper introduced OTPR, a novel method integrating optimal transport theory with diffusion policies to enhance the efficiency and adaptability of reinforcement learning fine-tuning. OTPR leverages the Q-function as a transport cost and uses masked optimal transport to guide state-action matching, improving learning stability and performance. Experiments demonstrated OTPR's superior performance across multiple tasks, especially in complex environments. Future work will focus on scaling OTPR to larger state-action spaces, and exploring its integration with other advanced policy architectures.
% \section*{Impact Statement}

% Authors are \textbf{required} to include a statement of the potential 
% broader impact of their work, including its ethical aspects and future 
% societal consequences. This statement should be in an unnumbered 
% section at the end of the paper (co-located with Acknowledgements -- 
% the two may appear in either order, but both must be before References), 
% and does not count toward the paper page limit. In many cases, where 
% the ethical impacts and expected societal implications are those that 
% are well established when advancing the field of Machine Learning, 
% substantial discussion is not required, and a simple statement such 
% as the following will suffice:

% ``This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.''

% The above statement can be used verbatim in such cases, but we 
% encourage authors to think about whether there is content which does 
% warrant further discussion, as this statement will be apparent if the 
% paper is later flagged for ethics review.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
% \section{Additional Details for Optimal Transport}

\section{Additional Details for Algorithm}
\begin{figure}[t]
\vskip 0.2in
	\begin{center}
		\subfigure{\includegraphics[trim = 0mm 0mm 15mm 0mm,  clip,width=0.485\columnwidth ]{figures/Toy1_Q.pdf}}
		\subfigure{\includegraphics[trim = 5mm 0mm 10mm 0mm,  clip,width=0.485\columnwidth ]{figures/Toy1_plan.pdf}}
		\caption{This example demonstrates a clear and concise visualization of a Q-value matrix alongside its corresponding estimated optimal transport plan. }
		\label{fig:toys}
	\end{center}
    \vskip -0.2in
\end{figure}

\subsection{Pseudo-codes of algorithm for training $u_{\omega}, v_{\omega}$.} \label{app:alOT} 
The pseudo-codes of the algorithm to learn the dual terms $u_{\omega}, v_{\omega}$, a.k.a., potentials, are given in Algorithm~\ref{alg:trainuv} \cite{seguy2018large,gu2023optimal}.

\begin{algorithm}[]
   \caption{Algorithm for estimating potentials $u_{\omega_1}, v_{\omega_2}$}
   \label{alg:trainuv}
\begin{algorithmic}
   \STATE {\bfseries Input:} $Q$-network and replay buffer $\mathcal{B}$, nets $u_{\omega}, v_{\omega}$, batch size $b$, learning rate $\alpha$, expert demonstrations $\mathcal{D}$ (if  available) .
   \STATE {\bfseries Output: }  Learned potential $u_{\omega_1}, v_{\omega_2}$.
   \FOR{iteration = $1, 2, \dots$}
   \STATE Calculate the cost between $\boldsymbol{s}$ and $\boldsymbol{a}$ in $\mathcal{B}$.
   \STATE Sample a state batch $\{s_1, s_2, \dots, s_b\}$ from $\mathcal{B}$.
   \STATE Sample a action batch $\{a_1, a_2, \dots, a_b\}$ from $\mathcal{B}$.
   
   \IF{Expert data $\mathcal{D}$ is available}
   \STATE Update $ \omega_1 \leftarrow \omega_1 + \alpha \sum_{i,j} \nabla_{\omega_1} u_{\omega_1}(s_i) + \partial_u \tilde{F}_\lambda(u_{\omega_1}(s_i), v_{\omega_2}(a_j)) \nabla_{\omega_1} u_{\omega_1}(s_i)$
   \STATE Update $ \omega_2 \leftarrow \omega_2 + \alpha \sum_{i,j} \nabla_{\omega_2} v_{\omega_2}(a_j) + \partial_v \tilde{F}_\lambda(u_{\omega_1}(s_i), v_{\omega_2}(a_j)) \nabla_{\omega_2} v_{\omega_2}(a_m)$
   \ELSE
   \STATE Update $ \omega_1 \leftarrow \omega_1 + \alpha \sum_{i,j} \nabla_{\omega_1} u_{\omega_1}(s_i) + \partial_u F_\lambda(u_{\omega_1}(s_i), v_{\omega_2}(a_j)) \nabla_{\omega_1} u_{\omega_1}(s_i)$
   \STATE Update $ \omega_2 \leftarrow \omega_2 + \alpha \sum_{i,j} \nabla_{\omega_2} v_{\omega_2}(a_j) + \partial_v F_\lambda(u_{\omega_1}(s_i), v_{\omega_2}(a_j)) \nabla_{\omega_2} v_{\omega_2}(a_m)$
   \ENDIF
   % \ENDFOR
   \ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{Training by fitting noise.}~\label{app:TFNoise}
We consider the VE-SDE and the VP-SDE as examples of forward SDEs. In the VE-SDE, $f(\boldsymbol{a}, t) = 0$ and $g(t) = \sqrt{\frac{d [\sigma^{2}(t)]}{dt}}$, where $\sigma > 0$ is an increasing function of t. For the VP-SDE, $f(\boldsymbol{a}, t) = -\frac{1}{2} \beta(t) \boldsymbol{a}$ and $g(t) = \sqrt{\beta(t)}$, with $\beta(t) = \beta_{min} + (\beta_{max} - \beta_{min}) t$. The distribution $p_{t|0}(\boldsymbol{a}_t | \boldsymbol{a}_0)$ for $\boldsymbol{a}_t$ given $\boldsymbol{a}_0$ is defined as: 
\begin{equation}
p_{t|0}(\boldsymbol{a}_t | \boldsymbol{a}_0) =  
\begin{cases}   
\mathcal{N}(\boldsymbol{a}_t | \boldsymbol{a}_0, \sigma^2(t)\mathbf{I}), & \text{for VE-SDE,} \\   
\mathcal{N}(\boldsymbol{a}_t | \boldsymbol{a}_0 e^{\frac{1}{2} h(t)}, (1 - e^{h(t)}) \mathbf{I}), & \text{for VP-SDE,}   
\end{cases}   
\end{equation} 
where $h(t) = -\frac{1}{2} t^2 (\beta_{max} - \beta_{min}) - t \beta_{min}$, and $\mathbf{I}$ is the identity matrix. 
% In~\cite{song2020score}, $p_{prior} = \mathcal{N}(0, \mathbf{I})$ for VP-SDE and $p_{prior} = \mathcal{N}(0, \frac{1}{2 \log \alpha} (\alpha^{2t} - 1) \mathbf{I})$ for VE-SDE.

We define the \( \sigma_t \mathbf{I} \) as the standard variation of \( p_{t|0}(\boldsymbol{a}_t|\boldsymbol{a}) \), specifically,  \( \sigma_t^2 = \sigma^2(t) \) for VE-SDE and \( \sigma_t^2 = 1 - e^{h(t)} \) for VP-SDE. Using the reparameterization trick, given sampling\( (\boldsymbol{s},\boldsymbol{a}) \), we have \( \boldsymbol{a}_t = \boldsymbol{a} + \sigma_t \epsilon \) for VE-SDE, and \( \boldsymbol{a}_t = e^{\frac{1}{2} h(t)} \boldsymbol{a} + \sigma_t \epsilon \) for VP-SDE, where \( \epsilon \sim \mathcal{N}(0,\mathbf{I}) \). Further, \( \nabla_{\boldsymbol{a}_t} \log p_{t|0}(\boldsymbol{a}_t|\boldsymbol{a}) = -\frac{1}{\sigma_t} \epsilon \). Therefore, the loss \( \mathcal{J}_{\boldsymbol{s},\boldsymbol{a}} \) for fitting noise can be written as  
\begin{equation}\label{eq:A-5}
\mathcal{J}_{\boldsymbol{s},\boldsymbol{a}} = \mathbb{E}_{t,\epsilon \sim \mathcal{N}(0,I)} \left[ \frac{w_t}{\sigma_t^2} \left\| s_\theta(\nu_t(\boldsymbol{a}) + \sigma_t \epsilon; \boldsymbol{s}, t)\sigma_t+\epsilon \right\|_2^2 \right]. 
\end{equation} 
For VE-SDE, \( \nu_t(\boldsymbol{a}) = \boldsymbol{a} \), while for VP-SDE, \( \nu_t(\boldsymbol{a}) = e^{\frac{1}{2} h(t)} \boldsymbol{a} \). Equation~\ref{eq:A-5} indicates that \( s_\theta(y_t; x, t) \) is trained to match the scaled noise \( -\frac{1}{\sigma_t} \epsilon \).


\section{Proofs}
\subsection{Proof of Proposition~\ref{propOTRL}}\label{app:ProofP1}
\begin{proposition}
Given an optimal behavior policy $\pi^\beta$ and a critic-based cost function $c = -Q^{\beta}$, let $\pi^*$ is the solution to Eq.~\ref{eq:monRL} with the $Q^{\beta}$ cost function. Then it holds that: $\mathcal{J}_{\text{RL}}(\pi^*) = \mathcal{J}_{\text{RL}}(\pi^\beta)$.
\end{proposition}

\begin{proof}
We use $\text{Supp}(\mu)$ and $\text{Supp}(\nu)$ to refer to the support of $\mu$ and  $\nu$, two subsets of $\mathcal{S}$ and $\mathcal{A}$, respectively, which are also the set of values which $\boldsymbol{s} \sim \mu$ and $\boldsymbol{a} \sim \nu$ can take. Given a point $s \in \text{Supp}(\mu)$, the Monge problem would send the whole mass at $x$ to a unique location $a \in \text{Supp}(\nu)$.
The a primal state-conditioned Monge OT problem discripted with 
\ref{eq:monRL} can be formulated as:
\begin{equation} 
    \inf_\pi \mathbb{E}_{\boldsymbol{s}\sim\mu} \left[-Q^\beta(\boldsymbol{s}, \mathcal{\pi}(\boldsymbol{s}))\right], \text{subject to}  \ \pi(s) \subset \text{Supp}(\nu) \ \text{for all} \ s \in \text{Supp}(\mu).
\end{equation}  

According to \cite{kakade2002approximately}, we can use the performance difference lemma to compare the performance of the two policies $\pi^*$ and $\pi^\beta$ :
\begin{align}
    J(\pi^*) - J(\pi^\beta) &= \frac{1}{1-\kappa} \mathbb{E}_{\boldsymbol{s}\sim \mu}\left[A^\beta(\boldsymbol{s}, \pi^*)\right] \\
    & = \frac{1}{1-\kappa} \mathbb{E}_{\boldsymbol{s}\sim \mu}\left[Q^\beta(\boldsymbol{s}, \pi^*(\boldsymbol{s}))-V^\beta(\boldsymbol{s})\right] \\
    &= \frac{1}{1-\kappa} \mathbb{E}_{\boldsymbol{s} \sim \mu} \left[Q^\beta(\boldsymbol{s}, \max_{\boldsymbol{a} \subset \text{Supp}(\beta(\cdot|\boldsymbol{s}))}[Q^\beta(\boldsymbol{s}, \boldsymbol{a})]) - V^\beta(\boldsymbol{s})\right]
\end{align}
In the setting of Proposition\label{app:ProofP1}, $\pi^\beta$ is an optimal expert policy, $V^\beta(\boldsymbol{s}) = \max_{\boldsymbol{a}}Q^\beta(\boldsymbol{s}, \boldsymbol{a})$, thus $\text{Supp}(\pi^\beta(\cdot|s))$ indicates the optimal actions $\boldsymbol{a}^*$ from $\pi^\beta$ which maximize $Q^\beta$. Then we have: $J(\pi^*) - J(\pi^\beta) = 0$.
\end{proof} 

\subsection{Proof of Proposition~\ref{prop2}}\label{app:ProofP2}
\begin{proposition}
Let \( \mathcal{C}(\boldsymbol{s}, \boldsymbol{a}) = \frac{1}{\mu(\boldsymbol{s})} \delta(\boldsymbol{s} - \boldsymbol{s}_{\text{cond}}(\boldsymbol{a})) \) where \( \delta \) is the Dirac delta function, then \( \mathcal{J}_{\text{DSM}}(\theta) \) in Eq.~\ref{eq:DSM} can be reformulated as  
\begin{align}\label{eq:proofP4.2-1}
\mathcal{J}_{\text{CDSM}}(\theta) = & \mathbb{E}_{t} w_{t} \mathbb{E}_{\boldsymbol{s} \sim \mu} \mathbb{E}_{\boldsymbol{a} \sim \nu} \mathcal{C}(\boldsymbol{s},\boldsymbol{a}) \mathbb{E}_{\boldsymbol{a}_{t} \sim \nu_{t}|0}(\boldsymbol{a}_{t}|\boldsymbol{a}) \notag \\
&\left\| s_{\theta}(\boldsymbol{a}_{t}; \boldsymbol{s}, t) - \nabla_{\boldsymbol{a}_{t}} \log \nu_{t|0}(\boldsymbol{a}_{t}|\boldsymbol{a}) \right\|^{2}_{2}. 
\end{align}
Furthermore, 
\begin{equation}\label{eq:proofP4.2-2}
\upsilon(\boldsymbol{s},\boldsymbol{a}) = \mathcal{C}(\boldsymbol{s},\boldsymbol{a}) \mu(\boldsymbol{s}) \nu(\boldsymbol{a}) 
\end{equation}
is a joint distribution for marginal distributions \( \mu \) and \( \nu \).  
\end{proposition}
\begin{proof}
We first prove Eq.~\ref{eq:proofP4.2-1}, and then demonstrate that \(\upsilon(\boldsymbol{s}, \boldsymbol{a})\) serves as a joint distribution for the marginal distributions \(\mu\) and \(\nu\).
(1) The right side of Eq.~\ref{eq:proofP4.2-1} is  

\begin{align}
    &\mathbb{E}_t w_t \mathbb{E}_{\boldsymbol{s} \sim \mu} \mathbb{E}_{\boldsymbol{a} \sim \nu} \mathcal{C}(\boldsymbol{s}, \boldsymbol{a}) \mathbb{E}_{\boldsymbol{a}_t \sim \nu_{t|0}(\boldsymbol{a}_t | \boldsymbol{a})} \| s_{\theta}(\boldsymbol{a}_t; \boldsymbol{s}, t) - \nabla_{\boldsymbol{a}_t} \log \nu_{t|0}(\boldsymbol{a}_t | \boldsymbol{a}) \|^2_2  \\
    =&\mathbb{E}_t w_t \mathbb{E}_{\boldsymbol{a} \sim \nu} \int \mu(\boldsymbol{s}) \mathcal{C}(\boldsymbol{s}, \boldsymbol{a}) \mathbb{E}_{\boldsymbol{a}_t \sim \nu_{t|0}(\boldsymbol{a}_t | \boldsymbol{a})} \| s_{\theta}(\boldsymbol{a}_t; \boldsymbol{s}, t) - \nabla_{\boldsymbol{a}_t} \log \nu_{t|0}(\boldsymbol{a}_t | \boldsymbol{a}) \|^2_2 d\boldsymbol{s}  \\
    =&\mathbb{E}_t w_t \mathbb{E}_{\boldsymbol{a} \sim \nu} \int \delta(\boldsymbol{s}-\boldsymbol{s}_{\text{cond}}(\boldsymbol{a})) \mathbb{E}_{\boldsymbol{a}_t \sim \nu_{t|0}(\boldsymbol{a}_t | \boldsymbol{a})} \| s_{\theta}(\boldsymbol{a}_t; \boldsymbol{s}, t) - \nabla_{\boldsymbol{a}_t} \log \nu_{t|0}(\boldsymbol{a}_t | \boldsymbol{a}) \|^2_2 d\boldsymbol{s}  \\
    =&\mathbb{E}_t w_t \mathbb{E}_{\boldsymbol{a} \sim \nu} \mathbb{E}_{\boldsymbol{a}_t \sim \mu_{t | 0} (\boldsymbol{a}_t | \boldsymbol{a})} \left\| s_{\theta}(\boldsymbol{a}_t; \boldsymbol{s}_{\text{cond}}(\boldsymbol{a}), t) - \nabla_{\boldsymbol{a}_t} \log \nu_{t|0}(\boldsymbol{a}_t | \boldsymbol{a}) \right\|^2_2, 
\end{align}  
which is the definition of $\mathcal{J}_{\text{DSM}}(\theta)$ in Eq.~\ref{eq:DSM}.

(2) We demonstrate that the marginal distributions of \(\upsilon(\boldsymbol{s},\boldsymbol{a})\) are \(\mu\) and \(\nu\) as follows. Firstly,
\begin{equation}
\int \upsilon(\boldsymbol{s},\boldsymbol{a}) \, d\boldsymbol{s} = \int \delta(\boldsymbol{s} - \boldsymbol{s}_{\text{cond}}(\boldsymbol{a})) \nu(\boldsymbol{a}) \, d\boldsymbol{s} = \nu(\boldsymbol{a}) \int \delta(\boldsymbol{s} - \boldsymbol{s}_{\text{cond}}(\boldsymbol{a})) \, d\boldsymbol{s} = \nu(\boldsymbol{a}) 
\end{equation}

Next, from the definition of \(\delta(\cdot)\), we obtain \(\delta(\boldsymbol{s} - \boldsymbol{s}_{\text{cond}}(\boldsymbol{a})) = \sum_{\boldsymbol{a}' : \boldsymbol{s}_{\text{cond}}(\boldsymbol{a}')=\boldsymbol{s}} \delta(\boldsymbol{a} - \boldsymbol{a}')\). Then, we have
\begin{align}
    \int \upsilon(\boldsymbol{s},\boldsymbol{a}) \, d\boldsymbol{a} &= \int \delta(\boldsymbol{s} - \boldsymbol{s}_{\text{cond}}(\boldsymbol{a})) \nu(\boldsymbol{a}) \, d\boldsymbol{a} \\
    &= \int \sum_{\{\boldsymbol{a}' : \boldsymbol{s}_{\text{cond}}(\boldsymbol{a}')=\boldsymbol{s}\}} \delta(\boldsymbol{a}' - \boldsymbol{a}) \nu(\boldsymbol{a}) \, d\boldsymbol{a} \\
    &= \sum_{\{\boldsymbol{a}' : \boldsymbol{s}_{\text{cond}}(\boldsymbol{a}')=\boldsymbol{s}\}} \int \delta(\boldsymbol{a}' - \boldsymbol{a}) \nu(\boldsymbol{a}) \, d\boldsymbol{a} \\
    &= \sum_{\{\boldsymbol{a}' : \boldsymbol{s}_{\text{cond}}(\boldsymbol{a}')=\boldsymbol{s}\}} \nu(\boldsymbol{a}') \\
    &= \mu(\boldsymbol{s}) 
\end{align}
\end{proof}


\subsection{Proof of Theorem~\ref{theorem1}}\label{app:ProofT1}
\begin{theorem}
  For \( \boldsymbol{s} \sim \mu \), consider the forward SDE   
\( \mathrm{d}\boldsymbol{a}_t = f(\boldsymbol{a}_t, t) \mathrm{d}t + g(t) \mathrm{d}\mathbf{w} \) with \( \boldsymbol{a}_0 \sim \hat{\gamma}(\cdot | \boldsymbol{s}) \) and \( t \in [0, T] \). Let \( \nu_t(\boldsymbol{a}_t|\boldsymbol{s}) \) be the distribution of \( \boldsymbol{a}_t \) and   
\(\mathcal{J}_{\text{CSM}}(\theta) = \mathbb{E}_t w_t \mathbb{E}_{\boldsymbol{s} \sim \mu} \mathbb{E}_{\boldsymbol{a}_t \sim \nu_t(\boldsymbol{a}_t|\boldsymbol{s})} \| s_\theta(\boldsymbol{a}_t; \boldsymbol{s}, t) - \nabla_{\boldsymbol{a}_t} \log \nu_t(\boldsymbol{a}_t|\boldsymbol{s}) \|^2_2,  \)
then we have  \(\nabla_\theta \mathcal{J}_{\text{HDSM}}(\theta) = \nabla_\theta \mathcal{J}_{\text{CSM}}(\theta).  \)
\end{theorem}
\begin{proof}
To establish the equivalence between $\mathcal{J}_{\text{HDSM}}(\theta)$ and $\mathcal{J}_{\text{CSM}}(\theta)$, we start by examining the difference between the two objective functions:
\begin{align}
    \mathcal{J}_{\text{HDSM}}(\theta) - \mathcal{J}_{\text{CSM}}(\theta) &= \mathbb{E}{w_t} \mathbb{E}_{\boldsymbol{s} \sim \mu} \mathbb{E}_{\boldsymbol{a}_0 \sim \nu} H(\boldsymbol{s}, \boldsymbol{a}_0) \mathbb{E}_{\boldsymbol{a}_t \sim \nu_{t | 0}(\boldsymbol{a}_t | \boldsymbol{a}_0)} \| s_\theta(\boldsymbol{a}_t; \boldsymbol{s}, t) - \nabla_{\boldsymbol{a}_t} \log \nu_{t | 0}(\boldsymbol{a}_t | \boldsymbol{a}_0) \|^2_2 - \mathcal{J}_{\text{CSM}}(\theta) \notag \\
    &= \mathbb{E}{w_t} \mathbb{E}_{\boldsymbol{s} \sim \mu} \mathbb{E}_{\boldsymbol{a}_0 \sim \hat{\gamma}(\boldsymbol{a}_0 | \boldsymbol{s})} \mathbb{E}_{\boldsymbol{a}_t \sim \nu_{t | 0}(\boldsymbol{a}_t | \boldsymbol{a}_0)} \| s_\theta(\boldsymbol{a}_t; \boldsymbol{s}, t) - \nabla_{\boldsymbol{a}_t} \log \nu_{t | 0}(\boldsymbol{a}_t | \boldsymbol{a}_0) \|^2_2 - \mathcal{J}_{\text{CSM}}(\theta)
    % &\quad -  \mathbb{E}_t w_t \mathbb{E}_{\boldsymbol{s} \sim \mu} \mathbb{E}_{\boldsymbol{a}_t \sim \nu_t(\boldsymbol{a}_t|\boldsymbol{s})} \| s_\theta(\boldsymbol{a}_t; \boldsymbol{s}, t) - \nabla_{\boldsymbol{a}_t} \log \nu_t(\boldsymbol{a}_t|\boldsymbol{s}) \|^2_2 \notag 
\end{align}
% Given \( \boldsymbol{s} \) and \( t \), \( \nu_{t} (\boldsymbol{a}_t | \boldsymbol{s}) \) is the distribution of \( \boldsymbol{a}_t \) produced by the forward SDE \( d\boldsymbol{a}_t = f(\boldsymbol{a}_t, t) \mathrm{d}t + g(t) \mathrm{d}\mathbf{w} \) with initial state \( \boldsymbol{a}_0 \sim \hat{\gamma}(\boldsymbol{a}_0 | \boldsymbol{s}) \). This implies that
Since \( \boldsymbol{s} \to \boldsymbol{a}_0 \to \boldsymbol{a}_t \) is a Markov Chain in the forward SDE process, the distribution \( \nu_{t | 0}(\boldsymbol{a}_t | \boldsymbol{a}_0, \boldsymbol{s}) \) of \( \boldsymbol{a}_t \) simplifies to \( \nu_{t | 0}(\boldsymbol{a}_t | \boldsymbol{a}_0, \boldsymbol{s}) = \nu_{t | 0}(\boldsymbol{a}_t | \boldsymbol{a}_0) \), which is the distribution of \( \boldsymbol{a}_t \) by the forward SDE \( d\boldsymbol{a}_t = f(\boldsymbol{a}_t, t) \mathrm{d}t + g(t) \mathrm{d}\mathbf{w} \) with initial state \( \boldsymbol{a}_0 \). 
Then, we have
\begin{align}\label{eq:PT3-2}
\mathcal{J}_{\text{HDSM}}(\theta) - \mathcal{J}_{\text{CSM}}(\theta) = &\mathbb{E}{w_t} \mathbb{E}_{\boldsymbol{s} \sim \mu} \mathbb{E}_{\boldsymbol{a}_0 \sim \hat{\gamma}(\boldsymbol{a}_0 | \boldsymbol{s})} \mathbb{E}_{\boldsymbol{a}_t \sim \nu_{t | 0}(\boldsymbol{a}_t | \boldsymbol{a}_0, \boldsymbol{s})} \| s_\theta(\boldsymbol{a}_t; \boldsymbol{s}, t) - \nabla_{\boldsymbol{a}_t} \log \nu_{t | 0}(\boldsymbol{a}_t | \boldsymbol{a}_0, \boldsymbol{s}) \|^2_2 \notag \\
&-  \mathbb{E}_t w_t \mathbb{E}_{\boldsymbol{s} \sim \mu} \mathbb{E}_{\boldsymbol{a}_t \sim \nu_t(\boldsymbol{a}_t|\boldsymbol{s})} \| s_\theta(\boldsymbol{a}_t; \boldsymbol{s}, t) - \nabla_{\boldsymbol{a}_t} \log \nu_t(\boldsymbol{a}_t|\boldsymbol{s}) \|^2_2.
\end{align}


According to \cite{6795935}, given any \( \boldsymbol{s} \) and \( t \), we have  
\begin{align}
&\mathbb{E}_{\boldsymbol{a}_0 \sim \hat{\gamma}(\boldsymbol{a}_0 | \boldsymbol{s})} \mathbb{E}_{\boldsymbol{a}_t \sim \nu_{t | 0}(\boldsymbol{a}_t | \boldsymbol{a}_0, \boldsymbol{s})} \| s_\theta(\boldsymbol{a}_t; \boldsymbol{s}, t) - \nabla_{\boldsymbol{a}_t} \log \nu_{t | 0}(\boldsymbol{a}_t | \boldsymbol{a}_0, \boldsymbol{s}) \|^2_2  \notag \\
= &\mathbb{E}_{\boldsymbol{a}_t \sim \nu_{t}(\boldsymbol{a}_t | \boldsymbol{s})} \| s_\theta(\boldsymbol{a}_t; \boldsymbol{s}, t) - \nabla_{\boldsymbol{a}_t} \log \nu_{t | 0}(\boldsymbol{a}_t | \boldsymbol{s}) \|^2_2 + C_{\boldsymbol{s}, t}, 
\end{align}
where \( C_{\boldsymbol{s}, t} \) is a constant to \( \theta \) depending on \( \boldsymbol{s} \) and \( t \). Substituting this result into the previous Eq.~\ref{eq:PT3-2}, we get
\begin{equation}
\mathcal{J}_{\text{HDSM}}(\theta) - \mathcal{J}_{\text{CSM}}(\theta) = \mathbb{E}_{\boldsymbol{s} \sim \mu}\mathbb{E}_tw_tC_{\boldsymbol{s}, t}.
\end{equation}
% Then, we have  
% \begin{align}
% &w_t \left( \mathbb{E}_{\boldsymbol{a}_t \sim \nu_{t | \boldsymbol{s}}(\boldsymbol{a}_t | \boldsymbol{s})} \| s_\theta(\boldsymbol{a}_t; \boldsymbol{s}, t) - \nabla_{\boldsymbol{a}_t} \log \nu_{t | 0}(\boldsymbol{a}_t | \boldsymbol{s}) \|^2_2 + C_{\boldsymbol{s}, t} \right) \notag \\
% = &w_t \mathbb{E}_{\boldsymbol{a}_0 \sim \hat{\gamma}(\boldsymbol{a}_0 | \boldsymbol{s})} \mathbb{E}_{\boldsymbol{a}_t \sim \nu_{t | 0}(\boldsymbol{a}_t | \boldsymbol{a}_0)} \| s_\theta(\boldsymbol{a}_t; \boldsymbol{s}, t) - \nabla_{\boldsymbol{a}_t} \log \nu_{t | 0}(\boldsymbol{a}_t | \boldsymbol{a}_0) \|^2_2. 
% \end{align}
% Taken expectation over \( \boldsymbol{s} \) and \( t \) in the above equation, we have  
% \begin{align}
% &\mathcal{J}_{\text{CSM}}(\theta) + \mathbb{E}_{\boldsymbol{s} \sim \mu} \mathbb{E}_{\boldsymbol{a}_0 \sim \hat{\gamma}(\boldsymbol{a}_0 | \boldsymbol{s})} \mathbb{E}_{\boldsymbol{a}_t \sim \nu_{t | 0}(\boldsymbol{a}_t | \boldsymbol{a}_0)} \| s_\theta(\boldsymbol{a}_t; \boldsymbol{s}, t) - \nabla_{\boldsymbol{a}_t} \log \nu_{t | 0}(\boldsymbol{a}_t | \boldsymbol{a}_0) \|^2_2 \\
% = &\mathbb{E}_{w_t} \mathbb{E}_{\boldsymbol{s} \sim \mu} \mathbb{E}_{\boldsymbol{a}_0 \sim \hat{\gamma}(\boldsymbol{a}_0 | \boldsymbol{s})} H(\boldsymbol{s}, \boldsymbol{a}_0) \mathbb{E}_{\boldsymbol{a}_t \sim \nu_{t | 0}(\boldsymbol{a}_t | \boldsymbol{a}_0)} \| s_\theta(\boldsymbol{a}_t; \boldsymbol{s}, t) - \nabla_{\boldsymbol{a}_t} \log \nu_{t | 0}(\boldsymbol{a}_t | \boldsymbol{a}_0) \|^2_2 \\
% = &\mathcal{J}_{\text{HDSM}}(\theta).
% \end{align}
Since the right-hand side is a constant to \( \theta \), we have conclude that
\begin{equation}
\nabla_{\theta} \mathcal{J}_{\text{HDSM}}(\theta) = \nabla_{\theta} \mathcal{J}_{\text{CSM}}(\theta).
\end{equation}
\end{proof}

\section{Details for Experiments}
All experiments are conducted on an NVIDIA Tesla A100 80GB GPU, and all fine-tuning methods use the same pre-trained policy.
\subsection{Details and Hyper-parameters for OTPR}

\textbf{Details for training $u_{\omega}, v_{\omega}$.}
The architecture of both $u_{\omega}$ and $v_{\omega}$ is a two MLP. $\lambda$ is set to $1e-5$. The batch size is set $64$. We employ the Adam algorithm to update the
parameters with $1e-6$ learning rate.

\textbf{Details for training $s_\theta$.} We take the VP-SDE~\cite{song2020score} as the forward SDE. In inference, we take the sampling method in DDIM~\cite{ddim} to perform the reverse SDE to generate action. The observations and actions are normalized to $[0, 1]$ using min/max statistics from the pre-training dataset. For diffusion-based policies, we use MLP with two-layer residual connection similar to DPPO. 

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[htbp]
  \centering
  \caption{Hyper-parameters for OTPR}
    \begin{tabular}{ccccc}
    \toprule
    \multirow{2}[2]{*}{Parameter} & \multicolumn{4}{c}{Task} \\
          & Franka-Ketichen & CALVIN & Robomimic-Can & Robomimic-Square \\
    \midrule
    Buffer size  & 1000000 & 250000 & 250000 & 250000 \\
    Actor Learning Rate & 1.00E-05 & 1.00E-05 & 1.00E-05 & 1.00E-05 \\
    Discount $\kappa$ & 0.99 & 0.99 & 0.999 & 0.999 \\
    Optimizer & \multicolumn{4}{c}{Adam} \\
    $L$ & 8 & 8 & 8 & 8 \\
    $T$ & 20 & 20 & 20 & 20 \\
    $\tau$ & 0.7 & 0.7 & 0.7 & 0.7 \\
    Actor Batch Size & 1024  & 1024  & 1024  & 1024 \\
    Critic (Q and V) Hidden Layer Sizes  & [512, 512, 512] & [512, 512, 512] & [256, 256, 256] & [256, 256, 256] \\
    Critic (Q and V) Batch Size & 256   & 256   & 256   & 256 \\
    \bottomrule
    \end{tabular}%
  \label{tab:addlabel}%
\end{table}%


\subsection{Details and Hyper-parameters for Baselines}


\textbf{DPPO} For the state-based tasks Robomimic and FrankaKitchen, we trained DPPO-MLP following the original paper's specifications, using an action chunking size of 4 for Robomimic and 8 for FrankaKitchen. For the pixel-based task CALVIN, we trained DPPO-ViT-MLP with an action chunking size of 4.

\textbf{IDQL} We employ the IDQL-Imp version of IDQL, wherein the Q-function, value function, and diffusion policy are refined through new experiences. For Robomimic tasks, we employ the same network architecture as OTPR, while the original IDQL architectures are preserved for Franka-Kitchen and CALVIN. For the IQL $\tau$ expectile, we set it to 0.7 for each task. 

\textbf{DQL} We set the weighting coefficient to $0.5$ for Robomimic, $0.005$ for Franka-Kitchen and $0.01$ for CALVIN.

\textbf{IBRL} We adhere to the original implementations' hyperparameters, with wider (1024) MLP layers and dropout during pre-training.

\textbf{Cal-QL} We set the mixing ratio to $0.25$ for Franka-Kitchen and $0.5$ for CALVIN and Robomimic.

% \section{Additional Results of Experiments}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
