% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}


\usepackage{booktabs}
\usepackage{amsfonts}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Streaming Speaker Change Detection and Gender Classification for Transducer-Based Multi-Talker Speech Translation}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

\author{\textbf{Peidong Wang}\textsuperscript{}, \textbf{Naoyuki Kanda}\textsuperscript{}, \textbf{Jian Xue}\textsuperscript{}, \textbf{Jinyu Li}\textsuperscript{}, \textbf{Xiaofei Wang}\textsuperscript{}, \\ \textbf{Aswin S. Subramanian}\textsuperscript{}, \textbf{Junkun Chen}\textsuperscript{}, \textbf{Sunit Sivasankaran}\textsuperscript{}, \textbf{Xiong Xiao}\textsuperscript{}, \textbf{Yong Zhao}\textsuperscript{} \\
        \\
        \textsuperscript{}Microsoft \\ 
        One Microsoft Way, Redmond WA, USA}

%\author{
%  \textbf{First Author\textsuperscript{}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{}},
%  \textbf{Fourth Author\textsuperscript{}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{}},
%  \textbf{Seventh Author\textsuperscript{}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{}},
%  \textbf{Tenth Author\textsuperscript{}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{}},
%  \textbf{Sixteenth Author\textsuperscript{}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{}}
%\\
%\\
%  \textsuperscript{}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
Streaming multi-talker speech translation is a task that involves not only generating accurate and fluent translations with low latency but also recognizing when a speaker change occurs and what the speaker's gender is. Speaker change information can be used to create audio prompts for a zero-shot text-to-speech system, and gender can help to select speaker profiles in a conventional text-to-speech model. We propose to tackle streaming speaker change detection and gender classification by incorporating speaker embeddings into a transducer-based streaming end-to-end speech translation model. Our experiments demonstrate that the proposed methods can achieve high accuracy for both speaker change detection and gender classification.
\end{abstract}

\section{Introduction}
\label{sec:intro}
Speech translation (ST) aims to convert spoken words in one language to text in another language with high accuracy and fluency. A common approach is to use two components: an automatic speech recognition (ASR) system that converts the speech input into text, and a machine translation (MT) system that translates the text into the target language~\cite{Ney1999ST, Matusov2005ST, Post2013ST}. This method is commonly referred to as cascaded ST.

To avoid error accumulation and fully exploit the audio information, end-to-end (E2E) ST was studied~\cite{vila2018end,sperber2020speech,li2022recent}, which uses a single model to translate directly from speech to text. 
The most well-studied architecture for E2E ST is attention-based encoder-decoder (AED) models~\cite{wang2019token,wang2019large,wang21t_interspeech}, which use an attention layer to combine audio representations and text.
The authors of~\cite{Berard2016ST} suggest using AED models~\cite{chan2015listen,wang21t_interspeech} for a small French-English dataset. A similar model architecture is used in~\cite{weiss2017sequence} for the Fisher Callhome Spanish-English task and achieves better results than the cascaded approach on the Fisher test set. AED-based models are also applied in~\cite{Berard2018ST} for a large-scale E2E ST task. However, AED models usually work in an offline mode that requires the whole utterance to be available before decoding can begin.
% It is difficult to design the streaming strategy for such models.
% It is thus difficult to design the streaming strategy for such models. 
Monotonic chunkwise attention (MoChA)~\cite{chiu2018monotonic} is an attention mechanism that enables efficient and online decoding. Several variants of MoChA have been developed, such as monotonic infinite lookback attention (MILk)~\cite{arivazhagan2019monotonic}, monotonic multi-head attention~\cite{ma2019monotonic, ma2021streaming}, multitask learning~\cite{miao2019online}, and minimum latency training strategies~\cite{inaguma2020minimum}. These methods aim to enhance the performance and robustness of MoChA in various domains and scenarios.
% E2E ST and ASR are similar in that they are both sequence-to-sequence mappings. Many model architectures can thus be shared, especially between ST using monotonic alignments~\cite{raffel2017online} and ASR. To enable more effective communication between users, streaming (i.e., simultaneous) models are topics of investigation in both areas. Monotonic chunkwise attention (MoChA)~\cite{chiu2018monotonic} was used in both MT and ASR. The MT version was extended to monotonic infinite lookback attention (MILk)~\cite{arivazhagan2019monotonic} and monotonic multi-head attention~\cite{ma2019monotonic, ma2021streaming}, and the ASR version was improved by multitask learning~\cite{miao2019online} and minimum latency training strategies~\cite{inaguma2020minimum}. Another streaming model architecture is the neural transducer~\cite{prabhavalkar2017asr,sainath2020asr,li2020asr,saon2021asr}, which outperforms MoChA and has emerged to be the state-of-the-art (SOTA) streaming E2E model in ASR~\cite{li2021recent}, but has been less investigated in ST. Recently, Liu \emph{et al.} proposed cross attention augmented transducer (CAAT) for ST~\cite{liu2021caat}. It uses Transformers in the joint network to combine encoder and prediction network outputs. Due to the use of Transformers and multi-step decision for memory footprint reduction, the latency of CAAT is large. In addition, to train a CAAT, complicated regularization terms and extensive hyper-parameter tuning are required.

Recently, transducer-based models have emerged as promising candidates for end-to-end speech translation (E2E ST)~\cite{Graves-RNNSeqTransduction,prabhavalkar2017asr,sainath2020asr,li2020asr,saon2021asr}, especially in streaming scenarios.
Streaming ST is important for real-time communications, where we do not want to wait until the end of a sentence to perform translation~\cite{papi2024real}.
The transducer loss accounts for all the potential paths from the source audio to the target texts, which may improve the model's convergence.
The authors of~\cite{xue2022large} proposed to use neural transducers for ST, which was later extended to a many-to-many ST and ASR model~\cite{wang2022lamassu} and applied to large-scale datasets with true zero-shot capability~\cite{xue2023weakly}. 
Many methods were proposed to combine AED and neural transducer models. Liu \emph{et al.} proposed cross-attention augmented transducer networks for streaming ST~\cite{liu2021caat}. Tang \emph{et al.} proposed hybrid transducer and attention-based encoder-decoder modeling~\cite{tang2023hybrid}.

One of the main difficulties of applying ST to real-world situations is dealing with multiple talkers~\cite{wang2019speech,wang2020speaker,wang2018utterance}. In video dubbing tasks, the ST models should not only transcribe the speech, but also identify the speaker change points and their genders. In this way, a text-to-speech (TTS) module can generate realistic and consistent audio output. Speaker change detection is particularly important for the state-of-the-art zero-shot TTS models, which rely heavily on the quality of the audio prompt. For conventional TTS models using speaker profiles, gender classification is required to ensure that the selected speaker profile has the correct gender. The speaker change segmentation and the ST text output should be synchronized. This means that existing speaker diarization methods such as 
% Whisper~\cite{radford2023robust} and 
speaker clustering-based method~\cite{park2021review} or neural speaker diarization (e.g., EEND~\cite{fujita2019end,fujita2019end2}) may not be easily applied to this task.

Only a few studies have been conducted on this new challenge. Zuluaga-Gomez \emph{et al.}~\cite{zuluaga2023end} investigated speaker diarization for offline ST based on an AED model. Moreover, Yang \emph{et al.}~\cite{yang2023diarist} proposed t-SOT~\cite{kanda22arxiv} and t-vector~\cite{kanda2022streaming} based streaming multi-talker ST methods, where they evaluated the model based on the speaker diarization task. 

% This paper aims to tackle this new challenge. 
In this paper, we focus on streaming speaker change detection and gender classification, which are essential for streaming speech-to-speech translation models that have an ST frontend and a TTS backend. We note that our methods are not limited to English-to-many (EN-to-many) translation, but can also be applied to many-to-EN translation. For streaming speaker embedding generation, we used the t-vector method~\cite{kanda2022streaming}, which was originally proposed to produce a token-wise speaker embedding for multi-talker speech recognition. 
A speaker change detection and gender classification are performed on the estimated t-vector, which are evaluated with multiple language pairs.
Our work is closely related to that of Yang \emph{et al.}~\cite{yang2023diarist}, with a greater focus on speaker change detection and gender detection, areas that have not been previously investigated.
% We kept the ST part fixed during training to avoid affecting the translation quality. 
% Yang \emph{et al.} proposed t-SOT and t-vector based ST diarization methods~\cite{yang2023diarist}, but they had a different objective than ours. We wanted to use our ST model specifically for a zero-shot TTS backend, so we mainly concentrated on speaker change detection and gender classification. Zuluaga-Gomez \emph{et al.} also investigated speaker diarization for ST using an AED model~\cite{zuluaga2023end}.

% Transducer-based speech translation (ST) models face a critical challenge when applied to real-world scenarios such as video dubbing. In such applications, zero-shot text-to-speech (TTS) models are used to generate realistic audio output, which requires a consistent audio prompt without multiple speakers. Hence, we need to accurately detect and segment speaker changes and classify speaker genders from the input audios. This is a novel task that combines ST and speaker information. In this paper, we propose a streaming method that leverages t-vectors and ST to perform speaker change detection and gender classification.


% End-to-end (E2E) speech translation (ST) is a challenging task that aims to translate speech from one language to another without intermediate steps. Recently, E2E ST models based on transducers have achieved remarkable performance, especially for streaming scenarios. However, these models face difficulties when dealing with multi-speaker audio, where speaker change detection and gender classification are essential for improving translation quality and readability. In this paper, we propose to use speaker embedding as an auxiliary feature for transducer-based E2E ST models. We show that speaker embedding can help the models to detect speaker changes and classify speaker gender in real recordings. We conduct experiments on two multi-speaker ST datasets and demonstrate that our proposed method can improve the translation performance and the speaker diarization accuracy compared to the baseline models.

% Similarly to automatic speech recognition (ASR), ST needs to process multi-talker audio. It is more relevant with zero-shot text-to-speech (TTS), since zero-shot TTS is sensitive to the prompt audio input. 


% % spee
The remainder of this paper is organized as follows. Section \ref{sec:method} presents the proposed method. Section \ref{sec:exp} describes the experimental setup. Section \ref{sec:eval} discusses the results and their implications. Section \ref{sec:conc} summarizes the main contributions.


\section{Method}
\label{sec:method}
First, we introduce multilingual speech translation based on transducers, a method that achieves high-quality streaming speech translation. We then describe the t-vector method. Finally, we show how we combined these two methods for streaming speaker change detection and gender classification.

\subsection{Transducer-based streaming multilingual ST}
\label{ssec:trans_streaming_multilingual}

% We adopt the LAMASSU-UNI approach to support English-to-many ST. It uses language ID as the initial token for the prediction network.
\subsubsection{Transformer transducer}
\label{sssec:trans}
A transducer model consists of three components, as shown in Figure \ref{fig:T-T_fig}: an encoder, a prediction network, and a joint network. The encoder takes $d_x$-dimensional audio features $\textbf{x}_t \in \mathbb{R}^{d_x}$ as input and generates $d_e$-dimensional hidden states $\textbf{h}_t^{\mathrm{enc}} \in \mathbb{R}^{d_e}$. The prediction network uses the embedding of the previous non-blank output token $\textbf{y}_{u-1} \in \mathbb{R}^{1}$ to generate the hidden state $\textbf{h}_u^{\mathrm{pred}} \in \mathbb{R}^{d_p}$ for step $u$. The joint network combines $\textbf{h}_t^{\mathrm{enc}}$ and $\textbf{h}_u^{\mathrm{pred}}$ into a $T \times U$ matrix represented by $\textbf{z}_{t,u} \in \mathbb{R}^{d_z}$, and then applies a softmax function to obtain probabilities for paths that align audio frames with token sequences. The model uses a blank token at the output to handle the alignment between audio and text. During training, the model considers all possible paths and maximizes the probabilities of the correct paths.
In this study, we use Transformer transducer (T-T). It uses Transformer blocks in the encoder, which have a multi-head self-attention layer and a feedforward layer.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/TT_module.png}
    \caption{Illustration of Transformer transducer for ST.}
    \label{fig:T-T_fig}
\end{figure}

\subsubsection{Streaming capability}
\label{sssec:streaming}
The encoder receives the audio input in chunks to enable streaming. To stream with low latency and computation, an attention mask, as in~\cite{xiechen2021tt}, is used. At layer $l$, the input $\textbf{x}^l_{1:T}$ is split into chunks $\textbf{c}^l_{1:S}$ over time with chunk size $U$. At time step $t$, $\textbf{x}^l_t$ only attends to frames in its chunk $\textbf{c}^l_{t / U + 1}$ and $B$ left chunks $\textbf{c}^l_{\max(1, t / U + 1 - B):t / U}$. The reception field at each layer grows linearly with the number of layers, allowing the model to use a longer history for better performance, as shown in Figure \ref{fig:reception_fig}. The frames cannot see frames outside their chunk, keeping a fixed number of look-ahead frames.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/reception_field.png}
    \caption{Illustration of the reception field of a streaming T-T at position $f_{10}$ with chunk size 3 and the number of left chunks 1.}
    \label{fig:reception_fig}
\end{figure}

\subsubsection{Multilingual capability}
\label{sssec:multilingual}
We adopt the LAMASSU-UNI approach~\cite{wang2022lamassu}, which is illustrated in Fig \ref{fig:multilingual_fig}, to perform speech translation from English to various other languages, by providing the prediction network with a starting token that specifies the language. The language identifications (LIDs) are appended to the vocabulary list and are treated as special tokens.


% Fig. \ref{fig:transducer_single_decoder} shows LAMASSU using a unified prediction and joint network for multiple target languages (LAMASSU-UNI). During training, it uses target LID to replace the start of sentence token ($<SOS>$) in the input to the prediction network. At test time, target LID is fed to the prediction network as the initial token.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.36\textwidth]{figures/transducer_single_decoder.png}
    \caption{Illustration of LAMASSU-UNI.}
    \label{fig:multilingual_fig}
\end{figure}


\subsection{t-vector for ST}
\label{ssec:tvec}
A t-vector is a type of speaker embedding vector that captures the speaker characteristics at the token level~\cite{kanda2022streaming}. It is based on the d-vector, which is obtained from audio segments. A t-vector module is typically added to a transducer model that has been well-trained. It was first developed for multi-talker ASR and has been recently used for ST tasks~\cite{yang2023diarist}.


\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/t-vector.png}
    \caption{Illustration of t-vector model for ST.}
    \label{fig:t_vec}
\end{figure}

The t-vector model, which is built on top of an ST model, is illustrated in Figure \ref{fig:t_vec}. The ST model is fixed during training, as depicted by the gray box. The t-vector module consists of a speaker encoder and a speaker decoder. The speaker encoder has multi-head attention layers that use external attention to extract the speaker information from the lower layer and the corresponding ST encoder layer. Specifically, the attention layers generate their own key and query from the lower layer, and use the output of the corresponding ST encoder layer as value. Figure \ref{fig:speaker_encoder} shows the details of the speaker encoder layers. The output of the speaker encoder, together with the embedding of the output non-blank token, is fed to the speaker decoder. The speaker decoder has two long short-term memory (LSTM) layers, whose output is passed through a linear layer to produce t-vectors.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{figures/speaker_encoder_layers.png}
    \caption{Illustration of the speaker encoder layers. The speaker ID extractor is typically a d-vector extractor.}
    \label{fig:speaker_encoder}
\end{figure}

For each output non-blank token of the ST model, t-vectors are generated in a streaming manner during inference. Unlike Yang \emph{et al.}~\cite{yang2023diarist}, we do not need speaker clustering for speaker diarization for this task, as our focus is on speaker change detection and gender classification. Moreover, since we did not change the ST part during t-vector training, the translation performance remains unaffected.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/scd.png}
    \caption{Illustration of speaker change detection using t-vectors. $w1$ to $w7$ denote non-blank output tokens. If the cosine similarity between adjacent tokens is below the threshold, we insert an $\langle\mathrm{SC}\rangle$ token to the output.}
    \label{fig:scd}
\end{figure*}

\subsection{t-vector for speaker change detection and gender classification}
\label{ssec:scd_gc}
In this section, we explain how we use transducer-based streaming multilingual ST and t-vector to perform streaming speaker change detection and gender classification.
Our model is data-efficient because it treats the speaker change detection problem as a speaker identification (SID) generation task. This means it does not rely on a large amount of real-world training data, which is difficult to acquire and may not cover all possible scenarios.

% We use these tasks to select the best audio segments for zero-shot TTS.
% Zero-shot TTS models require high-quality audio prompts to synthesize speech in various voices. To filter out the noisy or unsuitable prompts, our model performs two tasks: speaker change detection and gender classification.

\subsubsection{Speaker change detection}
\label{sssec:scd}
We use the cosine similarity between two adjacent t-vectors to detect a speaker change. If the similarity value is lower than a threshold, we assume that a different speaker is speaking. 
Our method does not require model retraining for the ST model part, and therefore can preserve the ST model performance. An alternative way to perform speaker change detection is to train a model with real conversations that have speaker changes. However, collecting such training data is challenging. If we use simulated data, the model tends to learn the channel variations between different audio segments rather than the speaker differences. Therefore, we choose the t-vector method for our study.

\subsubsection{Gender classification}
\label{sssec:gc}
% Correct gender classification ensures that the output audio from the TTS backend is distinguishable 
In conventional TTS systems, a specific speaker profile is chosen for each output audio segment. While these systems cannot retain all the audio nuances like zero-shot TTS, they are essential for on-device deployment to prevent malicious use of zero-shot TTS. To ensure natural-sounding conversations, it's crucial to accurately classify the gender of speakers for each audio segment; otherwise, listeners might become confused in understanding the translated conversations. Since we already produce t-vectors for detecting speaker changes, we aim to utilize this information for gender classification too. We begin by gathering a collection of speaker profiles categorized as male or female. We then calculate the cosine similarity between the t-vector of each token and the male and female speaker profiles. Lastly, we determine the gender of the token by selecting the gender with the highest cosine similarity score.
% We note that there are other methods for gender classification based on audio features, such as fundamental frequency detection, but they require aligning the translated text with the corresponding audio after processing the time-domain information. This is challenging for streaming ST tasks, where the output text is not monotonic. Therefore, our t-vector-based method is preferable because it operates on the token level.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.82\textwidth]{figures/gc.png}
    \caption{Illustration of gender classification using t-vectors. See the caption of Figure \ref{fig:scd} for the definitions of $w1$ to $w7$. The male and female speaker profiles contain d-vectors. $F$ and $M$ denote female and male, respectively. 
    % For each t-vector, we compare the cosine similarities with male and female speaker profiles, and assign $F$ or $M$ estimations corresponding to the largest cosine similarities between the two categories.
    }
    \label{fig:gc}
\end{figure*}

\section{Experimental setup}
\label{sec:exp}

\subsection{Data}
\label{ssec:exp_data}
\subsubsection{Training data}
\label{sssec:exp_data_training}
We have about 75K hours of English audio in the training data for the ST model. The audio is collected from various sources and is anonymized. For each of the five output languages Germany (DE), Spanish (ES), Hindi (HI), Italian (IT), and Russian (RU), we use text machine translation models to generate texts with pseudo-labels. 

We train the t-vector model using Voxceleb~\cite{nagrani2017voxceleb} as the training data and use our well-trained ST model to generate the label texts in each target language. We then apply a Viterbi algorithm to align the tokens and the audio frames. The t-vector training stage uses the alignment produced by this step to associate the t-vector with the non-blank tokens. This allows a cross-entropy training for the t-vector model that is memory-efficient and also produces the t-vector together with the output token during inference.

\subsubsection{Test data}
\label{sssec:exp_data_test}
Our test data consisted of 5 real recorded long audios. The test samples had an average duration of about 30 minutes.
We collected real conversational audios with various numbers of speakers, up to eight, for speaker change detection. Human annotators segmented the audio and marked the speaker change points at the segment boundaries. We concatenated the segments into audio samples, each containing one speaker change. The total number of samples was 688, with an average duration of about 5 seconds. Some samples were 30 seconds or longer.
For gender classification, we divided all the speaker profiles in the training set of Voxceleb-1~\cite{nagrani2017voxceleb} and Voxceleb-2~\cite{nagrani2020voxceleb} into two gender categories: male and female. We used the test set of Voxceleb-1 as the test audio. The total number of utterances we tested was 4874.

\subsection{Model}
\label{ssec:exp_model}
We applied a streaming multilingual T-T model that performs EN-to-many speech translation with a chunk size of 1s and 18 history chunks. The model has 12 Transformer encoder layers and about 100 million (M) parameters. The five output languages use a shared vocabulary of about 18587 tokens. We also added a language ID for each language to the vocabulary list. The total number of tokens, including $\langle\mathrm{EOS}\rangle$ and $\langle\mathrm{blank}\rangle$, is 18594.
We trained the model for 96M steps, with a peak learning rate of 3e-4 and 0.8M warm-up steps. We used fp16 in Deepspeed and 32 32G GPUs for training. For inference, we used PyTorch decoding with a beam size of 8.

The t-vector model consists of a speaker encoder and a speaker decoder. The speaker encoder uses a Res2Net module for SID extraction~\cite{yang2023diarist}. It has the same number of encoder layers as the ST encoder, which is 12 in our study. The attention dimension and the attention head are 128 and 8, respectively. The speaker decoder is a two-layer LSTM model with a hidden size of 512 and an input size of 128. The t-vector dimension is 128. We fix the ST model parameters during training. We also initialize the SID module inside the encoder with a pre-trained SID model, which is trained on the Voxceleb-1~\cite{nagrani2017voxceleb} and Voxceleb-2~\cite{nagrani2020voxceleb} training sets. The SID module remains fixed during training.

\section{Evaluation results}
\label{sec:eval}
% \subsection{Speech Translation}
% \label{ssec:eval_st}
% As shown in Table \ref{tab:st}, our transducer-based streaming ST model achieves high BLEU scores on this real-world test set, using a very small 100M model size and 1s chunk size. We also include the offline cascaded ASR and MT results for comparison. However, the cascaded model is not suitable for streaming translation, as it requires rewriting the translation output.

% \begin{table}[]
%     \centering
%     \begin{tabular}{c | c c }
%         \toprule
%          language & offline & ours \\
%          \midrule
%          EN-DE & 28.5 & 27.1 \\
%          EN-ES & 31.4 & 30.8 \\
%          % EN-HI & - & 35.7 \\
%          EN-IT & 26.0 & 26.3 \\
%          EN-RU & 23.6 & 21.7 \\
%          \midrule
%          Avg. & 27.4 & 26.5 \\
%          \bottomrule
%     \end{tabular}
%     \caption{BLEU scores of the offline cascaded model and our transducer-based streaming ST model. Note that our offline cascaded model does not support EN-HI translation. The BLEU score of our model is 35.7.}
%     \label{tab:st}
% \end{table}

% \begin{algorithm}[Htb]
% \SetAlgoLined
% \SetKwInOut{Input}{Input}
% \SetKwInOut{Output}{Output}
% \SetKwFunction{Function}{Function}
% \Input{(1) Detected speaker change time $\mathcal{D}$, $\delta \in \mathcal{D}$ denotes all the detected speaker change timestamps in an utterance, and $d \in \delta$ refers to each detected speaker change time; (2) Reference speaker change time $\mathcal{R}$, $r[\delta] \in \mathcal{R}$ denotes the reference timestamp corresponding to $\delta$; (3) Tolerance time range $tr$.}
% \Output{(1) $Recall$; (2) $Precision$; (3) $F_1$.}

%  $total\_count \leftarrow 0$\;
%  $recall\_count \leftarrow 0$\;
%  $total\_alarm\_count \leftarrow 0$\;
%  $false\_alarm\_count \leftarrow 0$\;

%  \For{each $\delta$ \in $\mathcal{D}$}{
%     % $r[d]$ \leftarrow $r$ \in $\mathcal{R}$\;
%     $total\_count += 1$\;
%     \If { \textbf{recall}($\delta$, $r[\delta]$, $tr$) is True
%     }
%     {
%     $recall\_count += 1$\;
%     }
%     $false\_alarm$, $alarm$ \leftarrow \textbf{precision}($\delta$, $r[\delta]$, $tr$)\;\\
%     $false\_alarm\_count$ += $false\_alarm$\;
%     $total\_alarm\_count$ += $alarm$\;
%  }
%  $Recall$ \leftarrow $recall\_count$ / $total\_count$\;\\
%  $Precision$ \leftarrow 1 - $false\_alarm\_count$ / $total\_alarm\_count$\;\\
%  $F_1$ \leftarrow 2 * $Recall$ * $Precision$ / ($Recall$ + $Precision$)\;

%  \textbf{Function 1:} \textbf{recall}($\delta$, $r[\delta]$, $tr$)\\
%  \For{d \in \delta}{
%  \If{$d$ >= $r[\delta]$ - $tr$ and $d$ <= $r[\delta]$ + $tr$}{
%   \textbf{return} True\;
%  }
%  }
%  \textbf{return} False\;\\

% \textbf{Function 2:} \textbf{precision}($\delta$, $r[\delta]$, $tr$)\\
%  false\_alarm\_count \leftarrow 0\;\\

%  \For{d \in \delta}{
%  \If{$d$ < $r[\delta]$ - $tr$ or $d$ > $r[\delta]$ + $tr$}{
%   false\_alarm\_count += 1\;
%  }
%  }
%  \textbf{return} false\_alarm\_count, |\delta|\;
% \caption{Speaker change detection metrics calculation.}
% \label{algo:scd_metrics_calculation}
% \end{algorithm}

\begin{table*}[]
    \centering
    \begin{tabular}{c c | c c c c c | c }
        \toprule
         threshold & metrics & EN-DE & EN-ES & EN-HI & EN-IT & EN-RU & Avg. \\
         \midrule
              & recall & 0.89 & 0.91 & 0.90 & 0.88 & 0.89 & 0.89 \\
         0.99 & precision & 0.55 & 0.55 & 0.55 & 0.55 & 0.55 & 0.55 \\
              & ${F_1}$ & 0.68 & 0.69 & 0.68 & 0.67 & 0.68 & 0.68 \\
         \midrule
              & recall & 0.63 & 0.67 & 0.63 & 0.67 & 0.65 & 0.65 \\ 
         0.94 & precision & 0.66 & 0.69 & 0.67 & 0.68 & 0.69 & 0.68 \\
              & ${F_1}$ & 0.65 & 0.68 & 0.65 & 0.67 & 0.67 & 0.66 \\
         \midrule
              & recall & 0.49 & 0.53 & 0.51 & 0.52 & 0.53 & 0.52 \\
         0.89 & precision & 0.74 & 0.77 & 0.74 & 0.77 & 0.77 & 0.76 \\
              & ${F_1}$ & 0.59 & 0.63 & 0.60 & 0.62 & 0.62 & 0.61 \\
         \bottomrule
    \end{tabular}
     \caption{Speaker change detection results using different thresholds and time range +/-2s. The recall, precision, and ${F_1}$ of Whisper is 0.85, 0.69, and 0.76. Those of EEND is 0.60, 0.88, and 0.71, respectively.}
    \label{tab:scd}
\end{table*}

\begin{table*}[]
    \centering
    \begin{tabular}{c | c c c c c | c}
        \toprule
           language & EN-DE & EN-ES & EN-HI & EN-IT & EN-RU & Avg. \\
        \midrule
         accuracy & 0.989 & 0.989 & 0.989 & 0.989 & 0.989 & 0.99 \\
         \bottomrule
    \end{tabular}
    \caption{Token-level gender classification accuracy for different languages.}
    \label{tab:gc}
\end{table*}

\subsection{Speaker change detection}
\label{ssec:eval_scd}
A sample translation result with speaker change labels and the corresponding timestamps for text space speaker change detection is shown in Table \ref{tab:output_sentences}. Since the output is not deterministic, it is difficult to design a reference speaker change metric for translation output. Therefore, we only present a sample translation result with the speaker change label and the corresponding timestamp in Table \ref{tab:output_sentences}. We can see that the speaker change label $\langle\mathrm{SC}\rangle$ is correctly assigned to different translation results using the proposed method. The exact values of the timestamps vary slightly, but the differences are small. In fact, the difference between EN-DE and others is 1 frame. The estimated timestamps corresponding to the $\langle\mathrm{SC}\rangle$ tokens in both Table \ref{tab:output_sentences} and \ref{tab:scd} are calculated by finding the first frame generating $\langle\mathrm{SC}\rangle$ in the best hypothesis, and multiplying the frame index with frame shift, which is 0.04 second (sec) in our study.

\begin{table}[!ht]
    \setlength{\tabcolsep}{2pt}
    \centering
    \caption{A sample translation output with speaker change labels $\langle\mathrm{SC}\rangle$. 
    % The first utterance is from a male speaker and the content in English is ``It is obviously unnecessary for us to point out how luminous these criticisms are, how delicate in expression.'' The second utterance is from a female speaker and the content is ``Mister Morton then made a careful memorandum of the various particulars of Waverley's interview with Donald Bean Lean and the other circumstances which he had communicated.''. 
    % The first utterance is from a male speaker and the second is from a female speaker.
    The ground truth speaker change time is at about 8.7 sec. EN-RU and EN-HI have the same $\langle\mathrm{SC}\rangle$ locations and timestamps as those of EN-ES and EN-IT.}
    % The cosine threshold for speaker change detection is 0.94 for all the samples below except EN-RU, whose threshold is 0.77.}
    \label{tab:output_sentences}
    \begin{tabular}{@{}l p{0.74\linewidth}@{}}
        % \small
        \toprule
        language & sentence \\
        \midrule
        EN-DE &  Es ist offensichtlich unnötig darauf hinweisen, wiellos diese Kritik, wie zart im Ausdruck.\\
              & $\langle\mathrm{SC}\rangle$	8.68 sec\\
              & Herr Morton machte dann ein sorgfältiges Memorandum über die verschiedenen Einzelheiten von Waverlys Interview mit Donald Beam Lane. ...\\
              % Und die anderen Umstände, die er kommunizierte. \\
        \midrule
        EN-ES &   Obviamente es innecesario para nosotros señalar cuán brillante son estas críticas, cuán delicadas son expresión.\\
              & $\langle\mathrm{SC}\rangle$	8.72 sec\\
              & Sr. Morton luego hizo un memorándum cuidadoso de los diversos detalles de la entrevista de Waverly con Donald Beam Lane. ...\\
              % Y las otras circunstancias que había comunicado.\\
        \midrule
        % EN-HI &  "यह स्पष्ट रूप से हमारे लिए अनावश्यक है कि अभिव्यक्ति कितनी नाजुक हैं।" \\
              % & $\langle\mathrm{SC}\rangle$	8.72sec\\
              % & { श्री मॉर्टन ने वेवरली के साक्षात्कार के साथ डोनाल्ड बीम के साथ साक्षात्कार की एक सावधानीपूर्वक याद दिलाया। और अन्य परिस्थितियों में उन्होंने संवाद किया था।} \\
        EN-IT &  Ovviamente è inutile per noi sottolineare quanto siano luminosi queste critiche, quanto siano delicate un'espressione.\\
              & $\langle\mathrm{SC}\rangle$	8.72 sec\\
              & Il signor Morton poi ha fatto un attento memorandum dei vari particolari dell'intervista di Waverly con Donald Beam Lean. ...\\
              % E le altre circostanze che aveva comunicato.\\
        % \midrule
        % EN-RU & \selectlanguage{russian}
        %     Очевидно, что мы отметили, насколько ярко эти критики, насколько нежны выражения.\\
        %       & $\langle\mathrm{SC}\rangle$	8.72sec\\
        %       & \selectlanguage{russian} Затем мистер Мортон сделал тщательный меморандум о различных частях интервью Уэверли с Дональдом Бэмом. ...\\
        %       % И другие обстоятельства, о которых он сообщил.\\
        \bottomrule
    \end{tabular}
\end{table}

In addition to text space speaker change detection above, we also perform audio space speaker change detection. We use three metrics to measure the performance of speaker change detection in the time domain: recall, precision, and ${F_1}$ score. 
% Since our evaluation is based on the time domain, which has continuous values, we need a special algorithm for this task. Algorithm 1 shows how we do it. 
We define a tolerance time range for each speaker change estimation. A detected speaker change is correct if it is within the time range of the reference speaker change timestamp.

% \begin{algorithm}[Htb]
% \SetAlgoLined
% \SetKwInOut{Input}{Input}
% \SetKwInOut{Output}{Output}
% \Input{(1) t-vectors for the whole test set $\mathcal{T}$, where $\tau \in \mathcal{T}$ for all the t-vectors in one sample, and $t \in \tau$ for the t-vector of a token; (2) Male speaker profiles $M$; (3) Female speaker profiles $F$; (4) Reference gender $R$, where $r[t]$ denotes the gender of each token.}
% \Output{(1) $Accuracy$.}

%  $correct\_count \leftarrow 0$\;
%  $total\_count \leftarrow 0$\;

%  \For{each $t$ \in $\mathcal{T}$}{
%     % $r[d]$ \leftarrow $r$ \in $\mathcal{R}$\;
%     $total\_count += 1$\;
%     \If { \textbf{gender}($t$, $M$, $F$) == $r[t]$}{
%     $correct\_count += 1$\;
%     }
%  }
%  $Accuracy$ \leftarrow $correct\_count$ / $total\_count$\;

%  \textbf{Function:} \textbf{gender}($t$, $M$, $F$)\\
%   $male\_similarity$ \leftarrow \textbf{cosine\_similarity}($t$, $M$)\;\\
%   $female\_similarity$ \leftarrow \textbf{cosine\_similarity}($t$, $F$)\;\\
%   \If {\textbf{max}($male\_similarity$) > \textbf{max}($female\_similarity$)} {
%   \textbf{return} $Male\_tag$\;
%   }
%   \textbf{return} $Female\_tag$\;
% \caption{Gender tagging and accuracy calculation.}
% \label{algo:gender_tag}
% \end{algorithm}

% \begin{table*}[]
%     \centering
%     \begin{tabular}{c c | c c c c c | c }
%         \toprule
%          threshold & metrics & EN-DE & EN-ES & EN-HI & EN-IT & EN-RU & Avg. \\
%          \midrule
%               & recall & 0.89 & 0.91 & 0.90 & 0.88 & 0.89 & 0.89 \\
%          0.99 & precision & 0.55 & 0.55 & 0.55 & 0.55 & 0.55 & 0.55 \\
%               & ${F_1}$ & 0.68 & 0.69 & 0.68 & 0.67 & 0.68 & 0.68 \\
%          \midrule
%               & recall & 0.63 & 0.67 & 0.63 & 0.67 & 0.65 & 0.65 \\ 
%          0.94 & precision & 0.66 & 0.69 & 0.67 & 0.68 & 0.69 & 0.68 \\
%               & ${F_1}$ & 0.65 & 0.68 & 0.65 & 0.67 & 0.67 & 0.66 \\
%          \midrule
%               & recall & 0.49 & 0.53 & 0.51 & 0.52 & 0.53 & 0.52 \\
%          0.89 & precision & 0.74 & 0.77 & 0.74 & 0.77 & 0.77 & 0.76 \\
%               & ${F_1}$ & 0.59 & 0.63 & 0.60 & 0.62 & 0.62 & 0.61 \\
%          \bottomrule
%     \end{tabular}
%      \caption{Speaker change detection results using different thresholds and time range +/-2s.}
%     \label{tab:scd}
% \end{table*}

% \begin{table*}[]
%     \centering
%     \begin{tabular}{c | c c c c c | c}
%         \toprule
%            language & EN-DE & EN-ES & EN-HI & EN-IT & EN-RU & Avg. \\
%         \midrule
%          accuracy & 0.989 & 0.989 & 0.989 & 0.989 & 0.989 & 0.99 \\
%          \bottomrule
%     \end{tabular}
%     \caption{Token-level gender classification accuracy for different languages.}
%     \label{tab:gc}
% \end{table*}

% \begin{algorithm}[Htb]
% \SetAlgoLined
% \SetKwInOut{Input}{Input}
% \SetKwInOut{Output}{Output}
% \SetKwFunction{Function}{Function}
% \Input{(1) Detected speaker change time $\mathcal{D}$, $\delta \in \mathcal{D}$ denotes all the detected speaker change timestamps in an utterance, and $d \in \delta$ refers to each detected speaker change time; (2) Reference speaker change time $\mathcal{R}$, $r[\delta] \in \mathcal{R}$ denotes the reference timestamp corresponding to $\delta$; (3) Tolerance time range $tr$.}
% \Output{(1) $Recall$; (2) $Precision$; (3) $F_1$.}

%  $total\_count \leftarrow 0$\;
%  $recall\_count \leftarrow 0$\;
%  $total\_alarm\_count \leftarrow 0$\;
%  $false\_alarm\_count \leftarrow 0$\;

%  \For{each $\delta$ \in $\mathcal{D}$}{
%     % $r[d]$ \leftarrow $r$ \in $\mathcal{R}$\;
%     $total\_count += 1$\;
%     \If { \textbf{recall}($\delta$, $r[\delta]$, $tr$) is True
%     }
%     {
%     $recall\_count += 1$\;
%     }
%     $false\_alarm$, $alarm$ \leftarrow \textbf{precision}($\delta$, $r[\delta]$, $tr$)\;\\
%     $false\_alarm\_count$ += $false\_alarm$\;
%     $total\_alarm\_count$ += $alarm$\;
%  }
%  $Recall$ \leftarrow $recall\_count$ / $total\_count$\;\\
%  $Precision$ \leftarrow 1 - $false\_alarm\_count$ / $total\_alarm\_count$\;\\
%  $F_1$ \leftarrow 2 * $Recall$ * $Precision$ / ($Recall$ + $Precision$)\;

%  \textbf{Function 1:} \textbf{recall}($\delta$, $r[\delta]$, $tr$)\\
%  \For{d \in \delta}{
%  \If{$d$ >= $r[\delta]$ - $tr$ and $d$ <= $r[\delta]$ + $tr$}{
%   \textbf{return} True\;
%  }
%  }
%  \textbf{return} False\;\\

% \textbf{Function 2:} \textbf{precision}($\delta$, $r[\delta]$, $tr$)\\
%  false\_alarm\_count \leftarrow 0\;\\

%  \For{d \in \delta}{
%  \If{$d$ < $r[\delta]$ - $tr$ or $d$ > $r[\delta]$ + $tr$}{
%   false\_alarm\_count += 1\;
%  }
%  }
%  \textbf{return} false\_alarm\_count, |\delta|\;
% \caption{Speaker change detection metrics calculation.}
% \label{algo:scd_metrics_calculation}
% \end{algorithm}

% Table \ref{tab:scd} shows the recall, precision, and ${F_1}$ scores of the t-vector based speaker change detection model. 
Table \ref{tab:scd} shows the recall, precision, and $F_1$ scores of the proposed method with different thresholds. The $F_1$ scores are all above 0.6 for the three thresholds of 0.99, 0.94, and 0.89. As the threshold increases, the recall improves but the precision declines. This is expected because the cosine similarity values are more likely to be below the threshold. Our test data contains noise and silence, which are challenging for the model to handle since it is not trained with such type of data. We observed that most incorrect speaker change detection points occur at noisy or silent points.
The proposed method is streaming with a chunk size of 1s, which means that speaker change detection can be estimated with very low latency. Moreover, since speaker change detection on translated text is also required for the subsequent TTS module, it is hard for traditional methods such as Whisper~\cite{radford2023robust} and EEND~\cite{Plaquet2023} to align the speaker change time with text output. Therefore, these methods are not comparable with the proposed t-vector-based method, which operates directly in token space. For comparison, we also computed the metrics for two offline models. Whisper got recall: $0.85$, precision: $0.69$ and ${F_1}$: $0.76$. EEND obtained recall: $0.60$, precision: $0.88$ and ${F_1}$: $0.71$.
Our streaming speaker change detection model achieved similar ${F_1}$ scores as the two offline models. In addition, unlike Whisper and EEND, the proposed method does not need additional steps to align the speaker change timestamp and the translated text.
% Note that Whisper is an offline model and does not support English-to-many ST. Therefore, it needs additional steps to map the speaker change timestamp to translated text.

% \begin{table*}[]
%     \centering
%     \begin{tabular}{c c | c c c c c | c}
%         \toprule
%          threshold & metrics & EN-DE & EN-ES & EN-HI & EN-IT & EN-RU & Avg. \\
%          \midrule
%               & recall & 0.89 & 0.91 & 0.90 & 0.88 & 0.89 & 0.89 \\
%          0.99 & precision & 0.55 & 0.55 & 0.55 & 0.55 & 0.55 & 0.55 \\
%               & ${F_1}$ & 0.68 & 0.69 & 0.68 & 0.67 & 0.68 & 0.68 \\
%          \midrule
%               & recall & 0.63 & 0.67 & 0.63 & 0.67 & 0.65 & 0.65 \\
%          0.94 & precision & 0.66 & 0.69 & 0.67 & 0.68 & 0.69 & 0.68 \\
%               & ${F_1}$ & 0.65 & 0.68 & 0.65 & 0.67 & 0.67 & 0.66 \\
%          \midrule
%               & recall & 0.49 & 0.53 & 0.51 & 0.52 & 0.53 & 0.52 \\
%          0.89 & precision & 0.74 & 0.77 & 0.74 & 0.77 & 0.77 & 0.76 \\
%               & ${F_1}$ & 0.59 & 0.63 & 0.60 & 0.62 & 0.62 & 0.61 \\
%          \bottomrule
%     \end{tabular}
%     \caption{Speaker change detection results using different thresholds and time range +/-2s.}
%     \label{tab:scd}
% \end{table*}

% \begin{table*}[]
%     \centering
%     \begin{tabular}{c | c c c c c | c}
%         \toprule
%             & EN-DE & EN-ES & EN-HI & EN-IT & EN-RU & Avg. \\
%         \midrule
%          accuracy & 0.989 & 0.989 & 0.989 & 0.989 & 0.989 & 0.99 \\
%          \bottomrule
%     \end{tabular}
%     \caption{Token-level gender classification accuracy for different languages.}
%     \label{tab:gc}
% \end{table*}

\subsection{Gender classification}
\label{ssec:eval_gc}
% We achieved high accuracy by using the cosine similarity values, without needing more complex methods such as $k$-nearest neighbors with $k>1$. Moreover, many methods work in the time domain and need extra steps to match the variable ST output. Hence, we did not include those methods in our comparison.
As shown in Table \ref{tab:gc}, the token-level gender classification accuracy, which also accounts for punctuation marks, was 0.989 across different languages. This indicates that the proposed method is robust to output languages and has a high accuracy for gender classification.
% \begin{algorithm}[Htb]
% \SetAlgoLined
% \SetKwInOut{Input}{Input}
% \SetKwInOut{Output}{Output}
% \Input{(1) t-vectors for the whole test set $\mathcal{T}$, where $\tau \in \mathcal{T}$ for all the t-vectors in one sample, and $t \in \tau$ for the t-vector of a token; (2) Male speaker profiles $M$; (3) Female speaker profiles $F$; (4) Reference gender $R$, where $r[t]$ denotes the gender of each token.}
% \Output{(1) $Accuracy$.}

%  $correct\_count \leftarrow 0$\;
%  $total\_count \leftarrow 0$\;

%  \For{each $t$ \in $\mathcal{T}$}{
%     % $r[d]$ \leftarrow $r$ \in $\mathcal{R}$\;
%     $total\_count += 1$\;
%     \If { \textbf{gender}($t$, $M$, $F$) == $r[t]$}{
%     $correct\_count += 1$\;
%     }
%  }
%  $Accuracy$ \leftarrow $correct\_count$ / $total\_count$\;

%  \textbf{Function:} \textbf{gender}($t$, $M$, $F$)\\
%   $male\_similarity$ \leftarrow \textbf{cosine\_similarity}($t$, $M$)\;\\
%   $female\_similarity$ \leftarrow \textbf{cosine\_similarity}($t$, $F$)\;\\
%   \If {\textbf{max}($male\_similarity$) > \textbf{max}($female\_similarity$)} {
%   \textbf{return} $Male\_tag$\;
%   }
%   \textbf{return} $Female\_tag$\;
% \caption{Gender tagging and accuracy calculation.}
% \label{algo:gender_tag}
% \end{algorithm}

% \begin{table*}[]
%     \centering
%     \begin{tabular}{c | c c c c c | c}
%         \toprule
%             & EN-DE & EN-ES & EN-HI & EN-IT & EN-RU & Avg. \\
%         \midrule
%          accuracy & 0.989 & 0.989 & 0.989 & 0.989 & 0.989 & 0.99 \\
%          \bottomrule
%     \end{tabular}
%     \caption{Token-level gender classification accuracy for different languages.}
%     \label{tab:gc}
% \end{table*}

% As shown in Table \ref{tab:gc}, we applied the t-vector method with the T-T based ST model to perform gender classification. The token-level gender classification accuracy, which also accounts for punctuation marks, was 0.989 across different languages. This indicates that the proposed method is robust to output languages and has a high accuracy for gender classification.


% \begin{table*}[]
%     \centering
%     \begin{tabular}{c | c c c c c | c}
%         \toprule
%             & EN-DE & EN-ES & EN-HI & EN-IT & EN-RU & Avg. \\
%         \midrule
%          accuracy & 0.989 & 0.989 & 0.989 & 0.989 & 0.989 & 0.99 \\
%          \bottomrule
%     \end{tabular}
%     \caption{Token-level gender classification accuracy for different languages.}
%     \label{tab:gc}
% \end{table*}


\section{Conclusions}
\label{sec:conc}
This paper presents a novel challenge and a solution for streaming multi-talker ST. We combine a transducer-based multilingual ST system with a t-vector module that can identify speaker changes and gender in real time. By comparing the cosine similarity between t-vectors, our method can effectively address the speaker change detection and gender classification problems.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

% \appendix

% \section{Example Appendix}
% \label{sec:appendix}

% This is an appendix.

\end{document}
