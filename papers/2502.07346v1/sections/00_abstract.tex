\begin{abstract}
Previous multilingual benchmarks focus primarily on simple understanding tasks, but for large language models~(LLMs), we emphasize proficiency in instruction following, reasoning, long context understanding, code generation, and so on.
However, measuring these advanced capabilities across languages is underexplored.
To address the disparity, we introduce \name, a multi-way multilingual evaluation benchmark that allows for fair comparisons of these important abilities across languages. 
To maintain high quality, three distinct native-speaking annotators independently annotate each sample within all tasks after the data was machine-translated from English into 16 other languages. 
Additionally, we present a novel translation challenge stemming from dataset construction.
Extensive experiments on \name reveal varying effectiveness of core capabilities across languages, highlighting performance gaps that cannot be bridged by simply scaling up model size.
\name serves as a comprehensive multilingual evaluation platform, providing a promising test bed to promote the development of multilingual language models. The dataset\footnote{\url{https://huggingface.co/collections/LLaMAX/benchmax-674d7a815a57baf97b5539f4}} and code\footnote{\url{https://github.com/CONE-MT/BenchMAX.git}} are publicly accessible.

% mere model scaling fails to resolve. 
% \name pays more attention to multilingual variations of LLM core capabilities, which provides a promising test bed and hence promotes further development to bridge the gap between languages~\footnote{The dataset and evaluation code will be publicly accessible}.




% Constructing appropriate benchmarks to evaluate the key capabilities of large language models is crucial for further development. There is a series of works targeting the core capabilities of large language models such as math and code reasoning, long context understanding, and instruction following, 
% We find that even among the advanced large language models, there remains a significant performance gap across different languages, and merely scaling the model size does not consistently mitigate the gap. Our work provides a promising multilingual test bed for evaluating frontier capabilities and hence promotes further development to bridge the gap between languages.
\end{abstract}