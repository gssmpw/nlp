\section{Tasks}
Currently, the key capabilities of LLMs people care about have witnessed a major shift.
These abilities, including instruction following, code generation, long context processing, reasoning, and tool use, are of a more advanced and intricate nature than than before.
We are interested in whether these capabilities will be affected in the multilingual scenarios.
Machine translation, a traditional task, is also considered to be a useful method for assessing the multilingual capabilities.
% To evaluate the multilingual capabilities of LLMs comprehensively, we include the following six categories of tasks.
We develop BenchMaX to evaluate the multilingual capabilities of LLMs comprehensively
It is fully parallel across 17 languages and most of tasks are annotated by native speakers.
% Each category contains one to two tasks that we think important and valuable, and each task contains parallel testsets with 17 languages.
The tasks we think important and valuable are grouped into the following six categories.
The general pipeline for constructing the testsets involves first translating samples from English into other languages using machine translation systems, and then apply manual post-editing by native speakers.

\subsection{Instruction Following}
\subsubsection{IFEval}
\paragraph{Task description}
IFEval~\cite{zhou2023instruction} is a benchmark for evaluating the instruction following abilities of LLMs. 
It is composed of around 500 verifiable instructions, the responses to which can be verified via the application of automated rules.
For instance, instructions may require responses to contain specific keywords or a minimum number of words.
We are curious about how LLMs will perform on the multilingual instructions.

\paragraph{Data construction}
We first filter out some English-specific instructions from the original dataset, such as changing the letter cases.
After filtering, the number of remaining samples is 429.
The next problem is how to extract the keywords from the translated instruction since the keywords are also translated and are required in the verification step.
Inspired by \citet{yuan-etal-2020-enhancing}, we enclose the keywords in the original instruction with special marks, as illustrated in Figure~\ref{fig:ifeval}, thus they can be easily extracted from the translated instruction.
In addition, the number-word constraints for non-English languages are multiplied by a ratio in order to make the difficulty of the same instruction in different languages comparable.
Specifically, we calculate the ratio of the word count of English to that of a non-English language in the Flores-101 corpus.
When counting the number of words, the sentences are tokenized using the language-specific tokenizer.

During post-editing, we ask human annotators to check whether the translated keywords in the kwargs, which are used by the rule-based program, appear in the translated instruction.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figure/ifeval.pdf}
    \caption{The translation process of an example in IFEval.}
    \label{fig:ifeval}
\end{figure}

\paragraph{Evaluation metric}
Following Llama3~\cite{dubey2024llama}, we compute the average of the four accuracy scores used in IFEval: Prompt-level strict-accuracy, Inst-level strict-accuracy, Prompt-levelloose-accuracy and Inst-level loose-accuracy.
Note that we also slightly modify the verification rules, like how to check the punctuation or count sentences, to better adapt to multiple languages involved.

\subsubsection{Arena-Hard}
\paragraph{Task description}
IFEval only evaluates the instruction following ability at a low level, without considering the overall quality of the generated texts.
As a supplement, we add the Arena-hard~\cite{li2024crowdsourced} dataset, which contains 500 real world instructions from the Chatbot Arena~\cite{chiang2024chatbot}.
This challenging dataset can provide better model separability and higher alignment with human preference.

\paragraph{Data construction}
Ten of the sixteen languages required have been provided by m-ArenaHard\footnote{\url{https://huggingface.co/datasets/CohereForAI/m-ArenaHard}}, which has translated the original dataset into 22 languages using Google Translate.
Based on m-ArenaHard, we further translate the English data into six other languages via Google Translate.
Subsequently, we ask human annotators to review and edit the translated instructions in all 16 languages.

\paragraph{Evaluation metric}
We calculate the win rate of the test model against GPT-4o-mini, judged by GPT-4o.

\subsection{Code Generation}
\subsubsection{HumanEval+}
\paragraph{Task description}
HumanEval~\cite{chen2021evaluating} is a widely used code generation benchmark for LLMs.
The generation problem gives a definition of a Python function accompanied by an English docstring, and requires LLMs to complete the function.
A set of test cases are used to judge the correctness of the generated function.
HumanEval+~\cite{liu2024your} is an augmented version of HumanEval, comprising an expanded set of test cases.
This approach is designed to provide comprehensive assurance of the correctness of the functions.
Although current strong LLMs have demonstrated proficiency on this benchmark, there is no research on whether they perform consistently when users input non-English comments.

\paragraph{Data construction}
The objective is to translate only the natural texts within the function comments.
However, it is challenging to prevent Google Translate from translating other elements, such as function names.
Alternatively, we instruct GPT-4o to complete this translation task with well-designed prompts (see Appendix~\ref{}).
Furthermore, a human post-editing process is employed to refine the quality of the generated translation.

\paragraph{Evaluation metric}
Similar to \citet{chen2021evaluating}, we compute the \texttt{Pass@1} metric with 10 generations.

\subsubsection{LiveCodeBench}
\paragraph{Task description}
We include LiveCodeBench~\cite{jain2024livecodebench}, a much harder benchmark, to provide a more rigorous assessment of the code generation capabilities of LLMs.
LiveCodeBench collects coding problems, presented in natural language, from real competition platforms with live updates.
We adopt the code generation subset in LiveCodeBench v4 as the original English dataset, which contains 713 problems. 

\paragraph{Data construction}
Similar to HumanEval, we employ GPT-4o to translate the English problems into other 16 languages with a well-designed prompt, since Google Translate cannot distinguish the parts that should remain untranslated.
Human review is also used to ensure the overall quality of the translated texts.

\paragraph{Evaluation metric}
We compute $pass@1$ and $pass@10$ as the metrics with 10 generations for each problem.

\subsection{Long Context}
Long-context modeling is a crucial aspect of modern large language models.
This study extends the long-context evaluation to multilingual settings based on the RULER benchmark~\cite{hsieh2024ruler}.
\paragraph{Task description}
RULER contains several synthetic long-context tasks with pre-defined context length,  such as the needle-in-a-haystack (NIAH) test and question answering (QA) test.
Since the NIAH test is unrealistic and many models perform perfect on it, we add a new task called QA-in-a-heystack (QAIAH), where one or several paragraphs are inserted into the heystack.
Then the model answers the question related to the inserted paragraph instead of finding the obtrusive needle.
We reserve the tasks of NIAH, QAIAH, and variable tracking (VT) in our task list, while others are excluded.

\paragraph{Data construction}
The haystacks, needles, paragraphs and questions related to QAs are translated to other languages.
We use the parallel testsets from the UN corpus~\cite{ziemski2016united} as the haystack.
The English version contains about 128k tokens, and we extend it to other languages using Google Translate.
% The official dataset provides data in six languages, and we extend it to additional eleven languages using Google Translate.
The sentence of the needle is also translated into 16 other languages, in which UUIDs are employed as keys and values that are not translated.
With respect to the QA data, we translate the paragraphs and questions in \textsc{XQuAD}~\cite{artetxe2020cross} to the languages we need.
Note that we also use the trick in translating IFEval to extract the answer spans.
With access to our multilingual haystacks, needles and paragraphs, we are able to synthesize the multilingual long-context testsets.

\paragraph{Evaluation metric}
The primary metric employed for each task is exact match, consistent with the approach taken in the original paper.
The final score is the average of exact match on each task.
In addition, we compute the F1 score for the QAIAH task as a supplement measure.
We only consider the 128k context length as this is the most difficult setting to experiment with.

\subsection{Reasoning}
\subsubsection{MGSM}
\paragraph{Task description}
MGSM~\cite{shi2023language} is a multilingual extension of the GSM8k dataset and contains 250 primary school maths problems written in ten languages.
It has been widely used to evaluate multilingual reasoning abilities of language models.
Each problem requires two to eight reasoning steps to solve and the final answer is a number.

\paragraph{Data construction}
Given that the MGSM examples are written in ten languages we need, we only translate the English version into the remaining six languages via Google Translate.
This is also followed by a manual checking procedure.

\paragraph{Evaluation metric}
We use the zero-shot CoT accuracy as the metric, following the report of Llama3~\cite{dubey2024llama}.

\subsubsection{GPQA}
\paragraph{Task description}
The GPQA dataset~\cite{Rein2023GPQAAG} comprises 448 multiple-choice questions, formulated by experts in the fields of biology, physics, and chemistry.
This dataset is usually used to assess the scientific reasoning capabilities of language models.
The questions are of high quality and pose extreme challenges, with even human experts achieving accuracy below 70\%.

\paragraph{Data construction}
We choose Google Translate to translate the GPQA-main dataset, since we find that GPT-4o makes more mistakes on translating chemical terminologies than Google Translate in our preliminary study.
The question and the four options of each sample are translated into 16 other languages.
Due to the professionalism of the testset, we require annotation personnel with a Bachelor's degree or higher to complete the annotation.

\paragraph{Evaluation metric}
Zero-shot CoT accuracy is used as the metric.

\subsection{Tool Use}
We use Nexus~\cite{srinivasan2023nexusraven} as the base testset, which is also adopted by Llama3~\cite{dubey2024llama}.

\paragraph{Task description}
Nexus offers a set of functions and user queries.
For each query, the language model is required to generate a function call from a list of noisy functions, in accordance with the function definitions and docstrings.
We only adopt the standardized\_queries subset which contains 318 samples.

\paragraph{Data construction}
We only translate the user queries from English into other languages, given that the majority tool descriptions are written in English.
The user queries are initially translated by Google Translate and subsequently adjusted by human annotators.

\subsection{Machine Translation}
\subsubsection{Flores-200}
Flores-200~\cite{costa2022no} is a commonly used benchmark for multilingual translation.
We include parallel data from 17 languages we care about to form the machine translation testset.
\subsubsection{BenchMAX-17}
As a by-product, we curate a new translation testset from our previous human-translated data.
The dataset is 17-way parallel and exhibits a wide diversity of domains.
To illustrate, the data obtained from LiveCodeBench is in the code domain, requiring a translation model to distinguish whether a given segment should undergo translation or not.
These prompt translation tasks provide a new perspective to evaluate the multilingual translation capabilities of the current language models.