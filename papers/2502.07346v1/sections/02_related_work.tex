\section{Related Work}
% \subsection{Expanding LLM's Multilingual Support} % in Large Language Models
% Various training strategies have been proposed to empower LLM on multilingual scenarios, raning from pre-training~\cite{he2024scaling,alves2024tower} to post-training~\cite{zhu2024power,she2024mapo}.
% In contrast to existing efforts focused on constructing multilingual training datasets~\cite{singh2024aya, li2023bactrian, lai2023okapi}, 
Prior to the era of large language models, most multilingual benchmarks are designed to evaluate discriminative models and take the form of classification tasks, such as \textsc{XNLI}~\cite{conneau2018xnli}, \textsc{XCOPA}~\cite{ponti2020xcopa}, \textsc{XCSQA}~\cite{talmor2019commonsenseqa}, and so on~\cite{xstorycloze-lin-etal-2022-shot,xwinograd-muennighoff-etal-2023-crosslingual}.
However, due to their limited task complexity and the lack of diversity in format, these tasks become less practical.
% In this paper, we focus on curating multilingual benchmarks tailored for LLM evaluation.
% \noindent\paragraph{Benchmarking general capability}
Recently, MGSM has become the most frequently used dataset in technical reports from leading LLM teams~\cite{dubey2024llama,team2024gemini,openai2024gpt4o}, which measures the mathematical reasoning capability across eleven languages.
In this paper, we extend it to cover six additional diverse languages. % , further broadening its scope.
Another widely used benchmark is the multilingual translated version of MMLU~\cite{hendrycks2021measuring,lai2023okapi,singh2024global}, which assesses LLMs on knowledge-intensive tasks. 
However, due to the lack of a unified dataset version, scores are often difficult to compare across studies. 
Moreover, recent analyses have revealed that MMLU contains numerous ground truth errors~\cite{gema2024are}, obscuring the accurate evaluation of LLM capabilities.
To address these limitations, our work builds upon GPQA dataset~\cite{Rein2023GPQAAG} instead of MMLU, which offers higher-quality annotations and poses greater challenges in domain-specific knowledge and reasoning evaluation. 
In addition to curating multilingual versions of MGSM and GPQA, we incorporate a broader range of capabilities, including long context modeling~\cite{hsieh2024ruler}, tool use~\cite{srinivasan2023nexusraven}, instruction following~\cite{zhou2023instruction}, and more. 
This allows our benchmark to evaluate LLMs' multilingual capabilities more comprehensively compared to previous aggregated benchmarks, such as SeaEval~\cite{wang2024seaeval} and P-MMEval~\cite{zhang2024pmmeval}.
More importantly, all translations except the long context data in our benchmark are post-edited by native human experts after machine translation.
This significantly improves both the quality and reliability of the dataset.

\begingroup
\renewcommand{\arraystretch}{1.1} % Default value: 1
\begin{table*}[!ht]
    \caption{\name covers a wide range of language families and script systems.}
    \label{tab:lg_selection}
    \vskip 0.1in
    \centering
    \footnotesize
    \resizebox{0.95\linewidth}{!}{
    \begin{tabular}{c|c|c|c|c|c|c|c}
    \toprule
         \textbf{Language} & \textbf{ISO} & \textbf{Language Family} & \textbf{Script System} & \textbf{Language} & \textbf{ISO} & \textbf{Language Family} & \textbf{Script System} \\
        \midrule
        Hungarian & hu & Uralic & \multirow{6}{*}{Latin} & Serbian & sr & Indo-European & Serbian Cyrillic  \\
        \cline{1-3} \cline{5-8}
        Vietnamese & vi & Austroasiatic & ~ & Korean & ko & Koreanic & Hangul / Chosŏn'gŭl \\
        \cline{1-3} \cline{5-8}
        Spanish & es & \multirow{5}{*}{Indo-European} & ~  & Japanese & ja & Japonic & \makecell{ Mixed scripts of \\ Chinese Characters \\and Hiragana, Katakana} \\
        \cline{1-2} \cline{5-6} \cline{7-8}
        Czech & cs & ~ & ~ & Arabic & ar & Afro-Asiatic & Arabic alphabet \\
        \cline{1-2} \cline{5-8}
        French & fr & ~ & ~ &Thai & th & Kra–Dai & Thai \\
        \cline{1-2} \cline{5-6}  \cline{7-8}
        German & de & ~ & ~ & Swahili & sw & Niger–Congo & Latin \\
        \cline{1-2} \cline{5-6}  \cline{4-4} \cline{7-8}
        Russian & ru & ~ & Cyrillic &  Chinese & zh & Sino-Tibetan & Chinese Characters    \\
        \cline{1-2} \cline{5-6}  \cline{4-4} \cline{7-8}
        Bengali & bn & ~ & Bengali–Assamese&Telugu & te & Dravidian & Telugu \\
    \bottomrule
    \end{tabular}}
    % \vskip -0.1in
\end{table*}


\renewcommand{\arraystretch}{1.3} % Default value: 1
\begin{table*}[!t]
    \caption{Selection of core capabilities and details of task data. For IFEval, we filter out all language-specific instructions, thus remaining 429 samples. For Nexus, we only adopt the standardized\_queries subset which contains 318 samples. For general translation datasets, the number of samples may vary in different translation directions, according to the number of parallel samples in TED and WMT24.}
    \label{tab:task_detail}
    \vskip 0.15in
    \footnotesize
    \centering
    \resizebox{0.95\linewidth}{!}{
    \begin{tabular}{c|c|c|c|c||c|c|c|c|c}
    \toprule
        \textbf{Capability} & \textbf{Category} & \textbf{Dataset} & \textbf{\# Samples} & \textbf{Metric} & \textbf{Capability} & \textbf{Category} & \textbf{Dataset} & \textbf{\# Samples} & \textbf{Metric} \\ 
    \midrule
        Instruction & Rule-based & IFeval & 429 & Accuracy  & \multirow{2}[3]{*}{Code Generation} & \makecell{Function\\Completion} & Humaneval+ & 164 & \multirow{2}[3]{*}{Pass@1}  \\ 
        \cline{2-5} \cline{7-9}
        Following & Model-based & Arena-hard & 500 & Win Rate &  & \makecell{Problem\\Solving} & LiveCodeBench\_v4 & 713 & \\
        \hline
        \multirow{2}{*}{Reasoning} & Math & MGSM & 250 & \multirow{2}{*}{Exact Match} & \multirow{2}{*}{Translation} & General & Flores+TED+WMT24 & $\ge$1012 & \multirow{2}{*}{spBLEU} \\
        \cline{2-4} \cline{7-9}
        & Science & GPQA & 448 &  & & Domain & Annotated data above& 2781 &  \\
        \hline
        Tool Use & \makecell{Multiple\\Functions} & Nexus & 318 & Accuracy &  Long Context Modeling & \makecell{Question\\Answering} & RULER & 800 & Exact Match \\
        
    \bottomrule
    \end{tabular}}
    % \vskip -0.2in
\end{table*}
\endgroup


% Apart from curating multilingual version of \textsc{MGSM} and \textsc{GPQA}, we consider a broader spectrum of capabilities, including long-context modeling~\cite{hsieh2024ruler}, tool use~\cite{srinivasan2023nexusraven}, instruction-following~\cite{zhou2023instruction}, etc. 
% This makes our benchmark being able to evaluate LLM's multilingual capability more comprehensively compared to previous benchmarks aggregated benchmark, such as SeaEval\cite{wang2024seaeval} and P-MMEVAL\cite{zhang2024pmmeval}.
% More importantly, the translation process in our benchmark are all done by native human experts instead of machine translation, this significantly enhances its quality and reliability.
% In addition to curating multilingual versions of MGSM and GPQA, This allows our benchmark to evaluate LLMs' multilingual capabilities more comprehensively compared to previous aggregated benchmarks, such as SeaEval\cite{wang2024seaeval} and P-MMEVAL\cite{zhang2024pmmeval}.
% Crucially, all translations in our benchmark are performed by native human experts rather than relying on machine translation. This significantly improves both the quality and reliability of the dataset.

% Our approach not only expands beyond previous evaluation suites, , but also incorporates assessments guided by human expertise.
% Our approach not only expands beyond previous evaluation suites, such as SeaEval\cite{wang2024seaeval} and P-MMEVAL\cite{zhang2024pmmeval}, but also incorporates assessments informed by human expertise.

% However, due to the lack of unifying dataset version, scores can hardly be compared across papers and recent analysis finds that MMLU contains numerous ground truth errors~\cite{gema2024are}, which may obscure the true capabilities of LLMs.
% Therefore, in this work we build upon GPQA~\cite{Rein2023GPQAAG} instead of MMLU, which is of higher quality and more challenging on domain-speicific knowledge.
% This not only consider .. than previous evaluation suite, e.g., \textsc{SeaEval}~\cite{wang2024seaeval}, \textsc{P-MMEVAL}~\cite{zhang2024pmmeval}, but also build upon human experts.
% for evaluating LLM's multilingual reasoning capability.
% and LLMs' performance quickly saturate on them\footnote{For example, xx and xx achieves over ... on these benchmarks according to their technical report.}
% evaluating the multilingual capabilities of advanced LLMs.
% \citet{shi2023language} introduces multilingual math reasoning benchmark \textsc{MGSM}, which is the most 
% which requires models to generate numerical answers (often using chain-of-thought reasoning) based on given math problems.
% \textsc{Aya Evaluation Suite}~\cite{singh2024aya} are constructed to present
% is wide covered benchmark to evaluate south east asia languages.

% \noindent\paragraph{Benchmarking translation capability}
% Translation capability is a critical aspect of evaluating LLMs' multilingual performance~\cite{zhu2024multilingual}.
% Previous research often relies on general-domain multilingual translation datasets, such as WMT'23~\cite{kocmi2023findings} and Flores~\cite{goyal2022flores,costa2022no}.
% In this work, we curate a new and challenging translation test set by identifying the most difficult translation cases through the construction of our multilingual evaluation suite.
% Compared to traditional test sets, our dataset spans a wider range of domains and provides multiple human-annotated references for each case, enabling more robust and reliable evaluation of multilingual translation performance.

% TODO: \citet{gureja2024m} focus on evaluating the reward model's multilingual performance. This paper constructs a benchmark for the action model and leaves the evaluation of the reward model as future work.
% MMMLU\footnote{\url{https://huggingface.co/datasets/openai/MMMLU}}, which evaluates model's multilingual understanding capability.




