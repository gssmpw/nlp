\clearpage
\appendix


\section{Capability and Task Data Selection}
\label{sec:appendix-task}

% \subsection{Capability Selection}

\paragraph{Instruction Following Capability} involves understanding and executing commands accurately and efficiently. In the light of varied evaluation methods - rule-based or model-based - we include two distinct tasks.


\begin{itemize}[nosep,itemsep=1pt,leftmargin=0.1cm]
    \item \underline{Rule-based Intruction Following:} We collect data from IFEval~\cite{zhou2023instruction}, which is a benchmark for evaluating the instruction following abilities of LLMs, composed of around 500 verifiable instructions and can be evaluated for accuracy using automated rules.
    % After filtering out all English-specific instructions, such as changing the letter cases, the number of remaining samples is 429.
    Note that the accuracy for IFEval is the average of the four accuracies~(i.e. prompt-strict, prompt-loose, inst-strict and inst-loose accuracies), following \cite{dubey2024llama}.
    \item \underline{Model-based Instruction Following:} We collect data from Arena-hard~\cite{li2024crowdsourced} which contains 500 real-world instructions from the Chatbot Arena~\cite{chiang2024chatbot}, and m-ArenaHard\footnote{\url{https://huggingface.co/datasets/CohereForAI/m-ArenaHard}} which contains translated multilingual versions. This benchmark can provide better model separability and higher alignment with human preference. It is assessed by the Win Rate of the testing model in comparison to the baseline model, GPT-4o, judged against GPT-4o-mini.
\end{itemize}


\paragraph{Code Generation Capability} refers to automatically producing functional code scripts based on given requirements. Considering variations in difficulty, two separate tasks are included. 
\begin{itemize}[nosep,itemsep=1pt,leftmargin=0.1cm]
    \item \underline{Function Completion:} We collect data from Humaneval+~\cite{liu2024your} which is an augmented version of HumanEval~\cite{chen2021evaluating}, comprising an expanded test cases. Each problem in the benchmark gives a definition of a Python function accompanied by an English docstring, and requires LLMs to complete the function.
    \item \underline{Problem Solving:} We collect data from LiveCodeBench~\footnote{We adopt the code generation subset in LiveCodeBench v4 as the original English dataset.}~\cite{jain2024livecodebench} which provides a more rigorous assessment of the code generation capabilities. It is a much harder benchmark by collecting coding problems in natural language from real competition platforms with live updates.
\end{itemize}


\paragraph{Long Context Modeling Capability} involves understanding and generating coherent text from extensive input sequences, allowing the model to capture dependencies and relationships within lengthy texts. This paper focuses on the long-context evaluation of multilingual settings based on the RULER benchmark~\cite{hsieh2024ruler}.
\begin{itemize}[nosep,itemsep=1pt,leftmargin=0.1cm]
\item \underline{Question Answering:} We build synthetic testsets based on RULER, which contains several question answering long-context tasks with pre-defined context length, such as the needle-in-a-haystack~(NIAH) test and question answering~(QA) test.
Since the NIAH test is unrealistic and many models perform perfectly on it, we add a new task called QA-in-a-heystack~(QAIAH), where one or several paragraphs are inserted into the haystack. The model then answers the question related to the inserted paragraph instead of finding the obtrusive needle. We reserve the tasks of NIAH, QAIAH, and variable tracking~(VT) in our task list, while others are excluded. 
\end{itemize}


\paragraph{Reasoning} encompasses thinking logically, drawing conclusions, making inferences, and solving problems by processing data, applying rules, and utilizing various forms of logic and knowledge representation. Pushing LLMs beyond surface-level tasks, we extend MGSM~\cite{shi2023language} and GPQA~\cite{Rein2023GPQAAG} requiring deeper understanding and reasoning across different context.
\begin{itemize}[nosep,itemsep=1pt,leftmargin=0.1cm]
    \item \underline{Math Reasoning:} We collect data from MGSM which evaluates the capability of LLM to solve math reasoning problems in multiple languages, focusing on grade-school level complexity.
    \item \underline{Science Reasoning:} We collect data from GPQA which is crucial for assessing LLM capability for advanced, unsearchable reasoning and critical thinking across diverse, complex domains. It comprises multiple choice questions formulated by experts in the domains of biology, physics, and chemistry, posing extreme challenges where human experts achieve accuracy lower than 70\%.
\end{itemize}

\paragraph{Tool Use Capability} requires the model to translate user queries into executable functions for calling in operating software tools. We extend Nexus~\cite{srinivasan2023nexusraven} to a multilingual version, which is adopted by Llama3~\cite{dubey2024llama}. 
\begin{itemize}[nosep,itemsep=1pt,leftmargin=0.1cm]
    \item \underline{Multiple Functions:} Nexus offers a set of functions and user queries. For each query, the language model is required to generate a function call from a list of noisy functions, in accordance with the function definitions and docstrings. 
\end{itemize}


\paragraph{Translation Capability} needs the model to convert text between multiple languages while maintaining semantic meaning accurately. To comprehensively evaluate this capability, we introduce general and task-specific translation datasets.
\begin{itemize}[nosep,itemsep=1pt,leftmargin=0.1cm]
    \item \underline{General:} General domain data are composed of Flores-200~\cite{costa2022no}, TED~\cite{cettolo-etal-2012-wit3} and WMT24~\cite{kocmi2024findings} testsets. In \name, we include parallel data from 17 selected languages.
    \item \underline{Domain:} Domain translation data is a by-product of the \name construction process, encompassing a 17-way parallel task across diverse domains, such as reasoning, code generation, tool usage, and instruction following. Unlike traditional translation tasks, this poses a new challenge to the model by requiring it to determine whether a given segment should be translated or not.
\end{itemize}


\begin{table*}[!t]
    \caption{The recall of keywords when translating IFEval English data to other languages.}
    \label{tab:recall}
    \vskip 0.15in
    \centering
    \small
    \setlength{\tabcolsep}{4pt}
    \begin{tabular}{ccccccccccccccccc}
    \toprule
    & zh & es & fr & de & hu & ru & ja & th & sw & bn & te & ar & ko & vi & cs & sr \\
    \midrule
    w/o special symbols & 0.68 & 0.68 & 0.68 & 0.68 & 0.68 & 0.68 & 0.68 & 0.68 & 0.68 & 0.68 & 0.68 & 0.68 & 0.68 & 0.68 & 0.68 & 0.68 \\
    +symbol 1 & 0.91 & 0.89 & 0.88 & 0.92 & 0.93 & 0.93 & 0.91 & 0.91 & 0.92 & 0.95 & 0.96 & 0.90 & 0.88 & 0.90 & 0.92 & 0.99 \\
    +symbol 2 & 0.93 & 0.93 & 0.90 & 0.92 & 0.95 & 0.94 & 0.92 & 0.93 & 0.93 & 0.97 & 0.99 & 0.94 & 0.91 & 0.92 & 0.93 & 1.00 \\
    \bottomrule
    \end{tabular}
\end{table*}


\section{Dataset Construction}
\label{sec:append_if_keywords}
\subsection{Rule-based Instruction Following Dataset}
We first filter out some English-specific instructions from the original dataset, such as changing the letter cases.
After filtering, the number of remaining samples is 429.
The next problem is how to extract the keywords from the translated instruction since the keywords are also translated and are required in the verification step.
We try different groups of special symbols to extract translated keywords.
The recall rates are presented in Table~\ref{tab:stat_symbol_translation}, where order 1 achieves the best performance.
Complete results across all languages are provided in Table~\ref{tab:recall}.
In addition, the number-word constraints for non-English languages are multiplied by a ratio in order to make the difficulty of the same instruction in different languages comparable.
Specifically, we calculate the ratio of the word count of English to that of a non-English language in the Flores-200 corpus using language-specific tokenizers.
we also adapt verification rules to multilingual scenarios.
For instance, word and sentence segmentation methods may vary across different languages.

During post-editing, we ask human annotators to check whether the translated keywords in the kwargs, which are used by the rule-based program, appear in the translated instruction.

% We test different groups of special symbols to extract the translated keywords.
% The recall rates are demonstrated in Table~\ref{tab:stat_symbol_translation}, where order 1 exhibits the best results.
% The complete results across all languages are shown in Table~\ref{tab:recall}.

\begin{figure*}[!t]
    \vskip 0.2in
    \centering
    \includegraphics[width=0.98\linewidth]{figure/radar_chart.pdf}
    \caption{The radar charts visualize the performance of models on each subtask in different languages. Most model evaluated have imbalanced performance across different languages.}
    \label{fig:radar_chart}
    \vskip -0.2in
\end{figure*}


\subsection{Model-based Instruction Following Dataset}
Ten of the sixteen languages required have been provided by m-ArenaHard, which has translated the original dataset into 22 languages using Google Translate.
Based on m-ArenaHard, we further translate the English data into six other languages via Google Translate.
Subsequently, we ask human annotators to review and edit the translated instructions in all 16 languages.

\subsection{Function Completion Dataset}
The objective is to translate only the natural texts within the function comments.
However, it is challenging to prevent Google Translate from translating other elements, such as function names.
Alternatively, we instruct GPT-4o to complete this translation task with well-designed prompts~(Table~\ref{tab:translate_func_compl}).
Furthermore, a human post-editing process is employed to refine the quality of the generated translation.

\subsection{Problem Solving Dataset}
Similar to the Function Completion Dataset, we employ GPT-4o to translate the English problems into other 16 languages with a well-designed prompt~(Table~\ref{tab:translate_prob_solving}), since Google Translate cannot distinguish the parts that should remain untranslated.
Human review is also used to ensure the overall quality of the translated texts.

\subsection{Math Reasoning Dataset}
Given that the MGSM examples are written in ten languages we need, we only translate the English version into the remaining six languages via Google Translate.
This is also followed by a manual checking procedure.

\subsection{Science Reasoning Dataset}
The question and the four options of each sample are translated into 16 other languages by Google Translate.
In particular, the question and options are concatenated by option markers like ``(A)''.
After translation, we extract the translated question and options to form a new sample.
% Due to the professionalism of the testset, we require annotation personnel with a Bachelor's degree or higher to complete the annotation.

\subsection{Long-Context Question Answering Dataset}
The haystacks, needles, paragraphs and questions related to QAs are translated to other languages.
We use the parallel testsets from the UN corpus~\cite{ziemski2016united} as the haystack.
The English version contains about 128k tokens, and we extend it to other languages using Google Translate.
The sentence of the needle is also translated into 16 other languages, in which UUIDs are employed as keys and values that are not translated.
With respect to the QA data, we translate the paragraphs and questions in \textsc{XQuAD}~\cite{artetxe2020cross} to the languages we need.
Note that we also use the trick in translating IFEval to extract the answer spans.
With access to our multilingual haystacks, needles and paragraphs, we are able to synthesize the multilingual long-context testsets.

\subsection{Multiple Functions Dataset}
We only translate the user queries from English into other languages, given that the majority tool descriptions are written in English.
The user queries are initially translated by Google Translate and subsequently adjusted by human annotators.
To preserve the English parameters, we replace them with placeholders before machine translation and restore them afterward.

\section{Model Information}
\label{sec:model_info}
Here we list the evaluated models in this section.

\paragraph{Llama3.1-Instruct~\cite{dubey2024llama}} series contains three multilingual large language models with number of parameters ranging from 8B to 405B.
The pre-training corpus of Llama3.1 contains 8\% multilingual tokens, and multilingual alignment is also optimized during post-training.
In our experiments, we evaluate the 8B version and the 70B version of Llama3.1-Instruct.

\paragraph{Qwen2.5-Instruct~\cite{qwen2.5}} is a collection of multilingual language models with several sizes, ranging from 0.5B to 72B.
The models are trained with multilingual tokens in both pre-training stage and post-training stage, and are rigorously evaluated on several multilingual tasks.
In our experiments, we evaluate the 7B version and the 72B version of Qwen2.5-Instruct.

\paragraph{Aya-Expanse~\cite{dang2024aya}} is an open-weight research of models with advanced multilingual capabilities, supporting 23 languages.
The Aya Expanse 8B and 32B variants are instruction-tuned and beat Llama3.1-instruct models on the m-ArenaHard, a multilingual instruction following benchmark.



\paragraph{Gemma2-IT~\cite{team2024gemma}} family demonstrates strong multilingual capabilities, although this is not highlighted in the technical report.
We benchmark the 9B and 27B variants of Gemma2-IT.

\paragraph{InternLM2.5-chat~\cite{cai2024internlm2}} is the successor of InternLM~\cite{team2023internlm}, which is claimed as a multilingual model.
We include the 7B version and 20B version in our experiments.
InternLM2.5-7B-chat-1m is a long-context variant supporting context windows with 1M tokens.

% \paragraph{LLaMAX3-8B-Alpaca~\cite{lu2024llamax}} is a multilingual model base on Llama3-8B.
% It is continue pre-trained on corpus containing 102 languages and further fine-tuned on the English Alpaca dataset.

\paragraph{DeepSeek-V3~\cite{DeepSeekAI2024DeepSeekV3TR}} is one of state-of-the-art open-source models that achieve performance comparable to that of the best proprietary models.
It is a 671B MoE model, with 37B activated for each token.
A multilingual corpus and a multilingual-optimized tokenizer are incorporated into their training process.

\paragraph{GPT-4o \& GPT-4o-mini~\cite{openai2024gpt4o}} are two of the best proprietary models that also achieve remarkable performance on multilingual tasks.
Their tokenizer can better compress multilingual texts than that of GPT-4.
GPT-4o-mini is the smaller version of GPT-4o with powerful performance.

% \paragraph{Claude-3.5-Sonnet} is also a state-of-the-art proprietary model that performs strongly on various tasks.
% It claims that Claude has the multilingual capabilities although the performonce in low-resource languages is comparatively inferior to that of high-resource ones. 


\section{Details about Prompt Templates}
\label{sec:prompts}
We present the prompt templates used in each task in this section.
Table~\ref{tab:mgsm_prompt} and Table~\ref{tab:gpqa_prompt} show the native-CoT prompts for MGSM and GPQA.
Table~\ref{tab:template} shows the prompt templates for some tasks where the original English template is used.
Table~\ref{tab:ruler_prompts} shows the prompt templates of the long-context modelling task.
Table~\ref{tab:llmjudge} shows the LLM-Judge Instruction for comparing two translations. 

\begin{table}[ht]
    \caption{The native-CoT prompts of the mathematical reasoning task.}
    \label{tab:mgsm_prompt}
    \vskip 0.15in
    \centering
    \includegraphics[width=0.5\textwidth]{figure/mgsm_prompt.pdf}
\end{table}


\begin{table*}[ht]
    \caption{The native-CoT prompts of the scientific reasoning task.}
    \label{tab:gpqa_prompt}
    \vskip 0.15in
    \centering
    \includegraphics[width=0.97\textwidth]{figure/gpqa_prompt.pdf}
\end{table*}

\begin{table*}[ht]
    \caption{The prompt templates of the listed tasks. The prompt in the template is multilingual.}
    \label{tab:template}
    \vskip 0.15in
    \centering
    \small
    \begin{tabularx}{1\textwidth}{l|X}
        \toprule
        Task & Prompt Template \\
        \midrule
        Rule-based instruction following & \{prompt\} \\
        \midrule
        Model-based instruction following & \{prompt\} \\
        \midrule
        Problem Solving & \textbf{\textcolor{gray}{[System Message]}}\newline
        You are an expert Python programmer. You will be given a question (problem specification) and will generate a correct Python program that matches the specification and passes all tests. You will NOT return anything except for the program.\newline
        \textbf{\textcolor{gray}{[User Message]}}\newline
        \#\#\# Question:\newline
        \{question\}\newline
        \#\#\# Format: Read the inputs from stdin solve the problem and write the answer to stdout (do not directly test on the sample inputs). Enclose your code within delimiters as follows.\newline
        \newline
        \textasciigrave\textasciigrave\textasciigrave python\newline
        \# YOUR CODE HERE\newline
        \textasciigrave\textasciigrave\textasciigrave\newline
        \newline
        \#\#\# Answer: (use the provided format with backticks)\\
        \midrule
        Function Completion & \textbf{\textcolor{gray}{[User Message]}}\newline
        Please provide a self-contained Python script that solves the following problem in a markdown code block:\newline
        \textasciigrave\textasciigrave\textasciigrave\newline
        \{prompt\}\newline
        \textasciigrave\textasciigrave\textasciigrave\newline
        \textbf{\textcolor{gray}{[Assistant Message]}}\newline
        Below is a Python script with a self-contained function that solves the problem and passes corresponding tests:\newline
        \textasciigrave\textasciigrave\textasciigrave python\\
        \midrule
        Tool use & {\textbf{\textcolor{gray}{[Tool Info]}}}\newline \{prompt\}\\
        \bottomrule         
    \end{tabularx}
\end{table*}

\begin{table*}
    \caption{The prompt templates of the long-context modelling task.}
    \label{tab:ruler_prompts}
    \vskip 0.15in
    \centering
    \small
    \begin{tabularx}{\textwidth}{l|X}
    \toprule
    Subtask & Prompt Template \\
    \midrule
    NIAH & \textbf{\textcolor{gray}{[User Message]}}\newline
    Some special magic uuids are hidden within the following text. Make sure to memorize it. I will quiz you about the uuids afterwards.\newline
    \{heystack\}\newline
    What are all the special magic uuids for \{query\} mentioned in the provided text?\newline
    \textbf{\textcolor{gray}{[Assistant Message]}}\newline
    The special magic uuids for \{query\} mentioned in the provided text are \\
    \midrule
    QA in a heystack (QAIAH) & \textbf{\textcolor{gray}{[User Message]}}\newline
    Answer the questions based on the given documents. Only give me the answers and do not output any other words.\newline\newline
    The following are given documents.\newline\newline
    \{context\}\newline\newline
    Answer the questions based on the given documents. Only give me the answers and do not output any other words.\newline\newline
    Questions:\newline
    \{query\}
    \textbf{\textcolor{gray}{[Assistant Message]}}\newline
    Answers:\\
    \midrule
    Variable Tracking (VT) & \textbf{\textcolor{gray}{[User Message]}}\newline
    Memorize and track the chain(s) of variable assignment hidden in the following text.\newline\newline
    \{context\}\newline
    Question: Find all variables that are assigned the value \{query\} in the text above.\newline
    \textbf{\textcolor{gray}{[Assistant Message]}}\newline
    Answer: According to the chain(s) of variable assignment in the text above, 5 variables are assgined the value \{query\}, they are: \\
    \midrule
    QA & \textbf{\textcolor{gray}{[User Message]}}\newline
    Answer the question based on the given documents. Only give me the answer and do not output any other words.\newline\newline
    The following are given documents.\newline\newline
    \{context\}\newline\newline
    Answer the question based on the given documents. Only give me the answer and do not output any other words.\newline\newline
    Question: \{query\}\newline
    \textbf{\textcolor{gray}{[Assistant Message]}}\newline
    Answer:\\
    \bottomrule
    \end{tabularx}
\end{table*}

\begin{table*}[ht]
    \caption{The GEMBA-SQM prompt.}
    \label{tab:gemba_sqm}
    \vskip 0.15in
    \centering
    \begin{tabularx}{0.95\textwidth}{X}
    \toprule
    % GEMBA-SQM \\
    % \midrule
    Score the following translation from \{src\_lang\} to \{tgt\_lang\} with respect to the human reference on a continuous scale from 0 to 100 that starts with ``No meaning preserved'', goes through ``Some meaning preserved'', then ``Most meaning preserved and few grammar mistakes'', up to ``Perfect meaning and grammar''\newline
    \newline
    \{src\_lang\} source: ``\{source\}''\newline
    \{tgt\_lang\} translation: ``\{target\}''\newline
    Score:\\
    \bottomrule
    \end{tabularx}
\end{table*}

\begin{table*}[ht]
    \caption{LLM-Judge Instruction}
    \label{tab:llmjudge}
    \vskip 0.15in
    \centering
    \begin{tabularx}{0.95\textwidth}{X}
    \toprule
    \textbf{\textcolor{gray}{[System Message]}}\newline
    Please act as an impartial judge and evaluate the quality of the {lang} translations provided by two humans for the English source sentence displayed below. You will be given human A's translation and human B's translation. Your job is to evaluate which human's translation is better.\newline
    \newline
    You must identify and correct any mistakes or inaccurate information.\newline
    \newline
    Consider if the human's translations are accurate and fluent. Accurate means the translation conveys the same meaning, information, and nuances as the original source text. Fluent refers to the quality of the translation in terms of its naturalness, readability, and adherence to the grammatical, stylistic, and idiomatic conventions of the target language.\newline
    \newline
    Then consider whether the human's translations are consistent with the context. Code input/output and programming language syntax should not be translated. Finally, review the formatting of the translated text, including indentation, to ensure it is consistent and appropriate.\newline
    \newline
    After providing your explanation, you must output only one of the following choices as your final verdict with a label:\newline
    \newline
    1. Human A is significantly better: [[A$>>$B]]\newline
    2. Human A is slightly better: [[A$>$B]]\newline
    3. Tie, relatively the same: [[A$=$B]]\newline
    4. Human B is slightly better: [[B$>$A]]\newline
    5. Human B is significantly better: [[B$>>$A]]\newline
    \newline
    Example output: ``My final verdict is tie: [[A$=$B]]''.\newline
    \textbf{\textcolor{gray}{[User Message]}}\newline
    \textless$|$ Source Text$|$\textgreater\newline
    \{source\}\newline
    \newline
    \textless$|$The Start of Human A's Translation$|$\textgreater\newline
    \{translation\_1\}\newline
    \textless$|$The End of Human A's Translation$|$\textgreater\newline
    \newline
    \textless$|$The Start of Human B's Translation$|$\textgreater\newline
    \{translation\_2\}\newline
    \textless$|$The End of Human B's Translation$|$\textgreater\\
    \bottomrule
    \end{tabularx}
\end{table*}

\begin{table*}[ht]
    \caption{Prompt for translating the Function Completion task.}
    \label{tab:translate_func_compl}
    \vskip 0.15in
    \centering
    \begin{tabularx}{0.95\textwidth}{X}
    \toprule
    \textbf{\textcolor{gray}{[System Message]}}\newline
    You are a professional translator specializing in technical content. Please translate the following English Python codes into \{tgt\_lang\}, adhering to these specific guidelines:\newline\newline
    1. **Do not translate** content representing code input/output or programming language syntax. Only translate content in comments.\newline
    2. **Maintain the original formatting** of the text, structure and indentation.\newline
    3. **Do not translate** any LaTeX code.\newline
    4. **Only output the translation** without any additional comments or explanations.\newline
    \textbf{\textcolor{gray}{[User Message]}}\newline
    \{problem\}\\
    \bottomrule
    \end{tabularx}
\end{table*}

\begin{table*}[ht]
    \caption{Prompt for translating the Problem Solving task.}
    \label{tab:translate_prob_solving}
    \vskip 0.15in
    \centering
    \begin{tabularx}{0.95\textwidth}{X}
    \toprule
    \textbf{\textcolor{gray}{[System Message]}}\newline
    You are a professional translator specializing in technical content. Please translate the following English coding problems into \{tgt\_lang\}, adhering to these specific guidelines:\newline\newline
    1. **Do not translate** any LaTeX code.\newline
    2. **Do not translate** content representing code input/output or programming language syntax.\newline
    3. **Maintain the original formatting** of the text and structure.\newline
    4. **Only output the translation** without any additional comments or explanations.\newline
    \textbf{\textcolor{gray}{[User Message]}}\newline
    \{problem\}\\
    \bottomrule
    \end{tabularx}
\end{table*}

\section{Detailed results}
\label{sec:detailed_results}
Figure~\ref{fig:radar_chart} illustrates the detailed results of each model on each task.



% \begin{table*}[ht]
% \centering
% \scriptsize
% \begin{tabular}{cccccccc}
% \toprule
% Model       & \textsc{mGSM} & \textsc{M-MMLU} & \textsc{WMT23} & \textsc{Flores-200} & \textsc{XCOPA} & \textsc{XWinograd} & \textsc{XStoryCloze} \\
% \midrule
% GPT-4o      & \Checkmark &            &            &            &            &            &             \\
% GPT-4o mini & \Checkmark &            &            &            &            &            &             \\
% Claude-3.5  & \Checkmark &            &            &            &            &            &             \\
% Gemini-1.5  & \Checkmark &            & \Checkmark &            &            &            &             \\
% LLaMA-3.1   & \Checkmark & \Checkmark &            &            &            &            &             \\
% Qwen2.5     & \Checkmark & \Checkmark &            &            & \Checkmark & \Checkmark & \Checkmark  \\
% PaLM-2      & \Checkmark &            &            &            & \Checkmark &            &             \\
% % Aya-23      & \Checkmark & \Checkmark &            & \Checkmark & \Checkmark & \Checkmark & \Checkmark  \\
% \bottomrule
% \end{tabular}
% \caption{checklist}
% \end{table*}

% En prompt & Invent a funny tagline for a local comedy show, and put your whole response in double quotes. \\ 
%          Zh prompt & \begin{CJK}{UTF8}{gbsn}为当地喜剧节目设计一个有趣的标语，并将你的整个回答用双引号括起来。\end{CJK} \\
%          Es prompt & Inventa un eslogan divertido para un programa de comedia local y pon toda tu respuesta entre comillas dobles. \\
%          Fr prompt & Inventez un slogan amusant pour une émission humoristique locale et mettez toute votre réponse entre guillemets. \\
%          De prompt & Erfinden Sie einen lustigen Slogan für eine lokale Comedy-Show und setzen Sie Ihre gesamte Antwort in Anführungszeichen. \\
%          Ru prompt & 