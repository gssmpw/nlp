\section{Benchmark Construction}
This section will introduce how to extend the evaluation of the core capabilities of LLMs into multilingual scenarios. As shown in Figure~\ref{fig:overview}, \name encompasses multi-way parallel data spanning 17 languages~(\S~\ref{sec:lg_selection}), enabling thorough cross-lingual assessments and comparisons. Meanwhile, its comprehensive coverage of diverse tasks~(\S~\ref{sec:task_selection}) facilitates the evaluation of multiple capabilities across a spectrum of linguistic contexts. Noteworthy is its precise human annotation~(\S~\ref{sec:construction_process}), guaranteeing the integrity and trustworthiness of the data.

\subsection{Language Selection}
\label{sec:lg_selection}

To demonstrate \name's effectiveness in assessing the cross-lingual transferability of LLMs, we keep the language selection result~(en, es, fr, de, ru, bn, ja, th, sw, zh, te) of MGSM task, and introduce six additional languages~(ar, ko, sr, cs, hu, vi) based on their diverse language families and script systems (illustrated in Table~\ref{tab:lg_selection}). 


\begingroup
\renewcommand{\arraystretch}{1.4} % Default value: 1
\begin{table*}[!ht]
    \centering
    \footnotesize
    \resizebox{0.95\linewidth}{!}{
    \begin{tabular}{c|c|c|c|c|c|c|c}
    \toprule
         \textbf{Language} & \textbf{ISO} & \textbf{Language Family} & \textbf{Script System} & \textbf{Language} & \textbf{ISO} & \textbf{Language Family} & \textbf{Script System} \\
        \midrule
        Spanish & es & \multirow{5}{*}{Indo-European} & \multirow{3}{*}{Latin}  & Japanese & ja & Japonic & \makecell{ Mixed scripts of \\ Chinese characters \\and Hiragana, Katakana} \\
        \cline{1-2} \cline{5-6} \cline{7-8}
        French & fr & ~ & ~ &Thai & th & Kra–Dai & Thai \\
        \cline{1-2} \cline{5-6}  \cline{7-8}
        German & de & ~ & ~ & Swahili & sw & Niger–Congo & Latin \\
        \cline{1-2} \cline{5-6}  \cline{4-4} \cline{7-8}
        Russian & ru & ~ & Cyrillic &  Chinese & zh & Sino-Tibetan & Chinese Characters    \\
        \cline{1-2} \cline{5-6}  \cline{4-4} \cline{7-8}
        Bengali & bn & ~ & Bengali–Assamese&Telugu & te & Dravidian & Telugu \\
        \hline
        \rowcolor{lightgray!5}
        Arabic & ar & Afro-Asiatic & Arabic alphabet & Czech & cs & Indo-European & ~ \\
        % \cline{3-4} \cline{7-7}
        \cline{1-7}
        \rowcolor{lightgray!5}
        Korean & ko & Koreanic & Hangul / Chosŏn'gŭl & Hungarian & hu & Uralic & Latin \\
        % \cline{3-4} \cline{7-7}
        \cline{1-7}
        \rowcolor{lightgray!5}
        Serbian & sr & Indo-European & Serbian Cyrillic & Vietnamese & vi & Austroasiatic & ~\\
    \bottomrule
    \end{tabular}}
    \caption{Non-English language selection information.}
    \label{tab:lg_selection}
\end{table*}
\endgroup


\subsection{Task Selection}
\label{sec:task_selection}

LLMs have demonstrated proficiency in NLU tasks such as text classification, sentiment analysis, and named entity recognition. However, their capabilities transcend text comprehension, possessing the intrinsic capability to follow complex instructions, reason through intricate scenarios, handle long contextual information, generate code autonomously, utilize tools effectively, and navigate the complicated landscape of machine translation. Here, we extend these capabilities in multilingual scenarios, as shown in Table~\ref{tab:task_detail}.

\begingroup
\renewcommand{\arraystretch}{1.4} % Default value: 1
\begin{table*}[!ht]
    \centering
    \resizebox{0.95\linewidth}{!}{
    \begin{tabular}{c|c|c|c|c|c|c|c}
    \toprule
        \textbf{Task} & \textbf{Dataset} & \textbf{\# Sample} & \textbf{Metric} & \textbf{Task} & \textbf{Dataset} & \textbf{\# Sample} & \textbf{Metric} \\ 
    \midrule
        Instruction& IFeval & 429 & Accuracy & \multirow{2}{*}{Reasoning} & MGSM & 250 & \multirow{2}{*}{Exact Match} \\ 
        \cline{2-4} \cline{6-7}
        Following & Arena-hard & 500 & Win Rate & ~ & GPQA & 448 & ~ \\ 
        \hline
        Code & Humaneval+ & 164 & Pass@1 & Machine & Flores & 1012 & \multirow{2}{*}{spBLEU} \\ 
        \cline{2-3} \cline{6-7}
        Generation & LiveCodeBench & 713 & Pass@10 & Translation & Domain Translation & ~ & ~ \\ 
        \hline
        Long Context & RULER & 100 & Exact Match & Tool Use & Nexus & 318~\footnote{We only adopt the standardized\_queries subset which contains 318 samples.} & Accuracy \\ 
    \bottomrule
    \end{tabular}}
    \caption{Tasks Details.}
    \label{tab:task_detail}
\end{table*}
\endgroup

\paragraph{Instruction Following Capability} involves understanding and executing commands accurately and efficiently. In the light of varied evaluation methods - rule-based or model-based - we introduce two distinct tasks.


\begin{itemize}[nosep,itemsep=1pt,leftmargin=0.1cm]
    \item \underline{IFEval}~\cite{zhou2023instruction} is a benchmark for evaluating the instruction following abilities of LLMs, composed of around 500 verifiable instructions and can be evaluated for accuracy using automated rules. After filtering out all English-specific instructions, such as changing the letter cases, the number of remaining samples is 429.
    \item \underline{Arena-Hard}~\cite{li2024crowdsourced} contains 500 real-world instructions from the Chatbot Arena~\cite{chiang2024chatbot}, which can provide better model separability and higher alignment with human preference. It is assessed by the Win Rate of the testing model in comparison to the baseline model, GPT-4o mini, judged against GPT-4o.
\end{itemize}


\paragraph{Code Generation Capability} refers to automatically producing functional code scripts based on given requirements. Considering variations in difficulty, two separate tasks are included. 
\begin{itemize}[nosep,itemsep=1pt,leftmargin=0.1cm]
    \item \underline{HumanEval+}~\cite{liu2024your} is an augmented version of HumanEval~\cite{chen2021evaluating}, comprising an expanded test cases. Each problem in the benchmark gives a definition of a Python function accompanied by an English docstring, and requires LLMs to complete the function.
    \item \underline{LiveCodeBench}~\footnote{We adopt the code generation subset in LiveCodeBench v4 as the original English dataset.}~\cite{jain2024livecodebench} provides a more rigorous assessment of the code generation capabilities. It is a much harder benchmark by collecting coding problems in natural language from real competition platforms with live updates.
\end{itemize}


\paragraph{Long Context Modeling Capability} involves understanding and generating coherent text from extensive input sequences, allowing the model to capture dependencies and relationships within lengthy texts. This paper focuses on the long-context evaluation of multilingual settings based on the RULER benchmark~\cite{hsieh2024ruler}.
\begin{itemize}[nosep,itemsep=1pt,leftmargin=0.1cm]
    \item \underline{RULER} contains several synthetic long-context tasks with pre-defined context length,  such as the needle-in-a-haystack~(NIAH) test and question answering~(QA) test.
Since the NIAH test is unrealistic and many models perform perfectly on it, we add a new task called QA-in-a-heystack~(QAIAH), where one or several paragraphs are inserted into the haystack. The model then answers the question related to the inserted paragraph instead of finding the obtrusive needle. We reserve the tasks of NIAH, QAIAH, and variable tracking~(VT) in our task list, while others are excluded. 
\end{itemize}


\paragraph{Reasoning} encompasses thinking logically, drawing conclusions, making inferences, and solving problems by processing data, applying rules, and utilizing various forms of logic and knowledge representation. Pushing LLMs beyond surface-level tasks, we introduce MGSM~\cite{shi2023language} and GPQA~\cite{Rein2023GPQAAG} requiring deeper understanding and reasoning across different context.
\begin{itemize}[nosep,itemsep=1pt,leftmargin=0.1cm]
    \item \underline{MGSM} evaluates the capability of LLM to solve math reasoning problems in multiple languages, focusing on grade-school level complexity.
    \item \underline{GPQA} is crucial for assessing LLM capability for advanced, unsearchable reasoning and critical thinking across diverse, complex domains. It comprises multiple choice questions formulated by experts in the domains of biology, physics, and chemistry, posing extreme challenges where human experts achieve accuracy lower than 70\%.
\end{itemize}

\paragraph{Tool Usage Capability} requires the model to translate user queries into executable functions for calling in operating software tools. We extend Nexus~\cite{srinivasan2023nexusraven} to a multilingual version, which is adopted by Llama3~\cite{dubey2024llama}. 
\begin{itemize}[nosep,itemsep=1pt,leftmargin=0.1cm]
    \item \underline{Nexus} offers a set of functions and user queries. For each query, the language model is required to generate a function call from a list of noisy functions, in accordance with the function definitions and docstrings. 
\end{itemize}


\paragraph{Translation Capability} needs the model to convert text between multiple languages while maintaining semantic meaning accurately. To comprehensively evaluate this capability, we introduce general and task-specific translation datasets.
\begin{itemize}[nosep,itemsep=1pt,leftmargin=0.1cm]
    \item \underline{Flores-200}~\cite{costa2022no} is a widely used translation benchmark derived from web articles. In \name, we include parallel data from 17 selected languages.
    \item \underline{Domain Translation} is a by-product of the \name construction process, encompassing a 17-way parallel task across diverse domains, such as reasoning, code generation, tool usage, and instruction following. Unlike traditional translation tasks, this poses a new challenge to the model by requiring it to determine whether a given segment should be translated or not.
\end{itemize}



\subsection{Construction}
\label{sec:construction_process}

The way to extend those capabilities into the multilingual setting consists of three main steps: 
\begin{itemize}[nosep,itemsep=1pt,leftmargin=0.1cm]
\item \textbf{Step 1:} Translating data from English to 16 non-English by GPT-4o or Google Translator; 
\item \textbf{Step 2:} Post-editing by three native speaker annotators for each sample in all tasks; 
\item \textbf{Step 3:} Selecting the final translation version by human or LLM. 
\end{itemize}

For Step 1, we first conduct a preliminary study by randomly selecting a small sample set and translating it with both GPT-4o and Google Translator. 
Interestingly, GPT-4o's translations don't always outperform, even for high-resource languages. 
Hence, our choice of translator is based on the performance of this preliminary study.

Additionally, as shown in Table~\ref{tab:construction-case}, the IFEval task requires extra processing to extract keywords from the translated instruction, as they are needed for verification. Inspired by \citet{yuan-etal-2020-enhancing}, we enclose the keywords in the original instruction with special symbols, making them easy to extract from the translated result. Furthermore, we refine the number-word constraint for non-English languages based on the English to their word count ratio in the Flores-101 corpus, ensuring comparable difficulty across languages.


During Step~2, each native speaker annotator needs to create a new translation using the English input text and the result from Step~1. Therefore, the basic requirement for annotators is proficiency in both English and their native language. Additionally, since GPQA is a knowledge-intensive task, we require annotators to have at least a Bachelor's degree and to consult relevant materials to ensure the accuracy of proper noun translations.

In Step 3, due to time and budget constraints, we use additional human annotation for some languages and LLMs for others to select the final translation. Furthermore, we analyze the consistency and correlation in the experimental section.


\begin{table}[t!]
    \centering 
    \tiny
    \begin{tabular}{|p{7.2cm}|}
        \hline
        \textbf{[Original Text]:} \{"key": 2602, \\
        "prompt": "Write a limerick about a Zelda fan named Rodney. Make sure to include these items: Zelda, Hyrule, Link, Ganon. Use less than 100 words.",\\ "instruction\_id\_list": ["keywords:existence"],\\
        "kwargs": [\{"keywords": ["zelda", "hyrule", "link", "ganon"]\}]\} \\
        \hline
        \textbf{[Translation Input]:} Write a limerick about a Zelda fan named Rodney. Make sure to include these items: \textcolor{red}{<b>}Zelda\textcolor{red}{</b>}, \textcolor{red}{<b>}Hyrule\textcolor{red}{</b>}, \textcolor{red}{<b>}Link\textcolor{red}{</b>}, \textcolor{red}{<b>}Ganon\textcolor{red}{</b>}. Use less than 100 words.\\ 
        \textbf{[Google Translation Result]:} \begin{CJK}{UTF8}{gbsn}写一首关于名叫罗德尼的塞尔达粉丝的打油诗。请确保包含以下内容：\textcolor{red}{<b>}塞尔达\textcolor{red}{</b>}、\textcolor{red}{<b>}海拉鲁\textcolor{red}{</b>}、\textcolor{red}{<b>}林克\textcolor{red}{</b>}、\textcolor{red}{<b>}加农\textcolor{red}{</b>}。字数不得超过 100 个。\end{CJK}\\
        \textbf{[Case Reform]} \{"key": 2602, \\
        "prompt": " \begin{CJK}{UTF8}{gbsn}写一首关于名叫罗德尼的塞尔达粉丝的打油诗。请确保包含以下内容：塞尔达、海拉鲁、林克、加农。字数不得超过 100 个。\end{CJK}",\\ "instruction\_id\_list": ["keywords:existence"],\\
        "kwargs": [\{"keywords": [\begin{CJK}{UTF8}{gbsn}"塞尔达", "海拉鲁", "林克", "加农"\end{CJK}]\}]\} \\    
        \hline
        \textbf{[Human Post-Editing]} \{"key": 2602, \\
        "prompt": " \begin{CJK}{UTF8}{gbsn}写一首关于名叫罗德尼的塞尔达粉丝的打油诗。请确保包含以下内容：塞尔达、海拉鲁、林克、加侬。字数不得超过 100 个。\end{CJK}",\\ "instruction\_id\_list": ["keywords:existence"],\\
        "kwargs": [\{"keywords": [\begin{CJK}{UTF8}{gbsn}'塞尔达', '海拉鲁', '林克', '加侬'\end{CJK}]\}]\} \\   
        \hline
    \end{tabular}
    \caption{IFEval case}
    \label{tab:construction-case}
    % \vspace{-0.4cm}
\end{table}




