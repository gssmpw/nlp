\begin{table*}[ht]
    \caption{Performance comparison across models on \name tasks, averaged over 17 languages. Bold numbers indicate the best performance in each column. "Func Compl." refers to Function Completion, "Prob. Solving" to Problem Solving, and "Multi Func." to Multiple Functions scenarios where models must select and call one function from multiple options. Note that although DeepSeek-V3 supports 128 context, its API only supports 64K context, and we cannot deploy it on our server. Therefore, we have not evaluated its long-context capabilities yet.}
    % \caption{The results of different models on \name. The  numbers represents the \textbf{average score} across 17 languages. All of the models are instruction-tuned versions. The bolded numbers represent the highest values in each column. Func Compl. demotes Function Completion, while Prob. Coding means Problem Coding. Multi Func. denotes the Multiple Function scenario, where a model is required to call one function from multiple functions. Note that although DeepSeek-V3 supports 128 context, its API only supports 64K context, and we cannot deploy it on our server. Therefore, we have not evaluated its long-context capabilities yet.}
    \label{tab:main_results}
    \vskip 0.1in
    \centering
    \scriptsize
    \setlength{\tabcolsep}{3pt}
    \begin{tabular}{crcccccccccccc}
    \toprule
        \multirow{3}[2]{*}{\textbf{Model}} & \multirow{3}[2]{*}{\textbf{Size}} & \multicolumn{2}{c}{\textbf{Instruction Following}} & \multicolumn{2}{c}{\textbf{Code Generation}} & \multicolumn{2}{c}{\textbf{Reasoning}} & \textbf{Long Context} & \textbf{Tool Use} & \multicolumn{4}{c}{\textbf{Translation}} \\
        \cmidrule(lr){3-4}\cmidrule(lr){5-6}\cmidrule(lr){7-8}\cmidrule(lr){9-9}\cmidrule(lr){10-10}\cmidrule(lr){11-14}
        & & \multirow{2}{*}{\textbf{Rule-based}} & \multirow{2}{*}{\textbf{Model-based}} & \multirow{2}{*}{\textbf{Func Compl.}} & \multirow{2}{*}{\textbf{Prob. Solving}} & \multirow{2}{*}{\textbf{Math}} & \multirow{2}{*}{\textbf{Science}} & \multirow{2}{*}{\textbf{Question Answering}} & \multirow{2}{*}{\textbf{Multi Func.}} & \multicolumn{2}{c}{\textbf{General}} & \multicolumn{2}{c}{\textbf{Domain}} \\
        & & & & & & & & & & En-X & X-En & En-X & X-En \\
        \midrule
        \multirow{2}{*}{InternLM2.5}     & 7B  & 45.7 & 2.6  & 45.4 & 10.3 & 37.4 & 20.6 & 37.5 & 53.2 & 12.7 & 20.2 & 34.4 & 54.0 \\
             & 20B & 51.9 & 4.5  & 51.2 & 14.4 & 42.9 & 24.0 & -    & 26.6 & 14.9 & 19.7 & 34.9 & 53.9 \\
        \midrule
        \multirow{2}{*}{Aya-Expanse}     & 8B  & 51.2 & 9.5  & 33.8 & 7.8  & 50.8 & 26.2 & -    & 41.1 & 21.5 & 26.8 & 45.6 & 51.6 \\
             & 32B & 61.9 & 14.6 & 52.0 & 15.8 & 66.7 & 27.7 & -    & 59.8 & 25.2 & 32.8 & 54.8 & 62.3 \\
        \midrule
        \multirow{2}{*}{Gemma2}          & 9B  & 63.0 & 9.1  & 53.9 & 16.6 & 72.0 & 23.9 & -    & 61.4 & 27.2 & 33.2 & 57.5 & 61.9 \\
                  & 27B & 62.4 & 16.0 & 66.7 & 24.6 & 75.3 & 26.7 & -    & 64.7 & 30.4 & 34.5 & 64.8 & 66.2 \\
        \midrule
        \multirow{2}{*}{Llama3.1}        & 8B  & 62.6 & 5.1  & 52.9 & 14.1 & 63.4 & 23.8 & 68.3 & 45.0 & 24.6 & 29.8 & 53.9 & 62.9 \\
                & 70B & 76.2 & 11.1 & 69.7 & 29.8 & 79.7 & 35.8 & 57.4 & 44.3 & 31.1 & \textbf{35.1} & 64.5 & \textbf{68.2} \\
        \midrule
        \multirow{2}{*}{Qwen2.5}         & 7B  & 65.9 & 10.3 & 68.2 & 24.7 & 63.4 & 27.6 & 53.5 & 48.9 & 16.6 & 25.6 & 46.4 & 60.0 \\
                 & 72B & 80.8 & 34.1 & 78.6 & 45.5 & 77.8 & 39.4 & 80.6 & 61.8 & 25.8 & 33.3 & 60.4 & 66.9 \\
        \midrule
        DeepSeek-V3 & 671B & \textbf{83.9} & \textbf{46.3} & \textbf{83.2} & \textbf{60.4} & \textbf{84.2} & \textbf{47.4} & - & 69.2 & \textbf{33.9} & 34.5 & \textbf{70.3} & 67.8 \\
        \midrule
        GPT-4o-mini     & -   & 79.1 & 23.6 & 78.7 & 37.0 & 76.9 & 34.1 & \textbf{82.1} & \textbf{70.9} & 30.3 & 33.9 & 67.7 & 67.6 \\
        \bottomrule
    \end{tabular}
\end{table*}

\section{Experimental Results}

\subsection{Evaluation Setup}

% \vskip 0.1in
\noindent\textbf{Evaluated Models}
We focus primarily on multilingual post-trained models and evaluate both open-source and proprietary language models\footnote{Unless otherwise specified, all models discussed in this paper are post-trained versions.}, including Llama3.1~\cite{dubey2024llama}, Qwen2.5~\cite{qwen2.5}, Gemma2~\cite{team2024gemma}, InternLM2.5~\cite{cai2024internlm2}, Aya-Expanse~\cite{dang2024aya}, DeepSeek-V3~\cite{DeepSeekAI2024DeepSeekV3TR}, and GPT-4o-mini~\cite{openai2024gpt4o}.
Models detailed descriptions are in Appendix~\ref{sec:model_info}.

% \vskip 0.1in
\noindent\textbf{Inference Configuration}
We adopt greedy decoding on most tasks except the problem solving task, on which the temperature is set to $0.2$.
The default chat template and system prompt of each model are applied.
% Zero-shot prompts are used for most tasks, while zero-shot CoT prompts are used for reasoning tasks.
Detailed prompts are provided in Appendix~\ref{sec:prompts}.
Specifically, for instruction following tasks, the translated instructions are directly used as prompts.
For reasoning tasks, we adopt the zero-shot native chain-of-thought~(CoT) templates in LM-Evaluation-Harness~\cite{eval-harness}.
% For the long-context evaluation, the prompt template is in English, while the haystack, needles and questions are in the language being assessed.
% For other tasks, we use the original prompt templates provided in the corresponding repositories\footnote{\url{https://github.com/EleutherAI/lm-evaluation-harness} \\\url{https://github.com/LiveCodeBench/LiveCodeBench} \\\url{https://github.com/evalplus/evalplus} \\\url{https://github.com/NVIDIA/RULER}}, and change the user inputs to other languages.
For other tasks, we use the original prompt templates provided in the corresponding repositories\footnote{\url{https://github.com/EleutherAI/lm-evaluation-harness} \newline
\url{https://github.com/LiveCodeBench/LiveCodeBench} \newline  
\url{https://github.com/evalplus/evalplus} \newline
\url{https://github.com/NVIDIA/RULER}\newline
\url{https://github.com/lmarena/arena-hard-auto}}, and change the user inputs to other languages.
% More details about the prompt templates can be found in Appendix~\ref{sec:prompts}.


\begin{figure*}[ht]
    % \vskip 0.2in
    \centering
    \includegraphics[ width=1\linewidth]{figure/line_plot.pdf}
    % \vskip -0.1in
    \caption{Performance of several models on the science reasoning task and the domain translation task across different languages. Models show unbalanced multilingual capabilities on these tasks.}
    \label{fig:line_plot}
\end{figure*}

\begin{figure}[ht]
    % \vskip 0.2in
    \centering
    \includegraphics[width=0.9\linewidth]{figure/gap_winrate.pdf}
    \caption{The proportion of tasks where larger models achieve a smaller GAP versus where smaller models perform better across different model families. larger models do not consistently have a smaller GAP, particularly in the Gemma2 family, where the smaller model outperforms the larger one in most cases.}
    \label{fig:winrate}
    % \vskip -0.2in
\end{figure}


\subsection{Multilingual Benchmark Results}
Table~\ref{tab:main_results} shows the overall average performance of each model on each multilingual task.
We take the top-performing ones and present their detailed performance on science reasoning and domain translation tasks, as illustrated in Figure~\ref{fig:line_plot}.
More detailed results are in Appendix~\ref{sec:detailed_results}.

% Main results are demonstrated in Figure~\ref{fig:radar_chart} and Table~\ref{tab:main_results}.
% Each cell in the table displays the average score computed across 17 languages, along with the average performance gap (GAP) between English and other languages.
% GAP is defined as $\frac{\sum_{l\neq en}\max(s(en) - s(l), 0)}{n-1}$, where $s(l)$ denotes the score on the task with language $l$, and $n$ is the number of languages.

% \paragraph{Scaling the model size enhances the multilingual capabilities consistently, but it does not universally mitigate the performance gap.}

\paragraph{Model scaling improves overall multilingual performance while language disparities persist.}
As shown in Table~\ref{tab:main_results}, larger models consistently demonstrate enhanced multilingual capabilities across all domains, with few exceptions.
% One obvious trend in Table~\ref{tab:main_results} is that as a model becomes bigger, the average multilingual capabilities are enhanced in all domains, with few exceptions.
% All evaluated capabilities of Qwen2.5-72B, surpass those of Qwen2.5-7B in multilingual settings. % including instruction following, code generation, reasoning and so on
However, the performance gap between English and non-English languages does not invariably diminish.
We define GAP as the average performance gap between English and other languages:
$$GAP=\frac{\sum_{l\neq \mathrm{en}}\max(s(\mathrm{en}) - s(l), 0)}{n-1},$$ where $s(l)$ denotes the score on the task with language $l$, and $n$ is the number of languages including English.
As shown in Figure~\ref{fig:winrate}, when comparing models of different sizes, the proportion of larger models achieving smaller GAPs only slightly exceeds 0.5 for most model families except Qwen2.5.
Gemma2-9B achieves smaller GAPs than Gemma2-27B in most tasks.
These findings suggest that while scaling model size effectively improves overall multilingual performance, additional strategies may be needed to address the performance disparities across languages.
% This indicates that simply scaling the model size is not a promising way to completely close the performance gap between English and other languages.
% Gemma2-9B achieves even smaller GAPs than Gemma-27B in most tasks.
% As depicted in Figure~\ref{fig:winrate}, most proportions of larger models with smaller GAPs, except for Qwen2.5, are just slightly above 0.5.
% We find that as the model size increases, GAPs on most tasks decreases, but there are still a number of exceptions.
% As demonstrated in Figure~\ref{fig:radar_chart} which shows detailed results of each task in different languages, the capability enhancement of Llama3.1 and Qwen2.5 can be observed in all languages when the average task performance is improved, suggesting that scaling model size can consistently improve the multilingual capabilities across all languages.
% Qwen2.5 shows an improvement of at least 11\% on the rule-based instruction following task across all evaluated languages.
% However, Llama3.1 does not exhibit improvement on the long-context modelling task and the tool use task, which may attributed to the limitations in the corresponding capabilites of Llama3.1-70B.



% \vskip 0.1in
\paragraph{The effective utilization of language-agnostic capabilities remains challenging in multilingual contexts.}
Models' reasoning capabilities vary significantly across languages (Figure~\ref{fig:line_plot}).
% As shown in the upper panel of
While models generally achieve better performance in dominant languages like English, their performance in other languages often lags behind.
This disparity can be attributed to the fact that multilingual task execution depends not only on language-agnostic reasoning but also on language-specific capabilities such as comprehension and generation.
Therefore, when operating in languages where the model has weaker proficiency, it becomes difficult to fully leverage its language-agnostic capabilities.
Interestingly, we observed some unexpected patterns where certain models excel in specific non-dominant languages compared to English.
For example, Qwen2.5 demonstrates superior performance in Korean over English on the science reasoning task.
This counter-intuitive phenomenon warrants further investigation in future work.

% the language-agnostic reasoning capabilities exhibit substantial variations across different languages.
% Models tend to achieve superior performance when utilizing dominant languages, such as English.
% The underlying reason for this phenomenon lies in the fact that multilingual task execution also relies on the language-specific capabilities, such as understanding and generation.
% Consequently, it is difficult to fully leverage language-agnostic capabilities when using a weak language.
% Additionally, it has been observed that certain models demonstrate superior performance in a specific non-dominant language in comparison to English.
% For instance, Qwen2.5 performs better in Korean than in English on the science reasoning task.
% This unreasonable phenomenon requires further investigation, which we leave as future work.
% Through the comparison of multilingual performance across a range of tasks in \name, researchers can obtain a more profound understanding of the language-agnostic capabilities in the multilingual context.
% We define the language-agnostic capability of an LLM as a problem-solving ability that can be applied consistently across different linguistic and cultural environments, without depending on unique language-specific or culture-specific constructs.
% For instance, reasoning and code generation are considered as language-agnostic capabilities ideally.
% The LLM capabilities listed in Figure~\ref{fig:capability} except translation are considerd as the language-agnostic capabilites.
% However, as observed earlier, performance on these tasks exhibits significant variation across different languages.

\paragraph{Model performance exhibits systematic bias towards high-resource languages, with Gemma2 as a notable exception.}
As shown in Figure~\ref{fig:line_plot}, the performance curves of most models exhibit significant fluctuations across languages.
High-resource languages such as French and Chinese consistently outperform low-resource languages like Telugu, Swahili, and Bengali.
For instance, while DeepSeek-V3 achieves over 50\% accuracy in science reasoning tasks for English and French, its performance drops notably to below 40\% for Telugu.
Our evaluation shows that almost all models have at least one language where they significantly underperform.
This pattern can be partially attributed to development strategies - models like Aya-Expanse were not specifically optimized for the full range of languages in our evaluation.
Notably, Gemma2 demonstrates unexpectedly balanced performance across most tasks (Figure~\ref{fig:radar_chart}), despite not being explicitly marketed as a multilingual model.


\paragraph{DeepSeek-V3 narrows the gap between open-source and closed-source models.}
As demonstrated in Table~\ref{tab:main_results} and Figure~\ref{fig:radar_chart}, GPT-4o-mini demonstrates strong multilingual capabilities across various tasks.
While its performance matches that of Qwen2.5-72B on several tasks, it falls short of DeepSeek-V3, the leading open-source model in our evaluation.
This finding suggests that state-of-the-art open-source models are becoming increasingly competitive with their closed-source counterparts.
Due to budget constraints, our evaluation is limited to GPT-4o-mini among closed-source models, and a more comprehensive comparison would be valuable for validating this trend.
% However, Due to limited budget, our evaluation of closed-source models focuses solely on GPT-4o-mini. a more comprehensive comparison involving additional closed-source models would be valuable for validating this trend.
% \paragraph{Comprison between open-source models and closed-source models}
% We only evaluate GPT-4o-mini on our benchmarks due to limited resources.
% As demonstrated in Table~\ref{tab:main_results} and Figure~\ref{fig:radar_chart}, GPT-4o-mini exhibits strong multilingual capabilities across various tasks.
% Its performance on several tasks is comparable to that of the open-source model Qwen2.5-72B, and inferior to the strongest open-source model evaluated in this study, DeepSeek-V3.
% We believe that, with the emergence of DeepSeek-V3, the gap between open-source and closed-source models is narrowing.
% Further efforts are required to conduct a comprehensive comparison between open-source and closed-source models.


% \vskip 0.1in
\paragraph{Translation capabilities exhibit a positive correlation with other evaluated capabilities.}
We analyze the relationship between model's English-to-X translation capability and other capabilities using Spearman correlation coefficients (the right panel of Figure~\ref{fig:spearman}).
When calculating correlations between domain-specific translation and task performance, we exclusively use data from the corresponding domains.
The analysis reveals that domain-specific translation performance generally exhibits stronger correlations with task performance compared to general translation capabilities.
A notable exception is that in the rule-based instruction-following task, we observe an inverse scaling effect: larger LLMs produce lower-quality translations compared to their smaller counterparts.
This occurs because larger LLMs are more likely to execute instructions rather than strictly perform translation, known as prompt injection.
% However, rule-based instruction-following tasks present a notable exception to this pattern.
% We observe a reverse scaling phenomenon where larger LLMs produce poorer translations compared to smaller LLMs.
% This counter-intuitive phenomenon occurs because larger LLMs tend to prioritize instruction execution over literal translation, a behavior known as prompt injection.


% \paragraph{The multilingual capabilities of most models are unbalanced, especially on low-resource languages.}

% Generally, high-resource languages like French and Chinese exhibit high performance compared to low-resource languages like Teluge, Swahili, and Bengali.
% the fluctuating lines for most models indicate substantial performance discrepancies across languages.
% For example, while Qwen2.5-7B demonstrates robust rule-based instruction-following abilities in English and Chinese, achieving accuracy rates exceeding 75\%, its performance on Swahili falls significantly, with accuracy below 30\%.
% For instance, while DeekSeek-V3 demonstrates strong science reasoning abilities in English and French, achieving accuracy over 50\%, its performance on Teluge falls significantly, with accuracy below 40\%.
% Among the languages we include, almost all models have one or more weak languages.
% One reason is that models like Aya-Expanse do not account for or optimize across all languages here.
% Interestingly, the performance of Gemma2 is relatively balanced in the majority of tasks as shown in Figure~\ref{fig:radar_chart}, despite not claiming multilingualism.
% Gemma2-27B achieves accuracies exceeding 55\% on the rule-based instruction-following task across all languages.


\begin{figure}[!t]
    % \vskip 0.2in
    \centering
    \includegraphics[width=\linewidth]{figure/heatmap.pdf}
    \vskip -0.1in
    \caption{F1 scores of correct answers in other languages to English across six tasks. Using English correctness as a reference, we treat other languagesâ€™ correctness as predictions to measure their agreement with English. Results show high agreement between English and other languages on these tasks.}
    % \vskip -0.2in
    \label{fig:agreement}
\end{figure}




\begin{table*}[ht]
    \caption{Different metrics are used to evaluate the En-X translation performance of selected models on specific domains.}
    \label{tab:domain_metrics}
    \vskip 0.15in
    % \small
    \scriptsize
    \centering
    \begin{tabular}{c|c|cccc|cccc|cccc}
        \toprule
        \multirow{2}{*}{\textbf{Metric}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{Translation}\\ \textbf{Model}\end{tabular}} & \multicolumn{4}{c|}{\textbf{Reasoning - Math}} & \multicolumn{4}{c|}{\textbf{Reasoning - Science}} & \multicolumn{4}{c}{\textbf{Code generation - Prob. Solving}} \\
         & & zh & de & sw & te & zh & de & sw & te & zh & de & sw & te \\
        \midrule
        \multirow{3}{*}{spBLEU}& Gemma2-27B & 40.0 & 51.4 & 38.2 & 29.2 & 80.6 & 84.8 & 66.2 & 57.5 & 85.5 & 78.5 & 76.2 & 52.3 \\
        & Llama3.1-70B & 35.2 & 54.4 & 36.6 & 35.0 & 71.8 & 84.9 & 64.0 & 65.3 & 84.8 & 78.5 & 75.5 & 56.7 \\
        & Qwen2.5-72B & 37.7 & 50.1 & 13.5 & 12.6 & 77.0 & 79.4 & 41.2 & 40.9 & 84.6 & 73.0 & 48.8 & 42.1 \\
        \midrule
        \multirow{3}{*}{TER} & Gemma2-27B & 36.2 & 32.1 & 40.2 & 58.6 & 15.7 & 12.8 & 26.9 & 33.5 & 15.3 & 15.6 & 17.4 & 33.3 \\
        & Llama3.1-70B & 36.0 & 30.1 & 44.0 & 51.8 & 19.6 & 12.9 & 28.4 & 26.9 & 15.1 & 15.4 & 17.9 & 46.8 \\
        & Qwen2.5-72B & 33.1 & 33.5 & 76.8 & 85.7 & 15.4 & 16.8 & 65.8 & 51.4 & 14.3 & 19.6 & 38.3 & 53.5 \\
        \midrule
        \multirow{3}{*}{\textsc{xCOMET}} & Gemma2-27B & 86.0 & 96.1 & 68.1 & 71.8 & 63.2 & 77.6 & 36.3 & 44.1 & 45.2 & 46.7 & 27.0 & 25.0 \\
        & Llama3.1-70B & 86.8 & 95.6 & 66.1 & 74.0 & 63.7 & 77.4 & 37.1 & 46.8 & 43.5 & 46.3 & 27.8 & 28.9 \\
        & Qwen2.5-72B & 87.6 & 95.6 & 24.5 & 30.1 & 65.2 & 76.3 & 20.3 & 28.4 & 45.0 & 45.7 & 18.0 & 18.4 \\
        \midrule
        \multirow{3}{*}{Retention Rate} & Gemma2-27B & 1.00 & 1.08 & 0.98 & 0.99 & 0.98 & 0.98 & 1.07 & 0.81 & 1.02 & 0.96 & 0.89 & 0.95 \\
        & Llama3.1-70B & 1.01 & 1.06 & 1.00 & 0.97 & 0.92 & 1.06 & 0.99 & 0.77 & 1.01 & 0.98 & 0.91 & 0.89 \\
        & Qwen2.5-72B & 1.03 & 1.04 & 0.71 & 0.71 & 0.90 & 0.98 & 1.04 & 0.79 & 1.00 & 1.04 & 0.93 & 0.86 \\
        \bottomrule
    \end{tabular}
\end{table*}

\begin{figure*}[ht]
    % \vskip 0.1in
    \centering
    \includegraphics[width=0.9\linewidth]{figure/spearman.pdf}
    \vskip -0.1in
    \caption{\textbf{Left:} Spearman Correlations between the performance of two models within the same family across different languages. \textbf{Right:} Spearman Correlations between the performance of models on general translation and domain translation.}
    \label{fig:spearman}
    % \vskip -0.2in
\end{figure*}

\paragraph{Models within the same family exhibit consistent performance patterns across languages.}
We calculate Spearman correlation coefficients to analyze the performance similarity between models of the same family across different languages for each task.
As shown in the left panel of Figure~\ref{fig:spearman}, models within the same family demonstrate strong correlations across various tasks, with most correlation coefficients exceeding 0.7.
The exception is Llama3.1's negative correlation in tool use tasks.
% More results on other tasks are illustrated in Figure~\ref{fig:radar_chart}, where we can find that Llama3.1-8B and Llama3.1-70B exhibit similar performance pattern across various tasks.
% Both Qwen2.5-7B and Qwen2.5-72B perform relatively poor on Swahili and Telegu on all tasks.


% \paragraph{Models within the same family share similar patterns of unbalanced performance across various tasks.}
% We calculate the Spearman correlation coefficients between the performance of two models from the same family on a task in different languages.
% Left image of Figure~\ref{fig:spearman} demonstrates the strong correlations between models within the same family across various tasks, with most Spearman correlation coefficients exceed 0.7.
% The negative correlation of Llama3.1 on tool use can be attributed to the deficiency of tool use capability of Llama3.1-70B. 


% Language-specific capabilities such as understanding and generation can generally affect the performance of downstream tasks.
% \begin{table*}[ht]
%     \caption{Spearman correlation coefficients between models within the same familiy on each task in different languages. The performance of models in the same family on a task show strong correlations across different languages.}
%     \label{tab:spearman}
%     \vskip 0.15in
%     \centering
%     \footnotesize
%     \begin{tabular}{ccccccccc}
%         \toprule
%         Family & Rule-based & Model-based & Func Compl. & Prob. Coding & Math & Science & Synthetic & Multi Func. \\
%         % Family & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
%         \midrule
%         Llama3.1 & 0.77 & 0.89 & 0.73 & 0.84 & 0.57 & 0.70 & 0.90 & -0.17 \\
%         Qwen2.5  & 0.92 & 0.49 & 0.79 & 0.83 & 0.60 & 0.29 & 0.92 &  0.75 \\
%         \bottomrule
%     \end{tabular}
% \end{table*}


% \paragraph{Scaling the model size can mitigate the GAPs in most cases, but it is not universal.}

% For example, the GAP of Llama3.1-70B is bigger than Llama3.1-8B on several tasks, such as science reasoning and long context modeling.
% Gemma2-27B has larger GAPs on the reasoning tasks and the tool use task.
% The standard deviation reflects the degree of performance variability or imbalance across different languages.
% Other multilingual models have a higher GAP due to the poor performance on some low-resource languages.


% \begin{table*}[ht]
%     \caption{Spearman correlation coefficients between translation tasks and other tasks, with all $p$-values less than 0.05. Translation capabilities exhibit a strong correlation with other multilingual capabilities.}
%     \label{tab:translation_spearman}
%     \vskip 0.15in
%     \centering
%     \footnotesize
%     \begin{tabular}{cccccccc}
%         \toprule
%         & Rule-based & Model-based & Func Compl. & Prob. Coding & Math & Science & Multi Func. \\
%         \midrule
%         Genaral Translation & 0.71 & 0.76 & 0.76 & 0.73 & 0.94 & 0.68 & 0.60 \\
%         Domain Translation & 0.59 & 0.85 & 0.85 & 0.78 & 0.92 & 0.71 & 0.70 \\
%         \bottomrule
%     \end{tabular}
% \end{table*}

% We calculate the Spearman correlation coefficients between En-X translation performance and the performance on other tasks, as shown in right image of Figure~\ref{fig:spearman}.
% Only specific domain data is used when calculating correlations between the domain translation performance and the task performance.
% In general, domain-specific translation performance shows a stronger correlation with other tasks compared to general translation performance.
% However, an obvious exception to this trend is the rule-based instruction-following task.






    % \setlength{\tabcolsep}{4.9pt}
    % \begin{tabular}{cc|cccccccccccccccccc}
        % \toprule
        % \textbf{\textit{\ifeval}} & Size & STD$\downarrow$ & En & Zh & Es & Fr & De & Ru & Ja & Th & Sw & Bn & Te & Ar & Ko & Vi & Cs & Hu & Sr \\
        % \midrule
        % InternLM2.5        &   7B & 14.0 & 66.1 & 61.9 & 61.6 & 57.5 & 62.0 & 51.3 & 53.5 & 29.3 & 26.4 & 25.6 & 27.1 & 37.6 & 50.7 & 43.7 & 51.7 & 36.8 & 34.7 \\
        % Qwen2.5            &   7B & 14.6 & 80.2 & 75.8 & 77.8 & 76.4 & 76.9 & 71.3 & 72.3 & 67.0 & 26.6 & 56.4 & 35.8 & 68.4 & 69.7 & 72.9 & 70.2 & 61.4 & 60.5 \\
        % % LLaMAX3-Alpaca     &   8B &  3.8 & 37.4 & 30.6 & 36.1 & 35.5 & 34.8 & 32.3 & 30.3 & 31.3 & 28.7 & 25.2 & 23.8 & 31.5 & 26.6 & 32.8 & 34.5 & 32.8 & 30.5 \\
        % Llama3.1           &   8B & 11.6 & 81.7 & 68.0 & 72.8 & 75.7 & 74.3 & 67.8 & 61.6 & 60.6 & 43.7 & 46.6 & 36.7 & 64.4 & 60.1 & 63.6 & 66.3 & 60.2 & 59.7 \\
        % Aya-Expanse        &   8B & 17.8 & 71.2 & 64.1 & 67.0 & 65.9 & 66.6 & 57.8 & 64.3 & 32.2 & 21.7 & 25.3 & 20.0 & 64.5 & 56.8 & 56.9 & 61.5 & 38.1 & 36.3 \\
        % Gemma2             &   9B &  5.0 & 70.3 & 63.5 & 68.6 & 70.4 & 65.1 & 65.9 & 63.2 & 63.7 & 60.7 & 54.4 & 51.8 & 64.7 & 63.2 & 64.3 & 62.2 & 61.6 & 56.7 \\
        % InternLM2.5        &  20B & 16.1 & 74.9 & 69.9 & 66.1 & 65.6 & 68.4 & 59.0 & 59.1 & 36.3 & 27.1 & 28.3 & 23.0 & 50.7 & 55.5 & 55.7 & 57.4 & 48.3 & 37.3 \\
        % Gemma2             &  27B &  3.1 & 69.7 & 62.8 & 66.5 & 66.5 & 62.8 & 61.8 & 59.1 & 62.9 & 60.7 & 58.4 & 56.7 & 61.6 & 61.6 & 63.9 & 62.2 & 62.2 & 61.0 \\
        % Aya-Expanse        &  32B & 15.3 & 77.0 & 70.0 & 71.9 & 73.7 & 74.2 & 69.5 & 75.4 & 49.6 & 29.5 & 42.6 & 29.8 & 69.9 & 68.4 & 69.1 & 67.3 & 58.0 & 55.5 \\
        % Llama3.1           &  70B &  7.5 & 90.3 & 77.8 & 82.1 & 83.4 & 84.9 & 75.4 & 75.6 & 68.3 & 70.9 & 68.2 & 57.0 & 75.4 & 75.8 & 78.1 & 77.7 & 78.2 & 76.5 \\
        % Qwen2.5            &  72B & 10.6 & 90.5 & 86.1 & 86.8 & 89.3 & 86.5 & 83.6 & 83.4 & 81.0 & 52.7 & 77.6 & 55.1 & 83.5 & 83.8 & 83.1 & 85.2 & 82.5 & 82.1 \\
        % \midrule
        % \textbf{\textit{\mgsm}} & Size & STD$\downarrow$ & En & Zh & Es & Fr & De & Ru & Ja & Th & Sw & Bn & Te & Ar & Ko & Vi & Cs & Hu & Sr \\
        % \midrule
        % InternLM2.5    &   7B & 24.6 & 64.4 & 73.2 & 61.6 & 53.2 & 58.0 & 58.8 & 34.0 &  7.6 &  0.0 &  0.4 &  1.2 & 24.0 & 55.6 & 38.8 & 52.4 & 24.0 & 28.0 \\
        % Qwen2.5        &   7B & 17.3 & 81.2 & 76.8 & 73.2 & 63.2 & 69.2 & 65.6 & 70.0 & 70.0 & 14.0 & 60.0 & 27.2 & 70.0 & 76.8 & 73.2 & 64.0 & 59.6 & 63.2 \\
        % % LLaMAX3-Alpaca &   8B &  3.1 & 12.8 &  7.2 & 14.0 & 12.8 &  8.4 & 12.0 &  6.4 & 10.4 &  7.6 &  8.8 &  3.2 & 13.2 & 10.4 & 14.0 & 12.8 & 13.2 &  9.2 \\
        % Llama3.1       &   8B & 10.2 & 77.2 & 69.6 & 72.8 & 66.0 & 66.8 & 70.0 & 61.6 & 70.4 & 58.0 & 51.2 & 34.0 & 62.8 & 70.4 & 68.8 & 62.4 & 52.8 & 63.6 \\
        % Aya-Expanse    &   8B & 23.9 & 79.6 & 68.8 & 67.2 & 61.2 & 62.4 & 63.2 & 64.4 & 18.4 &  6.8 & 24.0 &  6.0 & 65.6 & 70.0 & 66.0 & 64.8 & 33.2 & 42.4 \\
        % Gemma2         &   9B &  4.0 & 76.4 & 74.8 & 73.6 & 61.6 & 74.4 & 70.0 & 72.0 & 75.2 & 68.8 & 70.8 & 64.8 & 70.4 & 76.8 & 74.0 & 73.6 & 74.4 & 71.6 \\
        % InternLM2.5    &  20B & 23.3 & 65.6 & 77.2 & 66.8 & 46.4 & 64.0 & 52.0 & 49.2 & 18.0 &  8.0 &  3.6 &  2.8 & 33.6 & 62.8 & 49.6 & 58.8 & 37.6 & 34.0 \\
        % Gemma2         &  27B &  4.1 & 80.4 & 77.2 & 76.0 & 63.2 & 75.6 & 74.0 & 74.0 & 76.4 & 69.2 & 75.6 & 72.8 & 78.8 & 78.4 & 76.4 & 76.4 & 76.0 & 80.4 \\
        % Aya-Expanse    &  32B & 18.8 & 88.0 & 78.4 & 79.2 & 68.0 & 74.0 & 75.2 & 78.4 & 53.6 & 25.2 & 56.4 & 20.4 & 76.0 & 79.6 & 78.4 & 74.8 & 60.8 & 66.8 \\
        % Llama3.1       &  70B &  7.6 & 82.8 & 84.4 & 81.2 & 68.8 & 82.0 & 83.2 & 81.6 & 82.4 & 85.6 & 73.6 & 55.2 & 81.6 & 87.6 & 82.0 & 79.6 & 80.8 & 82.0 \\
        % Qwen2.5        &  72B &  9.7 & 85.2 & 82.0 & 81.6 & 75.2 & 79.2 & 76.4 & 82.4 & 74.4 & 57.6 & 83.2 & 50.4 & 83.2 & 88.4 & 81.6 & 80.0 & 80.4 & 82.0 \\
        % \midrule
    %     \textbf{\textit{\gpqa}} & Size & STD$\downarrow$ & En & Zh & Es & Fr & De & Ru & Ja & Th & Sw & Bn & Te & Ar & Ko & Vi & Cs & Hu & Sr \\
    %     \midrule
    %     InternLM2.5    &   7B &  5.9 & 31.0 & 26.8 & 25.7 & 27.7 & 25.4 & 23.7 & 21.9 & 10.0 & 20.1 & 13.6 & 18.3 & 17.4 & 22.1 & 19.2 & 20.8 & 10.5 & 15.8 \\
    %     Qwen2.5        &   7B &  3.6 & 30.4 & 27.7 & 30.6 & 29.5 & 27.5 & 27.0 & 30.6 & 32.8 & 18.8 & 24.6 & 20.8 & 27.5 & 28.3 & 30.8 & 29.0 & 25.2 & 28.1 \\
    %     % LLaMAX3-Alpaca &   8B &  1.9 & 21.4 & 24.3 & 23.2 & 22.8 & 25.0 & 20.8 & 19.6 & 23.9 & 24.8 & 19.0 & 22.1 & 20.8 & 19.6 & 20.5 & 23.0 & 22.8 & 20.3 \\
    %     Llama3.1       &   8B &  3.7 & 27.0 & 23.4 & 24.8 & 29.9 & 28.1 & 26.1 & 25.7 & 27.0 & 21.4 & 20.5 & 15.0 & 22.1 & 19.6 & 26.6 & 20.5 & 23.2 & 24.3 \\
    %     Aya-Expanse    &   8B &  2.9 & 25.9 & 27.7 & 26.8 & 25.4 & 25.7 & 31.0 & 25.7 & 25.9 & 21.2 & 23.9 & 20.1 & 26.8 & 30.1 & 30.6 & 25.7 & 26.1 & 26.1 \\
    %     Gemma2         &   9B &  1.5 & 25.4 & 22.3 & 22.5 & 25.2 & 26.6 & 24.3 & 23.7 & 24.1 & 24.1 & 26.3 & 22.8 & 20.8 & 23.0 & 24.8 & 23.2 & 24.1 & 22.8 \\
    %     InternLM2.5    &  20B &  6.5 & 28.8 & 34.8 & 27.7 & 29.7 & 29.5 & 26.6 & 25.2 & 18.1 &  9.8 & 13.4 & 17.6 & 24.3 & 23.7 & 25.9 & 30.6 & 22.1 & 20.8 \\
    %     Gemma2         &  27B &  2.8 & 30.8 & 29.0 & 28.3 & 27.5 & 24.3 & 28.8 & 24.1 & 31.2 & 23.4 & 26.3 & 29.2 & 27.5 & 22.5 & 27.5 & 24.1 & 27.7 & 22.3 \\
    %     Aya-Expanse    &  32B &  3.1 & 28.3 & 30.4 & 31.2 & 29.5 & 30.4 & 30.6 & 31.0 & 24.3 & 21.6 & 25.2 & 21.9 & 29.2 & 29.5 & 29.5 & 25.9 & 27.2 & 25.0 \\
    %     Llama3.1       &  70B &  5.3 & 41.7 & 38.6 & 40.0 & 43.5 & 40.6 & 39.3 & 32.4 & 33.0 & 28.8 & 29.9 & 23.4 & 36.6 & 35.5 & 39.1 & 37.7 & 38.2 & 31.0 \\
    %     Qwen2.5        &  72B &  5.2 & 39.7 & 41.3 & 41.5 & 42.4 & 43.8 & 42.4 & 43.5 & 40.8 & 29.7 & 33.3 & 26.1 & 36.8 & 43.5 & 41.1 & 39.5 & 44.6 & 39.7 \\
    % \bottomrule
    % \end{tabular}



% \begin{table*}[ht]
% \centering
% \setlength{\tabcolsep}{3pt}
% \scriptsize
% \begin{tabular}{lccccccccccccccccc}
% \toprule
%  & En & Zh & Es & Fr & De & Ru & Ja & Th & Sw & Bn & Te & Ar & Ko & Vi & Cs & Hu & Sr \\
%  \midrule
% Llama3.1-8B-Instruct & 43.48 & 48.72 & 46.33 & 46.41 & 43.41 & 32.60 & 36.48 & 29.29 & 24.82 & 15.58 & 40.81 & 31.76 & 35.51 & 30.15 & 34.49 & 34.64 & 68.75 \\
% Qwen2.5-7B-Instruct & 68.32 & 59.58 & 58.64 & 56.06 & 56.23 & 55.79 & 49.27 & 64.50 & 34.76 & 48.16 & 40.57 & 55.12 & 52.44 & 59.46 & 53.11 & 46.30 & 51.00 \\
% InternLM2.5-7B-chat-1m & 63.10 & 43.48 & 48.72 & 46.33 & 46.41 & 43.41 & 32.60 & 36.48 & 29.29 & 24.82 & 15.58 & 40.81 & 31.76 & 35.51 & 30.15 & 34.49 & 34.64 \\
% Llama3.1-70B-Instruct & 75.67 & 50.50 & 68.31 & 65.44 & 67.78 & 63.94 & 53.78 & 63.18 & 37.29 & 46.49 & 34.95 & 56.39 & 51.75 & 62.56 & 58.57 & 53.86 & 65.70 \\
% Qwen2.5-72B-Instruct & 86.50 & 85.39 & 83.34 & 82.75 & 82.46 & 79.22 & 80.88 & 85.47 & 73.03 & 75.45 & 71.66 & 80.84 & 82.66 & 84.42 & 80.45 & 76.44 & 79.53 \\
% GPT-4o-mini &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  & \\
% \bottomrule
% \end{tabular}
% \caption{The results of different models on xRULER.}
% \label{tab:ruler}
% \end{table*}


