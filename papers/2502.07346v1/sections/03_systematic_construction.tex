\section{Benchmark Construction}
In this section, we extend the evaluation of the core capabilities of LLMs into multilingual scenarios.
% As shown in Figure~\ref{fig:overview}, \name encompasses multiway parallel data spanning 17 languages~(\S~\ref{sec:lg_selection}), enabling fair cross-lingual assessments and comparisons. 
To ensure sufficient linguistic diversity, we select 16 non-English languages~(\S~\ref{sec:lg_selection}).
% Meanwhile, its comprehensive coverage of diverse tasks~(\S~\ref{sec:capability_selection}) facilitates the evaluation of multiple capabilities across a spectrum of linguistic contexts.
Meanwhile, a diverse set of tasks designed to evaluate 6 crucial LLM capabilities is chosen to facilitate comprehensive assessment~(\S~\ref{sec:capability_selection}).
% Notably, its precise human annotation~(\S~\ref{sec:construction_process}) guarantee the integrity and trustworthiness of the data.
Subsequently, we introduce a rigorous pipeline~(\S~\ref{sec:construction_process}) that incorporates human annotators and LLMs to obtain a high-quality benchmark.


\subsection{Language Selection}
\label{sec:lg_selection}
\name supports 17 selected languages to represent diverse language families and writing systems~(Table~\ref{tab:lg_selection}).
% 17 languages~(\textit{en, es, fr, de, ru, bn, ja, th, sw, zh, te, ar, ko, sr, cs, hu, vi}) supported by \name, cover diverse language families and script systems~(Table~\ref{tab:lg_selection}). 


\subsection{Capabilities Selection}
\label{sec:capability_selection}
% \renewcommand{\arraystretch}{1.4} % Default value: 1
% \begin{table*}[!ht]
%     \caption{Selection of core capabilities and details of task data.}
%     \label{tab:task_detail}
%     \vskip 0.15in
%     \centering
%     \resizebox{0.95\linewidth}{!}{
%     \begin{tabular}{c|c|c|c|c|c|c|c}
%     \toprule
%         \textbf{Capability} & \textbf{Dataset} & \textbf{\# Sample} & \textbf{Metric} & \textbf{Capability} & \textbf{Dataset} & \textbf{\# Sample} & \textbf{Metric} \\ 
%     \midrule
%         Instruction& IFeval & 429 & Accuracy & \multirow{2}{*}{Reasoning} & MGSM & 250 & \multirow{2}{*}{Exact Match} \\ 
%         \cline{2-4} \cline{6-7}
%         Following & Arena-hard & 500 & Win Rate & ~ & GPQA & 448 & ~ \\ 
%         \hline
%         Code & Humaneval+ & 164 & \multirow{2}{*}{Pass@1} & \multirow{2}{*}{Translation} & Flores+TED+WMT24 & 1012 & \multirow{2}{*}{spBLEU} \\ 
%         \cline{2-3} \cline{6-7}
%         Generation & LiveCodeBench & 713 & & & Domain Translation & 2781 & ~ \\ 
%         \hline
%         Long Context & RULER & 100 & Exact Match & Tool Use & Nexus & 318~\footnote{We only adopt the standardized\_queries subset which contains 318 samples.} & Accuracy \\ 
%     \bottomrule
%     \end{tabular}}
%     \vskip -0.1in
% \end{table*}


LLMs have demonstrated proficiency in understanding tasks such as text classification, sentiment analysis, and so on. 
However, their capabilities transcend text understanding, possessing the following intrinsic capabilities:

\begin{itemize} [nosep,itemsep=1pt,leftmargin=0.1cm]
    \item \textbf{Instruction Following:}  Following instructions capability is categorized into two distinct tasks based on evaluation paradigms: rule-based and model-based assessment.
    \item \textbf{Reasoning:} The capability to reason through intricate scenarios including both math reasoning and natural scientific~(physics, chemistry, and biology) reasoning tasks.
    \item \textbf{Code Generation:} We primarily consider Python executable code generation in two settings, function completion and programming problem solving.
    % \item \textbf{Long Context:} The ability to handle long contextual information. We mainly use synthetic long-context data to assess the ability. 
    \item \textbf{Long Context Modeling:} The ability to extract evidence from lengthy documents. We evaluate this capability through question-answering tasks with long documents ~(128k tokens).
    \item \textbf{Tool Use:} We assess the capability of utilizing tools effectively to correctly select and invoke a single function from multiple available functions based on given user queries.
    \item \textbf{Translation:} Translation involves accurately converting text between languages while preserving semantic meaning. Beyond traditional translation tasks, we introduce the Domain Translation task, a by-product of the \name construction process. This task challenges models to translate specialized terminology and determine whether specific segments should be translated.
\end{itemize}

% \begingroup
% \renewcommand{\arraystretch}{1.3} % Default value: 1
% \begin{table}[!t]
%     \caption{Selection of core capabilities and details of task data. For IFEval, we filter out all language specific instructions, thus remaining 429 samples. For Nexus dataset, we only adopt the standardized\_queries subset which contains 318 samples. For general translation datasets, the number of samples may vary in different translation directions, according to the number of parallel samples in TED and WMT24.}
%     \label{tab:task_detail}
%     \vskip 0.15in
%     \footnotesize
%     \centering
%     \resizebox{0.95\linewidth}{!}{
%     \begin{tabular}{c|c|c|c|c}
%     \toprule
%         \textbf{Capability} & \textbf{Category} & \textbf{Dataset} & \textbf{\# Samples} & \textbf{Metric} \\ 
%     \midrule
%         Instruction & Rule-based & IFeval & 429 & Accuracy  \\ 
%         \cline{2-5}
%         Following & Model-based & Arena-hard & 500 & Win Rate \\ 
%         \hline
%         \multirow{2}{*}{Reasoning} & Math & MGSM & 250 & \multirow{2}{*}{Exact Match} \\
%         \cline{2-4}
%         & Science & GPQA & 448 & \\
%         \hline
%         Code & \makecell{Function\\Completion} & Humaneval+ & 164 & \multirow{2}{*}{Pass@1}  \\ 
%         \cline{2-4}
%         Generation & \makecell{Problem\\Solving} & LiveCodeBench\_v4 & 713 & \\ 
%         \hline
%         Long Context & \makecell{Question\\Answering} & RULER & 800 & Exact Match \\
%         \hline
%         Tool Use & \makecell{Multiple\\Functions} & Nexus & 318 & Accuracy \\
%         \hline
%         \multirow{2}{*}{Translation} & General & Flores+TED+WMT24 & $\ge$1012 & \multirow{2}{*}{spBLEU} \\
%         \cline{2-4}
%         & Domain & Annotated data above& 2781 &  \\
%     \bottomrule
%     \end{tabular}}
%     \vskip -0.2in
% \end{table}
% \endgroup


Further details on the datasets, sample sizes, and evaluation metrics are provided in Table~\ref{tab:task_detail}.
More detailed information can be found in Appendix~\ref{sec:appendix-task}.


\begingroup
\renewcommand{\arraystretch}{1.3}
\begin{table}[t!]
    \caption{One example in rule-based instruction following task, which includes complex constraints. First, we enclose these constraints with special symbols and then use a machine translation system to translate from English to the target language. Finally, we reform the case structure by extracting the constraint from machine translation for human post-editing.}
    \label{tab:construction-case}
    \vskip 0.1in
    \centering 
    \tiny
    \begin{tabular}{|p{7.8cm}|}
        \hline
        \textbf{[Original Text]:} 
\{prompt: Create an ad copy by expanding "Get 40 miles per gallon on the highway" in the form of a QA with a weird style. Your response should contain less than 8 sentences. Do not include keywords 'mileage' or 'fuel' in your response. \\
instruction\_id\_list: ['length\_constraints: number\_sentences', 'keywords: forbidden\_words'] \\
kwargs: [\{'relation': 'less than', 'num\_sentences': 8\}, \{'forbidden\_words': ['mileage', 'fuel']\}]\} \\
        \hline
        
        \textbf{[Translation Input]:} 
Create an ad copy by expanding "Get 40 miles per gallon on the highway" in the form of a QA with a weird style. Your response should contain less than 8 sentences. Do not include keywords '\textcolor{red}{$<$b$>$}mileage\textcolor{red}{$<$/b$>$}' or '\textcolor{red}{$<$b$>$}fuel\textcolor{red}{$<$/b$>$}' in your response. \\
        \hline
        
        \textbf{[Google Translation Result]:} 
        \begin{CJK}{UTF8}{gbsn}以风格怪异的问答形式扩展“在高速公路上每加仑行驶 40 英里”来创建广告文案。您的回复应少于 8 个句子。请勿在回复中包含关键字“\textcolor{red}{$<$b$>$}里程\textcolor{red}{$<$/b$>$}”或“\textcolor{red}{$<$b$>$}燃料\textcolor{red}{$<$/b$>$}”。\end{CJK} \\
        
        % \begin{CJK}{UTF8}{gbsn}写一首关于名叫罗德尼的塞尔达粉丝的打油诗。请确保包含以下内容：\textcolor{red}{<b>}塞尔达\textcolor{red}{</b>}、\textcolor{red}{<b>}海拉鲁\textcolor{red}{</b>}、\textcolor{red}{<b>}林克\textcolor{red}{</b>}、\textcolor{red}{<b>}加农\textcolor{red}{</b>}。字数不得超过 100 个。\end{CJK}\\
        
        \hline
    
        \textbf{[Case Reform]} \{
        prompt: \begin{CJK}{UTF8}{gbsn}以风格怪异的问答形式扩展“在高速公路上每加仑行驶 40 英里”来创建广告文案。您的回复应少于 8 个句子。请勿在回复中包含关键字“里程”或“燃料”。\end{CJK}
        \\
instruction\_id\_list: ['length\_constraints:number\_sentences', 'keywords:forbidden\_words']
kwargs: [\{'relation': 'less than', 'num\_sentences': 8\}, {'forbidden\_words': [\begin{CJK}{UTF8}{gbsn}'里程', '燃料'\end{CJK}]}] \} \\ 
        \hline
        \textbf{[Human Post-Editing]} \{
        "prompt": \begin{CJK}{UTF8}{gbsn}以一种奇特风格的问答形式展开“在高速公路上每加仑行驶40英里”这句话，创建为一个广告文案。你的回答应该少于8句话。不要在你的回复中包含关键字“里程”或“燃料”。\end{CJK},\\ 
        instruction\_id\_list: ['length\_constraints:number\_sentences', 'keywords:forbidden\_words']
kwargs: [\{'relation': 'less than', 'num\_sentences': 8\}, {'forbidden\_words': [\begin{CJK}{UTF8}{gbsn}'里程', '燃料'\end{CJK}]}] \} \\ 
        \hline
    \end{tabular}
        % \vskip -0.55in
    % \vspace{-0.4cm}
\end{table}
\endgroup


\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.7\linewidth]{figure/process.pdf}
    \caption{The construction process of \name involves three steps: Step 1) translating data from English to non-English; Step 2) post-editing each sample by three human annotators; Step 3) selecting the final translation version.}
    \label{fig:overview}
    \vskip -0.2in
\end{figure*}


% follow complex instructions~(\textit{Instruction Following Capability}), reason through intricate scenarios~(\textit{Reasoning}), handle long contextual information~(\textit{Long Context Modeling Capability}), generate code autonomously~(\textit{Code Generation Capability}), utilize tools effectively~(\textit{Tool Usage Capability}), and navigate the complicated landscape of machine translation~(\textit{Translation Capability}). 
% Here, we extend the evaluation of these complex capabilities in multilingual scenarios, as shown in Table~\ref{tab:task_detail}.

% \begin{itemize} [nosep,itemsep=1pt,leftmargin=0.3cm]
%     \item Instruction Following Capability: In the light of varied evaluation methods - rule-based or model-based - we introduce two distinct tasks.
%     \item Reasoning: We include both mathematical reasoning and scientific (physics, chemistry, and biology) reasoning tasks 
%     \item Long Context Modeling Capability: We focus on the evaluation of long context in multilingual settings.
%     \item Code Generation Capability: We primarile consider Python code generation in two settings, function completion and programming problem solving.
%     \item Tool Usage Capability: We assess the ability to correctly select and invoke a single function from multiple available functions based on a given user query.
%     \item Translation Capability: Beyond traditional translation tasks, we introduce the Domain Translation task, a by-product of the \name construction process. This task challenges the model to determine whether a given segment should be translated.
% \end{itemize}



\subsection{Construction}
\label{sec:construction_process}
The way to obtain \name consists of three steps, as shown in Figure~\ref{fig:overview}: 1) translate data from English to non-English by machines; 2) post-edit each sample by three native annotators; 3) pick the final translation version by GPT-4o-mini.


% For Step 1, we first conduct a preliminary study by randomly selecting a small sample set and translating it with both GPT-4o and Google Translate. 
% Interestingly, GPT-4o's translations don't always outperform, even for high-resource languages. 
% Hence, our choice of translator is based on the performance of this preliminary study.
\paragraph{Step 1: Translating data from English to selected non-English languages by machine translation systems.}
We select between traditional translators such as Google Translate, and LLM-based ones like GPT-4o, depending on whether the task contains extractable constraints.
As illustrated in Figure~\ref{fig:step1}, if the task data contains constraints that are hard to extract, we prompt GPT-4o to translate the data and satisfy the constraints.
Otherwise, we use Google Translate along with extraction tools.
Extraction tools can include methods for extracting translated keywords by enclosing source keywords with special symbols, and for preserving source constraints by replacing constraints with placeholders before translation and restoring them afterwards.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.8\linewidth]{figure/step1.pdf}
    \caption{Flow chart illustrating the constraint extraction and preprocessing pipeline in the first step of our benchmark construction.}
    \label{fig:step1}
\end{figure}

% \begin{enumerate}
%     \item If the task data contains no keywords and constraints that are necessary for evaluation, use Google Translate.
%     \item If the task data contains extractable keywords or constraints, use Google Translate along with extraction tools.
%     \item If the task data contains keywords or constraints that are not easily extractable like program variable names, attempt to prompt GPT-4o for translation.
% \end{enumerate}


Taking an example of rule-based instruction following task as an example, as shown in Table~\ref{tab:construction-case}, it requires extra processing to extract constraints from the translated instruction, as they are needed for verification.
Inspired by~\citet{yuan-etal-2020-enhancing}, we enclose the keywords in the original instruction with special symbols, making them easy to extract from the translated result.
% In constraints-contained translation, we explore various groups of special symbols, and improve the recall of constraints~(the detailed recall results are displayed in Appendix~\ref{sec:append_if_keywords}.) through multiple rounds of extraction.
As shown in Table~\ref{tab:stat_symbol_translation}, we explore various groups of special symbols and different orders and calculate the recall rates of keywords.
Comparing to not using special symbols, apply any symbol group can greatly improve the recalls, while combining different symbol groups in multiple rounds can further improve the recalls.
We choose \textit{Order 1} as it can achieve better results with fewer groups than \textit{Order 2}.
The detailed recall results are in Appendix~\ref{sec:append_if_keywords}.


\begin{table}[t]
    \centering
    \caption{The recall rates using different groups of special symbols}
    \label{tab:stat_symbol_translation}
    \vskip 0.15in
    \scriptsize
    \begin{tabular}{cc|cccc}
        \toprule
         \multicolumn{2}{c|}{\multirow{2}{*}{\textbf{Setting}}} & \multicolumn{4}{c}{\textbf{Target Language}}   \\
         & & zh & es & fr & hu \\
         \midrule
         \multicolumn{2}{c|}{w/o special symbols} & 0.68 & 0.68 & 0.68 & 0.68 
         \\
         \multicolumn{2}{l|}{symbol 1: $<$b$>$ $<$/b$>$} & 0.91 & 0.89 & 0.88 & 0.93 \\
         \multicolumn{2}{l|}{symbol 2: (\texttt{ })} & 0.88 & 0.91 & 0.89 & 0.92 \\
         \multicolumn{2}{l|}{symbol 3: ([\texttt{ }])} & 0.82 & 0.89 & 0.87 & 0.92 \\
        \midrule
         \multirow{2}{*}{Order 1} & + symbol 1 & 0.91 & 0.89 & 0.88 & 0.93 \\
         & + symbol 2 & 0.93 & 0.93 & 0.90 & 0.95 \\
         \midrule
        \multirow{3}{*}{Order 2} & + symbol 2 & 0.88 & 0.91 & 0.89 & 0.92 \\
        & + symbol 1 & 0.90 & 0.93 & 0.90 & 0.95 \\
        & + symbol 3 & 0.92 & 0.93 & 0.90 & 0.95 \\
         \bottomrule
    \end{tabular}
    % \vskip -0.1in
\end{table}

% Furthermore, we adjust the verification rules to better adapt to multiple languages, and refine the the number-word constraint for non-English languages based on the English to their word count ratio in the Flores-200 corpus, ensuring fair comparison across languages.



% During Step~2, each native speaker annotator needs to create a new translation using the English input text and the result from Step~1. 
% Therefore, the basic requirement for annotators is proficiency in both English and their native language. Additionally, since GPQA is a knowledge-intensive task, we require annotators to have at least a Bachelor's degree and to consult relevant materials to ensure the accuracy of proper noun translations.
% \vskip 0.1in
\paragraph{Step 2: Post-editing each sample by three distinct native-speaking annotators in all tasks.}
To ensure high-quality dataset, we implement a multi-round annotation and verification process. 
1) Each sample is given to three native-speaking annotators who are proficient in English and their native language. Considering the specialized nature of datasets like Science reasoning, annotators are required to hold at least a Bachelor's degree. 
2) Two automatic verifiers - rule-based verifiers and model-based verifiers - are used to assess the quality of human annotation. Rule-based verifiers are used to ensure the satisfaction of constraints for certain tasks, such as the rule-based instruction following task.
For model-based verifiers, we utilize the GEMBA-SQM prompt~\cite{kocmi-federmann-2023-large} and employ Qwen2.5-72B-Instruct, a powerful multilingual model, estimating the quality of translations. 
Along with providing an overall score, the model offers detailed explanations of translation errors as feedback to annotators. 
Samples failing the rule-based verifier or scoring below a predefined threshold are identified as failed samples and refined in subsequent iterations. 
Each manually annotated dataset undergoes at least three iterations.



% \vskip 0.1in
\paragraph{Step 3: Selecting the final translation version by LLMs.}

Initially, we ask a fourth annotator, uninvolved in the annotation process, to choose the final version from the results revised by three individuals. Intriguingly, the selection by the fourth annotator exhibited a strong position bias, often favoring the initial annotation. This preference could be attributed to the uniformly high quality of the annotations, resulting in minimal discernible differences among them.

% In preliminary studies, we find that even human annotators have position bias, prefering the first translation when the three translations are similar.
Debiasing for human annotators is costly in terms of both time and finance, because three translations encompass all permutations of six.
Consequently, we employ GPT-4o-mini to select the final translation, as it is a powerful and balanced LLM across different languages.
In particular, following~\citet{li2024crowdsourced}, we adapt the LLM-Judge system instruction~(see Appendix~\ref{sec:prompts}) to suit pairwise translation evaluation.
% We conduct two battles to determine the winner among the three candidates.
We firstly shuffle the positions of the three translations and conduct two battles to select a final version.
In each battle between two translations, we perform two judgments by swapping their positions and determine one winner.
The winner of the first two translations then battles against the third translation, determining the final winner.

