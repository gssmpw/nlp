\section{Introduction}
% 多语言能力不够强
% 首当其冲就是因为英语能力未能在不同语言间共享

Large language models~\cite{openai2024gpt4,team2024gemini,DeepSeekAI2024DeepSeekV3TR} have displayed remarkable proficiency across a wide range of tasks, mainly because they excel in instruction following, reasoning, long context understanding, code generation, and so on~\cite{ouyang2022training,cobbe2021training,su2024roformer,roziere2023code,lu2024llamax,sun2024survey}. Inherently, these capabilities are language-agnostic. 
Consider a simple task like the acquisition of mathematical concept: the numerical outcome remains consistent regardless of whether one learns the arithmetic expression $1+1=2$ in English or Chinese.  
% Taking the acquisition of mathematical concepts as an example, whether learning arithmetic expressions in English or Chinese, the numerical meaning is the same.
Similarly, when it comes to coding tasks, the choice between English or Chinese for articulating these instructions does not alter the fundamental logic of the code. 
However, numerous empirical studies have shown that LLMs' multilingual performance is quite unbalanced across different languages when handling same tasks~\cite{shi2023language, zhu2024multilingual, qi2023cross}.

\begin{figure}[t]
    % \vskip 0.2in
    \centering
    \includegraphics[width=0.8\linewidth]{figure/tasks.pdf}
    \caption{\name evaluates diverse advanced capabilities of LLMs in multilingual context.}
    \label{fig:capability}
    \vskip -0.2in
\end{figure}

However, current benchmarks~\cite{hendrycks2021measuring,lai2023okapi,singh2024global,wang2024seaeval} do not support comprehensive testing of the language-agnostic abilities of LLMs, particularly in low-resource language settings, for several reasons. Tasks like XWinograd~\cite{xwinograd-muennighoff-etal-2023-crosslingual} and XStoryCloze~\cite{xstorycloze-lin-etal-2022-shot}, based on multiple-choice formats, do not fully evaluate the generative capacities of LLMs. 
Additionally, the limited language overlap across existing benchmarks poses challenges in assessing LLM performance in specific languages. 
Recently, P-MMEval~\cite{zhang2024pmmeval} is proposed as a multilingual multitask benchmark, with the majority of its tasks still following a multiple-choice format. 
While it includes assessments like MGSM~\cite{shi2023language} and MIFEVAL that cover partial language-agnostic capabilities, LLMs exhibit remarkable performance, as shown in Table~\ref{tab:benchmark_comp}. 
This narrow focus leaves a significant gap between research evaluation and real-world applications.

To tackle this problem, we develop a comprehensive, multi-way, and challenging multilingual evaluation suite, called \name, to help the community better analyze and improve the language-agnostic capabilities of LLMs. 
Covering 17 languages\footnote{The 17 languages include English, Spanish, French, German, Russian, Bengali, Japanese, Thai, Swahili, Chinese, Telugu, Arabic, Korean, Serbian, Czech, Hungarian, and Vietnamese.}, \name not only includes a broader range of language families but also emphasizes the diversity of writing systems across languages (Table~\ref{tab:benchmark_comp}). 
As demonstrated in Table~\ref{tab:benchmark_comp}, \name increases the percentage of studied languages that utilize the non-Latin script.

\begin{table}[t]
    \centering
    \caption{\name provides a more comprehensive analysis of LLM language-agnostic capabilities by covering a broader range of capability scenarios, language families, and script systems. \# LG and \# LG-Family denote the number of supported languages and the language families they belong to, respectively. $R_{\mathrm{non-Latin}}$ refers to the proportion of languages that do not use the Latin script among supported languages. $*$ The results are from P-MMEval. }
    \label{tab:benchmark_comp}
    \vskip 0.1in
    \footnotesize
    \resizebox{0.95\linewidth}{!}{
    \begin{tabular}{l|cc|ccc}
    \toprule
         \textbf{Tasks} & \makecell{\textbf{Llama3.1}\\ \textbf{70B}} & \makecell{\textbf{Qwen2.5}\\ \textbf{72B}} & \makecell{\textbf{\# LG} \\ } & \makecell{\textbf{\# LG-Family} \\ \textbf{Diversity}} & \makecell{\textbf{$R_{\mathrm{non-Latin}}$}}\\
    \midrule
        XWinograd & 69.7 & 83.7 & 6 & 3 & 50.0 \\
        XStoryCloze & 70.3 & 83.6 & 13 & 11 & 38.5 \\
        MGSM$^*$  & 88.3 & 79.2 & 10 & 7 & 50.0 \\
        MIFEVAL$^*$ & 91.0 & 87.6 & 10 &  7 & 50.0 \\
        \midrule
        \multicolumn{6}{l}{\name} \\
        \midrule
        - Instruction Following & 11.1 & 34.1 & \multirow{4}{*}{17} & \multirow{4}{*}{11} & \multirow{4}{*}{58.8} \\
        - Code Generation & 29.8 & 45.5 \\
        - Science Reasoning & 35.8 & 39.4 \\
        - Tool Use & 44.3 & 61.8 \\
    \bottomrule
    \end{tabular}
    }
    % \vskip -0.25in
\end{table}


Meanwhile, \name highlights diverse language-agnostic advanced capabilities  (Figure~\ref{fig:capability}). 
We assess instruction following capability with rule-based~\cite{zhou2023instruction} and model-based~\cite{li2024crowdsourced} evaluations, code generation capability in diverse scenarios~(function-completion~\cite{liu2024your}~/~problem solving~\cite{jain2024livecodebench}), long context understanding capability~\cite{hsieh2024ruler}, a verity of reasoning in math~\cite{shi2023language} and science~\cite{Rein2023GPQAAG}, tool use~\cite{srinivasan2023nexusraven} in agent environments, and general~\cite{costa2022no}~/~domain translation. 
Domain translation, a by-product of data construction, poses a new challenge for LLM by necessitating fine-grained control and domain-specific terminology understanding over the translation process.


To ensure high quality, we devise an annotation framework to optimize the dataset quality with human effort and LLM feedback. 
The process involves translating data from English to selected non-English languages using machine translation systems, post-editing each sample by three native-speaking annotators across all tasks, and picking the final translation version using a superior LLM that involves swapping sample positions for debiasing~\cite{wang2024large,li2024crowdsourced}.

% Large language models (LLMs) have demonstrated remarkable proficiency across a wide range of tasks~\cite{openai2024gpt4,team2024gemini}.
% But numerous empirical studies have shown that LLMs' performance are quite unbalanced across different languages~\cite{shi2023language, zhu2024multilingual, qi2023cross}.

% One key factor is that their language-agnostic capabilities, such as reasoning, are difficult to be fully utilized across different languages~\cite{shi2023language,huang2023languages}.
% For instance, while LLMs can successfully solve queries presented in English, they often struggle with the same queries when posed in non-English languages.

% TODO: mention other multilingual evaluation suite
% Despite recent efforts to enhance LLMs' multilingual support~\cite{dubey2024llama,team2024gemini,lu2024llamax}, evaluations remain largely limited to mathematical reasoning (\textsc{mGSM})~\cite{shi2023language}.
% This overlooks other crucial aspects like instruction-following~\cite{zhou2023instruction} long-context modeling~\cite{hsieh2024ruler}, tool use~\cite{srinivasan2023nexusraven}, etc. 
% This narrow focus leaves a significant gap between research evaluation and real-world applications.
% Recently, P-MMEval~\cite{zhang2024pmmeval} has been proposed as a multilingual multitask benchmark.
% Despite the extension of several tasks for the evaluation of capabilities such as instruction following and code generation, other crucial capabilities are absent from the work, including long-context modeling and tool use.
% Meanwhile, the included languages do not encompass some low-resource languages, such as Swahili and Telugu.

% Through the comparison of multilingual performance across a range of tasks in \name, researchers can obtain a more profound understanding of the language-agnostic capabilities in the multilingual context.

% The absence of a comprehensive and challenging multilingual evaluation suite narrows the scope of current multilingual research and leaves a significant gap between empirical evaluation and real-world deployment.
% However, the absence of a comprehensive and challenging multilingual evaluation benchmark constrained multilingual evaluation to a single test scenario, i.e., mathematical reasoning (\textsc{mGSM})~\cite{shi2023language} (Table 1), while overlooking many other essential capabilities, such as multilingual instruction-following~\cite{zhou2023instruction} and multilingual long-context modeling~\cite{hsieh2024ruler}. 
% This limitation narrows the scope of multilingual research and creates a significant gap between empirical evaluation and real-world deployment.
% To enable LLM support in multilingual scenarios, researchers propose a variety of training strategies, including pre-training approaches~\cite{he2024scaling, alves2024tower} and post-training methods~\cite{zhu2024power, she2024mapo}.
% Recent trends also show that newly released LLMs start to support an expanding number of languages. 
% For example, while LLaMA3.1 claims support for 36 languages~\cite{dubey2024llama}, Qwen2.5 reportedly supports 30 languages~\cite{qwen2.5}


% 能力的提升离不开benchmark的指导
% 本文主要关注在多语言evaluation，希望为多语言研究提供新的guidance
% 我们的benchmark做了什么
% In this work, we develop a comprehensive, parallel, and challenging multilingual evaluation suite, called \name, to help the community better analyze the multilingual capabilities of LLMs.
% % that covers a broad spectrum of crucial capabilities (),
% Our evaluation suite encompasses diverse user-focused advanced capabilities (Figure~\ref{fig:capability}), selects challenging and widely-used English test sets and translates them into sixteen typologically diverse languages through meticulous translation and human post-editing.
% Moreover, through our manual construction process, we compile the parallel human translations into a hard multilingual translation dataset.
% Unlike tranditional translation dataset~\cite{goyal2022flores}, this test set challenges LLM with domain-specific translation and fine-grained controlled translation tasks, presenting novel challenges for machine translation research.


% across a broader range of scenarios.
% Specifically, we consider a wide spectrum of attractive capabilities (Figure~\ref{fig:tasks}) and build a multilingual evaluation suite: .

% 有价值的分析结果
% Evaluating current multilingual large language models on \name provides a more profound understanding of these language-agnostic capabilities.
% Experimental results indicate that even advanced LLMs struggle to fully leverage those language-agnostic capabilities in the multilingual contexts.
% They exhibit unbalanced multilingual performance across various tasks, particularly in low-resource languages.
% While model scaling consistently enhances multilingual performance, it fails to universally bridge the performance disparity between English and non-English languages.

Leading multilingual LLMs are evaluated on \name, revealing that language significantly influences language-agnostic capabilities of existing LLMs. 
Interestingly, simply increasing the parameters can boost average performance on these tasks but does not universally reduce the performance gap across languages. 
Moreover, compared to general translation, domain translation not only poses new challenges for LLMs but also requires new evaluation metrics. 
The main contributions can be summarized as follows:
\begin{itemize} [nosep,itemsep=1pt,leftmargin=0.3cm]
    \item We develop a comprehensive, multi-way multilingual benchmark across \textbf{17} languages for evaluating \textbf{6} crucial capabilities of LLMs on \textbf{10} diverse tasks.
    \item We propose a pipeline for curating high-quality mutlilingual datasets incorporating both human annotation and LLM-as-a-judge.
    \item Leading multilingual LLMs are evaluted on \name, and the related analyses provide a further understanding of the language-agnostic capabilities.
\end{itemize}