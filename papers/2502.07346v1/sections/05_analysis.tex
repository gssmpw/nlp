% \begin{table*}[ht]
%     \caption{F1 scores of correct answers in other languages compared to English are reported for five seleted tasks. We use the correctness of English answers as labels and treat the correctness of answers in other languages as predictions, to investigate the correctness agreement between English and other languages. Results show high agreement between English and other languages on these tasks.}
%     \label{tab:consistency}
%     \vskip 0.15in
%     \centering
%     \footnotesize
%     \setlength{\tabcolsep}{3pt}
%     \begin{tabular}{c|c|ccccccccccccccccc}
%     \toprule
%         Model & Task & cs & de & es & fr & ru & sr & bn & zh & ja & ko & ar & th & hu & vi & sw & te & AVG \\
%     \midrule
%         \multirow{5}{*}{\rotatebox{90}{Llama3.1-70B}} & Math & 0.89 & 0.89 & 0.93 & 0.84 & 0.90 & 0.89 & 0.84 & 0.89 & 0.85 & 0.88 & 0.89 & 0.88 & 0.89 & 0.92 & 0.89 & 0.71 & 0.87 \\
%         & Science & 0.62 & 0.62 & 0.61 & 0.61 & 0.60 & 0.53 & 0.50 & 0.62 & 0.57 & 0.57 & 0.60 & 0.54 & 0.55 & 0.61 & 0.45 & 0.40 & 0.56 \\
%         & Func Compl. & 0.91 & 0.91 & 0.92 & 0.93 & 0.94 & 0.91 & 0.88 & 0.93 & 0.92 & 0.91 & 0.93 & 0.92 & 0.93 & 0.93 & 0.87 & 0.76 & 0.91 \\
%         & Prob. Coding & 0.86 & 0.87 & 0.88 & 0.87 & 0.88 & 0.87 & 0.82 & 0.86 & 0.86 & 0.85 & 0.85 & 0.84 & 0.84 & 0.87 & 0.83 & 0.81 & 0.85 \\
%         & Multi Func. & 0.96 & 0.94 & 0.94 & 0.94 & 0.94 & 0.95 & 0.90 & 0.95 & 0.93 & 0.94 & 0.94 & 0.94 & 0.90 & 0.95 & 0.93 & 0.91 & 0.94 \\
%     \midrule
%         \multirow{5}{*}{\rotatebox{90}{Qwen2.5-72B}} & Math & 0.91 & 0.91 & 0.94 & 0.90 & 0.90 & 0.93 & 0.89 & 0.91 & 0.88 & 0.90 & 0.94 & 0.89 & 0.90 & 0.94 & 0.78 & 0.68 & 0.89 \\
%         & Science & 0.60 & 0.65 & 0.62 & 0.60 & 0.63 & 0.57 & 0.57 & 0.64 & 0.60 & 0.63 & 0.54 & 0.61 & 0.63 & 0.62 & 0.44 & 0.46 & 0.59 \\
%         & Func Compl. & 0.95 & 0.94 & 0.94 & 0.96 & 0.93 & 0.94 & 0.92 & 0.93 & 0.93 & 0.92 & 0.92 & 0.92 & 0.90 & 0.94 & 0.86 & 0.83 & 0.92 \\
%         & Prob. Coding & 0.90 & 0.91 & 0.92 & 0.92 & 0.91 & 0.90 & 0.89 & 0.92 & 0.90 & 0.89 & 0.90 & 0.90 & 0.90 & 0.91 & 0.84 & 0.86 & 0.90 \\
%         & Multi Func. & 0.92 & 0.93 & 0.94 & 0.94 & 0.94 & 0.94 & 0.88 & 0.94 & 0.92 & 0.94 & 0.92 & 0.93 & 0.93 & 0.94 & 0.77 & 0.87 & 0.91 \\
%     \bottomrule
%     \end{tabular}
% \end{table*}


\section{Analysis}
\subsection{High correctness agreement between English and other languages}
Although sometimes similar performance can be achieved across different languages for certain tasks, the specific problems being addressed may vary significantly.
To examine this language alignment, we compute F1 scores measuring the agreement between problem-solving correctness in English versus other languages.
Using English correctness as the reference labels, Figure~\ref{fig:agreement} shows the F1 scores of Llama3.1-70B and DeepSeek-V3 on six subtasks of \name.
Both these strong multilingual models demonstrate high correctness agreement on most tasks, with average F1 scores exceeding 0.9.
F1 scores for low-resource languages are notably lower than those for high-resource languages.
This pattern is particularly pronounced in science reasoning tasks, suggesting these knowledge-intensive problems pose unique challenges for cross-lingual knowledge transfer.
% To examine the extent of correctness agreement, we compute F1 scores measuring the agreement between the correctness of each problem in English and its corresponding correctness in another language.
% The correctness in English is treated as labels.
% Table~\ref{fig:agreement} demonstrates the F1 scores of Llama3.1-70B and Qwen2.5-72B on five subtasks of \name between English and other languages.
% Both the two strong multilingual models have a high level of correctness agreement on most tasks, with average F1 scores exceeding 0.9.
% As anticipated, F1 scores on the low-resource languages are comparatively lower those on high-resource languages.
% The relative low agreement on the science reasoning knowledge-intensive task may indicate that knowledge is difficult to share between languages.

% We find that consistency is lower for low-resource languages than for high-resource ones, which is similar to the pattern of average performance.
% Furthermore, the consistency observed in the last three tasks is significantly higher compared to the first two tasks.
% We hypothesize that demanding more language-specific capabilities leads to lower consistency, where models are required to generate native CoT in the reasoning tasks.
% In contrast, the subsequent three tasks primarily require models to generate outputs in programming languages or English function calls, which do not challange the multilingual generation capabilities.


\subsection{Challenges in evaluating domain-specific translation quality}
% \subsection{Comparison of domain translation metrics}
Domain-specific texts often contain substantial content that does not require translation, which leads to inflated spBLEU scores.
% A substantial proportion of domain-specific text does not require translation, leading to extremely high spBLEU.
To address this limitation, we explore alternative evaluation approaches: the edit-distance metric TER~\cite{snover2006study}, the model-based metric \textsc{xCOMET}-XXL~\cite{guerreiro2024xcomet}, and a novel performance retention rate that compares downstream task performance between model self-translation and human translations.
% We also consider other machine translation metrics, including an edit-distance metric, TER~\cite{snover2006study}, and a model-based metric \textsc{xCOMET}-XXL~\cite{guerreiro2024xcomet}.
% Another potential approach is to utilize the translations as downstream task data and then evaluate a model on this data.
% The retention rate of performance on model tanslations compared to that on human translations can be used as a metric for translation quality.
Table~\ref{tab:domain_metrics} presents these metric scores across selected tasks and languages. % , revealing several evaluation challenges.
Traditional metrics prove unreliable for domain-specific translation evaluation.
Both spBLEU and TER show extreme values in Science and Programming tasks due to large portions of unchanged text, failing to capture the quality of crucial translated segments.
The model-based metric \textsc{xCOMET} shows high variance across different scenarios, particularly struggling with low-resource languages and specialized domains.
Moreover, the performance retention rate doesn't work as expected as many values are greater than 1, which fails to evaluate translations subtly.
These findings highlight the need for specialized metrics for domain-specific translation evaluation, which we identify as an important direction for future research.

% Scores of different translation metrics on selected tasks and languages are demonstrated in Table~\ref{tab:domain_metrics}.
% There remains a problem for these metrics to fairly evaluate domain translations.
% Specifically, spBLEU has extremely high values on Science and Prob. Coding task.
% TER also shows extremely low values on these tasks.
% Due to the large proportion of unchanged text, these two metrics may not reflect the true quality of the crucial parts.
% The model-based metric \textsc{xCOMET}, shows divergent scores across a large span.
% Low-resource languages and specific domains pose challanges for the metric to work effectively.
% This is because translations may not have critical errors and LLMs are robust to disfluency of instructions.
% We leave the domain translation metric problem as future work.

% When evaluating the domain translations, we observe extremely high spBLEU scores in several domains, including the two code generation tasks and the science reasoning task.
% This can be attributed to the substantial proportion of text in the domain specific data that does not require translation.
% For instance, many programming statements and variable names are language-agnostic and should remain unchanged across translations.
% As shown in Table~\ref{tab:translation_downstream}, the performance of Qwen2.5 drops on the self-translated MGSM, indicating the poor translation quality.
% However, the performance on GPQA in Swahili is surprisingly improved, although many translations are terrible.
% After checking the results, we find that models have a high probability of guessing the right answer on this multiple choice dataset.
% This contingency and instability can affect our evaluation of the quality of translations.
% \begin{table}[h]
%     \centering
%     \small
%     \begin{tabular}{cccccc}
%         \toprule
%         & & \multicolumn{2}{c}{MGSM} & \multicolumn{2}{c}{GPQA} \\
%         & & sw & te & sw & te \\
%         \midrule
%         \multirow{2}{*}{Qwen2.5-7B} & Human & 14.0 & 27.2 & 18.8 & 20.8\\
%         & Trans & 10.8 & 8.4 & 21.2 & 16.52 \\
%         \multirow{2}{*}{Qwen2.5-72B} & Human & 57.6 & 50.4 & 30.0 & 26.1 \\
%         & Trans & 48.4 & 46.4 & 31.3 & 23.2 \\
%         \bottomrule
%     \end{tabular}
%     \caption{The performance of models on the tasks composed of original data and self-translated data, respectively.}
%     \label{tab:translation_downstream}
% \end{table}

\subsection{Machine-translated task data leads to inconsistent assessment of LLM capabilities}
Evaluating models on machine-translated data can lead to both overestimation and underestimation of model performance.
This inconsistency is evidenced by the retention rates exceeding 1.0 in Table~\ref{tab:domain_metrics}, suggesting occasional overestimation.
However, underestimation is more prevalent~(Figure~\ref{fig:difference}).
Both Llama3.1-70B and Qwen2.5-72B achieve significantly higher scores on human-translated rule-based instruction following tasks across several languages, including French, Arabic, and Hungarian, with performance gaps exceeding 4 points in some cases.
\begin{figure}[t]
    % \vskip 0.2in
    \centering
    \includegraphics[width=0.8\linewidth]{figure/difference.pdf}
    \caption{The performance differences between using human-translated data and machine-translated data. The upper figure depicts the performance difference of Llama3.1-70B on the rule-based instruction-following task, while the lower figure presents the performance difference for Qwen2.5-72B on the same task.}
    \label{fig:difference}
    \vskip -0.2in
\end{figure}
% \footnote{We use the paired t-test with $p$-value $< 0.05$.}
% \subsection{Machine-translated task data is not a reliable indicator of the real capabilities of LLMs}
% Evaluating models on machine-translated data can occasionally overestimate the capabilites, analogous to the retention rates greater than 1 observed in Table~\ref{tab:domain_metrics}.
% But more often, machine-translated data underestimates performance of models.
% For instance, as illustrated in Figure~\ref{fig:difference}, both LLama3.1-70B and Qwen2.5-72B show significantly\footnote{We use the paired t-test with $p$-value $< 0.05$.} better performance on human-translated rule-based instruction following task across several languages such as and French, Arabic and Hungarian, with notable differences exceeding 4 points for some languages.


% For instance, as illustrated in Figure~\ref{fig:difference}, human-translated data provides a more accurate measure of LLMs' capabilities compared to machine-translated data.
% Similarly, Qwen2.5-72B also benefits significantly from human translations, with substantial gains in languages like French and Hungarian.
% However, machine-translated data occasionally outperforms human translations in a few languages, albeit by samller margins.
% Note that in some cases, human-translated data does not consistently outperform machine-translated data. This can be attributed to the high quality of machine translations and the robustness of LLMs to noise.

\subsection{Conventional understanding tasks are inadequate to evaluate multilingual capdbilities of LLMs}
Prior work like Aya-Expanse~\cite{dang2024aya} relies on conventional understanding tasks such as XCOPA and XWinograd for multilingual evaluation.
On these metrics, Gemma2-9B achieves the best performance, followed by Aya-Expanse-8B, Llama3.1-8B, and Qwen2.5-7B.
However, our evaluation through \name reveals a different pattern: Qwen2.5-7B demonstrates superior multilingual capabilities on generation tasks, while Aya-Expanse models show notably weaker performance on code generation tasks, as shown in Table~\ref{tab:aya_task}.
This discrepancy highlights the importance of comprehensive evaluation frameworks that incorporate both understanding and generation tasks to accurately assess the multilingual capabilities of LLMs.

% Aya-Expanse models~\cite{dang2024aya} are evaluated on conventional understanding tasks like XCOPA and XWinograd.
% Their results show that Gemma2-9B achieve the best performance, followed by Aya-Expanse-8B, Llama3.1-8B, and Qwen2.5-7B.
% However, this is not often the case in \name.
% Qwen2.5-7B exhibits better multilingual capabilites on generation tasks among the four models.
% Aya-Expanse models demonstrate inferior performance on the code generation tasks.