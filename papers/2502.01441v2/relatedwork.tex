\section{Related Works}
\label{related}
% \subsection{Diffusion Model and Fast Sampling Technique}
% Diffusion models \citep{song2020score, song2019generative, ho2020denoising} have recently been raised as the most powerful generative model and outperform GAN \citep{goodfellow2014generative} in many applications. Diffusion models can generate high-fidelity images and possess good mode coverage, allowing diverse samples compared to GAN. However, diffusion models require many function evaluations (NFEs) during inference time. This drawback hinders its application in the real world. Many works are trying to tackle this drawback and achieve promising results. They can be divided into two main research categories: training from scratch and building upon pretrained diffusion models. Following the first category, there are several works, such as DDGAN \citep{xiao2021tackling}, LDM \citep{rombach2021highresolution}, and VQDiff \citep{gu2022vector}. DDGAN \citep{xiao2021tackling} proposes to use a larger step size in the forward process to reduce the NFEs; they use GAN models to implicitly learn the backward transition. Even though DDGAN \citep{xiao2021tackling} requires only a few sampling timesteps, it still suffers from low recall due to mode collapse from GAN. LDM \citep{rombach2021highresolution} does not directly reduce the number of sampling time steps; instead, it compresses the image to latent with a much smaller resolution. By training on latent space, LDM \citep{rombach2021highresolution} can significantly reduce both time and memory budget, and the inference is much faster than other pixel diffusion models. LDM \citep{rombach2021highresolution} has become the core technique in many large-scale diffusion models. Most real-world applications rely on LDM since it allows to scale diffusion models up on enormous high-resolution training datasets, which is impractical if training pixel diffusion models. In the second category, several works such as \citep{lu2022dpm, zhang2022fast} propose the high-order solver during inference. These works could successfully reduce sample NFEs to 10 without any training. However, they cannot sample with little NFEs such as 1 or 2. The other line of work is a distillation-based method. Progressive distillation \citep{salimans2022progressive} proposes to progressively distill diffusion model; each stage distills to reduce the sampling NFEs by half. This technique is costly since it requires to train many stages. Later works such as Guided-Distill \citep{meng2023distillation}, Swiftbrush \citep{nguyen2024swiftbrush}, DMD \citep{yin2024one}, and UFOGEN \citep{xu2024ufogen} manage to distill diffusion into few-step generation without compromising generative quality. The major drawback of these techniques is that additional training is required.
% Furthermore, some techniques, such as Swiftbrush \citep{nguyen2024swiftbrush} and DMD \citep{yin2024one}, do not have few-step sampling. Other methods, such as UFOGEN \citep{xu2024ufogen} and ADD \citep{sauer2023adversarial}, require training GAN, which could lead to training instability and low mode coverage. A standout among these techniques is the consistency model. The consistency model \citep{song2023consistency} is defined based on probability flow ODE (PF-ODE), allowing single- and multi-step sampling. The consistency model could be achieved via training from scratch and distillation from the diffusion model.

% \subsection{Consistency Model}

Consistency model \citep{song2023consistency, song2023improved} proposes a new type of generative model based on PF-ODE, which allows 1, 2 or multi-step sampling. The consistency model could be obtained by either training from scratch using an unbiased score estimator or distilling from a pretrained diffusion model. Several works improve the training of the consistency model. ACT \citep{kong2023act}, CTM \citep{kim2023consistency} propose to use additional GAN along with consistency objective. While these methods could improve the performance of consistency training, they require an additional discriminator, which could need to tune the hyperparameters carefully. MCM \citep{heek2024multistep} introduces multistep consistency training, which is a combination of TRACT \citep{berthelot2023tract} and CM \citep{song2023consistency}. MCM increases the sampling budget to 2-8 steps to tradeoff with efficient training and high-quality image generation. ECM \citep{geng2024consistency} initializes the consistency model by pretrained diffusion model and fine-tuning it using the consistency training objective. ECM vastly achieves improved training times while maintaining good generation performance. However, ECM requires pretrained diffusion model, which must use the same architecture as the pretrained diffusion architecture. Although these works successfully improve the performance and efficiency of consistency training, they only investigate consistency training on pixel space. As in the diffusion model, where most applications are now based on latent space, scaling the consistency training \citep{song2023consistency, song2023improved} to text-to-image or higher resolution generation requires latent space training. Otherwise, with pretrained diffusion model, we could either finetune consistency training \citep{geng2024consistency} or distill from diffusion model \citep{song2023consistency, luo2023latent}. CM \citep{song2023consistency} is the first work proposing consistency distillation (CD) on pixel space. LCM \citep{luo2023latent} later applies consistency technique on latent space and can generate high-quality images within a few steps. However, LCM's generated images using 1-2 steps are still blurry \citep{luo2023latent}. Recent works, such as Hyper-SD \cite{ren2024hyper} and TCD \cite{zheng2024trajectory}, have introduced notable improvements to latent consistency distillation. TCD \cite{zheng2024trajectory} employed CTM \cite{kim2023consistency} instead of CD \cite{song2023consistency}, significantly enhancing the performance of the distilled student model. Building on this, Hyper-SD \cite{ren2024hyper} divided the Probability Flow ODE (PF-ODE) into multiple components inspired by Multistep Consistency Models (MCM) \cite{heek2024multistep}, and applied TCD \cite{zheng2024trajectory} to each segment. Subsequently, Hyper-SD \cite{ren2024hyper} merged these segments progressively into a final model, integrating human feedback learning and score distillation \cite{yin2024one} to optimize one-step generation performance.