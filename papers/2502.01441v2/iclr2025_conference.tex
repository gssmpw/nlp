
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{cleveref}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{adjustbox} % To adjust table sizes
\usepackage{float}

% \iclrfinalcopy

% For table
\usepackage{multirow}

% For figures
\usepackage{graphicx}
% \usepackage[table]{xcolor}
\title{Improved Training Technique for Latent Consistency Models}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.
\iclrfinalcopy

\author{Quan Dao$^{*\dagger}$\\
Rutgers University \\
\texttt{quan.dao@rutgers.edu} \\ \And 
Khanh Doan$^{*}$\\
Movian AI, Vietnam \\
\texttt{dnkhanh.k63.bk@gmail.com} \\ \And
Di Liu\\
Rutgers University \\
\texttt{di.liu@rutgers.edu} \\   \And
Trung Le\\
Monash University \\
\texttt{trunglm@monash.edu} \\   \And
Dimitris Metaxas\\
Rutgers University \\
\texttt{dnm@cs.rutgers.edu} \\
}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\khanh}[1]{\textcolor{orange}{[Khanh: #1]}}
\newcommand{\quan}[1]{\textcolor{red}{[Quan: #1]}}
\newcommand{\diliu}[1]{\textcolor{purple}{[Di Liu: #1]}}
\newcommand{\trung}[1]{\textcolor{cyan}{[Trung: #1]}}
\newcommand{\metaxas}[1]{\textcolor{blue}{[Metaxas: #1]}}
\newcommand{\minisection}[1]{\noindent{\textbf{#1}}}


%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle
\def\thefootnote{\textsuperscript{$*$}}\footnotetext{Equal contributions.}
\def\thefootnote{\textsuperscript{$\dagger$}}\footnotetext{Project Lead \& Corresponding Author.}

\begin{abstract}
Consistency models are a new family of generative models capable of producing high-quality samples in either a single step or multiple steps. Recently, consistency models have demonstrated impressive performance, achieving results on par with diffusion models in the pixel space. However, the success of scaling consistency training to large-scale datasets, particularly for text-to-image and video generation tasks, is determined by performance in the latent space. In this work, we analyze the statistical differences between pixel and latent spaces, discovering that latent data often contains highly impulsive outliers, which significantly degrade the performance of iCT in the latent space. To address this, we replace Pseudo-Huber losses with Cauchy losses, effectively mitigating the impact of outliers. Additionally, we introduce a diffusion loss at early timesteps and employ optimal transport (OT) coupling to further enhance performance. Lastly, we introduce the adaptive scaling-$c$ scheduler to manage the robust training process and adopt Non-scaling LayerNorm in the architecture to better capture the statistics of the features and reduce outlier impact. With these strategies, we successfully train latent consistency models capable of high-quality sampling with one or two steps, significantly narrowing the performance gap between latent consistency and diffusion models. The implementation is released here: \url{https://github.com/quandao10/sLCT/}
\end{abstract}

\section{Introduction}
In recent years, generative models have gained significant prominence, with models like ChatGPT excelling in language generation and Stable Diffusion \citep{rombach2021highresolution}. In computer vision, the diffusion model \citep{song2020score, song2019generative, ho2020denoising, sohl2015deep} has quickly popularized and dominated the Adversarial Generative Model (GAN) \citep{goodfellow2014generative}. It is capable of generating high-quality diverse images that beat SoTA GAN models \citep{dhariwal2021diffusion}. Additionally, diffusion models are easier to train, as they avoid the common pitfalls of training instability and the need for meticulous hyperparameter tuning associated with GANs. The application of diffusion spans the entire computer vision field, including text-to-image generation \citep{rombach2021highresolution, gu2022vector}, image editing \citep{meng2021sdedit, cyclediffusion, huberman2024edit, han2024proxedit, he2024dice}, text-to-3D generation \citep{poole2022dreamfusion, wang2024prolificdreamer}, personalization \citep{ruiz2022dreambooth, van2023anti, kumari2023multi} and control generation \citep{zhang2023adding, brooks2022instructpix2pix, zhangli2024layout}. Despite their powerful capabilities, they require thousands of function evaluations for sampling, which is computationally expensive and hinders their application in the real world. Numerous efforts have been made to address this sampling challenge, either by proposing new training frameworks \citep{xiao2021tackling, rombach2021highresolution} or through distillation techniques \citep{meng2023distillation, yin2024one, sauer2023adversarial, dao2024self}. However, methods like \citep{xiao2021tackling} suffer from low recall due to the inherent challenges of GAN training, while \citep{rombach2021highresolution} still requires multi-step sampling. Distillation-based approaches, on the other hand, rely heavily on pretrained diffusion models and demand additional training.

Recently, \citep{song2023consistency} introduced a new family of generative models called the consistency model. Compared to the diffusion model \citep{song2019generative, song2020score, ho2020denoising}, the consistency model could both generate high-quality samples in a single step and multi-steps. The consistency model could be obtained by either consistency distillation (CD) or consistency training (CT). In previous work \citep{song2023consistency}, CD significantly outperforms CT. However, the CD requires additional training budget for using pretrained diffusion, and its generation quality is inherently limited by the pretrained diffusion. Subsequent research \citep{song2023improved} improves the consistency training procedure, resulting in performance that not only surpasses consistency distillation but also approaches SoTA performance of diffusion models. Additionally, several works \citep{kim2023consistency, geng2024consistency} have further enhanced the efficiency and performance of CT, achieving significant results. However, all of these efforts have focused exclusively on pixel space, where data is perfectly bounded. In contrast, most large-scale applications of diffusion models, such as text-to-image or video generation, operate in latent space \citep{rombach2021highresolution, gu2022vector}, as training on pixel space for large-scale datasets is impractical. Therefore, to scale consistency models for large datasets, the consistency must perform effectively in latent space. This work addresses the key question: How well can consistency models perform in latent space? To explore this, we first directly applied the SoTA pixel consistency training method, iCT \citep{song2023improved}, to latent space. The preliminary results were extremely poor, as illustrated in \cref{fig:qualitative_ict}, motivating a deeper investigation into the underlying causes of this suboptimal performance. We aim to improve CT in latent space, narrowing the gap between the performance of latent consistency and diffusion.

We first conducted a statistical analysis of both latent and pixel spaces. Our analysis revealed that the latent space contains impulsive outliers, which, while accounting for a very small proportion, exhibit extremely high values akin to salt-and-pepper noise. We also drew a parallel between Deep Q-Networks (DQN) and the Consistency Model, as both employ temporal difference (TD) loss. This could lead to training instability compared to the Kullback-Leibler (KL) loss used in diffusion models. Even in bounded pixel space, the TD loss still contains impulsive outliers, which \citep{song2023improved} addressed by proposing the use of Pseudo-Huber loss to reduce training instability. As shown in \cref{fig:impulsive_noise}, the latent input contains extremely high impulsive outliers, leading to very large TD values. Consequently, the Pseudo-Huber loss fails to sufficiently mitigate these outliers, resulting in poor performance as demonstrated in \cref{fig:qualitative_ict}. To overcome this challenge, we adopt Cauchy loss, which heavily penalizes extremely impulsive outliers. Additionally, we introduce diffusion loss at early timesteps along with optimal transport (OT) matching, both of which significantly enhance the model's performance. Finally, we propose an adaptive scaling $c$ schedule to effectively control the robustness of the model, and we incorporate Non-scaling LayerNorm into the architecture. With these techniques, we significantly boost the performance of latent consistency model compared to the baseline iCT framework and bridge the gap between the latent diffusion and consistency training.

\section{Related Works} \label{related}
% \subsection{Diffusion Model and Fast Sampling Technique}
% Diffusion models \citep{song2020score, song2019generative, ho2020denoising} have recently been raised as the most powerful generative model and outperform GAN \citep{goodfellow2014generative} in many applications. Diffusion models can generate high-fidelity images and possess good mode coverage, allowing diverse samples compared to GAN. However, diffusion models require many function evaluations (NFEs) during inference time. This drawback hinders its application in the real world. Many works are trying to tackle this drawback and achieve promising results. They can be divided into two main research categories: training from scratch and building upon pretrained diffusion models. Following the first category, there are several works, such as DDGAN \citep{xiao2021tackling}, LDM \citep{rombach2021highresolution}, and VQDiff \citep{gu2022vector}. DDGAN \citep{xiao2021tackling} proposes to use a larger step size in the forward process to reduce the NFEs; they use GAN models to implicitly learn the backward transition. Even though DDGAN \citep{xiao2021tackling} requires only a few sampling timesteps, it still suffers from low recall due to mode collapse from GAN. LDM \citep{rombach2021highresolution} does not directly reduce the number of sampling time steps; instead, it compresses the image to latent with a much smaller resolution. By training on latent space, LDM \citep{rombach2021highresolution} can significantly reduce both time and memory budget, and the inference is much faster than other pixel diffusion models. LDM \citep{rombach2021highresolution} has become the core technique in many large-scale diffusion models. Most real-world applications rely on LDM since it allows to scale diffusion models up on enormous high-resolution training datasets, which is impractical if training pixel diffusion models. In the second category, several works such as \citep{lu2022dpm, zhang2022fast} propose the high-order solver during inference. These works could successfully reduce sample NFEs to 10 without any training. However, they cannot sample with little NFEs such as 1 or 2. The other line of work is a distillation-based method. Progressive distillation \citep{salimans2022progressive} proposes to progressively distill diffusion model; each stage distills to reduce the sampling NFEs by half. This technique is costly since it requires to train many stages. Later works such as Guided-Distill \citep{meng2023distillation}, Swiftbrush \citep{nguyen2024swiftbrush}, DMD \citep{yin2024one}, and UFOGEN \citep{xu2024ufogen} manage to distill diffusion into few-step generation without compromising generative quality. The major drawback of these techniques is that additional training is required.
% Furthermore, some techniques, such as Swiftbrush \citep{nguyen2024swiftbrush} and DMD \citep{yin2024one}, do not have few-step sampling. Other methods, such as UFOGEN \citep{xu2024ufogen} and ADD \citep{sauer2023adversarial}, require training GAN, which could lead to training instability and low mode coverage. A standout among these techniques is the consistency model. The consistency model \citep{song2023consistency} is defined based on probability flow ODE (PF-ODE), allowing single- and multi-step sampling. The consistency model could be achieved via training from scratch and distillation from the diffusion model.

% \subsection{Consistency Model}

Consistency model \citep{song2023consistency, song2023improved} proposes a new type of generative model based on PF-ODE, which allows 1, 2 or multi-step sampling. The consistency model could be obtained by either training from scratch using an unbiased score estimator or distilling from a pretrained diffusion model. Several works improve the training of the consistency model. ACT \citep{kong2023act}, CTM \citep{kim2023consistency} propose to use additional GAN along with consistency objective. While these methods could improve the performance of consistency training, they require an additional discriminator, which could need to tune the hyperparameters carefully. MCM \citep{heek2024multistep} introduces multistep consistency training, which is a combination of TRACT \citep{berthelot2023tract} and CM \citep{song2023consistency}. MCM increases the sampling budget to 2-8 steps to tradeoff with efficient training and high-quality image generation. ECM \citep{geng2024consistency} initializes the consistency model by pretrained diffusion model and fine-tuning it using the consistency training objective. ECM vastly achieves improved training times while maintaining good generation performance. However, ECM requires pretrained diffusion model, which must use the same architecture as the pretrained diffusion architecture. Although these works successfully improve the performance and efficiency of consistency training, they only investigate consistency training on pixel space. As in the diffusion model, where most applications are now based on latent space, scaling the consistency training \citep{song2023consistency, song2023improved} to text-to-image or higher resolution generation requires latent space training. Otherwise, with pretrained diffusion model, we could either finetune consistency training \citep{geng2024consistency} or distill from diffusion model \citep{song2023consistency, luo2023latent}. CM \citep{song2023consistency} is the first work proposing consistency distillation (CD) on pixel space. LCM \citep{luo2023latent} later applies consistency technique on latent space and can generate high-quality images within a few steps. However, LCM's generated images using 1-2 steps are still blurry \citep{luo2023latent}. Recent works, such as Hyper-SD \cite{ren2024hyper} and TCD \cite{zheng2024trajectory}, have introduced notable improvements to latent consistency distillation. TCD \cite{zheng2024trajectory} employed CTM \cite{kim2023consistency} instead of CD \cite{song2023consistency}, significantly enhancing the performance of the distilled student model. Building on this, Hyper-SD \cite{ren2024hyper} divided the Probability Flow ODE (PF-ODE) into multiple components inspired by Multistep Consistency Models (MCM) \cite{heek2024multistep}, and applied TCD \cite{zheng2024trajectory} to each segment. Subsequently, Hyper-SD \cite{ren2024hyper} merged these segments progressively into a final model, integrating human feedback learning and score distillation \cite{yin2024one} to optimize one-step generation performance.

\section{Preliminaries} \label{sec:bg}
Denote $\pdata(\rvx_0)$ as the data distribution, the forward diffusion process gradually adds Gaussian noise with monotonically increasing standard deviation $\sigma(t)$ for $t \in \{0,1,\dots,T\}$ such that $p_t(\rvx_t|\rvx_0) = \gN(\rvx_0, \sigma^2(t)\mI)$ and $\sigma(t)$ is handcrafted such that $\sigma(0) = \sigma_{\min}$ and $\sigma(T)=\sigma_{\max}$. By setting $\sigma(t) = t$, the probability flow ODE (PF-ODE) from \citep{Karras2022edm} is defined as:
\begin{equation}
    \frac{\rd\rvx_t}{\rd t} = -t\nabla_{\rvx_t} \log p_t(\rvx_t) = \frac{\left( \rvx_t - \vf(\rvx_t, t) \right)}{t},  \label{eq:pf_ode}
\end{equation}
where $\vf:(\rvx_t, t) \rightarrow \rvx_0$ is the denoising function which directly predicts clean data $\rvx_0$ from given perturbed data $\rvx_t$. 
\citep{song2023consistency} defines consistency model based on PF-ODE in \cref{eq:pf_ode}, which builds a bijective mapping $\vf$ between noisy distribution $p(\rvx_t)$ and data distribution $\pdata(\rvx_0)$. The bijective mapping $\vf:(\rvx_t, t) \rightarrow \rvx_0$ is termed the consistency function. A consistency model $\vf_\theta(\rvx_t, t)$ is trained to approximate this consistency function $\vf(\rvx_t, t)$. The previous works \citep{song2023consistency, song2023improved, Karras2022edm} impose the boundary condition by parameterizing the consistency model as:
\begin{equation}
    \vf_\theta(\rvx_t, t) = c_{skip}(t)\rvx_t + c_{out}(t)\mF_\theta(\rvx_t, t), \label{eq:cm_param}
\end{equation}
where $\mF_\theta(\rvx_t, t)$ is a neural network to train. Note that, since $\sigma(t) = t$, we hereafter use $t$ and $\sigma$ interchangeably. $c_{skip}(t)$ and $c_{out}(t)$ are time-dependent functions such that $c_{skip}(\sigma_{\min}) = 1$ and $c_{out}(\sigma_{\max}) = 0$.

To train or distill consistency model, \citep{song2023consistency, song2023improved, Karras2022edm} firstly discretize the PF-ODE using a sequence of noise levels $\sigma_{\min} = t_{\min} = t_1 < t_2 < \dots < t_{N} = t_{\max} = \sigma_{\max}$, where $t_i = \left( t_{\min}^{1/\rho} + \frac{i-1}{N-1}(t_{\max}^{1/\rho
} - t_{\min}^{1/\rho})\right)^\rho$ and $\rho = 7$. 

\textbf{Consistency Distillation} Given the pretrained diffusion model $\vs_\phi(\rvx_t, t) \approx \nabla_{\rvx_t} \log p_t(\rvx_t)$, the consistency model could be distilled from the pretrained diffusion model using the following CD loss:
\begin{equation}
    \gL_{\text{CD}}(\theta, \theta^-) = \E\left[ \lambda(t_i)d(\vf_\theta(\rvx_{t_{i+1}}, t_{i+1}), \vf_{\theta^{-}}(\Tilde{\rvx}_{t_i}, t_{i})) \right], \label{loss:cd}
\end{equation}
where $\rvx_{t_{i+1}} = \rvx_0 + t_{i+1} \rvz$ with the $\rvx_0 \sim \pdata(\rvx_0)$ and $\rvz \sim \gN(0, \mI)$ and $\rvx_{t_i} = \rvx_{t_{i+1}} - (t_{i}-t_{i+1})t_{i+1} \nabla_{\rvx_{t_{i+1}}} \log p_{t_{i+1}}(\rvx_{t_{i+1}}) = \rvx_{t_{i+1}} - (t_{i}-t_{i+1})t_{i+1}\vs_\phi(\rvx_{t_{i+1}}, t_{i+1})$. 

\textbf{Consistency Training}
The consistency model is trained by minimizing the following CT loss:
\begin{equation}
    \gL_{\text{CT}}(\theta, \theta^-) = \E\left[ \lambda(t_i)d(\vf_\theta(\rvx_{t_{i+1}}, t_{i+1}), \vf_{\theta^{-}}(\rvx_{t_i}, t_{i})) \right], \label{loss:ct}
\end{equation}
where $\rvx_{t_i} = \rvx_0 + t_{i} \rvz$ and $\rvx_{t_{i+1}} = \rvx_0 + t_{i+1} \rvz$ with the same $\rvx_0 \sim \pdata(\rvx_0)$ and $\rvz \sim \gN(0, \mI)$

In \cref{loss:cd} and \cref{loss:ct}, $\vf_\theta$ and $\vf_{\theta^-}$ are referred to as the online network and the target network, respectively. The target's parameter $\theta^-$ is obtained by applying the Exponential Moving Average (EMA) to the student's parameter $\theta$ during the training and distillation as follows:
\begin{equation}
    \theta^- \leftarrow \text{stopgrad}(\mu\theta^- + (1-\mu)\theta), \label{ema}
\end{equation}
with $0\leq\mu<1$ as the EMA decay rate,  weighting function $\lambda(t_i)$ for each timestep $t_i$, and $d(\cdot, \cdot)$ is a predefined metric function. 

In CM \citep{song2023consistency}, the consistency training still lags behind the consistency distillation and diffusion models. iCT \citep{song2023improved} later propose several improvements that significantly boost the training performance and efficiency. First, the EMA decay rate $\mu$ is set to $0$ for better training convergence. Second, the Fourier scaling factor of noise embedding and the dropout rate are carefully examined. Third, iCT introduces Pseudo-Huber losses to replace $L_2$ and LPIPS since LPIPS introduces the undesirable bias in generative modeling \citep{song2023improved}. Furthermore, the Pseudo-Huber is more robust to outliers since it imposes a smaller penalty for larger errors than the $L_2$ metric. Fourth, iCT proposes an exp curriculum for total discretization steps N, which doubles N after a predefined number of training iterations. Moreover, uniform weighting $\lambda(t_i) = 1$ is replaced by $\lambda(t_i)=1/(t_{i+1}-t_i)$. Finally, iCT adopts a discrete Lognormal distribution for timestep sampling as EDM \citep{Karras2022edm}. With all these improvements, CT is now better than CD and performs on par with the diffusion models in pixel space.

\section{Method}
\label{method}
In this paper, we first investigate the underlying reason behind the performance discrepancy between latent and pixel space using the same training framework in \cref{sec:analysis}. Based on the analysis, we find out the root of unsatisfied performance on latent space could be attributed to two factors: the impulsive outlier and the unstable temporal difference (TD) for computing consistency loss. To deal with impulsive outliers of TD on pixel space, \citep{song2023improved} proposes the Pseudo-Huber function as training loss. For the latent space, the impulsive outlier is even more severe, making Pseudo-Huber loss not enough to resist the outlier. Therefore,  \cref{sec:cauchy} introduces Cauchy loss, which is more effective with extreme outliers. In the next \cref{sec:diff_loss} and \cref{sec:ot}, we propose to use diffusion loss at early timesteps and OT matching for regularizing the overkill effect of consistency at the early step and training variance reduction, respectively. Section \ref{sec:c} designs an adaptive scheduler of scaling $c$ to control the robustness of the proposed loss function more carefully, leading to better performance. Finally, in \cref{sec:norm}, we investigate the normalization layers of architecture and introduce Non-scaling LayerNorm to both capture feature statistic better and reduce the sensitivity to outliers.

\subsection{Analysis of latent space} \label{sec:analysis}

We first reimplement the iCT model \citep{song2023improved} on the latent dataset CelebA-HQ $32 \times 32 \times 4$ and pixel dataset Cifar-10 $32 \times 32 \times 3$. Hereafter, we refer to the latent iCT model as iLCT. We find that iCT framework works well on pixel datasets as claim \citep{song2023improved}. However, it produces worse results on latent datasets as in \cref{fig:qualitative_ict} and \cref{tab:main_exp}. The iLCT gets a very high FID above 30 for both datasets, and the generative images are not usable in the real world. This observation raises concern about the sensitivity of CT algorithm with training data, and we should carefully examine the training dataset. In addition, we notice that the DQN and CM use the same TD loss, which update the current state using the future state. Furthermore, they also possess the training instability. This motivates to carefully examine the behavior of TD loss with different training data.


While the pixel data lies within the range $[-1, 1]$ after being normalized, the range of latent data varies depending on the encoder model, which is blackbox and unbound. After normalizing latent data using mean and variance, we observe that the latent data contains high-magnitude values. We call them the impulsive outliers since they account for small probability but are usually very large values. In the bottom left of \cref{fig:impulsive_noise}, the impulsive outlier of latent data is red, spanning from $-9$ to $7$, while the first and third quartiles are just around $-1.4$ and $1.4$, respectively. We evaluate how the iCT will be affected by data outliers by analyzing the temporal difference $\text{TD} = f_\theta(\rvx_{t_{i+1}}, t_{i+1})-f_{\theta^-}(\rvx_{t_i}, t_{i})$. In the top right of \cref{fig:impulsive_noise}, the impulsive outliers of pixel TD range from -1.5 to 1.7, which are not too far from the interquartile range compared to latent TD. The impulsive outliers of latent TD range is much wider from -3.2 to 5. iCT uses Pseudo-Huber loss instead of $L_2$ loss since the Huber is less sensitive to outliers, see \cref{fig:loss}. However, for latent data, the Huber's reduction in sensitivity to outliers is not enough. This indicates that even using Pseudo-Huber loss, the iLCT training on latent space could still be unstable and lead to worse performance, which matches our experiment results on iLCT. Based on the above analysis, we hypothesize that the TD value statistic highly depends on the training data statistic.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/impulsive_noise.pdf}
    \caption{\textbf{Box and Whisker Plot:} Impulsive noise comparison between pixel and latent spaces. The right column shows the statistics of TD values at 21 discretization steps. Other discretization steps exhibit same behavior, where impulsive outliers are consistently present regardless of the total discretization steps. The blue boxes represent interquartile ranges of the data, while the green and orange dashed lines indicate inner and outer fences, respectively. Outliers are marked with red dots.}
    \label{fig:impulsive_noise}
\end{figure}

%To understand the root of TD's impulsive outlier, we look into Deep Q Learning (DQN) from Reinforcement Learning (RL). There is a strong correlation between DQN and CM. While the DQN uses Q-value of future state as the ground truth for Q-value of the current state, the CM updates the current timesteps $f(\rvx_{t_{i+1}}, t_{i+1})$ using the smaller timesteps $f(\rvx_{t_{i}}, t_{i})$. This loss type is called the temporal difference in RL. For stable training, DQN uses target network $\theta^-$ to estimate Q-value of future state, and CM similarly adopts the same technique for consistency loss. The target network $\theta^-$ could be updated differently, such as Polyak-averaging, periodic, and standard TD updates \citep{lee2019target}. The Polyak-averaging update is simply EMA update using in \citep{song2023consistency}, and standard update corresponds to \citep{song2023improved} which $\theta^- \leftarrow \theta$ every iteration. The periodic update is a standard update but after every fixed number of iterations. In RL, Polyak-averaging and periodic updates are more stable but slowly convergent \citep{lee2019target}. Even using these stable target updates, there is still instability in TD training. Since the target network needs to change along with the online model, the target value of TD can never be fixed, which makes the loss highly oscillate. Therefore, even though the pixel data is very well-bounded within [-1, 1], the CM training is still affected by impulsive outliers due to the nature of TD loss.

To mitigate the impact of impulsive outliers, we could use more stable target updates like Polyak or periodic in TD loss \cite{lee2019target}, but they lead to very slow convergence, as shown in \citep{song2023consistency}. Even though CM is initialized by a pretrained diffusion model, the Polyak update still takes a long time to converge. Therefore, using Polyak or periodic updates is computationally expensive, and we keep the standard target update as in \citep{song2023improved}. Another direction is using a special metric for latent like LPIPS on pixel space \citep{song2023consistency}. \citep{kang2024diffusion2gan} proposes the E-LatentLPIPS as a metric for distillation and performs well on distillation tasks. However, this requires training a network as a metric and using this metric during the training process will also increase the training budget. To avoid the overhead of the training, we seek a simple loss function like Pseudo-Huber but be more effective with outliers. We find that the Cauchy loss function \citep{black1996robust, barron2019general} could be a promising candidate in place of Pseudo-Huber for latent space.
\subsection{Cauchy Loss against Impulsive Outlier} \label{sec:cauchy}
In this section, we introduce the Cauchy loss \citep{black1996robust, barron2019general} function to deal with extreme impulsive outliers. The Cauchy loss function has the following form:
\begin{equation}
    d_{\text{Cauchy}}(\rvx, \rvy)=  \log \left(1+\frac{||\rvx-\rvy||_2^2}{2c^2}\right), \label{loss:cauchy}
\end{equation}
and we also consider two additional robust losses, which are Pseudo-Huber \citep{song2023improved, barron2019general} and Geman-McClure \citep{geman1986bayesian, barron2019general}
\begin{equation}
    d_{\text{Pseudo-Huber}}(\rvx, \rvy)= \sqrt{||\rvx-\rvy||_2^2 + c^2} - c, \label{loss:huber}
\end{equation}
\begin{equation}
    d_{\text{Geman-McClure}}(\rvx, \rvy)= \frac{2||\rvx-\rvy||_2^2}{||\rvx-\rvy||_2^2 + 4c^2}, \label{loss:gm}
\end{equation}
where $c$ is the scaling parameter to control how robust the loss is to the outlier. We analyze their robustness behavior against outliers. As shown in \cref{fig:loss_val}, the Pseudo-Huber loss linearly increases like $L_1$ loss for the large residuals $\rvx-\rvy$. In contrast, the Cauchy loss only grows logarithmically, and the Geman-McClure suppresses the loss value to $1$ for the outliers. 

The Pseudo-Huber loss works well if the residual value does not grow too high and, therefore, has a good performance on the pixel space. However, for the latent space, as shown in the bottom right of \cref{fig:impulsive_noise}, the TD suffers from extremely high values coming from the impulsive outlier in the latent dataset, the Cauchy loss could be more suitable since it significantly dampens the influence of extreme outliers. Otherwise, even Geman-McClure is very highly effective for removing outlier effects than two previous losses; it gives a gradient $0$ for high TD value and completely ignores the impulsive outliers as \cref{fig:loss_derivative}. This is unexpected behavior because even though we call the high-value latent impulsive outlier, they actually could encode important information from original data. Completely ignoring them could significantly hurt the performance of training model. Based on this analysis, we choose Cauchy loss as the default loss for latent CM for the rest of the paper. The loss ablation is provided in \cref{tab:ablate_robust}.


\begin{figure}[!ht]
    \centering
    \begin{subfigure}[t]{0.40\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/func.png}
        \caption{Robust Loss}
        \label{fig:loss_val}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.40 \textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/derivative.png}
        \caption{Derivative of Robust Loss}
        \label{fig:loss_derivative}
    \end{subfigure}
    \caption{Analysis of robust loss: Pseudo-Huber, Cauchy, and Geman-McClure}
    \label{fig:loss}
\end{figure}
% \vspace{-3mm}


\subsection{Diffusion Loss at small timestep} \label{sec:diff_loss}
For small noise level $\sigma$, the ground truth of $f(\rvx_\sigma, \sigma)$ can be well approximated by $\rvx_0$, but this does not hold for large noise levels. Therefore, for low-level noise, the consistency objective seems to be overkill and harms the model's performance since instead of optimizing $f_\theta(\rvx_\sigma, \sigma)$ to approximated ground truth $\rvx_0$, the consistency objective optimizes through a proxy estimator $f_{\theta^-}(\rvx_{<\sigma}, <\sigma)$ leading to error accumulation over timestep. To regularize this overkill, we propose to apply an additional diffusion loss on small noise level as follows:

\begin{equation}
    L_{diff} = ||f_\theta(\rvx_{t_i}, t_i) - \rvx_0||^2_2 \quad \forall i \leq \text{int(N $\cdot$ r)}, \label{loss:diff}
\end{equation}

where N is the number of training discretization steps and $r\in[0;1]$ is the diffusion threshold, and we heuristicly choose $r=0.25$. We do not apply diffusion loss for large noise levels since $f(\rvx_\sigma, \sigma)$ will differ greatly from the target $\rvx_0$, leading to very high $L_2$ diffusion loss. This could harm the training consistency process, misleading to the wrong solution. We provide the ablation study in \cref{tab:diff_loss}. Furthermore, CTM \citep{kim2023consistency} also proposes to use diffusion loss, but they use them on both high and low-level noise, which is different from us. 

\subsection{OT matching reduces the variance} \label{sec:ot}
% \vspace{-5mm}
In this section, we adopt the OT matching technique from previous works \citep{pooladian2023multisample, lee2023minimizing}. \citep{pooladian2023multisample} proposes to use OT to match noise and data in the training batch, such as the moving $L_2$ cost is optimal. On the other hand, \citep{lee2023minimizing} introduces $\beta\text{VAE}$ for creating noise corresponding to data and train flow matching on the defined data-noise pairs. By reassigning noise-data pairs, these works significantly reduce the variance during the diffusion/flow matching training process, leading to a faster and more stable training process. According to \citep{zhang2023emergence}, the consistency training and diffusion models produce highly similar images given the same noise input. Therefore, the final output solution of the consistency and diffusion models should be close to each other. Since OT matching helps reduce the variance during training diffusion, it could be useful to reduce the variance of consistency training. In our implementation, we follow \citep{pooladian2023multisample, tong2023improving} using the POT library to map from noise to data in the training batch. The overhead caused by minibatch OT is relatively small, only around $0.93\%$ training time, but gains significant performance improvement as shown in \cref{tab:strategy}.

\subsection{Adaptive $c$ scheduler} \label{sec:c}

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=0.5\linewidth]{figures/C_by_NFE.pdf}
%     \caption{Our robust adaptive $c$ scheduler}
%     \label{fig:proposed_c}
% \end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/C_merge.pdf}
    \caption{Model convergence plot on different $c$ schedule. (Left) Our proposed $c$ values. Performance on FID (Middle) and Recall (Right) of our proposed $c$ in comparison with different choices.}
    \label{fig:fid_vary_c}
\end{figure}
% \vspace{-5mm}

In this section, we examine the choice of scaling parameter $c$ in robust loss functions. The scaling parameter controls the robustness level, which is very important for model performance. The previous work \citep{song2023improved} proposes to use fixed constant $c_0 = 0.00054\sqrt{d}$, where $d$ is the dimension of data. We find that using this simple fixed $c$ is not yet optimal for the training consistency model. Especially in this paper, we follow the Exp curriculum specified by \cref{exp_cur} in \citep{song2023improved}, which doubles the total discretization step after a defined number of training iterations. 
\begin{equation}
    \text{NFE}(k)=\min \left(s_0 2^{\left\lfloor\frac{k}{K^{\prime}}\right\rfloor}, s_1\right)+1, \quad K^{\prime}=\left\lfloor\frac{K}{\log _2\left\lfloor s_1 / s_0\right\rfloor+1}\right\rfloor, \label{exp_cur}
\end{equation}
where $k$ is current training iteration, $K$ is total training iteration and $s_0 = 10, s_1=640$. During training, we notice that the variance of TD is significantly reduced as doubling total discretization steps using \cref{exp_cur}. Since the more discretization steps, the closer distance of $\rvx_{t_i}$ and $\rvx_{t_{i+1}}$, the TD value's range between them should be smaller. However, the impulsive outlier still exists regardless of the number of discretization steps. Intuitively, we propose a heuristic adaptive $c$ scheduler where the $c$ is scaled down proportional to the reduction rate of TD variance as the number of discretization steps increases. We plot our $c$ scheduler versus discretization steps in \cref{fig:fid_vary_c} and we fit the $c$ scheduler to get the scheduler equation as following:

\begin{equation}
    c = \exp(-1.18 * \log(\text{NFE}(k) - 1) - 0.72) \label{eq:c_scheduler}
\end{equation}

\subsection{Non-scaling Layernorm} \label{sec:norm}
As mentioned in \cref{sec:analysis}, the statistic of training data could play an important role in the success of consistency training. Furthermore, in architecture design, the normalization layer specifically handles the statistics of input, output, and hidden features. In this section, we investigate the normalization layer choice for consistency training, which is sensitive to training data statistics. 

Currently, both \citep{song2023improved, song2023consistency} use the UNet architecture from \citep{dhariwal2021diffusion}. In UNet \citep{dhariwal2021diffusion}, GroupNorm is used in every layer by default. The GroupNorm only captures the statistics over groups of local channels, while the LayerNorm further captures the statistics' overall features. Therefore, LayerNorm is better at capturing fine-grained statistics over the entire feature. We further carry out the experiments for other types of normalization, such as LayerNorm, InstanceNorm, RMSNorm in \cref{tab:norm_layer} and observe that the GroupNorm and InstanceNorm perform relatively well compared to others, especially LayerNorm. This could be due to that they are less sensitive to the outliers since they only capture the statistic over groups of channels. Therefore, the impulsive features only affect the normalization of a group containing them. For the LayerNorm, the impulsive features could negatively impact the overall features's normalization. We further look into the LayerNorm implementation and suspect that the scaling term could significantly amplify the outliers across features by serving as a shared parameter. This observation is also mentioned in \citep{wei2022outlier} for LLM quantization. In implementation, we set the \textbf{scaling term of LayerNorm to $1$} and \textbf{disabled the gradient update} for it \eqref{operation:layernorm}. We refer to it as Non-scaling LayerNorm (NsLN) as \citep{wei2022outlier}.

\begin{equation}
    \text{LN}_{\gamma, \beta}(\rvx) = \frac{\rvx - u(\rvx)}{\sqrt{\sigma^{2}(\rvx) + \epsilon}} \cdot \gamma + \beta, \quad
    \text{NsLN}_{\beta}(\rvx) = \frac{\rvx - u(\rvx)}{\sqrt{\sigma^{2}(\rvx) + \epsilon}} + \beta, \label{operation:layernorm}
\end{equation}

where $u(\rvx)$ and $\sigma^{2}(\rvx)$ are mean and variance of $\rvx$.

% \subsection{Improve Consistency Distillation}

% \vspace{-15mm}
\section{Experiment} \label{exp}

\subsection{Performance of our training technique} \label{exp:main}
% \vspace{-3mm}
\begin{table}[t]
    \centering
    \begin{tabular}{cc}
        \begin{minipage}[c]{0.58\textwidth}
            \centering
            \begin{subtable}[t]{\textwidth}
                \resizebox{\textwidth}{!}{%
                \begin{tabular}{l c c c c c}
                    \toprule
                    Model & NFE$\downarrow$ & FID$\downarrow$ & Recall$\uparrow$ & Epochs & Total Bs\\
                    \midrule 
                    \multicolumn{5}{c}{\textbf{Pixel Diffusion Model}}\\
                    \midrule
                    WaveDiff \citep{phung2023wavediff} & 2 & 5.94 & 0.37 & 500 & 64\\
                    Score SDE \citep{song2020score} & 4000 & 7.23 & - & ~6.2K & - \\
                    DDGAN \citep{xiao2021tackling} & 2 & 7.64 & 0.36 & 800 & 32 \\
                    RDUOT \citep{dao2024high} & 2 & 5.60 & 0.38 & 600 & 24 \\
                    RDM \citep{teng2023relay} & 270 & 3.15 & 0.55 & 4K & - \\
                    UNCSN++ \citep{kim2021soft} & 2000 & 7.16 & - & - & -\\
                    \midrule 
                    \multicolumn{5}{c}{\textbf{Latent Diffusion Model}}\\
                    \midrule
                    LFM-8 \citep{dao2023flow} & 85 & 5.82 & 0.41 & 500 & 112\\ 
                    LDM-4 \citep{rombach2021highresolution} & 200 & 5.11 & 0.49 & 600 &48 \\
                    LSGM \citep{vahdat2021score} & 23 & 7.22 & - & 1K &-\\
                    DDMI \citep{park2024ddmi} & 1000 & 7.25 & - & - &-\\
                    
                    DIMSUM \citep{phung2024dimsum} & 73 & 3.76  & 0.56 & 395 &32\\
                    $\text{LDM-8}^\dagger$ & 250 & {8.85}  & - & 1.4K &128\\
                    
                    \midrule
                    \multicolumn{5}{c}{\textbf{Latent Consistency Model}}\\
                    \midrule
                    iLCT \citep{song2023improved} & 1 & 37.15 & 0.12 & 1.4K &128\\
                    iLCT \citep{song2023improved} & 2 & 16.84 & 0.24 & 1.4K &128\\
                    Ours  & 1 & 7.27 & 0.50 & 1.4K &128\\
                    Ours  & 2 & 6.93 & 0.52 & 1.4K &128\\
                    \bottomrule
                \end{tabular}%
                }
            \caption{CelebA-HQ}
            \label{tab:celeb}
            \end{subtable}
        \end{minipage}
        \hfill
        \begin{minipage}[c]{0.42\textwidth}
            \centering
            \begin{subtable}[t]{\textwidth}
                \resizebox{\textwidth}{!}{%
                \begin{tabular}{l c c c c c}
                    \toprule
                    Model & NFE$\downarrow$ & FID$\downarrow$ & Recall$\uparrow$ & Epochs & Total Bs\\
                    \midrule 
                    \multicolumn{5}{c}{\textbf{Pixel Diffusion Model}}\\
                    \midrule
                    WaveDiff \citep{phung2023wavediff} & 2 & 5.94 & 0.37 & 500 & 64\\
                    Score SDE \citep{song2020score} & 4000 & 7.23 & - &6.2K & -\\
                    DDGAN \citep{xiao2021tackling} & 2 & 5.25 & 0.36 & 500 & 32\\
                    \midrule 
                    \multicolumn{5}{c}{\textbf{Latent Diffusion Model}}\\
                    \midrule
                    LFM-8 \citep{dao2023flow} & 90 & 7.70 & 0.39 & 90 &112\\
                    LDM-8 \citep{rombach2021highresolution} & 400 & 4.02 & 0.52 & 400 &96\\
                    $\text{LDM-8}^\dagger$ & 250 & {10.81} & - & 1.8K &256\\
                    \midrule
                    \multicolumn{5}{c}{\textbf{Latent Consistency Model}}\\
                    \midrule
                    iLCT \citep{song2023improved} & 1 &52.45  &0.11  & 1.8K &256\\
                    iLCT \citep{song2023improved} & 2 &24.67  &0.17  & 1.8K &256\\
                    Ours  & 1 &8.87  &0.47  & 1.8K &256\\
                    Ours  & 2 &7.71  &0.48  & 1.8K &256\\
                    \bottomrule
                \end{tabular}%
                }
            \caption{LSUN Church}
            \label{tab:lsun}
            \end{subtable}
            \hfill
            \begin{subtable}[t]{\textwidth}
                \resizebox{\textwidth}{!}{%
                \begin{tabular}{l c c c c c}
                    \toprule
                    Model & NFE$\downarrow$ & FID$\downarrow$ & Recall$\uparrow$ & Epochs &Total Bs \\
                    \midrule 
                    \multicolumn{5}{c}{\textbf{Latent Diffusion Model}}\\
                    \midrule
                    LFM-8 \citep{dao2023flow} & 84 & 8.07 & 0.40 & 700 &128\\
                    LDM-4 \citep{rombach2021highresolution} & 200 & 4.98 & 0.50 & 400 &42\\
                    $\text{LDM-8}^\dagger$ & 250 &{10.23} & - & 1.4K &128\\
                    \midrule
                    \multicolumn{5}{c}{\textbf{Latent Consistency Model}}\\
                    \midrule
                    iLCT \citep{song2023improved} & 1 & 48.82  & 0.15 & 1.4K &128 \\
                    iLCT \citep{song2023improved} & 2 & 21.15 & 0.19 & 1.4K &128\\
                    Ours  & 1 & 8.72  &0.42 & 1.4K &128\\
                    Ours  & 2 & 8.29  &0.43  & 1.4K &128\\
                    \bottomrule
                \end{tabular}%
                }
            \caption{FFHQ}
            \label{tab:ffhq}
            \end{subtable}
        \end{minipage}
    \end{tabular}
    \caption{Our performance on CelebA-HQ, LSUN Church, FFHQ datasets at resolution $256 \times 256$. ($\dagger$) means training on our machine with the same diffusion forward and equivalent architecture.}
    \label{tab:main_exp}
\end{table}


\minisection{Experiment Setting:}
We measure the performance of our proposed technique on three datasets: CelebA-HQ \citep{celeba}, FFHQ \citep{karras2019style}, and LSUN Church \citep{lsun}, at the same resolution of $256 \times 256$. Following LDM \citep{rombach2021highresolution}, we use pretrained VAE KL-8 \footnote{https://huggingface.co/stabilityai/sd-vae-ft-ema} to obtain latent data with the dimensionality of $32 \times 32 \times 4$. We adopt the OpenAI UNet architecture \citep{dhariwal2021diffusion} as the default architecture throughout the paper. Furthermore, we use the variance exploding (VE) forward process for all the consistency and diffusion experiments following \citep{song2023consistency, song2023improved}. The baseline iCT is self-implemented based on official implementation CM \citep{song2023consistency} and iCT \citep{song2023improved}. We refer to this baseline as iLCT. Furthermore, we also train the latent diffusion model for each dataset using the same VE forward noise process for fair comparisons with our technique. This LDM model is referred to as $\text{LDM-8}^{\dagger}$ in \cref{tab:main_exp}. All three frameworks, including ours, iLCT, and $\text{LDM-8}^{\dagger}$, use the same architecture.

\minisection{Evaluation:} During the evaluation, we first generate 50K latent samples and then pass them through VAE's decoder to obtain the pixel images. We use two well-known metrics, FrÃ©chet Inception Distance (FID) \citep{fid} and Recall \citep{kynkaanniemi2019improved}, for measuring the performance of the model given the training data and 50K generated images. 

\minisection{Model Performance:} We report the performance of our model across all three datasets in \cref{tab:main_exp}, primarily to compare it with the baseline iLCT \citep{song2023improved} and LDM \citep{rombach2021highresolution}. For both 1 and 2 NFE sampling, we observe that the FIDs of iLCT for all datasets are notably high (over 30 for 1-NFE sampling and over 16 for 2-NFE sampling), consistent with the qualitative results shown in \cref{fig:qualitative_ict}, where the generated image is unrealistic and contain many artifacts. This poor performance of iLCT in latent space is expected, as the Pseudo-Huber training losses are insufficient in mitigating extreme impulsive outliers, as discussed in \cref{sec:analysis} and \cref{sec:cauchy}. In contrast, our proposed framework demonstrates significantly better FID and Recall than iLCT. Specifically, we achieve 1-NFE sampling FIDs of 7.27, 8.87, and 8.29 for CelebA-HQ, LSUN Church, and FFHQ, respectively. For 2-NFE sampling, our FID scores improve across all three datasets. Notably, our 1-NFE sampling outperforms $\text{LDM-8}^{\dagger}$, using the same noise scheduler and architecture. However, our models still exhibit higher FIDs compared to LDM \citep{rombach2021highresolution} and LFM \citep{dao2023flow}. In contrast, we only need 1 or 2 timestep sampling, whereas they require multiple timesteps for high-fidelity generation.
 It's important to note that we employ the VE forward process, whereas these other methods use VP and flow-matching forward processes. Furthermore, the qualitative results of our framework, as shown in \cref{fig:qualitative_1nfe}, highlight our ability to generate high-quality images.

\begin{figure}[ht]
\centering
    \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/lct_celeba.pdf}
    \caption{CelebA-HQ}
    \label{fig:qualitative_celeba}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/lct_church.pdf}
    \caption{LSUN Church}
    \label{fig:qualitative_lsun_church}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/ffhq.pdf}
    \caption{FFHQ}
    \label{fig:qualitative_ffhq}
    \end{subfigure}
    \caption{Our qualitative results using 1-NFE at resolution $256 \times 256$}
    \label{fig:qualitative_1nfe}
\end{figure}

\begin{figure}[ht]
\centering
    \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/lct_celeba_baseline.pdf}
    \caption{CelebA-HQ}
    \label{fig:qualitative_ict_celeba}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/lsun_church.pdf}
    \caption{LSUN Church}
    \label{fig:qualitative_ict_lsun_church}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/lct_ffhq_baseline.pdf}
    \caption{FFHQ}
    \label{fig:qualitative_ict_ffhq}
    \end{subfigure}
    \caption{iLCT qualitative results using 1-NFE at resolution  $256 \times 256$}
    \label{fig:qualitative_ict}
\end{figure}


\subsection{Ablation of proposed framework} \label{exp:ablation}

We ablate our proposed techniques on the CelebA-HQ $256\times256$ dataset, with all FID and Recall metrics measured using 1-NFE sampling. All models are trained for 1,400 epochs with the same hyperparameters. As shown in \cref{tab:strategy}, replacing Pseudo-Huber losses with Cauchy losses makes our model's training less sensitive to impulsive outliers, resulting in a significant FID reduction from $37.15$ to $13.02$. This demonstrates the effectiveness of Cauchy losses in handling extremely high-value outliers, as discussed in \cref{sec:cauchy}. Additionally, applying diffusion loss at small timesteps further reduces FID by approximately 4 points to $9.11$, as this loss term stabilizes the training process at small timesteps, as described in \cref{sec:diff_loss}. Introducing OT coupling during minibatch training reduces training variance, improving the FID to $8.89$. Notably, by replacing the fixed scaling term $c=c_0$, \citep{song2023improved} with an adaptive scaling schedule, our model achieves an additional FID reduction of more than 1 point, reaching $7.76$, highlighting the importance of the scaling term $c$ in robustness control. Finally, we propose using NsLN, which removes the scaling term from LayerNorm to handle outliers more effectively. NsLN captures feature statistics while mitigating the negative impact of outliers, resulting in our best FID of $7.27$.

\minisection{Robustness Loss} \label{exp:ablation:robust_loss}
To analyze the impact of different robust loss functions, we conduct an ablation study using our best settings but replace the Cauchy loss with alternatives such as L2, E-LatentLPIPS \cite{kang2024diffusion2gan}, the Huber and the Geman-McClure loss. The results, shown in \cref{tab:ablate_robust}, indicate that both Huber and Geman-McClure underperform compared to the Cauchy loss when applied in the latent space. This is because the Huber loss remains too sensitive to extremely impulsive outliers, while the Geman-McClure loss tends to ignore such outliers entirely, leading to a loss of important information. This behavior is also discussed in \cref{sec:cauchy}.

% \vspace{-2mm}
\begin{table}[h!]
    \centering
    \begin{tabular}{cc}
        \begin{minipage}[c]{0.40\textwidth}
            \centering
            
            \begin{subtable}[t]{\textwidth}
                \centering
                \begin{tabular}{lcc}
                    \toprule
                    Framework                      & FID $\downarrow$   & Recall $\uparrow$   \\
                    \midrule
                    iLCT                           & 37.15              & 0.12                \\
                    \midrule
                    Cauchy                         & 13.02              & 0.36                \\
                    + Diff                         & 9.11               & 0.41                \\
                    + OT                           & 8.89               & 0.42                \\
                    + Scaled $c$                   & 7.76               & 0.47                \\
                    + NsLN       & \textbf{7.27}               &\textbf{0.50}                \\
                    % \rowcolor{pink!60}+ NsLN       & 7.27               & 0.50                \\
                    \bottomrule
                \end{tabular}
                \caption{Components of proposed framework}
                \label{tab:strategy}
            \end{subtable}
            \hfill
            % \vspace{2mm}
            \begin{subtable}[t]{\textwidth}
                \centering
                \begin{tabular}{lcc}
                    \toprule
                    $r$           & FID $\downarrow$   & Recall $\uparrow$   \\
                    \midrule
                    1.0                    & 7.47               & 0.49                \\
                    0.6                      & 7.33               & 0.49                \\
                    % \rowcolor{pink!60}0.25   & 7.27               & 0.50                \\
                    0.25   & \textbf{7.27}               & \textbf{0.50}                \\
                    \bottomrule
                \end{tabular}
                \caption{Threshold using Diffusion loss}
                \label{tab:diff_loss}
            \end{subtable}
            
        \end{minipage}
        \hfill
        \begin{minipage}[c]{0.40\textwidth}
            \centering
            \begin{subtable}[t]{\textwidth}
                \centering
                \begin{tabular}{lcc}
                    \toprule
                    Loss                        & FID $\downarrow$   & Recall $\uparrow$   \\
                    \midrule
                     L2                          & 50.40              & 0.04                \\
                    E-LatentLPIPS               & 11.49              & 0.47                \\
                    \midrule
                    Huber                       & 9.97               & 0.44                \\
                    Geman McClure               & 11.28              & 0.44                \\
                    % \rowcolor{pink!60} Cauchy   & 7.27               & 0.50                \\
                    Cauchy   & \textbf{7.27}               & \textbf{0.50}                \\
                    \bottomrule
                \end{tabular}
                \caption{Robust losses.}
                \label{tab:ablate_robust}
            \end{subtable}
            \hfill
            % \vspace{2mm}
            \begin{subtable}[t]{\textwidth}
                \centering
                \begin{tabular}{lcc}
                    \toprule
                    Norm layer                             & FID $\downarrow$   & Recall $\uparrow$   \\
                    \midrule
                    $\text{GN}$                           & 7.76               & 0.47                \\
                    \midrule
                    % \midrule
                    IN                                      & 8.47               & 0.43                \\
                    % $\text{IN}^\dagger$                     & 8.03               & 0.46                \\
                    % \midrule
                    LN                                      & 9.05               & 0.46                \\
                    % $\text{LN}^\dagger$                     & 7.92               & 0.47                \\
                    % \midrule
                    RMS                                     & 8.96               & 0.46                \\
                    % $\text{RMS}^\dagger$                    & 7.62               & 0.47                \\
                    % \midrule
                    NsLN                 &\textbf{7.27}               &\textbf{0.50}                \\
                    % \rowcolor{pink!60} NsLN                 & 7.27               & 0.50                \\
                    % \rowcolor{white}$\text{NsLN}^\dagger$   & 7.64               & 0.47                \\
                    \bottomrule
                \end{tabular}
                \caption{Norm Layer}
                \label{tab:norm_layer}
            \end{subtable}
        \end{minipage}
    \end{tabular}
    \caption{Ablation Studies on CelebA-HQ $256\times256$ dataset at epoch 1400}
    \label{tab:ablation}
\end{table}

\minisection{Diffusion Threshold} \label{exp:ablation:diff_loss}
In this section, we explore the impact of varying the threshold for applying the diffusion loss function in combination with the consistency loss. We observe that using the diffusion loss at every timestep improves consistency training; however, it underperforms compared to applying the diffusion loss selectively at smaller timesteps such as $r=0.25$ as shown in \cref{tab:diff_loss}. This suggests that applying diffusion losses primarily at small noise levels improves performance as discussed \cref{sec:diff_loss}. At larger timesteps, the diffusion loss may conflict with the consistency loss, potentially guiding the model toward incorrect solutions, thereby reducing overall performance.



\minisection{Scaling term $c$ scheduler} \label{exp:ablation:vary_c}
In this section, we compare the performance of our adaptive scaling $c$ scheduler with the fixed scaling $c$ scheduler proposed in \citep{song2023improved}. Our model demonstrates better convergence with the proposed adaptive $c$ scheduler. The rationale behind this improvement lies in the fact that, as the discretization steps increases using the exponential curriculum, the value of the TD scales down. Despite the reduced TD value, impulsive outliers still persist. A fixed large scaling $c$ is not effective in handling these outliers. To address this, we scale $c$ down as discretization steps increases, which leads to better performance, as shown in \cref{fig:fid_vary_c}.


\minisection{Normalizing Layer} \label{exp:ablation:norm_layer}
We denote GN, IN, LN, RMS, and NsLN as GroupNorm, InstanceNorm, LayerNorm, RMSNorm, and Non-scaling LayerNorm, respectively. The baseline UNet architecture from \citep{dhariwal2021diffusion} uses GroupNorm by default. We replace the normalization layers in the baseline with each of these types and train the model on CelebA-HQ using the best settings. The results are reported in \cref{tab:norm_layer}. GN and IN only capture local statistics, making them more robust to outliers, as outliers in one region do not affect others. In contrast, LN captures statistics from all features, making it more vulnerable to outliers because an outlier affects all features through a shared scaling term. By removing the scaling term in LN, we obtain NsLN, which is both effective in capturing feature statistics and resistant to outliers. As shown in \cref{tab:norm_layer}, NsLN outperforms the second-best GN by 0.5 FID and significantly outperforms LN.
\section{Conclusion}
CT is highly sensitive to the statistical properties of the training data. In particular, when the data contains impulsive noise, such as latent data, CT becomes unstable, leading to poor performance. In this work, we propose using the Cauchy loss, which is more robust to outliers, along with several improved training strategies to enhance model performance. As a result, we can generate high-fidelity images from latent CT, effectively bridging the gap between latent diffusion models and consistency models. Future work could explore further improvements to the architecture, specifically by investigating normalization methods that reduce the impact of outliers. For example, removing the scaling term from group normalization or instance normalization may help mitigate outlier effects. Another promising future direction is the integration of this technique with Consistency Trajectory Models (CTM) \cite{kim2023consistency}, as CTM has demonstrated improved performance compared to traditional Consistency Models (CM) \cite{song2023consistency}.
\section*{Acknowledgements}
Research funded by research grants to Prof. Dimitris Metaxas from NSF: 2310966, 2235405, 2212301, 2003874, 1951890, AFOSR 23RT0630, and NIH 2R01HL127661.


\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\newpage
\appendix
\section{Appendix}
We provide additional uncurated samples of our models for three datasets: CelebaA-HQ (\ref{fig:appendix:celeba_onestep}, \ref{fig:appendix:celeba_twostep}), LSUN Church (\ref{fig:appendix:lsun_onestep}, \ref{fig:appendix:lsun_twostep}), and FFHQ (\ref{fig:appendix:ffhq_onestep}, \ref{fig:appendix:ffhq_twostep}). We also provide additional uncurated samples of our models on CelebaA-HQ trained with L2 loss (\ref{fig:appendix:celeba_onestep_ilct_l2}) and E-LatentLPIPS loss (\ref{fig:appendix:celeba_onestep_ilct_elatentlpips}).

\begin{figure}[h]
\centering
    \includegraphics[width=0.6\textwidth]{figures/lct_celeba_more.pdf}
    \caption{One-step samples on CelebA-HQ $256 \times 256$}
    \label{fig:appendix:celeba_onestep}
\end{figure}

\begin{figure}[h]
\centering
    \includegraphics[width=0.6\textwidth]{figures/lct_celeba_more_2step.pdf}
    \caption{Two-step samples on CelebA-HQ $256 \times 256$}
    \label{fig:appendix:celeba_twostep}
\end{figure}

\begin{figure}[h]
\centering
    \includegraphics[width=0.6\textwidth]{figures/lct_lsun_more.pdf}
    \caption{One-step samples on LSUN Church $256 \times 256$}
    \label{fig:appendix:lsun_onestep}
\end{figure}

\begin{figure}[h]
\centering
    \includegraphics[width=0.6\textwidth]{figures/lct_lsun_more_2step.pdf}
    \caption{Two-step samples on LSUN Church $256 \times 256$}
    \label{fig:appendix:lsun_twostep}
\end{figure}

\begin{figure}[h]
\centering
    \includegraphics[width=0.6\textwidth]{figures/lct_ffhq_more.pdf}
    \caption{One-step samples on FFHQ $256 \times 256$}
    \label{fig:appendix:ffhq_onestep}
\end{figure}

\begin{figure}[h]
\centering
    \includegraphics[width=0.6\textwidth]{figures/lct_ffhq_more_2step.pdf}
    \caption{Two-step samples on FFHQ $256 \times 256$}
    \label{fig:appendix:ffhq_twostep}
\end{figure}


\begin{figure}[h]
\centering
    \includegraphics[width=0.6\textwidth]{figures/ilct_l2_celeba.pdf}
    \caption{One-step samples on CelebA-HQ $256 \times 256$ (L2 loss)}
    \label{fig:appendix:celeba_onestep_ilct_l2}
\end{figure}

\begin{figure}[h]
\centering
    \includegraphics[width=0.6\textwidth]{figures/ilct_latentlpips_celeba.pdf}
    \caption{One-step samples on CelebA-HQ $256 \times 256$ (E-LatentLPIPS loss)}
    \label{fig:appendix:celeba_onestep_ilct_elatentlpips}
\end{figure}

\end{document}
