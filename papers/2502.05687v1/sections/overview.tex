\section{Results and Technical Overview} \label{sec:overview}
In a nutshell, our result is that we establish a directed LDD with \emph{near-optimal} loss $\Order(\log n \log\log n)$. This comes tantalizingly close to the unconditional lower bound of $\Omega(\log n)$. It resembles a similar milestone in the development of probabilistic tree embeddings~\cite{Bartal98}, and also the current state of the art for low-stretch spanning trees~\cite{AbrahamN19}. In fact, similar $\log\log n$ barriers show up also in different contexts for directed graphs~\cite{ChechikLRS20}. In a sense, all of these results with $\log\log n$ overhead (including ours) apply a careful recursive approach that can be traced back to work by Seymour~\cite{Seymour95} (though with strongly varying implementations depending on the setting). 

We state our result in three \cref{thm:main-existential,thm:main-det,thm:main-fast}, where the first theorem is purely existential, the latter two are algorithmic, and the last has \emph{near-linear} running time.

\subsection{Near-Optimal LDDs via Expander Decompositions} \label{sec:overview:sec:ldd-exp}
The main conceptual contribution of our paper is that Low-Diameter Decompositions are closely and in a very practical way related to Expander Decompositions. Based on this insight, we prove the following theorem.

\begin{restatable}{theorem}{thmMainExistential} \label{thm:main-existential}
	For every directed graph there exists a directed LDD with loss $O(\log n \log \log n)$.
\end{restatable}

For the remainder of \cref{sec:overview:sec:ldd-exp} we will elaborate on the proof of \cref{thm:main-existential}. It involves two separate steps: reducing the problem to \emph{cost-minimizing} using the \emph{Multiplicative Weight Update} method, and then showing that a so called \emph{lopsided} expander decomposition is the desired \emph{cost-minimizer}. We emphasize that in \cref{sec:overview:sec:ldd-exp} we only focus on Quest 1 from the introduction, which is minimizing the loss $L$ of an LDD.

\paragraph{Reduction to Unit Lengths.}
Let us assume throughout that we only deal with unit-length graphs (i.e., where $\ell(e) = 1$ for all edges). This assumption is in fact without loss of generality, as we can simply replace each edge $e$ of length $\ell(e)$ by a path of $\ell(e)$ unit-length edges. This transformation may blow up the graph, but as we focus on the existential result first this shall not concern us.\footnote{We are assuming throughout that all edge weights are bounded by $\poly(n)$, therefore this transformation leads to a graph on at most $n_0 \leq \poly(n)$ nodes. In particular, to achieve a loss of $L = \Order(\log n \log \log n)$ on the original graph it suffices to achieve a loss of $\Order(\log n_0 \log\log n_0)$ on the transformed graph.} Whenever the LDD cuts at least one of the path edges in the transformed graph, we imagine that the entire edge in the original graph is cut. This way, when we cut each edge with probability at most $\frac{L}{D}$, in the original graph we cut each edge with probability at most $\frac{\ell(e) \cdot L}{D}$ (by a union bound) as required.

\paragraph{Reduction to Cost-Minimization.}
Perhaps it appears surprising that we claim a connection between LDDs---which are inherently \emph{probabilistic} objects---and EDs---which rather have a \emph{deterministic} flavor. As a first step of bringing these notions together consider the following lemma based on the well-studied Multiplicative Weight Update \cite{AroraHazanKale12} method.
 

\begin{restatable}[Multiplicative Weight Update]{lemma}{mwu} \label{lem:mwu}
Let $G = (V, E)$ be a directed graph, and suppose that for all cost functions $c : E \to [|V|^{10}]$ there is a set of cut edges $S \subseteq E$ satisfying the following two properties:
\begin{itemize}
	\item For any two nodes $u, v \in V$ that are part of the same strongly connected component in $G \setminus S$, we have $d_G(u, v) \leq D$ and $d_G(v, u) \leq D$.
	\item $c(S) \leq c(E) \cdot \frac{L}{D}$.
\end{itemize}
Then there exists a directed LDD for $G$ with loss $\Order(L)$.
\end{restatable}

Intuitively, the lemma states that in order to construct an LDD that cuts each edge with small probability $L / D$, it suffices to instead find a cut which \emph{globally} minimizes the total cost of the cut edges while ensuring that every remaining strongly connected component has diameter at most~$D$. This however comes with the price of introducing a \emph{cost function $c$} to the graph, and the goal becomes to cut edges which collectively have at most an $L / D$ fraction of the total cost. For the reader's convenience, we include a quick proof sketch of \Cref{lem:mwu}.

\begin{proof}[Proof Sketch.]
	Assign a unit cost to every edge in the graph, and consider the following iterative process. In each iteration we find a set of cut edges $S_i$ with the desired guarantees, and adjust the costs of the edges multiplicatively by doubling the cost of any every edge $e \in S_i$. Repeat this process for $R = 100\ln |E| \cdot D/L$ iterations. We claim that the uniform distribution over the cut sets~$S_i$ encountered throughout is an LDD as required. 

	Clearly the diameter condition holds for each set $S_i$, but it remains to bound the edge-cutting probability. Each iteration doubles the cost of every cut edge, and hence the total cost increases by at most $c(S_i) \leq c(E) \cdot \frac{L}{D}$, i.e., by a factor $1 + \frac{L}{D}$. After all $R$ repetitions the total cost is at most $|E| \cdot (1+L/D)^R < |E| \cdot |E|^{100} = |E|^{101}$. It follows that each edge is cut in at most $101 \log |E|$ iterations as otherwise its cost alone would already be more than $|E|^{101}$. Thus, the probability of cutting any fixed edge in a randomly sampled cut set $S_i$ is at most $101 \log |E| / R = \Order(L / D)$.
\end{proof}

Henceforth, we refer to the process of finding a cut with the properties stated in \Cref{lem:mwu} as a \emph{cost-minimizer}, which we aim to construct in the following paragraphs. That is, we now also consider a cost function $c$, and our goal is to cut edges $S$ of total cost~\smash{$c(S) \leq c(E) \cdot \frac{L}{D}$} (without worrying about a per-edge guarantee), while ensuring that the strongly connected components in~\makebox{$G \setminus S$} have diameter $\leq D$. We remark that this cost-minimization framework is quite standard.

\paragraph{Directed Expander Decomposition.}
In a leap forward, let us rename the \emph{costs $c$} to \emph{capacities~$c$}. Our idea is simple: We want to use an Expander Decomposition to remove edges of small total capacity so that all strongly connected components in the graph become expanders and thus, in particular, have small diameter.

To make this more precise, we first introduce some (standard) notation. A node set $U \subseteq V$ naturally induces a \emph{cut} (between $U$ and $\overline U = V \setminus U$). We write $c(U, \overline U)$ for the total capacity of edges crossing the cut from $U$ to $\overline U$, and define the \emph{volume} of $U$ as
\begin{equation*}
	\vol(U) = c(U, V) = \sum_{e \in E \cap (U \times V)} c(e),
\end{equation*}
and also write $\minvol(U) = \min\set{\vol(U), \vol(\overline U)}$. The \emph{sparsity} of the cut $U$ is defined by
\begin{equation*}
	\phi(U) = \frac{c(U, \overline U)}{\minvol(U)}.
\end{equation*}
We say that $U$ is \emph{$\phi$-sparse} if $\phi(U) \leq \phi$, and we call a graph without $\phi$-sparse cuts a \emph{$\phi$-expander.} The standard Expander Decomposition can be stated as follows.

\begin{lemma}[Directed Expander Decomposition] \label{lem:exp-decomp}
Let $\phi > 0$. For any directed graph $G = (V, E, c)$ there is an edge set $S \subseteq E$ of total capacity $c(S) \leq c(E) \cdot \phi \log c(E)$ such that every strongly connected component in the remaining graph $G \setminus S$ is a $\phi$-expander.
\end{lemma}

Moreover, an important property of a $\phi$-expander decomposition is that the diameter of its strongly connected components depends on $\phi$ in the following manner. 

\begin{lemma} \label{lem:exp-diam}
Any $\phi$-expander has diameter $\Order(\phi^{-1} \log \vol(V))$.
\end{lemma}

To understand the intuition behind \Cref{lem:exp-diam}, imagine that we grow a ball (i.e., a breadth-first search tree) around some node. With each step, the expansion property (related to the absence of $\phi$-sparse cuts) guarantees that we increase the explored capacity by a factor of $(1 + \phi)$; thus after $\Order(\phi \log \vol(V))$ steps we have explored essentially the entire graph.

Already at this point we have made significant progress and can recover the $\Order(\log^2 n)$-loss LDD: Apply the Expander Decomposition in \cref{lem:exp-decomp} with parameter $\phi = \log \vol(V) / D$. This removes edges with total capacity at most a $\Order(\log^2 \vol(V) / D)$-fraction of the total capacity. In the remaining graph each strongly connected component is a $\phi$-expander and thus, by \cref{lem:exp-diam}, has diameter at most $\Order(\phi^{-1} \log\vol(V)) = \Order(D)$ (by choosing the constants appropriately, this bound can be adjusted to~$\leq D$). Finally, \cref{lem:mwu} turns this into an LDD with loss $\Order(\log^2 \vol(V)) = \Order(\log^2 n)$.

Unfortunately, both log-factors in the Expander Decomposition (fraction of the cut capacity and diameter) are tight. Nevertheless, with some innovation we manage to bypass these bounds and improve the $\Order(\log^2 n)$ loss. To this end we propose a refined notion of expanders called \emph{lopsided expanders.}

\paragraph{Lopsided Expander Decomposition.}

The notion of $\phi$-sparsity defined above is oblivious to the ratio between $\vol(V)$ and $\minvol(U)$. For example, two cuts $U$ and $W$ can have the same sparsity, even though $\vol(U) \gg \vol(\overline U)$ while $\vol(W) \approx \vol(\overline W)$. It turns out that this leaves some unused potential, and that we should incentivize cutting cuts with large volume on both sides compared to more lopsided cuts with large volume only on one side.

Formally, we define \emph{$\psi$-lopsided sparsity} of a cut $U$ as 
\begin{align*}
	\psi(U) = \frac{c(U, \overline U)}{\minvol(U) \cdot \log \frac{\vol(V)}{\minvol(U)}},
\end{align*}
where we include the ratio~\smash{$\frac{\vol(V)}{\minvol(U)}$} in the denominator. Since $\vol(V)>\minvol(U)$, a cut can only have smaller lopsided sparsity than regular sparsity, so a graph with no sparse cuts may still have lopsided sparse cuts. A $\psi$-lopsided expander is defined as a graph with no $\psi$-lopsided sparse cuts, and can be thought of as a subclass of expanders which in addition to having no sparse cuts also has no cuts that are both sufficiently lopsided and sufficiently sparse.

The \emph{lopsided expander decomposition} is otherwise identical to the standard expander decomposition defined previously, except that every strongly connected component is required to be a \emph{$\psi$-lopsided} expander instead of a $\phi$-expander. Our Lopsided Expander Decomposition has the same global ``total capacity cut'' guarantee as the standard expander decomposition, as stated in the following lemma coupled with a proof sketch.

\begin{restatable}[Lopsided Expander Decomposition]{lemma}{lemLexpDecomp} \label{lem:lexp-decomp}
	Let $\psi > 0$. For any directed graph $G = (V, E, c)$ there is an edge set $S \subseteq E$ of total capacity $c(S) \leq c(E) \cdot \psi \log c(E)$ such that every strongly connected component in the remaining graph $G \setminus S$ is a $\psi$-lopsided expander.
\end{restatable}

\begin{proof}[Proof Sketch.]
	Consider the following algorithm: If there are no $\psi$-lopsided sparse cuts, then the graph is already a $\psi$-lopsided expander and we can stop. Otherwise, we cut a $\psi$-lopsided sparse cut (add the cut edges to $S$), and recurse on the remaining strongly connected components. It is clear that this eventually produces a $\psi$-lopsided expander decomposition. In order to prove that the cut edges have capacity at most $c(E) \cdot \psi \log c(E)$, we use a potential argument. We assign to each edge~$e$ a potential of $c(e) \log c(E)$. Throughout the procedure we maintain the invariant that each edge holds a potential of at least $c(e) \log \vol(C)$, where $C$ is the strongly connected component containing edge $e$. When cutting a $\psi$-lopsided cut $(U, \overline U)$ in a component $C$, an edge $e$ on the smaller side by volume (say $U$) suddenly needs to hold a potential of $c(e)\log c(U)$ instead of $c(e)\log c(C)$. In particular, the amount of excess potential we have freed is
	\begin{equation*}
		\sum_{e \in E \cap (U \times V)} c(e) \cdot (\log \vol(C) - \log \vol(U)) = \sum_{e \in E \cap (U \times V)} c(e) \cdot \log \frac{\vol(C)}{\vol(U)} = \vol(U) \cdot \log \frac{\vol(C)}{\vol(U)}.
	\end{equation*}
	Since the cut is $\psi$-lopsided sparse it follows that the total capacity of the cut edges is at most
	\begin{equation*}
		c(U, \overline U) \leq \psi \cdot \vol(U) \cdot \log \frac{\vol(C)}{\vol(U)}.
	\end{equation*}
	Thus, we can afford to donate to each cut edge $e \in S$ a potential of $c(e) / \psi$ while maintaining our invariant. Since the total potential in the graph is $c(E) \log c(E)$ and every cut edge $e \in S$ receives a potential of at least $c(e) / \psi$, we finally have that $c(E) \log c(E) \geq c(S) / \psi$ and the claim follows.
\end{proof}

The main motivation for defining $\psi$-lopsided sparsity and taking the ratio~\smash{$\frac{\vol(V)}{\minvol(U)}$} into account lies in the following lemma: Compared to standard expanders, lopsided expanders only suffer a loglog-factor in the diameter.

\begin{restatable}{lemma}{lemLexpDiam} \label{lem:lexp-diam}
	Any $\psi$-lopsided expander has diameter $\Order(\psi^{-1} \log \log \vol(V) + \log\vol(V))$.
\end{restatable}

\begin{proof}[Proof Sketch.]
	The proof is similar in spirit to the proof for standard expanders in \Cref{lem:exp-diam}, for which we restate the intuition for clarity. Imagine growing a ball (i.e., a breadth-first search tree) around some node. With each step, the expansion property (related to the absence of $\phi$-sparse cuts) guarantees that we increase the explored capacity by a factor of $(1 + \phi)$; thus after $\Order(\phi \log \vol(V))$ steps we have explored essentially the entire graph.
	
	The difficulty in adopting the same proof for $\psi$-lopsided expanders stems from the fact that $\psi$-lopsided sparsity of a cut $U \subseteq V$ depends on the volume of $U$, so the expansion of the ball around a node differs in every step, e.g., if the volume of the ball around some node is constant, after one ball-growing step it becomes $O(\psi \log \vol(V))$. We resolve this challenge by analyzing the number of steps needed for the volume of the ball to grow from $\vol(V)/2^{i+1}$ to $\vol(V)/2^{i}$. This turns out to be roughly $\lceil (\psi \cdot i)^{-1} \rceil$, implying that after
	\begin{equation*}
		\sum_{i=1}^{\log \vol(V)} \lceil (\psi \cdot i)^{-1} \rceil = \Order(\psi^{-1} \log\log \vol(V) + \log \vol(V)) 
	\end{equation*}
	steps we have explored essentially the entire graph.
\end{proof}

Putting these two lemmas for lopsided expanders together, we indeed obtain the low-loss LDD in \Cref{thm:main-existential}. Specifically, we apply the Lopsided Expander Decomposition from \cref{lem:lexp-decomp} with parameter $\psi = \log\log \vol(V) / D$. We thereby cut only a $\Order(\log \vol(V) \log\log \vol(V) / D)$-fraction of the total capacity, and end up with a graph in which every strongly connected component is a $\psi$-lopsided expander. By \cref{lem:lexp-diam} said components have diameter $\Order(\psi^{-1} \log\log \vol(V)) = \Order(D)$ (which, again, can be made $\leq D$ by adjusting constants). Plugging this procedure into \cref{lem:mwu} we conclude that there is an LDD with loss $\Order(\log \vol(V) \log\log \vol(V)) = \Order(\log n \log\log n)$, completing the proof sketch of \cref{thm:main-existential}.

\subsection{A Deterministic Algorithm} \label{sec:overviewDet}
So far we have neglected Quest~2, i.e., the design of efficient algorithms. But how far from algorithmic is this approach of \Cref{sec:overview:sec:ldd-exp} really? It turns out that implementing this framework with some simple tricks leads to the following algorithmic result.

\begin{restatable}{theorem}{thmMainDet} \label{thm:main-det}
	For every directed graph there exists a directed LDD with loss $\Order(\log n \log\log n)$ and support size $\Order(D \log n)$ that can be computed in time $\widetilde\Order(m \poly(D))$ by a \emph{deterministic} algorithm.
\end{restatable}

This theorem comes with a strength and a weakness---the strength is that the theorem is deterministic. Note that the algorithm produces the \emph{explicit} support of an LDD; in fact, the probability distribution is simply a uniform distribution over a given list of cut sets $S$. For this reason, our algorithm might lead to some derandomized applications down the road by executing an LDD-based algorithm one-by-one for all $\Order(D \log n)$ cut sets. Derandomizations of this sort are common and sought after, especially in distributed and parallel domains. The weakness is that the running time has an overhead of $\poly(D)$. We remark that almost all algorithmic applications of LDDs (with the notable exception of~\cite{BernsteinNW22}) set $D = \polylog(n)$ or $D = n^{\order(1)}$, in which case the overhead is not dramatic, and the runtime is possibly near-linear.

\paragraph{Proof Idea.}
It can be quickly verified that implementing the Multiplicative Weights Update method from \Cref{lem:mwu} is not a problem algorithmically, and requires $O(R \cdot T)$ time, where $R=O(\log |E| \cdot D/L)$ and $T$ is the time required by the cost-minimizer. Hence, only computing the Lopsided Expander Decomposition could be costly. Since this is a strictly stronger definition that the standard Expander Decomposition, at first glance it appears hopeless that we could achieve a simple algorithm that avoids the cut-matching games machinery. Luckily, we find yet another simple insight that allows us to find a simple and intuitive algorithm after all: While it may generally be hard to find a sparse cut (even NP-hard!), in our case we only ever need to find a sparse cut \emph{in a graph with diameter more than $D$.} Indeed, if the graph has already diameter at most $D$, we can stop immediately (recall that we ultimately only care about the diameter and not the expander guarantee). One way to view the algorithm is that we construct a \emph{truncated} $\psi$-lopsided expander decomposition, where we terminate prematurely if the diameter is small.

Fueled by this insight, consider the following two-phase process for computing a (truncated) $\psi$-lopsided expander decomposition:
\begin{itemize}
    \setlength\parindent{1.6em}
    \setlength\parskip{0pt}
 	\item \emph{Phase (I):} We repeatedly take an arbitrary node $v$ and grow a ball around $v$ in the hope of finding a $\psi$-lopsided sparse cut. By choosing $\psi = \Theta(\log\log \vol(V) / D)$ we can guarantee two possible outcomes: 
 	\begin{enumerate}
 		\item[(a)] We find a sparse cut after, say, $\frac{D}{4}$ steps, in which case we cut the edges crossing the cut and recur on both sides (as in the Lopsided Expander Decomposition).
 		\item[(b)] The problematic case happens: the volume of the ball reaches $\frac{3}{4} \cdot \vol(V)$. In this case we have spent linear time but made no progress in splitting off the graph, so we remember $v$ and move to Phase (II).
 	\end{enumerate}
	
 	\item \emph{Phase (II):} We compute a \emph{buffer zone} around $v$, which consists of all nodes with distance $\frac{D}{2}$ to and from $v$. We repeatedly take an arbitrary node $u$ from outside this buffer zone and grow a ball around $u$ in the hope of finding a $\psi$-lopsided sparse cut. Because we know that the $\frac{D}{4}$-radius ball around $v$ contains most of the volume of the graph, and node $u$ is picked outside of the $\frac{D}{2}$-radius buffer zone, we can prove (using similar techniques to the proof sketch of \Cref{lem:lexp-diam}) that we will always find a $\psi$-lopsided sparse cut $U$ around $u$. Upon finding it, we cut the edges crossing the cut $(U, \overline U)$ and start Phase (I) for the graph induced by nodes in $U$, while continuing Phase (II) in graph $G \setminus U$.
 	
 	When eventually there are no nodes left outside the buffer zone, we stop, since the remaining graph has diameter $\leq D$.
\end{itemize}

The correctness of the procedure above is straightforward to show: we are essentially constructing a $\psi$-lopsided expander decomposition from \Cref{lem:lexp-decomp}, but terminating prematurely if the diameter is small. Hence, the total capacity of the cut edges is at most the total capacity of the cut edges in a ``complete'' $\psi$-lopsided expander, which is a $\Order(\log \vol(V) \log\log \vol(V) / D)$-fraction of the total capacity. The diameter guarantee follows directly from the stopping condition. By plugging this procedure into the algorithmic version of \cref{lem:mwu} we arrive at an LDD with loss $\Order(\log \vol(V) \log\log \vol(V)) = \Order(\log n \log\log n)$ in time $\widetilde\Order(m \poly(D))$, completing the proof sketch of \cref{thm:main-det}.


\subsection{A Near-Optimal Randomized Algorithm} \label{sec:overviewFast}

Our third and final contribution is achieving a near-linear running time, regardless of the magnitude of the diameter, by a randomized algorithm:

\begin{restatable}{theorem}{thmMainFast} \label{thm:main-fast}
	For every directed graph there exists a directed LDD with loss $\Order(\log n \log\log n)$ which can be computed (i.e., sampled from) in expected time $\widetilde\Order(m)$.
\end{restatable}

In contrast to \cref{thm:main-existential,thm:main-det}, our approach to \cref{thm:main-fast} is rather technical. We try more directly to extend the ideas of Bernstein, Nanongkai and Wulff-Nilsen~\cite{BernsteinNW22} (which in turn borrow ideas from~\cite{BernsteinGW20}). On a very high level, their algorithm classifies nodes $v$ \emph{heavy} if it reaches more than $\frac{n}{2}$ nodes within distance $\frac{D}{2}$, or \emph{light} otherwise. We can afford to cut around light nodes $v$ (with a specifically sampled radius $r \leq D$), and recur on both sides of the cut---since $v$ is light, the inside of the cut reduces by a constant fraction which is helpful in bounding the recursion depth. And if there are only heavy nodes in the graph then we can stop as the diameter is already at most $D$---indeed, the radius-$\frac{D}{2}$ balls around heavy nodes must necessarily intersect. The radius is sampled from a geometric distribution with rate $\Order(\log n / D)$ inspired by classical LDDs in undirected graphs~\cite{Bartal96}. The two logarithmic factors stem from (i) this $\log n$ overhead in the sampling rate, and (ii) the logarithmic recursion depth.

Our approach refines these ideas by classifying not into two classes---heavy or light---but into $\log\log n$ different levels. We essentially say that a node $v$ is at level $\ell$ if it reaches roughly $n / 2^{2^\ell}$ nodes within some distance roughly $D$. We can take advantage of this in two ways: At the one extreme, we can only cut around few nodes of small level $\ell$. Indeed, when we cut around a node at level $\ell$ we remove roughly $n / 2^{2^\ell}$ nodes from the graph, so this can be repeated at most $2^{2^{\ell}} \ll n$ times. For these levels we can afford to sample the radius in the geometric distribution with a significantly smaller rate, $\Order(2^i / D)$, thereby improving~(i). At the other extreme, whenever we cut around nodes with large level $\ell$ then in the recursive call of the inside of the cut there are only~$n / 2^{2^\ell} \ll n$ nodes. This effectively reduces the recursion depth of these levels to much less than $\log n$, improving (ii). In our algorithm, we find a balance between these two extreme cases. 

Unfortunately, while this idea works out existentially, there are several issues when attempting to implement this approach in near-linear time. The first issue---initially classifying which nodes are at what level in time $\widetilde\Order(m)$---can be solved by an algorithm due to Cohen~\cite{Cohen97}. However, over the course of the algorithm when we repeatedly remove nodes and edges from the graphs the level of a node might \emph{change}. Our solution involves a cute trick: First we prove that during the execution of our algorithm the level of a node can only \emph{increase} (and never decrease). Second, instead of cutting around an arbitrary node $v$, we always pick a \emph{random} node. The argument is as follows: If the classification is still correct for at least half of the nodes, then in each step we make progress with probability $\frac12$. And otherwise we can in fact afford to reclassify by Cohen's algorithm as the level of the nodes can only increase a small number of times, leading to only few repetitions of Cohen's algorithm in total.

We omit further details here and refer to the technical \cref{sec:ldd-fast}.