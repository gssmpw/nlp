\section{Introduction} \label{sec:intro}
In the design of combinatorial graph algorithms, decomposing graphs into smaller regions where problems become naturally easy to solve has emerged as a remarkably powerful paradigm. Two such graph decompositions stand out as particularly successful: \emph{Low-Diameter Decompositions} (LDDs) and \emph{Expander Decompositions}~(EDs) (defined formally in \Cref{def:ldd} and \Cref{sec:prelims}, respectively). Both decompositions remove some edges from the graph so that the remaining components either have a low diameter and thus behave nicely with respect to distance-type problems (in the case of LDDs), or are expanders which are well-suited for flow-type problems (in the case of EDs). This paradigm is particularly appealing as it typically leads to natural and efficient implementations in \emph{parallel,} \emph{distributed} and \emph{dynamic} models, and often also to \emph{deterministic} algorithms.

The vast majority of algorithmic results following this broader framework have been established for \emph{undirected} graphs. However, a recently increasing number of papers have successfully applied this paradigm also to \emph{directed} graphs~\cite{BernsteinGS20,BernsteinNW22,HuaKGW23,ChuzhoyK24,BernsteinBST24}, despite the serious technical difficulties that typically arise compared to the undirected setting. One celebrated example is the breakthrough \emph{near-linear time} algorithm for negative-length Single-Source Shortest Paths by Bernstein, Nanongkai and Wulff-Nilsen~\cite{BernsteinNW22}. In fact, their work was the first to define and apply Low-Diameter Decompositions in directed graphs.

Given these rapid and recent developments, it is to be expected that many new results will follow in this line of research. Consequently, anticipating a wave of researchers in need of directed Low-Diameter Decompositions, we initiate the systematic study of directed LDDs. Our main technical contribution is a \emph{near-optimal} LDD---near-optimal with respect to the loss parameter and the near-linear running time. Beyond improving the state of the art, we also unveil a compelling and novel connection between directed Low-Diameter Decompositions and Expander Decompositions, bringing together two previously studied concepts that are independently well-established. We believe this connection to be the main take-away of our paper, and are confident that it will inspire further applications. 

\subsection{Low-Diameter Decompositions} \label{sec:intro:sec:ldd}
Low-Diameter Decompositions in undirected graphs have been introduced almost 40 years ago by Awerbuch~\cite{Awerbuch85}, and, have since evolved into an irreplaceable tool in the design of combinatorial algorithms~\cite{AwerbuchGLP89,AwerbuchP92,AwerbuchBCP92,LinialS93,Bartal96,BlellochGKMPT14,MillerPX13,PachockiRSTW18,ForsterG19,ChechikZ20,BernsteinGW20,ForsterGV21,BernsteinNW22}. More generally, LDDs have been utilized in the development of diverse graph-theoretic structures such as oblivious routings~\cite{ZuzicGYHS22}, hopsets, neighborhood covers, and notably \emph{probabilistic tree embeddings}~\cite{Bartal96,Bartal98,FakcharoenpholRT04} (where the rough goal is to approximate an arbitrary (graph) metric by a simpler tree metric with polylogarithmic stretch) and \emph{low-stretch spanning trees}~\cite{AlonKPW95,ElkinEST08,AbrahamBN08,KoutisMP11,AbrahamN19,AbrahamCEFN20}. These structures in turn have led to further applications in approximation algorithms, online algorithms and network design problems~\cite{Bartal96,BorodinY98,HaeuplerHZ21}.

While the precise definitions of LDDs differed at first, all of them had in common that the graph was decomposed by removing few edges (on average) so that the remaining connected components have bounded diameter. More precisely, the modern definition is that an LDD is a probability distribution over edge cuts that cut each individual edge with probability at most $L/D$ (for some small loss factor~$L$) so that the resulting connected components have diameter at most $D$. For undirected graphs, the best-possible loss factor turns out to be $L = \Theta(\log n)$ with natural matching upper and lower bounds (see e.g.~\cite{Bartal96,FakcharoenpholRT04} and \cref{foot:logn-lower-bound}).

Only very recently, low-diameter decompositions debuted in \emph{directed} graphs. They played a key role in the near-linear time algorithm for the Single-Source Shortest Paths problem in graphs with negative edge lengths by Bernstein, Nanongkai and Wulff-Nilsen~\cite{BernsteinNW22} (following the paradigm outlined before). Curiously, before their seminal work, low-diameter decompositions for directed graphs were mostly unexplored---to the extent that it was even unclear what the definition should be. An even more recent application of directed LDDs is in the context of restricted (i.e., bicriteria) shortest paths~\cite{AshvinkumarBK25}.

\begin{restatable}[Directed Low-Diameter Decomposition]{definition}{defLDD} \label{def:ldd} 
	A \emph{directed low-diameter decomposition} with \emph{loss $L$} for a directed edge-weighted graph $G = (V, E, \ell)$ and a parameter $D \geq 1$ is a probability distribution over edge sets $S \subseteq E$ that satisfies the following two properties:
	\begin{itemize}
		\item For any two nodes $u, v \in V$ that are part of the same strongly connected component in $G \setminus S$, we have $d_G(u, v) \leq D$ and $d_G(v, u) \leq D$.
		\item For all edges $e \in E$, it holds that $\Pr(e \in S) \leq \frac{\ell(e)}{D} \cdot L$.
	\end{itemize}
\end{restatable}

That is, for directed graphs we require that all remaining \emph{strongly} connected components have diameter $D$ in the sense that each pair of nodes is at distance at most $D$ in the original uncut graph (this property is also called a ``weak'' diameter guarantee; see more details in the paragraph towards the end of \cref{sec:intro:sec:ldd}).

In light of this definition, it is natural to study directed LDDs with respect to two objectives: Minimizing the loss factor $L$, and \emph{computing} the LDD efficiently. Both of these requirements typically translate immediately to algorithmic improvements (e.g., the loss factor $L$ typically becomes a factor in the running time of algorithmic applications).

\paragraph{Quest 1: Minimizing the Loss \boldmath$L$.}
It is easy to see that a loss factor of $L \geq 1$ is necessary: Consider a graph consisting of disjoint copies of $D + 1$-cycles (of unit length, say). Any LDD is forced to cut at least one edge from every cycle, and thus some edges are deleted with probability~$\frac{1}{D+1}$. In fact, from the same lower bound as for undirected graphs one can show that $L \geq \Omega(\log n)$ is necessary\footnote{Specifically, using e.g., the probabilistic method, one can construct an undirected $n$-node graph $G$ with the following two properties: (i) the graph contains $c n$ edges (for some constant $c > 1$), and (ii) the girth of $G$ (i.e., the length of the shortest cycle) is at least $g = \Omega(\log n)$. Such a graph constitutes an obstruction against undirected LDDs with parameter $D = \frac{g}{2} - 1$, say. Indeed, all connected components that remain after the LDD cuts edges cannot contain cycles (as any cycle contains two nodes at distance at least $\frac{g}{2} > D$). This means that all connected components are trees, and thus the remaining graph is a forest containing at most $n - 1$ edges. Therefore, the LDD has cut at least~\makebox{$c n - n = \Omega(n)$} edges. In particular, the per-edge cutting probability of some edges must be~\makebox{$\Omega(1) = \Omega(\log n / D)$}. The same argument applies for directed LDDs by taking the same undirected graph and making all edges bidirected.\label{foot:logn-lower-bound}} for some directed graphs.

Conversely, Bernstein, Nanongkai and Wulff-Nilsen~\cite{BernsteinNW22} showed that a loss of $\Order(\log^2 n)$ can be achieved. Their work leaves open whether this factor can be improved, possibly to $\Order(\log n)$ which would match the existing unconditional lower bound, or whether $\Omega(\log^2 n)$ is necessary.


\paragraph{Quest 2: Efficient Algorithms.}
To be useful in algorithmic applications, it is necessary to be able to compute the LDD (or, formally speaking, to be able to \emph{sample} from the LDD efficiently). This is a non-negligible problem as many graph decompositions are much simpler to prove existentially than constructively and efficiently---for instance, for Expander Decompositions there is a stark contrast between the extremely simple existential proof and the involved efficient algorithms based on cut-matching games~\cite{RackeST14,KhandekarRaoVazirani09}. The $\Order(\log^2 n)$-loss due to~\cite{BernsteinNW22} is of course efficient.

\paragraph{Side Quest: Weak versus Strong.}
Another dimension is the distinction between~``weak'' and ``strong'' diameter guarantees. Specifically, \cref{def:ldd} requires the weak guarantee by requiring that any two nodes in the same strongly connected component are at distance at most $D$ \emph{in the original graph $G$}. The strong version instead requires that each strongly connected component in~$G \setminus S$ has diameter at most~$D$ \emph{in the graph $G \setminus S$.} While strong LDDs give a theoretically more appealing guarantee, for most algorithmic applications it turns out that weak LDDs suffice. The LDD developed by Bernstein, Nanongkai and Wulff-Nilsen~\cite{BernsteinNW22} has a weak guarantee, but follow-up work~\cite{BringmannCF23} later extended their results to a strong LDD with cubic loss $\Order(\log^3 n)$.

In this paper, we will mostly ignore the distinction between weak and strong and follow \cref{def:ldd} as it is. We remark however that all of our existential results do in fact have the strong diameter guarantee.