This section reviews contributions across RAG domains, semantic search using vector embeddings, knowledge graph construction, NMF, and legal information systems.

\subsection{Non-Negative Matrix Factorization for Pattern Discovery}
NMF is a dimensionality reduction technique used to uncover latent patterns in data. \cite{lee2001algorithms} analyzed NMF as an interpretable method for extracting features and topics from large datasets, specifically highlighting its ability to identify meaningful and non-overlapping components. Building on this work, \cite{hoyer2004nonnegative} introduced sparseness constraints for better interpretability,  improving applications of NMF in real-world scenarios through more focused feature selection. In the legal domain, NMF has been valuable for analyzing complex textual data, such as case law and statutes, and assisting with topic discovery and clustering. For instance, \cite{Budahazy2021AnalysisOL} applied NMF to legal documents to extract latent topics and visualize relationships, demonstrating how NMF’s interpretable structure aids researchers in identifying underlying topics not readily apparent in raw text. They even applied NMF hierarchically to find fine-grained topics, however did not have a way to approximate the number of clusters automatically at each decomposition. More recently, \cite{Li2022GuidedSN} proposed a guided semi-supervised NMF approach for topic discovery in legal documents, using domain knowledge to steer factorization and ensure the extracted topics remain highly relevant. This semi-supervised extension bridges the gap between fully automated unsupervised techniques and expert-driven analysis.  NMF’s utility in legal contexts is particularly significant, providing an interpretable framework for analyzing large textual datasets.

\subsection{Retrieval-Augmented Generation}
RAG has emerged as a foundational approach for improving AI systems across various domains, including law. For instance, \cite{lewis2020retrieval} introduced a framework that dynamically retrieves relevant documents to ground generative outputs, achieving notable gains in accuracy. Building on this idea, \cite{guu2020realm} proposed a retrieval-augmented pretraining method that integrates external knowledge for improved downstream task performance. In contrast, \cite{izacard2021leveraging} demonstrated the effectiveness of retrieval in open-domain question answering. These advances lay the groundwork for applying RAG to the legal sector, where the method’s ability to ground LLMs in authoritative texts reduces hallucinations and increases accuracy in tasks such as case law retrieval, statutory reasoning, and judgment prediction. Notable examples include CBR-RAG, which incorporates Case-Based Reasoning to structure retrieval for legal QA \cite{10.1007/978-3-031-63646-2_29}, and LegalBench-RAG. This benchmark suite tailors evaluation metrics to the demands of legal information synthesis \cite{pipitone2024legalbenchragbenchmarkretrievalaugmentedgeneration}. Parallel work has demonstrated RAG’s capabilities in other domains, such as malware data analysis, by combining embeddings, KGs, and NMF \cite{barron2024domainspecificretrievalaugmentedgenerationusing}. Other works show how LLMs can dynamically decide when and what to retrieve to improve legal reasoning \cite{asai2023retrieve} and how multi-step legal judgment prediction can benefit from iterative retrieval and generation \cite{cui2023multistep}, further demonstrating th effectiveness of combining retrieval strategies with LLMs.


\subsection{Semantic Search with Vector Embeddings}
Semantic search operates on dense vector representations to find the deeper semantic relationships in texts, going beyond keyword matching to proper context retrieval \cite{karpukhin2020dense, guu2020realm}. This search is especially valuable in the legal domain, where queries often demand conceptual rather than surface-level understanding. Domain-specific pretraining has become increasingly important, as in LEGAL-BERT \cite{chalkidis2020legalbert}, outperforming general-purpose models by better capturing legal language nuances. Benchmarks like LeCaRD \cite{ma2021lecard} show the effectiveness of dense retrievers—such as SBERT—in legal case retrieval of Chinese law and the abilities of hybrid approaches that integrate both lexical and dense retrieval methods. Hierarchical transformer architectures \cite{xiao2021hierarchical} and long-context models like Longformer \cite{beltagy2020longformer} further address the challenges of lengthy legal documents. In contrast, citation-driven approaches such as SPECTER \cite{cohan2020specter} leverage metadata to improve retrieval.
Furthermore, work on neighborhood contrastive learning for scientific document representations (SciNCL) \cite{ostendorff2022neighborhood} demonstrates how controlled nearest neighbor sampling over citation graph embeddings can provide continuous similarity. This strategy could also inform citation-based retrieval improvements in legal domains. Challenges remain around scaling, explainability, and incorporating heterogeneous data sources in real-world legal workflows.

\subsection{Knowledge Graphs in Legal Research}
In the legal domain, researchers have used KGs for knowledge extraction \cite{Sovrano2020LegalKE}, constructed domain-specific graphs tailored to legal contexts \cite{Dong2021AKG}, and explored their potential for addressing trust, privacy, and transparency concerns \cite{a9b58e3cf6ea4306b01b44386cbf9c1f}. Recent studies have demonstrated the effectiveness of KGs in recommending similar legal cases \cite{dhani2024similarcasesrecommendationusing}, linking case law with statutes for improved retrieval \cite{zhou2024automaticknowledgegraphconstruction}, and using graphs to enhance legal case law search in Chinese legal systems \cite{BI2022109046}. Beyond retrieval, KGs have been incorporated into knowledge-aware machine reading systems for legal question answering \cite{long2021kalm}. Collectively, these works demonstrate KGs' capacity to represent intricate legal concepts, structure regulatory frameworks, and encourage reasoning over legal data.

\subsection{Legal Systems and Case Retrieval}
 
Legal information systems have evolved rapidly with the advent of neural architectures and hybrid retrieval pipelines, enabling precise tasks such as precedent retrieval, statute matching, and judgment prediction \cite{chalkidis2019neural, robaldo2020neural, chalkidis2021paragraph}. Benchmarks like LeCaRD \cite{ma2021lecard} and LexGLUE \cite{chalkidis2022lexglue}, along with LEGAL-BERT-based systems \cite{chalkidis2020legalbert}, have demonstrated the capability of these neural methods to improve accuracy in analyzing legal corpora. In particular, researchers have leveraged structured reasoning with transformers and graph representations to link statutes and precedents, as evidenced in the COLIEE competition \cite{rabelo2021coliee}. Nonetheless, several limitations remain: data scarcity and jurisdictional bias continue to restrict the generalizability of such models. At the same time, resource-intensive retrievers like BERT-based cross-encoders \cite{shao2022bert} have challenges scaling to large-scale legal databases. Earlier works in juris-informatics have highlighted the potential of automating legal reasoning and document analysis \cite{ashley2017artificial}, laying the groundwork for modern approaches that fuse knowledge graphs, transformers, vector stores, and agent-oriented RAG pipelines to deliver more explainable and efficient legal research workflows.

