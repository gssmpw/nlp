\section{Introduction}
\label{sec:intro}
Visual Sentiment Understanding (VSU)~\cite{Emoset,tsl} focuses on leveraging explicit scene information (e.g., facial expression) to understand the sentiments of images or videos. However, there exist many surveillance videos in the real world, where implicit scene information (e.g., human action, object relation and visual background) can more truly reflect visual sentiments compared with explicit scene information. In light of this, this paper defines the need to rely on implicit scene information to precisely identify visual sentiments as implicit visual sentiments, such as robbery, shooting and other negative implicit visual sentiments under surveillance videos. More importantly, current VSU studies mainly focus on identifying the visual sentiments, yet they ignore exploring when and why these sentiments occur. Nevertheless, this information is critical for sentiment applications, such as effectively filtering negative or abnormal contents in the video to safeguard the mental health of children and adolescents~\cite{mental-health1,mental-health2,mental-health3}. 

Building on these considerations, this paper proposes a new \textbf{Omni}-scene driven visual \textbf{S}entiment \textbf{I}dentifying, \textbf{L}ocating and \textbf{A}ttributing in videos (Omni-SILA) task, which leverages Video-centred Large Language Models (Video-LLMs) for interactive visual sentiment identification, location and attribution. This task aims to identify what is the visual sentiment, locate when it occurs and attribute why this sentiment through both explicit and implicit scene information. Specifically, the Omni-SILA task identifies, locates and attributes the visual sentiment segments through interactions with LLM. As shown in Figure \ref{fig:intro}, a strong man is fighting another man during the timestamps from 8s to 15s, where the LLM is asked to identify, locate and attribute this \emph{fighting} implicit visual sentiment. In this paper, we explore two major challenges when leveraging Video-LLMs to comprehend omni-scene (i.e., both explicit and implicit scene) information for addressing the Omni-SILA task.  

On one hand, how to model explicit and implicit scene information is challenging. Existing Video-LLMs primarily devote to modeling general visual information for various video understanding tasks. Factually, while LLMs encode vast amounts of world knowledge, they lack the capacity to perceive scenes~\cite{3DMIT,cot}. Compared to general visual information, explicit and implicit scene information is crucial in the Omni-SILA task. Taking Figure \ref{fig:intro} as an example, the negative implicit visual sentiment \emph{fighting} in the video is clearly conveyed through the action \emph{twists an arm} and \emph{forces to ground}. However, due to the heterogeneity of these omni-scene information (i.e., various model structures and encoders), a single, fixed-capacity transformer-based model fails to capitalize on this inherent redundancy, making Video-LLMs difficult to capture important scene information. Recently, MoE has shown scalability in multi-modal heterogeneous representation fusion tasks~\cite{moe}. Inspired by this, we take advantage of the MoE architecture to model explicit and implicit scene information in videos, thereby evoking the omni-scene perceptive ability of Video-LLMs.  

On the other hand, how to highlight the implicit scene information beyond the explicit is challenging. Since explicit scene information (e.g., facial expression) has richer sentiment semantics than implicit scene information (e.g., subtle actions), it is easier for models to model explicit scene information, resulting in modeling bias for explicit and implicit scene information. However, compared with explicit scene information, implicit scene information has more reliable sentiment discriminability and often reflects real visual sentiments as reported by~\citet{AffectGPT}. For the example in Figure \ref{fig:intro}, the strong man is \emph{laughing} while \emph{twists another man's arm} and \emph{forces him to the ground}, where the facial expression \emph{laughing} contradicts the negative \emph{fighting} visual sentiment conveyed by the actions of \emph{twists the arm} and \emph{forces to ground}. Recently, causal intervention~\cite{pearl} has shown capability in mitigating biases among different information~\cite{catt}. Inspired by this, we take advantage of causal intervention to highlight the implicit scene information beyond the explicit, thereby mitigating the modeling bias to improve a comprehensive understanding of visual sentiments. 

To tackle the above challenges, this paper proposes an \textbf{I}mplicit-enhanced \textbf{C}ausal \textbf{M}oE (ICM) approach, aiming to identify, locate and attribute visual sentiments in videos. Specifically, a \textbf{S}cene-\textbf{B}alanced \textbf{M}oE (SBM) module is designed to model both explicit and implicit scene information. Furthermore, an \textbf{I}mplicit-\textbf{E}nhanced \textbf{C}ausal (IEC) module is tailored to highlight the implicit scene information beyond the explicit. Moreover, this paper constructs two explicit and implicit Omni-SILA datasets to evaluate the effectiveness of our ICM approach. Comprehensive experiments demonstrate that ICM outperforms several advanced Video-LLMs across multiple evaluation metrics. This justifies the importance of omni-scene information for identifying, locating and attributing visual sentiment, and the effectiveness of ICM for capturing such information.
