\section{Results and Discussion}

\setlength{\tabcolsep}{1.8pt}
\begin{table*}[]
\renewcommand{\arraystretch}{0.7}
\addtolength{\tabcolsep}{8pt}
\begin{center}
\setlength{\abovecaptionskip}{-0.1 ex}
\setlength{\belowcaptionskip}{-0.5 ex}
\caption{The effectiveness study of various scenes in Omni-SILA, where \Checkmark means that we capture the current scene. All the experiments are conducted on Explicit and Implicit Omni-SILA datasets and evaluate VSI, VSL and VSA three tasks. Fac, Act, Obj and Back are short for facial expression, human action, object relation and visual background scene, respectively.}
\setlength{\belowcaptionskip}{-5 ex}
\label{tab:scene_analysis}
\resizebox{\linewidth}{!}{
    \begin{tabular}{cccc|cccccc|cccccc}
\toprule[1.2pt]
\multirow{2}{*}{Fac} & \multirow{2}{*}{Act} & \multirow{2}{*}{Obj} & \multirow{2}{*}{Back} & \multicolumn{6}{c|}{Explicit Omni-SILA Dataset}                                                                & \multicolumn{6}{c}{Implicit Omni-SILA Dataset}                                                                \\ \cline{5-16} 
                              &                               &                               &                                & Acc & F2 & FNRs$\downarrow$ & mAP@tIoU & Sen-A & Atr-R & Acc & F2 & FNRs$\downarrow$ & mAP@tIoU & Sen-A & Atr-R \\ \hline
                   & \Checkmark                     & \Checkmark                     &  \Checkmark                     & 68.81        & 67.79       & 36.65         & 23.25             & 63.29          & 12.38          & 46.18        & 45.93       & 34.52         & 25.77             & 79.26          & 12.70          \\
            \Checkmark         &                     & \Checkmark                     & \Checkmark                      & 68.66        & 67.87       & 37.52         & 22.64             & 63.48          & 12.16          & 43.89        & 44.97       & 37.89         & 24.62             & 77.78          & 11.36          \\
            \Checkmark         & \Checkmark                     &                     & \Checkmark                      & 70.03        & 69.06       & 34.97         & 24.07             & 64.15          & 13.01          & 45.34        & 46.22       & 34.54         & 26.31             & 79.41          & 12.39          \\
            \Checkmark         & \Checkmark                     & \Checkmark                     &                      & 69.16        & 68.32       & 36.07         & 23.01             & 63.62          & 12.66          & 44.57        & 45.49       & 35.86         & 25.39             & 78.36          & 11.78          \\
            \Checkmark       & \Checkmark                   &  \Checkmark                   & \Checkmark                     & 71.41        & 70.21       & 33.38         & 25.51             & 65.38          & 13.97          & 47.39        & 48.36       & 32.76         & 27.88             & 81.94          & 13.63          \\ 
\bottomrule[1.2pt]
\end{tabular}
}
\end{center}
\end{table*}

\subsection{Experimental Results}
% 从三个不同的任务上面进行描述
Table \ref{tab:main_results} and Table \ref{tab:cause_results} show the performance comparison of different approaches on our Omni-SILA task (including VSI, VSL and VSA). From this table, we can see that:
\textbf{(1)} For VSI, our ICM approach outperforms all the Video-LLMs on the implicit Omni-SILA dataset, and achieves comparable results on the explicit Omni-SILA dataset. For instance, compared to the best-performing Video-LLaVA, ICM achieves average improvements by 6.93\% ($p$-value<0.01) on implicit Omni-SILA dataset and 3.18\% ($p$-value<0.05) on explicit Omni-SILA dataset. This indicates that identifying implicit visual sentiments is more challenging than the explicit, and justifies the effectiveness of ICM in identifying what is visual sentiment.  
\textbf{(2)} For VSL, similar to the results on VSI task, our ICM approach outperforms all the baselines on the implicit Omni-SILA dataset while achieves comparable results on the explicit. For instance, compared to the best-performing results underlined, ICM achieves the average improvements by 5.44\% ($p$-value<0.01) on the implicit and 3.65\% ($p$-value<0.05) on the explicit. Particularly, our ICM approach surpasses all the Video-LLMs on FNRs by 17.58\% ($p$-value<0.01) on the implicit and 10.94\% ($p$-value<0.01) on explicit compared with the best results underlined. This again justifies the challenge in locating the implicit visual sentiments, and demonstrates the effectiveness of ICM in locating when the visual sentiment occurs.
\textbf{(3)} For VSA, our ICM approach outperforms all the Video-LLMs on both implicit and explicit Omni-SILA datasets. Specifically, compared to the best-performing approach on all VSA metrics, ICM achieves total improvements of 5.9\%, 14.28\%, 10.55\% and 5.18 on Sem-R, Sem-C, Sen-A and Atr-R in two datasets. Statistical significance tests show that these improvements are significant ($p$-value<0.01). This demonstrates that ICM can better attribute why are both explicit and implicit visual sentiments compared to advanced Video-LLMs, and further justifies the importance of omni-scene information.

\begin{figure}
\begin{center}
    \includegraphics[width=\linewidth]{images/analysis.pdf}
    \setlength{\abovecaptionskip}{-2 ex}
    \setlength{\belowcaptionskip}{-4 ex}
\caption{Two line charts to compare several well-performing Video-LLMs with our ICM approach on 11 implicit visual sentiments of FNRs (a) and Atr-R (b) two metrics, and the red boxes indicate the categories \emph{Vandalism} of FNRs and \emph{Fire} of Atr-R where the performance difference is biggest.}
\label{fig:analysis}
\end{center}
\end{figure} 

\subsection{Contributions of Key Components}
To further study contributions of key components in ICM, we conduct a series of ablations studies, the results of which are detailed in Table \ref{tab:main_results} and Table \ref{tab:cause_results}. From these tables, we can see that:
\textbf{(1) w/o IEC} block shows inferior performance compared to ICM, with an average decrease of VSI, VSL and VSA tasks by 4.82\%($p$-value<0.05), 6.6\%($p$-value<0.01) and 10.37\%($p$-value<0.01). This indicates the existence of bias between explicit and implicit information, and further justifies the effectiveness of IEC block to highlight the implicit scene information beyond the explicit.  
\textbf{(2) w/o SBM} block shows inferior performance compared to ICM, with an average decrease of VSI, VSL and VSA tasks by 5.99\%($p$-value<0.01), 9.29\%($p$-value<0.01) and 13.12\%($p$-value<0.01). This indicates the effectiveness of SBM block in modeling and balancing the explicit and implicit scene information via the MoE architecture, encouraging us to model heterogeneous information via MoE.   
\textbf{(3) w/o scene tuning} exhibits obvious inferior performance compared to ICM, with an average decreases of VSI, VSL and VSA tasks by 11.39\%($p$-value<0.01), 20.47\%($p$-value<0.01) and 23.72\%($p$-value<0.01). This confirms that the backbone lacks the ability to understand omni-scene information. This further demonstrates the necessity and effectiveness of pre-tuning, and encourages us to introduce more high-quality datasets to improve the scene understanding ability of Video-LLMs.



\begin{figure}
\begin{center}
    \includegraphics[width=\linewidth]{images/effciency.pdf}
    \setlength{\abovecaptionskip}{-2 ex}
    \setlength{\belowcaptionskip}{-4 ex}
\caption{Two statistical charts to illustrate the efficiency of our ICM approach. The histogram (a) compares the inference time of ICM with baselines, while the line chart (b) shows the convergence of training losses of ICM, two well-performing Video-LLMs and the variants of ICM across training steps.}
\label{fig:effciency}
\end{center}
\end{figure} 


\subsection{Effectiveness Study of Scene Information}
To delve deeper into the impact of various scene information, we conduct a series of ablations studies, the results of which are detailed in Table \ref{tab:scene_analysis}. From this table, we can see that:
\textbf{(1) w/o Facial Expression Modeling} shows more obvious inferior performance on the explicit than the implicit Omni-SILA dataset, with the total decrease of VSI, VSL and VSA by 5.02\%($p$-value<0.01), 5.53\%($p$-value<0.01) and 3.68\%($p$-value<0.05) on the explicit; 3.64\%($p$-value<0.05), 3.87\%($p$-value<0.05) and 3.61\%($p$-value<0.05) on the implicit. This is reasonable that most of videos in the implicit dataset may have no visible faces. 
\textbf{(2) w/o Human Action Modeling} exhibits obvious inferior performance on both explicit and implicit Omni-SILA datasets, with the total decrease of VSI, VSL and VSA by 5.99\%($p$-value<0.01), 7.7\%($p$-value<0.01) and 5.07\%($p$-value<0.01). This indicates that human action information is important to recognize explicit and implicit visual sentiments. For example, we can precisely identify, locate and attribute the \emph{theft} via \emph{an obvious human action of stealing}. 
\textbf{(3) w/o Object Relation Modeling} shows slight inferior performance on both explicit and implicit Omni-SILA datasets. This is reasonable that the visual sentiment of visual object relations is very subtle, unless a specific scene like \emph{<a man, holding, guns>} can be identified as \emph{robbery}.
\textbf{(4) w/o Visual Background Modeling} exhibits obvious inferior performance on both explicit and implicit Omni-SILA datasets, with the total decrease of VSI, VSL and VSA by 4.92\%($p$-value<0.05), 5.39\%($p$-value<0.01) and 4.25\%($p$-value<0.05). This indicates that video background is also important to recognize explicit and implicit visual sentiments. For example, we can precisely identify, locate and attribute the \emph{fire} via the \emph{forest on fire with flames raging to the sky} background.  

\subsection{Applicative Study of ICM Approach}
To study the applicability of ICM, we compare the FNRs and Atr-R of ICM with other Video-LLMs. From Table \ref{tab:main_results}, we can see that ICM performs the best on the metric of FNRs. For example, ICM outperforms the best-performing Video-LLaVA by 10.94\% ($p$-value<0.01) and 17.58\% ($p$-value<0.01) on the explicit and implicit Omni-SILA dataset respectively. From Table \ref{tab:cause_results}, ICM achieves state-of-the-art performance on both implicit and explicit Omni-SILA datasets. These results indicate that ICM is effective in reducing the rates of FNRs and providing reasonable attributions, which is important in application. Furthermore, recognizing that the implicit Omni-SILA dataset comprises 11 distinct real-world crimes, we perform an analysis of each negative implicit visual sentiment on the performance of FNRs (Figure~\ref{fig:analysis} (a)) and Atr-R (Figure~\ref{fig:analysis} (b)). From two figures, we can see that ICM surpasses all other Video-LLMs across 11 crimes. Particularly, ICM performs best on \emph{Vandalism} (4) in FNRs and \emph{Fire} (9) in Atr-R. This indicates that ICM is effective in reducing FNRs and improving the interpretability of negative implicit visual sentiments, encouraging us to consider the omni-scene information, such as \emph{human action} in \emph{Vandalism} and \emph{visual background} in \emph{Fire}, for precisely identifying, locating and attributing visual sentiments.

\subsection{Efficiency Analysis of ICM Approach}
To study the efficacy of ICM, we compare the inference time of ICM with other Video-LLMs (Figure~\ref{fig:effciency} (a)), and analyze the convergence of training loss for Video-ChatGPT, Video-LLaVA, ICM and its variants over different training steps (Figure~\ref{fig:effciency} (b)). As shown in Figure~\ref{fig:effciency} (a), we can see that ICM achieves little difference in inference time compared with other Video-LLMs. This is reasonable because MoE can improve the efficiency of inference~\cite{Loramoe,MoE-LoRA}, encouraging us to model omni-scene information via MoE architecture. From Figure~\ref{fig:effciency} (b), we can see that: 
\textbf{(1)} ICM shows fast convergence compared to Video-LLMs. At the convergence fitting point, the loss of ICM is 0.97, while Video-LLaVA is 2.21. This indicates that ICM has more high efficiency than other Video-LLMs, which further shows the potential of ICM for quicker training times and less source use, thereby improving its applicative use in real-world applications.
\textbf{(2)} ICM shows fast convergence compared to its variants, indicating that the integration of MoE architecture and causal intervention can accelerate the convergence process.
\textbf{(3)} ICM shows fast convergence compared to without scene-tuning, where the loss is 3.95 at the convergence fitting point. This again justifies the importance of scene understanding before Omni-SILA tuning.


\begin{figure}
    \centering
    \includegraphics[scale=0.33]{images/case.pdf}
    \setlength{\abovecaptionskip}{0 ex}
    \setlength{\belowcaptionskip}{-5 ex}
    \caption{Two samples to compare ICM with other baselines.}
    \label{fig:case}
\end{figure}

\subsection{Qualitative Analysis}
As illustrated in Figure \ref{fig:case}, we provide a qualitative analysis to intuitively compare the performance of ICM with other Video-LLMs on the Omni-SILA task. Specifically, we randomly select two samples from each of explicit and implicit Omni-SILA datasets, asking these approaches to ``\emph{Identify and locate the visual sentiment in the following video, and attribute this sentiment}''. Due to the space limit, we choose the top-3 well-performing approaches. From this figure, we can see that:
\textbf{(1)} Identifying and locating implicit visual sentiment is more challenging than the explicit. For instance, Video-LLaVA can roughly locate and precisely identify \emph{positive} sentiment in Example 1, but has difficulties in locating \emph{traffic accident} in Example 2. However, ICM can precisely identify and locate the \emph{traffic accident}.
\textbf{(2)} Attributing both explicit and implicit visual sentiments is challenging. All the advanced Video-LLMs are difficult to attribute visual sentiments, even some approaches are nonsense. While ICM can provide reasonable attributions due to the capture of omni-scene information, such as \emph{surprise} face, \emph{hugging} action in Example 1, and \emph{speeds, hitting} action, \emph{pedestrians} visual background in Example 2. This again justifies the importance of omni-scene information, and effectiveness of ICM for capturing such information.

