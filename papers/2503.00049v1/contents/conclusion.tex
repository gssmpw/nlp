\section{Conclusion}
In this paper, we address a new Omni-SILA task, aiming to identify, locate and attribute the visual sentiment in videos, and we propose an ICM approach to address this task via omni-scene information. The core components of ICM involve SBM and IEC blocks to effectively model scene information and highlight the implicit scene information beyond the explicit. Experimental results on our constructed Omni-SILA datasets demonstrate the superior performance of ICM over several advanced Video-LLMs. In our future work, we would like to train a Video-LLM supporting more signals like audio from scratch to further boost the performance of sentiment identifying, locating and attributing in videos. In addition, we would like to leverage some light-weighting technologies (e.g., LLM distillation and compression) to further improve ICM's efficiency.
