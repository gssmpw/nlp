\section{Experimental Settings}
\subsection{Datasets Construction}
\label{sec:datasets}
To assess the effectiveness of our ICM approach for the Omni-SILA task, we construct instruction datasets for two stages.

For \textbf{Scene-Tuning} stage, we choose CMU-MOSEI~\cite{mosei}, HumanML3D~\cite{HumanML3D}, RefCOCO~\cite{ReferItGame} and Place365~\cite{Places365} four datasets and manually construct instructions to improve Video-LLaVA's ability in understanding facial expression, human action, object relations and visual backgrounds four scenes. For instance in Figure \ref{fig:model}, with the instruction ``\emph{Please describe the facial/action/image region/background}'', the responses are ``\emph{A woman with sad face./A person waves his hand./A dog <0.48,2.23,1.87,0.79> on the grass./A blue sky.}''. Particularly, we use SAM-V1~\cite{sam} to segment objects and capture visual backgrounds in Place365. Since CMU-MOSEI and HumanML3D contain over 20K videos, we sample frames at an appropriate rate to obtain 200K frames. To ensure scene data balance, we randomly select 200K images from RefCOCO and Place365.

For \textbf{Omni-SILA Tuning} stage, we construct an Omni-SILA tuning dataset consisting of 202K video clips, and we sample 8 frames for each video clip, resulting in 1.62M frames. This dataset consists of an explicit Omni-SILA dataset (training: 52K videos, test: 25K videos) and an implicit Omni-SILA dataset (training: 102K videos, test: 23K videos). The explicit Omni-SILA dataset is based on public TSL-300~\cite{tsl}, which contains explicit \emph{positive}, \emph{negative} and \emph{neutral} three visual sentiment types. Due to its lack of sentiment attributions, we leverage GPT-4V~\cite{gpt4v} twice to generate and summarize the visual sentiment attribution of each frame from four scene aspects, and manually check and adjust inappropriate attributions. Implicit Omni-SILA dataset is based on public CUVA~\cite{cuva}, which contains implicit \emph{Fighting} (1), \emph{Animals Hurting People} (2), \emph{Water Incidents} (3), \emph{Vandalism} (4), \emph{Traffic Accidents} (5), \emph{Robbery} (6), \emph{Theft} (7), \emph{Traffic Violations} (8), \emph{Fire} (9), \emph{Pedestrian Incidents} (10), \emph{Forbidden to Burn} (11), \emph{Normal} twelve visual sentiment types. We manually construct instructions for each video clip. Specifically, with the beginning of instruction ``\emph{You will be presented with a video. After watching the video}'', we ask the model to identify ``\emph{please identify the explicit/implicit visual sentiments in the video}'', locate ``\emph{please locate the timestamp when ...}'', and attribute ``\emph{please attribute ... considering facial expression, human action, object relations and visual backgrounds}'' visual sentiments. The corresponding responses are ``\emph{The explicit/implicit visual sentiment is ...}'', ``\emph{The location of ... is from 4s to 18s.}'', ``\emph{The attribution is several ...}''. Particularly, due to the goal of identifying, locating and attributing visual sentiments, we evaluate our ICM approach by inferring three tasks with the same instruction on both explicit and implicit Omni-SILA datasets.

\setlength{\tabcolsep}{1.8pt}
\begin{table*}[]
\renewcommand{\arraystretch}{0.9}
\addtolength{\tabcolsep}{8pt}
\begin{center}
\setlength{\abovecaptionskip}{-0 ex}
\setlength{\belowcaptionskip}{-0.5 ex}
\caption{Comparison of several Video-LLMs and our ICM approach on Explicit and Implicit Omni-SILA datasets for visual sentiments attributing, where GPT-based and Human indicate two methods to evaluate the Atr-R metric.}
\label{tab:cause_results}
\resizebox{\linewidth}{!}{
    \begin{tabular}{c|ccccc|ccccc}
\toprule[1.2pt]
\multirow{3}{*}{Approach} & \multicolumn{5}{c|}{Explicit Omni-SILA Dataset}                                                                                               & \multicolumn{5}{c}{Implicit Omni-SILA Dataset}                                                                                              \\ \cline{2-11} 
                                   & \multirow{2}{*}{Sem-R} & \multirow{2}{*}{Sem-C} & \multirow{2}{*}{Sen-A} & \multicolumn{2}{c|}{Atr-R} & \multirow{2}{*}{Sem-R} & \multirow{2}{*}{Sem-C} & \multirow{2}{*}{Sen-A} & \multicolumn{2}{c}{Atr-R} \\ \cline{5-6} \cline{10-11} 
                                   &                                 &                                 &                                 & GPT-based     & Human   &                                 &                                 &                                 & GPT-based    & Human   \\ \hline
mPLUG-Owl                 & 0.216                           & 42.06                           & 53.23                           & 6.57             & 2.92             & 0.506                           & 59.65                           & 69.68                           & 5.51            & 2.09             \\
PandaGPT                  & 0.231                           & 43.12                           & 56.72                           & 6.73             & 3.08             & 0.516                           & 60.47                           & 72.65                           & 5.68            & 2.68             \\
Valley                    & 0.252                           & 45.41                           & 57.93                           & 6.94             & 3.36             & 0.535                           & 63.36                           & 70.72                           & 6.07            & 2.46             \\
VideoChat                 & 0.243                           & 45.06                           & 58.16                           & 7.06             & 3.51             & 0.527                           & 63.79                           & 71.65                           & 6.29            & 2.95             \\
Video-ChatGPT             & {\ul 0.272}                     & {\ul 48.55}                     & 60.54                           & 7.39             & 3.89             & {\ul 0.558}                     & {\ul 65.53}                     & {\ul 75.37}                     & 6.75            & 3.42             \\
ChatUniVi                 & 0.254                           & 46.69                           & 59.33                           & 7.24             & 3.72             & 0.532                           & 63.57                           & 74.54                           & 6.87            & 3.15             \\
Video-LLaVA               & 0.266                           & 47.27                           & {\ul 61.40}                     & {\ul 7.95}       & {\ul 4.05}       & 0.547                           & 64.45                           & 74.12                           & {\ul 7.04}      & {\ul 3.38}       \\ \hline
\textbf{ICM}                       & \textbf{0.290}                  & \textbf{54.79}                  & \textbf{65.38}                  & \textbf{9.02}    & \textbf{4.95}    & \textbf{0.599}                  & \textbf{73.57}                  & \textbf{81.94}                  & \textbf{8.89}   & \textbf{4.74}    \\
\rowcolor{color1} w/o SBM                   & 0.275                           & 49.76                           & 63.44                           & 8.36             & 4.21             & 0.561                           & 68.07                           & 77.22                           & 7.51            & 3.77             \\
\rowcolor{color2} w/o IEC                & 0.280                           & 50.22                           & 63.62                           & 8.52             & 4.26             & 0.567                           & 69.65                           & 78.66                           & 7.87            & 3.95             \\
\rowcolor{color3} w/o scene tuning                & 0.252                           & 45.92                           & 60.53                           & 7.76             & 3.96             & 0.539                           & 63.79                           & 73.63                           & 6.86            & 3.19             \\ 
\bottomrule[1.2pt]
\end{tabular}
}
\end{center}
\end{table*}



\subsection{Baselines}
Due to the requirement of interaction and pre-processing videos, traditional VSU approaches are not directly suitable for our Omni-SILA task. Therefore, we choose several advanced Video-LLMs as baselines. \textbf{mPLUG-Owl}~\cite{mPLUG-Owl} equips LLMs with multimodal abilities via modular learning. \textbf{PandaGPT}~\cite{PandaGPT} shows impressive and emergent cross-modal capabilities across six modalities: image/video, text, audio, depth, thermal and inertial measurement units. \textbf{Valley}~\cite{Valley} introduces a simple projection to unify video, image and language modalities with LLMs. \textbf{VideoChat}~\cite{VideoChat} designs a VideoChat-Text module to convert video streams into text and a VideoChat-Embed module to encode videos into embeddings. \textbf{Video-ChatGPT}~\cite{Video-ChatGPT} combines the capabilities of LLMs with a pre-trained visual encoder optimized for spatio-temporal video representations. \textbf{Chat-UniVi}~\cite{Chat-UniVi} uses dynamic visual tokens to uniformly represent images and videos, and leverages multi-scale representations to capture both high-level semantic concepts and low-level visual details. \textbf{Video-LLaVA}~\cite{Video-LLaVA} aligns the representation of images and videos to a unified visual feature space, and uses a shared projection layer to map these unified visual representations to the LLMs.

\subsection{Implementation Details}
Since the above models target different tasks and employ different experimental settings, for a fair and thorough comparison, we re-implement these models and leverage their released codes to obtain experimental results on our Omni-SILA datasets. In our experiments, all the Video-LLMs size is 7B. The hyper-parameters of these baselines remain the same setting reported by their public papers. The others are tuned according to the best performance. For ICM approach, during the training period, we use AdamW as the optimizer, with an initial learning rate 2e-5 and a warmup ratio 0.03. We fine-tune Video-LLaVA (7B) using LoRA for both scene-tuning stage and Omni-SILA stage, and we set the dimension, scaling factor, dropout rate of the LoRA matrix to be 16, 64 and 0.05, while keeping other parameters at their default values. The parameters of MTCNN, HigherHRNet, RelTR and ViT are frozen during training stages. The number of experts in Causal MoE is 4, and the layers of each expert are set to be 8. The hyper-parameters $\alpha$ and $\beta$ of $\mathcal{L}_{rb}$ are set to be 1e-4 and 1e-2. ICM approach is trained for three epochs with a batch size of 8. All training runs on 1 NVIDIA A100 GPU with 40GB GPU memory. It takes around 18h for scene-tuning stage, 62h for training Omni-SILA stage and 16h for inference. 

\subsection{Evaluation Metrics}
To comprehensively evaluate the performance of various models on the Omni-SILA task, we use commonly used metrics and design additional task-specific ones. We categorized these evaluation metrics into three tasks, as described below.

\textbf{$\bullet$ Visual Sentiment Identifying (VSI)}. We leverage Accuracy (Acc) to evaluate the performance of VSI following ~\citet{confede}. Besides, we prioritize Recall over Precision and report F2-score~\cite{tsl}. 

\textbf{$\bullet$ Visual Sentiment Locating (VSL)}. Following prior studies~\cite{mAP-tIoU1,mAP-tIoU2}, we use mAP@IoU metric to evaluate VSL performance. This metric is calculated as the mean Average Precision (mAP) under different intersections over union (IoU) thresholds (0.1, 0.2 and 0.3). More importantly, we emphasize false-negative rates (FNRs)~\cite{FNR,Hawk}, denoted as: $\frac{number \: of \: false-negative \: frames}{number \: of \: positive \: frames}$, which refer to the rates of ``\emph{misclassifying a positive/normal frame as negative}''. FNRs indicate that it is preferable to classify all timestamps as negative than to miss any timestamp associated with negative sentiments, as this could lead to serious criminal events.   

\textbf{$\bullet$ Visual Sentiment Attributing (VSA)}. We design four specific metrics to comprehensively evaluate the accuracy and rationality of generated sentiment attributions. Specifically, semantic relevance (Sem-R) leverages the Rouge score to measure the relevance between generated attribution and true cause. Semantic consistency (Sem-C) leverages cosine similarity to assess the consistency of generated attribution and true cause. Sentiment accuracy (Sen-A) calculates the accuracy between generated attribution and true sentiment label. Attribution rationality (Atr-R) employs both automatic and human evaluations to assess the rationality of generated attributions. For automatic evaluation, we use ChatGPT~\cite{chatgpt} to score based on two criteria: sentiment overlap and sentiment clue overlap, on a scale of 1 to 10. For human evaluation, three annotators are recruited to rate the rationality of the generated attributions on a scale from 1 to 6, where 1 denotes ``\emph{completely wrong}'' and 6 denotes ``\emph{completely correct}''. After obtaining individual scores, we average all the scores to report as the final results. Moreover, $t$-test~\cite{ttest1,ttest2} is used to evaluate the significance of the performance.

