\begin{abstract}
  Prior studies on Visual Sentiment Understanding (VSU) primarily rely on the explicit scene information (e.g., facial expression) to judge visual sentiments, which largely ignore implicit scene information (e.g., human action, objection relation and visual background), while such information is critical for precisely discovering visual sentiments. Motivated by this, this paper proposes a new \textbf{Omni}-scene driven visual \textbf{S}entiment \textbf{I}dentifying, \textbf{L}ocating and \textbf{A}ttributing in videos (Omni-SILA) task, aiming to interactively and precisely identify, locate and attribute visual sentiments through both explicit and implicit scene information. Furthermore, this paper believes that this Omni-SILA task faces two key challenges: modeling scene and highlighting implicit scene beyond explicit. To this end, this paper proposes an \textbf{I}mplicit-enhanced \textbf{C}ausal \textbf{M}oE (ICM) approach for addressing the Omni-SILA task. Specifically, a \textbf{S}cene-\textbf{B}alanced \textbf{M}oE (SBM) and an \textbf{I}mplicit-\textbf{E}nhanced \textbf{C}ausal (IEC) blocks are tailored to model scene information and highlight the implicit scene information beyond explicit, respectively. Extensive experimental results on our constructed explicit and implicit Omni-SILA datasets demonstrate the great advantage of the proposed ICM approach over advanced Video-LLMs. 
\end{abstract}