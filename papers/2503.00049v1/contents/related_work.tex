\section{Related Work}
\label{sec:vsu}
\textbf{$\bullet$ Visual Sentiment Understanding.} Previous studies on Visual Sentiment Understanding (VSU) utilize multiple affective information to predict the overall sentiment of images~\cite{image,Emoset} or videos~\cite{mosei,sims}. For image, traditional studies extract sentiment features to analyze sentiments~\cite{image3,image2,mdan}, while recent studies fine-tune LLMs via instructions to predict sentiments~\cite{EmoVIT}. For videos, traditional studies require pre-processing video features and predict sentiments by elaborating fusion strategies~\cite{tfn,mult,routing,mmim} or learning representations~\cite{misa,selfmm,unimse,confede}. To achieve end-to-end goal, some studies~\cite{tsl,tsg,VadCLIP} input the entire videos, and explore the location of segments that convey different sentiments or anomalies. Recently, a few studies gradually explore the causes of anomalies~\cite{cuva} and sentiments~\cite{AffectGPT} via Video-LLMs. However, these efforts have not addressed visual sentiment identification, location and attribution of videos at the same time. Different from all the above studies, this paper proposes a new Omni-SILA task to interactively answer what, when and why are the visual sentiment through omni-scene information, aiming to precisely identify and locate, as well as reasonably attribute visual sentiments in videos.   


\begin{figure*}
    \centering
    \includegraphics[width=\textwidth, scale=0.27]{images/model.pdf}
    \setlength{\abovecaptionskip}{-2 ex}
    \setlength{\belowcaptionskip}{-2 ex}
    \caption{The overall architecture of our ICM approach, consisting of a Scene-Enriched Modeling (SEM) block and an Implicit-enhanced Causal MoE framework, which comprises a Scene-Balanced MoE (SBM) block (right, see Section~\ref{sec:sbm}) and an Implicit-Enhanced Causal (IEC) block (left, see Section~\ref{sec:iec}), where (a) and (b) are causal graphs for IEC block. FEE, HAE, ORE and VBE represent Facial Expression Expert, Human Action Expert, Object Relation Expert and Visual Background Expert.}
    \label{fig:model}
\end{figure*}

\noindent\textbf{$\bullet$ Video-centred Large Language Models.} Recently, large language models (LLMs)~\cite{chatgpt}, such as LLaMA~\cite{llama2} and Vicuna~\cite{vicuna2023}, have shown remarkable abilities. Given the multimodal nature of the world, some studies~\cite{blip2, llava, minigpt4, minigptv2} have explored using LLMs to enhance visual understanding. Building on these, Video-LLMs have extended into the more sophisticated video area. According to the role of LLMs, Video-LLMs can be broadly categorized into three types. \textbf{(1)} LLMs as text decoders means LLMs decode embeddings from the video encoder into text outputs, including Video-ChatGPT~\cite{Video-ChatGPT}, Video-LLaMA~\cite{Video-LLaMA}, Valley~\cite{Valley}, Otter~\cite{Otter}, mPLUG-Owl~\cite{mPLUG-Owl}, Video-LLaVA~\cite{Video-LLaVA}, Chat-UniVi~\cite{Chat-UniVi}, VideoChat~\cite{VideoChat} and MiniGPT4-Video~\cite{MiniGPT4-Video}. \textbf{(2)} LLMs as regressors means LLMs can predict continuous values, including TimeChat~\cite{TimeChat}, GroundingGPT~\cite{GroundingGPT}, HawkEye~\cite{HawkEye} and Holmes-VAD~\cite{Holmes-VAD}. \textbf{(3)} LLMs as hidden layers means LLMs connect to a designed task-specific head to perform tasks, including OneLLM~\cite{OneLLM}, VITRON~\cite{Vitron} and GPT4Video~\cite{GPT4Video}. Although the aforementioned Video-LLMs studies make significant progress in video understanding, they remain limitations in their ability to perceive omni-scene information and are unable to analyze harmful video content. Therefore, this paper proposes the ICM approach, aiming to evoke the omni-scene perception capabilities of Video-LLMs and highlight the implicit scenes beyond the explicit.
