\section{Approach}
In this section, we formulate our Omni-SILA task as follows. Given a video $v$ consisting of $N$ segments, each segment $n$ is labeled with a time $t$, visual sentiment $s$ and cause $c$. The goal of Omni-SILA is to interactively identify, locate and attribute the visual sentiment within $v$. Thus, the model generates a set of segments $\{(t_1, s_1, c_1),...,(t_i, s_i, c_i),...,(t_n, s_n, c_n)\}$, where $t_i$, $s_i$ and $c_i$ denote the time, visual sentiment and cause for each video segment $v_i$. In this paper, we propose an \textbf{I}mplicit-enhanced \textbf{C}ausal \textbf{M}oE (ICM) approach to address the Omni-SILA task, which involves two challenges: modeling scene and highlighting implicit scene beyond explicit. To address these challenges, we design \textbf{S}cene-\textbf{B}alanced \textbf{M}oE (SBM) and \textbf{I}mplicit-\textbf{E}nhanced \textbf{C}ausal (IEC) blocks. Particularly, we choose the open-sourced Video-LLaVA~\cite{Video-LLaVA} as the backbone, which achieves state-of-the-art performance on most video understanding benchmarks. The overall framework is shown in Figure \ref{fig:model}. 

\subsection{Scene-Enriched Modeling Block}
\label{sec:see}
Given a set of video segments $v=[v_1, ..., v_i, ..., v_n]$, we leverage four blocks to model omni-scene information as shown in Figure \ref{fig:model}. 
\textbf{Facial Expression Modeling} is used to model explicit facial expression by MTCNN~\cite{MTCNN}, which is a widely-used network to detect faces and key point locations in each video segments $v_i$, and them employ CNN to obtain the facial expression representation $x_{\rm f}$.
\textbf{Human Action Modeling} is used to model implicit human action by HigherHRNet~\cite{HigherHRNet}, which is a well-studied network to detect the location of action key points or parts (e.g., elbow, wrist, etc) in each video segment $v_i$, and encode action heatmaps to obtain the human action representation $x_{\rm a}$. 
\textbf{Object Relation Modeling} is used to model implicit object relations from each video segment $v_i$ by RelTR~\cite{RelTR}, which is a well-studied model to generate the relations between subjects and objects, and extract the visual feature context and entity representations to obtain object relations representation $x_{\rm o}$.
\textbf{Visual Background Modeling} is used to model implicit visual backgrounds from each video segment $v_i$ by ViT~\cite{vit} and SAM-V1~\cite{sam}, which are two advanced visual encoding and segmenting tools to segment the visual backgrounds with pure black to fill out the masked parts, and transform them into ViT to obtain the final visual background representation $x_{\rm b}$.

\subsection{Scene-Balanced MoE Block}
\label{sec:sbm}
In this study, we design a \textbf{S}cene-\textbf{B}alanced \textbf{M}oE (SBM) block to model scene information. Specifically, we address two crucial questions: (1) how to model different types of scene information; (2) how to balance the contributions of different scene information for the Omni-SILA task. We will provide comprehensive answers to these two questions in the subsequent section.

\textbf{Scene Experts} are introduced to answer question (1), which model both explicit and implicit scene information inspired by \citet{OneLLM}, consisting of Facial Expression Expert (FEE), Human Action Expert (HAE), Object Relation Expert (ORE) and Visual Background Expert (VBE) four scene experts. Each scene expert is a stack of transformer layers, aiming to dynamically learn different scene information. As shown in Figure \ref{fig:model},  our four scene experts operate externally to the LLM, enabling effective alignment of various scene information. Formally, for the representations $x_i, i \in \{{\rm f},{\rm a},{\rm o},{\rm b}\}$ of the four scene modeling blocks, the output representation $h_i$ of each expert $\text{Expert}_i$ can be denotes as: $h_i=\text{Expert}_i(x_i)$, where $\text{Expert}_i$ represents the general term of FEE ($\rm f$), HAE ($\rm a$), ORE ($\rm o$) and VBE ($\rm b$) four scene experts.

\textbf{Balanced MoE} is leveraged to answer question (2), which balances different scene information contributions, managed by a dynamic scene router \textit{R} as shown in Figure \ref{fig:model}. Balanced MoE is structured as a straightforward MLP that processes input features $h$ of four scene experts and computes routing weights for each expert, effectively functioning as a soft router~\cite{Soft-MoE}. Formally, the output $y_{\rm moe}$ of the balanced MoE can be denoted as follows: 
\begin{equation}
\label{eq: moe_out}
y_{\rm moe} = \textrm{LayerNorm} (\sum\nolimits^{L}\nolimits_{j=1}{g_j(h)}\times{E_j(h)})
\end{equation}
where $g_j(h)$ and $E_j(h)$ denote the corresponding weight and the output of the $j$-th scene expert, and $L$ is the number of scene experts.

To obtain $g(h)$, we computer the gating probability $P$ of each scene expert for input $h$, formulated as: $g(h) = \bf{P} = \textrm{softmax}(\bf{W} \cdot h)$, where ${\bf{W}} \in {\mathbb{R}} ^ {L \times d}$ is a learnable parameter for scene router \textit{R}, and $d$ is the hidden dimension of each expert. $\bf{P}$ is a vector size $L$ and $\bf{P}_j$ denotes the probability of the $j$-th scene expert $E_j$ to process $h$.

Furthermore, to optimize the scene router \textit{R}, we design a router loss with balancing constraints $\mathcal{L}_{\rm rb}$, encouraging \textit{R} to dynamically adjust the contributions of all scene experts, formulated as:
\begin{equation}
    \label{eq: loss_rb}
    \mathcal{L}_{\rm rb} = -\alpha \cdot \sum\nolimits^{L}\nolimits_{j=1}{\bf{P}}_j \ast \log ({\bf{P}}_j) + \beta \cdot L \cdot \sum\nolimits^{L}\nolimits_{j=1} {\bf{G}}_j \ast {\bf{H}}_j  
\end{equation}
The first term with the hyper-parameter $\alpha$ measures the contribution of various scene information, encouraging the scene router \textit{R} to assign a different weight to each scene expert within the constraints of $\bf{P}$, thereby preventing \textit{R} from uniformly assigning the same weight and leading to wrong visual sentiment judgment. We expect the routing mechanism to select the scene experts that are more important for the Omni-SILA task, thus we minimize the entropy of the gating probability distribution $\bf{P}$ to ensure that each input feature $h_i$ could be assigned the appropriate weight coefficient. The second term with the hyper-parameter $\beta$ balances scene experts of different sizes (since the output dimension $d$ of four scene modeling blocks are different), forcing the model not to pay too much attention to scene experts with high dimensions, while ignoring scene experts with low dimensions during the learning process. ${\bf{G}}_j=\frac{1}{L}\sum^{L}_{j=1}\mathrm{1}\{{e_j \in E_j\} \times d}$ represents the average dimension of the hidden state of the scene expert $e_j$ on the entire input $h$, which imports the influence of scene expert sizes $d$ when the model focuses more on large scene experts, the loss rises, which direct the model to more economically utilize smaller scene experts. ${\bf{H}}_j=\frac{1}{L}\sum^{L}_{j=1}\bf{P}_j$ represents the gating probability assigned to $e_j$.

\subsection{Implicit-Enhanced Causal Block}
\label{sec:iec}
In this study, we take advantage of the causal intervention technique~\cite{pearl} and design an \textbf{I}mplicit-\textbf{E}nhanced \textbf{C}ausal (IEC) block to highlight implicit scene beyond explicit. Specifically, there are also two crucial questions to be answered: (1) how to highlight implicit scene information through the front-door adjustment strategy~\cite{pearl}; (2) how to implement this front-door adjustment strategy in the Omni-SILA task. Next, we will answer the two questions.
% , formulated as follows

\textbf{Causal Intervention Graph} is introduced to answer question (1), which formulates the causal relations among the scene information $\rm X$, the fusion scene features $\rm M$, visual sentiment outputs $\rm Y$, and confounding factors $\rm C$ as shown in Figure \ref{fig:model} (a). In this graph, $\rm X \rightarrow \rm M \rightarrow \rm Y$ represents the desired causal effect from the scene information $\rm X$ to visual sentiment outputs $\rm Y$, with the fusion scene features $\rm M$ serving as a mediator. $\rm X \leftarrow \rm C \rightarrow \rm M$ represents the causal effect of the invisible confounding factors $\rm C$ on both scene information $\rm X$ and fusion scene features $\rm M$.

To highlight implicit scene information, we consider mitigating the modeling bias between $\rm X$ and $\rm C$ present in the path $\rm M \rightarrow \rm Y$, thus we leverage \emph{do}-operator~\cite{pearl} to block the back-door path $\rm X \leftarrow \rm C \rightarrow \rm M \rightarrow \rm Y$ through conditioning on $\rm X$ as shown in Figure \ref{fig:model} (b). Then, we utilize the front-door adjustment strategy to analyze the causal effect of $\rm X \rightarrow \rm Y$, denoted as: $P({\rm Y}=y|do({\rm X}=x))=\sum_{m}P(m|x)\sum_{x}P(x)[P(y|x,m)]$.

\textbf{Deconfounded Causal Attention} is leveraged to answer question (2), which implements the front-door adjustment strategy through the utilization of attention mechanisms. Given the expensive computational cost of network forward propagation, we use the Normalized Weighted Geometric Mean (NWGM)~\cite{nwgm1,nwgm2} approximation. Therefore, we sample $\rm X$, $\rm M$ and compute $P({\rm Y}|do({\rm X}))$ through feeding them into the network, and then leverage NWGM approximation to achieve the goal of deconfounding scene biases, represented as: $P({\rm Y}|do({\rm X})) \approx {\textrm{softmax}}[f(y_x, y_m)]$, where $f(.)$ followed by a \textrm{softmax} layer is a network, which is used to parameterize the predictive distribution $P(y|x,m)$. In addition, $y_m=\sum_{m}P({\rm M}=m|p({\rm X})){\bm m}$ and $y_x=\sum_{x}P({\rm X}=x|q({\rm X})){\bm x}$ estimate the self-sampling and cross-sampling, where the variables $m,x$ correspond to the embedding vectors of $\bm {m,x}$. $p(.)$ and $q(.)$ are query embedding functions parameterized as networks. Therefore, we utilize the attention mechanism to estimate the self-sampling $y_m$ and cross-sampling $y_x$ as shown in Figure \ref{fig:model}:
\begin{equation}
\label{equ:self}
    y_m=\bm{{\rm V}\!_M} \cdot {\rm softmax} (\bm{{\rm Q}_{M}}^\top \bm{{\rm K}_M})
\end{equation}
where Eq.(\ref{equ:self}) denotes self-sampling attention to compute intrinsic effect of fusion scene features $\rm M$ and confounding factors $\rm C$. 
\begin{equation}
\label{equ:cross}
    y_x=\bm{{\rm V}\!_C} \cdot {\rm softmax} (\bm{{\rm Q}_{C}}^\top \bm{{\rm K}_C})
\end{equation}
where Eq.(\ref{equ:cross}) represents the cross-sampling attention to compute the mutual effect between the fusion scene features $\rm M$ and confounding factors $\rm C$. In the implementation of two equations, $\bm{{\rm Q}_M}$ and $\bm{{\rm Q}_C}$ are derived from $p({\rm X})$ and $q({\rm X})$. $\bm{{\rm K}_M}$ and $\bm{{\rm V}\!_M}$ are obtained from the current input sample, while $\bm{{\rm K}_C}$ and $\bm{{\rm V}\!_C}$ come from other samples in the training set, serving as the global dictionary compressed from the whole training dataset. Specifically, we initialize this dictionary by using K-means clustering~\cite{k-means} on all the embeddings of samples in the training set. To obtain the final output $y_{iec}$ of the IEC block, we employ an FFN to integrate the self-sampling estimation $y_m$ ans cross-sampling estimation $y_x$, formulated as: $y_{iec}=\textrm{FFN}(y_m + y_x)$. 

\setlength{\tabcolsep}{1.8pt}
\begin{table*}[]
\renewcommand{\arraystretch}{0.9}
\addtolength{\tabcolsep}{3pt}
\begin{center}
\setlength{\abovecaptionskip}{-0 ex}
\setlength{\belowcaptionskip}{-0.5 ex}
\caption{Comparison of several Video-LLMs and our ICM approach on Explicit and Implicit Omni-SILA dataset for identifying and locating sentiments. The $\downarrow$ beside FNRs indicates the lower the metric, the better the performance. Bold and underlined indicate the highest and second-highest performance, respectively (the same below).}
% Avg denotes the mean values of metric at IoU threshold 0.1, 0.2, 0.3.
\label{tab:main_results}
\resizebox{\linewidth}{!}{
    \begin{tabular}{c|ccccccc|ccccccc}
\toprule[1.2pt]
\multirow{3}{*}{Approach} & \multicolumn{7}{c|}{Explicit Omni-SILA Dataset}                                                                                                                                            & \multicolumn{7}{c}{Implicit Omni-SILA Dataset}                                                                                                                                            \\ \cline{2-15} 
                                   & \multirow{2}{*}{Acc} & \multicolumn{1}{c|}{\multirow{2}{*}{F2}} & \multirow{2}{*}{FNRs$\downarrow$} & \multicolumn{4}{c|}{mAP@IoU}                             & \multirow{2}{*}{Acc} & \multicolumn{1}{c|}{\multirow{2}{*}{F2}} & \multirow{2}{*}{FNRs$\downarrow$} & \multicolumn{4}{c}{mAP@IoU}                              \\ \cline{6-7} \cline{13-14}
                                   &                               & \multicolumn{1}{c|}{}                             &                                & 0.1   & 0.2   & 0.3   & Avg   &                               & \multicolumn{1}{c|}{}                             &                                & 0.1   & 0.2   & 0.3   & Avg   \\ \hline
mPLUG-Owl                 & 60.33                         & \multicolumn{1}{c|}{59.57}                        & 71.37                          & 30.30          & 12.20          & 3.36           & 15.29          & 28.88                         & \multicolumn{1}{c|}{30.06}                        & 73.98                          & 31.42          & 13.21          & 4.46           & 16.36          \\
PandaGPT                  & 64.22                         & \multicolumn{1}{c|}{64.12}                        & 49.12                          & 28.28          & 17.17          & 7.98           & 17.81          & 32.48                         & \multicolumn{1}{c|}{33.87}                        & 49.62                          & 29.36          & 18.28          & 8.87           & 18.83          \\
Valley                    & 65.75                         & \multicolumn{1}{c|}{65.01}                        & 56.07                          & 31.35          & 15.15          & 6.76           & 17.75          & 34.66                         & \multicolumn{1}{c|}{35.94}                        & 53.49                          & 32.24          & 16.26          & 7.66           & 18.75          \\
VideoChat                 & 66.57                         & \multicolumn{1}{c|}{65.80}                        & 44.50                          & 30.93          & {\ul 20.62}    & 8.25           & {\ul 22.63}    & 35.12                         & \multicolumn{1}{c|}{36.44}                        & 50.79                          & 31.96          & {\ul 21.73}    & 9.26           & {\ul 20.98}    \\
Video-ChatGPT             & 67.88                         & \multicolumn{1}{c|}{66.84}                        & 61.26                          & 25.56          & 18.89          & {\ul 10.00}    & 18.15          & 37.82                         & \multicolumn{1}{c|}{39.31}                        & 61.47                          & 26.65          & 19.91          & {\ul 11.03}    & 19.19          \\
ChatUniVi                 & 67.23                         & \multicolumn{1}{c|}{66.57}                        & 61.81                          & 18.82          & 10.61          & 9.05           & 12.83          & 37.95                         & \multicolumn{1}{c|}{38.88}                        & 62.52                          & 19.89          & 11.62          & 10.02          & 13.84          \\
Video-LLaVA               & {\ul 68.19}                   & \multicolumn{1}{c|}{{\ul 67.08}}                  & {\ul 44.32}                    & {\ul 31.41}    & 15.78          & 8.82           & 18.67          & {\ul 40.02}                   & \multicolumn{1}{c|}{{\ul 41.88}}                  & {\ul 50.34}                    & {\ul 32.41}    & 16.79          & 9.92           & 19.71          \\ \hline
\textbf{ICM}                       & \textbf{71.41}                & \multicolumn{1}{c|}{\textbf{70.21}}               & \textbf{33.38}                 & \textbf{31.91} & \textbf{23.39} & \textbf{18.75} & \textbf{25.21} & \textbf{47.39}                & \multicolumn{1}{c|}{\textbf{48.36}}               & \textbf{32.76}                 & \textbf{34.79} & \textbf{26.14} & \textbf{19.08} & \textbf{27.88} \\
\rowcolor{color1} w/o SBM                   & 69.32                         & \multicolumn{1}{c|}{68.36}                        & 37.92                          & 30.33          & 22.23          & 15.59          & 22.72          & 43.18                         & \multicolumn{1}{c|}{44.52}                        & 40.11                          & 32.44          & 23.49          & 15.65          & 23.68          \\
\rowcolor{color2} w/o IEC                & 69.71                         & \multicolumn{1}{c|}{68.82}                        & 35.85                          & 31.27          & 23.23          & 16.53          & 23.68          & 44.12                         & \multicolumn{1}{c|}{45.08}                        & 38.62                          & 33.18          & 24.20          & 16.23          & 24.54          \\
\rowcolor{color3} w/o scene-tuning                & 67.87                         & \multicolumn{1}{c|}{66.32}                        & 43.59                          & 26.80          & 18.51          & 12.05          & 19.12          & 39.64                         & \multicolumn{1}{c|}{40.76}                        & 49.74                          & 27.88          & 19.65          & 13.14          & 20.23          \\ 
\bottomrule[1.2pt]
\end{tabular}
}
\end{center}
\end{table*}

\subsection{Two-Stage Training Optimization}
\label{sec:two-stage}
Due to the lack of scene perception abilities in Video-LLaVA, we design a two-stage training process, where scene-tuning stage is pre-tuned to perceive omni-scene information, while Omni-SILA tuning stage is trained to address the Omni-SILA task better via the perception abilities of scene information, detailed as follows. 

For \textbf{Scene-Tuning} stage, we utilize four manually annotated instruction datasets (detailed in Section~\ref{sec:datasets}) to pre-tune Video-LLaVA, aiming to evoke the scene perception abilities of Video-LLaVA, where the model is asked to ``\emph{Please describe the facial/action/image region/background}''.
For \textbf{Omni-SILA Tuning} stage, we meticulously construct an Omni-SILA dataset (detailed in Section~\ref{sec:datasets}) to make our ICM approach better tackling the Omni-SILA task through instruction tuning, where the ICM approach is asked through the instruction ``\emph{Please identify and locate the implicit visual sentiment, and attribute it}'' as shown in Figure~\ref{fig:model}. Note that the instruction will be text tokenized inside Video-LLaVA to obtain the textual token $y_t$, which is added with the normalized combination of the SBM block output $y_{\rm moe}$ and IEC block output $y_{\rm iec}$. Thus, the input of the LLM inside Video-LLaVA will be ``$\textrm{Norm}(y_{\rm moe} + y_{\rm iec}) + y_t + y_v$'', where $y_v$ denotes the visual features encoded by intrinsic visual encoder LanguageBind~\cite{LanguageBind} of Video-LLaVA. Moreover, the whole loss of our ICM approach can be represented as $\mathcal{L}=\mathcal{L}_{\rm lm}+\mathcal{L}_{\rm rb}$, where $\mathcal{L}_{\rm lm}$ is the original language modeling loss of the LLM. 



