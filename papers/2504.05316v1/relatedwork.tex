\section{Related Work}
\subsection{Vision Language Models}
\noindent In recent years, significant progress has been made in Vision Language Models(VLMs)\cite{clip,align,albef,blip,lin2024mope,lin2024duquant,doge,MM23,TMM23}. Typical models like CLIP~\cite{clip} and ALIGN \cite{align} achieve cross-modal understanding by leveraging contrastive learning on large-scale image and text pairs. Li \etal~\cite{albef} introduce image-text matching and masked language modeling (MLM) tasks during training to enhance fine-grained matching. BLIP~\cite{blip}  equip the pre-trained models with text generation capabilities by language modeling (LM). 
With a similar spirit, some recent works further fine-tune the cross-modality model for different downstream tasks, such as text-based person retrieval~\cite{yang2023towards} and drone localization~\cite{chu2025towards}.
The emergence of various Large Language Models (LLMs)~\cite{gpt4,llama,chain,vicuna} has also influenced the development of visual language models, as they possess vast knowledge and powerful text generation capabilities. 
LLaVa~\cite{llava} directly maps visual features to LLMs and aligns spaces through fintuning. 
BLIP2~\cite{blip2} establishes a bridge between vision language base models and various open-source LLMs by deploying a Q-Former on filtered data. 
InstructBLIP~\cite{instructblip} further improves performance using instruction tuning and exhibits enhanced text generation capabilities while reducing training costs through LLM freezing. 
We deploy instruction tuning to composed image retrieval and make it possible to generate modifiers using related images.

\subsection{Composed Image Retrieval}

\noindent Image retrieval is an important research task in multi-modal field. It aims to retrieve target images from a gallery based on a given query. This can be done by solely using text descriptions or by using images~\cite{deepfashion,sketch} to retrieve similar or related images. However, single-modal tasks such as text-based image retrieval or image-based image retrieval cannot accurately and conveniently meet the specific retrieval needs of certain scenarios. To address this issue, the Composed Image Retrieval task has been proposed\cite{cirr,tirg,Comprehensive,fashioniq}, which involves integrating the reference image feature and supplementing or modifying the textual feature to retrieve the target image.
There have been efforts in training lightweight connection layers to obtain fused features from image and text representations. ARTEMIS~\cite{atermis} combines triplets through explicit matching and implicit similarity, and Baldrati \etal~\cite{clip4cir2} proposes a combiner to leverage CLIP visual and textual representations. Liu \etal~\cite{rerank} proposes a re-ranking method after the first selection. In very recent works, {many existing works\cite{circo,gu2024lincir,pic2word,vaze2023gen,magiclens} leverage large amounts of external data to achieve zero-shot CIR capabilities. MagicLens\cite{magiclens} achieves strong performance in zero-shot CIR while also making progress in richer relations beyond image similarity.} SPRC~\cite{sprc} utilizes Q-Former to extract sentence-level prompts and guides sentence-level prompt generation aligned with an auxiliary text prompt. In addition, there have been works that enhance task performance by introducing additional datasets for pre-training~\cite{covr,dataroaming,compodiff}. These datasets provide extra training examples and diverse data distributions, allowing the models to learn more comprehensive and robust representations. 
In our work, we design a prototypical two-hop alignment network to decompose CIR into an implicit prototype learning module and fusion module. In the implicit prototype learning module, we utilize generated reversed modifiers to benefit implicit prototype learning.
\subsection{Composed Image Retrieval Triplet Generation}
\noindent
In previous works, CIR triplet generation has been primarily achieved through manual and automatic methods: 
\textbf{Manual Annotated.}
The CIRR dataset~\cite{cirr} consists of manually annotated textual triplets, which are derived from a subset of images from the NLVR2 dataset~\cite{nlvr2}, representing real-world scenarios. The FashionIQ dataset~\cite{fashioniq} comprises manually selected pairs of similar fashion images, along with human-annotated textual triplets, specifically curated for the fashion domain. 
\textbf{Automatic Annotated.}
Han \etal~\cite{fashion200k} employed the differences in manually annotated attribute labels of fashion200k dataset images to generate modified texts using a triplet template. In recent years, there have been proposed methods that leverage automatic generation techniques from other tasks and models. LaSCo~\cite{covr} utilized VQA 2.0~\cite{vqa2} to construct triplets by using different answers for similar images and the same question, employing GPT 3.0~\cite{gpt3}, followed by manual quality control. CompoDiff~\cite{compodiff} built triplets based on {InstructPix2Pix}\cite{instuctp2p}, using text descriptions collected from both human annotators and large-scale model generation and generating images using Stable Diffusion\cite{stablediffusion}. CoVR~\cite{covr} employed similar video captions to filter similar image pairs and trained an MTG-LLM to generate a modifier using two similar captions, forming triplets.
Compared to existing methods, we incorporate images into the training of modifier generation and map visual features into the space of a large model. We propose a lightweight text generation method that is more flexible, diverse, and controllable in length while maintaining low training costs.