@inproceedings{Comprehensive,
author = {Zhang, Feifei and Yan, Ming and Zhang, Ji and Xu, Changsheng},
title = {Comprehensive Relationship Reasoning for Composed Query Based Image Retrieval},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3548126},
doi = {10.1145/3503161.3548126},
abstract = {Composed Query Based Image Retrieval (CQBIR) aims at searching images relevant to a composed query, i.e., a reference image together with a modifier text. Compared with conventional image retrieval, which takes a single image or text to retrieve desired images, CQBIR encounters more challenges as it requires not only effective semantic correspondence between the heterogeneous query and target, but also synergistic understanding of the composed query. To establish robust CQBIR model, four critical types of relational information can be included, i.e., cross-modal, intra-sample, inter-sample, and cross-sample relationships. Pioneer studies mainly exploit parts of the information, which are hard to make them enhance and complement each other. In this paper, we propose a comprehensive relationship reasoning network by fully exploring the four types of information for CQBIR, which mainly includes two key designs. First, we introduce a memory-augmented cross-modal attention module, in which the representation of the composed query is augmented by considering the cross-modal relationship between the reference image and the modification text. Second, we design a multi-scale matching strategy to optimize our network, aiming at harnessing information from the intra-sample, inter-sample, and cross-sample relationships. To the best of our knowledge, this is the first work to fully explore the four pieces of relationships in a unified deep model for CQBIR. Comprehensive experimental results on five standard benchmarks demonstrate that the proposed method performs favorably against state-of-the-art models.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {4655–4664},
numpages = {10},
keywords = {composed query based image retrieval, memory, multi-scale matching, relational information},
location = {<conf-loc>, <city>Lisboa</city>, <country>Portugal</country>, </conf-loc>},
series = {MM '22}
}

@inproceedings{MM23,
  author       = {Zhenyang Li and
                  Yangyang Guo and
                  Kejie Wang and
                  Xiaolin Chen and
                  Liqiang Nie and
                  Mohan S. Kankanhalli},
  title        = {Do Vision-Language Transformers Exhibit Visual Commonsense? An Empirical Study of {VCR}},
  booktitle    = {Proceedings of the 31st {ACM} International Conference on Multimedia},
  pages        = {5634--5644},
  publisher    = {{ACM}},
  year         = {2023}
}

@article{TMM23,
  author       = {Zhenyang Li and
                  Yangyang Guo and
                  Kejie Wang and
                  Fan Liu and
                  Liqiang Nie and
                  Mohan S. Kankanhalli},
  title        = {Learning to Agree on Vision Attention for Visual Commonsense Reasoning},
  journal      = {{IEEE} Transactions on Multimedia},
  volume       = {26},
  pages        = {1065--1075},
  year         = {2024}
}

@inproceedings{albef,
      title={Align before Fuse: Vision and Language Representation Learning with Momentum Distillation}, 
      author={Junnan Li and Ramprasaath R. Selvaraju and Akhilesh Deepak Gotmare and Shafiq Joty and Caiming Xiong and Steven Hoi},
      year={2021},
      booktitle={NeurIPS},
}

@inproceedings{align,
  title={Scaling up visual and vision-language representation learning with noisy text supervision},
  author={Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc and Sung, Yun-Hsuan and Li, Zhen and Duerig, Tom},
  booktitle={International conference on machine learning},
  pages={4904--4916},
  year={2021},
  organization={PMLR}
}

@inproceedings{atermis,
  title={ARTEMIS: Attention-based Retrieval with Text-Explicit Matching and Implicit Similarity},
  author={Delmas, Ginger and Rezende, Rafael S and Csurka, Gabriela and Larlus, Diane},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@InProceedings{blip,
  title = 	 {{BLIP}: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},
  author =       {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {12888--12900},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/li22n/li22n.pdf},
  url = 	 {https://proceedings.mlr.press/v162/li22n.html}

@inproceedings{blip2,
  title={BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models},
  author={Junnan Li and Dongxu Li and Silvio Savarese and Steven C. H. Hoi},
  booktitle={International Conference on Machine Learning},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:256390509}
}

@article{chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@inproceedings{chu2025towards,
  title={Towards natural language-guided drones: GeoText-1652 benchmark with spatial relation matching},
  author={Chu, Meng and Zheng, Zhedong and Ji, Wei and Wang, Tingyu and Chua, Tat-Seng},
  booktitle={European Conference on Computer Vision},
  pages={213--231},
  year={2025},
  organization={Springer}
}

@misc{circo,
      title={Zero-Shot Composed Image Retrieval with Textual Inversion}, 
      author={Alberto Baldrati and Lorenzo Agnolucci and Marco Bertini and Alberto Del Bimbo},
      year={2023},
      eprint={2303.15247},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@INPROCEEDINGS{cirr,
  author={Liu, Zheyuan and Rodriguez-Opazo, Cristian and Teney, Damien and Gould, Stephen},
  booktitle={2021 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Image Retrieval on Real-life Images with Pre-trained Vision-and-Language Models}, 
  year={2021},
  volume={},
  number={},
  pages={2105-2114},
  keywords={Visualization;Computer vision;Limiting;Codes;Image retrieval;Natural languages;Computer architecture;Vision + language;Datasets and evaluation;Image and video retrieval;Representation learning},
  doi={10.1109/ICCV48922.2021.00213}}

@article{clip,  
 title={Learning Transferable Visual Models From Natural Language Supervision}, 
 journal={Cornell University - arXiv,Cornell University - arXiv}, 
 author={Radford, Alec and Kim, JongWook and Hallacy, Chris and Ramesh, A. and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Amanda, Askell and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya}, 
 year={2021}, 
 month={Feb}, 
 language={en-US} 
 }

@article{clip4cir2,
  title={Composed Image Retrieval using Contrastive Learning and Task-oriented CLIP-based Features},
  author={Baldrati, Alberto and Bertini, Marco and Uricchio, Tiberio and Bimbo, Alberto Del},
  journal={ACM Transactions on Multimedia Computing, Communications and Applications},
  publisher={ACM New York, NY}
}

@article{compodiff,
    title={CompoDiff: Versatile Composed Image Retrieval With Latent Diffusion},
    author={Geonmo Gu and Sanghyuk Chun and Wonjae Kim and HeeJae Jun and Yoohoon Kang and Sangdoo Yun},
    journal={Transactions on Machine Learning Research},
    issn={2835-8856},
    year={2024},
    url={https://openreview.net/forum?id=mKtlzW0bWc},
    note={Expert Certification}
}

@article{covr,
   title={CoVR: Learning Composed Video Retrieval from Web Video Captions},
   volume={38},
   ISSN={2159-5399},
   url={http://dx.doi.org/10.1609/aaai.v38i6.28334},
   DOI={10.1609/aaai.v38i6.28334},
   number={6},
   journal={Proceedings of the AAAI Conference on Artificial Intelligence},
   publisher={Association for the Advancement of Artificial Intelligence (AAAI)},
   author={Ventura, Lucas and Yang, Antoine and Schmid, Cordelia and Varol, Gül},
   year={2024},
   month=mar, pages={5270–5279} }

@article{dataroaming,
 title={Data Roaming and Quality Assessment for Composed Image Retrieval},
  volume={38}, url={https://ojs.aaai.org/index.php/AAAI/article/view/28081},
  DOI={10.1609/aaai.v38i4.28081},
  abstractNote={The task of Composed Image Retrieval (CoIR) involves queries that combine image and text modalities, allowing users to express their intent more effectively. However, current CoIR datasets are orders of magnitude smaller compared to other vision and language (V&amp;L) datasets. Additionally, some of these datasets have noticeable issues, such as queries containing redundant modalities. To address these shortcomings, we introduce the Large Scale Composed Image Retrieval (LaSCo) dataset, a new CoIR dataset which is ten times larger than existing ones. Pre-training on our LaSCo, shows a noteworthy improvement in performance, even in zero-shot. Furthermore, we propose a new approach for analyzing CoIR datasets and methods, which detects modality redundancy or necessity, in queries.
    We also introduce a new CoIR baseline, the Cross-Attention driven Shift Encoder (CASE). This baseline allows for early fusion of modalities using a cross-attention module and employs an additional auxiliary task during training. Our experiments demonstrate that this new baseline outperforms the current state-of-the-art methods on established benchmarks like FashionIQ and CIRR.},
  number={4},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence},
  author={Levy, Matan and Ben-Ari, Rami and Darshan, Nir and Lischinski, Dani},
  year={2024},
  month={Mar.},
  pages={2991-2999}
}

@inproceedings{deepfashion,
title={DeepFashion: Powering Robust Clothes Recognition and Retrieval with Rich Annotations},
url={http://dx.doi.org/10.1109/cvpr.2016.124},  
DOI={10.1109/CVPR.2016.124},  
booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},  
author={Liu, Ziwei and Luo, Ping and Qiu, Shi and Wang, Xiaogang and Tang, Xiaoou},  
year={2016},  
month={Jun},  
language={en-US}  }

@misc{doge,
      title={DOGE: Towards Versatile Visual Document Grounding and Referring}, 
      author={Yinan Zhou and Yuxin Chen and Haokun Lin and Shuyu Yang and Li Zhu and Zhongang Qi and Chen Ma and Ying Shan},
      year={2024},
      eprint={2411.17125},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2411.17125}, 
}

@inproceedings{fashion200k,  
 title={Automatic Spatially-aware Fashion Concept Discovery}, 
 url={http://dx.doi.org/10.1109/iccv.2017.163}, 
 DOI={10.1109/ICCV.2017.163}, 
 booktitle={2017 IEEE International Conference on Computer Vision (ICCV)}, 
 author={Han, Xintong and Wu, Zuxuan and Huang, Phoenix X. and Zhang, Xiao and Zhu, Menglong and Li, Yuan and Zhao, Yang and Davis, Larry S.}, 
 year={2017}, 
 month={Oct}, 
 language={en-US} 
 }

@INPROCEEDINGS{fashioniq,
  author={Wu, Hui and Gao, Yupeng and Guo, Xiaoxiao and Al-Halah, Ziad and Rennie, Steven and Grauman, Kristen and Feris, Rogerio},
  booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Fashion IQ: A New Dataset Towards Retrieving Images by Natural Language Feedback}, 
  year={2021},
  volume={},
  number={},
  pages={11302-11312},
  keywords={Visualization;Computer vision;Image retrieval;Natural languages;Clothing;Buildings;Ontologies},
  doi={10.1109/CVPR46437.2021.01115}}

@article{gpt3,
  author       = {Tom B. Brown and
                  Benjamin Mann and
                  Nick Ryder and
                  Melanie Subbiah and
                  Jared Kaplan and
                  Prafulla Dhariwal and
                  Arvind Neelakantan and
                  Pranav Shyam and
                  Girish Sastry and
                  Amanda Askell and
                  Sandhini Agarwal and
                  Ariel Herbert{-}Voss and
                  Gretchen Krueger and
                  Tom Henighan and
                  Rewon Child and
                  Aditya Ramesh and
                  Daniel M. Ziegler and
                  Jeffrey Wu and
                  Clemens Winter and
                  Christopher Hesse and
                  Mark Chen and
                  Eric Sigler and
                  Mateusz Litwin and
                  Scott Gray and
                  Benjamin Chess and
                  Jack Clark and
                  Christopher Berner and
                  Sam McCandlish and
                  Alec Radford and
                  Ilya Sutskever and
                  Dario Amodei},
  title        = {Language Models are Few-Shot Learners},
  journal      = {CoRR},
  volume       = {abs/2005.14165},
  year         = {2020},
  url          = {https://arxiv.org/abs/2005.14165},
  eprinttype    = {arXiv},
  eprint       = {2005.14165},
  timestamp    = {Thu, 25 May 2023 10:38:31 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2005-14165.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{gpt4,  
 title={GPT-4 Technical Report}, 
 author={Achiam, OpenAI:Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, FlorenciaLeoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and Avila, Red and Babuschkin, Igor and Balaji, Suchir and Balcom, Valerie and Baltescu, Paul and Bao, Haiming and Bavarian, Mo and Belgum, Jeff and Bello, Irwan and Berdine, Jake and Bernadett-Shapiro, Gabriel and Berner, Christopher and Bogdonoff, Lenny and Boiko, Oleg and Boyd, Madelaine and Brakman, Anna-Luisa and Brockman, Greg and Brooks, Tim and Brundage, Miles and Button, Kevin and Cai, Trevor and Campbell, Rosie and Cann, Andrew and Carey, Brittany and Carlson, Chelsea and Carmichael, Rory and Chan, Brooke and Chang, Che and Chantzis, Fotis and Chen, Derek and Chen, Sully and Chen, Ruby and Chen, Jason and Chen, Mark and Chess, Ben and Cho, Chester and Chu, Casey and Chung, HyungWon and Cummings, Dave and Currier, Jeremiah}, 
 year={2023}, 
 month={Dec}, 
 language={en-US} 
}

@inproceedings{gu2024lincir,
    title={Language-only Training of Zero-shot Composed Image Retrieval},
    author={Gu, Geonmo and Chun, Sanghyuk and Kim, Wonjae and and Kang, Yoohoon and Yun, Sangdoo},
    year={2024},
    booktitle={Conference on Computer Vision and Pattern Recognition (CVPR)},
}

@inproceedings{instructblip,
 author = {Dai, Wenliang and Li, Junnan and LI, DONGXU and Tiong, Anthony and Zhao, Junqi and Wang, Weisheng and Li, Boyang and Fung, Pascale N and Hoi, Steven},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {49250--49267},
 publisher = {Curran Associates, Inc.},
 title = {InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/9a6a435e75419a836fe47ab6793623e6-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@article{instuctp2p,
  title={InstructPix2Pix: Learning to Follow Image Editing Instructions},
  author={Brooks, Tim and Holynski, Aleksander and Efros, Alexei A},
  journal={arXiv preprint arXiv:2211.09800},
  year={2022}
}

@inproceedings{lin2024duquant,
  title={Duquant: Distributing outliers via dual transformation makes stronger quantized llms},
  author={Lin, Haokun and Xu, Haobo and Wu, Yichen and Cui, Jingzhi and Zhang, Yingtao and Mou, Linzhan and Song, Linqi and Sun, Zhenan and Wei, Ying},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024}
}

@inproceedings{lin2024mope,
  title={Mope-clip: Structured pruning for efficient vision-language models with module-wise pruning error metric},
  author={Lin, Haokun and Bai, Haoli and Liu, Zhili and Hou, Lu and Sun, Muyi and Song, Linqi and Wei, Ying and Sun, Zhenan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={27370--27380},
  year={2024}
}

@article{llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@misc{llava,
      title={Visual Instruction Tuning}, 
      author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
      publisher={NeurIPS},
      year={2023},
}

@inproceedings{magiclens,
  title = 	 {{M}agic{L}ens: Self-Supervised Image Retrieval with Open-Ended Instructions},
  author =       {Zhang, Kai and Luan, Yi and Hu, Hexiang and Lee, Kenton and Qiao, Siyuan and Chen, Wenhu and Su, Yu and Chang, Ming-Wei},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {59403--59420},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  url = 	 {https://proceedings.mlr.press/v235/zhang24an.html}
}

@inproceedings{nlvr2,  
 title={A Corpus for Reasoning About Natural Language Grounded in Photographs.}, 
 url={http://dx.doi.org/10.18653/v1/p19-1644}, 
 DOI={10.18653/v1/P19-1644}, 
 booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, 
 author={Suhr, Alane and Zhou, Stephanie and Zhang, Ally and Zhang, Iris and Bai, Huajun and Artzi, Yoav}, 
 year={2019}, 
 month={Jan}, 
 language={en-US} 
 }

@INPROCEEDINGS{pic2word,
  author={Saito, Kuniaki and Sohn, Kihyuk and Zhang, Xiang and Li, Chun-Liang and Lee, Chen-Yu and Saenko, Kate and Pfister, Tomas},
  booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Pic2Word: Mapping Pictures to Words for Zero-shot Composed Image Retrieval}, 
  year={2023},
  volume={},
  number={},
  pages={19305-19314},
  keywords={Training;Computer vision;Codes;Image retrieval;Supervised learning;Training data;Benchmark testing;Vision;language;reasoning},
  doi={10.1109/CVPR52729.2023.01850}}

@article{rerank,
	title     = {Candidate Set Re-ranking for Composed Image Retrieval with Dual Multi-modal Encoder},
	author    = {Zheyuan Liu and Weixuan Sun and Damien Teney and Stephen Gould},
	journal   = {Transactions on Machine Learning Research},
	issn      = {2835-8856},
	year      = {2024},
	url       = {https://openreview.net/forum?id=fJAwemcvpL}
}

@inproceedings{sketch,  
 title={Deep Multimodal Embedding Model for Fine-grained Sketch-based Image Retrieval}, 
 url={http://dx.doi.org/10.1145/3077136.3080681}, 
 DOI={10.1145/3077136.3080681}, 
 booktitle={Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval}, 
 author={Huang, Fei and Cheng, Yong and Jin, Cheng and Zhang, Yuejie and Zhang, Tao}, 
 year={2017}, 
 month={Aug}, 
 language={en-US} 
 }

@inproceedings{stablediffusion,  
 title={High-Resolution Image Synthesis with Latent Diffusion Models}, 
 url={http://dx.doi.org/10.1109/cvpr52688.2022.01042}, 
 DOI={10.1109/CVPR52688.2022.01042}, 
 booktitle={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
 author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bjorn}, 
 year={2022}, 
 month={Jun}, 
 language={en-US} 
 }

@inproceedings{tirg,  
 title={Composing Text and Image for Image Retrieval - An Empirical Odyssey}, 
 url={http://dx.doi.org/10.1109/cvpr.2019.00660}, 
 DOI={10.1109/CVPR.2019.00660}, 
 booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
 author={Vo, Nam and Jiang, Lu and Sun, Chen and Murphy, Kevin and Li, Li-Jia and Fei-Fei, Li and Hays, James}, 
 year={2019}, 
 month={Jun}, 
 language={en-US} 
 }

@inproceedings{vaze2023gen,
        title={GeneCIS: A Benchmark for General Conditional Image Similarity},
        author={Sagar Vaze and Nicolas Carion and Ishan Misra},
        booktitle={CVPR},
        year={2023}
        }

@article{vqa2,  
 title={Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering}, 
 url={http://dx.doi.org/10.1007/s11263-018-1116-0}, 
 DOI={10.1007/s11263-018-1116-0}, 
 journal={International Journal of Computer Vision}, 
 author={Goyal, Yash and Khot, Tejas and Agrawal, Aishwarya and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi}, 
 year={2019}, 
 month={Apr}, 
 pages={398–414}, 
 language={en-US} 
 }

@inproceedings{yang2023towards,
  title={Towards unified text-based person retrieval: A large-scale multi-attribute and language search benchmark},
  author={Yang, Shuyu and Zhou, Yinan and Zheng, Zhedong and Wang, Yaxiong and Zhu, Li and Wu, Yujiao},
  booktitle={Proceedings of the 31st ACM International Conference on Multimedia},
  pages={4492--4501},
  year={2023}
}

