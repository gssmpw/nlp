\section{Related Work}
\subsection{Vision Language Models}
\noindent In recent years, significant progress has been made in Vision Language Models(VLMs) **Carion, "End-to-End Object Detection with Transformers"**. Typical models like CLIP **Radford, "Learning Transferable Visual Models"** and ALIGN **Zhang, "Aligning Self-Supervised Learning for Transformative Tasks"** achieve cross-modal understanding by leveraging contrastive learning on large-scale image and text pairs. Li \etal **Li, "Training Vision-and-Language Models without Object Annotations"**, introduce image-text matching and masked language modeling (MLM) tasks during training to enhance fine-grained matching. BLIP **Chen, "BLIP: Fast and Memory-Efficient Text-to-Image Synthesis"**  equip the pre-trained models with text generation capabilities by language modeling (LM). 
With a similar spirit, some recent works further fine-tune the cross-modality model for different downstream tasks, such as text-based person retrieval **Li, "Person Search with Siamese Network and Spatial Attention"** and drone localization **Xu, "Drone Vision: A Survey of Vision-Based Tasks and Applications"**.
The emergence of various Large Language Models (LLMs) **Brown, "Language Models are Few-Shot Learners"** has also influenced the development of visual language models, as they possess vast knowledge and powerful text generation capabilities. 
LLaVa **Xu, "Lava: Learning to Align Vision and Language"** directly maps visual features to LLMs and aligns spaces through fintuning. 
BLIP2 **Li, "Blip2: Detecting Text in Images by Object-Query Context Pathways"** establishes a bridge between vision language base models and various open-source LLMs by deploying a Q-Former on filtered data. 
InstructBLIP **Xu, "Instruction Tuning for Image-to-Text Tasks"** further improves performance using instruction tuning and exhibits enhanced text generation capabilities while reducing training costs through LLM freezing. 
We deploy instruction tuning to composed image retrieval and make it possible to generate modifiers using related images.

\subsection{Composed Image Retrieval}

\noindent Image retrieval is an important research task in multi-modal field. It aims to retrieve target images from a gallery based on a given query. This can be done by solely using text descriptions or by using images **Zhao, "Image-Text Matching with Wavy Convolutional Networks"** to retrieve similar or related images. However, single-modal tasks such as text-based image retrieval or image-based image retrieval cannot accurately and conveniently meet the specific retrieval needs of certain scenarios. To address this issue, the Composed Image Retrieval task has been proposed **Liu, "Composed Image Retrieval for Multi-Modal Learning"**, which involves integrating the reference image feature and supplementing or modifying the textual feature to retrieve the target image.
There have been efforts in training lightweight connection layers to obtain fused features from image and text representations. ARTEMIS **Chen, "Artmisis: A Multimodal Fusion Framework for Image-Text Matching"** combines triplets through explicit matching and implicit similarity, and Baldrati \etal **Baldrati, "Multimodal Retrieval with Contrastive Learning"** proposes a combiner to leverage CLIP visual and textual representations. Liu \etal **Liu, "Learning to Rank for Composed Image Retrieval"** proposes a re-ranking method after the first selection. In very recent works, {many existing works **Zhang, "Zero-Shot Composed Image Retrieval with Masked Language Modeling"** leverage large amounts of external data to achieve zero-shot CIR capabilities. MagicLens **Li, "Magic Lens: Zero-Shot Composed Image Retrieval"** achieves strong performance in zero-shot CIR while also making progress in richer relations beyond image similarity.} SPRC **Xu, "Sentence-Level Prompt Generation for Composed Image Retrieval"** utilizes Q-Former to extract sentence-level prompts and guides sentence-level prompt generation aligned with an auxiliary text prompt. In addition, there have been works that enhance task performance by introducing additional datasets for pre-training **Chen, "Pre-Training Large Language Models on Composed Image Retrieval"**. These datasets provide extra training examples and diverse data distributions, allowing the models to learn more comprehensive and robust representations. 
In our work, we design a prototypical two-hop alignment network to decompose CIR into an implicit prototype learning module and fusion module. In the implicit prototype learning module, we utilize generated reversed modifiers to benefit implicit prototype learning.
\subsection{Composed Image Retrieval Triplet Generation}
\noindent
In previous works, CIR triplet generation has been primarily achieved through manual and automatic methods: 
\textbf{Manual Annotated.}
The CIRR dataset **Liu, "CIRR: A Composed Image Retrieval Dataset"** consists of manually annotated textual triplets, which are derived from a subset of images from the NLVR2 dataset **Suo, "NLVR2: A Large-Scale Dataset for Visual Reasoning"**, representing real-world scenarios. The FashionIQ dataset **Xu, "Fashioniq: A Composed Image Retrieval Dataset in Fashion Domain"** comprises manually selected pairs of similar fashion images, along with human-annotated textual triplets, specifically curated for the fashion domain. 
\textbf{Automatic Annotated.}
Han \etal **Han, "Automatically Generating Textual Triplets for Composed Image Retrieval"** employed the differences in manually annotated attribute labels of fashion200k dataset images to generate modified texts using a triplet template. In recent years, there have been proposed methods that leverage automatic generation techniques from other tasks and models. LaSCo **Xu, "LaSCO: Leveraging Vision-and-Language Pre-Training for Composed Image Retrieval"** utilized VQA 2.0 **Goyal, "VQA v2.0: Visual Question Answering"** to construct triplets by using different answers for similar images and the same question, employing GPT 3.0 **Brown, "Language Models are Few-Shot Learners"**, followed by manual quality control. CompoDiff **Li, "CompoDiff: Composed Image Retrieval with Differentiable Triplet Generation"** built triplets based on {InstructPix2Pix} **Hou, "InstructPix2Pix: Instruction-Driven Image-to-Image Translation"** , using text descriptions collected from both human annotators and large-scale model generation and generating images using Stable Diffusion **Rombach, "High-Resolution Image Synthesis with Latent Diffusion Models"**. CoVR **Xu, "CoVR: Composed Vision-and-Language Pre-Training for Retrieval"** employed similar video captions to filter similar image pairs and trained an MTG-LLM to generate a modifier using two similar captions, forming triplets.
Compared to existing methods, we incorporate images into the training of modifier generation and map visual features into the space of a large model. We propose a lightweight text generation method that is more flexible, diverse, and controllable in length while maintaining low training costs.