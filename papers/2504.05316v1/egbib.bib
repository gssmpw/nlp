@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})




@INPROCEEDINGS{cirr,
  author={Liu, Zheyuan and Rodriguez-Opazo, Cristian and Teney, Damien and Gould, Stephen},
  booktitle={2021 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Image Retrieval on Real-life Images with Pre-trained Vision-and-Language Models}, 
  year={2021},
  volume={},
  number={},
  pages={2105-2114},
  keywords={Visualization;Computer vision;Limiting;Codes;Image retrieval;Natural languages;Computer architecture;Vision + language;Datasets and evaluation;Image and video retrieval;Representation learning},
  doi={10.1109/ICCV48922.2021.00213}}

@inproceedings{Comprehensive,
author = {Zhang, Feifei and Yan, Ming and Zhang, Ji and Xu, Changsheng},
title = {Comprehensive Relationship Reasoning for Composed Query Based Image Retrieval},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3548126},
doi = {10.1145/3503161.3548126},
abstract = {Composed Query Based Image Retrieval (CQBIR) aims at searching images relevant to a composed query, i.e., a reference image together with a modifier text. Compared with conventional image retrieval, which takes a single image or text to retrieve desired images, CQBIR encounters more challenges as it requires not only effective semantic correspondence between the heterogeneous query and target, but also synergistic understanding of the composed query. To establish robust CQBIR model, four critical types of relational information can be included, i.e., cross-modal, intra-sample, inter-sample, and cross-sample relationships. Pioneer studies mainly exploit parts of the information, which are hard to make them enhance and complement each other. In this paper, we propose a comprehensive relationship reasoning network by fully exploring the four types of information for CQBIR, which mainly includes two key designs. First, we introduce a memory-augmented cross-modal attention module, in which the representation of the composed query is augmented by considering the cross-modal relationship between the reference image and the modification text. Second, we design a multi-scale matching strategy to optimize our network, aiming at harnessing information from the intra-sample, inter-sample, and cross-sample relationships. To the best of our knowledge, this is the first work to fully explore the four pieces of relationships in a unified deep model for CQBIR. Comprehensive experimental results on five standard benchmarks demonstrate that the proposed method performs favorably against state-of-the-art models.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {4655–4664},
numpages = {10},
keywords = {composed query based image retrieval, memory, multi-scale matching, relational information},
location = {<conf-loc>, <city>Lisboa</city>, <country>Portugal</country>, </conf-loc>},
series = {MM '22}
}
@inproceedings{joint,
author = {Zhang, Feifei and Xu, Mingliang and Mao, Qirong and Xu, Changsheng},
title = {Joint Attribute Manipulation and Modality Alignment Learning for Composing Text and Image to Image Retrieval},
year = {2020},
isbn = {9781450379885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394171.3413917},
doi = {10.1145/3394171.3413917},
abstract = {Cross-model retrieval has attracted much attention in recent years due to its wide applications. Conventional approaches usually take one modality as query to retrieve relevant data of another modality. In this paper, we devote to an emerging task in cross-modal retrieval, Composing Text and Image to Image Retrieval (CTI-IR), which aims at retrieving images relevant to a query image with text describing desired modifications to the query image. Compared with conventional cross-modal retrieval, the new task is particularly useful for the retrieval that the query image does not perfectly match the user's expectations. Generally, the CTI-IR involves two underlying problems: how to manipulate visual features of the query image specified by the text, and how to model the modality gap between the query and target. Most previous methods focus on solving the second problem. In this paper, we aim to deal with both problems simultaneously in a unified model. Specifically, the proposed method is based on the graph attention network and adversarial learning network, which enjoys several merits. First, the query image and the modification text are constructed in a relation graph for learning text-adaptive representations. Second, semantic contents from the text are injected into the visual features through graph attention. Third, an adversarial loss is incorporated into the conventional cross-modal retrieval loss to learn more discriminative modality invariant representations for CTI-IR. Extensive experiments on three benchmark datasets demonstrate that the proposed method performs favorably against state-of-the-art methods.},
booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
pages = {3367–3376},
numpages = {10},
keywords = {CTI-IR, adversarial learning, cross-model retrieval, graph attention},
location = {Seattle, WA, USA},
series = {MM '20}
}
 @article{clip,  
 title={Learning Transferable Visual Models From Natural Language Supervision}, 
 journal={Cornell University - arXiv,Cornell University - arXiv}, 
 author={Radford, Alec and Kim, JongWook and Hallacy, Chris and Ramesh, A. and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Amanda, Askell and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya}, 
 year={2021}, 
 month={Feb}, 
 language={en-US} 
 }

@InProceedings{blip,
  title = 	 {{BLIP}: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},
  author =       {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {12888--12900},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/li22n/li22n.pdf},
  url = 	 {https://proceedings.mlr.press/v162/li22n.html},
  abstract = 	 {Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to video-language tasks in a zero-shot manner. Code and models are available at https://github.com/salesforce/BLIP.}
}

@inproceedings{tx,
  title={Text-based image retrieval using progressive multi-instance learning},
  author={Li, Wen and Duan, Lixin and Xu, Dong and Tsang, Ivor Wai-Hung},
  booktitle={2011 international conference on computer vision},
  pages={2049--2055},
  year={2011},
  organization={IEEE}
}
@inproceedings{blip2,
  title={BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models},
  author={Junnan Li and Dongxu Li and Silvio Savarese and Steven C. H. Hoi},
  booktitle={International Conference on Machine Learning},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:256390509}
}
@InProceedings{itr1,
author = {Zhang, Qi and Lei, Zhen and Zhang, Zhaoxiang and Li, Stan Z.},
title = {Context-Aware Attention Network for Image-Text Retrieval},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}
@inproceedings{chu2025towards,
  title={Towards natural language-guided drones: GeoText-1652 benchmark with spatial relation matching},
  author={Chu, Meng and Zheng, Zhedong and Ji, Wei and Wang, Tingyu and Chua, Tat-Seng},
  booktitle={European Conference on Computer Vision},
  pages={213--231},
  year={2025},
  organization={Springer}
}
@inproceedings{yang2023towards,
  title={Towards unified text-based person retrieval: A large-scale multi-attribute and language search benchmark},
  author={Yang, Shuyu and Zhou, Yinan and Zheng, Zhedong and Wang, Yaxiong and Zhu, Li and Wu, Yujiao},
  booktitle={Proceedings of the 31st ACM International Conference on Multimedia},
  pages={4492--4501},
  year={2023}
}

@InProceedings{itr2,
    author    = {Anwaar, Muhammad Umer and Labintcev, Egor and Kleinsteuber, Martin},
    title     = {Compositional Learning of Image-Text Query for Image Retrieval},
    booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
    month     = {January},
    year      = {2021},
    pages     = {1140-1149}
}
@InProceedings{itr3,
author = {Chen, Hui and Ding, Guiguang and Liu, Xudong and Lin, Zijia and Liu, Ji and Han, Jungong},
title = {IMRAM: Iterative Matching With Recurrent Attention Memory for Cross-Modal Image-Text Retrieval},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@article{karthikeyan2014survey,
  title={A survey on text and content based image retrieval system for image mining},
  author={Karthikeyan, T and Manikandaprabhu, P and Nithya, S},
  journal={International Journal of Engineering},
  volume={3},
  year={2014}
}

@INPROCEEDINGS{fashioniq,
  author={Wu, Hui and Gao, Yupeng and Guo, Xiaoxiao and Al-Halah, Ziad and Rennie, Steven and Grauman, Kristen and Feris, Rogerio},
  booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Fashion IQ: A New Dataset Towards Retrieving Images by Natural Language Feedback}, 
  year={2021},
  volume={},
  number={},
  pages={11302-11312},
  keywords={Visualization;Computer vision;Image retrieval;Natural languages;Clothing;Buildings;Ontologies},
  doi={10.1109/CVPR46437.2021.01115}}

@article{clip4cir2,
  title={Composed Image Retrieval using Contrastive Learning and Task-oriented CLIP-based Features},
  author={Baldrati, Alberto and Bertini, Marco and Uricchio, Tiberio and Bimbo, Alberto Del},
  journal={ACM Transactions on Multimedia Computing, Communications and Applications},
  publisher={ACM New York, NY}
}



@article{dataroaming,
 title={Data Roaming and Quality Assessment for Composed Image Retrieval},
  volume={38}, url={https://ojs.aaai.org/index.php/AAAI/article/view/28081},
  DOI={10.1609/aaai.v38i4.28081},
  abstractNote={The task of Composed Image Retrieval (CoIR) involves queries that combine image and text modalities, allowing users to express their intent more effectively. However, current CoIR datasets are orders of magnitude smaller compared to other vision and language (V&amp;L) datasets. Additionally, some of these datasets have noticeable issues, such as queries containing redundant modalities. To address these shortcomings, we introduce the Large Scale Composed Image Retrieval (LaSCo) dataset, a new CoIR dataset which is ten times larger than existing ones. Pre-training on our LaSCo, shows a noteworthy improvement in performance, even in zero-shot. Furthermore, we propose a new approach for analyzing CoIR datasets and methods, which detects modality redundancy or necessity, in queries.
    We also introduce a new CoIR baseline, the Cross-Attention driven Shift Encoder (CASE). This baseline allows for early fusion of modalities using a cross-attention module and employs an additional auxiliary task during training. Our experiments demonstrate that this new baseline outperforms the current state-of-the-art methods on established benchmarks like FashionIQ and CIRR.},
  number={4},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence},
  author={Levy, Matan and Ben-Ari, Rami and Darshan, Nir and Lischinski, Dani},
  year={2024},
  month={Mar.},
  pages={2991-2999}
}


@INPROCEEDINGS{famevil,
  author={Han, Xiao and Zhu, Xiatian and Yu, Licheng and Zhang, Li and Song, Yi-Zhe and Xiang, Tao},
  booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={FAME-ViL: Multi-Tasking Vision-Language Model for Heterogeneous Fashion Tasks}, 
  year={2023},
  volume={},
  number={},
  pages={2669-2680},
  keywords={Training;Learning systems;Adaptation models;Image retrieval;Pipelines;Computer architecture;Multitasking;Vision;language;and reasoning},
  doi={10.1109/CVPR52729.2023.00262}}

@article{covr,
   title={CoVR: Learning Composed Video Retrieval from Web Video Captions},
   volume={38},
   ISSN={2159-5399},
   url={http://dx.doi.org/10.1609/aaai.v38i6.28334},
   DOI={10.1609/aaai.v38i6.28334},
   number={6},
   journal={Proceedings of the AAAI Conference on Artificial Intelligence},
   publisher={Association for the Advancement of Artificial Intelligence (AAAI)},
   author={Ventura, Lucas and Yang, Antoine and Schmid, Cordelia and Varol, Gül},
   year={2024},
   month=mar, pages={5270–5279} }

@article{compodiff,
    title={CompoDiff: Versatile Composed Image Retrieval With Latent Diffusion},
    author={Geonmo Gu and Sanghyuk Chun and Wonjae Kim and HeeJae Jun and Yoohoon Kang and Sangdoo Yun},
    journal={Transactions on Machine Learning Research},
    issn={2835-8856},
    year={2024},
    url={https://openreview.net/forum?id=mKtlzW0bWc},
    note={Expert Certification}
}
@inproceedings{albef,
      title={Align before Fuse: Vision and Language Representation Learning with Momentum Distillation}, 
      author={Junnan Li and Ramprasaath R. Selvaraju and Akhilesh Deepak Gotmare and Shafiq Joty and Caiming Xiong and Steven Hoi},
      year={2021},
      booktitle={NeurIPS},
}

@article{gpt4,  
 title={GPT-4 Technical Report}, 
 author={Achiam, OpenAI:Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, FlorenciaLeoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and Avila, Red and Babuschkin, Igor and Balaji, Suchir and Balcom, Valerie and Baltescu, Paul and Bao, Haiming and Bavarian, Mo and Belgum, Jeff and Bello, Irwan and Berdine, Jake and Bernadett-Shapiro, Gabriel and Berner, Christopher and Bogdonoff, Lenny and Boiko, Oleg and Boyd, Madelaine and Brakman, Anna-Luisa and Brockman, Greg and Brooks, Tim and Brundage, Miles and Button, Kevin and Cai, Trevor and Campbell, Rosie and Cann, Andrew and Carey, Brittany and Carlson, Chelsea and Carmichael, Rory and Chan, Brooke and Chang, Che and Chantzis, Fotis and Chen, Derek and Chen, Sully and Chen, Ruby and Chen, Jason and Chen, Mark and Chess, Ben and Cho, Chester and Chu, Casey and Chung, HyungWon and Cummings, Dave and Currier, Jeremiah}, 
 year={2023}, 
 month={Dec}, 
 language={en-US} 
}
@article{llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}


@inproceedings{
vicuna,
title={Judging {LLM}-as-a-Judge with {MT}-Bench and Chatbot Arena},
author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2023},
url={https://openreview.net/forum?id=uccHPGDlao}
}

@article{baichuan,
  title={Baichuan 2: Open Large-scale Language Models},
  author={Baichuan},
  journal={arXiv preprint arXiv:2309.10305},
  url={https://arxiv.org/abs/2309.10305},
  year={2023}
}

@article{Falcon,
  title={The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only},
  author={Penedo, Guilherme and Malartic, Quentin and Hesslow, Daniel and Cojocaru, Ruxandra and Cappelli, Alessandro and Alobeidli, Hamza and Pannier, Baptiste and Almazrouei, Ebtesam and Launay, Julien},
  journal={arXiv preprint arXiv:2306.01116},
  year={2023}
}
@article{opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}
@article{flant5,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={arXiv preprint arXiv:2210.11416},
  year={2022}
}

@misc{llava,
      title={Visual Instruction Tuning}, 
      author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
      publisher={NeurIPS},
      year={2023},
}
@inproceedings{instructblip,
 author = {Dai, Wenliang and Li, Junnan and LI, DONGXU and Tiong, Anthony and Zhao, Junqi and Wang, Weisheng and Li, Boyang and Fung, Pascale N and Hoi, Steven},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {49250--49267},
 publisher = {Curran Associates, Inc.},
 title = {InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/9a6a435e75419a836fe47ab6793623e6-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}



@inproceedings{deepfashion,
title={DeepFashion: Powering Robust Clothes Recognition and Retrieval with Rich Annotations},
url={http://dx.doi.org/10.1109/cvpr.2016.124},  
DOI={10.1109/CVPR.2016.124},  
booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},  
author={Liu, Ziwei and Luo, Ping and Qiu, Shi and Wang, Xiaogang and Tang, Xiaoou},  
year={2016},  
month={Jun},  
language={en-US}  }
@inproceedings{sketch,  
 title={Deep Multimodal Embedding Model for Fine-grained Sketch-based Image Retrieval}, 
 url={http://dx.doi.org/10.1145/3077136.3080681}, 
 DOI={10.1145/3077136.3080681}, 
 booktitle={Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval}, 
 author={Huang, Fei and Cheng, Yong and Jin, Cheng and Zhang, Yuejie and Zhang, Tao}, 
 year={2017}, 
 month={Aug}, 
 language={en-US} 
 }

@inproceedings{align,
  title={Scaling up visual and vision-language representation learning with noisy text supervision},
  author={Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc and Sung, Yun-Hsuan and Li, Zhen and Duerig, Tom},
  booktitle={International conference on machine learning},
  pages={4904--4916},
  year={2021},
  organization={PMLR}
}

 @inproceedings{tirg,  
 title={Composing Text and Image for Image Retrieval - An Empirical Odyssey}, 
 url={http://dx.doi.org/10.1109/cvpr.2019.00660}, 
 DOI={10.1109/CVPR.2019.00660}, 
 booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
 author={Vo, Nam and Jiang, Lu and Sun, Chen and Murphy, Kevin and Li, Li-Jia and Fei-Fei, Li and Hays, James}, 
 year={2019}, 
 month={Jun}, 
 language={en-US} 
 }
 @inproceedings{cosmo,  
 title={CoSMo: Content-Style Modulation for Image Retrieval with Text Feedback}, 
 url={http://dx.doi.org/10.1109/cvpr46437.2021.00086}, 
 DOI={10.1109/CVPR46437.2021.00086}, 
 booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
 author={Lee, Seungmin and Kim, Dongwan and Han, Bohyung}, 
 year={2021}, 
 month={Jun}, 
 language={en-US} 
 }

@inproceedings{atermis,
  title={ARTEMIS: Attention-based Retrieval with Text-Explicit Matching and Implicit Similarity},
  author={Delmas, Ginger and Rezende, Rafael S and Csurka, Gabriela and Larlus, Diane},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@INPROCEEDINGS{pic2word,
  author={Saito, Kuniaki and Sohn, Kihyuk and Zhang, Xiang and Li, Chun-Liang and Lee, Chen-Yu and Saenko, Kate and Pfister, Tomas},
  booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Pic2Word: Mapping Pictures to Words for Zero-shot Composed Image Retrieval}, 
  year={2023},
  volume={},
  number={},
  pages={19305-19314},
  keywords={Training;Computer vision;Codes;Image retrieval;Supervised learning;Training data;Benchmark testing;Vision;language;reasoning},
  doi={10.1109/CVPR52729.2023.01850}}
@inproceedings{
sprc,
title={Sentence-level Prompts Benefit Composed Image Retrieval},
author={Yang Bai and Xinxing Xu and Yong Liu and Salman Khan and Fahad Khan and Wangmeng Zuo and Rick Siow Mong Goh and Chun-Mei Feng},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=m3ch3kJL7q}
}


@article{rerank,
	title     = {Candidate Set Re-ranking for Composed Image Retrieval with Dual Multi-modal Encoder},
	author    = {Zheyuan Liu and Weixuan Sun and Damien Teney and Stephen Gould},
	journal   = {Transactions on Machine Learning Research},
	issn      = {2835-8856},
	year      = {2024},
	url       = {https://openreview.net/forum?id=fJAwemcvpL}
}
 @inproceedings{nlvr2,  
 title={A Corpus for Reasoning About Natural Language Grounded in Photographs.}, 
 url={http://dx.doi.org/10.18653/v1/p19-1644}, 
 DOI={10.18653/v1/P19-1644}, 
 booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, 
 author={Suhr, Alane and Zhou, Stephanie and Zhang, Ally and Zhang, Iris and Bai, Huajun and Artzi, Yoav}, 
 year={2019}, 
 month={Jan}, 
 language={en-US} 
 }
 @inproceedings{fashion200k,  
 title={Automatic Spatially-aware Fashion Concept Discovery}, 
 url={http://dx.doi.org/10.1109/iccv.2017.163}, 
 DOI={10.1109/ICCV.2017.163}, 
 booktitle={2017 IEEE International Conference on Computer Vision (ICCV)}, 
 author={Han, Xintong and Wu, Zuxuan and Huang, Phoenix X. and Zhang, Xiao and Zhu, Menglong and Li, Yuan and Zhao, Yang and Davis, Larry S.}, 
 year={2017}, 
 month={Oct}, 
 language={en-US} 
 }

@article{instuctp2p,
  title={InstructPix2Pix: Learning to Follow Image Editing Instructions},
  author={Brooks, Tim and Holynski, Aleksander and Efros, Alexei A},
  journal={arXiv preprint arXiv:2211.09800},
  year={2022}
}

@article{vqa2,  
 title={Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering}, 
 url={http://dx.doi.org/10.1007/s11263-018-1116-0}, 
 DOI={10.1007/s11263-018-1116-0}, 
 journal={International Journal of Computer Vision}, 
 author={Goyal, Yash and Khot, Tejas and Agrawal, Aishwarya and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi}, 
 year={2019}, 
 month={Apr}, 
 pages={398–414}, 
 language={en-US} 
 }
@article{gpt3,
  author       = {Tom B. Brown and
                  Benjamin Mann and
                  Nick Ryder and
                  Melanie Subbiah and
                  Jared Kaplan and
                  Prafulla Dhariwal and
                  Arvind Neelakantan and
                  Pranav Shyam and
                  Girish Sastry and
                  Amanda Askell and
                  Sandhini Agarwal and
                  Ariel Herbert{-}Voss and
                  Gretchen Krueger and
                  Tom Henighan and
                  Rewon Child and
                  Aditya Ramesh and
                  Daniel M. Ziegler and
                  Jeffrey Wu and
                  Clemens Winter and
                  Christopher Hesse and
                  Mark Chen and
                  Eric Sigler and
                  Mateusz Litwin and
                  Scott Gray and
                  Benjamin Chess and
                  Jack Clark and
                  Christopher Berner and
                  Sam McCandlish and
                  Alec Radford and
                  Ilya Sutskever and
                  Dario Amodei},
  title        = {Language Models are Few-Shot Learners},
  journal      = {CoRR},
  volume       = {abs/2005.14165},
  year         = {2020},
  url          = {https://arxiv.org/abs/2005.14165},
  eprinttype    = {arXiv},
  eprint       = {2005.14165},
  timestamp    = {Thu, 25 May 2023 10:38:31 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2005-14165.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
 @inproceedings{stablediffusion,  
 title={High-Resolution Image Synthesis with Latent Diffusion Models}, 
 url={http://dx.doi.org/10.1109/cvpr52688.2022.01042}, 
 DOI={10.1109/CVPR52688.2022.01042}, 
 booktitle={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
 author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bjorn}, 
 year={2022}, 
 month={Jun}, 
 language={en-US} 
 }
 @inproceedings{webvids,  
 title={Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval}, 
 url={http://dx.doi.org/10.1109/iccv48922.2021.00175}, 
 DOI={10.1109/ICCV48922.2021.00175}, 
 booktitle={2021 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
 author={Bain, Max and Nagrani, Arsha and Varol, Gul and Zisserman, Andrew}, 
 year={2021}, 
 month={Oct}, 
 language={en-US} 
 }


@inproceedings{lavis,
    title = "{LAVIS}: A One-stop Library for Language-Vision Intelligence",
    author = "Li, Dongxu  and
      Li, Junnan  and
      Le, Hung  and
      Wang, Guangsen  and
      Savarese, Silvio  and
      Hoi, Steven C.H.",
    editor = "Bollegala, Danushka  and
      Huang, Ruihong  and
      Ritter, Alan",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-demo.3",
    doi = "10.18653/v1/2023.acl-demo.3",
    pages = "31--41",
    abstract = "We introduce LAVIS, an open-source deep learning library for LAnguage-VISion research and applications. LAVIS aims to serve as a one-stop comprehensive library that brings recent advancements in the language-vision field accessible for researchers and practitioners, as well as fertilizing future research and development. It features a unified interface to easily access state-of-the-art image-language, video-language models and common datasets. LAVIS supports training, evaluation and benchmarking on a rich variety of tasks, including multimodal classification, retrieval, captioning, visual question answering, dialogue and pre-training. In the meantime, the library is also highly extensible and configurable, facilitating future development and customization. In this technical report, we describe design principles, key components and functionalities of the library, and also present benchmarking results across common language-vision tasks.",
}



@article{vit,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@article{maaf,
  title={Modality-Agnostic Attention Fusion for visual search with text feedback},
  author={Dodds, Eric and Culpepper, Jack and Herdade, Simao and Zhang, Yang and Boakye, Kofi},
  journal={arXiv preprint arXiv:2007.00145},
  year={2020}
}

@article{curling,
  title={CurlingNet: Compositional Learning between Images and Text for Fashion IQ Data},
  author={Yu, Youngjae and Lee, Seunghwan and Choi, Yuncheol and Kim, Gunhee},
  journal={arXiv preprint arXiv:2003.12299},
  year={2020}
}

@InProceedings{blip4cir,
	author    = {Liu, Zheyuan and Sun, Weixuan and Hong, Yicong and Teney, Damien and Gould, Stephen},
	title     = {Bi-Directional Training for Composed Image Retrieval via Text Prompt Learning},
	booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
	month     = {January},
	year      = {2024},
	pages     = {5753-5762}
}
@inproceedings{cc3m,  
 title={Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning}, 
 url={http://dx.doi.org/10.18653/v1/p18-1238}, 
 DOI={10.18653/v1/P18-1238}, 
 booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, 
 author={Sharma, Piyush and Ding, Nan and Goodman, Sebastian and Soricut, Radu}, 
 year={2018}, 
 month={Jan}, 
 language={en-US} 
 }
 @article{t5xl,  
 title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}, 
 journal={arXiv: Learning,arXiv: Learning}, 
 author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, PeterJ.}, 
 year={2019}, 
 month={Oct}, 
 language={en-US} 
 }


@article{dalle2,
  title={Hierarchical Text-Conditional Image Generation with CLIP Latents},
  author={Aditya Ramesh and Prafulla Dhariwal and Alex Nichol and Casey Chu and Mark Chen},
  journal={ArXiv},
  year={2022},
  volume={abs/2204.06125},
  url={https://api.semanticscholar.org/CorpusID:248097655}
}

Copy
@article{prompttoprompt,
  title={Prompt-to-prompt image editing with cross attention control},
  author={Hertz, Amir and Mokady, Ron and Tenenbaum, Jay and Aberman, Kfir and Pritch, Yael and Cohen-Or, Daniel},
  booktitle={arXiv preprint arXiv:2208.01626},
  year={2022}
}



@inproceedings{chen2024composed,
author = "Chen, Yiyang and Zheng, Zhedong and Ji, Wei and Qu, Leigang and Chua, Tat-Seng",
title = "Composed Image Retrieval with Text Feedback via Multi-grained Uncertainty Regularization",
booktitle = "International Conference on Learning Representations (ICLR)",
code = "https://github.com/Monoxide-Chen/uncertainty\_retrieval",
year = "2024"
}

@ARTICLE{mar,
  author={Pang, Huaxin and Wei, Shikui and Zhang, Gangjian and Zhang, Shiyin and Qiu, Shuang and Zhao, Yao},
  journal={IEEE Transactions on Multimedia}, 
  title={Heterogeneous Feature Alignment and Fusion in Cross-Modal Augmented Space for Composed Image Retrieval}, 
  year={2023},
  volume={25},
  number={},
  pages={6446-6457},
  keywords={Image retrieval;Semantics;Task analysis;Visualization;Transformers;Feature extraction;Fuses;Composed image retrieval;embedding fusion;multi-modal learning;image retrieval},
  doi={10.1109/TMM.2022.3208742}}

@ARTICLE{mcl,
  author={Zhang, Gangjian and Wei, Shikui and Pang, Huaxin and Qiu, Shuang and Zhao, Yao},
  journal={IEEE Transactions on Multimedia}, 
  title={Enhance Composed Image Retrieval via Multi-Level Collaborative Localization and Semantic Activeness Perception}, 
  year={2024},
  volume={26},
  number={},
  pages={916-928},
  keywords={Semantics;Location awareness;Task analysis;Image retrieval;Training;Collaboration;Transformers;Composed image retrieval;multi-modal fusion and embedding;multi-modal representation learning;multi-modal retrieval;image retrieval},
  doi={10.1109/TMM.2023.3273466}}

@ARTICLE{tag,
  author={Lu, Dan and Liu, Xiaoxiao and Qian, Xueming},
  journal={IEEE Transactions on Multimedia}, 
  title={Tag-Based Image Search by Social Re-ranking}, 
  year={2016},
  volume={18},
  number={8},
  pages={1628-1639},
  keywords={Image retrieval;Visualization;Semantics;Cultural differences;Flickr;Media;Tagging;Image search;re-ranking;social clues;social media;tag-based image retrieval},
  doi={10.1109/TMM.2016.2568099}}

@ARTICLE{mmt,
  author={Xu, Yahui and Bin, Yi and Wei, Jiwei and Yang, Yang and Wang, Guoqing and Shen, Heng Tao},
  journal={IEEE Transactions on Multimedia}, 
  title={Multi-Modal Transformer With Global-Local Alignment for Composed Query Image Retrieval}, 
  year={2023},
  volume={25},
  number={},
  pages={8346-8357},
  keywords={Transformers;Image retrieval;Visualization;Task analysis;Feature extraction;Bit error rate;Fuses;Transformer;composed query image retrieval;local alignment;spatial attention;multi-modal learning},
  doi={10.1109/TMM.2023.3235495}}

@ARTICLE{nsfse,
  author={Wang, Yifan and Liu, Liyuan and Yuan, Chun and Li, Minbo and Liu, Jing},
  journal={IEEE Transactions on Multimedia}, 
  title={Negative-Sensitive Framework With Semantic Enhancement for Composed Image Retrieval}, 
  year={2024},
  volume={26},
  number={},
  pages={7608-7621},
  keywords={Image retrieval;Semantics;Task analysis;Visualization;Optimization;Correlation;Filtration;Composed image retrieval;semantic learning;cross-modal retrieval;distribution learning;attention mechanism},
  doi={10.1109/TMM.2024.3369898}}

@inproceedings{spn,
author = {Feng, Zhangchi and Zhang, Richong and Nie, Zhijie},
title = {Improving Composed Image Retrieval via Contrastive Learning with Scaling Positives and Negatives},
year = {2024},
isbn = {9798400706868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664647.3680808},
doi = {10.1145/3664647.3680808},
abstract = {The Composed Image Retrieval (CIR) task aims to retrieve target images using a composed query consisting of a reference image and a modified text. Advanced methods often utilize contrastive learning as the optimization objective, which benefits from adequate positive and negative examples. However, the triplet for CIR incurs high manual annotation costs, resulting in limited positive examples. Furthermore, existing methods commonly use in-batch negative sampling, which reduces the negative number available for the model. To address the problem of lack of positives, we propose a data generation method by leveraging a multi-modal large language model to construct triplets for CIR. To introduce more negatives during fine-tuning, we design a two-stage fine-tuning framework for CIR, whose second stage introduces plenty of static representations of negatives to optimize the representation space rapidly. The above two improvements can be effectively stacked and designed to be plug-and-play, easily applied to existing CIR models without changing their original architectures. Extensive experiments and ablation analysis demonstrate that our method effectively scales positives and negatives and achieves state-of-the-art results on both FashionIQ and CIRR datasets. In addition, our method also performs well in zero-shot composed image retrieval, providing a new CIR solution for the low-resources scenario. Our code and data are released at https://github.com/BUAADreamer/SPN4CIR.},
booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
pages = {1632–1641},
numpages = {10},
keywords = {composed image retrieval, contrastive learning},
location = {Melbourne VIC, Australia},
series = {MM '24}
}

@misc{eva,
      title={EVA: Exploring the Limits of Masked Visual Representation Learning at Scale}, 
      author={Yuxin Fang and Wen Wang and Binhui Xie and Quan Sun and Ledell Wu and Xinggang Wang and Tiejun Huang and Xinlong Wang and Yue Cao},
      year={2022},
      eprint={2211.07636},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2211.07636}, 
}

@misc{circo,
      title={Zero-Shot Composed Image Retrieval with Textual Inversion}, 
      author={Alberto Baldrati and Lorenzo Agnolucci and Marco Bertini and Alberto Del Bimbo},
      year={2023},
      eprint={2303.15247},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@article{CIReVL,
  title={Vision-by-Language for Training-Free Compositional Image Retrieval},
  author={Shyamgopal Karthik and Karsten Roth and Massimiliano Mancini and Zeynep Akata},
  journal={International Conference on Learning Representations (ICLR)},
  year={2024}
}

@inproceedings{gu2024lincir,
    title={Language-only Training of Zero-shot Composed Image Retrieval},
    author={Gu, Geonmo and Chun, Sanghyuk and Kim, Wonjae and and Kang, Yoohoon and Yun, Sangdoo},
    year={2024},
    booktitle={Conference on Computer Vision and Pattern Recognition (CVPR)},
}

@inproceedings{yang2024ldre,
  title={LDRE: LLM-based Divergent Reasoning and Ensemble for Zero-Shot Composed Image Retrieval},
  author={Yang, Zhenyu and Xue, Dizhan and Qian, Shengsheng and Dong, Weiming and Xu, Changsheng},
  booktitle={Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={80--90},
  year={2024}
}

@misc{li2024imagineseekimprovingcomposed,
      title={Imagine and Seek: Improving Composed Image Retrieval with an Imagined Proxy}, 
      author={You Li and Fan Ma and Yi Yang},
      year={2024},
      eprint={2411.16752},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2411.16752}, 
}

@inproceedings{magiclens,
  title = 	 {{M}agic{L}ens: Self-Supervised Image Retrieval with Open-Ended Instructions},
  author =       {Zhang, Kai and Luan, Yi and Hu, Hexiang and Lee, Kenton and Qiao, Siyuan and Chen, Wenhu and Su, Yu and Chang, Ming-Wei},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {59403--59420},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  url = 	 {https://proceedings.mlr.press/v235/zhang24an.html}
}


@inproceedings{vaze2023gen,
        title={GeneCIS: A Benchmark for General Conditional Image Similarity},
        author={Sagar Vaze and Nicolas Carion and Ishan Misra},
        booktitle={CVPR},
        year={2023}
        }

@article{css,
author = {Zhang, Xu and Zheng, Zhedong and Zhu, Linchao and Yang, Yi},
title = {Collaborative group: Composed image retrieval via consensus learning from noisy annotations},
year = {2024},
issue_date = {Sep 2024},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {300},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2024.112135},
doi = {10.1016/j.knosys.2024.112135},
journal = {Know.-Based Syst.},
month = nov,
numpages = {10},
keywords = {Noisy annotation, Data ambiguity, Compositional image retrieval, Image retrieval with text feedback, Multi-modal retrieval}
}

@article{Qwen2VL,
  title={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}

@inproceedings{chen2024internvl,
  title={Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks},
  author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={24185--24198},
  year={2024}
}


@article{TIP23,
  author       = {Zhenyang Li and
                  Yangyang Guo and
                  Kejie Wang and
                  Yinwei Wei and
                  Liqiang Nie and
                  Mohan S. Kankanhalli},
  title        = {Joint Answering and Explanation for Visual Commonsense Reasoning},
  journal      = {{IEEE} IEEE Transactions on Image Processing},
  volume       = {32},
  pages        = {3836--3846},
  year         = {2023}
}

@inproceedings{MM23,
  author       = {Zhenyang Li and
                  Yangyang Guo and
                  Kejie Wang and
                  Xiaolin Chen and
                  Liqiang Nie and
                  Mohan S. Kankanhalli},
  title        = {Do Vision-Language Transformers Exhibit Visual Commonsense? An Empirical Study of {VCR}},
  booktitle    = {Proceedings of the 31st {ACM} International Conference on Multimedia},
  pages        = {5634--5644},
  publisher    = {{ACM}},
  year         = {2023}
}

@article{TMM23,
  author       = {Zhenyang Li and
                  Yangyang Guo and
                  Kejie Wang and
                  Fan Liu and
                  Liqiang Nie and
                  Mohan S. Kankanhalli},
  title        = {Learning to Agree on Vision Attention for Visual Commonsense Reasoning},
  journal      = {{IEEE} Transactions on Multimedia},
  volume       = {26},
  pages        = {1065--1075},
  year         = {2024}
}


@inproceedings{lin2024duquant,
  title={Duquant: Distributing outliers via dual transformation makes stronger quantized llms},
  author={Lin, Haokun and Xu, Haobo and Wu, Yichen and Cui, Jingzhi and Zhang, Yingtao and Mou, Linzhan and Song, Linqi and Sun, Zhenan and Wei, Ying},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024}
}

@inproceedings{lin2024mope,
  title={Mope-clip: Structured pruning for efficient vision-language models with module-wise pruning error metric},
  author={Lin, Haokun and Bai, Haoli and Liu, Zhili and Hou, Lu and Sun, Muyi and Song, Linqi and Wei, Ying and Sun, Zhenan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={27370--27380},
  year={2024}
}

@misc{doge,
      title={DOGE: Towards Versatile Visual Document Grounding and Referring}, 
      author={Yinan Zhou and Yuxin Chen and Haokun Lin and Shuyu Yang and Li Zhu and Zhongang Qi and Chen Ma and Ying Shan},
      year={2024},
      eprint={2411.17125},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2411.17125}, 
}