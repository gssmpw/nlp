\section{Related Work}
\subsection{Vision Language Models}
\noindent In recent years, significant progress has been made in Vision Language Models(VLMs)____. Typical models like CLIP____ and ALIGN ____ achieve cross-modal understanding by leveraging contrastive learning on large-scale image and text pairs. Li \etal____ introduce image-text matching and masked language modeling (MLM) tasks during training to enhance fine-grained matching. BLIP____  equip the pre-trained models with text generation capabilities by language modeling (LM). 
With a similar spirit, some recent works further fine-tune the cross-modality model for different downstream tasks, such as text-based person retrieval____ and drone localization____.
The emergence of various Large Language Models (LLMs)____ has also influenced the development of visual language models, as they possess vast knowledge and powerful text generation capabilities. 
LLaVa____ directly maps visual features to LLMs and aligns spaces through fintuning. 
BLIP2____ establishes a bridge between vision language base models and various open-source LLMs by deploying a Q-Former on filtered data. 
InstructBLIP____ further improves performance using instruction tuning and exhibits enhanced text generation capabilities while reducing training costs through LLM freezing. 
We deploy instruction tuning to composed image retrieval and make it possible to generate modifiers using related images.

\subsection{Composed Image Retrieval}

\noindent Image retrieval is an important research task in multi-modal field. It aims to retrieve target images from a gallery based on a given query. This can be done by solely using text descriptions or by using images____ to retrieve similar or related images. However, single-modal tasks such as text-based image retrieval or image-based image retrieval cannot accurately and conveniently meet the specific retrieval needs of certain scenarios. To address this issue, the Composed Image Retrieval task has been proposed____, which involves integrating the reference image feature and supplementing or modifying the textual feature to retrieve the target image.
There have been efforts in training lightweight connection layers to obtain fused features from image and text representations. ARTEMIS____ combines triplets through explicit matching and implicit similarity, and Baldrati \etal____ proposes a combiner to leverage CLIP visual and textual representations. Liu \etal____ proposes a re-ranking method after the first selection. In very recent works, {many existing works____ leverage large amounts of external data to achieve zero-shot CIR capabilities. MagicLens____ achieves strong performance in zero-shot CIR while also making progress in richer relations beyond image similarity.} SPRC____ utilizes Q-Former to extract sentence-level prompts and guides sentence-level prompt generation aligned with an auxiliary text prompt. In addition, there have been works that enhance task performance by introducing additional datasets for pre-training____. These datasets provide extra training examples and diverse data distributions, allowing the models to learn more comprehensive and robust representations. 
In our work, we design a prototypical two-hop alignment network to decompose CIR into an implicit prototype learning module and fusion module. In the implicit prototype learning module, we utilize generated reversed modifiers to benefit implicit prototype learning.
\subsection{Composed Image Retrieval Triplet Generation}
\noindent
In previous works, CIR triplet generation has been primarily achieved through manual and automatic methods: 
\textbf{Manual Annotated.}
The CIRR dataset____ consists of manually annotated textual triplets, which are derived from a subset of images from the NLVR2 dataset____, representing real-world scenarios. The FashionIQ dataset____ comprises manually selected pairs of similar fashion images, along with human-annotated textual triplets, specifically curated for the fashion domain. 
\textbf{Automatic Annotated.}
Han \etal____ employed the differences in manually annotated attribute labels of fashion200k dataset images to generate modified texts using a triplet template. In recent years, there have been proposed methods that leverage automatic generation techniques from other tasks and models. LaSCo____ utilized VQA 2.0____ to construct triplets by using different answers for similar images and the same question, employing GPT 3.0____, followed by manual quality control. CompoDiff____ built triplets based on {InstructPix2Pix}____, using text descriptions collected from both human annotators and large-scale model generation and generating images using Stable Diffusion____. CoVR____ employed similar video captions to filter similar image pairs and trained an MTG-LLM to generate a modifier using two similar captions, forming triplets.
Compared to existing methods, we incorporate images into the training of modifier generation and map visual features into the space of a large model. We propose a lightweight text generation method that is more flexible, diverse, and controllable in length while maintaining low training costs.