\section{Related Work}
\begin{table}[tbp!]
\footnotesize 
    \centering
    \caption{Moe Combination Analysis Results}
    \label{tab:table3}
    \begin{tabular}{ccccccc}
        \toprule
        Method&MNIST&FMNIST	&Cifar10&Cifar100&TINY&AGNews \\
        \midrule
        Fed-Syn-MoE & 89.48 &91.47&78.31	&18.75	&13.59	&93.37 \\
        % \midrule
        PM-MOE & 99.85	&98.61	&93.95	&70.68	&66.33	&94.76\\
        \bottomrule
    \end{tabular}
    \vspace{-0.3cm}
\end{table}

\paragraph{Personalized Federated Learning and MOE}
% In the realm of personalized federated learning, methods combining Mixture of Experts~\cite{DBLP:journals/neco/JacobsJNH91, DBLP:conf/nips/ZhouLLDHZDCLL22} (MoE), such as PFL-MoE\cite{DBLP:conf/apweb/GuoMXW21} and FedMoE\cite{yi2024fedmoe}, deploy a local feature extractor model (local expert) and a globally shared feature extractor (global expert) on each client, adjusting the gating network to control the output weights of the local and global experts. PFL-MoE focuses on homogeneous models, adjusting the weights of the two experts through the gating network. In contrast, FedMoE emphasizes model heterogeneity, employing experts with a larger parameter count than the global model to capture local data characteristics. Both approaches utilize simultaneous training of parameters, employing continuous gradient descent to optimize the gating of local and global models.

% In personalized federated learning, methods integrating Mixture of Experts~\cite{DBLP:journals/neco/JacobsJNH91, DBLP:conf/nips/ZhouLLDHZDCLL22} (MoE) models, such as PFL-MoE~\cite{DBLP:conf/apweb/GuoMXW21} and FedMoE~\cite{yi2024fedmoe}, deploy both local feature extractors (local experts) and globally shared feature extractors (global experts) on each client, with a gating network controlling the output weights of these experts. PFL-MoE primarily addresses homogeneous models, modulating the experts' weights via the gating network. In contrast, FedMoE emphasizes model heterogeneity by incorporating experts with more parameters than the global model to better capture local data characteristics. 

In personalized federated learning, methods integrating Mixture of Experts~\cite{DBLP:journals/neco/JacobsJNH91, DBLP:conf/nips/ZhouLLDHZDCLL22} (MoE) models, such as PFL-MoE~\cite{DBLP:conf/apweb/GuoMXW21} and FedMoE~\cite{yi2024fedmoe}. PFL-MoE primarily addresses homogeneous models, modulating the experts' weights via the gating network. In contrast, FedMoE emphasizes model heterogeneity by incorporating experts with more parameters than the global model to better capture local data characteristics. 
% Both methods optimize expert gating through continuous gradient descent.


\paragraph{Energy-based denoising methods}
% Energy-based models~\cite{lecun2006tutorial}(EBMs) capture dependencies between variables by associating scalar energy values with each configuration of the input. EBMs have been applied in various fields, such as generative modeling\cite{DBLP:conf/nips/DuM19}, out-of-distribution detection\cite{DBLP:conf/nips/LiuWOL20}, open-set classification\cite{al2022energy}, and Incremental Learning\cite{DBLP:conf/aaai/WangMHWSH23}. EBMs can measure relationships between personalized federated learning parameters by using energy as a metric. When selecting personalized experts in Mixture of Experts (MoE) models, this approach can filter out experts that offer no gain, effectively denoising the model. However, the application of EBMs in this context, particularly for expert denoising within MoE, remains underexplored.


% Energy-based models~\cite{lecun2006tutorial} (EBMs) capture variable dependencies by assigning scalar energy values to each input configuration. EBMs have been applied across various domains, including generative modeling~\cite{DBLP:conf/nips/DuM19}, out-of-distribution detection~\cite{DBLP:conf/nips/LiuWOL20}, open-set classification~\cite{al2022energy}, and incremental learning~\cite{DBLP:conf/aaai/WangMHWSH23}. In personalized federated learning, EBMs are able to quantify relationships between model parameters using energy as a metric. For selecting personalized experts in Mixture of Experts (MoE) models, EBMs can filter out ineffective experts, thus denoising the model. Despite its potential, the use of EBMs for expert denoising in MoE remains underexplored.

Energy-based models (EBMs)~\cite{lecun2006tutorial} assign scalar energy values to input configurations, capturing variable dependencies. They have been applied in generative modeling~\cite{DBLP:conf/nips/DuM19}, out-of-distribution detection~\cite{DBLP:conf/nips/LiuWOL20,fan2022episodic}, open-set classification~\cite{al2022energy}, and incremental learning ~\cite{DBLP:conf/aaai/WangMHWSH23}. In personalized federated learning, EBMs quantify relationships between model parameters using energy as a metric. For Mixture of Experts (MoE) models, EBMs can filter ineffective experts, denoising the model. However, their use for expert denoising in MoE remains underexplored.