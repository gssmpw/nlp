%%
%% This is file `sample-authordraft.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `authordraft')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-authordraft.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
% \documentclass[sigconf,authordraft]{acmart}
% \documentclass[sigconf,review,anonymous]{acmart}
\documentclass[sigconf]{acmart}
% \documentclass[manuscript, screen, review]{acmart}
% \usepackage{lineno}
%\usepackage{lineno}
%% NOTE that a single column version may required for 
%% submission and peer review. This can be done by changing
%% the \doucmentclass[...]{acmart} in this template to 
%% \documentclass[manuscript,screen]{acmart}
%% 
%% To ensure 100% compatibility, please check the white list of
%% approved LaTeX packages to be used with the Master Article Template at
%% https://www.acm.org/publications/taps/whitelist-of-latex-packages 
%% before creating your document. The white list page provides 
%% information on how to submit additional LaTeX packages for 
%% review and adoption.
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}


% \usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[utf8]{inputenc}
% \usepackage{caption}
% \usepackage{graphicx}
% \usepackage{amsmath,amsthm,amsfonts}
\usepackage{amsthm}
% \usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
% \usepackage[switch]{lineno}
\usepackage{multirow}
\usepackage{enumitem}
% \usepackage{amsmath}
\usepackage{subcaption}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
% \setcopyright{acmlicensed}
% \copyrightyear{2024}
% \acmYear{2024}
% \acmDOI{XXXXXXX.XXXXXXX}


\copyrightyear{2025}
\acmYear{2025}
\setcopyright{acmlicensed}
\acmConference[WWW '25] {Proceedings of the ACM Web Conference 2025}{April 28--May 2, 2025}{Sydney, NSW, Australia.}
\acmBooktitle{Proceedings of the ACM Web Conference 2025 (WWW '25), April 28--May 2, 2025, Sydney, NSW, Australia}
\acmISBN{979-8-4007-1274-6/25/04}
\acmDOI{10.1145/3696410.3714561}

% \settopmatter{printacmref=false} 
\settopmatter{printacmref=true}
% Removes citation information below abstract
% \renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column
% \pagestyle{plain} % removes running headers 
% \setcopyright{none}



%% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Conference acronym 'XX]{Make sure to enter the correct
%   conference title from your rights confirmation emai}{June 03--05,
%   2018}{Woodstock, NY}
%
%  Uncomment \acmBooktitle if th title of the proceedings is different
%  from ``Proceedings of ...''!
%
%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%  June 03--05, 2018, Woodstock, NY} 
% \acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
% \acmSubmissionID{2024-4213}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%
% \settopmatter{printacmref=false}
% \renewcommand\footnotetextcopyrightpermission[1]{}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{PM-MOE: Mixture of Experts on Private Model Parameters for Personalized Federated Learning}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
% \author{Ben Trovato}
% \authornote{Both authors contributed equally to this research.}
% \email{trovato@corporation.com}
% \orcid{1234-5678-9012}
% \author{G.K.M. Tobin}
% \authornotemark[1]
% \email{webmaster@marysville-ohio.com}
% \affiliation{%
%   \institution{Institute for Clarity in Documentation}
%   \streetaddress{P.O. Box 1212}
%   \city{Dublin}
%   \state{Ohio}
%   \country{USA}
%   \postcode{43017-6221}
% }

\author{Yu Feng}
\affiliation{%
  \institution{Beijing Univ. of Posts and Telecomm.}
  % \streetaddress{1 Th{\o}rv{\"a}ld Circle}
  \city{Beijing}
  \country{China}
  }
\email{fydannis@bupt.edu.cn}


\author{Yangli-ao Geng}
\affiliation{%
  \institution{Beijing Jiaotong University.}
  % \streetaddress{1 Th{\o}rv{\"a}ld Circle}
  \city{Beijing}
  \country{China}
  }
\email{gengyla@bjtu.edu.cn}

\author{Yifan Zhu}
\authornote{Corresponding author.}
\affiliation{%
  \institution{Beijing Univ. of Posts and Telecomm.}
  % \streetaddress{1 Th{\o}rv{\"a}ld Circle}
  \city{Beijing}
  \country{China}
  }
\email{yifan_zhu@bupt.edu.cn}

\author{Zongfu Han}
\affiliation{%
  \institution{Beijing Univ. of Posts and Telecomm.}
  % \streetaddress{1 Th{\o}rv{\"a}ld Circle}
  \city{Beijing}
  \country{China}
  }
\email{michan325@bupt.edu.cn}

\author{Xie Yu}
\affiliation{%
  \institution{Beijing Univ. of Aeronautics and Astronautics.}
  % \streetaddress{1 Th{\o}rv{\"a}ld Circle}
  \city{Beijing}
  \country{China}
  }
\email{yuxie_scse@buaa.edu.cn}


\author{Kaiwen Xue}
\affiliation{%
  \institution{Beijing Univ. of Posts and Telecomm.}
  % \streetaddress{1 Th{\o}rv{\"a}ld Circle}
  \city{Beijing}
  \country{China}
  }
\email{xkw@bupt.edu.cn}

\author{Haoran Luo}
\affiliation{%
  \institution{Beijing Univ. of Posts and Telecomm.}
  % \streetaddress{1 Th{\o}rv{\"a}ld Circle}
  \city{Beijing}
  \country{China}
  }
\email{luohaoran@bupt.edu.cn}


\author{Mengyang Sun}
\affiliation{%
  \institution{Tsinghua University}
  % \streetaddress{1 Th{\o}rv{\"a}ld Circle}
  \city{Beijing}
  \country{China}
  }
\email{sunmy19@mails.tsinghua.edu.cn}

\author{Guangwei Zhang}
\affiliation{%
  \institution{Beijing Univ. of Posts and Telecomm.}
  % \streetaddress{1 Th{\o}rv{\"a}ld Circle}
  \city{Beijing}
  \country{China}
  }
\email{gwzhang@bupt.edu.cn}

\author{Meina Song}
\affiliation{%
  \institution{Beijing Univ. of Posts and Telecomm.}
  % \streetaddress{1 Th{\o}rv{\"a}ld Circle}
  \city{Beijing}
  \country{China}
  }
\email{mnsong@bupt.edu.cn}

%% You do not have to enter your paper ID
\renewcommand{\shortauthors}{Yu Feng et al.}
%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
% \renewcommand{\shortauthors}{Trovato and Tobin, et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}

% To address the statistical heterogeneity of data in federated learning, research on personalized federated learning has made notable progress. To generate personalized models that better match the data domain, model-split-based personalized federated learning algorithms divide the model into a globally shared part and a locally private part. However, optimizing the local model while aggregating makes it challenging to effectively utilize the personalized knowledge from various clients. The locally private parameters after model convergence better represent the knowledge of the data domain. To overcome these limitations, we propose a personalized model parameter mixing expert (PM-MOE) method. Notably, this architecture features a two-phase training process, allowing each client to autonomously select the personalized model parameters converged by other clients. With only a few training iterations, PM-MOE can enhance a range of model-split-based personalized federated learning algorithms. Additionally, we conducted extensive experiments on six widely used datasets, demonstrating the superiority of our proposed method across two data splitting modes. The source code is available at \url{https://anonymous.4open.science/r/PM-MOE-8315}.


Federated learning (FL) has gained widespread attention for its privacy-preserving and collaborative learning capabilities. Due to significant statistical heterogeneity, traditional FL struggles to generalize a shared model across diverse data domains. Personalized federated learning addresses this issue by dividing the model into a globally shared part and a locally private part, with the local model correcting representation biases introduced by the global model. Nevertheless, locally converged parameters more accurately capture domain-specific knowledge, and current methods overlook the potential benefits of these parameters. To address these limitations, we propose PM-MoE architecture. This architecture integrates a mixture of personalized modules and an energy-based personalized modules denoising, enabling each client to select beneficial personalized parameters from other clients. We applied the PM-MoE architecture to nine recent model-split-based personalized federated learning algorithms, achieving performance improvements with minimal additional training. Extensive experiments on six widely adopted datasets and two heterogeneity settings validate the effectiveness of our approach. The source code is available at \url{https://github.com/dannis97500/PM-MOE}.


\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010919</concept_id>
       <concept_desc>Computing methodologies~Distributed computing methodologies</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Distributed computing methodologies}
%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Personalized Federated Learning; Mixture of Experts; Energy-based denoising}

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
% \begin{teaserfigure}
%   \includegraphics[width=\textwidth]{sampleteaser}
%   \caption{Seattle Mariners at Spring Training, 2010.}
%   \Description{Enjoying the baseball game from the third-base
%   seats. Ichiro Suzuki preparing to bat.}
%   \label{fig:teaser}
% \end{teaserfigure}

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle


\section{Introduction}


The success of modern methods~\cite{kirillov2023segment, ouyang2022training, dai2022can, fan2022dtr,yu2024anchor} is largely driven by the growing availability of training data~\cite{lecun2015deep, krizhevsky2012imagenet, hinton2012deep, tian2022tcvm}. Unfortunately, there are still vast amounts of isolated data remain underutilized due to strict privacy requirements~\cite{regulation2016regulation, de2018guide}. As a result, federated learning (FL)~\cite{DBLP:conf/aistats/McMahanMRHA17, DBLP:series/synthesis/2019YangLCKCY, DBLP:journals/inffus/BarrosoSJRMGLVH20, adnan2022federated, DBLP:conf/aaai/LiuHLHLCFCYY20, DBLP:journals/corr/abs-1811-03604}, has gained significant attention for its strong privacy protection and collaborative learning capabilities. This innovative paradigm allows multiple clients to collaboratively train models, where the server only aggregates models and keep private data remaining on each client. Despite its effectiveness, traditional FL methods suffer from performance degradation due to statistical heterogeneity~\cite{DBLP:journals/ftml/KairouzMABBBBCC21}—data domains on each client are biased, with uneven class distributions, varying sample sizes, and significant feature differences.

Personalized federated learning (PFL)~\cite{DBLP:conf/pkdd/LiZSLS21, DBLP:conf/ijcai/0010W22, li2021fedbnfederatedlearningnoniid, DBLP:conf/aaai/TanLLZ00Z22} alleviates this limitation by allowing each client to better fit local data. Specifically, PFL methods focus on balancing local personalization with global consistency by splitting models into global and personalized modules~\cite{DBLP:conf/iclr/OhKY22, DBLP:conf/kdd/ZhangHWSXMG23}, where personalized modules capture unique local data characteristics, mitigating global model biases and better adapting to individual client data.
Recent efforts have been developed based on meta-learning~\cite{DBLP:conf/nips/0001MO20}, regularization~\cite{DBLP:conf/nips/DinhTN20, DBLP:conf/icml/00050BS21}, model splitting~\cite{DBLP:journals/corr/abs-1912-00818}, 
knowledge distillation~\cite{seo202216, DBLP:journals/corr/abs-2006-16765, wu2022communication, DBLP:conf/aaai/TanLLZ00Z22, DBLP:conf/iclr/XuTH23}, and personalized aggregation~\cite{DBLP:conf/pkdd/LiZSLS21, DBLP:conf/ijcai/0010W22, DBLP:conf/aaai/ZhangHWSXMG23}. 

\begin{figure}[tbp]
    \centering
    \includegraphics[width=0.5\textwidth]{images/fig1.png}
    \vspace{-0.3cm}
    \caption{
    Motivation of our study. (A) t-SNE graph shows the inference effects of different models on the same set of data. (B) Client A gets closer to the target when using Client B's personalized model, but moves farther from the target when using Client C's personalized model.
} 
    \label{fig:phenomenon}
    \vspace{-0.3cm}
\end{figure}

% \begin{figure}[tbp]
%     \centering
%     \includegraphics[width=0.5\textwidth]{images/fig1_2.png}
%     % \vspace{-0.2cm}
%     \caption{
%     Client A gets closer to the target when using Client B's personalized model, but moves farther from the target when using Client C's personalized model.} 
%     \label{fig:motivation}
%     \vspace{-0.4cm}
% \end{figure}


Given that the same types of data can be distributed across multiple clients, then a key question arises: \textbf{\textit{"Can personalized modules from different clients mutually enhance each other's performance?"}}, which is overlooked in current PFL methods. To investigate, we conducted experiments based on the state-of-the-art PFL approaches. 
We randomly select a client, and several data categories which distributed across different clients. Subsequently, we trained in both centralized and personalized federated learning manner. By comparison, We evaluated whether integrating personalized parameters from other clients could improve model's representation capability. 
As illustrated in Figure~\ref{fig:phenomenon}~(A), the selected client indeed benefited from the personalized modules from another client.

%As shown in Figure , globally trained models often fall short of desired performance. A client’s personalized model can reduce some global model biases, and adopting personalized models from other clients may either enhance or degrade performance. We further fused beneficial personalized models with the local personalized model and visualized t-SNE data representations, revealing that appropriate utilization of personalized models leads to performance gains.

% Driven by the above analysis, we explore how personalized modules from different clients can mutually enhance performance. In model-split-based PFL, all clients share a global model while using personalized modules to adjust for local data biases. However, not all personalized modules positively impact a given client. Therefore, we propose two key principles for model design: 1) dynamically weighting personalized modules based on input, and 2) filtering out modules that introduce negative impacts.

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.96\textwidth]{images/fig2.png}
    \caption{ 
    Overall Architecture of Personalized Model parameters with Mixture of Experts}
    \label{fig:overall-structure}
    \vspace{-0.3cm}
\end{figure*}

Driven by the above analysis, in this paper, we aim to explore how personalized modules from different clients can mutually enhance performance. As illustrated in Figure~\ref{fig:phenomenon}~(B), all clients utilize the same global model, while applying personalized module to debias according to the local data domain. For a single client, not all personalized modules contribute positively to the final representation. Therefore, we leverage two basic principles when designing our model: 
1) Dynamically weighting the effect of personalized modules based on the current input.
2) Filtering modules that exhibit negative effects;


In this paper, we introduce PM-MoE, a two-stage personalized federated leanring framework based on mixture of experts (MoE) architecture~\cite{titsias2002mixture, masoudnia2014mixture}. 
In the first stage, we pretrained models to get global and personalized modules;
In the second stage, we proposed the mixture of personalized modules method (MPM) and the energy-based denoising method(EDM) to make the personalized modules from different clients enhance each other.
\textbf{With the first principle}, PM-MoE employs the MPM based on MoE gate selection. \textbf{With the second principle}, PM-MoE incorporates the EDM to filter out noisy personalized models. Together, these two componets enable personalized modules from different clients to mutually enhance each other. Additionally, sharing converged personalized parameters will not break privacy requirements due to there is no gradient leakage during training. We evaluated PM-MoE on nine SOTA PFL benchmarks across six popular federated learning datasets. The experimental results demonstrate PM-MoE consistently improves the performance of various PFL methods.




In summary, we conclude our contributions as follows:
\begin{itemize}
[leftmargin=*,itemsep=0pt,parsep=0.5em,topsep=0.3em,partopsep=0.3em]
  \item We propose PM-MOE, a novel two-stage framework for personalized federated learning which exchanges personalized knowledge across clients. In the first stage, the PM-MOE pretrains PFL models, followed by a fine-tuning stage for knowledge exchanges.
  % We propose PM-MoE, a novel framework that enables cross-client utilization of personalized knowledge through a two-stage process: pre-training in personalized federated learning and local MoE fine-tuning. This approach effectively leverages independent personalized models in heterogeneous data environments.
  \item Specifically, PM-MOE employs a simple MOE structure to dynamically weighting the contribution of different personalized modules. Besides, PM-MOE introduces an energy-based denosing method to filter those clients with negative effects.
  % We introduce an energy-based denoising method that filters out noisy personalized models, allowing the framework to focus on models that provide performance gains.
  \item We conduct extensive experiments to nine state-of-art PFL methods across six datasets. The experimental results demonstrate PM-MOE’s consistently improvement on various settings.
  % We apply PM-MoE to nine state-of-the-art PFL algorithms and conduct extensive experiments on six widely-used datasets, demonstrating the scalability and effectiveness of the framework.
\end{itemize}

% As people increasingly value the protection of private data, legislation on privacy protection, such as the General Data Protection Regulation\cite{regulation2016regulation} (GDPR) in Europe and the California Consumer Privacy Act\cite{de2018guide} (CCPA) in the United States, has also been refined. This significantly hinders the development of centralized data collection followed by training robust AI models. Federated learning excels in privacy protection through distributed collaborative modeling. In traditional federated learning, exemplified by FedAvg\cite{DBLP:conf/aistats/McMahanMRHA17}, clients store private data locally while the server aggregates the models trained by these clients. 
% Over iterative convergence, this approach can achieve results comparable to centralized modeling. However, in practice, the data domains of each client exhibit certain biases—varying sample labels, counts, and features within the same category. 
% These issues are collectively referred to as statistical heterogeneity.


% To address the statistical heterogeneity\cite{DBLP:journals/ftml/KairouzMABBBBCC21} faced by traditional federated learning, personalized federated learning has emerged. 
% Since the single global model trained in traditional federated learning cannot meet the needs of every client, personalized federated learning allows each client to adjust according to their data distribution, generating models better suited to their data domain. 

% Recent research in personalized federated learning encompasses various approaches, including meta-learning\cite{DBLP:conf/nips/0001MO20}, regularization\cite{DBLP:conf/nips/DinhTN20}\cite{DBLP:conf/icml/00050BS21}, model splitting\cite{DBLP:journals/corr/abs-1912-00818}, knowledge distillation\cite{seo202216}\cite{DBLP:journals/corr/abs-2006-16765}\cite{wu2022communication}\cite{DBLP:conf/aaai/TanLLZ00Z22}\cite{DBLP:conf/iclr/XuTH23}, and personalized aggregation\cite{DBLP:conf/pkdd/LiZSLS21}\cite{DBLP:conf/ijcai/0010W22}\cite{DBLP:conf/aaai/ZhangHWSXMG23}. 
% Among these, model-splitting-based personalized federated learning, such as FedBABU\cite{DBLP:conf/iclr/OhKY22} and FedCP\cite{DBLP:conf/kdd/ZhangHWSXMG23}, has made significant strides, emphasizing the balance between personalization and global consistency through model splitting.

% Nevertheless, model-splitting-based personalized federated learning faces two core challenges: (1) How can the server better aggregate the personalized knowledge from each client's data domain? The prevalent approach is to set aside certain non-uploaded personalized parameters at the client side, which are initialized locally and gradually converge during the personalized federated learning process. However, the converged personalized model parameters do not facilitate knowledge transfer to other clients. (2) How can clients better utilize and filter personalized knowledge? Existing personalized federated learning attempts to balance global model parameters and local parameter weights, learning knowledge from other domains through a homogeneous global model.

% Nevertheless, model-splitting-based personalized federated learning still faces two core challenges. 


% \textbf{(1) How can the server effective collect and aggregate personalized knowledge from each client’s data domain?} 
% The widely adopted approach is to designate certain personalized parameters on the client side that are not uploaded to the server. 
% These parameters are initialized on each client and gradually converge during the personalized federated learning process. 
% However, the converged personalized model parameters do not transfer knowledge to other clients. 
% \textbf{(2) How can clients effective utilize or select personalized knowledge?} 
% Current personalized federated learning methods balance global model parameters and local parameters by using a homogeneous global model to learn from other domains. 
% Consequently, clients still encounter the challenge of learning heterogeneous data with a homogeneous model due to the limitations of the server aggregation algorithm.

% To address the above challenges, we propose the PM-MOE paradigm, a framework adaptable to all model-splitting-based personalized federated learning architectures. As shown in Figure \ref{fig:motivation}, the PM-MOE architecture is exemplified on FedCP. In this structure, there is a globally shared feature extractor and a global header. Compared to the original FedCP approach, our method introduces an additional personalized header pool and a gating network on client M. The global feature extractor projects raw data into a high-dimensional space, generating vectors. These vectors are then processed by the gating network, which computes expert weights. These weights are used to combine the results of each expert, and the final output is merged with the global header's result to produce the prediction.



% To address the aforementioned challenges, this paper proposes the PM-MOE paradigm, an architecture adaptable to all model-splitting-based personalized federated learning approaches. 
% As shown in Figure \ref{fig:motivation}, the data distribution of each client varies significantly and remains isolated. 
% Each client then utilizes a gated network to assign weights to the personalized model converged in local training, enabling efficient knowledge transfer. 
% This design effectively restructures existing model-splitting-based personalized federated learning methods.



% The PM-MOE structure requires only a small number of local training iterations to effectively transfer knowledge between personalized parameters across clients, thereby enhancing each client's model performance. 
% \textbf{To address the first challenge}, the server builds a personalized parameter pool to store converged personalized model parameters. On the one hand, these parameters retain key characteristics of heterogeneous data distributions. 
% On the other hand, sharing these converged parameters avoids gradient leakage, thereby enhancing privacy protection. 
% \textbf{To address the second challenge while maintaining scalability}, we propose two integration methods for personalized parameters: Mixture of Personalization Parameters and Mixture of Personalization Experts. 
% By freezing the existing model parameters, clients locally train additional gating parameters to reweight personalized parameters across all clients, allowing each client to effectively select the most relevant parameters for its own data domain. In summary, our contributions are as follows:

% \begin{itemize}
% [leftmargin=*,itemsep=0pt,parsep=0.5em,topsep=0.3em,partopsep=0.3em]
%   \item We introduce the PM-MOE architecture, which promotes personalized knowledge transfer among clients through a two-stage approach of pre-training personalized federated learning and local MOE fine-tuning. This architecture offers a novel method for knowledge transfer in the face of statistical heterogeneity by aggregating homogeneous models to learn from various client data domains.
%   \item We propose a personalized parameter pool for collecting converged personalized parameters from clients. The combination of Mixture of Personalization Parameters and Mixture of Personalization Experts enables clients to autonomously select model parameters that benefit their local data. 
%   \item We apply the PM-MOE architecture to nine recent state-of-the-art personalized federated learning algorithms, conducting extensive experiments on six widely used datasets to validate its superiority.
% \end{itemize}

% Pre-trained models have shown extensive generalization by fine-tuning on multiple downstream tasks~\cite{DBLP:journals/corr/abs-2302-09419}.
% However, this observation holds an assumption that the feature distributions of datasets are consistent. 
% Performance degradation usually occurs when the pre-trained model makes inference on a new dataset with a different feature distribution, known as data domain, under the same task setting ~\cite{hadsell2020embracing}. 
% A vivid example is shown in Figure \ref{fig:motivation}, where the learning model was firstly trained with quickdraw-style pictures, but then tested to classify the same object under different styles, such as infographics, painting, and clipart. 
% To overcome this issue, the pre-trained model needs to continuously learn the new arriving domain data, but inevitably forgets the knowledge learned from the old data domain, leading to catastrophic forgetting~\cite{mcclelland1995there,mccloskey1989catastrophic}. 
% Thus, in this domain-incremental learning (DIL) scenario, how to make the learning model sequentially learn new domains while keeping the performance on existing domains becomes the key challenge to be addressed.


% Ensuring the learning efficiency of the model in a new domain is also challenging in DIL.
% It is obvious that re-training the learning model with the entire datasets including the new domain is usually neither efficient nor privacy-preserving ~\cite{DBLP:conf/nips/AljundiLGB19,DBLP:conf/cvpr/BangKY0C21}. 
% Recent prompt learning methods such as L2P \cite{DBLP:conf/cvpr/0002ZL0SRSPDP22} and S-liPrompt \cite{DBLP:conf/nips/WangHH22} propose to learn a tiny set of parameters, namely prompts, to instruct the pre-trained model to learn the arriving domain sequentially without tuning model's parameters, thereby getting rid of dependence on historical domain data as well as low-efficient fine-tuning process.
% %这种详细介绍某种方法的，统统改在related work里面
% %Recent prompt learning methods such as L2P~\cite{DBLP:conf/cvpr/0002ZL0SRSPDP22} construct a prompt pool using key-value pairs, utilizing a query mechanism to determine which prompts to select for the current task. However, this method struggles to find suitable prompt key-value pairs and, within a fixed model structure, can only choose a fixed number of prompts, resulting in a loss of prompt knowledge.
% %Another effective solution, S-liPrompt~\cite{DBLP:conf/nips/WangHH22}, designs personalized prompts for each domain's data and selects prompts using features extracted during model training. This approach demonstrates significant performance improvement when domain feature differences are substantial. 
% However, they still fail to effectively utilize the shared information between domains since the total number of categories in domain-incremental tasks remain constant. 
% The shallow knowledge derived by current prompts neglects the fine-grained semantics which is contained in transformer layers in the pre-trained models. 
% Thus, a prompt learning strategy that considers both commonalities across multiple domains and deep intra-domain multi-layer fine-grained semantics is urgently needed for the pre-trained model in DIL.

% To this end, in this paper, we present a prompt learning framework, namely CP-Prompt, to instruct a pre-trained model to learn on incremental data domains with different feature distributions. 
% CP-Prompt follows the ``divide and rule" policy, that is, it aims to significantly improve model performance within each domain by prompting, and smartly selecting appropriate domains alongside well-trained prompts. 
% As Figure \ref{fig:motivation} depicts, CP-Prompt adopts a twin-prompt strategy: a parameter-shared prompt for learning commonalities is sequentially trained on data samples in the new arriving domain, thereby guiding the pre-train model to generalize among data domains.
% The other prompt is embedded within the multiple layers of the pre-trained model's multi-head attention, to extract domain-personalized latent semantic knowledge.
% By incorporating this twin-prompt together, the pre-trained can continually learn without tuning its original parameters, and generalize new data samples even if its feature distribution is unclear between domains. 
% The contributions of this paper are summarized as follows:
% \begin{itemize}
% [leftmargin=*,itemsep=0pt,parsep=0.5em,topsep=0.3em,partopsep=0.3em]
%   \item We present a simple yet effective prompt tuning framework CP-Prompt for domain-incremental learning, with a parameter-efficient twin-prompting design that significantly alleviates catastrophic forgetting across data domains.
%   \item We further design a common prompt for guiding shared knowledge among domains and a personalized Prefix-One prompt embedded in the key and value of the multi-head self-attention for guiding the intra-domain semantics learning.
%   \item CP-Prompt is evaluated on three widely used DIL benchmark datasets and outperforms existing state-of-the-art sample-free baselines. Furthermore, only minimal additional parameters (0.22\%) are tuned by CP-Prompt, and gaining at even 5.8\% improvement, showing its effectiveness in both parameter efficiency and model accuracy.
% \end{itemize}

% \section{Definitions in PFL}
\vspace{-0.3cm}
\section{Notations and Preliminaries}
\subsection{Notations}
In PFL, $M$ clients share the same model structure. Here, we denote notations following FedGen~\cite{venkateswaran2023fedgen} and FedRep~\cite{husnoo2022fedrep}. Each client is denoted as $C^j(j\in1,2,...,M)$, having its own data domain $\mathcal{D}^j$ with $N^j$ samples~$(j\in1,2,...,M)$. The data distribution of  $\mathcal{D}^j$ is denoted as $P^j$. Specifically, $\mathcal{D}^j={\{x_i^j,y_i^j\}}_{i=1}^{N^j}$, where $i$ is the number of training samples. $x_i$ is the $i$-th data sample and $y_i$ is its corresponding label. Each client $C^j$ in PFL has two modules: the global module and the personalized module, which is denoted as $W_g^j$ and $W_p^j$ respectively.

\vspace{-0.3cm}
\subsection{Preliminaries}
% This section defines the objective function of model-splitting-based personalized federated learning. 
% Each client $C^j(j\in1,2,...,M)$ has $N^j$ samples. The training data for client $C^j$ is $\mathcal{D}^j={\{x_i^j,y_i^j\}}_{i=1}^{N^j}$. The data distribution $P^j$ for client $C^j$ varies across clients (the local data of different clients are non-IID). 
In a typical PFL method, there is a centralized server who firstly aggregates clients' global modules~$\left\{ {W_{g}^{1},W_{g}^{2},...,W_{g}^{M}} \right \}$, and then distributes the aggregated module $W_g$ to each client. Therefore, each client is required to firstly train on $\mathcal{D}^j$ and upload their $W_g^j$ every $E_l$ iteration.
% The global module is updated as $W_g^j\gets W_g^j-\eta\nabla l\left(W_g^j;x_i^j,y_i^j\right)$, and the personalized module is updated as $W_p^j\gets W_p^j-\eta\nabla l\left(W_p^j;x_i^j,y_i^j\right)$.
% During a global training round $t\in(1,2,...,E_g)$, client $C^j$ trains on local data $D^j$ for $E_l$ iterations. 
% Client $C^j$ then uploads $W_g^j$ to the server, which aggregates the global models ${W_{g}^{1},W_{g}^{2},...,W_{g}^{M}}$ using an aggregation function $f$. 
The sever aggregates global modules by the function $f$ as:
\setlength\abovedisplayskip{1pt}
\setlength\belowdisplayskip{1pt}
\begin{equation}
\begin{aligned}
W_g=\frac{1}{N} \sum_{j=1}^{M}{{N^j}f(W_{g}^{j})},
\end{aligned}
\end{equation}
where $N=\sum_{j=1}^{M}N^j$ and $f$ can be algorithms like FedAvg~\cite{DBLP:conf/aistats/McMahanMRHA17}, FedProx~\cite{DBLP:journals/network/LuoHSOHD23}, etc. After aggregation, the server sends $W_g$ to client $C^j$. Then, client $C^j$ enters the next training. Therefore, the objective loss function $\mathcal{L}$ for the entire personalized federated learning task is as follows:
\begin{equation}
\begin{aligned}
&\min_{W_g^j, W_p^j} \mathcal{L} = \min \sum_{j=1}^{M} \mathbb{E}_{(x^j,y^j) \sim P^j} [L^j(x^j, y^j; W_g^j, W_p^j)]. \\
\end{aligned}
\end{equation}
Here, $L^j$ is the loss function for client $C^j$.

\vspace{-0.5cm}

\section{Method}
\subsection{The PM-MOE Overall Framework}
In this section, we introduce the overall framework of PM-MOE, which
which is a two-stage training framework. Specifically, our contributions lie in the mixture of personalized modules~(MPM) and an energy-based denoising method~(EDM). The MPM addresses the challenge of effectively utilizing personalized models, while the EDM method removes those personalized models with negative effets.
% In this section, we provide an overview of our approach PM-MOE, which fine-tunes local data using a pre-trained personalized parameter pool and a local mixture-of-experts model. Since the overlap in data distribution varies among clients, each client should proactively select personalized knowledge after convergence. This design allows each client to more effectively select a personalized model tailored to its local data distribution.

% Simply put, as shown in Figure \ref{fig:overall-structure}, the overall architecture is divided into two parts: the pre-training phase of personalized federated learning and the PM-MOE training phase. The pre-training phase aims to obtain each client's converged personalized model, thereby constructing a personalized prompt pool. The PM-MOE training phase focuses on enabling each client to select the personalized model that best supports decision-making. The following sections will provide a detailed explanation of these two key phases.
The training process of PM-MOE is divided into two steps, as shown in Figure~\ref{fig:overall-structure}. 
In pre-training step, we train model and obtain its converged global and personalized modules for each client, thereby constructing a personalized prompt pool.
In PM-MOE step, we leverage the proposed MPM and EDM to select the 
optimal combination among personalized modules for each client. 
The following sections provide a detailed explanation of these two key phases.



\paragraph{Phase 1: Pre-training.}
The statistically heterogeneous distribution data $\mathcal{D}^j$ of client $C^j$ is mapped to the feature space $x_{g,rep}^j$ through the global feature extractor $f_g:\mathbb{R}^U\rightarrow\mathbb{R}^D$, and to the feature space $x_{p,rep}^j$ via the personalized feature extractor $f_p:\mathbb{R}^U\rightarrow\mathbb{R}^D$. The weighted aggregated feature space $x_{rep}^j=x_{g,rep}^j+x_{p,rep}^j$ is then mapped to the corresponding label space through the global classifier $s_g:\mathbb{R}^D\rightarrow\mathbb{R}^C$ and the personalized classifier $s_p:\mathbb{R}^D\rightarrow\mathbb{R}^C$. $U$, $D$ and $C$ represent the input space, feature space, and label space, respectively. 
\begin{equation}
\begin{aligned}
x_{rep}^j=f_g\left(W_{g,fe}^j,x^j\right)+f_p\left(W_{p,fe}^j,x^j\right).
\end{aligned}
\end{equation}
Additionally, as seen in DBE~\cite{DBLP:conf/nips/ZhangHCWSXMG23}, there exists a personalized vector parameter ${{PP}^j\in\mathbb{R}}^D$ to correct the local data distribution. The associated expressions are as follows:



\begin{equation}
\begin{aligned}
{\hat{y}}^j=s_g\left(W_{g,hd}^j,h^j\right)+s_p\left(W_{p,hd}^j,h^j\right)+{PP}^j.
\label{eq:common_prompt}
\end{aligned}
\end{equation}

During training, the global model parameters $W_{g,fe}^j$ and $W_{g,hd}^j$ are uploaded to the server for aggregation, while the personalized model parameters $W_{p,fe}^j,W_{p,hd}^j$ and ${PP}^j$ are computed locally and not uploaded. After the global training process with $E_g$ epochs, the model converges.

\paragraph{Phase 2: PM-MOE Fine-Tuning.}
% After the convergence of the model-splitting-based series, the server collects the trained personalized model parameters to form a personalized parameter pool. The server then distributes this parameter pool to each client. Each client subsequently trains a gating network to assign weights to each personalized parameter, effectively leveraging knowledge from all clients. PM-MOE is a flexible adaptation component designed to accommodate various model-splitting-based personalized federated learning algorithms, featuring two adaptation methods: MPP and MPE.

First, after the convergence of the model-splitting-based series of models, the server collects the trained personalized model parameters to form a personalized parameter pool, which is then distributed to each client. Next, each client locally trains a gating network, which assigns weights to each personalized model based on the input data, thereby effectively utilizing the personalized knowledge from all clients. For detailed information, refer to Section~\ref{sec:mpm}. Finally, since some of the personalized knowledge from other clients is irrelevant to the local data distribution, training the gating network with these noisy parameters can hinder convergence. To address this, we designed an energy-based denoising method. For further details, see Section ~\ref{sec:epd}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.40\textwidth]{images/fig4.png}
    % \vspace{-0.3cm}
    \caption{Diagram of Mixture of Personalized Parameters.}
    \label{fig:mpp}
    \vspace{-0.3cm}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.45\textwidth]{images/fig3.png}
    
    \caption{Diagram of Mixture of Personalized Experts.}
    \label{fig:mpe}
    \vspace{-0.3cm}
    \setlength{\belowcaptionskip}{5mm}
\end{figure}





\subsection{Mixture of Personalized Modules}
\label{sec:mpm}
PM-MOE is a flexible architecture, and to accommodate the complex and diverse model-splitting-based personalized federated learning algorithms, we designed two adaptation methods: MPP and MPE, as shown in Figures \ref{fig:mpp} and \ref{fig:mpe}.


Assume that a personalized federated learning algorithm involves personalized parameters, these parameters do not project data into vectors of other dimensions. We define this type as local personalized parameters ($PP$). Suppose the personalized federated learning algorithm also involves personalized expert models, where the expert models $\mathcal{W}_p^j$ map data $\mathcal{D}^j$ to a new feature space. We define this type as local personalized experts ($PE$).
The server builds and synchronizes a set of personalized models. Depending on the type of personalized model, the server collects the converged model parameters from all clients, constructing a personalized parameter pool $\mathcal{W}_{PP}={\{{PP}^j\}}_{j=1}^M$ and a personalized expert pool $\mathcal{W}_{PE}={\{W_{PE}^j\}}_{j=1}^M$. The server then synchronizes these sets with all clients.


Clients build a gating network and fine-tune parameters. Since each personalized federated learning client is diverse, as shown in Figure 3, we divide the combination of the gating network and personalized models into two categories.
\textbf{The first is commonalities.} The calculation of set weights depends on the input data $x^j$. To achieve this, we construct gating networks $G_{PP}^j,G_{PE}^j$ for the personalized parameter and the personalized expert, with corresponding training parameters $\theta_{PP}^j,\ \theta_{PE}^j$. The weight calculations are formally represented as follows:
\begin{equation}
\begin{aligned}
\alpha_{PP}=G_{PP}^j\left(x^j,\theta_{PP}^j\right)
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
\alpha_{PE}=G_{PE}^j\left(x^j,\theta_{PE}^j\right)
\end{aligned}
\end{equation}
We then sort the weights calculated by formulas (1) and (2) in descending order. From the set, we select the top k parameters and construct the personalized parameter and expert subsets as $\{\alpha_{PP}^l\}=Top\left(k,\alpha_{PP}\right)$,$\{\alpha_{PE}^l\}=Top\left(k,\alpha_{PE}\right)$. Here, $l$ denotes the index of the selected clients, where $l\in[1,M]$.


\textbf{Second is Differences.} Since the personalized parameter pool $\mathcal{W}_{PP}$ does not process data, we directly compute the weighted sum of the personalized parameters, resulting in a vector with the same shape as the local personalized parameter ${PP}^j$ as follows:
\begin{equation}
\begin{aligned}
{PP}_{moe}^j=\mathcal{W}_{PP}^l\cdot{\alpha_{PP}^l}
\end{aligned}
\end{equation}
In our setting, the weighted vector $\{PP_{moe}\}^j$ replaces the local personalized parameter $\{PP\}^j$ on client $C^j$.
For the personalized expert set $\mathcal{W}_{PE}={{W_{PE}^j}}_{j=1}^M$, taking the personalized classifier $s_p$ as an example, each expert maps the data to a new feature space $h^l$, where:
\begin{equation}
\begin{aligned}
h^l\in\mathbb{R}^C=s_p^l(x^j,W_{PE}^l)
\end{aligned}
\end{equation}
We then compute the mixed weighted personalized parameter vector $x_{moe}^j$ as:
\begin{equation}
\begin{aligned}
x_{moe}^j=h^l\cdot{\alpha_{PE}^l}
\end{aligned}
\end{equation}
The client $C^j$ then replaces the output of the local personalized expert $W_{PE}^j$ with $x_{moe}^j\in\mathbb{R}^C$.

Since the converged parameters reflect the local data knowledge that each client has spent significant effort training, during the training process, the personalized parameter and expert sets $\mathcal{W}_{PP},\mathcal{W}_{PE}$ are frozen and not optimized together with the gating network.


\begin{table*}[htbp]
\small
\centering
\caption{Results of federated and personalized federated learning algorithms on six datasets with heterogeneous data distribution (Dirichlet distribution with $S=0$ and $S=20$). Bold: Best performance.}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c}
\specialrule{.16em}{0pt} {.65ex}
Spilt Type & \multicolumn{6}{c|}{$S=0$} & \multicolumn{6}{c}{$S=20$}     \\
\specialrule{.16em}{0pt} {.65ex}
Method  &MNIST  &FMNIST  &Cifar10 &Cifar100 &TINY &AGNews &MNIST  &FMNIST  &Cifar10 &Cifar100 &TINY &AGNews   \\

\specialrule{.16em}{0pt} {.65ex}
FedAvg   & 98.93	&88.64	&63.68	&32.94	&17.69	&62.40 &98.95	&90.69	&67.74	&35.37	&19.66	&71.68\\
FedProx  & 98.93	&88.50	&63.85	&33.07	&17.60	&65.75 &98.98	&90.79	&67.54	&35.42	&19.56	&72.90\\
SCAFFOLD & 99.12	&88.74	&64.19	&34.71	&19.67	&78.85 &99.18	&91.44	&70.40	&38.54	&19.67	&78.50\\
FedGEN   & 98.98	&88.79	&64.36	&32.72	&15.85	&63.13 &98.98	&90.90	&67.59	&34.52	&17.70	&71.65\\
MOON     & 98.92	&88.59	&63.87	&33.00	&17.57	&62.21 &98.98	&90.74	&67.53	&35.36	&17.57	&71.76\\
\specialrule{.16em}{0pt} {.65ex}
FedPer    & 99.49	&97.53	&89.90	&48.27	&36.36	&93.99 &98.62	&93.57	&76.67	&36.28	&25.91	&88.75\\
LG-FedAvg & 99.28	&97.25	&89.02	&47.03	&33.20	&94.33 &97.85	&92.23	&73.95	&35.90	&23.78	&87.77\\
FedRep   & 99.46	&97.58	&90.19	&49.44	&38.09	&93.78 &98.61	&93.77	&77.25	&36.52	&25.77	&89.02\\
FedRoD   & 99.68	&97.60	&90.07	&51.92	&38.90	&93.65 &99.33	&94.06	&79.50	&42.45	&29.07	&88.91\\
FedGH   & 99.29	&97.40	&84.50	&48.61	&25.80	&92.58 &97.94	&92.25	&73.79	&37.88	&21.51	&88.19\\
FedBABU   & 99.67	&97.74	&91.38	&50.83	&34.53	&92.87 &99.32	&94.71	&82.17	&40.46	&25.92	&87.58\\
GPFL    & 99.49	&94.91	&77.79	&57.41	&27.08	&90.84 &99.49	&93.21	&72.39	&49.01	&22.92	&82.41\\
FedCP   & 99.75	&98.31	&93.76	&69.83	&65.97	&92.40 &99.27	&94.45	&80.29	&41.79	&31.93	&87.62\\
DBE  & 98.17	&93.95	&89.11	&60.33	&38.29	&93.70 &96.97	&91.11	&79.76	&52.30	&31.11	&89.08\\
\specialrule{.16em}{0pt} {.65ex}
\textbf{PM-MOE}&\textbf{99.85} &\textbf{98.61} &\textbf{93.95} &\textbf{70.68} &\textbf{66.33} &\textbf{94.76} &\textbf{99.49} &\textbf{94.79} &\textbf{82.21} &\textbf{52.36} &\textbf{32.15} &\textbf{89.16}\\
\specialrule{.16em}{0pt} {.65ex}
\end{tabular}
\label{tab:table1}
\vspace{-0.3cm}
\end{table*}

% Assume that the personalized federated learning algorithm includes personalized parameters, which do not project data into vectors of other dimensions. We define this type as local personalized parameters ($PP$).

% As shown in Figure \ref{fig:pm-moe-mpp}, on the server side, the central server collects all clients' local personalized parameters to construct a personalized parameter pool $\mathcal{W}_{PP}=\{{PP}^1,{PP}^2,...,{PP}^j\},(j\in[M])$, which is then sent to all clients. On the client side, $C^j$ trains a gating network $G_{PP}^j$ with parameters $\theta_{PP}^j$ to compute the weights of the personalized parameter set $\alpha_{PP}$.

% \begin{equation}
% \begin{aligned}
% \alpha_{PP}=G_{PP}^j\left(x^j,\theta_{PP}^j\right)
% \end{aligned}
% \end{equation}

% \begin{equation}
% \begin{aligned}
% {\alpha_{PP}^l}=Top(k,\ \alpha_{PP})
% \end{aligned}
% \end{equation}

% Here, ${\alpha_{PP}^l}$ represents the set of the top $k$ weight values, with $l$ indicating the index of the selected client, $l\in[k]$. The selected weights are then applied to the filtered personalized parameter subset $\mathcal{W}_{PP}^l={\{PP^l\}}$ to compute the mixed weighted personalized parameter vector $x_{moe}^j$.

% \begin{equation}
% \begin{aligned}
% x_{moe}^j=\mathcal{W}_{PP}^l\cdot{\alpha_{PP}^l}
% \end{aligned}
% \end{equation}

% This $x_{moe}^j$ replaces the personalized parameter ${PP}^j$ on client $C^j$. During training, to retain each client's knowledge, all personalized parameters are frozen and not updated. Thus, the global objective loss for the Mixture of Personalization Parameters is as follows:


% \begin{equation}
% \begin{aligned}
% &\min_{\theta_{PP}^j} \mathcal{L} = \min \sum_{j=1}^{M} \mathbb{E}_{(x^j, y^j) \sim P^j} \left[ L^j(x^j, y^j; \theta_{PP}^j) \right] \\
% \end{aligned}
% \end{equation}








% \subsection{Mixture of Personalization Experts (MPE)}
% Additionally, assume the algorithm includes personalized expert models, where $\mathcal{W}_p^j$ maps the data $\mathcal{D}^j$ to a new feature space. We define this type as local personalized experts (PE). Such as the model parameters $W_{p,fe}$ of the personalized feature extractor $f_p$ and the model parameters $W_{p,hd}$ of the personalized classifier $s_p$.

% As shown in Figure \ref{fig:pm-moe-mpp}, in the MPE approach, the server $S$ collects all clients' local personalized experts to construct a personalized expert pool $\mathcal{W}_{PE}={{PE}^1,{PE}^2,...,{PE}^j},(j\in[M])$, which is then distributed to all clients. On the client side, $C^j$ trains a gating network $G_{PE}^j$ with parameters $\theta_{PE}^j$ to compute the weights $\alpha_{PE}$ of the personalized parameter set.

% \begin{equation}
% \begin{aligned}
% \alpha_{PE}=G_{PE}^j\left(x^j,\theta_{PE}^j\right)
% \end{aligned}
% \end{equation}

% \begin{equation}
% \begin{aligned}
% \{\alpha_{PE}^l\}=Top(k,\ \alpha_{PE})
% \end{aligned}
% \end{equation}

% Here, ${\alpha_{PE}^l}$ represents the set of the top $k$ weight values, with $l$ indicating the index of the selected client, $l\in[k]$. The selected weights are then applied to the filtered personalized expert set $\mathcal{W}_{PE}^l=\{W_{PE}^l\}$. For instance, using the personalized classifier $s_p$, each expert maps the data to the new feature space $h^l$.

% \begin{equation}
% \begin{aligned}
% h^l\in\mathbb{R}^C=s_p^l(x^j,W_{PE}^l)
% \end{aligned}
% \end{equation}

% The mixed weighted personalized parameter vector $x_{moe}^j$.
% \begin{equation}
% \begin{aligned}
% x_{moe}^j=h^l\cdot{\alpha_{PE}^l}
% \end{aligned}
% \end{equation}

% Subsequently, $C^j$ calculate the local loss $L^j(x_{moe}^j;y^j)$. During training, the personalized expert set $\mathcal{W}_{PE}$ remains frozen. Thus, the global objective loss for the Mixture of Personalization Experts is as follows:

% \begin{equation}
% \begin{aligned}
% &\min_{W_{PE}^j} \mathcal{L} = \min \sum_{j=1}^{M} \mathbb{E}_{(x^j,y^j) \sim P^j} \left[ L^j(x^j, y^j; W_{PE}^j) \right] \\
% \end{aligned}
% \end{equation}


% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.47\textwidth]{images/fig4.png}
%     % \vspace{-0.3cm}
%     \caption{The Structure of Mixture of Personalization Experts.}
%     \label{fig:pm-moe-moe}

% \end{figure}



\subsection{Energy-based Personalized Modules Denoising}
\label{sec:epd}
Due to MOE using Top-K to select appropriate experts, this ranking based solely on parameter magnitude lacks confidence and introduces noise to some extent. It leads to the gating network optimizing gradients in the wrong direction. To effectively remove noise from the personalized parameter pool, inspired by energy-based models, we propose an energy-based personalized expert denoising method. 


The core idea is to build an energy function to describe the dependency or similarity between inputs. Simply put, the essence of the method is to calculate energy—high energy corresponds to low similarity, while low energy indicates high similarity. Taking the personalized feature extractor experts as an example, for client $C^j$, the personalized expert pool $\mathcal{W}_{PE}={\{W_{PE}^j\}}_{j=1}^M$ uses a projection function $f_p:\mathbb{R}^U\rightarrow\mathbb{R}^D$ to map data $x^j$ to $H=\{h^1,h^2,...h^j,...h^M\}$. The vector $h^j\in\mathbb{R}^D$ from the local client is taken as the scalar for energy. Then, the vectors mapped by other client models are projected into the coordinate system of $h^j$.
Let’s define the energy function for a given input pair $(h^j,h^k)$ as follows:
\begin{equation}
\begin{aligned}
E^k(h^j,h^k) = -v^k\left[ \mathbb{I} \right]
\end{aligned}
\end{equation}

where $v^k \in \mathbb{R}^D=\frac{h^k\cdot h^j}{||h^k||\cdot||h^j||},\ (k \neq j)$. And each dimension of the vector is denoted by $\mathbb{I} \in D$. The projected vector set is defined as $V=\{v^1,...,v^k\},(k \in \left[ M \right], k \neq j)$. Then, Helmholtz free energy can be expressed as the negative logarithm of the partition function:
\begin{equation}
\begin{aligned}
F_T^k(v^k)=-Tlog\sum{exp(-E^k(h^j,h^k)/T)}
\end{aligned}
\end{equation}


Since we use the local client's vector as a fixed anchor, it is natural to choose the negative Helmholtz free energy as the confidence score for the similarity between $h^k$ and $h^j$

\begin{equation}
\begin{aligned}
\label{equ:confidence}
H^k(h^j,h^k)=-F_T^k(v^k)=Tlog\sum{exp(-E^k(h^j,h^k)/T)}
\end{aligned}
\end{equation}

where $T$ is the temperature parameter. Therefore, we use the confidence score to filter out noise (irrelevant experts). Then, we set a dropout ratio coefficient $\gamma \in(0,1)$. The confidence scores of all personalized feature extraction experts are sorted in ascending order, and the bottom $\gamma$-proportion of experts are removed.


Finally, after the two modules of PM-MOE framework, the total objective loss of PM-MOE is as follows:
\begin{equation}
\begin{aligned}
\min_{\theta_{PP}^j,\theta_{PE}^j}\mathcal{L}=min\sum_{j}^{K}{\mathbb{E}_{(x^j,y^j)~P^j}L^j(x^j,y^j;\theta_{PP}^j,\theta_{PE}^j)}
\end{aligned}
\end{equation}
% By adjusting $T$, we are able to find the optimal temperature for the current client. For two vectors, a more similar data distribution usually corresponds to lower free energy (higher confidence score). Therefore, we use the confidence score to filter out noise (irrelevant experts).





\subsection{Theoretical Analysis}

% Proposition: The personalized knowledge obtained after convergence is more beneficial for enhancing the local client's model performance and exhibits higher generalization ability compared to the gradually learned personalized knowledge.
% In the iterative convergence of each client's personalized parameters, these parameters stably capture each client's local knowledge. The server constructs a parameter pool from this local knowledge and distributes it to every client. Each client, by training the gating network, adjusts the weights of these personalized knowledge parameters, which helps improve the model performance locally.

% In model-split-based personalized federated learning, such methods divide the model into shared upload modules and local private modules. The shared parameters learn from the common data distribution of all parties, while local parameters fit the client's data distribution. We argue that this synchronous learning approach for sharing model structures reduces the efficiency of knowledge transfer between clients. Instead, enabling clients to transmit already stable, converged personalized parameters is a more effective way to share knowledge across multiple clients. Therefore, we designed the PM-MOE architecture. 

% \textbf{Our core theory is that the personalized knowledge gained after convergence is more beneficial for enhancing the performance of local client models and promoting knowledge transfer among clients more effectively.} This subsection theoretically proves that the PM-MOE architecture can converge to a lower bound. In extreme cases, where each client's data distribution is entirely different, this architecture does not compromise local model performance.

In this section, we demonstrate that \textbf{leveraging personalized models converged from other clients is more beneficial for improving the performance of local models.} In simple terms, model-split-based personalized federated learning shares uploaded models to learn from the data distribution of all parties involved. However, the heterogeneous nature of the data creates a tug-of-war, resulting in inefficient knowledge transfer between clients. Interestingly, the private parameters that are not uploaded by clients best capture local knowledge. 

Therefore, we propose that utilizing these converged personalized models is necessary to enhance performance, leading to the design of the PM-MOE architecture. In this subsection, we theoretically prove that the PM-MOE architecture converges to a lower bound. Even in extreme cases, where each client’s data distribution is entirely different, this architecture does not degrade the performance of local models.




\begin{theorem}
\label{thm:low_bound}
\textbf{(Lower Bound on the Final Accuracy of MPE)}
Suppose there are $M (\geq 2)$ client experts predicting independently, each with an average accuracy rate of \( p (>0) \). If a trained gate network assigns samples to the client experts such that the ratio of the probability of assigning a sample to a correct expert versus an incorrect expert is \( 1 + \alpha \), where \( \alpha > 0 \). Then, the final accuracy of MPE is bounded from below by:
\begin{equation}
\label{equ:low_bound}
P_{\text{MPE}} \geq \frac{(1 + \alpha) p}{1 + \alpha (p+\frac{1-p}{M})}>p=P_{client}.
\end{equation}
\end{theorem}


We briefly proof the key steps, and other details are given in the appendix.
\begin{proof}
We define the event set:

$\mathcal{A}:=\{s \text{ out of } M \text{ experts are able to predict correctly}\}$.

\begin{equation}
P(\mathcal{A})=\binom{M}{s}p^s(1-p)^{M-s},
\end{equation}

Under the above condition, if the gated network assigns s client weights, the model is able to still correctly predict the sample. We have:

\begin{equation}
P(\mathcal{B}\mid\mathcal{A})=\frac{(1+\alpha)s}{(1+\alpha)s+(M-s)}=\frac{(1+\alpha)s}{M+\alpha s},
\end{equation}

And then, $\mathcal{B}:=\{\text{MPE can predict correctly}\}$. We have:

\begin{equation}
\label{equ:pb}
\begin{aligned}
P(\mathcal{B})&=\sum_{\mathcal{A}}P(\mathcal{A})P(\mathcal{B}\mid\mathcal{A})\\
&=\mathbb{E}\left[ \frac{(1+\alpha)Mp}{M+\alpha (t+1)} \right] \quad(\text{Let }t=s-1).
\end{aligned}
\end{equation}

Define the function:
\begin{equation}
f(t) = \frac{(1+\alpha)Mp}{M+\alpha (t+1)}.
\end{equation}

We observe that \( f(t) \) is a convex function. And by Jensen's inequality, we have:

\begin{equation}
\label{equ:jesseneq}
\mathbb{E}[f(t)] \geq f\left( \mathbb{E}[t] \right).
\end{equation}

Combining \eqref{equ:pb} and \eqref{equ:jesseneq} yields

\begin{equation}
\begin{aligned}
P_{\text{MPE}}=P(\mathcal{B}) \geq f\left( \mathbb{E}[t] \right)&= \frac{(1 + \alpha) p}{1 + \alpha (p+\frac{1-p}{M})}.
\end{aligned}
\end{equation}

The last expression is strictly increasing with respect to $\alpha$ when $\alpha > 0$, and thus:
\begin{equation}
\begin{aligned}
P_{\text{MPE}}\geq \frac{(1 + \alpha) p}{1 + \alpha (p+\frac{1-p}{M})}>\frac{(1 + 0) p}{1 + 0 (p+\frac{1-p}{M})}=p=P_{\text{client}}.
\end{aligned}
\end{equation}

% where the event set $\mathcal{A}:=\{s \text{ out of } M \text{ experts can predict correctly}\}$. Under the above condition, if the gate network can assign a sample to any of these $s$ client experts, then the MPE can predict the sample correctly. Therefore, we have:
% \begin{equation}
% P(\mathcal{B}\mid\mathcal{A})=\frac{(1+\alpha)s}{(1+\alpha)s+(M-s)}=\frac{(1+\alpha)s}{M+\alpha s},
% \end{equation}
% where $\mathcal{B}:=\{\text{MPE can predict correctly}\}$.
% According to the law of total probability, the probability that the MPE can predict correctly is:

% \begin{equation}
% \label{equ:tot_prob}
% \begin{aligned}
% P(\mathcal{B})&=\sum_{\mathcal{A}}P(\mathcal{A})P(\mathcal{B}\mid\mathcal{A})\\
% &=\sum_{s=0}^M\binom{M}{s}p^s(1-p)^{M-s}\frac{(1+\alpha)s}{M+\alpha s}\\
% &=\sum_{s=1}^{M}\binom{M-1}{s-1}p^{s-1}(1-p)^{M-s}\frac{(1+\alpha)Mp}{M+\alpha s}\\
% &=\sum_{t=0}^{M-1}\binom{M-1}{t}p^{t}(1-p)^{M-1-t}\frac{(1+\alpha)Mp}{M+\alpha (t+1)}\quad\quad(\text{Let }t=s-1)\\
% &=\mathbb{E}\left[ \frac{(1+\alpha)Mp}{M+\alpha (t+1)} \right].
% \end{aligned}
% \end{equation}

% Here, the expectation is taken over $t$ which follows $\text{Bin}(M-1,p)$. 
% Define the function:
% \begin{equation}
% f(t) = \frac{(1+\alpha)Mp}{M+\alpha (t+1)}.
% \end{equation}
% We observe that \( f(t) \) is a convex function with respect to \( t \) because its second derivative is positive:
% \begin{equation}
% f''(t) = \frac{2 (1+\alpha) M p \alpha^2}{\left( M + \alpha t + \alpha \right)^3} > 0,
% \end{equation}
% since \( \alpha,p,M > 0 \).
% By Jensen's inequality, we have:

% \begin{equation}
% \label{equ:jessen}
% \mathbb{E}[f(t)] \geq f\left( \mathbb{E}[t] \right).
% \end{equation}
% The expected value of \( t \) is:
% \begin{equation}

% \label{equ:expect}
% \mathbb{E}[t] = (M-1) p.
% \end{equation}
% Combining \eqref{equ:tot_prob}, \eqref{equ:jessen} and \eqref{equ:expect} yields
% \begin{equation}
% \label{equ:simplification}
% \begin{aligned}
% P_{\text{MPE}}=P(\mathcal{B}) \geq f\left( \mathbb{E}[t] \right) &=f((M-1) p)\\
% &= \frac{(1 + \alpha) M p}{M + \alpha ((M-1)p+1)}\\
% &= \frac{(1 + \alpha) p}{1 + \alpha (p+\frac{1-p}{M})}.
% \end{aligned}
% \end{equation}
% The last expression is strictly increasing with respect to $\alpha$ when $\alpha > 0$, and thus:
% \begin{equation}
% \label{equ:final}
% \begin{aligned}
% P_{\text{MPE}}\geq \frac{(1 + \alpha) p}{1 + \alpha (p+\frac{1-p}{M})}>\frac{(1 + 0) p}{1 + 0 (p+\frac{1-p}{M})}=p=P_{\text{client}}.
% \end{aligned}
% \end{equation}
\end{proof}

Theorem \ref{thm:low_bound} indicates that the accuracy of the MPE is bounded below by the average accuracy of an individual client. Furthermore, the lower bound $\frac{(1 + \alpha) p}{1 + \alpha (p+\frac{1-p}{M})}$ increases monotonically with respect to both $\alpha$ and $M$. In other words, the accuracy of the MPE will be improved as the training of the gate network. Specifically, when the gate network is well trained (i.e., $\alpha\gg 0$) and $M$ is large enough, the accuracy of MPE will asymptotically approach $100\%$.

% \vspace{-0.2cm}


\begin{table*}[htbp]
\small
\centering
\caption{Ablation experiments of PM-MOE across 9 state-of-the-art model-split-based personalized federated learning algorithms.}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|c}
\specialrule{.16em}{0pt} {.65ex}
Spilt Type & \multicolumn{6}{c|}{$S=0$} & \multicolumn{6}{c|}{$S=20$}   & Avg $\uparrow$ \\
\specialrule{.16em}{0pt} {.65ex}
Method  &MNIST  &FMNIST  &Cifar10 &Cifar100 &TINY &AGNews &MNIST  &FMNIST  &Cifar10 &Cifar100 &TINY &AGNews   \\
\specialrule{.16em}{0pt} {.65ex}

FedPer    & 99.49	& 97.53	& 89.90	& 48.27	& 36.36	& 93.99 & 98.62	& 93.57	& 76.67	& 36.28	& 25.91	& 88.75 & -\\
+PM-MOE   & \textbf{99.50}	& \textbf{97.55}	& \textbf{89.96}	& \textbf{48.34}	& \textbf{36.42}	& \textbf{93.99} & \textbf{98.62}	& \textbf{93.59}	& \textbf{76.69}	& \textbf{36.30}	& \textbf{25.96}	& \textbf{88.75} & 0.0275\\
\specialrule{.16em}{0pt} {.65ex}
LG-FedAvg & 99.28	& 97.25	& 89.02	& 47.03	& 33.20	& 94.33 & 97.85	& 92.23	& 73.95	& 35.90	& 23.78	& 87.77 & -\\
+PM-MOE    & \textbf{99.28}	& \textbf{97.28}	& \textbf{89.19}	& \textbf{47.10}	& \textbf{33.25}	& \textbf{94.76} & \textbf{97.85}	& \textbf{92.23}	& \textbf{74.01}	& \textbf{35.90}	& \textbf{23.78}	& \textbf{87.77}& 0.0325\\
\specialrule{.16em}{0pt} {.65ex}
FedRep   & 99.46	& 97.58	& 90.19	& 49.44	& 38.09	& 93.78 & 98.61	& 93.77	& 77.25	& 36.52	& 25.77	& 89.02 & -\\
+PM-MOE   & \textbf{99.47}	& \textbf{97.60}	& \textbf{90.24}	& \textbf{49.49}	& \textbf{38.12}	& \textbf{93.87} & \textbf{98.61}	& \textbf{93.82}	& \textbf{77.25}	& \textbf{36.52}	& \textbf{25.78}	& \textbf{89.02}& 0.0258\\
\specialrule{.16em}{0pt} {.65ex}
FedRoD   & 99.68	& 97.60	& 90.07	& 51.92	& 38.90	& 93.65 & 99.33	& 94.06	& 79.50	& 42.45	& 29.07	& 88.91 & -\\
+PM-MOE    & \textbf{99.69}	& \textbf{97.66}	& \textbf{90.22}	& \textbf{52.82}	& \textbf{39.25}	& \textbf{93.72} & \textbf{99.33}	& \textbf{94.06}	& \textbf{79.50}	& \textbf{42.60}	& \textbf{29.07}	& \textbf{88.93}& 0.1425\\
\specialrule{.16em}{0pt} {.65ex}
FedGH  & 99.29	& 97.40	& 84.50	& 48.61	& 25.80	& 92.58 & 97.94	& 92.25	& 73.79	& 37.88	& 21.49	& 88.10 & -\\
+PM-MOE    & \textbf{99.30}	& \textbf{97.40}	& \textbf{88.61}	& \textbf{48.69}	& \textbf{25.82}	& \textbf{92.66} & \textbf{97.94}	& \textbf{92.25}	& \textbf{73.79}	& \textbf{37.88}	& \textbf{21.51}	& \textbf{88.19}& 0.3675\\
\specialrule{.16em}{0pt} {.65ex}
FedBABU   & 99.67	& 97.74	& 91.38	& 50.83	& 34.53	& 92.87 & 99.32	& 94.71	& 82.17	& 40.46	& 25.92	& 87.58 & -\\
+PM-MOE   & \textbf{99.67}	& \textbf{97.76}	& \textbf{91.41}	& \textbf{50.83}	& \textbf{34.53}	& \textbf{93.55} & \textbf{99.32}	& \textbf{94.79}	& \textbf{82.17}	& \textbf{40.46}	& \textbf{25.92}	& \textbf{88.29}& 0.1267\\
\specialrule{.16em}{0pt} {.65ex}
GPFL    & 99.49	& 94.91	& 77.79	& 57.41	& 27.08	& 90.84 & 99.49	& 93.21	& 72.39	& 49.01	& 22.92	& 82.41 & -\\
+PM-MOE   & \textbf{99.50}	& \textbf{95.56}	& \textbf{82.28}	& \textbf{57.41}	& \textbf{27.08}	& \textbf{91.94} & \textbf{99.49}	& \textbf{93.21}	& \textbf{72.39}	& \textbf{49.01}	& \textbf{22.92}	& \textbf{82.41}& 0.5208\\
\specialrule{.16em}{0pt} {.65ex}
FedCP   & 99.75	& 98.31	& 93.76	& 69.83	& 65.97	& 92.40 & 99.27	& 94.45	& 80.29	& 41.79	& 31.93	& 87.62 & -\\
+PM-MOE    & \textbf{99.85}	& \textbf{98.61}	& \textbf{93.95}	& \textbf{70.68}	& \textbf{66.33}	& \textbf{92.48} & \textbf{99.30}	& \textbf{94.63}	& \textbf{80.51}	& \textbf{42.49}	& \textbf{31.96}	& \textbf{87.67} & 0.2575\\
\specialrule{.16em}{0pt} {.65ex}
DBE  & 98.17	& 93.95	& 89.11	& 60.33	& 38.29	& 93.70 & 96.97	& 91.11	& 79.76	& 52.30	& 31.10	& 89.08 & -\\
+PM-MOE    & \textbf{99.63}	& \textbf{97.23}	& \textbf{89.90}	& \textbf{60.53}	& \textbf{38.36}	& \textbf{93.74} & \textbf{99.38}	& \textbf{93.40}	& \textbf{80.05}	& \textbf{52.30}	& \textbf{31.11}	& \textbf{89.08} & 0.9033\\




% FedPer    & 0.9949	& 0.9753	& 0.8990	& 0.4827	& 0.3636	& 0.9399 & 0.9862	& 0.9357	& 0.7667	& 0.3628	& 0.2591	& 0.8875\\
% FedPer+PM-MOE   & \textbf{0.9950}	& \textbf{0.9755}	& \textbf{0.8996}	& \textbf{0.4834}	& \textbf{0.3642}	& \textbf{0.9399} & \textbf{0.9862}	& \textbf{0.9359}	& \textbf{0.7669}	& \textbf{0.3630}	& \textbf{0.2596}	& \textbf{0.8875}\\
% \specialrule{.16em}{0pt} {.65ex}
% LG-FedAvg & 0.9928	& 0.9725	& 0.8902	& 0.4703	& 0.3320	& 0.9475 & 0.9785	& 0.9223	& 0.7395	& 0.3590	& 0.2378	& 0.8777\\
% LG-FedAvg+PM-MOE    & \textbf{0.9928}	& \textbf{0.9728}	& \textbf{0.8919}	& \textbf{0.4710}	& \textbf{0.3325}	& \textbf{0.9476} & \textbf{0.9785}	& \textbf{0.9223}	& \textbf{0.7401}	& \textbf{0.3590}	& \textbf{0.2378}	& \textbf{0.8777}\\
% \specialrule{.16em}{0pt} {.65ex}
% FedRep   & 0.9946	& 0.9758	& 0.9019	& 0.4944	& 0.3809	& 0.9378 & 0.9861	& 0.9377	& 0.7725	& 0.3652	& 0.2577	& 0.8902\\
% FedRep+PM-MOE   & \textbf{0.9947}	& \textbf{0.9760}	& \textbf{0.9024}	& \textbf{0.4949}	& \textbf{0.3812}	& \textbf{0.9387} & \textbf{0.9861}	& \textbf{0.9382}	& \textbf{0.7725}	& \textbf{0.3652}	& \textbf{0.2578}	& \textbf{0.8902}\\
% \specialrule{.16em}{0pt} {.65ex}
% FedRoD   & 0.9968	& 0.9760	& 0.9007	& 0.5192	& 0.3890	& 0.9365 & 0.9933	& 0.9406	& 0.7950	& 0.4245	& 0.2907	& 0.8891\\
% FedRoD+PM-MOE    & \textbf{0.9969}	& \textbf{0.9766}	& \textbf{0.9022}	& \textbf{0.5282}	& \textbf{0.3925}	& \textbf{0.9372} & \textbf{0.9933}	& \textbf{0.9406}	& \textbf{0.7950}	& \textbf{0.4260}	& \textbf{0.2907}	& \textbf{0.8893}\\
% \specialrule{.16em}{0pt} {.65ex}
% FedGH  & 0.9929	& 0.9740	& 0.8450	& 0.4861	& 0.2580	& 0.9258 & 0.9794	& 0.9225	& 0.7379	& 0.3788	& 0.2149	& 0.8810\\
% FedGH+PM-MOE    & \textbf{0.9930}	& \textbf{0.9740}	& \textbf{0.8861}	& \textbf{0.4869}	& \textbf{0.2582}	& \textbf{0.9266} & \textbf{0.9794}	& \textbf{0.9225}	& \textbf{0.7379}	& \textbf{0.3788}	& \textbf{0.2151}	& \textbf{0.8819}\\
% \specialrule{.16em}{0pt} {.65ex}
% FedBABU   & 0.9967	& 0.9774	& 0.9138	& 0.5083	& 0.3453	& 0.9287 & 0.9932	& 0.9471	& 0.8217	& 0.4046	& 0.2592	& 0.8758\\
% FedBABU+PM-MOE   & \textbf{0.9967}	& \textbf{0.9776}	& \textbf{0.9141}	& \textbf{0.5083}	& \textbf{0.3453}	& \textbf{0.9355} & \textbf{0.9932}	& \textbf{0.9479}	& \textbf{0.8217}	& \textbf{0.4046}	& \textbf{0.2592}	& \textbf{0.8829}\\
% \specialrule{.16em}{0pt} {.65ex}
% GPFL    & 0.9949	& 0.9491	& 0.7779	& 0.5741	& 0.2708	& 0.9084 & 0.9949	& 0.9321	& 0.7239	& 0.4901	& 0.2292	& 0.8241\\
% GPFL+PM-MOE   & \textbf{0.9950}	& \textbf{0.9556}	& \textbf{0.8228}	& \textbf{0.5741}	& \textbf{0.2708}	& \textbf{0.9194} & \textbf{0.9949}	& \textbf{0.9321}	& \textbf{0.7239}	& \textbf{0.4901}	& \textbf{0.2292}	& \textbf{0.8241}\\
% \specialrule{.16em}{0pt} {.65ex}
% FedCP   & 0.9975	& 0.9831	& 0.9376	& 0.6983	& 0.6597	& 0.9240 & 0.9927	& 0.9445	& 0.8029	& 0.4179	& 0.3193	& 0.8762\\
% FedCP+PM-MOE    & \textbf{0.9985}	& \textbf{0.9861}	& \textbf{0.9395}	& \textbf{0.7068}	& \textbf{0.6633}	& \textbf{0.9248} & \textbf{0.9930}	& \textbf{0.9463}	& \textbf{0.8051}	& \textbf{0.4249}	& \textbf{0.3196}	& \textbf{0.8767}\\
% \specialrule{.16em}{0pt} {.65ex}
% DBE  & 0.9817	& 0.9395	& 0.8911	& 0.6033	& 0.3829	& 0.9370 & 0.9697	& 0.9111	& 0.7976	& 0.5230	& 0.3110	& 0.8908\\
% DBE+PM-MOE    & \textbf{0.9963}	& \textbf{0.9723}	& \textbf{0.8990}	& \textbf{0.6053}	& \textbf{0.3836}	& \textbf{0.9374} & \textbf{0.9938}	& \textbf{0.9340}	& \textbf{0.8005}	& \textbf{0.5230}	& \textbf{0.3111}	& \textbf{0.8908}\\

\specialrule{.16em}{0pt} {.65ex}
\end{tabular}
\vspace{-0.3cm}
\label{tab:table2}
\end{table*}








% \begin{table}
%     \centering
%     \begin{tabular}{lcccc}
%         \toprule
%         Method & Buffer size & Prompt & AA     & AF \\
%         \midrule
%         LRCIL  &           & $\times$  & 76.39*   & -4.39* \\
%         iCaRL  & 100/class & $\times$  & 79.76*   & -8.73* \\
%         LUCIR  &           & $\times$  & 82.53*   & -5.34* \\
%         \midrule
%         LRCIL  & \multirow{4}*{50/class} & $\times$  & 74.01*   & -8.62* \\
%         iCaRL  &   & $\times$  & 73.98*   & -14.50*\\
%         LUCIR  &   & $\times$  & 80.77*   & -7.85* \\
%         DyTox  &   & $\checkmark$   & 86.21*   & -1.55* \\
%         \midrule
%         EWC    &   \multirow{8}*{No Buffer} & $\times$  & 50.59*   & -42.62*\\
%         LwF    &     & $\times$  & 60.94*   & -13.53*\\
%         DyTox  &     & $\checkmark$  & 51.27*   & -45.85*\\
%         L2P    &     & $\checkmark$   & 61.28*   & -9.23*\\
%         HiDe-Prompt    &     & $\checkmark$   & 84.32   & -2.61\\
%         S-liPrompts&           & $\checkmark$  & 88.79   & -0.63\\
%         Dual-Prompt    &     & $\checkmark$   & 92.51   & -0.76\\
%         \textbf{CP-Prompt(Ours)}&           & $\checkmark$  & \textbf{93.65}   & \textbf{-0.25}\\
%         \bottomrule
%     \end{tabular}
%     \caption{DIL Results on CDDB-Hard. * represents the result is quoted from ~\protect\cite{DBLP:conf/nips/WangHH22}.}
%     \label{tab:CDDB-Hard-results}
%     \vspace{-0.5cm}
% \end{table}



% \begin{figure*}[htbp]
%     \centering
%     \includegraphics[width=0.9\textwidth]{images/fig4.jpg}
%     \caption{
%     The model performance variation results from inserting personalized prompts into different consecutive transformer layers. The vertical axis represents the starting layer index for inserting personalized prompts, while the horizontal axis represents the ending layer index.
%     }
%     \label{fig:layer_analysis}
%     \vspace{-0.2cm}
% \end{figure*}




% \begin{table}
%     \centering
%     \begin{tabular}{lccc}
%         \toprule
%         Method & Buffer size & Prompt & AA    \\
%         \midrule
%         ER  &    \multirow{6}*{50/class}       & $\times$  & 79.75±0.84*    \\
%         GDumb  &  & $\times$  & 74.92±0.25* \\
%         BiC  &           & $\times$  & 79.28±0.30* \\
%         DER++  &  & $\times$  & 79.70±0.44* \\
%         Co$^2$L  &   & $\times$  & 79.75±0.84* \\
%         L2P  &   & $\times$  & 81.07±0.13* \\
%         \midrule
%         EWC    &   \multirow{7}*{No Buffer} & $\times$  & 74.82±0.60*\\
%         LwF    &     & $\times$  & 75.45±0.40*\\
%         L2P  &     & $\checkmark$  & 78.33±0.06*\\
%         HiDe-Prompt  &     & $\checkmark$  & 80.81±0.76\\
%         S-liPrompts&           & $\checkmark$  & 87.07±0.65\\
%         Dual-Prompt  &     & $\checkmark$  & 88.74±0.36\\
%         \textbf{CP-Prompt(Ours)}&           & $\checkmark$  & \textbf{90.67±0.55}  \\
%         \bottomrule
%     \end{tabular}
%     \caption{DIL Results on CORe50. * represents the result is quoted from ~\protect\cite{DBLP:conf/nips/WangHH22}.}
%     \label{tab:CORe50}
%     \vspace{-0.5cm}
% \end{table}














% \vspace{-0.3cm}


% \begin{figure}[t]
%     \centering
%     \vspace{-0.3cm}
%     \includegraphics[width=0.4\textwidth]{images/fig7.png}
%     % \vspace{-0.3cm}
%     \caption{Common prompts Comparison of different MSA layers embedding General prompts. The vertical axis is the average accuracy of the model. The horizontal axis from left to right is the domain data sequentially learned by the model.}
%     \label{fig:dual_cp}
%     \vspace{-0.5cm}
% \end{figure}












\section{Experiment}
%In this section, our method's competitive performance in handling incremental tasks in the processing domain was validated through experiments compared to state-of-the-art DIL methods. Additionally, a series of ablative experiments were conducted to better ascertain the parameter choices and component influences of the proposed method.

% \begin{figure*}[htbp]
%     \centering
%     \includegraphics[width=0.85\textwidth]{images/fig5.jpg}
%     \caption{Performance variation of CP-Prompt by (a) using different prompt lengths; (b) adding new domain data.
%     }
%     \label{fig:length_and_prefixone_analysis}
%     \vspace{-0.3cm}
% \end{figure*}


% In this section, we compare the proposed PM-MOE with nine state-of-the-art model-splitting-based personalized federated learning algorithms. We validate the effectiveness of our method across six datasets. Additionally, we conduct comprehensive ablation studies on these algorithms to better determine parameter choices and the impact of various components.

% In this section, we evaluate our proposed PM-MOE against nine state-of-the-art personalized federated learning methods across six datasets. We also perform extensive ablation studies to analyze parameter choices and the impact of key components.

% \paragraph{Dataset and Data Partitioning.} 
% To effectively compare the performance of the proposed approach, we use six widely-adopted benchmark datasets for personalized federated learning: AG News~\cite{DBLP:conf/nips/ZhangZL15}, Cifar10~\cite{krizhevsky2009learning}, Cifar100~\cite{krizhevsky2009learning}, MNIST~\cite{DBLP:journals/pieee/LeCunBBH98}, Fashion-MNIST~\cite{DBLP:journals/corr/abs-1708-07747}, and Tiny-ImageNet~\cite{DBLP:journals/corr/ChrabaszczLH17}. 



% \begin{itemize}
% [leftmargin=*,itemsep=0pt,parsep=0.5em,topsep=0.3em,partopsep=0.3em]
%     \item \textbf{CDDB} is a dataset used for continuous deepfake detection, where the DIL objective involves recognizing authentic and fake images across different domains. We adopted the Hard Setting from ~\cite{DBLP:conf/nips/WangHH22}, requiring learning on 5 continuous deepfake detection domains: GauGAN, BigGAN, WildDeepfake, WhichFaceReal, and SAN on $\sim$27,000 images.
%     \item \textbf{CORe50} is designed for continuous object recognition, consisting of 11 domains, each with 50 categories. In DIL, the goal is to sequentially learn from 8 domains for incremental training and assess performance on the remaining 3 domains (unseen) with $\sim$160,000 images.
%     \item \textbf{DomainNet} is a domain adaptation dataset commonly used as a standard benchmark for DIL methods. It comprises 6 domains, each with 345 categories. The DIL setup aligns with CaSSLe ~\cite{DBLP:conf/cvpr/FiniCA0AM22} which involves $\sim$600,000 images in total.
% \end{itemize}



% \subsection{Experiment Setup}
\subsection{Baseline Methods.} 
We referred to the Personal Federated Learning library, PFLlib~\cite{DBLP:journals/corr/abs-2312-04992}. Furthermore, we compared  general federated learning algorithms such as FedAvg~\cite{DBLP:conf/aistats/McMahanMRHA17}, FedProx~\cite{DBLP:journals/network/LuoHSOHD23}, SCAFFOLD~\cite{DBLP:conf/icml/KarimireddyKMRS20}, MOON~\cite{DBLP:conf/cvpr/LiHS21}, and FedGen~\cite{DBLP:conf/icml/ZhuHZ21}, alongside recent state-of-the-art personalized federated learning methods, including personalized feature extractors like FedGH~\cite{DBLP:conf/mm/YiWLSY23}, LG-FedAvg~\cite{DBLP:journals/corr/abs-2001-01523}, FedBABU~\cite{DBLP:conf/iclr/OhKY22}, FedCP\cite{DBLP:conf/kdd/ZhangHWSXMG23}, GPFL~\cite{DBLP:conf/iccv/ZhangHWSXMCG23}, FedPer\cite{DBLP:journals/corr/abs-1912-00818}, FedRep~\cite{DBLP:conf/icml/CollinsHMS21}, FedRod~\cite{DBLP:conf/iclr/ChenC22}, and DBE~\cite{DBLP:conf/nips/ZhangHCWSXMG23} for personalized parameters.








\subsection{Experimental Results}

\paragraph{Main Results.} 
Tables \ref{tab:table1} show that PM-MOE consistently outperforms other personalized federated learning methods across both partitioning settings in tasks ranging from 4 to 200 classes. Compared to traditional federated learning methods, personalized federated learning better handles data heterogeneity, with a performance improvement of up to 48.64\% over the FedAvg baseline. Interestingly, when data heterogeneity decreases, the overall performance of personalized methods also drops.
The PM-MOE framework leverages the personalized models converged from all clients to improve each client’s performance. If parameters from other clients are noisy, the local gating network assigns weights to prioritize the local personalized model, protecting its performance. Conversely, if external parameters are useful, the network allocates weights accordingly, enhancing the local model’s performance.




\paragraph{Analysis of Gating Network Parameters.} 
As the most critical component for each client in this framework is the training of the gating network, this section presents parameter experiments focused on tuning the number of layers, activation functions, and initialization parameters of the gating network. In this subsection, to highlight the differences between methods across different dimensions, we will apply sigmoid normalization to the data in the experimental group.


\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{images/fig6.png}
    \caption{ 
    Results of Gating Network Parameters
    }
    \label{fig:exp-gating-param}
    \vspace{-0.2cm}
\end{figure*}




\begin{itemize}
[leftmargin=*,itemsep=0pt,parsep=0.5em,topsep=0.3em,partopsep=0.3em]
\item \textbf{Number of Layers in the Gating Network}: We conducted four sets of experiments on the number of layers in the gating network. 1 layer: (input dimension, number of experts); 2 layers: (input dimension, 128, number of experts); 3 layers: (input dimension, 128, 256, number of experts); 4 layers: (input dimension, 128, 256, 128, number of experts).
As shown in Figure \ref{fig:exp-gating-param}-(a), increasing the depth of the gating network proves to be effective. The gating network needs to determine the weights for all personalized parameters based on input data, requiring deeper neural networks on the client side to capture data features effective.

\item \textbf{Activation Functions of the Gating Network}: For the 4-layer feedforward gating network, we tested common neural network activation functions such as ReLU, LeakyReLU, PReLU, ELU, SELU, SiLU, and Mish. As shown in Figure \ref{fig:exp-gating-param}-(b), the most effective activation function is LeakyReLU. LeakyReLU's non-linearity in the negative region allows the neural network to learn and model more complex data, effectively assigning personalized model parameters the appropriate weights.

\item \textbf{Initialization Methods for the Gating Network}: We compared commonly used parameter initialization methods, including uniform distribution, normal distribution, Xavier, Kaiming, Orthogonal, and Spectral. As shown in Figure \ref{fig:exp-gating-param}-(c), most results indicated that the Orthogonal initialization method yields the best performance for gated network models. This method draws initial weights from the orthogonal group, maintaining dynamic isometry throughout the network's learning process, which helps preserve a relatively stable proportional relationship between input and output signals.
\end{itemize}



% \vspace{-0.3cm}
\paragraph{Ablation Study.} 
% We conducted an ablation study to evaluate the effectiveness of each individually designed module. Without loss of generality, we set data heterogeneity to $S=0$ (100\% label heterogeneous data) and $S=20$ (80\% label heterogeneous data). Specifically, we applied our proposed PM-MOE framework to nine SOTA personalized federated learning algorithms and calculated the model accuracy for 20 clients. As shown in Table \ref{tab:table2}, PM-MOE improves the performance of all personalized federated learning algorithms across six widely-used datasets.



% In this section, we conducted ablation studies to assess the effectiveness of each individually designed module. The experiments confirmed that the PM-MOE component and the denoising component enhance the performance of the model-split-based personalized federated learning algorithm. As shown in Table \ref{tab:table4}, adding the MOE component led to an average improvement across personalized federated learning algorithms in the 4, 10, and 100 class settings. The addition of the denoising component also resulted in an average improvement.

In this section, we conducted ablation studies to evaluate the effectiveness of each individually designed module. The experiments confirmed that the PM-MOE component and the denoising component improve the performance of the model-split-based personalized federated learning algorithm. We selected the best performing personalized federated learning methods in the comparison dataset for comparison. As shown in Table \ref{tab:table4}, adding the MOE component resulted in an average improvement of 0.2\% across the 4, 10, and 100 class settings. The regular denoising ratio was set to 0.2, and adding the denoising component led to an average improvement of 0.53\%.

\begin{table}[tbp!]
    \centering
    \caption{Ablation Experiment Analysis Results}
    \label{tab:table4}
    \begin{tabular}{ccccc}
        \toprule
        Method & AGNews & FMNIST & Cifar100& Avg $\uparrow$\\
        \midrule
        pFL   & 94.33 &98.31& 69.83& -\\
        % \midrule
        +MOE & 94.52 & 98.39 &70.12	&0.19\%\\
        +MOE+Denoising & 94.76	&98.61	&70.68 &0.53\%\\
        \bottomrule
    \end{tabular}
    \vspace{-0.3cm}
\end{table}

In detail, we demonstrated that our proposed PM-MOE framework improves nine state-of-the-art personalized federated learning algorithms. Specifically, we used data heterogeneity settings of $S=0$ (100\% heterogeneity) and $S=20$ (80\% heterogeneity). As shown in Table \ref{tab:table2}, PM-MOE enhances the performance of all personalized federated learning algorithms across six widely adopted datasets.





\paragraph{Model Parameter Analysis.} 
\begin{itemize}
[leftmargin=*,itemsep=0pt,parsep=0.5em,topsep=0.3em,partopsep=0.3em]
\item \textbf{Top-k Impact Analysis}: The number of personalized parameters determines the breadth of knowledge. If top $k$ is too small, it may not fully utilize knowledge from other clients. If top $k$ is too large, it may introduce excessive noisy knowledge. Therefore, we conducted extensive experiments on the choice of $k$. Keeping other conditions constant, we set $k=2,4,8,16,20$ across 20 clients. As shown in Figure \ref{fig:topk}, in highly heterogeneous data settings $(S=0)$, many clients do not share categories. Thus, setting $k$ to half the number of clients helps the gating network select more effective personalized parameters. In settings with some shared data $(S=20)$, where each client shares a few categories, a larger $k$, typically equal to the number of clients, is preferable as it allows the gating network to reference personalized knowledge from all clients.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.48\textwidth]{images/fig7_topk.png}
    \caption{Impact of Top k in PM-MOE.}
    \label{fig:topk}
    \setlength{\belowcaptionskip}{2mm}
    \vspace{-0.2cm}
\end{figure}

\item \textbf{Gating Network Learning Rate Analysis}: Keeping other variables constant, we set the learning rate of the gating network $\eta_{moe}$ to 0.05, 0.1, and 0.5. As shown in Figure \ref{fig:lr}, for both $S=0$ and $S=20$ heterogeneous data settings, the MOE gating network should be assigned a higher learning rate. A smaller learning rate may cause the model to get stuck in local minima or saddle points, leading to worse performance.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.48\textwidth]{images/fig8_lr.png}
    \caption{Impact of Gating Network Learning Rate in PM-MOE.}
    \label{fig:lr}
    \vspace{-0.2cm}
\end{figure}

\item \textbf{Impact of MOE Training Iterations}: We further explored whether the number of local training iterations affects PM-MOE. As shown in Figure \ref{fig:epoch}, after adding PM-MOE to three algorithms, performance slightly decreases with more training iterations, possibly due to overfitting. Therefore, in the case of converged pre-trained personalized federated learning, training for 50 epochs per client is sufficient.
\end{itemize}
 
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.48\textwidth]{images/fig9_epoch.png}
    \caption{Impact of Training Epochs in PM-MOE.}
    \label{fig:epoch}
    \vspace{-0.3cm}
\end{figure}



% \begin{figure}[tbp]
%     \centering
%     \vspace{-0.3cm}
%     \includegraphics[width=0.3\textwidth]{images/fig6.png}
%     \vspace{-0.3cm}
%     \caption{Different $K$ values for $K$-Means.}
%     \label{fig:different-k}
%     \vspace{-0.6cm}
% \end{figure}
\subsection{Analysis of the combination of personalized federated learning and MOE}

% For the integration of MOE in personalized federated learning, both PFL-MoE and FedMoE use gated networks to adjust the weights between local personalized models and the global model. Both approaches optimize the local models in a synchronous training mode. For a fair comparison, we set a gated network to balance the weights of the global and local models and trained the network synchronously using gradient optimization. We refer to this integration method as synchronous MoE. We trained pre-training phase for 2000 rounds. The experimental results are shown in Table \ref{tab:table3}. The performance of the gated network with synchronous training drops significantly. For the 4-class AGNews dataset, the performance decline of synchronous MoE is minor. However, as task complexity increases, such as with 10-class datasets like MNIST, FMNIST, and CIFAR-10, or even 100-class CIFAR-100 and 200-class TINY, the performance degradation of synchronous MoE becomes substantial. The likely reason is that during synchronous training, the gated network balances unconverged global parameters and local private parameters, making it prone to noise, which negatively impacts the overall model by assigning suboptimal weights.

For integrating MoE in personalized federated learning, both PFL-MoE and FedMoE use gated networks to adjust the weight balance between local personalized models and the global model. Both methods train the local models synchronously. For fair comparison, we employed a gated network to balance the global and local models' weights, training it synchronously with gradient optimization, referred to as synchronous MoE. The pre-training phase lasted 2000 rounds, with results presented in Table \ref{tab:table3}. While performance degradation in synchronous MoE is minor for the 4-class AGNews~\cite{DBLP:conf/nips/ZhangZL15} dataset, it becomes significant as task complexity increases, particularly for datasets with 10, 100, or 200 classes like MNIST~\cite{DBLP:journals/pieee/LeCunBBH98}, FMNIST~\cite{DBLP:journals/corr/abs-1708-07747}, CIFAR-10~\cite{krizhevsky2009learning}, CIFAR-100~\cite{krizhevsky2009learning}, and TINY~\cite{DBLP:journals/corr/ChrabaszczLH17}. This decline likely occurs because synchronous training forces the gated network to balance unconverged global and local parameters, making it more susceptible to noise and assigning suboptimal weights, which degrades overall model performance.

 




\section{Related Work}



\begin{table}[tbp!]
\footnotesize 
    \centering
    \caption{Moe Combination Analysis Results}
    \label{tab:table3}
    \begin{tabular}{ccccccc}
        \toprule
        Method&MNIST&FMNIST	&Cifar10&Cifar100&TINY&AGNews \\
        \midrule
        Fed-Syn-MoE & 89.48 &91.47&78.31	&18.75	&13.59	&93.37 \\
        % \midrule
        PM-MOE & 99.85	&98.61	&93.95	&70.68	&66.33	&94.76\\
        \bottomrule
    \end{tabular}
    \vspace{-0.3cm}
\end{table}

\paragraph{Personalized Federated Learning and MOE}
% In the realm of personalized federated learning, methods combining Mixture of Experts~\cite{DBLP:journals/neco/JacobsJNH91, DBLP:conf/nips/ZhouLLDHZDCLL22} (MoE), such as PFL-MoE\cite{DBLP:conf/apweb/GuoMXW21} and FedMoE\cite{yi2024fedmoe}, deploy a local feature extractor model (local expert) and a globally shared feature extractor (global expert) on each client, adjusting the gating network to control the output weights of the local and global experts. PFL-MoE focuses on homogeneous models, adjusting the weights of the two experts through the gating network. In contrast, FedMoE emphasizes model heterogeneity, employing experts with a larger parameter count than the global model to capture local data characteristics. Both approaches utilize simultaneous training of parameters, employing continuous gradient descent to optimize the gating of local and global models.

% In personalized federated learning, methods integrating Mixture of Experts~\cite{DBLP:journals/neco/JacobsJNH91, DBLP:conf/nips/ZhouLLDHZDCLL22} (MoE) models, such as PFL-MoE~\cite{DBLP:conf/apweb/GuoMXW21} and FedMoE~\cite{yi2024fedmoe}, deploy both local feature extractors (local experts) and globally shared feature extractors (global experts) on each client, with a gating network controlling the output weights of these experts. PFL-MoE primarily addresses homogeneous models, modulating the experts' weights via the gating network. In contrast, FedMoE emphasizes model heterogeneity by incorporating experts with more parameters than the global model to better capture local data characteristics. 

In personalized federated learning, methods integrating Mixture of Experts~\cite{DBLP:journals/neco/JacobsJNH91, DBLP:conf/nips/ZhouLLDHZDCLL22} (MoE) models, such as PFL-MoE~\cite{DBLP:conf/apweb/GuoMXW21} and FedMoE~\cite{yi2024fedmoe}. PFL-MoE primarily addresses homogeneous models, modulating the experts' weights via the gating network. In contrast, FedMoE emphasizes model heterogeneity by incorporating experts with more parameters than the global model to better capture local data characteristics. 
% Both methods optimize expert gating through continuous gradient descent.


\paragraph{Energy-based denoising methods}
% Energy-based models~\cite{lecun2006tutorial}(EBMs) capture dependencies between variables by associating scalar energy values with each configuration of the input. EBMs have been applied in various fields, such as generative modeling\cite{DBLP:conf/nips/DuM19}, out-of-distribution detection\cite{DBLP:conf/nips/LiuWOL20}, open-set classification\cite{al2022energy}, and Incremental Learning\cite{DBLP:conf/aaai/WangMHWSH23}. EBMs can measure relationships between personalized federated learning parameters by using energy as a metric. When selecting personalized experts in Mixture of Experts (MoE) models, this approach can filter out experts that offer no gain, effectively denoising the model. However, the application of EBMs in this context, particularly for expert denoising within MoE, remains underexplored.


% Energy-based models~\cite{lecun2006tutorial} (EBMs) capture variable dependencies by assigning scalar energy values to each input configuration. EBMs have been applied across various domains, including generative modeling~\cite{DBLP:conf/nips/DuM19}, out-of-distribution detection~\cite{DBLP:conf/nips/LiuWOL20}, open-set classification~\cite{al2022energy}, and incremental learning~\cite{DBLP:conf/aaai/WangMHWSH23}. In personalized federated learning, EBMs are able to quantify relationships between model parameters using energy as a metric. For selecting personalized experts in Mixture of Experts (MoE) models, EBMs can filter out ineffective experts, thus denoising the model. Despite its potential, the use of EBMs for expert denoising in MoE remains underexplored.

Energy-based models (EBMs)~\cite{lecun2006tutorial} assign scalar energy values to input configurations, capturing variable dependencies. They have been applied in generative modeling~\cite{DBLP:conf/nips/DuM19}, out-of-distribution detection~\cite{DBLP:conf/nips/LiuWOL20,fan2022episodic}, open-set classification~\cite{al2022energy}, and incremental learning ~\cite{DBLP:conf/aaai/WangMHWSH23}. In personalized federated learning, EBMs quantify relationships between model parameters using energy as a metric. For Mixture of Experts (MoE) models, EBMs can filter ineffective experts, denoising the model. However, their use for expert denoising in MoE remains underexplored.


\section{Conclusion}
In this article, we propose the PM-MOE framework to integrate the construction of a personalized parameter pool with local MOE training. 
PM-MOE aggregates the converged private model parameters from all clients, allowing each client to selectively reference the knowledge of others. 
This architecture effectively enhances the ability of model-splitting-based personalized federated learning algorithms to learn global knowledge. 
Through extensive experiments and theoretical analysis, we demonstrate the superiority of PM-MOE.










% \begin{table}
%   \caption{Frequency of Special Characters}
%   \label{tab:freq}
%   \begin{tabular}{ccl}
%     \toprule
%     Non-English or Math&Frequency&Comments\\
%     \midrule
%     \O & 1 in 1,000& For Swedish names\\
%     $\pi$ & 1 in 5& Common in math\\
%     \$ & 4 in 5 & Used in business\\
%     $\Psi^2_1$ & 1 in 40,000& Unexplained usage\\
%   \bottomrule
% \end{tabular}
% \end{table}



% \begin{table*}
%   \caption{Some Typical Commands}
%   \label{tab:commands}
%   \begin{tabular}{ccl}
%     \toprule
%     Command &A Number & Comments\\
%     \midrule
%     \texttt{{\char'134}author} & 100& Author \\
%     \texttt{{\char'134}table}& 300 & For tables\\
%     \texttt{{\char'134}table*}& 400& For wider tables\\
%     \bottomrule
%   \end{tabular}
% \end{table*}


%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
% \begin{acks}
% To Robert, for the bagels and explaining CMYK and color spaces.
% \end{acks}


\begin{acks}
% The work is supported by the NSFC for Distinguished Young Scholar (61825602) and 
% Tsinghua-Bosch Joint ML Center. 
This work is supported by the National Natural Science Foundation of China under Grant 62406036, the National Key Research and Development Program of China under Grant 2024YFC3308503, the Key Laboratory of Target Cognition and Application Technology under Grant 2023-CXPT-LC-005, and also sponsored by SMP-Zhipu.AI Large Model Cross-Disciplinary Fund under Grant ZPCG20241029322.
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\clearpage
\bibliographystyle{ACM-Reference-Format}

\bibliography{sample-base}


\clearpage
\appendix


\section{Appendix}

\subsection{Theoretical Derivations}
\begin{proof}
Considering $M$ client experts predict independently, the probability that exactly $s$ experts can predict correctly is given by the binomial distribution $\text{Bin}(M,p)$:

\begin{equation}
P(\mathcal{A})=\binom{M}{s}p^s(1-p)^{M-s},
\end{equation}

where the event set $\mathcal{A}:=\{s \text{ out of } M \text{ experts can predict correctly}\}$. Under the above condition, if the gate network can assign a sample to any of these $s$ client experts, then the MPE can predict the sample correctly. Therefore, we have:
\begin{equation}
P(\mathcal{B}\mid\mathcal{A})=\frac{(1+\alpha)s}{(1+\alpha)s+(M-s)}=\frac{(1+\alpha)s}{M+\alpha s},
\end{equation}
where $\mathcal{B}:=\{\text{MPE can predict correctly}\}$.
According to the law of total probability, the probability that the MPE can predict correctly is:
\begin{equation}
\label{equ:tot_prob}
\begin{aligned}
P(\mathcal{B})&=\sum_{\mathcal{A}}P(\mathcal{A})P(\mathcal{B}\mid\mathcal{A})\\&=\sum_{s=0}^M\binom{M}{s}p^s(1-p)^{M-s}\frac{(1+\alpha)s}{M+\alpha s}\\
&=\sum_{s=1}^{M}\binom{M-1}{s-1}p^{s-1}(1-p)^{M-s}\frac{(1+\alpha)Mp}{M+\alpha s}\\
&=\sum_{t=0}^{M-1}\binom{M-1}{t}p^{t}(1-p)^{M-1-t}\frac{(1+\alpha)Mp}{M+\alpha (t+1)}\quad\quad(\text{Let }t=s-1)\\
&=\mathbb{E}\left[ \frac{(1+\alpha)Mp}{M+\alpha (t+1)} \right].
\end{aligned}
\end{equation}
Here, the expectation is taken over $t$ which follows $\text{Bin}(M-1,p)$. 
Define the function:
\begin{equation}
f(t) = \frac{(1+\alpha)Mp}{M+\alpha (t+1)}.
\end{equation}
We observe that \( f(t) \) is a convex function with respect to \( t \) because its second derivative is positive:
\begin{equation}
f''(t) = \frac{2 (1+\alpha) M p \alpha^2}{\left( M + \alpha t + \alpha \right)^3} > 0,
\end{equation}
since \( \alpha,p,M > 0 \).
By Jensen's inequality, we have:
\begin{equation}
\label{equ:jessen}
\mathbb{E}[f(t)] \geq f\left( \mathbb{E}[t] \right).
\end{equation}
The expected value of \( t \) is:
\begin{equation}
\label{equ:expect}
\mathbb{E}[t] = (M-1) p.
\end{equation}
Combining \eqref{equ:tot_prob}, \eqref{equ:jessen} and \eqref{equ:expect} yields
\begin{equation}
\label{equ:simplification}
\begin{aligned}
P_{\text{MPE}}=P(\mathcal{B}) \geq f\left( \mathbb{E}[t] \right) &=f((M-1) p)\\
&= \frac{(1 + \alpha) M p}{M + \alpha ((M-1)p+1)}\\
&= \frac{(1 + \alpha) p}{1 + \alpha (p+\frac{1-p}{M})}.
\end{aligned}
\end{equation}
The last expression is strictly increasing with respect to $\alpha$ when $\alpha > 0$, and thus:
\begin{equation}
\label{equ:final}
\begin{aligned}
P_{\text{MPE}}\geq \frac{(1 + \alpha) p}{1 + \alpha (p+\frac{1-p}{M})}>\frac{(1 + 0) p}{1 + 0 (p+\frac{1-p}{M})}=p=P_{\text{client}}.
\end{aligned}
\end{equation}
\end{proof}



\subsection{Algorithm}

Following the design principles of MPP, MPE and denoising module, we apply this algorithm to nine state-of-the-art model-splitting-based personalized federated learning algorithms. 
The general training process is outlined in Algorithm ~\ref{alg:pmoe}.


\begin{algorithm}[!htbp]
\caption{Personalized Model Training with MOE}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\label{alg:pmoe}
\begin{algorithmic}[1]
\REQUIRE $M$: Number of clients; 
$W_{g,fe}$: Pre-trained parameters of the global feature extractor; $W_{g,hd}$: Pre-trained parameters of the global head; 
$\{W_{p,fe}^j,W_{p,hd}^j\}_{j=1}^M$: Pre-trained parameters of the client $j$ personalized head; 
$\eta_{moe}$: Local MOE learning rate; $k$: Top-k value for MOE weights; 
$E_{moe}$: Local MOE training iterations; 
$\theta_G^j$: Client $j$'s model parameters of the gating network; $\mathcal{W}_{PE}$, $\mathcal{W}_{PP}$: Personalized expert set and personalized parameter set, respectively. $\gamma$: Dropout ratio.
\ENSURE $\{\theta_{PE}^1, \ldots, \theta_{PE}^M\}$, $\{\theta_{PP}^1, \ldots, \theta_{PP}^M\}$: Reasonable personalized gate network models.

\STATE Server collects all the client's personalized experts and parameters to form a personalized pool $\mathcal{W}_{PE}$, $\mathcal{W}_{PP}$ and sends them to $M$ clients.
\STATE Server sends $W_{g,fe}$, $W_{g,hd}$ to $M$ clients.
\FOR{$j \in [M]$ in parallel}
    \STATE \hfill \textbf{local initialization}
    \STATE Client $j$ overwrites $W_{g,fe}$, $W_{g,hd}$ with the server parameters and freezes all of them.
    \STATE Client $j$ initializes the gated network $\theta_{PE}^j$, $\theta_{PP}^j$ for $\mathcal{W}_{PE}$, $\mathcal{W}_{PP}$ collected by the server.
    \STATE The MOE combination of $\{\theta_{PE}^j,\mathcal{W}_{PE}\}$, $\{\theta_{PP}^j,\mathcal{W}_{PP}\}$ replaces the local personalized experts and parameters.
    \STATE \hfill \textbf{local MOE learning}
    \FOR{$t = 0$ \textbf{to} $E_{moe}$}
        \STATE Extract negative Helmholtz free energy $H^k(h^j,h^k)$ by Eq.\ref{equ:confidence}
        \STATE Removed $\gamma$-proportion part by Dropout($\gamma$, $\mathcal{W}_{PE}$,  $\mathcal{W}_{PP}$)
        \STATE Client $j$ updates $\theta_{PE}^j$, $\theta_{PP}^j$ simultaneously:
        \STATE $\theta_{PE}^j \leftarrow \theta_{PE}^j - \eta_{moe} \nabla_{\theta_{PE}^j}G_{PE}^j$
        \STATE $\theta_{PP}^j \leftarrow \theta_{PP}^j - \eta_{moe} \nabla_{\theta_{PP}^j}G_{PP}^j$
    \ENDFOR
\ENDFOR
\RETURN $\{\theta_{PE}^1, \ldots, \theta_{PE}^M\}$, $\{\theta_{PP}^1, \ldots, \theta_{PP}^M\}$
\end{algorithmic}
\end{algorithm}

\subsection{Preliminary related work}
\paragraph{Personalized Federated Learning}
% Personalized Federated Learning (PFL) was proposed to address the limitations of traditional federated learning in handling non-independent and identically distributed (Non-IID) data and personalization needs. PFL employs various technical strategies, such as regularization, meta-learning, knowledge distillation, and model splitting. These methods not only enhance model performance on specific clients but also improve generalization capabilities. Typical regularization methods include pFedMe\cite{DBLP:conf/nips/DinhTN20}, which utilizes the convexity and smoothness of Moreau Envelopes to facilitate convergence analysis, and Ditto\cite{DBLP:conf/icml/00050BS21}, which balances the similarity between personalized and global models using an appropriate regularization parameter, $\lambda$. A prominent meta-learning approach is Per-FedAvg\cite{DBLP:conf/nips/0001MO20}, which introduces the concept of meta-learning into federated learning by first learning a global model across all clients, then adapting it to local data distributions for personalized models. Notable knowledge distillation methods include FedDistill\cite{seo202216}, FML\cite{DBLP:journals/corr/abs-2006-16765}, FedKD\cite{wu2022communication}, FedProto\cite{DBLP:conf/aaai/TanLLZ00Z22}, and FedPAC\cite{DBLP:conf/iclr/XuTH23}, which typically combine knowledge from the global model (aggregated via a central server) with knowledge from local models (trained on client data) to transfer global insights to local models through distillation. Additionally, model splitting strategies like FedPer\cite{DBLP:journals/corr/abs-1912-00818} and LG-FedAvg\cite{DBLP:journals/corr/abs-1912-00818} enhance personalization capabilities through model partitioning.

Personalized Federated Learning (PFL) was introduced to address the limitations of traditional federated learning in handling non-IID data and personalized requirements. PFL employs various strategies such as regularization~\cite{DBLP:conf/nips/DinhTN20, DBLP:conf/icml/00050BS21}, meta-learning~\cite{DBLP:conf/nips/0001MO20}, knowledge distillation~\cite{seo202216, DBLP:journals/corr/abs-2006-16765, wu2022communication, DBLP:conf/aaai/TanLLZ00Z22, DBLP:conf/iclr/XuTH23}, model splitting~\cite{DBLP:journals/corr/abs-1912-00818}, and personalized aggregation~\cite{DBLP:conf/pkdd/LiZSLS21, DBLP:conf/ijcai/0010W22, DBLP:conf/aaai/ZhangHWSXMG23}. pFedMe~\cite{DBLP:conf/nips/DinhTN20} leverages the convexity and smoothness of Moreau Envelopes to facilitate its convergence analysis, while Per-FedAvg~\cite{DBLP:conf/nips/0001MO20} incorporates meta-learning into federated learning. FedDistill~\cite{seo202216} transfers global knowledge to local models through distillation.




\paragraph{Model-Splitting-Based Personalized Federated Learning}
% Model-splitting-based personalized federated learning has gained significant traction in recent years. These methods share a common approach of balancing personalization and global consistency through model splitting. They can be categorized into three types: personalized feature extractors combined with a globally shared classifier, such as FedGH\cite{DBLP:conf/mm/YiWLSY23} and LG-FedAvg\cite{DBLP:journals/corr/abs-1912-00818}, which allow each client to have a personalized feature extractor while sharing a global classifier. This design captures the uniqueness of each client's data while maintaining model consistency through the global classifier. The second type features a globally shared feature extractor and personalized classifiers, exemplified by methods like FedBABU\cite{DBLP:conf/iclr/OhKY22}, FedCP\cite{DBLP:conf/kdd/ZhangHWSXMG23}, GPFL\cite{DBLP:conf/iccv/ZhangHWSXMCG23}, FedPer\cite{DBLP:journals/corr/abs-1912-00818}, FedRep\cite{DBLP:conf/icml/CollinsHMS21}, and FedRod\cite{DBLP:conf/iclr/ChenC22}, which utilize shared feature extractors with client-specific classifiers, enabling adaptation to local data distributions while preserving feature extraction commonality. The third type, such as DBE\cite{DBLP:conf/nips/ZhangHCWSXMG23}, integrates local personalized parameters with a globally shared feature extractor and classifier to further enhance performance on specific clients.



Model-splitting-based personalized federated learning has recently gained traction by balancing personalization and global consistency through model partitioning. These methods fall into three categories: the first combines personalized feature extractors with a globally shared classifier, as seen in FedGH\cite{DBLP:conf/mm/YiWLSY23} and LG-FedAvg~\cite{DBLP:journals/corr/abs-1912-00818}, allowing clients to maintain unique feature extraction while ensuring consistency through a shared classifier. The second type uses a globally shared feature extractor and personalized classifiers, as demonstrated by FedBABU~\cite{DBLP:conf/iclr/OhKY22}, FedCP~\cite{DBLP:conf/kdd/ZhangHWSXMG23}, GPFL~\cite{DBLP:conf/iccv/ZhangHWSXMCG23}, FedPer~\cite{DBLP:journals/corr/abs-1912-00818}, FedRep~\cite{DBLP:conf/icml/CollinsHMS21}, and FedRod~\cite{DBLP:conf/iclr/ChenC22}, enabling client-specific adaptation while preserving shared feature extraction. The third type, such as DBE~\cite{DBLP:conf/nips/ZhangHCWSXMG23}, integrates local personalized parameters with shared feature extractors and classifiers, enhancing performance on individual clients.


\subsection{Privacy Analysis}
For model-splitting-based personalized federated learning algorithms combined with PM-MOE, data privacy is ensured in both phases.

\textbf{In the pre-training phase}, each client uploads only the shared parameters to the server, while personalized parameters are trained locally. Due to the model splitting, the link between shared and personalized parameters is severed. The gradient information of personalized parameters remains private to each client, making it difficult to breach data privacy through model inversion attacks~\cite{DBLP:journals/tifs/Al-RubaieC16}.

\textbf{In the PM-MOE phase}, both the server and clients only receive the converged personalized model parameters. Clients cannot infer the training data or other private information from the model parameters. Therefore, the proposed approach effectively safeguards data privacy.


\subsection{Experimental Details}
To ensure fairness, we employ a 4-layer CNN model as the backbone for Cifar10, Cifar100, MNIST~\cite{DBLP:journals/pieee/LeCunBBH98}, Fashion-MNIST~\cite{DBLP:journals/corr/abs-1708-07747}, and Tiny-ImageNet~\cite{DBLP:journals/corr/ChrabaszczLH17} datasets, and a fastText~\cite{DBLP:conf/eacl/GraveMJB17} model for AG News. Each personalized model is pre-trained for 2000 epochs until convergence. We optimize three key parameters: $\eta_{moe}$ (local MOE learning rate), $k$ (top k MOE weights), and $E_{moe}$ (local MOE training iterations). All experiments are executed on a single RTX 3090 GPU.


\subsection{Dataset and Data Partitioning.}
We use public datasets to perform experiments and evaluate the performance of PM-MOE. Specifically, we adopt a Dirichlet distribution~\cite{DBLP:conf/nips/LinKSJ20} with a shared ratio $S (0 < S < 100)$ for data partitioning.
\begin{itemize}
[leftmargin=*,itemsep=0pt,parsep=0.5em,topsep=0.3em,partopsep=0.3em]
\item \textbf{Dirichlet distribution with $S=20$}: In the first setting, 20\% of the data for each class is uniformly distributed among $M$ clients, and the remaining data is assigned based on Dirichlet-distributed weights.
\item \textbf{Dirichlet distribution with $S=0$}: In the second setting, no constraints are placed on class distribution across clients, with all data allocated based on Dirichlet-distributed weights.
\end{itemize}

The detailed descriptions and statistics of these datasets are as follows:
\begin{itemize}[leftmargin=*,itemsep=0pt,parsep=0.5em,topsep=0.3em,partopsep=0.3em]
    % \vspace{-0.2cm}
    \item \textbf{MNIST~\cite{DBLP:journals/pieee/LeCunBBH98}} dataset is a widely used collection for handwritten digit recognition, compiled by the National Institute of Standards and Technology (NIST). It consists of 60,000 training images and 10,000 test images, each a 28x28 grayscale representation of digits from 0 to 9.
    \item \textbf{FMNIST~\cite{DBLP:journals/corr/abs-1708-07747}} is a dataset of fashion product images intended as a more challenging alternative to the traditional MNIST. It contains 10 categories of clothing items, such as T-shirts, trousers, and sweaters, with 7,000 grayscale images per category. There are 60,000 training images and 10,000 test images, all at 28x28 pixels. Fashion MNIST presents a greater challenge in terms of image quality and diversity, featuring more background details and varying perspectives.
    
    \item \textbf{Cifar10~\cite{krizhevsky2009learning}} consists of 60,000 32x32 color images divided into 10 classes, with 6,000 images per class. Of these, 50,000 are used for training and 10,000 for testing. The dataset is split into five training batches and one test batch, each containing 10,000 images. The test batch includes 1,000 randomly chosen images from each class, while the training batches may have varying class distributions across batches.
    
    \item \textbf{Cifar100~\cite{krizhevsky2009learning}} dataset contains 60,000 32x32 color images, but it is divided into 100 classes, with 600 images per class. Each class has 500 images for training and 100 for testing. These 100 classes are grouped into 20 super-classes, with each image having both a "fine" label (its specific class) and a "coarse" label (its super-class).
    
    \item \textbf{TINY~\cite{DBLP:journals/corr/ChrabaszczLH17}} dataset is a subset of ImageNet, released by Stanford University. It comprises 200 classes, each with 500 training images, 50 validation images, and 50 test images. The images have been preprocessed and resized to 64x64 pixels and are commonly used in deep learning for image classification tasks.
    
    \item \textbf{AGNews~\cite{DBLP:conf/nips/ZhangZL15}} dataset is an open dataset for text classification, containing 120,000 news headlines and descriptions from four categories: World, Sports, Business, and Technology. Each category includes 30,000 samples, with 120,000 samples in the training set and 7,600 in the test set.
    
\end{itemize}








\subsection{Baselines}

In our experiments, the comparison baselines mainly include traditional federated learning methods (FedAvg, FedProx, SCAFFOLD, MOON, and FedGen), federated learning of personalized experts (FedGH, LG-FedAvg, FedBABU, FedCP, GPFL, FedPer, FedRep, FedRod), and federated learning of personalized parameters (DBE).



\label{appendix:baselines}
\begin{itemize}[leftmargin=*,itemsep=0pt,parsep=0.5em,topsep=0.3em,partopsep=0.3em]
    \item \textbf{FedAvg~\cite{DBLP:conf/aistats/McMahanMRHA17}} is a pioneering algorithm in federated learning. Its core idea is to send the global model from the server to participating clients, where each client trains the model using their local data. The updated model parameters are then uploaded to the server, which computes the average of these parameters to update the global model. FedAvg can encounter performance bottlenecks when faced with highly imbalanced data or significant differences in client computing power. 
    
    \item \textbf{FedProx~\cite{DBLP:journals/network/LuoHSOHD23}} aims to address the performance degradation of FedAvg when dealing with non-i.i.d. data. It introduces a regularization term during local training to penalize the deviation of model parameters from the global model, stabilizing the optimization process and preventing local models from straying too far from the global model.
    
    \item \textbf{SCAFFOLD~\cite{DBLP:conf/icml/KarimireddyKMRS20}} tackles the issue of client drift by using control variates to reduce the variance between local updates and the global model. This ensures closer alignment between local models and the global objective, especially in non-i.i.d. data scenarios.
    
    \item \textbf{MOON~\cite{DBLP:conf/cvpr/LiHS21}} is a federated learning algorithm based on contrastive learning. It aims to minimize the feature representation difference between the local and global models while maximizing the difference between successive local models. By contrasting the global and local model representations, MOON enhances the generalization ability of the global model in federated environments.
    
    \item \textbf{FedGen~\cite{DBLP:conf/icml/ZhuHZ21}} is a federated learning algorithm using knowledge distillation without data. It employs a lightweight generator on the server side to synthesize data, which is broadcasted to clients to assist their model training. This method not only optimizes the global model but also introduces inductive bias to local models, improving generalization in non-i.i.d. settings.
    
    \item \textbf{FedGH~\cite{DBLP:conf/mm/YiWLSY23}} is a federated learning framework for heterogeneous models. It trains a shared Global Prediction Header (GPH) to integrate diverse model structures from different clients. The GPH is trained using feature representations extracted by clients' private feature extractors and learns global knowledge from various clients. The server then transmits the shared GPH to all clients, replacing their local prediction heads.
    
    \item \textbf{LG-FedAvg~\cite{DBLP:journals/corr/abs-2001-01523}} is a variant of FedAvg that trains both global and local models simultaneously. The global model acts as a classifier, while the local model is a feature extractor. During each iteration, both the classifier and feature extractor are updated concurrently without freezing any part of the model.

    \item \textbf{FedBABU~\cite{DBLP:conf/iclr/OhKY22}} updates only the body of the model during training, leaving the head randomly initialized and unchanged. This allows the global model to improve generalization during training, while the head is fine-tuned for personalization during evaluation, achieving efficient personalization with consistent performance improvements.
    
    \item \textbf{FedCP~\cite{DBLP:conf/kdd/ZhangHWSXMG23}} introduces conditional layers tailored to each client's data, which split the output of a shared extractor into personalized and global representations. The shared classifier handles global representations, while personalized classifiers manage personalized ones. Additionally, FedCP sets a regularization loss, ensuring that global feature representations remain as consistent as possible across rounds.
    
    \item \textbf{GPFL~\cite{DBLP:conf/iccv/ZhangHWSXMCG23}}  personalizes federated learning by incorporating personalized layers into the global model, capturing client-specific features. GPFL aims to adapt to each client’s unique needs while maintaining privacy, making it suitable for scenarios with highly heterogeneous data distributions.

    \item \textbf{FedPer~\cite{DBLP:journals/corr/abs-1912-00818}}  personalizes federated learning by keeping certain model layers (typically the final few) private to each client while sharing the remaining layers globally. This enables each client to fine-tune their local layers for personalized tasks while benefiting from the shared global model. Unlike FedBABU, the local classification head in FedPer is not frozen, and both the feature extractor and classification head are optimized during local training.

    \item \textbf{FedRep~\cite{DBLP:conf/icml/CollinsHMS21}}  first learns a shared representation through a matrix method, followed by alternating updates between clients and the server. FedRep demonstrates strong convergence in multilinear regression problems and significantly reduces sample complexity for new clients joining the system.

    \item \textbf{FedRod~\cite{DBLP:conf/iclr/ChenC22}}  introduces a robust loss function that allows clients to train a universal predictor on non-identically distributed categories. It also includes a lightweight adaptive module (personalized classifier) that minimizes each client’s empirical risk based on the shared universal predictor.
    
    \item \textbf{DBE~\cite{DBLP:conf/nips/ZhangHCWSXMG23}}  is a method designed to tackle data heterogeneity in federated learning. It eliminates domain shifts in the representation space, optimizing the bidirectional knowledge transfer process between the server and clients. DBE sets a group of locally optimized private parameters to align and correct global model discrepancies.
    
\end{itemize}


% \begin{figure*}[htbp]
%     \centering
%     \includegraphics[width=0.9\textwidth]{images/fig10_mnist_data.png}
%     \caption{ 
% Distribution of class data across all clients for the MNIST dataset.
%     }
%     \label{fig:mnist}
%     % \vspace{-0.2cm}
% \end{figure*}



% \begin{figure*}[htbp]
%     \centering
%     \includegraphics[width=0.9\textwidth]{images/fig11_cifar10_data.png}
%     \caption{ 
%     Distribution of class data across all clients for the CIFAR-10 dataset.
%     }
%     \label{fig:cifar10}
%     % \vspace{-0.2cm}
% \end{figure*}



% \begin{figure*}[htbp]
%     \centering
%     \includegraphics[width=0.9\textwidth]{images/fig12_cifar100_data.png}
%     \caption{ 
%     Distribution of class data across all clients for the CIFAR-100 dataset (top 10 classes).
%     }
%     \label{fig:cifar100}
%     % \vspace{-0.2cm}
% \end{figure*}


\subsection{Evaluation Metrics}
In personalized federated learning, the assessment of global accuracy can be formulated as the weighted sum of each client's accuracy rate multiplied by its sample proportion. The formal expression is as follows:

\begin{equation}
\begin{aligned}
A_{total}=\sum_{j=1}^{M}{\frac{N^j}{N} \cdot  A^j}
\end{aligned}
\end{equation}

where $A_{total}$ denotes the weighted total accuracy. $M$ is the total number of clients. $A^j$ represents the accuracy of the $j-th$ client, and $N^j$ is the number of samples from the $j-th$ client. $N=\sum_{j=1}^{M}N^j$ is the total number of samples across all clients. $\frac{N^j}{N}$ signifies the proportion of samples from the $j-th$ client.


\subsection{Concerns about time cost}
The performance here refers to the average improvement across all datasets. For instance, on the MNIST dataset, it has already exceeded 99.80\%. We calculate the improvement by summing the values across all datasets and dividing by the total number of datasets, which may make the performance gains appear less significant.

In our ablation study (Table \ref{tab:table2}), the proposed PM-MOE architecture applied to the recent state-of-the-art DBE algorithm shows an improvement of 3.28\%, particularly on the FMNIST algorithm.

We also have case studies to address your concerns about the local MOE's time consumption. Since local fine-tuning adjusts only a small number of gating network parameters, PM-MOE typically requires just 50 iterations, resulting in minimal overall time consumption. 

\textbf{Case:}
The major computational burden lies in the pre-training phase. For instance, using FedCP on AGNews:
\begin{itemize}
    \item FedCP (Pre-training): 53 hours.
    \item PM-MOE fine-tuning: extra 12.68 minutes (\textbf{0.39\%} of pre-training).
\end{itemize}


% \subsection{Concerns about communication}
% In the pre-training phase, once convergence is achieved, each client uploads its personalized parameters to the server, which then distributes all personalized parameters back to all clients. This process requires only one upload and one download.

% For a quantitative analysis:

% \begin{itemize}
%     \item Each client uses a 32-bit floating point (4 bytes) with a reference bandwidth of 100 Mbps and 20 clients.
%     \item CNN model structure:
%     \begin{lstlisting}[
%     basicstyle=\ttfamily\small, 
%     breaklines=true, 
%     escapechar=@
% ]
%     Conv1: Conv2D (3 @$\times$@ 32, 5 @$\times$@ 5, stride=1) 
%     LeakyReLU
%     MaxPool2D (2 @$\times$@ 2, stride=2).
%     Conv2: Conv2D (32 @$\times$@ 64, 5 @$\times$@ 5, stride=1)
%     LeakyReLU
%     MaxPool2D (2 @$\times$@ 2, stride=2).
%     FC1: Linear (1600 @$\times$@ 512)
%     LeakyReLU
%     FC2: Linear (512 @$\times$@ 100).
% \end{lstlisting}
%     \item Parameter transmission volumes for various methods:
%     \begin{itemize}
%         \item Personalized feature extractors (e.g., FedGH, LG-FedAvg);
%         \item Personalized classifier heads (e.g., FedBABU, FedCP, GPFL, FedPer, FedRep, FedRod);
%         \item Personalized parameters (e.g., DBE).
%     \end{itemize}
%     \item \textbf{Note:} Personalized Experts (PE) include \textbf{P-Feature Extractors} and \textbf{P-Classifier Heads}, while \textbf{Personalized Parameters} are referred to as \textbf{PP}.
% \end{itemize}

% \begin{table}[tbp!]
% \footnotesize
%     \centering
%     \caption{Parameter Transmission Volumes and Time Costs}
%     \label{tab:transmission}
%     \begin{tabular}{cccc}
%         \toprule
%         & \textbf{P-Feature Extractors} & \textbf{P-Classifier Heads} & \textbf{P-Parameters} \\
%         \midrule
%         Total param size & 3.3 MB & 200 KB & 2 KB \\
%         Time cost & 5.54 s & 0.34 s & 0.0033 s \\
%         \bottomrule
%     \end{tabular}
%     \vspace{-0.3cm}
% \end{table}

% \begin{table}[tbp!]
% \footnotesize 
%     \centering
%     \caption{Moe Combination Analysis Results}
%     \label{tab:table3}
%     \begin{tabular}{ccccccc}
%         \toprule
%         Method&MNIST&FMNIST	&Cifar10&Cifar100&TINY&AGNews \\
%         \midrule
%         Fed-Syn-MoE & 89.48 &91.47&78.31	&18.75	&13.59	&93.37 \\
%         % \midrule
%         PM-MOE & 99.85	&98.61	&93.95	&70.68	&66.33	&94.76\\
%         \bottomrule
%     \end{tabular}
%     \vspace{-0.3cm}
% \end{table}



% \subsection{Data Distribution Visualization}
% We demonstrate here the data distribution with S=0 and S=20 in our experiments. As illustrated in Figure \ref{fig:mnist}, \ref{fig:cifar10}, \ref{fig:cifar100}, it can be observed that with S=20, the proportion of category samples across all clients is very low, with data evenly distributed among clients but in relatively small quantities. Consequently, classification performance for such data is generally poor. Each client particularly needs to leverage knowledge from other clients to enhance its performance, necessitating effective use of personalized models converged from other clients.






% \section{Online Resources}

% Nam id fermentum dui. Suspendisse sagittis tortor a nulla mollis, in
% pulvinar ex pretium. Sed interdum orci quis metus euismod, et sagittis
% enim maximus. Vestibulum gravida massa ut felis suscipit
% congue. Quisque mattis elit a risus ultrices commodo venenatis eget
% dui. Etiam sagittis eleifend elementum.

% Nam interdum magna at lectus dignissim, ac dignissim lorem
% rhoncus. Maecenas eu arcu ac neque placerat aliquam. Nunc pulvinar
% massa et mattis lacinia.


% \subsection{Algorithm and Complexity}
% The pseudocode for the algorithm is detailed in Appendix A.2. The identified time components are:

% \begin{enumerate}
%     \item \textbf{Transmission time for personalized parameters}: Complexity \( O(1) \), as parameters are uploaded and downloaded only once.
%     \item \textbf{Local MOE computation}: Complexity \( O(k \cdot n) \), where \( k \) is the number of fine-tuning iterations, and \( n \) is the number of samples.
%     \item \textbf{Energy computation in the MOE component}:
%     \begin{itemize}
%         \item Vector dot product: \( O(D) \), where \( D \) is the vector dimensionality.
%         \item Energy for \( M-1 \) models: \( O((M-1)D) \).
%         \item Summation of exponential terms: \( O(M-1) \).
%         \item Logarithmic operation: \( O(1) \).
%     \end{itemize}
%     Total energy computation complexity: \( O((M-1)D + M) \approx O(MD) \).
% \end{enumerate}
% Overall complexity: \( O(k \cdot n \cdot M \cdot D) \).


% \subsection{Limitations and Future Work}
% The PM-MOE framework is adaptable to various PFL algorithms based on model partitioning. In our implementation, the gating network design targets feature extractors, classifier heads, and personalized model parameters. Future work could explore more effective personalized fine-tuning structures for MOE gating networks. Conversely, new PFL algorithms can be designed to further leverage the characteristics of PM-MOE.
% \FloatBarrier
\balance
\clearpage
% \vfill

% \flushend


\end{document}
\endinput
%%
%% End of file `sample-authordraft.tex'.


