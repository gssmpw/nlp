\section{Benchmark Overview}
\label{sec:overview}
%\wenya{Need to reorganize this section. There are two pieces of information to deliver here; one is the characteristics of the dataset and the other is the distribution plot. For characteristics, we should make a point-wise description instead of enumerating everything without clear separation. Let's break it down. The first paragraph gives a brief summary which is ok. The second paragraph mentions two aspects, namely queries (i.e., textual, factual knowledge) and retrieval knowledge base. Under retrieval knowledge base, you talk about (1) multiple clue images and (2) hard negatives, both reflecting real-world scenarios. Then in the next paragraph, you discuss query again. This should be included in the first point when talking about queries. So for query aspect, you have two points to mention, one is text-only and the other is factual knowledge. Please use clear indicators, either a small title for each point or just numbers to separate each characteristics.}
\vspace{-0.5em}
We propose \dsns, a \textbf{text-only} Question Answering benchmark for challenging, \textbf{visual} knowledge intensive factual questions. It is an evaluation benchmark for visual knowledge centric text-to-image retrieval, as well as a benchmark for evaluating MLLMs' abilities on extracting visual knowledge as augmentation under retrieval-augmented generation settings. 

\vspace{-0.3em}
\paragraph{Visual Knowledge Intensive Queries} 
Our \ds focuses on text-only queries that probe factual knowledge of visual feature which generally hold true of an entity (e.g. ``\textit{What colour are the stamens of Chamaecrista nictitans}''), rather than instance-specific attributes commonly addressed in standard VQA tasks (e.g. ``\textit{What colour is the cup on table}''). Hence, we specifically select the organism domain which enables specialized queries. Unlike existing works, we deliberately exclude images from the queries due to the reason that including an image often reduces the problem to mere entity recognition or unimodal-similarity match, instead of our main objective of cross-modal knowledge extraction. Numerous established benchmarks already cover entity recognition, including the OVEN~\citep{Hu_2023_ICCV} dataset which serves as the foundation for the the InfoSeek dataset, and the iNaturalist 2021 (\citealp{Van_Horn_2021_CVPR}) dataset, which we employ in this work. Moreover, text-only queries better align with real-world scenarios where users tend to raise questions about an entity without an image.
%due to the task nature, if we include an image in the query, the query image shall not depict queried visual feature; however, in real-world scenarios,  chatbot users are unlikely to upload an image and ask about visual features \textbf{not} found in that image.%; rather, they tend to inquire about elements that are visible. For features that are absent, a more natural user question would be, “What is the name of this butterfly?” rather than, “What does the caterpillar of this butterfly look like?” 

\vspace{-0.3em}
\paragraph{Naturally Co-occurring Hard Negative Images} For each question, multiple clue images might be available, reflecting real-world scenarios. Additionally, the dataset incorporates a substantial number of naturally co-occurring hard negative images, i.e., images of the same species that do not exhibit the visual feature in query. Notably, all clue images for a given question can be retrieved purely based on visual information, without reliance on captions.



% Existing works~\citep{chen-etal-2023-pre-trained, Mensink_2023_ICCV} primarily focus on using images to identify entities, with the required knowledge typically existing in textual descriptions of those entities.
 %Previous work~\citep{Chang_2022_CVPR} focuses on questions requiring a single image (or two images for two-hop reasoning question) as a knowledge source, with hard negatives curated from data mining.
 %(see section \ref{sec:eval_ret} for detailed explanation). %Previous work~\citep{Chang_2022_CVPR} relies heavily on captions to identify names or locations of entities.

\vspace{-0.3em}
\paragraph{Dataset Statistics}
The current version of \ds contains 400 queries, forming an image knowledge base of total 103824 images. The average clue image count for each query is 13.89, and the clue images occupy 5.35\% of the image knowledge base. The distribution of organism categories and query types is shown in Figure \ref{fig:q_distribution}.

\begin{figure}[ht]
    \begin{subfigure}[]{\linewidth}
    \centering
        \includegraphics[trim={220px, 15px, -50px, 0px}, scale=0.3, clip]{image/test_pie.pdf}
        \caption{Distribution of organism categories in \ds}
    \end{subfigure}
    \vspace{1em}
    \setlength{\belowcaptionskip}{-0.5cm}
    \begin{subfigure}[]{\linewidth}
    \centering
        \includegraphics[scale=0.3,trim={65px, 65px, 65px, 45px},clip]{image/test_pie_qtype.pdf}
        \caption{Distribution of question categories}
    \end{subfigure}
    \setlength{\belowcaptionskip}{-0.2cm}
    \caption{Distribution of organisms and question categories.} %\wenya{Can you better align the colors? For example, in (a), you can use blue-like colors for animals and yellow-like colors for plants. Then all the subcategories should follow this color code. For (b), use less saturated colors because the texts are hard to visualize now. In addition, the box in (a) looks ugly (missing the right boundary and is too close to the figure).}
    \label{fig:q_distribution}
\end{figure}