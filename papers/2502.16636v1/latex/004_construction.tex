\section{Benchmark Construction}
\label{sec:const}

\subsection{Data source}
Following Encyclopedic VQA~\cite{Mensink_2023_ICCV}, we adopt iNaturalist 2021 (iNat21, \citealp{Van_Horn_2021_CVPR}) as the cornerstone of our benchmark. iNat21 contains 2.7M images of organism (animals, plants, fungi, etc.), spanning across 10000 taxonomy species. Each species in iNat21 has 200–300 images, originally collected for species-level image classification. 

For organism species selection in query construction, we aim to ensure that the chosen species are relatively underrepresented in textual resources. To achieve that, we filter them based on the length of their Wikipedia summary and description sections. Well-known species typically have multiple sections (e.g., habitats, behaviour, etc.), indicating extensive documentation. We therefore include only those species whose summary and description account for no more than 50\% length of the entire Wikipedia article. This also helps minimize reliance on prior knowledge: widely recognized species may have detailed descriptions on visual features across LLM pre-training corpora, even if these details are not explicitly mentioned in Wikipedia.

\subsection{Query Collection}
\paragraph{Question generation.}
Our objective is to create queries about distinct visual features of organisms, ensuring that only a small fraction of images within each species display the queried feature. Each question must yield a unique answer and generally hold true for every instance of that species, preserving both specificity and accuracy. Typically, composing such specialized queries would require domain expertise and a labour-intensive review of the 200–300 images per species.

Consequently, we employ OpenAI-o1~\citep{o1} for query generation to replace human experts. As creating meaningful questions requires expert knowledge, we provide the Wikipedia summary and description sections for the model to understand the species. We explicitly instruct the model NOT to generate questions about properties already covered in these text passages, prompting it to focus on more specialized visual attributes unlikely to appear in standard references. The reasoning and self-validating ability of o1 enables it to follow the complex instructions, leading to high-quality and diverse prototype queries. The model is tasked with generating both a set of candidate queries and their corresponding visual features. A refined human annotator filtering and rewriting process is then applied based on these visual features, to ensure query validity\footnote{More details about the query generation prompt and human filtering process can be found in Appendix \ref{sec:app-anno}, \ref{sec:app_prompt}.}.


%For the 1577 species sampled, we generated $N_{question\_before\_filter}$ questions. For each species, the model generates 2-4 questions.

\paragraph{Question Filtering and Image Annotation.}
The question generation process cannot guarantee compliance with the previously mentioned requirement, i.e., clue images containing the answer should remain a minority in the set; also, query should not be so difficult that no clue image exists in the corpus. We designate a threshold clue image rate, that at most 20\% of images in a species level sub-corpus can be clue.

Firstly, we employ an open-sourced MLLM\footnote{Llama-3.2-11B-Vision-Instruct was used. Prompt template can be found in Appendix \ref{sec:app_prompt}} as a coarse-level filter. For each query, the model iterates over the sub-corpus one image at a time, binary labelling the images by whether the referenced feature is present. These labels are used to roughly filter out queries that exceed the 20\% clue-image threshold. Our rationale is that if an unusually large fraction of images exhibit a visual feature, it is likely prominent for the MLLM to detect and generate labels that are reliable enough.

Secondly, we recruit volunteer university students as annotators to label the images. These annotators identify whether each image provides evidence for the question and thereby confirm whether (a) at least one valid clue image exists, excluding overly difficult queries having no clue image and (b) fewer than 20\% of the images are clue, excluding remaining “easy” queries not recognized by the coarse-filter. At the same time, the annotators provide the ground-truth answers for each query. This two-stage process ensures our final dataset meets both the clue-image minority requirement and the fundamental criterion that each query should be indeed answerable by evidence clue image within the corpus.

In the current version of the benchmark, we initially select around 1000 questions that passed the coarse filter. After human annotation, 400 of these questions satisfy the clue-image rate threshold and are confirmed answerable with at least one valid clue image.
