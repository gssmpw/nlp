\section{Introduction}
\label{sec:intro}

\begin{figure*}[t]
    \centering
    % \setlength{\abovecaptionskip}{0.2cm}
    %\setlength{\belowcaptionskip}{-0.7em}
    \includegraphics[trim={3em, 1em, 0.5em, 1em},width=0.95\textwidth, clip]{image/question_sample.pdf}
    \caption{Comparison on query format and QA pipeline between InfoSeek-style and our \dsns. \textcolor{infoseek}{InfoSeek-style}: The entity in the query image must first be recognized to extract textual evidence, which is then used independently to answer the question. \textcolor{ours}{\dsns}: The query is text-only, but direct answers are rarely available in textual form. The model must retrieve relevant images and interpret them to generate an answer.}
    \label{fig:question_sample}
\end{figure*}

\newcommand{\iconimg}{\includegraphics[trim={-10px, 25px, 0px, 20px},width=1.2em, clip]{image/image.png}}
\newcommand{\icontext}{\includegraphics[trim={-15px, -10px, -15px, 5px},width=1.3em, height=1.02em,clip]{image/text.png}}
\newcommand{\iconarrow}{\includegraphics[width=1.1em]{image/rightarrow.png}}
\newcommand{\iconlink}{\includegraphics[trim={0em, 0em, 0.3em, 0em},width=1em, clip]{image/link.png}}

\begin{table*}[ht]
    \centering
    \begin{adjustbox}{max width=0.85\textwidth}
    \begin{tabular}{l|ccc}
    \toprule
         Benchmark  &Query Modality   &Knowledge Modality  &Retrieval Modality\\
        \midrule
         InfoSeek~\citep{chen-etal-2023-pre-trained}   &\raisebox{-2pt}{\iconimg\icontext} &\raisebox{-2pt}{\icontext}  &\raisebox{-2pt}{\iconimg  \iconarrow \iconimg \iconlink \hspace{-5pt} \icontext \iconarrow \icontext}\\
         E-VQA~\citep{Mensink_2023_ICCV}  &\raisebox{-2pt}{\iconimg\icontext} &\raisebox{-2pt}{\icontext}  &\raisebox{-2pt}{\iconimg  \iconarrow \iconimg \iconlink\hspace{-2pt}\icontext \iconarrow \icontext}\\
         WebQA~\citep{Chang_2022_CVPR}  &\raisebox{-2pt}{\icontext}  &\raisebox{-2pt}{\iconimg\icontext}  &\raisebox{-2pt}{\icontext \iconarrow \icontext\hspace{-2pt}\iconlink \hspace{-3pt} \iconimg}\\
         MRAG-Bench~\citep{hu2024mragbenchvisioncentricevaluationretrievalaugmented} &\raisebox{-2pt}{\iconimg\icontext}  &\raisebox{-2pt}{\iconimg}   &\raisebox{-2pt}{\iconimg\icontext \iconarrow \iconimg}\\
         \ds (Ours) &\raisebox{-3pt}{\icontext}  &\raisebox{-4pt}{\iconimg}   &\raisebox{-4pt}{\icontext \iconarrow \iconimg}\\
    \bottomrule
    \end{tabular}
    \end{adjustbox}
    %\setlength{\belowcaptionskip}{-1.5em}
    \caption{Comparing modalities of knowledge intensive VQA benchmarks. The link icon \raisebox{-2pt}{\iconlink} denotes that the two objects are paired, retrieving one will automatically link to the other. For InfoSeek and E-VQA, \citet{yan-xie-2024-echosight} demonstrated that by image-to-image retrieval on Wikipedia images, the Wikipedia articles containing those images are also retrieved at a significantly higher recall comparing to image-to-text retrieval. Similarly for WebQA, a question-to-caption (text-to-text) retrieval achieves better performance than text-to-image retrieval, as shown in Table \ref{tab:ret_webqa} in Appendix. Our \ds enforces text-to-image retrieval for visual knowledge.}
    \label{tab:comapre_bench}
\end{table*}


We have observed rapid advancement of Large Language Models (LLMs) in recent years. However, they are prone to generating hallucinated responses~\cite{10.1145/3571730}. Retrieval-Augmented Generation (RAG) has emerged as a widely adopted approach to mitigate this challenge~\citep{NEURIPS2020_6b493230, karpukhin-etal-2020-dense, izacard-grave-2021-leveraging}. By retrieving and integrating relevant textual documents from external knowledge sources, it enables LLMs to answer knowledge-intensive questions more accurately. Yet, many real-world scenarios demand multimodal knowledge (e.g., visual cues or geographical maps), suggesting that text-only retrieval may be insufficient.

%Advanced LLM service providers are expanding their models to incorporate modalities beyond text, and particularly, images~\citep{openai_chatgpt_2024, claude, gemini}. The open-source research community has also introduced numerous Multimodal LLM (MLLM) systems~\citep{abdin2024phi3technicalreporthighly, Qwen2VL, chen2024expandingperformanceboundariesopensource,grattafiori2024llama3herdmodels,pixtral}. %This naturally raises the question: \textit{can images be used as augmentation material in RAG, and how effectively could multimodal RAG enhance MLLMs performance in multimodal-knowledge-intensive QA?}

Building on the success of text-only RAG, various evaluation benchmarks have been proposed to assess the capability of Multimodal LLMs (MLLMs, a.k.a. Large Vision Language Models LVLM) in using multimodal RAG to address complex, multimodal knowledge intensive questions, including InfoSeek~\citep{chen-etal-2023-pre-trained}, Encyclopedic-VQA~\citep{Mensink_2023_ICCV}, etc. They feature mixed-modal queries where the question is asked based on a paired image.

%\wenya{This paragraph is not pointwise clear. Need to separate them into points with clear summaries.} 
However, existing datasets reveal several limitations. Firstly, although images are included in the query, \textbf{MLLMs do not utilize the visual knowledge from these images when generating answers}. Secondly, \textbf{questions in these datasets typically do not require reasoning over knowledge extracted from images}. Figure \ref{fig:question_sample} gives an example on InfoSeek-styled query. To answer the query, a model needs to first recognize the entity in query image, then locate the Wikipedia article and augment relevant passage to LLMs to generate an answer. Query image information is only required in the entity recognition step. %Also, the nature of queries makes the relevant knowledge very unlikely to be recorded in images: it is usually regarding an ``encyclopedic'' property of an entity, such as the habitat in this case.
These existing multimodal benchmarks are \textbf{textual} knowledge intensive, highlighting the need for a new benchmark for \textbf{visual} knowledge intensive QA.
%They predominantly focus on text-based knowledge as evidence, and the query image only serves as the anchor of the entity. LLM is \textbf{not utilizing visual knowledge in query image} when answering question.  Also, \textbf{the questions are not regarding visual features}, consequently under such problem setting, \textbf{images cannot be used as augmentation}, and such benchmarks cannot evaluate how effectively retrieval-augmented MLLMs leverage visual knowledge. 

%\quanyu{Motivation: first, previous works' queries contain image, which is not practical in real-world, since text-only query is more common; second, those queries is not visual knowledge-intensive, the visual kownledge-augmented generation ability of VLM cannot be evaluated; third, previous works are retrieving texts and LLM can answer the quries, in this work, we are retrieving the images as visual augmentations, and aim to benchmarking the visual augmented generation. (rethink about the motivations, and reorganize them in this paragraph.)}

To this end, We present \dsns, a visual knowledge intensive QA benchmark designed to evaluate text-to-image retrieval and answer generation augmented by retrieved images. Key features of \ds are:
\vspace{-0.5em}
\begin{enumerate}
    %\item The queries and answers are text-only. The queries are regarding visual features, enabling image to serve as evidence for answering them.
    \item The entities in queries are obscure, with their descriptions being brief, rarely documented in standard references (e.g., Wikipedia)
    \vspace{-0.5em}
    \item The visual features queried are fine-grained, with only 5.35\% of the images in the corpus containing the specific visual evidence needed to answer the questions, posing a challenging text-to-image retrieval task.
    \item The majority of non-clue images are hard-negatives which depict the same entities and are visually similar. This subtle distinction further increases the difficulty of cross-modal retrieval.
\end{enumerate}
%\wenya{The above summary is not well organized. The first feature should mention why text-only queries are necessary. The second and third points both talk about ``long-tail'' and could be combined. You can add an additional point emphasizing the challenge of retrieval here, as we have many hard negatives.}

%\wenya{This paragraph is illogical. You should first refer to Figure 1 as a comparison illustration, following the above three features. Then you need to briefly summarize how you create the dataset instead of only referring to Section numbers. Then following introduction to dataset creation, you can use the next paragraph to briefly introduce your evaluation.}
\vspace{-0.5em}
%The right part of Figure\ref{fig:question_sample} demonstrates the QA workflow in our \ds that differs largely with InfoSeek. Given a textual query, a cross-model retriever needs to first retrieve relevant images within the image knowledge base. There are typically larger amounts of hard negative images than the desired clue images within the retrieved image set. The MLLM, while augmented with the retrieved possibly relevant images, need to distinguish the clue image and extract visual knowledge to generate answers.

To construct \dsns, we begin with LLM-generated candidate queries, which are then refined through human filtering and rewriting. Next, we employ open-sourced MLLM and human annotators to perform another filtering step to ensure an appropriate level of difficulty for the queries. Detailed procedures for the entire benchmark construction can be found in Section \ref{sec:const}. 

%Current version of \ds contains 98 questions with an image knowledge base of 24083 images. The images are photos of wildlife and plants collected from the iNaturalist 2021 (iNat21) dataset~\citep{Van_Horn_2021_CVPR}, a fine-grained organism image classification dataset. 


%It is worth noting that, queries and answers of \ds are \textbf{text-only}, with respect to visual features that are rarely displayed on photos, and such queries are hard to be answered without viewing relevant clue image. Current version of \ds contains 98 questions with an image knowledge base of 24083 images. The images are photos of wildlife and plants collected from the iNaturalist 2021 (iNat21) dataset~\citep{Van_Horn_2021_CVPR}, a fine-grained organism image classification dataset. With \dsns, we are able to evaluate the ability of MLLMs to utilize visual knowledge embodied in retrieved images as evidence for augmented generation. Figure \ref{fig:question_sample} compares the differences in query format and QA workflow between InfoSeek-style benchmark and ours. \quanyu{this paragraph should mention the exclusive key features of the constructed benchmark.}

With \dsns, we comprehensively evaluate 8 mainstream MLLMs, including 5 open-sourced models and 3 proprietary ones. Firstly, we demonstrate that images can serve as powerful evidence for augmented generation through baseline experiments; secondly, we experiment with various RAG settings, testing the ability of MLLMs digesting images as augmentation, and analyse model behaviour on identifying clue from irrelevant images. Our benchmark sheds light on real-world scenarios for evaluating MLLMs, when users raise natural language queries for knowledge-intensive questions where textual evidence is scarce but visual evidence possibly exists, posing the challenging task of retrieving and utilizing relevant images in the wild.

%Other than image as augmentation for multimodal RAG, \ds could also potentially benefit future MLLM research in fields including but not limited to: multi-image reasoning, long-multimodal-context processing 
%\wenya{No need to mention this as the connection is not so clear. It feels more important to point out here that our benchmark significantly benefit real-world scenarios where users raise language queries for knowledge-intensive questions which require challenging retrieval from similar images in the wild}. %\quanyu{more details about how you conduct diverse experiments and how those experiments connect with your motivation. And with such benchmark, researchers can do what kind of research, like long-context, evaluating VLM ability of fetching knowledge in multiple retrieved images.}

The key findings from our experiments are summarized as follows: %\wenya{You are missing an important finding regarding the contrastive behaviors of open-sourced and proprietary models} %\wenya{As shown in my comments following all the points below, a clearer categorization of findings should be included.}
\begin{itemize}
    \item \textbf{Cross-modal retrieval is challenging}. The commonly used small cross-modal retriever (CLIP) faces significant challenges addressing difficult queries requiring the identification of fine-grained visual features. %\wenya{this is about retrieval only}
    \item \textbf{The amount of evidence affects retrieval-augmented generation.} The evaluated MLLMs demonstrate the ability to extract visual knowledge to answer questions. However, a single ground-truth clue image is insufficient for the models to achieve optimal performance.
    \item \textbf{Negatives can be helpful.} Providing multiple images as input, including both clue images and non-clue images, could enhance performance for proprietary models. This improvement likely stems from the models' ability to contrast clue and non-clue images, enabling them to better process the relationship between the question and the images.
    \item \textbf{Inverted dynamics of open-sourced and proprietary models.} Open-source models excel with a single clue image but fail to differentiate the clue when irrelevant images are also introduced; while proprietary models are initially conservative with a single clue but ultimately achieve better performance through multi-image augmentation. This reveals open-source models are weaker at handling multiple images, compared to proprietary models. %Also, when provided with highly irrelevant distractors (easy-negatives), open-source models surprisingly appear to be more confused while the clue becomes more prominent, while proprietary model remain powerful in filtering the distractors.
    %\item \textbf{Highly irrelevant distractors reduce model confidence.} By mixing irrelevant images from other irrelevant entities with clue images, the clue images might appear more distinct -- for instance, if the question is about a bird but the distractors depict fish. Whereas experiments shows that presence of irrelevant images creates additional confusion. It becomes more difficult for models to isolate and interpret the visual evidence.%\wenya{this is about augmented generation using irrelevant distractors.}
    %\item Open sourced models tested exhibit the ``lost-in-the-middle'' phenomenon, they fail to extract visual knowledge when the clue image is not positioned at the beginning of image inputs. On the contrary, proprietary models manage to extract the visual knowledge regardless of position of clue image. \wenya{still on augmented generation, but on the position of the clue image.}
\end{itemize}

