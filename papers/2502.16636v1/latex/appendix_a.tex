\section{Relation with previous works}
\label{sec:app_relation}

Knowledge intensive VQA benchmarks focusing textual knowledge, including but not limited to: ViQuAE~\citep{10.1145/3477495.3531753}, InfoSeek~\citep{chen-etal-2023-pre-trained} and Encyclopedic-VQA~\citep{Mensink_2023_ICCV}, are discussed in related works. These benchmarks contain questions paired with images of certain entities, and knowledge for such entities can be found in an external textual knowledge base (e.g., Wikipedia). The questions are deliberately constructed to omit the entity names (e.g., ``How many feet tall does the plant grow to?'' instead of ``How many feet tall does Acacia paradoxa grow to?'').

Typically, given a question and its associated image, a model is expected to: 1) identify the entity depicted in the image, 2) retrieve relevant documents or passages from a knowledge base that pertain to both the question and the entity, and 3) generate an answer based on the question, image, and retrieved documents. Step 1) and 2) is usually carried out by leveraging vision-language pretrained encoders, such as CLIP~\citep{pmlr-v139-radford21a}, to facilitate cross-modal (image-to-text) retrieval.

While the above mentioned benchmarks are all claimed to be Knowledge-Intensive VQA benchmarks, it is noteworthy that the \textbf{knowledge} to be retrieved is textual knowledge. The image is only served as an anchor of the entity. While cross-modal retrieval was commonly used, the knowledge retrieval process can actually be achieved in uni-modal manner. A recent work~\citep{yan-xie-2024-echosight} achieved ``SoTA'' performance on InfoSeek and E-VQA employing image-to-image retrieval within a knowledge corpus comprising Wikipedia pages and their associated images. Entity recognition is performed by identifying the most visually similar Wikipedia image to the query image, which automatically enables retrieval of the related knowledge document (i.e., the Wikipedia page containing that image). Answers are then generated using a text-only LLM, augmented with the relevant paragraph from the Wikipedia page, which is extracted through fine-grained reranking.

WebQA dataset~\citep{Chang_2022_CVPR} included questions that require retrieving images and utilizing visual knowledge to generate answers. However, its retrieval process is heavily reliant on image captions, as the images themselves typically lack indicators identifying them as clue images for answering a question. For instance, considering the question: ``\textit{Are the land dinosaurs guarded by rail in both the Display Museum of Natural History in University of Michigan and the Museo Jurassic de Asturias?}'' Without captions, the model would be unable to discern which dinosaur fossil image corresponds to which museum.

Table \ref{tab:ret_webqa} shows the retrieval scores in our implementation of retrieval experiments on WebQA benchmark for queries having images as knowledge source. While the queries are text-only, aligning with our setting, the text-to-image retrieval scores significantly lag behind text-to-caption retrieval, especially for lower $k$.

\begin{table}[h]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|ccccc}
    \toprule
        Recall  & @1  &@5 &@10    &@20    &@50 \\
        \midrule
        Text-to-Image &15.17	&37.08	&47.63	&58.43	&72.32\\
        Text-to-Caption &43.25	&68.74	&76.86	&83.79	&89.92\\
    \bottomrule
    \end{tabular}}
    \caption{Retrieval scores for WebQA dataset.}
    \label{tab:ret_webqa}
\end{table}

MRAG-Bench~\citep{hu2024mragbenchvisioncentricevaluationretrievalaugmented} shares a similar objective to ours by proposing a benchmark that requires retrieving visual knowledge to solve questions. However, despite this similarity, the focus remains on entity recognition. Most tasks are structured such that an image displaying an incomplete or partially visible entity is provided, and the retrieval system is tasked with identifying images that depicts the entire entity. This approach primarily enhances the MLLM's ability to recognize entities rather than addressing broader knowledge-intensive tasks. Furthermore, non-clue images in MRAG-Bench are from different entities, which are visually distinct, in contrast to the numerous hard negatives in our \ds. 

Concurrent to our work, the iNaturalist team released 2024 version of their natural image dataset, iNat24~\citep{vendrow2024inquire}, coming with a text-to-image retrieval benchmark named INQUIRE, containing 250 queries. Though, the queries in INQUIRE are designated for image retrieval only, and are not QA style questions that have explicit answers. As iNat24 and iNat21 share the same taxonomy classes of organisms, we do consider adding the new images in iNat24 to our benchmark.

We also note a related line of research~\citep{10.1609/aaai.v37i11.26598,Van_Landeghem_2023_ICCV} on extracting knowledge from document images (e.g., PDF files, Powerpoint slides), conventionally referred to as OCR-based VQA. Although these tasks also require models to retrieve relevant page and interpret images to answer textual queries, the underlying knowledge is primarily textualâ€”obtained via OCR. This focus diverges considerably from our goal of visual feature extraction, where visual cues are inherently sparser in the wild.