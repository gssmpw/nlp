\section{Experimental Setup}
\label{sec:exp}

\subsection{Retrieval Evaluation}
\label{sec:eval_ret}
For retrieval, we evaluate the performance with Normalized Discounted Cumulative Gain (NDCG) and hit rate -- the probability that at least one clue image is found among the top-$k$ retrieved samples (NDCG@k and hit@k). %We omit the recall score as the number of clue images fluctuates greatly across the queries (from 1 to 60). \textcolor{red}{Or don't mention recall at all?}  %We evaluate retrieval performance under three levels of granularity:


%\textcolor{red}{As lack of labels for new queries, delete the other two settings of retrieval, only left within corpus}
%Due to the challenging domain specific queries, the performance of retrievers at whole corpus (2.7M images) or restricted corpus (sub-corpus of the 100k images that are associated with queries) is not satisfactory. Hence, we limit the retrieval corpus to the image set of 200-300 images for each query. 
Due to the challenge of complex domain-specific queries, it is significantly difficult for the retrievers to search through the entire iNat21 corpus (2.7M images) while the query is about a sophisticated feature for a single species. To enhance retrieval effectiveness and emphasize the more fine-grained and distinguishing features of a species, we narrow the search space by limiting the retrieval corpus to a set of 200–300 images corresponding to the species for each query.
In all following experiments, we adopt the commonly used retriever \verb|clip-vit-large-patch14-336|~\citep{pmlr-v139-radford21a} for image retrieval. The search indexes are constructed with the FAISS library~\citep{douze2024faisslibrary} using flat inner product index.

%\paragraph{Within Species Retrieval}
%For each question, text-to-image retrieval is performed within the image set of the corresponding species. Each species has 200–300 images, with 1–60 clue images displaying the visual features referenced in the question.

%\paragraph{Restricted Corpus Retrieval}
%For each question, text-to-image retrieval is conducted within a restricted subset of the full corpus, comprising 24,083 images that are explicitly paired with questions.

%\paragraph{Full Corpus Retrieval}
%For each question, text-to-mixed retrieval is performed over the entire iNat21 corpus of 2.7M images. Given the challenge of distinguishing between visually similar species, particularly for smaller retriever models, we incorporate the species name as a caption and transform to imagetext-to-image retrieval. %Retrieval is then based on the sum of the image embedding and the text embedding of the species name to enhance similarity search performance.



\subsection{Generation Evaluation}
\label{sec:eval_gen}

%\paragraph{AI-assisted evaluation}
Previous work has relied predominantly on exact-match or recall-based metrics to assess answer correctness. However, such methods are prone to overlooking partial hallucinations blended with ground-truth answers. For example, if the true answer is “black and white” but the model predicts “black and white with yellow dots,” the spurious mention of “yellow dots” remains undetected under exact-match criteria.
To address this limitation, we employ GPT-4o as a more nuanced evaluator. Detailed evaluation prompt template can be found in Appendix~\ref{sec:app_prompt}. A series of works, including but not limited to \citet{kamalloo-etal-2023-evaluating,zheng2023judging,huang2024empiricalstudyllmasajudgellm}, have demonstrated that general-purpose LLMs can reliably judge open-ended QA responses. We also manually validate the automatic evaluation results on a sample of 100 predicted answers, achieving a 94\% accuracy rate. %\wenya{Can you cite a relevant work showing LLM is good at open-ended answer evaluation? Provide a percentage showing the alignment between humans and GPT-4o using a small portion of the dataset.} %We provide GPT-4o with the text-only question, ground-truth answer, and predicted answer, along with several in-context examples in the prompt to leverage its in-context learning capability for more accurate evaluation.

In addition, we report the ROUGE score~\citep{lin-2004-rouge}, a metric commonly used in summarization evaluation, as a relaxed version of exact match score. The ROUGE scores are omitted in result analysis section, and can be found in Appendix~\ref{sec:app_results}. %Given that our ground-truth answers often consist of single keywords or short phrases, the ROUGE recall score accommodates variations such as word order changes or phrase segmentation, making it better suited for our dataset compared to strict exact-match criteria.

\subsection{Multimodal LLMs}
We evaluate 3 advanced proprietary models: GPT-4o~\citep{openai_chatgpt_2024}, Gemini-1.5-Pro~\citep{gemini}, Claude-3.5-Sonnet~\citep{claude}; and 5 recent open-sourced models that can process multiple images, of sizes ranging from 4B to 12B: Phi-3.5-vision-instruct (4B, \citealp{abdin2024phi3technicalreporthighly}), Qwen2-VL-7B-Instruct~\citep{Qwen2VL}, InternVL-2.5-8B~\citep{chen2024expandingperformanceboundariesopensource}, Llama-3.2-11B-Vision-Instruct~\citep{grattafiori2024llama3herdmodels}, and Pixtral-12B~\citep{pixtral}. We use the default setting given by each model to perform question answering.


\section{Results}
\subsection{Text-to-Image Retrieval}
\label{sec:ret_result}
\vspace{-0.5em}

\begin{table}[ht]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lrrrrr}
        \toprule
        Metric &@1 &@5 &@10 &@20 &@30 \\
        \midrule
        NDCG  &26.06	&26.46	&30.67	&38.50	&45.80\\
        Hit Rate &26.06	&52.73	&64.55	&72.73	&77.87\\
        Hit Count  &0.26   &1.12   &2.08   &3.60   &4.85\\
        % \midrule
        % \multirow{3}{*}{Restricted}&Recall  &0.34	&1.91	&3.48	&6.50	&9.12\\
        % &Hit Rate &10.23	&31.82	&42.05	&57.95	&63.64\\
        % &Hit Count  &0.10	&0.63	&1.15	&2.15	&3.05\\

        % \midrule
        % \multirow{3}{*}{Full}&Recall  &0.34	&1.14	&2.34	&4.33	&5.55\\
        % &Hit Rate  &10.23	&22.73	&31.82	&42.05	&43.18\\
        % &Hit Count  &0.10	&0.35	&0.75	&1.40	&1.80\\
        \bottomrule
    \end{tabular}}
    %\setlength{\belowcaptionskip}{-0.8em}
    %\setlength{\abovecaptionskip}{0.1em}
    \caption{Retrieval Results using the sub-corpus of 200-300 images for each query. Even within the small species level corpus, the model struggles with our challenging text-to-image retrieval task.}
    \label{tab:ret_result}
\end{table}

Successfully retrieving clue images that contain the required visual knowledge is critical to the overall performance of RAG systems in answering the questions.
The hit rate highlights the difficulty of text-to-image retrieval in our {\dsns} benchmark. Even within the small sub-corpus of 200 - 300 images, there is still 22\% chance that no clue image can be retrieved within the top-30 samples. The NDCG score also indicates the challenge of this task, i.e., the retrieved clue images are not assigned top similarity scores comparing to the hard negative images. 

%We observe a notable drop in retrieval performance for the restricted corpus and full corpus setting compared to the within-species setting. Since the questions focus on the visual features of specific species, distinguishing between visually similar species poses a significant challenge, particularly for smaller retriever models.

\subsection{Retrieval-Augmented Generation}
\label{sec:rag_result}

\begin{table*}[ht]
    \centering
    \resizebox{0.85\textwidth}{!}{
    \begin{tabular}{c|ccccc|ccc}
         \toprule
         Model&Phi3.5-V&Qwen2VL&InternVL2.5&Pixtral&Llama3.2-V&GPT-4o&Gemini&Claude\\[-0.2em]
         \midrule
         \multicolumn{9}{c}{\textit{Baselines}}\\[-0.3em]
         \midrule
    Zero-shot    &27.57	&32.04	&26.17	&30.21	&35.80  &47.21	&42.78	&42.43\\
    Oracle  &32.91	&43.19	&38.58	&37.79	&41.51  &37.65	&18.56	&32.19\\
    Non-clue &27.77	&33.27	&31.53	&34.94	&31.63 &16.61	&6.25	&8.92\\[-0.3em]
         \midrule
         \multicolumn{9}{c}{\textit{Retrieved Top-K}}\\[-0.3em]
         \midrule
    K=1 &\cc{g1}33.47	&\cc{r1}39.24	&\cc{r1}34.13	&\cc{r1}37.45	&\cc{r1}34.59  &\cc{r1}28.09	&\cc{g1}19.42	& \cc{r1}18.53\\
    3   &\cc{r1}31.14	&\cc{r1}38.05	&\cc{r1}32.93	&\cc{g1}\textbf{39.44}	&\cc{r1}36.72  &\cc{r1}35.26	&\cc{g1}45.48	& \cc{r1}28.49\\
    5   &\cc{g1}\textbf{35.66}	&\cc{r1}40.24	&\cc{r1}35.52	&\cc{r1}37.25	&\cc{r1}\textbf{39.51}  &\cc{g1}40.84	&\cc{g1}45.35	& \cc{r1}30.88\\
    7   &\cc{g1}33.67	&\cc{r1}\textbf{40.44}	&\cc{r1}35.33	&\cc{r1}37.25	&\cc{r1}36.65  &\cc{g1}43.82	&\cc{g1}46.81	& \cc{g1}35.86\\
    10  &\cc{g1}33.47	&\cc{r1}39.04	&\cc{r1}\textbf{37.18}	&\cc{r1}36.85	&\cc{r1}37.18  &\cc{g1}44.02	&\cc{g1}48.40	& \cc{g1}38.84\\
    15  &\cc{g1}33.86	&\cc{r1}39.64	&\cc{r1}34.39	&\cc{r1}37.25	&\cc{r1}35.99  &\cc{g1}44.42	&\cc{g1}51.20	& \cc{g1}\textbf{45.62}\\
    20  &\cc{r1}32.27	&\cc{r1}40.84	&\cc{r1}36.59	&-	&-  &\cc{g1}\textbf{47.61}	&\cc{g1}\textbf{53.19}	& \cc{g1}42.23\\
        \hline
        Oracle  &32.91	&43.19	&38.58	&37.79	&41.51    &37.65	&18.56	&32.79\\[-0.3em]
        \midrule
        \multicolumn{9}{c}{\textit{One-in-K}}\\[-0.3em]
        \midrule
    K=3 &\cc{g1}33.03	&\cc{r1}\textbf{40.72}	&\cc{r1}\textbf{36.72}	&\cc{r1}37.21	&\cc{r1}\textbf{37.00}   &\cc{g1}43.90	&\cc{g1}45.64	& \cc{g1}32.87\\
    5   &\cc{g1}\textbf{33.97}	&\cc{r1}39.68	&\cc{r1}35.22	&\cc{g1}\textbf{39.70}	&\cc{r1}35.92 &\cc{g1}45.90	&\cc{g1}46.35	& \cc{g1}35.98\\
    7   &\cc{g1}33.35	&\cc{r1}39.36	&\cc{r1}34.80	&\cc{r1}35.66	&\cc{r1}36.53 &\cc{g1}45.31	&\cc{g1}45.46	& \cc{g1}36.60\\
    10  &\cc{r1}32.27	&\cc{r1}39.16	&\cc{r1}35.61	&\cc{r1}36.57	&\cc{r1}34.51    &\cc{g1}46.14	&\cc{g1}46.90	& \cc{g1}37.63\\
    15  &\cc{r1}32.23	&\cc{r1}39.40	&\cc{r1}34.99	&\cc{r1}37.32	&\cc{r1}34.59    &\cc{g1}\textbf{46.54}	&\cc{g1}46.58	& \cc{g1}\textbf{37.86}\\
    20  &\cc{r1}32.39	&\cc{r1}37.73	&\cc{r1}34.08	&-	&-    &\cc{g1}45.62	&\cc{g1}\textbf{49.57}	& \cc{g1}35.43\\
        \hline
        Oracle  &32.91	&43.19	&38.58	&37.79	&41.51    &37.65	&18.56	&32.79\\[-0.3em]
        \bottomrule
    \end{tabular}}
    %\setlength{\belowcaptionskip}{-1.5em}
    %\setlength{\abovecaptionskip}{0.1em}
    \caption{Main experiment results. The coloured cells shows the difference with oracle score, best performing $k$ values for each model is highlighted with \textbf{boldface}. }
    \label{tab:main_results}
\end{table*}


% \begin{table*}[ht]
%     \centering
%     \resizebox{\textwidth}{!}{
%     \begin{tabular}{c|ccccc|ccc}
%          \toprule
%          Model&Phi3.5-V&Qwen2VL&InternVL2.5&Pixtral&Llama3.2-V&GPT-4o&Gemini&Claude\\
%          \midrule
%          \multicolumn{9}{c}{\textit{Baselines}}\\
%          \midrule
%          Zeroshot   &33.52	&34.66	&26.70	&27.84	&42.05	&52.27	&48.30	&51.14\\
%          Oracle  &40.68	&43.86	&40.12	&39.32	&42.27	&47.95	&46.48	&46.48\\
%          Non-clue    &33.52	&32.61	&29.88	&32.05	&33.86	&17.84	&22.73	&12.96\\
%          \midrule
%          \multicolumn{9}{c}{\textit{Retrieved Top-K}}\\
%          \midrule
% K=1 &\cc{r4}36.93	&\cc{r3}40.91	&\cc{r4}35.80	&\cc{r3}35.80	&\cc{r3}39.20	&\cc{r5}38.07	&\cc{r5}29.55	&\cc{r5}25.00\\
% 3   &\cc{r2}39.20	&\cc{r5}38.07	&\cc{r4}35.80	&\cc{g3}42.05	&\cc{r1}41.48	&\cc{r4}43.18	&\cc{r1}44.89	&\cc{r4}36.36\\
% 5   &\cc{r1}40.34	&\cc{r2}42.61	&\cc{r1}39.77	&\cc{g2}40.91	&\cc{g2}44.32	&\cc{g2}50.57	&\cc{r1}44.89	&\cc{r3}37.50\\
% 7   &\cc{r1}40.34	&\cc{r3}40.91	&\cc{r1}39.20	&\cc{r1}38.64	&\cc{r4}38.64	&\cc{r1}47.73	&\cc{r1}44.89	&\cc{r3}38.07\\
% 10  &\cc{g4}43.75	&\cc{r4}39.77	&\cc{r5}33.52	&\cc{g2}41.48	&\cc{r4}38.07	&\cc{g4}55.11	&\cc{g1}48.30	&\cc{r1}44.32\\
% 15  &\cc{g3}42.61	&\cc{r5}38.64	&\cc{r3}37.50	&\cc{g5}44.32	&\cc{g1}43.18	&\cc{g4}53.98	&\cc{g4}51.70	&\cc{g1}47.73\\
% 20  &\cc{g1}40.91	&\cc{r3}40.91	&\cc{g2}42.61	&-	           &-      	   &\cc{g5}59.09	&\cc{g5}56.25	&\cc{r2}42.61\\
%         \hline
%         Oracle&40.68	&43.86	&40.12	&39.32	&42.27	&47.95	&46.48	&46.48\\
%         \midrule
%         \multicolumn{9}{c}{\textit{One-in-K}}\\
%         \midrule
% K=3 &\cc{g2}42.73	&\cc{r4}39.55	&\cc{r3}36.82	&\cc{g1}39.43	&\cc{r5}35.91	&\cc{g5}55.57	&\cc{g5}51.70	&\cc{r4}42.50\\
% 5   &\cc{r1}40.34	&\cc{r5}37.73	&\cc{r5}35.00	&\cc{r1}38.64	&\cc{r4}37.61	&\cc{g4}54.89	&\cc{g4}50.91	&\cc{r3}43.86\\
% 7   &\cc{r1}40.57	&\cc{r4}39.89	&\cc{r2}38.64	&\cc{r1}38.07	&\cc{r3}40.11	&\cc{g5}56.36	&\cc{g4}50.79	&\cc{r4}42.27\\
% 10  &\cc{g3}43.75	&\cc{r3}41.02	&\cc{r1}39.43	&\cc{r2}36.70	&\cc{r3}38.75	&\cc{g5}56.48	&\cc{g3}50.45	&\cc{r4}42.73\\
% 15  &\cc{r2}38.30	&\cc{r5}37.27	&\cc{r3}36.82	&\cc{r2}36.70	&\cc{r2}40.57	&\cc{g5}55.68	&\cc{g5}51.70	&\cc{r3}43.52\\
% 20  &\cc{r2}37.96	&\cc{r3}39.77	&\cc{r1}39.43	&-	&-	&\cc{g3}53.52	&\cc{g5}52.05	&\cc{r5}40.91\\
%         \hline
%         Oracle&40.68	&43.86	&40.12	&39.32	&42.27	&47.95	&46.48	&46.48\\
%         \bottomrule
%     \end{tabular}}
%     \caption{Temp caption for main results}
%     \label{tab:main_results}
% \end{table*}

\paragraph{Does image as augmentation benefit MLLMs?} 

\noindent To assess whether image can be leveraged as evidence in the RAG system, we conduct baseline experiments and compare performance under three conditions: 1) \textbf{Zero-shot}: directly prompting MLLMs with questions without retrieval; 2) \textbf{Oracle}: augmenting the question with a gold clue image; 3) \textbf{Non-clue}: augmenting each question with a non-clue image within the same species, which does not contain visual knowledge relevant to the question\footnote{For 2) and 3), the results reported are the average of 5 runs with different clue / non-clue images selected as augmentation.}, as shown in the first section of Table \ref{tab:main_results}.

All five open-sourced models exhibit performance gains of up to 15 points when using oracle image as an augmentation. This indicates the necessity of including relevant visual knowledge to answer the questions. We conclude that for open-sourced models, a single relevant image can serve as good evidence to enhance RAG performance.

Whereas interestingly, oracle performance for proprietary models drops below the zero-shot setting. We hypothesize that the single-image clue provides insufficient evidence to establish the correct visual relationship, causing the model to respond conservatively with ``I cannot decide''. These conservative responses count as failing to produce an answer, thus reducing the overall scores. Specifically, such conservative response occurs in 15.8\% of samples for GPT-4o, 7.4\% for Gemini, and 16.4\% for Claude. Meanwhile, results for the Non-clue setting indicate that the models effectively identify these non-clue images as not containing visual knowledge. Specifically, all proprietary models frequently respond with ``I don't know'' with non-clue images, leading to a substantial drop in scores comparing to the zero-shot and oracle setting.

%Most open-sourced models also experienced some performance decline compared to the oracle setting. However, for InternVL and Pixtral, performance slightly exceeded the zero-shot baseline. We attribute this phenomenon to the possibility that, even when the provided image lacks direct visual knowledge, it may assist the model in identifying the species referenced in the question. This, in turn, allows the model to leverage its prior knowledge, enabling it to make a more educated guess.

\paragraph{Can MLLMs distinguish the clue from retrieved hard negatives?}

We have shown that a relevant clue image can serve as good evidence in augmentation. However, retrieval performance for these challenging queries is far from perfect, as shown in the previous section. For RAG system in real implementation where a number of possibly irrelevant images are retrieved, does retrieval still help the final QA performance?

To explore this realistic RAG scenario, we provide the top-$k$ retrieved images, derived from Section \ref{sec:ret_result}, as augmentation to the MLLMs\footnote{For pixtral and Llama3.2-V, $k=20$ was not evaluated due to hardware limitation.}. %Image retrieval is conducted under the within-species setting, which represents the easiest retrieval scenario. 
The retrieved images are sorted in descending order based on their similarity scores. 
As shown in the second section of Table \ref{tab:main_results}, the open-source models generally lag behind their single-oracle scores, indicating limited multi-image processing capabilities. Most open-source models struggle to isolate the clue image and effectively leverage its visual information when it is mingled with hard negatives.

%However, most open-sourced models except phi3.5 show renewed improvement as $k$ further increases, reaching an optimal range at $k=20$.

In contrast, all three proprietary models exhibit a generally monotonic increase in performance as $k$ increases, except Claude which decreases at $k=20$. Notably, the performance at $k=5$ begins to surpass oracle baselines, continuing to improve along with higher $k$ values.
We attribute this improvement to the comparative evaluation enabled by providing both clue and non-clue images. Compared to a single clue image, the top-$k$ RAG setting allows the models to contrast relevant and irrelevant visual features, leading to greater confidence in extracting knowledge from the clue images, whereas a single clue image may be insufficient to fully establish its relationship to the question. However, the proprietary models fail to exceed their zero-shot scores until fairly large $k$ values, suggesting that interpreting complex visual cues remains challenging, and they may even struggle with reconciling wrongly interpreted visual knowledge with their prior knowledge gained through pre-training.

As indicated by the retrieval results, even at $k=20$, there is 27\% chance that no clue image is included in the retrieved set. This likely explains the low QA performance at smaller $k$ values, where failing to retrieve the clue image entirely negates any benefit of visual augmentation.
To investigate further, we conduct an additional experiment, constructing image sets in which exactly one clue image and $k-1$ non-clue images are provided\footnote{Both clue and non-clue images were randomly selected across 5 different combinations, results reported are the average of 5 runs for each $k$.}. The clue image is positioned at the beginning of the $k$ images to simulate the perfect retrieval scenario.

Demonstrated in the last section of Table \ref{tab:main_results}, the open-sourced models now exhibit decreasing performance as $k$ increases, further strengthen the conclusion of their limited capacity for handling multiple images simultaneously. By contrast, all three proprietary models surpass their single-image oracle baseline from $k=3$ onward and maintain near-constant performance at higher $k$ values. This suggests that proprietary models are capable of isolating the clue image from irrelevant images, regardless of the number of input images\footnote{Additional experiments on randomly positioning the clue image within the set is also carried out. The proprietary models keeps the flat performance curve against $k$, indicating there is no positional bias.}.

%While the open-sourced models largely retained their ``N''-shaped performance curves, the proprietary models exhibited almost flat performance curves. This suggests that proprietary models are capable of isolating the clue image from irrelevant images, regardless of the number of input images for $k$ values up to 20. This behavior also explains the monotonic increasing trend observed in the basic RAG setting, where a larger $k$ improves performance by increasing the chance of retrieving clue images.

\begin{table}[h]
    \centering
    \small
    \begin{tabular}{l|p{4em}p{5em}}
    \toprule
         Model& Zeroshot Score & Augmenting Text \\
         \midrule
         Phi3.5-V   &27.57  & 26.02\\
         Qwen2VL    &32.04  & 25.61\\
         InternVL2.5   &26.17  & 19.70\\
         Pixtral    &30.21  & 19.74\\
         Llama3.2-V &35.80  & 29.73\\
    \bottomrule
    \end{tabular}
    \setlength{\belowcaptionskip}{-0.3em}
    \caption{Results when augmenting retrieved Wikipedia text as evidence.}
    \label{tab:aug_text}
\end{table}


\subsection{Ablation}
\paragraph{Can the answers for \ds be found in Wikipedia?}
%It is a natural question that, descriptions of appearance of organisms can also be made in textual form, and maybe augmenting such knowledge can also let LLMs answer these questions regarding visual features?

%We collect a subset of Wikipedia articles corresponding to the 9614 species that have a matched Wikipedia page in the iNat21 dataset. %We then applied basic text chunking to these articles, splitting them into chunks of approximately 150 tokens. Each chunk was carefully bounded within a single section of the original article to preserve contextual integrity. Sections such a ``Reference'', ``See also'', ``External links'', etc., which lack informative content, were excluded. 
We construct a small textual knowledge base from Wikipedia articles for the 10k species in iNat21 dataset. This minimum knowledge base comprising approximately 97,000 chunks, each consisting of $\sim$150 tokens. The detailed collection process is described in Appendix \ref{sec:text_kb}. For text retrieval, we use a pre-trained dense text retriever, DRAGON Plus~\citep{lin-etal-2023-train}. Given that the ground-truth labels for relevant text chunks are not available, we only assess whether the retrieved text chunks correspond to the correct Wikipedia page, and the retriever achieves HitRate@10 of 97\%. Next, we present the QA performance across open-sourced models augmented with top-10 retrieved textual knowledge chunks. As shown in Table \ref{tab:aug_text}, augmenting the question with textual knowledge from Wikipedia does not improve answer quality for most models. Instead, in numerous cases, models fail to locate relevant information within the augmented text and output ``I don't know''. This behaviour in turn leads to scores lower than the zero-shot setting.

These experiments provide strong evidence that the knowledge required to answer the questions related to visual features in our benchmark is unlikely to be contained in textual knowledge bases.

% \begin{table}[h]
%     \centering
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{l|p{4em}p{4.5em}p{5em}p{6em}}
%     \toprule
%          Model& Oracle Score & Best TopK Score & Ensemble w/ ret similarity &Ensemble w/o ret similarity \\
%          \midrule
%          Phi3.5-V   &40.68  &43.75  &35.80&38.64\\
%          Qwen2VL    &43.86  &42.61  &40.91&42.05\\
%          InternVL2.5   &40.12  &42.61  &42.61&40.91\\
%          Pixtral    &39.32  &44.32  &43.18&39.77\\
%          Llama3.2-V &42.27  &44.32  &39.77&47.73\\
%     \bottomrule
%     \end{tabular}}
%     \caption{Ensemble top-30 performance}
%     \label{tab:ensemble}
% \end{table}

\paragraph{Is More Clues Always Better?}
We observed that for proprietary models, top-$k$ scores at higher $k$ values excel their 1-in-$k$ score. Similarly, the open-source models also achieve their best performance at larger $k$ value in top-$k$ setting than in 1-in-$k$.  A likely explanation is that the expected number of clue images retrieved in top-$k$ is substantially higher than when only a single clue image is guaranteed. We in turn ablate with effect of multiple clue images. Fixing $k=10$, we experiment with varying numbers of clue images with $n\in\{1,2,3,4,5\}$ ($n$-in-10). We select queries that contain at least 5 clues to enable this experiment. As shown in Figure \ref{fig:nin10}, all models exhibit increasing performance as $n$ grows, aligning with the intuition that additional clue images provide more robust visual evidence.
%From the task nature, the queries require some extent of ``induction'', that it need to see more than one instance to induce the general fact (e.g. in Figure 1 in intro, ``What color is stamen of sensitive cassia?'' -- see one image having purple color, but not sure about others.) In this sense, it naturally requires viewing multiple images, preferably multiple clue images.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth, trim={12px, 5px, 15px, 10px},clip]{image/nin10.pdf}
    %\setlength{\abovecaptionskip}{-1.2em}
    %\setlength{\belowcaptionskip}{-1em}
    \caption{Impact from number of clue images}
    \label{fig:nin10}
\end{figure}

\paragraph{Hard-negatives can be helpful; how about easy-negatives?}
As an ablation to the previous experiment on whether multiple images benefit the overall QA performance, we now mix one clue image with $k-1$ highly irrelevant images from subsets that are not the query species. The results indicate that both open-source and proprietary models benefit more from hard negatives coming from the same species compared with easy negatives coming from different species. This further strengthen our conclusion that hard negatives are helpful in contrasting the queried features to inform the correct answer.
%have instead been confused by the highly irrelevant images, underlining our previous observation that models need to reason across multiple images of the queried species to confirm which visual feature is related to the question. 
Detailed results can be found in Table \ref{tab:diff_distractor} in Appendix.


%\paragraph{\wenya{Same here, change to questions} Ensemble of single retrieved image input does not fit problem setting in \ds} \wenya{This is still interesting. You can just summarize your findings here and move all the details to Appendix as this is not high priority.}

%In earlier text-based RAG models, a popular approach involves having the generator LLM process one piece of the top-$k$ retrieved documents at a time as augmentation, followed by generating predictions through an ensemble of $k$ rounds. The ensemble can be implemented in two ways: Token-by-token aggregation—calculating the log probabilities of each token at each position across all $k$ branches; or Sequence-level voting—generating $k$ output sequences. These methods were primarily developed to overcome the input sequence length limitations of earlier LLMs, which typically could not handle sequences exceeding 4096 or 8192 tokens. Also, introducing the retrieval similarity may benefit model in assessing relevance of knowledge documents to the query. Here, we apply the sequence level voting strategy.

%However, as demonstrated in our baseline experiments, the performance of the one-oracle clue image setting is generally outperformed by the standard retrieval setting. Consequently, we do not expect the ensemble setting to achieve substantially better performance than the oracle setting, and we do observe QA scores in ensemble setting not surpassing normal top-$k$ setting as shown in Table \ref{tab:ensemble}.

%\textcolor{violet}{When multiplying softmax normalized retrieval probability, there hardly exists candidate after top-3, and the last most candidate ever selected is the 7th, for top-30 ensemble.}
