\section{Related Work}
While numerous benchmarks evaluate the capabilities of MLLMs, we focus here on those designed for knowledge-intensive visual question answering (VQA), where RAG is the primary solution paradigm.  Table \ref{tab:comapre_bench} compares the modalities covered by commonly used benchmarks. A more detailed discussion of how \ds differs from prior work is provided in Appendix~\ref{sec:app_relation}.
%\vspace{-0.8em}

\paragraph{Textual Knowledge Centric VQA Benchmarks}
OK-VQA~\citep{Marino_2019_CVPR} and its augmented version A-OKVQA~\citep{10.1007/978-3-031-20074-8_9} focus on commonsense or world knowledge, though they do not strongly depend on external knowledge retrieval. ViQuAE~\citep{10.1145/3477495.3531753}, InfoSeek~\citep{chen-etal-2023-pre-trained} and Encyclopedic-VQA  (E-VQA, \citealp{Mensink_2023_ICCV}) address ``encyclopedic'' questions, paired with query images of specific entities, utilizing information retrieved from external textual sources (e.g., Wikipedia) to answer.

%There exists a series of works focusing on knowledge-intensive VQA (KI-VQA), e.g. the OK-VQA~\citep{Marino_2019_CVPR} and its augmented version A-OKVQA~\citep{10.1007/978-3-031-20074-8_9}. These benchmarks are featuring commonsense knowledge or world knowledge. Each question is paired with one image displaying a certain scenario, and the question usually requires some extent of reasoning within the scenario. Some questions are hard to be answered solely without searching knowledge from external knowledge base, though, these benchmarks' main focus is on reasoning with visual information rather than knowledge retrieval from external KB.

%KB-centric benchmarks were then proposed, including but not limited to: ViQuAE~\citep{10.1145/3477495.3531753}, InfoSeek~\citep{chen-etal-2023-pre-trained} and Encyclopedic-VQA~\citep{Mensink_2023_ICCV}. These benchmarks contain questions paired with images of certain entities, and knowledge for such entities can be found in an external textual knowledge base (e.g., Wikipedia). The questions are deliberately constructed to omit the entity names (e.g., ``How many feet tall does the plant grow to?'' instead of ``How many feet tall does Acacia paradoxa grow to?'').

%Typically, given a question and its associated image, a model is expected to: 1) identify the entity depicted in the image, 2) retrieve relevant documents or passages from a knowledge base that pertain to both the question and the entity, and 3) generate an answer based on the question, image, and retrieved documents. Step 1) and 2) by leveraging vision-language pretrained encoders, such as CLIP~\citep{pmlr-v139-radford21a}, to facilitate cross-modal (image-to-text) retrieval.
%\vspace{-0.8em}
\paragraph{Visual Knowledge Centric QA Benchmarks}
%\wenya{In summary, I do not see a very convincing justification of the difference and exciting contribution compared to the following two related works based on the descriptions. More distinct and interesting features should be emphasized.} 
While the above benchmarks all position themselves as knowledge-intensive VQA tasks, it is important to note that the required knowledge remains primarily textual, with images functioning mostly as entity anchors. In contrast, to our knowledge, only two related works truly require image retrieval as evidence for answering questions.

WebQA~\citep{Chang_2022_CVPR} is an earlier effort that requires retrieving images for visual knowledge. However, it heavily relies on captions because the images themselves usually lack indicators of which are clue images. 
%\textcolor{red}{Can delete the following example if overlength} For example, consider the query: ``\textit{Are the land dinosaurs guarded by rail in both the Display Museum of Natural History in University of Michigan and the Museo Jurassic de Asturias?}'' Without captions, the model cannot discern which fossil image belongs to which museum.
A concurrent work, MRAG-Bench~\citep{hu2024mragbenchvisioncentricevaluationretrievalaugmented} shares a similar motivation in using image retrieval for question answering. However, it primarily aims to recognize an entity when the instance in query image is incomplete or partially obscured, relying on retrieved images that present a complete view of the same entity. This entity-recognition objective differs from our focus on answering visual-feature-oriented factual queries. 
%\textcolor{red}{Delete the detailed discussion if overlength} Furthermore, non-clue images in MRAG-Bench are from different entities, which are visually distinct, in contrast to the numerous hard negatives in our \ds. 
As queries in MRAG-Bench include image, the model can use the “shortcut” of matching a visually similar retrieved candidate, bypassing the need for deeper text-to-image reasoning.
%\wenya{This comparison is not convincing to me. From Table 1, it seems they use both textual and visual features to retrieve? Can we mention this distinction? How about long-tail? And again, shall we emphasize the challenge of retrieval due to hard negatives and augmented generation due to the necessity of differentiating sophisticated features in similar images in a separate paragraph as a clear distinction from existing methods (not sure if this is indeed a distinction)?}


%While the above mentioned benchmarks are all claimed to be Knowledge-Intensive VQA benchmarks, it is noteworthy that the \textbf{knowledge} to be retrieved is textual knowledge. The image is only served as an anchor of the entity. While cross-modal retrieval was commonly used, the knowledge retrieval process can actually be achieved in uni-modal manner. A recent work~\citep{yan-xie-2024-echosight} achieved ``SoTA'' performance on InfoSeek and E-VQA employing image-to-image retrieval within a knowledge corpus comprising Wikipedia pages and their associated images. Entity recognition is performed by identifying the most visually similar Wikipedia image to the query image, which automatically enables retrieval of the  related knowledge document (i.e., the Wikipedia page containing that image). Answers are then generated using a text-only LLM, augmented with the relevant paragraph from the Wikipedia page.

%An earlier work, WebQA dataset~\citep{Chang_2022_CVPR}, also included questions that require retrieving images and utilizing visual knowledge to generate answers. However, its retrieval process is heavily reliant on image captions, as the images themselves typically lack indicators identifying them as clue images for answering a question. 

%A concurrent work, MRAG-Bench~\citep{hu2024mragbenchvisioncentricevaluationretrievalaugmented} shares a similar objective to ours by proposing a benchmark that requires retrieving visual knowledge to solve questions. However, despite this similarity, the focus remains on entity recognition. Most tasks are structured such that an image displaying an incomplete or partially visible entity is provided, and the retrieval system is tasked with identifying images that depicts the entire entity. This approach primarily enhances the MLLM's ability to recognize entities rather than addressing broader knowledge-intensive tasks.

%\subsection{OCR fashioned VQA}

