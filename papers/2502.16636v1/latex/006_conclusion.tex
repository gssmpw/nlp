\section{Conclusion}
In this paper, we present \dsns, a challenging VQA benchmark that requires visual knowledge as evidence to answer questions. Different from previous works that include images as part of queries and let models retrieve textual knowledge, our \ds is formed with text-only queries and enforces text-to-image retrieval for challenging visual knowledge. We evaluate 8 popular MLLMs under various multimodal RAG settings. The results indicate that images can serve as knowledge sources for answering visual knowledge intensive questions, but the models struggle to effectively extract visual knowledge from a single clue image. Surprisingly, the current models are able to reason across clue and non-clue images for better understanding of the questions. As one of the pioneer works in visual-knowledge centric multimodal RAG, our benchmark can help promote the research and development of MLLMs in extracting fine-grained visual knowledge and enhance cross-modal retrieval in a more realistic setting.

\section*{Limitations}

While our \ds  addresses the lack of visual knowledge intensive QA benchmarks, it has several design constraints. First, our query generation and manual rewriting process focus on single-hop questions only. Although a small subset of queries might involve simple comparisons within the same image, such as checking the colour of a bird’s upper beak vs. lower beak, there are no truly multi-hop queries requiring reasoning across multiple images. Designing such multi-image or even cross-species queries would demand significantly more expert knowledge, and we leave this as an avenue for future work.

Secondly, \ds focuses exclusively on organisms, which raises questions about the broader generalizability of our findings. We chose the domain of organisms because they allow for factual queries regarding visual features that remain consistent across multiple individuals of a species, yet are present in only a minority of images. Other commonly used domains in knowledge-intensive VQA such as buildings, artworks, news event, etc., do not always meet these criteria, as answers may not hold universally for all instances or there exists only one instance, and the required visual features are often too prevalent or too trivial to foster a genuinely challenging retrieval scenario.

Lastly, the overall scale of \ds is limited compared to larger benchmarks like InfoSeek and Encyclopedic-VQA. Their heuristic-driven methods for automatically generating queries and answers do not readily apply to our specialized setting, which hinges on expert-informed checks to ensure the presence (or absence) of specific visual features. Despite these constraints, \ds represents an important step toward truly visual knowledge intensive QA, and we hope it will inspire further research into more complex, multi-hop, and cross-domain tasks.

\section*{Ethical Considerations}
\paragraph{Licensing}
We do not own, nor do we redistribute the images of iNaturalist 2021 dataset. \ds only provides the annotations of queries and answers, as well as image labels on whether it is clue to query, linked to image GUID in iNatrualist 2021 dataset. All images in iNat21 dataset are shared under one of the following Creative Commons licenses, in particular: CC BY 4.0, CC BY-NC 4.0, CC BY-NC-ND 4.0, CC BY-NC-SA 4.0, CC0 1.0, CC BY-ND 4.0, CC BY-SA 4.0. Any usage of the images is subject to iNaturalist and iNaturalist 2021 dataset's term of use. We strictly adhere to the intended non-commercial research use granted by the iNaturalist community and iNaturalist 2021 dataset, and we share our annotation in \ds under CC BY-NC 4.0 license. The authors assume no responsibility for any non-compliant usage or legal/ethical issues related to the original iNaturalist 2021 dataset.

\paragraph{Potential Risks}
The annotation in this dataset, i.e. the queries, answers and image labels, are intended for research purposes only. Our dataset’s queries, answers, and image labels are provided by volunteer annotators who are not professional taxonomists or biologists. We make every effort to ensure correctness, such as proactively consulting publicly available expertised resources of biology and taxonomy over internet, and annotators are undergraduate students who possess sufficient English proficiency to utilize such publicly available resources for fact validation. Though, we cannot guarantee the absence of factual errors. We make no guarantees regarding completeness or correctness for real-world decision-making.

Our annotation does not contain any personally identifiable information, nor does it reference private data, locations, or individuals. Our annotation does not reveal exact geographical coordinates or sensitive ecological information that might pose risks to endangered species or protected habitats. For other potential risks regarding images in iNaturalist 2021 dataset, please refer to the original paper and iNaturalist website (www.inaturalist.org).