Our main theorem states that in the noiseless setting, given a sufficiently large dataset and a warm start, Algorithm \ref{alg:pgd} converges exponentially fast to a solution equivalent to the ground-truth matrix. For the sake of conciseness, we introduce the following constants: 
\begin{align*}
    \gamma \triangleq 2/(n_1(n_2 - 1)), \quad
    \tau \triangleq \xi/\Xi, \quad
    \alpha \triangleq \xi \gamma \sigma^*_r.
\end{align*}
Let $\mathcal{B}(\varepsilon) = \{Z: \norm{\Delta(Z)}_{F}^2 \leq \varepsilon\sigma^*_r\}$ denote a `ball' around the true solution. With this notation in place, we can now state the main theorem.
\begin{restatable}{theorem}{tM}\label{thm:main}
    Suppose the following conditions hold:
    \begin{itemize}
        \item The dataset $\mathcal{D}$ consists of $m$ i.i.d. samples generated according to the model presented in Section \ref{sec:generative_model}
        \item The number of samples $m$ is at least $10^7\left(\mu r \kappa / \tau \right)^2 n \log\left(8n/\delta\right)$ for some $\delta \in (0,1)$.
        \item The initial point $Z^0$ lies in $\mathcal{B}(\tau/50)$.
        \item The stepsize satisfies $\eta \alpha \leq 2.5 \cdot 10^{-6} (\tau/\mu r \kappa)^2$.
    \end{itemize}
    Then, with probability at least $1-\delta$, the iterates $Z^1, Z^2, \ldots$ of Algorithm \ref{alg:pgd} satisfy:
    \begin{align*}
        \norm{\Delta(Z^t)}_F^2 \leq \left(1 - \frac{\alpha\eta}{4} \right)^{t} \norm{\Delta(Z^0)}_F^2 \quad \forall \ t \in \mathbb{N}.
    \end{align*}
\end{restatable}
We highlight two important takeaways from the above theorem. First, the dependence on the problem size is $O(nr^2\log n)$, which is near-optimal.
Second, for a well-chosen step-size, the algorithm convergences exponentially at rate $O((\tau/\mu r \kappa )^2)$. Although the constants in the sample complexity result and convergence rate are quite large in the statement of Theorem \ref{thm:main}, our experimental results in Section \ref{sec:simulations} show that in practice, these constants are moderate.
The proof of this theorem follows the typical proof of the convergence of projected gradient descent for a strongly convex and smooth function. The strong-convexity like property comes from Lemma \ref{lem:strong_convexity} and the smoothness property from Lemma \ref{lem:smoothness}. The full proof of Theorem \ref{thm:main} is given in Appendix \ref{sec:main_proofs}.

\newpage




