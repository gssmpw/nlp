The matrix completion problem can be stated as follows: recover a low-rank matrix given a small subset of its entries, possibly corrupted by noise. There are two approaches that provide theoretically optimal solutions for this problem. One approach involves posing a nuclear norm minimization program, subject to the constraints that some select entries must match the observation. \citet{candes2009exact} was the first to theoretically establish that a low rank matrix can be recovered exactly, given a small, randomly sampled, subset of its entries (without noise). Later work establish similar guarantees for the setting with noisy observations \citep{candes2010matrix, negahban2012restricted}. 

An alternate approach to pose the problem in its \textit{matrix factorization} form. This is based on the observation that a low-rank matrix $X$ admits a factorization into two smaller matrices $(U, V)$: $X = UV^T$. One can pose an squared-loss minimization problem in terms of the factors $(U, V)$ \citep{mnih2007probabilistic}. While this alternate formulation leads to a nonconvex objective function, it is much faster to solve and yields good results on real data \cite{koren2009matrix}. This led to a lot of research on trying to explain why gradient-based solutions were able to find a good solution to this nonconvex optimization problem.

The work of \citet{keshavan2010matrix} was the first to provide theoretical guarantees for this nonconvex formulation. They first perform a singular value decomposition of the partially observed matrix, which leads to a candidate solution close to the ground-truth. Next, using this matrix as a starting point, they show that a gradient-descent like method converges to the true solution. Other works have built upon this initial result to show slightly stronger theoretical guarantees with improved proof techniques \citep{chen2015fast, sun2016guaranteed, zheng2016convergence}. All these works follow the two-step approach prescribed by \citet{keshavan2010matrix}; they focus on proving that there exists a basin of attraction around the true solution that is sufficiently large. 
Notably, all these papers use a key concentration result developed by \citet{candes2009exact} (Theorem 4.1). This result, in turn, relies on the following assumptions
(i) the ground-truth matrix is \textit{incoherent} (no row or column of the matrix dominates the rest) and (ii) the observed entries are chosen uniformly at random from all the entries of the matrix. 


Further work on this problem has led to more impressive results. Firstly, \citet{ge2016matrix} and  \citet{ge2017no} show that all local minima are global in the nonconvex formulation. This implies gradient-based methods are guaranteed to converge to a global optimum, even without the initialization procedure. Secondly, \citet{ma2020implicit} shows that gradient descent has implicit regularization and thus can converge to the optimal solution without an explicit regularizer or a projection operation. 
