Learning from comparison data is an area that has seen a lot of work in recent years. A central problem in this space is to learn scalar parameters for each item, indicative of their preference order/rank, given a dataset of pairwise comparisons. A popular approach to solve this problem is to assume the comparisons arise from a probabilistic choice model, such as the Bradley-Terry-Luce choice model. 
Theoretical guarantees for learning the parameters of this model have been established by \citet{negahban2012iterative}, \citet{maystre2015fast}, and \citet{shah2016estimation}.

In addition to the offline setting, the active learning problem of choosing a sequence of item pairs to present to the users in order to learn their preferences quickly has been studied. The contextual dueling bandit problem is one such learning framework that has rigorous theoretical guarantees \citep{saha2021optimal, bengs22stochastic}. Recently, active learning from comparisons has been used to fine-tune large language models \citep{ouyang2022training}.

The problem of learning a low-rank user-item score matrix from comparison data was first formulated by \citet{rendle2009bpr}. They applied this learning framework to implicit user feedback such as views, clicks, and purchases, and showed that such data is better treated as ordinal information (a preference of the viewed item over the rest) instead of cardinal information (a positive rating of the viewed item). The similarity of this problem to the matrix completion formulation were brought out explicitly by \citet{park2015preference}. They posed both a convex version of the problem, for which they derived generalization error bounds, and a nonconvex version, which they applied to a comparison dataset derived from movie ratings (higher rated movie is preferred over a lower rated one). They showed that such a procedure yields identical results compared to processing the ratings directly. 
\citet{negahban2018learning} studies this problem in much greater detail, providing matrix recovery guarantees with optimal sample complexity. It also looks at  generalizations such as sampling item pairs in a nonuniform fashion and learning from one-out-of-$k$ choices. 
Ultimately, the paper focuses only on the convex formulation, stating that the analysis of the corresponding nonconvex formulation is an important open problem.
\newpage