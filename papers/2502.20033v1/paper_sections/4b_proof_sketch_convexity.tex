\paragraph{Showing Strong Convexity}
We begin by breaking down the proof of Lemma \ref{lem:strong_convexity} into three smaller lemmas. 
\begin{restatable}{lemma}{lSCA}\label{lem:convexity_algebra}
    For any $Z \in \overline{\Incoherentset}$,
    \begin{align*}
        \llangle \nabla \mathcal{L}, \Delta \rrangle &\geq \frac{\xi}{2} \mathcal{D}\left(\Delta\solset^T\right) - \frac{5\Xi}{8} \mathcal{D}\left(\Delta\Delta^T\right) %
    \end{align*}
\end{restatable}

\begin{restatable}{lemma}{lSCLB}\label{lem:convexity_lowerbound}
    Let some $\epsilon, \delta \in (0, 1)$ be given. Suppose the number of samples $m$ exceeds $96 \mu r  \left(\kappa/\epsilon\right)^2 n\log\left(n/\delta\right)$. Then, with probability at least $1 - \delta$, $\forall \ Z \in \mathcal{H},$
    \begin{align*}
         \mathcal{D}\left(\Delta\solset^T\right) \geq \gamma\left((1- \epsilon)\sigma^*_r\norm{\Delta}_F^2 + 2 \llangle \solset_U \Delta_V^T, \Delta_U \solset_V^T \rrangle \right).
    \end{align*}
\end{restatable}

\begin{restatable}{lemma}{lSCUB}\label{lem:convexity_upperbound}
    Let some $\epsilon, \delta \in (0, 1)$ be given. Suppose the number of samples $m$ exceeds $845  \left(\mu r \kappa/\epsilon\right)^2 n \log\left(n/\delta\right)$. Then, with probability at least $1 - \delta$, $\forall \ Z \in \overline{\Incoherentset} \cap \mathcal{B}(\epsilon)$, 
    \begin{align*}
        \Dataset(\Delta\Delta^T)\leq 10 \epsilon \gamma  \sigma^*_r \norm{\Delta}_F^2.
    \end{align*}
\end{restatable}

Using these three lemmas, Lemma \ref{lem:strong_convexity} can be derived in a straightforward manner (proof in Appendix \ref{sec:main_proofs}). Indeed, if we ignore the cross-term $\llangle \solset_U \Delta_V^T, \Delta_U \solset_V^T \rrangle$ in Lemma \ref{lem:convexity_lowerbound}, it is not hard to see that the three lemmas combined lead to the lower bound $\llangle \nabla \mathcal{L}, \Delta \rrangle \geq O(1) \gamma \sigma^*_r \norm{\Delta}_F^2$. The gradient of the regularizer helps cancel out this cross-term, but leads to the additional $\norm{\Delta D \solset}_F^2$ term.

The steps in the proof of Lemma \ref{lem:convexity_algebra} are algebraic in nature and largely follow the pattern presented in \citet{zheng2016convergence}; the proof is given in Appendix \ref{sec:initial_lemmas}. The main technical contribution of our work is in the proof of Lemmas \ref{lem:convexity_lowerbound} and \ref{lem:convexity_upperbound}. Although the statements of these lemmas are similar to Lemmas 10 and 8 respectively of \citet{zheng2016convergence}, we prove these results in different ways. We outline the broad steps taken to prove these results, filling in the details in Appendices \ref{sec:lower_bound} and \ref{sec:upper_bound} respectively. 


A key step to prove Lemma \ref{lem:convexity_lowerbound} is to show the identity:
\begin{align}
\label{eq:quadratic_form1}
    &\mathcal{D}\left(\Delta\solset^T\right) = v^T S_{\Dataset}v, \ \text{where} \ v \triangleq \vectext{\Delta R^T}, \nonumber \\
    & \quad S_{\Dataset} \triangleq \frac{1}{m} \sum_{k = 1}^m a_ka_k^T,\  a_k \triangleq \vectext{(A_k+A_k^T) Z^*}. 
\end{align}
Here, we use the notion of vectorization of a matrix, \textit{i.e.}, stacking the columns of a matrix to form a vector. Thus, for a matrix $Z \in \Real{n \times r}$, $\vectext{Z}$ is a vector in $\Real{nr}$. 

Given this quadratic form, it follows that:
\begin{align*}
    \vert \Dataset\left(\Delta\solset^T \right) - \mathbb{E}\left[\Dataset\left(\Delta\solset^T \right)\right] \vert 
    & \leq \norm{S_{\Dataset} - \mathbb{E}[S_{\Dataset}]}_{2} \norm{v}_2^2   
\end{align*}
The term $\norm{S_{\Dataset} - \mathbb{E}[S_{\Dataset}]}_{2}$ can be bounded with high probability using the matrix Bernstein inequality (see Lemma \ref{lem:SD_concentration}). To complete the proof of Lemma \ref{lem:convexity_lowerbound}, it remains to calculate $\mathbb{E}\left[\Dataset\left(\Delta\solset^T\right)\right]$. In Lemma \ref{lem:expectation_of_D}, we show that \(\mathbb{E}\left[\Dataset\left(\Delta\solset^T\right)\right] = \gamma \norm{\Delta_U\solset_V^T + \solset_U\Delta_V^T}_F^2.\)




The proof of Lemma \ref{lem:convexity_upperbound}, just like the one for Lemma \ref{lem:convexity_lowerbound}, involves analyzing a quadratic form around a random matrix, which we split into the mean (expectation) term and the deviation from the mean. We show that:
\begin{align*}
    &\Dataset(\Delta \Delta^T) = y^T B_{\Dataset} y =  y^T \mathbb{E}[B_{\Dataset}] y + y^T (B_{\Dataset} - \mathbb{E}[B_{\Dataset}]) y; \\
    &y \in \Real{n} : \, y_j = \norm{\Delta_j}_2^2 \, \forall j , \ B_{\Dataset} = \frac{1}{m}\sum_{(u; i, j) \in \Dataset} e_u(\Tilde{e}_i + \Tilde{e}_j).
\end{align*}
The first term is bounded above with the warm-start assumption: $\norm{\Delta}_F^2 \leq O(1) \sigma^*_r$. The second term is bounded using the matrix Bernstein inequality (see Lemma \ref{lem:BD_concentration}). 
