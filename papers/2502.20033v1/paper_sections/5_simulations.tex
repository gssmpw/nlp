


\textbf{Data Generation:} We generated a random ground truth matrix $X^*\in\mathbb R^{n_1\times n_2}$ with entries selected independently at random according to normal distribution and calculated its rank-$r$ SVD, $U^*\Sigma^*V^{*T}$. 
We have two settings: a low-dimensional setting with $(n_1,n_2)=(200,300)$ and a high-dimensional setting with $(n_1,n_2)=(2000,3000)$. In both settings, we had $r=3$, $\mu\approx 1.01, \kappa=1.1$. 
Using this matrix, we randomly and independently collected $m$ comparison data points. Specifically, for each setting, the comparison dataset took the form $\{(A_k, w_k):k=1,\ldots,m\}$, where $A_k$ represents the $k\textsuperscript{th}$ sampling matrix as in \eqref{eq:def_A2} and and $w_k=g(\llangle A_k, Z^*Z^{*T}\rrangle)$. In this work, we set the regularizer coefficient to be $\lambda = \gamma/40$.
Subsequently, we applied Algorithm \ref{alg:pgd} using the stepsize $\eta$ as recommended by Theorem \ref{thm:main} (Our code can be found \href{https://docs.google.com/document/d/e/2PACX-1vSVqAZIjeDTXuklix4ETGayBs3TMl0lbPxiukHshpuJXtyUyrZQ92TOvFqYA_rqCYyU0ES5YIFT2Rz7/pub}{here}).
The quality of the algorithm's output at iteration $t$ is measured by $||\Delta(Z^t)||_F/\sqrt{n_1n_2}$.
Figure \ref{fig:1} presents the resulting plots.


\textbf{Initialization:} We initialize the algorithm with $Z^{0T} = Z^{*T} + \vartheta(N_1^T, N_2^TJ)$, where $N_1 \in \mathbb{R}^{n_1 \times r}$ and $N_2 \in \mathbb{R}^{n_2 \times r}$, with their entries drawn from a standard normal distribution.  For our experiments, we use $\vartheta \in \{0.5, 1, 2\}$. Figures \ref{fig:1} (a) and (c) show the effect of different initial solutions and also the projection steps in low and high dimensional settings, respectively. 
In both settings, the number of data points $m$ and also the stepsize were chosen as recommended in Theorem \ref{thm:main}. This result confirms the linear convergence of Algorithm \ref{alg:pgd} as predicted by our theoretical analysis. It is important to emphasize that while both a warm start and the projection step are required for our theoretical guarantees, these simulation results suggest that they are not needed in practice.

\textbf{Dataset size:} We examine the impact of dataset size $m$ on the algorithm's performance. Figures \ref{fig:1} (b) and (d) demonstrate the resulting normalized errors in low and high dimensional settings, respectively.  
As depicted in these plots, a large enough $m$ leads to linear convergence of the algorithm while for a small $m$, the error $\norm{\Delta(Z^t)}_F$ does not zero as $t$ increases. In both plots, the red curves show the converges rate for $m$ computed by $c_0(\mu r\kappa)^2n\log(n/\delta)$ with $\delta=0.05$ and $c_0$ being $1/4$ for low-dimensional and $1/2$ for high-dimensional setting. 

\input{paper_sections/5a_figures}
