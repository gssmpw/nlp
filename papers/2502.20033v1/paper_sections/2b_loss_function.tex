Given any $Z \in \Real{n \times r}$, we interpret $Z$ as the concatenation of some candidate user features $U \in \Real{n_1 \times r}$ and item features $V \in \Real{n_2 \times r}$. 
The likelihood of the dataset $\Dataset$ under $Z$ is simply the probability of observing $\Dataset$ if the data was generated according to the parameters $Z$. 
In this work, we use the maximum likelihood approach to learn the latent parameters. \textit{I.e.,} we use the negative log likelihood as the loss function, which we shall minimize using a gradient-descent-like method. 
Here, we present the loss function and its gradient, using notation that will be useful later on.

Let $e_1, e_2, \ldots e_{n_1}$ denote unit vectors in $\Real{n_1}$ and let $\Tilde{e}_1, \Tilde{e}_2, \ldots, \Tilde{e}_{n_2}$ denote unit vectors in $\Real{n_2}$. Let $\llangle C, D \rrangle = \sum_{i,j} c_{i,j}d_{i,j}$ denote the matrix inner product between two matrices of the same size. Therefore:
\begin{align}\label{eq:def_A1}
    \llangle e_u(\Tilde{e}_i - \Tilde{e}_j)^T, X^* \rrangle = x^*_{u,i} - x^*_{u,j}.
\end{align}

For any triplet $(u; i, j)$, define the corresponding \textit{sampling matrix} $A \in \Real{n \times n}$ to be:
\begin{align}\label{eq:def_A2}
    A = \begin{bmatrix}
        0 & e_u(\Tilde{e}_i - \Tilde{e}_j)^T\\
        0 & 0
    \end{bmatrix} \Rightarrow \llangle A, Y^* \rrangle = x^*_{u,i} - x^*_{u,j}.
\end{align}
In the equation above, $0$ denotes matrices with all entries zero of the appropriate size. With this notation, for any data point $((u; i, j), w)$, we have:
\begin{align}
    \mathbb{P}(w = 1 \, | \, (u; i, j)) = \mathbb{P}(w = 1 \, | \, A) = g(\llangle A, Y^* \rrangle).
\end{align}


Given a binary outcome $w$, the likelihood of the outcome under a Bernoulli distribution with parameter $p$ is $p^{w}(1-p)^{1-w}$. Therefore, the negative log-likelihood of this observation is $-w\log(p) -(1-w)\log(1-p)$. Next, consider a datapoint $((u; i,j), w)$ with the corresponding sampling matrix $A$. The negative log-likelihood of this observation under our model with parameters $Z \in \Real{n \times r}$ is 
\begin{align*}
    -w \log(g(\llangle A, ZZ^T \rrangle)) - (1-w) \log(1 - g(\llangle A, ZZ^T \rrangle)).
\end{align*}
Let $A_k$ denote the sampling matrix corresponding to the datapoint $\mathcal{D}_k$. Then, for the entire dataset, the (normalized) negative log likelihood is given by:
\begin{align}\label{eq:log_likelihood}
    \Loglikelihood(Z) &= \frac{1}{m} \sum_{k = 1}^{m} -w_k \log(g(\llangle A_k, ZZ^T \rrangle)) \nonumber \\
    &\quad - (1-w_k) \log(1 - g(\llangle A_k, ZZ^T \rrangle)).
\end{align}
The gradient of $\Loglikelihood(Z)$ is 
\begin{align} \label{eq:gradient_likelihood}
    \nabla \Loglikelihood(Z) &= \frac{1}{m}\sum_{k=1}^{m}
    h_k (A_k+ A_k^T) Z, \text{ where }\\
    h_k &\triangleq \frac{g'(z_k)\left(g(z_k) - w_k\right)}{g(z_k)(1-g(z_k))}, \ z_k \triangleq \llangle A_k,ZZ^T \rrangle. \nonumber
\end{align}
Here, $\nabla \Loglikelihood(Z)$ is a matrix of the same size as $Z$ while $h_k$ and $z_k$ are scalars. %
