

\subsection{Matrix Inner Product Identities}

We state some basic identities of the matrix inner product operator, which are trivial to verify but are used frequently in the paper. In the following identities, $D, E,$ and $F$ are arbitrary matrices so long as their sizes are compatible with the equations.
\begin{align}
    \llangle E, F \rrangle &= \Tr{EF^T} = \Tr{FE^T} \label{eq:identity_trace} \\
    \llangle E, F \rrangle &= \llangle F, E \rrangle = \llangle E^T, F^T \rrangle \label{eq:identity_transpose} \\
    \llangle DE, F \rrangle &= \llangle D, FE^T \rrangle = \llangle E, D^TF \rrangle, \quad \llangle D, EF \rrangle = \llangle DF^T, E \rrangle = \llangle E^TD, F \rrangle \label{eq:identity_shift}
\end{align}
From these identities, we get that for any sampling matrix $A$ (defined in \eqref{eq:def_A2}) and any $Y, Z \in \Real{n \times r}$:
\begin{align}\label{eq:A_AT_identity}
    \llangle (A + A^T)Y, Z \rrangle &= \llangle AY, Z \rrangle + \llangle A^TY, Z \rrangle \nonumber \\
    & = \llangle A, ZY^T \rrangle + \llangle A^T, ZY^T \rrangle \nonumber \\
    & = \llangle A, ZY^T + YZ^T \rrangle
\end{align}

Let $W$ and $Z$ be two matrices in $\Real{n \times r}$. Recall the notation convention introduced in Section \ref{sec:proof}. Using the above identity and \eqref{eq:def_A2}, we get that for any sampling matrix $A$ corresponding to the triplet $(u; i, j)$,
\begin{align}\label{eq:A_AT_YZT_identity}
    \llangle (A + A^T), WZ^T \rrangle &= \llangle e_u(\Tilde{e}_i - \Tilde{e}_j)^T, W_UZ_V + Z_UW_V \rrangle\\
    &= \llangle W_u, Z_i - Z_j \rrangle + \llangle Z_u, W_i - W_j \rrangle \label{eq:A_AT_YZT_identity2}
\end{align}


\subsection{The Frobenius Norm of the Product of Two Matrices}
Let $X$ be any matrix and let $\sigma_{\max}(X)$ and $\sigma_{\min}(X)$ denote the largest and smallest singular values of $X$. Let $v$ be any vector such that the product $Xv$ is compatible.
By the definition of singular values:
\begin{align*}
    \sigma_{\min} (X)\norm{v}_2 \leq \norm{Vx}_2 \leq \sigma_{\max}(X)\norm{v}_2
\end{align*}
Using this basic fact, we can prove the following result.
\begin{lemma}\label{lem:bounds_on_product_norms}
    Let $U \in \Real{n_1 \times r}$ and $V\in \Real{n_2 \times r}$ be any two matrices. Let $\sigma_1(U) \geq \ldots \geq \sigma_r(U)$ denote the singular values of $U$ and $\sigma_1(V) \geq \ldots \geq \sigma_r(V)$ denote the singular values of $V$. Then $\norm{UV^T}_F^2$ satisfies the following bounds:
    \begin{align*}
        \sigma_r(U)^2 \norm{V}_F^2 &\leq \norm{UV^T}_F^2 \leq \sigma_1(U)^2 \norm{V}_F^2 \\
        \sigma_r(V)^2 \norm{U}_F^2 &\leq \norm{UV^T}_F^2 \leq \sigma_1(V)^2 \norm{U}_F^2         
    \end{align*}
\end{lemma}
\begin{proof}
    We first prove the inequality $\norm{UV^T}_F^2 \geq \sigma_r(U)^2 \norm{V}_F^2$. Let $V_j$ denote the $j\textsuperscript{th}$ row of $V$, written as a column vector ($r \times 1$ matrix). Let $(UV^T)^j$ denote the $j\textsuperscript{th}$ column of $UV^T$. Finally, note that the squared Frobenius norm of a matrix is the sum of the squared $\ell_2$ norms of its rows or of its columns. Stitching together these simple facts, we get.
    \begin{align*}
        \norm{UV^T}_F^2 &= \sum_{j = 1}^{n_2} \norm{(UV^T)^j}_2^2 = \sum_{j = 1}^{n_2} \norm{UV_j}_2^2 \\
        &\geq \sum_{j = 1}^{n_2} \sigma_r(U)^2 \norm{V_j}_2^2  = \sigma_r(U)^2 \sum_{j = 1}^{n_2} \norm{V_j}_2^2 \\
        &= \sigma_r(D)^2 \norm{V}_F^2
    \end{align*}    
    The upper bound $\norm{UV^T}_F^2 \leq \sigma_1(U)^2 \norm{V}_F^2$ can be derived using the same steps, except we use the inequality $\norm{UV_j}_2 \leq \sigma_1(U) \norm{V_j}_2$ instead of $\norm{UV_j}_2 \geq \sigma_r(U) \norm{V_j}_2$. Finally, the second set of bounds follow by applying the first set of bounds to the matrix $VU^T$, and noting that $\norm{UV^T}_F = \norm{VU^T}_F$.
\end{proof}


\subsection{The Incoherence of the Iterates}
Recall that we have assumed that the initial point $Z^0$ satisfies the bound $\norm{\Delta(Z^0)}_F^2 \leq \sigma^*_r/16$, i.e., we are given a warm start (see Section \ref{sec:algorithm}). With this assumption, we can prove the following lemmas.
\begin{lemma}\label{lem:solset_in_C}
    Let $\Incoherentset$ be the set defined in \eqref{eq:def_incoherent_set}, i.e.,
    \begin{align*}
        \Incoherentset \triangleq \left\{Z \in  \mathbb{R}^{n \times r} \, : \, \norm{Z}_{2, \infty} \leq \frac{4}{3}\sqrt{\frac{\mu}{n} }\norm{Z^0}_F \right\}
    \end{align*}
    Then all the equivalent ground-truth matrices lie in $\Incoherentset$, i.e. $\solset \subseteq \Incoherentset$.
\end{lemma}
\begin{proof}
Start with the identity $Z^0 = \solset(Z^0) + \Delta(Z^0)$ (which follows from \eqref{eq:def_difference_sol}). By the triangle inequality, we get
\begin{align*}
\norm{\solset(Z^0)}_F - \norm{\Delta(Z^0)}_F  \leq \norm{Z^0}_F \leq \norm{\solset(Z^0)}_F + \norm{\Delta(Z^0)}_F.
\end{align*}
Note that all matrices in $\solset$ have the same Frobenius norm. This implies that $\norm{\solset(Z^0)}_F = \norm{Z^*}_F$. Combining this with the bound on $\norm{\Delta(Z^0)}_F$, we get
\begin{align}\label{eq:Z0_bounds}
    \norm{Z^*}_F - \sqrt{\sigma^*_r}/4  \leq \norm{Z^0}_F \leq \norm{Z^*}_F + \sqrt{\sigma^*_r}/4
\end{align}
Recall that the singular values of $Z^*$ are $\sqrt{2\sigma^*_1}, \sqrt{2\sigma^*_2}, \ldots, \sqrt{2\sigma^*_r}$. We know that the Frobenius norm of a matrix is the $\ell_2$ norm of the vector of its singular values. Therefore:
\begin{align*}
    \norm{Z^*}_F &= \sqrt{2\sum_{i = 1}^r \sigma^*_i} \Rightarrow \frac{\sqrt{\sigma^*_r}}{4} \leq \frac{\norm{Z^*}_F}{4} \\
    \Rightarrow \norm{Z^0}_F &\geq \norm{Z^*}_F - \sqrt{\sigma^*_r}/4 \geq \frac{3}{4}\norm{Z^*}_F \\
    \Rightarrow \norm{Z^*}_{2, \infty} &= \sqrt{\mu/n}\norm{Z^*}_{F} \leq \frac{4}{3}\sqrt{\mu/n}\norm{Z^0}_F 
\end{align*}
Thus, we see that $Z^* \in \Incoherentset$. Because all $Z \in \solset$ have the same $\ell_2/\ell_\infty$ norm, it follows that $\solset \subseteq \Incoherentset$. 
\end{proof}


Before proceeding further, we introduce some new notation. Recall the convention (established in Section \ref{sec:proof}) that any matrix $Z$ can be viewed as a concatenation of two matrices: $Z = (Z_U, Z_V)$. To index the rows of $Z$, we use $Z_u, u \in [n_1]$ for the user features and $Z_i, Z_j, j \in [n_2]$ for the item features. In expressions involving matrix multiplication, we view $Z_u, Z_i, Z_j$ as row vectors, i.e., as $1 \times r$ matrices. By the definition of $\norm{Z}_{2, \infty}$, we get: 
\begin{align}\label{eq:def_l2inf_norm}
    \norm{Z}_{2, \infty} = \max \{\max_{u \in [n_1]} \norm{Z_u}_2, \max_{i \in [n_2]} \norm{Z_i}_2\}.
\end{align}

Equipped with this new notation, we can state and prove the next result.
\begin{lemma}\label{lem:incoherence_of_projection}
    For any $Z \in \Incoherentset$, let $W = \mathcal{P}_{\mathcal{H}}(Z)$. Then $W \in \overline{\Incoherentset}$, i.e., $W$ satisfies
    \begin{align*}
        \norm{W}_{2, \infty}^2 \leq \frac{12\mu}{n} \norm{Z^*}_F^2
    \end{align*}
\end{lemma}
\begin{proof}
    Let $z$ denote the mean of the rows of $Z_V$, i.e.,
    \begin{align*}
        z \triangleq \frac{1}{n_2} \sum_{i \in [n_2]} Z_i
    \end{align*}
    It follows that
    \begin{align*}
        \Rightarrow \norm{z}_2 &= \frac{1}{n_2} \norm{\sum_{i \in [n_2]} Z_i}_2 \leq \frac{1}{n_2} \sum_{i \in [n_2]} \norm{Z_i}_2 \leq \frac{1}{n_2} \sum_{i \in [n_2]} \norm{Z}_{2, \infty} = \norm{Z}_{2, \infty} \quad (\text{by \eqref{eq:def_l2inf_norm}})
    \end{align*}
    The operation of projecting onto the subspace $\mathcal{H}$ is such that $W_U = Z_U$ and $W_i = Z_i - v$ for all item rows $i$ (see Section \ref{sec:symmetries}). By the triangle inequality, we get:
    \begin{align*}
        \norm{W_i}_2 &= \norm{Z_i - z}_2 \leq \norm{Z_i}_2 + \norm{z}_2 \\
        \Rightarrow \max_{i \in [n_2]} \norm{W_i}_2 &\leq \max_{i \in [n_2]} \norm{Z_i}_2 + \norm{z}_2 \leq \norm{Z}_{2, \infty} + \norm{z}_2 \leq 2\norm{Z}_{2, \infty}
    \end{align*}
    Because the rows of $U$ remain unchanged, we have
    $\norm{W}_{2, \infty} \leq 2\norm{Z}_{2, \infty}$.

    Next, note that $Z \in \Incoherentset$. Therefore, 
    $$\norm{Z}_{2, \infty} \leq \frac{4}{3}\sqrt{\frac{\mu}{n}} \norm{Z^0}_F \leq \frac{5}{3}\sqrt{\frac{\mu}{n}} \norm{Z^*}_F$$
    The last step uses the inequality $\norm{Z^0}_F \leq (5/4)\norm{Z^*}_F$, which follows from \eqref{eq:Z0_bounds} in the derivation of Lemma \ref{lem:solset_in_C}.
    By combining the above inequalities, we get the desired result:
    \begin{align*}
        \norm{\Hat{Z}}_{2, \infty}^2 \leq 4\norm{Z}_{2, \infty}^2 \leq 4\frac{25}{9}\frac{\mu}{n} \norm{Z^*}_F^2\leq \frac{12\mu}{n} \norm{Z^*}_F^2.
    \end{align*}
\end{proof}
The above result is important because it establishes a useful bound that holds for all iterates $Z^t, t \in \mathbb{Z}_+$.
(Recall that Algorithm \ref{alg:pgd} takes successive projections, first on to $\Incoherentset$ and then onto $\mathcal{H}$.) 

\subsection{Bounds on the Scores}
In this subsection, we derive two related bounds on any $Z \in \Real{n \times r}$ and any sampling matrix $A$:
\begin{align}
    |\llangle A, ZZ^T \rrangle| &\leq 2\norm{Z}_{2, \infty}^2 \label{eq:score_bound1}\\
    \norm{(A + A^T)Z}_F^2 &\leq 6\norm{Z}_{2, \infty}^2 \label{eq:score_bound2}
\end{align}
Before we prove these bounds, let us explore its consequence. By the definition of the incoherence parameter $\mu$ \eqref{eq:def_mu}, $\norm{Z^*}_{2, \infty}^2 = (\mu/n) \norm{Z^*}_{F}^2$. Therefore,
\begin{align}
    |\llangle A, Z^*Z^{*T} \rrangle| &\leq \frac{2\mu}{n}\norm{Z^*}_F^2 \label{eq:score_bound1_Zstar}\\
    \norm{(A + A^T)Z^*}_F^2 &\leq \frac{6\mu}{n}\norm{Z^*}_F^2 \label{eq:score_bound2_Zstar}
\end{align}
Moreover, for all $Z \in \overline{\Incoherentset}$,
\begin{align}
    |\llangle A, ZZ^{T} \rrangle| &\leq \frac{24\mu}{n}\norm{Z^*}_F^2 \label{eq:score_bound1_Zcbar}\\
    \norm{(A + A^T)Z}_F^2 &\leq \frac{72\mu}{n}\norm{Z^*}_F^2 \label{eq:score_bound2_Zcbar}
\end{align}
As argued in the previous subsection, all iterates $(Z^t)_{t \in \mathbb{Z}_+}$ of Algorithm \ref{alg:pgd} lie in $\overline{\Incoherentset}$ and consequently satisfy the above bound.

We now proceed to the derivation of \eqref{eq:score_bound1}.
Let $Z\in \Real{n \times r}$ be some candidate feature matrix and let $X = Z_UZ_V^T$ be the corresponding score matrix. Let $(u; i, j)$ be an arbitrary triplet and let $A$ denote the corresponding sampling matrix. Recall the definition of the sampling matrix $A$ corresponding to a triplet $(u; i, j)$ from \eqref{eq:def_A1} and \eqref{eq:def_A2}. We have
\begin{align*}
    |\llangle A, ZZ^T \rrangle| = |x_{u,i} - x_{u,j}| = |\langle Z_u, (Z_i - Z_j) \rangle| \leq \norm{Z_u}_2 \norm{Z_i - Z_j}_2 \leq \norm{Z_u}_2 (\norm{Z_i}_2 + \norm{Z_j}_2) \leq 2\norm{Z}_{2, \infty}^2
\end{align*}
The last inequality follows from the definition of $\norm{Z}_{2, \infty}$ (see \eqref{eq:def_l2inf_norm}).

The derivation of \eqref{eq:score_bound2} proceeds as follows.
\begin{align*}
    A &= \begin{bmatrix}
        0 & e_u(\Tilde{e}_i - \Tilde{e}_j)^T\\
        0 & 0
    \end{bmatrix} \\
    \Rightarrow A + A^T &= \begin{bmatrix}
        0 & e_u(\Tilde{e}_i - \Tilde{e}_j)^T\\
        (\Tilde{e}_i - \Tilde{e}_j)e_u^T & 0
    \end{bmatrix} \\
    \Rightarrow (A + A^T)Z &= \begin{bmatrix}
        0 & e_u(\Tilde{e}_i - \Tilde{e}_j)^T\\
        (\Tilde{e}_i - \Tilde{e}_j)e_u^T & 0
    \end{bmatrix} \begin{bmatrix} Z_U \\ Z_vV \end{bmatrix} \\
    &= \begin{bmatrix} e_u(\Tilde{e}_i - \Tilde{e}_j)^T Z_V \\ (\Tilde{e}_i - \Tilde{e}_j)e_u^T Z_U \end{bmatrix} \\
    &= \begin{bmatrix} e_u(Z_i - Z_j) \\ (\Tilde{e}_i - \Tilde{e}_j)Z_u \end{bmatrix} \\
    \Rightarrow \norm{(A + A^T)Z}_F^2 &= \norm{e_u(Z_i - Z_j)}_F^2 + \norm{(\Tilde{e}_i - \Tilde{e}_j)Z_u}_F^2 \\
    &=  \norm{e_u}_2^2\norm{Z_i - Z_j}_2^2 + \norm{\Tilde{e}_i - \Tilde{e}_j}_2^2\norm{Z_u}_2^2 \\
    &= \norm{Z_i - Z_j}_2^2 + 2\norm{Z_u}_2^2 \quad (\norm{e_u}_2^2 = 1, \ \norm{\Tilde{e}_i - \Tilde{e}_j}_2^2 = 2) \\
    &\leq 2(\norm{Z_i}_2^2 + \norm{Z_j}_2^2) + 2\norm{Z_u}_2^2 \quad (\norm{Z_i - Z_j}_2^2 \leq (\norm{Z_i}_2 + \norm{Z_j}_2)^2 \leq 2(\norm{Z_i}_2^2 + \norm{Z_j}_2^2)) \\
    &\leq 6\norm{Z}_{2, \infty}^2 \quad (\text{by definition of $\norm{Z}_{2, \infty}$ \eqref{eq:def_l2inf_norm}})
\end{align*}
This establishes the second inequality.

\subsection{The Matrix Bernstein Inequality}
Here, we state a special version of the matrix Bernstein inequality that we use in our proofs. The statement is identical to Corollary 6.2.1 in \cite{tropp2015introduction}, barring a change in notation. %

This concentration result is stated in terms of the operator norm of a matrix $X$, which we denote as $\norm{X}_{2}$  and is defined as follows:
\begin{align}\label{eq:def_operator_norm}
    \norm{X}_{2} \triangleq \sup_{v: \norm{v}_2 = 1} {\norm{Xv}_2}
\end{align}
It follows that $\norm{X}_{2} = \sigma_{\max}(X)$.
For square matrices $X$, an alternate definition of the operator norm is:
\begin{align}\label{eq:def_operator_norm2}
    \norm{X}_{2} \triangleq \sup_{v: \norm{v}_2 = 1} {v^TXv}
\end{align}

\begin{lemma}[Matrix Bernstein Inequality]\label{lem:matrix_bernstein}
    Consider a random matrix $X$ of shape $n_1 \times n_2$ that satisfies:
    \begin{align*}
        \mathbb{E}[X] = \Bar{X} \quad \text{ and } \quad \norm{X}_2 \leq L \text{ almost surely}.
    \end{align*}
    Let $b$ be an upper bound on the second moment of $X$:
    \begin{align*}
        \norm{\mathbb{E}[XX^T]}_2 \leq b \quad \text{ and } \quad \norm{\mathbb{E}[X^TX]}_2 \leq b.
    \end{align*}
    Let $X_{\mathcal{D}} = \frac{1}{m}\sum_{k = 1}^m X_k$, where each $X_k$ is an i.i.d. copy of $X$. Then, for all $t \geq 0$,
    \begin{align*}
        P( \norm{X_{\mathcal{D}} - \Bar{X}}_2 \geq t) &\leq (n_1 + n_2) \exp\left(\frac{-mt^2/2}{b + 2Lt/3}\right)
    \end{align*}
\end{lemma}
\newpage
