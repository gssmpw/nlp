In this section, we present the proof of Lemma \ref{lem:convexity_lowerbound}, following the approach presented in Section \ref{sec:proof}. Recall that the goal is to find a lower bound for $\Dataset\left(\Delta\solset^T\right)$ that holds with high probability. Our approach will be to first derive an expression for $\mathbb{E}\left[\Dataset\left(\Delta\solset^T \right)\right]$ and then show that $\Dataset\left(\Delta\solset^T\right)$ is close enough to its expected value. Crucially, we want this result to hold with high probability uniformly for all $Z \in \overline{\Incoherentset}$.

\subsection{Computing Expectations}\label{sec:expectations}
Recall the definition of the sampling matrix $A$ corresponding to a triplet $(u; i, j)$ from \eqref{eq:def_A1} and \eqref{eq:def_A2}.
In this section, we view the triplet $(u; i, j)$ as a random variable where $u$ is chosen uniformly at random from $[n_1]$ and the pair of item indices $(i,j)$ is chosen uniformly at random from the set of $n_2(n_2-1)$ pairs of distinct items, independent from $u$. Consequently, $e_u$ is a random vector in $\Real{n_1}$,  $\Tilde{e}_i - \Tilde{e}_j$ is a random vector in $\Real{n_2}$, and the sampling matrix $A$ is a random matrix in $\Real{n \times n}$. With this interpretation, we can compute:
\begin{align}\label{eq:expectation_of_A}
    \mathbb{E}[e_ue_u^T] = \frac{1}{n_1}I_{n_1}, \quad \mathbb{E}[(\Tilde{e}_i - \Tilde{e}_j)(\Tilde{e}_i - \Tilde{e}_j)^T] = \frac{2}{n_2-1}J, \ \text{where } J = I_{n_2} - \frac{1}{n_2} 11^T
\end{align}
Also recall that $\gamma$ denotes the constant $2/(n_1(n_2 - 1))$. 

Using these identities, we can show the following result.
\begin{lemma}\label{lem:expectation_of_A}
    For any matrix $X \in \Real{n_1 \times n_2}$,
    \begin{align*}
        \mathbb{E}\left[\llangle e_u(\Tilde{e}_i - \Tilde{e}_j)^T, X \rrangle^2 \right] = \gamma  \norm{XJ}_F^2
    \end{align*}
\end{lemma}
\begin{proof}
This proof makes repeated use of the following properties of the trace operator:
\begin{itemize}
    \item the trace is invariant under cyclic shifts, i.e., $\Tr{ABC} = \Tr{CAB} = \Tr{BCA}$
    \item the trace of a scalar is the scalar itself
    \item the trace is a linear operator which commutes with the expectation
\end{itemize}
We also use the fact that the indices $u$ and $(i,j)$ are independent, so the expectation $\mathbb{E}[\cdot]$ can be decomposed into $\mathbb{E}_{i,j}[\mathbb{E}_{u}[\cdot]]$.
\begin{align*}
    \llangle e_u(\Tilde{e}_i - \Tilde{e}_j)^T, X \rrangle &= \Tr{e_u(\Tilde{e}_i - \Tilde{e}_j)^TX^T} 
    = \Tr{(\Tilde{e}_i - \Tilde{e}_j)^TX^Te_u} = (\Tilde{e}_i - \Tilde{e}_j)^TX^Te_u = e_u^TX(\Tilde{e}_i - \Tilde{e}_j) \\
    \Rightarrow \llangle e_u(\Tilde{e}_i - \Tilde{e}_j)^T, X \rrangle^2 &= (\Tilde{e}_i - \Tilde{e}_j)^TX^Te_ue_u^TX(\Tilde{e}_i - \Tilde{e}_j) \\
    \Rightarrow \mathbb{E}[\llangle e_u(\Tilde{e}_i - \Tilde{e}_j)^T, X \rrangle^2] &= \mathbb{E}[(\Tilde{e}_i - \Tilde{e}_j)^TX^Te_ue_u^TX(\Tilde{e}_i - \Tilde{e}_j)] = \mathbb{E}_{i,j}[\mathbb{E}_{u}[(\Tilde{e}_i - \Tilde{e}_j)^TX^Te_ue_u^TX(\Tilde{e}_i - \Tilde{e}_j)]] \\
    &= \mathbb{E}_{i,j}[(\Tilde{e}_i - \Tilde{e}_j)^TX^T\mathbb{E}_{u}[e_ue_u^T]X(\Tilde{e}_i - \Tilde{e}_j)] = \frac{1}{n_1} \mathbb{E}_{i,j}[(\Tilde{e}_i - \Tilde{e}_j)^TX^TX(\Tilde{e}_i - \Tilde{e}_j)] \ (\text{by \eqref{eq:expectation_of_A}})\\
    &= \frac{1}{n_1} \mathbb{E}_{i,j}[\Tr{(\Tilde{e}_i - \Tilde{e}_j)^TX^TX(\Tilde{e}_i - \Tilde{e}_j)}] =  \frac{1}{n_1} \mathbb{E}_{i,j}[\Tr{X^TX(\Tilde{e}_i - \Tilde{e}_j)(\Tilde{e}_i - \Tilde{e}_j)^T}] \\
    &= \frac{1}{n_1}\Tr{X^TX \mathbb{E}_{i,j}[(\Tilde{e}_i - \Tilde{e}_j)(\Tilde{e}_i - \Tilde{e}_j)^T]} = \frac{2}{n_1(n_2 - 1)}\Tr{X^TXJ}  \ (\text{by \eqref{eq:expectation_of_A}})\\
    &= \frac{2}{n_1(n_2 - 1)}\Tr{X^TXJJ^T} = \frac{2}{n_1(n_2 - 1)}\Tr{(JX)^TXJ} \\
    &= \frac{2}{n_1(n_2 - 1)}\norm{XJ}_F^2 = \gamma\norm{XJ}_F^2
\end{align*}
In the last but one step, we make use of the fact that $J$ is a projection matrix, which implies $J = JJ^T$.
\end{proof}

We use Lemma \ref{lem:expectation_of_A} to prove the next result. 
\begin{lemma}\label{lem:expectation_of_D}
    For any $Z \in \mathcal{H}$,
    \begin{align*}
        \mathbb{E}\left[\Dataset\left(\Delta\solset^T\right)\right] = \gamma \norm{\Delta_U\solset_V^T + \solset_U\Delta_V^T}_F^2,
    \end{align*}
    where $\solset = (\solset_U, \solset_V)$ and $\Delta = (\Delta_U, \Delta_V)$ denote the split of $\solset$ and $\Delta$ into the first $n_1$ and last $n_2$ rows.
\end{lemma}
\begin{proof}
\begin{align*}
    \mathbb{E}\left[\Dataset\left(\Delta\solset^T\right)\right] &= \mathbb{E}\left[\frac{1}{m} \sum_{k = 1}^m \llangle A_k + A_k^T, \Delta\solset^T\rrangle^2 \right] \\
    &= \mathbb{E}\left[\llangle A + A^T, \Delta\solset^T \rrangle^2 \right] \\
    &= \mathbb{E}\left[\llangle e_u(\Tilde{e}_i - \Tilde{e}_j)^T, \solset_U \Delta_V^T + \Delta_U \solset_V^T \rrangle^2 \right] \quad (\text{by \eqref{eq:A_AT_YZT_identity}})\\
    &= \gamma \norm{(\solset_U \Delta_V^T + \Delta_U \solset_V^T)J}_F^2 \quad (\text{by Lemma \ref{lem:expectation_of_A}})\\
    &= \gamma \norm{\solset_U \Delta_V^T + \Delta_U \solset_V^T}_F^2
\end{align*}
The last step uses the fact that $\solset_V^TJ = \solset_V^T$ and $\Delta_V^TJ = \Delta_V^T$. These identities can be shown as follows. By our assumption on $Z^*$, we know that $Z^* \in \mathcal{H}$. It follows that the entire equivalence class of solutions $\solset$ lies in $\mathcal{H}$. In particular, $\solset(Z) \in \mathcal{H}$. We are given some $Z \in \mathcal{H}$. This implies $\Delta(Z) \in \mathcal{H}$, because $\Delta(Z) = Z - \solset(Z)$ and $\mathcal{H}$ is a vector space. A characterization of $\mathcal{H}$ is that for any $Z = (U, V)$ in  $\mathcal{H}$, $JV = V$, or equivalently, $V^TJ = V^T$ ($J$ is symmetric). Thus, it follows that $\solset_V^TJ = \solset_V^T$ and $\Delta_V^TJ = \Delta_V^T$.
\end{proof}

We end this section by bounding the expression in Lemma \ref{lem:expectation_of_D} from below.
\begin{lemma}\label{lem:D_lower_bound}
    For any $Z \in \mathcal{H}$,
    \begin{align*}
        \mathbb{E}\left[\Dataset\left(\Delta\solset^T\right)\right] \geq \gamma \left(\sigma^*_r \norm{\Delta}_F^2 + 2 \llangle \solset_U \Delta_V^T, \Delta_U \solset_V^T \rrangle\right)
    \end{align*}
\end{lemma}
\begin{proof}
    By Lemma \ref{lem:expectation_of_D}, 
    \begin{align*}
        \mathbb{E}\left[\Dataset\left(\Delta\solset^T\right)\right] &= \gamma \norm{\Delta_U\solset_V^T + \solset_U\Delta_V^T}_F^2 \\
        &= \gamma \left(\norm{\Delta_U\solset_V^T}_F^2  + \norm{\solset_U\Delta_V^T}_F^2 + 2 \llangle \solset_U \Delta_V^T, \Delta_U \solset_V^T \rrangle\right) \\
        & \geq \gamma \left(\sigma^*_r\norm{\Delta_U}_F^2  + \sigma^*_r\norm{\Delta_V}_F^2 + 2 \llangle \solset_U \Delta_V^T, \Delta_U \solset_V^T \rrangle\right) \ (\text{by Lemma \ref{lem:bounds_on_product_norms}})\\
        &= \gamma \left(\sigma^*_r\norm{\Delta}_F^2 + 2 \llangle \solset_U \Delta_V^T, \Delta_U \solset_V^T \rrangle\right)
    \end{align*}
    Here, we use the fact that $\sigma_r(\solset_U) = \sigma_r(\solset_V) = \sqrt{\sigma^*_r}$. This can be shown as follows. Recall $Z^* = (U^*\Sigma^{*1/2}, V^*\Sigma^{*1/2})$ and $\solset = Z^*R$ for some orthogonal matrix $R$. Therefore, $\solset_U = U^*\Sigma^{*1/2}R$ and $\solset_V = V^*\Sigma^{*1/2}R$. These expressions are already in SVD form. Therefore, the singular values for both $\solset_U$ and $\solset_V$ are the diagonal elements of $\Sigma^{*1/2}$, namely, $\sqrt{\sigma^*_1}, \ldots, \sqrt{\sigma^*_r}$.
\end{proof}

\subsection{Vectorization and a Quadratic Form}\label{sec:quadratic_form}

In this section, we shall show that $\Dataset\left(\Delta\solset^T\right)$ can be expressed as a quadratic form around a random matrix. This identity will help us prove the desired concentration result in the next section.
Let us establish the following notation.
\begin{align}\label{eq:def_SD}
    v \triangleq \vectext{\Delta R^T}, \ a_k \triangleq \vectext{(A_k+A_k^T) Z^*}, \ S_k \triangleq a_ka_k^T, \ S_{\Dataset} \triangleq \frac{1}{m} \sum_{k = 1}^m S_k
\end{align}
where for any matrix $Z \in \Real{n \times r}$, $\vectext{Z}$ is a vector in $\Real{nr}$, obtained by stacking the columns of the matrix one after another. This operation is called `vectorization of a matrix'. With this notation in place, we proceed to establish the following identities:
\begin{lemma}\label{lem:D_to_quadratic_form}
    \begin{align*}
        \Dataset\left(\Delta\solset^T\right) &= v^T S_{\Dataset} v \\
        \norm{v}_2^2 &= \norm{\Delta}_F^2
    \end{align*}
\end{lemma}
\begin{proof}
    Recall from \eqref{eq:def_closest_sol} that $\solset = Z^* R$.
    Let $\Tilde{\Delta} \triangleq \Delta R^T$. Then
    \begin{align}\label{eq:pf_D_to_quadratic_form}
        \Delta\solset^T = \Delta R^T Z^{*T} = \Tilde{\Delta}Z^{*T} \Rightarrow \left(\Delta\solset^T + \solset\Delta^T\right) &= (\Tilde{\Delta}Z^{*T} + Z^*\Tilde{\Delta}^T)
    \end{align}
    Next, invoking the notion of vectorization, we get that for any $k \in m$:
    \begin{align*}
        \llangle A_k, \Tilde{\Delta}Z^{*T} + Z^*\Tilde{\Delta}^T \rrangle 
        &= \llangle (A_k + A_k^T)Z^*, \Tilde{\Delta} \rrangle \quad (\text{by \eqref{eq:A_AT_identity}}) \\
        &= \langle \vectext{(A_k + A_k^T)Z^*}, \vectext{\Tilde{\Delta}} \rangle \\
        &= \langle a_k, v \rangle \quad (\text{by } \eqref{eq:def_D_operator})\\
        \therefore \llangle A_k, \Tilde{\Delta}Z^{*T} + Z^*\Tilde{\Delta}^T \rrangle^2 &= \langle v, a_k \rangle\,\langle a_k, v \rangle \\
        &= v^T S_k v \\
        \therefore \Dataset\left(\Delta\solset^T + \solset\Delta^T\right) 
        &= \Dataset\left(\Tilde{\Delta}Z^{*T} + Z^*\Tilde{\Delta}^T\right) \quad (\text{by } \eqref{eq:pf_D_to_quadratic_form})\\
        &= \frac{1}{m} \sum_{i = 1}^m \llangle A_k, \Tilde{\Delta}Z^{*T} + Z^*\Tilde{\Delta}^T \rrangle^2 \quad (\text{by } \eqref{eq:def_D_operator})\\
        &= \frac{1}{m} \sum_{i = 1}^m v^T S_k v \\
        &= v^T \left(\frac{1}{m} \sum_{i = 1}^m S_k\right)v \\
        &= v^T S_{\Dataset} v 
    \end{align*}
    The second statement can be derived easily as shown below:
    \begin{align*}
        \norm{v}_2^2 &= \norm{\vectext{\Delta R^T}}_2^2 \\
        &= \langle \vectext{\Delta R^T}, \vectext{\Delta R^T}\rangle \\
        &= \llangle \Delta R^T, \Delta R^T \rrangle \\
        &= \llangle \Delta R^TR, \Delta \rrangle  \quad (\text{by \eqref{eq:identity_shift}}) \\
        &= \llangle \Delta, \Delta \rrangle  \quad (\text{because } R \text{ is an orthonormal matrix, } R^TR = I) \\
        &= \norm{\Delta}_F^2
    \end{align*}
\end{proof}

\subsection{A Concentration Result on $S_{\Dataset}$}
Recall, from \eqref{eq:def_SD}, that $S_{\mathcal{D}}$ is the empirical mean of i.i.d. random matrices $(S_k)_{k \in m}$. Let $S$ denote the prototype random matrix of which $(S_k)_{k \in [m]}$ are i.i.d. copies, and let $\Bar{S}$ denote $\mathbb{E}[S]$. In this section, we will use the matrix Bernstein inequality (Lemma \ref{lem:matrix_bernstein}) to establish an upper bound on $\norm{S_{\mathcal{D}} - \Bar{S}}_2$. (Recall from \eqref{eq:def_operator_norm} that $\norm{X}_2$ denotes the operator norm of $X$.)


In order to apply the matrix Bernstein inequality, we need to compute two parameters, $b$ and $L$, that satisfy:
\begin{align*}
    \norm{S}_{2} \leq L \ \text{almost surely,} \quad \norm{\mathbb{E}[SS^T]}_2 \leq b
\end{align*}
Here, $S$ is symmetric, so $\mathbb{E}[SS^T] = \mathbb{E}[S^TS])$.

For any rank-one symmetric matrix $Y = yy^T$, $\norm{Y}_{2} = \norm{y}_2^2$. Here, $S = aa^T$ where $a = \vectext{(A + A^T)Z^*}$ for some sampling matrix $A$. Using this formula, we get
\begin{align*}
    \norm{S}_{2} = \norm{a}_2^2 = \norm{\vectext{(A + A^T)Z^*}}_2^2 = \norm{(A + A^T)Z^*}_F^2 \leq \frac{6\mu}{n} \norm{Z^*}_F^2 \ \text{almost surely} \ \  (\text{by \eqref{eq:score_bound2_Zstar}})
\end{align*}
Thus, $L = 6(\mu/n)\norm{Z^*}_F^2$. Moving on to the calculation for $b$, we get:
\begin{align*}
    \mathbb{E}[SS^T] = \mathbb{E}[aa^Taa^T] = \mathbb{E}[\norm{a}_2^2 aa^T] \Rightarrow \norm{\mathbb{E}[SS^T]}_2 = \norm{\mathbb{E}[\norm{a}_2^2 aa^T]}_2 \leq \sup_a (\norm{a}_2^2) \norm{\mathbb{E}[aa^T]}_2 \leq L \norm{\mathbb{E}[aa^T]}_2
\end{align*}
Where, in the last step, we use the fact that $\norm{a}_2^2 \leq L$ almost surely. The following lemma establishes the bound $\norm{\mathbb{E}[aa^T]}_2 \leq \frac{4\sigma^*_1}{n_1(n_2 - 1)}$. Thus, we can choose $b = 2 \gamma \sigma^*_1 L$.
\begin{lemma}\label{lem:bound_on_operator_norm}
    Let $a \in \Real{nr}$ denote a random vector such that $a = \vectext{(A + A^T)Z^*}$, with $A$ being the random sampling matrix defined in Section \ref{sec:expectations}. Then 
    \begin{align*}
        \norm{\mathbb{E}[aa^T]}_2 \leq 2 \gamma \sigma^*_1 
    \end{align*}
\end{lemma}
\begin{proof}
    We adapt the definition of the operator norm of a matrix as follows:
    \begin{align*}
        \norm{\mathbb{E}[aa^T]}_2 &= \sup_{v \in \Real{nr}: \norm{v}_2 = 1} v^T\mathbb{E}[aa^T]v \\
        &= \sup_{Z \in \Real{n \times r}: \norm{Z}_F = 1} \vectext{Z}^T\mathbb{E}[aa^T]\vectext{Z} \\
        &= \sup_{Z \in \Real{n \times r}: \norm{Z}_F = 1} \mathbb{E}[\vectext{Z}^Taa^T\vectext{Z}] \\
        &= \sup_{Z \in \Real{n \times r}: \norm{Z}_F = 1} \mathbb{E}[\langle \vectext{(A + A^T)Z^*}, \vectext{Z}\rangle^2] \\
        &= \sup_{Z \in \Real{n \times r}: \norm{Z}_F = 1} \mathbb{E}[\llangle {(A + A^T)Z^*}, Z\rrangle^2].        
    \end{align*}
    Following the same reasoning as given in the proof of Lemma \ref{lem:expectation_of_D}, we see that:
    \begin{align*}
        \mathbb{E}[\llangle {(A + A^T)Z^*}, Z\rrangle^2] 
        &= \gamma\norm{Z_U^* Z_V^T J + Z_U Z_V^{*T} J}_F^2 \\
        &\leq \gamma(\norm{Z_U^* Z_V^T J}_F + \norm{Z_U Z_V^{*T} J}_F)^2 \ (\text{by triangle inequality})\\
        &\leq 2\gamma(\norm{Z_U^* Z_V^T J}_F^2 + \norm{Z_U Z_V^{*T} J}_F^2) \ (\text{by } (a + b)^2 \leq 2(a^2 + b^2)\\
        &= 2\gamma (\norm{Z_U^* Z_V^T J}_F^2 + \norm{Z_UZ_V^{*T}}_F^2) \ (\text{because } Z_V^{*T} J = Z_V^{*T}) \\
        &\leq 2\gamma (\sigma^*_1\norm{Z_V^T J}_F^2 + \sigma^*_1\norm{Z_U}_F^2) \ (\text{by Lemma \ref{lem:bounds_on_product_norms}}; \ \sigma_1(Z_U^*) = \sigma_1(Z_V^*) = \sqrt{\sigma_1^*}) \\
        &\leq 2\gamma \sigma^*_1 (\norm{Z_V^T}_F^2 + \norm{Z_U}_F^2) \ (\text{by Lemma \ref{lem:bounds_on_product_norms}}; \sigma_1(J) = 1 ) \\        
        &= 2\gamma \sigma^*_1 \norm{Z}_F^2  
    \end{align*}
    Plugging this bound into the expression above, we get
    \begin{align*}
        \norm{\mathbb{E}[aa^T]}_2 &= \sup_{Z \in \Real{n \times r}: \norm{Z}_F = 1} \mathbb{E}[\llangle {(A + A^T)Z^*}, Z\rangle^2] \\
        &\leq \sup_{Z \in \Real{n \times r}: \norm{Z}_F = 1} 2\gamma \sigma^*_1 \norm{Z}_F^2 \\
        &= 2\gamma \sigma^*_1
    \end{align*}
\end{proof}
We now have all the ingredients to prove the bound on $\norm{S_{\mathcal{D}} - \Bar{S}}_2$.
\begin{lemma}\label{lem:SD_concentration}
    Let $\epsilon \in (0, 1)$ and $\delta \in (0, 1)$ be given. Suppose the number of samples $m$ is at least $96 \mu r n \left(\kappa/\epsilon\right)^2 \log\left(n/\delta\right)$.
    Then, with probability at least $1 - \delta$,
    \begin{align*}
        \norm{S_{\mathcal{D}} - \Bar{S}}_2 \leq \gamma \epsilon\sigma^*_r
    \end{align*}
\end{lemma}
\begin{proof}
    Let the amount of deviation we wish to tolerate be denoted by $t$, \textit{i.e.}, $t = \gamma \epsilon\sigma^*_r$. We have already established the bounds
    \begin{align*}
        \norm{S}_{2} \leq L \ \text{almost surely,} \quad \norm{\mathbb{E}[SS^T]}_2 \leq b ; \quad L = \frac{6\mu}{n} \norm{Z^*}_F^2 , \ b =  2\gamma\sigma^*_1L
    \end{align*}
    Note that $b = (2Lt\kappa/\epsilon)$, since $\kappa = \sigma^*_1/\sigma^*_r$.

    By Lemma \ref{lem:matrix_bernstein}, 
    \begin{align*}
        P( \norm{S_{\mathcal{D}} - \Bar{S}}_2 \geq t) &\leq 2nr \exp\left(\frac{-mt^2/2}{b + 2Lt/3}\right)
    \end{align*}
    We would like the right hand side to be less than $\delta$. I.e.,
    \begin{align*}
        2nr \exp\left(\frac{-mt^2/2}{b + 2Lt/3}\right) &\leq \delta \\
        \Leftrightarrow \frac{mt^2/2}{b + 2Lt/3} &\geq \log\left(\frac{2nr}{\delta}\right) \\
        \Leftrightarrow \frac{mt^2/2}{2Lt (\kappa/\epsilon + 1/3)} &\geq \log\left(\frac{2nr}{\delta}\right) \quad (\because b = 2Lt\kappa/\epsilon)\\
        \Leftrightarrow m &\geq  \frac{4L}{t} \left(\frac{\kappa}{\epsilon} + \frac{1}{3}\right) \log\left(\frac{2nr}{\delta}\right)
    \end{align*}
    Next, note that $n = n_1 + n_2$, which implies $n_1(n_2 - 1) \leq n^2$. Further, the Frobenius norm of a matrix is the $\ell_2$ norm of its singular values. We have noted before that the singular values of $Z^*$ are $\sqrt{2\sigma^*_1}, \ldots \sqrt{2\sigma^*_r}$. Therefore $\norm{Z^*}_F^2 \leq 2r\sigma^*_1$. Using these inequalities, we get
    \begin{align*}
        \frac{4L}{t} = 4\left(\frac{6\mu}{n}\norm{Z^*}_F^2\right) \left(\frac{1}{\gamma\epsilon\sigma^*_r}\right) = 4\left(\frac{6\mu}{n}\norm{Z^*}_F^2\right) \left(\frac{n_1(n_2 - 1)}{2\epsilon\sigma^*_r}\right) = 12\frac{\mu}{\epsilon} \left(\frac{n_1(n_2 - 1)}{n}\right)\left(\frac{\norm{Z^*}_F^2}{\sigma^*_r}\right) \leq 24\left(\frac{\mu r \kappa n}{\epsilon}\right)
    \end{align*}
    Also note that $\kappa > 1$ and $\epsilon < 1$, so $\kappa/\epsilon + 1/3$ is bounded above by $2\kappa/\epsilon$. Finally, note that $r \leq n_1$ and $r \leq n_2$, so $2r \leq n_1 + n_2 = n$. Therefore, $2nr/\delta \leq n^2/\delta \leq (n/\delta)^2$.
    Putting these inequalities together, we get: 
    \begin{align*}
        96 \mu r n \left(\frac{\kappa}{\epsilon}\right)^2 \log\left(\frac{n}{\delta}\right) \geq \frac{4L}{t} \left(\frac{\kappa}{\epsilon} + \frac{1}{3}\right) \log\left(\frac{2nr}{\delta}\right)
    \end{align*}
    Thus, the desired concentration result holds with probability at least $1-\delta$ if the number of samples $m$ exceeds $96 \mu r n \left(\kappa/\epsilon\right)^2 \log\left(n/\delta\right)$.
\end{proof}


\subsection{Completing the Proof of Lemma \ref{lem:convexity_lowerbound}}
\lSCLB*
\begin{proof}
In Lemma \ref{lem:D_to_quadratic_form}, we established that $\Dataset\left(\Delta\solset^T \right) = v^T S_{\Dataset} v$. Consequently, $\mathbb{E}[\Dataset\left(\Delta\solset^T \right)] = v^T \Bar{S} v$. Therefore,
\begin{align*}
    \vert \Dataset\left(\Delta\solset^T \right) - \mathbb{E}\left[\Dataset\left(\Delta\solset^T \right)\right] \vert &= \vert v^T S_{\Dataset} v - v^T \Bar{S} v \vert \quad (\text{by Lemma \ref{lem:D_to_quadratic_form}}) \\
    &= \vert v^T (S_{\Dataset} - \Bar{S}) v \vert \\
    & \leq \norm{S_{\Dataset} - \Bar{S}}_{2} \norm{v}_2^2 \quad (\text{by \eqref{eq:def_operator_norm2}}) \\
    &= \norm{S_{\Dataset} - \Bar{S}}_{2} \norm{\Delta}_F^2 \quad (\text{by Lemma \ref{lem:D_to_quadratic_form}}) \\
    \Rightarrow \Dataset\left(\Delta\solset^T\right) &\geq \mathbb{E}\left[\Dataset\left(\Delta\solset^T\right)\right] - \norm{S_{\Dataset} - \Bar{S}}_{2} \norm{\Delta}_F^2 \\
    &\geq \gamma \left(\sigma^*_r\norm{\Delta}_F^2 + 2 \llangle \solset_U \Delta_V^T, \Delta_U \solset_V^T \rrangle \right) - \norm{S_{\Dataset} - \Bar{S}}_{2} \norm{\Delta}_F^2 \quad (\text{by Lemma \ref{lem:D_lower_bound}}) \\
    &\geq \gamma \left((1- \epsilon)\sigma^*_r\norm{\Delta}_F^2 + 2 \llangle \solset_U \Delta_V^T, \Delta_U \solset_V^T \rrangle \right) \quad (\text{by Lemma \ref{lem:SD_concentration}}) 
\end{align*}
\end{proof}









