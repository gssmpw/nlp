\subsection{The Dual Sampling Matrix}\label{sec:dual_sampling_matrix}
Associated with each triplet $(u; i, j)$, we define the \textit{dual sampling matrix} as follows:
\begin{align}\label{eq:def_B}
    B \in \mathbb{R}^{n_1 \times n_2} : B = e_u(\Tilde{e_i} + \Tilde{e}_j)^T
\end{align}
If we endow the triplets with randomness, $B$ is a random matrix, whose mean is:
\begin{align}\label{eq:dual_matrix_mean}
    \Bar{B} \triangleq \mathbb{E}[{B}] &= \mathbb{E}[e_u(\Tilde{e}_i + \Tilde{e}_j)^T] = \mathbb{E}[e_u]\mathbb{E}[(\Tilde{e}_i + \Tilde{e}_j)^T] = \frac{2}{n_1n_2}11^T
\end{align}
Here, $11^T$ is a matrix of all ones of shape $n_1 \times n_2$.

Let $B_1, \ldots,  B_{\Dataset}$ denote the dual sampling matrices for each of the datapoints, similar to the notation for $A$. Define the empirical mean of the dual sampling matrices, $B_{\Dataset}$, as follows:
\begin{align}
    B_{\Dataset} = \frac{1}{m}\sum_{k = 1}^mB_k
\end{align}
In our analysis, we will use the fact that this empirical mean $B_{\Dataset}$ is close to the statistical mean $\Bar{B}$, in a manner made precise by Lemma \ref{lem:BD_concentration}. In preparation for this concentration result, we two parameters, $L$ and $b$. (The same notation was used to denote related terms for the random matrix $S_{\Dataset}$ in the previous section; however, the correct interpretation should be clear from context.) $L$ is a uniform bound on $\norm{B}_2$. For each triplet $(u; i, j)$, the operator norm of the corresponding dual sampling matrix is $\sqrt{2}$. It follows that $L = \sqrt{2}$. The definition and bound for $v$ is given in the lemma below.
\begin{lemma}\label{lem:B_bounds}
    Let $B$ be the random dual sampling matrix as defined above. Let $b^1 \triangleq \norm{\mathbb{E}[B^TB]}_2$,  $b^2 \triangleq \norm{\mathbb{E}[BB^T]}_2$, and $b = \max\{b^1, b^2\}$. Then $$b \leq \frac{4}{\min\{n_1, n_2\}}.$$
\end{lemma}
\begin{proof}
We know that $\norm{e_u}_2^2 = 1$ and $\norm{\Tilde{e}_i + \Tilde{e}_j}_2^2 = 2$ almost surely. Further,
\begin{align*}
    \mathbb{E}[e_ue_u^T] = \frac{1}{n_1}I_{n_1} , \qquad 
    \mathbb{E}[(\Tilde{e}_i + \Tilde{e}_j)(\Tilde{e}_i + \Tilde{e}_j)^T] = \frac{1}{\binom{n_2}{2}}(11^T + (n_2-2)I_{n_2})
\end{align*}
Using these identities, we get
\begin{align*}
    \mathbb{E}[B^TB] 
    &= \mathbb{E}[(\Tilde{e}_i + \Tilde{e}_j)e_u^Te_u(\Tilde{e}_i + \Tilde{e}_j)^T] \\
    &= \mathbb{E}_{i,j}[(\Tilde{e}_i + \Tilde{e}_j)\mathbb{E}_u[e_u^Te_u]w^T] \\
    &= \mathbb{E}_{i,j}[(\Tilde{e}_i + \Tilde{e}_j)(\Tilde{e}_i + \Tilde{e}_j)^T] \\
    &= \frac{1}{\binom{n_2}{2}}(11^T + (n_2-2)I_{n_2}) \\
    \mathbb{E}[BB^T] 
    &= \mathbb{E}[e_u(\Tilde{e}_i + \Tilde{e}_j)^T(\Tilde{e}_i + \Tilde{e}_j)e_u^T] \\
    &= \mathbb{E}_u[e_u\mathbb{E}_{i,j}[(\Tilde{e}_i + \Tilde{e}_j)^T(\Tilde{e}_i + \Tilde{e}_j)]e_u^T] \\
    &= 2\mathbb{E}_u[e_ue_u^T] \\
    &= \frac{2}{n_1}I_{n_1}
\end{align*}
Computing the operator norms of these matrices is straightforward:
\begin{align*}
    b^1 &= \norm{\mathbb{E}[B^TB]}_2 = \frac{1}{\binom{n_2}{2}} \norm{11^T + (n_2-2)I_{n_2}}_2 \leq \frac{1}{\binom{n_2}{2}} \left(\norm{11^T}_2 + (n_2-2)\norm{I_{n_2}}_2 \right)= \frac{1}{\binom{n_2}{2}} \left(n_2 + (n_2-2)\right)= \frac{4}{n_2} \\
    b^2 &= \norm{\mathbb{E}[BB^T]}_2 = \frac{2}{n_1} \norm{I_{n_1}}_2 = \frac{2}{n_1} \leq  \frac{4}{n_1}
\end{align*}
\begin{align*}
    \therefore b = \max\{b^1, b^2\} \leq \frac{4}{\min\{n_1, n_2\}}
\end{align*}
\end{proof}

\subsection{Algebraic Upper Bounds on $\Dataset(WZ^T)$}
This subsection contains three lemmas that we shall use in the proof of Lemmas \ref{lem:convexity_upperbound} and \ref{lem:smoothness_upperbound}. The first of these three lemmas, Lemma \ref{lem:D_to_dual_matrix}, gives an upper bound on $\Dataset(WZ^T)$ as a quadratic form around the random matrix $B_{\Dataset}$ that we defined earlier in the section.

Before we state the result, we introduce some additional notation. Corresponding to any matrix $Z \in \Real{n \times r}$, define the vector $z \in \Real{n}$ as follows:
\begin{align}\label{eq:def_z}
    z_j = \norm{Z_j}_2 \ \forall \ j \in [n]
\end{align}
It follows from the definition that
\begin{align}\label{eq:z_Z_identities}
    \norm{z}_{1} = \norm{Z}_{F}^2, \ \norm{z}_{\infty} = \norm{Z}_{2, \infty}^2 
\end{align}
Following the convention of splitting the matrix $Z$ into user and item components $Z = (Z_U, Z_V)$, we split the vector $z$ into vectors $z_U \in \Real{n_1}$ and $z_V \in \Real{n_2}$ ($z = (z_U, z_V)$). The norms of these vectors satisfy the following relations:
\begin{align}\label{eq:z_vector_relations}
    \norm{z}_{1} = \norm{z_U}_{1} + \norm{z_V}_{1}, \ \norm{z}_{2}^2 = \norm{z_U}_{2}^2 + \norm{z_V}_{2}^2, \  \norm{z}_{\infty} = \max \{ \norm{z_U}_{\infty}, \norm{z_V}_{\infty}\}
\end{align}

With these notations and identities in place, we proceed to establish the following result.
\begin{lemma}\label{lem:D_to_dual_matrix}
    For any two matrices $W$ and $Z$ in $\Real{n \times r}$,
    \begin{align*}
        \mathcal{D}(WZ^T) \leq &4(w_U^T B_{\Dataset} z_V + z_U^T B_{\Dataset} w_V) 
    \end{align*}
\end{lemma}
\begin{proof}
\begin{align*}
    \mathcal{D}(WZ^T) 
    &= \frac{1}{m} \sum_{k = 1}^m \llangle A_k + A_k^T, WZ^T \rrangle^2 \quad (\text{by } \eqref{eq:def_D_operator})\\
    &= \frac{1}{m} \sum_{(u; i, j) \in \Dataset} (\llangle W_u, Z_i - Z_j \rrangle + \llangle Z_u, W_i - W_j \rrangle)^2 \quad (\text{by \eqref{eq:A_AT_YZT_identity2}})\\
    &\leq \frac{2}{m} \sum_{(u; i, j) \in \Dataset}  \llangle W_u, Z_i - Z_j \rrangle^2 + \llangle Z_u, W_i - W_j \rrangle^2 \quad (\text{by } (a+b)^2 \leq 2(a^2 + b^2))\\
    &\leq \frac{2}{m} \sum_{(u; i, j) \in \Dataset}  \norm{W_u}_2^2\norm{Z_i - Z_j}_2^2 + \norm{Z_u}_2^2\norm{W_i - W_j}_2^2  \quad (\text{by Cauchy-Schwarz inequality})\\
    &\leq \frac{2}{m} \sum_{(u; i, j) \in \Dataset}  \norm{W_u}_2^2(\norm{Z_i}_2 + \norm{Z_j}_2)^2 + \norm{Z_u}_2^2(\norm{W_i}_2 + \norm{W_j}_2)^2  \quad (\text{by triangle inequality})\\
    &\leq \frac{4}{m} \sum_{(u; i, j) \in \Dataset}  \norm{W_u}_2^2(\norm{Z_i}_2^2 + \norm{Z_j}_2^2) + \norm{Z_u}_2^2(\norm{W_i}_2^2 + \norm{W_j}_2^2)  \quad (\text{by } (a+b)^2 \leq 2(a^2 + b^2))\\
    &= \frac{4}{m} \sum_{(u; i, j) \in \Dataset} w_u(z_i + z_j) + \frac{4}{m} \sum_{(u; i, j) \in \Dataset} z_u(w_i + w_j) \\
    &= \frac{4}{m} \sum_{(u; i, j) \in \Dataset} w_U^T\left(e_u(\Tilde{e_i} + \Tilde{e}_j)^T\right)z_V + \frac{4}{m} \sum_{(u; i, j) \in \Dataset} z_U^T\left(e_u(\Tilde{e_i} + \Tilde{e}_j)^T\right)w_V  \\
    &= 4 w_U^T\left(\frac{1}{m} \sum_{(u; i, j) \in \Dataset}e_u(\Tilde{e_i} + \Tilde{e}_j)^T\right)z_V + 4 z_U^T\left(\frac{1}{m} \sum_{(u; i, j) \in \Dataset} e_u(\Tilde{e_i} + \Tilde{e}_j)^T\right)w_V  \\
    &= 4w_U^T B_{\Dataset} z_V + 4z_U^T B_{\Dataset} w_V
\end{align*}
\end{proof}

The next lemma builds upon the previous result to obtain an upper bound in terms of $\norm{B_{\Dataset} - \Bar{B}}_{2}$.
\begin{lemma}\label{lem:deltadelta_intermediate}
    For any $Z \in \Real{n \times r}$,
    \begin{align*}
        \Dataset(ZZ^T) &\leq 2\left( \gamma \norm{Z}_F^2 + 2\norm{B_{\Dataset} - \Bar{B}}_{2} \norm{Z}_{2,\infty}^2 \right)\norm{Z}_F^2
    \end{align*}
\end{lemma}
\begin{proof}
We start by using the relations in \eqref{eq:z_vector_relations} along with the arithmetic mean-geometric mean (AM-GM) inequality to obtain the following bound
\begin{align}\label{eq:am_gm_inequality}
    \norm{z_U}_1\norm{z_V}_1 \leq \left(\frac{\norm{z_U}_1 + \norm{z_V}_1}{2}\right)^2 = \frac{\norm{z}_1^2}{4}, \quad \norm{z_U}_2\norm{z_V}_2 \leq \left(\frac{\norm{z_U}_2 + \norm{z_V}_2}{2}\right)^2 \leq \frac{\norm{z}_2^2}{2} 
\end{align}
Using the bound in \eqref{eq:am_gm_inequality}, we can show the desired result as follows.
\begin{align*}
    \frac{\Dataset(ZZ^T)}{8}
    &\leq z_U^T B_{\Dataset}z_V \quad (\text{by Lemma \ref{lem:D_to_dual_matrix}})\\
    &=  z_U^T\Bar{B}z_V + z_U^T(B_{\Dataset} - \Bar{B})z_V \\
    &\leq z_U^T\Bar{B}z_V + \norm{z_U}_2 \norm{(B_{\Dataset} - \Bar{B})z_V}_2 \quad (\text{by the Cauchy-Schwarz inequality}) \\
    &\leq  z_U^T\Bar{B}z_V + \norm{B_{\Dataset} - \Bar{B}}_{2}\norm{z_U}_2\norm{z_V}_2  \quad (\text{by definition of the operator norm})\\
    &=  \frac{2}{n_1n_2}z_U^T11^Tz_V + \norm{B_{\Dataset} - \Bar{B}}_{2}\norm{z_U}_2\norm{z_V}_2\quad (\text{by } \eqref{eq:dual_matrix_mean})\\
    &\leq  \gamma z_U^T11^Tz_V + \norm{B_{\Dataset} - \Bar{B}}_{2}\norm{z_U}_2\norm{z_V}_2 \quad (2/(n_1n_2) \leq  2/(n_1(n_2 - 1)) = \gamma)\\
    &\leq \gamma\norm{z_U}_1\norm{z_V}_1 + \norm{B_{\Dataset} - \Bar{B}}_{2}\norm{z_U}_2\norm{z_V}_2 \quad (1^Tz \leq \norm{z}_1)\\
    &\leq \frac{1}{4}\left(\gamma\norm{z}_1^2 + 2\norm{B_{\Dataset} - \Bar{B}}_{2}\norm{z}_2^2\right) \quad (\text{by \eqref{eq:am_gm_inequality}})\\
    &\leq \frac{1}{4}\left(\gamma\norm{z}_1^2 + 2\norm{B_{\Dataset} - \Bar{B}}_{2}\norm{z}_\infty \norm{z}_1\right) \quad (\text{by Hölder's inequality})\\
    &= \frac{1}{4} \left(\gamma\norm{Z}_F^2 + 2\norm{B_{\Dataset} - \Bar{B}}_{2}\norm{Z}_{2,\infty}^2\right)\norm{Z}_F^2 \quad (\text{by } \eqref{eq:z_Z_identities}) \\
    \therefore \Dataset(ZZ^T) &\leq 2\left(\gamma\norm{Z}_F^2 + 2\norm{B_{\Dataset} - \Bar{B}}_{2}\norm{Z}_{2,\infty}^2\right)\norm{Z}_F^2
\end{align*}
\end{proof}

The third and final result of this section builds on Lemma \ref{lem:D_to_dual_matrix} in a different way as compared to the previous one. 
Here, we obtain a bound in terms of the $\ell_1$ operator norm of $B_{\Dataset}$. For any matrix $X \in \Real{n_1 \times n_2}$, 
\begin{align}\label{eq:def_operator_onenorm}
    \norm{X}_1 \triangleq \sup_{v: \norm{v}_1 = 1} \norm{Xv}_1
\end{align}
It follows that for any $v \in \Real{n_2}$,
\begin{align}\label{eq:operator_onenorm_consequence}
    \norm{Xv}_1 \leq \norm{X}_1 \norm{v}_1
\end{align}
It can be easily shown that
\begin{align}\label{eq:operator_onenorm_calculation}
    \norm{X}_1 =\max_{j \in [n_2]} \sum_{i \in [n_1]}|x_{ij}|
\end{align}
In addition, we will need Hölder's inequality, which states that for any vectors $a, b$, 
\begin{align}\label{eq:holder_inequality}
    \langle a, b \rangle \leq  \norm{a}_{\infty} \norm{b}_1  \ \Rightarrow \norm{a}_2^2 \leq \norm{a}_{\infty} \norm{a}_1 
\end{align}

Using these inequalities, we get the next result.
\begin{lemma}\label{lem:quadratic_form_to_one_norm}
    For any matrices $W, Z \in \mathbb{R}^{n \times r}$,
    \begin{align*}
        \mathcal{D}(WZ^T) \leq 4(\max\{\norm{B_\Dataset}_1, \norm{B_\Dataset^T}_1\}) \norm{Z}_{2, \infty}^2\norm{W}_F^2,
    \end{align*}
\end{lemma}
\begin{proof}
We start by invoking Lemma \ref{lem:D_to_dual_matrix}, we get:
\begin{align*}
    \mathcal{D}(WZ^T) &\leq 4w_U^T B_{\Dataset} z_V + 4z_U^T B_{\Dataset} w_V \\
    &= 4z_V^T B_{\Dataset}^T w_U + 4z_U^T B_{\Dataset} w_V
\end{align*}
Applying \eqref{eq:z_vector_relations}, \eqref{eq:operator_onenorm_consequence} and \eqref{eq:holder_inequality}, we get:
\begin{align*}
    z_V^T B_{\Dataset}^T w_U &= \langle z_V, B_{\Dataset}^T w_U \rangle \leq \norm{z_V}_{\infty}\norm{B_{\Dataset}^T w_U }_{1} \leq \norm{z_V}_{\infty} \norm{B_{\Dataset}^T}_1 \norm{w_U}_{1} \leq \norm{z}_{\infty} \norm{B_{\Dataset}^T}_1 \norm{w_U}_{1}\\
    z_U^T B_{\Dataset} w_V &= \langle z_U, B_{\Dataset} w_V \rangle \leq \norm{z_U}_{\infty}\norm{B_{\Dataset}^T w_V}_{1} \leq \norm{z_U}_{\infty} \norm{B_{\Dataset}}_1 \norm{w_V}_{1} \leq \norm{z}_{\infty} \norm{B_{\Dataset}}_1 \norm{w_V}_{1}
\end{align*}
Putting the above inequalities together, we get the desired result:
\begin{align*}
    \mathcal{D}(WZ^T) 
    &\leq 4z_V^T B_{\Dataset}^T w_U + 4z_U^T B_{\Dataset} w_V \\
    &\leq 4\norm{z}_{\infty} \norm{B_{\Dataset}^T}_1 \norm{w_U}_{1} + 4\norm{z}_{\infty} \norm{B_{\Dataset}}_1 \norm{w_V}_{1} \\
    &\leq 4\norm{z}_{\infty} (\max\{\norm{B_\Dataset}_1, \norm{B_\Dataset^T}_1\}) (\norm{w_U}_{1} + \norm{w_V}_{1}) \\
    &= 4\norm{z}_{\infty} (\max\{\norm{B_\Dataset}_1, \norm{B_\Dataset^T}_1\}) \norm{w}_{1} \quad (\text{by} \eqref{eq:z_vector_relations}) \\
    &= 4\norm{Z}_{2, \infty}^2 (\max\{\norm{B_\Dataset}_1, \norm{B_\Dataset^T}_1\}) \norm{W}_F^2 \quad (\text{by} \eqref{eq:z_Z_identities})   
\end{align*}
\end{proof}

\subsection{Norm Bounds on the Dual Sampling Matrix}
First, we provide an upper bound on $\norm{B_{\mathcal{D}} - \Bar{B}}_2$. This result will be used in conjunction with Lemma \ref{lem:deltadelta_intermediate} to prove Lemma \ref{lem:convexity_upperbound}.
\begin{lemma}\label{lem:BD_concentration}
    Let $\epsilon \in (0, 1)$ and $\delta \in (0, 1)$ be given. Suppose the number of samples $m$ is at least $(5/\epsilon^2)n\log(n/\delta)$.
    Then, with probability at least $1 - \delta$,
    \begin{align*}
        \norm{B_{\mathcal{D}} - \Bar{B}}_2 \leq \frac{\epsilon}{\min\{n_1, n_2\}}
    \end{align*}
\end{lemma}
\begin{proof}
    The matrix Bernstein inequality (Lemma \ref{lem:matrix_bernstein}) states that 
    \begin{align*}
        P(\norm{\bar{B}_m - \bar{B}}_2 \geq t) \leq n\exp\left(-\frac{mt^2/2}{v + 2Lt/3}\right),
    \end{align*}
    where $v = \max\{\norm{\mathbb{E}[BB^T]}_2, \norm{\mathbb{E}[B^TB]}_2\}$ and $L = \sup_{B} \norm{B}_2$. We have already established that $L = \sqrt{2}$ and $v = 4/(\min\{n_1, n_2\})$ (see Lemma \ref{lem:B_bounds}).  We would like $\norm{\bar{B}_m - \bar{B}}_2$ to be bounded above by $t = \epsilon/(\min\{n_1, n_2\})$ (for some $\epsilon \in (0, 1)$) with probability at least $1-\delta$. Therefore, the number of samples $m$ must satisfy:
    \begin{align*}
        n\exp\left(-\frac{mt^2/2}{v + 2Lt/3}\right) &\leq \delta \\
        \Leftrightarrow \frac{mt^2/2}{v + 2Lt/3} &\geq \log\left(\frac{n}{\delta}\right) 
    \end{align*}
    Plugging in the value $L = \sqrt{2}$ and noting that $v = 4t/\epsilon$, we get
    \begin{align*}
        \frac{mt^2/2}{(4t/\epsilon) + 2\sqrt{2}t/3} &\geq \log\left(\frac{n}{\delta}\right) \\
        \Leftrightarrow \frac{m}{4/\epsilon + 2\sqrt{2}/3} &\geq \frac{2}{t}\log\left(\frac{n}{\delta}\right) \\
        \Leftrightarrow m &\geq \left(\frac{4}{\epsilon} + \frac{2\sqrt{2}}{3} \right)\frac{2\min\{n_1, n_2\}}{\epsilon}\log\left(\frac{n}{\delta}\right)
    \end{align*}
    Finally, note that $4 + 2\sqrt{2}\epsilon/3 \leq 5$ ($\because \epsilon < 1$) and $2\min\{n_1, n_2\} \leq n_1 + n_2 = n$. Therefore, $m \geq (5/\epsilon^2) n\log\left(n/\delta\right)$ is a sufficient condition for the concentration result to hold.
\end{proof}

Next, we move on to proving a high probability bound on $\max\{\norm{B_\Dataset}_1, \norm{B_\Dataset^T}_1\}$. This result will be used in conjunction with Lemma \ref{lem:quadratic_form_to_one_norm} to prove Lemma \ref{lem:smoothness_upperbound}.

For this result, we need to introduce some new notation and some basic inequalities. Define the random matrix $C \in \Real{d_1 \times d_2}$ as follows:
\begin{align}\label{eq:def_C}
    C = \frac{1}{m} \sum_{k = 1}^m e_{i_k}\Tilde{e}_{j_k}^T
\end{align}
where $(i_k)_{k \in [m]}$ are sampled i.i.d. uniformly at random from $[n_1]$ and $(j_k)_{k \in [m]}$ are sampled i.i.d. uniformly at random from $[n_2]$, independent of $(i_k)_{k \in [m]}$. Let $C_i \in \Real{n_2}$ denote the $i\textsuperscript{th}$ row of $C$, but expressed as a column vector. Then
\begin{align}
    C_i = \frac{1}{m} \sum_{k = 1}^m \mathbf{1}_{i_k = i}\Tilde{e}_{j_k}
\end{align}
It follows that 
\begin{align}
    \norm{C_i}_1 = \frac{1}{m} \sum_{k = 1}^m \mathbf{1}_{i_k = i}
\end{align}
Note that $\norm{C_i}_1$ is the empirical mean of $m$ i.i.d. Bernoulli random variables of mean $1/n_1$. Thus, we can bound it from above by the Chernoff bound.
\begin{lemma}[Chernoff bound]\label{lem:chernoff_bound}
    Suppose $x_1, x_2, \ldots, x_m$ are i.i.d. Bernoulli random variables with parameter $p$ and let $\epsilon > 0$ be given. Then:
    \begin{align*}
        P\left(\frac{1}{m}\sum_{k = 1}^m x_k \geq p + \epsilon \right) \leq \exp\left(-\frac{m\epsilon^2}{2p(1-p)}\right)
    \end{align*}
\end{lemma}
Using Lemma \ref{lem:chernoff_bound} with $p = \epsilon = 1/n_1$, we get that for any $i \in [n_1]$, 
\begin{align*}
    P\left(\norm{C_i}_1 \geq \frac{2}{n_1}\right) \leq \exp\left(-\frac{m}{2n_1}\right)
\end{align*}
Using the union bound, it follows that 
\begin{align*}
    P\left(\max_{i \in [n_1]}\norm{C_i}_1 \geq \frac{2}{n_1}\right) \leq n_1\exp\left(-\frac{m}{2n_1}\right)
\end{align*}
Finally, by \eqref{eq:operator_onenorm_calculation}, we know that 
\begin{align*}
    \norm{C^T}_1 = \max_{i \in [n_1]}\norm{C_i}_1
\end{align*}
In conclusion, 
\begin{align}\label{eq:CT_bound}
    P\left(\norm{C^T}_1 \geq \frac{2}{n_1}\right) \leq n_1\exp\left(-\frac{m}{2n_1}\right)
\end{align}
Since $n_1$ and $n_2$ are arbitrary in the above analysis, one can use the same logic to show that
\begin{align}\label{eq:C_bound}
    P\left(\norm{C}_1 \geq \frac{2}{n_2}\right) \leq n_2\exp\left(-\frac{m}{2n_2}\right)
\end{align}

\begin{lemma}\label{lem:bound_on_one_norm}
    Suppose the number of samples $m$ is at least {$2n\log(4n/\delta)$}. Then, with probability at least $1-\delta$,
    \begin{align*}
        \max\{\norm{B_\Dataset}_1, \norm{B_\Dataset^T}_1\} \leq \frac{4}{\min\{n_1, n_2\}}
    \end{align*}
\end{lemma}
\begin{proof}
    Define the following two matrices
    \begin{align*}
        B_{\Dataset}^1 = \frac{1}{m} \sum_{(u; i, j) \in \mathcal{D}} e_{u}\Tilde{e}_{i}^T \, ; \quad B_{\Dataset}^2 = \frac{1}{m} \sum_{(u; i, j) \in \mathcal{D}} e_{u}\Tilde{e}_{j}^T
     \end{align*}
     Both $B_{\Dataset}^1$ and $B_{\Dataset}^1$ are statistically identical to the random matrix $C$ defined in \eqref{eq:def_C}. By \eqref{eq:C_bound}, we have that if $m \geq 2n_2\log(4n_2/\delta)$,
     \begin{align}\label{eq:B_bound_simple}
         P\left(\norm{B_{\Dataset}^1}_1 \geq \frac{2}{n_2}\right) &\leq \frac{\delta}{4} ,  \qquad P\left(\norm{B_{\Dataset}^2}_1 \geq \frac{2}{n_2}\right) \leq \frac{\delta}{4} \\
     \end{align}
     By construction, 
     $B_{\Dataset} = B_{\Dataset}^1 + B_{\Dataset}^2$. By the triangle inequality, we get $\norm{B_{\Dataset}}_1 \leq \norm{B_{\Dataset}^1}_1 + \norm{B_{\Dataset}^2}_1$. Therefore, 
     \begin{align}\label{eq:B_bound_union}
         \norm{B_{\Dataset}}_1 \geq \frac{4}{n_2} \Rightarrow \ \norm{B_{\Dataset}^1}_1 + \norm{B_{\Dataset}^2}_1 \geq \frac{4}{n_2} \ \Rightarrow \ \norm{B_{\Dataset}^1}_1 \geq \frac{2}{n_2} \text{ or } \norm{B_{\Dataset}^2}_1 \geq \frac{2}{n_2}.
     \end{align}
     Put together, we get that if $m \geq 2n_2\log(4n_2/\delta)$,
     \begin{align*}
         P\left(\norm{B_{\Dataset}}_1 \geq \frac{4}{n_2}\right) &\leq P\left(\norm{B_{\Dataset}^1}_1 \geq \frac{2}{n_2} \text{ or } \norm{B_{\Dataset}^2}_1 \geq \frac{2}{n_2}\right) \quad (\text{by \eqref{eq:B_bound_union}})\\
         &\leq \mathbb{P}\left(\norm{B_{\Dataset}^1}_1 \geq \frac{2}{n_2}\right) +  \mathbb{P}\left(\norm{B_{\Dataset}^2}_1 \geq \frac{2}{n_2}\right) \\
         &\leq \frac{\delta}{2}.  \quad (\text{by \eqref{eq:B_bound_simple}})
     \end{align*}     
     By a similar argument, we can show that if $m \geq 2n_1\log(4n_1/\delta)$,
     \begin{align*}
         P\left(\norm{B_{\Dataset}^T}_1 \geq \frac{4}{n_1}\right) \leq \frac{\delta}{2}
     \end{align*}
     Finally, note that 
     \begin{align*}
         \norm{B_{\Dataset}}_1 \leq \frac{4}{n_2} \text{ and } \norm{B_{\Dataset}^T}_1 \leq \frac{4}{n_1} &\Rightarrow 
        \max\{\norm{B_\Dataset}_1, \norm{B_\Dataset^T}_1\} \leq \frac{4}{\min\{n_1, n_2\}} \\
        \therefore \norm{B_\Dataset^T}_1 \geq \frac{4}{\min\{n_1, n_2\}} &\Rightarrow \norm{B_{\Dataset}}_1 \geq \frac{4}{n_2} \text{ or } \norm{B_{\Dataset}^T}_1 \geq \frac{4}{n_1} 
     \end{align*}
     Invoking the union bound once again, we get that if $m \geq 2n\log(4n/\delta)$,
     \begin{align*}
        P\left(\norm{B_\Dataset^T}_1 \geq \frac{4}{\min\{n_1, n_2\}}\right) &\leq P\left(\norm{B_{\Dataset}}_1 \geq \frac{4}{n_2} \text{ or } \norm{B_{\Dataset}^T}_1 \geq \frac{4}{n_1} \right) \\
        &\leq P\left(\norm{B_{\Dataset}}_1 \geq \frac{4}{n_2}\right) + P\left(\norm{B_{\Dataset}^T}_1 \geq \frac{4}{n_1} \right) \\
        &\leq \delta
    \end{align*}
\end{proof}





\subsection{Proof of Lemma \ref{lem:convexity_upperbound}}


\lSCUB*
\begin{proof}
The proof follows from the following facts:
\begin{itemize}
    \item $\Dataset(\Delta\Delta^T) \leq 2\left(\gamma\norm{\Delta}_F^2 + 2\norm{B_{\Dataset} - \Bar{B}}_{2}\norm{\Delta}_{2,\infty}^2\right)\norm{\Delta}_F^2$, by Lemma \ref{lem:deltadelta_intermediate}.
    \item $\norm{\Delta}_F^2 \leq \epsilon \sigma^{*}_r \ \forall \ Z \in \mathcal{B}(\epsilon)$.
    \item $\norm{\Delta}_{2,\infty}^2 \leq 52\mu r \sigma_1^{*}/n \ \forall \ Z \in \overline{\Incoherentset}$. This can be derived as follows. 
    $$\norm{\Delta}_{2,\infty}^2 = \norm{Z - \solset }_{2,\infty}^2 \leq 2\left(\norm{Z}_{2,\infty}^2 + \norm{\solset }_{2,\infty}^2\right) \leq 2\left(\frac{12\mu\norm{Z^*}_F^2}{n} + \frac{\mu\norm{Z^*}_F^2}{n}\right) \leq \frac{52\mu r \sigma_1^*}{n},$$
    where the last step follows from the fact that $\norm{Z^*}_F^2 \leq 2r\sigma^*_1$.
    \item The number of samples is at least $5\left(13 \mu r \kappa/\epsilon\right)^2 n\log\left(n/\delta\right)$ $(845 = 5 \cdot 13^2)$.  By Lemma \ref{lem:BD_concentration}, with probability at least $1-\delta$,
    $$\norm{B_{\Dataset} - \Bar{B}}_{2} \leq \frac{\epsilon}{13\mu r \kappa}\frac{1}{\min\{n_1, n_2\}}$$  
\end{itemize}
Combining these inequalities, we get that with probability at least $1-\delta$, $\forall \ Z \in \mathcal{B} \cap \overline{\Incoherentset}$,
\begin{align*}
    \Dataset(\Delta\Delta^T) &\leq 2\left(\gamma\norm{\Delta}_F^2 + 2\norm{B_{\Dataset} - \Bar{B}}_{2}\norm{\Delta}_{2,\infty}^2\right)\norm{\Delta}_F^2 \\
    &\leq 2\left(\epsilon\gamma\sigma^*_r + 2\frac{\epsilon}{13\mu r \kappa}\frac{1}{ \min\{n_1, n_2\}} \frac{52 \mu r \sigma^*_1}{n} \right)\norm{\Delta}_F^2 \\
    &\leq 10 \epsilon \gamma \sigma^*_r\norm{\Delta}_F^2
\end{align*}
The last step is reasoned as follows:
\begin{align*}
    \frac{2}{n\min\{n_1, n_2\}} = \frac{2}{(n_1 + n_2)\min\{n_1, n_2\}} \leq \frac{2}{\max\{n_1, n_2\}\min\{n_1, n_2\}} = \frac{2}{n_1n_2} \leq \frac{2}{n_1(n_2-1)} = \gamma
\end{align*}
\end{proof}


\subsection{Proof of Lemma \ref{lem:smoothness_upperbound}}
The proof of Lemma \ref{lem:smoothness_upperbound} depends on Lemmas \ref{lem:quadratic_form_to_one_norm} and \ref{lem:bound_on_one_norm}.
\lSUB*
\begin{proof}
By Lemma \ref{lem:quadratic_form_to_one_norm}, we have that for any matrices $W, Z \in \mathbb{R}^{n \times r}$,
    \begin{align*}
        \mathcal{D}(WZ^T) \leq 4(\max\{\norm{B_\Dataset}_1, \norm{B_\Dataset^T}_1\}) \norm{Z}_{2, \infty}^2\norm{W}_F^2,
    \end{align*}
By Lemma \ref{lem:bound_on_one_norm} and the assumption on the number of samples we have made, we get that with probability at least $1-\delta$,
    \begin{align*}
        \max\{\norm{B_\Dataset}_1, \norm{B_\Dataset^T}_1\} \leq \frac{4}{\min\{n_1, n_2\}}
    \end{align*}
Putting these inequalities together, we get that with probability at least $1-\delta$, for any matrices $W, Z \in \mathbb{R}^{n \times r}$,
    \begin{align*}
        \mathcal{D}(WZ^T) \leq \frac{16}{\min\{n_1, n_2\}} \norm{Z}_{2, \infty}^2\norm{W}_F^2,
    \end{align*}
For the first of the bounds we wish to prove, we replace $W$ by $\Delta$ and $Z$ by $\solset$. We know that
\begin{align*}
    &\norm{\solset}_{2, \infty}^2 = \frac{\mu}{n}\norm{Z^*}_F^2 \leq \frac{2\mu r \sigma^*_1}{n} \quad ( \because \norm{Z^*}_F^2 \leq 2r\sigma^*_1) \\
    \Rightarrow &\mathcal{D}(\Delta\solset^T) \leq \frac{16}{\min\{n_1, n_2\}} \frac{2\mu r \sigma^*_1}{n}\norm{\Delta}_F^2 \leq 16\gamma(\mu r \sigma^*_1) \norm{\Delta}_F^2
\end{align*}
Here, as in the proof of Lemma \ref{lem:convexity_upperbound}, we use the fact that $2/(n\min\{n_1, n_2\}) \leq \gamma$. The second and third bounds can be derived in a similar fashion.
For the second bound, we use the bound that we established in the proof of Lemma \ref{lem:convexity_upperbound}.
$$\norm{\Delta}_{2, \infty}^2 \leq \frac{52 \mu r \sigma^*_1}{n}$$
Finally, for the third bound, we use the fact that $Z \in \overline{\Incoherentset}$ (see Lemma \ref{lem:incoherence_of_projection}) to get the bound
$$\norm{\solset}_{2, \infty}^2 \leq 12 \frac{\mu}{n}\norm{Z^*}_F^2 \leq \frac{24 \mu r \sigma^*_1}{n}$$
\end{proof}
