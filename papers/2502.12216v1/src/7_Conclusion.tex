\section{Conclusion}

We presented \sys, a sparsity-adaptive attention mechanism for efficient long-context LLM inference. Unlike fixed token budget methods, \sys dynamically selects tokens based on cumulative attention scores, adapting to variations in attention sparsity. By leveraging clustering-based sorting and distribution fitting, \sys accurately estimates token importance with low overhead. Our results showed that \sys outperforms existing sparse attention methods, achieving higher accuracy and significant inference speedups, making it a practical solution for long-context LLMs.
