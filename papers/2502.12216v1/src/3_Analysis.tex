\section{Analysis}
\input{figure-tex/DistributionV}
\input{figure-tex/KLDvsDisSubfigure}
In this section, we first analyze the intrinsic sparsity in attention and how this sparsity affects downstream task performance (\Cref{subsec:analysis-intrinsic-sparsity} \& \Cref{subsec:analysis-attn-approx}).
We then present the drawbacks of existing fixed token budget approaches and propose cumulative \as{} as the new target (\Cref{subsec:analysis-cumulative-as} \& \Cref{subsec:analysis-challenge-as}).
Finally, we illustrate the challenges of applying cumulative \as{} and propose clustering and distribution-fitting techniques as the solution (\Cref{subsec:analysis-clustering} \& \Cref{subsec:analysis-curve-fitting}).


\subsection{Intrinsic Sparsity in Self-Attention Mechanisms}
\label{subsec:analysis-intrinsic-sparsity}

In the decode phase, for one request, assuming there are $n$ previous tokens, the attention formula in  \Cref{eq:attn} can be rewritten as

\begin{equation}
    o = \sum_{i=1}^{n}s_iv_i,~~s_i=\frac{\exp(\frac{qk_i^\top}{\sqrt{d}})}{\sum_{i=1}^{n}\exp(\frac{qk_i^\top}{\sqrt{d}})}.
\end{equation}

\fig{fig:distribution_v} shows that the distribution of $\|v_i\|$ for each token has a small variance. Thus, the contribution of the token is mostly determined by $s_i$. 
Due to the exponential term $\exp(\frac{qk_i^\top}{\sqrt{d}})$, only a small subset of tokens has a significant impact on the model output~\cite{zhang2023h2o,xiao2023streamingllm}, indicating that the attention operation in LLMs is inherently sparse, which motivates the possibility of only loading a subset of tokens to approximate the attention output and incur lower computational overhead. 



\subsection{Rethinking Attention Approximation}
\label{subsec:analysis-attn-approx}
The sparse attention methods can be formulated as
\begin{equation}
    \tilde{o}(I) =  \sum_{i\in I}\tilde{s_i}v_i,~~\tilde{s_i}=\frac{\exp(\frac{qk_i^\top}{\sqrt{d}})}{\sum_{i\in I}\exp(\frac{qk_i^\top}{\sqrt{d}})}
\end{equation}
where $I$ is the index set of tokens selected by the sparse attention method, $I\subset[n]$. The distance between $o$ and $\tilde{o}(I)$ can be formulated as 
\begin{equation}
    \epsilon(I)=\|o-\tilde{o}(I)\|.
\end{equation}

\input{figure-tex/SparsitySubfigure}
\input{figure-tex/AttnDisVary}

Intuitively, a smaller distance between $o$ and $\tilde{o}(I)$ reduces the difference between the output token distributions of sparse attention and full attention. We demonstrate this by measuring the Kullback–Leibler (KL) divergence, a statistical metric that quantifies the difference between probability distributions, on the output logits as a function of attention distance. \Cref{fig:kld_vs_attn_dis} shows that decreasing attention distance consistently lowers KL-divergence, meaning the sparse attention output more closely resembles the full attention output. This alignment in token distributions also improves downstream task accuracy, as demonstrated in \Cref{fig:score_vs_kld} for tasks in the RULER benchmark, which is widely used to assess the long-context abilities of a model. 



Therefore, the goal of sparse attention is to find an index set $I$ that minimizes $\epsilon(I)$ under some constraint of $|I|$.




\subsection{Fixed Token Budget Approaches Lead to Accuracy Variations}
\label{subsec:analysis-fixed-token-budget}
Several methods have been proposed to choose a small set of tokens $I$ minimizing the distance $\epsilon(I)$ between full and approximate attention. Some of the work, including Quest~\cite{tang2024questqueryawaresparsityefficient}, uniformly chooses tokens across attention heads and layers. These results in a large variance of $\epsilon(I)$, as shown in \fig{fig:attn_dis_vary}. This variance stems from the intrinsic sparsity difference across heads and layers. As illustrated in \fig{fig:headvar}, attention heads exhibit distinct sparsity patterns. Some heads display a more uniform distribution of $s_i$ (retrieval heads), whereas others are dominated by a few high-magnitude $s_i$ values (streaming heads). When a fixed number of tokens $|I|$ is selected per head, it leads to inefficiencies—allocating excessive tokens to streaming heads while introducing significant estimation errors in retrieval heads. Similarly, \fig{fig:layervar} highlights variation in sparsity across layers, where earlier layers exhibit lower sparsity compared to later ones, similarly making it inefficient to select a fixed number of tokens from different layers. 


Motivated by the diversity of sparsity patterns across heads and layers, some works, including AdaKV\cite{feng2024adakvoptimizingkvcache} and PyramidKV\cite{cai2024pyramidkvdynamickvcache}, fix the total budget $|I|$ but use calibration data or assumptions to statically assign different budgets to different layers and heads. However, as we show in \fig{fig:queryvar}, the sparsity of particular heads varies significantly depending on the query token. For example, in the model output \textit{``The Answer is ...''}, the token \textit{``Answer"} attends to far fewer tokens compared to \textit{``is"}. This is because \textit{``Answer"} relies primarily on local information to generate \textit{``is"}, whereas \textit{``is"} requires broader context to produce the subsequent answer. Thus, relying on static partitioning of a fixed token budget also falls short of maintaining a consistent low attention distance $\epsilon(I)$. 






\subsection{Cumulative \AS{}: A More Robust Target for Sparse Attention}
\label{subsec:analysis-cumulative-as}

The key drawback of existing work is the reliance on a fixed total token budget, making it hard to adapt to sparsity variations. Instead, we propose directly using the cumulative \as{} of tokens in $I$ to guide token selection.

Specifically, we define $p(I)$ as the cumulative \as{} of tokens in $I$, which is
\begin{equation}
    p(I)=\sum_{i\in I}s_i=\frac{\sum_{i\in I}\exp(\frac{qk_i^\top}{\sqrt{d}})}{\sum_{i=1}^{n}\exp(\frac{qk_i^\top}{\sqrt{d}})}
\end{equation}

These cumulative \as{} targets offer two key advantages over fixed token budgets. First, they inherently adapt to sparsity variations without requiring assumptions or calibration data. Less sparse heads, layers, query tokens, and contexts naturally require more tokens to reach a given cumulative \as{} than sparser ones. Second, targeting cumulative \as{} provides a theoretical guarantee on attention distance. Specifically, the attention distance is bounded by  

\begin{equation}
    \epsilon(I)\leq 2(1-p(I))\max_i\|v_i\|.
\end{equation}  

A detailed proof is provided in \Cref{sec:appendix-bound-proof}. Since value vectors $V$ have similar norms across tokens (\fig{fig:distribution_v}) , setting a threshold $P$ (typically close to 1.0) for $p(I)$ establishes a tight upper bound on $\epsilon(I)$. Identifying the minimal index set $I$ that satisfies $p(I) \geq P$ reduces the variance of the attention approximation error, as shown in \fig{fig:attn_dis_vary}. This improved attention distance approximation directly enhances downstream task performance, as demonstrated in \Cref{subsec:analysis-attn-approx} .  






\subsection{Challenges of Attaining Cumulative \AS{}s}
\label{subsec:analysis-challenge-as}


Identifying the minimal subset of tokens that achieve a target cumulative \as{} is a challenging task. The optimal way is to select tokens following a descending order of \as{} until the cumulative \as{} surpasses the target value. Therefore, like prior approaches, \sys must rank tokens by \as{} to minimize the number of tokens needed to reach the desired cumulative \as{}. However, unlike previous methods, \sys also requires the \as{} values for each token to track the cumulative sum of selected tokens in real-time. This process involves two key components: (1) computing the sum of attention intermediate values, $\exp(qk^\top/\sqrt{d})$, for the selected token set $I$, and (2) computing the total sum of $\exp(qk^\top/\sqrt{d})$ used for normalization. Additionally, this estimation must be computationally efficient, as it lies on the critical path during decoding.  





\subsection{Sorting Tokens via Clustering}
\label{subsec:analysis-clustering}
\input{figure-tex/KeyScatter}

Similar to prior works, \sys groups tokens to reduce computational overhead. However, existing methods rely on positional order, assuming consecutive tokens share similar attention patterns~\cite{tang2024questqueryawaresparsityefficient}. As shown in \Cref{fig:scatter_key}, this is suboptimal since Key vectors of consecutive tokens are often scattered in the embedding space, meaning positional proximity does not imply similarity in attention behavior. Moreover, modern attention kernels efficiently handle non-contiguous KV-cache access, making positional grouping unnecessary. Instead, \sys applies K-means clustering to group tokens based on Key-vector similarity, then ranks them using the dot product between Query vectors and cluster centroids, ensuring selection aligns with actual attention behavior.  


The runtime performance overhead of cluster-based sorting is $\frac{1}{2 \times \text{Average Cluster Size}}$,   compared to full attention\footnote{The term 2 comes from clustering being only performed on K-cache, while the KV cache is twice as large as K-cache.}, which in practice is below $2\%$. 


We validate the results of clustering by showing the ground truth \as{} of tokens after sorting them based on clustering and estimation in \fig{fig:distribution_curve}. Despite rare spikes, clustering-based sorting gives a high-fidelity approximation of full attention-based token ordering.
\subsection{Estimating \AS{} via Distribution Fitting}
\label{subsec:analysis-curve-fitting}
\input{figure-tex/Curve}


While clustering effectively sorts tokens by \as{}, it introduces large errors when estimating absolute \as{} values. This occurs because the cluster centroid represents the center of tokens, but due to non-linearity, its \as{} does not accurately reflect the average \as{} of individual tokens. Thus, \sys requires a more precise approach to estimating \as{}. We observe that after partial sorting, the \as{} distribution follows a consistent pattern across heads, layers, and contexts. For example, as shown in \fig{fig:distribution_curve}, the \as{} is high for a few tokens and then smoothly decreases, forming a long-tail distribution. This structure suggests that function fitting can be used to estimate \as{}. Despite outliers at the beginning of the curve, sampling tokens along the distribution allows accurate parameter estimation, enabling precise \as{} predictions.  




