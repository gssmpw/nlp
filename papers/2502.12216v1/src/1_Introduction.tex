\section{Introduction}
\label{sec:intro}



Large language models (LLMs) power a wide range of applications, from conversational assistants to document analysis systems and search engines. The demand for multi-turn interactions and long-document processing has driven an expansion of context length, growing from thousands to as many as one million tokens~\cite{liu2024scalinglawsropebasedextrapolation}.

However, supporting long contexts in LLM inference presents significant challenges, primarily due to the growing memory footprint of the Key-Value (KV) cache~\cite{tang2024questqueryawaresparsityefficient}. The memory requirements of the KV cache scale proportionally with the context length, therefore, it can quickly become a bottleneck despite optimizations such as Grouped-Query Attention (GQA)~\cite{ainslie2023gqa}. Furthermore, the need to repeatedly load the KV cache for every generated token becomes a bottleneck. For instance, loading the large KV cache can account for over 50\% of the total latency during auto-regressive decoding, significantly impeding the efficiency of large-scale serving systems.~\cite{tang2024questqueryawaresparsityefficient}









To mitigate the high cost of KV-cache loading, recent methods approximate full attention by selecting a subset of stored Key and Value vectors, corresponding to a subset of tokens, within a fixed token budget~\cite{liu2024clusterkvmanipulatingllmkv,tang2024questqueryawaresparsityefficient,zhang2023h2o,xiao2023streamingllm}. These approaches exploit the natural sparsity of attention, where only a small fraction of tokens significantly influence the output due to the softmax operation. By leveraging this sparsity, they aim to reduce the overhead of loading the KV-cache without sacrificing model accuracy.





\input{figure-tex/Teaser}

Alas, existing fixed budget-based methods have several shortcomings. Some methods employ a global fixed token budget~\cite{tang2024questqueryawaresparsityefficient,xiao2023streamingllm,zhang2023h2o}, not accounting for variations in attention sparsity across attention heads, and layers. In practice, some attention heads focus on significantly more tokens than others, and the level of sparsity fluctuates across layers. More adaptive methods~\cite{cai2024pyramidkvdynamickvcache,feng2024adakvoptimizingkvcache,ge2024modeltellsdiscardadaptive} attempt to distribute token budgets more effectively using calibration data or predefined rules, but they remain constrained by static allocation and cannot adapt to query tokens and contexts, often leading to suboptimal approximations in different cases.





To address the limitations of fixed-budget-based methods, we propose \sys, a sparsity-adaptive and
calibration-free post-training sparse attention mechanism that improves both the accuracy and efficiency of long-context LLM inference. \Cref{fig:teaser} shows a comparison between existing fixed budget-based methods and \sys. Instead of enforcing a fixed budget, \sys dynamically selects tokens starting from ones with the highest \as{} to ensure that their cumulative attention scores (where \as{} represents the softmax output of the Query-Key product) reach a target fraction of the full \as{}. 





Dynamic and selective accumulation of \as{}s offers two key advantages. First, it provides inherent flexibility---\sys selects fewer tokens in high-sparsity cases and more in low-sparsity cases without requiring calibration. Second, since \as{}s are multiplied by \( V \) vectors with similar norms, and the selected \as{}s cumulatively reach at least a fraction of the total \as{}, a cumulative \as{} target guarantees, unlike token budgets in prior works, a bounded difference between sparse and full attention (see \Cref{subsec:analysis-attn-approx} and \Cref{sec:appendix-bound-proof}).




However, efficiently selecting tokens to reach a certain fraction $P$ of cumulative \as{} is challenging. To minimize the number of tokens selected (i.e., loads from memory), the optimal way is to select tokens following a descending order of \as{} until the cumulative \as{} surpasses $P$. Thus, similar to prior works, efficiently sorting tokens by their contribution to the cumulative \as{} is crucial for \sys. However, unlike fixed budget-based methods that simply stop at a fixed token count, \sys must track cumulative \as{} in real time, requiring the exact \as{} values for each token, making the selection process more complex.



To approximate optimal token selection, \sys introduces two key techniques: clustering and distribution fitting. First, to efficiently sort tokens, \sys clusters similar tokens to reduce computational overhead. However, we observe that positional proximity, which is used for clustering tokens by prior work~\cite{tang2024questqueryawaresparsityefficient}, does not necessarily guarantee similarity in Key vectors, which are fundamental to attention computation. Since attention operates on Query-Key interactions rather than token positions, \sys groups tokens using K-means clustering based on Key-vector similarity (i.e., vector distance). During decoding, \sys approximates the sorted list of tokens by sorting clusters based on the similarity between Query vectors and cluster centroids. After approximating token sorting,  \sys estimates the \as{} for each token by leveraging the observation that attention scores follow a smooth distribution. Using \df, \sys effectively keep track of attained cumulative \as{} to determine the end of selection. 





By loading only the cluster centroids along with a small sampled subset of tokens ($\sim2.5\%$ of the KV cache size in practice), \sys efficiently selects the most critical tokens that reach the target cumulative \as{}. To balance efficiency and accuracy, \sys performs full attention on newly generated tokens and updates the clustering every fixed number of decoding steps (e.g., 2048).



Our experiments show that \sys achieves superior and consistent accuracy compared to existing algorithms including Quest~\cite{tang2024questqueryawaresparsityefficient}, PyramidKV~\cite{cai2024pyramidkvdynamickvcache} and Ada-KV~\cite{feng2024adakvoptimizingkvcache}, offering a more effective solution for long-context LLM inference in accuracy-sensitive applications. \sys achieves up to \attnspeedup decode attention speedup, which leads to \etoespeedup end-to-end speedup.

In summary, we contribute the following:
\begin{itemize}
    \item A detailed analysis of the dynamic nature of attention sparsity across heads, layers, queries, and contexts.
    \item \textbf{\sys}, a sparsity-adaptive attention algorithm that uses clustering and \df to dynamically determine the token budget for achieving cumulative \as{} targets. %
    \item A comprehensive evaluation of \sys{}, demonstrating \sys{} consistently achieves high accuracy and significant speedup.
\end{itemize}
