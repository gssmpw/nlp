\section{Background}
\subsection{Large Language Models}



LLMs consist of transformer blocks, each with an attention and a feed-forward module. In the attention module, input embeddings are projected into Query ($Q$), Key ($K$), and Value ($V$) vectors. For a given $Q$ vector, attention weights  $w_i$ for the i-th token are computed as $(\frac{QK^\top}{\sqrt{d}})_i$, then normalized via softmax to obtain \as{}s. These \as{}s are multiplies by the $V$ vectors to produce the output (\Cref{eq:attn}). This result is then processed by the feed-forward module, with residual connections and layer normalization refining the final token representation.

\begin{equation}
    \label{eq:attn}
    O=\text{softmax}\left(\exp\left(\frac{QK^\top}{\sqrt{d}}\right)\right)V
\end{equation}
At the request level, LLM inference consists of two phases: the prefill phase and the decode phase. In the prefill phase, all input tokens are processed simultaneously to generate $Q$, $K$, and $V$ vectors, with the $K$ and $V$ vectors stored in the KV-cache to avoid recomputation. In the decode phase, only the last generated token is processed, with its  $Q$, $K$, and $V$ vectors computed. The current $Q$ vector interacts with the cached $K$ and $V$ vectors to generate the output.

Unlike prefill, the decode phase is executed for each generated token, making it the primary bottleneck in inference for many workloads. For instance, in summarization tasks with 64K-token input documents, generating just 1024 tokens can take up to four times longer than the prefill phase.  

\subsection{Long-Context Models}

The recent increasing demand for long-context models has driven advancements in extending the context window of LLMs. Techniques like Rotary Position Embeddings~\cite{su2023roformerenhancedtransformerrotary} have enabled a significant expansion in context length, such as increasing LLaMA-2's context window to 32K in LongChat~\cite{longchat2023} and 128K in Yarn-Llama-2~\cite{peng2023yarn}, and even 1 million recently~\cite{liu2024scalinglawsropebasedextrapolation}. However, as context length grows dramatically, loading the KV cache becomes an increasingly significant bottleneck during token generation, which can account for over half of the total decode time~\cite{tang2024questqueryawaresparsityefficient}.

