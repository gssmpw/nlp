\begin{abstract}




Long-context models are essential for many applications but face inefficiencies in loading large KV caches during decoding. Prior methods enforce fixed token budgets for sparse attention, assuming a set number of tokens can approximate full attention. However, these methods overlook variations in the importance of attention across heads, layers, and contexts.

To address these limitations, we propose \sys, a sparsity-adaptive and calibration-free sparse attention mechanism that dynamically selects tokens based on their cumulative attention scores rather than a fixed token budget. By setting a target fraction of total attention scores, \sys ensures that token selection naturally adapts to variations in attention sparsity. To efficiently approximate this selection, \sys leverages clustering-based sorting and distribution fitting, allowing it to accurately estimate token importance with minimal computational overhead.

We show that \sys outperforms existing sparse attention algorithms, achieving superior accuracy and up to \attnspeedup decode attention speedup. This improvement translates to an overall \etoespeedup end-to-end inference speedup, making \sys a practical and effective solution for long-context LLM inference in accuracy-sensitive applications.



\end{abstract}
