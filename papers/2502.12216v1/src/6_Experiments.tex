\section{Experiments}


\subsection{Setting}

We evaluate \sys for both accuracy and efficiency. We use two models: Llama-3.1-8B-Instruct~\cite{grattafiori2024llama3herdmodels}, a widely used model with Grouped-Query Attention; and MegaBeam-Mistral-7B-512k~\cite{megabeam-mistral-7B-512k-2024}, an extended version of Mistral-7B-Instruct-v0.2 with a 512k token context window.

For accuracy evaluations, we use the PG19 language modeling dataset \cite{rae2019compressivetransformerslongrangesequence}, six tasks from the LongBench dataset\cite{bai-etal-2024-longbench}, including HotpotQA\cite{yang-etal-2018-hotpotqa}, TriviaQA\cite{joshi-etal-2017-triviaqa}, MultifieldQA\cite{bai-etal-2024-longbench},  NarrativeQA\cite{kocisky-etal-2018-narrativeqa}, Qasper\cite{dasigi-etal-2021-dataset}, and Musique\cite{bai-etal-2024-longbench}. Additionally, we conduct experiments on the RULER benchmark\cite{hsieh2024rulerwhatsrealcontext}, using 50 examples for each dataset. We compare \sys with the most popular fixed token budget KV cache eviction algorithms, \quest~\cite{tang2024questqueryawaresparsityefficient}, \pyramid~\cite{cai2024pyramidkvdynamickvcache} and \adakv~\cite{feng2024adakvoptimizingkvcache}. To ensure consistency, we set the page size in Quest and the cluster size in our method to 16. Both \adakv and \pyramid follow the configuration
settings outlined in \cite{feng2024adakvoptimizingkvcache}, including an observation window size of 32 and a max pooling kernel size of 7. 
For the clustering process, we limit the maximum number of iterations to 10.

For efficiency evaluations, we perform the evaluation on Nvidia Ada 6000 GPUs with CUDA 12.4 compared with full attention using Flashinfer~\cite{ye2025flashinfer}.

\subsection{Accuracy Evaluation}
\input{figure-tex/kld}
\input{figure-tex/TokenBudget}
\input{figure-tex/LongBench}
\subsubsection{Accuracy of Clustering \& Distribution Fitting}

To identify the minimal number of tokens to reach the threshold, \sys employs clustering and distribution fitting (explained in \Cref{sec:method}). We evaluate our method on the PG19 dataset, focusing on how well it aligns with the target cumulative \as{} and how many tokens it selects. We set specific \as{} thresholds and compare the actual cumulative score achieved by our method against two oracles: the global optimal, which sums tokens in the descending order of \as{}, and the clustering optimal, which sums \as{} from sorted clusters.
\Cref{tab:token_budget} presents the relative error between target and obtained cumulative \as{}, as well as the comparison of the number of tokens selected by these methods. \sys achieves the target threshold of cumulative \as{} on average with high success rates. Also, the values of \textit{Cluster Optimal} and \textit{\sys} are close, indicating that the distribution fitting presents an accurate estimation of number of tokens.

\subsubsection{Output Accuracy}
We assess the KL-divergence of model output probability distribution of \sys relative to the full attention under Top-K sampling using the PG19 test set\cite{rae2019compressivetransformerslongrangesequence}. 
We include all texts in PG19 with the number of tokens larger than 32k. In the prefill stage, we truncate the input to 32k tokens and feed it into the model. In the decode stage, we feed tokens one by one and collect the output logits of each decode step. We collect 32 decode steps in total. As shown in \Cref{fig:kld}, \sys achieves the most accurate output compared to all baselines.


\subsubsection{Accuracy for long-contexts tasks}
\input{figure-tex/LongBenchTokenBudget}

\textbf{LongBench.}
We evaluate \sys on six LongBench tasks, namely, HotpotQA, TriviaQA, MultiFieldQA, Qasper, NarrativeQA, and Musique, spanning a wide range of scenarios such as single-document QA, multi-document QA, few-shot learning, and synthesis tasks. For each dataset, we first evaluate \sys by setting the cumulative \as{} threshold as 70\% and 90\%. The average number of tokens selected at each threshold serves as the token budget for evaluating \quest, \pyramid, and \adakv.

As shown in \tab{tab:longbench}, \sys consistently outperforms all other baselines. At a threshold of 90\%, \sys achieves performance close to full attention. We provide a detailed table of the average number of tokens selected by \sys across various thresholds, datasets and models in \tab{tab:token-count}, which is set as token budgets for baselines.

\input{figure-tex/Ruler}
\input{figure-tex/RulerTokens}
\textbf{RULER.}
We evaluate \sys and baselines on all tasks in RULER~\cite{hsieh2024rulerwhatsrealcontext} with context length ranging from 16K to 96K.
As shown in the \tab{tab:ruler}, \sys consistently outperforms all baselines in each configuration in terms of average accuracy. Furthermore, at higher thresholds, \sys achieves similar accuracy to full attention, significantly higher than other methods. Similar as \tab{tab:token-count}, we provide a detailed table of token budgets used by baselines in \tab{tab:ruler-token-count}.

\subsection{Efficiency Evaluation}

We begin by analyzing the latency breakdown of \sys, focusing on token clustering during the prefill phase and attention computation for critical tokens during decoding (\Cref{sec:eval:breakdown}). Next, we evaluate \sys's end-to-end performance and its speed-up relative to full attention (\Cref{sec:eval:e2e}).

\subsubsection{Latency Breakdown}
\label{sec:eval:breakdown}

\textbf{Latency of clustering during prefill.}
We measure the time taken clustering for different sequence lengths in \Cref{tab:cluster_time}. We divide the clustering time into two steps. The first step is called distance calculation, where each K-vector computes its distance from the cluster centroids and assigns itself to the nearest cluster. The second step is called cluster update, where the centroids are updated based on the distances of K-vectors in the cluster.

We observe that, as the sequence length increases, the clustering time increases quadratically and is dominated by the distance calculation. However, large sequences also significantly increase the prefill time. Overall, across different sequence lengths, the clustering time always stays below 6\% of the prefill time.

\begin{table}[]
\centering
\caption{Comparison of clustering time and prefill time. Getting distance between K-vectors and cluster centroid dominates the clustering time. For all sequence lengths, clustering overhead is negligible compared to prefill. }
\vspace{0.1in}
\footnotesize
\resizebox{0.8\columnwidth}{!}{
\begin{tabular}{rrrr}
\hline
\textbf{SeqLen}  & \textbf{Get Distance (s)} & \textbf{Update (s)} & \textbf{Prefill (s)} \\ \hline
32768   & 0.15            & 0.01       & 5.66        \\
65536   & 0.61            & 0.01       & 15.80       \\
131072  & 2.72            & 0.02       & 49.53       \\

\end{tabular}
}
\label{tab:cluster_time}
\vspace{-0.1in}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[draft=false,width=\columnwidth]{figure/decode_time.pdf}
    \caption{Latency breakdown of \sys in the attention of decode stage for different sequence lengths and different thresholds.}
    \label{fig:decoding_time}
\end{figure}
\begin{figure}[t]
    \centering
    \includegraphics[draft=false, width=\columnwidth]{figure/union_ablation.pdf}
    \caption{Ablation study on taking union for the GQA model. Taking union significantly reduce the attention time. }
    \label{fig:union}
\end{figure}
\begin{figure}[t]
    \centering
    \includegraphics[draft=false,width=\columnwidth]{figure/e2e_speedup.pdf}
    \caption{End-to-end speedup of \sys compared to the full attention.}
    \label{fig:e2e_speedup}
\end{figure}

\textbf{Latency of attention during decode.}
In the decode stage, \sys identifies and performs attention on critical tokens. We break down this process into four parts: 1) Cluster sorting, where the clusters are ranked based on the dot product of centroids and queries, 2) Distribution fitting, where \sys samples a small portion of tokens and derives the attention score to identify the token budget for each attention head, 3) performing attention for the selected tokens. \Cref{fig:decoding_time} shows the latency of this breakdown for different sequence lengths. 

The latency of sparse attention during decode is reduced significantly, while the overheads of sorting and distribution fitting remain low across various sequence lengths. Overall, \sys achieves up to \attnspeedup speedup compared to the full attention.

\subsubsection{Ablation Study for Query Head Union}

We evaluate the benefits of taking the union of grouped query heads versus computing attention for each query head individually. As shown in \fig{fig:union} Across different context lengths and ratio $P$, taking unions can achieve up to $1.65\times$ attention speedup, due to the reduced memory loading.


\subsubsection{End-to-end performance}
\label{sec:eval:e2e}

We compute the end-to-end performance of \sys with different output tokens, sequence lengths, and ratios in \Cref{fig:e2e_speedup} considering the prefill stages and the clustering overhead. Overall, \sys achieves a speedup up to \etoespeedup compared to full attention.

















