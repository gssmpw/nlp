\documentclass{article}


\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsmath}
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphics
\graphicspath{{media/}}     % organize your images and other figures under media/ folder
\usepackage{natbib}
\usepackage{subcaption}
\usepackage{pifont}
\usepackage{color}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{longtable}

\input{commands}
\input{justification_commands}

%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

% Update your Headers here
\fancyhead[LO]{Vulnerability Mitigation for Safety-Aligned Language Models via Debiasing}
% \fancyhead[RE]{Firstauthor and Secondauthor} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}



  
%% Title
\title{Vulnerability Mitigation for Safety-Aligned Language Models via Debiasing}

\author{
  Thien Q. Tran\footnotemark[1]\\
  LY Corporation \\
  \And
  Akifumi Wachi\thanks{Correspondence to: \{tran.thien, akifumi.wachi\}@lycorp.co.jp} \\
  LY Corporation \\
  %% examples of more authors
   \AND
   Rei Sato \\
  LY Corporation \\
  \And
   Takumi Tanabe \\
  LY Corporation \\
   \And
  Youhei Akimoto \\
  University of Tsukuba, RIKEN AIP \\
}


\begin{document}
\maketitle


\begin{abstract}
Safety alignment is an essential research topic for real-world AI applications.
Despite the multifaceted nature of safety and trustworthiness in AI, current safety alignment methods often focus on a comprehensive notion of safety. By carefully assessing models from the existing safety-alignment methods, we found that, while they generally improved overall safety performance, they failed to ensure safety in specific categories. Our study first identified the difficulty of eliminating such vulnerabilities without sacrificing the model's helpfulness. We observed that, while smaller KL penalty parameters, increased training iterations, and dataset cleansing can enhance safety, they do not necessarily improve the trade-off between safety and helpfulness. We discovered that safety alignment could even induce undesired effects and result in a model that prefers generating negative tokens leading to rejective responses, regardless of the input context. To address this, we introduced a learning-free method, \algo~(\algoshort), to estimate and correct this bias during the generation process using randomly constructed prompts. Our experiments demonstrated that our method could enhance the model's helpfulness while maintaining safety, thus improving the trade-off Pareto-front.
\end{abstract}


\input{body/1_intro}
\input{body/2_preliminary}
\input{body/3_issues}
\input{body/4_method}
\input{body/5_experiment}
\input{body/6_conclusion}

%Bibliography
\bibliographystyle{plainnat}  
\bibliography{references}  

\newpage
\appendix

\input{body/97_proof}
\section{Additional Empirical Results}
\input{body/91_cleaned_data_result}
\input{body/92_bias_no_rejection_data}
\input{body/95_safety_green_full}
\input{body/95_other_experiment_results}
\input{body/93_implement_detail}
\input{body/96_numerical_scores}
\input{body/94_samples}


\end{document}
