\section{The Effectiveness of TSDI}
\label{sec:experiment}


\subsection{Experimental Setting}
We empirically evaluate the effectiveness of \algoshort~in improving the trade-off between safety (i.e., harmlessness) and helpfulness. We utilize the same SFT model as in Safe RLHF and SACPO, a replica of Alpaca-7B. We employ the PKU-SafeRLHF preference dataset, containing over 30,000 records of expert evaluations. Each record separately ranks a pair of responses to a specific prompt based on helpfulness and harmlessness. We conduct the same experiments using the entire dataset and the cleansed dataset, in which the samples satisfying $s(x, y_l) - s(x, y_w) > 0.25$ are removed.

\textbf{Implementation.\space}
We adopt the stepwise approach of SACPO. We first apply DPO to align for helpfulness, resulting in a model referred to as \texttt{DPO(H)}. We then align \texttt{DPO(H)} for safety under various settings. We employed $\beta=0.1$ for the helpfulness alignment and tested a range of $\beta/\lambda$ values for the safety realignment. The safety bias is estimated for the first $20$ output tokens using $500$ randomly constructed $(x, y)$ pairs, as discussed in Section~\ref{sec:method}. The token pool is constructed from the MMLU dataset. For more details on the implementation, see Appendix~\ref{appendix:implementation-detail}.

\textbf{Evaluation.\space}
We use two metrics to measure helpfulness improvement: compliance rate and helpfulness win rate. The compliance rate assesses if the models refuse to respond with expressions like ``I'm sorry'' or ``Unfortunately.'' We use 53 keywords, of which 47 keywords are from~\citet{zou2023universal} (see Appendix~\ref{appendix:rejective-keywords} for the completed list). This metric assesses the helpfulness at the token level, aligning with the intention of the proposed method. On the other hand, we measure the win rate against the SFT model using GPT-4 to evaluate the quality of the responses, which cannot be evaluated by the compliance rate. Our GPT-4 prompt is based on those in the SafeRLHF and SACPO study, with a minor adjustment in the output format (see Appendix~\ref{appendix:gpt4-prompt}). We use prompts from the AlpacaEval dataset, which are unlikely to elicit harmful content. To evaluate safety, we employ the balanced SALAD-Bench dataset with MD-Judge and Llama Guard 3 to ensure robust results across different evaluators.
Several generation examples are provided in Appendix~\ref{appendix:examples}.

\subsection{Experimental Results}
We finally present the experimental results showing the effectiveness of \algoshort~in improving the safety-helpfulness trade-off.
Although we conducted these experiments with both the entire dataset and the cleansed dataset, we only present the results for models trained with the entire dataset due to page limit constraints. The results with the cleansed dataset are provided in Appendix~\ref{appendix:experiment-cleansed-data}. It is important to note that similar results are obtained for both cases.


\textbf{Can \algoshort~effectively remove negative tokens?} We observe that \algoshort~significantly enhances the compliance rate without compromising safety, as shown in Figure~\ref{fig:trade-offs-compliance}. This figure illustrates the trade-offs between the MD-Judge's safety scores of three different categories and the compliance rate to harmless prompts for models trained with various $\beta/\lambda$ values and training iterations. We show a similar result evaluated with Llama Guard 3 in Appendix~\ref{appendix:performance_llama_guard}. Importantly, the improvement is consistent across all training settings, which matches our expectation since \algoshort~is token-based, aligning with how the compliance rate is measured.

\begin{figure*}[t]
    \begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=150mm,clip,trim=0 5 0 5]{figure/beta_scatter_full_mdjudge_rejection.pdf}
    \caption{Trade-offs between safety scores and compliance rate to harmless prompts.}
    \vspace{10pt}
    \label{fig:trade-offs-compliance}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=150mm,clip,trim=0 5 0 5]{figure/beta_scatter_full_mdjudge_helpful.pdf}
    \caption{Trade-offs between safety scores and the helpful win rate.}
    \label{fig:trade-offs-helpfulness}
    \end{subfigure}
    \caption{Trade-offs between MD-Judge's safety score of three different categories and (a) compliance rate to harmless prompts and (b) helpful win rate versus SFT model. The number in brackets indicates the category number. Different points correspond to the combinations of different $\beta/\lambda$ and the number of iterations.}
\end{figure*}

\textbf{Did \algoshort~improve the safety-helpfulness trade-off?} Overall, \algoshort~improves the safety-helpfulness Pareto-front, as illustrated in Figure~\ref{fig:trade-offs-helpfulness}. This figure shows the trade-off between the MD-Judge's safety score and the helpfulness win rates against the SFT model, evaluated by GPT-4 (see Appendix~\ref{appendix:performance_llama_guard} for the results with Llama Guard 3). We conduct significance testing and conclude that the results are statistically reliable (see Appendix \ref{appendix:significance-testing}). We also show that TSDI is robust to the choice of $L$ and the token pool in Appendix \ref{sec:changing_L} and \ref{sec:changing_token_pool}, respectively. Still, the improvement is less significant than the increment observed in the compliance rate, particularly for models with very low helpfulness. We found instances where \algoshort~effectively removes the negative tokens at the beginning. Still, these responses remained unhelpful, highlighting the limitations of a token-level debiasing approach. Alternative debiasing methods could be effective such as the ones addressing the hidden state \cite{li2024rethinkingjailbreaking,xu2024uncoveringsafety,zou2024improving}.
We leave the exploration of alternative methods for future work.

\textbf{Safety levels of other categories and comparison with existing methods.} Compared to other models from existing methods, TSDI achieves better balances between helpfulness and safety, demonstrating that TSDI successfully improves the model's safety across all categories while maintaining its helpfulness. Figure~\ref{fig:salad_bench_result} shows the safety scores of all categories and the trade-off between the helpfulness win rate versus the mean safety score. Importantly, our experiment was conducted under the \textit{same} conditions as SACPO and beaver-7b-v1.0, while under \textit{less favorable} conditions than -v2.0 and -v3.0, in terms of both data quantity and quality. For \algoshort, we employ the debiased model trained with $\beta/\lambda = 0.025$ for 200 iterations. After debiasing, the helpfulness win rate of our model improved from 0.59 to 0.67, while maintaining a high level of safety.