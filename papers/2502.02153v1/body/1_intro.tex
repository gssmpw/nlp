\section{Introduction}
\label{sec:intro}

Large language models (LLMs) have demonstrated exceptional capabilities across various real-world applications, including translation~\citep{zhang2023prompting}, content creation~\citep{yuan2022wordcraft}, and coding~\citep{chen2021evaluating,gao2023pal}. As the use of LLMs extends into high-stakes domains such as medicine~\citep{thirunavukarasu2023large}, law~\citep{cui2023chatlaw}, robotics~\citep{shah2023lm}, and autonomous driving~\citep{chen2023driving}, the importance of safety in AI systems becomes paramount to maximize benefits while minimizing societal risks~\citep{gehman2020realtoxicityprompts,lin2021truthfulqa,liu2023trustworthy}.

\textit{Alignment}~\citep{ji2023ai} has emerged as a promising approach to embed human values into LLMs, thereby improving their helpfulness and safety. Techniques such as reinforcement learning from human feedback (RLHF, \citet{christiano2017deep,ouyang2022training}) and direct preference optimization (DPO, \citet{rafailov2023direct}) have played a crucial role in making LLMs more helpful and harmless. However, these methods often rely on a single reward metric to determine output quality, which does not consistently ensure high safety levels \citep{dai2024safe}. 

Given the complexity of modeling both helpfulness and safety using a singular reward function, it is natural to formulate the LLM alignment problem using multiple separate functions. Safe RLHF~\citep{dai2024safe} is a pioneering approach that introduces the (constrained) safe RL paradigm into the alignment of LLMs. This approach enhances the development of LLMs that effectively balance reward (i.e., helpfulness) and safety (i.e., harmlessness). As computationally efficient approaches of Safe RLHF, \citet{wachi2024stepwise} and \citet{huang2024one} respectively proposed SACPO and CAN, in which constrained LLM alignment problems are solved using RL-free algorithms such as DPO.

A limitation common to the existing safety alignment methods (e.g., Safe RLHF, SACPO, CAN) is their tendency to focus on a singular notion of safety. This focus obscures specific vulnerabilities and fails to address the distinct safety requirements of different domains. AI safety is inherently multifaceted~\citep{amodei2016concrete,bostrom2018ethics}. Ensuring societal acceptance of AI systems requires considering diverse metrics such as harmlessness, bias, security, fairness, and privacy~\citep{wang2023decodingtrust}. Moreover, the previous safety alignment studies \citep{dai2024safe,wachi2024stepwise,huang2024one} suffer from misdirected safety evaluations; that is,
comparative evaluations based on humans or advanced LLMs may inaccurately deem a model safe if it performs better than a poor comparative model.
With potential vulnerabilities in specific topics possibly hidden by misdirected evaluations, existing works lack investigation for achieving multifaceted safety.

\subsection*{Our Contributions}

First, we demonstrate the challenges in eliminating the vulnerability of safety-aligned models and achieving a high level across all safety metrics. Specifically, predicting the vulnerabilities of LLMs before the alignment is difficult, even with a thorough analysis of pre-trained LLMs or the alignment dataset. The alignment process itself is complex, with varying difficulty and data quality across different categories. Moreover, applying safety alignment with various strengths shows that a smaller KL penalty leads to higher safety scores but also decreases the model's helpfulness. This trade-off highlights the difficulty in maintaining the model's helpfulness while eliminating these safety vulnerabilities. We also observed that, while data cleansing helps achieve higher safety levels, it does not necessarily improve the trade-off between helpfulness and safety.

We propose a learning-free method, \algo~(\algoshort), to estimate and mitigate the unwanted effects caused by safety alignment. By creating a synthetic dataset with random tokens, we estimate the safety bias for each output position. Interestingly, we found that the logits of negative tokens, leading to rejective and unhelpful responses, increase significantly even for randomly constructed prompts after safety alignment. \algoshort~subtracts this bias to adjust the output logits accordingly during the generation process. Our experiments show that \algoshort~improves the model's helpfulness while maintaining its safety, resulting in an improved safety-helpfulness trade-off.