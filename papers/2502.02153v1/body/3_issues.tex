\section{Limitation of Existing Works}
\subsection{Vulnerabilities in Specific Safety Categories}
\label{sec:vulnerabilities}


Existing safety alignment methods (e.g., Safe RLHF and SACPO) primarily focus on a comprehensive notion of safety. While improving the overall safety of the model, these approaches oversee specific risks associated with distinct safety categories. In practice, safety is multifaceted, including categories such as adult content, hate speech, and privacy violations. Each category represents a unique safety aspect and requires different safety bars.

We carefully assess the safety performance of LLMs trained by Safe RLHF and SACPO on various safety categories. In particular, we employed MD-Judge and Llama Guard 3 safety classifiers on a balanced subset of the SALAD-Bench dataset. We randomly selected $68$ prompts for each of the $66$ categories in this dataset, resulting in a dataset comprising $4488$ red-teaming prompts. For a prompt-response pair $(x, y)$, these safety evaluators provide a safety probability $s(x, y) \in [0, 1]$.
We call $(x, y)$ a safe pair of prompt and response if $s(x, y) \ge 0.5$ holds.
In this work, we define a safety score $p_\text{safe}(\pi; \overline{\calD_k})$ to calculate the safety level of an LM $\pi$ for the $k$-th category, based on a dataset $\overline{\calD_k}$.
Suppose we have access to a dataset $\overline{\calD_k} \coloneqq \{(x_i, y_i)\}_{i=1}^{n_k}$ with a set of input prompts $\{x_i\}_{i=1}^{n_k}$ from the $k$-th category of SALAD-Bench dataset, and corresponding repsponses $\{y_i\}_{i=1}^{n_k}$ for each prompt; that is, $y_i \sim \pi(\cdot \mid x_i)$ for all $i \in [n_k]$.
Note that $n_k \in \mathbb{Z}_+$ is the number of prompt-response pairs.
% Let $Y = \{y_i \mid y_i \text{ is the response generated by } \pi \text{ for prompt } x_i \in X\}$.
Then, the safety score is calculated as the percentage of responses classified as safe by each safety evaluator:
%
\begin{equation*}
    p_\text{safe}(\pi; \overline{\calD_k}) \coloneqq n_k^{-1} \cdot |\{(x_i, y_i) \in \overline{\calD_k} \mid s(x_i, y_i) \ge 0.5\}|.
\end{equation*}
%
Figure~\ref{fig:salad_bench_result} presents the safety scores of various models from existing works, evaluated by MD-Judge. We show a similar figure evaluated with Llama Guard 3 in Appendix \ref{appendix:performance_llama_guard}. Safe RLHF includes three models (beaver-7b-v1.0, -v2.0, and -v3.0) depending on the number of data collection and fine-tuning. We also show the helpfulness win rate against the SFT model, noting that all these models used the same SFT model. While existing methods improved the overall safety performance, they failed to ensure safety in specific categories, such as adult content. The only model demonstrating sufficient safety across all topics is beaver-7b-v2.0; however, it exhibits very low helpfulness, even worse than the SFT model. Figure~\ref{fig:salad_bench_result} emphasizes the importance of considering multiple safety categories to ensure complete safety.

Existing works lack discussion of achieving higher safety for such vulnerable safety categories. Since these works use a single cumulative safety measure, a model may be considered safe overall if it performs well on most topics despite significant weaknesses in certain areas. This masking effect hinders a thorough understanding and addressing the challenges of achieving a high safety level for all categories.
This paper aims to identify such overlooked vulnerabilities, discuss the challenges, and provide solutions.

\subsection{Challenges in Balancing Helpfulness and Safety}
\label{sec:challenges-safety-helpfulness}

\begin{figure}[t]
\centering
    \includegraphics[width=0.8\linewidth,clip,trim=0 10 0 7]{figure/full_iter_beta.pdf}
    \caption{Helpfulness win rate and safety score of Adult Content category for various  $\beta/\lambda$ and number of iterations.}
    \label{fig:full_iter_beta}
\end{figure}

We conduct experiments to reassess the challenges of achieving high safety for specific safety categories. Here, we focus on the Adult Content category (Category 03), the most significant vulnerability of existing models. We employ SACPO's stepwise approach, which initially applies DPO to align for helpfulness and then for safety. Our experiment setup is largely similar to that used in SACPO. We utilized the same SFT model as SACPO and Safe RLHF, a replicated version of Alpaca-7B~\citep{alpaca}. We also employed the same preference dataset, namely PKU-SafeRLHF \citep{ji2024beavertails}, in which each record contains a pair of responses to a specific prompt, ranked by helpfulness and harmlessness. We set the KL divergence penalty coefficient $\beta=0.1$ for helpfulness alignment and test various $\beta/\lambda$ values for safety alignment. We also vary the training iterations to consider the effect of longer safety alignment.

Figure~\ref{fig:full_iter_beta} shows the safety score for the Adult Content category and the helpfulness win rate against the SFT model.
We observed that higher safety is achieved using a smaller KL penalty or increasing training iterations. We note that increasing the training iterations might improve the safety score, but it often eventually plateaus. Conversely, using a smaller KL penalty has a much more pronounced effect in obtaining higher safety levels. However, since fine-tuning these parameters leads to higher safety, it often decreases the model's helpfulness. In particular, reducing $\beta/\lambda$ leads to higher safety scores for the Adult Content category but might significantly decrease the helpfulness win rate versus the SFT model. We also observed that a small $\beta/\lambda$ and excessive training iterations sometimes led to generation corruption (see Appendix \ref{appendix:corruption_examples} for examples). These results demonstrate the difficulty in mitigating all safety vulnerabilities while preserving the helpfulness of the model.


\subsection{Challenges in Improving the Dataset}
\label{sec:challenges-data-improvement}


We discuss the challenges in improving the safety preference dataset. Initially, we observed that there seems to be room for data improvement. We inspect the safety preference dataset by applying the safety evaluator MD-Judge to all samples in the PKU-SafeRLHF dataset. For each data tuple $(x, y_w, y_l)$, we assessed the safety probabilities $s(x, y_w)$ and $s(x, y_l)$ for chosen and rejected responses. Figure~\ref{fig:safety_prob_heatmap} illustrates the heatmap plot of safety probabilities for chosen and rejected responses. We observed a decent number of samples where the chosen response had a lower safety probability than the rejected one, raising questions about the potential benefits of cleansing the dataset in our setting.

First, we found that it is difficult to predict the vulnerabilities a priori by inspecting the reference LLMs (SFT model in Figure \ref{fig:salad_bench_result}) or the alignment dataset. Figure~\ref{fig:salad_bench_result} shows that the reference LLM is not particularly bad at handling adult content. Moreover, the adult-related samples are neither particularly low in quality nor lacking in quantity. As shown by \citet{ji2024pku}, the number of adult-related samples is comparable to other categories. We further investigate the distribution of safety scores for each category, using the category information assigned by MD-Judge when a prompt-response pair is classified as unsafe. We excluded the samples where both responses are classified as safe, as category information can not be identified. Figure~\ref{fig:stacked_bar_plot} shows that the fraction of data where $s(x, y_w) > s(x, y_l)$ is not particularly low for Category 03, indicating that the safety preference data is not of particularly low quality. This difficulty may arise because LLM alignment is not a straightforward procedure, and the hardness of aligning each category may vary. Moreover, these categories are interrelated and may influence each other.

We also found that removing the training samples where the safety probability for the chosen response was significantly lower than that for the rejected one does not necessarily improve the safety-helpfulness trade-off. We removed all the samples where $s(x, y_l) - s(x, y_w) > 0.25$, then conducted safety alignment using the cleansed dataset with the same settings as Section~\ref{sec:challenges-safety-helpfulness}. This cleansing procedure removed 577 samples (2.14\%) among the original 26,872 samples. Surprisingly, removing this small data subset significantly improved the safety level when training under identical training settings compared to using the entire dataset. We provide a detailed plot showing the safety levels of two datasets for different $\beta/\lambda$ values and training iterations in Appendix~\ref{appendix:safety-green-full}. However, Figure~\ref{fig:beta_scatter_plot} shows that data cleansing does not necessarily enhance the trade-off between safety and helpfulness. The resulting performance using the entire and cleansed dataset typically lies on the same Pareto-front, indicating that data cleansing does not fully resolve our challenges.


\begin{figure*}[t]
    \centering
%    \begin{subfigure}[b]{80mm}
    \begin{subfigure}[b]{0.26\hsize}
        \centering
        \includegraphics[width=0.95\linewidth,clip,trim=0 5 0 3]{figure/safety_prob_scatter_md_judge.pdf}
        \caption{}
        \label{fig:safety_prob_heatmap}
    \end{subfigure}%
    \begin{subfigure}[b]{0.4\hsize}
        \centering
        \includegraphics[width=0.95\linewidth,clip,trim=0 5 0 3]{figure/stacked_bar_plot.pdf}
        \caption{}
        \label{fig:stacked_bar_plot}
    \end{subfigure}%
    \hspace{2mm}
    \begin{subfigure}[b]{0.31\hsize}
        \centering
        \includegraphics[width=0.95\linewidth,clip,trim=0 5 0 3]{figure/beta_scatter_all_mdjudge.pdf}
        \caption{}        
        \label{fig:beta_scatter_plot}
    \end{subfigure}
    \caption{(a) Safety probabilities evaluated by MD-Judge for $(y_w, y_l)$ in the PKU-SafeRLHF dataset. (b) Number of samples for each safety category. (c) Helpfulness win rate and safety score for models trained with and without data cleansing.
    \label{fig:challenges}}
\end{figure*}