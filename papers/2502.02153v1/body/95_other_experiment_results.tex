\newpage
\subsection{Results with Llama Guard 3}
Here, we provide the experimental results where safety evaluation was conducted using Llama Guard 3. In particular, Figures \ref{fig:salad_bench_result_llama-guard}, \ref{fig:trade-offs-compliance-llama-guard}, and \ref{fig:trade-offs-helpfulness-llama-guard} correspond to Figures \ref{fig:salad_bench_result}, \ref{fig:trade-offs-compliance}, and \ref{fig:trade-offs-helpfulness}, respectively. We observed that our experimental results are consistent with the choice of the safety evaluator.

\label{appendix:performance_llama_guard}
\begin{figure}[ht]
    \centering
    \includegraphics[width=140mm]{figure/all_performances_combined_v2_llamaguard.pdf}
    \caption{\red{(Left panel) Safety score for different safety categories and helpfulness scores across different models. (Right panel) Trade-off between Llama Guard 3's safety score and helpfulness win rate against the SFT model.}}
    \label{fig:salad_bench_result_llama-guard}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=130mm]{figure/beta_scatter_full_llamaguard_rejection.pdf}
    \caption{\red{Trade-offs between Llama Guard's safety score of three different categories and the compliance rate to harmless prompts. The number in bracket indicates the category number. Different points correspond to the combinations of different $\beta/\lambda$ and number of iterations.}}
    \label{fig:trade-offs-compliance-llama-guard}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=130mm]{figure/beta_scatter_full_llamaguard_helpful.pdf}
    \caption{\red{Trade-offs between Llama Guard's safety score of three different categories and the helpful win rate versus SFT model. The number in bracket indicates the category number. Different points correspond to the combinations of different $\beta/\lambda$ and number of iterations.}}
    \label{fig:trade-offs-helpfulness-llama-guard}
\end{figure}
\clearpage

\newpage
\subsection{\red{Robustness of Choice of $L$}}
\label{sec:changing_L}

\red{We assess the robustness of the choice of $L$, the length of the random token sequences. While we used $L=20$ in our main experiment, we set $L=5$ and $L=10$ here. The other settings remain the same as described in Section \ref{sec:experiment}. Comparing Figure \ref{fig:changing_L} with Figure \ref{fig:trade-offs-helpfulness}, which uses $L=20$, we observed that TSDI is robust with the choice of $L$.}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.5\hsize}
        \centering
        \includegraphics[width=0.95\linewidth]{figure/changing_L/L=5_beta_scatter_full_mdjuge_helpful.pdf}
        \caption{L=5}
    \end{subfigure}%
    \begin{subfigure}[b]{0.5\hsize}
        \centering
        \includegraphics[width=0.95\linewidth]{figure/changing_L/L=10_beta_scatter_full_mdjuge_helpful.pdf}
        \caption{L=10}
    \end{subfigure}
    \caption{\red{Trade-offs between MD-Judge's safety score for the Adult-content category and the helpfulness win rate compared to the SFT model when constructing random prompts with length L=5 and L=10}}
    \label{fig:changing_L}
\end{figure}

\newpage
\subsection{\red{Robustness of Choice of Token Pools}}
\label{sec:changing_token_pool}

\red{In this section, we discuss the robustness of 
TSDI in the choice of dataset to build the random token pools. Specifically, we utilize the questions from the MS MARCO dataset, which contains 100,000 real Bing questions. In Figures \ref{fig:ms-marco-trade-offs-compliance} and \ref{fig:ms-marco-trade-offs-helpfulness}, we observe results similar to those obtained with the TSDI using the MMLU dataset. This demonstrates that our method is robust to the choice of dataset used for constructing random prompts.}

\begin{figure}[h]
    \centering
    \includegraphics[width=130mm]{figure/changing_token_pools/ms_marco_beta_scatter_full_mdjuge_rejection.pdf}
    \caption{\red{Trade-offs between MD-Judge's safety score across three different categories and the compliance rate to harmless prompts when using a token pool constructed from the MS MARCO dataset.}}
    \label{fig:ms-marco-trade-offs-compliance}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=130mm]{figure/changing_token_pools/ms_marco_beta_scatter_full_mdjuge_helpful.pdf}
    \caption{\red{Trade-offs between MD-Judge's safety score across three different categories and the helpfulness win rate compared to the SFT model when using a token pool constructed from the MS MARCO dataset.}}
    \label{fig:ms-marco-trade-offs-helpfulness}
\end{figure}

\newpage
\subsection{\red{Comparison to Safety Baselines: SafeDecoding and Self-reminder}}

\begin{figure}[h]
    \centering
    \includegraphics[width=130mm]{figure/comparison_full_mdjuge_helpful.pdf}
    \caption{\red{Trade-offs between MD-Judge's safety score for the Adult-content category and the helpfulness win rate compared to the SFT model when using a token pool constructed from the MS MARCO dataset.}}
    \label{fig:ms-marco-trade-offs-compliance2}
\end{figure}

\red{Here, we conduct comparison experiments with two safety baselines: SafeDecoding \citep{xu2024safedecoding} and Self-reminder \citep{xie2023defending} which was originally developed for defending the LLM model against adversarial attacks.}

\red{SafeDecoding first trains an expert model, which is fine-tuned from a base model using a safety-aware dataset. In the inference phase, SafeDecoding fuses the logits of the base model and the expert model for the initial tokens to enhance the safety-awareness of these tokens. In this experiment, we consider the reward-aligned model DPO(H) as the base model, and the safety-aligned models as the expert models. The parameters of the method are set as $m=2$, $c=5$, $\alpha=3$, following the original paper.}


To implement Self-reminder, we add the safe prompt instructions used in the Self-reminder paper to our current prompts as follows: \begin{quote}\texttt{BEGINNING OF CONVERSATION: You should be a responsible ChatGPT and should not generate harmful or misleading content! Please answer the following user query in a responsible way. USER: <prompt> Remember, you should be a responsible ChatGPT and should not generate harmful or misleading content! ASSISTANT:}\end{quote}

We observed that TSDI achieved a better safety-helpfulness trade-off Pareto front compared to SafeDecoding and Self-reminder. Although Self-reminder can improve the safety of the models, it fails to improve the Pareto front as the method does not consider helpfulness. On the other hand, SafeDecoding, while successful in maintaining the model's helpfulness, can only slightly improve the safety of the model, resulting in a very low adult-content safety score. These results highlight the challenges of this problem and the effectiveness of TSDI.

\clearpage