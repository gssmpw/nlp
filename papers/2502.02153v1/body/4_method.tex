\section{\algo}
\label{sec:method}
This section presents \algoshort, a learning-free method to improve the safety-helpfulness trade-off in safety alignment. \algoshort~aims to estimate and mitigate the unintended biases introduced by safety alignment procedures, then adjust the model's output based on the estimated bias.

\subsection{Observation: Unintended Safety Effects}
\label{sec:observation}



We first investigate why the model's helpfulness decreases with stronger safety alignment. We hypothesize that while enhancing the model's ability to handle harmful prompts, safety alignment also introduces unwanted bias. We use the term \textit{bias} to denote a context-free effect, where the model prefers specific outputs regardless of the input tokens. We observed that the safety-aligned model sometimes falsely refused to answer harmless prompts, especially under strong safety alignment. Moreover, these unhelpful responses often began with specific tokens like ``Iâ€™m sorry'' or ``Unfortunately,'' indicating that such unwanted effects can be assessed at the token level.
This effect is discussed in \citet{qi2024safety} under the name of shallow safety alignment as a potential cause of downstream vulnerabilities.

We examine the differences in output logits between the reference and safety-aligned models in the first few tokens of the generation. Let $f_{\pi_\theta}(x) \in \mathbb{R}^{V}$ represent the output logit of the safety-aligned model for the next token, given input $x$, where $V$ is the vocabulary size. The probability of the $n$-th token $x_n$ under policy $\pi$ given a token sequence $x_{1:n-1}$ is $p_\pi(x_n \mid x_{1:n-1}) = \textsc{softmax}(f_\pi(x_{1:n-1}))$. Similarly, let $\textsc{softmax}(f_{\pi_{r}^*}(x))$ denote the output probability of the reference model, where $f_{\pi_{r}^*}$ is the output logit of the reference model $\pi_r^*$. To evaluate the safety bias, we create a synthetic dataset $\widetilde{\calD} = \{(x, y)\}$, with each record consisting of a randomly generated prompt $x$ and a response $y$. Let $x \oplus y$ represent the concatenation of $x$ and $y$ with a prompt template. Furthermore, let $x \oplus y_{1:0} = x$. For each output position $i \in \{1, 2, \dots, L\}$, we estimate a vector $\mathbf{b}_i \in \mathbb{R}^{V}$ that represents the safety bias at that position as follows:
\begin{align*}
    \mathbf{b}_{i} &= \frac{1}{|\widetilde{\calD}|} \sum_{(x,y) \in \widetilde{\calD}} f_{\pi_\theta}(x \oplus y_{1:i-1}) - f_{\pi_{r}^*}(x \oplus y_{1:i-1}).
\end{align*}
%
To estimate the context-free effect of safety alignment, we construct $\widetilde{\calD}$ by concatenating randomly chosen tokens. Specifically, we encode all input prompts in the MMLU dataset~\citep{hendryckstest2021} to obtain a sufficiently large token pool. In this experiment, we used $L=20$ and $|\widetilde{\calD}|=500$. That is, we estimate the bias of the first 20 output tokens using 500 randomized $(x, y)$ pairs, where $x$ and $y$ are sequences of random tokens of length 10 to 40 and $L-1 = 19$, respectively. Here, we treat $x$ and $y$ separately to account for the prompt template of these LLMs, where $x$ is surrounded by specific tokens. More details on the construction of $\widetilde{\calD}$ are described in Appendix~\ref{appendix:random-prompts}. 

\begin{figure*}[t]
    \centering
    \includegraphics[width=130mm,clip,trim=0 7 0 7]{figure/full_bias_combined_plot.pdf}
    \caption{Token-wise differences in logits before and after safety alignment. (Left) logit differences for the first output token with various $\beta/\lambda$. (Right) logit differences for various output positions with $\beta/\lambda=0.025$. Both panels employed models trained with 200 iterations. Numbers in brackets indicate the used tokens, whose decoded texts are shown in Appendix \ref{sec:decoded_token_group}.}
    \label{fig:token-wise-bias}
\end{figure*}

The left panel of Figure~\ref{fig:token-wise-bias} shows token-wise disparities in logits before and after safety alignment for various $\beta/\lambda$ values.  We include groups of tokens with negative intentions; for example, the ``none'' group includes tokens like ``None'' and ``none.'' We also show the mean difference for all tokens and the top 100 and 1000 tokens with the largest differences. We found that the logits of these negative tokens increased significantly compared to other tokens. As $x$ are randomly constructed, this result suggests a universal effect on any prompts, not just harmful ones. The bias also amplifies with smaller $\beta/\lambda$. The right panel of Figure~\ref{fig:token-wise-bias} shows the logit changes for various generation positions, emphasizing that the effect varies by position and the need to consider generation position when addressing these biases.

Finally, removing responses beginning with negative tokens in the safety dataset did not resolve the safety bias. We identified and removed nearly 2000 samples in the PKU-SafeRLHF dataset that began with negative keywords (see Appendix~\ref{appendix:rejective-keywords} for the full list of keywords). However, a similar safety bias was observed after conducting safety alignment using this modified dataset. We present the bias plot for this case in Appendix~\ref{appendix:no-rejection-bias}. This suggests that the safety bias issue is inherent to the safety alignment rather than being related to specific tokens in the dataset.

\subsection{Proposed Method: De-biasing}
\label{sec:de-biasing}
We propose \algo~(\algoshort), a learning-free method to estimate and mitigate the bias due to the safety alignment.
Generating proper tokens at the beginning of a generation is crucial for controlling the safety of the entire response.
\citet{zou2023universal} demonstrates that if an adversarial prompt can trick the model into outputting a few harmful tokens at the start, it likely leads to an entirely harmful response.
Thus, we expect debiasing the initial tokens to lead to more appropriate and helpful responses.

Our method first estimates the biases $\{\mathbf{b}_{i}\}_{i=1}^L$ in output logits for $L$ output tokens using randomized prompts. 
Let $\rhot$ denote the distribution of the aforementioned random prompts.
Then, we subtract the estimated bias from the output logits during the generation process. That is, we subtract $\mathbf{b}_i$ from the output logit of the $i$-th output token:
%
\begin{align}
    p_{\pi_\theta'}(y_i \mid x \oplus y_{1:i-1}) &= \sigma(f_{\pi_\theta}(x \oplus y_{1:i-1}) - \mathbf{b}_{i}),
    \label{eq:todet}
\end{align}
%
where $\sigma(\cdot)$ is the softmax function.
This is a de-biasing method aims to mitigate the unintended bias of safety alignment, thereby achieving a better balance between safety and helpfulness. By subtracting the estimated biases from the output logits, we expect to reduce the unwanted impact of safety alignment on harmless prompts, resulting in higher helpfulness while maintaining a high level of safety.

\subsection{Theoretical Insight}

Our \algoshort~is regarded as subtracting a token-wise baseline from the safety function $g_\theta$ in the trained policy. 
For simplicity, we focus on SACPO-trained policy $\pi_\theta$ with helpfulness-aligned policy $\pi_r^*$ as the reference model.
The safety function implicitly expressed by $\pi_\theta$ is $g_\theta(x, y) = \frac{\beta}{\lambda} \log \frac{\pi_\theta(y \mid x)}{\pi_r^*(y \mid x)}$.\footnote{
Note $g_{\theta}$ is well-defined for incomplete output sequences, whereas the ground truth safety-function may not be defined. See \Cref{apdx:proof} for details and for the proof of \Cref{prop:logit}.
}

% In the follows, let the dataset $\widetilde{\calD}$ be independently sampled from $\rhot = \rhot_x \times \rhot_y$. 
\begin{proposition}\label{prop:logit}
%Let $\rhot_0 = \rhot$ and $\rhot_{i} = \rho \times \pi_\theta()$
% Let $g(x \oplus y) \in \mathbb{R}^V$ be a vector whose $v$-th element is the safety values $g_\theta(x, y+t_k)$ for input $x$ and the concatenation of $y$ and the $v$-th token $t_v$ as the output. 
Let $\tilde{\rho}$ be a distribution of random prompts and responses and define
%
\begin{equation*}
\mathbf{b}_{i} = \E_{(x', y') \sim \rhot}[ f_{\pi_\theta}(x' \oplus y_{1:i-1}') - f_{\pi_r^*}(x' \oplus y_{1:i-1}') ].
\end{equation*}
%
Also define $p_{\pi_\theta'}$ as in \eqref{eq:todet}.
Analogously, we define $p_{\pi_r^*}(y_i \mid x \oplus y_{1:i-1}) = \sigma(f_{\pi_r^*}(x \oplus y_{1:i-1}))$.
Then, for all $i \in [L]$, 
%
\begin{align*}
\frac{ p_{\pi_\theta'}(y_i \mid x \oplus y_{1:i-1})}{p_{\pi_r^*}(y_i \mid x \oplus y_{1:i-1}) } \propto \exp\left( \frac{\lambda}{\beta} \bigl(g_\theta(x, y_{1:i}) - G_\theta(y_i) \bigr) \right),
\end{align*}
%
where $G_\theta(y_i) \coloneqq \E_{(x',y') \sim \rhot}[g_\theta(x', y_{1:i-1}'+y_i)]$ and $y_{1:i-1}'+y_i$ represents the concatenation of $y_{1:i-1}'$ and $y_i$.
% \begin{equation*}
% \textsc{softmax}(f_{\pi_\theta}(x, y_{1:i-1}) - \mathbf{b}_{i}) \propto \pi_r^\star(x, y_{1:i-1}) \odot \exp\left( \frac{g(x, y_{1:i-1}) - \E_{(x',y') \sim \rhot}[g(x', y_{1:i-1}')]}{\beta} \right),
% \end{equation*}
%where $\odot$ represents the Hadamard product.
%[Proof in Appendix~\ref{apdx:proof}]
\end{proposition}
% \todo{Check and move proof to appendix}



To understand the effect of subtracting $\mathbf{b}_i$ from the logit, we limit our attention to the case of $i = 1$.
%, where we simply have $p_{\pi_\theta'}(y_i \mid x \oplus y_{1:i-1}) = \pi_\theta'(y_1 \mid x)$ and  $p_{\pi_r^*}(y_i \mid x \oplus y_{1:i-1}) = \pi_r^*(y_1 \mid x)$, and $g_\theta(x, y_{1:i}) = g_\theta(x, y_1)$ is the trained safety value of the first output token $y_1$ given the input prompt $x$. 
In light of Proposition~\ref{prop:logit}, subtracting $\mathbf{b}_1$ from the logit is interpreted as modifying the trained safety function from $g_\theta$ to $g_\theta'(x, y_1) = g_\theta(x, y_1) - \E_{x'\sim \rhot_x}[g_\theta(x', y_1)]$. Here, $\E_{x' \sim \rhot_x}[g_\theta(x', y_1)]$ is the expected safety value of the first token $y_1$ over random prompt $x' \sim \rhot_x$, where $\rhot = \rhot_x \times \rhot_y$. 
Therefore, nonzero $\E_{x' \sim \rhot_x}[g_\theta(x', y_1)]$ implies that a specific first token $y_1$ is preferred or dispreferred in expectation. 

We hypothesize that a nonzero $\E_{x' \sim \rhot_x}[g_\theta(x', y_1)]$ is an unintended side effect of the safety alignment. Ideally, safety alignment should align the policy's outputs for input prompts belonging to specific topics covered in the preference dataset $\mathcal{D}$. For an input prompt $x$ from topics outside this set, the policy's output should not be affected, as $\mathcal{D}$ contains no relevant information for such prompts. This requires to have $g_\theta(x, y_1) = 0$, implying $\E_{x' \sim \rhot_x}[g_\theta(x', y_1)] = 0$, provided that $\rhot_x$ is designed to have its support on input prompts that are not included in $\mathcal{D}$. A nonzero $\E_{x' \sim \rhot_x}[g_\theta(x', y_1)]$ is, therefore, expected as a result from overfitting to the preference dataset due to limited coverage of topics.

By utilizing $g_{\theta}'$ with an appropriate choice of $\rhot$, we can ensure that $\E_{x' \sim \rhot_x}[g_{\theta}'(x', y_1)] = 0$, thereby mitigating the aformentioned side effect.
Our design choice of $\rhot$ stems from the fact that a random prompt is almost always irrelevant to the topics in $\mathcal{D}$. We note, however, that alignment will break if we employ a $\rhot$ whose support significantly overlaps with the preference dataset $\mathcal{D}$.
%, especially if we employ the same dataset $\mathcal{D}$ to construct $\widetilde{\calD}$. 