\section{Conclusions}
This paper has demonstrated that the existing safety-alignment methods focused on a singular notion of safety, which often results in unrevealed vulnerabilities in specific safety categories. Our findings indicated that using smaller KL penalty parameters, more training, and dataset cleansing can improve safety but do not necessarily result in a better trade-off between safety and helpfulness. We also found that safety alignment can induce an undesired bias, in which the model tends to give negative or dismissive responses, regardless of the input context. To address this, we proposed \algoshort, which estimates the safety bias using random prompts and corrects it during the generation process. Our experiments demonstrated that \algoshort~improves the safety-helpfulness Pareto front, achieving high safety levels across all safety categories while preserving helpfulness.