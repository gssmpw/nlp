\subsection{Debiasing results for models trained with cleansed data}
\label{appendix:experiment-cleansed-data}
\begin{figure}[h]
    \centering
    \includegraphics[width=\hsize]{figure/beta_scatter_green_mdjudge_llamaguard_rejection.pdf}
    \caption{Trade-offs between adult-related safety score and the compliance rate to harmless prompts for models trained with the cleansed dataset.}
    \label{fig:trade-offs-compliance-cleansed}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\hsize]{figure/beta_scatter_green_mdjudge_llamaguard_helpful.pdf}
    \caption{Trade-offs between adult-related safety score and the helpful win rate versus SFT model for models trained with the cleansed dataset.}
    \label{fig:trade-offs-helpfulness-cleansed}
\end{figure}

Figure~\ref{fig:trade-offs-compliance-cleansed} and Figure~\ref{fig:trade-offs-helpfulness-cleansed} presents the debiasing experimental results for models trained with the cleansed safety dataset. As described in Section~\ref{sec:challenges-data-improvement}, we removed samples where the safety probability for the chosen response was significantly lower than that for the rejected one, i.e., \( s(x, y_l) - s(x, y_w) > c \). We set \( c = 0.25 \) for this purpose. Using the cleansed dataset, we conducted safety alignment and applied the same training and debiasing settings as in Section~\ref{sec:experiment}. The results were similar to those obtained when models were trained with the entire dataset. Particularly, \algoshort~ significantly improved the compliance rate without compromising safety across all iteration levels. Moreover, \algoshort~ also successfully enhanced helpfulness while maintaining a high level of safety, leading to an improved trade-off Pareto.

\clearpage