\section{Related Work and Preliminaries}
\label{sec:preliminary}

\subsection{Language Model Alignment}
Given a pre-trained LLM, alignment typically involves two stages~\citep{bai2022training, ouyang2022training, ziegler2019fine}: supervised fine-tuning (SFT) and learning from human feedback.
The SFT stage fine-tunes the model using high-quality human completions, generating $\pisft$, which improves token prediction accuracy for tasks like dialogue. Here, we review existing methods of the second stage, which aligns LLMs to human desiderata~\citep{christiano2017deep}.

\textbf{RLHF.\space}
The RLHF pipeline consists of reward modeling and RL fine-tuning. An LLM is a policy $\pi: \calX \rightarrow \calY$ that maps a prompt $x \in \calX$ to a response $y \in \calY$, where $\calX$ and $\calY$ are respectively the set of prompts and responses. Reward modeling uses a dataset $\calD = \{(x^{(i)}, y_w^{(i)}, y_l^{(i)})\}_{i=1}^N$ where $y_w$ and $y_l$ denote preferred and dispreferred outputs (i.e., $y_w \succ y_l$) for a prompt $x$ to train a reward model $r^\sharp$.
%
% \begin{equation}
%     \label{eq:bradley-terry-loss}
%     \Epref \left[- \log \sigma \bigl(r^\sharp(x, y_w) - r^\sharp(x, y_l) \bigr) \right],
% \end{equation}
% where $\sigma(\cdot)$ is the logistic function.
RL fine-tuning maximizes the reward while constraining the policy's divergence from the reference policy $\piref$:
%
\begin{equation}
    \mbE_{\rho, \pi_\theta} \left[\,r^\sharp \xy\,\right] - \beta \KL{\pi_\theta(\ymidx)}{\piref(\ymidx)},
    \label{eq:rlhf_obj}
\end{equation}
%
where $\rho$ is a distribution of prompts, and $\beta \in \mbR_+$ is a hyperparameter to tune the KL penalty's strength. 
Note that $\mbE_{\rho, \pi}[\cdot]$ is an abbreviation for $\mbE_{x \sim \rho, y \sim \pi(\cdot \mid x)}[\cdot]$ for any policy $\pi$.
Since \eqref{eq:rlhf_obj} is indifferentiable, RLHF uses such RL algorithms as PPO \citep{schulman2017proximal} to optimize it.

\textbf{DPO.\space}
%
RLHF is computationally expensive and unstable in practice, and many attempts have been made to overcome the issues.
A popular idea is to analytically derive the optimal policy of~\eqref{eq:rlhf_obj} parameterized by reward.
Speficially, for any reward function $r: \calX \times \calY \rightarrow \mbR$, the optimal policy $\piopt_{r}$ obtained by aligning $\piref$ with respect to $r$ satisfies
%
\begin{equation}
    \label{eq:op_policy}
    \piopt_{r}(\ymidx) \propto \piref(\ymidx) \exp\left(\frac{1}{\beta} \, r \xy \right).
\end{equation}

DPO~\citep{rafailov2023direct} applies reparametrization to a reward function $r$ using the parametrized policy $\pi_\theta$ and minimize a loss represented as 
%
\begin{align}
    \label{eq:loss_dpo}
    \calL_\text{DPO}(\pi_{\theta}, \piref, \beta)
    = - \Epref \left[\,\log \sigma \left(\beta \log \frac{\pi_{\theta}(y_w\mid x)}{\piref(y_w\mid x)} - \beta \log \frac{\pi_{\theta}(y_l\mid x)}{\piref(y_l\mid x)}\right)\,\right].
\end{align}
Extensions such as $\Psi$PO~\citep{azar2023general}, KTO~\citep{ethayarajh2024kto}, ORPO~\citep{hong2024reference}, and SimPO~\citep{meng2024simpo} further refine this approach.

\textbf{Constrained Language Model Alignment.\space}
%
Though all the aforementioned algorithms consider only a singular reward function, several algorithms incorporating constraints or multiple objectives have been recently proposed~\citep{zhou2023beyond,dai2024safe,zhong2024panacea,liu2024enhancing,wachi2024stepwise,huang2024one}. 

Safe RLHF~\citep{dai2024safe} introduces a safety function $g^\star$ and then formulates a problem of maximizing reward $r^\star$ under safety constraints as
%
\begin{align}
    \max_\theta \ \mbE_{\rho, \pi_\theta} [\,r^\sharp \xy\,] - \beta \KL{\pi_\theta(\ymidx)}{\piref(\ymidx)} \quad \text{subject to} \quad \mbE_{\rho, \pi_\theta} [\,g^\sharp \xy\,] \ge 0.
    \label{eq:saferlhf_obj}
\end{align}
%
They first train reward and safety models (i.e., $r^\sharp$ and $g^\sharp$) using two separate datasets with preferences about reward (i.e., helpfulness) and safety (i.e., harmlessness) and then optimize the policy using PPO Lagrangian~\citep{ray2019}.

Recently, \citet{wachi2024stepwise} have proposed an RL-free algorithm called SACPO for solving \eqref{eq:saferlhf_obj}. This algorithm is based on the finding that, under general reward and safety functions $r$ and $g$, the optimal policy of \eqref{eq:saferlhf_obj} satisfies 
%
\begin{align}
    \label{eq:op_policy_sacpo}
    \piopt(\ymidx)
    % &\propto \piref(\ymidx)
    % \exp \Big(\beta^{-1} \bigl(r \xy + \lambdaopt g \xy \bigr)\Big) \\
    &\propto \piopt_{r} (\ymidx)
    \exp \left(\frac{\lambda}{\beta} \cdot g \xy \right),
\end{align}
%
where $\lambda$ is the Lagrangian multiplier.
Recall that $\piopt_{r}$ is the optimal policy defined in \eqref{eq:op_policy}.
This equation justifies taking a stepwise approach where LLMs are aligned for reward and then for safety (or vice versa).
%
When safety alignment is executed via DPO for a reward-alined LLM $\piopt_r$, SACPO therefore uses the following loss function:
%
\begin{equation*}
    % \label{eq:loss_sacpo}
    \calL_\text{SACPO}\left(\pi_{\theta}, \piopt_r, \frac{\beta}{\lambdaopt}\right) = - \Epref \left[\,\log \sigma \left(\frac{\beta}{\lambdaopt} \log \frac{\pi_{\theta}(y_w\mid x)}{\piopt_r(y_w\mid x)} - \frac{\beta}{\lambdaopt} \log \frac{\pi_{\theta}(y_l\mid x)}{\piopt_r(y_l\mid x)}\right)\,\right].
\end{equation*}
Comparing the above loss function with \eqref{eq:loss_dpo}, the reference policy $\piref$ and KL penalty parameter $\beta$ are replaced with $\piopt_r$ and $\beta/\lambdaopt$, respectively.

\begin{figure*}[t]
    \centering
    \includegraphics[width=165mm,clip,trim=0 7 0 7]{figure/all_performances_combined_v2_mdjudge.pdf}
    \caption{(Left) Safety score for different safety categories evaluated by MD-Judge across different models. (Right) Trade-off between the mean safety score and the helpfulness win rate against the SFT model. Category 03 is the Adult Content category. The numerical scores and the names of other categories are shown in Appendix \ref{appendix:numerical-score}}
    \label{fig:salad_bench_result}
\end{figure*}

\subsection{Vulnerability of Safety-aligned Models}

Although significant efforts have been made for safety alignment, LLMs still potentially exhibit vulnerabilities in producing harmful generations~\citep{wei2024jailbroken, zou2023universal, yang2023shadow, yi2024vulnerability}. Prior research has shown that even if LLMs are trained to be safe and harmless, they can still be misused. Many studies have demonstrated that it is possible to conduct jailbreak attacks that provoke harmful behavior from an aligned LLM \citep{zou2023universal, liu2023autodan}. % \cite{wei2024jailbroken} discussed that these vulnerabilities arise due to the conflicting objectives between a model's capabilities and safety goals. These vulnerabilities also come from the generalization mismatch in which safety training does not cover all domains where the model has capabilities. 
Moreover, \citet{yang2023shadow} and \citet{yi2024vulnerability} have shown that fine-tuning with a few malicious examples can easily subvert the model and cause harmful content generation. 
This paper highlights vulnerabilities, in which a model, despite being safety-aligned and deemed safe overall, generates harmful responses on specific safety topics. Such vulnerabilities arise from insufficiency in the safety alignment and evaluation as well as inherent vulnerabilities of the reference model or quality of the dataset. 



\subsection{Safety Evaluators}

A widely adopted evaluation approach is to use advanced LLMs such as GPT-4~\citep{achiam2023gpt}.
For example, \citet{qi2023fine} and \citet{wang2023not} use GPT-4 to score the harmlessness of input-output pairs, while \citet{dai2024safe} and \citet{wachi2024stepwise} make GPT-4 choose the safer one between a pair of responses. Another line of work employs keyword-based methods~\citep{zou2023universal, wang2023decodingtrust}.
For example, \citet{zou2023universal} relies on detecting 47 predefined keywords like ``I'm sorry'' and ``As an.'' 
% While efficient, this method can produce false positives and negatives due to the presence or absence of keywords in responses. 
Finally, some studies proposed LLMs specialized in safety checks, which classify whether input-output pairs are safe and what safety categories have been violated. Notable instances are Llama Guards~\citep{inan2023llama, metallamaguard2,dubey2024llama3herdmodels} or MD-Judge introduced in SALAD-Bench ~\citep{li2024salad}.
% These works also differ in the adopted taxonomies for safety categories, with SALAD-Bench using a 3-level hierarchy of 66 categories and Llama Guard 3 using 13 hazard categories from the AI Safety taxonomy~\citep{vidgen2024introducing}. 
Safety evaluators serve multiple functions, primarily assessing the safety levels of trained LLMs and detecting harmful inputs and responses during deployment. Additionally, they are utilized in certain studies, such as \citet{dubey2024llama3herdmodels}, to evaluate and clean training datasets.


\subsection{Inference-time Alignment Methods}

\algoshort~can be interpreted as an inference-time alignment method \cite{liang2024controllable}, being represented by \citet{mudgal24a} or \citet{deng2023reward}.
As a notable example, \citet{mudgal24a} proposed a controlled decoding method to use a prefix scorer at inference time to improve generations of a frozen base model.
A key advantage of \algoshort~compared to \citet{mudgal24a} or \citet{deng2023reward} is that we do \textit{not} have to learn a separate reward model to control the generations at inference time.
Also, with access to embedding layers, \citet{zou2024improving} proposed circuit-breaking that directly circumvents the ability of LLMs to generate harmful texts.
Compared to this previous work, a major advantage of \algoshort~is that it can be used as long as we can access logits.
