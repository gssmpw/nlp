\section{Related Works}
\noindent\textbf{Multiple Instance Learning in WSI.}
Recently, MIL-based methods \cite{ilse2018attention, li2021dual, H2MIL, shao2021transmil, lu2021data, zheng2021deep, zhang2022dtfd, lu2023visual, huang2023conslide} have become the mainstream for processing the whole slide image in the field of computational pathology. The whole process mainly includes three steps: 1) a series of patches are cropped from the original WSI; 2) a pre-trained encoder is utilized to extract the patch features; 3) the patch features are aggregated to generate the final slide features. Conventional handcrafted patch feature aggregators include non-parametric max and mean pooling operators. Later, ABMIL \cite{ilse2018attention} proposes an attention-based aggregation function by learning the weight for each patch through a parameterized neural network. CLAM \cite{lu2021data} utilizes a pre-trained image encoder for patch feature extraction and proposes a multi-branch pooling operator trained for weakly-supervised WSI classification tasks. 
Based on CLAM, a series of methods \cite{shao2021transmil, zheng2021deep, zhang2022dtfd, lu2023visual, li2021dual} have been proposed to explore how to aggregate the patch features effectively. 
For example, DSMIL \cite{li2021dual} utilizes the multi-scale patch features as the input and aggregates it in a dual-stream architecture. 
TransMIL \cite{shao2021transmil} explores both morphological and spatial relations through several self-attention layers. 
GTMIL \cite{zheng2021deep} adopts a graph-based representation and vision Transformer for WSI. 
DTMIL \cite{zhang2022dtfd} utilizes a double-tier MIL framework by introducing the concept of pseudo-bags.
IBMIL \cite{lin2023interventional} achieves deconfounded bag-level prediction based on backdoor adjustment.
Although these methods have achieved great success in many pathological diagnostic tasks, they rely solely on bag-level labels for training, thus requiring the model to learn discriminative patterns from a large quantity of WSIs, without fully utilizing pathological prior knowledge as the guideline. 
In this work, our ViLa-MIL introduces the dual-scale visual descriptive text prompt as the language prior to guide the training of the model effectively. 

\noindent\textbf{Visual Language Models in WSI.}
The vision language models, like CLIP \cite{radford2021learning} and FLIP \cite{li2023scaling}, have shown remarkable performance in a wide variety of visual recognition tasks \cite{li2022blip, yu2023task, guo2023texts, khattak2023maple, lin2023clip, han20232vpt}. 
CLIP-based methods utilize a dual-tower structure, including an image encoder and a text encoder, which are pre-trained on a large number of web-source data by aligning the image-text pairs in the same feature space. 
Leveraging a class-name-replacement~(\textit{e.g.}, ``an image of \{class name\}."), CLIP classifies an image by matching it to the candidate text prompt of the highest similarity. 
In the field of pathology, several works~(\textit{e.g.}, MI-Zero \cite{lu2023visual} and PLIP \cite{huang2023visual}) have also been proposed by utilizing the CLIP model. 
For example, MI-Zero \cite{lu2023visual} aligns image and text models on pathological image patches by pre-training on 33k image-caption pairs. 
Later, PLIP \cite{huang2023visual} builds a large dataset of 208,414 pathological images paired with language descriptions and utilizes this dataset to fine-tune the CLIP model to the pathological field. 
QUILT \cite{ikezogwo2023quilt} also fine-tunes the CLIP model by creating over one million paired image-text samples based on YouTube and some public education resources.
Although these works \cite{lu2021data, huang2023visual, zhang2023text} have demonstrated significant classification performance and transferability in a new dataset, collecting a large number of image-text pairs is extremely time-consuming and labor-intensive. 
In the natural image field, several methods, such as CoOp \cite{zhou2022learning}, CLIP-Adapter \cite{gao2023clip} and TaskRes \cite{yu2023task}, have been proposed to transfer the CLIP model in a parameter-efficient fine-tuning way. However, these methods \cite{zhou2022learning, gao2023clip, yu2023task, guo2023texts} fail to take into account the hierarchical structure and large size of WSI, as well as the integration of the pathological knowledge.  
Recently, TOP \cite{qu2024rise} proposes a two-level prompt learning MIL framework, incorporating language prior knowledge. However, it still only utilizes the single-scale patches and the text prompt features are solely optimized based on the labels.
In this work, we utilize the frozen large language model to generate the dual-scale visual descriptive text prompt, which aids the model in transferring to the target dataset with the guidance of limited labeled data. Moreover, two lightweight and trainable decoders are proposed, with one for the image branch and the other for text, to transfer the VLM model to the field of pathology efficiently.

\begin{figure*}
\centering
\includegraphics[width=\linewidth]{Images/Framework.png}
\caption{
Pipeline of the proposed ViLa-MIL framework. 
The input of ViLa-MIL is a Question and WSI. 
The question is passed through a frozen large language model~(LLM) to generate the dual-scale visual descriptive text prompt. 
The prototype-guided patch decoder is introduced to progressively fuse the patch features into the slide features. 
The context-guided text decoder is introduced to refine the text features further by utilizing the multi-granular image contexts. 
}
\label{framework}
\vspace{-0.5cm}
\end{figure*}