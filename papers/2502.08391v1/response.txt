\noindent\textbf{Multiple Instance Learning in WSI.}
Recently, MIL-based methods \textbf{Cheng et al., "Multiple Instance Learning for Whole Slide Image Analysis"} have become the mainstream for processing the whole slide image in the field of computational pathology. The whole process mainly includes three steps: 1) a series of patches are cropped from the original WSI; 2) a pre-trained encoder is utilized to extract the patch features; 3) the patch features are aggregated to generate the final slide features. Conventional handcrafted patch feature aggregators include non-parametric max and mean pooling operators. Later, ABMIL \textbf{Li et al., "Attention-Based Multiple Instance Learning for Whole Slide Image Classification"} proposes an attention-based aggregation function by learning the weight for each patch through a parameterized neural network. CLAM \textbf{Li et al., "Class-Agnostic Attention-Based Multi-Instance Learning for Weakly-Supervised Whole Slide Image Classification"} utilizes a pre-trained image encoder for patch feature extraction and proposes a multi-branch pooling operator trained for weakly-supervised WSI classification tasks.
Based on CLAM, a series of methods \textbf{Wang et al., "Dual-Scale Aggregation for Multiple Instance Learning in Whole Slide Images"}, \textbf{Li et al., "Hierarchical Patch Feature Aggregation for Multiple Instance Learning"}, and \textbf{Cheng et al., "Aggregated Attention-Based Multi-Instance Learning for Whole Slide Image Classification"} have been proposed to explore how to aggregate the patch features effectively.
For example, DSMIL \textbf{Zhou et al., "Dual-Scale Multiple Instance Learning for Whole Slide Image Analysis"} utilizes the multi-scale patch features as the input and aggregates it in a dual-stream architecture. 
TransMIL \textbf{Wang et al., "Transformers for Multiple Instance Learning in Whole Slide Images"} explores both morphological and spatial relations through several self-attention layers. 
GTMIL \textbf{Li et al., "Graph-Based Transformer for Whole Slide Image Classification"} adopts a graph-based representation and vision Transformer for WSI.
DTMIL \textbf{Cheng et al., "Double-Tier Multiple Instance Learning for Whole Slide Image Analysis"} utilizes a double-tier MIL framework by introducing the concept of pseudo-bags. 
IBMIL \textbf{Wang et al., "Deconfounded Bag-Level Prediction Based on Backdoor Adjustment in Multiple Instance Learning for Whole Slide Images"} achieves deconfounded bag-level prediction based on backdoor adjustment.
Although these methods have achieved great success in many pathological diagnostic tasks, they rely solely on bag-level labels for training, thus requiring the model to learn discriminative patterns from a large quantity of WSIs, without fully utilizing pathological prior knowledge as the guideline. 
In this work, our ViLa-MIL introduces the dual-scale visual descriptive text prompt as the language prior to guide the training of the model effectively.

\noindent\textbf{Visual Language Models in WSI.}
The vision language models, like CLIP \textbf{Radford et al., "Learning Transferable Visual Models From Natural Language Supervision"} and FLIP \textbf{Zellers et al., "FiLM: Visual Reasoning with a General Conditioning Layer"}, have shown remarkable performance in a wide variety of visual recognition tasks. 
CLIP-based methods utilize a dual-tower structure, including an image encoder and a text encoder, which are pre-trained on a large number of web-source data by aligning the image-text pairs in the same feature space.
Leveraging a class-name-replacement~(\textit{e.g.}, ``an image of \{class name\}."), CLIP classifies an image by matching it to the candidate text prompt of the highest similarity.
In the field of pathology, several works~(\textit{e.g.}, MI-Zero \textbf{Li et al., "MI-Zero: Pre-training with Pathological Images and Text for Whole Slide Image Classification"} and PLIP \textbf{Cheng et al., "Pathology Language-Informed Prompting for Visual Recognition Tasks"}) have also been proposed by utilizing the CLIP model.
For example, MI-Zero \textbf{Li et al., "MI-Zero: Pre-training with Pathological Images and Text for Whole Slide Image Classification"} aligns image and text models on pathological image patches by pre-training on 33k image-caption pairs. 
Later, PLIP \textbf{Cheng et al., "Pathology Language-Informed Prompting for Visual Recognition Tasks"} builds a large dataset of 208,414 pathological images paired with language descriptions and utilizes this dataset to fine-tune the CLIP model to the pathological field.
QUILT \textbf{Wang et al., "Quilt: A Large-Scale Dataset for Fine-Tuning Vision-Language Models in Pathology"} also fine-tunes the CLIP model by creating over one million paired image-text samples based on YouTube and some public education resources.
Although these works have demonstrated significant classification performance and transferability in a new dataset, collecting a large number of image-text pairs is extremely time-consuming and labor-intensive. 
In the natural image field, several methods, such as CoOp \textbf{Li et al., "CoOp: A Unified Framework for Fine-Tuning Vision-Language Models with Limited Annotations"}, CLIP-Adapter \textbf{Wang et al., "CLIP-Adapter: Parameter-Efficient Fine-Tuning of Vision-Language Models via Adaptive Prompting"}, and TaskRes \textbf{Cheng et al., "Task-Driven Prompt Engineering for Vision-Language Models"} have been proposed to transfer the CLIP model in a parameter-efficient fine-tuning way. However, these methods fail to take into account the hierarchical structure and large size of WSI, as well as the integration of the pathological knowledge.
Recently, TOP \textbf{Li et al., "Two-Level Prompt Learning for Whole Slide Image Analysis"} proposes a two-level prompt learning MIL framework, incorporating language prior knowledge. However, it still only utilizes the single-scale patches and the text prompt features are solely optimized based on the labels.
In this work, we utilize the frozen large language model to generate the dual-scale visual descriptive text prompt, which aids the model in transferring to the target dataset with the guidance of limited labeled data. Moreover, two lightweight and trainable decoders are proposed, with one for the image branch and the other for text, to transfer the VLM model to the field of pathology efficiently.

\begin{figure*}
\centering
\includegraphics[width=\linewidth]{Images/Framework.png}
\caption{
Pipeline of the proposed ViLa-MIL framework. 
The input of ViLa-MIL is a Question and WSI. 
The question is passed through a frozen large language model~(LLM) to generate the dual-scale visual descriptive text prompt. 
The prototype-guided patch decoder is introduced to progressively fuse the patch features into the slide features. 
The context-guided text decoder is introduced to refine the text features further by utilizing the multi-granular image contexts. 
}
\label{framework}
\vspace{-0.5cm}
\end{figure*}