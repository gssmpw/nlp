% \section{Discussion and Future Work}

% %Onscad is insample data cause these LLMS have seen openscad

% The decision space of language design is enormous, so we had to make some decisions about what to explore in the language design of AIDL. In particular, we did not build a new constraint system from scratch and instead developed ours based on an open-source constraint solver. This limited the types of primitives we allow, e.g. ellipses are not currently supported. \jz{Additionally, rectangles in AIDL are constrained to be axis-aligned by default because we found that in most use cases, a rectangle being rotated by the solver was unintuitive, and we included a parameter in the language allows rectangles to be marked as rotatable. While this feature was included in the prompts to the LLM, it was never used by the model. We hope to explore prompt-engineering techniques to rectify this issue in the future. Similarly, we hope to reduce the frequency of solver errors by providing better prompts for explaining the available constraints.} \adriana{Add two other limitations to this paragraph: that we typically noticed that things are axis alignment, say why we use this as default and in the future could try to get the gpt to not use default more often. Mention that we still have Solver failures that could be addressed by better engineering in future. }

% In testing our front-end, we observed that repeated instances of feedback tends to reduce the complexity of models as the LLM would frequently address the errors by removing the offending entity. This leads to unnecessarily removed details. More extensive prompt-engineering could be employed in future work to encourage the LLM to more frequently modify, rather than remove, to fix these errors. \adriana{no idea what this paragraph is trying to say}


% \adriana{This seems  like a future work paragraph so maybe start by saying that in the future you could do other front end or fine tune a model with aidl, we just tested the few shot.  } \jz{In the future, we hope to improve our front-end generation pipeline by finetuning a pretrained LLM on example AIDL programs.} In addition, multi-modal vision-langauge model development has exploded in recent months. Visual modalities are an obvious fit for CAD modeling -- in fact, most procedural CAD models are produced in visual editors -- but we decided not to explore visual inputs yet based on reports ([PH] cite OPENAIs own GPT4V paper) that current vision-language models suffter from the same spatial reasoning issues as purely textual models do (identifying relative positions like above, left of, etc.). This also informed our decision to omit spline curves which are difficult to describe in natural language. This deficit is being addressed by the development of new spatial reasoning datasets ([PH] cite visual math reasoning paper), so allowing visual user input as well as visual feedback in future work with the next generation of models seems promising.

% The decision space of language design is enormous, so it was impossible to explore it all here. We had to make some decisions about what to explore, guided by experience, conjecture, technical limitations, and anecdotal experience. Since we primarily explore the interaction between language design and language models in order to overcome the shortcomings in the latter, we did not wish to focus effort on building new constraint systems. This led us to use an open-source constraint solver to build our solver off of. This limited the types of primitives we allow; in particular, most commercial geometric solvers also support ellipses.

% In testing our generation frontend, we observed that repeated instances of feedback tended to reduce the complexity of models as the LLM would frequently address the errors by removing the offending entity. This is a fine strategy for over-constrained systems, but can unnecessarily remove detail when done in response to a syntax or validation error. More extensive prompt-engineering could be employed to encourage the LLM to more frequently modify, rather than remove, to fix these errors


% In recent months, multi-modal vision-language model development has exploded. Visual modalities are an obvious fit for CAD modeling -- in fact, most procedural CAD models are produced in visual editors -- but we decided not to explore visual input yet based on reports (cite OpenAIs own GPT4V paper) that current vision-language models suffer from the same spatial reasoning issues as purely textual models do (identifying relative positions like above, left of, etc.). This also informed our decision to omit spline curves; they are not easily described in natural language. This deficit is being addressed by the development of new spatial reasoning datasets (cite visual math reasoning paper), so allowing visual user input as well as visual feedback in future work with the next generation of models seems promising.



\section{Conclusion}

AIDL is an experiment in a new way of building graphics systems for language models; what if, instead of tuning a model for a graphics system, we build a graphics system tailored for language models? By taking this approach, we are able to draw on the rich literature of programming languages, crafting a language that supports language-based dependency reasoning through semantically meaningful references, separation of concerns with a modular, hierarchical structure, and that compliments the shortcomings of LLMs with a solver assistance. Taking this neurosymbolic, procedural approach allows our system to tap into the general knowledge of LLMs as well as being more applicable to CAD by promoting precision, accuracy, and editability. Framing AI CAD generation as a language design problem is a complementary approach to model training and prompt engineering, and we are excited to see how advance in these fields will synergize with AIDL and its successors, especially as the capabilities of multi-modal vision-language models improve. AI-driven, procedural design coming to CAD, and AIDL provides a template for that future.

% Using procedural generation instead of direct geometric generation enables greater editability, accuracy, and precision
% Using a general language model allows for generalizability beyond existing CAD datasets and control via common language.
% Approaches code gen in LLMs through language design rather than training the model or constructing complexing querying algorithms. This could be a complimentary approach
% Embedding as a DSL in a popular language allows us to leverage the LLMs syntactic knowledge while exploiting our domain knowledge in the language design
% LLM-CAD languages should hierarchical, semantic, support constraints and dependencies




%In this paper, we proposed AIDL, a language designed specifically for LLM-driven CAD design. The AIDL language simultaneously supports 1) references to constructed geometry (\dgone{}), 2) geometric constraints between components (\dgtwo{}), 3) naturally named operators (\dgthree{}), and 4) first-class hierarchical design (\dgfour{}), while none of the existing languages supports all the above. These novel designs in AIDL allow users to tap into LLMs' knowledge about objects and their compositionalities and generate complex geometry in a hierarchical and constrained fashion. Specifically, the solver for AIDL supports iterative editing by the LLM by providing intermediate feedback, and remedies the LLM's weakness of providing explicit positions for geometries.

%\adriana{This seems  like a future work paragraph so maybe start by saying that in the future you could do other front end or fine tune a model with aidl, we just tested the few shot.  }
%\paragraph{Future work} In recent months, multi-modal vision-language model development has exploded. Visual modalities are an obvious fit for CAD modeling -- in fact, most procedural CAD models are produced in visual editors -- but we decided not to explore visual input yet based on reports (cite OpenAIs own GPT4V paper) that current vision-language models suffer from the same spatial reasoning issues as purely textual models do (identifying relative positions like above, left of, etc.). This also informed our decision to omit spline curves; they are not easily described in natural language. This deficit is being addressed by the development of new spatial reasoning datasets (cite visual math reasoning paper), so allowing visual user input as well as visual feedback in future work with the next generation of models seems promising. 