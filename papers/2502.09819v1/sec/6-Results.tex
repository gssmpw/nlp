
\section{Experiments}

% In this section, we first describe our front-end generative pipeline used to construct 2D CAD shapes in AIDL (\Cref{sec:code_generation}), then we show the results of LLM-based generation of 2D CAD shapes (\Cref{sec:main_results}). In addition, we compare AIDL with the OpenSCAD language on 2D CAD generation and ablate on the language design choices we have made (\Cref{sec:comparisons}).
\paragraph{Implementation}
For our experiments, we perform LLM-driven 2D CAD generations with AIDL. AIDL enables LLM-driven text-to-CAD through a front-end generation pipeline. The pipeline follows a common \textbf{validate-until-correct} pattern. 
We first prompt the LLM with a detailed language description of AIDL, which includes AIDL's syntax, primitive geometry types, and available constraints. Then the LLM is prompted with six manually designed example programs in AIDL for these objects: bottle opener, ruler, hanger, key, toy sword, and wrench. Please refer to the supplemental material for the full list of prompts. Finally, it is prompted to generated the full AIDL program of the desired model. The front-end then executes the generated program, returning tracebacks directly to the LLM in case of failure and prompting the LLM to fix the error. This generation loop is repeated until either a syntactically correct program is found or after $N=5$ failed attempts, taking advantage of incomplete executability to give feedback on partial generations. 
%We manually generated thirty-six test prompts, see the supplemental for the full list. 
For all our experiments, we use the OpenAI's gpt-4o model without finetuning, and we run each prompt ten times with different seeds and collect the runs that generated a valid program.

%For comparison, we perform 2D text-to-CAD with the OpenSCAD language, the most common option for directly coding geometries in CAD, unlike other languages that are typically used with UIs for end-user programming. Additionally, we ablate our language design by comparing AIDL against two variants: $\text{AIDL}{\text{no hierarchy}}$ and $\text{AIDL}{\text{no constraints}}$, which disable hierarchy or constraints, respectively. 
%We report CLIP scores for the renderings of the generated programs across all four approaches in \Cref{tab:clip}. 



%In addition, we conduct a perceptual study where, for each method, we show all the renderings of valid CAD programs for a particular prompt and ask users to select the best one. Due to the high number of prompts, the study is split into four blocks (one for each method) and we show the user one out of four blocks randomly. We collected $32$ responses in total ($8$ on average per method) and we show the aggregated results in\cref{app:user study results}.


\paragraph{Results}
We report both the rendering and program of all runs of on 36 manually generated prompts in the supplemental material. In \Cref{fig:Main result}, we show renderings for a diverse subset of the generated AIDL programs. Despite the LLM not being finetuned with our AIDL language, it successfully generates accurate CAD geometry based on its prior knowledge of these objects. Furthermore, the geometries are grouped hierarchically by semantically meaningful structures and constraints, making them easy to edit. See \cref{app:editability} for an illustration of how an AIDL model can be modified. 
%The CLIP scores in \cref{tab:clip} shows that objects generated with AIDL is visually closer to the user prompt compared to other languages. \adriana{I'm not sure we can say this}


\begin{figure}[htbp!]
  \centering
  \includegraphics[width=\linewidth]{images/main_gallery_linefix.png}
  \caption{\textbf{A sample of LLM-guided 2D CAD generations using AIDL.} An untuned general purpose LLM is able to generate a diverse range of objects with accuracy after being prompted by the AIDL language syntax and a few example programs.}
  \label{fig:Main result}
\end{figure}

\paragraph{Comparisons} For comparison, we perform 2D text-to-CAD with the OpenSCAD language, the most common language for directly coding geometries in CAD, unlike other languages that are typically used with GUIs for end-user programming. We directly prompt the LLM to generated CAD geometry in the OpenSCAD language since the gpt-4o model has prior knowledge about its syntax. We used the same 36 prompts and report all results in the supplemental material. Despite the LLMâ€™s familiarity with OpenSCAD, we observe that AIDL results are often closer to the prompt and achieve higher CLIP scores (see \Cref{tab:clip}). In addition to better prompt alignment, AIDL results exhibit more semantic structure.  
In particular, the OpenSCAD language does not support specifying relationships or dependencies between components, thus the LLM would often opt to generate polygons of the desired shape by specifying explicitly the vertex coordinates (see \Cref{fig:Comparison}), making the resulting program highly difficult to edit. 

We also attempted using FeatureScript and the DSL from the recent work~\cite{cascaval2023lineage} for LLM-drive 2D CAD generations. However, the LLM failed to generate syntactically correct programs in almost all cases. This issue was not rectified even when prompting the LLM with example programs and code documentations in those languages. These two languages are not syntactically based on common programming languages usually found in LLM training sets. This indicates the importance of designing a semantically rich language that is easy for the LLM to use and manipulate. 

\paragraph{Ablations}We ablate our language design choices by comparing AIDL  against two variants: $\text{AIDL}_{\text{no hierarchy}}$ and $\text{AIDL}_{\text{no constraints}}$, which disable hierarchy and constraints respectively. In $\text{AIDL}_{\text{no hierarchy}}$, all the geometries of a program will live on the same level under a single Structure instance, and all constraints will also be attributed to this single Structure. On the other hand, $\text{AIDL}_{\text{no constraints}}$ is a subset of AIDL where we have simply removed the ability to specify any constraints. For these language subsets, we modify our initial prompts to the LLM to reflect the altered language features. We report all runs on the same 36 prompts in the supplemental material.

%We observe that programs generated with   $\text{AIDL}_{\text{no constraints}}$ will more often exhibit disconnected components that should have been attached, as illustrated in~\Cref{fig:Comparison}. This makes editing the program difficult since simple scaling will require different changes be made to all the components, where having constraints allow a single edit to affect all constrained geometries. More results generated with $\text{AIDL}_{\text{no constraints}}$ in the supplemental show that, while the programs may occasionally have the correct placement, they more often exhibit detached components due to lack of constraints (e.g. results with prompt "barn on a farm" and "lighthouse") \adriana{say that this is particularly the case for models where multiple parts need to be aligned for example in fountain pen.}.  \jz{However, this does not seem to adversely affect CLIP scores for renderings of shapes generated with $\text{AIDL}_\text{no constraints}$ (\Cref{tab:clip}), due to the fact that CLIP scores placing more emphasis on local semantics than having precisely connected geometries.} 
While $\text{AIDL}_{\text{no constraints}}$ occasionally places components correctly, editing such programs is difficult because scaling requires individual adjustments for each component, whereas constraints allow a single edit to affect all geometries. Additionally, it often produces detached components due to the lack of constraints (see \Cref{fig:Comparison} and the ``fountain pen" example in the supplemental material).


%No hierarchy
Programs generated with $\text{AIDL}_{\text{no hierarchy}}$, while being visually similar to the ones generated with $\text{AIDL}$, are harder to refine, since the user cannot choose a particular part of the CAD shape to make local edits, as shown in \Cref{fig:Comparison}.

We observe that neither variation of AIDL significantly impacts CLIP scores for the renderings (\Cref{tab:clip}), because that CLIP scores do not take into account editability and they place more emphasis on local semantics than having precisely connected geometries.

%Programs generated with $\text{AIDL}_{\text{no hierarchy}}$, while being visually simlar to the ones generated with $\text{AIDL}$, are harder to refine, since the user cannot choose a particular part of the CAD shape to make local edits. \adriana{I would move the clip score discussion to the beginning of the section..}

\paragraph{Results Across Multiple Runs} All methods produced at least one valid output per prompt, with success rates as follows: ours: 64\%, $\text{AIDL}_{\text{no constraints}}$: 94\%, $\text{AIDL}_{\text{no hierarchy}}$: 77\%, and OpenSCAD: 79\%. Notably, our method's success rate is only slightly lower than OpenSCAD, which is included in the training data. To showcase the highest-quality output for each method side by side, considering that LLMs produce varying outputs across runs, we conducted a perceptual study to rank the valid CAD programs generated from the 10 runs per method and prompt. The study details are discussed in \cref{app:study}, and the results are provided in the supplemental material.
%\adriana{or are they going back to the appendix?}

\paragraph{Limitations}
Our experiments revealed limitations of our system, particularly around model complexity and underused language features. AIDL supports rectangle rotation, yet all rectangles used in generated examples are axis-aligned. Looking at the generated code and conversation history (see supplemental) shows that the LLM did frequently specify that rectangles were rotatable (a flag in the Rectangle constructor), but failed to rotate them. One shortcoming of the AIDL library is that rectangles can only be rotated by the constraint solver, so an appropriate constraint (usually \verb|Angle|) must be imposed to cause a rectangle to rotate. In cases where the LLM attempted to do this, it hallucinated a non-existent constraint like \verb|Rotate| instead. When errors are reported to the LLM, the most common response is to try removing constraints or structures until the error goes away. Since we apply a validate-until-correct pattern, this means that the removed design intent (e.g. rectangle rotation) is never returned to the model. These limitations stem from our choice to focus on DSL design rather than the complementary approaches of model training or tuning, or prompt engineering. Fine-tuning a model on AIDL code could reduce the incidence of language feature hallucination, and crafting a more interactive prompting and feedback system could allow an LLM to recover lost complexity and design intent in the face of errors.

%The primary limitations of our system lie in the interaction with the LLM. Several features of AIDL were rarely or never utilized in our experiments, despite being present in the in-context examples. Rectangles in AIDL are axis-aligned by default because we noticed, anecdotally, that rectangles in most designs are axis aligned, and found rotating a rectangle to satisfy a constraint to be an unintuitive solution. To address this AIDL provides an optional boolean flag on Rectangle construction that removes the axis alignment constraint, however this flag was only used in one in-context example and does not appear in any of the generated results. Boolean operations in AIDL are defined implicitly by the type of each structure: Solid, Hole, Drawing, or Assembly. While most of the in-context examples used a variety of structure types, Solids were over-represented, with Drawings being scarce and no Assembly examples given. The LLM underutilized non-Solid structures, such as in the ``old-school telephone'' example in \Cref{fig:teaser}, where the base \verb|telephone| structure would be best represented as an Assembly, and \verb|dial_plate.holes| would be better expressed as a Hole-type substructure. In addition to un-and-underused language features, our prompting strategy's termination condition encouraged the LLM to produce less complex models. When confronted with an error such as an unsolvable constraint, the LLM most commonly reduced the complexity of the model until no error was found. Because we stop generation as soon as a valid model is generated, the LLM was not given an opportunity to re-introduce complexity into the model. In the future we would address these issues by curating a more robust set of in-context examples and by crafting a more interactive feedback loop with finer-grained error messages. \adriana{we could also fine tune the LLM to work with aidl}

%As a remark, the decision space of language design is enormous, so we had to make some decisions about what to explore in the language design of AIDL. In particular, we did not build a new constraint system from scratch and instead developed ours based on an open-source constraint solver. This limited the types of primitives we allow, e.g. ellipses are not currently supported. Additionally, rectangles in AIDL are constrained to be axis-aligned by default because we found that in most use cases, a rectangle being rotated by the solver was unintuitive, and we included a parameter in the language allows rectangles to be marked as rotatable. While this feature was included in the prompts to the LLM, it was never used by the model. Furthermore, in testing our front-end, we observed that repeated instances of feedback tends to reduce the complexity of models as the LLM would frequently address the errors by removing the offending entity. This leads to unnecessarily removed details. More extensive prompt-engineering could be employed in future work to encourage the LLM to more frequently modify, rather than remove, to fix these errors. \adriana{no idea what this paragraph is trying to say}

% What this is trying to say:
% - smaller set of primitives than full-fat CAD systems have
% - ***LLM did not pick up on some language features
%   - Doesn't always use booleans when it should
%   - Doesn't use the axis-alignment flag
% - *** LLM will react to solver failures by reducing model complexity, and does not try to add it back in.
% Limitations on the solver

% \subsection{Front End}
% \label{sec:code_generation}

% % \begin{figure}
% %     \centering
% %     \includegraphics[scale=0.3]{images/pipeline.png}
% %     \caption{\textbf{LLM Generation Pipeline.} An initial text prompt is given to the LLM along with instructions to generate a hierarchical AIDL code. Guided by Solver feedback, the LLM then iterates to add compositional constraints between these structures until a feasible code is found. 
% %     % From these we generate a template AIDL program, and direct the solver to add to it, in a bottom-up fashion, to add first geometry and local constraints, followed by global constraints. Solver feedback is used to iterate until correctness at each of these stages as well.
% %     }
% %     \label{fig:frontend}
% % \end{figure}
% AIDL enables LLM-driven text-to-CAD through a front-end generation pipeline. The pipeline follows a common \textbf{validate-until-correct} pattern. 
% We first prompt the LLM with a detailed language description of AIDL, which includes AIDL's syntax, primitive geometry types, and available constraints. Then the LLM is prompted with a few example AIDL programs. Finally, it is prompted to generated the full AIDL program of the desired prompt. The front-end then executes the generated program, returning tracebacks directly to the LLM in case of failure and prompting the LLM to fix the error. This generation loop is repeated until either a syntactically correct program is found or after $N=5$ failed attempts, taking advantage of incomplete executability to give feedback on partial generations. For all our experiments, we use the OpenAI's gpt-4o model without finetuning.


% \subsection{Text-to-CAD Results}
% \label{sec:main_results}
% We show results of LLM-guided 2D CAD generation with AIDL. 
% To generate 2D CAD models with AIDL, \jz{the LLM is prompted with a detailed language description and six manually designed example programs in AIDL for these objects: bottle opener, ruler, hanger, key, toy sword, and wrench. }\adriana{how many?}
% In \Cref{fig:Main result}, we show renderings for a diverse subset of the generated AIDL programs. \adriana{point to supplemental where you have all the rest and say you run 10 times and they are all there (the ones that worked}. Despite the LLM not being finetuned with our AIDL language, it is still able to generate accurate CAD geometry with the LLM's prior knowledge of these objects. 


% \adriana{report how many didn't work - how many runs don't actually get you a result - either max out or don't render. }


% \subsection{Comparison and Ablation}
% \label{sec:comparisons}
% \jz{We compare our language on LLM-driven CAD generation with OpenSCAD, a CSG CAD language. To generate CAD in OpenSCAD, we directly prompt the LLM to generated CAD geometry in the OpenSCAD language since the gpt-4o model has prior knowledge about its syntax. We also ablate our language design choices by comparing AIDL to $\text{AIDL}_{\text{no hierarchy}}$ and $\text{AIDL}_{\text{no constraints}}$. Spefically, $\text{AIDL}_{\text{no hierarchy}}$ is a subset of the AIDL language where hierarchy is removed from the design. Namely, all the geometries of a program will live on the same level under a single Structure instance, and all constraints will also be attributed to this single Structure. On the other hand, $\text{AIDL}_{\text{no constraints}}$ is a subset of AIDL where we have simply removed the ability to specify any constraints. For these language subsets, we modify our initial prompts to the LLM to reflect the altered language features.

% We report CLIP scores for the renderings of the generated programs across all four methods in \Cref{tab:clip}. In addition, we conduct a perceptual study where, for each method,  we show all the renderings of valid CAD programs for a particular prompt and ask users to select the best one. Due to the high number of prompts, the study is split into four blocks (one for each method) and we show the user one out of four blocks randomly. We collected 32 responses in total (8 on average per method) and we show the aggregated results in \cref{app:user study results}.
% } 

% \adriana{I would add a paragraph here describing the study and pointing to the results. you can introduce what the comparison and the 2 ablations are, say that you run them all 10 times and you ask users to get the best and show the result in the figure.  }

% To generate CAD in OpenSCAD, we directly prompt the LLM to generated CAD geometry in the OpenSCAD language since the gpt-4o model has prior knowledge about its syntax. 
% 
% \Cref{fig:Comparison} shows a comparison between geometries generated with AIDL and with OpenSCAD.
% In particular, the OpenSCAD language doesn't support specifying relationships or dependencies between components, thus the LLM would often opt to generate polygons of the desired shape by specifying explicitly the vertex coordinates, making the resulting program highly difficult to edit. \adriana{Would it be good to also point to the results of the study in the appendix here?  and say that even though gpt doesn not know our language in training the generated results often look better? and then mention  that beyond visual appearance the programs we generate are more editable. and say what you had before }

% \label{sec:ablations}
% To highlight the benefits of our language design choices, we ablate AIDL by restricting it to certain subsets of functionalities. With the same LLM-based CAD generation task, we experiment with $\text{AIDL}_{\text{no hierarchy}}$ and $\text{AIDL}_{\text{no constraints}}$. 

% Spefically, $\text{AIDL}_{\text{no hierarchy}}$ is a subset of the AIDL language where hierarchy is removed from the design. Namely, all the geometries of a program will live on the same level under a single Structure instance, and all constraints will also be attributed to this single Structure. On the other hand, $\text{AIDL}_{\text{no constraints}}$ is a subset of AIDL where we have simply removed the ability to specify any constraints. For these language subsets, we modify our initial prompts to the LLM to reflect the altered language features. 
% We show the generated results in \Cref{fig:Comparison}. In particular, programs generated with   $\text{AIDL}_{\text{no constraints}}$ will more often exhibit disconnected components that should have been attached. This makes editing the program difficult since simple scaling will require different changes be made to all the components, where having constraints allow a single edit to affect all constrained geometries. Programs generated with $\text{AIDL}_{\text{no hierarchy}}$, while being visually simlar to the ones generated with $\text{AIDL}$, are harder to refine, since the user cannot choose a particular part of the CAD shape to make local edits. \adriana{I would move the clip score discussion to the beginning of the section..}

% Finally, we conducted a perceptual study where the users are shown all generated examples for a particular prompt and method and are instructed to pick the one that most resembles the prompt. We collected $32$ responses and we show the aggregated the results in \cref{app:user study results}. \adriana{32 responded feels misleading here. I would move this up to the beginning of the section(see comment above).}



\begin{figure}[htbp!]
  \centering
  \includegraphics[width=\linewidth]{images/all_comparison_3.png}
  \caption{\textbf{Comparison and Ablation.} For the task of text-to-CAD, we compare our language to OpenSCAD and ablate on our language design choices. (\textbf{Top Left}) In particular, generated OpenSCAD programs exihibit manually drawn polygons with explicit vertex positions which are difficult to edit. (\textbf{Bottom Left}) Programs generated with $\textbf{AIDL}_{\text{no constraints}}$ has detached parts due to not being able to constrain the relative positions of part geometries. (\textbf{Right}) When an AIDL model is created with a structure hierarchy it is easier to locally edit because of modular substructures (left), while a similar edit on a non-hierarchical model (right) results in the model breaking (the dial moves without the dial holes). Performing the same edit in a non-hierarchical model requires multiple, non-concurrent edits.}
  \label{fig:Comparison}
\end{figure}

% \begin{figure}[htbp]
%   \centering
%   \fbox{\parbox[c][0.3\linewidth][c]{0.8\linewidth}{\centering Ablation result PH}}
%   \caption{Ablation result ph}
%   \label{fig:Ablation result ph}
% \end{figure}


% ours avg: 27.861057512524784
% nc avg: 27.661548950525162
% nh avg: 27.75429911960118
% openscad avg: 26.510543021170914
% ours std: 2.157481509541088
% nc std: 1.9855580138660853
% nh std: 2.006689119383239
% openscad std: 1.752424687473368


\begin{table}[ht]

\centering
\caption{\textbf{Average CLIP scores for all prompts.} We perform text-to-CAD generation with $\textbf{AIDL}$, $\textbf{AIDL}_{\text{no hierarchy}}$, $\textbf{AIDL}_{\text{no constraints}}$, and \textbf{OpenSCAD} on our list of prompts for ten iterations each and show the average CLIP scores over the ones that produced valid programs.}
\label{tab:clip}
\begin{tabular}{lcccc}
\toprule
 & \textbf{AIDL} &$\textbf{AIDL}_{\text{no hierarchy}}$ & $\textbf{AIDL}_{\text{no constraints}}$ & \textbf{OpenSCAD} \\ 
\midrule
\textbf{$\uparrow$ CLIP Score Avg.}              & \textbf{28.90} & 28.64 & 28.89 &27.32 \\ 
\textbf{$\ \ $ CLIP Score Var.}              & 2.24& 1.98 & 2.05 & 1.87 \\ 
\bottomrule
\end{tabular}
\end{table}

% What we need to show:

% Basic Language Usage - hand-crafted examples showcase language features (could be in language section / implementation)
% \begin{itemize}
%     \item clapboard - structure, geometry constraints, expression constraints
%     \item ruler - compositional constraints, functional requirements
%     \item comb - spacing, linear pattern, union
%     \item bottle opener - compositional constraint, alignment, boolean difference
%     \item sundial - equational reasoning, basic 3D, functional requirements - (maybe bottom of results)
%     \item chair - good illustration of compositional constraints defining class - (maybe bottom of results)
% \end{itemize}

% The Language Can Support complex designs - show 1-3 complex designs (hand-crafted, LLM crafted if possible)

% The LLM frontend works:
% \begin{itemize}
%     \item Selected examples with discussion
%     \item Quantitative metrics about large-scale experiment
%     \begin{itemize}
%         \item How many valid programs?
%         \item How many ``look correct''
%         \item How complex are the generated examples
%     \end{itemize}
% \end{itemize}

% Insights about frontend:
% \begin{itemize}
%     \item Stats on frequency of types of feedback
%     \item Example failure cases (could be in limitations section)
% \end{itemize}

% Validate Design Decisions -- Could align to section 4 instead (key ``constructs'' / design decisions)
% \begin{itemize}
%     \item Indirect specification of geometry - covered by language comparison
%     \item Indirect specification of geometric relationships - covered by language comparison
%     \item Intuitively named operators - no sugaring, just write constraint equations
%     \item Hierarchical Design - no hierarchical solve
%     \item Partial Evaluation - no feedback loop
% \end{itemize}

% Front End Ablations - no feedback loop

% Comparisons to Existing Languages - line up to basic kinds of languages:
% \begin{itemize}
%     \item Constraint Based - SolveSpace (bare)
%     \item CSG - Open SCAD
%     \item Reference Based - CAD Query or Dan's language
% \end{itemize}
% Faster Alternative: Base on Table 1, ablate our language on these instead




\iffalse
\subsection{Examples}

We created \data{numExamples} examples. Testing loading \data{foo}. What about math mode?
\[
x + \data{bar}
\]

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/ours.pdf}
    \caption{Success rate at each pipeline stage. Each prompt with tried with 10 random seeds.}
    \label{fig:ours-success}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/complexity.pdf}
    \caption{Complexity of solutions under various ablations, shows that adding our stuff allows for more complex models to be generated}
    \label{fig:ablation-complexity}
\end{figure}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{images/results-mock.png}
    \caption{Fabricated results generated with GPT.}
    \label{fig:fabricated}
\end{figure*}

\begin{table}[h]
    \centering
    \begin{tabular}{c|c}
        A & B \\
        C & D
    \end{tabular}
    \caption{Ablation results. TODO - import from csv}
    \label{tab:ablations}
\end{table}

% Subjective Failure Examples
\begin{wrapfigure}{r}{0.25\textwidth}
    \centering
    \includegraphics[width=0.25\textwidth]{example-image-c}
\end{wrapfigure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{example-image-a}
    \caption{Removing hierarchy and compositional constraints causes these failures in spatial reasoning.}
    \label{fig:ablation-failures}
\end{figure}


\paragraph{Fabricated Examples}
\langname{} currently supports compositions sketch-and-extrude solids and draw sketches, which we realized using laser cutting. Using GPT-4-Turbo as our LM, we produced 5 physical models:

\begin{itemize}
    \item ruler: shows code-reuse (two scales), necessity of using a DSL over just structure (looping over markings), mixing solids and drawings
    \item comb: shows-off substructure for solids (teeth), and the use of layout constraints to align them
    \item sundial: shows-off 3D from 2D, cross-level constraints, mixing solids and drawings, very-much needs the DSL support to compute the correct marking positions and shows that the language models know about and can provide these functional requirements
    \item glasses: repeated substructures (earpieces), everything is symmetric
    \item table: simple model really only shows off the inside and on-top-of layout constraints, is here to demonstrate utility beyond just laser-cutting since we could realize by cutting thicker stock by another means
    \item box: Solver handling 3D reasoning (in front, behind, on top, perpendicular) so that language doesn't have to
\end{itemize}

\subsection{Ablations}

The design decisions come in many categories:
\begin{itemize}
    \item Arbitrary decisions because I had to choose something. These will be mentioned in the future work section.
    \item Design decisions made for implementation convenience / technical limitations. Specific benefits / limitations of these will be left for the future work section.
    \item Principled decisions made based on conjecture or experience, but without experimental backing
    \item Principled decision based on anecdotal observations and ad-hoc experiments. We could attempt to make rigorous ablations for each of these, or to construct single illustrative examples, but there is likely diminishing returns to that. These reasons and justifications will only exist in the methods / system design section
    \item Principled decisions based on evidence from related work
    \item Principled decisions that we ablate on. These will be mentioned when the design choice is specified in methods / system design, along with a reference to the ablation section where it is re-emphasized and demonstrated. This takes effort to do and paper space, so the most important decisions should be prioritized.
\end{itemize}

\paragraph{Language Design Ablations}

Design decisions to justify:

hierarchical structure:
Claimed Benefits:
 - enables LLM to find more complex designs
   - experiment: Run same prompts with flat versus hierarchical version of language, show visually the differences, measure how frequently it sucseeds at fixing from different seeds
 - assists in targeting feedback from solver
   - Break a flat vs. structured program in the same way
 - allows for precise scoping while editing
 - allows for 1:1 named references - e.g. click on a part and can see exactly what it's called in the program. E.g. we can do this like point to a substructer and say "make this taller" or "expose a parameter to adjust this height"
    - Experiment: handmake two programs, give to GPT-4, and ask for the same modification and for the same exposed parameter. Qualitatively compare and run N times to see how often it succeeds / fails

Experiments:
- Run with just the basic geometry and geometric helpers (this allows layout constraints to be used)
- To be fair, we should still include the natural-language reasoning prompt to illicit reasoning about substructure in the model context before the program is written
- Expectation: it naturally create some sub-structure in the code, but will fail in relative substructure positioning (e.g. frequently putting the scale outside of the ruler body, etc.). It may also be difficult to get it to correct individual parts of the program without modifying other pieces

Explicitly naming structures:
- assists in targetting feedback from the solver
  -  break programs with and without semantic names identically then try and repair, measure how frequently it can from different seeds

solver-aided design:
- exp 1: Just primitives, no constraints. Still create the subtrees, but just place objects in space within each subtree. The only exception is to allow ``perpendicular'' as a layout constraint to disentangle objects since we don't explicitly set workplanes and rely on constraints for relative 3D positioning
   - Is able to get small-scale details correct, but cannot get large scale details working
- exp 2: just low-level constraints, no high level. Can do some connectivity, but large-scale planning isn't working
- exp 3: only high-level constraints (no geometric CAD constraints). Connectivity between things (like comb-tooth corners) does not work, relative scales are wonky because when we need to resolve high-level, the low level shapes get stretched and skewed

DSL vs data-structure generation:
The claimed benefits of using a python-based DSL are

Modular Code reuse: comb teeth, marker scales, glasses earpieces
Calculated dimensions: sundial hour marks
Take advantage of lots of python pre-training: does DSL generation have a higher validation success rate or take fewer feedback iterations to get correct (valid)?


Prompt with the IR representation as json. Optionally use json-mode output from GPT4-turbo to guarantee well-formatted JSON output. Expect that repeated details are missed or have less detail (the ruler marks), and that it won't be able to get the sundial's marks correct at all because those require solving trigonometric functions.

\paragraph{Code Generation Ablations}

Pre-code natural-language reasoning: expect that we'll see less complex structures without it.

Multi-prompt vs all-at-once generation: after pre-code reasoning prompt, just ask it to generate the whole codebase at once

Few-Shot Learning:
 - providing API code
 - providing example programs
 - providing example interaction


\fi