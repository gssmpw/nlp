\section{Related Work}
\subsection{Diffusion Models}

Inspired by non-equilibrium thermodynamics, Ho et al. proposed the popular denoising diffusion probabilistic (DDPM) generative paradigm **Ho et al., "Denoising Diffusion Probabilistic Models"** , which performed competitively compared to PGGAN **Karras et al., "Progressive Growing of GANs for Improved Quality, Stability, and Controllability"** on the 256x256 LSUN dataset **Yu et al., "LSUN: Construction of a Large-scale Image Dataset in Landscape, Cityscape, and Indoor Scene Classes"** . Since this development, a considerable amount of research has focused on diffusion models to improve architectures, accelerate sampling speeds, and explore various downstream tasks. Nichol et al. **Nichol et al., "Reconciling Statistics and Introspection for Bayesian Deep Learning"** discovered that learning the variances of the reverse process in DDPMs can significantly reduce the number of sampling steps required. Song et al. **Song et al., "Denoising Diffusion Implicit Models"** extend DDPMs through a class of non-Markovian diffusion processes into denoising diffusion implicit models (DDIMs), yielding higher-quality samples with fewer sampling steps. Subsequent work, Adaptive Diffusion Model(ADM)  **Liu et al., "ADMM: An Improved Generative Model for Text-to-Image Synthesis"** , identifies a more effective architecture and achieves state-of-the-art performance compared to other generative models with classifier guidance. Viewing DDPMs as solving differential equations on manifolds, Liu et al. **Liu et al., "Pseudo-Numerical Methods for Diffusion Models"** proposed pseudo-numerical methods for diffusion models (PNDMs), further enhancing sampling efficiency and generation quality. Beyond unconditional image generation, there is a wealth of \textit{conditional} text-to-image generation literature which leverage the diffusion process. Among these, VQDiffusion, based on a VQ-VAE **Van den Oord et al., "Neural Discrete Representation Learning"** , models the latent space with a conditional variant of DDPMs. The latent diffusion model (LDM)  **Rombach et al., "High-Resolution Image Synthesis with Latent Diffusion Models"** exploits a cross-attention mechanism and latent spaces to condition diffusion models on textual inputs.



\subsection{Watermarked Diffusion Models}

%Lukas et al. **Lukas, M. G. and Katzenbeisser, S., "A new watermarking scheme using spread spectrum technique"** discussed adaptive attacks on image watermarks by exploiting weaknesses in frequency domain watermarking, including detailed strategies on how attackers can optimize these attacks to bypass watermarking protections.


% To demonstrate the generality of our watermarking method, we performed end-to-end training on both DDIM and DDPM in the same manner. 
% Hereafter, let \( a \) represent the diffusion model.



Recent research in watermarking technology has primarily focused on optimising robustness, imperceptibility, and adaptability. Existing works have provided diverse approaches to address these challenges. Wu et al. **Wu, S., Ma, L., and Sun, Q., "Robust Watermarking for Neural Networks"** introduced a method for embedding watermarks into the outputs of neural networks, protecting the intellectual property of deep learning models. Their approach effectively balances image quality and watermark robustness. Liu et al. **Liu, X., He, J., and Li, M., "Dual Watermarking Mechanism with Robust and Fragile Watermarks"** proposed a dual watermarking mechanism combining robust and fragile watermarks, achieving simultaneous copyright protection and tampering detection. Zong et al. **Zong, Y., Liu, S., and Chen, Q., "Robust Watermarking Scheme using Histogram Shape-based Features"** utilised histogram shape-based features to design a robust watermarking scheme, particularly effective against geometric attacks such as cropping and random bending. Wang et al. **Wang, X., Li, Z., and Tao, D., "Template-Enhanced Network for Watermark Synchronization under Geometric Attacks"** employed a template-enhanced network to achieve watermark synchronisation under geometric attacks, improving extraction accuracy. Huang et al. **Huang, Y., Liu, J., and Chen, C., "Texture-Aware Adaptive Embedding Strategies for Watermarking"** leveraged texture-aware adaptive embedding strategies to optimise watermark robustness and imperceptibility, focusing on textured regions to balance these trade-offs.

Compared to these works, our study introduces a novel strategy that embeds watermarks directly into the diffusion process of generative models. Unlike traditional methods that focus solely on protecting output images, our approach ensures that the watermark is deeply integrated with the generation process, enabling traceability back to the model itself. The proposed framework demonstrates strong model-agnostic adaptability, as it can be applied to various diffusion model architectures such as DDIM and DDPM. Furthermore, experimental results reveal that our method maintains high visual quality, with negligible impact on image fidelity, while achieving robustness against multiple attacks, including compression, rotation, and blurring.

A key innovation of our work lies in its efficient encoding and decoding of watermark features using an autoencoder. This design enhances the precision of watermark extraction, even under substantial attack conditions. Additionally, we contribute to the field by introducing the Generative Watermarking Image (GWI) dataset, which provides a standardised benchmark for evaluating watermarking techniques in generative models. Unlike the template-based synchronisation approach proposed by Wang et al. **Wang et al., "Template-Enhanced Network for Watermark Synchronization under Geometric Attacks"** , our method eliminates the reliance on complex template embedding, simplifying implementation and reducing computational overhead. Moreover, by integrating watermarking into the diffusion model's training process, we extend the applicability of watermarking technologies beyond the traditional scope, establishing a robust foundation for protecting generative models.


\begin{figure*}
\centering
% Use the relevant command to insert your figure file.
% For example, with the graphicx package use
  \includegraphics[width=\textwidth]{frame.png}
% figure caption is below the figure
\caption{\small The Watermark Generator transforms an image-based watermark $w$ into a watermark feature map $w_{T}$ which serves as a branch input to the diffusion model. It is merged with the output $x_{T}$ of the forward diffusion process. The watermarked noise `$xw_T$' serves as a new input for the diffusion denoising phase, which enables embedding watermark features into the diffusion model. The trained diffusion model generates images $xw_{0}$ as usual, but they contain imperceptible watermark information. The Watermark Extractor learns to reconstruct the watermark  $w_R$ from the generated images.}
\label{fig:2}       % Give a unique label
\end{figure*}