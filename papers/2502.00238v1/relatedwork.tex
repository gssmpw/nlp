\section{Related Work}
\label{sec:related}

Research efforts have developed tools to assess the structural integrity and content accuracy of assurance cases \cite{maksimov2019survey,10311213}. These tools typically focus on reviewing structural soundness (e.g., ensuring that claims lead to solutions) and providing tracking and reporting mechanisms; however, they offer limited support for comprehensive content and evidence analysis \cite{maksimov2019survey}. To evaluate the logical soundness of arguments, some studies \cite{murugesan2023semantic,muram2023attest, yuan2016automatically,rushby2015understanding} have explored predicate-based representations and semantic analysis to detect logical inconsistencies, counter-claims, and counter-evidence.
However, the majority of these works focus on logical fallacies.
More recently, some approaches \cite{gohar2024codefeater,AISupported} have sought to leverage LLMs to automatically identify and mitigate a broader range of defeaters in assurance cases. Although promising, they have limited coverage and rely on human oversight and judgment.

Several other works have sought to improve assurance case assessments, outside the context of defeaters. Yuan and Kelly \cite{yuan2012argument} introduced a framework using informal logic reasoning schemes with questions to assess the plausibility of the arguments. Luo \MakeLowercase{\textit{et al.}} \cite{luo2017systematic} proposed a general iterative process to streamline the ad-hoc nature of reviews. Separately, Chowdhury \MakeLowercase{\textit{et al.}} \cite{chowdhury2020systematic} introduced a set of criteria to evaluate the structural and content quality of an assurance case. Finally, Shahandashti \MakeLowercase{\textit{et al.}} \cite{shahandashti2024prisma} conducted a study to unify the literature on assurance case weakeners.