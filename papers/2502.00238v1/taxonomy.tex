\section{Taxonomy}
\label{sec:taxonomy}

\begin{table*}[ht]
\centering
\caption{Logical Defeaters}
\begin{tabularx}{\linewidth}{@{}l p{3.7cm} X }
\toprule
\textbf{} & \textbf{Sub-Type \cite{greenwell2006taxonomy}} & \textbf{Description and Illustrative Example} \\
\toprule
\multirow{4}{*}[-3.3ex]{\rotatebox[origin=r]{90}{Relevance}} & Appeal to Improper Authority & Referring to an authority who is not an expert in the relevant field. Example: \textit{``The sUAS is certified by NASA." NASA does not certify sUAS airframes.}\\ 
& Red Herring & Distracting the audience by introducing irrelevant content. Example: \textit{``Use the newest-looking battery." The outward appearance of the battery does not determine its health.} \\ 
& Drawing the Wrong Conclusion & The conclusion does not logically follow from the premises. Example: \textit{``The sUAS can receive signal on the ground, therefore it can receive signal in the air." This is not necessarily the case.}\\ 
& Using the Wrong Reasons & Providing irrelevant or weak reasons to support a claim. Example: \textit{``The sUAS is safe to operate because it has a high-resolution camera for accurate navigation.'' The argument cannot function as sufficient evidence of safety.}\\
\hline
\multirow{8}{*}[-9ex]{\rotatebox[origin=c]{90}{Acceptability}} & Fallacious Use of Language & Ambiguities or misleading language that affects the argument's validity. Example: \textit{``For a sUAS flight, use a battery fresh off the charger." This statement does not make clear how long the battery has been charging.}\\
& Arguing in a Circle & Using a conclusion as one of the premises. Example: \textit{``The battery won't fail if the battery works correctly." This is a restatement of the claim used as evidence.}\\ 
& Fallacy of Composition & Assuming what is true of a part is true for the whole. Example: \textit{``If the battery on an sUAS is new, the flight will be successful." There are other parts of an sUAS that could fail other than the battery. }\\ 
& Fallacy of Division & Assuming what is true of the whole is true for its parts. Example: \textit{``The sUAS previously flew successfully, so all parts function correctly." It assumes that the system's overall performance guarantees the reliability of each part.}\\ 
& False Dichotomy & Presenting only two options when more exist. Example: \textit{``Either the communications system will work or it won't." Communications might work most of the time, but not at a crucial moment.}\\ 
& Faulty Analogy & Comparing things that aren’t sufficiently alike. Example: \textit{``If a fixed-wing sUAS successfully completed a mission in this environment, a quadrotor sUAS will also." The argument doesn’t prove the systems’ differences are irrelevant.}\\ 
& Distinction without a Difference & Making an irrelevant distinction between similar things. Example: \textit{``The sUAS has been upgraded with a new flight control system." Meaningless unless arguments are supplied that demonstrate the differences between the systems.}\\ 
& Pseudo-precision & Using overly precise statistics that don't have meaningful implications. Example: \textit{``The sUAS is safe to operate as the wind forecast predicts gusts of 17.9 m/s, below the 18 m/s limit." The 
precision of .1 m/s is likely not perfect.}\\
\hline
\multirow{6}{*}[-7.5ex]{\rotatebox[origin=c]{90}{Sufficiency}} & Hasty Inductive Generalization & Drawing a conclusion based on insufficient evidence. Example: \textit{``The sUAS has sufficient power to safely complete an 8-minute flight, based on the battery consumption observed in the first two 1-minute flights.''}\\ 
& Arguing from Ignorance & Claiming something is true because it hasn't been proven false. Example: \textit{``If the battery has not had a fault before, then the battery is healthy." A lack of evidence of faults is not evidence of battery health.}\\ 
& Omission of Key Evidence & Leaving out important information that would alter the conclusion. Example: \textit{``The wind is currently less than the maximum allowed by the sUAS, therefore the wind is safe." The wind could be gusty.}\\ 
& Ignoring the Counter-Evidence & Overlooking data that contradicts the argument. Example: \textit{``The sUAS is safe because it has passed all standard safety tests.'' This ignores potential counter-evidence, like past failures in real-world conditions..}\\ 
& Confusion of Necessary and Sufficient Conditions & Misinterpreting necessary conditions as sufficient. Example: \textit{``The sUAS is safe because it has a collision avoidance system.'' While necessary, sufficient safety requires considering other factors (e.g., environmental conditions).} \\% or system maintenance.}\\ 
& Gambler's Fallacy & Believing past random events affect future ones. Example: \textit{``The sUAS is safe because it has flown successfully in the last 100 flights without any issues.'' Past success does not guarantee future safety.}\\
\hline

\end{tabularx}
\label{tab:logical_defeater}
\end{table*}

Our thematic analysis identified seven broad categories of real-world defeaters. The taxonomy, presented in Figure \ref{fig:taxonomy}, focuses on common defeater categories in assurance cases for cyber-physical systems. To provide a cohesive framework for the community, we built on existing taxonomies \cite{greenwell2006taxonomy}, classifications (e.g., for uncertainty \cite{ramirez2012taxonomy}), and terminologies to avoid re-inventing new terms. This taxonomy is intended to be flexible, extensible, and evolving, with the expectation that additional defeaters, evidence, and mitigation strategies will be incorporated over time. Below, we describe each major category, highlighting subcategories and providing examples.

\subsection{Logical Fallacies}

Logical fallacies are a distinct type of defeater that can undermine reasoning in safety assurance cases, leading to flawed conclusions. These fallacies, often subtle and implicit, primarily affect the \emph{Argumentation} component of a safety case and are difficult to identify. %
Therefore, practitioners must be adept at scrutinizing argument structures to detect these underlying flaws. Greenwell \MakeLowercase{\textit{et al.}} \cite{greenwell2006taxonomy} constructed a taxonomy limited to logical fallacies, which aligns closely with our analysis of the survey and experience with assurance cases.  
To maintain consistency, we adopt their established terminology for logical-argument sub-types and integrate them into our broader taxonomy.

Greenwell \MakeLowercase{\textit{et al.}} \cite{greenwell2006taxonomy} defined three categories of logical fallacies: \emph{Relevance}, \emph{Acceptability},  and \emph{Sufficiency} fallacies. Acceptability refers to a logical flaw where the argument's premises or evidence do not have enough credibility or support to be deemed acceptable due to faulty reasoning. For instance, a \emph{circular argument} occurs when the conclusion is assumed within the premises, offering no real evidence or reasoning to substantiate the claim. Relevance fallacies occur when an argument includes information that may seem related but does not directly support the main claim, hence diverting attention and weakening the argument's logical structure. It can deceive a reviewer with irrelevant premises. Finally, sufficiency fallacies include arguments where not enough evidence is provided to support the claim. Greenwell \MakeLowercase{\textit{et al.}} \cite{greenwell2006taxonomy} further refined these three categories into the subtypes in Table \ref{tab:logical_defeater}, for each of which we provide an example for ease of understanding.

\subsection{Contextual Defeaters}

\emph{Contextual} (or operational) defeaters refer to challenges that arise when a system's predefined standards or assumptions do not hold in its operating environment. Unlike logical defeaters, which focus on logical inconsistencies, contextual defeaters impact both the evidence and claims in an assurance case by highlighting how shifts in the operational context can undermine safety arguments. Our analysis identifies several granular sub-types of contextual defeaters, shown in Table \ref{tab:contextual_defeaters} including faults (physical and software), human errors, configuration errors, monitoring failures, and environmental factors. Faults involve malfunctions in hardware, software, or subsystems due to defects, wear, or operating conditions, which can degrade system performance and impact the claim's validity. Environmental factors, such as extreme temperatures, can impact operations and must be identified to minimize risk. Human errors are a common source of defeaters and include incorrect use, bias, misinterpretation of system behavior, non-malicious operator errors, or failure to follow procedures, which can lead to safety risks. Finally, accurate monitoring is essential to ensuring safety in critical cyber-physical systems. Addressing context defeaters requires demonstrating the system's ability to detect, handle, and recover from such failures through redundancy, fault-tolerant design \cite{dubrova2013fault}, regular maintenance procedures, and adequate training.


%%%% CONTEXTUAL TABLE %%%%
\begin{table}[h]
\centering
\caption{Contextual Defeaters}
\begin{tabularx}{\linewidth}{@{} X }
\toprule
 \textit{\textbf{Goal:}} \textit{The sUAS battery will be adequate for the mission, considering its charge, health, ongoing monitoring, self-diagnostics, and mission duration.} \\
\end{tabularx}

\begin{tabularx}{\linewidth}{@{} p{1.9cm} X }
\toprule
\textbf{Sub-Type} & \textbf{Example \textit{(Unless...)}} \\
\midrule
Faults (Physical) & ...the battery becomes physically detached during flight. \\
Human Errors & ...the pilot miscalculates the time of the mission.\\
Configuration & ...the battery monitor is misconfigured leading to erroneous readings.\\
Monitoring & ...there is interference such that the battery level cannot be transmitted to the ground station. \\
Environmental Factors & ...the wind is stronger than anticipated, leading to rapid battery loss. \\
\bottomrule
\end{tabularx}
\label{tab:contextual_defeaters}
\end{table}
%%%% END CONTEXTUAL TABLE %%%%

\subsection{Evidence Validity Defeaters} 

\emph{Evidence Validity} defeaters refer to challenges that compromise the trustworthiness, reliability, or completeness of the evidence used to support claims in safety assurance cases. These defeaters, shown in Table \ref{tab:evidence_integrity_defeaters}, 
undermine confidence in the data, testing results, or observations that underpin system safety evidence. They can arise from several sources, including faulty data collection, analysis errors, misinterpretation of results, or any factors that lead to incomplete, biased, or inaccurate evidence. These defeaters impact the evidence nodes of the assurance case. They can also cast doubt on the claims nodes if the evidence is insufficient or unreliable. 
With the growing integration of ML/AI in cyber-physical systems, new defeaters have emerged, including concerns about robustness, model variance, and drift, which can seriously challenge the validity of the evidence presented. Evidence validity defeaters are particularly important because they directly challenge the factual basis of the assurance case, potentially rendering the conclusions drawn from them less credible or even invalid. Possible mitigations include careful validation and verification of the evidence, especially data, robustness to perturbations, audit and review of tests, and assessing fidelity \cite{bloomfield2021safety}.

%%%% EVIDENCE Validity TABLE %%%%
\begin{table}[t]
\centering
\caption{Evidence Validity Defeaters}
\begin{tabularx}{\linewidth}{ @{} X }
\toprule
\textit{\textbf{Goal:}} \textit{The wind conditions in the operating region (OR) are within allowed limits based on published maximum winds, forecasts, and current indicators.} \\
\end{tabularx}

\begin{tabularx}{\linewidth}{@{} l X }
\toprule
\textbf{Sub-Type} & \textbf{Example \textit{(Unless...)}} \\
\midrule
ML/AI & ...unless local wind patterns (tall buildings) were not included in the training data of the ML model.\\
Data Drift & ...the wind conditions in the OR change over time and are not detected by the pilot.\\
Inadequate Metrics & ...the metrics used to determine the sUAS's safe maximum wind speed ignore an aging airframe.\\
Testing and Validation & ...the testing used to determine maximum wind speeds did not include turbulent winds.\\
\bottomrule
\end{tabularx}
\label{tab:evidence_integrity_defeaters}
\end{table}
%%%% END EVIDENCE Validty TABLE

\subsection{Requirements Engineering Defeaters}

In assurance cases, the ``acceptably safe” goals typically assert that all safety-related requirements have been adequately met \cite{bloomfield2021safety}. This directly relies on the safety requirements being sufficient, complete, and valid to capture all necessary safety properties. Common instances of such defeaters include missing, incorrect, ambiguous, outdated (needing change), and inconsistent requirements (see Table \ref{tab:requirements_defeaters} for examples). A requirements defeater impacts the validity of the assurance case's goals, evidence, and argument structure. Possible mitigations include agile development processes to revise requirements, detailed traceability, and change-impact analysis \cite{lams09, bloomfield2021safety}.

%%%% REQUIREMENTS TABLE %%%%
\begin{table}[ht]
\centering
\caption{Requirements Defeaters}
\begin{tabularx}{\linewidth}{@{} X }
\toprule
\textit{\textbf{Goal:}} \textit{The ground station will control the sUAS within the transmitter's maximum range, using system diagnostics and an EM interference detector.} \\
\end{tabularx}

\begin{tabularx}{\linewidth}{@{} l X }
\toprule
\textbf{Sub-Type} & \textbf{Example \textit{(Unless...)}} \\
\midrule
Missing Reqs. & ...the diagnostics are not designed to check the communications system. \\
Incorrect Reqs. & ...the electromagnetic (EM) interference detector is not sufficiently sensitive. \\ 
Ambiguous Reqs. & ...the maximum published communications distance does not indicate if it considers tall trees or buildings 
\\
Outdated Reqs. & ...the detector cannot detect new forms of wireless communication.\\
Inconsistent Reqs. & ...the sUAS can handle less interference than the scan checks for.\\
\bottomrule
\end{tabularx}
\label{tab:requirements_defeaters}
\end{table}
%%%% END REQUIREMENTS TABLE %%%%

\subsection{Structural Defeaters} 

%%%% STRUCTURAL TABLE %%%%
\begin{table}[ht]
\centering
\caption{Structural Defeaters}
\begin{tabularx}{\linewidth}{ @{} X }
\toprule
 \textit{\textbf{Goal:}} \textit{The ground station will control the sUAS within the transmitter's maximum range, using system diagnostics and an EM interference detector.} \\
\end{tabularx}

\begin{tabularx}{\linewidth}{@{} l X }
\toprule
\textbf{Sub-Type} & \textbf{Example \textit{(Unless...)}} \\
\midrule
Lack of Redundancy & ...the transmitter is damaged by an electrical short, and there is no backup transmitter. \\
Interdependencies & ...a fault in the ground station's electric system causes multiple failures, including the transmitter.. \\
\bottomrule
\end{tabularx}
\label{tab:structural_defeaters}
\end{table}
%%%% END STRUCTURAL TABLE %%%%

Structural defeaters typically involve risks inherent in the system’s configuration or design, such as lack of redundancy and issues of interdependencies. These flaws or weaknesses are localized in the system’s blueprint or underlying infrastructure, which must be highlighted in the review (see Table \ref{tab:structural_defeaters}).



\subsection{Adversarial Defeaters}

Our review identified a category of defeaters caused by deliberate actions or external influences that compromise a system’s safety and reliability, which we broadly classify as \emph{Adversarial} defeaters. These defeaters can impact both the evidence and goal nodes of an assurance case, extending beyond goals specifically related to security to impact safety (see Table \ref{tab:adversarial_defeaters}). For instance, the claim that \textit{``system configurations have been verified for correct operation''} may fail to account for the risk of deliberate sabotage, exposing unrelated vulnerabilities. Prior research highlights various adversarial threats defined through security goals and requirements such as availability, confidentiality, and integrity, which can exploit weaknesses in functionality, data, or system operations \cite{HaleyLMN08}. Additionally, domain-specific taxonomies for adversarial risks such as Computer Vision \cite{long2022survey} and Machine Learning \cite{tabassi2019taxonomy} 
can be incorporated into the defeater analysis based on context. However, the detailed integration of these specialized taxonomies is beyond the scope of this work. We broadly classify these as \emph{malicious intent} defeaters to guide assurance case review.

%%%% ADVERSARIAL TABLE %%%%
\begin{table}[t]
\centering
\caption{Adversarial Defeaters}
\begin{tabularx}{\linewidth}{ @{} X }
\toprule
 \textit{\textbf{Goal:}} \textit{The ground station will control the sUAS within the transmitter's maximum range, using system diagnostics and an EM interference detector.} \\
\end{tabularx}

\begin{tabularx}{\linewidth}{@{} l X }
\toprule
\textbf{Sub-Type} & \textbf{Example \textit{(Unless...)}} \\
\midrule
Malicious Intent & ...the communications system is intentionally jammed by a malicious actor. \\
\bottomrule
\end{tabularx}
\label{tab:adversarial_defeaters}
\end{table}
%%%% END ADVERSARIAL TABLE %%%%


\subsection{Uncertainty Defeaters}

Many risks in safety-critical systems arise from uncertainty, which can stem from gaps in knowledge and understanding of the system and unforeseen risks that emerge as systems evolve \cite{ramirez2012taxonomy}. \emph{Epistemic defeaters} results from gaps in knowledge or understanding of a system, which can be addressed by gathering more data or knowledge. These can be described as ``known unknowns" and include issues such as incomplete and inconsistent data. In contrast, \emph{aleatoric} defeaters refer to doubts due to inherent randomness or variability in a system, which cannot be reduced. Remedies include accounting for the intrinsic randomness by quantifying and identifying it to increase the threshold margin of error in safety-critical systems \cite{duan2017reasoning}. Finally, \emph{ontological} defeaters include factors or risks within a system that have not yet been identified or anticipated. These ``unknown unknowns" represent uncertainties about what could be present or emerge, even if they fall outside current knowledge or scope \cite{rushby2015interpretation}. (See Table \ref{tab:uncertainty_defeaters}).

%%%% UNCERTAINTY TABLE %%%%
\begin{table}[]
\centering
\caption{Uncertainty Defeaters}
\begin{tabularx}{\linewidth}{ @{} X }
\toprule
 \textit{\textbf{Goal:}} \textit{The SUAS’s obstacle detection system ensures safe navigation by accurately identifying and avoiding obstacles in real time.} \\
\end{tabularx}

\begin{tabularx}{\linewidth}{@{} l X }
\toprule
\textbf{Sub-Type} & \textbf{Example \textit{(Unless...)}} \\
\midrule
Epistemic & ...spurious data from the sensors leads to false negatives.\\
Aleatoric & ...random turbulence leads to failure in avoidance.  \\
Ontological & ...an unforeseen phenomenon causes the system to misidentify obstacles (unknown unknowns). \\
\bottomrule
\end{tabularx}
\label{tab:uncertainty_defeaters}
\end{table}
%%%% END UNCERTAINTY TABLE %%%%



