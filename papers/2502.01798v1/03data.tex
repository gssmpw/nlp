\section{Understanding Unfavorable Financial Terms}
\label{sec:topic_modeling_section}

In this section, we outline our detection goal and present \textit{TermMiner}, a pipeline for collecting, clustering, and topic modeling \termname terms from English shopping websites in the Tranco top 100,000~\citep{tranco} and datasets of fraudulent e-commerce sites~\citep{bitaab2023beyond, janaviciute2023fraudulent}. As shown in \autoref{fig:financial_term_pipeline}, the pipeline categorizes two types of terms: (1) financial terms that may have immediate or future financial impacts, and (2) \termname terms, identified as unfair, unfavorable, or concerning for customers. We then summarize the taxonomy of \termname terms, which fall into four broad categories: (1) purchase and billing, (2) post-purchase, (3) termination and account recovery, and (4) legal terms.

\subsection{Threat Model}
\label{ssec:ftc}

We aim to detect one-sided, imbalanced, unfair, or malicious \textit{financial} terms in online shopping websites' terms and conditions, which could pose significant risks to users, potentially leading to unexpected financial losses. These risks can arise from website operators seeking to limit liability or from intentional malfeasance.

To assess whether a financial term is unfavorable, we refer to Section 5 of the Federal Trade Commission (FTC) Act~\citep{ftcact}, which defines an act as unfair if it meets the following criteria:

\begin{itemize}
    \item \textbf{C1: Substantial Injury}. It causes or is likely to cause substantial injury to consumers;
    \item \textbf{C2: Unavoidable Harm}. Consumers cannot reasonably avoid it; and
    \item \textbf{C3: Insufficient Benefits}. It is not outweighed by countervailing benefits to consumers or competition. 
\end{itemize}

During the topic modeling of term clusters, we judge the topic representing each cluster by three criteria to evaluate their fairness.

\myparagraph{C1} Since we focus on financial terms with potentially detrimental impacts, all financial terms inherently satisfy this criterion.


\myparagraph{C2} Terms and conditions are often hidden or difficult to avoid. Fair financial terms must be clearly displayed at critical points, like the payment page. However, terms related to cancellation, refunds, and returns are rarely shown upfront. We evaluate terms for unexpected fees (e.g., cancellation charges, non-refundable items, costly returns) that place an undue burden on consumers.


\myparagraph{C3} We classify terms as benign if they serve legitimate user or business protection, such as terms prohibiting fraud or abuse, protecting intellectual property, or ensuring legal compliance.



\subsection{Data Collection and Topic Modeling}
\label{ssec:data_collection_and_topic_modeling}

As shown in~\autoref{fig:financial_term_pipeline}, we introduce \textit{TermMiner}, a data collection and topic modeling pipeline for identifying \termname term at scale.  By integrating LLMs like GPT-4o~\citep{openai2023gpt4}, \textit{TermMiner} significantly reduces the extensive manual efforts required in previous web content mining studies, such as those focused on detecting dark patterns~\citep{mathur2019dark}. 
\textit{TermMiner} is open-sourced and can be repurposed for various web-based text analysis tasks or longitudinal studies. Researchers can use our tools to explore different aspects of terms and conditions, such as readability, accessibility, or fairness.


\myparagraph{A Two-Pass Method}
In the data collection and topic modeling steps, we employ a two-pass method. The first pass focuses on modeling and detecting \textit{financial terms} to develop a corresponding topic template. In the second pass, we use the detected financial terms to re-conduct the classification and topic modeling modules. This time, the goal is to detect \termname terms within the financial terms identified. This approach is necessary because, to the best of our knowledge, there are no established templates or annotation schemes for (1) financial terms or (2) \termname terms in online shopping agreements. This two-pass process ensures comprehensive detection and accurate categorization of both financial and \termname terms.


\myparagraph{(1) Measurement Module} The measurement module collects terms and conditions from shopping websites to build a large, diverse dataset for analysis. For our large-scale measurement, we collect English shopping websites from two sources: the Tranco list~\citep{tranco}, a ranking of top global websites, and two fraudulent e-commerce datasets (FCWs~\citep{bitaab2023beyond} and the Fraudulent and Legitimate Online Shops Dataset~\citep{janaviciute2023fraudulent}). We filter out non-English content using Python's langdetect library~\citep{langdetect}. To classify shopping websites, we evaluate several configurations: (1) GPT-3.5-Turbo~\citep{gpt35} with URL, (2) GPT-3.5-Turbo with URL and HTML content, (3) GPT-4o~\citep{openai2023gpt4} with URL, and (4) GPT-4o with URL and website screenshot. To evaluate our classification methods, we manually annotated a sample of 500 websites from the Tranco list, categorizing them into ``shopping'' and ``non-shopping.'' GPT-4o, when prompted with URLs and screenshots, achieved an accuracy of 92\%, comparable to commercial website classification services~\citep{mathur2019dark} (see Appendix~\ref{sec:website_cls} for details). Therefore, we use this configuration throughout our work.





We subsequently crawl the shopping websites to collect terms and conditions pages. A snowballed regex matching method detects terms and any nested policy pages, refined through positive and negative regex patterns to improve accuracy. Starting with common anchor texts, we iteratively refine the regex patterns by analyzing T\&C links, which can be found in Appendix~\ref{sec:appendix_reg}. As shown in~\autoref{table:dataset_stats}, we collected \termcnt terms from \websitecnt websites in total. 



\myparagraph{(2) Classfication Module}
The classification module categorizes terms from shopping websites' terms and conditions into binary categories: positive or negative. The categorization is based on the detection goal (such as identifying financial terms or identifying \termname terms) using corresponding prompts with the GPT-4o model~\citep{openai2023gpt4}.




We opt for prompt engineering instead of fine-tuning the LLM for term classification to reduce costs. Prior work~\citep{sun2023text,openai2023bestpractices}, along with our empirical observation (see~\S\ref{sec:eva}), indicates that clear task descriptions and relevant examples (taxonomy) significantly enhance LLM performance in text classification. For \termname terms, we use the ``Unfavorable Term Taxonomy Prompt'' from Appendix~\ref{sec:prompts} and topics identified in the topic modeling step, to perform zero-shot term classification on a given set of terms and conditions. This process outputs sets of positive and negative terms, which are then used for clustering, inspection, and topic modeling. The resulting template generated from this analysis will, in turn, enhance the classification accuracy, creating a feedback loop that continuously improves our detection capabilities. 

\input{tables/dataset_stats}

\myparagraph{(3) Topic Modeling Module} The topic modeling module uses LLMs and manual inspection to organize terms into meaningful topics. We generate sentence embeddings with the T5 model~\citep{raffel2020exploring} and apply the DBSCAN clustering algorithm~\citep{ester1996density} to group terms by semantic similarity. The DBSCAN hyperparameters are decided through manual inspection. To extract high-frequency topics, we leverage GPT-4o~\citep{openai2023gpt4}, building on recent findings that show LLMs outperform traditional topic modeling methods like Latent Dirichlet Allocation (LDA)~\citep{blei2003latent} and BERTopic~\citep{grootendorst2022bertopic} in topic analysis~\citep{shrestha2023we, mu2024large}.



We develop an iterative topic modeling approach assisted by GPT-4o proceeds as follows: 
\begin{enumerate}
    \item We analyze DBSCAN clusters and create an initial topic template for financial terms.
    \item GPT-4o performs topic modeling on random samples from each cluster, assigning them to existing topics or suggesting new ones.
    \item We review and refine new topic suggestions through manual inspection, and updating the template.
    \item This process iterates until all clusters are assigned to a meaningful and satisfactory topic.
\end{enumerate}


This iterative workflow, combining clustering, human-guided template creation, and GPT-4o's advanced topic modeling capacity, enables efficient and comprehensive extraction of the topic template. We analyze 22,112 clusters in total, creating the \termname term taxonomy below.

\input{tables/malicious_terms}

\myparagraph{ShopTC-100K Dataset.} In the data collection stage, we extract 8,251 shopping websites from the Tranco top 100K, yielding 1.8 million terms.~\autoref{tab:dataset_stats} presents ShopTC-100K statistics alongside two fake e-commerce datasets, with unfavorable financial terms identified in later measurement study (\S\ref{sec:findings}). 



\subsection{\TermName Term Taxonomy}
\label{sec:financial_terms_section}




We categorize \termtypecnt types of \termname terms into \termcatcnt categories (\autoref{table:taxonomy}) and provide real-world examples analyzed against the three
criteria proposed by the FTC Act criteria (\S\ref{ssec:ftc})~\citep{ftcact}. A detailed taxonomy is in Appendix~\ref{sec:detailed_tax}. While not inherently deceptive, these terms often impose financial obligations consumers should recognize. We note that the severity of such terms depends on \textit{context}, which we leave for future research. We do not claim this list is exhaustive; however, it represents the most prominent types among the \termcnt terms from \websitecnt websites. We also report the financial term template in Appendix~\ref{sec:appendix_finaincial_terms}.

It is important to note that this paper \textit{does not} aim to analyze the fairness of terms from the legal perspective. We consider our work to be a complementary addition to the AI \& Law datasets~\citep{lippi2019claudette, galassi2024unfair}, by focusing on the natural phrasing found in online shopping websites' terms and conditions. A comparison between our \termname term template and previous work on online agreement fairness can be found in Appendix~\ref{sec:appendix_other_templates}.



%\elisa{need to mention it here, since in the TermLens section, we will say that the LLM module is pluggable and we can use fine-tuned version to do it, therefore we need to talk about the creation of fine-tuning dataset here. Make a table?}



