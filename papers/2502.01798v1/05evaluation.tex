
\section{Evaluation and Large-Scale Measurement}

\input{tables/classification_results}



We implement and evaluate \platform using a manually annotated dataset. Our evaluation focuses on two key aspects: (1) assessing detection performance to determine how effectively LLMs, including both zero-shot and fine-tuned models, identify \termname terms (\S\ref{sec:eva}), and (2) analyzing findings from large-scale measurements using \platform (\S\ref{sec:findings}).



\subsection{Evaluation on an Annotated Dataset}
\label{sec:eva}

\myparagraph{Dataset} We create an annotated dataset by randomly selecting 500 terms from clusters of both \termname terms and negative clusters (i.e., benign financial or non-financial terms). This yields 250 potential \termname terms and 250 benign terms. Three researchers independently labeled the terms using the \termname template, without knowledge of the clusters. Disagreements were resolved in a second pass, and duplicates were removed, resulting in 489 final terms. The dataset was split into 244 terms for fine-tuning and 245 terms for validation (\autoref{tab:dataset_stats}). 


\myparagraph{Baselines}
To our knowledge, no prior work has directly addressed the detection of unfavorable financial terms. Recent advances in large language models (LLMs) demonstrate superior performance in common sense reasoning, complex text classification, and contextual understanding~\citep{gpt35,openai2023gpt4,touvron2023llama}, outperforming older models like BERT~\citep{devlin2018bert} and RoBERTa~\citep{liu2019roberta}. Therefore, we evaluate state-of-the-art LLMs: (1) GPT-3.5-Turbo~\citep{gpt35}, (2) GPT-4-Turbo~\citep{openai2023gpt4}, and (3) GPT-4o, along with two open-source LLMs: (1) LLaMA 3 8B~\citep{touvron2023llama} and (2) Gemma 2B~\citep{team2024gemma}.


\myparagraph{Evaluation Configurations}
We evaluate two configurations: (1) Zero-shot classification with a simple binary prompt describing the unfavorable financial term and a multi-class taxonomy prompt explaining term types, and (2) Fine-tuning the LLM using the taxonomy to improve detection accuracy (see prompts in Appendix~\ref{sec:prompts}).





\myparagraph{Metrics} We evaluate the models in terms of false positive rate, true positive rate, F1 score, and Area Under the Curve. AUC represents the area under the ROC (Receiver Operating Characteristic) curve, measuring the model's ability to distinguish between classes.



%\subsection{Performance Analysis}
%\label{sec:eva_performance}



\myparagraph{Zero-shot Classification Performance}
As a baseline for \termname term detection, we evaluated zero-shot classification with two prompts: (1) a simple prompt defining unfavorable financial terms and (2) a taxonomy prompt explaining term types. Using the taxonomy improved the True Positive Rate (TPR) by 4.4\% to 27.4\% and boosted the F1 score by 4.5\% to 21.1\%, showing a better balance of precision and recall. However, the False Positive Rate (FPR) increased in most cases, except for GPT-4o, where it dropped by 24.2\%. GPT-4o achieved the best overall performance with a TPR of 96.6\% and an F1 score of 82.5\%, demonstrating the importance of a \termname term taxonomy for more accurate detection.




\myparagraph{Fine-tuned LLM Classification Performance}
We fine-tune GPT-3.5-Turbo and GPT-4o for 4 epochs with a batch size of 1. Fine-tuning resulted in significant performance improvements, with GPT-4o achieving a True Positive Rate (TPR) of 92.1\% and an F1 score of 94.6\%. The fine-tuned GPT-4o model outperforms other LLMs in distinguishing true positives from false positives. These results demonstrate that fine-tuning, even with a limited dataset, can substantially enhance detection performance.




\subsection{Large-Scale Measurement}
\label{sec:findings}


To understand the prevalence of \termname terms, we deploy the fine-tuned GPT-4o model with \platform for detection. The backend detection system was applied to English shopping websites filtered from the Tranco list's top 100,000 sites, along with two fake e-commerce website datasets: the FCWs dataset~\citep{bitaab2023beyond} and the FLOS dataset~\citep{janaviciute2023fraudulent}. This large-scale measurement serves as a qualitative study on the prevalence of \termname terms in popular shopping websites. We present our findings below. %\elisa{maybe do a category analysis?}


\label{sec:categoring}

\input{fig_tex/large_scale_measurement_stats}
\myparagraph{Categorizing Websites with \TermName Terms}
As shown earlier in~\autoref{table:dataset_stats},  we collect terms and conditions from \websitecnt English shopping websites, resulting in \termcnt terms. Using a GPT-4o model with the \termname term taxonomy, 10,150 terms (approximately \termpct) were flagged as \termname terms. Notably, \websitepct (3,471 out of 8,251) of the English shopping websites from the top 100,000 Tranco-ranked sites contain at least one type of non-legal \termname term. ~\autoref{fig:large_scale_stats}(a) and (b) show the number of terms and \termname terms across 8,251 websites, underscoring how difficult it is for consumers to review lengthy T\&Cs and pinpoint questionable financial terms thoroughly. This emphasizes the importance of automated detection systems to protect users from unfavorable terms.


\myparagraph{Trend Analysis}
\autoref{fig:large_scale_stats}(c) shows the distribution of unfavorable financial terms across categories in the top 100K Tranco-ranked websites~\citep{tranco}. Post-purchase terms (yellow) are the most common across all ranking levels, with a higher concentration in lower-ranked sites, suggesting these terms are more frequent on less popular websites. Purchase and billing terms (blue) also have significant representation. Termination and account recovery terms (red) and legal terms (green) are less frequent but more evenly spread across the rankings. This trend highlights the widespread presence of unfavorable financial terms, especially on lower-ranked sites, underscoring the need for greater regulation to protect consumers from harmful practices, particularly on less reputable websites.


\myparagraph{Comparing ShopTC-100K with Fake E-commerce Datasets}
Interestingly, the percentage of websites with \termname terms from the Tranco list (\websitepct) is similar to that of fraudulent e-commerce websites (46.70\%). This suggests that unfavorable financial terms are not limited to fraudulent sites but are also prevalent among high-ranking websites, pointing to a broader issue in consumer protection. \textit{ShopTC-100K} websites have more \termname legal terms, indicating that legitimate websites are more inclined to shift liability onto customers than fraudulent ones.



\myparagraph{Qualitative Study on User rating}
From the English shopping websites in the top 100k Tranco list, we select those with the highest frequency of \termname terms across categories. We analyze Trustpilot~\citep{trustpilot} reviews for the top 10 websites in each \termname term category with the highest presence, alongside 40 randomly selected websites. 
As shown in~\autoref{fig:large_scale_stats}(d), websites with \termname terms tend to have lower Trustpilot ratings, particularly those with ``Post-Purchase Terms'' and ``Purchase and Billing Terms,'' indicating negative customer satisfaction. ``Termination and Account Recovery'' and ``Legal Terms'' also correlated with lower ratings, though with more variation, suggesting mixed experiences. This suggests a link between \termname terms and consumer dissatisfaction.




\myparagraph{Qualitative Study on Current Ecosystem Defense}
We examine whether the top 10 websites with the highest frequency of \termname terms are flagged by ScamAdviser~\citep{scamadviser2024website}, Google Safe Browsing~\citep{google2024safebrowsing}, and Microsoft Defender SmartScreen~\citep{microsoft2024smartscreen}. Out of 40 websites, only 6 (15\%) have a ScamAdviser score below 90, and 5 (12.5\%) scored below 10, while the majority receive a perfect score of 100. None of the websites are flagged by Google Safe Browsing or Microsoft Defender, which is expected since \termname terms are not inherently indicative of scams. %To further investigate, we analyze 34 websites flagged by crowd-sourced scam reporting sites such as ScammerInfo~\citep{scammerinfo}, ScamAdvisor~\citep{scamadvisor}, and ScamWatcher~\citep{scamwatcher}. These sites are flagged by \platform, and we manually confirm the presence of \termname terms. However, only 1 out of the 34 was flagged by Google Safe Browsing, and none were flagged by Microsoft Defender.
%, highlighting the lack of detection mechanisms for \termname terms.


\myparagraph{Qualitative Study on User Perception} To illustrate the potential harm of \termname terms,  we present four case studies on user perception and financial harm in each category in Appendix~\ref{sec:case_studies}. This underscores the urgent need for automated systems to detect \termname terms effectively.
