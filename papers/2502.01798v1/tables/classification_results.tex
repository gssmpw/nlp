\begin{table*}[!ht]
\footnotesize
\centering
\caption{Performance of LLMs in detecting \termname terms evaluated using zero-shot classification with GPT-3.5-Turbo, GPT-4-Turbo, GPT-4o, Llama 3 8B, and Gemma 2B, along with fine-tuned GPT-3.5-Turbo and GPT-4o models.}
\label{table:results}
\begin{tabular}{p{1.5cm} p{2cm} p{1.1cm} p{1.1cm} p{1.1cm} p{1.1cm} p{1.1cm} p{1.1cm} p{1.1cm} p{1.1cm}}
\toprule
\multirow{2}{*}{\textbf{Configuration}} & \multirow{2}{*}{\textbf{Model}} & \multicolumn{4}{c}{\textbf{Simple Prompt}} & \multicolumn{4}{c}{\textbf{Unfavorable Term Taxonomy Prompt}} \\
\cmidrule{3-10}
& & \textbf{FPR (\%) (\(\downarrow\) better)} & \textbf{TPR (\%)  (\(\uparrow\) better)} & \textbf{F1 (\%) (\(\uparrow\) better)} & \textbf{AUC (\%)  (\(\uparrow\) better)} & \textbf{FPR (\%)  (\(\downarrow\) better)} & \textbf{TPR (\%)  (\(\uparrow\) better)} & \textbf{F1 (\%)  (\(\uparrow\) better)} & \textbf{AUC (\%)  (\(\uparrow\) better)} \\

\midrule
\multirow{4}{*}{\parbox{4cm}{Zero-Shot}} 
& GPT-3.5-Turbo & 59.5 & 71.9 & 59.9 & 56.2 & 74.0 & 96.5 & 68.5 & 61.2\\
& GPT-4o & 58.6 & 72.6 & 61.4 & 56.2 & \textbf{34.4} & 96.6 & \textbf{82.5} & \textbf{80.3} \\
& GPT-4-Turbo & 58.6 & 72.6 & 61.4 & 56.2 & 64.8 & \textbf{100.0} & 73.8 & 67.2 \\
& Llama 3 & 73.5 & 82.3 & 61.4 & 54.4 & 48.5 & 86.7 & 71.3 & 69.1\\
& Gemma & 56.8 & 80.5 & 65.2 & 61.9 & 74.2 & 100.0 & 69.8 & 62.8\\
\midrule
{Fine-Tuned}
& GPT-3.5-Turbo & -  & -  & -  & - & 5.5 & 91.5 & 92.7 & \textbf{91.8} \\
& GPT-4o & -  & -  & -  & - & \textbf{2.3} & \textbf{92.1} & \textbf{94.6} & \textbf{91.8} \\

\bottomrule
\end{tabular}

\end{table*}
