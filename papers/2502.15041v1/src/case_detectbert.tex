\section{Additional Benchmarking for DetectBERT}

\textbf{DetectBERT} is a recent approach proposed by Sun et al.~\cite{Sun:esem24}, which utilizes a pre-trained BERT-like model combined with Correlated Multiple Instance Learning (c-MIL) to process Smali code from APKs. This approach enhances class-level features and aggregates them into an app-level representation, improving the effectiveness of Android malware detection. 

\subsection{Benchmarking for DetectBERT}
DetectBERT was evaluated against two state-of-the-art Android malware
detection models, Drebin~\cite{Arp:NDSS14} and
DexRAY~\cite{Nadia:MALHat21}. Drebin utilizes SVM on static feature
vectors, while DexRAY is a DL framework that transforms low-level
bytecode into images and processes them using a CNN for malware
detection. The results indicate that DetectBERT slightly outperforms
both baseline models.

\subsection{Dataset for DetectBERT}
\textbf{DetectBERT Benchmark Dataset.} This large-scale dataset originates from the DexRay~\cite{Nadia:MALHat21} study and comprises 96,994 benign apps and 61,809 malware apps. The labeling process is based on VirusTotal reports: apps that are not flagged by any antivirus engines are considered benign, while those detected as malicious by more than two antivirus engines are labeled as malware.  

\subsection{Additional Benchmarking}

% DetectBERT
\begin{table}[t]
    \centering
    \begin{tabular}{lcccc}
        \toprule[0.8pt]
        \textbf{Model} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1 Score} \\
        \hline
        Drebin & 0.97 & 0.97 & 0.94 & 0.96 \\
        DexRay & 0.97 & 0.97 & 0.95 & 0.96 \\
        \textbf{DetectBERT} & \textbf{0.97} & \textbf{0.98} & \textbf{0.95} & \textbf{0.97} \\

        \midrule\midrule
        \rowcolor{gray!20} NB & 0.85 & 0.87 & 0.85 & 0.86 \\
        \rowcolor{gray!20} RF & 0.97 & 0.97 & 0.97 & 0.97 \\
        \rowcolor{gray!20} KNN & 0.97 & 0.97 & 0.97 & 0.97 \\
        \rowcolor{gray!20} SVM & 0.96 & 0.96 & 0.96 & 0.96 \\
        \rowcolor{gray!20} CatBoost & 0.98 & 0.98 & 0.98 & 0.98 \\
        \bottomrule[0.8pt]
    \end{tabular}
    \vspace{10pt}
    \caption{Performance comparison of various models on DetectBERT benchmark dataset. The unshaded are from the original paper, while the  shaded results are from our benchmarking.}
    \label{tab:detectbert_comparison}
    \vspace{-.2in}
  \end{table}

Sun et al.~\cite{Sun:esem24} open sourced their DetectBERT model for
Android malware detection, which enables us to use the exact same
dataset, data splits, and evaluation metrics for training and testing
various models, ensuring fair comparisons.

One of the challenges in applying the DetectBERT model is generating
the feature vector for each smali class in an APK file from a
large-scale dataset. To address this, we explored the performance of
more cost-effective approaches on this dataset.  Since DREBIN is one
of the baseline models used in the original research, we generated DREBIN-like static features for
each app, including requested permissions, component names, intents,
suspicious API calls, and network addresses. We then calculated the
mutual information of the unique static features in this dataset and
ranked them accordingly. By testing different feature vector sizes
based on the top N features, we found that a feature vector consisting
of 2,919 static features per app yielded optimal
results.  Table~\ref{tab:detectbert_comparison} presents the
performance of various ML models against DetectBERT. The shaded rows represent the models we tested with
these features. Except for NB, all other
models (KNN, SVM, RF, and CatBoost) achieved performance comparable to
DetectBERT. CatBoost slightly outperforms DetectBERT.
These ML models also require significantly less compute
time than DetectBERT.

In the paper the authors also conducted experiments
on data sets that adhere to temporal consistency. However
the published dataset does not include the train/test data split
for this experiment, and as a result we were not able
to compare additional ML models' results against the results
published in the paper.

% performance due to the unavailability of the
% original training and testing set lists from the open-source data. If
% we were to create our own splits based on release year, the resulting
% dataset might differ from the original work, compromising the fairness
% of the comparison. 

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
