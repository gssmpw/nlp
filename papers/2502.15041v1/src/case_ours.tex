\section{Benchmarking for CapsGNN}

\textbf{Capsule Graph Neural Networks.}  The Capsule Graph Neural
Network (CapsGNN) approach proposed by
Zhang~et~al.~\cite{Zhang:ICLR19} represents an attractive approach to
explore in the context of Android malware detection, since it captures
information at graph level and enhances graph embeddings obtained with
Graph Neural Networks (GNN)~\cite{Kipf:LCLR17} by adopting the Capsule
Neural Network (CapsNet)~\cite{Sabour:arXiv17} ideas. CapsNet helps
capture different aspects of the graph, corresponding to different
properties, in the final embeddings.

The Capsule Neural Networks (CapsNet) architecture was introduced by
Sabour~et~al.~\cite{Sabour:arXiv17} for image feature extraction,
although the core concept of a capsule in neural networks was created
by Hinton et al.~\cite{Hinton:ICANN2011}. The CapsNet's design is
based on CNNs, but in addition to detecting features as a CNN does,
CapsNet also aims to encode instantiation parameters (such as
position, orientation, texture) as part of the detected
features. Thus, rather than representing the features using scalar
values as in CNNs, the CapsNet uses capsules, which represent a group
of neurons, to encode features as vectors.  The length of a capsule
determines the probability that the corresponding feature is
present. The direction of the capsules reflects the instantitation
parameters of the feature.

Rather than using a pooling layer to transmit the feature information
between layers, CapsNet uses a dynamic routing mechanism to find the
best connections between the current and next layer capsules based on
agreement. The key innovation of routing by agreement is the
capability to capture the spatial relationships between parts and
their whole. In our case, the CapsNet is meant to encode precise
semantic information from the inter-procedural control flow
graphs (ICFGs) of an app.

As GCN, CapsGNN takes as input a graph data structure, while also
employing capsules (vectors) instead of scalar values to represent
different instantiation parameters of the features. Thus, CapsGNN is a
promising DL candidate for Android malware detection, as it can work
directly with the ICFG and capture different aspects of the
graphs. Figure~\ref{fig:capsgnn_view} illustrates the CappsGNN
architecture implemented in our Android malware detection
system. CapsGNN extracts a primary capsule for each node in an input
graph. As opposed to GCN, which extracts node embeddings from the last
layer of the network, CapsGNN extracts node embeddings from different
layers and represents them as primary capsules (more precisely, the
primary capsules are obtained by summing up node embeddings from
different layers). As graphs in other application domains, the ICFGs
generated from Android apps are diverse, with graph sizes varying
widely. As mentioned above, the number of primary capsules depends on
the number of nodes in a graph, as each capsule corresponds to a
node. Furthermore, the number of graph and class capsules depends on
the number of primary capsules. Thus, the node capsules need to be
scaled to ensure that the graph capsules from different graphs have
the same dimension and are comparable, despite differences in graph
sizes. The CapsGNN framework designed by
Zhang~et~al.~\cite{Zhang:ICLR19}, which we use in this work,
implements an attention module between primary capsules and graph
capsules. The attention module ensures that: 1) the model identifies
the most relevant parts of the graph, and 2) the node capsules are
scaled to the same dimension, so that they subsequently lead to
comparable graph and class embeddings.

 \begin{figure}[t]
   \centering
   \includegraphics[width=\linewidth]{figures/capsgnn.pdf}
   \caption{CapsGNN Architecture in Android Malware Detection. \(\textbf{N}\) is the number of nodes in one graph, \(\textbf{C}\) is the number of node attribute channels, \(\textit{d}\) is the node embedding dimension.}
   \label{fig:capsgnn_view}
   \vspace{-.2in}
\end{figure}


\subsection{Benchmarking for CapsGNN}

CapsGNN is a DL model designed to capture complex patterns within an
app. We include five widely used ML models, as mentioned in
Section~\ref{sec:alternative_ml}, as potential baselines for
comparison with the CapsGNN model.

In addition to traditional ML models, we also experimented with
ExcelFormer, a state-of-the-art neural network proposed by Chen et
al.~\cite{excelformer:kdd24}, which outperforms Gradient Boosted
Decision Trees (GBDTs) and existing DL models for tabular data. It
addresses key challenges in deep tabular learning through three key
innovations: (1) a semi-permeable attention module that reduces the
influence of less informative features, (2) data augmentation
techniques specifically designed for tabular data, and (3) an
attentive feedforward network that enhances model fitting
capability. These design choices make ExcelFormer a highly effective
solution for diverse tabular datasets. Extensive and rigorous
experiments on real-world datasets demonstrate that ExcelFormer
consistently outperforms previous models across various tabular
prediction tasks. Given that an APK's structural representations can
be effectively converted into tabular data, we selected ExcelFormer to
assess its potential for improving Android malware detection
performance.

The Android system is an event-based system, and the event-driven
control flow can involve various method calls based on the app's
components' life cycles. Independent method-level control flow graphs
(CFGs) or API call sequences could not capture the true invocation
order of API calls. In contrast, a ICFG provides a more accurate
representation of the actual execution sequence of
these. operations. Therefore, we utilize ICFGs as the input
representation for our CapsGNN model.

Traditional ML models usually need a predefined feature set to
represent an application, and the performance of the classifier highly
depends on the features. Section~\ref{sec:related} briefly explains
some prior works' approaches to feature selection. The most common
features used in prior works are API calls, app permissions, and a few
other pieces of information from raw app code. The feature sources are
similar but different approaches collect and organize the features in
different ways. Among prior works based on traidtional ML,
DREBIN~\cite{Arp:NDSS14} has shown high detection preformance on
datasets with realistic size and malware to benign app ratio. Recent
work~\cite{Daoudi:TOPS22} shows that DREBIN's features are still
robust for apps as late as 2019. DREBIN extracts static features such
as components, intents, API calls, permissions, and network addresses
totaling 500,000 features. Roy et al.~\cite{Roy:ACSAC15} achieved
similar performance as DREBIN by choosing only 471 static features
from the original DREBIN's feature set. To balance the computational
cost and performance on large-scale datasets, we use the same features
as those in Roy~et~al.'s work~\cite{Roy:ACSAC15}, which consist of 471
features including permissions, intent actions, discriminative APIs,
obfuscation signatures, and native code signatures. To support future
research, the complete feature list will release as open-source with
other related code following the acceptance of this work.

\subsection{Dataset for CapsGNN}

\begin{figure*}[t]
     \centering
     \begin{subfigure}[b]{0.38\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/window_detail.png}
         \caption{Breakdown of a 6-batch window}
         \label{fig:window_detail}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/sliding_window.pdf}
         \caption{6-batch sliding windows  (with 4 training batches)}
         \label{fig:sliding_window_overview}
     \end{subfigure}
    \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/batch_5.pdf}
         \caption{7-batch sliding windows  (with 5 training batches)}
         \label{fig:5_batches}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/batch_6.pdf}
         \caption{8-batch sliding windows  (with 6 training batches)}
         \label{fig:6_batches}
     \end{subfigure}
        \caption{Sliding Windows}
        \label{fig:Sliding_windows}
 \end{figure*}


The dataset plays a crucial role in ML-based malware detection
research. To ensure meaningful evaluation, the dataset size needs to
reflect the scale of Android apps found in real-world markets. A key
challenge in existing datasets is sampling
bias~\cite{Daniel:USENIXSecurity22}, which can arise when benign and
malicious apps are collected from different sources. To mitigate this
issue, data should be gathered from a single market, ensuring
consistency in distribution and characteristics.  Furthermore, the
organization of training and testing subsets should align with
real-world deployment scenarios. Specifically, training should be
performed on earlier data, while testing should be conducted on later
dataâ€”a methodology supported by prior
research~\cite{Pendlebury:USENIXSecurity19}. Additionally, the testing
subset must accurately represent the real-world ratio of benign to
malicious apps. Some industry
experts~\cite{Pendlebury:USENIXSecurity19} suggest that the proportion
of malware in app markets is relatively low, approximately 6\%.  Since
class imbalance can significantly impact classifier performance,
particularly in terms of precision and recall, it is crucial to
construct a dataset that mirrors these real-world distributions.  By
systematically addressing these concerns, we aim to create a more
representative and reliable dataset for Android malware detection
research.

To assess the impact of app evolution on model performance, we adopt a
\emph{sliding window} approach for constructing training, validation,
and test datasets, as illustrated in
Figure~\ref{fig:Sliding_windows}. Specifically, we first partition the
data into batches based on app release dates, with each batch
containing 5,000 appsâ€”comprising 300 malicious apps and 4,700 benign
apps within the same time frame.

We subsequently form windows of consecutive batches, with the first
few batches used for training, the batch before the last for
validation and the last batch for testing, as shown in
Figure~\ref{fig:window_detail} for a 6-batch window. In each
subsequent experiment, we slide the window forward by one batch to
form a new dataset for training and evaluation.  Our objective is to
maximize data availability for both training and testing. A larger
training set contributes to more robust models, while an extensive
test set ensures reliable performance estimation. Each batch is
designed to include 5,000 apps, which we believe is sufficient to
provide accurate performance evaluations. To determine the optimal
number of training batches for model stability, we experiment with
windows containing 4, 5, and 6 training batches, as shown in
Figure~\ref{fig:sliding_window_overview}, Figure~\ref{fig:5_batches}
and Figure~\ref{fig:6_batches}, respectively.

As illustrated in Figure\ref{fig:sliding_window_overview}, our dataset
configuration results in 22 overlapping 6-batch windows, with each
window consisting of 28,200 benign apps and 1,800 malicious
appsâ€”except for the final window, where the availability of malicious
apps from late 2021 was limited. Our data partitioning strategy
ensures that all experiments are conducted on subsets of consistent
size and with a fixed malicious-to-benign ratio. Additionally, this
approach enables the observation of temporal trends in Android malware
detection.

\begin{figure}[tbp]
  \centering
  \includegraphics[width=.95\linewidth]{figures/model_performance.pdf}
  \caption{Top-performing models among all tested models}
  \label{fig:top_ml_models}
  \vspace{-.2in}
\end{figure}

We will publicly release this dataset upon the publication of the paper.

\subsection{Benchmarking Results}

Figure~\ref{fig:top_ml_models} illustrates the top-performing ML models when using a window with four training batches. It can be observed that all models exhibit very similar performance. RF, CatBoost, and KNN show nearly identical performance, often overlapping on the graph, while SVM performs slightly lower than the other three models across most windows. 

\begin{figure*}[tbp]
    \centering
    \begin{subfigure}{0.47\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/diff_train_rf.pdf}
        \caption{Random forest performance over the sliding windows. The training datasize in a window is varied: 4-batches, 5-batches, and 6-batches.}
        \label{fig:diff_train_rf}
    \end{subfigure}
    \hspace{3mm}
    \begin{subfigure}{0.47\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/diff_train_capsgnn.pdf}
        \caption{CapsGNN performance over the sliding windows. The training datasize in a window is varied: 4-batches, 5-batches, and 6-batches.}
        \label{fig:diff_train_capsgnn}
    \end{subfigure}
    \caption{Main caption for both figures}
    \label{fig:main_figure}
\end{figure*}


% \begin{table}[htbp]
%   \centering
%   \begin{tabular}{l*{2}c}
%     \toprule[1pt]
%     $_{Size}$ \space \textbackslash \space $^{Model}$
%     & \begin{tabular}{@{}c@{}}RandomForest \\ (seconds) \end{tabular}
%     &  \begin{tabular}{@{}c@{}}CapsGNN \\ (hours) \end{tabular} \\
%     \midrule
%     4 batches & 55 & 13 \\
%     5 batches & 75 & 17 \\
%     6 batches & 89 & 20 \\
%     \bottomrule[1pt]
%   \end{tabular}
%   \vspace{10pt}
%   \caption{Model Running Time with Different Training Data Size}
%   \label{tab:model_run_time}
% \end{table}


% \begin{figure}
%   \centering
%   \includegraphics[width=.9\linewidth]{figures/all_model_best_performance.pdf}
%   \caption{RF, CatBoost, ExcelFormer and CapsGNN Performance Comparison: Training data consists of 6-batches per window.}
%   \label{fig:evaluation_result}
%   \vspace{-4mm}
% \end{figure}

\begin{figure*}[t]
    \centering
    \begin{minipage}{0.5\textwidth}
      \centering
        \includegraphics[width=.9\linewidth]{figures/all_model_best_performance.pdf}
        \caption{RF, CatBoost, ExcelFormer and CapsGNN Performance Comparison: Training data consists of 6-batches per window.}
        \label{fig:evaluation_result}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.45\textwidth}
      \centering
        \begin{tabular}{l*{2}c}
         \toprule[1pt]
         $_{Size}$ \space \textbackslash \space $^{Model}$
         & \begin{tabular}{@{}c@{}}RandomForest \\ (seconds) \end{tabular}
         &  \begin{tabular}{@{}c@{}}CapsGNN \\ (hours) \end{tabular} \\
         \midrule
         4 batches & 55 & 13 \\
         5 batches & 75 & 17 \\
         6 batches & 89 & 20 \\
         \bottomrule[1pt]
         \end{tabular}
         \vspace{10pt}
        \captionof{table}{Model Running Time with Different Training Data Size}
        \label{tab:model_run_time}
      \end{minipage}
      \vspace{-.2in}
\end{figure*}


\textbf{How much data is needed for model training?} 
To understand how many training batches need to be included in an
experiment window to obtain robust models, we explored windows with
different number of training batches as illustrated in
Figure~\ref{fig:sliding_window_overview}, Figure~\ref{fig:5_batches}
and Figure~\ref{fig:6_batches} , which show windows with 4, 5 and 6
training batches, respectively.

As can be seen in Figure~\ref{fig:diff_train_rf} and
Figure~\ref{fig:diff_train_capsgnn} for RF and CapsGNN, a larger
training dataset does not always result in increased model
performance.

For the traditional ML model, once the size of the training data
reaches a certain level, the performance of the models does not
improve significantly even when we further increase the size of the
training data. In particular, the models trained on 4 batches of data
have similar performance as the models trained on 6 batches of
data. This can be seen for most windows in
Figure~\ref{fig:diff_train_rf}.  For CapsGNN, as
Figure~\ref{fig:diff_train_capsgnn} shows, the performance of the
model improves when training data size in a window is increased from 4
batches to 6 batches (this is expected given that the CapsGNN model
has a large number of parameters that need to be learned). However,
the time and resources required to train the model also prohibitively
increase (see Table~\ref{tab:model_run_time} for time requirements of
the two models), which made it impossible for us to experiment with
even larger training data for CapsGNN.  In contrast, the RF model
needs less than two minutes to train regardless of the size of the
training subset. Given these results, the remaining experiments are
performed using a window size of 8 batches (with 6 batches being used
for training).

The results in Figure~\ref{fig:evaluation_result} show that the
CapsGNN model has similar performance to the other models for windows
from the start of 2018 until the middle of 2019 (i.e., from window 3
through window 14). However, CapsGNN does outperform baseline models
in most test batches past late 2019 (in particular, windows 15, 16,
17, and 18).  We observe that CapsGNN model has more robust
performance than baseline models especially when data evolves (or
undergoes significant changes) past late 2019.  This is not a
surprising result given that the CapsGNN's input includes much richer
program semantics information in the form of ICFG graphs, together
with features not present in the ICFG graph itself, including
permissions, intent actions and obfuscated/native code
signatures. Moreover, CapsGNN dynamically identifies predictive
features in the input as part of the learning process, making it
easier to keep up with changes in the data. As opposed to that, the
input of the RF model includes only manually designed static features
that do not change with changes in the data.  It is surprising to see
that, despite the strictly richer CapsGNN's input and also the
significantly more computational resources required by CapsGNN,
CapsGNN fails to produce significant performance improvements over the
RF model when the data is relatively stable.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
