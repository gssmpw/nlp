\section{Additional Benchmarking for Enc+MLP}

\textbf{Enc + MLP} is % one of the most recent deep active learning
% schemes
proposed by Chen et al.~\cite{Chen:USENIX23} to address the
concept drift~\cite{Jordaney:USENIX17} problem in Android malware
detection. This scheme integrates contrastive learning with active
learning, structuring the model into two subnetworks. The first
subnetwork, a hierarchical contrastive encoder
(\textbf{\textit{Enc}}), employs contrastive learning techniques to
encode input embeddings, ensuring that embeddings of the same malware
family are closer to each other. The second subnetwork is an
\textbf{MLP} classifier that utilizes these embeddings for malware
classification.

The model is initially trained on a labeled dataset to establish the
hierarchical contrastive classifier. Following this, an active
learning process begins, where the trained classifier predicts labels
for a batch of test samples. A pseudo-loss sample selector in the
hierarchical contrastive classifier identifies the most uncertain apps
within a predefined labeling budget and adds them to the training set
for the next iteration. The model is then retrained using a warm
start, and this process repeats continuously, refining the model with
each subsequent batch of test data. This approach improves upon
existing active learning baselines for Android malware detection while
minimizing the need for manual labeling, as demonstrated through
evaluations on two publicly available datasets.


\subsection{Benchmarking for \textit{Enc+MLP}}

\textit{Enc+MLP} uses multiple baseline models and explores
uncertainty sampling for both binary and multiclass classifiers.  The
binary classifiers include a fully connected neural network (Binary
MLP), a linear Support Vector Machine (Binary SVM), and Gradient
Boosted Decision Trees (GBDT).  The multiclass classifiers include an
MLP and an SVM, and the authors also experimented with a combined
classifier, ``Multiclass MLP + Binary SVM.''  Uncertainty is measured as
one minus the maximum prediction score across all classes.  All
baseline models employ a cold-start approach (retraining from scratch
on the updated training data) for active learning.

\subsection{Datasets for \textit{Enc+MLP}}

\textit{Enc+MLP} was evaluated on two datasets: APIGraph and AndroZoo.\\


\begin{table*}[t]
  \captionsetup{skip=4pt}
    \centering
    \begin{minipage}{0.49\textwidth} 
        \centering
        \begin{tabular}{c|c|c|c}
        \bottomrule
        \textbf{Year} & \multicolumn{1}{c|}{\begin{tabular}[c|]{@{}c@{}}\textbf{Malicious}\\ \textbf{Apps}\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\textbf{Benign} \\ \textbf{Apps}\end{tabular}} & \textbf{Total} \\ 
        \hline
        2012 & 3,061  & 27,472  & 30,533  \\ 
        2013 & 4,854  & 43,714  & 48,568  \\ 
        2014 & 5,809  & 52,676  & 58,485  \\ 
        2015 & 5,508  & 51,944  & 57,452  \\ 
        2016 & 5,324  & 50,712  & 56,036  \\ 
        2017 & 2,465  & 24,847  & 27,312  \\ 
        2018 & 3,783  & 38,146  & 41,929  \\ 
        \bottomrule
    \end{tabular}
    \caption{APIGraph Dataset}
    \label{tab:apigraph}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\textwidth} % Adjust width as needed
        \centering
        \begin{tabular}{c|c|c|c}
        \bottomrule
        \textbf{Year} & \multicolumn{1}{c|}{\begin{tabular}[c|]{@{}c@{}}\textbf{Malicious}\\ \textbf{Apps}\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\textbf{Benign} \\ \textbf{Apps}\end{tabular}} & \textbf{Total} \\ 
        \hline
        2019 & 4,542  & 40,947  & 45,489  \\ 
        2020 & 3,982  & 34,921  & 38,904  \\ 
        2021 & 1,676  & 13,985  & 15,662  \\ 
        \bottomrule
    \end{tabular}
    \caption{AndroZoo Dataset}
    \label{tab:androzoo}
  \end{minipage}
  \vspace{-.2in}
\end{table*}


\textbf{APIGraph Dataset.} Collected by Chen et
al.~\cite{Chen:USENIX23} using the app hash list provided by
APIGraph~\cite{Zhang:CCS20}, this dataset spans seven years of Android
apps from 2012 to 2018. The apps are ordered based on their appearance
timestamps in VirusTotal~\cite{VirusTotal}, addressing both spatial
and temporal biases~\cite{Pendlebury:USENIXSecurity19}. The dataset is
evenly distributed across the years, with each year consisting of
approximately 90\% benign apps and 10\% malicious apps. The malicious
apps are sourced from VirusTotal~\cite{VirusTotal},
VirusShare~\cite{VirusShare}, and the AMD dataset~\cite{AMD:dimva17},
while the benign apps are obtained from
AndroZoo~\cite{Allix:MSR16}. Table~\ref{tab:apigraph} provides a
detailed breakdown of this dataset.

\textbf{AndroZoo Dataset.} Collected by Chen et
al.~\cite{Chen:USENIX23} from AndroZoo~\cite{Allix:MSR16}, this dataset includes
Android apps from 2019 to 2021.The apps are also ordered based on their appearance
timestamps. Malicious apps were randomly selected
based on VirusTotal reports, where at least 15 antivirus engines
flagged them as malware. Benign apps were randomly chosen from those
with 0 positive detections by any antivirus engine. The dataset
maintains a similar malware/benign app ratio, with 90\%
benign apps and 10\% malicious apps each
year. Table~\ref{tab:androzoo} provides a detailed breakdown of the
collected data for each year.


\subsection{Additional Benchmarking}

 % APIGraph & AndroZoo
\input{src/apigraph_androzoo_table}

Enc + MLP was evaluated on the two datasets and compared with multiple baseline
models. % This is one of the most recent studies applying ML to Android
% malware detection.
The results showed that it outperforms all
baseline models, with a particularly significant improvement on the
AndroZoo dataset. The authors have made all research
artifacts publicly available, including the datasets used in the
experiments and the model-related code. This accessibility allows us
to integrate additional testing models into their framework
seamlessly, enabling the evaluation of more ML models using the
same input and evaluation metrics.

We selected additional ML models from the ones
mentioned in Section~\ref{sec:alternative_ml}: NB, KNN, RF, and CatBoost
(excluding SVM, as it was already included as a baseline in the
original work), to expand the set of baseline models. Table~\ref{tab:chen_dataset_comparison} presents
the performance of all tested models, with shaded rows indicating
models not included in the original work. If an additional model did
not outperform the original baseline models, we omitted its results
from the table. As a result, Table~\ref{tab:chen_dataset_comparison} only includes RF and CatBoost as
additional baseline models. The results show that RF achieves
performance comparable to the ‘Enc+MLP’ model on the APIGraph dataset
but performs poorly on the AndroZoo dataset. However, CatBoost
outperforms ‘Enc+MLP’ on both datasets across different labeling
budgets, except for the AndroZoo dataset when the labeling budget is
set to 400.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
