\documentclass[runningheads]{llncs}
\usepackage{color}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{multirow}

\usepackage{caption}
\usepackage{datatool}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{enumerate}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{adjustbox}
\usepackage{hyperref}
\usepackage{verbatim}
\usepackage{amsmath}

\hypersetup{
	colorlinks=true,
	citecolor=blue % change to your preferred color
}



\renewcommand{\labelenumii}{\Roman{enumii}}

\newcommand{\methodname}{FeatPCA}

 

\begin{document}

\title{{\methodname}: A feature subspace based principal component analysis technique for enhancing clustering of single-cell RNA-seq data}

\author{
    Md Romizul Islam, Swakkhar Shatabda 
}


%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
%
% \authorrunning{F. Author et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{}
%
\maketitle              % typeset 
	\begin{abstract}
		Single-cell RNA sequencing (scRNA-seq) has revolutionized our ability to analyze gene expression at the cellular level. By providing data on gene expression for each individual cell, scRNA-seq generates large datasets with thousands of genes. However, handling such high-dimensional data poses computational challenges due to increased complexity. Dimensionality reduction becomes crucial for scRNA-seq analysis. Various dimensionality reduction algorithms, including Principal Component Analysis (PCA), Uniform Manifold Approximation and Projection (UMAP), and t-Distributed Stochastic Neighbor Embedding (t-SNE), are commonly used to address this challenge. These methods transform the original high-dimensional data into a lower-dimensional representation while preserving relevant information. In this paper we propose {\methodname}. Instead of applying dimensionality reduction directly to the entire dataset, we divide it into multiple subspaces. Within each subspace, we apply dimension reduction techniques, and then merge the reduced data. {\methodname} offers four variations for subspacing. Our experimental results demonstrate that clustering based on subspacing yields better accuracy than working with the full dataset. Across a variety of scRNA-seq datasets, {\methodname} consistently outperforms existing state-of-the-art clustering tools.
	\end{abstract}

		\section{Introduction}
	Single-cell RNA sequencing technology is aiding us to have a better understanding of gene expression at single cell level. The technology allows us to measure the mRNA expression of every gene for each cell.  This capability is incredibly powerful because it helps us understand which genes are turned on, how much they’re expressed, and where they’re active within the cell \cite{seq1}. This technology has opened many windows for us to analyze the cells and genes expression \cite{seq2}. Single-cell data plays a pivotal role in cancer research, multi-omics studies, genome assembly, and various other aspects of biology and bioinformatics\cite{applications1}\cite{applications2}\cite{applications3}. Here, we have a cell-by-gene matrix and each cell of the matrix corresponds to the gene expression or mRNA count for a cell for a specific gene, thus giving us cellular-level data.\\\\In single-cell data, missing values (representing absent gene expression) are common\cite{missingvalues}. To address this, data imputation becomes necessary to recover these missing values\cite{imputing}. One effective approach is using autoencoders, which learn a compact representation of the data and can predict missing entries based on existing information\cite{autoimpute}. The single-cell datas are very high-dimensional. High-dimensional scRNA-seq data are typically normalized and projected to a lower-dimensional space by dimension reduction\cite{dimred1}\cite{dimred2}. A core analysis of the scRNA-seq transcriptome profiles is to cluster the single cells to reveal cell subtypes and infer cell lineages based on the relations among the cells\cite{clustering1}\cite{clustering2}. Clustering is a necessary step to identify the cell subpopulation structure in scRNA-seq data. Different algorithms such as K-means, Hierarchical clustering are generally used to cluster the cells\cite{kmeans}. Clustering algorithms are typically applied on the lower-dimensioned data to find the clusters. Dimensionality significantly impacts the performance of clustering algorithms in bioinformatics. As the number of features (dimensions) increases, the data space becomes sparser. Sparse data is challenging for clustering because reliable density estimates are harder to obtain. High-dimensional data can lead to instability and decreased classification accuracy in clustering\cite{dimensionality}. Clustering high-dimensional data is computationally expensive\cite{dimensionality1}. Different dimension reduction techniques such as PCA, UMAP\cite{umap}, t-SNE\cite{tsne}, LDA\cite{lda}, autoencoders\cite{autoencoders} etc. are typically used to reduce the dimensions of the data. Sometimes, the data points are very large and distributed over different servers. PCA is applied locally on each of the data divisions separately and later they get combined together\cite{d1}. These projected datas on the lower dimension can act as a proxy for the original data in k-means clustering\cite{d2}.
	\
	\\\\ 
	 The method we propose, called {\methodname}, partitions the feature sets into multiple slices rather than dividing the data points themselves. The process of division is described in detail in Section \ref{sec:division}. By doing so, we can more effectively analyze and process the features, gaining a deeper understanding of the underlying patterns. {\methodname} partitions the feature set, resulting in multiple smaller subsets of features. In our case, the feature sets denote the genes. Generally principle component analysis (PCA) is done on the whole dataset to reduce the dimension. Here, our method applies PCA after dividing the dataset into several pieces. {\methodname} employs dimension reduction techniques on each of the smaller subsections. These principal components serve as a reduced representation of the original data, capturing the most significant variance within each subset. FeatPCA then combines all the principal components or embeddings to create a representation that represents the entire dataset. This merged dataset, now transformed and dimensionally reduced. By applying PCA to each gene subset and then integrating the results, we ensure that the most informative features from each division contribute to the final analysis. Then {\methodname} runs clustering algorithms on the merged data. {\methodname} also employs denoising autoencoders for data imputation and subsequently applies the same algorithm to the imputed data. This approach leads to superior performance compared to existing state-of-the-art clustering methods.

\section{Related Work}
 In the context of single-cell data, imputation is a common step to handle missing values. Various algorithms are employed for this purpose, and one effective approach is using autoencoders\cite{autoimpute1}. Autoencoders learn the underlying structure of the data and predict missing values based on the available information.  Dimensionality reduction methods play a crucial role in single-cell analysis. PCA is commonly employed to reduce dimensionality by calculating the principal components. These components capture the most significant variation in gene expression across cells, allowing for efficient representation and visualization of the data\cite{pcaapplication}. The Uniform Manifold Approximation and Projection (UMAP) method is commonly employed to reduce dimensionality and visualize single-cell data\cite{umapapplication}. t-Distributed Stochastic Neighbor Embedding (t-SNE) is a dimensionality reduction technique commonly used in single-cell RNA sequencing analysis while preserving local structure\cite{tsneapplication}. Autoencoder is also used to reduce dimensions\cite{autoencoderapplication}. Data partitioning is a fundamental technique in machine learning, particularly when dealing with large datasets\cite{datap1}. In certain scenarios involving large datasets distributed across multiple servers, the data is divided into segments. Each segment is processed independently on its respective server\cite{d1}. Within each segment, dimensionality reduction techniques are applied to reduce the number of features while preserving relevant information. After dimensionality reduction, the reduced data from all segments is combined, serving as a proxy for the original dataset\cite{d2}. This combined reduced data is then used for clustering purposes. When dealing with large datasets and distributed clustering of high-dimensional, heterogeneous data, a technique called Collective PCA can be employed. Collective PCA is specifically designed for distributed scenarios and can be used independently of clustering algorithms. It aims to reduce the dimensionality of the data while preserving essential information\cite{d3}.


\section{Proposed Method}
A simplified diagram of our proposed methodology is shown in Figure \ref{pic:pipeline}. As illustrated in Figure \ref{pic:pipeline}, our proposed method {\methodname} takes scRNA-seq data with a dimension of $n \times d$ as input, where $n$ is the number of cells and $d$ is the number of genes (features). This data is first normalized and subsequently, SCANPY \cite{scanpy} is used to find the highly variable genes, leading to dimension reduction of the genes from ($d$) to ($d'$).  {\methodname} then performs a critical step in the analysis by dividing the data based on its features. The features of the data is partitioned into k equal parts while keeping the cell dimension $n$ the same. The dimensions of the features in each partition become $m=\frac{d'}{k} + o$, where $o$ is the overlapping size. Then {\methodname} applies PCA on each subspace of the data to capture the most variance from each data partition. It was observed that dividing the data into $m$ partitions can help PCA emphasize specific feature sets that capture more meaningful representations, ultimately leading to enhanced clustering accuracy.  {\methodname} then merges the whole data into one section by concatenating the smaller partitions. Finally, {\methodname} applies existing clustering algorithms on the merged data. It was observed that the most variance can be captured for $k$ values from 2 to 20 across all standard scRNA-seq datasets. This is because partitioning the datasets into more segments may emphasize less useful information, which can further decrease clustering performance. 

\begin{minipage}{\textwidth}
		\begin{adjustbox}{width=\textwidth}
			\begin{tikzpicture}
				
				\fill[lightgray, rounded corners=8pt] (0cm,0cm) rectangle (2.5cm,4cm);
				\node[anchor=south west] at (0cm,2cm) {Input data};
				\node[anchor=south west, text width=2.5cm] at (0, 5) {\textbf{Normalize}};
				\node[anchor=south west, text width=2.5cm] at (1, 4) {d};
				\node[anchor=south west, text width=2.5cm] at (-.5, 2) {n};
				
				\draw[thick,->] (2.5,2) -- (3.5cm,2);
				
				\fill[olive,rounded corners=8pt] (3.5cm,0cm) rectangle (6cm,4cm);
				\node[anchor=south west, text width=2.5cm] at (3.5, 2) {Normalized data};
				\node[anchor=south west, text width=2.5cm] at (3.5, 5) {\textbf{Highly Variable gene selection}};
				\node[anchor=south west, text width=2.5cm] at (4.5, 4) {d};
				
				\draw[thick,->] (6, 2) -- (7cm,2);
				
				\fill[teal,rounded corners=8pt] (7cm,0cm) rectangle (9.5cm,4cm);
				\node[anchor=south west, text width=2.4cm] at (7, 1.5) {Data containing variable genes};
				\node[anchor=south west, text width=2.5cm] at (7, 5) {\textbf{Subspacing}};
				\node[anchor=south west, text width=2.5cm] at (8, 4) {d'};
				
				\draw[thick,->] (9.5, 2) -- (10.5cm,2);
				
				\fill[cyan,rounded corners=8pt] (10.5cm,0cm) rectangle (11.5cm,4cm);
				\node[anchor=south west, text width=2.5cm] at (10.6, .4) {\rotatebox{90}{Subspaced data}};
				\node[anchor=south west, text width=2.5cm] at (10.3, 4) {$\frac{d'}{k} + o$};
				
				\fill[cyan,rounded corners=8pt] (11.7cm,0cm) rectangle (12.7cm,4cm);
				\node[anchor=south west, text width=2.5cm] at (11.8, .4) {\rotatebox{90}{Subspaced data}};
				\node[anchor=south west, text width=2.5cm] at (11.7, 4) {$\frac{d'}{k} + o$};
				
				\draw[dashed,-] (12.7, 2) -- (13cm,2);
				
				\fill[cyan,rounded corners=8pt] (13cm,0cm) rectangle (14cm,4cm);
				\node[anchor=south west, text width=2.5cm] at (13.1, .4) {\rotatebox{90}{Subspaced data}};
				\node[anchor=south west, text width=2.5cm] at (13, 4) {$\frac{d'}{k} + o$};
				
				\node[anchor=south west, text width=2.5cm] at (11.5, 5) {\textbf{PCA}};
				
				\draw[thick,-] (14, 2) -- (14.5cm,2);
				\draw[thick,-] (14.5, 2) -- (14.5cm,-3);
				
				\fill[orange,rounded corners=8pt] (10.5cm,-5cm) rectangle (11.5cm,-1cm);
				\node[anchor=south west, text width=2.5cm] at (10.6, -4.6) {\rotatebox{90}{Top components}};
				\node[anchor=south west, text width=2.5cm] at (10.6, -5.5) {m1};
				
				\fill[orange,rounded corners=8pt] (11.7cm,-5cm) rectangle (12.7cm,-1cm);
				\node[anchor=south west, text width=2.5cm] at (11.8, -4.6) {\rotatebox{90}{Top components}};
				\node[anchor=south west, text width=2.5cm] at (11.86, -5.5) {m2};
				
				\draw[dashed,-] (12.7, -3) -- (13cm,-3);
				
				\fill[orange,rounded corners=8pt] (13cm,-5cm) rectangle (14cm,-1cm);
				\node[anchor=south west, text width=2.5cm] at (13.1, -4.6) {\rotatebox{90}{Top components}};
				\node[anchor=south west, text width=2.5cm] at (13.1, -5.5) {mk};
				
				\node[anchor=south west, text width=2.5cm] at (11.5, -1) {\textbf{Merge}};
				
				\draw[thick,->] (14.5, -3) -- (14cm,-3);
				
				\draw[thick,->] (10.5, -3) -- (9.5cm,-3);
				
				\fill[yellow,rounded corners=8pt] (7cm,-5cm) rectangle (9.5cm,-1cm);
				\node[anchor=south west, text width=2.5cm] at (7, -3) {{Merged data}};
				\node[anchor=south west, text width=2.5cm] at (7, -1) {\textbf{Clustering}};
				\node[anchor=south west, text width=3cm] at (7, -5.6) {m1+m2 ..+mk};
				
				\draw[thick,->] (7, -3) -- (6cm,-3);
				
				\draw[blue,rounded corners=8pt] (0cm,-5cm) rectangle (6cm,-1cm);
				
				\fill[blue] (1.2,-2) circle (.7cm) node[white] {Type 1};
				\fill[cyan] (3.2,-2.5) circle (.7cm) node[white] {Type 2};
				\fill[magenta] (5.2,-2.1) circle (.7cm) node[white] {Type 3};
				\fill[gray] (1.2,-3.8) circle (.7cm) node[white] {Type 4};
				\fill[violet] (5,-3.6) circle (.7cm) node[white] {Type 5};
			\end{tikzpicture}
			
		\end{adjustbox}
		\captionof{figure}{Pipeline of the {\methodname} algorithm}
		\label{pic:pipeline}
	\end{minipage}

		\subsection{Data Preprocessing}
		
	We perform standard preprocessing on the scRNA-seq dataset that includes: log-normalization and highly variable gene selection. SCANPY \cite{scanpy} is used to apply normalization and determine the highly variable genes from the dataset. Only the top highly variable genes from the normalized input data are kept in the dataset. The new dimension of the dataset becomes $n\times d'$, where $d'$ is the number of highly variable genes. We chose top 10000 genes from the highly variable genes across all our datasets.



                \subsubsection{Data Imputation} 
                The data containing highly variable genes has missing values in it. {\methodname} uses denoising autoencoder to replace the missing values. TENSORFLOW is used to build the autoencoder. The bottleneck layer contains 50 neurons in our analysis. As illustrated in the figure, {\methodname} uses the values got from the reconstructed input to impute the missing values in the original data.
        
            
		\subsection{Subspace Generation}
		\label{sec:division}
		The most crucial part of our proposed method is generating subspaces based on the features. {\methodname} divides the features into multiple segments and according to that whole dataset is divided into k partitions, where k varies from 2 to 20. For $k = 2$, the dataset gets divided into 2 parts. Subspace generation consists of 4 different approaches for determining meaningful representations from the processed datasets which are given as follows:
		
		
		\begin{enumerate}[i.]
			\item \textbf{Subspacing of the genes into equal parts sequentially :} 
                \label{method:normal}
			
			\begin{algorithm}
				\caption{Sequential Overlapping Subspace}
				\label{alg:normal}
				\begin{algorithmic}[1]
					\Procedure{SequentialOverlappingDivision}{FeatureSet}
					\State Define NumberOfPartitions, PartitionSize, OverlapSize
					\State Initialize StartIndex to 0
					\While{StartIndex $<$ Length(FeatureSet)}
					\State EndIndex $\gets$ StartIndex + PartitionSize
					\State DividedFeatureSets.add(FeatureSet[StartIndex:EndIndex])
					\State StartIndex $\gets$ StartIndex + (PartitionSize - OverlapSize)
					\EndWhile
					\EndProcedure
				\end{algorithmic}
			\end{algorithm}
			
		Approach-1 in	{\methodname} divides the data into k equal parts sequentially based on the features. This approach creates k subsets of data from the original data. During partitioning, overlapping is performed between adjacent partitions. The degree of overlapping depends on the partition size, with the algorithm permitting 20\% to 30\% of the partition size as an overlapping area. The new dimension for the smaller subsets is $d'/k$. The final dimension of each partition is $d'/k + o$, where $o$ is the overlapping size. \label{itemone}. The pseudocode for this process is shown in Algorithm~\ref{alg:normal}.
			
			\item \textbf{Subspacing of the shuffled genes into equal parts sequentially :} 
			\label{method:shuffle}
			\begin{algorithm}
				\caption{Sequential Shuffled Subspace}
				\label{alg:shuffle}
				\begin{algorithmic}[1]
					\Procedure{RandomizedSequentialDivision}{FeatureSet}
					\State Shuffle(FeatureSet)
					\State Define NumberOfPartitions, PartitionSize
					\State Initialize StartIndex to 0
					\While{StartIndex $<$ Length(FeatureSet)}
					\State EndIndex $\gets$ StartIndex + PartitionSize
					\State ShuffledDividedFeatureSets.add(FeatureSet[StartIndex:EndIndex])
					\State StartIndex $\gets$ StartIndex + (PartitionSize - OverlapSize)
					\EndWhile
					\EndProcedure
				\end{algorithmic}
			\end{algorithm}
			
			The partitioning approach-2 is nearly identical to the previous one (\ref{itemone}), except that, in this case, the features are randomly shuffled before creating the partitions. Overlapping is performed here with the same degree. The pseudocode for this process is shown in Algorithm~\ref{alg:shuffle}.
			
			\item \textbf{Subspacing based on random gene selection :} 
                \label{method:random}
			
			\begin{comment}
				\begin{algorithm}
				\caption{Random Selection and Assignment}
				\begin{algorithmic}[1]
				\Procedure{RandomSelectionAndAssignment}{FeatureSet}
				\State Define NumberOfPartitions
				\State Initialize RandomDividedFeatureSets with NumberOfPartitions empty sets
				\For{each Feature in FeatureSet}
				\While{there are unselected genes in $FeatureSet$}
				\State RandomFeaturenIndex $\gets$ Random(0, NumberOfFeatures-1)
				\State RandomParititionIndex $\gets$ Random(0, NumberOfPartitions-1, 1)
				\If{$RandomFeaturenIndex \notin RandomParititionIndex$}
				\State RandomDividedFeatureSets[RandomPartitionIndex].add(Feature)
				\EndIf
				\EndWhile
				
				\EndProcedure
				\end{algorithmic}
				\end{algorithm}
			\end{comment}
			
		
			\begin{algorithm}
				\caption{Random Gene Selection}
				\label{alg:random}
				\begin{algorithmic}[1]
					\Procedure{DivideFeatures}{FeatureSet, k}
					\State Initialize $k$ empty divisions: $D_1, D_2, \ldots, D_k$
					\While{there are unselected genes in FeatureSet}
					\State Randomly select a gene $g$ from FeatureSet
					\State Randomly select a division $D_i$ from $D_1, D_2, \ldots, D_k$
					\If{$g \notin D_i$}
					\State Add $g$ to $D_i$
					\EndIf
					\EndWhile
					\State \textbf{return} $D_1, D_2, \ldots, D_k$
					\EndProcedure
				\end{algorithmic}
			\end{algorithm}
			
			In subspace generation approach-3, we create several buckets (or parts). Genes/features are then placed into these buckets through a random selection process. Specifically, genes are picked one by one at random and assigned to one of the randomly chosen buckets. This continues until every gene has been allocated to at least one bucket. A key rule is that a single bucket cannot contain the same gene more than once. However, different buckets can contain the same gene, allowing for overlap and ensuring that the distribution is similar to previous sections. This approach ensures a randomized yet controlled allocation of genes across multiple divisions. As shown in Algorithm~\ref{alg:random}, the procedure performs random gene selection.
			
			\item \textbf{Subspacing based on clustering of the genes }:
                \label{method:clsuter}
			
			\begin{algorithm}
				\caption{Cluster-Based Subspace}
				\label{alg:cluster}
				\begin{algorithmic}[1]
					\Procedure{ClusterBasedDivision}{FeatureSet}
					\State Define NumberOfClusters
					\State Clusters $\gets$ Cluster(FeatureSet, NumberOfClusters)
					\For{each ClusterIndex from 0 to NumberOfClusters-1}
					\State ClusteredDividedFeatureSets.add(Clusters[ClusterIndex])
					\EndFor
					\EndProcedure
				\end{algorithmic}
			\end{algorithm}
			
			 In approach 4, our algorithm employs the Leiden clustering algorithm to categorize genes into distinct groups. This method is referenced in the literature as Leiden\cite{Leiden}. We used the NETWORKX and IGRAPH packages in Python to construct the graph and the Leiden algorithm for clustering the features. Following the clustering process, the algorithm creates separate sections, each corresponding to one of the identified clusters. If the algorithm identifies 
            $k$ distinct clusters, then 
            $k$ separate divisions will be established. Within each division, genes that share similar characteristics are grouped together based on the clustering results. This organization ensures that each division is a collection of categorically similar genes, reflecting their shared cluster origin. The procedure that performs the clustering is shown in Algorithm~\ref{alg:cluster}
            		\end{enumerate}	
	
		\subsection{Data Extraction from different subspaces}
			
	   Principle component analysis(PCA) is a linear dimensionality reduction technique used for exploratory data analysis, visualization, and data preprocessing. It transforms the original data onto a new coordinate system, emphasizing directions (principal components) that capture the largest variation. The principal components are orthogonal unit vectors that form a basis where individual dimensions are linearly uncorrelated. It consists of mainly 5 stages, which are given below:
			\begin{enumerate}
				\item Standardizes the Range of Continuous Initial Variables, ensuring that all variables have the same scale (mean = 0, standard deviation = 1).
				\item Computes the Covariance Matrix to identify correlations between variables by calculating the covariance matrix.
				\item Computes Eigenvectors and Eigenvalues.
				\item Creates a Feature Vector to decide which principal components to keep based on their importance (determined by eigenvalues).	
				\item Finally, PCA transforms the data using the selected principal components.
			\end{enumerate}
			
	{\methodname} applies PCA on each subdivided section generated from the subspace generation process, leading to dimensionality reduction. PCA is performed to retain 95\% of the variance. As a result, the dimensionality of each section is reduced. As illustrated in Figure \ref{pic:pipeline}, the new dimensions for the gene axis obtained are m1, m2, ... and mk for k divisions.
			
		\subsection{Merging of the extracted datasets} 
			
		The {\methodname} algorithm then merges the k-divided partitions into a single datasets by concatenating them along the gene axis. As shown in Figure \ref{pic:pipeline}, the newly constructed dataset will have a dimension of  $m1 + m2 + ... + mk$, which is the sum of the dimensions of each section.
			
		\subsection{Clustering Algorithm Applied to the Merged Data}
	Any clustering algorithm can be used to determine clusters for our analysis. Our algorithm uses K-means clustering on the final merged dataset to determine the cluster information for each cell \cite{Kmeans_algo}.  We achieved remarkable results by applying the algorithm to the subspaced data. The clustering accuracy significantly improves when the input data is partitioned. These results are presented in Section~\ref{section:result}.
 % Here is an overview of how K-means works.
		
	% 		\subsubsection{K-means}
	% 		The primary goal of k-means clustering is to partition a set of objects (data points) into several clusters. Each observation belongs to the cluster with the nearest mean.\\\\Given a dataset with n observations (where each observation is a d-dimensional real vector), k-means clustering proceeds as follows:
	% 		\begin{itemize}
 %    				\item Initializes k cluster centroids randomly.
 %    				\item Assigns each observation to the nearest centroid (based on Euclidean distance).
 %    				\item Updates the centroids by computing the mean of the data points assigned to each cluster.
 %    				\item Repeats the assignment and centroid update steps until convergence (when the centroids no longer change significantly).
    				
				
	% 		\end{itemize}
		
	% 		\subsubsection{{\methodname} uses K-means clustering}
	% 		{\methodname} utilizes K-means for clustering analysis. 

			
		

		\begin{table}[!ht]
			\centering
			\begin{adjustbox}{width=1\textwidth}
				\begin{tabular}{|l|l|l|l|l|}
					\hline
					Datasets Name & No. of Cells & No. of Genes & No. of Clusters & References \\ \hline
					Yan & 90 & 20214 & 7 & \cite{Yan} \\ \hline 
                    Yan2 & 90  & 20214  & 6 &  \cite{Intestine_data} \\ 
                    \hline
					Pollen & 301 & 23730  & 11 & \cite{Pollen} \\ \hline
                    Goolam & 124 & 41480   & 5 & \cite{Goolam} \\ \hline
                    Deng-rpkms & 268 & 22431   & 10  & \cite{Deng} \\ \hline
                    Fan & 69 & 26357   & 6 & \cite{Fan} \\ \hline
                    Kolod & 704 & 38653  & 3 & \cite{Kolod} \\ \hline
                    
				\end{tabular}
			\end{adjustbox}
			\caption{Summary of the scRNA-seq data used}
            \label{datasets}
		\end{table}

\section{Experimental Analysis}
\label{section:result}
All code used in the experimental analysis was developend using Python 3.8. We used Scikit-learn, Leidanlg, Tensorflow packages for our analysis. Codes are available here: \url{https://github.com/infiniteloop0048/FeatPCA}.
        
	\subsection{Datasets}
        The datasets used in our analyses are summarized in Table \ref{datasets}. \textit{Yan, Pollen, Goolam, Deng-rpkms, Fan and Kolod} datasets were downloaded from 
        \url{https://hemberg-lab.github.io/scRNA.seq.datasets/}{here}; and the Intestine dataset was downloaded from \url{http://cb.csail.mit.edu/cb/scanorama/data.tar.gz}.

\subsection{Performance Measurement}
		{\methodname} uses ARI value to find accuracy between clusters.
			
			\subsubsection{Adjusted Rand Index}
			The Adjusted Rand Index (ARI) is a measure used to evaluate the similarity between two data clusterings. It’s an adjustment of the Rand Index (RI) that corrects for the chance grouping of elements. It works the following way:
			
			\begin{itemize}
				\item \textbf{Rand Index (RI):} It’s a measure of the similarity between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings. The RI score ranges from 0 (no pair classified in the same way under both clusterings) to 1 (all pairs are classified identically).
                Mathematically, the ARI value is calculated as follows - 
                \begin{equation}
                    R = \frac{{a + b}}{{nC2}}
                \end{equation}
                Where, $a$ represents the number of times a pair of elements belongs to the same cluster across both clustering methods. $b$ represents the number of times a pair of elements belongs to different clusters across both clustering methods. $nC2$ is the total number of unordered pairs in the dataset (where (n) is the number of elements).
				\\\\
				
				\item \textbf{Adjusted Rand Index (ARI):} Since the RI can be affected by chance, the ARI adjusts the RI by considering the expected index of agreement by chance. This makes the ARI more robust as it takes into account the possibility of random agreement. The ARI has a range of -1 to 1, where a value close to 1 indicates high similarity between clusterings, and a value close to 0 or negative indicates random or dissimilar clusterings.
				
			\end{itemize}
			
			Mathematically, the ARI value is calculated as follows - 
			\begin{equation}
				ARI = \frac{RI - Expected(RI)}{Max(RI) - Expected(RI)}
			\end{equation}


    \begin{table}[!ht]
        \centering
        \begin{adjustbox}{width=1\textwidth}
            \begin{tabular}{|p{3cm}|p{3cm}|p{3cm}|p{3cm}|}
                \hline
                Datasets Name & Sequencial Subspace & Shuffled sequential subspace & Subspace by random gene selection  \\ \hline
                Yan & 13/19 & 11/19 & 10/19  \\ \hline 
                Pollen & 15/19 & 3/19  & 3/19  \\ \hline
                Goolam & 10/19 & 6/19   & 13/19 \\ \hline
                Deng-rpkms & 16/19 & 12/19   & 12/19  \\ \hline
                Fan & 11/19 & 6/19   & 6/19  \\ \hline
                Kolod & 9/19 & 17/19  & 17/19  \\ \hline
                Intestine & 11/19  & 16/19  & 16/19  \\ \hline
            \end{tabular}
        \end{adjustbox}
        \caption{Win cases for the datasets. In the case of sequential subspace clustering for the “yan” dataset, the value “13/19” indicates that there are 13 instances where the ARI value improves with subspacing compared to using the undivided data.}
        \label{wincases}
    \end{table}

            \subsection{Results for the subspacing variations}
            \subsection*
			{Subspacing of the genes into equal parts sequencially}
			
			\begin{comment}

	
	\begin{figure}[h]
		
		
		\centering
		\begin{minipage}{0.22\textwidth}
			\centering
			\includegraphics[scale=0.17]{plots/final/normal1/yan.png}
			
			
		\end{minipage}
		\hfill
		\begin{minipage}{0.22\textwidth}
			\centering
			\includegraphics[scale=0.17]{plots/final/normal1/pollen.png}
			
			
		\end{minipage}	
		\hfill
		\begin{minipage}{0.22\textwidth}
			\centering
			
			\includegraphics[scale=0.17]{plots/final/normal1/kolod.png}
			
			
		\end{minipage}
		\begin{minipage}{0.22\textwidth}
			\centering
			
			\includegraphics[scale=0.17]{plots/final/normal1/intestine.png}
			
		\end{minipage}
		\begin{minipage}{0.22\textwidth}
			\centering
			
			\includegraphics[scale=0.17]{plots/final/normal1/goolam.png}
			
			
		\end{minipage}
		\begin{minipage}{0.22\textwidth}
			\centering
			
			\includegraphics[scale=0.17]{plots/final/normal1/fan.png}
			
			
		\end{minipage}
		\begin{minipage}{0.22\textwidth}
			\centering
			\centering
			\includegraphics[scale=0.17]{plots/final/normal1/deng-rpkms.png}
			
			
		\end{minipage}
		\caption{ARI value for sequencial division}
		\label{pic:normal}
	\end{figure}
	\end{comment}
	
	\begin{figure}
		\centering
		\begin{subfigure}{0.24\textwidth}
			\includegraphics[width=\linewidth]{plots/final/normal1/yan.png}

		\end{subfigure}
		\hfill
		\begin{subfigure}{0.24\textwidth}
			\includegraphics[width=\linewidth]{plots/final/normal1/pollen.png}

		\end{subfigure}
		\hfill
		\begin{subfigure}{0.24\textwidth}
			\includegraphics[width=\linewidth]{plots/final/normal1/kolod.png}

		\end{subfigure}
		\hfill
		\begin{subfigure}{0.24\textwidth}
			\includegraphics[width=\linewidth]{plots/final/normal1/yan2.png}

		\end{subfigure}
		\hfill
		\begin{subfigure}{0.24\textwidth}
			\includegraphics[width=\linewidth]{plots/final/normal1/goolam.png}

		\end{subfigure}
		\hfill
		\begin{subfigure}{0.24\textwidth}
			\includegraphics[width=\linewidth]{plots/final/normal1/fan.png}

		\end{subfigure}
		\hfill
		\begin{subfigure}{0.24\textwidth}
			\includegraphics[width=\linewidth]{plots/final/normal1/deng-rpkms.png}

		\end{subfigure}
		\caption{ARI value for sequencial subspacing}
		\label{pic:normal}
	\end{figure}
		
			
			
        Figure \ref{pic:normal} presents a series of seven bar plots, each corresponding to a distinct dataset. In every bar plot, 
	
	\begin{itemize}
		\item The initial bar represents the Adjusted Rand Index (ARI) value calculated for the undivided dataset.
		\item The second bar shows the average ARI across subspaces obtained using {\methodname}.
		\item The third bar indicates the maximum ARI among the subspaces.
	\end{itemize}
	
	The figure represents the Adjusted Rand Index (ARI) values for seven datasets when genes subspaceed sequencially. Across all datasets, the maximum ARI from subspaces consistently exceeds the ARI from the undivided dataset. Also for most of the datasets, the average Adjusted Rand Index (ARI) obtained from subspaces is superior. Notably, the ‘yan’ dataset achieves an ARI of 0.523 with the full dataset, but when divided into 14 subspaces, the maximum ARI reaches 0.716—a substantial improvement. Table \ref{wincases} shows the number of cases when subspacing produce better ARI values for the datasets. 
			
			
			
			\subsection*
	{Subspacing of the shuffled genes into equal parts sequencially}
	
	\begin{figure}
		\centering
		\begin{subfigure}{0.24\textwidth}
			\includegraphics[width=\linewidth]{plots/final/shuffle1/yan.png}
			
		\end{subfigure}
		\hfill
		\begin{subfigure}{0.24\textwidth}
			\includegraphics[width=\linewidth]{plots/final/shuffle1/pollen.png}
			
		\end{subfigure}
		\hfill
		\begin{subfigure}{0.24\textwidth}
			\includegraphics[width=\linewidth]{plots/final/shuffle1/kolod.png}
			
		\end{subfigure}
		\hfill
		\begin{subfigure}{0.24\textwidth}
			\includegraphics[width=\linewidth]{plots/final/shuffle1/yan2.png}
			
		\end{subfigure}
		\hfill
		\begin{subfigure}{0.24\textwidth}
			\includegraphics[width=\linewidth]{plots/final/shuffle1/goolam.png}
			
		\end{subfigure}
		\hfill
		\begin{subfigure}{0.24\textwidth}
			\includegraphics[width=\linewidth]{plots/final/shuffle1/fan.png}
			
		\end{subfigure}
		\hfill
		\begin{subfigure}{0.24\textwidth}
			\includegraphics[width=\linewidth]{plots/final/shuffle1/deng-rpkms.png}
			
		\end{subfigure}
		\caption{ARI value for shuffled sequencial subspacing based subspacing}
		\label{pic:shuffle}
	\end{figure}
		
			In Figure \ref{pic:shuffle}, the seven bar plots represent the Adjusted Rand Index (ARI) values for seven datasets when genes are shuffled and subspaced. Across all datasets, the maximum ARI from subspaces consistently surpasses the ARI from the undivided dataset. With the exception of the ‘yan’ dataset, the average ARI obtained from subspaces is superior. Notably, the ‘pollen’ dataset achieves an ARI of 0.802 with the full dataset, but when divided into three subspaces, the maximum ARI reaches 0.835—a substantial improvement. Table \ref{wincases} displays the count of cases where subspacing results in improved ARI values for the datasets.
			
			
			
			\subsection*
			{Subspacing based on random gene selection}
			
			\begin{figure}
		\centering
		\begin{subfigure}{0.24\textwidth}
			\includegraphics[width=\linewidth]{plots/final/bucket1/yan.png}
			
		\end{subfigure}
		\hfill
		\begin{subfigure}{0.24\textwidth}
			\includegraphics[width=\linewidth]{plots/final/bucket1/pollen.png}
			
		\end{subfigure}
		\hfill
		\begin{subfigure}{0.24\textwidth}
			\includegraphics[width=\linewidth]{plots/final/bucket1/kolod.png}
			
		\end{subfigure}
		\hfill
		\begin{subfigure}{0.24\textwidth}
			\includegraphics[width=\linewidth]{plots/final/bucket1/yan2.png}
			
		\end{subfigure}
		\hfill
		\begin{subfigure}{0.24\textwidth}
			\includegraphics[width=\linewidth]{plots/final/bucket1/goolam.png}
			
		\end{subfigure}
		\hfill
		\begin{subfigure}{0.24\textwidth}
			\includegraphics[width=\linewidth]{plots/final/bucket1/fan.png}
			
		\end{subfigure}
		\hfill
		\begin{subfigure}{0.24\textwidth}
			\includegraphics[width=\linewidth]{plots/final/bucket1/deng-rpkms.png}
			
		\end{subfigure}
		\caption{ARI value for randomly selected genes}
		\label{pic:bucket}
	\end{figure}
			
			Figure \ref{pic:bucket} depicts the adjusted Rand index (ARI) values obtained by randomly selecting genes and dividing them into partitions. Notably, specific divisions consistently yield higher ARI values compared to using the entire dataset. For the “deng-rpkms” dataset, the maximum ARI achieved from subspaces is 0.479, surpassing the ARI of 0.406 obtained when the dataset remains undivided. Additionally, the average ARI from the subspaces generally outperforms that of most other datasets. Table \ref{wincases} provides the count of cases where subspacing results in improved ARI values for the datasets.


			\subsection*
			{Subsapcing based on clustering of the genes}
			
				\begin{figure}
		\centering
		\begin{subfigure}{0.24\textwidth}
			\includegraphics[width=\linewidth]{plots/final/gene/yan.png}
			
		\end{subfigure}
		\hfill
		\begin{subfigure}{0.24\textwidth}
			\includegraphics[width=\linewidth]{plots/final/gene/pollen.png}
			
		\end{subfigure}
		\hfill
		\begin{subfigure}{0.24\textwidth}
			\includegraphics[width=\linewidth]{plots/final/gene/kolod.png}
			
		\end{subfigure}
		\hfill
		\begin{subfigure}{0.24\textwidth}
			\includegraphics[width=\linewidth]{plots/final/gene/Yan2.png}
			
		\end{subfigure}
		\hfill
		\begin{subfigure}{0.24\textwidth}
			\includegraphics[width=\linewidth]{plots/final/gene/goolam.png}
			
		\end{subfigure}
		\hfill
		\begin{subfigure}{0.24\textwidth}
			\includegraphics[width=\linewidth]{plots/final/gene/fan.png}
			
		\end{subfigure}
		\hfill
		\begin{subfigure}{0.24\textwidth}
			\includegraphics[width=\linewidth]{plots/final/gene/deng-rpkms.png}
			
		\end{subfigure}
		\caption{ARI value based on performing gene clustering}
		\label{pic:cluster}
	\end{figure}

			
			Figure \ref{pic:cluster} illustrates the ARI values when the genes are divided according to gene clustering. The figure comprises seven barplots, each corresponding to one of the seven datasets. In each barplot, the first bar represents the Adjusted Rand Index (ARI) value for the entire dataset, while the second bar indicates the ARI value for the subspace dataset based on feature clustering. The Figure shows that for 5 out of 7 datasets, certain divisions yield a higher ARI value compared to using the full dataset. For instance, for the "intestine" dataset, the ARI value is 0.440 when using the full dataset (no division). However, when the genes are divided, the ARI value increases to 0.574, significantly higher than that of the undivided data. Similarly, improved results are observed for other datasets when the data is divided, as illustrated in the Figure.
			%The complete table containing ARI values is shown in Appendix \ref{appendix:normal}.
	\subsection{Comparison with other state-of-the-art method}
	
	In our study, we conducted a comparative analysis by benchmarking our results against three state-of-the-art methods: SC3, Seurat, and FEATS. In our study, we conducted a comparative analysis by benchmarking our results against three state-of-the-art methods: SC3, Seurat, and FEATS. In our comparison, we initially imputed missing gene expression values using an autoencoder. Specifically, we replaced the missing entries with values obtained from the autoencoder model. In the \textit{yan} dataset, the highest Adjusted Rand Index (ARI) value (0.8938) was achieved using the \hyperref[method:random]{random gene selection subspacing} method. For the \textit{pollen, goolam, deng, and kolod} datasets, the maximum ARI values were obtained using the \hyperref[method:shuffle]{shuffled subspacing} method. In the case of the \textit{fan} datasets, the maximum ARI value was obtained by applying the \hyperref[method:normal]{sequencial subspacing} method. Remarkably, our method exhibited superior performance across nearly all cases. ARI values of all the methods are given in Table  \ref{comparison}. 
	
	
	\begin{table}[!htb]
		\centering
		\begin{adjustbox}{width=1\textwidth}
			\begin{tabular}{|p{3cm}|p{3cm}|p{3cm}|p{3cm}|p{3cm}|}
				\hline
				Datasets Name & {\methodname} & SC3{\cite{sc3}} & Seurat{\cite{seurat}} & Feats{\cite{feats}}  \\ \hline
				Yan & \textbf{0.8938} & 0.6584 & 0.6297  & 0.8029\\ \hline 
                Yan 2 & \textbf{0.8144} & 0.6549 & 0.6408  & 0.7421\\
                \hline
				Pollen & 0.9142 & 0.9581  & 0.8420 & \textbf{0.9754}\\ \hline
				Goolam & \textbf{0.9561} & 0.5441  & 0.3102 & 0.8518\\ \hline
				Deng-rpkms & 0.4457 & \textbf{0.6659}   & 0.5805 & 0.4562\\ \hline
				Fan & \textbf{0.3692} & 0.3442   & 0.2503  & 0.3677\\ \hline
				Kolod & 0.8111 & 0.1474  & 0.5885  & \textbf{0.9920}\\ \hline
               
			\end{tabular}
		\end{adjustbox}
		\caption{Comparing the Adjusted Rand Index (ARI) values of existing clustering methods with {\methodname}. For the intestine dataset, no results were obtained from Feats.}
		\label{comparison}
	\end{table}
 \vspace{-1cm}
\section{Conclusion}	
		 In our study, we explored the impact of data subspacing along feature sets on clustering accuracy. Notably, dividing the data into subspaces based on relevant features significantly improved clustering results. Specifically, applying Principal Component Analysis (PCA) to these subspaces outperformed applying PCA to the entire dataset when identifying distinct cell types. Among the four methods we investigated, sequential subspacing emerged as the most successful approach, yielding the highest number of “win cases" instances where subspacing improved the Adjusted Rand Index (ARI). However, for datasets with relatively fewer clusters, subspacing via random gene selection also demonstrated promising results. We systematically divided the data into 2 to 20 partitions, consistently observing better outcomes across most divisions. Our future work will delve deeper into understanding the relationship between the number of subspaces and clustering performance. It would be fascinating to predict the optimal subspaces count that maximizes ARI and explore how changes in the number of subspaces impact clustering accuracy. We will also extend our focus to multimodal data and apply the same principles. Our method is independent of the specific clustering and dimension reduction algorithms. However, in this analysis, we utilize PCA for dimensionality reduction and K-means for clustering. In summary, our findings emphasize the importance of thoughtful feature partitioning while clustering.
		\appendix
\begin{comment}
    
		\section{Supplementary Data}
		\label{appendix:normal}
		
		
		\begin{table}[!ht]
			\centering
			\begin{adjustbox}{width=.92\textwidth}
				\begin{tabular}{|l|l|l|l|l|l|l|l|}
					\hline
					No. of divisions & yan & pollen & kolodziejczyk & intestine & goolam & fan & deng-rpkms \\ \hline
					division 1 & 0.52 & 0.77 & 0.16 & 0.399 & 0.15 & 0.11 & 0.39 \\ \hline
					division 2 & 0.57 & 0.82 & 0.18 & 0.43 & 0.16 & 0.008 & 0.40 \\ \hline
					division 3 & 0.68 & 0.73 & 0.15 & 0.41 & 0.16 & 0.08 & 0.43 \\ \hline
					division 4 & 0.63 & 0.80 & 0.17 & 0.40 & 0.16 & 0.17 & 0.44 \\ \hline
					division 5 & 0.513 & 0.79 & 0.17 & 0.46 & 0.16 & 0.12 & 0.40 \\ \hline
					division 6 & 0.64 & 0.71 & 0.17 & 0.33 & 0.15 & 0.029 & 0.41 \\ \hline
					division 7 & 0.54 & 0.81 & 0.17 & 0.42 & 0.15 & 0.12 & 0.39 \\ \hline
					division 8 & 0.53 & 0.84 & 0.15 & 0.36 & 0.15 & 0.08 & 0.40 \\ \hline
					division 9 & 0.61 & 0.79 & 0.15 & 0.38 & 0.15 & 0.13 & 0.42 \\ \hline
					division 10 & 0.50 & 0.79 & 0.15 & 0.398 & 0.16 & 0.038 & 0.42 \\ \hline
					division 11 & 0.65 & 0.84 & 0.16 & 0.53 & 0.15 & 0.099 & 0.48 \\ \hline
					division 12 & 0.67 & 0.81 & 0.17 & 0.41 & 0.15 & 0.094 & 0.46 \\ \hline
					division 13 & 0.53 & 0.79 & 0.145 & 0.41 & 0.145 & 0.11 & 0.44 \\ \hline
					division 14 & 0.72 & 0.84 & 0.17 & 0.385 & 0.162 & 0.14 & 0.46 \\ \hline
					division 15 & 0.53 & 0.81 & 0.17 & 0.38 & 0.148 & 0.12 & 0.37 \\ \hline
					division 16 & 0.69 & 0.79 & 0.145 & 0.407 & 0.15 & 0.146 & 0.404 \\ \hline
					division 17 & 0.513 & 0.801 & 0.147 & 0.39 & 0.153 & 0.20 & 0.41 \\ \hline
					division 18 & 0.67 & 0.77 & 0.147 & 0.49 & 0.152 & 0.189 & 0.38 \\ \hline
					division 19 & 0.48 & 0.78 & 0.149 & 0.40 & 0.27 & 0.0345 & 0.37 \\ \hline
					division 20 & 0.65 & 0.85 & 0.166 & 0.38 & 0.149 & 0.131 & 0.404 \\ \hline
				\end{tabular}
			\end{adjustbox}
			\caption{ARI values for sequential division}
		\end{table}
	
	

		
		\begin{table}[!ht]
			\centering
			\begin{adjustbox}{width=.92\textwidth}
				\begin{tabular}{|l|l|l|l|l|l|l|l|}
					\hline
					No. of divisions & yan & pollen & kolodziejczyk & intestine & goolam & fan & deng-rpkms \\ \hline
					No division & 0.5372539216 & 0.8029128079 & 0.1593037628 & 0.3798318174 & 0.1484784468 & 0.1101966566 & 0.4057725269 \\ \hline
					division 2 & 0.5684817106 & 0.7971761257 & 0.1471339394 & 0.4240995452 & 0.141459559 & 0.1732615676 & 0.4619154756 \\ \hline
					division 3 & 0.5236869535 & 0.835239485 & 0.2055260412 & 0.4327606685 & 0.1871507595 & 0.09400528975 & 0.4935161949 \\ \hline
					division 4 & 0.5236869535 & 0.7711022742 & 0.159468667 & 0.4283716064 & 0.2511640273 & 0.1496728339 & 0.4444453531 \\ \hline
					division 5 & 0.6899428683 & 0.7902289054 & 0.2029617027 & 0.39168458 & 0.141459559 & 0.1015283196 & 0.4813506214 \\ \hline
					division 6 & 0.4928108686 & 0.782948906 & 0.1766590847 & 0.4396545633 & 0.1785162088 & 0.09160847834 & 0.4404703373 \\ \hline
					division 7 & 0.6612969735 & 0.7942989433 & 0.1762286645 & 0.4165213819 & 0.1549783924 & 0.1385886619 & 0.394146947 \\ \hline
					division 8 & 0.4928108686 & 0.7874720799 & 0.1892808315 & 0.3835113437 & 0.1842848508 & 0.1083861582 & 0.3902277052 \\ \hline
					division 9 & 0.5294287203 & 0.7305181836 & 0.1814765947 & 0.3778404759 & 0.1871507595 & 0.03532472819 & 0.4004108235 \\ \hline
					division 10 & 0.5020675316 & 0.7751343064 & 0.1710926205 & 0.4006386551 & 0.141459559 & 0.1075085524 & 0.4791737741 \\ \hline
					division 11 & 0.597970223 & 0.834145599 & 0.1766590847 & 0.3950851335 & 0.1584411977 & 0.04312686146 & 0.4871551987 \\ \hline
					division 12 & 0.6177043292 & 0.7871491102 & 0.1659905969 & 0.4286443702 & 0.1544059055 & 0.08641344324 & 0.4093761079 \\ \hline
					division 13 & 0.5294287203 & 0.7838151323 & 0.1666203364 & 0.3115863717 & 0.1564613578 & 0.1083861582 & 0.4059200804 \\ \hline
					division 14 & 0.5294287203 & 0.8180379036 & 0.2027411307 & 0.4111396475 & 0.1835870741 & 0.1336165907 & 0.4053210042 \\ \hline
					division 15 & 0.6388245101 & 0.7994548767 & 0.1448470525 & 0.4187373944 & 0.1537147066 & 0.07379679477 & 0.4863687701 \\ \hline
					division 16 & 0.6816756344 & 0.7875577739 & 0.1710926205 & 0.4405015215 & 0.141459559 & 0.1076248803 & 0.4463493512 \\ \hline
					division 17 & 0.6506402205 & 0.7407866097 & 0.174096435 & 0.4086330066 & 0.1871507595 & 0.1125702169 & 0.4733202607 \\ \hline
					division 18 & 0.5236869535 & 0.7933590189 & 0.1712199509 & 0.3986580932 & 0.1364856182 & 0.09187230729 & 0.4583608943 \\ \hline
					division 19 & 0.670595594 & 0.7470610774 & 0.1767799596 & 0.3055137354 & 0.140973299 & 0.07722760993 & 0.3858314891 \\ \hline
					division 20 & 0.6700619197 & 0.7976734274 & 0.1635183047 & 0.3977358377 & 0.1833448879 & 0.1418061434 & 0.44699094 \\ \hline
				\end{tabular}
			\end{adjustbox}
			\caption{ARI values for shuffled sequencial division}
		\end{table}



		
		\begin{table}[!ht]
			\centering
			\begin{adjustbox}{width=.92\textwidth}
				\begin{tabular}{|l|l|l|l|l|l|l|l|}
					\hline
					No. of divisions & yan & pollen & kolodziejczyk & intestine & goolam & fan & deng-rpkms \\ \hline
					No division & 0.5236869535 & 0.8049617215 & 0.1593037628 & 0.429002775 & 0.1484784468 & 0.1291258439 & 0.406414722 \\ \hline
					division 2 & 0.5236869535 & 0.7977106103 & 0.1617496354 & 0.4336897242 & 0.151985665 & 0.09160847834 & 0.3970375721 \\ \hline
					division 3 & 0.5236869535 & 0.7961591511 & 0.1593037628 & 0.4321926753 & 0.151985665 & 0.09160847834 & 0.391200547 \\ \hline
					division 4 & 0.6388245101 & 0.7979719675 & 0.1593037628 & 0.3944271129 & 0.1489776888 & 0.1661972948 & 0.4515335974 \\ \hline
					division 5 & 0.6700619197 & 0.811782552 & 0.1497579832 & 0.4218692092 & 0.1489776888 & 0.1003003239 & 0.4799117378 \\ \hline
					division 6 & 0.4987881788 & 0.8410588401 & 0.2166763971 & 0.3292033714 & 0.151985665 & 0.0738094432 & 0.4561897429 \\ \hline
					division 7 & 0.7111633962 & 0.77161334 & 0.1690419573 & 0.4634438831 & 0.1834781892 & 0.08451690643 & 0.4749352703 \\ \hline
					division 8 & 0.5372539216 & 0.778900246 & 0.1520916852 & 0.3965124206 & 0.1605709873 & 0.09757528354 & 0.4729182812 \\ \hline
					division 9 & 0.6506402205 & 0.72144778 & 0.1593037628 & 0.4111002377 & 0.1584411977 & 0.1291258439 & 0.4003340817 \\ \hline
					division 10 & 0.6700619197 & 0.7848819318 & 0.1689812448 & 0.4885268502 & 0.1871507595 & 0.09187230729 & 0.4137637154 \\ \hline
					division 11 & 0.6308004161 & 0.8400413498 & 0.1474310322 & 0.4019274486 & 0.1489776888 & 0.1247155118 & 0.4036203654 \\ \hline
					division 12 & 0.5181877978 & 0.7934984184 & 0.1789837795 & 0.3960725227 & 0.151985665 & 0.1050607066 & 0.4864694227 \\ \hline
					division 13 & 0.4972750041 & 0.7962456769 & 0.171557386 & 0.3975722967 & 0.151985665 & 0.1821075196 & 0.4600835799 \\ \hline
					division 14 & 0.6814888593 & 0.7188299308 & 0.174010343 & 0.4560576409 & 0.1950929803 & 0.1291258439 & 0.4462017683 \\ \hline
					division 15 & 0.4928108686 & 0.8049617215 & 0.1709423934 & 0.4111534138 & 0.1799976648 & 0.1349310087 & 0.4221687996 \\ \hline
					division 16 & 0.5236869535 & 0.8008150313 & 0.171557386 & 0.4266363728 & 0.1999916076 & 0.09108101277 & 0.4558807875 \\ \hline
					division 17 & 0.5294287203 & 0.8130779035 & 0.1635183047 & 0.453494729 & 0.1441630721 & 0.1278883566 & 0.3913721469 \\ \hline
					division 18 & 0.6388245101 & 0.8222350291 & 0.1474388356 & 0.4362088813 & 0.1489776888 & 0.1232538688 & 0.4765227714 \\ \hline
					division 19 & 0.670595594 & 0.6928279702 & 0.1405931488 & 0.3187652109 & 0.1584411977 & 0.1051029793 & 0.4103975037 \\ \hline
					division 20 & 0.5236869535 & 0.7883518778 & 0.1640465951 & 0.3832954844 & 0.1489776888 & 0.09396046155 & 0.4784969644 \\ \hline
				\end{tabular}
			\end{adjustbox}
			\caption{ARI values of divisions based on randomly selected genes}
		\end{table}	
		

		
		\begin{table}[!ht]
			\centering
			\begin{adjustbox}{width=.92\textwidth}
				\begin{tabular}{|l|l|l|l|l|l|l|l|}
					\hline
					division no & yan & pollen & kolodziejczyk & intestine & goolam & fan & deng-rpkms \\ \hline
					No division & 0.5294287203 & 0.7972117173 & 0.1593037628 & 0.4400976093 & 0.151985665 & 0.123359645 & 0.3950136069 \\ \hline
					division based on gene clustering & 0.4928108686 & 0.8133421076 & 0.2197994479 & 0.5743287577 & 0.1572673925 & 0.122273993 & 0.4580803272 \\ \hline
				\end{tabular}
			\end{adjustbox}
			\caption{ARI values of divisions based on gene clustering}
		\end{table}	

	\end{comment}	
			
		
		
		
		\bibliography{references}
		\bibliographystyle{unsrt}
	


\end{document}