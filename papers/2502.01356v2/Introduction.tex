\section{Introduction}
Deep learning methodologies have increasingly been applied to non-Euclidean domains, encompassing irregular structures such as graphs and manifolds. While traditional deep learning operates on grid-like data, such as images and sequences, many real-world problems, ranging from the analysis of social networks to the study of 3D shapes, require direct learning from data defined on non-Euclidean geometric structures. The ability to process and extract meaningful representations of data defined on these complex domains makes these approaches essential for advancing machine learning applications in various fields, including computer vision, medical imaging, and computer-aided design.

The non-Euclidean nature of such data presents several challenges, primarily due to the absence of familiar Euclidean properties such as common coordinate systems, vector space structures, and shift-invariance. As a result, fundamental operations like convolution, which are central to deep learning in Euclidean domains, are not well-defined in these non-Euclidean domains. Previous works have attempted to define convolution on manifolds using various techniques, including spectral methods and spatially defined patch-based constructions. Spectral methods operate in the frequency domain; while elegant, these approaches are computationally expensive, sensitive to the choice of basis, and lack spatial locality. In contrast, patch-based methods seek to define convolution in the spatial domain by transferring local patches onto tangent spaces or employing local coordinate systems. However, these methods often require manually defined metrics. The primary goal of our paper is to explore methods for adapting convolution to non-Euclidean domains, specifically to facilitate deep learning on Riemann surfaces embedded in 3D.

In this work, we introduce Quasi-conformal Convolution (QCC), a novel framework for defining convolution on Riemann surfaces based on quasi-conformal theories. Each QCC operator is associated with a specific quasi-conformal mapping, allowing the convolution operation to be adjusted through the manipulation of this mapping. 
Through a trainable module that generates data-responsive quasi-conformal mappings, QCC allows for the adjustment of convolution operators to align with the specific structures of Riemann surfaces. QCC encompasses a broad range of spatially defined convolutions, enabling the learning of customized convolution operators optimized for specific tasks on each underlying surface. Building on this foundation, we develop the Quasi-Conformal Convolutional Neural Network (QCCNN) to handle tasks involving information defined on Riemann surfaces. QCCNN learns the optimal convolution operator for each underlying Riemann surface based on the specific task, allowing for more accurate results when processing data defined on complex geometric structures. The idea of learning the optimal convolution operator is particularly essential in complex surfaces, where the functions on the surfaces are considered highly distorted compared to planar images.

% \han{One of the key advantages of our framework is its generalization over many patch-based convolution constructions used in previous manifold learning approaches. In our method, a quasi-conformal mapping estimator learns to find an optimal parametrization for the manifold figure maps according to the given data. }
%i think we can focus more on the idea of 'parameterizing' the class of convolution by QC maps...and we can finding the optimal convolution for the input data, by learning the optimal qc map?

To demonstrate the efficacy of the proposed QCC framework, we applied our approach to several tasks on Riemann surfaces. First, we tested our method on the classification of images on curvilinear Riemann surfaces, achieving outstanding results in accurately classifying these images. Next, we applied our framework to craniofacial analysis using 3D human face data, which exhibited promising performance. Additionally, we conducted lesion segmentation on 3D human facial images, and our framework significantly outperformed existing methods, providing more accurate and reliable segmentation of facial lesions. Finally, we performed self-ablation studies to assess the impact of various parameter choices on the performance of the proposed models, further validating the robustness and adaptability of our approach.

To summarize, the main contributions of this work are as follows:
\begin{itemize}
    \item We provide a systematic and comprehensive definition of manifold convolution and parametrized manifold convolution, along with a theoretical investigation of their relationships.
    \item We introduce Quasi-Conformal Convolution (QCC) as a novel method for defining convolution on 2D Riemannian manifolds, encompassing a substantial subset of manifold convolutions.
    \item Building on the proposed Quasi-Conformal Convolution, we design the QCC layer for deep learning on Riemann surfaces, enabling a learnable convolution that adapts to the data rather than relying on a predefined, untrainable approach.
    \item We develop QCC Neural Networks that incorporate the designed QCC layer, making the model versatile for tasks such as classification and segmentation. Experiments demonstrate its advantages over related approaches.
\end{itemize}

% \begin{definition}[PTC]
% Let $K\left(x_0, \cdot\right): \mathcal{M}_{x_0, \delta} \rightarrow \mathbb{R}$ be a compactly supported kernel function centered at $x_0$ with radius $\delta$. We assume $K\left(x_0, y\right)=0$ for $y \notin \mathcal{M}_{x_0, \delta}$ and require the radius of the compact support parameter $\delta$ be smaller than the injective radius of $\mathcal{M}$ to guarantee the bijectivity of the exponential map. Note that this is a very mild assumption, since most modern CNN architectures use filters which are much smaller than the entire image. It is also important to remark that parameterization of $K\left(x_0, \cdot\right)$ can be determined by user. It may be hand designed for specific applications, or be learned as a component of a neural network.

% Our idea of defining convolution on manifolds relies on transporting this compactly supported kernel $K\left(x_0, \cdot\right)$ to every other point on $\mathcal{M}$ in a way of reflecting the manifold geometry. More specifically, given any point $x \in \mathcal{M}$, we first construct a vector field transportation $\mathcal{P}_{x_0}^x: \mathcal{T}_{x_0, \delta} \mathcal{M} \rightarrow \mathcal{T}_{x, \delta} \mathcal{M}$ using the parallel transportation discussed in Section 2.2. Then $K\left(x_0, \cdot\right)$ can be transported on $\mathcal{M}$ as:
% $$
% \begin{aligned}
% K(x, \cdot): \mathcal{M}_{x, \delta} & \rightarrow \mathbb{R} \\
% y & \mapsto K\left(x_0, \exp _{x_0} \circ\left(\mathcal{P}_{x_0}^x\right)^{-1} \circ \exp _x^{-1}(y)\right)
% \end{aligned}
% $$

% Note that the above definition is analogous to convolution in the Euclidean space (1.1). Here, the exponential map $\exp _x^{-1}(y)$ mimics the vector $x-y$, and $\mathcal{P}_{x_0}^x$ is a generalizes the translation operation. In fact, it can be easily checked that the above definition is compatible with Euclidean case by setting the manifold $\mathcal{M}$ to be $\mathbb{R}^2$.

% By plugging (3.1) into (1.2), we can now formally define the parallel transport convolution operation of $f$ which a filter $k$, centered at $x_0$ :
% $$
% \begin{aligned}
% \left(f *_{\mathcal{M}} K\right)(x) & :=\int_{\mathcal{M}} f(y) K(x, y) \mathrm{d}_{\mathcal{M}} y \\
% & =\int_{\mathcal{M}} f(y) K\left(x_0, \exp _{x_0} \circ\left(\mathcal{P}_{x_0}^x\right)^{-1} \circ \exp _x^{-1}(y)\right) \mathrm{d}_{\mathcal{M}} y
% \end{aligned}
% $$
% \end{definition}

% \begin{definition}[GCC]
% Patch operator Kokkinos et al. [24] construct the patch operator as $$ \begin{aligned} (D(x) f)(\rho, \theta) & =\int_X v_{\rho, \theta}\left(x, x^{\prime}\right) f\left(x^{\prime}\right) d x^{\prime}, \\ v_{\rho, \theta}\left(x, x^{\prime}\right) & =\frac{v_\rho\left(x, x^{\prime}\right) v_\theta\left(x, x^{\prime}\right)}{\int_X v_\rho\left(x, x^{\prime}\right) v_\theta\left(x, x^{\prime}\right) d x^{\prime}} . \end{aligned} $$ The radial interpolation weight is a Gaussian $v_\rho\left(x, x^{\prime}\right) \propto$ $e^{-\left(d_X\left(x, x^{\prime}\right)-\rho\right)^2 / \sigma_\rho^2}$ of the geodesic distance from $x$, centered around $\rho$ (see Figure 1, right). The angular weight is a Gaussian $v_\theta\left(x, x^{\prime}\right) \propto e^{-d_X^2\left(\Gamma(x, \theta), x^{\prime}\right) / \sigma_\theta^2}$ of the point-to-set distance $d_X\left(\Gamma(x, \theta), x^{\prime}\right)=\min _{x^{\prime \prime} \in \Gamma(x, \theta)} d_X\left(x^{\prime \prime}, x^{\prime}\right)$ to the geodesic $\Gamma(x, \theta)$ (see Figure 1, center).

% Geodesic convolution (GC) layer replaces the convolutional layer used in classical Euclidean CNNs. Due to the angular coordinate ambiguity, we compute the geodesic convolution result for all $N_\theta$ rotations of the filters, 

% $$ 
% f_{\Delta \theta, q}^{\text {out }}(x)=\sum_{p=1}^P\left(f_p \star a_{\Delta \theta, q p}\right)(x), \quad q=1, \ldots, Q 
% $$

% where $a_{\Delta \theta, q p}(\theta, r)=a_{q p}(\theta+\Delta \theta, r)$ are the coefficients of the $p$ th filter in the $q$ th filter bank rotated by $\Delta \theta=$ $0, \frac{2 \pi}{N_\theta}, \ldots, \frac{2 \pi\left(N_\theta-1\right)}{N_\theta}$, and the convolution is understood in the sense of (7).
% \end{definition}
