\section{Quasi-Conformal Convolutional Neural Network on Riemann Surfaces}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{im/figure_ArchiCompare.png}
    \caption{Illustration of a conventional convolutional neural network with predefined, untrainable convolution operations (top) compared to our proposed model (bottom) featuring learnable, data-driven convolution operations.}
    \label{fig:archi-compare}
\end{figure}

In this section, we aim to introduce the Quasi-Conformal Convolutional Neural Network (QCCNN) using Quasi-conformal Convolution as described in the previous section, which is capable of performing learning tasks on Riemann surfaces. We will present the architecture for a QC convolutional layer, detailing how it can be implemented in various settings.

The proposed QCCNN model incorporates learnable convolution operations that adapt dynamically to data through representing manifold convolution via quasi-conformal mappings. Unlike conventional models that rely on predefined and untrainable convolution operations, our architecture dynamically learns the convolution operation before entering each convolutional layer (Figure \ref{fig:archi-compare}). Specifically, at each layer $\mathcal{G}_i$, an auxiliary module, referred to as the Estimator, generates a mapping $\phi_i$ based on the feature map from the previous layer. This mapping, as described in Definition \ref{them:parameterconv}, serves as a parameterization that defines the learnable convolution operation for the current layer. The resulting feature map from $\mathcal{G}_i$ is then passed to subsequent layers, enabling a cascade of adaptive transformations.

\subsection{QC Convolutional Layer}

In this work, a key innovation lies in utilizing quasi-conformal mappings to learn an optimal parameterized manifold convolution. This section explores how QCC is integrated into deep learning methodologies and introduces the Quasi-conformal Convolutional Layer (QCC Layer), a pivotal component that extends standard convolution operations to 3D surfaces.

The QCC layer shares a similar purpose with conventional 2D convolutional layers: extracting high-dimensional feature maps from input data. However, unlike standard convolutional layers that operate on regular Euclidean grids, the QCC layer is designed to process manifold data $h$, where the convolution operations leverage a learnable, data-driven manifold convolution operation.

As shown in Figure \ref{fig:qcclayer}, the process begins by parameterizing the manifold data $ h $ through a parameterization function $ \phi$, which yields a pullback feature function $ \tilde{h} = h \circ \phi $, providing a common planar domain for subsequent computations. The pullback feature function $ \tilde{h} $ is then passed through the quasi-conformal mapping estimator (QCE), a specialized module that computes a quasi-conformal mapping $ f $. This mapping $ f $ adapts dynamically to the input data, reparametrizing $ \tilde{h} $ into $ h^\# $ as $ h^\# = \tilde{h} \circ f^{-1} $. At this stage, any conventional 2D convolution applied to $ h^\# $ is equivalent to performing a deformable convolution with $ \tilde{h} $, effectively bridging Euclidean and non-Euclidean operations.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{im/figure_qcclayer.png}
    \caption{Illustration of the Quasi-conformal Convolutional layer: The process begins by parameterizing the manifold to establish a common planar domain for computations. A learnable, adaptive convolution is done through a data-driven quasi-conformal mapping. The dashed box highlights the equivalence between deformable convolution on the manifold, deformable convolution on the parameterized domain, and standard 2D convolution on the reparameterized domain via the quasi-conformal mapping.}
    \label{fig:qcclayer}
\end{figure}

Following this principle, the QCC layer allows the warped feature function $ h^\# $ to undergo convolution with a learnable kernel $ k $ in the planar domain. Mathematically, this operation represents a deformable convolution for $ \tilde{h} $, which can further be a deformable convolution with the original manifold data $ h $. Importantly, the kernel itself is deformable, with both its weights and shape dynamically adjusted based on the input data. This adaptability is achieved through the proposed QCE module, which enables the QCC layer to learn not only the optimal weights but also the geometric configuration of the kernels during training.

The output of this convolution operation, $ h^\#_\prime $, is then mapped back to the original manifold structure. This involves reversing the quasi-conformal deformation $ f $ and the parameterization function $ \phi $. This step ensures that the extracted features are mapped back onto the manifold, completing the process.

Importantly, the mapping $ f $ produced by the QCE, which we generally use a UNet indicated as in Figure \ref{fig:QCE}, should satisfy the property of being a quasi-conformal mapping, which requires its associated Beltrami coefficient to remain strictly less than 1. To enforce this condition, additional regularization terms are introduced:  
\begin{equation}\label{eq:losses}
\begin{aligned}
    \mathcal{L}_{\text{bc}} &= ||\mu(f)||_2, \\
    \mathcal{L}_{\text{lap}} &= ||\Delta f||_2,
\end{aligned}
\end{equation}
where $ \mu(f) $ represents the Beltrami coefficient of the mapping $ f $, computed via a Finite Difference Method implementation of Equation \eqref{eq:beleq}. 
To drive the Beltrami coefficient in our model to maintain a supremum norm strictly less than 1, we aim to control the size of this coefficient by minimizing a properly weighted $ \mathcal{L}_{\text{bc}} $. Empirical results demonstrate that our approach effectively reduces the Beltrami coefficient below this threshold, thereby encouraging the mapping $f$ to satisfy the quasi-conformal property. The term $ \Delta f $ denotes the Laplacian of the mapping $ f $, and the regularization $ \mathcal{L}_{\text{lap}} $ is included to promote smoothness in the mapping. Together, these regularizations ensure that $ f $ adheres to the quasi-conformal constraints while maintaining desirable geometric properties, such as continuity and smoothness.

The QCC layer's unique capability to integrate quasi-conformal mappings into the convolution process distinguishes it from traditional manifold learning approaches. By learning and dynamically adjusting both the kernel shape and kernel weightings, the QCC layer offers a robust framework for handling non-Euclidean domains. This adaptability leaves convolution operations dynamic and learnable, resolving a significant challenge in manifold convolution and enhancing the effectiveness of learning on manifold data.


\begin{figure}
    \centering
    \includegraphics[width=0.85\textwidth]{im/architectureQCE.png}
    \caption{The architecture of the QC mapping estimator.}
    \label{fig:QCE}
\end{figure}




\subsection{QC Convolutional Neural Network}
\label{QCCNN}

% \begin{equation}
%    \mathcal{L}(\theta,\phi) = \mathcal{L}_{\text{task}} + \alpha_{\text{bc}} \mathcal{L}_{\text{bc}} + \alpha_{\text{lap}} \mathcal{L}_{\text{lap}}.
% \end{equation}

Then, we delve into the construction of a Quasi-conformal Convolutional Neural Network (QCCNN), which integrates the Quasi-conformal Convolutional layer into a fully functional deep learning framework.

For simplicity in our discussion, we outline the design of a QCCNN where each layer has a single channel. Extending the architecture to multiple channels is straightforward. The network begins with an input manifold data $ h $, and subsequent feature maps $ h_i $ are computed iteratively using the QCC operation with kernel functions $k_i$ in each layer. Mathematically, the forward propagation of features through the network of $n$ layers can be expressed as follows:
\begin{equation}
\begin{aligned}
    h_1\, &= h,\\
    h_2\, &= \sigma(h_1 \ast_{\phi, f_1} k_1),\\
    h_3\, &= \sigma(h_2 \ast_{\phi, f_2} k_2),\\
    &\qquad \vdots\\    
    h_{out} &= \sigma(h_{n} \,\ast_{\phi, f_{n}} k_{n}),
\label{eq:formulationQCCNNmulti}
\end{aligned}
\end{equation}
where $ \sigma $ represents a non-linear activation function, $ k_i $ are the learnable convolution kernels, and $ f_i $ are the quasi-conformal mappings used to define the QCC at layer $ i $. The operator $ \ast_{\phi, f_i} $ denotes the QCC operation defined earlier.


A crucial aspect of the QCCNN is the role of the mapping $ f_i $, which determines the convolution operation on the manifold and governs the deformation of the convolution windows. While $ f_i $ can vary across layers, allowing each layer to learn a layer-specific mapping $ f_i $, a unified mapping $ f $, shared across all layers ($ f_i = f $ for every layer $ i $), is also a viable and meaningful choice. 
Since the first estimator captures most geometric features of the surfaces and outputs an optimal quasi-conformal mapping $f$ for the parametrized convolution, it is reasonable that such mapping $f$ is also effective for the latter layers. Moreover, using independent estimators to generate quasi-conformal mappings at each layer would significantly increase memory and computational costs for the entire network. Therefore, it is more efficient to estimate the quasi-conformal mapping at the beginning of the model and apply this mapping consistently across the QCC layers in the subsequent hidden layers to avoid unnecessary computations.

Considering that a consistent QCC operation across layers is often sufficient, we discuss using a uniform mapping $ f $ for each layer. This leads the QCCNN to the following formulation:
\begin{equation}
\begin{aligned}
    h_1\, &= h, \\
    h_2\, &= \sigma(h_1 \ast_{\phi, f} k_1), \\
    h_3\, &= \sigma(h_2 \ast_{\phi, f} k_2), \\
    &\qquad \vdots\\    
    h_{out} &= \sigma(h_n \,\ast_{\phi, f} k_n).
\label{eq:formulationQCCNNsingle}
\end{aligned}
\end{equation}

According to the formulation in Equation \eqref{eq:parametrizedQCC}, we can rewrite Equation \eqref{eq:formulationQCCNNsingle} above as
\begin{equation}
\begin{array}{rll}
    h_1\, &= h &= h_1^\# \circ f \circ \phi^{-1}, \\
    h_2\, &= \sigma(h_1 \ast_{\phi, f} k_1) &= \sigma(h_1^\# \ast k_1) \circ f \circ \phi^{-1}, \\
    h_3\, &= \sigma(h_2 \ast_{\phi, f} k_2) &= \sigma(h_2^\# \ast k_2) \circ f \circ \phi^{-1}, \\
    &\qquad \vdots \\
    h_{out} &= \sigma(h_n \ast_{\phi, f} k_n) &= \sigma(h_n^\# \ast k_n) \circ f \circ \phi^{-1}.
\end{array}
\end{equation}

From the observation on the above formulation, if one single quasi-conformal mapping is employed for each QCC layer, performing QC convolution between manifold feature function $ h_i $ and a sequence of kernels is equivalent to performing a sequence of kernels on the transformed pullback function $ h_i^\# $ in the parametrized domain. Thus, given the input data $ h $, performing 2D CNN on the transformed pullback function $ h^\# $ is equivalent to performing a QCCNN, which has the same architecture as the 2D CNN with each convolutional layer replaced by a QC convolutional layer under $ \phi $ and $ f $, on the manifold function $ h $.

In other words, given an input feature function $ h $ in a manifold and a QCE $ \mathcal{M}_{\text{QCE}} $, for any 2D CNN $ \mathcal{N} $, we could obtain a QCCNN $ \mathcal{N}_{f}(h) = \mathcal{N}(h^\#) $, where $ h^\# = h \circ \phi \circ f^{-1} $ and $ f = \mathcal{M}_{\text{QCE}}(h \circ \phi) $. The QCCNN $ \mathcal{N}_f $ should have the same architecture as the 2D CNN by replacing all plain 2D convolutions ($ \ast $) in $ \mathcal{N} $ with $ \ast_{\phi, f} $. This discussion demonstrates that to perform such a QCCNN, we can estimate one single QC mapping $ f $ to obtain $ h^\# $ and directly apply a plain 2D CNN on $ h^\# $, as shown in Figure \ref{fig:qccimplementation}. This is equivalent to directly applying a QCCNN on the manifold function $ h $ with a learned QC mapping $ f $.

In the Quasi-conformal Convolution framework, there is no direct pooling operation on the manifold itself. However, pooling, traditionally used to reduce the spatial dimensions of feature maps and adjust the domain's scale for learning, can be effectively implemented using standard 2D pooling operations in the planar parameterization domain. This approach is particularly advantageous in QCC, as the parameterization mapping $f$ can represent any regular manifold convolutions, thereby accommodating deformable pooling operations in a consistent manner.

\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{im/figure_qccimplementation.png}
    \caption{QCC implementation with a single learned quasi-conformal (QC) mapping $f$: The QCCNN is constructed by estimating a single QC mapping $f$, transforming the parameterized input manifold feature into a reparameterized domain, and then applying a standard 2D CNN.}
    \label{fig:qccimplementation}
\end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.6\textwidth]{im/figure_qccVSconv2d.png}
%     \caption{A QC-CNN with each qcc layer with the same mapping $f$ is quivalent to do a 2D CNN on the transformed pullback feature map $h^\#$, if the 2D CNN has the same architecture as the QC-CNN by replacing each QCC layer by a 2D conv layer.}
%     \label{fig:qccVSconv2d}
% \end{figure}
