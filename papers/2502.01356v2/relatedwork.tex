\section{Related Works}
\subsection{Computational Quasi-Conformal Mapping}
Computational quasi-conformal mapping is a powerful tool to control the geometric variation and topology in image science \cite{lam2014landmark} and surface processing \cite{levy2002least,gu2004genus}. Benefitting from the Beltrami representation, the mapping between two different domains can preserve good geometric properties like bijectivity and smoothness, through controlling the Beltrami coefficients with such representation of mappings. Driven by the motivation to preserve different geometric information, ways of parameterization methods are proposed~\cite{gu2003global}. Such convenient representations are also popular and succeed in the computational fabrication community~\cite{Soliman:2018:OCS,Crane:2013:RFC,panetta2019x}. With the capability to handle large deformations, the quasi-conformal method also succeeds in registration for images~\cite{lam2014landmark} and surfaces~\cite{choi2015fast} and segmentation with topology- and convexity prior~\cite{zhang2021topology,zhang2024qis}. In \cite{zhang2022nondeterministic,zhang2022new}, quasi-conformality is used for deformation analysis with uncertainties to study medical images for disease analysis. 


\subsection{Deformable Convolution}

Deformable convolution has been proposed as a solution to the limitations of the traditional convolution operation in Convolutional Neural Networks (CNNs). Jeon \etal proposed the Active Convolution (AC) \cite{jeon2017active}, which integrates a trainable attention mechanism into the convolution operation to adaptively select informative features for different input instances. Another related approach is the Spatial Transformer Network (STN) \cite{jaderberg2015spatial}, which introduces a learnable transformation module that can warp the input feature map based on a set of learnable parameters. Zhang \etal \cite{zhang2024learning,zhang2023deformation} extend it with a Relu-Jacobian regularization to make the produced mapping bijective. By introducing an explicit spatial transformation module, the STN allows the network to learn spatial transformations that can better align the input with the task at hand, leading to improved performance in tasks such as digit recognition and image classification. 

Building on the STN, Dai \etal proposed the Deformable Convolution (DCN) \cite{dai2017deformable}, which extends the idea of spatial transformation to the convolution operation itself, by introducing learnable offsets for each position in the convolutional kernel. This allows the DCN to dynamically adjust the sampling locations of the convolution kernel for each input instance, leading to improved performance on tasks such as object detection and semantic segmentation. However, the original DCN has limitations in handling large deformations and invariance to occlusion. To address these limitations, researchers have proposed several variations, such as the Deformable Convolution v2 (DCNv2) \cite{zhu2019deformable}, which introduces additional deformable offsets for the intermediate feature maps, and the Deformable RoI Pooling (DRoIPool) \cite{dai2017deformable}, which extends the DCN to the task of region-based object detection. However, Luo \etal found that the contribution of each pixel is not equal to the final results in DCN \cite{luo2016understanding}. These findings suggest the need for further improvements in the deformable convolution operation to address its limitations and maximize its performance.

\subsection{Geometric Learning}
In the field of geometric modelling, Bronstein \etal introduced manifold convolution with geodesic patch operators, demonstrating its success in various applications~\cite{bronstein2017geometric, masci2015geodesic}. Similarly, Boscaini \etal utilized an anisotropic heat kernel to define the convolution window, further contributing to the field~\cite{boscaini2016learning}. Other convolution definitions have also succeeded in registration tasks~\cite{bouritsas2019neural, gong2019spiralnet++}. Additionally, the MeshCNN framework by Hanocka \etal is noteworthy, as it redefined convolution using edges rather than vertices, offering a natural and straightforward approach to the concept~\cite{hanocka2019meshcnn}. Schonsheck \etal propose \cite{schonsheck2022parallel} Parallel Transport Convolution to enhance the translation invariance and allow the construction of compactly supported filters in manifold neural networks.