\section{Setup}
\textbf{Task.} We follow the experimental setup described in~\citep{surrogate-models}. For a given input $x$, let $\hat y(x)$ represent the model’s output and $y(x)$ represent the gold label. $R(\hat y, y)$ is the ground truth correctness of $\hat y(x)$. Since we work with multiple choice tasks, $R(\hat{y}, y) = \mathbbm{1}\{\hat{y}(x) = y(x)\}$. 

\begin{wrapfigure}{r}
{0.5\textwidth}
    \centering
    \vspace{-0.1in}
\includegraphics[width=0.5\textwidth]
{sections/figures/direct_confidence_prompt_instruction.png}
\centering
\caption{\centering\textbf{Direct Confidence Prompt Instruction.} Asks the model to directly score its confidence in its answer to a question.}
\vspace{-0.2in}
\label{fig:direct_prompt_inst}
\end{wrapfigure}

 $C(x, \hat y) \in [0, 1]$ is the model’s confidence in $\hat y(x)$ being the correct output for $x$. Our goal is to derive reliable confidence estimates from language models---i.e. higher $C(x, \hat y)$ where $R(\hat y, y)$ is 1 and lower $C(x, \hat y)$ where $R(\hat y, y)$ is 0. Reliable confidence estimates can help prioritize high-confidence outputs and defer low-confidence cases to human experts.

\textbf{Metrics.} We measure the reliability of confidence estimates through selective classification and focus on studying the AUC~\citep{elyaniv2010foundations,liang2022helm}, area under the selective accuracy-coverage curve. The AUC measures the accuracy of a model if it is allowed to abstain on low-confidence inputs. The selective accuracy $A(c)$ is the accuracy of the model on the top $c$ fraction of examples it is most confident about. AUC is computed by aggregating the selective accuracy $A(c)$ across all $c$. We compute the AUC as described by~\citep{surrogate-models}, adding a small amount of Gaussian noise to each confidence score to allow for tie-breaking across different examples with the same confidence score. For a model with reliable confidence estimates, accuracy on a dataset should increase by abstaining on a larger fraction of low-confidence examples.

\begin{wrapfigure}{r}
{0.5\textwidth}
    \centering
    \vspace{-0.1in}
\includegraphics[width=0.5\textwidth]
{sections/figures/relative_confidence_prompt.png}
\centering
\caption{\centering\textbf{Relative Confidence Prompt.} Asks model to compare its confidence in two questions.}
\vspace{-0.1in}
\label{fig:rel_confidence_prompt}
\end{wrapfigure}

Additionally, we also report the AUROC~\citep{hendrycks2017baseline,xiong2023can}, area under the receiver operating characteristic curve, in Appendix~\ref{appendix:auroc_results}. AUROC is a standard classification metric used to measure how well a model can separate correct and incorrect examples at different thresholds. In our setting, we use the outputted confidence scores as the thresholds for measuring AUROC. 

Expected calibration error (ECE)~\citep{Guo2017OnCO, Naeini2015ObtainingWC} is also a standard metric to measure how closely a model's confidence matches its accuracy. However, ECE does not assess a model's ability to discriminate between correct and incorrect answers---a model with accuracy 0.5 can achieve perfect ECE by outputting a confidence of 0.5 for all of its answers. Therefore, we focus our results on the AUC.

\textbf{Datasets.} We measure the quality of confidence estimates produced by the model on 14 challenging multiple-choice question answering datasets: GPQA~\citep{Rein2023GPQAAG}, MedQA~\citep{jin2021medqa}, TruthfulQA (TQA)~\citep{lin2021truthful}, CommonsenseQA (CSQA)~\citep{talmor2019commonsenseqa},
OpenbookQA (OBQA)~\citep{mihaylov2018openbook}, SIQA~\citep{siqa}, and eight diverse MMLU~\citep{hendrycks2021measuring} datasets---professional law (Law), business ethics (Ethics), conceptual physics (Physics),
econometrics (Econ), abstract algebra (Algebra), college chemistry (Chem), computer security (Security), and US Foreign Policy (Policy). We evaluate on 250 examples from the test set of each dataset. We tune the hyperparameters of our approach on a small heldout set for each task, when available, or otherwise use a fixed set of hyperparameters. See Appendix~\ref{appendix:hyperparameters} for more details.

\textbf{Models.} We evaluate our approach on five state-of-the-art models---Llama 3.1 405B~\citep{llama3.1}, GPT-4~\citep{gpt-4}, Gemini 1.5 Pro~\citep{gemini-1.5-pro}, GPT-4o~\citep{gpt-4o}, and Claude 3.5 Sonnet~\citep{claude-3.5-sonnet}.

\section{Absolute Confidence Estimation}
\label{sec:abs_conf_estimation}
Confidence estimation is often done in an \textit{absolute} setting, where a model assesses its confidence $C(x, \hat y)$ independently for each example $x$. Using a model's log probabilities as a measure of its confidence is a common absolute confidence estimation technique. We focus on \textit{linguistic} confidence estimation, where a user interacts with a model in natural language to assess its confidence, without assuming access to a model's internal representations or outputted log probabilities. Linguistic confidence estimation is becoming increasingly important as users interact with language models through chat interfaces, and several state-of-the-art models such as Claude 3.5 Sonnet and Gemini 1.5 Pro provide only API-level access to users.

We compare relative confidence estimation to two popular absolute linguistic confidence estimation methods: 

\textbf{Direct Confidence Prompting.} We zero-shot prompt the language model with an instruction to answer the question and provide a confidence estimate for that answer. Both the answer and confidence are outputted in a single generation, greedily with $T=0$. ~\cite{surrogate-models} study several different instructions for direct confidence prompting including asking the model to rate its confidence on different numerical scales, to reason about its confidence level with a chain of thought, and describe its confidence in words (e.g. ``not sure'', ``sure'', and ``very sure''). We use the direct confidence prompt from ~\cite{surrogate-models} resulting in the highest selective classification AUC across multiple language models. This prompt asks the model to rate its confidence on a scale of 0-1 and provides fake few-shot examples to allow the model to better understand the task. See Figure~\ref{fig:direct_prompt_inst} for the prompt instruction and Appendix~\ref{appendix:prompts} for the full prompt.

\textbf{Self-Consistency Confidence Prompting.} ~\cite{xiong2023can} present an extension to direct confidence prompting where motivated by work on self-consistency prompting~\citep{Wang2022SelfConsistencyIC}, multiple answers and confidences are sampled for a given question to get a more robust confidence estimate. These answers and confidences are aggregated via a post-processing procedure to produce a single answer and confidence score from the samples. See ~\cite{xiong2023can} for more details on the aggregation procedure. We follow the same procedure as ~\cite{xiong2023can}---prompting the model multiple times per question to sample different answers and confidences using the prompt in Figure~\ref{fig:direct_prompt_inst} (full prompt in Appendix~\ref{appendix:prompts}), then aggregating these samples through their post processing technique. We sample at $T=0.7$ and report results for $15$ samples.


