\section{Discussion}

As users increasingly interact with language models through chat interfaces, estimating linguistic confidences by asking the model about its confidence in natural language has become increasingly important. Most current approaches rely on absolute confidence estimates, where the model is asked to judge its confidence for a question in isolation, e.g., “rate your confidence on a scale of 0-1.” However, prior work shows that models struggle with absolute confidence estimation, as they are not specifically trained to produce such estimates~\citep{zhou2023navigating}. As a result, they tend to default to a narrow range of coarse-grained confidences for most questions (e.g., 0.9, 0.95), which fail to convey meaningful distinctions in certainty to users~\citep{surrogate-models}.

In contrast, relative preferences are ubiquitous in real life, from ranking players in games to conducting A/B testing for products. Relative preferences are also highly effective in machine learning. For example, relative annotations of generation quality lead to better reward estimates in RLHF, and models are shown to be better calibrated on multiple-choice questions~\citep{kadavath2022language}, which also involve relative judgments.

Given the challenges with absolute confidence estimation, we propose a shift towards relative confidence estimation. Rather than asking models to directly generate \textit{confidence scores}, we ask them to instead provide \textit{confidence preferences} by comparing their confidence levels across pairs of questions. These preferences can then be converted into confidence scores using rank aggregation methods, such as Elo rating~\citep{elo_ratings} and the Bradley-Terry model~\citep{bradley_terry}. By framing confidence estimation as a simpler binary decision—``more confident'' or ``less confident''—we reduce the complexity of the task and eliminate the need for models to generate fine-grained confidence scores in isolation. To our best knowledge, we are the first work to approach confidence estimation through the lens of relative comparisons.

Our method is further motivated by the notion that, for any given task, questions can be ranked along a spectrum of difficulty for a given model. Harder questions, which the model is more likely to answer incorrectly, should correspond to lower confidence scores. Relative confidence estimation leverages this principle, using pairwise confidence comparisons and rank aggregation to approximate a ranking of questions by ``difficulty'', thereby producing more meaningful confidence estimates.

We show the effectiveness of relative confidence estimation over absolute confidence estimation across a broad range of question answering tasks, demonstrating improved confidence estimates for five state-of-the-art language models.

