\section{Introduction}
To ensure users can make informed decisions when interpreting outputs from language models (LMs), it is crucial to develop methods for accurately gauging their confidence. Language models are widely deployed, yet they remain prone to errors in their outputs. For instance, even state-of-the-art models like GPT-4o and Llama 3.1 405B struggle to solve challenging datasets such as GPQA~\citep{Rein2023GPQAAG} and MATH~\citep{math}. To help users detect mistakes in their generations, models should provide reliable confidence estimates, signaling when their responses are more likely to be incorrect. By leveraging these estimates, users can disregard low-confidence answers or seek expert opinions.


Since users primarily engage with chatbots like ChatGPT~\citep{chatgpt} through language, asking language models to gauge their confidence is a natural tool.
A straightforward approach to this is absolute confidence estimation—--asking the model to directly rate its confidence without further context or grounding, e.g., \textit{``How confident are you on a scale of 0-1?''} However,~\cite{surrogate-models} find that absolute confidences can be too coarse-grained and lack discriminative power. For example, GPT-4 produces the same confidence score of 0.9 for 50\% of examples across 12 datasets, limiting its ability to distinguish between correct and incorrect answers. 
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{sections/figures/intro_fig.png} 
    \caption{\textbf{Relative Confidence Estimation.} We first prompt models to elicit their answers to different questions. For each question $q_i$, we match $q_i$ with $n$ other questions $q_j$ and generate confidence preference data. We ask the model to compare its level of confidence in the pair of questions and decide which question it is \textit{more} confident in answering correctly. We treat the questions and answers as ``players'' in these matchups and the confidence preferences as match outcomes. Leveraging rank aggregation techniques used in competitive games, such as Elo rating, we translate the model's confidence preferences into confidence scores.}
    \label{fig:intro-fig}
    \vspace{-0.2in}
\end{figure}
This may be due to a lack of realistic examples of confidence estimation in training data. For example, ~\cite{zhou2023navigating} find that many examples in the Pile dataset use hyperbolic terms like “I am 100\% confident,” rather than providing more nuanced estimates.

We introduce \textit{relative confidence estimation}, as an alternative to absolute confidence estimation. Rather than asking models to rate their confidence on an answer to a single question, we ask them to compare confidence across different questions: \textit{``Which question are you more confident in answering correctly?''}. Relative comparisons are used in many scenarios as an easier alternative to absolute judgments. For instance, in RLHF, annotators assess which generation is better, rather than assigning direct scores~\citep{instruct-gpt}. ~\cite{kadavath2022language} also show that LMs are better at making relative judgments of correctness by comparing multiple sampled outputs, rather than verifying a single generation. To the best of our knowledge, ours is the first study to explore confidence estimation through relative comparisons.

Figure~\ref{fig:intro-fig} illustrates our method. To estimate confidence for a language model's answers to questions $q_1$, $q_2$, ..., $q_n$, we generate confidence preference data by pairing each question $q_i$  with another question $q_j$  and asking the model, \textit{``Which question are you more confident in answering correctly, $q_i$ or $q_j$?''} We repeat this $n$ times for each question to gather pairwise confidence preferences. We then convert these preferences into confidence scores, treating this as a rank aggregation problem---determining scores or rankings from a set of partial and potentially inconsistent comparisons. Leveraging well-established solutions to rank aggregation like Elo rating~\citep{elo_ratings}, Bradley-Terry~\citep{bradley_terry}, and TrueSkill~\citep{true_skill}, we translate these relative judgments of confidence into confidence scores.

We compare relative confidence estimation to state-of-the-art absolute confidence estimation methods. For absolute confidence estimates, we study direct prompting---eliciting model confidence through a single prompt---and self-consistency prompting---repeatedly prompting the model for its confidence and aggregating the results into a single score through post-processing~\citep{xiong2023can}. 

Our goal is to produce reliable confidence estimates that can allow users to detect potentially incorrect answers from the model, so we study the selective classification AUC which measures how accurate the model is if it is allowed to abstain on some (low-confidence) examples. Additionally, we also report the AUROC (Table~\ref{tab:avg_auroc_results}) to understand how well confidence scores can distinguish between correct and incorrect examples.
We evaluate relative confidence estimation on five state-of-the-art models—--Llama 3.1 405B, GPT-4, Gemini 1.5 Pro, GPT-4o, and Claude 3.5 Sonnet---on 14 challenging multiple-choice question answering tasks (GPQA, MedQA, TruthfulQA, OpenbookQA, SIQA, and 8 diverse MMLU datasets). 

Our approach matches or outperforms both direct confidence estimation and self-consistency methods for 4 out of 5 of these models (for Claude 3.5 Sonnet relative confidences slightly underperform self-consistency methods). For GPT-4o, we see 3.2\% and 1.8\% improvements respectively in AUC. For Llama 3.1 405B, we observe a 6.1\% improvement in the selective classification AUC over direct prompting and 4.9\% gain over self-consistency. Similar improvements are observed with the other models (Section~\ref{sec:results}). Our findings highlight the efficacy of relative confidences and introduce a new way of thinking about confidence estimation. 
