% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{multirow}
% \usepackage[table,xcdraw]{xcolor}
% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{multirow}
% \usepackage[table,xcdraw]{xcolor}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}
\begin{table}[t!]
\centering
\begin{tabular}{@{}cccc|ccc@{}}
\toprule
Category & Dataset & Direct & Hybrid SC & Elo Rating & TrueSkill & Bradley-Terry \\ \midrule
 & GPQA & 0.356 & 0.293 & 0.453 & \cellcolor[HTML]{BAEFFE}\textbf{0.454} & 0.451 \\
 & MedQA & 0.864 & 0.859 & 0.914 & \cellcolor[HTML]{BAEFFE}\textbf{0.918} & 0.915 \\
 & OBQA & 0.926 & 0.959 & \cellcolor[HTML]{BAEFFE}\textbf{0.970} & 0.969 & 0.968 \\
 & Physics & 0.862 & 0.793 & 0.907 & 0.934 & \cellcolor[HTML]{BAEFFE}\textbf{0.938} \\
 & Algebra & 0.378 & 0.448 & 0.467 & 0.466 & \cellcolor[HTML]{BAEFFE}\textbf{0.476} \\
 & Chem & 0.585 & 0.486 & 0.747 & \cellcolor[HTML]{BAEFFE}\textbf{0.751} & 0.746 \\
\multirow{-7}{*}{STEM} & Security & 0.861 & 0.899 & 0.895 & \cellcolor[HTML]{BAEFFE}\textbf{0.910} & 0.908 \\ \midrule
 & Law & 0.749 & 0.747 & 0.813 & \cellcolor[HTML]{BAEFFE}\textbf{0.834} & 0.825 \\
 & Ethics & 0.889 & \cellcolor[HTML]{BAEFFE}\textbf{0.963} & 0.922 & 0.922 & 0.917 \\
 & Econ & 0.711 & 0.703 & \cellcolor[HTML]{BAEFFE}\textbf{0.778} & 0.770 & 0.748 \\
\multirow{-4}{*}{Social Sciences} & Policy & 0.961 & \cellcolor[HTML]{BAEFFE}\textbf{0.993} & 0.989 & 0.987 & 0.990 \\ \midrule
 & TQA & 0.861 & \cellcolor[HTML]{BAEFFE}\textbf{0.899} & 0.876 & 0.874 & 0.877 \\
 & CSQA & 0.837 & \cellcolor[HTML]{BAEFFE}\textbf{0.920} & 0.865 & 0.868 & 0.870 \\
\multirow{-3}{*}{\begin{tabular}[c]{@{}c@{}}Commonsense\\ Reasoning\end{tabular}} & SIQA & 0.830 & \cellcolor[HTML]{BAEFFE}\textbf{0.879} & 0.868 & 0.867 & 0.871 \\ \midrule
 & Average & 0.762 & 0.774 & 0.819 & \cellcolor[HTML]{BAEFFE}\textbf{0.823} & 0.821 \\ \bottomrule
\end{tabular}
\caption{\textbf{Llama 3.1 405B AUCs All Methods.} We show the dataset-level results for Llama 3.1 405B, for the Direct and Hybrid SC absolute confidence baselines and for relative confidence estimation with different rank aggregation methods (Elo Rating, TrueSkill, Bradley-Terry). Relative confidences outperform absolute confidences for all STEM datasets, whereas absolute confidences with self-consistency (Hybrid SC) work best for commonsense reasoning tasks. Overall, relative confidences with TrueSkill rank aggregation lead to a 6.1\% improvement over direct prompting and a 4.9\% improvement over self-consistency prompting.}
\label{tab:llama_auc_results}
\end{table}