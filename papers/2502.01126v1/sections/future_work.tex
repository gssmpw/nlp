\section{Future Work}
\noindent \textbf{Eliciting Confidence Preference Data.} There can be several different ways of eliciting relative confidence judgments. Prompts could allow for ties in confidence or compare confidence across more than two questions. Kahneman-Tversky Optimization (KTO)~\citep{Ethayarajh2024KTOMA} for LM alignment 
achieves DPO~\citep{Rafailov2023DirectPO} levels of performance by using binary signals of desirability for generations. We can apply KTO to confidence preference data generation by asking for binary signals—--confident or not—--and then converting these into relative judgments, ranking “not confident” answers below “confident” ones.\\\\
\noindent \textbf{Rank Aggregation.} In this work, we explore the most popular rank aggregation methods like Elo rating~\citep{elo_ratings}, Bradley-Terry~\citep{bradley_terry}, and TrueSkill~\citep{true_skill}. Another approach to rank aggregation is to represent preference data as a graph, with nodes as questions and directed edges reflecting match outcomes between questions. Since the outcome of some of these matchups can be inconsistent and non-transitive, algorithms like Rank Centrality~\citep{Negahban2012RankCR}, PageRank~\citep{Page1999ThePC}, and Minimum Feedback Arc Set~\citep{Vahidi2024MinimumWF} could be used to reduce cycles in the graph and better manage these inconsistencies.\\\\
\noindent \textbf{Confidence Estimation for Longform Generations.} While we benchmark on multiple-choice tasks, relative confidence estimation can also extend to longform generation. Log probabilities on answer tokens are commonly used for confidence estimation in multiple-choice tasks, but token-level uncertainty doesn't translate well to longform sequences. Moreover, there may be different levels of uncertainty associated with different aspects of a longform generation, e.g. how complete a generation, vs how factual it is, etc. Relative confidence estimation could produce fine-grained confidence scores for different attributes of a longform response by adjusting the prompt for confidence preferences accordingly.\\\\
\noindent \textbf{Alignment with Relative Confidence.} Works like~\cite{Tian2023FinetuningLM} explore using absolute confidence scores to align language models for different attributes such as factuality, without human annotations (RLAIF). Since relative confidences are more calibrated than absolute confidences, we can instead use relative confidences to construct preference pairs for aligning models on different attributes. \\\\
\noindent \textbf{Curriculum Learning with Difficulty Estimates.} We also explore generating relative confidence judgments without revealing model answers (Section~\ref{sec:results}). These scores correspond to difficulty ratings, which could inform curriculum learning by first training on lower-difficulty examples.