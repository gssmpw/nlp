\section{Results}
\label{sec:results}
\input{sections/figures/grouped_bar_chart}
\textbf{Relative confidences outperform absolute confidences.} We compare relative confidence estimates with absolute confidence estimates using direct prompting and self-consistency prompting, and report gains over the best relative confidence approach for each model. Across 14 datasets, relative confidence estimates boost AUC over direct prompting by 6.1\% for Llama 3.1 405B, 4.1\% for GPT-4, 3.2\% for Gemini 1.5 Pro, 3.2\% for GPT-4o, and 1.1\% for Claude 3.5 Sonnet (Figure~\ref{fig:auc_plot}). Compared to self-consistency prompting, relative confidence raises AUC by 4.9\% for Llama 3.1 405B, 1.0\% for GPT-4, 0.8\% for Gemini 1.5 Pro, and 1.8\% for GPT-4o (Figure~\ref{fig:auc_plot}). For Claude 3.5 Sonnet, relative confidences slightly underperform self-consistency prompting (by 0.1\%). See Table~\ref{tab:llama_auc_results} and Table~\ref{tab:gpt4o_auc_results} respectively for the dataset-level AUC results on Llama 3.1 405B and GPT-4o, and Appendix~\ref{appendix:auc_results} for dataset-level results for the other models. Overall, relative confidence improves confidence estimation for 4 of the 5 state-of-the-art models, with Llama 3.1 405B seeing the largest gains, followed by the GPT-4 models and Gemini 1.5 Pro. While confidence estimates for Claude 3.5 Sonnet also improve over direct prompting, the gains are smaller due to Sonnet’s ability to make good absolute judgments of confidence.\\\\
\noindent \textbf{Does chain of thought improve confidence estimates?}
We experiment with augmenting relative confidence judgments in GPT-4o with chains of thought (CoTs). We update the relative confidence prompt for confidence preference data generation (Algorithm~\ref{alg:conf_pref_data}) by asking the model to reason about which question it is more confident in (Appendix~\ref{appendix:prompts}). We apply Elo rating, the best rank aggregation algorithm for GPT-4o, to the CoT confidence preference data to generate confidence scores. However, the CoT confidence estimates fail to improve performance and lead to worse outcomes when the model hallucinates evidence, becoming confident in both options. Overall, incorporating CoTs slightly decreases GPT-4o’s AUC averaged over datasets, from 87.2\% to 86.8\% while also requiring more inference-time compute.
\input{sections/tables/llama3-1_full_auc_table}\\\\
\noindent \textbf{How important are answers in determining confidence?} 
We investigate how important it is for a model to see its own answer to a question in order to gauge its confidence level in correctly answering the question. To assess this, we modify the relative confidence prompt, asking GPT-4o to judge which of the two questions is more \textit{difficult} for it to answer correctly, without providing it access to its own answers to these questions. See Appendix~\ref{appendix:prompts} for the exact prompt. We then apply the same rank aggregation methods to this difficulty preference data and produce confidence scores. This approach drops the average AUC for relative confidence estimation with Elo rating by 5.3\% from 87.2\% to 81.9\%, emphasizing that access to its own answers significantly enhances the model’s relative confidence judgments. Nevertheless, even without answers, relative confidence judgments are only 2.1\% less reliable than absolute confidence assessments with answers (81.9\% vs 84\%), suggesting that models are still reasonably good at judging a question's difficulty, even before answering it. \\\\

% \usepackage{booktabs}
\begin{wraptable}{r}{0.55\textwidth}
\begin{tabular}{@{}ccc@{}}
\toprule
\# Model Calls & $\%$ Gains GPT-4o & $\%$ Gains Llama 3.1 \\ \midrule
5 & $0.9\%$ & $2.2\%$ \\
10 & $1.8\%$ & $3.2\%$ \\
15 & $1.8\%$ & $4.9\%$ \\ \bottomrule
\end{tabular}
\caption{\textbf{Gains by scaling up comparisons.} We report the gains of relative confidence estimation over self-consistency across different numbers of model calls. }
\label{tab:scaling_comparisons}
\end{wraptable}

\noindent \textbf{Does scaling up comparisons help?} We hypothesize that increasing the number of relative confidence comparisons per question would lead to a better ranking of questions by confidence, and more reliable confidence scores. To test this, we scale up the number of judgments, going from 5 to 10 to 15 model calls per question. To ensure a fair comparison based on compute, we use a self-consistency baseline with the same number of model calls per confidence estimate (Section~\ref{sec:abs_conf_estimation}). We report improvements based on the best rank aggregation method for each model in Table~\ref{tab:scaling_comparisons}.
Even for a small number of model calls, relative confidences show improvements over self-consistency prompting. Further scaling up the number of relative confidence comparisons per question increases the improvements of relative confidence estimation over self-consistency prompting. However, as seen with GPT-4o, for some models further scaling model calls may show diminishing returns due to inherent noise in the model’s confidence preferences.
\input{sections/tables/gpt-4o_full_auc_table}\\\\
\noindent \textbf{Different methods for rank aggregation.} We evaluate multiple rank aggregation methods for converting relative confidence preferences into scalar scores. Relative confidence estimation with any rank aggregation method outperforms direct and self-consistency prompting (Figure~\ref{fig:auc_plot}) (except for slightly underperforming self-consistency prompting with Claude 3.5 Sonnet). While differences in the performance of the rank aggregation methods is small, TrueSkill is the best method for most models, except for Gemini 1.5 Pro where Bradley-Terry performs best and GPT-4o where Elo rating performs best. 

TrueSkill explicitly models player skill levels as probability distributions instead of single point estimates, as in Elo rating and Bradley-Terry. This allows it to capture uncertainty in each player’s skill rating and update it as they participate in more games, which may allow this method to be more robust to the noise in the relative comparison data. In general, for relative confidence estimation with a new model, we would recommend starting with TrueSkill rank aggregation. The online learning paradigm of Elo rating and TrueSkill may also be particularly suited to environments where confidence judgments accumulate over time, leading to more refined confidence estimates (i.e. confidences of a medical chatbot improving as it helps more patients), in contrast to Bradley-Terry where confidence scores are optimized over the full dataset of judgments at once.
