\section{Relative Confidence Estimation}
Linguistic confidence estimation, where a model is prompted to assess its own confidence, is typically done through absolute estimation methods, in which the model independently gauges its confidence for each question. However, without clear training examples demonstrating how to estimate confidence, the model may struggle to distinguish between different confidence levels (e.g., 85\% vs. 90\%) and generate appropriate scores. In contrast, it may be easier for the model to compare its confidence across different questions, making a simpler, binary judgment about whether it is \textit{more} or \textit{less} confident in answering one question versus another. This approach provides more grounding, as confidence is evaluated relative to another question, rather than globally assessed via a direct score. By aggregating many such relative comparisons, we can still derive global confidence estimates (e.g., determining whether a question is one the model is highly confident in answering correctly).

We propose relative confidence estimation, where the model compares pairs of questions, along with its answers, and provides preference judgments on which question it is \textit{more} confident in answering correctly. Given a set of $m$ questions and their corresponding answers, our task is to elicit pairwise confidence preferences and use these preferences to derive meaningful confidence scores for each question. This process involves two stages: Confidence Preference Data Generation (Section~\ref{subsec:conf_pref_data_gen}) and Rank Aggregation (Section~\ref{subsec:rank_aggregation}).

\input{sections/figures/algorithms_data_elo}
\subsection{Confidence Preference Data Generation}
\label{subsec:conf_pref_data_gen}
To generate confidence preference data, we employ the following procedure: for each question $i$, pair it with a randomly selected question $j\neq i$. The model compares the two questions, alongside its answers to the questions, and is then asked which one it feels more confident about answering correctly (Figure~\ref{fig:rel_confidence_prompt}). The answer for each question is obtained using the same prompt used for direct confidence prompting (Figure~\ref{fig:direct_prompt_inst}).
This process is repeated $n$ times for each question $i$, pairing it with different questions $j$ and recording the model's preferences (Algorithm~\ref{alg:conf_pref_data}).
The result is a list of confidence preference judgments $(i, j)$ indicating the model was more confident in answering question $i$ than question $j$ (or $(j, i)$ if model preferred question $j$ to $i$). 

Once this data is gathered, we move to the next step: aggregating these preferences to rank questions by confidence and using this to produce confidence scores (Section~\ref{subsec:rank_aggregation}). 

\subsection{Rank Aggregation}
\label{subsec:rank_aggregation}
Confidence preference data provides partial rankings of questions based on confidence. For instance, given questions 1, 2, and 3, the model may indicate 3 > 2 and 2 > 1. These partial rankings can be aggregated into a total ordering of questions by confidence, enabling the derivation of question-level confidence scores. This process, known as \textit{rank aggregation}, is well-studied in social choice theory for voting, consensus formation, and preference aggregation~\citep{arrow-social-choice, Tideman1987IndependenceOC, kemeny-young, Dwork2001RankAM}.

\input{sections/figures/bradley_terry_alg} The ideal ranking would place all correctly answered questions above incorrectly answered ones, reflecting a calibrated model's confidences. With a complete set of noiseless comparisons--—where correctly answered questions are consistently preferred—--a total ordering could be derived by straightforward sorting. However, our confidence preference data is \textit{noisy} (e.g., incorrectly answered questions are sometimes preferred), \textit{inconsistent} (e.g., occasional circular preferences among questions), and \textit{incomplete} (limited to \textit{n} comparisons per question for tractability).

Given these challenges, we aim to approximate the best total ordering that represents the confidence preference data while being robust to noise, inconsistency, and incompleteness. While finding the optimal total ordering (Kemeny-optimal solutions~\citep{kemeny-young}) is NP-hard, efficient approximation algorithms can provide practical solutions.

We explore three popular algorithms to perform rank aggregation and assign confidence scores based on our preference data: Elo rating, TrueSkill, and Bradley-Terry. These algorithms are typically used to score player skill levels in tournament-style games based on matchup data. 
In this setting, each question is treated as a ``player'' engaging in matchups with other questions, where the model’s confidence preferences dictate the outcomes of these matches.

\noindent\textbf{Elo Rating.}
Elo rating~\citep{elo_ratings} is commonly used in games like chess and leverages matchup data between players to iteratively update player ratings in an online learning fashion. We start by assigning all questions identical scores. For any pair of questions $i$ and $j$, the probability of $i$ ``winning'' the matchup is modeled as a logistic function of $i$ and $j$'s current scores $s_i$, $s_j$. $K$ determines how sensitive the player scores are to match outcomes. 
\begin{equation}
    P(i \text{ wins}) = \frac{1}{1 + 10^{(s_i-s_j)/K}}
\end{equation}


After each matchup the scores are adjusted based on how significantly the estimated win probabilities deviated from the true outcome (i.e. the model’s preference)---surprising outcomes (low-confidence wins) lead to more substantial score changes. We iterate over the confidence preference data multiple times to ensure score convergence. See Algorithm~\ref{alg:elo_scoring} for more details.

\noindent \textbf{TrueSkill.} TrueSkill~\citep{true_skill} is a Bayesian model designed for ranking players in competitive games. It is an extension of the Elo rating system that represents each player's skill score as a normal distribution, with the mean ($\mu$) indicating the best estimate of their current score and the variance ($\sigma$) reflecting the model's uncertainty about
that score. After each matchup between a pair of questions, the mean and variance of each question's scores are updated based on the difference between the expected result and the true outcome. The TrueSkill model uses factor graphs to represent the probabilistic relationships between player skill levels. A belief propagation algorithm is used on the factor graph to update beliefs about players' skills based on match outcomes. As more matchup data is processed for each question, the uncertainty ($\sigma$) decreases, refining the estimate of the question’s score over time. We leverage the trueskill Python package as the implementation of this technique. \\\\
\noindent\textbf{Bradley-Terry.} The Bradley-Terry model~\citep{bradley_terry} is a probabilistic framework for modeling pairwise comparisons, commonly used in ranking tasks. It provides an alternate means of modeling the probability of question $i$ winning a matchup against question $j$, based on their underlying scores. Bradley-Terry estimates the probability that question $i$ wins over question $j$ as:\\
\begin{equation}
    P(i \text{ wins}) = \frac{s_i}{s_i + s_j}
\end{equation}
where $s_i$ and $s_j$ are the scores for question $i$ and $j$. These scores are optimized using maximum likelihood estimation (MLE), with L2 regularization applied to control for overfitting and mitigate the impact of noisy comparisons. Bradley-Terry uses a different estimate of the player win probability than Elo rating.
Additionally unlike Elo, which updates scores iteratively after each comparison, the Bradley-Terry model optimizes the scores holistically, taking all pairwise comparisons into account simultaneously. We use the BFGS algorithm to perform this optimization. See Algorithm~\ref{alg:bradley_terry} for more details.\\\\
We optimize the rank aggregation hyperparameters using a small held out set (Appendix~\ref{appendix:hyperparameters}). Finally, we normalize the confidence scores to a range of 0-1 using min-max normalization.

