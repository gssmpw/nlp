\section{Related Work}
\textbf{Confidence Estimation.}  Recent studies have explored confidence estimation in language models. ~\citet{kadavath2022language} measure the calibration of outputted log probabilities from language models and find that models generally demonstrate good calibration on true/false and multiple-choice tasks. They also show that models can better estimate their confidence in an answer by comparing multiple answers for a given question. Our approach instead asks models to compare their confidence across different questions and finds this leads to reliable confidence estimates. ~\cite{surrogate-models} show that absolute linguistic confidence estimation (e.g. “Score your confidence from 0-1”) is a hard problem for closed models, and confidences for closed models can instead be estimated by transferring log probabilities from open models. Our work instead focuses on linguistic confidence estimates, without needing access to a model’s log probabilities. Other works on linguistic confidence estimation use self-consistency-like methods to sample multiple answers and corresponding confidences from models and aggregate them~\citep{xiong2023can}. We compare relative confidence estimation with the best performing self-consistency technique from~\cite{xiong2023can} and find that relative confidences tend to outperform self-consistency based estimates. Other approaches fine-tune language models to improve confidence estimation~\citep{lin2022teaching}, while our method elicits better estimates without requiring further training. 

\textbf{LMs as Evaluators. } Several works also use language models to evaluate the quality of a model’s responses. GPTScore~\citep{Fu2023GPTScoreEA} and LLM-as-a-judge~\citep{Zheng2023JudgingLW} use LMs to provide automated scoring or feedback on different aspects of text quality as an alternative to traditional text evaluation metrics such as ROUGE and BLEU. These approaches are similar to absolute linguistic confidence estimation (“Score your confidence from 0-1”). Other works use LMs to evaluate their responses through either a numerical score or natural language feedback to improve their own generations. This can occur through search at decoding time~\citep{Yao2023TreeOT}, prompting the model to self-correct its responses using its feedback~\citep{Madaan2023SelfRefineIR, Bai2022ConstitutionalAH}, or by aligning a model using its own reward signals~\citep{Yuan2024SelfRewardingLM}. Linguistic confidence estimation relates to self-evaluation with LMs, since we ask models to evaluate their own confidence levels. 

\textbf{Learning from Human Preference Data.} Several approaches have improved language models across diverse attributes (safety, fluency, etc.) by deriving a reward signal from human preferences. These preferences are typically framed as relative judgments by asking annotators to select their preferred output from a pair or set of responses for a given input, instead of asking them to directly score the quality of a single response~\citep{instruct-gpt, Ziegler2019FineTuningLM, Christiano2017DeepRL}. Motivated by this framing, we elicit relative confidence judgments from LMs and use these to produce more reliable confidence scores.

\textbf{Rank Aggregation.} There is a rich body of work studying the problem of rank aggregation--–converting partial orderings over a set into a better total ordering~\citep{arrow-social-choice, Tideman1987IndependenceOC, kemeny-young, Dwork2001RankAM}. This problem is common in domains such as sports and competitive games, election voting, and product recommendations. Our work leverages popular rank aggregation algorithms such as Elo rating~\citep{elo_ratings}, TrueSkill~\citep{true_skill}, and Bradley-Terry~\citep{bradley_terry} to convert the pairwise confidence preferences from a model into a total ordering of questions and corresponding answers by confidence. Other approaches such as Rank Centrality~\citep{Negahban2012RankCR} model rank aggregation through a Markov Chain and use the stationary distribution to determine the rank of each item.

\textbf{Calibration and Selective Classification.} The quality of confidence estimates is often measured through calibration---by determining how grounded the confidences are in true correctness~\citep{murphy1977reliability,degroot1983forecasters,naeini2014binary,guo2017calibration}, typically through the expected calibration error (ECE). However, the ECE cannot capture how well confidences distinguish between correct and incorrect examples: outputting the same confidence for all examples can lead to perfect ECE if the confidence matches the average model accuracy. This leads us to focus on selective classification~\citep{elyaniv2010foundations,khani2016unanimity,feng2019selective,jones2021selective} which measures if the model “knows what it doesn’t know” and can achieve high accuracy by abstaining on examples where it is uncertain.

