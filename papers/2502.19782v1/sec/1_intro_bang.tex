\section{Introduction}
\label{sec:intro}

The advancements in 3D technologies have led to an increased demand for automated analysis of 3D shapes. Among the related tasks, 3D part segmentation plays a pivotal role in supporting a wide spectrum of applications, including robotics and AR/VR. 

With the introduction of deep neural networks \cite{qi2017pointnet, qi2017pointnet++, thomas2019kpconv, xu2021paconv, zhao2021point} and labeled 3D datasets \cite{wu20153d, chang2015shapenet}, 3D part segmentation has seen remarkable progress in recent years through supervised training. Nonetheless, creating 3D datasets is expensive and time-consuming. Compared with image data, current 3D part-annotated datasets are orders of magnitude smaller in scale. Within the limited 3D data, the human category represents only a tiny fraction. Existing human parsing methods have been trained to segment clothed data \cite{bhatnagar2019multi, musoni2023gim3d, antic2024close} or underlying body parts \cite{bogo2014faust,takmaz20233d}, but they fall short of generalizing to unseen models and classes. Thus, enabling machines with the ability to parse objects into semantic parts and generalize to new categories still remains difficult, especially for human-related data, which usually contain more complex geometry with richer semantic attributes than general 3D objects.


Recent developments in vision-language learning gave rise to many 2D image-based models \cite{radford2021learning, li2022grounded} with exceptional zero-shot generalization capabilities. Many works seek to transfer 2D knowledge to 3D through pre-trained image-language models. \cite{zhang2022pointclip, zhu2023pointclip, abdelreheem2023satr, liu2023partslip, zhou2023partslip++} leverage these models through multi-view rendering and aggregate the information in 3D for the final segmentation result. Another line \cite{umam2024partdistill} focuses on distilling the information for a better 3D model. While these methods have shown promising improvements in object data, they have not exhibited the same quality of results on 3D human data.


% we propose a way to transfer knowledge from 2D to 3D through SAM-based part segmentation on rendering and a novel lifting algorithm using finetuned to fuse inconsistent masks and build correlations with desired semantic labels

In this paper, we aim to bring the open-vocabulary 3D part segmentation performance to human data. We introduce the first framework for 3D human parsing that semantically segments whatever parts you want and supports various 3D representations, including meshes, point clouds, and 3D Gaussians \cite{kerbl20233d}. Inspired by \cite{cheng2021per}, we formulate the segmentation task as a mask classification problem. Firstly, we generate class-agnostic instance mask proposals on rendered images through a pre-trained 2D segmentation model, SAM \cite{kirillov2023segment}. Secondly, we propose a novel HumanCLIP model that encodes each mask into embeddings within the CLIP feature space. Compared to the vanilla CLIP model \cite{radford2021learning}, HumanCLIP produces more accurate text-aligned embeddings for human-related cases, enhancing the precision of the final segmentation. Since mask proposals are class-agnostic and independent between views, we propose a novel MaskFusion module that simultaneously classifies the semantic labels given the text prompt and fuses the multi-view inconsistent masks to generate 3D semantic segmentation for the input. It decouples the mask proposal step from reliance on text prompts, thereby enhancing inference efficiency. In summary, our contributions mainly include:
% achieves highly competitive per-
% formance compared to the fully supervised counterpart. 
\begin{itemize}
    \item We introduce the first open-vocabulary framework that segments fine-grained parts for 3D humans.
    \item We present a HumanCLIP model capable of extracting discriminative CLIP embeddings for human-centric data. 
    \item We propose a MaskFusion module that simultaneously classifies semantic labels and converts multi-view 2D proposals into 3D segmentation, which significantly enhances inference efficiency. 
    \item Our framework shows state-of-the-art performance on five 3D human datasets and shows compatibility with various 3D representations including 3D Gaussian Splatting.
\end{itemize}

% New dataset for HumanCLIP