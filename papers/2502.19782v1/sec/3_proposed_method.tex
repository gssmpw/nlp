\section{Proposed Method}

\subsection{Overview}

The overview of the proposed framework is shown in Figure \ref{Fig.model}. We assume a point-based 3D shape with size $P$ as the input. Given the 3D human model and $K$ semantic text prompts, the goal is to parse the 3D human into segments that semantically correspond to the input prompts. 


Inspired by recent achievements in 2D and 3D segmentation methods \cite{cheng2022masked, ding2022decoupling, zhou2022lmseg, schult2023mask3d, takmaz20233d}, we formulate the semantic segmentation task as mask classification, originated from MaskFormer \cite{cheng2021per}. To bridge 3D data with 2D pre-trained models, we render the input from $V$ predefined camera views. Segment-Anything-Model (SAM) \cite{kirillov2023segment} is leveraged to generate mask proposals for each view (Section \ref{mask_proposals}). We introduce the novel HumanCLIP model, which encodes these proposals into embeddings within the unified CLIP feature space (Section \ref{mask_embeddings}). To lift 2D labels into 3D, it is usually required to assign ``super points'' and have carefully designed voting and grouping. In this work, we present a simple MaskFusion module. It takes HumanCLIP encoded text prompts and simultaneously performs classification and multi-view aggregation without the need for complex operations (Section \ref{mask_fusion}). Note that the generation of mask proposals and embeddings is performed just once per model. Subsequently, segmentation can be executed in just a few milliseconds per prompt, significantly enhancing efficiency compared to previous methods.


% Apart from the HumanCLIP, our framework requires no additional training with 3D data. 


% The following sections describe in detail the process of obtaining the 3D masks and corresponding embeddings.

\subsection{Multi-view Mask Proposals}
\label{mask_proposals}
% - Use SAM to generate class-agnostic 2D masks and unproject to 3D.
We choose SAM \cite{kirillov2023segment} to generate mask proposals on multi-view rendered images. SAM demonstrates remarkable zero-shot capabilities in image segmentation. From the predefined camera poses, we render the 3D human into $V$ RGB images $I_i \in \mathbb{R}^{H\times W \times 3}$ where $i \in [1, V]$ is the index of the view. Each image $I_i$ is then independently fed into SAM in the "segment everything" mode to generate $N_i$ class-agnostic overlapping masks: 
\begin{equation}
    [m_{i, 1}^{2D}, ..., m_{i, N_i}^{2D}] = SAM(I_i)
\end{equation}
where $m_{i, j}^{2D}$ is the $j$-th 2D mask generated by SAM from the $i$-th view.
This results in a total of $N=\sum_{i=1}^V N_i$ binary 2D masks at a varying granularity of whole, part, and subpart. 

Each 2D mask $m_{i, j}^{2D}$ is then unprojected to 3D using the camera parameters of view $i$ to construct the corresponding 3D proposal $m_{i, j}^{3D}$.
% The 2D masks are unprojected to point cloud with size $P$. 

\subsection{HumanCLIP Encoding}
\label{mask_embeddings}
We propose HumanCLIP to generate proposal embeddings with size $C$, where $C$ is the embedding dimension of a CLIP model. The unified image-text feature space of CLIP allows the framework to perform open-vocabulary mask classification. Each 2D mask $m_{i, j}^{2D}$ with its corresponding image $I_i$ is fed to the image encoder to get the proposal embedding $q_{i, j} \in \mathbb{R}^{C}$. 

It is well-discussed that the vanilla pre-trained CLIP encoder does not perform well on specialty-formed inputs \cite{liu2023partslip,liang2023open}, including masked and cropped images. Moreover, masking or cropping an image results in the loss of crucial contextual information, which is essential to the understanding of the specific area in an image. Therefore, we adopt the design of AlphaCLIP \cite{sun2024alpha} to build our image encoder. As shown in Figure \ref{Fig.alphaclip}, the encoder accepts an additional alpha channel as input, which highlights the region of interest on the original rendered images. The input mask is processed with a parallel convolution layer to the RGB image and combined to go through a series of attention blocks to produce the final mask embedding in CLIP feature space. To further mitigate the domain gap, we finetune the encoder on a dataset of over 1.3 million RGBA region-text pairs with human-centric contents. We visualize the image-text alignment before and after fine-tuning in Figure \ref{Fig.finetune}. The pre-trained AlphaCLIP model fails to provide well-aligned embeddings for small parts such as the glasses as well as to distinguish left and right parts. The proposed HumanCLIP model generates more discriminative mask embeddings, facilitating the downstream classification tasks. 

\begin{figure}[t]
    \centering
    \includegraphics[width=.9\hsize]{figures/alphaclip.pdf}
    \caption{AlphaCLIP Image Encoder.} 
    \label{Fig.alphaclip}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=.98\hsize]{figures/finetune.pdf}
    \caption{Comparison between (a) pre-trained AlphaCLIP and (b) the proposed HumanCLIP. The plots show the cosine similarity between the embedding of the masked region corresponding to \textit{face}, \textit{glasses}, \textit{left shoe}, and \textit{right shoe} and their text embeddings.} 
    \label{Fig.finetune}
\end{figure}

% \begin{table}
%     \centering
%     \caption{Statistics about mask data used to train HumanCLIP}
%     \label{tab:humanclip_data}
%     \resizebox{.98\columnwidth}{!}{
%     \begin{tabular}{c|ccc|c}
%        Dataset & Images & Original Masks & Generated Masks & Total Masks \\ \hline
%        LIP & 30462 & 173578 & 91505 & 265083 \\
%        ATR & 17706 & 175604 & 87698 & 263302 \\
%        DeepFashion & 12701 & 100632 & 43404 & 144036 \\
%        CIHP & 28280 & 647072 & 63616 & 710688 \\ \hline
%        \textbf{HumanCLIP} & \textbf{89149} & \textbf{1096886} & \textbf{286223} & \textbf{1383109} 
%     \end{tabular}
%     }
% \end{table}



\subsection{Region-Text Pair Generation}
\label{sec:regiontext}
To tailor the image encoder for processing human-centric data, we finetune the model with region-text pair data. A straightforward method to acquire this data is utilizing 2D human segmentation datasets, where segmentation maps and category names directly form region-text pairs. Although efficient, this method yields less diverse masks and less informative captions. Therefore, we devise a pipeline to augment the training data. We source images from LIP \cite{gong2017look}, ATR \cite{liang2015deep}, DeepFashion \cite{liu2016deepfashion}, and CIHP \cite{gong2018instance} datasets and employ KOSMOS-2 \cite{peng2023kosmos} and SAM \cite{kirillov2023segment} to automatically generate masks and corresponding captions for these images. An example of the generated pairs is depicted in Figure \ref{Fig.example_masks}. Compared with the original labels, it provides more descriptive captions and introduces novel masks for objects that humans typically interact with, such as as `a stool'. Further details of the data generation process are presented in the supplementary. 

\begin{figure}[t]
    \centering
    \includegraphics[width=.98\hsize]{figures/example_masks.pdf}
    \caption{Example of mask-caption pairs generated by utilizing KOSMOS-2 and SAM.} 
    \label{Fig.example_masks}
\end{figure}

% with KOSMOS-2 \cite{peng2023kosmos} and SAM \cite{kirillov2023segment} to automatically generate additional training data. One benefit of this is that it provides more descriptive captions such as color of a clothing. Another benefit is that it introduces novel masks for objects that humans typically interact with but are not included in the original dataset. 
% An example of the mask-caption pairs automatically genrerated by this pipeline is shown in Figure \ref{Fig.example_masks}. It can be seen that it creates descriptive captions about various garments as well as create masks for objects such as as `a stool'. 
% More details of this pipeline is shown in the supplementary.

% \noindent\textbf{HumanCLIP Dataset.}
% Training our HumanCLIP model requires mask-text pairs for human related concepts. A straightforward way to acquire this data is through 2D human segmentation datasets in which the segmentation maps and category names can be directly used as mask-caption pairs. While efficient, this results in captions that lack variation and only focused on body parts and clothings. Therefore, we devise a pipeline with KOSMOS-2 \cite{peng2023kosmos} and SAM \cite{kirillov2023segment} to automatically generate additional training data. One benefit of this is that it provides more descriptive captions such as color of a clothing. Another benefit is that it introduces novel masks for objects that humans typically interact with but are not included in the original dataset. More details of this pipeline is shown in the supplementary.



% Although the AlphaCLIP model provides more accurate mask-wise embeddings than the original CLIP model, our experiments show that it is still insufficient in providing distinct features for human body parts and garments. This can be observed in Figure \ref{Fig.finetune}. With the vanilla AlphaCLIP, it can fail to provide well-aligned embeddings for smaller parts such as the glasses as well as distinguish left and right parts. By finetuning on 2D human image datasets, the proposed HumanCLIP model generates mask embeddings that better align with the corresponding text.


\subsection{3D Semantic Segmentation}
\label{mask_fusion}
To obtain the segmentation result with the desired semantic labels, our pipeline accepts $K$ text prompts corresponding to the labels per inference. These texts are fed to the HumanCLIP text encoder to obtain CLIP text embeddings $\mathbf{W} \in \mathbb{R}^{K \times C}$. Then, the proposed MaskFusion module semantically classifies and synthesizes multi-view embeddings into 3D segmentation masks. Specifically, we utilize the correlations between the text embeddings and the mask embeddings to build correspondences and fuse the independent 3D masks. 

Recap the $N$ 3D mask proposals generated in Section. \ref{mask_proposals}. The proposals and their embeddings are stacked to get $\mathbf{M} \in \mathbb{R}^{P \times N}$ and $\mathbf{Q} \in \mathbb{R}^{N \times C}$ respectively. We compute the classification logits $\mathbf{P} \in \mathbb{R}^{N \times K}$ by taking the cosine similarity between each mask embedding and each text embedding:
\begin{equation}
    \mathbf{P}_{n, k} = \frac{\mathbf{Q}_n\cdot \mathbf{W}_k}{||\mathbf{Q}_n||||\mathbf{W}_k||}
\end{equation}
It is used to guide the grouping of raw masks, which are class-agnostic and inconsistent across views. 

In the final step, for each 3D point, we aggregate the class scores from the associated masks to get the final 3D segmentation result $\mathbf{Y} \in \mathbb{R}^{P \times K}$.
$Y$ is computed as the simple weighted average of 3D masks $\mathbf{M}$ based on the classification logits $\mathbf{P}$:
\begin{equation}
    \mathbf{Y} = \mathbf{M} \times \mathbf{P}
\end{equation}

We decouple the procedures for mask proposal and for text classification. Therefore, it is not guaranteed that each text input is valid for the 3D model. To ensure that only existed classes are segmented in the final result, we set a threshold $\tau$ on the final segmentation logits. If the maximum logits of a point fall below $\tau$, the point is attributed to an `other' class. 