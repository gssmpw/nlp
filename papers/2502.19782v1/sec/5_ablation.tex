\section{Ablation Study}

% \subsection{HumanCLIP in Segmentation}
\noindent\textbf{HumanCLIP in Segmentation. }
% We ablate the proposed HumanCLIP and the corresponding results are shown in Table \ref{tab:mask_classification}. 
Table \ref{tab:mask_classification} and Figure \ref{Fig.tsne} present the advantage of HumanCLIP in extracting discriminative embeddings on human-related mask-caption data. We further ablate the module within our framework to assess its contribution to overall performance. Figure \ref{Fig.compare_alphaclip} illustrates the segmentation results when replacing the HumanCLIP with pre-trained AlphaCLIP model. In the example on the left, while AlphaCLIP accurately segments areas such as the jacket, pants, and hands, it struggles to distinguish the inner layer of clothing. In the example on the right, AlphaCLIP results in noisy segmentation across the legs and the right side of the body. Conversely, using HumanCLIP enables precise segmentation of body parts and clothing, as well as neighboring objects like a binder.

\begin{figure}[]
    \centering
    \includegraphics[width=.95\hsize]{figures/compare_alphaclip.pdf}
    \caption{Visual comparison with AlphaCLIP in 3D human segmentation on the RenderPeople dataset.} 
    \label{Fig.compare_alphaclip}
\end{figure}

% \subsection{Number of views}
\noindent\textbf{Number of views. }
To evaluate how the number of views affects the segmentation quality, we increase the number of views from 2 to 16 and compare the performance on the CTD dataset. The results are shown in Table \ref{tab:num_views}. We observe that as we increase the number of views, the segmentation quality improves as it can better mitigate noisy mask proposals from affecting the final result. However, more views also require more time to preprocess, so we select 8 views to strike a good balance between quality and efficiency. Itâ€™s important to note that all comparison methods utilize more than 8 views. The difference in the number of views does not confer an advantage to our method.

\begin{table}
    \centering
    \caption{Effect of the number of views on the segmentation quality.}
    \label{tab:num_views}
    \resizebox{0.7\columnwidth}{!}{\begin{tabular}{c|ccccc}
        \multirow{2}{*}{Metrics} & \multicolumn{5}{c}{Number of Views}\\ \cline{2-6}
        & 2 & 4 & 8 & 10 & 16 \\ \hline
         Accuracy & 91.48 & 92.31 & 93.29 & 93.44 & 93.71\\
         mAcc. & 91.43 & 92.09 & 93.21 & 93.49 & 93.61 \\
         mIoU. & 80.49 & 81.43 & 83.36 & 83.87 & 84.24\\
    \end{tabular}
    }
\end{table}

\noindent\textbf{Limitations.} One limitation of this method is the slow run-time for a single inference caused by applying SAM to every view. This can make it difficult to apply our method to dynamic 3D humans. Another limitation is that we have to manually adjust the threshold to conduct promptable segmentation for different text inputs. 
In future works, we plan on applying our method to generate labeled data to train a model that can efficiently compute in 3D space.
