\section{3D Mesh Generation from Multi-View Scans}

This section provides additional details about the process of obtaining in-the-wild mesh data for human subjects in the real world. 

\subsection{System Description}
Our capture system consists of four Intel RealSense D415 cameras positioned around the human subject. The cameras are placed in the north, south, east, and west directions relative to the subject, providing a full 360-degree view from the top of the head to the thighs and hip (see Figure \ref{fig:capturing-setup}). This arrangement allows for minimal occlusion, though the very top of the head is not visible. Each camera is connected to a central Windows computer, which coordinates the capture process using a Python program based on the RealSense SDK.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/capturing_setup.png}
    \caption{A picture showing the capturing setup, where the human subject is seated at the center and captures are taken by four surrounding Intel RealSense cameras.}
    \label{fig:capturing-setup}
\end{figure}

\subsection{RGB and Depth Image Capturing}

\subsubsection{Setup and Initialization of Cameras}

The cameras are configured to capture both RGB and depth images at a resolution of 640x480 pixels and a frame rate of 60 frames per second (FPS). The IR emitter and laser power are set to their maximum values.

\subsubsection{Synchronization and Warm-up}

Before capturing the actual data, the system performs a warm-up process for 2 minutes to stabilize the camera streams. During this period, a set number of frames are captured and discarded to allow the cameras to adjust to the environmental conditions.

\subsubsection{Image Capture}

For each capture session, the cameras are triggered simultaneously to ensure synchronized image acquisition. Each camera captures an RGB frame and a corresponding depth frame. The captured frames are then processed to align the RGB images to the depth images, ensuring that the color information corresponds accurately to the depth data.

\subsubsection{Depth Map Processing}

The captured frames undergo the following processing steps to improve the quality of the depth images and reduce noise.

\begin{itemize}
    \item Decimation Filter: to reduce the resolution of the depth image to remove noise and improve processing speed.
    \item Disparity Transform: The depth data is converted to disparity space for more effective filtering.
    \item Spatial Filter: to smooth the depth image by reducing spatial noise.
    \item Temporal Filter: to reduce temporal noise by averaging multiple frames.
    \item Disparity Transform (Inverse): The processed disparity data is converted back to depth space.
\end{itemize}


In addition to these filters, the RealSense SDK is configured to generate a normal vector for each point in the depth map, which will be used to generate a mesh. After processing, the RGB and depth images, along with the computed normal vectors, are saved to the disk.

\subsection{Camera Calibration}

For camera calibration, we utilize a ChArUco board with an 8x11 grid and a checkerboard size of 60 mm as shown in Figure \ref{fig:calibration-setup}. For each pair of adjacent cameras, we capture 20 images of the ChArUco board from different angles and positions. Given that the four cameras surround the human subject, every camera has two other cameras adjacent to it. Therefore, the total number of pairs of cameras is four.\\

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/calibration_setup.png}
    \caption{A picture showing two adjacent RealSense cameras facing a ChArUco board. The images of the ChArUco board taken from these two cameras will be used to find the relative rotation and translation between them.}
    \label{fig:calibration-setup}
\end{figure}

The captured images are processed to detect ChArUco markers, using the ChArUco detector in OpenCV. The stereo-calibration involves computing the rotation and translation matrices using the positions of the ChArUco markers in corresponding images between each pair of cameras, and this is performed using OpenCV. Bundle adjustment is performed to optimize the computed rotation and translation by minimizing the reprojection error.\\

The system chooses the viewpoint of one of the cameras as the world frame, and the point clouds obtained from each camera will be transformed to this world frame. Once the adjacent pairs are calibrated, the system computes the global calibration by finding the relative transformation between each camera and the world frame. A schematic diagram showing the stereo-calibration pipeline is shown in Figure \ref{fig:capturing-pipeline}.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/calibration_pipeline.png}
    \caption{The stereo-calibration pipeline to retrieve the transformation of each camera in the world frame.}
    \label{fig:capturing-pipeline}
\end{figure}

\subsection{Mesh Generation and Filtering}

After capturing RGB and depth frames from each camera and obtaining their relative transformations, the next step is to combine the point clouds from each camera to create a unified point cloud and subsequently generate a mesh. Prior to generating point clouds, the depth maps are processed to remove noise on the border of the subject, by the following steps:

\begin{itemize}
    \item Depth values outside a specified range (0.1 to 1.5 meters) are dropped to remove unwanted items that are not part of the human model.
    \item The depth map is converted to a grayscale image and filtered using a mode filter to smooth out small variations.
    \item A binary mask is created to identify the main object and remove small noise regions.
\end{itemize}

\subsubsection{Point Cloud Extraction}

For each camera, the depth map is converted into a point cloud. This involves mapping each pixel $(u, v)$ in the depth image to a 3D point $(X, Y, Z)$ in the camera coordinate system using the intrinsic parameters (focal length $f_x$, $f_y$ and image center $c_x$, $c_y$). 

$$X = \frac{(u - c_x)\cdot Z}{f_x}$$
$$Y = \frac{(u - c_y)\cdot Z}{f_y}$$
$$Z = \text{depth}(u,v)$$

The RGB values from the color images are associated with each 3D point to provide color information. Each point cloud is transformed into a global coordinate system using the rotation and translation matrices obtained from the calibration process.

$$\mathbf{p}_{\text{global}} = \mathbf{R} \mathbf{p}_{\text{local}} + \mathbf{t}$$

Here, $\mathbf{p}_{\text{local}}$ is a point in the camera's coordinate system, $\mathbf{R}$ is the rotation matrix, and $\mathbf{t}$ is the translation vector.

\subsubsection{Point Cloud Processing}

To remove noise from the combined point cloud, we apply a radius outlier removal filter. This filter eliminates points that have fewer than a specified number of neighbors within a given radius, effectively removing isolated points that are likely to be noise.

\subsubsection{Mesh Generation}

The filtered point cloud is used to generate a mesh using the Poisson surface reconstruction method. The generated mesh is smoothed using a simple Laplacian filter to reduce high-frequency noise. The colors of the mesh vertices are adjusted to match the colors from the original point cloud. A $k$-decision tree is used to find the nearest neighbors in the point cloud for each vertex in the mesh, and the color is interpolated accordingly. The resulting mesh can then be saved to a .ply file and visualized.