\section{Related Works}
\label{sec:related_works}

\begin{figure*}[t]
    \centering
    \includegraphics[width=.95\hsize]{figures/pipeline.png}
    \caption{Overview of the proposed framework. Given a 3D human model, it is first rendered to obtain multi-view 2D images. The images are then fed to SAM to generate class-agnostic 2D masks and unprojected to obtain binary 3D masks. Additionally, each pair of image and 2D masks are fed to the human-centric mask-based text-aligned image encoder to obtain CLIP embeddings for each mask. Simultaneously, the input class texts are fed to the text encoder to obtain corresponding text embeddings. The 3D mask proposals, mask embeddings, and text embeddings are fed to the mask fusion module to obtain the final segmentation result.} 
    \label{Fig.model}
\end{figure*}


\subsection{3D Human Part Segmentation}
% - body part segmentation methods ï¼ˆHuman3D takmaz20233d)
% - clothing segmentation methods (GIM3D, CloSe-Net)
% - limitations of closed-set methods



The 3D human part segmentation field is largely driven by the advancement of 3D neural networks as well as the labeled dataset. \cite{jertec2019using, ueshima2021training, musoni2023gim3d} train point cloud \cite{qi2017pointnet, qi2017pointnet++, thomas2019kpconv} or mesh segmentation networks \cite{sharp2022diffusionnet, hanocka2019meshcnn, dong2023laplacian2mesh} through direct supervision or leveraging the unclothed human parametric templates or physical simulation of garments. \cite{takmaz20233d} curates synthetic data to boost the performance on body parts. Datasets such as SIZER \cite{tiwari2020sizer}, MGN \cite{bhatnagar2019multi}, and CTD \cite{chen2021tightcap}, provide coarse clothing labels from 3D scans of clothed humans, but having only three categories and less variations of poses limit their applications. \cite{antic2024close} presents Close-D which consists of 18 garment categories. To the best of our knowledge, it is the most comprehensive dataset up-to-date. However, due to the still limited size, none of these methods show generalizable ability and cannot segment labels outside of the predefined taxonomy. 

% \noindent\textbf{Human Part Segmentation.}


\subsection{Open-Vocabulary 3D Segmentation}
In recent years, large vision language models \cite{radford2021learning, jia2021scaling} have grown popular due to their ability to perform zero-shot recognition. As a result, many works have incorporated these models \cite{ding2022decoupling, li2022languagedriven, ghiasi2022scaling, liang2023open} to conduct open-vocabulary 2D segmentation. To transfer the knowledge into 3D, some methods \cite{zhang2022pointclip, huang2022clip2point, zhu2023pointclip} apply CLIP to depth maps for zero-shot object classification and segmentation. For scene-scale data, CLIP2Scene \cite{chen2023clip2scene} and CLIP$^2$ \cite{zeng2023clip2} train an additional 3D encoder with a contrastive loss. Although these models have shown their effectiveness on general 3D scenes or objects, we find that they do not work well for 3D humans. As one of the most important categories, a framework tailored for open-vocabulary 3D human parsing is demanded. 

\noindent\textbf{CLIP Embeddings.}
CLIP \cite{radford2021learning} is one of the most widely used vision-language models in both 2D and 3D open-world segmentation approaches. Due to being trained with natural images, the vanilla CLIP does not perform well on special input subsets, such as masked images or fashion images. Many approaches try to adapt CLIP models to new tasks. \cite{chia2022contrastive, cartella2023openfashionclip} presents fine-tuned CLIP on fashion data. \cite{liang2023open} propose the Mask-adapted CLIP. AlphaCLIP \cite{sun2024alpha} adds an additional alpha channel as input so that it composes both regional and global information for better understanding. While AlphaCLIP provides better embeddings than CLIP for a region of interest, we observe that it is still inadequate to distinguish human body parts and garments. 


% A common approach with open-vocabulary 2D segmentation methods \cite{ding2022decoupling, liang2023open} have been incorporating the CLIP \cite{radford2021learning} embeddings into the MaskFormer \cite{cheng2021per, cheng2022masked} framework. In this approach, they typically predict segments from an image and apply the vanilla CLIP image encoder to the cropped and/or masked image to get the corresponding CLIP embedding for each segment. To obtain accurate embeddings for a specific region, AlphaCLIP \cite{sun2024alpha} adds an additional alpha channel as input. While AlphaCLIP provides better embeddings than CLIP for a masked region, we observe that it is still inadequate for human body parts and garments. Therefore, similar to CLIP for fashion data \cite{chia2022contrastive, cartella2023openfashionclip}, we propose a human-centric AlphaCLIP model by finetuning it on various human segmentation datasets.


% 3D Part Segmentation - 3D Human Parts - Image-language models