\section{Experiments}
\subsection{Implementation Details}

\begin{table}
    \centering
    \caption{Statistics of region-text pairs used for HumanCLIP training. Each mask is accompanied by a descriptive caption. }
    \label{tab:humanclip_data}
    \resizebox{.98\columnwidth}{!}{
    \begin{tabular}{c|ccc|c}
       Dataset & Images & Original Masks & Generated Masks & Total Masks \\ \hline
       LIP & 30462 & 173578 & 91505 & 265083 \\
       ATR & 17706 & 175604 & 87698 & 263302 \\
       DeepFashion & 12701 & 100632 & 43404 & 144036 \\
       CIHP & 28280 & 647072 & 63616 & 710688 \\ \hline
       \textbf{HumanCLIP} & \textbf{89149} & \textbf{1096886} & \textbf{286223} & \textbf{1383109} 
    \end{tabular}
    }
\end{table}


\noindent\textbf{Segment Anything Model.} 
When applying SAM in our framework for both 3D segmentation and training data generation, we adopt the ViT-H model checkpoint. To create the mask proposals, we feed a rendered image of resolution $512 \times 512$ in the ``segment everything" mode where we sample 64 points along each side of the image.

\noindent\textbf{HumanCLIP.}
We initialize the HumanCLIP image encoder with the AlphaCLIP ViT-L/14 checkpoint, which is pre-trained on GRIT-20m dataset \cite{peng2023kosmos} with an image resolution of $224\times224$. The model is then finetuned on the curated HumanCLIP dataset, in which the images combines four 2D human parsing datasets: LIP \cite{gong2017look}, ATR \cite{liang2015deep}, DeepFashion \cite{liu2016deepfashion}, and CIHP \cite{gong2018instance}. We utilize both the ground truth segmentation maps and the augmented region-text pairs described in Section \ref{sec:regiontext}, resulting in approximately 1.38 million RGBA-caption pairs for training. The distribution of images and masks across these datasets is detailed in Table \ref{tab:humanclip_data}. We keep the text encoder frozen and finetune the image encoder for a total of 3 epochs with a batch size of 18. 


\subsection{Effectiveness of HumanCLIP}

\noindent
\textbf{Embedding Space.}
In Figure \ref{Fig.tsne} we draw the t-SNE \cite{van2008visualizing} projection of the mask embeddings extracted by CLIP, AlphaCLIP, and HumanCLIP on the LIP dataset. 
For CLIP and AlphaCLIP, significant overlap among embeddings of different categories is observed, complicating accurate class distinction based on text. In contrast, our proposed HumanCLIP forms more well-defined clusters for each class, enhancing the discriminativeness of the features.

\noindent
\textbf{Mask Classification.}
To further assess the effectiveness of the proposed HumanCLIP model in accurately embedding human parts, we conducted a mask classification task comparing it with the vanilla CLIP and pre-trained AlphaCLIP models. In this task, we first feed each ground truth image and mask to the image encoder to extract the mask embedding and then compute the cosine similarity with the text embedding for each class. For the CLIP models, the image segment cropped from the ground truth mask is input to the encoder to generate image embeddings. Classification is determined by the highest similarity score. The results, displayed in Table \ref{tab:mask_classification}, show classification accuracy on the LIP \cite{gong2017look} and CCP \cite{yang2014clothing} datasets, which feature 19 and 54 classes respectively. The results indicate that both CLIP and AlphaCLIP models underperform, attributed to their lack of training on specific human parts data. AlphaCLIP shows slight improvements over CLIP as it provides better mask-wise embeddings.
It is evident that the proposed HumanCLIP significantly outperforms both models in correctly classifying each mask based on the extracted embeddings.

\begin{table}[]
    \centering
    \caption{Comparison of CLIP, AlphaCLIP, and HumanCLIP on mask classification accuracy of the LIP \cite{gong2017look} and CCP \cite{yang2014clothing} dataset.}
    \label{tab:mask_classification}
    \resizebox{0.45\columnwidth}{!}{
        \begin{tabular}{c|cc}
        Model & LIP & CCP \\ \hline
        CLIP & 22.12 & 21.75\\
        AlphaCLIP & 27.86 & 22.51 \\
        HumanCLIP & \textbf{79.98} & \textbf{52.96}
    \end{tabular}
    }
\end{table}

\begin{figure}[]
    \centering
    \includegraphics[width=.98\hsize]{figures/tsne.pdf}
    \caption{Comparison of (a) CLIP, (b) AlphaCLIP and (c) HumanCLIP's t-SNE \cite{van2008visualizing} visualizations of the mask embeddings for categories in the LIP dataset.} 
    \label{Fig.tsne}
\end{figure}

\begin{table*}
    \centering
    \caption{Comparison with open-set 3D segmentation methods. OA, mAcc, and mIoU are the overall accuracy, mean class accuracy, and mean Intersection over Union respectively. For each metric, a higher value is better. The best results are shown in \textbf{bold}.}
    \label{tab:quantitative}
    \resizebox{\textwidth}{!}{\begin{tabular}{c|ccc|ccc|ccc|ccc|ccc|ccc}
        \multirow{2}{*}{Model} & \multicolumn{3}{c|}{MGN} & \multicolumn{3}{c|}{SIZER} & \multicolumn{3}{c|}{CTD} & \multicolumn{3}{c|}{THuman2.0} & \multicolumn{3}{c|}{PosedPro} & \multicolumn{3}{c}{Average} \\ \cline{2-19}
        & OA & mAcc & mIoU & OA & mAcc & mIoU & OA & mAcc & mIoU & OA & mAcc & mIoU & OA & mAcc & mIoU & OA & mAcc & mIoU\\ \cline{1-19}
        PointCLIP V2 & 21.41 & 24.15 & 13.31 & 44.80 & 34.42 & 20.92 & 11.06 & 13.68 & 5.94 & 3.46 & 14.51 & 1.77 & 6.67 & 13.53 & 2.33 & 17.48 & 20.06 & 8.85\\
        SATR & 84.72 & 77.30 & 67.17 & 82.00 & 81.97 & 67.38 & 78.55 & 86.20 & 64.98 & 56.05 & 34.31 & 20.60 & 51.61 & 43.28 & 22.43 & 70.59 & 64.61 & 48.51 \\
        PartSLIP & 90.03 & 86.53 & 78.63 & 84.94 & 82.18 & 70.79 & 75.11 & 71.20 & 55.46 & 77.46 & 44.94 & 33.70 & 70.38 & 34.61 & 24.80 & 74.58 & 63.89 & 52.68 \\
        PartSLIP++ & 91.41 & 87.98 & 81.14 & 86.63 & 83.49 & 72.93 & 80.73 & 76.06 & 62.36 & 82.00 & 49.82 & 38.96 & 70.80 & 35.07 & 24.99 & 82.31 & 66.48 & 56.08\\
        Ours & \textbf{94.61} & \textbf{95.03} & \textbf{88.78} & \textbf{91.24} & \textbf{90.73} & \textbf{82.55} & \textbf{93.29} & \textbf{93.21} & \textbf{83.36} & \textbf{89.88} & \textbf{69.40} & \textbf{54.50} & \textbf{80.23} & \textbf{46.37} & \textbf{37.27} & \textbf{89.85} & \textbf{78.95} & \textbf{69.29}\\
     \end{tabular}}
\end{table*}

\begin{figure}[t]
    \centering
    \includegraphics[width=.95\hsize]{figures/promptable_seg2.png}
    \caption{Examples of promptable segmentation.} 
    \label{Fig.promptable_seg2}
\end{figure}

\subsection{Comparison with Open-Vocabulary 3D Segmentation Methods}
% - numerical evaluation on various 3D datasets
% - visual comparison on Renderpeople data (accessories + objects)
\noindent
\textbf{Methods.}
To the best of our knowledge, there is no method dedicated to open-vocabulary 3D human parsing. Hence, we conduct comparisons with four general 3D segmentation approaches: PointCLIP v2 \cite{zhu2023pointclip}, SATR \cite{abdelreheem2023satr}, PartSLIP \cite{liu2023partslip}, and PartSLIP++ \cite{zhou2023partslip++}. PointCLIP v2 applies CLIP to multi-view depth maps for zero-shot 3D classification, part segmentation, and object detection. SATR applies the GLIP model \cite{li2022grounded} to rendered images and aggregates the multi-view bounding predictions for each prompt to yield a segmented mesh. It shows capabilities under unclothed human setting. PartSLIP also applies GLIP but for low-shot point cloud segmentation. This is then enhanced by PartSLIP++ which incorporates SAM and an EM algorithm.

\begin{figure*}[t]
    \centering
    \includegraphics[width=.85\hsize]{figures/visual_results.pdf}
    \caption{Qualitative analysis of segmentation results with (c) PointCLIPv2, (d) SATR, (e) PartSLIP, (f), PartSLIP++ on the 3D scans from MGN, SIZER, CTD, THuman, and PosedPro datasets.} 
    \label{Fig.visual}
\end{figure*}

\noindent
\textbf{Datasets.}
% 3d human segmentation datasets (SIZER, CTD, THuman, Posed Pro)
% Girish's data
For quantitative evaluation, we benchmark the models on five labeled 3D human datasets: MGN \cite{bhatnagar2019multi}, SIZER \cite{tiwari2020sizer}, CTD \cite{chen2021tightcap}, THuman2.0 \cite{tao2021function4d}, and Posed Pro \cite{RenderPeople}. These dataset contain a total of 3, 9, 12, 12, and 19 classes respectively.

% \noindent
% In addition to five datasets, we introduce a new dataset to show the effectiveness of our method on 3D scans captured in-the-wild. We collected this dataset on 15 subjects using four depth cameras and will be publicly released. Information about the setup and capturing process is detailed in the supplementary.

\noindent
\textbf{Quantitative Comparison.}
In Table \ref{tab:quantitative}, we show the quantitative comparison of our model with the four open-set 3D segmentation methods. We first notice that PointCLIP v2 performs poorly across all of the datasets. Since they apply CLIP to rendered depth maps, which differs greatly from real-world images that it was originally trained on, PointCLIP v2 is unable to effectively transfer the zero-shot capabilities to 3D. Among the other three methods, performance is generally best on the MGN dataset, which has the fewest classes, and declines as the number of classes increases. Across all datasets, it is evident that our proposed framework outperforms by a large margin in all metrics.

\noindent
\textbf{Visual Comparison.}
In Figure \ref{Fig.visual}, we show the visual comparison of various methods on the same five 3D human datasets. Akin to the numerical results, PointCLIPv2 fails to generate reasonable segmentation results. SATR and PartSLIP are able to get a coarse segmentation: the boundaries between various segments are unclear. PartSLIP++ shows improved boundaries but still struggles with specific areas like ‘hair’ and ‘face’. In contrast, our method delivers the most precise segmentation results.

% \subsection{Few-shot inference}
% Both PartSLIP and PartSLIP++ claim improved segmentation results in a few-shot setting. To confirm this, we finetune both models with few-shot prompt tuning on 3D human datasets. We follow their setting and train only the offset features for each prompt while keeping the pretrained GLIP parameters frozen.

\subsection{Promptable Segmentation}

The design of our MaskFusion module allows users to segment whatever they want, as highlighted in Figure \ref{Fig.promptable_seg2}. Beyond conducting a full segmentation of the entire body, our framework can precisely segment only the user-specified items. It also effectively recognizes and accurately segments unseen categories, such as ``uniform number'' and ``smartphone holder''.

% In each example, our proposed framework only segments the region specified by the prompt as it is able to threshold areas with low confidence. This shows that our model can be useful in cases where users only need to query specific regions of human as opposed to generating a full segmentation of the entire body.

\subsection{Run-time Efficiency}
Rendering 3D data into multi-view images for processing has raised efficiency concerns in applications. Our approach, which decouples mask proposal generation from textual prompt processing, offers a significant efficiency advantage over previous methods. The mask embedding serves as an attribute of the 3D model, which can be generated beforehand. During the segmentation phase, only the text encoder and MaskFusion module are active. In contrast, for \cite{abdelreheem2023satr, liu2023partslip, zhou2023partslip++}, the GLIP model relies on text prompts to generate bounding boxes and masks, requiring the entire pipeline to be executed for each segmentation attempt.


% A significant benefit of our pipeline over previous methods is its efficiency. Due to the design, if we need to segment the same model multiple times, we only need to compute the mask proposals and mask embeddings once and can be reused. This way, only the text encoder is required after the first use. With previous methods \cite{abdelreheem2023satr, liu2023partslip, zhou2023partslip++}, the texts are fed to the GLIP model \cite{li2022grounded} early in the pipeline to generate bounding boxes and masks. This means that, apart from the rendering process, the entire pipeline has to be run for every segmentation iteration.

Table \ref{tab:efficiency} displays the inference times for various methods when executed on a system equipped with a single 24GB RTX 4090 graphics card. We compare the inference time for a one-time only run, which executes all modules from start to finish, and the average time for 100 inferences of the same model, during which we reuse any pre-generated information where possible. For instance, we do not rerender multi-view images for the subsequent inferences. The results of our approach show a significant decrease in average cost as the number of inferences increases. The most time-consuming step in our framework is the ``segment everything" mode of SAM. However, this is only necessary once for multiple text inputs, making our model efficient for subsequent inferences. In scenarios common to the open-vocabulary setting, where there is a fixed amount of 3D assets but varying information is required based on user queries, our method offers considerable advantages. 

% The bottleneck of our method is the use of SAM in the ``segment everything" mode which is known to be slow. However, this is only required to run a single time for various text inputs, which allows our model to be fast for multiple inferences. In the open-vocabulary setting, often times users need to adjust the input prompts multiple times to get the optimal segmentation results, so in these cases our method should prove to be helpful.

% \begin{table}[]
%     \centering
%     \caption{Comparison of inference times between various open-vocabulary 3D segmentation methods. We compare the average time in seconds to compute a single inference and 100 inferences.}
%     \label{tab:efficiency}
%     \resizebox{\columnwidth}{!}{\begin{tabular}{c|ccccc}
%          & PointCLIPv2 & SATR & PartSLIP & PartSLIP++ & Ours \\ \hline
%         One-time Inference & 7.62 & 54.18 & 32.46 & 88.41 & 105.72\\
%         Average Inferences & 126.67  & 2768.52 & 2607.90 & 7455.34 & 106.21\\
%     \end{tabular}
%     }
% \end{table}

\begin{table}[t]
    \centering
    \caption{Comparison of inference times. We compare the average time cost in seconds assuming one-time inference only and 100 inference calls. }
    \label{tab:efficiency}
    \resizebox{\columnwidth}{!}{\begin{tabular}{c|ccccc}
         & PointCLIPv2 & SATR & PartSLIP & PartSLIP++ & Ours \\ \hline
        One-time Inference & 7.62 & 54.18 & 32.46 & 88.41 & 105.72\\
        Average Inference & 1.27  & 27.69 & 26.08 & 74.55 & 1.06\\
    \end{tabular}
    }
\end{table}

\begin{figure}[]
    \centering
    \includegraphics[width=.8\hsize]{figures/results_gaussian.pdf}
    \caption{Segmentation of 3D Gaussian Splatting.} 
    \label{Fig.results_gaussian}
\end{figure}


\subsection{3D Gaussian Splatting Segmentation}

Our framework design is compatible with various point-based 3D representations, including the popular 3D Gaussian Splatting \cite{kerbl20233d} format. We follow the standard protocol to generate 3DGS for each model in the THuman2.0 dataset. As demonstrated in Figure \ref{Fig.results_gaussian} our method can generate reasonable segmentation results, making it a generalizable solution for segmenting 3DGS. This approach eliminates the need to optimize per-Gaussian semantic labels \cite{lan20232d} or high-dimensional features \cite{zhou2024feature} during the resource-intensive training stage.

% We demonstrate that the proposed methods are not limited to traditional 3D representations. We evaluate on the popular 3D Gaussian Splatting \cite{kerbl20233d} format. We follow the standard protocol to generate 3DGS for each model in THuman2.0 dataset: we first create multi-view image data by rendering models using Blender to obtain images and camera poses. The 3DGS are then trained on these data and fed to our pipeline for segmentation. An example of the results are shown in Figure \ref{Fig.results_gaussian} where our proposed method is able to generate reasonable segmentation results. Thus, our method can be used as an generalizable solution to segment pre-trained 3D Gaussians in order to avoid optimizing per-Gaussian semantic labels \cite{lan20232d} or high-dimensional features \cite{zhou2024feature}.

\subsection{In-the-wild Segmentation}
Our method demonstrates no significant domain gap in real-world noisy scenarios. Figure \ref{Fig.results_wild} shows a visual comparison with PartSLIP and PartSLIP++ on two point clouds captured by consumer-level RGB-D sensors, where the surface is at relatively low definition and the model is incomplete. In the first example, all methods are able to accurately segment the clothing, but our method shows the best segmentation ability of the book. For the second example, both PartSLIP and PartSLIP++ are unable to segment the `skin' and `glasses' categories. In contrast, our method can accurately segment these classes as well as the small area for the watch. More details are explained in the supplementary. 

\begin{figure}[]
    \centering
    \includegraphics[width=.9\hsize]{figures/results_wild.pdf}
    \caption{Visual comparison of (a) PartSLIP, (b) PartSLIP++, and (c) Ours on our in-the-wild point cloud dataset.} 
    \label{Fig.results_wild}
\end{figure}



% Our method allows users to define arbitrary queries and search the query in the image, see Figure 5. Without train-ing our models to learn specific concepts, our model can lo-
% cate and segment Saturn V as the lego rocket, Oculus as the VR headset, and golden gate as the bridge in cor-
% responding images. This demonstrates the strong potentials of open vocabulary semantic segmentation.

% \begin{figure}[]
%     \centering
%     \includegraphics[width=.98\hsize]{figures/promptable_seg.pdf}
%     \caption{Comparison with PartSLIP on promptable segmentation.} 
%     \label{Fig.promptable_seg}
% \end{figure}



