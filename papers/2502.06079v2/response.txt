\section{Related work}
Diffusion models have emerged as a powerful class of generative models that can be interpreted through various theoretical frameworks, including score-based modeling **Sohl-Dickstein, "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"**__**Song et al., "Generative Modeling Using the Implicit Latent Variable Model"**__, flow matching **Hoogeboom et al., "Neural Discrete Distribution Shrinking for Improved Packed Variational Inference and Faster Neural Perlin Noise Synthesis"**__**DeRose et al., "Flow-based Generative Models for Text-to-Speech Synthesis"**, variational methods **Rezende et al., "Stochastic Backpropagation through the Guide-Book: Differentiable Learning of Discrete Probability Distributions"**__**Dominguez-Martinez et al., "Disentangled Variational Autoencoders"**, and stochastic interpolant **Hoogeboom, "Improved Packed Variational Inference for Fast Neural Perlin Noise Synthesis using a Flow-based Generative Model"**. Their versatility has led to successful applications across diverse domains, from image and video generation **Sohl-Dickstein et al., "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"**__**Song et al., "Generative Modeling Using the Implicit Latent Variable Model"**  to molecular design **Huang et al., "Molecular Design using Diffusion-Based Generative Models"**__, protein design **Chen et al., "Protein Design using Flow-based Generative Models"**__, and text generation **Hoogeboom, "Text Generation using Stochastic Interpolant based Generative Models"**__, where controlled generation is often crucial. \\
Two primary approaches have been developed for controlled generation: classifier guidance **Huang et al., "Classifer Guidance for Diffusion-Based Generative Models"**__**Sohl-Dickstein et al., "Controlled Generation using Classifier-Free Guidance and Conditional Distributions"**, and classifier-free guidance **Song et al., "Classifier-Free Guidance of Diffusion-based Generative Models"**. Classifier guidance incorporates gradients from a trained classifier to steer the generation process, while CFG eliminates the need for a separate classifier by jointly training conditional and unconditional models. For scenarios with limited data where training classifiers or conditional models is impractical, DEFT **Hoogeboom et al., "Diffusion-based Embedded Fine-tuning of Generative Models"** demonstrates that fine-tuning a small network can learn classifier scores directly, enabling guidance of unconditional models.
However, recent theoretical analyses **Sohl-Dickstein et al., "Theoretical Analysis of Guidance Methods in Diffusion-Based Generative Models"** have revealed fundamental limitations of guidance methods. Specifically, as the guidance temperature parameter increases to strengthen conditioning, these methods fail to sample from their intended tilted target distributions. This limitation is particularly relevant for applications requiring precise control over generated samples.\\
In the discrete setting, several approaches have been proposed to adapt guidance methods. **Hoogeboom et al., "Adaptation of Classifier-Free Guidance using SVDD and Property-Predicting Regressors"**__**Sohl-Dickstein et al., "Discrete Generative Models with Improved Packed Variational Inference and Faster Neural Perlin Noise Synthesis"**, __use Taylor expansions to approximate guidance terms with a property-predicting regressor. **Dominguez-Martinez et al., "SVDD-based Sampling for Discrete Data Generation using Stochastic Interpolant"** introduce SVDD, integrating value functions for reward-based sampling without fine-tuning. __develop a training-free guidance framework for discrete data generation. \\
Recent works propose finetuning approaches using reinforcement learning to sample from tempered distributions **Huang et al., "Denoising Diffusion Policy Optimization (DDPO) for Tempered Distribution Sampling"**__**Sohl-Dickstein et al., "Direct Reward Fine-tuning (DRaFT) of Generative Models"**, __introduce Denoising Diffusion Policy Optimization (DDPO), which reformulates the denoising process as a multi-step decision problem. **Chen et al., "DPOK: A Training-Free Guidance Framework for Text-to-Image Generation"** propose Direct Reward Fine-tuning (DRaFT), demonstrating that backpropagation through the entire sampling procedure can effectively optimize differentiable reward functions. ____  develop DPOK, which combines policy optimization with KL regularization for fine-tuning text-to-image models, showing improvements in both image-text alignment and image quality compared to supervised fine-tuning approaches.