\section{Related work}
Diffusion models have emerged as a powerful class of generative models that can be interpreted through various theoretical frameworks, including score-based modeling ____, flow matching ____, variational methods ____ and stochastic interpolant ____. Their versatility has led to successful applications across diverse domains, from image and video generation ____  to molecular design ____, protein design ____, and text generation ____, where controlled generation is often crucial. \\
Two primary approaches have been developed for controlled generation: classifier guidance ____ and classifier-free guidance ____. Classifier guidance incorporates gradients from a trained classifier to steer the generation process, while CFG eliminates the need for a separate classifier by jointly training conditional and unconditional models. For scenarios with limited data where training classifiers or conditional models is impractical, DEFT ____ demonstrates that fine-tuning a small network can learn classifier scores directly, enabling guidance of unconditional models.
However, recent theoretical analyses ____ have revealed fundamental limitations of guidance methods. Specifically, as the guidance temperature parameter increases to strengthen conditioning, these methods fail to sample from their intended tilted target distributions. This limitation is particularly relevant for applications requiring precise control over generated samples.\\
In the discrete setting, several approaches have been proposed to adapt guidance methods. ____ adapt Classifier Free Guidance, ____ use Taylor expansions to approximate guidance terms with a property-predicting regressor. ____ introduce SVDD, integrating value functions for reward-based sampling without fine-tuning. ____ develop a training-free guidance framework for discrete data generation. \\
Recent works propose finetuning approaches using reinforcement learning to sample from tempered distributions ____. ____ introduce Denoising Diffusion Policy Optimization (DDPO), which reformulates the denoising process as a multi-step decision problem. ____ propose Direct Reward Fine-tuning (DRaFT), demonstrating that backpropagation through the entire sampling procedure can effectively optimize differentiable reward functions. ____  develop DPOK, which combines policy optimization with KL regularization for fine-tuning text-to-image models, showing improvements in both image-text alignment and image quality compared to supervised fine-tuning approaches.