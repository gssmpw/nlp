\section{Related work}
Diffusion models have emerged as a powerful class of generative models that can be interpreted through various theoretical frameworks, including score-based modeling \citep{song2020generativemodelingestimatinggradients, song2021scorebasedgenerativemodelingstochastic}, flow matching \citep{lipman2023flowmatchinggenerativemodeling, lipman2024flowmatchingguidecode}, variational methods \citep{ho2020denoisingdiffusionprobabilisticmodels} and stochastic interpolant \citep{albergo2023buildingnormalizingflowsstochastic, albergo2023stochasticinterpolantsunifyingframework}. Their versatility has led to successful applications across diverse domains, from image and video generation \citep{rombach2022highresolutionimagesynthesislatent, podell2023sdxlimprovinglatentdiffusion, blattmann2023stablevideodiffusionscaling}  to molecular design \citep{cornet2024equivariant, vignac2023digress, hoogeboom2022equivariantdiffusionmoleculegeneration}, protein design \citep{watson2023novo}, and text generation \citep{diffusion-jiaxin, sedd, gulrajani2023likelihoodbaseddiffusionlanguagemodels}, where controlled generation is often crucial. \\
Two primary approaches have been developed for controlled generation: classifier guidance \citep{dhariwal2021diffusionmodelsbeatgans} and classifier-free guidance \citep{ho2022classifierfreediffusionguidance}. Classifier guidance incorporates gradients from a trained classifier to steer the generation process, while CFG eliminates the need for a separate classifier by jointly training conditional and unconditional models. For scenarios with limited data where training classifiers or conditional models is impractical, DEFT \citep{DEFT} demonstrates that fine-tuning a small network can learn classifier scores directly, enabling guidance of unconditional models.
However, recent theoretical analyses \citep{what-does-guidance-do, bradley2024classifierfreeguidancepredictorcorrector} have revealed fundamental limitations of guidance methods. Specifically, as the guidance temperature parameter increases to strengthen conditioning, these methods fail to sample from their intended tilted target distributions. This limitation is particularly relevant for applications requiring precise control over generated samples.\\
In the discrete setting, several approaches have been proposed to adapt guidance methods. \citet{unlock_guidance} adapt Classifier Free Guidance, \citet{vignac2023digress} use Taylor expansions to approximate guidance terms with a property-predicting regressor. \citet{li2024derivativefreeguidancecontinuousdiscrete} introduce SVDD, integrating value functions for reward-based sampling without fine-tuning. \citet{training-free-guidance} develop a training-free guidance framework for discrete data generation. \\
Recent works propose finetuning approaches using reinforcement learning to sample from tempered distributions \citep{domingoenrich2025adjointmatchingfinetuningflow, venkatraman2024amortizing, NEURIPS2023_fc65fab8, black2024trainingdiffusionmodelsreinforcement, clark2024directlyfinetuningdiffusionmodels, uehara2025inferencetimealignmentdiffusionmodels}. \citet{black2024trainingdiffusionmodelsreinforcement} introduce Denoising Diffusion Policy Optimization (DDPO), which reformulates the denoising process as a multi-step decision problem. \citet{clark2024directlyfinetuningdiffusionmodels} propose Direct Reward Fine-tuning (DRaFT), demonstrating that backpropagation through the entire sampling procedure can effectively optimize differentiable reward functions. \citet{NEURIPS2023_fc65fab8}  develop DPOK, which combines policy optimization with KL regularization for fine-tuning text-to-image models, showing improvements in both image-text alignment and image quality compared to supervised fine-tuning approaches.