\section{Related work}
\label{sec2}

\subsection{Neural Topic Models}
Topic modeling is a probabilistic method that maps a large collection of documents into a low-dimensional latent topic space.
Blei et al., "Latent Dirichlet Allocation" , the prominent topic model learned using a Bayesian inference process, generates each word of each document from a topic-word distribution in accordance with its latent topic specified by a document-topic distribution sampled from a Dirichlet prior. 
Hoffman et al., "Stochastic-Gradient Reduced Variational Bayes for Latent Dirichlet Allocation" , mainly on the basis of VAE, approximate the posterior distribution of topics using an encoder in the inference process and generate a bag-of-words (BoW) representation of each docuement using a decoder. 
Blei et al., "Probabilistic Topic Models" is the first to introduce a logistic normal prior into a VAE-based NTM to approximate a Dirichlet prior as in LDA. 
To mitigate the issue of topic collapsing, which commonly occurs in VAE-based models, Wu et al., "Variational Autoencoder Based Neural Topic Model with Embedding Clustering Regularization" proposed a VAE-based NTM with embedding clustering regularization. This regularization sets the topic embeddings as the cluster centers of the word embeddings and ensures that the clusters are as far apart as possible.
Building upon ProdLDA, SCHOLAR is a general NTM that incorporates various types of metadata and utilizes the background log-frequency of words. It achieves better topic quality since it excludes common words across different topics. It can be trained in both supervised and unsupervised settings.

Apart from SCHOLAR, researchers developed NTMs that incorporate external information to enhance topic quality.
Bai et al., "Document Network-based Neural Topic Model" incorporate an external relational citation network of documents into NTMs, where the topic distributions of two documents are fed into a neural network with multilayer perceptrons (MLPs) to predict whether they should be connected.
Wang et al., "Jointly Encoding Texts and Their Network Links for Document Clustering" jointly encode texts and their network links to derive topic distributions using an augmented encoder network, which consists of an MLP and a graph convolutional network.
To address the sparsity problem of short texts, Wang et al., "Enriching Word Embeddings with Word Co-occurrence Graphs for Short Text Topic Modeling" enrich the BoW representations by incorporating a word co-occurrence graph and constructing a word semantic correlation graph using pre-trained word embeddings.
On the basis of SCHOLAR, SCHOLAR+BAT leverages the rich knowledge from a pre-trained BERT to guide the learning of the NTM in a knowledge distillation framework.
In a more recent study, Tang et al., "Causal Topic Model" incorporate a structural causal module into an NTM to simultaneously capture causal relationships between topics and metadata, as well as relationships within the elements themselves.
In this study, we concentrate on NTMs that do not utilize external information. 
However, our models can be easily extended to incorporate external information, which could potentially lead to further performance enhancements.

\subsection{Neural Topic Models with contrastive learning}
In this field, research typically begin by identifying positive and negative samples at either the document level or latent topic distribution level. Then, contrastive learning is applied to the topic distribution level alongside the standard NTM learning objectives. 
Also on the basis of SCHOLAR, CLNTM uses a data augmentation method for texts that uses the tf-idf of each word in a document to create positive and negative samples. Positive (negative) samples are generated by replacing the values of insignificant (salient) words in the input BoW vector with the values of those chosen words in the recostructed BoW vector. This enables the application of contrastive learning, where the anchor sample's prototype representation is moved closer to the representation of the positive sample and pushed further from the representation of the negative sample.
Han et al., "Contrastive Learning Neural Topic Model" combine clustering based on a pre-trained language model with an NTM. They use contrastive learning to cluster document embeddings and select salient words from each cluster to form the vocabulary for the NTM. Contrastive learning is then applied again during the NTM training stage by leveraging the topic distributions of positive samples and the prior distribution.
For short texts, Wu et al., "Topic Modeling for Short Texts via Sample Selection" select positive and negative samples based on topic distributions to mitigate the data sparsity problem. 
Zhang et al., "Short Text Topic Modeling via Contrastive Learning" improve short text topic modeling in variable-length corpora by transferring semantic information from long texts to short texts, learning the semantic relationships through a contrastive learning approach.


\subsection{Self-supervised learning}
To address the collapse problem in joint embedding architectures, several intuitive self-supervised learning methods have been proposed to regularize the content of the embeddings.
Zhang et al., "Barlow Twins: Self-Supervised Learning via Redundancy Reduction" makes the normalized cross-correlation matrix between two embeddings of distorted versions of a sample move toward the identity matrix.
Hjelm et al., "Learning deep representations by mutual information estimation and maximization" minimizes the mean square distance between the two embeddings, ensures the standard deviation of each element remains above a threshold, and reduces the covariances between each pair of embedding variables toward zero.
Both methods project the representation vectors into a high-dimensional space where the regularization on the embeddings indirectly reduces the redundant information in the representation vectors.