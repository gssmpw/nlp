\section{Related work}
\label{sec2}

\subsection{Neural Topic Models}
Topic modeling is a probabilistic method that maps a large collection of documents into a low-dimensional latent topic space.
LDA~\cite{blei2003latent}, the prominent topic model learned using a Bayesian inference process, generates each word of each document from a topic-word distribution in accordance with its latent topic specified by a document-topic distribution sampled from a Dirichlet prior. 
NTMs, mainly on the basis of VAE~\cite{welling2014auto}, approximate the posterior distribution of topics using an encoder in the inference process and generate a bag-of-words (BoW) representation of each docuement using a decoder. 
ProdLDA~\cite{srivastava2017autoencoding} is the first to introduce a logistic normal prior into a VAE-based NTM to approximate a Dirichlet prior as in LDA. 
To mitigate the issue of topic collapsing, which commonly occurs in VAE-based models, Wu et al.~\cite{wu2023effective} proposed a VAE-based NTM with embedding clustering regularization. This regularization sets the topic embeddings as the cluster centers of the word embeddings and ensures that the clusters are as far apart as possible.
Building upon ProdLDA, SCHOLAR~\cite{card2018neural} is a general NTM that incorporates various types of metadata and utilizes the background log-frequency of words. It achieves better topic quality since it excludes common words across different topics. It can be trained in both supervised and unsupervised settings.

Apart from SCHOLAR, researchers developed NTMs that incorporate external information to enhance topic quality.
Bai et al.~\cite{bai2018neural} incorporate an external relational citation network of documents into NTMs, where the topic distributions of two documents are fed into a neural network with multilayer perceptrons (MLPs) to predict whether they should be connected.
Wang et al.~\cite{wang2021layer} jointly encode texts and their network links to derive topic distributions using an augmented encoder network, which consists of an MLP and a graph convolutional network.
To address the sparsity problem of short texts, Wang et al.~\cite{wang2021extracting} enrich the BoW representations by incorporating a word co-occurrence graph and constructing a word semantic correlation graph using pre-trained word embeddings.
On the basis of SCHOLAR, SCHOLAR+BAT~\cite{hoyle2020improving} leverages the rich knowledge from a pre-trained BERT~\cite{devlin2019bert} to guide the learning of the NTM in a knowledge distillation framework.
In a more recent study, Tang et al.~\cite{tang2024beyond} incorporate a structural causal module into an NTM to simultaneously capture causal relationships between topics and metadata, as well as relationships within the elements themselves.
In this study, we concentrate on NTMs that do not utilize external information. 
However, our models can be easily extended to incorporate external information, which could potentially lead to further performance enhancements.

\subsection{Neural Topic Models with contrastive learning}
In this field, research typically begin by identifying positive and negative samples at either the document level or latent topic distribution level. Then, contrastive learning is applied to the topic distribution level alongside the standard NTM learning objectives. 
Also on the basis of SCHOLAR, CLNTM~\cite{nguyen2021contrastive} uses a data augmentation method for texts that uses the tf-idf of each word in a document to create positive and negative samples. Positive (negative) samples are generated by replacing the values of insignificant (salient) words in the input BoW vector with the values of those chosen words in the recostructed BoW vector. This enables the application of contrastive learning, where the anchor sample's prototype representation is moved closer to the representation of the positive sample and pushed further from the representation of the negative sample.
Han et al.~\cite{han2023unified} combine clustering based on a pre-trained language model with an NTM. They use contrastive learning to cluster document embeddings and select salient words from each cluster to form the vocabulary for the NTM. Contrastive learning is then applied again during the NTM training stage by leveraging the topic distributions of positive samples and the prior distribution.
For short texts, Wu et al.~\cite{wu2022mitigating} select positive and negative samples based on topic distributions to mitigate the data sparsity problem. 
Zhang et al.~\cite{zhang2022meta} improve short text topic modeling in variable-length corpora by transferring semantic information from long texts to short texts, learning the semantic relationships through a contrastive learning approach.


\subsection{Self-supervised learning}
To address the collapse problem in joint embedding architectures, several intuitive self-supervised learning methods have been proposed to regularize the content of the embeddings.
Barlow Twins~\cite{zbontar2021barlow} makes the normalized cross-correlation matrix between two embeddings of distorted versions of a sample move toward the identity matrix.
VICReg~\cite{bardes2022vicreg} minimizes the mean square distance between the two embeddings, ensures the standard deviation of each element remains above a threshold, and reduces the covariances between each pair of embedding variables toward zero.
Both methods project the representation vectors into a high-dimensional space where the regularization on the embeddings indirectly reduces the redundant information in the representation vectors.