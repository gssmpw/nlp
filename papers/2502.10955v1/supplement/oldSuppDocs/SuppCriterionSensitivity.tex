\documentclass[12pt]{article}

\usepackage{amsmath,amsfonts,amssymb,mathtools}
\usepackage{algorithm,algpseudocode}
\usepackage{hyperref,xcolor,graphicx,gensymb}
\usepackage{microtype,geometry}
\geometry{a4paper, margin=1in}

\begin{document}

\title{Criterion and Sensitivity}
\date{\today}
\maketitle

\section{Criterion and Sensitivity in Signal Detection Theory}

Signal detection theory provides a framework for analyzing how observers detect signals in the presence of noise. Two key measures in this theory are criterion and sensitivity. The criterion represents the observer's decision threshold for reporting a signal as present. Mathematically, it is defined as:
\[c = -\frac{1}{2}(z(\theta_H) + z(\theta_{FA}))\]
Where $z(\theta_H)$ is the z-score of the hit rate ($\theta_H$) and $z(\theta_{FA})$ is the z-score of the false alarm rate ($\theta_{FA}$). Intuitively, the criterion reflects how liberal or conservative the observer is in reporting signals. A lower (more negative) criterion indicates a liberal bias - the observer is more willing to report signals even with weak evidence. A higher (more positive) criterion indicates a conservative bias - the observer requires stronger evidence to report a signal.


Sensitivity, often denoted as $d^\prime$ (d-prime), measures the observer's ability to discriminate between signal and noise. It is calculated as:
\[d' = z(\theta_H) - z(\theta_{FA})\]
Sensitivity represents the standardized difference between the means of the signal and noise distributions. A higher $d^\prime$ indicates better ability to distinguish signal from noise. These measures allow researchers to separate an observer's inherent sensitivity to signals from their decision-making strategy (criterion). By analyzing both, we can understand not just how well an observer detects signals, but also their underlying decision-making processes.

\section{Random Variable Interpretation of the Hit-Rate and False-Alarm Rate}
Let $X_H$ and $X_{FA}$ be random variables associated with the agent's outcome on a single trial. On change trials, when $X_H=1$, the agent successfully detected a change and the outcome is a hit. If $X_H=0$, the agent did not detect a change and the outcome of the trial is a miss. On no-change trials, $X_{FA}=1$ if the outcome of the trial was a false alarm and $X_{FA}=0$ when the outcome is a correct reject. If we condition on the trial type, then the two random variables are independent, i.e.,
\begin{align*}
    X_{H}|\{\text{\textquotedblleft Change Trial\textquotedblright}\} \perp X_{FA}|\{\text{\textquotedblleft No-Change Trial\textquotedblright}\}
\end{align*}
If we let $\theta_{H}$ and $\theta_{FA}$ be the hit-rate and false-alarm-rate, then 
\begin{equation}
\begin{aligned}
\hat{\theta}_{H} &= \frac{n_H}{n_H + n_M} & \hat{\theta}_{FA} &= \frac{n_{FA}}{n_{FA} + n_{CR}}
\end{aligned}
\end{equation}
where $\hat{\theta}_H$ and $\hat{\theta}_{FA}$ is an estimator for the true hit-rate and false-alarm-rate, $\theta_H$ and $\theta_{FA}$.
For brevity, let $X_p$ and $\theta_p$ be such that $p \in \{\text{H,FA}\}$, and assume the trial conditioning described above. Given a sequence of $n$ random variables, $\{X_p^{(i)}\}_{i=1}^{n}$, we can model $\theta_p$ as 
\begin{align*}
    \hat{\theta}_p &= \frac{1}{n}\sum_{i=1}^{n}X_p^{(i)} \\
    X_p^{(i)} &\sim \text{Bern}(\theta_p)
\end{align*}
where $\text{Bern}(\theta_p)$ is a Bernoulli random variable with parameter $\theta_p$. By the law of large numbers, we know that this estimator converges to the true parameter in the large sample limit
\begin{align*}
    \lim_{n \rightarrow \infty} \frac{1}{n}\sum_{i=1}^{n}X_p^{(i)} = \theta_p
\end{align*}

Using the Central Limit Theorem (CLT), we can also compute the variance of this estimate. By CLT, 
\begin{align*}
    \sqrt{n}\left[ \hat{\theta}_p - \theta_p \right] \rightarrow \mathcal{N}(0,\hat{\sigma}_p^2)
\end{align*}
where $\hat{\sigma}_p^{2} = \hat{\theta}_p(1-\hat{\theta}_p)$ and $\hat{\sigma}^2_p\rightarrow \sigma^2_p$ in the large sample limit. Thus, the variance of our parameter estimate is 
\begin{align*}
    Var(\hat{\theta}_p) \approx  \frac{\hat{\sigma}_p^2}{n}
\end{align*}

\section{z-Score as a Random Variable Transformation}
If we treat $\hat{\theta}_p$ as a random variable, then computing the z-score is a transformation of this random variable. Hence, we can use the Delta Method to derive the approximate properties of this transformation. By the Delta Method:
\begin{align*}
    \sqrt{n}\left[ z(\hat{\theta}_p) - z(\theta_p) \right] \rightarrow \mathcal{N}\left( 0, \hat{\sigma}_p^2 [z^\prime(\hat{\theta}_p)]^2 \right)
\end{align*}
where $z^\prime(\hat{\theta}_p)=\frac{dz}{dx}|_{\hat{\theta}_p}$. This gives
\begin{align*}
    Var(\hat{\theta}_p) \approx \frac{\hat{\sigma}_p^2}{n} z^\prime(\hat{\theta}_p)
\end{align*}
We can evaluate $z^\prime(\hat{\theta}_p)$ by using the Inverse Function Theorem (IFT). Let $\psi(x)$ be the standard normal probability density function, i.e.,
\begin{align*}
    \psi(x) = \frac{1}{\sqrt{2\pi}}e^{\frac{-x^2}{2}}
\end{align*}
Then the cumulative distribution function is
\begin{align*}
    \phi(x) = \int_{-\infty}^x \psi(t)dt
\end{align*}
This gives $z^\prime(\hat{\theta}_p) = \frac{d\phi^{-1}}{dx}|_{(\hat{\theta}_p)}$. By IFT,
\begin{align*}
    z^\prime(\hat{\theta}_p) &=\left. \frac{d\phi^{-1}}{dx}\right|_{\hat{\theta}_p} \\
    &= \frac{1}{\left( \left. \frac{d\phi}{dx} \right|_{\phi^{-1}(\hat{\theta}_p)} \right)} \\
    &= \frac{1}{\psi(\phi^{-1}(\hat{\theta}_p))}
\end{align*}
Together with the above, this yields
\begin{align*}
    Var(\hat{\theta}_p) \approx \left(\frac{\hat{\sigma}_p^2}{n}\right) \left(\frac{1}{\psi(\phi^{-1}(\hat{\theta}_p))}\right)
\end{align*}

\section{Criterion and Sensitivity are Normally Distributed in the Large Sample Limit}
Since we are conditioning on the trial type, our estimates of the hit-rate and false-alarm-rate are independent. Therefore, the criterion and sensitivity consists of the summation of two independent normal (approximate) random variables. For the estimate ($\hat{c}$) of a true criterion ($c$), we have
\begin{align*}
    \hat{c} \sim \mathcal{N}\left( c,  \frac{1}{4}\left(\frac{\hat{\sigma}_{H}^2}{n_{CT}} \left(\frac{1}{\psi(\phi^{-1}(\hat{\theta}_{H}))}\right) + \frac{\hat{\sigma}_{FA}^2}{n_{NT}} \left(\frac{1}{\psi(\phi^{-1}(\hat{\theta}_{FA}))}\right)\right)\right)
\end{align*}
For the estimate ($\hat{d}^\prime$) of a true sensitivity ($d^\prime$), we have
\begin{align*}
    \hat{d}^\prime \sim \mathcal{N}\left( d^\prime,  \left(\frac{\hat{\sigma}_{H}^2}{n_{CT}} \left(\frac{1}{\psi(\phi^{-1}(\hat{\theta}_{H}))}\right) + \frac{\hat{\sigma}_{FA}^2}{n_{NT}}  \left(\frac{1}{\psi(\phi^{-1}(\hat{\theta}_{FA}))}\right)\right)\right)
\end{align*}

\end{document}