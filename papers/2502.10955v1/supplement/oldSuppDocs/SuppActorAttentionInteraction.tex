\documentclass[12pt]{article}

\usepackage{amsmath,amsfonts,amssymb,mathtools}
\usepackage{algorithm,algpseudocode}
\usepackage{hyperref,xcolor,graphicx,gensymb}
\usepackage{microtype,geometry}
\geometry{a4paper, margin=1in}

\begin{document}

\title{Supplementary Information: Decoding Analysis of vLSTM Architecture}
\date{\today}
\maketitle

\section{Decoder Architecture}

To extract and interpret the information encoded within our vLSTM model, we implemented a decoder architecture. This decoder is designed to process the output from various layers of the vLSTM and produce task-relevant predictions. The architecture consists of a feedforward neural network with the following structure:

\begin{equation}
    \text{Decoder}(x) = f_3(f_2(f_1(x)))
\end{equation}

where $x \in \mathbb{R}^{d_\text{in}}$ is the input vector (typically a flattened output from a vLSTM layer), and $f_1$, $f_2$, and $f_3$ are layer functions defined as:

\begin{align}
    f_1(x) &= \text{ELU}(\text{LN}_1(W_1x + b_1)) \\
    f_2(x) &= \text{ELU}(\text{LN}_2(W_2x + b_2)) \\
    f_3(x) &= W_3x + b_3
\end{align}

Here, $W_1 \in \mathbb{R}^{512 \times d_\text{in}}$, $W_2 \in \mathbb{R}^{256 \times 512}$, and $W_3 \in \mathbb{R}^{d_\text{out} \times 256}$ are weight matrices, $b_1$, $b_2$, and $b_3$ are bias vectors, $\text{LN}_1$ and $\text{LN}_2$ are layer normalization operations, and $\text{ELU}$ is the Exponential Linear Unit activation function. The final output of the decoder is a vector in $\mathbb{R}^{d_\text{out}}$, with $d_\text{out}$ depending on the specific decoding task.

\section{Decoding Analysis of vLSTM Components}

\subsection{Decoding from the Complete Hidden State}

The hidden state in our vLSTM, denoted as $H \in \mathbb{R}^{4 \times 1024}$, comprises four slots corresponding to the four image patches in our visual field. To analyze the information content of this hidden state, we flatten $H$ to $H_\text{flat} \in \mathbb{R}^{4096}$ and train a decoder to predict the location of orientation change ($S_1$, $S_2$, $S_3$, or $S_4$).

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.99\linewidth]{suppFigures/ConfusionMatricesMemoryClassificationToChange.png}
    \caption{Confusion matrices for decoding change location from the complete hidden state under various conditions.}
    \label{fig:ConfMatHAllSlots}
\end{figure}


Figure \ref{fig:ConfMatHAllSlots} presents confusion matrices for our decoder evaluated on a test set. The decoder was trained on data from an environment identical to that of the RL agent's training, without artificial modulations. The results in Figure \ref{fig:ConfMatHAllSlots} demonstrate that the decoder can detect change locations, albeit with suboptimal accuracy. We observe several key phenomena. First, there is a temporal enhancement effect: increased time ($t=5$ to $t=6$) enhances change decodability from $H$ collected at later time points, suggesting a temporal integration of change information. Second, we note attentional modulation: directing attention towards a specific stimulus patch ($S_1$ in Figure \ref{fig:ConfMatHAllSlots}, right column) slightly improves decodability for changes in $S_1$ while marginally decreasing decodability for other patches, indicating a trade-off in representational capacity.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.99\linewidth]{suppFigures/decodingHAttentionS1.png}
    \caption{Confusion matrices for decoding change location with varying attentional conditions on $S_1$.}
    \label{fig:ConfMatHAllSlotsCue025}
\end{figure}

To further investigate the impact of attentional modulation, we conducted an experiment with a forced 0.25 cue on $S_1$, indicating a 0.25 probability of a stimulus becoming the change stimulus, given a change trial. Figure \ref{fig:ConfMatHAllSlotsCue025} presents the results of this analysis. In Figure \ref{fig:ConfMatHAllSlotsCue025}, we observe several important effects. The first column shows normal decoding performance under the 0.25 cue condition. In the second column, we enforced a uniform Self-Attention map $A$ ($A_{i,j} = 0.25$ for all $i$ and $j$), which leads to a bias in classifying $H$ as being derived from a no-change trial. This suggests that non-uniform attention is crucial for change detection. The third column shows the effect of attention inhibition on $S_1$ ($A_{i,1}=0$ for all $i$), resulting in an increased number of no-change classifications when $S_1$ is the change target. This demonstrates the importance of attention for change detection in the attended location. Finally, in the fourth column, we enforced maximum attention on $S_1$ ($\alpha_1=1$, i.e., $A_{i,1}=1$ for all $i$). This slightly increased change classifications when $S_1$ was the change target but significantly inhibited change classification for other targets, highlighting the competitive nature of attentional allocation.

\subsection{Decoding from a Single Hidden State Slot}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.99\linewidth]{suppFigures/decodingH1AttentionS1.png}
    \caption{Confusion matrices for decoding change occurrence from the first hidden state slot under varying attentional conditions.}
    \label{fig:ConfMatFirstSlot}
\end{figure}

To investigate the information transfer between patches facilitated by self-attention, we decoded exclusively from the first slot of $H$, $H_1 \in \mathbb{R}^{1024}$, corresponding to the $S_1$ patch. Figure \ref{fig:ConfMatFirstSlot} presents the results of this analysis. The results in Figure \ref{fig:ConfMatFirstSlot} reveal several key findings. In the baseline condition (first column), the decoder can correctly classify the occurrence of change in the majority of trials, indicating that a single slot contains information about changes across all patches. When we force maximum attention on $S_1$ ($\alpha_1=1$, second column), we observe a loss of 'change' classifications on change trials. This suggests that maximal attention on $S_1$ suppresses the propagation of change signals from other patches to the $S_1$ slot. Interestingly, when we force no attention on $S_1$ ($\alpha_1=0$, not shown), the decodability of change from slot $S_1$ is only slightly affected. This is because attention on other patches allows change information to flow into slot $S_1$, and $S_1$ changes can be decoded without attention due to direct access to patch information.

These results demonstrate the crucial role of self-attention in distributing change information across memory slots and highlight the competitive nature of attentional allocation in our model.

\subsection{Decoding from the Actor Network's First Activation Layer}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.99\linewidth]{suppFigures/confusionMatrixMu1.png}
    \caption{Confusion matrices for decoding change location and occurrence from the actor network's first activation layer.}
    \label{fig:ActorLayer1Activation}
\end{figure}

To understand how the actor network processes the information from the vLSTM, we trained a decoder on the first activation layer of the actor network. Figure \ref{fig:ActorLayer1Activation} presents the results of this analysis. The results in Figure \ref{fig:ActorLayer1Activation} reveal several important aspects of information processing in the actor network. The left two columns show that much of the spatial information is lost in this first activation layer, as evidenced by the poor classification of orientation change locations compared to decoding from the hidden state (Figure \ref{fig:ConfMatHAllSlots}). This suggests a loss of spatial specificity in the actor network. However, the right two columns demonstrate that changes can still be successfully decoded from this layer, indicating that change information is preserved and potentially emphasized in the actor network.

When we force $\alpha_1=1$ (rightmost column), we observe a loss of change classification. Importantly, this is not due to the absence of the change signal (as it is still present in the memory slot associated with the change location) but rather due to a weaker presence of the change signal. This suggests that self-attention serves to amplify the change signal by propagating it across all memory slots.

To further investigate the impact of attentional modulation on change detection in the actor network, we conducted an additional experiment focusing only on $S_1$ changes with varying levels of attention inhibition. Figure \ref{fig:ActorLayer1ActivationS1Changes} presents these results.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.99\linewidth]{suppFigures/confusionMatrixMu1_inhibitAttention.png}
    \caption{Confusion matrices for decoding $S_1$ changes from the actor network's first activation layer with varying levels of attention inhibition.}
    \label{fig:ActorLayer1ActivationS1Changes}
\end{figure}

In Figure \ref{fig:ActorLayer1ActivationS1Changes}, we observe a clear progression as the inhibition of attention is relaxed. When self-attention on $S_1$ is completely inhibited (left column), the model fails to classify the majority of changes. However, the change signal is not entirely eradicated, as evidenced by the non-zero number of change classifications. As inhibition decreases (right columns), the confusion matrices show increasingly accurate classification structures. This demonstrates the graded nature of attentional modulation in the detection of changes in the actor network.

\subsection{Analysis of Actor Network Logits}



\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.99\linewidth]{suppFigures/LogitsAndStimulusIntensities.png}
    \caption{Relationship between 'Declare Change' and 'Wait' logits in the actor network's final layer.}
    \label{fig:LogitsChangeLocation}
\end{figure}

To gain insight into the decision-making process of the actor network, we analyzed the logits of its final layer (pre-softmax activation). Figure \ref{fig:LogitsChangeLocation} presents this analysis. Figure \ref{fig:LogitsChangeLocation} reveals several key aspects of the decision-making process of the actor network. First, we observe a clear inverse linear relationship between 'Declare Change' and 'Wait' logits, indicating a competitive decision-making process. Second, we note a strong dependence on change intensity: 'Wait' logits tend to be high when change intensity ($\Delta$) is low, while 'Declare Change' logits are high for large $\Delta$ values. This suggests that the actor network has learned to base its decisions on the magnitude of orientation changes. Interestingly, the spatial location of the change does not show any obvious influence on this relationship, indicating that the actor network has learned to make decisions based primarily on change magnitude rather than location.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.99\linewidth]{suppFigures/LogitsAndS1Attention.png}
    \caption{Effect of attentional modulation on actor network logits for $S_1$ changes.}
    \label{fig:LogitsS1Attention}
\end{figure}

To investigate the impact of attentional modulation on the actor network's decision-making process, we analyzed the logit structure under varying attention conditions for $S_1$ changes. Figure \ref{fig:LogitsS1Attention} presents these results. The results in Figure \ref{fig:LogitsS1Attention} demonstrate the significant impact of attentional modulation on the decision-making process of the actor network. Maximizing $\alpha_1$ slightly increases the density of points in the high 'Declare Change' logit space, suggesting that increased attention facilitates change detection. Conversely, minimizing $\alpha_1$ significantly decreases the density of logits in the high-valued \lq Declare Change \rq logit space and increases the density in the high-valued \lq Wait \rq logit space. This demonstrates that attentional withdrawal substantially impairs the network's ability to detect changes, even when they occur in the $S_1$ location.

These findings collectively demonstrate the intricate interplay between the vLSTM's self-attention mechanism and the actor network's decision-making process, highlighting the crucial role of attention in facilitating effective change detection and appropriate action selection.

\newpage

\section{Self-Attention Amplifies Change Signals}

Let $H^{(t)} \in \mathbb{R}^{4 \times 1024}$ represent the hidden state of the VISTA at time $t$, where each row corresponds to a memory slot associated with one of the four visual patches. Additionally, let $k^*$ be the indexed location of the stimuli that experienced an orientation change, $S_{k^*}$, and $\psi(\cdot)$ be some measure of the strength of a change signal. While the many parameters and complexity make a closed form description of how a change signal is communicated among memory slots challenging, based on the above results and structure of the VISTA, we can characterize some key properties of the process. From the structure of VISTA, we know that self-attention is the only mechanism that allows information to flow from one memory slot to another. Thus, if  $i \neq k^*$ and $a_{i,k^*}^{(t)} = 0: \forall t \geq t_{change}$, then $\psi(h_i^{(t)}) = 0$, where $h_i^{(t)}$ is the hidden state associated with slot $i$. Additionally, we know that if $i = k^*$, then attention is not needed to propagate change information to the memory slot $h_i^{(t)}$, i.e., $\psi(h_i^{(t)};a_{i,k^*}^{(t)}) \approx \psi(h_i^{(t)}; 0): \forall t \geq t_{change}$. Finally, we know that if we apply this measure to all memory slots, $\psi(H^{(t)})$, we know that $\psi(H^{(t)}; \alpha_{k^*})$ increases as $\alpha_{k^*}$ increases, where 
\begin{align*}
    \alpha_{k^*} = \sum_{i=1}^4 a_{i,k^*}
\end{align*}
Since the first layer of the actor network seems to corrupt spatial information but preserve binary change information \autoref{fig:ActorLayer1Activation}, we can extrapolate that the primary function of self-attention in relation to action selection is to amplify change signals.  

\end{document}