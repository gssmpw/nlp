\documentclass[twocolumn,12pt,compact]{article}

% Font and Page Setup
\usepackage[scaled=0.92]{helvet} % Helvetica font
\renewcommand{\familydefault}{\sfdefault}
\usepackage[%
  margin=0.5in,
  includehead,        % Include header in the top margin calculation
  includefoot,        % Include footer in the bottom margin calculation
  headheight=0.05in,    % Adjust as needed
  headsep=0.4in,     % Adjust as needed
  footskip=0.1in      % Adjust as needed
]{geometry}

\usepackage{fancyhdr} % For headers and footers
\usepackage{multicol} % For two columns (later in the document)
% Reset dblbotnumber to avoid redefinition conflicts:

\usepackage{cuted} % Add this to your preamble
\usepackage{caption}  % For caption positioning
\usepackage{stfloats} % For forcing placement at the bottom
\usepackage{tabularx}

% Other Packages (as before)
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{xcolor}
\usepackage{gensymb}
\usepackage{cite}
\usepackage{etoolbox}
\usepackage{quoting}
\usepackage{booktabs}
\usepackage{titling}
\usepackage{authblk}
\usepackage{titlesec}
% Now, relax the counter right before loading dblfloatfix:

%\usepackage{dblfloatfix}

% improve float placement
\setcounter{topnumber}{2}
\setcounter{dbltopnumber}{2}
\renewcommand{\topfraction}{.85}
\renewcommand{\dbltopfraction}{.85}
\setcounter{bottomnumber}{2}
\renewcommand{\bottomfraction}{.85}
\setcounter{totalnumber}{4}
\renewcommand{\textfraction}{.15}

% set up captions:
\captionsetup{
    labelfont=bf,     % Make "Figure X" bold
    textfont=small,   % Make caption text slightly smaller
    labelsep=period   % Ensure "Figure X." format (not "Figure X:")
}

% adjust whitespace surrounding section / subsection headers
\raggedbottom
\titlespacing{\section}{0pt}{12pt}{6pt}
\titlespacing{\subsection}{0pt}{8pt}{4pt}

% define indentation rule
\setlength{\parindent}{2em}

% Title Customization
\makeatletter
\def\@maketitle{%
  \newpage
  {\fontsize{24}{28}\selectfont\bfseries\noindent\@title\par}
  \vskip 0.5em
  \noindent\rule{\linewidth}{0.4pt}
  \vskip 0.5em
  {\raggedright\@author}
  \vskip -0.8em
}
\makeatother

\renewenvironment{abstract}{%
  \noindent\textbf{Abstract}\par\noindent
  \noindent\ignorespaces
}{%
  \par\noindent
}

\renewcommand\Authfont{\selectfont\fontsize{20}{22}\bfseries}
\renewcommand\Affilfont{\small}

% no date:
\date{}

% Define rule command
\newcommand{\authabstractrule}{\noindent\rule{\linewidth}{0.4pt}} 

% Header and Footer Setup
\fancypagestyle{plain}{
    \fancyhf{} % Clear all headers/footers
    \fancyhead[L]{\textcolor{gray}{Preprint}\\\noindent\rule{\linewidth}{0.4pt}}
    \fancyfoot[R]{\rule{\linewidth}{0.4pt}\\\thepage}
}
\pagestyle{plain}

% Optional: Add this to enforce spacing
\setlength{\parskip}{0pt}

% customize author and affiliation information:
\renewcommand\Authfont{\normalsize}
\renewcommand\Affilfont{\small}
\setlength{\affilsep}{0.5em}

\author[1]{Jonathan Morgan}
\author[2]{Badr Albanna}
\author[1]{James P. Herman}
\affil[1]{Department of Ophthalmology, University of Pittsburgh, Pittsburgh, PA 15219}
\affil[2]{Duolingo}

\begin{document}

\title{A recurrent vision transformer shows signatures of primate visual attention} % Add this line

% title / authors / affiliations / abstract span two columns:
\twocolumn[
% create title
\maketitle

 % Add rule between author block and abstract
\authabstractrule

% Abstract with Rule
% \begin{abstract}
% We introduce a recurrent Vision Transformer (Recurrent ViT) that integrates a capacity-limited memory module with self-attention to emulate primate-like attentional dynamics on a spatially cued orientation-change detection task. Trained using sparse reward feedback on this single task, our model replicates key features of primate attention—demonstrating improved accuracy and faster responses for cued stimuli that scale with cue validity. Analysis of its self-attention maps reveals that the model maintains and reactivates task-relevant spatial information over time, closely mirroring the temporal dynamics observed in primate studies. Importantly, targeted manipulations of the attention weights yield performance changes analogous to those produced by microstimulation in primate attentional control regions. Overall, our results demonstrate that recurrent processing in Vision Transformers can endow these architectures with behavior that resembles primate visual attention.
% \end{abstract}


\begin{abstract}
\noindent Attention has emerged as a core component of both biological and artificial intelligences (AIs). Despite decades of parallel research, studies of animal and AI attention remain largely separate. The self-attention mechanism ubiquitous in contemporary AI applications is not grounded in biology, and the powerful capabilities of AIs equipped with self-attention have yet to offer fresh insight into the biological mechanisms of attention. Here, we offer a unifying perspective, drawing together insights from primate neurophysiology and contemporary machine learning in a Recurrent Vision Transformer (Recurrent ViT). Our model extends self-attention by allowing both input and memory to guide attention allocation. Our model learns purely through sparse reward feedback, emulating how animals must learn in a laboratory environment. We benchmark our model by challenging it to perform a spatially cued orientation-change detection task widely used to study attention in the laboratory and comparing its performance to non-human primates (NHPs). The model exhibits hallmark behavioral signatures of primate visual attention --- improved accuracy and faster responses for cued stimuli, both scaling with cue validity. Analysis of self-attention maps reveals rich attention dynamics, with the model maintaining spatial priorities through delays and stimulus onsets, reactivating them prior to anticipated change events. Perturbing these attention maps produces performance changes that mirror effects of causal manipulations in primate attention nodes such as the frontal eye fields (FEF) or superior colliculus (SC). These findings not only validate the effectiveness of integrating recurrent memory with self-attention for emulating primate-like attention, but also establish a promising framework for probing the neural underpinnings of attentional control. Ultimately, our work attempts to bridge the gap between biological and artificial attention, paving the way for more interpretable and neurologically informed AI systems.
\end{abstract}

% Our work suggests that despite their distinct origins, biological and artificial attention may share deeper computational principles than previously recognized.
% Our work advances machine learning by introducing a novel form of self-attention that integrates both bottom-up sensory input and top-down memory signals to guide attention allocation. For neuroscience, it provides a framework that generates specific testable predictions about the causal relationships amongst attention, working memory and value estimation in the brain. Our work suggests that despite their distinct origins, biological and artificial attention may share deeper computational principles than previously recognized.


\noindent\rule{\linewidth}{0.4pt}
\vspace{1mm}
]
% Start Two-Column Environment for Main Text
\section{Introduction}
Visual attention is a foundational cognitive function that supports behavioral flexibility by allowing biological organisms to guide behavior selectively on the basis of a subset of visual input. Perceptual judgments are more accurate and reaction times are faster for attended stimuli compared to unattended \cite{carrasco2011visual,clark2015visual,hoffman2016visual,bhatnagar2022meta,rust2022priority}. Neuronal correlates include heightened spiking activity and decreased spike-count correlations for attended stimuli \cite{mcadams1999effects,mcadams1999effects_orientation,thiele2018neuromodulation,cohen2009attention,ruff2016attention}. Classic paradigms use spatial cues to direct attention \cite{posner1980attention}, where a delay separates cue and stimulus. This delay is crucial as it allows any difference in the behavioral response to cued versus uncued stimuli to be ascribed to the organism's internal "attentional" state. As a consequence of this delay, the cue's location must be maintained in visual working memory (VWM). Visual attention and VWM are unsurprisingly strongly linked \cite{awh2006interactions,gazzaley2012top,kiyonaga2013working,panichello2021attvwm}, as working memory contents guide attention and vice versa \cite{oberauer2002access,mcnab2008prefrontal,carlisle2011attentional,van2014competition,berggren2018visual,carlisle2018visual,van2019human}.

Transformers \cite{vaswani2017attention,dosovitskiy2020image,khan2022transformers} have achieved remarkable success in both language and vision by employing self-attention mechanisms that bear a superficial resemblance to how biological systems allocate attentional resources \cite{itti2001computational,le2006coherent,kruger2017measuring}. However, whether the self-attention in these models truly mirrors the selective, goal-driven attention observed in humans or non-human primates remains a subject of debate. For instance, while transformers can exhibit human-like patterns in text-based tasks \cite{zou2023human}, in vision they tend to emphasize low-level grouping rather than task-dependent selection \cite{mehrani2023self}. Similarly, unsupervised methods such as DINO \cite{yamamoto2024emergence} can produce attention maps that resemble human gaze distributions, yet these models generally lack explicit top-down control mechanisms. In contrast, models like V-JEPA \cite{bardes2023v} adopt a predictive coding framework in self-supervision and process entire sequences of frames as input. Although this approach may capture certain aspects of visual working memory, the continuous access to past stimuli (or their compressed representations) reduces the need for selective encoding and storage. Consequently, such models may not fully adhere to key biological constraints—such as limited capacity \cite{luck1997capacity,luck2013visual,brady2013probabilistic,emrich2017attention} and dynamic internal states \cite{olivers2011different,teng2019visual,bays2024representation}—that are critical for accurately modeling primate attention.

The close association between visual working memory (VWM) and attention in primates, combined with the fact that systems processing entire image sequences concurrently do not require selective encoding, suggests a novel approach for endowing transformers with a flexible, primate-like attention mechanism. We propose a Vision Transformer (ViT) variant that incorporates a spatial memory module feeding back into the self-attention mechanism, thereby integrating ideas from recurrent neural networks \cite{hochreiter1997long,beck2024xlstm}. To demonstrate the model's similarity to primate attention, we train it on a cued orientation-change detection task \cite{srinath2021attention,posner1980attention} using a reinforcement-learning framework. Our model exhibits improved performance and faster responses for cued stimuli, closely paralleling the attentional benefits observed in primate studies \cite{carrasco2011visual,clark2015visual,hoffman2016visual,bhatnagar2022meta,rust2022priority}. Moreover, targeted manipulations of the model’s attention weights yield behavioral changes reminiscent of those seen following causal perturbations in the frontal eye fields (FEF) \cite{moore2003selective} and superior colliculus (SC) \cite{cavanaugh2004subcortical,cavanaugh2006enhanced}. These findings underscore that incorporating spatial memory and feedback into vision transformers can recover core signatures of primate attention, offering a promising path for reconciling transformer-based architectures with biological principles of attentional control.

% Importantly, our model is also a quantitative framework for neuroscience research, offering specific, testable predictions about the interplay between attention and working memory in the brain.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.99\linewidth]{figures/OrientationChangeTask_wbg.png}
    \caption{Each trial in the task comprises seven time steps. In each time step, a $50 \times 50$ grayscale image is input to the model. Black images are shown at $t=0$ and $t=2$. The cue is shown at $t=1$ and can either be at $S_1$ (top left) or $S_4$ (bottom right). The cue can take four configurations, where the portion of the circumference subtended by the ring around the center disk indicates the probability (25\%, 50\%, 75\%, or 100\%) that the change will appear at the cued location if the trial is a change trial.}
    \label{fig:environment}
\end{figure}

\section{Task and Model}

\subsection{Cued Orientation Change Detection Task Environment}

We trained our model on a spatially cued orientation change detection task similar to those used in primate neurophysiology labs \autoref{fig:environment}. Each trial comprised 7 time steps. At each time step, a 50x50 grayscale image was shown to the agent. At $t=0$, the trial began with a black image. At $t=1$, a spatial cue was displayed. The cue appeared at stimulus position 1 (Cue $S_1$) in one-half of the trials and at position 4 (Cue $S_4$) in the other half. At $t=2$, a black screen was displayed again. At $t=3$, stimulus onset occurred, displaying four ``Gabor" stimuli in randomly chosen orientations at fixed positions: top left ($S_1$), bottom left ($S_2$), top right ($S_3$), and bottom right ($S_4$). At $t=4$, the stimuli remained unchanged with the exception of orientation ``noise" added in each time step of stimulus presentation to control task difficulty (see Methods). If the trial was a no-change trial, stimuli remain unchanged from $t=4$ to $t=6$. In a change trial, at $t=5$ the orientation of one of the four stimuli changed by $\Delta$ degrees (where $\Delta$ varied from trial to trial). The orientations then remained unchanged from $t=5$ to $t=6$. Half of all trials were ``change trials" and the other half were ``no-change trials" (balanced across cue presentation positions).

Visually distinct cues indicated different levels of "cue validity": the probability of an orientation-change event occurring at the cued position. Cue validity levels were 25\%, 50\%, 75\%, or 100\%. For example, the 50\% valid cue presented at $S_1$ meant that, if this was a change trial, there was a 50\% probability that the orientation change would occur at position $S_1$. Cue validity was depicted visually by a white arc that subtended 25\%, 50\%, 75\% or 100\% of a central disc's circumference (\autoref{fig:environment}). We use the term ``cue validity" to align with the convention established in the human and NHP psychophysics literature ~\cite{posner1980attention, egly1994shifting, thomsen2005processing, brisson2008express}.

Much like NHPs trained in visual attention tasks, we trained our model in an RL setting. At each time step the agent could either choose to wait ($a^{(t)}=\text{\lq wait\rq}$) or declare a change ($a^{(t)}=\text{\lq declare change\rq}$). For $t<6$, waiting resulted in no reward ($r=0$), and the trial advanced to the next time step. At $t=6$, waiting rewarded the agent $r=1$ in a no-change trial and $r=0$ in a change trial. Declaring a change always ended the trial. When the agent correctly declared a change at $t \geq 5$, it received a reward $r=1$. In all other cases, declaring a change resulted in $r=0$. Before describing the model's behavior, we first briefly describe model components and their interconnections to facilitate interpretation of the model's attention dynamics.

\begin{figure}[!t]
    \includegraphics[width=0.99\linewidth]{figures/figModel2.png}
    \caption{Model Schematic. At each timestep a single image is input, parsed into four patches, and passed through a pre-processing stage (see Methods). The resulting low-level visual features, \(X^{(t)} = \{x_i^{(t)}\}_{i=1}^{4}\), are combined with the activated memory, \(H^{(t-1)} = \{h_i^{(t-1)}\}_{i=1}^{4}\) in a self-attention mechanism, producing spatio-temporal context vectors (\(\alpha_i \xi_i\)). Context vectors are added to the low-level features and processed together to yield \(Z^{(t)} = \{z_i^{(t)}\}_{i=1}^{4}\). The memory is then updated \(C^{(t)} = \{c_i^{(t)}\}_{i=1}^{4}\) using both \(Z^{(t)}\) and the previous memory \(H^{(t-1)}\). The updated memory \(H^{(t)}\) is both fed back into the self-attention mechanism and forward to the RL Agent's actor and critic networks. The actor network uses \(H^{(t)}\) to select an action ($\text{\lq wait\rq}$ or $\text{\lq declare change\rq}$), while the critic network estimates upcoming cumulative rewards. Purple lines indicate weights updated by reward feedback.}
    \label{fig:model}
\end{figure}

\section{Model}

Our Recurrent ViT has three parts: the self-attention (SA) module, the patch-based long-short-term-memory (LSTM) module ("working memory" module), and an actor-critic RL agent. The environment generates the current visual scene and the agent converts this scene to a visual representation, $X^{(t)}$, through low-level convolutional operations (see Supplement). Processing results in 4 visual patches: $X^{(t)} = \{ x_i^{(t)} \}_{i=1}^{4}$, and this 4-patch structure is retained throughout the attention and working memory modules (number of patches is a hyperparameter). The primary benefit of our choice to structure the visual patches around stimulus positions is the interpretability it affords to the model's ``attention map". Specifically, it allows the attention map to be visualized as a 4x4 array in which we can interpret as the bias, $\alpha_i^{(t)}$, assigned to an internal representation ($\xi_i^{(t)}$) associated with stimulus $S_i$. An important distinction here is that $\alpha_i^{(t)}$ does not just describe the the attention on the current visual patch $x_i^{(t)}$. Instead it describes the attention on an internal representation, $\xi_i^{(t)}$ that consists of the immediate visual information in $x_i^{(t)}$ and information derived from activated memory describing relevant past temporal and spatial context, $h_i^{(t-1)}$.

The patch-based LSTM module receives a transmission, $Z^{(t)}=\{z_i^{(t)}\}_{i=1}^{4}$, that contains visual information derived from the immediate visual scene in addition to spatial and temporal context derived from the self-attention mechanism. This information is utilized to update the internal states of the LSTM, $C^{(t)}=\{c_i^{(t)}\}_{i=1}^{4}$ (see Methods). Activated memory, $H^{(t)}=\{h_i^{(t)}\}_{i=1}^{4}$, is derived from these internal states and sent: (1) recurrently back into the LSTM; (2) to the self-attention module; (3) to the actor and critic networks. This allows attention to be allocated both on the basis of visual inputs as in traditional transformer architectures ~\cite{vaswani2017attention, dosovitskiy2020image}, and on the basis of memory.

To understand which model architecture elements were required to recapitulate primate-like attention, we also tested several alternative models. We briefly describe alternative model performance after results from the Recurrent ViT. Full descriptions of all models can be found in the Methods section.

\begin{figure*}[!t]
    \includegraphics[width=0.99\linewidth]{figures/fig_CUEeffect.png}
    
    \caption{
        \textbf{A--F} shows the response rates (\textbf{A--C}) and reaction times (\textbf{D--F}) of our agent over varying cue validities with respect to the $S_1$ location and either a change on $S_1$ or a change on $S_4$ positions. Each data point was 500 trials where $\Delta$ specifies the magnitude of the orientation change. The response-rate was computed as $n_\text{dc}/n_\text{trials}$, where $n_\text{dc}$ is the total number of trials in which the agents selected the action $a^{(t)}=\text{``declare change''}$ and $n_{trials}$ is the total number of trials. The reaction times were computed as $1/500 \sum_i \tau_{i}$, where $\tau_{i}$ is the time the trial ended, either by the agent declaring a change or waiting through the final timestep. \textbf{A} Response rates over each possible cue condition w.r.t. the $S_1$ position where changes also occurred at the $S_1$ position. \textbf{B} Response rates computed over trials with a cue at the $S_1$ position comparing changes at the $S_1$ versus the $S_4$ locations. \textbf{C} Similar to \textbf{B} but with a 100 \% cue at the $S_1$ location. \textbf{D--F} Same conditions as \textbf{A--C} showing the mean reaction times.}
    \label{fig:behavior}
\end{figure*}

\section{Results}

\subsection{A recurrent ViT exhibits behavior signature of visual attention}

Our model exhibited orderly "psychometric" and "chronometric" functions with characteristic sigmoidal shapes commonly observed in human and NHP experiments (\autoref{fig:behavior}A,D). Larger orientation-change $\Delta$ values were associated with higher hit rates and shorter reaction times, qualitatively comparable to those seen in countless human and NHP psychophysics experiments. For cued orientation changes, higher cue validity improved correct response rate (\autoref{fig:behavior}A) and sped reaction time modestly (\autoref{fig:behavior}D). This pattern mirrors experimental findings that attentional benefits in biological systems are most pronounced when discriminating subtle changes ~\cite{lu1998sens}. No effects of cue validity were observed on the slope of the fitted psychometric function, the guess rate, or the lapse rate. These results indicate that, like spatial attention in biological visual systems, the attention mechanism of our model produced primarily additive effects on perceptual sensitivity rather than changing the shape of the psychometric function (Supplement)~\cite{lu1998sens, solomon2004sens, cameron2002sens}.

% \begin{table}[h]
%     \centering
%     \begin{tabular}{|c|c|c|c|c|c|}
%         \hline
%         \textbf{Cue} & \textbf{Change} & \textbf{A} & \textbf{B} & \textbf{C} & \textbf{D} \\ 
%         \hline
%         % Add your data here, with each row separated by \\
%         25\% & $S_1$ & 0.07 & 0.07 & 0.30 & 13.00 \\ 
%         50\% & $S_1$ & 0.07 & 0.07 & 0.28 & 11.89 \\ 
%         75\% & $S_1$ & 0.06 & 0.05 & 0.3 & 10.50 \\ 
%         100\% & $S_1$ & 0.06 & 0.06 & 0.3 & 9.83 \\ 
%         25\% & $S_4$ & 0.075 & 0.07 & 0.27 & 12.66 \\ 
%         50\% & $S_4$ & 0.084 & 0.08 & 0.28 & 13.18 \\ 
%         75\% & $S_4$ & 0.09 & 0.09 & 0.27 & 14.83 \\ 
%         100\% & $S_4$ & 0.10 & 0.24 & 0.29 & 17.49 \\ 
%         \hline
%     \end{tabular}
%     \caption{An 8x6 Table Example}
%     \label{tab:example}
% \end{table}

Contrasting performance for cued orientation changes compared to uncued revealed a clear ``cueing effect", again recapitulating results from human and NHP literature \cite{muller1987sensitivity, hawkins1990visual, carrasco2011visual, rust2022priority, saltzman1948reaction, carlson1983reaction, prinzmetal2005attention, jehu2015prioritizing}. Using the trained model with fixed weights, we were able to test how the model responded to uncued orientation changes even in the 100\% cue validity case, despite the necessary absence of uncued changes in trials with the 100\% valid cue during training (\autoref{fig:behavior}C,F). In that 100\% cue validity condition, for example, $10^\circ$ cued orientation changes were detected in roughly 50\% of trials, but uncued changes of the same magnitude were detected in roughly 15\% of trials (\autoref{fig:behavior}C). The magnitude of this cueing effect varied systematically with cue validity: compared to the 100\% validity condition, cue effects were mostly absent in the 25\% condition (\autoref{fig:behavior}B,C), which is expected - because there are 4 stimulus positions, 25\% cue validity indicates equal probability of an orientation change at any stimulus position. Overall, these findings confirm that our model robustly mirrors primate attention task behavior: it exhibits a strong cueing effect when spatial cues are highly predictive, and the effect diminishes as cue validity decreases.

\begin{figure*}[!t]
    \centering
    \vspace*{-0mm} % Adjust as needed
    \includegraphics[width=0.99\linewidth]{figures/newAttentionPlots.png}
    
    \caption{\textbf{A}~Averaged self-attention maps at each timestep when \(S_1\) is cued and orientation change \(\Delta=0\) (no-change trials). Rows correspond to different cue validities (25\%, 50\%, 75\%, 100\%), and columns to timesteps \(t=0,\ldots, 6\). Darker squares in each \(2\times2\) attention map reflect stronger attention. \textbf{B--C}~Attentional bias ($\alpha_1^{t_\text{change}}$) on $S_1$ as a function of orientation change $\Delta$ for each cue validity. The bias $\alpha_1^{(t)}$ is the top-left value from the heatmaps in \textbf{A}. \textbf{D}~Attentional bias on \(S_1\) (blue: $\alpha_1^{(t)}$) and \(S_4\) (red: $\alpha_4^{(t)}$) as a function of timestep when \(S_1\) is cued and orientation change \(\Delta=0\) (no-change trials).}
    \label{fig:attention}
\end{figure*}

\subsection{A recurrent ViT deploys attention in an human/NHP-like strategy}

To visualize how the model allocates attention in each timestep, we generated averaged self-attention heatmaps from trials with the cue at location $S_1$ and no orientation change occurred ($\Delta = 0$; \autoref{fig:attention}A). This established heatmap approach ~\cite{dosovitskiy2020image} for interpreting self-attention in vision transformers illustrates the relative attentional weight assigned to each stimulus location over time. At the start of each trial ($t=0$), the attention landscape was flat (\autoref{fig:attention}A). When the spatial cue appeared ($t=1$), attention became strongly biased toward $S_1$ ($\alpha_1^{(t=1)}\approx 1$). This bias persisted during the subsequent blank interval ($t=2$), indicating that the memory ($h_1^{(t)}$) can maintain attention allocation without visual input. At stimulus onset ($t=3$), attention maps became largely flat again. Just before the change timestep ($t=4$), the model began to allocate attention to the cued location, with stronger attention for higher cue validity. At the time of change ($t=5$), higher cue validity caused much stronger attention allocation to the cued location, and this bias persisted in the next time step ($t=6$). These findings show that the Recurrent ViT has learned to allocate attention both based on the behavioral relevance of the input (strongly attending to the appearance of the cue) and on reliable temporal associations (e.g. attending to the cued location in advance of the change time step).

Complementing spatial heatmaps, time-courses of attention allocation cued ($\alpha_1^{(t)}$) and uncued ($\alpha_4^{(t)}$) locations illustrate attention dynamics (\autoref{fig:attention}D). These time-courses clearly illustrate the absence of any difference between attention allocated to the cued vs. uncued locations at stimulus onset ($t=5$), the dependence of memory-based attention allocation before, during, and after the change on cue validity, and the reduction of attention allocated to $S_4$ accompanied by increased attention to $S_1$. 

Two features of the Recurrent ViT's attention allocation within and across timesteps closely resemble observations from human and NHP studies of attention. First, the lack of attention-related modulation of NHP neuronal activity at stimulus onset has been observed frequently (see for example ~\cite{herman2017colorchange, ghose2002temporal, ilaria2017v4gain, wang2015v1att, thompson1996fef}). Second, the allocation of attention to locations anticipated to contain behaviorally relevant visual information mirrors attention dynamics observed in humans and animals performing tasks in which sensory events occur at predictable times ~\cite{herman2017colorchange, ghose2002temporal, sharma2015temporal, jaramillo2011temporal, nobre2018temporal}. These parallels between our model's attention dynamics and those in primate brains suggest the Recurrent ViT has discovered attention deployment strategies that mirror those employed by primate visual systems.

Having found that input (e.g. cue onset) and memory interact in driving attention allocation, we were curious if there might be an interaction between orientation change magnitude $\Delta$ and cue-validity at the time of orientation change. We examined how attention allocated to the cued location varied with both change magnitude and cue validity, comparing trials where changes occurred at either the cued location (\autoref{fig:attention}B) or the opposing location (\autoref{fig:attention}C). When the change was at the cued location, $\alpha_1^{(t_\text{change})}$ increased sigmoidally with increasing $\Delta$ (\autoref{fig:attention}B). Large cued orientation changes "captured" attention regardless of cue validity but at smaller values attention allocation reflected cue validity more directly, consistent with the idea that cues primarily improve performance for subtle visual events. Conversely, when the change was at the uncued location, larger $\Delta$ values decreased attention allocated to the cued $S_1$ location ($\alpha_1^{(t_\text{change})}$) as attention was drawn to the uncued change location. There was also a more subtle interaction between uncued changes and cue validity: With 25\% cues, large uncued changes at $S_4$ drove $\alpha_1^{(t_\text{change})}$ near 0, indicating strong attentional capture by the uncued change. However, with increasing cue validity (\autoref{fig:attention}C), $\alpha_1^{(t_\text{change})}$ maintained progressively higher values even for large $\Delta$ at $S_4$, demonstrating that strongly predictive cues can partially maintain attention at the cued location in the face of competing visual events.

\subsection{Manipulating Bias Affects Response Rate and Reaction Times}

The ability to causally manipulate activity in primate brain regions like the frontal eye fields (FEF) and superior colliculus (SC) has provided foundational insights into the neuronal mechanisms of attention ~\cite{moore2003selective, cavanaugh2004subcortical, cavanaugh2006enhanced, mirpour2010microstimulation, bollimunta2018fefsc, monosov2011fef, zenon2012attention, herman2018midbrain}. The clear interpretability of our model’s spatial and temporal attention allocation dynamics offers a unique opportunity to test whether targeted perturbations of its self-attention mechanism produce effects analogous to these biological interventions. Demonstrating such parallels would provide a stringent validation that our Recurrent ViT captures not only correlational but also causal principles underlying primate attentional control.

Again resembling the results of NHP experiments, the behavioral consequences of attention manipulation were only observed when they were applied at the time of the change - manipulating attention at the time of cue presentation had minimal effects. In Supplemental Figure X, we systematically explore the effects of manipulating attention during cue presentation versus during the change event, and quantify those effects using signal detection theory's sensitivity ($d'$) and criterion. Increasing attention to the cued location during cue presentation ($\alpha_1^{(t_{\text{cue}})}$) had minimal impact on either sensitivity ($d'$) or criterion (Supplemental Figure A,D). The lack of an effect from manipulations at the time of the cue but presence of those effects from manipulations at the time of the change closely mirror findings from primate SC microstimulation experiments ~\cite{cavanaugh2006enhanced}. Thus, precisely timed attention perturbations reveal causal parallels to NHP microstimulation data, extending beyond simple correlation.

Several past works have proposed that attention manipulations resulting from perturbation of specific nodes in the primate brain might selectively influence ($d'$) or criterion ~\cite{sridharan2017does, luo2018attentional}. However, our model suggests a more nuanced reality - manipulating attention at the time of change produces complex, interrelated effects on both sensitivity and criterion that depend on cue validity and change location (Supplemental Figure 16). This finding highlights how attempting to assign distinct signal detection theory metrics to specific neural circuits may artificially compartmentalize what is fundamentally an integrated process. The joint modulation of $d'$ and criterion in our model emerges naturally from manipulating a single computational mechanism, suggesting that clean dissociations between these measures may not reflect the underlying neural implementation of attention. 

\begin{figure*}[!t]
    \centering
    \vspace*{-0mm} % Adjust this value to reduce the top margin
    \includegraphics[width=0.99\linewidth]{figures/fig_attendMod.png}
    \caption{Plots showing the effect of artificially modulating the bias. All data points are the result of an average over 500 trials. Artificial modulation involves inducing a high bias in a single spatial region (increasing the value of one of the patches in the self-attention maps from Figure~\ref{fig:attention}G). In all cases, the bias is induced with respect to the $S_1$ ($\alpha_1^{(t)}$) or the $S_4$ ($\alpha_4^{(t)}$) stimulus location. In \textbf{A--F}, we plot the response rates and reaction times versus the orientation change $\Delta$. If $\alpha_i^{(t_{\text{change}})}=1$, this indicates that the transmission $Z^{(t_{\text{change}})}$ has been completely biased toward $\xi_i^{(t_{\text{change}})}$. In \textbf{A--C} we show the effects of this manipulation on the response rates, and then again for the reaction times in \textbf{D--F}.}
    \label{fig:2}
\end{figure*}

\subsection{Alternative Architectures and Training Approaches Fail to Capture Primate-like Attention}

To validate our architectural choices and training approach, we systematically evaluated several alternative model variants. We tested different memory-attention integration schemes (tokens, additive, and multiplicative feedback) and we examined whether reinforcement learning was necessary by training two supervised variants of our architecture. Supervised models trained either on trial-type labels (change / no-change) or target action sequences achieved reasonable task performance but did not show a "cueing effect", instead detecting orientation changes similarly regardless of cue validity (\autoref{tab:model_comparison}). Alternative memory-attention architectures showed similar limitations - while the additive attention model demonstrated a weak version of anticipatory attention reallocation, both it and the token-based model failed to capture the rich temporal dynamics observed in primates. Only our RL-trained Recurrent ViT with multiplicative feedback produced the full-compliment of primate-like features we have documented above. These results suggest that both reinforcement learning and multiplicative interactions between memory and attention are critical for developing temporally structured attentional control that mirrors primate behavior.
\begin{table*}[b]
\caption{Comparison of model variants}
\centering
\renewcommand{\arraystretch}{1.2} % Adjust row height if needed
\begin{tabularx}{\textwidth}{
  >{\raggedright\arraybackslash}p{2.5cm} | 
  >{\raggedright\arraybackslash}p{3.0cm} | 
  >{\raggedright\arraybackslash}p{3.7cm} | 
  >{\raggedright\arraybackslash}p{3.5cm} | 
  >{\raggedright\arraybackslash}p{4.2cm}}
\toprule
\textbf{Model} & \textbf{Learning Source} & \textbf{Causal Perturbation Effects} & \textbf{Cueing Effect} & \textbf{Attention Dynamics} \\
\midrule
Recurrent ViT & Reward feedback from change detection & Strong modulation of behavior matching microstimulation effects & Maintains selective responding even at large $\Delta$ & Strong cue attention, broad monitoring, anticipatory reallocation \\
\midrule
Memory as Tokens & Reward feedback from change detection & Weak modulation of behavior & Weak separation between cue validities & No clear temporal structure \\
\midrule
Additive Attention & Reward feedback from change detection & Moderate modulation of behavior & Moderate separation between cue validities & Weak version of anticipatory pattern \\
\midrule
Supervised Beliefs & Binary trial-type labels & Minimal effect on behavior & Responds to all large changes regardless of cue & Weak, unstructured attention allocation \\
\midrule
Supervised Actions & Target action sequences & Minimal effect on behavior & Responds to all large changes regardless of cue & Weak, unstructured attention allocation \\
\bottomrule
\end{tabularx}
\label{tab:model_comparison}
\end{table*}


\section{Discussion}
In this work, we introduce a Recurrent Vision Transformer (Recurrent ViT) enhanced with a spatial memory module designed for a cued orientation change-detection task. Our central goal was to determine whether augmenting standard vision transformers—which typically rely on feedforward processing of single frames \cite{dosovitskiy2020image}—with a recurrent feedback mechanism can enable top-down, internally guided attentional control akin to that observed in human and non-human primate (NHP) vision \cite{mehrani2023self,baluch2011mechanisms,carrasco2011visual}. Our experiments demonstrate that the proposed Recurrent ViT successfully recapitulates many hallmark effects of primate visual attention.

\subsection{Recovering Hallmark Signatures of Primate Attention.}

First, our trained model shows improved performance and faster detection of orientation changes at cued locations, mirroring the well-documented behavioral effects of selective spatial attention \cite{carrasco2011visual,bisley2010lip,rust2022priority,hoffman2016visual}. These benefits emerge in situations where high-validity cues bias internal representations toward the cued location, but they taper off or reverse if competing salience signals (e.g., a large orientation change elsewhere) dominate the model’s self-attention. This interplay between cue validity and exogenous salience resonates with empirical observations that attentional allocation reflects both top-down predictions and bottom-up feature-driven signals \cite{baluch2011mechanisms,knudsen2007fundamental}. In standard feedforward ViTs, attention is inherently limited to correlational or grouping-based processes \cite{mehrani2023self,lamy2006grouping}, whereas our recurrent module explicitly integrates memories of cue identity, location, and temporal context—restoring top-down selectivity typically absent in off-the-shelf architectures.

\subsection{Relevance to Neural Mechanisms of Attention and Working Memory.}

The success of our Recurrent ViT underscores the deep links between attention and working memory reported in neuroscience \cite{awh2006interactions,kiyonaga2013working,bahle2018architecture,panichello2021attvwm}. Much like the “attentional template” theory, which proposes that memory representations guide attention to relevant features and locations \cite{cameron2002sens,wheeler2002binding,botta2014spatial}, our model maintains a set of spatial codes over time. These memory states re-enter a self-attention module to bias ongoing visual processing, effectively bridging top-down and bottom-up circuits \cite{desimone1995neural,moore2003selective,cavanaugh2006enhanced,krauzlis2013superior}. Correspondingly, the interplay between memory and perception in our model echoes the reciprocal loops seen in primate frontal, parietal, and subcortical structures, where neural firing maintains spatial priority during blank intervals and facilitates rapid reactivation at anticipated moments of stimulus change \cite{silver2005topographic,huda2020distinct,srinath2021attention}.

\subsection{Subcortical and Dopaminergic Influences.}
Although our model already uses reward feedback to guide learning, we have not explicitly integrated dopaminergic-like prediction error signals or examined how reward history might adaptively modulate attentional policies in the superior colliculus and related circuits \cite{bolton2015diencephalic,pradel2021superior,essig2016warning,perez2017direct}. In biological systems, dopamine critically mediates plasticity, enabling more nuanced shaping of attentional priorities over extended time scales \cite{hikosaka2006basal,hickey2010reward,failing2018selection}. A potential future extension is to incorporate a free-energy principle-inspired, unsupervised component \cite{friston2006free,feldman2010attention, friston2012dopamine,khezri2022free,mazzaglia2022free}, which could allow the model to learn latent, generative structure in its environment—paralleling how dopamine modulates not only immediate reward but also uncertainty and exploration in real brains. Such a framework would unify reward-driven reinforcement learning with a broader predictive coding approach, further enhancing the model’s capacity for dynamic, context-sensitive attention. 

\subsection{Constraints, Biological Plausibility, and Interpretability.}
A central feature of our model is the introduction of a recurrent spatial memory component that constrains information flow between image frames shown at different points in time. Unlike standard transformers—which can re-attend to entire sequences without constraint \cite{vaswani2017attention,dosovitskiy2020image,hassanin2024visattmod} or models that process full sequences of images \cite{bardes2023v}—our approach assigns each spatial patch of an immediate image to a single hidden-state slot within an LSTM. This imposed bottleneck encourages competition among representations, echoing the biased competition framework which posits that a finite “priority map” mediates interactions between bottom-up inputs and top-down influences \cite{desimone1995neural,reynolds1999competitive,bisley2019neural,wolfe2021guided,rust2022priority}. Although this capacity-limited design is consistent with psychophysical findings on the limited nature of visual working memory \cite{luck1997capacity,luck2013visual,brady2013probabilistic,emrich2017attention} and helps ensure that primarily task-relevant information is retained \cite{desimone1996neural,van2009limits,lee2015encoding}, it represents just one plausible mechanism among several. In contrast, other models may integrate information over time without such strict constraints, potentially capturing different aspects of attentional processing. Our approach, therefore, offers a balanced compromise that mirrors key behavioral observations while providing a tractable, interpretable framework for studying top-down attention.

Furthermore, our framework offers a degree of interpretability by linking each attention weight to both an immediate visual patch and its corresponding memorized representation. This mapping holds promise for developing in silico experiments that could approximate, in a controlled manner, the effects observed in microstimulation or lesion studies in non-human primates \cite{cavanaugh2006enhanced,mirpour2010microstimulation}. In our preliminary experiments, targeted adjustments of self-attention weights led to systematic changes in detection rates and reaction times. While further validation is required, this controlled perturbation approach may serve as a valuable tool for exploring causal relationships between attention and working memory in computational models \cite{moore2003selective,ruff2016attention,herman2020attention}.

\subsection{Broader Implications and Future Directions.}
This Recurrent ViT opens several avenues for future research. First, richer scenarios—such as multi-object tracking \cite{bettencourt2009effects,meyerhoff2017studying}, dynamic scene understanding \cite{wolfe2011visual,lamy2006grouping}, or tasks requiring mid-trial updates to memorized stimuli \cite{tas2016relationship,gresch2024shifting}—would extend our approach and further test its alignment with primate attentional performance. Incorporating saccadic eye movements, akin to real-world visual search, could allow the model to learn optimal covert and overt strategies in tandem \cite{krauzlis2013superior,gupta2024presaccadic}. Additionally, scaling up to deeper, multilayer recurrent architectures may capture the intricate, multi-level feedback loops characteristic of the primate cortex \cite{felleman1991distributed,kietzmann2019recurrence,zhuang2021unsupervised,khan2022transformers}.

Second, bridging our Recurrent ViT with reinforcement learning frameworks that incorporate explicit dopamine-like signals \cite{botvinick2020deep,babayan2018belief,hikosaka2006basal} could elucidate how value-based attentional modulation emerges in tandem with memory demands. This expansion would dovetail with broader theories of attention, memory, and decision-making as components of a common, computationally grounded process \cite{knudsen2007fundamental,monosov2020outcome}. Finally, the interpretability of our approach may inspire future “virtual lesion” or “virtual microstimulation” studies to dissect precisely how feedback, gating, and local competition produce emergent attentional dynamics—a goal shared across computational neuroscience and AI \cite{mante2013context, krauzlis2013superior, gattass2014effect,miconi2016feedback, liu2024human, cartella2024trends}.

\subsection{Conclusion.}
Taken together, these findings demonstrate that recapitulating human/NHP-like attention in transformer architectures is possible by introducing explicit top-down influences and recurrent feedback. Our Recurrent ViT significantly narrows the gap between standard, feedforward models of attention \cite{vaswani2017attention,dosovitskiy2020image} and the iterative, memory-intensive processes that characterize primate visual cognition \cite{rust2022priority,panichello2021attvwm}. By unifying principles of biased competition, working memory, and reward-driven learning within a single framework, we not only advance the biological plausibility of deep vision models but also generate a versatile platform for addressing fundamental questions about how perception, memory, and attention converge to guide adaptive behavior.

\section{Methods}

\subsection{Model Overview}

The objective of the model is to utilize immediate visual inputs in order to update an internal state with sufficient immediate and past visual information such that downstream decoders can estimate value and take action. We utilize a self-attention (SA) mechanism to construct the visual percept used to update an internal state of an RNN. Self-attention is computed based on the immediate visual inputs and feedback from the RNN. The following sections will describe the motivations and details of this process in more depth.   



\subsection{Pre-Processing, Content Selection, and Construction for Visual Working Memory}
Given a visual scene (an image) of dimension $H \times W \times C$, denoted by $\mathbf{\mathcal{O}}^{(t)} \in \mathbb{R}^{H \times W \times C}$ at time $t$, the agent views the entire scene through a fixation at center field. During preprocessing, the image is partitioned into a set of patches, $\{\mathbf{{o}_i}^{(t)}\}_{i=1}^{n_{patch}}$, where each patch is of size $H_{patch} \times W_{patch} \times C_{patch}$. The original visual patches are then transformed into a compact set of internal representations, $\{\mathbf{x_i}^{(t)}\}_{i=1}^{n_{patch}}$, each $\mathbf{x_i}^{(t)} \in \mathbb{R}^{H_{patch} \times W_{patch} \times C_{patch}}$ and typically satisfying
\[
    \dim(\mathbf{x_i}^{(t)}) \ll \dim\bigl(\mathbf{o_i}^{(t)}\bigr).
\]
Together, the collection of these feature patches forms the immediate visual information available to the agent at time $t$, denoted by $\mathbf{X}^{(t)} = \{\mathbf{x_i}^{(t)}\}_{i=1}^{n_{patch}}$.


\subsection{Spatially Oriented Visual Working Memory}

Following numerous experimental findings, we allow our model to maintain a spatially arranged visual working memory \cite{wheeler2002binding, pertzov2014privileged,schneegans2017neural, van2019human}, in which each patch location $i$ has a corresponding patched memory component $\mathbf{c_i}^{(t)} \in \mathbb{R}^{d_{mem}}$ within the RNN. For updating the internal state of our RNN, we utilize the operations and functions described in the LSTM architecture \cite{hochreiter1997long, beck2024xlstm}. For the remainder of our description, we will refer to the collection $C^{(t)}=\{c_i^{(t)}\}_{i=1}^{n_{patch}}$ the VWM state and $c_i^{(t)}$ a VWM patch, where
\begin{equation}
    \label{eq:C}
    \mathbf{c_i}^{(t)} =  \mathbf{f_i}^{(t)} \odot \mathbf{c_i}^{(t-1)} + \mathbf{u_i}^{(t)} \odot \boldsymbol{\psi_i}^{(t)},
\end{equation}
where 
\begin{align*}
\mathbf{f_i}^{(t)} &= F\!\bigl(\mathbf{x_i}^{(t)},\, \mathbf{h_i}^{(t-1)}\bigr), \\
\mathbf{u_i}^{(t)} &= U\!\bigl(\mathbf{x_i}^{(t)},\, \mathbf{h_i}^{(t-1)}\bigr), \\
\boldsymbol{\psi_i}^{(t)} &= \Psi\!\bigl(\mathbf{x_i}^{(t)},\, \mathbf{h_i}^{(t-1)}\bigr).
\end{align*}
The operator $\odot$ denotes elementwise multiplication. Here, $\mathbf{f_i}^{(t)} \in [0,1]^{\,d_{mem}}$ determines which parts of $\mathbf{c_i}^{(t-1)}$ are \textit{forgotten} (i.e., decayed), while $\mathbf{u_i}^{(t)} \in [-1,1]^{\,d_{mem}}$ and $\boldsymbol{\psi_i}^{(t)} \in \mathbb{R}^{d_{mem}}$ selectively modulate and propose new content. Altogether, these operations enable dynamic insertion, maintenance, and forgetting of information in $\mathbf{c_i}^{(t)}$. 

The activated memory patch, $h_i^{(t)}$ is constructed from the VWM patch, $c_i^{(t)}$:
\[
    \mathbf{h_i}^{(t)} = \boldsymbol{\phi_i}^{(t)} \odot \Bigl(\tfrac{\mathbf{c_i}^{(t)}}{\mathbf{n_i}^{(t)}}\Bigr),
\]
where $\boldsymbol{\phi_i}^{(t)} = \Phi\!\bigl(\mathbf{x_i}^{(t)}, \mathbf{h_i}^{(t-1)}\bigr) \in [0,1]^{\,d_{mem}}$ selects elements of $\mathbf{c_i}^{(t)}$ for downstream processing, and $\mathbf{n_i}^{(t)}$ is a normalization term. Each function $F, U, \Psi, \Phi$ is parameterized by a feedforward neural network that receives $\mathbf{x_i}^{(t)}$ and $\mathbf{h_i}^{(t-1)}$ as inputs. Although the parameters of these functions remain fixed once trained, the recurrent operation through $\mathbf{h_i}^{(t)}$ allows the system to track temporal dynamics.


\subsection{A Disjoint Memory}
Following the ideas presented by Knudsen \cite{knudsen2007fundamental}, the activated subset of working memeory is central for decision-making and planning. However, a patched RNN as described above presents a clear shortcoming: each $\mathbf{h_i}^{(t)}$ encodes the content of its own patch independently, without explicit awareness of neighboring patches. If downstream processes (\emph{e.g.}, a decoder $\pi$) require spatial or contextual relationships among patches, they must construct these relationships entirely from the population of VWM patches. Moreover, the problem intensifies over time. If the network must integrate information from patches across multiple timesteps (e.g., $\mathbf{x_i}^{(\tau)}$ and $\mathbf{x_j}^{(\tau)}$ for $\tau < t$), then each VWM patch must \emph{retain} all potentially significant current and past features useful for task-relevant decoding by downstream networks. This approach quickly becomes intractable, as it demands that the architecture store a large number of unique \emph{conjunctions} of spatio-temporal features. This challenge aligns with the combinatorial explosion recognized by Tsotsos \cite{tsotsos1988complexity} as a core difficulty in perceptual organization. 

To circumvent these limitations, we introduce self-attention into the encoding process, encouraging each patch’s representation to reflect the context provided by the other patches within the same timestep. In doing so, we create a spatially integrated or \emph{context-aware} activated memory before the information even updates the VWM.

\subsection{Self-Attention and Spatially Aware activated memory}

We wish to obtain a scene-level representation $\mathbf{Z}^{(t)} = \{\mathbf{z_i}^{(t)}\}_{i=1}^{n_{patch}}$ such that each $\mathbf{z_i}^{(t)}$ encodes the task-relevant spatial relationships among the visual feature patches $\mathbf{x_1}^{(t)}, \dots, \mathbf{x_{n_{patch}}}^{(t)}$. By doing so, the patch-based LSTM will be able to utilize immediate visual information within a patch and task-relevant spatial context to update internal states. Formally, we want
\[
\mathbf{z_i}^{(t)} = f_z\Bigl(\mathbf{x_i}^{(t)}, \{\mathbf{x_j}^{(t)}\}_{j\neq i}\Bigr),
\]
A straightforward way to implement this is via self-attention:
\begin{equation}
    \label{eq:Z}
    \mathbf{z_i}^{(t)} = \mathbf{x_i}^{(t)} + \sum_{j=1}^{n_{patch}} a_{ij}^{(t)}\, \mathbf{v_j}^{(t)},
\end{equation}
where $\mathbf{v_j}^{(t)} = V\!\bigl(\mathbf{x_j}^{(t)}\bigr)$ is a function that maps the feature patch into a latent space and $a_{ij}^{(t)} = A\bigl(\mathbf{x_i}^{(t)}, \mathbf{x_j}^{(t)}\bigr)$ indicates the \emph{relative importance} of $\mathbf{x_j}^{(t)}$ with respect to $\mathbf{x_i}^{(t)}$. To ensure a proper probability-like weighting, we impose $\sum_{j=1}^{n_{patch}} a_{ij}^{(t)} = 1$ with $a_{ij}^{(t)} \in (0,1)$. A typical choice for $a_{ij}^{(t)}$ is:
\[
    a_{ij}^{(t)} \;=\; \frac{\exp\Bigl(\bigl\langle \mathbf{q_i}^{(t)}, \mathbf{k_j}^{(t)}\bigr\rangle\Bigr)}
    {\sum_{m=1}^{n_{patch}} \exp\Bigl(\bigl\langle \mathbf{q_i}^{(t)}, \mathbf{k_m}^{(t)}\bigr\rangle\Bigr)},
\]
where $\mathbf{q_i}^{(t)} = Q\!\bigl(\mathbf{x_i}^{(t)}\bigr)$ and $\mathbf{k_j}^{(t)} = K\!\bigl(\mathbf{x_j}^{(t)}\bigr)$ are \emph{query} and \emph{key} functions, respectively. Interpreting $a_{ij}^{(t)}$ as a salient feature map has strong parallels to the saliency map hypothesis \cite{koch1984selecting}; however, we adopt a \emph{winner-takes-most} approach rather than a strict winner-takes-all (WTA), common in many self-attention applications. In principle, should $a_{i,j^*}^{(t)} \approx 1$ for some $j^*$ and $a_{i,m}^{(t)} \approx 0$ for $m \neq j^*$, we recover WTA-like mechanism.

After computing $\mathbf{Z}^{(t)}$ for the entire scene, the visual percept patches, $z_i^{(t)}$, are used to update the VWM patches:
\[
    \mathbf{c_i}^{(t)} \;=\;  \mathbf{f_i}^{(t)} \odot \mathbf{c_i}^{(t-1)} \;+\; \mathbf{u_i}^{(t)} \odot \boldsymbol{\psi_i}^{(t)},
\]
where each function is now evaluated using $\mathbf{z_i}^{(t)}$ rather than the immediate visual scene patch in isolation, $\mathbf{x_i}^{(t)}$. This approach solves the \emph{spatial} integration problem in the current timestep. Yet, any feature with contextual importance \emph{across} timesteps remains challenging: we still need a mechanism to capture top-down feedback or \emph{memory-based} salience.

\subsection{Recurrent Feedback From Memory}

Knudsen \cite{knudsen2007fundamental} describes a feedback loop in which working memory provides top-down signals that bias neural representations relevant to the organism’s current goals. In the context of the biased competition model \cite{desimone1995neural}, working memory holds an \emph{attentional template} that biases competition in favor of task-relevant representations. However, in practice it is not clear how this mnemonic feedback is/should be implemented. In this we simplify (and constrain) the problem to implementing recurrent feedback from the patch-based LSTM to the self-attention mechanism of the ViT. Hence, we evaluate three different methods in terms of their ability to yield primate-like behavior signatures of attention. We call the vision transformer with mnemonic feedback the recurrent ViT.   

\subsection{Mnemonic Guidance}

\subsubsection{Visual working memory as tokens}

The first recurrent feedback method we evaluate is one in which we concatenate the mnemonic percept to the visual input. Thus, the input to the self-attention mechanism is
\[
\mathbf{\tilde{X}} = Concatenate[\mathbf{X}^{(t)},\mathbf{H}^{(t-1)}]
\]
where $\mathbf{\tilde{X}}\in\mathbb{R}^{2n_{patch},d_{model}}$. From here we define:
\begin{align*}
\mathbf{q}_{\tilde{X},i}^{(t)} &= Q_{\tilde{X}}\bigl(\mathbf{\tilde{x}}_i^{(t)}\bigr), \\
\mathbf{k}_{\tilde{X},j}^{(t)} &= K_{\tilde{X}}\bigl(\mathbf{\tilde{x}}_j^{(t)}\bigr), \\
\mathbf{v}_{\tilde{X},j}^{(t)} &= V_{\tilde{X}}\bigl(\mathbf{\tilde{x}}_j^{(t)}\bigr),
\end{align*}


The attention weights are given by
\begin{equation}
\label{eq:add_alpha}
\alpha_{i,j}^{(t)} \;=\;
\frac{\exp\Bigl(\langle \mathbf{q}_{\tilde{X},i}^{(t)} ,\;
\mathbf{k}_{\tilde{X},j}^{(t)}  \rangle\Bigr)}
{\sum_{m=1}^{n_{\text{patch}}}
\exp\Bigl(\langle \mathbf{q}_{\tilde{X},i}^{(t)} ,\;
\mathbf{k}_{\tilde{X},m}^{(t)} \rangle\Bigr)}.
\end{equation}
We then compute the output representation as
\begin{equation}
\label{eq:add_z}
\mathbf{z}_i^{(t)} \;=\; \mathbf{x}_i^{(t)}
\;+\; \sum_{j=1}^{2n_{\text{patch}}}
\alpha_{i,j}^{(t)} \mathbf{v}_{\tilde{X},j}^{(t)} .
\end{equation}
Here, we only take the first $n_{patch}$ entries $\{\mathbf{z}^{(t)}_{i}\}_{i=1}^{n_{patch}}$. The reason for this is because there are only $n_{patch}$ recurrent states in the patch-based LSTM, and $Z^{(t)}=\{\mathbf{z}^{(t)}\}_{i=1}^{n_{patch}}$ is only used as the input to the LSTM. 

\subsubsection{Additive Feedback from Visual Working Memory}

We split the standard self-attention operation into two parallel pathways: one for the bottom-up immediate visual inputs, \(\mathbf{x}_i^{(t)}\), and one for the top-down mnemonic inputs, \(\mathbf{h}_i^{(t)}\). Define:
\begin{align*}
\mathbf{q}_{X,i}^{(t)} &= Q_X\bigl(\mathbf{x}_i^{(t)}\bigr), \\
\mathbf{k}_{X,j}^{(t)} &= K_X\bigl(\mathbf{x}_j^{(t)}\bigr), \\
\mathbf{v}_{X,j}^{(t)} &= V_X\bigl(\mathbf{x}_j^{(t)}\bigr).
\end{align*}
and
\begin{align*}
\mathbf{q}_{H,i}^{(t)} &= Q_H\bigl(\mathbf{h}_i^{(t)}\bigr), \\
\mathbf{k}_{H,j}^{(t)} &= K_H\bigl(\mathbf{h}_j^{(t)}\bigr), \\
\mathbf{v}_{H,j}^{(t)} &= V_H\bigl(\mathbf{h}_j^{(t)}\bigr).
\end{align*}
The attention weights are given by
\begin{equation}
\label{eq:add_alpha}
\alpha_{i,j}^{(t)} \;=\;
\frac{\exp\Bigl(\langle \mathbf{q}_{X,i}^{(t)} + \mathbf{q}_{H,i}^{(t)},\;
\mathbf{k}_{X,j}^{(t)} + \mathbf{k}_{H,j}^{(t)} \rangle\Bigr)}
{\sum_{m=1}^{n_{\text{patch}}}
\exp\Bigl(\langle \mathbf{q}_{X,i}^{(t)} + \mathbf{q}_{H,i}^{(t)},\;
\mathbf{k}_{X,m}^{(t)} + \mathbf{k}_{H,m}^{(t)} \rangle\Bigr)}.
\end{equation}
We then compute the output representation as
\begin{equation}
\label{eq:add_z}
\mathbf{z}_i^{(t)} \;=\; \mathbf{x}_i^{(t)}
\;+\; \sum_{j=1}^{n_{\text{patch}}}
\alpha_{i,j}^{(t)} \Bigl(\mathbf{v}_{X,j}^{(t)} \;+\; \mathbf{v}_{H,j}^{(t)}\Bigr).
\end{equation}
In this additive design, features from the visual inputs and the mnemonic percept patches \(\{\mathbf{h}_i^{(t)}\}\) both contribute to the self-attention mechanism by modifying the inner product in the numerator of~\eqref{eq:add_alpha} and by merging the corresponding values in~\eqref{eq:add_z}. 

\subsubsection{Multiplicative Feedback from Visual Working Memory}

To incorporate multiplicative feedback, we instead define:
\begin{equation}
\label{eq:mult_alpha}
\alpha_{i,j}^{(t)} \;=\; 
\frac{\exp\Bigl(\langle \mathbf{q}_{X,i}^{(t)} \odot \mathbf{q}_{H,i}^{(t)},\;
\mathbf{k}_{X,j}^{(t)} \odot \mathbf{k}_{H,j}^{(t)} \rangle\Bigr)}
{\sum_{m=1}^{n_{\text{patch}}}
\exp\Bigl(\langle \mathbf{q}_{X,i}^{(t)} \odot \mathbf{q}_{H,i}^{(t)},\;
\mathbf{k}_{X,m}^{(t)} \odot \mathbf{k}_{H,m}^{(t)} \rangle\Bigr)},
\end{equation}
and
\begin{equation}
\label{eq:mult_z}
\mathbf{z}_i^{(t)} \;=\;
\mathbf{x}_i^{(t)}
\;+\; \sum_{j=1}^{n_{\text{patch}}}
\alpha_{i,j}^{(t)} 
\Bigl(\mathbf{v}_{X,j}^{(t)} \;\odot\; \mathbf{v}_{H,j}^{(t)}\Bigr).
\end{equation}
Here, the top-down feedback pathway multiplicatively gates the bottom-up signals. As a result, larger (smaller) magnitudes in the memory pathway can amplify (suppress) the corresponding magnitudes in the immediate visual pathway. This scheme allows for more direct \emph{control} (through multiplication) of attention weights and context vectors, enabling stronger or weaker gating of specific patches.

Additive operations can be dominated by whichever pathway has a larger magnitude, potentially diminishing subtler signals. By contrast, multiplicative modulation can act as a direct ``sign-flip'' mechanism or a global rescaling factor, making it inherently well-suited for precise top-down control. For instance, consider a scenario in which \(\mathbf{q}_{X,i}\) or \(\mathbf{k}_{X,j}\) contain elements \(\pm 2\). A feedback mechanism that must flip selected signs via \emph{addition} could require large compensatory values in \(\mathbf{q}_{H,i}\) or \(\mathbf{k}_{H,j}\). In contrast, a multiplicative pathway can achieve such sign flips with a scalar factor of \(-1\), regardless of the original magnitude in \(\mathbf{q}_{X,i}\) or \(\mathbf{k}_{X,j}\).


\section{Model Architecture}

Our model integrates a Vision Transformer (ViT) with a patch-based LSTM. First, a VAE is used to preprocess the raw visual features in a purely feed-forward method. Secondly, we utilize a recurrent ViT in which self-attention has been modified to incorporate immediate and recurrent inputs in order to construct the visual percept transmitted to the patch-based LSTM. Thirdly, the LSTM utilizes the projection from the recurrent ViT to update the patch-based internal states. 

\subsection{VAE Pre-Processing}

A Variational Autoencoder (VAE) is a generative model that learns to encode input data into a latent space and reconstructs the data from this latent representation. It combines principles from deep learning and probabilistic inference, making it suitable for modeling complex data distributions. It consists of two primary components, and encoder ($F$) that encodes visual inputs to a probabilistic latent space ($z_{latent}$), and a decoder ($G$) that decodes a sampled latent vector into a visual input.  

The encoder network $F$ entails multiple operations, $f\in F$ which serve to map an input image patch $\mathbf{o}_i \in \mathbb{R}^{H_{patch} \times W_{patch} \times C}$ to a latent representation characterized by a mean vector $\boldsymbol{z_{\mu}} \in \mathbb{R}^{d_{latent}}$ and a log-variance vector $\boldsymbol{z_{logvar}} \in \mathbb{R}^{d_{latent}}$, where $d_{latent}$ is the dimensionality of the latent space. The encoder consists of convolutional and fully connected layers as follows:

\begin{enumerate}
    \item \textbf{First Convolutional Layer}: Applies a convolution with 16 filters, each of size $3 \times 3$, stride 2, and padding 1. This operation reduces the spatial dimensions while increasing the feature depth. The activation function is ReLU:
    \[
    \mathbf{z_{Conv,1}} = \mathrm{ReLU}\left(f_{Conv,1}^{(1,16,3,2,1)}(\mathbf{o_i}) \right)
    \]
    \item \textbf{Second Convolutional Layer}: Applies a convolution with 32 filters, each of size $3 \times 3$, stride 2, and padding 1:
    \[
    \mathbf{z_{Conv,2}} = \mathrm{ReLU}\left( f_{Conv,2}^{(16,32,3,2,1)}(\mathbf{z_{Conv,1}}) \right)
    \]
    \item \textbf{Flattening}: The output tensor is reshaped into a vector:
    \[
    \mathbf{z_{flat,1}} = \mathrm{Flatten}(\mathbf{z_{Conv,2}})
    \]
    \item \textbf{First Fully Connected Layer}: Maps the flattened vector to a 128-dimensional feature vector:
    \[
    \mathbf{z_{flat,2}} = \mathrm{ReLU}\left( \mathbf{W}_1 \mathbf{z}_\text{flat} + \mathbf{b}_1 \right)
    \]
    \item \textbf{Latent Variable Parameters}: Computes the mean and log-variance vectors using two separate linear transformations:
    \begin{align*}
    \boldsymbol{z_\mu} &= \mathbf{W}_\mu \mathbf{z_{flat,2}} + \mathbf{b}_\mu, \\
    \boldsymbol{z_{\log\sigma^2}} &= \mathbf{W}_{\log\sigma^2} \mathbf{z_{flat,2}} + \mathbf{b}_{\log\sigma^2}
    \end{align*}
\end{enumerate}

To allow gradient-based optimization through stochastic sampling, we employ the reparameterization trick. Letting $\mu = \mathbf{z_\mu}$ and $\sigma = \exp(0.5 \mathbf{z_{logvar}})$ we draw a latent vector $\mathbf{z_{latent}}$ from the approximate posterior:
\[
\mathbf{z_{latent}} = \boldsymbol{\mu} + \boldsymbol{\sigma} \odot \boldsymbol{\epsilon}, \quad \boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})
\]
where $\boldsymbol{\sigma} = \exp\left( \frac{1}{2} \boldsymbol{\log\sigma^2} \right)$, and $\odot$ denotes element-wise multiplication.

The decoder network $G$ maps the latent vector $\mathbf{z}$ back to the reconstructed image $\hat{\mathbf{o}_i}$. The decoder mirrors the encoder but uses transposed convolutions:

\begin{enumerate}
    \item \textbf{First Fully Connected Layer}: Transforms the latent vector to a 128-dimensional vector:
    \[
    \mathbf{\hat{o}_{flat,1}} = \mathrm{ReLU}\left( \mathbf{W}_{flat,1} \mathbf{z_{latent}} + \mathbf{b}_{flat,1} \right)
    \]
    \item \textbf{Second Fully Connected Layer}: Maps the 128-dimensional vector to a shape suitable for convolutional layers:
    \[
    \mathbf{\hat{o}_{flat,2}} = \mathrm{ReLU}\left( \mathbf{W}_{flat,2} \mathbf{\hat{o}_{flat,1}} + \mathbf{b}_{flat,2} \right)
    \]

    \item \textbf{First Transposed Convolutional Layer}: Applies a transposed convolution with 16 filters:
    \[
    \mathbf{\hat{o}_{ConvT,1}} = \mathrm{ReLU}\left( g_{ConvT,1}^{(32,16,3,2,1,0)}(\mathbf{\hat{o}_{flat,2}}) \right)
    \]
    \item \textbf{Second Transposed Convolutional Layer}: Applies a transposed convolution to reconstruct the image:
    \[
    \mathbf{\hat{o}_{ConvT,2}} = \textrm{Sigmoid}\left( g_{ConvT,2}^{(16,1,3,2,1,0)}(\mathbf{\hat{o}_{ConvT,1}}) \right)
    \]
\end{enumerate}

The VAE optimizes a loss function that combines reconstruction accuracy and the Kullback-Leibler (KL) divergence between the approximate posterior and the prior distribution. Letting $\mathbf{\hat{o}_i} = \mathbf{\hat{o}_{ConvT,2}}$ the loss is described as:
\[
\mathcal{L} = \frac{1}{d_{image}} \| \mathbf{o_i} - \hat{\mathbf{o_i}} \|^2 - \beta \cdot \frac{1}{2} \left( 1 + \log \boldsymbol{\sigma_i}^2 - \boldsymbol{\mu_i}^2 - \boldsymbol{\sigma_i}^2 \right)
\]
where $\beta$ is a hyperparameter that balances the two terms, i represents the image patch number, and $d_{image} = H_{patch} \times W_{patch} \times C$


\subsection{ViT}
Input images $\mathbf{O}^{(t)} \in \mathbb{R}^{50 \times 50}$ are sub-divided into four equal patches $\{\mathbf{o_1}^{(t)}, \mathbf{o_2}^{(t)}, \mathbf{o_3}^{(t)}, \mathbf{o_4}^{(t)}\}$, with $\mathbf{o_i}^{(t)}\in\mathbb{R}^{(25\times25)}$. We found that our RL agent learned fastest, was most interpretable, and demonstrated best performance when we used the second flattend encoder layer ($\mathbf{o_{flat,2}}$) as input to the ViT (as oppose to the latent encoding). Hence, for a given patch input $\mathbf{o_i}^{(t)}$ at time $t$, we have the encoding
\begin{equation*}
    \mathbf{\hat{o}_i}^{(t)} = f^*(\mathbf{o_i}^{(t)})
\end{equation*}
where $f^*(\cdot)$ includes encoder components (1)--(4). We also concatenate a (one-hot) positional ($\boldsymbol{\rho_i}$) and temporal  ($\boldsymbol{\tau}$) encoding. Thus the full pre-processed patch input at timestep $t$ is
\begin{equation}
    \mathbf{x_i}^{(t)} = \text{Concat}[\mathbf{\hat{o}_i}^{(t)}, \boldsymbol{\rho_i}, \boldsymbol{\tau}]
\end{equation}

The complete input to the ViT at time step $t$ is:
\begin{equation}
    \mathbf{X}^{(t)}= (\mathbf{x_1}^{(t)}, \mathbf{x_2}^{(t)}, \mathbf{x_3}^{(t)}, \mathbf{x_4}^{(t)})^T \in \mathbb{R}^{4 \times 140}
\end{equation}
The transformer computes queries, keys, and values as:
\begin{align}
    \mathbf{Q} &= (\mathbf{X^{(t)} W_{XQ}}) \odot (\mathbf{H^{(t-1)} W_{HQ}}) \\
    \mathbf{K} &= (\mathbf{X^{(t)} W_{XK}}) \odot (\mathbf{H^{(t-1)} W_{HK}}) \\
    \mathbf{V} &= (\mathbf{X^{(t)} W_{XV}}) \odot (\mathbf{H^{(t-1)} W_{HV}})
\end{align}
where $\mathbf{W_{X\cdot}} \in \mathbb{R}^{140 \times 140}$, $\mathbf{W_{H\cdot}} \in \mathbb{R}^{1024 \times 140}$, $\mathbf{H}^{(t-1)}$ is the activated memory from the previous timestep, $\odot$ denotes Hadamard product, and we have dropped the temporal superscript (implicit). Self-attention is computed as:
\begin{equation}
    \mathbf{V_{{filtered}}} = \text{Softmax}(\mathbf{QK}^T)\mathbf{V}
\end{equation}
The spatially and temporally aware visual percept is constructed as follows:
\begin{equation}
    \mathbf{Z}^{(t)} = \mathbf{X}^{(t)} + \mathbf{V_{filtered}} \in \mathbb{R}^{4 \times 140}
\end{equation}

\subsection{Spatial LSTM}

We adapt the xLSTM architecture \cite{beck2024xlstm} for spatial memory. The LSTM operations are shown in \autoref{eq:recurrent_updates}. The projection matrices have dimensions $\mathbf{W_{x}} \in \mathbb{R}^{140 \times 1024}$, and $\mathbf{R_{x}} \in \mathbb{R}^{140 \times 1024}$. This ensures all output variables have shape $4 \times 1024$. As described above, we call this a patch-based LSTM because there is a hidden state for each patch of the visual scene. Importantly, within the LSTM the hidden states are updated independently. The matrices $\mathbf{Z}^{(t)}$, $\mathbf{C}^{(t)}$, $\mathbf{H}^{(t)}$, $\mathbf{M}^{(t)}$, and $\mathbf{N}^{(t)}$ are of shape $n_{patch}$ by $d$, where $d\in{d_{latent}, d_{mem}}$. Right multiplication by the matrices $\mathbf{W_x}$ or $\mathbf{R_x}$ projects the latent embedding or hidden state of a specific patch to another space, independent of the other patches. By constructions, self-attention is the only mechanism by which information from visual patches (or mnemonic patches) is communicated to other patches. 
% Inside the document where you want the equation:
\begin{table*}[b] % The [b] forces placement at the bottom
\captionsetup{position=bottom} % Ensures caption is below the equation
\centering
\small
\begin{align*}
    \mathbf{\tilde{I}}^{(t)} &= \mathbf{Z}^{(t)} \mathbf{W_i} + \mathbf{H}^{(t-1)} \mathbf{R_i} & 
    \mathbf{I}^{(t)} &= \exp(\mathbf{\tilde{I}}^{(t)} - \mathbf{M}^{(t)}) & 
    \mathbf{O}^{(t)} &= \sigma(\mathbf{\tilde{O}}^{(t)}) \\
    \mathbf{\tilde{F}}^{(t)} &= \mathbf{Z}^{(t)} \mathbf{W_f} + \mathbf{H}^{(t-1)} \mathbf{R_f} & 
    \mathbf{F}^{(t)} &= \exp(\mathbf{\tilde{F}}^{(t)} + \mathbf{M}^{(t-1)} - \mathbf{M}^{(t)}) & 
    \mathbf{N}^{(t)} &= \mathbf{F}^{(t)} \odot \mathbf{N}^{(t-1)} + \mathbf{I}^{(t)} \\
    \mathbf{\tilde{O}}^{(t)} &= \mathbf{Z}^{(t)} \mathbf{W_o} + \mathbf{H}^{(t-1)} \mathbf{R_o} & 
    \mathbf{M}^{(t)} &= \max(\mathbf{\tilde{F}}^{(t)} + \mathbf{M}^{(t-1)}, \mathbf{\tilde{I}}^{(t)}) & 
    \mathbf{U}^{(t)} &= \tanh(\mathbf{\tilde{U}}^{(t)}) \\
    \mathbf{\tilde{U}}^{(t)} &= \mathbf{Z}^{(t)} \mathbf{W_u} + \mathbf{H}^{(t-1)} \mathbf{R_z} & 
    \mathbf{C}^{(t)} &= \mathbf{C}^{(t-1)} \odot \mathbf{F}^{(t)} + \mathbf{U}^{(t)} \odot \mathbf{I}^{(t)} & 
    \mathbf{H}^{(t)} &= \mathbf{O}^{(t)} \odot (\mathbf{C}^{(t)} / \mathbf{N}^{(t)})
\end{align*}
\caption{Equations defining the recurrent network update process.}
\label{eq:recurrent_updates}
\end{table*}

\subsection{Actor-Critic Reinforcement Learning}

Our model is trained using an actor–critic reinforcement learning (RL) framework~\cite{sutton2018reinforcement} in which the agent learns to select actions that maximize long‐term rewards. At each timestep, the agent observes a mnemonic percept 
\[
\mathbf{H}^{(t)} \in \mathbb{R}^{4\times1024},
\]
which encodes spatial and temporal context, and selects an action from a binary set:
\[
a_t = 
\begin{cases}
0, & \text{(``wait'' action)}\\[1mm]
1, & \text{(``declare change'' action)}
\end{cases}.
\]
The value function
\begin{equation}
    V(H^{(t)}) = \mathbb{E}\!\left[\sum_{\tau=t}^{T} \gamma^{\tau-t} \, r_\tau \,\bigg|\, H^{(t)}\right]
    \label{eq:V_new}
\end{equation}
estimates the expected return from the current memory state, where \(\gamma\in[0,1]\) is the discount factor, \(r_\tau\) is the reward at time \(\tau\), and \(T\) is the final timestep.

Learning is driven by the temporal difference (TD) error,
\begin{equation}
    \delta_t = r_t + \gamma\,V(H^{(t+1)}) - V(H^{(t)}),
    \label{eq:td_error_new}
\end{equation}
which is used to update both the critic (value) and the actor (policy) networks. The policy is adjusted to favor actions with higher estimated returns, enabling the agent to improve its decision-making based on experience.

% \vspace{1mm}
% \noindent\rule{\linewidth}{0.4pt}
% \vspace{1mm}
\subsection{Distributional Framing, Network Architecture, and Loss Functions}

In our approach the critic network estimates a \emph{distributional} Q-function rather than a single scalar value. For a state–action pair \((H_t,a_t)\), the critic outputs a probability distribution over 15 discrete Q-value bins. The critic network is defined as follows:
\begin{align*}
    a' &= a_t W_a + b_a, \\
    q_0 &= \text{Concat}[H',\, a'], \\
    q_1 &= \text{ELU}(q_0 W_1 + b_1), \\
    q_2 &= \text{ELU}(q_1 W_2 + b_2), \\
    q_3 &= \text{ELU}(q_2 W_3 + b_3), \\
    p_\theta(q\mid H_t,a_t) &= \text{Softmax}\Bigl(q_3 W_{\text{out}} + b_{\text{out}}\Bigr),
\end{align*}
where \(H'\in\mathbb{R}^{4096}\) is the flattened mnemonic percept.

The improved (target) policy is defined as
\begin{equation*}
    \pi_{\text{imp}}(a_t\mid s_t) \propto \exp\!\Bigl(\frac{Q_{\theta'}(s_t,a_t)}{\eta}\Bigr)\,\pi_{\theta'}(a_t\mid s_t),
    \label{eq:pi_imp}
\end{equation*}
where \(\theta'\) denotes the parameters of a target network and \(\eta>0\) is a temperature parameter.

The target Q-distribution is computed via the distributional Bellman operator:
\begin{equation*}
\begin{split}
    \Gamma_{\theta'}(q\mid s_t,a_t) =\; &\mathbb{E}_{s_{t+1}}\Biggl[
    \mathbb{E}_{a'\sim\pi_\theta(\cdot\mid s_{t+1})}\Bigl[
    \mathbb{E}_{q'\sim p_\theta(\cdot\mid s_{t+1},a')}\, \\
    &\quad \mathbf{1}_{\left[q-\frac{\epsilon}{2},\,q+\frac{\epsilon}{2}\right]}\Bigl(r_t+\gamma\,q'\Bigr)
    \Bigr]\Biggr].
\end{split}
\label{eq:bellman_new}
\end{equation*}

The actor network maps the mnemonic percept \(H_t\) to an action distribution through a 4-layer feed-forward network:
\begin{align*}
    \mu_1 &= \text{ELU}(H' W_1 + b_1), \\
    \mu_2 &= \text{ELU}(\mu_1 W_2 + b_2), \\
    \mu_3 &= \text{ELU}(\mu_2 W_3 + b_3), \\
    \pi_\theta(a_t\mid H_t) &= \text{Softmax}\Bigl(\mu_3 W_{\text{out}} + b_{\text{out}}\Bigr).
\end{align*}

Training employs a KL-regularized objective that jointly updates the actor and the critic. The actor loss is defined as the KL divergence between the improved policy and the current policy:
\begin{equation*}
\mathcal{L}_{\text{actor}}(\theta) = D_{\text{KL}}\Bigl(\pi_{\text{imp}}(\cdot\mid s_t) \,\|\, \pi_\theta(\cdot\mid s_t)\Bigr),
\label{eq:actor_loss}
\end{equation*}
which, up to an additive constant, is equivalent to
\begin{equation*}
\mathcal{L}_{\text{actor}}(\theta) = -\mathbb{E}_{a\sim \pi_{\text{imp}}}\Bigl[\log \pi_\theta(a\mid s_t)\Bigr].
\label{eq:actor_loss_expanded}
\end{equation*}
Similarly, the critic loss is defined as the KL divergence between the target Q-distribution and the predicted Q-distribution:
\begin{equation*}
\mathcal{L}_{\text{critic}}(\theta) = \beta\,D_{\text{KL}}\Bigl(\Gamma_{\theta'}(q\mid s_t,a_t) \,\|\, p_\theta(q\mid s_t,a_t)\Bigr),
\label{eq:critic_loss}
\end{equation*}
where \(\beta>0\) is a balancing hyperparameter. The overall loss is given by
\begin{equation*}
\mathcal{L}(\theta) = \mathcal{L}_{\text{actor}}(\theta) + \mathcal{L}_{\text{critic}}(\theta).
\label{eq:total_loss}
\end{equation*}

In summary, our model learns to select actions that maximize long-term rewards by jointly training the actor and the distributional critic with a KL-regularized objective \cite{springenberg2024offline}. The network architecture—designed to process spatially structured memory representations—utilizes self-attention and feed-forward layers, with long equations split over multiple lines to ensure clarity in our two-column format.


\subsection{Task Difficulty}

To control task difficulty, Gabor stimuli were corrupted with rotational "noise". Defnining $\theta^*_i$ as the "true" Gabor orientation for $S_i$, the  orientation in the input image shown to the agent is:
\[
\theta_i = \theta^*_i + \delta_{it}
\]
where $\delta_{it} \sim \mathcal{N}(0,\sigma)$ is the rotational noise at time step $t$. If the stimulus is selected for change, then at $t=5$ and $t=6$:
\[
\theta_i = \theta^*_i + \Delta + \delta_{it}
\]
The orientation noise parameter $\sigma$ is set to 5. The orientation change parameter $\Delta$ is a random variable drawn at the beginning of a change trial, with $\Delta \sim \textrm{U}(-k, k)$, where $k$ is adjusted based on the agent's performance, starting at $k=65$ and decreasing as performance improves to increase task difficulty.



% \section{Discussion}

% In this study, we developed a neural network model of visual attention (Recurrent ViT) that embodies the functional components of attention as described by Knudsen \cite{knudsen2007fundamental}. The model also integrates both bottom-up and top-down processes as described within a biased competition framework \cite{desimone1995neural}. Our model leverages self-attention mechanisms inspired by Vision Transformers \cite{vaswani2017attention, dosovitskiy2020image} and incorporates an LSTM-like working memory module \cite{hochreiter1997long, beck2024xlstm} that provides feedback and top-down biases.

% By training the model using reinforcement learning with reward feedback \cite{sutton2018reinforcement}, we emulated how attentional biases can be shaped by reward contingencies, mirroring learning processes in the brain. The Recurrent ViT exhibits key behavioral signatures of visual attention observed in experimental studies, such as improved performance and faster reaction times for cued stimuli compared to uncued ones \cite{carrasco2011visual, clark2015visual, hoffman2016visual, bhatnagar2022meta, rust2022priority}. Furthermore, we found that the self-attention map demonstrated priority map-like configuration from which we could easily measure and manipulate the bias towards some representations over others.

% Artificially manipulating the bias within the attention module affected the model's performance in ways analogous to microstimulation experiments in the frontal eye fields (FEF) and superior colliculus (SC) of primates \cite{moore2003selective, cavanaugh2004subcortical, cavanaugh2006enhanced}. These manipulations could increase or decrease response rates and reaction times, depending on the location of an orientation change and the internal representations being biased.

% % Furthermore, manipulating the bias in our model also affected criterion and sensitivity measurements. While the exact modulations are likely task dependent, we found that our model accomplished these modulation through multiple interacting pathways. Assining a high bias on the cue location at the time a high validity cue was shown resulted in a criterion increase and sensitivity decrease at non-cued locations. However, the sensitivity decrease was caused by a high bias being assigned to non-cued locations during the time of change, an effect from cue information being stored in working memory. We also demonstrated that manipulating the bias at the time of change significantly affects sensitivity, and that this is a result of either amplifying or suppressing the relevant change detection signals that the reinforcement learning module requires to make optimal decisions.

% % Notably, inducing bias in favor of certain spatially oriented ANRs affected the model's value estimates and temporal difference (TD) errors, depending on which representations were biased. This finding aligns with the idea that attention can influence reinforcement learning signals \cite{maunsell2004neuronal, lim2011decision, leong2017dynamic}, suggesting that biased competition mechanisms may interact with value-based learning in the brain. Our model may offer predictions about microstimulation in a biased competition framework, where artificially biasing neural activity could affect dopamine-related TD error signals, with the specific impact depending on the neural representations being biased.

% We found that the model's incorporation of spatial context and recurrent feedback mirrors aspects of biological attention mechanisms. However, our model has limitations that warrant further investigation. The simplicity of the task environment most certainly do not capture the full complexity of natural visual scenes. Additionally, while our model shows NHP behavior signatures, it does not account for the intricacies of neural processing in the brain. Future work could focus on extending the model to more complex and naturalistic tasks, incorporating more detailed neural architectures, and exploring the role of attention in multisensory integration.

% The implications of our findings extend to both artificial intelligence and cognitive neuroscience. For AI, incorporating biologically plausible attention mechanisms, such as top-down biases to the self-attention mechanism, could enhance the performance of transformer neural networks in reinforcement learning tasks involving partial observability. In neuroscience, our results demonstrate that there may be promise in exploring recurrent self-attention mechanisms as a potential model for visual attention, potentially informing experimental designs and the interpretation of neurophysiological data. Future work will be devoted to exploring the model's behavior and performance applied to a variety of both, neuro and AI, inspired tasks. 
% \newpage

\bibliographystyle{unsrt}
\bibliography{ref2}  % This assumes your .bib file is named "bib.tex"

\end{document}