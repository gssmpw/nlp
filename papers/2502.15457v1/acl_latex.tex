% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage[ruled,norelsize]{algorithm2e}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{fdsymbol}
\usepackage{makecell}

\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot}
\makeatother

\DeclareMathOperator{\LLM}{LLM}

% Helper macro: toggles math mode & spacing
\newcommand{\mycmdhelper}[3]{%
  \ifx*#1%
    \mathchoice{#3}{#3}{#3}{#3} % Math mode only
  \else
    \ensuremath{#3}\xspace % Ensures correct mode
  \fi
}

% Define commands where \xspace is optional
\newcommand{\epmem}[1][]{\mycmdhelper{#1}{}{\mathcal{M}}}
\newcommand{\longmem}[1][]{\mycmdhelper{#1}{}{\mathcal{M}^{l}}}
\newcommand{\shortmem}[1][]{\mycmdhelper{#1}{}{\mathcal{M}^{s}}}
\newcommand{\gtshortmem}[1][]{\mycmdhelper{#1}{}{\mathcal{M}^{s}_{gt}}}
\newcommand{\genshortmem}[1][]{\mycmdhelper{#1}{}{\mathcal{M}^{s}_{gen}}}

\newcommand{\event}[3][]{%
  \if\relax\detokenize{#2}\relax
    \mycmdhelper{#1}{}{e_{#3}} % No superscript
  \else
    \mycmdhelper{#1}{}{e^{#2}_{\!#3}} % Superscript present
  \fi
}
\newcommand{\narration}[3][]{%
  \if\relax\detokenize{#2}\relax
    \mycmdhelper{#1}{}{t_{#3}}
  \else
    \mycmdhelper{#1}{}{t^{#2}_{\!#3}}
  \fi
}
\newcommand{\gennarration}[3][]{%
  \if\relax\detokenize{#2}\relax
    \mycmdhelper{#1}{}{\hat t_{#3}}
  \else
    \mycmdhelper{#1}{}{\hat t^{#2}_{\!#3}}
  \fi
}
\newcommand{\video}[1][]{\mycmdhelper{#1}{}{\mathcal{E}}}
\newcommand{\data}[1][]{\mycmdhelper{#1}{}{\mathcal{D}}}

\newcommand{\opt}{OPT-2.7B\xspace}
\newcommand{\vicuna}{Vicuna-7B\xspace}

\newcommand{\cameo}{CAMEO\xspace}

\makeatother

% \newcommand{\opt}{OPT-2.7B\xspace}
% \newcommand{\vicuna}{Vicuna-7B\xspace}



% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Memory Helps, but Confabulation Misleads:\\ Understanding Streaming Events in Videos with MLLMs}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

\author{
 \textbf{Gengyuan Zhang\textsuperscript{$\spadesuit$,$\vardiamondsuit$}},
 \textbf{Mingcong Ding\textsuperscript{$\spadesuit$}},
 \textbf{Tong Liu\textsuperscript{$\spadesuit$,$\vardiamondsuit$}},
 \textbf{Yao Zhang\textsuperscript{$\spadesuit$,$\vardiamondsuit$}},
 \textbf{Volker Tresp\textsuperscript{$\spadesuit$,$\vardiamondsuit$}}
% \\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
% \\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
% \\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
% \\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
\\
 \textsuperscript{$\spadesuit$}Ludwig-Maximilians-Universität München, Germany \\
 \textsuperscript{$\vardiamondsuit$}Munich Center for Machine Learning, Germany
 % \textsuperscript{3}Affiliation 3,
 % \textsuperscript{4}Affiliation 4,
 % \textsuperscript{5}Affiliation 5
 % \small{
 %   \textbf{Correspondence:} \href{mailto:zhang@dbs.ifi.lmu.de}{zhang@dbs.ifi.lmu.de}
 % }
 \\
 \href{mailto:zhang@dbs.ifi.lmu.de}{{\tt zhang@dbs.ifi.lmu.de}}
}

\begin{document}
\maketitle
\begin{abstract}
Multimodal large language models (MLLMs) have demonstrated strong performance in understanding videos holistically, 
yet their ability to process streaming videos—videos are treated as a sequence of visual events—remains underexplored.
Intuitively, leveraging past events as memory can enrich contextual and temporal understanding of the current event.
In this paper, 
we show that leveraging memories as contexts helps MLLMs better understand video events.
However, because such memories rely on predictions of preceding events, they may contain misinformation, leading to confabulation and degraded performance.
To address this, we propose a confabulation-aware memory modification method that mitigates confabulated memory for memory-enhanced event understanding.
\end{abstract}


\section{Introduction}
% using LLMs to understand videos
Leveraging Multimodal Large Language Models (MLLMs) to understand events in videos \citep{yu2024eliciting,zhang2023video,maaz-etal-2024-video} has shown their effectiveness in a wide range of tasks due to the reasoning capabilities of LLMs.

% overlooking in a streaming setting, event understanding is conditioned on previous perception and prediction
Much of the existing research focuses on understanding holistic videos~\citep{zhang2023video,li2024llava}, where videos are treated as a single entity and MLLMs process them in one go. 
In real-world scenarios, however, videos are often streaming and multi-event, and events unfold sequentially.
Under such conditions, video events have significant temporal and contextual dependencies due to their inherent semantic relevance.
Akin to human cognition, knowledge of past memories can help in understanding the present effectively.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{pic/stream-teaser.pdf}
    \caption{Knowing the memory of past events can help understand the current event. However, for streaming events in videos, we cannot access ground-truth narrations for previous events and this leads to confabulation.}
    \label{fig:teaser}
\end{figure}

% We show that using memory as context can help reason
We thus begin by exploring how incorporating memory mechanisms in LLMs can help enhance event understanding in videos.
A common practice for forming memory in LLMs is to prepend relevant contexts to the inputs of LLMs/MLLMs~\citep{behrouz2024titans,zhang2024survey,wang2024augmenting} where memories and knowledge are retained and recalled in tokens.  
Specifically, we specify the memory in the streaming video setting as event-triggered memory~\citep{fountas2024human,hatalis2023memory} since they are contextualized and dependent on the current scenes in videos, other than external event-agnostic knowledge.
The \textbf{memory} consists of \textit{long-term} memory~\citep{hatalis2023memory}, recollected from other episodes or videos with relevant events, and \textit{short-term} memory of events that precede the current one.

As shown in Fig.~\ref{fig:teaser} and later in Tab.\ref{tab:perf}, integrating memory as contexts, which consists of events and their narrations (detailed in Sec.~\ref{sec:method}), significantly outperforms the original 0-shot model, highlighting that knowing the past notably helps MLLMs. 



% However, the real problem is the confabulation can interfere the performance 
Nevertheless, this performance represents a gold-standard baseline—feasible only in ideal scenarios rather than in a practical streaming setting.
In practice, MLLMs don't access the ground truth memory, rendering the leveraged short-term memory prone to mispredictions in the previous rounds due to hallucinations or incomplete observations. 
We reevaluate the model under streaming conditions and observe a substantial performance drop. We attribute this performance gap primarily to misinformation generated in the memory, a phenomenon referred to as \textbf{confabulation} \citep{sui-etal-2024-confabulation}.


% we propose an uncertainty-aware surgical method at inference time
To address the inevitable confabulated memory in the streaming setting, 
we introduce CAMEO, a confabulation-aware memory modification approach to mitigate the confabulation problem. 
% Specifically, we introduce using semantic uncertainty as a proxy for the factualness of generated memory that suffers from confabulation.
% We then probe the potential confabulation-prone attention heads in LLMs and reweight the attention attended to confabulated memories in the contexts. This approach demonstrates its effectiveness in improving model performance compared to completely relying on confabulated memories.



% summarizaton
Our paper can be summarized as follows:
\begin{enumerate}
    \setlength{\itemsep}{0pt}
    \setlength{\parskip}{0pt}
    \item We demonstrate that leveraging memory as context in MLLMs improves understanding of video events by introducing contextual and temporal knowledge.
    \item Yet ground-truth memory is not accessible in the real streaming setting since the memory suffers from misinformation in MLLMs' generation, namely confabulation;
    \item We propose CAMEO, a memory modification approach, to mitigate confabulated memory in MLLMs.
\end{enumerate}



\section{Leverage Memory as Context in Event Understanding}\label{sec:method}
\subsection{Preliminary}
Given a video episode \video consisting of a sequence of $N$ events $\{\event[*]{}{1},\dots,\event[*]{}{N}\}$ with an arbitrary event \event{}{i} in the episode as a short video clip consisting of multiple frames. 
LLMs need to predict the narration \gennarration{}{n} of the current query event \event{}{n} with only access to the preceding events \event{}{1:n-1}.

\paragraph{Memory} Memory~\citep{fountas2024human, das2024larimar} in LLMs, akin to human brains, refers to retrieving relevant events from past experiences. 
We denote the memory activated by the current event \event{}{n} as  \epmem. We adopt two types of memory: long-term memory \longmem from the set of other episodes as persistent memory \data; and short-term memory \shortmem, which respectively means the memories that occurred earlier and memories in the current episode. Memory as contexts \epmem is defined as a set of events recollected:
\vspace{-0.1cm}
\begin{equation}
\begin{aligned}
    \epmem &= \longmem \cup \shortmem,\\
    \longmem &= \{ (\event{l}{1}, \narration{l}{1}), \dots, (\event{l}{N_l}, \narration{l}{N_l}) \},\\
    \shortmem &= \{ (\event{s}{1}, \narration{s}{1}), \dots, (\event{s}{N_s}, \narration{s}{N_s}) \}.
\end{aligned}
\end{equation}
\vspace{-0.2cm}

To collect events \event[]{l}{j} in \longmem, we use a similarity-based retrieval method \citep{}. In contrast, for events \event[]{s}{k} in \shortmem, we rely on recency, selecting the most recent events relative to the current event (detailed in Sec.~\ref{sec:more-on-model}).


\paragraph{Memory as Context} A prominent trend in memory-enhanced LLMs is to use memory as contexts in LLMs’ inputs~\citep{fountas2024human, behrouz2024titans}, thereby leveraging the In-Context Learning (ICL) capability of LLMs~\citep{brown2020language,gao2023retrieval}. 
We formulate long-term and short-term memories as prepended contexts to the inputs.

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.92\linewidth]{pic/stream_model.pdf}
    \caption{Model Pipeline for memory as contexts for streaming events reasoning. We interleave the events and narrations from long-term and short-term memory as contextual inputs.}
    \label{fig:enter-label}
\end{figure*}

\vspace{-0.2cm}
\begin{algorithm}
\caption{Streaming Evaluation}
\label{alg:training}
\KwIn{Current query event \event{}{n}, \\Current episode up to current event $\video_{:n-1}$, \\ Persistent memory \data}
\KwOut{Prediction of narration \narration{}{n}}
Initialize $\shortmem \gets \emptyset$\;
\For{$n = 1$ to $N$}{
Collect long-term memory from \data \\
\For{$k = 1$ to $N_l$}{
    $\longmem \gets (\event{l}{k}, \narration{}{k}) \in \data$\;
}
$\gennarration{}{n} \gets \LLM(\epmem, \event{}{n})$ \;
Update short-term memory with new prediction \\
$\shortmem \gets (\event{}{n}, \gennarration{}{n})$ \;
}
\Return \gennarration{}{n}\;
\end{algorithm}
\vspace{-0.6cm}


\subsection{Gain \& Loss: Memory in Event Understanding}
\paragraph{An Upper Bound: Ground-truth Memory helps}
We begin by introducing a golden baseline: we use \epmem with ground-truth narrations in \shortmem, denoted by \gtshortmem. 
In this setup, all memories are factual and plausible, therefore providing an upper bound on the performance of memory-enhanced LLMs.

\paragraph{Confabulated Memory Misleads Streaming Event Understanding}
We next evaluate LLMs’ event understanding capabilities in a streaming scenario, as illustrated in Alg.~\ref{alg:training}. 
This setting is more practical and challenging since the model cannot access ground-truth short-term memory \gtshortmem or rather the true narrations and can only utilize generated narrations up to the current event \genshortmem.

As shown in Fig.~\ref{fig:enter-label}, our streaming evaluation relies on short-term memory that incorporates earlier events and their predicted narrations.
Each new event’s prediction is subsequently consolidated into the short-term memory of the current episode.

\subsection{CAMEO: Mitigating Confabulation}
We propose a probing-and-surgical approach, \cameo, \textbf{C}onfabulation-\textbf{A}ware \textbf{ME}mory m\textbf{O}dification, to mitigate confabulation in our setting.
We treat confabulated memories as less factual and less credible contexts for LLMs.
Accordingly, we propose to quantize the credibility of short-term memory and reduce the contribution of heavily confabulated memories.

\paragraph{Uncertainty Estimation}
We adopt the semantic entropy proposed by \citet{kuhn2023semantic,farquhar2024detecting} as a proxy for assessing the credibility of generated narrations. Specifically, we sample $S$ generations of target narrations $\{\gennarration{(1)}{n},\dots,\gennarration{(S)}{n}\}$ from the output distribution of the LLMs conditioned on the memory \epmem and query event \event{}{n} from the posterior distribution $p(\gennarration{}{n} | \epmem[*], \event[*]{}{n}))$. 
\textit{Next}, we cluster the sampled narrations by their semantic equivalence, as described by \citet{kuhn2023semantic}, to identify the semantic clusters $C$ of generated narrations. The resulting semantic entropy is calculated as in Eq.\ref{eq:se}
\vspace{-0.2cm}
\begin{equation}
    se(\gennarration{}{n}) = -\sum_{c} p(c|\epmem, \event{}{n}) \log p(c | \epmem[*], \event[*]{}{n}))
\label{eq:se}
\end{equation}
\vspace{-0.6cm}


\paragraph{Probing Confabulation-Prone Heads} 
Attention heads have been shown to represent different subspaces of contextual information in LLMs \citep{deng2024cram, elhage2021mathematical, meng2022locating} and \citep{deng2024cram} demonstrates that modifying attention heads can mitigate misinformation in textual contexts.

Inspired by \citep{deng2024cram}, we aim to locate the confabulation-prone attention heads in the transformer architecture. 
To do so, we manipulate one ground-truth narration \(\narration{}{j}\) with a random, irrelevant narration \(\narration{-}{j}\) in \(\data\), ablate an arbitrary attention head h in the transformer, and then calculate the output logits to quantify the indirect effect (IE) \citep{meng2022locating, deng2024cram}, thereby identifying the influence of this attention head.

\paragraph{Confabulation-aware Memory Modification} 
Once we identify the confabulation-prone heads, we can reweight the attention in the most influential attention heads to the confabulated memory \gennarration{}{n} them according to their semantic entropy, as detailed in Appx.~\ref{sec:cameo-more}. 
We convert semantic entropy into weights as follows:
\vspace{-0.2cm}
\begin{equation}
    w(\gennarration{}{n}) = 1/exp(-\tau \times se(\gennarration{}{n}))
\vspace{-0.2cm}
\end{equation}
where $\tau$ is a temperature coefficient, with which we can tweak the modification scale.

\section{Experimental Study}
We elaborate the model design (Sec.~\ref{sec:model}), implementation details(Sec.~\ref{sec:imple}), and results (Sec.~\ref{sec:result}).

\subsection{Model Design}\label{sec:model}
\paragraph{Model Architecture}
Our models follow common MLLM paradigm \citep{liu2023llava,instructblip,liu2023improvedllava,li2023blip}: a visual encoder, a cross-modal bridge, and an LLM backbone.
Events, treated as short video clips, can vary in length, so we downsample them to a fixed length $l_e=8$. We then encode each frame individually with the CLIP~\citep{radford2021learning} visual encoder.
Subsequently, we use an event-aware bridge to encode the events and align them to the language space of LLMs, as shown in Fig.~\ref{fig:enter-label}. More details can be found in Appx.~\ref{sec:more-on-model}.

By encoding all the events in the memory \epmem, we interleave these events with their narrations and provide them as context to the model, alongside the new query event and the question.

\paragraph{Model Selection} We employ two widely used Large Language Models in our experiments: \opt \citep{zhang2022opt} and \vicuna \citep{zheng2023judging}.
We also use the Q-Former proposed by \citet{li2023blip} as an effective bridge to compress visual features from clips. 
Although MLP-based projection \citep{liu2023improvedllava, liu2023llava} is a popular method for adapting vision to the language space, it increases computational overhead when processing multiple video inputs, making compression essential.
We initialize the Q-Former for \opt from \citet{instructblip} and for \vicuna from \citet{zhuminigpt}, then unfreeze each with training data.


% \begin{table}
%     \scriptsize 
%     \centering
%     \begin{tabular}{cccc}
%     \toprule
%         Model &  STS &  Rouge-L & BLEU\\
%     \midrule
%         \rowcolor{gray!20} \opt \\
%         w/o memory & 0.4953 & 0.5197 & 17.9401 \\
%          & & & \\
%         w/ ground-truth memory (16 shots) & 0.6548 & 0.6227 & 34.6116 \\
%         & & & \\
%         w/ confabulated memory (16 shots)  & 0.5026 & 0.5412 & 18.2918 \\
%     \midrule
%         \rowcolor{gray!20} \vicuna \\
%         w/o memory & 0.6146 & 0.5881 & 24.0131 \\
%         & & & \\
%         w/ ground-truth memory (16 shots) & 0.7926 & 0.7332 & 53.4041 \\
%         & & & \\
%         w/ confabulated memory (16 shots) & 0.6555 & 0.5908 & 31.4534 \\
%     \bottomrule
%     \end{tabular}
%     \caption{Model Performance. We notice that memory can help with understanding events since it brings contextual knowledge of the current event. However, confabulated memory from generated narrations in the streaming setting can mislead MLLMs.}
%     \label{tab:perf}
% \end{table}

\begin{table}
    \scriptsize 
    \centering
    \begin{tabular}{lccc}
    \toprule
        Model &  STS &  Rouge-L & BLEU\\
    \midrule
        \rowcolor{gray!20} \opt \\
        w/o memory & 0.495 & 0.520 & 0.179 \\
        $\Delta$    & \textcolor{green}{+0.160} & \textcolor{green}{+0.103} & \textcolor{green}{+0.167} \\
        w/ ground-truth memory (16 shots) & 0.655 & 0.623 & 0.346 \\
        $\Delta$   & \textcolor{red}{-0.152} & \textcolor{red}{-0.082} & \textcolor{red}{-0.163} \\
        w/ confabulated memory (16 shots)  & 0.503 & 0.541 & 0.183 \\
    \midrule
        \rowcolor{gray!20} \vicuna \\
        w/o memory & 0.615 & 0.588 & 0.240 \\
        $\Delta$   & \textcolor{green}{+0.178} & \textcolor{green}{+0.145} & \textcolor{green}{+0.293} \\
        w/ ground-truth memory (16 shots) & 0.793 & 0.733 & 0.534 \\
        $\Delta$   & \textcolor{red}{-0.137} & \textcolor{red}{-0.142} & \textcolor{red}{-0.220} \\
        w/ confabulated memory (16 shots) & 0.656 & 0.591 & 0.315 \\
    \bottomrule
    \end{tabular}
    \caption{Model Performance. Memory enhances event understanding by providing contextual knowledge of the current event, while confabulated memory in the streaming setting can mislead MLLMs.}
    \label{tab:perf}
\vspace{-0.4cm}
\end{table}

\subsection{Implementation}\label{sec:imple}
\paragraph{Training Details}
To elicit the multimodal in-context learning capability of MLLMs, we retrain them with memory \epmem as interleaved video-text data \citep{yu2024eliciting, alayrac2022flamingo, li2024textbind, wang2024cosmo}, following Alg.\ref{alg:training} in Appx.~\ref{sec:training}.
We pad 8 events sampled from \(\data\) and 8 most recent events in the current episode \video and adopt two new learnable tokens to distinguish different memories. The total shot number is 16.
We unfreeze the Q-Former to adapt to visual events.
% We leverage the ground-truth narrations to adapt the LLM for (1) gain in-context learning ability with interleaved inputs with events (visual) and narrations (textual) \citep{} (2) train the Q-Former to adapt to visual events in the memory in the context.


\paragraph{Dataset}
We use the Ego4D dataset \citep{grauman2022ego4d} for our task. To construct \data from past episodes and for training, we utilize the training split. To probe the confabulation-prone heads, we rely on the validation set. Finally, for all evaluations, we employ the test set.

\paragraph{Evaluation}
We evaluate the narrations with Semantic Textual Similarity (STS) by Sentence-BERT~\citet{reimers-gurevych-2019-sentence}, ROUGE-L by \citet{lin-2004-rouge}, BLEU by \citet{papineni2002BLEU}.






\subsection{Results}\label{sec:result}

\paragraph{Memory helps, but confabulation misleads}
We first compare 3 baselines: MLLMs without memory, MLLMs with ground-truth memory (w/ ground-truth narrations), and MLLMs with confabulated memory (w/ generated narrations) in Tab.~\ref{tab:perf}.

First, we observe a significant performance gain when MLLMs leverage memory as contexts, yielding a large margin of improvement across all mentioned metrics for both models.

However, the performance drop in streaming evaluation is concerning. Its primary cause is that in the streaming scenario, we replace ground-truth narrations with previously generated narrations of recent events. This inevitably leads to a performance drop, stemming from the confabulation introduced by earlier predictions.


\paragraph{Mitigating Confabulation}
As shown in Fig.~\ref{fig:mod}, with CAMEO, the reasoning capabilities of MLLMs for streaming event understanding improve significantly. This result underscores the effectiveness of CAMEO in combating confabulated memory.

\begin{figure}[ht]
    \centering
    
    % First subfigure
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=\textwidth]{pic/modification/vicuna-7b.pdf} % Replace with your image file
        \caption{\vicuna}
        \label{fig:mod-vicuna}
    \end{subfigure}
    \hfill
    % Second subfigure
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=\textwidth]{pic/modification/opt-2.7b.pdf} % Replace with your image file
        \caption{\opt}
        \label{fig:mod-opt}
    \end{subfigure}

    \caption{Performance improvements with \cameo.}
    \label{fig:mod}
\end{figure}

\section{Conclusion}
In this work, we demonstrated how memory, leveraged as context, can inject temporal knowledge into MLLMs and enhance their understanding of visual events in streaming videos. However, in practical streaming scenarios, accessing ground-truth memory is often impossible, leading to confabulation when predictions depend on previously generated narrations. We addressed this issue by proposing a confabulation-aware attention modification mechanism, which uses the semantic uncertainty of predicted narrations as a proxy for the credibility of potentially confabulation-prone memory. We show that this approach effectively mitigates confabulated memory.

\paragraph{Limitations}
\begin{enumerate}
    \item In this work, we focus on forming memory as context for MLLMs. While this approach is training-free and allows us to exploit MLLMs’ capabilities fully, future efforts could investigate parameterized memory components, as discussed by \citet{behrouz2024titans}.
    \item Our current setting relies on a predefined semantic boundary between events in the original dataset to simplify the task. Thus, videos are streamed at the event level rather than the frame level. A more realistic streaming scenario would be more challenging because the semantic boundary is unknown.
    \item Due to computational constraints, our model selection is limited. Implementing interleaved video-text inputs requires a powerful architecture to compress visual features and reduce token length (\eg, Q-Former). This hinders our ability to explore other models that do not compress visual features, as their computational complexity grows quadratically with the amount of memory shots.
\end{enumerate}

\paragraph{Ethics Statement}
In this work, we investigate the cause and effect of MLLMs' confabulation in streaming event understanding for videos. Such confabulation is caused by MLLMs' mispredictions, may reflect intrinsic hallucination, and lead to misinformation, resulting in unsolicited or undesired outputs.



% \section*{Acknowledgments}


% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

\newpage
\appendix

\section{Notation table}
We summarize the notations used in the paper in Tab.~\ref{tab:notations}. 

\begin{table}[h!]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Notation} & \textbf{Description} \\
\midrule
$\mathcal{E}$ & A video episode \\
$\mathcal{D}$ & Persistent memory \\
$\epmem$ & Memory (overall) \\
$\longmem$ & Long-term memory \\
$\shortmem$ & Short-term memory \\
$e_i$ & $i$-th event \\
$e_n$ & the current query event \\
$\narration{}{}$ & Narration of an event \\
$\gennarration{}{}$ & Generated narrations of an event \\
$(e, t)$ &  A pair of event and narration\\
$N_l$ & The size of long-term memory \\
$N_s$ & The size of short-term memory \\
$c$ & Semantic cluster of sampled narrations \\
$\LLM(\cdot)$ & generation of Large Language Model \\
\bottomrule
\end{tabular}
\caption{Notations used in the paper.}
\label{tab:notations}
\end{table}


\section{Related Works}
\subsection{Temporal Understanding in Videos}
Understanding temporal ques in videos is challenging and MLLM-based methods draw great attention. A wide range of work~\citep{maaz-etal-2024-video, qian2025streaming,zhang2024mm,he2024ma} leverages the reasoning ability of MLLMs and presents great performance in understanding holistic videos.

\subsection{Memory as Contexts in LLMs} Implementing memories has been a big topic. Due to the emergent abilities of LLMs~\citep{Wei2022EmergentAO} to understand and leverage context, using contexts as memory has been one mainstream approach for NLP tasks\citep{fountas2024human} and also multimodal tasks\citep{fan2024videoagent,chen2024camml}.




\section{Extension of Experimental Study}
\subsection{Model Implementation}\label{sec:more-on-model}
\paragraph{Similarity-based Retrieval} We retrieve relevant events from \longmem based on their semantic similarity with the current query event. We use the features in the output layer of the CLIP visual encoder and calculate their cosine similarity with the query event.

\paragraph{Recency-based Retrieval} We collect the events from \shortmem based on recency, namely the most $N_s$ recent events will be sampled from the current episode \video from the current event \event{}{n}.

\subsection{Training}\label{sec:training}
We elaborate our training algorithm as in Alg.~\ref{alg:training}. For training, we used 16 shots of memory events: 8 from long-term memory and 8 from short-term memory in the same episode. We use the ground-truth annotations in the training to adapt the MLLMs to learn the temporal and contextual knowledge from the memory.

We adopt the next token prediction loss. For training, we utilize 4 Nvidia A100 for 75 hours on the full training set of Ego4D with 5 epochs. The learning rate is set to 1e-5 and the batch size to 8. For streaming evaluation, we adopt 1 Nvidia A40 for 4 hours.

\begin{algorithm}
\caption{Training}
\label{alg:training}
\KwIn{Current query event \event{}{n}, \\Current episode \video, \\ Persistent memory \data}
\KwOut{Prediction of narration \narration{}{n}}
% Collect $l_s$ events \shortmem \gets \event{}{n-1};
Collect short-term memory from the current episode \video \\
\For{$j = 1$ to $N_s$}{
    $\shortmem \gets (\event{}{n-j}, \narration{}{n-j}) \in \video$\;
}
Collect long-term memory from \data \\
\For{$k = 1$ to $N_l$}{
    $\longmem \gets (\event{l}{k}, \narration{}{k}) \in \data$\;
}
$\epmem \gets \longmem \cup \shortmem$ \;
$\narration{}{n} \gets \LLM(\epmem, \event{}{n})$ \;
\Return \narration{}{n}\;
\end{algorithm}





\subsection{\cameo Implementation}\label{sec:cameo-more}
\paragraph{Probing Confabulation-prone Attention Heads}
We selected 9 episodes consisting of 189 events from the validation split of Ego4D. For each event, we compose the memory set \epmem with 8 long-term memories and 8 ground-truth short-term memories and randomly replace one narration of a short-term event with a wrong narration. Then we ablate each attention head in each layer to calculate the influential effect. As shown in Fig.~\ref{fig:probing-comparison}, we locate the top-$k$ confabulation-prone attention heads with the largest IE.

In the end, we pick up top-96 confabulation-prone attention heads and use a temperature of 0.6 for \vicuna; top-32 confabulation-prone attention heads and use a temperature of 0.8 for \opt.


\paragraph{Attention Modification in CAMEO}
Considering the memory as contexts $\mathcal{C}=\{\event{l}{1}, \narration{l}{1}, \dots,\event{l}{1}, \narration{l}{N_l}, \event{s}{N_l}, \gennarration{s}{1}, \dots,\event{s}{N_s}, \gennarration{s}{N_s}\}$, each generated narration \gennarration{}{} has its semantic entropy score $se(\gennarration{s}{})$ and its modification weight $w(\gennarration{s}{})$. For other contextual inputs like $\event{}{}$ and $\narration{l}{}$, the modification weight is set to $1$. The resulting modification weight is $\mathbf{w}$ of which $k$-th token has the weight,
\begin{equation}
\mathbf{w}_k =
\begin{cases} 
w(\gennarration{s}{}) & \text{if } token_k \text{ belongs to } \gennarration{s}{}, \\
1 & \text{otherwise}
\end{cases}
\end{equation}


For an arbitrary attention head $h$ in transformers, we multiply the attention matrix $\mathbf{A}_h$ by the modification weight $\mathbf{w}$ elementwisely $\mathbf{A}_h \circ \mathbf{w}$.



\subsection{Extended Results}

\paragraph{Different ratio of memory}
We explore different ratios of short-term memory \shortmem and long-term memory \longmem. We defined the total length of \epmem as $16$ and tried different ratios of \shortmem over a total length of \epmem.
As shown in Fig.~\ref{fig:minigpt4_eval}-\ref{fig:opt_eval}, we experiment with \vicuna and \opt under various ratios of memory in the training and evaluation stages (using ground-truth memory) respectively. With the memoryless baseline (0-shot) highlighted in the figures, we observe that a blend of short-term and long-term memory strikes a strong balance in event understanding performance. We also find that this trade-off is somewhat impacted by the ratio applied in the training phase.
Furthermore, all memory-augmented settings consistently outperform the memoryless baseline, thus reinforcing our finding that memory indeed aids event understanding.


\paragraph{Different shots of memory}
We also study the performance impact of different shots. We use the same ratio $1/2$ where half of the memory is short-term memory as in Fig.~\ref{fig:mini-gt}-\ref{fig:opt-gt}.
We find that an increasing number of events in memory can increase the performance. This aligns with our emphasis over the impact of memory and the in-context learning ability in LLMs \citep{yu2024eliciting, brown2020language}.

% % test on pic of diff ratio
% \begin{figure}[h]
%     \centering
%     \begin{subfigure}{0.48\columnwidth}
%         \centering
%         \includegraphics[width=\textwidth]{pic/minigpt4/minigpt4-1_4-gt-fmeasure.pdf}
%         \caption{ROUGE-L(training with $25\%$ short-term memory)}
%         \label{fig:minigpt4_1_4_fmeasure}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{0.48\columnwidth}
%         \centering
%         \includegraphics[width=\textwidth]{pic/minigpt4/minigpt4-1_4-gt-sbs.pdf}
%         \caption{STS(training with $25\%$ short-term memory)}
%         \label{fig:minigpt4_1_4_sbs}
%     \end{subfigure}
    
%     \begin{subfigure}{0.48\columnwidth}
%         \centering
%         \includegraphics[width=\textwidth]{pic/minigpt4/minigpt4-1_2-gt-fmeasure.pdf}
%         \caption{ROUGE-L(training with $50\%$ short-term memory)}
%         \label{fig:minigpt4_1_2_fmeasure}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{0.48\columnwidth}
%         \centering
%         \includegraphics[width=\textwidth]{pic/minigpt4/minigpt4-1_2-gt-sbs.pdf}
%         \caption{STS(training with $50\%$ short-term memory)}
%         \label{fig:minigpt4_1_2_sbs}
%     \end{subfigure}
    
%     \begin{subfigure}{0.48\columnwidth}
%         \centering
%         \includegraphics[width=\textwidth]{pic/minigpt4/minigpt4-3_4-gt-fmeasure.pdf}
%         \caption{ROUGE-L(training with $75\%$ short-term memory)}
%         \label{fig:minigpt4_3_4_fmeasure}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{0.48\columnwidth}
%         \centering
%         \includegraphics[width=\textwidth]{pic/minigpt4/minigpt4-3_4-gt-sbs.pdf}
%         \caption{STS(training with $75\%$ short-term memory)}
%         \label{fig:minigpt4_3_4_sbs}
%     \end{subfigure}

%     \caption{Evaluation of Vicuna-7b with different training ratios and metrics.}
%     \label{fig:minigpt4_eval}
% \end{figure}

% % test on pic of diff ratio
% \begin{figure}[htbp]
%     \centering
%     \begin{subfigure}{0.48\columnwidth}
%         \centering
%         \includegraphics[width=\textwidth]{pic/opt/opt-1_4-gt-fmeasure.pdf}
%         \caption{ROUGE-L(training with $25\%$ short-term memory)}
%         \label{fig:minigpt4_1_4_fmeasure}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{0.48\columnwidth}
%         \centering
%         \includegraphics[width=\textwidth]{pic/opt/opt-1_4-gt-sbs.pdf}
%         \caption{STS(training with $25\%$ short-term memory)}
%         \label{fig:minigpt4_1_4_sbs}
%     \end{subfigure}
    
%     \begin{subfigure}{0.48\columnwidth}
%         \centering
%         \includegraphics[width=\textwidth]{pic/opt/opt-1_2-gt-fmeasure.pdf}
%         \caption{ROUGE-L(training with $50\%$ short-term memory)}
%         \label{fig:minigpt4_1_2_fmeasure}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{0.48\columnwidth}
%         \centering
%         \includegraphics[width=\textwidth]{pic/opt/opt-1_2-gt-sbs.pdf}
%         \caption{STS(training with $50\%$ short-term memory)}
%         \label{fig:minigpt4_1_2_sbs}
%     \end{subfigure}
    
%     \begin{subfigure}{0.48\columnwidth}
%         \centering
%         \includegraphics[width=\textwidth]{pic/opt/opt-3_4-gt-fmeasure.pdf}
%         \caption{ROUGE-L(training with $75\%$ short-term memory)}
%         \label{fig:minigpt4_3_4_fmeasure}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{0.48\columnwidth}
%         \centering
%         \includegraphics[width=\textwidth]{pic/opt/opt-3_4-gt-sbs.pdf}
%         \caption{STS(training with $75\%$ short-term memory)}
%         \label{fig:minigpt4_3_4_sbs}
%     \end{subfigure}
%     \caption{Evaluation of \opt with different ratios and metrics.}
%     \label{fig:opt_eval}
% \end{figure}


\begin{figure*}[htbp]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \begin{subfigure}{0.48\textwidth}
            \centering
            \includegraphics[width=\textwidth]{pic/minigpt4/minigpt4-1_4-gt-fmeasure.pdf}
            \caption{ROUGE-L (25\% short-term memory in training)}
            \label{fig:minigpt4_1_4_fmeasure}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.48\textwidth}
            \centering
            \includegraphics[width=\textwidth]{pic/minigpt4/minigpt4-1_4-gt-sbs.pdf}
            \caption{STS (25\% short-term memory in training)}
            \label{fig:minigpt4_1_4_sbs}
        \end{subfigure}
        
        \begin{subfigure}{0.48\textwidth}
            \centering
            \includegraphics[width=\textwidth]{pic/minigpt4/minigpt4-1_2-gt-fmeasure.pdf}
            \caption{ROUGE-L (50\% short-term memory in training)}
            \label{fig:minigpt4_1_2_fmeasure}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.48\textwidth}
            \centering
            \includegraphics[width=\textwidth]{pic/minigpt4/minigpt4-1_2-gt-sbs.pdf}
            \caption{STS (50\% short-term memory in training)}
            \label{fig:minigpt4_1_2_sbs}
        \end{subfigure}
        
        \begin{subfigure}{0.48\textwidth}
            \centering
            \includegraphics[width=\textwidth]{pic/minigpt4/minigpt4-3_4-gt-fmeasure.pdf}
            \caption{ROUGE-L (75\% short-term memory in training)}
            \label{fig:minigpt4_3_4_fmeasure}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.48\textwidth}
            \centering
            \includegraphics[width=\textwidth]{pic/minigpt4/minigpt4-3_4-gt-sbs.pdf}
            \caption{STS (75\% short-term memory in training)}
            \label{fig:minigpt4_3_4_sbs}
        \end{subfigure}
        \caption{Evaluation of Vicuna-7b with different training ratios.}
        \label{fig:minigpt4_eval}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering

        
        \begin{subfigure}[t]{0.48\textwidth}
            \centering
            
            \includegraphics[width=\textwidth]{pic/opt/opt-1_4-gt-fmeasure.pdf}
            \caption{ROUGE-L (25\% short-term memory in training)}
            \label{fig:opt_1_4_fmeasure}
        \end{subfigure}
        \hfill
        \begin{subfigure}[t]{0.48\textwidth}
            \centering
            \includegraphics[width=\textwidth]{pic/opt/opt-1_4-gt-sbs.pdf}
            \caption{STS (25\% short-term memory in training)}
            \label{fig:opt_1_4_sbs}
        \end{subfigure}
        
        \begin{subfigure}{0.48\textwidth}
            \centering
            \includegraphics[width=\textwidth]{pic/opt/opt-1_2-gt-fmeasure.pdf}
            \caption{ROUGE-L (50\% short-term memory in training)}
            \label{fig:opt_1_2_fmeasure}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.48\textwidth}
            \centering
            \includegraphics[width=\textwidth]{pic/opt/opt-1_2-gt-sbs.pdf}
            \caption{STS (50\% short-term memory in training)}
            \label{fig:opt_1_2_sbs}
        \end{subfigure}
        
        \begin{subfigure}{0.48\textwidth}
            \centering
            \includegraphics[width=\textwidth]{pic/opt/opt-3_4-gt-fmeasure.pdf}
            \caption{ROUGE-L (75\% short-term memory in training)}
            \label{fig:opt_3_4_fmeasure}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.48\textwidth}
            \centering
            \includegraphics[width=\textwidth]{pic/opt/opt-3_4-gt-sbs.pdf}
            \caption{STS (75\% short-term memory in training)}
            \label{fig:opt_3_4_sbs}
        \end{subfigure}
        \caption{Evaluation of \opt with different ratios.}
        \label{fig:opt_eval}
    \end{minipage}
\end{figure*}




\begin{figure*}[htbp]
    \centering
    % BLEU subfigure
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{pic/diff_shot/mini-1_2-tt-gt-bleu.pdf}
        \caption{BLEU}
        \label{fig:mini-gt-BLEU}
    \end{subfigure}
    \hfill
    % ROUGE-L subfigure
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{pic/diff_shot/mini-1_2-tt-gt-rougeL.pdf}
        \caption{ROUGE-L}
        \label{fig:mini-gt-rouge}
    \end{subfigure}
    \hfill
    % SBS subfigure
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{pic/diff_shot/mini-1_2-tt-gt-sbs.pdf}
        \caption{STS}
        \label{fig:mini-gt-sbs}
    \end{subfigure}
    \caption{Results of \vicuna with ground-truth (GT) memory of different shots. We employ $50\%$ short-term memory in all the experiments.}
    \label{fig:mini-gt}
\end{figure*}
\vspace{-0.2cm}

\begin{figure*}[t]
    \centering
    % BLEU subfigure
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{pic/diff_shot/opt-1_2-tt-gt-bleu.pdf}
        \caption{BLEU}
        \label{fig:mini-gt-BLEU}
    \end{subfigure}
    \hfill
    % ROUGE-L subfigure
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{pic/diff_shot/opt-1_2-tt-gt-rougeL.pdf}
        \caption{ROUGE-L}
        \label{fig:mini-gt-rouge}
    \end{subfigure}
    \hfill
    % SBS subfigure
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{pic/diff_shot/opt-1_2-tt-gt-sbs.pdf}
        \caption{STS}
        \label{fig:mini-gt-sbs}
    \end{subfigure}
    \caption{Results of \vicuna with ground-truth (GT) memory of different shots.}
    \label{fig:opt-gt}
\end{figure*}



\begin{figure*}
    \centering
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{pic/modification/vicuna-7b-probing.pdf}
        \caption{Probing result of \vicuna in different layers and heads}
        \label{fig:vicuna-prob}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{pic/modification/OPT-probing.pdf}
        \caption{Probing result of \opt in different layers and heads}
        \label{fig:opt-prob}
    \end{subfigure}
    \caption{Comparison of probing results for Vicuna-7b and OPT-2.7b across different layers and heads.}
    \label{fig:probing-comparison}
\end{figure*}

\subsection{Qualitative Studies}
We showcase three examples as in Fig.~\ref{fig:example}. They demonstrate that leveraging short-term memory as contexts can significantly help to understand the current event. 

\begin{figure*}
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{pic/qualitative/qualitative1.pdf}
        \caption{Example 1}
        \label{fig:ex1}
    \end{subfigure}

    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{pic/qualitative/qualitative2.pdf}
        \caption{Example 2}
        \label{fig:ex2}
    \end{subfigure}
    
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{pic/qualitative/qualitative3.pdf}
        \caption{Example 3}
        \label{fig:ex2}
    \end{subfigure}

    \caption{Evaluation of \opt with different ratios and metrics.}
    \label{fig:example}
\end{figure*}

\end{document}
