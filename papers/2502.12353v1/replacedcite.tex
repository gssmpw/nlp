\section{Related Work}
There is a long tradition of analysis of asymptotic properties of Bayesian algorithms. 
____ made an explicit connection between the Gibbs loss used in PAC-Bayes analysis and the objective of VI. This led to finite sample generalization bounds, i.e., bounds on the difference between training and true errors, that hold uniformly. In turn, algorithms that minimize the sum of training error plus generalization bound,
which have a form similar to VI with a regularization parameter,
are both well motivated and have strong theoretical guarantees. 
In followup work
____ have extended these results to richer classes, whereas ____ developed risk bounds, i.e., bounds that directly quantify the true error of VI. 
Other work suggested alternative optimization criteria diverging from VI by changing the loss or regularization components
\cite[e.g.,][]{black-box-alpha,knoblauch2019generalized,dlm-sgp} and generalization and risks bounds have been developed for some such algorithms 
____. 
%However, to our knowledge none of these results yield non-vacuous bounds for Bayesian neural networks used in practice. 
However, these have not been demonstrated in practice.
____ provided a non-vacuous bound for a binary classification task on MNIST. 
We evaluate these bounds and compare them to the stability bound
in our experiments in a multi-class classification task with large neural networks.
%However, our experiment shows that when applied in multi-class classification and large networks, this bound is still vacuous.

Another important line of work aims to analyze standard (non-Bayesian) algorithms, where capacity arguments can be used to yield generalization bounds for neural networks ____.
%, but these are also loose. 
Recent work has developed an alternative approach that provides tighter bounds which are data-dependent and algorithm-dependent. This includes work using stability 
____
and analysis that works through bounds on mutual information
____. 
This has been specifically developed for SGLD, and extensions to SGD ____ are possible only as an approximation of SGLD.
While the approaches differs in technical details, the outcome is similar in that a generalization bound is obtained which can be expressed as a sum over training steps, of some function of the gradients.
Specifically the bound of ____ includes a sum of gradient norms whereas the bound of ____ includes a sum of the norms of gradient differences, which was found to be tighter in practice. 
As mentioned above,  
SGLD learns the parameter $W$ of the model and adds some noise duirng the optimization, hence it produces a sample from some distribution over parameters. 
%This analysis is applicable to non-Bayesian algorithm but requires the injection of homoscedastic noise by SGLD that generates a distribution over potential parameters, where SGLD samples from this distribution during optimization.
This differs from Bayesian algorithms that explicitly generate distributions over parameters as their posteriors, and aggregate their predictions, and unfortunately the analysis does not carry through to this case. 

In contrast, we directly analyze iterative update Bayesian algorithms, for example, using SGD for variational inference (VI), without noise injection. The primary challenge is that the distribution of the parameters of VI is intractable, making it difficult to apply the chain rule of divergence (as in Lemma 10 of ____). 
We provide an alternative analysis that first externalizes all sources of randomness of the algorithm, and then uses convexity to derive the bounds. 
This 
allows us to bound the stability gap in terms of parameter differences.
%Instead, we employ the parameter differences to bound the Bayes stability for VI, and 
With this in place we can
follow the approach used to prove the stability of SGD ____ to bound parameter differences and obtain the desired result.  
Moreover, we extend the original Bayes stability argument, which previously applied only to bounded loss functions ____ or loss functions with bounded second moments ____. We generalize this framework to Lipschitz continuous loss functions, allowing us to bound the generalization error using Wasserstein distances, which can be bounded using parameter differences. This extension is inspired by ____, which employ Wasserstein distances in PAC-Bayes bounds.

Finally,
while the discussion in the paper emphasizes the analysis of VI, 
%Although our focus is on applications to VI, 
the analysis and bounds are applicable to any iterative update 
approximate Bayesian 
algorithm that updates parameters of the approximate posterior, where the sensitivity of parameter updates can be easily calculated.
Hence it captures more cases than prior work, as illustrated by the application to DLM.