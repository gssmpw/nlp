% !TEX root = main.tex
\newpage
\onecolumn

\section{An explanation why the Proof of \citet{Li} does not Apply to Variational Inference}
\label{sec:Li-proof-not-for-vi}

We begin by summarizing the approach taken by \citet{Li} to establish a stability-based generalization bound. Starting from a theorem similar to \cref{thm:errgen}, \citet{Li} bounds the generalization error using the term $2C\E_{S, \Bar{S}} [\tv(Q_S, Q_{\Bar{S}})]$, where $Q_S$ and $Q_{\Bar{S}}$ represent the output distributions of the algorithm $\mathcal{A}$ on datasets $S$ and $\Bar{S}$ after $T$ optimization steps.

Stochastic Gradient Langevin Dynamics (SGLD) updates the parameters by adding isotropic Gaussian noise at each step:
\begin{align*}
    W_t \leftarrow W_{t-1} - \gamma_t g_t(W_{t-1}, B_t) + \sigma_t \mathcal{N}(0, I_d),
\end{align*}
where $g_t(W_{t-1}, B_t)$ denotes the gradient computed on batch $B_t$ at time $t$.

At each step, the distribution of $W_t$ given $W_{t-1}$ is a mixture of Gaussians:
\begin{align*}
    \frac{1}{|\mathcal{B}|} \sum_{B_t \in \mathcal{B}} \mathcal{N}(W_{t-1} - \gamma_t g_t(W_{t-1}, B_T), \sigma_t^2 I).
\end{align*}
Similarly, the distribution of $\bar{W}_t$ on $\Bar{S}$ is
\begin{align*}
    \frac{1}{|\mathcal{B}|} \sum_{B_t \in \mathcal{B}} \mathcal{N}(\Bar{W}_{t-1} - \gamma_t g_t(W_{t-1}, B_t), \sigma_t^2 I).
\end{align*}
This leads to the bound:
\begin{align*}
    \E_{S, \bar{S}}[\tv(Q_S, Q_{\bar{S}})] = \E_{S, \bar{S}}[\tv(W_T, \bar{W}_T)] \leq \E_{S, \bar{S}}\left[\sqrt{\frac{1}{2} \kl(W_T, \bar{W}_T)} \right].
\end{align*}

Applying the chain rule for KL divergence,
\begin{align}
    \kl(W_T, \bar{W}_T) &\leq \kl(W_{1:T}, \bar{W}_{1:T}) \\
    &= \sum_{t=1}^T \E_{w_{1:t-1} \sim W_{1:{t-1}}} \left[\kl(W_t | W_{1:{t-1}} = w_{1:{t-1}}, \bar{W}_t | \bar{W}_{1:{t-1}} = w_{1:{t-1}}) \right]. \label{ineq:kl-chain}
\end{align}
\citet{Li} further bounds the sum of conditional KL divergences using a factor dependent on the difference in gradients evaluated on samples $z$ and $\bar{z}$. As discussed in the main paper, this analysis requires a fixed variance term and is non-obvious to generalize. 

More importantly, 
this strategy does not extend to Variational Inference (VI). 
Recall that for VI at step $t$, the distribution of $W_t$ given past history is parameterized by $\theta_t$.
Hence the evolution of the random variables is over $\theta$ variables and the distribution over $W$ is induced,
i.e., we have 
$(\theta_{t-1}\rightarrow \theta_t)$, $(\theta_{t}\rightarrow W_{t})$, and $(\theta_{t-1}\rightarrow W_{t-1})$.
Due to this structure, the equation which corresponds to 
\cref{ineq:kl-chain} fails when conditioning on $\theta_{1:{t-1}}$ instead of $w_{1:t-1}$.

A counterexample illustrates this failure: let $\theta_1 = 0.4$ and $\bar{\theta}_1 = 0.6$, with $W_1 \sim \text{Bern}(\theta_1)$ and $\bar{W}_1 \sim \text{Bern}(\bar{\theta}_1)$. The parameter updates follow:
\begin{align}
    \theta_2 &= \theta_1 - 0.2, \quad W_2 | \theta_1 \sim \text{Bern}(\theta_1 - 0.2), \label{eq:update1} \\
    \bar{\theta}_2 &= \bar{\theta}_1 - 0.2, \quad \bar{W}_2 | \bar{\theta}_1 \sim \text{Bern}(\bar{\theta}_1 - 0.2). \label{eq:update2}
\end{align}

Then,
\begin{align}
    \kl((W_1, W_2), (\bar{W}_1, \bar{W}_2)) &\approx 0.173, \\
    \kl(W_1, \bar{W}_1) + \E_{\rho \sim \theta_1}[\kl(W_2 | \theta_1 = \rho, \bar{W}_2 | \bar{\theta}_1 = \rho)] &= \kl(W_1, \bar{W}_1) \approx 0.081.
\end{align}
The last equality holds because
$\kl(W_2 | \theta_1 = \rho, \bar{W}_2 | \bar{\theta}_1 = \rho) = 0$ due to the update rules in \cref{eq:update1} and \cref{eq:update2}.
We therefore see that the left-hand side of \cref{ineq:kl-chain} exceeds the right-hand side when conditioning on $\theta_{1:{t-1}}$. This shows that the method of \citet{Li}, using \cref{ineq:kl-chain}, cannot be used for VI.

\section{Omitted Proofs}


%\subsection{Proofs in \cref{sec:stability}}
%\label{sec:proof-stability}

%\begin{proof}[Proof of \cref{thm:errgen}]
%    It is clear that
%     \begin{align}
%         \E_{S} \E_{w \sim Q_S} [\loss(w, \mathcal{D})] = \E_{z\sim \mathcal{D}} \E_{w \sim Q_z} \E_{\Bar{z}\sim \mathcal{D}}[\loss(w, \Bar{z})] = \E_{\Bar{z}} \E_{Q_{\Bar{z}}} \E_{z} [\loss(w, z)],
%     \end{align}
%     and
%     \begin{align}
%         \E_S \E_{w\sim Q_S} \left[\frac{1}{n} \sum_{i=1}^n \loss(w, z_i) \right] &= \E_{z} \E_{Q_z} [\loss(w, z)].
%     \end{align}
%     Then the generalization error is 
%     \begin{align}
%         \errgen(\alg) &= \E_z \E_{\Bar{z}} \left[ \E_{w\sim Q_{\Bar{z}}}[\loss(w, z)] - \E_{w\sim Q_z}[\loss(w, z)]\right] \\
%         &\leq \E_{z, \Bar{z}} \int |\loss(w, z)| \lvert Q_{\Bar{z}}(w) - Q_z(w)\rvert dw \\
%         &\leq 2C \E_{z, \Bar{z}} [\tv(Q_z, Q_{\Bar{z}})] \\
%         &= 2C \E_{z, \Bar{z}}[\tv(\E_{S_{n-1}, \epsilon}[Q_{S_{n-1} \cup \{z\}, \epsilon}], \E_{S_{n-1}, \epsilon}[Q_{S_{n-1} \cup \{\Bar{z}\}, \epsilon }])] \\
%         &\leq 2C \E_{z, \Bar{z}, S_{n-1}, \epsilon}[\tv(Q_{S, \epsilon}, Q_{\Bar{S}, \epsilon})].
%     \end{align}
%     The last inequality is because of the convexity of total variation distance (see \cref{lemma:tv-convexity}).
%\end{proof}


%\begin{proof}[Proof of \cref{cor:kl}]
%    According to Pinsker's inequality, the total variation distance can be bounded by the KL divergence of the distributions. We thus first bound the KL divergence. 
%     \begin{align}
%         \kl(Q_{S, \epsilon}, Q_{\Bar{S}, \epsilon}) &= 1^\top (\log \sigma - \log \Bar{\sigma}) + \frac{1}{2} \left(1^\top \frac{\Bar{\sigma}^2}{\sigma^2} - d + 1^\top \frac{(\Bar{m} - m)^2}{\sigma^2} \right) \\
%        &\leq \frac{2\lVert \sigma - \Bar{\sigma} \rVert_1}{\sigma_0} + \frac{\lVert \sigma - \Bar{\sigma} \rVert_2^2}{2\sigma_0^2} + \frac{\lVert \Bar{m} - m \rVert_2^2}{2\sigma_0^2},
%        \label{eq:kl-form}
%    \end{align}
%    where $\sigma_0$ is a preset minimum value for the standard deviation, i.e., $\forall k, \sigma_k \geq \sigma_0$ and $\bar{\sigma}_k \geq \sigma_0$.
%    To derive \cref{eq:kl-form},
%    let $ \beta_i = |\sigma_i - \Bar{\sigma}_i |$. Consider $1^\top (\log \sigma - \log \Bar{\sigma}) = \sum_i \log \frac{\sigma_i}{\Bar{\sigma}_i}$. For each entry $i$, if $\sigma_i - \beta_i \leq \sigma_0$, then $\log \frac{\sigma_i}{\Bar{\sigma}_i} \leq \log \frac{\beta_i + \sigma_0}{\sigma_0} = \log (1 + \frac{\beta_i}{\sigma_0}) \leq \frac{\beta_i}{\sigma_0}$; if $\sigma_i - \beta_i > \sigma_0$, then $\log \frac{\sigma_i}{\Bar{\sigma}_i} \leq \log \frac{\sigma_i}{\sigma_i - \beta_i} = \log (1 + \frac{\beta_i}{\sigma_i - \beta_i}) \leq \log (1 + \frac{\beta_i}{\sigma_0}) \leq \frac{\beta_i}{\sigma_0}$. Overall, $1^\top (\log \sigma - \log \Bar{\sigma}) \leq \sum_i \frac{\beta_i}{\sigma_0} = \frac{\lVert \sigma - \Bar{\sigma}_0 \rVert_1}{\sigma_0}$. For $1^\top \frac{\Bar{\sigma}^2}{\sigma^2} = \sum_i \frac{\Bar{\sigma}_i^2}{\sigma_i^2} \leq \sum_i \frac{(\sigma_i + \beta_i)^2}{\sigma_i^2} \leq \sum_i (1 + 2\frac{\beta_i}{\sigma_i} + \frac{\beta_i^2}{\sigma_i^2}) \leq \sum_i (1 + \frac{2\beta_i}{\sigma_0} + \frac{\beta_i^2}{\sigma_i}) = d + \frac{2\lVert \sigma - \Bar{\sigma} \rVert_1}{\sigma_0} + \frac{\lVert \sigma - \Bar{\sigma} \rVert_2^2}{\sigma_0^2}$. Thus,
%    \begin{align}
%        \errgen(\alg) &\leq 2C \E_{S, \Bar{S}, \epsilon} [\tv(Q_{S, \epsilon}, Q_{\Bar{S}, \epsilon})] \\
%        &\leq C \E_{S, \Bar{S}, \epsilon} \sqrt{2 \kl (Q_{S, \epsilon}, Q_{\Bar{S}, \epsilon})} \\
%        &\leq \frac{2C}{\sqrt{\sigma_0}} \sqrt{\E [\lVert \Bar{\sigma} - \sigma \rVert_1]} + \frac{C}{\sigma_0} \E \left[\lVert \Bar{\sigma} - \sigma \rVert_2 \right] + \frac{C}{\sigma_0} \E \left[\lVert \Bar{m} - m \rVert_2 \right].
%    \end{align}
%\end{proof}

%\subsection{Proofs in \cref{sec:bounds}}
\label{sec:proof-bound}

%\begin{proof}[Proof of \cref{thm:diff}]
%    Let $S_t$ and $\Bar{S}_t$ denote the subset at step $t$. With respect to the same $\epsilon_t$ (including the same batch sequence), $S_t$ and $\bar{S}_t$ have at most one element different.
%    We have two cases:
%    \begin{itemize}
%    \item Case 1: the different element is not selected, hence $S_t = \Bar{S}_t$, and since $G$ is $\eta_t$ expansive:
%        \begin{align*}
%            \lVert \theta_t - \Bar{\theta}_t \rVert \leq \eta_t \lVert \theta_{t-1} - \Bar{\theta}_{t-1} \rVert.
%        \end{align*}
%        \item Case 2: the different element is selected.
%        \begin{align*}
%            \lVert \theta_t - \Bar{\theta}_t \rVert &= \lVert (\theta_{t-1} - \alpha_t \nabla F(\theta_{t-1}, S_t, \epsilon_{t})) \\
%            & \quad - (\Bar{\theta}_{t-1} - \alpha_t \nabla F(\Bar{\theta}_{t-1}, \Bar{S}_t, \epsilon_t)) \rVert \\
%            &= \lVert (\theta_{t-1} - \alpha_t \nabla F(\theta_{t-1}, \Bar{S}_t, \epsilon_t)) \\
%            & \quad - (\Bar{\theta}_{t-1} - \alpha_t \nabla F(\Bar{\theta}_{t-1}, \Bar{S}_t, \epsilon_t)) \\
%            & \quad + \alpha_t (\nabla F(\theta_{t-1}, \Bar{S}_t, \epsilon_t) - \nabla F(\theta_{t-1}, S_t, \epsilon_t)) \rVert \\
%            &\leq \eta_t \lVert \theta_{t-1} - \Bar{\theta}_{t-1} \rVert \\
%            &\quad + \alpha_t \lVert \nabla F(\theta_{t-1}, \Bar{S}_t, \epsilon_t) - \nabla F(\theta_{t-1}, S_t, \epsilon_t) \rVert.
%        \end{align*}
%        Since $\Bar{S}_t$ and $S_t$ only differs at one element, $\lVert \nabla F(\theta_{t-1}, \Bar{S}_t, \epsilon_t) - \nabla F(\theta_{t-1}, S_t, \epsilon_t) \rVert = \frac{1}{b} \lVert \nabla F(\theta_{t-1}, \Bar{z}, \epsilon_t) - \nabla F(\theta_{t-1}, z, \epsilon_t) \rVert = \frac{1}{b} \Delta_t$, where $b$ is the batch size.
%    \end{itemize}
%    Thus, 
%    \begin{align}
%        \lVert \theta_T - \Bar{\theta}_T \rVert &\leq \eta_T \lVert \theta_{T-1} - \Bar{\theta}_{T-1} \rVert + \mathbbm{1}_{z \in S_T} \frac{\alpha_T}{b} \Delta_T \\
%        &\leq \frac{1}{b} \sum_{t=1}^T \left(\prod_{i=t+1}^T \eta_i \right) \mathbbm{1}_{z\in S_t} \alpha_t \Delta_t,
%    \end{align}
%    where the base case is $\theta_0 = \bar{\theta}_0$.
%    Since the probability that $z \in S_t$ is $\frac{b}{n}$, then the expected difference is 
%    \begin{align}
%        \E \lVert \theta_T - \Bar{\theta}_T \rVert \leq \frac{1}{n} \sum_{t=1}^T \left(\prod_{i=t+1}^T \eta_i \right) \alpha_t \E_{S, \epsilon, \bar{z}} \Delta_t.
%    \end{align}
%\end{proof}

\begin{proof}[Proof of \cref{cor:logT}]
    If $\nabla F(\theta, S, \epsilon)$ is $L$-Lipschitz, then the update rule $G(\theta, S, \epsilon)$ with learning rate $\alpha$ is $(1 + \alpha L)$-expansive:
    \begin{align}
        \lVert G(\theta, S, \epsilon) - G(\theta', S, \epsilon)  \rVert &= \lVert (\theta - \alpha \nabla_\theta F(\theta, S, \epsilon)) - (\theta' - \alpha \nabla_\theta F(\theta', S, \epsilon))\rVert \\
        &\leq \lVert \theta - \theta' \rVert + \alpha \lVert \nabla_\theta F(\theta, S, \epsilon) - \nabla_\theta F(\theta', S, \epsilon) \rVert \\
        &\leq (1 + \alpha L) \lVert \theta - \theta' \rVert.
    \end{align}
    Thus, $\eta_t = 1 + \alpha_t L$. Then
    \begin{align}
        \prod_{i=t+1}^T (1 + \alpha_i L) &\leq \prod_{i=t+1}^T \exp(\alpha_i L) \\
        &= \exp\left(cL\sum_{i=t+1}^T \frac{1}{(i+2) \log (i+2)} \right) \\
        \label{eq:log-int}
        &\leq \exp \left( cL \int_{t+2}^{T+1} \frac{1}{x\log x} dx \right) \\
        &\leq \exp \left(cL (\log \log (T+1) - \log \log (t+2))  \right) \\
        &\leq \frac{\log (T+1)}{\log (t+2)}. \label{eq:logT1}
    \end{align}
    \cref{eq:log-int} is because of the monotonicity of the function $f(x)=\frac{1}{x \log x}$, $\int_{t}^{t+1} \frac{1}{x \log x} dx \geq \frac{1}{(t+1) \log (t+1)}$. 
    In \eqref{eq:intStep} below, we apply the same observation to the function $g(x) = \frac{1}{x \log^2 x}$.
    Using \eqref{eq:logT1} in the bound of \cref{eq:diff} we obtain:
    \begin{align}
        \E\lVert \theta_{T} - \Bar{\theta}_T \rVert &\leq \frac{2\beta \log (T+1)}{n} \sum_{t=1}^T \frac{c}{(t+2) \log^2 (t+2)} \\
        &\leq \frac{2c\beta \log (T+1)}{n} \int_{t=2}^{T+1} \frac{1}{t\log^2 t} dt \label{eq:intStep}\\
        &= \frac{2c\beta \log (T+1)}{n} \left( -\frac{1}{\log t} \Big\vert_{t=2}^T \right) \\
        &\leq \frac{2c\beta \log (T+1)}{n \log 2} = O \left(\frac{\log T}{n} \right).
    \end{align}
\end{proof}

%\subsection{Additional Proofs}
%\begin{lemma}
%\label{lemma:tv-convexity}
%    $\tv\left(\E_{P(X)}[P(Y|X)], \E_{P(X)}[Q(Y|X)] \right) \leq \E_{P(X)}[\tv(P(Y|X), Q(Y|X))]$.
%\end{lemma}
%\begin{proof}
%    \begin{align*}
%        & \tv(\E_{P(X)}P(Y|X), \E_{P(X)}Q(Y|X)) 
%        \\ &= \frac{1}{2} \int \left|\int P(x) P(y|x)dx - \int P(x) Q(y|x) dx\right| dy
%        \\ &= \frac{1}{2} \int \left|\int P(x) (P(y|x)- Q(y|x)) dx\right| dy 
%        \\ &\leq \frac{1}{2} \int P(x) \int \left|P(y|x)- Q(y|x)\right| dy dx 
%        \\ &= \E_{P(X)}[\tv(P(Y|X), Q(Y|X))].
%    \end{align*}
%\end{proof}

\section{An Extreme Scenario for PAC-Bayes Bounds}
\label{app:pacbayes}
%\paragraph{Example of a PAC-Bayes Bound Being Worse Than a Stability Bound}

The following example illustrates a situation where the PAC-Bayes bound deteriorates due to its inclusion of the KL term,
and where the stability bound is tight. This complements the qualitative comparison of the bounds in the main paper to illlustrate their strengths and weaknesses. 

Consider a simple logistic regression scenario where the data takes on two possible values, \( x \in \{-1, 1\} \), and the corresponding labels are \( y \in \{0, 1\} \), i.e., there are only two possible examples $(x=-1, y=0)$ and $(x=1, y=1)$. 
The dataset can contain duplicate elements. The log-likelihood in this case is given by:
\begin{align}
    \log p(y \mid w, x) &= -y \log (1 + \exp{(-wx)}) \\
    & \quad - (1-y) \log (1 + \exp(wx)).
\end{align}

Assume we use a Bayesian approach to learn this model, with \( q(w) = \mathcal{N}(m, \sigma^2) \). For simplicity, we assume \(\sigma^2\) is fixed. Recall that for any objective function, we can always evaluate PAC-Bayes bounds.
% It is important to note that for the PAC-Bayes bound, the specific objective used during training does not impact the bound.

Suppose our objective is \( F(m, (x, y)) = \mathbb{E}_{q(w)}[-\log p(y \mid w, x)] \). Considering the gradient with respect to \(m\), we have the following identity 
%\citep{rezende2014stochastic, Sheth2015, opper2008variational}:
\citep{rezende2014stochastic, opper2008variational}:
\begin{align}
    \nabla_m F(m, (x, y)) &= \mathbb{E}_{q(w)} [\nabla_w \log p(y \mid w, x)].
\end{align}

Observe that:
\begin{align*}
    \nabla_w -\log p(y=1 \mid w, x=1)
    =& \nabla_w \log (1+\exp(-w)) \\
    =& -\frac{\exp{(-w)}}{1+\exp(-w)}, \\
    \nabla_w -\log p(y=0 \mid w, x=-1) 
    =& \nabla_w \log (1+\exp(-w)) \\
    =& -\frac{\exp(-w)}{1+\exp(-w)}, 
\end{align*}
we can see that
\begin{align}
    &\nabla_w -\log p(y=1 \mid w, x=1) \nonumber \\
    = &\nabla_w -\log p(y=0 \mid w, x=-1) < 0. 
    \label{eq:same}
\end{align}
Therefore,
if we run stochastic gradient descent with a constant learning rate for sufficiently many steps, we reach a solution where \(m \rightarrow +\infty\).

Now, suppose the initial prior is \( P_0 = \mathcal{N}(0, \sigma^2) \). The KL divergence will eventually become:
\begin{align}
    \kl(\mathcal{N}(m, \sigma^2) \parallel \mathcal{N}(0, \sigma^2)) &= \frac{m^2}{2\sigma^2} \rightarrow +\infty.
\end{align}

However, if we consider the stability bound, which is based on the gradient difference, the situation changes. Itâ€™s clear that if \( z = \bar{z} \) (whether \( x = \bar{x} = 1, y = \bar{y} = 1 \) or \( x = \bar{x} = -1, y = \bar{y} = 0 \)), the gradient difference will be zero. Thus, we only need to consider the case where \( z = (1, 1) \) and \( \bar{z} = (-1, 0) \). As shown in \cref{eq:same}, the gradients are the same in this scenario as well.

Therefore, using the stability bound, the generalization error will be zero. In contrast, the PAC-Bayes bound gives a value of \( \infty \).
%, making the stability bound significantly more effective.


%\section{Additional Plots}
%\begin{figure}[!htb]
%    \centering
%    \begin{subfigure}[b]{0.45\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figs/CIFAR10-train_acc.png}
%         \caption{Train error.}
%    \end{subfigure}
%    \begin{subfigure}[b]{0.45\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figs/CIFAR10-test_acc.png}
%         \caption{Test error.}
%    \end{subfigure}
%    \caption{Train error and test error.}
%    \label{fig:err}
%\end{figure}

%\begin{figure}[!htb]
%    \centering
%    \begin{subfigure}[b]{0.45\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figs/CIFAR10-DLM-train_loss.png}
%         \caption{Train loss.}
%    \end{subfigure}
%    \begin{subfigure}[b]{0.45\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figs/CIFAR10-DLM-test_loss.png}
%         \caption{Test loss.}
%    \end{subfigure}
%    \caption{Train loss and test loss for ELBO and DLM.}
%    \label{fig:loss}
%\end{figure}

