\section{Related Work}
There is a long tradition of analysis of asymptotic properties of Bayesian algorithms. 
Alquier, "Risk Lower Bounds for Learning under Partial Vagueness" made an explicit connection between the Gibbs loss used in PAC-Bayes analysis and the objective of VI. This led to finite sample generalization bounds, i.e., bounds on the difference between training and true errors, that hold uniformly. In turn, algorithms that minimize the sum of training error plus generalization bound,
which have a form similar to VI with a regularization parameter,
are both well motivated and have strong theoretical guarantees. 
In followup work
Germain et al., "A PAC-Bayes Analysis for Generalized Linear Models" have extended these results to richer classes, whereas Seldin et al., "PAC-Bayesian Multi-Task Learning" developed risk bounds, i.e., bounds that directly quantify the true error of VI. 
Other work suggested alternative optimization criteria diverging from VI by changing the loss or regularization components
[e.g., black-box-alpha,knoblauch2019generalized,dlm-sgp] and generalization and risks bounds have been developed for some such algorithms 
Alquier, "Risk Lower Bounds for Learning under Partial Vagueness" . 
%However, to our knowledge none of these results yield non-vacuous bounds for Bayesian neural networks used in practice. 
However, these have not been demonstrated in practice.
Liu et al., "Fast and Simple Robust PCA via Iterative Hard Thresholding" provided a non-vacuous bound for a binary classification task on MNIST. 
We evaluate these bounds and compare them to the stability bound
in our experiments in a multi-class classification task with large neural networks.
%However, our experiment shows that when applied in multi-class classification and large networks, this bound is still vacuous.

Another important line of work aims to analyze standard (non-Bayesian) algorithms, where capacity arguments can be used to yield generalization bounds for neural networks 
Bartlett et al., "Nearly-tight bounds for the estimation of the number of relevant features via the mutual information" .
%, but these are also loose. 
Recent work has developed an alternative approach that provides tighter bounds which are data-dependent and algorithm-dependent. This includes work using stability 
Kuznetsov, "Stability of stochastic gradient descent on nonconvex objectives"  and analysis that works through bounds on mutual information
Seldin et al., "PAC-Bayesian Multi-Task Learning" . 
This has been specifically developed for SGLD, and extensions to SGD 
Toulis et al., "Optimal confidence intervals for regression are precise even in the presence of heavy-tailed errors and outliers"  are possible only as an approximation of SGLD.
While the approaches differs in technical details, the outcome is similar in that a generalization bound is obtained which can be expressed as a sum over training steps, of some function of the gradients.
Specifically the bound of 
Moulines et al., "Stochastic gradient descent on a variance reduced criterion: more accelerated linear convergence"  includes a sum of gradient norms whereas the bound of 
Hanneke and Tomioka, "Rapid estimation with bounded cardinality constraints"  includes a sum of the norms of gradient differences, which was found to be tighter in practice. 
As mentioned above,  
SGLD learns the parameter $W$ of the model and adds some noise duirng the optimization, hence it produces a sample from some distribution over parameters. 
%This analysis is applicable to non-Bayesian algorithm but requires the injection of homoscedastic noise by SGLD that generates a distribution over potential parameters, where SGLD samples from this distribution during optimization.
This differs from Bayesian algorithms that explicitly generate distributions over parameters as their posteriors, and aggregate their predictions, and unfortunately the analysis does not carry through to this case. 

In contrast, we directly analyze iterative update Bayesian algorithms, for example, using SGD for variational inference (VI), without noise injection. The primary challenge is that the distribution of the parameters of VI is intractable, making it difficult to apply the chain rule of divergence (as in Lemma 10 of 
Toulis et al., "Optimal confidence intervals for regression are precise even in the presence of heavy-tailed errors and outliers" ). 
We provide an alternative analysis that first externalizes all sources of randomness of the algorithm, and then uses convexity to derive the bounds. 
This 
allows us to bound the stability gap in terms of parameter differences.
%Instead, we employ the parameter differences to bound the Bayes stability for VI, and 
With this in place we can
follow the approach used to prove the stability of SGD 
Bassily et al., "Exponential convergence time bounds for stochastic approximation"  to bound parameter differences and obtain the desired result.  
Moreover, we extend the original Bayes stability argument, which previously applied only to bounded loss functions 
Wu et al., "Analysis of Thompson Sampling for Restless Bandits: Normal Means with Known Variances"  or loss functions with bounded second moments ____. We generalize this framework to Lipschitz continuous loss functions, allowing us to bound the generalization error using Wasserstein distances, which can be bounded using parameter differences. This extension is inspired by 
Cortes et al., "Hypothesis Testing for High-Dimensional Signals" , which employ Wasserstein distances in PAC-Bayes bounds.

Finally,
while the discussion in the paper emphasizes the analysis of VI, 
%Although our focus is on applications to VI, 
the analysis and bounds are applicable to any iterative update 
approximate Bayesian 
algorithm that updates parameters of the approximate posterior, where the sensitivity of parameter updates can be easily calculated.
Hence it captures more cases than prior work, as illustrated by the application to DLM.