\documentclass{article}
\usepackage[hybrid]{markdown}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amsfonts, amssymb}
\usepackage{graphicx}

\begin{document}


\begin{markdown}
# Rebuttal to Reviewer U5gK

### 1. Wasserstein Distance in Lemma 6
$\sigma^2$ and $\Bar{\sigma}^2$ are indeed vectors representing the diagonal elements of covariance matrices. Since these are not full matrices, there is no need to multiply by $d$ in this context. We apologize for any confusion and appreciate your attention to detail.

### Generalization bounds should decay to zero.
The bounds do decay to zero because expected parameter differences decay to zero. Although this is not explicit in Lemma 3, Lemma 6 it can be seen in the application, e.g., Theorem 8.


### 2. Insufficient Explanations
- **No Random Labels**:  
  In the absence of random labels, PAC-Bayes bounds are higher for models trained on augmented data (AD), while the generalization error is lower. This discrepancy sufficiently demonstrates that PAC-Bayes bounds fail to capture the benefits of data augmentation.  
- **With Random Labels**:  
  When random labels are present, PAC-Bayes bounds with AD are less than **twice** those without AD, yet the generalization error with AD exceeds **seven times** that without AD. Even if the relative relationships are consistent, this significant mismatch highlights the failure of PAC-Bayes bounds in accounting for the advantages of data augmentation.
  
Combining both observations, we believe it is sufficient to say ``PAC-Bayes bounds fail to account for the benefits of data augmentation''.

### 3. Figure 1
Corollary 9 establishes that the parameter difference grows slowly when the learning rate decays as $O(1/(t\log t))$. While this learning rate is theoretically sound, it is impractically slow. Figure 1 demonstrates that even with a faster decay rate, such as $O(0.9^{t/5})$, the parameter difference still grows slowly, affirming our claim. Moreover, the observed expansion rate is far smaller than the theoretical upper bound of $\log T \approx 9.2$ predicted in Corollary 9.

### 4. Novelty
- **Applicability of Related Works**:  
  The approaches of Li et al. (2020) and Banerjee et al. (2022) cannot be directly applied to variational inference (VI). In SGLD, the distribution of $W_t$ is fully dependent on $W_{t-1}$, enabling the use of the chain rule of KL divergence. However, in VI, the distribution of $W_t$ is parameterized by $\theta_t = (m_t, \sigma_t^2)$, and it depends on the previous distribution $\mathcal{N}(m_{t-1}, \sigma_{t-1}^2)$, not on a single sample of $W_{t-1}$. This distinction necessitated our incorporation of the expansion rate and the adaptation of stability ideas from SGD.  
- **KR-Duality**:  
  Our application of KR-duality is similar to Theorem 4.12 of Hellstr√∂m et al. (2024). 
  % {\bf RK: But we do not cite them? we cite a different source so rephrase "by ... which is similar to ..."?}
  However, their bounds measure the distance between the posterior and the prior, inheriting the limitations of PAC-Bayes bounds. Consequently, their approach is prone to failure in examples like the one in Section 5.1.  
- **Parameter Stability**:  
  Zhou et al. (2020) focus on parameter stability concerning the mean squared generalization error or generalization error for Lipschitz losses on $\theta = (m, \sigma^2)$, not directly on $W$. Demonstrating that the generalization error of VI is Lipschitz in $m$ and $\sigma^2$ is non-trivial, making their results incomparable with PAC-Bayes bounds. Our work bridges this gap by addressing generalization error in the context of VI.
\end{markdown}

\begin{markdown}
# Rebuttal to Reviewer Bw6L

### 1. Valid Bound
We argue that the generalization error, defined as the difference between the training error and the test error, reflects whether the model is overconfident or not. A larger gap indicates worse uncertainty quantification. As shown in Figures 2(a) and 3(a), the generalization error serves as the ground truth for evaluating uncertainty quantification.

Our approach leverages stability-based bounds to upper bound the generalization error. Since our bounds are tighter than PAC-Bayes bounds, this demonstrates that our methods provide better uncertainty quantification.

### 2. Performance
In Theorem 8, we prove that the parameter difference decreases as the sample size increases. Consequently, our bound becomes even tighter, leading to greater accuracy in quantifying uncertainty.

\end{markdown}

\begin{markdown}
# Rebuttal to Reviewer fcRK

### 1. Expansion in Empirical section

% {\bf RK: I think they did not understand that expansion rate in experiments is the product of $eta_i$ from the theory part; you might want to add that fact}
The expansion rate in Definition 7 quantifies how much the difference between two sets of parameters grows after one gradient-based update, given the same batch of data. When two sets of parameters (starting from the same initial values) are optimized on two datasets that differ by only one element, the updates will differ due to the presence of the distinct element. Over time, this difference expands and accumulates throughout the optimization process.

Our goal is to measure the parameter difference at the end of the optimization trajectories and use Lemma 3 or Lemma 5 to bound the generalization error. Figure 1 provides an empirical evaluation of the product of the expansion rate from the beginning to the end, which shows the upper bound of how many times the difference of the gradients expands. As the optimization goes, the expansion rate becomes flat. This empirically guarantees our bound will not explode. We use this value to evaluate our bound as well.


### 2. Novelty
Li et al. (2020) propose stability bounds for the SGLD algorithm, not for variational inference. In SGLD, the optimization trajectory is $W_0 \rightarrow W_1 \rightarrow \dots \rightarrow W_T$, where $W_t \sim \mathcal{N}(G(W_{t-1}), \sigma^2 I)$ and $G(W_{t-1})$ is the gradient-based update rule. They are able to bound:  
% {\bf RK: Eq was and is commented out, sentence not finished}
% $KL(W_T, W'_T) \leq \sum_{t=1}^T \mathbb{E}_{W_{0:t-1}} [KL(W_t, W'_t)]$. 

However, in variational inference, the optimization trajectory is $(m_0, \sigma_0^2) \rightarrow (m_1, \sigma_1^2) \rightarrow \dots \rightarrow (m_T, \sigma_T^2)$. It is challenging to determine the distribution of $m$ and $\sigma^2$ and apply the same formulation as in Li et al. (2020). This is the primary reason we introduce the expansion rate and bound the generalization error in terms of the parameter difference. Our focus is not just unbounded loss but generalization of VI.
% {\bf RK: add that our focus is not unbounded loss but this point of generalization to VI?}

Figure 3 illustrates the case for unbounded losses, and we will emphasize this point in the revision of our paper. For unbounded losses, the bound cannot be vacuous. To demonstrate that our bound is non-vacuous, we include the case for bounded losses in Figure 2.

### 3. Clarity
We will improve the clarity of our explanations in future revisions.

This paper primarily focuses on applying SGD to mean-field variational inference (VI) on deep neural networks, which is why we assume $Q$ is Gaussian. The results in Section 4 are general and do not depend on $Q$ being Gaussian. However, Section 5 specifically requires the distribution to be Gaussian, as this allows us to define the mean and (co)variance. By bounding the differences between means and (co)variances, we can derive bounds on the generalization error.

We adopt Assumption 1 from Li et al. (2020), which is an assumption on the algorithm not about the data.
% {\bf RK: add Note that this is an assumption on the algorithm, not about the data.} 
This assumption is reasonable because when using SGD, the data is randomly permuted, and mini-batches are selected during optimization.

Unbounded losses play a significant role in VI since we often rely on the bounded log loss to evaluate performance. For instance, consider two algorithms that produce the same prediction: one assigns a higher probability, and the other assigns a lower probability. While their 0-1 losses are identical, one might be overconfident, and the other underconfident. Bounding the generalization error in such cases helps us better understand the uncertainty quantification capabilities of different algorithms.

The data augmentation techniques used in this study include random cropping and horizontal flipping.

### 4. Intuitive Explanation of Why PAC-Bayes Bounds Are Vacuous  

In Section 5.1, we provide an example to illustrate why PAC-Bayes bounds can become vacuous. The primary reason is that as the number of parameters increases, the KL divergence term in the PAC-Bayes bounds begins to dominate, making the bounds less meaningful.  

It is important to clarify that the vacuousness of PAC-Bayes bounds is not due to whether the bounds consider the entire dataset or individual data points. When PAC-Bayes bounds evaluate the entire dataset, the differences are generally more stable than when focusing on a single data point. Thus, this aspect does not contribute to the vacuousness of the bounds.
\end{markdown}

\begin{markdown}
# Rebuttal to Reviewer cZB6

### 1. Justification of the Example

PAC-Bayes bounds, such as Equations (11), (12), and (13), hold for all possible distributions $Q_S$, regardless of how $Q_S$ is obtained. While one can derive algorithms by minimizing the bound to get the corresponding $Q_S$, the bounds remain valid for any $Q_S$, not just those obtained through bound optimization. Hence, our main point in the comparison is not the algorithms that are derived by PAC-Bayes bounds, but the bound itself as it applies to the variational inference algorithm.
{\bf RK: add Hence, our main point in the comparison is not the algorithms that are derived by PAC-Bayes bounds, but the bound itself as it applies to the variational inference algorithm.}

In our example, $Q_S$ is derived by minimizing the training loss without a regularizer. Since it is a valid distribution, it satisfies the PAC-Bayes bounds.

### 2. Significance

Our training objectives are the ELBO and DLM objectives. However, when we evaluate the PAC-Bayes bounds from [2], we optimize the parameter $\lambda$ in Equation (14) to achieve the tightest bound.
In contrast, optimizing the objective in [2] minimizes the sum of the training error and the bound. This approach does not necessarily guarantee the tightest bound. Therefore, we believe our evaluation approach is fair.

The learning rate in Corollary 9 is chosen to upper bound the expansion rate and achieve the asymptotic upper bound. However, our experiments show that even with an exponentially decaying learning rate, the expansion remains quite slow (see Figure 1). Furthermore, Figures 4 and 5 in the appendix demonstrate that such an exponentially decaying learning rate is sufficient to fit the data effectively.

### 3. Last Paragraph of Section 1

As noted by Wei and Khardon (2022), DLM has a higher generalization error than ELBO. However, the PAC-Bayes bounds for both ELBO and DLM are the same, meaning one cannot distinguish their generalization errors using these bounds. 

In contrast, our bounds differentiate between the two methods. The fact that our bound is higher for DLM reflects the larger generalization error of DLM. This is not because our bound is looser but because it accurately captures the true generalization error.

### new point
{\bf RK: add reference to slow learning rate which is not used in the experiments and the application of Thm8 and not 9 in the experiments?}

### 4. Notations in Section 4

Informally, $Q_{S, \epsilon} = \mathcal{N}(w \mid m_T, \sigma_T^2)$ represents the final variational distribution over the parameter space, which is a density. Moreover, $Q_S$ is the average of all such densities $Q_{S, \epsilon}$ over all possible $\epsilon$. This results in $Q_S$ being a density as well, akin to a mixture of Gaussians with infinitely many components, as there are infinitely many possible $\epsilon$ values.

Our objective is defined in terms of $m$ and $\sigma^2$, not $w$. Hence, $m_T$ and $\sigma_T^2$ are determined by  
$(m_T, \sigma_T^2) = \arg\min_{m, \sigma^2} F(m, \sigma^2; S, \epsilon).$

\end{markdown}


\end{document} 