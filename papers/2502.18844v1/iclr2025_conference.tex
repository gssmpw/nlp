
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{tabularray}

\newcommand{\hhl}[1]{\textcolor{blue}{#1}}

\usepackage{graphicx} % Required for inserting images

\title{BarkXAI: A Lightweight Post-Hoc Explainable Method for Tree Species Classification with Quantifiable Concepts}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Yunmei Huang\thanks{These authors contributed equally to this work.} \\
Department of Forestry and Natural Resources\\
Purdue University\\
West Lafayette, IN, USA \\
\texttt{huan1643@purdue.edu} \\
\And
Songlin Hou$^*$ \\
Department of Computer Science\\
Worcester Polytechnic Institute\\
Worcester, MA, USA\\
\texttt{shou@wpi.edu} \\
 \AND
Zachary Nelson Horve \\
Department of Forestry and Natural Resources \\
Purdue University \\
West Lafayette, IN, USA \\
\texttt{zhorve@purdue.edu} \\
\And
Songlin Fei \\
Department of Forestry and Natural Resources \\
Purdue University \\
West Lafayette, IN, USA \\
\texttt{sfei@purdue.edu} 
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
The precise identification of tree species is fundamental to forestry, conservation, and environmental monitoring. Though many studies have demonstrated that high accuracy can be achieved using bark-based species classification, these models often function as "black boxes", limiting interpretability, trust, and adoption in critical forestry applications. Attribution-based Explainable AI (XAI) methods have been used to address this issue in related works. However, XAI applications are often dependent on local features (such as a head shape or paw in animal applications) and cannot describe global visual features (such as ruggedness or smoothness) that are present in texture-dominant images such as tree bark. Concept-based XAI methods, on the other hand, offer explanations based on global visual features with concepts, but they tend to require large overhead in building external concept image datasets and the concepts can be vague and subjective without good means of precise quantification. To address these challenges, we propose a lightweight post-hoc method to interpret visual models for tree species classification using operators and quantifiable concepts. Our approach eliminates computational overhead, enables the quantification of complex concepts, and evaluates both concept importance and the model’s reasoning process. To the best of our knowledge, our work is the first study to explain bark vision models in terms of global visual features with concepts. Using a human-annotated dataset as ground truth, our experiments demonstrate that our method significantly outperforms TCAV and Llama3.2 in concept importance ranking based on Kendall’s Tau, highlighting its superior alignment with human perceptions.
%Precise identification of tree species is fundamental to forestry, conservation efforts, and environmental monitoring, serving as an essential means in forest management. Though many studies have demonstrated high accuracy can be achieved in bark-based identification in species classification, these models often function as "black boxes", limiting interpretability, trust, and adoption in critical forestry applications. 
% To mitigate this issue, Explainable AI (XAI) has been introduced to enhance model transparency including both attributions-based method and concept-based XAI.
%Many of related works use attribution-based Explainable AI (XAI) methods to address this issue. However, these methods work best with images with local features such as animals (head, paws, tails, etc) but cannot describe global visual features (rugged, smooth, etc) largely present in texture-dominant images such as tree barks. Concept-based XAI methods, on the other hand, offer explanations based on global visual features with concepts, but they tend to require large overhead in building external concept image datasets and the concepts can be vague and subjective without good means of precise quantification.
%To tackle these challenges, we propose a lightweight post-hoc method to interpret visual models for tree species classification using operators and quantifiable concepts. Our approach eliminates computational overhead, enables the quantification of complex concepts, and evaluates both concept importance and the model’s reasoning process. To the best of our knowledge, our work is the first study to explain bark vision models in terms of global visual features with concepts. Using a human-annotated dataset as ground truth, our experiments demonstrate that our method significantly outperforms TCAV and Llama3.2 in concept importance ranking based on Kendall’s Tau, highlighting its superior alignment with human perceptions. 
\textit{Source code will be released upon acceptance.}

% apply concept-based XAI to tree species classification. Our approach eliminates computational overhead, enables the quantification of complex concepts, and evaluates both concept importance and the model’s reasoning process. By enhancing interpretability and trust, this work promotes the adoption of AI-driven solutions in forestry and environmental monitoring.
  

% Due to the characteristics of bark images- featuring repetitive and unique textures without dominant local patterns for identification, the attributions-based method struggles to provide high-level explanations that align with human reasoning. Concept-based XAI provides a more intuitive interpretation by leveraging high-level, human-understandable concepts to explain model decisions. We propose a computational-efficient approach to interpret visual models for tree species classification. This is the first study to apply concept-based XAI to tree species classification. Our approach eliminates computational overhead, enables the quantification of complex concepts, and evaluates both concept importance and the model’s reasoning process. By enhancing interpretability and trust, this work promotes the adoption of AI-driven solutions in forestry and environmental monitoring.
 
\end{abstract}


\section{ Introduction }
%
% Forests play a crucial role in climate solutions by sequestering carbon, regulating ecosystems, and supporting biodiversity. Accurate tree species identification is fundamental to forestry, biodiversity conservation, and environmental monitoring. Traditional methods rely on expert visual inspection of morphological features such as bark, leaves, flowers, and fruits. However, these approaches are time-consuming, subjective, and dependent on observer expertise, seasonal variations, and species similarity, making large-scale identification challenging. The advancement of deep learning has revolutionized image recognition and classification tasks, offering powerful tools for automated tree species identification. \\
% Various studies  \citep{carpentier2018tree_cnn_bark_id,robert2020tree_deepBarkdataset,Wu2021Bark_wufanyou,bertrand2018bark_leaf_fusion,yamabe2022vision_barkid} have focused on using deep neural networks and achieved high accuracy in three species identification with bark images.


Forests play a crucial role in climate solutions by sequestering carbon, regulating ecosystems, and supporting biodiversity. Tree species identification, as an essential means of forest management, helps understanding forest composition, and ecosystems. With the rapid advancement in AI, tree species identification automation with deep neural networks is increasingly gaining attention. Many studies \citep{carpentier2018tree_cnn_bark_id,bertrand2018bark_leaf_fusion,robert2020tree_deepBarkdataset,Wu2021Bark_wufanyou,yamabe2022vision_barkid} have demonstrated that high accuracy in tree species identification can be achieved using bark images, which are available year-around compared with leaf, flower, and fruits. However, most of the existing studies fall short of explaining the reasoning process behind their models. The inherent complexity of these models often renders them black boxes, making it difficult to understand. This lack of transparency can hinder trust and transferability in the model's output, especially in critical applications like forestry, where accurate and reliable species identification is important~\citep{onishi2021explainable_uav,cheng2022improve_model_tree_explation,hohl2024recent_trend_remotesensing_explainable_ai}.

% While the inherent complexity of these models often renders them black boxes, making it difficult to understand the reasoning behind their predictions. This lack of transparency can hinder trust and transferability in the model's output, especially in critical applications like forestry, where accurate and reliable species identification is important~\citep{hohl2024recent_trend_remotesensing_explainable_ai,cheng2022improve_model_tree_explation,onishi2021explainable_uav}. 

Considering the intra- and inter-species complexity of tree bark images, understanding deep neural networks for tree species classification is important to unveil the black box and provide insights for future applications. 
To address these challenges, Explainable AI (XAI) has emerged to enhance transparency and trust in deep neural networks by shedding light on their reasoning processes~\citep{mostafa2023explainable_phenotypeing_reivew}.
% Specifically, Explainable AI (XAI) has gained increasing attention as a response to the "black-box" nature of deep neural networks. 
Existing XAI methods can be broadly categorized into attribution-based XAI (which focus on attributing importance to input features that drive model decisions) and concept-based XAI (which interprets models using high-level human-understandable concepts).

Attribution-based XAI methods such as Crown-CAM~\citep{marvasti2023crown__cam}, and Grad-CAM  \citep{onishi2021explainable_uav,ahlswede2022weakly_cam,kim2022identifying_bark_cam,huang2024temperate_gradcam} have been used to visualize which image regions contribute to classification decisions in aerial and ground imagery and improve the explainability of tree species identification. Similarly, Shapeley Additive explanations (SHAP)~\citep{lundberg2017unified}, have been utilized in species richness modeling\citep{brugere2023improved_richness_Explainablity}, urban vegetation mapping \citep{abdollahi2021urban_sheap_XAI} and mulberry leaf disease classification~\citep{nahiduzzaman2023explainable_mulberry}. Local Interpretable Model-Agnostic explanations(LIME) has been applied to provide localized visual interpretations in medicinal plant species identification~\citep{nikam2022explainable_spp_lime} and microscopic wood classification~\citep{zhan2023wood_id_lime}.\\

While attribution-based methods are more popular in explaining vision models, they usually cannot provide satisfactory results when interpreting vision classifiers on bark images due to a lack of prominent local features. Attribution-based methods work best on natural images containing distinct local features such as animals (head, paws, tails) and vehicles (wheels, doors) by providing explanations that highlight these local features, which are easily verifiable by users. However, for texture-dominant images that lack prominent local features, such as tree bark, these methods struggle to provide explanations that align with human reasoning. Contrary to the explanation from attribution-based methods, domain experts typically rely on global visual characteristics—such as stripe patterns, roughness, and surface irregularities—to distinguish tree species, which are not revealed by attribution-based methods. The gap in explanation is further illustrated in Figure \ref{fig:attr_xai_vs_human}.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{fig/attr_demo.pdf}
    \caption{Illustration of Attribution-Based Methods on a Bark Image (Trained and Evaluated with MobileNetV2) and domain expert explanation. While attribution-based methods are widely used in bark vision model explanation and similar fields, they cannot reveal global visual features used by domain experts.}
    \label{fig:attr_xai_vs_human}
\end{figure}


% For tree species features, particularly bark images often exhibit repeating, irregular patterns across the entire surface, where global texture plays a crucial role in identification. These feature-based XAI methods provide pixel-level visualization and emphasize small, localized areas rather than holistic features. Therefore, they cannot effectively capture global visual features in bark images and convey global and semantic meaning, making them difficult to interpret.  \\

In contrast, concept-based XAI~\citep{kim2018interpretability_TCAV, chen2019looks} methods offer a more intuitive approach to interpreting models using high-level, human-understandable concepts to explain model decisions, which aligns better with our intuition when distinguishing tree species based on bark images\citep{kazhdan2020now_CME_CXAI,sagar2023leaf_xai}. However, extant concept-based models typically require pre-defined concepts and external effort in collecting images for each concept, which quickly becomes labor-extensive when the number of concepts is large\citep{kim2018interpretability_TCAV,hou2024conceptXAI_skin}. One exception is ProtoPNet\citep{chen2019looks}, which eliminates the requirements of collecting external concept images by using image patches from training set as prototypes to provide concept-based explanations. However, the way image patches are used as concepts still limits the ability to describe global visual features, like attribution-based methods. Besides, concepts that exhibit a clear hierarchical relationship (e.g., light red vs. deep red) or require precise quantification (e.g., 30-degree vs. 60-degree inclination) pose significant challenges for existing concept-based methods since accurately defining and representing such concepts using external concept images is vague and subjective.

% , to the best of our knowledge, few, if not none, of the studies have adopt concept-based XAI methods when interpreting vision models on bark images.

To provide better explanation on bark visual models that matches the intuition of domain experts while mitigating the limitations discussed above, we propose BarkXAI, which is a lightweight post-hoc concept-based method optimized for bark image classifiers. To the best of our knowledge, few, if any, of the studies have adopted concept-based XAI methods when interpreting vision models on bark images. \textit{Our work is the first to provide a concept-based explanation on global visual features on vision models on bark image or texture-dominant images identification.} Our main contributions are listed as follows.

\begin{itemize}
    \item By using operators instead of concept images to reflect features related to bark images, we eliminate the overhead of collecting external concept images found in traditional concept-based XAI methods while ensuring greater practicality.
    \item We provide a way to construct quantifiable concepts that are hard to define and formalize in existing concept-based XAI approaches. 
    \item Our method can evaluate concept importance as well as the reasoning process with inter-concept relationships.
\end{itemize}

% Our approach eliminates the overhead associated with traditional concept-based XAI methods, ensuring greater efficiency and practicality. It enables the definition and quantification of complex concepts that are difficult to formalize in existing approaches. Additionally, it evaluates both concept importance and the model's reasoning process, enhancing interpretability and trust in decision-making.

%\hhl{Rewrite}\hhl{no overhead like other concept-based XAI methods}\hhl{Provide quantifiable concepts which are hard to define in other concept-based XAI methods}\hhl{Evaluate concept importance as well as reasoning process with inter-concept relationships}

% Few studies have integrated concept-XAI with tree species classification, which leads to most of the extant model left unexplained. \textit{As far as we know, we are the first one attempting to mitigate high overhead issue in  lightweight concept-based XAI method.} We built a tree species dataset for real-life application scenarios. We also implemented a pipeline to integrate tree detection and species identification with higher accuracy than existing studies. This paper proposes a novel method for explaining deep learning model predictions for tree species identification using smartphone images of tree trunks in temperate forests. Our method utilizes XAI techniques to highlight the high-level concepts and features that contribute to the model's prediction. By providing human-interpretable explanations, this approach enhances trust in the model's output and facilitates to improve of model transferability in further applications. The ability to explain how AI models arrive at species classifications can aid the adoption of AI in ecological monitoring and biodiversity assessments, furthering forest management and conservation efforts.

  
  

%This approach provides human-interpretable explanations, thereby enhancing trust in the model's output and improving its transferability to other applications. Our methodology involves building a comprehensive dataset for real-life scenarios and implementing an efficient pipeline that integrates tree detection and species identification with higher accuracy than existing studies. By providing human-interpretable explanations, this approach enhances trust in the model's output and facilitates to improve of model transferability in further applications.

 


%\subsubsection{Extant XAI Methods}
%For the task of explaining image classification process, existing XAI methods can be roughly categorized into ante-hoc and post-hoc methods, depending on if the explanation is provided intrinsically within the model or as an add-on after model training. While ante-hoc methods offers integrated explainability as built-in components and provide higher transparency in fidelity, they are usually less performant and require additional setup than the alternatives. Post-hoc methods provide retrospective explanations on trained models by analyzing existing models, which are generally more popular and works on a broader ranges of models, but they are likely to face criticism for potential misalignment with true reasoning processes.

%Ante-hoc methods. Attention mechanisms, such as those in Vision Transformers (ViTs), highlight regions of interest using learned attention weights, offering intuitive visual explanations (Dosovitskiy et al., 2020). While effective for identifying spatial relevance, attention maps may oversimplify complex feature interactions or fail to capture hierarchical dependencies. Prototype-based models (e.g., ProtoPNet) compare input features to learned prototypes (e.g., "bird beak" patterns), enhancing interpretability through part-based reasoning (Chen et al., 2019). However, prototype methods require careful initialization and may struggle with scalability in high-dimensional spaces. Self-Explaining Neural Networks (SENNs) combine neural networks with symbolic representations, linking predictions to human-understandable concepts (Alvarez-Melis & Jaakkola, 2018). Though promising, SENNs often trade accuracy for transparency and depend heavily on predefined concept libraries. Sparse models, enforced via L1 regularization or pruning, improve interpretability by activating fewer neurons per decision (e.g., sparse CNNs). While sparsity aids feature attribution, excessive regularization can degrade performance. Neural Additive Models (NAMs) decompose predictions into additive feature contributions, balancing flexibility and transparency (Agarwal et al., 2021). However, NAMs may oversimplify non-linear interactions. Finally, disentangled representations (e.g., β-VAE) isolate latent variables into semantically distinct factors (e.g., shape, texture), though achieving perfect disentanglement remains challenging (Higgins et al., 2017).

%Post-hoc methods. Saliency maps, such as Grad-CAM and Integrated Gradients, attribute importance to input pixels by leveraging gradient information (Selvaraju et al., 2017; Sundararajan et al., 2017). While widely adopted, these methods can suffer from noise or saturation effects, particularly in deep layers. Perturbation-based approaches, like LIME and SHAP, approximate local decision boundaries by perturbing inputs and fitting surrogate models (Ribeiro et al., 2016; Lundberg & Lee, 2017). Though model-agnostic, these methods are computationally intensive and sensitive to perturbation strategies. Surrogate models (e.g., decision trees) mimic complex models but often fail to capture intricate non-linearities, leading to fidelity issues. Feature visualization techniques, such as activation maximization, generate synthetic inputs that maximally activate neurons, offering insights into learned features (Erhan et al., 2009). However, these visuals may not correspond to real-world patterns. Concept-based explanations, like TCAV, quantify the influence of user-defined concepts (e.g., "stripes") using directional derivatives in activation space (Kim et al., 2018). While powerful, TCAV requires manual concept labeling and is limited by linearity assumptions. Counterfactual explanations identify minimal input changes to alter predictions, aiding actionable insights (Wachter et al., 2017), but they face challenges in generating realistic or diverse examples. Lastly, occlusion sensitivity systematically masks image regions to assess predictive impact, though it lacks granularity compared to gradient-based methods (Zeiler & Fergus, 2014).


 

%Wood identification of Cyclobalanopsis (Endl.) Oerst based on microscopic features and CTGAN-enhanced explainable machine learning models: This paper utilizes LIME (Local Interpretable Model-Agnostic Explanations) to interpret the machine learning models and understand how they identify tree species based on wood cell geometric features.
%Weakly Supervised Semantic Segmentation of Remote Sensing Images for Tree Species Classification Based on Explanation Methods: This paper investigates the effectiveness of four explanation methods for deep neural networks: Class Activation Maps (CAM), Gradient-based CAM (GradCAM), Pixel Correlation Module (PCM), and Self-Enhancing Maps (SEM).
%Urban Vegetation Mapping from Aerial Imagery Using Explainable AI (XAI): This paper employs SHapley Additive explanations (SHAP) to interpret the output of a Deep Neural Network (DNN) model for classifying vegetation covers and to rank the input parameters and select appropriate features for classification.
%Opening the black box: explainable deep-learning classification of wood microscopic image of endangered tree species: This paper uses feature visualization methods to target the key features between species in machine identification.
%Mapping forest in the Swiss Alps treeline ecotone with explainable deep learning: This paper proposes a rule-informed deep learning model that explicitly quantifies intermediate variables like tree height and tree canopy density involved in forest definitions to provide explanations for its decisions.
%Improved prediction of tree species richness and interpretability of environmental drivers using a machine learning approach: This paper uses SHapley Additive exPlanations (SHAP) to interpret the machine learning models and explain the major environmental factors driving tree species richness.
%Explainable deep learning model for automatic mulberry leaf disease classification: This study uses SHapley Additive exPlanations (SHAP) to obtain the explainable capability of the proposed PDS-CNN model and evaluate it by a sericulture specialist.
%Deep neural networks for explainable feature extraction in orchid identification: This paper proposes a novel feature extraction method derived from the taxonomic characteristics of plants and uses deep neural networks to develop feature classifiers.
%Crown-CAM: Interpretable Visual Explanations for Tree Crown Detection in Aerial Images: This paper proposes Crown-CAM, an interpretable class activation mapping method, to generate reliable visual explanations for tree crown detection in aerial images.
%A novel explainable image classification framework: case study on skin cancer and plant disease prediction: This paper proposes a novel explainable image classification framework that combines segmentation and clustering techniques to extract texture features and uses an intrinsic linear white box prediction model and a hierarchy-based tree approach for interpretation.

 






\section{BarkXAI: proposed method}
\label{BarkXAI}
 
%Given the limitations identified in existing XAI models for explaining our vision models, the question we aim to address is: how can we design an XAI method 1) capable of explaining any trained black-box texture vision models 2) using quantifiable concepts meaningful to texture analysis 3) while minimizing additional computational overhead?To answer this question, we propose BarkXAI, a novel XAI approach optimized for image classifiers trained on texture images, such as bark images. Drawing inspiration from the design principles of LIME and TCAV (Testing with Concept Activation Vectors)~\citep{kim2018interpretability_TCAV}, we employ a mechanism that uses perturbed images to assess impacts of different concept-of-interest on the classification of the original image. Unlike LIME, which segments the image into superpixels and perturbs them to build an interpretable model based on the effects of these perturbations on predictions, we emphasize global visual features (such as smoothness or tone) that are either visually significant or meaningful for general texture analysis. Global features are extracted and perturbed, and the resulting impacts are analyzed using a surrogate model such as linear regression and decision tree. Similar to TCAV, our method also aims to explain the models with concepts. However, we rely on parameterized operators instead of collecting images externally for each concept, enabling quantifiable and descriptive concept evaluation without building curated dataset for each concept. Our proposed method is shown in Figure.
Given the limitations in existing XAI models for explaining vision models, we aim to address how to design an XAI method that:  
1) Can explain any trained black-box texture vision models,  
2) Uses quantifiable concepts meaningful for texture analysis, and  
3) Minimizes additional computational overhead.  
To solve this, we propose BarkXAI, a novel XAI approach optimized for image classifiers trained on texture images (e.g., bark images). Inspired by LIME and TCAV~\citep{kim2018interpretability_TCAV}, our method uses perturbed images to assess the impact of concept-of-interest on classification. Unlike LIME, which relies on superpixel segmentation, we focus on global visual features (e.g., smoothness or tone) that are visually significant for texture analysis. These features are extracted and perturbed, with impacts analyzed using surrogate models like linear regression or decision trees. Similar to TCAV, our method explains model decisions through concepts but uses parameterized operators instead of externally curated datasets, enabling efficient concept evaluation without extensive dataset preparation. Our proposed approach is illustrated in Figure~\ref{fig:explaining_process}.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{fig/algorithm_diagram.pdf}
    \caption{Pipeline of BarkXAI in Explaining Bark Vision Models}
    \label{fig:explaining_process}
\end{figure}

%Given the limitations of existing XAI models in explaining vision models, we address the question: how can we design an XAI method that (1) explains any trained black-box texture vision model, (2) uses quantifiable concepts relevant to texture analysis, and (3) minimizes computational overhead?

%To answer this, we propose BarkXAI, an XAI approach optimized for image classifiers trained on texture images, such as bark. Inspired by LIME and TCAV (Kim et al., 2018), BarkXAI perturbs images to assess the impact of different concepts. Unlike LIME, which segments images into superpixels, our method focuses on global visual features (e.g., smoothness, tone) that are both visually significant and meaningful for texture analysis. These features are perturbed and analyzed using surrogate models like linear regression or decision trees.

%Like TCAV, BarkXAI explains models through concepts. However, instead of relying on externally sourced images, we use parameterized operators, allowing for more quantifiable and descriptive concept evaluations without requiring curated datasets.


%In the remainder of this section, we first discuss the global visual features intended for use, followed by an explanation of how the extraction functions are designed to ensure commutativity under composition operations, and finally, we describe the training of a surrogate model based on predictions from perturbed images.

\subsection{Global Visual Features}
Global visual features in images refer to attributes or descriptors that depict the overall structure or appearance of an entire image rather than concentrating on specific objects or localized regions. These features encompass high-level concepts and they can be categorized into the following types.
\begin{itemize}
    \item  Color: Features of colors can be reflected in several ways, such as color histogram and color correlograms. They reflect the distribution or correlation of pixel colors across the image. For bark images, color can be an important indicator for species classification.
    \item Texture: Patterns or surface properties, such as smoothness, coarseness, or regularity, are utilized to characterize textures. In conjunction with variations in illumination conditions, smoothness, and coarseness can provide insights into the evenness or roughness of the surface. In the case of bark images, the presence of ridges and cracks serves as important indicators of tree species. These features also contribute to estimating the age and health condition of the trees. 
    \item Shape: The geometric structure of the texture surface can be described by properties such as lines, edges, and circles, among others. The dominant orientation or directionality, along with the presence of these features, provides valuable insights into the texture type.
    \item Groove and surfaces: Grooves in tree bark typically exhibit significant visual activity that can be meaningful to species classification. More complex surfaces are characterized by a greater number of edges, variations, and detail while simpler surfaces tend to display fewer visual elements and greater uniformity. %bumps in tree barks typically exhibit significant visual activity or complexity, characterized by a greater number of edges, variations, and details, along with a diverse range of shapes, sizes, and orientations, in contrast to simple textures that tend to display fewer visual elements and greater uniformity.
\end{itemize}

 
 
% \usepackage{tabularray}
\begin{table}
\centering
\caption{Operators (Concept Key-Value Pairs)}
\label{Tab:opts}
\begin{tblr}{
  cell{4}{1} = {r=2}{},
  cell{6}{1} = {r=2}{},
  hline{1-2,8} = {-}{},
}
\textbf{Concept}  & \textbf{Key} & \textbf{Value}      & \textbf{Notes}                                                      \\
Color             & Tune         & +5/+10              & Overall color tune is increased by 30 and ~50      \\
Texture           & Smooth       & +150/~+300       & Smooth image (+150/ +300)                                           \\
Shape ~           & Flip         & Horizontal/Vertical & Flip image horizontally/vertical                                    \\
                  & Rotate       & +$\pm30$/$\pm90$      & Rotate image by$\pm30$, $\pm90$ (clockwise/anticlockwise) \\
Groove and Surface & Groove         & Remove              & Remove the groove parts of the image.                                \\
                  & Surface      & Remove              & Remove the surface parts of the image.                              
\end{tblr}
\end{table}



% \begin{equation}
% p(y = i_{GT} | x_m)

% \end{equation}

\subsection{Commutative Operators}

To extract global visual features from texture images, we introduce a collection of operators $F$, which serve as feature extractors. Each operator $f \in F$ is generally designed as an unary function, which takes one input $x$ and outputs $x'$, which is a perturbed version of $x$. For some operators $f\in F$, there might be a parameterized version that can take a vector of parameters such as $f_{\theta}$.

Each of the operators ($f \in F$) is designed to involve exactly one concept key-value pair. The concept refers to the type of global visual feature being evaluated, while the key-value pairs under each concept represent specific values associated with that concept. We summarize the concept key-value pairs in Table~\ref{Tab:opts}. Note that the concept key-value pairs used in our method do not constitute a comprehensive list for describing global visual features, as it is impractical to enumerate and evaluate all possible concepts for each visual feature. Moreover, the classification of key-value pairs can be ambiguous, given that the aspects of concepts used to describe texture images often overlap.  

\subsubsection{Color (Tune) Operator}
The color operator is designed to manipulate the hue component of an image,  altering its color characteristics. This operator takes an image input, which is then transformed into the HSV (Hue, Saturation, Value) color space. Applying the HSV conversion allows us to separate the chromatic content (hue) from the image, allowing for pixel-intensity-agnostic manipulation.

We adjust the first channel (which is the hue channel) by adding a predefined parameter representing the changes in hue. The adjustment is performed modulo 180 (range of hue value) to ensure that the updated hue values remain within valid bounds. The adjusted image is converted back to the RGB color space to keep consistency with the original input image. We use the standard deviation ($\sigma$) of hue values from all bark images as a standard unit to quantify the variation of hue values. In experiments, we increase the tune value by 5 (close to $1 \times \sigma$) and 10 (close to $2 \times \sigma$), to simulate the color changes in texture images.  


\subsubsection{Smooth Operator}

%The smooth operator smooths the surface of the texture image. While there are various smoothing algorithms, such as Gaussian blur and median blur, many of them blur image edges along with the surface, making geometric structures harder to recognize. To prevent edge information loss, we employ a bilateral filter to achieve surface smoothing in texture images while preserving edge integrity. The bilateral filter is characterized by its ability to perform edge-preserving smoothing by considering both spatial proximity and pixel intensity differences, with adjustable sensitivity to pixel intensity differences to achieve varying levels of smoothness.

%Two sensitivity values ($+150, +300$) are utilized in experiments; varying sensitivity levels result in different intensities of smoothing for color changes in the surface area, leading to texture detail loss. The smoothed outcomes are visually inspected to ensure integrity.

The smooth operator refines the texture image surface. While various smoothing algorithms, such as Gaussian and median blur, often blur edges and obscure geometric structures, a bilateral filter is used to preserve edge integrity. This filter achieves edge-preserving smoothing by considering both spatial proximity and pixel intensity differences, with adjustable sensitivity to control smoothness levels. Two sensitivity values ($+150, +300$) are tested, affecting the degree of smoothing and potential texture detail loss. The results are visually inspected to ensure integrity.


\subsubsection{Groove/Surface Removal Operator}
%\hhl{Remove grooves, use fissure instead Bumps and Surface}
A  groove is a small valley structure found on the surface of stems, branches, and other plant organs of woody plants; for bark images, they are roughly divided into grooves and surface areas for ease of evaluation. Compared to the surface areas, which are generally smoother, grooved areas are spongier or more porous, with a slightly rougher or raised texture relative to the surrounding regions. The differences in shape often result in color variation due to uneven illumination, and the edges with the highest color contrast are identified as the contours of the grooved area.

While accurate segmentation of the grooved area should be ideal for perturbing images accurately, it defeats our purpose of building a simple and intuitive operator for image explanation with extra human effort. Additionally, the accurate segmentation of grooves, even if manageable, does not improve the explainability in our proposed solution. Instead, we propose and implement a simple vision-based pipeline to segment the grooved areas based on the color contrastness of the texture images. 
\subsection{Process to explain image }
The pipeline consists of four steps. (1) First, it begins by converting the input image to a grayscale representation to simplify the image data as a way to reduce computational complexity while preserving essential structural information. (2) The resultant image is thresholded to separate the foreground from the background, resulting in a binary image. To determine an optimal threshold value automatically, we employ Otsu's thresholding method which adaptively determines the threshold value. 3) We apply morphological operations to enhance feature detection by removing noises and filling small gaps. With these two operations, only the significant features can be retained. (4) Lastly, we scan the image and identify all the contours, which are the boundaries of connected components in the image, returning a list of contour points.

The contours which correspond to the significant features in the image are used as binary masks to segment the area of the grooves. Conversely, we can segment the surface area using a flipped mask generated by applying logical NOT operation on the groove mask. Figure~\ref{chart_barkXAI} shows the intermediate results of each step. The figure shows the results after processing the groove- and surface-only images using seam carving, which can further demonstrate the effectiveness of our proposed pipeline.


\subsubsection{Shape Operators}

%Flipping and rotating the image are simple yet effective methods to alter the directionality of dominant features commonly present in texture images. The implementation of these operators is straightforward; however, it is important to note that they are designed to maintain the dimensional consistency of the output image with the input. This may lead to pixel information loss due to clipping during $\pm90$-degree rotations when the image's height and width differ or the introduction of additional black pixels during $\pm30$-degree rotations. Flipping operators shift dominant features of the texture image from one side to the opposite side. When combined with rotation operators, they assist in evaluating how the positioning of visual features influences a vision model.


 Flipping and rotating images effectively alter the directionality of dominant texture features while preserving dimensional consistency. However, $\pm90$-degree rotations may cause pixel loss due to clipping in non-square images, and $\pm30$-degree rotations can introduce black pixels. Flipping shifts dominant features to the opposite side, and when combined with rotation, helps assess the impact of feature positioning on vision models.







\subsubsection{Commutativity under Composition Operations}

Commutativity under Composition Operations (\textbf{CUCO}) is a property defined for a set of operators $F$. Let $f: X \to X$  and $g: X \to X$  be two operators/functions defined on a set $X$. The operation of composition of these functions, denoted  $(f \circ g)(x) = f(g(x))$ , is said to be commutative if and only if:

\begin{equation}
    f \circ g = g \circ f
\end{equation}

That is, for all  $x \in X$ ,

\begin{equation}
    f(g(x)) = g(f(x))
\end{equation}

\textbf{CUCO} property is desired for the operators in our settings because it allows us to study the independent influence of each operator on the vision model without accounting for the order of their application. This simplifies the modeling process by eliminating the need to consider inter-correlations between operators based on sequence order. Specifically, for any combination of the mentioned operators, we expect the final result to be unique and independent of the order in which the operators are applied. While it may not be feasible for our designed operators to fully satisfy this property due to unavoidable information loss in certain operations, such as rotation, operators that partially satisfy \textbf{CUCO} remain valuable. Furthermore, testing this property theoretically can be impractical given the dissimilarity in the internal procedures of each operator, so we employ a numerical approach to assess the degree to which our operators achieve the \textbf{CUCO} property. 

In our experiment, we calculate the average MAE value between images (with pixel range $0-255$) generated with same set of operators but applied in different orders. The average MAE across our dataset is less than $30$, which indicates the operators we defined partially satisfy this property.


\subsection{Explaining with Surrogate Models}
We randomly sample a sequence of operators $f_1, f_2, ..., f_N$ from the set of all defined operators $f \in F$ and apply them to perturb the input image $x$. The perturbed image, denoted as $x' = f_1 \circ f_2 \circ \cdots \circ f_N(x)$, is obtained after sequentially applying all the operators. The image $x'$ is then used as input to the vision model, and the confidence value (probability) $p(y = i_{GT} | x')$, corresponding to the class ID $i_{GT}$ in the output probability distribution (typically generated by a softmax activation function), is recorded. Operators included in the sampled sequence are labeled as 1, while those excluded are labeled as 0, resulting in a binary vector $\varphi \in \{0, 1\}^{|F|}$, where $|F|$ denotes the cardinality of the operator set.
The sampling process is repeated for $m > |F|$ iterations to ensure that every operator is selected at least once. To avoid redundancy, the confidence value is computed only for unique sequences, regardless of the order of the operators. The confidence values are concatenated to form a vector $c \in \mathbb{R}^m$, while the binary vectors for all sampled sequences are concatenated to construct a 2D binary matrix $\Phi \in \{0, 1\}^{m \times |F|}$.
The selection matrix $\Phi$ and the confidence vector $c$ are utilized to train a surrogate model. Both linear regression and Classification and Regression Tree (CART) models are employed to analyze the impact of operator combinations on the vision model's confidence values. Linear regression identifies the independent effect of each operator, while CART explores interactions between operators, providing a comprehensive analysis of the causal relationship between operator usage and the vision model's performance. This process is illustrated in Figure \ref{chart_barkXAI_operation}.

With these surrogate models, we can interpret the impact of operators on the confidence value as the influence of the concepts behind the operators to the performance of the models. For example, if there is a drastic confidence value change after a color operator is used, we can interpret this behavior as \textit{for this image $x$ which has class $c$, color plays a significant role when the vision model classifies it to be in class $c$}. Together with manual observation and domain knowledge, we can verify the correctness of the model and analyze/debug the model when the decision process is against our intuition. We will showcase real examples of explaining vision models in the experiment section.
 %If the most influential operator is the smooth operator, we can claim, \textit{the smoothness of the texture is the leading cause of why the vision model makes its prediction.} 
% \end{figure}
\begin{figure}[h]
\begin{center}
\includegraphics[width=0.6\textwidth]{operations.png}  
  \caption{Ten operations demonstration on Black cherry~(\textit{Prunus serotina}). }
\label{chart_barkXAI_operation}
\end{center}
\end{figure}

\section{Experiment results: Understand bark images with XAI}
\subsection{Data Collection and BarkXAI Evaluation}

We built a tree species dataset comprising bark images from 21 species (Table~\ref{21species_list}), with 8,184 images in total.
Images were collected by dendrologists in Indiana, with 3 to 5 photos of each tree from different distances and perspectives, ranging from 1 to 4 meters from the tree trunks. Due to the diversity of species present in both natural forests and plantations, the number of images per species varies but each class contains at least 80 images in the dataset. We further integrated an automated pipeline using YOLOv11~\citep{Jocher_Ultralytics_YOLO_2023} and the Segment Anything Model~\citep{kirillov2023segment}, which includes tree detection and bark segmentation. With the clean bark images (examples in Figure~\ref{chart_barkXAI_operation}), we trained MobileNetv2, a light-weighted deep neural network, to classify tree species only using bark images, with an accuracy of 90+\%. \\
To interpret how models predict tree species, we used all operators (Table~\ref{Tab:opts}) and evaluated how each operator impacts the model's decision process. 
In Figure~\ref{chart_barkXAI}, we illustrate the feature importance of various operators applied on the model in all images. For the vision model, MobileNetV2, "remove grooves" and "remove surface" emerge as the most influential operations affecting species prediction. Meanwhile, different rotation angles produced similar effects on both models.  

%We extracted five feature concepts from bark images and 

%- Clearly list and describe each factor, explaining its role in determining species classification. For example:     
%- **Feature Importance**: Identify which image features (e.g., color, texture, shape) are most critical for accurate species 
%Show examples of images after typical processing. 

\subsection{Performance Comparison}

% To verify if BarkXAI can identify the most important visual features, we 

% BarkXAI, as a concept-XAI method, enhances our understanding of tree species identification. The widely used TCAV method, a popular concept-XAI approach, requires collecting concept images and training on a customized dataset, making it time-consuming with uncertain results. In contrast, our method efficiently extracts tree bark feature concepts with reduced computational and time requirements.\\

To quantitatively compare our proposed method with others in providing explanations that align with human intuition, we created a test dataset with the reserved bark images and labeled concepts (21 classes with 20 images in each class). For each image, five concepts (rugged, plated, furrow, vertical stripped, and smooth) were ranked by human visual interpretation as ground truth based on how their importance in identifying tree species. Each concept-based method then predicted the ranking of each concept for every image. The predicted orderings were compared with the ground truth using Kendall’s Tau.

Notably, while our method allows for quantifiable concepts by adjusting parameters, we need to compromise by using limited concepts that are easier to define in most concept-based methods such as TCAV. Similar to other variants, TCAV relies solely on collected concepts images and quantifiable concepts are thus hard to define. Some of the concepts, such as color, are also not well-defined concept images, making quantifiable concepts difficult to define. Some concepts, such as color, are not well-defined within TCAV, complicating the collection of concept images. Therefore, for the propose of fair comparison, we only selected concepts that are generally applicable to concept-based methods. the purpose of a fair comparison, we only pick concepts which are easy to be used in general concept-based methods. 

We prepared externally collected concept images for each concept (100 images for each concept) and the average magnitude of the last 3 feature layers is used as the metric to rank each concept given a test image during TCAV evaluation. We also use llama3.2-vision~\citep{dubey2024llama} to provide rank based on each bark image (zero-shot style) depending on its multi-modal ability. There is no direct mapping between our defined operators to test concepts, so the importance of test concepts are inferred with sensitivities from one or more operators. The same test dataset is used in all methods in the experiment and except for Llama3.2-vision, all the XAI methods are evaluated using the pretrained MobileNetv2 model we discussed earlier. We include further discussion in the appendix.

Table \ref{Tab:tau_comparision} shows the average $-1 \leq \tau \leq 1$ value in each species from several methods, where values close to 1 indicates strong agreement and values close to -1 indicate strong disagreement. BXAI (DT), BXAI(LR), and BXAR(RF) are all variants of our proposed method but with different surrogate models (decision tree, linear regression and random forest respectively). We can observe that nearly half of the predictions from TCAV and zero-shot llama3.2 are against human intuition ($\tau < 0$), and almost all BXAI methods provide importance rankings that agree with ground truth results ($\tau > 0$). Compared with llama3.2, TCAV achieves slightly better performance, however, our proposed method outperformed both by large margins. This indicates that our proposed method, compared with TCAV and llama3.2, gives better model explanations that conform to human understanding. We also include examples of explanation in Section \ref{explanation_example} as well as examples of using decision tree as surrogate model to visualize reasoning process in Figure \ref{chart_barkXAI_TREE} in appendix. 

% Specifically, $Imp_{rugged}(p)$ is set to $Imp_{remove\_dark}(p)$. $Imp_{vertical\_stripped}(p)$ is set to the average value of $Imp_{rotate\_30+}(p)$ and $Imp_{rotate\_30-}(p)$. $Imp_{smooth}(p)$ is set to be $Imp_{surface\_smoothed}$. 




 




%To evaluate its efficiency, we compared this approach with TCAV, which also enables post hoc analysis using high-level human concepts. In accordance with TCAV's requirements, we constructed a dataset comprising five key concepts aligned with our processing operators. These images represent the following concepts: strip (vertical feature), rugged, smooth, plated and scaly, and furrow. Each concept includes 100 images to train a linear classifier for concept extraction. 

 % - Explain how my method integrates TCAV into ConceptNet-XAI.
 %  - Describe any differences or similarities between my approach and pure TCAV.
 %  - Interpretability: Were the explanations from my method easier for humans to understand compared to TCAV's output?
%   - Consistency with Human Expertise: Do the explanations align with what experts in ecology or botany know about tree species?

   

% \begin{figure}[h]
%\begin{center}
%\includegraphics[width=0.5\textwidth]{tcav.png} 
%  \caption{\hhl{Add examples with TCAV, Grad-CAM, LIME}}
% \label{tcav}
% \end{center}
%\end{figure}


%\subsection{Evaluation of the BarkXAI}
%Evaluate the method we proposed for understanding the 



\section{Conclusion}
In this paper, we propose a lightweight post-hoc XAI method that provides human-interpretable explanations for tree bark vision models, enhancing transparency and facilitating expert validation. Compared to other similar works in vision model explanation, our approach utilizes concepts rather than the commonly used attribution-based methods to explain the global visual features in tree barks. While other concept-based XAI methods typically require building external concept image datasets, we employ operators to create quantifiable concepts without additional overhead. Compared to TCAV and Llama3.2, our method offers explanations that align more closely with human perception. Although our approach demonstrates superior explainability for bark images, we acknowledge that its performance can heavily depend on carefully designed operators, which may require expert knowledge for adaptation across different domains. Furthermore, similar to other concept-based XAI methods, while our approach effectively captures global visual features that are difficult to achieve with attribution-based methods, the explanations remain susceptible to subjectivity.

% The results demonstrate the effectiveness of the approach in accurately identifying tree species while providing insights into the model's decision-making process. This research contributes to the growing trend of using AI for environmental monitoring and conservation. By providing accurate and interpretable tree species identification, the proposed method can support sustainable forest management, biodiversity assessment, and ecological research. The integration of deep learning with XAI paves the way for more reliable and trustworthy AI systems in the field of forestry, enabling informed decision-making and effective conservation efforts.
 


%\subsubsection*{Author Contributions}
%Yunmei Huang: Data curation, Conceptualization, Writing. Songlin Hou: Conceptualization, Conceptualization, Writing; Zachary Nelson Horve: Writing; Charles C Warner: Data curation; Gazo Rado: Data curation; Songlin Fei: Writing, Resources, Funding acquisition, Conceptualization.
 

%\subsubsection*{Acknowledgments}
%This work was supported by the National Institute of Food and Agriculture, United States [grant numbers 2023-68012-38992].  

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\appendix
\section{Appendix}
% Table \ref{21species_list}: Tree species with scientific name in the bark dataset.
 
 \begin{figure}[h]
 \begin{center}
 \includegraphics[width=0.9\textwidth]{mobilenet_ranking_feature.png} 
   \caption{Feature Importance of Operators on All Testing Images}
  \label{chart_barkXAI}
  \end{center}
\end{figure}

\subsection{Concept Mapping}
\label{concept_mapping}
For easier comparison with other XAI methods, we map our calculated concepts derived from operators into inferred concepts (Smooth, Plated, Rugged, Furrow, Vertical Stripped) which are more closely related to bark features by combining several calculated operations. We denote feature importance of a calculated concept $c$ with key $k$ and value $v$ as $FI(k, v)$ and significance of inferred concept as $Sig(c)$. We used values related to each operator and sort the inferred concepts from the most significant to the least significant based on significance values. Specifically, $Sig(smooth)$ is the maximum value between $FI({smooth}, +150)$ and $FI({smooth}, +300)$. $Sig(vertical\ stripped)$ is calculated as the average value of $FI({rotate}, -30)$ and $FI({rotate}, +30)$. $Sig(rugged)$ is the direct value of $FI(groove, remove)$. $Sig(plated)$ is the average value of $FI({rotate}, -30)$, $FI({rotate}, +30)$, $FI({flip}, horizontal)$ and $FI({flip}, vertical)$. $Sig(furrow)$ is the average value of $Sig(rugged)$ and $Sig(vertical\ stripped)$. The inferred concepts are then ordered based on the significance values. Please note, the way how the concept significance values is calculated is based on how each concept can be further decomposed and represented with the calculated concepts from operators, alternative interpretation can exist since perception of concept can always be subjective, due to the nature of concept-based XAI methods.

\subsection{Explanation Example}
\label{explanation_example}
In Figure \ref{fig:example_explain}, we illustrate several explanation examples generated with BarkXAI(LR) on the trained vision model. Feature importance is calculated on each bark image and feature importance related to each calculated concept is shown. In the first image (American beech), the two most dominant calculated concept are both related to surface smooth, which indicates of the selected calculated concepts, smooth surface is the most important visual feature for the vision model to classify it as an American beech. In the second image (Northern red oak), the two most dominant calculated concept are both related to rotation. It shows the directionality (vertical) in the image is more important than others, such as color information (tuned\_10 and tuned\_5). The inferred concepts (Smooth, Plated, Rugged, Furrow, Vertical Stripped) are derived based on how they can be decomposed into the calculated concepts, which are used to facililate the comparison with other concept-based methods.


\begin{table}
\centering
\caption{Importance Rankings of Several Methods with Kendall’s Tau}
\label{Tab:tau_comparision}
\scalebox{0.8}{
\begin{tblr}{
  hline{1-2,23} = {-}{},
}
Species              & TCAV               & Llama3.2-Vision    & \textbf{BXAI (DT)}           & \textbf{BXAI (LR)}         & \textbf{BXAI (RF)}         \\
American basswood   & $-0.011 \pm 0.328$ & $-0.2 \pm 0.415 $  & $0.126 \pm 0.431 $  & $0.442 \pm 0.479$ & $0.063 \pm 0.454$ \\
American beech      & $-0.063 \pm 0.225$ & $0.032 \pm 0.441 $ & $0.484 \pm 0.375 $  & $0.863 \pm 0.146$ & $0.495 \pm 0.442$ \\
American elm        & $0.09 \pm 0.337 $  & $-0.12 \pm 0.515 $ & $0.47 \pm 0.365 $   & $0.83 \pm 0.255 $ & $0.44 \pm 0.459 $ \\
American sycamore   & $0.418 \pm 0.324 $ & $-0.236 \pm 0.389$ & $0.509 \pm 0.446 $  & $0.927 \pm 0.23 $ & $0.473 \pm 0.299$ \\
Bitternut hickory   & $-0.015 \pm 0.199$ & $0.108 \pm 0.32 $  & $0.354 \pm 0.438 $  & $0.631 \pm 0.57 $ & $0.477 \pm 0.404$ \\
Black cherry        & $-0.244 \pm 0.43 $ & $0.044 \pm 0.445 $ & $0.289 \pm 0.5 $    & $0.733 \pm 0.333$ & $0.456 \pm 0.498$ \\
Black oak           & $0.089 \pm 0.486 $ & $-0.1 \pm 0.423 $  & $0.367 \pm 0.477 $  & $0.722 \pm 0.406$ & $0.489 \pm 0.378$ \\
Black walnut        & $-0.14 \pm 0.254 $ & $0.08 \pm 0.325 $  & $0.16 \pm 0.28 $    & $0.66 \pm 0.269 $ & $0.18 \pm 0.34 $  \\
Eastern cottonwood  & $-0.117 \pm 0.264$ & $0.033 \pm 0.415 $ & $-0.017 \pm 0.436 $ & $0.567 \pm 0.446$ & $0.233 \pm 0.399$ \\
Eastern white pine & $-0.28 \pm 0.271 $ & $0.2 \pm 0.438 $   & $0.24 \pm 0.427 $   & $0.24 \pm 0.496 $ & $0.4 \pm 0.335 $  \\
Hackberry            & $-0.19 \pm 0.313 $ & $-0.12 \pm 0.354 $ & $0.26 \pm 0.415 $   & $0.55 \pm 0.46 $  & $0.31 \pm 0.462 $ \\
Honeylocust          & $0.022 \pm 0.457 $ & $0.022 \pm 0.346 $ & $-0.111 \pm 0.285 $ & $0.467 \pm 0.481$ & $0.022 \pm 0.416$ \\
Northern red oak   & $0.221 \pm 0.361 $ & $-0.2 \pm 0.486 $  & $0.189 \pm 0.483 $  & $0.347 \pm 0.415$ & $0.179 \pm 0.366$ \\
Pignut hickory      & $-0.07 \pm 0.376 $ & $-0.24 \pm 0.403 $ & $0.05 \pm 0.275 $   & $0.64 \pm 0.463 $ & $0.23 \pm 0.541 $ \\
Sassafras            & $0.046 \pm 0.491 $ & $-0.2 \pm 0.392 $  & $0.092 \pm 0.512 $  & $0.4 \pm 0.376 $  & $0.292 \pm 0.397$ \\
Shagbark hickory    & $-0.057 \pm 0.386$ & $0.171 \pm 0.345 $ & $0.133 \pm 0.438 $  & $0.667 \pm 0.442$ & $0.114 \pm 0.522$ \\
Silver maple        & $0.2 \pm 0.245 $   & $-0.35 \pm 0.218 $ & $0.35 \pm 0.384 $   & $0.6 \pm 0.316 $  & $0.45 \pm 0.456 $ \\
Sugar maple         & $0.19 \pm 0.306 $  & $0.03 \pm 0.324 $  & $-0.13 \pm 0.359 $  & $0.01 \pm 0.436 $ & $-0.04 \pm 0.463$ \\
White ash           & $0.088 \pm 0.36 $  & $0.05 \pm 0.357 $  & $0.225 \pm 0.429 $  & $0.437 \pm 0.459$ & $0.175 \pm 0.463$ \\
White oak           & $0.25 \pm 0.296 $  & $0.12 \pm 0.349 $  & $0.0 \pm 0.335 $    & $0.54 \pm 0.415 $ & $0.23 \pm 0.381 $ \\
Yellow poplar       & $0.12 \pm 0.299 $  & $-0.04 \pm 0.383 $ & $0.37 \pm 0.359 $   & $0.61 \pm 0.412 $ & $0.25 \pm 0.384 $ 
\end{tblr}
}
\end{table}


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{fig/barkXAI_example.pdf}
    \caption{Examples of Explanations Generated using BarkXAI(LR). Feature importance values are calculated with softmax performed on slope values. The relative importance of inferred concepts are calculated based on feature importance.}
    \label{fig:example_explain}
\end{figure}



 
 \begin{figure}[h]
     \centering
    \includegraphics[width=0.9\textwidth, height=180pt]{fig/decison_tree_demo.pdf} 
   \caption{Example of Using Decision Tree as Surrogate Model to Show Inter-Concept Relationships between Concepts. The path denoted by green arrows is the most likely path of the reasoning process of the vision model given the particular image input. This serves as an example of interpreting the vision model based on calculated concepts. The inter-concept relationship can be explored with decision tree.}
  \label{chart_barkXAI_TREE}
\end{figure}

% \usepackage{tabularray}
\begin{table}
\centering
\caption{ Tree species with scientific name in the bark dataset.}
\label{21species_list}
\begin{tblr}{
  hline{1-2,13} = {-}{},
}
\textbf{Common Name} & \textbf{Species}               & \textbf{Common Name} & \textbf{Species}                \\
American basswood    & \textit{Tilia americana}       & Honey locust         & \textit{Gleditsia triacanthos}  \\
American beech       & \textit{Fagus grandifolia}     & Northern red oak     & \textit{Quercus~rubra}          \\
American elm         & \textit{Ulmus americana}       & Pignut hickory       & \textit{Carya glabra}           \\
American sycamore    & \textit{Platanus occidentalis} & Sassafras            & \textit{Sassafras albidum}      \\
Bitternut hickory    & Carya \textit{cordiformis}     & Shagbark hickory     & \textit{Carya ovata}            \\
Black cherry         & \textit{Prunus serotina}       & Silver maple         & \textit{Acer saccharinum}       \\
Black oak            & \textit{Quercus velutina}      & Sugar maple          & \textit{Acer saccharum}         \\
Black walnut         & \textit{Juglans nigra}         & White ash            & \textit{Fraxinus americana}     \\
~Eastern cottonwood  & \textit{~ ~Populus deltoides}  & White oak            & \textit{Quercus alba}           \\
Eastern white pine   & \textit{Pinus strobus}         & Yellow poplar        & \textit{Liriodendron tulipifer} \\
Hackberry            & \textit{Celtis occidentalis}   &                      &                                 
\end{tblr}
\end{table}

\clearpage
\begin{figure}[h]
\begin{center}
\includegraphics[width=0.98\textwidth]{operator_example.png} 
  \caption{Ten Operations Demonstration Examples Among 12 Species: (1. Northern red oak, 2. Bitternut hickory, 3. Sugar maple, 4. White oak, 5. Pignut hickory, 6. American beech 7. White ash, 8. Shagbark hickory, 9. American basswood, 10. Black cherry, 11. Black walnut, 12. Eastern cottonwood). }
\label{chart_barkXAI}
\end{center}
\end{figure}





\end{document}
