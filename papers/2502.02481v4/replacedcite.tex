\section{Related Work}
\subsection{Multilinguality in LLMs}

Large language models have gained substantial attention from the research community due to their outstanding performance on various tasks ____. However, most advancements in LLMs have primarily focused on English or Chinese, resulting in inadequate performance in low-resource languages ____. The reason is that the language distribution of the training data for LLMs is highly imbalanced and the quality varies across languages ____. Numerous efforts have been made to enhance the multilingual capabilities of LLMs. Some studies seek to enhance the performance of LLMs on low-resource languages through continual pretraining ____ or supervised finetuning ____ using data from these languages. Additionally, some researchers ____ leverage contrastive learning to align the internal representations of different languages, allowing the model to improve its cross-lingual capabilities with minimal training data. In our study, we assessed the latest open-source LLMs on translation tasks across $28$ languages. We find that these latest models still show significant performance disparities across different languages, indicating that low-resource languages continue to pose a significant challenge for these models. Therefore, we expand the Gemma2 model to cover $28$ commonly used languages, enabling it to demonstrate strong generative and translation capabilities across these languages.

\subsection{Multilingual MT with LLMs}

The use of LLMs has shown significant progress in MT tasks ____, leading to different approaches on LLM-based translation. One line of work focuses on the in-context translation capabilities, and LLMs are provided with parallel sentences to guide the model in generating the target sentence. Some studies ____ show that MT performance can be improved by using semantically related parallel sentences as examples, exhibiting promising results in scenarios with limited computational resources and insufficient parallel data.

Another line involves finetuning with translation instructions. ____ initially pretrained the base model on monolingual data, followed by finetuning on small human-written parallel datasets, which resulted in strong translation performance. ____ challenge the perspective that the impact of parallel data is reduced in the LLM era and demonstrate the effectiveness of parallel data during continual pretraining on enhancing multilingual translation. However, the distribution of the parallel datasets could be highly imbalanced across different languages for massively multilingual translation scenarios. ____ incorporates a fixed multilingual mixture of monolingual (two-thirds) and parallel (one-third) data, further enhancing LLMs' translation capabilities. Our work differs in that we primarily focus on exploring the optimal mixing strategy of monolingual and parallel data during continual pretraining to achieve the best translation performance.