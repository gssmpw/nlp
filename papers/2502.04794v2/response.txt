\section{Related Works}
\label{II}

\subsection{Pre-trained Models for Extracting Image Features}

The evolution of pre-trained models has fundamentally transformed image feature extraction, particularly in domains with limited annotated data. Early work by Shin et al. **Shin**, "Learning Fee-Backward Neural Networks" leveraged convolutional neural networks (CNNs) pre-trained on large-scale datasets and enhanced them through data augmentation techniques. While these CNN-based approaches improved performance on small medical datasets, they often struggled with spatial representation challenges and orientation-specific tasks.  

The introduction of Transformer-based architectures by Vaswani et al. **Vaswani**, "Attention Is All You Need" marked a significant shift in representation learning. Although their self-attention mechanism effectively captured long-range dependencies, its direct application to images was computationally prohibitive. To address this, localized attention strategies proposed by Parmar et al. **Parmar**, "Dense Connection for Image Features Extraction" constrained self-attention to smaller, more manageable regions, making Transformers more practical for image feature extraction.  

A major breakthrough came with the Vision Transformer (ViT) by Dosovitskiy et al. **Dosovitskiy**, "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale" , which processed images as sequences of non-overlapping patches. This approach not only simplified image representation but also achieved state-of-the-art performance, provided that large-scale datasets were available for pre-training. However, its reliance on massive pre-training data remained a challenge for specialized fields like medical imaging, where data collection is inherently limited.  


To mitigate data dependency issues, Oquab et al. **Oquab**, "Learning and Transferring Mid-Level Image Representations Using Convolutional Neural Networks" introduced DINOv2, a self-supervised learning framework that employs an automated preprocessing pipeline and a teacher-student training approach to address data limitations. By integrating key elements of DINO and iBOT, it enhances transfer learning for both classification and dense prediction tasks. However, its effectiveness in multimodal settings remains uncertain. Additionally, Perez-Garcia et al. **Perez-Garcia**, "Biomedical Image Encoder Pretrained with Self-Supervised Learning" demonstrated that DINOv2 generalizes well in medical imaging by developing RAD-DINO, a biomedical image encoder pretrained solely on imaging data. Their findings show that RAD-DINO matches or outperforms state-of-the-art models trained with text supervision across multiple benchmarks, underscoring the potential of DINOv2-based self-supervised learning for biomedical feature extraction.

From CNN-based methods to the latest Transformer-driven architectures, all approaches share a common principle: they transform an input image into a high-level representation that can be applied across various tasks. This transformation process has driven renewed interest in how large-scale pre-training, effective data augmentation, and self-supervised learning can enhance performance, even in data-scarce domains.  

\subsection{Multimodal Fusion Using Self-Attention}

Integrating heterogeneous data sources is a fundamental challenge in multimodal learning. Self-attention mechanisms have emerged as a powerful solution, offering a flexible approach to capturing both local and global relationships across modalities **Dai**, "Attention Feature Fusion: A New Framework for Multimodal Learning" .  

One notable example is the Attention Feature Fusion framework by Dai et al. **Dai**, "Attention Feature Fusion: A New Framework for Multimodal Learning" , which refines multimodal representations through a multi-scale Channel Attention Module. This method dynamically adjusts the relative importance of each modality while preserving essential information via skip connections, demonstrating how self-attention can enhance task-relevant feature extraction.  

Originally developed for natural language processing, self-attention has naturally extended to vision and multimodal contexts. Multi-head attention, which divides its focus across multiple subspaces, enables parallelized attention computations, making models more expressive and better equipped to handle spatial, temporal, and semantic variations across data streams.  

By facilitating cross-modal interactions, self-attention has become a cornerstone of advanced multimodal fusion strategies. Its adaptability and ability to dynamically highlight relevant information from each data source make it an essential tool for improving the integration of diverse modalities.