\clearpage
\setcounter{page}{1}
\maketitleappendix

\section{Related Work: Case Studies}
\label{sec:case_study_related}
\textbf{Action recognition/forecasting.} Recent advancements in action recognition have significantly improved our ability to understand and classify human activities in videos, starting with the SlowFast network~\citep{feichtenhofer2019slowfast}, which introduced a two-pathway approach that processes visual information at different frame rates to capture slow and fast motion patterns. This resembles ventral and dorsal pathways of human brain for action understanding and object recognition, respectively. Following the rise of vision transformers~\citep{dosovitskiy2020image}, MViT~\citep{fan2021multiscale} showed promising results on action understanding benchmarks with multi-scale transformers. Recently, Hiera~\citep{ryali2023hiera}, presented a hierarchical vision transformer that leverages multi-scale feature learning to enhance action recognition performance, by utlizing masked image pretraining as in MAE~\cite{he2022masked}. LART~\citep{rajasegaran2023benefits},  expanded on these prior works by incorporating 3D human pose trajectories to achieve better action prediction performance. Some works forecast and anticipate actions~\cite{lai2024human}. ~\citep{sun2019relational} perform action forecasting on videos using relational information. ~\citep{loh2022long} train an RNN on long-form videos to contextualize the long past and improve predictions of the future.

\medskip
\noindent
\textbf{Car trajectory prediction.}
In the autonomous driving literature, forecasting the future motion of cars is a popular problem~\citep{huang2022survey,cui2024survey}, facilitated by an influx of datasets in recent years~\citep{chang2019argoverse,nuscenes,sun2020scalability}. Many important approaches have focused on modeling the environment in conjunction with multiple agents~\citep{casas2018intentnet, cui2019multimodal,salzmann2020trajectron++}; our framework only focuses on multi-agent interactions.
More recent advancements have seen the rise of transformer-based methods in trajectory prediction~\citep{ngiam2021scene,yuan2021agentformer}.
In particular, MotionLM~\citep{seff2023motionlm} forecasts multi-agent trajectories by encoding motion in discrete acceleration tokens and passing these tokens through a transformer decoder that cross-attends to the Wayformer~\citep{nayakanti2023wayformer} scene encoder. When applying the PAR framework to trajectory prediction, we use acceleration tokens to discretize car motion.


\medskip
\noindent
\textbf{6D pose estimation and hand-object interaction.}
6D pose estimation from monocular camera images has been extensively studied~\citep{xiang2017posecnn, li2018deepim, trabelsi2021pose, wang2021gdr}. Additionally, a related area of research known as 6D object pose tracking leverages temporal cues to improve the accuracy of 6D pose estimation in video sequences~\citep{wen2020se, deng2021poserbpf, wen2023bundlesdf, wen2024foundationpose}. There is also significant interest in learning state and action information of hands and objects through hand-object interaction data, sourced from both curated and in-the-wild video data~\citep{wu2024reconstructing}. Of particular relevance to 6D pose estimation is the DexYCB dataset~\citep{chao2021dexycb}, which contains 1000 videos of human subjects interacting with 20 objects on a table with randomized tabletop arrangements and 6D object poses. For the third case study in this paper, we propose using the PAR framework to model hand-object interactions, demonstrating that incorporating the hand as an agent provides a useful prior for enhancing object rotation and translation predictions compared to AR modeling.


\section{Additional PAR Framework Details}
\label{sec:appendix_impl_details}

\begin{figure*}
    \centering
    \includegraphics[width=.95\textwidth]{fig/architecture.pdf}
    \caption{
    \textbf{Architecture} Top: \textit{PAR training with teacher forcing}. Here, we see that the tokens are input to the model and projected to a token embedding dimension of size $d_h=128$, where embeddings such as the agent ID embedding can be summed. Then, the transformer output is sampled (discrete tokens; left) or deprojected (continous tokens; right) to produce a predicted token. At inference, the same embeddings are summed and the same conversion from transformer output to token space occurs. Bottom left: \textit{PAR on discrete tokens}. In this case, the token in question is an integer index into a codebook and the standard transformer emebedding layer is used to project to $d_h$. Here we show the agent ID embedding and our Location Positional Encoding summed to the embedded token. After being passed to the transformer, the output is a distribution of logits, from which a token can be sampled---we use argmax in our experiments. In the discrete case, the token is converted back to the actual modality via a detokenizing step. Bottom right: \textit{PAR on continuous tokens}. Here, the token is projected to $d_h$ using a learned projection layer. The output from the transformer is of size $d_h$, and we have a trained deprojection layer to project back from $d_h$ to the token dimension. We do not add any operations after the deprojection to constrain predicted token values.
    }
    \label{fig:arch}
    \vspace{-.5cm}
\end{figure*}

\subsection{Implementation details}
\label{sec:par_impl_details}
\medskip \noindent \textbf{Token embeddings and loss.} For discrete tokens, we use a standard learned embedding layer to convert the tokens to the hidden dimension $d_{h}$ of the model. To compute the loss, we use a classification loss between the predicted distribution (output logits) and the input ground truth tokens. For continuous inputs with dimension $d$, we learn a linear layer to project from $d$ to $d_h$, and a second un-projection layer to project from $d_h$ back to $d$. To compute the loss, we take the last hidden state of the model, un-project it back to $d$, and then compute a regression loss in the original token space.

\medskip \noindent \textbf{Next-timestep prediction.} 
In standard autoregressive models (such as our single-agent model in section~\ref{sec:framework}) the next token prediction objective is enforced by computing the loss on an input and predicted target that are both shifted by one. Now, we will instead shift both by $N$, so that for a given token, the model operating on our flattened sequence of $N*T$ tokens predicts a token corresponding to the next timestep but the same agent.

\medskip \noindent \textbf{Inference.}
For a single-agent model, starting with an initial sequence history of $h$ tokens, we feed these into the model to get the next token, which we then append to our sequence to form a new sequence of $h+1$ tokens. We repeat this process to generate arbitrarily long sequences.

For our multi-agent model, we start with a ground-truth history of $h$ timesteps, which corresponds to $h*N$ tokens, including the ego agent, agent $N$. Inputting this to the model results in the last output token being our ego agent at timestep $h+1$. Then, to predict the next timestep $h+2$, we concatenate to the ground truth $h*N$ tokens the ground truth of agents $1:N-1$ at timestep $h+1$ and our prediction of the ego agent at timestep $h+1$, and we repeat this process.

For a multiagent next-token prediction ablation, to predict the ego agent at timestep $h+1$, we feed in the ground truth of agents $1:N-1$ at $h+1$ to our model to predict our ego agent, agent $N$, at timestep $h+1$. We continue this process of giving our model the ground truth tokens of agents $1:N-1$ to predict agent $N$ at each timestep. This means that this ablation necessarily has more information available at inference time.

\subsection{Architecture Details}

A detailed diagram of our architecture can be seen in Fig.~\ref{fig:arch}, where we show the overall PAR method during training with teacher forcing, and in-depth details of how discrete and continuous tokens are processed.


\section{Additional Experimental Results}

\subsection{Additional Results on AVA Action Forecasting}

We see the results of our 1-agent AR and 2-agent PAR methods on the AVA 1-person classes in Fig.~\ref{fig:ava_1_person_classes}. On the vast majority of these classes, our 2-agent PAR method is still stronger than 1-agent AR. This is likely because there are many actions that people carry out together, whether it be 2 people both \textit{dancing} (+2.0), \textit{walking} together (+11.3), \textit{watching TV} (+1.7), or \textit{listening to music} (+5.4). We show a qualitative example of some of these single-agent action classes in Fig.~\ref{fig:ava_qual_1_person}, where we see that the PAR approach helps the model make better predictions.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{fig/AVA_supp_coocurring.pdf}
    \caption{\textbf{Qualitative examples of action prediction on single-person actions}. While \textit{dance}, \textit{ride} and \textit{sit} are not multi-person actions, our method is able to predict them more accurately in these examples (and overall by margins of $+2.0$, $+1.0$ and $+2.7$ mAP points respectively, see Fig.~\ref{fig:ava_1_person_classes}). This is likely because these actions are co-occuring between two agents who are both partaking in the activity in question, and our 2-agent PAR model is able to reason over this. Note that in the horse-riding example, the second agent is present in the history, but the tracking failed and they are not present at the timesteps we are predicting (see full history and predictions in supplementary video). This context in the history is sufficient to help  our PAR model make better predictions.}
\label{fig:ava_qual_1_person}
\end{figure}


\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{fig/other_classes_comparison.pdf}
    \caption{\textbf{Per-class mAP on AVA single-person actions}. On these actions, our PAR method is still stronger for the majority of action classes as compared to single-agent AR. For instance, we get an absolute $11.3$ mAP gain on walking - people often walk in groups, so it makes sense that this action would benefit from our PAR method. }
\label{fig:ava_1_person_classes}
\end{figure*}


\medskip \noindent \textbf{Evaluation dataset} The AVA test set annotations are not released. Since we are focused on action forecasting from ground-truth past annotations instead of predicting actions from video frames, we evaluate on the validation set.

\subsection{Additional Results on Object Pose Estimation}\label{sec:app_ho_qual}

See Figures~\ref{fig:ho_qual_supp} and~\ref{fig:ho_qual_supp2} for more qualitative results on rotation and translation predictions, respectively. 

Additionally, we perform an ablation on the effect of the agent ID embedding on the performance of our system for the hand-object interaction. The results are presented on the test split of the dataset, using the best checkpoint selected based on performance on the validation split after 500 epochs of training (to convergence) with agent ID embeddings. As seen in Table~\ref{tab:ho_par_ablation}, removing the agent ID embedding for the 2-agent PAR has a more significant effect on the rotation estimation than the translation estimation. This may be attributed to the fact that in the translation estimation, both the hand and the object tokens are represented as 3D translations, but in the case of the rotation estimation, the hand token is represented as a 3D translation, while the object token is represented as a quaternion. Thus, having the agent ID embedding is helpful for tokens that measure different types of quantities. 

Please note that we do not include an ablation on the next timestep prediction, because that would entail inputting the hand token and predicting the object token. Shifting by 1 instead of by 2 (our number of agents -- see Figure~\ref{fig:PAR_training} and Section~\ref{sec:par_impl_details}) would entail computing a loss on tokens of different dimensions which is not possible. The next-timestep prediction component of the PAR framework is necessary for a task such as this one with multiple data modalities.

\begin{table}
% 
\centering
\setlength{\tabcolsep}{2pt}
\begin{tabular}{@{}lccccc@{}}
\toprule
Type & Method & Ag-ID embd & MSE ($m^2$) $\downarrow$ & GEO ($rad$) $\downarrow$           \\
\midrule
Transl  & 1-ag AR & N/A   & 3.68 $\times 10^{-3}$ & - \\
Transl   & 2-ag PAR  & \xmark & 2.26 $\times 10^{-3}$ & -        \\
Transl   & 2-ag PAR  & \cmark & \textbf{2.17} $\boldsymbol{\times 10^{-3}}$ & -        \\
Rot  & 1-ag AR & N/A & - & 0.919 \\
Rot  & 2-ag PAR & \xmark & - & 0.895 \\
Rot  & 2-ag PAR & \cmark & - & \textbf{0.837} \\
\bottomrule
\end{tabular}
\caption{\textbf{Object pose estimation PAR ablation.} All results are on the test split of the dataset. We see that for the case of object pose forecasting, using the agent ID embedding helps improve the performance on both translation and rotation prediction.}
\label{tab:ho_par_ablation}
\end{table}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{fig/new_128dim_rot_supp.pdf}
    \caption{
    % We present a qualitative result from the validation split for the rotation prediction task.
    \textbf{Rotation prediction qualitative results.}
    We show results from two videos. The projected 3D model in blue has the ground-truth translation for visualization purposes and our predicted rotation. In the top row (AR), the results depict the object of interest as the sole agent, while the bottom row (2-agent PAR) demonstrates improved performance by incorporating the human hand as a second agent in the grasping interaction.}
    \label{fig:ho_qual_supp}
    \vspace{-.5cm}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{fig/new_128dim_tx_supp.pdf}
    \caption{
    % We present a qualitative result from the validation split for the rotation prediction task.
    \textbf{Translation prediction qualitative result.}
    We show results from two videos. The projected 3D model in blue has the ground-truth rotation for visualization purposes and our predicted translation. In the top row (AR), the results depict the object of interest as the sole agent, while the bottom row (2-agent PAR) demonstrates improved performance by incorporating the human hand as a second agent in the grasping interaction.}
    \label{fig:ho_qual_supp2}
    \vspace{-.5cm}
\end{figure*}

% \begin{figure*}
%     \centering
%     \includegraphics[width=\textwidth]{fig/rot_supp.png}
%     \caption{
%     % We present a qualitative result from the validation split for the rotation prediction task.
%     \textbf{Rotation prediction qualitative result.}
%     The projected 3D model in blue has the ground-truth translation for visualization purposes and our predicted rotation. In the top row (AR), the results depict the object of interest as the sole agent, while the bottom row (2-agent PAR) demonstrates improved performance by incorporating the human hand as a second agent in the grasping interaction.}
%     \label{fig:ho_qual_supp}
%     \vspace{-.5cm}
% \end{figure*}

% \begin{figure*}
%     \centering
%     \includegraphics[width=\textwidth]{fig/rot_supp2.png}
%     \caption{
%     % We present a qualitative result from the validation split for the rotation prediction task.
%     \textbf{Rotation prediction qualitative result.}
%     The projected 3D model in blue has the ground-truth translation for visualization purposes and our predicted rotation. In the top row (AR), the results depict the object of interest as the sole agent, while the bottom row (2-agent PAR) demonstrates improved performance by incorporating the human hand as a second agent in the grasping interaction.}
%     \label{fig:ho_qual_supp2}
%     \vspace{-.5cm}
% \end{figure*}

% \begin{figure*}
%     \centering
%     \includegraphics[width=\textwidth]{fig/tx_supp.png}
%     \caption{
%     % We present a qualitative result from the validation split for the rotation prediction task.
%     \textbf{Translation prediction qualitative result.}
%     The projected 3D model in blue has the ground-truth rotation for visualization purposes and our predicted translation. In the top row (AR), the results depict the object of interest as the sole agent, while the bottom row (2-agent PAR) demonstrates improved performance by incorporating the human hand as a second agent in the grasping interaction.}
%     \label{fig:ho_qual_supp3}
%     \vspace{-.5cm}
% \end{figure*}

% \begin{figure*}
%     \centering
%     \includegraphics[width=\textwidth]{fig/tx_supp2.png}
%     \caption{
%     % We present a qualitative result from the validation split for the rotation prediction task.
%     \textbf{Translation prediction qualitative result.}
%     The projected 3D model in blue has the ground-truth rotation for visualization purposes and our predicted translation. In the top row (AR), the results depict the object of interest as the sole agent, while the bottom row (2-agent PAR) demonstrates improved performance by incorporating the human hand as a second agent in the grasping interaction.}
%     \label{fig:ho_qual_supp4}
%     \vspace{-.5cm}
% \end{figure*}


\begin{table}
% 
\centering
\begin{tabular}{@{}lccccc@{}}
\toprule
Method & Timestep pred & Ag-ID embd & ADE & FDE $\uparrow$           \\
\midrule
1-ag AR   & N/A  & N/A   & 1.44 & 3.57\\
3-ag AR   & \xmark  & \xmark & 1.36 & 3.37        \\
3-ag PAR*  & \xmark     & \cmark     &1.36 & 3.37           \\
3-ag PAR*   & \cmark     & \xmark   & 1.36 & 3.35         \\
3-ag PAR &\cmark  & \cmark & \textbf{1.35}    & \textbf{3.34}   \\
\bottomrule
\end{tabular}
\caption{\textbf{Car trajectory prediction PAR ablation.} All results use acceleration tokens, and the 3-agent methods use the location positional encoding.}
\label{tab:cars_par_ablation}
\end{table}

\setlength{\tabcolsep}{10pt}
\renewcommand{\arraystretch}{0.9} % Default value: 1
\begin{table}
% 
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
LPE & Method &  ADE $\downarrow$ & FDE $\downarrow$           \\
\midrule
 \xmark & 3-agent PAR  & 1.40    & 3.44    \\
 \xmark & 10-agent PAR  & 1.39    & 3.43    \\
 \cmark& 3-agent PAR   & \textbf{1.35} & \textbf{3.34} \\
\cmark& 10-agent PAR   & \textbf{1.35} & \textbf{3.35} \\
\bottomrule
\end{tabular}
\caption{\textbf{Car trajectory prediction with 3 vs 10 agents.} All results use acceleration tokens.}
\label{tab:cars_10_agent}
\end{table}


\subsection{Additional Results on Car Trajectory Prediction}

We conduct an ablation on our the agent ID embedding and next timestep prediction for the 3-agent PAR model in table~\ref{tab:cars_par_ablation}. We see that in our cars case study, our 3-agent PAR model slightly outperforms the 3-agent models without the next timestep prediction and agent ID embedding. Note that not using next timestep prediction actually results in the model having more information at inference time (see the last paragraph of Sec.~\ref{sec:par_impl_details}), so this combined with nuScenes being a relatively simple dataset, and the small acceleration-based motion token vocabulary could explain why the results are comparable.

\subsection{Results with more than Three Agents}

We experiment with using 10-agent PAR instead of 3-agent PAR for car trajectory prediction on nuScenes, Table~\ref{tab:cars_10_agent}. We see that our model has similar performance across both numbers of agents. While PAR using our small model can still learn and reason through the increased complexity of 10 agents, the increased agents do not help. We hypothesize that for cars driving on the road, there are two most influential agents: the car directly in front and the one in the adjacent lane, especially during lane changes. Therefore, we hypothesize that beyond two neighboring agents, other agents add limited value, especially on a simple dataset such as nuScenes, which is supported by these results.

Since AVA scenes often involve at most two people and DexYCB inherently includes only one hand and one object, we only go beyond two agents on nuScenes. However, PARâ€™s ability to handle more than 3 agents could be useful in tasks with complex group interactions, such as team sports like basketball, where many agents play key roles simultaneously.

\section{Additional Case Study Implementation Details}

We stabilize learning by using Exponential Moving Average (EMA) for training our experiments with a decay rate of $0.999$ for action prediction and object translation/rotation estimation, and $0.9999$ for car trajectory prediction.

\subsection{Car Trajectory Prediction}
\label{sec:appendix_car_impl_details}

\medskip \noindent \textbf{Tokenization} Instead of discretizing the $xy$ position space, we discretize the motion, resulting in discrete velocity or acceleration tokens computed as follows. We take each agents ground truth trajectory (past and future), shift it so that the trajectory starts at $x, y = 0, 0$, and then rotate the trajectory such that its initial heading at $t=0$ is $0$ radians. We divide velocity space into 128 even bins in $[-18, 18]$ meters. We then, separately for $x$ and $y$, take the difference between each pair of coordinates in the trajectory, to get a length $T-1$ sequence of deltas. Each of these deltas is mapped to a bin index. 

We first experimented with velocity tokens, taking the Cartesian product of bin space to give each $xy$-delta one single integer index between $1$ and $128*128 = 16384$. To get acceleration tokens, we take the difference between each $x$ delta and $y$ delta, and bin these differences into $13$ bins. We then take the Cartesian product of bin space to get a vocabulary between $1$ and $13*13=169$.

\medskip \noindent \textbf{Location Positional Encoding (LPE)} 
We implement our location positional encoding as follows. 

We first compute relative location to the agent we are predcting (the ``ego" agent) at the first timestep of the history. The ego agent trajectory is shifted to be at location $(0,0)$ at time $t=0$, and all other agents are shifted to be relative to the ego agents position. We also rotate the ego agent trajectory to have a heading of 0, and rotate all other agents trajectories relative to this ego agent trajectory. 

We normalize these relative locations (in meters) to be between 0 and 1. We then quantize these normalized locations to be an integer between 0 and 100. We next pass these locations (x and y separately) into a sin-cos positional encoding. Instead of operating on sequence position indices, the positional encoding operates on the quantized locations. We compute separate positional encodings for x and y. The encoding dimensions is half of the hidden dimension, and we concatenate the x and y encodings to get one encoding. We then sum the result of this encoding to the model inputs at training for the full trajectory (history and future).

At inference, we compute this encoding on the full trajectory (history and future) for agents 1 to N-1, but for our ego agent, we only use the history location ground truth. To get the future locations, at each sampling step, we integrate over our velocity or acceleration token to update the predicted location one step at a time, and then pass that location into our encoding.

\medskip \noindent \textbf{Evaluation dataset} Since the nuScenes test set can only be evaluated by submitting to the leaderboard, but we are interested in demonstrating the effectiveness of PAR over AR, we evaluate on the nuScenes validation set.  

\subsection{Object Pose Estimation}
As mentioned in the main text, we use relative translations with respect to the hand location for computing the PAR results. In translation prediction, the relative translation is defined with respect to the hand's position at the current timestep. This means the hand token is always treated as the origin (a zero vector) at each timestep, while the object translation corresponds to the difference between the current positions of the hand and the object.

For rotation prediction, however, we cannot compute the object rotation relative to the hand because our dataset does not provide hand rotation information. Instead, for the hand token, we compute the relative translation with respect to the hand's position in the first frame of the sequence. This ensures the hand location is only treated as the origin in the first timestep, enabling the hand's motion to influence the rotation prediction throughout the sequence.

For training both prediction tasks, we stabilize learning by employing Exponential Moving Average (EMA); rotation prediction uses a decay rate of 0.999, while translation prediction uses a decay rate of 0.99. Both prediction tasks use the AdamW optimizer with learning rate of $10^{-4}$.

% Additionally, to improve robustness against compounding errors during inference, we introduce a noising schedule in translation prediction training. Specifically, we add a constant perturbation to the teacher-forced inputs, sampled from a normal distribution with a mean of 0 and a standard deviation of 0.07. The standard deviation was determined by calculating the training error's standard deviation over the first few epochs and taking the square root of this value. A mean of 0 was chosen to ensure that the perturbations remain centered and do not introduce significant bias. 

% For rotation training, we further stabilize learning by employing Exponential Moving Average (EMA) with a decay rate of 0.999.
% As mentioned in the main text, we use relative translations with respect to the hand location for the PAR results. In the translation prediction, the relative translation is taken with respect to the hand position in the current timestep, which means the hand token is always the origin (zero vector) at every timestep, while the object translation is between the current hand and object position difference. However, for the rotation prediction, we cannot take the object rotation with respect to the hand, because we are not provided with the hand rotation in our dataset. Thus, for the hand token, we take the relative translation with respect to the hand location in the first frame of the sequence. This means that the hand location is the origin only in the first timestep. This ensures that the hand signal influences the rotation. 

% Furthermore, for the translation prediction we use a noising schedule in the training to make it more robust to compounding errors at inference. In practice this means that we apply a constant perturbation to the teacher forced inputs, sampled from a normal distribution with mean 0 and standard deviation of 0.07. Additionally, for the rotation training we use EMA with decay of 0.999.

\section{Supplementary Video}

Our supplementary video contains the full video for all qualitative results shown in this paper, and additional qualitative results. No temporal smoothing is applied, nor are any other modifications made to the results shown in the videos.