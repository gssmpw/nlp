\section{Case Study 3: Object Pose Forecasting During Hand-Object Interaction}

Our final case study explores how hand-object interaction can be leveraged for object pose estimation. We define the human hand and the interacting object as two agents, with tokens representing distinct state types. We show that our PAR framework can jointly model these agents, improving 3D translation and rotation predictions for the object. 

\begin{figure}
    \centering
    % \includegraphics[width=\linewidth]{fig/rotation_result_2_bigger_10sr_cropped_remove_whitespace.png}
    \includegraphics[width=\linewidth]{fig/new_128dim_rot.pdf}
    \vspace{-20pt}
    \caption{
    % We present a qualitative result from the validation split for the rotation prediction task.
    \textbf{Rotation forecasting qualitative result on test set.} 3D predictions are projected onto the image, isolating rotation results by showing the ground-truth translation. Incorporating the hand agent in the PAR framework (right) improves object pose prediction over object-only AR (left). %Last frame of sequence shown.
    % The projected 3D model in blue has the ground-truth translation for visualization purposes and our predicted rotation. [say how much history there was and how far into the future]
    % To account for the low dynamics between consecutive frames, we sample every 10th frame. Left (AR), the results depict the object of interest as the sole agent, while the right (2-agent PAR) demonstrates improved performance by incorporating the human hand as a second agent in the grasping interaction. \shiry{say which frame in the future are you showing}
    }
    \label{fig:ho_qual}
    % \vspace{-.5cm}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{fig/new_128dim_tx.pdf}
    \vspace{-20pt}
    \caption{
    \textbf{Translation forecasting qualitative result on test set.} 3D predictions are projected onto the image, isolating translation results by showing the ground-truth rotation. Using the PAR framework (right) instead of AR (left) improves object pose prediction.
    }
    \label{fig:ho_qual2}
    \vspace{-.5cm}
\end{figure}

\subsection{Experimental Setup}
\noindent \textbf{Dataset.} We use the DexYCB dataset, which includes 1000 videos of 10 subjects performing object manipulation tasks with 20 distinct objects from the YCB-Video dataset. The data is split into 800 training, 40 validation, and 160 testing videos. We use one of 8 provided camera views. In each trial, subjects pick up and lift objects in randomized conditions. Labels include the object's SO(3) rotation and 3D translation, and the hand's 3D translation. We focus on predicting the object's rotation or translation.
% For this case study, we utilize the DexYCB dataset~\cite{chao2021dexycb}, which contains 1000 videos of 10 human subjects performing object manipulation tasks. Each subject picks up 20 distinct objects from the YCB-Video dataset~\cite{xiang2017posecnn}, with multiple trials conducted for each object. The dataset is divided into 800 training videos, 40 validation videos, and 160 testing videos. Although the videos are recorded from 8 RGB-D cameras, we work with a single camera view. In each trial, the subject starts in a relaxed pose with their hand by their side (often out of the camera’s view), grasps the target object, and lifts it into the air. For each subject-object pair, there are 5 trials where the object’s rotation, placement, and surrounding distractor objects are randomized. The dataset provides labels such as the object's SO(3) rotation and 3D translation, and the 3D positions of 21 hand joints in camera space. We focus on predicting either the object's rotation or translation as it is being picked up in each video.

\medskip \noindent \textbf{Task-specific considerations.} We tokenize object information in object-only experiments and both object and hand information in hand-object experiments. The object is represented as a 4D token for rotation forecasting (quaternion from SO(3) rotation) or a 3D token for translation forecasting (Euclidean coordinates). In hand-object experiments, the hand token is included with a 3D translation vector, and agent ID embeddings distinguish between the hand and object. 
Normalization is applied to all 3D translation vectors in both AR and PAR experiments; quaternions are normalized by definition and require no additional processing. 
An embedding layer projects the tokens into the transformer's hidden dimension, and another layer projects them back for prediction. 

For rotation-only forecasting, the loss is \(\mathscr{L}_{rot} = 1 - |\hat{q} \cdot q|\), where \(\hat{q}\) is the predicted quaternion and \(q\) the ground-truth quaternion. For translation-only forecasting, the loss \(\mathscr{L}_t\) is the mean squared error (MSE) between predicted and ground-truth translations. For PAR we predict relative object-to-hand translations at each frame, using the current hand position as origin, while for AR, we predict absolute object translations without considering the interacting agent. 
% For PAR, we treat the current hand translation as the origin, and predict the relative translation between the object and hand at each frame, while the hand is relative to itself at every frame. This is in contrast to AR, where we predict the absolute object translation since the interacting agent is not incorporated. 
For PAR models, we add the loss \(\mathscr{L}_h\), a MSE on hand translation. The object-only AR rotation model is optimized with \(\mathscr{L}_{rot}\), while the PAR rotation model combines \(\mathscr{L}_{rot} + \mathscr{L}_h\); similarly, the object-only translation model is trained with \(\mathscr{L}_t\), and the hand-object translation model uses \(\mathscr{L}_t + \mathscr{L}_h\). 
At inference, the first half of each video is provided, and object predictions are autoregressively generated for the second half. Translation is evaluated using MSE, while rotation is measured using geodesic distance (GEO) on SO(3).


% \medskip \noindent \textbf{Tokenization} For object-only experiments, we tokenize only the object information, while in hand-object experiments, we tokenize both the object and hand information.

% The object is represented differently depending on the task: for rotation-only prediction, we use 4-dimensional tokens derived from the quaternion representing its SO(3) rotation, while for translation-only prediction, we use 3-dimensional tokens representing the object's Euclidean coordinates. In hand-object experiments, where the hand is treated as a second agent interacting with the object, the hand is represented by a 63-dimensional vector corresponding to the Euclidean coordinates of 21 hand joints (3 joints and 1 fingertip per finger, plus 1 wrist joint). In hand-object interaction models, we also incorporate agent ID embeddings to distinguish between the hand and object.

% We use an embedding layer to project the token(s) into the transformer's hidden dimension and another to project them back into the token space for loss computation and generation. During training, we apply teacher forcing to the tokenized hand and object data. In validation, we teacher force the hand joint information while generating the object’s rotation or translation.

% %Object pose is represented as a 7D vector - 4D quaternion rotation and 3D XYZ translation. Hand pose is represented as a 63D vector of 21 XYZ joints. 

% %We learn an embedding layer to project the token into the transformers hidden dimension and another one to unproject back into the token dimension for loss computation and generation. \neerja{Since we are doing rotation and tx separately we should mention this here that we have different cases}

% %We explicitly require rotations to be valid quaternions.

% %\neerja{Explain the cases of object only, hand and object as separate tokens, and this 1.5 agent displacement thing. Currently we do loss on hand and object (but teacher force hand at inference), lay that out here.}

% \medskip \noindent \textbf{Loss} 
% For rotation-only prediction, the loss we optimize is:
% \[
%     \mathscr{L}_{rot} = 1 - |\hat{q} \cdot q|,
% \]
% where \(\hat{q}\) is the predicted quaternion representing the object's rotation in camera space, and \(q\) is the ground-truth quaternion. We apply the absolute value to account for the double-covering of quaternions in SO(3), i.e., \(q = -q\). Additionally, we ensure that the quaternions predicted by the de-projection layer are valid, meaning they have unit norm and a positive scalar component. 

% For translation-only prediction, the loss function \(\mathscr{L}_t\) is the mean squared error (MSE) between the predicted and ground-truth translations of the object. In experiments involving the hand, we also optimize for \(\mathscr{L}_h\), the MSE loss on the predicted and ground-truth hand joint positions. Additionally, we normalize the hand joints and object translations to be between 0 and 1 during training.

% The object-only rotation model is optimized with \(\mathscr{L}_{rot}\), while the hand-object rotation model uses the combined loss \(\alpha \mathscr{L}_{rot} + (1-\alpha) \mathscr{L}_h\), where $\alpha=0.33$. Similarly, the object-only translation model is trained with \(\mathscr{L}_t\), and the hand-object translation model is optimized with \(\mathscr{L}_t + \mathcal{L}_h\), where the two losses are equally weighted since they represent the same type of measurement.

% % In hand-object interaction models, we also incorporate agent ID embeddings to distinguish between the hand and object.

% \medskip

% \noindent \textbf{Metrics} During validation, we provide the first half of each video and autoregressively generate object predictions for the second half. For object translation, we evaluate performance using the MSE metric, while for object rotation, we measure error using the geodesic distance (GEO) on SO(3), which is the shortest path in radians between the predicted and ground-truth rotations. We convert the quaternions to SO(3) matrices to compute the GEO metric.

% % We use a MSE regression loss and train the model with teacher forcing - at each timestep, the previous ground-truth tokens are used in prediction. 

% % \neerja{Describe whatever metrics we settle on here - currently for rotation I think single shortest path between GT and pred rotation on the surface of the unit sphere in 3D rotation space, and MSE for translation}

% \newpage
\subsection{Results}
We compare the object-only AR models to the hand-object PAR models in Table~\ref{tab:ho_res} for the two prediction tasks. We also present the baselines described in Sec.~\ref{sec:task-specific} in Table~\ref{tab:ho_baselines}. Figures~\ref{fig:ho_qual} and~\ref{fig:ho_qual2} show qualitative results on the rotation and translation predictions, respectively. In both prediction tasks, we observe that incorporating the human hand's interaction with the object enhances accuracy: for rotation, PAR results in a relative improvement of $8.9\%$ over AR, and for translation, $41\%$. See Section~\ref{sec:app_ho_qual} for additional qualitative results with more sampled frames.
% In Figure~\ref{fig:ho_qual}, we see that the AR model (top row) achieves high-fidelity predictions early on, when much of its history still relies on ground truth data from the first half of the sequence. However, as the video progresses and the history becomes increasingly dependent on predicted object rotations, the AR model’s performance rapidly deteriorates. In contrast, our PAR model (bottom row) reasons over the 3D hand joint positions to predict the object's SO(3) rotation much more accurately. 
% Please see~\ref{app:pose} for more results on pose estimation.

\begin{table}
% \centering \footnotesize
\begin{tabular}{@{}lccc@{}}
\toprule
Type & Method &  MSE $\downarrow$ & GEO ($rad$) $\downarrow$ \\
% Type & Method &  MSE ($\times$10$^{-3}m^2$) $\downarrow$ & GEO ($rad$) $\downarrow$ \\
\midrule
Translation & 1-agent AR & 3.68 $\times$ 10$^{-3}$ & - \\
% Translation & 2-agent PAR & \textbf{3.28} & - \\
Translation & 2-agent PAR & \textbf{2.17} $\boldsymbol{\times}$ \textbf{10}$^{\boldsymbol{-3}}$ & - \\
\midrule
Rotation & 1-agent AR & - &  0.919 \\
Rotation & 2-agent PAR & - &  \textbf{0.837} \\
\bottomrule
\end{tabular}
\caption{\textbf{Test set results on DexYCB dataset.} For both rotation and translation forecasting, the 2-agent PAR model, which treats the hand as an additional agent, improves results.}
\label{tab:ho_res}
\end{table}

% \begin{table*}
% \centering \footnotesize
% \begin{tabular}{@{}lcccccc@{}}
% \toprule
% Type & Object Token & Hand Token & Ag ID Emb & Agents &  MSE ($m^2$) $\downarrow$ & GEO ($rad$) $\downarrow$           \\
% \midrule
% Translation & \cmark  & \xmark & \xmark & 1 & 2.97$\times$ 10$^{-2}$ & - \\
% Translation &  \cmark  & \cmark & \cmark & 2 & \textbf{1.90} $\boldsymbol{\times}$ \textbf{10}$^{\boldsymbol{-3}}$ & - \\
% \midrule
% Rotation & \cmark  & \xmark & \xmark & 1 & - &  0.944 \\
% Rotation & \cmark  & \cmark & \cmark & 2 & - &  \textbf{0.890} \\
% \bottomrule
% \end{tabular}
% \caption{\textbf{Test set results on DexYCB dataset.} Top two rows: translation prediction, bottom two rows: rotation prediction. In both cases, the 2-agent PAR model, which accounts hand-object interaction by integrating the hand as an additional agent, yields improved results.}
% \label{tab:ho_res}
% \end{table*}


\begin{table}
\centering \footnotesize
\begin{tabular}{@{}lcc@{}}
\toprule
Baseline &  Translation - MSE ($m^2$) $\downarrow$ & Rotation - GEO ($rad$) $\downarrow$           \\
\midrule
% Random  & 0.327 & 2.147 \\
% Random Trajectory & 1.37$\times$ 10$^{-2}$ & 2.189 \\
% NN  & 1.54$\times$ 10$^{-2}$ &  2.077\\
% Multiagent NN  & 1.31$\times$ 10$^{-2}$ & 2.268  \\
% Mirror  & 1.20$\times$ 10$^{-2}$  & N/A  \\
Random  & 0.244 & 2.196 \\
Random Trajectory & 1.60$\times$ 10$^{-2}$ & 2.146 \\
NN  & 1.69$\times$ 10$^{-2}$ &  2.179 \\
Multiagent NN  & 1.71$\times$ 10$^{-2}$ & 2.170 \\
Mirror  & 1.20$\times$ 10$^{-2}$  & -  \\
\bottomrule
\end{tabular}
\caption{\textbf{Test set results for DexYCB baselines.} We cannot provide rotation results for the Mirror baseline, because the ground-truth does not include hand rotation, only 3D translation.}
\label{tab:ho_baselines}
\end{table}








