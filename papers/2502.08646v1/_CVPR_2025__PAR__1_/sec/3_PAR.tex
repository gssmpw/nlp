
\begin{figure*}
\centering
    \includegraphics[width=.99\textwidth]{fig/par_method.pdf}  
    \caption{\textbf{The PAR Framework}. We begin by collecting a video dataset, such as AVA (top) or DexYCB (bottom). Then, using dataset labels or computer vision techniques, a trajectory of a given modality for our prediction task is extracted for each agent, such as action class labels (top) or object pose and 3D hand translation (bottom). Data is then tokenized, either through discretization or directly using continuous values, with our framework supporting both formats. Based on the tokenization and prediction task, we choose the appropriate loss function for PAR training. After training with PAR, predicted tokens can be decoded back to data space and evaluated with relevant metrics.}
    \label{fig:PAR_pipeline}
\vspace{-1em}
\end{figure*}

\section{Poly-Autoregressive Modeling}

Our goal is to model the behavior of an agent or entity while taking into account any other agents it interacts  with, if any. To evaluate the performance of our model in capturing interaction dynamics, we predict the agent's future behavior and compare it against ground-truth data. 

We define the following task: \textit{In an interaction comprised of $N$ agents, given the observed past states of the $N-1$ interacting agents, and the observed or previously-predicted past states of the $N^{\text{th}}$ ego agent, predict the future states of the $N^{\text{th}}$  ego agent.}

We define a transformer-based poly-autoregressive (PAR) predictor, $\mathcal{P}$, that learns to model temporally long-range interactions in the input sequence. The inputs to the predictor are the past states of the $N$ interacting agents, and its output is the predicted future state of the $N^{\text{th}}$ ego agent.

\subsection{Problem Definition}
\label{sec:prob_def}

Let $\mathbf{S}=\{\mathbf{s}_i\}_{i=1}^T$ be a temporal sequence of agent states, $\mathbf{s}_i$.
We use $\mathbf{S}^N$ and $\mathbf{S}^{1:N-1}$ to denote the temporal sequences of states of the $N_{th}$ agent and of the other $N-1$ agents, respectively.
For each timestep $t \in [t_\pi,T]$, where $t_\pi \in [1,T]$ is the time we start predicting,
we take as input all other $N-1$ agents' past observed state sequences
$\mathbf{S}^{1:N-1}_{1:t-1}$
along with the $N_{th}$ agent's
past observed states up to $t_\pi$, 
$\mathbf{{S}}^N_{1:t_\pi}$,
and any of its previously predicted past states $\mathbf{\hat{S}}^N_{t_{\pi}+1:t-1}$,
if available (see Fig.~\ref{fig:polyauto}).
Our predictor, $\mathcal{P}$, then \textit{poly-autoregressively} predicts the $N_{th}$ agent's future states one time-step at a time:
\begin{equation}
       \mathbf{\hat{s}}^N_{t} = \mathcal{P}(\mathbf{S}^{1:N-1}_{1:t-1}, \mathbf{{S}}^N_{1:t_{\pi}},\mathbf{\hat{S}}^N_{t_\pi+1:t-1}). \\
\end{equation}
$\mathcal{P}$ learns to model the distribution over the next timestep of the $N_{th}$ agent's states, given all other agents' states:
\begin{equation}
p(\mathbf{\hat{s}}^N_{t} | \mathbf{S}^{1:N-1}_{1:t-1}, \mathbf{S}^N_{1:t-1}).
\end{equation}

While we provide the observed ground truth states of other agents at inference, during training, we jointly maximize the likelihood of all $N$ agents by computing losses on their future state predictions.

We train the predictor by maximizing the likelihood of the target state $y$ at time $t$:
\begin{equation*}
\label{eq:transformerloss}
    \mathscr{L_\mathcal{P}} = E_{y \sim p(y)}[-\log(p(\mathbf{s}^N_{t})],
\end{equation*}
where the target state $y$ at $t$ is computed from the $N_{th}$ agent ground truth future state.

\subsection{The Poly-Autoregressive Framework}
\label{sec:framework}

\begin{figure}
    \centering
    % \begin{subfigure}[b]{0.49\textwidth}
    %     \includegraphics[width=\textwidth]{fig/nexttoken_teacherforcing.pdf}
    %     \caption{AR: next-token training.}
    %     \label{fig:AR_nexttoken}
    % \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{fig/mutli-agent_next-token.pdf}
        \caption{AR: multi-agent, next-token training.}
        \label{fig:AR_multiagent_nexttoken}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{fig/next_timestep.pdf}
        \caption{PAR: same-agent, next-timestep training.}
        \label{fig:PAR_nexttimestep}
    \end{subfigure}    
    \caption{Training with teacher forcing for (a) multi-agent next-token prediction in autoregressive models and (b) multi-agent poly-autoregressive models. Solid vs striped indicates a ground-truth vs predicted token, respectively. Color denotes agent identity. The AR model is trained for next-token prediction, while the PAR model is trained to predict the next timestep of the same agent. Three agents are shown for ease of visualization, but the PAR model supports an arbitrary number of agents.}
    \label{fig:PAR_training}
\vspace{-.75cm}
\end{figure}

We address the problem of forecasting the future states of an agent (from time $t$ to $T$) in a data-driven way, given a temporal sequence of past states (from time $1$ to $t-1$). 
We assume that our agent has some feature, or a set of features, of interest in a video (e.g., 3D pose) that we can tokenize. We predict the future states of the agent in terms of this tokenized feature (or set of), where we use one token (or set of tokens) per time step. The predicted tokens can be discrete (i.e., an index into a feature codebook) or continuous (i.e., a vector of one or more continuous values). The loss $\ell$ will depend on the problem's specifics and the type of token used. To train the model to predict the future, we rely on all the interaction dynamics of length $T$ in our training dataset as ground truth examples.


As a baseline, we consider the \textbf{single-agent autoregressive (AR)} paradigm, where a transformer is trained to perform next-token-prediction with teacher forcing. AR uses greedy sampling to generate sequences at inference time, predicting one next token at a time (Fig.~\ref{fig:polyauto}(a)). 

In contrast, our \textbf{multi-agent poly-autoregressive (PAR)} framework considers the other $N-1$ agents in the scene when predicting the future state of the $Nth$ agent. In this setup, we tokenize the features of interest of all $N$ agents, yielding $N$ tokens at each timestep for a total of $N*T$ tokens. In practice, we operate on a flattened sequence of $N*T$ tokens. 
% In cases where we consider a multimodal set of tokens of interest, we have $T*N*K$ where $K$ is the number of token types. \neerja{The only case where we have multiple token types is hand-object, but there we treat the hand as one agent and the object as another (or equivalently: 1 agent and 2 token types), but regardless $N*K=2$, so we haven't shown $N>1$ and $K>1$} \shiry{ok then we keep this for future ;-) }
Instead of using the AR training procedure in this multi-agent case (as in Fig.~\ref{fig:AR_multiagent_nexttoken}), we jointly model the $N$ agents at each timestep by introducing the following features to our PAR framework.

\vspace{0.2cm}
\noindent \textbf{Next-timestep prediction.} 
A standard AR model predicts the next token. Given the flattened sequence of $N*T$ tokens our model operates on, next token prediction would take as input an agent $k$ at timestep $t$ and predict agent $k+1$'s state at the same timestep $t$ (as in Fig.~\ref{fig:AR_multiagent_nexttoken}). However, our goal is to predict the input agent $k$'s future state at time $t+1$. Therefore, we perform \textit{same-agent next-timestep} prediction rather than next-token prediction (see Fig.~\ref{fig:PAR_nexttimestep} for an illustration of same-agent next-timestep at training).

\medskip \noindent \textbf{Learned agent identity embedding.} When giving a model information corresponding to multiple agents, the model can benefit from knowing which token corresponds to which agent. We give the model this information with a learned agent ID embedding. 
 


 % \medskip \noindent \textbf{Multimodality} \neerja{We have multimodality in the sense of hand and object are different modalities, and we add location to accl/velocity tokens in a pos emb - we should mention that we go beyond just one single type of token per case study, but we also do things in different ways for different case studies so not sure what is the best way to mention this here}


\medskip \noindent \textbf{Joint training.} We train the model to jointly predict the future of all agents by computing a loss on the predicted tokens of all agents (Fig.~\ref{fig:PAR_nexttimestep}). Please refer to Section~\ref{sec:prob_def} for our inference paradigm.

\subsection{Task-Specific Considerations}
\label{sec:task-specific}
Our simple PAR approach unifies diverse problems under a single framework and architecture without any modifications. In order to formulate a problem as interaction-conditioned prediction, users must consider several task-specific details. Fig.~\ref{fig:PAR_pipeline} gives an overview of how the PAR framework disentangles multi-agent learning from problem-specific modeling.

\medskip \noindent \textbf{Data.} The input data source in our example tasks is always a collection of videos. From these videos, we extract various modalities relevant to the task at hand. These modalities can range from high-level features, such as action class labels, to low-level ones, such as 3D pose (Fig.~\ref{fig:PAR_pipeline} first two columns). We assume that each agent in the dataset is detected at each frame and is associated with an agent ID.
 
\medskip \noindent \textbf{Tokenization.} Our framework supports both discrete, quantized tokens and continuous vector tokens. The choice between discrete and continuous depends on the nature of the task.  
In the case of discrete tokens, we use a standard embedding layer to project to the hidden dimension. For continuous tokens, we train a projection layer to project the token into the hidden dimension of the transformer. For instance, if our continuous token is a 3D vector with an $(x,y,z)$ 3D location coordinate and our hidden dimension is $128$, our projection layer will project from $3$ to $128$ dimensions. We also train an un-projection layer that reverts the hidden dimension to the original token dimension.

\medskip \noindent \textbf{Loss.} The type of token and task-specific considerations dictate the loss function $\ell$ applied during model training. For discrete tokens, a classification loss is appropriate. For continuous tokens, we use a regression loss on the original token dimension. 
% \shiry{there is a nuance here that we did not yet have time to explore given the last-minute nature of how this came together: the discrete classification loss allows us to predict a multinomial distribution over possible future timesteps, from which we can sample. This makes the prediction non-deterministic, capturing the true variability of the response of the $N_th$ agent to their surroundings. After all, given other people's actions, our actions are still not determined 100 percent. This is the whole beauty of using autoregressive prediction for these problems, unlike the feed-forward UNets we used in e.g. speech to getsture. This is not at all explored yet in our experiments due to lack of time mostly, and is a major hole in this submission (unlike our previous works on this direction). If a reviewer is familiar with the literature, they will complain about this (I know I would). There is nothing for us to do about this now, but this is something we should be aware of.} \neerja{Adding sampling is high on my priority list!} \shiry{yes, I would reject you without ;-) }

\medskip \noindent \textbf{Baselines.}
We compare to the following baselines, where applicable on a case-by-case basis:

\noindent $\sbullet$ \textit{Random token}: pick random tokens from the best available token space and use as the prediction. 

\noindent $\sbullet$ \textit{Random trajectory}: pick at random a trajectory from the training dataset to use as the prediction. 

\noindent $\sbullet$ \textit{NN}: Given an input agent $A$'s trajectory history, find the closest trajectory to it in the training set, belonging to $A^T$. Use $A^T$'s future as the predicted future.

\noindent $\sbullet$ \textit{Multiagent NN}: In a dataset with two interacting partners $A$ and $B$, where $B$ is the ego agent, given an input agent $A$'s trajectory history, find the closest trajectory to it in the training set, belonging to $A^T$. Use $A^T$'s interaction partner's $B^T$'s future as the prediction.

% Assume two agents, $A$ and $B$ in interaction, where we are predicting agent $B$. Take the history of agent $A^V$ in the validation set and find the closest $A^T$ in the training dataset. Use the future of the corresponding interacting agent $B_T$ in the training trajectory as the predicted future for agent $B_V$.

\smallskip
\noindent $\sbullet$ \textit{Mirror}: In a dataset with two interacting partners $A$ and $B$, use the ground truth future of agent $B$ as the predicted future for agent $A$.



\subsection{Framework Implementation Details}
We keep the following implementation details constant for all case studies (see also Sec.~\ref{sec:appendix_impl_details}).

\medskip
\noindent \textbf{Learned agent ID embedding.} Our learned agent ID embedding consists of the integer agent ID mapped to a hidden dim-sized vector, and summed to the token embedding.

\medskip \noindent \textbf{Architecture.} For all case studies, we use the  Llama~\citep{touvron2023llamaopenefficientfoundation} transformer decoder architecture with $8$ layers, $8$ attention heads, and a hidden and intermediate dimension of $128$. The decoder has $\sim$4.4M learned parameters, not including learned embedding layers which add a few thousand more parameters. A rotary positional encoding~\citep{su2024roformer} is used in addition to other summed encodings (i.e. agent ID embedding, locational positional encoding in Sec.~\ref{sec:car_trajs}). We train using teacher forcing. The only hyperparameter that changes between case studies is the learning rate.
