\section{Related Work}

% 1951 Shannonâ€™s language paper proposing AR modeling~\citep{shannon1951prediction}

% Some informational aspects of visual perception~\citep{attneave1954some}

% Autoregressive models~\citep{larochelle2011neural}


% In deep learning with continuous values~\citep{gregor2014deep,theis2015generative}

% With discrete tokens using a using a
% multinomial distribution~\citep{van2016pixel}

% Massive success in language modeling 
% % \shiry{cite GPT and friends}
% GPT1:~\citep{radford2018improving}

% Then move towards multimodal - Flamingo~\citep{alayrac2022flamingo} etc


\textbf{Autoregressive models.} Autoregressive modeling has a rich history in information theory and deep learning, tracing back to Shannon's paper on language prediction~\citep{shannon1951prediction} and Attneave's study on visual perception~\citep{attneave1954some}. These foundational works laid the groundwork for modern applications in deep learning, including~\citep{larochelle2011neural}, which revisits neural autoregressive models and ~\citep{gregor2014deep, theis2015generative}, which explore continuous-valued modeling. ~\citep{van2016pixel} developed PixelRNN and PixelCNN to autoregressively generate images one pixel at a time using RNN and CNN architectures, respectively. 

\medskip
The development of the transformer model~\citep{vaswani2017attention} spurred progress in computer vision with the image transformer~\citep{parmar2018imagetransformer} and the vision transformer~\citep{dosovitskiy2020image}, and autoregressive models ~\cite{chen2020generative, rajasegaran2025empirical} and more notably in NLP, where the GPT family of models~\citep{radford2018improving, radford2019language, brown2020language} has demonstrated the power of large-scale unsupervised autoregressive pre-training. Recent research has focused on multimodal learning, exemplified by the Flamingo~\citep{alayrac2022flamingo} or LlaVa~\citep{liu2023improved} models, which combine vision and language processing capabilities, illustrating the versatility of autoregressive models across various domains in artificial intelligence. While these approaches operate on image patches and word tokens, we operate on symbolic representations extracted from in-the-wild videos showing natural interactions.  A recent approach~\citep{radosavovic2024humanoid} frames humanoid locomotion as an autoregressive next-token prediction task that operates on two types of continuous tokens: observations and actions. This approach projects continuous tokens to the hidden dimension and uses a shifted loss, similar to the next-timestep prediction proposed in our framework.
% \neerja{add more multimodal examples?}


\medskip
\noindent
\textbf{Multi-agent regressive models.} 
Several prior works addressed modeling specific multi-agent problems via regressive models as one-off case studies. We introduce the PAR framework to unify these efforts into a single cohesive framework. Many behavior prediction works focus on two agents engaging in social interaction, whether it be dyadic communication~\citep{Ng_2022_CVPR, ng2023text2listen, ng2024audio2photoreal} or social dance~\citep{siyao2024duolando,maluleke2024synergy}. These studies primarily tackle the challenge of predicting the state of an interacting partner (Person B) based on the input from Person A's state, sometimes extending predictions into the future~\citep{guo2022multi,maluleke2024synergy}. While earlier works used architectures such as variational RNNs~\citep{baruah2020multimodal}, recent works have predominantly adopted transformer architectures for social interaction modeling~\citep{guo2022multi,Ng_2022_CVPR, 10036100, ng2023text2listen, siyao2024duolando}, with some works exploring diffusion~\citep{liang2024intergen}, or diffusion with attention~\citep{ghosh2024remos}. Our PAR framework focuses on transformer models.

\medskip
Works encompassed by the PAR framework extend beyond human social interaction. Many multi-agent human or car trajectory prediction approaches use autoregressive prediction.  For instance, MotionLM~\citep{seff2023motionlm} utilizes a transformer decoder that processes multi-agent tokens, incorporating a learned agent ID embedding. This methodology informs our approach across all our case studies. \textit{Critically, in contrast to all prior multi-agent regressive works that designed solutions to address specific applications, we demonstrate that we can unify a diverse set of multi-agent regressive problems under a single PAR framework.} See Appendix Sec.~\ref{sec:case_study_related} for case-study-specific works.

% Ilija paper that uses the PAR next timestep prediction for humanoid next token pred~\citep{radosavovic2024humanoid}
% Dyadic communication - Evonne Learning to Listen~\citep{Ng_2022_CVPR}, L2L follow up~\citep{ng2023text2listen}, Audio to Photoreal~\citep{ng2024audio2photoreal}

% Vongani dance paper~\citep{maluleke2024synergy}

% Motion InterGen~\citep{liang2024intergen} diffusion based, only two people in interaction. In contrast, we can model an arbitrary number of people. \shiry{this one is diffusion, not AR - shiry check. Can mention but just to differentiate.}

% \shiry{Need to go over the related work from Vongani's paper to make sure we aren't missing anything that is not specifically dance... like this \url{https://arxiv.org/abs/2403.18811}}

% papers in Lea table:

% A Multimodal Predictive Agent Model for Human Interaction Generation (CVPR-W 2020)~\citep{baruah2020multimodal}: variational RNN w attention, predict agent B given agent A and then predict into the future 

% ReMoS: 3D Motion-Conditioned Reaction
% Synthesis for Two-Person Interactions (ECCV 2024)~\citep{ghosh2024remos}: diffusion + cross-modal attention, predict agent B given agent A

% Duolando (ICLR 2024)~\citep{siyao2024duolando}: uses GPT transformer model AR next token prediction + RL, given music + person A dance, predict/generate person B dance


% InterFormer (2023)~\citep{10036100}: transformer encoder-decoder, given person A pose predict person B  pose

% ExPI (CVPR 2022)~\citep{guo2022multi}: sort of transformer? (cross-interaction
% attention modules); 


% Waymo MotionLM~\citep{seff2023motionlm} \neerja{We are basically implementing a stripped down/simpler version of this paper but showing that their agent ID embedding and next timestep prediction helps on different tasks - so there is some subtlety on how we actually mention this paper}



\medskip
\noindent
