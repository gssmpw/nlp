\section{Case Study 1: Social Action Forecasting}

\begin{figure*}[t]
    \centering
    \includegraphics[width=.9\textwidth]{fig/ava_qual.pdf}
    \caption{\textbf{Action forecasting example.} The distribution over ground truth actions are in white, and our predictions in red. A 6s action history (1Hz) is input, and 6s of future actions predicted. In the scene, the man and woman alternate between talking and listening. Initially, the man is talking. The AR model predicts he will continue talking, while the 2-agent PAR model recognizes the woman is talking and predicts more accurate turn-taking behavior.}
    \label{fig:ava_qual}
    \vspace{-.6cm}
\end{figure*}


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{fig/specified_classes_comparison.pdf}
    \small
    \caption{\textbf{Per-class mAP for AVA 2-person actions}. We see performance improvement on almost all 2-person AVA action classes ((P) stands for ``a person"). Some absolute mAP gains are particularly significant: \textit{listen to} $+7.0$, \textit{kiss} $+8.3$, \textit{fight/hit} $+5.7$, \textit{talk to} $+4.4$, \textit{hug} $+5.7$, and
    \textit{hand shake} $+4.0$.}
\label{fig:ava_2_person_classes}
\vspace{-.5cm}
% \end{wrapfigure}
\end{figure}

Our first case study involves forecasting human actions. Human behaviors are fundamentally social; for instance, individuals frequently walk in groups and alternate between speaking and listening roles when conversing. Certain actions, like hugging or handshaking, are intrinsically multi-person. Therefore, modeling human interactions should help improve action forecasting performance, especially on multi-person actions, which we show in this case study.


\subsection{Experimental Setup}
\noindent \textbf{Dataset.}  The Atomic Visual Actions (AVA) dataset~\citep{gu2018ava} comprises 235 training and 64 15-minute validation videos from movies. Annotations are provided at a 1Hz frequency, detailing bounding boxes and tracks for individuals within the frame, and each person's actions within a 1-second timeframe. Individuals may engage in multiple concurrent actions from a repertoire of 60 distinct action classes (e.g., sitting and talking simultaneously). For our analysis, we select clips featuring a continuous sequence of an agent's actions spanning at least $4$s, splitting sequences exceeding $12$s.  We use the first half of each clip as history to predict the second half. For any ego agent trajectory, we pick a second agent by selecting the person present in the scene for the longest subset of the ego agent's trajectory.

% \medskip \noindent \textbf{Tokenization.}
\medskip \noindent \textbf{Task-specific considerations.} Each agent's token $\mathcal{A}$ represents an 60-dimensional vector that corresponds to the actions performed at a specific timestep. Each element denotes the probability of a particular action class being enacted; ground-truth inputs are a binary vector. We implement an embedding layer that projects these tokens into the transformer's hidden dimension, as well as an un-projection layer that reverts them back to the original 60D token space for the purposes of loss calculation and output generation. We do not explicitly require the outputs to be values between 0 and 1.
% \medskip \noindent \textbf{Loss.} 
We use a MSE regression loss on the 60D action tokens: $\mathscr{L} = \frac{1}{n} \sum_{i=1}^{n} (\mathcal{A}_i - \hat{\mathcal{A}}_i)^2$.
% \medskip \noindent \textbf{Metrics.} 
Our evaluation metric is the
% We measure 
mean average precision (mAP) on the 60 AVA classes.

We implement all baselines described in \ref{sec:task-specific}, where \textit{Random Token} corresponds to a random 60D vector sampled from 0 to 1. \textit{NN} and \textit{Multiagent NN} use Hamming distance as the distance metric.
\begin{table}
% 
\centering
\begin{tabular}{@{}ccccc@{}}
\toprule
Method & Timestep pred & Ag-ID embd & mAP $\uparrow$           \\
\midrule
1-agent AR   & N/A         & N/A        & 40.7        \\
2-agent AR   & \xmark      & \xmark     & 38.0      \\
2-agent PAR*  & \xmark     & \cmark     & 40.2         \\
2-agent PAR*   & \cmark     & \xmark   & 40.0         \\
2-agent PAR &\cmark  & \cmark & \textbf{42.6}     \\
\bottomrule
\end{tabular}
\caption{\textbf{PAR action forecasting performance on AVA} We evaluate 1 and 2-agent AR methods, two 2-agent PAR ablations (rows 3 and 4, PAR*), and our PAR method. Without next-timestep prediction (see Fig.~\ref{fig:PAR_training}) or a learned agent ID embedding, our model struggles with multi-agent reasoning, performing worse than the AR baseline. With both components, the 2-agent PAR model achieves a +1.9 mAP gain over the AR method (see Fig.~\ref{fig:ava_1_person_classes} and Fig.~\ref{fig:ava_2_person_classes} for class breakdown).}
\label{tab:ava_PAR_ablation}
\vspace{-.25cm}
\end{table}
% neerja: experiment names and wandb links
% 1-agent	https://wandb.ai/nthakkar/PAR_ava_action_prediction_post_ICLR/runs/s9sa5w5o?nw=nwusernthakkar	action_pred_ava_1_agent_lr_5e-5_ema_decay_0.999_seed_1_val_full_val
% 2-agent AR	https://wandb.ai/nthakkar/PAR_ava_action_prediction_post_ICLR/runs/nh9gvjlu?nw=nwusernthakkar	action_pred_ava_2_agent_no_ag_id_emb_no_same_ag_loss_lr_5e-5_ema_decay_0.999_seed_1_val_full_val
% 2-agent no timestep pred	https://wandb.ai/nthakkar/PAR_ava_action_prediction_post_ICLR/runs/xzlllmta	action_pred_ava_2_agent_no_same_ag_loss_lr_5e-5_ema_decay_0.999_seed_1_val_full_val_vis
% 2-agent no ag id emb	https://wandb.ai/nthakkar/PAR_ava_action_prediction_post_ICLR/runs/3uzemj2m?nw=nwusernthakkar	action_pred_ava_2_agent_no_ag_id_emb_lr_5e-5_ema_decay_0.999_seed_1_val_full_val_vis
% 2-agent PAR	https://wandb.ai/nthakkar/PAR_ava_action_prediction_post_ICLR/runs/u5yoe6vs?nw=nwusernthakkar	actual_final_ava_1hz_action_2_agent_ag_id_emb_full_val_val_full_val
% https://wandb.ai/nthakkar/PAR_ava_action_prediction/runs/ramhpxe1?nw=nwusernthakkar	
\begin{table}
% 
\centering
\begin{tabular}{@{}lcc@{}}
\toprule
Baseline & Agents &  mAP $\uparrow$           \\
\midrule
Random Token         & 1 & 3.46          \\
Random Training Traj  & 1   & 3.44          \\
Nearest Neighbor    & 1        & 13.17 \\
Multiagent NN &2      & 5.10      \\
Mirror   & 2 & 7.97         \\
\bottomrule
\end{tabular}
\caption{\textbf{AVA baselines} While the nearest neighbor baseline performs best among baselines, it is still significantly worse than the AR model.}
\label{tab:ava_baselines}
\vspace{-.5cm}
\end{table}

\subsection{Results}

We report the performance of a single-agent AR model as a baseline, in the first line of Table~\ref{tab:ava_PAR_ablation}. The AR model is significantly better than our baselines (see Table~\ref{tab:ava_baselines}), the strongest baseline being the single-agent NN. We compare these baselines to our 2-agent PAR model (last line) and various ablations where we remove the agent ID embedding and perform next-token rather than same-agent next-timestep prediction. The second line of the table corresponds to multi-agent next-token prediction (Fig.~\ref{fig:AR_multiagent_nexttoken}). We see that this approach confuses the model, and the performance is significantly worse than just training on and considering a single agent. However, as we add various components of our PAR approach, the performance improves, and with both the next timestep prediction and agent ID embedding, we get a $+1.9$ mAP gain. When only considering 2-person action classes (enumerated in Fig.~\ref{fig:ava_2_person_classes}), our mAP is  $36.3$ on the single agent PAR model and $39.8$ on the 2-agent PAR model, a \textbf{$+3.5$} mAP gain.

In Fig.~\ref{fig:ava_qual} we see an example of action forecasting. In the input history, the man talks and the woman listens. In the future, the woman talks, and the man listens. Our 2-agent PAR model (bottom row) better understands that talking and listening actions are complementary actions, while the AR model doesn't learn this correlation. We see quantitative evidence of this in Fig.~\ref{fig:ava_2_person_classes}, with per-class mAPs for our AR vs 2-agent PAR model for 2-person action classes. Here, \textit{talk to} gets a $+4.4$ mAP gain and \textit{listen to} gets a $+7.0$ mAP gain when we train a multi-agent model. We see a significant boost on many other interaction-related action classes---for instance, \textit{kiss a person} $+8.3$ and \textit{fight/hit a person} $+5.7$ mAP---and on single-person actions, see Fig.~\ref{fig:ava_1_person_classes}.









