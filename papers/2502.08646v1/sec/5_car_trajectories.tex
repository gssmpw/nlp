\section{Case Study 2: Multiagent Car Trajectory Prediction}
\label{sec:car_trajs}
Our second case study focuses on predicting car trajectories. Trajectory prediction requires a vehicle to be aware of other cars on the road to avoid collisions and promote cooperative behavior. This study demonstrates how our framework enables the joint modeling of multiple vehicles' movements.

\subsection{Experimental Setup}
\noindent \textbf{Dataset.} We use nuScenes~\citep{nuscenes} , inputting 2 seconds of positions to forecast vehicle positions 6 seconds ahead. Specifically, our objective is to predict the $xy$ coordinates of each agent, exclusively considering vehicles as agents. We use the \texttt{trajdata} interface~\citep{ivanovic2023trajdata} to load and visualize the data.

\noindent \textbf{Task-specific considerations.}   Instead of discretizing the $xy$ position space, we discretize the motion, resulting in discrete velocity or acceleration tokens. These integer tokens are projected to the transformer hidden dimension using the Llama token embedding layer. Inputting only these tokens results in our PAR model knowing what speed the other agents are going at, but not where they are. It is important the model has this awareness (it should know if two agents are going to collide), so our model needs to reason over this second modality of location. We implement this by passing locations relative to the agent we are predicting into a sin-cosine positional embedding (see details in Sec.~\ref{sec:appendix_car_impl_details}), which we denote a location positional encoding (LPE). The LPE is summed to our token embeddings.

We use a cross-entropy classification loss on our discrete tokens:
$\mathscr{L} = E_{y \sim p(y)}[-\log(p(\mathbf{s}^{T}_{t_{\pi}})].$  We use the standard average displacement error (ADE) and final displacement error (FDE) to evaluate our predicted trajectories. For our baselines (Sec.~\ref{sec:task-specific}), we use the closest agent at the current timestep for \textit{Multiagent NN} and \textit{Mirror}. For \textit{NN} and \textit{Multiagent NN} we use MSE as the distance metric.


\subsection{Results}
\begin{table}
% 
\centering
\begin{tabular}{@{}lcccc@{}}
\toprule
Token type & LPE & Method &  ADE $\downarrow$ & FDE $\downarrow$           \\
\midrule
Velocity  & \xmark & 1-agent AR & 1.50 & 3.64 \\
Velocity & \xmark & 3-agent PAR & 1.45 & 3.51 \\
Accleration  & \xmark   & 1-agent AR & 1.44    & 3.57  \\
Accleration  & \xmark & 3-agent PAR  & 1.40    & 3.44    \\
Accleration  & \cmark& 3-agent PAR   & \textbf{1.35} & \textbf{3.34} \\
\bottomrule
\end{tabular}
\caption{\textbf{Car trajectory prediction performance.} Using acceleration tokens and 3-agent PAR results in a stronger performance over velocity tokens and single-agent AR. Adding location via a positional encoding (LPE) further improves results. }
\label{tab:cars_locs}

% Neerja: experiment names/wandb links
% 1-agent velocity	https://wandb.ai/nthakkar/ICLR_paper_cars/runs/ccg7y19d?nw=nwusernthakkar	final_car_traj_1_agent_vel_tok_a100_full_val
% 3-agent PAR velocity	https://wandb.ai/nthakkar/ICLR_paper_cars/runs/sgi43f24	final_car_traj_3_agent_vel_tok_a100_full_val
% 1-agent acc	https://wandb.ai/nthakkar/PAR_car_traj_prediction_nuscenes/runs/k5na3q37?nw=nwusernthakkar	final_car_traj_1_agent_accl_tok_a100_full_val
% 3-agent PAR acc	https://wandb.ai/nthakkar/PAR_car_traj_prediction_nuscenes/runs/9flpua7w	final_car_traj_3_agent_accl_tok_a100_full_val
% 3-agent PAR acc + LPE	https://wandb.ai/nthakkar/PAR_car_traj_prediction_nuscenes/runs/l8fbaj7m?nw=nwusernthakkar	final_car_traj_3_agent_location_pos_embedding_sin_cos_accl_tok_relative_loc_a100_full_val

\end{table}
% 
% 
\begin{table}
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
Baseline & Agents &  ADE $\downarrow$ & FDE $\downarrow$           \\
\midrule
Random Trajectory & 1  & 8.89 & 16.51      \\
NN    & 1   & 1.80 & 4.13 \\
 Multiagent NN & N & 6.40 & 12.04      \\
Mirror  & N & 11.59 & 14.93      \\
\bottomrule
\end{tabular}
\caption{\textbf{Car trajectory prediction baselines.} Nearest neighbor performs best overall, but our learned single-agent AR models outperform all baselines.}
\label{tab:cars_baselines}
\vspace{-0.25cm}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=0.48\textwidth]{fig/cars_qual.pdf}
    \caption{Example results from our single-agent AR model (top row) and three-agent PAR model with location positional encoding (bottom row) on nuScenes. The predicted agent's ground truth trajectory is in pink, and the predicted future in blue. For the PAR model, the other two agents' ground truth states are in green. Qualitatively, the PAR model handles situations where single-agent predictions might lead to collisions (A, B), uses other agents' behavior to better adhere to road areas (A, C) without environment data, and predicts based on the speed changes of other cars (D).}
    \label{fig:cars_qual}
    \vspace{-.6cm}
\end{figure}


We train AR and 3-agent PAR models using velocity tokens, acceleration tokens, and acceleration tokens combined with our location positional encoding. The results can be seen in Table~\ref{tab:cars_locs}. Note that the 3-agent PAR model uses the agent ID embedding and next timestep prediction.
Acceleration tokens consistently outperform velocity tokens both for agent AR and 3-agent PAR models. This could be because the vocabulary size for acceleration tokens is much smaller and therefore easier to optimize. Regardless, both ways of tokenizing result in models that outperform our baselines (see Table~\ref{tab:cars_baselines} - NN has a relatively low error on this dataset), and highlight that our framework is flexible such that a user can experiment with different ways of representing entities. For both token types, the 3-agent PAR model that is blind to location outperforms the AR model. While location information should help the model, it is possible that simply knowing whether other agents are slowing down or accelerating can help the model make better predictions.  When adding location information via the LPE to our 3-agent PAR model, we see another performance gain in ADE and FDE. 

Qualitative examples of the AR model (top row) and 3-agent location-aware PAR model (bottom row) can be seen in Figure~\ref{fig:cars_qual}. Our method uses no image or environment data (e.g., lanes) as input. However, by reasoning over multiple agents, its predictions lead to fewer collisions and better reasoning about speed changes and driveable areas based solely on other agents' behaviors.
% Note that our method does not take as input any pixels/image information, or any information about the environment such as lanes. We see evidence that when reasoning over multiple agents, our method is able to make predictions that result in fewer collisions, and better reasoning about changes in speed and what parts of the road are driveable, just based on the behaviour of other agents.





