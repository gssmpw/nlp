
A popular model to study learning problems in statistical, computer science and machine learning is the {\emph {multi-index model}}, where the target function depends on a low-dimensional subspace of the covariates.  In this problem, one aim at identifying the $p$-dimensional linear subspace spanned by a family of orthonormal vectors $\bw_{\star,1},\dots,\bw_{\star,p}\in\mathbb{R}^{d}$ from $n$ independent observations $(\bx_{i},y_{i})$ from the model:
\begin{align}
    y_{i}=g\left(\langle \bw_{\star,1},\bx_{i}\rangle,\dots,\langle \bw_{\star,p},\bx_{i}\rangle\right), \qquad i\in\integset{n}.
\end{align}
This formulation encompasses several fundamental problems in machine learning, signal processing, and theoretical computer science, including:
(i) Linear estimation, where $p=1$ and $g(z)=z$, (ii) Phase retrieval, where $p=1$ and $g(z)=|z|$.
(iii) Learning Two-layer neural networks, where $p$ is the width and ${g(\bz)=\sum_{j\in \integset{p}}a_{j}\sigma(z_{j})}$ for some non-linear activation function $\sigma:\mathbb{R}\to\mathbb{R}$, or (iv)  learning Sparse parity functions, where $g(\bz)=\sign(\prod_{j\in\integset{p}}z_{j})$.

A classical problem in statistics \cite{friedman1981projection, yuan2011identifiability, Babichev2018}, the multi-index model has recently gained in popularity in the machine learning community as a generative model for supervised learning data where the labels only depend on an underlying low-dimensional latent subspace of the covariates \cite{aubin2018committee, damian2022neural, dandi2024two}.

Of particular interest to this work are spectral methods, which play a fundamental role in machine learning by offering an efficient and computationally tractable approach to extracting meaningful structure from high-dimensional noisy data. A paradigmatic example is the Baik–Ben Arous–Péché (BBP) transition \cite{baik2005phase}, where the leading eigenvalue of a matrix correlates with the hidden signal — a phenomenon that is ubiquitous in machine learning theory. Beyond their practical utility, spectral methods often serve as a starting point for more advanced approaches, including iterative and nonlinear techniques. This leads us to the central question of this paper:

\begin{center} 
\textit{Can one design optimal spectral methods that minimizes the amount of data required for identifying the hidden subspace in multi-index models?} 
\end{center}

While the optimal spectral method for single-index models are well understood \cite{Luo2019,mondelli18a,maillard22a,Mondelli2022}, and their optimality in terms of weak recovery threshold established \cite{Barbier2019,mondelli18a} their counterparts for multi-index models remain largely unexplored. This gap is particularly important, as multi-index models serve as a natural testbed for studying the computational complexity of feature learning in modern machine learning, and have attracted much attention recently \cite{abbe2023sgd, damian2022neural,dandi2024two,arnaboldi2023high,collins2024hitting,berthier2024learning,simsek2024learning}.

\paragraph{Main contributions ---}
In this work, we step up to this challenge by constructing optimal spectral methods for multi-index models. We introduce and analyze spectral algorithms based on a linearized message-passing framework, specifically tailored to this setting. Our main contribution is to present two such constructions, establish the  reconstruction threshold for these methods and to show that they achieve the provably optimal threshold for weak recovery among efficient algorithms \cite{troiani2024fundamental}.

\paragraph{Other Related works --}
\begin{itemize}[leftmargin=2em,wide=1pt]
    \item Recently, multi-index models have become a proxy model for studying non-convex optimization \cite{veiga2022phase, arnaboldi2023high, collins2024hitting}. \cite{arous2021online} has shown that the sample complexity of one-pass SGD for single-index model is governed by the \emph{information exponent} of the link function. This analysis was generalized in several directions, such as to other architectures \cite{berthier2024learning}, larger batch sizes \cite{arnaboldi2024online} and to overparametrised models \cite{arnaboldi2024escaping}. A similar notion, known as the \emph{leap exponent}, was introduced for multi-index models, where it was shown that different index subspaces might be learned hierarchically according to their interaction \cite{abbe2022merged, abbe2023sgd, bietti2023, mousavihosseini2024}. The picture was found to be  different for batch algorithms exploiting correlations in the data \cite{dandi2024benefits, arnaboldi2024repetita, lee2024neural}, achieving a sample complexity closer to optimal first order methods \cite{Barbier2019, damian24a, troiani2024fundamental}. \cite{troiani2024fundamental} in particular, provided optimal asymptotic thresholds for weak recovery within the class of first order methods.
    
    \item Spectral methods are widely employed as a warm start strategy for initializing other algorithms, in particular for iterative schemes (such as gradient descent) for which random initialization is a fixed point. Relevant to this work is the class of approximate message passing (AMP) algorithms, which have garnered significant interest in the high-dimensional statistics and machine learning communities over the past decade \cite{donoho2009message,Bayati2011,rangan2011generalized,fletcher2018iterative}. AMP for multi-index models was discussed in \cite{aubin2018committee, troiani2024fundamental}. Spectral initialization for AMP in the context of single-index models has been studied by \cite{mondelli21a}.  
    
    \item The interplay between AMP and spectral methods has been extensively studied in the literature, see for example \cite{saade2014spectral,lesieur2017constrained,aubin2019spiked,mondelli18a,Mondelli2022,maillard22a,venkataramanan2022estimation}. In particular the idea of using message passing algorithm to derive spectral method has been very successful, leading to the non-backtracking matrix \cite{krzakala2013spectral} and more recently to the Kikuchi hierarchy \cite{wein2019kikuchi,hsieh2023simple}, and to non-linear optimal spectral methods for matrix and tensor factorization \cite{lesieur2017constrained,perry2018optimality,guionnet2023spectral,pak2024optimal}. 
\end{itemize}









