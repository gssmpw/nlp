

In this work, we tackled weak recovery in high-dimensional multi-index models via spectral methods, deriving two estimators inspired by a linearization of AMP. We showed they achieve the optimal reconstruction threshold, closing a key gap in prior approaches that required additional side information. Our analysis establishes that above the critical sample complexity, the leading eigenvectors of the proposed spectral operators align with the ground-truth subspace, echoing the BBP transition in random matrix theory.

This work advances our understanding of weak subspace recovery in multi-index models and provides a principled framework for designing optimal spectral estimators. It bridges ideas from random matrix theory, approximate message passing, and neural feature learning.

Several directions remain open. A random matrix theory analysis of our spectral analysis --- which requires a challenging control of the spectral norms --- could be used to prove the two conjectures. Extending our AMP linearization to higher-order schemes, such as the Kikuchi hierarchy, may unlock insights into harder generative exponent problems, including the notorious sparse parity function. We hope this work sparks further research at the intersection of spectral methods and high-dimensional inference.
