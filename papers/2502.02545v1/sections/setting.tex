\paragraph{Notations ---} To enhance readability, we adopt the following consistent notations through the paper: scalar quantities are denoted using non-bold font (e.g., \(a\) or \(A\)), vectors are represented in bold lowercase letters (e.g., \(\vect{a}\)), matrices are written in bold uppercase letters (e.g., \(\mat{A}\)), and tensors are depicted using underlined bold uppercase letters (e.g., \(\tens{A}\)). We further differentiate random variables depending on the noise with non-italic font (e.g, $\rdm{a}, \rdmvect{a}, \rdmmat{A}, \rdmtens{A}$).  We denote by $\langle \vect{a} , \vect{b} \rangle$   the standard Euclidean scalar product, $\| \vect{a} \|$ the $\ell^2$-norm of a vector $\vect{a}$, $\normop{\mat{A}}$ the operator norm of a matrix $\mat{A}$ and $\normf{\mat{A}}$ its Frobenius norm.  Given $n \in \mathbb{N}$, we use the shorthand $\integset{n} = \{1, \dots, n\}$. We denote $\mathbb{S}_+^p$ the cone of positive semi-definite $p\times p$ matrices. $(x)_+ := \max(0,x)$. 


\paragraph{The Gaussian Multi Index Model ---}
We consider the supervised learning setting with $n$ i.i.d. samples $(\rdmvect{x}_i, \rdm{y}_i)_{i\in \integset{n}}$ with covariates $\vx_i\sim \ndist (\bzero, d^{-1}\bI_d)\in\R^d$ and labels $\rdm{y}$ drawn conditionally  from the Gaussian \textit{multi-index model} defined by
\begin{equation}
\label{eq:def:gmim}
    \rdm{y} \sim \mathsf{P} \big( . | \matw^T\vx \big)  = \mathsf{P} \left(. \Bigg |
    \begin{bmatrix}
    \langle \vect{w}_{\star,1}, \vx \rangle \\
    \vdots \\
     \langle \vect{w}_{\star,p}, \vx \rangle
    \end{bmatrix}
    \right) \, , 
\end{equation}
where $\bW_\star := (\bw_{\star,1}, \dots, \bw_{\star,p}) \in\R^{d\times p}$ is a weight matrix with independant columns with $\bw_{\star,j}\sim \ndist(\bzero,\bI_d)$, $j\in\integset{p}$ and $\mathsf{P}(\cdot | \cdot)$ is a conditional probability distribution. Additionally, we define the \emph{link function} $g:\R^p \ni \vect{z} \mapsto  g(\vect{z}) \in \R$ as the conditional mean
\begin{equation}
\label{eq:def:multi_index_model}
     g(\vect{z}) := \mathbb{E} \{ \rdm{y} | \matw^T\vx = \vect{z}  \}  = \int y \,  \mathrm{d}\mathsf{P}(y|\bz)\,,
\end{equation}
and the labels' marginal distribution
\begin{equation}
    \Py(y) = \E_{\bW_\star}\E_{\rdmvect{x}\sim\ndist(\bzero,d^{-1}\bI_d)}[\mathsf{P}(y|\bW_\star^T\rdmvect{x})].
\end{equation}
Note that, in the limit $d\to\infty$, for $\rdmvect{x}\sim\ndist(\bzero,d^{-1}\bI)$, the Central Limit Theorem implies $\bW_\star^T\rdmvect{x}\sim\ndist(\bzero,\bI_p)$ and $ \Py(y) = \E_{\rdmvect{z}\sim\ndist(\bzero,\bI_p)}[\mathsf{P}(y|\rdmvect{z})]$.\\
We investigate the problem of reconstructing $\bW_\star$ in the proportional \textit{high-dimensional} limit
\begin{align}
    d,n \equiv n(d)\to\infty \quad \mbox{such that}  \quad \nicefrac{n}{d}\to\alpha\in\R_{+} \, .
\end{align}
In particular we are interested in the existence of an estimator $\matwhat$ that correlates with the weight matrix $\matw$ better than a random estimator. This is formalized as follows.
\begin{definition}(Weak subspace recovery) Given an estimator $\matwhat$ of $\matw$ with  $\normf{\matwhat} = \mathrm{\Theta}(d)$, we say we have \emph{weak recovery} of a subspace $V \subset \mathbb{R}^d$,  if
\begin{equation}
    \label{eq:def_weak_recovery}
    \underset{\vect{v} \in V \cap \mathbb{S}^{d-1}}{\mathrm{inf}} \norm{\matwhat^T \matw \vect{v}} = \mathrm{\Theta}(1) \, ,
\end{equation}
with high probability. 
\end{definition}
Computational bottlenecks for weak recovery in the Gaussian multi-index models have been studied by \cite{troiani2024fundamental} using an optimal \textit{generalized approximate message passing} (GAMP) scheme, see Appendix \ref{app:sec:gamps} for a detailed discussion of the algorithm. In particular, for the appropriate choice of denoiser functions, given in eq. (\ref{app:def:optimal_gamp}), AMP is provably optimal among first-order methods \cite{celentano2020estimation, montanari2024statistically}.
Their work provides a classification of the directions in $\R^p$ in terms of computational complexity of their weak learnability. In particular, if and only if 
\begin{equation}\label{eq:condition_non_triviality}
    \E_{\rdmvect{z}\sim\ndist(\bzero,\bI_p)}[\rdmvect{z}\mathsf{P}(\rdm{y}|\rdmvect{z})] \propto \E[\rdmvect{z}\big|\rdm{y}]= \bzero
\end{equation}
almost surely over $\rdm{y}\sim\ \Py$, the subspace of directions that can be learned in a finite number of iterations is empty, for AMP randomly initialized. Nonetheless, if the initialization contains an arbitrarily small (but finite) amount of \textit{side-information} about the ground truth weights $\bW_\star$, AMP can weakly recover a subspace of $\R^p$, provided that $\alpha>\alpha_c$, where the critical sample complexity is characterized in Lemma \ref{lemma:critical_sample_complexity}.

\begin{lemma}\label{lemma:critical_sample_complexity}\cite{troiani2024fundamental}, Stability of the uninformed fixed point and critical sample complexity]
\label{def:critical_alpha}
    \label{thm:unin:stability}
    If $\bM=\bzero\in\mathbb{R}^{p\times p}$ is a fixed point of the state evolution associated to the optimal GAMP (\ref{app:def:optimal_gamp}), then it is an unstable fixed point if and only if $\norm{\mathcal{F}(\bM)}_F > 0$ and $n>\alpha_c d$, where the critical sample complexity $\alpha_c$ is:
    \begin{equation} 
    \label{eq:alpha_c}
        \alpha_c^{-1} = \sup_{\bM\in \mathbb{S}^+_p,\, \normf{\bM}=1} \normf{\mathcal{F}(\bM)},
    \end{equation}
    with 
    \begin{equation}\label{eq:def:operator_F}
        \cF(\bM) \coloneqq \E_{\rdm{y}\sim\Py}\left[\left(\Cov[\rdmvect{z}\big| \rdm{y}]-\bI\right)\mat{M} \left(\Cov[\rdmvect{z}\big| \rdm{y}]-\bI\right)\right],
    \end{equation} 
    Moreover,
   if $\normf{\mathcal{F}(\bM)} = 0$, then $\bM=\bzero$ is a stable fixed point for any $n = \Theta(d)$.
\end{lemma}

The aim of our work is to close this gap, providing an estimation procedure that achieves weak recovery at the same critical threshold $\alpha_{c}$ defined in eq.~\eqref{eq:alpha_c}, but crucially {\it does not require an informed initialization}. 

In what follows we restrict the problem defined in (\ref{eq:def:multi_index_model}) to the set of link functions functions satisfying eq. (\ref{eq:condition_non_triviality}). These functions (said to have a \emph{generative exponent} $2$ in the terminology of \cite{damian24a}) covers a large class of the relevant problems, with the exception of the really hard functions such as sparse parities, which cannot be solved efficiently with a linear (in $d$ number of samples \cite{troiani2024fundamental}.

Throughout this manuscript, we adopt the following definitions for tensor operations and tensor eigenpairs.
\begin{definition}(Tensor Contraction) For a tensor of order 4 of the form $\tens{A}=(A_{ijkh}) \in\R^{\nu\times p \times \nu \times p}$ with $\nu = n$ or $\nu = d$, we define the contraction of such tensor with a matrix $\mat{B}\in\mathbb C^{\nu\times p}$ by the operation $\tens{A}\cdot\mat{B} \in \mathbb{R}^{d \times p}$ such that
\begin{equation}
\begin{aligned}
    (\tens{A}\cdot\mat{B})_{kh}
    &:=
\sum_{i\in\integset{\nu}}\sum_{j\in\integset{p}}A_{ijkh}B_{ij} \quad k\in\integset{\nu},h\in\integset{p}.
\end{aligned}
\end{equation}
\end{definition}
The tensor $\tens{A}$ is symmetric if $\langle\tens{A}\cdot\mat{B},\mat{C}\rangle = \langle\mat{B},\tens{A}\cdot\mat{C}\rangle$ (equivalently $A_{ijkh} = A_{jikh}$ and $A_{ijkh} = A_{ijhk}$, for all $i,j\in\integset{\nu}$, $k,h\in\integset{p}$). 

\begin{definition}(Tensor Inversion) A tensor $\tens{A}$ is \emph{invertible} if and only if there exists a tensor $\tens{A}^{-1}\in\R^{\nu\times p \times \nu \times p}$ such that, 
\begin{align}
    \tens{A}^{-1}\cdot(\tens{A}\cdot\mat{B}) = \tens{A}\cdot(\tens{A}^{-1}\cdot\mat{B}) = \tens{I}\mat{B} = \mat{B}
\end{align}
holds for all $\mat{B}\in\mathbb \mathbb{R}^{\nu\times p} $ and where $\tens{I}= (I_{ijkh})$ is the identity tensor, $I_{ijkh}:= \delta_{ij}\delta_{kh}$.
\end{definition}
 In this framework, $\tens{A}$ acts as a linear operator in the vector space of $\nu\times p$ matrices, with inner product $\langle\mat{B},\mat{C}\rangle \coloneqq \Tr(\mat{B}^T\mat{C})$ leading to the following natural definition of eigenpairs.
\begin{definition}\label{def:tensor_eigenpairs}(Tensor Eigenpairs)  We say that $(\lambda, \mat{V}) \in \mathbb{C} \times \mathbb{R}^{\nu \times p}\setminus \{\bm{0}_{\nu \times p}\}$ is an \emph{eigenpair} of a tensor $\tens{A} \in \mathbb{R}^{\nu \times p \times \nu \times p}$ if the following equation holds
\begin{equation}
    \tens{A}\cdot\mat{V} = \lambda \mat{V}.
\end{equation}
\end{definition}
For the spectral theorem, a symmetric tensor admit an orthonormal basis of eigenvectors in $\R^{\nu\times p}$, corresponding to real eigenvalues.

\begin{remark}(Flattened Tensor) One can equivalently consider a \emph{flatten} version of tensor, that is the matrix $\mat{flat}(\tens{A}) \in \mathbb{R}^{\nu p \times \nu p}$ such that
\begin{align}
    &(\mat{flat}(\tens{A}))_{uv} := A_{ijkh} \notag\\ & u=(i-1)p+j, \qquad v =(k-1)p+h \, .
\end{align}
Similarly a \emph{flatten} version of a matrix is defined as $\mat{flat}(\mat{V)} \in \mathbb{R}^{\nu p}$ such that $(\mat{flat}(\mat{V)} )_{u} = V_{ij}$ with $ u=(i-1)p+j$.  A classical eigenpair of $\mat{flat}(\tens{A})$ is then related to the former definition \ref{def:tensor_eigenpairs} of the tensor eigenpair by $(\lambda, \mat{flat}(\mat{V}) )$. By abuse of notation, we will write $\normop{\tens{T}} = \normop{\mat{flat}(\tens{T})}$. 
\end{remark}


We can now introduce the Spectral Methods we aim to investigate. Given a matix $\rdmmat{X}\in\R^{n\times d}$ with rows $\rdmvect{x}_i\sim\ndist(\bzero,d^{-1}\bI_d)$ and a vector of labels $\rdmvect{y} \in\R^n$ with elements sampled as in (\ref{eq:def:multi_index_model}), define the tensor $\tens{G}\in\R^{n\times p \times n \times p}$, acting on $n\times p$ matrices as described in Definition \ref{def:tensor_eigenpairs}, as
\begin{equation}\label{eq:def:tensor_G}
    G_{ikjh} = \delta_{ij}\E[\rdm{z}_k\rdm{z}_h\big|\rdm{y}_i] - \delta_{ij}\delta_{kh},\quad i,j\in\integset{n},\;k,h\in\integset{p}.
\end{equation}
where $\delta_{ij}$ is the Kronecker delta. 

Given these definitions, define the following two spectral estimators $\matwhat_{\tens{L}}$,$\matwhat_{\tens{T}}$ of the weight matrix $\bW_\star$ as
\begin{enumerate}
    \item \underline{Asymmetric spectral method:}
    \begin{equation}\label{eq:def:spectral_asymmetric}
    \matwhat_{\tens{L}}:= \sqrt{dN}\frac{\rdmmat{X}^T\tens{G}\cdot\bOmega_1}{\normf{\rdmmat{X}^T\tens{G}\cdot\bOmega_1}},
\end{equation}
 where $\bOmega_1$ is the (matrix) eigenvector corresponding to the eigenvalue $\gamma_1^{\tens{L}}$ with largest real part of the tensor  $\tens{L}\in\R^{n\times p\times d\times p}$ defined for any $\mat{\Omega} \in \mathbb{R}^{n \times p}$ by:
\begin{equation}\label{eq:def:spectral_asymmetric}
\tens{L} \cdot \bOmega \coloneqq (\rdmmat{X}\rdmmat{X}^T - \bI_n)\tens{G}\cdot\bOmega \, .
\end{equation}
Note that the constant $N$ can be chosen to fix the normalization of
$d^{-1}\normf{\matwhat}^2 = N$.
\item \underline{Symmetric spectral method:}
\begin{equation}\label{eq:def:spectral_symmetric}
    \matwhat_{\tens{T}} := \sqrt{dN}\frac{\mat{W}_1}{\normf{\mat{W}_1}},
\end{equation}
 where $\mat{W}_1$ is the (matrix) eigenvector associated to the largest eigenvalue $\gamma_1^{\tens{T}}$ of the symmetric tensor map $\tens{T}\in\R^{d\times p\times d\times p}$ defined as
\begin{equation}
\tens{T}\cdot \mat{W} \coloneqq \rdmmat{X}^T\tens{G}\cdot\left((\tens{G}+\tens{I})^{-1}\cdot(\rdmmat{X}\mat{W})\right)
\end{equation}
\end{enumerate}
Note that the constant $N$ is arbitrary and can be chosen to fix
$d^{-1}\normf{\matwhat_{\tens{L}}}^2 = d^{-1}\normf{\matwhat_{\tens{T}}}^2 = N$.


At first glance, these two spectral estimators may appear ad hoc or lacking a clear theoretical justification. However, they can be motivated by a linearization of the optimal GAMP algorithm around the non-informative fixed point:
\begin{align} \label{eq:linearized_AMP}
    \delta\bOmega^t &= \rdmmat{X}\;\delta\matwhat^t - \tens{G}\cdot\delta\bOmega^{t-1},\\\label{eq:linearized_AMP_W}
    \delta\matwhat^{t+1} &= \rdmmat{X}^T\tens{G}\cdot\delta\bOmega^t. 
\end{align}
Substituting eq. (\ref{eq:linearized_AMP_W}) into eq. (\ref{eq:linearized_AMP_W}), this is equivalent to $\delta\bOmega^{t+1} = \tens{L}\cdot\delta\bOmega^t$. Moreover, assuming convergence,\footnote{Here and in the rest of the paper, we drop the time index $t$ to refer to the quantities at convergence.} one ends up with
\begin{align}
    \delta\bOmega &= \tens{L} \cdot  \delta\bOmega \\
    \delta\matwhat &= \tens{T}\cdot\delta\matwhat.
\end{align}
This suggests that at first leading order in the estimates, the dynamics is governed by power-iteration (and respectively a variant of it) on the tensor $\tens{L}$ (respectively  the symmetric tensor $\tens{T}$) defined previously. As power iteration converges under mild assumptions to the top (matrix) eigenvector of the tensor, this further suggests to use the top eigenvectors $\matwhat_{\tens{L}}$, $\matwhat_{\tens{T}}$ of the corresponding tensors as an estimate for the weight matrix $\matw$.

Additional details on the linearized GAMP are reported in Appendix \ref{app:sec:linear_GAMP}.
Similar approaches have been thoroughly investigated in the context of single-index models \cite{mondelli18a, maillard22a}, community detection \cite{krzakala2013spectral, saade2014spectral}, spiked matrix estimation \cite{lesieur2017constrained, aubin2019spiked}, where they have been provably shown to provide a non-vanishing correlation with the ground truth exactly at the optimal weak recovery threshold. It is interesting to notice that the spectral estimators $\matwhat_{\tens{L}}$ and $\matwhat_{\tens{T}}$ correspond to the generalization for multi-index models of the spectral methods derived in \cite{maillard22a}, respectively from the linearization of the optimal Vector Approximate Message Passing \cite{schniter_vgamp_2016,rangan_vgamp_2017} and the Hessian of the TAP free energy associated to the posterior distribution for the weights \cite{saade2014spectral}. In particular, for $p\!=\!1$, the matrices proposed in our manuscript exactly reduce to the two ones (called respectively "TAP" and "LAMP") investigated in \cite{maillard22a}.\looseness=-1

