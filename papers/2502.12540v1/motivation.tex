

\subsection{Challenges \& Research Gaps in Edge Serverless Computing}

The above sections have discussed significant advancements in mitigating cold start latency, scaling, and orchestration for serverless platforms. But since these impose unique challenges to edge environments (limited resources, workload  variability, and stringent latency requirements). Addressing these gaps requires a deeper understanding of the limitations of current solutions and their implications for edge Function-as-a-Service (FaaS)~\cite{rausch_towards_2019}.

\subsubsection{Cold Start Latency in Edge Environments}

As higlighted, cold starts remain a major challenge in edge systems due to limited computational slack. Unlike cloud platforms, which can afford to allocate idle resources, edge devices operate under strict constraints, worsening pre-warming misses during unpredictable traffic. For example, snapshotting techniques like \textbf{FaaSnap}~\cite{ao_faasnap_2022} improve initialization but require considerable memory resources, making them less suitable for constrained environments. The absence of lightweight and adaptive mechanisms for maintaining warm states leads to increased latencies, particularly for latency-critical tasks like IoT or real-time analytics.

\subsubsection{Inefficient Resource Utilization and Placement}

Heterogeneous edge nodes complicate resource allocation and function placement, often leading to contention or under utilization. Current efforts predominantly focus on deriving optimal placement strategies, yet these approaches are computationally expensive and often impractical due to the NP-hard nature of the problem. Therefore, the focus must shift toward lightweight frameworks which can prioritize efficient resource usage, increasing the capacity of edge nodes to handle more requests.

\subsubsection{Workload Variability and Adaptability}

Edge workloads differ fundamentally from cloud workloads in terms of scope and size. Cloud-optimized policies typically adapt to traffic variability over longer invocation periods, which is infeasible for edge environments where workloads change rapidly and unpredictably. Studies such as those on \textbf{Hermod}~\cite{kostis_hermod_2022} reveal that load- and locality-aware schedulers outperform static policies by dynamically adjusting to real-world workload patterns. However, these approaches require further refinement to minimize the overhead introduced by frequent scaling adjustments. Without effective adaptation, edge systems risk over-provisioning during low traffic and queuing delays during demand surges.

\subsubsection{Platform Agnostic Orchestration for Edge}

Platform-agnostic orchestration frameworks, which enable uniform deployment across diverse devices and architectures, are essential for commercial and market adaptation of edge solutions. Existing systems often rely on proprietary APIs or customized platforms, reducing their portability and hindering widespread adoption. For example, lightweight FaaS platforms like \textbf{TinyFaaS}~\cite{pfandzelter_tinyfaas_2020} demonstrate the feasibility of platform-agnostic designs by focusing on resource efficiency and modularity. Such frameworks not only improve scalability but also lower the entry barrier for integrating edge FaaS solutions into diverse environments, fostering greater innovation and adoption.




\subsection{Workload Analysis}\label{sec:work_anly}
By analyzing workload traces and identifying trends, we can glean valuable insights into invocation patterns, resource usage, and scaling inefficiencies. This understanding will inform the design of adaptive and lightweight edge FaaS policies that optimize resource utilization, minimize cold starts, and enhance platform portability. 
Function-as-a-Service (FaaS) workload consist of applications with different sized containers.
Small, lightweight and stateless containers are invoked more frequently than larger ones. Their high frequency requires persistent caching to reduce cold starts and ensure low latency for critical applications. 
On the other hand, large containers have significant memory and dependency requirements but exhibit infrequent invocations. Their cold starts impose longer delays, which require demand-driven strategies to balance responsiveness and resource efficiency. 


We conduct a workload analysis to gain a deeper understanding of Function-as-a-Service (FaaS) workload dynamics. Using the Azure functions dataset (2019)~\cite{mohammad_serverless_2020} available for 2 weeks, we identify critical function characteristics—\textit{memory footprint}, \textit{invocation frequency} along with patterns, and \textit{execution time}—as foundational dimensions for efficient resource orchestration. These insights form the basis of the KiSS framework's multi-level warm pool design, enabling it to address the gaps, specifically for edge environments.



\subsubsection{Memory Footprint}
Existing solutions in the literature fail to prioritize high frequency functions effectively since there is no defined analysis on the distribution of container sizes in applications. 

We plot the percentile distribution of application and function memory footprint as shown in Figure~\ref{fig:memory_footprint}.
We collect the application memory footprint from the Azure Functions data~\cite{mohammad_serverless_2020} and estimate the function memory using Equation~\ref{eq:func_mem}. 
The results show that more than 98\% of functions with small memory footprint consume below 225MB while large functions consume upto 500MB of memory. 


Our analysis of Azure Functions data~\cite{mohammad_serverless_2020} identified a memory footprint spike at around 225 MB. This was determined using percentile distributions of application memory data across the 12 days available in the dataset, followed by the estimation of function memory:
\begin{equation}\label{eq:func_mem}
    \text{Function Memory} = \frac{\text{Application Memory} \times \text{Function Duration}}{\text{Application Duration}}
\end{equation}


\begin{figure}[h]
    \centering
    \includegraphics[width=1\columnwidth]{FunAndAPPMem.pdf}
    \caption{Percentile distribution of memory footprints for Azure Functions workloads.}
    \label{fig:memory_footprint}
\end{figure}

\subsubsection{Traffic Frequency Correlation}
Frameworks such as \textit{Hermes}~\cite{kaffes_practical_2021} and \textit{HotC}~\cite{suo_tackling_2021} emphasize the importance of traffic-aware resource allocation.
However, they fail to account for the interplay between container size and invocation patterns. This leads to inefficiencies in caching policies.




Hence, we analyze the invocation frequency of small and large containers by categorizing the minute-by-minute invocation counts from the Azure Functions data with function memory sizes. 
Figure~\ref{fig:traffic_frequency} shows the invocation results over different times of a day.
The results show a clear distinction between the invocation frequency of small and large functions resulting in a 4–6.5$\times$ the number compared to large functions at any given time of the day. 

\begin{figure}[h]
    \centering
    \includegraphics[width=1\columnwidth]{New_Invo_Paterns.pdf}
    \caption{Normalized invocation trends for small and large functions.}
    \label{fig:traffic_frequency}
\end{figure}


\subsubsection{Invocation Patterns and Inter-Arrival Times (IATs)}

We analyzed inter-arrival times (IATs) for Azure Functions using a sliding window approach. This method computed IATs within defined time windows (default: 60 minutes) with overlapping intervals (30 minutes) for smooth transitions. Outliers were filtered using a Z-score threshold to remove anomalies.

The average IAT distribution for small and large functions is shown in Figure~\ref{fig:iat_distribution}. The results reveal that:
\begin{itemize}
   \item Large functions invoke at similar or better intervals than small functions, especially at higher percentiles.
   \item Despite similar periodicity, the sheer volume of small functions leads to resource contention, exacerbating cold starts for large containers.
\end{itemize}

\begin{figure}[h]
   \centering
   \includegraphics[width=1\columnwidth]{new_IAT.pdf}
   \caption{Percentile distribution of inter-arrival times for small and large functions.}
   \label{fig:iat_distribution}
\end{figure}

\subsubsection{Cold Start Latency and Resource Contention}
Prioritizing caching of small functions is essential because (i)~small functions experience cold start delays if not prioritized for caching, and (ii)~caching large functions instead of small functions results in excessive resource contention since these functions not only consume large amount of memory but also have longer runtimes. 

However, it is essential to ensure proper caching of large functions also because large functions tend to have longer cold start delays. 
To demonstrate this, we analyze the cold start latency of large vs. small functions and plot the percentile distribution of latency in Figure~\ref{fig:cold_start_latency}. 
The distribution reveals that the large functions have longer latency with large functions exhibit latency of upto 100 seconds, compared to upto 15 seconds for small functions at the 85th percentile.


\begin{figure}[h] 
    \centering
    \includegraphics[width=1\columnwidth]{New_Cold.pdf}
    \caption{Percentile distribution of cold start latency for small and large functions.}
    \label{fig:cold_start_latency}
\end{figure}




