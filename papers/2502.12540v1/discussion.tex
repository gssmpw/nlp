\section{Discussions and Conclusions}\label{sec:discuss}
In edge environments (4--8 GB), KiSS effectively reduced cold start percentages and drops compared to the baseline. Small containers showed cold start reductions of up to 30\%, while large containers achieved a 33\% improvement. Similarly, drops were nearly halved for small containers (47\%) and reduced by 40\% for large containers at 8 GB. These gains stem from workload-aware partitioning, which isolates resource pools to prioritize small, high-frequency containers.

While the static 80-20 split performed well, slight increases in drops were observed in very low memory ranges (2--3 GB) due to partitioning constraints. This trade-off suggests that adaptive partitioning could further optimize resource allocation under extreme constraints.

\subsection{Fairness and Resource Efficiency}

KiSS achieved equitable improvements for both small and large containers. Speedup ratios remained consistent across memory configurations, reflecting balanced resource utilization without overloading or underutilizing memory. This is particularly important for edge environments, where non-server-grade hardware requires careful resource management to avoid risks like thermal throttling and hardware wear.

\subsection{Implications for Edge FaaS Deployments}

The findings position KiSS as a robust solution for edge environments. Its ability to reduce cold starts, request drops, balancing fairness and preventing over-utilization, makes it particularly suitable for latency-sensitive applications such as IoT event processing and real-time analytics. 


\subsection{Opportunities for Further Optimization}

While KiSS performed strongly, its reliance on static partitioning presents opportunities for improvement. Adaptive partitioning informed by real-time workload monitoring could address the observed trade-offs in very low memory ranges. Additionally, testing KiSS under highly variable and bursty traffic patterns would provide a broader perspective on its applicability. Incorporating reinforcement learning-based caching strategies could also enhance its ability to adapt dynamically to workload shifts.

The static 80-20 split in this evaluation serves as a representative configuration to assess the benefits of partitioning in a simulated environment. Future studies could explore adaptive partitioning strategies that dynamically adjust memory allocation in response to changing workload demands.
