\section{Design}\label{sec:design}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{figure*}[t]
    \centering
    \includegraphics[trim = 15 530 15 15, width=1\textwidth]{Algorithm_drawio.pdf}
    \caption{Overview of KiSS}
    \label{fig:overview}
\end{figure*}


The results we gleaned from the previous section (see Section~\ref{sec:work_anly}) helped in developing our policy: KiSS. The KiSS or \textbf{Keep it Separated Serverless} policy aims to address critical challenges in Function-as-a-Service (FaaS) platforms, particularly in edge computing environments, by achieving the following objectives:

\begin{itemize}
    \item \textbf{Reduced Cold Start Latency:} Prioritizes high-frequency functions to minimize delays in real-time applications.
    \item \textbf{Improved Resource Efficiency:} Optimizes memory and compute usage while avoiding unnecessary overhead from static warm states.
    \item \textbf{Minimized Inter-Function Interference:} Enhances throughput and scalability through modular resource partitioning.
    \item \textbf{Improved Function Service Rate:} Adopts resource-aware policies to reduce dropped requests and maximize system reliability.
\end{itemize}


\subsection{KiSS Policy Overview}

KiSS introduces a modular, data-driven orchestration strategy designed to optimize serverless execution in resource-constrained environments, particularly at the edge. By leveraging our workload analysis (refer Section 2.5), our policy segments functions based on key metrics—memory footprint, invocation frequency, and execution time—to optimize performance across diverse workloads.

The edge computing context introduces unique challenges like limited memory, heterogeneous resources, and dynamic workloads. Generalized cloud strategies often fail to adapt to such constraints. KiSS addresses this gap by analyzing workload characteristics and implementing a resource-efficient, modular strategy that aligns with edge-specific demands.

\subsection{Components of KiSS Policy Design}
Figure~\ref{fig:overview} shows the overall architecture of KiSS. 
The incoming \textit{FaaS traffic} will include both small and large functions. 
The \textit{request handler} accepts the incoming functions and shares the function information to the workload analyzer. 
The \textit{workload analyser} processes the function information to profile the incoming function traffic information and generate data such as invocation frequency, memory footprint etc.
The \textit{KiSS policy} uses this data to estimate where this function will be placed between the two different warm pool partitions.

The \textit{load balancer} implements a partitioning logic where functions are allocated to distinct warm pools using (\textit{invoker 1} and \textit{invoker 2}) based on profiling thresholds:

(i)~Small Functions Pool: Dedicated to high-frequency, low-memory functions to ensure low latency, and (ii)~Large Functions Pool: Allocated for low-frequency, memory-intensive functions, minimizing contention with smaller containers.
Each warm pool operates autonomously achieving Policy Independence.
The \textit{Warm Pool Replacement Policy} for each warm container pool can independently implement different workload-specific strategies to reduce contention and enhance temporal locality.


These factors form the foundation of KiSS’s multi-tiered warm pool framework, allowing it to effectively manage serverless resources and enhance performance in edge computing. By addressing these challenges, KiSS positions itself as a practical and scalable solution for FaaS platforms in environments with diverse and demanding resource constraints.


\subsection{Innovations of KiSS Policy}

One of the most innovative features of KiSS is its multi-level warm pool partitioning, which isolates high- and low-frequency functions into separate pools. This design eliminates inefficiencies inherent in monolithic resource strategies by ensuring that small, frequently invoked functions are always ready to execute, while larger, less frequent functions remain accessible without competing for resources. This adaptability extends to the ability to add more pools as workload patterns evolve, making KiSS a flexible and future-proof solution. Moreover, its modular architecture supports diverse deployment scenarios, from centralized clouds to resource-constrained edge environments. Integration with traffic-aware schedulers ensures that KiSS maintains scalability and responsiveness even under fluctuating workloads.


\subsubsection{Advantages of KiSS}

The advantages of KiSS are particularly pronounced in edge environments. By keeping frequently accessed containers in warm states, it drastically reduces cold start latency, which is critical for real-time applications such as IoT and AI analytics. Static warm pool partitioning, based on workload analysis, optimizes memory usage by eliminating unnecessary overhead, ensuring that resources are used efficiently even in environments with stringent memory constraints. This strategy not only enhances performance but also reduces operational costs by consolidating memory usage and minimizing cold starts. KiSS’s platform-agnostic design further enhances its versatility, enabling seamless deployment across various serverless frameworks.

