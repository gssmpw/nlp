\section{Implementation}\label{sec:implement}

The effectiveness of the KiSS policy relies on fine-tuning three critical parameters: container size thresholds, memory size per pool, and total memory allocation, explained in the following section. 

\subsection{Tuning Parameters}

Parameters were systematically refined within a simulation environment to optimize performance, minimize resource contention, and effectively address cold start latency.

\subsubsection{Container Size Thresholds}
The categorization of containers into \textit{small} and \textit{large} based on memory footprint was central to our policy's design. The calibration process for container size thresholds considered our central paradigm of keeping functions separated (low and high frequency). The calibration process involved:

\begin{itemize}
    \item \textbf{Empirical Benchmarking:} Initial thresholds were derived from the distribution of function memory footprints, aligning with observed workload patterns.
    \item \textbf{Simulation Validation:} Various threshold values were tested to evaluate their impact on latency, memory usage, and throughput.
    \item \textbf{Dynamic Adjustments:} Iterative simulations ensured that small containers achieved low-latency performance while large containers benefited from efficient inter-function interference handling mechanism.
\end{itemize}


\subsubsection{Memory Size Per Pool}
Allocating appropriate memory resources for each warm pool was critical to balancing readiness and efficiency:
\begin{itemize}
    \item \textbf{Small Container Pool:} This pool was allocated a larger cache share to accommodate high-frequency, latency-sensitive functions, with dynamic adjustments made to handle traffic spikes and ensure low-latency performance.
    \item \textbf{Large Container Pool:} A smaller cache allocation was designated for this pool, relying on snapshot-based provisioning to manage sporadic invocations without consuming excess memory resources.
\end{itemize}

The tuning process for cache size included:
\begin{itemize}
    \item \textbf{Hit Rate Optimization:} The simulator tracked container warm pool hit rates for each pool under varying workloads to identify the optimal allocation that minimized cold start occurrences.
    \item \textbf{Latency Analysis:} Observing response times helped fine-tune memory-footprint distributions, ensuring resource efficiency without compromising performance.
\end{itemize}

\subsubsection{Total Memory Size}
The overall memory pool capacity was calibrated to ensure function readiness without over-provisioning. This helps us to study different resource environments, like Edge:
\begin{itemize}
    \item \textbf{Stress Testing:} Workload bursts traces were tested memory size adequacy under varying demand.
    \item \textbf{Iterative Refinement:} Adjustments balanced latency reduction and memory utilization, ensuring consistent performance across diverse workloads.
\end{itemize}


\subsection{Metrics for Performance Evaluation}
The simulator tracks six key metrics to quantify performance across diverse configurations:
\begin{enumerate}
    \item \textbf{Cold Starts (Misses):} Instances where a new container must be initialized because no matching container exists in the resource pool.
    \item \textbf{Hits:}  Function invocations that successfully utilize an existing container, avoiding a cold start.
    \item \textbf{Drops:} Scenarios where a missed function cannot allocate a new container due to all containers being actively utilized. This extended metric offers deeper insights into resource contention.
    \item \textbf{Total Accesses:} The total number of function invocations, encompassing hits, misses, and drops.
    \item \textbf{Serviceable Accesses:} Invocations that were successfully serviced, combining hits and misses.
    \item \textbf{Execution Durations:} The cumulative execution time for all functions, calculated from cold start and warm start durations.
\end{enumerate}

These metrics provide a comprehensive framework for evaluating resource allocation efficiency and enable rigorous comparisons with baseline setups and existing state-of-the-art methods.

