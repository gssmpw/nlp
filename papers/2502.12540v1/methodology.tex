\section{Methodology}\label{sec:method}

In this section, we outline the experimental setup, and the methodology employed by KiSS to evaluate cold start metrics.

\subsection{Simulation Environment}
We develop an enhanced and modified version of the \textbf{FaaSCache Simulator}~\cite{alexander_faascache_2021} to evaluate the KISS framework, tailoring it to address serverless resource management challenges in low resource constraint environments. This discrete event simulator models Function-as-a-Service (FaaS) systems as a dynamic warm pool, enabling analysis of cold start mitigation and resource allocation strategies. Key modifications support KiSS’s modular design and workload-specific approach.

The simulation is conducted across memory pool sizes ranging from 1 GB to 80 GB to capture diverse deployment scenarios, from low-resource edge nodes to well-provisioned cloud setups. For this study, results focus on the 1–24 GB range, as beyond that point resources are not heavily constrained.



Our approach provides a controlled and reproducible environment for testing KiSS across a broad range of workloads, enabling detailed insights into its performance and applicability.



The static 80-20 split in this evaluation serves as a representative configuration to assess the benefits of partitioning in a simulated environment.
Future studies could explore adaptive partitioning strategies that dynamically adjust memory allocation in response to changing workload demands.

\subsection{Workloads and Traffic Patterns}

Workload evaluation for the KiSS framework was based on trace derived from the 2019 Azure Function trace dataset~\cite{mohammad_serverless_2020}. This dataset provides detailed traffic patterns for serverless computing workloads required for this study. While no real-world edge-specific FaaS trace is publicly available, our assumption is that core properties of FaaS workloads remain consistent when extended to edge environments. Leveraging this assumption, we adapted the Azure trace to reflect the unique characteristics of edge deployments.

Container memory sizes were adjusted to align with typical edge constraints, with small containers ranging between 30--60 MB and large containers between 300--400 MB. The overall memory pool sizes were constrained to a range of 1--24 GB to reflect edge-specific hardware limitations. Invocation patterns and packet sizes were adapted to represent edge environments, focusing on smaller, frequent invocations (e.g., IoT event streams) alongside less frequent, resource-intensive tasks (e.g., video analytics).


\paragraph{Workload Diversity}
The synthesized trace allowed for the evaluation of KiSS under diverse conditions, including:
\begin{itemize}
    \item \textbf{High-Frequency Functions:} Representing lightweight, frequently invoked tasks such as IoT data processing.
    \item \textbf{Low-Frequency Functions:} Reflecting resource-intensive workloads like batch processing or analytics.
    \item \textbf{Bursty Traffic Patterns:} Simulating real-world traffic spikes, critical for understanding the framework’s behavior under sudden load surges.
    \item \textbf{Steady-State Operations:} Providing a baseline for performance under consistent invocation patterns.
\end{itemize}

By adapting the Azure trace to mimic edge-specific features, this evaluation captures a realistic approximation of how FaaS platforms would operate in constrained edge environments. This methodology bridges the gap between existing public cloud traces and the unique characteristics of edge deployments, ensuring that the insights gained are relevant to real-world applications.

\subsection{Evaluation Metrics}

The KiSS framework’s performance was evaluated using the following metrics:
\begin{itemize}
    \item \textbf{Cold Start Percentage:} The proportion of invocations requiring container initialization, critical for latency-sensitive applications.
    \item \textbf{Drop Percentage:} The proportion of invocations dropped due to memory contention, analyzed separately for small and large containers.
      \item \textbf{Fairness:} Performance consistency across small and large containers, ensuring equitable resource allocation.
\end{itemize}



\subsection{Fairness Analysis}

Fairness in resource allocation is a critical consideration for KiSS, as it ensures that both high-frequency (small) and low-frequency (large) containers receive equitable access to resources. Without careful planning, partitioned memory pools could lead to imbalances where one category of functions dominates resources. To address this, we conducted a fairness (equity) analysis to validate the effectiveness of the KiSS design in maintaining balance and meeting diverse service requirements.

The analysis focused on three key aspects. First, we evaluated equity in resource distribution to ensure that both small and large containers achieve comparable performance improvements. Second, we examined the avoidance of resource monopolization, ensuring that high-frequency small containers do not dominate memory resources at the expense of resource-intensive workloads. Lastly, we assessed support for diverse Quality of Service (QoS) requirements, validating that both latency-sensitive and resource-heavy functions could meet their respective performance goals.

This fairness analysis was performed by comparing cold start percentages and drop percentages across small and large containers. By doing so, we aimed to ensure that the KiSS framework delivers consistent and balanced performance across all workload categories, aligning with our goal of optimizing serverless execution in diverse and resource-constrained environments.



Fairness was assessed by comparing cold start percentages, drop percentages, and Policy performance across small and large containers.

\subsection{Policy Evaluation and Baseline Comparison}

The KiSS framework was evaluated with three different caching policies, alongside a unified warm pool as the baseline configuration. These evaluations aimed to validate the modularity and adaptability of KiSS across diverse resource management strategies:
\begin{itemize}
    \item \textbf{Least Recently Used (LRU):} The default policy, applied uniformly in the baseline and within partitioned memory pools for KiSS~\cite{jonas_cloud_2019,alexander_faascache_2021}.
    \item \textbf{Greedy Dual(GD):} A Greedy Dual policy inspired by FaaSCache~\cite{alexander_faascache_2021} that incorporates multiple features like invocation frequency and memory footprint to make eviction decisions.
    \item \textbf{Frequency-Based (Freq):} A policy that prioritizes caching for frequently invoked functions, irrespective of resource type~\cite{alexander_faascache_2021}.
\end{itemize}

The baseline configuration used a unified warm pool with the LRU caching policy, treating all containers equally. KiSS was tested with the same policies (LRU, GD, and Freq) to measure the relative benefits of its partitioned architecture.




