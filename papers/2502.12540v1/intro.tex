\section{Introduction}\label{sec:intro}

Serverless computing simplifies deploying event-driven applications by removing infrastructure management. Platforms such as AWS Lambdas~\cite{aws_lambda_run_2024}, Google Cloud Functions~\cite{google_cloud_google_2024}, and Azure Functions~\cite{miscrosoft_ignite_azure_2023} dynamically allocate resources to meet fluctuating demand, making them particularly attractive for workloads that are unpredictable~\cite{ao_faasnet_2021,ustiugov_benchmarking_2021}. These economic efficiencies reduce costs associated with idle resources, driving widespread adoption across industries. Serverless has enabled diverse applications, from large-scale data processing and machine learning to real-time analytics and IoT (Internet of Things). According to Gartner, global cloud spending increased by over 20\% in 2022, highlighting the scalability and flexibility offered by serverless computing and other cloud-native models.

Despite its scalability and flexibility, traditional serverless computing faces critical performance challenges, particularly in high-demand and emerging edge node scenarios. Cold start delays disrupt real-time responsiveness in latency-sensitive applications such as financial transactions and IoT analytics~\cite{alexander_faascache_2021,ustiugov_benchmarking_2021}. To address these issues in typical high-demand, cloud scenarios, solutions such as caching (e.g., FaaSCache, SONIC)~\cite{ashraf_sonic_2021, alexander_faascache_2021} and snapshot-based state restoration (e.g., FaaSnap, Catalyzer)~\cite{ao_faasnap_2022, dong_catalyzer_2020} have been proposed to reduce initialization delays. Techniques like predictive pre-warming and hardware-optimized memory access (e.g., REAP)~\cite{ustiugov_benchmarking_2021} also show promise but often rely on specialized infrastructure, limiting their adaptability to other environments. These challenges, while partially mitigated in cloud settings, become significantly more pronounced in resource-constrained edge environments.

An emerging application space for the FaaS model lies in extending function execution to the edge node for quicker response times and greater data privacy~\cite{rausch_optimized_2021, xie_when_2021}. Edge FaaS, which brings computation closer to data sources, is increasingly adopted for latency-sensitive applications such as industrial IoT, autonomous systems, and smart cities. However, edge environments lack the computational slack of cloud data centers to maintain large pre-warmed pools or deploy infrastructure-heavy optimizations. When it comes to heterogeneous devices, the diversity in hardware capabilities and network conditions across edge nodes complicates function placement and resource scheduling. Unlike cloud setups, edge nodes cannot over-provision resources to accommodate workload surges, instead functions which cannot execute immediately must be punted up to the cloud, obviating the gains of edge FaaS. Thus adaptive scaling in the edge node FaaS is critical.



\begin{figure}[tb!] 
    \centering 
    \includegraphics[trim = 15 350 15 15, width= 1\columnwidth]{KiSS_Scenario_Mockup_drawio.pdf} 
    \caption{Memory contention in warm pools: (a) Large containers displace small containers, disrupting locality and increasing cold starts; (b) Small containers dominate due to high invocation ratio starving Large containers.} 
    \label{fig:intro_pools} 
\end{figure}
\subsection{Inter-Function Memory Contention: A Key Issue}
Among the challenges in Edge environments, inter-function memory contention stands out as one of the most significant problems. Warm memory pools, designed to minimize cold starts, are often dominated by small containers due to their higher invocation frequency. This imbalance restricts large containers from entering the warm pool, leading to frequent cold starts. Conversely, when large containers manage to enter the warm pool, they displace multiple small containers, disrupting their temporal locality and resulting in cascading cold starts for frequently invoked small functions.

Furthermore, function chaining, a common pattern in serverless workflows,\textbf{ exacerbates this issue}. Research on chaining frameworks such as Xanadu~\cite{daw_xanadu_2020} and SpecFaaS~\cite{stojkovic_specfaas_2023} highlight the importance of maintaining temporal locality in warm pools to avoid unnecessary cold start penalties. Without effective memory management, the performance of interconnected functions deteriorates, undermining the responsiveness of latency-sensitive applications.

As illustrated in Figure~\ref{fig:intro_pools}, this dynamic leads to cascading inefficiencies. Small containers, which dominate warm pools, exclude larger containers, forcing them into frequent cold starts. When larger containers enter the pool, they displace numerous small containers, degrading their performance by increasing cold start percentages and disrupting their temporal locality. Addressing this imbalance is critical for improving latency-sensitive applications in edge environments. 

In this paper we introduce a new, FaaS memory management policy cognizant of the resource constraints of FaaS in the edge node.  \textit{KiSS} (Keep It Separated Serverless) breaks the available memory into two pools, preventing the interference between large, less frequently used containers and smaller, higher locality containers.  The paper makes the following contributions:
\begin{itemize}
    \item The first work to examine memory management for FaaS in highly resource constrained, edge node systems and identify the interaction between larger and smaller containers in these environments.
    \item Introduces a new, container size aware, memory management policy for edge node FaaS systems.
    \item Shows that KiSS reduces cold start latency by up-to 60\% while also reducing the number of times functions must be dropped (pushed to the cloud for execution) by up-to 56.5\%.
\end{itemize}

