\documentclass[11pt]{article}
\usepackage[sfdefault]{carlito}

% Define Packages
\usepackage[margin=1in]{geometry}  % set the margins to 1in on all sides
\usepackage[singlespacing]{setspace}
\usepackage{epsfig}
\usepackage{graphics,graphicx,color}
\usepackage{amssymb,amsfonts,amsthm,amsmath}
\usepackage{authblk}
\usepackage{multirow}
\usepackage{mathrsfs}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{natbib}
\usepackage{float}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{subcaption} 
\usepackage{url}
\usepackage[colorlinks=true, linkcolor=blue, citecolor=blue]{hyperref}
\usepackage{bm}
\usepackage{verbatim}
\usepackage{booktabs}
\usepackage{makecell}

%\linespread{1.5}

\newtheorem{theorem}{Theorem}
\newtheorem{example}{Example}%[section]
\newtheorem{lemma}{Lemma}%[section]
\newtheorem{definition}{Definition}%[section]
\newtheorem{proposition}{Proposition}%[section]
\newtheorem{corollary}{Corollary}%[section]
\newtheorem{assumption}{Assumption}%[section]
%\newtheorem{algorithm}{Algorithm}[section]
\newcommand{\diag}{\mbox{diag}}
\newcommand{\dd}{\mbox{d}}
\newcommand{\cov}{\mbox{cov}}
\newcommand{\var}{\mbox{var}}
\newcommand{\tr}{\mbox{tr}}
\newcommand{\dif}{{\rm d}}
\newcommand{\vz}{\boldsymbol{z}}
\newcommand{\va}{\boldsymbol{a}}
\newcommand{\vb}{\boldsymbol{b}}
\newcommand{\mH}{{\mathsf H}}
\newcommand{\mS}{{\mathsf S}}
\newcommand{\mT}{{\mathsf T}}
\newcommand{\mI}{{\mathsf I}}
\newcommand{\mX}{{\mathsf X}}
\newcommand{\mF}{{\mathsf F}}
\newcommand{\mG}{{\mathsf G}}
\newcommand{\vx}{\boldsymbol{x}}
\newcommand{\vm}{\boldsymbol{m}}
\newcommand{\vmu}{\boldsymbol{\mu}}
\newcommand{\vd}{\boldsymbol{d}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\E}{E}
\newcommand{\bluetext}[1]{\textcolor{blue}{#1}}
\newcommand{\redtext}[1]{\textcolor{red}{#1}}
\newcommand{\greentext}[1]{\textcolor{green}{#1}}

\graphicspath{ {./Figures/} }

\title{Optimal Kernel Learning for Gaussian Process Models with High-Dimensional Input}
\author[1]{Lulu Kang}%\thanks{lulukang@umass.edu}}
\author[1]{Minshen Xu}
\affil[1]{Department of Mathematics and Statistics, University of Massachusetts Amherst.}

\begin{document}

\date{}
\maketitle

\begin{abstract}
Gaussian process (GP) regression is a popular surrogate modeling tool for computer simulations in engineering and scientific domains. 
However, it often struggles with high computational costs and low prediction accuracy when the simulation involves too many input variables. 
For some simulation models, the outputs may only be significantly influenced by a small subset of the input variables, referred to as the ``active variables''. 
We propose an optimal kernel learning approach to identify these active variables, thereby overcoming GP model limitations and enhancing system understanding.
Our method approximates the original GP model's covariance function through a convex combination of kernel functions, each utilizing low-dimensional subsets of input variables.
Inspired by the Fedorov-Wynn algorithm from optimal design literature, we develop an optimal kernel learning algorithm to determine this approximation. 
We incorporate the effect heredity principle, a concept borrowed from the field of ``design and analysis of experiments'', to ensure sparsity in active variable selection.
Through several examples, we demonstrate that the proposed method outperforms alternative approaches in correctly identifying active input variables and improving prediction accuracy.
It is an effective solution for interpreting the surrogate GP regression and simplifying the complex underlying system.

\noindent{\bf Keywords}: Additive Gaussian Process Model, Functional ANOVA, Heredity Principle, High-Dimensional Input, Kernel Function, Optimal Design.
\end{abstract}


\section{Introduction}\label{sec:intro}

Gaussian process (GP) regression is a powerful surrogate modeling approach for both computer and physical experiments characterized by highly nonlinear input-output relationships. 
However, like many supervised learning methods, GP models face the ``curse of dimensionality'' when dealing with high-dimensional input data and relatively small sample sizes--the so-called ``large $p$ small $n$'' problem. 
This challenge is particularly acute when estimating unknown parameters of an anisotropic covariance function in the GP model \citep{joseph2011regression,liu2020gaussian}.
Fortunately, many computer simulation models and physical systems are inherently influenced by only a small subset of input variables, known as ``active variables'' or ``active dimensions''. 
Identifying these variables can alleviate computational bottlenecks and improve the prediction accuracy of GP models. 
%In this paper, we propose a novel approach for GP models that approximates the covariance kernel function with the original high-dimensional input space via a convex combination of kernel functions of lower-dimensional input spaces. 
%Essentially, it is equivalent to approximating the original GP of the high-dimensional input space by the functional ANOVA decomposition, which leads to an additive GP model and each added component is a GP of low-dimensional input space. 

\subsection{Related Literature}\label{subset:literature}

Researchers have proposed various methods to address the issue of high input dimensionality in GP models. 
Each approach has unique strengths and limitations, with performance varying depending on the application and data. 
These approaches can be broadly categorized into three groups.

The first group of methods uses automatic relevance determination and sensitivity analysis \citep{moon2012two,borgonovo2014transformations, Iooss2017introduction}. 
The most influential variables are kept by ranking the input variables based on their influence on the response variable, and the rest are removed. 
However, they typically require an estimated surrogate model before the dimension reduction step. 

The second class of methods seeks to build the GP model on a linear transformation of the input space \citep{constantine2014active}, i.e., $Y(\bm x)=Z(\bm B^\top \bm x)+\epsilon$, where $Z(\cdot)$ is a GP with unknown parameters and $\bm B$ is full-rank and of dimension $p\times d$ such that $\bm B^\top $ projects the original $p$-dimensional vector $\bm x$ onto a lower $d$-dimensional space with $d<p$.
The number $d$ is usually user-specified. 
The column space of $\bm B$ is referred to as the ``active subspace'', which corresponds to the projected directions in which $\bm x$ has the most influence on $\bm y$.
It is preferred that the columns $\bm B$ are orthonormal to avoid identifiability issue, and thus $\bm B$ is restricted on a certain manifold \citep{tripathy2016gaussian, seshadri2018dimension}. 
The GP single-index models \citep{choi2011gaussian, gramacy2012gaussian} is a special case of the active subspace approach with $d=1$.
While effective, these methods often require significant computation and sometimes gradient information of the underlying unknown function \citep{constantine2014active, fukumizu2014gradient}.

The third group of methods is based on functional ANOVA decomposition, such as two recent works by \cite{borgonovo2018functional} and \cite{sung2019multiresolution}. 
As a popular statistical tool, the functional ANOVA decomposition provides an approximation of the underlying unknown function $Y(\bm x)$ as a linear combination of basis functions, each involving only one or two input variables. 
While this approach can handle high-dimensional input variables, including more complex basis functions inevitably increases computational demands.


%For example, if the underlying function $Y(\bm x)$ truly resides in an active subspace of $\bm B\bm x$, then any functional ANOVA decomposition method will not achieve the same level of approximation accuracy as the active subspace methods. 
%But the assumption on the active subspace is indeed restrictive. 
%If it does not hold, then a proper functional ANOVA decomposition is more likely to better approximate $Y(\bm x)$ as long as sufficient low-dimensional basis functions are included. 
%In this paper, we pursue the direction of functional ANOVA decomposition.

\subsection{Our Contribution}
In this paper, we introduce a novel approach to building GP regression models. 
As reviewed in Section \ref{sec:review}, the computational bottleneck for estimating the GP model lies in the covariance kernel function.
The commonly used anisotropic covariance kernel function, also known as the product covariance function, contains the same number of unknown parameters as the input dimension. 
Therefore, compared to a low input dimension problem, with too many input variables, the optimization procedure to obtain the maximum likelihood estimates would take many more iterations to converge, which would result in a much larger computational cost, considering the matrix inversion and determinant are re-calculated in each optimization iteration. 
More importantly, the estimated parameters are likely to be local optimal solutions due to the high dimensionality. 

Recognizing this bottleneck, we propose to approximate the covariance kernel function of the original high-dimensional input variables via a convex combination of kernel functions of lower-dimensional input variables. 
This is equivalent to approximating the original GP of the high-dimensional input space by functional ANOVA decomposition, which leads to an additive GP model with each added component being a GP of low-dimensional input space. 

The low-dimensional kernel functions are identified by minimizing the regularized loss function. 
As explained in Section \ref{sec:learning}, given a compact space of kernel functions, 
 \cite{micchelli2005learning} has proved the existence of the optimal convex combination of kernel functions, and there are only a finite number of kernel functions with non-zero weights. 
Inspired by a similar optimization problem and the Fedorov–Wynn-type of algorithm in \emph{optimal design} \citep{dean2015handbook, li2022maximin}, we propose an optimal kernel learning algorithm that sequentially selects kernels from the candidate set and simultaneously updates the weights of all selected the kernels.
For high-dimensional input data, the candidate set of low-dimensional kernels can be large, so the computation needed to prepare the candidate kernels also increases drastically. 
To overcome this issue, we follow the \emph{effect heredity principle} from the design and analysis of experiments (DAE) literature \citep{hamada1992analysis, wu2011experiments} and construct the candidate set of kernel functions stage-wise based on the previously selected input variables. 
This work showcases the potential of applying traditional concepts from DAE literature to modern statistical learning problems, bridging established methodologies with contemporary challenges in data analysis and modeling.

The paper is organized as follows. 
In Section \ref{sec:review}, we review the GP regression from the parametric stochastic modeling perspective and nonparametric kernel method perspective. 
Section \ref{sec:learning} explains the discrete nature of the kernel learning problem. 
In Section \ref{sec:opt} and \ref{sec:low}, we develop the forward stepwise optimal kernel learning approach with theoretical convergence and then use it to learn the optimal kernel from the set of low-dimensional basic kernel functions. 
In Section \ref{sec:num}, we demonstrate that the proposed method can accurately identify the active input dimensions and improve prediction accuracy. 
We conclude the paper in \ref{sec:end} with a discussion of future works. 

\section{Review of the GP Model}\label{sec:review}

GP regression is a parametric stochastic model. 
Interestingly, its simplest form is equivalent to the regularized reproducing kernel Hilbert space regression, a nonparametric kernel method. 
In this section, we define the notation and review GP regression from both perspectives.  

\subsection{Stochastic GP model}\label{subsec:statGP}

A Gaussian process is a stochastic process or a random function such that every finite collection of function values evaluated at different input design points has a multivariate normal distribution.
Any GP can be completely characterized by a mean and a covariance function.
Given the input domain $\Omega \subset \mathbb{R}^p$, the simplest stationary GP model assumes the response $Y(\bm x)$ for any $\bm x\in \Omega$ as follows,
\begin{equation}\label{eq:GP}
Y(\bm x)=Z(\bm x)+\epsilon
\end{equation}
where $Z(\bm x)\sim GP(0,\tau^2K_{\bm \theta}(\cdot,\cdot))$ and $\epsilon\sim N(0,\sigma^2)$. 
The two are independent of each other.
The correlation function of $Z(\bm x)$ is $K_{\bm\theta}(\cdot,\cdot): \Omega \times \Omega \rightarrow \mathbb{R}^{+}$, a positive definite kernel function with unknown parameters $\bm \theta$. 
A commonly used kernel function is the separable Gaussian kernel function $K_{\bm\theta}(\bm x_1,\bm x_2)=\exp(-\sum_{i=1}^p\theta_i(x_{1,i}-x_{2,i})^2)$. 
This kind of separable kernel functions, or product covariance functions, are \emph{anisotropic}. 
An \emph{isotropic} Gaussian kernel have the same $\theta_i$'s, i.e., $K_{\theta}(\bm x_1,\bm x_2)=\exp(-\theta\sum_{i=1}^p(x_{1,i}-x_{2,i})^2)$.
The methodology developed in this paper can be used for other kernel functions, such as the Mat\'{e}rn kernel function, a popular choice especially for spatial statistics. 


Suppose we observe data $\{\bm x_i,y_i\}_{i=1}^n$. 
Denote $\mathcal{X}=\{\bm x_i\}_{i=1}^n\subset \Omega$ as $n$ design points, $\bm y=[y_1,\cdots,y_n]^\top$ is the vector of $n$ output observations, and $\bm K$ as the $n\times n$ kernel matrix with $K_{i,j}=K_{j,i}=K(\bm x_i,\bm x_j)$. 
Based on the assumption in \eqref{eq:GP}, the conditional distribution of $Y(\bm x)$ at any query point is $Y(\bm x)|\bm y, \bm \theta, \tau^2, \sigma^2 \sim \mathcal{N}(\hat{Y}(\bm x), \sigma^2_{\bm x|n}(\bm x))$. 
The conditional mean is 
\begin{equation}\label{GP-prediction}
\hat{Y}(\bm x)=\E(Y(\bm x)|\bm y)=\bm k(\bm x)^\top(\bm K+\eta\bm I_n)^{-1}\bm y,
\end{equation}
where $\bm k(\bm x)=[K(\bm x, \bm x_1),\ldots, K(\bm x, \bm x_n)]^\top$ are the correlations between $Y(\bm x)$ and $Y(\bm x_i)$'s, $\eta=\sigma^2/\tau^2$, and $\bm I_n$ is the $n\times n$ identity matrix. 
The conditional variance is
\[
\sigma^2_{\bm x|n}(\bm x)=\var(\hat{Y}(\bm x)|\bm y)=\tau^2\left\{1-\bm k(\bm x)^\top(\bm K+\eta \bm I_n)^{-1}\bm k(\bm x)\right\}. 
\]
Based on the conditional distribution, the prediction and inference at any query point can be obtained. 

The unknown parameters $\tau^2$, $\sigma^2$ and $\bm\theta$ can be estimated using maximum likelihood estimation (MLE). 
For physical experiments or stochastic computer experiments, $\sigma^2$ is estimated from replicated output observations at the same design point.
For deterministic computer experiments, since no measurement noise is involved, $\sigma^2$ should be specified as $0$, and thus $\eta=0$. 
In this case, the predictor in \eqref{GP-prediction} interpolates all the training data.
However, it is usually advantageous to specify a small positive value for $\eta$ known as the \emph{nugget} value \citep{peng2014choice} to overcome the possible ill-conditioning of the kernel matrix $\bm K$. 
%The ill-conditioning can occur due to a combination of factors including sample size, location of the design points, and $\bm \theta$ values. 
Given $\bm \theta$ and $\eta$, the maximum likelihood estimator (MLE) for $\tau^2$ is $\hat\tau^2=\frac{1}{n}\bm y^T(\bm K+\eta\bm I)^{-1}\bm y$. 
The MLE for $\bm\theta$ and $\eta$ are the solution of 
\[\min_{\bm\theta, \mu} n\log\hat{\tau}^2+\log\det(\bm K+\eta\bm I).\]
The minimization is computationally demanding as it needs to compute $(\bm K+\eta\bm I)^{-1}$, $\log \det(\bm K+\eta\bm I)$, and the gradient of the objective function in each iteration. 
The complexity of evaluating the gradient of the objective is $O((p+1)n^3)$, which increases with respect to (w.r.t.) the input dimension $p$.

\subsection{GP model as a regularized RKHS regression}\label{subsec:RKHS}

For every positive definite kernel function $K(\cdot,\cdot)$ defined on the domain $\Omega\subseteq\mathbb{R}^p$, let $\mathcal{F}$ be the set of functions $f:\Omega\rightarrow\mathbb{R}$ such that $f(\cdot)=\sum_{i=1}^n c_iK(\bm x_i,\cdot)$, for any positive integer $n$, any $\bm c\in\mathbb{R}^n$, and any $\mathcal{X}=\{\bm x_1,\cdots,\bm x_n\}\subseteq\mathbb{R}^p$.
Therefore, $\mathcal{F}$ is a linear space.
In addition, for any $f,g\in\mathcal{F}$ with $f=\sum_{i=1}^n c_iK(\bm x_i,\cdot)$ and $g=\sum_{j=1}^n d_iK(\bm y_j,\cdot)$, define the bilinear operator
\begin{equation} \label{inner-product}
<f,g>=\sum_{i=1}^n\sum_{j=1}^nc_id_j K(\bm x_i,\bm y_j).
\end{equation}
It can be easily shown that \eqref{inner-product} defines an inner product on $\mathcal{F}$ \citep{wahba1990spline, wendland2004scattered}. 
The completion of $\mathcal{F}$ with the inner product forms a Hilbert space, which is called the reproducing kernel Hilbert space (RKHS) induced by the kernel $K(\cdot,\cdot)$. 
Conventionally, it is denoted by $\mathcal{H}_K$. 
It satisfies the following two conditions:
\begin{enumerate}
\item For any $\bm x\in\Omega$, we have $K(\bm x,\cdot)\in\mathcal{H}_K$;
\item For any $f\in\mathcal{H}_K$ and any $\bm x\in\Omega$, we have $\langle f,K(\bm x,\cdot)\rangle_{\mathcal{H}_K}=f(\bm x)$, also known as reproducing property. 
Here $\langle\cdot,\cdot\rangle_{\mathcal{H}_K}$ stands for the inner product of $\mathcal{H}_K$ that is deduced from the bilinear operator $(\cdot,\cdot)$ defined by \eqref{inner-product}.
\end{enumerate}

Given data $\{\bm x_i,y_i\}_{i=1}^n$, for any kernel function $K(\cdot,\cdot)$ and $f\in\mathcal{H}_K$, define the regularized loss function 
\begin{equation*}%\label{eq:Q}
Q_{\eta}(f,K,\mathcal{X},\bm y) = Q(f,K,\mathcal{X},\bm y) + \eta \|f\|^2_{\mathcal{H}_K},
\end{equation*}
where $Q(f,K,\bm X,\bm y)$ is a user-specified loss function measuring the goodness-of-fit and $\eta>0$ is the regularization parameter. 
The penalized regression problem is to solve the following minimization problem
\begin{equation}\label{eq:minQ}
\min_{f\in\mathcal{H}_K}Q_{\eta}(f,K,\mathcal{X},\bm y). 
\end{equation}
Considering $f\in \mathcal{H}_K$, we can write $f(\bm x)=\sum_{i=1}^n c_i K(\bm x, \bm x_i)=\bm c^\top \bm k(\bm x)$.
Then \eqref{eq:minQ} is equivalent to
\begin{equation}\label{eq:minQ2} 
\min_{\bm c\in\mathbb{R}^n}Q_{\eta}(\bm c,K)=Q(\bm c,K)+\eta\bm c^\top \bm K\bm c,
\end{equation}
where $Q(\bm c,K)= Q(\bm c^\top \bm k(\bm x) ,K,\mathcal{X},\bm y)$ ($\mathcal{X}$ and $\bm y$ are dropped for simplicity).  
Define the vector $\bm f=[f(\bm x_1),\ldots, f(\bm x_n)]^\top$. 
It is a well-known result that when $Q(f,K)$ is the squared-error loss, i.e., $Q(f,K)=\|\bm y-\bm f\|_2^2$, the solution to \eqref{eq:minQ2} is equivalent to the predictor $\hat{Y}(\bm x)$ in \eqref{GP-prediction} obtained under the GP assumption, i.e., the optimal $\bm c^*=(\bm K+\eta \bm I_n)^{-1}\bm y$ and thus $\hat{f}(\bm x)=\hat{Y}(\bm x)$ \citep{wahba1990spline, girosi1995regularization}. 

% \begin{theorem}\label{Thm-1}
%     Let $Q(f,K,\bm X,\bm y)=\|\mathbf{f}-\bm y\|_2^2$, where $\mathbf{f}=(f(\bm x_1),f(\bm x_2),\cdots,f(\bm x_n))^T$.
%     Then the solution of \eqref{min Q: f,K} results in the prediction formula \eqref{GP-prediction}.
% \end{theorem}

% \begin{proof}
%     By the Representer Theorem of RKHS (see e.g., \cite{wendland2004scattered}), any solution of \eqref{min Q: f,K} takes the following format:
%     \begin{equation}\label{representer}
%     f(\cdot)=\sum_{i=1}^n c_iK(\cdot,\bm x_i)=\bm k(\cdot)^T\bm c,
%     \end{equation}
%     for some $\bm c\in\mathbb{R}^n$.
%     In this case we have $\mathbf{f}=\bm K\bm c$, where $\bm K$ is the $n\times n$ matrix with its $ij$-th entry being $K(\bm x_i,\bm x_j)$.
%     In addition, by the definition of RKHS inner product, we have $\|f\|^2_{\mathcal{H}_K}=\bm c^T\bm K\bm c$.
%     Therefore, when $Q(f,K,\bm X,\bm y)=\|\mathbf{f}-\bm y\|_2^2$, the variational problem \eqref{min Q: f,K} can be converted to the following quadratic programming:
%     \begin{equation}\label{min Q: c}
%     \min_{\bm c\in\mathbb{R}^n} Q_{\mu}(\bm c,K,\bm X,\bm y)=(\bm y-\bm K\bm c)^T(\bm y-\bm K\bm c)+\mu\bm c^T\bm K\bm c
%     \end{equation}
%     Let $\bm c^*$ denote the optimal $\bm c$ that solves \eqref{min Q: c}, then $f^*(\cdot)=\bm k(\cdot)^T\bm c^*$ solves \eqref{min Q: f,K}.
%     To find $\bm c^*$, differentiating $Q(\bm c,K,\bm X,\bm y)$ w.r.t. $\bm c$ and set the derivative equal to 0, we obtain
%     \begin{equation}\label{optimal c}
%     \bm c^*=(\bm K+\mu\bm I)^{-1}\bm y
%     \end{equation}
%     Therefore, for any prediction site $\bm x$, we obtain
%     \begin{equation}
%     \hat y(\bm x)=f^*(\bm x)=\bm k(\bm x)^T(\bm K+\mu\bm I)^{-1}\bm y
%     \end{equation}
%     This completes the proof.
% \end{proof}

% For notational simplicity, we omit the dependence of $Q(\cdot)$ and $Q_{\mu}(\cdot)$ on the data $\bm X$ and $\bm y$ from now on.
% We rewrite \eqref{min Q: c} as the following:
% $$\min_{\bm c\in\mathbb{R}^n}Q_{\mu}(\bm c,K)=Q(\bm c,K)+\mu\bm c^T\bm K\bm c,$$ where with a slight abuse of notation, we have defined $Q(\bm c,K)=\|\bm y-\bm K\bm c\|^2$.

\section{Discrete Nature of the Optimal Kernel}\label{sec:learning}

Suppose we can define a space of kernel functions, denoted by $\mathcal{K}$.
Given data, how should we find the optimal kernel function $K^*\in\mathcal{K}$ for a specific kernel learning method, such as the GP regression model or the support vector machine?
This general question is sometimes referred to as a multiple kernel learning (MKL) problem, a topic thoroughly discussed in a review by \cite{gonen2011multiple}.
In this paper, we narrow our focus on the special case of MKL for the GP model.

To determine the optimal $K^* \in\mathcal{K}$, we aim to minimize the regularized loss in \eqref{eq:minQ} or \eqref{eq:minQ2} w.r.t. $K$, expressed as:
\begin{equation}\label{eq:min-minQK}
Q_{\eta}(\mathcal{K})=\min_{K\in\mathcal{K}}Q_{\eta}(K),
\end{equation}
where $Q_{\eta}(K)=\min_{f\in\mathcal{H}_K}Q_{\eta}(f,K)$ represents the optimal solution of \eqref{eq:minQ2}.
The minimal solution in \eqref{eq:min-minQK} is attainable due to the property of $\mathcal{K}$, which is elaborated later. 

Consider the squared-error loss function $Q(f,K)=\|\bm y-\bm f\|^2_2$. 
Then, $Q_{\eta}(K)=\min_{f\in\mathcal{H}_K}Q_{\eta}(f,K)$ takes the form:
\begin{equation}\label{eq:min-minQL2}
 Q_{\eta}(K)=\min_{\bm c\in\mathbb{R}^n} \left\{ Q(\bm c,K)+\eta {\bm c}^\top \bm K\bm c \right\} =(\bm y-\bm K\bm c^*)^\top(\bm y-\bm K\bm c^*)+\eta {\bm c^*}^\top \bm K \bm c^*,
\end{equation}
where $\bm c^*=(\bm K+\eta \bm I_n)^{-1}\bm y$.
Substituting \eqref{eq:min-minQL2} into \eqref{eq:min-minQK}, the minimization problem to find optimal kernel is 
\begin{equation*}%\label{eq:min-minQL2-2}
Q_{\eta}(\mathcal{K})=\min_{K\in\mathcal{K}}\left\{ (\bm y-\bm K\bm c^*)^\top(\bm y-\bm K\bm c^*)+\mu {\bm c^*}^\top \bm K \bm c^*\right\}. 
\end{equation*}
According to \cite{argyriou2005learning} and \cite{micchelli2005learning}, the solution to the general problem \eqref{eq:min-minQK} has a discrete nature, even if $\mathcal{K}$ is a space of continuously parameterized kernel functions. 
To explain this point, we introduce some new notation and concepts. 


Let $\mathcal{A}_+(\Omega)$ denote the set of kernel functions such that for any set of design points in $\Omega$ the resulting kernel matrix $\bm K$ is positive definite. 
If $\mathcal{K}$ is a compact and convex subset of $\mathcal{A}_+(\Omega)$ and $Q:\mathbb{R}^n\rightarrow \mathbb{R}$ is continuous, then the solution of \eqref{eq:min-minQK} exists. See Lemma 2 of \cite{micchelli2005learning}. 
Furthermore, let $\mathcal{G}\subset \mathcal{A}_{+}(\Omega)$ be a compact set of basic kernels and assume the loss function $Q:\mathbb{R}^n\rightarrow \mathbb{R}$ is continuous and $\eta>0$. 
If $\mathcal{K}$ is the closure of the convex hall of $\mathcal{G}$, denoted by $\overline{\text{conv}(\mathcal{G})}$, then there exists a subset $\mathcal{S}\subset \mathcal{G}$ that contains at most $n+2$ basic kernels from $\mathcal{G}$ such that $Q_{\eta}(\mathcal{K})$ admits a minimizer $K^*\in \text{conv}(\mathcal{S})$ and $Q_{\eta}(\text{conv}(\mathcal{S}))=Q_{\eta}(\mathcal{K})$, recalling the definition of $Q_{\eta}$ in \eqref{eq:min-minQK}. 


In simpler terms, it implies that the optimal kernel $K^*$ solving $Q_{\eta}(\mathcal{K})$ is a convex combination of at most $n+2$ basic kernels from $\mathcal{G}$ when $\mathcal{K}$ is a closed convex hull of $\mathcal{G}$. 
The uniqueness of the solution is achieved if $Q$ is a strict convex function of $\mathbb{R}^n$. 
This result reveals two key steps in learning the optimal kernel. 
\begin{enumerate}
\item[(1)] specify the compact set of basic kernel functions $\mathcal{G}$;
\item[(2)] given $\mathcal{G}$, identify the optimal convex combination of at most $n+2$ kernel functions from $\mathcal{G}$ by minimizing $Q_{\eta}(K)$ w.r.t. $K$, where $K\in \overline{\text{conv}(\mathcal{G})}$. 
\end{enumerate}
Each step presents unique challenges. 
As reviewed in Section \ref{subsec:RKHS}, one assumption of the RKHS regression is that the underlying function $f$ belongs to $\mathcal{H}_K$. 
Hence, the basic kernels of $\mathcal{G}$ should be chosen judiciously and $\mathcal{G}$ must be sufficiently large to ensure that the RKHS $\mathcal{H}_{K^*}$ induced by the optimal kernel $K^*$ includes the unknown underlying function $f$. 
Furthermore, given $\mathcal{G}$, a computationally feasible algorithm is required to solve \eqref{eq:min-minQK} to find the optimal convex combination. 
To tackle these challenges, we draw inspiration from the literature on optimal design, a sub-area within the field of ``design and experiment analysis''. 


\section{Learning the Optimal Kernel Function}\label{sec:opt}

This section focuses on learning the optimal kernel from a \emph{pre-defined} set of basic kernels $\mathcal{G}$, i.e., the step (2) described above.
The task of optimal kernel learning, given $\mathcal{G}$, closely resembles the optimal design problem.
We introduce the conventional notation from the optimal design literature to establish this connection.

\subsection{Optimal Kernel and Optimal Design}\label{subsec:optkernel}

In approximate design terms \citep{kiefer1974general, atkinson2014optimal}, a design $\xi$ belongs to a class $\Xi$ of probability measures on a compact design space $\mathcal{X}\in \mathbb{R}^d$, and we require that $\Xi$ includes all discrete measures. 
Under the linear or generalized linear regression model with $\bm x$ as the input variables, denote $M(\bm x)$ is the information matrix at a design point $\bm x$. 
The information matrix of a design, $M(\xi)$, is defined as $M(\xi)=\int_{\mathcal{X}} M(\bm x)\xi(\dd \bm x)$.  
Common design criteria, such as $D$- and $I$-optimal criteria, which are convex in the information matrix $M$, are also convex in $\xi$ \citep{kiefer1974general}. 
The optimal design $\xi^*$ minimizing such a design criterion consists of $m$ support points $\{\bm x_1,\ldots, \bm x_m\}\subset \mathcal{X}$ and the optimal weights $\bm \lambda^*$, where $0<\lambda_i^*\leq 1$ and $\sum_{i=1}^m \lambda_i^* =1$.
Thus, $\lambda_i^*$ is the optimal probability mass allocated to each support point $\bm x_i^*$. 

Returning to the context of the optimal kernel, we introduce the concept of \emph{design}, which is a probability measure $\xi\in \Xi$, where $\Xi$ is a class of probability measures on the compact set of basic kernels $\mathcal{G}\subset \mathcal{A}_{+}(\Omega)$ including all discrete measures. 
Let $\mathcal{K}= \overline{\text{conv}(\mathcal{G})}$. 
It is evident that for any $K\in \mathcal{K}$, there exists a $\xi\in \Xi$, such that $K=\int G \xi(\dd G)$, where $G$ is the notation for any kernel in $\mathcal{G}$, and vice versa. 
Note that such a relationship between kernel function and design is not necessarily one-to-one, but it does not affect the technical derivation. 
If $\mathcal{G}$ is a countable and compact set, i.e., $\mathcal{G}=\{G_1,G_2,\ldots \}$, then $K=\sum_{i=1}\xi_i G_i $, where $0\leq \xi_i\leq 1$ is the probability mass for $G_i$ and $\sum\xi_i=1$. 
We can denote a kernel function by $K(\xi)$ as a function of $\xi$ when emphasizing the connection between the kernel function and the design. 
Therefore, finding the optimal kernel is equivalent to finding the optimal design $\xi^*$ with $m$ \emph{support kernels} $\mathcal{S}=\{K_1,\ldots, K_m\}$ selected from $\mathcal{G}$, borrowing the term ``support points'' and the optimal weights $\bm \lambda^*$ corresponding to the support kernels. 
Here $0<\lambda_i^*\leq 1$ for $i=1,\ldots, m$, and $\sum_{i=1}^m \lambda_i^*=1$. 
The optimal kernel then can be expressed by $K(\xi^*)=\sum_{i=1}^m \lambda_i^*K_i$. 
Acknowledging the result reviewed Section \ref{sec:learning}, $m\leq \min\{n+2,|\mathcal{G}|\}$, where $|\mathcal{G}|$ is the cardinality of $\mathcal{G}$ and it can be $\infty$. 

In the rest of the paper, we only consider the squared-error loss function and the corresponding $Q_{\eta}(K)$, since it is the most commonly used loss function for the GP regression model. 
The most direct formula of $Q_{\eta}(K)$ is given in \eqref{eq:min-minQL2}. 
Since any $K\in \overline{\text{conv}(\mathcal{G})}$ can be represented by a design $\xi \in \Xi$, $Q_{\eta}(K(\xi))$ can be seen as a functional of $\xi$, which we denote by $Q_{\eta}(\xi)$ for short. 
We are going to show that $Q_{\eta}(K)$ is convex in $K$ and thus also convex in $\xi$. 
Comparing optimal kernel and optimal design, we can conclude that they are essentially the same problem. 
The effort invested in connecting the two is worthwhile because the theories and algorithms for solving optimal design can also be adapted for optimal kernel learning. 

\subsection{General Equivalence Theorem}\label{subsec:gen_equ}

The General Equivalence Theorem \citep{kiefer1974general} is fundamental for optimal design. 
It provides crucial guidelines for constructing optimal designs for convex design criteria in $\xi$. 
The derivation of the General Equivalence Theorem varies based on the definition of the design criterion, as shown in works like \cite{atkinson2015designs}, \cite{yang2013optimal}, \cite{atkinson2015designs}, and \cite{li2022maximin}.
Inspired by the optimal design literature, we also derive the Generalized Equivalence Theorem for optimal kernel learning. 
First, we define two versions of the directional derivative. 
One version is the directional derivative of loss function $Q_{\eta}(K)$ w.r.t. the kernel function $K$. 
Due to the relationship between any $K$ and design $\xi$, we can also define another version of the directional derivative of $Q_{\eta}(\xi)$ w.r.t. the design $\xi$. 
Proposition \ref{prop:dir-diff} gives the formula of the directional derivative of $Q_{\eta}(\xi)$. 
All results obtained here follow the notation and assumptions earlier. 
The proof and derivation are included in the Appendix. 

\begin{definition}[Directional Derivative w.r.t. Kernel]
Let $K$ and $K^\prime$ be two kernel functions in the kernel space $\mathcal{K}$, where $\mathcal{K}$ is a compact and convex subset of $\mathcal{A}_{+}(\Omega)$. 
The directional derivative of $Q_{\eta}(K)$ in the direction of $K^{\prime}$ is
\begin{equation*}
  \phi(K^{\prime},K):= \bigtriangledown_{K^{\prime}}Q_{\eta}(K)=\lim_{\alpha \to 0^+} \frac{Q_{\eta}((1-\alpha)K+\alpha K^{\prime})-Q_{\eta}(K)}{\alpha}.
\end{equation*}
\end{definition}

\begin{definition}[Directional Derivative w.r.t. Design]\label{def:dir-diff}
Given a compact set of kernel functions $\mathcal{G}\subset \mathcal{A}_{+}(\Omega)$, let $\xi$ and $\xi^\prime$ be two probability measures in $\Xi$ on $\mathcal{G}$, including all discrete measures.
As a function of $\xi$, the directional derivative of $Q_{\eta}(\xi)$ in the direction of $\xi^{\prime}$ is
\begin{equation*}
  \phi(\xi^{\prime},\xi):= \bigtriangledown_{\xi^{\prime}}Q_{\eta}(\xi)=\lim_{\alpha \to 0^+} \frac{Q_{\eta}((1-\alpha)\xi+\alpha \xi^{\prime})-Q_{\eta}(\xi)}{\alpha}.
\end{equation*}
\end{definition}

\begin{proposition}\label{prop:dir-diff}
The directional derivative of $Q_{\eta}(\xi)$ in the direction of $\xi^{\prime}$ is given as,
\begin{equation}\label{eq:dir-diff}
\phi (\xi^{\prime},\xi)=\frac{\partial Q_{\eta}(\xi)}{\partial \alpha}\bigg|_{\alpha =0}=-\eta \bm y^\top((\bm K_{\xi}+\eta \bm I_n)^{-1}(\bm K_{\xi^{\prime}}-\bm K_{\xi})(\bm K_{\xi}+\eta\bm I_n)^{-1})\bm y,
\end{equation}
where $\bm K_{\xi}$ and $\bm K_{\xi'}$ are the $n\times n$ kernel matrices computed by evaluating $K(\xi)$ and $K(\xi')$ on $\mathcal{X}=\{\bm x_i\}_{i=1}^n$. 
\end{proposition}

Based on Proposition 13 of \cite{micchelli2005learning}, if $Q:\mathbb{R}^n \rightarrow \mathbb{R}_+$ is convex, which is the case of the squared-error loss function and $\eta >0$, the functional $Q_{\eta}(K):\mathcal{A}_+(\Omega)\rightarrow \mathbb{R}_+$ is convex.
Lemma \ref{lemma:convex} here shows $Q_{\eta}(\xi)$ is convex in $\xi\in \Xi$ as well. 
With these results, we can obtain the General Equivalence Theorem for the optimal $\xi^*\in \Xi$ that minimizes $Q_{\eta}(\xi)$.

\begin{lemma}\label{lemma:convex}
Assume $Q_{\eta}(K):\mathcal{A}_+(\Omega)\rightarrow \mathbb{R}_+$ is convex in $\mathcal{K}$ where $\eta>0$ and $\mathcal{K}$ and $\mathcal{G}$ are defined in Section \ref{subsec:optkernel}. 
Then $Q_{\eta}(\xi)$ is also convex in $\Xi$. 
\end{lemma}

\begin{theorem}[General Equivalence Theorem]\label{thm:GE}
Assume the same definition of $\Xi$, $\mathcal{G}$, $\mathcal{K}$, and $Q_{\eta}(\cdot)$ in Section \ref{subsec:optkernel}. 
The following conditions of a design $\xi \in \Xi$ are equivalent:
\begin{enumerate}
\item[(1)] The design $\xi^*\in \Xi$ minimizes $Q_{\eta}(\xi)$;
\item[(2)] $\phi (\xi^{\prime},\xi^*)\geq 0$ holds for any $\xi^{\prime}\in \Xi$; 
\item[(3)] $\phi(G, \xi^*)\geq 0$ holds for any $G\in \mathcal{G}$ and the inequality becomes equality if $G$ is a support kernel of $\xi^*$. Here, the derivative $\phi(G,\xi)$ is a simplified notation for $\phi(\xi_G,\xi)$. The design $\xi_G$ is a probability measure assigning unit probability to the single kernel $G$ in $\mathcal{G}$.
\end{enumerate}
\end{theorem}

\subsection{Forward Stepwise Optimal Kernel Learning}\label{subsec:weights}

The General Equivalence Theorem \ref{thm:GE} provides some insight on how to select the support kernels sequentially. 
Based on it, we propose an optimization algorithm that iterates between selecting a kernel from $\mathcal{G}$ as a new support kernel and updating the weights, which can be considered as a Fedorov-Wynn type of algorithm \citep{dean2015handbook}.
In each iteration, we check the sign of $\phi(G,\xi^r)$ for any kernel function $G$ that has not been selected into the current design $\xi^r$.
If $\phi(G,\xi^r)$ is non-negative for all $G$, then $\xi^{r}$ reaches the optimal. 
But if $\phi(G,\xi^r)<0$ for some $G$, it indicates that $G$ is a potential support kernel and should be added to the design. 
To achieve the maximum reduction of the loss function $Q_{\eta}(\xi^r)$ in each iteration, we add the kernel $G^{*}=\argmin_{G} \phi(G,\xi^r)$ with $\phi(G,\xi^r)<0$ into the current set of support kernels for $\xi^r$.
This procedure is outlined in Algorithm \ref{alg:fed-wynn}. 
Because it sequentially selects one support kernel from the set of the basic kernels, we call it the \emph{Forward Stepwise Optimal Kernel Learning Algorithm}.
We also show the convergence of Algorithm \ref{alg:fed-wynn} in Theorem \ref{thm:convergence} and it requires Lemma \ref{lem:inequality}. 


\begin{lemma}\label{lem:inequality}
For any design $\xi\in \Xi$ and the optimal design $\xi^*$ that minimizes $Q_{\eta}(\xi)$, the following inequality holds: 
\begin{equation}\label{eq:ineq}
\min_{G\in \mathcal{G}} \phi(G, \xi)\leq \phi(\xi^*, \xi)\leq Q_{\eta}(\xi^*)-Q_{\eta}(\xi)\leq 0,
\end{equation}
where $\phi(G, \xi)$ and $\phi(\xi^*,\xi)$ are the directional derivative of $Q_{\eta}(\xi)$ defined in Definition \ref{def:dir-diff} and computed by Proposition \ref{prop:dir-diff}.
\end{lemma}
\begin{theorem}[Convergence of Algorithm \ref{alg:fed-wynn}]\label{thm:convergence}
Assume the optimal weight procedure in Algorithm \ref{alg:multiplicative} converges to the optimal solution. Given the compact set of basic kernels $\mathcal{G}\subset \mathcal{A}_{+}(\Omega)$ and let $\mathcal{K}=\overline{\text{conv}(\mathcal{G})}$, the design constructed by Algorithm \ref{alg:fed-wynn} (without the optional delete step at the end) converges to $\xi^*$ that minimizes $Q_{\eta}(\xi)$, i.e., 
\[
\lim_{r\to \infty} Q_{\eta}(\xi^r)=Q_{\eta}(\xi^*). 
\]
\end{theorem}

\begin{algorithm}[htb]
\caption{Forward Stepwise Optimal Kernel Learning Algorithm\label{alg:fed-wynn}}
\begin{algorithmic}[1]
\State {\bf Setup:} Given data $\{\bm x_i, y\}_{i=1}^n$, construct the set of basic kernels $\mathcal{G}$, and set the following parameters.
\begin{itemize}
\item $\eta$: the nugget effect;
\item \texttt{DEL}: a small positive value as the backward elimination threshold;
\item \texttt{Tol}: a small positive value to check convergence;
\item \texttt{MaxIter}: a large integer as the maximum iterations allowed.
\end{itemize} 
\State {\bf Initiate:} Randomly choose one kernel function from $\mathcal{G}$. Without loss of generality, denote it by $K_1$. At the step $r=0$, the design $\xi^{0}$ contains the support kernel $\mathcal{S}^{0}=\{K_1\}$ with weight $\bm \lambda^{(0)}=\{1\}$. Compute the objective function value $Q_{\eta}(\xi^{0})$ from \eqref{eq:min-minQL2}. 
\State Set counter $r=0$, \texttt{change}$=1$, and $m=|\mathcal{S}^{r}|=1$. 
\While {\texttt{change} $> $ \texttt{Tol} and $r < $ \texttt{MaxIter} and $m\leq \min\{n+2,|\mathcal{G}|\}$}
\If{$\phi(G, \xi^{r})\geq 0$ for any $G\in \mathcal{G}\setminus \mathcal{S}^{r}$} Break;
\Else
\State Add the kernel $K_{r+1}=\argmin_{G\in \mathcal{G}\setminus\mathcal{S}^{r}} \phi(G,\xi^{r})$ and $\phi(K_{r+1},\xi^{r})<0$ to the set of the support kernels of the current design, i.e., $\mathcal{S}^{r+1}=\mathcal{S}^{r}\cup \{K_{r+1}\}$, where $\phi(G,\xi^{r})$ is calculated by \eqref{eq:dir-diff} in Proposition \ref{prop:dir-diff}. 
\State $m \leftarrow m+1$
\State $r \leftarrow r+1$
\State Obtain optimal weights $\bm \lambda^{(r)}$ for the support kernels in $\mathcal{S}^{r}$ using Algorithm \ref{alg:multiplicative}. The current design $\xi^{r}$ consists of support kernels $\mathcal{S}^{r}$ and weights $\bm \lambda^{(r)}$. 
\State Compute $Q_{\eta}(\xi^{r})$ and 
\texttt{change}$=\bigg|\frac{Q_\eta(\xi^{r})-Q_\eta(\xi^{r-1})}{Q_\eta(\xi^{r-1})}\bigg|$
\EndIf
\EndWhile
\State {\bf Delete (Optional):} Given $\xi^{r}$ with support kernels $\mathcal{S}^{r}$ and weights $\bm \lambda^{(r)}$, remove the support kernels whose weights satisfy $\lambda_{i}^{(r)}<\texttt{DEL}$. Update the design $\xi^r$ by re-scaling the remaining weights such that their sum is equal to 1. 
\end{algorithmic}
\end{algorithm}

To achieve sparsity, we add an optional step at the end of Algorithm \ref{alg:fed-wynn}. We trim excessive small weights and the associated support kernels from the optimal design via hard thresholding.
In future research, we will incorporate more sophisticated sparsity constraints, like in shrinkage regression.

\subsection{An Optimal Weight Procedure Given Support Kernels}

After a new support kernel is added to the current design, we need to update the weights of all the current support kernels. 
We first obtain the conditions of optimal weights presented in the following Corollary \ref{thm:weight} and then develop Algorithm \ref{alg:multiplicative} to update the weights given the support kernels. 

\begin{corollary}[Conditions of Optimal Weights]\label{thm:weight}
Restrict the set of basic kernel $\mathcal{G}$ to be a finite set, $\mathcal{G}=\{K_1,\ldots, K_M\}$ and $\Xi$ is the class of discrete measure on $\mathcal{G}$. 
For any $\xi\in \Xi$, the corresponding weight vector $\bm \lambda=[\lambda_1,\ldots, \lambda_M]^\top$ with $0\leq \lambda_i\leq 1$ becomes the only variable that decides $Q_{\eta}(\xi)$. 
The following two conditions on the optimal design $\xi^*$ and its weight vector $\bm \lambda^*$ are equivalent. 
 \begin{enumerate}
\item The weight vector $\bm{\lambda}^{*}$ minimizes $Q_{\eta}(\xi)$;
\item For all $K_i$ with $\lambda_i^*>0$, $\phi(K_i,\xi^*)=0$; for all $K_i$ with $\lambda_i^{*}=0$, $\phi(K_i,\xi^*)\geq 0$.
\end{enumerate}
\end{corollary} 

Consider any design $\xi$ in Corollary \ref{thm:weight} with weight vector $\bm \lambda$. 
Its corresponding kernel function is $K(\xi)$ and the kernel matrix is $\bm K$.
As $K(\xi)=\sum_{i=1}^M \lambda_iK_i$, so is true for $\bm K=\sum_{i=1}^M \lambda_i \bm K_i$. 
For such a design $\xi$, we define 
\begin{equation}
\label{eq:d-fun}
d(K_i,\xi):=\bm y^\top((\bm K+\eta \bm I_n)^{-1}\bm K_i(\bm K+\eta \bm I_n)^{-1})\bm y.
\end{equation}

Corollary \ref{thm:weight} is a special case of Theorem \ref{thm:GE} when $\mathcal{G}$ is finite, but it can be extended to the more general case where $\mathcal{G}$ is not finite but countable. 
Based on it, given the set of kernels $K_1,\cdots,K_M$, a sufficient condition that $\bm{\lambda}^{*}$ minimizes $Q_{\eta}(\xi)$ is $\phi(K_i,\xi^*)=0$ for $i=1,\cdots,M$, or equivalently,
\begin{equation}\label{eq:corov2}
\bm y^\top (\bm K^*+\eta \bm I_n)^{-1}\bm K_i(\bm K^*+\eta \bm I_n)^{-1}\bm y=\bm y^\top (\bm K^*+\eta \bm I_n)^{-1}\bm K^*(\bm K^*+\eta \bm I_n)^{-1}\bm y.
\end{equation}
Here $\bm K^*$ is the kernel matrix obtained by evaluating the kernel function $K(\xi^*)$ on $\mathcal{X}$ and $\bm K_i$ is the kernel matrix of the kernel function $K_i$. 
It is easy to see that \eqref{eq:corov2} is equivalent to 
\begin{equation}\label{weight equation}
\sum_{j=1}^M \lambda_j^* d(K_j,\xi^*)=d(K_i,\xi^*).
\end{equation}

To obtain the optimal weights $\bm{\lambda}^{*}$, the non-optimal weights of a design could be adjusted according to two sides of \eqref{weight equation}.
At the $k$th iteration, denote the current design $\xi^k$ with weights $\bm \lambda^{(k)}$ for the basic kernels $\mathcal{G}=\{K_1,\ldots, K_M\}$. 
The corresponding kernel function is $K(\xi^k)=\sum_{i=1}^M \lambda_i^{(k)}K_i$ and the kernel matrix is $\bm K^{(k)}=\sum_{i=1}^M \lambda_i^{(k)} \bm K_i$. 
If $d(K_i,\xi^k)>\sum_{j=1}^M \lambda_j d(K_j,\xi^k)$, then the weight of kernel $K_i$ should be increased base on \eqref{weight equation}.
Otherwise, if $d(K_i,\xi^k)<\sum_{j=1}^M \lambda_j d(K_j,\xi^k)$ , the weight of kernel $K_i$ should be decreased.
Therefore, following the similar idea in the multiplicative procedure used in \cite{silvey1978algorithm, yu2010monotonic, li2022maximin}, the ratio $\left[d(K_i,\xi^k)/\sum_{j=1}^M\lambda_j^{(k)}(d(K_j,\xi^k))\right]^{\delta}$
would be a good adjustment for the weight of kernel $K_i$. 
Here $\delta\in (0, 1]$ is a tuning parameter that controls the speed of convergence.
We update the weight of kernel $K_i$ at the $k$th iteration by
\begin{equation}\label{update-1}
{\widetilde\lambda_i^{(k+1)}}=\lambda_i^{(k)}\left[\frac{d(K_i,\xi^k)}{\sum_{j=1}^M\lambda_j^{(k)}d(K_j,\xi^k)}\right]^{\delta}.
\end{equation}
Then we normalize the weights to ensure the sum to one condition by
\begin{equation}\label{update-2}
\lambda_i^{(k+1)}=\frac{{\widetilde\lambda_i^{(k+1)}}}{\sum_{j=1}^M{\widetilde\lambda_j^{(k+1)}}}
\end{equation}
Plugging \eqref{update-1} into \eqref{update-2}, we get \eqref{eq:update} in Algorithm \ref{alg:multiplicative}.

It is easy to see that Algorithm \ref{alg:multiplicative} solves a convex minimization problem since $Q_{\eta}$ is convex w.r.t. $\bm \lambda$ and the constraints $\sum_{i=1}^M \lambda_i=1$ and $\lambda_i\geq 0$ for all $i$'s are convex. 
Therefore, convex optimization methods other than the multiplicative algorithm can be used here.
We chose the multiplicative algorithm because \cite{yu2010monotonic} established the monotonic convergence for a general class of multiplicative algorithms. 
Comparing the formula in \eqref{update-1} that updates the weight for each support kernel with Equation (2) in \cite{yu2010monotonic}, we can see that Algorithm \ref{alg:multiplicative} is a special case of the general multiplicative algorithm. 
Thus, the monotone convergence concluded in \cite{yu2010monotonic} should apply to Algorithm \ref{alg:multiplicative} as well. 
Due to limited space, we omit the rigorous proof on the convergence of Algorithm \ref{alg:multiplicative} in this paper. 
Based on the above discussion, we should note that the premise of Theorem \ref{thm:convergence} is satisfied and the convergence of Algorithm \ref{alg:fed-wynn} should hold. 

\begin{algorithm}[htb]
\caption{Optimal Weight Procedure: A Modified Multiplicative Algorithm \label{alg:multiplicative}}
\begin{algorithmic}[1]
\State {\bf Setup:}  Given the set of basic kernels $\mathcal{G}=\{K_1,\ldots,K_M\}$ of size $M$, set the following parameters.
\begin{itemize}
\item $\delta\in (0, 1]$: controls the speed of convergence, and we set $\delta=1$ by default;
\item \texttt{Tol}: a small positive value to check convergence, which can be the same as in Algorithm \ref{alg:fed-wynn};
\item \texttt{MaxIter}$_0$: a large integer as the maximum iterations allowed.
\end{itemize} 
\State {\bf Initiate:} Assign a uniform initial weight vector $\bm{\lambda}^{(0)}=[\lambda_1^{(0)},\cdots,\lambda_M^{(0)}]^\top$. Set the counter $k=0$ and initial value \texttt{change}$_0$=1. 
\While {\texttt{change}$_0 > $ \texttt{Tol} and $k < $ \texttt{MaxIter}$_0$}
\For {$i=1,\ldots,M$}
\State Update the weight of kernel $K_i$:
\begin{equation}\label{eq:update}
\begin{aligned}
\lambda_i^{(k+1)}&=\lambda_i^{(k)}\frac{\left[d(K_i,\xi^k)\right]^{\delta}}{\sum\limits_{j=1}^M\lambda_j^{(k)}\left[d(K_j,\xi^k)\right]^{\delta}}\\
&=\lambda_i^{(k)}\frac{\left[\bm y^\top(\bm K^{(k)}+\eta \bm I_n)^{-1}\bm K_i(\bm K^{(k)}+\eta \bm I_n)^{-1}\bm y\right]^{\delta}}{\sum\limits_{j=1}^n\lambda_j^{(k)}\left[\bm y^\top(\bm K^{(k)}+\eta \bm I_n)^{-1}\bm K_j(\bm K^{(k)}+\eta \bm I_n)^{-1})\bm y\right]^{\delta}}
\end{aligned}
\end{equation}
\State \texttt{change}$_0=\bigg|\frac{Q_\eta(\xi^{k+1})-Q_\eta(\xi^{k})}{Q_\eta(\xi^{k})}\bigg|$
\State $k \leftarrow k+1$
\EndFor
\EndWhile
\end{algorithmic}
\end{algorithm}

\subsection{Tuning Parameters}

Parameters such as \texttt{Tol}, \texttt{MaxIter}, and \texttt{MaxIter}$_0$ are stopping rules and their settings should be chosen based on trial-and-error and the available computing time, which is similar to most optimization algorithms. 
We set $\texttt{Tol}=0.005$ and $\texttt{MaxIter}=\texttt{MaxIter}_0=1000$ in all the examples in Section \ref{sec:num}. 
The parameter \texttt{DEL} is the threshold for removing a certain support kernel if its weight is too small. 
A proper \texttt{DEL} setting can produce a sparse model with fewer support kernels in the optimal kernel. 
On the other hand, if \texttt{DEL} is too large, it leads to a sub-optimal solution and less accurate approximation of the underlying function $f(\bm x)$. 
In our simulations, we have tried a series of \texttt{DEL} values in $[0.01, 0.1]$ and discovered that $\texttt{DEL}=0.05$ gives sufficiently accurate results. 
So we set $\texttt{DEL}=0.05$ in all the examples. 
The parameter $\delta \in (0,1]$ controls the convergence's speed.
Originally, $\delta$ was introduced in the classical multiplicative algorithm \citep{silvey1978algorithm}, and some settings were suggested based on different optimal design criteria \citep{torsney1983moment}. 
Unfortunately, such rules-of-thumb are unsuitable for our objective function, but our numerical simulations have shown that $\delta=1$ leads to sufficiently fast convergence. 

We only tune $\eta$, the nugget effect in the probabilistic GP model, which is also the regularization parameter in nonparametric RKHS regression. 
Since the optimal kernel learning is from the nonparametric perspective, we use the leave-one-out cross-validation (LOO-CV) procedure to choose the optimal $\eta$. 
It entails computing the LOO-CV error for a given $\eta$ value with the RKHS regression whose kernel is obtained from the proposed Algorithm \ref{alg:fed-wynn}.  
To compute the LOO-CV error, the existing short-cut formula for GP regression \citep{dubrule1983cross} can be used to save computation. 
To show the effectiveness of the LOO-CV procedure, we use the Michalewicz test function with $p=2$ true active input variables. 
The surface of a two-dimensional Michalewicz function is shown in Figure \ref{fig:michale2d}. 
The training data has $n=200$ sample size and $d=6$ input dimension (four extra ``fake'' input dimensions).
The LOO-CV error curve in Figure \ref{fig:loocv} is plotted at the mean value of $10$ simulations for each $\eta$ value. 
The curve's minimum (marked by the red dashed vertical line) leads to the optimal $\eta$ value. 

\begin{figure}[htb]
\centering
\includegraphics[scale=0.5]{test_cv_smooth.pdf}
\caption{Leave-one-out cross-validation error w.r.t. $\eta$.\label{fig:loocv}}
\end{figure}


\section{Set of Basic Kernels: Low-Dimensional Kernel Functions}\label{sec:low}
 
In this section, we answer the question of how to construct the set of basic kernels $\mathcal{G}$ to identify the active input dimensions, i.e., the step (1) described at the end of Section \ref{sec:learning}.

\subsection{ANOVA model}\label{subsec:anova}
Functional Analysis of Variance model \citep{huang1998functional, huang1998projection, hooker2007generalized} takes the following general model assumption
\begin{equation}\label{ANOVA}
y(\bm x)=\sum_{j=1}^p Z_j(x_{j}) + \sum_{j=1}^{p-1}\sum_{k=j+1}^p Z_{jk}(x_{j}, x_{k}) +\ldots+ \epsilon,
\end{equation}
where each $Z_j(x_j)$ is the 1-dim sub-model for the $j$-th dimension of $\bm x$, and $Z_{jk}(x_j,x_k)$ is the 2-dim sub-model for the $j$-th and $k$-th dimensions of $\bm x$, and so on.
It is common to restrict the model \eqref{ANOVA} to contain sub-models up to certain lower input dimensions.
We focus on the special case where each $Z(\cdot)$ in \eqref{ANOVA} is a low-dimensional Gaussian process.
Such a model is a special case of \emph{additive GP} and has been studied in, for example, \cite{kaufman2010bayesian}, \cite{duvenaud2011additive}, \cite{durrande2012additive}, \cite{durrande2013anova}, etc.
Many believe such a model works only when the underlying response function has a coordinate-wise additive structure (see e.g.,\cite{tripathy2016gaussian}).
However, as is shown in \cite{durrande2012additive} and \cite{duvenaud2011additive}, functional ANOVA GP models can recover arbitrarily flexible underlying data relationships, regardless of whether an additive structure is present.
%For example, it is shown in \cite{durrande2012additive} that the functional ANOVA GP model outperforms other algorithms in recovering Sobol's $g$-function $g(x)=\prod_{k=1}^d\frac{|4x_k-2|+a_k}{1+a_k}$ where $a_k>0$. 
Under most circumstances, it is sufficient to include only the low-dimensional sub-models such as 1-dim and 2-dim ones in \eqref{ANOVA}.
The rationale is analogous to the traditional ANOVA employed in the analysis of experiments, namely the effect sparsity principle \citep{wu2011experiments}.
It states that main (single-factor) effects and two-factor interactions are the most likely to be statistically significant than higher-order interactions in the analysis of experimental data

One major limitation of \eqref{ANOVA} is that only one GP sub-model is included each subset of input variables, such as $\{x_j\}$, $\{x_j,x_k\}$, etc. 
In \cite{ba2012composite}, the authors observed that it is advantageous to model the covariance function as a summation of two covariance functions that are used to capture the local and global variations of the underlying function.
Motivated by this idea, instead of specifying a single GP model for each subset of input variables, we propose supplying multiple ones to allow for more flexibility, and generalize model \eqref{ANOVA} to the following form:
\begin{equation}\label{ANOVA-generalized}
y(\bm x)=\sum_{j=1}^p\sum_{m=1}^{M_j} \beta_{j,m} Z_{j,m}(x_j) + \sum_{j\neq k}^{p-1}\sum_{m=1}^{M_{jk}} \beta_{j,k,m} Z_{j,k,m}(x_j,x_k) +\cdots +\epsilon,
\end{equation}
Here each $Z_{j,m}(x_j)$ is a GP with input $x_j$ and $Z_{j,m} \sim GP(0, \tau_{j,m}^2 K_{j,m})$ where $K_{j,m}$ is a kernel function of input variable $x_j$. 
Similarly, $Z_{j,k,m}(\bm x_j,\bm x_k)$ is a GP with input $(x_j,x_k)$ and  $Z_{j,k,m} \sim GP(0, \tau_{j,k,m}^2 K_{j,k,m})$ where $K_{j,k,m}$ is a kernel function of input variables $(x_j,x_k)$. 
Other GP models in the omitted part in \eqref{ANOVA-generalized} are defined in the same way. 

From the additive GP model \cite{ba2012composite}, we know that if $Z(\bm x)\sim GP(0, \tau^2 K(\cdot,\cdot))$ can be considered as the sum of two different GPs, i.e., $Z(\bm x)=Z_1(\bm x)+Z_2(\bm x)$, where $Z_i(\bm x) \sim GP(0,\tau_i^2 K_i(\cdot,\cdot))$ for $i=1,2$, then equivalently, 
\[\tau^2 K(\cdot,\cdot)=\tau_1^2 K_1(\cdot,\cdot)+\tau_2^2 K_2(\cdot,\cdot).\]
In other words, $K=\lambda_1 K_1+\lambda_2 K_2$ where $\lambda_1+\lambda_2=1$ and $\lambda_i=\tau_i^2/\tau^2$ for $i=1,2$. 
This result applies to the more general case of \eqref{ANOVA-generalized} and we have the following equivalence. 
\begin{theorem}\label{thm:additveGP}
Assume the Gaussian process $Z(\bm x)\sim GP(0,\tau^2 K(\cdot,\cdot))$ has covariance function that can be written as 
\[
\tau^2 K=\sum_{j=1}^p \sum_{m=1}^{M_j} \tau_{j,m}^2 K_{j,m} + \sum_{j\neq k}^{p}\sum_{m=1}^{M_{jk}} \tau_{j,k,m}^2 K_{j,k,m}+\ldots,
\]
or equivalently, 
\[
K=\sum_{j=1}^p \sum_{m=1}^{M_j} \lambda_{j,m} K_{j,m} + \sum_{j\neq k}^{p}\sum_{m=1}^{M_{jk}} \lambda_{j,k,m} K_{j,k,m}+\ldots
\]
where $\lambda_{*}=\tau_{*}^2/\tau^2$ and $\sum \lambda_{*}=1$. Here we use ``*'' in the subscription to represent all possible indices. 
The Gaussian process equals the following linear combination of different Gaussian processes.
\[
Z(\bm x)=\sum_{j=1}^p\sum_{m=1}^{M_j}\sqrt{\lambda_{j,m}}Z_{j,m}(x_j)+\sum_{j\neq k}^{p}\sum_{m=1}^{M_{jk}}\sqrt{\lambda_{j,k,m}}Z_{j,k,m}(x_j,x_k)+\ldots
\]
where each $Z_{*}(\cdot)\sim GP(0, \tau_{*}^2 K_{*})$. 
\end{theorem}

We aim to build a generalized functional ANOVA model in \eqref{ANOVA-generalized}.
The set of basic kernels for Algorithm \ref{alg:fed-wynn} is 
$\mathcal{G}=(\cup_{j=1}^p\mathcal{G}_j)\cup(\cup_{j\neq k}^p\mathcal{G}_{jk})\cup\cdots$, where $\mathcal{G}_j$ consists of $M_j$ kernel functions defined on the $j$th dimension $x_j$, $\mathcal{G}_{jk}$ consists of $M_{jk}$ kernel functions defined on $(x_j,x_k)$, and so on.
So $\mathcal{G}$ has a finite number of basic kernels, and thus is a compact set in $\mathcal{A}_{+}(\Omega)$. 
In this paper, we choose the isotropic Gaussian kernel as follows.
\begin{align*}
\mathcal{G}_j&=\left\{K_{j,m}(x_{1,j},x_{2,j})=\exp(-\theta_{j,m}(x_{1,j}-x_{2,j})^2), \text{where } \theta_{j,m} \in \Theta, m=1,\cdots, M_j\right\}\\
\mathcal{G}_{jk}&=\left\{K_{j,k,m}((x_{1,j},x_{1,k}),(x_{2,j},x_{2,k}))=\exp(-\theta_{j,k,m}[(x_{1,j}- x_{2,j})^2+(x_{1,k}-x_{2,k})^2]),\right. \\
&\left. \text{where } \theta_{j,k,m} \in \Theta, m=1,\cdots,M_{jk}\right\}.
\end{align*}
Here $\Theta$ is a pre-defined set of values for the parameter $\theta$. 
In our examples, we set $\Theta=\{\theta=a\times 10^b, a=1, 3, 5, 7, 9, b= -2, -1, 0, 1, 2\}$ for all the input data that have been standardized before modeling and use the same $M=25$. 
Users can set $M$ and $\Theta$ based on their data and available computing resources. 
Other radial basis kernel functions can be used if the Gaussian kernel is too smooth for the underlying function. 
We prefer the radial basis function because it significantly reduces the number of basic kernels. 

\subsection{Forward Stage-wise Optimal Kernel Learning}\label{subsec:heredity}

If we only include 1-dim and 2-dim basic kernel functions in \eqref{ANOVA-generalized} and set all $M_j$ and $M_{jk}$ the same at $M=25$, then $\mathcal{G}$ has size $p\times M+\binom{p}{2}*M$. 
If the 3-dim basic kernel functions are considered too, the size of $\mathcal{G}$ is even larger. 
For large $p$ and $M$, this number can be prohibitive. 
To overcome the issue of large number of basic kernels, we propose selecting support kernels by running the Algorithm \ref{alg:fed-wynn} in sequential stages. 
Our inspiration comes from the \emph{effect heredity principle} in \cite{wu2011experiments}. 
It’s widely used in the design and analysis of experiments and other areas like in \cite{haris2016convex}.
The \emph{strong} heredity principle states that if an interaction term of two variables is statistically significant, both main effects of the two variables must be included in the model.
The \emph{weak} heredity principle only requires one of the two main effects to be included. 


For stage-wise optimal kernel learning, we first consider only 1-dim kernels $\mathcal{G}=(\cup_{j=1}^p\mathcal{G}_j)$.  
After running Algorithm \ref{alg:fed-wynn}, we identify the active variables that are the input dimensions of the selected basic kernels.
In the second stage, we only add 2-dim basic kernels of the active variables and proceed with Algorithm \ref{alg:fed-wynn}. 
The strong heredity requires both input variables of a 2-dim kernel to be active in the first stage, while the weak heredity only requires one. 
So the strong heredity reduces the number of basic kernels in the second stage. 
We can continue to the future stages to include basic kernels of higher dimensional variables if needed. 
The procedure is summarized in Algorithm \ref{alg:stage-opt}. 
For short, we denote the proposed additive GP regression model \eqref{ANOVA-generalized} estimated via Algorithm \ref{alg:stage-opt} by \texttt{optK}. 
All the proposed algorithms are implemented \texttt{R} and are available at \url{https://github.com/tonyxms/ConvexKernel}. 
Our codes included \texttt{R} packages \texttt{foreach} \citep{foreach} and \texttt{doParallel} \citep{doParallel} to boost computing speed. 

\begin{algorithm}
\caption{Forward Stage-wise Optimal Kernel Learning \label{alg:stage-opt}}
\begin{algorithmic}[1]
\State {\bf Setup:}  Given data $\{\bm x_i,y_i\}_{i=1}^n$ and set the following parameters for the algorithm.
\begin{itemize}
\item \texttt{MaxDim}: maximum dimensions of input variables allowed for the basic kernel functions; 
\item $\eta$, \texttt{DEL}, \texttt{Tol}, and \texttt{MaxIter}: parameters of Algorithm \ref{alg:fed-wynn};
\item $\delta$, \texttt{Tol}, \texttt{MaxIter}$_0$: parameters of Algorithm \ref{alg:multiplicative}. 
\end{itemize} 
\State {\bf Initiate:} construct the set $\Theta$ and the set $\mathcal{G}=(\cup_{j=1}^p\mathcal{G}_j)$ of basic kernels of $x_j$ for $j=1,\ldots,p$. The current dimension of kernels is \texttt{Dim}=1. 
\State Run Algorithm \ref{alg:fed-wynn} and return $Q_{\eta}(\xi)$, where $\xi$ is the current optimal design. The set $\mathcal{S}$ is the set of selected support kernels and $\bm \lambda$ as the vector of non-zero weights. 
\State Identify the active variables based on the selected kernels in $\mathcal{S}$. For the same $\Theta$, prepare the $\mathcal{G}$ to include the kernels of (\texttt{Dim}+1) the active variables, according to either strong or weak heredity principle. If the weak heredity principle is used, inactive variables are included too. Set \texttt{Dim}=\texttt{Dim}+1. 
\State Keep previously selected kernels. Return to Step 3 and resume Algorithm \ref{alg:fed-wynn}. 
\State Check convergence of $Q_{\eta}$. Stop Algorithm \ref{alg:stage-opt} if the change of $Q_{\eta}$ is less than \texttt{Tol} or $\texttt{Dim} > \texttt{MaxDim}$. 
\end{algorithmic}
\end{algorithm}

\section{Examples}\label{sec:num}
We demonstrate the proposed optimal kernel learning approach through three examples.
Two of these are simulated examples based on well-known test functions from the literature, while the third is a real-world case study involving satellite drag.
These examples illustrate that the proposed optimal kernel method effectively identifies the correct active variables for datasets with large input dimensions and small sample sizes. 

We compare the proposed method with three alternative approaches:
\begin{itemize}[leftmargin=*]
\item The GP model based on maximum likelihood estimation (MLE) \citep{santner2003design},
\item The local GP method by \cite{gramacy2015local}, and
\item The multiresolution functional ANOVA (MRFA) method by \cite{sung2019multiresolution}.
\end{itemize}
Each of these methods has distinct characteristics. 
The MLE GP is a conventional approach that performs well for small-scale problems.
The local GP method is computationally efficient and provides reasonably accurate predictions for problems with large sample sizes, but it may struggle with high-dimensional input spaces.
The MRFA method, as discussed in Section \ref{subset:literature}, is based on functional ANOVA and is well-suited for high-dimensional inputs. It also requires significantly less computation than MLE GP while delivering accurate predictions.
All numerical experiments were conducted in \texttt{R} \citep{R2024}. The MLE GP, local GP, and MRFA methods are implemented in the \texttt{R} packages \texttt{mlegp} \citep{dancik2008mlegp}, \texttt{lagp} \citep{gramacy2016lagp}, and \texttt{MRFA} \citep{sung2019multiresolution}, respectively.

\paragraph{Algorithm Settings} We summarize the settings used for all algorithms in the examples. 
As previously mentioned, we employ the isotropic Gaussian kernel function for all basic kernels and define the parameter set $\Theta=\{\theta=a\times 10^b, a=1, 3, 5, 7, 9, b= -2, -1, 0, 1, 2\}$.
Strong heredity is used to construct the basic kernel set in Algorithm \ref{alg:stage-opt}.
For the tuning parameter $\eta$, we use a leave-one-out cross-validation procedure to select the optimal value from $\{0.005, 0.01, 0.02, 0.05, 0.1, 0.5\}$, as illustrated in Figure \ref{fig:loocv}.
In Algorithm \ref{alg:fed-wynn}, we set $\texttt{Tol}=0.005$, $\texttt{DEL}=0.05$, and $\texttt{MaxIter}=1000$.
In Algorithm \ref{alg:multiplicative}, we set $\delta=1$, $\texttt{Tol=0.005}$, and $\texttt{MaxIter}_0=1000$. 
Both algorithms share the same maximum allowed iterations. 
In Algorithm \ref{alg:stage-opt}, we set $\texttt{MaxDim}=4$. 
However, as noted in Section \ref{subsec:anova}, accurate predictions are often achieved using only 1-dimensional and 2-dimensional sub-models, and the algorithm typically converges before reaching $\texttt{MaxDim}=4$.
For all four methods under comparison, we have always scaled the training and testing designs in the same $[0,1]^d$ domain before calling the estimation and prediction procedure. 

\paragraph{Performance Metrics} To evaluate the performance of the different methods, we compute the standard root mean square error (Standard RMSE), defined as:
\begin{equation}\label{eq:SRMSE}
\text{Standard root mean square error}=\frac{\sqrt{\frac{1}{n}\sum_{i=1}^n(y_i-\hat{y}_i)^2}}{\sqrt{\frac{1}{n}\sum_{i=1}^n(y_i-\bar{y})^2}},
\end{equation}
where $y_i$ is the $i$th observation, $\hat{y}_i$ is the predicted one, and $\bar{y}$ is the sample mean of all $n$ observations.
To assess the proposed method's ability to correctly identify active variables, we report the number of false positives (variables included in the estimated model \eqref{ANOVA-generalized} by Algorithm \ref{alg:stage-opt} but absent in the true data-generating function) and false negatives (variables omitted from the estimated model but present in the true function).

The strength of \texttt{laGP} is in its fast computation and can deal with extremely large $n$ problems. 
Based on local approximation, it is expected that the prediction by \texttt{laGP} is not as good as the ones using the complete data. 
Therefore, we treat it as a baseline method and try \texttt{laGP} before \texttt{mlegp} for the large input-dimensional data. 
For \texttt{mlegp}, the separable Gaussian kernel function is used. 
If the estimated $\theta_i$ in the kernel satisfies $\theta_i\geq 0.01$, then we identify the corresponding $x_i$ as an active variable. 
The threshold is based on the scaled input designs in $[0,1]^d$.  
For \texttt{MRFA}, active variables are identified based on their inclusion in the ``active group'' as described by \cite{sung2019multiresolution}.

\subsection{Michalewicz function}
The Michalewicz function \citep{simulationlib} is defined as follows:
\begin{equation*}
f(\bm x) = \sum_{j=1}^p \sin(x_j) \sin^{2k}\left(\frac{jx_j^2}{\pi}\right),
\end{equation*}
where $\bm x\in [0, \pi]^p$ and $k=10$.
Here, $p$ denotes the number of active variables in $f(\bm x)$. 
For $p=2$, the surface of the Michalewicz function is illustrated in the left panel of Figure \ref{fig:michale2d}. 
The right panel of Figure \ref{fig:michale2d} displays the predicted surface generated by \texttt{optK} using a training dataset of size $n=200$. 
As expected, the proposed method nearly perfectly recovers the Michalewicz function, owing to its additive structure composed of $p$ functions of 1-dimensional input.

In each simulation, we generate a $d$-dimensional maximin Latin hypercube design \citep{lhs} of size $n$ within the domain $[0,1]^d$, along with a random Latin hypercube design of size $m=3481$. 
The designs are scaled to the range of $[0,\pi]^d$. 
Since the Michalewicz function depends on $p$ active variables, we randomly select $p$ columns from the $d$ columns of the designs to compute the response values. 
The remaining $d-p$ columns serve as irrelevant variables.

For $p=2$ and $d=6$, Table \ref{table-2} summarizes the average performance measures of the four methods over $B=50$ simulations for $n=200$, $B=20$ for $n=500$, and $B=5$ for $n=1000$. 
The number of simulations is reduced for larger $n$ due to the increased computational cost of \texttt{mlegp}. 
Figure \ref{figure-3} presents the boxplots (or dotplots for $n=1000$) of the standardized RMSE for \texttt{mlegp}, \texttt{MRFA}, and \texttt{optK}. 
The results demonstrate that \texttt{optK} achieves the highest prediction accuracy and consistently identifies the true $p=6$ active variables.

With $p$ fixed at 6, Table \ref{table-3} reports the average performance measures for various combinations of $d$ and $n$
The number of simulations is $B=50$ for $n=200$ and $B=20$ for $n=500$. 
Figure \ref{figure-4} shows the boxplots of the standardized RMSEs for \texttt{mlegp}, \texttt{MRFA}, and \texttt{optK} for $d=60$. 
As $d$ increases and the sample size remains relatively small, identifying the active variables becomes more challenging. 
Nevertheless, \texttt{optK} achieves the lowest prediction error and accurately identifies the active variables. 
The \texttt{MRFA} method also performs well, as it is based on a functional ANOVA approach and is well-suited for additive test functions.

\begin{figure}[htb]
\centering
\begin{subfigure}[H]{.45\textwidth}
\includegraphics[width=\textwidth]{truesurface}
\end{subfigure}
\begin{subfigure}[H]{.45\textwidth}
\includegraphics[width=\textwidth]{predsurface}
\end{subfigure}
\caption{The true (left) and predicted surface by \texttt{optK} (right) of the Michalewicz function. \label{fig:michale2d}}
\end{figure}

\begin{table}[htb]
    \centering
    \caption{Michalewicz function: comparison for $p=2$, $d=6$, and $n=200,500,1000$.}\label{table-2}
    \begin{tabular}{ccccccc}
        \toprule
        \makecell[c]{Input dimension $d$ \\ Active dimension $p$} & Train size $n$ & Method & \makecell[c]{Standard RMSE \\ (Standard Deviation)} & \makecell[c]{False \\ Positive} & \makecell[c]{False \\ Negative}  \\ \midrule
        \multirow{12}*{\makecell[c]{$d=6$ \\ $p=2$}} & \multirow{4}*{$n=200$} & lagp & $0.4188(0.2185)$ & / & / \\ 
        ~ & ~ & mlegp & $0.2652(0.0273)$ & $0.0088$ & $0$ \\ 
        ~ & ~ & MRFA  & $0.0742(0.0246)$ & $0.042$  & $0$ \\ 
        ~ & ~ & optK  & $\bm{0.0275(0.0023)}$ & $\bm{0}$ & $\bm{0}$  \\ \cline{2-6}
        ~ & \multirow{4}*{$n=500$} & lagp & $0.1423(0.0121)$ & / & / \\ 
        ~ & ~ & mlegp & $0.0942(0.0070)$ & $0$ & $0$ \\ 
        ~ & ~ & MRFA  & $0.0704(0.0326)$ & $0.125$  & $0$  \\ 
        ~ & ~ & optK  & $\bm{0.0168(0.0020)}$ & $\bm{0}$ & $\bm{0}$  \\ \cline{2-6}
        ~ & \multirow{4}*{$n=1000$} & lagp & $0.0874(0.0042)$ & / & / \\ 
        ~ & ~ & mlegp & $0.0427(0.0029)$ & $0$ & $0$ \\ 
        ~ & ~ & MRFA  & $0.0265(0.0075)$ & $0.72$  & $0$  \\ 
        ~ & ~ & optK  & $\bm{0.0115(0.0006)}$ & $\bm{0}$ & $\bm{0}$  \\ 
        \bottomrule
    \end{tabular}
    \end{table}

\begin{figure}[htb]
    \centering
    \begin{subfigure}[htb]{.33\linewidth}
    \includegraphics[width=\linewidth]{Michal_2dim_6dim_boxplot_200_50_2.pdf}
    \caption{n=$200$}
    \end{subfigure}
    \begin{subfigure}[htb]{.33\linewidth}
    \includegraphics[width=\linewidth]{Michal_2dim_6dim_boxplot_500_20_2.pdf}
    \caption{n=$500$}
    \end{subfigure}
    \begin{subfigure}[htb]{.33\linewidth}
        \includegraphics[width=\linewidth]{Michal_2dim_6dim_boxplot_1000_5_2.pdf}
        \caption{n=$1000$}
        \end{subfigure}
    \caption{Michalewicz function: boxplots and dotplot of standard RMSEs for $p=2$ and $d=6$.}\label{figure-3}
\end{figure}

\begin{table}[htb]
    \centering
    \caption{Michalewicz function: comparison for $p=6$, $d=10,20,60$, and $n=300,500$.}\label{table-3}
    \begin{tabular}{ccccccc}
        \toprule
        \makecell[c]{Input dimension $d$ \\ Active dimension $p$} & Train size $n$ & Method & \makecell[c]{Standard RMSE \\ (Standard Deviation)} & \makecell[c]{False \\ Positive} & \makecell[c]{False \\ Negative}  \\ \midrule
        \multirow{8}*{\makecell[c]{$d=10$ \\ $p=6$}} & \multirow{4}*{$n=300$} & lagp & $0.9414(0.0193)$ & / & / \\ 
        ~ & ~ & mlegp & $0.9148(0.0230)$ & $2.6$ & $0.68$ \\ 
        ~ & ~ & MRFA  & $0.1528(0.0166)$ & $1.336$  & $0$ \\ 
        ~ & ~ & optK  & $\bm{0.0390(0.0133)}$ & $\bm{0}$ & $\bm{0}$  \\ \cline{2-6}
        ~ & \multirow{4}*{$n=500$} & lagp & $0.9135(0.0142)$ & / & / \\ 
        ~ & ~ & mlegp & $0.8725(0.0149)$ & $2.6$ & $0.7$ \\ 
        ~ & ~ & MRFA & $0.0593(0.0070)$ & $1.85$ & $0$  \\ 
        ~ & ~ & optK  & $\bm{0.0195(0.0012)}$ & $\bm{0}$ & $\bm{0}$  \\ 
        \midrule
        \multirow{8}*{\makecell[c]{$d=20$ \\ $p=6$}} & \multirow{4}*{$n=300$} & lagp & $0.9594(0.0390)$ & / & / \\ 
        ~ & ~ & mlegp & $0.9526(0.0302)$ & $12.32$ & $0.12$ \\
        ~ & ~ & MRFA  & $0.1724(0.0238)$ & $4.26$  & $0$ \\
        ~ & ~ & optK  & $\bm{0.0546(0.0461)}$ & $\bm{0}$ & $\bm{0}$  \\ \cline{2-6}
        ~ & \multirow{4}*{$n=500$} & lagp & $0.9171(0.0270)$ & / & / \\
        ~ & ~ & mlegp & $0.9309(0.0221)$ & $12.5$ & $0.05$ \\
        ~ & ~ & MRFA  & $0.0649(0.0063)$ & $5.5$  & $0$ \\
        ~ & ~ & optK  & $\bm{0.0196(0.0010)}$ & $\bm{0}$ & $\bm{0}$  \\
        \midrule
        \multirow{8}*{\makecell[c]{$d=60$ \\ $p=6$}} & \multirow{4}*{$n=300$} & lagp & $1.5077(0.0088)$ & / & / \\
        ~ & ~ & mlegp & $0.9560(0.0178)$ & $54$ & $0$ \\
        ~ & ~ & MRFA  & $0.2096(0.0354)$ & $12.34$  & $0$ \\
        ~ & ~ & optK  & $\bm{0.1096(0.0855)}$ & $\bm{0}$ & $\bm{0}$  \\ \cline{2-6}
        ~ & \multirow{4}*{$n=500$} & lagp & $1.5052(0.0064)$ & / & / \\
        ~ & ~ & mlegp & $0.9247(0.0167)$ & $53.85$ & $0.05$ \\
        ~ & ~ & MRFA  & $0.0839(0.0102)$ & $13.1$  & $0$ \\
        ~ & ~ & optK  & $\bm{0.0226(0.0055)}$ & $\bm{0}$ & $\bm{0}$  \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[htb]
    \centering
    \begin{subfigure}[htb]{.45\linewidth}
    \includegraphics[width=\linewidth]{Michal_60dim_boxplot_300_50_2.pdf}
    \caption{n=300}
    \end{subfigure}
    \begin{subfigure}[htb]{.45\linewidth}
    \includegraphics[width=\linewidth]{Michal_60dim_boxplot_500_20_2.pdf}
    \caption{n=500}
    \end{subfigure}
    \caption{Michalewicz function: boxplots of standard RMSEs for $p=6$, $d=60$, and $n=300,500$. }\label{figure-4}
\end{figure}


\subsection{Borehole function}
The borehole function is a widely used benchmark example for studying GP regressions \citep{morris1993bayesian}. 
It models water flow through a borehole and is defined as follows:
\begin{equation*}
f(\bm x)=\frac{2 \pi T_u(H_u-H_l)}{ln(r/r_w)(1+\frac{2LT_u}{ln(r/r_w)r_w^2 K_w}+\frac{T_u}{T_l})},
\end{equation*}
where the response $f(\bm x)$ represents the water flow rate in units of $m^3/yr$.
The function depends on $p=8$ input variables, whose definitions and ranges are listed in Table \ref{table-4}.
Unlike the Michalewicz function, the borehole function does not exhibit an additive structure. 
Consequently, functional ANOVA-based methods such as \texttt{MRFA} and \texttt{optK} are not expected to have inherent advantages in approximating it.

We conduct simulations in a manner similar to the previous example.
In each simulation, we generate an $n\times d$ maxinmin Latin hypercube design for training and a $1000\times d$ random Latin hypercube design for testing. 
We randomly select the same $8$ columns from both designs and scale them according to the ranges specified in Table \ref{table-4}.
The response data are then computed based on the scaled designs, while the remaining $d-8$ columns serve as irrelevant variables. 

Table \ref{table-5} compares the performance of \texttt{lagp}, \texttt{mlegp}, \texttt{MRFA}, and \texttt{optK} for $d=20, 40, 60$ and $n=200, 500$. 
The average performance measures are computed over $B=50$ simulations for $n=200$ and $B=20$ simulations for $n=500$. 
Figure \ref{figure-5} presents the boxplots of the standard RMSEs returned by \texttt{mlegp}, \texttt{MRFA}, and \texttt{optK} for $d=60$. 
Notably, both \texttt{MRFA} and \texttt{optK} outperform \texttt{lagp} and \texttt{mlegp} in terms of prediction accuracy, despite the borehole function's lack of an additive structure. 
Furthermore, \texttt{optK} surpasses \texttt{MRFA} in both prediction accuracy and number of false positives.

Finally, we address why \texttt{optK} consistently identifies approximately 3 false negatives. 
While this may appear to indicate that \texttt{optK} misses 3 active variables, it aligns with the sensitivity analysis of the borehole function conducted by \cite{osti_5355549}. 
Their study revealed that only five variables, $r_w$, $H_u-H_l$, $L$, and $K_w$, significantly influence the response. 
The remaining 3 variables have negligible impact. 
Thus, \texttt{optK} effectively identifies the truly important input variables, demonstrating its robustness in variable selection.

\begin{table}[htb]
    \centering
    \caption{Input variables and their ranges of the borehole function.}\label{table-4}
    \begin{tabular}{cccc}
        \toprule
        Symbol & Parameter & Range & Unit \\ \midrule
        $r_w$ & radius of borehole & $[0.05,0.15]$ & $m$ \\
        $r$ & radius of influence & $[100,50000]$ & $m$ \\
        $T_u$ & transmissivity of upper aquifer & $[63070,115600]$ & $m^2/yr$ \\
        $H_u$ & potentiometric head of upper aquifer & $[990,1110]$ & $m$ \\
        $T_l$ & transmissivity of lower aquifer & $[63.1,116]$ & $m^2/yr$ \\
        $H_l$ & potentiometric head of lower aquifer & $[700,820]$ & $m$ \\
        $L$ & length of borehole & $[1120,1680]$ & $m$ \\
        $K_w$ & hydraulic conductivity of borehole & $[9855,12045]$ & $m/yr$ \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[htb]
    \centering
    \caption{Borehole function: comparison for $d=20, 40, 60$ and $n=200, 500$.}\label{table-5}
    \begin{tabular}{ccccccc}
        \toprule
        \makecell[c]{Input dimension $d$ \\ Active dimension $p$} & Train size $n$ & Method & \makecell[c]{Standard RMSE \\ (Standard Deviation)} & \makecell[c]{False \\ Positive} & \makecell[c]{False \\ Negative}  \\ \midrule
        \multirow{8}*{\makecell[c]{$d=20$ \\ $p=8$}} & \multirow{4}*{$n=200$} & lagp & $1.4073(0.8661)$ & / & / \\
        ~ & ~ & mlegp & $0.0451(0.0411)$ & $2.14$ & $3.36$ \\
        ~ & ~ & MRFA  & $0.1395(0.0196)$ & $3.64$  & $2.08$ \\
        ~ & ~ & optK  & $\bm{0.0776(0.0327)}$ & $\bm{0.12}$ & $\bm{3.04}$ \\ \cline{2-6}
        ~ & \multirow{4}*{$n=500$} & lagp & $0.0513(0.0036)$ & / & / \\
        ~ & ~ & mlegp & $0.0640(0.0842)$ & $3.85$ & $2.35$ \\
        ~ & ~ & MRFA  & $0.0434(0.0051)$ & $3.3$  & $1.9$ \\
        ~ & ~ & optK  & $\bm{0.0442(0.0245)}$ & $\bm{0}$ & $\bm{3.05}$ \\ \midrule
        \multirow{8}*{\makecell[c]{$d=40$ \\ $p=8$}} & \multirow{4}*{$n=200$} & lagp & $1.9740(0.0167)$ & / & / \\
        ~ & ~ & mlegp & $0.0736(0.1087)$ & $3.9$ & $3.86$ \\
        ~ & ~ & MRFA  & $0.1471(0.0279)$ & $7.66$  & $2.5$ \\
        ~ & ~ & optK  & $\bm{0.0792(0.0308)}$ & $\bm{0.08}$ & $\bm{3.12}$ \\ \cline{2-6}
        ~ & \multirow{4}*{$n=500$} & lagp & $1.9780(0.0181)$ & / & / \\
        ~ & ~ & mlegp & $0.0250(0.0099)$ & $0.25$ & $4.85$ \\
        ~ & ~ & MRFA  & $0.0517(0.0062)$ & $7.35$  & $2.25$ \\
        ~ & ~ & optK  & $\bm{0.0435(0.0238)}$ & $\bm{0.05}$ & $\bm{3.05}$ \\ \midrule
        \multirow{8}*{\makecell[c]{$d=60$ \\ $p=8$}} & \multirow{4}*{$n=200$} & lagp & $1.9738(0.0159)$ & / & / \\
        ~ & ~ & mlegp & $0.1690(0.0402)$ & $50.66$ & $0.1$ \\
        ~ & ~ & MRFA  & $0.1508(0.0232)$ & $9.84$  & $2.46$ \\
        ~ & ~ & optK  & $\bm{0.0750(0.0248)}$ & $\bm{0.08}$ & $\bm{3.06}$ \\ \cline{2-6}
        ~ & \multirow{4}*{$n=500$} & lagp & $1.9852(0.0189)$ & / & / \\
        ~ & ~ & mlegp & $0.0729(0.0219)$ & $31.4$ & $1.5$ \\
        ~ & ~ & MRFA  & $0.0498(0.0070)$ & $11.05$  & $2.15$ \\
        ~ & ~ & optK  & $\bm{0.0571(0.0400)}$ & $\bm{0.1}$ & $\bm{3.2}$ \\ \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[htb]
    \centering
    \begin{subfigure}[htb]{.45\linewidth}
    \includegraphics[width=\linewidth]{Borehole_60dim_boxplot_200_50_2.pdf}
    \caption{n=200}
    \end{subfigure}
    \begin{subfigure}[htb]{.45\linewidth}
    \includegraphics[width=\linewidth]{Borehole_60dim_boxplot_500_20_2.pdf}
    \caption{n=500}
    \end{subfigure}
    \caption{Borehole function: boxplots of standard RMSEs for $d=60$ and $n=200, 500$.}\label{figure-5}
\end{figure}



\subsection{Predicting satellite drag}

Accurate estimation of satellite drag coefficients in low Earth orbit (LEO) is critical for precise positioning and collision avoidance. 
\cite{gramacy2020surrogates} provides a practical example of satellite drag modeling. 
Researchers at Los Alamos National Laboratory (LANL) developed a simulator that simulates the environment variables encountered by satellites in LEO.
A computer experimental data set of size $N=1000$ by the LANL simulator for the GRACE (Gravity Recovery and Climate Experiment) satellite is available from the website of \cite{gramacy2020surrogates}. 
The input variables of the simulator are the position and orientation of a satellite, which are listed in Table \ref{table-6} alongside their units and ranges. 
They are the same in Table 2.5 in \cite{gramacy2020surrogates} except for the ranges of input variable yaw ($\theta$) and pitch ($\phi$) angles, which are the narrowed version according to Table 2.6 from \cite{gramacy2020surrogates}. 
The response variables are the pure species of atmospheric chemicals. 
In our example here, we focus on pure helium ($He$), molecular nitrogen ($N_2$), and molecular oxygen ($O_2$).
Therefore, there are $p=7$ active variables and three response variables. 
Here, we model the three responses separately since our purpose is to demonstrate the proposed \texttt{optK}. 

In each simulation, we randomly select a training data set of size $n=200$ or $n=500$ and another testing data set of $m=100$ from the complete data set of size $N=1000$. 
We also add $7$ extra irrelevant input variables and thus $d=14$. 
The total number of simulations is $B=50$ when $n=200$ and $B=20$ when $n=500$. 
Tables \ref{table-7}, \ref{table-8}, and \ref{table-9} show the average performance measures for the three response variables using \texttt{lagp}, \texttt{mlegp}, \texttt{MRFA}, and \texttt{optK}. 
Figure \ref{figure-6}, \ref{figure-7}, \ref{figure-8} are the boxplots of the standard RMSEs returned by the four methods for $d=14$ and $n=200, 500$.
The results indicate that \texttt{lagp} performs well in these examples. 
In contrast, \texttt{mlegp} exhibits large standardized RMSE values and generally poor performance. 
\texttt{MRFA} is slightly worse than \texttt{lagp} in prediction accuracy. 
\texttt{optK} outperforms \texttt{lagp} in terms of accuracy and stability across different training sizes and response variables. 
It achieves low false positives and approximately one false negative, suggesting that one of the seven active variables may not be truly influential to responses. 

\begin{table}[htp]
    \centering
    \caption{Satellite design variables (AC: accommodation coefficient)}\label{table-6}
    \begin{tabular}{ccccccc}
        \toprule
        Symbol[ascii] & Parameter [units] & Range \\ 
        \midrule
        $v_{rel}$ [Umag] & velocity [$m/s$] & $[5500,9500]$ \\
        $T_s$ [Ts] & surface temperature [$K$] & $[100,500]$ \\
        $T_a$ [Ta] & atmospheric temperature [$K$] & $[200,2000]$ \\
        $\theta$ [theta] & yaw [radians] & $[0,0.06]$ \\
        $\phi$ [phi] & pitch [radians] & $[-0.06,0.06]$ \\
        $\alpha_n$ [alphan] & normal energy AC [unitless] & $[0,1]$ \\
        $\sigma_t$ [sigmat] & tangential momentum AC [unitless] & $[0,1]$ \\ 
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[htb]
    \centering
    \caption{Satellite Drag: comparison on the modeling of $He$.}\label{table-7}
    \begin{tabular}{ccccccc}
        \toprule
        \makecell[c]{Input dimension $d$ \\ Active dimension $p$} & Train size $n$ & Method & \makecell[c]{Standard RMSE \\ (Standard Deviation)} & \makecell[c]{False \\ Positive} & \makecell[c]{False \\ Negative}  \\ \midrule
        \multirow{8}*{\makecell[c]{$d=14$ \\ $p=7$}} & \multirow{4}*{$n=200$} & lagp & $0.1080(0.0139)$ & / & / \\ 
        ~ & ~ & mlegp & $0.3093(0.1062)$ & $6.76$ & $0.78$ \\ 
        ~ & ~ & MRFA  & $0.2655(0.2690)$ & $2.06$  & $1$ \\ 
        ~ & ~ & optK  & $\bm{0.0991(0.0101)}$ & $\bm{0.2}$ & $\bm{1}$  \\ \cline{2-6}
        ~ & \multirow{4}*{$n=500$} & lagp & $0.0753(0.0098)$ & / & / \\ 
        ~ & ~ & mlegp & $0.2566(0.0744)$ & $6.65$ & $0.65$ \\ 
        ~ & ~ & MRFA & $0.0779(0.0021)$ & $1.65$ & $1$  \\ 
        ~ & ~ & optK  & $\bm{0.0714(0.0135)}$ & $\bm{0.1}$ & $\bm{0.95}$  \\ 
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[htb]
    \centering
    \caption{Satellite Drag: comparison on the modeling of $N_2$.}\label{table-8}
    \begin{tabular}{ccccccc}
        \toprule
        \makecell[c]{Input dimension $d$ \\ Active dimension $p$} & Train size $n$ & Method & \makecell[c]{Standard RMSE \\ (Standard Deviation)} & \makecell[c]{False \\ Positive} & \makecell[c]{False \\ Negative}  \\ \midrule
        \multirow{8}*{\makecell[c]{$d=14$ \\ $p=7$}} & \multirow{4}*{$n=200$} & lagp & $0.1353(0.0100)$ & / & / \\ 
        ~ & ~ & mlegp & $0.3627(0.1044)$ & $6.6$ & $0.94$ \\ 
        ~ & ~ & MRFA  & $0.2558(0.4651)$ & $1.7$  & $0.92$ \\ 
        ~ & ~ & optK  & $\bm{0.1291(0.0365)}$ & $\bm{0.2}$ & $\bm{1.32}$  \\ \cline{2-6}
        ~ & \multirow{4}*{$n=500$} & lagp & $0.1025(0.0073)$ & / & / \\ 
        ~ & ~ & mlegp & $0.3590(0.1112)$ & $6.7$ & $0.75$ \\ 
        ~ & ~ & MRFA & $0.1658(0.4127)$ & $1.95$ & $1$  \\ 
        ~ & ~ & optK  & $\bm{0.0763(0.0269)}$ & $\bm{0.2}$ & $\bm{1.05}$  \\ 
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[htb]
    \centering
    \caption{Satellite Drag: comparison on the modeling of $O_2$.}\label{table-9}
    \begin{tabular}{ccccccc}
        \toprule
        \makecell[c]{Input dimension $d$ \\ Active dimension $p$} & Train size $n$ & Method & \makecell[c]{Standard RMSE \\ (Standard Deviation)} & \makecell[c]{False \\ Positive} & \makecell[c]{False \\ Negative}  \\ \midrule
        \multirow{8}*{\makecell[c]{$d=14$ \\ $p=7$}} & \multirow{4}*{$n=200$} & lagp & $0.1300(0.0193)$ & / & / \\ 
        ~ & ~ & mlegp & $0.3741(0.1307)$ & $6.74$ & $0.7$ \\ 
        ~ & ~ & MRFA  & $0.1569(0.1084)$ & $2.12$  & $0.92$ \\ 
        ~ & ~ & optK  & $\bm{0.1196(0.0280)}$ & $\bm{0.1}$ & $\bm{1.02}$  \\ \cline{2-6}
        ~ & \multirow{4}*{$n=500$} & lagp & $0.0885(0.0050)$ & / & / \\ 
        ~ & ~ & mlegp & $0.2848(0.0962)$ & $6.85$ & $0.55$ \\ 
        ~ & ~ & MRFA & $0.1221(0.2670)$ & $2.3$ & $0.2$  \\ 
        ~ & ~ & optK  & $\bm{0.0661(0.0193)}$ & $\bm{0.15}$ & $\bm{1}$  \\ 
        \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[htb]
    \centering
    \begin{subfigure}[htb]{.45\linewidth}
    \includegraphics[width=\linewidth]{Satellite_he_boxplot_200.pdf}
    \caption{n=200}
    \end{subfigure}
    \begin{subfigure}[htb]{.45\linewidth}
    \includegraphics[width=\linewidth]{Satellite_he_boxplot_500.pdf}
    \caption{n=500}
    \end{subfigure}
    \caption{Satellite Drag: boxplots of standard RMSEs for $He$.}\label{figure-6}
\end{figure}

\begin{figure}[htb]
    \centering
    \begin{subfigure}[htb]{.45\linewidth}
    \includegraphics[width=\linewidth]{Satellite_n2_boxplot_200.pdf}
    \caption{n=200}
    \end{subfigure}
    \begin{subfigure}[htb]{.45\linewidth}
    \includegraphics[width=\linewidth]{Satellite_n2_boxplot_500.pdf}
    \caption{n=500}
    \end{subfigure}
    \caption{Satellite Drag: boxplots of standard RMSEs for $N_2$.}\label{figure-7}
\end{figure}

\begin{figure}[htb]
    \centering
    \begin{subfigure}[htb]{.45\linewidth}
    \includegraphics[width=\linewidth]{Satellite_o2_boxplot_200.pdf}
    \caption{n=200}
    \end{subfigure}
    \begin{subfigure}[htb]{.45\linewidth}
    \includegraphics[width=\linewidth]{Satellite_o2_boxplot_500.pdf}
    \caption{n=500}
    \end{subfigure}
    \caption{Satellite Drag: boxplots of standard RMSEs for $O_2$.}\label{figure-8}
\end{figure}

\section{Conclusion}\label{sec:end}

In this paper, we propose an optimal kernel learning approach to overcome the bottleneck of high-dimensional input for GP regression models. 
It approximates the covariance kernel function of the original input variables via a convex combination of kernel functions of lower-dimensional input variables. 
This is equivalent to approximating the original GP by functional ANOVA decomposition, which leads to an additive GP model with each added component being a GP of low-dimensional input space.
The optimal kernel learning problem is formulated as a minimization of a regularized least square loss function. 
Inspired by a similar optimization problem and the Fedorov-Wynn-type of algorithm in optimal design, we develop an optimal kernel learning algorithm that sequentially selects kernels from the candidate set and simultaneously updates the weights of all selected the kernels. 
For high-dimensional input data, the candidate set of low-dimensional kernels can be large, so the computation needed to prepare the candidate kernels also increases drastically.
To overcome this issue, we follow the eﬀect heredity principle from the design and analysis of experiments literature, and construct the candidate set of kernel functions stage-wise based on the previously selected input variables. 

GP regression is one speical case of the kernel methods that compute inner products between data points using
kernel functions and enable efficient non-linear learning across diverse domains. 
By mapping data to spaces where linear separation becomes possible, kernel methods provide remarkable flexibility, computational efficiency, and robust generalization capabilities. 
Selecting appropriate kernel functions and associated parameters continues to be a critical theoretical and computational problem. 
The performance of kernel methods heavily depends on factors such as function smoothness and bandwidth parameters. 
This proposed optimal kernel learning framework can be widely applied across kernel-based techniques, including support vector machine classification, spline methos, and even deep kernel networks. 
We are going to pursue these directions in the future and address the fundamental bottleneck in kernel-based statistical learning methods: effectively handling high-dimensional input data.


\bibliographystyle{ECAjasa}
\bibliography{Ref}

\setcounter{figure}{0}
\setcounter{table}{0}
\setcounter{lemma}{0}
\setcounter{theorem}{0}
\setcounter{proposition}{0}

\makeatletter 
\renewcommand{\thefigure}{A\@arabic\c@figure}
\renewcommand{\thetable}{A\@arabic\c@table}
\renewcommand{\thelemma}{A\@arabic\c@lem}
\renewcommand{\theproposition}{A\@arabic\c@proposition}
\renewcommand{\thetheorem}{A\@arabic\c@theorem}
\makeatother

\section*{Appendix}

\subsection*{A1. Proof of Lemma \ref{lemma:convex}}
\begin{proof}
Following the notation and assumption in Section \ref{sec:opt}, we have 
\[
Q_{\eta}((1-\alpha)K+\alpha K')\leq (1-\alpha)Q_{\eta}(K)+\alpha Q_{\eta}(K'), \quad \forall \alpha \in [0,1],
\]
due to the convexity of $Q_{\eta}(\cdot)$ in $\mathcal{K}$. 
Since $\mathcal{K}=\overline{\text{conv}(\mathcal{G})}$, let $\xi$ and $\xi'$ in $\Xi$ such that $K=\int G\xi(\dd G)$ and $K'=\int G\xi'(\dd G)$. 
Then, 
\begin{align*}
&Q_{\eta}((1-\alpha)\xi+\alpha \xi')=Q_{\eta}\left(K((1-\alpha)\xi+\alpha \xi')\right)=Q_{\eta}\left(\int G ((1-\alpha)\xi+\alpha \xi')(\dd G)\right)\\
=&Q_{\eta}\left(\int G ((1-\alpha)\xi)(\dd G)+\int G (\alpha \xi')(\dd G)\right)=Q_{\eta}\left((1-\alpha)\int G \xi(\dd G)+\alpha \xi' \int G \xi'(\dd G))\right)\\
=&Q_{\eta}\left((1-\alpha)K(\xi)+\alpha K(\xi')\right)\leq (1-\alpha)Q_{\eta}(K(\xi))+\alpha Q_{\eta}(K(\xi'))=(1-\alpha)Q_{\eta}(\xi)+\alpha Q_{\eta}(\xi').
\end{align*}
Therefore, $Q_{\eta}(\cdot)$ is also convex in $\Xi$. 
\end{proof}

\subsection*{A2. Proof of Proposition \ref{prop:dir-diff}}

\begin{proof}
To simplify notation, we omit the design and let $K=K(\xi)$, $K'=K(\xi')$, $\bm K=\bm K_{\xi}$, $\bm K'=\bm  K_{\xi'}$. 
\begin{equation*}
\begin{aligned}
Q_\mu(K)&=(\bm y-\bm K\bm c^*)^\top (\bm y-\bm K\bm c^*)+\eta \bm (c^*)^\top \bm K \bm c^* \qquad with \qquad \bm c^*=(\bm K+\eta \bm I_n)^{-1}\bm y\\
&=(\bm y-\bm K(\bm K+\eta \bm I_n)^{-1}\bm y)^\top (\bm y-\bm K(\bm K+\eta \bm I_n)^{-1}\bm y)+\eta \bm y^\top (\bm K+\eta \bm I_n)^{-\top}\bm K(\bm K+\eta \bm I_n)^{-1}\bm y\\
&=\bm y^\top (\bm I_n-\bm K(\bm K+\eta \bm I_n)^{-1})^\top (\bm I-\bm K(\bm K+\eta \bm I_n)^{-1})\bm y+\eta \bm y^\top(\bm K+\eta \bm I_n)^{-\top}\bm K(\bm K+\eta \bm I_n)^{-1}\bm y\\
&=\bm y^\top (\bm I_n-(\bm K+\eta \bm I_n)^{-\top}\bm K^\top -\bm K(\bm K+\eta \bm I_n)^{-1}+(\bm K+\eta \bm I_n)^{-\top}\bm K^\top \bm K(\bm K+\eta \bm I_n)^{-1})\bm y\\
&\quad+\eta \bm y^\top (\bm K+\eta \bm I_n)^{-\top}\bm K(\bm K+\eta \bm I_n)^{-1}\bm y\\
&=\bm y^\top \left[\bm I_n-(\bm K+\eta \bm I_n)^{-1}\bm K-\bm K(\bm K+\eta \bm I_n)^{-1}+(\bm K+\eta \bm I_n)^{-1}\bm K^2(\bm K+\eta \bm I_n)^{-1}\right]\bm y\\
&\quad+\eta \bm y^\top (\bm K+\eta \bm I_n)^{-1}\bm K(\bm K+\eta \bm I_n)^{-1}\bm y
\end{aligned}       
\end{equation*}

Compute the derivative of different part of $Q_\eta(K)$ with $\alpha$, considering $K$ is a function of $\alpha$.
\begin{align*}
&\frac{\partial (\bm K+\eta \bm I_n)^{-\top} \bm K^\top}{\partial \alpha}=-(\bm K+\eta \bm I_n)^{-1}\frac{\partial \bm K}{\partial\alpha}(\bm K+\eta \bm I_n)^{-1}\bm K+(\bm K+\eta \bm I_n)^{-1}\frac{\partial \bm K}{\partial\alpha}=\bm A+\bm B\\
&\frac{\partial \bm K(\bm K+\eta \bm I_n)^{-1}}{\partial \alpha}=-\bm K(\bm K+\eta \bm I_n)^{-1}\frac{\partial \bm K}{\partial\alpha}(\bm K+\eta \bm I_n)^{-1}+\frac{\partial \bm K}{\partial\alpha}(\bm K+\eta \bm I_n)^{-1}=\bm A^\top +\bm B^\top
\end{align*}
\begin{align*}
&\frac{\partial (\bm K+\eta \bm I_n)^{-1}\bm K^2 (\bm K+\eta \bm I_n)^{-1}}{\partial \alpha}\\
=&-(\bm K+\mu \bm I_n)^{-1}\frac{\partial \bm K}{\partial \alpha} (\bm K+\eta \bm I_n)^{-1}\bm K^2 (\bm K+\eta \bm I_n)^{-1}+(\bm K+\eta \bm I_n)^{-1}\left[\frac{\partial \bm K}{\partial \alpha} \bm K+\bm K\frac{\partial \bm K}{\partial\alpha}\right](\bm K+\eta\bm  I_n)^{-1}\\
&-(\bm K+\eta \bm I_n)^{-1}\bm K^2(\bm K+\eta \bm I_n)^{-1}\frac{\partial \bm K}{\partial\alpha}(\bm K+\eta \bm I_n)^{-1}\\
=&\bm A \bm K(\bm K+\eta\bm I_n)^{-1}+\bm B \bm K(\bm K+\eta\bm I_n)^{-1}+(\bm K+\eta \bm I_n)^{-1}\bm K\bm B^\top +(\bm K+\eta \bm I_n)^{-1}\bm K\bm A^\top
\end{align*}
\begin{align*}
&\frac{\partial (\bm K+\eta \bm I_n)^{-1}\bm K(\bm K+\eta \bm I_n)^{-1}}{\partial \alpha}\\
=&-(\bm K+\mu \bm I_n)^{-1}\frac{\partial \bm K}{\partial \alpha} (\bm K+\eta \bm I_n)^{-1}\bm K(\bm K+\eta \bm I_n)^{-1}+(\bm K+\eta \bm I_n)^{-1}\frac{\partial \bm K}{\partial \alpha}(\bm K+\eta\bm  I_n)^{-1}\\
&-(\bm K+\eta \bm I_n)^{-1}\bm K(\bm K+\eta \bm I_n)^{-1}\frac{\partial \bm K}{\partial\alpha}(\bm K+\eta \bm I_n)^{-1}\\
=&\bm A(\bm K+\eta \bm I_n)^{-1}+(\bm K+\eta \bm I_n)^{-1}\bm A^\top+(\bm K+\eta \bm I_n)^{-1}\frac{\partial \bm K}{\partial \alpha}(\bm K+\eta\bm  I_n)^{-1}.
\end{align*}
Sum up the derivatives and some of the terms are canceled. 
\begin{align*}
&-\frac{\partial (\bm K+\eta \bm I_n)^{-\top} \bm K^\top}{\partial \alpha}-\frac{\partial \bm K(\bm K+\eta \bm I_n)^{-1}}{\partial \alpha}+\frac{\partial (\bm K+\eta \bm I_n)^{-1}\bm K^2 (\bm K+\eta \bm I_n)^{-1}}{\partial \alpha}\\
&+\eta\frac{\partial (\bm K+\eta \bm I_n)^{-1}\bm K(\bm K+\eta \bm I_n)^{-1}}{\partial \alpha}\\
=&-\bm A -\bm B-\bm A^\top -\bm B^\top + \bm A \bm K(\bm K+\eta\bm I_n)^{-1}+\bm B \bm K(\bm K+\eta\bm I_n)^{-1}+(\bm K+\eta \bm I_n)^{-1}\bm K\bm B^\top \\
&+(\bm K+\eta \bm I_n)^{-1}\bm K\bm A^\top+\eta\bm A(\bm K+\eta \bm I_n)^{-1}+\eta (\bm K+\eta \bm I_n)^{-1}\bm A^\top+\eta (\bm K+\eta \bm I_n)^{-1}\frac{\partial \bm K}{\partial \alpha}(\bm K+\eta\bm  I_n)^{-1}\\
=&\bm A\left[-\bm I_n+\bm K(\bm K+\eta \bm I_n)^{-1}+\eta (\bm K+\eta \bm I_n)^{-1}\right]+\left[-\bm I_n+(\bm K+\eta \bm I_n)^{-1}\bm K+\eta (\bm K+\eta \bm I_n)^{-1}\right]\bm A^\top \\
&+\bm B(\bm K(\bm K+\eta \bm I_n)^{-1}-\bm I_n)+((\bm K+\eta \bm I_n)^{-1}\bm K-\bm I_n)\bm B^\top 
+\eta (\bm K+\eta \bm I_n)^{-1}\frac{\partial \bm K}{\partial \alpha}(\bm K+\eta\bm  I_n)^{-1}\\
=&\bm A\left[-\bm K-\eta \bm I_n+\bm K+\eta\bm I_n \right](\bm K+\eta \bm I_n)^{-1}+(\bm K+\eta \bm I_n)^{-1}\left[-\bm K-\eta \bm I_n+\bm K+\eta \bm I_n \right]\bm A^\top \\
&+\bm B(\bm K-\bm K-\eta \bm I_n)(\bm K+\eta \bm I_n)^{-1}+(\bm K+\eta \bm I_n)^{-1}(\bm K-\bm K-\eta \bm I_n)\bm B^\top 
+\eta (\bm K+\eta \bm I_n)^{-1}\frac{\partial \bm K}{\partial \alpha}(\bm K+\eta\bm  I_n)^{-1}\\
=&-\eta (\bm K+\eta \bm I_n)^{-1}\frac{\partial \bm K}{\partial \alpha}(\bm K+\eta\bm  I_n)^{-1}.
\end{align*}
The derivative of $Q_{\eta}(K)$ w.r.t. $\alpha$ is
\begin{equation*}
\frac{\partial Q_{\eta}(K)}{\partial \alpha}=-\eta \bm y^\top (\bm K+\eta \bm I_n)^{-1}\frac{\partial \bm K}{\partial \alpha}(\bm K+\eta\bm  I_n)^{-1}\bm y.
\end{equation*}
Replace the general $K$ in the above derivative by $(1-\alpha)K+\alpha K'$ for any $K, K'\in \mathcal{K}$. 
It is apparent that 
\[
\frac{\partial ((1-\alpha)K+\alpha K')}{\partial \alpha}=K'- K.
\]
The corresponding derivative of the kernel matrix is 
\[
\frac{\partial ((1-\alpha)\bm K+\alpha \bm K')}{\partial \alpha}=\bm K'-\bm K.
\]
Finally, we obtain
\begin{equation*}
\phi (K^{\prime},K)=\frac{\partial Q_\eta(K)}{\partial \alpha}\bigg|_{\alpha =0}=-\eta \bm y^\top ((\bm K+\eta \bm I_n)^{-1}(\bm K^{\prime}-\bm K)(\bm K+\eta \bm I_n)^{-1})\bm y,
\end{equation*}
and $\phi(\xi', \xi)$ follows by definition. 
\end{proof}

\subsection*{A3. Proof of Theorem \ref{thm:GE}}

\begin{proof}
We show the equivalence of the three statements by showing (i) the equivalence of (1) and (2); (ii) (1) is sufficient for (3); and (iii) (3) is sufficient for (2).
\begin{enumerate}
\item (1) $\leftrightarrow$ (2): Based on the convexity of $Q_{\eta}(\xi)$ in terms of $\xi$, the equivalence of (1) and (2) is a direct result. 
\item (1) $\to$ (3): Since $\phi(G, \xi^*)$ is simply $\phi(\xi_G, \xi^*)$, a special case of $\phi(\xi', \xi^*)$, and thus if (1) is true, we have $\phi(G,\xi^*)\geq 0$ for any $G\in \mathcal{G}$. 


Next, we need to show that given (1), if $G$ is a support kernel of $\xi^*$, then $\phi(G,\xi^*)=0$. 
We assume that the design $\xi^*$ consists of support kernel functions $\{K_1,\ldots, K_m\}$ which is a subset of $\mathcal{G}$. 
The corresponding weights are $\{\lambda_1,\ldots, \lambda_m\}$ and for $i=1,\ldots, m$, $0<\lambda_i<1$ if $m\geq 2$. 
Without loss of generality, we only need to show that $\phi(K_1,\xi^*)=0$. 
From Section \ref{sec:learning}, we know that $m$ is finite, but the following proof still applies if $m$ is infinite.

We can write $K(\xi^*)$ as
\[K^*=\lambda_1K_1+\sum_{i>1} \lambda_i K_i=\lambda_1K_1+(1-\lambda_1)K^*_{-1},\]
where $K_{-1}^*=\sum_{i>1}\frac{\lambda_i}{1-\lambda_1} K_i$, and thus $K_{-1}^*$ is a convex of $K_2,\ldots, K_m$ and belongs to $\mathcal{K}$. 
Denote the design corresponding to $K_{-1}^*$ by $\xi_{-1}^*$, which has support kernels $\{K_2,\ldots,K_m\}$ and weights $\{\lambda_2/(1-\lambda_1),\ldots, \lambda_m/(1-\lambda_1)\}$. 
Next we prove $\phi(K_1,\xi^*)=0$ by contradiction.

Given (1), we have $\phi(\xi_{-1}^*, \xi^*) \geq 0$. Following Proposition \ref{prop:dir-diff}, 
\begin{align}\nonumber
& \bm y^\top (\bm K^*+\eta \bm I_n)^{-1}(\bm K_{-1}^* - \bm K^*)(\bm K^*+\eta \bm I_n)\bm y\leq 0\\\label{eq:neq1}
\Longleftrightarrow \quad & \bm y^\top (\bm K^*+\eta \bm I_n)^{-1}\bm K_{-1}^* (\bm K^*+\eta \bm I_n)\bm y \leq  \bm y^\top (\bm K^*+\eta \bm I_n)^{-1} \bm K^*(\bm K^*+\eta \bm I_n)\bm y.
\end{align}
Also, $\bm K_1$ is a positive definite matrix and thus $\bm y^\top (\bm K^*+\eta \bm I_n)^{-1}\bm K_1(\bm K^*+\eta \bm I_n)\bm y\geq 0$.

If $\phi(K_1,\xi^*) >0$ instead of $\phi(K_1,\xi^*)=0$, then 
\begin{equation}\label{eq:neq2}
\bm y^\top (\bm K^*+\eta \bm I_n)^{-1}\bm K_1(\bm K^*+\eta \bm I_n)\bm y < \bm y^\top (\bm K^*+\eta \bm I_n)^{-1}\bm K^*(\bm K^*+\eta \bm I_n)\bm y.
\end{equation}
From the two inequalities \eqref{eq:neq1} and \eqref{eq:neq2},
\begin{align*}
&\bm y^\top (\bm K^*+\eta \bm I_n)^{-1}\bm K^* (\bm K^*+\eta \bm I_n)^{-1}\bm y
=\bm y^\top (\bm K^*+\eta \bm I_n)^{-1}(\lambda_1 \bm K_1+(1-\lambda_1)\bm K_{-1}^*)(\bm K^*+\eta \bm I_n)\bm y\\
=&\lambda_1 \bm y^\top (\bm K^*+\eta \bm I_n)^{-1}\bm K_1(\bm K^*+\eta \bm I_n)\bm y+(1-\lambda_1)\bm y^\top (\bm K^*+\eta \bm I_n)^{-1}\bm K_{-1}^*(\bm K^*+\eta \bm I_n)\bm y \\
< & \lambda_1 \bm y^\top (\bm K^*+\eta \bm I_n)^{-1}\bm K^*(\bm K^*+\eta \bm I_n)\bm y + (1-\lambda_1)\bm y^\top (\bm K^*+\eta \bm I_n)^{-1}\bm K^* (\bm K^*+\eta \bm I_n)^{-1}\bm y^\top\\
=& \bm y^\top (\bm K^*+\eta \bm I_n)^{-1}\bm K^* (\bm K^*+\eta \bm I_n)^{-1}\bm y,
\end{align*}
which is a contradiction. 
Thus, it is only possible that $\phi(K_1,\xi^*)=0$.

\item (3)$\to$(2): if $\phi(G,\xi^*)\geq 0$ holds for any $G\in \mathcal{G}$, then
\begin{align*}
\phi(G, \xi^*)=-\eta \bm y^\top ((\bm K^*+\eta \bm I_n)^{-1}(\bm G-\bm K^*)(\bm K^*+\eta \bm I_n)^{-1})\bm y \geq 0.
\end{align*}
Let $\xi'$ be any design in $\Xi$. 
The corresponding kernel matrix $\bm K_{\xi'}=\int \bm G\xi'(\dd \bm G)$ because 
$K(\xi')=\int G\xi'(\dd G)$ and $\bm G$ is the kernel matrix of the kernel function $G$ evaluated at $\mathcal{X}$. 
Therefore, 
\begin{align*}
\phi(\xi',\xi^*)&=-\eta \bm y^\top ((\bm K^*+\eta \bm I_n)^{-1}(\bm K_{\xi'}-\bm K^*)(\bm K^*+\eta \bm I_n)^{-1})\bm y \\
&=-\eta \bm y^\top ((\bm K^*+\eta \bm I_n)^{-1}(\int \bm G\xi'(\dd \bm G)-\bm K^*)(\bm K^*+\eta \bm I_n)^{-1})\bm y\\
&=\int -\eta \bm y^\top ((\bm K^*+\eta \bm I_n)^{-1}(\bm G-\bm K^*)(\bm K^*+\eta \bm I_n)^{-1})\bm y \xi'(\dd \bm G)\geq 0.
\end{align*}
\end{enumerate}
\end{proof}

\subsection*{A4. Proof of Theorem \ref{thm:convergence}}

\noindent{\bf Proof of Lemma \ref{lem:inequality}}
\begin{proof}
Denote $K$ as the solution $K=\argmin_{G\in \mathcal{G}}\phi(G,\xi)$. 
Thus, for any $G\in \mathcal{G}$, $\phi(K,\xi)\leq \phi(G,\xi)$. 
The optimal design $\xi^*$ has the support kernel set $\mathcal{S}=\{K_1,\ldots, K_m\}$ and the corresponding optimal weights $\bm \lambda^*=[\lambda^*_1,\ldots, \lambda_m^*]^\top$ such that $0<\lambda_i^*\leq 1$ and $\sum_{i=1}^m \lambda_i^*=1$. 
Based on the definition of $K$, we also have $\phi(K,\xi)\leq \phi(K_i,\xi)$ for any $K_i\in \mathcal{S}$. 
From the formula \eqref{eq:dir-diff} to compute directional derivative, it is straightforward to obtain $\phi(\xi^*, \xi)=\sum_{i=1}^m \lambda_{i}^*\phi(K_i, \xi)$. 
Therefore, 
\[
\phi(K,\xi)=\sum_{i=1}^m \lambda_i^* \phi(K,\xi)\leq \sum_{i=1}^m \lambda_i^* \phi(K_i,\xi)=\phi(\xi^*,\xi).
\]
Furthermore, based on the definition of directional derivative in Definition \ref{def:dir-diff}, 
\begin{align*}
\phi(\xi^*,\xi)&=\lim_{\alpha \to 0^+} \frac{1}{\alpha}\left[Q_{\eta}((1-\alpha)\xi+\alpha \xi^*)-Q_{\eta}(\xi)\right]\\
&\leq \lim_{\alpha \to 0^+} \frac{1}{\alpha} \left[(1-\alpha)Q_{\eta}(\xi)+\alpha Q_{\eta}(\xi^*)-Q_{\eta}(\xi)\right] \quad \text{due to the convexity of $Q_{\eta}(\xi)$ in Lemma \ref{lemma:convex}.}\\
&=Q_{\eta}(\xi^*)-Q_{\eta}(\xi).
\end{align*}
Since $\xi^*$ is the optimal design minimizing $Q_{\eta}$, $Q_{\eta}(\xi^*)-Q_{\eta}(\xi)\leq 0$. 
\end{proof}

\noindent{\bf Proof of Theorem \ref{thm:convergence}}
\begin{proof}
The proof is established by proof of contradiction. Suppose that Algorithm \ref{alg:multiplicative} does not converge to the optimal design $\xi^*$, then we have 
\[
\lim_{r\to \infty} Q_{\eta}(\xi^r) > Q_{\eta}(\xi^*).
\]
For any iteration $r+1\geq 1$, since $\mathcal{S}^r \subset \mathcal{S}^{r+1}$ and the optimal weight procedure in Algorithm \ref{alg:multiplicative} returns the optimal weights, the optimal design $\xi^{r+1}$ cannot be worse than the design in the previous iteration $\xi^r$, i.e.,
\[
Q_{\eta}(\xi^{r+1})\leq Q_{\eta}(\xi^r).
\]
Thus, for all $r\geq 0$, there exist $c>0$ such that 
\[
Q_{\eta}(\xi^{r})> Q_{\eta}(\xi^*)+c.
\]
Recall from Algorithm \ref{alg:fed-wynn}, $K_{r+1}=\argmin_{G\in \mathcal{G}\setminus\mathcal{S}^{r}} \phi(G,\xi^{r})$. 
According to Lemma \ref{lem:inequality},
\[
\phi(K_{r+1}, \xi^{r})\leq \phi(\xi^*,\xi^{r})\leq Q_{\eta}(\xi^*)-Q_{\eta}(\xi^{r})<-c,
\]
for any $r\geq 0$. Then, the Taylor expansion of $Q_{\eta}((1-\alpha)\xi^{r}+\alpha K_{r+1})$ w.r.t. $\alpha$ has the following upper bound. 
Here we slightly abuse the notation and consider $K_{r+1}$ as $\xi_{K_{r+1}}$ assigning unit probability to the single kernel $K_{r+1}$.
\begin{align*}
Q_{\eta}((1-\alpha)\xi^r+\alpha K_{r+1})&=Q_{\eta}(\xi^{r})+\phi(K_{r+1}, \xi^{r})\alpha+\frac{u}{2}\alpha^2\\
&< Q_{\eta}(\xi^r)-c \alpha+\frac{u}{2}\alpha^2,
\end{align*}
where $u\geq 0$ is the second-order directional derivative of $Q_{\eta}(\xi)$ evaluated at a value between $0$ and $\alpha$. 
It is positive because of the convexity of $Q_{\eta}$ w.r.t. the design. 

For Algorithm \ref{alg:fed-wynn}, the loss function $Q_{\eta}(\xi)$ is minimized, for all $0\leq \alpha \leq 1$ we have 
\begin{align*}
Q_{\eta}(\xi^{r+1})&\leq Q_{\eta}((1-\alpha)\xi^r+\alpha K_{r+1})\\
&< Q_{\eta}(\xi^r)-c\alpha+\frac{u}{2}\alpha^2,
\end{align*}
or equivalently, 
\begin{align*}
Q_{\eta}(\xi^{r+1})-Q_{\eta}(\xi^r) &< -c \alpha+\frac{u}{2}\alpha^2=\frac{u}{2}\left[\alpha-\frac{c}{u}\right]-\frac{c^2}{2u}\\
& < \left\{
\begin{array}{ll}
-c^2/2u <0, & \text{choosing } \alpha= c/u \text{ if } c\leq u,\\
(u-4 c)/8 <0, & \text{choosing } \alpha=0.5 \text{ if } c > u.
\end{array}
\right.
\end{align*}
Therefore, $\lim_{r\to \infty}Q_{\eta}(\xi^r)=-\infty$, which contradicts with the fact that $Q_{\eta}(\xi^r)\geq 0$ for any design $\xi^r$. So we have shown $\lim_{r\to \infty}Q_{\eta}(\xi^r)=Q_{\eta}(\xi^*)$. 
\end{proof}

\end{document}