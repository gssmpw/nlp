\section{Protocols Implementation}\label{sec:use_cases-appendix}
We next present the implementation details of four distributed systems shown in Table~\ref{tab:use_cases_options} using \projecttitle{}, presented in Section~\ref{sec:use_cases}.

\begin{table}
\begin{center}
\small
\begin{tabular}{ |c|c|c|c| } 
 \hline
 System & $N$ & $f$ ($N=3$) & Byzantine faults \\ [0.5ex] \hline \hline
 A2M    & 1 & 0 & Prevention\\
 BFT &  $2f+1$ & $f=1$ & Prevention\\
 Chain Replication &  $f+1$ & $f=2$ & Prevention\\
 PeerReview & $f+1$ & $f=2$ & Detection\\
 \hline
\end{tabular}
\end{center}
\caption{Properties of the four trustworthy distributed systems implemented with \projecttitle{}.}
\label{tab:use_cases_options}
\end{table}

\subsection{Clients} Clients in a \projecttitle{} distributed system execute requests by sending singed request messages to \projecttitle{} nodes through the network. \projecttitle{} assumes Byzantine (untrusted) clients; as such, its installed shared keys cannot be outsourced. We assume that at the initialization, the System Designer also loads to \projecttitle{} devices a (per-device) key pair $C_{pub, priv}$ where the $C_{pub}$ is distributed to clients. \projecttitle{} then replies to a client by verifying the (under transmission) attested message and signing it with $C_{priv}$. As such, \projecttitle{} is restricted to only sending valid attested messages to clients where clients can prove the transferable authentication and validity of the message. The only attack vector open to a Byzantine machine is to try to equivocate by sending a stale, valid, attested message that does not reflect the current execution round. However, clients can detect this by verifying that the original request is theirs.    % the such replies to clients are signed with a private key within the \projecttitle{} Furthermore, clients do not need to have access to a \projecttitle{}

%Similarly to classical BFT systems, \projecttitle{} clients require a {\em quorum certificate}, a set of identical messages collected from different participants~\cite{10.1145/800215.806583} to consider their request as committed. In contrast to the traditional BFT, where any $f$ out of the total 3$f$+1 nodes could equivocate, our strategy to prevent equivocation improves message complexity, allowing clients to wait for (at least) $f+1$ identical replies to consider their request committed.


\subsection{Attested Append-Only Memory (A2M)}\label{sec:use_cases::a2m}
We designed a single-node trusted log system based on the A2M system (Attested Append-Only Memory)~\cite{A2M} using \projecttitle{}. A2M has been proven to be an effective building block in improving the scalability and performance of various classical BFT systems~\cite{sundr, Castro:2002, AbdElMalek2005FaultscalableBF}. We show the {\em how} to use \projecttitle{} to build this foundational system while we also show that \projecttitle{} minimizes the system's TCB jointly with the performance improvements demonstrated in $\S$~\ref{sec:eval}.

\myparagraph{System model} Our \projecttitle{} version and the original A2M systems are single-node systems that target a similar goal; they both build a trusted append-only log as an effective mechanism to combat equivocation. The clients can only append entries to a log; each log entry is associated with a monotonically increasing sequence number. Each data item, e.g., a network message, is bound to a unique sequence number, a well-known approach for equivocation-free operations~\cite{clement2012, hybster}. 

A2M was originally built using CPU-side TEEs---specifically, Intel SGX--- whereas we build its \projecttitle{} derivative. While the original A2M system keeps its entire state and the log within the TEE, we use \projecttitle{} to keep the (trusted) log in the untrusted memory. As such, in contrast to the original A2M, \projecttitle{} effectively reduces the overall system's TCB. Our evaluation showed that naively porting the application within the TEE has adverse performance implications in lookup operations.


%the trusted component \atsushi{Here explains that TNIC-log brings better memory efficiency than A2M, which could also be written in the first paragraph to highlight the advantage of TNIC}.

\myparagraph{Execution} Similarly to A2M, we expose three core operations: the \texttt{append}, \texttt{lookup}, and \texttt{truncate} operations to add, retrieve, and delete items of the log, respectively. A2M stores the lowest and highest sequence numbers for each log. Upon appending an entry, A2M increases the highest sequence number and associates it with the newly appended entry. When truncating the log, the system advances the lowest sequence number accordingly. We next discuss how we designed the operations using \projecttitle{} APIs.




\begin{algorithm}
\SetAlgoLined
\small
%\vspace{0.02cm}
\textbf{function} \texttt{append(id, ctx)} \{ \\
\Indp
 [$\alpha$,\texttt{i},\texttt{ctx}] $\leftarrow$ \texttt{local\_send(id,ctx)};\\
 \texttt{log[id].append(log\_entry($\alpha$,\texttt{i},\texttt{ctx}))};\\
 {\bf return} \texttt{[$\alpha$,\texttt{i},\texttt{ctx}]};\\
\Indm
\} \\

\vspace{0.15cm}

\textbf{function} \texttt{lookup(id, i)} \{ \\
\Indp
    {\bf return} \texttt{log[id].get(i)};\\
\Indm
\} \\
\vspace{0.15cm}
\textbf{function} \texttt{truncate(id, head, z)} \{ \\
\Indp
    [$\alpha$,\texttt{tail},\texttt{ctx}] $\leftarrow$ \texttt{append(id,} \textsc{trnc}\texttt{||id||z||head)};\\
        
    %[$\alpha_{2}$,\texttt{idx},\texttt{ctx}$_{2}$]
    \texttt{e} $\leftarrow$ \texttt{append(}\textsc{manifest}\texttt{,[$\alpha$,\texttt{tail},\texttt{ctx}])};\\
    {\bf return} \texttt{e};\\
\Indm
\} \\

\vspace{0.15cm}
\textbf{function} \texttt{verify\_lookup(id, e, head, tail)} \{ \\
\Indp
    \textbf{assert}(\texttt{e.i}$>=$\texttt{tail)};\\
    \texttt{local\_verify(id, e)};\\
\Indm
\} \\
\vspace{-1pt}
\caption{Attested Append-Only Memory (A2M) using \projecttitle{}.}
\label{algo:tnic_log}
\end{algorithm}

\myparagraph{Append operation} The \texttt{append(id,ctx)} operation takes a data item, \texttt{ctx}, and appends it to the log with identifier \texttt{id}. A log entry at index \texttt{i} is comprised of three items: the sequence number of that entry (\texttt{i}), the context of the entry (\texttt{ctx}), and the {\em authenticator} field, namely the digest of the \texttt{ctx||i} as in~\cite{levin2009trinc}. In our implementation, we additionally support the original A2M {\em authenticator} format calculated as the cumulative digest \texttt{c\_digest[i]} for that entry which is calculated as \texttt{c\_digest[i]=hash(ctx||sq||c\_digest[i-1])} where \texttt{c\_digest[0]=0}. The sequence number \texttt{i} is then increased to distinguish any entry that will be appended in the future. %With the cumulative digest, we create a set of chains, and as such, our method does not cause any values to be forgotten.





\myparagraph{Lookup operation} The \texttt{lookup(id, i)} retrieves the log entry at index \texttt{i} of log with identifier \texttt{id}. Compared to A2M, where lookups are compelled to access the trusted hardware, \projecttitle{}-log only performs a local memory access. 
The function does not verify whether the entry is legitimate. Developers need to implement the \texttt{verify\_lookup(id, entry, head, tail)} to verify the attestation. The boundaries of the log (i.e., \texttt{head} and \texttt{tail}) can constantly be retrieved by replaying a specific log, which keeps the state changes, the \textsc{manifest}. We explain how \textsc{manifest} works in the next paragraph.

\myparagraph{Truncate operation} The \texttt{truncate(id, head, z)}, where \texttt{z} is a nonce provided by the client for freshness, ``forgets'' all log entries with sequence numbers lower than \texttt{head}. A non-Byzantine client can never successfully verify a forgotten log entry. To do that, \projecttitle{}-log uses an additional log \textsc{manifest}, which keeps the logs' state changes. First, the operation attests to the {\em tail} of the log by appending a specific entry, which includes the nonce for a correct client to be later able to verify the operation. Then, the algorithm will append the last attested message of the log to the \textsc{manifest} log and return the attested message for the second append. To retrieve the boundaries of a log, clients can always attest to the tail of the \textsc{manifest} and read backward until they find a \textsc{trnc} entry.


\noindent\fbox{\begin{minipage}{\columnwidth}
\myparagraph{System design takeaway} \projecttitle{} minimizes the required TCB in the A2M system while offering faster lookup operations than its original version.
\end{minipage}}
%\atsushi{Can we explain more which point is improved thanks to TNIC?}

%Since the logs reside in the untrusted host memory, their integrity can be compromised by malicious adversaries. However, these adversaries cannot impersonate \projecttitle{} and generate verifiable attestations in any way. As such we do not worry about entries that are not written yet (these will never be verifiable). 
%If an adversary compromises the \textsc{MANIFEST}, \projecttitle{}-log might become responsive. However, this affects availability but not safary and it is beyond the scope of this work similar to other systems [A2M, Trinc, Damysus]. 


\subsection{Byzantine Fault Tolerance (BFT)}\label{sec:use_cases::byz_smr}
%\atsushi{unclear to me what is the advantage of using TNIC to implement the Byzantine SMR. Can we somehow highlight this point?}
%\dimitra{@Atnoni: Can we afford a network partition? 2f+1 w/ f=2, Assume Byz. leader and Byz. follower that drive execution with one correct replica---the others are on purpose exluded by the faulty leader. The client will have a correct reply always because it will wait for f+1 (=3) identical replies. Although if the most up to date correct replica afterwards is partitioned out, then we just block; the remaining correct replicas will have lost one message and block until they get it ... }
%\pramod{ToDo: Pesudocode.}

As a second example of \projecttitle{} applications, we build a Byzantine Fault-Tolerant protocol (BFT) that implements a robust counter based on {\em state machine replication} (SMR). Clients send increment counter requests to the SMR and receive the updated value of the counter. Despite its simplicity, this particular system can represent an ordering service, which is a fundamental building block of various distributed applications ranging from event logging and database systems to serverless and blockchain~\cite{rafthyperledger, Kafka, boki, 10.1145/3286685.3286686, scalog}. Our BFT combats equivocation by leveraging the attestation kernel of \projecttitle{}. As such, via \projecttitle{}, it reduces \textit{(i)} the number of replicas and \textit{(ii)} the message complexity (and latency) required by classical BFT.

\myparagraph{System model} We consider a system of $N=2f+1$ replicas (or {\em nodes}) that communicate with each other over unreliable point-to-point network links. At most $f$ of these replicas can be Byzantine (aka {\em faulty}), i.e., can behave arbitrarily. The rest of the replicas are {\em correct}. Recall that classical BFT protocols require an extra set of $f$ replicas, in total $3f+1$, to handle $f$ Byzantine failures.  One of the replicas is the {\em leader} that drives the protocol, whereas the remaining replicas are (passive) followers. There is only a single active leader at a time.

For liveness, we assume a partial synchrony model~\cite{FLP, 10.1145/226643.226647}. We have only explored deterministic protocol specifications; the correct replicas begin in the same state, and receiving the same inputs in the same order will arrive at the same state, generating the same outputs. Lastly, as in classical BFT protocols, we cannot prevent Byzantine clients who otherwise follow the protocol from overwriting correct clients' data.


\myparagraph{Execution} We implement BFT with \projecttitle{} as a leader-based SMR protocol for a Byzantine model that stores and increases the counter's value. The leader receives clients' requests to increment the counter. The leader, in turn, executes the protocol and applies the changes to its state machine---in our case, the leader computes and stores the next available counter value. Subsequently, the leader broadcasts the request along with some metadata to the passive followers. The metadata includes, among others, the leader's calculated output in response to the client's command, namely, the increased counter value the leader has calculated.
% and the {\em state} of the followers known to the leader.
% In our implementation, the {\em state} is represented as the signed hash of the counter value of each follower. %(known to the leader).

The followers, in turn, execute and apply the incremented counter value to their state machines. However, they first attest to the leader's (and other followers') actions to detect misbehavior. Importantly, followers validate if the state (counter) of the replicas (including the leader and all other replicas) match the expected value.

%The followers, in turn, execute and apply the incremented counter value to their state machines. owever, they first attest to the leader's actions to detect misbehavior. o do so, they audit and validate its sent output through re-execution. recisely, the followers except for their state machine, {\em simulate} the leader's state machine. ach follower replica must add an extra counter representing the state the value counter is expected to have, leading to a $2\times$ extra space complexity.   follower will update the leader's value based on the commands received and then compare its calculated leader's value with the received one. n addition, the followers will validate the state of the replicas (including the leader and all other replicas). hey only have to check if their previous state equals the other replicas' state. 


After a follower applies the increments to its local counter, it replies to the client.
% and the leader with the result of the operation. 
In addition, it forwards the leader's request to every other replica to ensure that all correct replicas will eventually receive and apply the same command. Replicas that have already applied the request ignore it; otherwise, they validate it and apply it. The leader, upon successful validation, will also reply to the client. The client can trust the result if they receive identical replies from a majority quorum, i.e., at least $f+1$ identical messages from different replicas (including the leader). This guarantees that at least one correct replica has responded with the correct result.


\myparagraph{Failure handling} Our strategy to verify the replica's execution jointly with the primitives of non-equivocation and transferable authentication offered by \projecttitle{} shields the protocol against Byzantine behavior. The leader cannot equivocate; even if it attempts to send different requests for the same round to different followers, executing the {\tt local\_send()} will assign different counter values, which healthy followers will detect. As such, a leader in that case will be exposed. 

Likewise, the equivocation mechanism allows correct followers to discard stale message requests sent through replay attacks on the network. If a follower is Byzantine, a healthy leader or replica can detect it. For $f\geq2$, it is impossible for a faulty leader and, at most, $f-1$ remaining Byzantine followers to compromise the protocol. Either these faults will be detected by a healthy replica during the validation phase, or the protocol will be unavailable, i.e., if the leader in purpose only communicates with the Byzantine followers. This directly affects BFT correctness requirements; a client will never get at least $f+1$ matching replies. Even in the extreme case of a network partition or a faulty leader that purposely excludes some healthy replicas from its multicast group, when the network is restored, these replicas will not accept any future messages unless they receive all missed ones. Suppose the leader fails in the middle of the broadcast. In that case, the last step in the follower's protocol ensures that if a correct replica accepts the requests, all correct replicas will eventually apply the same request. Since the reliability aspect and FIFO ordering are implemented in hardware, healthy replicas will ultimately receive all past messages in the proper order. For protocols to progress in the case of a faulty leader, they must pass through a recovery protocol or view-change protocols similar to those described in previous works~\cite{minBFT, Castro:2002}. Recovering is beyond the scope of this work, and as such, we did not implement it.


\noindent\fbox{\begin{minipage}{\columnwidth}
\myparagraph{System design takeaway} \projecttitle{} optimizes the replication factor and the message rounds compared to classical BFT.
\end{minipage}}


\begin{algorithm}
\SetAlgoLined
\small
%\vspace{0.02cm}
\textbf{function} \texttt{leader(req)} \{ \\
\Indp
 {\tt output} $\leftarrow$ \texttt{execute(req)};\\
 {\tt msg} $\leftarrow$ \texttt{req||output};\\
 {\tt attested\_msg} $\leftarrow$ \texttt{local\_send(msg)};\\
 \texttt{rem\_write(}\textsc{followers[:]}{\tt, attested\_msg)};\\

{\bf upon reception of {\tt ack} from \textsc{followers}:}\\
    \Indp
        {\tt [{$\alpha$ || f\_attested\_msg || f\_output || f\_id}]} \\\hspace{22pt} $\leftarrow$ \texttt{upon\_delivery(ack)};\\
        {\bf assert(}\texttt{validate\_follower(f\_attested\_msg,\\\hspace{22pt} f\_output)}{\bf)};\\
        \texttt{incr\_req\_acks\_if\_not\_incr\_before(f\_id)};\\
    \Indm

 \texttt{auth\_send(}\textsc{client}{\tt,msg)};\\
\Indm
\} \\

\vspace{0.15cm}

% \textbf{function} \texttt{follower(}\textsc{sender}{\tt, attested\_msg)} \{ \\
\textbf{function} \texttt{follower()} \{ \\
\Indp
{\bf upon reception of {\tt attested\_msg}:}\\
    \Indp
        {\tt [{$\alpha$ || req || output}]} $\leftarrow$ \\\hspace{22pt}\texttt{upon\_delivery(attested\_msg)};\\

    {\bf assert(}\texttt{validate\_sender(req, output)}{\bf)};\\
    {\bf if }{\tt (in\_order\_not\_applied(req))}\\
    \Indp
    {\tt current\_output} $\leftarrow$ \texttt{execute(req)};\\
    {\tt f\_attested\_msg} $\leftarrow$ \texttt{local\_send(req||current\_output)};\\
    ack $\leftarrow$ {\tt f\_attested\_msg}\\
    % \texttt{auth\_send(}\textsc{client} $\cup$ \textsc{leader} $\cup$ \textsc{followers[:]},\\{\tt\hspace{40pt} attested\_msg)};\\    
    \texttt{auth\_send(}\textsc{leader}, {\tt ack)};\\    
    % \texttt{auth\_send(}\textsc{client} $\cup$ \textsc{followers[:]},\\{\tt\hspace{40pt} attested\_msg)};\\    
    \texttt{auth\_send(}\textsc{client} $\cup$ \textsc{followers[:]},\\{\tt\hspace{40pt} f\_attested\_msg)};\\    
    % \texttt{auth\_send(}\textsc{leader}{\tt, req||current\_output)};\\    
    % \texttt{auth\_send(}\textsc{client}{\tt, req||current\_output)};\\
    % {\bf if {\tt not\_seen(req)} {\sc and  sender = leader}:}\\
    % \Indp
        % {\bf for} (\textsc{followers[:]}) 
            % \texttt{auth\_send(}\textsc{followers[:]}{\tt, msg)};\\
            % \texttt{rem\_write(}\textsc{LEADERfollowers[:]}{\tt, attested\_msg)};\\
    \Indm
    \Indm
\Indm
\} \\
\vspace{-1pt}
\caption{BFT using \projecttitle{}.}
\label{algo:tnic_bft}
\end{algorithm}

%
%\dimitra{>Github code issues:
%\begin{itemize}
%    \item Line 239: has a logical bug regarding the message batching
%    \item Lines 205--210: unnecessary hash re-calcucations--- might improve performance if fixed
%    \item Continuation function needs improvement for correctness/completeness (leader should store the output for each on-going command).
%    \item For correctness, leader should have only one outstanding operation at a time.
%\end{itemize}}



%\lstinputlisting[language=C++]{codelets/pb.cc}




\subsection{Chain Replication (CR)}\label{sec:use_cases::byz_chain_rep}

\begin{algorithm}
\SetAlgoLined
\small
%\vspace{0.02cm}
\textbf{function} \texttt{head\_operation(req)} \{ \\
\Indp
 {\tt output} $\leftarrow$ \texttt{execute(req)};\\
 {\tt msg} $\leftarrow$ \texttt{req||output};\\
 \texttt{auth\_send(}\textsc{middle}{\tt,msg)};\\ \texttt{auth\_send(}\textsc{client}{\tt,msg)};\\
\Indm
\} \\

\vspace{0.15cm}

\textbf{function} \texttt{middle\_tail\_operation(msg)} \{ \\
\Indp
    {\bf assert(}\texttt{validate\_chain(msg)}{\bf)};\\
    {\tt output} $\leftarrow$ \texttt{execute(req)};\\
    {\tt chained\_msg} $\leftarrow$ \texttt{msg||output};\\
    {\bf if} (!\textsc{tail})\\
    \Indp
    \texttt{auth\_send(}\textsc{middle}{\tt,chained\_msg)};\\
    \Indm
    \texttt{auth\_send(}\textsc{client}{\tt,req||output)};\\
\Indm
\} \\
\vspace{0.15cm}
\textbf{function} \texttt{validate(msg)} \{ \\
\Indp
    \texttt{len} $\leftarrow$ \texttt{sz};\\
    \texttt{[req, out, cmt]} $\leftarrow$ \texttt{unmarshall(msg[0:len])};\\
    {\bf assert(}\texttt{memcmp(req, out)}{\bf)};\\
    {\bf assert(}\texttt{(cmt == expected\_cmt)}{\bf)};\\
    {\bf for} {\tt(i = 1; i < }{\sc node\_id}; {\tt i++)} \{\\
    \Indp
    \texttt{[out, cmt]} $\leftarrow$ \texttt{unmarshall(msg[len:len+\textsc{sz}])};\\
    {\bf assert(}\texttt{memcmp(req, out)}{\bf)};\\
    {\bf assert(}\texttt{(cmt == expected\_cmt)}{\bf)};\\
    \texttt{len} $\leftarrow$ \texttt{len} + \texttt{sz};\\
    \Indm
    {\bf return} {\tt True};\\
\Indm
\} \\
\vspace{-1pt}
\caption{Chain Replication using \projecttitle{}.}
\label{algo:tnic_chain_replication}
\end{algorithm}



We implement a Byzantine Chain Replication using \projecttitle{} that represents the replication layer of a Key-Value store. Chain Replication is a foundational protocol for building state machine replication and initially operates under the CFT model using $f+1$ nodes to tolerate up to $f$ failures. We show {\em how} to use \projecttitle{} to shield the protocol without changes to the core of the algorithm (states, rounds, etc.) while keeping the same replication factor.




\myparagraph{System model} We make the same assumptions for the system as in the previous BFT system. For error detection and reconfiguration, we assume a centralized (trusted) configuration service as in~\cite{10.1007/978-3-642-35476-2_24} that generates new configurations upon receiving reconfiguration requests from replicas. Recall that the classical Chain Replication under the CFT model relies on reliable failure detectors~\cite{chain-replication}. For liveness, we also assume that the configuration service will eventually create a configuration of correct replicas that do not intentionally issue reconfiguration requests to perform Denial-of-Service attacks. 

Clients send requests to {\tt put} or {\tt get} a value and receive the result. The replicas (e.g., head, middle, and {\em tail} nodes) are chained, and the requests flow from the head node to the tail through the intermediate middle replicas. 

Malicious primaries, i.e., the head that does not forward the message intentionally, are detected on the client's side and trigger reconfiguration~\cite{Castro:2002, minBFT}.


\myparagraph{Execution} To execute a request \texttt{req}, e.g., {\tt put}/{\tt get}, a client first obtains the current configuration from the configuration service and sends the {\tt req} to the head of the chain. The head orders and executes the request, and then it creates a {\em proof of execution message}, which is sent along the chain. The proof of execution includes the {\tt req} and the leader's action ({\tt out}) in response to that request. In our case, the leader sends the {\tt req} along with the assigned commit index. The message is then sent (signed) to the middle node that follows in the chain.

The middle node checks the message's validity by verifying that the head's output is correct, executes the {\tt req}, and forwards the request to the following replica. Similarly, every other node executes the original request, verifies the output of all previous nodes, and sends the original request and a vector of all previous outputs. A replica must construct a {\em proof of execution message} that achieves one goal. t allows the following replicas in the chain to verify all previous replicas. s such the messages is of the form < < <{\tt req}, {\tt out$_{leader}$}>$_{\sigma_0}$, {\tt out$_{middle1}$}>$_{\sigma_1}$, .., {\tt out$_{tail}$}>$_{\sigma_N}$. The tail is the last node in the chain that will execute and verify the execution of the request. 


In contrast to the CFT version of the Chain Replication protocol, local operations in the tail, {\tt get} or {\tt ack} in a {\tt put} request cannot be trusted. As such, the replicas in the chain need to reply to the clients with their output after they have forwarded their proof of execution message. Clients can wait for at least $f$ replicas replies and the tail reply to collide. Clients can execute the {\tt get} requests similarly to {\tt write} requests, traversing the entire chain, or clients can consult the majority and broadcast the request to $f+1$ replicas, including the tail. 


\myparagraph{Failure handling} By the protocol definition, all nodes will see and execute all messages in the same order imposed by the head node. As such, all correct replicas will always be in the same state. In addition, network partitions that may split the chain into two (or more) individual chains that operate independently cannot affect safety: the clients must verify at least $f+1$ identical replies. Suppose a correct replica or a client detects a violation (by examining the proof of execution message or having to hear for too long from a node). In that case, they can expose the faulty node and request a reconfiguration.

\noindent\fbox{\begin{minipage}{\columnwidth}
\myparagraph{System design takeaway} \projecttitle{} {\em seamlessly} shields the Chain Replication system for Byzantine settings with the same replication factor as the original CFT system.
\end{minipage}}

%\dimitra{>Github code issues:
%\begin{itemize}
%    \item check\_outputs function (L:67): Did you miss a validation step regarding transferable authentication?
%\end{itemize}}



\subsection{Accountability (PeerReview)}
\label{sec:use_cases::accountability}





We implement an accountability protocol based on the PeerReview system~\cite{bftdetection, peer-review}. Compared to the previous three BFT systems that prohibit an improper action from taking effect, accountability protocols~\cite{268272, bftdetection, peer-review} slightly weaken the system (fault) model in favor of performance and scalability. Specifically, our protocol {\em allows} Byzantine faults to happen (e.g., correct nodes might be convinced by a malicious replica to permanently delete data). Still, it guarantees that malicious actions can always be detected. Accountability protocols can be applied to different systems as generic guards that trade security for performance~\cite{peer-review}, e.g., NFS, BitTorrent, etc. 

The original version of the system did not use trusted components. t incurs a high message complexity, i.e., {\em all-to-all} communication to combat equivocation. We use \projecttitle{} to improve that message complexity.

\myparagraph{System model} We only detect faults that directly or indirectly affect a message, implying that {\em (i)} correct nodes 
can observe all messages sent and received by that node and {\em (ii)}  Byzantine faults that are not observable through the network cannot be detected. For example, a faulty storage node might report that it is out of disk space, which cannot be verified without knowing the actual state of its disks.

We further assume that each protocol participant acts according to a deterministic specification protocol. As such, detection can be accomplished even with a single correct machine, requiring only $f+1$ machines.  This does not contradict the impossibility results for agreement~\cite{FLP} because detection systems do not guarantee safety.


\myparagraph{Execution} The participants communicate through network messages generated by \projecttitle{}.  In addition, each participant maintains a {\em tamper-evident} log that stores all messages sent and received by that node as a chain. A log entry is associated with an entry index, the entry data, and an authenticator, calculated as the signed hash of the tail of the log and the current entry data. 

We frame our protocol in the context of an overlay multicast protocol~\cite{10.1145/945445.945474} widely used in streaming systems. The nodes are organized as a tree where the streaming content (e.g., audio, video) flows from a source, i.e., {\em root} node, to clients ({\em children} nodes). To support many clients, each can be a source to other clients, which will be connected as children nodes. 

In our implementation, we consider nodes in a tree topology. The tree's height equals one, comprising one source node and two client (children) nodes connected to the source. Algorithm~\ref{algo:tnic_accountability_protocol} ($\S$~\ref{sec:use_cases-appendix}) shows the operations of our implemented accountability protocol.  hen the source sends a context (executes the \texttt{root()} function), it implicitly includes a signed statement that this message has a particular sequence number (generated by \projecttitle{}). he clients execute the {\tt child()} function that validates the received message, logs the received message, executes the result, and responds to the source. 


\begin{algorithm}[t]
\SetAlgoLined
\small
%\vspace{0.02cm}
\textbf{function} \texttt{root(ctx)} \{ \\
\Indp
 \texttt{auth\_send(}\textsc{child}{\tt,ctx)};\\
 {\bf upon reception of \texttt{response}:};\\
 \Indp
    {\bf assert(}\texttt{validate\_reception(response)}{\bf)};\\
    \texttt{log(response)};\\
\Indm
\Indm
\} \\

\vspace{0.15cm}

\textbf{function} \texttt{child($\alpha$||cmd||seq)} \{ \\
\Indp
    {\bf assert(}\texttt{validate\_reception($\alpha$||cmd||seq)}{\bf)};\\
    \texttt{log($\alpha$||cmd||seq)};\\
    {\tt result} $\leftarrow$ \texttt{execute(cmt)};\\
    {\tt response} $\leftarrow$ \texttt{log(result||cmd)};\\
    \texttt{auth\_send(}\textsc{root}{\tt, response)};\\
\Indm
\} \\
\vspace{0.15cm}
\textbf{function} \texttt{log\_audit()} \{ \\
\Indp
    {\bf{while}} \texttt{last\_id < log\_tail} \{\\
    \Indp

        \texttt{entry} $\leftarrow$ \texttt{validate\_log\_entry\_at(last\_id)};\\
        \texttt{last\_id++};\\
        {\bf assert(}\texttt{replay(entry)}{\bf{)}};\\
    \Indm
    \}\\
\Indm
\} \\
%\vspace{-1pt}
\caption{PeerReview using \projecttitle{}.}
\label{algo:tnic_accountability_protocol}
\end{algorithm}


Each node is assigned to a set of {\em witness} processes to detect faults. Similarly to the original system, we assume that the set of nodes and its witnesses set {\em always} contain a correct process. The witnesses audit and monitor the node's log. To detect destructive behaviors (or expose non-responsive nodes), the witnesses read the node's log and replay it to run the participant's state machine. As such, they ensure the participant's state is consistent with proper operation. 

Specifically, each witness for a participant node keeps track of n, a log sequence number, and s, the state that the participant should have been in after sending or receiving the message in log entry n. t initializes n to 0 and s to the initial state of the participant.

Whenever a witness wants to audit a node, it sends its n and a nonce (for freshness).
The participant returns an attestation of all entries between n and its current log entry using the nonce. The witness then runs the reference implementation, starting at state $s$, and progressing through all the log entries. f the reference implementation sends the same messages in the log, then the witness updates n, %\antonis{Is this the point of truncating the log?}
which is the state of the reference implementation at that point. If not, then the witness has proof it can present of the participant's failure to act correctly.




The original PeerReview system requires a receiver node to forward messages to the original sender's witnesses so they can ensure this message is {\em legitimate}, i.e., it appears in the sender's log. No other conflicting message is sent to another peer (equivocation). As such, a peer must communicate (in every round) with the witness set of any other peer, leading to a quadratic message complexity. \projecttitle{} eliminates the overhead; a participant that sends or receives a message needs to attest and append the message and its attestation in each log. A participant can process received messages only if they are accompanied by attestations generated by the sender's \projecttitle{} hardware. 



\noindent\fbox{\begin{minipage}{\columnwidth}
\myparagraph{System design takeaway} \projecttitle{} can be used to optimize the message complexity in accountable systems.
\end{minipage}}






