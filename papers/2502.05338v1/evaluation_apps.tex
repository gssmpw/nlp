\subsection{Distributed Systems Evaluation}
\label{subsec:use_cases_eval}

\if 0

\fi
%We implement the four systems of $\S~\ref{sec:use_cases}$ with \projecttitle{} in a three-node setup ($N=3$) except for the single-node A2M system.
% the systems' properties where $N$ refers to the number of machines used for the protocol, and $f$ is the number of failures the system can tolerate.

We next evaluate four distributed systems described in $\S~\ref{sec:use_cases}$.% based on \projecttitle{}.


\myparagraph{Methodology and experiments} 
We execute all four of our codebases on Intel Cluster in three servers (as the minimum required setup capable of withstanding a single failure, $N=2f+1$, where $f=1$). \rev{E4}{We only use a single port of the U280 for network communication because of a limitation introduced in our system by the Coyote codebase~\cite{coyote}, on top of which we base \projecttitle{} implementation.} Due to our limited resources, we cannot install Alveo U280 cards on all these servers.  Instead, we build our codebase using the DRCT-IO stack (detailed in $\S$~\ref{subsec:net_lib}) and inject busy waits to emulate the delays incurred by \projecttitle{} for generating and verifying attested messages.
% in the \projecttitle{} system. %Our code uses busy waiting to accurately emulate latency rather than sleep functions. 
% We execute all four of our codebases on Intel Cluster, utilizing all its three servers (as the minimum required setup capable of withstanding a single failure, $N=2f+1$, where $f=1$). Furthermore, due to our limited resources, with only two U280 cards available and the physical separation of Intel Cluster from AMD-FPGA Cluster, we cannot access U280 cards for these specific experiments. Instead, we compiled our code using the DRCT-IO network stack as previously detailed in $\S$~\ref{subsec:net_lib} and do busy waiting to accurately replicate the \projecttitle{} delays within the CPU for generating and verifying attested messages.% in the \projecttitle{} system. %Our code uses busy waiting to accurately emulate latency rather than sleep functions. 

We evaluate each codebase using five systems that generate and verify the attestations: {\em (i)} SSL-lib (no tamper-proof), {\em (ii)} SSL-server (no tamper-proof), {\em (iii)} SGX, {\em (iv)} AMD-sev, and {\em (v)} \projecttitle{}. To perform a fair comparison, we integrate into our codebases a library that accurately emulates all latencies (measured in $\S$~\ref{subsec:hw_eval}) within the CPU. For the AMD latency, we use 30us, representing the lower bound of the latencies measured in $\S$~\ref{subsec:hw_eval}. We do not emulate the SSL-lib latency. 
% We evaluate each protocol using five systems that generate and verify the attestations: {\em (i)} SSL-lib, an SSL library that is integrated into the codebase (no tamper-proof), {\em (ii)} SSL-server (no tamper-proof), {\em (iii)} SGX, {\em (iv)} AMD-sev, and {\em (v)} \projecttitle{}. To perform a fair comparison, we integrate into our protocols' codebases an library that accurately emulate all latencies (measured in $\S$~\ref{subsec:hw_eval}) within the CPU. We do not emulate the SSL-lib latency and for the AMD latency we use 30us which represents the lower bound of the latencies measured in $\S$~\ref{subsec:hw_eval}.

 %which are all an integrated library to the codebase and further adds a configurable delay to represent the operation's latency in each system. The added delay for each system is the respective latency we measured in

Given that DRCT-IO, which is used for the emulation, is at least 3$\times$ slower than the hardware RDMA network stack (RDMA-hw), the latencies outlined in this section are anticipated to reflect the upper limit for all four systems with \projecttitle{}.

\rev{(c)}{We additionally evaluate two TEEs-hosted CFT replication protocols (TEEs-Raft and TEEs-CR) where the entire protocol codebase (Raft~\cite{raft} and Chain replication~\cite{chain-replication} respectively) resides within the TEE. We compare the TEEs-hosted systems with \projecttitle{} and discuss the trade-offs between their threat model, TCB, and performance.}
% Given that DRCT-IO for small messages operates roughly three times the speed of the hardware-based RDMA network stack implementation, the latencies outlined in this section are anticipated to reflect the upper limit for all four systems with \projecttitle{}.


\begin{table}[t!]
\begin{center}
\small
\minipage{0.22\textwidth}
  \centering
\begin{tabular}{lrr}
\hline
& \multicolumn{2}{c}{Throughput (Op/s)} \\
System          & append    & lookup  \\
\hline
SSL-lib         & 790K      & 256M      \\
SGX-lib             & 380K      & 3.8M       \\
AMD-sev         & 30K       & 263M      \\
\projecttitle{} & 158K      & 257M      \\
\hline
\end{tabular}
\endminipage
\hfill
\minipage{0.18\textwidth}
\centering
\begin{tabular}{rrr}
\hline
\multicolumn{2}{c}{Latency (us)} \\
 append     & lookup  \\
\hline
 1.26       & 0.0039      \\
 2.6        & 0.26       \\
 32.37      & 0.0038      \\
 6.34       & 0.0039      \\
 \hline
\end{tabular}
\endminipage
\end{center}
\caption{Throughput and latency of A2M.}%\dimitra{@experiments: maybe you try run A2M into an SGX server to also show the communication costs}}
\label{fig:a2m_eval}
\vspace{-4pt}
\end{table}

\myparagraph{A2M} We first evaluate our \projecttitle{}-A2M system. 
% We evaluate our prototype of the A2M system with \projecttitle{}. 
% For SGX, we port the entire log within the TEE, labeled as SGX-lib. All other versions place the attested log in the untrusted host memory, using the trusted systems to generate attestations as in~\cite{levin2009trinc}.
\rev{B6}{
We evaluate two TEE baselines: SGX-lib, which places the entire log within the TEE, and AMD-sev, which places the attested log outside the TEE as in the implementation of TrInc~\cite{levin2009trinc} and has been shown to be effective. 
% adapting the implementation of TrInc~\cite{levin2009trinc}
% We evaluate another TEE baseline using SGX, labeled SGX-lib, which places the entire log within the TEE. We also evaluate AMD-sev adapting the implementation of TrInc~\cite{levin2009trinc}, which places the attested log outside the TEE and has been shown to be effective. 
}
%, using the trusted systems to generate attestations as in~\cite{levin2009trinc}. 
% In this experiment, we first construct a log of size 9GB with 100M entries and then sequentially we lookup for them individually. Each log entry is comprised of 64B of appended data (context) and an extra 36B for the metadata.
In this experiment, we construct a 9.3GiB log with 100 million entries and then lookup them sequentially/individually. %Each log entry is 100B, which contains 64B appended context and 36B metadata.
% \rev{B6}{We evaluated A2M (which ports the entire log within the SGX) with the implementation of TrInc~\cite{levin2009trinc} (which places the log outside the SGX and has been shown to be effective) with \projecttitle{} (AMD-sev).}
% \atsushi{@Dimitra: let me make sure, does AMD-sev represent the implementation of TrInc (the log is outside the enclave)?}
% SSL-lib (no tamper-proof), SGX-lib (the entire log in the enclave)


%\myparagraph{A2M} We evaluate our prototype of the A2M system with \projecttitle{}. 
% an effective building block for improving the scalability and performance in various systems~\cite{A2M, sundr, Castro:2002, AbdElMalek2005FaultscalableBF}. 
%For the SGX, similarly to the prior work~\cite{A2M, sundr, Castro:2002, AbdElMalek2005FaultscalableBF}, we port the entire log within the TEE, building a large TCB. We refer to this system as SGX-lib. All other versions place the attested log in the untrusted host memory, using the trusted systems to generate attestations as in~\cite{levin2009trinc}. %\atsushi{Does this paragraph repeat explanations in s7.1? If so, like the following applications, can we simply say "We evaluate the A2M system (TNIC-log described in $\S$~\ref{sec:use_cases::a2m})..."?}
% In this experiment, we first construct a log of size 9GB with 100M entries and then sequentially we lookup for them individually. Each log entry is comprised of 64B of appended data (context) and an extra 36B for the metadata.
%In this experiment, we construct a 9.3GiB log with 100 million entries, and then lookup them sequentially/individually. Each log entry is 100B, which contains 64B appended context and 36B metadata.

%specifically the computed authenticate (32B) and the sequence number of the entry (4B). In short, we append 100M of 100B entries, leading to a log size of approximately 9GB. \atsushi{Can we somehow shorten the description?}


\noindent{\underline{Results.}} 
Table~\ref{fig:a2m_eval} shows the throughput and mean latency of the append/lookup operations. The native execution (SSL-lib) achieves the highest throughput as it incurs no communication costs. 
% Specifically, its latency is 1.26us, which is dominated by the HMAC computation.
Compared to SSL-lib, SGX-lib experiences only a 2$\times$ slowdown because we avoid the costly communication w.r.t. an SGX-based server implementation. On the other hand, AMD-sev, which runs the SSL server, incurs a 15$\times$ slowdown. Lastly, \projecttitle{} incurs 5$\times$ and 2.4$\times$ slowdown compared to SSL-lib and SGX-lib, respectively, due to the HMAC calculation.

% Table~\ref{fig:a2m_eval} shows the throughput (operations/s) and the mean latency of our A2M system using various systems. The append operation throughput in the SSL-lib case illustrates the throughput upper bound as it incurs no communication costs. Specifically, the A2M with SSL-lib running natively in the CPU reports a latency of 1.26us, which is dominated by the HMAC computation latency.
% Placing the log within the SGX (SSL-lib), we avoid the costly communication w.r.t. to an SGX-based server implementation, and as such, the system only experiences a 2$\times$ slowdown compared to the native case. The communications costs are reflected in the AMD-sev case; that runs the SSL-server. Compared to when porting to SGX, AMD-sev incurs 15$\times$. Lastly for \projecttitle{}, we observe approximately 5$\times$ and 2.4$\times$ slowdown compared to the SSL-lib and SGX-lib execution, respectively which is due to the HMAC calculation.

Regarding the lookup operation, SSL-lib, AMD-sev, and \projecttitle{} report similar throughput and latency because they lookup untrusted host memory for the requested entry. However, SGX-lib reports a 66$\times$ slowdown due to its trusted memory size constraints and expensive paging mechanism~\cite{treaty} \rev{C3}{because we have to support a log of 9GB within the SGX enclave that only provides 94MB of memory. In contrast, AMD-sev is faster as it only accesses the untrusted host memory. Similar findings have also been demonstrated in~\cite{levin2009trinc}}. As a result, while \projecttitle{} increases append latencies, it greatly optimizes lookup latencies due to its minimal TCB.
% As a result, while \projecttitle{} offers slower append operations than porting the entire log into the TEE, it greatly optimizes lookup latencies due to its minimal TCB.


\begin{figure*}
\centering
\minipage{0.33\textwidth}
\centering
    %\includegraphics[width=\linewidth]{atc-submission-plots/bft_pb.pdf} 
    \includegraphics[width=\linewidth]{eval-plots/cf-protos-in-amd-eval/bft_pb_exte.pdf} 
    \vspace{-4mm}
    \caption{Throughput (and latency numbers) of BFT.} \label{fig:byz_smr_throuthput}
\endminipage%
\minipage{0.33\textwidth}
  \centering
  %\includegraphics[width=\linewidth]{atc-submission-plots/bftcr_lat_throughput.pdf} 
  \includegraphics[width=\linewidth]{eval-plots/cf-protos-in-amd-eval/bftcr_lat_throughput_exte.pdf} 
    \vspace{-6mm}
    \caption{Throughput (and latency numbers) of Chain Replication.} \label{fig:byz_chain_replication}
\endminipage %
\minipage{0.33\textwidth}
     \includegraphics[width=\linewidth]{atc-submission-plots/bftpr_lat_throughput.pdf} 
    \vspace{-6mm}
    \caption{Throughput (and latency numbers) of PeerReview.} \label{fig:accountability_protocol}
\endminipage
\vspace{-2mm}
\end{figure*}


\myparagraph{BFT} We evaluate the performance of our BFT protocol with various network batching factors. We implement network batching as part of the application's message format. % In this experiment, we allocate one {\em message} structure for each client's request, which contains the initial command, the results of the command's execution in a node, the incremented counter values, etc. % and the signed hashes of the replicas' states known to each node.

% In this experiment, we allocate one {\em message} structure for each client's request. The {\em message} contains command and output fields (16B each), which store the initial command and results of the command's execution in a node, respectively. In addition, the message contains metadata, which includes a sequencer and signed hashes of the replicas' states known to each node. The sequencer is used for serialization, and it is equivalent to the counter value assigned to each message from \projecttitle{}. The signed hashes representing the replicas' states are obtained from the last messages received from each replica.

\noindent{\underline{Results.}} Figure~\ref{fig:byz_smr_throuthput} shows the throughput and latency of the protocol, which highlights that \projecttitle{} significantly outperforms TEE-based versions (SGX, AMD-sev), improving the throughput and latency 4---6$\times$. On the other hand, \projecttitle{} incurs 2.4$\times$ throughput overhead and up to 7$\times$ higher latency compared to SSL-lib. 
We recall that SSL-lib is not tamper-proof (Table~\ref{tab:hw_options}) and eliminates the communication overheads incurred by other tamper-proof solutions (SGX, AMD-sev).
% as the security-related processing runs in the untrusted host CPU. % and can be accessed and compromised by malicious adversaries.
% Recall that SSL-lib is the upper bound in performance because it calculates the attestations natively and eliminates the communication overheads incurred by a realistic solution (SGX, AMD-sev). 

% Figure~\ref{fig:byz_smr_throuthput} show the throughput and the latency of the protocol. Our \projecttitle{} incurs about 2.4$\times$ throughput overhead and up to 7$\times$ higher latency compared to the SSL-lib version. Similarly, SGX and AMD-sev versions incur 13.5$\times$ and 9.6$\times$ throughput overheads compared to the SSL-lib. As such, our \projecttitle{} improves throughput and latency 4---6$\times$ compared to TEE-based versions. Recall that the SSL-lib version is the upper bound in performance we can possibly achieve as it calculates the attestations natively and eliminates the communication overheads that a realistic solution (SGX, AMD-sev) occurs. However, in contrast to our \projecttitle{}, SSL-lib is not tamper-proof (Table~\ref{tab:hw_options}) as the security-related processing runs in the untrusted host CPU and can be accessed and compromised by malicious adversaries.

% However, the \projecttitle{} throughput is better than SGX and AMD-sev, which incur 13.5$\times$ and 9.6$\times$ overheads compared to SSL-lib, respectively. 
% As such, our \projecttitle{} improves throughput and latency 4---6$\times$ compared to TEE-based versions. 

%\atsushi{Can we highlight that TNIC is temper-proof but the Nat-lib (best perf) is not?}.
%\antonis{Are the differences/benefits of TEE vs. TFPGA tamper-proofness discussed earlier than the evaluation? Seems its something important.}


We also observe that batching improves the throughput and latency proportionally to the number of batched messages. For all except SSL-lib, the batching factors equal to 8 and 16 achieve 7$\times$ and 15$\times$ higher throughput than without batching, respectively. For SSL-lib, they are moderately effective: approximately 4---6$\times$ faster. It is primarily because the native execution of the attestation function is fast enough to saturate the network bandwidth. 
As such, conventional techniques can drastically eliminate the overheads for BFT and improve \projecttitle{}'s adoption into practical systems.
% In addition, we observe that batching increases throughput (and decreases latency) proportionally to the number of batched messages. For all but the SSL-lib version, we report a 7$\times$ and 15$\times$ throughput boost for batching factor to be equal to 8 and 16 respectively compared with the experiment run with batching factor equal to 1. This is because our batching technique improves the network utilization and reduces the overall attestation calculations, e.g., one attestation per 8 and 16 messages. The technique is moderately effective when using the SSL-lib, approximately 4---6$\times$ faster than without batching. This is primarily because, in that case, the latency is dominated by the network stack latency as the attestation function running natively generates attestations fast. As such, we show that conventional techniques can drastically eliminate the overheads for BFT and improve \projecttitle{}'s adoption into practical systems.




%\begin{figure}[t!]
%    \centering
%    \includegraphics[width=\linewidth]{atc-submission-plots/bft_pb.pdf} 
%    \caption{Throughput (and latency numbers) of BFT using various trusted components.} \label{fig:byz_smr_throuthput}
%\end{figure}

%\begin{figure}[t!]
%    \centering
%    \includegraphics[width=0.7\linewidth]{atc-submission-plots/bft_pb_lat.pdf} 
%    \caption{Average latency of the BFT SMR using various trusted components.} \label{fig:byz_smr_lat}
%\end{figure}


\myparagraph{CR} 
In this experiment, we evaluate the performance of our CR. 
% We evaluate the performance of our Chain Replication. In this experiment, 
We allocate one message structure per client request comprising 60B context, 4B operation type, and a 32B signature.

% (that includes metadata, e.g., source/destination nodes, message ID), 
%We implemented the replication protocol without any underlying Key-Value store data structure.
%of the message allocates 60B comprised of an 8B key and a 32B value as well as 16B for metadata (e.g., source and destination nodes, message idx, etc.) and
\noindent{\underline{Results.}} Figure~\ref{fig:byz_chain_replication} shows the throughput and latency of our Chain Replication. We highlight that our \projecttitle{} is 5$\times$ and 3.4$\times$ faster than SGX and AMD-sev, respectively. While \projecttitle{} incurs 4.6$\times$ overheads compared to SSL-lib, it is 30\% faster than SSL-server, which is not tamper-proof. The performance benefit stems primarily from hardware acceleration by the \projecttitle{}'s attestation kernel on the transmission/reception data path.

%\begin{figure}
%    \centering
%  \includegraphics[width=\linewidth]{atc-submission-plots/bftcr_lat_throughput.pdf} 
%    \caption{Throughput (and latency numbers) of Chain Replication using various trusted components.} \label{fig:byz_chain_replication}
%\end{figure}

%\begin{figure}
%  \includegraphics[width=\linewidth]{atc-submission-plots/bftpr_lat_throughput.pdf} 
%    \caption{Throughput (and latency numbers) of PeerReview using various trusted components.} \label{fig:accountability_protocol}
%\end{figure}

\if 0
\begin{figure*}[t!]
\begin{center}
\minipage{0.5\textwidth}
  \centering
  \includegraphics[width=0.8\linewidth]{atc-submission-plots/bftcr_lat_throughput.pdf} 
    \caption{Throughput-latency evaluation of a Byzantine Chain Replication using various trusted components.} \label{fig:byz_chain_replication}
\endminipage
\minipage{0.5\textwidth}
  \centering
  \includegraphics[width=0.8\linewidth]{atc-submission-plots/bftpr_lat_throughput.pdf} 
    \caption{Throughput-latency evaluation of the accountability protocol using various trusted components.} \label{fig:accountability_protocol}
\endminipage
\end{center}
\end{figure*}
\fi


\if 0
\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{atc-submission-plots/bftcr_lat_throughput.pdf} 
    \caption{Throughput-latency evaluation of a BFT version of CR using various trusted components.} \label{fig:lat_throughput_kernel}
\end{figure}
\fi






\myparagraph{PeerReview} 
We evaluate our PeerReview system's performance by both activating and deactivating the audit protocol. The system uses one witness for the source node that {\em periodically} audits its log. 
% The witness process is co-located in the same node, and as such, reading the log implies reading a shared memory. We decide in favor of that implementation to carefully isolate the overheads for inspection and replay of the log. 
In our experiments, the witness audits the log after every send operation in the source node until both clients acknowledge the receipt of all source messages. % Each message allocates about 200B, comprising a context, its hash, the cumulative digest of the hashes, etc. 
% We evaluate the performance of our PeerReview system. % using various trusted components. 
% We evaluate the protocol by both activating and deactivating the audit protocol. In our implementation, we use one witness for the source node that {\em periodically} audits its log. The witness process is co-located in the same node, and as such, reading the log implies reading a shared memory. We decide in favor of that implementation to carefully isolate the overheads for inspection and replay of the log. In our experiments, the witness is configured to audit the log after every send operation in the source node until both clients acknowledge the receipt of all source messages.

% The log entries consist of the context, a sequencer, a signed hash of the context (authenticator), and the cumulative digest, which is the signed hash of the authenticator and the previous entry's digest in the log. In total, the messages allocate about 200B.
% The log entries consist of the context, a sequencer, a signed hash of the context (authenticator), and the cumulative digest, which is the signed hash of the authenticator and the previous entry's digest in the log. In total, the messages allocate about 200B.
% and we do not use batching. 
%\atsushi{I feel that the description of these data formats (as well as the other applications) is too detailed, so it'll be fine to omit for cutting the text.}

%The log entries consist of the context, a sequencer, a signed hash of the context, and the sequencer (authenticator) \antonis{is this different than the aforementioned "sequencer"?} as well as the cumulative digest, which is the signed hash of the authenticator and the previous entry's digest in the log. The context is comprised of the data (16B), the output of the protocol specification for that input data (16B), some metadata, and the response sent from the clients that have a similar format. In total, the messages allocate about 200B, and we do not use batching. \atsushi{I feel that the description of these data formats (as well as the other applications) is too detailed, so it'll be fine to omit for cutting the text.}

\noindent{\underline{Results.}} Figure~\ref{fig:accountability_protocol} shows the throughput and latency of our PeerReview system with and without enabling the audit protocol.
Without the audit protocol, the TEE-based systems (SGX, AMD-sev) result in up to 30$\times$ slower throughput than SSL-lib, whereas our \projecttitle{} mitigates the overheads: 3---5$\times$ better throughput compared to AMD-sev and SGX.


Similarly, \projecttitle{} outperforms AMD-sev and SGX by 3.7---5$\times$ with the audit protocol. Importantly, when using \projecttitle{}, the audit protocol itself consumes about 25\% (17us) of the overall latency, leading to 1.33$\times$ performance slowdown. % compared to when being disabled. 
% The audit protocol itself consumes approximately 25\% (17us) of the overall execution latency. 
However, even with the audit protocol, \projecttitle{} offers 3.7---5.42$\times$ lower latency compared to its TEE-based competitors.
% With the audit protocol, we observe similar system behavior; \projecttitle{} outperforms both AMD-sev and SGX by 3.7---5$\times$. Importantly, when using \projecttitle{}, the audit protocol leads to a performance slowdown of 1.33$\times$ compared to when being disabled. The audit protocol itself consumes approximately 25\% (17us) of the overall execution latency. Even with the audit protocol, \projecttitle{} offers 3.7---5.42$\times$ lower latency compared to its TEEs-based competitors.

%\atsushi{a bit difficult to read because 'log' is used as both noun and verb. Can we somehow use another word?} When the auditing protocol is de-activated, the source node sends the (streaming) data to the client nodes. The clients execute and append the message to their log, and then reply to the source. The source appends its own message to the log along with the client's reply. This is required to make sure that any participant cannot lie to a witness about the data it sends to the clients. \atsushi{The sentences until here explain the application workflow, but we should show the results first and then start the discussion with an extra explanation.} We see that using a TEE to generate attested messages increases the throughput slowdown up to 30$\times$ compared to the native embedded library (Nat-lib). Using our \projecttitle{} we improve the overheads---specifically, we incur 5.8$\times$ slowdown, that is 3---5$\times$ better throughput compared to when using AMD-sev and SGX as trusted messages' singers.

\begin{table}[t]
    \centering
    \small
    \setlength\tabcolsep{4.2pt}
    \begin{tabular}{ccrrrrr}
        \hline
        & &\multicolumn{4}{c}{TCB size (LoC)} \\ \cmidrule{3-6}
        System          &Threat model &OS & Att. kernel & App & Total \\ \midrule
        TEEs-Raft       &CFT &2,307K & 1,268 & 856 & 2,309K \\
        TEEs-CR         &CFT &2,307K & 1,268 & 992 & 2,309K \\ 
        \projecttitle{} &BFT & -     & 2,114 & -   & 2,114  \\ \hline
    \end{tabular}
    % \caption{\rev{(c)}{\projecttitle{} compared with TEE-hosted applications.}}
    \caption{\rev{(c)}{\projecttitle{} compared with TEE-hosted applications.}}
    \label{table:qualitative_comparison}
    \vspace{-2mm}
\end{table}

%~ configured with virtio with batch factor equal to one
\myparagraph{\rev{(c)}{TEEs-hosted baselines}} 
\revcont{We compare \projecttitle{} with TEEs-hosted systems implementing two prototypes based on the failure-free execution of Raft (TEEs-Raft) and CR (TEEs-CR). The code runs within three AMD-sev machines. Prior works~\cite{avocado, nimble} suggested this setup for performance---however, at the cost of (1) significantly increased TCB size and (2) a weaker threat model from the application perspective. 
Table~\ref{table:qualitative_comparison} summarizes the security costs. Regarding (1), the TCB of TEEs-hosted systems includes the entire OS~\cite{gramine_tdx}, OpenSSL libraries for messages authentication~\cite{openssl_hmac} (labeled as Att. kernel), and the application codebase, which is over $2$M LoCs in total. In contrast, \projecttitle{}'s TCB only includes our hardware attestation kernel, which is 2,114 LoC of HLS/HDL code. It is only 0.09\% of TEE-hosted systems. 
% In contrast, our \projecttitle{}'s attestation kernel is minimal; only 2,114 LoC of HLS/HDL code (0.09\% of TEE-hosted systems). 
Regarding (2), the TEE-hosted application can only fail by crashing; it can be thought to remain protected from a potentially Byzantine cloud environment, whereas \projecttitle{} targets BFT settings, handling up to $f$ arbitrary failures. 
}

\revcont{We compare TEE-Raft with our \projecttitle{}-based BFT (Figure~\ref{fig:byz_smr_throuthput}) as both are broadcast-based protocols, and TEEs-CR with our \projecttitle{}-based CR (Figure~\ref{fig:byz_chain_replication}) as both require all messages to traverse the entire chain of nodes. TEE-Raft achieves approximately $2.5\times$ higher throughput than \projecttitle{}-based BFT. The performance difference is primarily due to Raft's one-phase commitment compared to our \projecttitle{}-based BFT. Similarly, TEE-CR achieves $2\times$ higher throughput than the \projecttitle{}-based CR. While both versions of CR involve the same number of network Round-Trip Times (RTTs), 
\projecttitle{} involves a higher number of the attestation kernel invocations to verify all the chained messages in the PoE.
}

% \begin{table}[t]
% \centering
% \small
% \begin{tabular}{ c c  c  }
% \hline
%  System & Threat model &  TCB size \\ 
%  \hline
%  \projecttitle{} & BFT  & $<2.5$K LoC  \\  
%  TEEs-app & CFT  & OS  $>2000$K LoC\\
%  \hline
% \end{tabular}
% \caption{\projecttitle{} compared with TEE-hosted applications.}\label{table:qualitative_comparison}
% \vspace{-3mm}
% \end{table}

\subsection{\revcont{FPGA Resource Usage}}
\rev{(d), E2}{Lastly, we perform a resource utilization analysis to show \projecttitle{}'s scalability capabilities. We measure the resource consumption of \projecttitle{}'s primary hardware components~\cite{easynet} and estimate maximum connections on the latest FPGA. }
% , e.g., how many instances of the attestation kernel as well as the RoCE can fit on the same FPGA.

\revcont{
Table~\ref{tab:resource-utilization} shows the resource consumption details. The overall \projecttitle{} design consumes 16.6\% of LUTs, 16.3\% of Flip-Flops (FF), and 16.6\% of RAMB36 (3.46~\% of the entire on-chip memory) on the U280 FPGA. Note that \projecttitle{} only requires commodity FPGA NIC designs to add the attestation kernel, whose utilization is comparable with the other modules, XDMA and RoCE. 
% , which occupies 15.7\% of LUTs, 13.4\% of FFs, and 24.1\% of RAMB36 compared to the entire design.
}

\revcont{
Figure~\ref{fig:scalability} shows the scaling capabilities of \projecttitle{} hardware. 
% The XDMA module, CMAC module, and RoCE kernel are single for each in the design because the XDMA and CMAC modules are independent of the number of connections and the RoCE kernel is configured to hold 500 queue pairs to establish the same number of network connections~\cite{storm}. 
As the number of network connections increases, we only need to replicate the attestation kernel because the XDMA and CMAC modules are independent of the number of connections, and the RoCE kernel is configured to hold up to 500 connections~\cite{storm}. The result demonstrates that \projecttitle{} can support up to 32 concurrent connections on a single U280 FPGA. 
%  to sustain the throughput per connection
% 500 queue pairs to establish the same number of
}
% \rev{E2}{Our implementation is based on~\cite{coyote}, a fork of which has been used in prior works~\cite{storm} showing that 500 queue pairs (QPs) occupy 9\% of the on-chip memory, while the logic resource usage remains below 1\% when scaling from 500 to 16,000 QPs. Our evaluation and modern deployments use more powerful FPGAs, suggesting that even a larger number of connections could be supported compared to the work in~\cite{storm}. }

\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{L}[1]{>{\raggedright\arraybackslash}p{#1}}
\newcolumntype{R}[1]{>{\raggedleft\arraybackslash}p{#1}}

\begin{table}[t]
    \small
    \centering
    \setlength\tabcolsep{4.2pt}
    \begin{tabular}{C{14mm}R{10mm}R{7mm}R{10mm}R{7mm}R{6mm}R{7mm}}
    \hline
        Name & \multicolumn{2}{c}{LUT (\%)} & \multicolumn{2}{c}{FF (\%)} & \multicolumn{2}{c}{RAMB36 (\%)} \\ \cmidrule(lr){1-1} \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
        U280               & 1303680 &  (100) & 2607360 &  (100) & 2016 &  (100) \\ \cmidrule(lr){1-1} \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
        TNIC               &  216905 & (16.6) &  423891 & (16.3) &  335 & (16.6) \\               
        XDMA               &   48258 &  (3.7) &   50701 &  (1.9) &   64 &  (3.1) \\ 
        Att. kernel        &   34138 &  (2.6) &   56914 &  (2.2) &   81 &  (4.0) \\ 
        RoCE               &   30379 &  (2.3) &   75804 &  (2.9) &   46 &  (2.3) \\ 
        CMAC               &    1484 &  (0.1) &    3433 &  (0.1) &    0 &  (0.0) \\ \hline
    \end{tabular}
    \caption{\rev{(d)}{\projecttitle{}'s resource usage. The relative (\%) compares with the U280 FPGA capacity. \projecttitle{} means the entire design.}} 
    % available resources on the 
    \label{tab:resource-utilization}
    \vspace{-2mm}
\end{table}


% Nevertheless, our \projecttitle{} does not assume a specific FPGA board. Therefore, the findings from previous works on other boards are still relevant.


\if 0
\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{atc-submission-plots/bftpr_lat_throughput.pdf} 
    \caption{Throughput-latency evaluation of a BFT version of PR using various trusted components.} \label{fig:lat_throughput_kernel}
\end{figure}
\fi