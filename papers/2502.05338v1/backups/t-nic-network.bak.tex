\section{Trusted Network Stack}
\label{sec:t-nic-network}


\begin{figure}[t!]
    \centering
    %\includegraphics[width=0.5\textwidth]{figures/trusted-nic-overview.drawio.pdf}
    \includegraphics[width=0.5\textwidth]{figures/trusted_nic_overview_lat-network_stack_system_design.drawio-1.pdf}
    \caption{\projecttitle{} system network stack.}
    \label{fig:network_stack_design}
\end{figure}


We build a software \projecttitle{} system network stack that operates as the {\em middle layer} between the \projecttitle{} programming APIs (see $\S$~\ref{sec:net-lib}) and the hardware implementation of \projecttitle{}. Figure~\ref{fig:network_stack_design} shows the overview of \projecttitle{}'s system network stack design. Specifically, \projecttitle{} system stack comprised of two core components: {\em (1)} the \projecttitle{} driver and the mapped REGs pages that are responsible for the configuration (shown in dashed lines) and host---device communication respectively and {\em (2)} the RDMA OS abstractions that executes networking operations. 

\subsection{\projecttitle{} Driver and Mapped REGs pages} 

The \projecttitle{} driver is invoked at the initialization of the device to make the appropriate configurations of the \projecttitle{} hardware.  To achieve that, it loads a kernel module that sets up the device and provides its static configuration, specifically the device MAC address, the device QSFP port and a (configurable) network IP that is used by the application code to exchange messages. 

The driver additionally provides an implementation of the {\tt mmap} system call as part of its {\tt file\_operations structure}~\cite{device_drivers} to reserve and map the device to a user-space addresses range. We refer to these mappings with the term Mapped REGs pages. In our \projecttitle{} we reserve one page of 4KB that equals the page granularity of our system for each connected device and are represented as pseudo-devices in {\tt /dev/fpga<ID>}. Specifically, accessing and writing to the pseudo-device equals to accessing the control and status registers of the FPGA. As such the software system stack, and consequently the applications, can directly intact with the control path of the \projecttitle{} hardware at low latency bypassing the operating system. 

These device mappings also expose the control registers for the  transmission/reception queues. In our current \projecttitle{} configuration, the depth is set to 22. If the queues are full, the ibv library temporarily blocks and continuously monitors the queues for finding empty slots.  The application pass down to the device any requests and their related parameters whereas the ibv library encodes and stores to the pages the following data request opcode, queue id and other identifiers, remote and local offset and data len. %The \projecttitle{} hardware network stack decodes the metadata and parameters into the  update the pages entries about the completed requests and other statistics.

\subsection{RDMA OS Abstractions} The RDMA OS abstractions is a user-space library that enables the networking operations in the \projecttitle{} hardware bypassing the kernel for performance. As shown in Figure~\ref{fig:network_stack_design}, the application is using the \projecttitle{} programming API that calls into the RDMA OS library to materialize the networking operations. The blue colored lines show the data path that operate on a separate bus from the control path (black lines). The RDMA OS abstractions system stack overall is comprised of two parts, the network (RDMA) library that is colored in purple in Figure~\ref{fig:network_stack_design} (e.g., ibv library, ibv structs and ibv memory) and the OS library that is colored in red in Figure~\ref{fig:network_stack_design} (process and scheduler libraries).

\myparagraph{Network (RDMA) library} The network (RDMA) library includes all the logic and the data structures required for implementing the RDMA protocol. The ibv library is executing the application's networking operations by posting the requests to the hardware. Specifically, it creates an internal representation of the request and the associated data and metadata (i.e. request opcode, remote IP, local address, remote base address and offset, data length, etc.) encodes all related parameters and writes them in their specific offsets in the REGs pages to update the control registers in the \projecttitle{} hardware. The \projecttitle{} decodes the inputs to execute the appropriate RDMA protocol. Similarly to the classical RDMA implementation, the ibv structs manage all the connections' internal metadata, such as queues per connection, local and remote memory addresses, rkeys, etc. 

As shown in Figure~\ref{fig:network_stack_design}, the submission and reception of
requests and responses mandate the allocation of application network
buffers. The \projecttitle{} system stack follows the memory management design that has already been established in widely direct network I/O libraries~\cite{erpc, dpdk, rdma} for performance. Specifically, in \projecttitle{} application network buffers need to be mapped to a specific \projecttitle{}-memory, called the ibv memory. The ibv memory is allocated at the connection creation by the application through the ibv library. It resides within the application's address space with full read/write permissions and it is registered to the \projecttitle{} for remote reads and writes without involving the CPU.

Applications should wisely use this memory to eliminate unnecessary data copies from  applications' buffers to network buffers. Specifically, while applying \projecttitle{} into four systems and use-cases ($\S$~\ref{sec:use_cases}), we allocated all application buffers from the ibv memory, hence the network buffers (registered RDMA memory). In \projecttitle{} the ibv memory buffers is allocated in the {\tt huge page} memory area (2MB) and is available for DMA transfers from and to the \projecttitle{} hardware.  



\myparagraph{OS library} The OS library in \projecttitle{} implements the scheduling and coordination of the application threads regarding the network operations and \projecttitle{} device access. Specifically, each \projecttitle{} device is represented by one or more \projecttitle{}-processes (process library) depending on the number of applications that are using the device. The \projecttitle{}-process in \projecttitle{} is not a separate  scheduling entity (i.e., a thread as in classical OSes). In contrast, it is an object handle managed by the scheduler library (in Figure~\ref{fig:network_stack_design}) that acquires locks on the respective REG pages and invokes data transfers and network operations. Specifically, when the scheduler marks the the \projecttitle{}-process as eligible for execution, the process can write the request's data and metadata in its associated offset in the corresponding REG page. The scheduler library in \projecttitle{} is lightweight and its core responsibility is to ensure that the access to the mapped REG pages, hence the device, are executed in an isolated way. For performance, the data to be sent (blue lines) is never copied across \projecttitle{} system stack, instead, the \projecttitle{}-process only sets the necessary encoded metadata to the REGs pages and directly posts the command in the first available command slot. As discussed, the \projecttitle{} hardware will later fetch the data from the ibv memory through a DMA transfer (blue lines in the Figure~\ref{fig:network_stack_design}).

%Upon a transmission the {\tt ibv library} which is creates an internal representation of the request and the associated data and metadata (i.e. request opcode, remote IP, local address, remote base address and offset, data lenght, etc.). Then the request is executed as part of a (virtual) process that is mapped to a specific id ({\tt pid}) similar to the classical OS process abstraction. When the scheduler marks the process as eligible for execution, the process writes the request's data and metadata in its associated REG page. To improve for performance the actual data to be sent are not copied within all the system stack, instead, the process writes the local address and the length of the data (in bytes) to be sent and posts the command. The {\dimitra{;....}}
