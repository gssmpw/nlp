\section{Evaluation}
\label{sec:eval}
\myparagraph{Implementation}
We implement our prototype of \projecttitle{} extending the Coyote~\cite{coyote} codebase on top of Alveo U280~\cite{u280_smartnics}. We build the attestation kernel based on the HMAC module provided by Xilinx with the SHA-384 as the hashing algorithm~\cite{vitis-security-lib}. We wire the attestation kernel to the RoCE kernel input and output data cables (\texttt{axis\_rdma\_sink} and \texttt{axis\_host\_sink}) through 64B data paths (512-bit AXI4 Stream interfaces). The attestation kernel and the host memory communicate through an AXI4 memory-mapped Master. Lastly, the RoCE kernel is connected to a 32B bus width (parameters), a 20B bus width (requests opcodes), and a 12B bus width for local DMA operations. For the data transfers, \projecttitle{} builds on top of an XDMA IP~\cite{xdma, fpga_dma} that enables DMA over PCIe. The 100Gb MAC is implemented with a CMAC IP~\cite{license} that exposes two 512-bit AXI4-Stream interfaces to the RoCE protocol kernel for the transmitting (Tx) and receiving (Rx) network paths.

%Precisely, we extend the data path of Coyote, adopting the abstraction of {\em virtual} FPGA areas to plug in our attestation functionality. \projecttitle{} leverages the Coyote paradigm and wires the control domain and the data domain into different paths.


%The data flow from the host memory to the device memory using the Xilinx streaming protocol (i.e., \texttt{hls\_stream<>}) that divides data into 512-bit packets. 
%At the transmission path, we implemented and plugged in two FIFO queues that redirect the packets from the host memory to the device memory -- specifically into the HMAC module and the RoCE IP. The role of the first queue is to buffer data and calculate the HMAC of all packets of the transmission.
%The module is based on the HMAC module in the Xilinx Vitis Security Library using SHA-384 as the hash function~\cite{vitis-security-lib}.
%The data of the second queue is passed through to the RoCE IP until the last packet of the transmission is reached.
%When the HMAC has been calculated, the last packet of the transmission is replaced by the 384-bit HMAC padded to 512 bits.

%At the reception path, we also augment the stack with two queues.
%The first queue sends the data to the verification module that will calculate the {\em expected} HMAC for verification. If the verification process succeeds we reconstruct the received message and deliver it to the application layer (host memory). Otherwise, all corresponding packets are dropped which naturally affects liveness.

\if 0
\myparagraph{Implementation}
We implement \projecttitle{} extending Coyote~\cite{coyote} codebase. Precisely, we extend the data path of Coyote, adopting the abstraction of {\em virtual} FPGA areas to plug in our attestation functionality. \projecttitle{} leverages the Coyote paradigm and wires the control domain and the data domain into different paths.

The RoCE kernel input and output data cables, e.g., \texttt{axis\_rdma\_sink} and \texttt{axis\_host\_sink}, are connected through the Attestation kernel through 64B data paths (512-bit AXI4-Stream interfaces). Then, the attestation kernel and the host memory communication are achieved through an AXI4 memory-mapped Master interface using the Xilinx streaming protocol. It is the responsibility of the host code to allocate and initialize host memory. Further, the kernel is connected to a 32B bus to receive the parameters, a 20 B bus to issue RDMA write operations, and a 12 B bus to issue local DMA commands.

%The data flow from the host memory to the device memory using the Xilinx streaming protocol (i.e., \texttt{hls\_stream<>}) that divides data into 512-bit packets. 
At the transmission path, we implemented and plugged in two FIFO queues that redirect the packets from the host memory to the device memory -- specifically into the HMAC module and the RoCE IP. The role of the first queue is to buffer data and calculate the HMAC of all packets of the transmission.
The module is based on the HMAC module in the Xilinx Vitis Security Library using SHA-384 as the hash function~\cite{vitis-security-lib}.
The data of the second queue is passed through to the RoCE IP until the last packet of the transmission is reached.
When the HMAC has been calculated, the last packet of the transmission is replaced by the 384-bit HMAC padded to 512 bits.

At the reception path, we also augment the stack with two queues.
The first queue sends the data to the verification module that will calculate the {\em expected} HMAC for verification. If the verification process succeeds we reconstruct the received message and deliver it to the application layer (host memory). Otherwise, all corresponding packets are dropped which naturally affects liveness.
\fi 

\myparagraph{Evaluation setup}
We perform our experiments on a real hardware testbed using two clusters: AMD-FPGA Cluster and Intel Cluster. AMD-FPGA Cluster is equipped with two machines powered by AMD EPYC 7413 (24 cores, 1.5 GHz) and 251.74 GiB memory. Each machine also has two U280 Alveo cards~\cite{u280_smartnics} that are connected over a 100 Gbps interconnect. Intel Cluster is equipped with three machines powered by Intel(R) Core(TM) i9-9900K (8 cores, 3.2 GHz) with 64 GiB memory and three Intel Corporation Ethernet Controllers (XL710).

%, caches: 32 KiB (L1 data and code), 256 KiB (L2) and 16 MiB (L3). Cluster 2 nodes are connected over a 40GbE QSFP+ network switch.


%, caches: 32 KiB (L1 data and code), 512 KiB (L2) and 32 MiB (L3). Each machine also has two U280 Alveo cards~\cite{u280_smartnics} that are connected over a 100 Gbps cable.
%\atsushi{Can we simplify the above like this: Cluster 1 is equipped with two machines powered by AMD EPYC 7413 (24 cores, 1.5 GHz) and 251.74 GiB memory.}

%Cluster 2 is equipped with three machines with CPU (marked as CPU-2 in the respective plots): Intel(R) Core(TM) i9-9900K (8 cores, 3.2 GHz), memory: 64 GiB, caches: 32 KiB (L1 data and code), 256 KiB (L2) and 16 MiB (L3). Cluster 2 nodes are connected over a 40GbE QSFP+ network switch.
%\antonis{CPU-1 and CPU-2 are not memorable names, consider AMD, x86?} %\atsushi{I agree with Antonis. Also, can we rename Cluster 1 and 2 like this: AMD-FPGA Cluster and Intel Cluster?} \atsushi{Cluster 2 is equipped with three machines powered by Intel(R) Core(TM) i9-9900K (8 cores, 3.2 GHz) with 64GiB memory.}

%We perform our experiments on a real hardware testbed using two clusters: Cluster 1 and Cluster 2. Cluster 1 is equipped with two machines with CPU (marked as CPU-1 in the respective plots): AMD EPYC 7413 (24 cores, 1.5 GHz), memory: 251.74 GiB, caches: 32 KiB (L1 data and code), 512 KiB (L2) and 32 MiB (L3). Each machine also has two U280 Alveo cards~\cite{u280_smartnics} that are connected over a 100 Gbps cable.
%\atsushi{Can we simplify the above like this: Cluster 1 is equipped with two machines powered by AMD EPYC 7413 (24 cores, 1.5 GHz) and 251.74 GiB memory.}

%Cluster 2 is equipped with three machines with CPU (marked as CPU-2 in the respective plots): Intel(R) Core(TM) i9-9900K (8 cores, 3.2 GHz), memory: 64 GiB, caches: 32 KiB (L1 data and code), 256 KiB (L2) and 16 MiB (L3). Cluster 2 nodes are connected over a 40GbE QSFP+ network switch.
%\antonis{CPU-1 and CPU-2 are not memorable names, consider AMD, x86?} %\atsushi{I agree with Antonis. Also, can we rename Cluster 1 and 2 like this: AMD-FPGA Cluster and Intel Cluster?} \atsushi{Cluster 2 is equipped with three machines powered by Intel(R) Core(TM) i9-9900K (8 cores, 3.2 GHz) with 64GiB memory.}



We evaluate the performance of \projecttitle{} across three dimensions: {\em (i)} the hardware evaluation which focuses on the Attestation kernel evaluation (\S~\ref{subsec:hw_eval}), {\em (ii)} the network library evaluation which assesses and compares the \projecttitle{} with competitive network stack baselines (\S~\ref{subsec:net_lib}) and {\em (iii)} different use-cases we built to evaluate our four secure primitive-examples we constructed on top of \projecttitle{} (\S~\ref{subsec:use_cases_eval}). 





\subsection{Hardware Evaluation: \trustedfpga{}}
\label{subsec:hw_eval}


\begin{table}
\begin{center}
\small
\begin{tabular}{ |c|c|c| } 
 \hline
 System &  (host) TEE-free & Tamper-proof \\ [0.5ex] \hline \hline
 SSL-lib & Yes & No\\
 SSL-server/x86*/AMD  &  Yes & No\\
 SGX/AMD-sev &  No & Yes\\
% TEE-P [Hybster, Damysus, Trinc] & Yes & Yes & Yes & Assumes trusted local persistent state\\
 %TEE-DS & Yes & Yes & Yes & Builds a DS of $f+2u+1$ TEEs\footnote{$f$ is the compromised TEEs and $u$ is the number of unresponsive TEEs}\\
 \projecttitle{} & Yes & Yes\\
 \hline
\end{tabular}
\end{center}
%\vspace{-10pt}
\caption{(Trusted) Host-sided hardware baselines and \projecttitle{}. (*) We also the term SSL-server for this run unless stated otherwise.}
\label{tab:hw_options}
%\vspace{-18pt}
\end{table}

\myparagraph{Baselines} We evaluate the performance of the \projecttitle{}'s attestation kernel {\tt Attest} function (Algorithm~\ref{algo:primitives}) compared with four competitive (host-sided) systems: (Intel-)x86 (untrusted), AMD (untrusted), SGX (TEE-based) and AMD-sev (TEE-based). For these host-sided versions we build servers based on OpenSSL that run natively or within a TEE and attest and forward the network messages to the interested host process. From now on we use the term x86 and AMD to refer to a native run of the server process (SSL-server) and the terms SGX and AMD-sev to refer to their tamper-proof versions within a TEE. Both the server process and the host process run in the same machine to eliminate network latency and communicate through TCP sockets. We implement the SGX system using the {\scone{}} framework~\cite{scone} that optimizes (exit-less) syscalls execution while AMD-sev system runs in a QEMU VM using the official supported kernel image~\cite{AMDSEV}. In addition, we build the SSL-lib which integrates the {\tt Attest} function as a library (natively). Recall that both SSL-lib and SSL-server are not tamper-proof. Table~\ref{tab:hw_options} summarizes our baselines with \projecttitle{}. %To avoid distortion, our plots do not include Trinc~\cite{levin2009trinc} because its reported latencies are an order of magnitude slower (85-105 ms) than all other evaluated solutions.


%(Table~\ref{tab:hw_options})
%Nat-lib is an OpenSSL-based library integrated into the code logic that generates and verifies messages. Nat is an OpenSSL-based server that communicates through kernel TCP sockets with the interested process to attest and verify its messages. \atsushi{can we shorten the description of Nat and Nat-lib?} Similarly, SGX and AMD-sev are OpenSSL-based servers that run within a tamper-proof TEE---specifically, Intel SGXv1~\cite{cryptoeprint:2016:086} and AMD-sev~\cite{amd-sev} (configured without offering confidentiality), respectively.  We implement the SGX-based server using the {\tt SCONE} framework~\cite{scone} to execute (exit-less) syscalls in a performant fashion. We run AMD-sev as a QEMU VM using the official supported kernel image~\cite{AMDSEV}, which efficiently runs un-modified Linux applications. Experiments with CPU-1, CPU-2, and SGX ran on Cluster 2, whereas we ran AMD-sev experiments on Cluster 1 \atsushi{CPU-1 should also be on Cluster 1, right?}. 
%On purpose, we exclude  Trinc~\cite{levin2009trinc} because it reported latencies that are an order of magnitude slower (85-105 ms) than \projecttitle{}. 
%To avoid distortion, our plots do not include 
%Trinc~\cite{levin2009trinc} because its reported latencies are an order of magnitude slower (85-105 ms) than all other evaluated solutions.
% \projecttitle{}

%\antonis{Differences between Nat and Nat-lib are not clear. Also, names could be more descriptive/memorable (Nat-lib is also very similar to NET-LIB).}

%\atsushi{can we remove this paragraph? (due to the space)} In short, we seek to answer the following research questions:
%\begin{itemize}
%\item {\bf{RQ1.}} What is the performance for generating and verifying attested messages in \projecttitle{}?
%\item {\bf{RQ2.}} What is the performance breakdown and latencies?
%    \item[\bf{RQ3}]  How much resources \projecttitle{} uses for the attestation kernel (Table~\ref{table:resources_usage})?
 %   \item[\bf{RQ4}] What is the hardware network latency and throughput (Fig~\ref{fig:hw_lat_breakdown})?
%\end{itemize}

\myparagraph{Methodology and experiments}
We use Vitis XRT v2022.2 for the stand-alone evaluation of \projecttitle{} attestation kernel. We load the shell \texttt{xilinx\_u280\_gen3x16\_xdma\_base\_1} and use Vitis XRT synchronous data transfers from host to device and vice versa. We measure and report the {\tt Attest} function average latency as well as the communication costs executing an empty funcation body.
%To isolate the latencies between data transfers and computation we further execute the same experiment without computing the HMAC (empty kernel). \atsushi{I wanna simplify the texts here... work on it later}

%We use Vitis XRT v2022.2 for the stand-alone evaluation of \projecttitle{} attestation kernel on top of which we build the host and the \trustedfpga{} processes (\texttt{xilinx\_u280\_gen3x16\_xdma\_base\_1}). We use Vitis XRT synchronous data transfers from host to device and vice versa. We measure and report the HMAC average latency on these competitive systems. To isolate the latencies between data transfers and computation we further execute the same experiment without computing the HMAC (empty kernel). \atsushi{I wanna simplify the texts here... work on it later}


\myparagraph{Results}
Figure~\ref{fig:attest_kernel} shows the average latency of the {\tt Attest} function based on the HMAC algorithm for data inputs of 64B and 128B. The {\tt Verify} function's latency is similar and, as such, it is omitted. Our \projecttitle{} achieves performance in the microseconds range (23 us) and outperforms its equivalent TEE-based competitors at least by a factor of 2. Importantly, \projecttitle{} is approximately 1.2$\times$ faster than the native runs of the SSL-server in AMD system, although it is approximately 2$\times$ slower than the Intel-x86. Recall that both ADM and Intel-x86 are not tamper-proof.

Figure~\ref{fig:latency_breakdown} shows the latency breakdown of the  {\tt Attest} function. Accessing the \projecttitle{} device and the TEEs can be expensive ranging from 30\% to 90\% of the total operation latency among the systems. Specifically, for \projecttitle{}, the transfer time takes about 16 us, which accounts for 70\% of the total execution time. This is not a concern in \projecttitle{} as it effectively eliminates this cost by enabling asynchronous (user-space) DMA data transfers. Regarding the native runs, i.e., Intel-x86 and AMD, the communication costs including the syscalls execution, data transfers between the two processes account for the $~$90\% of the latency.  

Our evaluation showed that in the TEE-based systems, e.g., SGX and AMD-sev, the communication and system call execution costs account for up to 40\% of the total execution (on average). To our surprise this implies that the HMAC computation itself within any of the two TEEs experiences more than 30$\times$ overheads compared to its native run. We further analyzed TEEs' behavior instrumenting the client's code to accurately measure the operations' individual latency at various periods of time during the experiment. 

Figure~\ref{fig:latency_distribution} illustrates the individual operations latency for three systems: SGX (which runs the HMAC within an SGX SSL-server), SGX-empty (the SGX SSL-server copies and returns the input data without HMAC computation) and Intel-x86 (the SSL-server runs natively).  As shown in Figure~\ref{fig:latency_distribution}, the execution of HMAC within the SGX quite often experiences huge spikes in latency. While these spikes are very frequent, they are not guaranteed. We calculate an average mean of 45us and a geometric mean of 30 us. We attribute this behavior to scheduling effects and the asynchronous exitless system call API which is involved~\cite{scone}. Similar variations were observed for the AMD runs with latencies spiking up to 200--500 us.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{atc-submission-plots/hw_eval_attest_latency.pdf} 
  \caption{{\tt Attest} function latency.}
  \label{fig:attest_kernel}
\end{figure}

\begin{figure}
    \centering
  \includegraphics[width=\linewidth]{atc-submission-plots/latency_breakdown.pdf}
  \caption{{\tt Attest} function latency breakdown.}\label{fig:latency_breakdown}
\end{figure}

\begin{figure}
    \centering
  \includegraphics[width=\linewidth]{atc-submission-plots/foo100.pdf}
  \caption{{\tt Attest} function latency over time in SGX.}\label{fig:latency_distribution}\label{fig:latency_breakdown}
\end{figure}

\if 0
\begin{figure*}[t!]
\begin{center}
\minipage{0.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{atc-submission-plots/hw_eval_attest_latency.pdf} 
  \caption{HMAC (Attest) latency}
  \label{fig:attest_kernel}
\endminipage
\minipage{0.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{atc-submission-plots/latency_breakdown.pdf}
  \caption{Latency breakdown}\label{fig:latency_breakdown}
\endminipage

\minipage{0.50\textwidth}%
  \centering
  %\includegraphics[width=\linewidth]{atc-submission-plots/lat_distribution_sgx.pdf}
  \includegraphics[width=\linewidth]{atc-submission-plots/foo50.pdf}
  \caption{Latency distribution over time}\label{fig:latency_distribution}
\endminipage
\end{center}
%\caption{Performance evaluation of the trusted component in different hardware setups.} \label{fig:hw_eval}
\end{figure*}
\fi

\if 0
\begin{figure}[t!]
    \centering
    \includegraphics[width=.5\textwidth]{eval-plots/plots/hw_net_lat_throughput.pdf}
    \caption{Throughput latency plots for network operation.}
    \label{fig:hw_lat_breakdown}
\end{figure}
\fi

\if 0
\begin{center}
\begin{table}[ht]
\centering
\begin{tabular}{ |m{1.5cm}||m{1cm}| m{1cm}| m{1cm}|}
 \hline
  & LUTs &  BRAM & Regs  \\
 \texttt{Attest()} &  &  &\\
 \texttt{Verify()} &  &  &\\
 \hline
 \end{tabular}
\caption{Resources usage.}
\end{table}\label{table:resources_usage}
\end{center}
\fi

