%\section{Introduction}

\antonis{Suggestion: Let's keep \textit{names} (baselines, our solutions, etc.) short, consistent, descriptive, and memorable. Maybe tNIC (tFPGA, tNET-LIB) rather than TNIC (T-FPGA, NET-LIB).}
%\antonis{are we open-sourcing anything from this paper? if so, let's mention it somewhere.}


\myparagraph{intro notes}
\begin{itemize}
    \item[1] distributed systems + cloud, - networking a play an important role and the need for building trusted distributed systems 
    \item[2] secure distributed system properties: safe, live, and correct in presence Byzantine adversary. (define Byzantine here)
    \item[3]  There is PATH: silicon root of trust is promising approach to build trusted distributed systems, in particular TEEs. It is provided by all cloud-providers -- BUT have limitations for DS:  (background of challenges-> bring networking aspect)
\end{itemize}

The challenges of conventional TEE appraoch:

1) host-side TEEs: Progamaaiblity + security/correct challenges (Beefy TCP/OS which we do not need or complicate correctness) + performance + Heterogenity (lack of unified abstraction). How do you stitch all these heterogenous TEEs all together? One unidied abstarction (heterogeneours nodes, blah) at the Network level 


2)  - They provide a lot of features, which mean still High TCB, we don't even need to know all those features!

    => Our goal to find minimal abstraction for trusted distributed systems.

    => Minimal requirements: a) non-equivocation and transferable authentication

3) Performance: Distributed systems are relying of networked-systems, and they need network to be fast and  protocol offloading is already prominent, can we build it on fast path. Why can't we offload the security processing threre too?


Overall question: 

What is the unified, minimal and high-performance networking abstraction for building trusted distributed systems in the untrusted heterogenous cloud?



Our approach:

- Unified abstraction for network level isolation.

- Minimalistic: two properties (just state the properties)-> are guaranteed to build trusted systems. TEEs cannot be verified whereas our minimal can do this! In fact, we do this and ..


- Performance: implement at the NIC.



Cross-layer approach:

- bottom up: hardware nic
- network stack
- library
- applications.

implementation

Evaluation.

Contributions: 

- formal verification


\myparagraph{Design requirements for distributed systems notes}

- Requirements for trustworthy distributed systems:

    - crash vs byzantine systems

-bring what do they need actually? trade-offs? what  how can we secure them with bringing the trusted hardware?
required properties to transform? security only two properties.


- Performance 

 - heterogenous nodes -- unified a set of heterogenosu nodes


 [IMPORTANT] THis section end with the message -- we need to end the NIC level trusted hardware...

\myparagraph{overview notes}

- section: Design requirement for trusted NICs

- system overview

- System model.



%To make matters worse,  large-scale datacenters (DCs) are highly \emph{heterogeneous}: with machines not only differing in terms of configuration space (memory, storage, etc.), but more importantly, with carrying different (host) CPU architectures with various capabilities too~\cite{6756704}. This is an inevitable result of the continuous growth of the cloud~\cite{cloud_growth, google_servers} and the limited lifespan of the machines~\cite{dc_lifespan}. Consequently, the replication protocols and systems need to be designed and implemented to be able to run transparently and preserve their security and correctness properties even in heterogeneous CPUs. 


\myparagraph{Motivation} An ever-increasing amount of data storage and computing is performed in the cloud due to the cost benefits for both the users and the cloud providers. Large-scale cloud providers, (e.g., Amazon EC2~\cite{amazon_ec2}, Microsoft Windows Azure~\cite{microsoft_azure}, Rackspace~\cite{rackspace} and Google Compute Engine~\cite{google_engine}) have built datacenters (DCs) to provide a computing infrastructure for users to run their applications. 

The cloud infrastructure is characterized by {\em (1)} heterogeneity and {\em (2)} distribution. The DCs have an heterogeneous computing infrastructure with machines not only differing in configuration space (memory, storage, etc.), but more importantly, with carrying different (host) CPU architectures~\cite{6756704} with limited lifespan~\cite{dc_lifespan}. At the same time, tens of thousands of user applications mandate more than one distributed machines to cooperate or rely on provided high-end distributed systems (i.e., storage systems and databases~\cite{dynamo,  azure_storage, tao, spanner, 51, zippy, AmazonS3}, management services~\cite{Hunt:2010, Burns2016} and computing frameworks~\cite{aws_lambda, azure_functions, google_cloud_functions}). 

Undoubtedly, these distributed systems and user applications must be highly available, fast, and trustworthy. They need to remain correct even when failures occur~\cite{ford2010availability, Mazieres2002b, Garay2000}. As such, their vast majority ensure availability in the presence of faults by deploying replication protocols~\cite{Raft, azure, Fan-MemC3, DeCandia2007, ford2010availability, cockroachdb, farm, Hermes:2020, fasst, zippydb}. These replication protocols usually target a very limited range of faults: they assume a Crash Fault Tolerant (CFT) model where the participating nodes and the cloud infrastructure are trusted and fail only by crashing~\cite{delporte}. In the rest of the paper, we refer to such protocols as CFT protocols.


%\dimitra{Talk about security implications}

Unfortunately, this widespread and multi-dimensional adoption of the cloud has drastically increased the surface area of faults and attacks that go beyond the CFT model, ranging from software bugs and configuration errors to malicious attacks~\cite{Gunawi_bugs-in-the-cloud, Shinde2016, high_resolution_side_channels}. CFT protocols are fundamentally incapable of providing consistent replication in the presence of non-benign faults.




\if 0
To make matters worse, these systems need to be designed and run in large-scale cloud environments equipped with thousands of machines: e.g., Google is estimated to own more than 2\% of all servers in the world~\cite{google_servers}.  Several generations of machines with different specifications are continuously being encountered: First, as the underlying machine types evolve over time, newer machines offer a more attractive price-performance solution. Secondly, (old) machines wear out and are replaced by newer and more powerful ones: the average useful lifespan of a server is estimated to be 3---5 years~\cite{dc_lifespan}. Lastly, the global market of the public cloud is continuously growing---specifically, it is estimated to grow at least $20\%$ each year~\cite{cloud_growth}. As a result, modern datacenters are highly\emph{heterogeneous}: the machines not only can differ in terms of configuration space (memory, storage, etc.), but more importantly, they carry different (host) CPU architectures with various capabilities too~\cite{6756704}. Users need to be able to have the right tools available so they can use the right machines/processors in the right place at the right time.
\fi

%\dimitra{Talk about cloud services and distributed protocols}
\if 0
At the same time, while we more and more rely on online cloud-hosted services, there is an urgent need for systems and applications that are highly available, fast, and trustworthy, and, importantly, they remain correct even when failures occur. For this reason, the vast majority of cloud services are designed to ensure availability in the presence of faults by deploying replication protocols~\cite{Raft, azure, Fan-MemC3, DeCandia2007, ford2010availability, cockroachdb, farm, Hermes:2020, fasst, zippydb}. These replication protocols target a limited spectrum of faults assuming that the participant nodes and the cloud infrastructure can be trusted. More specifically, they target the Crash Fault Tolerant (CFT) model that assumes a fail-stop fault model; the participant machines are trusted (e.g., execute the application code correctly, do not have corrupted memory/storage and network packets, etc.) and can only fail by crashing~\cite{delporte}. We refer to such protocols as CFT protocols.
\fi

%\dimitra{Talk about security implications}
\if 0
However, this wide adoption of the third-party cloud infrastructure has drastically increased the surface of faults and attacks that range, beyond the CFT model, from software bugs and configuration errors to malicious attacks~\cite{Gunawi_bugs-in-the-cloud, Shinde2016, high_resolution_side_channels}. Unfortunately, CFT protocols are fundamentally incapable of providing consistent replication in the presence of non-benign faults.
\fi
%\dimitra{Talk about prior work limitations}

\myparagraph{Limitations of state-of-the-art approaches} Prior work has established Byzantine Fault Tolerant (BFT) replication that goes beyond the CFT model and offers important foundations for developing protocols with stronger guarantees~\cite{Castro:2002, DBLP:journals/corr/abs-1803-05069, 10.1145/1658357.1658358, engraft, hybster, 10.1145/2168836.2168866, DBLP:journals/corr/LiuLKA16a, 10.1145/3492321.3519568}. These protocols offer consistent replication when processes that fail in arbitrary ways~\cite{Lamport:1982}. For the rest of the paper, we refer to these protocols as BFT protocols. 
Existing BFT protocols present severe trade-offs between performance, scalability, and adoption that impede their widespread application in the heterogeneous cloud~\cite{bft-time-is-now, 10.1145/1711475.1711494}. %System designers can have BFT either by giving up on scalability and performance~\cite{Castro:2002} \antonis{cite?} or by relying on specific CPU-based hardware trusted modules~\cite{}.


On the one hand, traditional BFT protocols~\cite{Castro:2002, DBLP:journals/corr/abs-1803-05069, 10.1145/1658357.1658358}, can transparently run on any CPU architecture but they have significantly limited performance and scalability~\cite{avocado}. In contrast to CFT protocols that usually operate with $2f+1$ nodes, BFT protocols add up an extra set of $f$ participant nodes, requiring at least $3f+1$ nodes, to tolerate up to $f$ arbitrary faults. They additionally introduce high message complexity~\cite{Castro:2002} or many protocol rounds~\cite{DBLP:journals/corr/abs-1803-05069} which greatly limits low-latency and high-performance operations. 

On the other hand, optimized (hybrid) BFT protocols~\cite{10.1145/3492321.3519568, minBFT, hybster, 10.1145/2168836.2168866, DBLP:journals/corr/LiuLKA16a, 10.1145/1294261.1294280} can operate  with fewer nodes at the cost of generality adoption. In particular, they heavily rely on trusted hardware---more specifically, Trusted Execution Environments (TEEs)~\cite{cryptoeprint:2016:086,arm-realm,amd-sev,riscv-multizone,intelTDX}---to prevent the Byzantine counterparts from making conflicting statements for the same round (equivocation). Unfortunately, hybrid protocols have been built on top of very specific and CPU-dependant TEEs rendering them ill-suited and not easily adopted in modern heterogeneous DCs. 

%In this work, we argue that this reliance on specific TEE limits generality and adoption in modern heterogeneous cloud infrastructure. For example, to run these protocols, a cloud provider must guarantee that there would always be available a required number of machines equipped with specific CPU-generation and TEE hardware version. In cases where this is impossible, protocol designers must be able to program different TEEs. This is rather challenging and raises questions when it comes to performance and correctness: different TEEs not only do they have different programming and performance characteristics but they greatly vary when it comes to security~\cite{10.1007/978-3-031-16092-9_7}. As an example of this, Keystone (RISC-V)~\cite{keystone_eurosys} and Intel SGX (x86)~\cite{intel-sgx} have quite different programming APIs. In addition, Intel SGX can only support a very limited Trusted Computing Base (TCB) compared to Intel TDX~\cite{intelTDX} and AMD-SEV~\cite{amd-sev}. Importantly, commercially available TEEs~\cite{arm-trustzone, intel-sgx, amd-sev, keystone_eurosys, 197162, timber} offer different levels of security (i.e., integrity, freshness and confidentiality) whereas they do not necessarily come with build-in support for secure bootstrapping and remote attestation~\cite{10.1007/978-3-031-16092-9_7, 7807249, secTEE}.


%\dimitra{Research question here:}

\myparagraph{System design and contributions} In this paper, we carefully address these limitations. Specifically, we seek to answer the following question: \emph{How can we design and implement a unified abstraction that is well suited for the heterogeneous modern cloud DCs, to aid in the design of robust replication protocols (e.g., under the BFT model), while targeting performance and scalability??}

%In this paper, we carefully address these limitations. Specifically, we seek to answer the following question: \emph{How can a trusted NIC \antonis{the NIC comes out of the blue here. NICs must be mentioned before.} architecture be designed and implemented to provide a unified abstraction that is well suited for the heterogeneous modern cloud DCs, to aid in the design of robust replication protocols (e.g., under the BFT model), while targeting performance and scalability??}
% \emph{How to design and implement a trusted NIC architecture that offers a unified abstraction well-suited to accommodate performant and scalable BFT replication protocols in heterogeneous DCs?}


%\dimitra{Introduce our \projecttitle{} + properties}

Since network operations are at the core of any distributed system, we tackle the question on the foundational level of a cloud system stack, i.e., the NIC level. In this work, we introduce \projecttitle{}, a trusted NIC architecture for heterogeneous cloud environments. \projecttitle{} aims to address the conflict between performance and adoption by materializing a network stack that allows us to translate CFT protocols to BFT ones without requiring extra rounds and nodes (as in classical BFT), neither homogeneous CPUs or TEEs (as in hybrid BFT). Our \projecttitle{} guarantees the two security properties of non-equivocation and transferable authentication~\cite{clement2012} for the (untrusted) network ensuring that a translation of a CFT protocol to a BFT protocol with $2f+1$ \emph{always} exists ($\S$~\ref{sec:background}). Our \projecttitle{} is carefully designed at the NIC's hardware---specifically, we leverage the SmartNIC technology~\cite{alveo_smartnics, intel_smartnics, bluefield_smartnics, broadcom_smartnics, liquidIO_smartnics, netronome_smartnics}, allowing protocols to seamlessly run in heterogeneous CPUs without relying on homogeneous TEEs hardware.

%\dimitra{Say about components + implementation}

%We designed \projecttitle{} adopting the SmartNIC technology shifting the requirement for homogeneous hardware from the CPU to the NIC-level, removing that way the burden from the protocol designers. 
We implemented \projecttitle{} from the ground-up on real hardware leveraging FPGA-based SmartNICs and, particularly, Alveo U280~\cite{alveo_smartnics}. Our system is comprised of two core components the hardware design of the \projecttitle{} ($\S$~\ref{subsec:tfpga}), and the \projecttitle{} software network library ($\S$~\ref{sec:net-lib}). We build a trusted user-space networking stack based on the RDMA protocol, which we further extend at the hardware level with a minimal hardware component, the attestation kernel, that guarantees the authenticity of the network messages. On top of our trusted network stack, we build the \projecttitle{} networking library that exposes to the developers a trusted, unified-to-heterogeneous-machines, API that \emph{(1)} follows the well-established for distributed programming RDMA programming paradigm and \emph{(2)} improves the security guarantees.

To evaluate the generality and performance of \projecttitle{}, we build four systems using \projecttitle{} APIs ($\S$~\ref{sec:use_cases}): {\em (1)} a single-node trusted log system, {\em (2)} a Byzantine SMR protocol, {\em (3)} a Byzantine Chain Replication protocol, and {\em (4)} an accountability protocol. We compare the \projecttitle{}-based protocols implementations with two TEE-based protocols versions with AMD-sev~\cite{} and Intel SGX~\cite{} ($\S$~\ref{}). Our evaluation shows that \projecttitle{} can improve up to $6\times$ the protocols' throughput compared to their TEE-based versions.


To sum up, we make the following contributions:
\begin{itemize}
    \item We design and implement \projecttitle{}, a trusted NIC architecture on top of FPGA-based SmartNICs, that aids the adoption and the design of highly-performant, scalable, and CPU-independent robust protocols that can be seamlessly deployed in modern heterogeneous DCs.
    \item We build a secure RDMA-based API on top of our \projecttitle{} following the current trends in implementing fast, yet trustworthy distributed protocols.
    \item We show how to use the \projecttitle{} to design and build robust distributed protocols ($\S$~\ref{sec:use_cases}).
\end{itemize}

%\myparagraph{Limitations of \projecttitle{}}
%Contributions:
%\begin{itemize}
    %\item An FPGA kernel that satisfies the CFT$\leftarrow$BFT transformation, by materializing the lower bounds of non-equivocation and transferable authentication from~\cite{}.
    %\item An RPC-library ..
    %\item 3 well-known use-cases
    %\item experimental evaluations

%    \item secure hardware network stack
%    \item secure RCP library
%    \item generality: application to protocols (leader + leaderless)
%\end{itemize}