\section{Design goals for the \projecttitle{} architecture}
\label{sec:requirements}
Our proposal for the design of a \projecttitle{} architecture targets to resolve the tension between security and performance when it comes to the design and implementation of replication protocols that are widely deployed in modern heterogeneous datacenters. To this end, our \projecttitle{} is specifically designed to help system designers build efficient, fault-tolerant, and robust systems under the presence of Byzantine actors in the untrusted cloud infrastructure. Specifically, our system targets the following design properties: 
\begin{itemize}
    \item {\bf{Robustness}} for building and deploying fault-tolerant distributed protocols and applications that remain correct and available in the untrusted cloud infrastructure.
    \item {\bf{Performance and scalability}} for efficiently exchanging and authenticating network messages. Particularly, our \projecttitle{} allows for BFT with the minimum possible number of involved servers ($2f+1$), mitigating clients' costs and resource usage.
    \item {\bf{Adaptability}} for allowing protocol designers to build robust protocols that can seamlessly run in modern heterogeneous DCs without them having to be BFT experts. 
\end{itemize}

\vspace{1pt}
Our system achieves these goals by implementing a minimal hardware-based authentication subsystem, the \emph{attestation kernel}, that guarantees the necessary security properties required for BFT. The attestation kernel generates and verifies authentication certificates for the network messages to ensure two core security properties: the non-equivocation and the (transferable) authentication properties. These two properties have been proven to be necessary and sufficient for decreasing the replication factor for BFT protocols~\cite{clement2012}. Further, we carefully implemented \projecttitle{} to optimize for performance and scalability by integrating the attestation kernel ``on path'' at the NIC-level. That way, our system offers security without introducing unnecessary overheads both in performance and adoption: data is processed \emph{on their way} to the network whereas the protocols do not have to rely on a specific TEE in the host CPU layer. 

\antonis{Consider another subsection here.}
Next, we explain the two core problems aroused from prior research efforts on BFT protocols and how our \projecttitle{} overcomes those.


%\dimitra{
%\begin{enumerate}
%    \item Problem
%    \item Prior work 
%    \item Our solution
%\end{enumerate}
%}

\myparagraph{Problem 1: Security and performance} Prior research~\cite{Castro:2002, DBLP:journals/corr/abs-1803-05069, 10.1145/1658357.1658358} introduced replication protocols that are executed correctly in the presence of Byzantine actors (e.g., malicious adversaries, compromised participant nodes, faulty network links and memory). Unfortunately, while these protocols offer BFT, they give up on performance and scalability rendering them impractical for modern high-end distributed systems~\cite{bftForSkeptics}. 

More importantly, conventional BFT protocols present the following characteristics. First, they require an extra set of $f$ participant machines to tolerate up to $f$ failures. Compared to CFT protocols that operate with $2f+1$ participants, classical BFT protocols present limited scalability as they require at least $3f+1$ participants~\cite{BFT_THEORY}. In addition to this, BFT protocols can be slow as they usually run at least three phases of broadcasts~\cite{Castro:2002, DBLP:journals/corr/abs-1803-05069} and incur high message complexity (e.g., $O(n^2)$). Lastly, BFT protocols are complex: they are hard to understand, let alone be optimised~\cite{10.1145/2658994}. Even intuitive algorithmic improvements to optimize for the common case or recovery can significantly affect other parts of the protocol (e.g., view-change in~\cite{10.1145/1658357.1658358}, normal case adds 2 extra phases in~\cite{DBLP:journals/corr/abs-1803-05069}) .% Consequently, they have seen little adoption in commercial cloud applications due to their limited scalability and performance.

\myparagraph{Proposal} In our work, we target to bridge this gap between robustness (BFT) and performance. Our \projecttitle{} offers the system designers the necessary foundations for building BFT protocols while it optimizes the performance and scalability of the derived protocols. Specifically, we designed and implemented \projecttitle{} to offer the two properties of non-equivocation and transferable authentication that allow us to design and build BFT protocols with the minimum possible participant nodes ($2f+1$), resolving the trade-off of scalability, performance, and BFT at once. 

More precisely, our design relies on the theoretical findings of Clement et al.~\cite{clement2012}. Their work has shown that a translation between any CFT protocol to a BFT protocol always exists iff the following properties are guaranteed:
\begin{itemize}
    \item {\bf{Transferable authentication.}} Potentially malicious nodes cannot impersonate other (honest) nodes. Essentially, any node can verify that a message is signed by the correct sender, even for forwarded messages.
    \item {\bf{Non-equivocation.}} A sender cannot send different messages to different nodes in the same round while it is supposed to send the same message according to the protocol.
\end{itemize}

Our \projecttitle{} materializes these properties on the NIC-level by implementing and integrating an attestation kernel for generating message authentication certificates or attestations and verifying those when messages are received ($\S$~\ref{subsec:tfpga}). That way \projecttitle{} builds and exposes the minimal abstraction required for implementing robust protocols under the Byzantine Fault model with $2f+1$ participant nodes.

%In contrast, our trusted NIC allows heterogeneous nodes to communicate with each other in a secure manner over the untrusted cloud infrastructure. 
\myparagraph{Problem 2: Seamless adoption in heterogeneous DCs} While the classical BFT protocols significantly limit performance, a new line of research has made an attempt to optimize such protocols introducing a new family of hybrid BFT protocols that operate with fewer replicas. Hybrid BFT protocols~\cite{10.1145/3492321.3519568, minBFT, hybster, 10.1145/2168836.2168866, DBLP:journals/corr/LiuLKA16a, trinc} rely on Trusted Execution Environments (TEEs)~\cite{cryptoeprint:2016:086, arm-realm, amd-sev, riscv-multizone, intelTDX}  to prevent equivocation.  Unfortunately, when it comes to their implementation, hybrid protocols have been built on top of specific and CPU-dependant TEEs---in fact, we found that all open-sourced hybrid protocols~\cite{hybster, 10.1145/3492321.3519568, minBFT, DBLP:journals/corr/LiuLKA16a} are specifically built on top Intel SGX~\cite{intel-sgx}.

We argue that the protocols' reliance on such specific TEEs limits generality and adoption in modern heterogeneous cloud infrastructure. For example, to deploy these protocols, a cloud provider must guarantee that there would always be available a required number of machines equipped with specific CPU generation and TEE hardware versions. In cases where there are no machines available, or the TEE has been discontinued (e.g., this is the case with Intel SGX~\cite{sgx_deprecated}), protocol designers are compelled to be able to quickly learn and program another available TEE. However, programming heterogeneous TEEs is a task rather challenging~\cite{10.1145/3460120.3485341}, and it raises questions regarding performance and correctness. Heterogeneous TEEs not only have different programming and performance characteristics but the security properties they offer can greatly vary too~\cite{10.1007/978-3-031-16092-9_7}. As an example of this, Keystone (RISC-V)~\cite{keystone_eurosys} and Intel SGX (x86)~\cite{intel-sgx} have quite different programming APIs. In addition, Intel SGX can only support a very limited Trusted Computing Base (TCB), only up to 256MB, compared to Intel TDX~\cite{intelTDX} and AMD-SEV~\cite{amd-sev}. 
\antonis{may mention arm as well?}
Importantly, commercially available TEEs~\cite{arm-trustzone, intel-sgx, amd-sev, keystone_eurosys, 197162, timber} offer different levels of security (i.e., integrity, freshness, and confidentiality), whereas not all of them come with built-in support for secure bootstrapping and remote attestation~\cite{10.1007/978-3-031-16092-9_7, 7807249, secTEE}. All these characteristics make the widespread adoption of existing BFT protocols impractical.

\myparagraph{Proposal} Our \projecttitle{} attacks this challenge and removes the protocols' dependencies on homogeneous CPUs and TEEs. We offer a unified abstraction, a network library ( ..), that allows designers to implement BFT protocols on various CPU machines. To achieve this, we implement our entire network stack, including the trusted subsystem for non-equivocation and authentication, on the NIC's hardware-level, leveraging the SmartNIC technology~\cite{}. Essentially, our \projecttitle{} shifts the homogeneity from the CPU layer to the NIC layer:  while the host CPUs participating in a protocol can be heterogeneous, our \projecttitle{} is built on top of homogeneous SmartNICs \antonis{seems stricter than what we assume? maybe FPGA-enabled networking?}, exposing a unified abstraction for exchanging and verifying network messages.

We implemented our network stack on top of FPGA-based SmartNICs, specifically Alveo U280~\cite{alveo_smartnics}. While our design could be adapted to be applicable to  SoC-based SmartNICs (e.g., Mellanox BlueField~\cite{bluefield_smartnics}, Alveo SN1000~\cite{alveo_sn1000}), we decided against it due to their performance limitations in 100G era~\cite{211249}.\antonis{weak argument, Bluefield 3 is up to 400Gbits. Let's say: "Our design is applicable to ... and our evaluation shows that hardware implementation is more beneficial.} Instead, given the increasingly wide deployment of such specialized hardware in DCs as an efficient way to offload network processing our architectural design is not hypothetical. \projecttitle{} fits well in recent deployments in commercial clouds. An example of this is Microsoftâ€™s Catapult, where the FPGA, which sits on the data path in front of the network card and applies ``smart'' processing, could be extended (as in $\S$~\ref{subsec:tfpga}) to improve security.

%\dimitra{
%\begin{enumerate}
%    \item Problem
%    \item Prior work 
%    \item Our solution
%\end{enumerate}
%}

%Our proposal for the Trusted NIC architecture design needs to consider three design properties: security, performance and adaptability to heterogeneous host CPUs. Our system is based on a hardware-based trusted subsystem that guarantees the minimum properties that are required to be exposed to system and protocols designers to implement robust distributed protocols~\cite{} while it is designed for heterogeneous (untrusted) cloud infrastructures. 


%Prior research has introduced distributed (replication) protocols~\cite{} that are executed correctly even when some participants or parts of the infrastructure (e.g., network links, memory) are malicious, e.g., \emph{Byzantine}~\cite{}. Unfortunately, these protocols usually present a trade-off between performance and heterogenity. To operate under the Byzantine Fault Model~\cite{},  they need to increase their replication factor~\cite{}. Compared to the deployed protocols that only target crashed, but honest nodes (Crash Fault Model~\cite{}), traditional BFT protocols will add an extra $f$ set of replicas, in total $3f+1$ replicas to operate when up to $f$ nodes behave arbitrarily. As such, while necessary in the modern untrusted cloud infrastructure~\cite{}, these protocols have seen little recognition and adoption due to their performance and scalability limitations~\cite{}. On the other hand, state-of-the-art BFT protocols rely on trusted CPU-dependant hardware components (i.e., Trusted Execution Environments~\cite{}) to cut down on the replication factor using only $2f+1$ nodes. Unfortunately, their reliance on TEEs requires that all participant machines support such hardware which comes with different security guarantees and programmability. As such, these protocols give up on their adaptability to modern heterogeneous data centers. 


\dimitra{
\begin{itemize}
    \item section 2: requirements for robust distributed systems (problem/solution briefly)
    \item section 3: background on the technologies: smartNICs -- translation properties 
    \item section 4: overview --> Discuss the system/fault/threat model here too!
\end{itemize}
}






\section{Background} 

\myparagraph{Translation requirements}


\myparagraph{\trustednic{} compared to other SmartNICs}
SmartNIC devices~\cite{liquidIO_smartnics, u280_smartnics, bluefield_smartnics, broadcom_smartnics, netronome_smartnics, alibaba_smartnics, nitro_smartnics, msr_smartnics} have started to appear in several applications showing their benefits to both applications and network acceleration~\cite{211249, 10.1145/3341302.3342079}. Indeed, major cloud providers~\cite{alibaba_smartnics, nitro_smartnics, msr_smartnics} have launched SmartNICs in their datacenters to offload tasks from the host CPU cores onto dedicated hardware.
These cloud solutions build primarily on top of programmable FPGA-based SmartNICs for accelaration~\cite{msr_smartnics} or SoC-based SmartNICs~\cite{bluefield_smartnics} that integrate off-path arrays of wimpy ARM cores (up to 16) to improve programmability (at the cost of performance).

Our \trustednic{} has been built on top of FPGA-based SmartNICs to optimize for performance. \trustednic{} is an on-path SmartNIC that processes all incoming/outgoing messages directly on the communication path, ensuring that their security properties are met. In contrast, off-path SoC-based SmartNICs, which do not offer any security guarantees out of the box, do not optimize for performance. For example, the host needs to send the outgoing messages to the SoC cores for further processing. As DMA transfers are not supported between host and SoC cores, the re-routing is resolved at the NIC level, incurring extra overheads.

Overall, \trustednic{} is a simpler SmartNIC because it just involves an FPGA on the communication path as in~\cite{msr_smartnics}. %Commercial SmartNICs are complete Systems-on-Chip, involving far more hardware (not
%only arrays of cores but also dedicated ASICs for network
%processing, packet switching, memory controllers, potentially
%storage controllers, etc.). As such, they can do more, but they
%also cost more, require more energy, and are more complex
%to deploy, program, and use.
