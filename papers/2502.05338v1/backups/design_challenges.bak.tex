\section{Overview}
\subsection{Design goals and overview}
\label{sec:requirements}
\pramod{let's keep it consistent high-performance trustworthy distributed systems.}
We propose a trusted NIC architecture to accommodate the requirements for high-performant trustworthy distributed systems for the untrusted cloud infrastructure. Our trusted NIC aims to attack the trade-offs between performance, security, and heterogeneity, targeting the following design goals:

\pramod{i think it's still not crystal clear the three design goals. Nevertheless we should blend the formal verification part of it.}
\begin{itemize}
    \item {\bf{Security}} for guaranteeing the correct execution of a distributed system in the Byzantine (untrusted) cloud infrastructure.
    \item {\bf{Unification and minimalism}} for {\em seamlessly} building distributed systems with strong security in the heterogeneous cloud infrastructure. 
    \item {\bf{High-performance networking}} for efficiently exchanging and authenticating network messages required for BFT guarantees.
\end{itemize}


%\subsection{\projecttitle{} Overview} 

\myparagraph{System overview} To this end, we propose \projecttitle{}, a trusted NIC architecture that fulfills the aforementioned design goals. Figure~\ref{fig:overview} shows the high-level overview of our \projecttitle{} system. A \projecttitle{} system must be attached to each participating machine of the distributed application.% A \projecttitle{} instance is connected to the (host) CPU through the PCIe~\cite{pcie} whereas the distributed \projecttitle{} instances are connected through direct (Ethernet) links or a switch. 


\pramod{i think it would be good to explain the system components in bottom up manner because this is how we structure the design section.}

\pramod{please highlight each componenent in BOLD.}

As shown in Figure~\ref{fig:overview}, \projecttitle{} is comprised of three layers: (1) the \projecttitle{} trusted network library (blue box) that exposes \projecttitle{}'s programming API ($\S$~\ref{sec:t-nic-software}) to the application layer, (2) the \projecttitle{} trusted system network stack (yellow box) that intermediates between the application layer and the \projecttitle{} hardware access ($\S$~\ref{sec:t-nic-network}) and (3) the \projecttitle{} hardware architecture (green box) that implements trusted network operations on top of SmartNIC devices ($\S$~\ref{sec:t-nic-hardware}). 


\pramod{Same here -- bottom up explaination.}
The  \projecttitle{} trusted network library (blue box) exposes a {\em trusted} API that is built on top of one-sided RDMA (reliable) operations. The \projecttitle{} trusted network stack (yellow box) runs in user-space and it is responsible for configuring the \projecttitle{} device (MAC address, IP, etc.), while it runtime if offers kernel-bypass device access for low-latency operations. Lastly, the \projecttitle{} hardware architecture (green box) implements and extends the networking IB/RDMA protocol~\cite{rdma_specification} on top of the FPGA-based SmartNIC devices, Alveo U280~\cite{u280_smartnics}. Critically, it extends the conventional protocol implementation with a minimal hardware security module, the attestation kernel, to efficiently materialize the security properties of the non-equivocation and the transferable authentication.


%The hardware implementation of \projecttitle{} runs the RDMA protocol on top of FPGA-based SmartNIC hardware. We further extend the network protocol kernel with a minimal hardware security module, the attestation kernel which shields and verifies the network traffic across the participant nodes. Specifically, the security module  constructs a secure message format that is verifiable and the network stack module implements the RDMA network protocol. In contrast to the control and configuration paths shown in Figure~\ref{fig:overview}, the message to be sent, is fetched from the application's memory directly to the device through DMA transfers. After being shielded, the message is then forwarded to the network stack module for transmission.  % to preserve the authentication and non-equivocation properties across all network operations.
%On top of our trusted network stack, we implement a networking library, \projectlibrary{}, that exposes the developers to the classical RDMA API enhanced with two security properties, i.e., the transferable authentication and the non-equivocation.


\begin{figure}[t!]
    \centering
    %\includegraphics[width=0.5\textwidth]{figures/trusted-nic-overview.drawio.pdf}
    \includegraphics[width=0.45\textwidth]{figures/system_overview-2.pdf}
    \caption{\projecttitle{} system overview.}
    \label{fig:overview}
\end{figure}


%Our proposal for \projecttitle{} seeks to resolve the tension between security and performance for (distributed) applications in the modern cloud infrastructure, specifically targeting replication protocols\pramod{focus on building trusted distributed systems -- PLEASE DONT MAKE THIS PAPER a REPLICATION protocol paper.}. To this end, our goal is to help system designers to build efficient, fault-tolerant, and robust systems that remain correct even when the untrusted cloud infrastructure fails arbitrarily. 

%\section{Design}
%\dimitra{
%\begin{itemize}
%    \item Intro
%    \item Background + Motivation
%    \item Overview (including sys/data model etc.)
%    \item Design + Implementation (TNIC: hw architecture, TNIC libraries: sw abstraction, Applications: use-cases)
%\end{itemize}
%}


%\subsection{System Model}

%\myparagraph{Model sketch}
%We model the distributed system as a set of {\tt N} nodes each of which is attached to a single \projecttitle{} instance that is loaded into an FPGA-based SmartNIC as Alveo U280~\cite{u280_smartnics}. The nodes communicate by exchanging messages through bi-directional network links that connect their FPGAs. The system is managed and owned by the third-party cloud infrastructure which is untrusted.

\myparagraph{Threat model} We inherit the fault and threat model from the classical BFT~\cite{Castro:2002} and trusted computing domains~\cite{intel-sgx} where the cloud infrastructure, e.g., machines, network, etc. can exhibit (Byzantine) arbitrary behavior. In addition, a \projecttitle{} system is subject to privileged attackers that can gain control over the host CPU, including the privileged software (e.g., the OS, VMM, and device drivers) and the FPGA-based SmartNIC devices (post-manufacturing). Such a powerful adversary can even attempt to re-program the FPGA; however, they cannot compromise the cryptographic primitives as in~\cite{levin2009trinc, minBFT, Castro:2002}. 

We assume that the physical package, supply chain, and manufacturer of the SmartNICs are trusted~\cite{10.1145/3503222.3507733, 10.1145/2168836.2168866}. The \projecttitle{} implementation (bitstream) is synthesized by a trusted IP vendor with a trusted tool flow that runs in a secure environment (for covert channels resilience). The protocol designers source the \projecttitle{} from trusted IP vendors.

%We do not consider denial-of-service (DoS) attacks; the cloud provider has physical control of the hardware and can simply unpower it. Nevertheless such attacks affect availability and not the correctness.


%( target a strong adversary that can gain full control over the cloud infrastructure including any privileged CPU software (e.g., the OS, VMM, and device drivers) and the FPGA-based SmartNIC devices (post-manufacturing). Such an adversary can also have full access to the infrastructure (e.g., shutdown the servers, or even try to re-program the FPGAs) although, as in ~\cite{levin2009trinc, minBFT, Castro:2002} they cannot compromise the cryptographic primitives. As in~\cite{10.1145/3503222.3507733, 10.1145/2168836.2168866}, we further assume that the physical package, supply chain, and manufacturer of the FPGAs are trusted. The bitstreams are implemented and synthesized correctly whereas any secret data are distributed securely using remote attestation ($\S$~\ref{subsec:nic_attest_kernel}). The tool flow used to generate the accelerator is trusted and run in a secure environment for covert channels resilience. 

%The host CPU is considered untrusted and we do not rely on any CPU TEE. Specifically, while we use the application code to transfer data, we assume the host CPU is untrusted, i.e., can exhibit Byzantine behavior~\cite{Lamport:1982}, as well as the local host memory, storage, etc. Similarly, we make no assumptions about the network infrastructure, e.g., adversaries can drop, re-order, double-send, read, and manipulate the messages in any way. 

%We do not consider denial-of-service (DoS) attacks; the cloud provider has physical control of the hardware and can simply unpower it. Nevertheless such attacks affect availability and not the correctness.


%The CSP is incentivized to prevent DoS attacks due to the revenue loss when the FPGA instance is unavailable. %We do not consider attacks against the CSP, as the Shell already protects against malicious FPGA users. 


%We trust that the RDMA stack, along with the attestation kernel, are implemented and synthesized correctly, whereas the bitstream and the keys are distributed and flushed to the FPGAs securely using remote attestation.





%The \projecttitle{} architecture implements a  network stack, similarly to~\cite{erpc, rdma-scale}, but focuses on security leveraging SmartNICs---specifically, FPGA-based NICs~\cite{intel_smartnics, alveo_smartnics}. 




\if 0
\begin{figure}[t!]
    \centering
    \includegraphics[width=.45\textwidth]{figures/trusted-nic-attestation_kernel.drawio.pdf}
    \caption{\trustedfpga{} attestation kernel overview (transmission path).}
    \label{fig:attestation_kernel}
\end{figure}    
\fi

\pramod{we should not restrict ourselves to replication protocols.}

\pramod{Minimalism and verifiabile hardware interface should be a separate challenge.}

\subsection{Design Challenges}
\projecttitle{} aims to offer high-performant and unified trusted networking that can apply to a wide range of distributed systems. As such, we need to overcome the following challenges:

\myparagraph{Challenge \#1: Performance vs. security} Distributed systems in the third-party cloud infrastructure need to be fast, scalable, and trustworthy. While performance is critical in all of them, with TEEs themselves introducing significant performance limitations, the problem is even more exacerbated in fault-tolerant systems. Building replication protocols for distributed systems under the Byzantine fault model has always been a complex endeavor with performance, scalability, and engineering challenges~\cite{bftForSkeptics}. Even well-studied protocols, e.g., PBFT~\cite{Castro:2002}, are limited in scalability (it requires at least $f$ nodes w.r.t to its CFT counterpart), incur high latency (it runs three all-to-all broadcasting phases with $O(n^2)$ message complexity) and are hard to verify and optimize~\cite{10.1145/2658994}.


\myparagraph{Solution} Our \projecttitle{} bridges the gap between performance, scalability, and security. We implement a minimal TCB, the attestation kernel ($\S$~\ref{subsec:nic_attest_kernel}), that materializes all the necessary properties for transforming systems for Byzantine settings. We build the attestation kernel as part of the state-of-the-art NIC hardware for high performance ($\S$~\ref{subsec:roce_protocol_kernel}). The semantics we provide are powerful and generic too; we can use \projecttitle{} to transform a wide range of distributed systems for Byzantine settings (we show four use-cases in $\S$~\ref{sec:use_cases})  and even for transforming CFT replication protocols to BFT ones with the same number of machines as the CFT protocol. As such, our \projecttitle{} improves scalability compared to traditional BFT systems. We show this generic {\em recipe} of such a transformation using \projecttitle{} in $\S$~\ref{subsec:transformation}.

%protocol, to help system designers building more robust protocols. To achieve this, we build an extended implementation of the classical, widely-adopted, RDMA network stack~\cite{rdma} on programmable hardware, i.e., FPGA-based SmartNICs~\cite{u280_smartnics}, offloading on this hardware the necessary required security processing and mechanisms. 

%\myparagraph{Challenge \#1: Security vs high-performance} The distributed applications hosted in the third-party cloud infrastructure need to be highly available. As such, cloud services and applications build on top of replication protocols that offer fault tolerance and can remain available when failures occur~\cite{Jimenez-Peris2001}. The vast majority of such deployed protocols in the cloud operate under the Crash Fault Tolerant model (CFT) where the machines can {\em only} fail by crashing or omitting some steps. However, the CFT model is inadequate in modern cloud infrastructure as it has been observed that the machines or other parts of the infrastructure can fail arbitrarily (i.e., exhibit Byzantine behavior~\cite{Lamport:1982}) due to malicious adversaries, compromised OS/hypervisor in machines, faulty network links and compromised memory and storage medium~\cite{Gunawi_bugs-in-the-cloud, ciad, fast-08-corruption, security-one-sided-communication, accountable-cloud}. Consequently, the current replication protocols target a quite limited fault model which does not match modern's applications security needs that are hosted in the (untrusted) cloud.

%\myparagraph{Solution} Our \projecttitle{} overcomes this limitation by offering  trusted and easily adoptable network operations, which are at the core of any distributed protocol, to help system designers building more robust protocols. To achieve this, we build an extended implementation of the classical, widely-adopted, RDMA network stack~\cite{rdma} on programmable hardware, i.e., FPGA-based SmartNICs~\cite{u280_smartnics}, offloading on this hardware the necessary required security processing and mechanisms. 

%More importantly, conventional BFT protocols present the following characteristics. First, they require an extra set of $f$ participant machines to tolerate up to $f$ failures. Compared to CFT protocols that operate with $2f+1$ participants, classical BFT protocols present limited scalability as they require at least $3f+1$ participants~\cite{BFT_THEORY}. In addition to this, BFT protocols can be slow as they usually run at least three phases of broadcasts~\cite{Castro:2002, DBLP:journals/corr/abs-1803-05069} and incur high message complexity (e.g., $O(n^2)$). Lastly, BFT protocols are complex: they are hard to understand, let alone be optimised~\cite{10.1145/2658994}. Even intuitive algorithmic improvements to optimize for the common case or recovery can significantly affect other parts of the protocol (e.g., view-change in~\cite{10.1145/1658357.1658358}, normal case adds 2 extra phases in~\cite{DBLP:journals/corr/abs-1803-05069}) .% Consequently, they have seen little adoption in commercial cloud applications due to their limited scalability and performance.

\myparagraph{Challenge \#2: Minimalism vs TEEs TCBs} 

\myparagraph{Challenge \#2: Safety vs Heterogeneity} Prior BFT systems with trusted hardware were implemented on top of CPU-specific TEEs. In fact, we found that almost all of the open-source system implementations~\cite{hybster, 10.1145/3492321.3519568, minBFT, DBLP:journals/corr/LiuLKA16a}  require {\em homogeneous} {\tt x86} machines with SGX extensions of a specific version; a trusted hardware that for long had been under fire for its major security vulnerabilities~\cite{intel_sgx_vulnerabilities1, intel_sgx_vulnerabilities2, intel_sgx_vulnerabilities3, intel_sgx_vulnerabilities4, intel_sgx_vulnerabilities5}. Safety in BFT systems has always been a concern---e.g., Zyzzyva~\cite{unsafe_Zyzzyva} has proven to be unsafe almost ten years after its original publication---let alone when distributed systems rely on unstable heterogeneous TEEs.  

\myparagraph{Solution} To attack this challenge, \projecttitle{}'s key idea is minimalism. We design a minimalistic fully verifiable (TCB) attestation kernel at NIC-hardware level that exposes a unified API to system designers. As such, we offer BFT while removing the distributed system's dependencies on the host CPU, rendering our approach highly favorable in the heterogeneous cloud infrastructure. In addition, our minimalistic hardware-assisted TCB allows us to fully verify the safety and security properties of our \projecttitle{} from its initialization and remote attestation process to its normal operation ($\S$~\ref{subsec::formal_verification_remote_attestation}).

%\myparagraph{Challenge \#2: Performance and scalability} Researchers~\cite{Castro:2002, DBLP:journals/corr/abs-1803-05069, 10.1145/1658357.1658358} presented a range of robust replication protocols that remain correct when arbitrary failures occur targeting the BFT model~\cite{Lamport:1982}. Unfortunately, these BFT protocols have recognised little adoption because they cannot meet the performance requirements of deployed applications~\cite{bft-time-is-now}. In addition, the vast majority~\cite{Castro:2002, DBLP:journals/corr/abs-1803-05069} introduces resources overheads and limits scalability because it requires at least $3f+1$ machines to tolerate up to $f$ faults. That is, at least more $f$ machines compared to currently deployed CFT protocols. Consequently, BFT protocols are not well suited for performance in modern high-end distributed systems~\cite{bftForSkeptics}.

%\myparagraph{Solution} We bridge the gap between performance, scalability and robustness. Our \projecttitle{} offers robustness by materialising the necessary foundations for building BFT protocols~\cite{clement2012} in programmable, yet fast, hardware, while it also improves performance and scalability by limiting the number of required participant machines to the minimum, i.e., $2f+1$. More specifically, \projecttitle{} implements the theoretical foundations of Clement et. al~\cite{clement2012} to translate a CFT protocol to a BFT protocol without having to increase the CFT protocol's replication degree. We explain this mechanism in $\S$~\ref{sec:background}.  %Their work has shown that a translation between any CFT protocol to a BFT protocol {\em always} exists if the security properties of the transferable authentication and the non-equivocation are guaranteed. We discuss the properties and the translation mechanism in$\S$~\ref{}. 

%\begin{itemize}
 %   \item {\bf{Transferable authentication.}} Potentially malicious nodes cannot impersonate other (honest) nodes. Essentially, any node can verify that a message is signed by the correct sender, even for forwarded messages.
 %   \item {\bf{Non-equivocation.}} A sender cannot send different messages to different nodes in the same round while it is supposed to send the same message according to the protocol.
%\end{itemize}

%we designed and implemented \projecttitle{} to offer the two properties of non-equivocation and transferable authentication that allow us to design and build BFT protocols with the minimum possible participant nodes ($2f+1$), resolving the trade-off of scalability, performance, and BFT at once. 

%More precisely, our design relies on the theoretical findings 

%Our \projecttitle{} materializes these properties on the NIC-level by implementing and integrating an attestation kernel for generating message authentication certificates or attestations and verifying those when messages are received ($\S$~\ref{subsec:tfpga}). That way \projecttitle{} builds and exposes the minimal abstraction required for implementing robust protocols under the Byzantine Fault model with $2f+1$ participant nodes.


%\myparagraph{Challenge \#3: Adaptability} Due to the traditional BFT protocols limitations, a new line of research has attempted to optimize them~\cite{10.1145/3492321.3519568, minBFT, hybster, 10.1145/2168836.2168866, DBLP:journals/corr/LiuLKA16a, trinc} making use of trusted hardware, precisely, Trusted Execution Environments (TEEs)~\cite{cryptoeprint:2016:086, arm-realm, amd-sev, riscv-multizone, intelTDX}. Unfortunately, the safety requirements of these optimized protocols highly depend on very specific and CPU-dependant TEEs. Consequently, in addition to their limit adaptability and generality, these protocols' correct implementation and deployment requires that there will {\em always} be available the required number of machines equipped with specific CPU generation and TEE hardware versions. In any other case, system designers are compelled to be able to quickly learn and program any another available TEE. This complicates the widespread adoption of such protocols because the task of programming heterogeneous TEEs as rather challenging~\cite{10.1145/3460120.3485341} as error prone; various TEEs present different programming models and security properties~\cite{10.1007/978-3-031-16092-9_7}. %To sum up, this heterogeneity complicates the widespread adoption of such protocols.
%it raises questions regarding performance and correctness. 

%---in fact, we found that all such open-sourced protocols~\cite{hybster, 10.1145/3492321.3519568, minBFT, DBLP:journals/corr/LiuLKA16a} are built on top Intel SGX~\cite{intel-sgx}.

%We argue that the protocols' reliance on such specific TEEs limits generality and adoption in modern heterogeneous cloud infrastructure. For example, to deploy these protocols, a cloud provider must guarantee that there would always be available a required number of machines equipped with specific CPU generation and TEE hardware versions. In cases where there are no machines available, or the TEE has been discontinued (e.g., this is the case with Intel SGX~\cite{sgx_deprecated}), protocol designers are compelled to be able to quickly learn and program another available TEE. However, programming heterogeneous TEEs is a task rather challenging~\cite{10.1145/3460120.3485341}, and it raises questions regarding performance and correctness. Heterogeneous TEEs not only have different programming and performance characteristics but the security properties they offer can greatly vary too~\cite{10.1007/978-3-031-16092-9_7}. As an example of this, Keystone (RISC-V)~\cite{keystone_eurosys} and Intel SGX (x86)~\cite{intel-sgx} have quite different programming APIs. In addition, Intel SGX can only support a very limited Trusted Computing Base (TCB), only up to 256MB, compared to Intel TDX~\cite{intelTDX} and AMD-SEV~\cite{amd-sev}. 
%\antonis{may mention arm as well?}
%Importantly, commercially available TEEs~\cite{arm-trustzone, intel-sgx, amd-sev, keystone_eurosys, 197162, timber} offer different levels of security (i.e., integrity, freshness, and confidentiality), whereas not all of them come with built-in support for secure bootstrapping and remote attestation~\cite{10.1007/978-3-031-16092-9_7, 7807249, secTEE}. All these characteristics make the widespread adoption of existing BFT protocols impractical.


%\myparagraph{Solution} We attack this challenge by removing any dependencies on CPU-based TEEs and unshackling the designers from having to continuously learn and program various TEEs. Our \projecttitle{} makes use of programmable hardware, i.e., FPGAs, to implement a trusted network stack offloading any security-related processing in the NIC hardware ($\S$~\ref{subsec:tfpga}) and to offer a unified abstraction (network library) to the system designers ($\S$~\ref{sec:net-lib}). While our \projecttitle{} shifts the homogeneity from the CPU layer to the FPGA-based NIC layer, our architectural design is not hypothetical; \projecttitle{} fits well in recent deployments in commercial clouds, e.g. Microsoft's Catapult design~\cite{msr_smartnics, 211249}. 

%machines. To achieve this, we implement our entire network stack, including the trusted subsystem for non-equivocation and authentication, on the NIC's hardware-level, leveraging the SmartNIC technology~\cite{}. Essentially, our \projecttitle{} shifts the homogeneity from the CPU layer to the NIC layer:  while the host CPUs participating in a protocol can be heterogeneous, our \projecttitle{} is built on top of homogeneous SmartNICs \antonis{seems stricter than what we assume? maybe FPGA-enabled networking?}, exposing a unified abstraction for exchanging and verifying network messages.

%We implemented our network stack on top of FPGA-based SmartNICs, specifically Alveo U280~\cite{alveo_smartnics}. While our design could be adapted to be applicable to  SoC-based SmartNICs (e.g., Mellanox BlueField~\cite{bluefield_smartnics}, Alveo SN1000~\cite{alveo_sn1000}), we decided against it due to their performance limitations in 100G era~\cite{211249}.\antonis{weak argument, Bluefield 3 is up to 400Gbits. Let's say: "Our design is applicable to ... and our evaluation shows that hardware implementation is more beneficial.} Instead, given the increasingly wide deployment of such specialized hardware in DCs as an efficient way to offload network processing our architectural design is not hypothetical. \projecttitle{} fits well in recent deployments in commercial clouds. An example of this is Microsoft’s Catapult, where the FPGA, which sits on the data path in front of the network card and applies ``smart'' processing, could be extended (as in $\S$~\ref{subsec:tfpga}) to improve security.

%Our system achieves these goals by implementing a minimal hardware-based authentication subsystem, the \emph{attestation kernel}, that guarantees the necessary security properties required for BFT. The attestation kernel generates and verifies authentication certificates for the network messages to ensure two core security properties: the non-equivocation and the (transferable) authentication properties. These two properties have been proven to be necessary and sufficient for decreasing the replication factor for BFT protocols~\cite{clement2012}. Further, we carefully implemented \projecttitle{} to optimize for performance and scalability by integrating the attestation kernel ``on path'' at the NIC-level. That way, our system offers security without introducing unnecessary overheads both in performance and adoption: data is processed \emph{on their way} to the network whereas the protocols do not have to rely on a specific TEE in the host CPU layer. 


\if 0

\subsection{Design Challenges}
\dimitra{here}
\myparagraph{Challenge \#1: Security} The distributed applications hosted in the third-party cloud infrastructure need to be highly available. As such, cloud services and applications build on top of replication protocols that offer fault tolerance and, importantly, can remain available when failures occur~\cite{Jimenez-Peris2001}. The vast majority of such deployed protocols in the cloud operate under the Crash Fault Tolerant model (CFT) where the machines can {\em only} fail by crashing or omitting some steps. However, the CFT model is inadequate in modern cloud infrastructure as it has been observed that the machines or other parts of the infrastructure can fail arbitrarily (i.e., exhibit Byzantine behavior~\cite{Lamport:1982}) due to malicious adversaries, compromised OS/hypervisor in machines, faulty network links and compromised memory and storage medium~\cite{Gunawi_bugs-in-the-cloud, ciad, fast-08-corruption, security-one-sided-communication, accountable-cloud}. Consequently, the current replication protocols target a quite limited fault model which does not match modern's applications security needs that are hosted in the (untrusted) cloud.

\myparagraph{Solution} Our \projecttitle{} overcomes this limitation by offering  trusted and easily adoptable network operations, which are at the core of any distributed protocol, to help system designers building more robust protocols. To achieve this, we build an extended implementation of the classical, widely-adopted, RDMA network stack~\cite{rdma} on programmable hardware, i.e., FPGA-based SmartNICs~\cite{u280_smartnics}, offloading on this hardware the necessary required security processing and mechanisms. 

%More importantly, conventional BFT protocols present the following characteristics. First, they require an extra set of $f$ participant machines to tolerate up to $f$ failures. Compared to CFT protocols that operate with $2f+1$ participants, classical BFT protocols present limited scalability as they require at least $3f+1$ participants~\cite{BFT_THEORY}. In addition to this, BFT protocols can be slow as they usually run at least three phases of broadcasts~\cite{Castro:2002, DBLP:journals/corr/abs-1803-05069} and incur high message complexity (e.g., $O(n^2)$). Lastly, BFT protocols are complex: they are hard to understand, let alone be optimised~\cite{10.1145/2658994}. Even intuitive algorithmic improvements to optimize for the common case or recovery can significantly affect other parts of the protocol (e.g., view-change in~\cite{10.1145/1658357.1658358}, normal case adds 2 extra phases in~\cite{DBLP:journals/corr/abs-1803-05069}) .% Consequently, they have seen little adoption in commercial cloud applications due to their limited scalability and performance.


\myparagraph{Challenge \#2: Performance and scalability} Researchers~\cite{Castro:2002, DBLP:journals/corr/abs-1803-05069, 10.1145/1658357.1658358} presented a range of robust replication protocols that remain correct when arbitrary failures occur targeting the BFT model~\cite{Lamport:1982}. Unfortunately, these BFT protocols have recognised little adoption because they cannot meet the performance requirements of deployed applications~\cite{bft-time-is-now}. In addition, the vast majority~\cite{Castro:2002, DBLP:journals/corr/abs-1803-05069} introduces resources overheads and limits scalability because it requires at least $3f+1$ machines to tolerate up to $f$ faults. That is, at least more $f$ machines compared to currently deployed CFT protocols. Consequently, BFT protocols are not well suited for performance in modern high-end distributed systems~\cite{bftForSkeptics}.

\myparagraph{Solution} We bridge the gap between performance, scalability and robustness. Our \projecttitle{} offers robustness by materialising the necessary foundations for building BFT protocols~\cite{clement2012} in programmable, yet fast, hardware, while it also improves performance and scalability by limiting the number of required participant machines to the minimum, i.e., $2f+1$. More specifically, \projecttitle{} implements the theoretical foundations of Clement et. al~\cite{clement2012} to translate a CFT protocol to a BFT protocol without having to increase the CFT protocol's replication degree. We explain this mechanism in $\S$~\ref{sec:background}.  %Their work has shown that a translation between any CFT protocol to a BFT protocol {\em always} exists if the security properties of the transferable authentication and the non-equivocation are guaranteed. We discuss the properties and the translation mechanism in$\S$~\ref{}. 

%\begin{itemize}
 %   \item {\bf{Transferable authentication.}} Potentially malicious nodes cannot impersonate other (honest) nodes. Essentially, any node can verify that a message is signed by the correct sender, even for forwarded messages.
 %   \item {\bf{Non-equivocation.}} A sender cannot send different messages to different nodes in the same round while it is supposed to send the same message according to the protocol.
%\end{itemize}

%we designed and implemented \projecttitle{} to offer the two properties of non-equivocation and transferable authentication that allow us to design and build BFT protocols with the minimum possible participant nodes ($2f+1$), resolving the trade-off of scalability, performance, and BFT at once. 

%More precisely, our design relies on the theoretical findings 

%Our \projecttitle{} materializes these properties on the NIC-level by implementing and integrating an attestation kernel for generating message authentication certificates or attestations and verifying those when messages are received ($\S$~\ref{subsec:tfpga}). That way \projecttitle{} builds and exposes the minimal abstraction required for implementing robust protocols under the Byzantine Fault model with $2f+1$ participant nodes.


\myparagraph{Challenge \#3: Adaptability} Due to the traditional BFT protocols limitations, a new line of research has attempted to optimize them~\cite{10.1145/3492321.3519568, minBFT, hybster, 10.1145/2168836.2168866, DBLP:journals/corr/LiuLKA16a, trinc} making use of trusted hardware, precisely, Trusted Execution Environments (TEEs)~\cite{cryptoeprint:2016:086, arm-realm, amd-sev, riscv-multizone, intelTDX}. Unfortunately, the safety requirements of these optimized protocols highly depend on very specific and CPU-dependant TEEs. Consequently, in addition to their limit adaptability and generality, these protocols' correct implementation and deployment requires that there will {\em always} be available the required number of machines equipped with specific CPU generation and TEE hardware versions. In any other case, system designers are compelled to be able to quickly learn and program any another available TEE. This complicates the widespread adoption of such protocols because the task of programming heterogeneous TEEs as rather challenging~\cite{10.1145/3460120.3485341} as error prone; various TEEs present different programming models and security properties~\cite{10.1007/978-3-031-16092-9_7}. %To sum up, this heterogeneity complicates the widespread adoption of such protocols.
%it raises questions regarding performance and correctness. 

%---in fact, we found that all such open-sourced protocols~\cite{hybster, 10.1145/3492321.3519568, minBFT, DBLP:journals/corr/LiuLKA16a} are built on top Intel SGX~\cite{intel-sgx}.

%We argue that the protocols' reliance on such specific TEEs limits generality and adoption in modern heterogeneous cloud infrastructure. For example, to deploy these protocols, a cloud provider must guarantee that there would always be available a required number of machines equipped with specific CPU generation and TEE hardware versions. In cases where there are no machines available, or the TEE has been discontinued (e.g., this is the case with Intel SGX~\cite{sgx_deprecated}), protocol designers are compelled to be able to quickly learn and program another available TEE. However, programming heterogeneous TEEs is a task rather challenging~\cite{10.1145/3460120.3485341}, and it raises questions regarding performance and correctness. Heterogeneous TEEs not only have different programming and performance characteristics but the security properties they offer can greatly vary too~\cite{10.1007/978-3-031-16092-9_7}. As an example of this, Keystone (RISC-V)~\cite{keystone_eurosys} and Intel SGX (x86)~\cite{intel-sgx} have quite different programming APIs. In addition, Intel SGX can only support a very limited Trusted Computing Base (TCB), only up to 256MB, compared to Intel TDX~\cite{intelTDX} and AMD-SEV~\cite{amd-sev}. 
%\antonis{may mention arm as well?}
%Importantly, commercially available TEEs~\cite{arm-trustzone, intel-sgx, amd-sev, keystone_eurosys, 197162, timber} offer different levels of security (i.e., integrity, freshness, and confidentiality), whereas not all of them come with built-in support for secure bootstrapping and remote attestation~\cite{10.1007/978-3-031-16092-9_7, 7807249, secTEE}. All these characteristics make the widespread adoption of existing BFT protocols impractical.


\myparagraph{Solution} We attack this challenge by removing any dependencies on CPU-based TEEs and unshackling the designers from having to continuously learn and program various TEEs. Our \projecttitle{} makes use of programmable hardware, i.e., FPGAs, to implement a trusted network stack offloading any security-related processing in the NIC hardware ($\S$~\ref{subsec:tfpga}) and to offer a unified abstraction (network library) to the system designers ($\S$~\ref{sec:net-lib}). While our \projecttitle{} shifts the homogeneity from the CPU layer to the FPGA-based NIC layer, our architectural design is not hypothetical; \projecttitle{} fits well in recent deployments in commercial clouds, e.g. Microsoft's Catapult design~\cite{msr_smartnics, 211249}. 

%machines. To achieve this, we implement our entire network stack, including the trusted subsystem for non-equivocation and authentication, on the NIC's hardware-level, leveraging the SmartNIC technology~\cite{}. Essentially, our \projecttitle{} shifts the homogeneity from the CPU layer to the NIC layer:  while the host CPUs participating in a protocol can be heterogeneous, our \projecttitle{} is built on top of homogeneous SmartNICs \antonis{seems stricter than what we assume? maybe FPGA-enabled networking?}, exposing a unified abstraction for exchanging and verifying network messages.

%We implemented our network stack on top of FPGA-based SmartNICs, specifically Alveo U280~\cite{alveo_smartnics}. While our design could be adapted to be applicable to  SoC-based SmartNICs (e.g., Mellanox BlueField~\cite{bluefield_smartnics}, Alveo SN1000~\cite{alveo_sn1000}), we decided against it due to their performance limitations in 100G era~\cite{211249}.\antonis{weak argument, Bluefield 3 is up to 400Gbits. Let's say: "Our design is applicable to ... and our evaluation shows that hardware implementation is more beneficial.} Instead, given the increasingly wide deployment of such specialized hardware in DCs as an efficient way to offload network processing our architectural design is not hypothetical. \projecttitle{} fits well in recent deployments in commercial clouds. An example of this is Microsoft’s Catapult, where the FPGA, which sits on the data path in front of the network card and applies ``smart'' processing, could be extended (as in $\S$~\ref{subsec:tfpga}) to improve security.

%Our system achieves these goals by implementing a minimal hardware-based authentication subsystem, the \emph{attestation kernel}, that guarantees the necessary security properties required for BFT. The attestation kernel generates and verifies authentication certificates for the network messages to ensure two core security properties: the non-equivocation and the (transferable) authentication properties. These two properties have been proven to be necessary and sufficient for decreasing the replication factor for BFT protocols~\cite{clement2012}. Further, we carefully implemented \projecttitle{} to optimize for performance and scalability by integrating the attestation kernel ``on path'' at the NIC-level. That way, our system offers security without introducing unnecessary overheads both in performance and adoption: data is processed \emph{on their way} to the network whereas the protocols do not have to rely on a specific TEE in the host CPU layer. 
\fi