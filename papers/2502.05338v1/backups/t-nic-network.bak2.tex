\section{Trusted Network Stack}
\label{sec:t-nic-network}


\begin{figure}[t!]
    \centering
    %\includegraphics[width=0.5\textwidth]{figures/trusted-nic-overview.drawio.pdf}
    \includegraphics[width=0.45\textwidth]{figures/network_stack_system_design.drawio.pdf}
    \caption{\projecttitle{} system network stack.}
    \label{fig:network_stack_design}
\end{figure}


We build a software \projecttitle{} system network stack that operates as the {\em middle layer} between the \projecttitle{} programming APIs (see $\S$~\ref{sec:net-lib}) and the hardware implementation of \projecttitle{}. Figure~\ref{fig:network_stack_design} shows the overview of \projecttitle{}'s system network stack design that is comprised of two core components: {\em (1)} the \projecttitle{} driver and the mapped REGs pages that are responsible for configuring the device at the initialization and host---device communication respectively, and {\em (2)} the RDMA OS abstractions that execute networking operations. 

\subsection{\projecttitle{} Driver and Mapped REGs Pages} 

%\atsushi{What's the meaning of "the appropriate configurations"? Also, explaining a device driver might be a bit boring}
The \projecttitle{} driver is invoked at the initialization of the device, prior to the remote attestation protocol ($\S$~\ref{subsec:nic_controller}), to configure the \projecttitle{} hardware with its static configuration (the device MAC address, the device QSFP port, and the network IP used by the application). 
%To achieve that, it loads a kernel module that sets up the device and provides its static configuration, specifically the device MAC address, the device QSFP port, and a (configurable) network IP that is used by the application code to exchange messages. 

Additionally, the driver enables kernel-bypass networking---similarly to the original (user-space) RDMA protocol---by mapping the \projecttitle{} device to a user-space addresses range, the Mapped REGs pages. In our \projecttitle{}, we reserve one page at the page granularity of our system for each connected device that are represented as pseudo-devices in {\tt /dev/fpga<ID>}. Read and write access to the pseudo-device is equal to accessing the control and status registers of the FPGA. As such, the applications can directly intact with the control path of the \projecttitle{} hardware at low latency while bypassing the operating system. 

%\atsushi{The implementation optimization shown here can be simplified or omitted (not so bad, but may be less prioritized)}
%The driver additionally provides an implementation of the {\tt mmap} system call as part of its {\tt file\_operations structure}~\cite{device_drivers} to reserve and map the device to a user-space addresses range. We refer to these mappings with the term Mapped REGs pages. In our \projecttitle{}, we reserve one page of 4KB that equals the page granularity of our system for each connected device and are represented as pseudo-devices in {\tt /dev/fpga<ID>}. Specifically, accessing and writing to the pseudo-device is equal to accessing the control and status registers of the FPGA. As such, the software system stack, and consequently the applications, can directly intact with the control path of the \projecttitle{} hardware at low latency while bypassing the operating system. 

%\atsushi{Very implementation-specific, so we can simplify them; not explain how we implement, but what the device driver does.}
%These device mappings also expose the control registers for the transmission/reception queues. In our current \projecttitle{} configuration, the depth is set to 22. If the queues are full, the ibv library temporarily blocks and continuously monitors the queues to find empty slots.  The application passes down to the device any requests and their related parameters, whereas the ibv library encodes and stores to the pages the following data request opcode, queue id and other identifiers, remote and local offset, and data len. %The \projecttitle{} hardware network stack decodes the metadata and parameters into the  update the pages entries about the completed requests and other statistics.

\subsection{RDMA OS Abstractions} 
%\atsushi{Overall: can we highlight what functionalities the OS abstractions enable rather than how they are implemented?}
The RDMA OS abstractions are a user-space library that enables the networking operations in the \projecttitle{} hardware bypassing the kernel for performance. As shown in Figure~\ref{fig:network_stack_design}, invocations of the \projecttitle{} programming API calls into the RDMA OS library are comprised of two parts: \textit{the network (RDMA) library} (colored in purple) that implements the software part of the RDMA protocol, and \textit{the OS library} (colored in red) that schedules the \projecttitle{} requests.


\myparagraph{Network (RDMA) library}  The network (RDMA) library includes all the logic and data (e.g., the queues per connection, the local and remote memory addresses, the RDMA keys that denote the memory access permissions) required for implementing the RDMA protocol. It executes the application's networking operations by posting the requests to the hardware. More specifically, it creates an internal representation of the request and the associated data and metadata (i.e., request opcode, remote IP, local address, remote base address, offsets, data length, etc.), it encodes all related parameters and, finally, it writes them in specific offsets in the REGs pages to update the control registers of \projecttitle{} hardware. In case there are no empty transmission queues, the ibv library {\tt yields} until there is an empty slot in the queue.

%These device mappings also expose the control registers for the transmission/reception queues. In our current \projecttitle{} configuration, the depth is set to 22. If the queues are full, the ibv library temporarily blocks and continuously monitors the queues to find empty slots.  The application passes down to the device any requests and their related parameters, whereas the ibv library encodes and stores to the pages the following data request opcode, queue id and other identifiers, remote and local offset, and data len.

%The \projecttitle{} decodes the inputs to execute the appropriate RDMA protocol. The ibv library manages the ibv structs manage that store all the connections' internal metadata, such as queues per connection, local and remote memory addresses, rkeys, etc. 

As shown in Figure~\ref{fig:network_stack_design}, the transmission and reception of requests and responses mandate the allocation of application network buffers. We adopt similar memory management as in widely-used user-space networking libraries~\cite{erpc, dpdk, rdma}. Importantly, in \projecttitle{} application network buffers need to be mapped to a specific \projecttitle{}-memory, called the ibv memory. The ibv memory area is allocated at the connection creation in the {\tt huge page} area by the application through the ibv library. It resides within the application's address space with full read/write permissions, it is eligible for DMA transfers and it is registered to the \projecttitle{} for remote reads and writes without involving the CPU. 

%Applications should wisely use this memory to eliminate unnecessary data copies from applications' buffers to network buffers. Specifically, while applying \projecttitle{} into four systems and use-cases ($\S$~\ref{sec:use_cases}), we allocated all application buffers from the ibv memory, hence the network buffers (registered RDMA memory). In \projecttitle{}, the ibv memory buffers are allocated in the {\tt huge page} memory area (2MB) and available for DMA transfers from and to the \projecttitle{} hardware.  


\myparagraph{OS library} The \projecttitle{}-OS library is a lightweight user-space library that is responsible for scheduling the \projecttitle{} requests and ensuring isolated access to the mapped REG pages, hence the device. To improve performance, the \projecttitle{} data path eliminates unnecessary data copies throughout the network stack. In fact, the data to be sent (shown in blue lines in Figure~\ref{fig:network_stack_design} is directly fetched from the hardware through DMA transfers. 

The OS library creates a \projecttitle{}-process object to represent each \projecttitle{} device.  This \projecttitle{}-process in \projecttitle{} is not a separate scheduling entity (i.e., a thread as in classical OSes). In contrast, it is an object handle, exposed to the ibv library but managed by the \projecttitle{}-OS library that manages and acquires locks on the respective REG pages to ensure isolated access to the \projecttitle{} hardware.




%never copied across \projecttitle{} system stack; instead, the \projecttitle{}-process only sets the necessary encoded metadata to the REGs pages and directly posts the command in the first available command slot. As discussed, the \projecttitle{} hardware will later fetch the data from the ibv memory through a DMA transfer (blue lines in Figure~\ref{fig:network_stack_design}).

%Upon a transmission the {\tt ibv library} which is creates an internal representation of the request and the associated data and metadata (i.e. request opcode, remote IP, local address, remote base address and offset, data lenght, etc.). Then the request is executed as part of a (virtual) process that is mapped to a specific id ({\tt pid}) similar to the classical OS process abstraction. When the scheduler marks the process as eligible for execution, the process writes the request's data and metadata in its associated REG page. To improve for performance the actual data to be sent are not copied within all the system stack, instead, the process writes the local address and the length of the data (in bytes) to be sent and posts the command. The {\dimitra{;....}}
