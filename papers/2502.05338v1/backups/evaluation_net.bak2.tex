\subsection{Software Evaluation: \projecttitle{} Network Stack}\label{subsec:net_lib}

\myparagraph{Baselines} We evaluate the performance of our \projecttitle{} where we seek to show {\em{(i)}} the effectiveness of our decision to shift the network stack from host CPUs to modern data accelerators and {\em{(ii)}} the overheads that our system incurs due to materializing the requirements for CFT systems transformation (discussed in $\S~\ref{sec:requirements-ds}$). As such, we evaluate and compare \projecttitle{} across four other network stacks implemented on software or hardware with different security properties. %Specifically, we use the acronym D-I/O to refer to a Direct I/O network stack that bypasses the kernel stack (for performance). The acronyms D-I/O w/ A. and \projecttitle{} w/ A. means that the network stack generates and sends attested messages without verifying them at the receiving side. 
%\atsushi{What do you think new labels like this: RDMA-hw, DRCT-IO, DRCT-IO-acc, TNIC, TNIC-acc}

Specifically, we evaluate five different network stacks: {\em (i)} RDMA-hw which implements a reliable, untrusted RoCE protocol on FPGAs, {\em (ii)} our \projecttitle{}, {\em (iii)} DRCT-IO, a direct I/O, untrusted, software-based network stack that bypasses the kernel stack, {\em (iv)} DRCT-IO-att, the previous stack that offers trust by sending attested messages (using an SGX-based SSL-server) without verifying them at the receiving side and {\em (v)} \projecttitle{}-att that similarly sends attested messages but does not verify them. We build the RDMA-hw experiment on top of Coyote~\cite{coyote} network stacks imilarly to \projecttitle{}, . For the DRCT-IO versions, we base our design on eRPC~\cite{erpc} with DPDK~\cite{dpdk} that offers similar reliability guarantees with RDMA-hw. The benchmarks with hardware implementation run on AMD-FPGA Cluster, whereas the rest run on Intel Cluster.
%\antonis{again names are not very memorable. Something more %descriptive/relevant to the paper than D-I/O, etc.?}
%\atsushi{Which term we should use for our proposal: \projecttitle{} or \projectlibrary{}?}

%Specifically, our evaluation answers the following questions:
%\begin{itemize}
%    \item {\bf RQ1.} How much does \projecttitle{}'s offered security cost?
%    \item {\bf RQ2.} How does \projectlibrary{} performs compared to competitive network %stacks?
%\end{itemize}

\begin{figure*}
    \centering
   \includegraphics[width=\linewidth]{atc-submission-plots/rpc_lat.pdf} 
  \caption{Latency evaluation of send operations for various packet sizes across five competitive network stacks with various security properties.}
  \label{fig:net_latencies}
\end{figure*}

\begin{figure}
    \centering
   \includegraphics[width=0.9\linewidth]{atc-submission-plots/rpc_thr.pdf} 
    \caption{Throughput evaluation of send operations for various packet sizes across five competitive network stacks with various security properties.}
  \label{fig:net_throughput}
\end{figure}

\if 0
\begin{figure}[t!]
\begin{center}
\minipage{0.7\linewidth}
  \centering
  \includegraphics[width=\linewidth]{atc-submission-plots/tnic_overheads.pdf} 
\endminipage
\minipage{0.3\linewidth}
  \centering
  \includegraphics[width=\linewidth]{atc-submission-plots/tnic_speedup.pdf} 
  %\label{fig:net_speedup}
\endminipage
\end{center}
\caption{Left figure shows the slowdown of \projecttitle{} w.r.t. to RDMA-hw. Right figure shows the speedup of \projecttitle{} w.r.t. DRCT-IO-att.}\label{fig:net_slowdown}
\end{figure}
\fi

\myparagraph{Methodology and experiments} All our runs in the section have been conducted through a single-threaded client-server implementation that uses the respective network stack. In the throughput experiments the client issues more than one on-going operation whereas to optimize for the latency (and accurately measure it) the client process sends one operation at a time in the respective experiments.  We decided on these settings for two reasons. First, commercial blockchain systems whose design relies on BFT systems to implement ordering services~\cite{10.1145/3190508.3190538} for the ledger have shown that ordering operations dominates the overall latency. As such we want to reliably illustrate \projecttitle{} latencies. Secondly, parallelism in (totally-ordered) consensus protocols (e.g., ~\cite{10.1145/3190508.3190538}) have been evaluated to have little or minimal performance improvements~\cite{f04eb9b864204bab958e72055062748c, 10.1145/3299869.3319893}.
%have already shown that parallelism in (totally-ordered) consensus protocols (as in~\cite{10.1145/3190508.3190538})  offer minimal to no performance improvements.
%\atsushi{Can we simplify the paragraph like this: we run experiments in such settings... for two reasons. First, commercial blockchain systems... Second, parallelism in consensus... has already been evaluated. }

\myparagraph{Results} 
Figure~\ref{fig:net_latencies} and~\ref{fig:net_throughput} show the latency and the throughput of the send operation with of various sizes data transfers between two machines across the network. The latencies of both \projecttitle{} and \projecttitle{}-att. increase linearly with the packet size in contrast to the (untrusted) RDMA-hw latencies which remain stable (5---7us) for packet sizes up to 4KiB and are tripled thereafter (18---20us). Our \projecttitle{}'s offers trusted networking with latencies that are 3$\times$---20$\times$ higher than the untrusted RDMA-hw. \projecttitle{} network latency is dominated primarily by the HMAC calculation. As this algorithm fundamentally cannot be parallelized, the higher the message size, the higher the latency our \projecttitle{} incurs. More specifically, in \projecttitle{}, for packet sizes that are less than 1~KiB, doubling the packet size results in 13\%---20\% increment in the overall latency. For packet sizes bigger or equal to 1~KiB, doubling the packet size increases the latency by 30\%---40\%. 

Compared to the DRCT-IO-att (82us) our \projecttitle{} is up to 5.6$\times$ faster. Importantly, DRCT-IO-att reported extreme latencies (2000us or more) for packet sizes larger than 521B. We attribute these latencies to the scheduling effects of the framework to access the TEE~\cite{scone} and we omitted these numbers to avoid plot distortion. 

For packet sizes up to 1~KiB the DRCT-IO offers minimal latency (16-16.6us) due to zero-copy transmission/reception optimizations~\cite{erpc} that are only effective for up to 1460B (MTU is 1500B but 40B are reserved for metadata).  The RDMA-hw achieves 3$\times$ lower latency (5-5.5us) for packet sizes up to 2~KiB. For bigger data transfers, the RDMA-hw latency increases steadily up to 19~us whereas DRCT-IO does not scale well with latencies up to 100us.  Overall, the RDMA-hw is 3$\times$---5$\times$ faster than the DRCT-IO showing that network offloading to the hardware boosts performance.

%We now compare the untrusted software and hardware implementations of the network stacks. The RDMA-hw is 3$\times$---5$\times$ faster than the DRCT-IO. For packet sizes up to 1~KiB the DRCT-IO offers minimal latency (16-16.6us) due to zero-copy transmission/reception optimizations~\cite{erpc} that are only effective for up to 1460B (MTU is 1500B but 40B are reserved for metadata). Recall that even with such optimizations .  For bigger data transfers, the RDMA-hw latency increases steadily up to 19~us for packet size to be equal to 32KiB. DRCT-IO, on the other hand, does not scale well for bigger packet sizes, reporting latency up to 100us, mainly because network stack operations (e.g., packet fragmentation, packet construction, etc.) are running in software in contrast to RDMA-hw. 


%Lastly, D-I/O-att. shows stable latency (82us) for packet sizes up to 512. For larger packet sizes, we measured extreme latencies (2000us or more), which we attribute to the scheduling effects of the framework to access the TEE~\cite{scone}. We omit these numbers to avoid plot distortion.


%Latencies of both \projecttitle{} and \projecttitle{} w/ A. increase linearly with the packet size. This is primarily due to HMAC calculation, which fundamentally is not improved by parallelism. As such, the higher the message size, the higher the latency our \projecttitle{} incurs. Specifically, in \projecttitle{}, for packet sizes that are less than 1~KiB, doubling the packet size results in 13\%---20\% increment in the overall latency. For packet sizes bigger or equal to 1024, doubling the packet size increases the latency by 30\%---40\%. \projecttitle{} w/ A. shows similar behavior. Lastly, D-I/O w/ A. shows stable latency (82us) for packet sizes up to 512. For larger packet sizes, we measured extreme latencies (2000us or more), which we attribute to the scheduling effects of the framework to access the TEE~\cite{scone}. We omit these numbers to avoid plot distortion.


Lastly, we calculated the slowdown of \projecttitle{} compared to the RDMA-hw (Figure~\ref{fig:net_slowdown} (left)) and the speedup of \projecttitle{} compared to DRCT-IO-att  (Figure~\ref{fig:net_slowdown} (right)). As expected, the bigger the packet size, the higher the respective overhead; this is due to HMAC calculations, as explained.  \projecttitle{} has 3---20$\times$ overhead compared to RDMA-hw and 3$\times$---5$\times$ speedup compared to its equivalent software based implementation that uses SGX as network packets authenticator. The overheads are the result of the HMAC calculation whereas the speedup is the result of both the network stack offloading in hardware and the on-data-path security processing thanks to \projecttitle{} attestation kernel.
