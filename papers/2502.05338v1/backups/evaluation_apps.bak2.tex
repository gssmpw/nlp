\subsection{Distributed Applications Evaluation}
\label{subsec:use_cases_eval}

\if 0
\begin{table}
\begin{center}
\small
\begin{tabular}{ |c|c|c|c| } 
 \hline
 System & $N$ & $f$ ($N=3$) & Byz. faults \\ [0.5ex] \hline \hline
 A2M    & 1 & 0 & Prevention\\
 BFT &  $2f+1$ & $f=1$ & Prevention\\
 Chain Replication &  $2f+1$ & $f=1$ & Prevention\\
 PeerReview & $f+1$ & $f=2$ & Detection\\
 \hline
\end{tabular}
\end{center}
%\vspace{-10pt}
\caption{Properties of the four systems implemented with \projecttitle{}.}
\label{tab:use_cases_options}
%\vspace{-18pt}
\end{table}
\fi
We implement the four systems of $\S~\ref{sec:use_cases}$ with \projecttitle{}. All systems run in three nodes ($N=3$) except for the A2M that is a single node system.
% the systems' properties where $N$ refers to the number of machines used for the protocol, and $f$ is the number of failures the system can tolerate.

\myparagraph{Methodology and experiments} We execute all four of our codebases on Intel Cluster, utilizing all its three servers (as the minimum required setup capable of withstanding a single failure, $N=2f+1$, where $f=1$). Furthermore, due to our limited resources, with only two Alveo U280 cards available and the physical separation of Intel Cluster from AMD-FPGA Cluster, we cannot access Alveo U280 cards for these specific experiments. Instead, we compiled our code using the DRCT-IO network stack as previously detailed in $\S$~\ref{subsec:net_lib} and do busy waiting to accurately replicate the \projecttitle{} delays within the CPU for generating and verifying attested messages.% in the \projecttitle{} system. %Our code uses busy waiting to accurately emulate latency rather than sleep functions. 

We evaluate each protocol using five systems that generate and verify the attestations: {\em (i)} the SSL-lib a library that is integrated into the codebase (no tamper-proof), {\em (ii)} the SSL-server (no tamper-proof), {\em (iii)} SGX and {\em (iv)} AMD-sev, and {\em (v)} \projecttitle{}. To perform a fair comparison we integrate into our protocols' codebases an library that accurately emulate all latencies (measured in $\S$~\ref{subsec:hw_eval}) within the CPU. We do not emulate the SSL-lib latency and for the AMD latency we use 30us which represents the lower bound of the latencies measured in $\S$~\ref{subsec:hw_eval}.

 %which are all an integrated library to the codebase and further adds a configurable delay to represent the operation's latency in each system. The added delay for each system is the respective latency we measured in

Given that the DRCT-I/O for small messages operates roughly three times the speed of the hardware-based RDMA network stack implementation, the latencies outlined in this section are anticipated to reflect the upper limit for all four systems with \projecttitle{}.

\myparagraph{A2M} We implement a prototype of A2M system with \projecttitle{}, an effective building block for imporving the scalability and performance in various systems~\cite{A2M, sundr, Castro:2002, AbdElMalek2005FaultscalableBF}. Similarly to its original implementation, we port the entire log within the TEE, i.e., SGX, building a large TCB. We refer to this system as SGX-lib. All other versions place the attested log in the (untrusted) host memory, using the trusted systems to generate attestations as in~\cite{levin2009trinc}. %\atsushi{Does this paragraph repeat explanations in s7.1? If so, like the following applications, can we simply say "We evaluate the A2M system (TNIC-log described in $\S$~\ref{sec:use_cases::a2m})..."?}

In this experiment, we first construct a log of size 9GB with 100M entries and then sequentially we lookup for them individually. Each log entry is comprised of 64B of appended data (context) and an extra 36B for the metadata.

%specifically the computed authenticate (32B) and the sequence number of the entry (4B). In short, we append 100M of 100B entries, leading to a log size of approximately 9GB. \atsushi{Can we somehow shorten the description?}

\noindent{\underline{Results.}} Figure~\ref{fig:a2m_eval} shows the throughput (operations per second) and the average (mean) latency of our implemented A2M system using various systems. The append operation throughput in the SSL-lib case illustrates the throughput upper bound as it incurs no communication costs. Specifically, the A2M with SSL-lib running natively in the CPU reports a latency of 1.26us, which is dominated by the HMAC computation latency.
Placing the log within the SGX (SSL-lib), we avoid the costly communication w.r.t. to an SGX-based server implementation, and as such, the system only experiences a 2$\times$ slowdown compared to the native case. The communications costs are reflected in the AMD-sev case; that runs the SSL-server. Compared to when porting to SGX, AMD-sev incurs 15$\times$. Lastly for \projecttitle{}, we observe approximately 5$\times$ and 2.4$\times$ slowdown compared to the SSL-lib and SGX-lib execution, respectively which is due to the HMAC calculation.

Regarding the lookup operation, all of the SSL-lib, AMD-sev and \projecttitle{} systems report similar throughput and latency as the operation lookups the untrusted host memory for the requested entry. However, the SGX-lib system reports a 66$\times$ slowdown for lookups due to its trusted memory size constraints and the effects of the Intel SGX paging mechanism~\cite{treaty}. As a result, while \projecttitle{} offers slower append operations compared to when porting the entire log into the TEE, \projecttitle{} greatly optimizes lookup latencies due to its minimal TCB.



\begin{figure}[t!]
\begin{center}
\small
\minipage{0.22\textwidth}
  \centering
\begin{tabular}{lrr}
\hline
& \multicolumn{2}{c}{Throughput (Op/s)} \\
System          & append    & lookup  \\
\hline
SSL-lib         & 790K      & 256M      \\
SGX-lib             & 380K      & 3.8M       \\
AMD-sev         & 30K       & 263M      \\
\projecttitle{} & 158K      & 257M      \\
\hline
\end{tabular}
\endminipage
\hfill
\minipage{0.18\textwidth}
\centering
\begin{tabular}{rrr}
\hline
\multicolumn{2}{c}{Latency (us)} \\
 append     & lookup  \\
\hline
 1.26       & 0.0039      \\
 2.6        & 0.26       \\
 32.37      & 0.0038      \\
 6.34       & 0.0039      \\
 \hline
\end{tabular}
\endminipage
\end{center}
\caption{Throughput and latency evaluation of A2M using various trusted components.}%\dimitra{@experiments: maybe you try run A2M into an SGX server to also show the communication costs}}
\label{fig:a2m_eval}
\end{figure}

\myparagraph{BFT} We evaluate the performance of our implemented BFT protocol (discussed in $\S$~\ref{sec:use_cases::byz_smr}) with various network batching factors. Network batching is a standardized optimization; we implement it in software as part of the application's message format. 
% We evaluate the performance of our implemented Byzantine SMR protocol (discussed in $\S$~\ref{sec:use_cases::byz_smr}) using various trusted components and network batching factors. Network batching is a standardized optimization; we implement it in software as part of the application's message format. 

We allocate one {\em message} structure for each client's request in this experiment. The {\em message} contains command and output fields (16B each), which store the initial command and results of the command's execution in a node, respectively. In addition, the message contains metadata, which includes a sequencer and signed hashes of the replicas' states known to each node. The sequencer is used for serialization, and it is equivalent to the counter value assigned to each message from \projecttitle{}. The signed hashes representing the replicas' states are obtained from the last messages received from each replica.

\noindent{\underline{Results.}} Figure~\ref{fig:byz_smr_throuthput} show the throughput and the latency of the protocol. Our \projecttitle{} incurs about 2.4$\times$ throughput overhead and up to 7$\times$ higher latency compared to the SSL-lib version. Similarly, SGX and AMD-sev versions incur 13.5$\times$ and 9.6$\times$ throughput overheads compared to the SSL-lib. As such, our \projecttitle{} improves throughput and latency 4---6$\times$ compared to TEE-based versions. Recall that the SSL-lib version is the upper bound in performance we can possibly achieve as it calculates the attestations natively and eliminates the communication overheads that a realistic solution (SGX, AMD-sev) occurs. However, in contrast to our \projecttitle{}, SSL-lib is not tamper-proof (Table~\ref{tab:hw_options}) as the security-related processing runs in the untrusted host CPU and can be accessed and compromised by malicious adversaries.

%\atsushi{Can we highlight that TNIC is temper-proof but the Nat-lib (best perf) is not?}.
%\antonis{Are the differences/benefits of TEE vs. TFPGA tamper-proofness discussed earlier than the evaluation? Seems its something important.}


In addition, we observe that batching increases throughput (and decreases latency) proportionally to the number of batched messages. For all but the SSL-lib version, we report a 7$\times$ and 15$\times$ throughput boost for batching factor to be equal to 8 and 16 respectively compared with the experiment run with batching factor equal to 1. This is because our batching technique improves the network utilization and reduces the overall attestation calculations, e.g., one attestation per 8 and 16 messages. The technique is moderately effective when using the SSL-lib, approximately 4---6$\times$ faster than without batching. This is primarily because, in that case, the latency is dominated by the network stack latency as the attestation function running natively generates attestations fast. As such, we show that conventional techniques can drastically eliminate the overheads for BFT and improve \projecttitle{}'s adoption into practical systems.


\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{atc-submission-plots/bft_pb.pdf} 
    \caption{Throughput (and latency numbers) of BFT using various trusted components.} \label{fig:byz_smr_throuthput}
\end{figure}

%\begin{figure}[t!]
%    \centering
%    \includegraphics[width=0.7\linewidth]{atc-submission-plots/bft_pb_lat.pdf} 
%    \caption{Average latency of the BFT SMR using various trusted components.} \label{fig:byz_smr_lat}
%\end{figure}


\myparagraph{Chain Replication} We evaluate the performance of the Chain Replication protocol (discussed in $\S$~\ref{sec:use_cases::byz_chain_rep}) without bactching. % using various trusted components. 
We allocate one message structure per client request comprising of 60B context (that includes metadata, e.g., source, destination nodes, message id, etc.) and  4B for the operation type. As before, the 32B attestation  adds to the message. %We implemented the replication protocol without any underlying Key-Value store data structure.
%of the message allocates 60B comprised of an 8B key and a 32B value as well as 16B for metadata (e.g., source and destination nodes, message idx, etc.) and

\noindent{\underline{Results.}} Figure~\ref{fig:byz_chain_replication} shows the throughput and average mean latency of our implemented Chain Replication with various trusted components. Compared with the SSL-lib implementation, \projecttitle{} incurs 4.6$\times$ overhead and SSL-server incurs 5.8$\times$ overhead. Our trusted \projecttitle{} is 30\% faster compared to an non-tamper-proof (native) SSL-server. The performance benefit stems primarily from the \projecttitle{}'s attestation kernel being on the transmission/reception data path.
Compared to the SGX and AMD-sev versions, Byzantine Chain Replication with \projecttitle{} is 5$\times$ and 3.4$\times$ faster.
\begin{figure}
    \centering
  \includegraphics[width=\linewidth]{atc-submission-plots/bftcr_lat_throughput.pdf} 
    \caption{Throughput (and latency numbers) of Chain Replication using various trusted components.} \label{fig:byz_chain_replication}
\end{figure}

\begin{figure}
  \includegraphics[width=\linewidth]{atc-submission-plots/bftpr_lat_throughput.pdf} 
    \caption{Throughput (and latency numbers) of PeerReview using various trusted components.} \label{fig:accountability_protocol}
\end{figure}

\if 0
\begin{figure*}[t!]
\begin{center}
\minipage{0.5\textwidth}
  \centering
  \includegraphics[width=0.8\linewidth]{atc-submission-plots/bftcr_lat_throughput.pdf} 
    \caption{Throughput-latency evaluation of a Byzantine Chain Replication using various trusted components.} \label{fig:byz_chain_replication}
\endminipage
\minipage{0.5\textwidth}
  \centering
  \includegraphics[width=0.8\linewidth]{atc-submission-plots/bftpr_lat_throughput.pdf} 
    \caption{Throughput-latency evaluation of the accountability protocol using various trusted components.} \label{fig:accountability_protocol}
\endminipage
\end{center}
\end{figure*}
\fi


\if 0
\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{atc-submission-plots/bftcr_lat_throughput.pdf} 
    \caption{Throughput-latency evaluation of a BFT version of CR using various trusted components.} \label{fig:lat_throughput_kernel}
\end{figure}
\fi






\myparagraph{PeerReview} We evaluate the performance of our PeerReview system (discussed in $\S$~\ref{sec:use_cases::accountability}). % using various trusted components. 
We evaluate the protocol by both activating and deactivating the audit protocol. In our implementation, we use one witness for the source node that {\em periodically} audits its log. The witness process is co-located in the same node, and as such, reading the log implies reading a shared memory. We decide in favor of that implementation to carefully isolate the overheads for inspection and replay of the log. In our experiments, the witness is configured to audit the log after every send operation in the source node until both clients acknowledge the receipt of all source messages.

The log entries consist of the context, a sequencer, a signed hash of the context (authenticator) as well as the cumulative digest, which is the signed hash of the authenticator and the previous entry's digest in the log. In total, the messages allocate about 200B.% and we do not use batching. 
%\atsushi{I feel that the description of these data formats (as well as the other applications) is too detailed, so it'll be fine to omit for cutting the text.}

%The log entries consist of the context, a sequencer, a signed hash of the context, and the sequencer (authenticator) \antonis{is this different than the aforementioned "sequencer"?} as well as the cumulative digest, which is the signed hash of the authenticator and the previous entry's digest in the log. The context is comprised of the data (16B), the output of the protocol specification for that input data (16B), some metadata, and the response sent from the clients that have a similar format. In total, the messages allocate about 200B, and we do not use batching. \atsushi{I feel that the description of these data formats (as well as the other applications) is too detailed, so it'll be fine to omit for cutting the text.}

\noindent{\underline{Results.}} Figure~\ref{fig:accountability_protocol} shows the throughput and latency measurements of PeerReview system with and without enabling the audit protocol.
Without the audit protocol, the use of TEEs to generate attestations incurs a throughput slowdown up to 30$\times$ compared to the SSL-lib. Using our \projecttitle{} we improve the overheads---specifically, we incur 5.8$\times$ slowdown, that is 3---5$\times$ better throughput compared to when using AMD-sev and SGX.


When we activate the audit protocol, the systems behavior is similar. The \projecttitle{} outperforms both AMD-sev and SGX implementations by 3.7---5$\times$. Importantly, when using \projecttitle{}, the audit protocol leads to a performance slowdown of 1.33$\times$ compared to when not enabling it. The audit protocol itself consumes approximately 25\% (17us) of the overall execution latency. Even with the audit protocol, \projecttitle{} offers 3.7---5.42$\times$ lower latency compared to its TEEs-based competitors.

%\atsushi{a bit difficult to read because 'log' is used as both noun and verb. Can we somehow use another word?} When the auditing protocol is de-activated, the source node sends the (streaming) data to the client nodes. The clients execute and append the message to their log, and then reply to the source. The source appends its own message to the log along with the client's reply. This is required to make sure that any participant cannot lie to a witness about the data it sends to the clients. \atsushi{The sentences until here explain the application workflow, but we should show the results first and then start the discussion with an extra explanation.} We see that using a TEE to generate attested messages increases the throughput slowdown up to 30$\times$ compared to the native embedded library (Nat-lib). Using our \projecttitle{} we improve the overheads---specifically, we incur 5.8$\times$ slowdown, that is 3---5$\times$ better throughput compared to when using AMD-sev and SGX as trusted messages' singers.


\if 0
\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{atc-submission-plots/bftpr_lat_throughput.pdf} 
    \caption{Throughput-latency evaluation of a BFT version of PR using various trusted components.} \label{fig:lat_throughput_kernel}
\end{figure}
\fi
