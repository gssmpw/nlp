\subsection{Distributed Applications Evaluation}
\label{subsec:use_cases_eval}

\begin{table}
\begin{center}
\begin{tabular}{ |c|c|c|c| } 
 \hline
 System & $N$ & $f$ ($N=3$) & Byz. faults \\ [0.5ex] \hline \hline
 A2M    & 1 & 0 & Prevention\\
 BFT &  $2f+1$ & $f=1$ & Prevention\\
 Chain Replication &  $2f+1$ & $f=1$ & Prevention\\
 PeerReview & $f+1$ & $f=2$ & Detection\\
 \hline
\end{tabular}
\end{center}
%\vspace{-10pt}
\caption{Properties of the four systems implemented with \projecttitle{}.}
\label{tab:use_cases_options}
%\vspace{-18pt}
\end{table}
We implement the four systems of $\S~\ref{sec:use_cases}$ with \projecttitle{}. Table~\ref{tab:use_cases_options} summarizes the systems' properties where $N$ refers to the number of machines used for the protocol, and $f$ is the number of failures the system can tolerate.

\myparagraph{Methodology and experiments} We execute all four of our codebases on Cluster 2, utilizing its three servers, which form the minimum required setup capable of withstanding a single failure ($N=2f+1$, where $f=1$). Furthermore, due to our limited resources, with only two Alveo U280 cards available and the physical separation of Cluster 2 from Cluster 1, we were unable to access Alveo U280 cards for these specific experiments. Instead, we compiled our code using the D-I/O network stack as previously detailed in $\S$~\ref{subsec:net_lib}. We replicate these delays within the CPU to emulate latency delays for generating and verifying attested messages in the \projecttitle{} system. Our code uses busy waiting to accurately emulate latency rather than sleep functions. 

We evaluate each protocol using five systems that generate and verify the attestations: {\em (i)} the Nat-lib a library that is integrated into the codebase (no tamper-proof), {\em (ii)} the Nat, the OpenSSL-based server where we run one server instance in each protocol node (no tamper-proof), {\em (iii)} SGX and {\em (iv)} AMD-sev, and {\em (v)} \projecttitle{} which are all an integrated library (Nat-lib) to the codebase and further adds a configurable delay to represent the operation's latency in each system. The added delay for each system is the respective latency we measured in $\S$~\ref{subsec:hw_eval}.

Given that the D-I/O for small messages operates roughly three times the speed of the hardware-based RDMA network stack implementation, the latencies outlined in this section for \projecttitle{} are anticipated to reflect the upper limit.

\myparagraph{\projecttitle{}-log} We implement a prototype of A2M system, \projecttitle{}-log, an effective building block for reducing the replication factor and improving performance in various BFT systems~\cite{A2M, sundr, Castro:2002, AbdElMalek2005FaultscalableBF}. Similarly to the original implementation of the system, our SGX-based implementation ports the entire log within the TEE. All other versions place the attested log in the (untrusted) host memory, using the implemented systems to generate attested entries as  in~\cite{levin2009trinc}. \atsushi{Does this paragraph repeat explanations in s7.1? If so, like the following applications, can we simply say "We evaluate the A2M system (TNIC-log described in $\S$~\ref{sec:use_cases::a2m})..."?}

In this experiment, we first append 100M entries in the log and sequentially lookup for them individually. Each entry is comprised of the appended data (context)---in our case, 64B---and an extra 36B for the metadata, specifically the computed authenticate (32B) and the sequence number of the entry (4B). In short, we append 100M of 100B entries, leading to a log size of approximately 9GB. \atsushi{Can we somehow shorten the description?}

\noindent{\underline{Results.}} Figure~\ref{fig:a2m_eval} shows the throughput---measured in operations per second---and the average (mean) latency in microseconds of our implemented \projecttitle{}-log system using various system configurations. The append operation throughput in the Nat-lib case illustrates the maximum bound. Specifically, the operation running natively in the CPU reports a latency of 1.26us, which is dominated by the HMAC computation latency.
Placing the log within the SGX, we avoid costly communication, and as such, the system only experiences a 2$\times$ slowdown compared to the native case. On the other hand, we are using the AMD-sev as the trusted subsystem, which incurs 15$\times$ overheads compared to when porting the log within the SGX \antonis{Why?}. Lastly for \projecttitle{}, we observe approximately 5$\times$ and 2.4$\times$ slowdown compared to the native and SGX execution, respectively. The slowdown is primarily due to the HMAC calculation.

Regarding the lookup operation, all of the Nat-lib, AMD-sev and \projecttitle{} systems report similar throughput and latency as the operation lookups the untrusted host memory for the requested entry. However, the SGX-lib system reports a 66$\times$ slowdown for lookups due to its trusted memory size constraints and the effects of the Intel SGX paging mechanism~\cite{treaty}.



\begin{figure}[t!]
\begin{center}
\minipage{0.22\textwidth}
  \centering
\begin{tabular}{lrr}
\hline
& \multicolumn{2}{c}{Throughput (Op/s)} \\
System          & append    & lookup  \\
\hline
Nat-lib         & 790K      & 256M      \\
SGX-lib             & 380K      & 3.8M       \\
AMD-sev         & 30K       & 263M      \\
\projecttitle{} & 158K      & 257M      \\
\end{tabular}
\endminipage
\hfill
\minipage{0.18\textwidth}
\centering
\begin{tabular}{rrr}
\hline
\multicolumn{2}{c}{Latency (us)} \\
 append     & lookup  \\
\hline
 1.26       & 0.0039      \\
 2.6        & 0.26       \\
 32.37      & 0.0038      \\
 6.34       & 0.0039      \\
 
\end{tabular}
\endminipage
\end{center}
\caption{Performance evaluation of \projecttitle{}-log using various trusted components.}%\dimitra{@experiments: maybe you try run A2M into an SGX server to also show the communication costs}}
\label{fig:a2m_eval}
\end{figure}

\myparagraph{Byzantine SMR} 
We evaluate the performance of our implemented Byzantine SMR protocol (discussed in $\S$~\ref{sec:use_cases::byz_smr}) with various network batching factors. Network batching is a standardized optimization; we implement it in software as part of the application's message format. 
% We evaluate the performance of our implemented Byzantine SMR protocol (discussed in $\S$~\ref{sec:use_cases::byz_smr}) using various trusted components and network batching factors. Network batching is a standardized optimization; we implement it in software as part of the application's message format. 

We allocate one {\em message} structure for each client's request in this experiment. The {\em message} contains command and output fields (16B each), which store the initial command and results of the command's execution in a node, respectively. In addition, the message contains metadata, which includes a sequencer and signed hashes of the replicas' states known to each node. The sequencer is used for serialization, and it is equivalent to the counter value assigned to each message from \projecttitle{}. The signed hashes representing the replicas' states are obtained from the last messages received from each replica.

\noindent{\underline{Results.}} Figures~\ref{fig:byz_smr_throuthput} and~\ref{fig:byz_smr_lat} show the throughput and the latency of the implemented protocol. Our \projecttitle{} incurs about 2.4$\times$ overheads compared to the Nat-lib version. Similarly, SGX and AMD-sev versions incur 13.5$\times$ and 9.6$\times$ overheads compared to the Nat-lib. As such, leveraging the TEEs can be 4---6$\times$ less performant our \projecttitle{}. Recall that the Nat-lib version is the upper bound in performance we can possibly achieve as it calculates the attestations natively and does not incur the communication overheads that a realistic solution (SGX, AMD-sev) would occur. Nat-lib is not tamper-proof \atsushi{Can we highlight that TNIC is temper-proof but the Nat-lib (best perf) is not?}.
\antonis{Are the differences/benefits of TEE vs. TFPGA tamper-proofness discussed earlier than the evaluation? Seems its something important.}


Secondly, we observe that batching can increase throughput (and decrease the latency) proportionally to the number of batched messages. Specifically, for all but the Nat-lib version, we report a 7$\times$ and 15$\times$ throughput boost for 8 and 16 batching factor compared with the experiment run with batching factor equal to 1. This is because our batching technique improves the network utilization and reduces the overall attestation calculations, e.g., one per 8 and 16 messages. The technique is moderately effective when using the Nat-lib, approximately 4---6$\times$ faster than without batching. This is primarily because, in that case, the latency is dominated by the network stack latency as the attestation function running natively generates attestations fast. As such, we show that conventional techniques can drastically eliminate the overheads for BFT and improve \projecttitle{}'s adoption into practical systems.


\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{atc-submission-plots/bft_pb.pdf} 
    \caption{Throughput of the BFT SMR using various trusted components.} \label{fig:byz_smr_throuthput}
\end{figure}

%\begin{figure}[t!]
%    \centering
%    \includegraphics[width=0.7\linewidth]{atc-submission-plots/bft_pb_lat.pdf} 
%    \caption{Average latency of the BFT SMR using various trusted components.} \label{fig:byz_smr_lat}
%\end{figure}


\myparagraph{Byzantine Chain Replication} We evaluate the performance of the Byzantine Chain Replication protocol (discussed in $\S$~\ref{sec:use_cases::byz_chain_rep}). % using various trusted components. 
In this experiment, we do not employ batching, and we allocate one message structure as discussed in $\S$~\ref{sec:use_cases::byz_chain_rep} per client request. The context of the message allocates 60B comprised of an 8B key and a 32B value as well as 16B for metadata (e.g., source and destination nodes, message idx, etc.) and 4B for the operation type. As before, the HMAC adds 32B to the message. We implemented the replication protocol without any underlying Key-Value store data structure.

\noindent{\underline{Results.}} Figure~\ref{fig:byz_chain_replication} shows the throughput and average mean latency of Byzantine Chain replication using various trusted components. Concerning the Nat-lib implementation, \projecttitle{} incurs 4.6$\times$ overhead. In addition, in the Nat version, where the OpenSSL server incurs the communication costs with the protocol process, we see 5.8$\times$ overhead. Our \projecttitle{} is 1.2$\times$ faster compared to Nat execution; the performance benefit is primarily due to the on-the-data-path approach we follow.
Compared to the SGX and AMD-sev versions, Byzantine Chain Replication with \projecttitle{} is 5$\times$ and 3.4$\times$ faster.
\begin{figure}
    \centering
  \includegraphics[width=\linewidth]{atc-submission-plots/bftcr_lat_throughput.pdf} 
    \caption{Throughput-latency evaluation of a Byzantine Chain Replication using various trusted components.} \label{fig:byz_chain_replication}
\end{figure}

\begin{figure}
  \includegraphics[width=\linewidth]{atc-submission-plots/bftpr_lat_throughput.pdf} 
    \caption{Throughput-latency evaluation of the accountability protocol using various trusted components.} \label{fig:accountability_protocol}
\end{figure}

\if 0
\begin{figure*}[t!]
\begin{center}
\minipage{0.5\textwidth}
  \centering
  \includegraphics[width=0.8\linewidth]{atc-submission-plots/bftcr_lat_throughput.pdf} 
    \caption{Throughput-latency evaluation of a Byzantine Chain Replication using various trusted components.} \label{fig:byz_chain_replication}
\endminipage
\minipage{0.5\textwidth}
  \centering
  \includegraphics[width=0.8\linewidth]{atc-submission-plots/bftpr_lat_throughput.pdf} 
    \caption{Throughput-latency evaluation of the accountability protocol using various trusted components.} \label{fig:accountability_protocol}
\endminipage
\end{center}
\end{figure*}
\fi


\if 0
\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{atc-submission-plots/bftcr_lat_throughput.pdf} 
    \caption{Throughput-latency evaluation of a BFT version of CR using various trusted components.} \label{fig:lat_throughput_kernel}
\end{figure}
\fi






\myparagraph{Accountability protocol} We evaluate the performance of the Accountability protocol (discussed in $\S$~\ref{sec:use_cases::accountability}). % using various trusted components. 
We evaluate the protocol by both activating and deactivating the audit protocol. In our implementation, we use one witness for the source node that {\em periodically} audits its log. The witness process is co-located in the same node, and as such, reading the log implies reading a shared memory. We decide in favor of that implementation to carefully isolate the overheads for inspection and replay of the log. In our experiments, the witness is configured to audit the log after every send operation in the source node until both clients acknowledge the receipt of all source messages.

The log entries consist of the context, a sequencer, a signed hash of the context, and the sequencer (authenticator) \antonis{is this different than the aforementioned "sequencer"?} as well as the cumulative digest, which is the signed hash of the authenticator and the previous entry's digest in the log. The context is comprised of the data (16B), the output of the protocol specification for that input data (16B), some metadata, and the response sent from the clients that have a similar format. In total, the messages allocate about 200B, and we do not use batching. \atsushi{I feel that the description of these data formats (as well as the other applications) is too detailed, so it'll be fine to omit for cutting the text.}

\noindent{\underline{Results.}} Figure~\ref{fig:accountability_protocol} shows the throughput and latency measurements of the accountability protocol with the audit protocol (blue line) and without the audit protocol (green line).
\atsushi{a bit difficult to read because 'log' is used as both noun and verb. Can we somehow use another word?} When the protocol does not activate the witness process to audit the log, the source node sends the streaming data to the client nodes. The clients execute and log the message and then reply to the source. Then, the source logs its own message along with the client's reply. This is required to make sure that any participant cannot lie to a witness about the data it sends to the clients. \atsushi{The sentences until here explain the application workflow, but we should show the results first and then start the discussion with an extra explanation.} We see that using a TEE to generate attested messages increases the throughput slowdown up to 30$\times$ compared to the native embedded library (Nat-lib). Using our \projecttitle{} we improve the overheads---specifically, we incur 5.8$\times$ slowdown, that is 3---5$\times$ better throughput compared to when using AMD-sev and SGX as trusted messages' singers.

When we activate the audit protocol, we observe similar performance behavior as before. The \projecttitle{} outperforms both AMD-sev and SGX implementations by 3.7---5$\times$. Importantly, activating the witness in the source node, when using \projecttitle{}, leads to a performance slowdown of 1.33$\times$ compared to when not executing the witness. More specifically, the audit protocol consumes approximately 25\% (17us) of the overall execution latency. \atsushi{What's the takeaway from this experiment?}

\if 0
\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{atc-submission-plots/bftpr_lat_throughput.pdf} 
    \caption{Throughput-latency evaluation of a BFT version of PR using various trusted components.} \label{fig:lat_throughput_kernel}
\end{figure}
\fi