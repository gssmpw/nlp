
\begin{figure*}
    \centering
   \includegraphics[width=0.80\linewidth]{atc-submission-plots/rpc_lat.pdf} 
   \vspace{-12pt}
  \caption{Latency of send operations across five competitive network stacks with various security properties.}
  % \caption{Latency evaluation of send operations for various packet sizes across five competitive network stacks with various security properties.}
  \label{fig:net_latencies}
   \vspace{-10pt}
\end{figure*}



%%%%%%%%



\vspace{-4pt}
\subsection{Software Evaluation: \projecttitle{} Network Stack}\label{subsec:net_lib}
\vspace{-2pt}



\myparagraph{Baselines} 
To evaluate the \projecttitle{} performance, we discuss (1) the effectiveness of offloading the network stack to the \projecttitle{} hardware and (2) the overheads incurred by the CFT systems transformation for the BFT model. We compare \projecttitle{} across four other software/hardware network stacks with different security properties as follows: 
% We evaluate the \projecttitle{} performance to show {\em{(i)}} the effectiveness of offloading the network stack from host CPUs to accelerators and {\em{(ii)}} the overheads that our system incurs due to materializing the requirements for CFT systems transformation (discussed in $\S~\ref{sec:requirements-ds}$). As such, we evaluate and compare \projecttitle{} across four other network stacks implemented on software or hardware with different security properties. 
%Specifically, we use the acronym D-I/O to refer to a Direct I/O network stack that bypasses the kernel stack (for performance). The acronyms D-I/O w/ A. and \projecttitle{} w/ A. means that the network stack generates and sends attested messages without verifying them at the receiving side. 
%\atsushi{What do you think new labels like this: RDMA-hw, DRCT-IO, DRCT-IO-acc, TNIC, TNIC-acc}
% 
% Specifically, we evaluate five different network stacks
{\em (i)} RDMA-hw, an untrusted RoCE protocol on FPGAs, {\em (ii)} DRCT-IO (direct I/O), untrusted software-based kernel-bypass stack, {\em (iii)} DRCT-IO-att, altered DRCT-IO that offers trust by sending attested messages but does not verify them, and {\em (iv)} \projecttitle{}-att, altered \projecttitle{} that similarly sends attested messages without verification. We build {\em (i)} RDMA-hw on top of Coyote~\cite{coyote} network stack similarly to \projecttitle{}. For {\em (ii) (iii)} DRCT-IOs, we base our design on eRPC~\cite{erpc} with DPDK~\cite{dpdk} that offers similar reliability guarantees with RDMA-hw. The hardware network stacks run on AMD-FPGA Cluster, whereas the rest run on Intel Cluster.
% The benchmarks with hardware implementation run on AMD-FPGA Cluster, whereas the rest run on Intel Cluster.
% Specifically, we evaluate five different network stacks: {\em (i)} RDMA-hw which implements a reliable, untrusted RoCE protocol on FPGAs, {\em (ii)} our \projecttitle{}, {\em (iii)} DRCT-IO, a direct I/O, untrusted, software-based network stack that bypasses the kernel stack, {\em (iv)} DRCT-IO-att, the previous stack that offers trust by sending attested messages (using an SGX-based SSL-server) without verifying them at the receiving side and {\em (v)} \projecttitle{}-att that similarly sends attested messages but does not verify them. We build the RDMA-hw experiment on top of Coyote~\cite{coyote} network stacks imilarly to \projecttitle{}, . For the DRCT-IO versions, we base our design on eRPC~\cite{erpc} with DPDK~\cite{dpdk} that offers similar reliability guarantees with RDMA-hw. The benchmarks with hardware implementation run on AMD-FPGA Cluster, whereas the rest run on Intel Cluster.
%\antonis{again names are not very memorable. Something more %descriptive/relevant to the paper than D-I/O, etc.?}
%\atsushi{Which term we should use for our proposal: \projecttitle{} or \projectlibrary{}?}

%Specifically, our evaluation answers the following questions:
%\begin{itemize}
%    \item {\bf RQ1.} How much does \projecttitle{}'s offered security cost?
%    \item {\bf RQ2.} How does \projectlibrary{} performs compared to competitive network %stacks?
%\end{itemize}



\if 0
\begin{figure}[t!]
\begin{center}
\minipage{0.7\linewidth}
  \centering
  \includegraphics[width=\linewidth]{atc-submission-plots/tnic_overheads.pdf} 
\endminipage
\minipage{0.3\linewidth}
  \centering
  \includegraphics[width=\linewidth]{atc-submission-plots/tnic_speedup.pdf} 
  %\label{fig:net_speedup}
\endminipage
\end{center}
\caption{Left figure shows the slowdown of \projecttitle{} w.r.t. to RDMA-hw. Right figure shows the speedup of \projecttitle{} w.r.t. DRCT-IO-att.}\label{fig:net_slowdown}
\end{figure}
\fi




\myparagraph{Methodology and experiments} 
Our experiments measure the latency and throughput for respective network stacks, which run through a single-threaded client-server implementation.
For the latency measurement, the client sends one operation at a time, whereas for the throughput measurement, one client can have multiple outstanding operations.




%issue multiple operations as parallelism in (totally-ordered) consensus protocols (e.g., ~\cite{10.1145/3190508.3190538}) has little performance improvements~\cite{f04eb9b864204bab958e72055062748c, 10.1145/3299869.3319893}. 

%we reliably measure the \projecttitle{} latencies in this way as the ordering operations dominate the overall latency of BFT systems~\cite{10.1145/3190508.3190538}.
%Whereas, 

% , while for the throughput, the client issues multiple operations. These settings are due to two reasons. First, commercial BFT-supported blockchain systems~\cite{10.1145/3190508.3190538} report that ordering operations dominate the overall latency, so we reliably measure the \projecttitle{} latencies in such a way. 
% As such, we want to reliably illustrate \projecttitle{} latencies. 
% First, commercial blockchain systems whose design relies on BFT systems to implement ordering services~\cite{10.1145/3190508.3190538} for the ledger have shown that ordering operations dominates the overall latency. As such, we want to reliably illustrate \projecttitle{} latencies. 
% Second, parallelism in (totally-ordered) consensus protocols (e.g., ~\cite{10.1145/3190508.3190538}) have been evaluated to have little or minimal performance improvements~\cite{f04eb9b864204bab958e72055062748c, 10.1145/3299869.3319893}.

% All our runs in the section have been conducted through a single-threaded client-server implementation that uses the respective network stack. In the throughput experiments the client issues more than one on-going operation whereas to optimize for the latency (and accurately measure it) the client process sends one operation at a time in the respective experiments.  We decided on these settings for two reasons. First, commercial blockchain systems whose design relies on BFT systems to implement ordering services~\cite{10.1145/3190508.3190538} for the ledger have shown that ordering operations dominates the overall latency. As such we want to reliably illustrate \projecttitle{} latencies. Secondly, parallelism in (totally-ordered) consensus protocols (e.g., ~\cite{10.1145/3190508.3190538}) have been evaluated to have little or minimal performance improvements~\cite{f04eb9b864204bab958e72055062748c, 10.1145/3299869.3319893}.

%have already shown that parallelism in (totally-ordered) consensus protocols (as in~\cite{10.1145/3190508.3190538})  offer minimal to no performance improvements.
%\atsushi{Can we simplify the paragraph like this: we run experiments in such settings... for two reasons. First, commercial blockchain systems... Second, parallelism in consensus... has already been evaluated. }

\myparagraph{Results} 
Figure~\ref{fig:net_latencies} and~\ref{fig:net_throughput} show the latency and throughput of the send operation with various packet sizes. First, regarding (1) the effectiveness of network stack offloading, RDMA-hw is 3$\times$---5$\times$ faster than DRCT-IO, which indicates that the network offloading boosts performance. Although DRCT-IO offers minimal latency (16-16.6us) for small packet sizes up to 1~KiB due to its zero-copy transmission/reception optimizations~\cite{erpc}, they are only effective for up to 1460B (MTU is 1500B, but 40B are reserved for metadata), and RDMA-hw still achieves 3$\times$ lower latency (5-5.5us). For bigger data transfers, the RDMA-hw latency increases steadily up to 19~us, whereas DRCT-IO does not scale well with latencies up to 100us. 


Second, regarding (2) the \projecttitle{} performance overhead, \projecttitle{} offers trusted networking with 3$\times$---20$\times$ higher latencies than the untrusted RDMA-hw. 
% The latencies of both \projecttitle{} and \projecttitle{}-att increase linearly with the packet size. Whereas, the (untrusted) RDMA-hw latencies remain stable (5---7us) for packet sizes up to 4KiB and are tripled thereafter (18---20us). 
The latency increase stems from the HMAC calculation of the \projecttitle{} hardware. As this algorithm fundamentally cannot be parallelized, the higher the message size, the higher the latency our \projecttitle{} incurs. More specifically, for packet sizes less than 1~KiB, doubling the packet size in \projecttitle{} results in a 13\%---20\% increment in the overall latency. For packet sizes bigger or equal to 1~KiB, doubling the packet size increases the latency by 30\%---40\%. 
Compared to DRCT-IO-att (82us), \projecttitle{} is up to 5.6$\times$ faster. Importantly, DRCT-IO-att reports extreme latencies (2000us or more) for packet sizes larger than 521B, which are omitted to avoid plot distortion. We attribute these latencies to the scheduling effects of {\sc scone}~\cite{scone}. 


% Figure~\ref{fig:net_latencies} and~\ref{fig:net_throughput} show the latency and throughput of the send operation with various packet sizes. As shown in Figure~\ref{fig:net_latencies}, \projecttitle{} offers trusted networking with 3$\times$---20$\times$ higher latencies than the untrusted RDMA-hw. 
% % The latencies of both \projecttitle{} and \projecttitle{}-att increase linearly with the packet size. Whereas, the (untrusted) RDMA-hw latencies remain stable (5---7us) for packet sizes up to 4KiB and are tripled thereafter (18---20us). 
% The latency increase stems from the HMAC calculation of the \projecttitle{} hardware. As this algorithm fundamentally cannot be parallelized, the higher the message size, the higher the latency our \projecttitle{} incurs. More specifically, for packet sizes less than 1~KiB, doubling the packet size in \projecttitle{} results in a 13\%---20\% increment in the overall latency. For packet sizes bigger or equal to 1~KiB, doubling the packet size increases the latency by 30\%---40\%. 
% 
% Compared to DRCT-IO-att (82us), \projecttitle{} is up to 5.6$\times$ faster. Importantly, DRCT-IO-att reports extreme latencies (2000us or more) for packet sizes larger than 521B, which are omitted to avoid plot distortion. We attribute these latencies to the scheduling effects of {\sc scone}~\cite{scone}. 
% 
% For packet sizes up to 1~KiB, DRCT-IO offers minimal latency (16-16.6us) due to zero-copy transmission/reception optimizations~\cite{erpc} which are only effective for up to 1460B (MTU is 1500B but 40B are reserved for metadata). RDMA-hw achieves 3$\times$ lower latency (5-5.5us) for packet sizes up to 2~KiB. For bigger data transfers, the RDMA-hw latency increases steadily up to 19~us, whereas DRCT-IO does not scale well with latencies up to 100us. Overall, RDMA-hw is 3$\times$---5$\times$ faster than DRCT-IO showing that network offloading to the hardware boosts performance.


%We now compare the untrusted software and hardware implementations of the network stacks. The RDMA-hw is 3$\times$---5$\times$ faster than the DRCT-IO. For packet sizes up to 1~KiB the DRCT-IO offers minimal latency (16-16.6us) due to zero-copy transmission/reception optimizations~\cite{erpc} that are only effective for up to 1460B (MTU is 1500B but 40B are reserved for metadata). Recall that even with such optimizations .  For bigger data transfers, the RDMA-hw latency increases steadily up to 19~us for packet size to be equal to 32KiB. DRCT-IO, on the other hand, does not scale well for bigger packet sizes, reporting latency up to 100us, mainly because network stack operations (e.g., packet fragmentation, packet construction, etc.) are running in software in contrast to RDMA-hw. 


%Lastly, D-I/O-att. shows stable latency (82us) for packet sizes up to 512. For larger packet sizes, we measured extreme latencies (2000us or more), which we attribute to the scheduling effects of the framework to access the TEE~\cite{scone}. We omit these numbers to avoid plot distortion.


%Latencies of both \projecttitle{} and \projecttitle{} w/ A. increase linearly with the packet size. This is primarily due to HMAC calculation, which fundamentally is not improved by parallelism. As such, the higher the message size, the higher the latency our \projecttitle{} incurs. Specifically, in \projecttitle{}, for packet sizes that are less than 1~KiB, doubling the packet size results in 13\%---20\% increment in the overall latency. For packet sizes bigger or equal to 1024, doubling the packet size increases the latency by 30\%---40\%. \projecttitle{} w/ A. shows similar behavior. Lastly, D-I/O w/ A. shows stable latency (82us) for packet sizes up to 512. For larger packet sizes, we measured extreme latencies (2000us or more), which we attribute to the scheduling effects of the framework to access the TEE~\cite{scone}. We omit these numbers to avoid plot distortion.


%Lastly, we calculated the slowdown of \projecttitle{} compared to the RDMA-hw (Figure~\ref{fig:net_slowdown} (left)) and the speedup of \projecttitle{} compared to DRCT-IO-att  (Figure~\ref{fig:net_slowdown} (right)). As expected, the bigger the packet size, the higher the respective overhead; this is due to HMAC calculations, as explained.  \projecttitle{} has 3---20$\times$ overhead compared to RDMA-hw and 3$\times$---5$\times$ speedup compared to its equivalent software based implementation that uses SGX as network packets authenticator. The overheads are the result of the HMAC calculation whereas the speedup is the result of both the network stack offloading in hardware and the on-data-path security processing thanks to \projecttitle{} attestation kernel.