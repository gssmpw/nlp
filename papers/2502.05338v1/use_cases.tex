\section{Protocols Implementation}\label{sec:use_cases-appendix}
We next present the implementation details of four distributed systems shown in Table~\ref{tab:use_cases_options} using \projecttitle{}, presented in Section~\ref{sec:use_cases}.

\begin{table}
\begin{center}
\small
\begin{tabular}{ |c|c|c|c| } 
 \hline
 System & $N$ & $f$ ($N=3$) & Byz. faults \\ [0.5ex] \hline \hline
 A2M    & 1 & 0 & Prevention\\
 BFT &  $2f+1$ & $f=1$ & Prevention\\
 Chain Replication &  $2f+1$ & $f=1$ & Prevention\\
 PeerReview & $f+1$ & $f=2$ & Detection\\
 \hline
\end{tabular}
\end{center}
%\vspace{-10pt}
\caption{Properties of the four trustworthy distributed systems implemented with \projecttitle{}.}
\label{tab:use_cases_options}
%\vspace{-18pt}
\end{table}

\subsection{Attested Append-Only Memory (A2M)}\label{sec:use_cases::a2m}



%\atsushi{What's the purpose of designing the TNIC-log? Can we explain the motivation in the first paragraph?}


We designed a single-node trusted log system based on the A2M system (Attested Append-Only Memory)~\cite{A2M} using \projecttitle{}. A2M has been proven to be an effective building block in improving the scalability and performance of various BFT systems~\cite{sundr, Castro:2002, AbdElMalek2005FaultscalableBF}. We show the {\em how} to use \projecttitle{} to build this foundational system while we also show that \projecttitle{} minimizes the system's TCB jointly with the performance improvements discussed in $\S$~\ref{sec:eval}.

\myparagraph{System model} Our \projecttitle{} version and the original A2M systems are single-node systems that target a similar goal; they both build a trusted append-only log as an effective mechanism to combat equivocation. The clients are only allowed to append entries to a log and each log entry is associated with a monotonically increasing sequence number. As such, each data item, e.g., a network message, is bound to a unique sequence number which is a well-known approach for equivocation-free operations~\cite{clement2012, hybster}. 

A2M was originally built using CPU-side TEEs---specifically, Intel SGX--- whereas we build its \projecttitle{} derivative. While the original A2M system keeps its entire state and the log within the TEE, we use \projecttitle{} to keep the (trusted) log in the untrusted memory. As such, in contrast to the original A2M, \projecttitle{} effectively reduces the overall system's TCB. Our evaluation showed that naively porting the application within the TEE has adverse performance implications in lookup operations.


%the trusted component \atsushi{Here explains that TNIC-log brings better memory efficiency than A2M, which could also be written in the first paragraph to highlight the advantage of TNIC}.

\myparagraph{Execution} Similarly to A2M, we expose three core operations: the \texttt{append}, \texttt{lookup}, and \texttt{truncate} operations to add, retrieve, and delete items of the log, respectively. A2M stores the lowest and highest sequence numbers for each log. Upon appending an entry, A2M increases the highest sequence number and associates it with the newly appended entry. When truncating the log, the system advances the lowest sequence number accordingly. We now show how we designed the operations using \projecttitle{} APIs.




\begin{algorithm}
\SetAlgoLined
\small
%\vspace{0.02cm}
\textbf{function} \texttt{append(id, ctx)} \{ \\
\Indp
 [$\alpha$,\texttt{i},\texttt{ctx}] $\leftarrow$ \texttt{local\_send(id,ctx)};\\
 \texttt{log[id].append(log\_entry($\alpha$,\texttt{i},\texttt{ctx}))};\\
 {\bf return} \texttt{[$\alpha$,\texttt{i},\texttt{ctx}]};\\
\Indm
\} \\

\vspace{0.15cm}

\textbf{function} \texttt{lookup(id, i)} \{ \\
\Indp
    {\bf return} \texttt{log[id].get(i)};\\
\Indm
\} \\
\vspace{0.15cm}
\textbf{function} \texttt{truncate(id, head, z)} \{ \\
\Indp
    [$\alpha$,\texttt{tail},\texttt{ctx}] $\leftarrow$ \texttt{append(id,} \textsc{trnc}\texttt{||id||z||head)};\\
        
    %[$\alpha_{2}$,\texttt{idx},\texttt{ctx}$_{2}$]
    \texttt{e} $\leftarrow$ \texttt{append(}\textsc{manifest}\texttt{,[$\alpha$,\texttt{tail},\texttt{ctx}])};\\
    {\bf return} \texttt{e};\\
\Indm
\} \\

\vspace{0.15cm}
\textbf{function} \texttt{verify\_lookup(id, e, head, tail)} \{ \\
\Indp
    \textbf{assert}(\texttt{e.i}$>=$\texttt{tail)};\\
    \texttt{local\_verify(id, e)};\\
\Indm
\} \\
\vspace{-1pt}
\caption{Attested Append-Only Memory (A2M) using \projecttitle{}.}
\label{algo:tnic_log}
\end{algorithm}

\myparagraph{Append operation} The \texttt{append(id,ctx)} operation takes a data item, \texttt{ctx}, and appends it to the log with identifier \texttt{id}. A log entry at index \texttt{i} is comprised of three items: the sequence number of that entry (\texttt{i}), the context of the entry (\texttt{ctx}), and the {\em authenticator} field, namely the digest of the \texttt{ctx||i} as in~\cite{levin2009trinc}. In our implementation, we additionally support the original A2M {\em authenticator} format calculated as the cumulative digest \texttt{c\_digest[i]} for that entry which is calculated as \texttt{c\_digest[i]=hash(ctx||sq||c\_digest[i-1])} where \texttt{c\_digest[0]=0}. The sequence number \texttt{i} is then increased to distinguish any entry that will be appended in the future. %With the cumulative digest we create a set of chains and as such we cannot his method does not cause any values to be forgotten.





\myparagraph{Lookup operation} The \texttt{lookup(id, i)} retrieves the log entry at index \texttt{i} of log with identifier \texttt{id}. Compared to A2M where lookups are compelled to access the trusted hardware, \projecttitle{}-log only performs a local memory access. 
The function does not verify whether the entry is legitimate. Developers need to implement the \texttt{verify\_lookup(id, entry, head, tail)} to verify the attestation. The boundaries of the log (i.e., \texttt{head} and \texttt{tail}) can always be retrieved by replaying a specific log, which keeps the state changes, the \textsc{manifest}. We explain how \textsc{manifest} works in the next paragraph.

\myparagraph{Truncate operation} The \texttt{truncate(id, head, z)}, where \texttt{z} is a nonce provided by the client for freshness, ``forgets'' all log entries with sequence numbers lower than \texttt{head}. A non-Byzantine client will never be able to successfully verify a forgotten entry of the log. To do that, \projecttitle{}-log makes use of an additional log \textsc{manifest}, which keeps the logs' state changes. First, the operation attests to the {\em tail} of the log by appending a specific entry which includes the nonce for a correct client to be later able to verify the operation. Then, the algorithm will append the last attested message of the log to the \textsc{manifest} log and return the attested message for the second append. To retrieve the boundaries of a log, clients can always attest to the tail of the \textsc{manifest} and read backward until they find a \textsc{trnc} entry.


\noindent\fbox{\begin{minipage}{\columnwidth}
\myparagraph{System design takeaway} \projecttitle{} minimizes the required TCB in A2M system while it offers faster lookup operations compared to its original version.
\end{minipage}}
%\atsushi{Can we explain more which point is improved thanks to TNIC?}

%Since the logs reside in the untrusted host memory, their integrity can be compromised by malicious adversaries. However, these adversaries cannot impersonate \projecttitle{} and generate verifiable attestations in any way. As such we do not worry about entries that are not written yet (these will never be verifiable). 
%If an adversary compromises the \textsc{MANIFEST}, \projecttitle{}-log might become responsive. However, this affects availability but not safary and it is beyond the scope of this work similar to other systems [A2M, Trinc, Damysus]. 


\subsection{Byzantine Fault Tolerance (BFT)}\label{sec:use_cases::byz_smr}
%\atsushi{unclear to me what is the advantage of using TNIC to implement the Byzantine SMR. Can we somehow highlight this point?}
%\dimitra{@Atnoni: Can we afford a network partition? 2f+1 w/ f=2, Assume Byz. leader and Byz. follower that drive execution with one correct replica---the others are on purpose exluded by the faulty leader. The client will have a correct reply always because it will wait for f+1 (=3) identical replies. Although if the most up to date correct replica afterwards is partitioned out then we just block; the remaining correct replicas will have lost one message and block until they get it ... }
%\pramod{ToDo: Pesudocode.}

As a second example for \projecttitle{} applications, we build a Byzantine Fault Tolerance protocol (BFT) that implements a State Machine Replication (SMR) based on a counter. Clients send increment counter requests to the SMR and receive the updated counter value. Despite its simplicity, this particular system can represent an ordering service that is a fundamental building block in various distributed applications ranging from event logging and databases systems to serverless and blockchain systems~\cite{rafthyperledger, Kafka, boki, 10.1145/3286685.3286686, scalog}. Our BFT combats equivocation through \projecttitle{} that serializes all messages. As such, \projecttitle{} optimizes the number of participant nodes compared to the classical BFT and further reduces the message complexity.

\myparagraph{System model} We consider a system of $N=2f+1$ replicas (or {\em nodes}) that communicate with each other over unreliable point-to-point network links. At most $f$ of these replicas can be Byzantine (aka {\em faulty}), i.e., can behave arbitrarily. The rest of the replicas are {\em correct}. Recall that classical BFT protocols require an extra set of $f$ replicas, in total $3f+1$ replicas to handle $f$ Byzantine failures. One of the replicas is the {\em leader} that drives the protocol, whereas the remaining replicas are (passive) followers. There is only a single active leader at a time.

For liveness, we assume a partial synchrony model~\cite{FLP, 10.1145/226643.226647}. We have only explored deterministic protocol specifications; the correct replicas begin in the same state, and receiving the same inputs in the same order will arrive at the same state, generating the same outputs. Lastly, as in classical BFT protocols, we cannot prevent Byzantine clients who otherwise follow the protocol from overwriting correct clients’ data.


\myparagraph{Execution} We implement BFT with \projecttitle{} as a leader-based State-Machine-Replication (SMR) protocol for a Byzantine model that stores and increases the value of the counter. The leader receives clients' requests to increment the counter. The leader, in turn, executes the protocol and applies the changes to its state machine---in our case, the leader computes and stores the next available counter value. Subsequently, the leader broadcasts the request along with some metadata to the passive followers. The metadata includes, among others, the leader's calculated output in response to the client's command, namely, the increased counter value the leader has calculated and the {\em state} of the followers known to the leader. In our implementation, the {\em state} is represented as the signed hash of the counter value of each follower (known to the leader).

The followers, in turn, execute and apply the incremented counter value to their state machines. However, they first attest to the leader's actions to detect misbehavior. To do so, they audit and validate its sent output through re-execution. Precisely, the followers except for their state machine, {\em simulate} leader's state machine. Each follower replica must add an extra counter representing the state the value counter is expected to have, leading to a $2\times$ extra space complexity. A follower will update the leader's value based on the commands received and then compare its calculated leader's value with the received one. In addition, the followers will validate the state of the replicas (including the leader and all other replicas). They only have to check if their previous state equals the other replicas' state.

After a replica validates the protocol, it applies the increments to its local counter. The followers will reply to the client with the result of the operation. In addition, they will reply to the leader, which will validate the follower's output. The leader, upon successful validation, will also reply to the client. The client can trust the result if they receive identical replies from the quorum, i.e., at least $f+1$ identical messages from different replicas (including the leader). This guarantees that at least one correct replica has responded with the correct result.


\myparagraph{Failure handling} Our strategy to verify the leader's execution jointly with the primitives of non-equivocation and transferable authentication offered by \projecttitle{} shields the protocol against a Byzantine leader. The leader cannot equivocate; even if it attempts to send different requests for the same round to different followers, executing the {\tt local\_send()} will assign different counter values that healthy followers can detect. As such, a leader in that case will be exposed. 

Likewise, the equivocation mechanism allow correct followers to discard stale message requests sent through replay attacks on the network. If a follower is Byzantine, a healthy leader or any healthy replica will detect it. For $f\geq2$, it is impossible for a faulty leader and, at most, $f-1$ remaining Byzantine follower to compromise the protocol. Either these faults will be detected by a healthy replica during the validation phase, or the protocol will be unavailable, i.e., if the leader in purpose only communicates with the Byzantine followers. This directly affects BFT correctness requirements; a client will never get at least $f+1$ matching replies. For protocols to progress in case of a faulty leader, they must pass through a recovery protocol or view-change protocols similar to those described in previous works~\cite{minBFT, Castro:2002}. Recovering is beyond the scope of this work, and as such, we did not implement it.


\noindent\fbox{\begin{minipage}{\columnwidth}
\myparagraph{System design takeaway} \projecttitle{} optimizes the replication factor and the message complexity of BFT.
\end{minipage}}

%\antonis{Could be phrased a bit better.}

%\begin{figure}[t!]
%    \centering
%    \includegraphics[width=0.6\linewidth]{figures/bft-pb.drawio.pdf} 
%    \caption{\projecttitle{} application on a Byzantine SMR protocol.\pramod{We can drop this figure.}} \label{fig:byz_smr}
%\end{figure}

\begin{algorithm}
\SetAlgoLined
\small
%\vspace{0.02cm}
\textbf{function} \texttt{leader(req)} \{ \\
\Indp
 {\tt output} $\leftarrow$ \texttt{execute(req)};\\
 {\tt msg} $\leftarrow$ \texttt{req||output};\\
 {\tt attested\_msg} $\leftarrow$ \texttt{local\_send(msg)};\\
 \texttt{rem\_write(}\textsc{followers[:]}{\tt, attested\_msg)};\\

{\bf upon reception of {\tt ack} from \textsc{followers}:}\\
    \Indp
        {\tt [{$\alpha$ || attested\_msg || follower\_output}]} $\leftarrow$ \texttt{upon\_delivery(ack)};\\
        {\bf assert(}\texttt{validate\_follower(attested\_msg, follower\_output)}{\bf)};\\
    \Indm

 \texttt{auth\_send(}\textsc{client}{\tt,msg)};\\
\Indm
\} \\

\vspace{0.15cm}

\textbf{function} \texttt{follower(msg)} \{ \\
\Indp
{\bf upon reception of {\tt msg}:}\\
    \Indp
        {\tt [{$\alpha$ || req || output}]} $\leftarrow$ \texttt{upon\_delivery(req)};\\
    \Indm

    {\bf assert(}\texttt{validate\_leader(req, output)}{\bf)};\\
    {\tt current\_output} $\leftarrow$ \texttt{execute(req)};\\
    
    \texttt{auth\_send(}\textsc{client}{\tt, req|| current\_output)};\\
    {\tt ack\_msg} $\leftarrow$ \texttt{msg|| current\_output};\\
    \texttt{auth\_send(}\textsc{middle}{\tt, ack\_msg)};\\
\Indm
\} \\
\vspace{-1pt}
\caption{BFT using \projecttitle{}.}
\label{algo:tnic_bft}
\end{algorithm}

%
%\dimitra{>Github code issues:
%\begin{itemize}
%    \item Line 239: has a logical bug regarding the message batching
%    \item Lines 205--210: unnecessary hash re-calcucations--- might improve performance if fixed
%    \item Continuation function needs improvement for correctness/completeness (leader should store the output for each on-going command).
%    \item For correctness, leader should have only one outstanding operation at a time.
%\end{itemize}}



%\lstinputlisting[language=C++]{codelets/pb.cc}




\subsection{Chain Replication (CR)}\label{sec:use_cases::byz_chain_rep}

\begin{algorithm}
\SetAlgoLined
\small
%\vspace{0.02cm}
\textbf{function} \texttt{head\_operation(req)} \{ \\
\Indp
 {\tt output} $\leftarrow$ \texttt{execute(req)};\\
 {\tt msg} $\leftarrow$ \texttt{req||output};\\
 \texttt{auth\_send(}\textsc{middle}{\tt,msg)};\\ \texttt{auth\_send(}\textsc{client}{\tt,msg)};\\
\Indm
\} \\

\vspace{0.15cm}

\textbf{function} \texttt{middle\_tail\_operation(msg)} \{ \\
\Indp
    {\bf assert(}\texttt{validate\_chain(msg)}{\bf)};\\
    {\tt output} $\leftarrow$ \texttt{execute(req)};\\
    {\tt chained\_msg} $\leftarrow$ \texttt{msg||output};\\
    {\bf if} (!\textsc{tail})\\
    \Indp
    \texttt{auth\_send(}\textsc{middle}{\tt,chained\_msg)};\\
    \Indm
    \texttt{auth\_send(}\textsc{client}{\tt,req||output)};\\
\Indm
\} \\
\vspace{0.15cm}
\textbf{function} \texttt{validate(msg)} \{ \\
\Indp
    \texttt{len} $\leftarrow$ \texttt{sz};\\
    \texttt{[req, out, cmt]} $\leftarrow$ \texttt{unmarshall(msg[0:len])};\\
    {\bf assert(}\texttt{memcmp(req, out)}{\bf)};\\
    {\bf assert(}\texttt{(cmt == expected\_cmt)}{\bf)};\\
    {\bf for} {\tt(i = 1; i < }{\sc node\_id}; {\tt i++)} \{\\
    \Indp
    \texttt{[out, cmt]} $\leftarrow$ \texttt{unmarshall(msg[len:len+\textsc{sz}])};\\
    {\bf assert(}\texttt{memcmp(req, out)}{\bf)};\\
    {\bf assert(}\texttt{(cmt == expected\_cmt)}{\bf)};\\
    \texttt{len} $\leftarrow$ \texttt{len} + \texttt{sz};\\
    \Indm
    {\bf return} {\tt True};\\
\Indm
\} \\
\vspace{-1pt}
\caption{Chain Replication using \projecttitle{}.}
\label{algo:tnic_chain_replication}
\end{algorithm}



We implement a Byzantine Chain Replication using \projecttitle{} that represents the replication layer of a Key-Value store. Chain Replication is a foundational protocol to build state-machine-replication and originally operates under the CFT model using $f+1$ nodes to tolerate up to $f$ failures. We show {\em how} to use \projecttitle{} to shield the protocol without changes to the core of the algorithm (states, rounds, etc.) while keeping the same replication factor.




\myparagraph{System model} We make the same assumptions for the system as in the BFT in Appendix $\S$~\ref{sec:use_cases::byz_smr}. For error detection and reconfiguration, we further assume a centralized (trusted) configuration service as in~\cite{10.1007/978-3-642-35476-2_24} that generates new configurations upon receiving reconfiguration requests from replicas. Recall that classical Chain Replication under the CFT model also relies on reliable failure detectors~\cite{chain-replication}. For liveness, we also assume that the configuration service will eventually create a configuration of correct replicas that do not intentionally issue reconfiguration requests to perform Denial-of-Service attacks. 

Clients send requests to {\tt put} or {\tt get} a value and receive the result. The replicas (e.g. head, middle, and {\em tail} nodes) are connected in a chained fashion, and the requests flow from the head node to the tail through the intermediate middle replicas. 

Similar to previous BFT protocols~\cite{Castro:2002, minBFT}, malicious primaries, i.e., the head, that do not forward the message intentionally are detected in clients' side and trigger reconfiguration.


\myparagraph{Execution} To execute a request \texttt{req}, e.g., {\tt put}/{\tt get}, a client first obtains the current configuration from the configuration service and sends {\tt req} to the head of the chain. The head orders and executes the request, and then it creates a {\em proof of execution message}, which is sent along the chain. The proof of execution includes the {\tt req} and the leader's action ({\tt out}) in response to that request. In our case, the leader sends the {\tt req} along with the commit index it has assigned to it. The message is then sent (signed) to the middle node that follows in the chain.

The middle node checks the validity of the message by verifying that the head's output is correct, executes the {\tt req}, and forwards the request to the following replica. Similarly, every other node executes the original request, verifies the output of all previous nodes, and sends the original request as well as a vector of all previous outputs. A replica needs to construct a {\em proof of execution message} that achieves one goal. It allows the following replicas in the chain to verify all previous replicas so far. As such the messages is of the form < < <{\tt req}, {\tt out$_{leader}$}>$_{\sigma_0}$, {\tt out$_{middle1}$}>$_{\sigma_1}$, .., {\tt out$_{tail}$}>$_{\sigma_N}$. The tail is the last node in the chain that will execute and verify the execution of the request. 


In contrast to the CFT version of the Chain Replication protocol, local operations in the tail, {\tt get} or {\tt ack} in a {\tt put} request cannot be trusted. 

As such the replicas in the chain need to reply to the clients with their output after they have forwarded their proof of execution message. Clients can wait for at least $f$ replicas replies and the tail reply to collide. Clients can execute the {\tt get} requests similarly to {\tt write} requests, which will traverse the entire chain, or clients can consult the majority and broadcast the request to $f+1$ replicas, including the tail. 


\myparagraph{Failure handling} By definition of the protocol, all nodes will see and execute all messages in the same order that is imposed by the head node. As such, all correct replicas will always be in the same state. In addition, network partitions that may split the chain into two (or more) individual chains that operate independently cannot affect safety: the clients will need to verify at least $f+1$ identical replies. In case a correct replica or a client detects a violation (by examining the proof of execution message or having to hear for too long from a node), they can expose the faulty node and request a reconfiguration.

\noindent\fbox{\begin{minipage}{\columnwidth}
\myparagraph{System design takeaway} \projecttitle{} {\em seamlessly} shields the Chain Replication system for Byzantine settings with the same replication factor as the original CFT system while it improves performance compared to its TEEs-based competitors.
\end{minipage}}

%\dimitra{>Github code issues:
%\begin{itemize}
%    \item check\_outputs function (L:67): Did you miss a validation step regarding transferable authentication?
%\end{itemize}}

%\lstinputlisting[language=C++]{codelets/cr.cc}
%\myparagraph{Write protocol} {\underline{Failure-free execution.}} The operation flows from the head to the tail. The leader executes the request and sends the original request as well as the output of that request. Every other node executes the original request, verifies the output of all previous nodes and sends the original request as well as a vector of all previous outputs. Lastly, the tail executes the request and replies back to the client with the proof of protocol execution.

%\myparagraph{Read protocol} The client can execute a read request similarly to the write request. As the requests can be re-ordered by malicious nodes, including the tail, local reads in the tail, in contrast to original CFT version of the protocol, cannot be trusted. As an optimization for the read-only requests, clients can consult the majority and broadcast the request to $f+1$ replicas including the tail. Since all replicas see all messages (by definition of the protocol), at least one replica will be correct and up to date. Therefore upon $f+1$ replies that collide, clients can trust the result.

%\myparagraph{{Failures detection}} The non-equivocation and transferable authentication primitives ensure that the nodes cannot impersonate others and cannot replay network traffic. As such, while malicious nodes might try deviate from the protocol in any way, e.g., re-order commands, compromise the commands, etc., correct replicas will detect misbehavior's. Even in the scenario where the tail node is faulty and execute the protocol erroneous, the fault will eventually be detected (upon access) as all nodes see all messages and thus, the vector of outputs will not be correct. %Lastly, the unprotected area can be compromised which is detected as the fpga\_cache will have for each key the lastly send command.



\subsection{Accountability (PeerReview)}
\label{sec:use_cases::accountability}





We implement an accountability protocol based on the PeerReview system~\cite{bftdetection, peer-review}. Compared to the previous three BFT systems that prohibit an improper action from taking effect, accountability protocols~\cite{268272, bftdetection, peer-review} slightly weaken the system (fault) model in favor of performance and scalability. Specifically, our protocol {\em allows} Byzantine faults to happen (e.g., correct nodes might be convinced by a malicious replica to permanently delete data). Still, it guarantees that malicious actions can always be detected. Accountability protocols can be applied to different systems as generic guards that trade security for performance~\cite{peer-review}, e.g., networked file systems, bit-torrent systems, etc. 

The original version of the system did not use trusted components and as such, incurred a high message complexity O($n^2$) to combat equivocation. \projecttitle{} to improve that message complexity.

\myparagraph{System model} We only detect faults that directly or indirectly affect a message, implying that {\em (i)} correct nodes 
can observe all messages sent and received by that node and {\em (ii)}  Byzantine faults that are not observable through the network cannot be detected. For example, a faulty storage node might report that it is out of disk space, which cannot be verified without knowing the actual state of its disks.

We further assume that each protocol participant acts according to a deterministic specification protocol. As such, detection can be accomplished even with a single correct machine, requiring only $f+1$ machines. This does not contradict the impossibility results for agreement~\cite{FLP} because detection systems do not guarantee safety.


\myparagraph{Execution} The participants communicate through network messages generated by \projecttitle{}.  In addition, each participant maintains a {\em tamper-evident} log that stores all messages sent and received by that node as a chain. A log entry is associated with an entry index, the entry data, and an authenticator, calculated as the signed hash of the tail of the log and the current entry data. 

We frame our protocol in the context of an overlay multicast protocol~\cite{10.1145/945445.945474} widely used in streaming systems. The nodes are organized as a tree where the streaming content (e.g., audio, video) flows from a source, i.e., {\em root} node, to clients ({\em children} nodes). To support many clients, each client can act as a source to other clients, which will be connected as children nodes. 

In our implementation, we consider nodes in a tree topology. The tree's height is equal to one, comprising one source node and two client (children) nodes connected to the source. Algorithm~\ref{algo:tnic_accountability_protocol} ($\S$~\ref{sec:use_cases-appendix}) shows the operations of our implemented accountability protocol. When the source sends a context (executes the \texttt{root()} function), it implicitly includes a signed statement that this message has a particular sequence number (generated by \projecttitle{}). The clients execute the {\tt child()} function that validates the received message, logs the received message, executes the result, and responds back to the source. 


\begin{algorithm}[t]
\SetAlgoLined
\small
%\vspace{0.02cm}
\textbf{function} \texttt{root(ctx)} \{ \\
\Indp
 \texttt{auth\_send(}\textsc{child}{\tt,ctx)};\\
 {\bf upon reception of \texttt{response}:};\\
 \Indp
    {\bf assert(}\texttt{validate\_reception(response)}{\bf)};\\
    \texttt{log(response)};\\
\Indm
\Indm
\} \\

\vspace{0.15cm}

\textbf{function} \texttt{child($\alpha$||cmd||seq)} \{ \\
\Indp
    {\bf assert(}\texttt{validate\_reception($\alpha$||cmd||seq)}{\bf)};\\
    \texttt{log($\alpha$||cmd||seq)};\\
    {\tt result} $\leftarrow$ \texttt{execute(cmt)};\\
    {\tt response} $\leftarrow$ \texttt{log(result||cmd)};\\
    \texttt{auth\_send(}\textsc{root}{\tt, response)};\\
\Indm
\} \\
\vspace{0.15cm}
\textbf{function} \texttt{log\_audit()} \{ \\
\Indp
    {\bf{while}} \texttt{last\_id < log\_tail} \{\\
    \Indp

        \texttt{entry} $\leftarrow$ \texttt{validate\_log\_entry\_at(last\_id)};\\
        \texttt{last\_id++};\\
        {\bf assert(}\texttt{replay(entry)}{\bf{)}};\\
    \Indm
    \}\\
\Indm
\} \\
%\vspace{-1pt}
\caption{ (PeerReview) Accountable systems using \projecttitle{}.}
\label{algo:tnic_accountability_protocol}
\end{algorithm}


Each node is assigned to a set of {\em witness} processes to detect faults. Similarly to the original system, we assume that the set of nodes and its witnesses set {\em always} contain a correct process. The witnesses audit and monitor the node's log. To detect bad behaviors (or expose non-responsive nodes), the witnesses read the node's log and replay it to run the participant’s state machine. As such, they ensure the participant’s state is consistent with proper operation. 

Specifically, each witness for a participant node keeps track of n, a log sequence number, and s, the state that the participant should have been in after sending or receiving the message in log entry n. It initializes n to 0 and s to the initial state of the participant.

Whenever a witness wants to audit a node, it sends its n and a nonce.
The participant returns an attestation of all entries between n and its current log entry using the nonce. The witness then runs the reference implementation, starting at state $s$, and progressing through all the log entries. If the reference implementation sends the same messages in the log, then the witness updates n, which is the state of the reference implementation at that point. If not, then the witness has proof it can present of the participant’s failure to act correctly.




The original PeerReview system requires a receiver node to forward messages to the original sender’s witnesses so they can ensure this message is {\em legitimate}, i.e., it appears in the sender’s log. No other conflicting message is sent to another peer (equivocation). As such, a peer must communicate (in every round) with the witness set of any other peer, leading to a quadratic message complexity. \projecttitle{} eliminates the overhead; in fact a participant that sends or receives a message needs to attest and append the message and its attestation in each own log. A participant can process received messages only if they are accompanied by attestations generated by the sender's \projecttitle{} hardware. 



\noindent\fbox{\begin{minipage}{\columnwidth}
\myparagraph{System design takeaway} \projecttitle{} can be used to optimize accountable systems, e.g., PeerReview, while optimizing their message complexity thanks to how it handles equivocation.
\end{minipage}}


%As demonstrated with A2M, TrInc can
%easily supply a trusted log without the assistance of a
%witness set. Our first modification is to include such a
%trusted log. Whenever a participant sends or receives a
%message, it logs that message with an attestation from
%its trinket. A participant should only process a received
%message if it is accompanied by an attestation that the
%message has been logged by the sender’s trinket.


%As the source might cheat by equivocating with witnesses regarding the messages it sends to clients \atsushi{what does it mean?}, the root is also required to log the (signed) response received by clients that includes proof that clients executed the protocol. That could be either the client's hash of its own log or, in our particular example, the command the client received from the source. 

%Lastly, the source might on purpose 
%For this reason, when a participant receives a message from another, it also logs the attestation generated forwards this message to the
%sender’s witnesses, so they can ensure this message actually appears in the sender’s log.

\if 0
\begin{itemize}
    \item (f+1) nodes
    \item deterministic actions (let's say it is a log for NFS or a DB)
    \item each node: has an in-memory (append-only) log
    \item each message: signed(H) || H (payload) || payload = {action, log\_index}
    \item nodes cannot equivocate + cannot impersonate others -> we mask Byz-faults as network partition (or slow replicas)
    \item Correct nodes will identify unresponsive nodes and can prove their correctness by auditing their log with their own log
    \item Logs are chains of the history -> tamperproof
\end{itemize}
\fi


\if 0
\dimitra{SOOOOS -> finalize the pseudocode for all protocols + recovery}

\dimitra{
Motivate the replication protocols in the context of widely-adopted generally purposed DS (transactions, storage system, shared log).
\begin{itemize}
    \item Raft (use cache for replicated distributed txs~\cite{cockroachdb_raft})
    \item Chain replication (use case for fault-tolerant distributed storage system~\cite{corfudb})
    \item AllConcur (use case for a leaderless shared-log~\cite{})
\end{itemize}
}
\dimitra{SOS -> discuss correctness of recovery (reconciliation)}


\subsection{Raft}

\myparagraph{Write protocol} {\underline{Failure-free operation.}} The leader executes the client request, e.g., assigns to it an id, stores it in a queue with all on-going requests and generates an output (the assigned id with hash of the command). Then it broadcasts the message to all followers (replication phase). A follower executes the protocol, replicating the command but also verify that the request from the leader is generated through a valid execution scenario. In other words, it simulates the leader's protocol. As a result, it sends back its output (as a follower) and the calculated output (as a leader). When the leader receives $f+1$ identical replies, which denotes that at least one correct replica validated that the leader executed the protocol's round correctly. The execution returns back to the untrusted cpu where the leader procceeds with the second round. 

The leader replies back to the client assigning the proofs of the majority (for the protocol execution). 

\lstinputlisting[language=C++]{codelets/raft.cc}

\myparagraph{Read protocol} Read commands can be executed identically to writes as leaders can violate correctness and consistency guarantees by returning valid, yet stale replies to the client. As an optimization, read-only commands can be executed as a multicast broadcast to all replicas, waiting for $f+1$ identical replies.


\myparagraph{Failure detection} The non-equivocation layer guarantees that all followers will receive the messages in the same order and will not miss any past messages. The transferable authentication layer guarantees that a faulty replica, including the leader, cannot impersonate other nodes and compromise the protocol. For example, the client expects $f+1$ identical proofs that at least $f+1$ replicas have executed and validated the protocol.
\fi



\if 0
\subsection{Decentralized atomic broadcast}
We implement a decentralized (leaderless) atomic broadcast based on~\cite{}, \texttt{BFTcast}. All replicas receive all messages and apply the requests in a pre-defined order.

\myparagraph{Write protocol} \noindent{\underline{Failure-free execution.}} Each node multicasts (a batch of) requests to every other node. The first round is completed when all nodes have received all messages of the protocol. Then each node executes the requests in the pre-defined order, e.g., based on the node id and computes the outcome (e.g., certificate) which is send as a confirmation to every other node. Similarly to the \texttt{BFTcr}, \texttt{BFTcast} replicas are receiving all messages from all other replicas in the system. As such, misbehavior of Byzantine nodes might only affect availability but not safety.


\lstinputlisting[language=C++]{codelets/decentralized_atomic_broadcast.cc}


\myparagraph{Read protocol} In line with the previous protocols clients cannot trust individuals. Reads can be executed similarly to writes, requiring all replicas to execute the request. As an optimization, clients can consult the majority ($f+1$) by broadcasting the reads and requiring as in classical BFT protocols at least $f+1$ identical replies.


\myparagraph{{Failures detection}} The transferable authentication primitives guarantees that replicas cannot impersonate others, therefore correct replicas can distinguish if the protocol has been executed correctly. The non-equivocation primitive guarantees that all replicas will see the same message for the same round. Byzantine nodes can violate the pre-defined order of execution. Thanks for the all-to-all communication of the second phase of the protocol whose goal is to verify that everyone executed the protocol correctly, failures will be detected from the correct replicas which will halt the protocol. Note that in this protocol we do not require the fpga\_cache as in each round the correct replicas will have the correct value of the log tail.

\subsection{Dealing with failures} In the case of faulty followers, the service remains available because the correct replicas (majority) include the leader. Consequently, the protocol is executed correctly.

In case the leader is faulty, the protocol execution can be violated. This in turn might lead to replicas states not being consistent, the system not making any progress and the clients not receiving valid responses to their requests. 

Followers detect the leader's failures (e.g., the leader deviates from the protocol or performs Denial-of-Service attacks to all or some servers), by \emph{(i)} validating the leader's execution and \emph{(ii)} using timers (to detect an un-responsive leader), in similar fashion as in~\cite{Veronese2013EfficientBF, Castro:2002, 10.1007/978-3-642-35476-2_24}. The use of timers do not violate safety, instead it might trigger a view-change operation and recovery. Specifically, when a replica $s_i$ times-out it broadcasts to all a \texttt{prep-view-change} request. When the replica $s_i$ receives $(f+1)$ positive responses to its prep-view-change request (which implies that at least one healthy replica has agreed to perform this view-change), then it broadcasts to all a view-change request. The view change request includes the new epoch and a concatenation of the log. At this point, that replica stops accepting
messages for the current view. A replica receives at least $(f+1)$ logs from other replicas so that it will receive at least the biggest log. The $f+1$ healthy replicas will need to decide on a new leader, e.g., in Raft or Chain-replication and then prior to executing new reqs, the replicas needs to synchronize their state based on the logs. The new leader, needs to re-execute the protocol for each request in the ``longest'' log. Similarly, with the leader-validation the replicas can detect faulty leaders.




A faulty leader can introduce bogus operations by itself. However, this is indistinguishable from a Byzantine client that executes bogus operations and BFT does not protect against malicious clients that want follow the protocol. To avoid cases where the leader executes requests to overwrite the data, the message format could also include the clients' request. The replicas should then only executed requests that were originated by authenticated clients.

\fi