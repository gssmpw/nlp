


\begin{table}
\begin{center}
\footnotesize
\begin{tabular}{ |c|c|c| } 
 \hline
 System &  (host) TEE-free & Tamper-proof \\ [0.5ex] \hline \hline
 SSL-lib & Yes & No\\
 SSL-server/Intel-x86*/AMD  &  Yes & No\\
 SGX/AMD-sev &  No & Yes\\
% TEE-P [Hybster, Damysus, Trinc] & Yes & Yes & Yes & Assumes trusted local persistent state\\
 %TEE-DS & Yes & Yes & Yes & Builds a DS of $f+2u+1$ TEEs\footnote{$f$ is the compromised TEEs and $u$ is the number of unresponsive TEEs}\\
 \projecttitle{} & Yes & Yes\\
 \hline
\end{tabular}
\end{center}
%\vspace{-10pt}
\caption{Host-sided baselines and \projecttitle{}. (*) We use the term SSL-server for this run unless stated otherwise.}
% \caption{(Trusted) Host-sided hardware baselines and \projecttitle{}. (*) We also use the term SSL-server for this run unless stated otherwise.}
\label{tab:hw_options}
\vspace{-8pt}
\end{table}

\section{Evaluation}
\label{sec:eval}

We evaluate \projecttitle{} across three dimensions: {\em (i)} 
hardware (\S~\ref{subsec:hw_eval}), {\em (ii)} network stack (\S~\ref{subsec:net_lib}) and {\em (iii)} distributed systems (\S~\ref{subsec:use_cases_eval}).

% \if 0
% \myparagraph{Implementation}
% We implement our prototype of \projecttitle{} extending the Coyote~\cite{coyote} codebase on Alveo U280~\cite{u280_smartnics}. We build the attestation kernel based on the HMAC module from the Vitis Security Library~\cite{vitis-security-lib} with SHA-384 as the hashing algorithm. For the data transfers, \projecttitle{} builds on top of an XDMA IP~\cite{xdma, fpga_dma} that enables DMA over PCIe. The 100Gb MAC is implemented with a CMAC IP~\cite{license} that exposes two 512-bit AXI4-Stream interfaces to the RoCE protocol kernel for the transmitting (Tx) and receiving (Rx) network paths.


% Our implementation is based on~\cite{coyote}, a fork of which has been used in prior works~\cite{strom} showing that 500 queue pairs (QPs) occupy 9\% of the on-chip memory, while the logic resource usage remains below 1\% when scaling from 500 to 16,000 QPs. Our evaluation and modern deployments use more powerful FPGAs, suggesting that even a larger number of connections could be supported compared to the work in~\cite{strom}. 
% Nevertheless, our \projecttitle{} does not assume a specific FPGA board. Therefore, the findings from previous works on other boards are still relevant.

% \fi 
%Precisely, we extend the data path of Coyote, adopting the abstraction of {\em virtual} FPGA areas to plug in our attestation functionality. \projecttitle{} leverages the Coyote paradigm and wires the control domain and the data domain into different paths.





%The data flow from the host memory to the device memory using the Xilinx streaming protocol (i.e., \texttt{hls\_stream<>}) that divides data into 512-bit packets. 
%At the transmission path, we implemented and plugged in two FIFO queues that redirect the packets from the host memory to the device memory -- specifically into the HMAC module and the RoCE IP. The role of the first queue is to buffer data and calculate the HMAC of all packets of the transmission.
%The module is based on the HMAC module in the Xilinx Vitis Security Library using SHA-384 as the hash function~\cite{vitis-security-lib}.
%The data of the second queue is passed through to the RoCE IP until the last packet of the transmission is reached.
%When the HMAC has been calculated, the last packet of the transmission is replaced by the 384-bit HMAC padded to 512 bits.

%At the reception path, we also augment the stack with two queues.
%The first queue sends the data to the verification module that will calculate the {\em expected} HMAC for verification. If the verification process succeeds we reconstruct the received message and deliver it to the application layer (host memory). Otherwise, all corresponding packets are dropped which naturally affects liveness.

\if 0
\myparagraph{Implementation}
We implement \projecttitle{} extending Coyote~\cite{coyote} codebase. Precisely, we extend the data path of Coyote, adopting the abstraction of {\em virtual} FPGA areas to plug in our attestation functionality. \projecttitle{} leverages the Coyote paradigm and wires the control domain and the data domain into different paths.

The RoCE kernel input and output data cables, e.g., \texttt{axis\_rdma\_sink} and \texttt{axis\_host\_sink}, are connected through the Attestation kernel through 64B data paths (512-bit AXI4-Stream interfaces). Then, the attestation kernel and the host memory communication are achieved through an AXI4 memory-mapped Master interface using the Xilinx streaming protocol. It is the responsibility of the host code to allocate and initialize host memory. Further, the kernel is connected to a 32B bus to receive the parameters, a 20 B bus to issue RDMA write operations, and a 12 B bus to issue local DMA commands.

%The data flow from the host memory to the device memory using the Xilinx streaming protocol (i.e., \texttt{hls\_stream<>}) that divides data into 512-bit packets. 
At the transmission path, we implemented and plugged in two FIFO queues that redirect the packets from the host memory to the device memory -- specifically into the HMAC module and the RoCE IP. The role of the first queue is to buffer data and calculate the HMAC of all packets of the transmission.
The module is based on the HMAC module in the Xilinx Vitis Security Library using SHA-384 as the hash function~\cite{vitis-security-lib}.
The data of the second queue is passed through to the RoCE IP until the last packet of the transmission is reached.
When the HMAC has been calculated, the last packet of the transmission is replaced by the 384-bit HMAC padded to 512 bits.

At the reception path, we also augment the stack with two queues.
The first queue sends the data to the verification module that will calculate the {\em expected} HMAC for verification. If the verification process succeeds we reconstruct the received message and deliver it to the application layer (host memory). Otherwise, all corresponding packets are dropped which naturally affects liveness.
\fi 

\myparagraph{Evaluation setup}
We perform our experiments on a real hardware testbed using two clusters: AMD-FPGA Cluster and Intel Cluster. AMD-FPGA Cluster consists of two machines powered by AMD EPYC 7413 (24 cores, 1.5 GHz) and 251.74 GiB memory. Each machine also has two Alveo U280 cards~\cite{u280_smartnics} that are connected through 100 Gbps QSFP28 ports. Intel Cluster consists of three machines powered by Intel(R) Core(TM) i9-9900K (8 cores, 3.2 GHz) with 64 GiB memory and three Intel Corporation Ethernet Controllers (XL710).





%, caches: 32 KiB (L1 data and code), 256 KiB (L2) and 16 MiB (L3). Cluster 2 nodes are connected over a 40GbE QSFP+ network switch.


%, caches: 32 KiB (L1 data and code), 512 KiB (L2) and 32 MiB (L3). Each machine also has two U280 Alveo cards~\cite{u280_smartnics} that are connected over a 100 Gbps cable.
%\atsushi{Can we simplify the above like this: Cluster 1 is equipped with two machines powered by AMD EPYC 7413 (24 cores, 1.5 GHz) and 251.74 GiB memory.}

%Cluster 2 is equipped with three machines with CPU (marked as CPU-2 in the respective plots): Intel(R) Core(TM) i9-9900K (8 cores, 3.2 GHz), memory: 64 GiB, caches: 32 KiB (L1 data and code), 256 KiB (L2) and 16 MiB (L3). Cluster 2 nodes are connected over a 40GbE QSFP+ network switch.
%\antonis{CPU-1 and CPU-2 are not memorable names, consider AMD, x86?} %\atsushi{I agree with Antonis. Also, can we rename Cluster 1 and 2 like this: AMD-FPGA Cluster and Intel Cluster?} \atsushi{Cluster 2 is equipped with three machines powered by Intel(R) Core(TM) i9-9900K (8 cores, 3.2 GHz) with 64GiB memory.}

%We perform our experiments on a real hardware testbed using two clusters: Cluster 1 and Cluster 2. Cluster 1 is equipped with two machines with CPU (marked as CPU-1 in the respective plots): AMD EPYC 7413 (24 cores, 1.5 GHz), memory: 251.74 GiB, caches: 32 KiB (L1 data and code), 512 KiB (L2) and 32 MiB (L3). Each machine also has two U280 Alveo cards~\cite{u280_smartnics} that are connected over a 100 Gbps cable.
%\atsushi{Can we simplify the above like this: Cluster 1 is equipped with two machines powered by AMD EPYC 7413 (24 cores, 1.5 GHz) and 251.74 GiB memory.}

%Cluster 2 is equipped with three machines with CPU (marked as CPU-2 in the respective plots): Intel(R) Core(TM) i9-9900K (8 cores, 3.2 GHz), memory: 64 GiB, caches: 32 KiB (L1 data and code), 256 KiB (L2) and 16 MiB (L3). Cluster 2 nodes are connected over a 40GbE QSFP+ network switch.
%\antonis{CPU-1 and CPU-2 are not memorable names, consider AMD, x86?} %\atsushi{I agree with Antonis. Also, can we rename Cluster 1 and 2 like this: AMD-FPGA Cluster and Intel Cluster?} \atsushi{Cluster 2 is equipped with three machines powered by Intel(R) Core(TM) i9-9900K (8 cores, 3.2 GHz) with 64GiB memory.}



%We evaluate the performance of \projecttitle{} across three dimensions: {\em (i)} the hardware evaluation for the Attestation kernel performance (\S~\ref{subsec:hw_eval}), {\em (ii)} the network library evaluation which assesses and compares the \projecttitle{} with competitive network stack baselines (\S~\ref{subsec:net_lib}) and {\em (iii)} our four secure primitive-examples constructed on top of \projecttitle{} (\S~\ref{subsec:use_cases_eval}). 
% We evaluate the performance of \projecttitle{} across three dimensions: {\em (i)} the hardware evaluation which focuses on the Attestation kernel evaluation (\S~\ref{subsec:hw_eval}), {\em (ii)} the network library evaluation which assesses and compares the \projecttitle{} with competitive network stack baselines (\S~\ref{subsec:net_lib}) and {\em (iii)} different use-cases we built to evaluate our four secure primitive-examples we constructed on top of \projecttitle{} (\S~\ref{subsec:use_cases_eval}). 


\subsection{Hardware Evaluation: \trustedfpga{}}
\label{subsec:hw_eval}




\myparagraph{Baselines} 
We evaluate the performance of {\tt Attest()} of the \projecttitle{}'s attestation kernel  ($\S$~\ref{subsec:nic_attest_kernel}) compared with four host-sided systems shown in Table~\ref{tab:hw_options}. For these host-sided versions, we build OpenSSL \rev{C3}{v3.1} servers that run natively or within a TEE \rev{C3}{with the same BIOS configuration (AES-NI enabled)}. The servers attest and forward network messages to the host application. We use the terms Intel-x86 and AMD for a native run of the server process (SSL-server) and SGX and AMD-sev for their tamper-proof versions within a TEE. 
\rev{B5}{The TEE baselines follow the same system model as in state-of-the-art hybrid systems~\cite{10.1145/3492321.3519568, minBFT, 10.1145/2168836.2168866, levin2009trinc}, where the host BFT application runs on the untrusted CPU and communicates with a separate TEE-based process to generate and verify message attestations.}
\rev{D6}{\projecttitle{} implements similar abstractions for counter and message attestation. Thus, \projecttitle{} does not introduce additional protocol alterations compared to them.}%as those used in the hybrid systems.
% (SGX, AMD-sev)
The server and host process run in the same machine to eliminate network latency and communicate through TCP sockets. We implement SGX using the {\sc scone} framework~\cite{scone} while AMD-sev runs in a QEMU VM using the official VM image~\cite{AMDSEV}. In addition, we build (non-temper-proof) SSL-lib, which integrates the {\tt Attest} function as a library. 
% : two untrusted native systems (Intel-x86 and AMD) and two TEE-based systems (SGX and AMD-sev). 
% Recall that both SSL-lib and SSL-server are not tamper-proof. 
% Table~\ref{tab:hw_options} summarizes our baselines with \projecttitle{}. 
%To avoid distortion, our plots do not include Trinc~\cite{levin2009trinc} because its reported latencies are an order of magnitude slower (85-105 ms) than all other evaluated solutions.

% \rev{B5}{We use the same system model as in state-of-the-art hybrid systems~\cite{10.1145/3492321.3519568,minBFT}, where the BFT application code runs on the (untrusted) CPU and communicates with a separate TEE-based process to generate and verify message attestations. Avocado~\cite{avocado}, which constructs a trusted computing base (TCB) encompassing both the protocol code and the attestation/verification layer, targets a weaker threat model than \projecttitle{}. Importantly, Avocado assumes that all participating TEEs adhere to the protocol as specified, whereas \projecttitle{} can tolerate up to $f$ nodes that deviate from the protocol.}

% We evaluate the performance of {\tt Attest()} of the \projecttitle{}'s attestation kernel  ($\S$~\ref{subsec:nic_attest_kernel}) compared with four host-sided systems shown in Table~\ref{tab:hw_options}. For these host-sided versions, we build OpenSSL servers that run natively or within a TEE. The servers attest and forward network messages to the host application. We use the terms Intel-x86 and AMD for a native run of the server process (SSL-server) and SGX and AMD-sev for their tamper-proof versions within a TEE. The server and host process run in the same machine to eliminate network latency and communicate through TCP sockets. We implement SGX using the {\sc scone} framework~\cite{scone} while AMD-sev runs in a QEMU VM using the official VM image~\cite{AMDSEV}. In addition, we build (non-temper-proof) SSL-lib, which integrates the {\tt Attest} function as a library. 

%(Table~\ref{tab:hw_options})
%Nat-lib is an OpenSSL-based library integrated into the code logic that generates and verifies messages. Nat is an OpenSSL-based server that communicates through kernel TCP sockets with the interested process to attest and verify its messages. \atsushi{can we shorten the description of Nat and Nat-lib?} Similarly, SGX and AMD-sev are OpenSSL-based servers that run within a tamper-proof TEE---specifically, Intel SGXv1~\cite{cryptoeprint:2016:086} and AMD-sev~\cite{amd-sev} (configured without offering confidentiality), respectively.  We implement the SGX-based server using the {\tt SCONE} framework~\cite{scone} to execute (exit-less) syscalls in a performant fashion. We run AMD-sev as a QEMU VM using the official supported kernel image~\cite{AMDSEV}, which efficiently runs un-modified Linux applications. Experiments with CPU-1, CPU-2, and SGX ran on Cluster 2, whereas we ran AMD-sev experiments on Cluster 1 \atsushi{CPU-1 should also be on Cluster 1, right?}. 
%On purpose, we exclude  Trinc~\cite{levin2009trinc} because it reported latencies that are an order of magnitude slower (85-105 ms) than \projecttitle{}. 
%To avoid distortion, our plots do not include 
%Trinc~\cite{levin2009trinc} because its reported latencies are an order of magnitude slower (85-105 ms) than all other evaluated solutions.
% \projecttitle{}

%\antonis{Differences between Nat and Nat-lib are not clear. Also, names could be more descriptive/memorable (Nat-lib is also very similar to NET-LIB).}

%\atsushi{can we remove this paragraph? (due to the space)} In short, we seek to answer the following research questions:
%\begin{itemize}
%\item {\bf{RQ1.}} What is the performance for generating and verifying attested messages in \projecttitle{}?
%\item {\bf{RQ2.}} What is the performance breakdown and latencies?
%    \item[\bf{RQ3}]  How much resources \projecttitle{} uses for the attestation kernel (Table~\ref{table:resources_usage})?
 %   \item[\bf{RQ4}] What is the hardware network latency and throughput (Fig~\ref{fig:hw_lat_breakdown})?
%\end{itemize}

\myparagraph{Methodology and experiments}
We use Vitis XRT v2022.2 and the shell \texttt{xilinx\_u280\_gen3x16\_xdma\_base\_1} for the stand-alone evaluation of the \projecttitle{} attestation kernel: synchronous data transfers between the host and device (U280). We measure and report the average latency and communication costs by executing an empty function body of \texttt{Attest()}.
%To isolate the latencies between data transfers and computation we further execute the same experiment without computing the HMAC (empty kernel). 

% We use Vitis XRT v2022.2 for the stand-alone evaluation of \projecttitle{} attestation kernel. We load the shell \texttt{xilinx\_u280\_gen3x16\_xdma\_base\_1} and use Vitis XRT synchronous data transfers from host to device and vice versa. We measure and report the {\tt Attest} function average latency as well as the communication costs executing an empty funcation body.

%We use Vitis XRT v2022.2 for the stand-alone evaluation of \projecttitle{} attestation kernel on top of which we build the host and the \trustedfpga{} processes (\texttt{xilinx\_u280\_gen3x16\_xdma\_base\_1}). We use Vitis XRT synchronous data transfers from host to device and vice versa. We measure and report the HMAC average latency on these competitive systems. To isolate the latencies between data transfers and computation we further execute the same experiment without computing the HMAC (empty kernel). \atsushi{I wanna simplify the texts here... work on it later}


\myparagraph{Results}
Figure~\ref{fig:attest_kernel} shows the average latency of {\tt Attest()} based on the HMAC algorithm for 64B and 128B data inputs. The latency of {\tt Verify()} is similar, and as such, it is omitted. Our \projecttitle{} achieves performance in the microseconds range (23 us) and outperforms its equivalent TEE-based competitors at least by a factor of 2. Importantly, \projecttitle{} is approximately 1.2$\times$ faster than AMD, which is not tamper-proof. 
% although it is approximately 2$\times$ slower than Intel-x86. Recall that neither AMD nor Intel-x86 are tamper-proof.

Figure~\ref{fig:latency_breakdown} shows the latency breakdown of {\tt Attest()}. Accessing the \projecttitle{} device and TEEs can be expensive, ranging from 30\% to 90\% of the total operation latency among the systems. 
Regarding \projecttitle{}, the transfer time (16us) accounts for 70\% of the execution time. We expect that \projecttitle{} effectively eliminates this cost by enabling asynchronous (user-space) DMA data transfers. 
% Specifically, for \projecttitle{}, the transfer time takes about 16 us, which accounts for 70\% of the execution time. This is not a concern in \projecttitle{} as it effectively eliminates this cost by enabling asynchronous (user-space) DMA data transfers. 
% Regarding the native runs, i.e., Intel-x86 and AMD, the communication costs account for $\sim$90\% of the latency.  
% Figure~\ref{fig:latency_breakdown} shows the latency breakdown of {\tt Attest()}. Accessing the \projecttitle{} device and the TEEs can be expensive ranging from 30\% to 90\% of the total operation latency among the systems. Specifically, for \projecttitle{}, the transfer time takes about 16 us, which accounts for 70\% of the total execution time. This is not a concern in \projecttitle{} as it effectively eliminates this cost by enabling asynchronous (user-space) DMA data transfers. Regarding the native runs, i.e., Intel-x86 and AMD, the communication costs including the syscalls execution and data transfers between the two processes account for the $~$90\% of the latency.  
% 
Regarding the TEE-based systems (SGX, AMD-sev), the communication and system call execution costs account for up to 40\% of the total execution. To our surprise, this implies that the HMAC computation within any of the two TEEs experiences more than 30$\times$ overheads compared to its native run. To analyze TEEs' behavior, we instrument the client's code to measure the operations' individual latency at various periods of time during the experiment accurately. 
% Our evaluation shows that in the TEE-based systems, e.g., SGX and AMD-sev, the communication and system call execution costs account for up to 40\% of the total execution (on average). To our surprise, this implies that the HMAC computation itself within any of the two TEEs experiences more than 30$\times$ overheads compared to its native run. We further analyzed TEEs' behavior instrumenting the client's code to accurately measure the operations' individual latency at various periods of time during the experiment. 

Figure~\ref{fig:latency_distribution} illustrates the individual operation latency, where SGX-empty indicates SGX without HMAC computation. 
% for three systems: SGX, SGX-empty (SGX without HMAC computation), and Intel-x86. 
% SGX (the SSL-server runs in the SGX enclave), SGX-empty (the SGX SSL-server without HMAC computation) and Intel-x86 (the SSL-server runs natively). 
As shown in Figure~\ref{fig:latency_distribution}, the HMAC execution within the TEE often experiences huge latency spikes. 
% quite often experiences huge spikes in latency. 
% While these spikes are very frequent, they are not guaranteed. 
% We calculate an average mean of 45us and a geometric mean of 30 us. 
\rev{A6}{We attribute this behavior to the scheduling effects and asynchronous exitless system calls inherent in our SGX framework, {\sc scone}~\cite{scone}. We observe similar latency variations during executions on AMD systems, spiking up to 200--500 us. }
% \atsushi{@Dimitra: The reviewer says, "transitions to in and out of SGX system typically involve TLB flushes but what does an asynchronous exitless system call in SCONE correspond to?" Do we have an answer?}
% We observe similar variations for AMD with latencies spiking up to 200--500 us.

% We attribute this behavior to the scheduling effects and asynchronous exitless system calls inherent in our SGX framework, Scone~\cite{scone}. Similar latency variations were also observed during executions on AMD systems


% Figure~\ref{fig:latency_distribution} illustrates the individual operations latency for three systems: SGX (which runs the HMAC within an SGX SSL-server), SGX-empty (the SGX SSL-server copies and returns the input data without HMAC computation) and Intel-x86 (the SSL-server runs natively).  As shown in Figure~\ref{fig:latency_distribution}, the execution of HMAC within the SGX quite often experiences huge spikes in latency. While these spikes are very frequent, they are not guaranteed. We calculate an average mean of 45us and a geometric mean of 30 us. We attribute this behavior to scheduling effects and the asynchronous exitless system call API which is involved~\cite{scone}. Similar variations were observed for the AMD runs with latencies spiking up to 200--500 us.

\if 0
\begin{figure*}[t!]
\begin{center}
\minipage{0.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{atc-submission-plots/hw_eval_attest_latency.pdf} 
  \caption{HMAC (Attest) latency}
  \label{fig:attest_kernel}
\endminipage
\minipage{0.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{atc-submission-plots/latency_breakdown.pdf}
  \caption{Latency breakdown}\label{fig:latency_breakdown}
\endminipage

\minipage{0.50\textwidth}%
  \centering
  %\includegraphics[width=\linewidth]{atc-submission-plots/lat_distribution_sgx.pdf}
  \includegraphics[width=\linewidth]{atc-submission-plots/foo50.pdf}
  \caption{Latency distribution over time\dimitra{increase font size}}\label{fig:latency_distribution}
\endminipage
\end{center}
%\caption{Performance evaluation of the trusted component in different hardware setups.} \label{fig:hw_eval}
\end{figure*}
\fi

\if 0
\begin{figure}[t!]
    \centering
    \includegraphics[width=.5\textwidth]{eval-plots/plots/hw_net_lat_throughput.pdf}
    \caption{Throughput latency plots for network operation.}
    \label{fig:hw_lat_breakdown}
\end{figure}
\fi

\if 0
\begin{center}
\begin{table}[ht]
\centering
\begin{tabular}{ |m{1.5cm}||m{1cm}| m{1cm}| m{1cm}|}
 \hline
  & LUTs &  BRAM & Regs  \\
 \texttt{Attest()} &  &  &\\
 \texttt{Verify()} &  &  &\\
 \hline
 \end{tabular}
\caption{Resources usage.}
\end{table}\label{table:resources_usage}
\end{center}
\fi




\begin{figure}
    \centering
   \includegraphics[width=0.75\linewidth]{atc-submission-plots/rpc_thr.pdf}
   \vspace{-10pt}
    \caption{Throughput of send operations across the three selected network stacks.}
    % \caption{Throughput evaluation of send operations for various packet sizes across five competitive network stacks with various security properties.}
    \vspace{-4pt}
  \label{fig:net_throughput}
\end{figure}