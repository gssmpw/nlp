\section{\rev{(a)}{\projecttitle{} Network Stack}}
% \section{Trusted Network Stack}
\label{sec:t-nic-network}




 
We build a software \projecttitle{} system network stack that operates as the {\em middle layer} between the \projecttitle{} programming APIs (see $\S$~\ref{sec:net-lib}) and the hardware implementation of \projecttitle{}. 
Figure~\ref{fig:network_stack_design} shows an overview of the network stack design that is comprised of two core components: {\em (1)} the \projecttitle{} driver and mapped REGs pages that are responsible for initializing the device and configuring host---device communication and {\em (2)} the RDMA OS abstractions that execute networking operations. 


\subsection{\projecttitle{} Driver and Mapped REGs Pages} 
The \projecttitle{} driver is invoked at the device initialization, before the remote attestation protocol ($\S$~\ref{subsec:nic_controller}), to configure the hardware with its static configuration (the device MAC address, the device QSFP port, and the network IP used by the application). 

The driver enables kernel-bypass networking---similar to the original (user-space) RDMA protocol---by mapping the \projecttitle{} device to a user-space addresses range, the Mapped REGs pages. \projecttitle{} reserves one page at the page granularity of our system for each connected device that is represented as pseudo-devices in {\tt /dev/fpga<ID>}. Read and write access to the pseudo-device is equal to accessing the control and status registers of the FPGA. Applications directly interact with the control path of the \projecttitle{} hardware bypassing the host OS.

\begin{figure}[t]
    \centering
    %\includegraphics[width=0.5\textwidth]{figures/trusted-nic-overview.drawio.pdf}
    % \includegraphics[width=0.9\linewidth]{figures/network_stack_system_design.drawio.pdf}
    \includegraphics[width=0.75\linewidth]{figures/trusted-nic-network_stack_system_design_atsushi.drawio.pdf}
    \caption{\projecttitle{} network system stack.}
    \label{fig:network_stack_design}
\end{figure}


\subsection{RDMA OS Abstractions} 
The RDMA OS abstractions are a user-space library that enables the networking operations in the \projecttitle{} hardware, bypassing the OS kernel for performance. 
As shown in Figure~\ref{fig:network_stack_design}, the RDMA OS library is comprised of two parts: \textit{the network (RDMA) library} (colored in purple) that implements the software part of the RDMA protocol and \textit{the OS library} (colored in red) that schedules the \projecttitle{} requests. 

% As shown in Figure~\ref{fig:network_stack_design}, invocations of the \projecttitle{} programming API calls into the RDMA OS library are comprised of two parts: \textit{the network (RDMA) library} (colored in purple) that implements the software part of the RDMA protocol and \textit{the OS library} (colored in red) that schedules the \projecttitle{} requests. 




\myparagraph{Network (RDMA) library}  The network (RDMA) library includes all the logic and data (e.g., Tx/Rx queues per connection, local and remote memory addresses, RDMA keys that denote memory access permissions) required to implement the RDMA protocol. It executes the application's networking operations by posting the requests to the hardware. More specifically, it creates an internal representation of the request and the associated data and metadata (i.e., request opcode, remote IP, source/destination addresses, data length, etc.) and writes them into specific offsets in the REGs pages to update the control registers of the \projecttitle{} hardware. 
% In case there are no empty transmission queues, the ibv library {\tt yields} until there is an empty slot in the queue.
% (i.e., request opcode, remote IP, local address, remote base address, offsets, data length, etc.), encodes all related parameters, and finally, 


As shown in Figure~\ref{fig:network_stack_design}, the transmission and reception of requests and responses mandate the allocation of application network buffers. 
We adopt memory management similar to that in widely used user-space networking libraries~\cite{erpc, dpdk, rdma}.
% We adopt similar memory management as in widely-used user-space networking libraries~\cite{erpc, dpdk, rdma}. 
Importantly, the network buffers need to be mapped to a specific \projecttitle{}-memory, called the ibv memory. The ibv memory area is allocated at the connection creation in the {\tt huge page} area by the application through the ibv library. It resides within the application's address space with full read/write permissions and is eligible for DMA transfers. 
% without involving the CPU. 
% It resides within the application's address space with full read/write permissions, it is eligible for DMA transfers, and it is registered to the \projecttitle{} for remote reads and writes without involving the CPU. 

\myparagraph{OS library} The \projecttitle{}-OS library is responsible for scheduling the requests and ensuring isolated access to the mapped REG pages. 
% The \projecttitle{}-OS library is a lightweight user-space library responsible for scheduling the requests and ensuring isolated access to the mapped REG pages.
For performance, the \projecttitle{} data path eliminates unnecessary data copies throughout the network stack; the data to be sent is directly fetched by the hardware through DMA transfers. The OS library creates a \projecttitle{}-process object to represent each \projecttitle{} device. This \projecttitle{}-process in \projecttitle{} is not a separate scheduling entity (i.e., a thread as in classical OSes). 
In contrast, it is an object handle, exposed to the ibv library but managed by the \projecttitle{}-OS library that acquires locks on the respective REG pages to ensure isolated access to the \projecttitle{} hardware.

% In fact, the data to be sent (shown in blue lines in Figure~\ref{fig:network_stack_design} is directly fetched from the hardware through DMA transfers. 