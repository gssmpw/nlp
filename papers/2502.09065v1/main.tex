%% LaTeX Template for ISIT 2025
%%
%% by Stefan M. Moser, October 2017
%% (with minor modifications by Tobias Koch, November 2023 and Mich√®le Wigger, November 2024)
%% 
%% derived from bare_conf.tex, V1.4a, 2014/09/17, by Michael Shell
%% for use with IEEEtran.cls version 1.8b or later
%%
%% Support sites for IEEEtran.cls:
%%
%% http://www.michaelshell.org/tex/ieeetran/
%% http://moser-isi.ethz.ch/manuals.html#eqlatex
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%%

\documentclass[conference,letterpaper]{IEEEtran}

%% depending on your installation, you may wish to adjust the top margin:
\addtolength{\topmargin}{9mm}

%%%%%%
%% Packages:
%% Some useful packages (and compatibility issues with the IEEE format)
%% are pointed out at the very end of this template source file (they are 
%% taken verbatim out of bare_conf.tex by Michael Shell).
%
% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
%
\usepackage{siunitx}
\usepackage{booktabs}
\usepackage{comment}

\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{ifthen}
\usepackage{cite}
%\usepackage[cmex10]{amsmath} % Use the [cmex10] option to ensure complicance
                             % with IEEE Xplore (see bare_conf.tex)
\usepackage[cmex10]{amsmath}   
\usepackage{cases}
\usepackage{multirow}
\usepackage{empheq}
\usepackage{verbatim}
\usepackage{algorithm}
\usepackage {algpseudocode}
\usepackage{algorithmicx}
\usepackage{algcompatible}
% \usepackage{kotex}
\usepackage{amssymb}
\usepackage{optidef}
\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
\else
  \usepackage[dvips]{graphicx}
\fi
%\usepackage{epsfig}
%\usepackage[caption=false]{caption}
%\usepackage[font=footnotesize]{subfig}
\usepackage[caption=false,font=footnotesize]{subfig}

\usepackage{color}
\usepackage{multirow}
%\usepackage{subcaption}

%% Please note that the amsthm package must not be loaded with
%% IEEEtran.cls because IEEEtran provides its own versions of
%% theorems. Also note that IEEEXplore does not accepts submissions
%% with hyperlinks, i.e., hyperref cannot be used.

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{fact}{Fact}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{conjecture}{Conjecture}
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}
\newtheorem{property}{Property}
\newtheorem{claim}{Claim}
\newtheorem{example}{Example}

\interdisplaylinepenalty=2500 % As explained in bare_conf.tex


%%%%%%
% correct bad hyphenation here
% ------------------------------------------------------------
\begin{document}
\title{Lowering the Error Floor of Error Correction Code Transformer} 

% %%% Single author, or several authors with same affiliation:
% \author{%
%  \IEEEauthorblockN{Author 1 and Author 2}
% \IEEEauthorblockA{Department of Statistics and Data Science\\
%                    University 1\\
 %                   City 1\\
  %                  Email: author1@university1.edu}% }


%%% Several authors with up to three affiliations:
\author{%
    \IEEEauthorblockN{Taewoo Park\textsuperscript{*}, Seong-Joon Park\textsuperscript{\textdagger}, Hee-Youl Kwak\textsuperscript{\textdaggerdbl}, Sang-Hyo Kim\textsuperscript{\S}, and Yongjune Kim\textsuperscript{*}}
    \IEEEauthorblockA{\textsuperscript{*}Department of Electrical Engineering and \textsuperscript{\textdagger}Institute of Artificial Intelligence, POSTECH, Pohang 37673, South Korea \\
    Email: \{parktaewoo, seongjoon, yongjune\}@postech.ac.kr\\
    \textsuperscript{\textdaggerdbl}Department of Electrical, Electronic and Computer Engineering, University of Ulsan, Ulsan 44610, South Korea\\
    Email: hykwak@ulsan.ac.kr\\
    \textsuperscript{\S}Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon 16419, South Korea\\
    Email: iamshkim@skku.edu
    }    
}

\maketitle

%%%%%%
%% Abstract: 
%% If your paper is eligible for the student paper award, please add
%% the comment "THIS PAPER IS ELIGIBLE FOR THE STUDENT PAPER
%% AWARD." as a first line in the abstract. 
%% For the final version of the accepted paper, please do not forget
%% to remove this comment!
%%

\begin{abstract}
    With the success of transformer architectures across diverse applications, the error correction code transformer (ECCT) has gained significant attention for its superior decoding performance. 
    % An important challenge in the implementation of neural decoders for emerging applications lies in addressing the error floor.
    % In particular, various model-based approaches have been proposed to lower the error floor.
    % However, there haven't been any studies that explore the error floor for transformer-based decoders.
    In spite of its advantages, the error floor phenomenon in ECCT decoding remains unexplored.
    We present the first investigation of the error floor issue in ECCT and propose a hybrid decoding approach that integrates hard decision decoders as pre- and post-decoders with ECCT to effectively lower the error floor.
    In particular, we introduce a novel loss function for ECCT that considers the dynamics of hybrid decoding algorithm. 
    Training ECCT with the proposed loss function enhances its ability to correct specific error patterns by taking into account its interaction with the auxiliary decoders.
    Simulation results demonstrate that the proposed hybrid decoder with the novel loss function significantly outperforms the original ECCT in both the waterfall and the error floor regions.
    % Notably, for the BCH (31,16) code, our hybrid decoder significantly improves the decoding performance, achieving the maximum likelihood (ML) bound.

  % THIS PAPER IS ELIGIBLE FOR THE STUDENT PAPER AWARD.
\end{abstract}

\section{Introduction}
    
    % 1. technical domain 1 neural decoder (Why?)
    % Error correction codes~(ECCs) are fundamental components of reliable digital communication systems.
    % With the advancements in deep learning, neural decoders have been developed.
    % A model-based decoder employing the belief propagation (BP) algorithm was proposed, assigning trainable weights to the edges of a Tanner graph~\cite{Nachmani2016learning}.
    % However, the model-based approach is closely tied to the conventional message-passing structure to achieve symmetry conditions, which limits its decoding performance~\cite{Bennatan2018deep}.
    % To overcome this issue, a preprocessing method that makes the input independent of the codeword is proposed, enabling the training of arbitrary structure of decoders with a single codeword~\cite{Bennatan2018deep}. 
    % Based on this work, various model-free neural decoders have been developed leveraging powerful deep learning models~\cite{Bennatan2018deep, Choukroun2022error, Choukroun2024foundation, Choukroun2024learning, Park2025multiple, Park2025crossmpt, Choukroun2022denoising}.

    % 2. Technical domain 2 ECCT (Why?)
    The significant success of the transformer architecture~\cite{Vaswani2017attention} across various applications~\cite{Dosovitskiy2021image, Devlin2018bert} has inspired the development of a transformer-based error correction code decoder, known as the error correction code transformer (ECCT)~\cite{Choukroun2022error}, which has achieved notable decoding performance, particularly for short codes.
    ECCT effectively captures the relevance between bits using the attention mechanism, while leveraging the code structure by masking the attention map based on the parity check matrix.
    In line with this approach, Choukroun and Wolf presented a foundation model for ECCT~\cite{Choukroun2024foundation} and the end-to-end optimization of both the encoder and decoder~\cite{Choukroun2024learning}.
    Furthermore, Park~\textit{et al.} proposed novel transformer architectures that utilize the multiple mask matrices~\cite{Park2025multiple} and the cross-attention mechanism~\cite{Park2025crossmpt}.
    These transformer-based decoders~\cite{Choukroun2022error, Choukroun2024foundation, Choukroun2024learning, Park2025multiple, Park2025crossmpt} have shown superior decoding performance compared to conventional decoding algorithms and other neural decoders.

    % 3. Error floor in ECCT (Which problem?)
    
    In spite of its notable decoding performance in the waterfall region, the error floor of ECCT has yet to be investigated. The error floor refers to the phenomenon where the decrease in error probability slows significantly in the high signal-to-noise ratio (SNR) region, compared to the waterfall region~\cite{Richardson2003error}.    
    For emerging applications demanding extremely low error rate, such as the next-generation ultra-reliable and low-latency communications~(xURLLC)~\cite{Hong20226g}, it is important to mitigate the error floor problem~\cite{Kwak2023boosting}.
    In particular, several techniques have been developed to mitigate the error floor phenomenon in low-density parity-check~(LDPC) codes by leveraging decoder diversity~\cite{Xiao2021faid} and boosting-based learning~\cite{Kwak2023boosting}.
    However, to the best of our knowledge, the error floor in transformer-based decoders remains unexplored.

    % 4. Main 1. Error floor for ECCT
    In this paper, we present the first investigation into the error floor phenomenon in transformer-based decoders, which has neither been observed nor discussed in prior studies.
    Specifically, we find that ECCT suffers from an error floor even when decoding Bose-Chaudhuri-Hocquenghem~(BCH) codes.
    Our simulation results reveal that this error floor prevents ECCT from achieving extremely low frame error rates (FERs) in the high SNR region.
    This finding motivates the development of decoding algorithms to mitigate the error floor in ECCT. 
    
    % 5. Main 2. Hybrid decoding
    To address this issue, we propose a simple yet effective hybrid decoding approach that effectively lowers the error floor in ECCT decoding.
    The proposed hybrid decoder integrates hard decision decoders as pre- and post-decoders for ECCT.
    This approach is motivated by the observation that ECCT can reduce errors in received vectors, even when it fails to accurately estimate the transmitted codewords.
    By leveraging hard decision decoders, the hybrid decoder can correct a broader range of error patterns, significantly improving decoding performance in both the error floor and waterfall regions.

    \begin{figure*}[!t] 
        \centering
        % \subfloat[(63, 36) BCH code]{\includegraphics[width=0.4\textwidth]{fig/error_floor_BCH_n_63_k_36_FER.eps}\label{fig:errorfloor_6336}}
        % \hfil
        \subfloat[BCH codes]{\includegraphics[width=0.4\textwidth]{fig/error_floor_BCH_FER.eps}\label{fig:errorfloor_bch}}
        \hfil
        \subfloat[LDPC codes]{\includegraphics[width=0.4\textwidth]{fig/error_floor_LDPC_FER.eps}\label{fig:errorfloor_ldpc}}
        \hfil
        % \subfloat[(31, 16) BCH code]{\includegraphics[width=0.4\textwidth]{fig/error_floor_BCH_n_31_k_16_FER.eps}\label{fig:errorfloor_3116}}
        % \hfil
        % \subfloat[(31, 16) BCH]{\includegraphics[width=0.3\textwidth]{fig/error_floor_BCH_n_31_k_16_FER.eps}\label{fig:errorfloor_3116}}
        \caption{Error floor phenomenon of ECCT: (a) FER performance of ECCT for BCH codes, and (b) FER performance of ECCT for LDPC codes.}
        \label{fig:errorfloor}
         \vspace{-4mm}
    \end{figure*}
    % Section 3ÏóêÏÑú error floor ÌòÑÏÉÅÏùÄ LDPCÏôÄ Í∞ôÏùÄ Îã§Î•∏ ÏΩîÎìúÏóê ÎåÄÌïú Í≤∞Í≥ºÎèÑ Ï†úÏãúÌïòÎäî Í≤ÉÏù¥ Ï¢ãÏùÑ Í≤É Í∞ôÎã§.
    % Ìï¥Í≤∞Ï±ÖÏùÄ ISITÏóêÏÑúÎäî BCHÎßå Ï†úÏãú
    % section 3ÏôÄ 4Î•º Ìï©ÏπòÎäî Í≤ÉÎèÑ Í≥†Î†§
    
    % 6. Main 2-1. Training method
    % Ïù¥Ï†Ñ Î¨∏Îã®ÏóêÏÑúÎäî generalÌïòÍ≤å, Ïù¥ Î¨∏Îã®ÏóêÏÑúÎäî three-stage hybrid decodingÎ°ú Ï†úÌïú.
    In particular, we propose a novel loss function for the hybrid decoder to further improve its decoding performance.
    % Furthermore, we propose a novel loss function for ECCT specifically designed for the hybrid decoding algorithm.
    % We show that the decoding performance of the hybrid decoder can be enhanced by carefully training ECCT considering the pre- and post-hard decision decoders.
    This loss function is specifically designed to enhance the interactions between ECCT and the pre- and post-hard decision decoders.
    First, considering the pre-decoder, the training dataset is constrained to include only the received vectors with errors exceeding the error correction capability of the hard decision decoder.
    Second, for the post-decoder, the loss function is adjusted depending on whether the number of errors in ECCT's output exceeds the error correction capability.
    Rather than training ECCT to correct all errors for all received vectors as in~\cite{Choukroun2022error}, the proposed loss function enables ECCT to focus on reducing critical errors in specific received vectors.
    This targeted approach enhances error correction throughout the overall hybrid decoding process. 


    % 8. Experiments
    % We evaluate the decoding performance of the proposed hybrid decoding algorithm on BCH codes.
    % Experimental results demonstrate that the hybrid decoding algorithm with the proposed training method and loss function is highly effective in mitigating the error floor phenomenon.
    % Additionally, the hybrid decoding algorithm achieves a substantial performance enhancement in the waterfall region.
    % Notably, for the $(31,16)$ BCH code, the hybrid decoder outperforms the original ECCT by over 1~dB, achieving the maximum likelihood (ML) bound.
    
    % 10. organization
    % The rest of this paper is organized as follows. Section~\ref{sec:preliminaries} presents a brief overview of the system model and ECCT. Section~\ref{sec:main} presents the proposed hybrid decoding algorithm and its training method. Section~\ref{sec:results} provides experimental results and Section~\ref{sec:conclusion} concludes the paper. 
    \section{Preliminaries}\label{sec:preliminaries}
    
    % \begin{figure}[t] 
    %     \centering        
    %     \includegraphics[width=0.48\textwidth]{fig/system_model_v1.png}
    %     \caption{System model of a digital communication.}
    %     \label{fig:system model}
    %     \vspace{-4mm}
    % \end{figure}
    \subsection{Notation}
    We denote an $(n,k)$ linear block code as $\mathcal{C}$, which is characterized by a generator matrix $G$, a parity check matrix $H$, and a minimum Hamming distance $d_{\text{min}}$.
    The message and the codeword are denoted by $\mathbf{m}\in\{0,1\}^k$ and $\mathbf{x}\in \mathcal{C} \subseteq \{0,1\}^n$.    
    Additionally, $\mathbf{X}$ represents the random variable of $\mathbf{x}$.
    We consider the binary phase shift keying (BPSK) modulation with the additive white Gaussian noise (AWGN) channel. 
    The modulated signal and the received vector are denoted by $\mathbf{x}_s$ and $\mathbf{y}$ with the noise vector $\mathbf{z}\sim\mathcal{N}(0,\sigma^2I)$.
    The Hamming distance between two codewords $\mathbf{x}_1$ and $\mathbf{x}_2$ is denoted by $d_H(\mathbf{x}_1,\mathbf{x}_2)$.
    The decoder $f(\cdot)$ outputs the estimated codeword $\widehat{\mathbf{x}}$ from the received vector $\mathbf{y}$.
    The hard decision decoder corrects all the received vectors $\mathbf{y}$ such that $d_H(\mathbf{x},\mathbf{y})\leq t_c$, where the error correction capability is denoted as $t_c = \lfloor (d_{\text{min}}-1)/2\rfloor$.    
    %The system model considered in this paper is illustrated in Fig.~\ref{fig:system model}.

    \subsection{Error Correction Code Transformer}\label{sec:ecct}
    % figure of ecct?
    ECCT is a model-free neural decoder based on the transformer architecture~\cite{Choukroun2022error}.
    As in~\cite{Bennatan2018deep}, ECCT preprocesses the input as \mbox{$h(\mathbf{y})=[|\mathbf{y}|,s(\mathbf{y})]$}, where $[\cdot,\cdot]$ denotes the concatenation of two vectors and $s(\mathbf{y})$ represents the syndrome \mbox{$H\mathbf{y}_b=H\text{bin}(\text{sign}(\mathbf{y}))$}. Here, $\text{sign}(\cdot)$ is the sign function such that $\text{sign}(\alpha)=1$ if $\alpha\geq 0$ and $-1$ otherwise; and $\text{bin}(\cdot)$ is the binarization function defined as $\text{bin}(\alpha)=0.5(1-\alpha)$.
    Each element of the preprocessed input is then linearly mapped into an embedding vector of dimension $d$ for the attention layers.
    % The preprocessed input $h(\mathbf{y})\in \mathbb R^{2n-k}$ is then linearly projected to a dimension $d$ as $\boldsymbol{\Phi}\in \mathbb{R}^{2n-k\times d}$.
    
    The code-aware self-attention operation is defined as~\cite{Choukroun2022error}:
    \begin{equation}
        A_H\left(Q,K,V\right)=\text{Softmax}(d^{-1/2}(QK^{\mathsf{T}}+{g}(H)))V,\nonumber
    \end{equation}
    where $Q$, $K$, $V$, and ${g}(H)$ denote the query, key, value, and mask matrix, respectively.
    The mask matrix $g(H)$ is constructed using the parity check matrix $H$, enabling the self-attention to focus selectively on the important relations between the embedding vectors.

    The decoder of ECCT, denoted as $f_{\boldsymbol{\theta}}(\mathbf{y})$ with trainable parameters $\boldsymbol{\theta}$, aims to predict the multiplicative noise $\tilde{\mathbf{z}}_s$~\cite{Choukroun2022error}. 
    The multiplicative noise is defined by  $\mathbf{y}=\mathbf{x}_s+\mathbf{z}=\mathbf{x}_s\odot\tilde{\mathbf{z}}_s$, where $\odot$ denotes element-wise multiplication.
    ECCT is trained using the following binary cross entropy loss function:
    % ÏàòÏ†ï ÌïÑÏöî
    \begin{align}\label{eq:bce}
        l_\text{BCE}(\mathbf x,f_{\boldsymbol{\theta}}(\mathbf y)) &=-\sum_{i=1}^n \{\tilde z_i\log(1-\sigma(f_{\boldsymbol{\theta}}(\mathbf y)_i)) \nonumber \\
        &\quad +(1-\tilde z_i)\log \sigma(f_{\boldsymbol{\theta}}(\mathbf{y})_i)\},
    \end{align}
    where $\tilde{\mathbf{z}}=\text{bin}(\text{sign}(\tilde{\mathbf{z}}_s))$ denotes the binarized multiplicative noise and $\sigma(\cdot)$ denotes the sigmoid function $\sigma(\alpha)=1/(1+e^{-\alpha})$.
    Lastly, the estimated codeword can be obtained as 
    \begin{equation}
        \widetilde{f}_{\boldsymbol{\theta}}(\mathbf{y})=\text{bin}(\text{sign}(f_{\boldsymbol{\theta}}(\mathbf{y})\odot \mathbf{y})).
    \end{equation}

    \section{Error Floor of Error Correction Code Transformer and Hybrid Decoders}
    % experimentÏóê ÎåÄÌïú ÏÑ§Î™Ö, code Ïñ¥ÎîîÏÑú Íµ¨ÌñàÎäîÏßÄ
    
    We report the error floor phenomenon of ECCT for the first time and propose a hybrid decoding approach to address the error floor. 
    % Notably, ECCT may exhibit the error floor for certain codes, even when the corresponding conventional decoders do not.
    Fig.~\ref{fig:errorfloor} shows the error floor phenomenon of ECCT when decoding BCH and LDPC codes.
    ECCT suffers from the error floor for both BCH and LDPC codes, preventing it from achieving extremely low FER in the high SNR region.
    % The results for BCH codes indicate that ECCT may exhibit the error floor, even when the corresponding conventional decoders do not.
    For BCH codes, it is unexpected that ECCT exhibits an error floor, as this phenomenon does not occur in conventional decoders.
    In addition, the results for LDPC codes imply that ECCT cannot avoid the error floor, which is a critical issue even in the belief propagation (BP) algorithm.
    % These findings imply that the model-free neural decoder employing even advanced architectures can encounter the error floor phenomenon. 
    % This emphasizes the critical need to mitigate the error floor to achieve ultra-reliability demands of various emerging applications.
    
    \begin{figure*}[t] 
        \centering
        \subfloat[Hybrid decoding with hard decision pre- and post-decoders]{\includegraphics[width=0.9\textwidth]{fig/architecture_pre_post_v2.eps}\label{fig:architecture_pre_post}}
        \hfil
        \subfloat[Hybrid decoding with hard decision pre-decoder]{\includegraphics[width=0.45\textwidth]{fig/architecture_pre_v2.eps}\label{fig:architecture_pre}}
        \hfil
        \subfloat[Hybrid decoding with hard decision post-decoder]{\includegraphics[width=0.45\textwidth]{fig/architecture_post_v2.eps}\label{fig:architecture_post}}
        \hfil
        \caption{Block diagrams of hybrid decoding architectures.}
        \label{fig:architecture}
         \vspace{-4mm}
    \end{figure*}
        
    % \section{Hybrid Decoding}\label{sec:main}
    
    We propose a hybrid decoding approach that combines ECCT with hard decision decoders to lower the error floor of ECCT.
    The block diagrams of the proposed architectures are shown in Fig.~\ref{fig:architecture}.
    As shown in Figs.~\ref{fig:architecture}\subref{fig:architecture_pre_post}, \subref{fig:architecture_pre}, and \subref{fig:architecture_post}, hard decision decoders can be employed as both pre- and post-decoder, or solely as a pre-decoder or a post-decoder, respectively.
    Since hard decision decoders and ECCT can correct different classes of error patterns, the hybrid decoding approach can correct a broader range of errors, effectively lowering the error floor.
    
    % process
    In the hybrid decoding architecture with hard decision pre- and post-decoders (see Figs.~\ref{fig:architecture}\subref{fig:architecture_pre_post}), the decoding process terminates if the hard decision pre-decoder successfully estimates the correct codewords; otherwise, it proceeds to the next stage.
    If the pre-decoder fails, the received vector $\mathbf{y}$ is passed to ECCT for decoding in the subsequent stage. 
    If ECCT also fails, the process continues to the hard decision post-decoder.
    Unlike the pre-decoder, which operates on the received vector $\mathbf{y}$, the post-decoder takes the output of ECCT as its input. 
    The ECCT output typically contains fewer errors since ECCT can reduce errors even when it fails to decode.
    Depending on the situation, either the pre-decoder or the post-decoder can be omitted.
    
    \begin{figure}[t] 
        \centering
        \subfloat[$d_H(\mathbf{x},\mathbf{y})\leq t_c$]{\includegraphics[width=0.36\textwidth]{fig/BCH_n63_k45_error_distribution_lessequal_t.eps}\label{fig:error_distribution-a}}
        \hfil
        \subfloat[$d_H(\mathbf{x},\mathbf{y}) > t_c$]{\includegraphics[width=0.36\textwidth]{fig/BCH_n63_k45_error_distribution_more_t.eps}\label{fig:error_distribution-b}}
        \caption{Histograms of the number of errors in the output vector of ECCT in decoding (63, 45) BCH codes.}
        \label{fig:error distribution}
        \vspace{-4mm}
    \end{figure} 
    
    % 2. Error distribution
    
    The roles of the pre-decoder and post-decoder in hybrid decoding can be better understood by examining the distribution of errors in the output of ECCT.
    Fig.~\ref{fig:error distribution} shows the histogram of the number of errors in the output vectors of ECCT for the $(63, 45)$ BCH code.
    Figs.~\ref{fig:error distribution}\subref{fig:error_distribution-a} and \subref{fig:error_distribution-b} show the histogram of received vectors with errors less than or equal to $t_c$ and the histogram of the received vectors with errors more than $t_c$, respectively.
    As shown in Fig.~\ref{fig:error distribution}\subref{fig:error_distribution-a}, ECCT may fail to correct all errors, in some cases, even increase the number of errors for received vectors that initially have $t_c$ or fewer errors.
    Since the hard decision decoder can correct all the received vectors with $t_c$ or fewer errors, using it as a pre-decoder improves the overall decoding performance.
    For some received vectors with more than $t_c$ errors, ECCT is capable of estimating the correct codewords. 
    Even when it fails, it frequently reduces the number errors as shown in Fig.~\ref{fig:error distribution}\subref{fig:error_distribution-b}.
    This behavior occurs since the binary cross entropy loss~\eqref{eq:bce} trains ECCT to reduce errors in its output. 
    Consequently, employing a hard decision post-decoder can correct the remaining errors ($\leq t_c$), thereby enhancing the overall decoding performance.
    % This indicates that combining the post-decoder leads to the correction of more errors since the hard decision decoder can correct reduced errors below the $t_c$ after ECCT decoding.

    % Remark. scenario
    % In the scenario assuming retransmission, the hybrid decoding algorithm can achieve high decoding performance while reducing the number of retransmissions.
    % If the hard decision decoder at the receiver fails to decode the signal, the traditional system requests a retransmission from the transmitter.
    % However, if the receiver has a neural decoder such as ECCT, the received vector can be decoded again using this neural decoder, rather than requesting a retransmission.
    % This method of reducing the number of retransmissions through hybrid decoding is well-suited for scenarios demanding low latency and high reliability.
    % Rather than retransmitting and decoding the newly received code, re-decoding the same code with hybrid decoding can effectively reduce communication latency.

    \section{Proposed Loss Function for Hybrid Decoders}\label{sec:training}
    % propositionÌòïÏãùÏúºÎ°ú, Ï¶ùÎ™Ö Í∞ÑÎã®ÌïòÎ©¥ Î≥∏Î¨∏ÏóêÏÑú, Ï°∞Í∏à Î≥µÏû°ÌïòÎ©¥ appendixÎ°ú.
    % hardÎäî Î≥∏Î¨∏ÏóêÏÑú, softÎäî appendixÏóêÏÑú Ìï¥ÎèÑ Îê† ÎìØ.
    
    This section proposes a training method for the hybrid decoding algorithm.
    To improve the error correction performance of the hybrid decoder, the ECCT decoder should be trained with consideration for its integration with hard decision decoders.
    Specifically, we focus on the hybrid decoding scheme that combines ECCT with both pre- and post-decoders, as shown in Fig.~\ref{fig:architecture}\subref{fig:architecture_pre_post}.
    % Let the pre-decoding output be $\widehat{\mathbf{x}}_1 = f_{\text{pre}}(\mathbf{y})$, the output of ECCT be $\widehat{\mathbf{x}}_2 = f_{\boldsymbol{\theta}}(\widehat{\mathbf{x}}_1)$, and the post-decoding output be $\widehat{\mathbf{x}} = f_{\text{post}}(\widehat{\mathbf{x}}_2)$.
    Suppose that $\widehat{\mathbf{x}}_1 = f_{\text{pre}}(\mathbf{y})$, $\widehat{\mathbf{x}}_2 = f_{\boldsymbol{\theta}}(\widehat{\mathbf{x}}_1)$, and $\widehat{\mathbf{x}} = f_{\text{post}}(\widehat{\mathbf{x}}_2)$ represent the outputs of the pre-decoder, ECCT, and the post-decoder, respectively.
    The bit error rate~(BER) loss for this hybrid decoder is given by
    \begin{align}
        \mathcal{L}(\boldsymbol{\theta})&=\mathbb{E}_{\mathbf{X},\widehat{\mathbf{X}}}\left[d_H(\mathbf{X},\widehat{\mathbf{X}})\right] \nonumber \\
        &=\mathbb{E}_{\mathbf{X},\mathbf{Y}}\left[d_H(\mathbf{X}, f_{\text{post}}\circ f_{\boldsymbol{\theta}}\circ f_{\text{pre}}(\mathbf{Y}) )\right].\label{eq:loss_hybrid2}
    \end{align}
    
    However, ECCT cannot be trained directly using the loss~\eqref{eq:loss_hybrid2} since the hard decision decoders $f_{\text{pre}}$ and $f_{\text{pre}}$ are not differentiable.
    To enable gradient-based training of ECCT, we derive the following equivalent loss for hybrid decoding: 
    % the pre-decoder $f_{\text{pre}}(\cdot)$ and the post-decoder $f_{\text{post}}(\cdot)$ should be  from the loss, thereby making it differentiable.
    % To train ECCT effectively with the gradient descent method, we should take off the pre-decoder $f_{\text{pre}}(\cdot)$ and the post-decoder $f_{\text{post}}(\cdot)$ from the loss function, making it differentiable.
    % In particular, an equivalent loss for hybrid decoding with hard decision decoders can be derived by modifying the loss according to the error correction capability $t_c$.
    \begin{lemma}\label{lemma:loss_hard}
        Consider the hybrid decoding algorithm with the hard decision decoders as pre- and post-decoders. The loss~\eqref{eq:loss_hybrid2} is equivalent to:
        % \begin{equation}\label{eq:lemma1}
        %     \mathbb E_{\mathbf X, \mathbf Y}\left[u(d_H(\mathbf{X},\widetilde{f}_{\boldsymbol{\theta}}(\mathbf{y}))-t_c)d_H(\mathbf X, \widetilde{f}_{\boldsymbol{\theta}}(\mathbf{y})) | d_H(\mathbf X,\mathbf Y) > t_c
        %     \right],
        % \end{equation}
        \begin{align}
            \mathbb{E}_{\mathbf{X}, \mathbf{Y}} \Big[ &
                u(d_H (\mathbf{X}, \widetilde{f}_{\boldsymbol{\theta}}(\mathbf{Y})) - t_c) \nonumber \\
                &   \times d_H (\mathbf{X}, \widetilde{f}_{\boldsymbol{\theta}}(\mathbf{Y}) ) |
                d_H (\mathbf{X}, \mathbf{Y}) > t_c \Big],\label{eq:lemma1}
        \end{align}
        where $u(\alpha)$ denotes the step function, defined as $u(\alpha)=0$ if $\alpha \leq 0$ and $1$ otherwise.
    \end{lemma}
    \begin{IEEEproof}
        The expectation in~\eqref{eq:loss_hybrid2} can be evaluated by considering two events: $d_H(\mathbf x,\mathbf y)\leq t_c$ and $d_H(\mathbf x,\mathbf y)> t_c$.
        Using the law of total expectation, we can expand the loss as follows:
        \begin{align}
            \mathcal{L}(\boldsymbol{\theta})
            &=\mathbb E_{\mathbf X, \mathbf Y}\left[d_H(\mathbf X, f_{\text{post}}\circ f_{\boldsymbol{\theta}}\circ f_{\text{pre}}(\mathbf Y)) | d_H(\mathbf X,\mathbf Y) > t_c\right]  \nonumber\\
            &\quad \times \Pr(d_H(\mathbf{X},\mathbf{Y})>t_c)\label{eq:total_exp_1}\\
            &=\mathbb E_{\mathbf X, \mathbf Y}\left[d_H(\mathbf X, f_{\text{post}}\circ f_{\boldsymbol{\theta}}(\mathbf Y)) | d_H(\mathbf X,\mathbf Y) > t_c\right] \nonumber\\
            & \quad \times \Pr(d_H(\mathbf{X},\mathbf{Y})>t_c)\label{eq:total_exp_2},
        \end{align}
        where~\eqref{eq:total_exp_1} holds since the hard decision pre-decoder can correct all errors whenever $d_H(\mathbf{x},\mathbf{y})\leq t_c$, and ~\eqref{eq:total_exp_2} holds because the pre-decoder passes the received vector to the subsequent stage if it fails to decode.
        % where~\eqref{eq:total_exp_1} and~\eqref{eq:total_exp_2} hold since the hard decision decoder estimates the true codeword if the errors are less than or equal to $t_c$ and it cannot correct any errors if errors are more than $t_c$.

        Since the decoding success of the post-decoder is fully determined by the number of errors in the output vector of ECCT, the loss can be expressed as follows:
        \begin{align}
            &\mathbb E_{\mathbf X, \mathbf Y}\left[u(d_H(\mathbf{X},\widetilde{f}_{\boldsymbol{\theta}}(\mathbf{Y}))-t_c)d_H(\mathbf X, \widetilde{f}_{\boldsymbol{\theta}}(\mathbf{Y})) | d_H(\mathbf X,\mathbf Y) > t_c\right]\nonumber\\
            &\times\Pr(d_H(\mathbf{X},\mathbf{Y})>t_c)\nonumber.
        \end{align}        
        By omitting $\Pr(d_H(\mathbf{X},\mathbf{Y})>t_c)$, which is independent of $\boldsymbol{\theta}$, we obtain~\eqref{eq:lemma1}.        
    \end{IEEEproof}

    \begin{figure*}[!t] 
        \centering
        \subfloat[(31, 16) BCH code]{\includegraphics[width=0.4\textwidth]{fig/error_floor_BCH_n_31_k_16_FER_loss13.eps}\label{fig:errorfloor_3116}}
        \hfil
        \subfloat[(63, 36) BCH code]{\includegraphics[width=0.4\textwidth]{fig/error_floor_BCH_n_63_k_36_FER_loss13.eps}\label{fig:errorfloor_6336}}
        \hfil
        \subfloat[(63, 45) BCH code]{\includegraphics[width=0.4\textwidth]{fig/error_floor_BCH_n_63_k_45_FER_loss13.eps}\label{fig:errorfloor_6345}}
        \hfil
        \subfloat[(63, 51) BCH code]{\includegraphics[width=0.4\textwidth]{fig/error_floor_BCH_n_63_k_51_FER_loss13.eps}\label{fig:errorfloor_6351}}
        \hfil
        \caption{FER performance of ML decoding~\cite{Helmling2019database}, hard decision decoding, ECCT~\cite{Choukroun2022error}, and hybrid decoding.}
        \label{fig:hybrid_bch}
        % \vspace{-4mm}
    \end{figure*} 
    
    
    \begin{comment}
    \begin{table*}[h]
    \centering
    \caption{Comparison of the negative logarithm of BER of various decoders for decoding BCH codes.}
    \begin{tabular}{lccccc ccccc ccccc}
    \toprule
    \multicolumn{1}{l}{Codes} & \multicolumn{5}{c}{Hard decision decoding} & \multicolumn{5}{c}{ECCT} & \multicolumn{5}{c}{Hybrid decoding}\\
    \cmidrule(rl){2-6} \cmidrule(rl){7-11} \cmidrule(rl){12-16}
    {} & {4} & {5} & {6} & {7} & {8} & {4} & {5} & {6} & {7} & {8} & {4} & {5} & {6} & {7} & {8} \\
    \midrule
    (31, 16) BCH & 4.45 & 5.76 & 7.51 & 9.79 & 12.72 &
    6.35 & 8.26 & 10.60 & 13.47 & 16.67 &
    \textbf{7.73} & \textbf{10.37} & \textbf{13.80} & \textbf{18.04} & \textbf{24.02}\\
    (63, 36) BCH & 5.01 & 6.99 & 9.74 & 13.42 & 18.13 & 
    5.04 & 6.89 & 9.42 & 13.13 & 17.05 & 
    \textbf{6.00} & \textbf{8.60} & \textbf{12.38} & \textbf{17.56} & \textbf{24.67}\\
    (63, 45) BCH & 4.85 & 6.53 & 8.86 & 12.00 & 15.99 & 
    5.60 & 7.76 & 10.85 & 15.40 & 19.70 & 
    \textbf{6.27} & \textbf{8.99} & \textbf{12.62} & \textbf{17.75} & \textbf{24.83}\\
    (63, 51) BCH & 4.75 & 6.23 & 8.25 & 10.90 & 14.30 &
    5.63 & 7.91 & 11.03 & 15.14 & 19.75 & 
    \textbf{5.93} & \textbf{8.33} & \textbf{11.87} & \textbf{16.53} & \textbf{22.29}\\
    \bottomrule
    \end{tabular}
    \vspace{-4mm}
    \label{tab:ber_bch}
    \end{table*}
    \end{comment}
    
    In Lemma~\ref{lemma:loss_hard}, the pre-decoder is addressed through a conditional expectation, ensuring that ECCT is trained exclusively on received vectors $\mathbf{y}$ containing more than $t_c$ errors, which the pre-decoder is unable to correct.
    In addition, the post-decoder is incorporated by multiplying the loss function with the step function $u(d_H(\mathbf{X},f_{\boldsymbol{\theta}}(\mathbf Y))-t_c)$.
    ECCT trained with this loss aims to reduce errors to $t_c$ or fewer, rather than correcting all errors. 
    The remaining errors, which are less than or equal to $t_c$, are corrected by the post-decoder. 
    Furthermore, the loss~\eqref{eq:loss_hybrid2} can be upper bounded as in the following proposition.
    \begin{proposition}
    The loss~\eqref{eq:loss_hybrid2} can be upper bounded by
        \begin{align}
            \widetilde{\mathcal{L}}({\boldsymbol{\theta}})={}&\frac{1}{\log{2}}\mathbb{E}_{\mathbf{X}, \mathbf{Y}} \Big[ 
                u(d_H (\mathbf{X}, \widetilde{f}_{\boldsymbol{\theta}}(\mathbf{Y})) - t_c) \nonumber \\
                &   \times l_{\text{BCE}} (\mathbf{X}, f_{\boldsymbol{\theta}}(\mathbf{Y}) ) |
                d_H (\mathbf{X}, \mathbf{Y}) > t_c \Big].\label{eq:prop1}
        \end{align}
    \end{proposition}
    \begin{IEEEproof}
        Following Lemma~\ref{lemma:loss_hard}, the loss $\mathcal L(\boldsymbol{\theta})$ can be reformulated as~\eqref{eq:lemma1}.
        In~\eqref{eq:lemma1}, $d_H(\mathbf{x}, \widetilde{f}_{\boldsymbol{\theta}}(\mathbf{y}))=\sum_{i=1}^n d_H(x_i, \widetilde{f}_{\boldsymbol{\theta}}(\mathbf{y})_i)$ is equivalent to $\sum_{i=1}^n l_{0\text{--}1}((\tilde{z}_s)_i \cdot f_{\boldsymbol{\theta}}(\mathbf{y})_i)$, where $l_{0\text{--}1}(\alpha)$ is defined as $l_{0\text{--}1}(\alpha)=1$ if $\alpha\leq 0$ and $0$ otherwise.
        Here, $l_{0\text{--}1}(\alpha)$ can be upper bounded by the logistic loss function as follows~\cite{Bartlett2006convexity}:
        \begin{equation}
        l_{0\text{--}1}((\tilde{z}_s)_i \cdot f_{\boldsymbol{\theta}}(\mathbf{y})_i) \le -\log_2(\sigma((\tilde{z}_s)_i \cdot f_{\boldsymbol{\theta}}(\mathbf{y})_i)).\nonumber
        \end{equation}        
        Since this logistic loss is equivalent to the scaled binary cross entropy loss in~\eqref{eq:bce}, the loss function \eqref{eq:lemma1} can be upper bounded by~\eqref{eq:prop1}.
    \end{IEEEproof}

    In practice, $\widetilde{\mathcal{L}}({\boldsymbol{\theta}}) $ can be computed using the following sample average, omitting the constant scaling term:
    % \begin{align}\label{eq:hard_loss_practice}
    %     \mathcal{L}_{\boldsymbol{\theta}}
    %     &\simeq \frac{1}{M}\sum_{i=1}^M u(d_H(\mathbf{x}^{(i)},\widetilde{f}_{\boldsymbol{\theta}}(\mathbf y^{(i)}))-t_c) l_{\text{BCE}}(\mathbf x^{(i)}, f_{\boldsymbol{\theta}}(\mathbf y^{(i)})),
    % \end{align}
    \begin{align}\label{eq:hard_loss_practice}
    \widetilde{\mathcal{L}}({\boldsymbol{\theta}})
    &\simeq \frac{1}{M} \sum_{i=1}^M 
    \Big\{ u\big(d_H(\mathbf{x}^{(i)}, \widetilde{f}_{\boldsymbol{\theta}}(\mathbf{y}^{(i)})) - t_c\big) \nonumber \\
    &\quad \times l_{\text{BCE}}\big(\mathbf{x}^{(i)}, f_{\boldsymbol{\theta}}(\mathbf{y}^{(i)})\big) \Big\},
    \end{align}
    where $\mathbf{x}^{(i)}$ can be fixed as the zero codeword and $\mathbf{y}^{(i)}$ is sampled as $\mathbf{x}_s^{(i)} + \mathbf{z}^{(i)}$, retaining only the samples that satisfy $d_H(\mathbf{x}^{(i)},\mathbf{y}^{(i)})>t_c$. 
    The noise variance $\sigma^2$ is uniformly sampled from the training SNR range.
    In addition, the step function $u(x)$ is implemented via a straight-through estimator~(STE)~\cite{Bengio2013estimating} with the sigmoid function as a proxy, as described in~\cite{Xiao2020designing}. 
    The Hamming distance $d_H(\mathbf{x},\widetilde{f}_{\boldsymbol{\theta}}(\mathbf{y}))$ is estimated by multiplying the code length $n$ with the soft BER loss defined in~\cite{Lian2019learned}.
    % d_H(x,y) >t_cÎ≥¥Îã§ ÌÅ¨Îã§Îäî Í≤ÉÏù¥ Ï§ëÏöîÌïòÎØÄÎ°ú Í∞ïÏ°∞
    
    \section{Experimental Results}\label{sec:results}
    
    In this section, we present the experimental results for the proposed hybrid decoding algorithms. 
    To evaluate its effectiveness, we measure the BER and FER performance for four BCH codes: $(31, 16)$, $(63, 36)$, $(63, 45)$, and $(63, 51)$, which have error correcting capabilities of $t=3$, $t=5$, $t=3$, and $t=2$, respectively.
    In all simulations involving ECCT, the number of decoding layers $N$ is fixed to $6$, and the embedding dimension $d$ is set to $128$.
    Additionally, the parity check matrices used in the experiments and maximum likelihood~(ML) decoding results are obtained from~\cite{Helmling2019database}. 
    % In the simulations, each decoder is evaluated until 50 frame errors are observed in more than $10^5$ received vectors or until a total of $10^{10}$ received vectors have been tested. (Ïã§ÌóòÏùò Íµ¨Ï≤¥Ï†ÅÏù∏ Ïã§ÌñâÌöüÏàò)
    % N =6, d= 128
    
    Fig.~\ref{fig:hybrid_bch} shows the FER performance of the proposed hybrid decoder along with other decoders: ML, hard decision, and the original ECCT.
    The hybrid decoder incorporates hard decision decoders as both pre- and post-decoders and is trained using the proposed loss~\eqref{eq:hard_loss_practice}. 
    The results indicate that the hybrid decoder effectively mitigates the error floor, significantly outperforming the original ECCT in the high SNR region.
    Furthermore, even in the waterfall region, the hybrid decoder outperforms both the original ECCT and hard decision decoder.
    Notably, the hybrid decoder achieves the ML decoding performance for the $(31, 16)$ BCH code.
    By combining two distinct decoders and training them with the proposed loss, the hybrid decoding approach can achieve significant coding gains. 
    
    \begin{comment}
    \begin{table*}[!t]
    \centering
    \caption{Comparison of BER of Decoders with Different Configurations}
    \begin{tabular}{lcccccc}
    \toprule
    \multicolumn{1}{l}{\multirow[c]{2}{*}[-1mm]{Methods}} & \multicolumn{3}{c}{(31, 16) BCH code} & \multicolumn{3}{c}{(63, 45) BCH code} \\
    \cmidrule(rl){2-4} \cmidrule(rl){5-7}
    {} & {\SI{4}{\decibel}} & {\SI{5}{\decibel}} & {\SI{6}{\decibel}} & {\SI{4}{\decibel}} & {\SI{5}{\decibel}} & {\SI{6}{\decibel}} \\
    \midrule
    ECCT~\cite{Choukroun2022error} & $1.74\mathrm{e}{-3}$ & $2.59\mathrm{e}{-4}$ & $2.49\mathrm{e}{-5}$ & $3.69\mathrm{e}{-3}$ & $4.24\mathrm{e}{-4}$ & $1.95\mathrm{e}{-5}$\\
    Pre + ECCT & $1.68\mathrm{e}{-3}$ & $2.51\mathrm{e}{-4}$ & $2.41\mathrm{e}{-5}$ & $3.03\mathrm{e}{-3}$ & $2.93\mathrm{e}{-4}$ & $1.22\mathrm{e}{-5}$\\
    ECCT + Post & $1.24\mathrm{e}{-3}$ & $1.68\mathrm{e}{-4}$ & $1.34\mathrm{e}{-5}$ & $2.69\mathrm{e}{-3}$ & $2.61\mathrm{e}{-4}$ & $9.45\mathrm{e}{-6}$\\
    Pre + ECCT + Post & $1.20\mathrm{e}{-3}$ & $1.63\mathrm{e}{-4}$ & $1.30\mathrm{e}{-5}$ & $2.37\mathrm{e}{-3}$ & $1.97\mathrm{e}{-4}$ & $6.54\mathrm{e}{-6}$\\
    Pre + ECCT + Post + loss~\eqref{eq:hard_loss_practice} & $\mathbf{4.38\mathrm{\mathbf{e}}{-4}}$ & $\mathbf{3.13\mathrm{\mathbf{e}}{-5}}$ & $\mathbf{1.02\mathrm{\mathbf{e}}{-6}}$ & $\mathbf{1.89\mathrm{\mathbf{e}}{-3}}$ & $\mathbf{1.24\mathrm{\mathbf{e}}{-4}}$ & $\mathbf{3.31\mathrm{\mathbf{e}}{-6}}$\\
    \bottomrule
    \end{tabular}
    \vspace{-4mm}
    \label{tab:ablation}
    \end{table*}
    \end{comment}

    \begin{table*}[!t]
    \centering
    \caption{Comparison of FER of Decoders with Different Configurations}
    \begin{tabular}{lcccccc}
    \toprule
    \multicolumn{1}{l}{\multirow[c]{2}{*}[-1mm]{Methods}} & \multicolumn{3}{c}{(31, 16) BCH code} & \multicolumn{3}{c}{(63, 45) BCH code} \\
    \cmidrule(rl){2-4} \cmidrule(rl){5-7}
    {} & {\SI{4}{\decibel}} & {\SI{5}{\decibel}} & {\SI{6}{\decibel}} & {\SI{4}{\decibel}} & {\SI{5}{\decibel}} & {\SI{6}{\decibel}} \\
    \midrule
    ECCT~\cite{Choukroun2022error} & $1.52\mathrm{e}{-2}$ & $2.53\mathrm{e}{-3}$ & $2.86\mathrm{e}{-4}$ & $6.41\mathrm{e}{-2}$ & $8.65\mathrm{e}{-3}$ & $4.68\mathrm{e}{-4}$\\
    Pre + ECCT & $1.45\mathrm{e}{-2}$ & $2.43\mathrm{e}{-3}$ & $2.76\mathrm{e}{-4}$ & $4.96\mathrm{e}{-2}$ & $5.63\mathrm{e}{-3}$ & $2.77\mathrm{e}{-4}$\\
    ECCT + Post & $6.70\mathrm{e}{-3}$ & $9.23\mathrm{e}{-4}$ & $7.47\mathrm{e}{-5}$ & $3.36\mathrm{e}{-2}$ & $3.42\mathrm{e}{-3}$ & $1.26\mathrm{e}{-4}$\\
    Pre + ECCT + Post & $6.47\mathrm{e}{-3}$ & $8.96\mathrm{e}{-4}$ & $7.25\mathrm{e}{-5}$ & $2.91\mathrm{e}{-2}$ & $2.55\mathrm{e}{-3}$ & $8.63\mathrm{e}{-5}$\\
    Pre + ECCT + Post + loss~\eqref{eq:hard_loss_practice} & $\mathbf{2.23\mathrm{\mathbf{e}}{-3}}$ & $\mathbf{1.63\mathrm{\mathbf{e}}{-4}}$ & $\mathbf{5.41\mathrm{\mathbf{e}}{-6}}$ & $\mathbf{2.26\mathrm{\mathbf{e}}{-2}}$ & $\mathbf{1.56\mathrm{\mathbf{e}}{-3}}$ & $\mathbf{4.26\mathrm{\mathbf{e}}{-5}}$\\
    \bottomrule
    \end{tabular}
    % \vspace{-4mm}
    \label{tab:ablation}
    \end{table*}
    
    % \subsection{Ablation Study}\label{sec:ablation}
       
    To assess the impact of each contribution within the hybrid decoding framework, we compare the FER performance of different decoder configurations, as shown in Table~\ref{tab:ablation}. 
    In this table, ``Pre'' and ``Post'' represent hard decision pre- and post-decoders, respectively while ``ECCT'' refers to the original ECCT trained on the binary cross entropy loss function~\eqref{eq:bce}.
    The results show that the hybrid decoder combining ECCT with both pre- and post-decoders outperforms decoder configurations using only a single auxiliary decoder, implying the importance of both pre- and post-decoders.
    Moreover, the hybrid decoder trained with the proposed loss~\eqref{eq:hard_loss_practice} achieves the best FER performance among all configurations.
    This confirms that the proposed loss function effectively improves ECCT within the hybrid decoding framework, thereby enhancing overall decoding performance.

    \section{Conclusion}\label{sec:conclusion}
    
    In this paper, we presented the first observation of the error floor problem of ECCT.
    To address this issue, we proposed hybrid decoding algorithms for ECCT, along with an optimized loss function.
    By integrating ECCT with hard decision decoders, the hybrid decoding approach effectively corrects a wider range of errors, thereby enhancing decoding performance.
    The proposed loss function enables ECCT to focus on correcting only critical errors, optimizing the overall hybrid decoding process.
    Our simulation results show that the hybrid decoder effectively lowers the error floor, achieving improved FER performance in the high SNR region. 
    Moreover, the hybrid decoder significantly enhances decoding performance in the waterfall region.
    The proposed hybrid decoding algorithm and its associated training method, which accounts for the pre- and post-decoders, can be generalized to other neural decoders. 
    This approach provides a versatile strategy for integrating neural decoders and conventional decoders to effectively lower the error floor and enhance decoding performance.

    \section*{Acknowledgment}
    
    This work was partly supported by Institute of Information \& communications Technology Planning \& Evaluation (IITP) grant funded by the Korea government (MSIT) (RS-2024-00398449, Network Research Center: Advanced Channel Coding and Channel Estimation Technologies for Wireless Communication Evolution) and the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. RS-2023-00212103).
%%%%%%
%% Appendix:
%% If needed a single appendix is created by
%%
%\appendix
%%
%% If several appendices are needed, then the command
%%
% \appendices
%%
%% in combination with further \section commands can be used.
%%%%%%

% \section*{Acknowledgment}

% We are indebted to Michael Shell for maintaining and improving
% \texttt{IEEEtran.cls}. 

    \appendices

    \bibliographystyle{IEEEtran}
    \bibliography{abrv,mybib}
%%%%%%
%% To balance the columns at the last page of the paper use this
%% command:
%%
%\enlargethispage{-1.2cm} 
%%
%% If the balancing should occur in the middle of the references, use
%% the following trigger:
%%
%\IEEEtriggeratref{7}
%%
%% which triggers a \newpage (i.e., new column) just before the given
%% reference number. Note that you need to adapt this if you modify
%% the paper.  The "triggered" command can be changed if desired:
%%
%\IEEEtriggercmd{\enlargethispage{-20cm}}
%%
%%%%%%


%%%%%%
%% References:
%% We recommend the usage of BibTeX:
%%
%\bibliographystyle{IEEEtran}
%\bibliography{definitions,bibliofile}
%%
%% where we here have assumed the existence of the files
%% definitions.bib and bibliofile.bib.
%% BibTeX documentation can be obtained at:
%% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
%%%%%%



%% Or you use manual references (pay attention to consistency and the
%% formatting style!):




\end{document}


%%%%%%
%% Some comments about useful packages
%% (extract from bare_conf.tex by Michael Shell)
%%

% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.


% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.


% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex


% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath


% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment for describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx


% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array

% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig


% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm's dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix


% *** PDF and URL PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.



% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
%%%%%%


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
