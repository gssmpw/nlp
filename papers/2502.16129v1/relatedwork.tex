\section{Related Work}
\label{sec:relat}

\textbf{Dynamic Facial Expression Recognition.} In contrast to SFER techniques that focus solely on spatial features within an image, DFER approaches must consider both spatial and temporal information simultaneously. Transformer-based networks have gained popularity in recent literature for their ability to extract spatial and temporal information. Zhao et al. \cite{zhao2021former} introduce the dynamic facial expression recognition transformer (Former-DFER), comprising a convolutional spatial transformer (CS-Former) and a temporal transformer (T-Former). Ma et al. \cite{ma2022spatio} propose the spatial-temporal Transformer (STT) to capture discriminative features within each frame and model contextual relationships among frames. To reduce the impact of noisy frames on the dynamic facial expression recognition (DFER) task, a dynamic-static fusion module is used to extract more robust and discriminative spatial features from both static and dynamic features \cite{li2022nr,li2022intensity}. Wang et al. \cite{wang2022dpcnet} propose the Dual Path multi-excitation Collaborative Network (DPCNet) to learn critical information for facial expression representation from fewer key frames in videos. In addition, Zhang et al. proposed to try to improve dynamic expression recognition by means of increasing zero-shot generalization ability\cite{zhang2025generalizable}, sample feature diversification\cite{zhang2024leave} and symmetric noisy labels\cite{zhang2024open}.

% While DFER shares several commonalities with general video understanding tasks, the form and property of dynamic facial expressions differ from other tasks such as action recognition. Facial expressions involve significantly smaller movements than actions, and emotions are relatively stable human faculties, unlike the ability to perform multiple actions simultaneously. Our proposal, in contrast to acquiring generalized video representations, involves disentangling representations of short-term movements and long-term emotional changes in DFER. Furthermore, we introduce a Key Expression Re-sampling Framework to identify key expressions and mitigate the effects of non-target expressions.

\begin{figure*}[ht!]
  \centering
  \includegraphics[width=0.9\linewidth]{architecture.pdf}

  \caption{An overview of the Key Expression Re-sampling Framework. (a) The Key Expression Detecting Network. The input video is sampled uniformly and fed into a tiny backbone network to quickly obtain a global summary and predict the key expression. (b) The Dual-Stream Hierarchical Network. Taken the key expression predicted by (a), this network learns the representation through disentangling the short-term facial movements and long-term emotion changes with a dual-stream hierarchical design. }
  \label{fig:architecture}
\end{figure*}

\textbf{Long-tailed noisy label problem.} The topic of long-tailed noisy label problem has gained prominence due to its relevance to real-world applications and its inherent complexity. The effectiveness of existing noisy label methods, which rely on small loss tricks, has been found to be limited when dealing with long-tail situations. Therefore, there is a need for a new framework to address this challenge. RoLT \cite{wei2021robust} has identified this issue and proposed a prototypical noise detection method that is robust to label noise by designing a distance-based metric. JA+SL \cite{karthik2021learning} has demonstrated that self-supervised learning approaches are effective in handling severe class imbalance. Additionally, H2E \cite{yi2022identifying} has defined long-tailed noises as 'hard' noises and proposed the Hard-to-Easy framework to convert 'hard' noises to 'easy' ones.

% This research paper presents a novel approach to addressing the issue of noisy labels through the examination of various frames in a video. The proposed method differs from the conventional small loss trick, which is ineffective when small-class or challenging samples share significant losses with noisy samples. Instead, the agreement approach effectively differentiates between hard and noisy samples, thereby allowing for adjustments to the loss function by appropriately increasing or decreasing it.