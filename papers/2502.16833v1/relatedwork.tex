\section{Related Work}
To better situate this study and provide references for system design, we conducted a thorough literature review of research on the following three topics: 1) how do researchers integrate AI in the design process, including AI-driven design tools and AI collaborators, 2) common communication channels in collaboration and how they were transferred to human-AI collaboration, and 3) mechanisms for supporting awareness in co-located and distributed human-human collaboration, and human-AI collaboration.

\subsection{Integrating AI in Design}
Design requires knowledge, information, and skills from diverse areas, and has long been a task that demands efforts beyond a single designer due to its burgeoning complexity~\cite{Rodgers1998role, Tang1996AI, Zhou2024Understanding}. The need for collaboration and assistance thus naturally appeared, and AI is one important technology that can help. In early research, AI was considered as a design support tool to assist in quantifiable or functional design tasks~\cite{Rodgers1998role}. For example, \citet{Chakrabarti1994Two} proposed a two-step approach to work out viable mechanical design solutions based on a set of known rules, and \citet{Grecu1996Learning} explored AI support for parametric design, where the product has already been defined except for combining several detailed values (e.g., color) with various options. In these tasks, AI was expected to reduce tedious work and give predictable outputs. As research on computational creativity booms, AI gained creativity and moved from the above ``routine design tasks'' to ``creative design tasks'' in the design process. Designers can now use various AI-driven tools to obtain ideas~\cite{Liu20233DALL, Koch2019May}, organize thoughts~\cite{Zhong2024Causal}, explore design space~\cite{Camburn2020Computer, Wang2024RoomDreaming}, etc. However, these applications are mainly design tools that require designers to initiate operations.

Recently, the development of AI abilities in generation and reasoning abilities~\cite{Wang2024ModaVerse, Edwards2024Sketch2Prototype} allowed AI to take an active role in design~\cite{Li2023Impression}, thus being prospective in becoming a design collaborator. Researchers are striving for better human-AI design collaboration from both theoretical and practical perspectives. \citet{Rezwana2023COFI} did a thorough literature review on human-AI co-creation and proposed a COFI framework to describe the interactions between collaborators and with the shared product in co-creative systems. \citet{Shi2023Understanding} also reviewed articles but specifically focused on designer-AI collaboration, and concluded five characteristics, including scope, access, agency, flexibility, and visibility, to help describe designer-AI collaboration and identify existing problems. Results from both works elaborated on problems in human-AI communication, with the former one presenting the lack of ``human to AI consequential communication'' and ``AI to human communication'', and the latter one pointing out the importance and a dearth of multiple communication modalities other than visual ones. Other more practice-oriented studies focus on specific design tasks, exploring interaction pathways and guidelines for human-AI collaboration. For example, \citet{Zhou2024Understanding} explored the nonlinear collaboration between graphic designers and AI in the OptiMuse system using the WoZ method, and proposed a co-design framework. \citet{Pan2023Diagrams} explored how AI can intelligently collaborate with people through a WoZ-like method, and implemented a collaborative diagram editing system guided by obtained insights.

Similar to \citet{Zhou2024Understanding} and \citet{Pan2023Diagrams}, our work seeks to explore the human-AI collaborative design process. Grounded in the problems identified by \citet{Rezwana2023COFI} and \citet{Shi2023Understanding}, we specifically focus on problems in human-AI communication and explore the potential of AI awareness to alleviate this problem.

\subsection{Communicating in Human-Human and Human-AI Collaboration}
In traditional human-human co-located design collaboration, people share expertise, ideas, resources, or responsibilities in the design process, to which communication is crucial~\cite{Chiu2002187an}. Verbal communication is the most common way that people exchange information. People leverage various linguistic strategies and non-verbal communicative cues to enhance the efficiency of verbal communication, for example, deictic words can specify the content of the current discussion in a highly contracted way~\cite{Tory2008deictic}, gestures can focus attention and represent ideas or features, and facial expressions can assist in judging one's true feelings~\cite{Ge2021emotion}. When collaboration is moved online and people work distributedly through collaborative systems, verbal communication, and its corresponding communication strategies require special support to function~\cite{Gutwin2002descriptive}. However, as remote collaboration becomes common and frequent, collaborative systems using text-based communication have matured rapidly~(e.g., Comment function in Figma\footnote{\url{https://www.figma.com/}} and Miro\footnote{\url{https://miro.com}}).

Similar to human-human distributed collaboration, communication and interaction in current human-AI collaboration is mainly based on text-based conversations. For example, \citet{muller2024group} implemented a text-based chatbot to facilitate the brainstorming process, and \citet{Kuang2024UX} designed a collaborative system for UX designers to identify usability problems with text-based conversation AI assistants. However, communicating with AI through text is not necessarily an optimal choice for human-AI communication, especially in the design process. Previous research has found communicating through voice leads to higher trust in both humans and chatbots compared to text~\cite{Burri2018, Bente2008Networking}. Additionally, the need for composing textual prompts and conducting other operations through visual channel can cause increased cognitive load~\cite{Tankelevitch2024Metacognitive, Vermeulen2008Sensory}, thus making it harder for mutual understanding in the design process~\cite{Gmeiner2023Exploring}. In the more general field of human-AI collaboration, researchers have found verbal communication to be a favored communication strategy because of its direct and efficient feature in difficult tasks~\cite{Zhang2021ideal}. In a recent research about sketching, researchers pointed out that people ``\textit{usually use language to talk about real, distant, invisible, imaginary, or conceptual objects}''~\cite[p.~4-5]{Rosenberg2024DrawTalking}, which are similar to what happens in the design process~\cite{Kokotovich2016abstraction, Ungureanu2021Analysing}. However, systems supporting designers to communicate with AI through speech are underexplored~\cite{Rezwana2023COFI, Shi2023Understanding}. Therefore, we allow designers to communicate with AI through speech in this study to gain insights for future human-AI collaborative design systems that support verbal communication.

\begin{figure*}[!th]
    \centering
    \includegraphics[width=1\linewidth]{interface.png}
    \caption{The interface example of the human-AI design collaborative system: a) Canvas. Canvas is the main interaction area where designers edit information using tools in the toolbar on the bottom right corner, and AI can also upload generated images to the canvas. b) Chat box. Communication history and speech recognition results are displayed in this area, and designers can also type here to initiate conversation in case of need. c) Camera screen. This is presented \textit{in the Aware condition only} to indicate what AI can see. \textit{Note: The interface is a translated version of the original one used for user studies.}}
    \label{fig:interface}
    \Description{This figure illustrates the system interface. The interface consists of three areas: canvas, chat box, and camera screen. The canvas, marked as area A, is on the right side, taking up about three-quarters of the interface. There is a user case from one participant, a toolbar in the bottom right corner, and a delete button in the top right corner of the canvas. The chat box and camera screen areas together take up about a quarter of the interface, marked as area B and area C respectively. The chat box takes up more than two-thirds of this area, with some communication histories of the participant and AI presenting in it and an input area for presenting speech recognition results at the bottom. The camera screen area takes up less than a third of this area, displaying the captured screen.}
\end{figure*}


\subsection{Supporting Awareness in Collaboration}
The term ``awareness'' had been combined with different words to coin a new term and specify a specific research scope, like shared awareness~\cite{Chakrabarti1994Two} and situation awareness~\cite{Endsley1988Design}. Similar as their overall meanings were, ``awareness'' has not been defined and used consistently~\cite{Gross2013Supporting}. In this paper, we adopt the definition provided by~\citet[p.~107]{Dourish1992awareness}:

\begin{quote}
    \textit{``[...]awareness is an understanding of the activities of others, which provides a context for your own activity. This context is used to ensure that individual contributions are relevant to the group's activity as a whole, and to evaluate individual actions with respect to group goals and progress. The information, then, allows groups to manage the process of collaborative working.''}
\end{quote}

Awareness is critical to successful design collaboration for providing information including shared information, group and individual activities, and coordination, to contextualize one's activity~\cite{Dourish1992awareness}. In co-located human-human collaboration, awareness activities are supported by human perceptions, which can happen naturally~\cite{Gutwin2002descriptive}. However, when collaboration is distributed, awareness activities require supporting mechanisms in the collaborative system to proceed~\cite{Gutwin2002descriptive}, and researchers have developed various mechanisms to enable people to have awareness of other collaborators and the environment in different scenarios. \citet{Gutwin1996Widgets} designed a set of interface widgets to support distributed workers having awareness of collaborators' current activities and view, including radar view, WYSIWIS (What You See Is What I See) view, WYSIWID (What You See Is What I Do) view, etc. \citet{Cidota2016HMD} compared different notification mechanisms for people collaborating through AR. \citet{Dehler2011guiding} designed a collaborative system and presented participants' knowledge levels to allow a better understanding of the group and each other. \citet{Janssen2011Group} visualized group members' contributions in online collaboration to examine how collaborative learning is impacted by group awareness.

Although studies and applications of awareness mechanisms in human-human collaboration are abundant, the idea of equipping AI with awareness ability is underexplored due to the infancy of human-AI collaboration research and the difficulty of transferring this human ability to AI. However, previous research has demonstrated that allowing users to have awareness of AI presence positively contributes to user engagement and collaborative experience~\cite{Rezwana2021penpal}, implying the potential advantages of applying the awareness theory in human-AI collaboration. Therefore, in this study, we first explored the impact of allowing AI to have awareness (i.e., AI is aware of the designer's design activities and current working content) through the WoZ method to ground future design and research. By exploring the potential of transferring the ``awareness'' concept in human-AI collaboration, we expect to guide the design of AI collaborators that have a certain level of initiative in a shared workspace~\cite{Salikutluk2024situational, He2024Canvas}, adjust their output according to real-time contextual information~\cite{Ma2023ProactiveAgent}, and infer deeper reasons and meaning behind the literal senses of obtained visual and textual information~\cite{Liu2024ComPeer, Liang2019Implicit} to support more dynamic and difficult tasks like design through human-AI collaboration.

\begin{figure*}[bthp]
    \centering
    \includegraphics[width=1\linewidth]{systemStructure.png}
    \caption{The overview of system structure and data stream. In our study, the participants \textbf{a)} interacted and communicated with the frontend (i.e., the user interface in Figure~\ref{fig:interface}) and received feedback from AI. The backend consisted of two Wizards and generative models. In the Aware condition, Wizard A would \textbf{b)} obtain awareness information from the camera screen and canvas, \textbf{c)} compose prompts based on predefined scripts and send them to Wizard B. In the Non-aware condition, the prompt sent in process \textbf{(c)} would not contain awareness information. Wizard B in both conditions supported the human-AI communication process by \textbf{d)} processing speech recognition results and input with canvas information, and \textbf{e)} sending prompts coming from the participants and Wizard A to generative models. \textbf{f)} All generation results would be uploaded to the canvas.}
    \Description{The overview of system structure and data stream. Areas from left to right are named: User area, Frontend area, Backend area. The user area presents a box representing the end user, the frontend area presents a draft of the system interface described in Figure 1, and the backend area consists of three boxes, with two boxes symbolizing two wizards in respective, and the remaining box symbolizing the generative models at the backend. The data streams are illustrated by two types of arrows, with the solid purple arrows representing data streams in both the Aware and Non-aware conditions, and the dashed gray arrows only representing data streams in the Aware condition. Data stream A describes the mutual data exchange between the user and the frontend, illustrated by a solid purple double arrow connecting the user area and the frontend area. Data stream B describes Wizard A's comprehension of awareness information, gained from the canvas and the camera screen, and is illustrated by two dashed gray arrows, pointing from the canvas and camera screen areas in the frontend area to Wizard A in the backend area. Data stream C describes the process of Wizard A sending prompts to Wizard B, illustrated by a solid purple arrow pointed from Wizard A to Wizard B. Data stream D describes the process Wizard B receives information from the frontend, illustrated by two solid purple arrows pointed from the canvas and chat box areas in the frontend area to Wizard B in the backend area. Data stream E describes the process of Wizard B sending crafted prompts to the generative models, illustrated by a solid purple arrow pointed from Wizard B to Generative Models. Data stream F describes the process of generative models sending the outputs to the frontend, illustrated by two solid purple arrows pointed from the generative model in the backend area to the canvas and chat box areas in the frontend area.}
    \label{fig:systemStructure}
\end{figure*}