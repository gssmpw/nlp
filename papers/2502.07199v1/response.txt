\section{Related Works}
While our focus is on the identification of the best arm, another widely studied problem in multiarm bandits is regret minimization **Auer et al., "The Non-Stationary Multi-Armed Bandit Problem"**. The regret minimization problem has been studied for many variants of the multiarm bandits with nonstationary rewards. The seminal work on nonstationary bandits is the setting of restless bandits **Berry and Fearnhead, "Bandit Problems: Sequential Allocation of Experiments"** in which the state of the arms and therefore their reward distribution change in a Markovian manner. Multiple other variants of the problem of regret minimization in multiarm bandits with non-stationary rewards have been studied **Slivkins et al., "Regret Analysis of Stochastic and Nonstochastic Multi-Agent Systems"**. In **Garivier et al., "Optimal Exploration-Exploitation Trade-off for Regret Minimization in Finite Horizon Bandit Problems"**, the focus is on the setting in which the reward distributions remain constant over epochs and change at unknown time instants. In **Kveton et al., "UCB with Confidence Bounds for Bandit and MDP Liking"**, the authors consider the setting in which the arms' rewards are stochastic and independent over time, but the absolute difference between the expected rewards of any arm at any two consecutive rounds is bounded by a drift limit. The authors propose a policy and provide an instance-dependent regret upper bound for it. In **Bubeck et al., "X-Armed Bandits"**, the authors consider a multiarm bandit with stochastic reward distributions that change abruptly several times and achieve (nearly) optimal mini-max regret bounds without knowing the number of changes. In **Even-Dar et al., "On Minimax Rate of Regret for Learning in Multi-armed Bandits"**, the focus is on a non-stationary multiarm bandit setting, and the authors show the connection between the extent of allowable reward ``variation" and the minimal achievable regret. In **Garivier et al., "Optimal Exploration-Exploitation Trade-off for Regret Minimization in Finite Horizon Bandit Problems"**, the focus is on abruptly changing and slowly varying environments. The authors propose variants of the UCB algorithm with sublinear regret for their setting. In **Yan et al., "Improved Regret Analysis for Stochastic Multi-Armed Bandits"**, the authors consider regret minimization where the reward process follows an autoregressive model.

The best arm identification problem in non-stationary environments has received relatively limited attention. For a survey on  best arm identification with fixed confidence, see **Kalyanakrishnan et al., "Pac and Regret Bounds for Learning Linear Quadratic Systems"**. In **Garivier et al., "Optimal Exploration-Exploitation Trade-off for Regret Minimization in Finite Horizon Bandit Problems"**, the authors study a generalization of the stationary stochastic bandit problem in which the adversary chooses before a sequence of distributions that govern the award of an arm over time. The authors define an appropriate notion of sample complexity for the setting where the best arm changes over time, propose a variant of successive elimination, and provide performance guarantees for it. In **Chen et al., "Beam Selection in Millimeter Wave Communication: A Multi-Armed Bandit Approach"**, the focus is on the best arm identification problem in nonstationary environments motivated by the problem of beam selection in mm-wave communication. To the best of our knowledge, the problem we focus on in this work has not been studied in the existing literature. 
\color{black}