\section{Related Works}
While our focus is on the identification of the best arm, another widely studied problem in multiarm bandits is regret minimization \cite{lattimore2020bandit,auer2010ucb}. The regret minimization problem has been studied for many variants of the multiarm bandits with nonstationary rewards. The seminal work on nonstationary bandits is the setting of restless bandits \cite{whittle1988restless} in which the state of the arms and therefore their reward distribution change in a Markovian manner. Multiple other variants of the problem of regret minimization in multiarm bandits with non-stationary rewards have been studied \cite{garivier2008upper, krishnamurthy2021slowly, auer2019adaptively, besbes2014stochastic, wei2018abruptly,bacchiocchi2024autoregressive}. In \cite{garivier2008upper}, the focus is on the setting in which the reward distributions remain constant over epochs and change at unknown time instants. In \cite{krishnamurthy2021slowly}, the authors consider the setting in which the arms' rewards are stochastic and independent over time, but the absolute difference between the expected rewards of any arm at any two consecutive rounds is bounded by a drift limit. The authors propose a policy and provide an instance-dependent regret upper bound for it. In \cite{auer2019adaptively}, the authors consider a multiarm bandit with stochastic reward
distributions that change abruptly several times and achieve (nearly) optimal mini-max regret bounds without knowing the number of changes. In \cite{besbes2014stochastic}, the focus is on a non-stationary multiarm bandit setting, and the authors show the connection between the extent
of allowable reward ``variation" and the minimal achievable regret. In \cite{wei2018abruptly}, the focus is on abruptly changing and slowly varying environments. The authors propose variants of the UCB algorithm with sublinear regret for their setting. In \cite{bacchiocchi2024autoregressive}, the authors consider regret minimization where the reward process follows an autoregressive model.

The best arm identification problem in non-stationary environments has received relatively limited attention. For a survey on  best arm identification with fixed confidence, see \cite{jamieson2014best}. In \cite{allesiardo2017non}, the authors study a generalization of the stationary stochastic bandit problem in which the adversary chooses before a sequence of distributions that govern the award of an arm over time. The authors define an appropriate notion of sample complexity for the setting where the best arm changes over time, propose a variant of successive elimination, and provide performance guarantees for it. In \cite{ghatak2024best}, the focus is on the best arm identification problem in nonstationary environments motivated by the problem of beam selection in mm-wave communication. To the best of our knowledge, the problem we focus on in this work has not been studied in the existing literature. 
\color{black}