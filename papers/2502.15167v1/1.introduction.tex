

\section{Introduction}
\IEEEPARstart{I}{n} recent years, flourishing of Artificial Intelligence Generated Content (AIGC) has sparked significant advancements in modalities such as text, image, audio, and even video. 
Among these, AI-Generated Image (AGI) has garnered considerable interest from both researchers and the public.
Plenty of remarkable AGI models and online services, such as StableDiffusion\footnote{\url{https://stability.ai/}}, Midjourney\footnote{\url{https://www.midjourney.com/}}, and FLUX\footnote{\url{https://blackforestlabs.ai/}}, offer users an excellent creative experience.
However, users often remain critical of the quality of the AGI due to image distortions or mismatches with user intentions.
Consequently, methods for assessing the quality of AGI are becoming increasingly crucial to help improve the generative capabilities of these models.

Unlike Natural Scene Image (NSI) quality assessment, which focuses primarily on perception aspects such as sharpness, color, and brightness, AI-Generated Image Quality Assessment (AGIQA) encompasses additional aspects like correspondence and authenticity. 
Since AGI is generated on the basis of user text prompts, it may fail to capture key user intentions, resulting in misalignment with the prompt.
Furthermore, authenticity refers to how closely the generated image resembles real-world artworks, as AGI can sometimes exhibit logical inconsistencies.
While traditional IQA models may effectively evaluate perceptual quality, they are often less capable of adequately assessing aspects such as correspondence and authenticity.

\begin{figure}\label{fig:radar}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/radar_plot.pdf}
    \caption{A comparison on quality, correspondence, and authenticity aspects of AIGCIQA2023~\cite{wang2023aigciqa2023} dataset illustrates the superior performance of our method.}
\end{figure}

Several methods have been proposed specifically for the AGIQA task, including metrics designed to evaluate the authenticity and diversity of generated images~\cite{gulrajani2017improved,heusel2017gans}. 
Nevertheless, these methods tend to compare and evaluate grouped images rather than single instances, which limits their utility for single image assessment.
Beginning with AGIQA-1k~\cite{zhang2023perceptual}, a series of AGIQA databases have been introduced, including AGIQA-3k~\cite{li2023agiqa}, AIGCIQA-20k~\cite{li2024aigiqa}, etc.
Concurrently, there has been a surge in research utilizing deep learning methods~\cite{zhou2024adaptive,peng2024aigc,yu2024sf}, which have significantly benefited from pre-trained models such as CLIP~\cite{radford2021learning}. 
These approaches enhance the analysis by leveraging the correlations between images and their descriptive texts.
While these models are effective in capturing general text-image alignments, they may not effectively detect subtle inconsistencies or mismatches between the generated image content and the detailed nuances of the textual description.
Moreover, as these models are pre-trained on large-scale datasets for broad tasks, they might not fully exploit the textual information pertinent to the specific context of AGIQA without task-specific fine-tuning.
To overcome these limitations, methods that leverage Multimodal Large Language Models (MLLMs)~\cite{wang2024large,wang2024understanding} have been proposed.
These methods aim to fully exploit the synergies of image captioning and textual analysis for AGIQA.
Although they benefit from advanced prompt understanding, instruction following, and generation capabilities, they often do not utilize MLLMs as encoders capable of producing a sequence of logits that integrate both image and text context.

In conclusion, the field of AI-Generated Image Quality Assessment (AGIQA) continues to face significant challenges: 
(1) Developing comprehensive methods to assess AGIs from multiple dimensions, including quality, correspondence, and authenticity; 
(2) Enhancing assessment techniques to more accurately reflect human perception and the nuanced intentions embedded within prompts; 
(3) Optimizing the use of Multimodal Large Language Models (MLLMs) to fully exploit their multimodal encoding capabilities.

To address these challenges, we propose a novel method M3-AGIQA (\textbf{M}ultimodal, \textbf{M}ulti-Round, \textbf{M}ulti-Aspect AI-Generated Image Quality Assessment) which leverages MLLMs as both image and text encoders. 
This approach incorporates an additional network to align human perception and intentions, aiming to enhance assessment accuracy. 
Specially, we distill the rich image captioning capability from online MLLMs into a local MLLM through Low-Rank Adaption (LoRA) fine-tuning, and train this model with human-labeled data. The key contributions of this paper are as follows:
\begin{itemize}
    \item We propose a novel AGIQA method that distills multi-aspect image captioning capabilities to enable comprehensive evaluation. Specifically, we use an online MLLM service to generate aspect-specific image descriptions and fine-tune a local MLLM with these descriptions in a structured two-round conversational format.
    \item We investigate the encoding potential of MLLMs to better align with human perceptual judgments and intentions, uncovering previously underestimated capabilities of MLLMs in the AGIQA domain. To leverage sequential information, we append an xLSTM feature extractor and a regression head to the encoding output.
    \item Extensive experiments across multiple datasets demonstrate that our method achieves superior performance, setting a new state-of-the-art (SOTA) benchmark in AGIQA.
\end{itemize}

In this work, we present related works in Sec.~\ref{sec:related}, followed by the details of our M3-AGIQA method in Sec.~\ref{sec:method}. Sec.~\ref{sec:exp} outlines our experimental design and presents the results. Sec.~\ref{sec:limit},~\ref{sec:ethics} and~\ref{sec:conclusion} discuss the limitations, ethical concerns, future directions and conclusions of our study.