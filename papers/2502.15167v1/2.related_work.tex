
\section{Related Work} \label{sec:related}
\subsection{Blind Image Quality Assessment}
\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/m3-agiqa-methods.pdf}
    \caption{Different implementations of IQA and AGIQA Methods.}
    \label{fig:related_work}
\end{figure}
For NSIs, numerous studies over the past few decades have focused on IQA task using various approaches. 
Depending on whether a referenced image is used during assessment, these methods can be classified into Full-Reference (FR) and No-Reference (NR) IQA, with NR IQA being more challenging and often referred to as Blind IQA (BIQA).
Traditional BIQA methods, such as NIQE~\cite{mittal2012making}, utilize spatial Natural Scene Statistics (NSS) from a statistical perspective. 
Subsequently, similar works have been developed~\cite{xue2014blind,zhang2014blind,zhang2015feature,xu2016blind} following NIQE.
A significant advancement over these handcrafted feature-based methods is the adoption of deep learning, which has demonstrated superior performance in BIQA tasks~\cite{hou2014blind,kang2014convolutional,zhang2018blind,yang2020ttl,yang2020blind,sun2022blind,zhu2022blind,sun2022graphiqa}.
For example, DBCNN~\cite{zhang2018blind} employs two Convolutional Neural Networks (CNNs) to address both synthetic and authentic distortions. 
HyperIQA~\cite{Su_2020_CVPR} introduces a self-adaptive hyper network that addresses real-world variations through a three-stage process: content understanding, perception rule learning, and quality prediction.
StairIQA~\cite{sun2022blind} integrates information hierarchically from different stages of a ResNet~\cite{he2016deep} backbone with a staircase structure.
As vision-language pre-trained models like CLIP~\cite{radford2021learning}, BLIP~\cite{li2022blip}, and ViT~\cite{dosovitskiy2020image} gain popularity, new methods have emerged that leverage these models to evaluate image quality by assessing text-image similarity.
CLIP-IQA~\cite{wang2023exploring} calculates the similarity between an image and two quality-implied prompts to align with human-labeled scores.
Similarly, LIQE~\cite{zhang2023blind} introduces a multitask learning approach with an expanded set of textual templates and utilizes CLIP to evaluate vision-text similarities. TempQT~\cite{shi2023blind} leverages Transformer~\cite{vaswani2017attention} encoder and decoder to pre-train the error map and then a feature fusion is done byy a vision Transformer to predict the quality score.

\subsection{AI-Generated Image Quality Assessment}
With advancements in text-to-image synthesis, assessing the quality of AGIs has become increasingly important to align with the human visual system (HVS) and the intent implied in the generation prompt.
HPS~\cite{wu2023better} and PickScore~\cite{kirstain2023pick} train a CLIP-based function to predict user preferences by selecting the most preferred one from a group or a pair of AGIs.
ImageReward~\cite{xu2024imagereward} developed the first text-to-image reward model based on BLIP~\cite{li2022blip}, using a dataset of 137k prompt-image pair rankings sampled from DiffusionDB~\cite{wang2022diffusiondb}, which contains a vast collection of prompt-image pairs without scores. 
Some research pays attention specifically on artistry aspect such as ArtScore~\cite{chen2024learning}, which could be imcomplete for AGIQA task.
Then, several datasets using Mean Opinion Score (MOS) as the evaluation target which presents with human perference well, are introduced to support AGIQA task: AGIQA-1k~\cite{zhang2023perceptual}, AGIQA-3k~\cite{li2023agiqa}, AIGCIQA2023~\cite{wang2023aigciqa2023}, AIGCIQA-20k~\cite{li2024aigiqa}, and I2IQA~\cite{yuan2023pku}. 
Based on these datasets, more methods that perform much better have emerged.
AMFF-Net~\cite{zhou2024adaptive} evaluates AGIs from quality, correspondence and authenticity aspects by scaling images up and down to capture the global and local features, then utilizes CLIP as the text and image encoders.
During the NTIRE2024 Quality Assessment of AI-Generated Content Challenge~\cite{liu2024ntire}, further advancements were made.
Inspired by LIQE~\cite{zhang2023blind}, IPCE~\cite{peng2024aigc} utilizes CLIP to encode quality-aware prompts that include both the quality label and the image prompt, achieving first place in the challenge.
SF-IQA~\cite{yu2024sf}, employing a multilayer feature extractor and fusion module, excels in identifying local and global quality-aware features.
Moreover, MoE-AGIQA~\cite{yang2024moe} sets new benchmarks by integrating visual degradation-aware and semantic-aware networks with a mixture-of-experts module.

While these methods effectively leverage pre-trained vision-language models, they do not fully exploit the potential of textual analysis, which would result in inferior performance compared with larger scale models.

\subsection{Multimodal Large Language Models IQA}
Despite great advancements in models like CLIP, which can align images with text, recent approaches have started to explore the potential of Multimodal Large Language Models (MLLMs) for the IQA task.
MLLMs inherit strong reasoning and instruction-following capabilities of Large Language Models (LLMs) and can serve as powerful image evaluators when given appropriate prompts.
TIFA~\cite{hu2023tifa} introduces a metric to evaluate the faithfulness of text-to-image generation by using LLMs to generate relevant questions for existing Visual Question Answering (VQA) methods.
LLMScore~\cite{lu2024llmscore} utilizes multi-granularity compositionality capture to evaluate the correspondence between the image and the text prompt by leveraging LLMs as image descriptor and evaluator.
Additionally, Q-Bench~\cite{wu2023q} proposes a systematic benchmark to measure the low-level visual perception and understanding abilities of MLLMs, employing a simple softmax pooling strategy to quantitatively assess the text-image correspondence.
Q-Instruct~\cite{wu2024q} builds upon the same Softmax pooling strategy by constructing 200k instruction-response pairs related to low-level visual attributes.
Q-Align~\cite{wu2023qalign} teaches MLLM to judge based on discrete text levels by converting human rating scores into text, and demonstrates an advantage above score-level variants.
While these methods address general IQA tasks, specialized approaches for AGIQA are also emerging.
MA-AGIQA~\cite{wang2024large} integrates semantically informed guidance with quality-aware features through a Mixture of Experts (MoEs) structure, leveraging both MLLM and traditional DNN approaches for superior performance.
MINT-IQA~\cite{wang2024understanding} extends AIGCIQA2023~\cite{wang2023aigciqa2023} database to AIGCIQA2023+ by adding image quality descriptions and applying instruction tuning to MLLMs, resulting in considerable improvements in assessing human visual preferences from multiple perspectives.

Although these methods utilize MLLM abilities like prompt understanding, instruction following and text generation, they often overlook the potential encoding capability of MLLMs.

\subsection{Long Short-Term Memory Network}
The idea of Long Short-Term Memory Network (LSTM)~\cite{hochreiter1997long} was developed to address the gradient vanishing problem in traditional Recurrent Neural Networks (RNNs). 
Despite its effectiveness, LSTM faces challenges in handling long sequence and lacks parallelizability, which can be critical in contemporary deep learning tasks.
The extended LSTM (xLSTM)~\cite{beck2024xlstm} introduces additional exponential gating and modified memory structures, making it competitive with advanced architectures like Transformers and State Space Models (SSMs).
VisionLSTM~\cite{alkin2024visionlstm} (ViL) represents a further evolution, incorporating a stack of alternating mLSTM blocks designed to efficiently process nonsequential inputs like image patches.
Our method employs xLSTM blocks with linear computational and memory complexity as sequential data processing units, which boosts the performance.
