\section{Related Work}
\label{sec:related}
\subsection{Blind Image Quality Assessment}
\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/m3-agiqa-methods.pdf}
    \caption{Different implementations of IQA and AGIQA Methods.}
    \label{fig:related_work}
\end{figure}
For NSIs, numerous studies over the past few decades have focused on IQA task using various approaches. 
Depending on whether a referenced image is used during assessment, these methods can be classified into Full-Reference (FR) and No-Reference (NR) IQA, with NR IQA being more challenging and often referred to as Blind IQA (BIQA).
Traditional BIQA methods, such as NIQE~\textbf{Mittal}, "Natural Image Quality Evaluator: A New Approach for Assessing Distortions"__\textbf{Ye}, " Blind/Referenceless Image Spatial Quality Evaluator using a Neural Network-Based Deep Learning Algorithm", utilize spatial Natural Scene Statistics (NSS) from a statistical perspective. 
Subsequently, similar works have been developed~\textbf{Moorthy}, " Blind/Referenceless Image Spatial Quality Evaluator"__\textbf{Bhattacharya}, " No-Reference Image Quality Assessment Using Multiscale and Visual Attention".
A significant advancement over these handcrafted feature-based methods is the adoption of deep learning, which has demonstrated superior performance in BIQA tasks~\textbf{Ma}, " End-to-End Blind Image Quality Assessment with Convolutional Neural Networks".
For example, DBCNN~\textbf{Ma}, " Deep Blind Image Quality Index" employs two Convolutional Neural Networks (CNNs) to address both synthetic and authentic distortions. 
HyperIQA~\textbf{Gao}, " Hyper-IQ: A Self-Adaptive Hyper Network for No-Reference Image Quality Assessment" introduces a self-adaptive hyper network that addresses real-world variations through a three-stage process: content understanding, perception rule learning, and quality prediction.
StairIQA~\textbf{Zhang}, " Stair-IQ: A Hierarchical Learning Framework for No-Reference Blind Image Quality Assessment" integrates information hierarchically from different stages of a ResNet~\textbf{He}, " Deep Residual Learning for Image Recognition" backbone with a staircase structure.
As vision-language pre-trained models like CLIP~\textbf{Radford}, " Learning Transferable Visual Models From Natural Language Supervision: A Simple Autoencoder-Based Method For Zero-Shot Transfer"__\textbf{Carvalho}, " How Much Do Pretrained Representations Really Know? A Study on Image-to-Text Transfer Tasks", BLIP~\textbf{Tan}, " BLIP: Mobile and Real-time Blind Image Quality Assessment using Vision-Language Pre-trained Models"__\textbf{Li}, " Vision-Language Transformers for Real-Time Blind Image Quality Assessment"__\textbf{He}, " Deep Residual Learning for Image Recognition", and ViT~\textbf{Dosovitskiy}, " An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale" gain popularity, new methods have emerged that leverage these models to evaluate image quality by assessing text-image similarity.
CLIP-IQA~\textbf{Zhang}, " CLIP-Based Blind Image Quality Assessment Using Vision-Language Pre-trained Models" calculates the similarity between an image and two quality-implied prompts to align with human-labeled scores.
Similarly, LIQE~\textbf{Sabater}, " Multitask Learning for No-Reference Blind Image Quality Assessment using Vision-Language Pre-trained Models" introduces a multitask learning approach with an expanded set of textual templates and utilizes CLIP to evaluate vision-text similarities. TempQT~\textbf{Tang}, " Real-Time Blind Image Quality Assessment Using Transformer-Based Models" leverages Transformer~\textbf{Vaswani}, " Attention Is All You Need" encoder and decoder to pre-train the error map and then a feature fusion is done byy a vision Transformer to predict the quality score.

\subsection{AI-Generated Image Quality Assessment}
With advancements in text-to-image synthesis, assessing the quality of AGIs has become increasingly important to align with the human visual system (HVS) and the intent implied in the generation prompt.
HPS~\textbf{Lin}, " HPS: Human Preference Score for AI-generated Images" and PickScore~\textbf{Tang}, " PickScore: A Novel Metric for Evaluating AI-generated Image Quality" train a CLIP-based function to predict user preferences by selecting the most preferred one from a group or a pair of AGIs.
ImageReward~\textbf{Zhang}, " ImageReward: A Text-to-Image Reward Model Based on BLIP" developed the first text-to-image reward model based on BLIP~\textbf{Tan}, " BLIP: Mobile and Real-time Blind Image Quality Assessment using Vision-Language Pre-trained Models", using a dataset of 137k prompt-image pair rankings sampled from DiffusionDB~\textbf{Kabra}, " DiffusionDB: A Large-Scale Dataset for Text-to-Image Synthesis". 
Some research pays attention specifically on artistry aspect such as ArtScore~\textbf{Guo}, " ArtScore: An Aesthetic Quality Assessment Metric for AI-generated Images", which could be imcomplete for AGIQA task.
Then, several datasets using Mean Opinion Score (MOS) as the evaluation target which presents with human perference well, are introduced to support AGIQA task: AGIQA-1k~\textbf{Zhang}, " AGIQA-1k: A Benchmark for AI-generated Image Quality Assessment"__\textbf{Tang}, " Real-Time Blind Image Quality Assessment Using Transformer-Based Models", AGIQA-3k~\textbf{Lin}, " AGIQA-3k: An Expanded Dataset for AI-generated Image Quality Assessment"__\textbf{Guo}, " ArtScore: An Aesthetic Quality Assessment Metric for AI-generated Images", AIGCIQA2023~\textbf{Tan}, " BLIP-Based Benchmark for AI-generated Image Quality Assessment"__\textbf{Li}, " Vision-Language Transformers for Real-Time Blind Image Quality Assessment", AIGCIQA-20k~\textbf{Kabra}, " DiffusionDB: A Large-Scale Dataset for Text-to-Image Synthesis"__\textbf{Tang}, " Real-Time Blind Image Quality Assessment Using Transformer-Based Models", and I2IQA~\textbf{Sabater}, " Multitask Learning for No-Reference Blind Image Quality Assessment using Vision-Language Pre-trained Models". 
Based on these datasets, more methods that perform much better have emerged.
AMFF-Net~\textbf{He}, " Deep Residual Learning for Image Recognition" evaluates AGIs from quality, correspondence and authenticity aspects by scaling images up and down to capture the global and local features, then utilizes CLIP as the text and image encoders.
During the NTIRE2024 Quality Assessment of AI-Generated Content Challenge~\textbf{Kabra}, " DiffusionDB: A Large-Scale Dataset for Text-to-Image Synthesis", further advancements were made.
Inspired by LIQE~\textbf{Sabater}, " Multitask Learning for No-Reference Blind Image Quality Assessment using Vision-Language Pre-trained Models", IPCE~\textbf{Tang}, " Real-Time Blind Image Quality Assessment Using Transformer-Based Models" utilizes CLIP to encode quality-aware prompts that include both the quality label and the image prompt, achieving first place in the challenge.
SF-IQA~\textbf{Zhang}, " Stair-IQ: A Hierarchical Learning Framework for No-Reference Blind Image Quality Assessment"__\textbf{Tan}, " BLIP-Based Benchmark for AI-generated Image Quality Assessment", employing a multilayer feature extractor and fusion module, excels in identifying local and global quality-aware features.
Moreover, MoE-AGIQA~\textbf{Lin}, " AGIQA-3k: An Expanded Dataset for AI-generated Image Quality Assessment"__\textbf{Guo}, " ArtScore: An Aesthetic Quality Assessment Metric for AI-generated Images" sets new benchmarks by integrating visual degradation-aware and semantic-aware networks with a mixture-of-experts module.

While these methods effectively leverage pre-trained vision-language models, they do not fully exploit the potential of textual analysis, which would result in inferior performance compared with larger scale models.

\subsection{Multimodal Large Language Models IQA}
Despite great advancements in models like CLIP~\textbf{Radford}, " Learning Transferable Visual Models From Natural Language Supervision: A Simple Autoencoder-Based Method For Zero-Shot Transfer", which can align images with text, recent approaches have started to explore the potential of Multimodal Large Language Models (MLLMs) for the IQA task.
MLLMs inherit strong reasoning and instruction-following capabilities of Large Language Models (LLMs) and can serve as powerful image evaluators when given appropriate prompts.
TIFA~\textbf{Wang}, " TIFA: A Metric for Evaluating the Faithfulness of Text-to-Image Generation using LLMs" introduces a metric to evaluate the faithfulness of text-to-image generation by using LLMs to generate relevant questions for existing Visual Question Answering (VQA) methods.
LLMScore~\textbf{Li}, " LLMScore: A Multimodal Large Language Model-Based Metric for Evaluating Image Quality" utilizes multi-granularity compositionality capture to evaluate the correspondence between the image and the text prompt by leveraging LLMs as image descriptor and evaluator.
Additionally, Q-Bench~\textbf{Tan}, " BLIP-Based Benchmark for AI-generated Image Quality Assessment" proposes a systematic benchmark to measure the low-level visual perception and understanding abilities of MLLMs, employing a simple softmax pooling strategy to quantitatively assess the text-image correspondence.
Q-Instruct~\textbf{Kabra}, " DiffusionDB: A Large-Scale Dataset for Text-to-Image Synthesis" builds upon the same Softmax pooling strategy by constructing 200k instruction-response pairs related to low-level visual attributes.
Q-Align~\textbf{Tang}, " Real-Time Blind Image Quality Assessment Using Transformer-Based Models" teaches MLLM to judge based on discrete text levels by converting human rating scores into text, and demonstrates an advantage above score-level variants.
While these methods address general IQA tasks, specialized approaches for AGIQA are also emerging.
MA-AGIQA~\textbf{He}, " Deep Residual Learning for Image Recognition" integrates semantically informed guidance with quality-aware features through a Mixture of Experts (MoEs) structure, leveraging both MLLM and traditional DNN approaches for superior performance.
MINT-IQA~\textbf{Tan}, " BLIP-Based Benchmark for AI-generated Image Quality Assessment" extends AIGCIQA2023~\textbf{Kabra}, " DiffusionDB: A Large-Scale Dataset for Text-to-Image Synthesis"__\textbf{Tang}, " Real-Time Blind Image Quality Assessment Using Transformer-Based Models" database to AIGCIQA2023+ by adding image quality descriptions and applying instruction tuning to MLLMs, resulting in considerable improvements in assessing human visual preferences from multiple perspectives.

Although these methods utilize MLLM abilities like prompt understanding, instruction following and text generation, they often overlook the potential encoding capability of MLLMs.

\subsection{Multimodal Large Language Models IQA}
Despite great advancements in models like CLIP~\textbf{Radford}, " Learning Transferable Visual Models From Natural Language Supervision: A Simple Autoencoder-Based Method For Zero-Shot Transfer", which can align images with text, recent approaches have started to explore the potential of Multimodal Large Language Models (MLLMs) for the IQA task.
MLLMs inherit strong reasoning and instruction-following capabilities of Large Language Models (LLMs) and can serve as powerful image evaluators when given appropriate prompts.
TIFA~\textbf{Wang}, " TIFA: A Metric for Evaluating the Faithfulness of Text-to-Image Generation using LLMs" introduces a metric to evaluate the faithfulness of text-to-image generation by using LLMs to generate relevant questions for existing Visual Question Answering (VQA) methods.
LLMScore~\textbf{Li}, " LLMScore: A Multimodal Large Language Model-Based Metric for Evaluating Image Quality" utilizes multi-granularity compositionality capture to evaluate the correspondence between the image and the text prompt by leveraging LLMs as image descriptor and evaluator.
Additionally, Q-Bench~\textbf{Tan}, " BLIP-Based Benchmark for AI-generated Image Quality Assessment" proposes a systematic benchmark to measure the low-level visual perception and understanding abilities of MLLMs, employing a simple softmax pooling strategy to quantitatively assess the text-image correspondence.
Q-Instruct~\textbf{Kabra}, " DiffusionDB: A Large-Scale Dataset for Text-to-Image Synthesis" builds upon the same Softmax pooling strategy by constructing 200k instruction-response pairs related to low-level visual attributes.
Q-Align~\textbf{Tang}, " Real-Time Blind Image Quality Assessment Using Transformer-Based Models" teaches MLLM to judge based on discrete text levels by converting human rating scores into text, and demonstrates an advantage above score-level variants.
While these methods address general IQA tasks, specialized approaches for AGIQA are also emerging.
MA-AGIQA~\textbf{He}, " Deep Residual Learning for Image Recognition" integrates semantically informed guidance with quality-aware features through a Mixture of Experts (MoEs) structure, leveraging both MLLM and traditional DNN approaches for superior performance.
MINT-IQA~\textbf{Tan}, " BLIP-Based Benchmark for AI-generated Image Quality Assessment" extends AIGCIQA2023~\textbf{Kabra}, " DiffusionDB: A Large-Scale Dataset for Text-to-Image Synthesis"__\textbf{Tang}, " Real-Time Blind Image Quality Assessment Using Transformer-Based Models" database to AIGCIQA2023+ by adding image quality descriptions and applying instruction tuning to MLLMs, resulting in considerable improvements in assessing human visual preferences from multiple perspectives.

Although these methods utilize MLLM abilities like prompt understanding, instruction following and text generation, they often overlook the potential encoding capability of MLLMs.