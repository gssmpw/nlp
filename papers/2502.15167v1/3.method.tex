
\section{METHODOLOGY} \label{sec:method}
Our work introduces a novel method to address the AGIQA task, which seeks to establish a function $f$ that accepts an AGI $i$ and its corresponding text prompt $p$ as inputs, and outputs a predicted MOS:
%
\begin{equation}
\hat{y} = f\left(i, p\right).
\end{equation}
%
Specifically, for datasets that provide MOSs across various aspects, we train distinct functions tailored to each aspect. 
These functions $f_q$, $f_c$, and $f_a$, are dedicated to assessing quality, correspondence, and alignment, respectively. 
This specialization allows for a more nuanced evaluation of AGIs, aligning closely with human perceptual judgments.
As illustrated in Fig.~\ref{fig:overall}, M3-AGIQA comprises three stages during training. 

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures/m3-agiqa-overall.pdf}
    \caption{Overview of the M3-AGIQA Framework. This diagram illustrates the three-stage process using the quality aspect of AGIs as an example. \textbf{(a) Initial Description Stage:} By using an online MLLM service via API, we obtain descriptions related to quality, correspondence, and authenticity for each image. 
    These descriptions are then used to fine-tune an open-source MLLM, enhancing its understanding of these aspects.
    \textbf{(b) Inference Stage:} Distilled captioning capabilities of the fine-tuned MLLM was leveraged coupled with the textual prompt to generate the required response. This step utilizes the refined ability of the MLLM to interpret and articulate the nuances of the input prompt and image.
    \textbf{(c) Training Stage:} We freeze all the parameters within the fine-tuned MLLM to ensure stability and reproducibility. A xLSTM feature extractor along with a regression head is then trained to predict the MOS, effectively translating enhanced perceptual and textual understanding of the MLLM into quantifiable assessments.}
    \label{fig:overall}
\end{figure*}

\subsection{Initial Description Stage} \label{subsec:distil}
The initial description stage generates high-quality descriptions for quality, correspondence, and authenticity aspects of AI-Generated Images (AGIs), forming the foundation for downstream training.
Distillation from online Multimodal Large Language Models (MLLMs), which excel in image understanding~\cite{jin2024efficient}, transfers their capabilities to an open-source MLLM, reducing cost and latency while maintaining performance.

In the first stage of our method, we use the online MLLM to generate descriptions of AGIs from three aspects: quality, correspondence, and authenticity.
To achieve this, we manually create a prompt that is then refined by \textit{GPT-4}, and the prompt embeds both the MOS and the specific text prompt of the AGI.
The dual embedding guides the MLLM in producing descriptions that not only resonate with human perceptions but also exploit the object detection capabilities of the model to enhance text-image alignment judgments.\footnote{Details of the prompts, templates and specific cases examined in this study will be elaborated upon in the supplementary materials}
Then, by an automatic script, we call the API of the online MLLM to generate descriptions of the AGIs from datasets:
\begin{equation}
    \boldsymbol{D^i} = \textrm{MLLM}_\textrm{online}.\textrm{generate}\left(i, \textrm{T}_\textrm{desc}\left(p, y\right)\right),
\end{equation}
\begin{equation}
    \boldsymbol{D^i} = \left\{D^i_q, D^i_c, D^i_a\right\},
\end{equation}
where $\boldsymbol{D^i}$ is the description which consists of descriptions on three aspects $D^i_q, D^i_c, D^i_a$ for AGI $i$, $\textrm{MLLM}_\textrm{online}$ is the online MLLM we use, and $\textrm{T}_\textrm{desc}$ is the prompt template which accepts the text prompt $p$ and MOS $y$ of the AGI as inputs.

After generating descriptions for the AGIs, we initiate a two-round conversational interaction between a \textit{user} and an \textit{assistant} with a new prompt template, as depicted in Fig.~\ref{fig:overall}~(a).
This template incorporates Likert-scale of five-level descriptors [bad, poor, fair, good, excellent] as part of the prompt construction, following methodologies similar to those of previous works~\cite{ghadiyaram2015massive, zhang2023blind, wu2023qalign, zhu2024adaptive}.

Then, the conversations and their corresponding AGIs are divided into training and validation sets.
These sets are then fed into a local open-source MLLM for training with LoRA fine-tuning technique on the vision encoder only inside the MLLM as illustrated in Fig.~\ref{fig:localmllm}.
Initially, the AGI is resized and segmented into patches before being sent to the Vision Encoder, after which the encoded data for each patch are projected into visual tokens.
These visual tokens are combined with text tokens and input into the LLM.
Inside the LLM, all the tokens are processed, and the linear layer at the end outputs the logits of the sequence, which can be subsequently decoded into text.
Thus, we can obtain the fine-tuned $\textrm{MLLM}_\textrm{ft}$ which will accept AGI $i$ and text prompt $p$ as inputs.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/m3-agiqa-localmllm.pdf}
    \caption{LoRA fine-tuning process of the local MLLM. The MLLM processes both the image and conversation text as inputs. The embedded text and image are forwarded through the internal LLM (Llama3). The LoRA parameters on the Visual Encoder are optimized by minimizing the cross-entropy loss between the output logits and the input conversation tokens. In both the inference and training stages, the parameters of the Visual Encoder are frozen.}
    \label{fig:localmllm}
\end{figure}

\subsection{Inference Stage} \label{subsec:inference}
In the inference stage, we harness the zero-shot Chain of Thought (CoT) capabilities of LLMs~\cite{kojima2022large} to generate intermediate reasoning processes that refine the final conclusions.
As illustrated in Fig.~\ref{fig:overall}~(b), this stage enhances the accuracy of assessments by leveraging the generated descriptions to judge the quality of AGIs, while the two-round conversation process augments the data for subsequent training.
The fine-tuned local open-source MLLM in Sec.~\ref{subsec:distil} produces meaningful logits during this process, representing the distribution of the AGI and text across vocabulary probabilities, which are crucial for accurately predicting the MOS.
Skipping this step results in inadequate understanding in image assessment and disrupts the connection between distilled knowledge and the final score prediction.
The process of generating description $D^i_{ft}$ can be simplified as follows:
\begin{equation}
    D^i_{ft} = \textrm{MLLM}_\textrm{ft}.\textrm{generate}\left(i, p\right),
\end{equation}
where $\textrm{MLLM}_\textrm{ft}$ is the fine-tuned model.

\subsection{Training Stage}
In order to align the output of the MLLM with the MOS which is a human perspective assessment of AGIs, an additional step involving a well-designed predictor network was introduced, which led to our method functioning as an encoder for both image and text modalities, inspired by ~\cite{behnamghader2024llm2vec}.
As such, we utilize the generated conversation $C^i$, which includes the generated description $D^i_{ft}$ from the previous inference stage~\ref{subsec:inference} along with the corresponding AGI as inputs to encode them as follows:
\begin{equation}
    \boldsymbol{E^i} = \textrm{MLLM}_\textrm{ft}.\textrm{forward}\left(i, C^i\right),
\end{equation}
where $\boldsymbol{E^i} \in \mathbb{R}^{L \times d_\textrm{vocab}} $ represents the output logits, with $L$ denoting the sequence length of the input conversation $C^i$ and $d_\textrm{vocab}$ representing the vocabulary size of MLLM tokenizer.
The last element $\boldsymbol{E^i[-1]}$ of the output logits, typically represents the probability of the next token, which is possibly valuable for predicting the target.
However, as~\cite{wang2024my} mentioned, the next first-token probability might lead to a mismatch with the expected answer. This observation is also supported by our experiments~\ref{sec:mllm_encoder}. %TODO: 我们的实验（需要补充这个）
Therefore, we take advantage of the entire sequence $\boldsymbol{E^i}$.
Given its sequential nature, we employ an xLSTM feature extractor followed by a mean pooling strategy on the sequence dimension for better alignment:
\begin{equation}
    \boldsymbol{E^i_\textrm{proj}} = \mathbf{W}_\textrm{proj} \boldsymbol{E^i},
\end{equation}
\begin{equation}
    \boldsymbol{E^i_\textrm{out}} = \textrm{xLSTMHead}\left(\boldsymbol{E^i_\textrm{proj}}\right),
\end{equation}
\begin{equation}
    \boldsymbol{z^i} = \textrm{MeanPooling}\left(\boldsymbol{E^i_\textrm{out}}\right),
\end{equation}
where $\mathbf{W}_\textrm{proj} \in \mathbb{R}^{d_{\textrm{vocab}} \times d}$ represents the learnable linear layer that projects from the vocabulary space to the xLSTM hidden space of dimension $d$. 
This projection facilitates the integration of linguistic information into the sequential processing framework of xLSTM, enabling effective feature extraction and representation learning for subsequent tasks.

Then, a regression head composed of linear layers is used to obtain the floating-point prediction score for each instance:
\begin{equation}
    \hat{y}^i = \textrm{RegressionHead}\left(\boldsymbol{z^i}\right).
\end{equation}

To optimize the model, Mean Square Error (MSE) loss is employed for backpropagation through the trainable parameters in the xLSTM blocks and regression head:
\begin{equation}
    L = \frac{1}{n} \sum_{i=1}^{n}{\left(\hat{y}^i - y^i\right)^2},
\end{equation}
where $y^i$ is the ground truth MOS.

