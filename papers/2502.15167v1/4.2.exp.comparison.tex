
\subsection{Comparison with SOTAs - RQ1}
\begin{table*}
    \centering
    \caption{Comparison results on AGIQA-3k~\cite{li2023agiqa}, AIGCIQA2023~\cite{wang2023aigciqa2023}, and AIGCIQA-20k~\cite{li2024aigiqa} among different methods, results of methods with asterisk symbol ``$^\ast$'' are directly retrieved from corresponding papers. \textbf{Bold} and \underline{underlined} values indicate the best and second-best results, respectively.}
    \label{tab:comparison}
    \begin{tabular}{c|c c|c c| c c|c c|c c| c c}
        \toprule
        % heads
        \multirow{3}{*}{Methods} & \multicolumn{4}{c|}{AGIQA-3k} & \multicolumn{6}{c|}{AIGCIQA2023} & \multicolumn{2}{c}{AIGCIQA-20k} \\
        \cline{2-13}
        & \multicolumn{2}{c|}{Qual.} & \multicolumn{2}{c|}{Corr.} & \multicolumn{2}{c|}{Qual.} & \multicolumn{2}{c|}{Corr.} & \multicolumn{2}{c|}{Auth.} & \multicolumn{2}{c}{Qual.} \\
        \cline{2-13}
        & SRCC & PLCC & SRCC & PLCC & SRCC & PLCC & SRCC & PLCC & SRCC & PLCC & SRCC & PLCC \\
        \midrule
        VGG16~\cite{simonyan2014very} & 0.8167 & 0.8752 & 0.6867 & 0.8108 & 0.8157 & 0.8282 & 0.6839 & 0.6853 & 0.7399 & 0.7465 & 0.8133 & 0.8660 \\
        ResNet50~\cite{he2016deep} & 0.8552 & 0.9072 & 0.7493 & 0.8564 & 0.8190 & 0.8503 & 0.7230 & 0.7270 & 0.7571 & 0.7563 & 0.8036 & 0.8661 \\
        ViT/B/16~\cite{dosovitskiy2020image} & 0.8659 & 0.9115 & 0.7478 & 0.8449 & 0.8370 & 0.8618 & 0.7293 & 0.7439 & 0.7783 & 0.7697 & 0.8407 & 0.8904 \\
        ViL~\cite{alkin2024visionlstm} & 0.8750 & 0.9145 & 0.7570 & 0.8561 & 0.8436 & 0.8770 & 0.7174 & 0.7296 & 0.7753 & 0.7770 & 0.8459 & 0.8852 \\ 
        \midrule
        DBCNN$^\ast$~\cite{zhang2018blind} & 0.8154 & 0.8747 & 0.6329 & 0.7823 & 0.8339 & 0.8577 & 0.6837 & 0.6787 & 0.7485 & 0.7436 & 0.7941 & 0.8542 \\
        StairIQA~\cite{sun2022blind} & 0.8439 & 0.8989 & 0.7345 & 0.8474 & 0.8264 & 0.8483 & 0.7176 & 0.7133 & 0.7596 & 0.7514 & 0.8126 & 0.8746 \\
        MGQA~\cite{wang2021multi} & 0.8283 & 0.8944  & 0.7244 & 0.8430 & 0.8093 & 0.8308 & 0.6892 & 0.6745 & 0.7367 & 0.7310 & 0.8107 & 0.8534  \\
        HyperIQA~\cite{Su_2020_CVPR} & 0.8526 & 0.8975 & 0.7437 & 0.8471 & 0.8357 & 0.8504 & 0.7318 & 0.7222 & 0.7758 & 0.7790 & 0.8171 & 0.8584  \\
        \midrule
        AMFF-Net$^\ast$~\cite{zhou2024adaptive} & 0.8565 & 0.9050 & 0.7513 & 0.8476 & 0.8409 & 0.8537 & 0.7782 & 0.7638 & 0.7749 & 0.7643 & - & - \\
        MA-AGIQA$^\ast$~\cite{wang2024large} & 0.8939 & 0.9273 & - & - & - & - & - & - & - & -& 0.8644 & 0.9050 \\
        IPCE$^\ast$~\cite{peng2024aigc} & 0.8841 & 0.9246 & 0.7697 & 0.8725 & \textbf{0.8640} & \underline{0.8788} & \underline{0.7979} & \underline{0.7887} & \underline{0.8097} & \underline{0.7998} & \textbf{0.9076} & \underline{0.9274} \\
        SF-IQA$^\ast$~\cite{yu2024sf} & \underline{0.9024} & \underline{0.9314} & \underline{0.8454} & \underline{0.9072} & - & - & - & - & - & - & \underline{0.9009} & 0.9268  \\
        \midrule
        M3-AGIQA (Ours) & \textbf{0.9045} & \textbf{0.9317} & \textbf{0.8523} & \textbf{0.9142} & \underline{0.8618} & \textbf{0.8922} & 
        \textbf{0.8060} & \textbf{0.7973} & \textbf{0.8282} & \textbf{0.8165} & 0.8988 & \textbf{0.9292} \\
        \bottomrule
    \end{tabular}
\end{table*}


\begin{figure*}
    \centering
    \includegraphics[width=0.24\linewidth]{figures/regplots/vgg16.pdf}
    \includegraphics[width=0.24\linewidth]{figures/regplots/resnet50.pdf}
    \includegraphics[width=0.24\linewidth]{figures/regplots/vit.pdf}
    \includegraphics[width=0.24\linewidth]{figures/regplots/visionlstm.pdf}
    \includegraphics[width=0.24\linewidth]{figures/regplots/stairiqa.pdf}
    \includegraphics[width=0.24\linewidth]{figures/regplots/mgqa.pdf}
    \includegraphics[width=0.24\linewidth]{figures/regplots/hyperiqa.pdf}
    \includegraphics[width=0.24\linewidth]{figures/regplots/m3-agiqa.pdf}
    
    \caption{Scatter plot of predicted MOS vs. ground truth MOS on the quality aspect of AGIQA-3k~\cite{li2023agiqa} dataset. The x-axis represents the ground truth MOS, while the y-axis shows the predicted MOS. The fitted curves are obtained using a fourth-order polynomial regression.}
    \label{fig:scatter}
\end{figure*}

Experiments on the datasets have shown our proposed method M3-AGIQA significantly outperforms the counterparts. As depicted in Table~\ref{tab:comparison},
simple vision encoders perform well on AGIQA task, ViL~\cite{alkin2024visionlstm} which utilizes the latest xLSTM~\cite{beck2024xlstm} architecture shows strong advancement beyond the traditional ResNet50~\cite{he2016deep}. 
With respect to the traditional BIQA methods, VGG~\cite{simonyan2014very} based DBCNN~\cite{zhang2018blind} is simple and fast but not competitive due to a lack of quality related feature extraction ability;
ResNet50~\cite{he2016deep} based StairIQA~\cite{sun2022blind}, MGQA~\cite{wang2021multi}, and HyperIQA~\cite{Su_2020_CVPR} do not improve much or are even inferior to their backbone, this could occur when they are designed specifically from the perspective of NSIs quality but not AGIs, which would ignore the impact from correspondence and authenticity aspects.
AGIQA methods perform significantly better than the vision encoder and BIQA methods do, especially in terms of correspondence and authenticity apsects.
As an example on AIGCIQA2023~\cite{wang2023aigciqa2023} dataset, IPCE~\cite{peng2024aigc} which combines quality aware text with the AGI as input of CLIP~\cite{radford2021learning} model, achieves overall superior results to those of non-AGIQA methods, not only because of the extraordinary ability of CLIP model to align with text and image, but also because of the meaningful text adoption on the three aspects.
Notably, IPCE~\cite{peng2024aigc} performs slightly worse than its counterpart SF-IQA~\cite{yu2024sf} on AGIQA-3k~\cite{li2023agiqa} dataset.
But in NTIRE2024 challenge, it beats SF-IQA~\cite{yu2024sf} on AIGCIQA-20k dataset.
The reason could be IPCE~\cite{peng2024aigc} utilizes techniques like model integration which is common in competitions.
For SF-IQA~\cite{yu2024sf}, SwinTransformer is employed as a feature extractor, CLIP is used for text-image alignment, and Transformer encoder is used as a fusion module, better results are achieved on AGIQA-3k~\cite{li2023agiqa} dataset.

In contrast to previously discussed methods, our approach demonstrates superior performance across most metrics.
The scatter plot in Fig~\ref{fig:scatter} illustrates that M3-AGIQA aligns closely with the data, indicating a strong fit.
However, M3-AGIQA underperforms compared with NTIRE2024 champion IPCE~\cite{peng2024aigc} on AIGCIQA-20k~\cite{li2024aigiqa} dataset, this discrepancy may be attributed to the optimization specially for the competitions settings of IPCE~\cite{peng2024aigc}.
Our result can be in the fourth position in the competition and has the best PLCC score, which indicates M3-AGIQA performs well in fitting the data but may lack some ranking capability.
