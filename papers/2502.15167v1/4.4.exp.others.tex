
\begin{table*}
    \centering
    \caption{Cross-dataset validation on quality aspect, results of methods with asterisk symbol ``$^\ast$'' are directly retrieved from corresponding papers.}
    \label{tab:crossdataset1}
    \begin{tabular}{c|cc|cc|cc|cc}
        \toprule
        \multirow{2}{*}{Models} & \multicolumn{2}{c|}{AGIQA-3k$\rightarrow$AIGCIQA2023} & \multicolumn{2}{c|}{AGIQA-3k$\rightarrow$AIGCIQA-20k} & \multicolumn{2}{c|}{AIGCIQA2023$\rightarrow$AGIQA-3k} & \multicolumn{2}{c}{AIGCIQA-20k$\rightarrow$AGIQA-3k}  \\
        \cline{2-9}
    & SRCC & PLCC & SRCC & PLCC & SRCC & PLCC & SRCC & PLCC \\
        \midrule
        VGG16 & 0.6373 & 0.6429 & 0.6874 & 0.7045 & 0.6017 & 0.6396 & 0.7149 & 0.7221 \\ 
        ResNet50 & 0.6749 & 0.6786 & 0.6695 & 0.7235 & 0.6752 & 0.7475 & 0.6130 & 0.4861 \\
        ViT/B/16 & 0.5662 & 0.5744 & 0.6659 & 0.7133 & 0.5057 & 0.5579 & 0.7497 & 0.7493 \\
        ViL & 0.6088 & 0.6033 & 0.6894 & 0.7459 & 0.6574 & 0.6731 & 0.7829 & 0.8090 \\
        \midrule
        DBCNN$^\ast$ & 0.654 & 0.664 & - & - & 0.627 & 0.688 & - & - \\
        StairIQA & 0.6681 & 0.6566 & 0.7001 & 0.7579 & 0.5958 & 0.6264 & 0.7606 & 0.7825 \\
        MGQA & 0.6609 & 0.6696 & 0.6730 & 0.7348 & 0.6090 & 0.6624 & 0.7315 & 0.7363 \\
        HyperIQA & 0.6451 & 0.6521 & 0.6780 & 0.7115 & 0.4834 & 0.5241 & 0.6252 & 0.6014 \\
        \midrule
        AMFF-Net$^\ast$ & 0.678 & 0.669 & - & - & 0.654 & 0.695 & - & - \\
        MA-AGIQA$^\ast$ & - & - & \textbf{0.7722} & \textbf{0.8414} & - & - & \textbf{0.8503} & 0.8430 \\
        \midrule
        M3-AGIQA (Ours) & \textbf{0.7489} & \textbf{0.7461} & 0.6862 & 0.7340 & \textbf{0.7427} & \textbf{0.7542} & 0.8452 & \textbf{0.8772} \\
        M3-AGIQA (w/ TP) & \textit{0.8557} & \textit{0.8845} & \textit{0.8973} & \textit{0.9308} & \textit{0.9027} & \textit{0.9314} & \textit{0.8961} & \textit{0.9310} \\
        \midrule
        \bottomrule
    \end{tabular}
\end{table*}

\subsection{Cross-dataset Validation - RQ5}

In order to demonstrate the generalization ability of our method, we further conduct cross-dataset validation experiments. 
Following the previous settings, we focused solely on the quality aspect.
Specifically, we leveraged the fine-tuned local MLLM from one dataset A to generate intermediate image quality descriptions for another dataset B.
The trained model checkpoint from A was applied to predict outcomes on B using these descriptions.
As shown in Table~\ref{tab:crossdataset1}, the results indicate that our method has a strong generalization ability compared with its counterparts.
However, the performance transition from AGIQA-3k to AIGCIQA-20k was only average, suggesting that our method may overfit on smaller datasets and struggle to generalize to much larger datasets.
This limitation could be attributed to the predictor comprising the xLSTM feature extractor and regression head, which was trained on one dataset but exhibited a diminished generalization ability when applied to a substantially different dataset.
Also, we tried with additionally training the predictor (w/ TP), the performance was close to that of fully trained on that dataset, which proves the effectiveness of the predictor. 
