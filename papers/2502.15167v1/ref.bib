@inproceedings{rombach2022high,
  title={High-resolution image synthesis with latent diffusion models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10684--10695},
  year={2022}
}

@article{gulrajani2017improved,
  title={Improved training of wasserstein gans},
  author={Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron C},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{heusel2017gans,
  title={Gans trained by a two time-scale update rule converge to a local nash equilibrium},
  author={Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

% agiqa-1k
@inproceedings{zhang2023perceptual,
  title={A perceptual quality assessment exploration for aigc images},
  author={Zhang, Zicheng and Li, Chunyi and Sun, Wei and Liu, Xiaohong and Min, Xiongkuo and Zhai, Guangtao},
  booktitle={2023 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)},
  pages={440--445},
  year={2023},
  organization={IEEE}
}

% AMFF-net
@article{zhou2024adaptive,
  title={Adaptive mixed-scale feature fusion network for blind AI-generated image quality assessment},
  author={Zhou, Tianwei and Tan, Songbai and Zhou, Wei and Luo, Yu and Wang, Yuan-Gen and Yue, Guanghui},
  journal={IEEE Transactions on Broadcasting},
  year={2024},
  publisher={IEEE}
}

% agiqa-3k
@article{li2023agiqa,
  title={Agiqa-3k: An open database for ai-generated image quality assessment},
  author={Li, Chunyi and Zhang, Zicheng and Wu, Haoning and Sun, Wei and Min, Xiongkuo and Liu, Xiaohong and Zhai, Guangtao and Lin, Weisi},
  journal={IEEE Transactions on Circuits and Systems for Video Technology},
  year={2023},
  publisher={IEEE}
}

% aigiqa-20k
@article{li2024aigiqa,
  title={Aigiqa-20k: A large database for ai-generated image quality assessment},
  author={Li, Chunyi and Kou, Tengchuan and Gao, Yixuan and Cao, Yuqin and Sun, Wei and Zhang, Zicheng and Zhou, Yingjie and Zhang, Zhichao and Zhang, Weixia and Wu, Haoning and others},
  journal={arXiv preprint arXiv:2404.03407},
  volume={2},
  number={3},
  pages={5},
  year={2024}
}

% IPCE
@inproceedings{peng2024aigc,
  title={Aigc image quality assessment via image-prompt correspondence},
  author={Peng, Fei and Fu, Huiyuan and Ming, Anlong and Wang, Chuanming and Ma, Huadong and He, Shuai and Dou, Zifei and Chen, Shu},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops},
  volume={6},
  year={2024}
}

% SF-IQA
@inproceedings{yu2024sf,
  title={Sf-iqa: Quality and similarity integration for ai generated image quality assessment},
  author={Yu, Zihao and Guan, Fengbin and Lu, Yiting and Li, Xin and Chen, Zhibo},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={6692--6701},
  year={2024}
}

% MA-AGIQA
@inproceedings{wang2024large,
  title={Large multi-modality model assisted ai-generated image quality assessment},
  author={Wang, Puyi and Sun, Wei and Zhang, Zicheng and Jia, Jun and Jiang, Yanwei and Zhang, Zhichao and Min, Xiongkuo and Zhai, Guangtao},
  booktitle={Proceedings of the 32nd ACM International Conference on Multimedia},
  pages={7803--7812},
  year={2024}
}

% MINT-IQA
@article{wang2024understanding,
  title={Understanding and Evaluating Human Preferences for AI Generated Images with Instruction Tuning},
  author={Wang, Jiarui and Duan, Huiyu and Zhai, Guangtao and Min, Xiongkuo},
  journal={arXiv preprint arXiv:2405.07346},
  year={2024}
}

@article{mittal2012making,
  title={Making a “completely blind” image quality analyzer},
  author={Mittal, Anish and Soundararajan, Rajiv and Bovik, Alan C},
  journal={IEEE Signal processing letters},
  volume={20},
  number={3},
  pages={209--212},
  year={2012},
  publisher={IEEE}
}

@article{xue2014blind,
  title={Blind image quality assessment using joint statistics of gradient magnitude and Laplacian features},
  author={Xue, Wufeng and Mou, Xuanqin and Zhang, Lei and Bovik, Alan C and Feng, Xiangchu},
  journal={IEEE Transactions on Image Processing},
  volume={23},
  number={11},
  pages={4850--4862},
  year={2014},
  publisher={IEEE}
}

@article{zhang2014blind,
  title={Blind image quality assessment using the joint statistics of generalized local binary pattern},
  author={Zhang, Min and Muramatsu, Chisako and Zhou, Xiangrong and Hara, Takeshi and Fujita, Hiroshi},
  journal={IEEE Signal Processing Letters},
  volume={22},
  number={2},
  pages={207--210},
  year={2014},
  publisher={IEEE}
}
@article{zhang2015feature,
  title={A feature-enriched completely blind image quality evaluator},
  author={Zhang, Lin and Zhang, Lei and Bovik, Alan C},
  journal={IEEE Transactions on Image Processing},
  volume={24},
  number={8},
  pages={2579--2591},
  year={2015},
  publisher={IEEE}
}

@article{xu2016blind,
  title={Blind image quality assessment based on high order statistics aggregation},
  author={Xu, Jingtao and Ye, Peng and Li, Qiaohong and Du, Haiqing and Liu, Yong and Doermann, David},
  journal={IEEE Transactions on Image Processing},
  volume={25},
  number={9},
  pages={4444--4457},
  year={2016},
  publisher={IEEE}
}

@article{hou2014blind,
  title={Blind image quality assessment via deep learning},
  author={Hou, Weilong and Gao, Xinbo and Tao, Dacheng and Li, Xuelong},
  journal={IEEE transactions on neural networks and learning systems},
  volume={26},
  number={6},
  pages={1275--1286},
  year={2014},
  publisher={IEEE}
}
@inproceedings{kang2014convolutional,
  title={Convolutional neural networks for no-reference image quality assessment},
  author={Kang, Le and Ye, Peng and Li, Yi and Doermann, David},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1733--1740},
  year={2014}
}

% DBCNN
@article{zhang2018blind,
  title={Blind image quality assessment using a deep bilinear convolutional neural network},
  author={Zhang, Weixia and Ma, Kede and Yan, Jia and Deng, Dexiang and Wang, Zhou},
  journal={IEEE Transactions on Circuits and Systems for Video Technology},
  volume={30},
  number={1},
  pages={36--47},
  year={2018},
  publisher={IEEE}
}

% StairIQA
@inproceedings{sun2022blind,
  title={Blind quality assessment for in-the-wild images via hierarchical feature fusion strategy},
  author={Sun, Wei and Duan, Huiyu and Min, Xiongkuo and Chen, Li and Zhai, Guangtao},
  booktitle={2022 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB)},
  pages={01--06},
  year={2022},
  organization={IEEE}
}

% ResNet
@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

% CLIP
@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

% BLIP
@inproceedings{li2022blip,
  title={Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation},
  author={Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  booktitle={International conference on machine learning},
  pages={12888--12900},
  year={2022},
  organization={PMLR}
}

% ViT
@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

% Clip-IQA
@inproceedings{wang2023exploring,
  title={Exploring clip for assessing the look and feel of images},
  author={Wang, Jianyi and Chan, Kelvin CK and Loy, Chen Change},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={2},
  pages={2555--2563},
  year={2023}
}

% LIQE
@inproceedings{zhang2023blind,
  title={Blind image quality assessment via vision-language correspondence: A multitask learning perspective},
  author={Zhang, Weixia and Zhai, Guangtao and Wei, Ying and Yang, Xiaokang and Ma, Kede},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={14071--14081},
  year={2023}
}

% AIGCIQA2023
@inproceedings{wang2023aigciqa2023,
  title={Aigciqa2023: A large-scale image quality assessment database for ai generated images: from the perspectives of quality, authenticity and correspondence},
  author={Wang, Jiarui and Duan, Huiyu and Liu, Jing and Chen, Shi and Min, Xiongkuo and Zhai, Guangtao},
  booktitle={CAAI International Conference on Artificial Intelligence},
  pages={46--57},
  year={2023},
  organization={Springer}
}

% i2iqa
@article{yuan2023pku,
  title={Pku-i2iqa: An image-to-image quality assessment database for ai generated images},
  author={Yuan, Jiquan and Cao, Xinyan and Li, Changjin and Yang, Fanyi and Lin, Jinlong and Cao, Xixin},
  journal={arXiv preprint arXiv:2311.15556},
  year={2023}
}

% ImageReward
@article{xu2024imagereward,
  title={Imagereward: Learning and evaluating human preferences for text-to-image generation},
  author={Xu, Jiazheng and Liu, Xiao and Wu, Yuchen and Tong, Yuxuan and Li, Qinkai and Ding, Ming and Tang, Jie and Dong, Yuxiao},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

% DiffusionDB
@article{wang2022diffusiondb,
  title={Diffusiondb: A large-scale prompt gallery dataset for text-to-image generative models},
  author={Wang, Zijie J and Montoya, Evan and Munechika, David and Yang, Haoyang and Hoover, Benjamin and Chau, Duen Horng},
  journal={arXiv preprint arXiv:2210.14896},
  year={2022}
}

% Pickscore
@article{kirstain2023pick,
  title={Pick-a-pic: An open dataset of user preferences for text-to-image generation},
  author={Kirstain, Yuval and Polyak, Adam and Singer, Uriel and Matiana, Shahbuland and Penna, Joe and Levy, Omer},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={36652--36663},
  year={2023}
}

% HPS: human preference score
@article{wu2023better,
  title={Better aligning text-to-image models with human preference},
  author={Wu, Xiaoshi and Sun, Keqiang and Zhu, Feng and Zhao, Rui and Li, Hongsheng},
  journal={arXiv preprint arXiv:2303.14420},
  volume={1},
  number={3},
  year={2023}
}

% Multimodal Large Language Model is a 
% Human-Aligned Annotator for Text-to-Image
% Generation
% 用来表明MLLM的能力？
@misc{wu2024multimodallargelanguagemodel,
      title={Multimodal Large Language Model is a Human-Aligned Annotator for Text-to-Image Generation}, 
      author={Xun Wu and Shaohan Huang and Furu Wei},
      year={2024},
      eprint={2404.15100},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2404.15100}, 
}

% ntire2024
@inproceedings{liu2024ntire,
  title={NTIRE 2024 Quality Assessment of AI-Generated Content Challenge},
  author={Liu, Xiaohong and Min, Xiongkuo and Zhai, Guangtao and Li, Chunyi and Kou, Tengchuan and Sun, Wei and Wu, Haoning and Gao, Yixuan and Cao, Yuqin and Zhang, Zicheng and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={6337--6362},
  year={2024}
}

% MOE-AGIQA
@inproceedings{yang2024moe,
  title={Moe-agiqa: Mixture-of-experts boosted visual perception-driven and semantic-aware quality assessment for ai-generated images},
  author={Yang, Junfeng and Fu, Jing and Zhang, Wei and Cao, Wenzhi and Liu, Limei and Peng, Han},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={6395--6404},
  year={2024}
}

% Tifa (LLM)
@inproceedings{hu2023tifa,
  title={Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering},
  author={Hu, Yushi and Liu, Benlin and Kasai, Jungo and Wang, Yizhong and Ostendorf, Mari and Krishna, Ranjay and Smith, Noah A},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={20406--20417},
  year={2023}
}

% LLMScore
@article{lu2024llmscore,
  title={Llmscore: Unveiling the power of large language models in text-to-image synthesis evaluation},
  author={Lu, Yujie and Yang, Xianjun and Li, Xiujun and Wang, Xin Eric and Wang, William Yang},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

% Q-bench
@article{wu2023q,
  title={Q-bench: A benchmark for general-purpose foundation models on low-level vision},
  author={Wu, Haoning and Zhang, Zicheng and Zhang, Erli and Chen, Chaofeng and Liao, Liang and Wang, Annan and Li, Chunyi and Sun, Wenxiu and Yan, Qiong and Zhai, Guangtao and others},
  journal={arXiv preprint arXiv:2309.14181},
  year={2023}
}

% Q-instruct
@inproceedings{wu2024q,
  title={Q-instruct: Improving low-level visual abilities for multi-modality foundation models},
  author={Wu, Haoning and Zhang, Zicheng and Zhang, Erli and Chen, Chaofeng and Liao, Liang and Wang, Annan and Xu, Kaixin and Li, Chunyi and Hou, Jingwen and Zhai, Guangtao and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={25490--25500},
  year={2024}
}

% Q-Align
@article{wu2023qalign,
  title={Q-align: Teaching lmms for visual scoring via discrete text-defined levels},
  author={Wu, Haoning and Zhang, Zicheng and Zhang, Weixia and Chen, Chaofeng and Liao, Liang and Li, Chunyi and Gao, Yixuan and Wang, Annan and Zhang, Erli and Sun, Wenxiu and others},
  journal={arXiv preprint arXiv:2312.17090},
  year={2023}
}

% MLLM survey2024
@article{jin2024efficient,
  title={Efficient multimodal large language models: A survey},
  author={Jin, Yizhang and Li, Jian and Liu, Yexin and Gu, Tianjun and Wu, Kai and Jiang, Zhengkai and He, Muyang and Zhao, Bo and Tan, Xin and Gan, Zhenye and others},
  journal={arXiv preprint arXiv:2405.10739},
  year={2024}
}

% zeroshot CoT
@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}

% myanswer is C
@article{wang2024my,
  title={" My Answer is C": First-Token Probabilities Do Not Match Text Answers in Instruction-Tuned Language Models},
  author={Wang, Xinpeng and Ma, Bolei and Hu, Chengzhi and Weber-Genzel, Leon and R{\"o}ttger, Paul and Kreuter, Frauke and Hovy, Dirk and Plank, Barbara},
  journal={arXiv preprint arXiv:2402.14499},
  year={2024}
}

% llm2vec
@article{behnamghader2024llm2vec,
  title={Llm2vec: Large language models are secretly powerful text encoders},
  author={BehnamGhader, Parishad and Adlakha, Vaibhav and Mosbach, Marius and Bahdanau, Dzmitry and Chapados, Nicolas and Reddy, Siva},
  journal={arXiv preprint arXiv:2404.05961},
  year={2024}
}

% prenorm, postnorm
@inproceedings{nguyen-salazar-2019-transformers,
    title = "Transformers without Tears: Improving the Normalization of Self-Attention",
    author = "Nguyen, Toan Q.  and
      Salazar, Julian",
    editor = {Niehues, Jan  and
      Cattoni, Rolando  and
      St{\"u}ker, Sebastian  and
      Negri, Matteo  and
      Turchi, Marco  and
      Ha, Thanh-Le  and
      Salesky, Elizabeth  and
      Sanabria, Ramon  and
      Barrault, Loic  and
      Specia, Lucia  and
      Federico, Marcello},
    booktitle = "Proceedings of the 16th International Conference on Spoken Language Translation",
    month = nov # " 2-3",
    year = "2019",
    address = "Hong Kong",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2019.iwslt-1.17",
    abstract = "We evaluate three simple, normalization-centric changes to improve Transformer training. First, we show that pre-norm residual connections (PRENORM) and smaller initializations enable warmup-free, validation-based training with large learning rates. Second, we propose l2 normalization with a single scale parameter (SCALENORM) for faster training and better performance. Finally, we reaffirm the effectiveness of normalizing word embeddings to a fixed length (FIXNORM). On five low-resource translation pairs from TED Talks-based corpora, these changes always converge, giving an average +1.1 BLEU over state-of-the-art bilingual baselines and a new 32.8 BLEU on IWSLT '15 English-Vietnamese. We ob- serve sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Surprisingly, in the high-resource setting (WMT '14 English-German), SCALENORM and FIXNORM remain competitive but PRENORM degrades performance.",
}

% ViL
@article{alkin2024visionlstm,
  title={Vision-LSTM: xLSTM as Generic Vision Backbone},
  author={Benedikt Alkin and Maximilian Beck and Korbinian P{\"o}ppel and Sepp Hochreiter and Johannes Brandstetter},
  journal={arXiv preprint arXiv:2406.04303},
  year={2024}
}

% xLSTM
@article{beck2024xlstm,
  title={xLSTM: Extended Long Short-Term Memory},
  author={Beck, Maximilian and P{\"o}ppel, Korbinian and Spanring, Markus and Auer, Andreas and Prudnikova, Oleksandra and Kopp, Michael and Klambauer, G{\"u}nter and Brandstetter, Johannes and Hochreiter, Sepp},
  journal={arXiv preprint arXiv:2405.04517},
  year={2024}
}

% DeiT
@inproceedings{touvron2021training,
  title={Training data-efficient image transformers \& distillation through attention},
  author={Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  booktitle={International conference on machine learning},
  pages={10347--10357},
  year={2021},
  organization={PMLR}
}

% visionMamba
@article{zhu2024vision,
  title={Vision mamba: Efficient visual representation learning with bidirectional state space model},
  author={Zhu, Lianghui and Liao, Bencheng and Zhang, Qian and Wang, Xinlong and Liu, Wenyu and Wang, Xinggang},
  journal={arXiv preprint arXiv:2401.09417},
  year={2024}
}

% MGQA
@inproceedings{wang2021multi,
  title={A multi-dimensional aesthetic quality assessment model for mobile game images},
  author={Wang, Tao and Sun, Wei and Min, Xiongkuo and Lu, Wei and Zhang, Zicheng and Zhai, Guangtao},
  booktitle={2021 International Conference on Visual Communications and Image Processing (VCIP)},
  pages={1--5},
  year={2021},
  organization={IEEE}
}

%HyperIQA
@InProceedings{Su_2020_CVPR,
author = {Su, Shaolin and Yan, Qingsen and Zhu, Yu and Zhang, Cheng and Ge, Xin and Sun, Jinqiu and Zhang, Yanning},
title = {Blindly Assess Image Quality in the Wild Guided by a Self-Adaptive Hyper Network},
booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

% MiniCPM
@article{yao2024minicpmv,
      title={MiniCPM-V: A GPT-4V Level MLLM on Your Phone}, 
      author={Yao, Yuan and Yu, Tianyu and Zhang, Ao and Wang, Chongyi and Cui, Junbo and Zhu, Hongji and Cai, Tianchi and Li, Haoyu and Zhao, Weilin and He, Zhihui and Chen, Qianyu and Zhou, Huarong and Zou, Zhensheng and Zhang, Haoye and Hu, Shengding and Zheng, Zhi and Zhou, Jie and Cai, Jie and Han, Xu and Zeng, Guoyang and Li, Dahai and Liu, Zhiyuan and Sun, Maosong},
      journal={arXiv preprint 2408.01800},
      year={2024},
}

% VGG
@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}

% Likert scale xxx
@article{ghadiyaram2015massive,
  title={Massive online crowdsourced study of subjective and objective picture quality},
  author={Ghadiyaram, Deepti and Bovik, Alan C},
  journal={IEEE Transactions on Image Processing},
  volume={25},
  number={1},
  pages={372--387},
  year={2015},
  publisher={IEEE}
}

% compare2score
@article{zhu2024adaptive,
  title={Adaptive Image Quality Assessment via Teaching Large Multimodal Model to Compare},
  author={Zhu, Hanwei and Wu, Haoning and Li, Yixuan and Zhang, Zicheng and Chen, Baoliang and Zhu, Lingyu and Fang, Yuming and Zhai, Guangtao and Lin, Weisi and Wang, Shiqi},
  journal={arXiv preprint arXiv:2405.19298},
  year={2024}
}

% lstm
@article{hochreiter1997long,
  title={Long Short-term Memory},
  author={Hochreiter, S},
  journal={Neural Computation MIT-Press},
  year={1997}
}

%GRU
@article{cho2014learning,
  title={Learning phrase representations using RNN encoder-decoder for statistical machine translation},
  author={Cho, Kyunghyun},
  journal={arXiv preprint arXiv:1406.1078},
  year={2014}
}

% mamba2
@inproceedings{mamba2,
  title={Transformers are {SSM}s: Generalized Models and Efficient Algorithms Through Structured State Space Duality},
  author={Dao, Tri and Gu, Albert},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2024}
}

% transformer
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

% KLT
@article{yang2020blind,
  title={Blind image quality assessment based on multi-scale KLT},
  author={Yang, Chao and Zhang, Xinfeng and An, Ping and Shen, Liquan and Kuo, C-C Jay},
  journal={IEEE Transactions on Multimedia},
  volume={23},
  pages={1557--1566},
  year={2020},
  publisher={IEEE}
}
% GraphIQA
@article{sun2022graphiqa,
  title={GraphIQA: Learning distortion graph representations for blind image quality assessment},
  author={Sun, Simeng and Yu, Tao and Xu, Jiahua and Zhou, Wei and Chen, Zhibo},
  journal={IEEE Transactions on Multimedia},
  volume={25},
  pages={2912--2925},
  year={2022},
  publisher={IEEE}
}

@article{yang2020ttl,
  title={TTL-IQA: Transitive transfer learning based no-reference image quality assessment},
  author={Yang, Xiaohan and Li, Fan and Liu, Hantao},
  journal={IEEE Transactions on Multimedia},
  volume={23},
  pages={4326--4340},
  year={2020},
  publisher={IEEE}
}

@article{zhu2022blind,
  title={Blind image quality assessment via cross-view consistency},
  author={Zhu, Yucheng and Li, Yunhao and Sun, Wei and Min, Xiongkuo and Zhai, Guangtao and Yang, Xiaokang},
  journal={IEEE Transactions on Multimedia},
  volume={25},
  pages={7607--7620},
  year={2022},
  publisher={IEEE}
}

% TempQT
@article{shi2023blind,
  title={Blind Image Quality Assessment via Transformer Predicted Error Map and Perceptual Quality Token},
  author={Shi, Jinsong and Gao, Pan and Smolic, Aljosa},
  journal={IEEE Transactions on Multimedia},
  year={2023},
  publisher={IEEE}
}

% ArtScore
@article{chen2024learning,
  title={Learning to Evaluate the Artness of AI-generated Images},
  author={Chen, Junyu and An, Jie and Lyu, Hanjia and Kanan, Christopher and Luo, Jiebo},
  journal={IEEE Transactions on Multimedia},
  year={2024},
  publisher={IEEE}
}

