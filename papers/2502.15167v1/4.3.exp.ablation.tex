
\subsection{Ablation Studies - RQ2, RQ3, and RQ4}
To determine the contributions of each component to the overall performance of our method, experiments were conducted at every stage of the process.

\subsubsection{Image Descriptions}\label{sec:img_desc}
To demonstrate the importance of inferred image descriptions in terms of quality, correspondence, and authenticity, and their roles as contextual inputs for enhancing deep understanding of the AGI, we assessed their impact specifically on the quality aspect of dataset AGIQA-3k~\cite{li2023agiqa}, with results detailed in Table~\ref{tab:img_desc}.
The column ``Present Descriptions'' denotes the various combinations of generated image descriptions used throughout the experimental stages.
The first line, which lacks any descriptions, depicts a scenario where the model directly predicts the outcome without any contextual input. 
This configuration suggests we only fine-tuned the local MLLM with a single-round conversation, excluding any detailed responses from the MLLM, and can be interpreted as \textit{w/o CoT}. 
In other setups, the MLLM was prompted to describe the image across different aspects, thereby enriching the context for the prediction task.

\begin{table}[h]
    \centering
    \caption{Comparison results on AGIQA-3k~\cite{li2023agiqa} quality aspect with different combinations of image description aspects.}
    \label{tab:img_desc}
    \begin{tabular}{ccc| cc}
        \toprule
        \multicolumn{3}{c|}{Present Descriptions} & 
        \multicolumn{2}{c}{Qual.} \\
        \hline
        Qual. & Corr. & Auth. & SRCC & PLCC  \\
        \midrule
        \ding{55} & \ding{55} & \ding{55} & 0.8816 & 0.9193 \\
        \ding{51} & \ding{55} & \ding{55} & 0.8989 & 0.9342 \\
        \ding{51} & \ding{51} & \ding{55} & 0.9045 & 0.9317 \\
        \ding{51} & \ding{51} & \ding{51} & 0.8999 & 0.9321 \\
        \bottomrule
    \end{tabular}
\end{table}

The results reveal that the model trained without any intermediate contextual descriptions has the weakest performance, suggesting that leveraging the innate capabilities of the original MLLM is insufficient.
In contrast, adding descriptions that cover both quality and correspondence aspects leads to the best performance, indicating the correspondence aspect likely contributes to human-like perspective enhancements that positively influence quality judgments.
However, including additional aspects such as authenticity slightly degrades performance. 
This might be because authenticity has a less direct relationship with image quality aspect, and its inclusion introduces noise that could detract from the effectiveness of the model in assessing quality.

\subsubsection{Distillation \& Fine-tuning}
Additional experiments on AGIQA-3k~\cite{li2023agiqa} quality aspect in Table~\ref{tab:ab2} were conducted to validate the effectiveness of distilling image description capabilities from an online MLLM to a local MLLM.
We included the inference stage but maintained the absence of the initial description stage (w/o ft, w/ ID), which resulted in diminished performance.


\begin{table}
    \centering
    \caption{Ablation studies on AGIQA-3k~\cite{li2023agiqa} quality aspect.}
    \label{tab:ab2}
    \begin{tabular}{c| cc}
        \toprule
        Model settings & SRCC & PLCC  \\
        \midrule
        w/o ft, w/o ID & 0.8886 & 0.9208 \\
        w/o ft, w/ ID & 0.8800 & 0.9030 \\
        w/o ft, w/ FullConv & 0.8720 & 0.8939 \\
        w/ ft, w/o ID & 0.8921 & 0.9249 \\
        w/ ft, w/ FullConv & 0.8968 & 0.9278 \\
        \midrule
        w/o xLSTM & 0.8959 & 0.9274 \\
        w/o lm\_header & 0.8949 & 0.9325 \\
        \midrule
        w/ last token & 0.8795 & 0.9245 \\
        w/ max pooling & 0.8884 & 0.9236 \\
        w/ mean of the first and last token & 0.8953 & 0.9286\\
        \midrule
        w/ LSTM & 0.8944 & 0.9239 \\
        w/ GRU & 0.8928 & 0.9065 \\
        w/ Mamba2 & 0.8961 & 0.9283 \\
        w/ Transformer & 0.8964 & 0.9291 \\
        \midrule
        
        M3-AGIQA (Ours) & 0.9045 & 0.9317 \\
        \bottomrule
    \end{tabular}
\end{table}

After inferencing the result, we approached the AGIQA task as a classification problem, assigning labels [bad, poor, fair, good, excellent] within dataset AGIQA-3k~\cite{li2023agiqa}.
Classification labels are segmented according to ranges of MOS, for example scores from [0-1], [1-2], and [2-3] correspond to bad, poor, and fair, respectively.
Thus, an image with a target label score 1.99 but predicted as 2.01 would be classified as fair instead of poor.
Given these nuances, adopting a less stringent classification metric termed ``Rough Accuracy'' is beneficial:
\begin{equation}
    \textrm{Rough Acc.} = \frac{\sum_{i=1}^{N}{\mathbbm{1}\left(\left|\hat{y^i}-y^i\right| \leq 1 \right)}}{N},
\end{equation}
where $\mathbbm{1}(\cdot)$ is the indicator function, $\hat{y_i}$ and $y_i$ are the integers ranging from $0$ to $4$ representing the five quality categories, and $N$ is the total number of samples evaluated. 
As detailed in Table~\ref{tab:ab3cls}, the overall performance is quite favorable, demonstrating the effectiveness of fine-tuning.

\begin{table}[h]
    \centering
    \caption{Classification task on AGIQA-3k~\cite{li2023agiqa} quality aspect.}
    \label{tab:ab3cls}
    \begin{tabular}{c| cccc}
        \toprule
        Model settings & Rough Acc. & Acc. & Precision & F1  \\
        \midrule
        w/o ft, w/ ID & 0.5427 & 0.1005 & 0.2615 & 0.1018 \\
        w/ ft, w/ ID & 0.9916 & 0.6616 & 0.6462 & 0.6534 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsubsection{MLLM as an Encoder}\label{sec:mllm_encoder}
The LLM inside the MLLM utilizes a linear head to project hidden representations to the vocabulary dimension, producing the output logits.
We explored the impact of removing this projection head (w/o lm\_head), the inferior result in Table \ref{tab:ab2} demonstrates the vocabulary distribution learned during distillation remains indispensable.
Furthermore, removing the xLSTM head (w/o xLSTM) led to decreased performance, underscoring the critical role of xLSTM in enhancing sequential feature extraction.

Additionally, we conducted experiments on the pooling methods used after the xLSTM head, as detailed in Table \ref{tab:ab2}.
Using only the last token (w/ last token) for pooling resulted in poor performance, indicating insufficient contextual information was captured.
Implementing max pooling (w/ max pooling) led to even worse outcomes, suggesting a significant loss of context.
Alternatively, using a mean of the first and last tokens (w/ mean of the first and last tokens) also proved inferior to our method with mean pooling, which was found to best preserve rich contextual data.

We also evaluated different architectures as the sequential feature extractor, including LSTM~\cite{hochreiter1997long}, GRU~\cite{cho2014learning}, Mamba2~\cite{mamba2}, and Transformer~\cite{vaswani2017attention}.
Our findings indicate that configurations with Mamba2 (w/ Mamba2) and Transformer (w/ Transformer) outperform those with LSTM (w/ LSTM) and GRU (w/ GRU), which makes sense since Mamba2 and Transformers are somehow equivalent and have been proven to be better than previous sequential models.
However, our implementation with an xLSTM header advances this further benefiting from specially designed modules within xLSTM that enhance its competitive edge.

\subsubsection{Round of Conversations}
Additionally, the influence of conversation rounds on model performance was investigated, building on the settings discussed in Section~\ref{sec:img_desc}, which considers the content of context by various aspects.

Initially, we tested a configuration without fine-tuning the local MLLM with an online MLLM (w/o ft) and without image descriptions (w/o ID), relying solely on the local MLLM with a simple prompt requesting quality results within categories [bad, poor, fair, good, excellent], this configuration unsurprisingly yielded the poorest performance.
Then with the image description (w/o ft, w/ ID) performs worse,
even when incorporating a full conversation including the generated final quality result (w/o ft, w/ FullConv), performance decreases further.
This is likely because the original local MLLM did not align well with human perspective evaluation, leading to increased bias with more descriptive context.
By distillation with online MLLM but without image descriptions (w/ ft, w/o ID), the model outperformed the configuration without fine-tuning, indicating the strong image description capabilities related to the three aspects from online MLLM can be effectively transferred to the local MLLM.
Furthermore, we tested the configuration to infer the classification result (w/ ft, w/ FullConv), the performance decline could be caused by the determinate results potentially misleading the subsequent training step.
Our M3-AGIQA implementation (w/ ft, w/ ID), which leverages conversations including an initial prompt from user, a response from MLLM, and another user prompt for asking for the result, further emphasizes the crucial role of image descriptions in achieving the results outlined in Sec.~\ref{sec:img_desc}.
