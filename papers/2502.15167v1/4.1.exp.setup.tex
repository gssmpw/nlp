

\section{Experiments} \label{sec:exp}
This section presents extensive experiments conducted to evaluate our proposed method M3-AGIQA in comparison with other SOTA models.
Our experimental design is structured to address the following key research questions:
\textbf{RQ1:} How does M3-AGIQA compare in performance with current SOTA methods?
\textbf{RQ2:} What is the contribution of each component within M3-AGIQA to its overall performance?
\textbf{RQ3:} Does the distillation process enhance the overall performance of the model?
\textbf{RQ4:} How effective is using an MLLM as an encoder within our framework?
\textbf{RQ5:} How does the fine-tuned model perform across different datasets?
\subsection{Setup} \label{subsec:setup}
\subsubsection{Datasets}
As summarized in Table~\ref{tab:dataset}, we utilize three public AGIQA datasets for our experiments, including AGIQA-3k~\cite{li2023agiqa}, AIGCIQA2023~\cite{wang2023aigciqa2023}, and AIGCIQA-20k~\cite{li2024aigiqa}.
Each dataset is widely recognized in the field and provides Mean Opinion Scores (MOS) that assess quality, correspondence, and authenticity aspects either fully or partially.
\textbf{AGIQA-3k}~\cite{li2023agiqa} includes $2982$ images generated by $6$ different models which are GAN, auto-regression, and diffusion-based models. It annotates MOSs for image perception quality and correspondence with the prompt.
\textbf{AIGCIQA2023}~\cite{wang2023aigciqa2023} is composed of $2400$ images from $6$ cutting-edge models with MOSs for three aspects including quality, correspondence, and authenticity. Each prompt generates $4$ images for one model.
\textbf{AIGCIQA-20k}~\cite{li2024aigiqa} from the NTIRE 2024 Quality Assessment Challenge has $20,000$ images generated by $15$ popular models, along with MOSs collected from 21 subjects.

\begin{table}
    \caption{Statistics of the datasets.\label{tab:dataset}}
    \centering
    \begin{tabular}{cccc}
        \toprule
        Statistics & AGIQA-3k & AIGCIQA2023 & AIGCIQA-20k \\
        \midrule
        No. of images & 2,982 & 2,400 & 20,000 \\
        No. of T2I models & 6 & 6 & 15 \\
        quality MOS & \ding{51} & \ding{51} & \ding{51} \\
        correspondence MOS & \ding{51} & \ding{51} & \ding{55} \\
        authenticity MOS & \ding{55} & \ding{51} & \ding{55} \\ 
        \bottomrule
    \end{tabular}
\end{table}

\subsubsection{Baselines}
To demonstrate the effectiveness of our proposed method M3-AGIQA, we select several baselines for comparative analysis: 
(1) \textbf{Simple Vision Encoders with Regression Heads}: ResNet50~\cite{he2016deep}, ViT/B/16~\cite{dosovitskiy2020image}, and ViL~\cite{alkin2024visionlstm}. We integrate a two-layer MLP as the regression head to directly predict the MOS; 
(2) \textbf{Established IQA methods}: DBCNN~\cite{zhang2018blind}, HyperIQA~\cite{Su_2020_CVPR}, StairIQA~\cite{sun2022blind}, and MGQA~\cite{wang2021multi}; 
(3) \textbf{AGIQA methods}: AMFF-Net$^\ast$~\cite{zhou2024adaptive}, MA-AGIQA$^\ast$~\cite{wang2024large}, IPCE$^\ast$~\cite{peng2024aigc}, and SF-IQA$^\ast$~\cite{yu2024sf}. 
Since these models have not been open-sourced, we obtained their results directly from the respective published papers.
\subsubsection{Implementation Details}
The MLLM used in the experiments is \textit{openbmb/MiniCPM-Llama3-V-2\_5}~\cite{yao2024minicpmv}, a lightweight \textit{GPT-4V} level MLLM. 
To generate image descriptions for distillation, we utilize the free online MLLM Google \textit{gemini-1.5-pro} API as the teacher model.
The distillation was conducted via an official fine-tuning script\footnote{\url{https://github.com/OpenBMB/MiniCPM-V}}. 
The xLSTM head in our model is made up of four layers, including one sLSTM layer positioned at layer $2$ and three mLSTM layers at positions $1$, $3$, and $4$.
The regression head is structured with two linear layers.
Our experiments employed \textit{PyTorch} and \textit{TorchLightning 2.3.0}\footnote{\url{https://lightning.ai/docs/pytorch/2.3.0/}} to implement the training process.
The datasets were partitioned based on practices established in previous studies: 4:1:0 for AGIQA-3k, 3:1:0 for AIGCIQA2023, and 7:2:1 for AGIQA-20k, in terms of training, test, and validation sets respectively.
During fine-tuning, the learning rate was set to $2e$-$6$, and the batch size was fixed at $2$.
The vision encoder was fine-tuned using the LoRA technique while the LLM parameters remained frozen.
In addition, deepspeed ZeRO-3 offload was employed to minimize GPU VRAM usage.
The fine-tuning process ranged from $2,000$ to $4,000$ steps for AGIQA-3k and AIGCIQA2023, and around $20,000$ steps for AIGCIQA-20k.
In the training stage, AdamW was utilized as the optimizer.
The hidden dimension size $d$ was set to $512$ and the vocabulary size of the MLLM tokenizer $d_{vocab}$ was $128256$.
All experiments were conducted using an NVIDIA A100-PCIE-40GB GPU and an Intel Xeon Gold 6248R CPU.
\subsubsection{Metrics}
To evaluate the performance of our method, we utilized two widely-used metrics in IQA tasks: Spearman's Rank-Order Correlation Coefficient (SRCC) and Pearson's Linear Correlation Coefficient (PLCC).
SRCC measures ability of the model to preserve the rank order of the predictions relative to the ground truth, indicating its effectiveness in ranking images based on quality.
PLCC evaluates the linear correlation between the predicted and actual scores, representing how the model fits the data.
Both metrics range from $-1$ to $1$, with higher values indicating better performance.
