\section{Related Works}
\label{appendix:relatedwork}

\subsection{Concept Drift}

In the comprehensive survey conducted by Lu et al. ____, existing approaches for concept drift handling are systematically classified into three primary categories: error rate-based methods ____, data distribution-based methods ____, and multiple hypothesis-based techniques ____. Our proposed methodology falls under the category of distribution-based concept drift adaptation approaches. These distribution-driven techniques distinguish themselves by not only enabling precise drift identification through explicit statistical distribution analysis but also providing a multidimensional characterization of drift patterns - including temporal occurrence detection, affected feature space localization, and quantitative severity assessment. This dual capability of detection coupled with comprehensive drift diagnostics establishes distribution-based methods as particularly valuable for developing adaptive systems that require interpretable drift understanding and targeted model adjustments.


Besides, Online Boosting Adaptive Learning (OBAL) method ____ is proposed to address the challenges of concept drift and negative transfer in multistream classification. OBAL employs a dual-phase approach: an initial model is built using the Adaptive Covariate Shift Adaptation (AdaCOSA) algorithm to handle covariate shifts and learn dynamic correlations among streams. In the online phase, a Gaussian Mixture Model-based weighting mechanism is integrated to manage asynchronous drift. Meanwhile, CDMLLM ____ reveals that vision-language models suffer significant bias from concept drift during pre-training and fine-tuning. To address this, the authors propose a unified concept drift framework integrating T-distribution-based adaptation for long-tailed calibration and explicit OOD detection, demonstrating enhanced robustness in open-world multi-modal alignment through systematic distribution modelling. Beyond on drift detection in single data stream, GDDM ____ focuses on addressing group concept drift across multiple data streams, where individual drifts may go undetected due to subtle changes in underlying distributions. The proposed method introduces a distribution-free test statistic to detect concept drift in these complex scenarios. By designing an online learning algorithm for streaming data, the approach accurately identifies concept drift caused by hypothesis testing. Beyond that, DDG-DA ____ proactively models predictable factors influencing environmental evolution. The approach involves training a predictor to estimate future data distribution, using this information to generate training samples, and then training models on the generated data. By leveraging predictable factors to forecast data distribution changes, DDG-DA aims to enhance model performance in handling concept drift in streaming data. Furthermore, STUDD____ proposes a teacher-student paradigm to enable unsupervised drift detection through deviation analysis of their predictive consistency. This approach leverages model disagreement as a proxy signal, bypassing dependency on ground-truth labels during deployment while maintaining detection sensitivity.

\subsection{Causal Inference}

Recently, increasing researchers have incorporated causal inference into deep-learning models, especially in large models. Deconfounded Image Captioning (DIC) ____ is proposed to address dataset bias in vision-language models through a causal lens, that integrates backdoor and front-door adjustments for systematic bias mitigation. The framework provides principled causal analysis of spurious correlations in multimodal alignment, offering theoretical grounding for decomposing bias sources through structured interventions. Likewise, aiming for spurious correlations induced by visual and linguistic biases during training, CIIC ____ is proposed as a causal intervention framework combining an Interventional Object Detector (IOD) and Interventional Transformer Decoder (ITD) guided by structural causal models. By applying backdoor adjustment through IOD's feature disentanglement and ITD's dual de-confounding mechanism, their approach systematically mitigates confounding effects across encoding and decoding stages, demonstrating enhanced generalization through causal correlation modeling. Similarly, targeting multi-hop fact verification bias in the large language model, Causal Walk ____ is proposed, a front-door adjustment framework that disentangles complex spurious correlations in evidence chains. 
The method models reasoning paths as mediators in structural causal models, decomposing causal effects via random walk-based treatment-mediator estimation and geometric mean-based mediator-outcome approximation. By integrating adversarial and symmetric datasets synthesized with large language models, the approach demonstrates superior debiasing performance. 

Additionally, causal inference is widely used in representation learning. Comprehensive Interventional Distillation (CID) ____ integrates causal intervention with class-aware representation alignment. By reinterpreting teacher logits as contextual confounders and applying counterfactual pruning through structural causal models, CID systematically disentangles beneficial semantic patterns from dataset-specific biases. This approach demonstrates enhanced generalization through bias-invariant knowledge transfer. Besides, De-confound-TDE ____ establishes a causal framework for long-tailed classification, identifying SGD momentum as a paradoxical confounder that simultaneously harms tail-class predictions while benefiting representation learning. Through causal intervention during training and counterfactual reasoning at inference, the method disentangles momentum’s detrimental bias from its beneficial mediation effects. Meanwhile, CCM ____ is proposed to address domain generalization through causal invariance principles. The framework integrates front-door adjustment with contrastive learning to quantify stable causal effects across domains, explicitly modeling domain shifts via a three-stage process: domain-conditioned supervision for feature correlation, causal effect measurement through structured path manipulation, and contrastive clustering for class-consistent representations. Similarly, CIRL ____ advances domain generalization through causal factorization, proposing a structural causal model that decomposes inputs into invariant causal mechanisms and domain-specific non-causal factors. It enforces three critical properties: causal/non-causal separation, joint independence, and causal sufficiency for classification. 

Besides, C2L ____ addresses model fragility to spurious patterns through contrastive counterfactual synthesis, proposing a collective decision framework that aggregates predictions across generated counterfactual sets. Unlike conventional augmentation limited by dataset-inherent biases, this approach probabilistically supervises causal invariance through distributional consensus, demonstrating enhanced robustness against attribution bias and domain shifts. Furthermore, ABCD ____ establishes a causal interpretation of Transformer self-attention mechanisms, modeling them as structural equation estimators that capture conditional independence relations through partial correlation analysis in deep attention layers. This framework enables zero-shot causal discovery over input sequences while accounting for latent confounders, effectively repurposing pre-trained models for causal graph inference. What’s more, Causal Attention (CATT) ____ implements front-door adjustment to address confounding bias in attention mechanisms via dual-path processing of In-Sample and Cross-Sample Attention. By forcibly integrating external sample contexts through CS-ATT while maintaining standard attention conventions, CATT dynamically mitigates spurious correlations without requiring confounder specification. 

\subsection{Contrastive Pre-training}

The seminal work of SimCLR ____ establishes foundational principles for contrastive pretraining through instance discrimination objectives and systematic augmentation strategies. While achieving remarkable performance in static environments, its reliance on fixed augmentation policies and stationary negative sampling assumes temporal consistency of latent patterns – an assumption violated under concept drift scenarios. Nevertheless, SimCLR's demonstration of invariant representation learning through normalized temperature-scaled loss provides crucial architectural groundwork for developing drift-aware contrastive frameworks, particularly in modeling evolving feature relationships through dynamic positive/negative pair formulation.

Besides, the MoCo framework ____ and its subsequent evolution MoCo v3 ____ establish critical momentum-based mechanisms for contrastive learning through dynamic memory banks and stabilized key encoder updates. It is primarily designed to maintain consistency in negative sample maintenance and gradient stability. However, their fixed momentum schedules and stationary target assumptions limit adaptability to abrupt distribution shifts as we discussed aforementioned. Likewise, BYOL ____ employs two neural networks—an online network and a target network—that interact and learn from each other. Specifically, the online network is trained to predict the representation of an augmented image view produced by the target network, while the target network is updated using a slow-moving average of the online network's parameters. Similarly, the SiamSim framework ____ advances contrastive representation learning through dynamic similarity calibration, employing a dual-path Siamese architecture with momentum-aligned feature projectors. 

Meanwhile, DINO ____ and DINO v2 ____ advance self-distillation paradigms through momentum-based teacher-student mechanisms, leveraging global-local attention consistency in Vision Transformers to learn semantically structured representations. While achieving state-of-the-art in static self-supervised learning, their fixed teacher-student update schedules and monolithic prototype banks implicitly assume stationarity of feature distributions. DINO v2's introduction of partitioned expertise through specialized sub-networks demonstrates partial adaptability to data variations, suggesting pathways for concept drift mitigation through dynamic expert routing. These works collectively establish critical baselines for stability-aware distillation architectures in non-stationary pre-training scenarios.

Furthermore, DenseCL ____  advances contrastive learning through localized feature alignment, introducing dense instance discrimination that operates at both image-level and pixel-level granularity. By enforcing spatial consistency through region-to-region contrastive pretext tasks, the method enhances model sensitivity to fine-grained visual patterns. While primarily designed for dense prediction tasks, its hierarchical contrast mechanism demonstrates the importance of multi-scale feature stabilization.

While contemporary contrastive pretraining methods achieve remarkable performance under static data distributions, their reliance on closed-world stationarity assumptions presents a fundamental limitation: they inherently lack mechanisms for concept drift adaptation when deployed in non-stationary environments with evolving data streams.