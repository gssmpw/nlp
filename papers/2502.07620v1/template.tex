\documentclass{article}


\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{graphicx}
% \graphicspath{ {./images/} }
\usepackage{subcaption}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage{multirow}
\usepackage[table,xcdraw]{xcolor}

\title{Causal-Informed Contrastive Learning: Towards Bias-Resilient Pre-training under Concept Drift}


\author{
  Xiaoyu Yang \\
  Australian Artificial Intelligence Institute (AAII) \\
  Faculty of Engineering and Information Technology \\
  University of Technology Sydney \\
  Australia \\
  \texttt{xiaoyuyang386@gmail.com} \\
  %% examples of more authors
  \And
  Jie Lu * \\
  Australian Artificial Intelligence Institute (AAII) \\
  Faculty of Engineering and Information Technology \\
  University of Technology Sydney \\
  Australia 
  \And
  En Yu \\
  Australian Artificial Intelligence Institute (AAII) \\
  Faculty of Engineering and Information Technology \\
  University of Technology Sydney \\
  Australia 
}

\begin{document}
\maketitle

\begin{abstract}

  The evolution of large-scale contrastive pre-training propelled by top-tier datasets has reached a transition point in the scaling law. Consequently, sustaining and enhancing a model's pre-training capabilities in drift environments have surfaced as a notable challenge.
  In this paper, we initially uncover that contrastive pre-training methods are significantly impacted by concept drift wherein distributions change unpredictably, resulting in notable biases in the feature space of the pre-trained model. 
  Empowered by causal inference, we construct a structural causal graph to analyze the impact of concept drift to contrastive pre-training systemically, and propose the causal interventional contrastive objective. Upon achieving this, we devise a resilient contrastive pre-training approach to accommodate the data stream of concept drift, with simple and scalable implementation. Extensive experiments on various downstream tasks demonstrate our resilient contrastive pre-training effectively mitigates the bias stemming from the concept drift data stream. Codes are available at \url{https://anonymous.4open.science/r/ResilientCL/}.
  
  
  
  % The concept drift is characterized by unpredictable distribution changes, encompassing gradual drift exemplified by long-tailed scenarios and sudden drift like Out-of-Distribution (OOD) drift. While these challenges have been examined across different domains, their effects on contrastive pre-training in the context of drift data stream have been largely underexplored. 
  
  % 高质量数据推动下的scaling law逐渐陷入了瓶颈
\end{abstract}


% keywords can be removed
%\keywords{First keyword \and Second keyword \and More}



\section{Introduction}

Contrastive learning has proven to be highly effective in pre-training large-scale models, especially in large vision models exemplified by frameworks like SimCLR \cite{chenSimpleFrameworkContrastive2020,chenBigSelfSupervisedModels2020}, MoCo series 
\cite{heMomentumContrastUnsupervised2020,chenEmpiricalStudyTraining2021}, DINO series \cite{caronEmergingPropertiesSelfSupervised2021,zhouDINOWMWorldModels2024}. 
% , CLIP \cite{radfordLearningTransferableVisual2021} and BLIP \cite{liBLIPBootstrappingLanguageImage2022, liBLIP2BootstrappingLanguageImage2023}
However, with the ongoing scaling of large models, data hunger for contrastive learning is raising more attention in the community towards pre-training effectively from drift data. 
It could be caused by long-tailed data, noise, and domain shift, where concept drift \cite{luLearningConceptDrift2019, yang2024adaptingmultimodallargelanguage} is utilized to uniformly summarize this phenomenon of unpredictable distribution changes in the pre-training through contrastive learning. 
Hence, a pertinent question emerges: beyond the existing contrastive learning methods,
\textit{can contrastive paradigm learn from drift pre-training?} In this work, we aim to bridge this gap by providing a systematic analysis of the above question. Our findings highlight critical vulnerabilities of the current contrastive pre-training paradigm in adapting to these challenges, underscoring the need for novel strategies to enhance their robustness in drift data streams. More related works are provided in Appendix \ref{appendix:relatedwork}.


Current contrastive pre-training methods predominantly adhere to the paradigm of comparing two distinct views of the same object, typically derived from different encoders. Especially in terms of large vision models, student-teacher structures are used to extract different features from two views of images for contrasting agreement, like DINO \cite{caronEmergingPropertiesSelfSupervised2021}, MoCo v3 
 \cite{chenEmpiricalStudyTraining2021}. The student network is propelled by the contrastive loss, whereas the teacher network is momentum updated through an exponential moving average (EMA) of the student parameters. In the context of drift pre-training, this contrastive manner will be subtly affected by the concept drift of data streams and be accumulated through momentum, leading to amplified bias. We utilize long-tailed drift as a representative case to demonstrate the impact of concept drift on contrastive pre-training. Illustrated in Fig. \ref{fig:analysis}, we visualize the feature space to investigate how long-tailed drift impacts the contrastive pre-training of MoCo v3 \cite{chenEmpiricalStudyTraining2021}. ImageNet \cite{russakovskyImagenetLargeScale2015} and  ImageNet-LT \cite{liuLargeScaleLongTailedRecognition2019} are separately leveraged as balanced and long-tailed sources for contrastive pre-training, where the test data for generating feature space is balanced. Categories are differentiated by varying colors ranging from dark to light, indicating a spectrum from categories with few training samples to those with abundant samples. It is evident from Fig. \ref{fig:imbalanced}, under imbalanced drift pre-training, the tail categories are confined to a small portion of the feature space, whereas the majority is dominated by the head categories with ample samples. The model is steered away from the center of balanced feature space, due to the accumulated bias in momentum update.


\begin{figure}[htbp]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
      \includegraphics[height=0.2\textheight]{./images/balanced.pdf}
        \caption{Balanced Pre-training}
        \label{fig:balanced}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[height=0.2\textheight]{./images/long-tail.pdf}
        \caption{Long-Tailed Drift Pre-training}
        \label{fig:imbalanced}
    \end{subfigure}
    \caption{The t-SNE visualization of feature space under the different conditions of pre-training within ImageNet and ImageNet-LT. The dark colors signify the region corresponding to the tail category with limited pre-training samples, whereas light colors denote the head category characterized by abundant samples.}
    \label{fig:analysis}
\end{figure}


Under the above assumption, we formulate the causal relationships \cite{pearl2014interpretation} among samples $X$, predictions $Y$, the concept drift $D$ within data streams and the bias $B$ in the pre-trained teacher network with momentum update in a causal graph as shown in Fig.\ref{fig:causal_model}. We find the concept drift $D$ within data streams is a confounder of $X$ and $Y$, where the backdoor \cite{pearl1995causal} path $X\leftarrow D \rightarrow B \rightarrow Y$ leads to the false correlation, and the mediation \cite{pearl2022direct} path $X \rightarrow B \rightarrow Y$ confounds the direct contribution of $X \rightarrow Y$. More analyses and details are given in Section \ref{section:causal}. 

Therefore, we propose a novel contrastive learning-based pre-training method with causal intervention \cite{pearl2016causal} to mitigate the bias caused by concept drift in data streams.

In summary, our paper mainly makes the following contributions:
\begin{enumerate} 
    \item We are pioneers in uncovering the causal effects of concept drift on contrastive pre-training, especially in the accumulated bias through momentum update. It will facilitate future research in conducting a more comprehensive examination of concept drift on contrastive pre-training and leveraging larger datasets for pre-training large models.

    
    \item By constructing the structural causal graph, we leverage causal inference theory to systemically analyze the concept drift within the biased datastream under the contrastive objective. With the aforementioned discussion, interventional contrastive pre-training is proposed to disentangle the drift effect during the pre-training. It offers a straightforward and scalable implementation for more abundant datasets and larger models.

    
    \item Extensive experiments evaluate the performance of our contrastive method under drift pre-training. In the context of downstream tasks of long-tailed classification, OOD detection and domain shift, ours demonstrates superior performance with results of fine-tuning and linear probing. Crucially, we assess the inter-category distances to evaluate the efficacy of pre-training, where our model outperforms others, demonstrating our model effectively addresses drift in contrastive pre-training, facilitating large-scale pre-training.
\end{enumerate}


\section{Methodology}

\subsection{Causal Views of Drift Pre-training}
\label{section:causal}
Concept drift is a statistical phenomenon wherein the distributional characteristics of a target domain undergo arbitrary changes over the course of training \cite{luLearningConceptDrift2019}. Regarding the drift pre-training, we formula these unpredictable distribution changes with data stream $S_{0,t} = \{s_{0},...,s_{t}\}$, where $s_{i}=(x_{i},z_{i})$ represents a sample with feature vectors $x_{i}$ and contrastive label $z_{i}$, and $t$ denotes the timestamp in the training data stream, corresponding to the iteration step of the pre-training process. In iteration $t$, $S_{0,t}$ follows a certain distribution $F_{0,t}(x,z)$, thus the drift pre-training can be formalized as: 
\begin{equation}
    \exists t: P_{t}(x,z) \neq P_{t+1}(x,z)
\end{equation}
where the joint probability $P_{t}(x,z)$ can be decomposed as $P_{t}(x,z) = P_{t}(x)\times P_{t}(z | x)$. 
% Therefore, concept drift theory provides a unified framework to harmonize the tailed shift and OOD shift that often occur together, enabling more robust and adaptive deep learning models.

Regarding the concept drift in contrastive learning, the momentum update of the teacher network can be expressed as the following formula:
\begin{equation}
\label{eq:ema}
    \theta^{m}_{t} = \lambda\theta^{m}_{t-1} +(1-\lambda) \theta^{g}_{t-1}  
\end{equation}
where $\theta^{m}$ signifies the parameters of the teacher network, $\theta^{g}$ denote those of the student network, and $\lambda\in [0,1)$ represents the momentum coefficient. Momentum $ \lambda\theta^{m}_{t-1}$ induces a slower evolution of the teacher network $\theta^{m}$ relative to the student network $\theta^{s}$. While this approach contributes to the stabilization of training, it can lead to the accumulation of biases that are not readily rectified, especially the large momentum advised by MoCo v3 (e.g. $\lambda = 0.999$).Hence, in order to tailor contrastive learning for drift pre-training, it is crucial to adjust the model to synchronize with the evolving data distribution for maximum agreement, formally defined as
\begin{equation}
    \label{eq:max}
    \max_{g^{(t)},g^{(t+1)},...,g^{(t+\tau)}} \sum_{i=t}^{t+\tau} L(g^{(i)}(\tilde{x}^{i}), m^{(i)}(\hat{x}^{i}))
\end{equation}
where $\hat{x}$ and $\tilde{x}$ symbolize different augmented samples obtained from $x$, $g_{\theta}^{(t)}$ denotes the student model trained by the data stream $S_{t-k, t-1}$ from the drift adaption window with the size of $k$, and $m_{\theta}^{(t)}$ represents the momentum model. And the model is driven by the target metric $L$ continuously to adapt the drift in a given drift adaptation window of $[t, t+\tau]$ time period.
\begin{figure}[htbp]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
      \includegraphics[width=0.49\textwidth]{./images/causal-1.pdf}
        \caption{Structural Causal model}
        \label{fig:causal_model}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=0.49\textwidth]{./images/causal-2.pdf}
        \caption{Intervention}
        \label{fig:intervention}
    \end{subfigure}
    \caption{The proposed causal graph of contrastive pre-training. \textbf{X}: Sample Features, \textbf{Y}: Prediction, \textbf{D}: Latent Concept Drift within Data Streams, and \textbf{B}: Sample Bias in the Momentum Update.}
    \label{fig:causal}
\end{figure}

% 这个bias是在student里还是在teacher里，要想清楚讲故事

For analyzing contrastive learning under the drift pre-training, we construct a structural causal graph \cite{pearl1995causal,pearl2016causal} to formula the causal relationship among sample features ($X$), prediction (Y), the contrastive teacher network ($T$) and the drift in the pre-training ($D$) as illustrated in Fig.\ref{fig:causal}, where $A\rightarrow B$ denotes that $A$ is the causer of $B$. The causal graph of $\{X,Y,D,B\}$ presents the following causal connections:

$D\rightarrow X$: $X$ denotes features extracted by samples drawn from the data stream with concept drift $D$, which is obviously trained under the effect of the drift pre-training.  


$ (X, D) \rightarrow B$: $ B$ represents the sample bias deviated from feature $ X$ under the effect of the concept drift $ D$ within the data stream. In the context of drift pre-training, the bias will accumulate during the momentum update of the teacher network, which will be amplified in the subsequent iteration of the contrastive pre-training.


$ (X, B) \rightarrow Y$: This link presents that, apart from the regular $X \rightarrow Y$, the prediction is also impacted by the concept drift within the data stream through the mediation bias of $B$. 


In the constructed causal graph, nodes $D$ and $B$ are identified as the confounder and the mediator \cite{pearl2022direct}, respectively. The confounder $D$ influences both the independent variable $X$ and the outcome variable $Y$ from the backdoor path $X\leftarrow D\rightarrow B \rightarrow Y$, thereby distorting the estimated effect of the sample features on the predictions and resulting in the spurious correlation. For example, in the case of long-tailed drift pre-training, tail samples may be incorrectly associated with head categories through this backdoor path, a misalignment attributable to this spurious correlation. Besides, mediator $B$ conveys the drift effect of sample features $X$ on predictions $Y$ via the pathway $X\rightarrow B \rightarrow Y$, weakening the direct impact of $X$ on $Y$. 



\subsection{Causal Interventional Contrastive Objective}


Drawing from the aforementioned causal relationships, it can be argued that current contrastive pertaining approaches involving the InfoNCE loss \cite{oordRepresentationLearningContrastive2019} essentially leverages the likelihood $P(Y|X)$ to drive the whole network, which is vulnerable by the concept drift confounder $D$ and leads to the emergence of spurious correlations. $P(Y|X)$ can be formulated as:
\begin{equation}
    P_{t}(Y|X) = \sum_{i}^{|D|}P_{t}(Y|X, B=h(X,d_{i}))P_{t}(d_{i} | X)
\end{equation}
where $h(X,d_{i})$ abstractly denotes the sample bias $B$ is a combination from $X$ and $d_{i}$.
% 这里需不需要添加一些例子，参考show, deconfound S3.1 

Motivated by the recent success of applying causal inference in deep learning \cite{yangCausalAttentionVisionLanguage2021,gowdaPullingCausalBootstraps2021,lvCausalityInspiredRepresentation2022,miaoDomainGeneralizationContrastive2022,choiC2LCausallyContrastive2022,liuShowDeconfoundTell2022,rohekarCausalInterpretationSelfAttention2023}, we employ causal intervention $P_{t}(Y | \text{do}(X))$ to cut off the pathway of $D\rightarrow X$ at timestamp $t$ in drift data stream as illustrated in Fig.\ref{fig:intervention}, where $\text{do}(\cdot)$ denotes the interventional operation \cite{pearl2016causal, pearl2000models}.
In accordance with the backdoor adjustment, the de-confounding operation  $\text{do}(X)$ involves stratifying the confounder into discrete segments $D = \{d_1, d_2, \ldots, d_{|D|}\}$, thereby rendering $D$ no longer a confounder between$X$ and $Y$. Thus, $P_{t}(Y | \text{do}(X))$ can be expressed as:
% \begin{equation}
% \label{eq.do}
%     P_{t}(Y | \text{do}(X)) = \sum_{i}^{|D|} P_{t}(Y | X, D = d_{i}) P_{t}(D= d_{i})
% \end{equation}
\begin{equation}
\label{eq.do}
    P_{t}(Y | \text{do}(X)) = \sum_{i}^{|D|}P_{t}(Y|X, B=h(X,d_{i}))P_{t}(d_{i})
\end{equation}

In Eq. \eqref{eq.do}, $P_{t}(Y | \text{do}(X))$ compels $X$ to equally consider each confounding factor $d_{i}$ and integrate them collectively to predict $Y$. Through this approach, the original classifier $P_{t}(Y|X)$ is replaced by $P_{t}(Y | \text{do}(X))$ at the iteration $t$, as shown:  
\begin{equation}
    P_{t}(Y | \text{do}(X)) \triangleq P_{t}(Y|X)
\end{equation}
The de-confound classifier mitigates the confounding influence and captures the genuine causality from $X$ to $Y$, thereby enhancing the quality of the contrastive pre-training. Nevertheless, Eq. \eqref{eq.do} necessitates costly sampling to approximate $P(Y | \text{do}(X))$ when implementing it in contrastive pre-training, resulting in impractical training times. Fortunately, the Normalized Weighted Geometric Mean (NWGM) \cite{xuShowAttendTell2016, liuShowDeconfoundTell2022,yangDeconfoundedImageCaptioning2023} explores an alternative to approximate Eq. \eqref{eq.do} in a single feed-forward process as following:
\begin{equation}
\label{eq:inv}
    P_{t}(Y | \text{do}(X)) \approx P_{t}(Y | X, B =\sum_{i}^{|D|}  h(X,d_{i}) P_{t}(d_{i}))
\end{equation}
As a result, according to the interventional probability outlined in Eq. \eqref{eq:inv}, contrastive pre-training is compelled to grasp the genuine causal effect: $X \rightarrow Y$ rather than the misleading correlations induced by the concept drift confounder $D$. Thus, combined with Eq. \eqref{eq:max}, the final interventional contrastive pre-training objective is formulated as:
\begin{equation}
\label{eq:obj}
\begin{aligned}
     L(g(\tilde{x}^{i}), m(\hat{x}^{i})) = P_{t}(Y | X, \sum_{i}^{|D|}  h(X,d_{i}) P_{t}(d_{i})) \\
     = P_{t}(Y|g(\tilde{x}^{i}), \text{Softmax}(h(g(\tilde{x}^{i}) m(\hat{x}^{i}) )))
\end{aligned}     
\end{equation}


\subsection{Resilient Contrastive Pre-training}

Upon achieving the causal interventional objective for contrastive learning, we devise a resilient contrastive pre-training approach to accommodate the data stream of concept drift, as depicted in Fig. \ref{fig:workflow}. 


\begin{figure}[htb]
    \centering
    \includegraphics[width=0.45\textwidth]{./images/workflow.pdf}
    \caption{The workflow of our causal contrastive pre-training under concept drift streaming. Within the data streaming, a large batch size is opted for a wider drift adaptation window sliding to adapt changes in data distribution. Undergoes various random augmentations, the transformed instances from the identical sample are feature-extracted by both the encoder and the momentum encoder to get the key and value, respectively. An MLP head is utilized to obtain the query of the encoder features. Subsequently, causal intervention is utilized to alleviate concept drift in the data stream within the adaptation window, resulting in the acquisition of two objects for contrastive learning.}
    \label{fig:workflow}
\end{figure}


Following the student-teacher architecture of MoCo v3 \cite{chenEmpiricalStudyTraining2021}, an encoder and a momentum encoder are utilized for causal interventional contrastive pre-training. Within the drift adaption window $[t, t+\tau]$ of the data stream, two distinct crops of each image are utilized under random data augmentation to produce two contrasting views for comparison. The encoder extracts the image feature $v=g(\tilde{x}^{i})$, while the momentum encoder is leveraged for the stability of training as the teacher network, generating the features $k=m(\hat{x}^{i})$. The momentum encoder has the same structure as the encoder, driven by the exponential moving average (EMA) as Eq.\eqref{eq:ema}. Obtained the image feature $v$, an MLP head produce the projection of the encoder feature as $q=h(g(\tilde{x}^{i}))$. Consequently, the intervention module is employed to eliminate latent biases based on features $q$, $v$ and $k$ within the concept drift data stream, and create the contrastive objective $c=\text{Softmax}(q\cdot k)\cdot v$ as Eq. \eqref{eq:obj}. In particular, we use cross-fusion to estimate the concept drift in the data stream, i.e. $q$ and $k$ are obtained from different views, helping the stability of the pre-training. Finally, the InfoNCE loss \cite{oordRepresentationLearningContrastive2019} is adopted to drive the whole network as follows:
\begin{equation}
\label{eq:infoNCE}
    \mathcal{L} = -\log \frac{\exp{(c_{1}\cdot c_{2}^{i+}/\tau)}}{\sum_{i=t}^{t+\tau} \exp{(c_{1}\cdot c_{2}^{i}/\tau)}}
\end{equation}
where $\tau$ is a temperature hyper-parameter \cite{wuUnsupervisedFeatureLearning2018a}. The sum is over one positive and all negative samples within the drift adaptation window $[t,t+\tau]$ of the data stream.


Intuitively, $q$ akin to a "query" \cite{heMomentumContrastUnsupervised2020}, the momentum output $k$ acts as the "key" for sampling key drift of the data stream, and the "value" $v$ resembles a blend of the essential contrastive object and the concept drift. Therefore, the goal of causal intervention is to mitigate the bias of the contrastive objective through the drift sampling within the adaption window. We find the derived causal intervention module has a similar structure to the self-attention mechanism \cite{vaswaniAttentionAllYou2017}. Thus, it is argued that the causal intervention contrastive learning has the ability to capture long-sequence relationships similar to self-attention, which aids in capturing concept drift within a broader drift adaptation window in contrastive pre-training. However, it is important to highlight that we employ "key" and "query" sampling for concept drift within the data stream to mitigate bias in the "value", which is different from the aim of sequence position modelling in the self-attention mechanism.



\subsection{Simple and Scalable Implementation}

Our implementation of the proposed causal interventional contrastive pre-training is highly efficient and simple, requiring minimal specialized operations. As illustrated in the workflow depicted in Fig. \ref{fig:workflow}, we have made only minor adjustments to the MoCo v3 code \cite{chenEmpiricalStudyTraining2021}. These modifications primarily pertain to the interventional steps following the acquisition of image feature embeddings from the encoder and the momentum encoder. Additionally, we have incorporated the masked contrastive strategy proposed in \cite{yangMaskedImageContrastive2024} to facilitate a large batch size which enables the acquisition of the border drift adaption window.

\begin{table*}[htb]
\caption{Evaluation results of fine-tuning on long-tailed classification tasks with ImageNet-LT \cite{liuLargeScaleLongTailedRecognition2019}  and iNatualist2018  \cite{vanhornINaturalistSpeciesClassification2018a}. The best-performing contrastive pre-training results are highlighted in red. Many, Medium and Few denote the evaluated splits of many-shot ($>$100 training samples), medium-shot (20-100 samples) and few-shot ($<$20 samples). Top-1 accuracy is applied to evaluate the performance of different methods. $\dagger$  denotes methods that are adjusted for the long-tailed fine-tuning. }
\centering
\label{table:ft-lt}
\begin{tabular}{@{}lccccccccc@{}}
\toprule
\multicolumn{1}{c}{}                                                           &                                                    & \multicolumn{4}{c}{ImageNet-LT}                                                                                       & \multicolumn{4}{c}{iNaturalist2018}                                                                                   \\
\multicolumn{1}{c}{\multirow{-2}{*}{Methods}}                                  & \multirow{-2}{*}{Backbones}                        & Many                        & Medium                      & Few                         & All                         & Many                        & Medium                      & Few                         & All                         \\ \midrule
\multicolumn{10}{l}{{\color[HTML]{808080} Training from   scratch}}                                                                                                                                                                                                                                                                                                                 \\ \midrule
{\color[HTML]{808080} cRT   \cite{kangDecouplingRepresentationClassifier2019}} & {\color[HTML]{808080} }                            & {\color[HTML]{808080} 61.8} & {\color[HTML]{808080} 46.2} & {\color[HTML]{808080} 27.3} & {\color[HTML]{808080} 49.6} & {\color[HTML]{808080} 69.0} & {\color[HTML]{808080} 66.0} & {\color[HTML]{808080} 63.2} & {\color[HTML]{808080} 65.2} \\
{\color[HTML]{808080} LWS   \cite{kangDecouplingRepresentationClassifier2019}} & {\color[HTML]{808080} }                            & {\color[HTML]{808080} 60.2} & {\color[HTML]{808080} 47.2} & {\color[HTML]{808080} 30.3} & {\color[HTML]{808080} 49.9} & {\color[HTML]{808080} 65.0} & {\color[HTML]{808080} 66.3} & {\color[HTML]{808080} 65.5} & {\color[HTML]{808080} 65.9} \\
{\color[HTML]{808080} RIDE   \cite{wangLongtailedRecognitionRouting2020}}      & {\color[HTML]{808080} }                            & {\color[HTML]{808080} 68.2} & {\color[HTML]{808080} 53.8} & {\color[HTML]{808080} 36.0} & {\color[HTML]{808080} 56.9} & {\color[HTML]{808080} 70.9} & {\color[HTML]{808080} 72.4} & {\color[HTML]{808080} 73.1} & {\color[HTML]{808080} 72.6} \\
{\color[HTML]{808080} PaCo   \cite{cuiParametricContrastiveLearning2021}}      & \multirow{-4}{*}{{\color[HTML]{808080} ResNet-50}} & {\color[HTML]{808080} 68.2} & {\color[HTML]{808080} 58.7} & {\color[HTML]{808080} 41.0} & {\color[HTML]{808080} 60.0} & {\color[HTML]{808080} 70.3} & {\color[HTML]{808080} 73.2} & {\color[HTML]{808080} 73.6} & {\color[HTML]{808080} 73.2} \\ \midrule
\multicolumn{10}{l}{{\color[HTML]{808080} Masked Image   Modeling Pre-training}}                                                                                                                                                                                                                                                                                                     \\ \midrule
{\color[HTML]{808080} MAE   \cite{heMaskedAutoencodersAre2022}}                & {\color[HTML]{808080} }                            & {\color[HTML]{808080} 74.7} & {\color[HTML]{808080} 48.2} & {\color[HTML]{808080} 19.4} & {\color[HTML]{808080} 54.5} & {\color[HTML]{808080} 79.6} & {\color[HTML]{808080} 70.8} & {\color[HTML]{808080} 65.0} & {\color[HTML]{808080} 69.4} \\
{\color[HTML]{808080} LiVT $\dagger$    \cite{xuLearningImbalancedData2023}}              & \multirow{-2}{*}{{\color[HTML]{808080} ViT-B/16}}  & {\color[HTML]{808080} 73.6} & {\color[HTML]{808080} 56.4} & {\color[HTML]{808080} 41.0} & {\color[HTML]{808080} 60.9} & {\color[HTML]{808080} 78.9} & {\color[HTML]{808080} 76.5} & {\color[HTML]{808080} 74.8} & {\color[HTML]{808080} 76.1} \\ \midrule
\multicolumn{10}{l}{Contrastive   Learning Pre-training}                                                                                                                                                                                                                                                                                                                             \\ \midrule
ViT   \cite{dosovitskiyImageWorth16x162021}                                    &                                                    & 50.5                        & 23.5                        & 6.9                         & 31.6                        & 65.4                        & 55.3                        & 50.9                        & 54.6                        \\
DeiT \cite{touvronDeiTIIIRevenge2022}                                          &                                                    & 70.4                        & 40.9                        & 12.8                        & 48.4                        & 72.9                        & 62.8                        & 55.8                        & 61.0                        \\
BYOL \cite{grillBootstrapYourOwn2020}                                          &                                                    & 30.6                        & 8.6                         & 0.9                         & 15.9                        & 28.4                        & 22.6                        & 24.5                        & 24.0                        \\
DINO   \cite{caronEmergingPropertiesSelfSupervised2021}                        &                                                    & 64.8                        & 35.5                        & 11.3                        & 43.3                        & 76.7                        & 67.1                        & 61.2                        & 65.5                        \\
DINO v2 \cite{zhouDINOWMWorldModels2024}                                       &                                                    & 68.4                        & 36.9                        & 11.7                        & 45.4                        & 71.9                        & 62.7                        & 58.2                        & 61.6                        \\
MoCo v3   \cite{chenEmpiricalStudyTraining2021}                                & \multirow{-6}{*}{ViT-B/16}                         & 70.8                        & 40.7                        & 14.3                        & 48.5                        & 76.6                        & 65.8                        & 62.8                        & 65.6                        \\
\rowcolor[HTML]{D9D9D9} 
Ours                                                                           & \multicolumn{1}{l}{\cellcolor[HTML]{D9D9D9}}       & {\color[HTML]{FE0000} 72.2} & {\color[HTML]{FE0000} 43.1} & {\color[HTML]{FE0000} 16.0} & {\color[HTML]{FE0000} 50.3} & {\color[HTML]{FE0000} 77.7} & {\color[HTML]{FE0000} 68.7} & {\color[HTML]{FE0000} 63.8} & {\color[HTML]{FE0000} 67.5} \\ \bottomrule
\end{tabular}
\end{table*}


\begin{table}[htb]
\caption{Evaluation results of linear probing on long-tailed classification tasks with ImageNet-LT and iNatualist2018. The best-performing models are highlighted in red.}
\centering
\label{table:lb-lt}
    \setlength{\tabcolsep}{0.8mm}{
    \begin{tabular}{@{}lcccc@{}}
\toprule
Methods                                                 & Many                        & Medium                      & Few                         & All                         \\ \midrule
\multicolumn{5}{l}{ImageNet-LT \cite{liuLargeScaleLongTailedRecognition2019}}                                                                                                                                                 \\ \midrule
BYOL   \cite{grillBootstrapYourOwn2020}                 & 8.5                         & 0.5                         & 0.1                         & 3.5                         \\
DINO   \cite{caronEmergingPropertiesSelfSupervised2021} & 47.4                        & 20.5                        & 0.1                         & 27.9                        \\
DINO v2    \cite{zhouDINOWMWorldModels2024}             & 49.4                        & 23.4                        & 5.7                         & 30.8                        \\
MoCo v3    \cite{chenEmpiricalStudyTraining2021}        & 45.8                        & 18.7                        & 0.0                         & 27.1                        \\
\rowcolor[HTML]{D9D9D9} 
Ours                                                    & {\color[HTML]{FF0000} 52.7} & {\color[HTML]{FF0000} 24.0} & {\color[HTML]{FF0000} 7.9}  & {\color[HTML]{FF0000} 32.8} \\ \midrule
\multicolumn{5}{l}{iNaturalist2018  \cite{vanhornINaturalistSpeciesClassification2018a}}                                                                                                                                             \\ \midrule
BYOL   \cite{grillBootstrapYourOwn2020}                 & 7.0                         & 3.1                         & 2.5                         & 3.2                         \\
DINO   \cite{caronEmergingPropertiesSelfSupervised2021} & 43.1                        & 33.5                        & 32.3                        & 34.0                        \\
DINO v2    \cite{zhouDINOWMWorldModels2024}             & 36.3                        & 30.2                        & 30.0                        & 30.7                        \\
MoCo v3    \cite{chenEmpiricalStudyTraining2021}        & 32.9                        & 25.4                        & 22.4                        & 24.9                        \\
\rowcolor[HTML]{D9D9D9} 
Ours                                                    & {\color[HTML]{FF0000} 48.4} & {\color[HTML]{FF0000} 37.0} & {\color[HTML]{FF0000} 35.1} & {\color[HTML]{FF0000} 37.3} \\ \bottomrule
\end{tabular}
    }
\end{table}



Furthermore, in comparison to prior approaches, causal contrastive learning presents exceptional scalability. Its scalability is twofold: firstly, it accommodates the inclusion of numerous drift datasets in the pre-training. Following the scaling law \cite{muennighoffScalingDataConstrainedLanguage2023}, augmenting data correlates with an enhancement in model performance. Causal contrastive pre-training guarantees that model performance is bolstered by increasing data without succumbing to the effects of concept drift. Secondly, it strengthens the stability of larger models during drift pre-training, enabling the pre-training of larger models with more parameters and deeper layers.

\section{Experiments}

In this section, we initially showcase the robust performance of our resilient contrastive pre-training in the downstream task of long-tailed classification under drift pre-training, with results of fine-tuning and linear probing. Subsequently, from the perspective of generalization, we conduct experiments of out-of-distribution (OOD) detection and domain shift, to assess the resilience of the model against gradual drift and sudden drift. Following that, the feature embedding space of pre-trained models is intuitively visualized with degrees, illustrating how resilient contrastive pre-training mitigates the concept drift within the feature space. Finally, we present the scaling ability of our method facing with the drift pre-training. More detailed experimental implementations are given in Appendix \ref{appendix:expdetails}.


\subsection{Navigating Tailed Drift Pre-training}
% Navigating Tailed Drift Pre-training Within Causal Interventional Contrastive Learning

% In a long-tail scenario, data imbalance can lead to a gradual drift during pre-training, resulting in the model being biased towards the head categories while overlooking the distinctive features of the tail categories, consequently causing deviation.

We compare our proposed resilient contrastive pre-training method with other models to explicitly demonstrate its superior performance in tailed drift pre-training. Two large-scale datasets, namely the ImageNet-LT \cite{liuLargeScaleLongTailedRecognition2019} and iNaturalist 2018 \cite{vanhornINaturalistSpeciesClassification2018a} are utilized as source datasets to perform pre-training, fine-tuning and linear probing, respectively. 
To effectively illustrate the efficacy of our resilient contrastive pre-training in mitigating tail drift, we follow the criterion and metrics of long-tailed classification \cite{yangTdistributedSphericalFeature2023} to report the Top-1 accuracy across various splits, including Many split with over 100 training samples, Medium split with 20-100 training samples, and Few split with fewer than 20 training samples. 




In terms of fine-tuning results shown in Table \ref{table:ft-lt}, we compare mainstream contrastive pre-training methods, namely DeiT \cite{touvronDeiTIIIRevenge2022}, BYOL \cite{grillBootstrapYourOwn2020} , DINO  \cite{caronEmergingPropertiesSelfSupervised2021} , DINO v2 \cite{zhouDINOWMWorldModels2024}, MoCo v3 \cite{chenEmpiricalStudyTraining2021}  with the baseline of ViT \cite{dosovitskiyImageWorth16x162021}, and our resilient contrastive learning exhibits superior results beyond other contrastive pre-training methods. Compared to other methods that employ the student-teacher paradigm, such as BYOL, DINO, DINO v2, and MoCo v3, our resilient contrastive pre-training method demonstrates additional performance enhancements on both the Medium and Few splits, especially in the iNaturalist2018 dataset. It is demonstrated that the proposed causal interventional objective significantly alleviates the accumulated bias due to momentum updates of the teacher network. 
% 这里再加的分析几句
Meanwhile, under the concept drift scenario, BYOL without negative sample pairs has the inferior performance of long-tailed classification to the baseline ViT, while it is not noticeable in equilibrium \cite{chenEmpiricalStudyTraining2021}. We argue that, in drift pre-training, the essence of negative sample pairs is similar to the causal interventional objective, taking into account the causal relationship of all drifts for each positive sample like de-confounding operation in Eq. \eqref{eq.do} to delineate the boundary of the feature space. Relying solely on maximizing positive agreement as a pre-training strategy will cause the model to be overwhelmed by the head category, eventually exacerbating concept drift.



Besides, we also provide other fine-tuning results of long-tailed classification under scratch training and masked image modelling pre-training for comprehensive analysis, as shown in Table \ref{table:ft-lt}. It is worth noting that our focus is on the impact of drift environments on contrastive pre-training. Therefore, we exclusively utilized resilient contrastive pre-training solely during the pre-training phase, without making any adjustments to the downstream classification tasks. Consequently, in comparison to other methods, all contrastive pre-training approaches exhibit relatively lower performance.
In the context of the masked image modeling strategy, it is found that contrastive learning is more susceptible to concept drift in the data stream. we attribute it to the fact that contrastive learning relies more heavily on global information within the data flow to construct feature representations. Encountering concept drift within the data stream leads to accumulated deviations in momentum updates of contrastive learning, consequently resulting in degraded performance.




Furthermore, we provide linear probing results of contrastive pre-training methods in Table \ref{table:lb-lt}. It is evidenced that our resilient contrastive learning method outperforms other contrastive pre-training methods, corroborating our main contributions of causal interventional contrastive objective for drift pre-training. It is noteworthy that on the Few split of ImageNet-LT, several contrastive learning methods failed to perform effectively, whereas we attained an accuracy of 7.9\%. It indicates that our approach can accurately construct feature representations for tail categories even in the presence of concept drift.



\begin{table}[htb]
\caption{Evaluation results of domain shift on ImageNet-V2 \cite{rechtImageNetClassifiersGeneralize2019}, ImageNet-Sketch \cite{wangLearningRobustGlobal2019} and ImageNet-R \cite{hendrycksManyFacesRobustness2021}. The best-performing models are highlighted in red. Many, Medium and Few denote the evaluated splits of many-shot ($>$100 training samples), medium-shot (20-100 samples) and few-shot ($<$20 samples). Top-1 and Top-5 accuracy are applied to evaluate the performance of different methods.  We compare our methods with other contrastive pre-training models, including BYOL  \cite{grillBootstrapYourOwn2020}, DINO   \cite{caronEmergingPropertiesSelfSupervised2021},  DINO v2 \cite{zhouDINOWMWorldModels2024} and MoCo v3  \cite{chenEmpiricalStudyTraining2021}. }
\label{table:ds}
\centering
% \setlength{\tabcolsep}{0.6mm}{
\begin{tabular}{@{}lccccc@{}}
\toprule
Methods & Many                        & Medium                      & Few                         & Top-1                       & Top-5                       \\ \midrule
\multicolumn{6}{l}{ImageNet-V2   \cite{rechtImageNetClassifiersGeneralize2019}}                                                                               \\ \midrule
BYOL    & 29.7                        & 7.8                         & 1.0                         & 15.2                        & 31.8                        \\
DINO    & 63.4                        & 33.6                        & 11.6                        & 41.9                        & 65.1                        \\
DINO v2 & 66.6                        & 35.4                        & 11.9                        & 44.0                        & 66.9                        \\
MoCo v3 & 62.9                        & 37.3                        & 12.6                        & 43.1                        & 66.0                        \\
\rowcolor[HTML]{D9D9D9} 
Ours    & {\color[HTML]{FF0000} 69.7} & {\color[HTML]{FF0000} 41.2} & {\color[HTML]{FF0000} 14.9} & {\color[HTML]{FF0000} 48.3} & {\color[HTML]{FF0000} 72.1} \\ \midrule
\multicolumn{6}{l}{ImageNet-Sketch  \cite{wangLearningRobustGlobal2019}}                                                                                      \\ \midrule
BYOL    & 1.9                         & 0.2                         & 0.0                         & 0.8                         & 2.8                         \\
DINO    & 12.4                        & 5.0                         & 0.5                         & 7.1                         & 16.2                        \\
DINO v2 & 17.1                        & 6.8                         & 1.4                         & 9.9                         & 20.8                        \\
MoCo v3 & 17.6                        & 7.6                         & 1.4                         & 10.4                        & 21.9                        \\
\rowcolor[HTML]{D9D9D9} 
Ours    & {\color[HTML]{FF0000} 22.2} & {\color[HTML]{FF0000} 9.7}  & {\color[HTML]{FF0000} 2.3}  & {\color[HTML]{FF0000} 13.4} & {\color[HTML]{FF0000} 27.0} \\ \midrule
\multicolumn{6}{l}{ImageNet-R   \cite{hendrycksManyFacesRobustness2021}}                                                                                      \\ \midrule
BYOL    & 4.2                         & 1.7                         & 0.0                         & 2.7                         & 6.8                         \\
DINO    & 13.4                        & 7.6                         & 0.4                         & 9.5                         & 18.3                        \\
DINO v2 & 15.8                        & 8.6                         & 0.7                         & 11.1                        & 21.1                        \\
MoCo v3 & 17.2                        & 9.8                         & 1.0                         & 12.3                        & 22.8                        \\
\rowcolor[HTML]{D9D9D9} 
Ours    & {\color[HTML]{FF0000} 21.3} & {\color[HTML]{FF0000} 12.5} & {\color[HTML]{FF0000} 1.6}  & {\color[HTML]{FF0000} 15.5} & {\color[HTML]{FF0000} 27.5} \\ \bottomrule
\end{tabular}
% }
\end{table}

\subsection{Taming OOD Drift and Domain Drift in Downstream Tasks}

\begin{table*}[htb]
\caption{Evaluation results of OOD detection with the OOD datasets of Texture \cite{cimpoi2014describing}, iNat-OOD \cite{van2018inaturalist,huangMOSScalingOutofDistribution2021}, ImageNet-O \cite{hendrycksNaturalAdversarialExamples2021} and OpenImage-O \cite{wangViMOutofDistributionVirtualLogit2022}. ViT-B/16 is selected as the image encoder. The best-performing method is highlighted in red. FPR$\downarrow$ and AUROC$\uparrow$ are applied to evaluate the performance of different methods. PT denotes the pre-training dataset, where '-21k' and '-1k' represent ImageNet-21k \cite{ridnikImageNet21KPretrainingMasses2021} and ImageNet-1k \cite{russakovskyImagenetLargeScale2015}, respectively. ID means the in-distribution dataset, where '-LT' symbolizes ImageNet-LT \cite{liuLargeScaleLongTailedRecognition2019}.}
\centering
\label{table:ood}
\setlength{\tabcolsep}{0.7mm}{
\begin{tabular}{@{}lcccccccccccc@{}}
\toprule
                                                        &                         &                        & \multicolumn{2}{c}{Texture}                               & \multicolumn{2}{c}{iNaturalist}                           & \multicolumn{2}{c}{ImageNet-O}                            & \multicolumn{2}{c}{OpenImage-O}                           & \multicolumn{2}{c}{Overall}                               \\
\multirow{-2}{*}{Methods}                               & \multirow{-2}{*}{PT}    & \multirow{-2}{*}{ID}   & AUROC                       & FPR                         & AUROC                       & FPR                         & AUROC                       & FPR                         & AUROC                       & FPR                         & AUROC                       & FPR                         \\ \midrule
MSP   \cite{hendrycks2016baseline}                      &                         &                        & 71.3                        & 77.1                        & 90.7                        & 43.7                        & 60.8                        & 90.6                        & 84.3                        & 61.8                        & 76.8                        & 68.3                        \\
Energy   \cite{liu2020energy}                           &                         &                        & 54.1                        & 86.3                        & 76.6                        & 72.7                        & 61.6                        & 81.0                        & 71.1                        & 74.0                        & 65.9                        & 78.5                        \\
MaxLog   \cite{hendrycks2019scaling}                    &                         &                        & 67.2                        & 78.0                        & 89.9                        & 45.6                        & 61.7                        & 88.6                        & 82.7                        & 62.5                        & 75.4                        & 68.7                        \\
KL   \cite{hendrycks2019scaling}                        &                         &                        & 82.6                        & 67.3                        & 87.6                        & 69.7                        & 66.6                        & 88.2                        & 84.3                        & 74.2                        & 80.3                        & 74.8                        \\
Residual \cite{wang2022vim}                             &                         &                        & 82.4                        & 64.6                        & 73.7                        & 86.0                        & 68.4                        & 87.5                        & 74.9                        & 78.0                        & 74.9                        & 79.0                        \\
React \cite{sun2021react}                               &                         &                        & 62.1                        & 80.5                        & 91.2                        & 38.7                        & 63.7                        & 81.0                        & 80.4                        & 60.4                        & 74.3                        & 65.2                        \\
Mahalanobis  \cite{lee2018simple}                       &                         &                        & 84.9                        & 66.1                        & 84.9                        & 81.6                        & 71.5                        & 88.9                        & 84.2                        & 74.7                        & 81.4                        & 77.8                        \\
ViM \cite{wang2022vim}                                  &                         &                        & 83.5                        & 62.7                        & 77.8                        & 81.7                        & 71.0                        & 86.6                        & 78.3                        & 74.6                        & 77.7                        & 76.4                        \\
MOODv1 \cite{li2023rethinking}                          &                         &                        & 93.0                        & 30.9                        & 98.8                        & 5.9                         & 86.8                        & 63.2                        & 95.5                        & 26.5                        & 93.5                        & 31.6                        \\
MOODv2  \cite{liMOODv2MaskedImage2024}                  & \multirow{-10}{*}{-21K} & \multirow{-10}{*}{-1K} & 94.3                        & 24.7                        & 99.6                        & 1.8                         & 91.5                        & 40.8                        & 97.4                        & 13.6                        & 95.7                        & 20.2                        \\ \midrule
BYOL   \cite{grillBootstrapYourOwn2020}                 &                         &                        & 58.4                        & 92.1                        & 80.1                        & 77.5                        & 52.0                        & 92.2                        & 65.3                        & 81.3                        & 64.0                        & 85.8                        \\
DINO   \cite{caronEmergingPropertiesSelfSupervised2021} &                         &                        & 72.9                        & 80.7                        & 85.7                        & {\color[HTML]{FF0000} 65.3} & {\color[HTML]{FF0000} 74.2} & {\color[HTML]{FF0000} 80.2} & 74.7                        & 83.0                        & 76.9                        & 77.3                        \\
DINO v2    \cite{zhouDINOWMWorldModels2024}             &                         &                        & 67.1                        & 85.1                        & 78.9                        & 88.4                        & 63.1                        & 88.3                        & 73.9                        & 85.4                        & 70.7                        & 86.8                        \\
MoCo v3    \cite{chenEmpiricalStudyTraining2021}        & \multirow{-4}{*}{-LT}   & \multirow{-4}{*}{-LT}  & 81.5                        & 63.9                        & 80.1                        & 68.6                        & 65.0                        & 91.0                        & 69.9                        & 84.2                        & 74.1                        & 76.9                        \\
\rowcolor[HTML]{D9D9D9} 
Ours                                                    &                         &                        & {\color[HTML]{FF0000} 83.1} & {\color[HTML]{FF0000} 57.8} & {\color[HTML]{FF0000} 88.0} & 67.7                        & 70.1                        & 82.9                        & {\color[HTML]{FF0000} 75.5} & {\color[HTML]{FF0000} 78.3} & {\color[HTML]{FF0000} 79.2} & {\color[HTML]{FF0000} 71.7} \\ \bottomrule
\end{tabular}
}
\end{table*}



Beyond that, to validate the generalization capability of our resilient contrastive pre-training under concept drift scenarios, we conducted experiments from perspectives of domain shift and out-of-distribution (OOD) detection.

In terms of domain shift generalization as exhibited in Table \ref{table:ds}, our primary focus is to assess the extent of the drift environment on the feature representation constructed during pre-training. Therefore, we select three subsets of ImageNet \cite{russakovskyImagenetLargeScale2015} to validate the domain shift 
 performance, namely, ImageNet-V2 \cite{rechtImageNetClassifiersGeneralize2019}, ImageNet-Sketch \cite{wangLearningRobustGlobal2019} and ImageNet-R \cite{hendrycksManyFacesRobustness2021}. Our results achieve superior results across three datasets, investigating the effectiveness of the proposed resilient contrastive pre-training method. It corroborates that, the feature space we construct in pre-training abstracts and captures the essential information of images, while disregarding any unexpected interference from the concept drift within the data stream.
Moreover, judging from the results of Few split in ImageNet-Sketch and ImageNet-R, the majority of contrastive pre-training methods struggle to differentiate the tail categories. This difficulty arises from two main challenges. First, the bias induced by tail drift tends to skew models toward favoring head categories. On the flip side, the small sample size of tail categories limits the ability of pre-training methods to accurately extract their fundamental features, a problem that is exacerbated in the presence of domain shifts. Accordingly, our outstanding results demonstrate our proficiency in mitigating tail drift and extracting crucial features from limited samples.

Concerning out-of-distribution (OOD) detection, we also evaluate the capability of our resilient contrastive pre-training approach to delineate the boundaries of the feature space. While exhibiting slightly lower performance than DINO on ImageNet-O, we significantly outperform contrastive pre-training methods on the other three out-of-distribution datasets, and surpasses other approaches in overall performance. It demonstrates that our model possesses the benefits of intra-class compactness and inter-class separability in feature representation. 
Meanwhile, it is worth noting that in larger-scale balanced pre-training datasets, such as ImageNet-21k \cite{ridnikImageNet21KPretrainingMasses2021}, our approach outperforms numerous traditional OOD detection methods in overall performance, such as ViM \cite{wang2022vim}. It underscores our resilient contrastive pre-training methods can effectively alleviate concept drift bias in the model while also enhancing the characterization of the model's decision boundary.


% Meanwhile, in more large-scale balanced pre-training datasets, like ImageNet-21k, our approach outperforms many traditional OOD detection methods such as ViM and Mah. This illustrates that our adaptable comparison pre-training technique can effectively mitigate concept drift bias in the model while also enhancing the characterization of the model's decision boundary.



\subsection{Boosting Pre-training Feature Embedding}


To directly and intuitively demonstrate the feature space of the pre-trained model, we quantitatively calculate the feature embedding distances from three perspectives: In-Distribution (ID) intra-class compactness, ID inter-class separability, and the separability between ID and OOD categories, as shown in Table \ref{table:pt}.
Intra-class compactness within the In-Distribution (ID) measures the average distance between the category center and samples. ID Inter-class separability evaluates the distances between centers of different categories. Lastly, we assess the separability between ID categories and OOD samples.




\begin{table}[htb]
\caption{Evaluation results of different contrastive learning strategies in the stage of pre-training, from three perspectives: ID intra-class compactness, ID inter-class separability and the separability between ID and OOD categories.     
The cosine metric is utilized to measure these distances, which is expressed as average degrees.  We compare our methods with other contrastive pre-training models, including BYOL, DINO,  DINO v2 and MoCo v3. ImageNet-LT is utilized for pre-training. The best-performing models are highlighted in red.}
\label{table:pt}
\centering
\setlength{\tabcolsep}{0.7mm}{
\begin{tabular}{@{}lcccc@{}}
\toprule
Pre-training                                            & Many & Medium & Few  & All  \\ \midrule
\multicolumn{5}{l}{ID Intra-class   Compactness $\downarrow$}                                      \\ \midrule
BYOL   \cite{grillBootstrapYourOwn2020}                 & 53.1 & 59.1   & 59.9 & 56.9 \\
DINO   \cite{caronEmergingPropertiesSelfSupervised2021} & 34.2 & 51.4   & 59.0 & 45.9 \\
DINO v2    \cite{zhouDINOWMWorldModels2024}             & 31.2 & 50.1   & 57.8 & 43.9 \\
MoCo v3    \cite{chenEmpiricalStudyTraining2021}        & 29.1 & 47.8   & 57.8 & 42.8 \\
\rowcolor[HTML]{D9D9D9} 
Ours                                                    &  {\color[HTML]{FF0000} 28.5}  & {\color[HTML]{FF0000} 46.5}   &{\color[HTML]{FF0000} 55.8} & {\color[HTML]{FF0000} 40.9}  \\ \midrule
\multicolumn{5}{l}{ID Inter-class   Separability $\uparrow$}                                     \\ \midrule
BYOL   \cite{grillBootstrapYourOwn2020}                 & 81.5 & 79.2   & 78.3 & 80.0 \\
DINO   \cite{caronEmergingPropertiesSelfSupervised2021} & 89.0 & 88.4   & 87.6 & 88.5 \\
DINO v2    \cite{zhouDINOWMWorldModels2024}             & 88.9 & 88.1   & 87.2 & 88.3 \\
MoCo v3    \cite{chenEmpiricalStudyTraining2021}        & 89.3 & 88.8   & 88.2 & 88.9 \\
\rowcolor[HTML]{D9D9D9} 
% Ours                                                    & 89.4 & 89.0   & 88.4 & 89.1 \\ \midrule
Ours                                                    &  {\color[HTML]{FF0000} 89.4}  & {\color[HTML]{FF0000} 89.0}   &{\color[HTML]{FF0000} 88.4} & {\color[HTML]{FF0000} 89.1}  \\ \midrule
\multicolumn{5}{l}{ID vs. OOD   Separability $\uparrow$}                                         \\ \midrule
BYOL   \cite{grillBootstrapYourOwn2020}                 & 75.7 & 71.9   & 70.3 & 73.1 \\
DINO   \cite{caronEmergingPropertiesSelfSupervised2021} & 86.2 & 84.4   & 81.4 & 84.7 \\
DINO v2    \cite{zhouDINOWMWorldModels2024}             & 86.0 & 83.7   & 80.5 & 84.1 \\
MoCo v3    \cite{chenEmpiricalStudyTraining2021}        & 86.9 & 85.7   & 83.0 & 85.8 \\
\rowcolor[HTML]{D9D9D9} 
% Ours                                                    & 87.1 & 86.1   & 83.4 & 86.1 \\ \bottomrule
Ours                                                    &  {\color[HTML]{FF0000} 87.1}  & {\color[HTML]{FF0000} 86.1}   &{\color[HTML]{FF0000} 83.4} & {\color[HTML]{FF0000} 86.1}  \\ \bottomrule
\end{tabular}
}
\end{table}


The intra-class compactness results demonstrate that our model effectively validates the efficacy of the proposed resilient contrastive pre-training in delineating feature boundaries, especially in long-tailed scenarios. Moreover, the inter-class separability among different splits is very similar, suggesting that the pre-training primarily influences intra-class compactness rather than inter-class separability.
Furthermore, with the exception of BYOL, the inter-class separability of the other contrastive pre-training methods is nearly the same. This observation underscores the significance of negative samples in expanding inter-class separation.
It also demonstrates that the bias is induced by accumulated drift within momentum updates, where features of head categories dominated the whole model under tailed drift. In the context of ID vs. OOD separability, we attained the optimal results, signifying that our feature space exhibits distinct feature boundaries. 
In addition, we find that the performance of ID vs. OOD separability aligns with the ID intra-class compactness, suggesting that enhancing the model's performance hinges on mitigating data stream drift to enhance feature extraction capabilities.




\subsection{Scaling with Drift Pre-training}



\begin{table}[htb]
\caption{Evaluation results of scaling ability of our resilient contrastive pre-training on ImageNet-LT with ViT-Small/16, ViT-Base/16 and ViT-Large/16. Top-1 accuracy is utilized to evaluate the performance.}
\label{table:scale}
\centering
\begin{tabular}{@{}lcccc@{}}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{Backbones}} & \multicolumn{4}{c}{ImageNet-LT}                                                           \\
\multicolumn{1}{c}{}                           & Many                 & Medium               & Few                  & All                  \\ \midrule
ViT-S/16  & 67.7 & 39.6 & 11.6 & 46.4 \\
ViT-B/16  & 72.2 & 43.1 & 16.0 & 50.3 \\
ViT-L/16  & 73.8 & 45.0 & 16.9 & 52.0 \\ \bottomrule
\end{tabular}
\end{table}

To showcase the scalability of our resilient contrastive pre-training approach in scenarios involving concept drift, we evaluate the efficiency and scalability of our model in Table \ref{table:scale}. As the number of parameters in ViT increases, our model exhibits a scalable effect, leading to performance improvements with larger models in downstream long-tailed classification tasks. In comparison to ViT-S/16, our proposed resilient contrastive learning method achieves nearly a 4\% enhancement with ViT-B/16. This suggests that we can effectively train larger models to attain superior performance in the face of concept drift within data streams.


More critically, our experiments reveal that pre-trained models exhibit consistent scaling behavior even on datasets with great distribution drift. It demonstrates that our proposed resilient contrastive pre-training can effectively leverage larger-scale, minimally curated datasets, thereby substantially expanding the pool of usable data for pre-training without relying on complicated cleaning procedures.



\section{Conclusion and Outlooks}

In this paper, we present resilient contrastive pre-training, a novel, straightforward and effective pre-training paradigm tailored for concept drift data stream. We employ causal inference theory to methodically examine the source of bias in the momentum update of contrastive pre-training and put forward a causal interventional objective to mitigate this bias within the drifting data stream. By virtue of this objective, resilient contrastive pre-training is devised to counteract the unpredictable distribution changes occurring within the data stream.


We hope that our work will inspire future advancements in contrastive pre-training paradigm, specifically addressing biases originating from real-world data challenges. In future research, we will leverage causal inference to analyze mixure of concept drift in pre-training.


\bibliographystyle{unsrt}  
\bibliography{example_paper}  %%% Remove comment to use the external .bib file (using bibtex).
%%% and comment out the ``thebibliography'' section.


\newpage
\appendix
\onecolumn


\section{Related Works}
\label{appendix:relatedwork}

\subsection{Concept Drift}

In the comprehensive survey conducted by Lu et al. \cite{luLearningConceptDrift2019, lu2020data}, existing approaches for concept drift handling are systematically classified into three primary categories: error rate-based methods \cite{wangSelfadaptiveEnsembleUser2024, jiaoDynamicEnsembleSelection2024}, data distribution-based methods \cite{yang2024adaptingmultimodallargelanguage,cerqueiraSTUDDStudentTeacher2023}, and multiple hypothesis-based techniques \cite{yu2024online, yuLearnadaptConceptDrift2022}. Our proposed methodology falls under the category of distribution-based concept drift adaptation approaches. These distribution-driven techniques distinguish themselves by not only enabling precise drift identification through explicit statistical distribution analysis but also providing a multidimensional characterization of drift patterns - including temporal occurrence detection, affected feature space localization, and quantitative severity assessment. This dual capability of detection coupled with comprehensive drift diagnostics establishes distribution-based methods as particularly valuable for developing adaptive systems that require interpretable drift understanding and targeted model adjustments.


Besides, Online Boosting Adaptive Learning (OBAL) method \cite{yu2024online} is proposed to address the challenges of concept drift and negative transfer in multistream classification. OBAL employs a dual-phase approach: an initial model is built using the Adaptive Covariate Shift Adaptation (AdaCOSA) algorithm to handle covariate shifts and learn dynamic correlations among streams. In the online phase, a Gaussian Mixture Model-based weighting mechanism is integrated to manage asynchronous drift. Meanwhile, CDMLLM \cite{yang2024adaptingmultimodallargelanguage} reveals that vision-language models suffer significant bias from concept drift during pre-training and fine-tuning. To address this, the authors propose a unified concept drift framework integrating T-distribution-based adaptation for long-tailed calibration and explicit OOD detection, demonstrating enhanced robustness in open-world multi-modal alignment through systematic distribution modelling. Beyond on drift detection in single data stream, GDDM \cite{yuDetectingGroupConcept2023} focuses on addressing group concept drift across multiple data streams, where individual drifts may go undetected due to subtle changes in underlying distributions. The proposed method introduces a distribution-free test statistic to detect concept drift in these complex scenarios. By designing an online learning algorithm for streaming data, the approach accurately identifies concept drift caused by hypothesis testing. Beyond that, DDG-DA \cite{liDDGDataDistributionGeneration2022} proactively models predictable factors influencing environmental evolution. The approach involves training a predictor to estimate future data distribution, using this information to generate training samples, and then training models on the generated data. By leveraging predictable factors to forecast data distribution changes, DDG-DA aims to enhance model performance in handling concept drift in streaming data. Furthermore, STUDD\cite{cerqueiraSTUDDStudentTeacher2023} proposes a teacher-student paradigm to enable unsupervised drift detection through deviation analysis of their predictive consistency. This approach leverages model disagreement as a proxy signal, bypassing dependency on ground-truth labels during deployment while maintaining detection sensitivity.

\subsection{Causal Inference}

Recently, increasing researchers have incorporated causal inference into deep-learning models, especially in large models. Deconfounded Image Captioning (DIC) \cite{yangDeconfoundedImageCaptioning2023} is proposed to address dataset bias in vision-language models through a causal lens, that integrates backdoor and front-door adjustments for systematic bias mitigation. The framework provides principled causal analysis of spurious correlations in multimodal alignment, offering theoretical grounding for decomposing bias sources through structured interventions. Likewise, aiming for spurious correlations induced by visual and linguistic biases during training, CIIC \cite{liuShowDeconfoundTell2022} is proposed as a causal intervention framework combining an Interventional Object Detector (IOD) and Interventional Transformer Decoder (ITD) guided by structural causal models. By applying backdoor adjustment through IOD's feature disentanglement and ITD's dual de-confounding mechanism, their approach systematically mitigates confounding effects across encoding and decoding stages, demonstrating enhanced generalization through causal correlation modeling. Similarly, targeting multi-hop fact verification bias in the large language model, Causal Walk \cite{zhangCausalWalkDebiasing2024} is proposed, a front-door adjustment framework that disentangles complex spurious correlations in evidence chains. 
The method models reasoning paths as mediators in structural causal models, decomposing causal effects via random walk-based treatment-mediator estimation and geometric mean-based mediator-outcome approximation. By integrating adversarial and symmetric datasets synthesized with large language models, the approach demonstrates superior debiasing performance. 

Additionally, causal inference is widely used in representation learning. Comprehensive Interventional Distillation (CID) \cite{dengComprehensiveKnowledgeDistillation2021} integrates causal intervention with class-aware representation alignment. By reinterpreting teacher logits as contextual confounders and applying counterfactual pruning through structural causal models, CID systematically disentangles beneficial semantic patterns from dataset-specific biases. This approach demonstrates enhanced generalization through bias-invariant knowledge transfer. Besides, De-confound-TDE \cite{tangLongTailedClassificationKeeping2021} establishes a causal framework for long-tailed classification, identifying SGD momentum as a paradoxical confounder that simultaneously harms tail-class predictions while benefiting representation learning. Through causal intervention during training and counterfactual reasoning at inference, the method disentangles momentum’s detrimental bias from its beneficial mediation effects. Meanwhile, CCM \cite{miaoDomainGeneralizationContrastive2022} is proposed to address domain generalization through causal invariance principles. The framework integrates front-door adjustment with contrastive learning to quantify stable causal effects across domains, explicitly modeling domain shifts via a three-stage process: domain-conditioned supervision for feature correlation, causal effect measurement through structured path manipulation, and contrastive clustering for class-consistent representations. Similarly, CIRL \cite{lvCausalityInspiredRepresentation2022} advances domain generalization through causal factorization, proposing a structural causal model that decomposes inputs into invariant causal mechanisms and domain-specific non-causal factors. It enforces three critical properties: causal/non-causal separation, joint independence, and causal sufficiency for classification. 

Besides, C2L \cite{choiC2LCausallyContrastive2022} addresses model fragility to spurious patterns through contrastive counterfactual synthesis, proposing a collective decision framework that aggregates predictions across generated counterfactual sets. Unlike conventional augmentation limited by dataset-inherent biases, this approach probabilistically supervises causal invariance through distributional consensus, demonstrating enhanced robustness against attribution bias and domain shifts. Furthermore, ABCD \cite{rohekarCausalInterpretationSelfAttention2023} establishes a causal interpretation of Transformer self-attention mechanisms, modeling them as structural equation estimators that capture conditional independence relations through partial correlation analysis in deep attention layers. This framework enables zero-shot causal discovery over input sequences while accounting for latent confounders, effectively repurposing pre-trained models for causal graph inference. What’s more, Causal Attention (CATT) \cite{yangCausalAttentionVisionLanguage2021} implements front-door adjustment to address confounding bias in attention mechanisms via dual-path processing of In-Sample and Cross-Sample Attention. By forcibly integrating external sample contexts through CS-ATT while maintaining standard attention conventions, CATT dynamically mitigates spurious correlations without requiring confounder specification. 

\subsection{Contrastive Pre-training}

The seminal work of SimCLR \cite{chenSimpleFrameworkContrastive2020} establishes foundational principles for contrastive pretraining through instance discrimination objectives and systematic augmentation strategies. While achieving remarkable performance in static environments, its reliance on fixed augmentation policies and stationary negative sampling assumes temporal consistency of latent patterns – an assumption violated under concept drift scenarios. Nevertheless, SimCLR's demonstration of invariant representation learning through normalized temperature-scaled loss provides crucial architectural groundwork for developing drift-aware contrastive frameworks, particularly in modeling evolving feature relationships through dynamic positive/negative pair formulation.

Besides, the MoCo framework \cite{heMomentumContrastUnsupervised2020} and its subsequent evolution MoCo v3 \cite{chenEmpiricalStudyTraining2021} establish critical momentum-based mechanisms for contrastive learning through dynamic memory banks and stabilized key encoder updates. It is primarily designed to maintain consistency in negative sample maintenance and gradient stability. However, their fixed momentum schedules and stationary target assumptions limit adaptability to abrupt distribution shifts as we discussed aforementioned. Likewise, BYOL \cite{grillBootstrapYourOwn2020} employs two neural networks—an online network and a target network—that interact and learn from each other. Specifically, the online network is trained to predict the representation of an augmented image view produced by the target network, while the target network is updated using a slow-moving average of the online network's parameters. Similarly, the SiamSim framework \cite{chenExploringSimpleSiamese2021} advances contrastive representation learning through dynamic similarity calibration, employing a dual-path Siamese architecture with momentum-aligned feature projectors. 

Meanwhile, DINO \cite{caronEmergingPropertiesSelfSupervised2021} and DINO v2 \cite{zhouDINOWMWorldModels2024} advance self-distillation paradigms through momentum-based teacher-student mechanisms, leveraging global-local attention consistency in Vision Transformers to learn semantically structured representations. While achieving state-of-the-art in static self-supervised learning, their fixed teacher-student update schedules and monolithic prototype banks implicitly assume stationarity of feature distributions. DINO v2's introduction of partitioned expertise through specialized sub-networks demonstrates partial adaptability to data variations, suggesting pathways for concept drift mitigation through dynamic expert routing. These works collectively establish critical baselines for stability-aware distillation architectures in non-stationary pre-training scenarios.

Furthermore, DenseCL \cite{wangDenseContrastiveLearning2021a}  advances contrastive learning through localized feature alignment, introducing dense instance discrimination that operates at both image-level and pixel-level granularity. By enforcing spatial consistency through region-to-region contrastive pretext tasks, the method enhances model sensitivity to fine-grained visual patterns. While primarily designed for dense prediction tasks, its hierarchical contrast mechanism demonstrates the importance of multi-scale feature stabilization.

While contemporary contrastive pretraining methods achieve remarkable performance under static data distributions, their reliance on closed-world stationarity assumptions presents a fundamental limitation: they inherently lack mechanisms for concept drift adaptation when deployed in non-stationary environments with evolving data streams.

\section{Experiments}

In this section, we first introduce the details of utilized datasets for pre-training and various downstream tasks. Subsequently, implementation details are provided. 

\subsection{Datasets}

ImageNet-LT \cite{liuLargeScaleLongTailedRecognition2019} is a long-tailed subset derived from the ImageNet \cite{russakovskyImagenetLargeScale2015} dataset, designed to benchmark machine learning models under realistic class imbalance. It contains 115k training images across 1,000 categories, with class frequencies ranging from 1,280 (head classes) to merely 5 (tailed classes), simulating real-world data skewness. The validation set is balanced (20 images per class), while the test set aligns with the standard ImageNet validation data (50,000 images). It serves as a pivotal resource for studying long-tail recognition challenges, including few-shot learning, open-set generalization, and algorithmic fairness, while retaining compatibility with the original ImageNet hierarchy and evaluation protocols.


iNaturalist2018 \cite{vanhornINaturalistSpeciesClassification2018a} is a large-scale dataset designed to study fine-grained recognition under naturally occurring long-tailed class distributions. It comprises 675k training and validation images spanning 5,089 species hierarchically organized into 13 super-categories, with severe imbalance reflecting ecological rarity. The dataset integrates taxonomic metadata, geospatial coordinates, and temporal annotations, supporting research in hierarchical learning, transfer learning, and self-supervised pretraining. 

% OOD

In terms of OOD detection, the Texture (DTD) \cite{cimpoi2014describing} is a comprehensive collection of 5,640 texture images, meticulously organized into 47 categories based on human-centric perceptual attributes. Designed to support texture analysis and recognition tasks, DTD is divided into training, validation, and test sets, each containing 40 images per category. Besides, the iNat-OOD dataset \cite{huangMOSScalingOutofDistribution2021} is a specialized subset of the iNaturalist  \cite{vanhornINaturalistSpeciesClassification2018a} dataset, designed for evaluating out-of-distribution (OOD) detection methods in large-scale image classification tasks. The dataset is particularly valuable for testing the robustness of models in identifying novel or unseen categories, especially in ecological and biodiversity contexts. Meanwhile, ImageNet-O \cite{hendrycksNaturalAdversarialExamples2021}  is a specialized dataset designed to evaluate the robustness of visual models in detecting out-of-distribution (OOD) samples. It consists of 2,000 images from classes not included in the standard ImageNet-1k \cite{russakovskyImagenetLargeScale2015}  dataset, making it a valuable benchmark for testing OOD detection methods. Additionally, OpenImage-O \cite{wangViMOutofDistributionVirtualLogit2022}is a large-scale, manually annotated dataset designed for out-of-distribution (OOD) detection tasks. It is derived from the OpenImage-V3 test set, which contains a diverse and natural distribution of images collected from Flickr without predefined class names or tags. This dataset aims to overcome the limitations of existing OOD benchmarks by providing a more realistic and challenging testbed for evaluating the robustness of computer vision models

% domain shift

Regarding domain shift, ImageNet-V2 \cite{rechtImageNetClassifiersGeneralize2019} is a comprehensive test set designed to evaluate the robustness and generalization of image classification models. It comprises 10,000 images, with 10 images per class, closely following the original labeling protocol of ImageNet. This dataset is instrumental in assessing how well models trained on the original ImageNet dataset can generalize to new, unseen data, making it a valuable resource for advancing computer vision research. The ImageNet-Sketch \cite{wangLearningRobustGlobal2019} is a unique collection of 50,000 hand-drawn sketch images, with 50 images for each of the 1,000 ImageNet classes. Constructed using Google Image queries, this dataset is designed to evaluate models' ability to learn out-of-domain semantics at the ImageNet scale. ImageNet-R \cite{hendrycksManyFacesRobustness2021} is a collection of 30k images representing 200 ImageNet \cite{russakovskyImagenetLargeScale2015}  classes. These images are artistic renditions, including art, cartoons, graffiti, embroidery, origami, paintings, and other forms of creative expressions. The dataset is designed to test the robustness and generalization capabilities of image classification models when faced with non-standard inputs .


\subsection{Implementation Details}
\label{appendix:expdetails}
We employ ViT-Small, ViT-Base and ViT-Large as our visual backbones, respectively. Among them, Vit-Base consists of 12 transformer encoder layers and an FFN intermediate size of 3,072. The hidden dimensions of the ViT-Base are 768, with 12 attention heads. The number of parameters is about 86 million. The input image size is set to $224\times 224$. For ViT-Small, ViT-S/16 comprises 12 transformer encoder layers with an FFN intermediate size of 1,536. The input image resolution is maintained at $224\times 224$, utilizing a patch size of $16\times 16$. The hidden dimension of ViT-Small is 384, featuring 6 parallel attention heads. The total parameter count approximates 22 million. In terms of the ViT-Large, ViT-L/16 consists of 24 transformer encoder layers and an FFN intermediate size of 4,096. The input image size is set to $224\times 224$, with a patch size of $16\times 16$. The hidden dimensions of the ViT-Large are 1,024, with 16 attention heads. And, the number of parameters is about 307 million. 


\begin{table}[htbp]
    \caption{The pre-training hyperparameters.}
    \label{table:pretrain}
    \centering
    \setlength{\tabcolsep}{3mm}{
        \begin{tabular}{@{}lccc@{}}
        \toprule
                            & ViT-S/16    & ViT-B/16    & ViT-L/16    \\ \midrule
        Training Epochs     & 800         & 800         & 800         \\
        Warmup Epochs       & 40          & 40          & 40          \\
        Optimizer           & AdamW       & AdamW       & AdamW       \\
        Base Learning Rate  & 1.5e-4      & 1.5e-4      & 1.5e-4      \\
        Learning Rate Decay & Cosine      & Cosine      & Cosine      \\
        Adam $\beta$        & (0.9, 0.95) & (0.9, 0.95) & (0.9, 0.95) \\
        Weight Decay        & 0.05        & 0.05        & 0.05        \\
        Eff. Batch Size     & 32,000      & 9,600       & 2,400       \\ \bottomrule
        \end{tabular}
        }    
\end{table}



In terms of the pre-training progress, the hyperparameters are presented in Table \ref{table:pretrain}. 
We utilize the AdamW optimizer, which is configured with a cosine annealing schedule as the learning policy. The initial learning rate is set to $2\times10^{-5}$, and the AdamW optimizer is employed with hyperparameters $\beta= (0.9, 0.98)$. Additionally, we set the weight decay to 0.05 and the dropout rate to 0.1. During the first 40 warm-up epochs, the learning rate increases to $1.5\times10^{-4}$, and subsequently decays to $10^{-7}$. Unless otherwise specified, the pre-training of our resilient contrastive model consists of 800, executed on $2\times 2$ NVIDIA A100 GPUs. 



% \begin{table}[htb]
%     \caption{The fine-tuning hyperparameters.}
%     \label{table:finetune}
%     \centering
%     \setlength{\tabcolsep}{3mm}{
%         \begin{tabular}{@{}lccc@{}}
%         \toprule
%                             & ViT-S/16    & ViT-B/16    & ViT-L/16    \\ \midrule
%         Training Epochs     & 100         & 100         & 50          \\
%         Warmup Epochs       & 5           & 5           & 5           \\
%         Optimizer           & AdamW       & AdamW       & AdamW       \\
%         Base Learning Rate  & 5e-4        & 5e-4        & 1e-3        \\
%         Learning Rate Decay & Cosine      & Cosine      & Cosine      \\
%         Adam $\beta$        & (0.9, 0.95) & (0.9, 0.95) & (0.9, 0.95) \\
%         Weight Decay        & 0.05        & 0.05        & 0.05        \\
%         Eff. Batch Size     & 1,024       & 1,024       & 1,024       \\ \bottomrule
%         \end{tabular}
%         }    
% \end{table}
% \begin{table}[htb]
%     \caption{The linear probing hyperparameters.}
%     \label{table:lin}
%     \centering
%     \setlength{\tabcolsep}{3mm}{    
%         \begin{tabular}{@{}lccc@{}}
%         \toprule
%                             & ViT-S/16  & ViT-B/16 & ViT-L/16 \\ \midrule
%         Training Epochs     & 90        & 90       & 50       \\
%         Warmup Epochs       & 10        & 10       & 10       \\
%         Optimizer           & LARS      & LARS     & LARS     \\
%         Base Learning Rate  & 0.1       & 0.1      & 0.1      \\
%         Learning Rate Decay & Cosine    & Cosine   & Cosine   \\
%         Weight Decay        & 0.0       & 0.0      & 0.0      \\
%         Eff. Batch Size     & 8,192     & 8,192    & 1,024    \\ \bottomrule
%         \end{tabular}
%     }
% \end{table}

\begin{table}[htbp]
\caption{The linear probing hyperparameters.}
\label{table:hyper-ds}
\centering
\begin{tabular}{@{}lcccccc@{}}
\toprule
Downstream Tasks    & \multicolumn{3}{c}{Fine-tuning} & \multicolumn{3}{c}{Linear Probing} \\
Models              & ViT-S/16  & ViT-B/16 & ViT-L/16 & ViT-S/16   & ViT-B/16  & ViT-L/16  \\ \midrule
Training Epochs     & 100       & 100      & 50       & 90         & 90        & 50        \\
Warmup Epochs       & 5         & 5        & 5        & 10         & 10        & 10        \\
Optimizer           & AdamW     & AdamW    & AdamW    & LARS       & LARS      & LARS      \\
Base Learning Rate  & 5e-4      & 5e-4     & 1e-3     & 0.1        & 0.1       & 0.1       \\
Learning Rate Decay & Cosine    & Cosine   & Cosine   & Cosine     & Cosine    & Cosine    \\
Weight Decay        & 0.05      & 0.05     & 0.05     & 0.0        & 0.0       & 0.0       \\
Eff. Batch Size     & 1,024     & 1,024    & 1,024    & 8,192      & 8,192     & 1,024     \\ \bottomrule
\end{tabular}
\end{table}

While in the fine-tuning and linear probing on downstream task of classification, the hyperparameters are exhibited in Table \ref{table:hyper-ds}. In the fine-tuning, the initial learning rate is $5\times10^{-4}$ in ViT-S/16 and ViT-B/16, while $10^{-3}$ in ViT-L/16. Likewise, ViT-S/16 and ViT-B/16 need more training iterations with 100 epochs while 50 epochs within ViT-L/16, which are executed on $2\times 2$ NVIDIA A100 GPUs.



\end{document}
