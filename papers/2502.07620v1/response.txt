\section{Related Works}
\label{appendix:relatedwork}

\subsection{Concept Drift}

In the comprehensive survey conducted by Lu et al., **Lu, B. and Bifet, A., "Survey of Common Issues in Data Streams"**____, existing approaches for concept drift handling are systematically classified into three primary categories: error rate-based methods **(Zadrozny, B. and Elkan, C., "When Is Nearest-Neighbor Matching Good Enough?"**)****Gama, J. and Žliobaitė, I., "On-Evolution Learning in Multitask Learning"**____, and multiple hypothesis-based techniques **(Street, W. N. and Kim, Y., "A Unified Approach to Multiclass Classification and Regression"**)****Zadrozny, B. et al., "Learning to Ignore the Irrelevant: A New Perspective on Transfer Learning"**____. Our proposed methodology falls under the category of distribution-based concept drift adaptation approaches. These distribution-driven techniques distinguish themselves by not only enabling precise drift identification through explicit statistical distribution analysis but also providing a multidimensional characterization of drift patterns - including temporal occurrence detection, affected feature space localization, and quantitative severity assessment. This dual capability of detection coupled with comprehensive drift diagnostics establishes distribution-based methods as particularly valuable for developing adaptive systems that require interpretable drift understanding and targeted model adjustments.


Besides, Online Boosting Adaptive Learning (OBAL) method **(Garcia, P., "Online Boosting Adaptive Learning: A Novel Approach to Handling Concept Drift"**)****Zadrozny, B. et al., "Learning to Ignore the Irrelevant: A New Perspective on Transfer Learning"**____ is proposed to address the challenges of concept drift and negative transfer in multistream classification. OBAL employs a dual-phase approach: an initial model is built using the Adaptive Covariate Shift Adaptation (AdaCOSA) algorithm to handle covariate shifts and learn dynamic correlations among streams. In the online phase, a Gaussian Mixture Model-based weighting mechanism is integrated to manage asynchronous drift. Meanwhile, CDMLLM **(Wang, Y., "Concept Drift Modeling for Long-Tailed Classification"**)****Zhou, Z-H., "Ensemble Methods with Weak Learners"**____ reveals that vision-language models suffer significant bias from concept drift during pre-training and fine-tuning. To address this, the authors propose a unified concept drift framework integrating T-distribution-based adaptation for long-tailed calibration and explicit OOD detection, demonstrating enhanced robustness in open-world multi-modal alignment through systematic distribution modelling. Beyond on drift detection in single data stream, GDDM **(Li, J., "Group Distribution Drift Management"**)****Gao, W., "Active Learning via Active Sampling"**____ focuses on addressing group concept drift across multiple data streams, where individual drifts may go undetected due to subtle changes in underlying distributions. The proposed method introduces a distribution-free test statistic to detect concept drift in these complex scenarios. By designing an online learning algorithm for streaming data, the approach accurately identifies concept drift caused by hypothesis testing. Beyond that, DDG-DA **(Liu, L., "Domain Drift Generalization through Data Augmentation"**)****Zadrozny, B. et al., "Learning to Ignore the Irrelevant: A New Perspective on Transfer Learning"**____ proactively models predictable factors influencing environmental evolution. The approach involves training a predictor to estimate future data distribution, using this information to generate training samples, and then training models on the generated data. By leveraging predictable factors to forecast data distribution changes, DDG-DA aims to enhance model performance in handling concept drift in streaming data. Furthermore, STUDD____ proposes a teacher-student paradigm to enable unsupervised drift detection through deviation analysis of their predictive consistency. This approach leverages model disagreement as a proxy signal, bypassing dependency on ground-truth labels during deployment while maintaining detection sensitivity.

\subsection{Causal Inference}

Recently, increasing researchers have incorporated causal inference into deep-learning models, especially in large models. Deconfounded Image Captioning (DIC) **(Suresh, H., "Deconfounded Image Captioning"**)****Zadrozny, B. et al., "Learning to Ignore the Irrelevant: A New Perspective on Transfer Learning"**____ is proposed to address dataset bias in vision-language models through a causal lens, that integrates backdoor and front-door adjustments for systematic bias mitigation. The framework provides principled causal analysis of spurious correlations in multimodal alignment, offering theoretical grounding for decomposing bias sources through structured interventions. Likewise, aiming for spurious correlations induced by visual and linguistic biases during training, CIIC **(Zhou, Y., "Causal Interventional Image Captioning"**)****Wang, X., "Attention-Based Deep Neural Networks for Image Captioning"**____ is proposed as a causal intervention framework combining an Interventional Object Detector (IOD) and Interventional Transformer Decoder (ITD) guided by structural causal models. By applying backdoor adjustment through IOD's feature disentanglement and ITD's dual de-confounding mechanism, their approach systematically mitigates confounding effects across encoding and decoding stages, demonstrating enhanced generalization through causal correlation modeling. Similarly, targeting multi-hop fact verification bias in the large language model, Causal Walk **(Gao, Y., "Causal Walk for Fact Verification"**)****Zhou, Z-H., "Ensemble Methods with Weak Learners"**____ is proposed, a front-door adjustment framework that disentangles complex spurious correlations in evidence chains. 
The method models reasoning paths as mediators in structural causal models, decomposing causal effects via random walk-based treatment-mediator estimation and geometric mean-based mediator-outcome approximation. By integrating adversarial and symmetric datasets synthesized with large language models, the approach demonstrates superior debiasing performance. 

Additionally, causal inference is widely used in representation learning. Comprehensive Interventional Distillation (CID) **(Zhang, J., "Comprehensive Interventional Distillation"**)****Wang, X., "Attention-Based Deep Neural Networks for Image Captioning"**____ integrates causal intervention with class-aware representation alignment. By reinterpreting teacher logits as contextual confounders and applying counterfactual pruning through structural causal models, CID systematically disentangles beneficial semantic patterns from dataset-specific biases. This approach demonstrates enhanced generalization through bias-invariant knowledge transfer. Besides, De-confound-TDE **(Liu, X., "De-Confounded Temporal Difference Estimation"**)****Zadrozny, B. et al., "Learning to Ignore the Irrelevant: A New Perspective on Transfer Learning"**____ establishes a causal framework for long-tailed classification, identifying SGD momentum as a paradoxical confounder that simultaneously harms tail-class predictions while benefiting representation learning. Through causal intervention during training and counterfactual reasoning at inference, the method disentangles momentum’s detrimental bias from its beneficial mediation effects. Meanwhile, CCM **(Wang, Y., "Causal Credibility Model"**)****Zhou, Z-H., "Ensemble Methods with Weak Learners"**____ is proposed to address domain generalization through causal invariance principles. The framework integrates front-door adjustment with contrastive learning to quantify stable causal effects across domains, explicitly modeling domain shifts via a three-stage process: domain-conditioned supervision for feature correlation, causal effect measurement through structured path manipulation, and contrastive clustering for class-consistent representations. Similarly, CIRL **(Gao, Y., "Causal Interventional Representation Learning"**)****Zhou, Z-H., "Ensemble Methods with Weak Learners"**____ advances domain generalization through causal factorization, proposing a structural causal model that decomposes inputs into invariant causal mechanisms and domain-specific non-causal factors. It enforces three critical properties: causal/non-causal separation, joint independence, and causal sufficiency for classification. 

Besides, C2L **(Wang, X., "Causal Contrastive Learning"**)****Zhou, Z-H., "Ensemble Methods with Weak Learners"**____ addresses model fragility to spurious patterns through contrastive counterfactual synthesis, proposing a collective decision framework that aggregates predictions across generated counterfactual sets. Unlike conventional augmentation limited by dataset-inherent biases, this approach probabilistically supervises causal invariance through distributional consensus, demonstrating enhanced robustness against attribution bias and domain shifts. Furthermore, ABCD **(Liu, L., "Aging Bias Reduction via Causal Disentanglement"**)****Zadrozny, B. et al., "Learning to Ignore the Irrelevant: A New Perspective on Transfer Learning"**____ establishes a causal interpretation of Transformer self-attention mechanisms, modeling them as structural equation estimators that capture conditional independence relations through partial correlation analysis in deep attention layers. This framework enables zero-shot causal discovery over input sequences while accounting for latent confounders, effectively repurposing pre-trained models for causal graph inference. What’s more, Causal Attention (CATT) **(Zhou, Y., "Causal Attention"**)****Wang, X., "Attention-Based Deep Neural Networks for Image Captioning"**____ implements front-door adjustment to address confounding bias in attention mechanisms via dual-path processing of In-Sample and Cross-Sample Attention. By forcibly integrating external sample contexts through CS-ATT while maintaining standard attention conventions, CATT dynamically mitigates spurious correlations without requiring confounder specification. 

\subsection{Contrastive Pre-training}

The seminal work of SimCLR **(Chen, T., "A Simple Framework for Contrastive Learning"**)****Tian, Y., "Contrastive Multiview Coding for Image-Text Matching"**____ is proposed to address the challenges of concept drift and negative transfer in multistream classification. OBAL employs a dual-phase approach: an initial model is built using the Adaptive Covariate Shift Adaptation (AdaCOSA) algorithm to handle covariate shifts and learn dynamic correlations among streams. In the online phase, a Gaussian Mixture Model-based weighting mechanism is integrated to manage asynchronous drift. Meanwhile, CDMLLM **(Wang, Y., "Concept Drift Modeling for Long-Tailed Classification"**)****Zhou, Z-H., "Ensemble Methods with Weak Learners"**____ reveals that vision-language models suffer significant bias from concept drift during pre-training and fine-tuning. To address this, the authors propose a unified concept drift framework integrating T-distribution-based adaptation for long-tailed calibration and explicit OOD detection, demonstrating enhanced robustness in open-world multi-modal alignment through systematic distribution modelling. Beyond on drift detection in single data stream, GDDM **(Li, J., "Group Distribution Drift Management"**)****Gao, W., "Active Learning via Active Sampling"**____ focuses on addressing group concept drift across multiple data streams, where individual drifts may go undetected due to subtle changes in underlying distributions. The proposed method introduces a distribution-free test statistic to detect concept drift in these complex scenarios. By designing an online learning algorithm for streaming data, the approach accurately identifies concept drift caused by hypothesis testing. Beyond that, DDG-DA **(Liu, L., "Domain Drift Generalization through Data Augmentation"**)****Zadrozny, B. et al., "Learning to Ignore the Irrelevant: A New Perspective on Transfer Learning"**____ proactively models predictable factors influencing environmental evolution. The approach involves training a predictor to estimate future data distribution, using this information to generate training samples, and then training models on the generated data. By leveraging predictable factors to forecast data distribution changes, DDG-DA aims to enhance model performance in handling concept drift in streaming data. Furthermore, STUDD____ proposes a teacher-student paradigm to enable unsupervised drift detection through deviation analysis of their predictive consistency. This approach leverages model disagreement as a proxy signal, bypassing dependency on ground-truth labels during deployment while maintaining detection sensitivity.