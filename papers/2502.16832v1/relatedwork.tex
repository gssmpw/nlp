\section{Related Work}
\label{sec:related-work}
We introduce existing methods of classifier debiasing, model alignment and data augmentation in federated learning.

\subsection{Classifier Debiasing in Federated Learning}
Federated learning (FL) provides a new solution to handle privacy concerns in distributed training. 
As the pioneering method, FedAvg~\cite{fedavg}, trains a global model by aggregating local models from multiple clients without accessing their raw data. However, it undergoes considerable performance degradation when the data of clients are heterogeneous due to various imaging protocols, disease incidences, or population demographics. One of the main reasons is that data heterogeneity results in divergent local classifiers. Current approaches to this problem can be divided into three categories.

The first type of approaches~\cite{luo2021no,huang2024federated,zhou2023fedfa} aims to generate a balanced feature set to train local classifiers. For example, CCVR~\cite{luo2021no} exploits feature representations of all clients to build an approximated Gaussian mixture model, which is sent to each client for sampling more virtual representations. RUCR~\cite{huang2024federated} broadcasts global prototypes to clients and arbitrarily fuses them and local features to synthesize virtual features.
The second category~\cite{huang2023rethinking, dai2023tackling, tan2022fedproto, qi2023cross, long2023fedcd} tends to replace local classifiers with class prototypes. For instance, FedProto~\cite{tan2022fedproto} directly aggregates prototypes of each class as local classifiers of clients. FedNH~\cite{dai2023tackling} produces uniformly-distributed class prototypes as initial local classifiers, and then smoothly infuses the class semantics into class prototypes. FPL~\cite{huang2023rethinking} uploads feature representations of all clients to the server and clusters them to get different prototype centers for each class. These prototype centers are further averaged as local classifiers.

The third branch of works~\cite{fedETF, zhang2024upload} pre-constructs a fixed classifier before federated training. According to the theory of neural collapse~\cite{papyan2020prevalence} that classifier vectors converge to an optimal simplex equiangular tight frame (ETF) when the dataset is balanced and sufficient, FedETF~\cite{fedETF} and FedKTL~\cite{zhang2024upload} introduce a synthetic simplex ETF as a fixed classifier for all clients. However, the orthogonal relation between classifier vectors is too strict and lacks of semantic interpretability. In this work, we propose to borrow linguistic knowledge from pre-trained language models to construct local classifiers.  

\subsection{Model Alignment in Federated Learning}
Data heterogeneity also leads to misalignment between client models, i.e., client-level variance,  resulting in unstable and slow convergence during federated learning~\cite{li2020federated}. 
FedProx~\cite{fedprox} is the first work to solve this problem by introducing a proximal term into the objective during local training to restrict the distance between the current global model and the local model.  SCAFFOLD~\cite{karimireddy2020scaffold} introduces control variates to correct the drift in local updates. Nevertheless, the direct constraint in the parameter space may negatively affect model learning. 

Apart from the above solutions, another way is to introduce the constraint in the feature space to solve this problem. For example,
MOON~\cite{li2021model} presents model-level contrastive learning to maximize the similarity between the features of local models in the current round and the global model and minimize the similarity between the features of local models of the current round and the previous round.  
FedFA~\cite{zhou2023fedfa}, FedFM~\cite{ye2023fedfm}, and FedPAC~\cite{xu2023personalized} collect local class prototypes to generate global prototypes. These global prototypes are sent to clients as the alignment objective of feature representations during local training. 
RUCR~\cite{huang2024federated} pulls features within the same class towards corresponding global prototypes and
pushes features of the other classes away.
Although these prototype-based methods can improve representation learning, their performance highly relies on the quality of global prototypes. Unlike the existing methods, we directly narrow the distribution gap between client data.
\begin{figure*}[!t]
%\vspace{-1.0mm}
\centering
\includegraphics[width = 1.0\textwidth]{./figs/framework.pdf}
%\vspace{-2.0mm}
\caption{The overview of the proposed FedBM framework. FedBM contains Linguistic Knowledge-based Classifier Construction (LKCC) and Concept-guided Global Distribution Estimation (CGDE). LKCC uses class concepts, prompts and PLMs to build latent concept distributions, which are sent to clients as local classifiers. CGDE samples probabilistic concept embeddings from the distributions to train a conditional generator. The generator is shared by all clients and produces pseudo data to calibrate updates of local feature extractors. (Best viewed in color) }
\label{framework}
%\vspace{-2.0mm}
\end{figure*}

\subsection{Data Augmentation in Federated Learning}
Data Augmentation is a commonly-used way to relieve data heterogeneity issues in federated learning. 
\cite{de2022mitigating} have verified that some common data augmentation techniques can significantly improve out-of-domain generalization in federated settings, such as random cropping,  horizontal flipping and color transformations. Besides, Mixup~\cite{zhang2017mixup} also obtains the widespread attention~\cite{guo2023fedbr, yoon2021fedmix, shin2020xor}. For example, FedMix~\cite{yoon2021fedmix} averages local batches to produce synthetic data. The server gathers these data and then sends them to clients. Clients combine these synthetic data with their local data to perform Mixup in local training.

In addition, FedOV~\cite{diao2023towards} and FedOSS~\cite{FedOSS} are inspired by adversarial training and use fast gradient sign method (FGSM) to generate unknown samples.
Moreover, FedRDN~\cite{yan2023simple} and CCVR~\cite{luo2021no} hypothesize that the images of each client are sampled from a multivariate Gaussian distribution. By sharing Gaussian distributions across clients, each client can sample augmented data to enhance local data, thereby reducing the domain gap.
SDA-FL~\cite{li2022federated} pre-trains a generative adversarial network (GAN)~\cite{goodfellow2014generative} to generate synthetic data in each client. These synthetic data are then collected by the server to construct a global synthetic dataset to optimize global model. 
% Federated Learning with GAN-Based Data Synthesis for Non-IID Clients li2022federated
FedDiff~\cite{mendieta2024navigating} trains a class-conditioned diffusion model~\cite{ho2020denoising} on local data at the clients. These local diffusion models are sent to the server to generate data for training global model.

Data-free knowledge distillation is also a popular way to synthesize samples in the federated setting~\cite{zhang2022fine, zhang2022dense, wang2024dfrd}. For instance, DENSE~\cite{zhang2022dense} and FedFTG~\cite{zhang2022fine} utilize the ensemble client models to train a generator and then generate synthetic data to train global model at the server. However, these methods learn a generator to capture the mapping between random noises (from Gaussian distribution) and samples in the image space. The random noises lack semantically meaningful information and do not form well-organized class clusters, enabling the generator to produce low-quality images. The proposed FedBM framework introduces the text information of classes to remedy this disadvantage.