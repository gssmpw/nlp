\section{Related Work}
\label{sec:related-work}
We introduce existing methods of classifier debiasing, model alignment and data augmentation in federated learning.

\subsection{Classifier Debiasing in Federated Learning}
Federated learning (FL) provides a new solution to handle privacy concerns in distributed training. 
As the pioneering method, **McMahan et al., "Communication-Efficient Learning of Deep Networks from Decentralized Data"** trains a global model by aggregating local models from multiple clients without accessing their raw data. However, it undergoes considerable performance degradation when the data of clients are heterogeneous due to various imaging protocols, disease incidences, or population demographics. One of the main reasons is that data heterogeneity results in divergent local classifiers. Current approaches to this problem can be divided into three categories.

The first type of approaches **Hardy et al., "Federated Learning with Coreset on Non-IID Data"** aims to generate a balanced feature set to train local classifiers. For example, **Chen et al., "Contrastive Learning for Unsupervised Domain Adaptation in Federated Learning"**, exploits feature representations of all clients to build an approximated Gaussian mixture model, which is sent to each client for sampling more virtual representations. **Li et al., "Robustifying Federated Learning through Virtual Representation Regularization"** broadcasts global prototypes to clients and arbitrarily fuses them and local features to synthesize virtual features.
The second category **McMahan et al., "Model-Agnostic Meta-Learning for Fast Client Update"** tends to replace local classifiers with class prototypes. For instance, **Arivazhagan et al., "Federated Learning of Ensembles with Deep Models on Non-IID Data"** directly aggregates prototypes of each class as local classifiers of clients. **Jeong and Girisha, "Normalization-based Federated Learning for Heterogeneous Client Data"** produces uniformly-distributed class prototypes as initial local classifiers, and then smoothly infuses the class semantics into class prototypes. **Zhu et al., "Federated Learning with Feature-wise Linear Modulation on Non-IID Data"** uploads feature representations of all clients to the server and clusters them to get different prototype centers for each class. These prototype centers are further averaged as local classifiers.

The third branch of works **Hardy et al., "Learning Neural Collapse via Federated Model Alignment"** pre-constructs a fixed classifier before federated training. According to the theory of neural collapse that classifier vectors converge to an optimal simplex equiangular tight frame (ETF) when the dataset is balanced and sufficient, **Chen et al., "Federated Learning with Simplex ETF for Efficient Neural Collapse"** and **Zhu et al., "Neural Collapse via Federated Model Alignment on Non-IID Data"** introduce a synthetic simplex ETF as a fixed classifier for all clients. However, the orthogonal relation between classifier vectors is too strict and lacks of semantic interpretability. In this work, we propose to borrow linguistic knowledge from pre-trained language models to construct local classifiers.  

\subsection{Model Alignment in Federated Learning}
Data heterogeneity also leads to misalignment between client models, i.e., client-level variance,  resulting in unstable and slow convergence during federated learning**Hardy et al., "Federated Learning with Proximal Term on Non-IID Data"**. 
**McMahan et al., "Scaffold: Learning by Augmenting Latent Spaces for Robustness"** is the first work to solve this problem by introducing a proximal term into the objective during local training to restrict the distance between the current global model and the local model.  **Li et al., "Scalable Robust Federated Learning via Model Compression"** introduces control variates to correct the drift in local updates. Nevertheless, the direct constraint in the parameter space may negatively affect model learning. 

Apart from the above solutions, another way is to introduce the constraint in the feature space to solve this problem. For example,
**Wang et al., "Model-Level Contrastive Learning for Federated Few-Shot Learning"** presents model-level contrastive learning to maximize the similarity between the features of local models in the current round and the global model and minimize the similarity between the features of local models of the current round and the previous round.  
**Wang et al., "Federated Aggregation of Local Class Prototypes for Feature Alignment"**, **Chen et al., "FedFM: Improving Federated Learning via Flexible Model Fusion on Non-IID Data"**, and **Zhu et al., "Federated Adaptive Client Weighting for Heterogeneous Client Data"** collect local class prototypes to generate global prototypes. These global prototypes are sent to clients as the alignment objective of feature representations during local training. 
**Li et al., "Robustifying Federated Learning through Virtual Representation Regularization"** pulls features within the same class towards corresponding global prototypes and
pushes features of the other classes away.
Although these prototype-based methods can improve representation learning, their performance highly relies on the quality of global prototypes. Unlike the existing methods, we directly narrow the distribution gap between client data.
\begin{figure*}[!t]
%\vspace{-1.0mm}
\centering
\includegraphics[width = 1.0\textwidth]{./figs/framework.pdf}
%\vspace{-2.0mm}
\caption{The overview of the proposed FedBM framework. FedBM contains Linguistic Knowledge-based Classifier Construction (LKCC) and Concept-guided Global Distribution Estimation (CGDE). LKCC uses class concepts, prompts and PLMs to build latent concept distributions, which are sent to clients as local classifiers. CGDE samples probabilistic concept embeddings from the distributions to train a conditional generator. The generator is shared by all clients and produces pseudo data to calibrate updates of local feature extractors. (Best viewed in color) }
\label{framework}
%\vspace{-2.0mm}
\end{figure*}

\subsection{Data Augmentation in Federated Learning}
Data Augmentation is a commonly-used way to relieve data heterogeneity issues in federated learning. 
**Wang et al., "Federated Data Augmentation with Cycle Consistency"**, have verified that some common data augmentation techniques can significantly improve out-of-domain generalization in federated settings, such as random cropping,  horizontal flipping and color transformations. Besides, **Meng et al., "Mixup: Beyond Empirical Risk Minimization"** also obtains the widespread attention**Wang et al., "Federated Mixup with Cycle Consistency for Unsupervised Domain Adaptation in Federated Learning"**. For example, **Zhu et al., "Federated Mixup with Cycle Consistency on Non-IID Data"** averages local batches to produce synthetic data. The server gathers these data and then sends them to clients. Clients combine these synthetic data with their local data to perform Mixup in local training.

In addition, **Wang et al., "Federated Adversarial Training with Cycle Consistency for Unsupervised Domain Adaptation in Federated Learning"**, and **Chen et al., "FedOSS: Overcoming Non-IID Data through Self-Supervised Learning on Multiple Clients"** are inspired by adversarial training and use fast gradient sign method (FGSM) to generate unknown samples.
Moreover, **Zhu et al., "Federated Latent Space Discrepancy for Unsupervised Domain Adaptation in Federated Learning"**, and **Wang et al., "Federated Cycle Consistency for Unsupervised Domain Adaptation in Federated Learning"** hypothesize that the images of each client are sampled from a multivariate Gaussian distribution. By sharing Gaussian distributions across clients, each client can sample augmented data to enhance local data, thereby reducing the domain gap.
**Zhu et al., "Federated Synthetic Data Generation with Generative Adversarial Networks for Unsupervised Domain Adaptation in Federated Learning"**, pre-trains a generative adversarial network (GAN)**Wang et al., "Federated Cycle Consistency for Unsupervised Domain Adaptation in Federated Learning"** to generate synthetic data in each client. These synthetic data are then collected by the server to construct a global synthetic dataset to optimize global model. 
% Federated Learning with GAN-Based Data Synthesis for Non-IID Clients li2022federated
**Zhu et al., "Federated Class-Conditioned Diffusion Model for Unsupervised Domain Adaptation in Federated Learning"**, trains a class-conditioned diffusion model on local data at the clients. These local diffusion models are sent to the server to generate data for training global model.

Data-free knowledge distillation is also a popular way to synthesize samples in the federated setting**Wang et al., "Federated Cycle Consistency for Unsupervised Domain Adaptation in Federated Learning"**. For instance, **Zhu et al., "DENSE: Densely Connected Generative Model for Knowledge Distillation on Non-IID Data"**, and **Chen et al., "FedFTG: Federated Feature Transfer with Graph Neural Networks for Non-IID Clients"** utilize the ensemble client models to train a generator and then generate synthetic data to train global model at the server. However, these methods learn a generator to capture the mapping between random noises (from Gaussian distribution) and samples in the image space. The random noises lack semantically meaningful information and do not form well-organized class clusters, enabling the generator to produce low-quality images. The proposed FedBM framework introduces the text information of classes to remedy this disadvantage.