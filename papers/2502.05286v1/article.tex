%%
%% This is file `sample-manuscript.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,manuscript')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-manuscript.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[manuscript,screen]{acmart}%%
%PREPRINT
%review,anonymous

%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
%\setcopyright{acmlicensed}%PREPRINT
%\copyrightyear{2025}%PREPRINT
%\acmYear{2025}%PREPRINT
%\acmDOI{XXXXXXX.XXXXXXX}%PREPRINT
%% These commands are for a PROCEEDINGS abstract or paper.
%\acmConference[FAccT â€™25]{the 2025 ACM Conference on Fairness, Accountability, and Transparency}{TBA,
%  2025}{Athens, Greece} %PREPRINT
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/18/06}

\setcopyright{none}
% AUTHOR-ADDED PACKAGES
\usepackage{color}
\usepackage{amsmath}
%\usepackage{bbm}
\usepackage{tikz}
\usepackage{dsfont}
\usepackage{subcaption}
%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\input{macros}
\setlength{\parindent}{0pt}
\begin{document}

% Submissions must be no longer than 14 pages, excluding references.

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Fairness and Sparsity within Rashomon sets: Enumeration-Free Exploration and Characterization}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

\input{authors_long} 


%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Langlade et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  We introduce an enumeration-free method based on mathematical programming to precisely characterize various properties such as fairness or sparsity within the set of ``good models", known as \emph{Rashomon set}. This approach is generically applicable to any hypothesis class, provided that a mathematical formulation of the model learning task exists. It offers a structured framework to define the notion of business necessity and evaluate how fairness can be improved or degraded towards a specific protected group, while remaining within the Rashomon set and maintaining any desired sparsity level. 

We apply our approach to two hypothesis classes: scoring systems and decision diagrams, leveraging recent mathematical programming formulations for training such models. As seen in our experiments, the method comprehensively and certifiably quantifies trade-offs between predictive performance, sparsity, and fairness. We observe that a wide range of fairness values are attainable, ranging from highly favorable to significantly unfavorable for a protected group, while staying within less than 1\% of the best possible training accuracy for the hypothesis class. Additionally, we observe that sparsity constraints limit these trade-offs and may disproportionately harm specific subgroups. As we evidenced, thoroughly characterizing the tensions between these key aspects is critical for an informed and accountable selection of models.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010147.10010257.10010258.10010259</concept_id>
<concept_desc>Computing methodologies~Supervised learning</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10002950.10003624.10003625.10003630</concept_id>
<concept_desc>Mathematics of computing~Combinatorial optimization</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10002950.10003714.10003716</concept_id>
<concept_desc>Mathematics of computing~Mathematical optimization</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Supervised learning}
\ccsdesc[500]{Mathematics of computing~Combinatorial optimization}
\ccsdesc[500]{Mathematics of computing~Mathematical optimization}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Supervised learning, Rashomon Sets, Fairness, Interpretability, Sparsity, Mathematical Optimization} %  Scoring Systems, Decision Diagrams

%\received{20 February 2007}
%\received[revised]{12 March 2009}
%\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

The increasing reliance on machine learning models for high-stakes decision-support tasks, such as predictive justice~\cite{angwin2016machine}, 
hiring~\cite{langenkamp2020hiring}, and medicine~\cite{DBLP:conf/aaai/0001CDM21} raises important ethical questions and is subject to legal requirements. 
For instance, article 13 of the recent EU AI Act\footnote{\url{https://artificialintelligenceact.eu/article/13/}} mandates transparency for AI-based systems classified 
as ``high-risk",  a category that encompasses a broad spectrum of applications. % such as the aforementioned ones. 
This legal and ethical context highlights the importance of developing predictive models that are inherently \emph{interpretable} and \emph{sparse}. %\emph{Sparsity} is often adopted as a proxy for the models' interpretability.
Fairness is another critical consideration, reinforced by legal frameworks such as the ``80 percent rule'' for statistical parity~\cite{DBLP:conf/kdd/FeldmanFMSV15} established by the US Equal Employment Opportunity Commission~\cite{uniformguidelinesemployeeselection} in the context 
of hiring.

% Rashomon sets 
When training a machine learning model, the primary objective is typically to maximize its utility. However, multiple models with equivalent performance can produce substantially different predictions, a phenomenon known as predictive multiplicity. This observation gave rise to the concept of $\epsilon$-Rashomon sets~\citep{breiman2001statistical,fisher2019all}, which encompass all models within a given hypothesis class whose utility deviates by no more than $\epsilon$ from the optimal value. Due to predictive 
multiplicity, models within an $\epsilon$-Rashomon set can exhibit markedly different ---sparsity or fairness--- properties.

% Exploration of Rashmon Sets
Different methods have been proposed to explore Rashomon sets for specific hypothesis classes.
On one hand, enumeration-based 
methods~\citep{DBLP:conf/nips/XinZ0TSR22,DBLP:journals/corr/abs-2204-11285,ciaperoni2024efficient} sample all models within 
the Rashomon set of interest. This can be computationally prohibitive since the Rashomon sets might contain an untractable number of models. On the other hand, enumeration-free approaches~\citep{coker2021theory,DBLP:conf/aaai/Watson-DanielsP23,fisher2019all,zhong2024exploring,coston2021characterizing} can characterize properties across all models in the Rashomon set without explicitly enumerating them. Despite their efficiency, existing enumeration-free approaches tend to underestimate the size of Rashomon sets and their corresponding fairness-utility trade-offs. This limitation arises from the reliance on convex upper bounds (e.g., logistic loss, hinge loss) as approximations of the actual model error ($0/1$ loss). Moreover, these methods do not explore the effects of sparsity requirements, which further influence these trade-offs.

To overcome these limitations, we introduce an enumeration-free method based on mathematical programming to precisely explore various properties within 
Rashomon sets, such as fairness or sparsity. Our approach provides a structured, quantitative framework for evaluating the concept of ``business necessity''~\cite{grover1995business}, a legal argument often used by companies with unbalanced employment outcomes among protected groups. A key aspect of proving ``business necessity'' is demonstrating that no alternative employment policy could achieve the same objectives with less discriminatory impact. Within our framework, this translates to asserting that all high-performing models within a given hypothesis class exhibit a disparate impact toward a specific group.
Equivalently, the search for less discriminatory alternative models is becoming a legal requirement~\citep{DBLP:journals/corr/abs-2406-06817}, and our framework automates this task. More precisely, it can be used to precisely and provably bound the achievable fairness values within given performance and sparsity levels.
Our main contributions are: 
\begin{itemize}
    \item We propose an enumeration-free framework for exploring the properties of models within the Rashomon set, focusing on key aspects such as fairness and sparsity. This framework is broadly applicable to any hypothesis class where the learning process can be formulated as a mathematical program.
    \item To illustrate the versatility of our framework, we apply it to two different hypothesis classes: scoring systems and decision diagrams. The associated source code is openly accessible at \url{https://github.com/vidalt/Rashomon-Explorer}, under a MIT license.%(provided in the supplementary material)
    \item We conduct extensive experiments validating the effectiveness of our approach in certifiably quantifying the trade-offs (and tensions) between predictive performance, fairness, and sparsity for a given hypothesis class. Our results precisely characterize the range of fairness values achievable under specified sparsity and performance constraints. Additionally, we observe that sparsity does not come for free ---imposing stringent sparsity requirements significantly limits the achievable trade-offs between fairness and performance.
\end{itemize} 


\section{Technical Background}
\label{sec:technical_background}

\paragraph{Supervised Machine Learning.}
Let \smash{$\data:=(\attributes[\example], \labels[\example])_{\example=1}^\nexamples$} be a dataset in which each example \smash{$\example \in \{1..\nexamples\}$} is characterized
 by a feature vector \smash{$\attributes[\example]\in\R^{\nfeatures}$} with $\nfeatures$ attributes and a binary label \smash{$\labels[\example]\in\{\minus 1,1\}$}.
The objective of a supervised learning algorithm is to produce a predictive model \smash{$\h: \R^{\nfeatures}\rightarrow\{\minus 1, 1\}$} from a given hypothesis 
space $\H$ that minimizes a loss function \smash{$\lossgeneric:\{\minus 1, 1\}\times\{\minus 1, 1\}\rightarrow \R_+$} encoding the error between a predicted and 
actual outcome. In practice, the empirical loss \smash{$\emploss{\data}{\h}:=\frac{1}{\nexamples}\sum_{\example=1}^\nexamples\lossgeneric(\h(\attributes[\example]), 
\labels[\example])$} is minimized to get a good predictor $\h_\data$:
\begin{align}
    \h_\data\in \argmin_{\h\in\H}\,\,\,\,\emploss{\data}{\h} + \text{Regularization}(\h).
    \label{eq:erm}
\end{align}
The regularization term steers an \emph{a priori} preference toward certain hypotheses in $\H$. For this study, we adopt the commonly used $0/1$ loss, defined as  \smash{$\loss(\predictions[], 
\labels[]):=\indicatorfunction [\predictions[]=\labels[]]$},
with $\predictions[]=\h(\attributes[])$.

\paragraph{Fairness.} Undesirable biases ---specifically harming some individuals or demographic groups--- can be embedded in the dataset $\data$, introduced or amplified by the learning algorithm, or arise at any other step of the machine learning pipeline~\cite{mehrabi2021survey}. Because learning such spurious correlations and using them for decision-making raises ethical questions and is often legally prohibited, different notions of fairness have been proposed~\cite{10.1145/3194770.3194776}. In particular, \emph{statistical fairness} metrics are widely adopted due to their quantifiability and their ability to align with legal standards, such as the ``80 percent rule" for statistical parity~\cite{DBLP:conf/kdd/FeldmanFMSV15}, as outlined by the US Equal Employment Opportunity Commission~\cite{uniformguidelinesemployeeselection} in the context of hiring practices.\\
Statistical fairness metrics assess disparities in specific statistical measures between different \emph{protected groups}, defined by the values of \emph{sensitive features} (e.g., race, gender). The goal of these metrics is to ensure that such features do not influence individual outcomes. Typically, these measures are derived from the confusion matrix of the predictor $\h$.
Let \smash{$\agroup[1] \subset \data$} and \smash{$\agroup[2] \subset \data$} represent two protected groups differentiated by a given sensitive feature. For example, in a hiring context, \smash{$\agroup[1]$} might represent the set of male applicants, while \smash{$\agroup[2]$} represents the set of female applicants. We consider two widely used fairness metrics:

\textbullet\ \emph{Statistical parity}~\citep{dwork2012fairness} quantifies the difference in positive prediction rates (e.g., acceptance rates for job applicants) between the two protected groups:
    \begin{align}
        \valueSP(\h,\data) := \frac{\sum_{\example\in \agroup[1]}  \indicatorfunction [\h(\attributes[\example])=1]}{|\agroup[1]|} - 
    \frac{\sum_{\example \in \agroup[2]} \indicatorfunction [\h(\attributes[\example])=1]}{|\agroup[2]|} \label{eq:sp}
    \end{align}
\textbullet\ \emph{Equal opportunity}~\citep{hardt2016equality} measures the difference in true positive rates (e.g., acceptance rates for genuinely qualified applicants) between the two protected groups:
    \begin{align}
        \valueEO(\h,\data) := \frac{\sum_{\example\in \agrouppos[1]}  \indicatorfunction [\h(\attributes[\example])=1]}{|\agrouppos[1]|} - 
    \frac{\sum_{\example \in \agrouppos[2]} \indicatorfunction [\h(\attributes[\example])=1]}{|\agrouppos[2]|} \label{eq:eo}
    \end{align}
with \smash{$\agrouppos[1]=\{\example \in \agroup[1] | \labels[\example] = 1\}$} and \smash{$\agrouppos[2]=\{\example \in \agroup[2] | \labels[\example] = 1\}$}.
For both metrics, values closer to $0$ indicate better fairness in the model. Positive values suggest a bias favoring \smash{$\agroup[1]$}, while negative values indicate a bias toward \smash{$\agroup[2]$}.

\paragraph{Interpretability.}
It can be defined as ``the ability to explain or to present something in understandable terms to a human"~\cite{doshi2017towards}. It is a critical property for ensuring the trustworthiness of machine learning systems and is often a legal requirement in real-world applications. One possible approach to achieving interpretability is through \emph{post-hoc} explanations~\cite{guidotti2018survey} of black-box models, which aim to clarify either individual decisions or the model's overall behavior.
However, such methods can be unreliable in certain contexts and are vulnerable to manipulation~\cite{aivodji2019fairwashing,slack2020fooling}. 
An alternative is to develop inherently \emph{interpretable} models, such as decision trees or rule lists, which do not share these weaknesses~\cite{rudin2019stop}. While interpretability lacks a universal definition, \emph{sparsity} 
(such as the number of nodes in a decision tree) is often used as a proxy~\cite{rudin2022interpretable}.
Enforcing sparsity constraints effectively restricts the hypothesis space to a more interpretable subset, $\H_{\interpretable} \subset \H$~\citep{dziugaite2020enforcing}.

\paragraph{Mathematical Programming.} 
Mathematical programming involves defining a set of \emph{decision variables}, each constrained to a specific domain, and specifying \emph{constraints} that describe relationships between these variables. A general-purpose solver is used to find \emph{feasible} assignments of the decision variables that satisfy the given constraints. When an \emph{objective function} is provided, the solver seeks a feasible solution that either maximizes or minimizes the function. The types of domains and constraints that can be expressed depend on the chosen paradigm. For example, \emph{Mixed-Integer Linear Programming} (MILP) solvers can accommodate both continuous and discrete variables but are restricted to linear constraints and objective functions.

\section{Related Works on Exploring Rashomon Sets}
\label{sec:related_works}

Beyond predictive performance, other model properties, such as fairness and sparsity, are often desirable. Since these properties are typically not aligned with maximizing predictive performance, it is necessary to tolerate a small drop in performance, quantified as \smash{$\epsilon$}, to search for alternative models \smash{$\h_\text{alt}$} satisfying \smash{$\emploss{\data}{\h_{\text{alt}}} \leq \emploss{\data}{\h_{\data}} + \epsilon$}.  The set of such alternative models is referred to as the \emph{Rashomon set}~\citep{breiman2001statistical,fisher2019all}, defined as:
\begin{align}
    \Rashomon := \{\h\in \H : \emploss{\data}{\h}\leq \emploss{\data}{\h_{\data}}+\epsilon\}.
\end{align}
Rashomon sets have been studied in the context of \emph{predictive multiplicity}, demonstrating that 
different models can have conflicting predictions on a substantial subset of data~\citep{marx2020predictive,DBLP:conf/nips/HsuC22,DBLP:conf/aaai/Watson-DanielsP23}. 
A key result from \citet{marx2020predictive} establishes that for any alternative $\h_{\text{alt}}\in \Rashomon$, the following tight bound holds:
\smash{$\frac{1}{\nexamples}\sum_{\example=1}^\nexamples \indicatorfunction[\h_{\text{alt}}(\attributes[\example])\neq \h_{\data}(\attributes[\example])]\leq 2\emploss{\data}
{\h_{\data}}+\epsilon$}.
This implies that even with a small $\epsilon$, models within the Rashomon set can differ significantly in their predictions whenever the empirical loss \smash{$\emploss{\data}{\h_{\data}}$} is non-zero. For instance, if \smash{$\h_{\data}$} has an empirical loss of 10\%, alternative models in the Rashomon set could disagree with \smash{$\h_{\data}$} on up to 20\% of the dataset. Similarly, for a predictor \smash{$\h_{\data}$} with a 25\% empirical loss, disagreements with alternative models could extend to 50\% of the dataset. These substantial differences emphasize that models within the Rashomon set, despite achieving nearly equivalent predictive performance, can vary markedly in their predictions. This variability has important implications for fairness, as fairness metrics such as statistical parity (Equation~(\ref{eq:sp})) and equal opportunity (Equation~(\ref{eq:eo})) are aggregates of predictions across demographic subgroups. Consequently, the Rashomon set may contain alternative models with more desirable fairness or sparsity properties. However, identifying such models efficiently remains a significant challenge. Existing methods for exploring the Rashomon set can be categorized into two main approaches: \emph{enumeration-based} and \emph{enumeration-free}.

Enumeration-based methods sample all models within the Rashomon set (or a substantial subset thereof). Existing approaches have applied enumeration to the Rashomon sets of rule lists~\citep{DBLP:journals/corr/abs-2204-11285}, rule sets~\citep{ciaperoni2024efficient}, and decision trees~\citep{DBLP:conf/nips/XinZ0TSR22} using branch-and-bound techniques. These methods explore the combinatorial hypothesis space $\H$ while leveraging error lower bounds to prune the search space efficiently. These past works have shown that competing models exhibit different fairness properties. However, a key limitation of these methods is their reliance on enumerating (and storing) a large number of models.

\input{related_works_rashomon_sets}

Alternatively, enumeration-free methods focus on the identification of models within the Rashomon set that achieve the extreme values of a specific functional \smash{$\property: \H \rightarrow \R$}.
This approach allows targeted exploration of the Rashomon set by optimizing for particular properties without exhaustive enumeration. Previous work has investigated the min-max range of the functional $\property(\h)=\h(\attributes[])$ for 
linear models with the hinge-loss \citep{coker2021theory} and the logistic loss \citep{DBLP:conf/aaai/Watson-DanielsP23}. 
Other studies have explored the extreme values of feature importance scores for linear models under the squared or hinge loss \citep{fisher2019all}, or Generalized Additive Models (GAMs) under the logistic loss \citep{zhong2024exploring}. 
Finally, the min-max range of the functional $\property$ underlying fairness metrics (cf. Equations (\ref{eq:sp}) \& (\ref{eq:eo})) has been characterized for linear models with logistic loss \citep{coston2021characterizing}. 
Table~\ref{tab:rashomon} summarizes these previous works. Building on this, our study:
\begin{itemize}
    \item explores the range of unfairness within the Rashomon sets using the ``true'' $0/1$ loss, whereas previous works relied on convex upper bounds such as the logistic or hinge loss;
    \item characterizes the effect of sparsity constraints on the range of possible disparities within the Rashomon set;
    \item provides a framework applicable to many hypothesis classes $\H$
    provided their learning process can be formulated as a mathematical optimization problem.
\end{itemize}

\section{Exploring Rashomon Sets Through Mathematical Programming}

We first introduce our generic framework for exploring the Rashomon set of a given hypothesis class whose learning is formulated as a mathematical program. We then instantiate it for two widely used classes of interpretable models, namely scoring systems and decision diagrams.

\subsection{Generic Framework}

As stated in Equation~(\ref{eq:erm}), the goal of a machine learning algorithm is to explore the hypothesis space $\H$ to identify a model $\h_\data$ 
that minimizes (on a training dataset $\data$) a given objective function, which consists of its empirical loss \smash{$\emploss{\data}{\h_\data}$} and, optionally, a regularization 
term. We focus on the common scenario where the regularization term measures the model's sparsity, with the trade-off between sparsity and 
predictive performance governed by a coefficient $\coeffsparsity$. The general mathematical formulation of this learning process is:
\begin{align}
    \min_{\h\in \H}&\,\,\,\,\emploss{\data}{\h} + \coeffsparsity \cdot \text{Sparsity}(\h). \label{eq:opt_general}
\end{align}
The model's structure and parameters are encoded through \emph{decision variables}, while its internal predictions and adherence to the hypothesis 
space are enforced through a set of \emph{constraints}.

The objective of our proposed framework is to provably determine the maximum and minimum values of a given fairness metric (along with the 
corresponding models) subject to a desired sparsity constraint while remaining within an $\epsilon$-Rashomon set of the hypothesis space $\H$. To achieve this, we first solve Problem~(\ref{eq:opt_general}) with $\coeffsparsity=0$ to obtain the optimal empirical loss value 
\smash{$\emploss{\data}{\h_{\data}}$}, which by definition constitutes the reference value for the Rashomon set computation. We then formulate and solve the following problem:
\begin{align}
    \min_{\h}&\,\,\,\,\valueSP(\h,\data)  \label{eq:opt_fairness}\\
    \text{s.t.}& \quad \h \in \H \nonumber\\
    & \quad \text{Sparsity}(\h) \leq \sparsityvalue \nonumber\\
    & \quad \emploss{\data}{\h}\leq \emploss{\data}{\h_{\data}}+\epsilon \nonumber
\end{align}
Here, $\sparsityvalue$ represents the desired sparsity, which sets an upper bound on the model's size, and \smash{$\valueSP(\h,\data)$} is the 
statistical parity metric, although any other fairness measure can replace it. By reversing the sign of the objective, the full range of fairness 
values within the $\epsilon$-Rashomon set can be characterized. Additionally, by varying the desired sparsity level~$\sparsityvalue$, the impact 
of sparsity constraints on the accuracy-fairness trade-off can be further explored.

\subsection{Instantiation for Scoring Systems}

Scoring systems are sparse linear classification models with integer coefficients, widely used in fields like medicine and criminal justice due to their interpretability~\citep{rudin2022interpretable}. To make a prediction with such a model on a given example $\attributes[]$, one multiplies each feature's value $x_j$ by its corresponding coefficient $\slimcoefficient[j]$ selected within an acceptable range of values $\slimcoefficientsrange{}_\feature \subset \N$, sums the results, and compares the total to a fixed threshold. %An example scoring system for binary attributes is provided in Table~\ref{tab:example_scoring_system}. 
The hypothesis space of scoring systems 
is then:
\begin{align*}
     &\H := \big\{\attributes[] \mapsto\  \text{sign}(\attributes[]^T\slimcoefficients{})
    \mid \slimcoefficient[j]\in \slimcoefficientsrange{}_\feature, \feature=1..\nfeatures\big\}.
\end{align*}
Table~\ref{tab:example_scoring_system} presents an example scoring system trained on the Default of Credit Card Clients dataset~\cite{yeh2009comparisons}.
The classification task involves predicting whether a person will default on payment based on demographic information and payment histories. In addition to the coefficients associated with the $\nfeatures$ features (only non-zero ones are shown), an additional \textbf{threshold} is included. This threshold is usually handled by concatenating an additional feature with a value of $1$ to all examples before training or inference.
As visible in the table, the modelâ€™s interpretability allows for straightforward identification of the features influencing predictions. For instance, features indicating delays in previous payments or high payment amounts are associated with an increased likelihood of predicting a default on the next payment. However, the model also exhibits a bias against females, as the attribute ``SEX\_Female'' increases the computed score, thereby increasing the probability of predicting a default for females. 
This aligns with the measured statistical parity value (Equation~(\ref{eq:sp})) of $-0.046$, whose negativity indicates a higher positive prediction rate for group $\agroup[2]$ (females) over group $\agroup[1]$. In this example, interpretability facilitates the detection of such discriminations.

\input{example_scoring_system}



\slim{} \emph{(Supersparse Linear Integer Model)}~\cite{slim2014} is a MILP formulation designed to learn optimal 
scoring systems. We use it within our framework to instantiate the learning problem defined in Problem~(\ref{eq:opt_general}). The original \slim{} formulation aims at finding the coefficients $\slimcoefficients{}_\data$ minimizing the following objective:
    \begin{align}
        \min_{\slimcoefficients{}} \,\,\, & \sum_{\example = 1}^{\nexamples} 
        \indicatorfunction[\labels[\example] \attributes[\example]^T \slimcoefficients{} \leq 0] + 
        \coeffsparsity \| \slimcoefficients{} \| _0 
        \label{eq:slim_obj}
    \end{align} 
where $\slimcoefficients{}$ is the vector of coefficients within the scoring system, $\attributes[\example]^T \slimcoefficients{}$ is the scoring system's total score 
for example~$\example$ (whose sign determines the output label), and $\coeffsparsity$ is a regularization coefficient. Then, \smash{$\sum_{\example = 1}^{\nexamples} 
\indicatorfunction[\labels[\example] \attributes[\example]^T \slimcoefficients{} \leq 0]$} computes the $0/1$ empirical loss of the model, while 
\smash{$\| \slimcoefficients{} \| _0$} is a sparsity regularizer, penalizing the number of non-zero coefficients. 

We now present our modified formulation, which instantiates Problem~(\ref{eq:opt_fairness}) to characterize fairness and sparsity within the Rashomon set of scoring systems. Recall that the optimal loss value \smash{$\slimbestloss{}$} is first obtained by solving the original \slim{} formulation with \smash{$\coeffsparsity=0$} (i.e., ensuring that objective~(\ref{eq:slim_obj}) focuses solely on predictive performance).

\begin{align}
\min_{\slimcoefficients{}} \quad & \frac{\sum_{\example \in \agroup[1]} \slimpredictions[\example]}{|\agroup[1]|} - 
    \frac{\sum_{\example \in \agroup[2]} \slimpredictions[\example]}{|\agroup[2]|}\label{constr:slimfair_objective}\\
\text{s.t.} \quad & \slimcoefficient[\feature] = \sum_{\slimcoeffvalueindex{} \in \slimcoefficientsrange_\feature} 
    \slimcoeffvalueindex{} \cdot \slimcoeffchoicevar_{j \slimcoeffvalueindex{}} &\feature \in \{1,..,\nfeatures\} \label{constr:slimfair_coeff_set}\\
 & \sum_{\slimcoeffvalueindex{} \in \slimcoefficientsrange_\feature}
    \slimcoeffchoicevar_{\feature \slimcoeffvalueindex{}} \leq 1 &\feature \in \{1,..,\nfeatures\}\label{constr:slimfair_coeff_choice}\\
 & \sum_{\feature=1}^\nfeatures \sum_{\slimcoeffvalueindex{} = 1}^{\slimcoefficientsrange_\feature} \slimcoeffchoicevar_{\feature \slimcoeffvalueindex{}} 
    \leq \sparsityvalue &\text{(Sparsity)}\label{constr:slimfair_sparsity}\\
 & \frac{1}{\nexamples} \sum_{\example = 1}^{\nexamples} \slimlossvariables[\example] \leq \slimbestloss+\epsilon &\text{(Performance)}\label{constr:slimfair_rashomon_set}\\
 & \bigMone[\example] \slimlossvariables[\example] \geq \gamma - \labels[\example] \attributes[\example]^T\slimcoefficients{} & 
    \example \in \{1,..,\nexamples\}\label{constr:slimfair_loss_1}\\
 & \bigMtwo[\example](1 - \slimlossvariables[\example]) \geq \labels[\example] \attributes[\example]^T\slimcoefficients{} & 
    \example \in \{1,..,\nexamples\}\label{constr:slimfair_loss_2}\\
 & \slimpredictions[\example] = (1-\slimlossvariables[\example])\indicatorfunction[\labels[\example] = 1] +\slimlossvariables[\example] \indicatorfunction[\labels[\example] = -1] & \example \in \{1,..,\nexamples\}\label{constr:slimfair_predictions}\\
 & \slimcoefficient[\feature] \in \slimcoefficientsrange{}_\feature  &\feature \in \{1,..,\nfeatures\} \nonumber\\
 & \slimlossvariables[\example] \in \{ 0, 1 \} &\example \in \{1,..,\nexamples\} \nonumber\\
 & \slimpredictions[\example] \in \{ 0, 1 \} &\example \in \{1,..,\nexamples\} \nonumber \\
 &  \slimcoeffchoicevar_{\feature \slimcoeffvalueindex{}} \in \{ 0, 1 \} &\hspace{-100pt}\feature \in \{1,..,\nfeatures\},~\slimcoeffvalueindex{} \in \slimcoefficientsrange_\feature\nonumber 
\end{align}

Each coefficient $\slimcoefficient[\feature]$ associated to feature $\feature$ within the scoring system must take a value within a user-defined domain~$\slimcoefficientsrange{}_\feature$.
 Specifically, Constraint~(\ref{constr:slimfair_coeff_set}) ensures that the coefficient $\slimcoefficient[\feature]$ takes value $\slimcoeffvalueindex{} \in \slimcoefficientsrange_\feature$ if and only if $\slimcoeffchoicevar_{\feature \slimcoeffvalueindex{}}=1$. Constraint~(\ref{constr:slimfair_coeff_choice}) guarantees that at most one value $\slimcoeffvalueindex{} \in \slimcoefficientsrange_\feature$ is set to $1$.
Note that $\slimcoefficient[\feature]=0$ if none of the variables $\slimcoeffchoicevar_{\feature \slimcoeffvalueindex{}}$ equals $1$.

Objective~(\ref{constr:slimfair_objective}) represents the statistical parity metric introduced in Equation~(\ref{eq:sp}). By minimizing it, we aim to find the scoring system that maximally favors the protected group $\agroup[2]$ over $\agroup[1]$. Reversing the sign of this difference allows us to optimize the fairness value in the opposite direction. Constraint~(\ref{constr:slimfair_rashomon_set}) limits the hypothesis space to the $\epsilon$-Rashomon set, 
leveraging the previously computed optimal loss \smash{$\slimbestloss$} (as defined in Equation~(\ref{eq:slim_obj})). Constraint~(\ref{constr:slimfair_sparsity}) restricts the number of non-zero coefficients in $\slimcoefficients{}$ to at most $\sparsityvalue$, thereby enforcing sparsity.

The remaining constraints handle the intermediate computations of the scoring system's predictive performance and predictions.
Specifically, the loss variables $\slimlossvariables[]$ indicate whether each example $\example$ is incorrectly classified: 
\smash{$\slimlossvariables[\example] = \indicatorfunction[\labels[\example] \attributes[\example]^T \slimcoefficients{} \leq 0]$}. These variables are determined by  
Constraints~(\ref{constr:slimfair_loss_1}--\ref{constr:slimfair_loss_2}), which compare the sign of each example $\example$'s predictions 
\smash{$(\attributes[\example]^T\slimcoefficients{})$} with its true label $\labels[\example]$. Note that \smash{$\bigMone[\example \in \{1..\nexamples\}]$} and 
\smash{$\bigMtwo[\example \in \{1..\nexamples\}]$} are pre-computed constants large enough to enforce the constraints, and $\gamma$ is a small constant 
representing a margin, ensuring that \smash{$(\labels[\example] \attributes[\example]^T\slimcoefficients{})$} for all examples $\example$ is lower-bounded. 

In the original \slim{} formulation, because the sum of the loss variables was minimized in the objective, 
Constraint~(\ref{constr:slimfair_loss_1}) alone was sufficient to set $\slimlossvariables[\example]$ to $1$ if and only if example $\example$ is misclassified. As this is no longer the case here, we must additionally include Constraint~(\ref{constr:slimfair_loss_2}) to force $\slimlossvariables[\example]$ to $0$ in case of correct classification.

Finally, the predictions $\slimpredictions[]$ are computed leveraging the loss variables and the actual labels (given as input constants to the model) 
through Constraint~(\ref{constr:slimfair_predictions}). For each example $\example$, we then have: 
$\slimpredictions[\example] = \indicatorfunction[\attributes[\example]^T \slimcoefficients{} > 0]$. 

This formulation precisely determines the extent to which each protected group can be favored over the other, given a specified sparsity level $\sparsityvalue$ 
(maximum number of non-zero coefficients) and predictive performance threshold (defined by the $\epsilon$-Rashomon set). By varying $\sparsityvalue$ and $\epsilon$, 
one can explore the trade-offs between these different desiderata.


\subsection{Instantiation for Decision Diagrams}

Decision diagrams are popular interpretable models exhibiting a top-down hierarchical structure similar to trees. Yet, unlike decision trees, their branches can be merged. This fundamental property avoids the replication and fragmentation problems of decision trees~\citep{oliver1992decision,kohavi1994bottom,ddiagrams}, hence enhancing interpretability. Formally, a decision diagram is a rooted directed acyclic graph $\graph=(\treestructure,\treestructurearcs)$, where each internal node $\node \in \treestructureinternal$ represents a splitting hyperplane and each terminal node $\node \in \treestructureleaves$ is uniquely associated with a prediction $\class_{\node}$. This hypothesis class generalizes rule-lists (\smash{$\H_{\text{rule-list}}\subset \H_{\text{diagrams}}$}), 
so investigating its $\epsilon$-Rashomon set is an enumeration-free alternative to the approach of \citet{DBLP:journals/corr/abs-2204-11285}.
As with \slim{}, the objective \smash{$\valueSP(\h,\data)$}, and Sparsity/Performance constraints of Problem~(\ref{eq:opt_fairness}) are easily 
expressed as linear functions of decision variables, allowing for a MILP formulation.

We build upon the original MILP formulation by \citet{ddiagrams} for learning optimal decision diagrams for classification. In essence, given a user-specified maximum structure, the formulation aims to determine which nodes and edges should be utilized within this structure and how their splitting hyperplanes should be defined. Sparsity is then computed as the number 
$\sparsityvalue$ of active (utilized) internal nodes. The objective is as follows:
\begin{align}
    \min_{(\nodeused[], \hyperplanea[], \hyperplaneb[])_{\node \in \treestructureinternal}} \,\,\, & \sum_{\example = 1}^{\nexamples} 
        \slimlossvariables[\example] + 
        \coeffsparsity \| \nodesused{} \| _0. 
        \label{eq:dds_objective}
\end{align}
Here, for each example $\example$, the loss variable $\slimlossvariables[\example]$ indicates whether it is misclassified, so $\smash{\sum_{\example = 1}^{\nexamples} \slimlossvariables[\example]}$ computes the $0/1$ loss. For each internal node within the predefined structure $\node \in \treestructureinternal$, the variable $\nodeused[\node] \in \{0, 1\}$ indicates whether it is used in the final structure. The term $\| \nodesused{} \| _0$ quantifies the sparsity of the resulting decision diagram by counting the number of nodes used in the trained structure.
Finally, for each internal node $\node$ where $\nodeused[\node]=1$, variables $(\hyperplanea[\node],\hyperplaneb[\node])$ define the hyperplane corresponding to the multivariate split performed by this node.
This objective effectively instantiates Problem~(\ref{eq:opt_general}).

We hereafter provide our modified formulation, which instantiates Problem~(\ref{eq:opt_fairness}) to characterize fairness and sparsity within the Rashomon set of decision diagrams. Recall that the optimal loss value \smash{$\ddsbestloss{}$} is first obtained by solving the original MILP formulation with \smash{$\coeffsparsity=0$} (i.e., ensuring that Objective~(\ref{eq:dds_objective}) focuses solely on predictive performance).

{\allowdisplaybreaks
\begin{align}
\min_{(\nodeused[], \hyperplanea[], \hyperplaneb[])_{\node \in \treestructureinternal}} \quad & \frac{\sum_{\example \in \agroup[1]} \slimpredictions[\example]}{|\agroup[1]|} - 
    \frac{\sum_{\example \in \agroup[2]} \slimpredictions[\example]}{|\agroup[2]|}\label{constr:dds_objective}\\
\text{s.t.} \quad
 & \frac{1}{\nexamples} \sum_{\example = 1}^{\nexamples} \slimlossvariables[\example] \leq \ddsbestloss{}+\epsilon &\text{(Performance)}\label{constr:dds_rashomon_set}\\
& \sum_{\node \in \treestructureinternal} \nodeused[\node] \leq \sparsityvalue &\text{(Sparsity)}\label{constr:dds_sparsity}\\
& \slimlossvariables[\example] = \sum_{\node \in \treestructureleaves} \indicatorfunction[\labels[\example]\neq\class_{\node}] \ddswvars[\example,\node] & \example \in \{1,..,\nexamples\} \label{constr:dds_loss}\\
& \slimpredictions[\example] = (1-\slimlossvariables[\example])\indicatorfunction[\labels[\example] = 1] + \slimlossvariables[\example] \indicatorfunction[\labels[\example] = -1] & \example \in \{1,..,\nexamples\}\label{constr:dds_predictions}\\
& \ddswvars[\example,\node]^+ + \ddswvars[\example,\node]^- = \begin{cases}
    1~\text{if}~\node = 0\\
    \sum_{\nodebis \in \delta^-(\node)}(\ddsflowvars^+_{\example\nodebis\node} + \ddsflowvars^-_{\example\nodebis\node})
\end{cases} & \example \in \{1,..,\nexamples\},~\node \in \treestructureinternal\label{constr:dds_flow_1} \\
& \ddswvars[\example,\nodebis]^- = \sum_{\node\in\delta^+(\nodebis)} \ddsflowvars^-_{\example\nodebis\node} & \example \in \{1,..,\nexamples\},~\nodebis\in \treestructureinternal \label{constr:dds_flow_2}\\
& \ddswvars[\example,\nodebis]^+ = \sum_{\node\in\delta^+(\nodebis)} \ddsflowvars^+_{\example\nodebis\node} & \example \in \{1,..,\nexamples\},~\nodebis\in \treestructureinternal \label{constr:dds_flow_3}\\
& \sum_{\nodebis \in \treestructureinternal_\ddslevel} \ddswvars[\example,\nodebis]^- \leq 1 - \ddslambdavar_{\example\ddslevel} &\example \in \{1,..,\nexamples\},~\ddslevel \in \{0,..,\ddsdepth-1\}\label{constr:dds_flow_integrity_1}\\
& \sum_{\nodebis \in \treestructureinternal_\ddslevel} \ddswvars[\example,\nodebis]^+ \leq \ddslambdavar_{\example\ddslevel} &\example \in \{1,..,\nexamples\},~\ddslevel \in \{0,..,\ddsdepth-1\}\label{constr:dds_flow_integrity_2}\\
& \nodeused[\nodebis] = \sum_{\node \in \delta^+(\nodebis)} \ddslinkingvars^+_{\nodebis\node} = \sum_{\node \in \delta^+(\nodebis)} \ddslinkingvars^-_{\nodebis\node} & \nodebis \in \treestructureinternal \label{constr:dds_set_node_used_1}\\
& \nodeused[\node] \leq \sum_{\nodebis \in \delta^-(\node)}(\ddslinkingvars^+_{\nodebis\node} + \ddslinkingvars^-_{\nodebis\node}) &\node \in \treestructureinternal\setminus\{0\} \label{constr:dds_set_node_unused}\\
& \ddslinkingvars^+_{\nodebis\node} + \ddslinkingvars^-_{\nodebis\node} \leq \nodeused[\node] &\nodebis\in \treestructureinternal,~\node\in\delta^+(\nodebis)\label{constr:dds_set_node_used_2}\\
& \ddsflowvars^+_{\example\nodebis\node} \leq \ddslinkingvars^+_{\nodebis\node} &\nodebis\in \treestructureinternal, \node\in\delta^+(\nodebis),~\example \in \{1,..,\nexamples\} \label{constr:link_flow_1}\\
& \ddsflowvars^-_{\example\nodebis\node} \leq \ddslinkingvars^-_{\nodebis\node} &\nodebis\in \treestructureinternal, \node\in\delta^+(\nodebis),~\example \in \{1,..,\nexamples\} \label{constr:link_flow_2}\\
& \ddslinkingvars^-_{\nodebis\node} + \sum_{\nodeter \in \delta^+(\nodebis), \nodeter\leq \node} \ddslinkingvars^+_{\nodebis\nodeter} \leq 1 &\nodebis\in \treestructureinternal,~\node\in\delta^+(\nodebis) \label{constr:symmetry_breaking_1}\\
& \sum_{\nodeter\in \delta^-(\nodebis)}(\ddslinkingvars^+_{\nodeter\nodebis} +\ddslinkingvars^-_{\nodeter\nodebis}) \geq \sum_{\nodeter\in \delta^-(\node)}(\ddslinkingvars^+_{\nodeter\node} +\ddslinkingvars^-_{\nodeter\node}) & \ddslevel \in \{2,..,\ddsdepth-1\},\nodebis,\node\in\treestructureinternal_\ddslevel,~\nodebis<\node \label{constr:symmetry_breaking_2}\\
& (\ddswvars[\example,\node]^- = 1) \implies (\hyperplanea[\node]^T\attributes[\example] + \gamma \leq \hyperplaneb[\node]) & \example \in \{1,..,\nexamples\},~\node \in \treestructureinternal \label{constr:hyperplane_1}\\
& (\ddswvars[\example,\node]^+ = 1) \implies (\hyperplanea[\node]^T\attributes[\example] > \hyperplaneb[\node]) & \example \in \{1,..,\nexamples\},~\node \in \treestructureinternal \label{constr:hyperplane_2}\\
& \ddswvars[\example,\node] = \sum_{\nodebis\in\delta^-(\node)} (\ddsflowvars^+_{\example\nodebis\node} + \ddsflowvars^-_{\example\nodebis\node}) &\node\in \treestructureleaves,~\example \in \{1,..,\nexamples\} \label{constr:leaves_assignments}
\end{align}
}

Objective~(\ref{constr:dds_objective}) represents the statistical parity metric introduced in Equation~(\ref{eq:sp}). By minimizing it, we aim to find the decision diagram that maximally favors the protected group $\agroup[2]$ over $\agroup[1]$ in terms of positive prediction rate. Reversing the sign of this difference allows us to constrain the fairness value in the opposite direction. 
Constraint~(\ref{constr:dds_rashomon_set}) limits the hypothesis space to the $\epsilon$-Rashomon set, utilizing the previously computed optimal loss \smash{$\ddsbestloss{}$}. Constraint~(\ref{constr:dds_sparsity}) restricts the number of active nodes in $\nodesused{}$ 
to at most $\sparsityvalue$, thereby enforcing sparsity.
Constraint~(\ref{constr:dds_loss}) sets the loss variable $\slimlossvariables[\example]=1$ if and only if example $\example$ is assigned to a terminal node $\node \in \treestructureleaves$ whose predicted class $\class_{\node}$ differs from the example's true label $\labels[\example]$. Constraint~(\ref{constr:dds_predictions}) then uses the loss variables $\slimlossvariables[]$ to determine the decision diagram's predictions~$\slimpredictions[]$.

The remaining constraints remain unchanged and model the structure of the constructed decision diagram. Below, we briefly discuss the role of each constraint, and we refer to \citet{ddiagrams} for a more comprehensive explanation.
Constraints~(\ref{constr:dds_flow_1}--\ref{constr:dds_flow_3}) model the flow of each example through the nodes of the decision diagram. Specifically, $\delta^-(\nodebis)$ (respectively, $\delta^+(\nodebis)$) represents the set of possible predecessors (respectively, successors) of node $\nodebis$ in the user-provided decision diagram structure.
The variable $\ddswvars[\example,\nodebis]^-$ (respectively, $\ddswvars[\example,\nodebis]^+$) takes a non-zero value when example $\example$ passes through node $\nodebis$ on the negative (respectively, positive) side of the separating hyperplane. Additionally, the variable $\ddsflowvars^-_{\example\nodebis\node}$ (respectively, $\ddsflowvars^+_{\example\nodebis\node}$) models the flow from the negative (respectively, positive) side of $\nodebis$ to other nodes~$\node$.
Constraints~\mbox{(\ref{constr:dds_flow_integrity_1}--\ref{constr:dds_flow_integrity_2})} ensure flow integrity using the binary variable $\ddslambdavar_{\example\ddslevel}$, which determines, for each example~$\example$, whether it follows the negative or positive side at each level $\ddslevel \in \{0,..,\ddsdepth-1\}$, $\ddsdepth$ being the depth of the decision diagram.
Constraints~(\ref{constr:dds_set_node_used_1}--\ref{constr:dds_set_node_used_2}) specify that a node $\nodebis$ is used in the decision diagram ($\nodeused[\nodebis]=1$) if and only if it is connected to or from another node. The binary variable $\ddslinkingvars^-_{\nodebis\node}$ (respectively, $\ddslinkingvars^+_{\nodebis\node}$) indicates that node $\nodebis \in \treestructureinternal$ links to node $\node$ on the negative (respectively, positive) side. Note that both the root and terminal nodes are excluded from these constraints, as they are always used.
Constraints~(\ref{constr:link_flow_1}--\ref{constr:link_flow_2}) connect the linking variables to the examples' flows.
Constraints~(\ref{constr:symmetry_breaking_1}--\ref{constr:symmetry_breaking_2}) implement symmetry breaking, as many equivalent topologies could result from the previously defined constraints and variables.
Finally, Constraints~(\ref{constr:hyperplane_1}--\ref{constr:hyperplane_2}) ensure consistency between the hyperplane variables and the flow of examples, and Constraint~(\ref{constr:leaves_assignments}) determines the terminal node $\node$ to which each example $\example$ is assigned by setting $\ddswvars[\example,\node]$ based on the previously computed flows.

\section{Experimental Study}

Our numerical experiments serve two main objectives.
First, we demonstrate the applicability and effectiveness of our framework in characterizing fairness and sparsity within Rashomon sets, through an instantiation for two hypothesis classes.
Second, we explore the interplays between the three desiderata, highlighting the main trends.

\subsection{Experimental Setup}

\paragraph{Datasets.} We consider three datasets widely used in the fair and interpretable machine learning literature. First, the UCI Adult Income dataset~\cite{Dua:2019} contains records on $32,561$ individuals from the 1994 U.S. census, described by $36$ binary attributes. The binary classification task is to predict whether an individual earns more than \$50K per year. In our experiments, $\agroup[1]$ represents males and $\agroup[2]$ represents females.
Second, the Default of Credit Card Clients dataset~\cite{yeh2009comparisons} includes demographic information and payment histories for $29,986$ individuals in Taiwan, each described by $21$ attributes. The task is to predict whether a person will default on payment, with $\agroup[1]$ as males and $\agroup[2]$ as females.
Third, the COMPAS dataset~\cite{angwin2016machine} contains data on $7,214$ criminal offenders in Broward County, Florida, described by $27$ binary attributes. The classification task is to predict whether an individual will re-offend within two years. Here, $\agroup[1]$ represents African-Americans, and $\agroup[2]$ represents the rest of the population.

\paragraph{Learning Procedure.}  For each dataset, we randomly sub-sample training sets $\data$ of size $\nexamples=500$, with the remaining 
examples used as a test set, as this permits a fast and unbiased evaluation based on optimal solutions of the underlying mathematical models.
We generate five different random splits and report both the average values and standard deviations 
in our experiments. The two fairness metrics considered are statistical parity (Equation~(\ref{eq:sp})) and equal opportunity 
(Equation~(\ref{eq:eo})).
For each random split of each dataset, we determine the optimal loss \smash{$\emploss{\data}{\h_{\data}}$} and the majority classifier 
loss \smash{$\dummyloss{}$}. Then, the $\epsilon$ parameter is chosen so that the loss upper bound lies between these two
extreme losses : \smash{$(1-p)\emploss{\data}{\h_{\data}} + p\dummyloss{}$} with \smash{$p \in \{ 1\%, 5\%, 10\%, 20\% \}$}. 
Notably, the $0\%$-Rashomon set includes the optimal models and the $100\%$-Rashomon set includes all models 
not worse than a majority classifier. 

\paragraph{Hyperparameters -- Scoring Systems.} 
In all our experiments using scoring systems, the set of possible values is the same for all coefficients: \smash{$\forall \feature \in \{1,..,\nfeatures\},~\slimcoefficient[\feature] \in \slimcoefficientsrange{}_\feature = 
\{0, \pm1, \pm2, \pm5, \pm10, \pm20, \pm30, \pm50 \}$}. 
Sparsity values $\sparsityvalue$ (i.e., maximum numbers of non-zero 
coefficients) range from $1$ to $\nfeatures+1$ (to account for the additional bias coefficient).

\paragraph{Hyperparameters -- Decision Diagrams}
Based on preliminary experiments, we fix the squeleton of the decision diagrams to a maximum of 12 internal nodes, distributed across $5$ consecutive levels as follows: $(1,2,3,3,3)$. We consider sparsity values (i.e., the maximum number of active nodes within the decision diagrams) ranging from $\sparsityvalue=4$ to $\sparsityvalue=12$.

\paragraph{Exploration of the Rashomon set.}
 We use the \texttt{Gurobi} solver~\citep{gurobi} through its Python binding to solve Problems~(\ref{constr:slimfair_objective}) for scoring systems and~(\ref{constr:dds_objective}) for decision diagrams. Each solver execution is done on 16 threads using a computing cluster with Intel Platinum 8260 Cascade Lake @2.4GHz CPUs.
 To speed up our experiments, we exploit the fact that increasing either the allowed sparsity value $\sparsityvalue$ or the Rashomon set parameter $\epsilon$ relaxes the problem, so we can rely on previously found solutions to hot start the solver. Specifically, each run (for a fixed dataset, random split, sparsity value $\sparsityvalue$, and Rashomon set parameter $\epsilon$) is limited to one hour of CPU time and 36 GB of RAM. For runs where no feasible solution was found or optimality was not proven, we reuse solutions obtained from more constrained versions of the problem (i.e., tighter values of $\sparsityvalue$ or $\epsilon$) and restart the solver. Convergence was reached in all runs after at most five such iterations.

\subsection{Results}\label{sec:results}

We now highlight our key empirical findings and illustrate each of them with a subset of representative results. Complete results, including all datasets, fairness metrics, $\alpha$ and $\epsilon$ parameters, are provided in 
the Appendix~\ref{appendix:complete_results} for both considered hypothesis classes.

\paragraph{Result 1. Sparsity restricts the range of achievable fairness values and may harm certain protected groups.}
As discussed earlier, tightening the enforced sparsity value $\sparsityvalue$ confines the search to a subset of the hypothesis space $\H_I \subset \H$, which can limit the trade-offs between various objectives~\citep{dziugaite2020enforcing}, including fairness and predictive performance. While this result could be expected, our framework allows us to precisely and certifiably quantify this effect. Furthermore, the extent to which sparsity restricts the possible trade-offs between fairness and predictive performance indicates the severity of the tension between the three desiderata.

\input{adult-min-scoring-systems}

For instance, Figure~\ref{fig:effect1} shows the minimum achievable fairness values within a $20\%$-Rashomon set of scoring systems as a function of the enforced sparsity $\sparsityvalue$ for both the statistical parity (left) and equal opportunity (right) metrics on the UCI Adult Income dataset. Negative values for both metrics indicate a bias in favor of group $\agroup[2]$ (females) in predicting high salaries. By quantifying the minimum achievable value, we effectively measure the extent to which females can be advantaged over males given the specified sparsity and performance desiderata.

As expected, tightening the sparsity $\sparsityvalue$ reduces the range of achievable fairness. This suggests that enforcing sparsity excludes models with extreme fairness values, highlighting a conflict between these two criteria. Notably, the left plot shows that scoring systems with fewer than $\sparsityvalue=20$ non-zero coefficients systematically disadvantage group $\agroup[2]$ (females), as indicated by the positive minimum fairness values. In other words, if high sparsity is legally required, the resulting outcome imbalance favoring group $\agroup[1]$ (males) could be justified under the principle of ``business necessity''. 

\paragraph{Result 2: Different fairness metrics exhibit different trade-offs with sparsity.}
A comparison of the two plots in Figure~\ref{fig:effect1} reveals that the impact of sparsity on the minimum achievable fairness within a $20\%$-Rashomon set varies depending on the fairness metric considered. Specifically, sparsity consistently disadvantages females in terms of statistical parity (left plot). However, this is not the case for equal opportunity (right plot), where the minimum achievable value remains negative. This difference can be attributed to the fact that, as shown in Equation~(\ref{eq:eo}), equal opportunity is conditioned on the true labels and therefore aligns more closely with predictive accuracy, whereas statistical parity does not. 


\input{tabs-default-credit-both-models}

\input{compas-full-scoring-systems}

\paragraph{Result 3. High predictive performance requirements restrict the range of achievable fairness values.}
Table~\ref{tab:effect2_ss} shows the minimum and maximum achievable statistical parity for different Rashomon set parameters $\epsilon$ for scoring systems on the Default of Credit Card Clients 
dataset. We compare two sparsity levels: $\sparsityvalue=15$ (corresponding to the scoring system with the best achievable loss) and $\sparsityvalue=9$ (a sparser, 
arbitrary value). As previously noted, the range of achievable fairness values narrows with tighter sparsity (smaller $\sparsityvalue$).
At fixed sparsity, tightening the predictive performance constraint $\epsilon$ further restricts the achievable fairness range. Again, since enforcing tighter performance requirements amounts to shrinking the Rashomon set, this result could be expected. However, the extent to which it is the case indicates the severity of the tension between the two desiderata. Furthermore, it also allows discovering systematic biases, which, since the approach certifiably finds the minimum and achievable fairness values, can be used as legal arguments. For instance, tightening the predictive performance constraint can systematically disadvantage 
certain protected groups, as evidenced by the fact that the maximum achievable fairness becomes negative for $\epsilon \leq 10\%$ when $\sparsityvalue=9$. 
This implies that females (group $\agroup[2]$) are (on average) predicted to default on payment more often than males (group $\agroup[1]$) in a systematic manner. In other words, if one wants to build a scoring system no further than $10\%$ from the best achievable predictive performance, and with no more than $9$ non-zero coefficients (for interpretability purposes), discriminating females (in terms of statistical parity) is certifiably inevitable considering the Default of Credit Card Clients dataset.


\paragraph{Result 4. Accuracy, fairness, and sparsity have complex interplays.}
Figure~\ref{fig:effect3} plots the minimum and maximum achievable fairness values as a function of the desired sparsity level $\sparsityvalue$ for different $\epsilon$ 
parameters $\epsilon$. The experiments were conducted on scoring systems using the COMPAS dataset and two fairness metrics. This visualization reveals the complex interplays 
between the three desiderata: predictive performance, fairness, and sparsity. As previously noted, enforcing tighter sparsity (smaller $\sparsityvalue$) narrows the range 
of achievable fairness values (represented by the gap between the minimum and maximum plotted curves of a given color). Considering tight predictive performance constraints also limits the achievable sparsity values, as indicated by the fact that the curves corresponding to small Rashomon set parameters are unable to reach the smallest sparsity values. For instance, scoring systems within the $1\%$-Rashomon set exhibit at least $14$ non-zero coefficients, while the $20\%$-Rashomon set contains scoring systems with only $5$ non-zero coefficients. Again, our approach offers a precise quantification of the tension between interpretability and predictive accuracy for a given hypothesis class. Here, ``business necessity" could justify the inability to reach a target sparsity value.

Moreover, the curves not being centered around zero 
highlights inherent conflicts between predictive performance and fairness. Additionally, when a curve crosses the x-axis, one protected group becomes systematically 
disadvantaged across all models in the Rashomon set. For instance, scoring systems with $\sparsityvalue=5$ non-zero coefficients within a $20\%$-Rashomon set 
have a minimum statistical parity value of $18.7\%$, meaning that models in the Rashomon set consider higher recidivism risks for African-Americans. 
Furthermore, as can be seen in the complete results provided in the Appendix~\ref{appendix:complete_results}, the trade-offs between accuracy, fairness and sparsity are also influenced by the training data. While further investigations on this aspect could be conducted, key factors include the data intrinsic biases towards the considered protected groups, as well as the complexity of the underlying classification task.

\paragraph{Result 5. The complexity of the hypothesis class at hand strongly influences the observed trade-offs.}
Table~\ref{tab:effect2_dds} reports the minimum and maximum achievable statistical parity for different Rashomon set parameters $\epsilon$, based on our experiments with decision diagrams. We compare two sparsity levels: $\sparsityvalue=7$ (corresponding to the decision diagram with the best achievable loss) and $\sparsityvalue=4$ (a sparser value).
The main trends align with the key findings from our experiments on scoring systems: for a fixed sparsity $\sparsityvalue$, tightening the predictive performance requirement (smaller $\epsilon$) restricts the possible fairness ranges.
Similarly, for a fixed $\epsilon$, enforcing tighter sparsity further reduces the achievable fairness ranges.
The influence of the hypothesis class on the trade-offs between accuracy, fairness, and sparsity is evident when comparing the results in Table~\ref{tab:effect2_dds} with those in Table~\ref{tab:effect2_ss} (which correspond to scoring systems learned on the same data splits). Decision diagrams offer a broader range of trade-offs, with fairness ranges that are less constrained by performance and sparsity requirements. Notably, the minimum and maximum values of statistical parity systematically cross zero, implying that disparate impacts are hardly excusable by ``business necessity''. In other words, the resulting Rashomon sets systematically contain both models favoring group $\agroup[2]$ (females) and models favoring group $\agroup[1]$ (males).  These wider ranges are possible because the hypothesis class of decision diagrams is significantly more complex than that of scoring systems. Indeed, the considered decision diagrams partition the input space using multivariate splits~\citep{ddiagrams}, with each internal node functioning as a linear classifier.
In contrast, an entire scoring system corresponds to a linear classifier with integer coefficients: a single internal node of a multivariate decision diagram generalizes it, and we have: \smash{$\H_{\text{scoring systems}}\subset \H_{\text{diagrams}}$}, even for decision diagrams involving a single internal node.
However, this increased complexity comes at the expense of interpretability: understanding the resulting models is more difficult for humans due to the use of multivariate splits.
This explains why scoring systems remain very popular in high-stakes applications such as medicine~\citep{rudin2022interpretable}.
Indeed, the choice of the hypothesis space is another crucial dimension of the complex interplays between the considered ethical desiderata in machine learning. Thorough quantification of the trade-offs between fairness, sparsity, and predictive accuracy ---facilitated by our proposed framework--- can empower stakeholders to make informed decisions when navigating these complex interdependencies.

\section{Discussion}

This study has demonstrated that mathematical programming approaches can be used to explore the Rashomon set of any hypothesis class without enumeration by making generic modifications to a given baseline learning problem. Specifically, we introduced a framework to characterize fairness and sparsity within the Rashomon set and validated its versatility using two popular types of interpretable models: scoring systems and decision diagrams. The resulting tools enable the identification of sparser, less discriminatory alternative models, representing a significant step toward meeting legal and ethical requirements, despite the inherent challenges~\citep{laufer2024fundamental}.

Our extensive experiments highlighted the complex interplays between predictive accuracy, fairness, and sparsity. Our framework not only certifiably quantifies these interplays but also identifies model parameters leading to extreme values, effectively guiding the search for fairer and sparser alternatives. Importantly, we observed that imposing strict predictive performance or sparsity criteria might inherently disadvantage a protected group, underscoring the need for a thorough characterization of these trade-offs.

The research directions stemming from this work are diverse. First, we propose extending our generic framework to other hypothesis classes, such as rule-based models and tree ensembles, by leveraging recent advances in mathematical programming formulations for interpretable machine learning~\citep{DBLP:journals/eor/GambellaGN21,rudin2022interpretable}. Additionally, the declarative nature of the framework supports the integration of various additional desiderata, including alternative fairness or robustness metrics, as well as business-specific requirements. Overall, this makes it a promising tool for characterizing the tensions among key properties related to trustworthiness in machine learning.

%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}


%%
%% If your work has an appendix, this is the place to put it.
\appendix

\input{supplementary-material}


\end{document}
\endinput
%%
%% End of file `sample-manuscript.tex'.
