\section{Introduction to Conformal Prediction}\label{sec-intro-cp}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% shorhands for fonts stolen from djhsu
\def\ddefloop#1{\ifx\ddefloop#1\else\ddef{#1}\expandafter\ddefloop\fi}
\def\ddef#1{\expandafter\def\csname bb#1\endcsname{\ensuremath{\mathbb{#1}}}}
\ddefloop ABCDEFGHIJKLMNOPQRSTUVWXYZ\ddefloop
\def\ddef#1{\expandafter\def\csname bf#1\endcsname{\ensuremath{\mathbf{#1}}}}
\ddefloop ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\ddefloop
\def\ddef#1{\expandafter\def\csname bs#1\endcsname{\ensuremath{\boldsymbol{#1}}}}
\ddefloop ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\ddefloop
\def\ddef#1{\expandafter\def\csname sf#1\endcsname{\ensuremath{\mathsf{#1}}}}
\ddefloop ABCDEFGHIJKLMNOPQRSTUVWXYZ\ddefloop
\def\ddef#1{\expandafter\def\csname c#1\endcsname{\ensuremath{\mathcal{#1}}}}
\ddefloop ABCDEFGHIJKLMNOPQRSTUVWXYZ\ddefloop
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


In this section, we provide a brief introduction of
conformal prediction. 
Consider a classic supervised learning setting with 
$n$ independent and identically distributed (i.i.d.) samples 
$(x_1, y_1), \dots, (x_n, y_n) \in \cX \times \cY$.
Let $\mu: \cX \to \cY$ be our predictor.
Given a new test sample $(x, y)$ drawn from the same 
distribution, we want to give a valid prediction region
for $y$ based on $x$.
Formally, for a given failure probability $\alpha \in (0, 1)$,
our goal is to find a set $C$ for $y$ such that
\begin{equation}
    Pr \left(
        y \in C
    \right)
    \ge
    1 - \alpha.
\end{equation}
Conformal prediction offers a simple way to find such $C$.
At its core, it uses the following simple fact 
about exchangeable random variables.
\begin{lemma}
    \label{lem:quantile}
    Let $X, X_1, \dots, X_n$ be exchangeable random variables. 
    Let $X_{(k)}$ be the $k$-th smallest value among
    $X_1, \dots, X_n$.
    Then we have
    \[
        Pr \left(
            X \le X_{(k)}
        \right)
        =
        \frac{k}{n+1}.
    \]
\end{lemma}
\begin{proof}
For simplicity, assume that there are no ties among 
$X, X_1, \dots, X_n$ almost surely. The same arguments would still
apply but with more complicated notations.

Let $f$ be the joint density of $X, X_1, \dots, X_n$. Consider the 
event $E$ that $\{X, X_1, \dots, X_n\} = \{x_0, x_1, \dots, x_n\}$.
By exchangeability of $X, X_1, \dots, X_n$, we have
\[
    f(x_0, x_1, \dots, x_n) 
    = 
    f(x_{\sigma(0)}, x_{\sigma(1)}, \dots, x_{\sigma(n)}).
\]
Thus, for any permutation $\sigma$ of $0, \dots, n$.
Thus, for any $i \in \{0, \dots, n\}$, we have
\begin{equation}
\begin{aligned}
    Pr \left(
        X = x_i | E
    \right)
    &=
    \frac{
        \sum_{\sigma: \sigma(0) = i}
        f(x_{\sigma(0)}, x_{\sigma(1)}, \dots, x_{\sigma(n)})
    }{
        \sum_\sigma
        f(x_{\sigma(0)}, x_{\sigma(1)}, \dots, x_{\sigma(n)})
    } \\
    &=
    \frac{n!}{(n+1)!} \\
    &=
    \frac{1}{n+1}.
\end{aligned}
\end{equation}
It then follows that
\[
    Pr \left(
        \left. X \le x_{(k)} \right| E
    \right)
    =
    Pr \left(
        \left. X \le X_{(k)} \right| E
    \right)
    = \frac{k}{n+1}.
\]
But this holds for any other event $E'$ such that 
$\{X, X_1, \dots, X_n\} = \{x'_0, x'_1, \dots, x'_n\}$.
Therefore, we can marginalize and get
\[
    Pr \left(
        X \le X_{(k)}
    \right)
    = \frac{k}{n+1}.
\]\qed
\end{proof}
Let $\ell: \cY \times \cY \to \bbR$ be the nonconformity measure
that quantifies the quality of our prediction.
For example, one commonly used nonconformity measure is
the Euclidean distance: 
$\ell(y, \mu(x)) = \|y - \mu(x)\|_2$.
Let $s_i$ denote the nonconformity score of the $i$-th sample,
i.e. $s_i := \ell(y_i, \mu(x_i))$,
and let $s$ be the nonconformity score of $(x, y)$.
The main result offered by conformal prediction is the following:
\begin{theorem}
    \label{thm:cp}
    Given i.i.d. samples $(x_1, y_1), \dots, (x_n, y_n)$,
    a test sample $(x, y)$ from the same distribution,
    a predictor $\mu$,
    a nonconformity measure $\ell$ and corresponding
    nonconformity scores $s_1, \dots, s_n$.
    For a given failure probability $\alpha \in (0, 1)$,
    let $q := \lceil (n + 1) (1 - \alpha) \rceil$.
    Then the set
    \[
        C = 
        \left\{
            \hat{y} \in \cY: \ell(\hat{y}, \mu(x)) 
            \le 
            s_{(q)}
        \right\}
    \]
    satisfies 
    \[
        Pr \left(
            y \in C
        \right)
        \ge 1 - \alpha.
    \]
\end{theorem}
\begin{proof}
    Since $s_1, \dots, s_n$ are i.i.d., they are 
    exchangeable.
    Then applying Lemma~\ref{lem:quantile}, we get
    \[
        Pr \left(
            y \in C
        \right)
        =
        Pr \left(
            s \le s_{(q)}
        \right)
        =
        \frac{q}{n+1}
        \ge
        1 - \alpha.
    \]\qed
\end{proof}
This result offers a potential way to construct safety sets with rigorous probability guarantees purely from samples. This alleviates the need to know the underlying distribution, which can be really complex for robotic systems.