\documentclass[twoside]{article}

\usepackage[accepted]{aistats2025}
\usepackage{enumitem}
\usepackage{pifont}
\usepackage{threeparttable}
\usepackage{multirow}
\usepackage{fdsymbol}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{fdsymbol}
\usepackage{enumitem}
\usepackage{color}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{url}
\usepackage{soul}
\usepackage{amsthm}
\usepackage{pifont}
\usepackage{array}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{booktabs}

\newtheorem{Theorem}{Theorem}[section]
\newtheorem{Definition}{Definition}[section]
\newtheorem{Proposition}{Proposition}[section]
\newtheorem{Assumption}{Assumption}[section]

\theoremstyle{remark}
\newtheorem*{remark}{Remark}
% If your paper is accepted, change the options for the package
% aistats2025 as follows:
%
%\usepackage[accepted]{aistats2025}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.

% If you set papersize explicitly, activate the following three lines:
%\special{papersize = 8.5in, 11in}
%\setlength{\pdfpageheight}{11in}
%\setlength{\pdfpagewidth}{8.5in}

% If you use natbib package, activate the following three lines:
\usepackage[round]{natbib}
\renewcommand{\bibname}{References}
\renewcommand{\bibsection}{\subsubsection*{\bibname}}

% If you use BibTeX in apalike style, activate the following line:
\bibliographystyle{apalike}

\begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

\twocolumn[

\aistatstitle{Learning Identifiable Structures Avoids Bias in DNN-based Supervised Causal Learning}

\aistatsauthor{ Author 1 \And Author 2 \And  Author 3 }

\aistatsaddress{ Institution 1 \And  Institution 2 \And Institution 3 } ]

\begin{abstract}

% Causal discovery is a structured prediction task that asks to predict causal relations between variables based on a data sample of them, and Supervised Causal Learning (SCL) is an emerging paradigm in this field. 
% Causal discovery is a structured prediction task that aims to predict causal relations among variables based on their data samples. Supervised Causal Learning (SCL) is an emerging paradigm in this field. Existing Deep Neural Network (DNN)-based methods commonly adopt the “Node-Edge approach”, in which the model first computes an embedding vector for each variable-node, then uses these variable-wise representations to \emph{concurrently} and \emph{independently} predict for each directed causal-edge. In this paper, we first show that this architecture has some systematic bias that cannot be mitigated regardless of model size and data size. We then propose SiCL, a DNN-based SCL method that predicts a skeleton matrix together with a v-tensor (a third-order tensor representing the v-structures). According to the Markov Equivalence Class theory, both the skeleton and the v-structures are \emph{identifiable} causal structures, so predictions about skeleton and v-structures do not suffer from the identifiability limit in causal discovery, thus SiCL can avoid the systematic bias in Node-Edge architecture, and enable consistent estimators for causal discovery. Moreover, SiCL is also equipped with a specially designed pairwise encoder module with a unidirectional attention layer to model both internal and external relationships of pairs of nodes. Experimental results on both synthetic and real-world benchmarks show that SiCL significantly outperforms other DNN-based SCL approaches.

Causal discovery aims to infer causal relations among variables based on their data samples, and Supervised Causal Learning (SCL) is an emerging paradigm in this field. Existing Deep Neural Network (DNN)-based SCL methods commonly adopt the “Node-Edge” approach, where an embedding vector is computed for each variable node, and variable-wise representations are used to predict the existence of directed causal edges independently. In this paper, we first demonstrate that this approach introduces systematic biases that persist regardless of model size or data scale. To address these limitations, we propose \textbf{SiCL}, a novel DNN-based SCL method that predicts both a skeleton matrix and a v-tensor (a third-order tensor representing v-structures). According to the Markov Equivalence Class (MEC) theory, the skeleton and v-structures are identifiable, ensuring that SiCL avoids the identifiability limitations of edge-based predictions and enables consistent estimators for causal discovery. Furthermore, SiCL includes a pairwise encoder module with a unidirectional attention layer, allowing it to capture both internal and external relationships between pairs of nodes. Experimental results on both synthetic and real-world datasets demonstrate that SiCL significantly outperforms existing DNN-based SCL methods.
% , offering over 50\% improvement in Structural Hamming Distance (SHD) on a real-world benchmark.
\end{abstract}

\section{Introduction} \label{sec:int}
% Supervised Causal Learning (SCL) \cite{dai2023ml4c,ke2023learning} is a new learning paradigm that aims to learn causal relations from observational data in the supervised setting: the algorithm has access to datasets associated with ground truth causal relations and treats the inference process as learning the mapping from data to a causal structure, which is typically represented by a Directed Acyclic Graph (DAG) \cite{glymour2019review}. 
% The labeled data (i.e., the pairs of data and DAG) can be readily obtained via synthetic generation, or from a simulator if it is available \cite{lorchamortized}.

Causal discovery seeks to infer causal structures from an observational data sample.
Supervised Causal Learning (SCL) \citep{dai2023ml4c,ke2023learning,ma2022ml4s,lorchamortized} is an emerging paradigm in this field. The basic idea is to consider causal discovery as a \emph{structured prediction} task, and to train a prediction model using supervised learning techniques. 
% the algorithm has access to datasets associated with ground truth causal relations. The training process involves learning a mapping from data to the causal structure. 
%It has the training stage and the inference stage. 
% At training time, a dataset comprising a variety of different causal mechanisms and their associated data samples is generated, either via synthetic generation, or from a simulator if available \citep{lorchamortized}. 
At training time, a dataset comprising a variety of different causal mechanisms and their associated data samples is obtained (e.g. via synthetic generation, or via a simulator \citep{lorchamortized}). 
The prediction model is then trained to take such a data sample as input, and outputs predictions about the causal structures behind the data sample.
%During the inference stage, the causal structure is identified by simply applying the learned model to the target data.  
Compared to traditional rule-based or unsupervised 
%causal discovery 
methods~\citep{glymour2019review}, the SCL method has demonstrated strong empirical performance~\citep{dai2023ml4c,ma2022ml4s}, as well as robustness against sample size and distribution shift \citep{ke2023learning,lorchamortized}.

% Notably, Deep Neural Network (DNN) based SCL, where the prediction model for causal discovery is a DNN, is garnering attention because of several advantages, including its end-to-end training framework, which eliminates the needs of manual feature engineering, its capability to effectively handle both continuous and discrete data types, and the ability to learn latent representations.
% \textcolor{black}{Deep Neural Network (DNN)-based SCL is drawing attention due to its advantages. It provides an end-to-end training framework, removing the need for manual feature engineering. Additionally, it can handle both continuous and discrete data types effectively and learns latent representations.}
Deep Neural Network (DNN)-based SCL employs DNN as the prediction model, enabling end-to-end training and latent representation learning. 
% is drawing attention due to its advantages. 
% It provides an end-to-end training framework, removing the need for manual feature engineering. Additionally, it can handle both continuous and discrete data types effectively and learns latent representations.
% A specific DNN architecture, typically introduced by \citep{lorchamortized}, is particularly popular in recent DNN-based SCL approaches.
These methods typically utilize the following architecture, as introduced by \cite{lorchamortized}.
The model first transforms data samples into node-wise feature vectors, each representing a variable (a node in the causal graph).
Based on these features, the model then outputs a weighted adjacency matrix $A$, where $A_{ij}\in[0,1]$  estimates the probability of a directed edge $i \rightarrow j$ (indicating $i$ as a direct cause of $j$). 
Finally, the adjacency matrix of a predicted causal graph $G$ is inferred by $A$, where each entry $G_{ij} \in \{0,1\}$ is inferred \emph{independently} from estimated probability $A_{ij}$. 
For convenience, we call such a model architecture the ``Node-Edge'' architecture, as the representations are learned for individual nodes and the directed edges are predicted independently based on the corresponding estimated probability.
% the probability is estimated and sampled for individual directed edges. 
This architecture is common in both DNN-based SCL and general DNN-based causal discovery \citep{lorchamortized,Zhu2020Causal,dpdag,varamballydiscovering}.
% seems natural and has achieved encouraging results,}
However, we identify two limitations in the Node-Edge approach:

% \textcolor{black}{First, the Node-Edge architecture imposes a fundamental and unavoidable bias in the inferred causal relations.
% The reason is that it samples each edge separately and independently hence fails to capture the crucial inter-edge dependency in its probability estimation.
% Concretely, given an observational data sample $D$, the existence of a directed causal edge $i \rightarrow j$ may \emph{necessarily} depend on the existence of other edges, and yet the probability prediction $A_{ij}$ made by existing Node-Edge models is only conditioned on the sample $D$, not on the sampling result of other entries of $A$, thus has an inherent bias in capturing the inter-edge dependency.
% }
First, the Node-Edge architecture imposes a fundamental bias in the inferred causal relations. Specifically, given an observational data sample $D$, the existence of a directed causal edge $i \rightarrow j$ may \emph{necessarily} depend on the existence of other edges. However, the existing Node-Edge models predict each edge separately and independently, so the probability prediction $A_{ij}$ made by such models is only conditioned on the input sample $D$, not on the sampling result of other entries of $A$.  
Therefore, it fails to capture the crucial inter-edge dependency in its probability estimation.

As a simple example, a Node-Edge model maintaining the possibility of both $G_1: X\rightarrow T \rightarrow Y$ and $G_2: X\leftarrow T \leftarrow Y$ would necessarily have a non-zero probability to output the edges $X\rightarrow T$ and $T \leftarrow Y$, thus cannot rule out the possibility of $G_3: X\rightarrow T \leftarrow Y$, even though $G_3$ is impossible to be the groundtruth causal graph behind a data sample $D$ compatible with $G_1$ and $G_2$~\citep{verma1990equivalence}. 
% Crucially, it is known that, in general cases, there is no way to tell $G_1$ from $G_2$ based on observational data~\citep{andersson1997characterization,meek1995strong}, meaning that for any Node-Edge model to be sound, it has to maintain the possibility of both $G_1$ and $G_2$ (when observing a data sample compatible with any of them), which leads to an inevitable error probability to output the impossible graph $G_3$ on the other hand. 
Crucially, there is no way to tell $G_1$ from $G_2$ based on observational data in general cases~\citep{andersson1997characterization,meek1995strong}. 
It means that for any Node-Edge model to be sound, it has to maintain the possibility of both $G_1$ and $G_2$ (when observing a data sample compatible with any of them), leading to an inevitable error probability to output the impossible graph $G_3$ on the other hand. 


%First, the Bernoulli-sampling adjacency matrix approach can yield inconsistent results with respect to structure identifiability (identifiability for short). 
%    \textbf{Risk of Bernoulli-sampling adjacency matrix approach.} One inherent characteristic of causal learning is that a DAG is only identifiable up to its Markov equivalence class (MEC) \citep{andersson1997characterization,verma1990equivalence}, rendering it impossible to distinguish between two DAGs within the same MEC based on available data. 
    % Therefore, an optimal SCL learner will learn a probability between $0$ to $1$ on such identifiable edges with sufficient training data. 
    % What's worse, the output of the learner may contradict the MEC of the observed data if we sample from the output Bernoulli distribution.
    % One inherent characteristic of causal learning is that a DAG is only identifiable up to its Markov equivalence class (MEC) \cite{andersson1997characterization,verma1990equivalence}, rendering it impossible to distinguish between two DAGs within the same MEC based on available data. 
%    Considering a simulator that generates DAGs $G_1: X\rightarrow T \rightarrow Y$ and $G_2: X\leftarrow T \leftarrow Y$ where $X,T,Y$ are discrete random variables. As both $G_1$ and $G_2$ are within an MEC \citep{meek1995strong}, each observational data $D$ can be associated with either label $G_1$ or $G_2$. 
%    Even with sufficient training data generated by the simulator, if the Bernoulli-sampling adjacency matrix approach is adopted, an optimal SCL learner will learn that $0<P_{XT}<1$  and $0<P_{TY}<1$ for a test case $D$ (generated by the simulator). 
%        Consequently, there exists a non-zero probability of $P_{XT} (1-P_{TY} )$ resulting in the output of a v-structure $X\rightarrow T \leftarrow Y$, which contradicts the observed data. A concrete case study is elaborated in Sec. \ref{sec:met:lim}.
    %     Even with sufficient training data generated by the simulator, if the Bernoulli-sampling adjacency matrix approach is adopted, an optimal SCL learner will learn \textcolor{black}{that $P_{XT} = 0.5$  and $P_{TY} = 0.5$ for a test case $D$ (generated by the simulator). 
    % Consequently, there exists a non-zero probability of $0.25$ }resulting in the output of a v-structure $X\rightarrow T \leftarrow Y$, which contradicts the observed data. A concrete case study is elaborated in Sec. \ref{sec:met:lic}.
    % and $0<P_{T Y}<1$ for a test case $D$
    % Considering a simulator that generates DAGs only from two DAGs $G_1: X \rightarrow T \rightarrow Y$ and $G_2: X \leftarrow T \leftarrow Y$ where $X, T, Y$ are discrete random variables. 
    % 
    % As both $G_1$ and $G_2$ are within an MEC \cite{meek1995strong}, each observational data $D$ can potentially be associated with either label $G_1$ or $G_2$. 
    % With sufficient training data generated by the simulator, if adjacency matrix is used as learning target, an optimal SCL learner will learn that $0<P_{X T}<1$ and $0<P_{T Y}<1$ for a test case $D$ (generated by the simulator). 
    % Consequently, there exists a non-zero probability of $P_{X T}\left(1-P_{T Y}\right)$ resulting in the output of a v-structure $X \rightarrow T \leftarrow Y$, which contradicts the observed data. 

\textcolor{black}{Second, the Node-Edge architecture does not explicitly represent the features about node pairs, which we argue are essential for observational causal discovery.}
% However, we argue that such pairwise features are essential in observational causal discovery.}
% Second, we argue that information/features about not only individual nodes, but about node pairs, are essential in observational causal discovery; yet, such pairwise features are not explicitly represented in the existing Node-Edge models. 
For example, a causal edge $X\rightarrow Y$ can exist only if the node pair $\langle X, Y\rangle$ demonstrates \textit{persistent dependency} \citep{ma2022ml4s,spirtes2000causation}, meaning that $X$ and $Y$ remain statistically dependent regardless of conditioning on any subset of other variables. As another example, for causal DAGs, a sufficient condition to determine the causal direction between a persistently dependent node pair $\langle X, Y\rangle$ is that $X$ and $Y$ exhibits \emph{orientation asymmetry}, meaning that there exists a third variable $Z$ such that $X$ is persistently dependent to $Z$ but $Y$ can become independent to $Z$ conditioned on a variable-set $\mathbf{S}\not\ni X$ (or vice versa). A feature like persistent dependency or orientation asymmetry is, in its nature, a collective property of a node pair, but not of any individual node alone. 
%While in theory, a Node-Edge model might possibly encode these pairwise information as node-wise features -- such as in the form of predicate \texttt{is\_persistently\_dependent\_to\_Y(X)} -- 
%such encoding may require a huge feature space, with size linearly growing with the number of variables, and 
%this kind of ``unnatural'' encoding could be hard to learn without special structural support in the NN architecture.

%Second, although current attention mechanisms successfully capture sample-wise or node-wise invariance or equivariance, they do not adequately encode the essential causal information for learning causal structure.
%    \textbf{Essential causal information. }Causal learning encompasses two fundamental tasks: the determination of the adjacency relationship between each pair of variables (skeleton learning) and the identification of the causal directions (orientation) between adjacent variables \citep{yu2016review}. 
%    The essential information required for skeleton learning is termed \textit{persistent dependency} \cite{spirtes2000causation,ma2022ml4s}, indicating that a pair of variables are adjacent in the DAG if and only if they remain dependent regardless of conditioning on any subset of other variables. 
%    For orientation, an example of the essential information is termed \textit{orientation asymmetry}. 
%    After obtaining the skeleton from the first task, we proceed to orient each unshielded triple $X-T-Y$ into a v-structure by identifying a set of variables $\mathbf{S}$, satisfying $X \perp Y \mid \mathbf{S}$ and $T$ $\notin \mathbf{S}$.
%    This process necessitates distinct information for the pair $\langle X, Y\rangle$ compared to the information required for the variable $T$. 
%    This asymmetry poses a challenge in encoding potential conditional dependencies of the $\langle X, Y\rangle$, which is also relevant to their persistent dependency. 
%    \textcolor{black}{These essential pieces of information may not be adequately encoded by current model architectures, which is observed in our empirical experiments.}

To address these limitations, in this paper, we propose a novel DNN-based SCL approach, called Supervised Identifiable Causal Learning (SiCL). 
% Motivated by multiple constraint-based methods using a two-step paradigm (i.e., predicting the skeleton and v-structures separately),
% The neural network in our approach does not seek to predict the probabilities of directed edges, but tries to predict a skeleton matrix together with v-tensor, a third-order tensor representing the v-structures (notably, the previously utilized adjacency matrix is a second-order tensor in comparison). 
\textcolor{black}{The neural network in SiCL does not seek to predict the probabilities of directed edges, but tries to predict a skeleton matrix together with v-tensor, a third-order tensor representing the v-structures.}
According to the Markov Equivalence Class (MEC) theory of causal discovery, skeleton and v-structures are \emph{identifiable} causal structures (while the directed edges are not), so predictions about skeleton and v-structures do not suffer from the (non-)identifiability limit. 
By leveraging this insight, our theory-inspired DNN architecture completely avoids the systematic bias in edge-prediction models as previously discussed, and enables \emph{consistent} neural-estimators\footnote{Recall that a statistical estimator is \emph{consistent} if it converges to the groundtruth given infinite data.} for causal discovery. 
Moreover, SiCL is also equipped with a specially designed pairwise encoder module with a unidirectional attention layer.
With both node features and node-pair features as the layer input, it can model both internal and external relationships of pairs of nodes.
%SiCL is trained on synthetic data, and it shows significant improvement on both skeleton prediction and orientation tasks compared to other DNN-based approaches. 
Experimental results on both synthetic and real-word benchmarks show that \textcolor{black}{SiCL} can effectively address the two \textcolor{black}{above-mentioned limitations}, and the resulted SiCL solution significantly outperforms other DNN-based SCL approaches by over 50\% improvement on SHD (Structural Hamming Distance) metric of the real-world Sachs data. 
% \textcolor{red}{by how much? add a sentence to summarize the quantitative results}.

\section{Background and Related Work}\label{sec:bg}
%\section{Background} \label{sec:bg}
% \subsection{Basic Notations}
%Causal graph model
% Identifiability  persist depend
% -> CPDAG
%\subsection{ Causal Graphical Model}
A Causal Graphical Model is defined by a joint probability distribution $P$ over multiple random variables and a DAG $G$. Each node $X_i$ in $G$ represents a variable in $P$, and a directed edge $X_i \rightarrow X_j$ represents a direct cause-effect relation from $X_i$ to $X_j$.
A causal discovery task generally asks to infer about $G$ from an i.i.d. sample of $P$.

%\subsection{Identifiability} 
However, there is a well-known identifiability limit for causal discovery, that the causal DAG $G$ is, in general, only identifiable up to its Markov equivalence class (MEC).
Studies of this identifiability limit have led to a well-established theory \citep{frydenberg1990chain,verma1990equivalence}, which we briefly present in the following.


A \textit{skeleton} $E$ defined over the data distribution $P$ is an undirected graph where an edge exists between $X_i$ and $X_j$ if and only if $X_i$ and $X_j$ are always dependent in $P$, i.e., $\forall Z \subseteq\left\{X_1, X_2, \cdots, X_d\right\} \backslash \left\{X_i, X_j \right\}$, we have $X_i \nperp X_j | Z$.
%  A skeleton is defined as follows:
% \begin{Definition}[Skeleton]
%     A skeleton $E$ defined over the data distribution $P$ is an undirected graph where an edge exists between $X_i$ and $X_j$ if and only if $X_i$ and $X_j$ are always dependent in $P$, i.e., $\forall Z \subseteq\left\{X_1, X_2, \cdots, X_d\right\} \backslash \left\{X_i, X_j \right\}$, we have $X_i \nperp X_j | Z$.
% \end{Definition}
%The distribution $P$ is Markovian w.r.t. $G$, i.e., $
%    P\left(X_1, X_2, \cdots, X_d\right)=\prod_{i=1}^{d} P\left(X_i \mid pa_i^G\right)$,
%where $pa_i^G$ is the parent set of $X_i$ in $G$.
%In this work, we assume causal sufficiency, i.e., there are no latent common causes of any variables in the graph.
Under mild assumptions (such as that $P$ is Markovian and faithful to the DAG $G$; see details in Appendix Sec. \ref{sec:da}), 
the skeleton is the same as the corresponding undirected graph of the DAG $G$ \citep{spirtes2000causation}. 
% Unshielded Triples and v-structures are defined as follows:
% \begin{Definition}[Unshielded Triples (UTs) and v-structures]
A triple of variables $\langle X, T, Y \rangle$ is an \textit{Unshielded Triple (UT)} if $X$ and $Y$ are both adjacent to $T$ but not adjacent to each other in (the skeleton of) $G$.
It becomes a \textit{v-structure} denoted as $X \rightarrow T \leftarrow Y$ if the directions of the edges are from $X$ and $Y$ to $T$.

% \end{Definition}
% Formally, the Markov equivalence class is defined as follows:
% \begin{Definition}[Markov Equivalence]
Two graphs are Markov equivalent if and only if they have the same skeleton and v-structures. 
The \textit{Markov equivalence class (MEC)} can be represented by a \textit{Completed Partially Directed Acyclic Graph (CPDAG)} consisting of both directed and undirected edges. We use $CPDAG(G)$ to denote the CPDAG derived from $G$.
% \end{Definition}
According to the theorem of Markov completeness \citep{meek1995strong}, 
%assuming causal sufficiency and $P$ is Markovian and faithful w.r.t. $G$, 
we can only identify a causal graph up to its MEC, i.e., the CPDAG, unless additional assumptions are made (see the remark below).
%, for disc)ete data or linear Gaussian data. 
This means that each (un)directed edge in $CPDAG(G)$ indicates a (non)identifiable causal relation.
% \begin{Definition}[Identifiability]
%         Assuming $P$ is Markovian and faithful w.r.t. DAG $G$ and causal sufficiency, then each (un)directed edge in $CPDAG(G)$ indicates a (non)identifiable causal relation.
% \end{Definition}

\textbf{Remark:} The MEC-based identifiability theory is applicable in the general-case setting, when we take into account all possible distributions $P$. 
% \textcolor{red}{It is known that this identifiability limit could be broken (that is an undirected edge in the CPDAG could be oriented), \textit{if} we assume that only a certain class of distributions, such as linear non-Gaussians or additive noise models, are possible~\citep{peters2014causal,shimizu2011directlingam}. }
\textcolor{black}{It is known this identifiability limit could be broken (i.e., an undirected edge in the CPDAG could be oriented) \textit{if} we assume that the data follows some special class of distributions, e.g., linear non-Gaussians or additive noise models~\citep{peters2014causal,shimizu2011directlingam}. }
These assumptions are sometimes hard to verify in practice, so this paper considers the general-case setting.
%These assumptions are however counterfactual in nature, and could sometimes be hard to verify in practice. 

%The definitions of both skeleton and CPDAG are applicable for general data types, and not necessarily restricted to linear Gaussian or discrete data. So it is a clear target we can pursue. Regarding orientation, Markov completeness theorem states that for discrete or linear Gaussian data, we can only identify a causal graph up to its CPDAG; for continuous data with linear non-Gaussian mechanisms or additive noise assumptions, we can orient more causal directions , which is not our focus.





\subsection{Related Work}
In traditional methods of causal discovery, constraint-based methods are mostly related to our work.
%are categorized into constraint-based, score-based and continuous optimization. 
They aim to identify the DAG that is consistent with inter-variable conditional independence constraints. 
These methods first identify the skeleton and then conduct orientation based on v-structure identification \citep{yu2016review}. 
The output is a CPDAG which represents the MEC.
Notable algorithms in this category include PC \citep{spirtes2000causation}, along with variations such as Conservative-PC \citep{ramsey2012adjacency}, PC-stable \citep{colombo2014order}, and Parallel-PC \citep{le2016fast}. 
Compared to constraint-based methods, both our approach and theirs are founded upon the principles of MEC theory for estimating skeleton and v-structures. However, whereas traditional methods rely on symbolic reasoning based on explicit constraints, we employ DNNs to capture the essential causal information intricately linked with these constraints.
%Score-based methods aim to find an optimal DAG according to a predefined score function, subject to combinatorial constraints. 
%These methods employ specific optimization procedures such as forward-backward search GES \citep{chickering2002optimal}, hill-climbing \citep{koller2009probabilistic}, and integer programming \citep{cussens2011bayesian}.
%Continuous optimization methods transform the discrete search procedure into a continuous equality constraint.
%NOTEARS \citep{zheng2018dags} formulates the acyclic constraint as a continuous equality constraint and is further extended by DAG-GNN \citep{yu2019dag}, DECI \cite{geffner2022deep} to support non-linear causal relations. 
% DECI (Deep End-to-end Causal Inference, please refine) is a neural variational DAG learning method handles non-linear data.
%DECI \cite{geffner2022deep} is a flow-based model which can perform both causal discovery and inference on non-linear additive noise data.
%These methods can be viewed as unsupervised optimization since they do not access additional datasets associated with ground truth causal relations. 
For the score-based and continuous optimization methods, we refer to our Appendix Sec. \ref{sec:app:rw}, \cite{glymour2019review} and \cite{vowels2022d} for a thorough exploration of this literature. 


SCL begins from orienting edges in the bivariate cases under the functional causal model formalism. 
%The task is to predict the causal direction (i.e., whether $X \rightarrow Y$ or $X \leftarrow Y$) given a set of cause-effect samples (dataset with binary labels). 
% Supervised methods such as RCC \citep{lopez2015randomized} and NCC \citep{lopez2017discovering} have demonstrated superior performance in predicting pairwise causal relations compared to unsupervised approaches like ANM \citep{hoyer2008nonlinear} or IGCI \citep{janzing2012information}.
Methods such as RCC \citep{lopez2015randomized} and NCC \citep{lopez2017discovering} have outperformed unsupervised approaches like ANM \citep{hoyer2008nonlinear} or IGCI \citep{janzing2012information}.
For multivariate cases, ML4S \citep{ma2022ml4s} proposes a supervised approach specifically for skeleton learning. 
%It employs an order-based cascade learning procedure and generates training data from vicinal graphs. 
Complementary to ML4S, ML4C \citep{dai2023ml4c} takes both data and skeleton as input and classifies unshielded triples as either v-structures or non-v-structures. 
%Although ML4S and ML4C have demonstrated impressive empirical performance, it should be noted that both methods require manual feature engineering and are only applicable to discrete data.

DNN-based SCL has emerged as a prominent approach for enabling end-to-end causal learning. 
Two notable works in this line, namely AVICI \citep{lorchamortized} and CSIvA \citep{ke2023learning}, introduced an alternating attention mechanism to enable permutation invariance across samples and variables. Both methods learn individual representation for each node, which is then used to predict directed edges. Among them, AVICI considers the task of predicting DAG from observational data and adopts exactly the Node-Edge architecture, hence suffers from the issues as discussed in Sec. \ref{sec:int}. On the other hand, CSIvA requires additional interventional data as input to identify the full DAG, and applies an autoregressive DNN architecture where edges are predicted sequentially by multiple inference runs. Therefore, this autoregressive approach incurs very high inference cost due to the quadratic number of model runs required (w.r.t. the number of variables in question), as we experimentally verify in Appendix Sec. \ref{sec:auto}. In contrast, the method proposed in this paper only requires a single run of the DNN model. Besides that, our method also differs from both AVICI and CSIvA in terms of the usage of pairwise embedding vectors.
% A recent work ACD \cite{lowe2022amortized} presents a neural network-based methodology specifically tailored for causal discovery in time-series data. 
% Our research is primarily clear to two other notable methods, namely AVICI \citep{lorchamortized} and CSIvA \citep{ke2023learning}. 
% They introduce an alternating attention mechanism to enable permutation invariance across samples and variables. 
% Each node learns an individual representation, which is further used to predict directed edges.
% \textcolor{black}{
% Among them, CSIvA requires additional interventional data as input to identify the full DAG.
% It applies an autoregressive DNN architecture where edges are predicted sequentially by multiple inference runs. 
% Therefore, this autoregressive approach incurs high time costs due to the quadratic number of sequential inference runs w.r.t. number of variables required, as we experimentally verify in Appendix Sec. \ref{sec:auto}.
% On the other hand, AVICI considers the task of predicting DAG from observational data and adopts the Node-Edge architecture, hence suffers from the issue of inconsistency w.r.t. identifiability. 
% }
% \textcolor{black}{It also faces high time costs, as it adopts the self-regressive architecture and requires multiple inference runs to predict the edges.}
% In contrast, we propose predicting the skeleton and v-structures to address the issue of inconsistency w.r.t. identifiability. 
% We further design pairwise representations built upon the alternating attention mechanism to capture essential causal information.

% \section{Limitation of the Node-Edge \textcolor{black}{Architecture}} \label{sec:met:lim}

% % \subsection{Motivation} 

% % \subsubsection{Case Study of Limitation of Bernoulli-sampling adjacency matrix approach}

% %\paragraph{Limitation of the Node-Edge approach.} 
% \textcolor{black}{The Node-Edge architecture is common and has been adopted to generate the output DAG $G$ in the literature \citep{lorchamortized,Zhu2020Causal,dpdag,varamballydiscovering}.}
% % In previous work, the Node-Edge \textcolor{black}{architecture} is adopted to generate the output DAG $G$ \citep{lorchamortized}.
% In this \textcolor{black}{architecture}, each entry $G_{ij}$ in the DAG is independently sampled from $A_{ij}$, an entry in the adjacency matrix $A$. This entry $A_{ij}$ represents the probability that $i$ directly causes $j$. 
% We introduce a simple yet effective example setting with only three variables $X$, $Y$, and $T$ to reveal its limitation.

% \textcolor{black}{Considering a simulator that generates DAGs with equal probability from two DAGs: }$G_1: X \rightarrow T \rightarrow Y$ and $G_2: X \leftarrow T \leftarrow Y$.
% In $G_1$, the parametrized forms are $X \sim \mathcal{N} (0, 1)$, $T = X + \mathcal{N}(0, 1)$, and $Y = T + \mathcal{N}(0, 1)$.
% In $G_2$, the parametrized forms are $ Y = \mathcal{N}(0, 3)$, $T = \frac{2}{3}Y + \mathcal{N}(0, \frac{2}{3})$, and $X = 0.5T + \mathcal{N}(0, 0.5)$.
% The observational datasets coming from DAGs $G_1$ and $G_2$ follow the same joint distribution, which makes them inherently indistinguishable.

%  When using the adjacency matrix of a DAG as the learning target, an optimal neural network trained with binary cross-entropy loss will predict $0.5$ probabilities on the directions of the two edges.
% As the prediction is regarded as a Bernoulli distribution, with $0.25$ probability the sampling result is $X \rightarrow T \leftarrow Y$ (see Fig. \ref{fig:ps} in the Appendix).
% %It is incompatible with the observational data, resulting in a contradictory causal structure.
% This error probability is rooted from the fact that the Bernoulli sampling of the edge $X \rightarrow T$ is not conditioned on the sampling result of the edge $T \leftarrow Y$. Consequently, it is a bias that cannot be avoided even if the DNN has perfectly modeled the \emph{marginal probability} of each edge (marginalized over other edges) given input data.

% % It turns out that 0.25 is not the worst-case error rate yet. For example, the following proposition shows that for causal graphs with star-shaped skeleton, Node-Edge models can suffer from an inevitable error rate of 0.2642.  

% % \begin{Proposition}
% % Let $\mathcal{G}$ be the set of graphs where there is a central node $y$ such that (1) every other node is connected to $y$, (2) there is no edge between the other nodes, (3) there is at most one edge pointing to $y$. 
% % Denote $\mathcal{G}_n$ as the subset of $\mathcal{G}$ with graphs containing $n$ non-central nodes and $MEC_{\mathcal{G}_n}$ as the shared MEC for all $G$s in $\mathcal{G}_n$.
% % Denoting $Q_n$ as a distribution over $\mathcal{G}_n$ and $D_n$ as an i.i.d. sample of a joint distribution (over the $n+1$ variables) that is Markov compatible with all graphs in $\mathcal{G}_n$, for any Node-Edge model $M$ optimally trained on $Q_n$,  we have 
% %     \begin{align}
% % \sup_n \max_{Q_n} P_{G \sim M(D_n)}(G \nin MEC_{\mathcal{G}_n}) = 
% % 1 - \frac{2}{e} \approx 0.2642,
% %     \end{align}
% %     where $M(D_n)$ is denoted as the output graph distribution of $M$ with the given input data $D_n$.
% % \end{Proposition}
% % % \end{Proposition}
% % \color{black}

% \color{black}
% We further find that $0.25$ is not the worst-case error rate yet. 
% Formally, for a distribution $Q$ over a set of graphs, we define the graph distribution where the edges are independently sampled from the marginal distribution as $M(Q)$, i.e., for any causal edges $e_1$ and $e_2$, $P_{G\sim Q}(e_1 \in G) = P_{G\sim M(Q)}(e_1 \in G) = P_{G\sim M(Q)}(e_1 \in G | e_2 \in G)$.
% In general, a Node-Edge model optimally trained on data sample $D$ associated with the distribution $Q$ essentially predicts $M(Q)$ for given $D$.
% The following proposition shows that for causal graphs with star-shaped skeleton, there exists a rate of $0.2642$ that the graph with edges sampled from the marginal distributions is incorrect.
% % Node-Edge models can suffer from an inevitable error rate of 0.2642.  
% % For example, the following proposition shows that for causal graphs with star-shaped skeleton, Node-Edge models can suffer from an inevitable error rate of 0.2642.  

% % \begin{Proposition}
% % Let $\mathcal{G}_n$ be the set of graphs with $n+1$ nodes where there is a central node $y$ such that (1) every other node is connected to $y$, (2) there is no edge between the other nodes, (3) there is at most one edge pointing to $y$. 
% % For any distribution $Q$ over $\mathcal{G}_n$, let $M(Q)$ be another distribution over $\mathcal{G}_n$ such that for any causal edges $e, e'$, $P_{G\sim Q}(e \in G) = P_{G\sim M(Q)}(e \in G) = P_{G\sim M(Q)}(e \in G | e' \in G)$. We have 
% % \begin{align}
% % \sup_n \max_{Q} P_{G \sim M(Q)}(G \nin \mathcal{G}_n) = 
% % 1 - \frac{2}{e} \approx 0.2642,
% % \end{align}
% % \end{Proposition}
% \begin{Proposition}
% Let $\mathcal{G}_n$ be the set of graphs with $n+1$ nodes where there is a central node $y$ such that (1) every other node is connected to $y$, (2) there is no edge between the other nodes, \textcolor{black}{and} (3) there is at most one edge pointing to $y$. 
% % For any distribution $Q$ over $\mathcal{G}_n$, let $M(Q)$ be another distribution over $\mathcal{G}_n$ such that for any causal edges $e, e'$, $P_{G\sim Q}(e \in G) = P_{G\sim M(Q)}(e \in G) = P_{G\sim M(Q)}(e \in G | e' \in G)$. 
% We have 
% % \begin{align}
% $\sup_n \max_{Q} P_{G \sim M(Q)}(G \nin \mathcal{G}_n) = 
% 1 - \frac{2}{e} \approx 0.2642.$
% % \end{align}
% \label{prop:star}
% \end{Proposition}
% The proof is provided in Appendix Sec. \ref{sec:mgcs}. 
% % It indicates that a Node-Edge model will output $M(Q)$ when optimally trained on a data sample associated with a distribution $Q$.
% It indicates that a Node-Edge model could suffer from an inevitable error rate of $0.2642$ even though has been perfectly trained.
% \color{black}
% %\textbf{Remark:} We clarify that our critique is specifically aimed at the limitations of the Node-Edge approach, not the use of an adjacency matrix as a learning target.
% %\textcolor{black}{When the final prediction is up to an MEC rather than a fully identifiable DAG, the Bernoulli-sampling adjacency matrix approach results in inconsistency for UTs formed by non-identifiable edges.
% %For instance, }our case study shows that the entries in the adjacency matrix are not independent in determining the causal relations, thus the use of independent Bernoulli sampling over the adjacency matrix falls short of adequately representing causal relations. 
% In contrast, models that predict skeleton and v-structures have a theoretical asymptotic guarantee of the consistency under canonical assumption. 
% The details, proof and relevant discussions are provided in Appendix Sec. \ref{sec:app:tg}.


\section{Limitations of the Node-Edge \textcolor{black}{Architecture}} \label{sec:met:lim}

% \subsection{Motivation} 

% \subsubsection{Case Study of Limitation of Bernoulli-sampling adjacency matrix approach}

%\paragraph{Limitation of the Node-Edge approach.} 
\textcolor{black}{The Node-Edge architecture is common and has been adopted to generate the output DAG $G$ in the literature \citep{lorchamortized,Zhu2020Causal,dpdag,varamballydiscovering}.}
% In previous work, the Node-Edge \textcolor{black}{architecture} is adopted to generate the output DAG $G$ \citep{lorchamortized}.
In this \textcolor{black}{architecture}, each entry $G_{ij}$ in the DAG is independently sampled from $A_{ij}$, an entry in the adjacency matrix $A$. This entry $A_{ij}$ represents the probability that $i$ directly causes $j$. 
We introduce a simple yet effective example setting with only three variables $X$, $Y$, and $T$ to reveal its limitation.

\textcolor{black}{Considering a simulator that generates DAGs with equal probability from two causal models: }In model 1, the causal graph is $G_1: X \rightarrow T \rightarrow Y$, and the variables follow $X \sim \mathcal{N} (0, 1)$, $T = X + \mathcal{N}(0, 1)$, $Y = T + \mathcal{N}(0, 1)$.
In model 2, the causal graph is $G_2: X \leftarrow T \leftarrow Y$, and the variables follow $ Y = \mathcal{N}(0, 3)$, $T = \frac{2}{3}Y + \mathcal{N}(0, \frac{2}{3})$, $X = 0.5T + \mathcal{N}(0, 0.5)$.
In this case, data samples coming from both causal models follow the same joint distribution, which makes $G_1$ and $G_2$ inherently indistinguishable (from observational data sample).

More importantly, when the fully-directed causal DAGs are used as the learning target (as the Node-Edge approach does), an optimally trained neural network will predict $0.5$ probabilities on the directions of the two edges $X - T$ and $T - Y$.
As a result, with $0.25$ probability the graph sampling outcome would be $X \rightarrow T \leftarrow Y$ (see Fig. \ref{fig:ps} in the Appendix).
%It is incompatible with the observational data, resulting in a contradictory causal structure.
This error probability is rooted from the fact that the Bernoulli sampling of the edge $X \rightarrow T$ is not conditioned on the sampling result of the edge $T \leftarrow Y$. Consequently, it is a bias that cannot be avoided even if the DNN has perfectly modeled the \emph{marginal probability} of each edge (marginalized over other edges) given input data.

% It turns out that 0.25 is not the worst-case error rate yet. For example, the following proposition shows that for causal graphs with star-shaped skeleton, Node-Edge models can suffer from an inevitable error rate of 0.2642.  

% \begin{Proposition}
% Let $\mathcal{G}$ be the set of graphs where there is a central node $y$ such that (1) every other node is connected to $y$, (2) there is no edge between the other nodes, (3) there is at most one edge pointing to $y$. 
% Denote $\mathcal{G}_n$ as the subset of $\mathcal{G}$ with graphs containing $n$ non-central nodes and $MEC_{\mathcal{G}_n}$ as the shared MEC for all $G$s in $\mathcal{G}_n$.
% Denoting $Q_n$ as a distribution over $\mathcal{G}_n$ and $D_n$ as an i.i.d. sample of a joint distribution (over the $n+1$ variables) that is Markov compatible with all graphs in $\mathcal{G}_n$, for any Node-Edge model $M$ optimally trained on $Q_n$,  we have 
%     \begin{align}
% \sup_n \max_{Q_n} P_{G \sim M(D_n)}(G \nin MEC_{\mathcal{G}_n}) = 
% 1 - \frac{2}{e} \approx 0.2642,
%     \end{align}
%     where $M(D_n)$ is denoted as the output graph distribution of $M$ with the given input data $D_n$.
% \end{Proposition}
% % \end{Proposition}
% \color{black}

\color{black}
We further find that $0.25$ is not the worst-case error rate yet. 
Formally, for a distribution $Q$ over a set of graphs, we define the graph distribution where the edges are independently sampled from the marginal distribution as $M(Q)$, i.e., for any causal edges $e_1$ and $e_2$, $P_{G\sim Q}(e_1 \in G) = P_{G\sim M(Q)}(e_1 \in G) = P_{G\sim M(Q)}(e_1 \in G | e_2 \in G)$.
In general, a Node-Edge model optimally trained on data samples $D$ coming from the distribution $Q$ will essentially learn to predict $M(Q)$ (when given the same data samples $D$ at test time).
The following proposition shows that for causal graphs with star-shaped skeleton, with a chance of $26.42\%$ the graph sampled from the marginal distribution $M(Q)$ would be incorrect.
% Node-Edge models can suffer from an inevitable error rate of 0.2642.  
% For example, the following proposition shows that for causal graphs with star-shaped skeleton, Node-Edge models can suffer from an inevitable error rate of 0.2642.  

% \begin{Proposition}
% Let $\mathcal{G}_n$ be the set of graphs with $n+1$ nodes where there is a central node $y$ such that (1) every other node is connected to $y$, (2) there is no edge between the other nodes, (3) there is at most one edge pointing to $y$. 
% For any distribution $Q$ over $\mathcal{G}_n$, let $M(Q)$ be another distribution over $\mathcal{G}_n$ such that for any causal edges $e, e'$, $P_{G\sim Q}(e \in G) = P_{G\sim M(Q)}(e \in G) = P_{G\sim M(Q)}(e \in G | e' \in G)$. We have 
% \begin{align}
% \sup_n \max_{Q} P_{G \sim M(Q)}(G \nin \mathcal{G}_n) = 
% 1 - \frac{2}{e} \approx 0.2642,
% \end{align}
% \end{Proposition}
\begin{Proposition}
Let $\mathcal{G}_n$ be the set of graphs with $n+1$ nodes where there is a central node $y$ such that (1) every other node is connected to $y$, (2) there is no edge between the other nodes, \textcolor{black}{and} (3) there is at most one edge pointing to $y$. 
% For any distribution $Q$ over $\mathcal{G}_n$, let $M(Q)$ be another distribution over $\mathcal{G}_n$ such that for any causal edges $e, e'$, $P_{G\sim Q}(e \in G) = P_{G\sim M(Q)}(e \in G) = P_{G\sim M(Q)}(e \in G | e' \in G)$. 
We have 
% \begin{align}
$\sup_n \max_{Q} P_{G \sim M(Q)}(G \nin \mathcal{G}_n) = 
1 - \frac{2}{e} \approx 0.2642.$
% \end{align}
\label{prop:star}
\end{Proposition}
The proof is provided in Appendix Sec. \ref{sec:mgcs}. 
% It indicates that a Node-Edge model will output $M(Q)$ when optimally trained on a data sample associated with a distribution $Q$.
It indicates that an edge-predicting neural network could suffer from an inevitable error rate of $0.2642$ even if it is perfectly trained.
\color{black}
%\textbf{Remark:} We clarify that our critique is specifically aimed at the limitations of the Node-Edge approach, not the use of an adjacency matrix as a learning target.
%\textcolor{black}{When the final prediction is up to an MEC rather than a fully identifiable DAG, the Bernoulli-sampling adjacency matrix approach results in inconsistency for UTs formed by non-identifiable edges.
%For instance, }our case study shows that the entries in the adjacency matrix are not independent in determining the causal relations, thus the use of independent Bernoulli sampling over the adjacency matrix falls short of adequately representing causal relations. 
In contrast, models that predict skeleton and v-structures would have a theoretical asymptotic guarantee of the consistency under canonical assumption. 
The details, proof and relevant discussions are provided in Appendix Sec. \ref{sec:app:tg}.

% In contrast, models that predict skeleton and v-structures have a theoretical asymptotic guarantee of the consistency under canonical assumption:
% \color{black}
% \begin{Proposition}
% Under canonical assumption and the assumption that neural network can be used as a universal approximator,
% there exists neural network models that always predict the correct skeleton and v-structures with sufficient samples in $D$.
% \end{Proposition}
% \color{black}
% The details, proof and relevant discussions are provided in Appendix Sec. \ref{sec:app:tg}.

% It is noteworthy that neural network models have a theoretical guarantee of the asymptotic correctness with respect to the sample size on predicting skeleton and v-structures. Formally, we have
% \begin{Theorem}
% \color{black}
% Under canonical assumption (Causal sufficiency, Markovian, and faithfulness) and the assumption that neural network can be used as a universal approximator,
% there exists neural network models that always predict the correct skeleton and v-structures with sufficient samples in $D$.
% \end{Theorem}
% The details, proof and relevant discussions are provided in Appendix Sec. \ref{sec:app:tg}.
% \color{black}

%\paragraph{Limitation of only modeling node features.}
    % In the realm of DNN-based supervised causal learning, the prevalent methods focus on modeling node features by neural networks, and determine the edges between two nodes according to their features \cite{lorchamortized}.
    % However, the information from only two nodes is unsufficient to identify causal structures.
    % For skeleton learning, a pair of variables are adjacent in the DAG if and only if they remain dependent regardless of conditioning on any subset of other variables. 
    % For orientation, a UT $X-T-Y$ orientated as a v-structure if there exists a set of variables $\mathbf{S}$, satisfying $X \perp Y \mid \mathbf{S}$ and $T$ $\notin \mathbf{S}$.
    % Therefore, current methods might overlook the complex interplay and conditional dependencies among nodes, leading to an incomplete or inaccurate representation of the causal structures.

%In DNN-based supervised causal learning, prevailing methodologies predominantly concentrate on modeling the features of individual nodes using neural networks and deducing the connections between two nodes based on these characteristics \cite{lorchamortized}. However, relying solely on information from just a pair of nodes is insufficient for accurately identifying their causal structures. 
%In the context of skeleton learning, the adjacency of a pair of variables in a DAG is determined if and only if they remain dependent regardless of conditioning on any subset of other variables. 
%For orientation, a UT $\langle X,T,Y \rangle$ orientated as a v-structure if there exists a set of variables $\mathbf{S}$, satisfying $X \perp Y \mid \mathbf{S}$ and $T$ $\notin \mathbf{S}$.
%Consequently, current approaches may neglect the intricate interplay and conditional dependencies among nodes. This oversight can lead to an incomplete or inaccurate depiction of the causal structures, underscoring the need for methods that can more comprehensively capture these complex relationships.


% \begin{figure*}[ht]
%     \centering
%     \includegraphics[width=0.85\linewidth]{figures/ablation.pdf}
%     \caption{Illustration of the key improvements of SiCL compared with AVICI/CSIvA and several alternative variants. \textit{Best viewed in color.}}
%     \label{fig:abl}
% \end{figure*}
%Fig. \ref{fig:abl} presents the key improvements of SiCL compared with AVICI/CSIvA and several alternative variants. 



% \section{The SiCL Method} \label{sec:algo}
% In light of the limitations as discussed, we propose a new DNN-based SCL method in this section, named \textbf{SiCL} (\textbf{S}upervised \textbf{i}dentifiable \textbf{C}ausal \textbf{L}earning).

% \subsection{Overall Workflow}
% \begin{figure*}[htb]
% \centering
% \includegraphics[width=0.86\linewidth]{figures/new_workflow.pdf}
%       \vspace{-0.05in}
%     \caption{The inference workflow of SiCL.}
%     \label{fig:ww}
%       \vspace{-0.2in}
      
% \end{figure*}
% Following the standard DNN-based SCL paradigm, the core inference process is implemented as a DNN. The DNN takes a data sample encoded by a matrix $D\in\mathbb{R}^{n \times d}$ as input, with $d$ being the number of observable variables and $n$ being the number of observations. In contrast to previous \textcolor{black}{Node-Edge} approaches, the SiCL method does not use the DNN to directly predict the causal graph, but instead seeks to predict the skeleton and the v-structures of the causal graph, which amount to the \emph{identifiable} causal structures as argued in the last section. 

% Specifically, the DNN outputs two objects: a skeleton prediction matrix $S \in [0,1]^{d \times d}$ where $S_{ij}$ models the conditional probability  
% \textcolor{black}{of the existence of the undirected edge $X_i - X_j$  given the dataset $D$, }
% % $P(undirected~edge~(X_i,X_j)~exists~|~D)$, 
% and a v-structure prediction tensor $V \in [0,1]^{d\times d\times d}$ where $V_{ijk}$ models the conditional probability 
% % $P(v\text{-}structure~(X_j \rightarrow X_i \leftarrow X_k)~exists~|~D)$. 
% \textcolor{black}{of the existence of the v-structure edge $X_j \rightarrow X_i \leftarrow X_k$  given the dataset $D$.}
% In our implementation, $S$ and $V$ are generated by two separate sub-networks, called Skeleton Predictor Network (SPN) and V-structure Predictor Network (VPN), respectively. 
% %Both the SPN and the VPN utilize a Pairwise Encoder architecture to explicitly model features about node-pairs. 
% To further address the limitation of only extracting node-wise feature, we propose to equip both SPN and VPN with a Pairwise Encoder module to explicitly capture the pairwise features.
% % Due to the significance of the pairwise relationship \textcolor{black}{between vertices}, we propose a pairwise encoder module to model the pairwise representations.

% Based on the skeleton prediction matrix $S$ and v-structure prediction tensor $V$, we infer the skeleton and v-structures of the causal graph; from the two we can determine a unique CPDAG. The CPDAG encodes a Markov equivalence class, from which we can pick up a graph as the final prediction of the causal graph. 
% % See Fig. \ref{fig:ww} for a diagram of the overall inference workflow of SiCL.
% \textcolor{black}{Fig. \ref{fig:ww} provides a diagram of the overall inference workflow of SiCL, and the whole workflow of SiCL is also summarized at Alg. \ref{alg:workflow} in Appendix.}

% Parameters of the SPN and VPN are trained following the standard supervised learning procedure. In our implementation, we only use synthetic training data, which is relatively easy to obtain, and yet could often lead to strong performance on real-world workloads~\citep{ke2023learning}.
% % Each training graph $G$ is randomly sampled from a pre-defined graph meta-distribution $P(G)$, and the corresponding dataset matrix $D \in \mathbb{R}^{n \times d}$ is derived from a standard forward sampling $P(D|G)$\textcolor{black}{, i.e., generating observations by sampling from the distribution defined by $G$}.
% % In our setting, we use this way to synthetically generate numerous $\langle D^i, G^i\rangle$ pairs for training.

% In the following, we elaborate the DNN architecture, the learning targets, as well as the post-processing procedure used in the SiCL method.






% %We provide a theoretical guarantee of the asymptotic correctness with sufficient samples with these learning targets.

% % As we will show in Sec. \ref{sec:met:lic}, our learning target is set as the identifiable causal structures in $G$, which consists of the skeleton and the set of v-structures.
% % We use two neural networks as the skeleton predictor and the v-structure separately.
% % Due to the significance of the pairwise relationship, both two networks contain our proposed pairwise encoder module to model the pairwise representations.
% % During inference, the skeleton and v-structure predicted by the two networks are combined together for the final predicted CPDAG results.
% % As mentioned earlier, our learning target, i.e., the identifiable causal structures, consists of two parts: the skeleton, which indicates the existence of causal relationships between pairs of nodes, and the set of v-structures, which indicate the identifiable directions of these relationships. 

% % \paragraph{Motivation.}
% % As previously discussed, our learning objective, i.e., the identifiable causal structures, is composed of two key components: the skeleton, representing the presence of causal relationships between pairs of nodes, and the set of v-structures, signifying the identifiable directions of these causal relationships.
% % As we have shown in Sec. \ref{sec:bg}, an edge $X_i - X_j$ in the skeleton indicates the persistency dependency of the pair of  $X_i$ and $X_j$, i.e., they are always dependent regardless of conditioning.
% % Consequently, we introduce a pairwise encoder module within the neural network architecture to effectively model these pairwise representations.
% % For the v-structure prediction task, our target is to judge whether a UT $ X - T - Y$ is a v-structure $X \rightarrow T \leftarrow Y$.
% % % Motivated by traditional algorithms \cite{kalisch2007estimating,le2016fast} where the UT is classified as a v-structure 
% % If the node $T$ does not exist in all node subsets $\mathbf{S}$ where $X \perp Y | \mathbf{S}$, the UT $ X - T - Y$ will be classified as a v-structure  $X \rightarrow T \leftarrow Y$.
% % It indicates that the pairwise feature between $X, Y$ is crucial for the prediction of the v-structure.
% % Therefore, pairwise relationships are of paramount importance in the process of learning causal structures in both instances. 
% %\paragraph{Pairwise Encoder Module.}
% % In both cases, pairwise relationships play a crucial role in learning causal structures. 
% % Therefore, we propose a pairwise encoder module within the neural network to model these pairwise representations.

% % The context-aware pairwise relationships are of paramount importance in the process of learning causal structures due to their ability to model persistent dependency and orientation asymmetry. 





% \subsection{Feature Extraction} \label{sec:fem}
% % \subsection{Pairwise Encoder Module} \label{sec:pem}

% \textbf{Input Processing and Node Feature Encoder.} Given the dataset matrix $D\in\mathbb{R}^{n \times d}$, the input processing module contains a linear layer for continuous input data or an embedding layer for discrete input data, yielding the raw node features $\mathcal{F}^{raw}_{il}$ for node $i$ and observation $l$.
% % It consists of an input processing module, a node feature encoder module, and a pairwise encoder module sequentially.
% % The input processing module contains a linear layer for continuous input data or an embedding layer for discrete input data.
% After that, we employ a node feature encoder to further process the raw node features into the final node features $\mathcal{F}_{il}$.
% Similar to previous papers \citep{ke2023learning,lorchamortized}, the node feature encoder is a transformer-like network comprising attention layers over either the observation dimension or the node dimension alternately.
% It naturally maintains permutation equivalence across both variable and data dimension \textcolor{black}{because of the intrinsic symmetry of attention operations}.
% \textcolor{black}{More details about the node feature encoder are presented in Appendix Sec. \ref{sec:dnf} due to the page limit}.

% \textbf{Pairwise Encoder.}
% % \begin{figure*}[t]
% % \centering
% %     \includegraphics[width=\linewidth]{figures/pairwiseencoder.pdf}
% %     \caption{Illustration of the pairwise encoder module. \textcolor{black}{In Part \ding{172}, it initializes raw pairwise features. In Part \ding{173}, a unidirectional attention is applied to utilized information from node features and pairwise features. In Part \ding{174}, an MLP and residual connection is used to yield final pairwise features.}}
% %           % \vspace{-0.15in}
% %     \label{fig:pem}
% % \end{figure*}
% Given node features $\mathcal{F} \in \mathbb{R}^{d\times n \times h}$ corresponding to $d$ nodes, the goal of pairwise encoder is to encode their pairwise relationships by $d^2$ pairwise features, represented as a tensor $\mathcal{P} \in \mathbb{R}^{d\times d \times n \times h}$.
% We use $\mathcal{P}_{ijl} \in \mathbb{R}^{h}$ to represent the pairwise feature corresponding to node $i$ and node $j$ for observation $l$.
% % $$\mathbf{p}_{ij} \in \mathbb{R}^{h}$.}
% As shown in Sec. \ref{sec:int}, both the internal information (i.e., the pairwise relationship) and the external information (e.g., the context of the conditional separation set) of node pairs are necessary to acquire persistent dependency and orientation asymmetry.
% Our pairwise encoder module \textcolor{black}{is designed to model the internal relationship via node feature concatenation and the non-linear mapping by MLP.}
% Moreover, context information, including persistent dependency and orientation asymmetry, plays a crucial role in causal learning.
% To effectively capture the contextual relationships, the attention operation has been widely recognized as a popular approach.
% Therefore, we aim to employ the attention operation within the pairwise encoder to specifically capture these external relationships.
% As visualized in Appendix Fig. \ref{fig:pem}, The pairwise encoder module consists of the following parts:
% %context-based
% % Given a set of $d$ nodes, $h$-dimensional node features, the goal of the pairwise encoder $PE$ is to encapsulate their pairwise relationships by $d^2$ $h$-dimensional pairwise features.
% % \st{Given a set of $d$ nodes, each node is represented by an $h$ dimensional vector.
% % The goal of the pairwise encoder is to encapsulate their pairwise relationships by $d^2$ $h$-dimensional pairwise features.}
% \begin{enumerate}[leftmargin=*]
%     \item \textit{Pairwise Feature Initialization.} The initial step is to concatenate the node features \textcolor{black}{from the previous node feature encoder module} for every pair of nodes.
%     Subsequently, we employ a three-layer MLP to convert each concatenated vector $\mathcal{P}_{ijl} \in \mathbb{R}^{2h}$ to an $h$-dimensional raw pairwise feature, i.e., $\mathcal{P}_{ijl}^1 =  \mathrm{MLP}([\mathcal{F}_{il}; \mathcal{F}_{jl}])$. It is designed to capture the intricate relations that exist inside the pairs of nodes.
%     \item \textit{Unidirectional Multi-Head Attention.} In order to model the external information, we employ an attention mechanism where the query is composed of the aforementioned $d^2$ $h$-dimensional raw pairwise features, while the keys and values consist of $h$-dimensional features of $d$ individual nodes, i.e., $\mathcal{P}^2 = \mathrm{MultiHeadAttention}(\mathcal{P}^1, \mathcal{F}, \mathcal{F})$.
% Note that, this attention operation is unidirectional, which means we only calculate cross attention from raw pairwise features $\mathcal{P}^1$ to node features $F$.
% Such design is because \textcolor{black}{both the pairwise and node information are critical to model the essential information as discussed in Sec. \ref{sec:int}, while also maintaining a reasonable computational cost.}
% \item \textit{Final Processing.} Following the widely-adopted transformer architecture, we incorporate a residual structure and a dropout layer after the previous part, i.e., $\mathcal{P}^3 = \mathrm{Norm}(\mathcal{P}^1 + \mathcal{P}^2)$.
% Finally, we introduce a three-layer MLP to further capture intricate patterns and non-linear relationships between the input embeddings, as well as to more effectively process the information from the attention mechanism: $\mathcal{P} = \mathrm{Norm}(\mathrm{MLP}(\mathcal{P}^3) + \mathcal{P}^3)$. 
% % This approach allows for a comprehensive understanding of the complex relationships and leading to more robust and accurate modeling of causal structures.
% % For the given dataset matrix $D \in \mathbb{R}^ {n\times d}$ with $n$ observations, the pairwise encoder module yields a tensor of shape $\mathbb{R}^{n \times d \times d \times h}$, containing an $h$-dimension feature vector for each node pair of each observation. 
% It yields the final pairwise feature tensor $\mathcal{P} \in \mathbb{R}^{d \times d \times n \times h}$.
% % , giving an $h$-dimension feature vector for each node pair of each observation. 
% \end{enumerate}

% % Now we introduce the whole architecture of feature extractor.
% % It consists of an input processing module, a node feature encoder module, and a pairwise encoder module sequentially.
% % The input processing module contains a linear layer for continuous input data or an embedding layer for discrete input data.
% % Similar to previous papers \citep{ke2023learning,lorchamortized}, the node feature encoder is a transformer-like network comprising attention layers over either the observation dimension or the node dimension alternately.
% % It naturally maintains permutation equivariance across both the variable dimension and the data dimension \textcolor{black}{because of the intrinsic symmetry of attention operations}.
% % Subsequently, the pairwise encoder module is applied to obtain the pairwise features $\mathcal{P}$. 


% \subsection{Learning Targets} \label{sec:met:lic}

% % \subsubsection{Learning Identifiable Causal Structures}
% % To address the issue, we propose to allow the network model to learn solely the identifiable causal structures in $G$, i.e., its MEC. 
% As mentioned above, a combination of the skeleton and the set of v-structures are our learning targets, which represent an MEC. 
% We utilize two separate neural network models to learn these two targets separately.
% % As shown in Sec. \ref{sec:bg}, an MEC can be represented by a combination of the skeleton and the set of v-structures, which are our learning targets.
% % As discussed in Sec. \ref{sec:bg}, skeletons are entirely identifiable across all data types, and v-structures are also identifiable for Linear-Gaussian continuous data and discrete data. 
% % As discussed in Sec. \ref{sec:bg}, skeletons and v-structures are identifiable under our settings. 
% % Consequently, our learning target contains the skeleton and the set of v-structures, forming a representation of the MEC. % under the conditions of Linear-Gaussian continuous and discrete data.%, and only the skeleton under other conditions. 
% %It avoids unnecessary prediction errors and contradictions.
% %For instance, the two different causal structures share the same MEC in the above example, and our model can predict $X - T - Y$ as the MEC representation.
% %learn from unified precise labels of skeleton and v-structures.
% % This refined approach mitigates the challenges associated with unidentifiable causal structures and enhances the overall performance of the network model.

% \textbf{Skeleton Prediction.} 
% As the persistent dependency between pairs of nodes determines the existence of edges in the skeleton, the pairwise features correspond to edges in the skeleton naturally.
% Therefore, for the skeleton learning task, we initially employ a max-pooling layer over the observation dimension to obtain a single vector $\mathcal{S}_{ij} \in \mathbb{R}^{h}$ for each pair of nodes, i.e., $\mathcal{S}_{ij} = \max_{k} \mathcal{P}_{ijk}$.
% Then, a linear layer and a sigmoid function are applied to map the pairwise features to the final prediction of edges, i.e., $S_{ij} = \mathrm{Sigmoid}(\mathrm{Linear}(\mathcal{S}_{ij}))$.
% Our learning label, the undirected graph representing the skeleton, can be easily calculated by summing the adjacency of the DAG $G$ and its transpose $G^T$.
% % Denoting the combination of max-pooling and linear layer as a skeleton prediction module $SP$, 
% Therefore, our learning target for the skeleton prediction task can be formulated as $\min \mathcal{L}(S, G + G^T)$,
% where $\mathcal{L}$ is the popularly used binary cross-entropy loss function.

% % $FE$ is the feature extractor mentioned above, and $D$ denotes the input data.


% \textbf{V-structure Prediction.}
% % he orientation asymmetry is significant to judge the existence of v-structures.
% A UT $\langle X_i, X_k, X_j \rangle$ is a v-structure when $\exists \mathbf{S}$, such that $X_k \notin \mathbf{S}$ and $X_i \perp X_j | \mathbf{S}$.
% Motivated by this, we concatenate the corresponding pairwise features of the pair $\langle X_i, X_j \rangle$ with the node features of $X_k$ as the feature for each UT $\langle X_i, X_k, X_j \rangle$ after a max-pooling along the observation dimension, i.e., $\mathcal{U}_{kij} = [\max_l \mathcal{P}_{ijl};\max_l \mathcal{F}_{kl}]$.
% After that, we use a three-layer MLP with a sigmoid function to predict the existence of v-structures among all UTs, i.e., $\mathcal{U}_{kij} = \mathrm{Sigmoid}(\mathrm{MLP}( \mathcal{U}_{kij}))$.
% For the given dataset with $d$ nodes, it outputs a third-order tensor of shape $\mathbb{R}^{d \times d \times d}$, namely v-tensor, corresponding to the predictions of the existence of v-structures.
% The v-tensor label can be obtained by $\mathcal{V}_{kij} = G_{ik} G_{jk} (1 - G_{ij})(1 - G_{ji})$,
% where $\mathcal{V}_{kij}$ indicates the existence of v-structure $X_i \rightarrow X_k \leftarrow X_j$.
% Therefore, the learning target for the v-structure prediction task can be formulated as $\min \mathcal{L}_{UT}(\mathcal{U}, \mathcal{V})$,
% where $\mathcal{L}_{UT}$ is the binary cross-entropy loss masked by UTs, i.e., we only calculate such loss on the valid UTs.
% In practice, the parameters of the feature encoders are fine-tuned from the skeleton prediction task, as the UTs to be classified are obtained from the predicted skeleton and the skeleton prediction can be seen as a general pre-trained task.

% It is noteworthy that neural network models with our learning targets have a theoretical guarantee of the correctness, as mentioned in Sec. \ref{sec:met:lim}. 

% \subsection{Post-Processing} 
% \textcolor{black}{Although our method theoretically guarantees asymptotic correctness, conflicts in predicted v-structures might occasionally occur in practice. Therefore,} in the post-processing stage, we apply a straightforward heuristic to resolve the potential conflicts and cycles among predicted v-structures following previous work \citep{dai2023ml4c}.
% \textcolor{black}{After that, we use an improved version of Meek rules \citep{meek1995causal,tsagris2019bayesian} to obtain other identifiable edges without introducing extra cycles.}
% Combining the skeleton from the skeleton predictor model with all identifiable edge directions, we get the CPDAG predictions.

% We provide a more detailed description of the post-processing process in Appendix Sec. \ref{app:post}. Nonetheless, it is worth noting that our design in post-processing is as conservative as possible, and it is non-essential in our whole framework, with more discussion and evidence provided in Appendix Sec. \ref{app:post}.
% \color{black}



\section{The SiCL Method} \label{sec:algo}
In light of the limitations as discussed, we propose a new DNN-based SCL method in this section, named \textbf{SiCL} (\textbf{S}upervised \textbf{i}dentifiable \textbf{C}ausal \textbf{L}earning).

\subsection{Overall Workflow}
\begin{figure*}[htb]
\centering
\includegraphics[width=0.9\linewidth]{figures/new_workflow.pdf}
      \vspace{-0.05in}
    \caption{The inference workflow of SiCL.}
    \label{fig:ww}
      \vspace{-0.2in}
      
\end{figure*}
Following the standard DNN-based SCL paradigm, the core inference process is implemented as a DNN. The DNN takes a data sample encoded by a matrix $D\in\mathbb{R}^{n \times d}$ as input, with $d$ being the number of observable variables and $n$ being the number of observations. In contrast to previous \textcolor{black}{Node-Edge} approaches, the SiCL method does not use the DNN to directly predict the causal graph, but instead seeks to predict the skeleton and the v-structures of the causal graph, which amount to the \emph{identifiable} causal structures as mentioned previously. 

Specifically, our DNN outputs two objects: (1) a skeleton prediction matrix $S \in [0,1]^{d \times d}$ where $S_{ij}$ models the conditional probability  
\textcolor{black}{of the existence of the \emph{undirected} edge $X_i - X_j$, conditioned on the input data sample $D$, }
% $P(undirected~edge~(X_i,X_j)~exists~|~D)$, 
and (2) a v-tensor $V \in [0,1]^{d\times d\times d}$ where $V_{ijk}$ models the conditional probability 
% $P(v\text{-}structure~(X_j \rightarrow X_i \leftarrow X_k)~exists~|~D)$. 
\textcolor{black}{of the existence of the v-structure component $X_j \rightarrow X_i \leftarrow X_k$, again conditioned on $D$.}
In our implementation, $S$ and $V$ are generated by two separate sub-networks, called Skeleton Predictor Network (SPN) and V-structure Predictor Network (VPN), respectively. 
%Both the SPN and the VPN utilize a Pairwise Encoder architecture to explicitly model features about node-pairs. 
To further address the limitation of only having node-wise features for Node-Edge models, we propose to equip SPN and VPN with Pairwise Encoder modules to explicitly capture node-pair-wise features.
% Due to the significance of the pairwise relationship \textcolor{black}{between vertices}, we propose a pairwise encoder module to model the pairwise representations.

Based on the skeleton prediction matrix $S$ and v-tensor $V$, we infer the skeleton and v-structures of the causal graph; from the two we can determine a unique CPDAG. The CPDAG encodes a Markov equivalence class, from which we can pick up a graph instance as the prediction of the causal DAG (if needed). 
% See Fig. \ref{fig:ww} for a diagram of the overall inference workflow of SiCL.
\textcolor{black}{Fig. \ref{fig:ww} provides a diagram of the overall inference workflow of SiCL, and a pseudo-code of the workflow is given by Alg. \ref{alg:workflow} in Appendix.}

Parameters of the SPN and VPN are trained following the standard supervised learning procedure. In our implementation, we only use synthetic training data, which is relatively easy to obtain, and yet could often lead to strong performance on real-world workloads~\citep{ke2023learning}.
% Each training graph $G$ is randomly sampled from a pre-defined graph meta-distribution $P(G)$, and the corresponding dataset matrix $D \in \mathbb{R}^{n \times d}$ is derived from a standard forward sampling $P(D|G)$\textcolor{black}{, i.e., generating observations by sampling from the distribution defined by $G$}.
% In our setting, we use this way to synthetically generate numerous $\langle D^i, G^i\rangle$ pairs for training.

In the following, we elaborate the DNN architecture, the learning targets, as well as the post-processing procedure used in the SiCL method.






%We provide a theoretical guarantee of the asymptotic correctness with sufficient samples with these learning targets.

% As we will show in Sec. \ref{sec:met:lic}, our learning target is set as the identifiable causal structures in $G$, which consists of the skeleton and the set of v-structures.
% We use two neural networks as the skeleton predictor and the v-structure separately.
% Due to the significance of the pairwise relationship, both two networks contain our proposed pairwise encoder module to model the pairwise representations.
% During inference, the skeleton and v-structure predicted by the two networks are combined together for the final predicted CPDAG results.
% As mentioned earlier, our learning target, i.e., the identifiable causal structures, consists of two parts: the skeleton, which indicates the existence of causal relationships between pairs of nodes, and the set of v-structures, which indicate the identifiable directions of these relationships. 

% \paragraph{Motivation.}
% As previously discussed, our learning objective, i.e., the identifiable causal structures, is composed of two key components: the skeleton, representing the presence of causal relationships between pairs of nodes, and the set of v-structures, signifying the identifiable directions of these causal relationships.
% As we have shown in Sec. \ref{sec:bg}, an edge $X_i - X_j$ in the skeleton indicates the persistency dependency of the pair of  $X_i$ and $X_j$, i.e., they are always dependent regardless of conditioning.
% Consequently, we introduce a pairwise encoder module within the neural network architecture to effectively model these pairwise representations.
% For the v-structure prediction task, our target is to judge whether a UT $ X - T - Y$ is a v-structure $X \rightarrow T \leftarrow Y$.
% % Motivated by traditional algorithms \cite{kalisch2007estimating,le2016fast} where the UT is classified as a v-structure 
% If the node $T$ does not exist in all node subsets $\mathbf{S}$ where $X \perp Y | \mathbf{S}$, the UT $ X - T - Y$ will be classified as a v-structure  $X \rightarrow T \leftarrow Y$.
% It indicates that the pairwise feature between $X, Y$ is crucial for the prediction of the v-structure.
% Therefore, pairwise relationships are of paramount importance in the process of learning causal structures in both instances. 
%\paragraph{Pairwise Encoder Module.}
% In both cases, pairwise relationships play a crucial role in learning causal structures. 
% Therefore, we propose a pairwise encoder module within the neural network to model these pairwise representations.

% The context-aware pairwise relationships are of paramount importance in the process of learning causal structures due to their ability to model persistent dependency and orientation asymmetry. 





\subsection{Feature Extraction} \label{sec:fem}
% \subsection{Pairwise Encoder Module} \label{sec:pem}

\textbf{Input Processing and Node Feature Encoder.} Given input data sample, which is a matrix $D\in\mathbb{R}^{n \times d}$, the input processing module contains a linear layer for continuous input data or an embedding layer for discrete input data, yielding the raw node features $\mathcal{F}^{raw}_{il}$ for each node $i$ in each observation $l$.
% It consists of an input processing module, a node feature encoder module, and a pairwise encoder module sequentially.
% The input processing module contains a linear layer for continuous input data or an embedding layer for discrete input data.
After that, we employ a node feature encoder to further process the raw node features into the final node features $\mathcal{F}_{il}$.
Similar to previous papers \citep{ke2023learning,lorchamortized}, the node feature encoder is a transformer-like network comprising attention layers over the observation dimension and the node dimension alternately, which naturally maintains permutation equivalence across both variable and data dimension \textcolor{black}{because of the intrinsic symmetry of attention operations}.
\textcolor{black}{More details about the node feature encoder are presented in Appendix Sec. \ref{sec:dnf} due to page limit}.

\textbf{Pairwise Encoder.}
% \begin{figure*}[t]
% \centering
%     \includegraphics[width=\linewidth]{figures/pairwiseencoder.pdf}
%     \caption{Illustration of the pairwise encoder module. \textcolor{black}{In Part \ding{172}, it initializes raw pairwise features. In Part \ding{173}, a unidirectional attention is applied to utilized information from node features and pairwise features. In Part \ding{174}, an MLP and residual connection is used to yield final pairwise features.}}
%           % \vspace{-0.15in}
%     \label{fig:pem}
% \end{figure*}
Given node features $\mathcal{F} \in \mathbb{R}^{d\times n \times h}$ for all the $d$ nodes, the goal of pairwise encoder is to encode their pairwise relationships by $d^2$ pairwise features, represented as a tensor $\mathcal{P} \in \mathbb{R}^{d\times d \times n \times h}$, where $\mathcal{P}_{ijl} \in \mathbb{R}^{h}$ is a pairwise feature corresponding to the node pair $(i,j)$ in observation $l$.
% $$\mathbf{p}_{ij} \in \mathbb{R}^{h}$.}
As argued in Sec. \ref{sec:int}, both ``internal'' information (i.e., the pairwise relationship) and ``external information (e.g., the context of the conditional separation set) of node pairs are needed to capture persistent dependency and orientation asymmetry.
Our pairwise encoder module \textcolor{black}{is designed to model the internal relationship via node feature concatenation and the non-linear mapping by MLP.}
On the other hand, 
we employ attention operations within the pairwise encoder to capture the contextual relationships (including persistent dependency and orientation asymmetry).%, which is a common practice.

More specifically, the pairwise encoder module consists of the following parts (see Appendix Fig. \ref{fig:pem} for diagrammatic illustration):
%context-based
% Given a set of $d$ nodes, $h$-dimensional node features, the goal of the pairwise encoder $PE$ is to encapsulate their pairwise relationships by $d^2$ $h$-dimensional pairwise features.
% \st{Given a set of $d$ nodes, each node is represented by an $h$ dimensional vector.
% The goal of the pairwise encoder is to encapsulate their pairwise relationships by $d^2$ $h$-dimensional pairwise features.}
\begin{enumerate}[leftmargin=*]
    \item \textit{Pairwise Feature Initialization.} The initial step is to concatenate the node features \textcolor{black}{from the previous node feature encoder module} for every pair of nodes.
    Subsequently, we employ a three-layer MLP to convert each concatenated vector $\mathcal{P}_{ijl} \in \mathbb{R}^{2h}$ to an $h$-dimensional raw pairwise feature, i.e., $\mathcal{P}_{ijl}^1 =  \mathrm{MLP}([\mathcal{F}_{il}; \mathcal{F}_{jl}])$. It is designed to capture the intricate relations that exist inside the pairs of nodes.
    \item \textit{Unidirectional Multi-Head Attention.} In order to model the external information, we employ an attention mechanism where the query is composed of the aforementioned $d^2$ $h$-dimensional raw pairwise features, while the keys and values consist of $h$-dimensional features of $d$ individual nodes, i.e., $\mathcal{P}^2 = \mathrm{MultiHeadAttention}(\mathcal{P}^1, \mathcal{F}, \mathcal{F})$.
Note that, this attention operation is unidirectional, which means we only calculate cross attention from raw pairwise features $\mathcal{P}^1$ to node features $F$.
This design is meant to capture \textcolor{black}{both pair-wise and node-wise information (as both are critical to model the causality, as discussed in Sec. \ref{sec:int}) while at the same time to maintain a reasonable computational cost.}
\item \textit{Final Processing.} Following the widely-adopted transformer architecture, we incorporate a residual structure and a dropout layer after the previous part, i.e., $\mathcal{P}^3 = \mathrm{Norm}(\mathcal{P}^1 + \mathcal{P}^2)$.
Finally, we introduce a three-layer MLP to further capture intricate patterns and non-linear relationships between the input embeddings, as well as to more effectively process the information from the attention mechanism: $\mathcal{P} = \mathrm{Norm}(\mathrm{MLP}(\mathcal{P}^3) + \mathcal{P}^3)$. 
% This approach allows for a comprehensive understanding of the complex relationships and leading to more robust and accurate modeling of causal structures.
% For the given dataset matrix $D \in \mathbb{R}^ {n\times d}$ with $n$ observations, the pairwise encoder module yields a tensor of shape $\mathbb{R}^{n \times d \times d \times h}$, containing an $h$-dimension feature vector for each node pair of each observation. 
It yields the final pairwise feature tensor $\mathcal{P} \in \mathbb{R}^{d \times d \times n \times h}$.
% , giving an $h$-dimension feature vector for each node pair of each observation. 
\end{enumerate}

% Now we introduce the whole architecture of feature extractor.
% It consists of an input processing module, a node feature encoder module, and a pairwise encoder module sequentially.
% The input processing module contains a linear layer for continuous input data or an embedding layer for discrete input data.
% Similar to previous papers \citep{ke2023learning,lorchamortized}, the node feature encoder is a transformer-like network comprising attention layers over either the observation dimension or the node dimension alternately.
% It naturally maintains permutation equivariance across both the variable dimension and the data dimension \textcolor{black}{because of the intrinsic symmetry of attention operations}.
% Subsequently, the pairwise encoder module is applied to obtain the pairwise features $\mathcal{P}$. 


\subsection{Learning Targets} \label{sec:met:lic}

% \subsubsection{Learning Identifiable Causal Structures}
% To address the issue, we propose to allow the network model to learn solely the identifiable causal structures in $G$, i.e., its MEC. 
As mentioned above, our learning target is a combination of the skeleton and the set of v-structures, which together represent an MEC. 
Two separate neural (sub-)networks are trained for these two targets.
% As shown in Sec. \ref{sec:bg}, an MEC can be represented by a combination of the skeleton and the set of v-structures, which are our learning targets.
% As discussed in Sec. \ref{sec:bg}, skeletons are entirely identifiable across all data types, and v-structures are also identifiable for Linear-Gaussian continuous data and discrete data. 
% As discussed in Sec. \ref{sec:bg}, skeletons and v-structures are identifiable under our settings. 
% Consequently, our learning target contains the skeleton and the set of v-structures, forming a representation of the MEC. % under the conditions of Linear-Gaussian continuous and discrete data.%, and only the skeleton under other conditions. 
%It avoids unnecessary prediction errors and contradictions.
%For instance, the two different causal structures share the same MEC in the above example, and our model can predict $X - T - Y$ as the MEC representation.
%learn from unified precise labels of skeleton and v-structures.
% This refined approach mitigates the challenges associated with unidentifiable causal structures and enhances the overall performance of the network model.

\textbf{Skeleton Prediction.} 
As the persistent dependency between pairs of nodes determines the existence of edges in the skeleton, the pairwise features correspond to edges in the skeleton naturally.
Therefore, for the skeleton learning task, we initially employ a max-pooling layer over the observation dimension to obtain a single vector $\mathcal{S}_{ij} \in \mathbb{R}^{h}$ for each pair of nodes, i.e., $\mathcal{S}_{ij} = \max_{k} \mathcal{P}_{ijk}$.
Then, a linear layer and a sigmoid function are applied to map the pairwise features to the final prediction of edges, i.e., $S_{ij} = \mathrm{Sigmoid}(\mathrm{Linear}(\mathcal{S}_{ij}))$.
Our learning label, the undirected graph representing the skeleton, can be easily calculated by summing the adjacency of the DAG $G$ and its transpose $G^T$.
% Denoting the combination of max-pooling and linear layer as a skeleton prediction module $SP$, 
Therefore, our learning target for the skeleton prediction task can be formulated as $\min \mathcal{L}(S, G + G^T)$,
where $\mathcal{L}$ is the popularly used binary cross-entropy loss function.

% $FE$ is the feature extractor mentioned above, and $D$ denotes the input data.


\textbf{V-structure Prediction.}
% he orientation asymmetry is significant to judge the existence of v-structures.
A UT $\langle X_i, X_k, X_j \rangle$ is a v-structure when $\exists \mathbf{S}$, such that $X_k \notin \mathbf{S}$ and $X_i \perp X_j | \mathbf{S}$.
Motivated by this, we concatenate the corresponding pairwise features of the pair $\langle X_i, X_j \rangle$ with the node features of $X_k$ as the feature for each UT $\langle X_i, X_k, X_j \rangle$ after a max-pooling along the observation dimension, i.e., $\mathcal{U}_{kij} = [\max_l \mathcal{P}_{ijl};\max_l \mathcal{F}_{kl}]$.
After that, we use a three-layer MLP with a sigmoid function to predict the existence of v-structures among all UTs, i.e., $\mathcal{U}_{kij} = \mathrm{Sigmoid}(\mathrm{MLP}( \mathcal{U}_{kij}))$.
Given a data sample of $d$ nodes, it outputs a third-order tensor of shape $\mathbb{R}^{d \times d \times d}$, namely v-tensor, corresponding to the predictions of the existence of v-structures.
The v-tensor label can be obtained by $\mathcal{V}_{kij} = G_{ik} G_{jk} (1 - G_{ij})(1 - G_{ji})$,
where $\mathcal{V}_{kij}$ indicates the existence of v-structure $X_i \rightarrow X_k \leftarrow X_j$.
Therefore, the learning target for the v-structure prediction task can be formulated as $\min \mathcal{L}_{UT}(\mathcal{U}, \mathcal{V})$,
where $\mathcal{L}_{UT}$ is the binary cross-entropy loss masked by UTs, i.e., we only calculate such loss on the valid UTs.
In our current implementation, the parameters of the feature encoders are fine-tuned from the skeleton prediction task, as the UTs to be classified are obtained from the predicted skeleton and the skeleton prediction can be seen as a general pre-trained task.

Note that neural networks with our learning targets have a theoretical guarantee for correctness in asymptotic sense, as mentioned around the end of Sec. \ref{sec:met:lim}. 

\subsection{Post-Processing} 
\textcolor{black}{Although our method theoretically guarantees asymptotic correctness, conflicts in predicted v-structures might occasionally occur in practice. Therefore,} in the post-processing stage, we apply a straightforward heuristic to resolve the potential conflicts and cycles among predicted v-structures following previous work \citep{dai2023ml4c}.
\textcolor{black}{After that, we use an improved version of Meek rules \citep{meek1995causal,tsagris2019bayesian} to obtain other identifiable edges without introducing extra cycles.}
Combining the skeleton from the skeleton predictor model with all identifiable edge directions, we get the CPDAG predictions.

We provide a more detailed description of the post-processing process in Appendix Sec. \ref{app:post}. It is worth noting that our current design of post-processing is a very conservative one, and this module is also non-essential in our whole framework; see Appendix Sec. \ref{app:post} for more discussions and evidences.
\color{black}



\section{Experiments} \label{sec:exp}

\begin{table*}[!tb]
\centering
% \resizebox{\linewidth}{!}{%
\begin{threeparttable}
\caption{\textbf{General comparison of SiCL and other methods}. The average performance results in three runs are reported for SiCL. GES takes more than 24 hours per graph on WS-L-G, hence is marked with `*'. \textcolor{black}{Full results on all metrics are provided in Appendix Tab. \ref{tab:epder}}.}
\label{tab:epders}
\begin{tabular}{cccccccccccc}
\toprule
 \multirow{2}{*}{Method} & \multicolumn{2}{c}{WS-L-G}& \multicolumn{2}{c}{SBM-L-G}& \multicolumn{2}{c}{WS-RFF-G}& \multicolumn{2}{c}{SBM-RFF-G}& \multicolumn{2}{c}{ER-CPT-MC} \\
 & s-F1$\uparrow$ & o-F1$\uparrow$ &s-F1$\uparrow$ & o-F1$\uparrow$&s-F1$\uparrow$ & o-F1$\uparrow$&s-F1$\uparrow$ & o-F1$\uparrow$&s-F1$\uparrow$ & o-F1$\uparrow$\\
\midrule
 PC & $30.4 $ & $16.0 $& $58.8$&$35.9$&$36.1$&$16.1$&$57.5$&$34.2$&$82.2$&$40.6$ \\
 GES & * & * & $70.8$& $55.0$&$41.7$&$23.6$&$56.5$&$38.0$&$82.1$&$42.4$\\
 NOTEARS & $33.3 $ & $31.5$&$80.1$&$77.8$&$37.7$&$33.4$&$55.6$&$48.5$&$16.7$&$0.6$ \\
 DAG-GNN & $35.5$ & $32.7$ &$66.2$&$62.5$&$33.2$&$28.9$&$47.1$&$40.6$&$24.8$&$3.7$\\
 GRAN-DAG & $16.6$&$11.7$&$22.6$&$14.4$&$4.7$&$1.1$&$17.4$&$3.8$&$40.8$&$7.3$ \\
 % NOTEARS-MLP &$24.6$&$11.8$&$44.7$&$39.0$&$\mathbf{52.7}$&$\mathbf{47.7}$& $48.2$ &$43.5$& *& * & \\
 GOLEM &$30.0$ &$19.3$&$68.5$&$65.2$&$27.6$&$17.7$&$41.1$&$24.8$&$37.6$&$9.3$& \\
 % GRaSP &&&&&&&&&&$0.0$&$0.0$ \\
 AVICI & $39.9 $ & $35.8$ & $84.3$ & $81.6$& $47.7$& $45.2$& $76.6$& $72.7$& $76.9$& $57.6$\\
 SiCL & $\mathbf{44.7} $ & $\mathbf{38.5} $& $\mathbf{85.8} $ & $\mathbf{82.7} $ & $\mathbf{51.8}  $ & $ \mathbf{46.3}$ & $ {\mathbf{82.1}}$ & $\mathbf{78.0}$ &$\mathbf{84.2}$ & $\mathbf{59.9}$ \\
\bottomrule
\end{tabular}
\end{threeparttable}
% }
\vspace{-0.15in}
\end{table*}
% In this section, we report our experiment design and a series of results. 
% Extra results and discussions about \textcolor{black}{time costs}, generality, and acyclicity are included in Appendix Sec. \ref{sec:app:exp:e}.
In this section, we report the performance of SiCL on both synthetic and real-world benchmarks, followed by an ablation study. 
More results and discussions about \textcolor{black}{time cost}, generality, and acyclicity are deferred to Appendix Sec. \ref{sec:app:exp:e}, due to page limit.

\subsection{Experiment Design}
\textbf{Metrics.} We profile a causal discovery method's performance using the following two tasks:

\emph{Skeleton Prediction}: 
Given a data sample $D$ of $d$ variables, for each variable pair, we want to infer if there exists direct causation between them. The standard metric \textbf{s-F1} (short for \textbf{skeleton-F1}) is used, which considers skeleton prediction as a binary classification task over the $d(d-1)/2$ variable pairs. 
For completion, we also report classification accuracy results. 
For methods with probabilistic outputs, AUC and AUPRC scores are also measured.

% \emph{Graph Prediction} :
% Given a data sample $D$ of $n$ variables, for each variable pair we want to infer if there exists direct causation, and in that case we want to further infer the causal direction. Following ML4C \cite{dai2023ml4c}, we use the following \textbf{orientation-F1} metric for this structured prediction task: A variable pair is considered a positive item if there is direct causation between them, and a prediction about the pair is a positive prediction if at least one causal direction is predicted. A positive prediction is a true positive if it's over a positive item \emph{and the predicted causal direction is correct}. The standard F1 calculation is then applied.

% Another metric, called \textbf{edge-F1} in this paper, was used in some previous works \textcolor{red}{[which?]}, which considers the task as a binary classification problem over the $n^2$ \emph{ordered-pairs} (whether there is a directed edge in the groundtruth causal graph or not). We also measured edge-F1 for completion.

\emph{CPDAG Prediction}: 
% Given a data sample $D$ of $d$ variables, for each variable pair, we want to infer if there exists direct causation between them, in that case we want to infer if the causal direction is identifiable, and in that case we try to infer the causal direction. 
\textcolor{black}{Given a data sample $D$ of $d$ variables, for each pair of variables, we aim to determine if there is direct causality between them. If so, we then assess whether the causal direction is identifiable, and if it is, we attempt to infer the specific causal direction.}
As this task involves both directed and undirected edge prediction, we use \textbf{SHD} (Structural Hamming Distance) to measure the difference between the true CPDAG and the inferred CPDAG. Besides that, we also measure the \textbf{o-F1} (short for \textbf{orientation-F1}) of the directed sub-graph of the inferred CPDAG (compared against the directed sub-graph of the true CPDAG), which focuses on capturing the inference method's orientation capability in identifying \textit{identifiable} causal edges. 
% Finally, we also measure \textbf{v-F1}, which is F1 score with the set of v-structures in the true CPDAG as positive items (among all ordered-triples), and v-structures in the inferred CPDAG as positive predictions.
\textcolor{black}{Finally, we calculate the \textbf{v-F1} score, where the F1 score is based on the set of v-structures in the true CPDAG as the positive instances (from all ordered triples), and the v-structures in the inferred CPDAG as the positive predictions.}




% \begin{table}[!tb]
% \centering
% \begin{threeparttable}
% \caption{\textbf{General Comparison of SiCL and other methods}. The average and maximum deviation in three runs are provided for SiCL. GES and DAG-GNN take too much time on WS-L-G and ER-CPT-MC separately hence is not included.}
% \label{tab:epders}
% \begin{tabular}{@{}cccc@{}}
% \toprule
% Dataset & Method & s-F1 & o-F1 \\
% \midrule
% \multirow{5}{*}{WS-L-G} & PC & $30.4 $ & $16.0 $ \\
% % & GES & * & * \\
% & NOTEARS & $33.3 $ & $31.5$ \\
% & DAG-GNN & $35.5$ & $32.7$ \\
% & AVICI & $39.9 $ & $35.8$ \\
% & SiCL & $\mathbf{44.7} {\scriptstyle \pm 1.2}$ & $\mathbf{38.5} {\scriptstyle \pm 2.3}$ \\
% \midrule
% \multirow{6}{*}{SBM-L-G} & PC & $58.8$ & $35.9$ \\
% & GES & $70.8$ & $55.0$ \\
% & NOTEARS & $80.1$ & $77.8$ \\
% & DAG-GNN & $66.2$ & $62.5$ \\
% & AVICI & $84.3$ & $81.6$ \\
% & SiCL & $\mathbf{85.8} {\pm\scriptstyle 0.3}$ & $\mathbf{82.7} {\pm\scriptstyle 0.7}$ \\
% \midrule
% \multirow{6}{*}{WS-RFF-G} & PC & $36.1 $ & $16.1$ \\
% & GES & $ 41.7$ & $23.6$ \\
% & NOTEARS & $37.7 $ & $33.4 $ \\
% & DAG-GNN & $33.2 $ & $28.9 $ \\
% & AVICI & $47.7 $ & $45.2 $ \\
% & SiCL & $\mathbf{51.8} {\scriptstyle\pm 5.4} $ & $ \mathbf{46.3}{\scriptstyle\pm 5.3}$ \\
% \midrule
% \multirow{6}{*}{SBM-RFF-G} & PC & $57.5$ & $34.2$ \\
% & GES & $56.5 $ & $38.0$ \\
% & NOTEARS & $55.6 $ & $48.5$ \\
% & DAG-GNN & $ 47.1$ & $40.6$ \\
% & AVICI & $76.6$ & $72.7 $ \\
% & SiCL & $ {\mathbf{82.1}}{\scriptstyle\pm 0.2}$ & $\mathbf{78.0}{\scriptstyle\pm 0.3}$ \\
% \midrule
% \multirow{5}{*}{ER-CPT-MC} & PC & $82.2$ & $40.6$ \\
% & GES & $ 82.1$ & $42.4$ \\
% & NOTEARS & $16.7$ & $0.6$ \\
% & AVICI & $76.9$ & $57.6$ \\
% & SiCL & $\mathbf{84.2}$ & $\mathbf{59.9}$\\
% \bottomrule
% \end{tabular}
% \end{threeparttable}
% \end{table}










\textbf{Testing Data.} 
A testing instance consists of a groundtruth causal graph $G$, the structural equations $f_i$ and noise variables $\epsilon_i$ for each variable $X_i$, and an i.i.d. sample $D$. We use two categories of testing instances in our experiments:

\textit{Analytical Instances}: 
where $G$ is sampled from a DAG distribution $\mathcal{G}$, and $\{f_i,\epsilon_i\}$ sampled from a structural-equation distribution $\mathcal{F}$ and a noise meta-distribution $\mathcal{N}$. 
%It is called an analytical instance because 
%$\mathcal{G}$, $\mathcal{F}$, $\mathcal{N}$ all have analytical forms.
We consider three random graph distributions for $\mathcal{G}$: Watts-Strogatz (WS), Stochastic Block Model (SBM), Erdos-Rényi (ER); and three $\mathcal{F}$'s: random linear (L), Random Fourier Features (RFF), and conditional probability table (CPT). $\mathcal{N}$ is a uniform distribution over Gaussian's for continuous data, and a Dirichlet distribution over categorical distributions for discrete data. 
% We examined five combinations of these variations: \textbf{WS-L-G}, \textbf{SBM-L-G}, \textbf{WS-RFF-G}, \textbf{SBM-RFF-G}, \textbf{ER-CPT-MC}.
\textcolor{black}{We examine five combinations of testing instances: \textbf{WS-L-G}, \textbf{SBM-L-G}, \textbf{WS-RFF-G}, \textbf{SBM-RFF-G}, and \textbf{ER-CPT-MC}.}


% \textit{MEC-randomized Instances}: \textcolor{black}{They are testing instances} synthesized by replacing the groundtruth causal graph of an analytical instance $(G,P,D)$ with a randomly chosen graph $G'$ in the MEC of the original graph $G$. As $D$ is a data sample from a causal model with $G$ being the causal graph, there must exist another causal model where $G'$ is the groundtruth causal graph and variables in that model follow the same distribution with $D$ (i.e., follows $P$). 
% %Note however that the structural equations and noise variables in the causal model of $G'$ may fall outside the support of the original generating distributions $\mathcal{F}$ and $\mathcal{N}$. 
% Therefore,  $(G',P,D)$ represents an instance of a valid synthetic-instance family where the causal graph, structural equations, and noise distributions, all follow fixed distribution but its closed form is not easily written as an analytical model (yet can be simulated by MEC randomization over an analytic model). Each analytical-instance family above can induce a MEC-randomized family, e.g., \textbf{WS-L-G-MEC}, \textbf{SBM-L-G-MEC}, etc.
% denoted by \textbf{WS-L-U-MEC}, \textbf{WS-L-G-MEC}, \textbf{WS-RFF-G-MEC}, \textbf{SBM-L-G-MEC}, \textbf{SBM-RFF-G-MEC}, \textbf{ER-CPT-MC-MEC}. 


\textit{Real-world Instance}:
% The classic dataset \textbf{Sachs} is used. 
% It consists of a data sample recording the concentration levels of 11 phosphorylated proteins in 853 human immune system cells, and of a causal graph over these 11 variables identified by \cite{sachs2005causal} based on expert consensus and experimental biology literature. 
% %\textcolor{red}{[is the concentration levels discretized?]}
The classic dataset \textbf{Sachs} is used \textcolor{black}{to evaluate performance in real-world scenarios}. 
It consists of a data sample recording the concentration levels of 11 phosphorylated proteins in 853 human immune system cells, and of a causal graph over these 11 variables identified by \cite{sachs2005causal} based on expert consensus and biology literature.








\textbf{Algorithms.}
As baselines, we compare with a series of representative unsupervised methods, including \textbf{PC}~\citep{le2016fast}, 
\textbf{GES}~\citep{chickering2002optimal}, 
\textbf{NOTEARS}~\citep{zheng2018dags}, 
% \textbf{NOTEARS-MLP}~\citep{zheng2018dags},
\textbf{GOLEM}~\citep{ng2020role}, 
\textbf{DAG-GNN}~\citep{yu2019dag},
\textbf{GRANDAG}~\citep{Lachapelle2020Gradient-Based}, as well as \textbf{AVICI}~\citep{lorchamortized}, a DNN-based SCL method regarded as current state-of-the-art method.
% several strong baselines representing multiple categories are selected for comparison. These baselines include:
% \begin{itemize}[leftmargin=*]
%     \item PC: A classic constraint-based causal discovery algorithm based on conditional independence tests. The version with parallelized optimization is selected \citep{le2016fast}.
%     \item GES: A classic score-based greedy equivalence search algorithm \citep{chickering2002optimal}.
%   %  \item DirectLiNGAM: A function-based learning algorithm \cite{shimizu2011directlingam}.
%   \item NOTEARS: A gradient-based algorithm for linear data models \citep{zheng2018dags}.
%     \item DAG-GNN: A continuous optimization algorithm based on graph neural networks \citep{yu2019dag}.
%     % \item NOTEARS-MLP: A gradient-based algorithm for non-linear data models \citep{zheng2018dags}.
%     \item GOLEM: A more efficient version of NOTEARS \citep{ng2020role}.
%     \item GRAN-DAG: A gradient-based algorithm using neural network modeling for non-linear additive noise data \citep{Lachapelle2020Gradient-Based}. 
%     \item AVICI: A powerful deep learning-based supervised causal learning method \citep{lorchamortized}.
%     \color{black}
% \end{itemize}
% \color{black}
% As baselines, we compare with a series of unsupervised methods, including \textbf{PC}~\citep{le2016fast}, 
% \textbf{GES}~\citep{chickering2002optimal}, 
% \textbf{NOTEARS}~\citep{zheng2018dags}, 
% % \textbf{NOTEARS-MLP}~\citep{zheng2018dags},
% \textbf{GOLEM}~\citep{ng2020role}, 
% \textbf{DAG-GNN}~\citep{yu2019dag},
% \textbf{GRANDAG}~\citep{Lachapelle2020Gradient-Based}, 
% as well as with \textbf{AVICI}~\citep{lorchamortized}, a DNN-based SCL method regarded as the current state-of-the-art method.
%\textcolor{black}{which is regarded as the 
%current sota method}.

For our method, besides the full \textbf{SiCL} implementation as described by Sec. \ref{sec:algo}, 
we also implement 
\textbf{SiCL-Node-Edge}, which predicts the causal graph using the node features and can be regarded as equivalent to AVICI, and 
\textbf{SiCL-no-PF}, which skips pairwise feature extraction and predicts the skeleton and v-tensor using node-wise features (see Appendix Fig. \ref{fig:abl}). 
% It is noteworthy that SiCL contains 6 layers on the node feature encoder module while SiCL-no-PF and AVICI contains 8 layers to eliminate any potential bias arising from differences in model size. 
Notably, SiCL contains 2.8M parameters, while SiCL-Node-Edge and SiCL-no-PF contain 3.2M parameters, because SiCL contains fewer layers on the node feature encoder to eliminate potential bias from size difference. 

For DNN-based SCL methods, the DNNs are trained with synthetic data where the causal graphs follow the Erdos-Rényi (ER) and Scale-Free (SF) models and the structural equations and noise variables follow the same distribution type as the corresponding testing data. 
Therefore, the disparities between the causal graph distribution at training and testing time help to examine the generality of SiCL in \textbf{OOD} settings to some extent.
% Our evaluation part is mostly about OOD setting, i.e., the distribution of test set is OOD w.r.t. the distribution of training set.
%For interface mismatches (e.g. CPDAG prediction with a graph prediction algorithm, or discrete-data experiment on an algorithm that originally only accepts continuous data), straightforward adaptations are applied. 
% See Appendix Sec. \ref{sec:app:exp:set} for more details in the experimental setting. 
More details of the experimental setting are presented in Appendix Sec. \ref{sec:app:exp:set}.




% \begin{table*}[!tb]
% \centering
% \begin{threeparttable}
% \caption{Skeleton prediction results on linear Gaussian and general nonlinear continuous data. ``*'' indicates the case is considered failed as the algorithm takes more than $24$ hours per graph.}
% \label{tab:splg}
% \begin{tabular}{@{}ccccccc@{}}
% \toprule
% \multirow{2}{*}{Graph Type}&\multirow{2}{*}{Method} &\multirow{2}{*}{\#Params} & \multicolumn{2}{c}{LG }& \multicolumn{2}{c}{RFF} \\
%  & &  & F1  & Acc. & F1  & Acc. \\ \midrule
% \multirow{6}{*}{WS} &PC  &-& $0.304$  & $0.656$ &$0.361$&$0.699$\\
% &GES & -& *  & * &$0.417$&$0.666$\\
% %&LiNGAM  & $0.4571$  & $0.6024$ \\
% & NOTEARS &-& $0.333$ &  $0.651$&$0.376$&$0.646$\\
% &DAG-GNN  &-& $0.355$ & $0.554$ &$0.323$&$0.644$\\
% &AVICI  &3.2M& $0.446\pm 0.013$ & $0.734\pm 0.003$& $\mathbf{0.536 \pm 0.008}$&$0.728 \pm 0.002$ \\
% &SiCL &2.8M& $\mathbf{0.479\pm 0.015}$ & $\mathbf{0.750\pm 0.003}$&$0.500 \pm 0.037$& $\mathbf{0.771 \pm 0.004}$  \\ 
% \hline
% \multirow{6}{*}{SBM} &PC &- & $0.588$ & $0.900$&$0.575$& $0.893$ \\
% &GES  &-& $0.708$ &  $0.894$&$0.565$ & $0.849$ \\
% %&LiNGAM  & $0.4800$ & $0.8302$ &&\\
% & NOTEARS &-& $ 0.801 $ & $  0.945$ & $0.556$&$0.861$\\
% &DAG-GNN  &-& $0.662$  & $0.874$ & $0.471$& $0.821$\\
% &AVICI  &3.2M& $0.836\pm 0.004$ & $0.959\pm 0.001$ &$0.739 \pm 0.000$& $0.937 \pm 0.001$\\
% &SiCL &2.8M& $\mathbf{0.853\pm 0.007}$ & $\mathbf{0.962\pm 0.003}$ & $\mathbf{0.809 \pm 0.004}$ & $\mathbf{0.954 \pm 0.001}$\\ 
% \bottomrule
% \end{tabular}
% \end{threeparttable}
% \end{table*}



\subsection{Results on Synthetic Dataset} \label{sec:exp:gp}
% \paragraph{Skeleton Prediction. }
% We compare SiCL with the baseline algorithms on the skeleton prediction task.
% As the skeleton is always identifiable under all kinds of data distribution $P(D|G)$, the experiments are carried out on both Linear-Gaussian and Random-Fourier Function datasets. 
% The F1 score and Accuracy are reported in Tab. \ref{tab:splg} and Tab. \ref{tab:sprff}.
% Compared with traditional methods, DNN-based SCL methods, i.e., AVICI and SiCL, present an obvious superiority.
% Among all methods, our proposed AVICI performs the best, further verifying the superiority of our framework.



We conduct a comprehensive comparison of SiCL with various baselines in both skeleton prediction and CPDAG prediction tasks.
The main results of metrics skeleton-F1 and orientation-F1 are presented in Tab. \ref{tab:epders}, and results on full metrics are provided in Appendix Tab. \ref{tab:epder}.
% We perform experiments in Tab. \ref{tab:epder} and Appendix Tab. \ref{tab:mc}-\ref{tab:mc2} on both continuous datasets and discrete datasets to evaluate the performance of the competing methods. 
On continuous data, DNN-based SCL methods (i.e., AVICI and SiCL) demonstrate consistent and obvious advantages over traditional approaches.
SiCL consistently outperforms the other methods on both skeleton prediction task and CPDAG prediction task.
On the other hand, some unsupervised methods achieve comparable performance among DNN-based SCL methods on the discrete data ER-CPT-MC. 
Nonetheless, our proposed SiCL emerges as the top performer, further substantiating its superiority in addressing the causal learning problem. 



% Tab. \ref{tab:cplg} \textcolor{black}{and Appendix Tab. \ref{tab:mc2}} present the evaluation outcomes of various methods applied to the CPDAG prediction task.
% We present our result of CPDAG prediction on the linear Gaussian data, as CPDAG reflects the maximum number of causal directions that can be oriented in this setting \citep{meek1995strong}.
% A key observation from the results is that DNN-based SCL approaches still exhibit superior performance compared to classical algorithms across all types of graphs.
% Our proposed SiCL method outperforms all other methods, further validating the effectiveness of our proposals and the inherent advantages of SiCL. 

% \begin{table*}[!htb]
% \centering
% \begin{threeparttable}
% \caption{CPDAG prediction results on linear Gaussian data. ``*'' indicates the case is considered as failed as the algorithm takes more than $24$ hours per graph.}
% \label{tab:cplg}
% \begin{tabular}{@{}cccccc@{}}
% \toprule
% Graph Type & Method & \#Params & v-structure F1 & oF1
%  & SHD  \\ \midrule
% % \multirow{5}{*}{SF} &PC  & $0.428$ &$0.433$& $46.36$\\
% % &GES  &$0.719$& $0.715$& $38.96$\\
% % %&LiNGAM  & $0.3557$& $0.3518$& $97.51$  \\
% % &DAG-GNN  & $0.740$  & $0.738$ & $42.41 $  \\
% % &AVICI  & $\mathbf{0.949} $  & $\mathbf{0.950} $ & $\mathbf{5.16} $  \\
% % &\textbf{SiCL} & $0.914\pm 0.004$ & $0.916\pm 0.006$ & $8.730\pm 0.490$  \\ 
% % \hline
% \multirow{6}{*}{WS-LG} &PC  &-& $0.156$ &$0.160$& $170.36$\\
% &GES  &-& * & * & *  \\
% & NOTEARS &-& $0.279$ &  $0.315$ &   $159.82$ \\ 
% %&LiNGAM  & $0.3652$& $0.3720$& $193.7$\\
% &DAG-GNN  &-& $\mathbf{0.322} $  & $0.327$ & $193.7 $  \\
% &AVICI  &3.2M& $0.277 $  & $0.356 $ & $117.62 $  \\
% &\textbf{SiCL} &2.8M& $0.298 \pm 0.076$ & $\mathbf{0.370 \pm 0.062}$ & $\mathbf{116.797 \pm 7.253}$  \\ 
% \hline
% \multirow{6}{*}{SBM-LG} &PC &- & $0.349$ & $0.359$ & $56.44$  \\
% &GES  &-& $0.539$& $0.550$& $60.30$\\
% %&LiNGAM  & $0.3447$ & $0.3523$& $86.13$\\
% & NOTEARS &-&$0.762 $ &   $ 0.778 $ &   $26.70$\\
% &DAG-GNN  &-& $0.603$  & $0.625 $ & $61.04 $  \\
% &AVICI  &3.2M& $0.792 $  & $0.818 $ & $17.48 $  \\
% &\textbf{SiCL} &2.8M& $\mathbf{0.805 \pm 0.016}$ & $\mathbf{0.826 \pm 0.029}$ & $\mathbf{17.147 \pm 1.573}$\\ 
% % \hline
% % \multirow{6}{*}{GRG} &PC  & $0.467$ & $0.483$ & $9.09$  \\
% % % \multirow{6}{*}{GRG} &PC  & $0.234$ & $0.250$ & $14.44$  \\
% % &GES  & $0.520$& $0.515$& $9.31$\\
% % %&LiNGAM  & $0.088$ & $0.088$ & $5.52$  \\
% % & NOTEARS &$0.949 $ &   $ 0.943 $ &   $ 1.04$ \\
% % % & NOTEARS &$0.325 $ &   $ 0.330 $ &   $ 1.03$ \\
% % &DAG-GNN  & $0.107 $  & $0.119 $ & $18.65 $  \\
% % &AVICI  & $0.337 $  & $0.336 $ & $\mathbf{0.7} $ \\
% % &\textbf{SiCL} & $\mathbf{0.930 \pm 0.017}$ & $\mathbf{0.932 \pm 0.016}$ & $1.303 \pm 0.152$  \\ 
% \bottomrule
% \end{tabular}
% \end{threeparttable}
% \end{table*}












\subsection{Results on Real-world Dataset} \label{sec:exp:erd}
\begin{table}
\centering
% \vspace{-0.45in}
\caption{Comparison on Sachs dataset.}
\vspace{-0.05in}
\label{tab:sachs}
    \resizebox{\linewidth}{!}{%
    \begin{threeparttable}
\begin{tabular}{@{}ccccc@{}}
\toprule
 % & \multicolumn{2}{c}{CPDAG metrics} & \multicolumn{2}{c}{Skeleton Metrics} \\
 \multirow{2}{*}{Method} & \multicolumn{2}{c}{Skeleton Prediction }& \multicolumn{2}{c}{CPDAG Prediction} \\
 &  s-F1$\uparrow$ & s-Acc.$\uparrow$ & SHD$\downarrow$ & \#v-struc.$\downarrow$\\ \midrule
 PC& $68.6$& $80.0$ &$19$&$12$\\
 GES & $70.6$ & $81.8$ &$19$&$8$\\
 DAG-GNN & $21.1$ & $72.7$&$15$&$\mathbf{0}$ \\
 NOTEARS & $11.1$ & $70.9$ &$16 $ & $ \mathbf{0}$ \\
  GRAN-DAG & $45.5 $ & $78.2 $ &$ 12 $ & $\mathbf{0}$ \\
  GOLEM & $ 36.4$ & $ 74.5$ &$ 14 $ & $\mathbf{0}$ \\
  % CAM & $87.2$ & $90.9$ & $17$ & $6$\\
AVICI & $66.7 $&$ 83.5$  &$18 $&$ 14$\\ 
SiCL & $\mathbf{71.4}$ & $\mathbf{86.8}$&$\mathbf{6}$&$\mathbf{0}$   \\ 
\bottomrule
\end{tabular}
\end{threeparttable}
}
\vspace{-0.05in}
\end{table}
% To assess the practical applicability of SiCL, we conduct a comparison using the real-world dataset Sachs.
% The results are provided in Tab. \ref{tab:sachs}.
% The DNN-based SCL methods are trained on random synthetic graphs, making this also an \textbf{OOD} prediction task.


% Similarly as the rankings on the synthetic ER data, unsupervised methods (i.e., PC and GES) perform comparably with DNN-based SCL methods (i.e., AVICI and SiCL). Nonetheless, the skeleton F1 from SiCL is the highest, verifying its practical effectiveness on predicting causal relationships.
% For orientation task, it is noteworthy that the DAG label of the Sachs dataset contains no v-structure, implying a low number of predicted v-structures is desirable. 
% On the final CPDAG prediction tasks, SiCL predicts no v-structures with the lowest SHD with groundtruth CPDAG, which still surpasses other methods. 

To assess the practical applicability of SiCL, we conduct a comparison using the real-world dataset Sachs.
The DNN-based SCL methods are trained on random synthetic graphs, making this also an \textbf{OOD} prediction task.
The results are provided in Tab. \ref{tab:sachs}.

For the skeleton prediction task, SiCL performs the best, albeit with a modest gap (1$\sim$3 points higher than the runners-up). For the CPDAG prediction task, SiCL performs significantly better than all other methods (reducing SHD from 12 to 6, against the second best). Interestingly, the true causal DAG of the Sachs benchmark actually contains no v-structure, so any predicted v-structure is an error. We see that methods competitive with SiCL in skeleton prediction (AVICI, PC, GES) mistakenly predicted a large number of v-structures on the Sachs data, while SiCL correctly predict zero v-structure.


% This finding further substantiates the efficacy of SiCL. 
% \begin{table*}[tb]
%     \centering
%     \resizebox{\linewidth}{!}{%
% \begin{threeparttable}
% \caption{Ablation Study on MEC-randomized datasets.}
% \label{tab:cplg}
% \begin{tabular}{ccccccccc}
% \toprule
% \multirow{2}{*}{Dataset} &\multirow{2}{*}{Method} & \multicolumn{4}{c}{Skeleton Prediction }& \multicolumn{3}{c}{CPDAG Prediction} \\
%  && s-F1 & s-Acc. & s-AUC & s-AUPRC &  v-F1 & o-F1 & SHD\\
%  \midrule
% \multirow{3}{*}{WS-L-G-MEC}& AVICI & $38.6 $&$ 74.0$&$71.9$&$62.5$&$ 27.1$&$35.2 $ &$119.1 $\\
% % &SiCL-no-SV & $ 42.4$  & $ 74.4$ &$ 72.8$&$ 63.5$&$ 30.5$&$ 37.9$ &$118.4 $\\
% &SiCL-no-PF & $ 42.4$  & $ 74.4$ &$ 72.8$&$ 63.5$&$ 30.5$&$ 37.9$ &$118.4 $\\
% &SiCL & $\mathbf{44.7} {\scriptstyle \pm 1.2}$ & $\mathbf{75.3} {\scriptstyle \pm 0.1}$&$\mathbf{73.7} {\scriptstyle \pm 0.3}$&$\mathbf{65.4} {\scriptstyle \pm 0.4}$&$\mathbf{32.0} {\scriptstyle \pm 2.2}$&$\mathbf{38.5} {\scriptstyle \pm 2.3}$&$\mathbf{116.1} {\scriptstyle \pm 4.8}$\\ \hline
% \multirow{3}{*}{SBM-L-G-MEC}& AVICI & $ 83.5$&$ 96.1$&$98.2$&$92.8$&$ 78.6$&$81.5 $ &$18.8 $\\
% % &SiCL-no-SV & $ 42.4$  & $ 74.4$ &$ 72.8$&$ 63.5$&$ 30.5$&$ 37.9$ &$118.4 $\\
% &SiCL-no-PF & $85.5$  & $\mathbf{96.4}$ &$\mathbf{98.3}$&$93.3$&$79.4$&$82.2$&$17.3$\\
% &SiCL &  $\mathbf{85.8}{ \scriptstyle \pm 0.3}$  & $\mathbf{96.4} {  \scriptstyle \pm 0.1}$& $\mathbf{98.3} {  \scriptstyle\pm 0.1}$& $\mathbf{93.4} {  \scriptstyle\pm 0.3}$&$\mathbf{80.6}  {  \scriptstyle \pm0.6}$&$\mathbf{82.7}  { \scriptstyle \pm0.7}$&$\mathbf{17.1} {  \scriptstyle\pm 0.5}$\\
% \bottomrule
% \end{tabular}
% \end{threeparttable}
% }

% \end{table*}

% \begin{table*}[tb]
%     \centering
%     \resizebox{\linewidth}{!}{%
% \begin{threeparttable}
% \caption{Ablation Study on MEC-randomized datasets.}
% \label{tab:cplg}
% \begin{tabular}{ccccccccc}
% \toprule
% Dataset &Method & s-F1 & s-Acc. & s-AUC & s-AUPRC &  v-F1 & o-F1 & SHD\\
%  \midrule
% \multirow{3}{*}{WS-L-G-MEC}& AVICI & $38.6 $&$ 74.0$&$71.9$&$62.5$&$ 27.1$&$35.2 $ &$119.1 $\\
% &SiCL-no-PF & $ 42.4$  & $ 74.4$ &$ 72.8$&$ 63.5$&$ 30.5$&$ 37.9$ &$118.4 $\\
% &SiCL & $\mathbf{44.7} {\scriptstyle \pm 1.2}$ & $\mathbf{75.3} {\scriptstyle \pm 0.1}$&$\mathbf{73.7} {\scriptstyle \pm 0.3}$&$\mathbf{65.4} {\scriptstyle \pm 0.4}$&$\mathbf{32.0} {\scriptstyle \pm 2.2}$&$\mathbf{38.5} {\scriptstyle \pm 2.3}$&$\mathbf{116.1} {\scriptstyle \pm 4.8}$\\ \hline
% \multirow{3}{*}{SBM-L-G-MEC}& AVICI & $ 83.5$&$ 96.1$&$98.2$&$92.8$&$ 78.6$&$81.5 $ &$18.8 $\\
% &SiCL-no-PF & $85.5$  & $\mathbf{96.4}$ &$\mathbf{98.3}$&$93.3$&$79.4$&$82.2$&$17.3$\\
% &SiCL &  $\mathbf{85.8}{ \scriptstyle \pm 0.3}$  & $\mathbf{96.4} {  \scriptstyle \pm 0.1}$& $\mathbf{98.3} {  \scriptstyle\pm 0.1}$& $\mathbf{93.4} {  \scriptstyle\pm 0.3}$&$\mathbf{80.6}  {  \scriptstyle \pm0.6}$&$\mathbf{82.7}  { \scriptstyle \pm0.7}$&$\mathbf{17.1} {  \scriptstyle\pm 0.5}$\\
% \bottomrule
% \end{tabular}
% \end{threeparttable}
% }
% \end{table*}

\begin{table}[!tb]
\centering
% \resizebox{\linewidth}{!}{%
\begin{threeparttable}
\caption{Ablation study of SiCL components. Full metrics are available in Appendix Tab. \ref{tab:fcplg}.}
\label{tab:cplg}
\vspace{-0.05in}
\begin{tabular}{ccccc}
\toprule
 \multirow{2}{*}{Method} & \multicolumn{2}{c}{WS-L-G}& \multicolumn{2}{c}{SBM-L-G} \\
 & s-F1$\uparrow$ & o-F1$\uparrow$ &s-F1$\uparrow$ & o-F1$\uparrow$\\\midrule
 SiCL-Node-Edge & $39.9$ & $35.8$ & $84.3$ & $81.6$ \\
 SiCL-no-PF &$42.4$ & $37.9$ & $85.5$ & $82.2$ \\ 
 SiCL & $\mathbf{44.7}$ & $\mathbf{38.5}$ &$\mathbf{85.8}$& $\mathbf{82.7}$ \\
\bottomrule
\end{tabular}
\end{threeparttable}
% }
\vspace{-0.2in}
\end{table}


\subsection{\textcolor{black}{Ablation Study}}
% The above experimental results show that SiCL outperform the baseline methods.
% To further investigate the source of its effectiveness, we conduct ablation study on the comparison among variants without learning identifiable structure and pairwise representation.

\begin{figure}[t]
    \centering
    % \vspace{-0.4in}
    \includegraphics[width=0.95\linewidth]{figures/Cmp_on_of1.pdf}
    \caption{Comparison of SiCL-Node-Edge and SiCL-no-PF in o-F1 trend as observation samples increase on a constructed dataset.}
    \label{fig:cto}
    \vspace{-0.15in}
\end{figure}
\textbf{Effectiveness of Learning Identifiable Structures.} \label{sec:exp:elis}As discussed in Sec. \ref{sec:met:lic}, SiCL focuses on learning identifiable causal structures rather than directly learning the adjacency matrix.
To verify the effectiveness of this idea,
% in practice and eliminate the bias of sampling from specifically designed distribution as much as possible, 
we compare SiCL-Node-Edge with SiCL-no-PF.
% to provide empirical support for the superiority of learning identifiable causal structures.
% evaluate the effectiveness of learning identifiable structures.
% test models on MEC-randomized WS-L-G and SBM-L-G datasets, where the training data is also correspondingly MEC-randomized.
% The comparisons between AVICI and SiCL-no-PF 
These two models share a similar node feature encoder architecture but have different learning targets: SiCL-Node-Edge predicts the adjacency matrix, while the SiCL-no-PF predicts the skeleton and v-tensor.
The results are shown in Tab. \ref{tab:cplg}.
Consistently, SiCL-no-PF demonstrates higher performance on both skeleton and CPDAG prediction tasks. 
This observation echoes our theoretical conclusion % further strengthens our argument 
regarding the necessity and benefits of learning identifiable causal structures to improve overall performance.

% To further verify the importance of learning identifiable causal structures, we present a comparison on a designed dense WS-L-G-MEC test dataset. 
% As presented in Fig. \ref{fig:cto}, the orientation F1 of CPDAG predictions from AVICI are much lower, which cannot be reduced by adding more observation samples. 
% However, predictions from SiCL-no-PF keep accurate, verifying the effectiveness of learning identifiable causal structures.


% To further underscore the significance of learning identifiable causal structures (especially in the asymptomatic sense), we conduct a comparative analysis using a specially constructed dataset, which contains six nodes forming an independent v-structure and a UT. 

To further underscore the significance of learning identifiable causal structures (especially in the asymptomatic sense), we conduct a comparative analysis using a specially constructed dataset consisting of six nodes.
Three of the nodes form an independent v-structure, while the other three form a UT.
Fig. \ref{fig:cto} illustrates that the orientation F1 scores of CPDAG predictions from SiCL-Node-Edge suffer from an unavoidable error and do not improve with the addition of more observational samples. 
In contrast, predictions from SiCL-no-PF reach perfect accuracy, confirming the value of learning identifiable causal structures.
\color{black}
% It is important to note that this comparison serves as an ablation study, highlighting the necessity of learning identifiable causal structures. 
% SiCL is specifically designed to excel at predicting the skeleton and v-structure due to its ability to capture pairwise representations.



% Given that the v-structure prediction relies on pairwise features, which the model without the pairwise encoder cannot supply, we compare the two models on a skeleton prediction task for a fair comparison.
% In this task, the competing model employs the inner product method for skeleton prediction, following a similar approach to that described in \cite{lorchamortized}.}
% \textbf{Effectiveness of Pairwise Representation.} \label{sec:exp:epr} 
% To evaluate the effectiveness of pairwise representation, we compare SiCL with SiCL-no-PF.
% The results shown in Tab. \ref{tab:cplg} indicate SiCL outperforms SiCL-no-PF in both skeleton prediction and CPDAG prediction tasks, despite having a smaller model size.
% This highlights the critical role of pairwise representations in identifying causal structures.
% Nonetheless, we notice that the gain depends on the concrete data distribution. 
% Therefore, we include additional comparisons across more diverse settings for further validation. 
% The results are provided in Appendix Sec. \ref{sec:mpc} due to space limitations. 
% SiCL consistently outperforms SiCL-no-PF in these comparisons, further reinforcing the importance of pairwise representations in causal identification.

\textbf{Effectiveness of Pairwise Representation.} To assess the effectiveness of pairwise representation, we compare the full version of SiCL with the variant lacking pairwise features (SiCL-no-PF). As shown in Tab. \ref{tab:cplg}, the full-version of SiCL consistently outperforms SiCL-no-PF in both skeleton prediction and CPDAG prediction tasks. 
Notably, 
we have intentionally set the model size of the full-version SiCL (2.8M parameters) to be smaller than that of SiCL-no-PF (3.2M parameters) so as to avoid any potential advantage from increased model complexity brought by the pairwise feature encoder module.
% this comparison is particularly fair, as SiCL's model size (2.8M parameters) is intentionally smaller than SiCL-no-PF (3.2M parameters) to avoid any advantage from increased complexity. 
% This underscores the critical role of pairwise features in identifying causal structures. Additionally, we conduct further comparisons across more diverse settings, with results detailed in Appendix Sec. \ref{sec:mpc}. These results demonstrate even more pronounced improvements in favor of SiCL, 
The observed performance gains in this case underscore the critical role of pairwise features in identifying causal structures. Additionally, we conduct further comparisons across more diverse settings, with results detailed in Appendix Sec. \ref{sec:mpc}. These results demonstrate even more pronounced improvements in favor of SiCL, reinforcing the importance of pairwise representations in causal discovery.

% To assess the efficacy of our pairwise representation, we compare SiCL with SiCL-no-PF and the results are also provided in Tab. \ref{tab:cplg}.
% The results reveal that \textbf{SiCL outperforms SiCL-no-PF on both skeleton prediction and CPDAG prediction tasks, despite having a smaller model size.}
% It validates the importance of the pairwise representations in identifying causal structures.
% To further confirm it, we present more comparisons under more diverse settings.
% The results are shown in Appendix Sec. \ref{sec:mpc} due to the page limit. 
% SiCL outperforms SiCL-no-PF under all comparisons, further reinforcing the significance of pairwise representations in causal identification.
% All results confirm the importance of pairwise representations in identifying causal structures.
\color{black}

% \textbf{Generality on Testing Graph Sizes.}
% We offer an analytical perspective on the performance of the SiCL model when applied to larger WS-L-G graphs. 
% It is important to highlight that the models were initially trained on graphs comprising 30 vertices, positioning this task within an out-of-distribution setting in terms of graph size. 
% To establish a point of reference, we have included results from the PC algorithm as a baseline comparison.
% These findings can be examined in Tab. \ref{tab:mpva}.
% Despite the OOD conditions, SiCL maintains robust performance, reinforcing its scalability and the model's general applicability across varying graph sizes.


% \begin{table}[t]\color{black}
% \centering
% \resizebox{\linewidth}{!}{%
% \begin{threeparttable}
% \caption{\color{black}Performance comparison with varying amounts of graph sizes.}
% \label{tab:mpva}
% \begin{tabular}{l|ccc|ccc|ccc}
% \toprule
% Metric & \multicolumn{3}{c|}{s-F1} & \multicolumn{3}{c|}{v-F1} & \multicolumn{3}{c}{o-F1} \\
% Size & 50 & 70 & 100 & 50 & 70 & 100 & 50 & 70 & 100 \\
% \midrule
% PC       & $17.7$ & $14.8$ & $10.6$ & $6.4$ & $5.0$ & $3.7$ & $7.0$ & $5.6$ & $4.0$ \\
% SiCL & $\mathbf{41.6}$ & $\mathbf{37.4}$ & $\mathbf{28.3}$ & $\mathbf{34.9}$ & $\mathbf{30.7}$ & $\mathbf{22.6}$ & $\mathbf{37.9}$ & $\mathbf{33.7}$ & $\mathbf{24.8}$ \\
% \bottomrule
% \end{tabular}
% \end{threeparttable}
% }
% 
% \end{table}

% \textbf{Acyclicity.}
% % \begin{wraptable}[8]{r}{9cm}
% \begin{table}[t]\color{black}
% \centering
% \resizebox{\linewidth}{!}{%
% \begin{threeparttable}
% \caption{\color{black}Count of cycles in the CPDAG predictions without post-processing of removing cycles.}
% \label{tab:ccfc}
% \begin{tabular}{@{}ccc@{}}
% \toprule
% Dataset & WS-L-G & SBM-L-G  \\
% \midrule
% Rate of Graphs with Cycles & $0.66 \pm 0.66 \%$&$0.00 \pm 0.00 \%$ \\
% \bottomrule
% \end{tabular}
% \end{threeparttable}
% }
% 
% \end{table}
% We provide an empirical evidence supporting of the rarity of cycles in the predictions. The experimental data presented in Tab. \ref{tab:ccfc} corroborates that cycles are infrequently observed in the predicted CPDAGs, even though without any post-processing on removing cycles.
% \color{black}



% As discussed in Sec. \ref{sec:met:lic}, SiCL focuses on learning identifiable causal structures rather than directly learning the adjacency matrix.
% Experimental results in Tab. \ref{tab:crddp} provide empirical support for its superiority.
% We conducted a comparison between two models with identical network structures but different learning targets: one model predicts the adjacency matrix, while the other predicts the skeleton together with the v-tensor.
% Consistently, the predictions generated by the models that predict the skeleton together with the v-tensor exhibit higher accuracy compared to the alternative model.
% This observation reinforces our argument that learning identifiable causal structures is not only necessary but also beneficial for improving the overall performance.
% It is important to note that this comparison serves as an ablation study, highlighting the necessity of learning identifiable causal structures. 
% SiCL is specifically designed to excel at predicting the skeleton and v-structure due to its ability to capture pairwise representations.



% We compare two models with same network structures but different learning targets (i.e., adjacency matrix or skeleton together with v-tensor).
% The predictions generated by models predicting skeleton together with v-tensor consistently demonstrate higher accuracy than those obtained from the other one.
% This observation lends further credence to our argument that learning identifiable causal structures is necessary and beneficial for improving the overall performance. 
% Note that, this comparison is an ablation study to show the necessarity of learning identifiable causal structures, and SiCL is motivated to be suitable on predicting skeleton and v-structure because of the pairwise representations.

% By prioritizing identifiable causal structures, our approach offers a more robust and reliable solution for tackling complex causal inference problems.










%\subsection{Experiments for Asymptotic Analysis}







%our approach extends the existing framework by incorporating pairwise attention to capture essential causal information more effectively, such as persistent dependency and orientation asymmetry. 
%We further propose to use a combination of a skeleton and a set of v-structures as the learning target to address the issue of inconsistency w.r.t identifiability.



% \begin{figure*}[!htb]
%     \centering
% \includegraphics[width=0.9\linewidth]{figures/workflow_inference.pdf}
%     \caption{The inference workflow of SiCL. \textcolor{black}{The predicted skeleton and v-structures are combined together to yield the final predicted CPDAG. }}
%     \label{fig:ww}
% \end{figure*}

\section{Conclusion}
%In summary, our contributions include:
%\begin{itemize}[leftmargin=*]
    %\item We propose to use a skeleton matrix together with a v-tensor as the model output, forming a direct representation of MEC. It resolves the inconsistency w.r.t. identifiability.

 %   \item We design pairwise representations modeled by a unidirectional attention layer to capture essential causal information including persistent dependency and orientation asymmetry. % and present a practical pairwise encoder module in the neural network model.

    % \item We propose a novel transformer architecture with a pairwise attention mechanism to capture essential causal information.
  %  \item We provide a practical, fully-fledged, and state-of-the-art DNN-based SCL solution, SiCL, which can handle both discrete data and continuous data, and is equipped with synthetic training data generators. We benchmarked SiCL's performance with extensive experiments and ablation studies.
    % We focus on both discrete data and continuous data generated through linear Gaussian mechanisms, a domain where the study of identifiability is well established.
 %   Our code will be released for further research purposes.
%\end{itemize}

%We propose SiCL, a novel DNN-based SCL approach. 
% This paper presents SiCL, a novel pairwise represented causal structure learning approach.
%It incorporates a unique pairwise encoder module with a unidirectional attention layer designed to encode essential causal information. 
%We utilize a skeleton matrix and a v-tensor as outputs, representing the Markov Equivalence Class (MEC) to address inconsistencies related to identifiability.
%Through extensive experiments conducted on both synthetic and real-world datasets, we demonstrate the superiority of our SiCL framework over other DNN-based SCL methods.

% We propose SiCL, a novel DNN-based SCL approach designed to predict the corresponding skeleton and a set of v-structures. We show that such design do not suffer from the (non-)identifiability limit \textcolor{black}{existing in current architectures}. Moreover, SiCL is equipped with a pairwise encoder module to model both internal and external relationships of pairs of nodes. 
% Experimental results show that SiCL significantly outperforms other DNN-based SCL approaches. 

We proposed SiCL, a novel DNN-based SCL approach designed to predict the corresponding skeleton and a set of v-structures. We showed that such design do not suffer from the (non-)identifiability limit \textcolor{black}{that exists in current architectures}. Moreover, SiCL is equipped with a pairwise encoder module to explicitly model relationships between node-pairs. 
Experimental results validated the effectiveness of these ideas. %showed that SiCL significantly outperforms other DNN-based SCL approaches. 
% \color{black}
For limitations and future works, please refer to Appendix \ref{sec:lim}.
% \color{black}
% For current limitations and future work, please refer to Appendix Sec. \ref{sec:lim}.
% \color{black}

% In this paper, we propose a novel DNN-based SCL approach SiCL, which is designed to predict the corresponding skeleton and a set of v-structures. 
% We have shown that such design do not suffer from the (non-)identifiability limit \textcolor{black}{existing in current Node-Edge architectures}. 
% Moreover, SiCL is equipped with a pairwise encoder module to model both internal and external relationships of pairs of nodes. Experimental results on synthetic datasets and a real-world dataset show that SiCL significantly outperforms other DNN-based SCL approaches. 
% \color{black}
% For current limitations and future work, please refer to Appendix Sec. \ref{sec:lim}.
% \color{black}

% % \section{Limitations and Future Work} \label{sec:lim}
% \color{black}
% % This paper focuses on applying DNNs to solve SCL problems. 
% This paper also introduces a few interesting open problems that warrant further exploration.
% Due to the inherent complexity of DNNs, the explanation of such decision mechanism remains an open question. 
% Therefore, future work could consider to explore how decisions are made within the networks and provide some insights for traditional SCL methods. 
% Moreover, the proposed pairwise encoder modules needs $O(d^3)$ computational complexity, which may restrict its current application to scenarios with huge number of nodes.
% Future work could focus on simplifying these operations or exploring features with less complexity (e,g., low rank features) to reduce the overall computational cost.
% \color{black}

%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\clearpage
\bibliography{mybib}


%%
%% If your work has an appendix, this is the place to put it.
\appendix
\renewcommand\thesection{A\arabic{section}}
\renewcommand{\thetable}{A\arabic{table}}
\renewcommand{\thefigure}{A\arabic{figure}}
\renewcommand{\thealgorithm}{A\arabic{algorithm}}










%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Checklist}


% %%% BEGIN INSTRUCTIONS %%%
% The checklist follows the references. For each question, choose your answer from the three possible options: Yes, No, Not Applicable.  You are encouraged to include a justification to your answer, either by referencing the appropriate section of your paper or providing a brief inline description (1-2 sentences). 
% Please do not modify the questions.  Note that the Checklist section does not count towards the page limit. Not including the checklist in the first submission won't result in desk rejection, although in such case we will ask you to upload it during the author response period and include it in camera ready (if accepted).

% \textbf{In your paper, please delete this instructions block and only keep the Checklist section heading above along with the questions/answers below.}
% %%% END INSTRUCTIONS %%%


 \begin{enumerate}


 \item For all models and algorithms presented, check if you include:
 \begin{enumerate}
   \item A clear description of the mathematical setting, assumptions, algorithm, and/or model. [Yes] %[Yes/No/Not Applicable]
   \item An analysis of the properties and complexity (time, space, sample size) of any algorithm. [Yes] %[Yes/No/Not Applicable]
   \item (Optional) Anonymized source code, with specification of all dependencies, including external libraries. [Yes] %[Yes/No/Not Applicable]
 \end{enumerate}


 \item For any theoretical claim, check if you include:
 \begin{enumerate}
   \item Statements of the full set of assumptions of all theoretical results. [Yes] %[Yes/No/Not Applicable]
   \item Complete proofs of all theoretical results. [Yes] %[Yes/No/Not Applicable]
   \item Clear explanations of any assumptions. [Yes] %[Yes/No/Not Applicable]
 \end{enumerate}


 \item For all figures and tables that present empirical results, check if you include:
 \begin{enumerate}
   \item The code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL). [Yes] %[Yes/No/Not Applicable]
   \item All the training details (e.g., data splits, hyperparameters, how they were chosen). [Yes] %[Yes/No/Not Applicable]
         \item A clear definition of the specific measure or statistics and error bars (e.g., with respect to the random seed after running experiments multiple times). [Yes] %[Yes/No/Not Applicable]
         \item A description of the computing infrastructure used. (e.g., type of GPUs, internal cluster, or cloud provider). [Yes] %[Yes/No/Not Applicable]
 \end{enumerate}

 \item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets, check if you include:
 \begin{enumerate}
   \item Citations of the creator If your work uses existing assets. [Yes] %[Yes/No/Not Applicable]
   \item The license information of the assets, if applicable. [Not Applicable]
   \item New assets either in the supplemental material or as a URL, if applicable. [Not Applicable]
   \item Information about consent from data providers/curators. [Yes]
   \item Discussion of sensible content if applicable, e.g., personally identifiable information or offensive content. [Not Applicable]
 \end{enumerate}

 \item If you used crowdsourcing or conducted research with human subjects, check if you include:
 \begin{enumerate}
   \item The full text of instructions given to participants and screenshots. [Not Applicable]
   \item Descriptions of potential participant risks, with links to Institutional Review Board (IRB) approvals if applicable. [Not Applicable]
   \item The estimated hourly wage paid to participants and the total amount spent on participant compensation. [Not Applicable]
 \end{enumerate}

 \end{enumerate}


\newpage 
\clearpage
\onecolumn

\begin{algorithm}[!tb]
\caption{SiCL Workflow for Predicting Causal Structures}
\label{alg:workflow}
\begin{algorithmic}
% \STATE {\bfseries Input:} test\_data
% \STATE {\bfseries Output:} predicted\_cpdag
% \STATE
\STATE \textbf{Procedure} INFERENCE(data, target)
\STATE Calculate node features with node encoder
\STATE Calculate pairwise features with pairwise encoder following Sec. \ref{sec:fem}
\IF {target is skeleton }
\STATE Calculate skeleton with Sec. \ref{sec:met:lic}
\ELSE
\STATE Calculate v-structures with Sec. \ref{sec:met:lic}
\ENDIF
\STATE \textbf{End Procedure}
\STATE
\STATE \textbf{Procedure} TRAINING\_PHASE()
\STATE  skeleton\_predictor $\leftarrow$ init\_skeleton\_predictor()

\STATE  Sample graphs and corresponding data
\STATE Training the skeleton predictor with INFERENCE(data, skeleton)
\STATE Training the v-structure predictor with INFERENCE(data, v-structure), with feature encoders fine-tuned from skeleton predictor
\STATE \textbf{End Procedure}
\STATE
% \STATE graph\_distribution $\leftarrow$ DEFINE\_GRAPH\_DISTRIBUTION()
\STATE \textbf{Procedure} TESTING\_PHASE(test\_data)
\STATE  Calculate predicted skeleton with the trained skeleton predictor
\STATE  Calculate predicted v-structures with the trained v-structure predictor
\STATE Combine predicted skeleton and v-structures to obtain predicted CPDAG
\STATE \textbf{End Procedure}
\end{algorithmic}
\end{algorithm}

\begin{figure*}[t]
\centering
    \includegraphics[width=\linewidth]{figures/pairwiseencoder.pdf}
    \caption{Illustration of the pairwise encoder module. \textcolor{black}{In Part \ding{172}, it initializes raw pairwise features. In Part \ding{173}, unidirectional attention is applied to utilized information from node features and pairwise features. In Part \ding{174}, an MLP and residual connection is used to yield final pairwise features.}}
          % \vspace{-0.15in}
    \label{fig:pem}
    
\end{figure*}

\section{Theoretical Guarantee} \label{sec:app:tg}
% In this section, we present the theoretical analysis on the asymptotically correctness of our model. In Sec. \ref{sec:da}, we provide the necessary definitions and our assumptions of the problem. 
% In Sec. \ref{sec:sl} - \ref{sec:ol}, we prove the asymptotically correctness of the neural network model.
% In Sec. \ref{sec:d}, we discuss the practical superiority of the neural network models.
In this section, we delve into the theoretical analysis concerning the asymptotic correctness of our proposed model with respect to the sample size. Sec. \ref{sec:da} lays out the essential definitions and assumptions pertinent to the problem under study. Following this, from Sec. \ref{sec:sl} to \ref{sec:ol}, we rigorously demonstrate the asymptotic correctness of the neural network model. Finally, in Sec. \ref{sec:d}, we engage in a detailed discussion about the practical advantages and superiority of neural network models.

\subsection{Definitions and Assumptions} \label{sec:da}
As outlined in Sec. \ref{sec:bg}, a Causal Graphical Model is defined by a joint probability distribution $P$ over $d$ random variables $X_1, X_2, \cdots, X_{d}$, and a DAG $G$ with $d$ vertices representing the $d$ variables.
An observational dataset $D$ consists of $n$ records and $d$ columns, which represents $n$ instances drawn i.i.d. from $P$. 
In this work, we assume causal sufficiency:
\begin{Assumption}[Causal Sufficiency] \label{ass:cs}
    There are no latent common causes of any of the variables in the graph. 
\end{Assumption}
Moreover, we assume the data distribution $P$ is Markovian to the DAG $G$:
\begin{Assumption}[Markov Factorization Property]\label{ass:mk}
      Given a joint probability distribution $P$ and a DAG $G, P$ is said to satisfy Markov factorization property w.r.t. $G$ if $P:=$ $P\left(X_1, X_2, \cdots, X_d\right)=\prod_{i=1}^d P\left(X_i \mid \mathrm{pa}_i^G\right)$, where $\mathrm{pa}_i^G$ is the parent set of $X_i$ in $G$.
\end{Assumption}
It is noteworthy that the Markov factorization property is equivalent to the Global Markov Property (GMP) \citep{lauritzen1996graphical}, which is
\begin{Definition}[Global Markov Property (GMP)]
    $P$ is said to satisfy GMP (or Markovian) w.r.t. a DAG $G$ if $X \perp_G Y|Z \Rightarrow X \perp Y| Z$. Here $\perp_G$ denotes d-separation, and $\perp$ denotes statistical independence. 
\end{Definition}
GMP indicates that any d-separation in graph $G$ implies conditional independence in distribution $P$. We further assume that $P$ is faithful to $G$ by:
\begin{Assumption}[Faithfulness]\label{ass:f}
Distribution $P$ is faithful w.r.t. a DAG $G$ if $X \perp Y\left|Z \Rightarrow X \perp_G Y\right| Z$.
\end{Assumption}

\begin{Definition}[Canonical Assumption] \label{ass:ca}
    We say our settings satisfy the canonical assumption if the Assumptions \ref{ass:cs} - \ref{ass:f} are all satisfied.
\end{Definition}
We restate the definitions of skeletons, Unshielded Triples (UTs) and v-strucutres as follows.
\begin{Definition}[Skeleton]
    A skeleton $E$ defined over the data distribution $P$ is an undirected graph where an edge exists between $X_i$ and $X_j$ if and only if $X_i$ and $X_j$ are always dependent in $P$, i.e., $\forall Z \subseteq\left\{X_1, X_2, \cdots, X_d\right\} \backslash \left\{X_i, X_j \right\}$, we have $X_i \nperp X_j | Z$.
\end{Definition}
Under our assumptions, the skeleton is the same as the corresponding undirected graph of $G$ \citep{spirtes2000causation}. 
\begin{Definition}[Unshielded Triples (UTs) and V-structures]
A triple of variables $X, T, Y$ is an Unshielded Triple (UT) denoted as $\langle X, T, Y \rangle$, if $X$ and $Y$ are both adjacent to $T$ but not adjacent to each other in the DAG $G$ or the corresponding skeleton.
It becomes a v-structure denoted as $X \rightarrow T \leftarrow Y$, if the directions of the edges are from $X$ and $Y$ to $T$ in $G$.
\end{Definition}
We introduce the definition of separation set as:
\begin{Definition}[Separation Set]
    For a node pair $X_i$ and $X_j$, a node set $Z$ is a separation set if $X_i \perp X_j | Z $. Under faithfulness assumption, a separation set $Z$ is a subset of variables within the vicinity that d-separates $X_i$ and $X_j$.
\end{Definition}

Finally, we assume a neural network can be used as a universal approximator in our settings.
\begin{Assumption}[Universal Approximation Capability]
    A neural network model can be trained to approximate a function under our settings with arbitrary accuracy. \label{ass:uac}
\end{Assumption}

\subsection{Skeleton Learning} \label{sec:sl}
In this section, we prove the asymptotic correctness of neural networks on the skeleton prediction task by constructing a perfect model and then approximating it with neural networks. 
For the sake of convenience and brevity in description, we define the skeleton predictor as follows. 
\begin{Definition}[Skeleton Predictor]
    Given observational data $D$, a skeleton predictor is a predicate function with an input as observational data $D$ and predicts the adjacency between each pair of the vertices.
\end{Definition}
Now we restate the Remark from \cite{ma2022ml4s} as the following proposition. 
It proves the existence of a perfect skeleton predictor by viewing the skeleton prediction step of PC \citep{spirtes2000causation} as a skeleton predictor, which is proved to be sound and complete.
\begin{Proposition}[Existence of a Perfect Skeleton Predictor]
There exists a skeleton predictor that always yields the correct skeleton with sufficient samples in $D$. \label{prop:epsp}
\end{Proposition}
\begin{proof}
    We construct a skeleton predictor $SP$ consisting of two parts by viewing PC \citep{spirtes2000causation} as a skeleton predictor. 
    In the first part, it extracts a pairwise feature $\boldsymbol{x}_{i j}$ for each pair of nodes $X_i$ and $X_j$:
    \begin{align}
            % \boldsymbol{x}_{i j}= \left|\left\{Z | Z \subseteq V \backslash\{v_i, v_j\} \wedge  (v_i \perp v_j \mid Z)\right\}\right|, \label{equ:sp1}
        \boldsymbol{x}_{i j}=\min _{Z \subseteq V \backslash\left\{X_i, X_j\right\}}\left\{X_i \sim X_j \mid Z\right\}, \label{equ:sp1}
    \end{align}
    where $\left\{X_i \sim X_j \mid Z\right\} \in [0, 1] $ is a scalar value that measures the conditional dependency between $X_i$ and $X_j$ given a node subset $Z$. 
    % where $v_i \perp v_j \mid Z$ indicates whether the conditional dependency exists between $v_i$ and $v_j$ given a node subset $Z$. 
    % Intuitively, $\boldsymbol{x}_{i j}$ represents the counts of subsets to make $v_i$ and $v_j$ conditional dependency.
    Consequently, $\boldsymbol{x}_{i j} > 0$ indicates the persistent dependency between the two nodes.
    
    In the second part, it predicts the adjacency based on $\boldsymbol{x}_{i j}$:
    \begin{align}
        \left(X_i,  X_j\right)= \begin{cases} 1 \text { (adjacent) } & \boldsymbol{x}_{i j} \neq 0 \\ 0 \text { (non-adjacent) } & \boldsymbol{x}_{i j} = 0\end{cases} \label{equ:sp2}
    \end{align}

    Now we prove that $SP$ always yields the correct skeleton by proving the absence of false positive predictions and false negative predictions. Here, false positive prediction denotes $SP$ predicts a non-adjacent node pair as adjacent and false negative predictions denote $SP$ predicts an adjacent node pair as non-adjacent.

    \begin{itemize}[leftmargin=*]
        \item \textbf{False Positive.} Suppose $X_i, X_j$ are non-adjacent. Under the Markovian assumption, there exists a set of nodes $Z$ such that $\left\{X_i \sim X_j \mid Z\right\} = 0$ and hence $\boldsymbol{x}_{ij} = 0$. According to Eq. (\ref{equ:sp2}), $SP$ will always predict them as non-adjacent.
        \item \textbf{False Negative}. Suppose $X_i, X_j$ are adjacent. Under the faithfulness assumption, for any $Z \in V \backslash \left\{X_i, X_j\right\}, \left\{X_i \sim X_j \mid Z\right\} > 0$, which implies $\boldsymbol{x}_{ij} > 0$. Therefore, $SP$ always predicts them as adjacent.
    \end{itemize}

    Therefore, $SP$ never yields any false positive predictions or false negative predictions under the Markovian assumption and faithfulness assumption, i.e., it always yields the correct skeleton.
\end{proof}

% \textbf{Remark:} % The constructed perfect skeleton predictor is not a continuous function. 
% The mapping from $\boldsymbol{x}_{ij}$ to adjacency shown in Eq. \ref{equ:sp2} can be continuously approximated. 
% Therefore, there exists arbitrarily approximate continuous function for the perfect skeleton predictor. 
%The intuition behind is that the conditional dependency test in Eq. (\ref{equ:sp1}) can be estimated by some continuous functions like $G^2$ test \citep{agresti2012categorical} and mutual information . 

With the existence of a perfect skeleton predictor, we prove the correctness of neural network models with sufficient samples under our assumptions.
\begin{Theorem}
Under the canonical assumption and the assumption that a neural network can be used as a universal approximator (Assumption \ref{ass:uac}),
there exists a neural network model that always predicts the correct skeleton with sufficient samples in $D$.
\end{Theorem}
\begin{proof}
    From Proposition \ref{prop:epsp}, there exists a perfect skeleton predictor that predicts the correct skeleton. 
    Thus, according to the Assumption \ref{ass:uac}, a neural network model can be trained to approximate the perfect skeleton prediction and hence predicts the correct skeleton. 
\end{proof}

\subsection{Orientation Learning} \label{sec:ol}
Similarly to the overall thought process in Sec. \ref{sec:sl}, in this section we prove the asymptotic correctness of neural networks on the v-structure prediction task by constructing a perfect model and then approximating it with neural networks. 
\begin{Definition}[V-structure Predictor]
    Given observational data $D$ with sufficient samples from a $BN$ with vertices $V = \{X_1, \dots, X_p\}$, a v-structure predictor is a predicate function with an input as observational data $D$ and predicts existence of the v-structure for each unshielded triple.
\end{Definition}
The following proposition proves the existence of a perfect v-structure predictor by viewing the orientation step of PC \citep{spirtes2000causation} as a v-structure predictor.
\begin{Proposition}[Existence of a Perfect V-structure Predictor]
Under the Markov assumption and faithfulness assumption, there exists a skeleton predictor that always yields the correct skeleton. \label{prop:epvp}
\end{Proposition}
\begin{proof}
    We construct a v-structure predictor $VP$ consisting of two parts by viewing PC \citep{spirtes2000causation} as a v-structure predictor. 
    % In the first part, it extracts a feature $\boldsymbol{z}_{ijk}$ for each UT $\langle X_i, X_k, X_j \rangle$:
    % \begin{align}
    %     \boldsymbol{z}_{ijk} = \frac{\left|\left\{ (X_k, Z) | \{ X_i \sim X_j | Z\} = 0 \wedge X_k \in Z \right\}\right|}{\left|\left\{ Z | \{ X_i \sim X_j | Z\} = 0 \right\}\right|},
    % \end{align}
        In the first part, it extracts a boolean feature $\boldsymbol{z}_{kij}$ for each UT $\langle X_i, X_k, X_j \rangle$:
    \begin{align}
        \boldsymbol{z}_{kij} = (X_k \in Z), \text{ where } Z \text{ is called as a sepset, i.e. } X_i\perp Y_j | Z.  \label{equ:vp}
        % \frac{\left|\left\{ (X_k, Z) | \{ X_i \sim X_j | Z\} = 0 \wedge X_k \in Z \right\}\right|}{\left|\left\{ Z | \{ X_i \sim X_j | Z\} = 0 \right\}\right|},
    \end{align}
    \color{black}

% where $\left\{X_i \sim X_j \mid Z\right\} \in [0, 1] $ is a scalar value that measures the conditional dependency between $X_i$ and $X_j$ given a node subset $Z$, and $|\cdot|$ represents the cardinality of a set. 
% Note that the denominator is always positive because the separation set of a UT always exists (See Lemma 4.1 in \cite{dai2023ml4c}).
Note that the sepset $Z$ always exists because the separation set of a UT always exists (See Lemma 4.1 in \cite{dai2023ml4c}).
% % Moreover, $\boldsymbol{z}_{ijk}$ is either $0$ or $1$ under our assumptions and sufficient samples.
% Intuitively, $\boldsymbol{z}_{ijk}$ represents the proportion of supsets of $X_i$ and $X_j$ that include $X_k$.

In the second part, it predicts the v-structures based on $\boldsymbol{z}_{ijk}$:
% \begin{align}
% \langle X_i, X_k, X_j\rangle = \begin{cases} 0 \text { (not v-structure) } & \boldsymbol{z}_{i j k} \neq 0 \\ 1 \text { (v-structure) } & \boldsymbol{z}_{i j k} = 0\end{cases} \label{equ:vp2}
% \end{align}
\begin{align}
\langle X_i, X_k, X_j\rangle = \begin{cases} 0 \text { (not v-structure) } & \boldsymbol{z}_{k i j } = True \\ 1 \text { (v-structure) } & \boldsymbol{z}_{k i j } = False\end{cases} \label{equ:vp2}
\end{align}
Now we prove that $VP$ always yields the correct predictions of v-structures.
According to Theorem 5.1 on p.410 of \cite{spirtes2000causation}, assuming faithfulness and sufficient samples, if a UT $\langle X_i, X_k, X_j \rangle$ is a v-structure, then $X_k$ does not belong to any separation sets of $(X_i, X_j)$; if a UT $\langle X_i, X_k, X_j \rangle$ is not a v-structure, then $X_k$ belongs to every separation sets of $(X_i, X_j)$. Therefore, we have $\boldsymbol{z}_{kij} = False$ if and only if $X_k$ is not in any separation set of $X_i$ and $X_j$, i.e., $\langle X_i, X_k, X_j \rangle$ is a v-structure.
\color{black}
\end{proof}

With the existence of a perfect v-structure predictor, we prove the correctness of neural network models with sufficient samples under our assumptions.
\begin{Theorem}
Under the canonical assumption and the assumption that neural network can be used as a universal approximator (Assumption \ref{ass:uac}), there exists a neural network model that always predicts the correct v-structures with sufficient samples in $D$.
\end{Theorem}
\begin{proof}
    From Proposition \ref{prop:epsp}, there exists a perfect skeleton predictor that predicts the correct v-structures. 
    Thus, according to the Assumption \ref{ass:uac}, a neural network model can be trained to approximate the perfect v-structure predictions hence predicts the correct v-structures. 
\end{proof}
% \begin{Theorem}
% Under our assumptions, neural network models can predict the correct v-structures.
% \end{Theorem}
% \begin{proof}
%     From Proposition \ref{prop:epvp}, there exists a perfect v-structure predictor that predicts the correct v-structure. 
%     Thus, according to the Assumption \ref{ass:uac}, a neural network model can be trained to predict the correct skeleton. 
% \end{proof}

\subsection{Discussion} \label{sec:d}
In the sections above, we prove the asymptotic correctness of neural network models by constructing theoretically perfect predictors.
These predictors both consist of two parts: feature extractors providing features $\boldsymbol{x}_{ij}$ and $\boldsymbol{z}_{ijk}$, and final predictors of adjacency and v-structures.
Even though they have a theoretical guarantee of the correctness with sufficient samples, it is noteworthy that they are hard to be applied practically.
For example, to obtain $\boldsymbol{x}_{ij}$ in Eq. (\ref{equ:sp1}), we need to calculate the conditional dependency between $X_i$ and $X_j$ given every node subset $Z \subseteq V \backslash\left\{X_i, X_j\right\}$.
Leaving aside the fact that the number of $Z$s itself presents factorial complexity, the main issue is that when $Z$ is relatively large, due to the curse of dimensionality, it becomes challenging to find sufficient samples to calculate the conditional dependency. 
This difficulty significantly hampers the ability to apply the constructed prefect predictors in practical scenarios.

Some existing methods can be interpreted as constructing more practical predictors.
Majority-PC (MPC) \citep{colombo2014order} achieves better performance on finite samples by modifying Eq. (\ref{equ:vp}) - (\ref{equ:vp2}) as:
    \begin{align}
        \boldsymbol{z}_{kij} = \frac{\left|\left\{ (X_k, Z) | \{ X_i \sim X_j | Z\} = 0 \wedge X_k \in Z \right\}\right|}{\left|\left\{ Z | \{ X_i \sim X_j | Z\} = 0 \right\}\right|},
    \end{align}
% Note that the denominator is always positive because the separation set of a UT always exists (See Lemma 4.1 in \cite{dai2023ml4c}).
    and
\begin{align}
    \left\langle X_i, X_k, X_j\right\rangle= \begin{cases}0 \text { (not v-structure) } & \boldsymbol{z}_{i j k} > 0.5 \\ 1(\mathrm{v} \text {-structure) } & \boldsymbol{z}_{i j k} \leq 0.5,\end{cases}
\end{align}
    where $\left\{X_i \sim X_j \mid Z\right\} \in [0, 1] $ is a scalar value that measures the conditional dependency between $X_i$ and $X_j$ given a node subset $Z$, and $|\cdot|$ represents the cardinality of a set. 
\color{black}
Due to its more complex classification mechanism, it achieves better performance empirically. 
However, from the machine learning perspective, features from both the PC and MPC predictors are relatively simple.
As supervised causal learning methods, ML4S \citep{ma2022ml4s} and ML4C \citep{dai2023ml4c} provide more systematic featurizations by manual feature engineering and utilization of powerful machine learning models for classification.
% Even though they achieve better performance in practice, the manual feature engineering is complex.
% In out paper, we use neural networks as universal approximator to learn the prediction of identifiable causal structures.
% SCL with NN 也同时在其他地方讨论，如kenan...
While these methods show enhanced practical efficacy, their manual feature engineering processes are complex. 
In our paper, we utilize neural networks as universal approximators for learning the prediction of identifiable causal structures.
It not only simplifies the procedure but also potentially uncovers more nuanced and complex patterns within the data that manual methods might overlook.
It is noteworthy that the benefits of supervised causal learning using neural networks are also discussed elsewhere, as mentioned in CSIvA \citep{ke2023learning}.



\begin{algorithm}[!tb]
\caption{Post-processing}
\label{alg:post}
\begin{algorithmic}
\STATE {\bfseries Input:} weighted skeleton matrix $S$, weighted V-tensor $U$, threshold for skeleton $\tau_s$, threshold for v-structure $\tau_v$
\STATE {\bfseries Output:} predicted oriented edge set $\texttt{oriEdges}$, predicted skeleton $\texttt{skeleton}$
% \STATE
\STATE \textbf{Step 1:}  \\
// Obtaining a predicted skeleton by thresholding. \\
$\texttt{skeleton} = \{(i, j) | max(S_{ij}, S_{ji}) > \tau_s\}$ \\
// Obtaining raw v-structures $\texttt{vstructs}_\texttt{raw}$ by thresholding. \\
$\texttt{vstructs}_\texttt{raw} = \{(i, j, k) | (i, j) \in \texttt{skeleton} \text{ and } (i, k) \in \texttt{skeleton} \text{ and } (j, k) \notin \texttt{skeleton} \text{ and } max(U_{ijk}, U_{ikj}) > \tau_v\}$ \\
\STATE \textbf{Step 2:}  \\
// V-structure conflict resolving: discard any v-structure if there exists another conflicted v-structure with a higher predicted score, following \cite{dai2023ml4c}. \\
$\texttt{vstructs} = \{(i, j, k) \in \texttt{vstructs}_\texttt{raw}|\forall (i', j', k') \in \texttt{vstructs}_\texttt{raw}, (i' \neq k \text{ and }i' \neq j)\text{ or } (k'\neq i \text{ and } j' \neq i) \text{ or } U_{i'j'k'} < U_{ijk}\}$ 
\STATE \textbf{Step 3:}  \\
// Obtaining the predicted directed edge from $\texttt{vstructs}$. \\
$\texttt{oriEdges}_{\texttt{raw}} = \{(j, i)| \exists k, (i, j, k) \in \texttt{vstructs}\}$ \\
// Set a score for each edge with the highest v-structure's score containing this edge. \\
Set $\{p_{ij}\}$ such that $p_{ij} = max_v U_v$ for $v \in \texttt{oriEdges}_{\texttt{raw}}$ and $v \ni (i, j)$. \\
// If there exist any cycles, remove the edge with the smallest score in each cycle. \\
$\texttt{oriEdges} = \{(i, j) \in \texttt{oriEdges}_{\texttt{raw}} |\forall \text{cycle } C, (i, j) \notin C \text{ or } (\exists (i', j') \in C, p_{ij} > p_{i'j'})\}$ \\
\STATE \textbf{Step 4:} \\
// Meek rules: Add edges to $\texttt{oriEdges}$ for directed edges that (1) introducing the edges does not lead to cycles or new v-structures; (2) adding the opposite edges necessarily leads to cycles or new v-structures. \\
$\texttt{oriEdges} = \texttt{oriEdges} \cup \{(i, j) \in \texttt{skeleton}| (i, j) \text{ complies with Meek rules} \}$ \\
\end{algorithmic}
\end{algorithm}

\section{Details and Discussion about Post-processing} \label{app:post}
For comprehensive clarity, we provide a clear process about the post-processing algorithm in Alg. \ref{alg:post}.

\textbf{Discussion. }
It is worth noting that our design in post-processing is as conservative as possible. In fact, we simply adhere to the conventions in deep learning (i.e., thresholding) to obtain the skeleton and the initial v-structure set. Subsequently, we follow the conventions in constraint-based causal discovery methods to derive the final oriented edges. Therefore, we have not dedicated extensive efforts towards the meticulous design, nor do we intend to emphasize this aspect of our workflow.

The conflicts and cycles are not unique to SiCL; they are, in fact, common issues encountered by all constraint-based algorithms like PC. Moreover, it's worth noting that they never appear if the networks perform perfect. Therefore, the conflict resolving of v-structures and the removal of cycles are designed as fallback mechanisms to ensure the soundness of our workflow, rather than being central elements of our approach. To illustrate it, we experimented with an opposite variant (Intuitively, this is a bad choice) that prioritizes discarding the v-structure with the higher predicted probability. The minimal differences in outcomes between this variant and its counterpart, as detailed in Tab. \ref{tab:crm}, support our viewpoint that the conflict resolution process is of limited significance within our workflow. On the other hand, experimental results presented in Tab. \ref{tab:ccfc} underscore the infrequency of cycles in the predictions, reinforcing the non-essential nature of the cycle removal component.


\begin{table}[tb]
\centering
\begin{threeparttable}
\caption{The o-F1 comparison between the used conflict-resolving method and an opposite variant.}
\label{tab:crm}
\begin{tabular}{@{}ccc@{}}
\toprule
Conflict Resolving Method & WS-L-G & SBM-L-G  \\
\midrule
Original Conflict Resolving & $\mathbf{41.1}$&$\mathbf{83.3}$ \\
Opposite Conflict Resolving & $40.7$ & $83.2$ \\
\bottomrule
\end{tabular}
\end{threeparttable}
\end{table}

\color{black}
\section{Details about Node Feature Encoder} \label{sec:dnf}
Motivated by previous approaches \citep{lorchamortized,ke2023learning}, we employ a transformer-like architecture comprising attention layers over either the observation dimension or the node dimension alternately as the node feature encoder.
Concretely, for the raw node features $\mathcal{F} \in \mathbb{R}^{d \times n \times h}$ corresponding to $d$ nodes and $n$ observations, our goal is to capture the correlations between both different nodes and different observations.
Therefore, we utilize two transformer encoder layers over the observation dimension and the node dimension alternatively:
\begin{align}
\begin{aligned}
    \mathcal{F} & \leftarrow TransformerEncoderLayer(\mathcal{F}, \mathcal{F}, \mathcal{F}) \\
    \mathcal{F} & \leftarrow \mathcal{F}.transpose(0, 1) \\
        \mathcal{F} & \leftarrow TransformerEncoderLayer(\mathcal{F}, \mathcal{F}, \mathcal{F})\\
    \mathcal{F} & \leftarrow \mathcal{F}.transpose(0, 1). \\
\end{aligned}
\end{align}
The above operation is repeated multiple times for sufficiently feature encoding.
It yields the final node feature tensor $\mathcal{F} \in \mathcal{R}^{d \times \times h}$.
\color{black}
\section{Illustration of the Case Study in Sec. \ref{sec:met:lim}}
Fig. \ref{fig:ps} presents an illustration for the case study of the Node-Edge approach in Sec. \ref{sec:met:lim}. 
It clearly shows that observational data with the two different parametrized forms follow the same joint distribution:
\begin{align}
    P(\left[X, Y, T\right]) =\mathcal{N}\left([0,0,0],\left[\begin{array}{lll}1 & 1 & 1 \\ 1 & 3 & 2 \\ 1 & 2 & 2\end{array}\right]\right).
\end{align}
Therefore, the observational datasets coming from the two DAGs are inherently indistinguishable.


    
\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/ps.pdf}
    \caption{The problem setting to emphasize the limitations of the Node-Edge approach. \textit{Best viewed in color.}}
    \label{fig:ps}
\end{figure*}


\section{Proof and Discussion for Proposition \ref{prop:star}} \label{sec:mgcs}
\color{black}
We first restate the Proposition \ref{prop:star} with more details and provide the proof.
\begin{Proposition}
Let $\mathcal{G}_n$ be the set of graphs with $n+1$ nodes where there is a central node $y$ such that (1) every other node is connected to $y$, (2) there is no edge between the other nodes, (3) there is at most one edge pointing to $y$. 
For any distribution $Q$ over $\mathcal{G}_n$, let $M(Q)$ be another distribution over $\mathcal{G}_n$ such that for any causal edges $e, e'$, $P_{G\sim Q}(e \in G) = P_{G\sim M(Q)}(e \in G) = P_{G\sim M(Q)}(e \in G | e' \in G)$. We have 
\begin{align}\max_{Q} P_{G \sim M(Q)}(G \nin \mathcal{G}_n) = 
1 - \frac{2n-1}{n-1}(1 - \frac{1}{n})^n.
\end{align}
As a corollary, we have 
\begin{align}
\sup_n \max_{Q} P_{G \sim M(Q)}(G \nin \mathcal{G}_n) = 
1 - \frac{2}{e} \approx 0.2642,
\end{align}
\end{Proposition}
\begin{proof}
Denote other nodes except for the central node as $x_i$ where $i \in \left\{1, 2, \dots, n\right\}$. 
In our setting, the set $\mathcal{G}_n$ contains $n + 1$ DAGs
with the same skeleton and no v-structure
: $G_0: y \rightarrow x_i$ for all $x_i$, and $G_i: y \rightarrow x_j$ for all $x_j \neq x_i$ together with $x_i \rightarrow y$.
Denote the sampling probability of DAG $G_i$ from $\mathcal{G}_n$ as $P_i$.
Therefore, the marginal probability of the edge $y \rightarrow x_i$ is $1 - P_i$.

    % As the Node-Edge model $M$ is trained optimally, the prediction of the existence probability of the edge $y \rightarrow x_i$ is $1 - P_i$.
    
    
    If $\exists i$, $P_i = 1$, it means that $\mathcal{G}_n$ only contains the DAG $G_i$. Therefore, $M(Q)$ is equivalent to $Q$ and we have $P_{G \sim M(Q)}(G \nin \mathcal{G}_n) = 0$.

    If $\forall i$, $P_i < 1$, denoting $Q_i = 1 - P_i$ and $P(v)$ as the probability of $G$ containing no v-structures. In other words, $P(v) = P_{G \sim M(Q)}(G \in \mathcal{G}_n)$. We have 
    \begin{align}
        P(v) = \prod_{i=1}^n Q_i + \sum_{j=1}^n \frac{\prod_i^n Q_i}{Q_j} (1 - Q_j)
        &= ( \prod_{i=1}^n Q_i) \cdot (1 + \sum_{j=1}^{n} \frac{1-Q_j}{Q_j}).
    \end{align}
    As $P(v)$ is a probability, we have $P(v) > 0$. Denoting function 
    \begin{align}
    f(Q_1, Q_2, \dots, Q_n) = \log P(v) = \sum_{i=1}^n \log Q_i + \log (1 + \sum_{j=1}^n \frac{1-Q_j}{Q_j}),
    \end{align} we would like to find its minimum s.t. $\sum_i Q_i \geq n - 1$ and $Q_i \in (0, 1]$.

    Define its Lagrange function \begin{align}
        L(Q_1, Q_2, \dots, Q_n, \lambda) = f + \lambda (n-1-\sum_i Q_i).
    \end{align}

    We have 
    \begin{align}
        \frac{\partial L}{\partial \lambda} = n - 1 - \sum_i Q_i,
    \end{align}
    and 
    \begin{align}
        \frac{\partial L}{\partial Q_i} = \frac{1}{Q_i}(1 - \frac{1}{Q_i(1-n + \sum_{k=1}^{n}\frac{1}{Q_k})}) - \lambda.
    \end{align}

    Now we are going to find the extremums for $L(Q_1, Q_2, \dots, Q_n, \lambda)$.
    \begin{enumerate}
        \item[(1)] If $\lambda = 0$, we have $\forall i$, $\frac{\partial f}{\partial Q_i} = 0$, then
        \begin{align}
            \forall i, Q_i = \frac{1}{(1 - n + \sum_{k=1}^{n} \frac{1}{Q_k})}.
        \end{align}
        It indicates that $\forall i, Q_i = 1$, hence $f = 0$ and $P(v) = 1$.
        \item[(2)] If $\lambda \neq 0$, $\exists i$, we have $\forall i$, $\frac{\partial f}{\partial Q_i} = \lambda$ and $\sum_{i=1}^n = n - 1$. In other words, we have 
        \begin{align}
            \forall i, j, \frac{\partial f}{\partial Q_i} = \frac{\partial f}{\partial Q_j} = \lambda.
        \end{align}
        Define function 
        \begin{align}
        h(Q_i) = \frac{\partial f}{\partial Q_i} = \frac{1}{Q_i}(1 - \frac{1}{Q_i(1-n + \sum_{k=1}^{n}\frac{1}{Q_k})}).
        \end{align}
        we can rewrite the function as 
        \begin{align}
            h(Q_i) = \frac{1}{Q_i}(1 - \frac{1}{1 + AQ_i}),
        \end{align}
        where $A = 1 - n + \sum_{k\neq i} \frac{1}{Q_k} \geq 1 - n + \frac{(n-1)^2}{n-1-Q_i} > 0$.
        Therefore, $h(x)$ is a monotonic function in its domain. 

        It indicates that $\forall i, j$, $Q_i = Q_j = \frac{n-1}{n}$, where $P(v) = \frac{2n-1}{n-1}(1 - \frac{1}{n})^n$.
    \end{enumerate}
    Now we are going to list the boundary points for $f$.
    \begin{enumerate}
        \item[(1)] $\forall i$, $Q_i = 1$, it becomes the first extremum point.
        \item[(2)] $\exists i$, $Q_i$ is approaching to $0$. Due to the constraint of $\sum Q_i \geq n - 1$, other $Q$s are approaching to $1$. We have $\lim_{Q_i \rightarrow 0} f = 0$ and $P(v) = 1$.
    \end{enumerate}
    In conclusion, the maximum point of function $f$ is $\forall i$, $Q_i = \frac{n - 1}{n}$, where 
    \begin{align}
        P(v) = \frac{2n-1}{n-1}(1 - \frac{1}{n})^n,
    \end{align}
    and 
        \begin{align}
        P_{G \sim M(Q)}(G \nin \mathcal{G}_n) = 1- P(v) = 1 - \frac{2n-1}{n-1}(1 - \frac{1}{n})^n.
    \end{align}
\end{proof}
\textbf{Discussion. }It is worth noting that $\mathcal{G}_n$ is exactly the MEC of any graph in $\mathcal{G}_n$.
Hence, $P_{G \sim M(Q)}(G \nin \mathcal{G}_n)$ represents the probability that the graph sampled from $M(Q)$ is incorrect.
It indicates that a Node-Edge model could suffer from an inevitable error rate of $0.2642$ though has been perfectly trained to predict $M(Q)$.
\color{black}
% \section{More General Case Study} \label{sec:mgcs}
% % This section provide a more general case study of the inconsistency probability of the Node-Edge approach.
% % Consider a simulator that generates DAGs for nodes $\{y, x_1, x_2, \dots, x_n\}$ from $n + 1$ DAGs with the same skeleton without any v-structure: $G_0: y \rightarrow x_i$ for all $x_i$, and $G_i: y \rightarrow x_j$ for all $x_j \neq x_i$ together with $x_i \rightarrow y$. These DAGs are from the same MEC, and the parametrized forms are designed to yield same joint distribution for data sampled from all DAGs, making them inherently indistinguishable. Under this setting, we have
% This section provide a more general case study of the inconsistency probability of the Node-Edge approach. 
% Concretely, let $\mathcal{G}$ be the set of graphs where there is a central node $y$ such that (1) every other node is connected to $y$, (2) there is no edge between the other nodes, (3) $y$ has at most one edge pointing to it. Denote $\mathcal{G}_n$ as the subset of $\mathcal{G}$ with graphs containing $n$ non-central nodes. 
% Hence, all $G$s from $\mathcal{G}_n$ share a same MEC denoted as $MEC_{\mathcal{G}_n}$, which indicates that there exists a data distribution $D_n$ Markovian compatible with any graph $G \in \mathcal{G}_n$. 
% We have

% \begin{Proposition}
% Denoting $Q_n$ as a distribution over $\mathcal{G}_n$, for any Node-Edge model $M$ optimally trained on $Q_n$ and $D_n$, denoting $M(D_n)$ as the output graph distribution of $M$ with the given input data $D_n$, we have 
% \begin{align}\max_{Q_n} P_{G \sim M(D_n)}(G \nin MEC_{\mathcal{G}_n}) = 
% 1 - \frac{2n-1}{n-1}(1 - \frac{1}{n})^n.
% \end{align}
% As a corollary, we have 
%     \begin{align}
% \sup_n \max_{Q_n} P_{G \sim M(D_n)}(G \nin MEC_{\mathcal{G}_n}) = 
% 1 - \frac{2}{e} \approx 0.2642.
%     \end{align}
% \end{Proposition}
% \color{black}

% % \begin{Proposition}
% % Suppose models with the Node-Edge approach can be trained optimally to predict the DAGs. Denoting the probability of generating DAG $D_i$ as $P_i$, and the probability of yielding inconsistent predictions as $P(inconsistency)$, we have
% %     \begin{align}
% %         \max_{P_0, P_1, \dots, P_n} P(inconsistency) = 
% %         1 - \frac{2n-1}{n-1}(1 - \frac{1}{n})^n, 
% %     \end{align}
% %     under the condition that $P_0=0$ and $\forall i = 1, 2, \dots, n$, $P_i = \frac{1}{n}$. As a corollary, we have
% %     \begin{align}
% %             \sup_{n, P_0, P_1, \dots, P_n} P(inconsistency) = \lim_{n \rightarrow +\infty} \max_{P_0, P_1, \dots, P_n} P(inconsistency) =  1 - \frac{2}{e} \approx 0.2642.
% %     \end{align}

% % \end{Proposition}
% \begin{proof}
% Denote other nodes except for the central node as $x_i$ where $i \in \left\{1, 2, \dots, n\right\}$. 
% In our setting, the distribution $\mathcal{G}_n$ contains $n + 1$ DAGs with the same skeleton without any v-structure: $G_0: y \rightarrow x_i$ for all $x_i$, and $G_i: y \rightarrow x_j$ for all $x_j \neq x_i$ together with $x_i \rightarrow y$.
% Denote the sampling probability of DAG $D_i$ from $\mathcal{G}_n$ as $P_i$.

%     As the Node-Edge model $M$ is trained optimally, the prediction of the existence probability of the edge $y \rightarrow x_i$ is $1 - P_i$.
    
%     If $\exists i$, $P_i = 1$, it means that $\mathcal{G}_n$ only contains the DAG $G_i$. Therefore, the model $M$ only predicts DAG $G_i$ and we have $P_{G \sim M(D_n)}(G \nin MEC_{\mathcal{G}_n}) = P(G_i \nin MEC(G)) = 0$.

%     If $\forall i$, $P_i < 1$, denoting $Q_i = 1 - P_i$ and $P(consistency)$ as the probability of yielding consistent predictions, i.e., $P(consistency) = P_{G \sim M(D_n)}(G \in MEC_{\mathcal{G}_n})$, we have 
%     \begin{align}
%         P(consistency) = \prod_{i=1}^n Q_i + \sum_{j=1}^n \frac{\prod_i^n Q_i}{Q_j} (1 - Q_j)
%         &= ( \prod_{i=1}^n Q_i) \cdot (1 + \sum_{j=1}^{n} \frac{1-Q_j}{Q_j}).
%     \end{align}
%     As $P(consistency)$ is a probability, we have $P(consistency) > 0$. Denoting function 
%     \begin{align}
%     f(Q_1, Q_2, \dots, Q_n) = \log P(consistency) = \sum_{i=1}^n \log Q_i + \log (1 + \sum_{j=1}^n \frac{1-Q_j}{Q_j}),
%     \end{align} we would like to find its minimum s.t. $\sum_i Q_i \geq n - 1$ and $Q_i \in (0, 1]$.

%     Define its Lagrange function \begin{align}
%         L(Q_1, Q_2, \dots, Q_n, \lambda) = f + \lambda (n-1-\sum_i Q_i).
%     \end{align}

%     We have 
%     \begin{align}
%         \frac{\partial L}{\partial \lambda} = n - 1 - \sum_i Q_i,
%     \end{align}
%     and 
%     \begin{align}
%         \frac{\partial L}{\partial Q_i} = \frac{1}{Q_i}(1 - \frac{1}{Q_i(1-n + \sum_{k=1}^{n}\frac{1}{Q_k})}) - \lambda.
%     \end{align}

%     Now we are going to find the extremums for $L(Q_1, Q_2, \dots, Q_n, \lambda)$.
%     \begin{enumerate}
%         \item[(1)] If $\lambda = 0$, we have $\forall i$, $\frac{\partial f}{\partial Q_i} = 0$, then
%         \begin{align}
%             \forall i, Q_i = \frac{1}{(1 - n + \sum_{k=1}^{n} \frac{1}{Q_k})}.
%         \end{align}
%         It indicates that $\forall i, Q_i = 1$, hence $f = 0$ and $P(consistency) = 1$.
%         \item[(2)] If $\lambda \neq 0$, $\exists i$, we have $\forall i$, $\frac{\partial f}{\partial Q_i} = \lambda$ and $\sum_{i=1}^n = n - 1$. In other words, we have 
%         \begin{align}
%             \forall i, j, \frac{\partial f}{\partial Q_i} = \frac{\partial f}{\partial Q_j} = \lambda.
%         \end{align}
%         Define function 
%         \begin{align}
%         h(Q_i) = \frac{\partial f}{\partial Q_i} = \frac{1}{Q_i}(1 - \frac{1}{Q_i(1-n + \sum_{k=1}^{n}\frac{1}{Q_k})}).
%         \end{align}
%         we can rewrite the function as 
%         \begin{align}
%             h(Q_i) = \frac{1}{Q_i}(1 - \frac{1}{1 + AQ_i}),
%         \end{align}
%         where $A = 1 - n + \sum_{k\neq i} \frac{1}{Q_k} \geq 1 - n + \frac{(n-1)^2}{n-1-Q_i} > 0$.
%         Therefore, $h(x)$ is a monotonic function in its domain. 

%         It indicates that $\forall i, j$, $Q_i = Q_j = \frac{n-1}{n}$, where $P(consistency) = \frac{2n-1}{n-1}(1 - \frac{1}{n})^n$.
%     \end{enumerate}
%     Now we are going to list the boundary points for $f$.
%     \begin{enumerate}
%         \item[(1)] $\forall i$, $Q_i = 1$, it becomes the first extremum point.
%         \item[(2)] $\exists i$, $Q_i$ is approaching to $0$. Due to the constraint of $\sum Q_i \geq n - 1$, other $Q$s are approaching to $1$. We have $\lim_{Q_i \rightarrow 0} f = 0$ and $P(consistency) = 1$.
%     \end{enumerate}
%     In conclusion, the maximum point of function $f$ is $\forall i$, $Q_i = \frac{n - 1}{n}$, where 
%     \begin{align}
%         P(consistency) = \frac{2n-1}{n-1}(1 - \frac{1}{n})^n,
%     \end{align}
%     and 
%         \begin{align}
%         P_{G \sim M(D_n)}(G \nin MEC_{\mathcal{G}_n}) = 1- P(consistency) = 1 - \frac{2n-1}{n-1}(1 - \frac{1}{n})^n.
%     \end{align}
% \end{proof}

\section{More Related Work} \label{sec:app:rw}
% For the score-based and continuous optimization methods, we refer to our appendix and \citet{glymour2019review,vowels2022d} for a thorough exploration of this literature. 
Score-based methods aim to find an optimal DAG according to a predefined score function, subject to combinatorial constraints. 
These methods employ specific optimization procedures such as forward-backward search GES \citep{chickering2002optimal}, hill-climbing \citep{koller2009probabilistic}, and integer programming \citep{cussens2011bayesian}.
Continuous optimization methods transform the discrete search procedure into a continuous equality constraint.
NOTEARS \citep{zheng2018dags} formulates the acyclic constraint as a continuous equality constraint and is further extended by DAG-GNN \citep{yu2019dag}, DECI \citep{geffner2022deep} to support non-linear causal relations. 
DECI \citep{geffner2022deep} is a flow-based model that can perform both causal discovery and inference on non-linear additive noise data.
% These methods can be viewed as unsupervised optimization since they do not access additional datasets associated with ground-truth causal relations. 
Recently, ENCO \citep{lippe2021efficient} is proposed as a continuous optimization method where the edge orientation is modeled as a separate parameter to maintain the acyclicity.
It is guaranteed to converge to the correct graph if interventions on all variables are available.
 \textcolor{black}{RL-BIC \citep{Zhu2020Causal} utilizes Reinforcement Learning to search for the optimal DAG.}
These methods can be viewed as unsupervised since they do not access additional datasets associated with ground truth causal relations.


\section{Experimental Settings} \label{sec:app:exp:set}
% \paragraph{Metrics.} In all tables, $\pm$ indicates that the mean value and maximum deviation of three runs with different random seeds are reported.

% For metrics, F1-scores, accuracy and SHD are general metrics for traditional methods and DNN-based methods.
% However, for DNN-based SCL method, AUC and AUPRC are more reasonable metrics because they avoid the influence of threshold selection.

% In the field of skeleton prediction tasks, the F1 score has emerged as a widely adopted metric due to its ability to effectively balance precision and recall \citep{ding2020reliable,ma2022ml4s}. 
% This metric provides a comprehensive evaluation of the model's performance, particularly in cases where the data distribution is imbalanced.
% Accuracy, another commonly used metric, offers a direct measure of the proportion of misclassified edges within the graph. 
% It can also be interpreted as a normalized version of the Structural Hamming Distance (SHD), which has gained popularity in recent years \citep{ma2022ml4s,lorchamortized,ke2023learning}.

% Considering that deep learning models typically output probabilities rather than discrete labels, the Area Under the Receiver Operating Characteristic Curve (AUC) and the Area Under the Precision-Recall Curve (AUPRC)  are also employed as more robust metrics. 
% These metrics take into account all possible decision thresholds, providing a comprehensive evaluation of the model's performance across various operating points. 
% By incorporating these metrics, researchers can gain a deeper understanding of their model's performance, ultimately contributing to the development of more advanced and reliable skeleton prediction systems that are worthy of recognition in top AI conferences.
%For skeleton prediction task, F1 score is a popularly used metric in the domain \cite{ding2020reliable,ma2022ml4s}. 
% Accuracy is a straightforward metric on the number of misclassified edges in the graph, and can also be seen as a normalized version of SHD, which is another popularly used metric \cite{ma2022ml4s,lorchamortized,ke2023learning}. 
% As the output of deep learning models are probabilities instead of single labels, AUC and AUPRC are two more robust metrics as they consider all possible decision thresholds.

% For the CPDAG prediction task, accuracy is used as a comparison metric, which measures the ratio of misclassified edges in the predicted CPDAG. 
% Following the previous paper \citep{dai2023ml4c}, the F1-scores calculated for identifiable edges and v-structures are also provided for a more comprehensive comparison. 
% Following the ML4C approach, v-structure F1 score, Identifiable edges F1, and SHD are also used for CPDAG prediction evaluation.
% For the CPDAG prediction task, we also utilize accuracy as the comparison metric, which counts the ratio of misclassified edges in the predicted CPDAG. 
% Following the previous paper \cite{dai2023ml4c}, we also utilize the F1-score over identifiable edges and v-structures for a more comprehensive comparison. 
% Following ML4C, use v-structure F1 score, Identifiable edges F1, and SHD on CPDAG prediction. 


% \paragraph{Baselines.} To demonstrate the effectiveness and superiority of the proposed framework, several strong baselines representing multiple categories are selected for comparison. These baselines include:
% \begin{enumerate}
%     \item PC: A classic constraint-based causal discovery algorithm based on conditional independence tests. The version with parallelized optimization is selected \citep{le2016fast}.
%     \item GES: A classic score-based greedy equivalence search algorithm \citep{chickering2002optimal}.
%   %  \item DirectLiNGAM: A function-based learning algorithm \cite{shimizu2011directlingam}.
%   \item NOTEARS: A gradient-based algorithm for linear data models \citep{zheng2018dags}.
%     \item DAG-GNN: A continuous optimization algorithm based on graph neural networks \citep{yu2019dag}.
%     % \item NOTEARS-MLP: A gradient-based algorithm for non-linear data models \citep{zheng2018dags}.
%     \item GOLEM: A more efficient version of NOTEARS \citep{ng2020role}.
%     \item GRAN-DAG: A gradient-based algorithm using neural network modeling for non-linear additive noise data \citep{Lachapelle2020Gradient-Based}. 
%     \item AVICI: A powerful deep learning-based supervised causal learning method \citep{lorchamortized}.
%     \color{black}
% \end{enumerate}
\color{black}
\textbf{Baselines.} To demonstrate the effectiveness and superiority of the proposed framework, 
several representative baselines from multiple categories are selected for comparison. 
The PC algorithm is a classic constraint-based causal discovery algorithm based on conditional independence tests, and the version with parallelized optimization is selected \citep{le2016fast}.
GES, a classic score-based greedy equivalence search algorithm, is also included \citep{chickering2002optimal}.
For continuous optimization methods, we compare with NOTEARS \citep{zheng2018dags}, a representative gradient-based optimization method, and GOLEM \citep{ng2020role}, regarded as a more efficient variant of NOTEARS. 
For neural network based optimization algorithms, we compare with DAG-GNN \citep{yu2019dag}, an optimization algorithm based on graph neural networks, and GRAN-DAG, a gradient-based algorithm using neural network modeling \citep{Lachapelle2020Gradient-Based}.
For DNN-based SCL methods, we compare with AVICI, which is the most related work to ours and regarded as the current state-of-the-art method \citep{lorchamortized}.
\color{black}

\textbf{Implementation Details. }The implementation from gCastle \citep{zhang2021gcastle} is utilized for baselines except the DNN-based SCL method (i.e., AVICI). 
For the PC algorithm, we employ the Fisher-Z transformation with a significance threshold of $0.05$ for conditional independence tests, which is a prevalent choice in statistical analyses and current PC implementations \citep{zhang2021gcastle,zheng2024causal}.
Our criterion for graph selection in GES experiments is the Gaussian Bayesian Information Criterion (BIC), specifically the $l_\infty$-penalized Gaussian likelihood score. It is used in the original paper \citep{chickering2002optimal}, and remains a favored variant in the literature.
For NOTEARS, adhering to the official implementation's settings, we configure NOTEARS with a maximum of 100 dual ascent steps, and an edge dropping threshold of 0.3. For hyperparameters lacking specific default settings, such as the L1 penalty and loss function type, we default to settings used by gCastle \citep{zhang2021gcastle}, employing an L1 penalty of 0.1 and an L2 loss function.
For DAG-GNN, we utilize hyperparameter settings directly from the original implementation, ensuring consistency with established benchmarks.
For GOLEM and GRAN-DAG, we also use the default setting of gCastle \citep{zhang2021gcastle}.
\color{black}
Note that the CSIvA model \citep{ke2023learning} is also a closely related method, but it is not compared due to the unavailability of its relevant codes and its requirement for interventional data as input. 
The original AVICI model \citep{lorchamortized} does not support discrete data.
Therefore, we reproduce it, and use an embedding layer to replace its first linear layer when using AVICI on discrete data.
Our codes for SiCL implementation can be accessed at \url{https://anonymous.4open.science/r/sicl-744B/}.
% For DNN-based SCL methods, 
% We select several strong baselines to show the effectiveness and superiority of the proposed framework more comprehensively.
% PC is a classic constraint-based causal discovery algorithm based on conditional independence tests,  and we select the version with parallelized optimization \cite{le2016fast}. 
% GES is a classic score-based greedy equivalence search algorithm \cite{chickering2002optimal}. 
% We additionally compare with DirectLiNGAM \cite{shimizu2011directlingam}, a function-based learning algorithm, and DAG-GNN, a gradient-based learning algorithm based on graph neural networks.
% We utilize the implementation from gCastle \cite{zhang2021gcastle} for the baselines above.
% We also reproduce and compare with the AVICI model \cite{lorchamortized}, which is a powerful deep learning-based supervised causal learning method.
% Note that the CSIvA model \cite{ke2023learning} is also a closely related method, but its relevant codes are not publicly released and it requires interventional data as input, so we do not compare with it.
% All deep neural network-based baselines and our PAIRE are run at an Nvidia A100 GPU and all classic algorithms are run at an AMD EPYC 7V13 CPU.
% \paragraph{Environments.} Nvidia A100 GPU

\textbf{Synthetic Data.} We randomly generate random graphs from multiple random graph models. For continuous data, following previous work \citep{lorchamortized}, Erdős-Rényi (ER) and Scale-free (SF) are utilized as the training graph distribution $p(G)$.
The degree of training graphs in our experiments varies randomly among 1, 2, and 3.
\textcolor{black}{For testing graph distributions, Watts-Strogatz (WS) and Stochastic Block Model (SBM) are used, with parameters consistent with those in the previous paper \citep{lorchamortized}. }
All synthetic graphs for continuous data contain 30 nodes.
\textcolor{black}{The lattice dimension of Watts-Strogatz (WS) graphs is sampled from $\{2, 3\}$, yielding an average degree of about $4.92$. The average degrees of Stochastic Block Model (SBM) graphs are set at 2, following the settings in the aforementioned paper.}
For discrete data, 11-node graphs are used.
SF is utilized as the training graph distribution $p(G)$ and ER is used for testing.
\textcolor{black}{The synthetic training data is generated in real-time, and the training process does not use the same data repeatedly.}
All synthetic test datasets contain 100 graphs, and the average values of the metrics on the 100 graphs are reported to comprehensively reflect the performance.

For the forward sampling process from graph to continuous data, both the linear Gaussian mechanism and general nonlinear mechanism are applied.
Concretely, the Random Fourier Function mechanism is used for the general nonlinear data following the previous paper \citep{lorchamortized}.
% Therefore, it yields two types of continuous datasets. 
In synthesizing discrete datasets, the Bernoulli distribution is used following previous papers \citep{dai2023ml4c,ma2022ml4s}.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/abl.pdf}
    \caption{Illustration of the architecture comparison of Node-Edge models, SiCL-no-PF and SiCL.}
    \label{fig:abl}
\end{figure*}


\textbf{More Implementation Details and Computational Resources. } The two network modules, i.e., the SPN and VPN, are optimized by Adam optimizer with default hyperparameters. Following previous work \citep{lorchamortized}, the training batch size is set as 20. 
All classic algorithms are run on an AMD EPYC 7V13 CPU, and DNN-based methods are run on Nvidia 1080Ti, A40 and A100 GPUs. 
Training SiCL on a 30-node training set with batch size 20 needs about 60GB memory, while training on an 11-node training set needs about 20GB memory.

\color{black}

\begin{table*}[t]
\centering
    % \resizebox{\columnwidth}{!}{%
\begin{threeparttable}
\caption{\textbf{General comparison of SiCL and other methods}. The average performance results in three runs are provided for SiCL method. GES takes more than 24 hours per graph on WS-L-G hence the results are not included.}
\label{tab:epder}
\begin{tabular}{@{}ccccccccc@{}}
\toprule
\multirow{2}{*}{Dataset} &\multirow{2}{*}{Method} & \multicolumn{4}{c}{Skeleton Prediction }& \multicolumn{3}{c}{CPDAG Prediction} \\
 && s-F1$\uparrow$ & s-Acc.$\uparrow$ & s-AUC$\uparrow$ & s-AUPRC$\uparrow$ &  v-F1$\uparrow$ & o-F1$\uparrow$ & SHD$\downarrow$\\
 \midrule
\multirow{8}{*}{WS-L-G}&PC & $30.4 $&$ 65.6$&N/A&N/A&$ 15.6$&$16.0 $ &$170.4 $\\
% &GES & * &* &*&*&*&*&*\\
&NOTEARS & $33.3 $  & $ 65.1$&N/A&N/A&$ 27.9$&$31.5$ & $159.8$ \\
&DAG-GNN & $35.5$  & $ 55.4$&N/A&N/A&$32.2$&$32.7$ & $193.7 $ \\
&GRAN-DAG& $16.6$ & $62.1$ & N/A & N/A & $11.7$ & $11.7$ & $170.1$ \\
% &NOTEARS-MLP & $24.6$ & $60.3$ & N/A & N/A & $12.2$ & $11.8$ & $190.0$ \\
&GOLEM& $30.0$ & $63.4$ & N/A & N/A & $15.8$ & $19.3$ & $172.7$ \\
&AVICI & $39.9 $  & $ 74.0$ &$ 71.5$&$ 62.2$&$ 28.2$&$ 35.8$&$119.2$\\
&SiCL & $\mathbf{44.7} $ & $\mathbf{75.3} $&$\mathbf{73.7} $&$\mathbf{65.4} $&$\mathbf{32.0} $&$\mathbf{38.5} $&$\mathbf{116.1} $\\
 \midrule
\multirow{9}{*}{SBM-L-G}&PC & $58.8$&$90.0$&N/A&N/A &$34.8$&$35.9$&$56.4$\\
&GES & $70.8$&$89.4$ &N/A&N/A&$53.9$&$55.0$&$60.3$\\
&NOTEARS & $80.1$  & $94.5$&N/A&N/A&$76.2$&$77.8$ & $26.7$ \\
&DAG-GNN & $66.2$  & $87.4$&N/A&N/A&$60.3$&$62.5$ & $61.0$ \\
&GRAN-GAG & $22.6$  & $85.9$&N/A&N/A&$13.8$&$14.4$ & $64.7$ \\
% &NOTEARS-MLP & $ 44.7$ & $75.4 $ & N/A & N/A & $ 37.9$ & $39.0 $ & $112.5 $ \\
&GOLEM & $68.5$ &$88.5$ &N/A&N/A&$63.5$&$65.2$&$55.1$\\
&AVICI & $84.3$  & $96.2$ &$98.1$&$92.7$&$79.1$&$81.6$&$17.7$\\
&SiCL &  $\mathbf{85.8}  $  & $\mathbf{96.4}  $& $\mathbf{98.3}$& $\mathbf{93.4} $&$\mathbf{80.6}  $&$\mathbf{82.7}  $&$\mathbf{17.1}  $\\
\midrule
\multirow{9}{*}{WS-RFF-G}&PC & $36.1 $&$69.9$&N/A&N/A &$ 14.8$&$16.1$&$ 156.9$\\
&GES & $ 41.7$&$66.6$ &N/A&N/A&$21.1 $&$23.6$&$174.1$\\
&NOTEARS & $37.7 $  & $64.6 $& N/A & N/A &$30.9 $&$33.4 $ & $164.4 $ \\
&DAG-GNN & $33.2 $  & $ 65.4$&N/A&N/A&$27.0 $&$28.9 $ & $161.1 $ \\
&GRAN-DAG & $4.7 $  & $ 66.7$&N/A&N/A&$0.8 $&$1.1 $ & $146.9 $ \\
% &NOTEARS-MLP & $52.7 $ & $ 40.2$ & N/A & N/A & $ 44.2$ & $ 47.7$ & $282.8 $ \\
 &GOLEM & $27.6$ &$62.4$ &N/A&N/A&$13.8$&$17.7$&$175.8$\\
&AVICI & $47.7 $  & $75.9$ &$ 76.3$&$ 67.6$&$38.7$&$45.2 $&$110.6 $\\
&SiCL & $\mathbf{51.8}  $  & $ \mathbf{77.4}$ &$\mathbf{81.1}$&$ \mathbf{72.9}$&$ \mathbf{40.3}$&$ \mathbf{46.3}$&$ \mathbf{107.0} $\\
\midrule
\multirow{9}{*}{SBM-RFF-G}&PC & $57.5$&$89.3$&N/A&N/A &$32.7$&$ 34.2$&$60.9$\\
&GES & $56.5 $&$84.9$ &N/A&N/A&$37.0$&$38.0$&$82.4$\\
&NOTEARS & $55.6 $  & $86.2 $&N/A&N/A&$ 46.5$&$ 48.5$ & $66.3 $ \\
&DAG-GNN & $ 47.1$  & $82.1 $&N/A&N/A&$39.0$&$40.6$ & $86.2 $ \\
&GRAN-DAG & $17.4$ &$87.4$ & N/A& N/A &$3.2 $&$3.8$&$58.2$\\
% &NOTEARS-MLP & $48.2$ & $71.0$ & N/A & N/A & $41.0$ & $43.5$ & $130.5$ \\
&GOLEM & $31.1$ &$75.7$ & N/A& N/A &$23.0 $&$24.8$&$112.0$\\
&AVICI & $76.6$  & $ 94.5$ &$ 95.4$&$85.7 $&$69.3$&$72.7 $&$ 27.2$\\
&SiCL & $ \mathbf{82.1}$  & $ \mathbf{95.7}$ &$ \mathbf{97.1}$&$ \mathbf{90.7} $&$ \mathbf{75.7} $&$ \mathbf{78.0}$&$ \mathbf{21.9} $\\
\midrule
\multirow{8}{*}{ER-CPT-MC}&PC & $82.2$&$83.0$ &N/A&N/A&$39.2$&$40.6$&$16.4$\\
&GES & $ 82.1$&$81.8$ &N/A&N/A&$40.4$&$42.4$&$17.1$\\
&NOTEARS & $16.7$  & $74.8$&N/A&N/A&$0.2$&$0.6$& $16.1$ \\
&DAG-GNN & $ 24.8$  &  $73.5$&N/A&N/A&$ 3.4$&$3.7 $ & $15.9 $ \\
&GRAN-DAG & $40.8$ &$77.0$ & N/A& N/A &$6.8 $&$7.3$&$15.6$\\
% &NOTEARS-MLP & $ $ & $ $ & N/A & N/A & $ $ & $ $ & $ $ \\
&GOLEM& $37.6$ & $66.4$ & N/A & N/A & $4.6$ & $9.3$ & $21.9$ \\
&AVICI & $76.9$  & $88.4$ & $93.5 $& $87.9 $&$56.6$&$57.6$&$10.2$\\
&SiCL &  $\mathbf{84.2}$  & $\mathbf{90.1}$&$ \mathbf{96.6}$& $\mathbf{94.0} $& $ \mathbf{58.3}$&$\mathbf{59.9}$&$\mathbf{10.1}$\\
\bottomrule
\end{tabular}
\end{threeparttable}
% }
\end{table*}

\begin{table*}[tb]
    \centering
    % \resizebox{\linewidth}{!}{%
\begin{threeparttable}
\caption{Full ablation study results.}
\label{tab:fcplg}
\begin{tabular}{ccccccccc}
\toprule
Dataset &Method & s-F1$\uparrow$ & s-Acc.$\uparrow$ & s-AUC$\uparrow$ & s-AUPRC$\uparrow$ &  v-F1$\uparrow$ & o-F1$\uparrow$ & SHD$\downarrow$\\
 \midrule
\multirow{3}{*}{WS-L-G}& SiCL-Node-Edge & $39.9 $&$ 74.0$&$71.5$&$62.2$&$ 28.2$&$35.8 $ &$119.2 $\\
&SiCL-no-PF & $ 42.4$  & $ 74.4$ &$ 72.8$&$ 63.5$&$ 30.5$&$ 37.9$ &$118.4 $\\
&SiCL & $\mathbf{44.7} $ & $\mathbf{75.3} $&$\mathbf{73.7} $&$\mathbf{65.4} $&$\mathbf{32.0} $&$\mathbf{38.5} $&$\mathbf{116.1} $\\ \hline
\multirow{3}{*}{SBM-L-G}& SiCL-Node-Edge & $ 84.3$&$ 96.2$&$98.1$&$92.7$&$ 79.1$&$81.6 $ &$17.7 $\\
&SiCL-No-PF & $85.5$  & $\mathbf{96.4}$ &$\mathbf{98.3}$&$93.3$&$79.4$&$82.2$&$17.3$\\
&SiCL &  $\mathbf{85.8}$  & $\mathbf{96.4} $& $\mathbf{98.3} $& $\mathbf{93.4} $&$\mathbf{80.6}  $&$\mathbf{82.7}  $&$\mathbf{17.1}$\\
\bottomrule
\end{tabular}
\end{threeparttable}
% }
\end{table*}
\section{Extra Experimental Results} \label{sec:app:exp:e}
% \begin{table}[htbp]
% \centering
% \begin{threeparttable}
% \caption{Comparison between AVICI and PAIRE of Skeleton prediction task on RFF datasets}
% \label{tab:eprff}
% \begin{tabular}{@{}cccccc@{}}
% \toprule
% Graph  & Method & F1 & AUC & AUPRC & Acc.  \\
% \midrule
% % \multirow{2}{*}{SF} & AVICI &$0.776 \pm 0.006$ & $0.968 \pm 0.002$ & $0.889 \pm 0.001$ & $0.935 \pm 0.003$   \\
% % & PAIRE & $0.797 \pm 0.014$ & $0.979 \pm 0.001$ & $0.914 \pm 0.004$ & $0.942 \pm 0.006$\\ \hline
% \multirow{2}{*}{WS} & AVICI &$0.536 \pm 0.008$ & $0.756 \pm 0.003$ & $0.659 \pm 0.005$ & $0.728 \pm 0.002$   \\
% & PAIRE & $0.500 \pm 0.037$ & $0.792 \pm 0.012$ & $0.711 \pm 0.011$ & $0.771 \pm 0.004$\\ \hline
% \multirow{2}{*}{SBM} & AVICI &$0.739 \pm 0.000$ & $0.940 \pm 0.002$ & $0.827 \pm 0.002$ & $0.937 \pm 0.001$  \\
% & PAIRE &$0.809 \pm 0.004$ & $0.968\pm 0.001$ & $0.897\pm 0.002$ & $0.954\pm 0.001$\\ \hline
% \multirow{2}{*}{GRG} & AVICI &$0.882\pm 0.002$ & $0.966\pm 0.004$ & $0.891\pm 0.001$ & $0.994\pm 0.000$   \\
% & PAIRE & $0.901\pm 0.013$ & $0.979\pm 0.002$ & $0.923\pm 0.007$ & $0.995\pm 0.001$\\
% \bottomrule

% \end{tabular}
% \end{threeparttable}
% \end{table}


% \begin{figure}
%      \centering
%      \begin{subfigure}[b]{0.45\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{figures/wslg.pdf}
%          \caption{Linear Gaussian}
%          \label{fig:wslg}
%      \end{subfigure}
%      \hfill
%      \begin{subfigure}[b]{0.45\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{figures/wsrff.pdf}
%          \caption{General Nonlinear}
%          \label{fig:wsrff}
%      \end{subfigure}
%      \caption{Compairson between AVICI and SiCL of Skeleton prediction task on WS Graphs.}
%         \label{fig:cmpsws}
% \end{figure}

% \begin{figure}
%      \centering
%      \begin{subfigure}[b]{0.45\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{figures/grglg.pdf}
%          \caption{Linear Gaussian}
%          \label{fig:grglg}
%      \end{subfigure}
%      \hfill
%      \begin{subfigure}[b]{0.45\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{figures/grgrff.pdf}
%          \caption{General Nonlinear}
%          \label{fig:grgrff}
%      \end{subfigure}
%      \caption{Compairson between AVICI and PAIRE of Skeleton prediction task on GRG Graphs.}
%         \label{fig:cmpsgrg}
% \end{figure}


% \paragraph{Comparison with Extra Baselines.} To further demonstrate the effectiveness and superiority of the proposed framework, several other baselines representing multiple categories are selected for comparison. These baselines include:
% \begin{enumerate}
%     \item NOTEARS-MLP: A gradient-based algorithm for non-linear data models \citep{zheng2018dags}.
%     \item GOLEM: A more efficient version of NOTEARS \citep{ng2020role}.
%     \item GraNDAG: A gradient-based algorithm using neural network modeling for non-linear additive noise data \citep{Lachapelle2020Gradient-Based}. 
% \end{enumerate}


\begin{figure*}[t]
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/v_struc_ws.pdf}
         \caption{WS-LG}
         \label{fig:vws}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/v_struc_sbm.pdf}
         \caption{SBM-LG}
         \label{fig:vsbm}
     \end{subfigure}
     \caption{Variation trends of the test performance of the V-structure Prediction Network on WS-LG and SBM-LG during training.}
        \label{fig:tcs}
\end{figure*}
% 
% \textcolor{black}{Tab. \ref{tab:mc} - \ref{tab:mc2} presents additionally comparison between SiCL and these baseline methods on the WS dataset for both skeleton prediction task and CPDAG prediction task. 
% SiCL consistently demonstrates superior performance in comparison with these methods.}

% Fig. \ref{fig:cmpsws} presents an experimental comparison between AVICI and SiCL on WS random graphs for skeleton prediction. 
% These results reinforce the analysis in Sec. \ref{sec:exp:epr} and demonstrate the effectiveness of the proposed pairwise encoder module.


% \begin{table}[tb]
% \centering
% \caption{\color{black}More comparison of skeleton prediction results on WS-L-G.}
% \label{tab:mc}
% \begin{tabular}{ccc}
% \toprule
% Method & s-F1 & s-AUC \\
% \midrule
% GRANDAG & $16.3$ & $50.2$  \\
% NOTEARS-MLP & $25.6$ & $51.3$ \\
% GOLEM & $29.3$ & $53.9$ \\
% SiCL & $\mathbf{44.1}$ & $\mathbf{75.2}$\\
% \bottomrule
% \end{tabular}
% \end{table}

% \begin{table*}[tb]\color{black}
% \centering
% \caption{\color{black}More comparison of CPDAG prediction results on WS-L-G.}
% \label{tab:mc2}
% \begin{tabular}{cccc}
% \toprule
% Method  & v-F1 & o-F1 & SHD \\
% \midrule
% GRANDAG & $11.6$ & $11.5$ & $169.5$ \\
% NOTEARS-MLP  & $12.8$ & $12.6$ & $192.6$ \\
% GOLEM  & $15.8$ & $19.1$ & $172.6$ \\
% SiCL & $\mathbf{33.0}$ & $\mathbf{39.5}$ & $\mathbf{114.5}$ \\
% \bottomrule
% \end{tabular}
% \end{table*}
% \color{black}

% Fig. \ref{fig:tcs} presents the variation trends of the test performance of the v-structure prediction model on SBM and WS random graphs during the training process, where the feature extractor $FE$ in the model is fine-tuned from the skeleton prediction model.
% It increases sharply and reaches a relatively high performance after several initial epochs.
% It indicates the v-structure task is relatively easy and the pre-trained pairwise features from the skeleton prediction model are effective and general.
\subsection{Effectiveness of V-structure Prediction Network} Fig. \ref{fig:tcs} illustrates the test performance trends of the v-structure prediction model on SBM and WS random graphs during the training process. In this model, the feature extractor $FE$ is fine-tuned from the skeleton prediction model. The performance increases rapidly and achieves a relatively high level after just a few initial epochs. This suggests that our v-structure prediction network is capable of predicting v-structures, and indicates that the pre-trained pairwise features from the skeleton prediction model are both effective and generalizable.

%proving the effectiveness of the fine-tuning from skeleton prediction model. %the features from the skeleton prediction model 
% \paragraph{More Comparisons on Synthetic Discrete Data.}
% As the outputs from neural network models are the probability from $0$ to $1$ instead of a single prediction, the AUC and AUPRC are more suitable metrics for comparing DNN-based SCL methods.
% The comparison between AVICI and SiCL of skeleton prediction task on discrete data are provided in Tab. \ref{tab:epdersp1}.
% By adjusting the classification threshold of the DNN-based SCL methods, we can also compare them with classic methods.
% The results are presented in Tab. \ref{tab:epdersp2}.
% DNN-based SCL methods are obviously better and SiCL consistently performs the best under such a condition.
% Since neural network model outputs range from $0$ to $1$ as probabilities rather than single predictions, AUC and AUPRC are more appropriate metrics for comparing DNN-based SCL methods. The comparison between AVICI and SiCL for the skeleton prediction task on discrete data is provided in Tab. \ref{tab:epdersp1}. By adjusting the classification threshold of DNN-based SCL methods, we can also compare them with traditional methods. These results are presented in Tab. \ref{tab:epdersp2}. It is evident that DNN-based SCL methods outperform their counterparts, with SiCL consistently achieving the best performance under these conditions.


% \paragraph{More Results on Sachs.}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.45\textwidth]{figures/sprds.pdf}
%           \caption{Skeleton prediction results on the Sachs dataset.}
%     \label{fig:sprds}
% \end{figure}

% \begin{table}
% \centering
% \begin{threeparttable}
% \caption{Comparison of skeleton prediction task on ER random graphs of discrete data.}
% \label{tab:epdersp2}
% \begin{tabular}{@{}ccc@{}}
% \toprule
% Method & F1 & Accuracy  \\
% \midrule
% PC & $0.822$&$0.830$ \\
% GES & $ 0.821$&$0.818$ \\
% NOTEARS & $0.164$  & $0.747$ \\
% AVICI & $0.833$  & $0.914$ \\
% SiCL &  $\mathbf{0.862}$  & $\mathbf{0.921}$\\
% \bottomrule
% \end{tabular}
% \end{threeparttable}
% \end{table}
% We present the comparison on F1 score and accuracy on the Sachs dataset in Fig. \ref{fig:sprds}. The results demonstrate that DNN-based SCL methods consistently outperform classical approaches, thereby confirming their effectiveness and superiority in this context. 

\color{black}
\subsection{More Evidence on Effectiveness of Pairwise Representation}
To further support the effectiveness of using pairwise representation, we present additional experimental results on different training datasets and test datasets, including ER-L-G, SF-L-G, ER-RFF-G, and SF-RFF-G.
% other three test datasets. The graph distributions are Geometric Random Graphs (GRG) and Scale-free graphs with two different parameters (marked as SF1 and SF2). 
% The structural-equation distribution is random linear (L) and noise distribution is Gaussian distribution (G), following other experimental settings.
For models, we compare SiCL with a variant without pairwise representation, i.e., SiCL-no-PF.
% We also implement another variant as a baseline where the alternative attention in the encoder is removed and marked as SiCL-no-AA, and compare it with another variant without pariwise representation, i.e., SiCL-no-AA-no-PF.

The results are provided in Tab. \ref{tab:mpc}. Models with pairwise representation outperform the corresponding baseline models under almost all comparisons, further verifying the effectiveness of using pairwise representation in models.

% \begin{table}[t]\color{black}
% \centering
% % \resizebox{\linewidth}{!}{%
% \begin{threeparttable}
% \caption{\color{black}More performance comparison on the effectiveness of pairwise representation.}\label{sec:mpc}
% \label{tab:mpc}
% \begin{tabular}{cccccc}
% \toprule
% Test Dataset& Method & s-F1 & s-AUC & s-AUPRC & s-Acc. \\ \midrule
% \multirow{2}{*}{GRG-L-G} &SiCL-no-PF & $92.6$  &$99.2$&$95.8$& $99.6$ \\
% &SiCL & $\textbf{95.0}$ & $\textbf{99.5}$  &$\textbf{97.5} $ &$\textbf{99.7}$ \\ \hline
% \multirow{2}{*}{GRG-L-G} &SiCL-no-AA-no-PF &$0.3$ & $81.4$&$22.5$&$97.4$ \\
% &SiCL-no-AA & $\textbf{89.8}$& $\textbf{98.9}$& $\textbf{93.9}$& $\textbf{99.5}$\\ \hline

% \multirow{2}{*}{SF1-L-G} &SiCL-no-PF &$\textbf{78.0}$ &$98.0$ &$90.6$ &$95.1$\\
% &SiCL &$77.1$ &$\textbf{98.5}$ &$\textbf{93.0}$ &$\textbf{95.2}$\\ \hline
% \multirow{2}{*}{SF1-L-G} &SiCL-no-AA-no-PF &$0.1$ &$86.5$ &$51.3$ &$87.3$\\
% &SiCL-no-AA &$\textbf{66.4}$ &$\textbf{97.1}$ &$\textbf{86.0}$ &$\textbf{93.3}$\\ \hline

% \multirow{2}{*}{SF2-L-G} &SiCL-no-PF & $73.6$&$95.1$ &$86.1$ &$93.6$\\
% &SiCL& $\textbf{78.1}$&$\textbf{96.2}$ &$\textbf{89.9}$ &$\textbf{94.4}$\\ \hline
% \multirow{2}{*}{SF2-L-G} &SiCL-no-AA-no-PF &$0.03$&$75.6$ &$34.7$ &$86.7$\\
% &SiCL-no-AA & $\textbf{66.4}$&$\textbf{94.2}$ &$\textbf{82.1}$ &$\textbf{92.4}$\\ 
% \bottomrule
% \end{tabular}
% \end{threeparttable}
% % }
% \end{table}

\begin{table}[t]\color{black}
\centering
% \resizebox{\linewidth}{!}{%
\begin{threeparttable}
\caption{\color{black}More performance comparison on the effectiveness of pairwise representation.}\label{sec:mpc}
\label{tab:mpc}
\begin{tabular}{ccccccc}
\toprule 
Training Dataset & Test Dataset & Method & s-F1$\uparrow$ & s-AUC$\uparrow$ & s-AUPRC$\uparrow$ & s-Acc.$\uparrow$ \\
\midrule
\multirow{8}{*}{ER-L-G} & \multirow{2}{*}{ER-L-G} & SiCL-no-PF & 75.7 &84.6  & 83.1 & 78.5\\
 &  & SiCL &  80.2 & 89.6 & 90.1 &82.3 \\ \cline{2-7}
 & \multirow{2}{*}{SF-L-G} & SiCL-no-PF & 74.9 &  92.5& 87.3 &84.1 \\
 &  & SiCL & 79.0 & 96.0 & 93.7 & 87.0\\ \cline{2-7}
  & \multirow{2}{*}{ER-RFF-G} & SiCL-no-PF & 49.5 & 60.5 & 49.1 &58.8 \\
 &  & SiCL & 51.0 & 67.0 & 57.6 & 65.2\\ \cline{2-7}
  & \multirow{2}{*}{SF-RFF-G} & SiCL-no-PF & 40.4 & 57.9 & 38.9 & 57.5\\
 &  & SiCL & 46.4 & 71.4 & 53.7 & 69.0\\ \hline
 \multirow{8}{*}{SF-L-G} & \multirow{2}{*}{ER-L-G} & SiCL-no-PF & 64.6 & 77.3 & 68.7 & 70.7\\
 &  & SiCL & 68.0 & 82.1 & 76.4 & 74.3\\ \cline{2-7}
 & \multirow{2}{*}{SF-L-G} & SiCL-no-PF & 88.5 & 96.7 & 95.0 & 91.2\\
 &  & SiCL & 89.7 & 97.9 & 97.0 & 92.4\\ \cline{2-7}
  & \multirow{2}{*}{ER-RFF-G} & SiCL-no-PF & 44.3 & 62.3 & 50.9 & 58.4\\
 &  & SiCL & 47.0 & 66.2 & 55.8 & 63.3\\ \cline{2-7}
  & \multirow{2}{*}{SF-RFF-G} & SiCL-no-PF & 48.1 & 71.6 & 53.9 & 65.8 \\
 &  & SiCL & 56.0 & 79.6 & 64.8 & 74.2\\ \hline
  \multirow{8}{*}{ER-RFF-G} & \multirow{2}{*}{ER-L-G} & SiCL-no-PF & 64.0 & 73.3 & 65.8 & 67.3 \\
 &  & SiCL & 72.0 & 82.0 & 81.1 & 75.2 \\ \cline{2-7}
 & \multirow{2}{*}{SF-L-G} & SiCL-no-PF & 58.1 & 79.0 & 66.8 & 72.8\\
 &  & SiCL & 70.1 & 88.0 & 83.6 & 80.9\\ \cline{2-7}
  & \multirow{2}{*}{ER-RFF-G} & SiCL-no-PF & 63.2 & 74.3 & 67.7 & 71.0\\
 &  & SiCL & 74.8 & 85.7 & 84.5 & 79.7\\ \cline{2-7}
  & \multirow{2}{*}{SF-RFF-G} & SiCL-no-PF & 56.3 & 78.3 & 65.9 & 75.0 \\
 &  & SiCL & 68.2 & 87.0 & 81.5 & 82.1 \\ \hline
  \multirow{8}{*}{SF-RFF-G} & \multirow{2}{*}{ER-L-G} & SiCL-no-PF & 60.3 &  71.2 & 58.7 & 64.7\\
 &  & SiCL & 65.6 & 78.0 & 72.5 & 70.5\\ \cline{2-7}
 & \multirow{2}{*}{SF-L-G} & SiCL-no-PF & 73.6 & 90.5 & 82.9 & 81.0\\
 &  & SiCL & 79.1 & 94.2 & 90.0 & 85.5\\ \cline{2-7}
  & \multirow{2}{*}{ER-RFF-G} & SiCL-no-PF & 57.7 & 71.4 & 60.7 & 66.8\\
 &  & SiCL & 67.2 & 80.5 & 75.8 & 73.9\\ \cline{2-7}
  & \multirow{2}{*}{SF-RFF-G} & SiCL-no-PF & 74.8 & 90.2 & 82.4 & 83.5\\
 &  & SiCL & 80.4 & 94.2 & 90.5 & 87.3\\ 
\bottomrule 
\end{tabular}
\end{threeparttable}
% }
\end{table}

\subsection{Comparison with Autoregressive models on Inference Time Costs} \label{sec:auto}
To validate that the autoregressive models have relatively high time costs due to the quadratic number of inference runs w.r.t. number of variables, we reproduce the network architecture of a representative autoregressive model, i.e., CSIvA \citep{ke2023learning}, and compare SiCL with it.
We use the same random input for both models with an increasing number of variables.
The results are provided in Fig. \ref{fig:itc}.
The time costs of the autoregressive model show a fast increasing trend and are much more than costs of SiCL, validating the correctness of our analysis.

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{AISTATS2025PaperPack/figures/inference_time_costs_auto.pdf}
    \caption{Comparison between an autoregressive model and SiCL on inference time costs.}
    \label{fig:itc}
\end{figure}

\color{black}
\subsection{Training Data Diversity and Model Generalization} We present experimental evidence that highlights the significant contribution of training data diversity to the model's generalization capabilities, even when applied to out-of-distribution (OOD) datasets. 
To illustrate this, we train one SiCL model on a combined dataset of both SF and ER, and another solely on the SF dataset. 
The comparative performance of these models is detailed in Tab. \ref{tab:trainood}. 
The model trained on the combined ER and SF datasets exhibited markedly better performance, not only on the ER dataset but also on the other two OOD datasets, with only a marginal decrease in performance on the SF dataset. 
These findings suggest that enhancing the diversity of the training data correspondingly improves the model’s ability to generalize and maintain robust performance across novel OOD datasets.

\begin{table*}[tb]\color{black}
    \centering
    \caption{\color{black}Comparison of SiCL models with different training data diversity on skeleton prediction.}
    \label{tab:trainood}
    \begin{subtable}{\linewidth}
      \centering
        \caption{\color{black}Model trained on both ER and SF}
        \begin{tabular}{lcccc}
            \toprule
            Test Dataset & s-F1$\uparrow$     & s-AUC$\uparrow$    & s-AUPRC$\uparrow$  & s-Acc.$\uparrow$    \\
            \midrule
            WS-L-G      & 36.3 & 70.6 & 60.6 & 73.3 \\
            SBM-L-G     & 78.1 & 96.8 & 88.1 & 94.8 \\
            ER-L-G      & 80.7 & 96.0 & 89.2 & 94.7 \\
            SF-L-G      & 84.7 & 98.5 & 93.6 & 95.5 \\
            \bottomrule
        \end{tabular}
    \end{subtable}%
    \\
    \begin{subtable}{\linewidth}
      \centering
        \caption{\color{black}Model trained on SF}
        \begin{tabular}{lcccc}
            \toprule
            Test Dataset & s-F1$\uparrow$     & s-AUC$\uparrow$    & s-AUPRC$\uparrow$  & s-Acc.$\uparrow$    \\
            \midrule
            WS-L-G      & 40.1 & 63.0 & 46.1 & 63.5 \\
            SBM-L-G     & 64.3 & 91.7 & 72.9 & 90.9 \\
            ER-L-G      & 67.1 & 90.4 & 73.9 & 90.8 \\
            SF-L-G      & 87.8 & 98.9 & 95.3 & 96.1 \\
            \bottomrule
        \end{tabular}
    \end{subtable}
\end{table*}


\subsection{Varying Amount of Training Graphs}
We present an analysis of how varying the amount of the training graphs influences performance on the skeleton prediction task. The results, depicted in Fig. \ref{fig:ts}, illustrate a clear trend: model performance improves in tandem with the expansion of the training dataset. This trend underscores the potential of our method to achieve even greater accuracy given a more extensive dataset.
\begin{figure*}[!ht]
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
    \includegraphics[width=\linewidth]{figures/wstrainingsize.pdf}
         \caption{\color{black}WS dataset}
         \label{fig:ts1}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
    \includegraphics[width=\linewidth]{figures/sbmtrainingsize.pdf}
         \caption{\color{black}SBM dataset}
         \label{fig:ts2}
     \end{subfigure}
         \caption{\color{black}Model performance with varying amount of training graphs.}
        \label{fig:ts}
\end{figure*}

\subsection{Varying Sample Size} We assess SiCL across various quantities of observational samples per graph during testing (100, 200, ..., 1000). The outcomes for both the skeleton prediction task and the CPDAG prediction task are depicted in Fig. \ref{fig:vtss}. It is evident that the model's performance is enhanced with the augmentation of sample size. These consistent upward trends suggest that SiCL exhibits stability and is not overly sensitive to changes in sample size.

\begin{figure*}[t]
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/ws1.pdf}
         \caption{\color{black}Variation trends of skeleton prediction task performance on WS graph with varying sample sizes.}
         \label{fig:vtss1}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/ws2.pdf}
         \caption{\color{black}Variation trends of CPDAG prediction task performance on WS graph with varying sample sizes.}
         \label{fig:vtss2}
     \end{subfigure}
         \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/sbm1.pdf}
         \caption{\color{black}Variation trends of skeleton prediction task performance on SBM graph with varying sample sizes.}
         \label{fig:vtss3}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/sbm2.pdf}
         \caption{\color{black}Variation trends of CPDAG prediction task performance on SBM graph with varying sample sizes.}
         \label{fig:vtss4}
     \end{subfigure}
         \caption{\color{black}Variation trends of performance with varying sample sizes.}
        \label{fig:vtss}
\end{figure*}

\subsection{Varying Edge Density}
We evaluate SiCL over a range of edge densities in the test graphs, utilizing the SBM dataset, as it allows for the direct setting of average edge densities. The findings are presented in Fig. \ref{fig:vted}. It's apparent that the task is becomes more difficult as edge densities increase. However, the performance decline is not abrupt, indicating that SiCL's performance remains relatively stable across various edge densities, thereby confirming its versatility.
\begin{figure*}
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/sbmskeletondensity.pdf}
         \caption{\color{black}Variation trends of skeleton prediction task performance on SBM graph with varying edge densities.}
         \label{fig:vted1}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/sbmcpdagdensity.pdf}
         \caption{\color{black}Variation trends of CPDAG prediction task performance on SBM graph with varying edge densities.}
         \label{fig:vted2}
     \end{subfigure}
         \caption{\color{black}Variation trends of performance with varying edge densities.}
        \label{fig:vted}
\end{figure*}

\subsection{Generality on Testing Graph Sizes}
We offer an analytical perspective on the performance of the SiCL model when applied to larger WS-L-G graphs. 
It is important to highlight that the models were initially trained on graphs comprising 30 vertices, positioning this task within an out-of-distribution setting in terms of graph size. 
To establish a point of reference, we have included results from the PC algorithm as a baseline comparison.
These findings can be examined in Tab. \ref{tab:mpva}.
Despite the OOD conditions, SiCL maintains robust performance, reinforcing its scalability and the model's general applicability across varying graph sizes.


\begin{table}[t]\color{black}
\centering
% \resizebox{\linewidth}{!}{%
\begin{threeparttable}
\caption{\color{black}Performance comparison with varying amounts of graph sizes.}
\label{tab:mpva}
\begin{tabular}{l|ccc|ccc|ccc}
\toprule
Metric & \multicolumn{3}{c|}{s-F1$\uparrow$} & \multicolumn{3}{c|}{v-F1$\uparrow$} & \multicolumn{3}{c}{o-F1$\uparrow$} \\
Size & 50 & 70 & 100 & 50 & 70 & 100 & 50 & 70 & 100 \\
\midrule
PC       & $17.7$ & $14.8$ & $10.6$ & $6.4$ & $5.0$ & $3.7$ & $7.0$ & $5.6$ & $4.0$ \\
SiCL & $\mathbf{41.6}$ & $\mathbf{37.4}$ & $\mathbf{28.3}$ & $\mathbf{34.9}$ & $\mathbf{30.7}$ & $\mathbf{22.6}$ & $\mathbf{37.9}$ & $\mathbf{33.7}$ & $\mathbf{24.8}$ \\
\bottomrule
\end{tabular}
\end{threeparttable}
% }
\end{table}



\subsection{Acyclicity}
% \begin{wraptable}[8]{r}{9cm}
\begin{table}[t]\color{black}
\centering
% \resizebox{\linewidth}{!}{%
\begin{threeparttable}
\caption{\color{black}Count of cycles in the CPDAG predictions without post-processing of removing cycles.}
\label{tab:ccfc}
\begin{tabular}{@{}ccc@{}}
\toprule
Dataset & WS-L-G & SBM-L-G  \\
\midrule
Rate of Graphs with Cycles & $0.66 \pm 0.66 \%$&$0.00 \pm 0.00 \%$ \\
\bottomrule
\end{tabular}
\end{threeparttable}
% }

\end{table}
We provide an empirical evidence supporting of the rarity of cycles in the predictions. The experimental data presented in Tab. \ref{tab:ccfc} corroborates that cycles are infrequently observed in the predicted CPDAGs, even though without any post-processing on removing cycles.
\color{black}
\section{Limitations and Future Work} \label{sec:lim}
\color{black}
% This paper focuses on applying DNNs to solve SCL problems. 
This paper also introduces a few interesting open problems that warrant further exploration.
Due to the inherent complexity of DNNs, the explanation of such a decision mechanism remains an open question. 
Therefore, future work could consider exploring how decisions are made within the networks and provide some insights into traditional SCL methods. 
Moreover, the proposed pairwise encoder modules need $O(d^3)$ computational complexity, which may restrict its current application to scenarios with huge number of nodes.
Future work could focus on simplifying these operations or exploring features with less complexity (e,g., low rank features) to reduce the overall computational cost.
\color{black}

% \textbf{Acyclicity.}
% % \begin{wraptable}[8]{r}{9cm}
% \begin{table*}[tb]\color{black}
% \centering
% \begin{threeparttable}
% \caption{\color{black}Count of cycles in the CPDAG predictions without post-processing of removing cycles.}
% \label{tab:ccfc}
% \begin{tabular}{@{}ccc@{}}
% \toprule
% Dataset & WS & SBM  \\
% \midrule
% Rate of Graphs with Cycles & $0.66 \pm 0.66 \%$&$0.00 \pm 0.00 \%$ \\
% \bottomrule
% \end{tabular}
% \end{threeparttable}
% \end{table*}
% We provide an empirical evidence supporting of the rarity of cycles in the predictions. The experimental data presented in Tab. \ref{tab:ccfc} corroborates that cycles are infrequently observed in the predicted CPDAGs, even though without any post-processing on removing cycles.


% \textbf{Generality on Testing Graph Sizes.}
% We offer an analytical perspective on the performance of the SiCL model when applied to larger WS graphs. 
% It is important to highlight that the models were initially trained on graphs comprising 30 vertices, positioning this task within an out-of-distribution setting in terms of graph size. 
% To establish a point of reference, we have included results from the PC algorithm as a baseline comparison.
% These findings can be examined in Tab. \ref{tab:mpva}.
% Despite the OOD conditions, SiCL maintains robust performance, reinforcing its scalability and the model's general applicability across varying graph sizes.


% \begin{table*}[ht]\color{black}
% \centering
% \begin{threeparttable}
% \caption{\color{black}Performance comparison with varying amounts of graph sizes.}
% \label{tab:mpva}
% \begin{tabular}{l|ccc|ccc|ccc}
% \toprule
% Metric & \multicolumn{3}{c|}{s-F1} & \multicolumn{3}{c|}{v-F1} & \multicolumn{3}{c}{o-F1} \\
% Size & 50 & 70 & 100 & 50 & 70 & 100 & 50 & 70 & 100 \\
% \midrule
% PC       & $17.7$ & $14.8$ & $10.6$ & $6.4$ & $5.0$ & $3.7$ & $7.0$ & $5.6$ & $4.0$ \\
% SiCL & $41.6$ & $37.4$ & $28.3$ & $34.9$ & $30.7$ & $22.6$ & $37.9$ & $33.7$ & $24.8$ \\
% \bottomrule
% \end{tabular}
% \end{threeparttable}
% \end{table*}




% OFT also introduces a few interesting open problems. First, OFT guarantees the orthogonality via
% Cayley parametrization which involves a matrix inverse. It slightly limits the scalability of OFT.
% Although we address this limitation using block diagonal parametrization, how to speed up this
% matrix inverse in a differentiable way remains a challenge. Second, OFT has unique potential in
% compositionality, in the sense that the orthogonal matrices produced by multiple OFT finetuning tasks
% can be multiplied together and remains an orthogonal matrix. Whether this set of orthogonal matrices
% preserve the knowledge of all the downstream tasks remains an interesting direction to study. Finally,
% the parameter efficiency of OFT is largely dependent on the block diagonal structure which inevitably
% introduces additional biases and limits the flexibility. How to improve the parameter efficiency in a
% more effective and less biased way remains an important open problem.

% Note that although the skeleton predictions generated by PAIRE are somewhat less accurate than those produced by AVICI, the final CPDAG prediction results from PAIRE, which predict no v-structures, still surpass those of AVICI. 
    
% As we discussed above, AUC and AUPRC are more appropriate metrics for comparing DNN-based SCL methods. The comparison between AVICI and PAIRE for the skeleton prediction task on Sachs is provided in Tab. \ref{tab:cprdsaucauprc}. PAIRE outperforms AVICI under these comparisons, further verifying the effectiveness of the proposals.

% \subsection{Effectiveness of Learning Identifiable Structures} \label{sec:exp:elis}
% As theoretically analyzed in Sec. \ref{sec:met:lic}, learning identifiable causal structure instead of learning DAG reduces the learning difficulty and prediction error. 
% Experimental results presented in Tab. \ref{tab:crddp} support our analysis. 
% We compared PAIRE with the model with the same network structure where the DAG is directly predicted. 
% The predictions from the PAIRE framework are more accurate than the directly predicted DAG, especially on the GRG dataset, which supports our analysis that learning identifiable causal structures is necessary and helpful. 
% As discussed in Sec. \ref{sec:met:lic}, PAIRE focuses on learning identifiable causal structures rather than directly learning the adjacency matrix of the DAG. 
% In Tab. \ref{tab:crddp}, we provide empirical support for the superiority of learning identifiable structures.
% We conducted a performance comparison between PAIRE and a model that employs the same network structure but directly predicts the adjacency matrix.
% Consistently, PAIRE outperforms the alternative model by generating predictions with higher accuracy.
%, especially when evaluated on the GRG dataset.
% This compelling observation further strengthens our argument that learning identifiable causal structures is not only necessary but also beneficial for enhancing the overall performance of the model.
% Note that, Model with DAG pred is only an ablation study, 
% when prediction is not CPDAG, it cannot use the strength of PAIRE
% We compare the performance of PAIRE with a model that has the same network structure but directly predicts the adjacency matrix. 
% The predictions generated by PAIRE consistently demonstrate higher accuracy than those obtained from the other model, particularly on the GRG dataset. 
% This observation lends further credence to our argument that learning identifiable causal structures is necessary and beneficial for improving the overall performance of the model. 
% By prioritizing identifiable causal structures, our approach offers a more robust and reliable solution for tackling complex causal inference problems.


\end{document}
