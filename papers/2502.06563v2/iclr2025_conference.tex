
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{multirow}
\usepackage{hyperref}
\usepackage{url}
\usepackage{makecell}
\usepackage{colortbl}
\usepackage{multirow}
\usepackage{pifont}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{graphicx}


\definecolor{pale_red}{rgb}{0.90,0.61,0.58}
\definecolor{pale_green}{rgb}{0.55,0.75,0.60}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{darkgreen}{rgb}{0.09, 0.46, 0.16}
\definecolor{darkred}{rgb}{0.72, 0.13, 0.07}

\lstdefinestyle{mystyle}{
  frame=single,
  basicstyle=\ttfamily\footnotesize,
  backgroundcolor=\color{backcolour}, commentstyle=\color{codegreen},
  commentstyle=\color{darkgreen}\slshape,
  keywordstyle=\color{blue},
  stringstyle=\color{darkred},
  numberstyle=\tiny\color{codegray},
  emphstyle=\color{pink}\underbar,
  morekeywords={Verify, Question},
  escapeinside={(*@}{@*)},
  breakatwhitespace=false,         
  breaklines=true,                 
  captionpos=b,                    
  keepspaces=true,                    
  numbersep=5pt,                  
  showspaces=false,                
  showstringspaces=false,
  showtabs=false,
  tabsize=2
}

\title{Large Language Models Meet Symbolic Provers for Logical Reasoning Evaluation}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Chengwen Qi$^{1,2}$\thanks{Equal contribution} \quad Ren Ma$^{2*}$ \quad Bowen Li$^{2*}$ \quad \textbf{He Du}$^{2,3}$ \quad \textbf{Binyuan Hui} \quad \textbf{Jinwang Wu}$^{1}$ \\ \textbf{Yuanjun Laili}$^{1,4,5}$\thanks{Corresponding authors} \quad \textbf{Conghui He}$^{2\dagger}$ \\
$^1$Beihang University \quad  
$^2$Shanghai Artificial Intelligence Laboratory \quad \\
$^3$Fudan University \quad 
$^4$Zhongguancun Laboratory \quad \\
$^5$State Key Laboratory of Intelligent Manufacturing Systems Technology, Beijing \quad \\
\texttt{\{Chengwen\_qi, lailiyuanjun\}@buaa.edu.cn} \\
\texttt {\{maren, libowen, heconghui\}@pjlab.org} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\usepackage[most]{tcolorbox}
\newtcbox{\hlprimarytab}{on line, rounded corners, box align=base, colback=green!10,colframe=white,size=fbox,arc=3pt, before upper=\strut, top=-2pt, bottom=-4pt, left=-2pt, right=-2pt, boxrule=0pt}
\newtcbox{\hlsecondarytab}{on line, box align=base, colback=red!10,colframe=white,size=fbox,arc=3pt, before upper=\strut, top=-2pt, bottom=-4pt, left=-2pt, right=-2pt, boxrule=0pt}

\newtcbox{\oodtab}{on line, box align=center, colback=red!10,colframe=white,size=fbox,arc=3pt}
\newtcbox{\indtab}{on line, box align=center, colback=green!10,colframe=white,size=fbox,arc=3pt}
\newcommand{\dashifted}{\raisebox{0.5\depth}{\tiny$\uparrow$}}
\newcommand{\uashifted}{\raisebox{0.5\depth}{\tiny$\downarrow$}}
\newcommand{\dar}[1]{{\raisebox{0.6ex}{\tiny\hlsecondarytab{\uashifted{#1}}}}}
\newcommand{\uar}[1]{{\raisebox{0.6ex}{\tiny\hlprimarytab{\dashifted{#1}}}}}

\newcommand{\ood}[1]{{\raisebox{0.6ex}{\oodtab{#1}}}}
\newcommand{\ind}[1]{{\raisebox{0.6ex}{\indtab{#1}}}}



\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
First-order logic (FOL) reasoning, which involves sequential deduction, is pivotal for intelligent systems and serves as a valuable task for evaluating reasoning capabilities, particularly in chain-of-thought (CoT) contexts.
Existing benchmarks often rely on extensive human annotation or handcrafted templates, making it difficult to achieve the necessary complexity, scalability, and diversity for robust evaluation.
To address these limitations, we propose a novel framework called ProverGen that synergizes the generative strengths of Large Language Models (LLMs) with the rigor and precision of symbolic provers, enabling the creation of a scalable, diverse, and high-quality FOL reasoning dataset, ProverQA.
ProverQA is also distinguished by its inclusion of accessible and logically coherent intermediate reasoning steps for each problem.
Our evaluation shows that state-of-the-art LLMs struggle to solve ProverQA problems, even with CoT prompting, highlighting the dataset's challenging nature.
We also finetune Llama3.1-8B-Instruct on a separate training set generated by our framework.
The finetuned model demonstrates consistent improvements on both in-distribution and out-of-distribution test sets, suggesting the value of our proposed data generation framework. Code available at: \url{https://github.com/opendatalab/ProverGen}


\end{abstract}


\section{Introduction}
First-order logic (FOL) reasoning, which involves sequential deduction and the application of facts and logical rules to derive conclusions or make decisions, is a fundamental aspect of human intelligence~\citep{prontoqa1, logicllm}.
The complexity of performing multi-step reasoning while maintaining logical coherence across these steps makes FOL reasoning an ideal testbed for evaluating the reasoning capabilities of Large Language Models (LLMs), particularly in scenarios such as Chain-of-Thought (CoT) prompting~\citep{chainofthought} and planning~\citep{wang2024describe, valmeekam2022large}. 
An effective benchmark for FOL reasoning should satisfy several key criteria: 
(1) \emph{Scalability}, 
enabling expansion in both volume and complexity with minimal manual intervention;
(2) \emph{Natural and Diverse Language}, 
capturing a wide range of natural language expressions to reflect real-world linguistic variability; 
(3) \emph{Symbolic Representations},
providing formal symbolic structures that can be validated through automated symbolic provers, ensuring dataset integrity and supporting downstream tasks like NL-FOL translation and tool based logic problem solving~\citep{logicllm, linc}; and
(4) \emph{Faithful Reasoning Chains},
where each instance includes intermediate reasoning steps clearly articulated in both symbolic and natural language formats, promoting transparency and facilitating further model training.

However, existing FOL reasoning datasets only partially fulfill these requirements (see Table~\ref{tab:comparison} for more details). 
Template-based datasets such as ProntoQA~\citep{prontoqa1, prontoqaood}, ProofWriter~\citep{tafjord2021proofwriter}, and RuleTaker~\citep{ruletaker} are scalable but often lack diversity and rely on overly simplistic rules. 
On the contrary, manually curated datasets like FOLIO~\citep{folio} offer rich diversity in natural language expressions and more complex logic rules, but they are limited in size due to the extensive human effort needed for creation. 
Additionally, data contamination is also a significant issue in manually annotated datasets, as it is difficult to update them frequently. This limitation can lead to biased evaluations and hinder true generalization due to potential data leakage.
Another common limitation across previous datasets is the lack of well-defined and easily accessible reasoning chains.
FOLIO, for instance, does not provide any reasoning chains, while ProofWriter and RuleTaker include reasoning chains, but they are not readily accessible and require users to implement additional code to extract them for each problem. 

\begin{table}[ht]
\setlength\tabcolsep{4.5pt}
\caption{Comparison of existing FOL reasoning datasets with ProverQA.}
\label{tab:comparison}
\begin{center}
\begin{tabular}{lccccc}
\toprule
\small \textbf{Dataset} & \bf\small \makecell{Creation\\ Method} & \bf\small \makecell{Scalability} & \bf\small \makecell{Natural \& Diverse\\ Language} & \bf\small \makecell{Symbolic \\ Representations} & \bf\small \makecell{Faithful \\ Reasoning Chains}\\
\midrule
\small RuleTaker & \small Synthetic & \textcolor{darkgreen}{\ding{51}} & \textcolor{darkred}{\ding{55}} & \textcolor{darkred}{\ding{55}} & \textcolor{darkgreen}{\ding{51}}  \\
\small ProofWriter & \small Synthetic & \textcolor{darkgreen}{\ding{51}} & \textcolor{darkred}{\ding{55}} & \textcolor{darkred}{\ding{55}} & \textcolor{darkgreen}{\ding{51}} \\
\small ProntoQA & \small Synthetic & \textcolor{darkgreen}{\ding{51}} & \textcolor{darkred}{\ding{55}} & \textcolor{darkgreen}{\ding{51}} & \textcolor{darkgreen}{\ding{51}} \\
\small ProntoQA-OOD & \small Synthetic & \textcolor{darkgreen}{\ding{51}} & \textcolor{darkred}{\ding{55}} & \textcolor{darkgreen}{\ding{51}} & \textcolor{darkgreen}{\ding{51}} \\
\small LogicNLI & \small Synthetic & \textcolor{darkgreen}{\ding{51}} & \textcolor{darkred}{\ding{55}} & \textcolor{darkred}{\ding{55}} & \textcolor{darkred}{\ding{55}} \\
\small FOLIO & \small Manual & \textcolor{darkred}{\ding{55}} & \textcolor{darkgreen}{\ding{51}} &  \textcolor{darkgreen}{\ding{51}} & \textcolor{darkred}{\ding{55}} \\
\midrule
\small ProverQA & \small Synthetic & \textcolor{darkgreen}{\ding{51}} & \textcolor{darkgreen}{\ding{51}} & \textcolor{darkgreen}{\ding{51}} & \textcolor{darkgreen}{\ding{51}}\\
\bottomrule
\end{tabular}
\end{center}
\end{table}

To address these gaps, we propose a novel framework that leverages the strengths of LLMs and symbolic provers to generate an extensible, diverse, and high-quality dataset for FOL reasoning.  
While LLMs have been increasingly utilized for synthesizing datasets across various tasks~\citep{alpaca, acharya2023llm, gajjar2024oran}, they fall short in producing reliable reasoning, often lacking faithfulness and transparency~\citep{golovneva2022roscoe, ribeiro2022street, lyu2023faithful}, particularly when it comes to long reasoning chains.
On the other hand, symbolic provers are known for their reliability and precision in executing complex logical inferences. 
By combining the generative capabilities of LLMs with the rigor of symbolic provers, our framework offers a robust solution for producing FOL reasoning data that ensures both diversity and logical soundness. 
It also mitigates the data contamination challenge by enabling the generation of new datasets using diverse models and controlled complexity, ensuring that the datasets remain fresh and uncontaminated.
Our framework follows a structured three-step pipeline: 
(1) we first use an LLM to generate a unique background story for each problem based on a given subject and a characteristic keyword;
(2) we employ a symbolic prover (i.e., Prover9) to construct reasoning trees and compute the truth values of relevant facts; and 
(3) we utilize an LLM to translate these logical expressions into natural language statements.


We use Llama3.1-70B-Instruct within our proposed generation framework to create a new FOL reasoning benchmark, \textbf{ProverQA}, consisting of 1,500 instances across three difficulty levels: \textit{easy}, \textit{medium}, and \textit{hard}. 
We evaluate ProverQA on several state-of-the-art (SOTA) LLMs and find that even with CoT prompting, models barely exceed 50\% accuracy on the hard subset, highlighting the significant challenge posed by ProverQA.
ProverQA's difficulty settings are carefully designed to ensure appropriate complexity, avoiding both performance saturation and uninformative under-performance (e.g., random guessing), enabling accurate evaluation of both powerful and smaller language models' reasoning abilities.
To further explore the potential of our proposed generation framework, we finetune Llama3.1-8B-Instruct on a separately generated ProverQA training set and observe substantial performance gains on in-distribution (ID) evaluations.
Remarkably, this finetuned model also shows consistent improvements on out-of-distribution (OOD) test sets, significantly outperforming the same model finetuned on other existing FOL training sets.

\section{Related Works}
\label{sec:rela_work}

\paragraph{Logical Reasoning Datasets}
Logical reasoning is a fundamental ability of intelligent systems and researchers have proposed a number of datasets to evaluate logical reasoning abilities of language models. 
These datasets can be divided into two groups: manually crafted ones and automatically generated ones. 
Human-crafted datasets are typically sourced from existing examinations~\citep{reclor, logiqa, logiqa2, arlsat} or directly written by domain experts~\citep{falseqa, folio}. 
Although they often exhibit higher quality and complexity, they also suffer from limited quantity and high creation costs.
Automatically generated datasets are often created using rules and predefined natural language templates~\citep{clutrr, ltl, ruletaker, tafjord2021proofwriter, logicnli, prontoqa1, prontoqaood, conditionalqa}. 
While these datasets enable automatic problem generation, they often lack linguistic diversity and are constrained by rigid, monotonous rules. 
None of them cover all aspects of first-order logic, and they cannot be used to generate problems with more complex rules.


\paragraph{Symbolic Prover Augmented LLMs}
Symbolic provers are computer programs used to generate proofs and solutions of mathematical theorems automatically. 
In logical reasoning problems, symbolic provers, such as SymPy~\citep{sympy}, Z3~\citep{z3}, Lean~\citep{lean}, Pyke~\citep{pyke} and Prover9~\citep{prover9}, stand out with faithful solutions and traceable intermediate processes. 
Recent works~\citep{logicllm, linc, satlm, zhang2024sola} combine LLMs and symbolic provers to solve logic reasoning problems. 
They use LLMs to translate problems from natural language into symbolic representations and then solve that problem with a symbolic prover. 
In contrast to previous works that utilize symbolic provers solely as problem solvers, our approach integrates symbolic provers into the generation process of first-order logic (FOL) problems. 
This integration allows us to bypass the intricate interrelations and couplings among logical relations, enabling the generation of diverse rules and their correct chaining to form comprehensive problems.

\section{Methodology}
\label{sec:method}

\begin{figure}[ht]
\begin{center}
\includegraphics[width=\linewidth]{main_frame.pdf}
\end{center}
\caption{An overview of our ProverGen framework. 
(a) \textbf{Background Story Generation} (Section \ref{subsubsec:background}). 
Given a subject, \texttt{Sawyer}, and a seed keyword, \texttt{elegant}, LLMs generate a background story to establish context and ensure linguistic diversity. 
(b) \textbf{Logic Skeleton Generation} (Section \ref{subsubsec:premise_generation}). 
A top-down approach is used to generate the logic skeleton, forming the reasoning tree for the FOL problem. 
This involves two iterative steps: expression sampling and truth value calculation using the Prover9 prover. 
Distractions are also incorporated to test the robustness of model's reasoning capabilities.
(c) \textbf{Statement Translation} (Section \ref{subsubsec:translation}). 
LLMs translate the facts and rules from the logic skeleton into natural language, guided by the previously generated background story. Each rule is translated into a universal version and a specific version. The universal version is preferred if it does not contradict with common sense, otherwise the specific one is selected.}
\label{fig:main}
\end{figure}

\subsection{Task Formulation}
Given a set of premises $\mathcal{P}$, consisting of facts $\mathcal{F}$ and rules $\mathcal{R}$, denoted as $\mathcal{P}=(\mathcal{F}, \mathcal{R})$, the goal of FOL reasoning is to determine whether a given goal $\mathcal{G}$ (which can be a fact or a rule) is $\mathsf{True}$, $\mathsf{False}$, or $\mathsf{Uncertain}$.
Specifically, $\mathcal{F}=\{f_1, f_2, \cdots, f_m\}$ represents a set of facts, where each fact $f_i~(1 \leq i \leq m)$ is a declarative statement describing attributes or characteristics of subjects.
For example, \texttt{Sawyer has good dance skills} is a fact that specifies the attribute (\texttt{has good dance skills}) of the subject (\texttt{Sawyer}).
Similarly, $\mathcal{R}=\{r_1, r_2, \cdots, r_n\}$ comprises a set of rules, where each rule $r_j~(1 \leq j \leq n)$ defines relationships between multiple facts using seven FOL symbols: conjunction ($\land$), disjunction ($\lor$), negation ($\neg$), implication ($\rightarrow$), equivalence ($\equiv$) \footnote{In line with previous datasets, we replace equivalence ($\equiv$) with exclusive disjunction ($\oplus$), which can be interpreted as a combination of equivalence ($\equiv$) and negation ($\neg$): $f_1 \oplus f_2 = \neg (f_1 \equiv f_2)$}, universal quantifier ($\forall$), and existential quantifier ($\exists$). 
For example, the rule \texttt{If Sawyer has good dance skills and has charisma, then he is a successful dancer} illustrates a relationship between two facts and their conclusion. 
It is worth noting that some existing datasets, such as ProntoQA~\citep{prontoqa1} and ProofWriter~\citep{tafjord2021proofwriter}, do not fully encompass all seven FOL relationships, whereas our proposed framework ensures complete coverage.


\subsection{LLM \& Prover-Empowered Generation Framework}
\label{subsec:framework}

Our proposed generation framework, which integrates the generative capabilities of LLMs with the precision of a symbolic prover, generates FOL reasoning problems through a structured three-stage process, as illustrated in Figure~\ref{fig:main}.
First, a background story is created for each problem using sampled subject names and keywords to ensure linguistic diversity.
In the next stage, the logic skeleton generation employs a symbolic prover (i.e., Prover9) to construct reasoning trees and determine the truth values of the relevant facts.
Finally, the statement translation phase uses LLMs to convert these logical expressions, including both premises and goals, into natural language, ensuring clarity and coherence.
An example of a generated FOL problem is shown in Figure~\ref{fig:example}.


\subsubsection{Background Story Generation}
\label{subsubsec:background}

Achieving naturalness in language expression is crucial for creating a plausible FOL dataset.
Existing approaches often rely on extensive human annotation or rigidly structured templates.
For instance, ProntoQA~\citep{prontoqa1} and ProofWriter~\citep{tafjord2021proofwriter} employ handcrafted templates, which result in datasets lacking linguistic diversity and naturalness.\footnote{
Both ProofWriter and PrOntoQA generate natural language expressions by using predefined templates for each logical rule. For example, for a rule like $A \rightarrow B$, they may define a template such as \texttt{Every A is B}, and then randomly select words to replace $A$ and $B$. PrOntoQA intentionally avoids real-world concepts to prevent conflicts with real-world knowledge. They also evaluate LLMs' behavior on examples that contain facts and rules that are consistent (or inconsistent) with the real-world knowledge. In contrast, ProofWriter does not incorporate such a mechanism.}

In contrast, FOLIO~\citep{folio} retrieves documents from Wikipedia and manually extracts facts and rules. 
However, this process requires substantial human effort and is susceptible to syntactic typos and reasoning errors introduced during annotation, as observed in FOLIO. 
Additionally, models may exploit \emph{shortcuts} by leveraging pre-trained knowledge to predict answers directly, bypassing rigorous logical reasoning~\citep{qi2023investigation, geirhos2020shortcut}, since the facts and rules in the dataset are often derived from the real-life.

To efficiently synthesize plausible natural FOL problems, we leverage LLMs to generate a unique background story for the subject of each problem, which serves to instantiate the statements (both premises and goals) within the problem.
However, simply prompting LLMs to generate stories without any guidance can be suboptimal, as they struggle to produce diversified content with high quality, as noted by \citet{llm_repeat_answer, sudalairaj2024lablargescalealignmentchatbots}.
To mitigate these issues, we prepare a characteristic keyword for the subject within the problem to guide the generation.
Specifically, the process begins by assigning a unique name to the subject, which is randomly sampled from publicly accessible datasets of human names\footnote{\url{https://github.com/aruljohn/popular-baby-names}} and pet names\footnote{\url{https://github.com/fregante/pet-names/blob/master/pet-names.json}}.
Next, we sample a keyword from WordNet~\citep{wordnet} to serve as a defining characteristic or attribute of the subject, ensuring semantic diversity in the generated content.
These elements, the name and the characteristic keyword, are then integrated into the prompt to guide LLMs to generate a contextually rich and varied background story.
This controlled generation helps promote diversity in the dataset. 
The detailed prompt used in our experiments is outlined in Appendix~\ref{appendix-background}.

\begin{figure}[h]
\begin{center}
\includegraphics[width=\linewidth]{example.pdf}
\end{center}
\caption{A generated FOL problem (a) with its corresponding reasoning process (b).}
\label{fig:example}
\end{figure}


\subsubsection{Logic Skeleton Generation}
\label{subsubsec:premise_generation}
Logical soundness is a fundamental requirement in constructing a robust FOL reasoning dataset.
While LLMs are highly proficient at generating natural language, they often fall short in maintaining logical coherence, particularly when dealing with intricate, multi-step reasoning processes.  
To address this limitation, we introduce a novel approach that integrates symbolic prover into the data generation pipeline, allowing for the construction of logically rigorous FOL reasoning problems. 

In our data generation framework, we start with constructing a logic skeleton for each problem, represented as a reasoning tree with the ultimate goal at its root (as shown in Figure~\ref{fig:main}(b)).
Previous works typically generate reasoning problems in a \emph{bottom-up} approach by starting with premises and building toward conclusions.
We argue that this approach could be flawed as it can often result in disconnected or contradictory premises, making it difficult to ensure that all facts are logically coherent with each other. 
For instance, if the problem includes facts such as $f_1$:~\texttt{Sawyer is a poet}, $f_2$:~\texttt{Sawyer comes from Mars}, and $f_3$:~\texttt{Sawyer is a large, fluffy Maine Coon cat}, then integrating these elements into a consistent reasoning chain is challenging.
Therefore, our framework adopts a novel \emph{top-down} approach, which begins with the goal and works backward to derive the relevant premises and rules needed to logically support that goal.
This method ensures that each fact and rule contributes directly to the reasoning process, improving both the coherence and logical soundness of the generated problems.
Moreover, this top-down structure simplifies the problem of logical consistency by allowing LLMs to focus on facts that are directly relevant to the conclusion, without needing to reconcile them with unrelated information.

More specifically, as shown in Figure~\ref{fig:main}(b), we begin by setting the truth value of the goal, denoted as $f_3 = \mathsf{True}$, and treat this as the root node of the reasoning tree.
Next, we sample a logical expression for this node, $(f_1 \land f_2) \rightarrow f_3$, which establishes a relationship between the goal $f_3$ and the premises $f_1$ and $f_2$.
At this stage, we use the symbolic prover Prover9 to calculate the truth values of $f_1$ and $f_2$, ensuring that the premises logically support the goal.
Once $f_1$ and $f_2$ are assigned truth values, they become the current nodes in the reasoning tree. 
For each of these nodes, we continue the process by sampling new logical expressions.
For instance, for $f_1 = \mathsf{True}$, we sample a new rule $f_5 \rightarrow (f_1 \oplus f_4)$, where $f_5$ and $f_4$ are additional facts to be evaluated. 
We then calculate the truth values for these new premises using Prover9, and they become the next current nodes in the tree.
This iterative process is repeated for each current node, expanding the reasoning tree step by step. 
The process continues until we have generated the desired number of reasoning steps, ensuring that each layer of the reasoning tree is logically consistent and fully developed, forming a coherent and complete logical structure for the problem.

To further assess the robustness of models in handling FOL reasoning, we introduce distractions into the generated problems.
Distractions, denoted as $d$, are an additional set of facts and rules not essential to solving the problem. The inclusion of distractions forces models to distinguish between relevant and distracting information.
We implement two types of distractions in our framework.
The first type follows existing works \citep{tafjord2021proofwriter, prontoqa1}, where we randomly introduce facts or rules that are related to the problem’s domain but use a different subject name (see Figure \ref{fig:example}).
The second, more complex type of distraction leverages the power of symbolic provers to introduce facts and rules that are semantically or logically related to the reasoning chain, but which lead to an $\mathsf{Uncertain}$ judgment.
This type of distraction requires models to reason over logically relevant information that does not directly impact the truth value of the core facts needed to reach the final conclusion. 
For instance, suppose $f_1$ is a critical fact in the reasoning tree, and we introduce two distractions, $d_1 = \mathsf{True}$ and $d_2 = \mathsf{Uncertain}$, with the rule $(d_1 \oplus d_2) \rightarrow f_1$. 
Here, the truth value of $f_1$ remains $\mathsf{Uncertain}$ because the rule involves incomplete information. 
Models must reason through these distractions, yet ultimately recognize that they do not alter the critical facts that drive the reasoning chain forward.


\subsubsection{Statement Translation}
\label{subsubsec:translation}
After generating the logical skeleton of a problem, we use LLMs to translate each rule and fact into natural language within the framework of the background story generated in Section \ref{subsubsec:background}.
As illustrated in Figure~\ref{fig:main}(c), we first prompt LLMs to instantiate placeholders (such as $f_6$ and $f_4$) in the logic expressions with appropriate predicates, like \texttt{poet} and \texttt{musician}. 
Once the placeholders are instantiated, the logic expressions are translated into natural language. 
To maintain consistency throughout the translation process, we provide the previously translated facts and rules as references in the prompt for subsequent translations (details of the translation prompt can be found in Appendix~\ref{appendix-translation}).

To ensure the plausibility and soundness of the rules, we instruct LLMs to generate two versions of each rule: a universal and a specific version. 
The universal rule refers to statements that apply to everyone or everything, such as \texttt{Everyone is either a musician or a poet, but not both}.
The specific rule, on the other hand, applies to an individual subject, such as \texttt{Sawyer is either a musician or a poet, but not both}.
LLMs then assess whether the universal rule aligns with common sense.
If it does, we use the universal version; if not, we opt for the specific rule. 
For instance, the universal rule in Figure~\ref{fig:main}(c) does not align with common sense (as there are more professions than just musicians and poets), so we choose the specific version of the rule.
The final stage of the proposed framework involves assembling the facts, rules, and distractions generated in previous stages into a complete FOL problem. 
An example of the constructed problem with its corresponding reasoning steps is provided in Figure \ref{fig:example}.


\subsection{The ProverQA Benchmark}
\label{subsec:provergen_benchmark}

We utilize Llama3.1-70B-Instruct throughout our proposed generation framework to develop a new FOL reasoning benchmark, \textbf{ProverQA}, comprising 1,500 instances. 
The benchmark is evenly divided into three subsets based on the length of the reasoning chains: \textit{easy} (1-2 steps), \textit{medium} (3-5 steps), and \textit{hard} (6-9 steps). 
In addition to the number of reasoning steps, the \textit{hard} subset is characterized by more diverse reasoning directions. 
Reasoning in the \textit{easy} and \textit{medium} subsets typically follows a straightforward left-to-right progression. 
For example, given $f_1 = \mathsf{True}$ and $f_1 \rightarrow f_2$, the model can deduce that $f_2 = \mathsf{True}$. 
In contrast, the \textit{hard} subset involves more complex patterns, requiring models to infer $f_1 = \mathsf{False}$ given $f_2 = \mathsf{False}$ and $f_1 \rightarrow f_2$, thus demonstrating a more intricate reasoning process.
Each problem in the benchmark contains two types of distractions (see Section \ref{subsubsec:premise_generation}), where the number of distractions for each type is randomly sampled. 
Moreover, to further challenge the robustness of models, we shuffle the order of the premises within each problem, ensuring that models cannot exploit fixed patterns or sequences to derive conclusions.
Detailed examples of the ProverQA are provided in Appendix~\ref{appendix-example}.

\section{Evaluation on the ProverQA}
\label{sec:evaluation}

\subsection{Experimental Setup}
\label{subsec:exp-setpup}
We evaluate current SOTA proprietary LLMs (\texttt{GPT-4o}~\citep{gpt-4o} and \texttt{Claude3.5}~\citep{claude-3-5})\footnote{The version of \texttt{GPT-4o} we used in our experiments is \texttt{gpt-4o-2024-08-06}, and the version of \texttt{Claude3.5} we used is \texttt{claude-3-5-sonnet-20240620}.} and open-source LLMs (\texttt{Mistral-7B-Instruct}, \texttt{Mistral-Large-Instruct}, \texttt{Mixtral-8x22B-Instruct-v0.1}~\citep{mistral, mixtral} and \texttt{Llama3.1-8B-Chat, Llama3.1-70B-Chat}~\citep{llama3-1}) on the ProverQA. 
We utilize two prompting strategies: \textit{Standard} and \textit{CoT}. 
\textit{Standard} prompting uses 2-shot in-context learning to prompt LLMs to answer questions directly, whereas \textit{CoT} prompting employs 2-shot examples to instruct the model to solve questions step by step. Examples of both prompting strategies can be found in Appendix~\ref{appendix-prompt-examples}.

\subsection{Main Results}
\label{subsec:evaluation-performance}

\begin{table}[h]
\caption{The performance of LLMs on ProverQA and existing FOL reasoning datasets. $\dagger$: The average performance improvement of \textit{CoT} prompting over \textit{Standard} prompting across models on each evaluation set,  which indicates the average gain \textit{CoT} prompting brings on each set.}
\label{tab:eval_main}
\setlength\tabcolsep{5.2pt}
\begin{center}
\begin{tabular}{lccccccc}
\toprule
  & \multicolumn{3}{c}{\bf ProverQA} & \bf\small \multirow{2}{*}{ProntoQA} & \bf\small \multirow{2}{*}{ProofWriter} & \bf\small \multirow{2}{*}{FOLIO} & \bf\small \multirow{2}{*}{Avg}\\
  \cmidrule(r){2-4}
  & \bf\small Easy & \bf\small Medium & \bf\small Hard & & & \\
\midrule
\multicolumn{8}{c}{\textit{Standard Prompting}} \\
\midrule
GPT-4o & 87.20 & 68.60 & 46.20 & 91.80 & 56.33 & 67.86 & 69.66\\
Claude-3.5-Sonnet & 85.00 & 68.20 & 42.80 & 88.60 & 55.00 & 77.85 & 69.58\\
Llama3.1-8B-Instruct & 46.60 & 43.00 & 39.00 & 50.40 & 43.80 & 54.29 & 46.18\\
Llama3.1-70B-Instruct & 82.00 & 64.20 & 47.60 & 80.60 & 50.33 & 67.86 & 65.28 \\
Mistral-7B-Instruct & 56.80 & 46.80 & 37.20 & 50.00 & 42.33 & 54.29 & 47.90\\
Mistral-Large-Instruct & 84.60 & 69.20 & 49.60 & 71.00 & 60.33 & 77.14 & 68.65\\
Mixtral-8x22B-Instruct & 75.40 & 57.40 & 39.00 & 65.20 & 40.17 & 74.29 & 58.58\\
\midrule
\multicolumn{8}{c}{\textit{CoT Prompting}} \\
\midrule
GPT-4o & 94.20 & 79.40 & 50.00 & 100.00 & 67.33 & 72.14 & 77.18 \\
Claude-3.5-Sonnet & 95.20 & 83.60 & 56.40 & 99.20 & 76.33 & 80.71 & 81.91 \\
Llama3.1-8B-Instruct & 75.60 & 46.60 & 33.60 & 79.60 & 56.83 & 63.57 & 59.30 \\
Llama3.1-70B-Instruct & 90.40 & 73.20 & 46.80 & 95.40 & 71.17 & 74.29 & 75.21\\
Mistral-7B-Instruct & 72.00 & 51.00 & 41.80 & 61.20 & 46.00 & 63.58 & 55.93 \\
Mistral-Large-Instruct & 92.60 & 75.80 & 52.20 & 98.60 & 73.50 & 83.57 & 79.38 \\
Mixtral-8x22B-Instruct & 87.60 & 66.80 & 47.60 & 79.60 & 57.67 & 73.57 & 68.81 \\
\midrule
Avg  $\Delta$$^\dagger$ & 12.86 & 8.43 & 3.86 & 16.57 & 14.36 & 5.41 & 10.25\\
\bottomrule
\end{tabular}
\end{center}
\end{table}


\paragraph{ProverQA as a New Challenge}
Overall, model performance on ProverQA declines as problem difficulty increases, with the lowest scores observed on the hard subset \textit{across all existing datasets} as seen in Table \ref{tab:eval_main}. 
Specifically, strong models such as GPT-4o, Claude-3.5-Sonnet and Mistral-Large-Instruct barely exceed 50\% accuracy on the hard subset, even with CoT prompting. 
This result indicates that even the most advanced LLMs perform poorly on sufficiently complex problems with long reasoning chains.
In contrast, simpler benchmarks like ProntoQA appear to be nearly solved by models like GPT-4o and Claude-3.5-Sonnet.
However, smaller models still struggle to achieve satisfactory performance, even on the easy subset of ProverQA, suggesting that these models are not yet equipped to handle FOL reasoning effectively.

\paragraph{Comparison of prompting strategies}
Compared with \textit{Standard} prompting, \textit{CoT} prompting notably enhances the performance of models on easy datasets but does not yield a
similar improvement on harder datasets, as presented in the last row of Table \ref{tab:eval_main}. 
The improvement on the easy dataset is more pronounced because performance on easy problems is probably limited by the CoT capability, whereas the bottleneck for hard problems may involve other factors beyond CoT capability. 
We have also conducted experiments using 5-shot examples; the results can be found in Appendix~\ref{appendix-5-shot}.

\subsection{Ablation study on Distractions and Premise Order}
\label{subsec: distracting_premises}

By default, the ProverQA benchmark includes two types of distractions and shuffles the order of premises within each problem (see Section \ref{subsec:provergen_benchmark}). 
To assess how these two factors influence the evaluation results of LLMs, we conduct ablation studies using GPT-4o with \textit{CoT} prompting in this section. 
Specifically, in the first setting, we remove all distractions from the problems to observe how the absence of irrelevant information affects the model’s reasoning performance. Results in Table~\ref{tab:eval_noise_order} exhibit that the model performs better in this situation. 
In the second setting, we \emph{additionally} reorder the premises to match the logical progression of the underlying reasoning chain. This turns out that the model performs even better when premises are presented in a coherent sequence. Consequently, these findings suggest both factors are valuable for constructing a robust FOL reasoning benchmark for LLMs.

\begin{table}[h]
\caption{Performance of GPT-4o on different variants of ProverQA using CoT prompting. The first row represents the default ProverQA setting, which features distractions and shuffled premises.}
\label{tab:eval_noise_order}
\setlength\tabcolsep{8.1pt}
\begin{center}
\begin{tabular}{cc|ccc}
\toprule
\bf\small Distractions & \bf\small Shuffled Premises  & \bf\small Easy  & \bf\small Medium & \bf\small Hard\\
\midrule
\ding{51} & \ding{51} & 94.20 & 79.40 & 50.00 \\
\ding{55} & \ding{51} & 94.40 & 82.20 & 57.40 \\
\ding{55} & \ding{55} & 95.60 & 87.40 & 62.40 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\section{Finetuning with ProverQA Train Data}
\label{sec:train}

Beyond its role as an evaluation benchmark, we also investigate whether the training data synthesized by our proposed framework can improve a model's FOL reasoning capabilities, particularly in out-of-distribution (OOD) scenarios.
To this end, we generate a dedicated training set and finetune an open-source model, Llama3.1-8B-Instruct. 
For comparison, we also finetune the same model on existing FOL reasoning datasets, such as ProofWriter and FOLIO. 
We exclude ProntoQA from this experiment due to the absence of a publicly available training set. 
Detailed training configurations are provided in Appendix~\ref{appendix-train-details}.  

\subsection{Evaluation Results}
\label{subsec:train-main}

\begin{table}[h]
\caption{Performance of Llama3.1-8B-Instruct finetuned with different FOL training sets. 
\ind{Green cells} indicate in-distribution (ID) evaluation results, while \ood{red cells} represent out-of-distribution (OOD) evaluation results.
The change of performance after finetuning is shown in small fonts along with the evaluation results.
$\dagger$: the average \emph{change} of performance across all OOD evaluation sets for each training configuration.
}
\label{tab:train-main}
\setlength\tabcolsep{1.6pt}
\begin{center}
\begin{tabular}{lllllllc}
\toprule
\bf\small \multirow{2}{*}{\makecell[c]{FOL \\ Training Set}} & \multicolumn{3}{c}{\bf ProverQA\, \,} & \bf\small \multirow{2}{*}{ProntoQA} & \bf\small \multirow{2}{*}{ProofWriter\, \,} & \bf\small \multirow{2}{*}{FOLIO} &  \bf\small \multirow{2}{*}{\makecell[c]{OOD \\ Avg $\Delta$$^\dagger$}}\\
\cmidrule(r){2-4}
 & \bf\small \, Easy & \bf\small \, Medium  & \bf\small \, Hard & & & & \\
\midrule
\multicolumn{8}{c}{\small\emph{Llama3.1-8B-Instruct}} \\
\midrule
\makecell[c]{-} & \, 75.60 & \, 46.60 & \, 33.60 & \, 79.60 & \, 56.83 & \, 63.57 & - \\
\midrule
\multicolumn{8}{c}{\small\emph{Finetuned Llama3.1-8B-Instruct}} \\
\midrule
    ProofWriter & \ood{44.60}\tiny{$^{-31.0}$} & \ood{55.00}\tiny{$^{+8.4}$} & \ood{47.20}\tiny{$^{+13.6}$} & \bf{\ood{92.00}}\tiny{$^{+12.4}$} & \bf{\ind{71.67}}\tiny{$^{+14.8}$} & \ood{50.00}\tiny{$^{-13.6}$} & \ood{-2.0} \\
    FOLIO & \ood{53.20}\tiny{$^{-22.4}$} & \ood{44.80}\tiny{$^{-1.8}$} & \ood{31.00}\tiny{$^{-2.6}$} & \ood{63.40}\tiny{$^{-16.2}$} & \ood{42.83}\tiny{$^{-14.0}$} & \bf{\ind{70.00}}\tiny{$^{+6.4}$} & \ood{-11.4} \\
\midrule
    ProverQA & \bf{\ind{97.00}}\tiny{$^{+21.4}$} & \bf{\ind{90.60}}\tiny{$^{+44.0}$} & \bf{\ind{68.20}}\tiny{$^{+34.6}$} & \ood{88.40}\tiny{$^{+8.8}$} & \ood{65.67}\tiny{$^{+8.8}$} & \ood{68.57}\tiny{$^{+5.0}$} & \bf  \ood{+7.5}\\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\paragraph{In-Distribution (ID) Performance}
As shown in Table \ref{tab:train-main}, finetuning on the ProverQA training set results in a substantial performance improvement across all subsets of the ProverQA benchmark.
Specifically, the model's performance increases by 21.4\%, 44.0\%, and 34.6\% in the easy, medium, and hard subsets of the ProverQA benchmark, respectively. 
Similarly, fine-tuning on ProofWriter and FOLIO leads to performance gains on their corresponding test sets, as expected.

\paragraph{Out-of-Distribution (OOD) Performance}
While the ID performance of finetuned models can exhibit the positive effects of specialized finetuning, OOD performance provides more insights as it demonstrates the generalizability of the finetuned models. 
As shown in Table \ref{tab:train-main}, finetuning on ProverQA yields a performance increase on OOD test sets (ProntoQA, ProofWriter, FOLIO) by about 5\% to 8.8\%. 
Finetuning on ProverQA obtains the best OOD improvement on average with a large leading margin compared to models finetuned on ProofWriter and FOLIO, indicating the limited utility of these prior datasets for promoting the reasoning capabilities of LLMs. 
The model finetuned on ProofWriter achieves the highest performance on ProntoQA. 
This anomaly could be attributed to the high similarity between ProntoQA and ProofWriter: Both datasets utilize predefined templates for generating logic and corresponding natural language.


\subsection{case study}
\label{subsec: case_study}

We show examples from the FOLIO test set in Table \ref{tab:case-study} to demonstrate how finetuning on ProverQA improves model performance. 
Before finetuning, models fail to solve the problems because of various reasons, such as using a common sense shortcut, being unfaithful to the context, and being unable to extend the reasoning chain. 
After training on ProverQA, models are able to derive a valid reasoning chain up to the goal based on the context, and the reasoning process rigidly adheres to the facts and rules.

\begin{table}[h]
\caption{Typical errors made by models and how finetuning correct them. Reasoning chains before and after finetuning are both displayed for comparison.}
\label{tab:case-study}
 \setlength\tabcolsep{2.5pt}
\begin{center}
\begin{tabular}{m{2.1cm}m{3.5cm}m{4.3cm}m{3.2cm}}
\toprule
\multicolumn{1}{l}{\bf\small Error Type} & \bf\small \makecell[c]{Context}  & \bf\small \makecell[c]{Before Finetuning}  & \bf\small \makecell[c]{After Finetuning}\\ \midrule
\small \makecell[c]{ Common sense\\ shortcut} & \tiny All students are humans. If someone studies, then he/she is a student. A person either studies or teaches. No young person teaches. Jerry neither teaches nor is a manager. Rose is young or a student. \textcolor{pale_green}{Question: Jerry is not young.}\newline \newline \newline \newline \newline & \tiny A person either studies or teaches.No young person teaches. Therefore, a young person studies. If someone studies, then they are a student. All students are humans. Therefore, a young person is a human. People are either young or old. Therefore, a young person is not old. Jerry neither teaches nor is a manager. Therefore, Jerry studies. \textcolor{pale_red}{Since Jerry is a student and a human, Jerry is not old.} Therefore, Jerry is young. \newline \textcolor{codepurple}{Answer: false.}\newline & \tiny Jerry neither teaches nor is a manager. A person either studies or teaches. Jerry studies. Jerry studies. If someone studies, then he/she is a student. Jerry is a student. Jerry is a student. All students are humans. Jerry is a human. According to the context and the conclusions that have already been drawn, we can deduce that it is uncertain that Jerry is not young.\newline  \textcolor{codepurple}{Answer: uncertain.}\\
\midrule
\small \makecell[c]{Unfaithful to\\ the context} & \tiny Beijing is the capital of the People's Republic of China. Beijing is the world's most populous national capital city. Beijing is located in Northern China. Beijing hosted the 2008 Summer Olympics and 2008 Summer Paralympics Games. Beijing has hosted both the Summer and Winter Olympics, along with the Summer and Winter Paralympics.\newline \textcolor{pale_green}{Question: Beijing has hosted both the 2008 Summer Olympics and a winter olympics.} & \tiny Beijing hosted the 2008 Summer Olympics and 2008 Summer Paralympics Games. However, there is no information about Beijing hosting a Winter Olympics. Therefore, the statement is uncertain \textcolor{pale_red}{because it is not explicitly stated that Beijing has hosted a Winter Olympics}, but it is also not explicitly stated that it has not.\newline \textcolor{codepurple}{Answer: uncertain.}\newline \newline\newline\newline & \tiny Beijing hosted the 2008 Summer Olympics and 2008 Summer Paralympics Games. Beijing has hosted both the Summer and Winter Olympics, along with the Summer and Winter Paralympics. Beijing has hosted both the 2008 Summer Olympics and a winter olympics. Therefore, it is true that Beijing has hosted both the 2008 Summer Olympics and a winter olympics.\newline \textcolor{codepurple}{Answer: true.}\\
\midrule
\small \makecell[c]{Unable to\\ extend the\\ reasoning chain} & \tiny Robert Lewandowski is a striker. Strikers are soccer players. Robert Lewandowski left Bayern Munchen. If a player left a team they no longer play for that team. \textcolor{pale_green}{Question: Robert Lewandowski plays for Bayern Munchen.}\newline \newline \newline \newline & \tiny Robert Lewandowski is a striker. Strikers are soccer players. Therefore, Robert Lewandowski is a soccer player. Robert Lewandowski left Bayern Munchen. If a player left a team, they no longer play for that team. \textcolor{pale_red}{Therefore, Robert Lewandowski no longer plays for Bayern Munchen. However, the question is asking if he plays for Bayern Munchen, not if he left.} The information does not state that he currently plays for another team. \newline \textcolor{codepurple}{Answer: uncertain.} & \tiny Robert Lewandowski left Bayern Munchen. If a player left a team they no longer play for that team. Robert Lewandowski does not play for Bayern Munchen. Therefore, it is false that Robert Lewandowski plays for Bayern Munchen. \newline \textcolor{codepurple}{Answer: false.}\newline \newline\\
\bottomrule
\end{tabular}
\end{center}
\end{table}


\section{Conclusions}
In this work, we presented a novel framework called ProverGen for automatically generating FOL reasoning datasets by leveraging the strengths of LLMs in producing diverse natural language expressions and the rigor of symbolic provers in ensuring logical consistency. 
Using this framework, we developed ProverQA, a high-quality FOL reasoning benchmark that is both extensible and diverse. 
ProverQA encompasses complex rules, diverse natural language expressions, and faithful intermediate reasoning chains, providing a comprehensive evaluation benchmark for FOL reasoning.
Our evaluation shows that SOTA LLMs struggle to solve ProverQA problems, even with CoT prompting, demonstrating the unique challenge posed by our dataset.
We also finetuned Llama3.1-8B-Instruct on a separately generated training set and observed consistent performance improvements on both in-distribution and out-of-distribution test sets.
We hope that our generation framework and the resulting dataset could contribute to the development of more robust and capable models in this area.


\section{Ethics Statement}
The keywords and names used in this paper are sourced entirely from public repositories and databases. 
Specifically, the names were collected from two public GitHub repositories under MIT licenses, and the keywords were obtained from WordNet. 
All sources have licenses permitting free usage, and our work complies with these terms. Our dataset does not involve human participants, nor does it present any immediately harmful insights.
For the release, we plan to open source the code for creating the benchmark and the dataset we created for the experiments in this paper.


\section{Reproducibility Statement}
For our submission, we have uploaded the entirety of the source code as a zipped file that has been
properly anonymized. 
The source code contains inline documentation that details purpose and usage of different parts of the codebase. 
In addition, we also include the full set of model's responses and the evaluation script.
As discussed in the ethics statement, we plan to more formally release ProverQA to the public as an open source repository with thorough details that describes the framewrok, outlines the code, and details its usage.


\subsubsection*{Acknowledgments}
This work is supported by National Key Research and Development Program of China (Grant No. 2024YFB3311902), National Natural Science Foundation of China (Grant No. 62173017), Beijing Natural Science Foundation (Grant No. L241018), Beijing Nova Program (Grant No. 2024085), and Shanghai Artificial Intelligence Laboratory.



\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\appendix
\section{Prompt for Generating Background Story}
\label{appendix-background}

\lstset{
    style=mystyle,
    basicstyle=\ttfamily\scriptsize,
    backgroundcolor=\color{white},
    stringstyle=\color{black},
    keywordstyle=\color{black},
    breaklines=false,
    keepspaces=false
}
\begin{lstlisting}[language=Python]
(*@\color{codepurple}{\textbf{System}}@*):
You will be given a keyword and a name (can be a person's name or an animals' name). Your task 
is to generate a background story with no more than 150 words according to the keyword about 
this name. Your answer should be in JSON format with keys: category, story.

(*@\color{codepurple}{\textbf{User}}@*): 
keyword: swift
name: Jack

(*@\color{codepurple}{\textbf{Assistant}}@*):
{
  "category": "cat",
  "story": "Jack was a sleek, agile cat known for his swift movements. Born in the bustling 
            city, he quickly learned to navigate the narrow alleyways and rooftops with ease. 
            His speed and agility made him a legend among the local feline community. Jack's 
            favorite pastime was chasing the elusive city birds, darting through the air with 
            unmatched precision. Despite his wild spirit, he had a soft spot for the elderly 
            woman who fed him daily. She named him Jack after the nimble character from her 
            favorite childhood story, believing that his swiftness was a sign of good fortune. 
            Jack's swift nature wasn't just physical; he had a quick wit and an uncanny ability 
            to sense danger, keeping him safe in the urban jungle. His legend grew, and he 
            became a symbol of agility and cleverness in the neighborhood."
}

(*@\color{codepurple}{\textbf{User}}@*): 
keyword: success
name: Amy

(*@\color{codepurple}{\textbf{Assistant}}@*):
{
  "category": "human",
  "story": "Amy grew up in a small town where opportunities were scarce. Despite the odds, she 
            was determined to make something of herself. After excelling in high school, she 
            earned a scholarship to a prestigious university. There, she majored in computer 
            science and quickly became known for her innovative ideas. Upon graduation, Amy co-
            founded a tech startup that focused on sustainable technology. Her company quickly 
            gained traction, securing major investments and partnerships. Amy's relentless 
            work ethic and visionary leadership turned her startup into a multi-million dollar 
            enterprise within five years. Today, she is celebrated not only for her business 
            acumen but also for her commitment to mentoring young entrepreneurs and giving back 
            to her community. Amy's story is a testament to the power of perseverance, vision, 
            and hard work."
}

(*@\color{codepurple}{\textbf{User}}@*): 
keyword: loyal
name: Bella

(*@\color{codepurple}{\textbf{Assistant}}@*):
{
  "category": "dog",
  "story": "Bella was a golden retriever with a heart as pure as gold. From the moment she was 
            adopted by the Thompson family, she became their steadfast guardian and companion. 
            Bella had an uncanny ability to sense when someone in the family needed comfort, 
            always there with a nuzzle or a wagging tail. Her loyalty was most evident when 
            young Timmy got lost in the woods behind their house. Bella tracked his scent 
            tirelessly, refusing to rest until she found him. She stayed by his side through 
            the cold night, keeping him warm and safe until help arrived. Bella's unwavering 
            loyalty and love made her not just a pet, but a cherished member of the 
            family."
}

(*@\color{codepurple}{\textbf{User}}@*): 
keyword: [KEYWORD]
name: [NAME]


\end{lstlisting}

\section{Prompts for translating logic expressions into natural language}
\label{appendix-translation}
We utilized three examples for each type of logical expression to guide the LLMs in translating them into natural language. 
An example of the prompt we used in our experiments is shown below. 

\lstset{
    style=mystyle,
    basicstyle=\ttfamily\scriptsize,
    backgroundcolor=\color{white},
    stringstyle=\color{black},
    keywordstyle=\color{black},
    breaklines=false,
    keepspaces=false
}
\begin{lstlisting}[language=Python]
(*@\color{codepurple}{\textbf{System}}@*):
You will be provided a logic expression, a reference of the existed predicate in the logic 
expression, and a background story. Your task is to replace the placeholders in the logic 
expression with appropriate predicates (no more than 5 words) so that the provided logic 
expression represent a real world common sense rule.

Do not use 'not'. Do not use the words in the forbidden list. The words that are similar 
in meaning to the words in the forbidden list are also not allowed.

Your answer is not required to closely connected to the background story. You can use any 
predicates you like as long as their length is less than 5.

Your answer should be in JSON format with the provided keys.

(*@\color{codepurple}{\textbf{User}}@*): 
(*@{\textbf{background story:}}@*)
Moriarty was a large, fluffy Maine Coon cat with a surprisingly warm and gentle nature. 
Despite his intimidating size and the mysterious name inspired by a famous literary villain, 
Moriarty was the epitome of warmth and comfort. He lived in a cozy bookstore run by Mrs. 
Lindon, an elderly woman who had taken him in as a stray. Moriarty had a special talent for 
sensing when a customer needed a bit of extra warmth, often curling up in their laps as they 
read. His thick fur and gentle purring provided solace to many, especially during the cold 
winter months. Moriarty's presence turned the bookstore into a beloved local haven, where 
people came not just for the books, but for the warm, comforting presence of their favorite 
feline friend.

(*@{\textbf{reference}}@*): is_fluffy
is_fluffy(Moriarty) (*@$\rightarrow$@*) (warm_and_gentle(Moriarty) (*@$\oplus$@*) is_calming(Moriarty)): If Moriarty is 
fluffy, then he is either warm and gentle or calming, but not both.
(*@{\textbf{forbidden list:}}@*) ['warm_and_gentle', 'provide_comfort', 'attract_customers', 'is_friendly', 
'brings_joy', 'is_fluffy', 'is_calming']

(*@{\textbf{logic expression:}}@*) (*@$\forall$@*)x (F7(x) (*@$\rightarrow$@*) (is_fluffy(x) (*@$\land$@*) F8(x)))
(*@{\textbf{keys:}}@*) ['F7', 'F8', 'universal_rule', 'specific_rule']
(*@{\textbf{Note:}}@*) x belongs to cat.


(*@\color{codepurple}{\textbf{Assistant}}@*):
{
  "F7": "playful",
  "F8": "love_attention",
  "universal_rule": "All playful cats are fluffy and love attention.",
  "specific_rule": "If Moriarty is playful, then he is both fluffy and loves attention."
}

(*@\color{codepurple}{\textbf{----------------------}}@*)
(*@\color{codepurple}{\textbf{Another Two Examples}}@*)
(*@\color{codepurple}{\textbf{----------------------}}@*)

(*@\color{codepurple}{\textbf{User}}@*): 
(*@{\textbf{background story:}}@*)
Colt was a brilliant botanist with a passion for rare plant species. He spent years studying 
the Avicenniaceae family, specifically the mangrove plants, which thrive in the harshest of 
coastal environments. Colt's groundbreaking research on these unique plants led to the 
development of innovative solutions for coastal conservation. He discovered that the Avicennia 
species possessed remarkable adaptability, allowing them to survive in areas with high salinity 
and limited oxygen. Colt's findings opened doors to new methods for restoring damaged 
ecosystems and protecting endangered species. As a leading expert in his field, Colt 
collaborated with international organizations to implement his research, helping to preserve 
the delicate balance of our planet's mangrove ecosystems. His dedication to the Avicenniaceae 
family left a lasting impact on the scientific community.

(*@{\textbf{reference}}@*): contribute_to_environmental_conservation
dedicated_to_research(Colt) (*@$\rightarrow$@*) (make_groundbreaking_discoveries(Colt) (*@$\land$@*) 
contribute_to_environmental_conservation(Colt)): If Colt is dedicated to research, then he 
will make groundbreaking discoveries and contribute to environmental conservation.
(*@{\textbf{forbidden list:}}@*) ['dedicated_to_research', 'make_groundbreaking_discoveries', 
'contribute_to_environmental_conservation', 'pursue_higher_education', 'become_leading_expert']

(*@{\textbf{logic expression:}}@*) (*@$\forall$@*)x (F5(x) (*@$\rightarrow$@*) (contribute_to_environmental_conservation(x) (*@$\land$@*) F6(x)))
(*@{\textbf{keys:}}@*) ['F5', 'F6', 'universal_rule', 'specific_rule']
(*@{\textbf{Note:}}@*) x belongs to human.




\end{lstlisting}

\section{Potential Shortcut Problem}
\label{appendix-shortcut}
To further investigate the potential use of shortcuts, we conducted an experiment by removing universal rules from 60 randomly selected instances in ProverQA. We evaluated GPT-4 and Llama-3.1-70B-Instruct on this \emph{corrupted} dataset.
If the models were relying heavily on inherent knowledge as shortcuts, their performance would remain roughly unaffected despite the absence of universal rules. However, the results in Table~\ref{tab:shortcut} showed a significant drop in performance, indicating that these models do not heavily rely on background knowledge to solve the problems.

\begin{table}[h]
\caption{Comparative performance of GPT-4o and Llama-3.1-70B-Instruct on the original dataset and the corresponding corrupted version.}
\label{tab:shortcut}
\setlength\tabcolsep{12pt}
\begin{center}
\begin{tabular}{lcccc}
\toprule
\bf\small Model & \bf\small Prompting Strategy & \bf\small Original  & \bf\small Corrupted & \bf\small Acc $\Delta$\\
\midrule
GPT-4o & Direct & 58.33 & 43.33 & -15.00\\
GPT-4o & CoT & 68.33 & 45.00 & -23.33\\
Llama-3.1-70B-Instruct & Direct & 65.00 & 48.33 & -16.67\\
Llama-3.1-70B-Instruct & CoT & 65.00 & 53.33 & -11.67\\
\bottomrule
\end{tabular}
\end{center}
\end{table}



\section{Examples from the three subsets of ProverQA}
\label{appendix-example}

\textbf{ProverQA-Easy}
\lstset{
    style=mystyle,
    basicstyle=\ttfamily\scriptsize,
    backgroundcolor=\color{white},
    stringstyle=\color{black},
    keywordstyle=\color{black},
    breaklines=false,
    keepspaces=false
}
\begin{lstlisting}[language=Python]
(*@\color{codepurple}{\textbf{Context}}@*):
For every elephant, the elephant is either domesticated or wild, but not both. 
Anakin is a domesticated elephant.

(*@\color{codepurple}{\textbf{Question}}@*): Based on the above information, is the following statement true, false, or uncertain? 
Anakin is not a wild elephant.

(*@\color{codepurple}{\textbf{Options}}@*):
A) True
B) False
C) Uncertain

The correct option is: A

(*@\color{codepurple}{\textbf{--------------------------------------------Proofs--------------------------------------------}}@*)

(*@\color{codegray}{\textbf{------------------Step1------------------}}@*)

fact1: Anakin is a domesticated elephant.
rule: For every elephant, the elephant is either domesticated or wild, but not both.
conclusion: Anakin is not a wild elephant.

Therefore, it is true that Anakin is not a wild elephant. The correct option is: A.
\end{lstlisting}

\textbf{ProverQA-Medium}
\lstset{
    style=mystyle,
    basicstyle=\ttfamily\scriptsize,
    backgroundcolor=\color{white},
    stringstyle=\color{black},
    keywordstyle=\color{black},
    breaklines=false,
    keepspaces=false
}
\begin{lstlisting}[language=Python]
(*@\color{codepurple}{\textbf{Context}}@*):
Everyone who either practices waltz or masters tango (but not both) has good dance skills. 
Sawyer has charisma. Sawyer is a performer. If Sawyer has good dance skills and has charisma, 
then he is a successful dancer. Sawyer is either a musician or a poet, but not both. Alice is 
a poet. Sawyer is a poet. If Sawyer is a performer, then he is either skilled at dancing or a 
musician, but not both.

(*@\color{codepurple}{\textbf{Question}}@*): Based on the above information, is the following statement true, false, or uncertain? 
Sawyer is a successful dancer.

(*@\color{codepurple}{\textbf{Options}}@*):
A) True
B) False
C) Uncertain

The correct option is: A

(*@\color{codepurple}{\textbf{--------------------------------------------Proofs--------------------------------------------}}@*)

(*@\color{codegray}{\textbf{------------------Step1------------------}}@*)

fact1: Sawyer is a poet.
rule: Sawyer is either a musician or a poet, but not both.
conclusion: Sawyer is not a musician.

(*@\color{codegray}{\textbf{------------------Step2------------------}}@*)

fact1: Sawyer is a performer.
fact2: Sawyer is not a musician.
rule: If Sawyer is a performer, then he is either skilled at dancing or a musician, but 
not both.
conclusion: Sawyer has good dance skills.

(*@\color{codegray}{\textbf{------------------Step3------------------}}@*)

fact1: Sawyer has good dance skills.
fact2: Sawyer has charisma.
rule: If Sawyer has good dance skills and has charisma, then he is a successful dancer.
conclusion: Sawyer is a successful dancer.

Therefore, it is true that Sawyer is a successful dancer. The correct option is: A.

\end{lstlisting}

\textbf{ProverQA-Hard}
\lstset{
    style=mystyle,
    basicstyle=\ttfamily\scriptsize,
    backgroundcolor=\color{white},
    stringstyle=\color{black},
    keywordstyle=\color{black},
    breaklines=false,
    keepspaces=false
}
\begin{lstlisting}[language=Python]
(*@\color{codepurple}{\textbf{Context}}@*):
Buster does not respond to calls. All dogs that listen to commands and behave properly are 
well-trained. Buster is either obedient or playful, but not necessarily both. Buster follows 
routines. Buster is either playful or loving (or both). If Buster either responds to calls or 
follows routines (but not both), then he listens to commands. All well-trained dogs are 
obedient and have good manners. Buster behaves properly.

(*@\color{codepurple}{\textbf{Question}}@*): Based on the above information, is the following statement true, false, or uncertain? 
Buster is loving.

(*@\color{codepurple}{\textbf{Options}}@*):
A) True
B) False
C) Uncertain

The correct option is: A

(*@\color{codepurple}{\textbf{--------------------------------------------Proofs--------------------------------------------}}@*)

(*@\color{codegray}{\textbf{------------------Step1------------------}}@*)

fact1: Buster does not respond to calls.
fact2: Buster follows routines.
rule: If Buster either responds to calls or follows routines (but not both), then he listens 
to commands.
conclusion: Buster listens to commands.

(*@\color{codegray}{\textbf{------------------Step2------------------}}@*)

fact1: Buster listens to commands
fact2: Buster behaves properly.
rule: All dogs that listen to commands and behave properly are well-trained.
conclusion: Buster is well-trained.

(*@\color{codegray}{\textbf{------------------Step3------------------}}@*)

fact1: Buster is well-trained.
rule: All well-trained dogs are obedient and have good manners.
conclusion: Buster is obedient.

(*@\color{codegray}{\textbf{------------------Step4------------------}}@*)

fact1: Buster is obedient.
rule: Buster is either obedient or playful, but not necessarily both.
conclusion: Buster is not playful.

(*@\color{codegray}{\textbf{------------------Step5------------------}}@*)

fact1: Buster is not playful.
rule: Buster is either playful or loving (or both).
conclusion: Buster is loving.

Therefore, it is true that Buster is loving. The correct option is: A.
\end{lstlisting}

\section{Examples of the two prompting methods}
\label{appendix-prompt-examples}

The prompts of the two prompting strategies we used in our experiments are listed below. We adopt 2-shot in-context learning for both \texttt{Standard} and \texttt{CoT} prompting strategies.

\textbf{Standard Prompting}
\lstset{
    style=mystyle,
    basicstyle=\ttfamily\scriptsize,
    backgroundcolor=\color{white},
    stringstyle=\color{black},
    keywordstyle=\color{black},
    breaklines=false,
    keepspaces=false
}
\begin{lstlisting}[language=Python]
(*@\color{codepurple}{\textbf{System:}}@*)
Given a problem statement as contexts, the task is to answer a logical reasoning question. 
Your answer should be in JSON format with key: answer.

(*@\color{codepurple}{\textbf{Context:}}@*)
Anyone who can solve problems and communicate effectively has practical skills. If Paola 
appreciates beauty, then she values precision and enjoys manual work. Paola appreciates beauty. 
Paola is either skilled in mathematics or has practical skills, but not both. If someone enjoys 
manual work and values attention to detail, then they can craft with their hands. Paola is 
either skilled in mathematics or artistically inclined, but surprisingly, she is actually both. 
Paola values attention to detail. Anyone who crafts with their hands has practical skills.

(*@\color{codepurple}{\textbf{Question:}}@*) Based on the above information, is the following statement true, false, or uncertain? 
Paola is artistically inclined.

(*@\color{codepurple}{\textbf{Options:}}@*)
A) True
B) False
C) Uncertain

The correct option is: {
  "answer": "A"
}
(*@\color{codepurple}{\textbf{------------------}}@*)
(*@\color{codepurple}{\textbf{Another Example}}@*)
(*@\color{codepurple}{\textbf{------------------}}@*)
(*@\color{codepurple}{\textbf{Context:}}@*)
[[CONTEXT]]

(*@\color{codepurple}{\textbf{Question:}}@*) [[QUESTION]]

(*@\color{codepurple}{\textbf{Options:}}@*)
[[OPTIONS]]

The correct option is:
\end{lstlisting}

\textbf{CoT Prompting}
\lstset{
    style=mystyle,
    basicstyle=\ttfamily\scriptsize,
    backgroundcolor=\color{white},
    stringstyle=\color{black},
    keywordstyle=\color{black},
    breaklines=false,
    keepspaces=false
}
\begin{lstlisting}[language=Python]
(*@\color{codepurple}{\textbf{System:}}@*)
Given a problem statement as contexts, the task is to answer a logical reasoning question. 
Your answer should be in JSON format with keys: reasoning, answer.

(*@\color{codepurple}{\textbf{Context:}}@*)
Anyone who can solve problems and communicate effectively has practical skills. If Paola 
appreciates beauty, then she values precision and enjoys manual work. Paola appreciates beauty. 
Paola is either skilled in mathematics or has practical skills, but not both. If someone enjoys 
manual work and values attention to detail, then they can craft with their hands. Paola is 
either skilled in mathematics or artistically inclined, but surprisingly, she is actually both. 
Paola values attention to detail. Anyone who crafts with their hands has practical skills.

(*@\color{codepurple}{\textbf{Question:}}@*) Based on the above information, is the following statement true, false, or uncertain? 
Paola is artistically inclined.

(*@\color{codepurple}{\textbf{Options:}}@*)
A) True
B) False
C) Uncertain

The correct option is: {
  "reasoning": "Paola appreciates beauty. If Paola appreciates beauty, then she values 
                precision and enjoys manual work. Paola enjoys manual work. Paola enjoys manual 
                work. Paola values attention to detail. If someone enjoys manual work and 
                values attention to detail, then they can craft with their hands. Paola crafts 
                with her hands. Paola crafts with her hands. Anyone who crafts with their hands 
                has practical skills. Paola has practical skills. Paola has practical skills. 
                Paola is either skilled in mathematics or has practical skills, but not both. 
                Paola is not skilled in mathematics. Paola is not skilled in mathematics. Paola 
                is either skilled in mathematics or artistically inclined, but surprisingly, 
                she is actually both. Paola is artistically inclined. Therefore, it is true 
                that Paola is artistically inclined. The correct option is: A.",
  "answer": "A"
}
(*@\color{codepurple}{\textbf{------------------}}@*)
(*@\color{codepurple}{\textbf{Another Example}}@*)
(*@\color{codepurple}{\textbf{------------------}}@*)
(*@\color{codepurple}{\textbf{Context:}}@*)
[[CONTEXT]]

(*@\color{codepurple}{\textbf{Question:}}@*) [[QUESTION]]

(*@\color{codepurple}{\textbf{Options:}}@*)
[[OPTIONS]]

The correct option is:
\end{lstlisting}

\section{Training Details}
\label{appendix-train-details}
To create the ProverQA train set, we started by generating 5,000 instances for each difficulty level: easy, medium, and hard. We then applied data augmentation techniques to expand the dataset. Specifically, for each problem, we broke down every step of its reasoning chain to form new questions. Additionally, we replaced the final conclusions with random facts to create problems whose answer is uncertain. This process expanded our generated data pool to 50,000 instances.

Our experiments show that, as the number of training instances increased, the model's performance consistently improved on in-distribution datasets and two out-of-distribution datasets: ProofWriter and ProntoQA. However, performance on FOLIO fluctuated as the number of training instances increases. After further experimentation, we found that 5,000 is a sweet spot that reaches a good balance between in-distribution and out-of-distribution performance. Therefore, we sample 5,000 instances from the generated data pool as the final training set for finetuning experiments.

Since FOLIO lacks reasoning chains, we could not apply the same augmentation techniques on it. We used the full set of FOLIO, consisting of 1,001 instances. For ProofWriter, which includes multiple questions per context similar to our data augmentation, we did not apply further augmentation.

We also finetuned the model on the full set of ProofWriter, which has 45,290 instances. The performance is comparable compared to the performance of model trained on a sampled subset consisting of 5,000 instances, indicating that the model may already be saturated at this size for an 8B parameter model. As a result, we report the results of the sampled subset. 

For all finetuning experiments, we experimented with different training hyper-parameters, such as training epochs, learning rate and training set size, and selected the best configurations on held-out validation sets. 

\section{Experimenting with 5-shot examples}
\label{appendix-5-shot}

To further explore the impact of few-shot examples, we conducted additional experiments using 5-shot prompts. The results are presented in Table~\ref{tab:5-shot}. It can be concluded that incorporating more examples (5-shot vs. 2-shot) into the prompt did not consistently improve the performance of LLMs. This inconsistency may stem from several factors, including variations in the models' in-context learning capabilities.

\begin{table}[h]
\caption{The performance of LLMs in the 5-shot setting. The values in parentheses indicate the change in performance relative to the 2-shot setting.}
\label{tab:5-shot}
\setlength\tabcolsep{3.6pt}
\begin{center}
\begin{tabular}{lccccccc}
\toprule
  & \multicolumn{3}{c}{\bf ProverQA} & \bf\small \multirow{2}{*}{ProntoQA} & \bf\small \multirow{2}{*}{ProofWriter} & \bf\small \multirow{2}{*}{FOLIO} & \bf\small \multirow{2}{*}{Avg $\Delta$}\\
  \cmidrule(r){2-4}
  & \bf\small Easy & \bf\small Medium & \bf\small Hard & & & \\
\midrule
\multicolumn{8}{c}{\textit{Standard Prompting}} \\
\midrule
GPT-4o & \makecell{85.20 \\ \small{(-2.00)}} & \makecell{68.40 \\ \small{(-0.20)}} & \makecell{43.80 \\ \small{(-2.40)}} & \makecell{88.20 \\ \small{(-3.60)}} & \makecell{57.33 \\ \small{(+1.00)}} & \makecell{72.86 \\ \small{(+5.00)}} & -0.37\\
Claude-3.5-Sonnet & \makecell{93.80 \\ \small{(+8.20)}} & \makecell{76.80 \\ \small{(+8.60)}} & \makecell{51.00 \\ \small{(+8.20)}} & \makecell{85.20 \\ \small{(-3.40)}} & \makecell{50.33 \\ \small{(-4.67)}} & \makecell{74.29 \\ \small{(-3.56)}} & +2.23\\
Llama3.1-8B-Instruct & \makecell{64.40 \\ \small{(+17.80)}} & \makecell{41.80 \\ \small{(-1.20)}} & \makecell{38.00 \\ \small{(-1.00)}} & \makecell{58.00 \\ \small{(+7.60)}} & \makecell{44.50 \\ \small{(+0.70)}} & \makecell{46.43 \\ \small{(-7.86)}} & +2.67\\
Llama3.1-70B-Instruct & \makecell{82.80 \\ \small{(+0.80)}} & \makecell{64.40 \\ \small{(+0.20)}} & \makecell{49.20 \\ \small{(+1.60)}} & \makecell{74.00 \\ \small{(-6.60)}} & \makecell{51.33 \\ \small{(+1.00)}} & \makecell{69.29 \\ \small{(+1.43)}} & -0.26 \\
Mistral-7B-Instruct & \makecell{57.40 \\ \small{(+0.60)}} & \makecell{48.40 \\ \small{(+1.60)}} & \makecell{37.40 \\ \small{(+0.20)}} & \makecell{52.40 \\ \small{(+2.40)}} & \makecell{47.83 \\ \small{(+5.50)}} & \makecell{52.86 \\ \small{(-1.43)}} & +1.48\\
Mistral-Large-Instruct & \makecell{85.00 \\ \small{(+0.40)}} & \makecell{71.40 \\ \small{(+2.20)}} & \makecell{53.60 \\ \small{(+4.00)}} & \makecell{67.40 \\ \small{(-3.60)}} & \makecell{63.50 \\ \small{(+3.17)}} & \makecell{76.43 \\ \small{(-0.71)}} & +0.91\\
Mixtral-8x22B-Instruct & \makecell{77.60 \\ \small{(+2.20)}} & \makecell{58.00 \\ \small{(+0.60)}} & \makecell{39.60 \\ \small{(+0.60)}} & \makecell{60.00 \\ \small{(-5.20)}} & \makecell{40.50 \\ \small{(+0.33)}} & \makecell{73.57 \\ \small{(-0.72)}} & -0.37\\
\midrule
\multicolumn{8}{c}{\textit{CoT Prompting}} \\
\midrule
GPT-4o & \makecell{95.00 \\ \small{(+0.80)}} & \makecell{81.20 \\ \small{(+1.80)}} & \makecell{55.00 \\ \small{(+5.00)}} & \makecell{99.20 \\ \small{(-0.80)}} & \makecell{71.00 \\ \small{(+3.67)}} & \makecell{74.29 \\ \small{(+2.15)}} & +2.10 \\
Claude-3.5-Sonnet & \makecell{92.00 \\ \small{(-3.20)}} & \makecell{81.60 \\ \small{(-2.00)}} & \makecell{61.00 \\ \small{(+4.60)}} & \makecell{95.80 \\ \small{(-3.40)}} & \makecell{75.50 \\ \small{(-0.83)}} & \makecell{82.86 \\ \small{(+2.15)}} & -0.45 \\
Llama3.1-8B-Instruct & \makecell{80.60 \\ \small{(+5.00)}} & \makecell{45.40 \\ \small{(-1.20)}} & \makecell{31.20 \\ \small{(-2.40)}} & \makecell{74.80 \\ \small{(-4.80)}} & \makecell{58.67 \\ \small{(+1.84)}} & \makecell{57.86 \\ \small{(-5.68)}} & -1.21 \\
Llama3.1-70B-Instruct & \makecell{92.60 \\ \small{(+2.20)}} & \makecell{73.40 \\ \small{(+0.20)}} & \makecell{50.20 \\ \small{(+3.40)}} & \makecell{92.80 \\ \small{(-2.60)}} & \makecell{67.00 \\ \small{(-4.17)}} & \makecell{75.00 \\ \small{(+0.71)}} & -0.04 \\
Mistral-7B-Instruct & \makecell{65.60 \\ \small{(-6.40)}} & \makecell{50.20 \\ \small{(-0.80)}} & \makecell{38.00 \\ \small{(-3.80)}} & \makecell{65.80 \\ \small{(+4.60)}} & \makecell{48.33 \\ \small{(+2.33)}} & \makecell{63.57 \\ \small{(-0.01)}} & -0.68 \\
Mistral-Large-Instruct & \makecell{94.40 \\ \small{(+1.80)}} & \makecell{77.60 \\ \small{(+1.80)}} & \makecell{56.60 \\ \small{(+4.40)}} & \makecell{99.00 \\ \small{(+0.40)}} & \makecell{77.67 \\ \small{(+4.17)}} & \makecell{82.86 \\ \small{(-0.71)}} & +1.98\\
Mixtral-8x22B-Instruct & \makecell{91.00 \\ \small{(+3.40)}} & \makecell{73.80 \\ \small{(+7.00)}}& \makecell{50.20 \\ \small{(+2.60)}} & \makecell{86.20 \\ \small{(+6.60)}} & \makecell{59.67 \\ \small{(+2.00)}} & \makecell{72.14 \\ \small{(-1.43)}} & +3.36 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\section{Quality Control}
\label{appendix-qc}
The ProverGen framework employs three heuristic methods to ensure the quality of the generated data, addressing different aspects of data integrity and consistency. 

\begin{itemize}
    \item \textbf{Logic Validation}: Our framework includes a robust logic validation step. For each instance, we input both core premises and distractions into the symbolic prover to ensure they correctly deduce the conclusion's truth value. 
    \item \textbf{Conflicts Resolution}: During rule translation, we check for previously used predicates to avoid redundancy and potential conficting facts. Additionally, we utilize LLMs to assess whether the generated universal rules align with real-world knowledge. In cases of conflict, we opt for specific rules instead. 
    \item \textbf{Translation Quality Control:} We apply a heuristic method to ensure that all involved entities appear in both symbolic expression and natural language expression. For example, when translating "poet(Sawyer)" into "Sawyer is a poet", we verify that both the name "Sawyer" and the predicate "poet" are present in the translation.
\end{itemize}

Additionally, the training part of the paper also serves as a quality-checking process. 
Finetuning on the generated dataset enhances the performance of LLMs on both in-distribution and Out-of-Distribution datasets, indicating the relatively high quality of the generated data.

To further investigate the quality of the generated data, we performed a manual evaluation of 60 randomly sampled examples (20 from each subset) from our dataset.
The evaluation revealed no translation errors or indirect contradictions, suggesting that the likelihood of potential errors is less than 2\%.

\typeout{get arXiv to do 4 passes: Label(s) may have changed. Rerun}
\end{document}
