@article{ashkan2024mechanistic,
title={Mechanistic interpretability of large language models with applications to the financial services industry},
author={Ashkan, Golgoon and Khashayar, Filom and Arjun, Ravi, Kannan},
journal={arXiv preprint arXiv:2407.11215},
year={2024},
url={https://www.arxiv.org/abs/2407.11215}
}

@misc{belrose2023elicitinglatentpredictionstransformers,
title={Eliciting Latent Predictions from Transformers with the Tuned Lens},
author={Nora Belrose and Zach Furman and Logan Smith and Danny Halawi and Igor Ostrovsky and Lev McKinney and Stella Biderman and Jacob Steinhardt},
year={2023},
eprint={2303.08112},
archivePrefix={arXiv},
primaryClass={cs.LG},
url={https://arxiv.org/abs/2303.08112},
}
We analyze transformers from the perspective of iterative inference, seeking to understand how model predictions are refined layer by layer. To do so, we train an affine probe for each block in a frozen pretrained model, making it possible to decode every hidden state into a distribution over the vocabulary. Our method, the \emph{tuned lens}

@misc{bhalla2025unifyinginterpretabilitycontrolevaluation,
title={Towards Unifying Interpretability and Control: Evaluation via Intervention},
author={Usha Bhalla and Suraj Srinivas and Asma Ghandeharioun and Himabindu Lakkaraju},
year={2025},
eprint={2411.04430},
archivePrefix={arXiv},
primaryClass={cs.LG},
url={https://arxiv.org/abs/2411.04430},
}

@misc{ghandeharioun2024patchscopesunifyingframeworkinspecting,
title={Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models},
author={Asma Ghandeharioun and Avi Caciularu and Adam Pearce and Lucas Dixon and Mor Geva},
year={2024},
eprint={2401.06102},
archivePrefix={arXiv},
primaryClass={cs.CL},
url={https://arxiv.org/abs/2401.06102},
}

@article{jaturong2025an,
title={An Attempt to Unraveling Token Prediction Refinement and Identifying Essential Layers of Large Language Models},
author={Jaturong, Kongmanee},
journal={arXiv preprint arXiv:2501.15054v1},
year={2025},
url={https://www.arxiv.org/abs/2501.15054v1}
}

@misc{katz2024backwardlensprojectinglanguage,
title={Backward Lens: Projecting Language Model Gradients into the Vocabulary Space},
author={Shahar Katz and Yonatan Belinkov and Mor Geva and Lior Wolf},
year={2024},
eprint={2402.12865},
archivePrefix={arXiv},
primaryClass={cs.CL},
url={https://arxiv.org/abs/2402.12865},
}

@misc{nostalgebraist2020interpreting,
  author = {nostalgebraist},
  title = {Interpreting {GPT}: the logit lens},
  howpublished = {\url{https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens}},
  year = {2020},
  note = {Accessed: 2025-02-22},
  month = {August},
  day = {31}
}

@misc{wu2024semantichubhypothesislanguage,
title={The Semantic Hub Hypothesis: Language Models Share Semantic Representations Across Languages and Modalities},
author={Zhaofeng Wu and Xinyan Velocity Yu and Dani Yogatama and Jiasen Lu and Yoon Kim},
year={2024},
eprint={2411.04986},
archivePrefix={arXiv},
primaryClass={cs.CL},
url={https://arxiv.org/abs/2411.04986},
}

