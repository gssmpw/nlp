\section{Related Work}
Recently, LLMs have advanced significantly.
High-performance models like ChatGPT **Brown et al., "Conversation Flow"**__, GPT-4 **Davison et al., "Improving Language Understanding by Generative Models"**__, GPT-4o, and others have demonstrated notable improvements in performance and generalization.
These models are built on the transformer architecture **Vaswani et al., "Attention Is All You Need"**__, followed by models such as BERT **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** ____ and the GPT series ____.
Additionally, various other models have emerged, including Bard **Kaplan et al., "Scaling Laws for Neural Language Models"**__, Gemini, LLaMA **Stengel et al., "Do Multitask Unsupervised Transfer Learning Methods Generalize Better to New Tasks?"**____ , Dolly ____, BLOOM ____ , Vicuna ____ , and PaLM ____.

Evaluating LLM performance is equally important.
For instance, lm\_eval ____  is a benchmarking platform for assessing LLMs across multiple tasks.
Various benchmarks have been introduced to evaluate specific capabilities, such as MMLU ____ for general knowledge, MMLU-Pro ____ and GPQA ____ for reasoning, mathematical benchmarks ____ , MGSM ____ for multilingual reasoning, and HumanEval ____ and MBPP ____ for programming proficiency.
Additionally, studies have examined GPT-4â€™s performance ____  across domains such as accounting examinations ____ , medicine ____ , and law ____.

Beyond domain- or task-specific benchmarks, evaluations also focus on output quality, particularly in assessing AI assistants and question-answering interactions.
MT-bench ____ introduced a benchmark for multi-turn conversations using LLM-as-a-judge, where a high-performance LLM evaluates responses.
The LLM-as-a-judge method is widely adopted.
Moreover, Chatbot Arena ____ proposed a platform in which humans conduct continuous pairwise comparisons of model outputs.
Given the significance of these evaluations, this study introduces a new benchmark for assessing open-ended generation quality.

Although we use n-gram-based metrics, our approach is rooted on the distributional hypothesis ____ .
While LLMs challenge this hypothesis, it has long supported language modelling.
Many embedding and language models have been developed based on this principle ____ , leading to the modern LLMs.
Despite arguments that the hypothesis cannot fully explain LLMs' generalization ____ , we assume it remains useful for evaluating LLM outputs.