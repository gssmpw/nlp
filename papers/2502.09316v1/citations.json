[
  {
    "index": 0,
    "papers": [
      {
        "key": "chatgpt",
        "author": "OpenAI",
        "title": "{ChatGPT}"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "GPT4",
        "author": "Unknown",
        "title": "Unknown"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "Vaswani2017",
        "author": "Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\\L}ukasz and Polosukhin, Illia",
        "title": "{Attention Is All You Need}"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "Devlin2018",
        "author": "Devlin, Jacob  and\nChang, Ming-Wei  and\nLee, Kenton  and\nToutanova, Kristina",
        "title": "{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "GPT-1",
        "author": "Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya",
        "title": "{Improving Language Understanding by Generative Pre-Training}"
      },
      {
        "key": "GPT-2",
        "author": "Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya",
        "title": "{Language Models are Unsupervised Multitask Learners}"
      },
      {
        "key": "GPT-3",
        "author": "Brown, Tom and Mann, Benjamin and others",
        "title": "{Language Models are Few-Shot Learners}"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "bard",
        "author": "Google",
        "title": "Bard"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "touvron2023llama",
        "author": "Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\\'e}e and Rozi{\\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others",
        "title": "{LLaMA: Open and Efficient Foundation Language Models}"
      },
      {
        "key": "Touvron2023",
        "author": "Hugo Touvron and Louis Martin and others",
        "title": "{Llama 2: Open Foundation and Fine-Tuned Chat Models}"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "dolly",
        "author": "Databricks",
        "title": "Dolly"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "scao2022bloom",
        "author": "Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\\'c}, Suzana and Hesslow, Daniel and Castagn{\\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\\c{c}}ois and Gall{\\'e}, Matthias and others",
        "title": "{BLOOM: A 176B-Parameter Open-Access Multilingual Language Model}"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "vicuna",
        "author": "Vicuna",
        "title": "{Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\\%* ChatGPT Quality}"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "Chowdhery2022",
        "author": "Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sashank and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah",
        "title": "{PaLM: scaling language modeling with pathways}"
      },
      {
        "key": "Anil2023",
        "author": "Rohan Anil and Andrew M. Dai and Orhan Firat and Melvin Johnson and Dmitry Lepikhin and Alexandre Passos and Siamak Shakeri and Emanuel Taropa and Paige Bailey and Zhifeng Chen and Eric Chu and Jonathan H. Clark and Laurent El Shafey and Yanping Huang and Kathy Meier-Hellstern and Gaurav Mishra and Erica Moreira and Mark Omernick and Kevin Robinson and Sebastian Ruder and Yi Tay and Kefan Xiao and Yuanzhong Xu and Yujing Zhang and Gustavo Hernandez Abrego and Junwhan Ahn and Jacob Austin and Paul Barham and Jan Botha and James Bradbury and Siddhartha Brahma and Kevin Brooks and Michele Catasta and Yong Cheng and Colin Cherry and Christopher A. Choquette-Choo and Aakanksha Chowdhery and Cl\u00e9ment Crepy and Shachi Dave and Mostafa Dehghani and Sunipa Dev and Jacob Devlin and Mark D\u00edaz and Nan Du and Ethan Dyer and Vlad Feinberg and Fangxiaoyu Feng and Vlad Fienber and Markus Freitag and Xavier Garcia and Sebastian Gehrmann and Lucas Gonzalez and Guy Gur-Ari and Steven Hand and Hadi Hashemi and Le Hou and Joshua Howland and Andrea Hu and Jeffrey Hui and Jeremy Hurwitz and Michael Isard and Abe Ittycheriah and Matthew Jagielski and Wenhao Jia and Kathleen Kenealy and Maxim Krikun and Sneha Kudugunta and Chang Lan and Katherine Lee and Benjamin Lee and Eric Li and Music Li and Wei Li and YaGuang Li and Jian Li and Hyeontaek Lim and Hanzhao Lin and Zhongtao Liu and Frederick Liu and Marcello Maggioni and Aroma Mahendru and Joshua Maynez and Vedant Misra and Maysam Moussalem and Zachary Nado and John Nham and Eric Ni and Andrew Nystrom and Alicia Parrish and Marie Pellat and Martin Polacek and Alex Polozov and Reiner Pope and Siyuan Qiao and Emily Reif and Bryan Richter and Parker Riley and Alex Castro Ros and Aurko Roy and Brennan Saeta and Rajkumar Samuel and Renee Shelby and Ambrose Slone and Daniel Smilkov and David R. So and Daniel Sohn and Simon Tokumine and Dasha Valter and Vijay Vasudevan and Kiran Vodrahalli and Xuezhi Wang and Pidong Wang and Zirui Wang and Tao Wang and John Wieting and Yuhuai Wu and Kelvin Xu and Yunhan Xu and Linting Xue and Pengcheng Yin and Jiahui Yu and Qiao Zhang and Steven Zheng and Ce Zheng and Weikang Zhou and Denny Zhou and Slav Petrov and Yonghui Wu",
        "title": "{PaLM 2 Technical Report}"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "eval-harness",
        "author": "Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy",
        "title": "A framework for few-shot language model evaluation"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "hendryckstest2021",
        "author": "Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt",
        "title": "Measuring Massive Multitask Language Understanding"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "wang2024mmlu",
        "author": "Wang, Yubo and Ma, Xueguang and Zhang, Ge and Ni, Yuansheng and Chandra, Abhranil and Guo, Shiguang and Ren, Weiming and Arulraj, Aaran and He, Xuan and Jiang, Ziyan and others",
        "title": "Mmlu-pro: A more robust and challenging multi-task language understanding benchmark"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "rein2023gpqa",
        "author": "Rein, David and Hou, Betty Li and Stickland, Asa Cooper and Petty, Jackson and Pang, Richard Yuanzhe and Dirani, Julien and Michael, Julian and Bowman, Samuel R",
        "title": "{GPQA: A Graduate-Level Google-Proof Q\\&A Benchmark}"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "cobbe2021gsm8k",
        "author": "Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John",
        "title": "Training Verifiers to Solve Math Word Problems"
      },
      {
        "key": "hendrycksmath2021",
        "author": "Dan Hendrycks and Collin Burns and Saurav Kadavath and Akul Arora and Steven Basart and Eric Tang and Dawn Song and Jacob Steinhardt",
        "title": "Measuring Mathematical Problem Solving With the MATH Dataset"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "shi2022language",
        "author": "Freda Shi and Mirac Suzgun and Markus Freitag and Xuezhi Wang and Suraj Srivats and Soroush Vosoughi and Hyung Won Chung and Yi Tay and Sebastian Ruder and Denny Zhou and Dipanjan Das and Jason Wei",
        "title": "Language models are multilingual chain-of-thought reasoners"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "chen2021codex",
        "author": "Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba",
        "title": "Evaluating Large Language Models Trained on Code"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "austin2021program",
        "author": "Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others",
        "title": "Program synthesis with large language models"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "GPT4",
        "author": "Unknown",
        "title": "Unknown"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "Eulerich2023",
        "author": "Marc Eulerich and Aida Sanatizadeh and Hamid Vakilzadeh and David A. Wood",
        "title": "{Is it All Hype? ChatGPT\u2019s Performance and Disruptive Potential in the Accounting and Auditing Industries}"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "Nori2023",
        "author": "Harsha Nori and Nicholas King and Scott Mayer Mckinney and Dean Carignan and Eric Horvitz",
        "title": "{Capabilities of GPT-4 on Medical Challenge Problems}"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "Iu2023",
        "author": "Kwan Yuen Iu and Vanessa Man-Yi Wong",
        "title": "{ChatGPT by OpenAI: The End of Litigation Lawyers?}"
      },
      {
        "key": "Choi2023",
        "author": "Jonathan H. Choi and Kristin E. Hickman and Amy Monahan and Daniel B. Schwarcz",
        "title": "ChatGPT Goes to Law School"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "MT-bench",
        "author": "Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and Zhang, Hao and Gonzalez, Joseph E and Stoica, Ion",
        "title": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "chiang2024chatbot",
        "author": "Chiang, Wei-Lin and Zheng, Lianmin and Sheng, Ying and Angelopoulos, Anastasios N. and Li, Tianle and Li, Dacheng and Zhu, Banghua and Zhang, Hao and Jordan, Michael I. and Gonzalez, Joseph E. and Stoica, Ion",
        "title": "Chatbot arena: an open platform for evaluating LLMs by human preference"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "harris1954distributional",
        "author": "Zellig S. Harris",
        "title": "Distributional Structure"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "mikolov2013efficient",
        "author": "Mikolov, Tomas",
        "title": "Efficient estimation of word representations in vector space"
      },
      {
        "key": "sarzynska2021detecting",
        "author": "Sarzynska-Wawer, Justyna and Wawer, Aleksander and Pawlak, Aleksandra and Szymanowska, Julia and Stefaniak, Izabela and Jarkiewicz, Michal and Okruszek, Lukasz",
        "title": "Detecting formal thought disorder by deep contextualized word representations"
      },
      {
        "key": "bojanowski2017enriching",
        "author": "Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas",
        "title": "Enriching Word Vectors with Subword Information"
      },
      {
        "key": "Devlin2018",
        "author": "Devlin, Jacob  and\nChang, Ming-Wei  and\nLee, Kenton  and\nToutanova, Kristina",
        "title": "{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "chiang2023distributional",
        "author": "Chiang, Ting-Rui and Yogatama, Dani",
        "title": "The Distributional Hypothesis Does Not Fully Explain the Benefits of Masked Language Model Pretraining"
      },
      {
        "key": "enyan2024llms",
        "author": "Enyan, Zhang and Wang, Zewei and Lepori, Michael A and Pavlick, Ellie and Aparicio, Helena",
        "title": "Are LLMs Models of Distributional Semantics? A Case Study on Quantifiers"
      }
    ]
  }
]