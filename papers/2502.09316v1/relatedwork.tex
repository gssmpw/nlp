\section{Related Work}
Recently, LLMs have advanced significantly.
High-performance models like ChatGPT \cite{chatgpt}, GPT-4 \cite{GPT4}, GPT-4o, and others have demonstrated notable improvements in performance and generalization.
These models are built on the transformer architecture \cite{Vaswani2017}, followed by models such as BERT \cite{Devlin2018} and the GPT series \cite{GPT-1, GPT-2, GPT-3}.
Additionally, various other models have emerged, including Bard \cite{bard}, Gemini, LLaMA \cite{touvron2023llama, Touvron2023}, Dolly \cite{dolly}, BLOOM \cite{scao2022bloom}, Vicuna \cite{vicuna}, and PaLM \cite{Chowdhery2022, Anil2023}.

Evaluating LLM performance is equally important.
For instance, lm\_eval \cite{eval-harness} is a benchmarking platform for assessing LLMs across multiple tasks.
Various benchmarks have been introduced to evaluate specific capabilities, such as MMLU \cite{hendryckstest2021} for general knowledge, MMLU-Pro \cite{wang2024mmlu} and GPQA \cite{rein2023gpqa} for reasoning, mathematical benchmarks \cite{cobbe2021gsm8k, hendrycksmath2021}, MGSM \cite{shi2022language} for multilingual reasoning, and HumanEval \cite{chen2021codex} and MBPP \cite{austin2021program} for programming proficiency.
Additionally, studies have examined GPT-4â€™s performance of \cite{GPT4} across domains such as accounting examinations \cite{Eulerich2023}, medicine \cite{Nori2023}, and law \cite{Iu2023, Choi2023}.

Beyond domain- or task-specific benchmarks, evaluations also focus on output quality, particularly in assessing AI assistants and question-answering interactions.
MT-bench \cite{MT-bench} introduced a benchmark for multi-turn conversations using LLM-as-a-judge, where a high-performance LLM evaluates responses.
The LLM-as-a-judge method is widely adopted.
Moreover, Chatbot Arena \cite{chiang2024chatbot} proposed a platform in which humans conduct continuous pairwise comparisons of model outputs.
Given the significance of these evaluations, this study introduces a new benchmark for assessing open-ended generation quality.

Although we use n-gram-based metrics, our approach is rooted on the distributional hypothesis \cite{harris1954distributional}.
While LLMs challenge this hypothesis, it has long supported language modelling.
Many embedding and language models have been developed based on this principle \cite{mikolov2013efficient,sarzynska2021detecting,bojanowski2017enriching,Devlin2018}, leading to the modern LLMs.
Despite arguments that the hypothesis cannot fully explain LLMs' generalization \cite{chiang2023distributional,enyan2024llms}, we assume it remains useful for evaluating LLM outputs.