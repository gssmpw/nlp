\section{RELATED WORK}
\subsection{2D-Text Matching}
In recent years, 2D-text matching models such as CLIP \cite{radford2021learning}, BLIP \cite{li2022blip}, and Open-VCLIP \cite{weng2023open} have demonstrated impressive performance not only on retrieval tasks but also across numerous downstream tasks. The success is primarily attributed to the availability of large-scale image-text and video-text pretraining datasets like LAION-400M \cite{schuhmann2021laion} and HowTo100M \cite{miech2019howto100m}.
In particular, CLIP \cite{radford2021learning} pre-trained on 400M image-text pairs achieves remarkable zero-shot performance across 27 datasets, including ImageNet \cite{deng2009imagenet}. 

Recent methods \cite{Peng_2024_CVPR, doveh2023teaching} also leverage the generation capabilities of diffusion models \cite{podellsdxl} and large language models (LLMs) \cite{le2022bloom} for data augmentation. However, these methods do not generate data during training due to the high computational cost of diffusion models and LLMs, limiting the diversity of augmented data.
In contrast, we generate part-level captions using a multimodal large language model (MLLM) and then randomly sample multiple parts to compose complex 3D shapes with corresponding captions during the training process, introducing minimal additional computational cost. This approach ensures the diversity of both shape geometry and text semantics, leading to more robust and effective data augmentation.



\subsection{3D-Text Matching}
Text2Shape \cite{chen2019text2shape} introduces a 3D-text dataset by captioning 3D shapes from ShapeNet \cite{chang2015shapenet} and proposes a framework to learn joint embeddings of 3D shapes and natural languages. This framework consists of a 3D-CNN and GRU \cite{chung2014empirical} to encode 3D voxelized shapes and texts, followed by metric learning to achieve alignment between modalities. $\rm Y^{2}$Seq2Seq \cite{han2019y2seq2seq} models both multi-view images and texts in a sequence-to-sequence manner to jointly reconstruct and predict view and word sequences. TriCoLo \cite{ruan2024tricolo} proposes a trimodal training framework to jointly align 3D voxels, multi-view images, and texts.
Parts2Words \cite{tang2021parts2words} employs regional-based matching to compute local similarities and enhance retrieval performance. COM3D \cite{wu2024com3d} further considers cross-view correspondence and augments 3D features using SRT \cite{sajjadi2022scene}. However, these methods primarily focus on extracting more discriminative cross-modal representations, overlooking the scarcity of 3D-text paired data. We try to mitigate this issue by applying data augmentation with an MLLM to extensively create new 3D-text pairs, leading to robust and generalized retrieval capability.


In addition to the aforementioned 3D-text retrieval methods, PointCLIP \cite{zhang2022pointclip} and CLIP2Point \cite{huang2023clip2point} train additional adapters with depth maps to transfer 2D CLIP knowledge to 3D shape classification. Nevertheless, they do not effectively bridge the gap between 2D and 3D visual information including self-occlusion, due to the limited number of multi-view images. We adopt point clouds as 3D shape representations to better model geometric information.

\begin{figure*}[t]
\centering
  \subfloat{\includegraphics[width=1\linewidth]{pics/framework2.pdf}} 
   
\caption{\textbf{The pipeline of 3D-caption paired data augmentation.} The component library is created by captioning 3D shape parts through LLaVA. During training, different components are sampled from this library, and repositioning is applied to generate new 3D shapes with correct geometry and corresponding text captions.}
\label{figure::data augmentation}
\end{figure*}