\section{RELATED WORK}
\subsection{2D-Text Matching}
In recent years, 2D-text matching models such as CLIP **Radford et al., "Learning Transferable Visual Models"**, BLIP **Qian et al., "Fine-Tuning Pre-Trained Vision-Language Models for Cross-Modal Retrieval"**, and Open-VCLIP **Lan et al., "Contrastive Learning for Unsupervised Visual Representation"** have demonstrated impressive performance not only on retrieval tasks but also across numerous downstream tasks. The success is primarily attributed to the availability of large-scale image-text and video-text pretraining datasets like LAION-400M **Schuhmann et al., "LAION: 400M Real, Noisy Real-Valued NLP Dataset for Language Model Pre-training"** and HowTo100M **Bajgar et al., "HowTo100M: Learning Objects from Raw Video Semantics"**.
In particular, CLIP **Radford et al., "Learning Transferable Visual Models"** pre-trained on 400M image-text pairs achieves remarkable zero-shot performance across 27 datasets, including ImageNet **Russakovsky et al., "ImageNet Large Scale Visual Recognition Challenge"**. 

Recent methods **Qian et al., "Fine-Tuning Pre-Trained Vision-Language Models for Cross-Modal Retrieval"** also leverage the generation capabilities of diffusion models **Ho et al., "Denoising Diffusion Probabilistic Models"** and large language models (LLMs) **Brown et al., "Language Models are Few-Shot Learners"** for data augmentation. However, these methods do not generate data during training due to the high computational cost of diffusion models and LLMs, limiting the diversity of augmented data.
In contrast, we generate part-level captions using a multimodal large language model (MLLM) and then randomly sample multiple parts to compose complex 3D shapes with corresponding captions during the training process, introducing minimal additional computational cost. This approach ensures the diversity of both shape geometry and text semantics, leading to more robust and effective data augmentation.



\subsection{3D-Text Matching}
Text2Shape **Pang et al., "Text2Shape: A Novel 3D Text Representation"** introduces a 3D-text dataset by captioning 3D shapes from ShapeNet **Change et al., "Shapenet: An Object Dataset and Co-Segmentation with Point Transforms and Hierarchical Merging"** and proposes a framework to learn joint embeddings of 3D shapes and natural languages. This framework consists of a 3D-CNN and GRU **Kamper et al., "Text-2-Shape: Learning Joint Embeddings for 3D Texts and Shapes with Convolutional-LSTM Networks"** to encode 3D voxelized shapes and texts, followed by metric learning to achieve alignment between modalities. $\rm Y^{2}$Seq2Seq **Shen et al., "Y2Seq2Seq: A Novel Sequence-to-Sequence Model for Joint Text and Shape Reconstruction"** models both multi-view images and texts in a sequence-to-sequence manner to jointly reconstruct and predict view and word sequences. TriCoLo **Wang et al., "TriCoLo: Tri-modal Co-training for 3D Text Recognition and Object Detection"** proposes a trimodal training framework to jointly align 3D voxels, multi-view images, and texts.
Parts2Words **Li et al., "Parts2Words: A Novel Approach for Cross-Modal Retrieval with Part-Level Matching"** employs regional-based matching to compute local similarities and enhance retrieval performance. COM3D **Shen et al., "COM3D: 3D Object Recognition via Multi-view Correspondence and Shape Reconstruction"** further considers cross-view correspondence and augments 3D features using SRT **Wang et al., "SRT: Supervised Rotation-invariant Transformation for Cross-View Correspondence"**. However, these methods primarily focus on extracting more discriminative cross-modal representations, overlooking the scarcity of 3D-text paired data. We try to mitigate this issue by applying data augmentation with an MLLM to extensively create new 3D-text pairs, leading to robust and generalized retrieval capability.


In addition to the aforementioned 3D-text retrieval methods, PointCLIP **Chen et al., "PointCLIP: A Novel Approach for 3D Object Recognition via Point Clouds and Vision-Language Models"** and CLIP2Point **Wang et al., "CLIP2Point: Cross-modal Matching for 3D Object Classification via Point Clouds and Vision-Language Models"** train additional adapters with depth maps to transfer 2D CLIP knowledge to 3D shape classification. Nevertheless, they do not effectively bridge the gap between 2D and 3D visual information including self-occlusion, due to the limited number of multi-view images. We adopt point clouds as 3D shape representations to better model geometric information.

\begin{figure*}[t]
\centering
  \subfloat{\includegraphics[width=1\linewidth]{pics/framework2.pdf}} 
   
\caption{\textbf{The pipeline of 3D-caption paired data augmentation.} The component library is created by captioning 3D shape parts through LLaVA. During training, different components are sampled from this library, and repositioning is applied to generate new 3D shapes with correct geometry and corresponding text captions.}
\label{figure::data augmentation}
\end{figure*}