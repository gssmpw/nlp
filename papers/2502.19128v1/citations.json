[
  {
    "index": 0,
    "papers": [
      {
        "key": "radford2021learning",
        "author": "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others",
        "title": "Learning transferable visual models from natural language supervision"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "li2022blip",
        "author": "Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven",
        "title": "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "weng2023open",
        "author": "Weng, Zejia and Yang, Xitong and Li, Ang and Wu, Zuxuan and Jiang, Yu-Gang",
        "title": "Open-vclip: Transforming clip to an open-vocabulary video model via interpolated weight optimization"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "schuhmann2021laion",
        "author": "Schuhmann, Christoph and Kaczmarczyk, Robert and Komatsuzaki, Aran and Katta, Aarush and Vencu, Richard and Beaumont, Romain and Jitsev, Jenia and Coombes, Theo and Mullis, Clayton",
        "title": "LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "miech2019howto100m",
        "author": "Miech, Antoine and Zhukov, Dimitri and Alayrac, Jean-Baptiste and Tapaswi, Makarand and Laptev, Ivan and Sivic, Josef",
        "title": "Howto100m: Learning a text-video embedding by watching hundred million narrated video clips"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "radford2021learning",
        "author": "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others",
        "title": "Learning transferable visual models from natural language supervision"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "deng2009imagenet",
        "author": "Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li",
        "title": "Imagenet: A large-scale hierarchical image database"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "Peng_2024_CVPR",
        "author": "Peng, Wujian and Xie, Sicheng and You, Zuyao and Lan, Shiyi and Wu, Zuxuan",
        "title": "Synthesize Diagnose and Optimize: Towards Fine-Grained Vision-Language Understanding"
      },
      {
        "key": "doveh2023teaching",
        "author": "Doveh, Sivan and Arbelle, Assaf and Harary, Sivan and Schwartz, Eli and Herzig, Roei and Giryes, Raja and Feris, Rogerio and Panda, Rameswar and Ullman, Shimon and Karlinsky, Leonid",
        "title": "Teaching structured vision \\& language concepts to vision \\& language models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "podellsdxl",
        "author": "Podell, Dustin and English, Zion and Lacey, Kyle and Blattmann, Andreas and Dockhorn, Tim and M{\\\"u}ller, Jonas and Penna, Joe and Rombach, Robin",
        "title": "SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "le2022bloom",
        "author": "Le Scao, Teven and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\\'c}, Suzana and Hesslow, Daniel and Castagn{\\'e}, Roman and Sasha Luccioni, Alexandra and Yvon, Fran{\\c{c}}ois and Gall{\\'e}, Matthias and others",
        "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "chen2019text2shape",
        "author": "Chen, Kevin and Choy, Christopher B and Savva, Manolis and Chang, Angel X and Funkhouser, Thomas and Savarese, Silvio",
        "title": "Text2shape: Generating shapes from natural language by learning joint embeddings"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "chang2015shapenet",
        "author": "Chang, Angel X and Funkhouser, Thomas and Guibas, Leonidas and Hanrahan, Pat and Huang, Qixing and Li, Zimo and Savarese, Silvio and Savva, Manolis and Song, Shuran and Su, Hao and others",
        "title": "Shapenet: An information-rich 3d model repository"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "chung2014empirical",
        "author": "Chung, Junyoung and Gulcehre, Caglar and Cho, Kyunghyun and Bengio, Yoshua",
        "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "han2019y2seq2seq",
        "author": "Han, Zhizhong and Shang, Mingyang and Wang, Xiyang and Liu, Yu-Shen and Zwicker, Matthias",
        "title": "Y2Seq2Seq: Cross-modal representation learning for 3D shape and text by joint reconstruction and prediction of view and word sequences"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "ruan2024tricolo",
        "author": "Ruan, Yue and Lee, Han-Hung and Zhang, Yiming and Zhang, Ke and Chang, Angel X",
        "title": "TriCoLo: Trimodal contrastive loss for text to shape retrieval"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "tang2021parts2words",
        "author": "Tang, Chuan and Yang, Xi and Wu, Bojian and Han, Zhizhong and Chang, Yi",
        "title": "Parts2Words: Learning Joint Embedding of Point Clouds and Texts by Bidirectional Matching Between Parts and Words"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "wu2024com3d",
        "author": "Wu, Hao and Li, Ruochong and Wang, Hao and Xiong, Hui",
        "title": "COM3D: Leveraging Cross-View Correspondence and Cross-Modal Mining for 3D Retrieval"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "sajjadi2022scene",
        "author": "Sajjadi, Mehdi SM and Meyer, Henning and Pot, Etienne and Bergmann, Urs and Greff, Klaus and Radwan, Noha and Vora, Suhani and Lu{\\v{c}}i{\\'c}, Mario and Duckworth, Daniel and Dosovitskiy, Alexey and others",
        "title": "Scene representation transformer: Geometry-free novel view synthesis through set-latent scene representations"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "zhang2022pointclip",
        "author": "Zhang, Renrui and Guo, Ziyu and Zhang, Wei and Li, Kunchang and Miao, Xupeng and Cui, Bin and Qiao, Yu and Gao, Peng and Li, Hongsheng",
        "title": "Pointclip: Point cloud understanding by clip"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "huang2023clip2point",
        "author": "Huang, Tianyu and Dong, Bowen and Yang, Yunhan and Huang, Xiaoshui and Lau, Rynson WH and Ouyang, Wanli and Zuo, Wangmeng",
        "title": "Clip2point: Transfer clip to point cloud classification with image-depth pre-training"
      }
    ]
  }
]