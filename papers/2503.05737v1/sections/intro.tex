\section{Introduction}
The recent advancements in the performance of AI models\footnote{The policies discussed in this article mostly focus on the current systems commonly referred to as ``generative AI'', in that they can be used to generate synthetic texts/images etc. They are typically based on models of Transformer architecture pre-trained on large volumes of data (texts, images etc.). The exact definitions are rarely provided and are an active area of discussion in research (see references in \autoref{fig:sociotech-challenges}).} on a multitude of tasks, especially in zero-shot or few-shot scenarios \cite{KOCON2023101861, zhao2023survey}, have accelerated their adoption across different domains.  This rapid uptake has presented organisations with unprecedented challenges, necessitating the \textit{swift development of organisational guidelines} for the use of AI to manage associated risks and opportunities. These guidelines reflect bottom-up governance approaches tailored to local needs and operational contexts.

From the top-down perspective, the most significant governance effort currently is the EU AI Act (\aia) \cite{2024-ai-act} (\S\ref{sec:aia}), which provides overarching frameworks for managing high-risk AI systems. 
However, while such frameworks establish broad standards, their top-down approach often lacks the specificity required for effective implementation in diverse organisational settings. This creates gaps where organisations must independently interpret and address risks, resulting in guidelines that emphasise practical challenges, such as AI literacy, bias mitigation, and environmental sustainability, which are underdeveloped in existing legislation.
This raises our main research question: \textit{can the analysis of bottom-up initiatives (organisational guidelines and academic research) provide valuable insights into the areas that are not sufficiently addressed by the top-down framework of the \aia?}\looseness=-1

To address this question, this paper compiles a list of challenges known from AI research (\S\ref{sec:challenges}) and conducts case studies of organisational AI guidelines from two domains: universities and news organisations. Through iterative coding of guidelines developed by organisations in each domain (\S\ref{sec:method}), we examine discrepancies in how risks are classified, uses are prescribed, and the performance of AI models is perceived. We discuss the commonalities and discrepancies within each domain (news \S\ref{sec:policies:news}, universities \S\ref{sec:policies:uni}). %We then analyse how these priorities highlight gaps in the \aia, including insufficient attention to AI literacy, digital inequity, bias mitigation, attribution of training data, and environmental impact .
%
We identify multiple areas in need of clarifications and further research, and provide %recommendations (\autoref{}) For example, universities’ integration of AI literacy into academic practices and newsrooms’ emphasis on internal AI usage registers illustrate actionable strategies for addressing practical governance challenges. 
%Finally, we consider the perspective of sociotechnical challenges with AI systems that are known from AI research, but are currently insufficiently addressed in either \aia or organisational policies (\S\ref{sec:challenges:discuss}).
%
% Further, we point out implications for the future development of international legislation, such as the EU AI Act based on the specialised organisational AI policies, which have been used in practice. 
%By bridging the gap between local practices and international standards, this study provides 
actionable recommendations for policymakers % and highlights the critical role of research in addressing sociotechnical challenges and ensuring adaptive and effective AI governance at a global scale. We show the need for further discussion 
on the topics of AI literacy training, digital inequity, disclosure of AI-generated content, bias, attribution, and environmental impact in the context of AI legislation (\S\ref{sec:insights}). Our findings underscore the potential of organisational policies and academic research to inform and refine global frameworks like the \aia. \looseness=-1

%
% The increased use of Large Language Models (LLMs) across various sectors has raised significant concerns about their responsible use, particularly regarding transparency, accountability, and safety. 
% Different stakeholders have made efforts to assess and manage the risks posed by these technologies. At the global level, regulations such as the \aia have set standards for managing high-risk AI systems. Meanwhile, institutions in specific sectors, such as news organizations and educational institutions, are formulating localized policies to guide the use of AI within their domains. Additionally, developers of LLMs and other machine learning models have worked to identify key challenges, risks, and opportunities, contributing insights into technical limitations such as robustness, interpretability, and bias. However, there is limited research on how these efforts align—how local guidelines interact with international laws like the \aia and whether they effectively address the technical and ethical challenges identified by AI researchers and practitioners.

% This paper seeks to address this gap by (1) comparing and contrasting AI policies from local institutions, such as news publishers and educational institutions, with the \aia, and (2) examining how technological challenges are reflected in these regulations. Our analysis identifies discrepancies in the role of guidelines at different levels, variations in oversight mechanisms, the assignment of responsibility for accountable and fair use, and the consideration of technological challenges. We further identify use cases that are overlooked in global regulations, as well as global regulatory scenarios that remain unaddressed in local guidelines. These discrepancies reveal opportunities for mutual improvement of policies to achieve more comprehensive coverage and to better prevent potential harmful uses of AI.

% Finally, we observe anecdotal examples of opportunities for the use of machine learning applications in certain domains that fail to account for the current technological state of these models. These findings underscore the need for better alignment between the evolving capabilities of machine learning systems and the policies that govern their deployment. By exploring these issues, we aim to shed light on practical steps for improving AI governance and ensuring the safe, fair, and effective use of AI technologies in different domains. Finally, through this analysis, we highlight critical needs for further technical advancements in AI, particularly with respect to their safe and accountable use.



% However, even within a single domain, such as news organizations, the development of guidelines has often been inconsistent, reflecting diverse interpretations of risk, accountability, and ethical considerations (see Table \ref{table-news}). These inconsistencies are further amplified when comparing guidelines across different domains, exposing gaps in regulatory efforts and highlighting the fragmented nature of current governance frameworks. This fragmentation underscores the difficulty of creating cohesive policies that balance the specific needs of local organizations with broader societal and technological imperatives.


% For the latter, the most significant to develop an \textit{overarching AI governance framework} is the EU AI Act \cite{2024-ai-act} (\aia), which aims to establish comprehensive standards for managing high-risk AI systems. 

%Many crucial details about how global regulations should be adapted to specific domains, contexts, and use cases remain still unspecified or inadequately addressed. 
% This disconnect between high-level legislation and localized practices creates a regulatory gray area, where organizations are left to interpret and create organisational policies independently, often leading to inconsistencies. %Moreover, it remains uncertain whether the challenges addressed in organizational policies are comprehensively reflected in overarching frameworks. 




%We identify significant divergences in guidelines, reflecting the unique needs and interpretations of each domain, as well as gaps in the policies developed within each domain (\S\ref{sec:discusssion}). We uncover areas where guidelines diverge significantly, reflecting the unique needs and interpretations of each domain as well as gaps in the policies developed by each domain. These findings reveal shared challenges, such as the need for clear accountability mechanisms and the importance of addressing biases in AI outputs. They also reveal domain-specific issues, such as the role of AI in personalized learning within universities or content generation in newsrooms. By identifying these dynamics, our study provides actionable insights that can inform both the refinement of local guidelines and the evolution of international legislation.\looseness=-1


%we identify anecdotal references to AI use cases that reveal misunderstandings about the current state of these technologies (\S\ref{sec:challenges}), further complicating their integration into organizational workflows. This underscores the importance of aligning governance efforts with the actual performance and limitations of AI.  Misaligned expectations not only hinder the responsible deployment of AI but also risk undermining trust in this technology. 

%By bridging the gap between local practices and global frameworks, this paper contributes to the development of AI governance that would be both practical and robust across diverse organizational contexts.