\subsection{Sociotechnical Challenges for Organisations Known from AI Research}
\label{sec:challenges}
% \subsection{Sociotechnological challenges known from research on Generative AI}

\input{table-challenges}
%Generative models are currently rushed into many applications and have been adopted by many users. This poses a range of sociotechnical challenges, stemming from the technical imperfections of these models, legal uncertainty, questions about the authorship in ``AI-assisted'' content, misleading marketing, and the issues related to how people use such models. This work does not aim to provide a comprehensive survey, but we list some of the key issues previously identified in academic literature in \autoref{fig:sociotech-challenges}, together with the possible risks to organizations whose employees rely on this technology.\looseness=-1
% \subsection{Implications from Policies for Research}
%
AI research literature point to numerous sociotechnical challenges for organisations relying on the current AI technology. Our work does not aim to provide a comprehensive survey, but we list the issues that we identified through literature review in \autoref{fig:sociotech-challenges}, together with the possible risks to organisations whose employees rely on this technology. This list serves as background to the types of problems that organisational policy or regulatory frameworks may identify as issues that need addressing.


%The advancements in generative artificial intelligence (GenAI) models, particularly through the pre-training of large-scale architectures with extensive parameters on vast datasets, have significantly enhanced their capabilities across a broad range of domains. In particular, these models demonstrate promising strengths in contextual understanding; text, image, and video generation; multilingual applications; and zero-shot or few-shot adaptability to new tasks. Such abilities have facilitated their widespread adoption in diverse areas.

%Nevertheless, GenAI models still exhibit limited capabilities in certain tasks, especially those that demand deeper expertise or domain-specific knowledge \cite{rein2023gpqa}. While ongoing developments can improve such limited capabilities and make GenAI models applicable to an ever-growing set of tasks, their adoption also brings considerable challenges and risks. Addressing these issues is essential to ensure alignment with existing active policies (e.g., those within educational institutions or news organizations) and international regulations (e.g., the EU AI Act). Below, we outline the critical challenges associated with the use of GenAI models, as identified by existing research and which should be considered in the development of comprehensive and responsible policies and regulations.

%\paragraph{Inaccuracy and Hallucination}
%One of the most pressing challenges of GenAI models is their tendency to generate inaccurate or ``hallucinated'' content, where outputs are not grounded in training data or established facts \cite{liu-etal-2023-evaluating,10.1145/3703155}. Because their pre-training corpora often come from automatically scraped web sources--many of which include fabricated, outdated, previously generated, or biased information models can produce responses that \textit{conflict with real-world knowledge, user-provided input, or even their own previously generated content}. This issue poses particular risks in domains that demand high accuracy, such as legal practices, where considering the latest changes in the law is of critical importance \cite{cheong2024not}. Moreover, GenAI models may offer \textit{plausible-sounding but false information}, making it difficult for both automated systems and human reviewers to detect these subtle errors \cite{zhang2023sirenssongaiocean}. Furthermore, models often \textit{struggle to classify whether a question is within their scope of knowledge}, whereas even smaller or open-source models perform near random on such tasks \cite{amayuelas-etal-2024-knowledge}. Additionally, the reinforcement learning from human feedback (RLHF) technique \cite{ouyang2022training} for aligning model responses with human preferences, the vague knowledge boundary \cite{ren2024investigatingfactualknowledgeboundary}, and the inherently black-box nature of many GenAI models \cite{sun2022black} further complicate the detection, explanation, and mitigation of hallucinations. While plugins or vector databases that store validated, up-to-date domain information (e.g., legal statutes) could alleviate these issues, their integration must be both reliable and user-friendly. Finally, as GenAI models are expected to excel in multi-task, multi-lingual, and multi-domain environments \cite{bang-etal-2023-multitask}, evaluating and mitigating their hallucinations becomes even more challenging.

%\paragraph{Safety and Alignment}
%The \textit{misalignment between the training objectives of GenAI models and their desired behaviour} poses significant safety risks. For instance, while a large language model (LLM) may be optimized to predict the next word from a massive corpus--regardless of the content quality of that corpus--users typically want the model to generate factual, useful information rather than harmful or false content. This gap underscores the challenge of ``scalable oversight'', where evaluating and controlling highly capable models in complex tasks remains unsolved  \cite{amodei2016concreteproblemsaisafety,leike2017aisafetygridworlds}. 
% More importantly, simply enumerating known or planned use cases of foundation models is not sufficient to capture the full range of ways they might be deployed. 
%Techniques such as RLHF have been introduced to align models more closely with human values, yet these methods face fundamental limitations \cite{casper2023open} such as increased hallucination, ideological favouritism, sycophancy, or even resistance being shut down. In addition, they may inadvertently incentivize ``reward hacking'', in which the model learns to exploit the reward system to achieve high scores without actually solving the desired objectives \cite{10.1093/polsoc/puae020}. These risks are unintentional but become more concerning if the alignment process is corrupted by adversarial actors. Overall, there is concern that the alignment of GenAI models could instead lead to prioritizing simple engagement metrics at the expense of broader societal or consumer well-being.

%\paragraph{Security and Privacy Concerns}
%Generative models raise significant security and privacy issues \cite{iqbal2024llm, yao2024survey}. Interactions with large language models (LLMs) often \textit{lack privacy protections}, making their contents vulnerable to subsequent discovery or adversarial exploitation. Even when an LLM operates locally, chat records typically remain unprotected unless explicitly shielded from disclosure.
%Moreover, the training data for these models often comes from uncurated web sources \textit{susceptible to malicious ``poisoning''} \cite{10.5555/3042573.3042761}, potentially yielding harmful outputs. For instance, adversaries may inject hateful speech into just a few online posts to manipulate a foundation model’s training data, and even small-scale injections can significantly corrupt outputs \cite{schuster2021you}. Compounding these risks, \textit{the dual-use nature of LLMs allows them to be adapted for malicious purposes}, such as disinformation campaigns or targeted extortion attempts \cite{kaffee-etal-2023-thorny}.
% , and raises concerns about dual use, wherein models are repurposed beyond their originally foreseen tasks, potentially enabling overlearning or adversarial reprogramming \cite{elsayed2018adversarial}. 
% Furthermore, multimodality can expand a model’s attack surface by allowing cross-modal inconsistencies to be exploited, as shown when an apple labeled “iPod” was misclassified by CLIP. 
%Beyond data poisoning, the \textit{confidentiality, integrity, and availability} of LLM-based systems can also be undermined by inference or reconstruction attacks, adversarial examples \cite{biggio2013evasion, szegedy2013intriguing}, and resource-depletion exploits \cite{shumailov2021sponge, hong2021a}.

%\paragraph{Transparency and Accountability}
%The opacity characterising the development and application of generative models creates significant challenges for transparency and accountability. Even when GenAI model weight parameters are made public, \textit{many providers withhold critical information} regarding their training and fine-tuning procedures, preventing effective inspection and regulation of these models \cite{bender2021dangers}. Furthermore, beyond merely determining whether models produce correct outcomes, it is equally important to assess whether they do so for the right reasons. Current efforts to align model outputs with human preferences, for instance, have shown that fake, misleading alignment and sycophantic behaviour can emerge, raising concerns about the motivation and capabilities of GenAI models \cite{greenblatt2024alignmentfakinglargelanguage,vanderweij2024aisandbagginglanguagemodels}. Additionally, models have been found to produce increasingly persuasive yet potentially unfaithful and misleading content compounds the difficulty of explaining its behaviour \cite{rogiers2024persuasion,bommasani2021opportunities}. While models can supply explanations of their decisions, such as chain-of-thought responses \cite{wei2022chain}, these rationales are often found to be unfaithful to the model’s actual internal decision-making \cite{lanham2023measuring,atanasova-etal-2023-faithfulness}. Exploring the underlying reasons behind a model’s outputs thus remains an open research challenge, critical to ensure accountability among model providers fostering transparent usage of GenAI systems and creating a regulatory environment in which scientists, policymakers, and end-users can confidently employ GenAI technologies.

%\paragraph{Biases and Inequity}
% , including cultural and linguistic minorities, neurodivergent individuals, and other nontypical users. 
%GenAI systems can perpetuate biases that can often manifest as stereotypes or attitudes \cite{bolukbasi2016man, nadeem-etal-2021-stereoset, marchiori-manerba-etal-2024-social, stanczak2023quantifying,hutchinson-etal-2020-social}, and which can propagate through downstream models and reinforce misrepresentations in society. Overrepresentation of majority voices is another concern, resulting in the homogenization and the exclusion of minority viewpoints -- an echo chamber effect that may be further exacerbated if reliance on generative AI becomes widespread \cite{sharma2024generative}. Furthermore, studies have shown that current large language models (LLMs) often overlook certain demographic groups, such as individuals aged 65+ or widowed, even when explicitly steered to represent them \cite{santurkar2023whose}. 
% Notably, technical vulnerabilities, such as ``jailbreak'' attacks, reveal the fragility of safety training protocols, suggesting that merely scaling up these methods without fundamentally revising optimization objectives may exacerbate mismatched generalization. 
%Mitigating these inequities and biases requires (1) ensuring equitable access and skill development for using AI responsibly, (2) advancing technical research to reliably attribute and address the root sources of bias, and (3) implementing effective safeguards so that generative AI promotes fairness rather than reinforcing discrimination. Finally,  GenAI systems risk exacerbating existing inequities by privileging those with greater access and technical skills required to use GenAI models, thereby widening the divide between privileged and underprivileged populations.



%\paragraph{Known GenAI Limitations} Recent evaluations of GenAI models highlight a range of shortcomings that \textit{limit their performance in real-world scenarios}. Although LLMs can generate ideas that sometimes surpass human experts in perceived novelty, their \textit{creativity largely remains combinatorial rather than genuinely groundbreaking, often lacking diversity, feasibility, and depth} \cite{si2024can, padmakumar2024does}. LLM models further exhibit worse creativity skills in languages different from English \cite{marco-etal-2024-pron}. Additionally, exposure to LLM assistance in creative tasks has been found to lead to decreased creativity and diversity of ideas in subsequent non-assisted tasks \cite{kumar2024humancreativityagellms}. The latter reflects an increasing concern regarding the over-reliance on GenAI, leading to the erosion of critical thinking, specialized skills, and homogenisation of individual voices, especially in educational settings \cite{zhai2024effects}.
%LLMs face additional technical limitations. They exhibit limited robustness to distribution shifts, such as evolving world knowledge or language drift, which jeopardises the reliability of their outputs in high-stakes environments (e.g., in legal or medical applications) \cite{yuan2023revisiting}. Furthermore, LLMs have been increasingly used by being assignment personas, which has been found to unexpectedly skew their performance and introduce biases, highlighting broader concerns about stereotyping and privacy and personalisation applications of GenAI \cite{zhang2024personalization}. Existing evaluations also reveal subpar performance, among others, in abstract reasoning tasks \cite{gendron2023large}, event semantics \cite{tao2023eveval}, non-Latin script contexts \cite{lai-etal-2023-chatgpt}, and multimodal data. LLMs are also highly sensitive to adversarial or even slight prompt variations \cite{zhu2023promptbench}, calling for further research into prompt engineering and model robustness. Overall, while LLMs offer considerable promise, substantial work remains to address these limitations, among others, and improve their performance, particularly regarding creative potential, interpretative flexibility, and ethical concerns.

%The challenges and limitations outlined above underscore the complexity of aligning generative models with both local and global guidelines. 