\section{AI Policies in University Education}
\label{sec:policies:uni}

% \begin{table*}[ht]
% \centering
% \begin{tabular}{lcccccccccc}
% \toprule
%                     & TUM & TUD & KTH           & Aalto & DTU  & KUL  & ETH            & CUNI & Vie  & UiO    \\
% \midrule
% course design       & \use & -   & -             & \use  & \use & -    & \use           & \use & -    & \use   \\
% assessment          & -    & \use& -             & -     & -    & -    & \lock  & \use & -    & \use   \\
% self-learning       & \use & \use& \lock & \use  & \use & -    & -              & -    & \use & \use   \\
% personalisation     & \use & -   & -             & \use  & \use & -    & \use           & -    & \use & \use   \\
% feedback            & -    & \use& -             & \use  & -    & -    & \use           & -    & \use & \use   \\
% coding              & \use & \use& -             & \use  & -    & \use & -              & \use & -    & \use   \\
% gen. images         & -    & -   & -             & -     & -    & \use & \risk          & \use & -    & -      \\
% language tool       & -    & \use& \lock & \use  & -    & \use & \use           & -    & -    & -      \\
% ideation            & -    & \use& -             & \use  & -    & \use & \use           & -    & -    & \use   \\
% topic knowledge     & -    & \use& \use          & \use  & -    & \use & -              & -    & \use & \use   \\
% summarisation       & -    & \use& -             & \use  & -    & \use & -              & \use & \use & -      \\
% translation         & -    & -   & -             & \use  & -    & -    & -              & \use & \risk& \use   \\
% search              & -    & -   & -             & -     & -    & \use & -              & \use & -    & \rules \\
% \bottomrule
% \end{tabular}
% \caption{Suggested \textit{uses of AI systems} in teaching and learning activities. For each element, we denote whether the corresponding guidelines have suggested using AI for its purposes (\use), have enforced restrictions on the use of AI for it (\emoji{lock}), have issued warnings regarding the use of AI for it (\risk), or has issued official rules regarding the use of AI for it (\rules).}
% \label{table-uni-uses}
% \end{table*}


% \begin{table*}[ht]
% \begin{tabular}{llllllllllllllllllll}
% \toprule
% \rotatebox{90}{Institution} & \rotatebox{90}{knowledge cut-off} & \rotatebox{90}{persuasiveness} & \rotatebox{90}{factual accuracy} & \rotatebox{90}{no prioritisation} & \rotatebox{90}{source attribution} & \rotatebox{90}{assess skills} & \rotatebox{90}{not learning} & \rotatebox{90}{privacy} & \rotatebox{90}{copywright} & \rotatebox{90}{biased output} & \rotatebox{90}{plagiarism} & \rotatebox{90}{digital inequity} & \rotatebox{90}{environment} & \rotatebox{90}{exploited labor} & \rotatebox{90}{owner power} & \rotatebox{90}{declaration of use} \\
% \midrule
% TUM & \risk & \risk & \risk & \risk & \risk & \risk & - & \risk & - & \risk & \risk & - & - & - & - & - \\
% TUD & - & - & - & - & - & \risk & - & \risk & - & - & \risk & \risk & \risk & \risk & \risk & \risk/\rules \\
% KTH & - & - & \risk & \risk & - & - & - & - & - & - & - & - & - & - & - & \rules \\
% Aalto & \risk & \risk & \risk & - & \risk & - & - & \risk/\rules & - & \risk & - & - & - & - & - & \rules \\
% DTU & - & - & \risk & - & - & - & - & \risk/\rules & \risk & \risk & - & - & - & - & - & \rules \\
% KUL & - & - & \risk & \risk & - & - & - & \risk/\rules & \risk/\rules & \risk & - & - & \risk & - & - & \rules \\
% ETH & - & - & \risk & - & \risk & \risk & - & \rules & \rules & \risk & - & - & - & - & - & \rules \\
% CUNI & - & - & - & - & - & - & - & \risk/\rules & \rules & - & - & \risk & - & - & - & \rules \\
% Vie & - & - & \risk & \risk/\rules & - & - & \risk & \risk & \risk/\rules & - & \risk & - & - & - & - & \rules \\
% % UdL & - & - & - & - & - & - & - & - & \risk & - & - & \rules & - & - & - & - \\
% UiO & \risk & \risk & \risk & \risk & - & \risk & \risk & \rules & \risk/\rules & - & - & - & \risk & - & - & - \\
% \bottomrule
% \end{tabular}
% \caption{Issued \textit{warnings and rules regarding the use of AI systems} in teaching and learning activities. For each element, we denote whether the corresponding guidelines have enforced restrictions on the use of AI for it (\emoji{lock}), have issued warnings regarding the use of AI for it (\risk), or have issued official rules regarding the use of AI for it (\rules).}
% \label{table-uni-risks-rules}
% \end{table*}

\begin{table*}[!t]
\centering
\footnotesize
\begin{tabular}{lcccccccccc}
\toprule
 \textbf{Code} & \textbf{UiO} & \textbf{Aalto} & \textbf{TUD} & \textbf{KUL} & \textbf{ETH} & \textbf{TUM} & \textbf{Vie} & \textbf{CUNI} & \textbf{DTU} & \textbf{KTH} \\
\midrule
\multicolumn{11}{c}{\textbf{Suggested Uses}} \\
Self-learning      & \use & \use & \use & -    & -    & \use & \use & -    & \use & \lock \\ 
Course design      & \use & \use & -    & -    & \use & \use & -    & \use & \use & -            \\ 
Personalisation    & \use & \use & -    & -    & \use & \use & \use & -    & \use & -            \\
Coding             & \use & \use & \use & \lock & -    & \use & -    & \use & -    & -            \\
Topic knowledge    & \use & \use & \use & \use & -    & -    & \use & -    & -    & \use         \\
Feedback           & \use & \use & \use & -    & \use & -    & \use & -    & -    & -            \\
Language tool      & -    & \use & \use & \use & \use & -    & -    & -    & -    & \lock \\
Ideas           & \use & \use & \use & \use & \use & -    & -    & -    & -    & -            \\
Summarisation      & -    & \use & \use & \use & -    & -    & \use & \use & -    & -            \\
Translation        & \use & \use & -    & -    & -    & -    & \risk& \use & -    & -            \\
Assessment         & \use & -    & \use & -    & \lock & -   & -    & \use & -    & -        \\
Search             & \lock & -   & -    & \use & -    & -    & -    & \use & -    & -            \\
Image generation       & -    & -    & -    & \use & \risk& -    & -    & \use & -    & -            \\

\midrule
\multicolumn{11}{c}{\textbf{Issued Warning and Rules}} \\
Teacher restrictions & \rules & \rules & - & \rules & \rules & \rules & \rules & \rules & - & - \\
AI literacy training & \risk \use      & - & \use & \risk \use & \use & \risk \use & \risk \use & \use & - & -            \\
Human oversight   & \rules     & \rules & - & \rules & \rules & - & \rules & \rules & \rules & \rules \\
Privacy and sensitive data            & \rules      & \risk/\rules & \risk & \risk/\rules & \rules & \risk & \risk & \risk/\rules & \risk/\rules & -            \\
Factual accuracy   & \risk       & \risk    & \risk     & \risk        & \risk  & \risk & \risk & -            & \risk        & \risk        \\
Declaration of use & -           & \rules       & \risk/\rules & \rules & \rules & -     & \rules & \rules & \rules & \rules  \\
Copyright          & \risk/\rules& -            & \risk & \risk/\rules & \rules & \risk     & \risk/\rules & \risk/\rules & \risk & -      \\
Bias in AI      & -           & \risk        & -     & \risk        & \risk  & \risk & -     & -     & \risk & -      \\
No prioritisation  & \risk       & -            & -     & \risk        & -      & \risk & \risk/\rules & -     & -     & \risk  \\
Digital inequity   & \risk       & -            & \risk & \risk        & -      & -     & -     & \risk & -     & -      \\
Knowledge cut-off  & \risk       & \risk        & \risk     & \risk            & -      & \risk & -     & -     & -     & -      \\
Persuasiveness     & \risk       & \risk        & \risk     & -            & -      & \risk & -     & -     & -     & \risk      \\
Source attribution & -           & \risk        & \risk     & -            & \risk  & \risk & \risk     & -     & -     & -      \\
Skills assessment     & \risk       & -            & \risk & -            & \risk  & \risk & -     & -     & -     & -      \\
% Plagiarism         & -           & -            & \risk & -            & -      & \risk & \risk & -     & -     & -      \\
Environment        & \risk       & -            & \risk & \risk        & -      & -     & -     & -     & -     & -      \\
AI over-reliance       & \risk       & -            & -     & -            & -      & -     & \risk & -     & -     & -      \\
% exploited labor    & -           & -            & \risk & -            & -      & -     & -     & -     & -     & -      \\
% owner power        & -           & -            & \risk & -            & -      & -     & -     & -     & -     & -      \\
Teacher load & -           & -            & \risk  & -            & -      & -     & -     & -     & \risk     & -     \\  
\bottomrule
\end{tabular}
\caption{Suggested uses, and issued \textit{warnings and rules regarding the use of AI systems} in teaching and learning activities. For each element, we denote whether the corresponding guidelines have suggested using AI for its purposes (\use), have enforced restrictions on the use of AI for it (\lock), have issued warnings regarding the use of AI (\risk), or have issued official rules regarding the use of AI(\rules). See \autoref{tab:coding} for the meaning of individual codes.}
\label{table-uni}
\end{table*}

% how many give the teacher the opportunity to decide on the use
% how many mention it's a moving field
Table \ref{table-uni} presents a summary of the suggested use cases of AI in teaching and learning as well as the risks and rules enacted in the corresponding institutions. The table presents the points raised by at least two universities, we also discuss points raised by individual universities in the following section.

\subsection{Suggested Uses}
% In contrast, KTH enforces limitations to using AI systems for self-learning by not directly asking the AI to generate specific answers or complete their assignments.

The suggested uses of AI systems in education are framed from two primary perspectives: that of the teacher and the student. The most commonly proposed application is for \textbf{self-learning}, mentioned in seven organisational guidelines. Examples of self-learning activities supported by AI systems include providing students with learning materials and resources, assisting in planning and monitoring their learning, encouraging exploration of covered topics (TUM), and offering alternative perspectives (Aalto).

From the student perspective, AI systems are also frequently viewed as tools for enabling \textbf{personalised learning}, e.g. by recommending individualised learning plans, presenting material with explanations of varying difficulty levels, and enhancing accessibility for students with disabilities (ETH). Additionally, AI is suggested as highly effective for \textbf{coding tasks}, such as understanding concepts, breaking down problems into smaller parts, practising debugging skills, and receiving feedback on code. University guidelines further note that students can leverage AI to \textbf{gain an initial overview of a topic}, terms, or concepts. Moreover, AI can help students monitor their progress and \textbf{provide targeted feedback} on written work or ideas.

AI systems are also commonly suggested as \textbf{language tools}, particularly for grammar, spelling, and reference checks—often considered the safest and most permissible use of AI. For example, KTH identifies this as one of the three approved uses of AI, typically not requiring documentation in the declaration of use. Additional proposed applications for students include \textbf{idea generation, summarising academic literature, translation, student assignment assessment, search engine functionality, and image generation}. However, regarding translation, Vie cautions that AI may produce inaccuracies, particularly with new terminology or less common language combinations.

From the teacher's perspective, suggested uses of AI extend to \textbf{course design} activities, such as formulating learning objectives, drafting rubrics, planning workshops, designing assignments, creating questions, preparing courses, and even simulating a test student. Regarding assessment, ETH explicitly disallows the fully automated grading of student work, but most universities grant teachers the discretion to decide whether \textbf{AI use is restricted} in assignments and exams. \looseness=-1

While many universities provide lists of potential AI applications in teaching and learning, some impose restrictions on those applications. For instance, KTH limits AI usage to predefined prompts and prohibits directly asking the system to generate specific answers or complete assignments. Similarly, KUL restricts the use of AI in coding tasks to generating components of larger assignments, and only if explicitly approved by the teacher.

\subsection{Issued Warnings and Rules}
The most common warnings and rules in university policies regarding AI include considerations of \textbf{privacy} and \textbf{sensitive data}, \textbf{copyright}, \textbf{factual accuracy}, and the \textbf{declaration of use}. While some universities encourage students and teachers to consider privacy and copyright concerns and warn of potential violations, others enforce strict rules regarding the types of data that can be input into AI systems to prevent such issues. For example, Aalto specifies that teachers may only submit student work to university-approved systems and prohibits entering other students' answers or personal information into external systems. To support these policies, Aalto, along with DTU, KUL, and ETH, provides access to Microsoft Copilot for both teachers and students, which is meant to ensure that submitted data is not stored or used for future model training.
%
Regarding copyright, universities caution that AI can reproduce copyrighted material without proper acknowledgement. They also require users to avoid inputting proprietary information, such as the university’s intellectual property, into AI tools.

Other warnings address risks associated with the quality of generated content, including \textbf{biases} in the content, \textbf{lack of prioritisation} of arguments, \textbf{absence of source attribution} (making verification of accuracy, plagiarism, etc., difficult), \textbf{limited knowledge due to cut-off dates}, and a \textbf{persuasive tone} that can mislead users about the correctness of the information. Risks also arise in educational scenarios where AI integration might \textbf{necessitate course reorganisation or additional assessments} to ensure learning objectives are met and student skills are accurately evaluated. AI usage may also contribute to \textbf{digital inequity}, stemming from disparities in access to paid versus free tools and variations in the quality of generated content based on user skills. \textbf{Over-reliance on AI tools} is another identified risk. \looseness=-1

KUL explicitly highlights the lack of reproducibility as a concern, noting that output can vary between attempts. UiO is unique in warning that AI can produce inappropriate or offensive content. KUL also advises against ``humanising'' AI, emphasising that it is merely a tool.

Many universities stress the \textbf{importance of asking the right questions} and not settling for the first answer provided by AI. To support this, they offer guides for crafting effective prompts, and some institutions even provide dedicated courses on this topic. Teachers are encouraged to \textbf{set clear restrictions on AI use} within their courses and must communicate allowed uses transparently. Finally, most guidelines underscore that both teachers and students remain \textbf{fully responsible} for the content they incorporate into their work, regardless of AI use.


% All universities emphasize the importance of academic honesty, particularly regarding AI tools' potential to generate content that students, teachers, and researchers might pass off as their own. All universities classify the unacknowledged use of AI in assessments as a form of cheating and underscore the need for students to cite AI-assisted work, aiming to uphold standards of academic integrity. Universities acknowledge that there are no fully effective tools to detect the use of generative AI, which underlines the importance of academic integrity and the ethical academic standards of behaviour that are expected of all members of the academic community.

% AI as a supportive teaching tool is highlighted across institutions. Universities like \textit{Aalto} and \textit{University of Lisbon} encourage AI tools to support personalized learning and assist with activities like summarizing, idea generation, and coding assistance. \textit{ETH Zurich} focuses on AI to facilitate accessibility, particularly for students with disabilities, showing how AI can diversify educational support.

% Data privacy and security are central concerns, especially regarding sensitive student and university information. \textit{University of Oslo} and \textit{KU Leuven} explicitly restrict the use of personal data in AI tools and advise students to use incognito modes or avoid uploading sensitive content to maintain data security.

% Universities have mixed approaches to AI use in assessments. \textit{Technical University of Munich} and \textit{University College Dublin} present two main models: prohibiting AI in exams versus integrating it with a focus on transparency, critical engagement, and sometimes even oral evaluations.

% From the selected universities, guidelines regarding the use of AI in academia were available mostly in English, either for universities in English speaking countries or for universities that had programs in English. These universities had further courses to educate teachers in the use of AI for teaching and learning. For other universities there were no guidelines in place in the respective language (e.g., ). This indicates that countries where the AI tools would have lower performance due to lower training resources for non-English languages, there is also less guidance on the responsible use of AI and the increased risk of potential flaws in the respective languages.

% While many institutions point the use of AI for personalising the learning process both from the student and teacher perspective, universities where English is not the main language, like the University of Barcelona, emphasize that AI tools lack the cultural or learning situation contextualization necessary for personalisation of the learning experience.

% \textit{Charles University} in the Czech Republic emphasizes critical engagement, encouraging students to balance AI use with independent skill development. Conversely, \textit{University of Lisbon} endorses AI as a creative brainstorming aid but requires “declarations of honor” for assignments where AI is used, underscoring an ethical approach to its adoption.

% Unique to \textit{Imperial College} is an emphasis on the environmental and social impacts of AI, addressing concerns about the carbon footprint and potential biases from training data. This perspective aligns with a growing trend in academia to consider AI's broader ethical implications.

% Overall, the responsible use of AI and the responsibility regarding the generated texts are assigned to the AI user. Teachers are also assigned the responsibility to educate the students regarding the principles of academic integrity.

% \subsection{Further Technological Challenges}
% We find the following technological challenges not addressed in the reviewed guidelines. They are concerned with the suitability of LLMs for evaluation, which is a critical aspect of the educational process. Existing work finds that the current state-of-the-art LLM may not be suitable to serve as LLM TAs for all tasks.

% \paragraph{Unfit for Assessment} \citet{chiang-etal-2024-large} recently conducted a course with AI as an evaluator. Students involved in the survey reported that the LLM did not correctly follow the instructions for the output format (51.3\%), did not properly follow the evaluation criteria (21.5\%), yielding scores too low or too high, and yielded different ratings for the same assignment due to the randomness of LLM decoding. The output of LLMs is also known to be brittle to small changes in the input, even though the meaning of the text remains unchanged \cite{}. Finally, the authors advocate for transparency of the prompts used for automated evaluation, which was not specifically pointed across the reviewed university guidelines.

% \paragraph{Fooling LLM Assessment} 
% Guidelines have also to outline that students are prone to try different manipulation techniques that would fool the LLM to evaluate them with a high score, e.g., by goal hijacking \cite{}, where the malicious prompt aims to make the LLM generate the highest score or by using known vulnerabilities of LLMs such as that LLMs prefer longer responses \cite{}, assertive language \cite{}, etc. This is further completed by the known bias of LLMs used for evaluation to assign higher scores to text generated by the same model \cite{}. This can further introduce assessment inequality for students that, e.g., have access to some closed/paid models used also for assessment.
