\section{AI Policies in News Organisations}
\label{sec:policies:news}
%
\begin{table*}[!t]
\footnotesize
\centering
\begin{tabular}{lcccccccccc}
\toprule
\textbf{Code} & \textbf{FT} & \textbf{ANP} & \textbf{Guardian} & \textbf{Parisien} & \textbf{Spiegel} & \textbf{SZ} & \textbf{BBC} & \textbf{Mediahuis} & \textbf{Ringier} & \textbf{VG} \\
\midrule
\multicolumn{11}{c}{\textbf{Suggested Uses}} \\
Illustrations/Graphics &  \use &   \use &        - &        \use &       \use &  \use &   - &         - &       - &  \use \\
Image generation &  \rules &   \use &        - &        \use &       \use &  - &   - &         - &       - &  \rules \\
Article generation &  \rules &   \use &        - &        \rules &       \use &  \rules &   - &         - &       - &  - \\
Data analysis &  \use &   - &        \use &        - &       - &  \use &   - &         - &       - &  - \\
Language tool &  - &   \use &        \use &        \use &       - &  - &   - &         - &       - &  - \\
Transcription/Translation &  \use &   \use &        - &        - &       - &  \use &   - &         - &       - &  - \\
Ideas (Content) &  - &   \use &        - &        - &       - &  - &   - &         - &       - &  - \\
Ideas (Marketing) &  - &   - &        \use &        - &       - &  - &   - &         - &       - &  - \\
Content Moderation &  - &   - &        - &        - &       - &  \use &   - &         - &       - &  - \\
\midrule
\multicolumn{11}{c}{\textbf{Issued Warning and Rules}} \\
Human oversight &  \rules &   \rules &        \rules &        \rules &       \rules &  \rules &   \rules &         \rules &       \rules &  \rules \\
Declaration of use &  \rules &   - &        \rules &        \rules &       \rules &  \rules &   \rules &         \rules &       \rules &  \rules \\
Factual accuracy & \risk &  \risk &       \risk &        - &      \risk &  - &   \rules &         - &      \risk &  - \\
Bias in AI & \risk &  \risk &        \rules &        - &       - &  - &   - &         \rules &      \risk &  - \\
Privacy and sensitive data &  - &   - &        - &        - &       \rules &  - &   \rules &         \rules &       \rules &  \rules \\
Copyright &  - &   - &        \rules &       \risk &       - &  - &   \rules &         \rules &      \risk &  - \\
AI literacy training &  \rules &   - &        - &        - &       - &  - &   \rules &         \rules &       - &  - \\
Document AI use &  \rules &   - &        - &        - &       \rules &  - &   \rules &         - &       - &  - \\
\bottomrule
\end{tabular}

\caption{Suggested uses, and issued \textit{warnings and rules regarding the use of AI systems} in news organisations. For each element, we denote whether the corresponding guidelines have suggested using AI for its purposes (\use) (with the requirement for human oversight in most cases); have issued warnings regarding the use of AI (\risk), or have issued official rules regarding the use of AI(\rules). See \autoref{tab:coding} for the meaning of the individual codes.}
\label{table-news}
\end{table*}
%
We describe the policies of 10 news organisations, as summarised in Table~\ref{table-news} towards their suggested uses of AI and issued warnings and rules w.r.t. AI use in the newsroom.
%
\subsection{Suggested Uses}
All news outlets surveyed encourage the use of AI in their work; however, they propose different degrees of use and applications. Where in Table~\ref{table-news} outlets do not mention any of the listed codes, they still encourage AI usage but do not explicitly list suggested uses in their policy.
%
Further, all news outlets require human oversight for all or most AI use as well as labelling the output as AI generated.

The use of AI in news organisations has two directions. First, tooling to help automated processes in the day to day work of journalists such as \textbf{data analysis}, \textbf{language tools}, e.g., grammar correction, and \textbf{transcription and translation} of, e.g., interviews. %For these applications there have been automated tools for a long time, but with the improvements of AI technology, the news organisations seem to embrace the AI tools specific for these applications. 
Here, there seems to be very little differences in policies -- either, these use cases are mentioned in the policies as allowed or encouraged, or they are not explicitly mentioned, none of the news organisations forbids this type of AI use. In a similar vein, Süddeutsche Zeitung (SZ) explicitly mention the use of AI for \textbf{content moderation}, showing the wide range of possible AI application in news organisations.
%
The second set of proposed use cases of AI in news organisations is around the content of the published news, i.e., the use of AI for content production. Here policies diverge.
%
Interestingly, while many news organisations explicitly allow the use of AI for the generation of \textbf{illustrations/graphics}, Financial Times (FT) and VG prohibit the use of AI for \textbf{image generation}, i.e., the creation and publication of photorealistic images whereas ANP, Le Parisien, and Der Spiegel allow use for image generation.
%
Likewise, policies differ for \textbf{article generation}, i.e., the use of AI to create full or long parts of texts for news articles. While ANP and Der Spiegel allow article creation under human supervision, Financial Times (FT), Le Parisien, and Süddeutsche Zeitung (SZ) explicitly state that their articles are exclusively written by humans.
%
In the realm of ideation, only ANP encourages the use of AI to support journalists in finding ideas on headlines, articles, and even sources. The Guardian does not mention idea generation for news content, but they do encourage the use of AI for marketing campaign ideas. \looseness=-1
%

%AI has a significant influence on the news environment. While the emphasis on human oversight and labeling of generated content is a positive step, establishing common policies across media outlets is equally crucial. Standardised guidelines can help readers better understand AI's role in news production, fostering transparency and reducing distrust that may arise from assumptions about AI involvement.
%
\subsection{Issued Warning and Rules}

All news organisations mandate human involvement, emphasising \textbf{human oversight} before any generated content is used or published. Outlets like ANP and Der Spiegel require explicit editorial approval for publishing generated content, while BBC and ANP highlight that AI is a supportive tool rather than an independent generator.
%
Further, all news organisations agree on the \textbf{declaration of use}, setting standards about the transparency of AI use with policies mandating clear labeling of generated content.

Topics of broader society implications, such as \textbf{factual accuracy} and \textbf{bias in AI}, are often mentioned across the news organisations' policies, but few specific rules are proposed.
%
Generated text may contain factual inaccuracies (see \autoref{fig:sociotech-challenges}). Many of the news organisations warn about this, but only the BBC addresses this issue by requiring that generated content is assessed for ``accuracy and reliability''.
%
Similarly, inherent bias of AI systems is mentioned across multiple news organisations' policies, with only The Guardian and Mediahuis proposing concrete measures. The Guardian discusses the need to ``guard against the danger of bias'' in both AI models and their training data, while Mediahuis more generally ``watch out for biases in AI systems and work to address them'', emphasising the need to balance the different objectives of journalists, commercial interests, and the audience.

For journalists, sensitive data handling can be a crucial issue. Four of the news organisations have explicit policies on handling \textbf{privacy and sensitive data}. These include the ban of entering into AI systems sensitive data of, as Ringier states in their policy, ``journalistic sources, employees, customers or business partners or other natural persons''. For the BBC, this extends to using AI systems that respect privacy rights.

Given the issues with copyrighted texts used for AI training (see \autoref{fig:sociotech-challenges}), \textbf{copyright} is a central topic for news organisations. Five out of ten analysed news organisations' policies address this topic. 
%
Rigier only warns about copyright infringement in the context of entering programming code whereas Le Parisien only warns about copyright infringement in the context of publishing AI generated images. 
%
Unique to The Guardian is the explicit commitment to fair compensation for data creators whose works contribute to AI models, showcasing a progressive view on data usage and ownership rights.

\textbf{AI literacy training} can better equip journalists to deal with AI. Mediahuis, BBC, and Financial Times (FT) support internal training, fostering newsroom awareness and responsible AI deployment. Those trainings include ensuring accountability for AI development and use, train and qualify staff responsible for AI decision, and newsroom awareness (Mediahuis); clear communication on where AI is used, what data is collected and how it works, and affects both staff and audience (BBC); training on the use of AI for story discovery (FT).       

Some news organisations' policies advocate for documenting all instances of AI usage and experimentation. Der Spiegel uses this information to ``exchange information within the company, with other media and with other partners''. The BBC calls for ``clear accountability'' for the use of AI more generally. However, only the Financial Times (FT) explicitly mentions an internal register to track AI use, highlighting an additional layer of accountability not seen in other policies. %This approach to documentation aligns closely with transparency goals under the EU AI Act.
