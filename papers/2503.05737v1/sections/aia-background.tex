\subsection{EU AI Act}
\label{sec:aia}
Likewise, the scope of this work does not allow for a detailed discussion of \aia, but for our purposes a key factor is that it implements a risk-based approach to regulating AI, in which systems are categorised systems by their potential threats. AI systems deemed ``unacceptable'' for their \textit{potential risks to EU values and fundamental rights}, such as AI systems performing social scoring, are outright banned. Systems considered ``high-risk'', including those used in critical infrastructure, law enforcement, employment, and education, face stringent ex-ante rules and are also subject to post-market monitoring. Universities, one of our case policy case studies, are in the education sector. Perhaps surprisingly, news is not identified as a high-risk application, given its possible consequences in election cycles. For general- and minimal-risk AI, the \aia primarily relies on transparency obligations, requiring that users be informed when they are interacting with or viewing outputs from certain AI systems.

There is also an important distinction between ``providers'' and ``deployers'', each bearing distinct responsibilities. Providers are entities that develop an AI model, place it on the market under their name or trademark (e.g., `Llama 3' by Meta), or substantially modify an existing AI model. They must ensure the modelâ€™s compliance with relevant standards, document design choices and data governance procedures, and, where required, undergo conformity assessments. If the model is a general purpose AI model (as Llama), it falls under additional regulations. Deployers, on the other hand, are the organisations or individuals that integrate and use the AI models in their systems (e.g. a university that creates an AI system using Llama to provide access to it to its staff and students). Their obligations typically focus on correct implementation, ensuring that the system is used within the scope of its intended purposes and monitoring its real-world performance for safety, accuracy, and potential harm.
%
In the case of universities, they can be both deployers of AI systems, if they offer their own AI systems, as well as end-users, subscribing to other AI deployers services, such as `ChatGPT'. 
%
For news organisations, in a majority of cases, the policies assume that they subscribe to an external AI deployer's systems.
%
Downstream users and large-scale distributors of AI generated content, as could be the case for news organisations, do not currently have obligations under the \aia.
%
In this article, we focus on the \aia and exclude discussions about related EU regulation, such as the \textit{Directive on Copyright in the Digital Single Market}.

