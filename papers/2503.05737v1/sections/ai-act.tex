We now put the above findings from organisational policies in the perspective of EU AI Act \cite{2024-ai-act} (\aia). We recognise that these efforts towards AI governance are fundamentally different in scope and process through which they were created, and they are complementary. However, given the ongoing effort\footnote{\url{https://artificialintelligenceact.eu/ai-act-implementation-next-steps/}} to develop a more specific implementation guidelines for \aia, we believe that the insights from the bottom-up policies could help to identify areas where more clarity would be appreciated.

%While the organisational policies cannot be used to compare directly with the \aia, they serve as a good start to inform possible improvements that can be made to the legislation in upcoming iterations based on their existing experience and help point out possible points of confusion for practitioners applying the \aia in their domain.
%

\subsection{News Organisations' Policies}
%\textcolor{red}{TODO: Check which risk category news things fall under!}
%\textcolor{red}{TODO: Reference to the selling of news outlet data as training data}
\textit{\textbf{Similarities.}}
News organisations' policies align with the provisions introduced by the \aia for example on the emphasises on \textit{human oversight} with the \aia Article 14.
%
Article 50 of the \aia describes transparency obligations for providers and deployers, aligning with the requirements to \textit{disclose AI-generated content} by news organisations.
%
For high-risk AI systems, Article 10 of the \aia regulates \textit{data governance}, a question that also is relevant to downstream users such as journalists. In the context of the \aia this is limited to training, validation, and test data, whereas for news organisations the question of input of sensitive data into AI systems is crucial to preserve privacy of possible sources, reflecting the broader GDPR compliance required under the \aia.
%
%
%The proposal of documenting and sharing , similar to the technical documentation the \aia, Article 11 requires for high-risk systems.\looseness=-1

\textit{\textbf{Gaps.}}
The news' AI policies cover several points not covered by the \aia. In particular, The Guardian emphasises \textit{compensating those whose data is used for AI}, while the \aia lacks explicit provisions for data creators' compensation outside of existing copyright regulations. 
%
%
Financial Times and Mediahuis include newsroom \textit{AI training} in their policies. The EU Act currently only requires AI literacy training for developers and deployers of AI systems. If news organisations as downstream users of these systems are not considered as deployers, this requirement will not cover this type of distribution of AI-generated content. 
%
Multiple policies require addressing \textit{bias in the AI systems} used by journalists, a topic that is yet to be comprehensively covered by legislation.
%
The \textit{internal AI usage register policy} by Financial Times is an additional documentation practice not specified by the Act but useful for accountability. It could enable retroactive checks on which content was created with which AI system and where the systems where used, which would improve transparency and accountability of this outlet, and hence potentially increase trust in it. 

\subsection{University Education's Policies}
%
\textit{\textbf{Similarities.}}
The university AI guidelines align with several provisions in the \aia, particularly in their focus on \textit{transparency, human oversight, and data privacy}. Concerns about privacy and data protection, emphasised by universities like KUL and the UiO, align with Article 10 of the \aia, which mandates data governance and safeguards for personal data used for training, testing, and validation in AI systems. Universities advise students and teachers against uploading sensitive data to AI tools, reflecting the broader GDPR compliance required under the \aia.\looseness=-1

The \textit{human oversight of AI} in decision-making, particularly in \textit{assessments and exams}, is echoed in Article 14 of the \aia, which mandates human monitoring of AI decision-making processes in high-risk AI systems. Some universities, such as the TUM, prohibit AI in exams, while others integrate AI tools under strict human oversight, ensuring that AI does not replace independent student evaluation.

The \aia defines education as one of the high-risk areas for AI applications in Article 6 Annex III, enforcing \textit{bias mitigation and fairness obligations}. Universities also raise concerns about AI’s potential to reinforce grading biases and manipulation tactics in AI-driven assessments (e.g., response length bias, goal hijacking). Similarly, universities explore AI-driven adaptive learning and dropout risk prediction (though the latter is clearly a high-risk application that comes with extra considerations for the trade-off between improved performance of Transformer-based systems and the difficulty of regulatory compliance \cite{NielsenRaaschou-PedersenEtAl_2024_Trading_off_performance_and_human_oversight_in_algorithmic_policy_evidence_from_Danish_college_admissions}).

\textit{\textbf{Gaps.}}
University AI policies introduce measures that are either absent or not explicitly covered by the \aia. For example, institutions like UiO, TUD, and KUL highlight the \textit{environmental impact} of AI, focusing on sustainability and ethical AI use. 
Furthermore, university AI policies highlight the risk of increasing \textit{digital inequity} among students, which stems both from disparities in access to paid versus free tools and variations in the quality of generated content based on user skills. The \aia, while regulating AI safety and robustness, does not directly address AI’s carbon footprint, resource consumption, or exacerbated digital inequity.

While the \aia, Article 4 mandates \textit{AI literacy training }for developers and deployers, university policies extend this responsibility to educators and students. Educators are encouraged to equip students with the skills needed to critically evaluate AI outputs. Additionally, both educators and students are provided with resources to learn how to craft effective AI inputs to achieve optimal results. This proactive approach in universities contrasts with the \aia’s narrower focus on professional AI users rather than general AI literacy. %\looseness=-1
%These issues remain largely unaddressed in AI regulation but are critical for ensuring fairness in education.

Overall, while the \aia provides a legal framework for AI safety, transparency, and human oversight, universities take a context-specific approach to AI governance, addressing academic integrity, assessment reliability, and pedagogical challenges in ways that current EU regulation does not yet fully capture.

\subsection{Known research challenges not covered in \aia or organisational policies}
\label{sec:challenges:discuss}

Finally, let us consider the set of sociotechnical challenges that is more broadly known from the existing academic literature (\autoref{fig:sociotech-challenges}), but that we have not found to be addressed in sufficient detail in either organisational policies we considered or \aia:

\textit{\textbf{Definition.}} The organisational policies do not typically define what kind of `AI' is being addressed, and the definition proposed in \aia has been criticized by researchers \cite{Hooker_2024_On_Limitations_of_Compute_Thresholds_as_Governance_Strategy}.

\textit{\textbf{Enforceability.}} Many policies we considered require \textbf{declaration of AI use}, yet there are no robust detection mechanisms to verify compliance \cite{PuccettiRogersEtAl_2024_AI_News_Content_Farms_Are_Easy_to_Make_and_Hard_to_Detect_Case_Study_in_Italian}.

\textit{\textbf{Unsafe outputs.}} Only one university in our sample (UiO) mentioned the possibility of exposing students to inappropriate outputs from AI models.

\textit{\textbf{Misleading marketing claims.}} AI providers are constantly advertising new models claimed to be ever better at `reasoning', `understanding' and other constructs of questionable validity for the current AI \cite{Mitchell_2021_Why_AI_is_Harder_Than_We_Think}. Many policies we examined seem to be influenced by `fear of missing out', manufactured by such narratives. More stringent requirements of transparency for claimed benchmark results could alleviate this problem.

\textit{\textbf{Explainability.}} We saw no policies directly addressing the fact that the current AI systems are not interpretable, which has implications for their use (especially where decisions could have significant consequences, e.g. student grading or news fact-checking).

\textit{\textbf{Brittleness.}} Some university policies mention the need for AI literacy training, but we did not find that in news, and \aia also does not discuss that for the users of AI systems.

\textit{\textbf{Creativity.}} It is possible that over-reliance on AI systems could damage basic competences or creativity of its users, but most policies we examined do not seriously consider this factor.

\textit{\textbf{Carbon emissions.}} Only 3 universities and no news organisations considered this point, and it is not addressed in \aia beside documentation.



%
\section{Policy Recommendations Based on Gaps Identified in This Work}
%
To reiterate, while the \aia and organisational policies from universities and news organisations operate within different scopes and serve distinct purposes, they can still inform each other, and insights from academic research can further identify areas not sufficiently addressed by either efforts. This section lists the areas where we believe further governance and research efforts are needed the most. %The \aia establishes a legal framework for AI governance, focusing on risk-based regulation, transparency, and accountability. In contrast, university and newsroom policies address sector-specific challenges, such as academic integrity, journalistic standards, and AI literacy. These policy areas, shaped by on-the-ground experiences in academia and journalism, provide valuable insights for strengthening AI governance beyond the existing provisions of the \aia.

%Despite these differences, the practical implementation of AI policies in universities and news organisations reveals gaps in the \aia that could be clarified for its implementation. These gaps are not necessarily regulatory oversights but rather areas where organisational policies have evolved to meet real-world challenges that may not yet be fully reflected in AI legislation.
%

\textbf{\textit{Expanding AI literacy training.}} The \aia mandates AI literacy training for developers and deployers (Article 4), but it does not mandate AI literacy for students, teachers, journalists, the general public who generate or interact with AI-generated content, or even the media professionals or other users of AI models distributing their outputs on a large scale. Universities integrate AI literacy into academic policies, requiring students and educators to develop critical engagement with AI tools. Institutions like CUNI make a proactive step in this direction emphasising the education in AI ethics and responsible use. Newsrooms such as Financial Times and Mediahuis provide AI training for journalists, ensuring that AI-generated content is fact-checked and responsibly handled. Such training should include also critical reflection on the real functionality of the current AI models vs the marketing hype. While it is important to keep responsibility with the AI developers and deployers, supporting users on how to approach AI will be critical. %, which would also ideally help to shape the future policies. %The \aia does not mandate AI literacy training for media professionals or other users of AI models distributing their outputs on a large scale.

\textit{\textbf{Policies addressing Digital Inequity.}} The \aia does not explicitly address digital inequity, despite its potential to exacerbate social and economic disparities (e.g. due to unequal access to AI and different quality of the models available for different socio-economic and linguistic groups). %
This is particularly relevant in education, where university AI policies have highlighted concerns about disparities in access to paid vs. free AI tools, as well as differences in the quality of AI-generated content based on user skills. Other concerns include the temptation to use AI `education' as a low-cost solution substituting human teachers for the already underpriviledged groups, and siphoning of public resources to for-profit AI providers instead of building public AI infrastructure. All this requires more thought to develop more equitable education infrastructure and policies that consider socio-economic impact on various population groups. 

\textbf{\textit{Improved Transparency for Generated Content.}} While \aia Article 52 mandates disclosure of AI-generated content, it does not specify how AI-assisted work should be attributed or audited by downstream users, or how the record of AI assistance should be kept. Universities enforce strict AI citation rules, requiring students to disclose AI-generated content to uphold academic integrity. However, there is no standardised framework for disclosing AI use, which could aid AI literacy across sectors. For example, in student submissions (e.g. should it be a brief description, or a full prompt+output? How should the source system be specified?) An interesting practice is the internal AI usage registers in Financial Times, which allows editors to track which articles were AI-assisted. %The general guidelines for downstream distribution of AI output would ideally specify how to indicate where AI assistance (or its amount and nature) was used for the readers of the content.

\textbf{\textit{Getting specific about `bias'.}} Both news and university policies sometimes warn about the possibility of `bias', but it is not clear what kinds of bias should be addressed, or how. This is a gap legislation could address by providing a more comprehensive guidance (e.g. based on existing human rights and non-discrimination laws) for model providers, deployers, as well as downstream users and content distributors.

\textbf{\textit{Attribution and compensation of sources of AI training data.}} EU has copyright laws, but AI training data poses new challenges currently tested in both US \cite{Vynck_2023_Game_of_Thrones_author_and_others_accuse_ChatGPT_maker_of_theft_in_lawsuit,GrynbaumMac_2023_Times_Sues_OpenAI_and_Microsoft_Over_AI_Use_of_Copyrighted_Work} and in Europe\cite{Smith_2024_GEMA_Files_Copyright_Lawsuit_Against_OpenAI_in_Germany}. In our sample, only The Guardian advocates for compensation of content creators whose data is used as part of AI training. %, and there are creator groups\footnote{\url{https://www.thedpa.ai/}} advocating for licensing, compensation and transparency. 
In education, an equally important factor is source attribution, without which the students could be unwittingly plagiarising existing scholarly work. %This has plagiarism implications for students using AI-generated summaries or analyses based on uncredited scholarly work. 
%Given the huge implications for jobs and social norms, EU position on this should be clear. 
The question of data governance and compensation should be further investigated, taking into account concepts such as data collectives \cite{DBLP:conf/cscw/HsiehZKRDMGEZ24}.


\textbf{\textit{Disclosing Environmental Impact of AI}} The Act does not explicitly address the carbon footprint of AI models besides documentation, despite researchers' concerns about large-scale computational demands \cite{dodge2022measuring,bouza2023estimate,luccioniCountingCarbonSurvey2023,liMakingAILess2023}. Some universities and news outlets highlight the environmental costs of training and running large models, yet there are no regulatory incentives to optimise for sustainability. One ongoing research direction is developing more efficient models \cite{trevisoEfficientMethodsNatural2023}, but if the more efficient models just get used more, this will not help. Mandating a visible disclosure to the users of how much water and energy each AI query costs, and where the resources come from, could help to discourage excessive use. Organisations could also have AI use by their employees as a part of their sustainability reporting. % Legislative incentives for the responsible use of AI  

\textbf{\textit{Detection and enforceability.}} There are currently no reliable methods of detecting generated text, which makes any policies unenforceable. A promising solution is watermarking \cite{jiang2024watermark,roman2024proactive}, but the providers of commercial LLMs have no incentive to provide a mechanism that could reduce the usage of their services \cite{davisOpenAIWontWatermark2024,gloaguenBlackBoxDetectionLanguage2024}. This is where the considerations of social impact \cite{solaiman2023evaluating} should guide regulation mandating such transparency from the popular AI service providers. %An exchange between practitioners, policy makers, and downstream users can ensure that the right approach is chosen to fulfil all requirements.

\textbf{\textit{Clarifying liability.}} In compliance with \aia, providers of AI models may attempt to build in ``safeguards'' to avoid e.g. toxic outputs, and they will have to pass some evaluations to put the model on the market. But should something go wrong, e.g. seriously impacting the mental health of the user, it is currently not clear how much legal recourse the affected users would have. The question of AI liability \cite{Liability_Rules_for_Artificial_Intelligence_European_Commission} will get more pressing with the amount of cases that pose the question of responsibility for the consequences of AI use \cite{awfulai}.
% 

Finally, we would list the following public-interest areas with potential regulatory significance, which need much more research: detection of synthetic content, model interpretability, source attribution to training data, and long-term effects of AI `assistance' on productivity and skills of the workforce. Besides the above suggestions for regulation, these directions should be among priority areas for academic research funding allocation, as the incentives for working on them are just not present in industry.