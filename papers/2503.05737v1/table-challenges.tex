\begin{table}[t]
%\footnotesize
\begin{tabular}{p{11.7cm}p{2.7cm}}
\toprule
\textbf{Challenge} \& \textbf{Summary} & \textbf{Risk for the org.} \\
\midrule 
\textbf{What is regulated:} what kind of models even fall under the policy? Definitions can be based on training compute \cite{2024-ai-act}, data \cite{RogersLuccioni_2024_Position_Key_Claims_in_LLM_Research_Have_Long_Tail_of_Footnotes}, performance \cite{anderljungFrontierAIRegulation2023} etc. & Guidelines not scoped appropriately \\
\textbf{Detectability \& enforceability}: can we detect when AI models' usage violates the policy? Particularly, when generated content is used without disclosure? At present, no \cite{PuccettiRogersEtAl_2024_AI_News_Content_Farms_Are_Easy_to_Make_and_Hard_to_Detect_Case_Study_in_Italian}. & Guidelines not enforceable \\
\textbf{Factual errors}: the current models cannot reliably reject queries for which they do not have enough information \cite{amayuelas-etal-2024-knowledge}, and may output plausible-sounding but false results that are hard to identify and check \cite{zhang2023sirenssongaiocean,hicksChatgptBullshit2024}. Retrieval-augmented generation still has this problem \cite{mehrotraPerplexityBullshitMachine}. & Losing credibility and reputation \\
\textbf{Unsafe models}: in spite of attempts to force the models to follow certain content policies \cite{ouyang2022training}, the models can still violate them \cite{DerczynskiGalinkinEtAl_2024_garak_Framework_for_Security_Probing_Large_Language_Models}, and this training can even decrease the quality in some aspects \cite{10.1093/polsoc/puae020,casper2023open}. & Exposing employees to toxic outputs \\
\textbf{Privacy and security risks}: employees using non-local generative AI models may expose sensitive data from themselves and their organizations to the entity controlling such models \cite{Kim_2023_Amazon_warns_employees_not_to_share_confidential_information_with_ChatGPT_after_seeing_cases_where_its_answer_closely_matches_existing_material_from_inside_company} or third-party attackers \cite{WuZhangEtAl_2024_New_Era_in_LLM_Security_Exploring_Security_Concerns_in_Real-World_LLM-based_Systems}. Platform plugins may also increase vulnerabilities \cite{iqbal2024llm}. & Exposing sensitive data \\
\textbf{Misleading marketing claims}:  %With the training data too-big-to-inspect \cite{bender2021dangers}, the benchmark results may be compromised by test set contamination \cite{RogersLuccioni_2024_Position_Key_Claims_in_LLM_Research_Have_Long_Tail_of_Footnotes}. %But based on public benchmark numbers and especially claims of ``emergence'' \cite{RogersLuccioni_2024_Position_Key_Claims_in_LLM_Research_Have_Long_Tail_of_Footnotes}, 
employees may believe the claims of AI  model ``capabilities'' and trust the machine too much \cite{KheraSimonEtAl_2023_Automation_Bias_and_Assistive_AI_Risk_of_Harm_From_AI-Driven_Clinical_Decision_Support}, even though the benchmark results may be compromised by methodological problems and test set contamination \cite{RogersLuccioni_2024_Position_Key_Claims_in_LLM_Research_Have_Long_Tail_of_Footnotes}. %Automation also tends to make easy tasks easier and harder tasks harder \cite{SimkuteTankelevitchEtAl_Ironies_of_Generative_AI_Understanding_and_Mitigating_Productivity_Loss_in_Human-AI_Interaction}, and . 
& Degradation in the outputs of the organization \\
\textbf{Transparency \& accountability}: the social and legal norms on disclosing AI ``assistance'' and taking responsibility for the resulting text have not yet settled. The popular providers of these models do not accept responsibility for any faults in the output\footnote{}. & Public blame for any missteps \\
\textbf{Bias \& inequity}: The social biases in AI systems are well-documented  \cite{bolukbasi2016man,nadeem-etal-2021-stereoset,marchiori-manerba-etal-2024-social,stanczak2023quantifying,hutchinson-etal-2020-social,bender2021dangers,sharma2024generative}, %LLM training data may overrepresent some groups and underrepresent others \cite{bender2021dangers,sharma2024generative}. 
and the use of such models may reinforce misrepresentations in the society. & Discrimination, ethical code violation \\%, and the resulting models may fail to adequately represent minorities even when explicitly steered to do so \cite{santurkar2023whose}. \\ 
\textbf{Explainability}: checking model outputs would be easier if they were accompanied by rationales, the current interpretability methods are not faithful to the modelâ€™s actual decision-making \cite{lanham2023measuring,atanasova-etal-2023-faithfulness}. & Trusting unreliable solutions \\
\textbf{Brittleness}: Generative models perform worse outside of their training distribution \cite{McCoyYaoEtAl_2024_Embers_of_autoregression_show_how_large_language_models_are_shaped_by_problem_they_are_trained_to_solve,McCoyYaoEtAl_2024_When_language_model_is_optimized_for_reasoning_does_it_still_show_embers_of_autoregression_analysis_of_OpenAI_o1}. For language models, this includes changes in both language (idiolects, dialects, diachronic changes), content (e.g. evolving world knowledge), and slight variations in prompt formulation and examples \cite{zhu2023promptbench,LuBartoloEtAl_2022_Fantastically_Ordered_Prompts_and_Where_to_Find_Them_Overcoming_Few-Shot_Prompt_Order_Sensitivity}. & Employees wasting time and/or getting poor results \\
\textbf{Risks to creativity}: AI systems may generate unseen sequences of words, but their ``creativity'' is combinatorial, often lacking diversity, feasibility, and depth \cite{si2024can,padmakumar2024does}, and further degraded in languages other than English \cite{marco-etal-2024-pron}. Exposure to AI assistance could \textit{decrease} human creativity and diversity of ideas in non-assisted tasks \cite{kumar2024humancreativityagellms}. & Degradation in the outputs of the organization and its existing human resources \\
\textbf{Credit \& Attribution}: AI systems are commonly trained on copyrighted texts\cite{Gray_2024_OpenAI_Claims_it_is_Impossible_to_Train_AI_Without_Using_Copyrighted_Content} without author consent%, and memorization of high-quality human-authored texts is known to correlate with model performance \cite{liangHolisticEvaluationLanguage2022}
. This practice triggered multiple lawsuits  \cite{BrittainBrittain_2023_Lawsuits_accuse_AI_content_creators_of_misusing_copyrighted_work,Vynck_2023_Game_of_Thrones_author_and_others_accuse_ChatGPT_maker_of_theft_in_lawsuit,JosephSaveriLawFirmButterick_2022_GitHub_Copilot_investigation,GrynbaumMac_2023_Times_Sues_OpenAI_and_Microsoft_Over_AI_Use_of_Copyrighted_Work,Panwar_2025_Generative_AI_and_Copyright_Issues_Globally_ANI_Media_OpenAI_TechPolicyPress}, protests from the creators \cite{Heikkila_2022_This_artist_is_dominating_AI-generated_art_And_hes_not_happy_about_it,More_than_15000_Authors_Sign_Authors_Guild_Letter_Calling_on_AI_Industry_Leaders_to_Protect_Writers}, and questions about the credit for the author of an ``assisted'' text \cite{FormosaBankinsEtAl_2024_Can_ChatGPT_be_author_Generative_AI_creative_writing_assistance_and_perceptions_of_authorship_creatorship_responsibility_and_disclosure}. & Legal exposure, violating plagiarism policies/principles \\
\textbf{Carbon emissions:} The current AI systems are environmentally costly for both training and inference \cite{luccioniCountingCarbonSurvey2023,dodge2022measuring,bouza2023estimate,liMakingAILess2023}, and workflows that significantly rely on them would have adverse effect on climate action. & Not meeting sustainability goals\\
\bottomrule
\end{tabular}
\caption{Major sociotechnical challenges for organizations relying on the current AI technology}
\label{fig:sociotech-challenges}
\end{table}