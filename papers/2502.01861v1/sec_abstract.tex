\begin{abstract}
  When training large flexible models, practitioners often rely on grid search to select hyperparameters that control over-fitting.
  This grid search has several disadvantages: the search is computationally expensive, requires carving out a validation set that reduces the available data for training, and requires users to specify candidate values.
  In this paper, we propose an alternative: directly learning regularization hyperparameters on the full training set via the \emph{evidence lower bound} (``ELBo'') objective from variational methods.
  For deep neural networks with millions of parameters, we recommend a modified ELBo that upweights the influence of the data likelihood relative to the prior.
  Our proposed technique overcomes all three disadvantages of grid search.
  In a case study on transfer learning of image classifiers, we show how our method reduces the 88+ hour grid search of past work to under 3 hours while delivering comparable accuracy.
  We further demonstrate how our approach enables efficient yet accurate approximations of Gaussian processes with \emph{learnable} length-scale kernels.
\end{abstract}
