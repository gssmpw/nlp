\section{Discussion and Conclusion}

We proposed an alternative to grid search: directly learning hyperparameters that control model complexity on the full training set via model selection techniques based on the ELBo.
We showed that a modified ELBo that upweights the influence of the data likelihood relative to the prior improves model selection with limited training data or an overparameterized model.
Experiments for transfer learning examine 3 architectures (ResNet, ViT, and ConvNexT) and 3 distinct datasets (CIFAR-10, Pet-37, and Flower-102), each at several sizes. 
Results showed our DE ELBo achieves heldout accuracy comparable to existing approaches with far less compute time.

\textbf{Learning hyperparameters lets practitioners focus on other aspects that could improve accuracy more.}
Our DE ELBo reduces compute time by directly learning hyperparameters. With dozens of hours of saved time, practitioners can focus on other efforts that improve target task performance, such as better data augmentation or trying other pretrained weights.
For example, on Pet-37 $N=370$, we found that L2-zero using weights pre-trained with supervised learning on ImageNet resulted in a gain of 32.4 percentage points in accuracy over an alternative \emph{self-supervised} pretraining (see Tab.~\ref{tab:acc}).

Our DE ELBo would benefit from further exploration of how $\kappa$ values impact model performance.
Beyond saving practioners valuable time, we hope our work sparks interest in theoretical understanding of how modified ELBos can form a compelling basis for hyperparameter learning.

\section*{Acknowledgments}
Authors EH and MCH gratefully acknowledge support in part from the Alzheimerâ€™s Drug Discovery Foundation and the National Institutes of Health (grant \# 1R01NS134859-01). MCH is also supported in part by the U.S. National Science Foundation (NSF) via grant IIS \# 2338962. We are thankful for computing infrastructure support provided by Research Technology Services at Tufts University, with hardware funded in part by NSF award OAC CC* \# 2018149. We would like to thank Tim G. J. Rudner for helpful comments on an earlier draft of this paper.

\section*{Impact Statement}
This paper presents work whose goal is to advance the field of Machine Learning.
%There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.
Our work has the potential to reduce the energy consumption required for machine learning research and applications.
