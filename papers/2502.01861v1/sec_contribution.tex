\section{Contributions}
\label{sec:contributions}

\subsection{Problem setup: ELBo for hyperparameter selection}

Consider a generic template of a probabilistic model for observed data $\{y_i \}_{i=1}^N$, governed by high-dimensional parameters $\theta \in \mathbb{R}^D$. We assume a typical factorization structure:
\begin{align}
    p_{\eta}( y_{1:N}, \theta) = p_{\eta}(\theta ) \cdot  \textstyle \prod_{i=1}^N p_{\eta}( y_i | \theta).
\end{align}
This template is instantiated by specifying a concrete likelihood $p_{\eta}(y_i | \theta)$ and prior $p_{\eta}(\theta)$. The subscript of each indicates possible dependence on hyperparameter vector $\eta$.

The Bayesian approach to fit this model to data involves estimating the posterior distribution $p_{\eta}( \theta | y_{1:N})$, yet this is typically intractable. Instead, a popular approach is to pursue an approximate posterior via variational methods~\citep{blei2017variational,jordan1999introduction}. We first select an ``easy-to-use'' family of distributions $q_r(\theta)$ over the parameter $\theta$. Each concrete value of parameter $r$ defines a specific distribution over $\theta$. We then pose an optimization problem: find the variational parameter $r$ that makes $q_r(\theta)$ as close as possible to the true (intractable) posterior. % in Kullback-Leibler (KL) divergence.

A tractable way to do this is to estimate $r$ by maximizing an objective function known as the \emph{evidence lower bound} (``ELBo'', \citep{blei2017variational}), denoted as $J_{\text{ELBo}}$ and defined for our model $p$ and approximate posterior $q$ as $\JELBO :=$
\begin{align}
    \label{eq:elbo_objective_generic}
    \mathbb{E}_{q_{r}(\theta)} \left[ \sum_{i=1}^{N} \log p_{\eta}(y_i | \theta) \right] - \mathbb{KL}(q_{r}(\theta) \| p_{\eta}(\theta)).
\end{align}
This objective is a function of data $y_{1:N}$, variational parameters $r$, and hyperparameters $\eta$. Maximizing $\JELBO$ can be shown equivalent to finding the specific $q$ that is ``closest'' to the true posterior in the sense of Kullback-Leibler (KL) divergence. Further, as the name of the objective suggests, we can show mathematically that $\JELBO( y_{1:N}, r, \eta ) \leq \log \int_{\theta} p_{\eta}( y_{1:N}, \theta) d\theta$. That is, the ELBo is a lower bound on the log marginal likelihood or \emph{evidence}. The evidence is itself a function of $\eta$.

The bound relation underlying the ELBo suggests its potential utility for data-driven selection of hyperparameters $\eta$. 
Indeed, past work has used the ELBo to select hyperparameters in several contexts~\citep{uedaBayesianModelSearch2002,damianouDeepGaussianProcesses2013}.
Recent theoretical results~\citep{cherief2019consistency} argue that using ELBo for model selection enjoys strong theoretical guarantees on quality, even under misspecification.
Unfortunately, practical efforts remain dominated by grid search ~\citep{osawa2019practical,shwartz2022pre} rather than gradient-of-ELBo learning of hyperparameters.

In a similar line of work, \citet{immer2021scalable} use Laplace's approximation to estimate the marginal likelihood for hyperparameter selection.
While effective, this approach relies on expensive approximations of the Hessian, which are at least $\mathcal{O}(D^2N)$, and an inner loop to learn hyperparameters.

\subsection{Contribution: Data-emphasized ELBo}

Consider applications of very flexible models where dataset size $N$ is limited relative to the parameter size $D$.
In such cases, where $D \gg N$, classical arguments from statistical learning hardly favor large models due to the risk of overfitting; despite this, well-trained flexible models often enjoy practical success~\citep{sharif2014cnn,xuhong2018explicit}.
Our work offers two core contributions to improve hyperparameter tuning in such applications. 

First, we point out that straightforward use of the ELBo in the $D \gg N$ regime can be \emph{overly conservative} in hyperparameter selection. Selected hyperparameters often prefer simpler models with far worse performance at the target prediction task. 
See our own brief demo in Fig.~\ref{fig:elbo_comparison}, as well as our two later case studies.
This finding is corroborated by \citet{blundell2015weight}, who reportedly tried to learn hyperparameters of Bayesian neural nets via gradients of the ELBo, but found it ``not be useful, and yield worse results.''

Second, we suggest a modified ELBo objective that can overcome this issue by better emphasizing the data likelihood. Concretely, we suggest this \emph{data-emphasized} ELBo objective $J_{\text{DE ELBo}} :=$
\begin{align}
    \label{eq:de_elbo_objective}
    \kappa \cdot \mathbb{E}_{q_r(\theta)} \left[ \sum_{i=1}^{N} \log p_{\eta}(y_i | \theta) \right] 
    - \mathbb{KL}(q_r(\theta) \| p_{\eta}(\theta) ).
\end{align}
where we have introduced a scaling factor $\kappa$ on the likelihood term. 
When using the standard ELBo ($\kappa = 1$) in our target applications, we find the KL term comparing distributions over the high-dimensional random variable $\theta$ dominates the overall objective. By setting $\kappa > 1$, our approach emphasizes data fit more and overcomes this imbalance.
Concretely, in all our case studies we recommend setting $\kappa = D/N$ to achieve an improved balance between the expected likelihood and KL terms.

This objective with varying $\kappa$ has been used in previous work on \emph{tempering} the variational ELBo~\citep{mandt2016variational,aitchison2021statistical,pitas2024fine}. This past work was not directly motivated by concerns about hyperparameter tuning as we are. Instead, they sought to improve the posterior's downstream generalization performance or counter misspecification in the probabilistic model. See Sec.~\ref{sec:related_work} for a thorough discussion.

\textbf{Justification.} Beyond later empirical success across two case studies, we offer two arguments suggesting this revised objective as suitable for hyperparameter selection.
One justification that applies to all models in our framework views this approach as acting as if we are modeling $\kappa N$ \emph{i.i.d.} data instances, and we just happen to observe $\kappa$ copies of the size-$N$ dataset $y_{1:N}$. Under these assumptions, the ELBo of this replicated data is exactly the DE ELBo of the original data. Another justification applies only when each $y_i$ is a discrete random variable, such as a 1-of-C class label. In such cases, the log likelihood upweighted by $\kappa > 1$ is a log PMF and thus always negative ($\mathbb{E}_{q}[\log p( y_i | \theta)] \leq 0$).
Thus, the function in Eq.~\eqref{eq:de_elbo_objective} is still a valid lower bound on the evidence $\log p( y_{1:N} )$. While the bound may be ``loose'' in an absolute sense, what matters more is which hyperparameter values $\eta$ it favors.

\input{fig_elbo_comparison}

\textbf{Demo: Improved selection with $\kappa=D/N$.}
In Fig.~\ref{fig:elbo_comparison}, we compare $\kappa=1$ (left panel) and $\kappa=D/N$ (right) for models defined later in Case Study B: deep neural net classifiers originally trained on ImageNet that are transfered to the CIFAR-10 classification task. In each plot, we compare two possible $q$, colored \emph{pink} and \emph{purple}, across a range of $\eta$ values. The pink version fixes neural net parameters and variational parameters to a solution favored by ELBo, with known test-set accuracy 28.6\%. The purple version instead optimizes these parameters to other values via the \emph{data-emphasized ELBo}, with known test-set accuracy of 87.3\%. We see the poor-accuracy model is strongly favored by the conventional ELBo ($\kappa{=}1$), while our DE ELBo ($\kappa{=}D/N$) prefers the more accurate model.
