\subsection{Evaluation for Model A}

In Fig.~\ref{fig:caseA_regression_classification_demo}, we compare the decision functions of different modeling pipelines on a simple univariate regression dataset inspired by Fig.~2.2 in \citet{rasmussen2006gaussian} (top row) and the \emph{two moons} classification task (bottom).
The goal here is to illustrate sensitivity to hyperparameters $\eta = \{\gamma,\nu,\tau\}$ and the effective learning of $\eta$ enabled by our approach. 
Both dataset sizes are rather small ($N=5$ on top, $N=20$ on bottom).
For all RFFs, we set $R=1024$, so the effective dimension is $D=1024$. %, thus $D \gg N$.

For reference, the last column of the figure shows an ideal GP with hyperparameters tuned to optimize marginal likelihood (exact in the regression case, approximate for classification via a Dirichlet-based GP \citep{milios2018dirichlet}).
The GP is a ``gold standard'' for these toy tasks, yet the GP does not scale well to larger $N$.
While RFF models are in the $D \gg N$ regime we intend for our DE ELBo, the GP's natural formulation is not.

\textbf{Results.}
First, results in columns 1-2 of Fig.~\ref{fig:caseA_regression_classification_demo} show that hyperparameters matter. Past work on RFFs \cite{liu2020simple} recommended $\gamma = \sqrt{20}$, yet we see here that different $\gamma$ are preferred across the top and bottom datasets.

Next, we find the posteriors and hyperparameters estimated with our DE ELBo (column 4) result in better approximations of the gold-standard GP (column 5). Using the conventional ELBo (column 3), the chosen hyperparameters produce decision functions that \emph{underfit}, because the KL term dominates the unweighted likelihood when $D \gg N$.
