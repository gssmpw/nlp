\section{Learning $\lambda, \tau$}
\label{sec:learning_lambda_tau}

In our particular model in Eq.~\eqref{eq:joint_pdf_model}, the KL divergence between two Gaussians \citep{murphy2022Example} simplifies for the backbone KL term as:
\begin{align}
-\mathbb{KL}(q(w) \| p(w)) = -\frac{1}{2} \left[ \frac{\bar{\sigma}^2}{\lambda} \Tr (\Sigma_p^{-1}) + \frac{1}{\lambda} (\mu_p-\bar{w})^T\Sigma_p^{-1}(\mu_p-\bar{w}) - D + \log \left( \frac{\lambda^{D}\det(\Sigma_p)}{\bar{\sigma}^{2D}} \right) \right].
\end{align}

\textbf{Closed-form updates.} To find an optimal $\lambda$ value with respect to the $\JELBO$, notice that of the 3 additive terms in Eq.~\eqref{eq:de_elbo_objective}, only the KL term between $q(w)$ and $p(w)$ involves $\lambda$. We solve for $\lambda$ by taking the gradient of the KL term with respect to $\lambda$, setting to zero, and solving, with assurances of a local maximum of $\JELBO$ via a second derivative test (see App.~\ref{sec:second_derivative_test}). The gradient is
\begin{align}
    \nabla_\lambda -\mathbb{KL}(q(w) \| p(w)) = -\frac{1}{2} \left[ - \frac{\bar{\sigma}^2}{\lambda^2} \Tr(\Sigma_p^{-1}) - \frac{1}{\lambda^2} (\mu_p-\bar{w})^T \Sigma_p^{-1} (\mu_p-\bar{w}) + \frac{D}{\lambda} \right].
\end{align}
Setting $\nabla_\lambda -\mathbb{KL}(q(w) \| p(w)) = 0$ and solving for $\lambda$, we get 
\begin{align}
    \lambda^* = \frac{1}{D} \Big[ \bar{\sigma}^2 \Tr(\Sigma_p^{-1}) + (\mu_p-\bar{w})^T \Sigma_p^{-1} (\mu_p-\bar{w}) \Big].    
\end{align}

In our particular model in Eq.~(\ref{eq:joint_pdf_model}), the KL divergence between two Gaussians \citep{murphy2022Example} simplifies for the classifier head KL term as:
\begin{align}
    -\mathbb{KL}(q(V) \| p(V)) = -\frac{1}{2} \left[ \frac{\bar{\sigma}^2}{\tau} D + \frac{1}{\tau} || \text{vec}(\bar{V}) ||_2^2 - D + \log \left( \frac{\tau^{D}}{\bar{\sigma}^{2D}} \right)\right].
\end{align}
\textbf{Closed-form updates}
To find an optimal $\tau$ value with respect to the $\JELBO$, notice that of the 3 additive terms in Eq.~\eqref{eq:de_elbo_objective}, only the KL term between $q(V)$ and $p(V)$ involves $\tau$. We solve for $\tau$ by taking the gradient of the KL term with respect to $\tau$, setting to zero, and solving, with assurances of a local maximum of $\JELBO$ via a second derivative test (see App.~\ref{sec:second_derivative_test}). The gradient is
\begin{align}
    \nabla_\tau -\mathbb{KL}(q(V) \| p(V)) = -\frac{1}{2} \left[ - \frac{\bar{\sigma}^2}{\tau^2} D - \frac{1}{\tau^2} || \text{vec}(\bar{V}) ||_2^2 + \frac{D}{\tau} \right].
\end{align}
Setting $\nabla_\tau -\mathbb{KL}(q(V) \| p(V)) = 0$ and solving for $\tau$, we get 
\begin{align}
    \tau^* = \bar{\sigma}^2 + \frac{1}{D} || \text{vec}(\bar{V}) ||_2^2.
\end{align}
