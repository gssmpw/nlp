\section{Related work adjusting Bayesian objectives}
\label{sec:related_work}

\textbf{Upweighting data.}
The work most similar in spirit to ours also considers objectives that upweight the likelihood term of the ELBo, or equivalently downweight the KL term.
This line of work ____, which refers to such reweighting as \emph{tempering}, pursues variational methods for deep neural net classifiers in order to reap the benefits of modeling uncertainty well.  Yet these works miss the opportunity to use the modified ELBo to \emph{learn} hyperparameters efficiently. There is an awareness that it is ``favorable to tune regularization'' ____, yet often only a few candidate values of prior variances are tried in ____ or ____, perhaps due to large costs of each separate run.
____ keep regularization hyperparameters fixed with their tempered ELBo objectives. ____ claim in their supplement to ``optimize the prior variance using the marginal
likelihood'', yet the exact marginal likelihood of neural net classifiers is intractable and their work lacks reproducible details about how this is done or whether it is effective.

\textbf{Downweighting data.}
Other work also under the \emph{tempering} name downweights the likelihood in the ELBo.
____ develop a \emph{variational tempering} method which effectively downweights the likelihood in the ELBo in order to pursue the goal of avoiding local optima in the complex posteriors arising in mixture and topic models. That work also does not learn hyperparameters as we do.

Other approaches have recognized value in raising the likelihood to a power in the context of general Bayesian modeling, under the vocabulary terms of a \emph{power likelihood} ____, \emph{power posterior} ____, or ``Safe'' Bayesian learning____. 
However, these works focus on setting the likelihood power \emph{smaller than one}, with the stated purpose of counter-acting misspecification. Values of $\kappa$ larger than one (amplifying influence of data, as we do) are not considered. Work on $\beta$-variational autoencoders____ recommends upweighting the KL term of the ELBo, again effectively diminishing rather than emphasizing data.

\textbf{Other work.}
Other work in Bayesian deep learning has recommended \emph{cold posteriors} ____, which used multipliers to adjust the ``temperature'' of the entire log posterior, not just the likelihood.

Bayesian Data Reweighting____ learns instance-specific likelihood weights, some of which can be larger than one and thus emphasize data. However, that work's primary motivation is robustness, seeking to downweight the influence of observations that do not match assumptions. The authors discourage letting observations be ``arbitrarily up- or down-weighted''. In work with similar spirit to ours, Power sLDA inflates the likelihood of class label data relative to word data in supervised topic modeling____. However, they 
focused on models of much smaller size $D$ that differ from the neural nets we consider, and did not learn hyperparameters.

\textbf{Naming.}
To avoid potential confusion over which terms are upweighted or downweighted under past work labeled tempering, we prefer to call our objective the \emph{data-emphasized ELBo}. This name focuses on what term we upweight. It also reminds readers what matters to us is an \emph{objective for hyperparameter estimation}. While users could benefit from downstream use of estimated posteriors, we hope a primary use case is for non-Bayesian practitioners who ultimately just want point-estimated weights with good $\eta$. In practice, point-estimated weights often outperform Bayesian posteriors at test-set accuracy____.