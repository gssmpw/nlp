\section{Classification}
\label{sec:classification}

\subsection{Dataset details}
\label{sec:dataset_details}
We include experiments on CIFAR-10 \citep{krizhevsky2010cifar}, Oxford-IIIT Pet \citep{parkhi2012cats}, and Oxford Flower \citep{nilsback2008automated}.
For all experiments, we randomly select $N$ training images, stratifying by class to ensure balanced class frequencies.

We use the same preprocessing steps for all datasets.
For each distinct training set size $N$, we compute the mean and standard deviation of each channel to normalize images.
During fine-tuning we resize the images to $256 \times 256$ pixels, randomly crop images to $224 \times 224$, and randomly flip images horizontal with probability 0.5.
At test time, we resize the images to $256 \times 256$ pixels and center crop to $224 \times 224$.

\subsection{Classifier details}
\label{sec:classifier_details}

We use SGD with a Nesterov momentum parameter of 0.9 and batch size of 128 for optimization.
We train for 6,000 steps using a cosine annealing learning rate \citep{loshchilov2016sgdr}.

For \baseline, we select the initial learning rate from $\{0.1, 0.01, 0.001, 0.0001\}$. 
For linear probing and L2-zero, we select the regularization strength from $\{0.01, 0.001, 0.0001, 1\text{e-}5, 1\text{e-}6, 0.0\}$.
For L2-SP we select $\frac{\alpha}{N}$ from $\{0.01, 0.001, 0.0001, 1\text{e-}5, 1\text{e-}6, 0.0\}$ and $\frac{\beta}{N}$ from $\{0.01, 0.001, 0.0001, 1\text{e-}5, 1\text{e-}6, 0.0\}$.
For PTYL, we select $\lambda$ from 10 logarithmically spaced values between 1e0 to 1e9 and $\frac{1}{\tau N}$ from $\{0.01, 0.001, 0.0001, 1\text{e-}5, 1\text{e-}6, 0.0\}$.

While tuning hyperparameters, we hold out 1/5 of the training set for validation, ensuring balanced class frequencies between sets.

After selecting the optimal hyperparameters from the validation set NLL, we retrain the model using the selected hyperparameters on the combined set of all $N$ images (merging training and validation). 
All results report performance on the task in question's predefined test set.

For DE ELBo, we select the initial learning rate from $\{0.1, 0.01, 0.001, 0.0001\}$ and learn optimal $\lambda, \tau$ values.
