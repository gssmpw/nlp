\input{fig_caseA_regression_classification_demo}

\section{Case Study A: Random Fourier features}

To motivate this first case study, recall the Gaussian process (GP) \citep{rasmussen2006gaussian} as a flexible model for regression or classification.
A common choice for GPs is to select a \emph{radial basis function} (RBF) kernel $k(x, x') = \nu^2 \exp \left(- \frac{\|x - x'\|_2^2}{2\gamma^2} \right)$, where $\gamma > 0$ determines the length-scale and $\nu > 0$ the output-scale. 
This GP model allows decision function complexity to elegantly grow with $N$, the number of training instances.  However, a primary downside to GPs is scalability, with classic fitting algorithms scaling cubically with $N$ \citep{rasmussen2006gaussian}. As a possible remedy, we consider a \emph{random Fourier feature} (RFF) \citep{rahimi2007random} approach, which posits an alternative weight-space model that scales \emph{linearly} in $N$ during fitting yet approximates a GP. The approximation quality increases with a user-controlled size parameter $R$, yet $R$ increases the overall parameter dimension $D$. 

GPs are notoriously sensitive to hyperparameters \citep{rasmussen2006Varying}. RFFs are also sensitive, yet tuning hyperparameters well has been largely overlooked. \citet{rahimi2007random} just fix $\gamma = 1,\nu=1$. When \citet{liu2020simple,liu2023simple} used RFFs to build distance-aware neural net classifiers, they recommend $\gamma = 2$ (though released code sets $\gamma = \sqrt{20}$) and tune $\nu$ on a heldout validation set via expensive search. 

We intend to show how fitting this RFF weight-space model via a data-emphasized ELBo yields effective and affordable hyperparameter learning of $\gamma, \nu$ in the $D \gg N$ regime. Better hyperparameter learning translates to notable gains in downstream prediction quality, better matching the ideal GP on small datasets (as in Fig.~\ref{fig:caseA_regression_classification_demo}) but scaling better for large $N$. Our work here could be used as an efficient (\emph{linear in $N$}) approximation of GPs on large datasets, or as a drop-in modification of \citeauthor{liu2020simple}'s distance-aware neural nets to enable even better downstream performance.

\subsection{Model A definition}
\label{sec:caseA_model}

We assume a training set of $N$ pairs $x_i, y_i$ of feature vector $x_i \in \mathbb{R}^H$ and corresponding class label $y_i \in \{1, 2, \ldots C\}$.
We define a simple linear regression/classification model using RFFs of user-defined size $R$.
First, draw random values that define RFF weights $A \in \mathbb{R}^{H \times R}$ and $b \in \mathbb{R}^R$; once drawn they are fixed throughout training and prediction.
Next, use fixed $A,b$ and a cosine function to non-linearly map each $x_i$ to a representation vector $\phi(x_i) \in \mathbb{R}^{R}$. Formally, these two steps are:
\begin{align}
    A_{h,r} &\sim \mathcal{N}(0, 1),
    ~b_{r} \sim \text{Unif}(0, 2\pi)
    \text{~for all~} r,h. \\
    \phi(x_i) &= \nu \sqrt{\frac{2}{R}} \cos\left(\frac{1}{\gamma} A^T x_i + b\right).
    \label{eq:phi}
\end{align}
To complete the model, a linear layer with weights $V \in \mathbb{R}^{C \times R}$ maps $\phi(x_i)$ to predicted regression targets or logit-probabilities over $C$ classes. 

\textbf{Contribution: RFF for arbitrary length-scale.} 
Our featurization in Eq.~\eqref{eq:phi} generalizes the classic construction of RFFs by \citet{rahimi2007random} to any length-scale $\gamma > 0$ and output-scale $\nu > 0$.
In the appendix, we prove that our RFF construction allows a Monte Carlo approximation of the RBF kernel. That is, for any pair of feature vectors we have $\phi(x_i)^T \phi(x_j) \approx k(x_i, x_j)$, where $k$ is an RBF kernel whose $\gamma,\nu$ values match those used to construct $\phi$ in Eq.~\eqref{eq:phi}. The quality of this approximation increases with $R$, typically $100 < R < 10000$. Past work has proven this when $\gamma{=}1,\nu{=}1$ \citep{rahimi2007random}; however, to the best of our knowledge our use of RFF with an arbitrary $\gamma$ is potentially novel.
We later show how to \emph{learn} $\gamma,\nu$ with substantial practical utility.

\textbf{Point estimation view.} To fit the RFF model to observed data, an  empirical risk minimization strategy would find the value of weights $V$ that minimizes the loss function $L(V) := $
\begin{align}
  \label{eq:rff_loss_with_l2_penalty}
  \sum_{i=1}^N \ell(y_i, V\phi(x_i) ) + \frac{\tau^{-1}}{2} || \text{vec}(V) ||_2 ^2,
\end{align}
where $\ell$ is an appropriate loss function for prediction quality (e.g., mean squared error, cross entropy). The sum-of-squares (L2) penalty on $V$ encourages lower magnitude weights to avoid overfitting on the high-dimensional features. Ultimately, model quality is impacted by 3 key hyperparameters: length-scale $\gamma > 0$, output-scale $\nu > 0$, and L2-penalty strength $\tau > 0$. None of these can be effectively set using the training loss alone.

\textbf{Bayesian view.}
A Bayesian interpretation of the RFF problem assumes a joint probabilistic model $p(V, y_{1:N}) = p(V) \prod_{i} p(y_i | V)$, with factors for the classification case
\begin{align}
    \label{eq:rff_joint_pdf_model}
    p(V) &= \mathcal{N}( \text{vec}(V) | 0, \tau I), \\
    p(y_i | V) &= \text{CatPMF}( y_i | \textsc{sm}( V\phi(x_i) ) ).
\end{align}
For regression, instead of the softmax function $\textsc{sm}$ we'd just model $y_i$ as normally distributed with mean $V \phi(x_i)$. 
In either case, $\tau > 0$ is a hyperparameter controlling under/over-fitting.
%Under this model, each feature $x_i$ is a known fixed value, not a random variable; we leave fixed quantities out of conditioning notation for simplicity.
Maximum a-posteriori (MAP) estimation of $V$ recovers the objective in Eq.~\eqref{eq:rff_loss_with_l2_penalty} when we set $\ell$ to $- \log p(y_i | V)$. To fit this model into our general framework, we have $\theta = \{V\}$ and $\eta = \{\gamma, \nu, \tau\}$.

\subsection{Variational methods for Model A}
\label{sec:caseA_variational}

To apply the general variational recipe described in Sec.~\ref{sec:contributions} to model A, we first select an approximate posterior over parameter $V$. For simplicity, we chose a Normal with unknown mean and isotropic covariance.
\begin{align}
    q(V) &= \mathcal{N}( \text{vec}(V) | \text{vec}(\bar{V}), \bar{\sigma}^2 I).
\end{align}
Here, the free parameters that define $q$ are $r = \{\bar{V}, \bar{\sigma} \}$.

\subsection{Implementation for Model A}

\textbf{Learning $\tau$.}
To learn the prior precision $\tau$, we make use of closed-form expressions for the KL divergence between two Gaussians. We can solve for the optimal $\tau$ given other parameters and hyperparameters (see App.~\ref{sec:learning_lambda_tau}).

\textbf{Learning $\nu,\gamma$.}
To learn other hyperparameters, no such closed-form update exists. We employ first-order gradient descent as the gradient of each ELBo-based objective $J$ is easily computed with respect to $\nu$ and $\gamma$ via automatic differentiation. We reparameterize to handle the positivity constraints with the invertible softplus mapping, so that all gradient descent is done on unconstrained parameters.

\textbf{Training.}
For this case study A, our toy datasets are small enough that we perform gradient descent without minibatching. At each step of our ELBo or DE ELBo methods, we update variational parameters $r$ and hyperparameters $\gamma,\nu$ via a gradient step and then update $\tau$ to its closed-form optimal value. We run for a specified number of iterations, verifying convergence by manual inspection.

\textbf{Competitors.}
We compare to a baseline that performs MAP point-estimation of $V$, with a separate gradient descent run at each candidate $\tau$ configuration in a fixed grid and fix $\gamma,\nu$ to previously recommended values.
Given only small toy data, we pick the $\tau$ that delivers best train-set likelihood.