\section{Case Study B: Transfer learning}

For case study B, we investigate transfer learning of image classifiers using informative priors~\citep{xuhong2018explicit,shwartz2022pre}.

\subsection{Model B definition}

Consider training a neural network classifier composed of two parts. First, a backbone encoder $f$ with weights $w \in \mathbb{R}^D$, non-linearly maps input vector $x_i$ to a representation vector $z_i \in \mathbb{R}^H$ (which includes an ``always one'' feature to handle the need for a bias/intercept). Second, a linear-decision-boundary classifier head with weights $V \in \mathbb{R}^{C \times H}$, leads to probabilistic predictions over $C$ possible classes. We wish to find values of these parameters that produce good classification decisions on a provided \emph{target task} dataset of $N$ pairs $x_i, y_i$ of features $x_i$ and corresponding class labels $y_i \in \{1, 2, \ldots C\}$. For transfer learning, we assume the backbone weights $w$ have been pretrained to high-quality values $\mu$ on a source task.

%\textbf{Related work on Bayesian neural nets.}
%Variational inference for neural networks has a long history, dating back to work by \citet{hinton1993keeping} and \citet{graves2011practical}. Modern efforts include MOdel Priors Extracted from Deterministic DNN (MOPED) \cite{krishnan2019efficient}, which focuses on using informative priors to enable scalable variational inference, not to select regularization hyperparameters.

\textbf{Deep learning view.}
Typical approaches to transfer learning in the deep learning tradition (e.g., baselines in \citet{xuhong2018explicit}) would pursue empirical risk minimization with an L2-penalty on weight magnitudes for regularization, training to minimize the loss function $L(w, V) := $
\begin{align}
  \label{eq:loss_with_l2_penalty}
  \sum_{i=1}^N \ell(y_i, V f_w( x_i) ) + \frac{\alpha}{2} || w ||_2 ^2 + \frac{\beta}{2} || \text{vec}(V) ||_2 ^2 
\end{align}
where $\ell$ represents a cross-entropy loss indicating agreement with the true label $y_i$ (one of $C$ categories), while the L2-penalty on weights $w,V$ encourages their magnitude to \emph{decay} toward zero, and this regularization is thus often referred to as ``weight decay''.
The key hyperparameters $\alpha \geq 0$, $\beta \geq 0$ encode the strength of the L2 penalty, with higher values yielding simpler representations and simpler decision boundaries.
Model training would thus consist of solving $w^*, V^* \gets \arg\!\min_{w,V} L(w,V)$ via stochastic gradient descent, given fixed hyperparameters $\alpha, \beta$. 
In turn, the values of $\alpha,\beta$ would be selected via grid search seeking to optimize $\ell$ or error on a validation set. 

\textbf{Bayesian view.} Bayesian interpretation of this neural classification problem would define a joint probabilistic model $p(w, V, y_{1:N})$ decomposed into factors $p(w)p(V) \prod_{i} p(y_i | w, V)$ defined as:
\begin{align}
    \label{eq:joint_pdf_model}
    p(w) &= \mathcal{N}( w | \mu_p, \lambda \Sigma_p), \\
    p(V) &= \mathcal{N}( \text{vec}(V) | 0, \tau I), \\
    p(y_i | w, V) &= \text{CatPMF}( y_i | \textsc{sm}( V f_w(x_i) ) ).
\end{align}
Here, $\lambda > 0, \tau > 0$ are  hyperparameters controlling under/over-fitting, $\mu_p, \Sigma_p$ represent \emph{a priori} knowledge of the mean and covariance of backbone weights $w$ (see paragraph below), and $\textsc{sm}$ is the softmax function.
% MCH: cut for space
% Note $x_i$ is a known fixed value, not a random variable in the model; we leave such fixed quantities out of probabilistic conditioning notation for simplicity.
Pursing MAP estimation of both $w$ and $V$ recovers the objective in Eq.~\eqref{eq:loss_with_l2_penalty} when we set
$\alpha = \frac{1}{\lambda}, \beta = \frac{1}{\tau}, \mu_p=0, \Sigma_p=I$, and $\ell$ to $- \log p(y_i | w, V)$. To fit this model into our general framework, we have $\theta = \{w, V\}$ and $\eta = \{\lambda, \tau\}$.

\textbf{Need for validation set and grid search.}
Selecting $\alpha,\beta$ (or equivalently $\lambda,\tau$) to directly minimize Eq.~\eqref{eq:loss_with_l2_penalty} on the training set alone is not a coherent way to guard against over-fitting. Regardless of data content or weight parameter values, we would select $\alpha^* = 0,\beta^* = 0$ to minimize $L$ as a function of $\alpha,\beta$ and thus enforce no penalty on weight magnitudes at all. Carving out a validation set for selecting these hyperparameters is thus critical to avoid over-fitting when point estimating $w,V$.

\input{fig_ConvNeXt_Tiny_computational_time_comparison}

\setlength{\tabcolsep}{4pt}
\setlength{\columnsep}{6pt} % Default is 18.06749pt
\begin{wraptable}[7]{R}{117.43875pt}
    \caption{Possible priors.}
    \label{tab:transfer_learning_methods}
    \centering
    \footnotesize
    \begin{tabular}{lcc}
        \hline
        \bfseries Method & \bfseries $p(w)$ & Init. \\
        \hline
        L2-zero & $\mathcal{N}(0, \lambda I)$ & $\mu$ \\
        L2-SP & $\mathcal{N}(\mu, \lambda I)$ & $\mu$ \\
        PTYL & $\mathcal{N}(\mu, \lambda \Sigma)$ & $\mu$ \\
        \hline
    \end{tabular}
\end{wraptable}
\setlength{\tabcolsep}{6pt}
\textbf{Backbone priors.}
Several recent transfer learning approaches correspond to specific settings of the mean and covariance $\mu_p, \Sigma_p$ of the backbone prior $p(w)$. Let $\mu$ represent a specific pretrained vector of backbone weights that performs well on a source task.
Setting $\mu_p{=}0, \Sigma_p{=}I$ recovers a conventional approach to transfer learning, which we call L2-zero, where regularization pushes backbone weights to zero. The pretrained value $\mu$ only informs the initial value of backbone weights $w$ before SGD~\citep{xuhong2018explicit,harvey2024transfer}.  Instead, setting the prior mean as $\mu_p = \mu$ along with $\Sigma_p = I$ recovers \emph{L2 starting point} (L2-SP) regularization~\citep{xuhong2018explicit}, also suggested by \citet{chelba2006adaptation}. Further setting $\Sigma_p$ to the estimated covariance matrix $\Sigma$ of a Gaussian approximation of the posterior over backbone weights for the source task recovers the ``Pre-Train Your Loss'' (PTYL) method~\citep{shwartz2022pre}.

\textbf{Need to specify a search space.}
Selecting $\alpha,\beta$ (or equivalently $\lambda,\tau$) via grid search requires practitioners to specify a grid of candidate values spanning a finite range.
For the PTYL method, the optimal search space for these key hyperparameters is still unclear.
For the same prior and the same datasets, the search space has varied between works: PTYL's creators recommended large values from 1e0 to 1e10~\citep{shwartz2022pre}.
Later works search far smaller values (1e-5 to 1e-3)~\citep{rudner2024finetuning}.

\subsection{Variational methods for Model B}

To apply the general variational recipe described in Sec.~\ref{sec:contributions} to model B, we first select an approximate posterior over parameters $w, V$. For simplicity, we chose a factorized Normal with unknown means and isotropic covariance.
\begin{align}
    q(w, V) &= q(w)q(V), \\
    q(w) &= \mathcal{N}( w | \bar{w}, \bar{\sigma}^2 I), \\
    q(V) &= \mathcal{N}( \text{vec}(V) | \text{vec}(\bar{V}), \bar{\sigma}^2 I).
\end{align}
Here, the free parameters that define $q$ are $r = \{\bar{w}, \bar{V}, \bar{\sigma} \}$. 

To fit to data, we estimate $r$ and $\eta$ values that optimize the ELBo or the DE ELBo. To evaluate the KL term in each expression, we use the closed-form available because both prior and $q$ are Gaussian. To evaluate the expected log likelihood term, we use Monte Carlo averaging of $S$ samples from $q$ \citep{xu2019variance,mohamed2020monte}. We further use the \emph{reparameterization trick} to obtain gradient estimates of $\nabla_r J$ \citep{blundell2015weight}.
We find using \emph{just one sample} per training step is sufficient and most efficient.

\input{tab_computational_time_comparison}
\input{tab_acc_subset}

\subsection{Implementation for Model B}

\textbf{Learning $\lambda,\tau$.}
To learn the prior precisions $\lambda, \tau$, we make use of the closed-form expression for the KL term in our ELBo objectives.
For example, setting $\nabla_\lambda J = 0$ and solving for $\lambda$, we get 
\begin{align}
    \lambda^* = \frac{1}{D} \Big[ \bar{\sigma}^2 \Tr(\Sigma_p^{-1}) + (\mu_p-\bar{w})^T \Sigma_p^{-1} (\mu_p-\bar{w}) \Big]
    \label{eq:lambda_update}
\end{align}
with assurances of a local maximum of $\nabla_\lambda J$ via a second derivative test (see App.~\ref{sec:second_derivative_test}). Similar updates can be derived for $\tau$ (see App.~\ref{sec:learning_lambda_tau}).

\textbf{Training.}
For this case study B, we use stochastic gradient descent (SGD) with batches of 128 to fit to large datasets.
At each step of the algorithm, we update variational parameters $r$ via a gradient step, and then update $\lambda, \tau$ to its closed-form optimal values as in Eq.~\eqref{eq:lambda_update}.
%We run until a specified maximum number of iterations is reached.
Each run of our method and the baseline depends on the adequate selection of learning rate. All runs search over 4 candidate values and select the best according to either the DE ELBo (ours) or validation-set likelihood (baseline).

\textbf{Estimating ELBo.}
Given a fit $q$, we estimate the expected log likelihood term of the ELBo or DE ELBo for final selection decisions by averaging over 10 samples from $q$. 

\textbf{Evaluating accuracy.}
To evaluate classification accuracy for a $q$, we find that just plugging in the estimated posterior mean $\bar{w}, \bar{V}$ to make predictions on heldout images achieves similar accuracy to averaging over 10 posterior samples without the added computational cost.

\textbf{Competitors.}
We compare to a baseline that simply performs MAP point-estimation of $w,V$. This executes a separate SGD run at each candidate $\lambda, \tau$ configuration in a fixed grid (see App.~\ref{sec:classifier_details}).
We select the best according to the validation-set likelihood.
This ``grid search'' baseline is representative of cutting-edge work in transfer learning~\citep{shwartz2022pre,harvey2024transfer}.

Both our method and the MAP baseline can be implemented with any of the 3 settings of the backbone prior in Tab.~\ref{tab:transfer_learning_methods}. For the PTYL method \citep{shwartz2022pre}, we use released code from \citet{harvey2024transfer}, which fixes a key issue in the original implementation so that the learned low-rank covariance $\Sigma_p$ is properly scaled.
We use the Woodbury matrix identity \citep{woodbury1950inverting}, trace properties, and the matrix determinant lemma to compute the trace of the inverse, squared Mahalanobis distance, and log determinant of low-rank covariance matrix $\Sigma_p$ for the KL term. %See App.~\ref{sec:low-rank} for details.
