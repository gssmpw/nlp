\begin{figure*}[!t]
\begin{center}
	\includegraphics[width=\textwidth]{images/regression_classification_demo.pdf}
\begin{subfigure}{1.3246in}
	\subcaption{\makecell{RFF ($\gamma{=}1.0, \nu{=}1.0$)\\\baseline}}
\end{subfigure}
\begin{subfigure}{1.3246in}
	\subcaption{\makecell{RFF ($\gamma{=}\sqrt{20}, \nu{=}1.0$)\\\baseline}}
\end{subfigure}
\begin{subfigure}{1.3246in}
	\subcaption{\makecell{RFF (learned $\gamma,\nu$)\\ELBo}}
\end{subfigure}
\begin{subfigure}{1.3246in}
	\subcaption{\makecell{RFF (learned $\gamma,\nu$)\\DE ELBo (ours)}}
\end{subfigure}
\begin{subfigure}{1.3246in}
	\subcaption{\makecell{GP (learned $\gamma,\nu$)\\Marginal likelihood}}
\end{subfigure}
\end{center}
\vspace{-6pt}
\caption{
\textbf{Case Study A: Demo of hyperparameter sensitivity and selection for RFF models.}
The first four columns use the RFF regression/classification model in Sec.~\ref{sec:caseA_model}, varying estimation and selection techniques.
The last column shows the reference fit of a GP, a gold-standard for this toy data but less scalable.
Columns a and b show MAP point estimation, with fixed $\gamma,\nu$ and grid search (GS) for $\tau$ (separate GD run for each value).
Columns c and d use variational methods, \emph{learning} $\gamma,\nu,\tau$ in one run of GD.
For the regression dataset, we plot the mean function of $y$ over samples from $q$.
Our DE ELBo objective enables the best approximation of the GP.
}%endcaption
\label{fig:caseA_regression_classification_demo}
\end{figure*}