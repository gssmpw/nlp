\section{Introduction}

When training deep neural networks (DNNs) or other flexible models, a significant amount of computational resources are devoted to tuning hyperparameters that control the tradeoff between under- and over-fitting.
One widespread example would be setting the strength multiplier of an additive loss term that penalizes the sum-of-squares of weight parameters.
This is equivalently known in various communities as a \emph{ridge} penalty \citep{hastie2009elements,kobak2020optimal},
\emph{L2 regularization} \citep{murphy2022Regularization}, or \emph{weight decay}~\citep{krogh1991simple,goodfellow2016Regularization}.
A common way to tune such hyperparameters is to hold out a dedicated validation set and grid search for the hyperparameter value that performs best on that set~\citep{raschka2018model,murphy2022Picking}.

While reasonably effective and in widespread use to manage over-fitting, using grid search for hyperparameter selection has three key drawbacks.
First and perhaps most important, the need to train separate models at each possible value in the grid significantly increases computational runtime.
Second, the need to carve out a validation set reduces the amount of available data that can inform model training. This can cause under-fitting, especially when available data has limited size.
Finally, grid search requires a list of candidate values specified in advance, yet ideal values may vary depending on the data and classification task at hand.

We take another approach to hyperparameter selection, inspired by a pragmatic Bayesian perspective. 
Suppose we model observations $\{y_i \}_{i=1}^N$ via a likelihood $p_{\eta}( y_{1:N} | \theta)$, where $\theta$ is a high-dimensional parameter to be estimated. We also assume a prior $p_{\eta}( \theta )$. Both prior and likelihood are controlled by hyperparameters $\eta$ (typically just 1-5 dimensions). To effectively estimate both $\theta$ and $\eta$ from data, we can follow the type-II maximum likelihood recipe: estimate the posterior $p_{\eta}(\theta | y_{1:N})$ while simultaneously directly learning $\eta$ to maximize $p_{\eta}( y_{1:N} ) = \int_{\theta} p_{\eta}( y_{1:N}, \theta) d\theta$. This latter objective $p_{\eta}( y_{1:N} )$ is known as the \emph{marginal likelihood} or \emph{evidence} \cite{lotfi2022bayesian}. The evidence naturally encodes a notion of Occamâ€™s razor, favoring the hyperparameter that leads to the simplest model that fits the data well, while penalizing complex models that over-fit the training data~\citep{jeffreys1939theory,mackay1991bayesian,bishop2006Bayesian}.
Learning $\eta$ to maximize evidence (or equivalently, the logarithm of evidence) via gradient ascent avoids all three issues with grid search: we need only one run of gradient ascent (not separate efforts for each candidate $\eta$ value in a grid), we can use all available data for training without any validation set, and we can explore the full continuous range of $\eta$ values rather than a limited discrete set that must be predefined.

\let\thefootnote\relax\footnotetext{\noindent Code: \url{https://github.com/tufts-ml/data-emphasized-ELBo}}

While elegant in theory, this vision of selecting hyperparameters via maximizing evidence is difficult in practice for most models of interest due to the intractable high-dimensional integral that defines the evidence. For modern deep image classifiers with millions of parameters, computing this integral directly seems insurmountable even for a specific $\eta$, let alone optimizing evidence to select a preferred $\eta$ value. In this work, we use and extend tools from variational Bayesian methods \citep{jordan1999introduction,blei2017variational}, specifically tractable lower bounds on the evidence, to make \emph{learning} hyperparameters for DNNs and other flexible models possible.

Ultimately, we contribute a general framework for learning hyperparameters that control model complexity in just one run of a gradient-based fitting algorithm, avoiding grid search for these values.
We provide two case studies demonstrating success of this framework on distinct models.
While our approach uses Bayesian tools, there are huge gains in runtime possible even for non-Bayesian practitioners who use our objective to learn $\eta$ but then use point-estimated weights $\theta$ on downstream tasks instead of a posterior.
In our transfer learning case study, we show how our approach delivers competitive accuracy while reducing total time for the grid search recommended for recent state-of-the-art methods~\citep{xuhong2018explicit,shwartz2022pre} from over 88 hours to under 3 hours.

%88+ hours for  hours for L2-SP \cite{} and 148 hours for PTYL \cite{shwartz2022pre} (using the grid search ranges recommended by the original authors) to under 3 hours.


%When available data is plentiful, our experiments suggest our approach is competitive in accuracy while reducing total training time from 88 hours for L2-SP \cite{xuhong2018explicit} and 148 hours for PTYL \cite{shwartz2022pre} (using the grid search ranges recommended by the original authors) to under 3 hours.
%When available data is limited, our experiments suggest our approach can be particularly effective in improving accuracy and runtime.
