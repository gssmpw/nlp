\section{Low-Rank $\Sigma_p$}
\label{sec:low-rank}

The PTYL method \citep{shwartz2022pre} uses Stochastic Weight Averaging-Gaussian (SWAG) \citep{maddox2019simple} to approximate the posterior distribution $p(w|\mathcal{D}_S)$ of the source data $\mathcal{D}_S$ with a Gaussian distribution $\mathcal{N}(\mu, \Sigma)$ where $\mu$ is the learned mean and $\Sigma = \frac{1}{2}(\Sigma_{\text{diag}} + \Sigma_{\text{LR}})$ is a representation of a covariance matrix with both diagonal and \emph{low-rank} components.
The LR covariance has the form $\Sigma_{\textrm{LR}} = \frac{1}{K-1} Q Q^T$, where $Q \in \mathbb{R}^{D \times K}$.

We use the Woodbury matrix identity \cite{woodbury1950inverting}, trace properties, and the matrix determinant lemma to compute the trace of the inverse, squared Mahalanobis distance, and log determinant of the low-rank covariance matrix for the KL term.

The trace and log determinant of the low-rank covariance matrix can be calculated once and used during training.
Like in the PTYL method, the squared Mahalanobis distance needs to be re-evaluated every iteration of gradient descent.

\subsection{Trace of the inverse}
We compute the trace of the inverse of the low-rank covariance matrix using the Woodbury matrix identity and trace properties.
\begin{align*}
\Tr (\Sigma_p^{-1}) &= \Tr( (A + UCV )^{-1} ) \\
&= \Tr (A^{-1} - A^{-1}U(C^{-1} + VA^{-1}U)^{-1}VA^{-1}) && \text{Woodbury matrix identity} \\
&= \Tr (A^{-1}) - \Tr(A^{-1}U(C^{-1} + VA^{-1}U)^{-1}VA^{-1}) && \Tr(A+B) = \Tr(A) + \Tr(B) \\
&= \Tr (A^{-1}) - \Tr((C^{-1} + VA^{-1}U)^{-1}VA^{-1}A^{-1}U) && \Tr(AB) = \Tr(BA)
\end{align*}
where $A=\frac{1}{2}\Sigma_{\text{diag}}$, $C = I_K$, $U = \frac{1}{\sqrt{2K-2}}Q$, and $V=\frac{1}{\sqrt{2K-2}}Q^T$.
The last trace property, lets us compute the trace of the inverse of the low-rank covariance matrix without having to store a $D \times D$ covariance matrix.

\subsection{Squared Mahalanobis distance}
We compute the squared Mahalanobis distance $(\mu_p-\bar{w})^T\Sigma_p^{-1}(\mu_p-\bar{w})$ by distributing the mean difference vector into the Woodbury matrix identity.
\begin{align*}
\Sigma_p^{-1} &= (A + UCV )^{-1} \\
&= (A^{-1} - A^{-1}U(C^{-1} + VA^{-1}U)^{-1}VA^{-1}) && \text{Woodbury matrix identity}
\end{align*}

\subsection{Log determinant}
We compute the log determinant of the low-rank covariance matrix using the matrix determinant lemma.
\begin{align*}
\log \det(\Sigma_p) &= \log \det(A + UV) \\
&= \log (\det(I_K + VA^{-1}U) \det(A))  && \text{Matrix determinant lemma}
\end{align*}
