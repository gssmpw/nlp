\section{Related Work}
\label{sec:related_work}

% 近似メンバーシップクエリのためのデータ構造の開発は、長年にわたり研究されてきた分野であり、その中でもBloom filter~\citep{bloom1970space}は最も基本的で広く用いられているデータ構造の一つである。
% Bloom filterは，数回のハッシュ関数の評価と，数個のビットの参照だけで，高速にクエリに回答することができる．
% Bloom filterの回答にはある確率で偽陽性が含まれるが，偽陰性は発生しない．
% すなわち，Bloom filterは，このクエリは「確実に」$\mathcal{S}$に含まれない，という「棄却」が可能である．
% この性質から，Bloom filterはnetwork~\citep{broder2004network, tarkoma2011theory, geravand2013bloom}やdatabase~\citep{chang2008bigtable, goodrich2011invertible, lu2012bloomstore}などの，特にmemory constrained and/or latency sensitive なシナリオで活用されている．
% 近似メンバーシップクエリのデータ構造が必要とするbit数の下限は$n\log_{2}(1/\varepsilon)$であることが知られている, ただし$n$は集合の要素数であり、$\varepsilon$は偽陽性率~\citep{pagh2005optimal}．
% Bloom filterが必要とするbit数は、$\log_{2}(e) \cdot n\log_{2}(1/\varepsilon)$である, i.e., 理論的下界の$\log_{2}(e)$倍,である。
% このメモリ使用量をさらに理論的下限へと近づけるために、カッコーフィルタ~\citep{fan2014cuckoo}、バキュームフィルタ~\citep{wang2019vacuum}、Xorフィルタ~\citep{graf2020xor}、リボンフィルタ~\citep{peter2021ribbon}など、ブルームフィルタの改良版が提案されている。

% Data structures for approximate membership queries have been extensively studied over the past decades. 
Bloom filter~\citep{bloom1970space} is one of the most fundamental and widely used data structures for approximate membership queries. 
Bloom filters can quickly respond to queries by only performing a few hash function evaluations and checking a few bits.
Although Bloom filters may return false positives, they never yield false negatives. This property makes Bloom filters valuable in memory-constrained and latency-sensitive scenarios such as networks~\citep{broder2004network, tarkoma2011theory, geravand2013bloom} and databases~\citep{chang2008bigtable, goodrich2011invertible, lu2012bloomstore}.
The parameters of a Bloom filter, such as the length of the bit array and the number of hash functions, are determined by the number of keys to store and the target false positive rate.
Specifically, for a Bloom filter storing $n$ keys with a false positive rate $\varepsilon$, the required bit array length is $\log_{2}(e) \cdot n\log_{2}(1/\varepsilon)$, which is $\log_{2}(e)$ times the theoretical lower bound of $n\log_{2}(1/\varepsilon)$~\citep{pagh2005optimal}.
% The theoretical lower bound on the number of bits required for data structures supporting approximate membership queries is known to be $n\log_{2}(1/\varepsilon)$, where $n$ is the number of elements in the set, and $\varepsilon$ is the false positive rate~\citep{pagh2005optimal}. Bloom filters use $\log_{2}(e) \cdot n\log_{2}(1/\varepsilon)$ bits, i.e., they require $\log_{2}(e)$ times more memory than the theoretical lower bound.
Several improved versions of the Bloom filter, such as the Cuckoo filter~\citep{fan2014cuckoo}, Vacuum filter~\citep{wang2019vacuum}, Xor filter~\citep{graf2020xor}, and Ribbon filter~\citep{peter2021ribbon}, have been proposed to get closer to the theoretical lower bound.

% 近年、機械学習モデルを利用してブルームフィルタのメモリ効率をさらに向上させる「学習型ブルームフィルタ（Learned Bloom filter, LBF）」の概念が提案された~\citep{kraska2018case}。
% Kraskaらは、集合に含まれるか含まれないかの2値分類を行う機械学習モデルを，ブルームフィルタのプレフィルタとして利用するLBFを提案した(\cref{fig:existinglbf}(a))。
% このLBFでは，機械学習モデルが集合に含まれると判定した要素はブルームフィルタに追加せず，機械学習モデルが集合に含まれないと判定した要素のみをブルームフィルタに追加する．
% クエリに回答する際には，機械学習モデルが集合に含まれると判定した要素は即座に「クエリは集合に含まれる」と回答する．
% そうでない場合は，LBFはブルームフィルタに問い合わせることで最終的な回答を得る．
% これにより，機械学習モデルによって効率的にブルームフィルタが保持すべき要素数を削減し，LBFのメモリ使用量（機械学習モデルのメモリ使用量とブルームフィルタのメモリ使用量の和）はブルームフィルタのそれを下回ることができる。
% その後、LBFの構造をさらに改良する研究が数多く提案されている。
% sandwiched LBF~\citep{mitzenmacher2018model}は、機械学習モデルを2つのブルームフィルタでサンドウィッチする構造を持つ(\cref{fig:existinglbf}(b))．
% この2つのブルームフィルタのサイズを適切に調整することで、メモリ効率をさらに向上できることを示した。
% Ada-BF~\citep{dai2020adaptive}やPartitioned Learned Bloom filter （PLBF）~\citep{vaidya2021partitioned}は、機械学習モデルの出力スコア（「入力要素がどれだけ集合に含まれていそうか」を指し示す値）を利用することで、メモリ効率をさらに向上させる(\cref{fig:existinglbf}(c))。
% これらのLBFでは，異なる偽陽性率を持つ複数のブルームフィルタを用意し，スコアに応じて適切な精度のブルームフィルタを選択することで，機械学習モデルの出力スコアをより連続的に活用することができる．
% fast PLBFという手法は，高メモリ効率であるPLBFの構成を完全に保ったまま，高速に構築することができる~\citep{sato2023fast}．

Recently, the concept of Learned Bloom Filter (LBF), which enhances the memory efficiency of Bloom filters using machine learning models, was introduced~\citep{kraska2018case}. They proposed an LBF that uses a machine learning model, which predicts whether the input is included in the set $\mathcal{S}$, as a pre-filter before a classical Bloom filter (\cref{fig:existinglbf}(a)). 
In this LBF, elements predicted by the model as included in the set $\mathcal{S}$ are not inserted into the classical Bloom filter, while those predicted as not included are. 
When this LBF answers a query, it immediately answers $q \in \mathcal{S}$ if the model predicts the query is in the set. In contrast, if the model predicts the query is not in the set, this LBF uses the classical Bloom filter.
This design reduces the number of elements stored in the Bloom filter, thus reducing the total memory usage. 

Numerous subsequent studies have sought to improve this structure further.
Sandwiched LBF~\citep{mitzenmacher2018model} sandwiches the machine learning model with two Bloom filters (\cref{fig:existinglbf}(b)). 
It is demonstrated that the memory efficiency is further improved by optimizing the size of the two Bloom filters. 
Ada-BF~\citep{dai2020adaptive} and PLBF~\citep{vaidya2021partitioned} further enhance memory efficiency by utilizing the \textit{score}, which is the prediction of the machine learning model regarding the likelihood that an input element is included in the set (\cref{fig:existinglbf}(c)). 
These LBFs employ multiple Bloom filters with different false positive rates, selecting the appropriate filter based on the score. This approach allows for a more continuous and fine-grained utilization of the model predictions.
% Fast PLBF~\citep{sato2023fast} retains the high memory efficiency of PLBF while allowing faster construction.

% 既存のLBFに関する研究のほとんどが，学習済みの機械学習モデルに対してブルームフィルタをどのように配置するかということに焦点を当てている一方で，いくつかの研究は機械学習モデルの選択それ自体に焦点を当てている~\citep{fumagalli2022choice, dai2022optimizing, malchiodi2024role}。
% これらの研究では，いくつかの機械学習モデルと，Learned Bloom filterの構成（sandwiched LBFやPLBF）を網羅的に試し，メモリ効率や棄却に要する時間の計測を行っている．
% その結果から，データセットのNoisyか，学習しやすいか，Reject Timeが短いことが重要であるか，などによって，最適な機械学習モデルが異なることが示されている．
% ただし，それらの研究が与えている機械学習モデルの選択方法は，大雑把な傾向を捉えた経験則に過ぎず，最適な機械学習モデルを自動的に選択する手法は提案されていない．
While most research on LBFs has focused on the optimal configuration of Bloom filters for a fixed trained machine learning model, some studies have investigated the choice of the machine learning model itself~\citep{fumagalli2022choice, dai2022optimizing, malchiodi2024role}. These studies evaluate various machine learning models and LBF configuration methods (such as sandwiched LBF and PLBF) across different datasets, measuring memory efficiency and reject times. The results suggest that the optimal machine learning model varies depending on dataset noisiness, ease of learning, and the importance of minimizing reject time. However, these studies provide only general guidelines based on observed trends, and no method has yet been proposed for automatically selecting the optimal machine learning model.

Optimizing the hash functions is another approach to improving the memory efficiency.
Hash Adaptive Bloom Filter (HABF)~\citep{xie2021hashadaptive} uses a lightweight data structure called HashExpressor to select suitable hash functions for each key, and Projection Hash Bloom Filter (PHBF)~\citep{bhattacharya2022new} employs projections as hash functions.
Unlike LBFs, these approaches avoid classifier training and instead pack information into a lighter structure.

\begin{figure*}[t]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/ExistingLBF_Architecture_2.pdf}
        \caption{The architecture of Existing LBFs: (a) Naive LBF~\citep{kraska2018case} has a single backup Bloom filter. (b) Sandwiched LBF~\citep{mitzenmacher2018model} applies a pre-filter before the model inference. (c) PLBF~\citep{vaidya2021partitioned} uses multiple backup Bloom filters.}
        \label{fig:existinglbf}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/BigMacLBF_Architecture_2.pdf}
        \caption{The architecture of CLBF: CLBF alternates between score-based branching and Bloom filter-based filtering. This design generalizes the architectures of sandwiched LBF and PLBF. Note that $g^{(*)}_{*}$ and $h^{(*)}_{*}$ represent the proportions of keys and non-keys passing through each root when filtering using $\mathrm{TBF}$s is \textbf{not} performed.}
        \label{fig:bigmaclbf}
    \end{minipage}
\end{figure*}