%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

%% >> Package for using Japanese (to be removed later)
% \usepackage[whole]{bxcjkjatype}
%% << Package for using Japanese (to be removed later)

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}
\usepackage[preprint]{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% >>>>>>>>> Added by Author
\usepackage{bm}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\algrenewcommand\algorithmicdo{}
% <<<<<<<<< Added by Author

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Cascaded Learned Bloom Filter for Optimal Model-Filter Size Balance and Fast Rejection}

\begin{document}

\twocolumn[
\icmltitle{Cascaded Learned Bloom Filter for \\ Optimal Model-Filter Size Balance and Fast Rejection}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Atsuki Sato}{equal,tokyo}
\icmlauthor{Yusuke Matsui}{equal,tokyo}
\end{icmlauthorlist}

\icmlaffiliation{tokyo}{Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan}

\icmlcorrespondingauthor{Atsuki Sato}{a\_sato@hal.t.u-tokyo.ac.jp}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Recent studies have demonstrated that learned Bloom filters, which combine machine learning with the classical Bloom filter, can achieve superior memory efficiency. 
However, existing learned Bloom filters face two critical unresolved challenges: the balance between the machine learning model size and the Bloom filter size is not optimal, and the reject time cannot be minimized effectively.
We propose the Cascaded Learned Bloom Filter (CLBF) to address these issues.
Our dynamic programming-based optimization automatically selects configurations that achieve an optimal balance between the model and filter sizes while minimizing reject time. 
Experiments on real-world datasets show that CLBF reduces memory usage by up to 24\% and decreases reject time by up to 14 times compared to state-of-the-art learned Bloom filters.
\end{abstract}


\section{Introduction}
\label{sec:introduction}
% Bloom filter~\citep{bloom1970space}は，近似メンバーシップクエリのためのデータ構造の一つである．
% Bloom filterは，集合$\mathcal{S}$をビット配列へ圧縮し，それを用いてクエリ$q$が$\mathcal{S}$に含まれるかどうかを，少しの偽陽性を許して，判定する．
% そのメモリ効率の良さと，判定の高速さから，Bloom filterはさまざまなmemory constrained and/or latency sensitive applicationsで使われている~\citep{broder2004network, chang2008bigtable}．
% 近年提案されたLearned Bloom filter (LBF) という新しいクラスのBloom filterは，集合$\mathcal{S}$に含まれるかどうかを2値分類する機械学習モデルを活用することで，Bloom filterを上回るメモリ効率を達成することができることが示されている~\citep{kraska2018case}．
% これまで更なるメモリ効率の改善を目指して様々なLBFが提案されている~\citep{mitzenmacher2018model, dai2020adaptive}が，既存のLBFには，2つの大きな未解決の問題がある:(1)「machine learning Model サイズ」と「Bloom filterサイズ」のバランスの最適化がされていないことと(2)棄却時間の最小化がされていないこと．

% \textbf{(1) machine learning Model サイズと Bloom filterサイズのバランス}:
% 既存のLBFsはどれも，機械学習モデルと1つ以上のBloom filterを組み合わせたデータ構造である．
% これらのLBFは，ある精度を達成するという条件のもとで，そのメモリ使用量（機械学習モデルのメモリ使用量とBloom filterたちのメモリ使用量の合計）の最小化を目指している．
% しかし，既存のLBFsの構築方法はいずれも，固定された学習済み機械学習モデルに対して，最適にBloom filterたちを配置することに焦点を当てており，機械学習モデルとBloom filterのサイズのバランスを自動的に調整する仕組みを持っていない．

% \textbf{(2) 棄却時間の最小化}:
% Bloom filterは，集合に含まれないものに対して，「この要素は『絶対に』集合に含まれない」と偽陰性無しに素早く棄却することができる．
% LBFをBloom filterと同様のアプリケーションに適用するためには，LBFも高速な棄却を行えることが重要である．
% 既存のいくつかの研究はLBFの棄却時間に焦点をあてている~\citep{fumagalli2022choice, malchiodi2024role}．
% 彼らは，複数の分類器・LBFについて棄却時間の計測を行い，適切な分類器・LBFを選択する方法論を提案している．
% しかし，これはおおよその傾向を捉えた経験則を与えているに過ぎず，棄却時間を自動的に最小化する手法はいまだ提案されていない．

% 我々は，これらの問題を解決するために，カスケード構造を持つLBF，cascaded learned Bloom filter (CLBF)を提案する．
% (1)CLBFは，あらかじめ大きめに訓練したmachine learning Modelを用意し，それを適切なだけ削減することによって，最適なメモリサイズバランスを達成する．
% (2)さらに，CLBFは，暫定的なmachine learning Modelの出力値による条件分岐や，中間に差し込まれたBloom filterたちによって，素早い棄却を可能にする．
% ユーザーは，メモリ効率と棄却時間のどちらをどれだけ重視するか、を調整するハイパーパラメータを設定することができ，我々が提案する動的計画法を用いた最適化手法はそのハイパーパラメータに応じた最適化を行う．
% 我々の実験は，(1)CLBFがPLBF~\citep{vaidya2021partitioned}, SoTAのメモリ効率をもつLBF, の最大$24\%$のメモリ使用量の削減を達成すること，および(2)CLBFがPLBFの最大14倍の棄却時間の短縮を達成することを示した．

Bloom filters~\citep{bloom1970space} are ubiquitous data structures used for approximate membership queries.
A Bloom filter compresses a set $\mathcal{S}$ into a bit array, enabling rapid determination of whether a query $q$ is a \textit{key} (i.e., an element included in $\mathcal{S}$) or a \textit{non-key} (i.e., an element not included in $\mathcal{S}$).
While a Bloom filter may misidentify a non-key as a key (false positive), it never misidentifies a key as a non-key (false negative).
Due to their high memory efficiency, fast query performance, and the absence of false negatives, Bloom filters are widely employed in memory-constrained and latency-sensitive applications~\citep{broder2004network, chang2008bigtable}.
Recently, a new class of Bloom filters called Learned Bloom Filters (LBFs) has been proposed~\citep{kraska2018case}.
LBFs leverage a machine learning model to predict whether an input belongs to the set $\mathcal{S}$, achieving superior memory efficiency compared to classical Bloom filters.
Despite numerous attempts to further improve memory efficiency~\citep{mitzenmacher2018model, dai2020adaptive}, existing LBFs face two critical unresolved issues: (1) the balance between the machine learning model size and the Bloom filter size remains suboptimal, and (2) the reject time cannot be effectively minimized.

\textbf{(1) The Balance Between Machine Learning Model Size and Bloom Filter Size}:
Existing LBFs lack mechanisms to automatically balance the sizes of the machine learning model and the Bloom filters.
An LBF consists of a machine learning model and one or more Bloom filters, aiming to minimize the total memory usage, i.e., the sum of the memory consumed by the machine learning model and the Bloom filters.
Since a smaller machine learning model often---but not always---has lower accuracy, larger Bloom filters are needed to maintain the overall false positive rate of an LBF, whereas a larger model often---but not always---allows for smaller Bloom filters.
Thus, it is a challenging task to strike a balance between the sizes of the machine learning model and the Bloom filters in LBFs, and to minimize the overall memory consumption.
Existing LBF construction methods focus on optimizing the configuration of Bloom filters for a fixed, pre-trained model, without addressing the interplay between the sizes of the model and the Bloom filters.

\textbf{(2) The Reject Time}:
Existing approaches do not provide automatic methods for minimizing reject time in LBFs.
The short \textit{reject time} of a Bloom filter (i.e., the time it takes to answer ``$q$ does not belong to $\mathcal{S}$'') is a key property and often more critical than the \textit{accept time} (i.e., the time it takes to answer ``$q$ belongs to $\mathcal{S}$'').
For example, consider using a Bloom filter as a pre-filter before performing \textbf{strict} membership queries in a large-scale key-value store or a database system.
In such systems, checking whether a key exists often requires an expensive lookup operation, such as searching a B-tree, querying a distributed hash table, or accessing disk storage.
The Bloom filter can pre-filter the majority of non-key queries at the cost of a small overhead.
Let $T$ denote the time required to perform a naive strict membership check without a pre-filter.
Let $f$ represent the Bloom filter's false positive rate, and let $t_a$ and $t_r$ denote its accept and reject times, respectively.
The response time for key queries is $t_a + T$, but since $t_a \ll T$, this is almost equivalent to $T$.
On the other hand, the expected response time for non-key queries is given by:
\begin{equation}
\label{equ: approx time for non-key query}
    (1-f) t_r + f (t_a + T) \approx t_r + fT,
\end{equation}
since $t_a \ll T$ and $f \ll 1$.
The expected reject time, $t_r + fT$, is significantly smaller than $T$ when $t_r$ and $f$ are sufficiently small, highlighting the advantage of using a Bloom filter.
As shown in \cref{equ: approx time for non-key query}, reducing the reject time $t_r$ is as important as minimizing the false positive rate $f$.
However, most existing LBF research~\citep{kraska2018case,mitzenmacher2018model,vaidya2021partitioned} focuses primarily on accuracy.
While some studies~\citep{fumagalli2022choice,malchiodi2024role} address reject time, they offer only heuristic guidelines and lack automatic methods for minimizing reject time.

To address these issues, we propose a novel learned Bloom filter with a cascaded structure called Cascaded Learned Bloom Filter (CLBF).
CLBF offers two key features: (1) CLBF achieves an optimal balance of memory usage by training a larger machine learning model and then appropriately reducing its size, and 
(2) CLBF enables faster rejections through branching based on tentative outputs from the machine learning model and the intermediate Bloom filters.
We can set the hyperparameters to control the trade-off between memory efficiency and reject time. 
Our optimization approach, based on dynamic programming, automatically adjusts CLBF to the optimal configuration for the given hyperparameter.
Our experiments demonstrate that (1) CLBF reduces memory usage by up to 24\% and (2) reject time by up to 14 times compared to the Partitioned Learned Bloom Filter (PLBF)~\citep{vaidya2021partitioned}, the state-of-the-art LBF.

\section{Related Work}
\label{sec:related_work}

% 近似メンバーシップクエリのためのデータ構造の開発は、長年にわたり研究されてきた分野であり、その中でもBloom filter~\citep{bloom1970space}は最も基本的で広く用いられているデータ構造の一つである。
% Bloom filterは，数回のハッシュ関数の評価と，数個のビットの参照だけで，高速にクエリに回答することができる．
% Bloom filterの回答にはある確率で偽陽性が含まれるが，偽陰性は発生しない．
% すなわち，Bloom filterは，このクエリは「確実に」$\mathcal{S}$に含まれない，という「棄却」が可能である．
% この性質から，Bloom filterはnetwork~\citep{broder2004network, tarkoma2011theory, geravand2013bloom}やdatabase~\citep{chang2008bigtable, goodrich2011invertible, lu2012bloomstore}などの，特にmemory constrained and/or latency sensitive なシナリオで活用されている．
% 近似メンバーシップクエリのデータ構造が必要とするbit数の下限は$n\log_{2}(1/\varepsilon)$であることが知られている, ただし$n$は集合の要素数であり、$\varepsilon$は偽陽性率~\citep{pagh2005optimal}．
% Bloom filterが必要とするbit数は、$\log_{2}(e) \cdot n\log_{2}(1/\varepsilon)$である, i.e., 理論的下界の$\log_{2}(e)$倍,である。
% このメモリ使用量をさらに理論的下限へと近づけるために、カッコーフィルタ~\citep{fan2014cuckoo}、バキュームフィルタ~\citep{wang2019vacuum}、Xorフィルタ~\citep{graf2020xor}、リボンフィルタ~\citep{peter2021ribbon}など、ブルームフィルタの改良版が提案されている。

% Data structures for approximate membership queries have been extensively studied over the past decades. 
Bloom filter~\citep{bloom1970space} is one of the most fundamental and widely used data structures for approximate membership queries. 
Bloom filters can quickly respond to queries by only performing a few hash function evaluations and checking a few bits.
Although Bloom filters may return false positives, they never yield false negatives. This property makes Bloom filters valuable in memory-constrained and latency-sensitive scenarios such as networks~\citep{broder2004network, tarkoma2011theory, geravand2013bloom} and databases~\citep{chang2008bigtable, goodrich2011invertible, lu2012bloomstore}.
The parameters of a Bloom filter, such as the length of the bit array and the number of hash functions, are determined by the number of keys to store and the target false positive rate.
Specifically, for a Bloom filter storing $n$ keys with a false positive rate $\varepsilon$, the required bit array length is $\log_{2}(e) \cdot n\log_{2}(1/\varepsilon)$, which is $\log_{2}(e)$ times the theoretical lower bound of $n\log_{2}(1/\varepsilon)$~\citep{pagh2005optimal}.
% The theoretical lower bound on the number of bits required for data structures supporting approximate membership queries is known to be $n\log_{2}(1/\varepsilon)$, where $n$ is the number of elements in the set, and $\varepsilon$ is the false positive rate~\citep{pagh2005optimal}. Bloom filters use $\log_{2}(e) \cdot n\log_{2}(1/\varepsilon)$ bits, i.e., they require $\log_{2}(e)$ times more memory than the theoretical lower bound.
Several improved versions of the Bloom filter, such as the Cuckoo filter~\citep{fan2014cuckoo}, Vacuum filter~\citep{wang2019vacuum}, Xor filter~\citep{graf2020xor}, and Ribbon filter~\citep{peter2021ribbon}, have been proposed to get closer to the theoretical lower bound.

% 近年、機械学習モデルを利用してブルームフィルタのメモリ効率をさらに向上させる「学習型ブルームフィルタ（Learned Bloom filter, LBF）」の概念が提案された~\citep{kraska2018case}。
% Kraskaらは、集合に含まれるか含まれないかの2値分類を行う機械学習モデルを，ブルームフィルタのプレフィルタとして利用するLBFを提案した(\cref{fig:existinglbf}(a))。
% このLBFでは，機械学習モデルが集合に含まれると判定した要素はブルームフィルタに追加せず，機械学習モデルが集合に含まれないと判定した要素のみをブルームフィルタに追加する．
% クエリに回答する際には，機械学習モデルが集合に含まれると判定した要素は即座に「クエリは集合に含まれる」と回答する．
% そうでない場合は，LBFはブルームフィルタに問い合わせることで最終的な回答を得る．
% これにより，機械学習モデルによって効率的にブルームフィルタが保持すべき要素数を削減し，LBFのメモリ使用量（機械学習モデルのメモリ使用量とブルームフィルタのメモリ使用量の和）はブルームフィルタのそれを下回ることができる。
% その後、LBFの構造をさらに改良する研究が数多く提案されている。
% sandwiched LBF~\citep{mitzenmacher2018model}は、機械学習モデルを2つのブルームフィルタでサンドウィッチする構造を持つ(\cref{fig:existinglbf}(b))．
% この2つのブルームフィルタのサイズを適切に調整することで、メモリ効率をさらに向上できることを示した。
% Ada-BF~\citep{dai2020adaptive}やPartitioned Learned Bloom filter （PLBF）~\citep{vaidya2021partitioned}は、機械学習モデルの出力スコア（「入力要素がどれだけ集合に含まれていそうか」を指し示す値）を利用することで、メモリ効率をさらに向上させる(\cref{fig:existinglbf}(c))。
% これらのLBFでは，異なる偽陽性率を持つ複数のブルームフィルタを用意し，スコアに応じて適切な精度のブルームフィルタを選択することで，機械学習モデルの出力スコアをより連続的に活用することができる．
% fast PLBFという手法は，高メモリ効率であるPLBFの構成を完全に保ったまま，高速に構築することができる~\citep{sato2023fast}．

Recently, the concept of Learned Bloom Filter (LBF), which enhances the memory efficiency of Bloom filters using machine learning models, was introduced~\citep{kraska2018case}. They proposed an LBF that uses a machine learning model, which predicts whether the input is included in the set $\mathcal{S}$, as a pre-filter before a classical Bloom filter (\cref{fig:existinglbf}(a)). 
In this LBF, elements predicted by the model as included in the set $\mathcal{S}$ are not inserted into the classical Bloom filter, while those predicted as not included are. 
When this LBF answers a query, it immediately answers $q \in \mathcal{S}$ if the model predicts the query is in the set. In contrast, if the model predicts the query is not in the set, this LBF uses the classical Bloom filter.
This design reduces the number of elements stored in the Bloom filter, thus reducing the total memory usage. 

Numerous subsequent studies have sought to improve this structure further.
Sandwiched LBF~\citep{mitzenmacher2018model} sandwiches the machine learning model with two Bloom filters (\cref{fig:existinglbf}(b)). 
It is demonstrated that the memory efficiency is further improved by optimizing the size of the two Bloom filters. 
Ada-BF~\citep{dai2020adaptive} and PLBF~\citep{vaidya2021partitioned} further enhance memory efficiency by utilizing the \textit{score}, which is the prediction of the machine learning model regarding the likelihood that an input element is included in the set (\cref{fig:existinglbf}(c)). 
These LBFs employ multiple Bloom filters with different false positive rates, selecting the appropriate filter based on the score. This approach allows for a more continuous and fine-grained utilization of the model predictions.
% Fast PLBF~\citep{sato2023fast} retains the high memory efficiency of PLBF while allowing faster construction.

% 既存のLBFに関する研究のほとんどが，学習済みの機械学習モデルに対してブルームフィルタをどのように配置するかということに焦点を当てている一方で，いくつかの研究は機械学習モデルの選択それ自体に焦点を当てている~\citep{fumagalli2022choice, dai2022optimizing, malchiodi2024role}。
% これらの研究では，いくつかの機械学習モデルと，Learned Bloom filterの構成（sandwiched LBFやPLBF）を網羅的に試し，メモリ効率や棄却に要する時間の計測を行っている．
% その結果から，データセットのNoisyか，学習しやすいか，Reject Timeが短いことが重要であるか，などによって，最適な機械学習モデルが異なることが示されている．
% ただし，それらの研究が与えている機械学習モデルの選択方法は，大雑把な傾向を捉えた経験則に過ぎず，最適な機械学習モデルを自動的に選択する手法は提案されていない．
While most research on LBFs has focused on the optimal configuration of Bloom filters for a fixed trained machine learning model, some studies have investigated the choice of the machine learning model itself~\citep{fumagalli2022choice, dai2022optimizing, malchiodi2024role}. These studies evaluate various machine learning models and LBF configuration methods (such as sandwiched LBF and PLBF) across different datasets, measuring memory efficiency and reject times. The results suggest that the optimal machine learning model varies depending on dataset noisiness, ease of learning, and the importance of minimizing reject time. However, these studies provide only general guidelines based on observed trends, and no method has yet been proposed for automatically selecting the optimal machine learning model.

Optimizing the hash functions is another approach to improving the memory efficiency.
Hash Adaptive Bloom Filter (HABF)~\citep{xie2021hashadaptive} uses a lightweight data structure called HashExpressor to select suitable hash functions for each key, and Projection Hash Bloom Filter (PHBF)~\citep{bhattacharya2022new} employs projections as hash functions.
Unlike LBFs, these approaches avoid classifier training and instead pack information into a lighter structure.

\begin{figure*}[t]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/ExistingLBF_Architecture_2.pdf}
        \caption{The architecture of Existing LBFs: (a) Naive LBF~\citep{kraska2018case} has a single backup Bloom filter. (b) Sandwiched LBF~\citep{mitzenmacher2018model} applies a pre-filter before the model inference. (c) PLBF~\citep{vaidya2021partitioned} uses multiple backup Bloom filters.}
        \label{fig:existinglbf}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/BigMacLBF_Architecture_2.pdf}
        \caption{The architecture of CLBF: CLBF alternates between score-based branching and Bloom filter-based filtering. This design generalizes the architectures of sandwiched LBF and PLBF. Note that $g^{(*)}_{*}$ and $h^{(*)}_{*}$ represent the proportions of keys and non-keys passing through each root when filtering using $\mathrm{TBF}$s is \textbf{not} performed.}
        \label{fig:bigmaclbf}
    \end{minipage}
\end{figure*}

\section{Method: Cascaded Learned Bloom Filter}
\label{sec:method}

% この章では、提案手法であるCLBFのアーキテクチャとその構築方法について説明する。
% まず、\cref{sec:architecture_overview}では、CLBFのアーキテクチャを説明する。
% 次に，\cref{sec:problem_formulation}では，CLBFの構築における問題設定を定式化する。
% そして，\cref{sec:dp_solution}では、動的計画法を用いたCLBF構築の最適化手法について説明する。

This section describes the architecture and construction of our proposed Cascaded Learned Bloom Filter (CLBF). 
In \cref{sec:architecture_overview}, we describe the architecture of CLBF. 
Then, we formulate the problem of constructing CLBF in \cref{sec:problem_formulation}, and the optimization method for configuring CLBF using dynamic programming is described in \cref{sec:dp_solution}.

\subsection{Architectural Design and Workflow}
\label{sec:architecture_overview}

CLBF employs a cascade structure consisting of multiple machine learning models and multiple Bloom filters arranged alternately (\cref{fig:bigmaclbf}).
Any machine learning model can be used, including very simple ones.
In fact, in our experiments, each machine learning model in CLBF corresponds to each weak learner trained with a boosting algorithm (as detailed in \cref{sec:experiments}).
By optimizing the number of machine learning models (weak learners) and the false positive rate of each bloom filter, CLBF not only balances model and filter sizes but also shortens the reject time.
% (as detailed in \cref{sec:problem_formulation} and \cref{sec:dp_solution}).

CLBF processes a query $q$ as follows:
The query first enters the initial \textit{Trunk Bloom filter} $\mathrm{TBF}_1$.
If $\mathrm{TBF}_1$ determines $q \notin \mathcal{S}$, CLBF immediately returns $q \notin \mathcal{S}$.
Otherwise, the query is passed to the first machine learning model, $\mathrm{ML}_1$, which outputs a score indicating the likelihood that $q$ is in $\mathcal{S}$. 
If the score exceeds threshold $\theta_1$, the initial \textit{Branch Bloom filter} $\mathrm{BBF}_1$ makes the final determination.
Otherwise, $q$ is passed to the next Trunk Bloom filter, $\mathrm{TBF}_2$.
In this way, CLBF alternates between branching based on the score and filtering using Bloom filters.
When the query reaches the $D$-th (i.e., final) machine learning model, CLBF adopts the same approach as PLBF; multiple thresholds are set, and based on the range in which the query score falls, one of the $K$ \textit{Final Bloom filters} ($\mathrm{FBF}_1, \mathrm{FBF}_2, \dots, \mathrm{FBF}_K$) is used for the final determination.
When inserting a key into CLBF, the key is inserted into \textbf{all} Bloom filters traversed by the process outlined above, ensuring the absence of false negatives.
The exact algorithms for key insertion and query responses are detailed in \cref{app: algorithm insert and query}.

Here, we emphasize that the CLBF architecture generalizes existing LBF architectures (\cref{fig:existinglbf,fig:bigmaclbf}) and that our optimization method can identify the optimal configuration within this unified framework.
For example, a CLBF with only one Trunk Bloom filter and one Final Bloom filter is equivalent to a sandwiched LBF, and a CLBF that omits both Trunk and Branch Bloom filters while employing multiple Final Bloom filters is equivalent to a PLBF.
Within this generalized framework, we address more general and flexible optimization problems than those addressed by existing LBFs;
Our construction method optimizes the number of machine learning models, taking into account the reject time.
As a result, CLBF achieves superior memory efficiency and shorter reject time compared to existing LBFs.

The notations for CLBF description are as follows (most of the variables defined here are illustrated in \cref{fig:bigmaclbf}):
$D$ denotes the number of machine learning models CLBF uses (we optimize $D$ using dynamic programming).
The false positive rate of $\mathrm{TBF}_d$ ($d=1,2,\dots,D$), $\mathrm{BBF}_d$ ($d=1,2,\dots,D-1$), and $\mathrm{FBF}_k$ ($k=1,2,\dots,K$) are denoted by $f^{(t)}_d$, $f^{(b)}_d$, and $f^{(f)}_k$, respectively.
These false positive rates can also be expressed as vectors, e.g., $\bm{f}^{(t)}=[f^{(t)}_1, f^{(t)}_2, \dots, f^{(t)}_D]$.
Additionally, we collectively refer to these vectors as $\mathcal{F}=\{\bm{f}^{(t)},\bm{f}^{(b)},\bm{f}^{(f)}\}$ for convenience.
$\mathrm{Size}(\cdot)$ and $\mathrm{Time}(\cdot)$ represent the memory size and the inference time of the input, respectively, e.g., $\mathrm{Size}(\mathrm{ML}_d)$ represents the memory size of $\mathrm{ML}_d$.
$\mathcal{S}$ represents the set stored by CLBF, and $n$ denotes the number of elements in $\mathcal{S}$.
$\mathcal{Q}$ refers to the non-key set used for construction of CLBF.

\subsection{Problem Formulation}
\label{sec:problem_formulation}

% 一言で言えば、CLBFを構築する際に解決すべき問題は、精度の制約のもとで、メモリ使用量と予想拒否時間の加重和を最小化することである。
% ここでは、CLBFを構築する際に与えられる情報と、CLBFが最適化するパラメータ、および、最小化する目的関数について順に詳述する。

The goal of CLBF construction is to minimize the weighted sum of memory usage and the expected reject time, subject to an accuracy constraint.
Here, we provide a detailed explanation of the components required for CLBF construction, the parameters to be optimized, and the objective function to be minimized.

% CLBFを構築する際には以下を用意する必要がある: 複数の学習済みの機械学習モデル，2つのハイパーパラメータ,Validation用データセット．
% 機械学習モデルは集合$\mathcal{S}$と$\mathcal{Q}$の全てもしくは一部を用いて，keyとnon-keyの2値分類を行うように学習されている．
% 学習済みの機械学習モデルの個数を$\bar{D}$と表記する。
% これらのうち先頭の$D ~ (\leq \bar{D})$個の機械学習モデルのみがCLBFに用いられる、ただし、$D$はCLBFの構築の際に最適化される値。
% ハイパーパラメータは以下の2つである: 
% $F ~ (\in (0, 1))$, i.e., CLBFが許容する偽陽性率の上界,
% $\lambda ~ (\in [0, 1])$, i.e., メモリ効率と棄却時間のトレードオフを調整するパラメータ.
% CLBFは，期待偽陽性率が$F$以下であるという条件のもとで，メモリ使用量を小さく，棄却時間を短くするように最適化を行なって構築される．
% $\lambda$が大きいほど，メモリ効率が重視され，$\lambda = 1$の時はメモリ効率のみを，$\lambda = 0$の時は棄却時間のみを最適化する．

\paragraph{Requirements for CLBF Construction.}
To construct the CLBF, the user must provide the following components: several pre-trained machine learning models, two hyperparameters, and validation data. The machine learning models are trained to perform binary classification between key and non-key using the sets $\mathcal{S}$ and $\mathcal{Q}$ (or their subsets). 
The number of pre-trained models is denoted by $\bar{D}$. 
Among these pre-trained models $\mathrm{ML}_1, \mathrm{ML}_2, \dots, \mathrm{ML}_{\bar{D}}$, only the first $D ~ (\leq \bar{D})$ models, i.e., $\mathrm{ML}_1, \mathrm{ML}_2, \dots, \mathrm{ML}_D$, are used in the CLBF.
The value of $D$ is optimized during CLBF construction.
The two hyperparameters are $F ~ (\in (0, 1))$, which represents the target false positive rate for the CLBF, and $\lambda ~ (\in [0, 1])$, which controls the trade-off between memory efficiency and reject time. 
The CLBF is optimized to minimize memory usage and reject time, subject to the constraint that the expected false positive rate does not exceed $F$. 
As $\lambda$ increases, greater emphasis is placed on memory efficiency; specifically, when $\lambda = 1$, only memory usage is minimized, while when $\lambda = 0$, only reject time is minimized.

% CLBFを構築する際に最適化するパラメータは以下の4種類である:
% (i) $D$, i.e., CLBFの深さ($0\leq D \leq \bar{D}$).
% (ii) $f^{(t)}_d ~ (d \in\{ 1,2,\dots,D\})$, i.e., $\mathrm{TBF}_d$の偽陽性率($0 < f^{(t)}_d \leq 1$).
% (iii) $f^{(b)}_d ~ (d \in\{ 1,2,\dots,D-1\})$, i.e., $\mathrm{BBF}_d$の偽陽性率($0 < f^{(b)}_d \leq 1$).
% (iv) $f^{(f)}_k ~ (k \in\{ 1,2,\dots,K\})$, i.e., $\mathrm{FBF}_k$の偽陽性率($0 < f^{(f)}_k \leq 1$).
% 正確には，$\theta_d$, i.e., $\mathrm{ML}_d ~ (d \in\{1,\dots,D-1\})$の出力するスコアに応じて条件分岐する際の閾値, も最適化すべきパラメータである．
% しかし，この閾値は，上記の4つのパラメータと同時に最適化を行うのが難しい．
% なぜなら，$\theta_d$を変えると，$\mathrm{ML}_{d+1}$，およびそれ以降のモデルの出力するスコアの分布が変化し，またその変化を捉えることも難しいからである．
% したがって，我々はヒューリスティックな複数通りの決め方を試してみて，最も目的関数が小さくなるものを採用する方針を取る．
% 具体的には，Validationデータに含まれるnon-keyサンプルの$\mathrm{ML}_{d}$の出力スコアのうち，上位$\alpha \%$の値を$\theta_d$として試した
% ($\alpha\in\{0.5,0.2,0.1,\dots,0.0001,0.0\}$)．
% % ($\alpha=0.5,0.2,0.1,\dots,0.0001,0.0$)．
% また，最後の機械学習モデル, i.e., $\mathrm{ML}_D$,の出力するスコアにおける閾値については，PLBFのやり方と同様に，keyのスコアとnon-keyのスコア分布のKL divergenceを最大化するように設定した．

\paragraph{Parameters to be Optimized.}
In the construction of CLBF, the parameters $D$ and $\mathcal{F}~(=\{\bm{f}^{(t)}, \bm{f}^{(b)}, \bm{f}^{(f)}\})$ are optimized.
Another parameter, $\theta_d~(d=1,\dots, D-1)$, i.e., the threshold for branching based on the tentative score output by $\mathrm{ML}_d$, should also be optimized. 
However, jointly optimizing $\theta_d$ with the other parameters is too challenging, as the value of $\theta_d$ affects the score distributions output by $\mathrm{ML}_{d+1}$ and subsequent models, and capturing these effects is difficult. 
Thus, we adopt a heuristic approach, evaluating several candidates $\bm{\theta} ~ (=[\theta_1, \theta_2, \dots, \theta_{\bar{D} - 1}])$ and selecting the one that minimizes the objective function.
Specifically, for each $\alpha~(\in\{0.5,0.2,0.1,\dots,0.0001,0.0\})$, we evaluate $\bm{\theta}$ such that $\theta_d$ is the top-$\alpha$ (ratio) score of non-keys output by $\mathrm{ML}_{d}$.
For the final machine learning model, $\mathrm{ML}_D$, the thresholds are set to maximize the KL divergence between the score distributions of keys and non-keys, following the same method as in the PLBF~\citep{vaidya2021partitioned}.

Once the thresholds are fixed, we can measure the proportion of keys and non-keys passed to each Bloom filter and machine learning model using the validation data.
In this measurement, filtering using $\mathrm{TBF}$s is \textbf{not} performed; only the branching based on the tentative outputs of the machine learning models are applied.
We define $g^{(t)}_{d}$ and $h^{(t)}_{d}$ as the proportions of keys and non-keys in the validation data that are passed to $\mathrm{TBF}_d$, respectively.
Similarly, we define $g^{(b)}_{d}$ and $h^{(b)}_{d}$ as the proportions of keys and non-keys that are passed to $\mathrm{BBF}_d$.
Finally, we define $g^{(f)}_{D,k}$ and $h^{(f)}_{D,k}$ as the proportions of keys and non-keys passed to $\mathrm{FBF}_k$.

% CLBFは全体の偽陽性率の期待値が$F$以下であるという条件の下で，以下の目的関数を最小化するようにして構築される:
% \begin{equation}
%     \label{eq:objective}
%     \frac{\lambda}{M_\mathrm{BF}} \cdot (\mathrm{Memory~Size}) + \frac{1-\lambda}{R_\mathrm{BF}} \cdot \mathrm{Reject~Time}.
% \end{equation}
% ここで，$\mathrm{Memory~Size}$はCLBFのメモリ使用量を表し，$\mathrm{Reject~Time}$は集合$\mathcal{S}$に含まれない要素が入力された際にそれを集合$\mathcal{S}$に含まれるかどうかを判定するまでの時間の期待値を表す．
% ここで，期待値は集合$\mathcal{S}$とクエリがValidationデータと全く同じ分布からサンプリングされるという仮定のもとで計算される値をさす．
% また，$M$と$R$は，単位とスケールを合わせるための定数である．
% 今回の実装では同じ$n$と$F$の設定の下における通常のBloom filterのメモリ使用量とReject Timeをそれぞれ$M$と$R$とした．
% これは，Validationデータを用いて計測する．
% 目的関数のそれぞれの項は以下のように書き下すことができる:
% \begin{equation}
%     \label{eq:memory_size}
%     \mathrm{Memory~Size} = \sum_{d=1}^{D} \mathrm{Size}(\mathrm{ML}_d) + \sum_{d=1}^{D} \mathrm{Size}(\mathrm{TBF}_d) + \sum_{d=1}^{D-1} \mathrm{Size}(\mathrm{BBF}_d) + \sum_{k=1}^{K} \mathrm{Size}(\mathrm{FBF}_k),
% \end{equation}
% \begin{equation}
%     \label{eq:reject_time}
%     \mathrm{Reject~Time} \sim \sum_{d=1}^{D} \left(\mathrm{Time}(\mathrm{ML}_d) \cdot h^{(t)}_d \prod_{j=1}^{d} f^{(t)}_j\right).
% \end{equation}
% 期待棄却時間についての式, i.e., \cref{eq:reject_time}, では，reject timeは機械学習モデルの推論にかかる時間の総和で近似できるとしている．
% また，ここでは，$\mathcal{Q}$に含まれる要素のうち，$\mathrm{ML}_d$へ入力される要素の割合の期待値は$h^{(t)}_d \prod_{j=1}^{d} f^{(t)}_j$と推定されることを用いている。
% これはなぜなら，non-keyが$\mathrm{ML}_d$へ入力されるとは，全ての$\mathrm{TBF}_i$($i\in\{1,\dots,d\}$)を偽陽性として通過することを意味しているからである．
% ここで，$g^{(t)}_d$と$h^{(t)}_d$と$\mathrm{Size}(\mathrm{ML}_d)$と$\mathrm{Time}(\mathrm{ML}_d)$はValidationデータを用いて，あらかじめ求めておくことができる値である（$\mathrm{Time}(\mathrm{ML}_d)$はあらかじめいくつかのサンプルで推論を行い，その平均値を用いる），

\paragraph{Objective Function and Constraint.}
The following objective function is minimized under the constraint that the ``expected'' false positive rate does not exceed $F$:
\begin{equation}
    \label{eq:objective}
    \lambda \cdot \frac{M(D,\mathcal{F})}{M_\mathrm{BF}} + (1-\lambda) \cdot \frac{R(D,\mathcal{F})}{R_\mathrm{BF}},
\end{equation}
where $M(D,\mathcal{F})$ represents the memory usage of the CLBF, and $R(D,\mathcal{F})$ denotes the ``expected'' reject time.
The expected false positive rate and the expected reject time are calculated using the validation data.
The constants $M_\mathrm{BF}$ and $R_\mathrm{BF}$ are scaling factors to align the units of the two terms, with $M_\mathrm{BF}$ representing the memory usage and $R_\mathrm{BF}$ representing the reject time of a standard classical Bloom filter under the same settings of the number of keys $n$ and the false positive rate $F$.
These constants are measured using the validation data: we construct a Bloom filter with $F$ false positive rate that stores $n$ keys, then measure the average reject time by repeatedly querying for non-keys.
$M(D,\mathcal{F})$ and $R(D,\mathcal{F})$ can be written as follows:
\begin{multline}
    \label{eq:memory_size}
    M(D,\mathcal{F}) = \sum_{i=1}^{D} \mathrm{Size}(\mathrm{ML}_i) + \sum_{i=1}^{D} \mathrm{Size}(\mathrm{TBF}_i) + \\ \sum_{i=1}^{D-1} \mathrm{Size}(\mathrm{BBF}_i) + \sum_{k=1}^{K} \mathrm{Size}(\mathrm{FBF}_k),
\end{multline}
\begin{equation}
    \label{eq:reject_time}
    R(D,\mathcal{F}) = \sum_{i=1}^{D} \left(\mathrm{Time}(\mathrm{ML}_i) \cdot h^{(t)}_i \prod_{j=1}^{i} f^{(t)}_j\right).
\end{equation}
In \cref{eq:reject_time}, it is assumed that the reject time can be approximated by the total time taken by the inference of the machine learning models.
$\mathrm{Time}(\mathrm{ML}_i)$ is a constant obtained by performing several inference runs and averaging the observed inference times.

% ここで、\cref{eq:reject_time}に現れる$h^{(t)}_d \prod_{j=1}^{d} f^{(t)}_j$というfactorについて詳しく解説する。
% このファクターはthe expected proportion of non-key queries processed by $\mathrm{ML}_d$を表してる。
% $h^{(t)}_d$が、TBFsによるフィルタリングが行われ「なかった」時に$\mathrm{ML}_d$へ入力されるnon-keyの割合であったことを思い出して欲しい。
% $\mathrm{TBF}_1$, 偽陽性率$f^{(t)}_1$である、によるフィルタリングによって、$\mathrm{ML}_1$に到達するnon-keyの割合は、$h^{(t)}_1 f^{(t)}_1$となる。
% さらに、$\mathrm{ML}_2$にnon-keyが到達するためには、$\mathrm{TBF}_1$と$\mathrm{TBF}_2$のフィルタリングの両方を偽陽性としてパスする必要があるため、$\mathrm{ML}_2$に到達するnon-keyの割合は、$h^{(t)}_2 \prod_{j=1}^{2} f^{(t)}_j$となる。
% このようにして、TBFsによるフィルタリングを考慮したときに、$\mathrm{ML}_d$へ入力されるnon-keyの割合は、これを$h^{(t)}_d \prod_{j=1}^{d} f^{(t)}_j$したものになることが分かる。

Here, we provide a detailed explanation of the factor $h^{(t)}_i \prod_{j=1}^{i} f^{(t)}_j$ appearing in \cref{eq:reject_time}.
This factor represents the expected proportion of non-key queries processed by $\mathrm{ML}_i$.
Recall that $h^{(t)}_i$ denotes the proportion of non-key queries reaching $\mathrm{ML}_i$ when no filtering is applied by the $\mathrm{TBF}$s.
Taking into account the filtering by $\mathrm{TBF}_1$, which has a false positive rate of $f^{(t)}_1$, the expected proportion of non-key queries reaching $\mathrm{ML}_1$ is $h^{(t)}_1 f^{(t)}_1$.
This is because only non-keys queries that pass through $\mathrm{TBF}_1$ as false positives reach $\mathrm{ML}_1$.
Similarly, the expected proportion of non-key queries reaching $\mathrm{ML}_2$ is $h^{(t)}_2 \prod_{j=1}^{2} f^{(t)}_j$, because non-key queries that pass through both $\mathrm{TBF}_1$ and $\mathrm{TBF}_2$ as false positives reach $\mathrm{ML}_2$.
In this way, we can conclude that the proportion of non-key queries processed by $\mathrm{ML}_i$ is given by $h^{(t)}_i \prod_{j=1}^{i} f^{(t)}_j$.

\subsection{Dynamic Programming for CLBF Construction}
\label{sec:dp_solution}

\begin{figure*}[t]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/BigMacLBF_DP1.pdf}
        \subcaption{$\hat{\mathrm{dp}}(d, T)$ is the minimum objective function value under $\mathrm{TBF}_d$ subject to the constraint that $\prod_{j=1}^{d-1} f^{(t)}_j = T$ and $D = d$.}
        \label{fig:dp1}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/BigMacLBF_DP2.pdf}
        \subcaption{$\check{\mathrm{dp}}(d, T)$ is the minimum objective function value under $\mathrm{TBF}_d$ subject to the constraint that $\prod_{j=1}^{d-1} f^{(t)}_j = T$ and $D > d$.}
        \label{fig:dp2}
    \end{minipage}
    \caption{The value of $\mathrm{dp}(d,T)$ is calculated by selecting the appropriate value from the case where $D=d$, i.e., $\hat{\mathrm{dp}}(d, T)$, and the case where $D>d$, i.e., $\check{\mathrm{dp}}(d, T)$. The value of $\mathrm{dp}(d+1,Tf^{(t)}_d)$ is used recursively to calculate $\check{\mathrm{dp}}(d, T)$.}
\end{figure*}

% ここで、目的関数を最小化するパラメータを見つけるための動的計画法について説明する．
% 我々の動的計画法の基本的な考え方はとてもシンプルである:
% 深さ$d$の構成を、FBFsとする場合と、TBFとBBFとする場合の、どちらの方が良いかを、深さ$d+1$以下の最適な構成をもとに、再帰的に決定する（Fig）。
% これを$d=1$の場合まで再帰的に計算することによって、最適なCLBFの構成を得ることができる。
% ここで重要になるのが、$\prod_{j=1}^{d-1} f^{(t)}_j$というファクターである。
% 前の章で詳しく説明した通り、$\mathrm{ML}_d$によってクエリされるnon-keyの割合は$h^{(t)}_d \prod_{j=1}^{d} f^{(t)}_j$と表せる。
% 同様に、$\mathrm{ML}_d$より下にあるコンポーネント（e.g., $TBF_{d+1}$や$BBF_{d}$)によって処理されるnon-keyの割合も$h^{(t)}_d \prod_{j=1}^{d} f^{(t)}_j$に比例する。
% したがって、我々の動的計画法では、深さのパラメータ$d$だけでなく、$T=\prod_{j=1}^{d-1} f^{(t)}_j$というファクターを引数とする関数$\mathrm{dp}(d, T)$を再帰的に計算する。

Here, we introduce a dynamic programming method to determine the parameters that minimize the objective function given in \cref{eq:objective}.
This dynamic programming method obtains the optimal configuration by repeating the following process in order of $d=\bar{D}-1,\bar{D}-2,\dots,1$:
based on the information of the optimal configuration at depth $d+1$, make the optimal choice at depth $d$ of whether to use $\mathrm{FBF}$s (\cref{fig:dp1}) or to perform branching (\cref{fig:dp2}).
By repeating this process recursively up to $d=1$, we can identify the overall optimal configuration of the CLBF.
In this process, the factor $\prod_{j=1}^{d} f^{(t)}_j$ plays a critical role.
As detailed in the previous section, the proportion of non-key queries processed by $\mathrm{ML}_d$ is given by $h^{(t)}_d \prod_{j=1}^{d} f^{(t)}_j$.
Similarly, the proportion of non-keys handled by components below $\mathrm{ML}_d$ (e.g., $\mathrm{TBF}_{d+1}$, $\mathrm{BBF}_d$, and $\mathrm{FBF}$s) is also proportional to $\prod_{j=1}^{d} f^{(t)}_j$.
Therefore, the dynamic programming function we define below, $\mathrm{dp}(d, T)$, takes as inputs not only $d$ but also $T$, where $T = \prod_{j=1}^{d-1} f^{(t)}_j$.

% では、数式を用いて我々の動的計画法のエッセンスを記述する（詳細な数式表現はAppendixで与えている）。
% $\mathrm{dp}(d, T)$は、``the minimum value of the objective function under $\mathrm{TBF}_d$, subject to the constraint that $\prod_{j=1}^{d-1} f^{(t)}_j = T$.''である。
% ...

Now, we describe the essence of our dynamic programming method using mathematical formulas (detailed explanations are given in \cref{app: dp}).
The function $\mathrm{dp}(d, T): \{1,2,\dots,\bar{D}\} \times (0, 1] \rightarrow \mathbb{R}$ is defined intuitively as the minimum value of the objective function under $\mathrm{TBF}_d$, subject to the constraint that $\prod_{j=1}^{d-1} f^{(t)}_j = T$.
More precisely, $\mathrm{dp}(d, T)$ is defined as the solution to the following optimization problem:
\begin{equation}
    \label{eq:dp}
    \min_{D, \mathcal{F}} \Biggl( \lambda \cdot \frac{M_d(D,\mathcal{F})}{M_\mathrm{BF}} +
    (1-\lambda) \cdot \frac{R_d(D,\mathcal{F})}{R_\mathrm{BF}} \Biggr)
\end{equation}
\begin{equation}
    \label{eq:dp_constraint}
    \text{s.t. } \prod_{j=1}^{d-1} f^{(t)}_j = T,
\end{equation}
where $M_d(D,\mathcal{F})$ is defined as
\begin{multline}
    \label{eq:memory_size_under_bfi}
    M_d(D,\mathcal{F}) = \sum_{i=d}^{D} \mathrm{Size}(\mathrm{ML}_i) + \sum_{i=d}^{D} \mathrm{Size}(\mathrm{TBF}_i) +\\ \sum_{i=d}^{D-1} \mathrm{Size}(\mathrm{BBF}_i) + \sum_{k=1}^{K} \mathrm{Size}(\mathrm{FBF}_k),
\end{multline}
and $R_d(D,\mathcal{F})$ is defined as
\begin{equation}
    \label{eq:reject_time_under_bfi}
     R_d(D,\mathcal{F}) = \sum_{i=d}^{D} \left(\mathrm{Time}(\mathrm{ML}_i) \cdot h^{(t)}_i \prod_{j=1}^{i} f^{(t)}_j \right).
\end{equation}
By substituting $d=1, T=1$ into \cref{eq:dp,eq:dp_constraint}, we can see that $\mathrm{dp}(1,1)$ is equivalent to the minimum value of \cref{eq:objective} because the constraint $\prod_{j=1}^{d-1} f^{(t)}_j = T$ is no longer in effect and the objective function in \cref{eq:dp} is the same as \cref{eq:objective} when $d=1$ and $T=1$.

% 私たちは、$dp(d,T)$を、二つの場合を考えて、そのうち良い方を採用することによって再帰的に計算する。
% 1つ目の場合は$D=d$である場合、すなわち、$\mathrm{ML}_d$の直下を$\mathrm{FBF}$sとする場合である（Fig）。
% 我々は条件$D=d$を\cref{eq:dp_constraint}に追加した時の、\cref{eq:dp}の最適解を$\hat{\mathrm{dp}}(d,T)$と定義する。
% 2つ目の場合は$D>d$である場合、すなわち、$\mathrm{ML}_d$の直下ではbranchingを行い$\mathrm{TBF}_{d+1}$と$\mathrm{BBF}_{d}$を配置する場合である（Fig）。
% 我々は条件$D=d$を\cref{eq:dp_constraint}に追加した時の、\cref{eq:dp}の最適解を$\check{\mathrm{dp}}(d,T)$と定義する。
% すると、$dp(d,T)$は以下のように再帰的に計算することができる。

We calculate $\mathrm{dp}(d, T)$ recursively by evaluating two distinct cases and select the better one:
(1) $D = d$, where the immediate children of $\mathrm{ML}_d$ are $\mathrm{FBF}$s (\cref{fig:dp1}), and 
(2) $D > d$, where the two-way branching is performed directly under the $\mathrm{ML}_d$ (\cref{fig:dp2}).
We define $\hat{\mathrm{dp}}(d, T)$ as the solution to the optimization problem (\cref{eq:dp,eq:dp_constraint}) under the additional constraint $D = d$.
Similarly, we define $\check{\mathrm{dp}}(d, T)$ as the solution under the additional constraint $D > d$.
Then, we can compute $\mathrm{dp}(d, T)$ as follows:
\begin{equation}
    \label{eq:dp_recursive}
    \mathrm{dp}(d, T) = \begin{dcases}
        \hat{\mathrm{dp}}(d, T) & d = \bar{D}, \\
        \min\left(\hat{\mathrm{dp}}(d, T), \check{\mathrm{dp}}(d, T)\right) & \text{(else)}.
    \end{dcases}
\end{equation}
Note that when $d = \bar{D}$, the second case $\check{\mathrm{dp}}(d, T)$ is not evaluated because the number of machine learning models given is $\bar{D}$, and no further branching is possible.

% 私たちは、\hat{\mathrm{dp}}(d, T)や（近似的に）計算することができます、なぜならば、$T=\prod_{j=1}^{d-1} f^{(t)}_j$が深さ$d$の構成を決める上で十分な情報を提供するからです(\cref{fig:dp1})。
% $\mathrm{TBF}_d$の偽陽性率$f^{(t)}_d$を固定すると、PLBFのやり方でそれぞれの$\mathrm{FBF}$の最適に近い偽陽性率を求めることができます。
% $f^{(t)}_d$は$(0,1]$に含まれるあらゆる実数がありえますが、この連続空間で最適化するのは難しいです。
% そこで、我々は、ここでは$P \in \mathbb{N}$通りの離散化された値を試すことにします。
% 具体的には、ある定数$p \in (0,1)$について、$f^{(t)}_d \in \{p^0, p^1, \dots, p^{P-1}\}$の$P$通りの値を試します。
% それぞれについて、最適な$\mathrm{FBF}$sの偽陽性率を求め、そして、目的関数\cref{eq:dp}を計算し、最も小さいものを選ぶことで$\hat{\mathrm{dp}}(d, T)$を計算することができます。

% $\check{\mathrm{dp}}(d, T)$についても同様のやり方で近似的に計算することができます。
% $\mathrm{BBF}_d$についてはPLBFと同様のやり方で最適な偽陽性率を設定できます。
% $\mathrm{TBF}_{d+1}$とそれよりも深い部分については、$\mathrm{dp}(d+1, T f^{(t)}_d)$によって最適な構成を選択することができます（$\prod_{j=1}^{d} f^{(t)}_j = T f^{(t)}_d$であることに注意）。

We can (approximately) compute the values of $\hat{\mathrm{dp}}(d, T)$ because $T = \prod_{j=1}^{d-1} f^{(t)}_j$ provides sufficient information to determine the configuration at depth $d$ (\cref{fig:dp1}).
For a fixed $f^{(t)}_d$, i.e., the false positive rate of $\mathrm{TBF}_d$, the expected proportions of keys and non-keys reaching each $\mathrm{FBF}$ can be determined.
Thus, we can find the optimal false positive rate of each $\mathrm{FBF}$ using the method in PLBF~\cite{vaidya2021partitioned} (please refer to \cref{app: dp} for a detailed explanation).
Although $f^{(t)}_d$ can take any real value in $(0,1]$, optimizing it in this continuous space is challenging.
To address this, we evaluate $f^{(t)}_d$ over a finite set of values, $f^{(t)}_d \in \{p^0, p^1, ..., p^{P-1}\}$, for constants $p \in (0,1)$ and $P \in \mathbb{N}$ (in our experiment, we set $p=0.5, P=20$).
For each candidate $f^{(t)}_d$, we compute the optimal false positive rates for the $\mathrm{FBF}$s, evaluate the objective function in \cref{eq:dp}, and then select the candidate that yields the smallest objective value to determine $\hat{\mathrm{dp}}(d, T)$.

We can also calculate the value of $\check{\mathrm{dp}}(d, T)$ approximately in a similar way.
For $\mathrm{BBF}_d$, we can compute the optimal false positive rate using the PLBF method, just as we do when calculating $\hat{\mathrm{dp}}$.
For $\mathrm{TBF}_{d+1}$ and deeper parts, we can determine the optimal configurations using $\mathrm{dp}(d+1, T f^{(t)}_d)$.
This is because, by the definition of $\mathrm{dp}$, $\mathrm{dp}(d+1, T f^{(t)}_d)$ represents the objective function for $\mathrm{TBF}_{d+1}$ and deeper parts when they are optimally configured under the constraint $\prod_{j=1}^{d} f^{(t)}_j = T f^{(t)}_d$.
Since $T = \prod_{j=1}^{d-1} f^{(t)}_j$, the constraint is satisfied, allowing us to use $\mathrm{dp}(d+1, T f^{(t)}_d)$ to select the optimal configurations for $\mathrm{TBF}_{d+1}$ and deeper parts.

% 以上の動的計画法によって、用いる機械学習の個数が可変であり、かつ、Reject Timeの期待値も考慮するという、既存の研究たちが取り組んできた最適化問題よりも難しい問題を、効率的に解くことができる。
% 適切な実装上の工夫を行うことで、この動的計画法の計算量は$\mathcal{O}(\bar{D}P^2 + \bar{D}PK)$となる（詳しくはAppendixで）。
% $\bar{D},P,K$は通常、高々10~100程度、であるため、これは十分高速である。

Using the dynamic programming approach described above, we can efficiently tackle a more complex problem than those addressed in existing research.
Specifically, our setting introduces two additional challenges: the number of machine learning models is variable, and the expected reject time is explicitly considered.
With appropriate implementation, the computational complexity of this dynamic programming approach becomes $\mathcal{O}(\bar{D}P^2 + \bar{D}PK)$ (see \cref{app: dp} for details).
Since $\bar{D}$, $P$, and $K$ are typically no more than 100, the computation is sufficiently fast.


\section{Experiments}
\label{sec:experiments}

% この章では、CLBFのメモリ効率,Reject Time,Construction Timeを、Bloom filterと，既存のLearned Bloom filterたち（sandwiched LBF、PLBF）と比較することで実験的に評価する。
% Learned Bloom filtersのメモリ使用量には、機械学習モデルのそれも含まれる。
% 以下でtraining dataとは機械学習モデルの訓練に用いたデータ, validation dataとはLBFの構成を決定するために使われたデータ, test dataとはBFやLBFの精度やReject Timeを計測するために使われたデータを指す．
% 我々は以下の2種類のデータセットを用いて実験を行った:

% \textbf{Malicious URLs Dataset}: 過去のLearned Bloom filterの研究たち~\citep{dai2020adaptive, vaidya2021partitioned}にのっとり，我々はMalicious URLs Dataset~\citep{manu2021urlDataset}を用いた．このデータセットには223,088個の悪性なURLと428,103個の良性のURLが含まれる．悪性なURLの全てからなる集合が，Bloom filtersが保持すべき集合$\mathcal{S}$である．我々は良性のURLたちのうち，80\%をtraining data, 10\%をvalidation data,，10\%をtest dataとした(互いに排反)．すべての悪性のURLはtraining dataとvalidation dataとして使われた．

% \textbf{EMBER Dataset}: 過去のLearned Bloom filterの研究に~\citep{vaidya2021partitioned, sato2023fast}のっとり，我々はEMBER Dataset~\citep{anderson2018ember}を用いた．このデータセットには400,000個の悪性なファイルと400,000個の良性なファイルの，ベクタライズされた特徴とそのsha256が含まれている(unlabeledな200,000件のファイルは今回の実験では用いなかった)．我々は良性のファイルたちのうち，10\%をtraining data, 10\%をvalidation data,，80\%をtest dataとして用いた（互いに排反）．悪性のファイルの10\%はtraining dataとして用い，全ての悪性ファイルはvalidation dataとして使われた．EMBERデータセットは特徴量数が多く，training dataのサンプル数が多いと訓練に非常に時間がかかるため，このような分配としている．

In this section, we evaluate the memory efficiency and reject time of CLBF by comparing it with a standard Bloom filter and existing LBFs.
The memory usage of LBFs is calculated as the sum of the memory consumed by the machine learning model and the Bloom filters within it.
Here, training data refers to the data used for training the machine learning model, validation data refers to the data used for configuring the LBF, and test data refers to the data used to measure the accuracy and reject time of the Bloom filters. We conducted experiments using the following two datasets:

\textbf{Malicious URLs Dataset}: 
Following the previous studies on LBFs~\citep{dai2020adaptive, vaidya2021partitioned}, we used the Malicious URLs dataset~\citep{manu2021urlDataset}.
This dataset contains 223,088 malicious URLs and 428,103 benign URLs. The set of all malicious URLs constitutes the set $\mathcal{S}$, which the Bloom filters aim to store. 
We extracted 20 lexical features, including URL length, use of shortening services, and number of special characters, and used them to train a machine learning model.
We divided the benign URLs into 80\% as training data, 10\% as validation data, and 10\% as test data. All malicious URLs were used as training data and validation data.

\textbf{EMBER Dataset}:
Following the previous studies on LBFs~\citep{vaidya2021partitioned, sato2023fast}, we used the EMBER dataset~\citep{anderson2018ember}.
This dataset contains 400,000 malicious files and 400,000 benign files, their vectorized features, and sha256 hashes (the 200,000 unlabeled files were not used in our experiment).
We used 10\% of the benign files as training data, 10\% as validation data, and 80\% as test data.
For the malicious files, 10\% were used as training data, while all were used as validation data.
This split ratio was adopted to avoid excessive training time due to the high dimensionality of the features in the EMBER dataset.

% 機械学習モデルとしては任意のモデルを用いることができるが、我々は勾配ブースティングの有名な実装であるXGBoostを用いた．
% XGBoostにおける弱学習器一つ一つが提案手法における機械学習モデル$\mathrm{ML}_1, \dots, \mathrm{ML}_D$に対応する．
% 我々はまず\cref{sec:exp:memory_accuracy_trade_off}でメモリ使用量と精度のトレードオフを評価した(これは$F$によって制御されるトレードオフである)．
% 次に\cref{sec:exp:memory_reject_time_trade_off}でメモリ使用量とクエリ時間のトレードオフを評価した(これは$\lambda$によって制御されるトレードオフである)．
% 最後に\cref{sec:exp:construction_time}で構築にかかる時間とその内訳を示した．

All experiments were implemented in C++ and conducted on a Linux machine equipped with an Intel\textsuperscript{\textregistered}~Core\texttrademark{}~i9-11900H CPU @ 2.50\,GHz and 64\,GB of memory.
The code was compiled using GCC version 11.4.0 with the \texttt{-O3} optimization flag, and all experiments were performed in single-threaded mode.
Although any machine learning model can be used, we employed XGBoost~\citep{chen2016xgboost}, a widely used implementation of gradient boosting.
Each weak learner in XGBoost corresponds to each machine learning model $\mathrm{ML}_1, \dots, \mathrm{ML}_{\bar{D}}$ in our proposed method.
The number of weak learners in XGBoost, i.e., $\bar{D}$, is equal to \texttt{num\_round}, a training parameter in XGBoost that specifies the number of boosting rounds.
To clearly demonstrate the effectiveness of our CLBF, we have presented results from a straightforward comparison with a subset of the baseline methods. For more comprehensive experimental results, please refer to \cref{app: ablation study on hyperparameters of ML,app: other baseline,app: analysis of model filter balance,app: diverse learnablity}.

\subsection{Memory and Accuracy}
\label{sec:exp:memory_accuracy_trade_off}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{fig/memory_fpr2.pdf}
    \caption{Trade-off between memory usage and accuracy (lower-left is better): CLBF achieves equal to or better memory efficiency than any other PLBF with $D$.}
    \label{fig:memory_accuracy_trade_off}
\end{figure*}

% 我々はCLBFのメモリ使用量と精度のトレードオフをBloom Fitlerと，既存のLBF, PLBF, と比較した(sandwiched LBF はこのトレードオフの観点ではPLBFに劣るのでここでは省略している)．
% このトレードオフはハイパーパラメータ$F$を$0.1$から$0.001$まで動かすことによって制御される．
% ここでは，$\lambda=1.0$とした(このとき，CLBFは棄却時間を考慮せず，メモリ効率のみを最適化する)．
% CLBFの構築に用いるXGBoostの学習回数（我々の手法の$\bar{D}$に対応する）は$100$とした．
% PLBFについては，学習回数$D\in\{1,10,100\}$のXGBoostを用いて構築したものの結果を載せた(PLBFにはモデルサイズを自動で調整する機構が備わっていないことに注意されたい)．

We compared the trade-off between memory usage and accuracy of CLBF with that of a standard Bloom filter and existing LBFs, specifically PLBF~\citep{vaidya2021partitioned}.
Other baselines, such as sandwiched LBF~\citep{mitzenmacher2018model}, are omitted here because they perform worse than PLBF in this trade-off.
This trade-off is controlled by varying the hyperparameter $F$ from 0.1 to 0.001. 
In this evaluation, we set $\lambda=1.0$, meaning CLBF is optimized solely for memory efficiency.
For CLBF, we set $\bar{D}$, i.e., the number of boosting rounds in XGBoost, to 100. 
For PLBF, we present results using XGBoost with boosting rounds $D \in \{1, 10, 100\}$, because PLBF lacks a mechanism for automatically adjusting the model size.

% \cref{fig:memory_accuracy_trade_off}はその結果を示す．
% CLBFは，既存のどのnon-learnedな手法よりも良いメモリ効率を達成している．
% Malicious URLs Dataset においては，PLBFは$D=1$もしくは$D=10$の時、最適なメモリ効率となっており，CLBFはそれとほとんど同じメモリ効率となっている．
% 実際，ここでCLBFが最適化の結果選択した$D$は$1 \leq D \leq 18$であった．
% 一方で，EMBER Datasetでは，PLBFは，$F>10^{-2}$のときは$D=10$の時，$F<10^{-2}$のときは$D=100$の時，優れたメモリ効率となっている．
% CLBFはそのどちらの$D$の設定のPLBFよりも優れたメモリ効率を達成している．
% ここでCLBFが最適化の結果選択した$D$は$18 \leq D \leq 38$であった($F=10^{-1}$のとき$D=18$で$F=10^{-3}$のとき$D=38$)．
% EMBER Datasetで$F=0.01$のとき，CLBFはPLBFの$24\%$のメモリ使用量削減を達成している．
% このように，データセットや$F$によって最適なメモリ効率を達成するモデルサイズ$D$は異なり，我々のCLBFは自動的に最適な$D$を選択することができていることが分かる．

\cref{fig:memory_accuracy_trade_off} presents the results.
It shows that CLBF consistently achieves equal or better memory efficiency than the classic Bloom filter and PLBF, while CLBF does not need to evaluate different $D$ values as PLBF does. 
In the Malicious URLs dataset, PLBF achieves optimal memory efficiency with $D=1$ or $D=10$, and CLBF closely matches this efficiency.
On the other hand, in the EMBER dataset, PLBF performs best with $D=10$ when $F>10^{-2}$ and with $D=100$ when $F\leq10^{-2}$.
Across all values of $F$, CLBF has better memory efficiency than PLBF.
At $F=0.01$, CLBF achieves a 24\% reduction in memory usage compared to PLBF with $D=10$ and $D=100$. 
These results indicate that the optimal model size $D$ for achieving the best memory efficiency varies depending on both the dataset and the value of $F$, and that our CLBF can automatically select the optimal $D$.

\subsection{Memory and Reject Time}
\label{sec:exp:memory_reject_time_trade_off}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{fig/memory_query_time2.pdf}
    \caption{Trade-off between memory usage and average reject time (lower-left is better): Compared to PLBF with similar memory efficiency, CLBF achieves a significantly shorter average reject time (up to 14 times shorter).}
    \label{fig:exp:memory_reject_time_trade_off}
\end{figure*}

% 我々はCLBFのメモリ使用量と棄却時間のトレードオフを既存のlearned/non-learned手法たちと比較した．
% CLBFのこのトレードオフはハイパーパラメータ$\lambda$を$0$から$1$まで動かすことによって制御される．
% $F$は全ての手法で，$0.001$とした．
% XGBoostの学習回数（我々の手法の$\bar{D}$に対応する）はCLBFについては$100$を用いた．
% Bloom filterにはこのトレードオフを制御するパラメータが存在しないため，点で表されている．
% 他のLBFについては，$D$を$1$から$100$まで動かしたときの，メモリ使用量と棄却時間の変化を観測した．

We compared the trade-off between memory usage and reject time in CLBF against existing LBFs and the standard Bloom filter.
This trade-off in CLBF is controlled by varying the hyperparameter $\lambda$ within the range $[0, 1]$.
We set the false positive rate $F$ for all methods to 0.001. The number of XGBoost boosting rounds (corresponding to $\bar{D}$ in our method) was set to 100 for CLBF. Since the standard Bloom filter lacks a parameter to control this trade-off, it is represented as a point. For sandwiched LBF and PLBF, we observed the changes in memory usage and reject time when varying $D$ from 1 to 100.

% \cref{fig:exp:memory_reject_time_trade_off}はその結果を示す．
% まず，CLBFのプロットはほとんど全ての点が「最も左下」に位置していることが分かる．
% すなわち，ほとんど全てのCLBFについて，それよりも省メモリでかつReject Timeが短いようなlearned/non-learned Bloom Fitlerは，CLBF自身を含めてほとんど存在しない．
% それとは対照的に，PLBFやsandwiched LBFが形成する曲線の一部は右上へ伸びている．
% これは，弱学習器の個数$D$をある値より大きくすると，メモリ効率とReject timeの両方が悪化することを表している．
% このような境目となる$D$の値を事前に知ることは不可能であるため，慎重に$D$を選択しなければ，メモリ効率とReject Timeの両方の観点で無駄な学習器を保有したLBFを構築してしまう可能性があるといえる．

% sandwiched LBFはReject時間はPLBFに比べて短い傾向がある一方で，メモリ効率はPLBFやCLBFよりも悪い．
% 例えばEMBER Datasetでは，CLBFは最小でおよそ$400\,\mathrm{kB}$となっているが，sandwiched LBFは$500\,\mathrm{kB}$以下のものがない．
% また，PLBFはCLBFと同じくらいのメモリ効率を達成できる一方で，Reject Timeがとても遅いという欠点がある．
% EMBER Dataset におけるメモリ使用量が$500{\,kB}$付近では，CLBFはPLBFより棄却時間が約14倍短い．

\cref{fig:exp:memory_reject_time_trade_off} illustrates the results. 
We can see that CLBF greatly outperforms sandwiched LBF in terms of memory efficiency, and that CLBF greatly outperforms PLBF in terms of reject time.
While the reject time of sandwiched LBF tends to be shorter than that of PLBF, its memory efficiency is inferior to both PLBF and CLBF. For example, in the EMBER dataset, CLBF achieves a minimum memory usage of approximately $400\,\mathrm{kB}$, whereas no sandwiched LBF configuration uses less than $500\,\mathrm{kB}$. 
On the other hand, although PLBF can achieve comparable memory efficiency to CLBF, it suffers from significantly longer reject times. Specifically, when the memory usage is around $500\,\mathrm{kB}$ in the EMBER dataset, the average reject time of CLBF is approximately 14 times shorter than that of PLBF.

Additionally, we can see that the CLBF plot forms a curve that is almost a Pareto front.
In most cases, no other methods can achieve both lower memory usage and shorter reject time than CLBF.
For CLBF itself, improving (or worsening) memory efficiency leads to a corresponding worsening (or improvement) in reject time.
In contrast, for other LBFs, both the memory efficiency and reject time worsen as $D$ becomes too large.
Since it is impossible to know this turning point for $D$ in advance, an imprudent choice of $D$ risks constructing an LBF that is inefficient in terms of both memory and reject time.
Our CLBF has the advantage of avoiding the risk of setting such a needlessly large $D$.

% \subsection{Construction Time}
% \label{sec:exp:construction_time}

% \begin{figure*}[t]
%     \centering
%     \begin{minipage}[t]{0.32\textwidth}
%         \vspace{0pt}
%         \includegraphics[width=\textwidth]{fig/time_hist_url.pdf}
%         \subcaption{Malicious URLs Dataset}
%     \end{minipage}
%     \begin{minipage}[t]{0.32\textwidth}
%         \vspace{0pt}
%         \includegraphics[width=\textwidth]{fig/time_hist_ember.pdf}
%         \subcaption{EMBER Dataset}
%     \end{minipage}
%     \begin{minipage}[t]{0.25\textwidth}
%         \vspace{0pt}
%         \includegraphics[width=\textwidth]{fig/time_hist_legend.pdf}
%     \end{minipage}
%     % \caption{構築時間: CLBFは，同じ大きさの機械学習モデル($D=100$)を用いた既存のLBFの構築時間から見ると10\%から41\%程度の追加の計算時間を必要としている．}
%     \caption{Construction time: CLBF requires additional computation time of 10\% to 41\% compared to the construction time of existing LBFs using the same size machine learning model ($D=100$).}
%     \label{fig:exp:time_hist}
% \end{figure*}

% % CLBFは，既存のLBF（e.g., sandwiched LBFやPLBF）を包含するようなフレームワークのもとで，最もよい構成を，動的計画法を用いて効率的に選択している．
% % ここでは，CLBFの構築の構築に要する時間を，既存のLBFやBloom filterと比較する(original PLBFではなく，fast PLBF~\citep{sato2023fast}という，PLBFと同じデータ構造をより高速に構築する手法と比較していることに注意されたい)．
% % \cref{fig:exp:time_hist}に，その結果を示す．
% % ここで，Scoring Timeは，Validationデータを流して，それぞれのサンプルがそれぞれの機械学習モデルに対してどのようなスコアを取るかを計測するのにかかる時間を指す．
% % Configuration Timeは，Validationの結果を用いて，最適な構成を選択するための計算に要する時間を指す．
% % CLBFの場合はConfigurationとして，\cref{sec:dp_solution}で記述した動的計画法を行う．
% % fast PLBFやsandwiched LBFの場合も，CLBFの場合と同様に，Validationの結果を用いて，最適な閾値や各Bloom filterの偽陽性率の設定を行う．

% This section compares the time required to construct CLBF with other existing LBFs and the standard Bloom filter.
% The comparison is made with fast PLBF~\citep{sato2023fast}, a method that constructs the same data structure as PLBF more quickly.

% The results are shown in \cref{fig:exp:time_hist}.
% Here, ``Scoring Time'' refers to the time taken to measure the score of each sample against each machine learning model by passing validation data through them.
% ``Configuration Time'' refers to the time required to compute the optimal configuration using the results of the scoring phase.
% In the case of CLBF, the configuration process involves dynamic programming, as described in \cref{sec:dp_solution}.
% Similarly, the configuration for sandwiched LBF and fast PLBF involves determining the optimal thresholds and false positive rates for each Bloom filter based on the scoring.

% % 結果はCLBFは他の既存のLBFよりも長いconfiguration timeを要していることを示している。
% % 同じML modle sizeを使う既存のLBFsと比較して、CLBFは1.1倍から1.4倍の構築時間を要している。
% % しかし、我々はこれは小さい欠点だと信じている。
% % なぜなら、sandwiched LBFの時点ですでにstandard Bloom filterよりも100倍近く長い構築時間となっており、いまさら構築時間が1.4倍になったところで、実用上、大きな問題があるとは考えにくいからである。
% % むしろ、CLBFがとても大きい探索空間（一般化されたアーキテクチャのもとでD=1,\dots,100の全てのケース）のなかから最適なconfigurationを選択していることを考えれば、これは動的計画法の効率さを示しているといえる。

% The results indicate that CLBF requires a longer configuration time than other existing LBFs.
% Compared to the construction time of existing LBFs with the same machine learning model size ($D=100$), the construction time of CLBF is approximately 10\% to 40\% longer.
% However, we believe that this additional overhead is a minor drawback.
% {\color{blue}
% Considering that the construction time for the smallest sandwiched LBF is already about 100 times longer than that of a standard Bloom filter, we can assume that LBF is not something that is used in scenarios where construction speed is sensitive. 
% LBFs should be used in contexts where the frequency of reconstruction is low (once an hour or less). 
% For example, the (learned) Bloom filter used to filter malicious URLs does not need to be rebuilt frequently because the set of malicious URLs does not change that quickly. 
% In such cases, the construction time of CLBF, which is 1.4 times longer than that of the sandwiched LBF, is not a problem, and the benefits of the optimal configuration obtained by searching virtually all cases of $D \in \{1,2,\dots,100\}$ are considered to be greater.
% }
% % However, we believe that this additional overhead is a minor drawback.
% % Given that the construction time for the sandwiched LBF is around 100 times longer than that of a standard Bloom filter, an increase in construction time by about 40\% is unlikely to have a significant impact on practical applications.
% % Moreover, considering that CLBF selects the optimal configuration from a much larger search space (considering all $D \in \{1,2,\dots,100\}$ on generalized architectures), this demonstrates the efficiency of the dynamic programming.

\section{Limitations and Future Work}
\label{sec:limitation}

% 我々の手法は，ブースティングのような多数の弱学習器からなる学習モデルと相性が良い一方で，1つの大きな学習器からなるような学習モデル（e.g., 1つの深層学習モデル）にはそのままでは適用できない．
% このような1つの大きな学習器に適用できるように、我々の手法を発展するためには，その中間層から暫定的なスコアを出力する追加の機構が必要となる．
% その機構の追加によるメモリ的・時間的オーバーヘッドをどのように調整するかは，今後解決すべき課題である．
% また，我々の最適化の手法は，中間層の閾値, i.e., $\theta_1, \dots, \theta_{D-1}$, は必ずしも最適なものを選択できていない．
% 我々の実験結果は，いくつかの閾値の組の候補の中から最も成績が良かったものを選ぶ方法で十分既存のLBFを上回るメモリ効率・Reject Timeを得ることができるを示しているが，この閾値の最適化を改良することで，さらにメモリ効率よく・rejectが高速になる可能性がある．
% 中間層のBloom filterの偽陽性率, i.e., $\bm{f}^{(t)}$, についても、今は、ある粒度での離散化を行ったうえで最適なものを探索しているが、将来的にはより厳密な解の探索がより良い結果をもたらすかもしれない。
% また，Configurationにかかる時間はPLBFやsandwiched LBFなどの既存手法と比較して長い．
% 一部のアプリケーションでは，これは問題になる可能性があるため，最適化の高速化も重要なfuture workである．

Our current optimization method does not always select the optimal intermediate layer thresholds, i.e., $\bm{\theta}$. While our experimental results demonstrate that selecting the best-performing thresholds from a set of candidates is sufficient to outperform existing LBFs in terms of memory efficiency and reject time, further improvements in threshold optimization could lead to even greater performance gains. Furthermore, while we currently discretize the false positive rates of Trunk Bloom filters, i.e., $\bm{f}^{(t)}$, at a certain granularity, future work could achieve better results by exploring more precise solutions.

Additionally, although our method is highly compatible with learning models composed of multiple weak learners, such as boosting, it cannot be directly applied to models composed of a single large learner (e.g., a single deep learning model).
To extend our approach for such models, it is necessary to introduce an additional mechanism that outputs tentative scores from intermediate layers.
Here, a critical challenge is mitigating the memory and time overhead introduced by this mechanism. 
A novel optimization algorithm tailored to this framework may be required, and promising directions include leveraging advanced generic optimization techniques, such as Bayesian optimization or Adam.


% Moreover, the configuration time of our approach is longer compared to existing methods such as sandwiched LBF or PLBF, which may pose a problem for specific applications. Therefore, accelerating the optimization process is another important direction for future work.

\section{Conclusion}
\label{sec:conclusion}
% 本研究で提案したCLBFは既存のLBFが直面する2つの重要な問題に対する効果的な解決策を示した．
% (1)大きめに学習した機械学習モデルを適切なだけ削除する方法によって，最適な機械学習モデルとフィルタのサイズバランスを達成した．
% (2)暫定的なスコアによる条件分岐と，中間に挟んだBloom filterによる早期のフィルタリングによって，棄却時間を大幅に短縮した．
% これによりLBFの応用範囲を広げるとともに，これらの問題の解決に向けた有力な基盤を築いた．

In this research, we proposed CLBF, tackling two critical issues existing LBFs face.
(1) By training a large machine learning model and reducing it optimally, CLBF achieves an optimal balance between model and filter sizes, minimizing overall memory usage.
(2) By branching based on tentative scores and the insertion of intermediate Bloom filters, CLBF significantly reduces reject time.
As a result, CLBF not only broadens the applicability of LBFs but also establishes a strong foundation for addressing these issues.

\clearpage

% Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% We thank the anonymous reviewers for their constructive comments. This work was supported by JST AIP Acceleration Research JPMJCR23U2, Japan.

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include acknowledgements.  Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.

\section*{Impact Statement}

This paper presents work whose goal is to advance the field of 
Machine Learning. There are many potential societal consequences 
of our work, none which we feel must be specifically highlighted here.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{main}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Algorithm Details}
\label{app: algorithm}

\begin{algorithm}[t]
\caption{Key Insertion into CLBF}
\label{alg:key_insertion}
\begin{algorithmic}[1]
\State \textbf{Input:} Key $q$
\State \textbf{Function:} $\mathrm{GetFBFIndex}(s)$ returns the index of $\mathrm{FBF}$ corresponding to the score $s$.
\vspace{0.5em}
\For{$d = 1, 2, \dots, D$}
    \State $\mathrm{TBF}_d.\mathrm{Insert}(q)$
    \State $s \gets \mathrm{ML}_d(q)$
    \If{$d = D$}
        \State $k \gets \mathrm{GetFBFIndex}(s)$
        \State $\mathrm{FBF}_k.\mathrm{Insert}(q)$
        \State \textbf{break}
    \EndIf
    \If{$s \geq \theta_d$}
        \State $\mathrm{BBF}_d.\mathrm{Insert}(q)$
        \State \textbf{break}
    \EndIf
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[t]
\caption{Query Processing in CLBF}
\label{alg:query_processing}
\begin{algorithmic}[1]
\State \textbf{Input:} Query $q$
\State \textbf{Output:} $\mathrm{NotFound}$ or $\mathrm{Found}$
\State \textbf{Function:} $\mathrm{GetFBFIndex}(s)$ returns the index of $\mathrm{FBF}$ corresponding to the score $s$.
\vspace{0.5em}
\For{$d = 1, 2, \dots, D$}
    \If{$\mathrm{TBF}_d(q) = \mathrm{NotFound}$}
        \State \textbf{return} $\mathrm{NotFound}$
    \EndIf
    \State $s \gets \mathrm{ML}_d(q)$
    \If{$d = D$}
        \State $k \gets \mathrm{GetFBFIndex}(s)$
        \State \textbf{return} $\mathrm{FBF}_k(q)$
    \EndIf
    \If{$s \geq \theta_d$}
        \State \textbf{return} $\mathrm{BBF}_d(q)$
    \EndIf
\EndFor
\end{algorithmic}
\end{algorithm}

% ここでは、本文中で省略したCLBFの詳細なアルゴリズムの記述を与える。
% まず、\cref{app: algorithm insert and query}で、CLBFにkeyを挿入したり、CLBFでクエリを処理するアルゴリズムについて、疑似コードと共に説明する。
% 次に、\cref{app: dp}で、動的計画法の詳細なアルゴリズムと、その詳細な実装方法について記述する。

Here, we give a detailed description of the CLBF algorithm, which we omitted in the main text.
First, in \cref{app: algorithm insert and query}, we explain the algorithm for inserting keys into CLBF and processing queries with CLBF, along with pseudocode.
Then, in \cref{app: dp}, we describe the detailed algorithm for the dynamic programming method to determine the optimal configuration of CLBF.

\subsection{Algorithms for Key Insertion and Query Processing}
\label{app: algorithm insert and query}

% KeyをCLBFへinsertするアルゴリズムの疑似コードを\cref{alg:key_insertion}に示す．
% まず，$q$は1番目のTrunk Bloom filter, i.e., $\mathrm{TBF}_1$,にinsertされる．
% 次に，$q$に対する1番目の機械学習モデル, i.e., $\mathrm{ML}_1$,の出力スコアを得る．
% このスコアが$\mathrm{ML}_1$に対応する閾値$\theta_1$よりも大きい場合はBranch Bloom filterへ分岐する:
% すなわち，1番目のBranch Bloom filter, i.e., $\mathrm{BBF}_1$,へ$q$をinsertして終了となる．
% そうでない場合は次の層へ$q$が渡されていく．
% 最終層$i=D$までBranch Bloom filterへ分岐しなかった場合，その最終的なスコアに応じて適切なFinal Bloom filterへ$q$をinsertする．
% これらの操作をCLBFが保持する集合$\mathcal{S}$に含まれる全てのkey$q$に対して行う．

The pseudocode for the algorithm that inserts keys into the CLBF is shown in \cref{alg:key_insertion}. First, the key $q$ is inserted into the first Trunk Bloom filter, i.e., $\mathrm{TBF}_1$. Next, the output score from the first machine learning model, i.e., $\mathrm{ML}_1$, is obtained for the key $q$. If this score exceeds the threshold $\theta_1$ corresponding to $\mathrm{ML}_1$, the algorithm branches to a Branch Bloom filter; $q$ is inserted into $\mathrm{BBF}_1$, and the process terminates. Otherwise, $q$ is passed to the next depth. If $q$ does not branch into any Branch Bloom filters by the final depth $d = D$, it is inserted into the appropriate Final Bloom filter based on the final score. This process is repeated for all keys $q$ contained in $\mathcal{S}$, the set stored by the CLBF.

% 次に，CLBFがqueryをprecessするアルゴリズムの疑似コードを\cref{alg:query_processing}に示す．
% key insertの場合と同様に，まず，$q$は$\mathrm{TBF}_1$へ問い合わせられる．
% $\mathrm{TBF}_1$が$\mathrm{NotFound}$と回答した場合，$q\notin\mathcal{S}$であることが確定するので，即座にそのように回答する．
% そうでない場合は，$q$に対する$\mathrm{ML}_1$の出力スコアを得る．
% このスコアが閾値$\theta_1$よりも大きい場合はBranch Bloom filterへ分岐する:
% すなわち，$\mathrm{BBF}_1$へ$q$について問い合わせて，その回答を全体の回答とする．
% そうでない場合は，次の深さ$d$へ$q$が渡されていく．
% 最終層$d=D$までBranch Bloom filterへ分岐しなかった場合，その最終的なスコアに応じて適切なFinal Bloom filterへ$q$について問い合わせる．
% これにより，false negative freeという性質を守りながら，機械学習モデルの暫定的・最終的なスコアを活用して，高速にクエリに回答することを可能となる．

Next, the pseudocode for the query algorithm in the CLBF is shown in \cref{alg:query_processing}. Similar to key insertion, the query $q$ is first checked against $\mathrm{TBF}_1$. 
If $\mathrm{TBF}_1$ returns a $\mathrm{NotFound}$ result, it is certain that $q \notin \mathcal{S}$, and this result is returned immediately. Otherwise, the output score from $\mathrm{ML}_1$ is obtained for $q$. If this score exceeds the threshold $\theta_1$, the algorithm branches to the Branch Bloom filter; the algorithm queries $\mathrm{BBF}_1$ for $q$, and the result from this filter is used as the final result. Otherwise, $q$ is passed to the next depth $d$. If $q$ does not branch into any Branch Bloom filters by the final depth $d = D$, it is queried against the appropriate Final Bloom filter based on the final score. This approach leverages the tentative and final scores of the machine learning models to provide fast query responses while preserving the false-negative free property.

\subsection{Detailed Dynamic Programming Algorithm for Optimizing the CLBF Configuration}
\label{app: dp}

In this section, we explain the detailed dynamic programming algorithm for optimizing the CLBF configuration.
As explained in the main text, our optimization method obtains the optimal CLBF configuration by recursively calculating the value of $\mathrm{dp}(d,T)$, which is defined as the solution to the optimization problem (\cref{eq:dp,eq:dp_constraint}).
The value of $\mathrm{dp}(d,T)$ is calculated recursively using the two functions $\hat{\mathrm{dp}}$ and $\check{\mathrm{dp}}$ as in \cref{eq:dp_recursive}.
In the following, we first explain in detail the method for calculating $\hat{\mathrm{dp}}$, and then also explain how to calculate $\check{\mathrm{dp}}$.
Finally, we explain the implementation techniques and how they allow the computational complexity of this dynamic programming method to be $\mathcal{O}(\bar{D}P^2 + \bar{D}PK)$.


\paragraph{The Method for Calculating $\hat{\mathrm{dp}}$.}
$\hat{\mathrm{dp}}(d,T)$ is the optimal solution for the case where the $\mathrm{FBF}$s are placed directly under the $\mathrm{ML}_d$ (\cref{fig:dp1}).
In other words, $\hat{\mathrm{dp}}(d, T)$ is defined as the optimal solution of \cref{eq:dp,eq:dp_constraint} when the constraint $D = d$ is added to the constraint.
We determine the optimal $f^{(t)}_d$ and $\bm{f}^{(f)}$ by trying out several $f^{(t)}_d$ values.
For each fixed $f^{(t)}_d$, we determine the optimal $\bm{f}^{(f)}$, and then calculate the value of the objective function.
In the following, we first explain how to determine the optimal $\bm{f}^{(f)}$ for a fixed $f^{(t)}_d$, and then how to calculate the value of the objective function.

The important insight is as follows: 
For fixed $T$ and $f^{(t)}_d$, the expected values of the proportions of keys and non-keys going into each $\mathrm{FBF}_k$ can be determined.
For keys, this is exactly equal to $g^{(f)}_{d,k}$, since all keys pass the filtering by $\mathrm{TBF}$s due to the false-negative free property.
The expected proportion of non-keys entering $\mathrm{FBF}_k$ is $h^{(f)}_{d,k} T f^{(t)}_d$, because non-keys can reach the final layer if and only if they pass all filtering by $\mathrm{TBF}$s, whose false positives are $\bm{f}^{(t)}$.
Since $T = \prod_{j=1}^{d-1} f^{(t)}_j$, the expected proportion of non-keys entering $\mathrm{FBF}_k$ is $h^{(f)}_{d,k} \prod_{j=1}^{d} f^{(t)}_j = h^{(f)}_{d,k} T  f^{(t)}_d$.
Thus, for a fixed $T$ and $f^{(t)}_d$, we can calculate the expected values of the proportions of keys and non-keys that enter each of the $\mathrm{FBF}$s.

Using this insight, we can determine the optimal false positive rate for each $\mathrm{FBF}_k$.
We use the following insight from the PLBF paper~\cite{vaidya2021partitioned}:
When the proportion of keys and non-keys entering a Bloom filter (for making the final decision) is $g$ and $h$, respectively, we can minimize the overall memory usage by setting the false positive rate of this Bloom filter to be $Fg/h$, where $F$ is the overall target false positive rate.
Here, the term a Bloom filter for making the final decision refers to a Bloom filter such as $\mathrm{BBF}$ or $\mathrm{FBF}$, whose output result becomes the overall answer for LBF.
Following this insight, we can set the false positive rate of $\mathrm{FBF}_k$ to $(F g^{(f)}_{d,k}) / (h^{(f)}_{d,k} T f^{(t)}_d)$.
Here, we define two functions, $\tilde{f}(g, h)$ and $\tilde{s}(g, \epsilon)$, which we use to explain how to calculate the objective functions, as follows:
\begin{equation}
    \tilde{f}(g, h) = \min\left(\frac{Fg}{h}, 1\right), ~ \tilde{s}(g, \epsilon) = cng \cdot \log_{2}\left(\frac{1}{\epsilon}\right).
\end{equation}
In other words, $\tilde{f}(g, h)$ represents the false positive rate that is set for $\mathrm{FBF}$, where $g$ and $h$ are the expected proportions of keys and non-keys that are input to the Bloom filter, respectively.
$\tilde{s}(g, \epsilon)$ represents the memory usage of a Bloom filter with a false positive rate of $\epsilon$ that holds $n \cdot g$ keys. The constant $c = \log_{2}(e)$ for the standard Bloom filter, and if a variant of the Bloom filter is used instead of the Bloom filter as $\mathrm{TBF}$s, $\mathrm{BBF}$s, and $\mathrm{FBF}$s, then $c$ will be a different value.

With the above insight, we can evaluate the value of the objective function (\cref{eq:objective}) because we can find the optimal $\bm{f}^{(f)}$ for a fixed $T$ and $f^{(t)}_d$.
First, for the term $M_d(D,\mathcal{F})$, which is defined in \cref{eq:memory_size_under_bfi}, we can express it as follows:
\begin{equation}
    \label{eq:M_d req hat}
    M_d(D,\mathcal{F}) = \mathrm{Size}(\mathrm{ML}_d) + \tilde{s}(g^{(t)}_d, f^{(t)}_d) + \sum_{k=1}^{K} \tilde{s}\left(g^{(f)}_{d, k}, \tilde{f}(g^{(f)}_{d, k}, h^{(f)}_{d, k} T f^{(t)}_d)\right).
\end{equation}
We explain each term in \cref{eq:M_d req hat}, while corresponding to each term in the definition (\cref{eq:memory_size_under_bfi}):
\begin{itemize}
    \item The first term of \cref{eq:memory_size_under_bfi} is the first term of \cref{eq:M_d req hat} because $D=d$.
    \item The second term of \cref{eq:memory_size_under_bfi} is the second term of \cref{eq:M_d req hat} because $D=d$, the proportion of keys that go into $\mathrm{TBF}_d$ is $g^{(t)}_{d}$, and the false positive rate of the $\mathrm{TBF}_d$ is fixed at $f^{(t)}_d$ now.
    \item The third term in \cref{eq:memory_size_under_bfi} is 0 because $D=d$.
    \item The fourth term in \cref{eq:memory_size_under_bfi} is the third term of \cref{eq:M_d req hat} because, as we explained earlier, we set the false positive rate of the $\mathrm{FBF}_k$ to be $\tilde{f}(g^{(f)}_{d, k}, h^{(f)}_{d, k} T f^{(t)}_d)$.
\end{itemize}

Second, for the term $R_d(D,\mathcal{F})$, which is defined in \cref{eq:reject_time_under_bfi}, we can express it as follows:
\begin{equation}
    \label{eq:R_d req hat}
    R_d(D,\mathcal{F}) = \mathrm{Time}(\mathrm{ML}_d) \cdot h^{(t)}_d T f^{(t)}_d,
\end{equation}
because $D=d$ and $\prod_{j=1}^{d} f^{(t)}_j = T f^{(t)}_d$.

Following the above procedure, we can find the optimal $\bm{f}^{(f)}$ for fixed $T$ and $f^{(t)}_d$, and then compute the value of the corresponding value of the objective function.
We compute the value of the objective function for each of the $f^{(t)}_d \in \{p^0, p^1,\dots,p^{P-1}\}$.
Then we take the best of these results and set it as $\hat{\mathrm{dp}}(d,T)$.

\paragraph{The Method for Calculating $\check{\mathrm{dp}}$.}
$\check{\mathrm{dp}}(d, T)$ is the optimal solution for the case where the two-way branching is performed directly under the $\mathrm{ML}_d$ and the $\mathrm{TBF}_{d+1}$ and the $\mathrm{BBF}_{d}$ are arranged (\cref{fig:dp2}).
In other words, $\check{\mathrm{dp}}(d, T)$ is defined as the optimal solution of \cref{eq:dp,eq:dp_constraint} when the constraint $D > d$ is added to the constraint.
Similar to the case of $\hat{\mathrm{dp}}(d,T)$, we determine the optimal configuration under the $\mathrm{ML}_d$ by trying out several $f^{(t)}_d$ values.
We calculate the objective function for each fixed $f^{(t)}_d$, and then choose the optimal one.
In the following, we first explain how to determine the optimal configuration under the $\mathrm{ML}_d$, and then how to calculate the value of the objective function.

First, the optimal false positive rate for $\mathrm{BFB}_d$ is $\tilde{f}(g^{(b)}_d, h^{(b)}_d T f^{(t)}_d)$.
This is because the expected proportions of keys and non-keys going into the $\mathrm{BFB}_d$ is $g^{(b)}_d$ and $h^{(b)}_d T f^{(t)}_d$, respectively.
Following the insight of PLBF, we can obtain that the optimal false positive rate of the $\mathrm{BFB}_d$ is $\tilde{f}(g^{(b)}_d, h^{(b)}_d T f^{(f)}_d)$.
In addition, for the configuration of $\mathrm{TBF}_{d+1}$ and below, the result of $\mathrm{dp}(d+1, T f^{(t)}_d)$ can be used recursively.
Here, note that $\mathrm{dp}(d+1, T f^{(t)}_d)$ can be used because $\prod_{j=1}^{d} f^{(t)}_j = T f^{(t)}_d$.

Therefore, for a fixed $f^{(t)}_d$, the objective function in \cref{eq:dp} for the optimal configuration can be expressed as follows:
\begin{multline}
    \label{eq:dp_recursive2}
    \frac{\lambda}{M_\mathrm{BF}} \cdot \left\{\tilde{s}(g^{(t)}_d, f^{(t)}_d) + \mathrm{Size}(\mathrm{ML}_d) + \tilde{s}\left(g^{(b)}_d, \tilde{f}(g^{(b)}_d, h^{(b)}_d T f^{(t)}_i)\right)\right\} + \\
    \frac{1-\lambda}{R_\mathrm{BF}} \cdot h^{(t)}_d T f^{(t)}_d \cdot \mathrm{Time}(\mathrm{ML}_d) + \mathrm{dp}(d + 1, T f^{(t)}_d).
\end{multline}
The first term is the (scaled) memory usage for $\mathrm{TBF}_d$, $\mathrm{ML}_d$, and $\mathrm{BBF}_d$.
The second term is the (scaled) inference time for $\mathrm{ML}_d$.
The third term is the objective function (including memory usage and inference time) for $\mathrm{TBF}_{d+1}$ and below.
By performing the above calculation for each fixed $f^{(t)}_d$, we can calculate the value of $\check{\mathrm{dp}}(d, T)$.


\paragraph{Implementation Techniques and Computational Complexity of Dynamic Programming.}
With appropriate implementation, the computational complexity of this dynamic programming approach becomes $\mathcal{O}(\bar{D}P^2 + \bar{D}PK)$.
Here, after explaining the computational complexity involved in a straightforward implementation, we describe the implementation tricks that reduce the computational complexity.

The number of possible values for $d$---the first argument of $\mathrm{dp}$---is $\bar{D}$, and the number of possible values for $T$---the second argument of $\mathrm{dp}$---is $P$. 
For each pair of $d$ and $T$, up to $P$ values of $f^{(t)}_d$ are considered.
The evaluation of $\hat{\mathrm{dp}}(d,T)$ takes $\mathcal{O}(K)$ computational complexity, because the evaluation of $M_d(D, \mathcal{F})$, i.e.., \cref{eq:M_d req hat}, contains a summation from $k=1$ to $k=K$.
Therefore, if we compute it naively, the total computational complexity of this dynamic programming is $\mathcal{O}(\bar{D}P^2K)$.

However, by reducing the complexity for evaluating $M_d(D, \mathcal{F})$, we can reduce the total computational complexity of this dynamic programming to $\mathcal{O}(\bar{D}P^2 + \bar{D}PK)$.
By precomputing the summation in \cref{eq:M_d req hat} for each pair of $d$ and ``$Tf^{(t)}_d$,'' we can evaluate this function in $\mathcal{O}(1)$.
The time complexity of this precomputation is $\mathcal{O}(\bar{D}PK)$, because there are $\bar{D}$ values for $d$, $P$ values for ``$Tf^{(t)}_d$'', and $\mathcal{O}(K)$ computations are required for each case.
Therefore, the total computational complexity of this dynamic programming is $\mathcal{O}(\bar{D}P^2 + \bar{D}PK)$.


\section{Analysis of Model-Filter Memory Size Balance}
\label{app: analysis of model filter balance}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{fig/D_model_size_and_bf_size_clbf3.pdf}
    \caption{The model-filter memory size balance selected by CLBF for various $\bar{D}$ values: Beyond a certain point, increasing $\bar{D}$ further does not change the results.}
    \label{fig:exp:D_memory_clbf}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{fig/D_model_size_and_bf_size_plbf3.pdf}
    \caption{The model and filter memory size achieved by PLBF for various $D$ values: Up to a certain point, increasing $D$ reduces the overall memory usage, but beyond that point, increasing $D$ starts to increase the overall memory usage.}
    \label{fig:exp:D_memory_plbf}
\end{figure}

Here, we experimentally show that our CLBF achieves an appropriate balance between model and filter size by appropriately reducing the given learned model.
In order to clearly demonstrate the advantages of CLBF experimentally, we conduct experiments using a Random Dataset in addition to the two real-world datasets used in the main text:

\textbf{Random Dataset}:
This dataset simulates a scenario where the distributions of key and non-key samples are identical, making it extremely challenging for a machine learning model to distinguish between them.
The dataset is constructed as follows:
We generate 700,000 20-dimensional feature vectors, where the values of each dimension are uniformly distributed between 0 and 1. Among these, 200,000 vectors are labeled as ``key'' and 500,000 as ``non-key.''
We divide the 500,000 non-key vectors into 80\% as training data, 10\% as validation data, and 10\% as test data.
All 200,000 key vectors are used as training data and validation data.

\cref{fig:exp:D_memory_clbf} shows the relationship between the number of machine learning models given to construct CLBF, i.e., $\bar{D}$, and the model-filter memory size balance chosen as a result of optimization (we always set $F=0.001$ here). 
When $\bar{D}$ is small, increasing $\bar{D}$ monotonically decreases the overall memory usage and monotonically increases the size of the machine learning model used. 
On the other hand, for a certain range of large values of $\bar{D}$, the size of the Bloom filter and the machine learning models used do not change because CLBF selects an optimal subset $D$ from the given $\bar{D}$ models.
The optimal $D$ varies by dataset: approximately 20 for the Malicious URLs dataset, 40 for the EMBER dataset, and 0 for the Random Dataset.
This demonstrates that CLBF can efficiently reduce redundant models or avoid them entirely when they lack generalizability, as detected through validation data.

In contrast, in the case of PLBF~\citep{vaidya2021partitioned}, setting a larger $D$ can lead to an increase in memory usage.
The relationship between the number of machine learning models given when constructing PLBF, i.e., $D$, and the memory usage of PLBF is shown in \cref{fig:exp:D_memory_plbf} (we always set $F=0.001$ here).
As $D$ increases, of course, the memory usage of the PLBF machine learning models increases, because PLBF uses all of the $D$ machine learning models given.
On the other hand, the memory usage of the backup Bloom Filter used by PLBF tends to decrease.
This is because the accuracy of the machine learning model tends to improve as the size of the machine learning model increases, and even a small Bloom Filter can achieve the target false positive rate.
The overall memory usage decreases in the range of a certain small $D$, and increases in the range of a certain large $D$.
The optimal $D$ varies depending on the dataset, but in all cases, CLBF (shown as a red dot) selects a $D$ close to the optimal value.

\section{Comparison Experiments with Other Baselines}
\label{app: other baseline}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{fig/memory_fpr_all3.pdf}
    \caption{Trade-off between memory usage and accuracy (lower-left is better).}
    \label{fig:memory_accuracy_trade_off_all}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{fig/memory_query_time_all3.pdf}
    \caption{Trade-off between memory usage and average reject time (lower-left is better).}
    \label{fig:exp:memory_reject_time_trade_off_all}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{fig/time_hist_all3.pdf}
    \caption{Construction time.}
    \label{fig:exp:time_hist_all}
\end{figure}

% ここでは、結果の図が複雑になり過ぎないように、また、提案手法の有効性を明確に示すために、本文では割愛した他のベースラインとの比較実験について紹介する。
% 我々は本文中で比較したPLBF~\citep{vaidya2021partitioned}，Sandwiched LBF~\citep{mitzenmacher2018model}，Bloom Filter~\citep{bloom1970space}に加えて，disjoint Ada-BF~\citep{dai2020adaptive}とProjection Hash Bloom Filter (PHBF)~\citep{bhattacharya2022new}と我々のCLBFの比較を行う．disjoint Ada-BFは公開されているPython実装を参考に，我々がC++で実装し直して実験を行った．PHBFについては、公開されているPython実装を用いて実験を行った．そのため速度の比較はアンフェアであることに注意してください．PHBFのハイパーパラメータSampling factor, i.e., $s$, は10に設定した．

Here, we present the comparison experiments with other baselines, which we omitted from the main text to avoid overly complicated result figures and to clearly demonstrate the effectiveness of our proposed method. 
In addition to the PLBF~\citep{vaidya2021partitioned}, the Sandwiched LBF~\citep{mitzenmacher2018model}, and the Bloom filter~\citep{bloom1970space}, we compare our CLBF with disjoint Ada-BF~\citep{dai2020adaptive} and Projection Hash Bloom Filter (PHBF)~\citep{bhattacharya2022new}, and Hash Adaptive Bloom Filter (HABF)~\citep{xie2021hashadaptive}.
We implemented the disjoint Ada-BF in C++ based on the Python implementation published by the authors~\citep{githubdai2020adaptive}. 
For PHBF, we conducted experiments using both the Python implementation published by the authors~\citep{githubbhattacharya2022new} and our own re-implementation in C++.
The hyperparameter of PHBF, the sampling factor $s$ is always set to 10, and the number of hash functions $k$ is always set to 30.
For HABF, we used the C++ implementation published by the authors~\citep{githubnjulands2021hashadaptivebf}.

% まず，\cref{fig:memory_accuracy_trade_off_all}にそれぞれの手法の，Trade-off between memory usage and accuracyを示す．CLBFについては$\bar{D}=100$における結果，PLBF，disjoint Ada-BF，sandwiched LBFについては$D=1,10,100$における結果を載せている。
% PHBFについては，$k=10, 20, 30$の時の結果を載せている, where $k$はPHBFで使われるnumber of hash functionsであり，PHBFにおける重要なパラメータである。
% disjoint Ada-BFは，同じ機械学習モデルを用いるPLBFと比べて，常に劣ったトレードオフを示しています．
% また，PHBFについては，今回の実験設定では偽陽性率が常にとても高い結果となりました．
% これは，simple projection-based hash functionを用いるPHBFの機構では，今回のデータセットの複雑な分布を捉えて活用することができなかったためだと考えられます。
% PHBFのハイパーパラメータ$k$, i.e., ハッシュの個数, や$s$, i.e., sampling factor, を増やすことでメモリ効率を改善できる可能性は考えられるが，これは構築時間の増加に繋がる．
% 後で示す通り，現時点でPHBFの構築時間は非常に長いので，このようなやり方は許容しにくいと考えられる．

\subsection{Memory and Accuracy}
\label{app: memory and accuracy other baseline}

First, the trade-off between memory usage and accuracy for each method is shown in \cref{fig:memory_accuracy_trade_off_all}.
For CLBF, we always set $\bar{D}=100$, and for PLBF, disjoint Ada-BF, and sandwiched LBF, we show the results for $D=1, 10, 100$. 
For HABF, the results for $\texttt{bits\_per\_key}=1,2,\dots,15$ are shown.
For HABF, the false positive rate in the training data is also displayed.

We can see that sandwiched LBF and disjoint Ada-BF always show inferior trade-offs compared to PLBF and CLBF with the same machine learning model size. 
In addition, the false positive rate for PHBF was always almost $1$ for the Malicious URLs dataset and the EMBER dataset.
This is thought to be because the mechanism of using the projection as a hash function results in false positives when there are keys in the set that have features similar to the non-key query. 
It is possible to improve the accuracy by increasing the PHBF hyperparameters $k$, i.e., the number of hashes, and $s$, i.e., the sampling factor, but this will lead to an increase in construction time. 
Additionally, we observe that HABF has a worse accuracy than a simple Bloom filter.
However, the HABF false positive rate for non-keys in the ``training data,'' i.e., the data used for optimization, is much better than the false positive rate for ``test data.''
In other words, HABF is found to be prone to overfitting.
While HABF is effective in scenarios where the set of non-key queries is known at the time of construction, classic Bloom filters and LBFs achieve better accuracy in other cases.


\subsection{Memory and Reject Time}

% 次に，\cref{fig:exp:memory_reject_time_trade_off_all}にそれぞれの手法の，trade-off between memory usage and reject timeを示す．
% disjoint Ada-BFはハイパーパラメータが，PLBFやCLBFと異なり，全体のメモリ使用量であるため，統一した条件下で比較することが難しい．
% そこで，disjoint Ada-BFについては，$D=1$から$D=100$まで，様々なメモリ使用量のものを構築した上で，テスト時の偽陽性率が$0.001$に近かった（正確には$0.9 \times 0.001$より大きく$1.1 \times 0.001$より小さい）もののみを取り出してプロットした。
% また，PHBFについては、偽陽性率が十分小さいケースが得られなかったため，$k=10, 20, 30$で様々なメモリ使用量で構築した時の結果を，それぞれの$k$ごとに線で繋いで表示している（再度，PHBFはPythonで実装されていることに注意されたい）．

Next, the trade-off between memory usage and reject time for each method is shown in \cref{fig:exp:memory_reject_time_trade_off_all}.
For PLBF, sandwiched LBF, and Bloom filter, we show the results for $F=0.001$.
Because the hyperparameter that controls the accuracy of disjoint Ada-BF is the total memory usage (instead of the target false positive rate, as in PLBF and CLBF), it is difficult to compare them under consistent conditions.
Therefore, for disjoint Ada-BF, we constructed models with various total memory usages and $D$s, and then plotted only those with a false positive rate close to $0.001$ (more precisely, greater than $0.75 \times 0.001$ and less than $1.25 \times 0.001$).
For PHBF and HABF, we were unable to obtain a case with a sufficiently small false positive rate, so we have displayed the results for a case as a reference.
Please note that it is \textbf{not} a fair comparison because the false positive rate is completely different between PHBF/HABF and other methods.

From \cref{fig:exp:memory_reject_time_trade_off_all}, we observe that our CLBF consistently achieves a shorter reject time than any of the baselines.
For disjoint Ada-BF, when comparing with the same amount of memory usage, it was found that the reject time tended to be longer than PLBF.
PHBF showed a long reject time compared to the other methods.
In particular, the reject time of PHBF is particularly long for the EMBER dataset, which has a high dimensionality.
This is because PHBF takes a long time to calculate the projection of the input vector.

\subsection{Construction Time}

Finally, this section compares the time required to construct CLBF with other existing LBFs and the standard Bloom filter.
The comparison is made with fast PLBF~\citep{sato2023fast}, a method that constructs the same data structure as PLBF more quickly.

The results are shown in \cref{fig:exp:time_hist_all}.
Here, ``Scoring Time'' refers to the time taken to measure the score of each sample against each machine learning model by passing validation data through them.
``Configuration Time'' refers to the time required to compute the optimal configuration using the results of the scoring phase.
In the case of CLBF, the configuration process involves dynamic programming, as described in \cref{sec:dp_solution}.
Similarly, the configuration for sandwiched LBF and fast PLBF involves determining the optimal thresholds and false positive rates for each Bloom filter based on the scoring.
For the LBFs (i.e., CLBF, PLBF, and sandwiched LBF), the input machine learning model size $D=100$.
For PLBF, sandwiched LBF, and Bloom filter, we show the results for $F=0.001$.
For disjoint Ada-BF, we show the results for total memory usage is $200\,\mathrm{MB}$, and for HABF, we show for $\texttt{bits\_per\_key}=8$.

The results indicate that CLBF requires a longer configuration time than other existing LBFs.
The construction time of CLBF is approximately 10\% to 40\% longer than the construction time of existing LBFs
However, we believe that this additional overhead is a minor drawback.
Considering that the construction time for the smallest sandwiched LBF is already about 100 times longer than that of a standard Bloom filter, we can assume that LBF is not something that is used in scenarios where construction speed is sensitive. 
LBFs should be used in contexts where the frequency of reconstruction is low (once an hour or less). 
For example, the (learned) Bloom filter used to filter malicious URLs does not need to be rebuilt frequently because the set of malicious URLs does not change that quickly. 
In such cases, the construction time of CLBF, which is 1.4 times longer than that of the sandwiched LBF, is not a problem, and the benefits of the optimal configuration obtained by searching virtually all cases of $D \in \{1,2,\dots,100\}$ are considered to be greater.

We observe that the construction time for disjoint Ada-BF is almost identical to that of sandwiched LBF.
This is because CLBF and PLBF use dynamic programming to find the optimal parameters, while disjoint Ada-BF uses heuristics to determine the parameters, similar to sandwiched LBF.
The time required to build a PHBF is relatively short compared to LBFs, but the construction time is very long for the EMBER dataset, which has a high dimensionality.

The construction time of HABF is much shorter than that of the other LBFs.
Although this speed is remarkable, as mentioned in \cref{app: memory and accuracy other baseline}, HABFs tend to overfit, resulting in a higher false positive rate for unknown non-keys.
On the other hand, LBFs achieve low false positive rates even for unknown non-keys, while taking longer time to construct. Therefore, LBFs and HABF are expected to be used in different application scenarios. Specifically, LBFs may be better suited for situations with less strict constraints on construction time (e.g., when the frequency of reconstruction is low), while HABF is more appropriate for scenarios where there are strict constraints on construction time and most of the non-key queries are known.


\section{Ablation Study on Hyperparameters of Machine Learning Models}
\label{app: ablation study on hyperparameters of ML}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{fig/memory_fpr_max_depth.pdf}
    \caption{Trade-off between memory usage and accuracy (lower-left is better).}
    \label{fig:exp:memory_fpr_max_depth}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{fig/memory_query_time_max_depth.pdf}
    \caption{Trade-off between memory usage and average reject time (lower-left is better).}
    \label{fig:exp:memory_query_time_max_depth}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{fig/time_hist_max_depth.pdf}
    \caption{Construction time.}
    \label{fig:exp:time_hist_max_depth}
\end{figure}

CLBF obtains the optimal configuration by appropriately reducing the given trained machine learning model, but it is possible to further improve performance by appropriately tuning the machine learning model itself at the time of training.
Here, we show the results of our observations on how the hyperparameters of the machine learning model XGBoost, in particular, the value of \texttt{max\_depth}, affect the performance of LBF.
The smaller the \texttt{max\_depth}, the smaller each weak learner becomes, and while the size is smaller and inference time is shorter, the discriminative power of each weak learner becomes weaker.

The trade-off between memory usage and false positive rate for $\texttt{max\_depth} = 1, 2, 4, 6$ is shown in \cref{fig:exp:memory_fpr_max_depth}, and the trade-off between memory usage and reject time is shown in \cref{fig:exp:memory_query_time_max_depth}, and the construction time is shown in \cref{fig:exp:time_hist_max_depth}.
We confirmed that the properties of CLBF, as shown in \cref{sec:experiments,app: other baseline}, consistently appear for any value of $\texttt{max\_depth}$: (1) better memory-accuracy trade-off than existing LBFs, (2) better memory-reject time trade-off than existing LBFs, (3) slightly (up to 1.8 times) longer construction time than existing LBFs.

We find that there is a performance range that cannot be achieved by either the CLBF method or hyperparameter tuning alone, i.e., there is a performance that can only be achieved by using both techniques.
For example, in \cref{fig:exp:memory_query_time_max_depth}, the performance of $(\mathrm{Memory~Usage}, \mathrm{Reject~Time}) = (500\,\mathrm{kB}, 30\,\mathrm{ns})$ can be achieved when $\texttt{max\_depth} = 4$ and CLBF is used, but it cannot be achieved when $\texttt{max\_depth} = 1$ or PLBF is used.
These experimental results suggest the effectiveness of a hybrid method that combines the hyperparameter tuning of the machine learning model and the optimization technique in CLBF.


\section{Experimental Analysis on Artificial Datasets with Different Levels of Learning Difficulty}
\label{app: diverse learnablity}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{fig/random_scatter_delta.pdf}
    \caption{Visualization of Separation-Controlled Dataset in 2D.}
    \label{fig:exp:random_scatter_delta}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{fig/random_scatter_c.pdf}
    \caption{Visualization of Cluster-Number-Controlled Dataset in 2D.}
    \label{fig:exp:random_scatter_c}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{fig/random_results.pdf}
    \caption{Memory usage of CLBF and baselines in the Separation-Controlled Dataset with various values of $\delta$ and the Cluster-Number-Controlled Dataset with various values of $c$.}
    \label{fig:exp:random_results_delta_c}
\end{figure}

Here, we experimentally demonstrate that CLBF can adaptively select an appropriate machine learning model size based on the learning difficulty of the dataset.
We generate synthetic datasets with different levels of learning difficulty using two distinct methods.
Each dataset consists of 200,000 keys and 500,000 non-keys.
Each key and non-key is represented by a 20-dimensional feature vector, generated according to the following procedures:

\textbf{Separation-Controlled Dataset.}
This dataset is constructed to control the ease of learning by adjusting the parameter $\delta$, which determines the separation between the distributions of keys and non-keys.
Each feature vector is generated as follows:
\begin{itemize}
    \item The key feature vectors are sampled from a normal distribution $\mathcal{N}(\bm{0},I)$, where $\bm{0}$ is a 20-dimensional vector filled with zeros and $I$ is the $20 \times 20$ identity matrix.
    \item The non-key feature vectors are sampled from a normal distribution $\mathcal{N}(\delta \bm{1},I)$, where $\bm{1}$ is a 20-dimensional vector filled with ones.
\end{itemize}
As $\delta$ increases, the distributions of keys and non-keys become more separated, making it easier for a machine learning model to distinguish between them.
Conversely, a smaller $\delta$ makes the distributions overlap, increasing the difficulty of learning.
\cref{fig:exp:random_scatter_delta} shows the Separation-Controlled Dataset for the 2-dimensional case with $\delta=0.0, 2.5, 5.0$. 
The larger the value of $\delta$, the more the key and non-key distributions are separated, and the easier it is for the machine learning model to distinguish between keys and non-keys.

\textbf{Cluster-Number-Controlled Dataset.}
This dataset is designed to control the learning complexity by varying the parameter $c$, which specifies the number of clusters formed by the key and non-key feature vectors. 
Each feature vector is generated as follows:
\begin{itemize}
    \item The cluster centers for keys ($\{\bm{p}_i\}_{i=1}^{c}$) and non-keys ($\{\bm{q}_i\}_{i=1}^{c}$) are independently sampled from a uniform distribution over $[-10, 10]$.
    \item The key feature vectors are sampled equally from $\mathcal{N}(\bm{p}_1,I), \mathcal{N}(\bm{p}_2,I), \dots, \mathcal{N}(\bm{p}_c,I)$.
    \item Similarly, the non-key feature vectors are sampled equally from $\mathcal{N}(\bm{q}_1,I), \mathcal{N}(\bm{q}_2,I), \dots, \mathcal{N}(\bm{q}_c,I)$.
\end{itemize}
When $c$ is small, the distributions form a few well-separated clusters, making the classification task easier. Conversely, as $c$ increases, the clusters become more numerous, resulting in greater overlap and increased learning difficulty.
\cref{fig:exp:random_scatter_c} shows the Cluster-Number-Controlled Dataset for the 2-dimensional case with $c=1,8,64$.
The larger the value of $c$, the more complex the distribution of key and non-key data becomes, and the more difficult it is for the machine learning model to distinguish between keys and non-keys.

By measuring the memory usage of CLBF and PLBF for various values of $\delta$ and $c$, we confirm that CLBF dynamically adjusts its model size based on the dataset's learnability.
\cref{fig:exp:random_results_delta_c} shows the memory usage of CLBF and baselines in the Separation-Controlled Dataset ($\delta=0,0.5,1.0,1.5,\dots,5.0$) and the Cluster-Number-Controlled Dataset ($c=1,2,4,8,\dots,16384$).
The false positive rate $F$ is always set to $0.001$ here.

First, in the Separation-Controlled Dataset, we observe that the appropriate size of the machine learning model differs depending on the value of $\delta$, and that CLBF effectively selects the optimal size at all times.
When $\delta$ is very small ($\delta \sim 0.0$), it is optimal not to use a machine learning model ($D=0$).
However, PLBF retains unnecessary machine learning models, making it less memory efficient than a classical Bloom filter.
For $\delta$ values between 0.5 to 1.0, it is optimal to use a medium-sized machine learning model ($D \sim 20$).
As we increase the value of $\delta$ further, the optimal $D$ decreases, and when $\delta = 5.0$, it is optimal to use a very small machine learning model ($D=1$).
Across all these scenarios, CLBF consistently selects the optimal model size, achieving lower memory consumption than all baselines.

Similarly, results from the Cluster-Number-Controlled Dataset also demonstrate the effectiveness of CLBF.
When $c$ is very large ($c \geq 10^3$), it is optimal not to use a machine learning model ($D=0$), whereas PLBF retains redundant models, leading to inefficiencies compared to a classical Bloom filter.
For $10^2 \leq c < 10^3$, it is optimal to use a medium-sized machine learning model ($D \sim 50$).
As we decrease the value of $c$ further, the optimal $D$ decreases, and when $c = 1$, it is optimal to use a very small machine learning model ($D=1$).
In all cases, CLBF successfully adapts its model size and consistently achieves lower memory usage than the baselines.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
