\begin{table*}[t]
  \caption{Full results on DCLM 400M-4x. The number beside each task denotes the number of few-shot demonstrations used for evaluation. We exclude CommonsenseQA from the core score calculation due to its instability and limited informativeness. For instance, in the original DCLM paper, the 412M model dramatically outperforms the 1.4B model by 76.6\% on this task.}
  \label{tab:full-400m}
  \vskip 0.15in
  \begin{sc}
  \centering
  \resizebox{\textwidth}{!}{%
    \begin{tabular}{l|ccccc}
    \toprule
    \textbf{Tasks} & Random & Edu Classifier & MATES & Quad & Group-MATES \\
    \midrule
    agi\_eval\_lsat\_ar (3) & 0.19565 & \textbf{0.28696} & 0.20435 & 0.20000 & 0.27391 \\ 
arc\_challenge (10) & 0.29522 & \textbf{0.32253} & 0.29863 & 0.29181 & 0.27901 \\ 
arc\_easy (10) & 0.57912 & \textbf{0.59975} & 0.57323 & 0.58460 & 0.56818 \\ 
bigbench\_cs\_algorithms (10) & \textbf{0.44697} & 0.33712 & 0.39697 & 0.43258 & 0.43864 \\ 
bigbench\_dyck\_languages (10) & 0.19300 & 0.21600 & 0.18800 & 0.20300 & \textbf{0.25900} \\ 
bigbench\_language\_identification (10) & 0.24690 & 0.25320 & 0.25500 & 0.25310 & \textbf{0.25590} \\ 
bigbench\_operators (10) & 0.14762 & 0.18095 & 0.16190 & 0.14762 & \textbf{0.20476} \\ 
bigbench\_qa\_wikidata (10) & 0.52099 & \textbf{0.52492} & 0.52360 & 0.50431 & 0.51612 \\ 
bigbench\_repeat\_copy\_logic (10) & 0.00000 & 0.03125 & \textbf{0.06250} & 0.00000 & 0.03125 \\ 
boolq (10) & 0.56881 & 0.49021 & 0.59113 & 0.58899 & \textbf{0.60673} \\ 
commonsense\_qa (10) & \textbf{0.37838} & 0.22195 & 0.22523 & 0.31286 & 0.20475 \\ 
copa (0) & 0.62000 & 0.69000 & 0.66000 & \textbf{0.74000} & 0.67000 \\ 
coqa (0) & 0.21195 & 0.21283 & \textbf{0.22836} & 0.21308 & 0.21846 \\ 
hellaswag (0) & 0.45230 & 0.45399 & 0.45519 & \textbf{0.45589} & 0.45519 \\ 
hellaswag (10) & 0.45638 & 0.45688 & 0.45828 & 0.45818 & \textbf{0.45897} \\ 
jeopardy (10) & 0.12347 & \textbf{0.14875} & 0.11854 & 0.09442 & 0.12693 \\ 
lambada\_openai (0) & \textbf{0.50708} & 0.45624 & 0.50340 & 0.50010 & 0.50087 \\ 
mmlu\_fewshot (5) & 0.24948 & 0.24992 & 0.22825 & 0.25419 & \textbf{0.26678} \\ 
openbook\_qa & 0.33400 & 0.33600 & \textbf{0.34200} & \textbf{0.34200} & \textbf{0.34200} \\ 
piqa (10) & \textbf{0.70403} & 0.69369 & 0.70131 & 0.70022 & 0.69423 \\ 
squad (10) & 0.23709 & 0.23936 & \textbf{0.27436} & 0.23094 & 0.25326 \\ 
winograd (0) & 0.69231 & 0.70330 & 0.69963 & 0.68864 & \textbf{0.71062} \\ 
winogrande (0) & 0.54538 & \textbf{0.55406} & 0.53354 & 0.52802 & 0.53749 \\ 
\textbf{Core} & 0.21356 & 0.21821 & 0.22260 & 0.22358 & \textbf{0.23514} \\
    \bottomrule
    \end{tabular}
  }
  \end{sc}
\end{table*}