\begin{abstract}

% Efficiently utilizing data is critical for pretraining large-scale foundation models. 
% Existing data curation methods largely assume that each data contributes independently to the model performance while paying less attention to complex interactions between training points. 
% \cx{add more technical details and intuitions in abs.}
% What data to train on is as crucial as which model to train.
Data-efficient pretraining has shown tremendous potential to elevate scaling laws. This paper argues that effective pretraining data should be curated at the group level, treating a set of data points as a whole rather than as independent contributors.
% accounting for complex interactions among data points rather than treating them as independent contributors.
To achieve that, we propose \textit{Group-Level Data Influence Modeling (Group-MATES)}, a novel data-efficient pretraining method that captures and optimizes group-level data utility. Specifically, Group-MATES collects oracle group-level influences by locally probing the pretraining model with data sets. It then fine-tunes a relational data influence model to approximate oracles as relationship-weighted aggregations of individual influences.
% and iteratively bootstraps diverse oracles to cover tail cases. 
The fine-tuned model selects the data subset by maximizing its group-level influence prediction, with influence-aware clustering to enable efficient inference.
% The data influence model then selects data sequentially, predicting individual influences as well as pairwise relationships to maximize group-level utility.
Experiments on the DCLM benchmark demonstrate that Group-MATES achieves a 10\% relative core score improvement on 22 downstream tasks over DCLM-Baseline and 5\% over individual-influence-based methods, establishing a new state-of-the-art. Further analyses highlight the effectiveness of relational data influence models in capturing intricate interactions between data points.
% thereby benefiting the data curation outcome.
% We will open-source our code via GitHub.
\end{abstract}