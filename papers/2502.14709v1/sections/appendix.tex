% \section{Theoretical Analysis}

\section{Experimental Details}
\label{sec:exp-details}

\input{tables/config}

\paragraph{Language Model}. We pretrain all decoder-only language models from scratch following DCLM setups. The training employs cosine learning rate scheduler and AdamW optimizer~\cite{KingBa15}. All experiments are conducted on 8 GPUs, with detailed training hyperparameters provided in Table~\ref{tab:config}.

\paragraph{Relational Data Influence Model.} Our relational data influence model is fine-tuned from \texttt{bge-base-en-v1.5}~\cite{xiao2024bge}, which takes the last hidden state of the first token (i.e., [CLS]) as the sentence embedding $\textbf{h} \in \mathbb{R}^{768}$. As our base model only supports a maximum input sequence length of 512, but our pretraining sequence length extends to 2048, we split each sequence into four chunks and process them separately. The hidden states of four chunks are averaged to compute the final embedding $\textbf{h}$. This vector is then multiplied by a regression weight $\textbf{w}_o \in \mathbb{R}^{768}$ to predict individual influence $\textbf{w}_o\cdot\textbf{h}$. For pairwise relationships, the sim function is the cosine similarity between two embeddings, consistent with the original BGE design. The model is trained using the mean squared error loss between the predicted and normalized oracle influences. The validation set consists of 1,000 sampled oracles. All training hyperparameters are listed in Table~\ref{tab:config}.

\section{Additional Results}
\label{sec:additional}

\subsection{Scaling Curves}

In Figure~\ref{fig:flops}, we demonstrate the scaling curves of model performance w.r.t. pretraining steps on DCLM 400M-4x and 1B-1x setups.
Group-MATES shows gradually increased advantages over random selection as training proceeds, underlining the effectiveness of group-level data influence modeling to elevate the scaling laws. 

% \begin{itemize}
%     \item Design choices of relational data influence model (BERT, model-based similarity, dot)
%     \item Oracle size
%     % \item Influence update interval (early and late stages are more important)
% \end{itemize}

\subsection{Design of Relational Data Influence Model}

In this section, we vary the design choices of our relational data influence model, including replacing the model backbone with BERT-base~\cite{devlin-etal-2019-bert}, choosing dot product or an FFN model as the sim function. As shown in Figure~\ref{fig:model-design}, BERT demonstrates weaker abilities to approximate group-level influences than BGE, as the latter has been specifically optimized for sentence embeddings. Taking FFN as the sim function does not significantly decrease the approximation performance but introduces additional parameters; choosing dot product, the performance dramatically drops. This validates our choice to align the similarity measurement with the original BGE, i.e., cosine similarity.

\subsection{Oracle Collection Size}

In this section, we examine the effects of oracle collection size on the approximation performance of our relational data influence model. As shown in Figure~\ref{fig:oracle-size}, scaling up oracle size consistently elevates the performance but with diminishing returns. Considering the effectiveness-efficiency trade-off, we finally choose 80k as the size of our oracle collection.

\begin{figure}[h]
    \centering
    \begin{minipage}{0.49\textwidth}
    \centering
    \begin{subfigure}[t]{0.47\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/400M_steps.pdf}
    \caption{400M-4x setup.}
    \label{fig:400M-flops}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.47\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/1B_steps.pdf}
    \caption{1B-1x setup.}
    \label{fig:1B-flops}
    \end{subfigure}
    \caption{Core score w.r.t. pretraining steps on DCLM 400M-4x (a) and 1B-1x (b) setups.}
    \label{fig:flops}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\textwidth}
    \centering
    \begin{subfigure}[t]{0.47\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/model_ablation.pdf}
    \caption{Model design.}
    \label{fig:model-design}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.47\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/oracle_size.pdf}
    \caption{Oracle size.}
    \label{fig:oracle-size}
    \end{subfigure}
    \caption{Performance of relational data influence model with different design choices (a) and oracle sizes (b).}
    \end{minipage}
\end{figure}

\subsection{Comparison in MATES Setup}
\label{sec:mates}

In this section, we compare Group-MATES with previous pretraining data curation baselines, following the same setup as MATES~\cite{yu2024mates}. These methods include (1) DSIR~\cite{xie2023data}: proximity to Wikipedia based on n-gram features. (2) SemDeDup~\cite{abbas2023semdedup}: deduplicating semantically similar data. (3) DsDm~\cite{engstrom2024dsdm}: static approximation of influence functions by a converged proxy model. (4) QuRating~\cite{wettig2024qurating}: ranking with educational values distilled from GPT-3.5. As shown in Table~\ref{tab:mates}, Group-MATES achieves the best average downstream results with minimal additional costs, highlighting the potential of optimizing group influences in data-efficient pretraining.

\input{tables/full_400m}
\input{tables/full_1b}

\input{tables/mates}

% What data is selected by individual data influence but not pairwise, and vice versa.