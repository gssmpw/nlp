\section{Methods}

% Figure 2: pipeline.

% Decompose (1) toward group-level data influence valuation and (2) data selection.

In this section, we introduce Group-MATES, a novel group-level data influence modeling method that can be efficiently applied to large-scale pretraining setups. Specifically, we first formally define the optimization goal of data-efficient pretraining (§\ref{sec:prerequisite}). Then, we present the theoretical motivation behind our method (§\ref{sec:connection}). Finally, we present our approach to modeling group-level data influence (§\ref{sec:modeling}). 
The overall pipeline of our method is illustrated in Figure~\ref{fig:pipeline}.

\begin{figure*}
    \centering
    \vspace{-0.2cm}
    % \vspace{-0.5cm}
    \includegraphics[width=0.9\linewidth]{figures/method-1.pdf}
    \vspace{-0.5cm}
    \caption{Overview of Group-MATES. We fine-tune a relational data influence model to approximate, bootstrap, and maximize group-level influences, with efficient inference through influence-aware clustering.}
    % \cx{caption is too long. The figure should be more self-reflective thus we do not need detailed description. The clustering part is not very clear---we should illustrate that it is for efficient inference.}} \vspace{-0.3cm}
    \label{fig:pipeline}
    \vspace{-0.5cm}
\end{figure*}

\subsection{Data-Efficient Pretraining}
\label{sec:prerequisite}

% \cx{break down the paragraph to one high level one and leads to one detailed description.}
Data-efficient pretraining aims to maximize model performance while reducing computational overhead by curating training data $\mathcal{D}$. Here, $\mathcal{D}$ could include web data from multiple domains, synthetic data, or their arbitrary combinations. In this sense, we can treat data-efficient pretraining as an optimal subset selection problem.
% \cx{maybe state that the selection can be non-replacement or repeative. in this paper at research scale it is more non-replacement}. 
The selection process can be performed with or without replacement, depending on the size of the available data source. In this paper, we focus on research scales where the quantity of pretraining data exceeds the available computational FLOPs, allowing us to select data without replacement.

Formally, the optimal size-$n$ training subset $\mathcal{D}^*_{(n)}$ is the set that minimizes loss over the target distribution after training model $\mathcal{M}$ on $\mathcal{D}^*_{(n)}$. The target distribution can be estimated by any reference tasks $\mathcal{D}_r$ that we can access during pretraining, so the optimization objective is:

\vspace{-0.85em}
\begin{align}
    \mathcal{D}^*_{(n)} &= \argmin\limits_{\mathcal{D}_{(n)}}~\mathcal{L}(\mathcal{D}_r \mid \mathcal{A}(\mathcal{M}, \mathcal{D}_{(n)})), \\
    &= \argmin\limits_{\mathcal{D}_{(n)}}~\mathbb{E}_{(x, y) \sim \mathcal{D}_r} \ell(y \mid x; \mathcal{A}(\mathcal{M}, \mathcal{D}_{(n)})),
\end{align}
% \vspace{0.85em}

where $\mathcal{A}(\mathcal{M}, \mathcal{D}_{(n)}))$ denotes the model training algorithm on the data subset $\mathcal{D}_{(n)}$ using an optimizer like Adam~\cite{KingBa15} and $\ell$ denotes the function to compute the model loss on an input-output pair $(x, y)$.

Prior methods, such as DsDm~\cite{engstrom2024dsdm} and MATES~\cite{yu2024mates}, approach this group-level optimization by assuming that each training point $x_i\in\mathcal{D}$ contributes independently to the model performance, i.e., greedy Top-$n$ selection:

\vspace{-0.85em}
\begin{align}
    \mathcal{D}^*_{(n)} \approx \{\text{Top-}n~(-\mathbb{E}_{(x, y) \sim \mathcal{D}_r} \ell(y \mid x; \mathcal{A}(\mathcal{M}, x_i)))\}.
\end{align}

This assumption tackles the combinatorial optimization problem by considering only individual data influences and has been shown effective in pretraining data curation.

\subsection{Group-Level Data Influence Theory}
\label{sec:connection}

While independence assumption simplifies group-level selection, empirical results (Figure~\ref{fig:overlap-err}) show that data selected by using it deviates significantly from optimal group-level selection. Recent theoretical work highlights that this gap arises due to the relationship term in the representation of group-level data influence, which captures interactions between training points~\cite{koh2019accuracy,wang2024capturing}.

% Therefore, we need a more accurate way to characterize group-level influence.
% independent assumption lies in the theoretical insight that group-level influence is not merely the sum of individual influences. Instead, it should also include a relationship term that encapsulates the interaction between training points~\cite{wang2024capturing,wang2024greats}. This relationship term plays a pivotal role in shaping the group-level influence~\cite{saunshi2022understanding}.

Specifically, the training subset $\mathcal{D}_{(n)}$ can be decomposed as the batch sequence at every training step, i.e., $\{\mathcal{B}_{t_1},\mathcal{B}_{t_2}, \dots , \mathcal{B}_{t_r}\}$,~where $r=\frac{n}{b}$ and $b$ is the batch size. We denote the model sequence after being trained on each batch as $\{\mathcal{M}_{t_1},\mathcal{M}_{t_2}, \dots , \mathcal{M}_{t_m}\}$, i.e., $\mathcal{M}_{t_{k+1}} := \mathcal{A}(\mathcal{M}_{t_k}, \mathcal{B}_{t_k})$, and the final converged state $\mathcal{M}_{t_m}$ is simplified as $\mathcal{M}^*$. Following standard influence functions~\cite{weisberg1982residuals,koh2017understanding}, we examine the effect of upweighting one training data point $x_i$ from batch $\mathcal{B}_{t_k}$ by $\epsilon$ on the model state. The final model after the upweighting is denoted by $\mathcal{M}^*_{\epsilon, x_i}$.
Using Taylor expansion and recursion unrolling, TSLOO~\cite{wang2024capturing} derives the change in reference loss $\mathcal{L}(\mathcal{D}_r \mid \mathcal{M}^*_{\epsilon, x_i}) - \mathcal{L}(\mathcal{D}_r \mid \mathcal{M}^*)$ before and after the upweighting as: 
\vskip -0.05in
{\fontsize{7.1}{5}\selectfont
\begin{equation}
    \eta_{t_k} \nabla_{\mathcal{M}} \mathcal{L}(\mathcal{D}_r \mid \mathcal{M}^*)^\top \left[ \prod_{j=t_k+1}^{T-1} \left(I - \eta_j H_j \right) \right] \nabla_{\mathcal{M}} \mathcal{L}(x_i \mid \mathcal{M}_{t_k}),
    \label{eq:wang}
\end{equation}
}

% $\nabla_{\mathcal{M}} \mathcal{L}(\mathcal{D}_r \mid \mathcal{M}^*)^\top\left.\frac{d \mathcal{M}^*_{\epsilon, x_i}}{d\epsilon} \right|_{\epsilon=0}$.

% \begin{equation}
%     \small
%     \mathcal{L}(\mathcal{D}_r \mid \mathcal{M}^*_{\frac{1}{n}, x_i}) - \mathcal{L}(\mathcal{D}_r \mid \mathcal{M}^*) = \nabla_{\mathcal{M}} \mathcal{L}(\mathcal{D}_r \mid \mathcal{M}^*)^\top\left.\frac{d \mathcal{M}^*}{d\epsilon} \right|_{\epsilon=0}
% \end{equation}

where $\eta_{t_k}$ is the learning rate at step $t_k$, $I$ is the identity matrix, and $H_j = \sum_{x\in\mathcal{B}_{t_j}}\nabla^2_{\mathcal{M}} \mathcal{L}(x \mid \mathcal{M}_{t_j})$ is the Hessian. We can then expand Eq.~\ref{eq:wang} as the sum of two separate terms:
% \vspace{-0.85em}
\begin{small}
\begin{align}
    \eta_{t_k} \nabla_{\mathcal{M}} \mathcal{L}(\mathcal{D}_r \mid \mathcal{M}^*)^\top \nabla_{\mathcal{M}} \mathcal{L}(x_i \mid \mathcal{M}_{t_k})~+ \nonumber\\
    \eta_{t_k} \nabla_{\mathcal{M}} \mathcal{L}(\mathcal{D}_r \mid \mathcal{M}^*)^\top R \nabla_{\mathcal{M}} \mathcal{L}(x_i \mid \mathcal{M}_{t_k}), \label{eq:second}\\
    \text{where}~R=\left[ \prod_{j=t_k+1}^{T-1} \left(I - \eta_j H_j \right) \right]-I.
\end{align}
\end{small}

Intuitively, the first term in Eq.~\ref{eq:second} represents individual data influences regardless of other training points, while the second term encapsulates high-order relationships between the current training point and all subsequent data from $\mathcal{D}_{(n)}$. The relationship term plays a pivotal role in shaping group-level influences through interactive effects, such as cancellation and amplification~\cite{hu2024most}.
% This formula indicates that accurately calculating group-level influence requires considering both the individual influence of training points as well as the intrinsic relationships among them.

% In contrast, our relational data influence model is parametric and thus can approximate both the individual data influence as well as the relationship term more efficiently.
% \cx{I am debating about the role of this section. Probably even the necessity of it. Right now my first preference is that this will go between 3.1 and the actual method, to serve as a motivation/theory backup role in this section. It is kind of like what you are trying to do in the first paragraph of 3.2.}

\subsection{Group-Level Data Influence Modeling}
\label{sec:modeling}

% We introduce \textit{group-level data influence modeling}, an efficient and effective approach to model both individual influences and relationships to optimize group-level selection.
To efficiently capture group-level influences, we introduce \textit{group-level data influence modeling}, a parametric approach to model and optimize group-level influences in pretraining scenarios.
Following~\citet{hu2024most}, we first study data pairs as the simplest form of group data and then effectively generalize our method to larger groups.

% We choose to use data pairs to work on group-level influence because they provide the simplest characterization of data interactions. By analyzing relationships within data pairs, we can effectively capture higher-order interactions and dependencies that extend to larger groups~\cite{hu2024most}.

% an extension of individual data influence modeling~\cite{yu2024mates} that not only approximates individual data influences but also the relationship between data pairs, facilitating a more precise group-level data influence estimation.

% \paragraph{Oracle Group-Level Influence Collection.} 

To begin with, we obtain the group-level influence in the most direct and accurate manner—locally probing language model with the group data and measuring its impact on the reference loss. Formally, given the current model checkpoint $\mathcal{M}$, we first sample a series of data pairs from the hold-out data $\mathcal{D}_h$, constructing $\mathcal{P} = \{(x_{i1}, x_{i2}) \mid x_{i1} \in \mathcal{D}_h, x_{i2} \in \mathcal{D}_h\}$. Then, we probe oracle influences $\mathcal{I}$ by training $\mathcal{M}$ on each of the data pairs and collect its reference loss change:
\begin{align}
    \mathcal{I}_i &= \mathcal{L}(\mathcal{D}_r \mid \mathcal{M}) - \mathcal{L}(\mathcal{D}_r \mid \mathcal{M}_i),\\
\text{where}~\mathcal{M}_i &= \mathcal{A}(\mathcal{A}(\mathcal{M}, x_{i1}), x_{i2}),~(x_{i1}, x_{i2}) \in \mathcal{P}.
\end{align}

\paragraph{Building Relational Data Influence Model.} Extending the individual data influence model introduced in MATES~\cite{yu2024mates}, we propose a relational data influence model $\Theta$ to learn the mapping from data pairs to their oracle group-level influences. The relational data influence model $\Theta$ can be any encoder that produces the embedding $\textbf{h}$ of a data point, and 
% Motivated by Simfluence~\cite{guu2023simfluence} that models the reference loss trajectory as a linear Markov process \cx{unclear what it is. either it should be somethign discussed in related work, or here. Probably it is a miss in the related work part.}, 
its prediction $\Theta(x_{i1}, x_{i2})$ is formulated as:
\vspace{-0.45em}
\begin{equation*}
\underbrace{(\textbf{w}_o\cdot\textbf{h}_{x_{i1}})}_{\text{individual influence}} - \underbrace{\alpha * (\frac{\text{sim}(\textbf{h}_{x_{i1}},\textbf{h}_{x_{i2}})}{\beta} - 1)}_{\text{relationship weight}} *  \underbrace{(\textbf{w}_o\cdot\textbf{h}_{x_{i2}})}_{\text{individual influence}},
\end{equation*}
where $\textbf{w}_o$ denotes the regression output weight that transforms the embedding to the individual influence prediction. $\alpha$ and $\beta$ are two trainable parameters initialized both from $1$ and $\text{sim}$ is the relationship term (e.g., cosine similarity) of two embeddings, ranging within $[-1, 1]$. 

% The training loss is the mean squared error between the model prediction and the normalized oracle influences. 

% \cx{yeah split (oracle, DIM setup) and the bootstrap training into two subsections make this part awkward.}

This prediction formula can be intuitively interpreted as follows: the group-level influence of two data points $(x_{i1}, x_{i2})$ is the individual data influence of the first data $x_{i1}$ plus the relationship-weighted individual data influence of the second data $x_{i2}$. When two data points resemble each other, there will be \textit{cancellation effects} that penalize the second data influence; 
% otherwise\cx{this read like if not cancellation then it is amplification, which is wrong.}, there will be \textit{amplification effects} that magnify the second data influence. 
in contrast, \textit{amplification effects} occur when the second data point's influence is magnified.
Similar observations are shown by~\citet{hu2024most}.

% We use the to collect the oracle data influence. and then train the relational data influence model 
% to learn both the individual effect as well as .

% To efficiently calculate the group-level influence, we propose \textit{group-level data influence modeling}, an extension of individual data influence modeling~\cite{yu2024mates} that not only captures the individual data influence but also the relationship between the data within the group, thereby obtaining a more accurate group-level data influence.

% \citet{yu2024mates} find that Eq.~\ref{eq:first} can be learned via a small data influence model, while the second term is more computationally expensive as it requires intensive calculations of second-order derivatives.

% Traditional methods utilize influence functions to quantize the influence of a data subset on the model performance.

% \zichun{Why we cannot train as the sequence}

% This group-level influence can be understood as the sum of individual influence plus the relationship measurements between the data points within this group.

% We fine-tune a pairwise \textit{data influence model} $\Theta$ on $\{(x_i, \mathcal{I}_\mathcal{M}(x_i;\mathcal{D}_r)) \mid x_i \in \mathcal{D}_h\}$ to approximate the oracle.

% \subsection{Bootstrapping Data Influence Models}
% \label{sec:bootstrapping}

\paragraph{Training Relational Data Influence Model.} 
Ideally, our relational data influence model should capture the oracle distribution of the entire training dataset, with emphasis on the tail fractions that contain more informative data points.
% which typically follows a Gaussian distribution~\cite{yu2024mates}. 
% However, as the hold-out data $\mathcal{D}_h$ is subsampled from the full training set $\mathcal{D}$, particularly data from the tails of the distribution. 
While sampling more oracles could help mine such tail cases, it is prohibitively expensive, as each oracle requires actual model optimization.
% To ensure sufficient coverage of the oracle distribution,
To efficiently cover tail fractions, we propose \textit{bootstrapping data influence models}, a targeted oracle collection strategy to facilitate the inclusion of edge cases for better oracle informativeness and diversity.
% \cx{redo the motivation of this part. it is to reduce the needs of collecting too many oracle data? more like a targeted collecting? the collecting would be more for diversity? maybe we can illustrate the oracle distribution here that state that typically it is a Gaussian and we would like to ensure to collect sufficient information in the tails, to avoid model collapsing-ish?}
% As we train the relational data influence model $\Theta$ with limited oracle labels\cx{why limited? this is the main motivation of bootstrapping.}, we hope to leverage the most useful data pairs that can benefit the learning of $\Theta$, thereby achieving better label efficiency. (e.g., 10x larger than $\mathcal{D}_h$) 

Specifically, we use a fine-tuned relational data influence model to predict individual data influences (i.e., $\textbf{w}_o\cdot\textbf{h}_{x_{i}}$) over a larger data pool $\mathcal{D}_l$ and sample the first data with the lowest/highest influences in the predicted distribution, using Gumbel-Top-$k$ algorithm~\cite{kool2019stochastic}:
\begin{align}
    \mathcal{D}_f \leftarrow &\{\text{GT-}k( \textbf{w}_o\cdot\textbf{h}_{x_{i1}}) \mid x_{i1} \in \mathcal{D}_l\}~\cup \nonumber \\
    &\{\text{GT-}k(-\textbf{w}_o\cdot\textbf{h}_{x_{i1}}) \mid x_{i1} \in \mathcal{D}_l\}.
\end{align}
% (\textbf{w}_o\cdot\textbf{h}_{x_{i2}}) - \alpha * (\frac{\text{sim}(\textbf{h}_{x_{i2}},\textbf{h}_{x_{i1}})}{\beta} - 1) *  (\textbf{w}_o\cdot\textbf{h}_{x_{i1}})
Bootstrapping the second data is crucial as well, since many pairs are only weakly related in the pretraining corpus. We identify the second bootstrapped data set $\mathcal{D}_{s_i}$ as those having the lowest/highest similarities to the first data $x_{i1} \in \mathcal{D}_f$:

\begin{small}
\begin{align}
    \mathcal{D}_{s_i} \leftarrow &\{\text{GT-}m(\text{sim}(\textbf{h}_{x_{j2}},\textbf{h}_{x_{i1}})) \mid x_{j2} \in \mathcal{D}_l, x_{i1} \neq x_{j2}\}~\cup \nonumber \\
    &\{\text{GT-}m(-\text{sim}(\textbf{h}_{x_{j2}},\textbf{h}_{x_{i1}})) \mid x_{j2} \in \mathcal{D}_l, x_{i1} \neq x_{j2}\}, \nonumber \\
    \mathcal{P} \leftarrow & \bigcup_{x_{i1} \in \mathcal{D}_f}\{(x_{i1}, x_{j2}) \mid x_{j2} \in \mathcal{D}_{s_i}\}.
\end{align}
\end{small}

Finally, we collect the new round of oracle influences with the bootstrapped data pairs $\mathcal{P}$ and fit a new data influence model. The whole process can be conducted iteratively, where the relational data influence model from the previous round bootstraps more diverse data for the next round of oracle collection until its performance is saturated.

% We sample the probing data from a larger pool annotated by the last round of the data influence model to select the representative subset to collect influence on, thus making the data influence model learn better in the next round.
% \cx{this subsection is not very fluid. Start with the role being inference and then motivate this technique as for speed. Then discuss why we want influence-aware clustering?}

\input{tables/main}
\input{tables/flops}

\textbf{Efficient Inference for Relational Data Influence Model.}
During inference, we generalize the predictions of our relational data influence model from data pairs to larger groups, aiming to find the optimal subset that minimizes reference loss.
% to predict both individual influences of data points and the relationships between them, 
Specifically, given a size-$m$ selected subset $\mathcal{D}_{(m)}$, we choose the next data with:

\begin{equation}
    % (\textbf{w}_o\cdot\textbf{h}_{x_i}) - \frac{1}{m}\sum_{x \in \mathcal{D}_{(m)}}\alpha * (\frac{\text{sim}(\textbf{h}_{x_i},\textbf{h}_x)}{\beta} - 1) *  (\textbf{w}_o\cdot\textbf{h}_x).
     \mathcal{D}_{(m+1)} \leftarrow \mathcal{D}_{(m)} \cup \{\argmin\limits_{x_i \in \mathcal{D}\setminus\mathcal{D}_{(m)}} \sum_{x \in \mathcal{D}_{(m)}} \Theta(x_i, x)\}. \label{eq:infer}
\end{equation}

The $\argmin$ in Eq.~\ref{eq:infer} means that adding the selected data to the subset will minimize the reference loss by prediction. 

% To speed up inference, we propose \textit{influence-aware clustering}, which groups data points with similar influence representations (i.e., $\textbf{h}_{x_i}$) together. Points in the same cluster tend to exhibit stronger interactions, so we only perform sequential selections within each cluster with the relationship consideration. Furthermore, we parallelize the selection process across clusters, significantly reducing computational cost by $N^2$, where $N$ is the number of clusters. With influence-aware clustering, we avoid computing all relationship terms across the entire dataset, enabling our method to scale efficiently to large setups without sacrificing performance.

To speed up inference, we propose \textit{influence-aware clustering}, which partitions training data into smaller clusters based on influence representations (i.e., $\textbf{h}_{x_i}$). Points within the same cluster will have similar influence representations and thus tend to exhibit stronger interactions than those in different clusters. This allows us to perform selections locally within each cluster, as formulated in Eq.~\ref{eq:infer}, keeping meaningful interactions without computing relationships exhaustively across the entire dataset.

% This localized selection ensures that we capture meaningful interactions without requiring exhaustive relationship computation across the entire dataset.

With multi-threaded parallelization, we can further run the inference process independently within each cluster, significantly reducing computational cost by $N^2$, where $N$ is the number of clusters. In conclusion, influence-aware clustering eliminates the need to compute all relationship terms across the entire training data and scales our method efficiently to large setups while preserving performance.

%upweighting the importance of the relationship term. With the clustering-based method and simple parallelization quadratic complexity.

% We first group the data with similar influence representations (DIM's hidden states). Then, we adopt multi-armed bandit techniques to iteratively choose the source cluster and leverage pairwise DIM to sequentially select data within the source cluster.

% Following~\citet{zhang2024quad}, we propose to utilize Multi-Arm Bandit (MAB).

% Better efficiency (\zichun{do we need to update the clustering?}) and a balance of quality and diversity.