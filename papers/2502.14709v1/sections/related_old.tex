\section{Related Work}

% \paragraph{Data-Efficient Pretraining.} 

Data-efficient pretraining is one of the most crucial problems in the foundation model era, as people wish to leverage the most effective data to reduce the pretraining cost and achieve better scaling laws~\cite{kaplan2020scaling,Chinchilla}. Prior works endeavor to make pretraining more data-efficient through different data curation techniques, such as \textbf{(1) domain reweighting}~\cite{xie2024doremi}: grouping data by their source domains (e.g., wikipedia, github, etc.) and finding the optimal domain weights via proxy models~\cite{xie2024doremi,fan2023doge} or linear regression~\cite{liu2024regmix}; \textbf{(2) data selection}~\cite{t5}: filtering the high-quality data via manually defined heuristics~\cite{t5,rae2021scaling,penedo2024refinedweb,penedo2024fineweb}, deduplication~\cite{lee-etal-2022-deduplicating,abbas2023semdedup,penedo2024refinedweb,tirumala2024d4}, proximity-based classifiers~\cite{gao2020pile,wenzek2020ccnet,PaLM,du2022glam,gan2023ziya2,xie2023data,li2024datacomp}, LLM rating~\cite{wettig2024qurating,sachdeva2024askllm,penedo2024fineweb}, or influence functions\cx{add one sentence here and there to explain these methods, just a name is not very informative for newer methods.}~\cite{engstrom2024dsdm,yu2024mates,bai2024multi,zhang2024quad,wang2024greats}; \textbf{(3) synthetic data}: \cx{synthetic data is not as related to this paper?}\zichun{I think a quick mention here is fine since we position our work under the scope of data-efficient pretraining}
rephrasing~\cite{maini2024rephrasing,carranza_datologyai_2024,abdin2024phi} or transforming~\cite{yang2024synthetic,zhou2024programming} existing data with powerful generative models like Mistral~\cite{jiang2023mistral} and Llama~\cite{touvron2023llama,dubey2024llama}.

One commonality of these methods is that they need to define a criterion, either rule-based~\cite{t5,wettig2024qurating} or mathematics-based~\cite{engstrom2024dsdm,yu2024mates}, to measure the quality of individual data points, and optimize the overall data distribution toward the better side. Among these heterogeneous criteria, influence functions~\cite{weisberg1982residuals,koh2017understanding} stand out due to the precise attribution of each data's contribution to the model performance. 
\cx{The rest of this paragraph kinds of focused on something not as important for this paper?} The classic influence function requires a second-order derivative of the model parameter~\cite{koh2017understanding}, which is computationally prohibitive for a model with billion-level parameters. Recent works have proposed various mathematics-based~\cite{pruthi2020estimating,grosse2023studyinginfluence,park2023trak,choe2024your,wang2024greats,zhang2024quad} or model-based methods~\cite{ilyas2022datamodels,engstrom2024dsdm,yu2024mates,isonuma2024unlearning} to approximate the influence function, making its calculation efficient for large foundation models.

% Among these heterogeneous methods, model-aware data selection stands out due to its fine-grained control of the data distribution to match the pretraining model's preferences on-the-fly.

% \paragraph{Influence Functions and Data Shapley.}

\cx{this paragraph is the most related to us and perhaps cite more papers and add more high level discussions of why recent research all argue for group level data attribution/influence/selection.}
Despite the success of applying influence functions to data selection, these approaches assume that each data contributes independently to the model performance~\cite{engstrom2024dsdm,yu2024mates}. This hypothesis may not even hold in the simple linear regression~\cite{saunshi2022understanding,hu2024most}, let alone pretraining non-linear models. To precisely measure group-level influences, we need to consider not only pointwise influence but also how the selected data points interact with each other in the training process~\cite{wang2024capturing}. Traditional methods adopt first-order Taylor expansion~\cite{koh2019accuracy} to approximate the oracle group-level influence, but its precision is limited and the cost is still prohibitive~\cite{grosse2023studyinginfluence}. \citet{wang2024greats} break through the non-independent limits of pointwise influence by incorporating interaction terms into influence estimation and recalculating influences at every step, i.e., the greedy algorithm~\cite{nemhauser1978analysis}. 
% \citet{wang2024capturing} further propose data value embeddings to incorporate the cumulative and interactive effect of a data point into its influence estimation.
% eliminating the need to retrain the model to obtain subset influence~\cite{ghorbani2019data}.
\citet{wang2024capturing} further propose data value embeddings to quantify the cumulative and interactive effect of one data with other training points.
Although providing valuable insights, their findings are limited to small-scale setups, and scaling their method to a larger data pool remains challenging.

% \paragraph{Non-Additive Nature of Pointwise Data Influence and Previous Attempts to Address it.}