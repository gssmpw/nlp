\section{Experimental Setup}

\textbf{Model and Data.} We conduct our main experiments following standard setups in DataComp-LM (DCLM)~\cite{li2024datacomp}, a formalized competition to benchmark the effectiveness of pretraining data selection. 
% \cx{state that 1) this is a very realistic and formal setting (and why) 2) that the baselines are very strong, compared to noisy corpora like c4, fineweb, RPJ, etc. many dataselection methods do not work on this realist setting. thus we prefer this set up because benefits here are more likely to translate to real pretraining setting.} 
The data curation pipeline in DCLM integrates heuristic cleaning, deduplication, and model-based filtering, yielding stronger baseline performance compared to other open-source datasets such as C4~\cite{t5}, FineWeb~\cite{penedo2024fineweb}, and RedPajama~\cite{weber2024redpajama}. This rigorous pipeline poses challenges for many existing data selection methods, which may not even beat random selection. Beyond ensuring high data quality, DCLM also standardizes data loading, training hyperparameters, and evaluation tasks, making the competition strictly fair. Consequently, performance gains achieved on this benchmark are more likely to generalize effectively to real-world pretraining scenarios.

Specifically, we choose two experiment scales from DCLM, 400M-4x\footnote{400M-4x is not a predefined setup in the original DCLM, but we extend its 400M-1x setup to train for 4x longer (4x more tokens) for better evaluation stability.} and 1B-1x, where 400M/1B denotes the model size, and 4x/1x denotes the relative quantity of pretraining tokens for this model size. For both scales, the original data pool size is about 1.64T and the number of pretraining tokens is about 30B. We pretrain all models from scratch and evaluate pretrained models with 22 downstream tasks from DCLM-Core in either zero-shot or few-shot manners. Detailed configurations of the evaluation tasks can be found in Table~\ref{tab:full-400m}.  These tasks provide a holistic assessment of the essential abilities of pretrained models, including commonsense reasoning, language understanding, reading comprehension, symbolic problem solving, and world knowledge. We use centered accuracy as the primary evaluation metric.
% \zichun{DCLM 1B-5x? FineWeb? I think we don't really need FineWeb cause DCLM has been verified better than FineWeb and FineWeb-Edu.}
% Our data pool is 60B tokens from DCLM-Baseline~\cite{li2024datacomp}, the state-of-the-art open-source pretraining dataset.
% with a fine-tuned encoder model~\cite{merrick2024arctic}

\input{tables/ablation}

\textbf{Baselines.} We compare our method with (1) random selection (DCLM-Baseline), (2) Edu Classifier~\cite{penedo2024fineweb}: educational valuation of data distilled from LLama3-70B-Instruct~\cite{dubey2024llama}, (3) MATES~\cite{yu2024mates}: individual data influence estimation with data influence models, and (4) Quad~\cite{zhang2024quad}: cluster-level influence estimation and diversification with multi-armed bandit~\cite{vermorel2005multi}. These baselines cover state-of-the-art data selection techniques, including LLM rating, influence functions, and diversification. 
We also compare our method with earlier baselines in Appendix~\ref{sec:mates}, following the setup in MATES.
Some recent works, such as multi-agent collaboration~\cite{bai2024multi}, GREATS~\cite{wang2024greats} and TSLOO~\cite{wang2024capturing}, have not been open-sourced, hindering direct comparison.

\textbf{Implementation Details.} We sample a size-128 subset from FLAN~\cite{wei2022flan} as our reference task $\mathcal{D}_r$ for its exceptional generalization abilities~\cite{flan-t5} and fine-tune \texttt{bge-base-en-v1.5}~\cite{xiao2024bge} as our relational data influence model. The size of the data pairs $\mathcal{P}$ to collect oracles is 80k and the size of the bootstrapping data pool $\mathcal{D}_l$ is 800k. The bootstrapping hyperparameter of the first and second data is 1k (i.e., $k$) and 20 (i.e., $m$). In inference, we group all data points into 10k clusters using k-means~\cite{lloyd1982least} following the optimal choice in Quad. More implementation details can be found in Appendix~\ref{sec:exp-details}.

% We set the same selection ratio for all the baseline methods.

% We sample a 128-size subset from its training set to improve the efficiency of oracle collection.

% \cx{we should also include the FLOPs versus performance trend as in MATES, as reviewers will question about it.}

% \cx{Check this section carefully and make sure no important details are missing. Rightnow there are. e.g., evaluation is zero-shot. Also which model/backbone we pretrain? is it pretraining from scratch? Do not make the same mistake as in MATES's submission.}