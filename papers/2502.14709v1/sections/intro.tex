\vspace{-0.5cm}
\section{Introduction}
% \cx{somehow can we make the motivation more about data-efficient pretraining, with selecting the best data to train for the model's next stage as a way to achieve that? Kind of worried that reviewers may just say that we should not do data selection because Ilya said it is over.}

% The significance of pretraining data selection (pretraining has not ended). , making model with fewer compute budget beat the with counterpart tens of compute (DeepSeek).

% Scaling large foundation models to billions of parameters and pretraining them with trillions of tokens can significantly boost their generalization abilities~\cite{brown2020language,dubey2024llama}, as predicted by scaling laws~\cite{kaplan2020scaling,Chinchilla}. With more and more data incorporated into the training process and demanding more computation, recent research has leveraged heterogeneous data curation techniques~\cite{wettig2024qurating,engstrom2024dsdm,yu2024mates} to selectively utilize better data and enhance pretraining efficiency~\cite{li2024datacomp,carranza_datologyai_2024}.

% Data-efficient pretraining~\cite{albalak2024dataselectionsurvey} is critical in the foundation model era, enabling reduced pretraining costs while elevating scaling laws~\cite{kaplan2020scaling,Chinchilla}.
% Prior work has explored various data curation techniques, including rule-based heuristics~\cite{t5,penedo2024refinedweb}, semantic deduplication~\cite{abbas2023semdedup,tirumala2024d4}, identifying data similar to high-quality corpora~\cite{xie2023data,li2024datacomp}, scoring data quality with LLMs~\cite{wettig2024qurating,penedo2024fineweb}, and estimating data influence on downstream tasks~\cite{engstrom2024dsdm,yu2024mates}.

% a fundamental research question arises: \textit{Are we spending the available computation wisely on the most useful data to approach the optimal pretraining performance given a compute budget?}

% \cx{this is too detailed. More like related work. We should get to the challenge faster. Maybe merge the first two paragraphs directly talking about recent progress in data valuation for pretraining with influences, but mainly individual.}
% Recent works have found that, with careful data curation, foundation models can match or even outperform larger counterparts with orders of magnitude fewer pretraining FLOPs~\cite{li2024datacomp,carranza_datologyai_2024}.

% I don’t think we should go to that far mentioning “post-training”. 


\begin{figure}[t]
\vspace{-0.3cm}
    \centering
    \begin{subfigure}{0.227\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/overlap_err.pdf}
    \caption{Selected data overlap.}
    \label{fig:overlap-err}
    \end{subfigure}
    ~
    \begin{subfigure}{0.2355\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/eval_motivation.pdf}
    \caption{Downstream evaluation.}
    \label{fig:upperbound-err}
    \end{subfigure}
    % \caption{Reference loss trajectories of selecting data with their data influence at every step greedily (Greedy), selecting data with their local data influence at the beginning (Local Influence), and random selection (Random). We run this experiment in the decay stage of pretraining~\cite{hu2024minicpm} with five different data splits and plot the mean and std of the reference loss.}
    \vspace{-0.2cm}
    \caption{Misalignment (a) and performance gap (b) between data selected by optimizing group vs. individual influences.} 
    %The selection ratio is set to 20\%. We run this experiment in the decay stage of pretraining~\cite{hu2024minicpm} with five different data splits and plot the mean and std of metrics.\cx{caption too detailed for introduction. Move description to experimental details.}}
    \label{fig:upperbound}
    \vspace{-0.2cm}
\end{figure} 


What data to train upon is as crucial as which model to train. Recent research has developed various data curation methods that significantly improve pretraining efficiency and effectiveness~\cite{albalak2024dataselectionsurvey}.
For example, leveraging LLM-based quality rating and influence-based data valuation, one can select pretraining data that doubles the quality-FLOPs scaling of large language models~\cite{penedo2024fineweb,yu2024mates}, or enables smaller models to outperform larger counterparts~\cite{carranza_datologyai_2024}.

% or tailor the synthetic training data generation process that double the self-play effectiveness in post-training~\cite{motessori}. \cx{add references}

These data-efficient training methods often model the utility of each training data point \textit{individually}~\cite{engstrom2024dsdm}, a strong assumption as model training barely just uses one data point. 
In this paper, we argue that effective training data should be constructed at the \textit{group level}, where the influence of a set of data points is modeled jointly, and the goal of data curation is to find the set with \textit{maximum group influence}.
% As illustrated in Figure~\ref{fig:upperbound}, in a typical pretraining data selection setting~\cite{li2024datacomp, yu2024mates}, individual data influences quickly diverge from group influences after merely a hundred data points—less than a single batch in modern pretraining workflows. Notably, the dataset curated to maximize group influences exhibits a significantly higher upper bound than individual influences, nearly doubling the potential of data-efficient pretraining.
In a typical pretraining data selection setting~\cite{li2024datacomp, yu2024mates}, individual data influences quickly diverge from group influences after merely a hundred data points—less than a single batch in modern pretraining workflows (Figure~\ref{fig:overlap-err}). Notably, the dataset curated to maximize group influences exhibits a significantly higher upper bound than individual influences, nearly doubling the potential of data-efficient pretraining  (Figure~\ref{fig:upperbound-err}).

% data selected to optimize group and individual influence quickly diverges as the pool expands. Figure~\ref{fig:upperbound-err} further demonstrates the effectiveness of selected data on model performance in the decay setting~\cite{yu2024mates}, where group influence achieves a 22.6\% relative gain over individual influence. This result validates the limits of individual influences and the promises of group-level influences in determining optimal subsets for data-efficient pretraining.
% Prior work has explored various data curation techniques, including rule-based heuristics~\cite{t5,penedo2024refinedweb}, semantic deduplication~\cite{abbas2023semdedup,tirumala2024d4}, identifying data similar to high-quality corpora~\cite{xie2023data,li2024datacomp}, scoring data quality with LLMs~\cite{wettig2024qurating,penedo2024fineweb}, and estimating data influence on downstream tasks~\cite{engstrom2024dsdm,yu2024mates}.


% Pretraining large foundation models of billions of parameters with trillions of tokens necessitates tremendous computational resources~\cite{brown2020language,dubey2024llama}. This highlights the critical need for data-efficient pretraining, where carefully curated datasets can significantly enhance pretraining efficiency~\cite{li2024datacomp}. Techniques such as rule-based filtering~\cite{t5}, LLM-based rating~\cite{wettig2024qurating}, and influence-based selection~\cite{engstrom2024dsdm,yu2024mates} effectively reduce computational costs while improving model performance.

% Among these methods, influence-based selection stands out due to its effective attribution of individual data utility~\cite{koh2017understanding}. 
% However, a gap exists between individual and group-level data utility, as interactions among data points can jointly affect model performance through effects such as cancellation or amplification~\cite{saunshi2022understanding,hu2024most}. Capturing these interactions is crucial to evaluate the true utility of data group and to select the optimal training subset~\cite{wang2024capturing}. As illustrated in Figure~\ref{fig:overlap-err}, data selected to optimize group and individual influence quickly diverges as the pool expands. Figure~\ref{fig:upperbound-err} further demonstrates the effectiveness of selected data on model performance in the decay setting~\cite{yu2024mates}, where group influence achieves a 22.6\% relative gain over individual influence. This result validates the limits of individual influences and the promises of group-level influences in determining optimal subsets for data-efficient pretraining.

% Despite the effectiveness of these data curation strategies~\cite{li2024datacomp}, they primarily rely on the independent assumption of data utility—each training point contributes independently to model performance~\cite{engstrom2024dsdm,yu2024mates}. However, this assumption neglects the extensive and complex interactions between training points~\cite{wang2024greats}. These interactions, supported by both theoretical insights~\cite{saunshi2022understanding,hu2024most} and empirical evidence~\cite{wang2024capturing,wang2024greats}, are critical to understanding the true utility of data and determining the optimal training subset. 

% the overlap of data selected by maximizing group-level influence versus those selected by summing individual influence.

% \cx{this can be stated stronger and with more information. This is a well-believed intuition with theoretical backups.}, making it drift away from the optimal subset selection~\cite{saunshi2022understanding,hu2024most}.

% These curation strategies can be classified into three mainstream categories: domain reweighting~\cite{xie2024doremi,fan2023doge,liu2024regmix}, data selection~\cite{wettig2024qurating,yu2024mates,zhang2024quad}, and synthetic data~\cite{maini2024rephrasing,carranza_datologyai_2024,abdin2024phi}. Among them, model-aware data selection has shown prominent advantages by selecting data on-the-fly via influence functions~\cite{weisberg1982residuals,koh2017understanding}, making pretraining more data-efficient than traditional methods~\cite{yu2024mates,zhang2024quad}. has proposed heterogeneous ways to wisely leverage the pretraining data pool.

% \cx{we should use the intuition and theory in previous work as the main argument. Our analysis in 1.a is a quick conformation, make it quite short. 1.b is to be introduced in the paragraph when we talk about empirical result}

% Recent attempts~\cite{wang2024capturing,wang2024greats} have proposed mathematical ways to efficiently estimate the interactions between data points and incorporate them into data selection, but their findings are limited to small-scale experiments.

% The non-additive nature of data influence makes, especially in the long-term pretraining run (non-linear neural networks and non-convex loss functions).

% \cx{briefly mentioned the theorical back up in paragraph 3}
% a model-based method (\cx{what is model-based method?}) to efficiently approximate group-level data influence and approach the optimal subset selection for pretraining foundation models.
% a novel pretraining data selection method that models both individual influence and pairwise interactions to optimize group-level data utility.
% an efficient approach to approximating group-level influence for optimizing data efficiency in foundation model pretraining.
% Specifically, we decompose oracle group-level influence into a sum of individual influences, weighted by pairwise relationship terms, and fine-tune a relational data influence model to approximate this decomposition.
% The fine-tuned relational data influence model can predict both individual influences and the relationship between data points, aiming to select data that maximize group-level influence.


% In this paper, we introduce \textit{Group-Level Data Influence Modeling (Group-MATES)}, 
% an efficient data curation approach that captures and maximizes group-level data utility to optimize pretraining efficiency.

To efficiently capture and optimize group-level data influences, we introduce \textit{Group-Level Data Influence Modeling (Group-MATES)}, a novel data-efficient pretraining approach.
We collect oracle group-level influences by locally probing the pretraining model and fine-tune a relational data influence model to approximate them. Aligned with theoretical insights~\cite{wang2024capturing}, this model estimates oracles as a relationship-weighted aggregation of individual influences, with relationships learned by embedding similarity. 
The fine-tuned data influence model then selects data by maximizing group-level influence predictions over the entire training set.
To accelerate inference, data are clustered based on their embeddings generated by the data influence model. 
This influence-aware clustering ensures that interactions within the same cluster are significantly stronger than those across clusters, allowing independent selection within each cluster with minimal loss of relational information.
 
% Group-MATES performs virtual training runs on data pairs to obtain the oracle group-level influences and fine-tunes a relational data influence model to approximate them. The fine-tuned relational data influence model then iteratively bootstraps more diverse data from a larger pool to train its next iterate with better oracle labels.
% The relational data influence model selects data sequentially, scoring each data by its individual influence plus the relationship term with previously selected data to maximize group-level influence.

We empirically verify the effectiveness of Group-MATES on the DCLM~\cite{li2024datacomp} pretraining data selection benchmark. DCLM employs a state-of-the-art data processing pipeline that integrates extensive heuristic cleaning, deduplication, and model-based filtering. Consequently, many existing selection methods are ineffective when applied to it.
On this rigorous benchmark, Group-MATES achieves over 10\% relative performance gain over DCLM-Baseline across 22 downstream tasks. Our method also significantly outperforms all leading data curation baselines, including FineWeb-Edu Classifier~\cite{penedo2024fineweb}, MATES~\cite{yu2024mates}, and Quad~\cite{zhang2024quad}, with over 5\% relative performance improvements.

% We also demonstrate the upper bound of optimal group-level selection in Figure~\ref{fig:upperbound-err}. Optimal group-level selection consistently achieves the lowest reference loss throughout training while exhibiting greater stability across runs. Although individual selection may temporarily match group-level performance, it quickly falls behind over longer training periods. 

Further analyses validate the effectiveness of our relational data influence model in approaching oracle group-level influences and their strong agreements on relationships between data points. Additionally, case studies demonstrate the ability of our data influence model to identify cancellation and amplification effects between data points, revealing intricate interactions among training data.
% \cx{(what is captured in group influence and what are the differences with pintwise?) here}.
% Ablation studies underscore the critical role of considering pairwise relationships in our data selection pipeline.

We summarize the highlights of our work as follows:
\begin{enumerate}
    % \item \textbf{relational data influence modeling:} approximate not only the individual data influence but also the relationship between the selected data. This enables us to break through the non-additive limits of individual influence. \cx{because we do pairwise in a sequential way, we can claim we are set level?}
    % \item \textbf{Bootstrapping data influence models:} sample probing data from a larger pool annotated by the last round of the data influence model to select the representative subset to collect influence, thus making the data influence model learn better in the next round.
    % \item \textbf{Model-aware clustering-based selection:} group the data with similar influence representations (DIM's hidden states). Then, we adopt multi-armed bandit techniques to iteratively choose the source cluster and leverage pairwise DIM to sequentially select data within the source cluster.
    % \item We propose Group-MATES, a model-based method to approximate group-level data influence and select a more effective subset for data-efficient pretraining. \cx{something stronger?} 
    \item We propose Group-MATES, a novel data curation approach that captures and maximizes group-level data utility to advance data-efficient pretraining.
    \item We train a relational data influence model to efficiently approximate probed oracle group-level influences and enable its fast inference via influence-aware clustering.
    \item Group-MATES achieves a new state-of-the-art on the DCLM benchmark. Further analyses confirm the effectiveness of our relational data influence model in capturing intricate interactions between data points.
    % We examine the cancellation and amplification effects between data pairs, highlighting the need to consider complex interactions in group-level data selection.
\end{enumerate}