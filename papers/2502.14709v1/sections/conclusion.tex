\section{Conclusion}

% influences and pairwise interactions among training data

In this paper, we introduce \textit{Group-Level Data Influence Modeling (Group-MATES)}, a method to advance the frontier of data-efficient pretraining by capturing and optimizing group-level influences. Group-MATES collects oracle group-level influences by locally probing the pretraining model and fine-tunes a relational data influence model to approximate them.
% and iteratively bootstraps diverse oracles to better cover the entire distribution.
It then employs efficient inference through influence-aware clustering to maximize group-level influence prediction.
Evaluations on the rigorous DCLM benchmark demonstrate the superiority of Group-MATES, which achieves a 10\% relative core score gain over DCLM-Baseline and 5\% over individual-influence-based approaches, setting a new state-of-the-art.
Further analyses highlight the effectiveness of our method in capturing complex interactions between data points.
% and the benefits of incorporating these interactions into data selection.
% and the effectiveness of bootstrapping in improving oracle label efficiency.
Our work reveals the promising potential of modeling group-level influences in data-efficient pretraining, and we hope it inspires effective techniques to analyze, capture, and leverage group-level influences in the future.
% effective data curation strategies in the future.