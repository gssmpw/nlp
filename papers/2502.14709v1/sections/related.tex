\section{Related Work}

% \paragraph{Data-Efficient Pretraining.} 

Data-efficient pretraining~\cite{albalak2024dataselectionsurvey} aims to leverage the most effective data to reduce the pretraining cost and achieve better scaling laws~\cite{kaplan2020scaling,Chinchilla}. Prior works propose various data curation techniques, such as (1) domain reweighting: grouping data by their source domains (e.g., wikipedia, github, etc.) and finding the optimal domain weights via small proxy models~\cite{xie2024doremi,fan2023doge,liu2024regmix}; (2) synthetic data:
rephrasing~\cite{maini2024rephrasing,abdin2024phi} or transforming~\cite{yang2024synthetic,zhou2024programming} existing data with powerful generative models; (3) data selection: rule-based filtering~\cite{t5,penedo2024refinedweb}, deduplicating semantically similar data~\cite{abbas2023semdedup,tirumala2024d4}, upweighting data that resemble high-quality corpora~\cite{xie2023data,li2024datacomp}, scoring data quality with LLMs~\cite{wettig2024qurating,penedo2024fineweb}, and attributing data influence~\cite{engstrom2024dsdm,yu2024mates,zhang2024quad}.

Among them, influence-based methods effectively assess individual data utility through influence functions~\cite{weisberg1982residuals,koh2017understanding}, a theoretical tool to measure training data influence on model prediction.
Variations and extensions of influence functions have demonstrated effectiveness in many fields, including identifying mislabeled examples~\cite{pruthi2020estimating,yeh2022first}, selecting high-quality instruction data~\cite{xia2024less}, or generating influential synthetic data~\cite{li2024montessori}. Recent works, such as DsDm~\cite{engstrom2024dsdm} and MATES~\cite{yu2024mates}, efficiently scale influence functions to the pretraining data curation and significantly elevate scaling curves.

% in the simple linear regression~\cite{saunshi2022understanding,hu2024most}, let alone pretraining non-linear models
% cancellation and amplification effects
% \cx{The rest of this paragraph kinds of focused on something not as important for this paper?} The classic influence function requires a second-order derivative of the model parameter~\cite{weisberg1982residuals,koh2017understanding}, which is computationally prohibitive for a model with billion-level parameters. 
% \cx{add more high level discussions of why recent research all argue for group level data attribution/influence/selection.}. 
% Traditional methods adopt first-order Taylor expansion~\cite{koh2019accuracy} to approximate the oracle group-level influence, but its precision is limited and the cost is still prohibitive~\cite{grosse2023studyinginfluence}. When a data point is selected, the importance scores of the remaining data points will usually change.

% As demonstrated by~\citet{hu2024most}, interactions between data points can lead to both cancellation and amplification effects, where the group-level influence often deviates significantly from the summation of individual influences.

% To precisely measure group-level influences, it is crucial to consider both individual influences and the interactions among training points~\cite{koh2019accuracy,saunshi2022understanding}. 

Theoretical studies highlight the distinction between individual and group-level data influences~\cite{koh2019accuracy,saunshi2022understanding}, where data interactions can cancel or amplify individual influences~\cite{hu2024most,huang2024approximations}. To mitigate this gap, ZAMinfluence~\cite{broderick2020automatic} iteratively selects the most influential point to approximate the maximization of group-level influences, i.e., the greedy algorithm~\cite{nemhauser1978analysis}. Building upon this work, researchers effectively applied group-level influences in data pruning~\cite{yang2023dataset}, enhancing trustworthiness~\cite{wang2022understanding,sattigeri2022fair,chhabra2024data}, LLM fine-tuning~\cite{guu2023simfluence}, and notably, pretraining data curation~\cite{wang2023farewell,wang2024greats}. 
Recently, \citet{wang2024capturing} introduced data value embeddings as an alternative to the greedy algorithm, capturing both cumulative and interactive data influences.
Although providing valuable insights, their findings are limited to small-scale setups, and scaling their methods to real-world pretraining scenarios remains challenging.

% Simfluence~\cite{guu2023simfluence} models these interactions through a linear Markov process with both multiplicative and additive factors. 
% GREATS~\cite{wang2024greats} corrects data influences with second-order interaction terms and updates them step-by-step.
% But in reality, we observe that training examples interact in highly non-additive ways due to factors such as inter-example redundancy, training order, and curriculum learning effects. 