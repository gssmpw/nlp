\section{Evaluation Results}

% \cx{be more concise in section outline intro. (cut length by half)}
% In this section, we first evaluate the performance of Group-MATES as well as baseline methods on DCLM (§~\ref{sec:performance}). We further demonstrate the effectiveness of group-level data influence modeling (§~\ref{sec:eff-modeling}), bootstrapping data influence models (§~\ref{sec:eff-bootstrapping}), and influence-aware clustering (§~\ref{sec:eff-clustering}). Finally, we conduct comprehensive ablation studies to analyze the contributions of each component in our approach (§~\ref{sec:ablation}).

In this section, we evaluate Group-MATES and baselines on DCLM (§\ref{sec:performance}), conduct ablation studies (§\ref{sec:ablation}), and demonstrate the effectiveness of group-level data influence modeling (§\ref{sec:eff-modeling}), training relational data influence models (§\ref{sec:eff-bootstrapping}), and influence-aware clustering (§\ref{sec:eff-clustering}). Additional results and analyses can be found in Appendix~\ref{sec:additional}.
% with ablation studies to assess the contribution of each component (§~\ref{sec:ablation}).

\subsection{Overall Performance}
\label{sec:performance}

\begin{figure}[t]
    \centering
    % \begin{subfigure}{0.241\textwidth}
    % \centering
    % \includegraphics[width=1.0\linewidth]{figures/overlap_dim.pdf}
    % \caption{Selected data overlap.}
    % \label{fig:overlap-dim}
    % \end{subfigure}
    % ~
    \begin{subfigure}{0.218\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/upperbound_dim.pdf}
    \caption{Reference loss.}
    \label{fig:upperbound-dim}
    \end{subfigure}
    % ~
    \begin{subfigure}{0.236\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/eval_upperbound.pdf}
    \caption{Downstream evaluation.}
    \label{fig:upperbound-eval}
    \end{subfigure}
    \vspace{-0.35cm}
    \caption{Reference loss trajectories (a) and evaluation results (b) of data selected by upper bound group-level influence (Group), our relational data influence model (Rel), individual data influence model~\cite{yu2024mates} (Indiv), and random (Rand).}
    \vspace{-0.3cm}
    \label{fig:pairwise}
\end{figure}

% \cx{We should emphasize more on the benefits/behavior of set/pair level data influence modeling, with 1) ablation to show its empirical benefits in end results 2) analysis to show what different data it is able to pick up compared to MATES, both in terms of quantative overlap but also case studies? 3) the approximation to upper-bound inference in the next subsection.}

Table~\ref{tab:main} presents the overall performance on the DCLM benchmark. 
Group-MATES significantly outperforms random selection, achieving relative improvements of 10.1\% and 4.2\% core scores across 22 downstream tasks in 400M-4x and 1B-1x settings, respectively. 
These performance gains are substantial, as even the strong Edu Classifier—recognized for its effectiveness on less curated datasets~\cite{penedo2024fineweb}—fails to surpass random selection on 1B-1x setup. 
% \jie{do we want to reiterate DCLM has already gone through a set of rigorous selection steps, thus `random` here is not truly random. Easiest way is to refer readers to Sec.4}.
Furthermore, Group-MATES demonstrates superior generalization capabilities compared to other individual-influence-based data selection baselines, such as MATES and Quad, highlighting the benefits of modeling group-level influences in data curation.
% \cx{add the number of tasks won in table 1? if ours are stable enough, especially in 1b setting.}

% On commonsense reasoning tasks, Group-MATES seems to lag behind other baseline methods in the 400M-4x setting, while the reason is that 400M models demonstrate very unstable performances on CommonsenseQA, which dominates the score in this group. 
% This phenomenon does not affect the 1B model too much. 
On world knowledge tasks, Edu Classifier performs the best since it is specifically optimized for educational value that prefers knowledge-related data. As a price of favoring too much educational value, Edu Classifier performs worse than random selection on the majority of other tasks. This finding suggests that on a well-curated dataset (e.g., DCLM), simple heuristic-based curation, such as the educational value, may tend to overfit. In contrast, Group-MATES evaluates data utility based on model preferences and thus curates more effective data to enhance its generalization capabilities.
% Model-aware approaches, like Group-MATES, will consider the model's evolving data preferences, and thus curate more effective data to enhance its generalization capabilities. 

% In Figure~\ref{fig:flops}, we also plot the scaling curves of model performance w.r.t. total FLOPs, including both pretraining and data selection costs. 
We also provide a detailed breakdown of total FLOPs for Group-MATES in Table~\ref{tab:breakdown}. Notably, the data selection procedure of Group-MATES only accounts for 12.2\% and 6.7\% of the total FLOPs in 400M-4x and 1B-1x settings, respectively. The relative selection cost in larger setups is generally smaller because their pretraining FLOPs dominates the total computation, while the training and inference costs of our data influence model remain stable. Considering the remarkable improvements our method achieves on the DCLM benchmark, the associated cost becomes negligible.

\begin{figure}[t]
    \centering
    % \vspace{-0.3cm}
    \begin{subfigure}{0.212\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/all_pw.pdf}
    \caption{Relationship distribution.}
    \label{fig:dis-rel}
    \end{subfigure}
    \begin{subfigure}{0.212\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/all_overlap.pdf}
    \caption{Data overlap.}
    \label{fig:overlap-rel}
    \end{subfigure}
    \vspace{-0.35cm}
    \caption{Distributions of relationship terms (a) and data overlap (b) between our relational data influence model (DIM) and oracle.} 
    \label{fig:relationship}
    \vspace{-0.3cm}
\end{figure}

\input{tables/cases}

\begin{figure*}
    \centering
    \vspace{-0.2cm}
    \begin{subfigure}{0.238\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/bs_distribution.pdf}
    \caption{Group distribution.}
    \label{fig:pairwise-dis}
    \end{subfigure}
    ~
    \begin{subfigure}{0.245\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/bs_distribution_1.pdf}
    \caption{Individual distribution.}
    \label{fig:individual-dis}
    \end{subfigure}
    ~
    \begin{subfigure}{0.232\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/bs_ablation.pdf}
    \caption{Group approximation.}
    \label{fig:pairwise-approx}
    \end{subfigure}
    ~
    \begin{subfigure}{0.232\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/pw_ablation.pdf}
    \caption{Individual approximation.}
    \label{fig:individual-approx}
    \end{subfigure}
    \caption{Distributions of oracle group (a) and individual (b) influences across different numbers of bootstrapping iterations. Performance of group (c) and individual (d) data influence approximations.}
    \label{fig:pwdim}
    \vspace{-0.3cm}
\end{figure*}

\subsection{Ablation Studies}
\label{sec:ablation}

% \cx{It feels smoother if we start with this ablation right after overall, and then three analyses afterwwards, each illustrating benefit of one tech.}\zichun{I think this part is not as insightful as the other three to be prioritized? The most important message here may be just the importance of relationship modeling.} \cx{it is not insightful thus we can treat it as a lead/entry point to later more insightful ones. end the findings with the least insightful experiemtns is not ideal.}

Table~\ref{tab:ablation} shows the ablation studies of three key components in Group-MATES. When we remove relationship terms in the selection and only consider individual influences, the overall performance gain over random selection decreases by 3.6\%;  discarding the bootstrapping technique and replacing influence-aware clustering with BGE clustering also yield observable performance loss, but the drop is not as significant as removing relationship terms. This experiment highlights the benefits of having relationship measurements between training points in data-efficient pretraining.

\subsection{Analysis on Group-Level Data Influence Modeling}
\label{sec:eff-modeling}

% \zichun{I still tend to show the amplification/cancellation effect identified by our DIM since this is the most direct way to show our effectiveness? We can state that there do exist cases that are identified by oracle as amplification, but our DIM cannot capture well, while from our observation these cases are complex and even hard for human to interpret.}

% To study the characteristics of group-level influence modeling, we compare the overlap and reference loss trajectories of data selected by our relational data influence model with those selected by the individual data influence model~\cite{yu2024mates}.
To illustrate the effectiveness of group-level influence modeling, we compare the reference loss trajectories and the evaluation results of data selected by greedily maximizing group influences~\cite{broderick2020automatic}, our relational data influence model, individual data influence model~\cite{yu2024mates}, and random.
In this experiment, we set the selection ratio to 20\% and utilize the 100-step decay stage of pretraining~\cite{hu2024minicpm} to enlarge the gap between different selections. All analyses are run with five different data splits, and we calculate the mean and std of the metrics to ensure the stability of observations. 
% As shown in Figure~\ref{fig:overlap-dim}, the selected data from these two models quickly diverges as the scale of the data pool increases. 

As shown in Figure~\ref{fig:upperbound-dim}, the subset selected by our relational data influence model consistently achieves a lower reference loss than the individual one after the initial steps. The evaluation results in Figure~\ref{fig:upperbound-eval} further validate the superiority of our relational data influence model, with a 6\% relative performance gain compared to individual selection after the decay. We also emphasize the significant potential of optimal group-level selection, which nearly doubles the performance gain even in the short decay stage. Our method represents a critical step toward efficiently approaching optimal performance and has demonstrated its effectiveness. 

% While a gap remains between our selected data and the optimal group obtained by greedily minimizing reference loss at each step~\cite{nemhauser1978analysis}, our method effectively approaches the optimal performance with the relationship approximation.

\begin{figure*}
    \centering
    \vspace{-0.3cm}
    \begin{subfigure}{0.22\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/box_bge.pdf}
    \caption{BGE clustering.}
    \label{fig:box-bge}
    \end{subfigure}
    ~
    \begin{subfigure}{0.22\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/box_pairwise.pdf}
    \caption{Influence-aware clustering.}
    \label{fig:box-pairwise}
    \end{subfigure}
    ~
    \begin{subfigure}{0.228\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/cluster_corr.pdf}
    \caption{Influence correlation.}
    \label{fig:cluster-corr}
    \end{subfigure}
    ~
    \begin{subfigure}{0.243\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/intra_inter.pdf}
    \caption{Relationship distribution.}
    \label{fig:intra-inter}
    \end{subfigure}
    \caption{Comparison of individual influence distributions of BGE (a) and influence-aware clustering (b). Correlation between cluster-averaged influences and original individual influences (c). Relationship term distribution in intra-cluster and inter-cluster scenarios (d).}
    \label{fig:clustering}
    \vspace{-0.5cm}
\end{figure*}

We further compare the distributions of relationship terms identified by our relational data influence model and the oracle. Following~\citet{hu2024most}, we classify these relationships into two types: cancellation and amplification. Cancellation occurs when the relationship term, $\text{sim}(\textbf{h}_{x_{i1}},\textbf{h}_{x_{i2}})$, is greater than 0, whereas amplification occurs when it is lower than 0.
Figure~\ref{fig:dis-rel} shows a strong similarity between our relational data influence model and the oracle in relationship measurements. Figure~\ref{fig:overlap-rel} further reveals that our data influence model effectively captures cancellation effects, but struggles with amplification, particularly in the [-1, -0.5) interval (i.e., tail amplification cases). Explaining and modeling these challenging amplification effects will be an interesting direction for future work. 
% \jie{add some discussions on possible future directions to close the gap between our methods and oracle.}

% While our relational data influence model can represent these interactions, differences remain in the distributions of relationship terms between our model prediction and the oracle, as shown in Figure~\ref{fig:dis-rel}. Here, the oracle relationship term is calculated in the same way as the prediction formula $\Theta(x_{i1}, x_{i2})$, with the predicted individual influence replaced by the oracle. 
% Figure~\ref{fig:overlap-rel} further reveals that the relationship terms that fall in the [-1, -0.5) interval (i.e., tail amplification cases) are the most challenging for our data influence model to capture. 
% We manually observe these cases, but cannot interpret them comprehensively. 

% \cx{show the distribution of learned pariwise weights and oracle pairwise weight-ish, to show how many are cancelation and how many are amplificaiton, then do a case study of each of the four (oracle|learned by cancel|amplify. If learned amplify is not so good, discuss why you think so and make it a potential future work. This is the most important experiment missing. The next one is oracle performances comparisons of group versus individual.}

We also present a case study in Table~\ref{tab:cases}. The cancellation effect in the first example arises from misaligned perspectives on education, where data 1 emphasizes parental influence and data 2 highlights teachers' critical roles. In contrast, the amplification effect in the second example emerges from complementary concepts: data 1 requires gcd for its problem solution, while data 2 provides a formal definition of gcd.
% of artistic exploration, with one data reflecting personal growth through acting and the other analyzing directorial creativity. They complement each other to build a broader scope of the performing arts. 
The case study provides an in-depth analysis of how the relationship term in our relational data influence model shapes complex interactions between training points.

\subsection{Effectiveness of Bootstrapping Influence Models}
\label{sec:eff-bootstrapping}

% Figure: make the probing data distribution more spread-out and thus better for the data influence model to learn. (+ both first data \& second data sampling) (+ multiple iterations)

This set of experiments analyzes the effectiveness of bootstrapping relational data influence models. As shown in Figure~\ref{fig:pairwise-dis} and ~\ref{fig:individual-dis}, with bootstrapping, the sampled oracle distributions of both individual and group influences include more edge cases. This validates that bootstrapping effectively identifies diverse data from the tails to collect oracles. As a result, our relational data influence model better approximates the oracle, as illustrated in Figure~\ref{fig:pairwise-approx}. Specifically, a single iteration of bootstrapping can significantly enhance the upper bound of validation Spearman correlation by 0.2. 
The second iteration offers limited additional benefit, as the first iteration is already effective at mining tail cases. 
Considering the additional cost of the bootstrapping process, we adopt only one iteration of bootstrapping in our final setup. 

% We further investigate the fundamental reasons underlying the effectiveness of bootstrapping.

In Figure~\ref{fig:individual-approx}, we also compare our relational data influence model with the individual one in MATES~\cite{yu2024mates} in terms of individual data influence approximation. Interestingly, although our relational data influence model is not directly optimized to approximate individual data influence, it actually performs better than the individual model. We hypothesize that modeling relational information can refine influence representations $\textbf{h}_{x}$ and thus better capture individual influences. Our findings suggest that the relational data influence model can be a more effective way to approximate the influence, either at the individual or group level.

\subsection{Effectiveness of Influence-Aware Clustering}
\label{sec:eff-clustering}

This experiment demonstrates the advantages of using influence-aware clustering compared to vanilla semantic clustering (e.g., BGE~\cite{xiao2024bge}) in efficient inference. 
% To study whether influence-aware clustering can better group the data with similar influences,
% We first show the influence distributions of the lowest- and highest-influence clusters. 
As shown in Figure~\ref{fig:box-bge} and ~\ref{fig:box-pairwise}, influence-aware clustering can significantly reduce the variance of influence distributions within each cluster compared to BGE. 
% This suggests that data points within the same influence-aware cluster will likely have similar influence representations.
To quantify how well influence-aware clustering can group the data with similar influences, we assign each data with its cluster-averaged influence and calculate the Spearman correlation with the original influences. As illustrated in Figure~\ref{fig:cluster-corr}, influence-aware clustering has a consistently better correlation than BGE clustering with different numbers of clusters. Our results suggest that influence-aware clustering effectively groups similar data based on their influences.

We also examine the distributions of the relationship term (i.e., $\text{sim}(\textbf{h}_{x_{i1}},\textbf{h}_{x_{i2}})$) in intra-cluster and inter-cluster scenarios. As shown in Figure~\ref{fig:intra-inter}, the relationship terms are generally higher in the intra-cluster scenario, implying a greater cancellation effect for the data in each cluster. In contrast, inter-cluster relationship terms are distributed around 0 and less significant than the intra-cluster ones. 
This study shows that influence-aware clustering effectively approximates the relationship computation over the full dataset.

% \jie{do we want to briefly mention that we have some important ablations in the appendix, such as Sec B.1, etc.?}

% Clustering-based selection vs. Gumbel-Top-$k$ or brute-force sequential selection (not efficient), e.g., how many data points are selected within the clusters of highest/lowest influence. We should have better diversity.