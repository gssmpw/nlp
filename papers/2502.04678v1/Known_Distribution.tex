\section{The Known Distribution Setting}
Here we consider the situation where the context distribution $\nu$ is known in advance. We use this situation to prepare the readers for the more involved situation where the context distribution $\nu$ is unknown.


\subsection{The Algorithm in the Known Distribution Setting}
When the distribution $\nu$ is known, our algorithm is fairly simple. 
%
Recall that we denote the probability of observing the loss of arm $a$ at round $t$ as $w_t(a) \triangleq \E_{c \sim \nu}[p_{t,c}(\NodeIn(a))]$.
%
We construct an importance-weighted estimator 
\[\widetilde{\ell}_{t,c}(a) = \frac{\ell_{t,c}(a)}{w_t(a)} \indi(a_t \rightarrow a)\]
and determine the playing distribution by the standard FTRL subroutine:
\[p_{t,c} = \underset{p \in \Delta([K])}{\argmin} \left\langle p, \sum_{s=1}^{t-1} \widetilde{\ell}_{s,c} \right\rangle - \frac{1}{\eta} F(p)\]
where $F(p) = \sum_{i=1}^K p_i \log(p_i)$ is the unnormalized negative entropy and $\eta$ is the learning rate.

\begin{algorithm}
\label{alg: known}
\caption{The algorithm when the distribution is known}
\textbf{Input:} Parameters $\eta > 0$\\
\For{$t=1,\dots,T$}{
Set $p_{t,c}= \argmin_{p \in\Delta([K])} \left(\ip{p,\sum_{s=1}^{t-1}\widetilde{\ell}_{s,c}}-\eta^{-1}F(x)\right)$\\
Observe $c_t$ \\
Play $A_t \sim p_{t,c_t}$ \\
Set $\widetilde{\ell}_{t,c}(a) \leftarrow \frac{\ell_{t,c}(a)}{w_t(a)} \mathbb{I}\left(A_{t} \rightarrow a\right)$ for each $c$
} 
\end{algorithm}

\subsection{The analysis in the Known Setting}
The analysis of the algorithm is rather straightforward. We only need to notice the regret decomposition
\[\E[\Reg(\pi)] = \sum_c \Pr(c) \E\left[\sum_t \ell_{t,c}(a_t) - \ell_{t,c}(\pi_c)\right].\]
And for each $c$, we consider
\begin{align*}
    &\E\left[\sum_t \ell_{t,c}(a_t) - \ell_{t,c}(\pi_c)\right]\\
%
   =& \E\left[\sum_t \ip{p_{t,c} - \pi_c,\ell_{t,c}} \right]\\
%
   \le& \frac{1}{\eta} \log K + \frac{\eta}{2} \E\left[\sum_t \ip{p_{t,c}, \widehat{\ell}_{t,c}^2} \right]  \\
   =& \frac{1}{\eta} \log K + \frac{\eta}{2} \E\left[\sum_t \sum_a p_{t,c}(a) \frac{\ell_{t,c}(a)}{w^2_t(a)} \indi(a_t \rightarrow a)  \right] \\
    =&     \frac{1}{\eta} \log K + \frac{\eta}{2} \E\left[\sum_t \sum_a p_{t,c}(a) \frac{\ell_{t,c}(a)}{w_t(a)}  \right] \\
    \le& \frac{1}{\eta} \log K + \frac{\eta}{2} \E\left[\sum_t \sum_a p_{t,c}(a) \frac{1}{w_t(a)}  \right] .
\end{align*}
Here the first inequality comes from the standard FTRL analysis.

We then have
\begin{align*}
    &\E\left[\Reg(\pi)\right]\\
    =& \sum_c \Pr(c) \E\left[\sum_t \ell_{t,c}(a_t) - \ell_{t,c}(\pi_c)\right]\\
%
    \le& \sum_c \Pr(c)  \left(  \frac{1}{\eta} \log K + \frac{\eta}{2} \E\left[\sum_t \sum_a p_{t,c}(a) \frac{1}{w_t(a)}  \right] \right) \\
% 
    =&  \frac{1}{\eta} \log K  + \frac{\eta}{2}  \E\left[\sum_t \sum_a \sum_c \Pr(c) p_{t,c}(a) \frac{1}{w_t(a)}    \right]\\
    =& \frac{1}{\eta} \log K  + \frac{\eta}{2}  \E\left[\sum_t \sum_a p_{t}(a)  \frac{1}{w_t(a)} \right] .
\end{align*}

A standard lemma in graphical bandit (\Cref{lem: graph_inverse}) shows that $\sum_t \sum_a p_{t}(a)  \frac{1}{w_t(a)} = \widetilde{O}(\alpha T)$. Taking $\eta = \Theta(\frac{1}{\sqrt{\alpha T}})$, we get that the algorithm satisfies
\[\Reg(\pi) \le \widetilde{O}(\sqrt{\alpha T}).\]