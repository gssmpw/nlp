\section{Introduction}
%介绍问题，argue其重要性（model层面natural，技术层面open且既有研究fail。故事里要特别提出 Han Fail，所以后来的人不知道这一点。

%还要提到问题是stochastic的，我们的分析中使用adversarial的技巧是不可分割的一部分。因此我们直接得到了adversarial bandit的bound，并把stochastic 的bound作为它的推论。

The contextual bandit problem and the bandit with graphical feedback problem are two common problem settings that extend the vanilla bandit problem.
%
In the contextual bandit setting, a learner repeatedly observes a context, selects an action, and incurs a loss associated with that action.
%
In the bandit with graphical feedback setting, a feedback graph is defined over the arms, where a directed edge between two actions indicates that selecting one action provides loss information for the other.
%
These two settings have found numerous applications in fields such as online advertising, personalized recommendations, and clinical trials~\citep{background_Schapire_2010,backgroundref_Kale_2010,background_villar_2015}.
%
Theoretically, these two settings enrich the feedback structure and lead to interesting new results.

Recently, a new line of work introduced the cross-learning contextual bandits with graphical feedback problem \citep{Han24,MAS24}. 
%
In this setting, there is a contextual bandit with a feedback graph $G$ over the arms, where pulling an arm reveals the loss for all neighboring arms in the feedback graph across all contexts (hence the name cross-learning). 
%
This setting has broad applications in areas such
as bidding-in-auctions~\citep{Han24}.
%
In the bidding-in-auctions problem the context is the bidder’s private value for the item and the action is the bid.
%
The cross-learning structure comes from the fact that the bidder can deduce the utility of the bid under all contexts (i.e., the utility of the bid under different private valuations for the item).
%
The graphical feedback comes from the fact that the bidder can deduce the counterfactual utility of other bids. 
%
For example, if the bidder wins at a certain price, the bidder also wins at a higher price.

The key question for the cross-learning contextual bandits with graphical feedback problem is how to leverage the cross-learning structure to effectively combine the contextual structure with the graphical feedback.
%
A representative question of this type is:
\begin{quote}
Assume the feedback graph $G$ is strongly observable\footnote{The term "strongly observable" means that for every arm $a$, either $a \rightarrow a$ or $a^{\prime} \rightarrow a$ for all $a^{\prime} \neq a$, as defined in \citet{GraphAlon15}}, let $\alpha$ denote the independence number of the feedback graph $G$, does there exist an algorithm with $\widetilde{O}(\sqrt{\alpha T})$ regret?
\end{quote}

This question is of particular interest as the minimax regret for the vanilla bandits (i.e, bandits without contextual structure) with graphical feedback is $\widetilde{O}(\sqrt{\alpha T})$ for a strongly observable graph $G$ \citep{GraphAlon15}. 
%
If there is no cross-learning structure, then there is a trivial $\Omega(\sqrt{\alpha MT})$ lower bound, where $M$ is the number of  contexts.
%This problem is of particular interest because the minimax regret for the vanilla bandits with graphical feedback is $\widetilde{O}(\sqrt{\alpha T})$. 
Thus, this question addresses whether we can use the cross-learning structure to achieve a regret bound totally independent of the number of contexts and matching the minimax regret for the vanilla bandits with graphical feedback.
%

The very first work on the cross-learning contextual bandits with graphical feedback ~\citep{Han24} addresses  this question. 
%
\citet{Han24} showed that this is impossible for stochastic losses and adversarial contexts. 
%
However, \citet{Han24} did not resolve this question for stochastic losses and stochastic contexts. The term "stochastic contexts" means that in each round $t$ the context $c_t$ is drawn i.i.d. from a fixed but unknown distribution $\nu$.
%
\citet{Han24}  only provided an algorithm achieving $\widetilde{O}(\sqrt{\min(\alpha MT,KT)})$ regret for stochastic losses and stochastic contexts. The $\widetilde{O}(\sqrt{\min(\alpha MT,KT)})$ regret means that the  algorithm either does not leverage the graphical feedback structure or does not leverage the cross-learning structure.
%
Later, \citet{MAS24} explicitly listed this question as an open problem. 

In this work, we provide an affirmative answer to this question by giving an algorithm with $\widetilde{O}(\sqrt{\alpha T})$ regret. We propose an algorithm and establish the following theorem.

\begin{reptheorem}{thm:main}[Informal]
    For an oblivious adversarial cross-learning contextual bandit  with a strongly observable feedback graph $G$ and stochastic context distribution $\nu$,  our algorithm achieves a regret bound of order $\widetilde{O}(\sqrt{\alpha T})$.
\end{reptheorem}

Our $\widetilde{O}(\sqrt{\alpha T})$ bound is completely independent of the number of contexts and matches the classical minimax regret of the vanilla bandits (i.e., without contextual structure) with graphical feedback\citep{GraphAlon15}. Thus we provide a nearly tight regret bound (up to logarithmic factors)  for the  cross-learning contextual bandits with graphical feedback problem. 
%

Notably, although this question is listed as an open problem even for \textit{stochastic losses}, we directly solve the strictly stronger \textit{adversarial losses} version of the problem. The stochastic loss case naturally follows as a corollary of the adversarial loss case. 


\subsection{Technical Overview}

%解释我们 a regret bound independent of the contexts number $M$ 的原因。最重要的是 stochastic nature 和 adversarial bandit techniques.
The key to achieving our $\widetilde{O}(\sqrt{\alpha T})$ regret bound is leveraging the stochastic nature of the contexts and,
%
surprisingly, applying the adversarial bandit techniques to the stochastic bandit problem.
%
We first use the stochastic nature of the contexts to decompose the regret as
\[\Reg(\pi) = \sum_c \Pr(c) \left( \sum_{t} \ip{p_{t,c} - \pi_c, \ell_{t,c}} \right).\]
%
We then further leverage the stochastic nature of the contexts and the adversarial bandit techniques to ensure that, for each fixed context $c$ we have
%
\[\sum_{t} \ip{p_{t,c} - \pi_c, \ell_{t,c}} \le \widetilde{O}(\sqrt{\alpha T}).\]
%
By combining these two propositions, we obtain the desired  $\widetilde{O}(\sqrt{\alpha T})$ regret bound.

%简介Han24 的算法 
To understand the importance of leveraging the stochastic nature of the contexts and applying the adversarial bandit techniques,
%
we examine why the algorithm in~\citet{Han24} fails to achieve the desired $\widetilde{O}(\sqrt{\alpha T})$ regret bound.
%
Briefly speaking, the algorithm in~\citet{Han24} runs a separate Arm-Elimination type algorithm for each context.
%
The Arm-Elimination algorithm in~\citet{Han24} enhances the standard Arm-Elimination approach in two ways.
%
First, the algorithm in~\citet{Han24} utilizes the graphical feedback structure to enhance the Arm-Elimination algorithm.%
%
The Arm-Elimination algorithm used in~\citet{Han24} pulls the arm with the largest out-degree in the set of all under-explored arms when it explores.
%
Furthermore, the algorithm in~\citet{Han24} uses the cross-learning structure to further enhance the Arm-Elimination algorithm.
%
Let $\NodeOut(u) \triangleq \{ v \mid u \rightarrow v\}$ for each node $u$ in graph $G$.
%
For each round $t$, after pulling an arm $a_t$, the Arm-Elimination algorithm in~\citet{Han24} observes the loss $\ell_{t,c}(a)$ for every arm $a \in \NodeOut(a_t)$ and for every context $c$.

%Argue Han24 fail 是因为fail to achieve the bound 是因为没有用上 stochastic nature 和 adversarial bandit techniques，指出重点是我们 observable 的 arm 集合的问题。
The algorithm in~\citet{Han24} does not achieve the desired $\widetilde{O}(\sqrt{\alpha T})$ regret bound.
%
The reason is that the algorithm in~\citet{Han24} failed to properly leverage the stochastic nature of the contexts and apply the adversarial bandit techniques.
%
Although the aforementioned regret decomposition
\[\Reg(\pi) = \sum_c \Pr(c) \left( \sum_{t} \ell_{t,c}(a_t) - \ell_{t,c}(\pi_c) \right)\]
still holds (since the regret decompostion is only about the regret structure, independent of the algorithm structure),
%
the algorithm in~\citet{Han24} did not guarantee that
\[\sum_{t} \ip{p_{t,c} - \pi_c, \ell_{t,c}}\le \widetilde{O}(\sqrt{\alpha T}).\]
%
This is because, the algorithm in~\citet{Han24} runs an Arm-Elimination type algorithm for each context $c$.
%
For each context $c$, to ensure the inequality $\sum_{t} \ip{p_{t,c} - \pi_c, \ell_{t,c}}\le \widetilde{O}(\sqrt{\alpha T})$ holds,
%
the Arm-Elimination algorithm needs to observe the loss for the set of arms $\NodeOut(a{t,c}$ for every time $t$
%
where $a_{t,c}$ is the arm with the largest out-degree in the set of all under-explored arms under context $c$.
%
However,the algorithm observes the loss for the same set of arms $\NodeOut(a_t)$ for each context $c$, as the algorithm only pulls a single arm $a_t$.
%
But for different contexts, the sets of all under-explored arms are different, thus the arms with the largest out-degree $a_{t,c}$ are different,%
and the sets of arms the algorithm needs to observe are different.
%

%如要解决这一点，我们须考虑 importance weighted estimator, importance-weighted estimator 把我们带向adversarial bandit。
Now we can see the importance of leveraging the stochastic nature of the contexts and applying the adversarial bandit techniques.
%
We need to guarantee that
\[\sum_{t} \ip{p_{t,c} - \pi_c, \ell_{t,c}}\le \widetilde{O}(\sqrt{\alpha T})\]
%
for each context $c$.
%
The key problem is that when $c_t \neq c$, the set of observable arms $\NodeOut(a_t) $ is independent of the probability distribution $p_{t,c}$ we choose.
%
A natural approach to address this problem is to use the importance-weighted estimator.
%
The key insight  is that, since the context $c_t$ is stochastic, we can use the importance-weighted estimator to construct  an unbiased estimate of the loss.
%
Applying the importance-weighted estimator naturally leads us to adversarial bandit algorithms.
%
This is the reason why we solve an open problem about stochastic bandits by applying adversarial bandit techniques.


%对adversarial bandit，先考虑distribution已知
We first consider the situation where the context distribution $\nu$ is known.
%
Let $\NodeIn(u) \triangleq \{ v \mid v \rightarrow u\}$ for each node $u$ in graph $G$.
We denote the probability of observing the loss of arm $a$ at round $t$ as
\[w_t(a) \triangleq \E_{c \sim \nu}[\sum_{a^{\prime} \in \NodeIn(a)} p_{t,c}(a^{\prime})].\]
%
Note that the probability $w_t(a)$ of observing the loss of arm $a$ is independent of contexts thanks to the cross-learning structure.
%
We then use $w_t(a)$ as the importance to construct an importance-weighted estimator $\widetilde{\ell}_{t,c}$ as
\[\widetilde{\ell}_{t,c}(a) = \frac{\ell_{t,c}(a)}{w_t(a)} \indi(a_t \rightarrow a).\]
We determine the probability distribution $p_{t,c}$ by the standard FTRL subroutine:
\[p_{t,c} = \argmin_{p \in \Delta([K])} \left\langle p, \sum_{s=1}^{t-1} \widetilde{\ell}_{s,c} \right\rangle - \frac{1}{\eta} F(p)\]
where $F(p) = \sum_{i=1}^K p_i \log(p_i)$ is the unnormalized negative entropy and $\eta$ is the learning rate.
%
We then prove that the algorithm achieves $\widetilde{O}(\sqrt{\alpha T})$ regret using standard adversarial bandit techniques.

%再考虑distribution未知
We next  consider the situation where the context distribution $\nu$ is unknown.   
%
In this scenario, the probability $w_t(a) = \E_{c \sim \nu}[\sum_{a^{\prime} \in \NodeIn(a)} p_{t,c}(a^{\prime})]$ is uncomputable as the distribution $\nu$ is unknown.
%
Thus, the vanilla importance-weighted estimator $\widetilde{\ell}_{t,c}$ is uncomputable.
%
So we need to construct other loss estimates.
%
A naive temptation is to replace the true probability $w_t(a)$ by a probability estimation $\frac{1}{t} \sum_{s=1}^t \sum_{a^{\prime} \in \NodeIn(a)} p_{t,c_s}(a^{\prime})$.
%
However, detailed analysis shows that the probability estimation has no concentration guarantee due to a complex dependence issue.
%
We employ techniques in \citet{Sch23} to solve this problem.
%
We divide the time horizon into different epochs in a way such that different epochs are only weakly dependent. 
%
We utilize the weak dependency across different epochs to estimate the expected importance $w_t(a)$ and form an empirical importance $\widehat{w}_t(a)$.
%
We use the empirical importance $\widehat{w}_t(a)$ as the importance to form an alternative importance-weighted loss estimator $\widehat{\ell}$ with concentration guarantees.
%
Finally, we feed the new loss estimates $\widehat{\ell}$ to the FTRL subroutine to get the probability distribution of playing each arm.


%我们顺便解决了adversarial的问题。
The final note is that, since we use the adversarial bandit techniques, all of our analysis can be applied to adversarial bandits directly.
%
Thus, we actually give an algorithm with $\widetilde{O}(\sqrt{\alpha T})$ regret for adversarial bandits.
%
The regret bound for stochastic bandits can be seen as a trivial corollary of the regret bound for adversarial bandits.

\subsection{Related Work}
The bandit with graphical feedback problem was proposed by \citet{MS11}.
%
Later, \citet{GraphAlon15} gave a full characterization of the minimax regret of the problem in terms of the time horizon $T$.  \citet{neu2015,LuoGraph23} extended the minimax regret to the high probability case.
%
\citet{lykouris20a,Han24} considered the UCB style algorithm for graphical bandit.
%
\citet{GraphAlon15,Understanding2021} also studied the non-strongly observable feedback graph case.

The cross-learning bandit problem was proposed by \citet{Bal19} to solve bidding-in-auctions problem. \citet{Bal19} inspired a line of work to study different settings of the bidding-in-auctions problem\citep{Han2020Adversarial,Aiauction,WangAuction,Han24}. In all these scenarios, the cross learning structure is an essential component of the analysis. \citet{Sch23} solved a particularly technically challenging case. \citet{Sch23} considered the case with adversarial losses and stochastic contexts and proved that the minimax regret in this case is $\widetilde{O}(\sqrt{KT})$. \citet{Huang2025} extended the result of \citet{Sch23} to the high probability case.

The cross-Learning contextual bandits with graphical feedback problem was proposed by\citet{Han24}. \citet{Han24} showed that for adversarial contexts it is impossible to achieve $\widetilde{O}(\sqrt{\alpha T})$ regret. \citet{Han24} also gave a $\widetilde{O}(\sqrt{\min(\alpha MT,KT)})$ regret bound for stochastic losses and stochastic contexts. Later, \citet{MAS24} further studied this problem for stochastic losses and adversarial contexts and gave a more precise characterization of the regret shape. 