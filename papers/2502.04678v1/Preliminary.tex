\section{Preliminaries}
We study a contextual $K$-armed bandit problem over $T$ rounds with graphical feedback, where contexts belong to the set $[M]$. 
%
We consider the oblivious adversarial bandits.
%
At the beginning of the problem, an oblivious adversary selects a sequence of losses $\ell_{t, c}(a) \in [0,1]$ for every round $t \in[T]$, every context $c \in[M]$, and every arm $a \in[K]$. 
%
Note that all of our results apply to stochastic bandits as well, since the oblivious adversarial bandits are strictly stronger than  stochastic bandits. 

We assume that there is a directed feedback graph $G$ over the set of arms $[K]$ with edge set $E$. We use the following graph-theoretic notations.  For each arm $a$, let $\NodeOut(a) = \{v \in [K]: a \rightarrow v\}$ be the set of out-neighbors of $a$ (including $a$ itself), and let $\NodeIn(a)=\{v \in [K]: v \rightarrow a\}$ be the set of in-neighbors of $a$. The independence number $\alpha(G)$ is defined as the cardinal number of the maximal independence set of $G$. For any vector $p$ of dimension $[K]$ and set $V \subset [K]$, we denote $p(V) \triangleq \sum_{v \in V} p(v)$. For example, the notation $p(\NodeIn(a))$ means $\sum_{a' \rightarrow a} p(a')$.

In each round $t$, we begin by sampling a context $c_t \sim \nu$ i.i.d. from an unknown distribution $\nu$ over $[M]$, and we reveal this context to the learner. 
%
Based on this context, the learner selects an arm $a_t \in[K]$ to play.
%
The adversary then reveals the function $\ell_{t, c}\left(a\right)$ for all $a \in \NodeOut(a_t)$, and the learner suffers loss $\ell_{t, c_t}\left(a_t\right)$. 
%
Notably, the learner observes the loss for every context $c \in[M]$ and every arm $a \in \NodeOut(a_t)$.
%


We aim to design learning algorithms that minimize regret. 
%
Fix a policy $\pi:[M] \rightarrow[K]$.
%
With a slight abuse of notation, we also denote $\pi_c=e_k \in \Delta([K])$ for each $c \in[M]$. 
%
The expected regret with respect to policy $\pi$ is
\[\Reg(\pi)=\E \left[ \sum_{t=1}^T \ell_{t, c_t}\left(a_t\right)-\ell_{t, c_t}\left(\pi_{c_t}\right)\right]\]
We aim to upper bound this quantity (for an arbitrary policy $\pi$).
