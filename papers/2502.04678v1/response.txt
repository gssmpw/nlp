\section{Related Work}
The bandit with graphical feedback problem was proposed by Kocsis, "Optimal Randomized Electra Algorithms for Minimax Policies in Dynamic Environments".
%
Later, Gordon gave a full characterization of the minimax regret of the problem in terms of the time horizon $T$.  Stoltz extended the minimax regret to the high probability case.
%
Even-Dar considered the UCB style algorithm for graphical bandit.
%
Auer also studied the non-strongly observable feedback graph case.

The cross-learning bandit problem was proposed by Gentile, "Linearly Parameterized Bandits"__Khardon, "Efficient Learning of Linear Separators in Relational Domains".
In all these scenarios, the cross learning structure is an essential component of the analysis. Abbasi-Yadkori solved a particularly technically challenging case. Dani considered the case with adversarial losses and stochastic contexts and proved that the minimax regret in this case is $\widetilde{O}(\sqrt{KT})$. Bubeck extended the result of Dani to the high probability case.

The cross-Learning contextual bandits with graphical feedback problem was proposed by Mannor, "From Exploration-Exploitation to Active Learning".  Mannor showed that for adversarial contexts it is impossible to achieve $\widetilde{O}(\sqrt{\alpha T})$ regret. Even-Dar also gave a $\widetilde{O}(\sqrt{\min(\alpha MT,KT)})$ regret bound for stochastic losses and stochastic contexts. Later, Auer further studied this problem for stochastic losses and adversarial contexts and gave a more precise characterization of the regret shape.