\section{Conclusion}
In this paper, we study the cross-learning contextual bandits with graphical feedback problem.
%
We propose an algorithm that achieves the minimax $\widetilde{O}(\sqrt{\alpha T})$ regret bound for strongly observable graph, adversarial losses, and stochastic contexts.
%

A natural direction for future research is to further remove the assumption that the graph $G$ includes self-loops. While the case where graph $G$ includes self-loops is already an open question \citep{Han24,MAS24}, the case without self-loops is much more difficult. Below we briefly discuss the main difficulty in this setting.

The graphical bandit problem without self-loops is often much more difficult than the case with self-loops. For example, the first high probability bound for the graphical bandit without self-loops ~\citep{LuoGraph23} was established eight years after \citet{neu2015} established the high probability bound for the case with self-loops.
%
The main problem is that \Cref{lem: graph_inverse} does not hold for the non-including self-loops case. As a result, we often need a  new approach to bound the regret.
%

In our analysis, we heavily rely on high-probability bounds and concentration inequalities in the proof of \Cref{thm:main}. 
%
To extend our result to the case without self-loops, we need to combine high probability bounds in \citet{LuoGraph23} with our arguments.
%
We attempted to do this but failed. 
%
The reason is that the high probability bounds in the case without self-loops~\citep{LuoGraph23} rely on a very delicate balance.
%
Our algorithm is much more complex than the algorithm in \citet{LuoGraph23} as we need to consider the contextual structure. 
%
Thus, our algorithm breaks that delicate balance and we were unable to establish those needed high probability bounds.


We leave the extension of our results  to the case without self-loops as an interesting open question. Another interesting research direction is to extend our results to graphs that are not strongly observable. 
