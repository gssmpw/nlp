
\section{Experiments \& Human Survey}\label{sec:exp}

\begin{figure*}[!th]
    \centering
    \begin{subfigure}[b]{0.33\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/uganda-exp.png}
        \vskip -4pt
        \caption{\texttt{Uganda} environment}
        \label{fig:uganda-exp}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/mimic-iii-exp.png}
        \vskip -4pt
        \caption{\texttt{MimicIII} environment}
        \label{fig:mimic-iii-exp}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/heatalerts-exp.png}
        \vskip -4pt
        \caption{\texttt{HeatAlerts} environment}
        \label{fig:mimic-iii-exp}
    \end{subfigure}
    \vskip -4pt
    \caption{Results from Q1. Main comparison of RBRL on three resource allocation problems. The plots show the mean and standard error across three seeds, using exponentially weighted moving averages with a half-life of 100.}
    \label{fig:results-reward}
\vskip -6pt
\end{figure*}




\begin{figure*}[!th]
    \centering
    \begin{subfigure}[t]{0.34\linewidth}
        \centering
        \includegraphics[width=1\columnwidth]{Figures/q2-ablations.png}
        \vskip -4pt
        \subcaption{Results from Q2: ablations}
        \label{fig:tbrl}
    \end{subfigure}
    \begin{subfigure}[t]{0.3\linewidth}
        \centering
\includegraphics[width=1\columnwidth]{Figures/finetuning-results.png}
    \vskip -4pt
\subcaption{Results from Q3: LLM finetuning}
 \label{fig:finetune}
    \end{subfigure}
    \begin{subfigure}[t]{0.34\linewidth}
        \centering
        \includegraphics[width=1\columnwidth]{Figures/explain-reward-opt.png}
                \vskip -4pt
\subcaption{Results from Q4: no rule rewards}
        \label{fig:no-rulerew}
    \end{subfigure}

    \vskip -4pt
    \caption{Additional experiments and ablations. (a) Comparison of \rbrl with thoughts-based RL (TBRL) and the baseline rule-based LLM without RL training; (b) comparison against LLM finetuning with PPO at the token level from the environment reward with CoT generation for the \texttt{Mimic environment}; (c) shows the effect of removing the rule reward in the \texttt{HeatAlerts} environments. For (a) and (c), we show distribution of rewards in the last 20\% training steps.}
    \label{fig:additional-restuls}
\end{figure*}

In this section, we evaluate \rbrl and empirically show that it can achieve a joint improvement in both reward and explainability over comparable baselines
. We briefly summarize these environments here, with additional details in Appendix \ref{appdx:env-details}.





\paragraph{Domains} We evaluate \rbrl in three environments pertaining to the following two distinct domains:

$\bullet$ \texttt{WearableDeviceAssignment}: We use two environments, \texttt{Uganda} and \texttt{MimicIII}, from the vital sign monitoring domain introduced by \citet{boehmer2024optimizing}, modeling the allocation of limited wireless devices among postpartum mothers as an restless multi-armed bandit problem (RMAB). Each mother (``arm") is represented by a state of vital signs (e.g., heart rate, respiratory rate, SPO2) and their variability. Action vector elements are binary: assigning a device to a mother or not. The aim is to prioritize high-risk patients since there are only a limited number of $B$ devices. Rewards penalize abnormal vital signs, with monitoring enabling health-improving interventions.
 \citet{boehmer2024optimizing} suggests to recast the RMAB problem as an MDP, deciding from which patient to remove a device when a new patient arrives.  A penalty is incurred when removing the device from an active patient if there are free devices.

$\bullet$ \texttt{HeatAlerts}: We use the \texttt{weather2alert} environment from \citet{considine2023optimizing}
, which formulates issuing heat alerts as a constrained MDP. The action is binary, subject to a constraint on the total number of alerts that can be issued. The state includes the heat index, the remaining budget, and a history of previous alerts. Actions $\ba_t$ are binary and represent whether to issue an alert. Rewards are reductions in hospitalizations from the alert. A penalty is incurred when selecting to issue an alert when on budget. 

\subsection{Environment Reward Optimization}

We discuss the main results and refer to Appendix \ref{appdx:setup} for the detailed experiment setup. Unless otherwise specified, we use \texttt{gpt-4o-mini} \cite{openai2024gpt4omini} as LLM due to its reasonable cost and high performance. All baselines were trained on 3 seeds for 2000 environment steps each. 

\emph{Q1. Did \rbrl optimize the reward function?} \quad \rbrl is compared to Chain-of-thought (CoT) \cite{wei2022chain} for language reasoning and Proximal Policy Optimization (PPO) \cite{schulman2017proximal} for numeric states. Figure \ref{fig:results-reward} indicates \rbrl outperforms CoT, showing RL-optimized rule selection improves decision-making. 
\rev{
    The results show that \rbrl exceeds PPO in all environments with equivalent environment steps, suggesting a higher performance when learning in an online setting. For completeness, we also compare against PPO trained at 100k steps, 50 times more. For the \texttt{HeatAlerts} environment, \rbrl exceeds such asymptotic performance, consistently with the findings of \citet{considine2023optimizing} noting that PPO and other standard RL algorithms struggle to handle the constraints and exploration in this domain. For the \texttt{Uganda} and \texttt{MimicIII} environments, we observe the trend of \rbrl getting closer to 100k PPO, but not reaching the performance. As highlighted in Section \ref{sec:related}, various works have remarked that the trade-off between interpretability and performance can be justified to increase system trust, robustness, and adaptability in high-stakes environments.
}













\emph{Q2. Did structured rules help optimization?} \quad We conduct two ablation studies on structured rules. First, we benchmark the use of structured rules without RL, called baseline \texttt{RulesLLMOnly}, which is shown in Equation (2)-(5). Next, we compare \rbrl with a variant optimizing unstructured thoughts, termed thoughts-based RL (\texttt{TBRL}). The implementation mimics \rbrl, utilizing a candidate pool $\cR$ with the CoT prompt. Results in Figure \ref{fig:tbrl} show that comparing \rbrl with \texttt{RulesLLMOnly} highlights RL training gains, suggesting rule generation alone does not explain \rbrl's performance. Additionally, significant improvements over \texttt{TBRL} suggest optimizing structured rules is more effective than optimizing free reasoning traces.



\emph{Q3. How does \rbrl compare to token-level LLM finetuning with RL?} \quad  We implement LLM finetuning from the environment reward to a Llama 3.1 8B model \cite{meta2024llama3.1}, termed \texttt{FinetunePPO}. We train a value head on the last hidden states and use KL divergence from the reference model as a regularization reward \cite{ziegler2019fine}. A simple CoT generation is used, followed by a call for action question, optimizing the full CoT and action trace. We train 3 seeds for 18 hours on an Nvidia A100 40G GPU, achieving 1200 steps per seed. For compatibility, we also train \rbrl on the same Llama 3.1 8B model. Figure \ref{fig:finetune} compares results, showing a positive but relatively flat trend for finetuning compared to \rbrl, suggesting \rbrl is better online. Additionally, \rbrl runs on a regular laptop, unlike \texttt{FinetunePPO} that needs specialized hardware, and training time for \texttt{FinetunePPO} is about 4x longer on equivalent steps. Due to computation constraints, we only show this experiment for the \texttt{MimicII} domain, which had the less noise in the previous experiments.


 




























\subsection{Human Survey and Explainability}
 

\emph{Q4. Did \rbrl increase the explainability of post-hoc explanations?}  \quad 
A survey with 40 participants was conducted to assess explanation quality, detailed in Appendix \ref{appdx:survey}. Each prompt included the task, state, and action space as originally given to the LLM, followed by actions and explanations from the CoT agent and the \rbrl agent, without disclosing agent types. Participants were asked to choose preference for explanation A, B, or none. Prompts were split between \textit{Wearable Device Assignment} and \textit{Heat Alerts} domains. Figure \ref{fig:survey} shows results, favoring \rbrl's explanations in both domains, with a detailed breakdown in \ref{appdx:survey}. An additional experiment with an LLM judge \cite{gu2024survey} using a large \texttt{gpt-4o} model \cite{openai2024gpt4o} showed strong agreement with humans, preferring \rbrl's explanations in all but one case.


\begin{figure}[tbp]
\centering
\includegraphics[width=0.85\columnwidth]{Figures/Human_survey.png}
\vskip-10pt
\caption{Results from the human survey.}
\label{fig:survey}
\vskip -8pt
\end{figure}


\emph{Q5. What was the effect of the rule reward?}  During training of \rbrl, rules received rewards based on two prompts. We examine an ablation where this reward was omitted. Figure \ref{fig:no-rulerew} illustrates results for the \texttt{HeatAlerts} environment, noted for high variance and a challenging reward function. We extended training to 5k environment steps to better understand these dynamics. When \rbrl is trained without the rule reward, the environment reward remains steady (with a slight increase), but explainability scores drop significantly. Refer to Section \ref{sec:rulerew} for the definition of the rule reward metrics. A decline in metric 1 indicates that rules are less predictive of the optimal actions. A decline in metric 2 suggests rules lack detailed applicability to the current problem state, indicating more generic rather than specialized rule selection. Metric 3 (not shown) was always 1 in all steps, indicating the limitations of directly evaluating post hoc explanations.  
Although judged by the LLM, these results are encouraging, as our previous experiment showed alignment between the LLM and human assessments.














