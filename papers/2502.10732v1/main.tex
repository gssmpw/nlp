\documentclass{article}

\usepackage{microtype, xspace}
\usepackage{tcolorbox}
\usepackage{graphicx}
\usepackage{multirow} 

\usepackage{booktabs}
\usepackage{graphicx, array} %
\usepackage{amssymb}%

\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}


\usepackage[accepted]{icml2025}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{comment}
\usepackage{enumerate, enumitem}
\usepackage{subcaption}

\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}


\def\cA{{\mathcal{A}}} \def\cB{{\mathcal{B}}} \def\cC{{\mathcal{C}}} \def\cD{{\mathcal{D}}}
\def\cE{{\mathcal{E}}} \def\cF{{\mathcal{F}}} \def\cG{{\mathcal{G}}} \def\cH{{\mathcal{H}}}
\def\cI{{\mathcal{I}}} \def\cJ{{\mathcal{J}}} \def\cK{{\mathcal{K}}} \def\cL{{\mathcal{L}}}
\def\cM{{\mathcal{M}}} \def\cN{{\mathcal{N}}} \def\cO{{\mathcal{O}}} \def\cP{{\mathcal{P}}}
\def\cQ{{\mathcal{Q}}} \def\cR{{\mathcal{R}}} \def\cS{{\mathcal{S}}} \def\cT{{\mathcal{T}}}
\def\cU{{\mathcal{U}}} \def\cV{{\mathcal{V}}} \def\cW{{\mathcal{W}}} \def\cX{{\mathcal{X}}}
\def\cY{{\mathcal{Y}}} \def\cZ{{\mathcal{Z}}}

\def\ba{{\mathbf{a}}} \def\bb{{\mathbf{b}}} \def\bc{{\mathbf{c}}} \def\bd{{\mathbf{d}}} \def\be{{\mathbf{e}}}
\def\bff{{\mathbf{f}}} \def\bg{{\mathbf{g}}} \def\bhh{{\mathbf{h}}} \def\bi{{\mathbf{i}}} \def\bj{{\mathbf{j}}}
\def\bk{{\mathbf{k}}} \def\bl{{\mathbf{l}}} \def\bm{{\mathbf{m}}} \def\bn{{\mathbf{n}}} \def\bo{{\mathbf{o}}}
\def\bp{{\mathbf{p}}} \def\bq{{\mathbf{q}}} \def\br{{\mathbf{r}}} \def\bs{{\mathbf{s}}} \def\bt{{\mathbf{t}}}
\def\bu{{\mathbf{u}}} \def\bv{{\mathbf{v}}} \def\bw{{\mathbf{w}}} \def\bx{{\mathbf{x}}} \def\by{{\mathbf{y}}}
\def\bz{{\mathbf{z}}} \def\bh{\mathbf{h}}

\def\bA{{\mathbf{A}}} \def\bB{{\mathbf{B}}} \def\bC{{\mathbf{C}}} \def\bD{{\mathbf{D}}} \def\bE{{\mathbf{E}}}
\def\bF{{\mathbf{F}}} \def\bG{{\mathbf{G}}} \def\bH{{\mathbf{H}}} \def\bI{{\mathbf{I}}} \def\bJ{{\mathbf{J}}}
\def\bK{{\mathbf{K}}} \def\bL{{\mathbf{L}}} \def\bM{{\mathbf{M}}} \def\bN{{\mathbf{N}}} \def\bO{{\mathbf{O}}}
\def\bP{{\mathbf{P}}} \def\bQ{{\mathbf{Q}}} \def\bR{{\mathbf{R}}} \def\bS{{\mathbf{S}}} \def\bT{{\mathbf{T}}}
\def\bU{{\mathbf{U}}} \def\bV{{\mathbf{V}}} \def\bW{{\mathbf{W}}} \def\bX{{\mathbf{X}}} \def\bY{{\mathbf{Y}}}
\def\bZ{{\mathbf{Z}}}
\def\lang{\texttt{lang}} 
\def\parse{\texttt{parse}}
\def\task{\texttt{task}\xspace}
\def\expl{\pmb{\ell}^{expl}}
\def\llm{\textrm{LLM}}

\def\arule{\ba^{\text{rule}}}
\def\aenv{\ba^{\text{env}}}
\def\aexpl{\pmb{\ell}^{\text{expl}}}
\def\athought{\ba^{\text{thought}}}
\def\renv{\br^{\text{env}}}
\def\rrule{\br^{\text{rule}}}
\def\Rrule{R^{\text{rule}}}
\def\rexpl{R^{\text{expl}}}

\newcommand{\rsearch}{\texttt{Rule\_Search}\xspace}
\newcommand{\rbrl}{\texttt{RBRL}\xspace}
\newcommand{\mt}[1]{{\color{brown}MT: #1}}
\newcommand{\rev}[1]{{#1}}

\usepackage[textsize=tiny]{todonotes}


\icmltitlerunning{RBRL: Joint Explanation and Decision Optimization for Resource Allocation with Language Agents}

\begin{document}

\twocolumn[

\icmltitle{
    Rule-Bottleneck Reinforcement Learning: Joint Explanation and Decision Optimization for Resource Allocation with Language Agents%
}



\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Mauricio Tec}{equal,seas,hsph}
\icmlauthor{Guojun Xiong}{equal,seas}
\icmlauthor{Haichuan Wang}{seas}
\icmlauthor{Francesca Dominici}{hsph}
\icmlauthor{Milind Tambe}{seas,deepmind}
\end{icmlauthorlist}

\icmlaffiliation{seas}{Department of Computer Science, Harvard John A. Paulson School of Engineering and Applied Sciences}
\icmlaffiliation{hsph}{Department of Biostatistics, Harvard T.H. Chan School of Public Health}
\icmlaffiliation{deepmind}{Google DeepMind}

\icmlcorrespondingauthor{Mauricio Tec}{mauriciogtec@g.harvard.edu}
\icmlcorrespondingauthor{Guojun Xiong}{gjxiong@g.harvard.edu}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]



\printAffiliationsAndNotice{\icmlEqualContribution} %



\begin{abstract}
Deep Reinforcement Learning (RL) is remarkably effective in addressing sequential resource allocation problems in domains such as healthcare, public policy, and resource management. However, deep RL policies often lack transparency and adaptability, challenging their deployment alongside human decision-makers. In contrast, Language Agents, powered by large language models (LLMs), provide human-understandable reasoning but may struggle with effective decision making. To bridge this gap, we propose Rule-Bottleneck Reinforcement Learning (\rbrl), a novel framework that jointly optimizes decision and explanations. At each step, \rbrl generates candidate rules with an LLM, selects among them using an attention-based RL policy, and determines the environment action with an explanation via chain-of-thought reasoning. The RL rule selection is optimized using the environment rewards and an explainability metric judged by the LLM. Evaluations in real-world scenarios highlight \rbrl's competitive performance with deep RL and efficiency gains over LLM fine-tuning. A survey further confirms the enhanced quality of its explanations.
\end{abstract}
 




\input{Intro}
\input{Related_work}
\input{System_model}
\input{Methodology}
\input{Experiments}
\input{Conclusion}

\rev{
\section*{Acknowledgments}  
This work is supported by the Harvard Data Science Initative, Wadhwani AI, and the National Institues of Health (R01ES34021,
R01ES037156.)
}


\bibliography{references}
\bibliographystyle{icml2025}


\newpage
\appendix
\onecolumn
\input{appendix}


\end{document}
