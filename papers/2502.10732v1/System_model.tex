\section{Preliminaries and Problem Formulation}


\textit{Notations.} A bold lowercase letter $\ba \in\mathbb{R}^d$ represents a vector with dimension $d$, while a regular lowercase letter $a$ denotes a scalar. An uppercase letter $A$ denotes a mapping function, and a calligraphic letter $\cA$ denotes a set; $[h]$ denotes the set of $\{1, \ldots, h\}$; $\Delta(\cA)$ denotes the space of probability distributions supported in $\cA$.

\subsection{Resource-Constrained Allocation}
Resource-constrained allocation tasks are usually formulated as a special Constrained Markov Decision Process (CMDP), which is defined by the tuple $ \langle \cS, \cA, P, R, C, h,\gamma \rangle $, where $\cS$ denotes a state space and $\cA$ denotes a finite action space. The transition probability function, specifying the probability of transitioning to state $ \bs'\in\mathbb{R}^{d_1} $ after taking action $\ba\in\mathbb{R}^{d_2}$ in state $\bs$, is $ P(\bs' | \bs, \ba):\cS \times \cA \times \cS \to \Delta(\cS) $,  $ R(\bs, \ba):\cS \times \cA \to \mathbb{R}$ represents the reward function, defining the immediate reward received after taking action $\ba$ in state $\bs$, and we let $C(\bs, \ba): \cS \times \cA \to \mathbb{R}^{d_3}$ be the immediate cost incurred after taking action $\ba$ in state $\bs$. Often, each dimension $i\in[d_2]$ in $\ba$ is either $0$ or $1$ in resource-constrained allocation tasks. In addition, $h$ is the time horizon and $ \gamma \in [0, 1] $ denotes the discount factor, which determines the present value of future rewards.


The goal of resource-constrained allocation tasks is to find a policy $\pi\colon \cS \to \Delta(\cA)$ that maximizes the expected cumulative discounted reward while satisfying the cost constraints:
\begin{align}\label{eq:RCA_objective}
    \pi^* &= \arg\max_\pi \mathbb{E}_\pi J(\pi):=\left[ \sum_{t=1}^h \gamma^{t-1} R(\bs_t, \ba_t) \right]\nonumber\\
    & s.t. ~~~\forall t\in [h]\colon C(\bs_t, \ba_t) \leq B(\bs_t),
\end{align}
where $B\colon \cS \to \mathbb{R}^{d_3}$ is the budget function.







 





\begin{figure}[tbp]
    \centering
    \begin{subfigure}[b]{\linewidth}
\includegraphics[width=0.98\linewidth]{Figures/Task_Prompt_Final.pdf}
    \caption{Examples of initial task prompt, which contains the task description and available actions. }
    \label{fig:input_prompt}   
    \end{subfigure}
     \begin{subfigure}[b]{\linewidth}\includegraphics[width=0.98\linewidth]{Figures/gen_rules_prompt_final.pdf}
    \caption{Examples of generated rules for the Heat Alert Issuance task.}
    \label{fig:rule_prompt}   
    \end{subfigure}
    \vskip -4pt
    \caption{Examples of task prompts and generated rules.}
    \vspace{-0.6cm}
\end{figure}

 



\subsection{Language-augmented RL with Rules}


We outline the language-augmented RL framework, where the action space includes internal language actions $\tilde{\cA}=\cA \cup \cL$ \cite{yao2023react, carta2023grounding}. Language agents typically have two types of internal language actions: First, \emph{thoughts} $\athought \in \cL$, are reasoning traces from the current problem state used to inform environment action selection $\aenv \in \cA$ \cite{yao2023react}. Second, \emph{post-hoc explanations} $\aexpl$, are generated from actions and thoughts to enhance human trust and interpretability \cite{zhang2023understanding}, a focus of this work.

\textbf{Rules}\quad
Thoughts are useful to highly relevant aspects of a problem. However, they often lack detailed information to identify the next optimal action. In this work, we will consider ``rules" $\arule\in \cL$, which are structured language statements derived from thoughts that generally take the form ``[do/prioritize] [if/when]". More formally, each rule $\arule$ consists of a triple $(\texttt{\footnotesize background}, \texttt{\footnotesize\, rule\_statement}, \texttt{\footnotesize\, state\_relevance})$. Figure \ref{fig:rule_prompt} shows examples of generated rules from one of the domains used in the experiments.



\textbf{Task and Constraints Description}\quad
Language agents require: (1) a language description of the environment and the agent's goal, denoted \task; (2) a function describing the state of the environment in natural language, denoted $\lang:\cS\to \cL$.  At each state $\bs_t$, these descriptors are used to construct a natural language prompt $\bp_t=f(\task, \lang(\bs_t), C, B)$, where $C$ and $B$ are the costs and budget constraints defined in eq. \eqref{eq:RCA_objective}. For instance, the prompt can restrict certain actions, or the total number of allocations in a given period of time.
Figure \ref{fig:input_prompt} exemplifies rules generated for the environments in our experiments. 





\textbf{Baseline Rule-based Language Policy}\quad
Our objective is to jointly optimize the reward and explainability of the environment. Hence, we will take as baseline an LLM-driven policy $\pi_{\llm}$ for online interaction with the environment:
\begin{align}\label{eq:lang-policy}
\athought_t & \sim \pi_{\llm}(\athought_t \mid \bp_t), \allowdisplaybreaks\\\label{eq:lang-policy-rule}
\arule_t  & \sim \pi_{\llm}(\arule_t  \mid \athought_t, \bp_t), \\\label{eq:lang-policy-env}
\aenv_t   & \sim \pi_{\llm}(\aenv_t  \mid \arule_t, \athought_t, \bp_t),\allowdisplaybreaks \\ \label{eq:lang-policy-expl}
\aexpl_t  & \sim \pi_{\llm}(\aexpl_t \mid \aenv, \arule_t , \bp_t).
\end{align}



The rule acts as a ``bottleneck" to the action and explanation.

\begin{remark} This baseline builds on the baseline chain-of-though language agent, augmented by the generation of a single rule $\arule_t$. In the next section, we will introduce \rbrl, which replaces the generation of a single $\arule_t$ with an RL-based learnable selection policy $\pi_\theta$ choosing among a set of dynamically generated candidate rules.
\end{remark}


\textbf{Explainability}\quad
Explainability in AI encompasses multiple dimensions (see \citet{hoffman2018metrics} for an overview). Here, we evaluate the explainability of 
$\arule$ and $\aexpl$ through two key aspects:

\begin{enumerate}[leftmargin=2em,topsep=0pt,itemsep=0pt] \item \emph{Completeness of 
$\arule$}: Can the optimal action be predicted from the rule? Does it contain sufficient detail about its applicability to the current state?
\item \emph{Trust and understanding with 
$\aexpl$}: Do humans find the explanation useful to understand the system behavior? Does it foster trust in the system?
\end{enumerate}

Analyzing both $\arule$ and $\aexpl$ is critical since post hoc LLM explanations alone may be prone to ``satisfying" but misleading rationalizations \citep{zhang2023understanding}. We hypothesize that complete rules aid to generating better post-hoc explanations by outlining a more structured reasoning process.

















\subsection{Problem Statement}



Our primary challenge is to enable LLMs to jointly optimize a language policy that both solves the underlying optimization problem and enhances the quality of the explanations---a task that has received little attention in the literature.  We aim to increase the quality of $\aexpl$ while also optimizing decision-making. We aim to achieve this by selecting rules that encourage both good quality explanations and high reward.  In Section \ref{sec:rbrl}, we will describe our method for constructing a surrogate explainabilty ``rule reward" \(\Rrule_\llm(\arule)\) using an LLM as judge  \citep{shen2024explainable, bhattacharjee2024towards, gu2024survey}. We denote the joint environment/rule reward as $ \tilde{r}_t = R(\bs_t, \aenv_t) + \Rrule_\llm(\arule_t)$.
Then, we propose the following augmented optimization objective:
\begin{align}\label{eq:RCA_objective_new}
    \max_\pi \mathbb{E}_\pi  \tilde{J}(\pi)\!:=\!\left[ \sum_{t=1}^h \!\gamma^{t-1}\! \tilde{r}_t \right] s.t. \text{ constraint~in} ~\eqref{eq:RCA_objective}.
\end{align}

We emphasize that LLMs cannot fully replace the ultimate human assessment, but they they provide a scalable alternative during the optimization process. 





