\section{Related Work}
\label{sec:related}

Our work intersects with three distinct areas within the RL literature. We discuss related work in each of these domains.


\textbf{RL for Resource Allocation}\quad
RL has been widely studied for constrained resource allocation across domains. In maternal health, \citet{boehmer2024optimizing} apply RL to a restless multiarmed bandit (RMAB) problem \cite{whittle1988restless} to compute patient-specific intervention probabilities. Also in an RMAB setting, \citet{xiong2022index} propose a model-based RL approach that prioritizes users via an index and allocates resources under budget constraints. In public health, \citet{considine2023optimizing} propose RL to optimize  extreme heat warnings under a budget on the number of possible alerts. Other works include multi-agent RL for robotic warehouse allocation \cite{Shen2023} and exogenous MDPs for cloud resource management \cite{sinclair2023hindsight}. While these methods optimize rewards effectively, they often lack interpretability---critical for deployment in sensitive domains requiring trust, transparency, and accountability.


\textbf{RL and Language Agents}\quad 
The language agents \citep{sumers2024cognitive} paradigm developed somewhat independently of RL, with works like ReAct prompting \cite{yao2023react} extending chain-of-thought (CoT) \cite{wei2022chain} to action settings. These works have focused on tasks such as open-ended web navigation \citep{putta2024agent}, social simulations \citep{park2023generative}, and virtual assistants \citep{vezhnevets2023generative}. Meanwhile, language interfaces have also be been proposed within the RL literature, including leveraging external and commonsense knowledge \cite{feng2024natural, waytowich2024atari}, pre-training goal-based policies \cite{du2023guiding}, enhancing generalization in embodied agents \cite{szot2023large}, and aiding human-AI coordination \cite{srivastava2024policy, hu2023language}. Related works include GLAM \citep{carta2023grounding}, TWOSOME \cite{tantrue}, BAD \cite{wen2024reinforcing}, and TextGym \cite{xi2024agentgym}, which use LLM finetuning techniques in RL environments with a reward function. Relevant to our work is also \citet{wang2023describe}, which, inspired by open-ended settings like Minecraft, employs RL to optimize the goals of an LLM planner based on feasibility.



\textbf{Explainable RL (XRL)}\quad
Early XRL relied on methods like decision trees and concept-based explanations \citep{Das2023State2Explanation}, but these struggled with scalability in dynamic environments \citep{poeta2023concept}. Recent advances introduced large language models (LLMs) for post-hoc explanations, such as explaining decision paths from policy trees \citep{zhang2023understanding} or adding language descriptions to RL policies \citep{colas2022language}. However, these approaches focus on interpreting pre-existing policies rather than enabling LLMs to generate inherently explainable decisions, with challenges in aligning explanations to human reasoning \citep{singh2024rethinking}. By contrast, inherently (also known as intrinsically) interpretable policies  are those that have internal representation that allow explanations 
 \cite{peng2022inherently,milani2024explainable}. Our work sits this literature by using LLM reasoning traces as the basis for environment action selection.
 
 \rev{
    With various works acknowledging the trade-off between interpretability and performance, prioritizing interpretability appears to be crucial in practice for many critical applications \cite{rudin2019stop}: an approach that we subscribe to in this work. For example, in the clinical AI domain, physicians require transparency to validate recommendations and uphold ethical accountability, as mandated by regulatory frameworks (e.g., \citet{eu_ai_act_2024, ca_ai_act_2024}). High-performing black-box systems often face rejection in clinical workflows due to distrust \cite{shevtsova2024trust, dubois2019deep}.  By contrast, interpretable models allow clinicians to audit biases and adapt logic to local contexts, whereas opaque policies risk failures under real-world distribution shifts \cite{rudin2019stop,doshi2017towards}. Transparent reasoning facilitates iterative, clinician-driven refinement, ensuring collaborative decision aid rather than an inflexible oracle \cite{shevtsova2024trust, dubois2019deep}. Empirical surveys show clinicians favor models that enable shared decision-making, error accountability, and ethical oversight despite modest performance penalties \cite{shevtsova2024trust}â€”a critical stance in high-stakes healthcare environments where trust and adaptability outweigh narrow efficiency gains.
}