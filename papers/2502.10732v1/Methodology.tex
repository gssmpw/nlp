
\section{Rule-based Reinforcement Learning (\rbrl)}
\label{sec:rbrl}


\rbrl  leverages the strengths of LLMs and RL to achieve both interpretability and robust sequential decision-making.




\textbf{Algorithm Overview}\quad
The \rbrl framework in ALgorithm \ref{alg:rbrl} involves four steps: (1) \textsc{Rule Set Generation} (line 3), the LLM processes the state-task $\bp_t$ to create candidate rules $\cR_t$ for action selection; (2) \textsc{Rule Selection} (line 4), an attention-based RL policy $\pi_{\theta}$ selects the best rule $\arule_t\in\cR$; (3) \textsc{Decision, Rule Reward and Explanation} (lines 5-8), the LLM generate an environment action $\aenv_t$ and based on the chosen rule $\arule_t$ and gives a human-readable explanation $\pmb{\ell}_t^{expl}$; (4) \textsc{Reinforcement Learning} (lines 10-12), collected samples update the policy $\pi_\theta$ and value networks with the SAC algorithm \cite{haarnoja2018soft} and the combined environment and rule reward $\tilde{r}_t$. Algorithm \ref{alg:rbrl} details the entire process. Further sections elaborate on these steps.

\begin{algorithm}[h]
\caption{\rbrl}
\begin{algorithmic}[1]
\REQUIRE 
Rule-selection policy $\pi_\theta$; Value network $\{Q^j_\phi\}_{j\in\{1,2\}}$; and Replay buffer $\cB$.

\STATE \textbf{Initialization:} Initial state $\bs_0$ and task-state prompt $\bp_0$.
\FOR{$t=0, \ldots, \texttt{max\_iters} - 1$}
\STATE Generate rule candidates $\cR_t$ using CoT from $\bp_t$ and $\athought_t$. {// Section \ref{sec:rulegen}}
\STATE Select rule $\arule_t$ using the RL policy $\pi_\theta$ from $\cR_t$ and $\bs_t$. {// Section \ref{sec:ruleselection}}
\STATE Generate the environment action $\aenv_t$ with the LLM from $\arule_t$, $\bp_t$, and previous thoughts.
\STATE Apply the action in the environment and obtain new state $\bs_{t+1}$ and environment reward $\renv_t$.
\STATE Generate post-hoc explanation with the LLM from $\aenv_t$, $\rrule_t$, $\bp_t$, and previous thoughts.
\STATE Generate rule reward $\rrule$ with the LLM as judge. {// Section \ref{sec:rulerew}}
\STATE Update the prompt $\bp_{t+1}$ from $\bs_{t+1}$, and the constraints $C$ and budget $B$.
\STATE Append transition to the replay buffer $\cB \leftarrow \cB \cup\{(\tilde{\bs}_t, \arule_t, \tilde{r}_t, \tilde{\bs}_{t+1})\}$ where $\tilde{\bs}_t=(\bs_t, \cR_t)$ and $\tilde{r}_t=r^{\text{rule}}_t + r^{\text{env}}_t$.

\IF {$t\mod k=0$}
\STATE Sample from the replay buffer and use Soft-Actor Critic RL to update the policy network $\pi_\theta(\arule_t | \tilde{\bs}_t)$ and value networks $\{Q^j_\phi(\tilde{\bs}_t, \arule_t)\}_{j\in\{1,2\}}$. {// Section \ref{sec:sac-rl}}
\ENDIF
\ENDFOR
\label{alg:rbrl}
\end{algorithmic}
\end{algorithm}






\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{Figures/icml25_RuleSelection.pdf}
    \caption{Overview of the \textsc{Rule Selection} step. The current state is encoded as a key vector, while candidate rules are encoded as Queries using a text embedding API (e.b., BERT sentence embedding). An attention-based policy network $\pi_\theta$ trained with SAC computes a probability distribution over the candidate rules, enabling the selection of the most suitable rule for decision-making and explanation.}
    \label{fig:rule_selection}
\end{figure*}

\subsection{Rule Set Generation}
\label{sec:rulegen}

The rule generation process seeks to to create interpretable and actionable guidelines for decision-making.
Under this framework, a set of candidate rules $\cR_t$ is generated according to $\cR_t\sim \pi_{\llm}(\cR_t|\bp_t, \athought_t)$. To enhance interpretability, each rule is accompanied by a rationale explaining the reasoning behind the decision.  The LLM is instructed to generate rules as a JSON format, which is common for integration of LLMs with downstream applications \cite{together2024function}. Examples of generated rules are given in Figure \ref{fig:rule_prompt}. See Figure \ref{fig:combined_prompt} in the Appendix for the prompt templates used to generate the rules.


 A higher number of rules is always preferable, however, it also increases the generating costs and slows down each iteration of the method. For our experiments, we found that $|\cR_t|=5$ provided achieved a reasonable trade-off.







\subsection{Rule Selection}\label{sec:ruleselection}

In this step, rules are converted from text to vector form, and a trainable attention-based policy network $\pi_{\theta}$ provides the probability distribution for sampling a rule. Figure \ref{fig:rule_selection} illustrates the process, with a detailed procedure in Algorithm \ref{alg:rule_search} of the Appendix. Below are the major components of the architecture of $\pi_\theta$. We propose to base the architecture on cross-attention layers \cite{bahdanau2014neural, vaswani2017attention}, with the state acting as the keys and values, and the rules as the queries. This allows to learn from the embedding representations of rules, and efficiently handle dynamically changing number of rules if needed.


\textit{State Representation.} The numeric state is projected by a linear layer: $\mathbf{k}_t =\texttt{Linear}(\bs_t) \in \mathbb{R}^{d_h}$, with $d_h$ being to denote the architecture hidden dimension. 

\textit{Rule Candidate Embedding.}
Each rule in the {list of rule candidates} $\mathcal{R}_t = \{ \pmb{\rho}^t_1, \pmb{\rho}^t_2, \dots, \pmb{\rho}^t_q \}$ is embedded into a numeric representation using a {Sentence Embedding} language model (e.g., SentenceBERT \cite{reimers2019sentencebert}) and further processed by a projection layer similar to the state representation. This results in a \textit{query} matrix
$\mathbf{Q}_t \in \mathbb{R}^{q \times d_h}$.

\textit{Attention-based Policy Network.}  The vector $ \mathbf{k}_t$, serving as keys, engages with the rule embeddings $\mathbf{Q}_t$, acting as queries, via a cross-attention mechanism to derive a hidden state representation $\bh_t^{(1)}=\texttt{Attention}(\mathbf{Q}_t, \bk_t^\top, \bk_t^\top)\in\mathbb{R}^{q \times d_h}$, computed as
$$
\begin{aligned}
\texttt{Attention}(\mathbf{Q}_t, \bk_t^\top, \bk_t^\top) = \text{softmax} \left(\frac{\mathbf{Q}_t \bk_t}{\sqrt{d_h}} \right) \bk_t^\top,
\end{aligned}
$$
which facilitates the rule candidate vector embeddings in attending to the environment state. Subsequently, we sequentially apply self-attention layers to the hidden representation $\bh^{(k+1)}=\texttt{Attention}(\bh_t^{(k)}, \bh_t^{(k)}, \bh_t^{(k)})$, enabling the rule embeddings to attend to one another to rank an optimal candidate. Ultimately, following $K-1$ self-attention layers, a final linear layer converts the rule representations into logit vectors $\pmb{\alpha}^t_{\theta}=\texttt{Linear}(\bh_t^{(k)})\in\mathbb{R}^q$ used for the computation of the probability of selecting each rule.

For the implementation, the attention layer is realized using the multi-headed attention module from \citet{vaswani2017attention}. We incorporate a dropout layer, fixed at 0.05 for the experiments, along with SiLU activation and layer normalization, which are excluded from the notation for brevity.

\textit{Rule Selection.}
The policy distribution over the rules:
$$
\pi_{\theta, i}(\mathbf{Q}_t,\mathbf{k}_t) = \frac{\exp(\alpha^t_{\theta, i}(\mathbf{Q}_t,\mathbf{k}_t))}{\sum_{j=1}^q \exp(\alpha^t_{\theta, j}(\mathbf{Q}_t,\mathbf{k}_t))}, \quad i=1,\ldots, q.
$$
A rule is selected sampled from the distribution:
$\arule_t \sim \texttt{Categorical}(\cR; (\pi_{\theta,i}(\mathbf{Q}_t, \mathbf{k}_t))_{i=1}^q)$.


\subsection{Decision, Rule Reward, and Explanation} \label{sec:rulerew}
Upon selection of rule $\arule_t$, the LLM determines the action to be applied within the environment $\aenv_t \sim \pi_\llm(\aenv_t | \arule_t, \athought_t, \bp_t)$, ensuring concordance with the chosen strategy. Subsequently, the LLM formulates a post-hoc explanation $\aexpl_t \sim \pi_\llm(\aexpl_t | \aenv, \arule, \athought, \bp_t)$ contingent upon the rule. Figure \ref{fig:combined_prompt} in the Appendix illustrates the prompt template employed to generate both the action and explanation. 

This procedure concurrently produces the rule reward $\Rrule_\llm(r^{\text{rule}}_t)$, used for reinforcement learning (RL) in the next step. This rewards is derived from using the LLM as a judge to answer the following three questions:

$\texttt{ER}_1$. Without providing $\aenv_t$, is $\arule_t$ sufficient to predict/infer the optimal action?

$\texttt{ER}_2$. Does $\arule_t$ contain enough details about the applicability of the rule to the current state?

$\texttt{ER}_3$. Given $\aenv_t$, is $\arule_t$ compatible with this selection?

Each question scores as $0$ if negative or $1$ if positive. The rule reward is calculated as $r_t^\text{rule}=\Rrule_\llm(\arule_t)=(1/3)\sum_i \texttt{ER}_i$. Refer to Figure \ref{fig:combined_prompt} in the Appendix for the full prompt.
 


\subsection{RL with SAC}\label{sec:sac-rl}

\emph{Augmented state space}\quad Traditional RL frameworks fail here due to intermediate steps: generating the rule set \(\cR_t\), mapping rules \(\arule_t\) to actions \(\aenv_t\) in an LLM-driven environment. \rbrl addresses this issue by creating an augmented state \(\tilde{\bs}_t := (\bs_t, \cR_t)\) with transition dynamics \(P(\tilde{\bs}_{t+1} | \tilde{\bs}_t, \arule_t)\), integrating rules into the state space for reasoning over both the environment's dynamics and decision rules \(\arule_t\). This proposition explains the transition computation.
\begin{theorem}
The state transition of the \rbrl MDP  can be calculated as 
\begin{align}
&P(\tilde{\bs}_{t+1}|\tilde{\bs}_t, \arule_t)=P( \cR_{t+1}|\bs_{t+1})\times\nonumber\allowdisplaybreaks\\ &\qquad\int_\ba P(\bs_{t+1}|\aenv,\bs_t)
    \cdot P(\aenv| \arule_t, \bs_t)d\aenv,
\end{align}
where $P( \cR_{t+1}|\bs_{t+1})= \pi_\llm(\cR_{t+1} | \bp_t, \pmb{\tau}_t)$ is the probability of the LLM generating rule set $\cR_{t+1}$ provided the state $\bs_{t+1}$, $P(\bs_{t+1}|\aenv,\bs_t)$ is the original environment dynamics, and $P(\aenv| \arule_t, \bs_t)=\pi_\llm(\aenv | \bp_t, \arule_t)$ is the probability of the LLM selecting the environment action $\aenv$.
\label{prop:transition}
\end{theorem}


\emph{SAC step}\quad The attention-based policy network in Section \ref{sec:ruleselection} is optimized using the SAC algorithm, which balances reward maximization with exploration by incorporating an entropy term in the objective function. The SAC update process is outlined as follows.
First, we define an auxiliary target value:
\begin{align}
    y&=\Big( \tilde{r}_t + \gamma \mathbb{E}_{\arule_{t+1} \sim \pi_\theta} \Big[ \min_{j=1,2} \bar{Q}_{\bar{\phi}_j}(\tilde{\bs}_{t+1}, \arule_{t+1})\nonumber\\
    &\qquad\qquad\qquad- \beta \log \pi_\theta(\arule_{t+1}|\bQ_{t+1}, \bk_{t+1}) \Big] \Big),
\end{align}
The Q-networks are updated by minimizing the loss function: 
\begin{align}
    L_Q(\phi_i) &= \mathbb{E}_{\mathcal{D}} \Big[ \Big( Q_{\phi_i}(\tilde{\bs}_t, \arule_t) -y \Big)^2 \Big], \forall i=1,2.
\end{align}
The policy network is updated by minimizing the KL divergence between the policy and the Boltzmann distribution induced by \( Q_\phi \), which is expressed as
    \begin{align}
    \hspace{-0.2cm}
        L_\pi(\theta)\!\!=\!\! \mathbb{E}_{\cD} \!\Big[\! \beta \!\log \!\pi_\theta(\arule_t | \bQ_t, \!\newblock{}_t)\!\! -\! \!\min_{i=1,2}\!Q_{\phi_i}\!(\tilde{\bs}_t, \arule_t) \!\Big],
    \end{align}
where $\beta$ is a temperature parameter.  The full SAC update procedure is detailed in Algorithm \ref{algo:SAC} in Appendix \ref{Sec:algo_appendix}.
         

 