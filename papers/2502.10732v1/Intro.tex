\section{Introduction}


Sequential resource allocation is a fundamental problem in many domains, including healthcare, finance, and public policy \cite{considine2023optimizing,boehmer2024optimizing, yu2024fincon}. This task involves allocating limited resources over time while accounting for dynamic changes and competing demands. Deep reinforcement learning (RL) is an effective method to optimize decision-making for such challenges, offering efficient and scalable policies~\cite{yu2021reinforcement,talaat2022effective, xiong2023reinforcement,zhao2024towards}. However, deep RL policies generally provide action recommendations without human-readable reasoning and explanations. Such lack of interpretability poses a major challenge in critical domains where decisions must be transparent, justifiable, and in line with human decision-makers to ensure trust and compliance with ethical and regulatory standards.



For example, doctors may need to decide whether to prioritize intervention for Patient A or Patient B based on their current vital signs~\cite{boehmer2024optimizing}. An RL algorithm might suggest: \textit{ ``Intervene with Patient A "} with the implicit goal of maximizing the value function. However, the underlying reasoning may not be clear to the doctors, leaving them uncertain about the factors influencing the decision \cite{milani2024explainable}. For doctors, a more effective suggestion could be risk-based with specific information, e.g., \textit{``Patient A's vital signs are likely to deteriorate leading to higher potential risk compared to Patient B, so intervention with Patient A is prioritized"} \cite{gebrael2023enhancing, boatin2021wireless}.




\begin{figure*}[tbp]
    \centering
    \includegraphics[width=0.99\linewidth]{Figures/icml25_ProposedFramework.pdf}
    \caption{Overview of the \rbrl framework for joint sequential decision-making and explanation generation at time instance $t$. Starting with current state $\bs_t$,  a state-to-language descriptor generates \lang($\bs_t$), which is used to create the input prompt 
$\bp_t$. The LLM processes 
$\bp_t$
  to produce a thought 
$\pmb{\tau}_t$  and a set of candidate rules 
$\cR_t$ . An attention-based policy network selects a rule 
$\arule_t$ , which is used to derive an executable action $\aenv_t$ satisfying the budget constraint $B(\bs_t)$ for the environment 
  and a human-readable explanation $\pmb{\ell}_t^{expl}$, while also providing a rule reward $r_t^{\text{rule}}$ 
 . The environment transitions to the next state 
$\bs_{t+1}$ , returning an environment reward $r_t^{\text{env}}$ 
 . This process is repeated iteratively at subsequent time steps. 
}
    \label{fig:Proposed_framework}
\end{figure*}


Language agents \cite{sumers2024cognitive} leverage large language models (LLMs) for multi-step decision-making using reasoning techniques like chain of thought (CoT) \cite{wei2022chain} and ReAct \cite{yao2023react}. They enable natural language goal specification \cite{du2023guiding} and enhance human understanding \cite{hu2023language, srivastava2024policy}. However, LLMs struggle with complex sequential decision-making, such as resource allocation \cite{furuta2024exposing}, making RL a crucial tool for refining them into effective policy networks \cite{carta2023grounding, tantrue, wen2024reinforcing, zhai2024fine}. Yet, fine-tuning LLMs for policy learning is highly challenging due to the substantial computational costs and the complexity of token-level optimization \cite{rashid2024critical}, which remains an open challenge, particularly in sequential resource allocation.

Consequently, aiming to combine the strengths of both deep RL and language agents, we pose the following question:


\vspace{-0.1in}
\begin{tcolorbox}[colback=white!5!white,colframe=white!75!white]
\textit{%
Can we design a language agent framework that can simultaneously perform sequential resource allocation and provide human-readable explanations? }
\end{tcolorbox}
\vspace{-0.15in}






Motivated by existing work that employs predefined rules or concepts to explain RL policies \cite{Das2023State2Explanation} or guide RL exploration \cite{likmeta2020combining}, we explore the potential of using rules to prioritize individuals in resource allocation problems. In the context of language agents, rules are defined as ``structured statements" that capture prioritization among choices in a given state, aligning with the agent's goals \cite{srivastava2024policy}. 
Rules offer a flexible framework for encoding high-level decision criteria and priority logic, similar as the celebrated index policy for prioritizing arms in resource allocation problems \cite{whittle1988restless}, making them ideal for guiding resource allocation strategies while explaining the rationale behind decisions.%



Building on this, we propose a novel framework called Rule-Bottleneck Reinforcement Learning (\rbrl), which integrates the strengths of LLMs and RL to bridge the gap between decision-making and interoperability, by optimizing LLM-generated rules with RL. 
\rbrl provides a framework (as shown in Figure \ref{fig:Proposed_framework}) that simultaneously makes sequential resource allocation decisions and provides human-readable explanations. \rbrl leverages LLMs to generate candidate rules and employs RL to optimize policy selection, enabling the creation of effective decision policies while simultaneously providing human-understandable explanations. 

Our contributions are summarized as follows. \textit{First}, to avoid the computational cost and complexity of directly fine-tuning language agents, we leverage LLMs to generate a diverse set of rules, where each rule serves as a prioritization strategy for individuals in resource allocation. This approach enhances flexibility and interpretability in decision-making.
\textit{Second}, we extend the conventional state-action space by integrating the thoughts and rules generated by LLMs, creating a novel framework that enables reinforcement learning to operate on a richer, more interpretable decision structure.
\textit{Third}, we introduce an attention-based training framework that maps states to queries and rules to keys. The rule selection process is optimized by a policy network trained using the Soft Actor-Critic (SAC) algorithm \cite{haarnoja2018soft}, ensuring robust and efficient decision-making. In particular, the LLM also acts as a feedback mechanism, providing guidance during RL exploration to improve policy optimization and promote more effective learning. 
 



We evaluate our method in three environments from two real-world domains: \texttt{HeatAlerts}, where resources are allocated to mitigate extreme heat events; and \texttt{WearableDeviceAssignment}, for distributing monitoring devices to patients. 
Using cost-effective LLMs such as gpt-4o-mini \cite{openai2024gpt4omini} and Llama 3.1 8B \cite{meta2024llama3.1}, we first assess decision performance by comparing \rbrl with pure RL methods and language agent baselines. We then evaluate explanation quality through a human survey conducted under IRB approval. The results demonstrate \rbrl's effectiveness in both decision quality and interpretability.













