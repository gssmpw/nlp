 




 





 
























\section{Algorithmic and Mathematical Details}


\subsection{Algorithms}\label{Sec:algo_appendix}

In this subsection, we present the detailed pseudocodes for \texttt{Rule\_Search} in Algorithm \ref{alg:rule_search} and the SAC for attention-based policy network in Algorithm \ref{algo:SAC}.

 



\begin{algorithm}[H]
\caption{\texttt{Rule\_Search}: Rule Selection via Attention-Based Policies}
\begin{algorithmic}[1]
\REQUIRE Numeric state representation $\bs_t \in \mathbb{R}^{d_s}$  and rule set $\cR_t=\{\pmb{\rho}^t_{i}\}_{i=1}^q$. Hidden dimension $d_h$.

\STATE Embed each rule \( \pmb{\rho}^t_{i} \in \cR_t \) into a numeric vector using sentence embeddings $\bq^t_i=\texttt{embed}(\pmb{\rho}^t_i)\in\mathbb{R}^{d_h}$ (e.g., Sentence-BERT) and stack to form a query matrix \( \mathbf{Q}_t \in \mathbb{R}^{d_h \times q} \). {// \textit{Rule Candidate Embedding}}

\STATE The state $\bs_t$ is projected by a linear layer with SiLU activation: $\mathbf{k}_t =\texttt{SiLU}(\texttt{Linear}(\bs_t)) \in \mathbb{R}^{d_h}$, with $d_h$ being to denote the architecture hidden dimension. { // \textit{State Representation}}


\STATE Use cross-attention to obtain $\bh=\mathrm{CrossAttention}(\mathbf{Q}_t, \mathbf{k}_t, \mathbf{k}_t)\in \mathbb{R}^{q \times d_h}$.

\STATE (Optional) Further apply a self-attention network $\bh \leftarrow \mathrm{SelfAttention}(\bh)$.

\STATE Apply linear layer to obtain logits vector $\pmb{\alpha}_{\theta}^t=\mathrm{Linear}(\bh) \in \mathbb{R}^{q \times 1}$.

\STATE Calculate the policy distribution:
$\pi_{\theta, i}(\mathbf{Q}_t,\mathbf{k}_t) = \frac{\exp(\alpha^t_{\theta, i}(\mathbf{Q}_t,\mathbf{k}_t))}{\sum_{j=1}^q \exp(\alpha^t_{\theta, j}(\mathbf{Q}_t,\mathbf{k}_t))}$, where $\alpha^t_{\theta, i}$ is $i$-th element in $\pmb{\alpha}_{\theta}^t$.

\STATE Sample rule $\arule_t \sim \texttt{Categorical}(\cR; (\pi_{\theta,i}(\mathbf{Q}_t, \mathbf{k}_t))_{i=1}^q)$. {// \textit{Rule Selection with Attention}}
\STATE \textbf{Return} $\arule_t$.
\label{alg:rule_search}
\end{algorithmic}
\end{algorithm}

Algorithm \ref{alg:rule_search} outlines the process of rule selection using attention-based policies.   First, each rule candidate $\pmb{\rho}_i^t$ is embedded into a numeric vector $\bq_i^t$ using a sentence embedding technique (e.g., Sentence-BERT), forming a query matrix $\bQ_t$. The state $\bs_t$ is also converted into a numeric vector $\bk_t$. Cross-attention is applied between $\bQ_t$ and $\bk_t$ to generate an attention representation  $\bh$, which may optionally be refined using a self-attention mechanism. A linear layer processes to $\bh$ produce score vector $\pmb{\alpha}_{\theta}^t$. These scores define the policy distribution $\pi_\theta$, from which a rule $\arule_t$
is sampled. This attention-based approach ensures efficient selection of rules by leveraging contextual relationships between the state and rule candidates.

\begin{algorithm}
\caption{SAC for Attention-based Policy Network}
\label{algo:SAC}
\begin{algorithmic}[1]
\STATE Initialize Q networks $Q_{\phi_1}$, $Q_{\phi_2}$ and policy network $\pi_\theta$ with random parameters $\phi_1$, $\phi_2$, $\theta$.
\STATE Initialize target Q networks $\bar{Q}_{\bar{\phi}_1}$, $\bar{Q}_{\bar{\phi}_2}$ with weights $\bar{\phi}_1 \leftarrow \phi_1$, $\bar{\phi}_2 \leftarrow \phi_2$.
\STATE Initialize temperature parameter $\beta$ and target entropy $\mathcal{H}_{\text{target}}$;
 Initialize replay buffer $\mathcal{D}$.

\FOR{episode $= 1, \dots, M$}
    \STATE Initialize environment and observe initial state $\tilde{\mathbf{s}}_1$.
    \FOR{step $t = 1, \dots, T$}
        \STATE Sample action $\arule_t \sim \pi_\theta(\cdot|\tilde{\bs}_t)$.
        \STATE Execute action $\arule_t$, observe reward $\tilde{r}_t$ and next state $\tilde{\bs}_{t+1}$.
        \STATE Store transition $(\tilde{\bs}_t, \arule_t, \tilde{r}_t, \tilde{\bs}_{t+1})$ in replay buffer $\mathcal{D}$.
        
        \IF{enough samples in $\mathcal{D}$}
            \STATE Sample a mini-batch of transitions $(\tilde{\bs}_t, \arule_t, \tilde{r}_t, \tilde{\bs}_{t+1})$ from $\mathcal{D}$.
            
            \STATE Compute target Q values: $y_t = \tilde{r}_t + \gamma\mathbb{E}_{\arule_{t+1}\sim\pi_{\theta}(\cdot|\tilde{\bs}_{t+1})}\left[  \min_{j=1,2} \bar{Q}_{\bar{\phi}_j}(\tilde{\bs}_{t+1}, \arule_{t+1}) - \alpha \log \pi_\theta(\arule_{t+1}|\tilde{\bs}_{t+1}) \right]$.
            
            \STATE Update Q networks by minimizing:
            \STATE $L_Q(\phi_i) = \mathbb{E}_{(\tilde{\bs}_t, \arule_t, \tilde{r}_t, \tilde{\bs}_{t+1})} \left[ \left( Q_{\phi_i}(\tilde{\bs}_t, \arule_t) - y_t \right)^2 \right]$ for $i = 1, 2$.
            
            \STATE Update policy network by minimizing:
            \STATE $L_\pi(\theta) = \mathbb{E}_{\tilde{\bs}_t, \arule_t \sim \pi_\theta} \left[ \beta \log \pi_\theta(\arule_t|\tilde{\bs}_t) - \min_{j=1,2} Q_{\phi_j}(\tilde{\bs}_t, \arule_t) \right]$.
            
            \STATE Update temperature parameter by minimizing:
            \STATE $L_\beta(\beta) = \mathbb{E}_{\tilde{\bs}_t, \arule_t \sim \pi_\theta} \left[ -\beta \left( \log \pi_\theta(\arule_t|\tilde{\bs}_t) + \mathcal{H}_{\text{target}} \right) \right]$.
            
            \STATE Update target Q networks:
            \STATE $\bar{\phi}_i \leftarrow \tau \phi_i + (1 - \tau) \bar{\phi}_i$ for $i = 1, 2$.
        \ENDIF
    \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}

Algorithm~\ref{algo:SAC} presents the SAC algorithm tailored for training an attention-based policy network in selecting the desired rule. This method combines entropy-regularized policy optimization with a structured approach to handle rule-selection effectively.
The algorithm begins with the initialization of key components: Q networks \( Q_{\phi_1}, Q_{\phi_2} \), target Q networks \( \bar{Q}_{\phi_1}, \bar{Q}_{\phi_2} \), and a policy network \( \pi_\theta \). Random parameters are assigned to these networks, and the target Q networks are synchronized with the initial Q networks. A temperature parameter \( \alpha \) is initialized to regulate the entropy $\cH_{target}$ in the policy objective, ensuring a balance between exploration and exploitation. A replay buffer \( \mathcal{D} \) is set up to store transition data. Notice the entropy is defined as
\begin{align}
    \mathcal{H}_{target} = - \sum_{i=1}^q \pi_\theta(\arule_t | \bk_t, \mathbf{Q}_t) \log \pi_\theta(\arule_t | \mathbf{k}_t, \mathbf{Q}_t).
\end{align}


During training, each episode starts with the initialization of the environment, and the agent observes the initial state \( \tilde{\bs}_1 \). At every time step, the policy network generates an action \( \arule_t \) based on the current state. This action is executed in the environment, resulting in a reward \( \tilde{r}_t \) and a state transition to \( \tilde{\bs}_{t+1} \). These transitions are stored in the replay buffer for optimization.
When sufficient transitions are available in the buffer, the algorithm samples a mini-batch of transitions and computes the target Q values. The target Q values incorporate entropy regularization and are computed using the minimum of the target Q networks to ensure stability. The Q networks are updated by minimizing the mean squared error between the predicted Q values and the computed targets.
The policy network is optimized by minimizing a loss function that combines the policy entropy with the expected Q value, ensuring a stochastic and exploratory policy. The temperature parameter $\beta$ is updated to maintain the desired balance between exploration and exploitation. Finally, the target Q networks are softly updated to stabilize training. This iterative process continues across episodes and time steps, progressively refining the policy network to achieve optimal rule selection.








\subsection{Proof of Theorem \ref{prop:transition}}
In this section, we provide the detailed proofs for Theorem \ref{prop:transition}.
We start with the following equation
\begin{align}
P( \cR_{t+1}|\bs_{t+1})&\cdot \int_a P(\bs_{t+1}|\aenv,\bs_t)
    \cdot P(\aenv|\arule_t, \bs_t)d\aenv\nonumber\allowdisplaybreaks\\
   &= \underset{(a)~ {\color{blue} P(\bs_{t+1}|\aenv,\bs_t, \cR_t, \arule_t)=P(\bs_{t+1}|\aenv,\bs_t)}}{\underbrace{P( \cR_{t+1}|\bs_{t+1})\cdot\int_a P(\bs_{t+1}|\aenv,\bs_t, \cR_t, \arule_t)\cdot P(\aenv|\cR_{t}, \arule_t, \bs_t)d\aenv}}\nonumber\allowdisplaybreaks\\
   &=\underset{(b)~{\color{blue}P(\aenv|\bs_t,\cR_{t}, \arule_t)=P(\aenv|\cR_{t}, \arule_t)}}{\underbrace{ P( \cR_{t+1}|\bs_{t+1})\cdot\int_a P(\bs_{t+1}|\aenv,\bs_t, \cR_t, \arule_t)\cdot P(\aenv|\bs_t,\cR_{t}, \arule_t)d\aenv}}\nonumber\allowdisplaybreaks\\
   &= \underset{P(A|B,C)\cdot P(B|C)=P(A, B|C)}{\underbrace{P( \cR_{t+1}|\bs_{t+1})\cdot\int_a P(\bs_{t+1},\aenv|\bs_t, \cR_t, \arule_t)d\aenv}}\nonumber\allowdisplaybreaks\\
   &=P( \cR_{t+1}|\bs_{t+1})\cdot P(\bs_{t+1}|\bs_t, \cR_t, \arule_t)\nonumber\allowdisplaybreaks\\
   &=\underset{(c)~{\color{blue}P( \cR_{t+1}|\bs_{t+1}, \bs_t, \cR_t, \arule_t)=P( \cR_{t+1}|\bs_{t+1})}}{\underbrace{P( \cR_{t+1}|\bs_{t+1}, \bs_t, \cR_t, \arule_t)\cdot P(\bs_{t+1}|\bs_t, \cR_t, \arule_t)}}\nonumber\allowdisplaybreaks\\
    &=\underset{P(A|B,C)\cdot P(B|C)=P(A, B|C)}{\underbrace{P(\bs_{t+1}, \cR_{t+1}|\bs_t, \cR_t, \arule_t)}}\nonumber\allowdisplaybreaks\\
    &=\underset{\tilde{s}_t:=(\bs_t,\cR_t)}{\underbrace{P(\tilde{\bs}_{t+1}|\tilde{\bs}_t, \arule_t)}},
\end{align}
where $(a)$ follows from the fact that the transition to $\bs_{t+1}$ is fully determined by current state $\bs_t$ and current action to the environment $\aenv_t$, i.e., independent on rule set $\cR_t$ and selected rule $\arule_t$; $(b)$ holds since $\aenv_t$ is determined only by the selected rule $\arule_t$ and the state $\bs_t$; $(c)$ is due to our designed rule generation procedure where $\cR_{t+1}$ is generated by the LLM from the the latest state $\bs_{t+1}$.  
This completes the proof.
 




\section{Experiment Setup}
\label{appdx:setup}


\subsection{Environments Details}
\label{appdx:env-details}




\subsubsection{Wearable Device Assignment Domain}\label{appdx:health_care}
 

The simulator for the Uganda domain is adapted from \citep{boehmer2024optimizing} with minor modifications to simplify the problem.  This section provides an overview of the environment, with additional details available in the original paper.  In this environment, they want to allocate vital sign monitoring devices to mothers arriving in a maternal unit in order to better monitor mothers' health condition. Each mother's state is modeled by her vital signs (heart rate, respiratory rate, and blood oxygen saturation) along with each vital sign's variability. The mother's vital sign transition is governed by a multivariate Gaussian distribution defined over her vital signs at the current timestep and next timestep, learned from de-identified vital sign data collected from patients at Mbarara Regional Referral Hospital. MIMIC-III \cite{johnson2016mimic} is another de-identified clinical vital sign dataset that includes the same set of vital signs as the Uganda domain. The key difference is they have different data sources, as MIMIC-III's data comes from Beth Israel Deaconess Medical Center in Boston.

Wearing a monitoring device does not directly alter a mother's vital sign trajectory but has an indirect positive effect by triggering alerts when vital signs deviate from the normal range. These alerts often lead to medical interventions that improve the mother's condition.  If no monitoring device is assigned (passive action), the mother's next state is sampled from the multivariate Gaussian distribution conditioned on the current state. If a monitoring device is assigned and the vital signs remain within the normal range, the vital signs evolve as under the passive action. However, if any vital sign deviates from the normal range,  there is a $30\%$ chance the vital signs evolve as under the passive action, based on empirical evidence suggesting that clinicians fail to respond in such cases 30\% of the time \citep{boatin2021wireless}. Otherwise, vital signs are probabilistically adjusted towards the normal range before sampling the next state, modeling the positive impact of medical intervention.

The algorithm's goal is to optimize monitoring device allocation to maximize the aggregate reward across all mothers. We simplify the problem by requiring exactly one new mother to join the maternal unit at each timestep, starting with a single mother in the unit. The system has a budget of five monitoring devices. A device must be allocated to the new mother, and if all devices are already in use, one must be removed from a current user. Once removed, a device cannot be reassigned to the same mother. Each mother remains in the maternal unit for 10 timesteps, after which her vital sign trajectory no longer contributes to the reward. Once a device is taken from a mother, we directly sample her entire vital sign trajectory under passive action for the remaining timesteps she stays in the maternal unit and compute all her future rewards. We can directly compute future rewards because the mother will not receive the device again, so she will only undergo passive action in the remaining time. This observation enables us to maintain a smaller observation space, as we only need to keep track of the states of the mothers who own the device. 



In this domain, the constraints can be written as  $\|\ba_t\in\mathcal{R}^{d_2}\|_1\leq B, \forall t$ , which $d_2$ represents the number of patients in the system at each time slot, and the 1-norm of the action vector must remain within the budget $B$.









\subsubsection{Heat Alerts Domain}\label{appdx:heat-alerts}





The heat alert issuance problem can be modeled as an MDP in the context of RL \cite{considine2023optimizing}. The state at any given time, denoted as  $\bs_t$, encompasses both exogenous and endogenous factors. Exogenous factors include current weather conditions, such as the heat index, temperature, and humidity, which directly influence the risk of heat-related health issues. Endogenous factors include the history of issued alerts, such as the number and timing of past alerts, their effectiveness, and the remaining budget for the season. Additionally, the day of the week is considered, as public responsiveness to alerts may vary between weekdays and weekends.
The action space is binary, with $\ba_t\in\mathbb{Z}_2$. The decision to issue a heatwave alert $\ba_t=1$ or not $\ba_t=0$ is constrained by the remaining alert budget. If the budget is exhausted, no further alerts can be issued. The reward function is designed to reflect the reduction in heat-related hospitalizations, which depends on the effectiveness of the alert under current weather conditions. A Bayesian hierarchical framework could be employed to model the health impact of alerts, capturing the uncertainty in their effectiveness. Importantly, consecutive alerts tend to lose effectiveness, introducing a diminishing returns effect that must be accounted for in the decision-making process.

The transition dynamics, 
$P(\bs_{t+1} | \bs_t, \ba_t)$, describe how the system evolves over time. The next state is influenced by weather trajectories, the action taken, and public responsiveness to alerts. For instance, issuing an alert reduces the remaining budget and updates the history of issued alerts, while the weather conditions may change independently. Public responsiveness may also vary based on the frequency and timing of past alerts.
A key constraint in this problem is the limited alert budget, which necessitates strategic allocation of alerts throughout the season. The goal is to learn a policy 
$\pi(\bs_t|\ba_t)$ that maximizes cumulative rewards by effectively issuing alerts during severe heat conditions to minimize hospitalizations, while conserving the budget for future use. This involves balancing immediate health benefits against the potential need for alerts later in the season, addressing the trade-offs between short-term and long-term outcomes.


For this domain, the budget constraints can be expressed as $\sum_{t=1}^h \ba_t\in\mathbb{R}\leq B$, where the total sum of all actions over time horizon $h$ must not exceed a budget threshold $B$.



\subsection{Gym environments an Language Wrappers}

We implemented the \texttt{WearableDevicesAssignment} environments as \text{gymnasium} environments \cite{towers2024gymnasium}, while the \texttt{HeatAlerts} domain was already available in this format. We additionally created a \texttt{LanguageWrapper} Python class described in Table \ref{tab:language_wrapper}, which can be applied to any \texttt{gymnasium} environment. Our code implementations can be applied to any environment wrapped in this class.


\begin{table}[h]
    \centering
    \scalebox{0.75}{
    \begin{tabular}{|l|l|p{12cm}|}
        \hline
        \textbf{Method/Property} & \textbf{Type} & \textbf{Description} \\
        \hline
        \texttt{task\_text} & Property (Abstract) & Returns a description of the task that the environment is solving. \\
        \hline
        \texttt{action\_space\_text} & Property (Abstract) & Returns a description of the action space of the environment. \\
        \hline
        \texttt{state\_descriptor(obs, info)} & Abstract Method & Converts the observation into a text description. \\
        \hline
        \texttt{step(action)} & Method & Wraps the step method of the environment adding the text representation to the state space. \\
        \hline
        \texttt{reset(seed, options)} & Method & Wraps the reset method of the environment adding the text representation to the state space. \\
        \hline
        \texttt{action\_parser(s)} & Method & Parses an action string and converts it into an appropriate format for the environment's action space. \\ \hline
        \texttt{(rule\_examples} & Property (Optional) & Returns a list of string representation of rules. \\
        \hline
    \end{tabular}
    }
    \caption{Methods and properties of the \texttt{LanguageWrapper} class}
    \label{tab:language_wrapper}
\end{table}

\subsection{RL implementations, hyperparameters and Settings}

We implemented three main RL algorithms for the experiment sections: Attention-based SAC for \rbrl, numeric PPO, and Finetuning-based PPO. We based our implementation on the single-file, high-quality implementations from the \texttt{cleanrl} project \citep{huang2022cleanrl}. For Attention-based SAC, we required significant changes to keep track of the rule-augmented state space, as described in Section \ref{sec:sac-rl}. Other major changes to the baseline SAC implementation (originally designed for Atari) were more frequent target network updates and updating the actor and critic four times per iteration. This was done to improve sample efficiency and cope with the slow generation by the LLM. Numeric PPO was used practically without modification.

For the Finetuning-based PPO, we used low-rank adaptation (LoRA) \cite{hu2021lora} with the Transformers package and models hosted on Llama Hugging Face \cite{wolf-etal-2020-transformers}. We set the rank to $r=1$ and the adaptation weight to 2, resulting in only 0.8\% trainable parameters (still an order of magnitude larger than the Attention-based policy). Tables \ref{tab:parameters:sac}, \ref{tab:parameters:nppo}, and \ref{tab:parameters:lora} show the hyperparameters and settings used in these implementations.


\subsection{Computing environment}

SAC attention can run on a regular laptop since most of the computation happens in the cloud through API LLM calls, while the RL module is small and can run on personal CPUs. Nonetheless, the process is bottlenecked by the speed of generation from the APIs. A full run of 2 million environment steps, with parallelized API calls across four environments, took approximately four hours to complete. One training cycle did not exceed \$10 in API costs. However, all the experiments and development incurred approximately \$1,500 in API costs.
As described in the main text, the LLM fine-tuning experiments used an Nvidia A100 40GB GPU for each seed, training on three seeds for 18 hours each. Computations were performed on a Slurm-based high-performance computing cluster.


\begin{table*}[!h]
\centering
\caption{SAC Hyperparameters and Settings for \rbrl.}
\scalebox{0.6}{
\label{tab:parameters:sac}
\begin{tabular}{lll}
\toprule
\textbf{Parameter} & \textbf{Default Value} & \textbf{Description} \\
\midrule
\texttt{num\_envs} & 4 & Number of parallel environments \\
\texttt{total\_timesteps} & 500 & Total number of environment steps \\
\texttt{gamma} & 0.95 & Discount factor \(\gamma\) \\
\texttt{tau} & 1.0 & Target smoothing coefficient \\
\texttt{batch\_size} & 16 & Batch size of sample from the replay memory \\
\texttt{buffer\_size} & 4096 & The replay memory buffer size \\
\texttt{max\_episode\_steps} & 32 & Episode truncation  \\
\texttt{learning\_starts} & 256 & Timestep to start learning \\
\texttt{policy\_lr} & \(1 \times 10^{-4}\) & Learning rate of policy network optimizer \\
\texttt{q\_lr} & \(1 \times 10^{-4}\) & Learning rate of Q-network optimizer \\
\texttt{actor\_updates} & 4 & Number of actor updates per update cycle \\
\texttt{critic\_updates} & 4 & Number of critic updates per update cycle \\   
\texttt{target\_network\_frequency} & 64 & The frequency for the target network update\\
\texttt{alpha} & 0.01 & Initial entropy regularization coefficient \\
\texttt{autotune} & True & Automatic tuning of the entropy coefficient \\
\texttt{target\_entropy\_scale} & 0.89 & Coefficient for scaling the autotune entropy target \\ 
\texttt{dropout} & 0.05 & The dropout rate \\
\texttt{num\_rules} & 5 & Number of rules for \rbrl \\ 
\texttt{llm} & "gpt-4o-mini" & LLM for generation \\
\texttt{embedder\_lm} & "m2-bert-80M-8k-retrieval" & The LLM to use for embeddings \\
\texttt{embed\_dim} & 768 & Dimension of rule embeddings \\
\texttt{hidden\_dim} & 16 & Hidden dimension of networks \\
\texttt{rule\_reward\_coef} & 1.0 & The reward coefficient for the rules \\
\texttt{num\_self\_attention\_layers} & 1 & For the actor and critic\\
\texttt{num\_cross\_attention\_layers} & 1 & For the actor and critic\\
\bottomrule
\end{tabular}
}
\end{table*}

\begin{table}[!h]
\centering
\caption{Numeric PPO Hyperparameters and Settings.}
\label{tab:parameters:nppo}
\scalebox{0.6}{
\begin{tabular}{lll}
\toprule
\textbf{Parameter} & \textbf{Default Value} & \textbf{Description} \\
\toprule
\texttt{total\_timesteps} & 50000 & Total timesteps of the experiments \\
\texttt{learning\_rate} & \(2.5 \times 10^{-4}\) & Learning rate of the optimizer \\
\texttt{num\_envs} & 4 & Number of parallel environments \\
\texttt{num\_steps} & 512 & Steps per policy rollout \\
\texttt{anneal\_lr} & False & no learning rate annealing \\
\texttt{gamma} & 0.95 & Discount factor \(\gamma\) \\
\texttt{gae\_lambda} & 0.95 & Lambda for Generalized Advantage Estimation \\
\texttt{num\_minibatches} & 4 & Number of mini-batches \\
\texttt{update\_epochs} & 64 & Number of update epochs \\
\texttt{norm\_adv} & True & Whiten advantages \\
\texttt{clip\_coef} & 0.2 & Surrogate clipping coefficient \\
\texttt{clip\_vloss} & True & Clipped loss for value function \\
\texttt{ent\_coef} & 0.01 & Coefficient of entropy term \\
\texttt{vf\_coef} & 0.5 & Coefficient of value function \\
\texttt{max\_grad\_norm} & 0.5 & Maximum gradient clipping norm \\
\texttt{target\_kl} & None & Target KL divergence threshold \\
\texttt{hidden\_dim} & 16 & Hidden dimension of networks \\
\texttt{num\_hidden\_layers} & 2 & For policy and critic networks \\
\texttt{max\_episode\_steps} & 32 & Episode truncation  \\
\bottomrule
\end{tabular}
}
\end{table}


\begin{table}[!h]
\centering
\caption{LLM PPO Finetuning Hyperparameters and Settings.}
\label{tab:parameters:lora}
\scalebox{0.6}{
\begin{tabular}{lll}
\toprule
\textbf{Parameter} & \textbf{Default Value} & \textbf{Description} \\
\toprule
\texttt{total\_timesteps} & 500 & Total number of timesteps \\
\texttt{learning\_rate} & \(2.5 \times 10^{-4}\) & Learning rate of optimizer \\
\texttt{num\_envs} & 4 & Number of parallel game environments \\
\texttt{num\_steps} & 32 & Steps per policy rollout \\
\texttt{anneal\_lr} & True & Enable learning rate annealing \\
\texttt{gamma} & 0.95 & Discount factor \(\gamma\) \\
\texttt{gae\_lambda} & 0.95 & Lambda for Generalized Advantage Estimation \\
\texttt{update\_epochs} & 4 & Number of update epochs per cycle \\
\texttt{norm\_adv} & True & Advantages whitening \\
\texttt{clip\_coef} & 0.2 & Surrogate clipping coefficient \\
\texttt{clip\_vloss} & True & Clipped loss for value function \\
\texttt{ent\_coef} & 0.01 & Coefficient of entropy term \\
\texttt{vf\_coef} & 0.5 & Coefficient of value function \\
\texttt{kl\_coef} & 0.05 & KL divergence with reference model \\
\texttt{max\_grad\_norm} & 0.5 & Maximum gradient clipping norm \\
\texttt{target\_kl} & None & Target KL divergence threshold \\
\texttt{dropout} & 0.0 & Dropout rate \\
\texttt{llm} & "meta-llama/Llama-3.1-8B-Instruct" & Model to fine-tune \\
\texttt{train\_dtype} & "float16" & Training data type \\
\texttt{gradient\_accumulation\_steps} & 16 & Number of gradient accumulation steps \\
\texttt{minibatch\_size} & 1 & Mini-batch size for fine-tuning \\
\texttt{max\_chunk\_size} & 256 & Maximum length sequence for the back propagation \\
\texttt{max\_episode\_steps} & 32 & Maximum number of steps per episode \\
\bottomrule
\end{tabular}
}
\end{table}




\newpage

\section{Additional Survey Results}
Figure \ref{fig:survey_humans} illustrates the results of a human survey conducted to evaluate the quality of explanations generated by our method compared to alternatives. A total of 21 valid responses were collected for the \texttt{HeatAlert} environment (Figure \ref{fig:heatalert_human}), and 20 valid responses were gathered for the \texttt{Uganda} environment (Figure \ref{fig:Uganda_human}).
As shown in the figures, our method was favored by the majority of participants across all cases. In the \texttt{HeatAlert} environment, the preference for our approach is evident, although there is a small percentage of tied and ``Not Preferred" responses. In contrast, the preference for our method is even more pronounced in the \texttt{Uganda} environment, with a significantly higher number of participants selecting ``Ours Preferred." These results demonstrate the effectiveness of our approach in generating explanations that resonate better with human users, particularly in the \texttt{Uganda} domain.




Figure \ref{fig:survey_LLMs} illustrates the survey outcomes obtained by querying LLMs 20 times for each case in the \texttt{HeatAlerts} (Figure \ref{fig:heatalert_LLM}) and \texttt{Uganda} (Figure \ref{fig:Uganda_LLM}) environments. To ensure variability, the LLM's sampling temperature was controlled, enabling randomized responses for each trial. Similar to the human survey results, our method (``Ours Preferred") is overwhelmingly favored across all cases in both domains. Notably, the consistency of ``Ours Preferred" responses highlights the effectiveness of our approach in generating explanations that align well with the LLM's evaluation criteria, further validating the robustness of our methodology.





Figure \ref{fig:Hallucination_survey} illustrates the survey results evaluating hallucination occurrences across two environments (\texttt{Uganda} and \texttt{HeatAlert}) for three explanation types: Chain of Thought (\texttt{CoT}), Rule-Bottleneck Reinforcement Learning (\texttt{RBRL}), and \texttt{None} (indicating no explanation).

In Figure \ref{fig:Hallucination_human}, the results from the human survey indicate that \texttt{CoT}-based explanations had a significant proportion of hallucinations, particularly in the \texttt{Uganda} environment, where it accounted for 42.4\% of responses. \texttt{RBRL} explanations showed markedly fewer hallucinations in both domains, highlighting its robustness. A notable percentage of responses for \texttt{None} indicate scenarios where explanations were either absent or irrelevant.
In Figure \ref{fig:Hallucination_LLM}, results from the LLM survey further emphasize the trends observed in the human survey. Hallucination rates for \texttt{CoT} were even higher in the \texttt{Uganda} environment (81.7\%), whereas \texttt{RBRL} explanations exhibited almost no hallucinations across both domains. In the \texttt{HeatAlert} environment, the absence of explanations (\texttt{None}) led to the highest percentage of hallucinations, underlining the importance of well-structured, rule-based explanations like \texttt{RBRL}.
These results collectively demonstrate that the \texttt{RBRL} framework significantly mitigates hallucinations, providing more accurate and reliable explanations compared to other methods.

 

\begin{figure}[H]
\centering
\begin{minipage}{.4\textwidth}
\centering
\includegraphics[width=1\columnwidth]{Figures/Heatalert.png}
\subcaption{\texttt{HeatAlert}}
 \label{fig:heatalert_human}
\end{minipage}\hfill
\begin{minipage}{.4\textwidth}
\centering
\includegraphics[width=1\columnwidth]{Figures/Uganda.png}
\subcaption{\texttt{Uganda}}
 \label{fig:Uganda_human}
\end{minipage}
\caption{Results from \textbf{human} surveys conducted in the \texttt{HeatAlert} (a) and \texttt{Uganda} (b) environments. 21 participants provided feedback for the \texttt{HeatAlert} domain, while 20 valid responses were collected for the \texttt{Uganda} domain. The results indicate that our method (``Ours Preferred") was favored by a majority of participants, particularly in the \texttt{Uganda} domain, where the preference is more pronounced.}
        \label{fig:survey_humans}
\end{figure}



\begin{figure}[H]
\centering
\begin{minipage}{.4\textwidth}
\centering
\includegraphics[width=1\columnwidth]{Figures/Heatalert_LLM.png}
\subcaption{\texttt{HeatAlert}}
 \label{fig:heatalert_LLM}
\end{minipage}\hfill
\begin{minipage}{.4\textwidth}
\centering
\includegraphics[width=1\columnwidth]{Figures/Uganda_LLM.png}
\subcaption{\texttt{Uganda}}
 \label{fig:Uganda_LLM}
\end{minipage}
\caption{Survey results generated by querying \textbf{LLMs} 20 times for each case in the \texttt{HeatAlert} (a) and \texttt{Uganda} (b) environments. By varying the sampling temperature, randomized responses were collected for comparison. The results demonstrate that our method (``Ours Preferred") consistently outperforms alternatives across all cases, highlighting its robustness and alignment with the evaluation criteria of the LLMs.}
        \label{fig:survey_LLMs}
\end{figure}



\begin{figure}[H]
\centering
\begin{minipage}{.6\textwidth}
\centering
\includegraphics[width=1\columnwidth]{Figures/hallucination_Human.png}
\subcaption{Results from the \textbf{human} survey, showcasing the proportion of hallucination detected across three categories: \texttt{CoT}, \texttt{RBRL}, and \texttt{None}. In both domains, hallucinations were most frequently identified in \texttt{None}, with \texttt{RBRL} showing significantly fewer instances.}
 \label{fig:Hallucination_human}
\end{minipage}\hfill
\begin{minipage}{.6\textwidth}
\centering
\includegraphics[width=1\columnwidth]{Figures/hallucination_LLM.png}
\subcaption{Results from the \textbf{LLM}-based survey, where hallucination detection was assessed through multiple iterations of LLM evaluation. \texttt{CoT} exhibited higher hallucination rates in the \texttt{Uganda} domain, while \texttt{RBRL} demonstrated minimal hallucination occurrences in both domains.}
 \label{fig:Hallucination_LLM}
\end{minipage}
\caption{Survey results for hallucination detection across the \texttt{HeatAlert} and \texttt{Uganda} environments.}
        \label{fig:Hallucination_survey}
\end{figure}

\section{Prompt Templates and Rule Examples}\label{sec:prompt_appendix}

\subsection{Prompt Format}
In this section, we illustrate the prompt format used in our \rbrl for generating thoughts, rules, actions, rule scores, and explanations in Figure \ref{fig:combined_prompt}.






\begin{figure}[H]
    \centering    \includegraphics[width=0.65\linewidth]{Figures/Language_Wrapper.pdf}
    \caption{Prompts template for generating thoughts, rules, actions, rule scores, and explanations. }
    \label{fig:combined_prompt}
\end{figure}
 













\subsection{Rule Example}
In this section, we provide some rules for each domain in Figure \ref{fig:rule_example}.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Figures/rule_example_appendix.pdf}
    \caption{Rule examples for the considered two domains.}
    \label{fig:rule_example}
\end{figure}

\section{Survey Example}
\label{appdx:survey}
In this section, we present a survey example from the \texttt{Wearable\_Device\_Assignment} domain. The survey for the \texttt{HeatAlert} domain follows the same format and can be easily adapted by substituting the task and corresponding actions. For brevity, we include only one example case from the \texttt{Wearable\_Device\_Assignment}  domain.


\textbf{Task:} \emph{You are tasked with optimizing the allocation of limited vital sign monitoring devices among patients. Devices improve vital signs and prevent abnormalities, but their limited availability requires reallocating them from stable patients to higher-risk incoming patients, who must always receive a device. The normal range of vital signs are provided in Figure \ref{fig:vital sign}. The goal is to minimize costs associated with abnormal vital signs, where costs are calculated exponentially based on deviations from predefined thresholds. Wearing a device improves abnormal vital signs with a 70\% success rate.}


\textbf{Possible actions:} \emph{Choose the id of the device that will be reallocated to the new incoming patient. Your answer should be a single integer i from 0 to 4 (the number of devices) such that}:
\begin{itemize}
    \item \emph{Always choose a free device if available}
    \item \emph{If no free device is available, then choose device i whose current patient is at least risk or would benefit less from wearing the device.}
\end{itemize}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{Figures/vital_table.png}
    \caption{Normal range of vital signs.}
    \label{fig:vital sign}
\end{figure}

\emph{In the following, you will be presented with three cases. Each case includes two explanations. Please read the text for each case carefully and answer the questions provided.} 



\textbf{Case 1:}
Current state of the decision problem: \\
Number of devices: 5 \\
Number of free devices: 1\\
IDs of free devices: 3 


\textbf{Device 0:} Device is currently assigned to a patient with the following description: \\
*Timesteps wearing the device*: 1\\ 
*Pulse rate* - Last value: 95.22 - Mean: 105.12 - Standard deviation/volatility: 10.56 \\
*Respiratory rate* - Last value: 20.14 - Mean: 20.54 - Standard deviation/volatility: 0.64\\
 *SPO2* - Last value: 98.42 - Mean: 97.89 - Standard deviation/volatility: 0.88

\textbf{Device 1:} Device is currently assigned to a patient with the following description:\\ 
*Timesteps wearing the device*: 1\\ 
*Pulse rate* - Last value: 89.55 - Mean: 81.86 - Standard deviation/volatility: 8.55 \\
*Respiratory rate* - Last value: 14.85 - Mean: 20.81 - Standard deviation/volatility: 3.40 \\
*SPO2* - Last value: 95.31 - Mean: 96.22 - Standard deviation/volatility: 1.38 

\textbf{Device 2:} Device is currently assigned to a patient with the following description: \\
*Timesteps wearing the device*: 1 \\
*Pulse rate* - Last value: 106.05 - Mean: 105.09 - Standard deviation/volatility: 2.91 \\
*Respiratory rate* - Last value: 19.34 - Mean: 20.80 - Standard deviation/volatility: 2.69 \\
*SPO2* - Last value: 99.56 - Mean: 99.36 - Standard deviation/volatility: 0.27 

\textbf{Device 3:} Device is currently free. 

\textbf{Device 4:} Device is currently assigned to a patient with the following description: \\
*Timesteps wearing the device*: 1 \\
*Pulse rate* - Last value: 80.02 - Mean: 79.03 - Standard deviation/volatility: 1.58 \\
*Respiratory rate* - Last value: 22.71 - Mean: 21.31 - Standard deviation/volatility: 5.45 \\
*SPO2* - Last value: 99.61 - Mean: 99.86 - Standard deviation/volatility: 0.14


\underline{\textbf{Explanation A:}} I chose to reallocate device 4 because the patient currently using it has a stable pulse rate (80.02) and a high SPO2 level (99.61), indicating they are less at risk and may not require continuous monitoring. In contrast, reallocating this device allows for an incoming patient, who likely has more urgent health needs, to receive the device, thereby optimizing the overall allocation of resources to those who require immediate attention.

\underline{\textbf{Explanation B:}} I chose action {`device': 3} because device 3 is currently free and must be assigned to an incoming patient who requires immediate monitoring to prevent any potential deterioration in their health. This decision aligns with the prioritization rule that emphasizes the importance of providing care to all patients, especially those at risk, ensuring that the incoming patient receives the necessary vital sign monitoring.



\textbf{Q1.  Do Explanation A and Explanation B appear the same or different to you?}

\noindent
$\square$ Same (Skip Q2 and go to Question Q3) \\
$\square$ Different \\

\textbf{Q2. Which explanation do you find better?}

\noindent
$\square$ Explanation A \\
$\square$ Explanation B \\


\textbf{Q3. Do the explanations contain any hallucinations?}

\noindent
$\square$ Both \\
$\square$ Only Explanation A \\
$\square$ Only Explanation B \\
$\square$ None \\