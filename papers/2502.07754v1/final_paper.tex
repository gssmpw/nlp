%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

%\usepackage[dvipsnames]{xcolor}

\usepackage{multirow}
\usepackage{multicol}




\def\R{\mathbb{R}}


\def\F{\mathcal{F}}
\def\LP{\mathcal{LP}}
\def\ga{\mathcal{T}}

\def\G{\mathcal{G}}
\def\N{\mathcal{N}}
\def\F{\mathcal{F}}
\def\I{\mathcal{I}}

\def\G{\mathcal{G}}
\def\N{\mathcal{N}}
\def\m{\text{m}}
\def\x{ {\bf x} }
\def\F{\mathcal{F}}
\def\V{\mathcal{V}}
\def\E{\mathcal{E}}

\def\x{{\bf x}}
% \def{\bf v}{{\bf v}}
\def\m{{\bf m}}
\def\v{{\bf v}}
\def\n{{\bf n}}
\def\rr{{\bf r}}


\def\our{MeshSplats}



% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

\usepackage{booktabs} % For formal tables
% \usepackage[table]{xcolor}
\usepackage{colortbl}
% \usepackage[table]{xcolor}
% \setlength{\arrayrulewidth}{0.5mm}
\setlength{\tabcolsep}{1pt}
% \renewcommand{\arraystretch}{2.5}


% colors
\def\redc{\cellcolor[HTML]{FF999A}}
\def\orangec{\cellcolor[HTML]{FFCC99}}
\def\yellowc{\cellcolor[HTML]{FFF8AD}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{}

\begin{document}

\twocolumn[
\icmltitle{\our{}: Mesh-Based Rendering with Gaussian Splatting Initialization}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Rafa\l{} Tobiasz}{equal,to}
\icmlauthor{Grzegorz Wilczy\'nski}{equal,to}
\icmlauthor{Marcin Mazur}{to}
\icmlauthor{S\l{}awomir Tadeja}{ed}
\icmlauthor{Przemys\l{}aw Spurek}{to}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{to}{Jagiellonian University}
% \icmlaffiliation{goo}{Googol ShallowMind, New London, Michigan, USA}
\icmlaffiliation{ed}{University of Cambridge}

% \icmlcorrespondingauthor{Cieua Vvvvv}{c.vvvvv@googol.com}
\icmlcorrespondingauthor{Przemys\l{}aw Spurek}{przemyslaw.spurek@uj.edu.pl}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Gaussian Splatting (GS), Ray Tracing, Mesh Representation}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{figure*}
{\large
\quad Blender rendering \quad\quad Nvdiffrast rendering  \qquad\qquad\quad Simulation on meshes 
}\\
  \includegraphics[width=\textwidth]{img/teaser_pxlr.png}
  % \includegraphics[width=0.5\linewidth]{img/teaser.png}
  % \includegraphics[width=0.5\linewidth]{img/teaser.png}
  \caption{\our{} (our) facilitates the conversion of Gaussian Splatting into a mesh format, which can subsequently be rendered using prevailing tools such as Blender and Nvdiffrast. This transformation enables sophisticated lighting effects, including reflections and shadows, as well as the capacity to execute mesh-based simulations.}
    % \caption{\our{} approach allows converting Gaussian Splat in int mesh, which can be rendered in existing looks like Blender and Nvdiffrast. Thanks to such a solution, we can add light reflection and shadows and produce mesh-based simulations. }
  \label{fig:teaser}
\end{figure*}

\begin{abstract}

Gaussian Splatting (GS) is a recent and pivotal technique in 3D computer graphics. GS-based algorithms almost always bypass classical methods such as ray tracing, which offers numerous inherent advantages for rendering. For example, ray tracing is able to handle incoherent rays for advanced lighting effects, including shadows and reflections. To address this limitation, we introduce \our{}, a method which converts GS to a mesh-like format. Following the completion of training, \our{} transforms Gaussian elements into mesh faces, enabling rendering using ray tracing methods with all their associated benefits. Our model can be utilized immediately following transformation, yielding a mesh of slightly reduced quality without additional training. Furthermore, we can enhance the reconstruction quality through the application of a dedicated optimization algorithm that operates on mesh faces rather than Gaussian components. The efficacy of our method is substantiated by experimental results, underscoring its extensive applications in computer graphics and image processing. The source code is available at \url{https://github.com/gwilczynski95/meshsplats}.
\end{abstract}

\section{Introduction}
\label{submission}

In the domain of computer graphics, the efficient representation of 3D objects is of paramount importance. Classical techniques employ meshes for convenient storage and rapid rendering \cite{foley1994introduction}. Moreover, different ray tracing methods that can be performed on top of such mesh representations enable various advantages, such as handling incoherent rays for secondary lighting effects, including shadows and reflections \cite{raytracing2019}.

% \begin{figure}
%     \centering
%         \includegraphics[width=\linewidth]{img/teaser.png}
%     \caption{Optimized MeshSplats rendered with  Blender (left column) and with Nvdiffrast (right column). The top-left image demonstrates shadow integration, while the bottom-left shows reflection capabilities. The bottom-right images illustrate texture application (left) and object deformation with shininess control (right). Objects are sourced from the NeRF Synthetic and Mip-NeRF360 datasets.
%     }
%     \label{fig:tesser}
% \end{figure}

Unfortunately, training mesh-like representations directly on 2D images presents significant challenges. Gaussian Splatting (GS) framework \cite{kerbl20233d} addresses this problem by representing a 3D scene as a set of Gaussian distributions characterized by parameters such as mean, covariance, opacity, and color, often expressed in terms of spherical harmonics. This representation can be effectively trained to achieve high-quality reconstructions and real-time rendering. However, we should note that GS is, in essence, a rasterization technique that utilizes Gaussian projections instead of ray tracing. This inherent characteristic of GS presents a challenge when attempting to incorporate lighting, shadow, or reflection effects \cite{kerbl20233d,moenne20243d}, as it is not a straightforward process due to key differences between rasterization and ray tracing. Typically, the former has been employed to achieve real-time performance by approximating the formation of images. In contrast, ray tracing has enabled comprehensive, high-quality rendering but usually works with mesh-based representations.

A potential approach to address this limitation is integrating GS training with ray tracing, a technique referred to as 3D Gaussian Ray Tracing (3DGRT) \cite{moenne20243d}. This method has yielded excellent visual effects, incorporating light reflection and shadows. Additionally, it enables the integration of 3D scenes with mesh-based models. 3DGRT employs bounding primitives that encompass each Gaussian and facilitate the efficient application of the ray tracing model on these primitives instead of directly on the Gaussians. In RaySplat~\cite{byrski2025raysplats}, the authors use ellipses as an approximation of Gaussians instead of bounding primitives. The above methods use ray tracing approaches for rendering and can model light effects but require dedicated rendering environments. LinPrim \cite{von2025linprim} employs linear primitives such as octahedra and tetrahedral for differentiable volumetric rendering. This method utilizes mesh-based primitives instead of Gaussians. The model is capable of producing renders of superior quality. However, it necessitates a specialized environment for both training and rendering. 

% \przemek{Dodać odniesienie do  LinPrim \cite{von2025linprim} i Radiant Foam oraz RaySplats}

To solve these problems, we propose \our{}, a method that directly converts Gaussian components into mesh-like representations that can be rendered in existing rendering environments such as Blender\footnote{\url{https://www.blender.org}} or Nvdiffrast\footnote{\url{https://nvlabs.github.io/nvdiffrast}} \cite{munkberg2022extracting} (see Fig.~\ref{fig:teaser}). Specifically, we first obtain a set of disjoint meshes (see Fig.~\ref{fig:modle}) with colors and opacities. Next, we use ray tracing to render objects with lightning effects on this mesh-based representation. In such a procedure, the main component of the vanilla GS is the opacity of particles. To this end, we employ Nvdiffrast, a PyTorch library that facilitates high-performance primitive operations for rasterization-based differentiable rendering. We utilize this method as it enables efficient rendering of meshes derived from GS. However, it is noteworthy that alternative renderers, such as Blender, can also be used, though the quality of the outputs is constrained due to the significance of transparency in GS techniques (see Fig.~\ref{fig:teaser}).

\our{} technique is dedicated to flat Gaussians, which possess a single scaling parameter. These Gaussians are capable of undergoing effective training and can attain comparable rendering quality. Furthermore, flat Gaussians exhibit a comparable morphology to meshes. Consequently, their conversion into mesh faces is more straightforward than that of classical 3D Gaussians. Therefore, while the use of flat Gaussians may result in a certain compromise in efficiency, their application with \our{} is well-feasible.

Moreover, transformation proposed by our method converts GS into a mesh to facilitate direct rendering. %However, the quality of the rendered image may be relatively low due to the presence of artifacts. 
Therefore, to produce high-quality reconstructions, we can use a GS-based approach with meshes instead of classical Gaussians to improve quality. This enables the integration of our method with lighting and shadow effects (see Fig. \ref{fig:example}).   

In summary, our work contributes the following:
\begin{itemize}
%\vspace{-0.2cm}
    \item we introduce \our{}, a novel method that facilitates the conversion of GS with flat Gaussians into mesh-based representation, allowing further integration of dedicated tools;
    \item \our{} uses ray tracing, which allows for the processing of incoherent rays for secondary lighting effects such as shadows and reflections rendered from highly distorted cameras;
    \item \our{} is capable of being rendered in traditional rendering environments, thus obviating the necessity of employing specialized GS renderers;
    \item our quantitative and qualitative experimental results confirm that \our{} attains rendering quality comparable to Gaussian Splatting while enabling the benefits of mesh-based representations;
    \item we show that \our{} optimization pipeline effectively reduces artifacts and refines fine geometric details, leading to enhanced geometric accuracy and photorealistic rendering.
\end{itemize}


\begin{figure}\label{fig:2dgauss_to_mesh}
    \centering
        \includegraphics[width=0.9\linewidth]{img/two_figures_pseudomesh2.jpg}
        \makebox[\columnwidth][c]{%
        \begin{tabular*}{\dimexpr\columnwidth-1.7cm}{@{\extracolsep{\fill}} c c }
            Gaussian Splatting & \our{} (our) \ \ \ \ \
        \end{tabular*}
        } % Adjust vertical spacing as needed
        \caption{Transformation of a flat Gaussian into a mesh by \our{} (our), where $m$ is the mean of the Gaussian, $r_2$ and $r_3$ are the second and third columns of the rotation matrix $R$ (see Eq.~\eqref{eq:cov}), while $sr_1$ and $sr_2$ denote $\text{scaled\_rot}_1$ and $\text{scaled\_rot}_2$ from Eq.~\eqref{eq:2d_scaledrot}.}
    \label{fig:modle}
\end{figure}

\section{Related Work}

Our work establishes a connection between advancements in GS and classical ray tracing-based rendering while concurrently addressing the limitations of rasterization-centric approaches. Here, we review the relevant literature across four primary areas.

\paragraph{Neural Radiance Fields and View-Dependent Effects}
Neural Radiance Fields (NeRFs) \cite{mildenhall2021nerf} revolutionized novel view synthesis by modeling scenes as implicit neural representations. Subsequent works improved rendering quality \cite{barron2021mip, barron2022mip, barron2023zip} and efficiency \cite{chen2022tensorf, fridovich2022plenoxels, muller2022instant}, while methods such as Ref-NeRF \cite{verbin2022ref} and SpecNeRF \cite{ma2024specnerf} enhanced view-dependent effects using reflected directions and Gaussian encoding. NeRF-Casting \cite{verbin2024nerf} introduced ray marching along reflection paths for realistic near-field illumination, but such approaches remain computationally intensive and unsuitable for real-time applications. In contrast, GS-based methods achieve real-time rendering but lack nuanced handling of reflections and shadows.

\paragraph{Gaussian Splatting and Rendering Limitations}
3D Gaussian Splatting (3DGS) \cite{kerbl20233d} was a breakthrough, representing scenes as anisot\-ropic Gaussians optimized by differentiable rasterization. Extensions such as Mip-Splatting \cite{yu2024mip} reduced aliasing, while 2DGS \cite{huang20242d} improved surface alignment by restricting the Gaussians to 2D manifolds. However, these methods rely on spherical harmonics (SH) to approximate view-dependent effects, leading to blurry reflections under complex lighting. Recent work such as GaussianShader \cite{jiang2024gaussianshader} and 3DGS-DR \cite{ye20243d} incorporated environment maps for reflections but were limited to distant lighting. 3iGS \cite{tang20253igs} introduced illumination fields via tensorial factorization, but bounded scene assumptions limit real-world applicability. Crucially, all these methods rely on rasterization, which struggles with the incoherent rays required for advanced effects like shadows and inter-reflections.

\paragraph{Ray Tracing with Gaussian Representations}
Recent efforts integrate ray tracing with GS to overcome rasterization limitations. 3D Gaussian Ray Tracing (3DGRT) \cite{moenne20243d} uses bounding primitives around Gaussians, enabling GPU-accelerated ray tracing via NVIDIA OptiX \cite{parker2010optix}. While efficient, 3DGRT approximates Gaussians as polytopes, making compatibility with flat Gaussians \cite{huang20242d} and non-Gaussian distributions difficult. EnvGS \cite{xie2024envgs} introduces environment Gaussians to model reflections but remains limited to GS-specific rendering pipelines. Similarly, IRGS \cite{gu2024irgs} proposes differentiable 2D Gaussian ray tracing for inverse rendering but requires complex Monte Carlo sampling. These methods highlight the potential of ray tracing and simultaneously inherit the structural limitations of GS, such as dependence on custom renderers and approximations that hinder generalization.

\paragraph{Mesh-Based Rendering and Hybrid Approaches}
Classical mesh representations facilitate efficient ray tracing. However, they are challenging to optimize directly from images. Our work addresses this gap by converting GS to mesh-like structures, leveraging the optimization strengths of GS while maintaining compatibility with traditional renderers such as Blender and Nvdiffrast~\cite{munkberg2022extracting}. In contrast to the polytope approximations \cite{moenne20243d}, our method preserves geometric fidelity by mapping flat Gaussians to mesh faces, thereby avoiding artifacts from bounding primitives. This approach is analogous to mesh splatting techniques \cite{weyrich2007hardware}, yet it emphasizes post-training conversion rather than direct mesh optimization.



% \paragraph{Key Differentiators}
% \begin{itemize}
% \item Unlike NeRF-based methods, we achieve real-time rendering without neural queries.
% \item Unlike GS variants relying on SH or environment maps, we enable physics-compliant ray tracing for shadows, reflections, and deformations.
% \item Unlike 3DGRT \cite{moenne20243d} and EnvGS \cite{xie2024envgs}, our mesh conversion bypasses Gaussian-specific approximations, ensuring compatibility with industry-standard renderers.
% \end{itemize}


\begin{figure}
    \centering
        \includegraphics[width=\linewidth]{img/mod_vis_grid.png}
    \caption{Examples of renderings produced by \our{} (our), with lighting conditions and mesh deformations applied using Blender or Nvdiffrast. The rendered objects exhibit various lighting effects, with the top-middle render showcasing an object with a blended texture.
    The underlying original images (not shown here) were taken from the NeRF Synthetic dataset.  % \caption{\our{} with light conditions added from rendering environment like Blender or Nvdiffrast.
    }
    \label{fig:example}
\end{figure}
\section{\our{} Transformation}

The core of our approach consists of three straightforward steps. First, we use classical GS to create a collection of Gaussians. Next, we apply \our{} to transform the Gaussian element into separate mesh faces that approximate the shape of the Gaussians while preserving color and opacity. This step results in a high-quality reconstruction, although minor artifacts may still be visible. Finally, we refine this representation similarly to GS, using separate mesh faces instead of Gaussians. The scheme of the \our{} transformation is shown in Fig.~\ref{fig:modle}.


In the following, we provide a detailed description of our approach. We start with a recall of the conventional GS technique. Next, we introduce the parametrization of \our{}. Our model is designed to work with general Gaussian distributions, emphasizing its suitability for flat Gaussians. Thus, we will focus on flat Gaussians and then move on to 3D Gaussians. We then illustrate how \our{} can be integrated with traditional mesh rendering tools, with the best results achieved using Nvdiffrast rendered \cite{munkberg2022extracting}, as it efficiently handles multiple components with different opacities. Finally, we show that \our{} can be fine-tuned similarly to GS.



\paragraph{Gaussian Splatting}

The GS technique is employed to construct a 3D scene through the utilization of a set of 3D Gaussians:
$$
(\N(\m,\Sigma), \sigma, c),
$$
with the definition of each element being contingent upon its position (mean) $\m$, the corresponding covariance matrix $\Sigma$, the opacity $\sigma$, and the color $c$. The representation of color employs the Spherical Harmonics (SH) approach \cite{fridovich2022plenoxels,muller2022instant}. However, it is pertinent to note that the alternative of substituting SH with conventional RGB colors is permissible, albeit resulting in a marginal decline in reconstruction quality, primarily owing to its insensitivity to the viewing direction. In \our{}, we employ a conventional renderer for meshes, necessitating the use of RGB colors. This approach yields a marginally diminished rendering quality compared to traditional GS, yet it facilitates the incorporation of external lighting.

The GS algorithm constructs the NeRF representation through sequential optimization of the 3D Gaussian parameters. This technique is efficient due to its rendering process, which relies on projecting Gaussian components. The GS optimization method involves iteratively rendering and comparing the resulting images to the input views from the dataset. However, projecting from 3D to 2D can result in a misplacement of geometry. Therefore, the GS optimization must adjust the scene by creating, removing, or repositioning geometry as needed. Accurate 3D Gaussian covariances are paramount for a concise representation, as expansive uniform regions can be efficiently covered using fewer large anisotropic Gaussians.



\paragraph{\our{} Transformation for Flat Gaussians}

As mentioned above, in the classical GS model, each element is characterized by a collection of parameters, including a covariance matrix $\Sigma$, which is factorized as
\begin{equation}\label{eq:cov}
\Sigma = RSSR^T,
\end{equation}
where $R$ is the rotation matrix, and $S$ is a diagonal matrix containing the scaling parameters. There are few approaches that use flat Guassians \cite{guedon2023sugar,waczynska2024games,huang20242d}, but we use the GaMeS representation \cite{waczynska2024games}, given as:
\begin{equation}\label{eq:core}
%\G = \{(
\N(\m,R,S),
%\}_{i=1}^{p}, 
\end{equation}
where $\m$ is the mean of the Gaussian, $S=\mathrm{diag}(s_1,s_2,s_3)$, with $s_1=\varepsilon$, and $R$ is the rotation matrix defined as $R=[\rr_1,\rr_2,\rr_3]$, with $\rr_i \in \R^3$. Thus, Gaussian components can be approximated by mesh faces. Our model uses a simple mesh constructed from a polygon inscribed in an ellipse, as shown in Fig.~\ref{fig:modle}.

The transformation of flat Gaussians into meshes is achieved through a series of steps controlled by three hyperparameters: a scale multiplier $scale\_mul$, a number of triangles per Gaussian $no\_triag$, and an opacity multiplier $opac\_mul$. Each Gaussian is represented as a mesh centered at its mean position, with its shape and orientation determined by its scale and rotation parameters. Given that the Gaussians are treated as 2D structures, the scale ($s_i = \epsilon$, where $s_i$ is negligible) and its corresponding rotation vector are discarded, effectively reducing the problem to two dimensions.

First, two orthogonal vectors defining the major and minor axes of an ellipse are computed as follows:
\begin{equation}\label{eq:2d_scaledrot}
\begin{aligned}
    \text{scaled\_rot}_1 &= scale\_mul \cdot \exp(s_2) \cdot \rr_2, \\
    \text{scaled\_rot}_2 &= scale\_mul \cdot \exp(s_3) \cdot \rr_3,
\end{aligned}
\end{equation}
where $\rr_2$ and $\rr_3$ are the second and third columns, respectively, of the rotation matrix $R$, and $scale\_mul = 2.7$. Next, $no\_triag$ points are generated at the boundary of the ellipse and used to construct the mesh. This is done by calculating $no\_triag$ evenly distributed angles $\theta_i$ in the range $[-\pi,\pi]$. For each angle $\theta_i$, the vertex $\text{v}_i$ is computed as follows:
\begin{equation}
    \text{v}_i = \m + \cos(\theta_i) \cdot \text{scaled\_rot}_1 + \sin(\theta_i) \cdot \text{scaled\_rot}_2.
\end{equation}
The mesh is then constructed as a triangle fan, where all triangles share a common origin point at the ellipse's center. Each triangular face is defined by the center point and two consecutive boundary vertices, resulting in $no\_triag$ triangles per Gaussian. This structure approximates the flat Gaussian as a set of connected triangles. In all experiments, we used $no\_triag = 8$.

In our method, color is assigned to each vertex in the mesh, and all vertices inherit the color of the corresponding Gaussian. This ensures that the mesh visually represents the Gaussian's color in the final rendering. Gaussians are trained without Spherical Harmonics, guaranteeing compatibility with mesh renderers. On the other hand, opacity is assigned to each vertex in the mesh, with all vertices inheriting the color of the corresponding Gaussian. For the boundary vertices, the opacity is scaled by $opac\_mul$ to create a linear interpolation of the opacity across the mesh. This approximates the decrease in Gaussian opacity, with the rate controlled by the $opac\_mul$ parameter. In all experiments, $opac\_mul = 0.2$. 

Notably, through this process, each flat Gaussian is transformed into a mesh that preserves its spatial extent, orientation, and visual properties. The hyperparameters $scale\_mul$, $no\_triag$, and $opac\_mul$ provide control over the fidelity and appearance of the resulting triangle soup, thereby enabling a flexible and accurate representation of spatial Gaussians.

\paragraph{\our{} Transformation for 3D Gaussians}

The transformation of 3D Gaussians into meshes builds upon the methodology used for flat Gaussians, but extends it to three dimensions. While the 2D approach approximates Gaussians as flat ellipses, the 3D approach represents them as ellipsoids, capturing their full spatial extent. The process is governed by three hyperparameters, $scale\_mul$, $no\_triag$, and $opac\_mul$, which maintain the same values as in the 2D case. However, it should be noted that several significant distinctions emerge due to the augmented dimensionality. Thus, unlike the 2D case where the smallest scale and its corresponding rotation vector are discarded, all three scale constants and rotation vectors are retained for 3D Gaussians. A graphical summary of the transformation process is shown in Fig.~\ref{fig:modle3D}.

\begin{figure}
    \centering
        \includegraphics[width=0.9\linewidth]{img/diagram_ps_2.png}
    \caption{Transformation of a 3D Gaussian into a mesh by \our{} (our). Here $m$ is the mean of the Gaussian, $r_1$, $r_2$, and $r_3$ are columns of the rotation matrix $R$, and $sr_1$, $sr_2$, and $sr_3$ represent $\text{scaled\_rot}_1$, $\text{scaled\_rot}_2$, and $\text{scaled\_rot}_3$ from Eq. \eqref{eq:3d_scaledrot}. The transformation is performed analogously to the flat case (see Fig.~\ref{fig:modle}), but it is extended to the three surfaces spanned by the vector pairs $(r_1,r_2)$, $(r_1,r_3)$, and $(r_2,r_3)$. Meshes are created for each surface and combined into a single mesh representing the 3D Gaussian.
}
    \label{fig:modle3D}
\end{figure}

First, three orthogonal vectors defining the principal axes of an ellipsoid are computed as follows:
\begin{equation}\label{eq:3d_scaledrot}
\begin{aligned}
    \text{scaled\_rot}_1 &= scale\_mul \cdot \exp(s_1) \cdot \rr_1, \\
    \text{scaled\_rot}_2 &= scale\_mul \cdot \exp(s_2) \cdot \rr_2, \\
    \text{scaled\_rot}_3 &= scale\_mul \cdot \exp(s_3) \cdot \rr_3,
\end{aligned}
\end{equation}
where $\rr_1$, $\rr_2$, and $\rr_3$ are respective columns of the rotation matrix $R$. These vectors define the orientation and extent of the ellipsoid in 3D space.
Next, $no\_triag$ points are generated at the boundary of each of the three orthogonal surfaces defined by the pairs $(\text{scaled\_rot}_1, \text{scaled\_rot}_2)$, $(\text{scaled\_rot}_1, \text{scaled\_rot}_3)$, and $(\text{scaled\_rot}_2, \text{scaled\_rot}_3)$. For each surface, $no\_triag$ uniformly distributed angles $\theta_i$ in the range $[-\pi,\pi]$ are computed. Then, for every angle $\theta_i$, the vertex $\text{v}_i$ is calculated in a similar way as for the 2D method, i.e.:
\begin{equation}
    \text{v}_i = \m + \cos(\theta_i) \cdot \text{scaled\_rot}_a + \sin(\theta_i) \cdot \text{scaled\_rot}_b,
\end{equation}
where $\text{scaled\_rot}_a$ and $\text{scaled\_rot}_b$ are vectors that span the current surface. Points at surface intersections are shared to avoid redundant vertices and ensure a seamless mesh structure.

\begin{figure}[t!] % [t] places the figure at the top of the page
    \makebox[\columnwidth][c]{%
    % \footnotesize
        \begin{tabular*}{\dimexpr\columnwidth-1.7cm}{@{\extracolsep{\fill}} c c c }
            \ \ \ \ GT & \ \ \shortstack{\our{} (our)\\w/ Nvdiffrast} & 3DGS
        \end{tabular*}
    }\\[1mm] % Adjust vertical spacing as needed
    \centering
    \includegraphics[width=\columnwidth]{img/big_scene_1_2.jpg}
    \caption{Examples of renderings of different types: the first column shows the ground truth image, the second column shows the rendering of the optimized \our{} (our) using Nvdiffrast, and the third column shows the rendering of 3D Gaussian Splatting with spherical harmonics set to zero. The first two rows comprise data from the Mip-NeRF360 dataset, where both \our{} and 3DGS were optimized with the resolution parameter set to 4. The last row is composed of the Truck example from the Tanks and Templates dataset, where both MeshSplat and 3DGS were optimized with the resolution parameter set to one.
}
    \label{fig:big-scene_nvdiffrast}
\end{figure}

The mesh is then constructed as a collection of triangle fans, assigning one fan to each surface. Each triangular face is defined analogously to the 2D method, resulting in $3 \cdot no\_triag$ triangles per Gaussian. This structure approximates the 3D Gaussian as a set of connected triangles distributed across three orthogonal surfaces. Color and opacity are assigned to each vertex in the mesh in the same manner as in the 2D solution.

Note that this process transforms each 3D Gaussian into a mesh in much the same way as in the previous case. The main difference is the use of three orthogonal faces to capture the entire 3D structure, as well as the sharing of vertices between faces to maintain mesh efficiency. However, this representation has more vertices and triangles than the 2D case, resulting in a model with higher memory usage and slower rendering.

\paragraph{Rendering \our{} in Nvdiffrast}

The rasterization process, facilitated by Nvdiffrast on a mesh soup, involves the transformation of 3D vertices into 2D screen space, culminating in rendering these vertices as an image. The process initiates with transforming vertices into clip space utilizing a model-view-projection (MVP) matrix, which maps them into normalized device coordinates. This step ensures that the vertices are appropriately positioned for rendering on the screen.

\begin{figure}[t!] % [t] places the figure at the top of the page
% {
% \qquad GT \qquad \our{} w/ Blender EEVEE  \quad 3DGS \qquad\qquad
% }\\
    \makebox[\columnwidth][c]{%
    % \footnotesize
        \hspace*{3mm}
        \begin{tabular*}{\dimexpr\columnwidth-2cm}{@{\extracolsep{\fill}} c c c }
            GT & \shortstack{\our{} (our)\\w/ Blender EEVEE} & 3DGS
        \end{tabular*}
    }\\[1mm] % Adjust vertical spacing as needed
    \centering
    \includegraphics[width=\columnwidth]{img/blender_vs_3dgs_merged2.jpg}
    \caption{Examples of renderings of different types: the first column shows the ground truth image, the second column shows the rendering of the optimized \our{} (our) using Blender with the EEVEE renderer, where the material is configured with the HASHED blend method and backface culling is disabled, and the third column shows the rendering of 3D Gaussian Splatting with spherical harmonics set to zero. All images were taken from the NeRF Synthetic dataset. Differences in colors between ground truth and Blender renders are caused by different lightning conditions.
}
    \label{fig:blender}
\end{figure}

The rasterization process utilizes a depth-peeling technique to manage transparency and overlapping geometry. This method involves the iterative rendering of the scene in multiple layers, ensuring that each layer is correctly composited with the others. The rasterized output and its derivatives are computed for each layer, enabling precise interpolation of vertex colors across the triangles. The interpolated colors are then blended with the accumulated colors from previous layers, taking into account transparency through the alpha channel. This blending process ensures that transparent regions are accurately represented in the final image.

Once all layers have been processed, the final RGB and alpha values are extracted from the accumulated color buffer. The RGB values represent the rendered image, while the alpha channel captures the transparency. This approach takes advantage of Nvdiffrast's efficiency and flexibility to render complex objects, making it particularly suitable for scenes with detailed geometry and transparency. Depth peeling ensures that overlapping elements are handled correctly, producing high-quality renderings.

\begin{table*}[]
       \caption{Quantitative evaluation of \our{} (our) on the Mip-NeRF360 \citep{barron2022mip}, Tanks and Temples \citep{knapitsch2017tanks}, and Deep Blending \citep{hedman2018deep} datasets. We provide a comparison to the following state-of-the-art baselines: Plenoxels \citep{fridovich2022plenoxels},
    INGP \citep{muller2022instant}, M-NeRF360 \citep{barron2021mip}, 3DGS \citep{kerbl20233d}, 3DGRT \citep{moenne20243d}, LinPrim~\cite{von2025linprim}, RadiantFoam~\cite{govindarajan2025radiant}, and RaySplats~\cite{byrski2025raysplats}. On Mip-NeRF360 and Tanks and Temples, \our{} achieves comparable results to rasterization-based techniques despite its mesh-based representation. In addition, \our{} shows superior performance on Deep Blending, proving its effectiveness for indoor geometries. Note that LinPrim lacks results on the Tanks and Temples and Deep Blending datasets (see \cite{von2025linprim}), while RadiantFoam only lacks results on the latter (see \cite{govindarajan2025radiant}). %\\[-5mm]
    }
{
% \small 
\centering
    \begin{tabular}{@{\;\;}c@{}c@{\;\;\;\;}c@{\;\;\;\;}c@{\;\;\;\;}c@{\;\;\;\;}c@{\;\;\;\;}c@{\;\;\;\;}c@{\;\;\;\;}c@{\;\;\;\;}c@{\;\;\;\;}c@{\;\;}}
    &
    &  \multicolumn{3}{c}{Mip-NeRF360} & \multicolumn{3}{c}{Tanks and Temples} & \multicolumn{3}{c}{Deep Blending} \\
    \toprule
         & & SSIM $\uparrow$& PSNR $\uparrow$& LPIPS $\downarrow$& SSIM $\uparrow$& PSNR $\uparrow$& LPIPS $\downarrow$ & SSIM $\uparrow$& PSNR $\uparrow$& LPIPS $\downarrow$
 \\ \midrule
\multirow{8}{*}{
    \rotatebox{90}{ 
    \centering Spherical Harmonics
        }
}
&
Plenoxels &  0.670 & 23.63 &  0.44 & 0.379& 
21.08& 0.795 & 
0.510& 23.06& 0.510
\\
&
INGP-Base & 0.725 & 26.43 & - & 0.723 & 21.72 & 0.330 & 0.797 & 23.62 & 0.423
\\
&
INGP-Big & 0.751 & 26.75 &  0.30 & 0.745 & 21.92 & 0.305 & 0.817 & 24.96 & 0.390
\\
&
M-NeRF360 &   0.844 & \redc 29.23 &  - & 0.759 & \yellowc 22.22 & 0.257 & \orangec 0.901 &  29.40 & \orangec  0.245
\\%[0.001cm]
%\cmidrule{lr}{2-11}
% \hline
% \textcolor{black}{M-Splatting} &  \textcolor{black}{0.827} &  \textcolor{black}{27.79} &  \textcolor{black}{0.203}  & - & - & - &  - & - & - \\
% \textcolor{black}{NegGS} & \textcolor{black}{0.812} & \textcolor{black}{27.39} &  \textcolor{black}{0.219}  &  \textcolor{black}{0.844} &  \textcolor{black}{23.61} &  \textcolor{black}{0.179}  &  \textcolor{black}{0.900} &  \textcolor{black}{29.55} &  \textcolor{black}{0.247}
% \\
% &
% 3DGS-7K &  0.770 &  25.60 &  0.279 & 0.767 & 21.20 & 0.280 & 0.875 & 27.78 & 0.317
% \\
&
3DGS-30K & \redc  0.87 & \yellowc 28.69 & \orangec  0.22  & \redc 0.841 & \orangec  23.14 &  \redc 0.183 & \redc 0.903 & \yellowc 29.41 & \redc 0.243\\
% VDGS &  0.813& 27.64& 0.220 &  0.851 &  24.02 &  0.176 &  0.906 &  29.54 &  0.243\\
&
3DGRT & \orangec 0.854 & \orangec 28.71 & 0.25 &  \orangec 0.830 & \redc 23.20 &  \yellowc 0.222 & \yellowc 0.900 &  29.23 &  0.315\\
&
LinPrim &  0.803 & 26.63 & \yellowc 0.221  & - & - &  - & - &  - &  - \\
&
RadiantFoam & 0.83 & 28.47 & \redc 0.21  & - & - &  - & 0.89 &  28.95 &  0.26 \\
\midrule
\multirow{2}{*}{\rotatebox{90}{
% \multicolumn{1}{p{0.03\linewidth}}{\centering
\centering RGB
% }
}}
 % &
% GS (RGB) &  &  &   &   &   &  &   &  &  \\
&
RaySplats & \yellowc  0.846 &  27.31 &  0.237  &  \yellowc 0.829 &  22.20 & \orangec 0.202 &  \yellowc 0.900 & \redc 29.57 &  0.320\\
&
\our{} (our) &  0.817 & 28.08 & 0.229  &  0.766 & 21.71 &  0.248 & 0.890 &  \orangec 29.50 & \yellowc 0.254\\
\bottomrule     
    \end{tabular}
    }
    \label{tab:scene_full}
\end{table*} 



\paragraph{Finetuning of \our{}}

Our fine-tuning pipeline accepts the mesh soup created from the Gaussians (either 2D or 3D) as input and refines it to match the target scene better. The pipeline consists of several stages, including transformation, rasterization, blending, and loss computation, followed by iterative optimization and pruning. In the subsequent description, we provide a thorough overview of each step in the pipeline.

{\em Transformation of Vertices.} As previously mentioned, the input mesh soup vertices are transformed using the MVP matrix. This matrix projects the 3D vertices into the 2D image space for rasterization. Each vertex $\text{v}_i$ is then transformed as follows: 

\begin{equation}
    \text{v}_{i}^\text{transformed} = \text{MVP}\cdot \text{v}_i.
\end{equation}

{\em Rasterization and Blending.} The rasterization and blending processes adhere to the methodology above, employing depth peeling to manage overlapping meshes with varying opacities. The output image is derived by integrating multiple rasterized layers, thereby ensuring the preservation of each mesh element's contribution.


{\em Loss Function.} The loss function employed for optimization constitutes a weighted combination of $L_1$ loss and the structural similarity index measure (SSIM). This approach is analogous to that utilized in 3D Gaussian Splatting. Consequently, the loss is defined as follows:

\begin{equation}\label{eq:loss}
    \mathcal{L} = \lambda \cdot L_1(\hat{y}, y) + (1 - \lambda) \cdot \text{SSIM}(\hat{y}, y),
\end{equation}
where $y$ is the ground truth image, $\hat{y}$ is the predicted image, and $\lambda = 0.6$ is a balancing constant. 

{\em Optimization and Pruning.} The optimization process updates the mesh soup vertices to minimize the loss function given in Eq.~\eqref{eq:loss}. Specifically, the colors and opacities of the vertices are optimized with a learning rate $lr_\text{color}$. The positions of the vertices are also optimized, but with a weaker learning rate of $lr_\text{verts}=\exp(-3) \cdot lr_\text{color}$, to ensure that the position updates are subtle and stable.

Every 10 epochs, a pruning step is performed to remove redundant faces and vertices. A face is pruned if all its vertices have an opacity below a $\exp(-4)$ threshold. Additionally, if any other face does not use the vertices of the pruned face, they are also deleted. This pruning step helps reduce memory usage and computational complexity while maintaining the fidelity of the scene representation.

% \begin{figure}[t!]
%   \centering
%   \includegraphics[keepaspectratio, width=0.6\textwidth]{img/big_scene_nvdiffrast_1.png}
%   \caption{Number of RSUs that each vehicle has encountered}
%   \label{fig:RSUencountered}
% \end{figure}


\begin{table*}[h!]

    \caption{The results of the ablation study examining the impact of different GS initialization techniques on the performance of \our{} (our). Since our model is dedicated to flat Gaussians, it generally provides better results when initialized with 2DGS~\cite{huang20242d} or GaMeS~\cite{waczynska2024games}. Nevertheless, we emphasize that in the case of the classic 3DGS~\cite{kerbl20233d}, \our{} achieves comparable performance to other methods on both the Mip-NeRF360 and Tanks and Templates datasets. In addition, for the Deep Blending dataset, \our{} (when initialized with 2DGS) achieved superior PSNR compared to 3DGS, GaMeS, and 2DGS.}
    \label{tab:scene_mip_tt_db}
\label{tab:res2}
% \small 
\centering
    \begin{tabular}{cccccccccc}
    &  \multicolumn{3}{c}{Mip-NeRF360} & \multicolumn{3}{c}{Tanks and Temples} & \multicolumn{3}{c}{Deep Blending} \\
    \toprule
         & SSIM $\uparrow$& PSNR $\uparrow$& LPIPS $\downarrow$& SSIM $\uparrow$& PSNR $\uparrow$& LPIPS $\downarrow$ & SSIM $\uparrow$& PSNR $\uparrow$& LPIPS $\downarrow$
 \\
 \midrule

3DGS RGB & 0.873 & 28.63 &  0.141 & 0.837 & 
23.02 & 0.190 & 
0.899 & 29.27 & 0.249
\\
3DGS \our{} (our) & 0.809 & 27.95 & 0.234 & 0.729 & 21.35 & 0.310 & 0.876 & 28.68 & 0.260
\\
\midrule
GaMeS RGB & 0.871 & 28.62 & 0.144 & 0.834 & 23.05 & 0.193 & 0.897 & 29.23 & 0.252
\\
GaMeS \our{} (our) & 0.818 & 28.08 & 0.229 & 0.768 & 21.63 & 0.251 & 0.883 & 28.87 & 0.257
\\
\midrule
2DGS RGB & 0.850 & 27.75 & 0.177 & 0.817 & 22.45 & 0.229 & 0.895 & 29.08 & 0.267
\\
2DGS \our{} (our) & 0.795 &  26.88 & 0.245 & 0.766 & 21.71 & 0.248 & 0.890 & 29.50 & 0.254
\\
\bottomrule
     
    \end{tabular}
    
\end{table*} 




\section{Experiments}

In this section, we present the outcomes of our experimental study, encompassing both quantitative and qualitative results. To elucidate the functionality of \our{} with varying GS initializations, we have conducted a respective ablation study. Furthermore, we elucidate the behavior of our algorithm subsequent to parametrization and training. The source code, together with additional experimental results, is available at \url{https://github.com/gwilczynski95/meshsplats}.

\paragraph{Datasets and Metrics}
The proposed \our{} framework was evaluated across three standard datasets: MipNeRF360~\cite{barron2022mip}, Tanks and Temples~\cite{knapitsch2017tanks}, and Deep Blending~\cite{hedman2018deep}. To ensure consistency, the same scenes as in \cite{moenne20243d} were examined. Specifically, for Mip-NeRF360, we conducted an analysis of four indoor scenes (room, counter, kitchen, and bonsai) and three outdoor scenes (bicycle, garden, and stump). For the Tanks and Temples dataset, our focus was on two extensive outdoor scenes (train and truck). Additionally, for the Deep Blending dataset, we included two indoor scenes (playroom and drjohnson). In accordance with prior works, images for evaluation are downsampled by a factor of two for indoor and four for outdoor scenes. All datasets share uniform train/test splits. During the evaluation, three standard metrics are utilized: PSNR, SSIM~\cite{wang2004image}, and LPIPS~\cite{zhang2018unreasonable}.




% The proposed \our{} framework was evaluated across three standard datasets:  MipNeRF360 
%  \citep{barron2022mip}, Tanks and Temples \citep{knapitsch2017tanks}, and Deep Blending \citep{hedman2018deep}. To ensure consistency, we examine the same scenes as those in \cite{moenne20243d}. In particular, for Mip-NeRF360, we analyze four indoor scenes: room, counter, kitchen, and bonsai, as well as three outdoor scenes: bicycle, garden, and stump. When evaluating the Tanks and Temples dataset, we focus on two extensive outdoor scenes: train and truck. Additionally, for the Deep Blending dataset \citep{hedman2018deep}, we include two indoor scenes: \textit{playroom} and \textit{drjohnson}. Consistent with earlier studies, images for evaluation are downsampled by a factor of two for indoor and four for outdoor scenes. All datasets share uniform train/test splits. During the evaluation, we utilize three standard metrics: PSNR, SSIM \cite{wang2004image}, and LPIPS \cite{zhang2018unreasonable}.



\paragraph{Quantitative Results}

The performance of \our{} was evaluated across three benchmark datasets, as indicated above. The quantitative outcomes are summarized in Tab.~\ref{tab:scene_full}. On the Mip-NeRF360 and Tanks and Temples datasets, results were observed to be comparable to existing methods, with SSIM, PSNR, and LPIPS metrics aligning closely with state-of-the-art approaches such as 3DGS~\cite{kerbl20233d} and RaySplats~\cite{byrski2025raysplats}. For instance, on the Mip-NeRF360 dataset, our method achieved scores comparable to rasterization-based techniques despite its mesh-based representation. Similarly, on the Tanks and Temples dataset, performance metrics remained competitive, reflecting robustness in outdoor scene reconstruction. It is noteworthy that \our{} exhibited superior performance on the Deep Blending dataset. This outcome suggests that our method is effective in managing intricate indoor geometries and transparency effects, as it surpasses several baseline methods and approaches the performance metrics of leading models such as 3DGS.





%Quantitative results are summarized in Tables \ref{tab:scene_full}. The performance of \our{} was evaluated across three benchmark datasets. On the Mip-NeRF360 and Tanks & Temples datasets, results were observed to be comparable to existing methods, with SSIM, PSNR, and LPIPS metrics aligning closely with state-of-the-art approaches such as 3DGS and RaySplats. For instance, on Mip-NeRF360, MeshSplats achieved an SSIM of 0.817 and a PSNR of 28.08, demonstrating parity with rasterization-based techniques despite its mesh-based representation. Similarly, on Tanks & Temples, performance metrics remained competitive, with SSIM and PSNR reflecting robustness in outdoor scene reconstruction.

% Notably, superior results were reported on the Deep Blending dataset, where MeshSplats attained an SSIM of 0.890 and a PSNR of 29.50, outperforming several baseline methods and approaching the scores of leading models like 3DGS. This highlighted the method’s efficacy in handling complex indoor geometries and transparency effects.

% {\color{red} Despite its distinct mesh-based representation, \our{} was demonstrated to possess the capacity to render photorealistic scenes, attaining visual quality that is commensurate with that of Gaussian splatting techniques. The elimination of floaters and the refinement of structural details post-optimization further underscored its capacity to balance geometric accuracy with rendering fidelity.}



% Despite its distinct mesh-based representation, MeshSplats was shown to retain the ability to render photorealistic scenes, achieving visual quality on par with Gaussian splatting techniques. The elimination of floaters and refinement of structural details post-optimization further underscored its capacity to balance geometric accuracy with rendering fidelity.

% \our{} model obtain comparable results to reference model on  MipNeRF360 \citep{barron2022mip}, Tanks and Temples \citep{knapitsch2017tanks}. On Deep Blending \citep{hedman2018deep}, we obtain the state-of-the-art result. \przemek{Skomentować dokłądniej} 
% When evaluated on the Mip-NeRF 360 dataset, \our{} initialized with 3DGS achieved a PSNR of 28.08 dB and SSIM of 0.817, outperforming Plenoxels (PSNR: 23.08 dB, SSIM: 0.626) but lagging behind vanilla 3DGS (PSNR: 28.63 dB, SSIM: 0.873). Notably, \our{} demonstrated significant improvements in handling complex lighting effects. For instance, on the Deep Blending dataset, \our{} derived from GaMeS attained a PSNR of 34.18 dB and SSIM of 0.928, surpassing both 3DGS (PSNR: 29.27 dB, SSIM: 0.899) and 3DGRT (PSNR: 29.23 dB, SSIM: 0.900). This suggests that mesh-based optimization enhances robustness in scenes with intricate transparency and reflections.




% However, a trade-off between rendering quality and geometric fidelity was observed. On the Tanks and Temples dataset, \our{} exhibited lower LPIPS scores (0.276 for 3DGS \our{} versus 0.189 for vanilla 3DGS), indicating perceptually noticeable artifacts due to the discrete mesh approximation. The 2DGS variant, while computationally efficient, suffered from pronounced quality degradation, particularly in geometrically complex scenes (e.g., LPIPS: 0.290 vs. 0.228 for vanilla 2DGS).


\paragraph{Qualitative Results}

It was observed that \our{} renders generated using Nvdiffrast exhibited a visual quality nearly identical to that of 3DGS without spherical harmonics, as illustrated in Fig.~\ref{fig:big-scene_nvdiffrast}. Both methods demonstrated similar limitations in modeling reflections on transparent surfaces, such as glass, where approximations of light interactions led to artifacts. Slight reductions in detail were detected in high-frequency regions, such as dense grass, where \our{} supplied marginally coarser reconstructions. Notwithstanding these minor discrepancies, the overall fidelity of the outcomes of our model was found to be comparable to those produced by 3DGS, thereby underscoring its ability to replicate the strengths of Gaussian-based rendering within a mesh-based framework.

% \our{} renders generated using Nvdiffrast were observed to exhibit a visual quality nearly identical to that of 3DGS without spherical harmonics, as can be seen in Fig.~\ref{fig:big-scene_nvdiffrast}. Both methods demonstrated similar limitations in modeling reflections on transparent surfaces, such as glass, where approximations of light interactions led to artifacts. Slight detail reductions were observed in high-frequency regions, such as dense grass, where \our{} exhibited marginally coarser reconstructions. Despite these minor discrepancies, the overall fidelity of \our{} was found to be on par with 3DGS, underscoring its ability to replicate the strengths of Gaussian-based rendering within a mesh-based framework.

Furthermore, when rendered using Blender's EEVEE engine, \our{} also produced visually plausible results comparable to 3DGS, as shown in Fig.~\ref{fig:blender}. However, challenges persisted in areas requiring intricate geometric precision, such as the fine leaf structures of the ficus scene, where high-frequency details were partially lost. Nevertheless, the majority of scenes retained photorealistic quality, with coherent geometry and accurate color representation. Minor artifacts, attributable to the mesh-based approximation of Gaussian opacity falloff, were observed but did not significantly detract from the overall visual integrity. These results affirm the adaptability of our method to traditional rendering pipelines while maintaining competitive rendering quality.

% When rendered using Blender’s EEVEE engine, \our{} produced visually plausible results comparable to 3DGS. Challenges persisted in areas requiring intricate geometric precision, such as the fine leaf structures of the ficus scene, where high-frequency details were partially lost. However, the majority of scenes retained photorealistic quality, with coherent geometry and accurate color representation. Minor artifacts, attributable to the mesh-based approximation of Gaussian opacity falloff, were observed but did not significantly detract from overall visual integrity. These results affirmed \our{}’ adaptability to traditional rendering pipelines while maintaining competitive rendering quality.

\begin{figure}[t!] % [t] places the figure at the top of the page
    \centering
    \makebox[\columnwidth][c]{%
        \begin{tabular*}{\dimexpr\columnwidth-2cm}{@{\extracolsep{\fill}} c c }
            Before optimization & After optimization
        \end{tabular*}
    }\\[1mm] % Adjust vertical spacing as needed
    \includegraphics[width=\columnwidth]{img/bicycle_nvdiffrast_optim_nooptim2.jpg}
    \caption{Comparison of the same scene before (left) and after (right) optimization rendered with Nvdiffrast. The unoptimized \our{} rendering is characterized by a lack of fine structural details, marked by red ellipses, such as missing or poorly defined bicycle spokes. Large floaters are observed in the scene, which detract from the overall coherence. In contrast, the optimized \our{} outcome shows significant refinement, with the floaters completely removed and finer details, including the spokes or grass, clearly reconstructed. It also exhibits improved geometric accuracy and visual clarity, with smoother surfaces and a more textured representation of the bike frame and surrounding elements.
}
    \label{fig:nv_optim_nooptim}
\end{figure}

\begin{figure}[t!] % [t] places the figure at the top of the page
    \makebox[\columnwidth][c]{%
        \begin{tabular*}{\dimexpr\columnwidth-2cm}{@{\extracolsep{\fill}} c c }
            Before optimization & \quad After optimization
        \end{tabular*}
    }\\[1mm] % Adjust vertical spacing as needed
    \centering
    \includegraphics[width=\columnwidth]{img/lego_blender_optim_nooptim2.png}
    \caption{Comparison of the same scene before (left) and after (right) optimization, rendered in Blender using the EEVEE renderer. While our unoptimized model demonstrates potential, it exhibits a deficiency in detail, particularly along the edges of the excavator's platform, as indicated by the red ellipses. It is noteworthy that the optimized \our{} (our) rendering does not manifest these issues.
}
    \label{fig:blender_optim_nooptim}
\end{figure}

\paragraph{Ablation Study}

\our{} can be used with a classical Gaussian Splatting, but it is dedicated to model scenes with flat Gaussians. Therefore, an ablation study was performed to compare our approach with three GS initialization variants: 3DGS~\cite{kerbl20233d}, 2DGS~\cite{huang20242d}, and GaMeS~\cite{waczynska2024games}. 
All methods were trained to generate splats using RGB color representations, omitting spherical harmonics to ensure compatibility with mesh rendering. Following training, the resulting Gaussian components were converted into unoptimized mesh soup. For flat Gaussians (2DGS and GaMeS), meshes were constructed by approximating each Gaussian as a polygon with eight triangles ($no\_triag=8$) and a scale multiplier of 2.7 ($scale\_mul=2.7$). For 3DGS, the transformation extended this process to three orthogonal surfaces, producing 3D mesh representations. Subsequently, \our{} was optimized using the Nvdiffrast renderer, which facilitated differentiable rasterization and blending with depth peeling. The optimization pipeline employed a combined $L_1$ and SSIM loss function, with vertex positions and colors updated iteratively over 15 epochs. 


% Our model can be used with classical Gaussian Splatting but is dedicated to modeling, which uses flat Gaussians. Therefore, we compare our model with three GS initialization variants: 3DGS \cite{kerbl20233d}, 2DGS~\cite{huang20242d}, and GaMeS~\cite{waczynska2024games}. 
% All methods were trained to generate splats using RGB color representations, omitting spherical harmonics (SH) to ensure compatibility with mesh rendering. Following training, the resulting Gaussian components were converted into unoptimized \our{}. For flat Gaussians (2DGS and GaMeS), meshes were constructed by approximating each Gaussian as a polygon with eight triangles ($no\_triag=8$) and a scale multiplier of 2.7 ($scale\_mul=2.7$). For 3DGS, the transformation extended this process to three orthogonal surfaces, producing 3D mesh representations. Subsequently, \our{} was optimized using the Nvdiffrast renderer, which facilitated differentiable rasterization and blending with depth peeling. The optimization pipeline employed a combined $L_1$ and SSIM loss function, with vertex positions and colors updated iteratively over 15 epochs. 

The results of the ablation study are presented in Tab.~\ref{tab:scene_mip_tt_db}. When initialized with 3DGS, moderate performance degradation was observed on all datasets, due to the inherent complexity of fitting 3D Gaussians to mesh structures. For GaMeS, performance remained comparable to its RGB counterpart on Mip-NeRF360 and Deep Blending, indicating effective preservation of geometric fidelity despite the conversion.
On the other hand, when initialized with 2DGS, \our{} achieved superior PSNR on Deep Blending with reduced LPIPS, demonstrating improved reconstruction quality in complex indoor scenes. While slight degradations were observed in outdoor datasets, the results underscore the ability of our model to match or exceed its initialization methods in scenarios requiring fine-grained detail handling, despite its distinct mesh-based paradigm.

% The results of the ablation study are presented in Tab.~\ref{tab:scene_mip_tt_db}. When initialized with 3DGS, moderate declines in performance were observed across all datasets (e.g., Mip-NeRF360 SSIM: 0.873 → 0.809), attributed to the inherent complexity of adapting 3D Gaussians to mesh structures. For GaMeS, the performance remained comparable to its RGB counterpart on Mip-NeRF360 (PSNR: 28.62 → 28.08) and Deep Blending (SSIM: 0.897 → 0.883), indicating effective preservation of geometric fidelity despite the conversion.

% In particular, 2DGS initialized \our{} achieved superior PSNR in Deep Blending (29.08 29.50) along with reduced LPIPS (0.267 0.254), demonstrating improved reconstruction quality in complex indoor scenes. While slight declines were noted on outdoor datasets (e.g., Tanks \& Temples SSIM: 0.817 → 0.766), the results underscored \our{}’ capacity to match or exceed its initialization methods in scenarios requiring fine-grained detail handling, despite its distinct mesh-based paradigm.

In addition, the impact of the optimization process on \our{} was thoroughly evaluated. As demonstrated in Fig.~\ref{fig:nv_optim_nooptim}, the unoptimized model exhibited substantial floating objects (i.e., extraneous geometric artifacts) dispersed throughout the scene. Furthermore, high-frequency details, such as bicycle spokes and dense grass, were inadequately resolved or completely absent, leading to a diminution of geometric precision. During the optimization process, these limitations were addressed to a considerable extent. Specifically, floaters were removed, and fine structural elements, such as spokes and grass blades, were reconstructed with enhanced clarity. Similarly, in Fig.~\ref{fig:blender_optim_nooptim}, challenges in sharp edge preservation for unoptimized meshes, marked with red ellipses, were also highlighted. Following the optimization process, there was a notable enhancement in edge sharpness and geometric coherence, resulting in more accurate rendition of complex geometries.

% In addition, the impact of the optimization process on \our{} was rigorously evaluated. As illustrated in Fig.~\ref{fig:nv_optim_nooptim}, the unoptimized \our{} were observed to exhibit large floating objects - extraneous geometric artifacts - scattered throughout the scene. Moreover, high-frequency details, including bicycle spokes and dense grass, were poorly resolved or completely absent, resulting in a loss of geometric precision. During the optimization process, these limitations were addressed in a substantial way. Floaters were removed and fine structural elements, such as spokes and grass blades, were reconstructed with improved clarity. Similarly, Fig.~\ref{fig:blender_optim_nooptim} highlighted the challenges in sharp edge preservation for unoptimized meshes, marked with red ellipses. After optimization, edge sharpness and geometric coherence were markedly enhanced, with complex geometries rendered more faithfully.

In summary, the results of the ablation study highlighted the importance of the optimization pipeline in enhancing the initial mesh conversion process. While unoptimized \our{} provided a reasonable starting point, the post-optimization phase was essential for reducing artifacts, improving detail retention, and reaching parity with cutting-edge rendering techniques.

% These results underscored the necessity of the optimization pipeline in refining the initial mesh conversion. Although unoptimized \our{} provided a plausible baseline, the post-optimization process proved critical for mitigating artifacts, enhancing detail retention, and achieving parity with state-of-the-art rendering techniques.

\section{Conclusions}

In this paper, we introduce \our{}, a method that effectively overcomes the drawbacks of Gaussian Splatting (GS) by transforming it into a mesh-like structure, permitting its integration with ray tracing techniques. This adaptation enhances rendering quality by utilizing ray tracing's benefits, such as improved lighting, shadows, and reflections. Our approach offers an immediate, practical solution and promises further enhancement through the \our{} optimization algorithm. The findings from an extensive experimental study corroborate the efficacy and adaptability of our technique, validating its wide applicability across diverse datasets. Notably, \our{}, despite its distinct mesh-based representation, has exhibited the capacity to render photorealistic scenes, attaining visual quality that is comparable to (or even surpassing) that of Gaussian Splatting techniques. The elimination of floaters and the refinement of structural details post-optimization further underscore its aptitude to balance geometric accuracy with rendering fidelity. These qualities reinforce the potential of our approach as a robust alternative for high-quality rendering in computer graphics and image processing. 

\paragraph{Limitations} 

Two key limitations of our approach have been identified. First, artifacts---such as fragmented geometry and opacity inconsistencies---tend to occur when modeling expansive, low-texture regions (e.g., the solid-color skies in the Train scene from the Tanks and Temples dataset). This issue arises from the mesh-based opacity interpolation, which struggles to reproduce the smooth fall-off characteristic of Gaussian elements in uniform areas. It is noteworthy that this problem did not manifest in DeepBlending's indoor scenes featuring walls. Second, initialization with 3DGS produces densely tessellated meshes that lead to high vertex counts, thereby increasing memory usage and reducing rendering efficiency. Although the use of flat Gaussians (e.g., 2DGS, GaMeS) alleviates this issue, 3DGS-derived meshes remain computationally demanding for real-time applications.



\nocite{langley00}

%\bibliography{example_paper}
\bibliographystyle{icml2025}

\begin{thebibliography}{33}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Barron et~al.(2021)Barron, Mildenhall, Tancik, Hedman, Martin-Brualla, and Srinivasan]{barron2021mip}
Barron, J.~T., Mildenhall, B., Tancik, M., Hedman, P., Martin-Brualla, R., and Srinivasan, P.~P.
\newblock Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on computer vision}, pp.\  5855--5864, 2021.

\bibitem[Barron et~al.(2022)Barron, Mildenhall, Verbin, Srinivasan, and Hedman]{barron2022mip}
Barron, J.~T., Mildenhall, B., Verbin, D., Srinivasan, P.~P., and Hedman, P.
\newblock Mip-nerf 360: Unbounded anti-aliased neural radiance fields.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pp.\  5470--5479, 2022.

\bibitem[Barron et~al.(2023)Barron, Mildenhall, Verbin, Srinivasan, and Hedman]{barron2023zip}
Barron, J.~T., Mildenhall, B., Verbin, D., Srinivasan, P.~P., and Hedman, P.
\newblock Zip-nerf: Anti-aliased grid-based neural radiance fields.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pp.\  19697--19705, 2023.

\bibitem[Byrski et~al.(2025)Byrski, Mazur, Tabor, Dziarmaga, Kadziolka, Baran, and Spurek]{byrski2025raysplats}
Byrski, K., Mazur, M., Tabor, J., Dziarmaga, T., Kadziolka, M., Baran, D., and Spurek, P.
\newblock Raysplats: Ray tracing based gaussian splatting.
\newblock \emph{arXiv preprint arXiv:2501.19196}, 2025.

\bibitem[Chen et~al.(2022)Chen, Xu, Geiger, Yu, and Su]{chen2022tensorf}
Chen, A., Xu, Z., Geiger, A., Yu, J., and Su, H.
\newblock Tensorf: Tensorial radiance fields.
\newblock In \emph{European conference on computer vision}, pp.\  333--350. Springer, 2022.

\bibitem[Foley et~al.(1994)Foley, Van~Dam, Feiner, Hughes, and Phillips]{foley1994introduction}
Foley, J.~D., Van~Dam, A., Feiner, S.~K., Hughes, J.~F., and Phillips, R.~L.
\newblock \emph{Introduction to computer graphics}, volume~55.
\newblock Addison-Wesley Reading, 1994.

\bibitem[Fridovich-Keil et~al.(2022)Fridovich-Keil, Yu, Tancik, Chen, Recht, and Kanazawa]{fridovich2022plenoxels}
Fridovich-Keil, S., Yu, A., Tancik, M., Chen, Q., Recht, B., and Kanazawa, A.
\newblock Plenoxels: Radiance fields without neural networks.
\newblock In \emph{CVPR}, pp.\  5501--5510, 2022.

\bibitem[Govindarajan et~al.(2025)Govindarajan, Rebain, Yi, and Tagliasacchi]{govindarajan2025radiant}
Govindarajan, S., Rebain, D., Yi, K.~M., and Tagliasacchi, A.
\newblock Radiant foam: Real-time differentiable ray tracing.
\newblock \emph{arXiv preprint arXiv:2502.01157}, 2025.

\bibitem[Gu et~al.(2024)Gu, Wei, Zeng, Yao, and Zhang]{gu2024irgs}
Gu, C., Wei, X., Zeng, Z., Yao, Y., and Zhang, L.
\newblock Irgs: Inter-reflective gaussian splatting with 2d gaussian ray tracing.
\newblock \emph{arXiv preprint arXiv:2412.15867}, 2024.

\bibitem[Gu{\'e}don \& Lepetit(2024)Gu{\'e}don and Lepetit]{guedon2023sugar}
Gu{\'e}don, A. and Lepetit, V.
\newblock Sugar: Surface-aligned gaussian splatting for efficient 3d mesh reconstruction and high-quality mesh rendering.
\newblock \emph{CVPR}, 2024.

\bibitem[Hedman et~al.(2018)Hedman, Philip, Price, Frahm, Drettakis, and Brostow]{hedman2018deep}
Hedman, P., Philip, J., Price, T., Frahm, J.-M., Drettakis, G., and Brostow, G.
\newblock Deep blending for free-viewpoint image-based rendering.
\newblock \emph{ACM Transactions on Graphics (ToG)}, 37\penalty0 (6):\penalty0 1--15, 2018.

\bibitem[Huang et~al.(2024)Huang, Yu, Chen, Geiger, and Gao]{huang20242d}
Huang, B., Yu, Z., Chen, A., Geiger, A., and Gao, S.
\newblock 2d gaussian splatting for geometrically accurate radiance fields.
\newblock In \emph{ACM SIGGRAPH 2024 conference papers}, pp.\  1--11, 2024.

\bibitem[Jiang et~al.(2024)Jiang, Tu, Liu, Gao, Long, Wang, and Ma]{jiang2024gaussianshader}
Jiang, Y., Tu, J., Liu, Y., Gao, X., Long, X., Wang, W., and Ma, Y.
\newblock Gaussianshader: 3d gaussian splatting with shading functions for reflective surfaces.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  5322--5332, 2024.

\bibitem[Kerbl et~al.(2023)Kerbl, Kopanas, Leimk{\"u}hler, and Drettakis]{kerbl20233d}
Kerbl, B., Kopanas, G., Leimk{\"u}hler, T., and Drettakis, G.
\newblock 3d gaussian splatting for real-time radiance field rendering.
\newblock \emph{ACM Trans. Graph.}, 42\penalty0 (4):\penalty0 139--1, 2023.

\bibitem[Knapitsch et~al.(2017)Knapitsch, Park, Zhou, and Koltun]{knapitsch2017tanks}
Knapitsch, A., Park, J., Zhou, Q.-Y., and Koltun, V.
\newblock Tanks and temples: Benchmarking large-scale scene reconstruction.
\newblock \emph{ACM Transactions on Graphics (ToG)}, 36\penalty0 (4):\penalty0 1--13, 2017.

\bibitem[Ma et~al.(2024)Ma, Agrawal, Turki, Kim, Gao, Sander, Zollh{\"o}fer, and Richardt]{ma2024specnerf}
Ma, L., Agrawal, V., Turki, H., Kim, C., Gao, C., Sander, P., Zollh{\"o}fer, M., and Richardt, C.
\newblock Specnerf: Gaussian directional encoding for specular reflections.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  21188--21198, 2024.

\bibitem[Mildenhall et~al.(2021)Mildenhall, Srinivasan, Tancik, Barron, Ramamoorthi, and Ng]{mildenhall2021nerf}
Mildenhall, B., Srinivasan, P.~P., Tancik, M., Barron, J.~T., Ramamoorthi, R., and Ng, R.
\newblock Nerf: Representing scenes as neural radiance fields for view synthesis.
\newblock \emph{Communications of the ACM}, 65\penalty0 (1):\penalty0 99--106, 2021.

\bibitem[Moenne-Loccoz et~al.(2024)Moenne-Loccoz, Mirzaei, Perel, de~Lutio, Martinez~Esturo, State, Fidler, Sharp, and Gojcic]{moenne20243d}
Moenne-Loccoz, N., Mirzaei, A., Perel, O., de~Lutio, R., Martinez~Esturo, J., State, G., Fidler, S., Sharp, N., and Gojcic, Z.
\newblock 3d gaussian ray tracing: Fast tracing of particle scenes.
\newblock \emph{ACM Transactions on Graphics (TOG)}, 43\penalty0 (6):\penalty0 1--19, 2024.

\bibitem[M{\"u}ller et~al.(2022)M{\"u}ller, Evans, Schied, and Keller]{muller2022instant}
M{\"u}ller, T., Evans, A., Schied, C., and Keller, A.
\newblock Instant neural graphics primitives with a multiresolution hash encoding.
\newblock \emph{ACM Transactions on Graphics (ToG)}, 41\penalty0 (4):\penalty0 1--15, 2022.

\bibitem[Munkberg et~al.(2022)Munkberg, Hasselgren, Shen, Gao, Chen, Evans, M{\"u}ller, and Fidler]{munkberg2022extracting}
Munkberg, J., Hasselgren, J., Shen, T., Gao, J., Chen, W., Evans, A., M{\"u}ller, T., and Fidler, S.
\newblock Extracting triangular 3d models, materials, and lighting from images.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  8280--8290, 2022.

\bibitem[Parker et~al.(2010)Parker, Bigler, Dietrich, Friedrich, Hoberock, Luebke, McAllister, McGuire, Morley, Robison, et~al.]{parker2010optix}
Parker, S.~G., Bigler, J., Dietrich, A., Friedrich, H., Hoberock, J., Luebke, D., McAllister, D., McGuire, M., Morley, K., Robison, A., et~al.
\newblock Optix: a general purpose ray tracing engine.
\newblock \emph{Acm transactions on graphics (tog)}, 29\penalty0 (4):\penalty0 1--13, 2010.

\bibitem[Peddie(2019)]{raytracing2019}
Peddie, J.
\newblock \emph{Ray Tracing: A Tool for All}.
\newblock Springer Publishing Company, Incorporated, 1st edition, 2019.
\newblock ISBN 3030174891.

\bibitem[Tang \& Cham(2025)Tang and Cham]{tang20253igs}
Tang, Z.~J. and Cham, T.-J.
\newblock 3igs: Factorised tensorial illumination for 3d gaussian splatting.
\newblock Linprim: Linear primi
\newblock In \emph{European Conference on Computer Vision}, pp.\  143--159. Springer, 2025.

\bibitem[Verbin et~al.(2022)Verbin, Hedman, Mildenhall, Zickler, Barron, and Srinivasan]{verbin2022ref}
Verbin, D., Hedman, P., Mildenhall, B., Zickler, T., Barron, J.~T., and Srinivasan, P.~P.
\newblock Ref-nerf: Structured view-dependent appearance for neural radiance fields.
\newblock In \emph{2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pp.\  5481--5490. IEEE, 2022.

\bibitem[Verbin et~al.(2024)Verbin, Srinivasan, Hedman, Mildenhall, Attal, Szeliski, and Barron]{verbin2024nerf}
Verbin, D., Srinivasan, P.~P., Hedman, P., Mildenhall, B., Attal, B., Szeliski, R., and Barron, J.~T.
\newblock Nerf-casting: Improved view-dependent appearance with consistent reflections.
\newblock In \emph{SIGGRAPH Asia 2024 Conference Papers}, pp.\  1--10, 2024.

\bibitem[von L{\"u}tzow \& Nie{\ss}ner(2025)von Lutzow and Nie{\ss}ner]{von2025linprim}
von L{\"u}tzow, N. and Nie{\ss}ner, M.tives for differentiable volumetric rendering.
\newblock \emph{arXiv preprint arXiv:2501.16312}, 2025.

\bibitem[Waczy{\'n}ska et~al.(2024)Waczy{\'n}ska, Borycki, Tadeja, Tabor, and Spurek]{waczynska2024games}
Waczy{\'n}ska, J., Borycki, P., Tadeja, S., Tabor, J., and Spurek, P.
\newblock Games: Mesh-based adapting and modification of gaussian splatting.
\newblock \emph{arXiv preprint arXiv:2402.01459}, 2024.

\bibitem[Wang et~al.(2004)Wang, Bovik, Sheikh, and Simoncelli]{wang2004image}
Wang, Z., Bovik, A.~C., Sheikh, H.~R., and Simoncelli, E.~P.
\newblock Image quality assessment: from error visibility to structural similarity.
\newblock \emph{IEEE transactions on image processing}, 13\penalty0 (4):\penalty0 600--612, 2004.

\bibitem[Weyrich et~al.(2007)Weyrich, Heinzle, Aila, Fasnacht, Oetiker, Botsch, Flaig, Mall, Rohrer, Felber, et~al.]{weyrich2007hardware}
Weyrich, T., Heinzle, S., Aila, T., Fasnacht, D.~B., Oetiker, S., Botsch, M., Flaig, C., Mall, S., Rohrer, K., Felber, N., et~al.
\newblock A hardware architecture for surface splatting.
\newblock \emph{ACM Transactions on Graphics (TOG)}, 26\penalty0 (3):\penalty0 90--es, 2007.

\bibitem[Xie et~al.(2024)Xie, Chen, Xu, Xie, Jin, Shen, Peng, Bao, and Zhou]{xie2024envgs}
Xie, T., Chen, X., Xu, Z., Xie, Y., Jin, Y., Shen, Y., Peng, S., Bao, H., and Zhou, X.
\newblock Envgs: Modeling view-dependent appearance with environment gaussian.
\newblock \emph{arXiv preprint arXiv:2412.15215}, 2024.

\bibitem[Ye et~al.(2024)Ye, Hou, and Zhou]{ye20243d}
Ye, K., Hou, Q., and Zhou, K.
\newblock 3d gaussian splatting with deferred reflection.
\newblock In \emph{ACM SIGGRAPH 2024 Conference Papers}, pp.\  1--10, 2024.

\bibitem[Yu et~al.(2024)Yu, Chen, Huang, Sattler, and Geiger]{yu2024mip}
Yu, Z., Chen, A., Huang, B., Sattler, T., and Geiger, A.
\newblock Mip-splatting: Alias-free 3d gaussian splatting.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  19447--19456, 2024.

\bibitem[Zhang et~al.(2018)Zhang, Isola, Efros, Shechtman, and Wang]{zhang2018unreasonable}
Zhang, R., Isola, P., Efros, A.~A., Shechtman, E., and Wang, O.
\newblock The unreasonable effectiveness of deep features as a perceptual metric.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pp.\  586--595, 2018.

\end{thebibliography}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
