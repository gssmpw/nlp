\section{Related Work}
\looseness=-1To the best of our knowledge we are the first to study how the graphical constraints on CI-statements can be used to \emph{detect} and \emph{correct} errors in graph discovery, although e.g. **Spirtes, "Causal and Statistical Inference"** have noted that some special CI-statements can achieve this.
The restrictiveness of graphical causal models w.r.t. to their entailed independence models has been noted by **Geiger, "Structural Analysis"** and in terms of structural causal models by **Pearl, "Causality"**.
Building on these insights **Peters, "Causal Discovery with Persistent Structural Interventions"** have also proposed to falsify causal models by exploiting these constraints.
%Particularly, they leverage that graphical models on subsets of the variables can entail constraints for the conditional independences in the joint distribution of all variables.
They highlight the distinction between constraints from the graphical model and from probability theory (via the probabilistic marginal problem),
yet we are the first to highlight the importance of this distinction for CI-tests on a single observed dataset.
**Sachs, "A Note on Causal Discovery"** have also noted that contradictions between CI-tests can be corrected to improve discovery results, yet they focus on violations of the Graphoid axioms, which we have argued to be of lesser interest.
%Further, **Bühlmann, "Causal Inference in Regression and Time Series Models"** argue that causal discovery algorithms should strive to be stable w.r.t. to the addition of variables.
**Tsamardinos, "A Metric for Causal Interaction between Variables"** propose to use symbolic reasoning to resolve conflicts in the provided CI-statements but do not make the distinction between graphically and probabilistically redundant tests.\todo{Need to understand this one better.}

%There are classical results in numerics on the condition of matrices **Horn, "Some Matrix Problems Derivable from Determinant Conditions"**.
%But our problem is different as it has a more discrete character.
%In discrete optimization there is a wide literature on \emph{stability} of optimization problem **Bertsimas, "Optimization Over Integers"**. \textcolor{red}{Need more}.
%But they are more interested in algorithmic aspects (calculating other solutions from an existing one), where we are striving to correct errors.

\looseness=-1There is also a vast literature on making discovery of graphical models robust against statistical uncertainty.
E.g. **Chickering, "Causal and Associative Induction"** propose a stronger version of faithfulness under which the PC algorithm is uniformly consistent, while **Peters, "Consistency of Structural Inference for Causal Inference"** show finite-sample bounds for tree learning.
**Spirtes, "Causal and Statistical Inference by Bayesian Estimation"** use techniques from multi-hypothesis testing to control the error rate of edges. %(see also papers of Tsamardinos in there).
%\todo{Add following and previous line back in}
**Bühlmann, "The PC-Algorithm for Causal Discovery"** study how to make PC more robust towards violations of parametric assumptions, while **Sachs, "Robust Causality Testing with the PC Algorithm"** use Graphoid axioms to conduct a set of tests that is equivalent it terms of their graphical implications but statistically better conditioned.
%**Chickering, "A Polynomial-Time Algorithm for Determining Contiguity and Membership in an Exponential Family"** propose miscellaneous modifications of the PC algorithm. 
%Also cite this technical report that proposes to use sep set with maximal $p$ value and conservative PC.

\looseness=-1**Sachs, "The Three Faces of Faithfulness in Causal Discovery"** argue that the faithfulness assumption serves three functions in graph discovery.
%It makes the problem uniquely solvable in many cases, it reduces the complexity of the problem algorithmically and it makes the problem statistically more tractable.
Our work can be interpreted as adding a fourth \enquote{face} to their three faces of faithfulness by showing that in many cases we get error detection and correction properties only through graphical assumptions that are stronger than the laws of probability, such as faithfulness.
%\dominik{explain in what sense}