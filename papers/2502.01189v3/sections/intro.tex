\begin{figure*}[t]
    \centering
    \includegraphics[width=1\textwidth]{figures/teaser.pdf}
    \caption{Our proposed scheme (DDCM) produces visually appealing image samples with high compression ratios (bottom-right corners).}
    \label{fig:teaser}
\end{figure*}

\section{Introduction}\label{sec:intro}

Denoising Diffusion Models (DDMs)~\citep{sohl2015deep, ho2020denoising} have emerged as an effective tool for generating samples from complex signal distributions (e.g., natural images).
Hence, DDMs are commonly leveraged to facilitate a variety of downstream tasks, such as text-to-image synthesis~\cite{ramesh2021zero,rombach2022high,saharia2022photorealistic},
editing~\citep{meng2022sdedit,huberman2024edit}, compression~\cite{theis2022lossy, elata2024zero, korber2024perco}, and restoration~\citep{kawar2022denoising, chung2023diffusion}. 
Common to many of these applications is the reliance on iterative sampling from a continuous Gaussian distribution, yielding an unbounded representation space.


This work embarks on the hypothesis that such an infinite representation space is highly redundant.
For example, consider any stochastic diffusion generative process with $T=1000$ sampling steps (e.g., DDPM~\citep{ho2020denoising}).
Suppose that at each timestep, the generative process is restricted to choosing between only two fixed noise realizations.
Sampling could then lead to $2^{1000}$ different outputs, an incredibly large number exceeding the estimated amount of atoms in the universe.
Thus, in principle, such a process could cover the distribution of natural images densely.



We harness this intuition and propose \emph{Denoising Diffusion Codebook Models} (DDCM), 
a novel DDM generation scheme for continuous signals, leveraging a discrete and finite representation space. 
In particular, we first construct a chain of \emph{codebooks}, where each is a sequence of pre-sampled Gaussian noise vectors.
These codebooks are constructed once and remain fixed for the entire lifetime of the model.
Then, during the generative process, we simply randomly pick the noises from the codebooks instead of drawing them from a Gaussian distribution, as shown in \Cref{fig:overview}.
Since we alter only the sampling process, DDCM can be applied using any pre-trained DDM.
Interestingly, we find that our proposed discrete and finite representation space is indeed expressive enough to retain the generative capabilities of standard DDMs, even when using incredibly small codebooks.
Since our generative process is entirely governed by the noise \emph{indices} picked during the generation, an important consequence is that every generated image can be perfectly reconstructed by repeating the process with its corresponding indices.

We leverage this property to solve a variety of tasks, using gradient-free noise selection rules to guide the DDCM generation process.
In particular, by choosing the discrete noises to best match a given image, we achieve state-of-the-art perceptual compression results.
Moreover, using DDCM with different noise selection rules yields a versatile framework for other \emph{compressed} conditional generation tasks, such as compressed image restoration (see examples in \Cref{fig:teaser}).
Finally, we provide a mathematical interpretation of the proposed schemes based on score-based generative modeling with SDEs~\citep{song2020score}, showing a connection between our generalized selection rules and approximate posterior sampling for compressed conditional generation.



\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/overview.pdf}
    \caption{\textbf{Method overview.} 
    DDCM replaces the standard Gaussian noises in DDPM sampling with a selection of noise samples from pre-defined codebooks of fixed iid Gaussian vectors.
    This retains the high-quality generative properties of standard DDMs, while producing the results along with their compressed representations.
    By choosing the discrete noises according to different selection rules, DDCM can perform a variety of conditional image generation tasks. 
    Our highly condensed bit-stream representation is especially effective for image compression, leading to state-of-the-art results.
    }
    \label{fig:overview}
\end{figure*}

