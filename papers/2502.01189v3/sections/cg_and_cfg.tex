

\subsection{Compressed Classifier Guidance}\label{appendix:classifier-guidance}
\begin{figure*}[t]
    \centering
    \includegraphics[height=15cm]{figures/our_cg_256noises_2optnoises.jpg}\hspace{1cm}\includegraphics[height=15cm]{figures/standard_cg_scale=20.jpg}
    \caption{\textbf{Qualitative comparison of CCG (left) with CG (right).} CCG achieves superior image quality compared to CG while avoiding the use of classifier gradients. Additionally, CCG enables decompression without requiring access to the original class labels.}
    \label{fig:cg_visual_comparison}
\end{figure*}
Consider the case where $\rvy$ represents the \emph{class} of an image $\rvx_{0}$.
An unconditional score-based generative model can be \emph{guided} to generate samples the posterior $p_{0}(\rvx_{0}|\rvy)$, by perturbing the generated samples according to the gradient of a time-dependent trained classifier $c_{\theta}(\rvy;\rvx_{i},i)\approx p_{i}(\rvy|\rvx_{i})$~\citep{dhariwal2021diffusion}.
This approach is known as classifier guidance (CG).
Such a guidance method can be interpreted as an attempt to \emph{confuse} the classifier by perturbing its input adversarially~\citep{ho2021classifier}.
However, trained classifiers are typically not robust to adversarial perturbations, making their gradients largely unreliable and unaligned with human perception~\citep{advattacks,tsipras2018robustness,ganz-perceptual}.
Thus, the standard CG approach has not seen major success~\citep{ho2021classifier}.


We propose an alternative to this method, circumventing the reliance on the classifier's gradient.
Specifically, we set $\gL$ in \Cref{eq:k_choose_conditional} as
\begin{align}
\gL(\rvy,\rvx_{i},\gC_{i},k)=-\log{c_{\theta}(\rvy;\vmu_{i}(\rvx_{i})+\sigma_{i}\gC_{i}(k),i)}.\label{eq:classifier-guidance-ours}
\end{align}
Thus, $\gL(\rvy,\rvx_{i},\gC_{i},k)$ attains a lower value when $\sigma_{i}\gC_{i}(k)$ points in some direction that maximizes the probability of the class $\rvy$.
Note that since the codebooks remain fixed, choosing $k_{i}$ (out of $1,\hdots,K$) to minimize~\Cref{eq:k_choose_conditional} would always lead to the same generated sample for every $\rvy$.
Thus, we promote sample diversity by first \emph{randomly} selecting a subset of $\tilde{K}<K$ indices $k_{i,1},\hdots,k_{i,\tilde{K}}\sim\text{Unif}(\{1,\hdots,K\})$, and then choosing
\begin{align}
    k_{i}=\argmin_{k\in\{k_{i,1},\hdots,k_{i,\tilde{K}}\}}\gL(\rvy,\rvx_{i},\gC_{i},k).
\end{align}
We coin our method Compressed CG (CCG).


We compare our proposed CCG with the standard CG using the same unconditional diffusion model and time-dependent classifier trained on ImageNet $256\times256$~\citep{deng2009imagenet,dhariwal2021diffusion}.
We compare the methods ``on the same grounds'' by using the same standard DDPM noise schedule and $T=1000$ diffusion steps.
Our method is assessed with $K=256$ and $\tilde{K}=2$, while the standard CG is assessed with CG scales $s\in\{1,10,20\}$.
The quantitative comparison in~\Cref{tab:cg_comparison} shows that CCG achieves better (lower) FID and $\text{FD}_{\text{DINOv2}}$ scores.
A visual comparison is provided in~\Cref{fig:cg_visual_comparison}.
Note that while using DDCM with standard CG does still produces compressed output images, decoding the produced bit-streams requires access to $\rvy$.
Using DDCM with CCG instead sidesteps this limitation, as $\rvy$ is not needed for decompression.

\input{tables/cg_comparison}

\clearpage
\subsection{Compressed Classifier-Free Guidance}\label{appendix:classifier-free-guidance}
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/ccfg_fig_1.pdf}
    \caption{\textbf{Qualitative comparison of CCFG with CFG.} CCFG achieves comparable image quality and diversity to CFG, while enabling decompression without requiring the original inputs.}
    \label{fig:ccfg_visual_1}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/ccfg_fig_2.pdf}
    \caption{\textbf{Qualitative comparison of CCFG with CFG.} CCFG achieves comparable image quality and diversity to CFG, while enabling decompression without requiring the original inputs.}
    \label{fig:ccfg_visual_2}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/ccfg_cfg_comparison.pdf}
    \caption{\textbf{Quantitative evaluation of CCFG and CFG.} CCFG achieves comparable FID scores to CFG while achieving slightly lower fidelity to the input prompts. However, unlike CFG, CCFG enables decompression without access to the original conditioning inputs.}
    \label{fig:ccfg-cf-quantitative-comparison}
\end{figure*}
The task of text-conditional image generation can be solved using a \emph{conditional} diffusion model, which, theoretically speaking, learns to sample from the posterior distribution $p_{0}(\rvx_{0}|\rvy)$.
In practice, however, using a conditional model directly typically yields low fidelity to the inputs.
To address this limitation, CG can be used to improve this fidelity at the expense of sample quality and diversity~\citep{dhariwal2021diffusion}.
Classifier-Free Guidance (CFG) is used more often in practice, as it achieves the same tradeoff by mixing the conditional and unconditional scores during sampling~\citep{ho2021classifier}, thus eliminating the need for a classifier.
Particularly, assuming we have access to both the conditional score $\vs_{i}(\rvx_{i},\rvy)\coloneqq\nabla_{\rvx_{i}}\log{p_{i}(\rvx_{i}|\rvy)}$ and the unconditional one $\vs_{i}(\rvx_{i})$, CFG proposes to modify the conditional score by
\begin{align}
    \tilde{\vs}_{i}(\rvx_{i},\rvy)=(1+w)\vs_{i}(\rvx_{i},\rvy)-w\vs_{i}(\rvx_{i}),
\end{align}
where $w$, the CFG scale, is a hyper-parameter controlling the tradeoff between sample quality and diversity.

Here, we introduce a new CFG method that allows generating compressed conditional samples using any pair of conditional and unconditional diffusion models, while controlling the tradeoff between generation quality and the fidelity to the inputs.
Specifically, since $\nabla_{\rvx_{i}}\log{p_{i}(\rvy|\rvx_{i})}=\vs_{i}(\rvx_{i}|\rvy)-\vs_{i}(\rvx_{i})$, we simply use
\begin{align}
    \gL(\rvy,\rvx_{i},\gC_{i},k)=-\langle\gC_{i}(k),\vs_{i}(\rvx_{i}|\rvy)-\vs_{i}(\rvx_{i})\rangle.\label{eq:l_ccfg}
\end{align}
Note that optimizing~\Cref{eq:l_ccfg} is roughly  equivalent to optimizing $\gL_{\text{P}}$ when $\rvx_{i}$ is high dimensional (see~\Cref{appendix:compression_private_case}).
As in~\Cref{appendix:classifier-guidance}, we promote sample diversity by choosing $k_{i}$ from a randomly sampled subset of $\tilde{K}<K$ indices at each step during the generation.
We coin our method Compressed CFG (CCFG).

We implement our method using SD 2.1 trained on $768\times768$ images, adopting a DDPM noise schedule with $T=1000$ diffusion steps, $K=64$ fixed vectors in each codebook and $\tilde{K}\in\{2,3,4,6,9\}$.
We compare against the same diffusion model with standard DDPM sampling, using $T=1000$ steps and CFG scales $w\in\{2,5,8,11\}$.
The generative performance of both methods is assessed by computing the FID between 10k generated samples and MS-COCO, similarly to \Cref{sec:method}.
Additionally, we evaluate the alignment between the outputs and the input text prompts using the CLIP score~\citep{hessel2021clipscore} with the OpenAI CLIP ViT-L/14 model~\citep{pmlr-v139-radford21a}.

Figure~\ref{fig:ccfg-cf-quantitative-comparison} shows that our CCFG method is on par with CFG in terms of FID, while CFG produces higher CLIP scores.
This suggests that the outputs of CFG better align with the input text prompts compared to CCFG.
Yet, the qualitative comparisons in \Cref{fig:ccfg_visual_1,fig:ccfg_visual_2} show that there is no significant difference between the methods.
Importantly, decoding the bit-streams produced by CCFG does involve accessing the original input $\rvy$, and so our loss in CLIP scores are expected due to the rate-perception-distortion tradeoff~\citep{pmlr-v97-blau19a} (here, we achieve $\frac{1000\cdot\log_{2}(64)}{768^2}\approx 0.01$ BPP).
Note that using CCFG in DDCM is fundamentally different than using CFG (\Cref{sec:method}), since the latter requires access to $\rvy$.