\section{Compressed Conditional Generation}\label{sec:compressed_conditional_generation}

We showed that DDCM can be used as an image codec by following a simple index selection rule, guiding the generated image towards a target one.
Here, we generalize this scheme to any \emph{conditional} generation task, considering the more broad framework of \emph{compressing} conditionally generated samples.
This is a particularly valuable framework in scenarios where the input condition $\rvy$ is bit rate intensive, e.g., where $\rvy$ is a degraded image and the goal is to produce a \emph{compressed} high-quality reconstruction of it.
To the best of our knowledge, this task, which we name \emph{compressed} conditional generation, has only been thoroughly explored for text input conditions~\citep{bulla2023}.

A naive solution to this task could be to simply compress the outputs of any existing conditional generation scheme.
Here we propose a novel end-to-end solution that generates the outputs \emph{directly} in their compressed form.
Importantly, note that our approach in \Cref{sec:method} requires the condition $\rvy$ for decompressing the bit-stream.
While this is not a stringent requirement when the condition is lightweight (e.g., a text prompt), this approach is less suitable when storage of the condition signal itself requires a long bit-stream. 
The solutions we propose in this section enable decoding the bit-stream without access to $\rvy$.


\paragraph{Compressed Conditional Generation with DDCM.}We propose generating a conditional sample by choosing the indices $k_{i}$ in \Cref{eq:DDCM_sampling} via
\begin{align}
k_{i}=\argmin_{k\in\{1,\hdots,K\}}\gL(\rvy,\rvx_{i},\gC_{i},k),\label{eq:k_choose_conditional}
\end{align}
instead of picking them randomly.
Here, $\gL(\rvy,\rvx_{i},\gC_{i},k)$ can be any loss function that attains a lower value when $\gC_{i}(k)$ 
directs the generative process towards an image that matches $\rvy$.
For example, for the loss
\begin{align}
\gL_{\text{P}}(\rvy,\rvx_{i},\gC_{i},k)=\norm{\gC_{i}(k)-\sigma_{i}\nabla_{\rvx_{i}}\log{p_{i}(\rvy|\rvx_{i})}}^{2}\label{eq:l_score}
\end{align}
we obtain the following result (see proof in~\cref{appendix:cond_compression}):
\begin{proposition}\label{prob:ode_convergence}
Suppose that image samples are generated via process~\cref{eq:DDCM_sampling}, and the indices $k_{i}$ are chosen according to~\Cref{eq:k_choose_conditional} with $\gL=\gL_{\textnormal{P}}$.
Then, when $K\rightarrow\infty$, such a generative process becomes a discretization of a probability flow ODE over the posterior distribution $p_{0}(\rvx_{0}|\rvy)$.
\end{proposition}
In other words,~\Cref{prob:ode_convergence} implies that for the loss $\gL_{\text{P}}$, increasing $K$ leads to more accurate sampling from the posterior $p_{0}(\rvx_{0}|\rvy)$, though this results in longer bit-streams.
Thus, as long as we have access to $\nabla_{\rvx_{i}}\log{p_{i}(\rvy|\rvx_{i})}$ (or an  approximation of it) $\gL_{\text{P}}$ may serve as a sensible option for solving a compressed conditional generation task with DDCM.
Interestingly, we show in~\Cref{appendix:compression_private_case} that our compression scheme from~\Cref{section:compression} is a special case of the proposed compressed conditional generation method, with $\rvy=\rvx_{0}$ and $\gL=\gL_{\text{P}}$.

\subsection{Compressed Posterior Sampling for Image Restoration}\label{sec:zero-shot-restoration}


\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/posterior_sampling.pdf}
    \includegraphics{figures/linear_restoration_main_text_v2.pdf}
    \caption{\textbf{Comparison of zero-shot posterior sampling image restoration methods.} Our approach achieves better perceptual quality compared to previous methods, while maintaining competitive PSNR and automatically producing compressed bit-stream representations for each restored image.}
    \label{fig:zero-shot-posterior-sampling-qualitative}
\end{figure*}

Our compressed conditional sampling approach can be utilized for solving inverse problems via posterior sampling.
Specifically, we consider inverse problems of the form $\rvy=\mA\rvx_{0}$, where $\mA$ is some linear operator.
We restrict our attention to \emph{unconditional} diffusion models and solve the problems in a ``zero-shot'' manner (similarly to \citet{kawar2022denoising,chung2023diffusion,wang2023zeroshot}). 
To generate conditional samples, we propose using the loss
\begin{align}
\gL(\rvy,\rvx_{i},\gC_{i},k)=\norm{\rvy-\mA(\vmu_{i}(\rvx_{i})+\sigma_{i}\gC_{i}(k))}^{2}.\label{eq:l_posterior_sampling}
\end{align}
Note that~\Cref{eq:l_posterior_sampling} attains a lower value when $\sigma_{i}\mA\gC_{i}(k)$ points in the direction that perturbs $\mA\vmu_{i}(\rvx_{i})$ towards $\rvy$.
Thus, our conditional generative process aims to produce a reconstruction $\hat{\rvx}$ that satisfies $\mA\hat{\rvx}\approx\rvy$, implying that we approximate posterior sampling~\citep{pmlr-v202-ohayon23a}.
Notably, when assuming that $p_{i}(\rvy|\rvx_{i})$ is a multivariate normal distribution centered around $\mA\rvx_{i}$ (as in~\citep{jalal2021posterior}), the chosen codebook noise $\gC_{i}(k_{i})$ approximates the gradient $\nabla_{\rvx_{i}}\log{p_{i}(\rvy|\rvx_{i})}$ and~\Cref{eq:l_posterior_sampling} becomes a proxy of~\Cref{eq:l_score}.

Following~\citep{chung2023diffusion,wang2023zeroshot}, we implement our method using the unconditional ImageNet $256\times 256$ DDM trained by \citet{dhariwal2021diffusion}.
We fix $K=4096$ for all codebooks, resulting in a compressed bit-stream of approximately $0.183$ BPP for each generated image.
We compare our method with DPS~\citep{chung2023diffusion} and DDNM~\citep{wang2023zeroshot} on two noiseless tasks: colorization and $4\times $ super-resolution (using the bicubic kernel). We evaluate these methods using their official implementations and the same DDM.
We additionally compress the outputs of DPS and DDNM to assess whether such a naive approach would yield better results.
To do so, we adopt our proposed compression scheme (from \Cref{section:compression}), employing the same unconditional ImageNet DDM and using $K=4096$ noises per codebook.


Qualitative and quantitative results are reported in \Cref{fig:zero-shot-posterior-sampling-qualitative}.
As expected, due to the rate-perception-distortion tradeoff~\citep{pmlr-v97-blau19a}, we observe that compressing the outputs of DPS and DDNM harms either their perceptual quality (FID), or their distortion (PSNR), or both.
This is while our method achieves superior perceptual quality compared to both DPS and DDNM, including their compressed versions.
While our method achieves slightly worse PSNR, this is expected due to the perception-distortion trade-off~\citep{Blau_2018_CVPR}.
See \Cref{appendix:zero-shot} for more details.





\subsection{Compressed Real-World Face Image Restoration}\label{sec:bfr}
\begin{figure*}[t]
    \centering
    \includegraphics[width=1\textwidth]{figures/blind_face_restoration/real-world-restoration-wider.pdf}
\includegraphics[width=1\textwidth]{figures/blind_face_restoration/blind_face_restoration_main_text_wider.pdf}
    \caption{\textbf{Comparing real-world face image restoration methods on the WIDER-Test dataset}. We successfully optimize the NR-IQA measures and produce appealing output perceptual quality with less artifacts compared to previous methods.}
    \label{fig:real-world-wider-visual}
\end{figure*}
Real-world face image restoration is the practical task of restoring any degraded face image, without any knowledge of the corruption process it has gone through~\citep{wang2021gfpgan,vqfr,wang2022restoreformer,zhou2022codeformer,wang2023restoreformer++,2023diffbir,difface,bfrfussion,pmrf}.
We propose a novel method capable of optimizing any no-reference image quality assessment (NR-IQA) measure at test time (e.g., NIQE~\citep{niqe}), without relying on gradients.

Specifically, at each timestep $i$, we start by picking two indices -- one that promotes high perceptual quality, $k_{i,P}$, and another that promotes low distortion, $k_{i,D}$.
Then, we choose between $k_{i,P}$ and $k_{i,D}$ the index that better optimizes a desired balance of the perception-distortion tradeoff~\citep{Blau_2018_CVPR}.
Formally, letting $\vr(\rvy)\approx\mathbb{E}[\rvx_{0}|\rvy]$ denote the approximate Minimum Mean-Squared-Error (MMSE) estimator of this task, we pick $k_{i,D}$ via
\begin{align}
    k_{i,D}=\argmax_{k\in\{1,\hdots,K\}}\langle\gC_{i}(k),\vr(\rvy)-\hat{\rvx}_{0|i}\rangle.\label{eq:first_index_blind_face_restoration}
\end{align}
Note that this index selection rule is similar to that of our standard compression, replacing $\smash{\rvx_{0}}$ in \Cref{eq:compression_rule} with $\smash{\vr(\rvy)}$.
This choice of indices in DDCM would lead to a reconstructed estimate of the MMSE solution $\vy(\rvy)$, yielding blurry results with low distortion~\citep{Blau_2018_CVPR}.
In contrast, \emph{randomly} picking a sequence of indices in DDCM would produce a high quality sample from the data distribution $p_{0}$.
Therefore, we randomly choose $\smash{k_{i,P}\sim \text{Unif}(\{1,\hdots,K\})}$.
Then, we use the DDM and compute $\smash{\hat{\rvx}_{0|i-1}}$ for each index $\smash{k\in\{k_{i,D},k_{i,P}\}}$ separately, denoting each result accordingly by $\hat{\rvx}_{0|i-1}^{(k)}$.
The final index is picked to optimize the perception-distortion tradeoff via
\begin{align}
    k_{i}\!=\!\argmin_{k\in\{k_{i,D},k_{i,P}\}}\!\text{MSE}\!\left(\!\vr(\rvy),\hat{\rvx}_{0|i-1}^{(k)}\!\right)\!+\!\lambda Q\!\left(\!\hat{\rvx}_{0|i-1}^{(k)}\!\right)\!,\label{eq:optimize-perception-distortion-indices}
\end{align}
where $Q(\cdot)$ can be \emph{any} NR-IQA measure, even a non-differentiable one.
In \Cref{app:dmax-ot} we explain our choice to set $\vr(\rvy)$ as an MMSE estimator.


We assess our approach choosing $\vr(\rvy)$ as the FFHQ~\citep{stylegan} $512\times 512$ approximate MMSE model trained by \citet{difface}.
We set $\lambda=1$ and optimize three different $Q(\cdot)$ measures: NIQE, $\text{CLIP-IQA}^{+}$~\citep{Wang_Chan_Loy_2023}, and TOPIQ~\citep{chen2024topiq} adapted for face images by PyIQA~\citep{pyiqa}.
We utilize the FFHQ $512\times512$ DDM of \citet{difface} with $T=1000$ sampling steps and $K=4096$ for all codebooks.
We compare our approach against the state-of-the-art methods PMRF~\citep{pmrf}, DifFace~\citep{difface}, and BFRffusion~\citep{bfrfussion}, using the standard evaluation datasets CelebA-Test~\citep{karras2018progressive,wang2021gfpgan}, LFW-Test~\citep{lfw-original}, WebPhoto-Test~\citep{wang2021gfpgan}, and WIDER-Test~\citep{zhou2022codeformer}.
We use PSNR to measure the distortion of the outputs produced for the CelebA-Test dataset, and measure the ProxPSNR~\citep{pmrf,man2025proxiesdistortionconsistencyapplications} for the other datasets, which lack the clean original images.
Perceptual quality is measured by NIQE, $\text{CLIP-IQA}^{+}$, TOPIQ-FACE, and additionally $\text{FD}_{\text{DINOv2}}$~\citep{stein2023exposing} to assess our generalization performance to a common quality measure which we do not directly optimize.
Finally, as in \Cref{sec:zero-shot-restoration}, we \emph{compress} each evaluated method using our standard compression scheme, adopting the same FFHQ DDM with $K=4096$ and $T=1000$.

The results for the WIDER-Test dataset are reported in \Cref{fig:real-world-wider-visual} (see \Cref{app:dmax-ot} for the other datasets).
Our approach clearly optimizes each quality measure effectively and generalizes well according to the $\text{FD}_{\text{DINOv2}}$ scores.
This is also confirmed visually, where all of our solutions produce high-quality images with less artifacts compared to previous methods.
While our approach shows slightly worse distortion, this is once again expected due to the perception-distortion tradeoff~\citep{Blau_2018_CVPR}.
