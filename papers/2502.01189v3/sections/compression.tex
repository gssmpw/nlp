


\section{Image Compression with DDCM}\label{section:compression}
\begin{figure*}[t]
    \centering
    \includegraphics[width=1\textwidth]{figures/Kodak24_512_extreme.pdf}
    \caption{\textbf{Qualitative image compression results.} The presented images are taken from the Kodak24 ($512\times 512$) dataset.
    Our codec produces highly realistic outputs, while maintaining better fidelity to the original images compared to previous methods.
    }
    \label{fig:compression_examples}
\end{figure*}
\paragraph{Method.}
Since sampling with DDCM yields compact bit-stream representations, a natural endeavor is to harness DDCM for compressing real images.
In particular, to compress an image $\rvx_{0}$, we leverage the predicted $\hat{\rvx}_{0|i}$ (\Cref{eq:x0eq}) at each timestep $i$ and compute the residual error from the target image, $\rvx_0-\hat{\rvx}_{0|i}$.
Then, we guide the sampling process towards $\rvx_0$ by selecting the codebook entry that maximizes the inner product with this residual,
\begin{align}\label{eq:compression_rule} 
    k_i = \argmax_{k\in\{1,\hdots,K\}} \langle \gC_i(k), \rvx_0-\hat{\rvx}_{0|i}\rangle,
\end{align}
where the size of the first codebook $\gC_{T+1}$ is $K=1$. 
This process is depicted as the compression branch in \Cref{fig:overview}, where the resulting set of chosen indices $\{k_i\}_{i=2}^{T+1}$ is the compressed bit-stream representation of the given image.
Section~\ref{sec:compressed_conditional_generation} sheds more light on this choice of the noise selection from the perspective of score-based generative models~\citep{song2020score}. 
As in \Cref{sec:method}, decompression follows standard DDCM sampling \cref{eq:DDCM_sampling}, re-selecting the stored indices instead of picking them randomly.
When using latent space DDMs (e.g., SD), we 
first encode $\rvx_0$ into the latent domain, perform all subsequent operations in this domain, and decode the result with the decoder.

The bit rate of this approach is determined by the size of the codebooks $K$, and the number of sampling timesteps $T$. Specifically, the bit-stream length is given by \smash{$(T-1)\log_{2}(K)$}. Therefore, the bit rate can be reduced by simply decreasing the number of codebooks, or by using a smaller number of timesteps at generation, e.g., by skipping every other step, or by using specific timestep intervals (see \Cref{app:range_t}).
In the approach described so far, the length of the bitstream increases logarithmically with $K$, making it computationally demanding to increase the bit rate.
For instance, even for $K=8192$, $T=1000$ and $768\times 768$ images our BPP is approximately 0.022.
Thus, to produce higher bit rates, we propose to \emph{refine} the noise selected at timestep $i$ by employing matching pursuit (MP)~\citep{mallat1993matching}.
Specifically, at each step $i$, we construct the chosen noise as a convex combination of $M$ elements from 
$\gC_i$, gathered in a greedy fashion to best correlate with the guiding residual $\rvx_{0}-\hat{\rvx}_{0|i}$ (as in~\Cref{eq:compression_rule}). 
The resulting convex combination involves $M-1$ quantized scalar coefficients, chosen from a finite set of $C$ values taken from $[0,1]$.
Therefore, the resulting length of the bit-stream is given by $\smash{(T-1)(\log_{2}(K)M+C(M-1))}$, such that $M=1$ is similar to our standard compression scheme, and the length of the bit-stream increases linearly with $M$ and $C$.
We apply this algorithm when the absolute bits number crosses $(T-1)\cdot \log_2(2^{13})$.
Further details are available in \Cref{app:matching_pursuit}.

\paragraph{Experiments.}
We evaluate our compression method on Kodak24~\cite{franzen1999kodak}, DIV2K~\citep{agustsson2017ntire}, ImageNet 1K $256\times 256$~\citep{deng2009imagenet,pan2020dgp}, and CLIC2020~\citep{CLIC2020}.
For all datasets but ImageNet, we center crop and resize all images to $512\times512$.
We compare to numerous competing methods, both non-neural and neural, and both zero-shot, fine-tuning based, and training based approaches.
For the ImageNet dataset, we use the unconditional pixel space ImageNet $256\times 256$ model of \citet{dhariwal2021diffusion}, and compare our results to BPG~\citep{bpg}, HiFiC~\citep{mentzer2020high}, IPIC~\citep{xu2024idempotence}, and two  PSC~\citep{elata2024zero} configurations, distortion-oriented (PSC-D) and perception-oriented (PSC-P).
For all other datasets, we use SD 2.1 $512\times512$~\citep{rombach2022high} and compare to BPG, PSC-D, PSC-P, ILLM~\citep{muckley2023improving}, PerCo (SD)~\citep{korber2024perco, careil2023towards}, and twoCRDR~\citep{iwai2024controlling} configurations, distortion-oriented (CRDR-R) and perception-oriented (CRDR-R).
PSC shares the same pre-trained model as ours, while PerCo (SD) requires additional fine-tuning.
For our method, we apply SD 2.1 unconditionally, as we saw no improvement by adding prompts (see further details in \Cref{app:text_effect}).
We assess our method for several options of $T$, $K$, $M$, and $C$ to control the bit rate.
See further details in \Cref{app:compression_details}.
We evaluate distortion with PSNR and LPIPS~\citep{zhang2018perceptual} and perceptual quality with FID~\citep{bi≈Ñkowski2018demystifying}.
For ImageNet, FID is computed against the entire 50k $256\times 256$ validation set.
For the smaller datasets we follow \citet{mentzer2020high} and compute the FID over extracted image patches. 
Specifically, for DIV2K and CLIC2020 we extract $128\times 128$ sized patches, and for Kodak we use $64\times64$.


As shown in \Cref{fig:compression_graphs}, our compression scheme dominates previous methods on the rate-perception-distortion plane~\citep{pmlr-v97-blau19a} for lower bit rates, surpassing both the perceptual quality (FID) and distortion (PSNR and LPIPS) of previous methods.
For instance, our FID scores are lower than those of all other methods at around 0.1 BPP, while, for the same BPP, our distortion performance is better than the perceptually-oriented methods (e.g., PerCo, PSC-P, and IPIC).
However, our method under-performs at the highest bit rates, especially when using SD. This is expected due to the performance ceiling entailed by the pre-trained encoder-decoder of SD~\citep{korber2024perco, elata2024zero}.
The qualitative comparisons in \Cref{fig:compression_examples} further demonstrate our superior perceptual quality, where, even for extreme bit rates, our method maintains similarity to the original images in fine details.
See \Cref{app:compression_details} for more details and results.


\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/compression_graphs.pdf}
    \caption{\textbf{Compression quantitative evaluation.}
    We compare the perceptual quality (FID) and distortion (PSNR, LPIPS) achieved at different BPPs. 
    The image sizes of each dataset is denoted next to its name.
    Our method produces the best perceptual quality at most BPPs.
    Importantly, this is while we also attain lower distortion compared to perceptually-oriented methods (e.g., PSC-P and PerCo (SD)). 
    For the three rightmost datasets, note that our approach, PSC-P, and PerCo (SD) use the latent space Stable Diffusion 2.1 model, so its VAE imposes a distortion bound.
    Thus, we report the distortion attained by simply passing the images through this VAE (dashed line).
    }
    \label{fig:compression_graphs}
\end{figure*}

