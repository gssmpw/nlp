

\section{Image Compression Supplementary}\label{app:compression_details}

We compute all distortion and perceptual quality measures using \href{https://github.com/Lightning-AI/torchmetrics}{Torch Metrics} (which relies on Torch Fidelity~\citep{obukhov2020torchfidelity}).

\subsection{Experiment Configurations}\label{app:image-compression-experiments-configuration}
We specify here the different configurations used for the compression experiments in \Cref{section:compression}.
\begin{itemize}[parsep=2pt,itemsep=2pt]
    \item In our $256\times256$ experiments we use the \texttt{256x256\_diffusion\_uncond.pt} checkpoint from the \href{https://github.com/openai/guided-diffusion}{official GitHub repository}.
    In our $768\times768$ and $512\times512$ experiments we use Stable Diffusion 2.1 with the \href{https://huggingface.co/stabilityai/stable-diffusion-2-1}{\texttt{stabilityai/stable-diffusion-2-1}} and \href{https://huggingface.co/stabilityai/stable-diffusion-2-1-base}{\texttt{stabilityai/stable-diffusion-2-1-base}} official checkpoints from Hugging Face, respectively.
    The different $K$, $M$, $C$ and $T$ values for each of the $512\times512$ and the $768\times768$ experiments plotted in \Cref{fig:compression_graphs,fig:compression_graphs_768} are summarized in \Cref{tab:our_tmkc_configs}.
    

\begin{table}[h]
\centering
\caption{Image compression experiments configurations.}
\begin{tabular}{c|c|c|c|c|c}
\hline
Model & Image Resolution & $T$ & $K$ & $M$ & $C$ \\
\hline
\multirow{4}{*}{Pixel Space DDM} & \multirow{4}{*}{256$\times$256} & 1000 & 64, 128, 256, 4096 & 1 & - \\
& & 1000 & 2048 & 2, 3, 4, 5 & 3 \\
& & 500 & 128, 512 & 1 & - \\
& & 300 & 16, 32, 128, 512 & 1 & - \\
\hline
\multirow{6}{*}{Latent Space DDM} & \multirow{2}{*}{512$\times$512} & 1000 & 256, 1024, 8192 & 1 & - \\
& & 1000 & 2048 & 2, 3, 6 & 3 \\
\cline{2-6}
& \multirow{4}{*}{768$\times$768} & 1000 & 16, 32, 64, 256, 1024, 8192 & 1 & - \\
& & 1000 & 2048 & 2, 3, 6 & 3 \\
& & 500 & 16, 32, 64, 256, 1024, 8192 & 1 & - \\
& & Adapted 500 (\Cref{app:range_t}) & 16, 32, 64, 256, 1024, 8192 & 1 & - \\
\hline
\end{tabular}
\label{tab:our_tmkc_configs}
\end{table}

    % For th


% pixel
% 1000 - K=64 256 1024 4096
% 500 - K=128 512
% 300 - K=16 32 128 512 
% pursuit - p=2,4,3,5, | K= 2048 | C= 3
% latent - 
% 1000 - K=256 1024 8192
% pur - C=3, P=2,3,6
    
    % For the $768\times768$ image size experiments, we use $K\in\{16, 32, 64, 256, 1024,  8192\}$.
    \item For PSC-D and PSC-R we use the same pre-trained ImageNet $256\times256$ model as ours in the $256\times256$ experiments, and the same Stable Diffusion 2.1 model in the $512\times512$ experiments. We adopt the default hyper-parameters of the method as described by the authors~\citep{elata2024zero}, setting the number of measurements to $12\cdot2^i$ for $i=0,\ldots,8$.
    \item For IPIC we adopt the official implementation using the ELIC codec with five bit rates, combined with DPS sampling for decoding with $T=1000$ steps and $\zeta\in\{0.3, 0.6, 0.6, 1.2, 1.6\}$, as recommended by the authors.
    \item For BPG we considered quality factors $q\in\{51,50,48,46,42,40,38,36,34,32,30\}$. 
    \item For HiFiC we test the low, medium and high quality regimes, using the checkpoints available in the \href{https://github.com/Justin-Tan/high-fidelity-generative-compression}{official GitHub repository}.
    \item PerCo (SD) is tested using the three publicly available Stable Diffusion 2.1 fine-tuned checkpoints from their \href{https://github.com/Nikolai10/PerCo}{Official GitHub repository}, using the default hyper-parameters.
    \item For ILLM we use the MS-ILLM pre-trained models available in the \href{https://github.com/facebookresearch/NeuralCompression/tree/main/projects/illm}{official GitHub implementation}.
    For the $512\times512$ image size experiments we use \texttt{msillm\_quality\_X}, $\texttt{X}=2,3,4$. For the $768\times768$ image size experiments we use \texttt{msillm\_quality\_X}, $\texttt{X}=2,3$ and \texttt{msillm\_quality\_vloY}, $\texttt{Y}=1,2$.
    \item CRDR-D and CRDR-R are evaluated using quality factors of $\{0,1,2,3,4\}$, where CRDR-D uses $\beta=0$ and CRDR-R uses $\beta=3.84$, as recommended in the paper.
\end{itemize}


\subsection{Additional Evaluations}\label{app:compression_more_results}

In \Cref{fig:compression_examples_app,,fig:compression_examples_app_medium,,fig:pixel_space_comparison_low_bpp,,fig:pixel_space_comparison_mid_bpp} we provide additional qualitative comparisons on the Kodak24 ($512\times 512$) and ImageNet ($256\times 256$) datasets.
We additionally compare our method on images of size $768\times768$, and present the results in \Cref{fig:compression_graphs_768}.
Our method is implemented as before, while using a Stable Diffusion 2.1 model trained on the appropriate image size (see \Cref{app:image-compression-experiments-configuration}).

\subsection{Decreasing the Bit Rate via Timestep Sub-Sampling}\label{app:range_t}

As mentioned in \Cref{section:compression}, decreasing the bit rate of our compression scheme can be accomplished in two ways.
The first option is to reduce $K$, which sets the number of bits required to represent each communicated codebook index.
The second option relates to the number of generation timesteps, which sets the total number of communicated indices. Specifically, DDMs trained for $T=1000$ steps can still be used to generate samples with fewer steps, by skipping alternating timesteps and modifying the variance in \Cref{eq:ddpm}. 
Thus, we leverage such timestep sub-sampling in DDCM to shorten the compressed bit-stream.
We find that pixel space DDMs yields good results with this approach, while the latent space models struggle to produce satisfying perceptual quality.

Thus, for latent space models we propose a slightly different timestep sub-sampling scheme. 
Specifically, we keep $T=1000$ sampling steps at inference and set different $K$ values for different subsets of timesteps.
We choose $K=1$ for a subset of $L$ sampling steps, and $K>1$ for the rest $T-L$ steps.
Thus, our compression scheme only optimizes $T-L$ steps and necessitates transmitting only $T-L$ indices. The rest $L$ indices correspond to codebooks that contain only one vector, and thus do not affect the bit rate.

We use $T=1000$, set the same codebook size $K>1$ for every timestep $i\in\{899\ldots,400\}$, and use $K=1$ for all other steps.
We compare our proposed method against the aforementioned naive timestep skipping approach with $T=500$ sampling steps and the same $K>1$, which attains the same bit rate as our proposed alternative.

Quantitative results are shown in \Cref{fig:compression_graphs_768}, where our timestep adapted method is denoted by \emph{Ours Adapted}.
Our adapted approach achieves better perceptual quality compared to the naive one, at the expense of a slightly hindered PSNR.


\subsection{Increasing the Bit Rate via Matching Pursuit}\label{app:matching_pursuit}
In \Cref{section:compression} we briefly explain how to achieve higher bit rates with our method, by refining each selected noise via a matching pursuit inspired solution.
Formally, at each timestep $i$ we iteratively refine the selected noise by linearly combining it with $M-1$ other noises from the codebook.
We start by picking the first noise index $k_{i}$ according to \Cref{eq:compression_rule}.
Then, we set $k_{i}^{(1)}=k_{i}$, $\gamma_{i}^{(1)}=1$, and $\tilde{\vz}_{i}^{(1)}=\gC_{i}(k_{i})$, and pick the next indices and coefficients $(m=1,\hdots,M-1)$ via
\begin{align}
    k_i^{(m+1)}, \gamma_i^{(m+1)} = \argmax_{k\in\{1,\hdots,K\},\;\gamma \in \Gamma} \left\langle \gamma\tilde{\vz}_{i}^{(m)}+\left(1-\gamma\right)\gC_{i}(k) ,\,\rvx_0-\hat{\rvx}_{0|i}\right\rangle.
\end{align}
The noise vector $\tilde{\vz}_{i}^{(m+1)}$ is then updated via
\begin{align}
&\tilde{\vz}_{i}^{(m+1)}\leftarrow\gamma_{i}^{(m+1)}\tilde{\vz}_{i}^{(m)}+\left(1-\gamma_{i}^{(m+1)}\right)\gC_{i}\left(k_{i}^{(m+1)}\right),\\
&\tilde{\vz}_{i}^{(m+1)}\leftarrow \frac{\tilde{\vz}_{i}^{(m+1)}}{\text{std}\left(\tilde{\vz}_{i}^{(m+1)}\right)},
\end{align}
where $\text{std}(\vz)$ is the empirical standard deviation of the vector $\vz$.
We use the resulting vector $\tilde{\vz}_{i}^{(M)}$ as the noise in \Cref{eq:DDCM_sampling} to produce the next $\rvx_{i-1}$, and repeat the above process iteratively.
Note that setting $M=1$ is equivalent to our standard compression scheme.


In our experiments, the set of coefficients $\Gamma$ is a subset of $(0,1]$, containing $C=|\Gamma|$ values that are evenly spaced in this range.
We pick $C=3$ and assess $M\in\{2,3,6\}$ for the latent space model experiments, and $M\in\{2,3,4,5\}$ for the pixel space model experiments.




\subsection{Assessing the Effectiveness of Text Prompts in Compression using Text-to-Image Latent Space DDMs}\label{app:text_effect}

Stable Diffusion 2.1 is a text-to-image generative model, which both PerCo (SD) and PSC leverage for their compression approach.
Specifically, both of these methods start by generating a textual caption for every target image using BLIP-2~\citep{li2023blip}, and feed the captions as prompts to the SD model.
In our case, we find that using such prompts hinders the compression quality.
Specifically, we follow the same automatic captioning procedure as in PerCo (SD) and PSC, using the \texttt{Salesforce/blip2-opt-2.7b-coco} \href{https://huggingface.co/Salesforce/blip2-opt-2.7b-coco}{checkpoint} of BLIP-2 from Hugging Face.
We then continue with our standard compression approach, where the denoiser is used with standard classifier-free guidance (CFG).
Note that using text prompts requires transmitting additional bits that serve as a compressed version of the text.
Specifically, we use BLIP-2 with a maximum of $L=32$ word tokens, each picked from a dictionary containing a total of 30,524 words.
Thus, at most $32\cdot\log_{2}(30524)\approx 480$ bits are added to the bit-stream in our method.

We assess this text-conditional approach on the $512\times 512$ SD 2.1 DDM, using CFG scales of $3,6$.
We compare the performance of this conditional approach with that of the unconditional one we used in \Cref{section:compression}.
The results in \Cref{fig:compression_graphs_blip} show a disadvantage for using text-prompts for compression with our method.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.86\linewidth]{figures/Kodak24_512_extreme_app.pdf}
    \caption{\textbf{Qualitative extreme image compression results.} The presented images are taken from the Kodak24 dataset, cropped to $512\times512$ pixels.
    Our compression scheme produces highly realistic decompressed outputs, while maintaining better fidelity to the original images compared to previous methods.
    }
    \label{fig:compression_examples_app}
\end{figure*}


\begin{figure*}[t]
    \centering
    \includegraphics[height=0.93\textheight]{figures/Kodak24_512_med_app.pdf}
    \caption{
    \textbf{Qualitative image compression results.} The presented images are taken from the Kodak24 dataset, cropped to $512\times512$ pixels.
    Our compression scheme produces highly realistic decompressed outputs, while maintaining better fidelity to the original images compared to previous methods.
    }\label{fig:compression_examples_app_medium}
\end{figure*}

%pixel space compression compression
\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/pixel_space_comparison_low_bpp.pdf}
    \caption{\textbf{Qualitative image compression results.} the presented images are taken from the ImageNet $256\times 256$ dataset.
    Compared to previous methods, our compression scheme produces higher perceptual quality and better fidelity to the original images.
    }
    \label{fig:pixel_space_comparison_low_bpp}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/pixel_space_comparison_mid_bpp.pdf}
    \caption{\textbf{Qualitative image compression results.} the presented images are taken from the ImageNet $256\times 256$ dataset.
    Compared to previous methods, our compression scheme produces higher perceptual quality and better fidelity to the original images.
    }
    \label{fig:pixel_space_comparison_mid_bpp}
\end{figure*}


\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/compression_graphs_768.pdf}
    \caption{\textbf{Quantitative image compression results on $768\times768$ sized images.} At higher bit rates, our method achieves the lowest (best) FID scores in both datasets while maintaining better distortion metrics compared to PerCo (SD). At extremely low bit rates, while PerCo (SD) shows marginally better FID scores, our method attains superior PSNR performance.}
    \label{fig:compression_graphs_768}
\end{figure*}


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/compression_graphs_blip.pdf}
    \caption{\textbf{Evaluating the effectiveness of using text prompts in image compression.}
    We evaluate our unconditional compression method with the text-conditional one, while using the text captions generated by BLIP-2.
    We find that using such text prompts hinders our compression results, both in terms of perceptual quality and distortion.}
    \label{fig:compression_graphs_blip}
\end{figure*}

