\section{Discussion}\label{sec:discussion}

We introduced DDCM, a novel generative approach for DDMs that produces high-quality image samples jointly with their lossless compressed bit-stream representations.
We found that DDCM achieves comparable generative performance to DDPM, even when the codebooks are extremely small.
We leveraged DDCM to solve several compressed image generation tasks, including image compression and compressed restoration, where we achieved state-of-the-art results.
Besides image restoration, our compressed conditional generation framework can be used for any type of diffusion guidance, e.g., for text-conditional generation.
We demonstrate this option in~\Cref{appendix:classifier-guidance,appendix:classifier-free-guidance}, introducing new classifier-based and classier-free guidance methods that do not use $\rvy$ for decompression.
Moreover, we present in \Cref{app:editing} preliminary results for compressed image editing using DDCM, by decompressing an image using a desired edit text prompt.

While our empirical results are encouraging, our work does not explain theoretically why DDCM sampling and our simple index selection strategies work so effectively.
We encourage future works to investigate the principles behind the success of our methods.
Moreover, when operating in latent space, our codec's performance is bounded by the underlying VAE, particularly at higher bit rates.
Our results at higher bit rates could also be improved through better approaches than our current matching pursuit inspired solution.
Additionally, our compression efficiency could be improved through entropy coding of the selected indices, potentially reducing bit rates without sacrificing quality.
Lastly, all DDCM-based solutions can be improved further by optimizing the codebooks, e.g., through dictionary learning. 
To conclude, our work demonstrates promising results across numerous tasks while providing opportunities for both theoretical analysis and practical improvements.

