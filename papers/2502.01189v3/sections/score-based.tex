\subsection{Background and Proof of~\Cref{prob:ode_convergence}}
We will prove that~\Cref{prob:ode_convergence} holds for any score-based diffusion model~\citep{song2020score}.
For completeness, we first provide the necessary mathematical background and then proceed to the proof of the proposition.
\subsubsection{Background}

\paragraph{Score-Based Generative Models.}Score-based generative models~\citep{song2020score} define a diffusion process $\smash{\{\rvx(t):t\in [0,T]\}}$, where $p_{0}$ and $p_{T}$ denote the data distribution and the prior distribution, respectively, and $p_{t}$ denotes the distribution of $x(t)$.
Such a diffusion process can generally be modeled as the stochastic differential equation (SDE)
\begin{align}
\delt\rvx=f(\rvx,t)\delt t+g(t)\delt\rvw,\label{eq:forward_sde}
\end{align}
where $f(\cdot,t)$ is called the \emph{drift} coefficient, $g(t)$ is called the \emph{diffusion} coefficient, $\rvw(t)$ is a standard Wiener process, and $\delt t$ denotes an infinitesimal timestep.
Samples from the data distribution $p_{0}$ can be generated by solving the reverse-time SDE~\citep{ANDERSON1982313},
\begin{align}
    &\delt\rvx=\left[f(\rvx,t)-g^{2}(t)\vs_{t}(\rvx)\right]\delt t+g(t)\delt\bar{\rvw},\label{eq:reverse_sde}
\end{align}
starting from samples of $\rvx(T)$.
Here, $\vs_{t}(\rvx(t))\coloneqq\nabla_{\rvx(t)}\log{p_{t}(\rvx(t))}$ is the \emph{score} of $p_{t}$, $\bar{\rvw}(t)$ denotes a standard Wiener process where time flows backwards, and $\delt t$ is an infinitesimal \emph{negative} timestep.
Samples from the data distribution can also be generated by solving the \emph{probability flow} ODE,
\begin{align}
    &\delt\rvx=\left[f(\rvx,t)-\frac{1}{2}g^{2}(t)\vs_{t}(\rvx)\right]\delt t.\label{eq:reverse_ode}
\end{align}

\paragraph{Solving The Reverse-Time SDE.}The reverse-time SDE in~\Cref{eq:reverse_sde} can be solved with any numerical SDE solver (e.g., Euler-Maruyama), which corresponds to some time discretization of the forward and reverse stochastic dynamics.
For the sake of our proof, we adopt the simple solver proposed by~\citet{song2020score},
\begin{align}
    &\rvx_{i-1}=\rvx_{i}-f_{i}(\rvx_{i})+g_{i}^{2}\rvs_{i}(\rvx_{i})+g_{i}\rvz_{i},\quad\rvz_{i}\sim\mathcal{N}(\vzero,\mI),\label{eq:song_discretization}
\end{align}
where $i=T,\hdots,1$ and $f_{i}$ and $g_{i}$ are the time-discretized versions of $f$ and $g$, respectively.
Note that DDPMs~\citep{ho2020denoising} are score-based diffusion models that solve a reverse-time Variance Preserving (VP) SDE, where $f(\rvx(t),t)=-\frac{1}{2}\beta(t)\rvx(t)$ and $g(t)=\sqrt{\beta(t)}$ for some function $\beta$.

\subsubsection{Proof of~\Cref{prob:ode_convergence}}
Given any general score-based diffusion model, we can write the DDCM compressed conditional generation process as
\begin{align}
    &\rvx_{i-1}=\rvx_{i}-f_{i}(\rvx_{i})+g_{i}^{2}\rvs_{i}(\rvx_{i})+g_{i}\gC_{i}(k_{i}),\label{eq:score-based-ddcm}
\end{align}
where $k_{i}$ are picked according to~\Cref{eq:k_choose_conditional}.
Choosing $\gL=\gL_{\text{P}}$, we have
\begin{align}
    k_{i}=\argmin_{k\in\{1,\hdots,K\}}\norm{\gC_{i}(k)-g_{i}\nabla_{\rvx_{i}}\log{p_{i}(\rvy|\rvx_{i})}}^{2}.
\end{align}
Since each $\gC_{i}$ contains $K$ independent samples drawn from a normal distribution $\gN(\vzero,\mI)$, we have
\begin{align}
    \{\gC_{i}(1),\hdots,\gC_{i}(K)\}\underset{K\rightarrow\infty}{\longrightarrow}\mathbb{R}^{n},
\end{align}
where $n$ denotes the dimensionality of each vector in $\gC_{i}$, and $\{\gC_{i}(1),\hdots,\gC_{i}(K)\}$ is the set comprised of all the elements in the $\gC_{i}$ (without repetition).
Since $g_{i}\nabla_{\rvx_{i}}\log{p_{i}(\rvy|\rvx_{i})}\in\mathbb{R}^{n}$, we have
\begin{align}
    \min_{k\in\{1,\hdots,K\}}\norm{\gC_{i}(k)-g_{i}\nabla_{\rvx_{i}}\log{p_{i}(\rvy|\rvx_{i})}}^{2}\underset{K\rightarrow\infty}{\longrightarrow}0.
\end{align}
Thus,
\begin{align}
    \gC_{i}(k_{i})\underset{K\rightarrow\infty}{\longrightarrow}g_{i}\nabla_{\rvx_{i}}\log{p_{i}(\rvy|\rvx_{i})}.\label{eq:noise_convergence_to_grad}
\end{align}
Plugging~\Cref{eq:noise_convergence_to_grad} into~\Cref{eq:score-based-ddcm}, we get
\begin{align}
    \rvx_{i-1}\underset{K\rightarrow\infty}{\longrightarrow}&\rvx_{i}-f_{i}(\rvx_{i})+g_{i}^{2}\rvs_{i}(\rvx_{i})+g_{i}^{2}\nabla_{\rvx_{i}}\log{p_{i}(\rvy|\rvx_{i})}\\
    =&\rvx_{i}-f_{i}(\rvx_{i})+g_{i}^{2}\nabla_{\rvx_{i}}\log{p_{i}(\rvx_{i})}+g_{i}^{2}\nabla_{\rvx_{i}}\log{p_{i}(\rvy|\rvx_{i})}\\
    =&\rvx_{i}-f_{i}(\rvx_{i})+g_{i}^{2}\left[\nabla_{\rvx_{i}}\log{p_{i}(\rvx_{i})}+\nabla_{\rvx_{i}}\log{p_{i}(\rvy|\rvx_{i})}\right]\\
    =&\rvx_{i}-f_{i}(\rvx_{i})+g_{i}^{2}\nabla_{\rvx_{i}}\log{p_{i}(\rvx_{i}|\rvy)},\label{eq:bayes_rule_for_ode}
\end{align}
where in~\Cref{eq:bayes_rule_for_ode} we used Bayes rule and the fact that $\nabla_{\rvx_{i}}\log{p(\rvy)}=0$.
Note that~\Cref{eq:bayes_rule_for_ode} resembles a time discretization of a probability flow ODE (\Cref{eq:reverse_ode}) over the posterior distribution $p_{0}(\rvx_{0}|\rvy)$, with $f(\cdot,t)$ and $\sqrt{2}g(t)$ being the drift and diffusion coefficients in continuous time, respectively.
Thus, when $K\rightarrow\infty$, our compressed conditional generation process becomes a sampler from $p_{0}(\rvx_{0}|\rvy)$.

% for every $\gamma>0$ and $k$ we have
% \begin{align}
%     \mathbb{P}(\norm{\gC_{i}(k)-g_{i}\nabla_{\rvx_{i}}\log{p_{i}(\rvy|\rvx_{i})}}^{2}\leq\gamma)>0.
% \end{align}

\subsection{Image Compression as a Private Case of Compressed Conditional Generation}\label{appendix:compression_private_case}
We show that our standard image compression scheme from~\Cref{section:compression} is a private case of our compressed conditional generation scheme from~\Cref{sec:compressed_conditional_generation}, where $\rvy=\rvx_{0}$ and $\gL=\gL_{\text{P}}$.
When $\rvy=\rvx_{0}$ we have
\begin{align}
    \nabla_{\rvx_{i}}\log{p_{i}(\rvy|\rvx_{i})}&=\nabla_{\rvx_{i}}\log{p_{i}(\rvx_{0}|\rvx_{i})}\nonumber\\
    &=\nabla_{\rvx_{i}}\log{p_{i}(\rvx_{i}|\rvx_{0})}-\nabla_{\rvx_{i}}\log{p_{i}(\rvx_{i})}\\
    &=\nabla_{\rvx_{i}}\log{p_{i}(\rvx_{i}|\rvx_{0})}-\vs_{i}(\rvx_{i})\label{eq:compression-grad}
\end{align}
where $\nabla_{\rvx_{i}}\log{p_{i}(\rvx_{i}|\rvx_{0})}$ can be computed in closed-form via the forward diffusion process in~\Cref{eq:forward_gaussian_diffusion}.
In particular, we have $p_{i}(\rvx_{i}|\rvx_{0})=\gN(\rvx_{i};\sqrt{\bar{\alpha}_{i}}\rvx_{0},(1-\bar{\alpha}_{i})\mI)$~\citep{ho2020denoising}, and thus
\begin{align}
    \nabla_{\rvx_{i}}\log{p_{i}(\rvx_{i}|\rvx_{0})}&=-\nabla_{\rvx_{i}}\frac{\norm{\rvx_{i}-\sqrt{\bar{\alpha}_{i}}\rvx_{0}}^{2}}{2(1-\bar{\alpha}_{i})}\\
    &=-\frac{\rvx_{i}-\sqrt{\bar{\alpha}_{i}}\rvx_{0}}{1-\bar{\alpha}_{i}}\\
    &=\frac{\sqrt{\bar{\alpha}_{i}}\rvx_{0}-\rvx_{i}}{1-\bar{\alpha}_{i}}.\label{eq:vp-sde-forward-grad}
\end{align}
Moreover, it is well known that~\citep{ho2020denoising,song2020score}
\begin{align}
    \vs_{i}(\rvx_{i})=\frac{\sqrt{\bar{\alpha}_{i}}\hat{\rvx}_{0|i}-\rvx_{i}}{1-\bar{\alpha}_{i}},\label{eq:score-mmse-in-vp-sde}
\end{align}
where $\hat{\rvx}_{0|i}$ is the (time-aware) Minimum Mean-Squared-Error (MMSE) estimator of $\rvx_{0}$ given $\rvx_{i}$.
Plugging~\Cref{eq:score-mmse-in-vp-sde,eq:vp-sde-forward-grad} into~\cref{eq:compression-grad}, we get
\begin{align}
    \nabla_{\rvx_{i}}\log{p_{i}(\rvx_{0}|\rvx_{i})}=\frac{\sqrt{\bar{\alpha}_{i}}}{1-\bar{\alpha}_{i}}(\rvx_{0}-\hat{\rvx}_{0|i}).\label{eq:compression_vp_sde_likelihood}
\end{align}
Thus, we have
\begin{align}
    k_{i}&=\argmin_{k\in\{1,\hdots,K\}}\gL_{\text{P}}(\rvy,\rvx_{i},\gC_{i},k)\label{eq:compression_posterior}\\
    &=\argmin_{k\in\{1,\hdots,K\}}\bignorm{\gC_{i}(k)-\sigma_{i}\frac{\sqrt{\bar{\alpha}_{i}}}{1-\bar{\alpha}_{i}}(\rvx_{0}-\hat{\rvx}_{0|i})}^{2}\\
    &=\argmin_{k\in\{1,\hdots,K\}}\bignorm{\gC_{i}(k)}^{2}-2\langle\gC_{i}(k),\sigma_{i}\frac{\sqrt{\bar{\alpha}_{i}}}{1-\bar{\alpha}_{i}}(\rvx_{0}-\hat{\rvx}_{0|i})\rangle+\bignorm{\sigma_{i}\frac{\sqrt{\bar{\alpha}_{i}}}{1-\bar{\alpha}_{i}}(\rvx_{0}-\hat{\rvx}_{0|i})}^{2}\\
    &=\argmin_{k\in\{1,\hdots,K\}}\bignorm{\gC_{i}(k)}^{2}-2\langle\gC_{i}(k),\sigma_{i}\frac{\sqrt{\bar{\alpha}_{i}}}{1-\bar{\alpha}_{i}}(\rvx_{0}-\hat{\rvx}_{0|i})\rangle.
\end{align}
Below, we show that $\norm{\gC_{i}(k)}^{2}$ is roughly equal for every $k$.
Thus, it holds that
\begin{align}
    k_{i}&=\argmin_{k\in\{1,\hdots,K\}}\bignorm{\gC_{i}(k)}^{2}-2\langle\gC_{i}(k),\sigma_{i}\frac{\sqrt{\bar{\alpha}_{i}}}{1-\bar{\alpha}_{i}}(\rvx_{0}-\hat{\rvx}_{0|i})\rangle\\
    &\approx\argmin_{k\in\{1,\hdots,K\}}\text{const}-2\langle\gC_{i}(k),\sigma_{i}\frac{\sqrt{\bar{\alpha}_{i}}}{1-\bar{\alpha}_{i}}(\rvx_{0}-\hat{\rvx}_{0|i})\rangle\\
    &=\argmax_{k\in\{1,\hdots,K\}}\langle\gC_{i}(k),\sigma_{i}\frac{\sqrt{\bar{\alpha}_{i}}}{1-\bar{\alpha}_{i}}(\rvx_{0}-\hat{\rvx}_{0|i})\rangle\\
    &=\argmax_{k\in\{1,\hdots,K\}}\sigma_{i}\frac{\sqrt{\bar{\alpha}_{i}}}{1-\bar{\alpha}_{i}}\langle\gC_{i}(k),\rvx_{0}-\hat{\rvx}_{0|i}\rangle\\
    &=\argmax_{k\in\{1,\hdots,K\}}\langle\gC_{i}(k),\rvx_{0}-\hat{\rvx}_{0|i}\rangle.\label{eq:equiv_kstar_compression}
\end{align}
Note that the noise selection strategy in~\Cref{eq:equiv_kstar_compression} is similar to that of our standard compression scheme, namely~\Cref{eq:compression_rule}.
Thus, our compression method is a private case of our compressed conditional generation approach.
In practice, we used~\Cref{eq:compression_rule} instead of~\Cref{eq:compression_posterior} since the former worked slightly better.


To show that $\norm{\gC_{i}(k)}^{2}$ is roughly constant for every $k$, note that $\gC_{i}(k)$ is a sample from a $n$-dimensional multivariate normal distribution $\mathcal{N}(\vzero,\mI)$.
Thus, $\norm{\gC_{i}(k)}^{2}$ is a sample from a chi-squared distribution with $n$ degrees of freedom.
It is well known that samples from this distribution strongly concentrate around its mean $n$ for large values of $n$.
Namely, $\norm{\gC_{i}(k)}^{2}$ is highly likely to be close to $n$, especially for relatively small values of $K$.