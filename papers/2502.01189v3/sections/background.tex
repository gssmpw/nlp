\section{Background }
Diffusion models~\citep{sohl2015deep,ho2020denoising,song2020score} generate samples from a data distribution $p_{0}$ by \emph{reversing} a diffusion process that gradually adds random noise to samples from the data.
Specifically, the diffusion process starts with $\rvx_{0}\sim p_{0}$ and produces the chain $\rvx_{0},\rvx_{1},~\hdots~,\rvx_{T}$
via
\begin{align}
&\rvx_{i}=\sqrt{\alpha_{i}}\rvx_{i-1}+\sqrt{1-\alpha_{i}}\rvz_{i},\quad\rvz_{i}\sim\mathcal{N}(\vzero,\mI),\label{eq:forward_gaussian_diffusion}
\end{align}
where $\alpha_{1},\hdots,\alpha_{T}>0$ are some time-dependent constants.
The above is a time-discretization of a Variance Preserving (VP) SDE~\citep{song2020score}.
Then, samples from the data distribution $p_{0}$ are generated by solving the corresponding reverse-time VP SDE~\citep{ANDERSON1982313,song2020score}, i.e., by gradually \emph{denoising} samples, starting from $\rvx_{T}\sim\mathcal{N}(\vzero,\mI)$.
In this paper we adopt Denoising Diffusion Probabilistic Models (DDPMs)~\citep{ho2020denoising}, which propose the generative process ($i=T,\ldots,1$)
\begin{align}
    &\rvx_{i-1}=\vmu_{i}(\rvx_{i})+\sigma_{i}\rvz_{i},~\mbox{where}\label{eq:ddpm}\\
    &\vmu_{i}(\rvx_{i})=\frac{1}{\sqrt{\alpha_{i}}}(\rvx_{i}+(1-\alpha_{i})\vs_{i}(\rvx_{i})),
\end{align}
$\rvz_{i}\sim\mathcal{N}(\vzero,\mI)$, $\sigma_{i}=\sqrt{1-\alpha_{i}}$, and $\vs_{i}(\rvx_{i})$ denotes the \emph{score} $\nabla_{\rvx_{i}}\log{p_{i}(\rvx_{i})}$ of the probability density function $p_{i}(\rvx_{i})$.
Such a score function $\vs_{i}(\rvx_{i})$ is typically learned via \emph{denoising score matching}~\citep{dsm,NEURIPS2019_3001ef25,song2020score,ho2020denoising}, where a model $\hat{\rvx}_{0|i}$ is trained to predict $\rvx_{0}$ from $\rvx_{i}$ (i.e., a denoiser), and using the well-known equation~\citep{robbins1956empirical,miyasawa1961empirical, stein1981estimation}
\begin{align}\label{eq:x0eq}
\vs_{i}(\rvx_{i})=\frac{\sqrt{\bar{\alpha}_{i}}\hat{\rvx}_{0|i}-\rvx_{i}}{1-\bar{\alpha}_{i}},
\end{align}
where $\bar{\alpha}_{i}\coloneqq\prod_{s=1}^{i}\alpha_{s}$.
This generative process is applicable to both pixel space~\citep{dhariwal2021diffusion} and latent space~\citep{rombach2022high} diffusion models, by employing a VAE-based encoder-decoder.
