


\section{Denoising Diffusion Codebook Models}\label{sec:method}
\paragraph{Method.}Equation~(\ref{eq:ddpm}) depicts the standard DDPM sampling approach, where the added noise is sampled from a continuous Gaussian distribution.
DDCM instead uses a discrete noise space, by limiting each sampling step to choose from $K$ constant noise realizations, fixed separately for each step.
Formally, for each $i=2,\ldots,T+1$ we define a codebook of $K$ entries
\begin{align}
   \gC_i = \left[\vz_i^{(1)},\vz_i^{(2)}, \ldots, \vz_i^{(K)}\right],
\end{align}
where each $\gC_{i}(k)\coloneqq\vz_{i}^{(k)}$ is sampled independently from $\mathcal{N}(\vzero,\mI)$ and remains fixed throughout the model's lifetime.
Then, we modify the DDPM sampling process (\ref{eq:ddpm}), replacing the noise $\rvz_{i}$ by a randomly selected codebook entry,
\vspace{-0.19em}
\begin{align}\label{eq:DDCM_sampling}
    \rvx_{i-1} = \vmu_i(\rvx_i) + \sigma_i \gC_i(k_i),
\end{align}
where the process is initialized with $\rvx_{T}=\gC_{T+1}(k_{T+1})$, $k_i\sim\text{Unif}(\{1, \ldots, K\})$, and sampling step $i=1$ does not involve noise addition. This random selection procedure is depicted in the generation branch in \Cref{fig:overview}.
Importantly, running the generative process~(\ref{eq:DDCM_sampling}) with a given sequence of noise vectors $\{\gC_{i}(k_i)\}_{i=2}^{T+1}$ always produces the same output image.
Thus, as depicted in the bottom part of \Cref{fig:overview}, the sequence of indices $k_{T+1},\hdots,k_2$ can be considered a losslessly compressed bit-stream representation of each generated image. 

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/generation_fid_inception.pdf}\vspace{-0.3cm}
    \caption{\textbf{Comparing DDPM with DDCM for different codebook sizes $K$.}
    Interestingly, DDCM with $K=64$ achieves similar FID to DDPM, suggesting that the continuous representation space of DDPM (DDCM with $K=\infty$) is highly redundant.
    We use a class-conditional ImageNet model ($256\times256$) for pixel space, and the text-conditional SD 2.1 model ($768\times768$) for latent space, with prompts from MS-COCO.
    The $K$ axis is in log-scale.
}    
    \label{fig:generation_fid_inception_comparison}
\end{figure}
\paragraph{Experiments.}While DDPM is equivalent to DDCM with $K=\infty$, the first question we address is whether DDCM maintains the synthesis capabilities of DDPM for relatively small $K$ values.
We compare the performance of DDPM with that of DDCM using $K\in\{2,4,8,16,64\}$ for sampling from pre-trained pixel and latent space models.
We compute the Fr√©chet Inception Distance (FID)~\citep{heusel2017gans} to evaluate the generation performance.
In \Cref{app:more_random_gens} we report additional metrics and provide qualitative comparisons.
For pixel space generation, we use a class-conditional DDM trained on ImageNet $256\times256$~\citep{deng2009imagenet,dhariwal2021diffusion}, and apply classifier guidance (CG)~\citep{dhariwal2021diffusion} with unit scale.
We use the 50k validation set of ImageNet as the reference dataset, and sample 10k class labels randomly to generate the images.
For latent space, we use Stable Diffusion (SD) 2.1~\citep{rombach2022high} trained on $768\times768$ images and apply classifier-free guidance (CFG) with scale 3 (equivalent to $w=2$ in~\citep{ho2021classifier}).
As the reference dataset, we randomly select 10k images from MS-COCO~\citep{lin2014microsoft,chen2015microsoftcococaptionsdata} along with one caption per image, and use those captions as prompts for sampling.


As shown in~\Cref{fig:generation_fid_inception_comparison}, DDCM achieves similar FID scores to DDPM at $K=64$, suggesting that the Gaussian representation space of DDPM is redundant.
In the next sections we leverage our new representation space to solve a variety of tasks, including image compression and compressed image restoration.

