\section{Related Work}\label{sec:related}

\paragraph{Compression.}
Image compression has seen significant progress in recent years, with the penetration of neural networks to this domain. Neural methods range from constructing specialized architectures~\citep{end-to-end-compression,zhu2022unified, jiang2023mlic,jiang2023mlicpp} to relying on different generative models such as GANs~\citep{mentzer2020high,muckley2023improving, iwai2024controlling} and VAEs~\citep{theis2017lossy}.
Recent compression methods leverage DDMs and offer high perceptual quality results, by training models from scratch~\citep{NEURIPS2023_ccf6d8b4, ghouse2023residual}, fine-tuning existing models~\cite{careil2023towards,korber2024perco}, or using pre-trained DDMs in a zero-shot manner (without further training)~\citep{theis2022lossy,elata2024zero}. 
Current solutions in the latter category are highly computationally demanding, either due to their communication schemes (e.g., reverse channel coding~\citep{pmlr-v162-theis22a,theis2022lossy}) or their need to perform thousands of denoising operations~\citep{elata2024zero}. Our work falls into this last category, offering a novel and highly effective compression scheme with a fast bit-stream communication method and computational demands that match standard use of DDMs.

\paragraph{Discrete Generative Modeling for Continuous Data.}
Recent works have explored discrete generative modeling of continuous data distributions.
These employ various discrete representations, such as vector quantized latent tokens~\citep{wu2024rdpm} or hierarchical modeling schemes that gradually refine each generated sample~\citep{yang2024discretedistributionnetworks}. DDCM offers a new such discrete generative framework, building on the exceptional achievements of DDMs.

\paragraph{Conditional Image Generation with Pre-Trained DDMs.}
Pre-trained DDMs are commonly utilized for solving conditional image generation tasks, such as image restoration~\citep{kawar2022denoising,lugmayr2022repaint,wang2023zeroshot, chung2023diffusion,2023diffbir, song2023pseudoinverseguided,cohen2024posterior, difface,raphaeli2025silosolvinginverseproblems,man2025proxiesdistortionconsistencyapplications}
and editing~\citep{meng2022sdedit, huberman2024edit, cohen2024slicedit, manor2024zeroshot}. 
In this work we address these tasks from the lens of DDCM, where the conditional samples are generated along with their compressed bit-streams.

\paragraph{Compressed Image Generation.}The task of compressed image generation (generating images directly in their compressed form) has been previously explored. \citet{kang2019jointimagegenerationcompression} trained an unconditional Generative Adversarial Network (GAN)~\citep{NIPS2014_5ca3e9b1} to synthesize JPEG representations. \citet{bulla2023} trained a text-conditional GAN in the JPEG domain.
In this work, we use DDMs instead of GANs and introduce a novel compressed image representation space different than that of JPEG. Our approach is compatible with any pre-trained DDM without requiring additional training.