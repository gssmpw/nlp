\documentclass[10pt]{article} % For LaTeX2e
\usepackage[accepted]{tmlr/tmlr}
% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{tmlr}
% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:
%\usepackage[preprint]{tmlr}
\input{math_commands}

\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage[capitalise]{cleveref}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{cancel}
\usepackage{pdfpages}
\usepackage{comment}
\usepackage{array}% http://ctan.org/pkg/{array,booktabs}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{array}
\usepackage[ruled, vlined]{algorithm2e}
\usepackage{etoolbox}
\usepackage{tikz}
\usetikzlibrary{bayesnet}
\usepackage{float}
\usepackage{natbib}
\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\xmark}{\text{\ding{55}}}
\newcommand{\cmark}{\text{\ding{51}}}
\usepackage{booktabs} % for professional tables
\usepackage{natbib}
\usepackage{comment}
\usepackage{soul}
\setlength\parindent{0pt}
\newcommand{\blackline}{\raisebox{2pt}{\tikz{\draw[-,black,dashed,line width = 0.7pt](0,0) -- (5mm,0);}}}
\newcommand{\orangeline}{\raisebox{2pt}{\tikz{\draw[-,orange,solid,thin, line width = 0.7pt](0,0) -- (5mm,0);}}}
 % The following command is just for this sample document:
\newcommand{\cs}[1]{\texttt{\char`\\#1}}% remove this in your real article

\def\month{Jan}  % Insert correct month for camera-ready version
\def\year{2025} % Insert correct year for camera-ready version
\def\openreview{\url{https://shorturl.at/SUjLP}} % Insert correct link to OpenReview for camera-ready version
 % The optional argument of \title is used in the header
\title{Shared Stochastic Gaussian Process Latent Variable Models: A Multi-modal Generative Model for Quasar Spectra}

\author{\name Vidhi Lalchand \email vidrl@mit.edu \\
      \addr Eric and Wendy Schmidt Center\\
      Broad Institute of MIT and Harvard; MIT
      \AND
      \name Anna-Christina Eilers \email eilers@mit.edu \\
      \addr MIT Kavli Institute for Astrophysics and Space Research
      }
      
\begin{document}

\maketitle
\begin{abstract}
This work proposes a scalable probabilistic latent variable model based on Gaussian processes \citep{lawrence2004gaussian} in the context of multiple observation spaces. We focus on an application in astrophysics where it is typical for data sets to contain both observed spectral features as well as scientific properties of astrophysical objects such as galaxies or exoplanets. In our application, we study the spectra of very luminous galaxies known as quasars, and their properties, such as the mass of their central supermassive black hole, their accretion rate and their luminosity, and hence, there can be multiple observation spaces. A single data point is then characterised by different classes of observations, which may have different likelihoods. Our proposed model extends the baseline stochastic variational Gaussian process latent variable model (GPLVM) \citep{lalchand2022generalised} to this setting, proposing a seamless generative model where the quasar spectra and scientific labels can be generated \textit{simultaneously} when modelled with a shared latent space acting as input to different sets of Gaussian process decoders, one for each observation space. In addition, this framework allows training in the missing data setting where a large number of dimensions per data point may be unknown or unobserved. We demonstrate high-fidelity reconstructions of the spectra and the scientific labels during test-time inference and briefly discuss the scientific interpretations of the results along with the significance of such a generative model. The code for this work is available at \url{https://github.com/vr308/Quasar-GPLVM}.

\end{abstract}

% Keywords may be removed
%\begin{keywords}
%List of keywords
%\end{keywords}

\section{Introduction}
\label{sec:intro}

Many challenges in the contemporary physical sciences arise from the analysis of large-scale, noisy, heteroscedastic, and high-dimensional datasets \citep{clarke:big_data_physical_sciences}. Hence, there is increasing consensus that addressing the challenges posed by large-scale data in the experimental pipelines for discovery, forecasting, and prediction warrant scalable machine learning. Modern experiments in physics, chemistry, and astronomy are capable of producing extremely complex high-dimensional data, but it is not just the sheer volume of the data, but also the \textit{velocity}, referring to the rate of production \citep{carleo2019machine} of data, which poses an additional challenge. Further, an additional axis is the variety or heterogeneity inherent in scientific datasets in the form of multiple outputs or observation spaces. For instance, images (pixels) can be accompanied with attributes like illumination, pose, and resolution. In addition, some of these attributes or observed features are often unknown or unobserved, resulting in incomplete data sets with missing components. The multiple output setting is inadequately addressed in the probabilistic machine learning literature; in this work we tackle precisely this setting proposing a multi-output scalable probabilistic model for a scientific application in astronomy.  

 A preponderance of literature in unsupervised learning focuses on the setting of a single large-scale homogeneous dataset, for instance, images, text, speech, or continuous numerical values. Further, most parametric models also assume a complete dataset where each dimension of every data point is observed. Real-world data are often only partially observed and in some cases yield very sparse data matrices with majority of the dimensions missing \citep{corduneanu2012continuation}. Robust unsupervised learning in these settings is a challenge and one needs to account for sensible epistemic uncertainties. In the running application we consider in this work, handling missing data during training is crucial, as spectral pixels in real observations can be missing due to absorption features in the Earth's atmosphere or different wavelength coverage of the spectra when observed with different telescopes or spectrographs.

 Dimensionality reduction is a standard precursor to further analysis in scientific datasets, as the intrinsic dimensionality of the data might actually be quite low. It is usually possible to summarise the key axis of variation in the data with very few dimensions \citep{facco2017estimating} side-stepping the curse of dimensionality and facilitating further downstream analysis. Projection techniques like PCA, multidimensional scaling (MDS) and independent component analysis (ICA) utilise eigenvalue decomposition \citep{pml} while other non-linear techniques like t-SNE \citep{tsne}, SNE \citep{parametric-sne} and UMAP \citep{umap} construct a probability distribution on high dimensional points and replicate a similar distribution on low-dimensional points iteratively using the KL divergence. These methods, however, are not generative in their traditional incarnation. They cannot be used to generate instances of the high-dimensional data points; hence, they are not very useful in many astrophysical settings where the ability to generate points in high-dimensional data space is crucial. 
 
 Generative latent variable models supplement traditional dimensionality reduction techniques as they offer the simultaneous benefits of a probabilistic interpretation and data generation while learning a faithful embedding of the high-dimensional training data in low-dimensional latent space. A generative probabilistic framework like the GPLVM \citep{lawrence2004gaussian} works by optimising the parameters of a Gaussian process \textit{decoder} from a low dimensional latent space ($Z \in \mathbb{R}^{N\times Q}$) to high-dimensional data space ($X \in \mathbb{R}^{N \times D}$) such that $Q \ll D$ and points close in latent space are nearby in data space. Since the decoder is a non-parametric Gaussian process, the kernel function controls the inductive biases of the function mapping like smoothness and periodicity. There typically is no encoder mapping hence, these models are also called Gaussian process decoders. It is possible to additionally incorporate a back-constraint or an encoder which maps from the data to latent space putting GPLVMs on the same footing as variational auto-encoders \citep[VAEs;][]{lawrence2006local, bui2015stochastic}. This amortises the cost of variational inference in very large-scale datasets but we don't employ this setting as it is not straightforwardly applicable to missing data contexts. 

 This work proposes a novel formulation of the GPLVM based on the idea of a shared latent space. The earlier work by \citet{ek2009shared} was the first to propose the idea of a shared data generation process but precluded truly scalable inference due to the standard $\mathcal{O}(N^{3})$ scaling. We extend this framework in two important ways. First, we show that the shared GPLVM is compatible with stochastic variational inference (SVI) \citep{JMLR:v14:hoffman13a} where we derive a joint evidence lower bound which factorises across multiple observation spaces due to conditional independence but share predictive strength though inducing locations and latent variables. Secondly, we train the entire model in presence of missing dimensions in one or both of the observation spaces.
 
 We demonstrate this scalable model in an astrophysical application using data of quasars. Quasars are the most luminous galaxies in the universe, powered by accretion onto a central supermassive black hole (SMBH) with millions to billions of solar masses in size. Understanding the formation, growth, and evolution across cosmic time of quasars and their SMBHs is one of the major goals of observational cosmology today. To this end, precise measurements of the physical properties of quasars are crucial, but they typically demand very expensive and time-intensive observations, as multiple epochs are needed to accurately determine, for instance, the quasar’s SMBH mass. The high-dimensional data used in this work contains $\sim22,000$ quasars from the Sloan Digital Sky Survey \citep[SDSS;][]{Lyke2020, WuShen2022} with high-quality spectral information (binned to 590 spectral dimensions/pixels) along with four scientific labels per quasar, i.e.\ their black hole mass, luminosity, redshift and so-called Eddington ratio -- a measure of the quasar's accretion rate. By modelling the spectra and scientific labels through a generative model acting on a shared latent space we aim to reason about the physical properties of the quasars just through its ``single-epoch'' spectral information, thus circumventing the time-intensive multi-epoch observations. Earlier work on applying probabilistic generative modelling using a GPLVM to high-dimensional quasar spectra \citep{eilers2022generative} have been constrained on scalability and examine less than 50 astronomical objects. We demonstrate our framework on datasets over $400\times$ larger. We summarise our key contributions below:
 \paragraph{Contributions} We propose a probabilistic generative framework called the \textit{Shared stochastic GPLVM} which is designed for use cases with multiple outputs/observation spaces. We seamlessly account for missing dimensions both at training and test time due to the probabilistic nature of the model. We demonstrate through astrophysical experiments that we can reconstruct previously unseen/test spectral pixels to a high degree of fidelity, interpolate missing or unobserved spectral regions and predict scientific labels. Crucially, we demonstrate that it is possible to share predictive strength by learning a common latent variable space $Z$ across multiple-outputs $(X,Y)$ where $Y \in \mathbb{R}^{N \times L}$ is an additional view of the data, we refer to this as an additional observation space with $L$ dimensions. In this way we indirectly model the relationships and correlation structure between the different observation spaces. We demonstrate this concretely with an experiment where we generate/predict all the scientific attributes at test-time by using latent variables ($Z$) only informed by the quasar spectra ($X$), we can denote this cross-modal prediction as $X \rightarrow Z \rightarrow Y$. To the best of our knowledge, predicting multiple outputs with stochastic variational GPs for scalability is methodologically novel. Further, this is the first demonstration of a scalable probabilistic latent variable model in astrophysical settings. In \cref{ss} we discuss how the development of these unsupervised learning frameworks can instigate novel insights into some of the major open questions in astronomy today.

\section{Stochastic Variational GPLVM with a Shared latent space}

In this section, we first describe background on the stochastic variational GPLVM \citep{lalchand2022generalised}. We then develop the idea of a shared latent space and inducing points within the stochastic variational GPLVM framework. The fundamental contribution of this work is to develop an inference scheme to show that a shared latent space does not preclude scalable inference through SVI. 

\subsection{SV-GPLVM: Stochastic Variational GPLVM}

In the traditional formulation underlying GPLVMs we have a training set comprising of $N$ $D$-dimensional real-valued observations $X \equiv \{\bm{x}_{n}\}_{n=1}^{N} \in \mathbb{R}^{N \times D}$. These data are associated with $N$ $Q$-dimensional latent variables, $Z \equiv \{\bm{z}_{n}\}_{n=1}^{N}\in \mathbb{R}^{N \times Q}$ where $Q \ll D$ provides dimensionality reduction \citep{lawrence2004gaussian}. The forward mapping ($Z \longrightarrow X$) is governed by GPs independently defined across dimensions $D$. The sparse GP formulation describing the data is as follows:
\begin{align}
p(Z) &= \displaystyle \prod _{n=1}^N \mathcal{N} (\bm{z}_{n};\bm{0}, \mathbb{I}_{Q}), \nonumber\\
p(F|U, Z, \theta) &= \displaystyle \prod_{d=1}^{D}\mathcal{N}(\bm{f}_{d}; K_{nm}K_{mm}^{-1}\bm{u}_{d}, Q_{nn}),\\
p(X| F, Z) &= \prod_{n=1}^N \prod_{d=1}^D \mathcal{N}(x_{n,d}; f_{d}(\bm{z}_{n}), \sigma^{2}_{x}),\nonumber
\label{initial}
\end{align}
where $Q_{nn} = K_{nn} - K_{nm}K_{mm}^{-1}K_{mn}$ is $N \times N$, $F \equiv \{ \bm{f}_{d} \}_{d=1}^{D}$ where $\bm{f}_{d} \in \mathbb{R}^{N}$, $U \equiv \{\bm{u}_{d} \}_{d=1}^{D}$ where $\bm{u}_{d} \in \mathbb{R}^{M}$ and $\bm{x}_{d}$ is the $d^{th}$ column of $X$. $K_{nn}$ is the $N \times N$ covariance matrix corresponding to a user-chosen positive-definite kernel function $k_{\theta}(\bm{z}, \bm{z}^{\prime})$ evaluated at latent points $\{\bm{x}_{n}\}_{n=1}^{N}$ and parametrised by shared hyperparameters $\theta$. Inducing variables per dimension $\{\bm{u}_{d} \}_{d=1}^{D}$ are distributed with a GP prior $\bm{u}_{d}|\tilde{Z} \sim \mathcal{N}(\bm{u}_{d};\bm{0}, K_{mm})$ (where $K_{mm}$ is $M\times M$) computed on inducing input locations $[\tilde{\bm{z}}_{1}, \ldots \tilde{\bm{z}}_{M}]^{T} \equiv \tilde{Z} \in \mathbb{R}^{M \times Q}$ which live in latent space with $Z$ and have dimensionality $Q$ (matching $\bm{z}_{n}$). In addition, $K_{nm}$ is the $N \times M$ cross-covariance computed on the latents $\bm{z}_{n}$ and the inducing locations $\tilde{\bm{z}}_{m}$. 

%The crux of SVI applied to sparse variational GPs as proposed in the seminal work of \citet{hensman2013gaussian} is that we can variationally integrate out $\bm{u}_{d}$ by learning their variational distributions $q(\bm{u}_{d}) \sim \mathcal{N}(\bm{m}_{d}, S_{d})$ numerically using stochastic gradient methods, essentially, by keeping the representation of $\bm{u}_{d}$ uncollapsed.

The core idea of Stochastic Variational Inference (SVI) applied to sparse variational Gaussian processes (GPs), as introduced by \citet{hensman2013gaussian}, is that the inducing variables \(\bm{u}_{d}\) can be variationally marginalized. This is achieved by learning their variational distribution \(q(\bm{u}_{d}) \sim \mathcal{N}(\bm{m}_{d}, S_{d})\) using stochastic gradient methods. Crucially, this approach retains an uncollapsed representation of \(\bm{u}_{d}\) throughout the optimization process. While \citet{hensman2013gaussian} proposed SVI for GP regression, \cite{lalchand2022generalised} extended this work to GPLVMs where the inputs $Z$ to the GPs are unobserved and each dimension of the high-dimensional output space $\bm{x}_{d}$ is modelled by an independent GP $f_{d}$ with shared kernel hyperparameters. 

Succinctly, posterior inference entails minimising the KL-divergence between the posterior over unknowns $p(F,U,Z|X)$ and the variational approximation $q(F,U,Z)$. Following \citet{lalchand2022generalised} when the variational approximation admits the factorisation below as in \citet{titsias2009variational}:
\begin{equation}
q(F, U, Z) \approx p(F|U,Z)q(U)q(Z) = \prod_{d=1}^{D}p(\bm{f}_{d}|\bm{u}_{d}, Z)q(\bm{u}_{d})\prod_{n=1}^{N}q(\bm{z}_{n})  
\label{vapp}
\end{equation}
the evidence lower bound (ELBO) can be derived as, 
% \begin{align}\label{elbo}
%      \text{KL}(q(F,U,Z)||p(F,U,Z|X)) &= \int p(F|U,Z)q(U)q(Z) \log \dfrac{ p(F|U,Z)q(U)q(Z)}{p(F, U, Z|X)}dF dU dZ \\
%      &= - \underbrace{\int p(F|U,Z)q(U)q(Z)\log \dfrac{p(X|F, Z){\cancel{\color{red}{p(F|U,Z)}}}p(U)p(Z)}{{\cancel{\color{red}{p(F|U,Z)}}}q(U)q(Z)}dF dU dZ}_{\text{ELBO}}  + \log p(X) \nonumber 
% \end{align}
% Minimising the KL is tantamount to maximising the ELBO as is clear from \cref{elbo}, this yields the evidence lower bound, 
\begin{align}
\log p(X) &\geq \langle\log p(X|F, Z)\rangle_{q(.)} - \text{KL}(q(Z)||p(Z)) - \text{KL}(q(U)||p(U)),
\end{align}
where $q(.)$ denotes the variational approximation as in \cref{vapp}. For brevity, we suppress the conditioning over the inducing inputs $\tilde{Z}$ in the prior $p(U)$ and the kernel hyperparameters $\theta$ in $p(F|U,Z)$.

If we choose to optimize the latent variables $Z$ as point estimates rather than variationally integrate them out (basically we do not introduce $q(Z)$) we end up with the following simplification,
\begin{align}
p(X) \geq & \int p(F|U,Z)q(U)\log \dfrac{p(X|F,Z)p(U)p(Z)}{q(U)}dF dU = \mathcal{L}_{x}
\label{elbo1}
\end{align}
Re-writing the lower bound as a sum of terms across data points $N$ and outputs/dimensions $D$ we get,
\begin{align}\label{factorelbo}
  \mathcal{L}_{x} = & \sum_{n,d}\langle\log p(x_{n,d}|\bm{f}_{d},\bm{z}_{n}, \sigma^{2}_{x})\rangle_{p(\bm{f}_{d}|\bm{u}_{d}, Z)q(\bm{u}_{d})} - \sum_{d}\text{KL}(q(\bm{u}_{d})||p(\bm{u}_{d}|\tilde{Z})) + \sum_{n}\log p(\bm{z}_{n})
\end{align}
Latent point estimates $\{\bm{z}_{n}\}_{n=1}^{N}$ can be learnt along with $\theta$ and variational parameters $(\tilde{Z}, \bm{m}_{d}, S_{d})$ by taking gradients of the ELBO in \cref{factorelbo}. However, an important constraint is that this formulation assumes a single-kernel matrix (single set of kernel hyperparameters) underlying all the independent GPs $D$. In the next section, we introduce the concept of an additional observation space with dimensions \(L\), which can be modelled using an independent stack of Gaussian processes (GPs), \(f_{l}\). These GPs are equipped to learn their own set of hyperparameters for added flexibility while sharing the latent embedding \(Z\) and inducing inputs \(\tilde{Z}\) to capture correlations across the different output spaces.

\subsection{Shared joint variational lower bound}

In the astrophysical application we focus on in this work we have two observation spaces corresponding to $N$ quasars. We denote the quasar spectra (pixels) with the matrix $X \in \mathbb{R}^{N \times D}$ and the scientific labels corresponding to the $N$ objects with $Y \in \mathbb{R}^{N \times L}$. The GPLVM construction models each column (pixel dimension and label dimension) with an independent GP, with the GPs corresponding to the pixel dimensions $\{f_{d}\}_{d=1}^{D}$ and label dimensions $\{f_{l}\}_{l=D+1}^{D+L}$ modelled with their own independent kernels and kernel hyperparameters, $\theta_{x}$ and $\theta_{y}$. Within each observation space the kernel hyperparameters are shared, so we learn two sets of hyperparameters corresponding to two observation spaces. 
\begin{align}
f_{d} \sim \mathcal{GP}(0,k_{\theta_{x}}) \hspace{10mm} f_{l} \sim \mathcal{GP}(0,k_{\theta_{y}})
\end{align}
The priors over the function values are given by, 
\begin{align}
    p(\bm{f}_{d}|\theta_{x}) &= \mathcal{N}(\bm{0}, K_{nn}^{(d)})  \\
    p(\bm{f}_{l}|\theta_{y}) &= \mathcal{N}(\bm{0}, K_{nn}^{(l)})
    \label{priors}
\end{align}
where $K_{nn}^{(d)}$ and $K_{nn}^{(l)}$ denote the $N \times N$ kernel matrices which rely on their own set of hyperparameters. The two observation spaces also yield two data likelihoods given by,
\begin{align}
   p(X|\bm{f}_{1:D}, Z) = \prod_{n=1}^{N}p(\bm{x}_{n}|\bm{f}_{d}, Z) &= \prod_{n=1}^{N}\prod_{d=1}^{D}\mathcal{N}(x_{n,d}; f_{d}(\bm{z}_{n}), \sigma^{2}_{x}) \\
   p(Y|\bm{f}_{D+1:D+L}, Z) = \prod_{n=1}^{N} p(\bm{y}_{n}|\bm{f}_{l}, Z) &= \prod_{n=1}^{N}\prod_{l=D+1}^{D+L}\mathcal{N}(y_{n,l}; f_{l}(\bm{z}_{n}), \sigma^{2}_{y}) 
\end{align}
In the absence of sparsity the log-marginal likelihood of the joint model compartmentalises nicely due to the assumed factorisation in the likelihoods. We marginalise out the latent function values $f_{1:D}$ and $f_{D+1:D+L}$ per dimension,
\begin{align}
p(X,Y|\theta_{x}, \theta_{y}, Z) &= \int _{1:D}\int_{D+1:D+L} p(X|\bm{f}_{1:D}, Z) p(Y|\bm{f}_{D+1:D+L}, Z)   p(\bm{f}_{d}|\theta_{x})  p(\bm{f}_{l}|\theta_{y}) d\bm{f}_{1:D}d\bm{f}_{D+1:D+L} \\
&= \int _{1:D} p(X|\bm{f}_{1:D}, Z)p(\bm{f}_{d}|\theta_{x}) d\bm{f}_{1:D} \int_{D+1:D+L}  p(Y|\bm{f}_{D+1:D+L}, Z)  p(\bm{f}_{l}|\theta_{y})d\bm{f}_{D+1:D+L}\\
& = \prod_{d=1}^{D} p(\bm{x}_{d}|\theta_{x},Z) \prod_{l=D+1}^{D+L} p(\bm{y}_{l}|\theta_{y},Z) = \prod_{d=1}^{D} \mathcal{N}(\bm{0}, K_{nn}^{(d)} + \sigma^{2}_{x}) \prod_{l=D+1}^{D+L} \mathcal{N}(\bm{0}, K_{nn}^{(l)} + \sigma^{2}_{y}) 
\end{align}
where $\bm{x}_{d}$ and $\bm{y}_{l}$ denote a single column/dimension of the observation spaces $X$ and $Y$.
The log marginal likelihood objective is then given by the following,
\begin{equation}
\text{log }p(X,Y|\theta_{x}, \theta_{y}, Z) = \sum_{d=1}^{D}\text{log } p(\bm{x}_{d}|\theta_{x}, Z) + \sum_{l=D+1}^{D+L}\text{log } p(\bm{y}_{l}|\theta_{y}, Z)
\label{lml}
\end{equation}
In order to induce sparsity we introduce inducing variables $\bm{u}_{d}$ and $\bm{u}_{l}$ for each individual dimension in the observation spaces; however, they are underpinned by shared inducing inputs $\tilde{Z}$ which live in the shared latent space $Z$ and share the same dimensionality, $Q$. 
With sparse GPs each of the terms in the decomposition above can be bounded by $\mathcal{L}_{x}$, while the inducing points $\tilde{Z}$ can be shared between the terms yielding the joint evidence lower bound. 
\begin{align}\label{joint}
    \text{log }p(X,Y|\theta_{x}, \theta_{y}, Z) \geq & \sum_{n,d}\langle\log p(x_{n,d}|\bm{f}_{d}, \bm{z}_{n}, \sigma^{2}_{x})\rangle_{p(\bm{f}_{d}|\bm{u}_{d}, Z)q(\bm{u}_{d})} - \sum_{d}\text{KL}(q(\bm{u}_{d})||p(\bm{u}_{d})) \\+ &\sum_{n,l}\langle\log p(y_{n,l}|\bm{f}_{l}, \bm{z}_{n}, \sigma^{2}_{y})\rangle_{p(\bm{f}_{l}|\bm{u}_{l}, Z)q(\bm{u}_{l})} - \sum_{l}\text{KL}(q(\bm{u}_{l})||p(\bm{u}_{l})) + \log p(Z) \nonumber
\end{align}
\begin{figure}[t]
\centering
\input{shared_gplvm}
    \caption{The graphical  model of the shared GPLVM with two sets of independent GPs and their respective hyperparameter sets.}
    \label{shared}
\end{figure}

Overall we optimise the shared variational lower bound w.r.t kernel hyperparameters for the two groups of GPs, $\theta_{x}$ and $\theta_{y}$, variational parameters $\{ \bm{m}_{d}, S_{d}\}_{d=1}^{D}$ and $\{ \bm{m}_{l}, S_{l}\}_{l=D+1}^{D+L}$, and a single set of shared latent point estimates $Z$ and inducing inputs $\tilde{Z}$.
We include the full training algorithm in Algorithm \ref{algorithm}. 

\section{Predictions \& Reconstructions}

High-dimensional points can arrive in different formats for the test data, either we observe both modalities $\{\bm{x}^{*},\bm{y}^{*}\}$ where $ \bm{x}^{*} =[x_{1}^{*}, \ldots, x_{d}^{*}]^{T}$ and similarly for $\bm{y}^{*}$ or only one of the modalities with the other missing, i.e. $\{\bm{x}^{*}\}$ or $\{\bm{y}^{*}\}$ only. The prediction exercise then involves inferring the latent $\bm{z}^{*}$ corresponding to the unseen test point.  

Since the GPLVM is a decoder only model, we cannot obtain the latent embedding $\bm{z}^{*}$ deterministically; instead we re-optimize the ELBO with the additional test data point $(\bm{x}^{*}, \bm{y}^{*})$ while keeping all the global and model hyperparameters frozen at their trained values. Note that since the ELBO factorises across data points, $\mathcal{L}(\{\bm{x}_{n}, \bm{y}_{n}\}_{n=1}^{N}, \bm{x}^{*}, \bm{y}^{*}) = \sum\limits_{n=1}^{N+1}\sum\limits_{h=1}^{D+L} \mathcal{L}_{n,h}$, the gradients to derive the new latent point $\bm{z}^{*}$ only depend on the respective component terms connected to the data point. In the event of a missing dimension $d^{*}$ for a new data point $\bm{x}^{*}$, the augmented ELBO for the spectra dimensions can be written as follows:

\begin{equation}
    \mathcal{L}_x^{*} \gets \mathcal{L}_x + \log p(\bm{z}^{*}) + \sum_{d \neq d^{*}} \langle p(\bm{x}^{*}_{d} ,| \bm{f}_d, \bm{z}^{*}, \sigma_x^2) \rangle_{q(.)}
\end{equation}

 Once we infer $\bm{z}^{*}$ we can compute the full reconstruction distributions (we may be interested in this if the test data point $\bm{y}^{*}$ had any missing dimensions, for example $\bm{y}^{*} = [y_{1}^{*}, y_{2}^{*}, ?, ? , y_{5}^{*} , \ldots, y_{l}^{*}]^{T}$) which are just the GP posterior predictive for each column or dimension, without loss of generality, for dimension $y_{l}^{*}$:
\begin{align}
    p(y_{l}^{*}|\bm{z}^{*}) &= \int p(y_{l}^{*}|\bm{f}_{l}, \bm{u}_{l}, \bm{z}^{*})p(\bm{f}_{l}, \bm{u}_{l}|\bm{y}_{l})d\bm{f}_{l}d\bm{u}_{l} \nonumber \\
    &= \int p(y_{l}^{*}|\bm{f}_{l}, \bm{u}_{l}, \bm{z}^{*})p(\bm{f}_{l}| \bm{u}_{l})q(\bm{u}_{l})d\bm{f}_{l}d\bm{u}_{l} \\
    &= \int p(y_{l}^{*}|\bm{u}_{l}, \bm{z}^{*})q(\bm{u}_{l})d\bm{u}_{l} \nonumber
\end{align}
where $p(y_{l}^{*}|\bm{u}_{l}, \bm{z}^{*}) = \mathcal{N}(K_{*m}K_{mm}^{-1}\bm{u}_{l}, K_{**} - K_{*m}K_{mm}^{-1}K_{m*}+ \sigma^{2}_{y})$ and $q(\bm{u}_{l}) = \mathcal{N}(\bm{m}^{*}_{l}, S^{*}_{l})$ refers to the optimised variational distribution. The final integral is tractable and gives the following form:
\begin{equation}
    p(y_{l}^{*}|\bm{z}^{*}) = \mathcal{N}(K_{*m}K_{mm}^{-1}\bm{m}^{*}_{l}, K_{*m}K_{mm}^{-1}(S^{*}_{l} - K_{mm})K_{mm}^{-1}K_{m*} + \sigma^{2}_{y})
\end{equation}
and similarly for any $x^{*}_{d}$.
For cross-modal reconstruction (where we only observe one modality of the test data) the latent $\bm{z}^{*}$ acts as the information bottleneck, hence, the same posterior predictive distributions can be derived, $\bm{x}^{*} \rightarrow \bm{z}^{*} \rightarrow p(y_{l}^{*}|\bm{z}^{*})\quad \forall l=D+1,...,D+L.$

\section{Schematic of the Model}

In \cref{schematic} we present a schematic of the model architecture with two observation spaces $(X,Y)$, the corresponding stacks of individual GPs $\{f_{d}\}$ and $\{f_{l}\}$ which model the individual columns of the spectra $X$ and scientific attributes $Y$, respectively, and the low-dimensional latent space $Z$. The dimensionality of the latent and observation spaces is denoted by $Q$, $D, L$, respectively, and $N$ denotes the number of objects / data points (quasars). Note that the correlation between the two observation spaces is not explicitly but implicitly modelled through a shared latent space. Generating a single data point $(\bm{x}_{n}, \bm{y}_{n})$ (a row across $X$ and $Y$) entails a forward pass through the GPs, where $\bm{x}_{n} = [\ldots, x_{nd}, \ldots]$ is generated as $[f_{1}(\bm{z}_{n}), f_{2}(\bm{z}_{n}), \ldots, f_{D}(\bm{z}_{n})]$ and $\bm{y}_{n} = [\ldots, y_{nl}, \ldots]$ is generated as $[f_{D+1}(\bm{z}_{n}), f_{D+2}(\bm{z}_{n}), \ldots, f_{D+L}(\bm{z}_{n})]$.
\begin{figure}[t]
\centering
\vspace{-8mm}
\includegraphics[scale=0.80]{plots/schematic.pdf}
\caption{Shared GPLVM with multiple observation spaces. The blocks on the right-hand side denote the double observation spaces $(X,Y)$ of quasar spectra and scientific labels respectively. In the center are two stacks of GPs, one for each observation space which control the data generation process through the shared latent space. In the figure above we assume $Q=2$ (for ease of visualisation) since we denote the GPs are two dimensional surfaces, however, typically $Q$ can be higher than 2 corresponding to higher dimensional GPs.}
\label{schematic}
\end{figure}

\section{Algorithm}

We enclose the pseudo-code in Algorithm \ref{algorithm} for stochastic variational inference in the context of the shared model for clarity. Let $\mathcal{L}_{x}$ and $\mathcal{L}_{y}$ denote the ELBO's for each of the observation spaces and let $\mathcal{L}_{x}^{(B)}$ and $\mathcal{L}_{y}^{(B)}$ denote the ELBOs formed with a randomly drawn mini-batch of the data (across all dimensions). For a mini-batch (subset) of the data $X_{B} \subset X$, the mini-batch ELBO is given by,
\begin{align}
\mathcal{L}_{x} \simeq \mathcal{L}_{x}^{(B)} = \dfrac{N}{B}\left(\sum_{b,d}\langle\log p(x_{b,d}|\bm{f}_{d}, \bm{z}_{b}, \sigma^{2}_{x})\rangle_{p(\bm{f}_{d}|\bm{u}_{d}, Z)q(\bm{u}_{d})} + \sum_{b}\log p(\bm{z}_{b})\right) - \sum_{d}\text{KL}(q(\bm{u}_{d})||p(\bm{u}_{d}|\tilde{Z}))
\end{align}
where the scaling term is important for the mini-batch ELBO to be an estimator of the full-dataset ELBO. 

\begin{algorithm}[h]
\SetAlgoLined
\footnotesize
\caption{Training Framework}
\vspace{2mm}
\vspace{3mm}
\textbf{Input:} ELBO objective $\mathcal{L} = \mathcal{L}_{x} + \mathcal{L}_{y}$, gradient based optimiser \texttt{optim()}, observation spaces $X$ (spectra) and $Y$ (scientific labels) \\ 
Initial model params: \\
\quad $\theta = (\theta_{x}, \theta_{y})$ (covariance hyperparameters for GP mappings $f_{1:D}, f_{D+1:D+L}$), \\
\quad $\sigma^{2} = (\sigma^{2}_{x}, \sigma^{2}_{y})$ (variance of the noise model for each likelihood), \\
\quad $Z \equiv \{\bm{z}_{n}\}_{n=1}^{N}$ (point estimates for latent embedding)\\
 \vspace{1mm}
Initial variational params: \\
\quad $\tilde{Z} \in \mathbb{R}^{M \times Q}$ (inducing locations), \\
\quad $\lambda = \{m_{h}, S_{h} \}_{h=1}^{D+L}$ (global variational params for inducing variables per dimension $\bm{u}_{h}$), \\
\vspace{2mm}
\While{not converged}{ 
\begin{itemize}
  \setlength{\itemsep}{0pt}
   \item Choose a random mini-batch of the data from both the observation spaces $X_{B} \subset X, {Y}_{B} \subset Y$. \\
   %\item Sample $J$ samples from the noise distribution $\epsilon^{(j)} \sim \mathcal{N}(0,\mathbb{I}_{Q})$. \\
   \item Form a mini-batch estimate of the ELBO:  $\mathcal{L}_{x}^{(B)}+\mathcal{L}_{y}^{(B)}$\\
   \item Gradient step for global parameters $\bm{g} \leftarrow \nabla_{\theta, \sigma^{2}, \tilde{Z}, \lambda}(\mathcal{L}_{x}^{(B)}+\mathcal{L}_{y}^{(B)})$
    \item Gradient step for local parameters $\bm{l} \leftarrow \nabla_{Z_{B}}(\mathcal{L}_{x}^{(B)}+\mathcal{L}_{y}^{(B)})$ (where $Z_{B}$ are the latent embeddings corresponding to points in the mini-batch)
   %\qquad $\mathcal{L}(X_{B}, Y_{B}) = \dfrac{N}{B}\left(\sum_{b}\sum_{d}\mathcal{L}_{b,d} \right) - \sum_{d}\textrm{KL}(q(\bm{u}_{d})||p(\bm{u}_{d}|Z))$\\
   \item Update all parameters $\tilde{Z}, \theta, \sigma^{2}, \lambda, Z_{B} \equiv \{\bm{z}_{b}\}_{n=1}^{B} \longleftarrow $ \texttt{optim}$()$ using gradients $\bm{g}, \bm{l}$
\end{itemize}
}
\Return{$\theta, \sigma^{2}, \tilde{Z}, \lambda, Z$}\\
\end{algorithm}
\begin{algorithm}[h]
\SetAlgoLined
\footnotesize
\caption{Prediction Framework}
\vspace{5mm}
(Predict $Z^{*}$ corresponding to test $X^{*},Y^{*}$)\\
\vspace{3mm}
\textbf{Input:} Trained global and local parameters $\theta, \sigma^{2}, \tilde{Z}, \lambda, Z$, test observation spaces $X^{*}$ (spectra) and $Y^{*}$ (scientific labels).\\
1. Initialise latent embedding $Z^{*} \equiv \{\bm{z}_{n^{*}}\}_{n^{*}=1}^{N^{*}}$ corresponding to test points.\\
2. Extend the joint ELBO to include terms corresponding to the $N^{*}$ additional data points. \\
\quad \quad $\mathcal{L}_{x}^{*} \leftarrow \mathcal{L}_{x} + \sum_{n^{*},d}\langle\log p(x_{n^{*},d}|\bm{f}_{d}, \bm{z}_{n^{*}}, \sigma^{2}_{x})\rangle_{p(\bm{f}_{d}|\bm{u}_{d}, Z)q(\bm{u}_{d})} + \sum_{n^{*}}\log p(\bm{z}_{n^{*}})$ \\
\quad \quad $\mathcal{L}_{y}^{*} \leftarrow \mathcal{L}_{y} + \sum_{n^{*},l}\langle\log p(y_{n^{*},l}|\bm{f}_{l}, \bm{z}_{n^{*}}, \sigma^{2}_{y})\rangle_{p(\bm{f}_{l}|\bm{u}_{l}, Z)q(\bm{u}_{l})} + \sum_{n^{*}}\log p(\bm{z}_{n^{*}})$\\
\quad \quad $\mathcal{L}^{*} \leftarrow \mathcal{L}_{x}^{*} + \mathcal{L}_{y}^{*}$ \\
3. Freeze all global and local parameters except for $Z^{*}$\\
\While{not converged}{ 
\begin{itemize}
\item Gradient step for $Z^{*}$: $\bm{l}^{*} \leftarrow \nabla_{Z^{*}}\mathcal{L}^{*}$
\item Update $Z^{*} \leftarrow \texttt{optim}()$ using gradients $\bm{l}^{*}$. 
\end{itemize}
}
\textbf{return} $Z^{*}$ \\
\textit{(Note that the gradients of $\mathcal{L}_{x}$ and $\mathcal{L}_{y}$ with respect to $Z^{*}$ are 0 and the only terms that are optimised are the additional terms corresponding to the new data points.)}\label{algorithm}
\end{algorithm}

\subsection{Computational Cost}

The training cost of the canonical stochastic variational GPLVM is dominated by the number of inducing points $\mathcal{O}(M^{3}D)$ (free of $N$) where $M \ll N$ and $D$ is the data-dimensionality (we have $D$ GP mappings $f_{d}$, one per output dimension). The practical algorithm is made further scalable with the use of mini-batched learning. In our shared model with two sets of GPs, the dynamics of the training cost are the same except that they increase linearly in the number of additional dimensions ($L$), making the cost $\mathcal{O}(M^{3}(D + L))$. The number of global variational parameters to be updated in each step (parameters of $q(U)$) is $MQ + M(D+L) + M^{2}(D+L)$, where $MQ$ are the $M$ $Q$-dimensional inducing inputs $\tilde{Z}$ (shared), $M(D+L)$ is the size of the mean parameters of the inducing variables $\bm{u}_{d}$, $\bm{u}_{l}$ and $M^{2}(D+L)$ are the full-rank covariances of the inducing variables. The local variational parameters $Z$ (the latent embedding shared across GPs) are of size $NQ$ and model hyperparameters (kernel hyperparameters) are of size $2Q + 4$, which account for $Q$ input lengthscales, a scalar signal variance and noise variance per GP group $\{f_{d}\}$ and $\{f_{l}\}$.  We use the squared exponential kernel with automatic relevance determination across both sets of GPs. 

% \label{algo}
% \begin{algorithm*}[h]
% \SetAlgoLined
% \label{algorithm}
% \footnotesize
% \caption{Shared Stochastic Gaussian process decoders}
% \vspace{2mm}
% \textbf{Input:} ELBO objective $\mathcal{L}$, gradient based optimiser \texttt{optim()}, Observation spaces $X$ (spectra) and $Y$ (labels) \\ 
% Initial model params: \\
% \quad $\theta_{x}$ (covariance hyperparameters for GP mappings $f_{1:D+L}$), \\
% \quad $\sigma^{2}$ (overall variance of the noise model), \\
% \quad $Z \equiv \{\bm{z}_{n}\}_{n=1}^{N}$ (point estimates for latent covariates)\\

%  \vspace{1mm}
% Initial variational params: \\
% \quad $\tilde{Z} \in \mathbb{R}^{M \times Q}$ (inducing locations), \\
% %\quad $\phi = \{\mu_{n}, s_{n} \}_{n=1}^{N}$  (local variational params per latent point $\bm{x}_{n}$, alternatively weights of the neural network encoder),\\
% \quad $\lambda = \{m_{h}, S_{h} \}_{h=1}^{D+L}$ (global variational params for inducing variables per dimension $\bm{u}_{h}$), \\
% %\quad $\eta = \{(\bm{r}_{p})_d, (h_{p}) \}_{p=1}^{P}{}_{d=1}^D$ (global variational params for random effects $B$) \\
% \vspace{2mm}
% \While{not converged}{ 
% \begin{itemize}
%   \setlength{\itemsep}{0pt}
%    \item Choose a random mini-batch $X_{B} \subset X, {Y}_{B} \subset Y$. \\
%    %\item Sample $J$ samples from the noise distribution $\epsilon^{(j)} \sim \mathcal{N}(0,\mathbb{I}_{Q})$. \\
%    \item Form a mini-batch estimate of the ELBO: \\
%    \qquad $\mathcal{L}(X_{B}, Y_{B}) = \dfrac{N}{B}\left(\sum_{b}\sum_{d}\mathcal{L}_{b,d} \right) - \sum_{d}\textrm{KL}(q(\bm{u}_{d})||p(\bm{u}_{d}|Z))$\\
%    \item Gradient step: $\tilde{Z}, \theta, \sigma^{2}, \lambda, Z_{B} \equiv \{\bm{z}_{n}\}_{n=1}^{B} \longleftarrow $ \texttt{optim}$(\mathcal{L}(X_{B}, Y_{B}))$
% \end{itemize}
% }
% \Return{$ \theta, \sigma^{2}, \tilde{Z}, \lambda, Z$}
% \end{algorithm*}

% Each GP $f_{d}$ and $f_{l}$ is a $Q$-dimensional GP which take as input $Q$ dimensional vectors $\bm{z}_{n}$ to output the $D$ dimensional vector $\bm{x}_{n} = [f_{1}(\bm{z}_{n}), \ldots, f_{D}(\bm{z}_{n})]$ and $L$ dimensional vector $\bm{y}_{n} = [f_{D+1}(\bm{z}_{n}), \ldots f_{L}(\bm{z}_{n})]$. Hence, the generative model for the data is given by, 
% \begin{align}
% x_{n,d} &= f_{d}(\bm{z}_{n}) + \epsilon_{n,d}, \epsilon_{n,d} \sim \mathcal{N}(0, \sigma^{2}_{x})\\
% y_{n,l} &= f_{l}(\bm{z}_{n}) + \epsilon_{l,d}, \epsilon_{l,d} \sim \mathcal{N}(0, \sigma^{2}_{y}) 
% \end{align}
% The priors over the function values are given by, 
% \begin{align}
%     p(\bm{f}_{d}|\theta_{x}) &= \mathcal{N}(\bm{0}, K_{nn}^{(d)} + (1 + \beta)C_{d})  \\
%     p(\bm{f}_{l}|\theta_{y}) &= \mathcal{N}(\bm{0}, K_{nn}^{(l)} + C_{l})
%     \label{priors}
% \end{align}
% where $K_{nn}^{(d)}$ and $K_{nn}^{(l)}$ denote the $N \times N$ kernel matrices which rely on their own set of hyperparameters (see \cref{shared}) but differ only on the diagonal due to the additive components $C_{d}$ and $C_{l}$ which are dimension specific diagonal noise matrices. \hl{@Vidhi: are you taking the noise into account now? Need to explain $\beta$ here, which is just a fudge factor, maybe better to omit? } Note that the infinite functions $f_{d}$ and $f_{l}$ have underlying GP priors, 

% Point estimates of $\theta_{x}, \theta_{y}$ and $Z$ are learnt by taking gradients of the joint log marginal likelihood objective in eq. \ref{lml}. Note that the objective log-marginal likelihood derived above only factorises across dimensions $D$ and $L$, hence the entire covariance matrices $K_{nn}^{(d)}$ and  $K_{nn}^{(l)}$ of size $N \times N$ need to be inverted at each training iteration, costing $\mathcal{O}(N^{3}).$  

% The base kernels $k_{\theta_{x}}$ and $k_{\theta_{y}}$ are given by the SE-ARD kernel with $Q$ lengthscales and a scalar signal amplitude $\sigma^{2}_{f}$ collected in the hyperparameter vector $\theta_{x}$ and $\theta_{y}$, we assume each set of GPs $f_{1:D}$ and $f_{D+1:L}$ have $Q+1$ hyperparameters.
\section{Related methodological work}
In this section we present related work in multi-output Gaussian processes in more detail. Canonical multi-output Gaussian processes almost always refer to the supervised framework or multi-output regression \citep{alvarez2010efficient} where the targets $\{\bm{x}_{d}(\bm{z})\}_{d=1}^{D}$ are multidimensional, continuous and Gaussian distributed corresponding to inputs $\bm{z} \in \mathbb{R}^{p}$. The main focus is on defining a suitable cross-covariance function between the outputs, this allows treatment of the outputs as a single GP with a suitable covariance function \citep{alvarez2012kernels}. The intrinsic and linear co-regionalization models \citep{goovaerts1993spatial, journel1976mining} are a popular approach in which each output is modelled as a weighted sum of shared latent functions (GP) where typically the number of latent processes is smaller than the number of outputs enabling efficiencies. To some extent, multitask learning with GPs can be viewed as an instance of multi-output learning where we want to avoid tabula rasa learning for each task and evolve a framework for sharing information between multiple tasks \citep{bonilla2007multi}. Further, there is the simplistic paradigm where each output is modelled with its own independent single-output GP and no correlation between the outputs is assumed. This approach, while easy to implement, is severely limited in its ability to jointly model the outputs.   

In the unsupervised paradigm, the starting point is a high-dimensional data matrix $N \times D$. The conventional Gaussian process latent variable model (GPLVM) operates like a multi-output model by default where each column of the data is modelled by an independent GP on the same shared set of inputs, kernel function and hyperparameters. The GPLVM is a decoder-only model, and some of their prominent variants are the back-constrained GPLVM \citep{lawrence2006local}, the supervised GPLVM \citep{jiang2012supervised}, the discriminative GPLVM \citep{urtasun2007discriminative} and the shared GPLVM \citep{ek2009shared}. The latter considers the task of dealing with multiple views or observation spaces (each of which is high-dimensional). This work is built on the idea of a shared latent space underlying the multiple observation spaces similar to \citet{ek2007gaussian} but adapts it for scalable inference using stochastic variational inference (SVI). The stochastic variational GPLVM was proposed in \citet{lalchand2022generalised}, but only considered a single observation space as in a canonical GPLVM.

\section{Experiments}
\label{exp}
In this section, we demonstrate a range of experiments aimed at assessing the reconstruction quality of test (unseen) quasar spectra and scientific attributes, as well as the robustness of uncertainty quantification by computing the negative log predictive density (NLPD) of test labels $-\log p(Y^{*}|Z^{*})$ under the predictive distribution where $Z^{*}$ has been informed by both modalities (spectra and labels) and only spectra.

The data used in this work are quasar spectra observed as part of the Sloan Digital Sky Survey (SDSS) DR16 \citep{Lyke2020}. We chose all quasars with spectra that have a signal-to-noise ratio (SNR) per pixel $>10$, which results in a total of $22844$ quasar spectra. The observed spectra are shifted into the rest frame wavelength space, re-binned onto a common wavelength grid, and flux normalized to unity at around $2500$~{\AA}. We mask strong absorption lines that might arise in the spectra due to foreground galaxies along our line-of-sight to the quasar, and are thus not intrinsic spectral features of the quasar. 
The four scientific labels for these quasars are (1) their SMBH mass, (2) their bolometric luminosity, i.e.\ the total power output across all electromagnetic wavelengths, (3) their redshift which denotes the factor by which the emitted wavelengths have been ``stretched'' due to the expansion of the universe, and (4) their Eddington ratio, which is a measure of the accretion and growth rate of the SMBH. All measurements were previously uniformly determined by \citet{WuShen2022}. We conduct experiments across two data sets with 1k (see \cref{tab:err2}) and 22k points (as an ablation to assess performance with a smaller dataset). The "Baseline" model in the experiments refers to the canonical stochastic variational GPLVM \citep{lalchand2022generalised} which treats multiple observation spaces using the same set of independent GPs learning a single set of kernel hyperparameters. The "Specialised" model refers to the shared model but with individual hyperparameters for each scientific label.

\begin{table}[t]
%\hspace{89mm}$\overbrace{\hspace{32mm}}$
\centering
\begin{tabular}{l|c|c|c}
Metrics ($\longrightarrow$) & \multicolumn{3}{c}{Mean Absolute Error}  \\
\hline
Models ($\longrightarrow$) & Baseline & Shared (\textcolor{blue}{ours}) &  Specialised (\textcolor{blue}{ours}) \\
\hline
Attributes ($\downarrow$) &  &  &     \\
\hline \hline 
Spectra & 0.1103 $\pm$ 0.0035 & 0.0972 $\pm$ 0.0024 & 0.0969 $\pm$ 0.0013  \\
Bolometric Luminosity & 0.2444 $\pm$ 0.0043 & 0.2144 $\pm$ 0.0033   & \textbf{0.2069 $\pm$ 0.0014} \\
Black hole mass & 0.2712 $\pm$ 0.0031 & \textbf{0.2319 $\pm$ 0.0028}  & 0.2362 $\pm$ 0.0025  \\
Eddington Ratio & 0.2525 $\pm$ 0.0070 & \textbf{0.2118 $\pm$ 0.006} &  0.2226 $\pm$ 0.0015
\end{tabular}
\caption{Summary of test-time reconstruction abilities. Mean absolute error on denormalised data ($\pm$ standard error of mean) evaluated on average of 5 splits with 75$\%$ of the data used for training in the 22k dataset. The shared and specialised versions of the model outperform the baseline model in all the reconstruction tasks. In the shared vs. specialised comparison the performance difference is not statistically significant for the spectra reconstruction but overall favours the shared model with hyperparameter sharing across the scientific label reconstruction.}
\label{tab:err}
\end{table}

\subsection{Reconstructing Quasar spectra}
We assess the quality of our probabilistic generative model in reconstructing test quasar spectra. At test time we deal with spectra and scientific labels from test quasars denoted by $(X^{*}_{\text{gt}}, Y^{*}_{\text{gt}})$ (we use the index `gt' to denote ground truth). The 2-step prediction learns the low-dimensional shared latent variables $Z^{*}$ (point estimate per data point) which acts as input to the GP decoder predicting the corresponding spectra $Z^{*} \rightarrow X^{*}_{\text{est}}$. Note that the ground truth spectra contains several missing pixels (dimensions) and the probabilistic decoder provides a reasonable reconstruction at those locations. In \cref{fig:spectra_recon} we visualise the reconstruction (posterior predictive mean) of four test quasars along with ground truth measurements and $95\%$ prediction intervals. We achieve remarkably good reconstruction estimates from the latents; further, the prediction intervals capture the ground truth spectra providing robust coverage at peaks and extrapolated regions.    
\begin{figure}
    \centering
    \vspace{-7mm}
    \includegraphics[scale=0.70]{new_plots/final_test_spectra_recon1.png}
    \caption{Reconstruction plots of test quasar spectra with $\pm 1.96\sigma$ intervals. The blue curve denotes the posterior predictive mean at each dimension.}
    \label{fig:spectra_recon}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[scale=0.65]{new_plots/final_partial.png}
    \caption{Reconstruction of a single spectra from the latent informed by a partially observed spectrum. The shaded orange regions denote the ``observed'' wavelength regions for this experiment. Note in the 4th panel the $95\%$ prediction intervals are wider as they were observed over a shorter and less informative wavelength window.}
    %% not sure I understand the last sentence here. Do you just want to emphasize that less given data/information increases the predicted uncertainties? 
    %% the legend is missing a "=" again. 
    \label{fig:partial_recon}
\end{figure}
\subsubsection{Reconstructing missing spectra}

In this experiment, we test the generative model's ability to learn from massively missing chunks of the spectra at test-time. We observe a partial window of the spectra in each plot (given by the shaded region in \cref{fig:partial_recon}), hence the latent variables corresponding to these points are only informed by the observed region. We then reconstruct the whole spectrum from the latent variables informed by the partial spectra. We enclose our results in \cref{fig:partial_recon}. Reconstruction entails the following inference steps: $X^{*}_{\text{partial}} \rightarrow Z^{*} \rightarrow X^{*}_{\text{full}}$. We note that the quality of the mean prediction deteriorates compared to the fully observed test point predictions. However, the coverage of the prediction intervals is robust even as we move away from the shaded observed regions. Furthermore, if the model is given a spectral region with very little information (e.g., in the bottom panel of \cref{fig:partial_recon}, the shaded region contains information only about the quasar's continuum), the uncertainties increase significantly, as one would expect. 

\begin{figure}[h]

    \hspace{-10mm}\includegraphics[scale=0.75]{new_plots/triple_final.pdf}
    \caption{Scientific label prediction for the quasars' bolometric luminosity (left), black hole mass (middle) and Eddington ratio (right) colored by the SNR of their spectra based on test $X^*$ \textbf{only}. The dashed black line (\protect\blackline) denotes the 1-to-1 line to aid visualisation of reconstruction accuracy. The vertical and horizontal errorbars (\protect\orangeline) denotes posterior predictive standard deviation.}
    \label{fig:label_recon_split}
\end{figure}
\subsection{Predicting scientific labels \emph{only} from spectra \texorpdfstring{$X^{*}$}{}}

The dimensions $L$ corresponding to the scientific labels in the dataset governed by their own GP decoders $\{f_{l}\}_{l=D+1}^{D+L}$ are a critical prediction quantity. The ability to reconstruct these quantities from learned latent variables is an important test of the generalizability of our model. Very often astronomers want to reason about the scientific attributes of quasars just by analysing their spectra. In this experiment, we demonstrate precisely this use case in which latent variables $Z^{*}$ are informed only by spectra $X^{*}$, computing the cross-modal prediction involves learning $Z^{*}$ from the ground truth spectra and then using just $Z^{*}$ to predict the scientific labels $Y^{*}$, succinctly, we can write these steps as: $X^{*}_{} \rightarrow Z^{*} \rightarrow Y^{*}$.

In \cref{fig:label_recon_split} we demonstrate the accuracy of our reconstructions by plotting each of the dimensions against ground truth held-out data. We show reconstructions for 200 test points randomly sampled from the full test set. Each point in the scatter denotes a quasar, and the x-axis denotes the ground truth measurement. The orange vertical error bars denote the intervals $1\sigma$ computed by extracting the diagonals from the predictive posterior GP for each dimension. We can observe a high degree of prediction accuracy across the three scientific labels and, furthermore, the reconstruction quality is weakly correlated to the spectral signal-to-noise ratio (SNR) (at least beyond the data quality cut of SNR > 10 which we apply as a preprocessing step); this is mainly due to acute data imbalances where more than $90\%$ of the objects appear at the lower SNRs affecting prediction quality and generalization for the quasars at higher SNRs (see \cref{sens}). Note that we did not attempt to reconstruct the redshift label from the spectra $X$. This is because the spectra have already been normalized to account for their redshift. Specifically, the spectra were shifted to rest-frame wavelength space by dividing the observed wavelengths by \(1 + z\) (where \(z\) is the redshift). As a result, normalized spectral shapes no longer contain significant information about their redshift (as shown in \citet{Yang2021} and \citet{Onorato2024}).

However, we still include the redshift information in our multimodal GPLVM. This is because excluding it marginally weakens the results, both for spectra reconstruction and label prediction. We hypothesize that, while the spectral shapes themselves do not carry redshift information, the patterns of missing pixels may still encode some redshift-related information.

%Note that we omit the reconstruction of the redshift label $z$, as we do not expect any or only very little redshift evolution in the spectral shape of quasars (as shown previously for large samples of quasar spectra by \cite{Yang2021, Onorato2024}), but rather the GPLVM is likely learning where spectral pixels are measured vs.\ missing. Since we are using data from SDSS, all spectra have the same observed wavelength coverage, which is then shifted to the rest-frame wavelengths by dividing by $(1+z)$, and thus the observed pixel dimensions in our data set contains the redshift information, rather than any spectral differences. 
%location of which pixels contain measurements

\begin{table}[t]
%\hspace{89mm}$\overbrace{\hspace{32mm}}$
\centering
%\footnotesize{
\begin{tabular}{l|c|c|c}
Experiment & Luminosity & Black hole mass & Eddington Ratio \\
\hline \hline 
Baseline (Fully observed) &   0.076 $\pm$ 0.0012 & 0.148 $\pm$ 0.0043 & 0.1178 $\pm$ 0.0031\\ 
Baseline (Spectra observed) &  0.1143 $\pm$ 0.042 & 0.1742 $\pm$ 0.0621 & 0.1376 $\pm$ 0.0646\\ 
Shared (Fully observed) &  \textbf{-0.029 $\pm$ 0.0018} & \textbf{0.1009 $\pm$ 0.0013} & \textbf{0.0702 $\pm$ 0.018}\\ 
Shared (Spectra observed) &  0.0862 $\pm$ 0.051 & 0.1604 $\pm$ 0.0245 & 0.1294 $\pm$ 0.0461\\ 
\end{tabular}%}
\caption{Summary of test-time uncertainty quantification under the full and partial reconstruction framework. Mean negative log predictive density (lower is better) on de-normalised data across 5 train/test splits. A higher NLPD indicates lower confidence in the predictions and the model indicates this when reconstructing the labels of a quasar just on the basis of its spectrum. The fully observed model corresponds to predictions based on test data when the all modalities are observed and reconstructed, $(X^{*}_{\text{gt}}, Y^{*}_{\text{gt}}) \rightarrow Z^{*} \rightarrow Y^{*}_{\text{est}}$ or when only the spectrum of the quasar is informing the latent, ($X^{*}_{\text{gt}} \rightarrow Z^{*} \rightarrow Y^{*}_{\text{est}}$). The latents are in turn used to decode the scientific labels.}
\label{tab:nll}
\end{table}

From \cref{tab:nll} we can see that the variability of NLPD estimates is much higher under the cross-modal protocol of decoding scientific labels on the basis of the spectrum. In other words, reconstructing scientific labels from spectra alone has a much higher uncertainty than reconstructing scientific labels from both modalities. As a result, the shared model offers only a subtle improvement over the baseline model in terms of uncertainty quantification. In the fully observed protocol, the shared model is more confident (\cref{tab:nll}) and accurate (\cref{tab:err}) in its predictions relative to the baseline model. It is also important to note that under the much harder cross-modal protocol, the performance of the shared model surpasses that of the baseline model.

\begin{figure}
    \centering
    \includegraphics[scale=0.50]{new_plots/bhm_spectra1.pdf}
    \includegraphics[scale=0.49]{new_plots/lumin_spectra1.pdf}
    \includegraphics[scale=0.50]{new_plots/edd_spectra1.pdf}
    \caption{An experiment demonstrating cross-modal prediction $Y^{*}_{} \rightarrow Z^{*} \rightarrow X^{*}$. In the plots we show generated quasar spectra for simulated labels for black hole mass, bolometric luminosity and Eddington ratio. In each plot we vary the respective scientific label in a reasonable range (shown by the range on the colorbar) while keeping the other labels to fixed values.}
    \label{fig:gen_spectra}
    %% could you add the units of Angstrom {\AA} to the x-axis label in these figures?
\end{figure}
\subsection{Generating spectra corresponding to synthesized labels: an ablation study}

In this experiment, we demonstrate the ability of the model to generate spectra corresponding to synthesized scientific labels. Concretely, we simulate artificial labels by systematically varying only one of the labels within a reasonable range in each plot in Fig.~\ref{fig:gen_spectra}. The range of variation for each label is summarized by the colorbar in each plot, for instance, in the black hole mass simulation we generate 100 spectra ($X^{*}$) corresponding to simulated labels ($Y^{*}$) of black hole masses $\log_{10}(M_\bullet/M_\odot)$ in the range $[7.9, 10.0]$; in order to ablate the influence of other labels (redshift, bolometric luminosity, and the Eddington ratio) on the spectra we keep their values fixed to mean values computed from the training dataset. Lastly, we can summarize the inference steps as the inverse of the previous section: $Y^{*}_{} \rightarrow Z^{*} \rightarrow X^{*}$. The ability for prediction and generation of cross-modalities is an important strength of our design.  

Reassuringly, the model exhibits the expected behaviour. For instance, the emission lines are broader for quasars with higher black hole masses, and the spectral dependence with bolometric luminosity shows e.g. in the Mg, II line at $\approx 2800{\AA}$ the well-known Baldwin effect \citep{Baldwin1977}, which indicates that quasar spectra show a decreasing equivalent width of their UV and optical emission lines with increasing bolometric luminosity.

In addition, the gray bars denote the $95\%$ prediction intervals averaged across the 100 spectra at each dimension. The uncertainty intervals are wider at higher wavelengths, as there is a high concentration of missing pixels in the training data at those wavelengths; hence, there is greater uncertainty about the generated spectra. 

\subsection{On benchmarking with a traditional regression approach}

The dependency between the spectra and labels is modelled through the shared latent space, which generates both views of the data. Without a generative model it would not be possible to generate spectra corresponding to scientific labels (generate 
$X$ corresponding to $Y$), we would only be able to predict labels based on fixed observed spectra. With a joint generative model, it is possible to generate both spectra and labels or generate one conditioned only on observing the other view through a jointly learned compression. In other words, we can extract the shared latent ($Z$) corresponding to $X$ (spectra) to predict a reconstructed estimate of the spectra and the labels $(\hat{X}, \hat{Y})$, or the other way round $Y \xrightarrow{optimise} Z \xrightarrow{decode} (\hat{X}, \hat{Y})$
 as shown in experiments. Further, the spectra are only partially observed with several missing pixels making canonical regression less straightforward as one would need to account for missing/uncertain inputs. In the case of training a GP to predict labels directly from fixed high-dimensional spectra, one would have to impute the missing pixels at the outset, as the prior covariance matrix cannot be computed on missing inputs. 

\section{Limitations}

The primary limitation of the model arises from the fact that GPLVMs are decoder-only models, they constitute a (potentially) smooth mapping from the latent space to the data space. This means that points close in latent space will be close in data space but not the other way round. Data space similarities can be preserved by including a back-constraint or an encoder that additionally maps the data to latent space \citep{lawrence2006local, bui2015stochastic}. The encoder usually takes the form of a neural network, but other choices are possible. As the title of the manuscript emphasizes, the model we propose is a decoder-only model. 
The absence of an encoder complicates the test-time inference as there is no deterministic way to access the latent points $\bm{z}^{*}$ corresponding to the new unseen observation $(\bm{x}^{*},\bm{y}^{*})$. Inferring the latent point corresponding to the unseen test point entails freezing the model and variational parameters post-training and re-optimizing the ELBO objective subject to ($\bm{z}^{*}$) with the new data point(s) included (see Algorithm \ref{algorithm}). 

This optimization procedure for test-time inference ends up being too inconvenient in contrast to encoder-decoder models such as VAEs \citep{kingma2013auto}, where inferring a latent point corresponding to an unseen data point entails a forward pass through the trained encoder network (constant-time predictions $\mathcal{O}(1)$). The main reason an extension to an autoencoded shared stochastic GPLVM is not straightforward is due to the presence of missing data. The encoder network must be capable of handling arbitrarily missing dimensions. Methods like the partial VAE \citep{ma2018partial,ma2018eddi} address this challenge in the context of VAEs where each observation is augmented with a column index indicating the observed dimension; the encoder network then processes these tuples as a `set' with a permutation invariant set encoding function. A similar approach can be straightforwardly adapted from our shared latent space setup with Gaussian process decoders. The combination of a set encoder (to process missing dimensions) and a non-parametric decoder has not appeared in the literature to the best of our knowledge. We are currently working on incorporating this feature into our framework. Note that the presence of an encoder acts as a constraint in the model, and there is an inherent trade-off in terms of faster test inference and marginally weaker reconstructions / predictions. 

\section{Methodological extensions for future work}

A natural extension of the shared GPLVM proposed here is to incorporate an encoder, bringing the whole architecture more in line with modern deep-generative autoencoder models while simultaneously preserving the advantage of uncertainty quantification in the outputs. \citet{bui2015stochastic} propose the canonical GPLVM with an encoder and train it with SVI, therefore, a natural extension is to demonstrate it for the shared setting. The shared setting in the data context presented here opens up questions about the encoding of partially observed vectors along with fully observed ones, and masked encoders \citep{he2022masked} might be a relevant architectural paradigm to explore. 

Another extension is related to the structure of the kernels, since the kernels factorise over dimensions and are closed over the multiplicative operation, one approach is to select a composite product kernel over dimensions of the latent space instead of two sets of GPs with their own respective kernels. The former approach (not studied here) induces a partitioning of the latent space into dimensions which would cater to specific views of the data and can be modelled with individual kernel functions. In our context, we did not see the gain in the mixture kernel formulation; note that this approach raises an additional question as to how many latent dimensions to allocate to each kernel function. Nevertheless, partitioning the latent space and modelling with mixed or composite kernels could be beneficial in specific settings. More experiments are required to understand the exact settings in which this approach may outshine the alternative framework presented here. 


\section{Scientific Interpretation and Significance}
\label{ss}
Our new generative model allows us to \textit{simultaneously} model the spectral properties of quasars as well as their scientific labels, thus opening up novel possibilities to study the evolution of quasars and their dependency on physical parameters governing the SMBH growth. In the following, we will highlight just two possible and exciting future applications of this work. 

Astronomers observe SMBHs with billions of solar masses in size in the center of very distant, high-redshift quasars, at a time when the universe is still in its infancy and only a few hundred Myr (million years) old. This rapid growth of SMBHs in the very short amounts of available cosmic time has been an open puzzle for decades, and it has been argued that very high accretion rates in excess of the theoretical upper limit, the so-called Eddington limit, are required to explain the rapid black hole growth. However, obtaining precise black hole mass measurements of quasars is challenging and time-intensive as it requires multi-epoch observations of certain emission lines in the quasar spectra \citep{Peterson1993, Barth2015}. This procedure becomes increasingly challenging for very distant, high-redshift quasars, as relativistic time-dilation effects require longer timespans of these observations. Furthermore, the traditionally used rest-frame optical emission lines to calibrate the black hole masses are unobservable with ground-based observatories, as these optical wavelengths have been shifted to the infrared at larger distances, which require space-based telescopes, such as NASA's recently launched James Webb Space Telescope. Using the new generative model provides us with the possibility to determine the masses of SMBHs for quasars at all redshifts from single-epoch data observed with ground-based observatories, as it does not require wavelength coverage of specific emission lines; further, it can handle missing data. 

Furthermore, we have shown that our model is able to predict other physical properties of quasars such as their bolometric luminosities (see Fig.~\ref{fig:label_recon_split}). This suggests that we can obtain a measurement of the quasars' absolute luminosities using their spectral information alone, which provides a new opportunity to use quasars as the so-called ``standard candles.'' Standard candles are incredibly valuable for astronomy, as knowing the luminosity of an object allows one to determine its distance. Famously, supernovae have been used as standard candles, which led to the Nobel Prize winning discovery of the expansion of our universe and the existence of dark energy \citep{Riess1998}. 
The use of quasars as standard candles has previously been suggested leveraging the relationship between the X-ray and UV luminosities of quasars to determine their distances \citep[e.g.][]{Lusso2017, Sacchi2022}. Our generative model enables predictions of the quasars' bolometric luminosities leveraging these spectral dependencies on luminosity, enabling the use of quasars as standard candles (or ``standardizable candles'') to map the expansion of the universe to larger distances than possible with supernovae. Further testing of the required precision with which the quasars' luminosities and thus distances can be predicted by the generative model will be necessary in order to determine whether the constraints on the universe's expansion rate obtained by the more numerous quasars can improve on the current constraints by the fewer but very accurately measured distances from supernovae. 


\newpage
\bibliography{main}
\bibliographystyle{tmlr/tmlr}
\newpage
\appendix

\section{The evidence lower bound \texorpdfstring{$\mathcal{L}_{x}$}{}}
\label{apd:first}

In this section, we explicitly show the derivation of expected log-likelihood term in \cref{factorelbo}:

\begin{align}
p(X) \geq & \int p(F|U, Z)q(U)\log p(X|F, Z)dF dU -  \text{KL}(q(U)||p(U)) + \log p(Z) 
\end{align}
\begin{align}
\hspace{-4mm}
 \int p(F|U, Z)q(U)\log p(X|F, Z)&dF dU = \int q(\bm{u}_{d}) \int p(\bm{f}_{d}|\bm{u}_{d},Z) \log \prod_{n=1}^N \prod_{d=1}^D \mathcal{N}(x_{n,d}; f_{d}(\bm{z}_{n}), \sigma^{2}_{x})d \bm{f}_{d}\bm{u}_{d} \nonumber \\
 &=  \int q(\bm{u}_{d}) \int p(\bm{f}_{d}|\bm{u}_{d},Z) \sum_{n,d} \log \mathcal{N}(x_{n,d}; f_{d}(\bm{z}_{n}), \sigma^{2}_{x})d \bm{f}_{d}\bm{u}_{d} \nonumber\\
&=  \sum_{n,d}\langle\log p(x_{n,d}|\bm{f}_{d}, \bm{z}_{n}, \sigma^{2}_{x})\rangle_{p(\bm{f}_{d}|\bm{u}_{d}, Z)q(\bm{u}_{d})} 
\end{align}

\subsection{Tractability of the expectation term}
The expectation term above is not just factorisable but also tractable, we show this explicitly below: 
\begin{align}
 \int q(\bm{u}_{d}) \int p(\bm{f}_{d}|\bm{u}_{d},Z) \sum_{n,d} \log \mathcal{N}(x_{n,d}; f_{d}(\bm{z}_{n}), &\sigma^{2}_{x})d\bm{f}_{d}\bm{u}_{d}  \\ &= \int q(\bm{u}_{d})\sum_{n,d}\log \mathcal{N}(x_{n,d}; k^{T}_{n}K_{mm}^{-1}\bm{u}_{d},\sigma^{2}_{x}) -\dfrac{1}{2\sigma^{2}_{x}}q_{n,n} \nonumber 
 \end{align}

 $x_{n,d}$ is a scalar ($d^{th}$ dimension of point $\bm{x}_{n}$), $k^{T}_{n}$ is a matrix $1\times M$ - the $n^{th}$ row of $K_{nm}$, we know that $p(\bm{f}_{d}|\bm{u}_{d}, Z) = \mathcal{N}(K_{nm}K^{-1}_{mm}\bm{u}_{d}, K_{nn} - K_{nm}K^{-1}_{mm}K_{mn})$. Further, $f_{d}(\bm{z}_{n})$ is a scalar, denoting the value at index $\bm{z}_{n}$ of the vector $\bm{f}_{d}$. $q_{n,n}$ is the $n^{th}$ entry in the diagonal of the matrix $Q_{nn} = K_{nn} - K_{nm}K_{mm}^{-1}K_{mn}$. Next, performing the integration with respect to $q(\bm{u}_{d}) = \mathcal{N}(\bm{u}_{d}; \bm{m}_{d}, S_{d})$ yields, 

 \begin{align}
 \int q(\bm{u}_{d}) \Big[\sum_{n,d}\log\mathcal{N}(x_{n,d};k^{T}_{n}K_{mm}^{-1}\bm{u}_{d},  \sigma^{2}_{x}) 
     - \dfrac{1}{2\sigma^{2}_{x}}q_{n,n} \Big] d\bm{u}_{d} 
 = \sum_{n,d}\Big[ \log \mathcal{N}(&x_{n,d};k^{T}_{n}K_{mm}^{-1}\bm{m}_{d}, \sigma^{2}_{x})  -\dfrac{1}{2\sigma^{2}_{x}}q_{n,n} \nonumber \\ &- \dfrac{1}{2\sigma^{2}_{x}}\textrm{Tr}(S_{d}\Lambda_{n})\Big]
 \end{align}

Bringing it all together, the final lower bound can be written out explicitly as, 
\begin{align}
 \mathcal{L}_{x} &= \sum_{n,d}\langle\log p(x_{n,d}|\bm{f}_{d}, \bm{z}_{n}, \sigma^{2}_{x})\rangle_{p(\bm{f}_{d}|\bm{u}_{d}, Z)q(\bm{u}_{d})} - \text{KL}(q(U)||p(U)) + \log p(Z) \nonumber \\
 &=  \sum_{n,d}\Big[\log \mathcal{N}(x_{n,d};k^{T}_{n}K_{mm}^{-1}\bm{m}_{d}, \sigma^{2}_{x})  -\dfrac{1}{2\sigma^{2}_{x}}q_{n,n} - \dfrac{1}{2\sigma^{2}_{x}}\textrm{Tr}(S_{d}\Lambda_{n})\Big] - \sum_{d}\text{KL}(q(\bm{u}_{d})||p(\bm{u}_{d})) \nonumber \\
 & \hspace{90mm} + \sum_{n} \log p(\bm{z}_{n})  
\end{align}
where all the data dependent terms factorise enabling mini-batching of gradients. The shared model uses a sum of ELBOs $\mathcal{L}_{x} + \mathcal{L}_{y}$ with a shared latent embedding $Z$, which we call the joint evidence lower bound (\cref{joint} in the main paper). The additive structure ensures that the joint ELBO is also factorisable across data points $N$. 

\section{Experimental set-up}
\label{apd:exp}
In this section we detail the configuration of the experiments in \cref{exp} of the main paper. For each of the datasets, we repeat every experiment with five random seeds yielding different splits of the training data. The attributes of the data and sparse GP set-up are given in \cref{tab:exp}.
\begin{table}[h]
    \centering
    \begin{tabular}{c|c|c|c|c|c}
    Dataset & $N$ & $ D $ & $L$ & Num inducing $M$ & Latent dim. $Q$ \\
    \hline 
        1K &  996 & 590 & 4 & 120 & 10\\
        22K & 22844 & 657 & 4 &  250 & 10 
    \end{tabular}
    \caption{Experimental configuration to reproduce experiments in section 3 of the main paper.}
    \label{tab:exp}
\end{table}
%% are those numbers correct? did we not use the same D in both data sets? And N seems different from a value you quoted above.
We used a learning rate of 0.001 across all parameters and ran the mini-batch loop with a batch size of 128 for 15,000 iterations on an Intel Core i7 processor with a GeForce RTX 3070 GPU with 8GB RAM memory. In order to give an estimate of the scale of the model for the 22k dataset we enclose a summary snapshot of the number of trainable parameters in our shared model.

\section{Testing on smaller sampled dataset}

\begin{table}[h]
%\hspace{89mm}$\overbrace{\hspace{32mm}}$
\centering
\begin{tabular}{l|c|c}
Metrics ($\longrightarrow$) & \multicolumn{2}{c}{Mean Absolute Error}  \\
\hline
Models ($\longrightarrow$) &  {Baseline} & {Shared (\textcolor{blue}{ours})}  \\
\hline
Attributes ($\downarrow$) &  &   \\
\hline \hline 
Spectra & 0.0731 $\pm$ 0.0020 & 0.0707 $\pm$ 0.0033  \\
Black hole mass & 0.1882  $\pm$ 0.0046  &  \textbf{0.1765 $\pm$ 0.0054}  \\
B. Luminosity & 0.1731 $\pm$ 0.0043 & 0.1658 $\pm$ 0.0084   \\
Eddington Ratio & 0.1707 $\pm$ 0.0024  & \textbf{0.1653 $\pm$ 0.0026} 
\end{tabular}
\caption{Summary of test-time reconstruction abilities for a smaller 1k dataset sampled with the same SNR data quality cut of SNR > 10. Mean absolute error on denormalised data ($\pm$ standard error of mean) evaluated on average of 5 splits with 90$\%$ of the data used for training in the 1k dataset. The performance improvement is not statistically significant for the spectra reconstruction and the bolometric luminosity prediction but favours the shared model for black hole mass and Eddington ratio.}
\label{tab:err2}
\end{table}

\begin{figure}
\hspace{-5mm}
\includegraphics[scale=0.5]{plots/params.png}
\caption{Number of trainable parameters for the 22k model where $\texttt{model\_spectra}$ refers to the GPs corresponding to $X$ and $\texttt{model\_labels}$ refers to the GPs corresponding to $Y$ observation space.}
\end{figure}

\section{Sensitivity to SNR}
\label{sens}
In \cref{fig:snr} we essentially plot the SNR per data point (test quasars) vs. the absolute error. The triangular scatter denotes a density imbalance: more data points are clustered at lower SNR values (~10–15) compared to higher SNR values (>20). This is just an artefact of the data quality cut we deploy. The points seem evenly scattered across the range of absolute errors and SNR values. There doesn’t appear to be a consistent trend where absolute error systematically increases or decreases with SNR. There is a very weak negative correlation (more discernable in the zoomed in plots in the bottom row) for the black hole mass and weak positive correlation for the luminosity. The unusual weak positive correlation can be better understood in the context of the count and missing pixels analysis (\cref{fig:count-bis}) in each SNR bin. There are very few high SNR (> 40) quasars in the dataset and further, these quasars also have more missing pixels than quasars at lower SNRs. Since the training dataset is biased towards lower SNR quasars, the model generalises poorly to higher SNR examples, for luminosity and Eddington ratio. It is highly likely that the pattern of missing pixels is similar among brighter objects and in turn among less luminous objects, making it difficult for the model to adapt to this shift at test time due to the acute data imbalance.  

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{plots/snr_v_error_final.png}
    \includegraphics[scale=0.4]{plots/snr_v_error_final_zoom.png}
    \caption{Top: SNR vs. error for test quasars, Bottom: SNR vs. error zoomed in on higher density regions}
    \label{fig:snr}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[scale=0.5]{plots/snr_v_error_binned.png}
    \includegraphics[scale=0.5]{plots/count_snr_bins.png}
    \caption{The SNR vs. error visualised with one standard deviation in each bin. The bottom plot shows the average number of missing pixels for the same SNR bins as well as the total number of quasars in each bin.}
    \label{fig:count-bis}
\end{figure}

\section{Error vs. Uncertainty Calibration}

In \cref{fig:calibration}, we quantify uncertainty calibration by computing the mean predictive uncertainty for increasing error levels. We bin test points into five evenly spaced intervals over the range of the mean absolute error for each label. The x-axis denotes the centers of the bins for each label and the predictive uncertainty is computed as the square root of the variance --- the diagonal of the GP posterior predictive distribution. In each label, we observe the trend of predictive uncertainty increasing with absolute error, although this effect is very subdued at lower error levels. Further, it is important to note that there is inherent noise in this calculation emanating from the differing number of points in each bin; this might explain some of the variability in the bolometric luminosity uncertainty estimates. 

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.35]{plots/calib_bolom.png}
    \includegraphics[scale=0.35]{plots/calib_bhm.png}
    \includegraphics[scale=0.35]{plots/calib_edd.png}
    \caption{Uncertainty calibration across different error levels for each label computed over test data. The x-axis denotes centers of evenly spaced bins across the range of the MAE.}
    \label{fig:calibration}
\end{figure}

\section{Cross-validating $M$ and $Q$}

The two main parameters of our shared framework which need to be fixed at the outset are: the number of inducing points $M$ and the latent space dimensionality $Q$. We set $M$ at 250 in all 22k experiments after cross-validation to $M=1000$ and found that $M=250$ gave the best possible trade-off in terms of speed and accuracy. The reconstruction results with $M=1000$ were only marginally better than with inducing points $M=250$, but the compute was significantly increased due to the cubic scaling in inducing points. 

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.8]{plots/joint_elbos_q.png}
    \caption{Sensitivity to Q: The negative ELBO objective for varying latent space dimensionality (lower is better)}
    \label{fig:elbos}
\end{figure}
In \cref{fig:elbos}, we visualize the evolution of the ELBOs across varying latent dimensionality. We notice a meaningful improvement in increasing the dimensionality from $Q=2$ but very marginal gains beyond $Q=10$; we use this setting in experiments. It may be important to highlight that due to \textit{automatic relevance determination} of the squared exponential kernel, setting a high latent dimensionality should not degrade results as the model automatically prunes redundant dimensions by driving the corresponding inverse lengthscales to 0. However, they do increase the compute cost, hence, it is important to set $Q$ at a reasonable value which is flexible enough for structure discovery and not too constrained, while simultaneously minimising the computational burden. 

\end{document}
