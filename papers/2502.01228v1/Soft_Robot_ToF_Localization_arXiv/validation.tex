\section{Experiments Description}
\label{sec:validation}

\subsection{Experimental Setup}
\label{sec:setup}

\begin{figure}[]
	\centering
	\includegraphics[width=0.98\columnwidth]{setup_rev.png}
	\caption{The experimental setup is shown. It consists of the cuboid environment (0.7x0.7x0.6 m) considered for the localization task (top left corner), the soft robot with reflective markers rigidly mounted on it (1), three VL53L5CX ToF sensors mounted on the robot tip (2), and the OptiTrack tracking system (3) with four cameras recording the reflective markers poses which constitute the ground truth for the pose estimation.}
	\label{fig:setup}
\end{figure}

We validated the proposed approach with the experimental setup showed in \cref{fig:setup}. 
The soft robot consists of a three-chambers bellow actuator where three \gls{tof} sensors have been integrated into the tip. The design of the actuator stems from the one adopted in ~\cite{veronese_robosoft} with the main difference being a central empty channel that accommodates the \gls{tof} wires. The multi-material 3D printer Stratasys J735\textsuperscript{\textregistered} (Stratasys Ltd, USA) was utilized to print in one batch both the soft bellows and the rigid edges used to connect the sensors support and the reflective markers. For the soft bellows, a Shore hardness of $40$A was achieved with a digital material blend of VeroCyan\textsuperscript{\textregistered} (Stratasys Ltd, USA) and the
rubber-like soft material Agilus30\textsuperscript{\textregistered} (Stratasys Ltd, USA). The support material required for the print was chemically dissolved in a tank by using a solution of \SI{0.02}{kg/L} Sodium Hydroxide and \SI{0.01}{kg/L}
Sodium Metasilicate. The printed actuator has an overall length of \SI{10}{cm}, while the diameter of its bellows is \SI{2.4}{cm}.

The specific range sensors used in this study are the VL53L5CX from ST MicroElectronics, featuring a pyramidal \gls{fov} of \SI{65}{\degree}. These are mounted through a rigid support connected to the tip of the robot, and their relative pose is given by the CAD model of the support. 
Each \gls{tof} sensor weighs approximately \SI{2}{\gram}. The majority of the weight of this setup is given by the breakout board and the prototyping electronics weighing around \SI{10}{\gram}. Sensors are connected with a microcontroller through an I2C bus. Each \gls{tof} sensor provides an $8 \times 8$ depth map at \SI{15}{\hertz} rate that can be converted into a point cloud as described in \cref{sec:methodology}.  

In order to create the point cloud of the environment $X_k$ which is updated with new samples, the relative configuration of the soft robot in space must be known. In this paper, we estimated the position of the robot's tip with respect to the robot base by training a simple $k$-NN regressor mapping the actuation pressure with the corresponding tip pose, measured by a \gls{mocap} system, as shown in~\cite{ouyang2022modular}. In this work, we used an OptiTrack with four cameras to precisely retrieve the position of the reflective markers attached to the tip. The position of the cameras with respect to the environment was such that the markers were clearly visible at all times throughout the experiments.

\subsection{Data Collection}
We performed the data collection described in the following to both: (i) train the $k$-NN model and (ii) collect point cloud samples.
The robot was actuated with increasing pressures ranging from 3 to 18 kPa, with increments of 1 kPa. We started inflating one chamber at a time and then actuated two chambers simultaneously using every combination of pressure values. This led to 807 different robot postures. For each commanded pressure combination, we waited \SI{4}{\second} to reach the steady state and then collected both the position of the tip, to train the $k$-NN model, and the \gls{tof} measurements, to be converted into point clouds samples.

%

The dataset composed of pressure sequence and tip poses measured by the OptiTrack was split into two parts, $80\%$ of the sample was used for training and the remaining $20\%$ for validating the model. The model is trained to predict the robot pose for a combination of pressures fed to the actuators. Similarly to~\cite{ouyang2022modular}, to devise the best-fitting number of neighbors for the model, the training set was also used in k-fold cross-validation~\cite{kohavi1995study}, with $k_{fold} = 5$. For the collected data, we found $k=6$ to be the best option with mean squared errors $MSE_t = 10^{-4}$ m on the training set and $MSE_v = 3\times10^{-4}$ m on the validation set. As will be extensively shown in~\cref{sec:results}, the high performance of the trained model allows for a seamless substitution of the OptiTrack tracking system since the robot's pose can be predicted with accuracy for a given pressure sequence fed to the actuators. 

For what concerns the collected point cloud samples, as explained in the next Section, various subsets of the 807 samples are randomly picked to generate the point cloud of the environment $X_k$ that is used for the localization task.

\subsection{Validation}

The proposed approach was validated in a localization task. The robot is rigidly mounted to the frame of the cuboid with a 3D printed support which constrains the robot's base to be parallel to the $x$-$y$ plane (see~\cref{fig:setup})
at a fixed, known distance. Due to this, the localization task simplifies to the estimation of the pose $\hat{\mathbf{x}}\in SE(2)$ as we consider uncertainty on the position in the horizontal plane and on the rotation around the $z$-axis. Formally, the goal is to estimate  $\hat{\mathbf{x}} = \lbrace \hat{x}, \hat{y}, \hat{\gamma} \rbrace$  with respect to the aluminum frame consisting of a cuboid with two open faces as can be seen in the figure.
%  
A model of this environment was created with CAD software, from the knowledge of the frame's size, and exported as a point cloud consisting of 2000 points. 
%

The localization was performed with a \gls{pf} composed of 1000 particles. 
To evaluate how many point cloud samples are required to localize the robot we ran \gls{pf} using point clouds $X_k$ created with a variable number of samples. In particular, we evaluated the localization accuracy for point cloud composed of $k =[1, \dots, 10 ]$ samples, randomly picked from the 807 available. In addition to this, for a given number of samples, we performed the localization in 50 trials by randomly drawing diverse point clouds. 

Each point cloud has at most $64\times3 = 192$ points and is cropped to remove those points that belong to the environment outside the cuboid. The $k$-th sample is transformed with respect to the robot base using the $k$-NN model of the arm. Prior to merging $\bar{X}_k$ into $X_k$, in a similar fashion to what is proposed in~\cite{vizzo2023kiss}, the point clouds are downsampled using a voxel grid with a voxel size of \SI{0.05}{m}.

Particles were initialized as follows. The first two components of the particle, representing the translation on the plane were initialized with random numbers picked from a distribution centered on the true position of the robot base with a deviation varying in a range of \SI{0.2}{\meter}. Similarly, the third component of the particle, representing the in-plane angle, was initialized with a random distribution centered in the true angle with a deviation varying in a range of \SI{20}{\degree}.
%
For the update of the $i$-th particle weight $w_k^i$, the point-to-point ICP variant is adopted with considering the downsampled point cloud $X_k$, the map $\hat{X}$, and the transform devised by the particle $\mathbf{p}_k^i$.

It is also worth noting that, in general, for soft robots, the estimation of the robot configuration is affected by uncertainties~\cite{della2023model}. In this scenario, errors related to the tip's position are propagated to the corresponding point cloud, thus affecting the reconstruction of the environment and, as a consequence, the localization.
In this respect, we wanted to investigate how much the pose estimation performed with the $k$-NN model affects the localization accuracy. Therefore, we replicated the same experiments discussed above by generating the point clouds using measured robot tip position obtained with the \gls{mocap} system showed in~\cref{fig:setup}, whose precision is in the order of $10^{-4}$m.
 