

\section{Introduction}\label{sec:intro}

\glsresetall

The compliance and adaptability of soft robots make them a valuable option to be deployed to perform tasks in constrained environments. Unlike traditional rigid robots, soft robots can deform into tight spaces, and navigate complex and cluttered surroundings safely~\cite{arezzo2017total, greer2018obstacle,wang2018toward, luong2019eversion, karimi_2023}. 
%
However, for these robots to operate effectively, they must also be localized with respect to the environment they are exploring~\cite{sorensen2021,rosi_2022}.
Therefore, self-localization or \gls{slam} is essential to achieve self-aware soft robots that can adapt their movements appropriately. 
% 

Localization and \gls{slam} have been largely investigated for autonomous driving and walking robots~\cite{gouda_2013,rybczak2024,kazerouni2022,macario2022}. These systems are usually equipped with lidars or high-resolution RGB-D cameras to support algorithms performing mapping and localization.
%
Despite their reliability, these sensors cannot be easily integrated into small soft robots since they are heavy and bulky. Although~\cite{sorensen2021} has used RGB-D cameras to perform \gls{slam}, the soft robot was considerably bigger than a classical RGB-D camera. % into small soft robots such as \cite{}. 
%
An alternative solution more suitable for smaller soft robots is represented by miniaturized small RGB cameras. In~\cite{rosi_2022}, this solution has proven to be effective in estimating the posture of a soft robot through \gls{slam} algorithms. 
%
However, for soft robots, a single camera is typically integrated at the tip pointing downward \cite{rosi_2022, diodato2018soft,albeladi2022hybrid, arezzo2017total, kim2021origami}, thus limiting the \gls{fov}. A possible solution to increase spatial awareness consists of distributing sensors on the robot body. However, cameras cannot easily scale in number. The integration becomes difficult, not to mention the high bandwidth required to transmit and process data from multiple cameras. 
In this respect, the work in \cite{karimi_2023} equipped the soft robot using single-beam distance sensors, which are easier to distribute with respect to cameras. The work showed the possibility of incrementally reconstructing a point cloud of a 2D maze by acquiring samples from the distance sensors over time. These sensors, along with Inertial Measurement Units, were used to perform a \gls{slam} task in a maze. 
% 

\begin{figure}[t!]
	\centering
	\includegraphics[width=0.98\columnwidth]{intro_rev.png}
	\caption{Soft robot estimating its pose with respect to the environment using a Particle Filter based on measurements collected with miniaturized ToF sensors. The faded robots represent the poses associated with different particles. For each sensor, the measured 8Ã—8 depth map is converted into a point cloud and exploited for localization. The colored points belong to the point cloud associated with the current pose, while the shaded gray dots are the ones previously collected. The robot is successfully localized by using the coarse information retrieved with the ToF sensors.}
 % An image of the adopted sensor and its dimensions are also reported. 
	\label{fig:intro}
\end{figure}

Beyond the single-measurement distance sensors used in~\cite{karimi_2023}, miniaturized lidars based on \gls{tof} technology have been recently introduced. The measurement provided by this typology of \gls{tof} sensor consists of a low resolution depth map, which can be converted into a small point cloud representing the surrounding environment falling into the sensor's \gls{fov}.  
%
These sensors have generated significant interest among researchers since they are cheap, lightweight, small, and capable of multi-point sensing at medium range (typically up to \SI{4}{\meter})~\cite{hughes2018robotic, tsuji2021sensor,mu2024towards}.
So far, their usage in robotics has been predominantly related to collision avoidance, pre-touch sensing, in-hand manipulation, object detection, and gesture recognition, when equipped on rigid robots~\cite{ding2019proximity, ding2020collision, caroleo2024proxy, al2020towards, yang2017pre, sasaki2018robotic, koyama2018high, ruget2022pixels2pose}.

We argue that their compact form factor makes them suitable for distributed integration into the robot body, thereby enhancing its spatial awareness. However, it is worth noting that despite their advantages in terms of size compared to lidar or RGB-D cameras, they are significantly less reliable. In particular, miniaturized \gls{tof} presents the following drawbacks:
\begin{itemize}
    \item low spatial resolution - recent model can provide at most an 8x8 depth map;
    \item pyramidal \gls{fov} - the spatial resolution of the obtained point cloud is increasingly coarse depending on the distance between the robot and the environment;
    \item noise and uncertainties on measurements depend on environmental conditions and the distance with respect to the target. 
\end{itemize}
%
Performing self-localization or \gls{slam} is challenging with this type of sensing system. Indeed, state-of-the-art algorithms developed for these tasks are generally applied to much denser point clouds and more reliable measurements. 

In this paper, we want to investigate whether miniaturized \gls{tof} sensors, equipped on a soft robot, can be effectively used to perform self-localization with respect to a known map, using state-of-the-art algorithms.
%
Specifically, to estimate the robot's pose, we considered using a \gls{pf} algorithm whose particle weights are updated by considering the \gls{icp} score. This computes how well the point cloud reconstructed with \gls{tof} sensors aligns with the map when the transformation devised by the given particles is adopted.  

\cref{fig:intro} shows the miniaturized \gls{tof} equipped at the tip of the robot, illustrating their \gls{fov}. The points colored in red, yellow, and purple show the current measurements of the sensors, while the gray, the point cloud of the environment constructed by merging \gls{tof} measurements at different time instants.
%

The paper is structured as follows. 
\cref{sec:methodology} formally introduces the localization problem using distributed \gls{tof} sensors. The experimental setup and the validation experiments are described in~\cref{sec:validation}. Results and discussion are reported in~\cref{sec:results}. Conclusion follows.


