
\section{Introduction}
Creating ensembles of classifiers is a classic technique in machine learning, and thus studying generalization of such ensembles is natural. An example of the success of ensembles of classifiers is boosting algorithms, which are one of the pillars of classic machine
learning, often significantly improving the accuracy of a base
learning algorithm by creating an ensemble of multiple base
classifiers/hypotheses. More formally, consider binary classification over an
input domain $\cX$ and let $\cA$ be a base learning algorithm that on a
(possibly weighted) training sequence $S \in (\cX \times \{-1,1\})^*$, returns a hypothesis
$h_S \in \{-1,1\}^{\cX}$ better than guessing on the weighted training sequence. Boosting algorithms, like AdaBoost~\cite{adaboostyoavfreund}
then iteratively invokes $\cA$ on reweighed version of the training sequence
$S$ to produce hypotheses $h_1,\dots,h_t$. In each step, the training
sequence $S$ is weighed to put more emphasis on data points that
$h_1,\dots,h_{t-1}$ misclassified, forcing $h_t$ to focus on these
points. The obtained hypotheses are finally combined
via a weighted majority vote to obtain the \emph{voting classifier} $f(x) =
\sign(\sum_i \alpha_i h_i(x))$ with $\alpha_i >0$ for all $i$.

Much research has gone into understanding the impressive performance
of boosting algorithms and voting classifiers, with one particularly influential line of work
focusing on so-called \emph{margins}~\cite{Schapire19981651}. Given a
voting classifiers
$f(x)=\sign(\sum_i \alpha_i h_i(x))$, the margin of $f$ on a data point $(x,y) \in \cX
\times \{-1,1\}$ is then defined as
\[
  \margin(f,(x,y)):=\frac{y \sum_i \alpha_i h_i(x)}{\sum_i \alpha_i}.
\]
Observe that the margin is a number in $[-1,1]$. The margin is $1$ if
all hypotheses $h_i$ in $f$ agree and are correct, it is $-1$ if they
agree and are incorrect, and it is $0$ if there is a 50-50 weighted split in
their predictions for the label of $x$. The margins can thus be
thought of as a (signed) certainty in the prediction made by $f$.

Early boosting experiments~\cite{Schapire19981651} showed that the accuracy of voting
classifiers trained with AdaBoost often improves even when adding
more hypotheses $h_t$ to $f$ after the point where $f$ perfectly classifies the training
data $S$. As adding more hypotheses to $f$ results in a more
complicated model, this behavior was surprising.  Work
by~\cite{Schapire19981651} attribute these improvements in accuracy to improved
margins on the training data. In more detail, they proved
generalization bounds stating that large margins imply good
generalization performance. To give a flavor of these bounds, assume
for now that the hypotheses $h_i$ used for constructing a voting
classifier, all belong to a finite hypothesis class $\cH \subseteq
\{-1,1\}^{\cX}$. If we use $\ls^\gamma_{S}(f)$ to denote the
fraction of samples $(x,y)$ in $S$ for which $f$ has margin no more
than $\gamma$, then \cite{Schapire19981651} showed that for any data distribution
$\cD$ over $\cX \times \{-1,1\}$, any $0 < \delta < 1$ and $0 < \gamma
\leq 1$, it holds
with probability at least $1-\delta$ over a training sequence $\rS \sim
\cD^n$ that every voting classifier $f$ has
\begin{align}
 \ls_{\cD}(f):= \Pr_{(\rx, \ry) \sim \cD}[f(\rx) \neq \ry] =
  \ls^\gamma_\rS(f) + 
 O\left( \sqrt{\frac{\ln(|\cH|) \ln m}{\gamma^2 m} + \frac{\ln(1/\delta)}{m}}\right).\label{eq:simplegen}
\end{align}
As can be seen from this bound, large margins $\gamma > 0$ improve
generalization. For the case where all samples have margin at least
$\gamma$, i.e.\ $\ls_S^\gamma(f)=0$,~\cite{10.1162/089976699300016106} improved this to
\begin{align}
 \ls_{\cD}(f) = 
 O\left( \frac{\ln(|\cH|) \ln m}{\gamma^2 m} + \frac{\ln(1/\delta)}{m}\right).\label{eq:breimangen}
\end{align}
The current state-of-the-art margin generalization bounds nicely
interpolates between the two bounds above. Concretely,~\cite{GAO20131} proved the following generalization
of \cref{eq:simplegen} and~\cref{eq:breimangen} often referred to as the \emph{$k$'th margin
  bound} (for simplicity, we hide the dependency on $\delta$):
\begin{align}
  \ls_{\cD}(f) = \ls_{\rS}^\gamma(f) +
  O\left(\sqrt{\frac{\ls_{\rS}^\gamma(f) \ln(|\cH|) \ln m}{\gamma^2
  m}} +\frac{\ln(|\cH|) \ln m}{\gamma^2 m}\right). \label{eq:refinedgen}
\end{align}
Lower bounds show that this generalization bound is nearly
tight. In particular, the work~\cite{DBLP:conf/nips/GronlundKL20} showed that for any cardinality $N$, and parameters $1/m <\tau, \gamma < c$ for a sufficiently small constant
$c>0$, there is a data distribution $\cD$ and finite hypothesis class
$\cH$ with $|\cH|=N$, such that with constant probability over $\rS
\sim \cD^m$, there is a voting classifier $f$ over $\cH$ with
$\ls^\gamma_{\rS}(f) \leq \tau$ and
\begin{align}
  \ls_{\cD}(f) = \tau + \Omega\left(\sqrt{\frac{\tau \ln(N)
  \ln(1/\tau) }{\gamma^2 m}} +\frac{\ln(N) \ln m}{\gamma^2 m}\right). \label{eq:lower}
\end{align}
This matches the upper bound in \cref{eq:refinedgen} up to the gap
between $\sqrt{\ln(1/\tau)} \approx \sqrt{\ln(1/\ls^\gamma_{\rS}(f))}$ and
$\ln m$, improving by a $\sqrt{\ln(1/\tau)}$ factor over a previous
lower bound by~\cite{DBLP:conf/nips/GronlundKLMN19}. Note that we have simplified the lower bound slightly, as the
true statement would have $\ln m$ replaced by $\ln(\gamma^2
m/\ln N)$. A similar substitution of $\ln m$ by $\ln(\gamma^2
m/\ln(|\cH|))$ in the upper bound \cref{eq:refinedgen} is also possible.

If we instead turn to the more general case of voting classifiers over
a possibly infinite hypothesis class $\cH$ of VC-dimension $d$, the
current state of affairs is less satisfying. Also in the work
by~\cite{GAO20131} introducing the $k$'th margin bound, they show that for any
data distribution $\cD$ and hypothesis class
$\cH$ of VC-dimension $d$, it holds with probability at least
$1-\delta$ over $\rS \sim \cD^m$ that every voting classifier $f$ over
$\cH$ satisfies
\begin{align}
\ls_{\cD}(f) = \ls_{\rS}^\gamma(f) +
  O\left(\sqrt{\ls_{\rS}^\gamma(f) \left(\frac{d \ln(m/d) \ln m}{\gamma^{2} 
  m} + \frac{\ln(1/\delta)}{m} \right)}+\frac{ d \ln(m/d) \ln m}{\gamma^{2} m}+\frac{\ln(1/\delta)}{m}\right). \label{eq:kth}
\end{align}
The only lower bound for finite VC-dimension is \cref{eq:lower} with
$\ln(N)$ replaced by $d$. The gap here is thus a logarithmic
factor and the generalization bound in \cref{eq:kth} has not seen any
improvements since.

\paragraph{New Margin Generalization Bounds.}
Our first main contribution is improved and almost optimal generalization bounds for
voting classifiers with large margins. Concretely, we show the
following theorem
\begin{theorem}
\label{thm:mainintro}[Informal statement of \cref{thm:finalmarginbound}]
For any hypothesis class $ \cH \subseteq \{  -1,1\}^{\cX}$ of VC-dimension
$ d $, distribution $ \cD $ over $ \cX\times \{  -1,1\}  $, failure
parameter $0<\delta<1$ and any constant $ 0<\eps<1 $, it holds with
probability at least $ 1-\delta $ over $ \rS\sim \cD^{m} $ that for
any margin $ 0<\gamma\leq 1 $ and any voting classifier $f$ over
$\cH$, we have
    \begin{align*}
     \ls_{\cD}(f)=  
     \ls_{\rS}^{\gamma}(f)
     +O\left(\sqrt{ \ls_{\rS}^{\gamma}(f) \left(\frac{d\func{\left(\frac{\gamma^{2}m }{d} \right)}}{\gamma^{2}m}
     +\frac{\ln{\left(\frac{1}{\delta} \right)}}{m}\right)}  + \frac{d\func{\left(\frac{\gamma^{2}m }{d} \right)}}{\gamma^{2}m}+\frac{\ln{\left(\frac{1}{\delta} \right)}}{m}\right),
    \end{align*}
where $\func(x) = \ln(x) \ln^2(\ln  x)$.
  \end{theorem}
Our theorem improves over \cref{eq:kth} by nearly a logarithmic
factor and the gap between our upper bound and the lower bound
in~\cref{eq:lower} is essentially $\ln(\ln(\gamma^2 m/d))$
times the ratio between $\sqrt{\ln(1/\ls^\gamma_\rS(f))}$ and
$\sqrt{\ln(\gamma^2 m/d)}$. Furthermore, all logarithmic factors are now $\ln(\gamma^2
m/d)$ instead of $\ln(m/d)$ and $\ln m$. While the improvement inside
the $\ln$'s might seem 
minor, this has crucial implications for the development
of a new boosting algorithm explained later. Furthermore, and unlike in the
case of finite $\cH$ (see discussion after~\cref{eq:lower}), there does not seem to be a way of tweaking the
proof of the previous bound in~\cref{eq:kth} to improve the factors $\ln(m/d)$ and $\ln m$
to $\ln(\gamma^2 m/d)$. 

\paragraph{New Boosting Results.}
One of the prime motivations for studying generalization bounds for large margin voting
classifiers, is their application to boosting algorithms. When
studying boosting theoretically, we typically use the framework of
\emph{weak to strong} learning
by~\cite{kearns1988learning,kearns1994learning}. Let $t \in
\{-1,1\}^\cX$ be an unknown target concept assigning labels $t(x)$ to
samples $x \in \cX$. For a distribution $\cD$ over $\cX$, let $\cD_t$
be the distribution over $\cX \times \{-1,1\}$ obtained by drawing a
sample $\rx \sim \cD$ and returning the pair $(\rx, t(\rx))$.

A $\gamma$-weak
learner $\cW$, is a learning algorithm that for any distribution $\cD$
over $\cX$, when
given $m_0$ i.i.d.\ samples $(\rx_i,t(\rx_i)) \sim \cD_t$, $\cW$ produces with probability at least
$1-\delta_0$ a hypothesis $h$ with
$\ls_{\cD_t}(h) = \Pr_{(\rx,t(\rx)) \sim \cD_t}[h(\rx) \neq t(\rx)] \leq 1/2-\gamma$. Here $m_0$ and $\delta_0$ are
constants. A strong learner in contrast, is a learning
algorithm that for any distribution $\cD$ over $\cX$, when given $m(\eps,
\delta)$ i.i.d.\ samples from $\cD_t$, it produces with probability at least
$1-\delta$ a hypothesis with $\ls_{\cD_t}(h) \leq \eps$. A strong learner
thus obtains arbitrarily high accuracy when given enough training
data.

AdaBoost~\cite{adaboostyoavfreund} is the most famous algorithm for constructing a weak
learner from a strong learner. Concretely, it can be shown that if
AdaBoost is run for $O(\gamma^{-2} \ln m)$ iterations, then it
produces a voting classifier $f$ with margin $\Omega(\gamma)$ on all
samples in a given training sequence $S$ with $|S|=m$~\cite{boostingbook} [Theorem 5.8]. If the weak
learner/base learning algorithm always returns hypotheses from a
hypothesis class $\cH$ of VC-dimension $d$, this allows us to use our
new generalization bound in~\cref{thm:mainintro} to conclude
\begin{corollary}
  \label{cor:ada}
For any $\gamma$-weak learner $\cW$ using a hypothesis class $ \cH \subseteq \{  -1,1\}^{\cX}$ of VC-dimension
$ d $, distribution $ \cD $ over $ \cX$, target
concept $t$, failure
parameter $0<\delta<1$ and any constant $ 0<\eps<1 $, it holds with
probability at least $1-\delta$ over $\rS \sim \cD_t^m$, that the voting
classifier $f$ produced by AdaBoost on $\rS$ with weak learner $\cW$ has
\[
  \ls_{\cD_t}(f) = O\left(\frac{d \func(\gamma^2m/d)}{\gamma^2 m} +
  \frac{\ln(1/\delta)}{m}\right),
\]
where $\func(x) = \ln(x)\ln^2(\ln x)$.
\end{corollary}
The previous best known generalization bound for AdaBoost followed
from the margin generalization bound \cref{eq:kth} and was
$\ls_{\cD_t}(f) = O\left(\frac{d \ln(m/d)\ln m}{\gamma^2 m} +
  \frac{\ln(1/\delta)}{m}\right)$.
Moreover, our new bound is tight up to a $\ln^2(\ln(\gamma^2 m/d))$ factor as
demonstrated by a lower bound of~\cite{adaboostnotoptimal} showing that for
$c^{-1}\sqrt{d/m} < \gamma < c$ for sufficiently small constant $c>0$ and VC-dimension $d = \Omega(\ln(1/\gamma))$, there is a data
distribution $\cD$, a weak learner $\cW$ using a hypothesis class of
VC-dimension $d$, and  a concept $t$, such that AdaBoost run with
$\cW$ has
$\ls_{\cD_t}(f) = \Omega\left(\frac{d \ln(\gamma^2m/d)}{\gamma^2 m}\right)$,
 with constant probability over a training sequence $\rS \sim
 \cD_t^m$. 

 In addition to improving our understanding of AdaBoost, our new
 generalization bound for voting classifiers also allows us to design
 an improved weak to strong learner with an optimal in-expectation error. In the
 work~\cite{larsen2022optimalweakstronglearning}, it was shown that the optimal generalization
 error of any weak to strong learning algorithm with access to a
 $\gamma$-weak learner using a hypothesis class of VC-dimension $d$ is
 \begin{align}
   \ls_{\cD_t}(f) = \Theta\left(\frac{d}{\gamma^2m} + \frac{\ln(1/\delta)}{m}\right).\label{eq:optstrong}
 \end{align}
 In light of the lower bound above for AdaBoost, this implies that
 AdaBoost is not an optimal weak to strong learner. However, several optimal weak to strong
 learning algorithms have been developed. In~\cite{larsen2022optimalweakstronglearning}, the authors gave
 the first such algorithm. This algorithm uses the sub-sampling idea
 of~\cite{Hannekeoptimal} from optimal realizable PAC learning and runs AdaBoost on
 $m^{\lg_4 3}$ many sub-samples $S_i \subset S$ of the training
 data. It combines the produced voting classifiers by taking a
 majority vote among their predictions, i.e.\ a majority-of-majorities. Since each $S_i$ has $|S_i|= \Omega(m)$, this slows down
 AdaBoost by a factor $m^{\lg_4 3} \approx m^{0.79}$. Later work
 by~\cite{baggingoptimal} showed that running Bagging~\cite{Breiman1996BaggingP} to draw
 $O(\ln(m/\delta))$ many random sub-samples
 $S_i$ from $S$ and running AdaBoost on each also results in an
 optimal generalization error matching \cref{eq:optstrong}, thus
 reducing the computational overhead to a logarithmic factor. Finally,
 a recent work by~\cite{manyfacesofoptimalweaktostronglearning} built on a Majority-of-3 result in
 realizable PAC learning~\cite{majorityofthree} to show that simply partitioning a training
 sequence into $5$ disjoint pieces of $m/5$ samples each, and
 outputting a majority vote among voting classifiers trained with
 AdaBoost on each sub-sample, results in an optimal in-expectation
 error of $\e_{\rS}[\ls_{\cD_t}(f)] = O(d/(\gamma^2 m))$.

 Our new generalization bound in~\cref{thm:mainintro} allows us to
 improve the Majority-of-5 algorithm to a more natural Majority-of-3
 \begin{corollary}
 \label{cor:maj3intro}[Follows from \cref{thm:mainmajoritythree}]
For any $\gamma$-weak learner $\cW$ using a hypothesis class $ \cH \subseteq \{  -1,1\}^{\cX}$ of VC-dimension
$ d $, distribution $ \cD $ over $ \cX$, and concept $t$, it holds that the voting
classifiers $f_{\rS_1},f_{\rS_2},f_{\rS_3}$ produced by AdaBoost on
i.i.d.\ training sequences $\rS_1,\rS_2,\rS_3 \sim \cD_t^m$ with
weak learner $\cW$ satisfy
    \begin{align*}
    \e_{\rS_{1},\rS_{2},\rS_{3}\sim \cD_t^{m}}\left[\ls_{\cD_t}(\maj(f_{\rS_{1}},f_{\rS_{2}},f_{\rS_{3}}))\right] =O\left(\frac{d}{\gamma^{2}m}\right).
    \end{align*}
  \end{corollary}
It is in this result that it is critical that our generalization bound
in~\cref{thm:mainintro} has $\ln(\gamma^2 m/d)$ factors rather than
$\ln(m/d)$ or $\ln(m)$ factors. We elaborate on this in~\cref{sec:sketchmaj3}.