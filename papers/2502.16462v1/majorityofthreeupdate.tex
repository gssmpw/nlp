\section{Majority of Three Large Margin Classifiers}\label{sec:mainmajoritythree}
In this section we give the proof of \cref{thm:mainmajoritythree}, which implies \cref{cor:maj3intro}. 

To this end recall some notation. For a target concept $ t \in \{  -1,1\}^{\cX}  $ we use $ (\cX\times\{  -1,1\})_{t}^{*} $ for the set of all possible training sequences on $ \cX $, labelled by $ t $, that is for $ S\in (\cX\times\{  -1,1\})_{t}^{*} $, any train example $ (x,y)\in S $, is such that $ y=t(x).$ We remark that $ S $ is seen as a vector/sequence so it may have repetitions of similar train examples. Furthermore, for a distribution $ \cD $ over $ \cX $ we write $ \cD_{t} $ for the distribution over $ \cX\times\{  -1,1\}  $, defined by $ \p_{(\rx,\ry)\sim\cD_{t}}\left[(\rx,\ry)\in A\right]=\p_{\rx\sim \cD}\left[(\rx,t(\rx))\in A\right] $ for any  $ A\subseteq \cX\times\{  -1,1\}$. Furthermore, for $ R\subset \cX $, such that $ \p_{\rx\sim\cD}\left[\rx\in R\right]\not=0 $ , we define $ \cD_{t}\mid R $ as $$ \p_{(\rx,\ry)\sim\cD_{t}\mid R}\left[(\rx,\ry)\in A\right]=\p_{\rx\sim \cD}\left[(\rx,t(\rx))\in A\mid \rx\in R\right]= \frac{\p_{\rx\sim \cD}\left[(\rx,t(\rx))\in A, \rx\in R\right]}{\p_{\rx\sim \cD}\left[\rx\in R\right]}.$$

We define a learning algorithm $ L $  as a mapping from $ (\cX\times\{  -1,1\})_{t}^{*} $ to $ \mathbb{R}^{\cX} $, for $ S\in (\cX\times\{  -1,1\})_{t}^{*} $, we write $ L_{S} $ for short of $ L(S)\in\mathbb{R}^{\cX} $. Furthermore, if $ L_{S} \in \dlh$ for any $ S\in (\cX\times\{  -1,1\})_{t}^{*} $ we write $ L\in \dlh $ . For $ 0<\gamma<1 $ and target concept $ t $  we define a $ \gamma $-margin algorithm $ L $ for $ t $   as a mapping from $ (\cX\times\{  -1,1\})_{t}^{*} $ to $ \mathbb{R}^{\cX} $, such that for a $ S\in (\cX\times\{  -1,1\})_{t}^{*} $, we have that $ L_{S}(x)y\geq \gamma $ for all $ (x,y)\in S.$ Furthermore, for three functions $ f_{1},f_{2},f_{3} $ we define $ \maj(f_{1},f_{2},f_{3})=\sign(\sign(f_{1})+\sign(f_{2})+\sign(f_{3})) $, with $ \sign(0)=0.$  

 With the above notation introduced, we can now state \cref{thm:mainmajoritythree}, which is saying that the majority vote of a margin classifier algorithm run on 3 independent training sequences implies the following error bound.
\begin{theorem}\label{thm:mainmajoritythree}
    For distribution $ \cD $ over $ \cX $, target concept $ t $,  hypothesis class $ \cH \subseteq \{-1,1  \}^{\cX} $ with VC-dimension $ d $,  training sequence size $ m $, margin $ 0<\gamma<1 $ and i.i.d. training sequences $ \rS_{1},\rS_{2},\rS_{3}\sim \cD_{t}^{m} $, it holds for any $ \gamma $-margin learning algorithm $ L\in \dlh $ for $ t $ that  
    \begin{align*}
    \e_{\rS_{1},\rS_{2},\rS_{3}\sim \cD^{m}_{t}}\left[\ls_{\cD_{t}}(\maj(L_{\rS_{1}},L_{\rS_{2}},L_{\rS_{3}}))\right] =O\left(\frac{d}{\gamma^{2}m}\right).
    \end{align*}
\end{theorem}

Now as AdaBoost is a $ \Omega(\gamma)$-margin learning algorithm when given access to a $ \gamma $-weak learner $ \cW,$ the above \cref{thm:mainmajoritythree} implies \cref{cor:maj3intro}.

To give the proof of \cref{thm:mainmajoritythree} we need the following lemma, which bounds the expected value of two outputs of a $ \gamma $-margin learning algorithm trained on two independent training sequences erring simultaneously. 

\begin{lemma}\label{lem:twofailingsimilar}
    There exists a universal constant $ C\geq1 $ such that: For distribution $ \cD $ over $ \cX $, target concept $ t $,  hypothesis class $ \cH \subseteq \{-1,1  \}^{\cX} $ with VC-dimension $ d $,  training sequence size $ m $, margin $ 0<\gamma<1 $, i.i.d. training sequences $ \rS_{1},\rS_{2},\rS_{3}\sim \cD_{t}^{m} $, it holds for any $ \gamma $-margin learning algorithm $ L\in \dlh $ for $ t $  that  
    \begin{align*}
    \e_{\rS_{1},\rS_{2}\sim \cD^{m}_{t}}\left[\p_{\rx\sim\cD}\left[   \sign(L_{\rS_{1}}(\rx))\not=t(\rx),\sign(L_{\rS_{2}}(\rx))\not=t(\rx)\right]\right] =\frac{96Cd}{\gamma^{2}m}.
    \end{align*}
\end{lemma}

We postpone the proof of \cref{lem:twofailingsimilar} for later in this section and now give the proof of \cref{thm:mainmajoritythree}.

\begin{proof}[Proof of \cref{thm:mainmajoritythree}]
We observe for $ \maj(L_{\rS_{1}},L_{\rS_{2}},L_{\rS_{3}}) $ to fail on an example $ (x,y) $ it must be the case that two of the classifiers err, i.e.\ there exists $ i,j\in\{1,2,3  \}  $, where $ i\not=j $   such that $ \sign(L_{\rS_{i}}(x))\not=y,\sign(L_{\rS_{j}}(x))\not=y $. Thus by a union bound, $ \rS_{1},\rS_{2},\rS_{3} $ being i.i.d.\ and \cref{lem:twofailingsimilar}, we conclude that
\begin{align*}
    &\e_{\rS_{1},\rS_{2},\rS_{3}\sim \cD^{m}_{t}}\left[\ls_{\cD_{t}}(\maj(L_{\rS_{1}},L_{\rS_{2}},L_{\rS_{3}}))\right]
    \\
    &\leq \sum_{i>j}\e_{\rS_{i},\rS_{j}\sim \cD^{m}_{t}}\left[\p_{\rx\sim\cD}\left[   \sign(L_{\rS_{i}}(\rx))\not=t(\rx),\sign(L_{\rS_{j}}(\rx))\not=t(\rx)\right]\right]
    =\frac{288Cd}{\gamma^{2}m}
\end{align*} 
which concludes the proof. 
\end{proof}

We now prove \cref{lem:twofailingsimilar}. To the end of showing \cref{lem:twofailingsimilar} we need the following lemma which bounds the conditional error of a large margin learning algorithm under $ \cD_{t}|R $.

\begin{lemma}\label{lem:expectationlargemarign}
    There exists a universal constant $ c\geq1 $ such that: For distribution $ \cD $ over $ \cX $, target concept $ t $,  hypothesis class $ \cH \subseteq \{-1,1  \}^{\cX} $ with VC-dimension $ d $,  training sequence size $ m $, margin $ 0<\gamma<1 $, i.i.d. training sequence $ \rS\sim \cD_{t}^{m} $, subset $ R\subseteq \cX $ such that $ \p_{\rx\sim \cD}\left[R\right]:=\p_{\rx\sim\cD}\left[\rx\in R\right]\not=0 $, it holds for any $ \gamma $-margin learning algorithm $ L\in \dlh $ for $ t $  that  
    \begin{align*}
        \e_{\rS\sim \cD_{t}^{m}}\left[\ls_{\cD_{t}|R}(L_{\rS})\right] \leq\frac{28cd\ln^{2}{\left( \max(e^{2},\frac{\p_{\rx\sim \cD}\left[R\right]\gamma^{2}m}{2d})\right)}}{\p_{\rx\sim \cD}\left[R\right]\gamma^{2}m}.
    \end{align*}
\end{lemma}

We postpone the proof of \cref{lem:expectationlargemarign} to after the proof of \cref{lem:twofailingsimilar}, which we give now. 

\begin{proof}[Proof of \cref{lem:twofailingsimilar}]
    For $ i\in \{  0,1,\ldots \}  $ we define the disjoint regions  $ R_{i}\subseteq \cX $ $ R_{i}=\{x\in \cX:2^{-i-1}< \p_{\rS\sim\cD^{m}_{t}}\left[ \sign(L_{\rS}(x)) \not = t(x)  \right]\leq 2^{-i}  \}$  of $ \cX $ (we will write $ \p_{\rx\sim \cD}\left[R_{i}\right] $ for short for $ \p_{\rx\sim\cD}\left[\rx\in R_{i}\right] $). Then by the law of total probability $ \rS_{1} $ and $ \rS_{2} $ being independent we have
    \begin{align}\label{eq:twofailingsimilar2}
        &\e_{\rS_{1},\rS_{2}\sim \cD^{m}_{t}}\negmedspace\left[\p_{\rx\sim\cD}\left[   \sign(L_{\rS_{1}}(\rx))\not=t(\rx),\sign(L_{\rS_{2}}(\rx))\not=t(\rx)\right]\right]
        \negmedspace=\negmedspace\negmedspace\negmedspace\e_{\rx\sim\cD}\negmedspace\left[\p_{\rS\sim \cD^{m}_{t}}\left[   \sign(L_{\rS}(\rx))\not=t(\rx)\right]^{2}\right]\nonumber
        \\
        &= \sum_{i=0}^{\infty}  \e_{\rx\sim\cD}\left[\p_{\rS\sim \cD^{m}_{t}}\left[   \sign(L_{\rS}(\rx)\not=t(\rx))\right]^{2} \Big| R_{i}\right]\p_{\rx\sim\cD}\left[R_{i}\right]
        \leq
        \sum_{i=0}^{\infty}  2^{-2i}\p_{\rx\sim\cD}\left[R_{i}\right],
    \end{align}
    where the last inequality follows from the definition of $ R_{i}$. We will show that for each $ i\in\{  0,1,2,\ldots\}  $ we have that $ \p_{\rX\sim\cD}\left[R_{i}\right] \leq \frac{8C(i+1)^{2}2^{i}d}{\gamma^{2}m}$ for some universal constant   $ C\geq1 $. Using this, the above gives us that 
    \begin{align*}
        \e_{\rS_{1},\rS_{2}\sim \cD^{m}_{t}}\left[\p_{\rx\sim\cD}\left[   \sign(L_{\rS_{1}}(\rx))\not=t(\rx),\sign(L_{\rS_{2}}(\rx))\not=t(\rx)\right]\right]
        \leq
        \sum_{i=0}^{\infty}  2^{-2i}\frac{8C(i+1)^{2}2^{i}d}{\gamma^{2}m}
        =\frac{96Cd}{\gamma^{2}m},
    \end{align*}
    where the second inequality follows from $ \sum_{i=0}^{\infty}2^{-i}(i+1)^{2} =12$ and this gives the claim of \cref{lem:twofailingsimilar}. 
    We thus proceed to show that for each $ i\in\{  0,1,2,\ldots\}  $, we have that $ \p_{\rX\sim\cD}\left[R_{i}\right] \leq \frac{8C(i+1)^{2}2^{i}d}{\gamma^{2}m}$. 
    To this end let $ i\in\{  0,1,2\ldots\}.$ If $ \p_{\rx\sim\cD}\left[R_{i}\right]=0 $ then we are done, thus we consider the case that $ \p_{\rx\sim\cD}\left[R_{i}\right]\not=0 $.
    We first observe that:
    \begin{align*}
        \e_{\rS\sim \cD_{t}^{m}}\left[\ls_{\cD_{t}|R_{i}}(L_{\rS})\right]=\e_{\rx\sim \cD}\left[\p_{\rS\sim \cD_{t}^{m}}\left[\sign(L_{\rS}(\rx))\not=t(\rx)\right]\Big|\rx\in R_{i} \right]\geq 2^{-i-1},
    \end{align*}    
    where the equality follows by the definition of $ \sign(0)=0,$ and the inequality follows by the definition of $ R_{i}$.  Furthermore, by \cref{lem:expectationlargemarign} we have that 
    \begin{align*}
        \e_{\rS\sim \cD_{t}^{m}}\left[\ls_{\cD_{t}|R_{i}}(L_{\rS})\right]\leq  \frac{28 cd\ln^{2}{\left( \max(e^{2},\frac{\p\left[R\right]\gamma^{2}m}{2d})\right)}}{\p\left[R\right]\gamma^{2}m},
    \end{align*}
    where $ c \geq 1$ is a universal constant.  
    Thus, we conclude that 
    \begin{align}\label{eq:twofailingsimilar1}
        2^{-i-1}\leq \frac{28 cd\ln^{2}{\left( \max(e^{2},\frac{\p\left[R\right]\gamma^{2}m}{2d})\right)}}{\p\left[R\right]\gamma^{2}m}.
    \end{align}
    Now the function $ \frac{\ln^{2}{\left(\max(e^{2},x) \right)}}{x} $ for $ x>e^{2} $ is decreasing since it has derivative $\frac{2\ln{\left(x \right)}-\ln^{2}{\left(x \right)}}{x^{2}}  $, which is negative for $ x>e^{2} $. Furthermore, we have that $ \frac{\ln^{2}(max(e^{2},x))}{x} $ is decreasing for $ 0<x\leq e^{2} $, so   $ \frac{\ln^{2}{\left(\max(e^{2},x) \right)}}{x} $  is decreasing   for $ x>0.$ Now assume for a contradiction that $ \p\left[R\right]\geq \frac{8C(i+1)^{2}2^{i}d}{\gamma^{2}m} $ or equivalently that $ \frac{\p\left[R\right]\gamma^{2}m}{2d}\geq C(i+1)^{2}2^{i}$ for some $ C\geq e^{2} $  to be chosen large enough later.  Thus, since we concluded that $ \frac{\ln^{2}{\left(\max(e^{2},x) \right)}}{x} $  is decreasing   for $ x>0,$ and we assumed for contradiction that $ \frac{\p\left[R\right]\gamma^{2}m}{2d}\geq C(i+1)^{2}2^{i}$ we get that 
    \begin{align*}
        \frac{28 cd\ln^{2}{\left( \max(e^{2},\frac{\p\left[R\right]\gamma^{2}m}{2d})\right)}}{\p\left[R\right]\gamma^{2}m}
        \leq
        \frac{14 c\ln^{2}{\left( \max(e^{2},C(i+1)^{2}2^{i})\right)}}{ C(i+1)^{2}2^{i}}
        \leq \frac{14 c\ln^{2}{\left( C(i+1)^{2}2^{i}\right)}}{ C(i+1)^{2}2^{i}}
    \end{align*}
    where the last inequality follows from $ C\geq e^{2} $, and $ i\geq0.$ Now since it holds that $ \ln{\left(C(i+1)^{2}2^{i} \right)}\leq 3 \max(\ln{\left(C \right)},2\ln{\left(i+1 \right)},i\ln{\left(2 \right)}) $ we get that the following inequality holds   $ \ln^{2}{\left( C(i+1)^{2}2^{i}\right)}\leq 9 \max(\ln^{2}{\left(C \right)},4\ln^{2}{\left(i+1 \right)},i^{2}\ln^{2}{\left(2 \right)}) $. Thus, we conclude that 
    \begin{align*}
        \frac{28 cd\ln^{2}{\left( \max(e^{2},\frac{\p\left[R\right]\gamma^{2}m}{2d})\right)}}{\p\left[R\right]\gamma^{2}m}\leq \frac{126 c \max(\ln^{2}{\left(C \right)},4\ln^{2}{\left(i+1 \right)},i^{2}\ln^{2}{\left(2 \right)})}{ C(i+1)^{2}2^{i}}.
    \end{align*} 
    Furthermore since $ \ln^{2}{\left(i+1 \right)}/((i+1)^{2})\leq 1 $,$ i^{2}/((i+1)^{2})\leq 1 $ and $ \ln{\left(C^{\frac{1}{4}} \right)}\leq\ln{\left(1+C^{\frac{1}{4}} \right)} \leq C^{\frac{1}{4}}$  we conclude that 
    \begin{align*}
        \frac{28 cd\ln^{2}{\left( \max(e^{2},\frac{\p\left[R\right]\gamma^{2}m}{2d})\right)}}{\p\left[R\right]\gamma^{2}m}\leq \frac{126 c \max(\ln^{2}{\left(C \right)},4,\ln^{2}{\left(2 \right)})}{ C2^{i}}\leq \frac{126 c \max(16C^{\frac{1}{2}},4)}{ C2^{i}}\leq \frac{2016c}{C^{1/2}2^{i}}.
    \end{align*}
    where the last inequality follows from $ C\geq e^{2}.$ 
    Since the above is decreasing in $ C $ we get that for $ C=(4\cdot 2016)^2 $, it holds that
    \begin{align*}
        \frac{28 cd\ln^{2}{\left( \max(e^{2},\frac{\p\left[R\right]\gamma^{2}m}{2d})\right)}}{\p\left[R\right]\gamma^{2}m} <\frac{1}{2^{i+2}},
    \end{align*}  
    which is a contradiction with \cref{eq:twofailingsimilar1}, so it must be the case that $  \p\left[R\right]\leq \frac{8C(i+1)^{2}2^{i}d}{\gamma^{2}m}$, as claimed below \cref{eq:twofailingsimilar2} which concludes the proof.   
\end{proof}

We now give the proof of \cref{lem:expectationlargemarign}.

\begin{proof}[Proof of \cref{lem:expectationlargemarign}]
    If $ \frac{d}{\p\left[R\right]\gamma^{2}m} \geq1$ then we are done by $ \ls_{\cD_{t}|R} $ always being at most 1, thus for the remainder of the proof we consider the case $ \frac{d}{\p\left[R\right]\gamma^{2}m} <1$.
    
    Define $ \rN=\sum_{(x,y)\in \rS} \ind\{x\in R  \}   $, i.e. the number of examples in $ \rS $, that has its input point in $ R $. We notice that $ \rN $ has expectation $ \p\left[R\right]m $. Thus, by $ \rN $ being a sum of i.i.d. $ \{  0,1\}  $-random variables it follows by an application of Chernoff that 
    \begin{align*}
     \p_{\rS\sim \cD_{t}^{m}}\left[\rN\leq \p\left[R\right]m/2\right]\leq  \exp{\left(-\p\left[R\right]m/8 \right)}\leq \frac{8}{\p\left[R\right]m},  
    \end{align*} 
    where the last inequality follows from $ \exp(-x)\leq \frac{1}{x} $ for $ x>0 $.
    Thus, from the above and the law of total probability, we conclude that 
    \begin{align}\label{eq:condtionalexpectation-1}
     \e_{\rS\sim \cD_{t}^{m}}\left[\ls_{\cD_{t}|R}(L_{\rS})\right] 
      \leq \sum_{i=\left\lceil\p\left[R\right]m/2\right\rceil}^{m} \e_{\rS\sim \cD_{t}^{m}}\left[\ls_{\cD_{t}|R}(L_{\rS})|\rN=i \right]\p_{\rS\sim \cD_{t}^{m}}\left[\rN=i\right] +\frac{8}{\p\left[R\right]m}.
    \end{align}
    For each $ i\in \{\left\lceil\p\left[R\right]m/2\right\rceil,\ldots,m  \} ,$ we will show that $ \e_{\rS\sim \cD_{t}^{m}}\left[\ls_{\cD_{t}|R}(L_{\rS})|\rN=i \right]$ is upper bound by $\frac{20 cd\ln^{2}{( \max(e^{2},\frac{\p\left[R\right]\gamma^{2}m}{2d}))}}{\p\left[R\right]\gamma^{2}m} $, where $ c \geq 1$ is a universal constant, which implies that
    \begin{align*}
        \e_{\rS\sim \cD_{t}^{m}}\left[\ls_{\cD_{t}|R}(L_{\rS})\right] \leq  \frac{20cd\ln^{2}{\left( \max(e^{2},\frac{\p\left[R\right]\gamma^{2}m}{2d})\right)}}{\p\left[R\right]\gamma^{2}m}+\frac{8}{\p\left[R\right]m}\leq\frac{28cd\ln^{2}{\left( \max(e^{2},\frac{\p\left[R\right]\gamma^{2}m}{2d})\right)}}{\p\left[R\right]\gamma^{2}m}
    \end{align*}
    as claimed. We now show for each $ i\in \{\left\lceil\p\left[R\right]m/2\right\rceil,\ldots,m  \}  $ that $ \e_{\rS\sim \cD_{t}^{m}}\left[\ls_{\cD_{t}|R}(L_{\rS})|\rN=i \right]\leq \frac{20 cd\ln^{2}{( \max(e^{2},\frac{\p\left[R\right]\gamma^{2}m}{2d})))}}{\p\left[R\right]\gamma^{2}m}$.

    Let for now $ i\in \{\left\lceil\p\left[R\right]m/2\right\rceil,\ldots,m  \}  $. First since $ \ls_{\cD_{t}|R}(L_{\rS}) $ is nonnegative we have that its expectation can be calculated in terms of its cumulative distribution function,
    \begin{align}\label{eq:condtionalexpectation0}
     \e_{\rS\sim\cD_{t}^{m}}\left[\ls_{\cD_{t}|R}(L_{\rS})|\rN=i\right]=\int_{0}^{\infty}\p_{\rS\sim \cD_{t}^{m}}\left[\ls_{\cD_{t}|R}(L_{\rS})> x|\rN=i \right] \ dx\nonumber
     \\
     \leq \frac{4cd\Ln^{2}{\left( \frac{\gamma^{2}i}{d}\right)}}{\gamma^{2}i}+\int_{\frac{4cd\Ln^{2}{\left( \frac{\gamma^{2}i}{d}\right)}}{\gamma^{2}i}}^{\infty} \p_{\rS\sim\cD^{m}_{t}}\left[\ls_{\cD_{t}|R}(L_{\rS})> x|\rN=i \right] \ dx.
    \end{align} 

    We now notice that under the conditional distribution $ \rN=i $, it is the case that the $ \gamma $- margin learning algorithm $ L_{\rS} $, contains $ i $ labelled examples from $ \cD_{t}|R $. Furthermore, since $L_{\rS}  $ has     $ \ls_{\rS}^{\gamma}(L_{\rS})=0 $ it also has zero margin-loss on the examples drawn from $ \cD_{t}|R.$ Thus, by invoking \cref{thm:finalmarginbound}, it holds with probability at least $ 1-\delta $ over $ \rS \sim \cD_{t}^{m}$ conditioned on $ \rN=i $, that  
    \begin{align}\label{eq:condtionalexpectation1}
      \ls_{\cD_{t}|R}(L_{\rS})\leq \frac{cd\Ln^{2}\left(\frac{\gamma^{2}i}{d}\right)}{\gamma^{2}i}+ \frac{c\ln{\left(\frac{e}{\delta} \right)}}{i}
      \leq \max\left(\frac{2cd\Ln^{2}\left(\frac{\gamma^{2}i}{d}\right)}{\gamma^{2}i}, \frac{2c\ln{\left(\frac{e}{\delta} \right)}}{i}\right)
    \end{align}  
    where we have upper bounded $ \func $ by $ \Ln^{2}$, which holds since for $ x\leq e^{e} $  we have that $\func(x)= \Ln^{2}(\Ln(x))\Ln(x)=\Ln(x)\leq \Ln^{2}(x)$ and for $ x>e^{e} $ $\func(x)= \ln^{2}(\ln(x))\ln(x)\leq \ln^{2}(x)$, and used that $ a+b\leq 2\max(a,b) $ for $ a,b\geq0 $, and $ c\geq 1$ being the universal constant of \cref{thm:finalmarginbound}. 
    For $ x>2c/i $, we now notice that if we set $ \delta=e\cdot \exp(-ix/2c),$ which is strictly less than $1  $ by $ x>2c/i,$ we conclude from \cref{eq:condtionalexpectation1}  that
    \begin{align*} 
        \p_{\rS\sim \cD^{m}_{t}}\left[\ls_{\cD_{t}|R}(L_{\rS})
        > \max\left(\frac{2cd\Ln^{2}\left(\frac{\gamma^{2}i}{d}\right)}{\gamma^{2}i},x\right)\Big|\rN=i\right]\leq e\cdot \exp(-ix/2c),
      \end{align*}  
    Furthermore, we notice that if $ 0<x\leq 2c/i $ then the above right-hand side is at least $ 1,$ which is also upper bounding the left-hand side since it is at most $ 1,$ thus the above holds for any  $ x>0. $ 
    Thus, plugging this into \cref{eq:condtionalexpectation0} we get that 
    \begin{align*}
        \e_{\rS\sim\cD_{t}^{m}}\left[\ls_{\cD_{t}|R}(L_{\rS})|\rN=i\right]
        &\leq \frac{4cd\Ln^{2}{\left( \frac{\gamma^{2}i}{d}\right)}}{\gamma^{2}i}+\int_{\frac{4cd\Ln^{2}{\left( \frac{\gamma^{2}i}{d}\right)}}{\gamma^{2}i}}^{\infty} \p_{\rS\sim\cD^{m}_{t}}\left[\ls_{\cD_{t}|R}(L_{\rS})> x|\rN=i \right] \ dx
        \\
        &\leq \frac{4cd\Ln^{2}{\left( \frac{\gamma^{2}i}{d}\right)}}{\gamma^{2}i}+\int_{\frac{4cd\Ln^{2}{\left( \frac{\gamma^{2}i}{d}\right)}}{\gamma^{2}i}}^{\infty} e\cdot \exp(-ix/2c) \ dx 
        \\
        &\leq \frac{4cd\Ln^{2}{\left( \frac{\gamma^{2}i}{d}\right)}}{\gamma^{2}i}+
        \frac{2ec}{i} \cdot \exp\left(-\frac{4cd\Ln^{2}{\left( \frac{\gamma^{2}i}{d}\right)}}{2c\gamma^{2}} \right) 
        \\
        &\leq 
        \frac{10cd\Ln^{2}{\left( \frac{\gamma^{2}i}{d}\right)}}{\gamma^{2}i}
        \leq\frac{10cd\ln^{2}{\left( \max(e^{2},\frac{\gamma^{2}i}{d})\right)}}{\gamma^{2}i},
    \end{align*}
    where we have used in the third inequality that $ \int\exp(-ax) \ dx= -\exp(-ax)/a +C$, in the second to last inequality that $ 4+2e\leq 10 $, and in the last that $ \Ln(x)=\ln{\left(max(e,x) \right)}\leq\ln{\left(\max(e^{2},x)  \right)}.$ 
    Now for $ \frac{\gamma^{2}i}{d}\leq e^{2}  $, $ \frac{10cd\ln^{2}{( \max(e^{2},\frac{\gamma^{2}i}{d}))}}{\gamma^{2}i} $  is a decreasing function in $ \frac{\gamma^{2}i}{d}$. 
    Furthermore, since $ \ln^2{\left(x \right)}/x $ has derivative $ \frac{2\ln{\left(x \right)}-\ln^2{\left(x \right)}}{x^2} $ we conclude that $ \ln^2{\left(x \right)}/x $ is decreasing for $ x\geq e^{2} $, whereby we conclude that $ \frac{10cd\ln^{2}{( \max(e^{2},\frac{\gamma^{2}i}{d}))}}{\gamma^{2}i} $ is decreasing in $ \frac{\gamma^{2}i}{d} $ for $ \frac{\gamma^{2}i}{d} \geq e^{2}$, so we conclude that for $ \frac{\gamma^{2}i}{d} >0 $ the function $ \frac{10cd\ln^{2}{( \max(e^{2},\frac{\gamma^{2}i}{d}))}}{\gamma^{2}i} $ is decreasing in $ \frac{\gamma^{2}i}{d}  $. 
    Now $ i\geq \left\lceil\p\left[R\right]m/2\right\rceil\geq \p\left[R\right]m/2 >0$ thus we have $ \frac{\gamma^{2}i}{d}\geq \frac{\p\left[R\right]\gamma^{2}m}{2d}$, which by the above argued monotonicity implies that $ \frac{10cd\ln^{2}{( \max(e^{2},\frac{\gamma^{2}i}{d}))}}{\gamma^{2}i} \leq \frac{2\cdot10 cd\ln^{2}{( \max(e^{2},\frac{\p\left[R\right]\gamma^{2}m}{2d}))}}{\p\left[R\right]\gamma^{2}m} $. 
    Thus, we have argued that 
    \begin{align*}
        &\e_{\rS\sim\cD_{t}^{m}}\left[\ls_{\cD_{t}|R}(L_{\rS})|\rN=i\right]
        \leq \frac{10cd\ln^{2}{( \max(e^{2},\frac{\p\left[R\right]\gamma^{2}m}{2d}))}}{\p\left[R\right]\gamma^{2}m},
    \end{align*}
    which shows the claim below \cref{eq:condtionalexpectation-1} and concludes the proof. 
\end{proof}