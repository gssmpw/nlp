\section{Proof Overviews and Notation}
In this section, we first describe the ideas going into our improved generalization bound for voting classifiers, stated in~\cref{thm:mainintro}. We then proceed to give an overview of our proof that Majority-of-3 AdaBoosts gives an optimal in-expectation error for weak to strong learning as in~\cref{cor:maj3intro}. Along the way, we introduce notation that we will use in our proofs.


\subsection{New Margin Generalization Bounds}
Recall that our goal is to establish~\cref{thm:mainintro}, showing that with probability at least $1-\delta$ over $ \rS \sim \cD^m$, it holds for all $ \gamma\in\left[0,1\right] $ and voting classifiers $f$ over a hypothesis class $\cH$ of VC-dimension $d$ that:
\begin{align}
 \ls_{\cD}(f)= \ls^{\gamma}_{\rS}(f)+O\left(\sqrt{\ls^{\gamma}_{\rS}(f)\left(\frac{d\func{\left(\frac{\gamma^{2}m}{d} \right)}}{\gamma^{2}m}+\frac{\ln{\left(\frac{1}{\delta}\right)}}{m} \right)}+\frac{d\func{\left(\frac{\gamma^{2}m}{d} \right)}}{\gamma^{2}m}+\frac{\ln{\left(\frac{1}{\delta}\right)}}{m} \right),\label{eq:overviewgoal}
\end{align}
where $\func(x) = \ln(x)\ln^2(\ln x)$.

Let us first introduce a more convenient way of representing voting classifiers. Recall that voting classifiers $f$ are of the form $f(x) = \sign(\sum_i \alpha_i h_i(x))$ with all $\alpha_i > 0$. Furthermore, the margin on a training example $(x,y)$ is defined as $y \sum_i \alpha_i h_i(x)/\sum_j \alpha_j$. To avoid the tedious normalization by $\sum_j \alpha_j$, we henceforth assume all voting classifiers have $\sum_i \alpha_i=1$. In addition, we will drop the $\sign(\cdot)$ and instead write $f(x) = \sum_i \alpha_i h(x)$. In this way, we have that $\sign(f(x)) \neq y$ if and only if $y f(x) \leq 0$ (we define $\sign(0)=0$). Furthermore, the margin is no more than $\gamma$ if and only if $y f(x) \leq \gamma$. We hence define $\dlh$ as the set of all convex combinations $\sum_i \alpha_i h_i$ for $h_i \in \cH$ (i.e. $\sum_i \alpha_i =1$ and $ \alpha_{i}>0 $) and refer to $\dlh$ as the set of voting classifiers over $\cH$. With this notation, we have $\ls^\gamma_\cD(f) = \Pr_{(\rx, \ry) \sim \cD}[\ry f(\rx) \leq \gamma]$, $\ls^\gamma_S(f)$ is the fraction of training examples $(x,y) \in S$ with $yf(x) \leq \gamma.$ For $ \gamma=0 $ we will use  $\ls_\cD(f)= \ls^0_\cD(f) =\Pr_{(\rx, \ry) \sim \cD}[\ry f(\rx) \leq \gamma]=\Pr_{(\rx, \ry) \sim \cD}[\sign(f(\rx))\not=\ry]$.

\paragraph{Partitioning into Intervals.}
To establish~\cref{eq:overviewgoal}, we first simplify the task by partitioning the range of $\gamma$ and $\ls_\rS^\gamma(f)$ into small intervals $[\gamma_0^i,\gamma_1^i]$ and $[\tau_0^{i,j},\tau_1^{i,j}]$, respectively. For each interval $[\gamma_0,\gamma_1]=[\gamma_0^i,\gamma_1^i]$ and $[\tau_0,\tau_1] = [\tau_0^{i,j},\tau_1^{i,j}]$, we show that for any $\delta$, with probability at least $1-\delta$ we have for every $f \in \dlh$ and $\gamma \in [\gamma_0,\gamma_1]$ that either: $\ls^\gamma_{\rS}(f) \notin [\tau_0,\tau_1]$ or
\begin{align}\label{eq:proofsketch2}
 \ls_{\cD}(f)\leq \tau_{1}+O\left(\sqrt{\tau_{1}\left(\frac{d\func{\left(\frac{m \gamma_{0}^{2}}{d} \right)}}{\gamma_{0}^{2}m}+\frac{\ln{\left(\frac{1}{\delta}\right)}}{m} \right)}+\left(\frac{d\func{\left(\frac{m\gamma_{0}^{2}}{d} \right)}}{\gamma_{0}^{2}m}+\frac{\ln{\left(\frac{1}{\delta}\right)}}{m} \right) \right).
\end{align}
We can then union bound over all intervals, choosing appropriate values $\delta_{i,j} < \delta$, to conclude that with probability $1-\delta$, the guarantee~\cref{eq:proofsketch2} holds simultaneously for all intervals. If we choose the length of the intervals small enough, all $\gamma \in [\gamma_0, \gamma_1]$ and $\ls^\gamma_{\rS}(f) \in [\tau_0,\tau_1]$ are sufficiently close that we may substitute all occurrences of $\gamma_0$ and $\tau_1$ in~\cref{eq:proofsketch2} by $\gamma$ and $\ls^\gamma_{\rS}(f)$. Such a partitioning is standard in proofs of margin bounds, although we have to be a little careful in defining the intervals small enough. Having done so, this recovers~\cref{eq:overviewgoal}.

\input{proofsketchghostsetupdated.tex}

\paragraph{Clipping.}
While the above argument gives the correct type of bound  $\tau_{1} +\sqrt{\tau_{1}\ln{\left(|N|/\delta \right)}/m} +\ln{\left(|N|/\delta \right)}/m$, the size of the above suggested cover $ N $ turns out to be too large.
The intuitive reason is that the functions $f \in \dlh$ take values in the range $[-1,1]$, whereas we only really care about the values being larger than $\gamma$ or smaller than $-\gamma$ in the losses $\ls_{\rS}^\gamma(f)$ and $\ls_{\rS'}^\gamma(f)$. Constructing a cover for the full range $[-1,1]$ thus leads to a larger cover size than necessary and hence is too costly for a union bound.
Our idea to remedy this, is to \emph{clip} the voting classifiers in $\dlh$. For this let $ \gamma>0 $, and $ f $ be a function from $ \cX $ into $ [-1,1],$ we then define  $ f_{\left\lceil\gamma\right\rceil} $ as the function from $ \cX\rightarrow [-1,1] $ given by
\begin{align}
    f_{\left\lceil\gamma\right\rceil}(x)=\begin{cases}
        \gamma &\text{ if }f(x)> \gamma
        \\
        -\gamma &\text{ if } f(x)<- \gamma
        \\
        f(x) &\text{ else }
    \end{cases},
\end{align}         
and $ \dlh_{\left\lceil\gamma \right\rceil}=\{ f_{\left\lceil\gamma\right\rceil}:f\in \dlh \},$ i.e. the functions in $ \dlh $ capped to respectively $ -\gamma $ and $ \gamma $ if it goes below or above $ -\gamma $ or $ \gamma $. We will show that $\dlh_{\left\lceil\gamma_1 \right\rceil}$ has a small $\gamma_0/2$-cover $ \Net $ of cardinality just
\begin{align}
  \cN_{\infty}(X,\dlh_{\left\lceil\gamma_1 \right\rceil},\gamma_0/2)=\exp{\left(O\left( \frac{d}{\gamma_0^{2}}\func{\left(\frac{m\gamma_0^{2}}{d} \right)} \right)\right)}, \label{eq:coversize}
\end{align} 
where the notation $ \cN_{\infty}(\cdot,\cdot,\cdot) $  for a point set $ X\subseteq \cX$   function class $ \cF\subseteq \mathbb{R}^{\cX}$ and precision parameter $ \alpha $ means the smallest size, $ \cN_{\infty}(X,\cF,\alpha),$  of an $ \alpha $ $ \ell_{\infty} $-covering of $ \cF $, $ \Net,$  on $ X $. 
\paragraph{Relating Covering Number and Fat Shattering.}
We finally need to bound the covering number as in~\cref{eq:coversize}, where a key part of the argument is that we use $ \dlh_{\left\lceil\gamma_1 \right\rceil} $ instead of $\dlh $.  With the goal of establishing this bound on the cover size, we use a result by \cite{RudelsonVershynin} relating the covering number to fat shattering. Let us first recall the definition of fat shattering.

For at point set $\{  x_{1},\ldots,x_d\} $ of size $ d $ and a level parameter $ \beta>0 $, we say that a function class $ \cF $ $ \beta $-shatters   $ \{  x_{1},\ldots,x_d\} $ if there exists levels $ r_1,\ldots,r_{d} $ such that for any $ b\in \{  -1,1\}^{d} $, we have that there exists $ f\in \cF $ such that 
    \begin{align*}
        f(x_i)&\leq r_{i}-\beta  \quad \text{ if  } b_{i}=-1
        \\
        f(x_i)&\geq r_{i}+\beta  \quad \text{ if  } b_{i}=1,
    \end{align*}
    that is, the function class is rich enough to be $ \beta $  above or below the levels $ r_{1},\ldots,r_{d} $ on the point set $ \{  x_{1},\ldots,x_d\}$.  
    For a function class $ \cF $ and level $ \beta>0 $  we define $ \fat_{\beta}(\cF) $  as the largest number $ d $,  such that there exists a point set $ x_{1},\ldots,x_{d}   $ of size $ d $, which is $ \beta $-shattered by $ \cF $.


    With the definition of $\fat_\beta(\cF)$ in place,~\cite{RudelsonVershynin}  [Theorem 4.4] says that for any $ 0<\alpha <1/2$, any $0 < \eps < 1$, any function class $ \cF $ with $ \fat_{c\alpha\eps} $-dimension $ d_{c\alpha\eps} $, for a constant $ c>0 $, and any point set $ X\subseteq \cX $, with $ |X|=m $, such that $ \sum_{x\in X}|f(x)|/m \leq 1$ for any $ f\in \cF $, it holds that $ \ln{\left(\cN_{\infty}(X,\cF,\alpha) \right)}=O\left(d_{c\alpha\eps}\ln{\left(\frac{m}{d_{c\alpha\eps}\alpha} \right)}\ln^{\eps}{\left(\frac{m}{d_{c\alpha\eps}} \right)}\right)$. 
    The above bound looks quite different from \cref{eq:coversize}, but we will later choose the variable $ \eps $ in an appropriate way and recover \cref{eq:coversize}. 

A first naive approach would be to invoke the result of~\cite{RudelsonVershynin} directly on $\dlh_{\left\lceil\gamma_1 \right\rceil}$ (or even the unclipped $\dlh$), to conclude that
\begin{align}
\ln{\left(\cN_{\infty}(X,\dlh_{\left\lceil\gamma_1 \right\rceil},\gamma_0/2) \right)}=O\left(d_{c\gamma_0\eps/2}\ln\left(\frac{m}{d_{c\gamma_0\eps/2} \gamma_0} \right)\ln^{\eps}\left({\left(\frac{m}{d_{c\gamma_0\eps/2}} \right)}\right) \right), \label{eq:naivecover}
\end{align}
We will later show that $d_{c\gamma_0\eps/2} = O(d/ (\gamma_0\eps)^{-2})$ for $\dlh_{\left\lceil\gamma_1 \right\rceil}$, with $ d $ being the VC-dimension of $ \cH $. Inserting this in the above and considering $ \eps $ as a constant we see this  fails to recover our claimed covering number in~\cref{eq:coversize}. In particular, if considering $ \eps $ as a constant, the $\ln(m/(d_{c\gamma_0\eps/2}  \gamma_0))$ factor would become $\ln(\gamma_0 m/d)$ rather than the claimed $\ln(\gamma_0^2 m/d)$. Again, this difference turns out to be crucial for our Majority-of-3 algorithm as we will argue later.

To remedy this, we exploit the clipping. We observe that for $ f\in \dlh_{\left\lceil\gamma_1 \right\rceil} $, we have that $ \sum_{x\in X}|f(x)|/m \leq \gamma_1$. We may thus invoke the result of~\cite{RudelsonVershynin} on the scaled function class $ \gamma_1^{-1} \cdot\dlh_{\left\lceil\gamma_1 \right\rceil} =\{ f\mid \exists f'\in \dlh, f=\gamma_1^{-1} f'  \} $, i.e.\ the functions in $ \dlh_{\left\lceil\gamma_1 \right\rceil} $ scaled by $ \gamma_1^{-1} $ to get that
\[
  \ln{\left(\cN_{\infty}(X,\gamma_1^{-1} \cdot\dlh_{\left\lceil\gamma_1 \right\rceil},\gamma_0/(2\gamma_1)) \right)}=O\left(d_{c\gamma_0\eps/(2\gamma_1)}\ln^{1+\eps}{\left(\frac{m}{d_{c\gamma_0\eps/(2\gamma_1)}} \right)}\right),
\]
where $ d_{c\gamma_0\eps/(2\gamma_1)} $ is the $ \fat_{c\gamma_0\eps/(2\gamma_1)}$-dimension of $ \gamma_1^{-1} \cdot\dlh_{\left\lceil\gamma_1 \right\rceil}$. Picking a minimal $\gamma_0/(2 \gamma_1)$ covering $\Net$ for $\gamma_1^{-1} \cdot\dlh_{\left\lceil\gamma_1 \right\rceil}$ and downscaling all functions in $\Net$ by $\gamma_1$ results in $\gamma_1 \Net$ being a $\gamma_0/2$ covering for $\dlh_{\left\lceil\gamma_1 \right\rceil}$ as required. We have thus exploited the clipping to show that
\[
  \ln{\left(\cN_{\infty}(X,\dlh_{\left\lceil\gamma_1 \right\rceil},\gamma_0/2) \right)}=O\left(d_{c\gamma_0\eps/(2\gamma_1)}\ln^{1+\eps}{\left(\frac{m}{d_{c\gamma_0\eps/(2\gamma_1)}} \right)}\right),
\]
All that remains is thus to bound $d_{c\gamma_0\eps/(2 \gamma_1)}$, the $ \fat_{c\gamma_0\eps/(2\gamma_1)}$-dimension of $\gamma_1^{-1} \dlh_{\left\lceil\gamma_1 \right\rceil}$ and then set $ \eps $ appropriate. Now the  $ \fat_{c\gamma_0\eps/(2 \gamma_1)} $-dimension of $ \gamma_1^{-1} \cdot\dlh_{\left\lceil\gamma_1 \right\rceil}$ is, due to the scale invariance of $ \fat $-dimension, the same as the  $ \fat_{c\gamma_0\eps/2} $-dimension of $ \dlh_{\left\lceil\gamma_1 \right\rceil}$. We have thus improved~\cref{eq:naivecover} to
\begin{align*}
\ln{\left(\cN_{\infty}(X,\dlh_{\left\lceil\gamma_1 \right\rceil},\gamma_0/2) \right)}=O\left(d_{c\gamma_0\eps/2}\ln^{1+\eps}\left(\frac{m}{d_{c\gamma_0\eps/2} } \right) \right), 
\end{align*}
Inserting the claimed bound of $d_{c\gamma_0\eps/2} = O(d (\gamma_0\eps)^{-2})$ and setting $ \eps=1/\ln(\ln(m\gamma_{0}^{2}/d)) $  gives
\begin{align*}
  \ln{\left(\cN_{\infty}(X,\dlh_{\left\lceil\gamma_1 \right\rceil},\gamma_0/2) \right)}=O\left(\frac{d}{\gamma_{0}^{2}\eps^{2}}\ln^{1+\eps}\left(\frac{m\gamma_{0}^{2}\eps^{2}}{d} \right) \right)=O\left(\frac{d}{\gamma_{0}^{2}}\ln^{2}{\left(\ln{\left( \frac{m\gamma_{0}^{2}}{d}\right)} \right)}\ln\left(\frac{m\gamma_{0}^{2}}{d} \right) \right), 
  \end{align*}
where in the last inequality have used that $ \exp(\eps\ln{\left(\ln{\left(m\gamma_{0}^{2}/d \right)} \right)})=O(1).$ Since $ \func(x)=\ln^{2}{\left(\ln{\left(x \right)} \right)}\ln{\left(x \right)} $ the above establishes~\cref{eq:coversize} and completes our bound on the covering number and. All that remains is thus to argue that $d_{c\gamma_0\eps/2}= O(d (\gamma_0\eps)^{-2})$.

\paragraph{Bounding Fat Shattering Dimension.}
To bound $d_{c\gamma_0\eps/2}$, we use an argument similar to the proof of \cite{larsen2022optimalweakstronglearning} [Lemma 9]. Assume $ \dlh_{\left\lceil\gamma_1 \right\rceil}$ $ c\gamma_0\eps/2$-shatters a set of $n$ points $ x_1,\ldots,x_{n} $, with witness $ r_{1},\ldots,r_{n}\in [-\gamma_1,\gamma_1] $. By definition of shattering, we then have that for any $  b\in \{ -1, 1\}^{n}$, there exists $ f\in  \dlh_{\left\lceil\gamma_1 \right\rceil}$ such that $ b_{i}(f(x_{i})-r_{i})\geq c\gamma_0\eps/2$ for all $ i=1,\ldots,n$.

We next observe that since $ f\in  \dlh_{\left\lceil\gamma_1 \right\rceil}$ is equal to $ \min(\max(f',-\gamma_1),\gamma_1) $  for $ f'\in \dlh $, we have that $ f' $ also satisfies $ b_{i}(f'(x_{i})-r_{i})\geq c\gamma_0\eps/2,$ implying that $ \dlh$ also $c\gamma_0\eps/2$-shatters $ x_1,\ldots,x_{n} $ with the same witness. We can now upper and lower bound the Rademacher complexity of $ \dlh $ as follows
\begin{align}\label{eq:proofsketch10}
    c\gamma_0\eps/2 \leq \e_{\sigma\sim \{ -1,1 \}^{n} }\left[\sup_{f\in \dlh} \sum_{i=1}^{n}\sigma_{i}(f(x_{i})-r_{i})/n\right] = \e_{\sigma\sim \{ -1,1 \}^{n} }\left[\sup_{f\in \cH} \sum_{i=1}^{n}\sigma_{i}f(x_{i})\right]  \leq c' \cdot \sqrt{\frac{d}{n}},
\end{align}
for a constant $c'>0$. The first inequality holds because for any $\sigma \in \{-1,1\}^n$, by definition of $c\gamma_0\eps/2$-shattering, there is an $f \in \dlh$ with $ \sigma_{i}(f(x_{i})-r_{i})\geq c\gamma_0\eps/2$ for all $i$. The equality holds because $-\sigma_i r_i/n$ is independent of $f$, and thus can be moved outside the $\sup$, and we have $\e[\sigma_i r_i/n] = \e[\sigma_i]r_i/n=0$. Furthermore, observe that in the equality, we also replace $\sup_{f \in \dlh}$ by $\sup_{f \in \cH}$. This is true since for any convex combination $f \in \dlh$ with $f = \sum_j \alpha_j h_j$ we have $ \sum_{i}\sigma_{i}f(x_{i})=\sum_{j}\alpha_{j}\sum_{i}\sigma_{i}h_{j}(x_{i})\leq \sup_{h\in\cH}\sum_{i}\sigma_{i}h(x_{i}) $ implying $ \sup_{f\in\dlh} \sum_{i}\sigma_{i}f(x_{i})\leq \sup_{h\in\cH}\sum_{i}\sigma_{i}h(x_{i}).$ Furthermore, since $ \cH\subseteq\dlh $ the opposite inequality also holds so we have an equality. The last inequality is by classic bounds on the Rademacher complexity of classes with bounded VC-dimension, due to a bound by \cite{dudley} [see e.g. \cite{rademacherboundlecturenotes}, Theorem 7.2]. By rearrangement of \cref{eq:proofsketch10} we conclude that $ n= O(d (\gamma_0\eps)^{-2}) $ as claimed, which concludes the proof sketch.

\subsection{Majority-of-3}
\label{sec:sketchmaj3}
We finally describe the main ideas in our proof that Majority-of-3 AdaBoosts achieves an optimal in-expectation error of $O(d/(\gamma^2m))$ as stated in \cref{cor:maj3intro}. We will also explain why it is crucial for this result, that the logarithmic factors in our margin generalization bound~\cref{thm:mainintro} are $\ln(\gamma^2 m/d)$ and not $\ln(m/d), \ln(m)$ or $\ln(\gamma m/d)$. Let $\cD$ be the unknown data distribution over $\cX$ and let $t \in \{-1,1\}^\cX$ be the unknown target concept.
Recall that if AdaBoost is run for $\Omega(\gamma^{-2} \ln m)$ iterations with a $\gamma$-weak learner $\cW$, then it produces a voting classifier with margins $\Omega(\gamma)$ on all examples in the training sequence $\rS \sim \cD_t^m$. We now use an analysis idea by~\cite{majorityofthree}, used to show that the Majority-of-3 Empirical Risk Minimizers has an optimal in-expectation error for realizable PAC learning. 

Consider partitioning a training sequence $\rS \sim \cD^{(2k-1)m}$ into $2k-1$ equal sized training sequences $\rS_1,\dots\rS_{2k-1}$ (with $k=2$ for Majority-of-3 and $k=3$ for Majority-of-5) of $m$ training examples each (rescaling $m$ by $2k-1$ recovers the guarantees for a training sequence of size $m$). If we run AdaBoost on each to obtain voting classifiers $f_{\rS_1},\dots,f_{\rS_{2k-1}}$, then each $f_{\rS_i}$ has margins $\Omega(\gamma)$ on all of $\rS_i$. For concreteness, let us say all margins are at least $\gamma/2$. Furthermore, for any point $x \in \cX$, we have that if the majority vote $\maj(f_{\rS_{1}},\cdots,f_{\rS_{2k-1}})$ errs on $x$, where we define the majority vote as $ \maj(f_{\rS_{1}},\ldots,f_{\rS_{2k-1}})=\sign(\sum_{i=1}^{2k-1}\sign(f_{i})) $, then at least $k$ of the voting classifiers err on $x$. Let us fix an $x \in \cX$ and denote by $p_x$ the probability that $f_{\rS_i}$ errs on $x$, where the probability is over the random choice of $\rS_i$. Observe that since the training sequences $\rS_i$ are i.i.d., this is the same probability for each $\rS_i$. Moreover, by independence of the $\rS_i$'s, we have that the probability that a fixed set of $k$ of the voting classifiers all err on $x$ is precisely $p_x^k$. A union bound over all $\binom{2k-1}{k} \leq 2^{2k}$ choices of $k$ voting classifiers implies that
\[
  \Pr_{\rS}[\maj(f_{\rS_{1}},\cdots,f_{\rS_{2k-1}})(x) \neq t(x)] \leq 2^{2k} p_x^k.
\]
By swapping the order of expectation, we can bound the expected error of $\maj(f_{\rS_{1}},\cdots,f_{\rS_{2k-1}})$ as follows
\begin{align*}
  \e_{\rS}[\ls_{\cD_t}(\maj(f_{\rS_{1}},\cdots,f_{\rS_{2k-1}}))] &=\e_{(\rx,t(\rx)) \sim \cD_t}[\Pr_{\rS}[ \maj(f_{\rS_{1}},\cdots,f_{\rS_{2k-1}})\neq t(\rx)]] \\
  &=\e_{\rx \sim \cD}[2^{2k} p_\rx^k].
\end{align*}
Using the approach in~\cite{majorityofthree}, we now partition the input domain $\cX$ into regions $R_i$, such that $R_i = \{x \in \cX : p_x \in (2^{-i-1}, 2^{-i}]\}$ for $i=0,\dots,\infty$. Letting $\Pr[R_i]$ denote $\Pr_{\rx \sim \cD}[\rx \in R_i]$ and using the notation $\e_{\rx \sim \cD}[ \cdot \mid R_i]$ to denote the conditional expectation, when conditioning on $\rx \in R_i$, we now have that
\begin{align}
  \e_{\rx \sim \cD}[2^{2k} p_\rx^k] &= \sum_{i=0}^\infty \e_{\rx \sim \cD}[2^{2k} p_\rx^k \mid R_i] \Pr[R_i] \nonumber\\
  &\leq 2^{2k} \cdot \sum_{i=0}^\infty 2^{-ik} \Pr[R_i].\label{eq:boundexp}
\end{align}
The goal is thus to bound $\Pr[R_i]$. For this, the key is to exploit that $p_x > 2^{-i-1}$ for $x \in R_i$. Let $m_i = \Pr[R_i] m$ denote the expected number of samples from $R_i$ in a training sequence $\rS_j \sim \cD_t^m$. Since AdaBoost produces a voting classifier with margins $\Omega(\gamma)$ on all points in its training data set, it in particular has margins $\Omega(\gamma)$ on all data points in $\rS_j \cap R_i$. We note that these samples are i.i.d.\ from the conditional distribution of a $\rx \sim \cD$, conditioned on $\rx \in R_i$. Let us denote this conditional distribution by $\cD_t \mid R_i$. We can now invoke our new margin generalization bound in~\cref{thm:mainintro} to conclude that
\begin{align}
  \e_{\rS_i}[\ls_{\cD_t \mid R_i}(f_{\rS_i})] = O\left(\frac{d \func(\gamma^2 m_i/d)}{\gamma^2 m_i} \right), \label{eq:fromgenbound}
\end{align}
with $\func(x)=\ln(x)\ln^2(\ln x)$.
Note that~\cref{thm:mainintro} actually gives a high probability guarantee, which in particular implies the above guarantee on the expected error. The crucial point is that we invoke~\cref{thm:mainintro} with the conditional distribution $\cD_t \mid R_i$ and $\ls_{\rS_i}^{\gamma/2}(f_{\rS_i}) = 0$ since we have all margins at least $\gamma/2$, and we have $m_i$ samples from this distribution (in expectation). On the other hand, we have by definition of $R_i$ that $\e_{\rS_i}[\ls_{\cD_t \mid R_i}(f_{\rS_i})] > 2^{-i-1}$. Writing $x = \gamma^2 m_i/d$ for short, we thus conclude that
\begin{align*}
  \frac{\func(x)}{x} = \Omega\left(2^{-i}\right) \Rightarrow x = O\left(i \ln^2(i) 2^{i} \right) \Rightarrow m_i = O\left( \frac{d i \ln^2(i) 2^i}{\gamma^2}\right) \Rightarrow \Pr[R_i] = O\left( \frac{d i \ln^2(i)2^i}{\gamma^2m}\right)
\end{align*}
Inserting this in~\cref{eq:boundexp} bounds the expected error by (for constant $k$):
\begin{align}
\e_{\rS}[\ls_{\cD_t}(\maj(f_{\rS_{1}},\cdots,f_{\rS_{2k-1}}))] &= O\left( \sum_{i=0}^\infty \frac{2^{-ik} i \ln^2(i) 2^i d}{\gamma^2 m}\right).\label{eq:infsum}
\end{align}
Inserting $k=2$ (corresponding to Majority-of-3) gives the desired $O(d/(\gamma^2 m))$ as the $2^{-ik}=2^{-2i}$ decreases fast enough to cancel the $i\ln^2(i) 2^i$ factors.

\paragraph{Failure of Previous Bounds.}
Let us now discuss why the $\func(\gamma^2 m/d)$ factor is crucial compared to $\ln(m)$, $\ln(\gamma m/d)$ and $\ln(m/d)$ factors in the above analysis. Consider again~\cref{eq:fromgenbound} and assume for simplicity that the generalization bound had instead given us
\begin{align}\label{eq:proofsketch0}
  \e_{\rS_i}[\ls_{\cD_t \mid R_i}(f_{\rS_i})] = O\left(\frac{d \ln(\gamma m_i/d)}{\gamma^2 m_i} \right),
\end{align}
i.e.\ a slightly suboptimal dependency on $\gamma$ inside the $\ln(\cdot) \ln^2(\ln(\cdot ))$. We would then get the inequality
\[
  \frac{d \ln(\gamma m_i/d)}{\gamma^2 m_i} = \Omega(2^{-i}).
\]
Now again letting $ x=\gamma^{2}m_{i}/d $ then the above can be shown to imply
\begin{align*}
  \frac{\ln(x/\gamma)}{x} = \Omega\left(2^{-i}\right) \Rightarrow x = O\left(\ln{\left(2^i/\gamma \right)} 2^{i} \right) \Rightarrow \Pr[R_i] = O\left( \frac{d\ln{\left(2^i/\gamma \right)}2^i}{\gamma^2m}\right).
\end{align*}
Now plugging this bound $ \Pr[R_{i}] $  into \cref{eq:boundexp} (with $ k=2 $), we get the following error bounds for $ \e_{\rS}[\ls_{\cD_t}(\maj(f_{\rS_{1}},f_{\rS_{2}},f_{\rS_{3}}))]  $ of
\[
  O\left( \sum_{i=0}^\infty \frac{2^{-2i}d \ln(2^i/\gamma) 2^i }{\gamma^2 m}\right) = O\left(\frac{d \ln(1/\gamma)}{\gamma^2 m} \right).
\]
The obtained error bound of $ \e_{\rS}[\ls_{\cD_t}(\maj(f_{\rS_{1}},f_{\rS_{2}},f_{\rS_{3}}))]  $ thus has a superfluous $ \ln{\left(1/\gamma \right)}$ factor.

These shortcomings of previous margins bounds used in the above analysis of the expected error $ \e_{\rS}[\ls_{\cD_t}(\maj(f_{\rS_{1}},f_{\rS_{2}},f_{\rS_{3}}))]  $, adding a superfluous $ \ln{\left(1/\gamma\right)} $-factor, is precisely the reason why previous work needed a Majority-of-5. Being unable to use the margin generalization bounds with sub-optimal $\ln( \cdot )$ factors, the work~\cite{manyfacesofoptimalweaktostronglearning} instead relied on the much weaker guarantee that any voting classifier $f$ with margins $\gamma$ has  $\ls_{\cD_t}(f) = O(\sqrt{d/(\gamma^2 m)}),$ where this bound can be obtained by following the steps of~\cite{boostingbook} [page 107-111] and using the stronger bound on the Rademacher complexity for a function class with VC-dimension $ d $ of $ O(\sqrt{d/m}) $ due to \cite{dudley} [See e.g. Theorem 7.2 \cite{rademacherboundlecturenotes}], instead of the weaker $ O(\sqrt{d\ln{\left(m/d \right)}/m})$ used in \cite{boostingbook}. This bound has the right behaviour for $m \approx d/\gamma^2$ unlike the bounds with sub-optimal logarithmic factors and results in the guarantee $\Pr[R_i] = O(2^{2i})$ instead of $O(i \ln^2(i) 2^i)$. This needs $k=3$ for $2^{-ik}$ to dominate $2^{2i}$ in $\sum_{i=0}^\infty 2^{-ik} 2^{2i}$ from ~\cref{eq:infsum}, whereas it suffices for us with $k=2$ since we only need to bound $\sum_{i=0}^\infty 2^{-ik} i \ln^2(i) 2^{i}$, leading to the more natural Majority-of-3 instead of Majority-of-5.

\paragraph{Organization of Paper.}
The following sections are organized as follows. In~\cref{sec:upperbound} we prove the margin generalization bound in \cref{eq:proofsketch2}. In~\cref{sec:cover} we prove the bound on the covering number of the clipped function class $\dlh_{\left\lceil\gamma \right\rceil}$.  In~\cref{sec:finalbound}, we put together the results from~\cref{sec:upperbound} and~\cref{sec:cover} to prove the main margin generalization bound in~\cref{thm:mainintro}. In~\cref{sec:mainmajoritythree} we prove the main result on Majority-of-3 in \cref{cor:maj3intro}.  