\subsection{Notation and preliminaries}\label{sec:notationandpreliminaries}


We will in the following use $ \cX $  for the input space and $ (\cX\times \{ -1,1 \})^{*}  $ for the set of all possible labelled sequences, allowing for repetitions of labelled examples in the sequence. Furthermore, we will for a target concept $ t \in \{  -1,1\}^{\cX}  $  use $ (\cX\times\{  -1,1\})_{t}^{*} $ for the set of all possible training sequences on $ \cX $, labelled by $ t $, that is for $ S\in (\cX\times\{  -1,1\})_{t}^{*} $, any train example $ (x,y)\in S $, is such that $ y=t(x).$ We remark that $ S $ is seen as a vector/sequence so it may have repetitions of similar train examples. Furthermore, for a distribution $ \cD $ over $ \cX $ we write $ \cD_{t} $ for the distribution over $ \cX\times\{  -1,1\}  $, defined by $ \p_{(\rx,\ry)\sim\cD_{t}}\left[(\rx,\ry)\in A\right]=\p_{\rx\sim \cD}\left[(\rx,t(\rx))\in A\right] $ for any  $ A\subseteq \cX\times\{  -1,1\}$. Furthermore, for $ R\subset \cX $, such that $ \p_{\rx\sim\cD}\left[\rx\in R\right]\not=0 $ , we define $ \cD_{t}\mid R $ as $ \p_{(\rx,\ry)\sim\cD_{t}\mid R}\left[(\rx,\ry)\in A\right]=\p_{\rx\sim \cD}\left[(\rx,t(\rx))\in A\mid \rx\in R\right]=\p_{\rx\sim \cD}\left[(\rx,t(\rx))\in A, \rx\in R\right]/\p_{\rx\sim \cD}\left[\rx\in R\right].$

We will use $ \cH \subseteq \{  -1,1\}^{\cX} $ for a hypothesis class. We also define the function class $ \dlh =\{\sum_{h\in \cH}\alpha_{h} h\mid \sum_{h\in\cH} \alpha_{h}=1  \}$ of convex combinations of hypotheses from the hypothesis class $ \cH $.  We define a learning algorithm $ L $  as a mapping from $ (\cX\times\{  -1,1\})_{t}^{*} $ to $ \mathbb{R}^{\cX} $, for $ S\in (\cX\times\{  -1,1\})_{t}^{*} $, we write $ L_{S} $ for short of $ L(S)\in\mathbb{R}^{\cX} $. Furthermore, if $ L_{S} \in \dlh$ for any $ S\in (\cX\times\{  -1,1\})_{t}^{*} $ we write $ L\in \dlh $ . For $ 0<\gamma<1 $ and target concept $ t $  we define a $ \gamma $-margin algorithm $ L $ for $ t $   as a mapping from $ (\cX\times\{  -1,1\})_{t}^{*} $ to $ \mathbb{R}^{\cX} $, such that for a $ S\in (\cX\times\{  -1,1\})_{t}^{*} $, we have that $ L_{S}(x)y\geq \gamma $ for all $ (x,y)\in S.$ Furthermore, for three functions $ f_{1},f_{2},f_{3} $ we define $ \maj(f_{1},f_{2},f_{3})=\sign(\sign(f_{1})+\sign(f_{2})+\sign(f_{3})) $, with $ \sign(0)=0.$   

For $0< \gamma <1$ and function $ f:\cX\rightarrow [-1,1] $ let $ f_{\left\lceil\gamma\right\rceil} $ be the function from $ \cX\rightarrow [-1,1] $ given by
    \begin{align}
        f_{\left\lceil\gamma\right\rceil}(x)=\begin{cases}
            \gamma &\text{ if }f(x)> \gamma
            \\
            -\gamma &\text{ if } f(x)<- \gamma
            \\
            f(x) &\text{ else }
        \end{cases}
    \end{align}   
    Furthermore, for a function class $ \cF $ we  consider the function class $ \cF_{\left\lceil\gamma\right\rceil} =\{  f_{\left\lceil\gamma\right\rceil}\}_{f\in \cF}$, i.e. the family obtained by considering the functions $ f $  in $ \cF $ where the $ (\cdot)_{\{ \left\lceil\gamma\right\rceil \}} $  operator is used on.
    For a binary function class $ \cF\subseteq \{  -1,1\}^{\cX}  $ we define the VC-dimension $ d $ of $ \cF $ as the largest number such that there exists a point set $ \{  x_{1},\ldots,x_{d}\}  $ of size $ d $ and for every $ b\in \{  -1,1\}^{d}  $ there exists an $ f\in\cF $ such that for all $ i\in\{  1,\ldots,d\}  $,  $ f(x_{i}) =b_{i}$.      
    For a point set $ X=\{  x_{1},\ldots,x_{m}\}  \subseteq \cX $ of size $ |X|=m $, a function class $ \cF $ and $ \alpha>0 $,  we denote the minimal $ \alpha$-cover of $ \cF $ in infinity norm, by $\cN_{\infty}(X,\cF ,\alpha)  $, i.e. for all $ f\in \cF $ 
    \begin{align}
        \min_{f'\in\cN_{\infty}(X,\cF ,\alpha)   }\max_{x\in X}  \left| f(x)-f'(x) \right| \leq \alpha, 
    \end{align}
    and every other set of functions with this property has size at least $  \left| \cN_{\infty}(X,\cF ,\alpha)  \right|  $.

    For at point set $\{  x_{1},\ldots,x_d\} $ of size $ d $ and a level parameter $ \beta>0 $ we say that $ \cF $ $ \beta $-shatters     $ \{  x_{1},\ldots,x_d\} $ if there exists levels $ r_1,\ldots,r_{d} $ such that for any $ b\in \{  -1,1\}^{d} $, we have that there exists $ f\in \cF $ such that 
    \begin{align*}
        f(x_i)&\leq r_{i}-\beta  \quad \text{ if  } b_{i}=-1
        \\
        f(x_i)&\geq r_{i}+\beta  \quad \text{ if  } b_{i}=1,
    \end{align*}
    that is the function class is rich enough to be $ \beta $  above or below the levels $ r_{1},\ldots,r_{d} $ on the point set $ \{  x_{1},\ldots,x_d\}.$  
    For a function class $ \cF $ and level $ \beta>0 $  we define $ \fat_{\beta}(\cF) $  as the largest number $ d $  such that there exists a point set $ x_{1},\ldots,x_{d}   $ of size $ d $ which is $ \beta $-shattered by $ \cF $.

    For a distribution $ \cD $ over $ \cX\times \{  -1,1\}  $, margin $ 0<\gamma $  and a function $ f:\mathbb{R}^{\cX} $ we write $ \ls_{\cD}^{\gamma}(f) $ for its margin loss, i.e.
    \begin{align*}
        \ls_{\cD}^{\gamma}(f) =\p_{(\rx,\ry)\sim\cD}\left[f(\rx)\ry\leq \gamma\right]
    \end{align*}  
    when $ \gamma=0 $ we write  $\ls_{\cD}(f) =\p_{(\rx,\ry)\sim\cD}\left[f(\rx)\ry\leq 0\right]$. We remark that since we took $ \sign(0)=0 $ and labels $ y $ are in $ \{  -1,1\}  $   we have that $ \ls_{\cD}(f)=\p_{(\rx,\ry)\sim\cD}\left[f(\rx)\ry\leq 0\right]=\p_{(\rx,\ry)\sim \cD}\left[\sign(f(\rx))\not=\ry\right] $, i.e. we recover the $ 0 $-$ 1 $-loss for the function $ \sign(f).$     
    \textcolor{red}{$ \ls_{\cD} $ is not the 1-0 loss but the margin loss }
    In what follows we will also use the truncated natural logarithm $ \Ln$ which we define as 
    \begin{align*}
      \Ln(x)=\begin{cases}
        \ln{\left( x\right)} &\text{ if } x\geq e 
        \\
        1                     &\text{ if }   x<e.
      \end{cases}
    \end{align*}