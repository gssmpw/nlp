\paragraph{Ghost set argument:}
Thus, we focus on showing the claim above~\cref{eq:proofsketch2}. To this end, we consider the following event for any $ 0<\delta<1 $: there exists $ \gamma\in\left[\gamma_{0},\gamma_{1}\right]$ and $ f\in \dlh $ such that $ \ls_{\rS}^{\gamma}(f)\in[\tau_{0},\tau_{1}] $ and   
\begin{align}\label{eq:proofsketch3}
  \ls_{\cD}(f)> \tau_{1}+\Theta\left(\sqrt{\tau_{1}\frac{\ln{\left(\frac{1}{\delta}\right)}}{m} }+\left(\frac{\ln{\left(\frac{1}{\delta}\right)}}{m} \right) \right),
 \end{align}
 where we get the claim above \cref{eq:proofsketch2} by considering the complement of the above event and rescaling $ \delta $ to be $ \delta $ over the size of an appropriate cover, which we will make more precise soon.

 To the end of letter in the argument being able to employ a cover over the function class $ \dlh $  we now introduce a ghost set $ \rS',$ to interchange $ \ls_{\cD} $ in \cref{eq:proofsketch3} with $ \ls_{\rS'}.$ Using that $ \ls_{\cD}(f)\geq \Theta(\ln{\left(1/\delta \right)}/m),$ and a Chernoff bound, we can show that the probability of the event that there exists $ \gamma\in\left[\gamma_{0},\gamma_{1}\right]$ and $ f\in \dlh $ such that $ \ls_{\rS}^{\gamma}(f)\in[\tau_{0},\tau_{1}] $ and \cref{eq:proofsketch3} holds can be upper bounded by the probability (up to a factor of $ 2 $ ) of the event that there exists $ \gamma\in\left[\gamma_{0},\gamma_{1}\right]$ and $ f\in \dlh $ such that $ \ls_{\rS}^{\gamma}(f)\in[\tau_{0},\tau_{1}] $   and
 \begin{align}\label{eq:proofsketch3}
  \ls_{\rS'}(f)> \tau_{1}+\Theta\left(\sqrt{\tau_{1}\frac{\ln{\left(\frac{1}{\delta}\right)}}{m} }+\frac{\ln{\left(\frac{1}{\delta}\right)}}{m} \right),
 \end{align}
 where the constant in the $ \Theta(\cdot) $-term is slightly smaller now. 

 Notice that the above event implies that there exists $ f\in \dlh $ which, on an i.i.d. training sequence $ \rS ,$ $ f $  has small empirical margin error $ \ls_{\rS}^{\gamma}(f)\leq \tau_{1} $ and, on another i.i.d. training sequence $ \rS',$ $ f $  has large empirical error $ \ls_{\rS'}(f).$ This is unlikely since both empirical error's are strongly concentrated around their mean, by the margin error being larger than the error. To exploit that this event happens with small probability, we would like to perform a union bound over the hypothesis space $ \dlh,$ but this is currently not possible since the size of $ \dlh $ could be infinite. Thus, we aim to discretize $ \dlh$ while maintaining this imbalance of $ \ls_{\rS}{\gamma} $ and $ \ls_{\rS'}. $   

 To this end, we discretize $\dlh$ on $ \rX=\rS\cup\rS' $  via a $\gamma_0/2$ $\ell_\infty$-covering $ \Net $, i.e., for any $f \in \dlh$, there is an $f' \in \Net$ with $|f(x)-f'(x)| \leq \gamma_0/2$ for all $x$. Now observe that whenever $yf(x) > \gamma_0$, we also have $yf'(x) > \gamma_0/2$. Thus, for any $\gamma \in [\gamma_0,\gamma_1]$, we have $\ls_S^\gamma(f) \geq \ls_S^{\gamma_0/2}(f')$. Similarly, we have for $yf(x) \leq 0$ that $yf'(x) \leq \gamma_0/2$, and thus $\ls_{\rS'}(f) \leq \ls_{\rS'}^{\gamma_0/2}(f')$. Therefore, a $\gamma_0/2$ $\ell_\infty$-covering $ \Net $ maintains the imbalance between $ \ls_{\gamma}{\rS}(f) $ and $ \ls_{\rS'}(f)$ for $ \ls_S^{\gamma_0/2}(f') $ and $ \ls_S^{\gamma_0/2}(f') $   
 
 To consider a $\gamma_0/2$ $\ell_\infty$-covering $ \Net $ of $ \rX $, we need the point set $ \rX $ to be fixed, while still being able to exploit the imbalance between $ \ls_S^{\gamma_0/2}(f') $ and $ \ls_S^{\gamma_0/2}(f')$ for some $ f'\in \Net.$ To this end, we employ the following way of viewing the drawing of $ \rS $ and $ \rS'.$ First, we draw $ \rX\sim \cD^{2m} $, consisting of $ 2m $ i.i.d. training examples from $ \cD $, and then let $ \rS $ be $ m $ points drawn without replacement from $ \rX,$ and $ \rS' $ be the remaining $ m $  points of $ \rX,$ i.e., $ \rS'=\rX\backslash \rS'.$ Taking this viewpoint of drawing $ \rS $ and $ \rS $ allows us to fix the realization $ X $ of the points in $ \rX,$ while still having which training examples ending up in $ \rS $ and $ \rS' $ being random. This allows us to argue that an imbalance of $ \ls_S^{\gamma_0/2}(f') $ and $ \ls_S^{\gamma_0/2}(f')$ for some $ f'\in \Net,$ is unlikely, since the number of errors is decided by the realization $ X$ of $ \rX, $ and with high probability, these errors should be equally distributed between $ \rS $ and $ \rS'.$

 Thus, we now consider a realization $ X $ of $ \rX, $ and let $ \Net $ be a $\gamma_0/2$ $\ell_\infty$-covering of $ X. $  
By the above argument about $ \Net $ being such that for any $ f\in \dlh $ and $ \gamma\in [\gamma_{0},\gamma_{1}] $, it holds that $\ls_S^\gamma(f) \geq \ls_S^{\gamma_0/2}(f')$ and $\ls_\cD(f) \leq \ls_{\cD}^{\gamma_0/2}(f'),$ it suffices to consider bounding the probability of the event that there $ \exists f\in \Net $ and $ \gamma\in [\gamma_{0},\gamma_{1}] $ such that $ \ls_{\rS}^{\gamma_{0}/2}(f)\leq \tau_{1} $ and 
\begin{align}\label{eq:proofsketch6}
\ls^{\gamma_{0}/2}_{\rS'}(f)> \tau_{1}+\Theta\left(\sqrt{\tau_{1}\frac{\ln{\left(\frac{1}{\delta}\right)}}{m} }+\frac{\ln{\left(\frac{1}{\delta}\right)}}{m} \right).
\end{align}            
Furthermore, since $ \Net $ is finite, if we show the above event for a fixed $ f \in \Net$ happens with probability at most $ \delta,$ we can now employ the union bound and get that the above event happens with probability at most $ |\Net|\delta.$

Now, to argue that for a fixed $ f $, the event $ \ls_{\rS}(f)\leq \tau_{1} $ and $ \ls_{\rS'}(f)>\tau_{1}+\Theta(\sqrt{\tau_{1}\ln{\left(1/\delta \right)}/m}+\ln{\left(1/\delta \right)}/m) $ happens with probability at most $ \delta, $ we first notice that the fraction of mistakes $ f $ makes on $ X $ is equal to $ (\ls_{\rS}(f)+\ls_{\rS'}(f))/2,$ and has to be at least $ (\tau_{1}+\Theta(\sqrt{\tau_{1}\ln{\left(1/\delta \right)}/m}+\ln{\left(1/\delta \right)}/m))/2 $ for the event described by \cref{eq:proofsketch6} to have non-zero mass, since on this event $ \ls_{\rS'}(f) $ is double that number, call this number $ \mu.$ Since the fraction of mistakes on $ X $, $ \mu $ is at least $ \Theta(\ln{\left( 1/\delta\right)}/m) $ and $ \ls_{\rS}(f) $ has expectation equal to $ \mu,$ it follows by an invocation of a Chernoff bound (without replacement) that with probability at least $ 1-\delta $ over $ \rS $ (drawn from $ X $ ) 
\begin{align*}
\ls_{\rS}(f)\geq (1-\sqrt{\frac{2\ln{\left(1/\delta \right)}}{\mu m}})\mu=(\ls_{\rS}(f)+\ls_{\rS'}(f))/2-\sqrt{\frac{2\ln{\left(1/\delta \right)}(\ls_{\rS}(f)+\ls_{\rS'}(f))/2}{m}}, 
\end{align*}    
where doing some rearrangements and calculations implies the following inequality
\begin{align*}
\frac{\ls_{\rS}(f)}{2}+\sqrt{\frac{\ls_{\rS}(f)\ln{\left(1/\delta \right)}}{m}}\geq \frac{\ls_{\rS'}(f)}{2}-\sqrt{\frac{\ls_{\rS'}(f)\ln{\left(1/\delta \right)}}{m}},
\end{align*}
which is saying that $ \ls_{S'}(f) $ cannot be too large compared to $ \ls_{S}(f),$ where we show that for $ \ls_{S}(f)\leq \tau_{1} $ and $ \ls_{S'}(f)\geq \tau_{1}+\Theta(\sqrt{(\tau_{1}\ln{\left(1/\delta \right)})/m}+\ln{\left(1/\delta \right)}/m)$ the above inequality does not hold, whereby we conclude that the probability of $ \ls_{\rS}(f)\leq \tau_{1} $ and $ \ls_{\rS'}(f)>\tau_{1}+\Theta(\sqrt{\tau_{1}\ln{\left(1/\delta \right)}/m}+\ln{\left(1/\delta \right)}/m) $ happening is at most $ \delta. $ Thus, as mentioned earlier, by rescaling $ \delta $ to $ \delta/|N| $ we get that with probability at least $ 1-\delta $ over $ \rS $ it holds for any $ \gamma\in[\gamma_{0},\gamma_{1}] $ and $f\in \dlh $ that either $ \ls_{\rS}^{\gamma}(f) \not\in [\tau_{0},\tau_{1}]$ or
\begin{align}\label{eq:proofsketch7}
\ls_{\cD}(f)> \tau_{1}+\Theta\left(\sqrt{\tau_{1}\frac{\ln{\left(\frac{N}{\delta}\right)}}{m} }+\left(\frac{\ln{\left(\frac{N}{\delta}\right)}}{m} \right) \right).
\end{align}
