\begin{table*}[t]
\centering
\renewcommand{\arraystretch}{1.1} % Adjusts the row spacing
\resizebox{16cm}{!} 
{ 
\begin{tabular}{cccccccccc}
\hline
\rowcolor[HTML]{EFEFEF} 
\textbf{Mode} &
  \textbf{Models} &
  \multicolumn{1}{l}{\cellcolor[HTML]{EFEFEF}\textbf{BLEU}} &
  \textbf{METEOR} &
  \textbf{ROUGE-1} &
  \textbf{ROUGE-L} &
  \textbf{BERTScore} &
  \textbf{Correctness} &
  \textbf{Faithfulness} &
  \textbf{Fluency} \\ \hline \hline
                                      & \textbf{\texttt{Llama2}}           & 1.39  & 3.59  & 5.57  & 4.09  & 66.49 & 32.18 & 37.68 & 32.47 \\
                                      & \textbf{\texttt{StructLM}}         & 6.21  & 11.96 & 20.09 & 15.34 & 82.56 & 64.30 & 70.08 & 63.10 \\
                                      & \textbf{\texttt{Mistral}}          & 4.19  & 9.55  & 25.64 & 18.99 & 82.12 & 77.02 & 81.16 & 76.5  \\
                                      & \textbf{\texttt{GPT-4o-mini}}      & 7.14  & 16.12 & 29.44 & 19.47 & 83.75 & \textbf{80.89} & \textbf{83.92} & \textbf{80.81} \\
\multirow{-5}{*}{\textbf{Zero-Shot}}       & \textbf{\texttt{Gemini-1.5-flash}} & 8.8   & 15.18 & 30.38 & 21.51 & 84.05 & {\ul 78.79} & {\ul 83.04} & {\ul 78.54} \\ \hline
                                      & \textbf{\texttt{Llama2}}           & 29.36 & 40.2  & 48.36 & 39.25 & 90.05 & 61.38 & 63.78 & 61.47 \\
                                      & \textbf{\texttt{StructLM}}         & {\ul 31.06} & {\ul 42.3}  & {\ul 49.42} & {\ul 40.58} & {\ul 90.9}  & 69.70 & 72.46 & 69.93 \\
\multirow{-3}{*}{\textbf{Fine-tuned}} & \textbf{\texttt{Mistral}}          & \textbf{38.89} & \textbf{49.43} & \textbf{56.64} & \textbf{48.32} & \textbf{92.18} & 73.07 & 76.63 & 73.03 \\ \hline
\end{tabular}
}
\caption{Evaluation results of zero-shot and fine-tuned models on the \textbf{eC-Tab2Text} dataset. The best results are highlighted in \textbf{bold}, and the second-best results are {\ul underlined}.}
%\caption{Results of Trained vs. Base Models: LLAMA2, StructLM, and Mistral\_Instruct}
\label{table:results1}
\end{table*}


\begin{comment}

\begin{table}[t]
    \scriptsize
    \begin{tblr}{hline{1,2,Z} = 0.8pt, hline{3-Y} = 0.2pt, vlines=0.1pt,
                 colspec = {Q[c,m, 0.35cm] Q[c,m] Q[c,l, 3.5em] Q[c,m, 3.0em, font=\fontfamily{qcr}] Q[c,m, 3.3em, font=\fontfamily{qcr}] Q[c,m, 3.3em, font=\fontfamily{qcr}] Q[c,m, 3.3em, font=\fontfamily{qcr}] Q[c,m, 3.3em, font=\fontfamily{qcr}] Q[c,m, 3.3em, font=\fontfamily{qcr}] Q[c,m, 3.3em, font=\fontfamily{qcr}] Q[c,m, 3.3em, font=\fontfamily{qcr}] Q[c,m, 3.3em, font=\fontfamily{qcr}] Q[c,m, 3.3em, font=\fontfamily{qcr}]},
                 colsep  = 0pt,
                 row{1}  = {0.6cm, font=\bfseries, bg=gray!30},
                 row{2-Z} = {0.3cm},
                 }
    
    \SetCell[c=2]{c}            && Model             & Bleu          & Meteor        & Rouge-1  & Rouge-L  & Bert-\newline Score   & Correct\newline ness      & Faith-\newline fulness    & Fluency   \\
    \SetCell[r=3, c=2]{c}\rotatebox[origin=c]{90}{Trained}
                        && Llama2            & 29.36       & 40.2        & 48.36       &  39.25   &  90.05     & 61.38                   & 63.78                   & 61.47   \\
    %
                        && Struct\newline LM          & 31.06       & 42.3        & 49.42       &  40.58   &  90.9     & 69.70                   & 72.46                   & 69.93   \\
    %
                        && Mistral\newline Instruct & \textcolor{red}{\textbf{38.89}}       & \textcolor{red}{\textbf{49.43}}       & \textcolor{red}{\textbf{56.64}}   &  \textcolor{red}{\textbf{48.32}}   &  \textcolor{red}{\textbf{92.18}}  & 73.07                   & 76.63                   & 73.03   \\
    \SetCell[r=5, c=2]{c} \rotatebox[origin=c]{90}{Base}
                        && Llama2            & 1.39        & 3.59        & 5.57     & 4.09  &  66.49 & 32.18                   & 37.68                   & 32.47   \\
    %
                        && Struct\newline LM          & 6.21        & 11.96       & 20.09    & 15.34   & 82.56  & 64.30                   & 70.08                   & 63.10   \\
    %
                        && Mistral\newline Instruct & 4.19        & 9.55        & 25.64    & 18.99   & 82.12  & \textcolor{blue}{\textbf{77.02}}                   & \textcolor{blue}{\textbf{81.16}}                   & \textcolor{blue}{\textbf{76.5}}    \\
                        && GPT-4o-mini & 7.14 & 16.12 & 29.44 & 19.47 & 83.75 &  \textcolor{red}{\textbf{80.89}} &  \textcolor{red}{\textbf{83.92}} &  \textcolor{red}{\textbf{80.81}}\\
                        && Gemini-1.5-flash & 8.8 & 15.18 & 30.38 & 21.51 & 84.05 & 78.79 & 83.04 & 78.54\\
    \end{tblr}
    \caption{Results of Trained vs. Base Models: LLAMA2, StructLM, and Mistral\_Instruct}
    \label{table:results1}
\end{table}

\end{comment}