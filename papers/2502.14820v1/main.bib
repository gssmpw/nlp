@misc{Bergmann_2024,
  title   = {Build an LLM-Powered Data Agent for Data Analysis},
  url     = {https://www.ibm.com/topics/fine-tuning},
  journal = {IBM},
  author  = {Bergmann, Dave},
  year    = {2024},
  month   = {March}
}

@misc{gao2024jsontuning,
  title         = {JsonTuning: Towards Generalizable, Robust, and Controllable Instruction Tuning},
  author        = {Chang Gao and Wenxuan Zhang and Guizhen Chen and Wai Lam},
  year          = {2024},
  eprint        = {2310.02953},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}


@misc{he2023survey,
  title         = {A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics},
  author        = {Kai He and Rui Mao and Qika Lin and Yucheng Ruan and Xiang Lan and Mengling Feng and Erik Cambria},
  year          = {2023},
  eprint        = {2310.05694},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
} 

@misc{jiang2023mistral,
      title={Mistral 7B}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2023},
      eprint={2310.06825},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.06825}, 
}

@misc{liu2023reviewergpt,
  title         = {ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing},
  author        = {Ryan Liu and Nihar B. Shah},
  year          = {2023},
  eprint        = {2306.00622},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{naveed2024comprehensive,
  title         = {A Comprehensive Overview of Large Language Models},
  author        = {Humza Naveed and Asad Ullah Khan and Shi Qiu and Muhammad Saqib and Saeed Anwar and Muhammad Usman and Naveed Akhtar and Nick Barnes and Ajmal Mian},
  year          = {2024},
  eprint        = {2307.06435},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@article{REDDY2023101304,
  title    = {Evaluating large language models for use in healthcare: A framework for translational value assessment},
  journal  = {Informatics in Medicine Unlocked},
  volume   = {41},
  pages    = {101304},
  year     = {2023},
  issn     = {2352-9148},
  doi      = {https://doi.org/10.1016/j.imu.2023.101304},
  url      = {https://www.sciencedirect.com/science/article/pii/S2352914823001508},
  author   = {Sandeep Reddy},
  abstract = {The recent focus on Large Language Models (LLMs) has yielded unprecedented discussion of their potential use in various domains, including healthcare. While showing considerable potential in performing human-capable tasks, LLMs have also demonstrated significant drawbacks, including generating misinformation, falsifying data, and contributing to plagiarism. These aspects are generally concerning but can be more severe in the context of healthcare. As LLMs are explored for utility in healthcare, including generating discharge summaries, interpreting medical records and providing medical advice, it is necessary to ensure safeguards around their use in healthcare. Notably, there must be an evaluation process that assesses LLMs for their natural language processing performance and their translational value. Complementing this assessment, a governance layer can ensure accountability and public confidence in such models. Such an evaluation framework is discussed and presented in this paper.}
}

@inproceedings{singha2023tabular,
title={Tabular Representation, Noisy Operators, and Impacts on Table Structure Understanding Tasks in {LLM}s},
author={Ananya Singha and Jos{\'e} Cambronero and Sumit Gulwani and Vu Le and Chris Parnin},
booktitle={NeurIPS 2023 Second Table Representation Learning Workshop},
year={2023},
url={https://openreview.net/forum?id=Ld5UCpiT07}
}


@misc{touvron2023llama,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.09288}, 
}


@misc{Varshney_2024,
  title   = {Build an LLM-Powered Data Agent for Data Analysis},
  url     = {https://developer.nvidia.com/blog/build-an-llm-powered-data-agent-for-data-analysis/},
  journal = {Nvidia Developer},
  author  = {Varshney, Tanay},
  year    = {2024},
  month   = {Feb}
}

@misc{wu2023survey,
  title         = {A Survey on Large Language Models for Recommendation},
  author        = {Likang Wu and Zhi Zheng and Zhaopeng Qiu and Hao Wang and Hongchao Gu and Tingjia Shen and Chuan Qin and Chen Zhu and Hengshu Zhu and Qi Liu and Hui Xiong and Enhong Chen},
  year          = {2023},
  eprint        = {2305.19860},
  archiveprefix = {arXiv},
  primaryclass  = {cs.IR}
}

@misc{zhao2023survey,
  title         = {A Survey of Large Language Models},
  author        = {Wayne Xin Zhao and Kun Zhou and Junyi Li and Tianyi Tang and Xiaolei Wang and Yupeng Hou and Yingqian Min and Beichen Zhang and Junjie Zhang and Zican Dong and Yifan Du and Chen Yang and Yushuo Chen and Zhipeng Chen and Jinhao Jiang and Ruiyang Ren and Yifan Li and Xinyu Tang and Zikang Liu and Peiyu Liu and Jian-Yun Nie and Ji-Rong Wen},
  year          = {2023},
  eprint        = {2303.18223},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@inproceedings{zhuang2024structlm,
title={Struct{LM}: Towards Building Generalist Models for Structured Knowledge Grounding},
author={Alex Zhuang and Ge Zhang and Tianyu Zheng and Xinrun Du and Junjie Wang and Weiming Ren and Wenhao Huang and Jie Fu and Xiang Yue and Wenhu Chen},
booktitle={First Conference on Language Modeling},
year={2024},
url={https://openreview.net/forum?id=EKBPn7no4y}
}


@article{fan2023fatellm,
  title={Fate-llm: A industrial grade federated learning framework for large language models},
  author={Fan, Tao and Kang, Yan and Ma, Guoqiang and Chen, Weijing and Wei, Wenbin and Fan, Lixin and Yang, Qiang},
  journal={Symposium on Advances and Open Problems in Large Language Models (LLM@IJCAI'23)},
  year={2023}
}

@INPROCEEDINGS{10305960,
  author={Debbah, Mérouane},
  booktitle={2023 Eighth International Conference on Fog and Mobile Edge Computing (FMEC)}, 
  title={Large Language Models for Telecom}, 
  year={2023},
  volume={},
  number={},
  pages={3-4},
  keywords={Fault diagnosis;Sentiment analysis;Multi-access edge computing;Wireless networks;Computational modeling;Network security;Telecommunications},
  doi={10.1109/FMEC59375.2023.10305960}}

@inproceedings{xu-etal-2021-raise,
    title = "Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning",
    author = "Xu, Runxin  and
      Luo, Fuli  and
      Zhang, Zhiyuan  and
      Tan, Chuanqi  and
      Chang, Baobao  and
      Huang, Songfang  and
      Huang, Fei",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.749",
    doi = "10.18653/v1/2021.emnlp-main.749",
    pages = "9514--9528",
    abstract = "Recent pretrained language models extend from millions to billions of parameters. Thus the need to fine-tune an extremely large pretrained model with a limited training corpus arises in various downstream tasks. In this paper, we propose a straightforward yet effective fine-tuning technique, Child-Tuning, which updates a subset of parameters (called child network) of large pretrained models via strategically masking out the gradients of the non-child network during the backward process. Experiments on various downstream tasks in GLUE benchmark show that Child-Tuning consistently outperforms the vanilla fine-tuning by 1.5 8.6 average score among four different pretrained models, and surpasses the prior fine-tuning techniques by 0.6 1.3 points. Furthermore, empirical results on domain transfer and task transfer show that Child-Tuning can obtain better generalization performance by large margins.",
}

@misc{sun2023comparative,
      title={A Comparative Study between Full-Parameter and LoRA-based Fine-Tuning on Chinese Instruction Data for Instruction Following Large Language Model}, 
      author={Xianghui Sun and Yunjie Ji and Baochang Ma and Xiangang Li},
      year={2023},
      eprint={2304.08109},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{yu2022differentially,
title={Differentially Private Fine-tuning of Language Models},
author={Da Yu and Saurabh Naik and Arturs Backurs and Sivakanth Gopi and Huseyin A Inan and Gautam Kamath and Janardhan Kulkarni and Yin Tat Lee and Andre Manoel and Lukas Wutschitz and Sergey Yekhanin and Huishuai Zhang},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=Q42f0dfjECO}
}


@inproceedings{zheng2024llamafactory,
    title = "{L}lama{F}actory: Unified Efficient Fine-Tuning of 100+ Language Models",
    author = "Zheng, Yaowei  and
      Zhang, Richong  and
      Zhang, Junhao  and
      Ye, Yanhan  and
      Luo, Zheyan",
    editor = "Cao, Yixin  and
      Feng, Yang  and
      Xiong, Deyi",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-demos.38",
    doi = "10.18653/v1/2024.acl-demos.38",
    pages = "400--410",
    abstract = "Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods. It provides a solution for flexibly customizing the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LlamaBoard. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks. It has been released at https://github.com/hiyouga/LLaMA-Factory and received over 25,000 stars and 3,000 forks.",
}

@misc{zhu2024lift,
title={{LIFT}: Efficient Layer-wise Fine-tuning for Large Model Models},
author={Ligeng Zhu and Lanxiang Hu and Ji Lin and Song Han},
year={2024},
url={https://openreview.net/forum?id=u0INlprg3U}
}

@inproceedings{macková2023promap,
author = {Mackov\'{a}, Kate\v{r}ina and Pil\'{a}t, Martin},
title = {ProMap: Product Mapping Datasets},
year = {2024},
isbn = {978-3-031-56059-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-56060-6_11},
doi = {10.1007/978-3-031-56060-6_11},
abstract = {The goal of product mapping is to decide, whether two listings from two different e-shops describe the same products. Existing datasets of matching and non-matching pairs of products, however, often suffer from incomplete product information or contain only very distant non-matching products. In this paper, we introduce two new datasets for product mapping: ProMapCz consisting of 1,495 Czech product pairs and ProMapEn consisting of 1,555 English product pairs of matching and non-matching products manually scraped from two pairs of e-shops. The datasets contain both images and textual descriptions of the products, including their specifications, making them one of the most complete datasets for product mapping. Additionally, we divide the non-matching products into two different categories – close non-matches and medium non-matches, based on how similar the products are to each other. Even the medium non-matches are, however, pairs of products that are much more similar than non-matches in other datasets – for example, they still need to have the same brand and similar name and price. Finally, we train a number of product matching models on these datasets to demonstrate the advantages of having these two types of non-matches for the analysis of these models.},
booktitle = {Advances in Information Retrieval: 46th European Conference on Information Retrieval, ECIR 2024, Glasgow, UK, March 24–28, 2024, Proceedings, Part II},
pages = {159–172},
numpages = {14},
keywords = {Product Mapping, Product Matching, Similarity Computation, Machine Learning},
location = {Glasgow, United Kingdom}
}

@InProceedings{10.1007/978-3-319-20895-4_34,
author="Tan, Wee-Kek
and Teo, Hock-Hai",
editor="Fui-Hoon Nah, Fiona
and Tan, Chuan-Hoo",
title="Productpedia -- A Collaborative Electronic Product Catalog for Ecommerce 3.0",
booktitle="HCI in Business",
year="2015",
publisher="Springer International Publishing",
address="Cham",
pages="370--381",
abstract="Despite the advancements made in ecommerce technologies over the past years, the inability to define and exchange semantically rich and accurate product information among ecommerce websites/applications has continued to intrigue researchers. This problem has taken on greater urgency because it impedes the realization of the full benefits of Ecommerce 3.0. The present research conceptualizes, designs and implements a cloud computing-based platform that enables global merchants to maintain a collaborative Electronic Product Catalog (EPC) known as Productpedia. This collaborative EPC platform addresses numerous shortcomings of prior researches by (1) maintaining a single centralized EPC database; (2) negating the need to synchronize and convert data; (3) creating an integrated meta-model ontology for merchants to define previously unclassified product information without the involvement of domain experts; and (4) enabling an Open Application Programming Interface based on RESTful web services to facilitate direct modification of the EPC database by even third-party applications.",
isbn="978-3-319-20895-4"
}

@inproceedings{10.1145/3583780.3615503,
author = {Ryali, Gayatri and S, Shreyas and Kaveri, Sivaramakrishnan and Comar, Prakash Mandayam},
title = {TrendSpotter: Forecasting E-commerce Product Trends},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615503},
doi = {10.1145/3583780.3615503},
abstract = {Internet users actively search for trending products on various social media services like Instagram and YouTube which serve as popular hubs for discovering and exploring fashionable and popular items. It is imperative for e-commerce giants to have the capability to accurately identify, predict and subsequently showcase these trending products to the customers. E-commerce stores can effectively cater to the evolving demands of the customer base and enhance the overall shopping experience by offering recent and most sought-after products in a timely manner. In this work we propose a framework for predicting and surfacing trending products in e-commerce stores, the first of its kind to the best of our knowledge. We begin by defining what constitutes a trending product using sound statistical tests. We then introduce a machine learning-based early trend prediction system called TrendSpotter to help users identify upcoming product trends. TrendSpotter is a unique adaptation of the state-of-the-art InceptionTime modelciteInceptionTime that predicts the future popularity of a product based on its current customer engagement, such as clicks, purchases, and other relevant product attributes. The effectiveness of our approach is demonstrated through A/B tests, where we first showcase the effectiveness of our statistical test based labeling strategy, resulting in an incremental sales lift of 59 bpsfootnotebps or basis points are a measure of percentages. 1 bps = 0.01\% across two experiments on home page and search page. Subsequently, we conduct a comparison between our machine learning model and the statistical labeling baseline and observe an additional sales gain of 14 bps, reflecting the importance of early identification of trending products.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {4808–4814},
numpages = {7},
keywords = {trends, time series, e-commerce, convolutional neural networks},
location = {<conf-loc>, <city>Birmingham</city>, <country>United Kingdom</country>, </conf-loc>},
series = {CIKM '23}
}

@article{Liang_2020,
doi = {10.1088/1742-6596/1601/3/032012},
url = {https://dx.doi.org/10.1088/1742-6596/1601/3/032012},
year = {2020},
month = {jul},
publisher = {IOP Publishing},
volume = {1601},
number = {3},
pages = {032012},
author = {Jia-Hui Liang},
title = {Application of Big Data Technology in Product Selection on Cross-border E-commerce Platforms},
journal = {Journal of Physics: Conference Series},
abstract = {With Amazon as the study case, the application of big data in product selection on this e-commerce platform is studied. Two big data analysis tools commonly used in commerce, the MPP distributed database and the Hadoop distributed database, were analyzed. Based on big data technology, the search function of the platform, the analytical tools, and third-party data analytical tools, this study compared different levels of comments of customers for the same type of products and analyzed the product selection mechanism.}
}

@article{Muntjir2016,
title = {An Enhanced Framework with Advanced Study to Incorporate the Searching of E-Commerce Products Using Modernization of Database Queries},
journal = {International Journal of Advanced Computer Science and Applications},
doi = {10.14569/IJACSA.2016.070514},
url = {http://dx.doi.org/10.14569/IJACSA.2016.070514},
year = {2016},
publisher = {The Science and Information Organization},
volume = {7},
number = {5},
author = {Mohd Muntjir and Ahmad Tasnim Siddiqui}
}

@inproceedings{chen-etal-2022-bert2bert,
    title = "bert2{BERT}: Towards Reusable Pretrained Language Models",
    author = "Chen, Cheng  and
      Yin, Yichun  and
      Shang, Lifeng  and
      Jiang, Xin  and
      Qin, Yujia  and
      Wang, Fengyu  and
      Wang, Zhi  and
      Chen, Xiao  and
      Liu, Zhiyuan  and
      Liu, Qun",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.151",
    doi = "10.18653/v1/2022.acl-long.151",
    pages = "2134--2148",
    abstract = "In recent years, researchers tend to pre-train ever-larger language models to explore the upper limit of deep models. However, large language model pre-training costs intensive computational resources, and most of the models are trained from scratch without reusing the existing pre-trained models, which is wasteful. In this paper, we propose bert2BERT, which can effectively transfer the knowledge of an existing smaller pre-trained model to a large model through parameter initialization and significantly improve the pre-training efficiency of the large model. Specifically, we extend the previous function-preserving method proposed in computer vision on the Transformer-based language model, and further improve it by proposing a novel method, advanced knowledge for large model{'}s initialization. In addition, a two-stage learning method is proposed to further accelerate the pre-training. We conduct extensive experiments on representative PLMs (e.g., BERT and GPT) and demonstrate that (1) our method can save a significant amount of training cost compared with baselines including learning from scratch, StackBERT and MSLT; (2) our method is generic and applicable to different types of pre-trained models. In particular, bert2BERT saves about 45{\%} and 47{\%} computational cost of pre-training BERT$_{\rm BASE}$ and GPT$_{\rm BASE}$ by reusing the models of almost their half sizes.",
}



@inproceedings{agrawal2022large,
    title = "Large language models are few-shot clinical information extractors",
    author = "Agrawal, Monica  and
      Hegselmann, Stefan  and
      Lang, Hunter  and
      Kim, Yoon  and
      Sontag, David",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.130",
    doi = "10.18653/v1/2022.emnlp-main.130",
    pages = "1998--2022",
    abstract = "A long-running goal of the clinical NLP community is the extraction of important variables trapped in clinical notes. However, roadblocks have included dataset shift from the general domain and a lack of public clinical corpora and annotations. In this work, we show that large language models, such as InstructGPT (Ouyang et al., 2022), perform well at zero- and few-shot information extraction from clinical text despite not being trained specifically for the clinical domain. Whereas text classification and generation performance have already been studied extensively in such models, here we additionally demonstrate how to leverage them to tackle a diverse set of NLP tasks which require more structured outputs, including span identification, token-level sequence classification, and relation extraction. Further, due to the dearth of available data to evaluate these systems, we introduce new datasets for benchmarking few-shot clinical information extraction based on a manual re-annotation of the CASI dataset (Moon et al., 2014) for new tasks. On the clinical extraction tasks we studied, the GPT-3 systems significantly outperform existing zero- and few-shot baselines.",
}

@inproceedings{edunov-etal-2019-pre,
    title = "Pre-trained language model representations for language generation",
    author = "Edunov, Sergey  and
      Baevski, Alexei  and
      Auli, Michael",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1409",
    doi = "10.18653/v1/N19-1409",
    pages = "4052--4059",
    abstract = "Pre-trained language model representations have been successful in a wide range of language understanding tasks. In this paper, we examine different strategies to integrate pre-trained representations into sequence to sequence models and apply it to neural machine translation and abstractive summarization. We find that pre-trained representations are most effective when added to the encoder network which slows inference by only 14{\%}. Our experiments in machine translation show gains of up to 5.3 BLEU in a simulated resource-poor setup. While returns diminish with more labeled data, we still observe improvements when millions of sentence-pairs are available. Finally, on abstractive summarization we achieve a new state of the art on the full text version of CNN/DailyMail.",
}

@misc{brinkmann2024product,
      title={Product Attribute Value Extraction using Large Language Models}, 
      author={Alexander Brinkmann and Roee Shraga and Christian Bizer},
      year={2024},
      eprint={2310.12537},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@inproceedings{labrak2024biomistral,
    title = "{B}io{M}istral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains",
    author = "Labrak, Yanis  and
      Bazoge, Adrien  and
      Morin, Emmanuel  and
      Gourraud, Pierre-Antoine  and
      Rouvier, Mickael  and
      Dufour, Richard",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.348",
    doi = "10.18653/v1/2024.findings-acl.348",
    pages = "5848--5864",
    abstract = "Large Language Models (LLMs) have demonstrated remarkable versatility in recent years, offering potential applications across specialized domains such as healthcare and medicine. Despite the availability of various open-source LLMs tailored for health contexts, adapting general-purpose LLMs to the medical domain presents significant challenges.In this paper, we introduce BioMistral, an open-source LLM tailored for the biomedical domain, utilizing Mistral as its foundation model and further pre-trained on PubMed Central. We conduct a comprehensive evaluation of BioMistral on a benchmark comprising 10 established medical question-answering (QA) tasks in English. We also explore lightweight models obtained through quantization and model merging approaches. Our results demonstrate BioMistral{'}s superior performance compared to existing open-source medical models and its competitive edge against proprietary counterparts. Finally, to address the limited availability of data beyond English and to assess the multilingual generalization of medical LLMs, we automatically translated and evaluated this benchmark into 7 other languages. This marks the first large-scale multilingual evaluation of LLMs in the medical domain. Datasets, multilingual evaluation benchmarks, scripts, and all the models obtained during our experiments are freely released.",
}

@article{skondras2023generating,
  title={Generating Synthetic Resume Data with Large Language Models for Enhanced Job Description Classification},
  author={Skondras, Panagiotis and Zervas, Panagiotis and Tzimas, Giannis},
  journal={Future Internet},
  volume={15},
  number={11},
  pages={363},
  year={2023},
  publisher={MDPI}
}


@inproceedings{tang2024strucbench,
    title = "Struc-Bench: Are Large Language Models Good at Generating Complex Structured Tabular Data?",
    author = "Tang, Xiangru  and
      Zong, Yiming  and
      Phang, Jason  and
      Zhao, Yilun  and
      Zhou, Wangchunshu  and
      Cohan, Arman  and
      Gerstein, Mark",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-short.2",
    doi = "10.18653/v1/2024.naacl-short.2",
    pages = "12--34",
    abstract = "Despite the remarkable capabilities of Large Language Models (LLMs) like GPT-4, producing complex, structured tabular data remains challenging. Our study assesses LLMs{'} proficiency in structuring tables and introduces a novel fine-tuning method, cognizant of data structures, to bolster their performance. We unveil Struc-Bench, a comprehensive benchmark featuring prominent LLMs (GPT-NeoX-20B, GPT-3.5, GPT-4, and Vicuna), which spans text tables, HTML, and LaTeX formats. Our proposed FormatCoT aids in crafting format-specific instructions from the intended outputs to populate this benchmark. Addressing the gap in task-centered evaluation, we propose two innovative metrics, P-Score (Prompting Score) and H-Score (Heuristical Score), to more accurately gauge LLM performance. Our experiments show that applying our structure-aware fine-tuning to LLaMA-7B leads to substantial performance gains, outshining its LLM counterparts across most measures. In-depth error analysis and creating an ability map across six dimensions, coverage, formatting, reasoning, comprehension, pragmatics, and hallucination, highlight areas for future enhancements and suggest forthcoming research trajectories. Our code and models can be found at https://github.com/gersteinlab/Struc-Bench.",
}

@misc{xu2024emerging,
      title={Emerging Synergies Between Large Language Models and Machine Learning in Ecommerce Recommendations}, 
      author={Xiaonan Xu and Yichao Wu and Penghao Liang and Yuhang He and Han Wang},
      year={2024},
      eprint={2403.02760},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{zhang2021ebert,
      title={E-BERT: A Phrase and Product Knowledge Enhanced Language Model for E-commerce}, 
      author={Denghui Zhang and Zixuan Yuan and Yanchi Liu and Fuzhen Zhuang and Haifeng Chen and Hui Xiong},
      year={2021},
      eprint={2009.02835},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@inproceedings{zhou2023leveraging,
    title = "Leveraging Large Language Models for Enhanced Product Descriptions in e{C}ommerce",
    author = "Zhou, Jianghong  and
      Liu, Bo  and
      Acharya, Jhalak  and
      Hong, Yao  and
      Lee, Kuang-Chih  and
      Wen, Musen",
    editor = "Gehrmann, Sebastian  and
      Wang, Alex  and
      Sedoc, Jo{\~a}o  and
      Clark, Elizabeth  and
      Dhole, Kaustubh  and
      Chandu, Khyathi Raghavi  and
      Santus, Enrico  and
      Sedghamiz, Hooman",
    booktitle = "Proceedings of the Third Workshop on Natural Language Generation, Evaluation, and Metrics (GEM)",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.gem-1.8",
    pages = "88--96",
    abstract = "In the dynamic field of eCommerce, the quality and comprehensiveness of product descriptions are pivotal for enhancing search visibility and customer engagement. Effective product descriptions can address the {`}cold start{'} problem, align with market trends, and ultimately lead to increased click-through rates. Traditional methods for crafting these descriptions often involve significant human effort and may lack both consistency and scalability. This paper introduces a novel methodology for automating product description generation using the LLAMA 2.0 7B language model. We train the model on a dataset of authentic product descriptions from Walmart, one of the largest eCommerce platforms. The model is then fine-tuned for domain-specific language features and eCommerce nuances to enhance its utility in sales and user engagement. We employ multiple evaluation metrics{---}including NDCG, customer click-through rates, and human assessments{---}to validate the effectiveness of our approach. Our findings reveal that the system is not only scalable but also significantly reduces the human workload involved in creating product descriptions. This study underscores the considerable potential of large language models like LLAMA 2.0 7B in automating and optimizing various facets of eCommerce platforms, offering significant business impact, including improved search functionality and increased sales.",
}

@misc{yuan2023scaling,
      title={Scaling Relationship on Learning Mathematical Reasoning with Large Language Models}, 
      author={Zheng Yuan and Hongyi Yuan and Chengpeng Li and Guanting Dong and Keming Lu and Chuanqi Tan and Chang Zhou and Jingren Zhou},
      year={2023},
      eprint={2308.01825},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zhang2022opt,
      title={OPT: Open Pre-trained Transformer Language Models}, 
      author={Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},
      year={2022},
      eprint={2205.01068},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{creswell2022selectioninference,
title={Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning},
author={Antonia Creswell and Murray Shanahan and Irina Higgins},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=3Pf3Wg6o-A4}
}

@inproceedings{zhou-etal-2023-inform,
    title = "{INFORM} : Information e{N}tropy based multi-step reasoning {FOR} large language Models",
    author = "Zhou, Chuyue  and
      You, Wangjie  and
      Li, Juntao  and
      Ye, Jing  and
      Chen, Kehai  and
      Zhang, Min",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.216",
    doi = "10.18653/v1/2023.emnlp-main.216",
    pages = "3565--3576",
    abstract = "Large language models (LLMs) have demonstrated exceptional performance in reasoning tasks with dedicated Chain-of-Thought (CoT) prompts. Further enhancing CoT prompts with exquisite exemplars can significantly improve reasoning performance.However, the effectiveness of CoT prompts may fluctuate dramatically with different choices of in-context examples. Additionally, manual construction of rationale steps can be time-consuming, presenting challenges for the widespread adoption of CoT prompting. In this work, we propose a novel approach by introducing information entropy (IE) as a criteria on for CoT prompt selection. We extend this criterion to the CoT generation and inference stages, automatically generating CoT prompts with higher information entropy scores and adaptively determining the number of samples. These three stages together form our proposed information- entropy-based multi-step reasoning for large language models, named INFORM. Our experiments across seven reasoning benchmarks utilizing two language models(GPT-3.5-Turbo and text-davinci-003) demonstrate the superiority of INFORM both in performance and efficiency.",
}

@misc{minaee2024large,
      title={Large Language Models: A Survey}, 
      author={Shervin Minaee and Tomas Mikolov and Narjes Nikzad and Meysam Chenaghlu and Richard Socher and Xavier Amatriain and Jianfeng Gao},
      year={2024},
      eprint={2402.06196},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wang2024largescale,
      title={Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey}, 
      author={Xiao Wang and Guangyao Chen and Guangwu Qian and Pengcheng Gao and Xiao-Yong Wei and Yaowei Wang and Yonghong Tian and Wen Gao},
      year={2024},
      eprint={2302.10035},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{Lalor2017Improving,
title={Improving Machine Learning Ability with Fine-Tuning},
author={John P. Lalor and Hao Wu and Hong Yu},
journal={ArXiv},
year={2017},
volume={abs/1702.08563},
doi={}
}

@article{Catani2020A,
title={A mathematical framework for operational fine tunings},
author={L. Catani and M. Leifer},
journal={Quantum},
year={2020},
volume={7},
pages={948},
doi={10.22331/q-2023-03-16-948}
}


@misc{Shachaf2021A,
      title={A Theoretical Analysis of Fine-tuning with Linear Teachers}, 
      author={Gal Shachaf and Alon Brutzkus and Amir Globerson},
      year={2021},
      eprint={2107.01641},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2107.01641}, 
}

@article{Vrbancic2020Transfer,
title={Transfer Learning With Adaptive Fine-Tuning},
author={Grega Vrbancic and V. Podgorelec},
journal={IEEE Access},
year={2020},
volume={8},
pages={196197-196211},
doi={10.1109/ACCESS.2020.3034343}
}

@article{Xiao2023Offsite-Tuning:,
title={Offsite-Tuning: Transfer Learning without Full Model},
author={Guangxuan Xiao and Ji Lin and Song Han},
journal={ArXiv},
year={2023},
volume={abs/2302.04870},
doi={10.48550/arXiv.2302.04870}
}


@article{Reiter2018A,
    title = "A Structured Review of the Validity of {BLEU}",
    author = "Reiter, Ehud",
    journal = "Computational Linguistics",
    volume = "44",
    number = "3",
    month = sep,
    year = "2018",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/J18-3002",
    doi = "10.1162/coli_a_00322",
    pages = "393--401",
    abstract = "The BLEU metric has been widely used in NLP for over 15 years to evaluate NLP systems, especially in machine translation and natural language generation. I present a structured review of the evidence on whether BLEU is a valid evaluation technique{---}in other words, whether BLEU scores correlate with real-world utility and user-satisfaction of NLP systems; this review covers 284 correlations reported in 34 papers. Overall, the evidence supports using BLEU for diagnostic evaluation of MT systems (which is what it was originally proposed for), but does not support using BLEU outside of MT, for evaluation of individual texts, or for scientific hypothesis testing.",
}


@inproceedings{Agarwal2008Meteor,
author = {Agarwal, Abhaya and Lavie, Alon},
title = {METEOR, M-BLEU and M-TER: evaluation metrics for high-correlation with human rankings of machine translation output},
year = {2008},
isbn = {9781932432091},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {This paper describes our submissions to the machine translation evaluation shared task in ACL WMT-08. Our primary submission is the Meteor metric tuned for optimizing correlation with human rankings of translation hypotheses. We show significant improvement in correlation as compared to the earlier version of metric which was tuned to optimized correlation with traditional adequacy and fluency judgments. We also describe m-bleu and m-ter, enhanced versions of two other widely used metrics bleu and ter respectively, which extend the exact word matching used in these metrics with the flexible matching based on stemming and Wordnet in Meteor.},
booktitle = {Proceedings of the Third Workshop on Statistical Machine Translation},
pages = {115–118},
numpages = {4},
location = {Columbus, Ohio},
series = {StatMT '08}
}


@misc{Ganesan2015ROUGE,
      title={ROUGE 2.0: Updated and Improved Measures for Evaluation of Summarization Tasks}, 
      author={Kavita Ganesan},
      year={2018},
      eprint={1803.01937},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/1803.01937}, 
}


@inproceedings{Ng2015Better,
    title = "Better Summarization Evaluation with Word Embeddings for {ROUGE}",
    author = "Ng, Jun-Ping  and
      Abrecht, Viktoria",
    editor = "M{\`a}rquez, Llu{\'\i}s  and
      Callison-Burch, Chris  and
      Su, Jian",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D15-1222",
    doi = "10.18653/v1/D15-1222",
    pages = "1925--1930",
}


@article{Ji2022Survey,
author = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
title = {Survey of Hallucination in Natural Language Generation},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {12},
issn = {0360-0300},
url = {https://doi.org/10.1145/3571730},
doi = {10.1145/3571730},
abstract = {Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation, and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before.In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions, and (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, and machine translation. This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {248},
numpages = {38},
keywords = {Hallucination, intrinsic hallucination, extrinsic hallucination, faithfulness in NLG, factuality in NLG, consistency in NLG}
}


@article{Huang2023A,
author = {Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and Liu, Ting},
title = {A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1046-8188},
url = {https://doi.org/10.1145/3703155},
doi = {10.1145/3703155},
abstract = {The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), fueling a paradigm shift in information acquisition. Nevertheless, LLMs are prone to hallucination, generating plausible yet nonfactual content. This phenomenon raises significant concerns over the reliability of LLMs in real-world information retrieval (IR) systems and has attracted intensive research to detect and mitigate such hallucinations. Given the open-ended general-purpose attributes inherent to LLMs, LLM hallucinations present distinct challenges that diverge from prior task-specific models. This divergence highlights the urgency for a nuanced understanding and comprehensive overview of recent advances in LLM hallucinations. In this survey, we begin with an innovative taxonomy of hallucination in the era of LLM and then delve into the factors contributing to hallucinations. Subsequently, we present a thorough overview of hallucination detection methods and benchmarks. Our discussion then transfers to representative methodologies for mitigating LLM hallucinations. Additionally, we delve into the current limitations faced by retrieval-augmented LLMs in combating hallucinations, offering insights for developing more robust IR systems. Finally, we highlight the promising research directions on LLM hallucinations, including hallucination in large vision-language models and understanding of knowledge boundaries in LLM hallucinations.},
note = {Just Accepted},
journal = {ACM Trans. Inf. Syst.},
month = nov,
keywords = {Large Language Models, Hallucination, Factuality, Faithfulness}
}


@inproceedings{Xiao2021On,
    title = "On Hallucination and Predictive Uncertainty in Conditional Language Generation",
    author = "Xiao, Yijun  and
      Wang, William Yang",
    editor = "Merlo, Paola  and
      Tiedemann, Jorg  and
      Tsarfaty, Reut",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.236",
    doi = "10.18653/v1/2021.eacl-main.236",
    pages = "2734--2744",
    abstract = "Despite improvements in performances on different natural language generation tasks, deep neural models are prone to hallucinating facts that are incorrect or nonexistent. Different hypotheses are proposed and examined separately for different tasks, but no systematic explanations are available across these tasks. In this study, we draw connections between hallucinations and predictive uncertainty in conditional language generation. We investigate their relationship in both image captioning and data-to-text generation and propose a simple extension to beam search to reduce hallucination. Our analysis shows that higher predictive uncertainty corresponds to a higher chance of hallucination. Epistemic uncertainty is more indicative of hallucination than aleatoric or total uncertainties. It helps to achieve better results of trading performance in standard metric for less hallucination with the proposed beam search variant.",
}


@inproceedings{venkit2024confidently,
    title = "An Audit on the Perspectives and Challenges of Hallucinations in {NLP}",
    author = "Narayanan Venkit, Pranav  and
      Chakravorti, Tatiana  and
      Gupta, Vipul  and
      Biggs, Heidi  and
      Srinath, Mukund  and
      Goswami, Koustava  and
      Rajtmajer, Sarah  and
      Wilson, Shomir",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.375",
    doi = "10.18653/v1/2024.emnlp-main.375",
    pages = "6528--6548",
    abstract = "We audit how hallucination in large language models (LLMs) is characterized in peer-reviewed literature, using a critical examination of 103 publications across NLP research. Through the examination of the literature, we identify a lack of agreement with the term {`}hallucination{'} in the field of NLP. Additionally, to compliment our audit, we conduct a survey with 171 practitioners from the field of NLP and AI to capture varying perspectives on hallucination. Our analysis calls for the necessity of explicit definitions and frameworks outlining hallucination within NLP, highlighting potential challenges, and our survey inputs provide a thematic understanding of the influence and ramifications of hallucination in society.",
}


@inproceedings{agrawal2024language,
    title = "Do Language Models Know When They{'}re Hallucinating References?",
    author = "Agrawal, Ayush  and
      Suzgun, Mirac  and
      Mackey, Lester  and
      Kalai, Adam",
    editor = "Graham, Yvette  and
      Purver, Matthew",
    booktitle = "Findings of the Association for Computational Linguistics: EACL 2024",
    month = mar,
    year = "2024",
    address = "St. Julian{'}s, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-eacl.62",
    pages = "912--928",
    abstract = "State-of-the-art language models (LMs) are notoriously susceptible to generating hallucinated information. Such inaccurate outputs not only undermine the reliability of these models but also limit their use and raise serious concerns about misinformation and propaganda. In this work, we focus on hallucinated book and article references and present them as the {``}model organism{''} of language model hallucination research, due to their frequent and easy-to-discern nature. We posit that if a language model cites a particular reference in its output, then it should ideally possess sufficient information about its authors and content, among other relevant details. Using this basic insight, we illustrate that one can identify hallucinated references without ever consulting any external resources, by asking a set of direct or indirect queries to the language model about the references. These queries can be considered as {``}consistency checks.{''} Our findings highlight that while LMs, including GPT-4, often produce inconsistent author lists for hallucinated references, they also often accurately recall the authors of real references. In this sense, the LM can be said to {``}know{''} when it is hallucinating references. Furthermore, these findings show how hallucinated references can be dissected to shed light on their nature.",
}

@misc{mumtaz2024llmshealthcare,
      title={LLMs-Healthcare : Current Applications and Challenges of Large Language Models in various Medical Specialties}, 
      author={Ummara Mumtaz and Awais Ahmed and Summaya Mumtaz},
      year={2024},
      eprint={2311.12882},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@article{10.1093/bioinformatics/btz682,
    author = {Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
    title = "{BioBERT: a pre-trained biomedical language representation model for biomedical text mining}",
    journal = {Bioinformatics},
    volume = {36},
    number = {4},
    pages = {1234-1240},
    year = {2019},
    month = {09},
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/btz682},
    url = {https://doi.org/10.1093/bioinformatics/btz682},
    eprint = {https://academic.oup.com/bioinformatics/article-pdf/36/4/1234/48983216/bioinformatics\_36\_4\_1234.pdf},
}

@misc{zhao2024revolutionizing,
      title={Revolutionizing Finance with LLMs: An Overview of Applications and Insights}, 
      author={Huaqin Zhao and Zhengliang Liu and Zihao Wu and Yiwei Li and Tianze Yang and Peng Shu and Shaochen Xu and Haixing Dai and Lin Zhao and Gengchen Mai and Ninghao Liu and Tianming Liu},
      year={2024},
      eprint={2401.11641},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@misc{rfc8259,
    series =    {Request for Comments},
    number =    8259,
    howpublished =  {RFC 8259},
    publisher = {RFC Editor},
    doi =       {10.17487/RFC8259},
    url =       {https://www.rfc-editor.org/info/rfc8259},
    author =    {Tim Bray},
    title =     {{The JavaScript Object Notation (JSON) Data Interchange Format}},
    pagetotal = 16,
    year =      2017,
    month =     dec,
    abstract =  {JavaScript Object Notation (JSON) is a lightweight, text-based, language-independent data interchange format. It was derived from the ECMAScript Programming Language Standard. JSON defines a small set of formatting rules for the portable representation of structured data. This document removes inconsistencies with other specifications of JSON, repairs specification errors, and offers experience-based interoperability guidance.},
}

@inproceedings{HadiLargeLM,
  title={Large Language Models: A Comprehensive Survey of its Applications, Challenges, Limitations, and Future Prospects},
  author={Muhammad Usman Hadi and al tashi and Rizwan Qureshi and Abbas Shah and Muhammad Irfan and Anas Zafar and Muhammad Bilal Shaikh and Naveed Akhtar and Jia Wu and Seyedali Mirjalili and Qasem Al-Tashi and Amgad Muneer and Mohammed Ali Al-garadi and Gru Cnn and T5 RoBERTa},
  url={https://api.semanticscholar.org/CorpusID:266378240}
}

@misc{ling2024domain,
      title={Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey}, 
      author={Chen Ling and Xujiang Zhao and Jiaying Lu and Chengyuan Deng and Can Zheng and Junxiang Wang and Tanmoy Chowdhury and Yun Li and Hejie Cui and Xuchao Zhang and Tianjiao Zhao and Amit Panalkar and Dhagash Mehta and Stefano Pasquali and Wei Cheng and Haoyu Wang and Yanchi Liu and Zhengzhang Chen and Haifeng Chen and Chris White and Quanquan Gu and Jian Pei and Carl Yang and Liang Zhao},
      year={2024},
      eprint={2305.18703},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}


@inproceedings{vaswani2023attention,
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, \L{}ukasz and Polosukhin, Illia},
title = {Attention is all you need},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {6000–6010},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@misc{liu2019roberta,
      title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, 
      author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
      year={2019},
      eprint={1907.11692},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@article{Wang2023Emotional,
title={Emotional Intelligence of Large Language Models},
author={Xuena Wang and Xueting Li and Zi Yin and Yue Wu and Liu Jia Department of PsychologyTsinghua Laboratory of Brain and Intelligence and Tsinghua University and Departmentof Psychology and Renmin University},
journal={ArXiv},
year={2023},
volume={abs/2307.09042},
doi={10.48550/arXiv.2307.09042}
}

@article{Duong2023Analysis,
title={Analysis of large-language model versus human performance for genetics questions},
author={D. Duong and B. D. Solomon},
journal={medRxiv : the preprint server for health sciences},
year={2023},
doi={10.1101/2023.01.27.23285115}
}

@article{Suri2023Do,
title={Do Large Language Models Show Decision Heuristics Similar to Humans? A Case Study Using GPT-3.5},
author={Gaurav Suri and Lily R. Slater and Ali Ziaee and Morgan Nguyen},
journal={ArXiv},
year={2023},
volume={abs/2305.04400},
doi={10.48550/arXiv.2305.04400}
}

@misc{OnePlusNord35G2023,
  author = {Pricebaba.com},
  title = {OnePlus Nord 3 5G - Specifications and Reviews},
  year = {2023},
  url = {https://pricebaba.com/mobile/oneplus-nord-3-5g},
  note = {Accessed: 2023-07-13}
}


@inproceedings{kim2024prometheus2opensource,
    title = "Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models",
    author = "Kim, Seungone  and
      Suk, Juyoung  and
      Longpre, Shayne  and
      Lin, Bill Yuchen  and
      Shin, Jamin  and
      Welleck, Sean  and
      Neubig, Graham  and
      Lee, Moontae  and
      Lee, Kyungjae  and
      Seo, Minjoon",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.248",
    doi = "10.18653/v1/2024.emnlp-main.248",
    pages = "4334--4353",
    abstract = "Proprietary LMs such as GPT-4 are often employed to assess the quality of responses from various LMs. However, concerns including transparency, controllability, and affordability strongly motivate the development of open-source LMs specialized in evaluations. On the other hand, existing open evaluator LMs exhibit critical shortcomings: 1) they issue scores that significantly diverge from those assigned by humans, and 2) they lack the flexibility to perform both direct assessment and pairwise ranking, the two most prevalent forms of assessment. Additionally, they do not possess the ability to evaluate based on custom evaluation criteria, focusing instead on general attributes like helpfulness and harmlessness. To address these issues, we introduce Prometheus 2, a more powerful evaluator LM than its predecessor that closely mirrors human and GPT-4 judgements. Moreover, it is capable of processing both direct assessment and pair-wise ranking formats grouped with a user-defined evaluation criteria. On four direct assessment benchmarks and four pairwise ranking benchmarks, Prometheus 2 scores the highest correlation and agreement with humans and proprietary LM judges among all tested open evaluator LMs. Our models, code, and data are all publicly available.",
}


@article{lyu2024faithfulmodelexplanationnlp,
    title = "Towards Faithful Model Explanation in {NLP}: A Survey",
    author = "Lyu, Qing  and
      Apidianaki, Marianna  and
      Callison-Burch, Chris",
    journal = "Computational Linguistics",
    volume = "50",
    number = "2",
    month = jun,
    year = "2024",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2024.cl-2.6",
    doi = "10.1162/coli_a_00511",
    pages = "657--723",
    abstract = "End-to-end neural Natural Language Processing (NLP) models are notoriously difficult to understand. This has given rise to numerous efforts towards model explainability in recent years. One desideratum of model explanation is faithfulness, that is, an explanation should accurately represent the reasoning process behind the model{'}s prediction. In this survey, we review over 110 model explanation methods in NLP through the lens of faithfulness. We first discuss the definition and evaluation of faithfulness, as well as its significance for explainability. We then introduce recent advances in faithful explanation, grouping existing approaches into five categories: similarity-based methods, analysis of model-internal structures, backpropagation-based methods, counterfactual intervention, and self-explanatory models. For each category, we synthesize its representative studies, strengths, and weaknesses. Finally, we summarize their common virtues and remaining challenges, and reflect on future work directions towards faithful explainability in NLP.",
}

@inproceedings{madsen-etal-2022-evaluating,
    title = "Evaluating the Faithfulness of Importance Measures in {NLP} by Recursively Masking Allegedly Important Tokens and Retraining",
    author = "Madsen, Andreas  and
      Meade, Nicholas  and
      Adlakha, Vaibhav  and
      Reddy, Siva",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.125",
    doi = "10.18653/v1/2022.findings-emnlp.125",
    pages = "1731--1751",
}

@inproceedings{yin2022sensitivitystabilitymodelinterpretations,
    title = "On the Sensitivity and Stability of Model Interpretations in {NLP}",
    author = "Yin, Fan  and
      Shi, Zhouxing  and
      Hsieh, Cho-Jui  and
      Chang, Kai-Wei",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.188",
    doi = "10.18653/v1/2022.acl-long.188",
    pages = "2631--2647",
    abstract = "Recent years have witnessed the emergence of a variety of post-hoc interpretations that aim to uncover how natural language processing (NLP) models make predictions. Despite the surge of new interpretation methods, it remains an open problem how to define and quantitatively measure the faithfulness of interpretations, i.e., to what extent interpretations reflect the reasoning process by a model. We propose two new criteria, sensitivity and stability, that provide complementary notions of faithfulness to the existed removal-based criteria. Our results show that the conclusion for how faithful interpretations are could vary substantially based on different notions. Motivated by the desiderata of sensitivity and stability, we introduce a new class of interpretation methods that adopt techniques from adversarial robustness. Empirical results show that our proposed methods are effective under the new criteria and overcome limitations of gradient-based methods on removal-based criteria. Besides text classification, we also apply interpretation methods and metrics to dependency parsing. Our results shed light on understanding the diverse set of interpretations.",
}


@inproceedings{parcalabescu2024measuringfaithfulnessselfconsistencynatural,
    title = "On Measuring Faithfulness or Self-consistency of Natural Language Explanations",
    author = "Parcalabescu, Letitia  and
      Frank, Anette",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.329",
    doi = "10.18653/v1/2024.acl-long.329",
    pages = "6048--6089",
    abstract = "Large language models (LLMs) can explain their predictions through post-hoc or Chain-of-Thought (CoT) explanations. But an LLM could make up reasonably sounding explanations that are unfaithful to its underlying reasoning. Recent work has designed tests that aim to judge the faithfulness of post-hoc or CoT explanations. In this work we argue that these faithfulness tests do not measure faithfulness to the models{'} inner workings {--} but rather their self-consistency at output level.Our contributions are three-fold: i) We clarify the status of faithfulness tests in view of model explainability, characterising them as self-consistency tests instead. This assessment we underline by ii) constructing a Comparative Consistency Bank for self-consistency tests that for the first time compares existing tests on a common suite of 11 open LLMs and 5 tasks {--} including iii) our new self-consistency measure CC-SHAP. CC-SHAP is a fine-grained measure (not a test) of LLM self-consistency. It compares how a model{'}s input contributes to the predicted answer and to generating the explanation. Our fine-grained CC-SHAP metric allows us iii) to compare LLM behaviour when making predictions and to analyse the effect of other consistency tests at a deeper level, which takes us one step further towards measuring faithfulness by bringing us closer to the internals of the model than strictly surface output-oriented tests.",
}


@inproceedings{gat2023faithfulexplanationsblackboxnlp,
title={Faithful Explanations of Black-box {NLP} Models Using {LLM}-generated Counterfactuals},
author={Yair Ori Gat and Nitay Calderon and Amir Feder and Alexander Chapanin and Amit Sharma and Roi Reichart},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=UMfcdRIotC}
}


@inproceedings{jacovi-goldberg-2020-towards,
    title = "Towards Faithfully Interpretable {NLP} Systems: How Should We Define and Evaluate Faithfulness?",
    author = "Jacovi, Alon  and
      Goldberg, Yoav",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.386",
    doi = "10.18653/v1/2020.acl-main.386",
    pages = "4198--4205",
}


@inproceedings{yao2023predictinggeneralizationperformancecorrectness,
    title = "Predicting generalization performance with correctness discriminators",
    author = "Yao, Yuekun  and
      Koller, Alexander",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.686",
    doi = "10.18653/v1/2024.findings-emnlp.686",
    pages = "11725--11739",
    abstract = "The ability to predict an NLP model{'}s accuracy on unseen, potentially out-of-distribution data is a prerequisite for trustworthiness. We present a novel model that establishes upper and lower bounds on the accuracy, without requiring gold labels for the unseen data. We achieve this by training a *discriminator* which predicts whether the output of a given sequence-to-sequence model is correct or not. We show across a variety of tagging, parsing, and semantic parsing tasks that the gold accuracy is reliably between the predicted upper and lower bounds, and that these bounds are remarkably close together.",
}

@inproceedings{varshney-etal-2022-towards,
    title = "Towards Improving Selective Prediction Ability of {NLP} Systems",
    author = "Varshney, Neeraj  and
      Mishra, Swaroop  and
      Baral, Chitta",
    editor = "Gella, Spandana  and
      He, He  and
      Majumder, Bodhisattwa Prasad  and
      Can, Burcu  and
      Giunchiglia, Eleonora  and
      Cahyawijaya, Samuel  and
      Min, Sewon  and
      Mozes, Maximilian  and
      Li, Xiang Lorraine  and
      Augenstein, Isabelle  and
      Rogers, Anna  and
      Cho, Kyunghyun  and
      Grefenstette, Edward  and
      Rimell, Laura  and
      Dyer, Chris",
    booktitle = "Proceedings of the 7th Workshop on Representation Learning for NLP",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.repl4nlp-1.23",
    doi = "10.18653/v1/2022.repl4nlp-1.23",
    pages = "221--226",
}


@inproceedings{steen2023littlepushnlimodels,
    title = "With a Little Push, {NLI} Models can Robustly and Efficiently Predict Faithfulness",
    author = "Steen, Julius  and
      Opitz, Juri  and
      Frank, Anette  and
      Markert, Katja",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-short.79",
    doi = "10.18653/v1/2023.acl-short.79",
    pages = "914--924",
    abstract = "Conditional language models still generate unfaithful output that is not supported by their input. These unfaithful generations jeopardize trust in real-world applications such as summarization or human-machine interaction, motivating a need for automatic faithfulness metrics. To implement such metrics, NLI models seem attractive, since they solve a strongly related task that comes with a wealth of prior research and data. But recent research suggests that NLI models require costly additional machinery to perform reliably across datasets, e.g., by running inference on a cartesian product of input and generated sentences, or supporting them with a question-generation/answering step. In this work we show that pure NLI models {\_}can{\_} outperform more complex metrics when combining task-adaptive data augmentation with robust inference procedures. We propose: (1) Augmenting NLI training data toadapt NL inferences to the specificities of faithfulness prediction in dialogue;(2) Making use of both entailment and contradiction probabilities in NLI, and(3) Using Monte-Carlo dropout during inference. Applied to the TRUE benchmark, which combines faithfulness datasets across diverse domains and tasks, our approach strongly improves a vanilla NLI model and significantly outperforms previous work, while showing favourable computational cost.",
}


@inproceedings{zhao2023qtsummqueryfocusedsummarizationtabular,
    title = "{QTS}umm: Query-Focused Summarization over Tabular Data",
    author = "Zhao, Yilun  and
      Qi, Zhenting  and
      Nan, Linyong  and
      Mi, Boyu  and
      Liu, Yixin  and
      Zou, Weijin  and
      Han, Simeng  and
      Chen, Ruizhe  and
      Tang, Xiangru  and
      Xu, Yumo  and
      Radev, Dragomir  and
      Cohan, Arman",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.74",
    doi = "10.18653/v1/2023.emnlp-main.74",
    pages = "1157--1172",
    abstract = "People primarily consult tables to conduct data analysis or answer specific questions. Text generation systems that can provide accurate table summaries tailored to users{'} information needs can facilitate more efficient access to relevant data insights. Motivated by this, we define a new query-focused table summarization task, where text generation models have to perform human-like reasoning and analysis over the given table to generate a tailored summary. We introduce a new benchmark named QTSumm for this task, which contains 7,111 human-annotated query-summary pairs over 2,934 tables covering diverse topics. We investigate a set of strong baselines on QTSumm, including text generation, table-to-text generation, and large language models. Experimental results and manual analysis reveal that the new task presents significant challenges in table-to-text generation for future research. Moreover, we propose a new approach named ReFactor, to retrieve and reason over query-relevant information from tabular data to generate several natural language facts. Experimental results demonstrate that ReFactor can bring effective improvements to baselines by concatenating the generated facts to the model input. Our data and code are publicly available at https://github.com/yale-nlp/QTSumm.",
}

@misc{singha2023tabularrepresentationnoisyoperators,
      title={Tabular Representation, Noisy Operators, and Impacts on Table Structure Understanding Tasks in LLMs}, 
      author={Ananya Singha and José Cambronero and Sumit Gulwani and Vu Le and Chris Parnin},
      year={2023},
      eprint={2310.10358},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.10358}, 
}


@inproceedings{macková2023promapdatasetsproductmapping,
author = {Mackov\'{a}, Kate\v{r}ina and Pil\'{a}t, Martin},
title = {ProMap: Product Mapping Datasets},
year = {2024},
isbn = {978-3-031-56059-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-56060-6_11},
doi = {10.1007/978-3-031-56060-6_11},
abstract = {The goal of product mapping is to decide, whether two listings from two different e-shops describe the same products. Existing datasets of matching and non-matching pairs of products, however, often suffer from incomplete product information or contain only very distant non-matching products. In this paper, we introduce two new datasets for product mapping: ProMapCz consisting of 1,495 Czech product pairs and ProMapEn consisting of 1,555 English product pairs of matching and non-matching products manually scraped from two pairs of e-shops. The datasets contain both images and textual descriptions of the products, including their specifications, making them one of the most complete datasets for product mapping. Additionally, we divide the non-matching products into two different categories – close non-matches and medium non-matches, based on how similar the products are to each other. Even the medium non-matches are, however, pairs of products that are much more similar than non-matches in other datasets – for example, they still need to have the same brand and similar name and price. Finally, we train a number of product matching models on these datasets to demonstrate the advantages of having these two types of non-matches for the analysis of these models.},
booktitle = {Advances in Information Retrieval: 46th European Conference on Information Retrieval, ECIR 2024, Glasgow, UK, March 24–28, 2024, Proceedings, Part II},
pages = {159–172},
numpages = {14},
keywords = {Product Mapping, Product Matching, Similarity Computation, Machine Learning},
location = {Glasgow, United Kingdom}
}


@inproceedings{chen2021wikitabletlargescaledatatotextdataset,
    title = "{W}iki{T}able{T}: A Large-Scale Data-to-Text Dataset for Generating {W}ikipedia Article Sections",
    author = "Chen, Mingda  and
      Wiseman, Sam  and
      Gimpel, Kevin",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.17",
    doi = "10.18653/v1/2021.findings-acl.17",
    pages = "193--209",
}


@inproceedings{2019TabFactA,
title={TabFact: A Large-scale Dataset for Table-based Fact Verification},
author={Wenhu Chen and Hongmin Wang and Jianshu Chen and Yunkai Zhang and Hong Wang and Shiyang Li and Xiyou Zhou and William Yang Wang},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rkeJRhNYDH}
}



@misc{suri2023largelanguagemodelsdecision,
      title={Do Large Language Models Show Decision Heuristics Similar to Humans? A Case Study Using GPT-3.5}, 
      author={Gaurav Suri and Lily R. Slater and Ali Ziaee and Morgan Nguyen},
      year={2023},
      eprint={2305.04400},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2305.04400}, 
}

@misc{liu2024datasetslargelanguagemodels,
      title={Datasets for Large Language Models: A Comprehensive Survey}, 
      author={Yang Liu and Jiahuan Cao and Chongyu Liu and Kai Ding and Lianwen Jin},
      year={2024},
      eprint={2402.18041},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.18041}, 
}

@misc{kim2024biggenbenchprincipledbenchmark,
      title={The BiGGen Bench: A Principled Benchmark for Fine-grained Evaluation of Language Models with Language Models}, 
      author={Seungone Kim and Juyoung Suk and Ji Yong Cho and Shayne Longpre and Chaeeun Kim and Dongkeun Yoon and Guijin Son and Yejin Cho and Sheikh Shafayat and Jinheon Baek and Sue Hyun Park and Hyeonbin Hwang and Jinkyung Jo and Hyowon Cho and Haebin Shin and Seongyun Lee and Hanseok Oh and Noah Lee and Namgyu Ho and Se June Joo and Miyoung Ko and Yoonjoo Lee and Hyungjoo Chae and Jamin Shin and Joel Jang and Seonghyeon Ye and Bill Yuchen Lin and Sean Welleck and Graham Neubig and Moontae Lee and Kyungjae Lee and Minjoon Seo},
      year={2024},
      eprint={2406.05761},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.05761}, 
}


@inproceedings{goyal2023faithfulmodelevaluationmodelbased,
    title = "Faithful Model Evaluation for Model-Based Metrics",
    author = "Hu, Qian  and
      Goyal, Palash  and
      Gupta, Rahul",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.464",
    doi = "10.18653/v1/2023.emnlp-main.464",
    pages = "7484--7489",
    abstract = "Statistical significance testing is used in natural language processing (NLP) to determine whether the results of a study or experiment are likely to be due to chance or if they reflect a genuine relationship. A key step in significance testing is the estimation of confidence interval which is a function of sample variance. Sample variance calculation is straightforward when evaluating against ground truth. However, in many cases, a metric model is often used for evaluation. For example, to compare toxicity of two large language models, a toxicity classifier is used for evaluation. Existing works usually do not consider the variance change due to metric model errors, which can lead to wrong conclusions. In this work, we establish the mathematical foundation of significance testing for model-based metrics. With experiments on public benchmark datasets and a production system, we show that considering metric model errors to calculate sample variances for model-based metrics changes the conclusions in certain experiments.",
}


@inproceedings{kim2024prometheusinducingfinegrainedevaluation,
title={Prometheus: Inducing Fine-Grained Evaluation Capability in Language Models},
author={Seungone Kim and Jamin Shin and Yejin Cho and Joel Jang and Shayne Longpre and Hwaran Lee and Sangdoo Yun and Seongjin Shin and Sungdong Kim and James Thorne and Minjoon Seo},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=8euJaTveKw}
}


@Article{Li2022TextMO,
AUTHOR = {Li, Shugang and Liu, Fang and Zhang, Yuqi and Zhu, Boyi and Zhu, He and Yu, Zhaoxu},
TITLE = {Text Mining of User-Generated Content (UGC) for Business Applications in E-Commerce: A Systematic Review},
JOURNAL = {Mathematics},
VOLUME = {10},
YEAR = {2022},
NUMBER = {19},
ARTICLE-NUMBER = {3554},
URL = {https://www.mdpi.com/2227-7390/10/19/3554},
ISSN = {2227-7390},
ABSTRACT = {In the Web2.0 era, user-generated content (UGC) provides a valuable source of data to aid in understanding consumers and driving intelligent business. Text mining techniques, such as semantic analysis and sentiment analysis, help to extract meaningful information embedded in UGC. However, research on text mining of UGC for e-commerce business applications involves interdisciplinary knowledge, and few studies have systematically summarized the research framework and application directions of related research in this field. First, based on e-commerce practice, in this study, we derive a general framework to summarize the mainstream research in this field. Second, widely used text mining techniques are introduced, including semantic and sentiment analysis. Furthermore, we analyze the development status of semantic analysis in terms of text representation and semantic understanding. Then, the definition, development, and technical classification of sentiment analysis techniques are introduced. Third, we discuss mainstream directions of text mining for business applications, ranging from high-quality UGC detection and consumer profiling, to product enhancement and marketing. Finally, research gaps with respect to these efforts are emphasized, and suggestions are provided for future work. We also provide prospective directions for future research.},
DOI = {10.3390/math10193554}
}


@inproceedings{Elbattah2021TheRO,
  title={The Role of Text Analytics in Healthcare: A Review of Recent Developments and Applications},
  author={Mahmoud Elbattah and {\'E}milien Arnaud and Maxime Gignon and Gilles Dequen},
  booktitle={International Conference on Health Informatics},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:232094056}
}


@article{Luque2019AnAR,
  title={An advanced review on text mining in medicine},
  author={Carmen Luque and Jos{\'e} Mar{\'i}a Luna and Mar{\'i}a Luque and Sebasti{\'a}n Ventura},
  journal={Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  year={2019},
  volume={9},
  url={https://api.semanticscholar.org/CorpusID:86373507}
}

@article{Gupta2020ComprehensiveRO,
  title={Comprehensive review of text-mining applications in finance},
  author={Aaryan Gupta and Vinya Dengre and Hamza Abubakar Kheruwala and Manan Shah},
  journal={Financial Innovation},
  year={2020},
  volume={6},
  url={https://api.semanticscholar.org/CorpusID:226951077}
}

@article{Wang2021GeneralizingTU,
  title={Generalizing to Unseen Domains: A Survey on Domain Generalization},
  author={Jindong Wang and Cuiling Lan and Chang Liu and Yidong Ouyang and Tao Qin},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2021},
  volume={35},
  pages={8052-8072},
  url={https://api.semanticscholar.org/CorpusID:232110832}
}

@article{Yu2020ASO,
  title={A Survey of Knowledge-enhanced Text Generation},
  author={W. Yu and Wenhao Yu and Chenguang Zhu and Zaitang Li and Zhiting Hu and Qingyun Wang and Heng Ji and Meng Jiang},
  journal={ACM Computing Surveys},
  year={2020},
  volume={54},
  pages={1 - 38},
  url={https://api.semanticscholar.org/CorpusID:222272210}
}

@article{Liu2021PretrainPA,
  title={Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing},
  author={Pengfei Liu and Weizhe Yuan and Jinlan Fu and Zhengbao Jiang and Hiroaki Hayashi and Graham Neubig},
  journal={ACM Computing Surveys},
  year={2021},
  volume={55},
  pages={1 - 35},
  url={https://api.semanticscholar.org/CorpusID:236493269}
}

@article{Jin2023LargeLM,
  title={Large Language Models on Graphs: A Comprehensive Survey},
  author={Bowen Jin and Gang Liu and Chi Han and Meng Jiang and Heng Ji and Jiawei Han},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2023},
  volume={36},
  pages={8622-8642},
  url={https://api.semanticscholar.org/CorpusID:265659184}
}

@article{He2023ReviewOS,
  title={Review on Sentiment Analysis of E-commerce Product Comments},
  author={Aixiang He and Mideth B. Abisado},
  journal={2023 IEEE 15th International Conference on Advanced Infocomm Technology (ICAIT)},
  year={2023},
  pages={398-406},
  url={https://api.semanticscholar.org/CorpusID:266601440}
}


@inproceedings{wiseman2017challengesdatatodocumentgeneration,
    title = "Challenges in Data-to-Document Generation",
    author = "Wiseman, Sam  and
      Shieber, Stuart  and
      Rush, Alexander",
    editor = "Palmer, Martha  and
      Hwa, Rebecca  and
      Riedel, Sebastian",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1239",
    doi = "10.18653/v1/D17-1239",
    pages = "2253--2263",
    abstract = "Recent neural models have shown significant progress on the problem of generating short descriptive texts conditioned on a small number of database records. In this work, we suggest a slightly more difficult data-to-text generation task, and investigate how effective current approaches are on this task. In particular, we introduce a new, large-scale corpus of data records paired with descriptive documents, propose a series of extractive evaluation methods for analyzing performance, and obtain baseline results using current neural generation methods. Experiments show that these models produce fluent text, but fail to convincingly approximate human-generated documents. Moreover, even templated baselines exceed the performance of these neural models on some metrics, though copy- and reconstruction-based extensions lead to noticeable improvements.",
}


@inproceedings{chen2020logicalnaturallanguagegeneration,
    title = "Logical Natural Language Generation from Open-Domain Tables",
    author = "Chen, Wenhu  and
      Chen, Jianshu  and
      Su, Yu  and
      Chen, Zhiyu  and
      Wang, William Yang",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.708",
    doi = "10.18653/v1/2020.acl-main.708",
    pages = "7929--7942",
    abstract = "Neural natural language generation (NLG) models have recently shown remarkable progress in fluency and coherence. However, existing studies on neural NLG are primarily focused on surface-level realizations with limited emphasis on logical inference, an important aspect of human thinking and language. In this paper, we suggest a new NLG task where a model is tasked with generating natural language statements that can be \textit{logically entailed} by the facts in an open-domain semi-structured table. To facilitate the study of the proposed logical NLG problem, we use the existing TabFact dataset{\textasciitilde}(CITATION) featured with a wide range of logical/symbolic inferences as our testbed, and propose new automatic metrics to evaluate the fidelity of generation models w.r.t. logical inference. The new task poses challenges to the existing monotonic generation frameworks due to the mismatch between sequence order and logical order. In our experiments, we comprehensively survey different generation architectures (LSTM, Transformer, Pre-Trained LM) trained with different algorithms (RL, Adversarial Training, Coarse-to-Fine) on the dataset and made following observations: 1) Pre-Trained LM can significantly boost both the fluency and logical fidelity metrics, 2) RL and Adversarial Training are trading fluency for fidelity, 3) Coarse-to-Fine generation can help partially alleviate the fidelity issue while maintaining high language fluency. The code and data are available at \url{https://github.com/wenhuchen/LogicNLG}.",
}


@inproceedings{parikh2020tottocontrolledtabletotextgeneration,
    title = "{ToTTo}: A Controlled Table-To-Text Generation Dataset",
    author = "Parikh, Ankur  and
      Wang, Xuezhi  and
      Gehrmann, Sebastian  and
      Faruqui, Manaal  and
      Dhingra, Bhuwan  and
      Yang, Diyi  and
      Das, Dipanjan",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.89",
    doi = "10.18653/v1/2020.emnlp-main.89",
    pages = "1173--1186",
    abstract = "We present ToTTo, an open-domain English table-to-text dataset with over 120,000 training examples that proposes a controlled generation task: given a Wikipedia table and a set of highlighted table cells, produce a one-sentence description. To obtain generated targets that are natural but also faithful to the source table, we introduce a dataset construction process where annotators directly revise existing candidate sentences from Wikipedia. We present systematic analyses of our dataset and annotation process as well as results achieved by several state-of-the-art baselines. While usually fluent, existing methods often hallucinate phrases that are not supported by the table, suggesting that this dataset can serve as a useful research benchmark for high-precision conditional text generation.",
}

@inproceedings{cheng-etal-2022-hitab,
    title = "{H}i{T}ab: A Hierarchical Table Dataset for Question Answering and Natural Language Generation",
    author = "Cheng, Zhoujun  and
      Dong, Haoyu  and
      Wang, Zhiruo  and
      Jia, Ran  and
      Guo, Jiaqi  and
      Gao, Yan  and
      Han, Shi  and
      Lou, Jian-Guang  and
      Zhang, Dongmei",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.78",
    doi = "10.18653/v1/2022.acl-long.78",
    pages = "1094--1110",
    abstract = "Tables are often created with hierarchies, but existing works on table reasoning mainly focus on flat tables and neglect hierarchical tables. Hierarchical tables challenge numerical reasoning by complex hierarchical indexing, as well as implicit relationships of calculation and semantics. We present a new dataset, HiTab, to study question answering (QA) and natural language generation (NLG) over hierarchical tables. HiTab is a cross-domain dataset constructed from a wealth of statistical reports and Wikipedia pages, and has unique characteristics: (1) nearly all tables are hierarchical, and (2) QA pairs are not proposed by annotators from scratch, but are revised from real and meaningful sentences authored by analysts. (3) to reveal complex numerical reasoning in statistical reports, we provide fine-grained annotations of quantity and entity alignment. Experiments suggest that this HiTab presents a strong challenge for existing baselines and a valuable benchmark for future research. Targeting hierarchical structure, we devise a hierarchy-aware logical form for symbolic reasoning over tables, which shows high effectiveness. Targeting table reasoning, we leverage entity and quantity alignment to explore partially supervised training in QA and conditional generation in NLG, and largely reduce spurious predictions in QA and produce better descriptions in NLG.",
}

@inproceedings{moosavi2021scigen,
title={SciGen: a Dataset for Reasoning-Aware Text Generation from Scientific Tables},
author={Nafise Sadat Moosavi and Andreas R{\"u}ckl{\'e} and Dan Roth and Iryna Gurevych},
booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
year={2021},
url={https://openreview.net/forum?id=Jul-uX7EV_I}
}

@inproceedings{suadaa-etal-2021-towards,
    title = "Towards Table-to-Text Generation with Numerical Reasoning",
    author = "Suadaa, Lya Hulliyyatus  and
      Kamigaito, Hidetaka  and
      Funakoshi, Kotaro  and
      Okumura, Manabu  and
      Takamura, Hiroya",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.115",
    doi = "10.18653/v1/2021.acl-long.115",
    pages = "1451--1465",
    abstract = "Recent neural text generation models have shown significant improvement in generating descriptive text from structured data such as table formats. One of the remaining important challenges is generating more analytical descriptions that can be inferred from facts in a data source. The use of a template-based generator and a pointer-generator is among the potential alternatives for table-to-text generators. In this paper, we propose a framework consisting of a pre-trained model and a copy mechanism. The pre-trained models are fine-tuned to produce fluent text that is enriched with numerical reasoning. However, it still lacks fidelity to the table contents. The copy mechanism is incorporated in the fine-tuning step by using general placeholders to avoid producing hallucinated phrases that are not supported by a table while preserving high fluency. In summary, our contributions are (1) a new dataset for numerical table-to-text generation using pairs of a table and a paragraph of a table description with richer inference from scientific papers, and (2) a table-to-text generation framework enriched with numerical reasoning.",
}


@article{nan2021fetaqafreeformtablequestion,
    title = "{F}e{T}a{QA}: Free-form Table Question Answering",
    author = "Nan, Linyong  and
      Hsieh, Chiachun  and
      Mao, Ziming  and
      Lin, Xi Victoria  and
      Verma, Neha  and
      Zhang, Rui  and
      Kry{\'s}ci{\'n}ski, Wojciech  and
      Schoelkopf, Hailey  and
      Kong, Riley  and
      Tang, Xiangru  and
      Mutuma, Mutethia  and
      Rosand, Ben  and
      Trindade, Isabel  and
      Bandaru, Renusree  and
      Cunningham, Jacob  and
      Xiong, Caiming  and
      Radev, Dragomir  and
      Radev, Dragomir",
    editor = "Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "10",
    year = "2022",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2022.tacl-1.3",
    doi = "10.1162/tacl_a_00446",
    pages = "35--49",
    abstract = "Existing table question answering datasets contain abundant factual questions that primarily evaluate a QA system{'}s comprehension of query and tabular data. However, restricted by their short-form answers, these datasets fail to include question{--}answer interactions that represent more advanced and naturally occurring information needs: questions that ask for reasoning and integration of information pieces retrieved from a structured knowledge source. To complement the existing datasets and to reveal the challenging nature of the table-based question answering task, we introduce FeTaQA, a new dataset with 10K Wikipedia-based table, question, free-form answer, supporting table cells pairs. FeTaQA is collected from noteworthy descriptions of Wikipedia tables that contain information people tend to seek; generation of these descriptions requires advanced processing that humans perform on a daily basis: Understand the question and table, retrieve, integrate, infer, and conduct text planning and surface realization to generate an answer. We provide two benchmark methods for the proposed task: a pipeline method based on semantic parsing-based QA systems and an end-to-end method based on large pretrained text generation models, and show that FeTaQA poses a challenge for both methods.",
}

@article{Zhang2023InstructionTF,
  title={Instruction Tuning for Large Language Models: A Survey},
  author={Shengyu Zhang and Linfeng Dong and Xiaoya Li and Sen Zhang and Xiaofei Sun and Shuhe Wang and Jiwei Li and Runyi Hu and Tianwei Zhang and Fei Wu and Guoyin Wang},
  journal={ArXiv},
  year={2023},
  volume={abs/2308.10792},
  url={https://api.semanticscholar.org/CorpusID:261049152}
}


@article{Chang2023ASO,
author = {Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and Ye, Wei and Zhang, Yue and Chang, Yi and Yu, Philip S. and Yang, Qiang and Xie, Xing},
title = {A Survey on Evaluation of Large Language Models},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3641289},
doi = {10.1145/3641289},
abstract = {Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, education, natural and social sciences, agent applications, and other areas. Secondly, we answer the ‘where’ and ‘how’ questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing the performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at:},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {39},
numpages = {45},
keywords = {Large language models, evaluation, model assessment, benchmark}
}


@inproceedings{Agrawal2023CanKG,
    title = "Can Knowledge Graphs Reduce Hallucinations in {LLM}s? : A Survey",
    author = "Agrawal, Garima  and
      Kumarage, Tharindu  and
      Alghamdi, Zeyad  and
      Liu, Huan",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.219",
    doi = "10.18653/v1/2024.naacl-long.219",
    pages = "3947--3960",
    abstract = "The contemporary LLMs are prone to producing hallucinations, stemming mainly from the knowledge gaps within the models. To address this critical limitation, researchers employ diverse strategies to augment the LLMs by incorporating external knowledge, aiming to reduce hallucinations and enhance reasoning accuracy. Among these strategies, leveraging knowledge graphs as a source of external information has demonstrated promising results. In this survey, we comprehensively review these knowledge-graph-based augmentation techniques in LLMs, focusing on their efficacy in mitigating hallucinations. We systematically categorize these methods into three overarching groups, offering methodological comparisons and performance evaluations. Lastly, this survey explores the current trends and challenges associated with these techniques and outlines potential avenues for future research in this emerging field.",
}




@Article{Lee2023ASO,
AUTHOR = {Lee, Seungjun and Lee, Jungseob and Moon, Hyeonseok and Park, Chanjun and Seo, Jaehyung and Eo, Sugyeong and Koo, Seonmin and Lim, Heuiseok},
TITLE = {A Survey on Evaluation Metrics for Machine Translation},
JOURNAL = {Mathematics},
VOLUME = {11},
YEAR = {2023},
NUMBER = {4},
ARTICLE-NUMBER = {1006},
URL = {https://www.mdpi.com/2227-7390/11/4/1006},
ISSN = {2227-7390},
ABSTRACT = {The success of Transformer architecture has seen increased interest in machine translation (MT). The translation quality of neural network-based MT transcends that of translations derived using statistical methods. This growth in MT research has entailed the development of accurate automatic evaluation metrics that allow us to track the performance of MT. However, automatically evaluating and comparing MT systems is a challenging task. Several studies have shown that traditional metrics (e.g., BLEU, TER) show poor performance in capturing semantic similarity between MT outputs and human reference translations. To date, to improve performance, various evaluation metrics have been proposed using the Transformer architecture. However, a systematic and comprehensive literature review on these metrics is still missing. Therefore, it is necessary to survey the existing automatic evaluation metrics of MT to enable both established and new researchers to quickly understand the trend of MT evaluation over the past few years. In this survey, we present the trend of automatic evaluation metrics. To better understand the developments in the field, we provide the taxonomy of the automatic evaluation metrics. Then, we explain the key contributions and shortcomings of the metrics. In addition, we select the representative metrics from the taxonomy, and conduct experiments to analyze related problems. Finally, we discuss the limitation of the current automatic metric studies through the experimentation and our suggestions for further research to improve the automatic evaluation metrics.},
DOI = {10.3390/math11041006}
}





@article{Dobre2015ACB,
  title={A Comparison Between BLEU and METEOR Metrics Used for Assessing Students within an Informatics Discipline Course},
  author={Iuliana Dobre},
  journal={Procedia - Social and Behavioral Sciences},
  year={2015},
  volume={180},
  pages={305-312},
  url={https://api.semanticscholar.org/CorpusID:60373804}
}


@inproceedings{liu2023gevalnlgevaluationusing,
    title = "{G}-Eval: {NLG} Evaluation using Gpt-4 with Better Human Alignment",
    author = "Liu, Yang  and
      Iter, Dan  and
      Xu, Yichong  and
      Wang, Shuohang  and
      Xu, Ruochen  and
      Zhu, Chenguang",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.153",
    doi = "10.18653/v1/2023.emnlp-main.153",
    pages = "2511--2522",
    abstract = "The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-Eval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose analysis on the behavior of LLM-based evaluators, and highlight the potential concern of LLM-based evaluators having a bias towards the LLM-generated texts.",
}

@inproceedings{loshchilov2018decoupled,
title={Decoupled Weight Decay Regularization},
author={Ilya Loshchilov and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Bkg6RiCqY7},
}


@inproceedings{zhang2020bertscoreevaluatingtextgeneration,
title={BERTScore: Evaluating Text Generation with BERT},
author={Tianyi Zhang* and Varsha Kishore* and Felix Wu* and Kilian Q. Weinberger and Yoav Artzi},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SkeHuCVFDr}
}

@inproceedings{zhao-etal-2023-investigating,
    title = "Investigating Table-to-Text Generation Capabilities of Large Language Models in Real-World Information Seeking Scenarios",
    author = "Zhao, Yilun  and
      Zhang, Haowei  and
      Si, Shengyun  and
      Nan, Linyong  and
      Tang, Xiangru  and
      Cohan, Arman",
    editor = "Wang, Mingxuan  and
      Zitouni, Imed",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-industry.17",
    doi = "10.18653/v1/2023.emnlp-industry.17",
    pages = "160--175",
    abstract = "Tabular data is prevalent across various industries, necessitating significant time and effort for users to understand and manipulate for their information-seeking purposes. The advancements in large language models (LLMs) have shown enormous potential to improve user efficiency. However, the adoption of LLMs in real-world applications for table information seeking remains underexplored. In this paper, we investigate the table-to-text capabilities of different LLMs using four datasets within two real-world information seeking scenarios. These include the LogicNLG and our newly-constructed LoTNLG datasets for data insight generation, along with the FeTaQA and our newly-constructed F2WTQ datasets for query-based generation. We structure our investigation around three research questions, evaluating the performance of LLMs in table-to-text generation, automated evaluation, and feedback generation, respectively. Experimental results indicate that the current high-performing LLM, specifically GPT-4, can effectively serve as a table-to-text generator, evaluator, and feedback generator, facilitating users{'} information seeking purposes in real-world scenarios. However, a significant performance gap still exists between other open-sourced LLMs (e.g., Vicuna and LLaMA-2) and GPT-4 models. Our data and code are publicly available at https://github.com/yale-nlp/LLM-T2T.",
}

@inproceedings{dang-2006-duc,
    title = "{DUC} 2005: Evaluation of Question-Focused Summarization Systems",
    author = "Dang, Hoa Trang",
    editor = "Chua, Tat-Seng  and
      Goldstein, Jade  and
      Teufel, Simone  and
      Vanderwende, Lucy",
    booktitle = "Proceedings of the Workshop on Task-Focused Summarization and Question Answering",
    month = jul,
    year = "2006",
    address = "Sydney, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W06-0707",
    pages = "48--55",
}

@inproceedings{zhong-etal-2021-qmsum,
    title = "{QMS}um: A New Benchmark for Query-based Multi-domain Meeting Summarization",
    author = "Zhong, Ming  and
      Yin, Da  and
      Yu, Tao  and
      Zaidi, Ahmad  and
      Mutuma, Mutethia  and
      Jha, Rahul  and
      Awadallah, Ahmed Hassan  and
      Celikyilmaz, Asli  and
      Liu, Yang  and
      Qiu, Xipeng  and
      Radev, Dragomir",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.472",
    doi = "10.18653/v1/2021.naacl-main.472",
    pages = "5905--5921",
    abstract = "Meetings are a key component of human collaboration. As increasing numbers of meetings are recorded and transcribed, meeting summaries have become essential to remind those who may or may not have attended the meetings about the key decisions made and the tasks to be completed. However, it is hard to create a single short summary that covers all the content of a long meeting involving multiple people and topics. In order to satisfy the needs of different types of users, we define a new query-based multi-domain meeting summarization task, where models have to select and summarize relevant spans of meetings in response to a query, and we introduce QMSum, a new benchmark for this task. QMSum consists of 1,808 query-summary pairs over 232 meetings in multiple domains. Besides, we investigate a locate-then-summarize method and evaluate a set of strong summarization baselines on the task. Experimental results and manual analysis reveal that QMSum presents significant challenges in long meeting summarization for future research. Dataset is available at \url{https://github.com/Yale-LILY/QMSum}.",
}

@inproceedings{giorgi-etal-2023-open,
    title = "Open Domain Multi-document Summarization: A Comprehensive Study of Model Brittleness under Retrieval",
    author = "Giorgi, John  and
      Soldaini, Luca  and
      Wang, Bo  and
      Bader, Gary  and
      Lo, Kyle  and
      Wang, Lucy  and
      Cohan, Arman",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.549",
    doi = "10.18653/v1/2023.findings-emnlp.549",
    pages = "8177--8199",
    abstract = "Multi-document summarization (MDS) assumes a set of topic-related documents are provided as input. In practice, this document set is not always available; it would need to be retrieved given an information need, i.e. a question or topic statement, a setting we dub {``}open-domain{'} MDS. We study this more challenging setting by formalizing the task and bootstrapping it using existing datasets, retrievers and summarizers. Via extensive automatic and human evaluation, we determine: (1) state-of-the-art summarizers suffer large reductions in performance when applied to open-domain MDS, (2) additional training in the open-domain setting can reduce this sensitivity to imperfect retrieval, and (3) summarizers are insensitive to the retrieval of duplicate documents and the order of retrieved documents, but highly sensitive to other errors, like the retrieval of irrelevant documents. Based on our results, we provide practical guidelines to enable future work on open-domain MDS, e.g. how to choose the number of retrieved documents to summarize. Our results suggest that new retrieval and summarization methods and annotated resources for training and evaluation are necessary for further progress in the open-domain setting.",
}

@article{fang2024large,
title={Large Language Models ({LLM}s) on Tabular Data: Prediction, Generation, and Understanding - A Survey},
author={Xi Fang and Weijie Xu and Fiona Anting Tan and Ziqing Hu and Jiani Zhang and Yanjun Qi and Srinivasan H. Sengamedu and Christos Faloutsos},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2024},
url={https://openreview.net/forum?id=IZnrCGF9WI},
note={}
}


@misc{jaitly2023betterserializationtabulardata,
      title={Towards Better Serialization of Tabular Data for Few-shot Classification with Large Language Models}, 
      author={Sukriti Jaitly and Tanay Shah and Ashish Shugani and Razik Singh Grewal},
      year={2023},
      eprint={2312.12464},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2312.12464}, 
}

@misc{gao2024jsontuninggeneralizablerobustcontrollable,
      title={JsonTuning: Towards Generalizable, Robust, and Controllable Instruction Tuning}, 
      author={Chang Gao and Wenxuan Zhang and Guizhen Chen and Wai Lam},
      year={2024},
      eprint={2310.02953},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.02953}, 
}

@inproceedings{akhtar-etal-2023-exploring,
    title = "Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data",
    author = "Akhtar, Mubashara  and
      Shankarampeta, Abhilash  and
      Gupta, Vivek  and
      Patil, Arpit  and
      Cocarascu, Oana  and
      Simperl, Elena",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.1028",
    doi = "10.18653/v1/2023.findings-emnlp.1028",
    pages = "15391--15405",
    abstract = "Numerical data plays a crucial role in various real-world domains like finance, economics, and science. Thus, understanding and reasoning with numbers are essential in these fields. Recent benchmarks have assessed the numerical reasoning abilities of language models, revealing their limitations in limited and specific numerical aspects. In this paper, we propose a complete hierarchical taxonomy for numerical reasoning skills, encompassing over ten reasoning types across four levels: representation, number sense, manipulation, and complex reasoning. We conduct a comprehensive evaluation of state-of-the-art models on all reasoning types. To identify challenging reasoning types for different model types, we develop a diverse and extensive set of numerical probes and measure performance shifts. By employing a semi-automated approach, we focus on the tabular Natural Language Inference (TNLI) task as a case study. While no single model excels in all reasoning types, FlanT5 (few-/zero-shot) and GPT3.5 (few-shot) demonstrate strong overall numerical reasoning skills compared to other models in our probes.",
}

@inproceedings{zhao-etal-2024-docmath,
    title = "{D}oc{M}ath-Eval: Evaluating Math Reasoning Capabilities of {LLM}s in Understanding Long and Specialized Documents",
    author = "Zhao, Yilun  and
      Long, Yitao  and
      Liu, Hongjun  and
      Kamoi, Ryo  and
      Nan, Linyong  and
      Chen, Lyuhao  and
      Liu, Yixin  and
      Tang, Xiangru  and
      Zhang, Rui  and
      Cohan, Arman",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.852",
    doi = "10.18653/v1/2024.acl-long.852",
    pages = "16103--16120",
    abstract = "Recent LLMs have demonstrated remarkable performance in solving exam-like math word problems. However, the degree to which these numerical reasoning skills are effective in real-world scenarios, particularly in expert domains, is still largely unexplored. This paper introduces DocMath-Eval, a comprehensive benchmark specifically designed to evaluate the numerical reasoning capabilities of LLMs in the context of understanding and analyzing specialized documents containing both text and tables. We conduct an extensive evaluation of 48 LLMs with Chain-of-Thought and Program-of-Thought prompting methods, aiming to comprehensively assess the capabilities and limitations of existing LLMs in DocMath-Eval. We found that even the current best-performing system (i.e., GPT-4o) still significantly lags behind human experts in solving complex numerical reasoning problems grounded in long contexts. We believe that DocMath-Eval can serve as a valuable benchmark for evaluating LLMs' capabilities in solving challenging numerical reasoning problems within expert domains.",
}

@misc{brinkmann2024extractgptexploringpotentiallarge,
      title={ExtractGPT: Exploring the Potential of Large Language Models for Product Attribute Value Extraction}, 
      author={Alexander Brinkmann and Roee Shraga and Christian Bizer},
      year={2024},
      eprint={2310.12537},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.12537}, 
}

@inproceedings{shinzato-etal-2023-unified,
    title = "A Unified Generative Approach to Product Attribute-Value Identification",
    author = "Shinzato, Keiji  and
      Yoshinaga, Naoki  and
      Xia, Yandi  and
      Chen, Wei-Te",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.413",
    doi = "10.18653/v1/2023.findings-acl.413",
    pages = "6599--6612",
    abstract = "Product attribute-value identification (PAVI) has been studied to link products on e-commerce sites with their attribute values (e.g., ⟨Material, Cotton⟩) using product text as clues. Technical demands from real-world e-commerce platforms require PAVI methods to handle unseen values, multi-attribute values, and canonicalized values, which are only partly addressed in existing extraction- and classification-based approaches. Motivated by this, we explore a generative approach to the PAVI task. We finetune a pre-trained generative model, T5, to decode a set of attribute-value pairs as a target sequence from the given product text. Since the attribute value pairs are unordered set elements, how to linearize them will matter; we, thus, explore methods of composing an attribute-value pair and ordering the pairs for the task. Experimental results confirm that our generation-based approach outperforms the existing extraction and classification-based methods on large-scale real-world datasets meant for those methods.",
}

%% Latest Additions %%

@inproceedings{nayeem-etal-2018-abstractive,
    title = "Abstractive Unsupervised Multi-Document Summarization using Paraphrastic Sentence Fusion",
    author = "Nayeem, Mir Tafseer  and
      Fuad, Tanvir Ahmed  and
      Chali, Yllias",
    editor = "Bender, Emily M.  and
      Derczynski, Leon  and
      Isabelle, Pierre",
    booktitle = "Proceedings of the 27th International Conference on Computational Linguistics",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/C18-1102/",
    pages = "1191--1204",
}

@inproceedings{nayeem-chali-2017-extract,
    title = "Extract with Order for Coherent Multi-Document Summarization",
    author = "Nayeem, Mir Tafseer  and
      Chali, Yllias",
    editor = "Riedl, Martin  and
      Somasundaran, Swapna  and
      Glava{\v{s}}, Goran  and
      Hovy, Eduard",
    booktitle = "Proceedings of {T}ext{G}raphs-11: the Workshop on Graph-based Methods for Natural Language Processing",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-2407/",
    doi = "10.18653/v1/W17-2407",
    pages = "51--56",
}

@inproceedings{chali-etal-2017-towards,
    title = "Towards Abstractive Multi-Document Summarization Using Submodular Function-Based Framework, Sentence Compression and Merging",
    author = "Chali, Yllias  and
      Tanvee, Moin  and
      Nayeem, Mir Tafseer",
    editor = "Kondrak, Greg  and
      Watanabe, Taro",
    booktitle = "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = nov,
    year = "2017",
    address = "Taipei, Taiwan",
    publisher = "Asian Federation of Natural Language Processing",
    url = "https://aclanthology.org/I17-2071/",
    pages = "418--424",
}

@inproceedings{10.1145/3132847.3133106,
author = {Nayeem, Mir Tafseer and Chali, Yllias},
title = {Paraphrastic Fusion for Abstractive Multi-Sentence Compression Generation},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133106},
doi = {10.1145/3132847.3133106},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2223–2226},
numpages = {4},
keywords = {abstractive compression generation, lexical paraphrasing, multi-sentence compression, sentence fusion},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@InProceedings{10.1007/978-3-030-15719-7_14,
author="Nayeem, Mir Tafseer
and Fuad, Tanvir Ahmed
and Chali, Yllias",
editor="Azzopardi, Leif
and Stein, Benno
and Fuhr, Norbert
and Mayr, Philipp
and Hauff, Claudia
and Hiemstra, Djoerd",
title="Neural Diverse Abstractive Sentence Compression Generation",
booktitle="Advances in Information Retrieval (ECIR)",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="109--116",
url="https://link.springer.com/chapter/10.1007/978-3-030-15719-7_14",
isbn="978-3-030-15719-7"
}

@book{nayeem2017methods,
  title={Methods of Sentence Extraction, Abstraction and Ordering for Automatic Text Summarization},
  author={Nayeem, Mir Tafseer},
  year={2017},
  url={https://hdl.handle.net/10133/4993},
  publisher={Universtiy of Lethbridge, Department of Mathematics and Computer Science}
}

@inproceedings{chowdhury-etal-2021-unsupervised,
    title = "Unsupervised Abstractive Summarization of {B}engali Text Documents",
    author = "Chowdhury, Radia Rayan  and
      Nayeem, Mir Tafseer  and
      Mim, Tahsin Tasnim  and
      Chowdhury, Md. Saifur Rahman  and
      Jannat, Taufiqul",
    editor = "Merlo, Paola  and
      Tiedemann, Jorg  and
      Tsarfaty, Reut",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.224/",
    doi = "10.18653/v1/2021.eacl-main.224",
    pages = "2612--2619",
}

@article{FUAD2019216,
title = {Neural sentence fusion for diversity driven abstractive multi-document summarization},
journal = {Computer Speech \& Language},
volume = {58},
pages = {216-230},
year = {2019},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2019.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S0885230818303449},
author = {Tanvir Ahmed Fuad and Mir Tafseer Nayeem and Asif Mahmud and Yllias Chali},
keywords = {Abstractive multi-document Summarization, Sentence fusion, Neural fusion model, Document clustering},
}

%% Preprocessing %%

@inproceedings{nayeem-rafiei-2023-role,
    title = "On the Role of Reviewer Expertise in Temporal Review Helpfulness Prediction",
    author = "Nayeem, Mir Tafseer  and
      Rafiei, Davood",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Findings of the Association for Computational Linguistics: EACL 2023",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-eacl.125/",
    doi = "10.18653/v1/2023.findings-eacl.125",
    pages = "1684--1692",
}

@inproceedings{nayeem-rafiei-2024-kidlm,
    title = "{K}id{LM}: Advancing Language Models for Children {--} Early Insights and Future Directions",
    author = "Nayeem, Mir Tafseer  and
      Rafiei, Davood",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.277/",
    doi = "10.18653/v1/2024.emnlp-main.277",
    pages = "4813--4836",
}

%% Reaonsing and LLM %%

@inproceedings{islam-etal-2024-large,
    title = "Are Large Vision Language Models up to the Challenge of Chart Comprehension and Reasoning",
    author = "Islam, Mohammed Saidul  and
      Rahman, Raian  and
      Masry, Ahmed  and
      Laskar, Md Tahmid Rahman  and
      Nayeem, Mir Tafseer  and
      Hoque, Enamul",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.191/",
    doi = "10.18653/v1/2024.findings-emnlp.191",
    pages = "3334--3368",
}

@inproceedings{nahid-rafiei-2024-tabsqlify,
    title = "{T}ab{SQL}ify: Enhancing Reasoning Capabilities of {LLM}s Through Table Decomposition",
    author = "Nahid, Md  and
      Rafiei, Davood",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.320/",
    doi = "10.18653/v1/2024.naacl-long.320",
    pages = "5725--5737",
}



%% Entity Matching and Others%%

@inproceedings{10.1145/3583780.3615172,
author = {Naeim abadi, Ali and Nayeem, Mir Tafseer and Rafiei, Davood},
title = {Product Entity Matching via Tabular Data},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615172},
doi = {10.1145/3583780.3615172},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {4215–4219},
numpages = {5},
keywords = {pretrained language models, entity resolution, entity matching},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

%%%%%%%%%%%%

@inproceedings{kabir-etal-2024-benllm,
    title = "{B}en{LLM}-Eval: A Comprehensive Evaluation into the Potentials and Pitfalls of Large Language Models on {B}engali {NLP}",
    author = "Kabir, Mohsinul  and
      Islam, Mohammed Saidul  and
      Laskar, Md Tahmid Rahman  and
      Nayeem, Mir Tafseer  and
      Bari, M Saiful  and
      Hoque, Enamul",
    editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.201/",
    pages = "2238--2252",
}

@inproceedings{akash-etal-2023-shironaam,
    title = "Shironaam: {B}engali News Headline Generation using Auxiliary Information",
    author = "Akash, Abu Ubaida  and
      Nayeem, Mir Tafseer  and
      Shohan, Faisal Tareque  and
      Islam, Tanvir",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.4/",
    doi = "10.18653/v1/2023.eacl-main.4",
    pages = "52--67",
}

@inproceedings{shohan-etal-2024-xl,
    title = "{XL}-{H}ead{T}ags: Leveraging Multimodal Retrieval Augmentation for the Multilingual Generation of News Headlines and Tags",
    author = "Shohan, Faisal Tareque  and
      Nayeem, Mir Tafseer  and
      Islam, Samsul  and
      Akash, Abu Ubaida  and
      Joty, Shafiq",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.771/",
    doi = "10.18653/v1/2024.findings-acl.771",
    pages = "12991--13024",
}

@misc{nayeem2024lfosum,
      title={LFOSum: Summarizing Long-form Opinions with Large Language Models}, 
      author={Mir Tafseer Nayeem and Davood Rafiei},
      year={2024},
      eprint={2410.13037},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.13037}, 
}