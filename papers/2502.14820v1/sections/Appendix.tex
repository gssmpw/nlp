\clearpage
\appendix
\twocolumn[{%
 \centering
 \Large\bf Supplementary Material: Appendices \\ [20pt]
}]


\section{Evaluation Metrics}
\label{sec:evaluation-metrics}

%\paragraph{Metrics.} Evaluation metrics are essential for assessing the quality of text generation models. The most widely used metrics include: 

\begin{itemize} 
    \item \textbf{BLEU (Bilingual Evaluation Understudy)}: Commonly used in machine translation and natural language generation, BLEU measures the overlap of n-grams between generated and reference texts. Despite its popularity, BLEU has limitations, particularly in capturing semantic similarity and evaluating beyond exact matches \cite{Reiter2018A}. 
    \item \textbf{ROUGE (Recall-Oriented Understudy for Gisting Evaluation)}: Focuses on recall-oriented evaluation by comparing the overlap of n-grams, word sequences, and word pairs between generated summaries and reference texts. It is highly effective for summarization tasks \cite{Ganesan2015ROUGE}.
    \item \textbf{METEOR (Metric for Evaluation of Translation with Explicit ORdering)}: Incorporates stemming, synonymy, and flexible matching, providing a more nuanced evaluation than BLEU. It strongly correlates with human judgments, especially in translation tasks \cite{Dobre2015ACB}. 
    \item \textbf{BERTScore}: Leverages contextual embeddings from pre-trained transformer models to measure semantic similarity between generated and reference texts. Unlike n-gram-based metrics, BERTScore captures meaning and context, offering a robust evaluation for text generation tasks \cite{zhang2020bertscoreevaluatingtextgeneration}.
\end{itemize}

The reliability and faithfulness of generated text, particularly in applications requiring high accuracy, such as medical or financial domains is crucial. To identify inaccuracies, hallucination detection was conducted using Prometheus 2, a robust evaluation model designed for analyzing outputs of Large Language Models (LLMs) \cite{kim2024prometheus2opensource}. This framework helps evaluate three critical dimensions: 

\begin{itemize} 
    \item \textbf{Faithfulness}: Ensures that the generated content aligns with the source data and avoids unsupported claims \cite{madsen-etal-2022-evaluating, jacovi-goldberg-2020-towards}. 
    \item \textbf{Correctness}: Measures factual accuracy and checks for logical consistency in the output \cite{yao2023predictinggeneralizationperformancecorrectness, kim2024prometheus2opensource}. 
    \item \textbf{Fluency}: Evaluates the readability and linguistic quality of the text, ensuring it adheres to natural language norms \cite{suadaa-etal-2021-towards, Lee2023ASO}. 
\end{itemize}


\section{Models for Fine-tuning}
\label{sec:fine-tuning-models}

\begin{itemize}
    \item \textbf{LLaMA 2-Chat 7B} \cite{touvron2023llama}: LLaMA 2-Chat 7B is a fine-tuned variant of the LLaMA 2 series, optimized for dialogue applications. It employs an autoregressive transformer architecture and has been trained on a diverse dataset comprising 2 trillion tokens from publicly available sources. The fine-tuning process incorporates over one million human-annotated examples to enhance its conversational capabilities and alignment with human preferences for helpfulness and safety.

    \item \textbf{StructLM 7B} \cite{zhuang2024structlm}: StructLM 7B is a large language model fine-tuned specifically for structured knowledge grounding tasks. It utilizes the CodeLlama-Instruct model as its base and is trained on the SKGInstruct dataset, which encompasses a mixture of 19 structured knowledge grounding tasks. This specialized training enables StructLM to effectively process and generate text from structured data sources such as tables, databases, and knowledge graphs, making it robust in domain-specific text generation tasks.

    \item \textbf{Mistral 7B-Instruct} \cite{jiang2023mistral}: Mistral 7B-Instruct is an instruction fine-tuned version of the Mistral 7B model, designed to handle a wide array of tasks by following diverse instructions. It features a 32k context window and employs a Rope-theta of 1e6, without utilizing sliding-window attention. This configuration allows Mistral 7B-Instruct to perform effectively in multi-modal and domain-adapted text generation scenarios, achieving state-of-the-art performance in various benchmarks.
\end{itemize}




\section{Prometheus Evaluation} 
\label{appendix:Prometheus}

To evaluate model-based metrics, the Prometheus framework \cite{kim2024prometheus2opensource} was employed, utilizing structured prompts for three key evaluation criteria: fluency, correctness, and faithfulness. The primary framework leverages an Absolute System Prompt, which defines the role of the evaluator and ensures objective, consistent assessments based on established rubrics. This Absolute System Prompt, shown in Listing~\ref{code:ABS-System-Prompt}, forms the foundation for all evaluations across metrics.

\begin{lstlisting}[style=textstyle, frame = single, caption=Absolute System Prompt, label=code:ABS-System-Prompt]
You are a fair judge assistant tasked with providing clear, objective feedback based on specific criteria, ensuring each assessment reflects the absolute standards set for performance.
\end{lstlisting}
The task descriptions for evaluating fluency, correctness, and faithfulness share a similar structure, as shown in Listing~\ref{code:Task-description-Faithfulness},\ref{code:Task-description-fluency-correctness}. These instructions define the evaluation process, requiring detailed feedback and a score between 1 and 5, strictly adhering to a given rubric.

%\vspace{1.2cm}

\begin{lstlisting}[style=textstyle, frame = single, caption=Task description used for evaluation of faithfulness, label=code:Task-description-Faithfulness]
###Task Description:
An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.
1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.
2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.
3. The output format should look as follows: "Feedback: (write a feedback for criteria) [RESULT] (an integer number between 1 and 5)"
4. Please do not generate any other opening, closing, and explanations.
5. Only evaluate on common things between generated answer and reference answer. Don't evaluate on things which are present in reference answer but not in generated answer.
\end{lstlisting}


\subsection{Instructions for Evaluation}

Prometheus prompts are customized for each evaluation metric. Below are the specialized structures and rubrics for fluency, faithfulness, and correctness.

\paragraph{Faithfulness}
This metric ensures the generated response aligns with both the provided context and reference answers. The evaluation structure incorporates specific rubrics for relevance and information consistency.


\begin{lstlisting}[style=textstyle, frame = single, caption=Task description used for evaluation of fluency and correctness, label=code:Task-description-fluency-correctness]
###Task Description:
An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.
1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.
2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.
3. The output format should look as follows: "Feedback: (write a feedback for criteria) [RESULT] (an integer number between 1 and 5)"
4. Please do not generate any other opening, closing, and explanations.
\end{lstlisting}



\begin{lstlisting}[style=textstyle, frame = single, caption=Prompt structured correctness, label=code:estructured-faithfulness]
###The instruction to evaluate:
Evaluate the fluency of the generated JSON answer.
###Context:
{Prompt}
###Existing answer (Score 5):
{reference_answer}
###Generate answer to evaluate:
{response}
###Score Rubrics:
"score1_description":"If the generated answer is not matching with any of the reference answers and also not having information from the context.",
"score2_description":"If the generated answer is having information from the context but not from existing answer and also have some irrelevant information.",
"score3_description":"If the generated answer is having relevant information from the context and some information from existing answer but have additional information that do not exist in context and also do not in existing answer.",
"score4_description":"If the generated answer is having relevant information from the context and some information from existing answer.",
"score5_description":"If the generated answer is matching with the existing answer and also having information from the context."}
###Feedback:
\end{lstlisting}

\paragraph{Fluency}
This metric evaluates the grammatical accuracy and readability of the generated response.

\begin{lstlisting}[style=textstyle, frame = single, caption=Prompt structured fluency, label=code:estructured-fluency]
###The instruction to evaluate: Evaluate 
the fluency of the generated JSON answer
###Response to evaluate: {response}
###Reference Answer (Score 5): 
{reference_answer}
###Score Rubrics:
"score1_description":"The generated JSON answer is not fluent and is difficult to understand.",
"score2_description":"The generated JSON answer has several grammatical errors and awkward phrasing.",
"score3_description":"The generated JSON answer is mostly fluent but contains some grammatical errors or awkward phrasing.",
"score4_description":"The generated JSON answer is fluent with minor grammatical errors or awkward phrasing.",
"score5_description":"The generated JSON answer is perfectly fluent with no grammatical errors or awkward phrase
###Feedback:
\end{lstlisting}

\paragraph{Correctness}
This metric assesses the logical accuracy and coherence of the generated response compared to the reference.

\begin{lstlisting}[style=textstyle, frame = single, caption=Prompt estructured correctness, label=code:estructured-correctness]
###The instruction to evaluate:
Your task is to evaluate the generated answer and reference answer for the query: {Prompt}
###Response to evaluate:
{response}
###Reference Answer (Score 5):
{reference_answer}
###Score Rubrics:
"criteria": "Is the model proficient in generate a coherence response",
"score1_description": "If the generated answer is not matching with any of the reference answers.",
"score2_description": "If the generated answer is according to reference answer but not relevant to user query.",
"score3_description": "If the generated answer is relevant to the user query and reference answer but contains mistakes.",
"score4_description": "If the generated answer is relevant to the user query and has the exact same metrics as the reference answer, but it is not as concise.",
"score5_description": "If the generated answer is relevant to the user query and fully correct according to the reference answer.

###Feedback: 
\end{lstlisting}

\section{Fine-tuning models}
\label{appendix:fine-tuning-models}

The prompts outlined below utilized for training eC-Tab2Text models (Listing \ref{code:Prompt Dataset 1}) and for the QTSumm dataset (Listing \ref{code:Prompt Dataset 2}).

\begin{lstlisting}[style=textstyle, frame = single, caption=Prompt structure for eC-Tab2Text, label=code:Prompt Dataset 1]
"Given following json that contains specifications of a product, generate a review of the key characteristics with json format. Follow the structure on Keys to write the Output: 

### Product: Product for JSON specifications

### Keys: Combination of the keys of the JSON reviews

### Output: reviews for JSON reviews accordingly to the keys"
\end{lstlisting}

\vspace{1cm}

\begin{lstlisting}[style=textstyle, frame = single, caption=Prompt structure for QTSumm, label=code:Prompt Dataset 2]
"Given following json that contains specifications of a product, generate a review of the key characteristics with json format. Follow the structure on Keys to write the Output: 
### Product: Column table of JSON specifications
### Keys: Column query of the dataset
### Output: Column summary of the dataset"
\end{lstlisting}

\section{eC-Tab2Text Data Formats} 
\label{Appendix: eC-Tab2Text Data Collection}

\begin{lstlisting}[style=jsonstyle, frame = single, caption=JSON Data Format Product specification, label=code:JSON-specs]
{
  "url": {
    "keys_specifications": [],
    "full_specifications": [
      "Launch Date": "Launch Date",
      "General": {
        "subcategories1": [
            "value1" ...
            ],
        "subcategories2": [
            "value1" ...
            ], ...
      },
      "Characteristic1": {
        "subcategories1": [
            "value1" ...
            ],
        "subcategories2": [
            "value1" ...
            ], ...
      },
      "Characteristic2": {
        "subcategories1": [
            "value1" ...
            ],
        "subcategories2": [
            "value1" ...
            ], ...
      }, ...
    ]
  },
}
\end{lstlisting}


\begin{lstlisting}[style=jsonstyle, frame = single, caption=JSON Data Format reviews, label=code:JSON-reviews]
{
  "url": {
    "text": {
      "Characteristic1": ["Description1"],
      "Characteristic2": ["Description2"], ...
    }
  }
}
\end{lstlisting}

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=16cm]{figures/pricebaba_review_structure.pdf}
    \caption{An illustration of sample output texts generated for user-specific queries based on structured input from product tables.}
    %\cite{OnePlusNord35G2023}}
    \label{fig:pricebaba-review-structure}
\end{figure*}


\begin{figure*}[ht!]
    \centering
    \includegraphics[width=16cm]{figures/pricebaba_spec_structure.pdf}
    \caption{An example of a product specification table structure.}
    %\cite{OnePlusNord35G2023}.}
    \label{fig:pricebaba-spec-structure}
\end{figure*}

\begin{figure*}
\begin{lstlisting}[style=reviews, frame = single, caption=Output generated with Mistral Instruct trained with eC-Tab2Text., label=code:JSON-Mistral-eC-Tab2Text]
{
    "Software and Connectivity": "Tecno Spark Go 2020 will run Android Go UI on top of Android 10 out of the box. For the connectivity, the smartphone will support features like Dual sim VoLTE (Micro-Micro), Wi-Fi 802.11, b/g/n, 3.5mm jack, Bluetooth 4.2, 10-pin charging, GPS, and USB 2.0.",
    "Design and Display": "The Tecno Spark Go 2020 will reportedly showcase a 6.52 inch TFTdisplay with a resolution of 720 x 1600 pixelsand a pixel density of 269ppi. The display will supposedly have a waterdrop notch with minimum bezels.",
    "Battery and Performance": "The Tecno Spark Go 2020 will be embedded with a MediaTek Helio A20 chipset and PowerVR GE8300 GPU. The device will come with 2GB RAM and 32GB of internal memory, which can be expanded up to 256GB. Fuelling the device will be a huge 5000mAh battery."
}
\end{lstlisting}
\end{figure*}
\begin{figure*}
\begin{lstlisting}[style=reviews, frame = single, caption=Output generated with Gemini1.5-flash., label=code:JSON-Gemini]
{
    "Cameras": {
        "Main Camera": {
            "Description": "Single 13 MP main camera with LED flash, Auto Flash, and Auto Focus. Offers digital zoom.",
            "Video": "Specifications not provided."
        },
        "Front Camera": {
            "Description": "Single 8 MP main camera.",
            "Video": "Specifications not provided."
        }
    },
    "Battery and Performance": {
        "Battery": {
            "Capacity": "4000 mAh Li-ion battery",
            "Charging": "No fast charging or wireless charging."
        },
        "Performance": {
            "Processor": "Quad-core 1.3GHz processor.  Specific chipset and GPU information not provided.",
            "RAM": "4GB RAM",
            "Storage": "64GB internal storage, expandable up to 64GB via microSD card."
        }
    },
    "Design and Display": {
        "Display": {
            "Size": "6.53 inches (16.59 cm) IPS LCD with 720 x 1560 pixels resolution (263 ppi).",
            "Design": "Waterdrop notch display."
        },
        "Other": {
            "Colors": "Available in Blue and Purple.",
            "Fingerprint Sensor": "No fingerprint sensor."
        }
    }
}
\end{lstlisting}
\end{figure*}
\begin{figure*}
\begin{lstlisting}[style=reviews, frame = single, caption=Output generated with GPT-4o-mini., label=code:JSON-GPT4]
{
    "Software and Connectivity": {
        "Operating System": "Android 10 (Q) with Android Go custom UI",
        "Network Compatibility": "Supports 2G, 3G, and 4G networks, making it suitable for use across various locations",
        "SIM Slots": "Dual SIM (Nano + Nano), allowing for flexible connectivity options",
        "Wi-Fi and Bluetooth": "Equipped with Wi-Fi 802.11 b/g/n and Bluetooth 4.2 for seamless wireless connections",
        "GPS": "Includes A-GPS and Glonass for accurate location tracking",
        "FM Radio and Charging Port": "Features FM Radio and uses a microUSB 2.0 charging port"
    },
    "Design and Display": {
        "Dimensions": "165.6mm x 76.3mm x 9.1mm, making it a reasonably sized smartphone",
        "Weight": "193g, providing a solid feel without being overly heavy",
        "Display Size": "6.52 inches with a TFT display that offers a resolution of 720 x 1600 pixels",
        "Pixel Density": "269ppi, delivering decent clarity for everyday usage",
        "Design": "Waterdrop notch design adds a modern touch; available in colour options of Aqua Blue and Ice Jadeite"
    },
    "Battery and Performance": {
        "Battery Capacity": "5000 mAh, ensuring long-lasting performance between charges",
        "Battery Type": "Li-ion, which is standard for smartphones in this range",
        "Charging Options": "No wireless charging and no fast charging feature, but provides adequate battery life throughout the day",
        "Hardware": {
            "Chipset": "MediaTek Helio A20",
            "RAM": "2 GB, sufficient for basic tasks and everyday applications",
            "Internal Storage": "32 GB, expandable up to 256 GB via microSD card, allowing for additional storage for apps and media"
        }
    }
}
\end{lstlisting}
\end{figure*}

\begin{figure*}
\begin{lstlisting}[style=reviews, frame = single, caption=Output generated with StructLM trained with eC-Tab2Text., label=code:JSON-StructLM]
{
    "Software and Connectivity": "Tecno Spark Go 2020 will run on Android Go UI on top of Android 10 out of the box. The smartphone will offer connectivity options like Dual SIM VoLTE, Wi-Fi 802.11, b/g/n, GPS, and Bluetooth 4.2. The device will also come with a rear-mounted fingerprint sensor and a 3.5mm headphone jack.",
    "Design and Display": "The Tecno Spark Go 2020 is expected to feature a 6.52 inch (16.56 cm) display with a screen resolution of 720 x 1600 pixels. The display will come with a waterdrop notch design and a screen-to-body ratio of 40.97 %. The device will come in Aqua Blue and Ice Jade colors.",
    "Battery and Performance": "The Tecno Spark Go 2020 will be powered by a MediaTek Helio A20 processor clocked at 2GHz and a PowerVR GE8300 GPU. The device will come armed with 2GB RAM and an internal storage capacity of 32GB, which can be expanded upto 256GB via a microSD card."
}
\end{lstlisting}
\end{figure*}


\begin{figure*}
\begin{lstlisting}[style=reviews, frame = single, caption=Output generated with Llama2 trained with eC-Tab2Text., label=code:JSON-Llama2]
{
    "Battery and Performance": "The Tecno Spark Go 2020 is anticipated to be powered by MediaTek\u2019s latest entry-level Helio A20quad-core chipset coupled with a PowerVR GE8300 GPU. That will again reportedly be paired with 2 GB RAM and an internal storage capacity of only 32 GB, which can further be expanded up to 256GB. Further, the device will reportedly pack in a 5000mAh Li-ion battery but won\u2019t support fast charging.",
    "Cameras": "The Tecno Spark Go 2020 is expected to house a dual-camera setup on the back with a 13MP (Digital Zoom) camera as the primary sensor. Besides, there will also be a 2MP depth sensor onboard. On the front, the smartphone will supposedly feature an 8MP selfie shooter. There will also likely be a bunch of camera features such as Artificial Intelligence,Auto Flash,Auto Focus,Bokeh Effect,Continuos Shooting,Exposure compensation,Face detection,Geo tagging,High Dynamic Range mode (HDR),ISO control,Touch to focus,White balance presets.",
    "Design and Display": "The Tecno Spark Go 2020 will reportedly feature a 6.52 inch TFT panel tipped with a resolution of 720 x 1600 pixels. The pixel density will supposedly max out at 269ppi. The bezel-less display is further anticipated to boast a waterdrop notch design to furnish an immersive viewing experience."
}
\end{lstlisting}
\end{figure*}
