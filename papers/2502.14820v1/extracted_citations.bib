@InProceedings{10.1007/978-3-030-15719-7_14,
author="Nayeem, Mir Tafseer
and Fuad, Tanvir Ahmed
and Chali, Yllias",
editor="Azzopardi, Leif
and Stein, Benno
and Fuhr, Norbert
and Mayr, Philipp
and Hauff, Claudia
and Hiemstra, Djoerd",
title="Neural Diverse Abstractive Sentence Compression Generation",
booktitle="Advances in Information Retrieval (ECIR)",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="109--116",
url="https://link.springer.com/chapter/10.1007/978-3-030-15719-7_14",
isbn="978-3-030-15719-7"
}

@inproceedings{10.1145/3132847.3133106,
author = {Nayeem, Mir Tafseer and Chali, Yllias},
title = {Paraphrastic Fusion for Abstractive Multi-Sentence Compression Generation},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133106},
doi = {10.1145/3132847.3133106},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2223â€“2226},
numpages = {4},
keywords = {abstractive compression generation, lexical paraphrasing, multi-sentence compression, sentence fusion},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{2019TabFactA,
title={TabFact: A Large-scale Dataset for Table-based Fact Verification},
author={Wenhu Chen and Hongmin Wang and Jianshu Chen and Yunkai Zhang and Hong Wang and Shiyang Li and Xiyou Zhou and William Yang Wang},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rkeJRhNYDH}
}

@article{FUAD2019216,
title = {Neural sentence fusion for diversity driven abstractive multi-document summarization},
journal = {Computer Speech \& Language},
volume = {58},
pages = {216-230},
year = {2019},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2019.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S0885230818303449},
author = {Tanvir Ahmed Fuad and Mir Tafseer Nayeem and Asif Mahmud and Yllias Chali},
keywords = {Abstractive multi-document Summarization, Sentence fusion, Neural fusion model, Document clustering},
}

@inproceedings{chali-etal-2017-towards,
    title = "Towards Abstractive Multi-Document Summarization Using Submodular Function-Based Framework, Sentence Compression and Merging",
    author = "Chali, Yllias  and
      Tanvee, Moin  and
      Nayeem, Mir Tafseer",
    editor = "Kondrak, Greg  and
      Watanabe, Taro",
    booktitle = "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = nov,
    year = "2017",
    address = "Taipei, Taiwan",
    publisher = "Asian Federation of Natural Language Processing",
    url = "https://aclanthology.org/I17-2071/",
    pages = "418--424",
}

@inproceedings{chen2020logicalnaturallanguagegeneration,
    title = "Logical Natural Language Generation from Open-Domain Tables",
    author = "Chen, Wenhu  and
      Chen, Jianshu  and
      Su, Yu  and
      Chen, Zhiyu  and
      Wang, William Yang",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.708",
    doi = "10.18653/v1/2020.acl-main.708",
    pages = "7929--7942",
    abstract = "Neural natural language generation (NLG) models have recently shown remarkable progress in fluency and coherence. However, existing studies on neural NLG are primarily focused on surface-level realizations with limited emphasis on logical inference, an important aspect of human thinking and language. In this paper, we suggest a new NLG task where a model is tasked with generating natural language statements that can be \textit{logically entailed} by the facts in an open-domain semi-structured table. To facilitate the study of the proposed logical NLG problem, we use the existing TabFact dataset{\textasciitilde}(CITATION) featured with a wide range of logical/symbolic inferences as our testbed, and propose new automatic metrics to evaluate the fidelity of generation models w.r.t. logical inference. The new task poses challenges to the existing monotonic generation frameworks due to the mismatch between sequence order and logical order. In our experiments, we comprehensively survey different generation architectures (LSTM, Transformer, Pre-Trained LM) trained with different algorithms (RL, Adversarial Training, Coarse-to-Fine) on the dataset and made following observations: 1) Pre-Trained LM can significantly boost both the fluency and logical fidelity metrics, 2) RL and Adversarial Training are trading fluency for fidelity, 3) Coarse-to-Fine generation can help partially alleviate the fidelity issue while maintaining high language fluency. The code and data are available at \url{https://github.com/wenhuchen/LogicNLG}.",
}

@inproceedings{chen2021wikitabletlargescaledatatotextdataset,
    title = "{W}iki{T}able{T}: A Large-Scale Data-to-Text Dataset for Generating {W}ikipedia Article Sections",
    author = "Chen, Mingda  and
      Wiseman, Sam  and
      Gimpel, Kevin",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.17",
    doi = "10.18653/v1/2021.findings-acl.17",
    pages = "193--209",
}

@inproceedings{cheng-etal-2022-hitab,
    title = "{H}i{T}ab: A Hierarchical Table Dataset for Question Answering and Natural Language Generation",
    author = "Cheng, Zhoujun  and
      Dong, Haoyu  and
      Wang, Zhiruo  and
      Jia, Ran  and
      Guo, Jiaqi  and
      Gao, Yan  and
      Han, Shi  and
      Lou, Jian-Guang  and
      Zhang, Dongmei",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.78",
    doi = "10.18653/v1/2022.acl-long.78",
    pages = "1094--1110",
    abstract = "Tables are often created with hierarchies, but existing works on table reasoning mainly focus on flat tables and neglect hierarchical tables. Hierarchical tables challenge numerical reasoning by complex hierarchical indexing, as well as implicit relationships of calculation and semantics. We present a new dataset, HiTab, to study question answering (QA) and natural language generation (NLG) over hierarchical tables. HiTab is a cross-domain dataset constructed from a wealth of statistical reports and Wikipedia pages, and has unique characteristics: (1) nearly all tables are hierarchical, and (2) QA pairs are not proposed by annotators from scratch, but are revised from real and meaningful sentences authored by analysts. (3) to reveal complex numerical reasoning in statistical reports, we provide fine-grained annotations of quantity and entity alignment. Experiments suggest that this HiTab presents a strong challenge for existing baselines and a valuable benchmark for future research. Targeting hierarchical structure, we devise a hierarchy-aware logical form for symbolic reasoning over tables, which shows high effectiveness. Targeting table reasoning, we leverage entity and quantity alignment to explore partially supervised training in QA and conditional generation in NLG, and largely reduce spurious predictions in QA and produce better descriptions in NLG.",
}

@inproceedings{chowdhury-etal-2021-unsupervised,
    title = "Unsupervised Abstractive Summarization of {B}engali Text Documents",
    author = "Chowdhury, Radia Rayan  and
      Nayeem, Mir Tafseer  and
      Mim, Tahsin Tasnim  and
      Chowdhury, Md. Saifur Rahman  and
      Jannat, Taufiqul",
    editor = "Merlo, Paola  and
      Tiedemann, Jorg  and
      Tsarfaty, Reut",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.224/",
    doi = "10.18653/v1/2021.eacl-main.224",
    pages = "2612--2619",
}

@inproceedings{dang-2006-duc,
    title = "{DUC} 2005: Evaluation of Question-Focused Summarization Systems",
    author = "Dang, Hoa Trang",
    editor = "Chua, Tat-Seng  and
      Goldstein, Jade  and
      Teufel, Simone  and
      Vanderwende, Lucy",
    booktitle = "Proceedings of the Workshop on Task-Focused Summarization and Question Answering",
    month = jul,
    year = "2006",
    address = "Sydney, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W06-0707",
    pages = "48--55",
}

@inproceedings{giorgi-etal-2023-open,
    title = "Open Domain Multi-document Summarization: A Comprehensive Study of Model Brittleness under Retrieval",
    author = "Giorgi, John  and
      Soldaini, Luca  and
      Wang, Bo  and
      Bader, Gary  and
      Lo, Kyle  and
      Wang, Lucy  and
      Cohan, Arman",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.549",
    doi = "10.18653/v1/2023.findings-emnlp.549",
    pages = "8177--8199",
    abstract = "Multi-document summarization (MDS) assumes a set of topic-related documents are provided as input. In practice, this document set is not always available; it would need to be retrieved given an information need, i.e. a question or topic statement, a setting we dub {``}open-domain{'} MDS. We study this more challenging setting by formalizing the task and bootstrapping it using existing datasets, retrievers and summarizers. Via extensive automatic and human evaluation, we determine: (1) state-of-the-art summarizers suffer large reductions in performance when applied to open-domain MDS, (2) additional training in the open-domain setting can reduce this sensitivity to imperfect retrieval, and (3) summarizers are insensitive to the retrieval of duplicate documents and the order of retrieved documents, but highly sensitive to other errors, like the retrieval of irrelevant documents. Based on our results, we provide practical guidelines to enable future work on open-domain MDS, e.g. how to choose the number of retrieved documents to summarize. Our results suggest that new retrieval and summarization methods and annotated resources for training and evaluation are necessary for further progress in the open-domain setting.",
}

@inproceedings{nayeem-chali-2017-extract,
    title = "Extract with Order for Coherent Multi-Document Summarization",
    author = "Nayeem, Mir Tafseer  and
      Chali, Yllias",
    editor = "Riedl, Martin  and
      Somasundaran, Swapna  and
      Glava{\v{s}}, Goran  and
      Hovy, Eduard",
    booktitle = "Proceedings of {T}ext{G}raphs-11: the Workshop on Graph-based Methods for Natural Language Processing",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-2407/",
    doi = "10.18653/v1/W17-2407",
    pages = "51--56",
}

@inproceedings{nayeem-etal-2018-abstractive,
    title = "Abstractive Unsupervised Multi-Document Summarization using Paraphrastic Sentence Fusion",
    author = "Nayeem, Mir Tafseer  and
      Fuad, Tanvir Ahmed  and
      Chali, Yllias",
    editor = "Bender, Emily M.  and
      Derczynski, Leon  and
      Isabelle, Pierre",
    booktitle = "Proceedings of the 27th International Conference on Computational Linguistics",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/C18-1102/",
    pages = "1191--1204",
}

@book{nayeem2017methods,
  title={Methods of Sentence Extraction, Abstraction and Ordering for Automatic Text Summarization},
  author={Nayeem, Mir Tafseer},
  year={2017},
  url={https://hdl.handle.net/10133/4993},
  publisher={Universtiy of Lethbridge, Department of Mathematics and Computer Science}
}

@inproceedings{parikh2020tottocontrolledtabletotextgeneration,
    title = "{ToTTo}: A Controlled Table-To-Text Generation Dataset",
    author = "Parikh, Ankur  and
      Wang, Xuezhi  and
      Gehrmann, Sebastian  and
      Faruqui, Manaal  and
      Dhingra, Bhuwan  and
      Yang, Diyi  and
      Das, Dipanjan",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.89",
    doi = "10.18653/v1/2020.emnlp-main.89",
    pages = "1173--1186",
    abstract = "We present ToTTo, an open-domain English table-to-text dataset with over 120,000 training examples that proposes a controlled generation task: given a Wikipedia table and a set of highlighted table cells, produce a one-sentence description. To obtain generated targets that are natural but also faithful to the source table, we introduce a dataset construction process where annotators directly revise existing candidate sentences from Wikipedia. We present systematic analyses of our dataset and annotation process as well as results achieved by several state-of-the-art baselines. While usually fluent, existing methods often hallucinate phrases that are not supported by the table, suggesting that this dataset can serve as a useful research benchmark for high-precision conditional text generation.",
}

@inproceedings{wiseman2017challengesdatatodocumentgeneration,
    title = "Challenges in Data-to-Document Generation",
    author = "Wiseman, Sam  and
      Shieber, Stuart  and
      Rush, Alexander",
    editor = "Palmer, Martha  and
      Hwa, Rebecca  and
      Riedel, Sebastian",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1239",
    doi = "10.18653/v1/D17-1239",
    pages = "2253--2263",
    abstract = "Recent neural models have shown significant progress on the problem of generating short descriptive texts conditioned on a small number of database records. In this work, we suggest a slightly more difficult data-to-text generation task, and investigate how effective current approaches are on this task. In particular, we introduce a new, large-scale corpus of data records paired with descriptive documents, propose a series of extractive evaluation methods for analyzing performance, and obtain baseline results using current neural generation methods. Experiments show that these models produce fluent text, but fail to convincingly approximate human-generated documents. Moreover, even templated baselines exceed the performance of these neural models on some metrics, though copy- and reconstruction-based extensions lead to noticeable improvements.",
}

@inproceedings{zhao2023qtsummqueryfocusedsummarizationtabular,
    title = "{QTS}umm: Query-Focused Summarization over Tabular Data",
    author = "Zhao, Yilun  and
      Qi, Zhenting  and
      Nan, Linyong  and
      Mi, Boyu  and
      Liu, Yixin  and
      Zou, Weijin  and
      Han, Simeng  and
      Chen, Ruizhe  and
      Tang, Xiangru  and
      Xu, Yumo  and
      Radev, Dragomir  and
      Cohan, Arman",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.74",
    doi = "10.18653/v1/2023.emnlp-main.74",
    pages = "1157--1172",
    abstract = "People primarily consult tables to conduct data analysis or answer specific questions. Text generation systems that can provide accurate table summaries tailored to users{'} information needs can facilitate more efficient access to relevant data insights. Motivated by this, we define a new query-focused table summarization task, where text generation models have to perform human-like reasoning and analysis over the given table to generate a tailored summary. We introduce a new benchmark named QTSumm for this task, which contains 7,111 human-annotated query-summary pairs over 2,934 tables covering diverse topics. We investigate a set of strong baselines on QTSumm, including text generation, table-to-text generation, and large language models. Experimental results and manual analysis reveal that the new task presents significant challenges in table-to-text generation for future research. Moreover, we propose a new approach named ReFactor, to retrieve and reason over query-relevant information from tabular data to generate several natural language facts. Experimental results demonstrate that ReFactor can bring effective improvements to baselines by concatenating the generated facts to the model input. Our data and code are publicly available at https://github.com/yale-nlp/QTSumm.",
}

@inproceedings{zhong-etal-2021-qmsum,
    title = "{QMS}um: A New Benchmark for Query-based Multi-domain Meeting Summarization",
    author = "Zhong, Ming  and
      Yin, Da  and
      Yu, Tao  and
      Zaidi, Ahmad  and
      Mutuma, Mutethia  and
      Jha, Rahul  and
      Awadallah, Ahmed Hassan  and
      Celikyilmaz, Asli  and
      Liu, Yang  and
      Qiu, Xipeng  and
      Radev, Dragomir",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.472",
    doi = "10.18653/v1/2021.naacl-main.472",
    pages = "5905--5921",
    abstract = "Meetings are a key component of human collaboration. As increasing numbers of meetings are recorded and transcribed, meeting summaries have become essential to remind those who may or may not have attended the meetings about the key decisions made and the tasks to be completed. However, it is hard to create a single short summary that covers all the content of a long meeting involving multiple people and topics. In order to satisfy the needs of different types of users, we define a new query-based multi-domain meeting summarization task, where models have to select and summarize relevant spans of meetings in response to a query, and we introduce QMSum, a new benchmark for this task. QMSum consists of 1,808 query-summary pairs over 232 meetings in multiple domains. Besides, we investigate a locate-then-summarize method and evaluate a set of strong summarization baselines on the task. Experimental results and manual analysis reveal that QMSum presents significant challenges in long meeting summarization for future research. Dataset is available at \url{https://github.com/Yale-LILY/QMSum}.",
}

