@article{Dambrine2010,
	author = {Dambrine, Marc and Kateb, D.},
	title = {{On the ersatz material approximation in level-set methods}},
	journal = {ESAIM Control Optim. Calc. Var.},
	volume = {16},
	number = {3},
	pages = {618--634},
	year = {2010},
	doi = {10.1051/cocv/2009023}
}

@article{Belieres--Frendo2024Jul,
  title={{Volume-preserving geometric shape optimization of the Dirichlet energy using variational neural networks}},
  author={Frendo, Amaury B{\'e}li{\`e}res and Franck, Emmanuel and Michel-Dansac, Victor and Privat, Yannick},
  journal={Neural Netw.},
  pages={106957},
  year={2024},
  publisher={Elsevier}
}

@article{Alves2013Jun,
	author = {Alves, Carlos J. S. and Antunes, Pedro R. S.},
	title = {{The Method of Fundamental Solutions Applied to Some Inverse Eigenproblems}},
	journal = {SIAM J. Sci. Comput.},
	year = {2013},
	publisher = {Society for Industrial and Applied Mathematics},
	url = {https://epubs.siam.org/doi/10.1137/110860380}
}

@article{Bogosel2016Nov,
	author = {Bogosel, Beniamin},
	title = {{The method of fundamental solutions applied to boundary eigenvalue problems}},
	journal = {J. Comput. Appl. Math.},
	volume = {306},
	pages = {265--285},
	year = {2016},
	issn = {0377-0427},
	publisher = {North-Holland}
}



@article{Garcke2021Jul,
	author = {Garcke, Harald and H{\ifmmode\ddot{u}\else\"{u}\fi}ttl, Paul and Kahle, Christian and Knopf, Patrik and Laux, Tim},
	title = {{Phase-Field Methods for Spectral Shape and Topology Optimization}},
	archivePrefix={arXiv},
	year = {2021},
	eprint = {2107.03159},
	doi = {10.1051/cocv/2022090}
}

@article{Dapogny2023,
	author = {Dapogny, Charles and Feppon, Florian},
	title = {{Shape optimization using a level set based mesh evolution method: an overview and tutorial}},
	journal = {C. R. Math.},
	volume = {361},
	number = {G8},
	pages = {1267--1332},
	year = {2023},
	issn = {1778-3569}
}


@article{weinberger_isoperimetric_1956,
  title={An isoperimetric inequality for the N-dimensional free membrane problem},
  author={Weinberger, Hans F},
  journal={J. Ration. Mech. Anal.},
  volume={5},
  number={4},
  pages={633--636},
  year={1956},
  publisher={JSTOR}
}

@book{Henrot,
	author = {Henrot, Antoine},
	title = {{Extremum Problems for Eigenvalues of Elliptic Operators}},
	journal = {SpringerLink},
	isbn = {978-3-7643-7706-9},
	publisher = {Birkhäuser},
	address = {Basel, Switzerland}
}

@article{bucur_maximization_2018,
	author = {Bucur, Dorin and Henrot, Antoine},
	title = {{Maximization of the second non-trivial Neumann eigenvalue}},
	journal = {Acta Math.},
	volume = {222},
	number = {2},
	pages = {337--361},
	year = {2019},
	issn = {1871-2509},
	publisher = {International Press of Boston}
}

@article{bucur_maximization_2023,
	title = {Maximization of {Neumann} {Eigenvalues}},
	volume = {247},
	issn = {1432-0673},
	url = {https://doi.org/10.1007/s00205-023-01854-z},
	doi = {10.1007/s00205-023-01854-z},
	abstract = {This paper is motivated by the maximization of the k-th eigenvalue of the Laplace operator with Neumann boundary conditions among domains of \$\$\{\{{\textbackslash}mathbb \{R\}\}\}{\textasciicircum}N\$\$with prescribed measure. We relax the problem to the class of (possibly degenerate) densities in \$\${\textbackslash}mathbb \{R\}{\textasciicircum}N\$\$with prescribed mass and prove the existence of an optimal density. For \$\$k=1,2\$\$, the two problems are equivalent and the maximizers are known to be one and two equal balls, respectively. For \$\$k {\textbackslash}ge 3\$\$this question remains open, except in one dimension of the space, where we prove that the maximal densities correspond to a union of k equal segments. This result provides sharp upper bounds for Sturm-Liouville eigenvalues and proves the validity of the Pólya conjecture in the class of densities in \$\${\textbackslash}mathbb \{R\}\$\$. Based on the relaxed formulation, we provide numerical approximations of optimal densities for \$\$k=1, {\textbackslash}dots , 8\$\$in \$\${\textbackslash}mathbb \{R\}{\textasciicircum}2\$\$.},
	language = {en},
	number = {2},
	urldate = {2023-07-13},
	journal = {Archive for Rational Mechanics and Analysis},
	author = {Bucur, Dorin and Martinet, Eloi and Oudet, Edouard},
	year = {2023},
	pages = {19},
	file = {Full Text PDF:/home/martinet/Zotero/storage/CHIS27K5/Bucur et al. - 2023 - Maximization of Neumann Eigenvalues.pdf:application/pdf},
}

@article{martinet_numerical_2023,
  title={{Numerical optimization of Neumann eigenvalues of domains in the sphere}},
  author={Martinet, Eloi},
  journal={J. Comput. Phys.},
  volume={508},
  pages={113002},
  year={2024},
  publisher={Elsevier}
}

@article{antunes_numerical_2012,
	title = {Numerical {Optimization} of {Low} {Eigenvalues} of the {Dirichlet} and {Neumann} {Laplacians}},
	volume = {154},
	issn = {0022-3239, 1573-2878},
	language = {en},
	number = {1},
	urldate = {2021-10-19},
	journal = {J. Optim. Theory Appl.},
	author = {Antunes, Pedro R. S. and Freitas, Pedro},
	year = {2012},
	pages = {235--257}
}

@incollection{constanda_new_2020,
	address = {Cham},
	title = {New {Numerical} {Results} for the {Optimization} of {Neumann} {Eigenvalues}},
	isbn = {978-3-030-48185-8 978-3-030-48186-5},
	url = {http://link.springer.com/10.1007/978-3-030-48186-5_1},
	abstract = {We present new numerical results for shape optimization problems of interior Neumann eigenvalues. This ﬁeld is not well understood from a theoretical standpoint. The existence of shape maximizers is not proven beyond the ﬁrst two eigenvalues, so we study the problem numerically. We describe a method to compute the eigenvalues for a given shape that combines the boundary element method with an algorithm for nonlinear eigenvalues. As numerical optimization requires many such evaluations, we put a focus on the efﬁciency of the method and the implemented routine. The method is well suited for parallelization. Using the resulting fast routines and a specialized parametrization of the shapes, we found improved maximums for several eigenvalues.},
	language = {en},
	urldate = {2021-10-26},
	booktitle = {Computational and {Analytic} {Methods} in {Science} and {Engineering}},
	publisher = {Springer International Publishing},
	author = {Abele, Daniel and Kleefeld, Andreas},
	editor = {Constanda, Christian},
	year = {2020},
	doi = {10.1007/978-3-030-48186-5_1},
	pages = {1--20},
	file = {Abele et Kleefeld - 2020 - New Numerical Results for the Optimization of Neum.pdf:/home/martinet/Zotero/storage/W5YRAKYL/Abele et Kleefeld - 2020 - New Numerical Results for the Optimization of Neum.pdf:application/pdf},
}

@article{von_luxburg_consistency_2008,
	title = {Consistency of spectral clustering},
	volume = {36},
	issn = {0090-5364},
	url = {http://arxiv.org/abs/0804.0678},
	doi = {10.1214/009053607000000640},
	abstract = {Consistency is a key property of all statistical procedures analyzing randomly sampled data. Surprisingly, despite decades of work, little is known about consistency of most clustering algorithms. In this paper we investigate consistency of the popular family of spectral clustering algorithms, which clusters the data with the help of eigenvectors of graph Laplacian matrices. We develop new methods to establish that, for increasing sample size, those eigenvectors converge to the eigenvectors of certain limit operators. As a result, we can prove that one of the two major classes of spectral clustering (normalized clustering) converges under very general conditions, while the other (unnormalized clustering) is only consistent under strong additional assumptions, which are not always satisfied in real data. We conclude that our analysis provides strong evidence for the superiority of normalized spectral clustering.},
	number = {2},
	urldate = {2024-09-24},
	journal = {The Annals of Statistics},
	author = {von Luxburg, Ulrike and Belkin, Mikhail and Bousquet, Olivier},
	year = {2008},
	note = {arXiv:0804.0678 [math, stat]},
	keywords = {Mathematics - Statistics Theory, 62G20 (Primary) 05C50 (Secondary)},
	annote = {Comment: Published in at http://dx.doi.org/10.1214/009053607000000640 the Annals of Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical Statistics (http://www.imstat.org)},
	file = {arXiv Fulltext PDF:/home/martinet/Zotero/storage/H6SGSNJK/von Luxburg et al. - 2008 - Consistency of spectral clustering.pdf:application/pdf;arXiv.org Snapshot:/home/martinet/Zotero/storage/NY7WESYZ/0804.html:text/html},
}

@book{borodachov_discrete_2019,
  title={Discrete energy on rectifiable sets},
  author={Borodachov, Sergiy V and Hardin, Douglas P and Saff, Edward B},
  volume={4},
  year={2019},
  publisher={Springer}
}

@article{chandrasekhar_tounn_2021,
	title = {{TOuNN}: {Topology} {Optimization} using {Neural} {Networks}},
	volume = {63},
	issn = {1615-1488},
	shorttitle = {{TOuNN}},
	url = {https://doi.org/10.1007/s00158-020-02748-4},
	doi = {10.1007/s00158-020-02748-4},
	abstract = {Neural networks, and more broadly, machine learning techniques, have been recently exploited to accelerate topology optimization through data-driven training and image processing. In this paper, we demonstrate that one can directly execute topology optimization (TO) using neural networks (NN). The primary concept is to use the NN’s activation functions to represent the popular Solid Isotropic Material with Penalization (SIMP) density field. In other words, the density function is parameterized by the weights and bias associated with the NN, and spanned by NN’s activation functions; the density representation is thus independent of the finite element mesh. Then, by relying on the NN’s built-in backpropogation, and a conventional finite element solver, the density field is optimized. Methods to impose design and manufacturing constraints within the proposed framework are described and illustrated. A byproduct of representing the density field via activation functions is that it leads to a crisp and differentiable boundary. The proposed framework is simple to implement and is illustrated through 2D and 3D examples. Some of the unresolved challenges with the proposed framework are also summarized.},
	language = {en},
	number = {3},
	urldate = {2024-02-26},
	journal = {Structural and Multidisciplinary Optimization},
	author = {Chandrasekhar, Aaditya and Suresh, Krishnan},
	year = {2021},
	keywords = {Topology optimization, Neural networks, Machine learning},
	pages = {1135--1149},
	file = {Full Text PDF:/home/martinet/Zotero/storage/EEH4G8XU/Chandrasekhar et Suresh - 2021 - TOuNN Topology Optimization using Neural Networks.pdf:application/pdf},
}


@article{Bui2012Nov,
	author = {Bui, C. and Dapogny, C. and Frey, P.},
	title = {{An accurate anisotropic adaptation method for solving the level set advection equation}},
	journal = {Int. J. Numer. Methods Fluids},
	volume = {70},
	number = {7},
	pages = {899--922},
	year = {2012},
	issn = {0271-2091},
	publisher = {John Wiley {\&} Sons, Ltd},
	doi = {10.1002/fld.2730}
}

@article{Liu2020Jun,
	author = {Liu, Dong and Gu, Danping and Smyl, Danny and Deng, Jiansong and Du, Jiangfeng},
	title = {{B-Spline Level Set Method for Shape Reconstruction in Electrical Impedance Tomography}},
	journal = {IEEE Trans. Med. Imaging},
	volume = {39},
	number = {6},
	pages = {1917--1929},
	year = {2020},
	issn = {1558-254X},
	publisher = {See full text options at IEEE Engineering in Medicine and Biology Society},
	eprint = {31880544},
	doi = {10.1109/TMI.2019.2961938}
}
@article{wang_reaction_2022,
	title = {A reaction diffusion-based {B}-spline level set ({RDBLS}) method for structural topology optimization},
	volume = {398},
	issn = {0045-7825},
	url = {https://www.sciencedirect.com/science/article/pii/S0045782522003851},
	doi = {10.1016/j.cma.2022.115252},
	abstract = {This work uses the zero-level contour of a parameterized level set function, a linear combination of cubic B-spline basis functions, to express the structural profile in structural topology optimization. Together with mean compliance, diffusion energy is minimized under a volume constraint to control the structural complexity. The design variables, namely the coefficients of cubic B-spline basis functions, are updated by solving the reaction–diffusion equation within a finite element analysis framework. The bisectional algorithm accurately calculates the Lagrangian multiplier of the volume constraint in each iteration. In addition to expressing the optimized structure smoothly, the proposed method is highly efficient. For instance, it only takes 20 iterations to solve the cantilever and MBB beams in 2D. For 3D optimization, we obtain several elegant bridge designs using nearly one million elements, demonstrating the great potential of the proposed method for practical applications.},
	urldate = {2024-08-28},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Wang, Cong and Xie, Yi Min and Lin, Xiaoshan and Zhou, Shiwei},
	year = {2022},
	keywords = {Level set method, Topology optimization, B-spline functions, Reaction–diffusion method},
	pages = {115252},
	file = {ScienceDirect Snapshot:/home/martinet/Zotero/storage/BND3GBII/S0045782522003851.html:text/html},
}

@misc{joglekar_dmf-tonn_2023,
	title = {{DMF}-{TONN}: {Direct} {Mesh}-free {Topology} {Optimization} using {Neural} {Networks}},
	shorttitle = {{DMF}-{TONN}},
	url = {http://arxiv.org/abs/2305.04107},
	abstract = {We propose a direct mesh-free method for performing topology optimization by integrating a density field approximation neural network with a displacement field approximation neural network. We show that this direct integration approach can give comparable results to conventional topology optimization techniques, with an added advantage of enabling seamless integration with post-processing software, and a potential of topology optimization with objectives where meshing and Finite Element Analysis (FEA) may be expensive or not suitable. Our approach (DMF-TONN) takes in as inputs the boundary conditions and domain coordinates and finds the optimum density field for minimizing the loss function of compliance and volume fraction constraint violation. The mesh-free nature is enabled by a physics-informed displacement field approximation neural network to solve the linear elasticity partial differential equation and replace the FEA conventionally used for calculating the compliance. We show that using a suitable Fourier Features neural network architecture and hyperparameters, the density field approximation neural network can learn the weights to represent the optimal density field for the given domain and boundary conditions, by directly backpropagating the loss gradient through the displacement field approximation neural network, and unlike prior work there is no requirement of a sensitivity filter, optimality criterion method, or a separate training of density network in each topology optimization iteration.},
	language = {en},
	urldate = {2024-02-14},
	publisher = {arXiv},
	author = {Joglekar, Aditya and Chen, Hongrui and Kara, Levent Burak},
	year = {2023},
	note = {arXiv:2305.04107 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computational Engineering, Finance, and Science},
	file = {Joglekar et al. - 2023 - DMF-TONN Direct Mesh-free Topology Optimization u.pdf:/home/martinet/Zotero/storage/P62T9CSQ/Joglekar et al. - 2023 - DMF-TONN Direct Mesh-free Topology Optimization u.pdf:application/pdf},
}

@inproceedings{pytorch,
author = {Paszke, Adam and others},
title = {PyTorch: an imperative style, high-performance deep learning library},
year = {2019},
booktitle = {NeurIPS}
}

@article{grossmann2023physicsinformedneuralnetworksbeat,
  title={Can physics-informed neural networks beat the finite element method?},
  author={Grossmann, Tamara G and Komorowska, Urszula Julia and Latz, Jonas and Sch{\"o}nlieb, Carola-Bibiane},
  journal={IMA J. Appl. Math.},
  volume = {89},
  number = {1},
  pages = {143-174},
  year = {2024},
  publisher={Oxford University Press}
}


@article{ipopt,
  title={On the implementation of an interior-point filter line-search algorithm for large-scale nonlinear programming},
  author={W{\"a}chter, Andreas and Biegler, Lorenz T},
  journal={Math. Program.},
  volume={106},
  pages={25--57},
  year={2006},
  publisher={Springer}
}


@inproceedings{Zehnder2021Dec,
  author       = {Jonas Zehnder and
                  Yue Li and
                  Stelian Coros and
                  Bernhard Thomaszewski},
  title        = {{NTopo: Mesh-free Topology Optimization using Implicit Neural Representations}},
  booktitle    = {NeurIPS},
  year         = {2021}
}


@article{he_deep_2023,
	title = {Deep energy method in topology optimization applications},
	volume = {234},
	issn = {1619-6937},
	url = {https://doi.org/10.1007/s00707-022-03449-3},
	doi = {10.1007/s00707-022-03449-3},
	abstract = {This paper explores the possibilities of applying physics-informed neural networks (PINNs) in topology optimization (TO) by introducing a fully self-supervised TO framework based on PINNs. This framework solves the forward elasticity problem by the deep energy method (DEM). Instead of training a separate neural network to update the density distribution, we leverage the fact that the compliance minimization problem is self-adjoint to express the element sensitivity directly in terms of the displacement field from the DEM model. Thus, no additional neural network is needed for the inverse problem. The method of moving asymptotes is used as the optimizer for updating density distribution. The implementation of Neumann, Dirichlet, and periodic boundary conditions is described in the context of the DEM model. Three numerical examples are presented to demonstrate framework capabilities: (i) compliance minimization in 2D under different geometries and loading, (ii) compliance minimization in 3D, and (iii) maximization of homogenized shear modulus to design 2D metamaterial unit cells. The results show that the optimized designs from the DEM-based framework are very comparable to those generated by the finite element method and shed light on a new way of integrating PINN-based simulation methods into classical computational mechanics problems.},
	language = {en},
	number = {4},
	urldate = {2024-02-14},
	journal = {Acta Mechanica},
	author = {He, Junyan and Chadha, Charul and Kushwaha, Shashank and Koric, Seid and Abueidda, Diab and Jasiuk, Iwona},
	year = {2023},
	pages = {1365--1379},
	file = {Full Text PDF:/home/martinet/Zotero/storage/RMZWBHSD/He et al. - 2023 - Deep energy method in topology optimization applic.pdf:application/pdf},
}

@article{deng_parametric_nodate,
	author = {Deng, Hao and To, Albert C.},
	title = {{A Parametric Level Set Method for Topology Optimization Based on Deep Neural Network}},
	journal = {J. Mech. Des.},
	volume = {143},
	number = {9},
	year = {2021},
	issn = {1050-0472},
	publisher = {American Society of Mechanical Engineers Digital Collection}
}

@article{amos_input_nodate,
	title = {Input {Convex} {Neural} {Networks}},
	abstract = {This paper presents the input convex neural network architecture. These are scalar-valued (potentially deep) neural networks with constraints on the network parameters such that the output of the network is a convex function of (some of) the inputs. The networks allow for efﬁcient inference via optimization over some inputs to the network given others, and can be applied to settings including structured prediction, data imputation, reinforcement learning, and others. In this paper we lay the basic groundwork for these models, proposing methods for inference, optimization and learning, and analyze their representational power. We show that many existing neural network architectures can be made inputconvex with a minor modiﬁcation, and develop specialized optimization algorithms tailored to this setting. Finally, we highlight the performance of the methods on multi-label prediction, image completion, and reinforcement learning problems, where we show improvement over the existing state of the art in many cases.},
	language = {en},
	author = {Amos, Brandon and Xu, Lei and Kolter, J Zico},
	file = {Amos et al. - Input Convex Neural Networks.pdf:/home/martinet/Zotero/storage/ND37RWB3/Amos et al. - Input Convex Neural Networks.pdf:application/pdf},
}


@inproceedings{sitzmann_implicit_2020,
  author       = {Vincent Sitzmann and
                  others},
  title        = {{Implicit Neural Representations with Periodic Activation Functions}},
  booktitle    = {NeurIPS},
  year         = {2020}
}

@article{Cui2021Apr,
	author = {Cui, Mingtao and Luo, Chenchun and Li, Guang and Pan, Min},
	title = {{The parameterized level set method for structural topology optimization with shape sensitivity constraint factor}},
	journal = {Eng. Comput.},
	volume = {37},
	number = {2},
	pages = {855--872},
	year = {2021},
	issn = {1435-5663},
	publisher = {Springer London}
}


@article{etling2020first,
  title={First and second order shape optimization based on restricted mesh deformations},
  author={Etling, Tommy and Herzog, Roland and Loayza, Estefan{\'\i}a and Wachsmuth, Gerd},
  journal={SIAM J. Sci. Comput.},
  volume={42},
  number={2},
  pages={A1200--A1225},
  year={2020},
  publisher={SIAM}
}

@article{Jiang2020Jun,
  title={{Ghost point diffusion maps for solving elliptic PDEs on manifolds with classical boundary conditions}},
  author={Jiang, Shixiao Willing and Harlim, John},
  journal={Commun. Pure Appl. Math.},
  volume={76},
  number={2},
  pages={337--405},
  year={2023},
  publisher={Wiley Online Library}
}


@article{bogosel_optimization_2024,
	title={{Optimization of Neumann Eigenvalues under convexity and geometric constraints}}, 
    author={Beniamin Bogosel and Antoine Henrot and Marco Michetti},
    year={2024},
    journal={arXiv:2402.04117}, 
}

@misc{bogosel_numerical_2022,
	title = {Numerical shape optimization among convex sets},
	url = {http://arxiv.org/abs/2203.06981},
	abstract = {This article proposes a new discrete framework for approximating solutions to shape optimization problems under convexity constraints. The numerical method, based on the support function or the gauge function, is guaranteed to generate discrete convex shapes and is easily implementable using standard optimization software. The framework can handle various objective functions ranging from geometric quantities to functionals depending on partial diﬀerential equations. Width or diameter constraints are handled using the support function. Functionals depending on a convex body and its polar body can be handled using a uniﬁed framework.},
	language = {en},
	urldate = {2024-08-07},
	publisher = {arXiv},
	author = {Bogosel, Beniamin},
	year = {2022},
	note = {arXiv:2203.06981 [math]},
	keywords = {Mathematics - Optimization and Control, 49Q10, 52A27},
	file = {Bogosel - 2022 - Numerical shape optimization among convex sets.pdf:/home/martinet/Zotero/storage/X2ATNZDT/Bogosel - 2022 - Numerical shape optimization among convex sets.pdf:application/pdf},
}

@article{antunes_parametric_2022,
	author = {Antunes, Pedro R. S. and Bogosel, Beniamin},
	title = {{Parametric shape optimization using the support function}},
	journal = {Comput. Optim. Appl.},
	volume = {82},
	number = {1},
	pages = {107--138},
	year = {2022},
	issn = {1573-2894},
	publisher = {Springer US}
}

@article{Zhang2023Jun,
	author = {Zhang, Zeyu and Yao, Wen and Li, Yu and Zhou, Weien and Chen, Xiaoqian},
	title = {{Topology optimization via implicit neural representations}},
	journal = {Comput. Methods Appl. Mech. Eng.},
	volume = {411},
	pages = {116052},
	year = {2023},
	issn = {0045-7825},
	publisher = {North-Holland},
	doi = {10.1016/j.cma.2023.116052}
}

@article{belkin_towards_2008,
	series = {Learning {Theory} 2005},
	title = {Towards a theoretical foundation for {Laplacian}-based manifold methods},
	volume = {74},
	issn = {0022-0000},
	url = {https://www.sciencedirect.com/science/article/pii/S0022000007001274},
	doi = {10.1016/j.jcss.2007.08.006},
	abstract = {In recent years manifold methods have attracted a considerable amount of attention in machine learning. However most algorithms in that class may be termed “manifold-motivated” as they lack any explicit theoretical guarantees. In this paper we take a step towards closing the gap between theory and practice for a class of Laplacian-based manifold methods. These methods utilize the graph Laplacian associated to a data set for a variety of applications in semi-supervised learning, clustering, data representation. We show that under certain conditions the graph Laplacian of a point cloud of data samples converges to the Laplace–Beltrami operator on the underlying manifold. Theorem 3.1 contains the first result showing convergence of a random graph Laplacian to the manifold Laplacian in the context of machine learning.},
	number = {8},
	urldate = {2024-04-18},
	journal = {Journal of Computer and System Sciences},
	author = {Belkin, Mikhail and Niyogi, Partha},
	year = {2008},
	keywords = {Graph Laplacian, Laplace–Beltrami operator, Manifold methods},
	pages = {1289--1308},
}

@book{henrot_variation_2005,
	author = {Henrot, Antoine and Pierre, Michel},
	title = {{Shape Variation and Optimization}},
	year = {2018},
    publisher = {European Mathematical Society},
	issn = {2701-7826},
	isbn = {978-3-03719-178-1}

}

@misc{trillos_variational_2015,
	title = {A variational approach to the consistency of spectral clustering},
	url = {http://arxiv.org/abs/1508.01928},
	doi = {10.48550/arXiv.1508.01928},
	abstract = {This paper establishes the consistency of spectral approaches to data clustering. We consider clustering of point clouds obtained as samples of a ground-truth measure. A graph representing the point cloud is obtained by assigning weights to edges based on the distance between the points they connect. We investigate the spectral convergence of both unnormalized and normalized graph Laplacians towards the appropriate operators in the continuum domain. We obtain sharp conditions on how the connectivity radius can be scaled with respect to the number of sample points for the spectral convergence to hold. We also show that the discrete clusters obtained via spectral clustering converge towards a continuum partition of the ground truth measure. Such continuum partition minimizes a functional describing the continuum analogue of the graph-based spectral partitioning. Our approach, based on variational convergence, is general and flexible.},
	urldate = {2024-04-22},
	publisher = {arXiv},
	author = {Garc\'{i}a Trillos, Nicol\'{a}s and Slep\v{c}ev, Dejan},
	year = {2015},
	note = {arXiv:1508.01928 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Statistics Theory, 49J55, 49J45, 60D05, 68R10, 62G20},
	file = {arXiv Fulltext PDF:/home/martinet/Zotero/storage/KBSMXUBX/Trillos et Slepčev - 2015 - A variational approach to the consistency of spect.pdf:application/pdf;arXiv.org Snapshot:/home/martinet/Zotero/storage/4DV6IZQV/1508.html:text/html},
}


@article{von_luxburg_tutorial_2007,
  title={A tutorial on spectral clustering},
  author={Von Luxburg, Ulrike},
  journal={Stat. Comput.},
  volume={17},
  pages={395--416},
  year={2007},
  publisher={Springer}
}

@article{Allaire2014Dec,
	author = {Allaire, G. and Dapogny, C. and Frey, P.},
	title = {{Shape optimization with a level set based mesh evolution method}},
	journal = {Comput. Methods Appl. Mech. Eng.},
	volume = {282},
	pages = {22--53},
	year = {2014},
	issn = {0045-7825},
	publisher = {North-Holland}
}

@article{antunes_numerical_2017,
	author = {Antunes, Pedro R. S. and Oudet, {\ifmmode\acute{E}\else\'{E}\fi}douard},
	title = {{Numerical Minimization of Dirichlet Laplacian Eigenvalues of Four-Dimensional Geometries}},
	journal = {SIAM J. Sci. Comput.},
	year = {2017},
	publisher = {Society for Industrial and Applied Mathematics}
}

@article{garcia_trillos_error_2020,
	title = {Error {Estimates} for {Spectral} {Convergence} of the {Graph} {Laplacian} on {Random} {Geometric} {Graphs} {Toward} the {Laplace}–{Beltrami} {Operator}},
	volume = {20},
	issn = {1615-3383},
	url = {https://doi.org/10.1007/s10208-019-09436-w},
	doi = {10.1007/s10208-019-09436-w},
	language = {en},
	number = {4},
	urldate = {2024-04-18},
	journal = {Foundations of Computational Mathematics},
	author = {Garc\'ia Trillos, Nicol\'as and Gerlach, Moritz and Hein, Matthias and Slep\v cev, Dejan},
	year = {2020},
	keywords = {05C50, 58J50, 60D05, 62G20, 65N25, 68R10, Discrete to continuum limit, Graph Laplacian, Point cloud, Random geometric graph, Spectral clustering, Spectral convergence},
	pages = {827--887},
	file = {Full Text PDF:/home/martinet/Zotero/storage/I5MQXWGP/García Trillos et al. - 2020 - Error Estimates for Spectral Convergence of the Gr.pdf:application/pdf},
}

@article{calder_improved_2020,
  title={{Improved spectral convergence rates for graph Laplacians on $\varepsilon$-graphs and k-NN graphs}},
  author={Calder, Jeff and Garc\'ia Trillos, Nicol\'as},
  journal={Appl. Comput. Harmon. A.},
  volume={60},
  pages={123--175},
  year={2022},
  publisher={Elsevier}
}


@article{polya1948torsional,
  title={Torsional rigidity, principal frequency, electrostatic capacity and symmetrization},
  author={P{\'o}lya, George},
  journal={Quarterly of Applied Mathematics},
  volume={6},
  number={3},
  pages={267--277},
  year={1948}
}

@article{mishra2023estimates,
  title={Estimates on the generalization error of physics-informed neural networks for approximating PDEs},
  author={Mishra, Siddhartha and Molinaro, Roberto},
  journal={IMA Journal of Numerical Analysis},
  volume={43},
  number={1},
  pages={1--43},
  year={2023},
  publisher={Oxford University Press}
}


@InProceedings{anil2019sorting,
  title = 	 {Sorting Out {L}ipschitz Function Approximation},
  author =       {Anil, Cem and Lucas, James and Grosse, Roger},
  booktitle = 	 {ICML},
  year = 	 {2019}
}


@article{hasannasab2020parseval,
  title={Parseval proximal neural networks},
  author={Hasannasab, Marzieh and others},
  journal={J. Fourier Anal. Appl.},
  volume={26},
  pages={1--31},
  year={2020},
  publisher={Springer}
}

@inproceedings{zhu2003semi,
  title={{Semi-supervised learning using Gaussian fields and harmonic functions}},
  author={Zhu, Xiaojin and Ghahramani, Zoubin and Lafferty, John D},
  booktitle={ICML},
  year={2003}
}

@article{calder2023rates,
  title={{Rates of convergence for Laplacian semi-supervised learning with low labeling rates}},
  author={Calder, Jeff and Slep{\v{c}}ev, Dejan and Thorpe, Matthew},
  journal={Res. Math. Sci.},
  volume={10},
  number={1},
  pages={10},
  year={2023},
  publisher={Springer}
}

@article{calder2022lipschitz,
  title={{Lipschitz regularity of graph Laplacians on random data clouds}},
  author={Calder, Jeff and Garc\'ia Trillos, Nicol\'as and Lewicka, Marta},
  journal={SIAM J. Math. Anal.},
  volume={54},
  number={1},
  pages={1169--1222},
  year={2022},
  publisher={SIAM}
}

@incollection{allaire2021shape,
  title={Shape and topology optimization},
  author={Allaire, Gr{\'e}goire and Dapogny, Charles and Jouve, Fran{\c{c}}ois},
  booktitle={Handb. Numer. Anal.},
  volume={22},
  pages={1--132},
  year={2021},
  publisher={Elsevier}
}

@article{raissi2017physics,
      title={Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations}, 
      author={Maziar Raissi and Paris Perdikaris and George Em Karniadakis},
      year={2017},
      journal={arXiv:1711.10561}
}


@InProceedings{calder2020poisson,
  title = 	 {{Poisson Learning: Graph Based Semi-Supervised Learning At Very Low Label Rates}},
  author =       {Calder, Jeff and Cook, Brendan and Thorpe, Matthew and Slepcev, Dejan},
  booktitle = 	 {ICML},
  year = 	 {2020}
}


@article{bungert2024convergencerates,
      title={{Convergence rates for Poisson learning to a Poisson equation with measure data}}, 
      author={Leon Bungert and others},
      year={2024},
      journal={arXiv:2407.06783}, 
}

