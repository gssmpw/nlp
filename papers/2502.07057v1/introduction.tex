\section{Introduction}

Tokenization is a crucial preprocessing step in NLP, transforming raw text into smaller units such as words, subwords, or characters that serve as inputs for language models. This process directly impacts the efficiency and effectiveness of NLP tasks. While tokenization is a universal requirement across languages, its complexity increases for morphologically rich and agglutinative languages like Turkish, where words often consist of a root morpheme and multiple morphemes, each carrying distinct grammatical or semantic meaning \cite{schmidt_tokenization_2024}.

Recent research has explored various tokenization methods to address these challenges. Domingo et al. \cite{domingo2019how} and Fujii et al. \cite{fujii2023how} have highlighted the significant influence of tokenization on downstream tasks such as neural machine translation and scriptio continua languages, respectively. Theoretical studies by Guo \cite{guo1997critical} and Kudo \cite{kudo_sentencepiece_2018} examined the foundational principles of effective tokenization, providing the groundwork for widely used methods like Byte-Pair Encoding (BPE). Additionally, Zouhar et al. \cite{zouhar2023formal} emphasized the importance of balancing vocabulary size and segmentation granularity, while Berglund and van der Merwe \cite{berglund2023theoretical} proposed a framework for tokenization tailored to low-resource languages. These contributions collectively advance the understanding of tokenization's impact across diverse NLP tasks.

Tokenization challenges are amplified for non-English languages due to increased token fragmentation. Tokenizers often produce smaller chunks for non-English texts, significantly increasing token counts and bloating sequence lengths. This inefficiency wastes the context window of language models, degrading performance for non-English queries. These issues are closely tied to tokenizer design and the training data used for their development \cite{samiullah_technical_nodate}.

Recent advancements in subword tokenization techniques, such as Byte Pair Encoding (BPE) \cite{gage_new_1994} and SentencePiece \cite{kudo_sentencepiece_2018}, have demonstrated improvements in representing complex linguistic structures across languages. These methods segment words into subword units, enabling models to handle rare and unseen words more effectively. For instance, the \texttt{Aranizer-BPE-86k} tokenizer, developed for Arabic, effectively captures the morphological nuances of the language, offering insights for handling similar challenges in Turkish \cite{koubaa_githubcomriotu-labaranizer_2024}. These methods are also relevant for morphologically simpler languages, such as English, where they enhance representation efficiency, particularly in data-scarce scenarios.

\textbf{Motivational Example:} Consider the Turkish word \textit{"evlerimizden"} (\textit{"from our houses"}). A naive tokenizer might split it into segments like [\texttt{"ev"}, \texttt{"ler"}, \texttt{"imiz"}, \texttt{"den"}], which captures the morphemes roughly, while a linguistically uninformed tokenizer might produce unaligned fragments such as [\texttt{"e"}, \texttt{"vl"}, \texttt{"er"}, \texttt{"imizd"}, \texttt{"en"}], disrupting semantic coherence. While \texttt{"evler"} is a valid Turkish word and contributes to language-specific token percentages, it contains multiple meaningful morphemes and thus reduces token purity. Tokens like \texttt{"imiz"} or \texttt{"den"} are neither valid Turkish words nor linguistically pure tokens, highlighting the challenges of balancing these metrics.

Even in languages with simpler morphologies, such as English, tokenization can introduce challenges. For example, a naive tokenizer might incorrectly split the compound word \textit{"unbelievable"} into fragments like [\texttt{"un"}, \texttt{"belie"}, \texttt{"vable"}], losing semantic coherence and structure. A linguistically informed tokenizer would segment it meaningfully as [\texttt{"un"}, \texttt{"believe"}, \texttt{"able"}], preserving linguistic integrity and improving downstream performance.

Two critical metrics proposed in this study for evaluating tokenization quality are \textit{token purity} and \textit{language-specific token percentages}, offering complementary insights into the effectiveness of tokenization strategies.

\textbf{Token Purity} measures how well the generated tokens align with meaningful linguistic units, such as roots, valid morphemes, or coherent semantic segments. High token purity minimizes unnecessary fragmentation of words and ensures that the tokens retain their linguistic integrity. For instance, segmenting the Turkish word \textit{"evlerimizden"} into tokens like [\texttt{"ev"}, \texttt{"ler"}, \texttt{"imiz"}, \texttt{"den"}] achieves a certain level of purity, as it preserves the morphological components. However, tokens like [\texttt{"evler"}], while valid words in Turkish, are not considered pure because they contain multiple morphemes that could be further segmented into meaningful linguistic units. On the other hand, meaningless splits like [\texttt{"e"}, \texttt{"vl"}, \texttt{"eri"}, \texttt{"mizden"}] further reduce purity by fragmenting morphemes incoherently.

\textbf{Language-Specific Token Percentages} evaluate the proportion of generated tokens that are valid words in the target language. While token purity focuses on semantic and grammatical coherence within a word, language-specific token percentages assess whether tokens align with the vocabulary of the language, regardless of whether they represent finer linguistic units. For instance, a tokenizer that produces valid Turkish words like [\texttt{"ev"} and \texttt{"ler"}] contributes to a higher language-specific token percentage. Conversely, invalid tokens like [\texttt{"evlerd"}] or non-words like [\texttt{"imiz"}] negatively affect this metric because they are not recognized as valid words in Turkish.

These two metrics, while distinct, highlight different aspects of tokenization quality. High token purity ensures that meaningful parts of words are preserved, aiding linguistic understanding and model learning. High language-specific token percentages indicate that the tokens align with the vocabulary of the language, even if they do not capture finer-grained morphological units. However, language-specific tokens may still introduce noise by containing multiple meaningful morphemes, which could obscure semantic relationships and degrade model interpretability.

Despite advancements, achieving a balance between tokenization speed, vocabulary size, and linguistic fidelity remains challenging. Excessive fragmentation dilutes semantic meaning, while overly coarse tokenization overlooks critical linguistic details. This balance is essential for morphologically rich languages like Turkish and for optimizing performance in morphologically simpler languages \cite{neubeck_so_2024}.

This study evaluates tokenizers for Turkish using the MMLU benchmark, a widely recognized evaluation suite for language models. By analyzing tokenizers based on token purity, language-specific token percentages, vocabulary size, and processing speed, this work identifies effective approaches for Turkish NLP tasks. The findings contribute to optimized tokenization strategies that benefit diverse linguistic settings, advancing the accuracy and efficiency of NLP models.
