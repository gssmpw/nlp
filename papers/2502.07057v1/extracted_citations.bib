@article{diewald_tokenizing_2022,
	title = {Tokenizing on scale.{Preprocessing} large text corpora on the lexical and sentence level.},
	abstract = {When comparing different tools in the field of natural language processing (NLP), the quality of their results usually has first priority. This is also true for tokenization. In the context of large and diverse corpora for linguistic research purposes, however, other criteria also play a role – not least suffi cient speed to process the data in an acceptable amount of time. In this paper we evaluate several stateof theart tokenization tools for German – including our own – with regard to theses criteria. We conclude that while not all tools are applicable in this setting, no compromises regarding quality need to be made.},
	language = {en},
	author = {Diewald, Nils and Kupietz, Marc and Lüngen, Harald},
	year = {2022},
	file = {PDF:/Users/alibayram/Zotero/storage/ISJXVA4A/Diewald et al. - 2022 - Tokenizing on scale.Preprocessing large text corpora on the lexical and sentence level..pdf:application/pdf},
}

@inproceedings{erkaya_analysis_2023,
	title = {Analysis of {Subword} {Tokenization} {Approaches} for {Turkish} {Language}},
	url = {https://ieeexplore.ieee.org/document/10223973},
	doi = {10.1109/SIU59756.2023.10223973},
	abstract = {Various tokenization approaches have been proposed in natural language processing research. These approaches have further evolved from character- and word-level representations to subword-level representations. However, the impact of tokenizations on model performance has not been thoroughly discussed, especially for morphologically rich languages. In this paper, we comprehensively analyze subword tokenizers for Turkish which is a highly inflected and morphologically rich language and we propose a morphologically-based approach. Also, we examine how the tokenizer parameters like vocabulary and corpus sizes change the characteristics of tokenizers.},
	urldate = {2024-12-22},
	booktitle = {2023 31st {Signal} {Processing} and {Communications} {Applications} {Conference} ({SIU})},
	author = {Erkaya, Erencan and Güngör, Tunga},
	month = jul,
	year = {2023},
	note = {ISSN: 2165-0608},
	keywords = {Encoding, Hip, morphology, Morphology, natural language processing, notion, Signal processing, subword tokenizers, Tokenization, Transformers, Turkish, Vocabulary},
	pages = {1--4},
	file = {Full Text PDF:/Users/alibayram/Zotero/storage/EC85AC72/Erkaya and Güngör - 2023 - Analysis of Subword Tokenization Approaches for Turkish Language.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/alibayram/Zotero/storage/CSM3YXK8/10223973.html:text/html},
}

@misc{koubaa_githubcomriotu-labaranizer_2024,
	title = {github.com/riotu-lab/aranizer},
	url = {https://github.com/riotu-lab/aranizer},
	abstract = {Aranizer: A Custom Tokenizer based on SentencePiece and BPE tailored for Arabic Language Modeling},
	urldate = {2024-12-13},
	publisher = {RIOTU Lab},
	author = {Koubaa, Anis and Ghouti, Lahouari and Najar, Omar and Sebai, Serry},
	month = dec,
	year = {2024},
	note = {original-date: 2023-12-19T07:57:47Z},
}

@article{lin_not_nodate,
	title = {Not {All} {Tokens} {Are} {What} {You} {Need} for {Pretraining}},
	abstract = {Previous language model pre-training methods have uniformly applied a next-token prediction loss to all training tokens. Challenging this norm, we posit that “Not all tokens in a corpus are equally important for language model training”. Our initial analysis examines token-level training dynamics of language model, revealing distinct loss patterns for different tokens. Leveraging these insights, we introduce a new language model called RHO-1. Unlike traditional LMs that learn to predict every next token in a corpus, RHO-1 employs Selective Language Modeling (SLM), which selectively trains on useful tokens that aligned with the desired distribution. This approach involves scoring tokens using a reference model, and then training the language model with a focused loss on tokens with higher scores. When continual pretraining on 15B OpenWebMath corpus, RHO-1 yields an absolute improvement in few-shot accuracy of up to 30\% in 9 math tasks. After fine-tuning, RHO-1-1B and 7B achieved state-of-the-art results of 40.6\% and 51.8\% on MATH dataset, respectively — matching DeepSeekMath with only 3\% of the pretraining tokens. Furthermore, when continual pretraining on 80B general tokens, RHO-1 achieves 6.8\% average enhancement across 15 diverse tasks, increasing both data efficiency and performance of the language model pre-training.},
	language = {en},
	author = {Lin, Zhenghao and Gou, Zhibin and Gong, Yeyun and Liu, Xiao and Shen, Yelong and Xu, Ruochen and Lin, Chen and Yang, Yujiu and Jiao, Jian and Duan, Nan and Chen, Weizhu},
	file = {PDF:/Users/alibayram/Zotero/storage/MK468B9I/Lin et al. - Not All Tokens Are What You Need for Pretraining.pdf:application/pdf},
}

@misc{martins_eurollm_2024,
	title = {{EuroLLM}: {Multilingual} {Language} {Models} for {Europe}},
	shorttitle = {{EuroLLM}},
	url = {http://arxiv.org/abs/2409.16235},
	doi = {10.48550/arXiv.2409.16235},
	abstract = {The quality of open-weight LLMs has seen significant improvement, yet they remain predominantly focused on English. In this paper, we introduce the EuroLLM project, aimed at developing a suite of open-weight multilingual LLMs capable of understanding and generating text in all official European Union languages, as well as several additional relevant languages. We outline the progress made to date, detailing our data collection and filtering process, the development of scaling laws, the creation of our multilingual tokenizer, and the data mix and modeling configurations. Additionally, we release our initial models: EuroLLM-1.7B and EuroLLM-1.7B-Instruct and report their performance on multilingual general benchmarks and machine translation.},
	urldate = {2024-12-15},
	publisher = {arXiv},
	author = {Martins, Pedro Henrique and Fernandes, Patrick and Alves, João and Guerreiro, Nuno M. and Rei, Ricardo and Alves, Duarte M. and Pombal, José and Farajian, Amin and Faysse, Manuel and Klimaszewski, Mateusz and Colombo, Pierre and Haddow, Barry and Souza, José G. C. de and Birch, Alexandra and Martins, André F. T.},
	month = sep,
	year = {2024},
	note = {arXiv:2409.16235 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/alibayram/Zotero/storage/HQ7JNDC2/Martins et al. - 2024 - EuroLLM Multilingual Language Models for Europe.pdf:application/pdf;Snapshot:/Users/alibayram/Zotero/storage/AW3XN48Y/2409.html:text/html},
}

@misc{neubeck_so_2024,
	title = {So many tokens, so little time: {Introducing} a faster, more flexible byte-pair tokenizer},
	shorttitle = {So many tokens, so little time},
	url = {https://github.blog/ai-and-ml/llms/so-many-tokens-so-little-time-introducing-a-faster-more-flexible-byte-pair-tokenizer/},
	abstract = {We released a new open source byte-pair tokenizer that is faster and more flexible than popular alternatives.},
	language = {en-US},
	urldate = {2024-12-13},
	journal = {The GitHub Blog},
	author = {Neubeck, Alexander, Hendrik van Antwerpen},
	month = dec,
	year = {2024},
	file = {Snapshot:/Users/alibayram/Zotero/storage/YATGRA24/so-many-tokens-so-little-time-introducing-a-faster-more-flexible-byte-pair-tokenizer.html:text/html},
}

@misc{rashad_arabic_nodate,
	title = {Arabic {Tokenizers} {Leaderboard} - a {Hugging} {Face} {Space} by {MohamedRashad}},
	url = {https://huggingface.co/spaces/MohamedRashad/arabic-tokenizers-leaderboard},
	abstract = {Discover amazing ML apps made by the community},
	urldate = {2024-12-13},
	author = {Rashad, Mohamed},
	file = {Snapshot:/Users/alibayram/Zotero/storage/UF22ZSJ6/arabic-tokenizers-leaderboard.html:text/html},
}

@misc{rosa_nbailabtokenizer-benchmark_2024,
	title = {{NbAiLab}/tokenizer-benchmark},
	copyright = {Apache-2.0},
	url = {https://github.com/NbAiLab/tokenizer-benchmark},
	abstract = {Benchmark for Scandinavian Tokenizers},
	urldate = {2024-12-13},
	publisher = {Nasjonalbiblioteket AI Lab},
	author = {Rosa, Javier de la and Arild, Rolv},
	month = nov,
	year = {2024},
	note = {original-date: 2024-03-23T09:22:14Z},
}

@misc{rust_how_2021,
	title = {How {Good} is {Your} {Tokenizer}? {On} the {Monolingual} {Performance} of {Multilingual} {Language} {Models}},
	shorttitle = {How {Good} is {Your} {Tokenizer}?},
	url = {http://arxiv.org/abs/2012.15613},
	doi = {10.48550/arXiv.2012.15613},
	abstract = {In this work, we provide a systematic and comprehensive empirical comparison of pretrained multilingual language models versus their monolingual counterparts with regard to their monolingual task performance. We study a set of nine typologically diverse languages with readily available pretrained monolingual models on a set of ﬁve diverse monolingual downstream tasks. We ﬁrst aim to establish, via fair and controlled comparisons, if a gap between the multilingual and the corresponding monolingual representation model of that language exists, and subsequently investigate the reason for any performance difference. To disentangle conﬂating factors, we train new monolingual models on the same data, with monolingually and multilingually trained tokenizers. We ﬁnd that while the pretraining data size is an important factor, a designated monolingual tokenizer plays an equally important role in the downstream performance. Our results show that languages that are adequately represented in the multilingual model’s vocabulary exhibit negligible performance decreases over their monolingual counterparts. We further ﬁnd that replacing the original multilingual tokenizer with the specialized monolingual tokenizer improves the downstream performance of the multilingual model for almost every task and language.},
	language = {en},
	urldate = {2024-12-13},
	publisher = {arXiv},
	author = {Rust, Phillip and Pfeiffer, Jonas and Vulić, Ivan and Ruder, Sebastian and Gurevych, Iryna},
	month = jun,
	year = {2021},
	note = {arXiv:2012.15613 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:/Users/alibayram/Zotero/storage/KBXP9WV2/Rust et al. - 2021 - How Good is Your Tokenizer On the Monolingual Performance of Multilingual Language Models.pdf:application/pdf},
}

