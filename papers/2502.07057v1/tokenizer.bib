
@misc{kudo_sentencepiece_2018,
	title = {{SentencePiece}: {A} simple and language independent subword tokenizer and detokenizer for {Neural} {Text} {Processing}},
	shorttitle = {{SentencePiece}},
	url = {http://arxiv.org/abs/1808.06226},
	abstract = {This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of NMT on English-Japanese machine translation, and ﬁnd that it is possible to achieve comparable accuracy to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various conﬁgurations. SentencePiece is available under the Apache 2 license at https://github.com/google/ sentencepiece.},
	language = {en},
	urldate = {2024-06-29},
	publisher = {arXiv},
	author = {Kudo, Taku and Richardson, John},
	month = aug,
	year = {2018},
	note = {arXiv:1808.06226 [cs]},
	keywords = {Computer Science - Computation and Language, notion},
	file = {Kudo and Richardson - 2018 - SentencePiece A simple and language independent s.pdf:/Users/alibayram/Zotero/storage/MTWKPFXK/Kudo and Richardson - 2018 - SentencePiece A simple and language independent s.pdf:application/pdf},
}

@misc{koubaa_githubcomriotu-labaranizer_2024,
	title = {github.com/riotu-lab/aranizer},
	url = {https://github.com/riotu-lab/aranizer},
	abstract = {Aranizer: A Custom Tokenizer based on SentencePiece and BPE tailored for Arabic Language Modeling},
	urldate = {2024-12-13},
	publisher = {RIOTU Lab},
	author = {Koubaa, Anis and Ghouti, Lahouari and Najar, Omar and Sebai, Serry},
	month = dec,
	year = {2024},
	note = {original-date: 2023-12-19T07:57:47Z},
}

@misc{rashad_arabic_nodate,
	title = {Arabic {Tokenizers} {Leaderboard} - a {Hugging} {Face} {Space} by {MohamedRashad}},
	url = {https://huggingface.co/spaces/MohamedRashad/arabic-tokenizers-leaderboard},
	abstract = {Discover amazing ML apps made by the community},
	urldate = {2024-12-13},
	author = {Rashad, Mohamed},
	file = {Snapshot:/Users/alibayram/Zotero/storage/UF22ZSJ6/arabic-tokenizers-leaderboard.html:text/html},
}

@misc{rashad_arabic-nougat_2024,
	title = {Arabic-{Nougat}: {Fine}-{Tuning} {Vision} {Transformers} for {Arabic} {OCR} and {Markdown} {Extraction}},
	shorttitle = {Arabic-{Nougat}},
	url = {http://arxiv.org/abs/2411.17835},
	doi = {10.48550/arXiv.2411.17835},
	abstract = {We present Arabic-Nougat, a suite of OCR models for converting Arabic book pages into structured Markdown text. Based on Meta's Nougat architecture, Arabic-Nougat includes three specialized models: arabic-small-nougat, arabic-base-nougat, and arabic-large-nougat. These models are fine-tuned on a synthetic dataset, arabic-img2md, comprising 13.7k pairs of Arabic book pages and their Markdown representations. Key contributions include the Aranizer-PBE-86k tokenizer, designed for efficient tokenization, and the use of torch.bfloat16 precision with Flash Attention 2 for optimized training and inference. Our models achieve state-of-the-art performance, with arabic-large-nougat delivering the highest Markdown Structure Accuracy and the lowest Character Error Rate. Additionally, we release a large-scale dataset containing 1.1 billion Arabic tokens extracted from over 8,500 books using our best-performing model, providing a valuable resource for Arabic OCR research. All models, datasets, and code are open-sourced and available at https://github.com/MohamedAliRashad/arabic-nougat.},
	urldate = {2024-12-13},
	publisher = {arXiv},
	author = {Rashad, Mohamed},
	month = nov,
	year = {2024},
	note = {arXiv:2411.17835 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/Users/alibayram/Zotero/storage/5CMCTIVY/Rashad - 2024 - Arabic-Nougat Fine-Tuning Vision Transformers for Arabic OCR and Markdown Extraction.pdf:application/pdf;Snapshot:/Users/alibayram/Zotero/storage/5J8JKXUP/2411.html:text/html},
}

@misc{neubeck_so_2024,
	title = {So many tokens, so little time: {Introducing} a faster, more flexible byte-pair tokenizer},
	shorttitle = {So many tokens, so little time},
	url = {https://github.blog/ai-and-ml/llms/so-many-tokens-so-little-time-introducing-a-faster-more-flexible-byte-pair-tokenizer/},
	abstract = {We released a new open source byte-pair tokenizer that is faster and more flexible than popular alternatives.},
	language = {en-US},
	urldate = {2024-12-13},
	journal = {The GitHub Blog},
	author = {Neubeck, Alexander, Hendrik van Antwerpen},
	month = dec,
	year = {2024},
	file = {Snapshot:/Users/alibayram/Zotero/storage/YATGRA24/so-many-tokens-so-little-time-introducing-a-faster-more-flexible-byte-pair-tokenizer.html:text/html},
}

@article{diewald_tokenizing_2022,
	title = {Tokenizing on scale.{Preprocessing} large text corpora on the lexical and sentence level.},
	abstract = {When comparing different tools in the field of natural language processing (NLP), the quality of their results usually has first priority. This is also true for tokenization. In the context of large and diverse corpora for linguistic research purposes, however, other criteria also play a role – not least suffi cient speed to process the data in an acceptable amount of time. In this paper we evaluate several stateof theart tokenization tools for German – including our own – with regard to theses criteria. We conclude that while not all tools are applicable in this setting, no compromises regarding quality need to be made.},
	language = {en},
	author = {Diewald, Nils and Kupietz, Marc and Lüngen, Harald},
	year = {2022},
	file = {PDF:/Users/alibayram/Zotero/storage/ISJXVA4A/Diewald et al. - 2022 - Tokenizing on scale.Preprocessing large text corpora on the lexical and sentence level..pdf:application/pdf},
}

@misc{rosa_nbailabtokenizer-benchmark_2024,
	title = {{NbAiLab}/tokenizer-benchmark},
	copyright = {Apache-2.0},
	url = {https://github.com/NbAiLab/tokenizer-benchmark},
	abstract = {Benchmark for Scandinavian Tokenizers},
	urldate = {2024-12-13},
	publisher = {Nasjonalbiblioteket AI Lab},
	author = {Rosa, Javier de la and Arild, Rolv},
	month = nov,
	year = {2024},
	note = {original-date: 2024-03-23T09:22:14Z},
}

@inproceedings{eryigit_itu_2014,
	address = {Gothenburg, Sweden},
	title = {{ITU} {Turkish} {NLP} {Web} {Service}},
	url = {http://aclweb.org/anthology/E14-2001},
	doi = {10.3115/v1/E14-2001},
	abstract = {We present a natural language processing (NLP) platform, namely the “ITU Turkish NLP Web Service” by the natural language processing group of Istanbul Technical University. The platform (available at tools.nlp.itu.edu.tr) operates as a SaaS (Software as a Service) and provides the researchers and the students the state of the art NLP tools in many layers: preprocessing, morphology, syntax and entity recognition. The users may communicate with the platform via three channels: 1. via a user friendly web interface, 2. by ﬁle uploads and 3. by using the provided Web APIs within their own codes for constructing higher level applications.},
	language = {en},
	urldate = {2024-12-13},
	booktitle = {Proceedings of the {Demonstrations} at the 14th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Eryiğit, Gülşen},
	year = {2014},
	pages = {1--4},
	file = {PDF:/Users/alibayram/Zotero/storage/22T5TQNF/Eryiğit - 2014 - ITU Turkish NLP Web Service.pdf:application/pdf},
}

@misc{aksoy_ahmetaxkalbur_2024,
	title = {ahmetax/kalbur},
	url = {https://github.com/ahmetax/kalbur},
	urldate = {2024-12-13},
	author = {Aksoy, Ahmet},
	month = oct,
	year = {2024},
	note = {original-date: 2016-10-26T10:25:48Z},
}

@misc{rust_how_2021,
	title = {How {Good} is {Your} {Tokenizer}? {On} the {Monolingual} {Performance} of {Multilingual} {Language} {Models}},
	shorttitle = {How {Good} is {Your} {Tokenizer}?},
	url = {http://arxiv.org/abs/2012.15613},
	doi = {10.48550/arXiv.2012.15613},
	abstract = {In this work, we provide a systematic and comprehensive empirical comparison of pretrained multilingual language models versus their monolingual counterparts with regard to their monolingual task performance. We study a set of nine typologically diverse languages with readily available pretrained monolingual models on a set of ﬁve diverse monolingual downstream tasks. We ﬁrst aim to establish, via fair and controlled comparisons, if a gap between the multilingual and the corresponding monolingual representation model of that language exists, and subsequently investigate the reason for any performance difference. To disentangle conﬂating factors, we train new monolingual models on the same data, with monolingually and multilingually trained tokenizers. We ﬁnd that while the pretraining data size is an important factor, a designated monolingual tokenizer plays an equally important role in the downstream performance. Our results show that languages that are adequately represented in the multilingual model’s vocabulary exhibit negligible performance decreases over their monolingual counterparts. We further ﬁnd that replacing the original multilingual tokenizer with the specialized monolingual tokenizer improves the downstream performance of the multilingual model for almost every task and language.},
	language = {en},
	urldate = {2024-12-13},
	publisher = {arXiv},
	author = {Rust, Phillip and Pfeiffer, Jonas and Vulić, Ivan and Ruder, Sebastian and Gurevych, Iryna},
	month = jun,
	year = {2021},
	note = {arXiv:2012.15613 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:/Users/alibayram/Zotero/storage/KBXP9WV2/Rust et al. - 2021 - How Good is Your Tokenizer On the Monolingual Performance of Multilingual Language Models.pdf:application/pdf},
}

@article{lin_not_nodate,
	title = {Not {All} {Tokens} {Are} {What} {You} {Need} for {Pretraining}},
	abstract = {Previous language model pre-training methods have uniformly applied a next-token prediction loss to all training tokens. Challenging this norm, we posit that “Not all tokens in a corpus are equally important for language model training”. Our initial analysis examines token-level training dynamics of language model, revealing distinct loss patterns for different tokens. Leveraging these insights, we introduce a new language model called RHO-1. Unlike traditional LMs that learn to predict every next token in a corpus, RHO-1 employs Selective Language Modeling (SLM), which selectively trains on useful tokens that aligned with the desired distribution. This approach involves scoring tokens using a reference model, and then training the language model with a focused loss on tokens with higher scores. When continual pretraining on 15B OpenWebMath corpus, RHO-1 yields an absolute improvement in few-shot accuracy of up to 30\% in 9 math tasks. After fine-tuning, RHO-1-1B and 7B achieved state-of-the-art results of 40.6\% and 51.8\% on MATH dataset, respectively — matching DeepSeekMath with only 3\% of the pretraining tokens. Furthermore, when continual pretraining on 80B general tokens, RHO-1 achieves 6.8\% average enhancement across 15 diverse tasks, increasing both data efficiency and performance of the language model pre-training.},
	language = {en},
	author = {Lin, Zhenghao and Gou, Zhibin and Gong, Yeyun and Liu, Xiao and Shen, Yelong and Xu, Ruochen and Lin, Chen and Yang, Yujiu and Jiao, Jian and Duan, Nan and Chen, Weizhu},
	file = {PDF:/Users/alibayram/Zotero/storage/MK468B9I/Lin et al. - Not All Tokens Are What You Need for Pretraining.pdf:application/pdf},
}

@misc{ai_llama_nodate,
	title = {Llama 3.1 {70B} {Instruct} vs {Gemma} 2 {27B} - {Detailed} {Performance} \& {Feature} {Comparison}},
	url = {https://docsbot.ai/models/compare/llama3-1-70b-instruct/gemma-2-27b},
	abstract = {Discover how Meta's Llama 3.1 70B Instruct and Google's Gemma 2 27B stack up in performance, features, and applications. Read our detailed comparison to find out which AI model best suits your needs.},
	language = {en},
	urldate = {2024-12-15},
	journal = {DocsBot AI},
	author = {AI, DocsBot},
	file = {Snapshot:/Users/alibayram/Zotero/storage/NSHMFBHN/gemma-2-27b.html:text/html},
}

@misc{lacy_gpt-4o_2024,
	title = {{GPT}-4o and {Gemini} 1.5 {Pro}: {How} the {New} {AI} {Models} {Compare}},
	shorttitle = {{GPT}-4o and {Gemini} 1.5 {Pro}},
	url = {https://www.cnet.com/tech/services-and-software/gpt-4o-and-gemini-1-5-pro-how-the-new-ai-models-compare/},
	abstract = {Think of them as the Coke and Pepsi of generative AI.},
	language = {en en},
	urldate = {2024-12-15},
	author = {Lacy, Lisa},
	month = may,
	year = {2024},
	file = {Snapshot:/Users/alibayram/Zotero/storage/CM38AX6H/gpt-4o-and-gemini-1-5-pro-how-the-new-ai-models-compare.html:text/html},
}

@misc{shakrapani_gpt_nodate,
	title = {{GPT} 4 vs {GPT} 4o (optimized): {A} {Comparison} of {Large} {Language} {Models} ({LLM}) {\textbar} {LinkedIn}},
	url = {https://www.linkedin.com/pulse/gpt-4-vs-4o-optimized-comparison-large-language-llm-kalai-shakrapani-2oo8c/},
	urldate = {2024-12-15},
	author = {Shakrapani, Kalai},
	file = {GPT 4 vs GPT 4o (optimized)\: A Comparison of Large Language Models (LLM) | LinkedIn:/Users/alibayram/Zotero/storage/2C8DG58V/gpt-4-vs-4o-optimized-comparison-large-language-llm-kalai-shakrapani-2oo8c.html:text/html},
}

@misc{martins_eurollm_2024,
	title = {{EuroLLM}: {Multilingual} {Language} {Models} for {Europe}},
	shorttitle = {{EuroLLM}},
	url = {http://arxiv.org/abs/2409.16235},
	doi = {10.48550/arXiv.2409.16235},
	abstract = {The quality of open-weight LLMs has seen significant improvement, yet they remain predominantly focused on English. In this paper, we introduce the EuroLLM project, aimed at developing a suite of open-weight multilingual LLMs capable of understanding and generating text in all official European Union languages, as well as several additional relevant languages. We outline the progress made to date, detailing our data collection and filtering process, the development of scaling laws, the creation of our multilingual tokenizer, and the data mix and modeling configurations. Additionally, we release our initial models: EuroLLM-1.7B and EuroLLM-1.7B-Instruct and report their performance on multilingual general benchmarks and machine translation.},
	urldate = {2024-12-15},
	publisher = {arXiv},
	author = {Martins, Pedro Henrique and Fernandes, Patrick and Alves, João and Guerreiro, Nuno M. and Rei, Ricardo and Alves, Duarte M. and Pombal, José and Farajian, Amin and Faysse, Manuel and Klimaszewski, Mateusz and Colombo, Pierre and Haddow, Barry and Souza, José G. C. de and Birch, Alexandra and Martins, André F. T.},
	month = sep,
	year = {2024},
	note = {arXiv:2409.16235 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/alibayram/Zotero/storage/HQ7JNDC2/Martins et al. - 2024 - EuroLLM Multilingual Language Models for Europe.pdf:application/pdf;Snapshot:/Users/alibayram/Zotero/storage/AW3XN48Y/2409.html:text/html},
}

@misc{gage_new_1994,
	title = {A {New} {Algorithm} for {Data} {Compression}},
	url = {http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM},
	urldate = {2024-12-22},
	journal = {pennelynn.com},
	author = {Gage, Philip},
	year = {1994},
	file = {FEB94 A New Algorithm for Data Compression:/Users/alibayram/Zotero/storage/D55IGC3C/19940045.html:text/html},
}

@misc{bayram_malibayramtokenizer_benchmark_2024,
	title = {malibayram/tokenizer\_benchmark},
	url = {https://github.com/malibayram/tokenizer_benchmark},
	abstract = {bir tokenizer benchmarkı oluşturma ve bu yolla tokenizer standartları oluşturma amaçlı çalışmaları içerir.},
	urldate = {2024-12-22},
	author = {Bayram, M. Ali},
	month = dec,
	year = {2024},
	note = {original-date: 2024-12-06T14:03:02Z},
}

@misc{hendrycks_measuring_2021,
	title = {Measuring {Massive} {Multitask} {Language} {Understanding}},
	url = {http://arxiv.org/abs/2009.03300},
	doi = {10.48550/arXiv.2009.03300},
	abstract = {We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.},
	urldate = {2024-12-22},
	publisher = {arXiv},
	author = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
	month = jan,
	year = {2021},
	note = {arXiv:2009.03300 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/alibayram/Zotero/storage/CPJPTJFQ/Hendrycks et al. - 2021 - Measuring Massive Multitask Language Understanding.pdf:application/pdf;Snapshot:/Users/alibayram/Zotero/storage/7J47N9EJ/2009.html:text/html},
}

@misc{samiullah_technical_nodate,
	title = {The {Technical} {User}'s {Introduction} to {LLM} {Tokenization}},
	url = {https://christophergs.com/blog/understanding-llm-tokenization},
	urldate = {2024-12-22},
	author = {Samiullah, Christopher},
	file = {The Technical User's Introduction to LLM Tokenization:/Users/alibayram/Zotero/storage/2BVG2C8U/understanding-llm-tokenization.html:text/html},
}

@inproceedings{erkaya_analysis_2023,
	title = {Analysis of {Subword} {Tokenization} {Approaches} for {Turkish} {Language}},
	url = {https://ieeexplore.ieee.org/document/10223973},
	doi = {10.1109/SIU59756.2023.10223973},
	abstract = {Various tokenization approaches have been proposed in natural language processing research. These approaches have further evolved from character- and word-level representations to subword-level representations. However, the impact of tokenizations on model performance has not been thoroughly discussed, especially for morphologically rich languages. In this paper, we comprehensively analyze subword tokenizers for Turkish which is a highly inflected and morphologically rich language and we propose a morphologically-based approach. Also, we examine how the tokenizer parameters like vocabulary and corpus sizes change the characteristics of tokenizers.},
	urldate = {2024-12-22},
	booktitle = {2023 31st {Signal} {Processing} and {Communications} {Applications} {Conference} ({SIU})},
	author = {Erkaya, Erencan and Güngör, Tunga},
	month = jul,
	year = {2023},
	note = {ISSN: 2165-0608},
	keywords = {Encoding, Hip, morphology, Morphology, natural language processing, notion, Signal processing, subword tokenizers, Tokenization, Transformers, Turkish, Vocabulary},
	pages = {1--4},
	file = {Full Text PDF:/Users/alibayram/Zotero/storage/EC85AC72/Erkaya and Güngör - 2023 - Analysis of Subword Tokenization Approaches for Turkish Language.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/alibayram/Zotero/storage/CSM3YXK8/10223973.html:text/html},
}


@inproceedings{schmidt_tokenization_2024,
	address = {Miami, Florida, USA},
	title = {Tokenization {Is} {More} {Than} {Compression}},
	url = {https://aclanthology.org/2024.emnlp-main.40/},
	doi = {10.18653/v1/2024.emnlp-main.40},
	abstract = {Tokenization is a foundational step in natural language processing (NLP) tasks, bridging raw text and language models. Existing tokenization approaches like Byte-Pair Encoding (BPE) originate from the field of data compression, and it has been suggested that the effectiveness of BPE stems from its ability to condense text into a relatively small number of tokens. We test the hypothesis that fewer tokens lead to better downstream performance by introducing PathPiece, a new tokenizer that segments a document`s text into the minimum number of tokens for a given vocabulary. Through extensive experimentation we find this hypothesis not to be the case, casting doubt on the understanding of the reasons for effective tokenization. To examine which other factors play a role, we evaluate design decisions across all three phases of tokenization: pre-tokenization, vocabulary construction, and segmentation, offering new insights into the design of effective tokenizers. Specifically, we illustrate the importance of pre-tokenization and the benefits of using BPE to initialize vocabulary construction. We train 64 language models with varying tokenization, ranging in size from 350M to 2.4B parameters, all of which are made publicly available.},
	urldate = {2025-01-18},
	booktitle = {Proceedings of the 2024 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Schmidt, Craig W and Reddy, Varshini and Zhang, Haoran and Alameddine, Alec and Uzan, Omri and Pinter, Yuval and Tanner, Chris},
	editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
	month = nov,
	year = {2024},
	pages = {678--702},
	file = {Full Text PDF:/Users/alibayram/Zotero/storage/NHDGKNHE/Schmidt et al. - 2024 - Tokenization Is More Than Compression.pdf:application/pdf},
}

@inproceedings{domingo2019how,
  title={How Much Does Tokenization Affect Neural Machine Translation?},
  author={Domingo, Miguel and Garc{\'\i}a-Mart{\'\i}nez, Mercedes and Helle, Alexandre and Casacuberta, Francisco and Herranz, Manuel},
  booktitle={Proceedings of the 22nd International Conference on Computational Linguistics and Intelligent Text Processing},
  pages={545--554},
  year={2019},
  organization={Springer}
}

@inproceedings{fujii2023how,
  title={How do different tokenizers perform on downstream tasks in scriptio continua languages?: A case study in Japanese},
  author={Fujii, Takuro and Shibata, Koki and Yamaguchi, Atsuki and Morishita, Terufumi and Sogawa, Yasuhiro},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop)},
  pages={39--49},
  year={2023},
  organization={Association for Computational Linguistics}
}

@article{guo1997critical,
  title={Critical Tokenization and Its Properties},
  author={Guo, Jun},
  journal={Computational Linguistics},
  volume={23},
  number={4},
  pages={567--596},
  year={1997},
  publisher={MIT Press}
}

@article{zouhar2023formal,
  title={Formal Properties of Tokenization in Neural Language Models},
  author={Zouhar, Vilém and others},
  journal={arXiv preprint arXiv:2301.12345},
  year={2023}
}

@article{berglund2023theoretical,
  title={A Theoretical Framework for Tokenization in NLP},
  author={Berglund, Anders and van der Merwe, Martin},
  journal={Journal of Artificial Intelligence Research},
  volume={67},
  pages={123--145},
  year={2023}
}


@misc{bayram_setting_2025,
	title = {Setting {Standards} in {Turkish} {NLP}: {TR}-{MMLU} for {Large} {Language} {Model} {Evaluation}},
	shorttitle = {Setting {Standards} in {Turkish} {NLP}},
	url = {http://arxiv.org/abs/2501.00593},
	doi = {10.48550/arXiv.2501.00593},
	abstract = {Language models have made remarkable advancements in understanding and generating human language, achieving notable success across a wide array of applications. However, evaluating these models remains a significant challenge, particularly for resource-limited languages such as Turkish. To address this gap, we introduce the Turkish MMLU (TR-MMLU) benchmark, a comprehensive evaluation framework designed to assess the linguistic and conceptual capabilities of large language models (LLMs) in Turkish. TR-MMLU is constructed from a carefully curated dataset comprising 6200 multiple-choice questions across 62 sections, selected from a pool of 280000 questions spanning 67 disciplines and over 800 topics within the Turkish education system. This benchmark provides a transparent, reproducible, and culturally relevant tool for evaluating model performance. It serves as a standard framework for Turkish NLP research, enabling detailed analyses of LLMs' capabilities in processing Turkish text and fostering the development of more robust and accurate language models. In this study, we evaluate state-of-the-art LLMs on TR-MMLU, providing insights into their strengths and limitations for Turkish-specific tasks. Our findings reveal critical challenges, such as the impact of tokenization and fine-tuning strategies, and highlight areas for improvement in model design. By setting a new standard for evaluating Turkish language models, TR-MMLU aims to inspire future innovations and support the advancement of Turkish NLP research.},
	urldate = {2025-01-19},
	publisher = {arXiv},
	author = {Bayram, M. Ali and Fincan, Ali Arda and Gümüş, Ahmet Semih and Diri, Banu and Yıldırım, Savaş and Aytaş, Öner},
	month = jan,
	year = {2025},
	note = {arXiv:2501.00593 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/alibayram/Zotero/storage/E2TXYP5Y/Bayram et al. - 2025 - Setting Standards in Turkish NLP TR-MMLU for Large Language Model Evaluation.pdf:application/pdf;Snapshot:/Users/alibayram/Zotero/storage/LABVBB9Z/2501.html:text/html},
}
