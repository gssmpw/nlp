\section{Related Work}
Tokenization plays a fundamental role in (NLP), directly influencing the performance, efficiency, and accuracy of (LLMs). Recent research has explored various tokenization strategies and their downstream impacts, aiming to balance linguistic fidelity, computational efficiency, and model scalability.

The \textit{Arabic Tokenizers Leaderboard} ____ benchmarks tokenizers for Arabic, using datasets such as \texttt{rasaif-translations} and \texttt{Moroccan Arabic Wikipedia}, highlighting the unique challenges posed by Arabic's diverse dialects and orthographic complexity. Tools like \textit{AraNizer} ____ leverage subword-based techniques, such as Byte Pair Encoding (BPE) and SentencePiece, to better capture the morphological nuances of Arabic and enhance downstream performance.

Similarly, the \textit{NbAiLab Tokenizer Benchmark} ____ evaluates tokenization strategies for Scandinavian languages, emphasizing the critical need for language-specific adaptations in multilingual contexts. For German, Diewald et al. ____ assessed tokenizers like \texttt{KorAP-Tokenizer} and \texttt{SoMaJo}, achieving high accuracy in token boundary detection while ensuring computational efficiency for large-scale corpora.

Erkaya ____ provides a comprehensive analysis of subword tokenization methods, particularly focusing on their application to morphologically rich languages like Turkish. Erkaya evaluates the impact of corpus size and vocabulary size on tokenization characteristics, highlighting that larger corpora improve morphology encoding. The study also introduces a morphologically optimized tokenizer that improves downstream performance on tasks such as named-entity recognition, parts-of-speech tagging, question answering, and sentiment analysis. This work emphasizes the significance of incorporating morphological supervision into tokenization for languages with agglutinative structures.

A significant contribution to multilingual tokenization comes from the EuroLLM team, which emphasizes the importance of designing tokenizers with large vocabularies to support diverse linguistic structures ____. By employing a Byte Pair Encoding (BPE) tokenizer with byte fallback and a vocabulary of 128,000 pieces, EuroLLM achieves a balance between low fertility (tokens per word) and parameter efficiency. Their findings indicate that vocabulary size is a critical factor in determining a tokenizer’s ability to efficiently process multiple languages, including European and non-European ones. EuroLLM's comparison of fertility metrics across tokenizers, such as those from Mistral, LLaMA-3, and Gemma, further underscores the trade-offs between large vocabularies and computational cost.

Efficiency advancements have also been demonstrated in GitHub's faster BPE implementation ____, which significantly improves scalability for tasks requiring billions of tokens. This aligns with EuroLLM’s approach of optimizing tokenization to enhance downstream task performance while maintaining computational efficiency.

Rust et al. ____ highlight the effectiveness of monolingual tokenizers tailored to specific languages, showing notable downstream improvements for morphologically rich languages. Similarly, Lin et al. ____ propose Selective Language Modeling (SLM), which assigns utility scores to tokens and selectively trains on high-utility tokens, reducing noise and enhancing training efficiency. This approach is particularly relevant for languages like Turkish, where preserving meaningful tokens is essential for capturing linguistic richness.

The studies discussed collectively emphasize the necessity of tokenization strategies that balance linguistic integrity, computational efficiency, and downstream performance. Building on these advancements, this study evaluates tokenizers for Turkish, employing metrics such as token purity, vocabulary size, and processing speed. By integrating insights from multilingual projects like EuroLLM and tailoring techniques for morphologically rich languages, this work advances the understanding and optimization of tokenization for diverse linguistic contexts.