\section{Methodology}

This study evaluates tokenization strategies for morphologically rich and agglutinative languages, with Turkish chosen as a representative case. While Turkish serves as the primary focus, the methodology is designed to be flexible and adaptable to other languages with similar tokenization challenges, such as Finnish, Hungarian, and Uyghur.

The evaluation employs the Turkish MMLU (TR-MMLU) dataset \cite{bayram_setting_2025}, a meticulously designed benchmark for evaluating the linguistic and conceptual capabilities of large language models (LLMs) in Turkish. The dataset comprises 6,200 multiple-choice questions across 62 sections, drawn from a pool of 280,000 questions that span 67 disciplines and over 800 topics within the Turkish education system. TR-MMLU covers diverse subject areas, including law, healthcare, history, and natural sciences, ensuring a comprehensive representation of Turkish linguistic structures. This benchmark provides a culturally and linguistically relevant framework that avoids translation-related errors and reflects the unique morphological and syntactic complexities of Turkish.

The questions in TR-MMLU were sourced from standardized exams such as the University Entrance Examination and the Open Education Faculty (AUZEF) exams, which are designed to evaluate not only factual recall but also conceptual understanding, logical reasoning, and contextual knowledge. For example, a typical question might ask: \textit{"Hangi organ karaciğerin görevini destekler?"} (\textit{"Which organ supports the function of the liver?"}) with options like (A) \textit{Kalp} (\textit{Heart}), (B) \textit{Akciğer} (\textit{Lung}), (C) \textit{Böbrek} (\textit{Kidney}), and (D) \textit{Dalak} (\textit{Spleen}). Such questions test the model's ability to process Turkish text holistically, including its semantic, syntactic, and contextual dimensions.

This dataset is natively crafted by experts in the Turkish education system, ensuring alignment with cultural and linguistic norms while eliminating errors typically introduced through translation-based benchmarks. Additionally, TR-MMLU excludes questions likely to overlap with pretraining datasets, providing an unbiased evaluation of model performance. The inclusion of questions that require nuanced understanding and reasoning makes TR-MMLU particularly effective for evaluating linguistic alignment and conceptual comprehension.

The same framework can be extended to other languages by adapting analogous linguistic resources and educational content. For example, languages like Finnish, Hungarian, or Uyghur, which share morphological richness and agglutinative structures with Turkish, can benefit from a similar approach. The flexibility of the methodology allows researchers to evaluate tokenization strategies and LLM performance in a manner that respects the linguistic intricacies of different languages.

Several metrics are used to evaluate both computational and linguistic aspects of tokenization.

\textbf{Vocabulary Size:}  
Vocabulary size represents the total number of unique tokens, such as words, subwords, or characters, that a tokenizer can produce. For instance, a tokenizer with a vocabulary size of 50,000 might include tokens like \texttt{"run"}, \texttt{"ning"}, \texttt{"play"}, and \texttt{"ful"} to represent words such as \texttt{"running"}, \texttt{"playful"}, or \texttt{"played"}. A tokenizer with a smaller vocabulary might split \texttt{"unbelievable"} into [\texttt{"un"}, \texttt{"belie"}, \texttt{"vable"}], while a larger vocabulary might tokenize it as [\texttt{"un"}, \texttt{"believable"}], preserving linguistic coherence. Larger vocabularies allow for capturing longer word sequences, but they also increase memory usage and computational complexity. Conversely, smaller vocabularies may lead to excessive fragmentation, reducing linguistic and semantic interpretability.

\textbf{Total Token Count:}  
Total token count measures the number of tokens generated when a tokenizer processes a dataset. Consider the Turkish sentence \texttt{"Çocuklar bahçede oynayacak ve bahçede gülecek"} (\texttt{"The children will play in the garden and will laugh in the garden"}). A space-based tokenizer would yield eight tokens: [\texttt{"Çocuklar"}, \texttt{"bahçede"}, \texttt{"oynayacak"}, \texttt{"ve"}, \texttt{"bahçede"}, \texttt{"gülecek"}]. However, a subword-based tokenizer might generate: [\texttt{"Çocuk"}, \texttt{"lar"}, \texttt{"bahçe"}, \texttt{"de"}, \texttt{"oyna"}, \texttt{"yacak"}, \texttt{"ve"}, \texttt{"bahçe"}, \texttt{"de"}, \texttt{"gül"}, \texttt{"ecek"}]. This illustrates the effects of repeated tokens (\texttt{"bahçe"} and \texttt{"de"}) on total token count. Lower total token counts can improve computational efficiency but may overlook detailed morphological structures.

\textbf{Processing Time:}  
Processing time, measured in seconds, indicates the computational efficiency of the tokenizer. For example, tokenizing a dataset with one million words might take 2.5 seconds. For a Turkish example, processing the sentence \texttt{"Çocuklar bahçede oynayacak ve bahçede gülecek"} with a simple tokenizer may take 0.1 seconds, while a more complex tokenizer accounting for subword or morphological structures might take 0.3 seconds. Faster tokenization is crucial for real-time applications, whereas computationally intensive tokenization methods may better preserve linguistic nuances.

\textbf{Language-Specific Token Percentages (\%TR):}  
This metric evaluates the proportion of unique tokens that are valid words in the target language. It reflects the tokenizer's ability to produce linguistically valid tokens, regardless of whether they can be further decomposed. For instance, the Turkish sentence \texttt{"Çocuklar bahçede oynayacak ve bahçede gülecek"} might be tokenized as [\texttt{"Çocuklar"}, \texttt{"bahçe"}, \texttt{"de"}, \texttt{"oynayacak"}, \texttt{"ve"}, \texttt{"gül"}, \texttt{"ecek"}]. Here, \texttt{"Çocuklar"}, \texttt{"bahçe"}, , \texttt{"oynayacak"}, \texttt{"ve"}, and \texttt{"gül"} are valid Turkish words, contributing to a higher \%TR, while \texttt{"de"} and \texttt{"ecek"} are not standalone Turkish words and do not contribute. If five out of seven unique tokens are valid Turkish words, \%TR is calculated as:

\begin{equation}
\%TR = \frac{\text{Valid Unique Tokens}}{\text{Unique Tokens}} \times 100
\label{eq:tr_percentage}
\end{equation}

Substituting the values yields:
\[
\%TR = \frac{5}{7} \times 100 = 71.4\%.
\]

This metric ensures that the tokenizer aligns with the vocabulary of the language but does not evaluate token granularity or linguistic purity.

\textbf{Pure Token Percentage (\%Pure):}  
This metric measures the proportion of unique tokens that are semantically pure, meaning they cannot be further decomposed into smaller meaningful linguistic units. Using the same sentence \texttt{"Çocuklar bahçede oynayacak ve bahçede gülecek"}, the tokenizer might generate unique tokens such as [\texttt{"Çocuklar"}, \texttt{"bahçe"}, \texttt{"de"}, \texttt{"oynayacak"}, \texttt{"ve"}, \texttt{"gül"}, \texttt{"ecek"}]. Among these, \texttt{"bahçe"}, \texttt{"gül"}, and \texttt{"ve"} are considered pure because they represent atomic linguistic units in Turkish. In contrast, \texttt{"Çocuklar"} and \texttt{"oynayacak"} are not pure, as they combine multiple morphemes. If three out of 7 unique tokens are pure, \%Pure is calculated as:

\begin{equation}
\%Pure = \frac{\text{Pure Unique Tokens}}{\text{Unique Tokens}} \times 100
\label{eq:pure_percentage}
\end{equation}

Substituting the values:
\[
\%Pure = \frac{3}{7} \times 100 \approx 42.9\%.
\]

While \%TR evaluates a tokenizer's ability to generate valid words, \%Pure emphasizes preserving linguistic granularity and atomicity. A tokenizer achieving high \%TR by generating valid tokens like \texttt{"Çocuklar"} might sacrifice \%Pure if these tokens combine multiple linguistic units.

By evaluating these metrics together, this study provides a comprehensive framework for assessing tokenization strategies. High \%TR values indicate alignment with the target language’s vocabulary, while high \%Pure values reflect the semantic and grammatical integrity of the tokens. Balancing these metrics is crucial for morphologically rich languages, where both linguistic alignment and token granularity significantly impact downstream NLP tasks.

These metrics, defined in Equations \ref{eq:tr_percentage} and \ref{eq:pure_percentage}, allow for a detailed quantitative evaluation of tokenization quality, highlighting trade-offs between computational efficiency, linguistic alignment, and semantic granularity.

Morphological analysis and token validation are performed using language-specific tools, including the ITU Turkish NLP Web Service \cite{eryigit_itu_2014} and the Kalbur library \cite{aksoy_ahmetaxkalbur_2024}. These tools identify valid roots, morphemes, and semantically coherent units, ensuring precise evaluations of tokenization quality. For other languages, similar linguistic analyzers and rule-based systems can be employed.

Computational metrics, such as processing time and token counts, are derived using Python scripts and the Hugging Face Tokenizers library \cite{neubeck_so_2024}. To ensure reproducibility, all datasets, scripts, and configurations are documented and made publicly accessible in a GitHub repository \cite{bayram_malibayramtokenizer_benchmark_2024}. This repository includes Python scripts, experimental details, and resources necessary for replicating the analyses. While Turkish is the benchmark language, the framework is adaptable to other languages and datasets, offering a versatile and scalable approach for evaluating tokenization strategies across diverse linguistic settings.