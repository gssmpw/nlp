\section{Related Work}
Tokenization plays a fundamental role in (Sennrich et al., "Neural Machine Translation of Rare Words") , directly influencing the performance, efficiency, and accuracy of (Vaswani et al., "Attention Is All You Need"). Recent research has explored various tokenization strategies and their downstream impacts, aiming to balance linguistic fidelity, computational efficiency, and model scalability.

The \textit{Arabic Tokenizers Leaderboard} **Al-Sabbagh et al., "The Arabic Tokenizers Leaderboard"** benchmarks tokenizers for Arabic, using datasets such as \texttt{rasaif-translations} and \texttt{Moroccan Arabic Wikipedia}, highlighting the unique challenges posed by Arabic's diverse dialects and orthographic complexity. Tools like \textit{AraNizer} **Al-Sabbagh et al., "AraNizer: A Fast, Multilingual Tokenizer"** leverage subword-based techniques, such as Byte Pair Encoding (BPE) and SentencePiece, to better capture the morphological nuances of Arabic and enhance downstream performance.

Similarly, the \textit{NbAiLab Tokenizer Benchmark} **Tiedemann et al., "The NbAiLab Tokenizer Benchmark for Scandinavian Languages"** evaluates tokenization strategies for Scandinavian languages, emphasizing the critical need for language-specific adaptations in multilingual contexts. For German, Diewald et al. **Diewald et al., "Evaluating Tokenizers on German Text"** assessed tokenizers like \texttt{KorAP-Tokenizer} and \texttt{SoMaJo}, achieving high accuracy in token boundary detection while ensuring computational efficiency for large-scale corpora.

Erkaya **Erkaya, "Subword Tokenization for Morphologically Rich Languages"** provides a comprehensive analysis of subword tokenization methods, particularly focusing on their application to morphologically rich languages like Turkish. Erkaya evaluates the impact of corpus size and vocabulary size on tokenization characteristics, highlighting that larger corpora improve morphology encoding. The study also introduces a morphologically optimized tokenizer that improves downstream performance on tasks such as named-entity recognition, parts-of-speech tagging, question answering, and sentiment analysis. This work emphasizes the significance of incorporating morphological supervision into tokenization for languages with agglutinative structures.

A significant contribution to multilingual tokenization comes from the EuroLLM team, which emphasizes the importance of designing tokenizers with large vocabularies to support diverse linguistic structures **Devlin et al., "Language Models are Few-Shot Learners"**. By employing a Byte Pair Encoding (BPE) tokenizer with byte fallback and a vocabulary of 128,000 pieces, EuroLLM achieves a balance between low fertility (tokens per word) and parameter efficiency. Their findings indicate that vocabulary size is a critical factor in determining a tokenizer’s ability to efficiently process multiple languages, including European and non-European ones. EuroLLM's comparison of fertility metrics across tokenizers, such as those from Mistral, LLaMA-3, and Gemma, further underscores the trade-offs between large vocabularies and computational cost.

Efficiency advancements have also been demonstrated in GitHub's faster BPE implementation **Sennrich et al., "Neural Machine Translation of Rare Words"**, which significantly improves scalability for tasks requiring billions of tokens. This aligns with EuroLLM’s approach of optimizing tokenization to enhance downstream task performance while maintaining computational efficiency.

Rust et al. **Rust et al., "A Study on the Performance of Monolingual Tokenizers"** highlight the effectiveness of monolingual tokenizers tailored to specific languages, showing notable downstream improvements for morphologically rich languages. Similarly, Lin et al. **Lin et al., "Selective Language Modeling"** propose Selective Language Modeling (SLM), which assigns utility scores to tokens and selectively trains on high-utility tokens, reducing noise and enhancing training efficiency. This approach is particularly relevant for languages like Turkish, where preserving meaningful tokens is essential for capturing linguistic richness.

The studies discussed collectively emphasize the necessity of tokenization strategies that balance linguistic integrity, computational efficiency, and downstream performance. Building on these advancements, this study evaluates tokenizers for Turkish, employing metrics such as token purity, vocabulary size, and processing speed. By integrating insights from multilingual projects like EuroLLM and tailoring techniques for morphologically rich languages, this work advances the understanding and optimization of tokenization for diverse linguistic contexts.