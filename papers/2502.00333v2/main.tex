%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{xspace}
\usepackage{lipsum}  
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{tcolorbox}
\usepackage{colortbl}
% \usepackage{subfig}
\usepackage{float}
\usepackage{adjustbox}
\usepackage{bbding}
\usepackage{wrapfig}
\usepackage{svg}
\usepackage{nicefrac}
\usepackage{minitoc}
\usepackage{subcaption}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}


\definecolor{colorhead}{HTML}{e2ecda}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\newcommand{\ie}{{\emph{i.e.}},\xspace}
\newcommand{\eg}{\emph{e.g.},\xspace}
\newcommand{\etc}{etc\@ifnextchar.{}{.\@}}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{BiMaCoSR: Binary One-Step Diffusion Model Leveraging Flexible Matrix Compression for Real Super-Resolution}

\begin{document}

\twocolumn[
\icmltitle{BiMaCoSR: Binary One-Step Diffusion Model \\ Leveraging Flexible Matrix Compression for Real Super-Resolution}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\vspace{-5mm}
\icmlauthor{Kai Liu}{equal,sjtu}
\icmlauthor{Kaicheng Yang}{equal,sjtu}
\icmlauthor{Zheng Chen}{sjtu}
\icmlauthor{Zhiteng Li}{sjtu}\\
\icmlauthor{Yong Guo}{scut}
\icmlauthor{Wenbo Li}{huawei}
\icmlauthor{Linghe Kong$^{\dagger}$}{sjtu}
\icmlauthor{Yulun Zhang$^{\dagger}$}{sjtu}
\end{icmlauthorlist}

% \icmlaffiliation{sjtu}{Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China}
\icmlaffiliation{sjtu}{Shanghai Jiao Tong University}
\icmlaffiliation{scut}{South China University of Technology}
\icmlaffiliation{huawei}{Huawei Noahâ€™s Ark Lab}

\icmlcorrespondingauthor{Linghe Kong}{linghe.kong@sjtu.edu.cn}
\icmlcorrespondingauthor{Yulun Zhang}{yulun100@gmail.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Diffusion Model, Binarization, Super-Resolution}

% \vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.
\vspace{-10mm}
\begin{abstract}
% \vspace{-5mm}
While super-resolution (SR) methods based on diffusion models (DM) have demonstrated inspiring performance, their deployment is impeded due to the heavy request of memory and computation.
Recent researchers apply two kinds of methods to compress or fasten the DM.
One is to compress the DM into 1-bit, aka binarization, alleviating the storage and computation pressure.
The other distills the multi-step DM into only one step, significantly speeding up inference process.
Nonetheless, it remains impossible to deploy DM to resource-limited edge devices.
To address this problem, we propose \textbf{BiMaCoSR}, which combines binarization and one-step distillation to obtain extreme compression and acceleration.
To prevent the catastrophic collapse of the model caused by binarization, we propose \textbf{s}parse \textbf{m}atrix \textbf{b}ranch (\textbf{SMB}) and \textbf{l}ow \textbf{r}ank \textbf{m}atrix \textbf{b}ranch (\textbf{LRMB}).
Both auxiliary branches pass the full-precision (FP) information but in different ways.
SMB absorbs the extreme values and its output is high rank, carrying abundant FP information.
Whereas, the design of LRMB is inspired by LoRA and is initialized with the top $r$ SVD components, outputting low rank representation.
The computation and storage overhead of our proposed branches can be safely ignored.
Comprehensive comparison experiments are conducted to exhibit BiMaCoSR outperforms current state-of-the-art binarization methods and gains competitive performance compared with FP one-step model.
BiMaCoSR achieves a 23.8$\times$ compression ratio and a 27.4$\times$ speedup ratio compared to FP counterpart.
Our code and model are available at \href{https://github.com/Kai-Liu001/BiMaCoSR}{https://github.com/Kai-Liu001/BiMaCoSR}.

\end{abstract}

\setlength{\abovedisplayskip}{2pt}
\setlength{\belowdisplayskip}{2pt}
\vspace{-10mm}
\section{Introduction}
\vspace{-2mm}
Single image super-resolution (SR)~\cite{keys1981cubic,zibetti2007robust,lu2013image,ECCV2014LearningDong,yang2014single,TPAMI2016ImageDong} is a traditional yet challenging low-level vision problem.
Serving as a fundamental research task, it has attracted long-standing and considerable attention in the computer vision community.
The final object of SR is to restore a high-quality (HQ) image from its low-quality (LQ) observation, which suffers from various image quality degradations.
The difficulty of SR mainly lies in two parts: (1) the unknown degradations (\eg blur, downsampling, noise, compression and their combinations) of LQ. (2) the multiple solutions for a given LQ input image.

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figs/radar.pdf}
    \vspace{-7mm}
    \caption{Performance comparison between binarization methods on the RealSR dataset. BiMaCoSR achieves consistently leading scores on all evaluation metrics.}
    \vspace{-9mm}
    \label{fig:intro-visual-comp}
\end{figure}

In recent years, numerous studies have been made to tackle this challenge, utilizing convolution neural networks (CNNs)~\cite{ECCV2014LearningDong,TPAMI2016ImageDong,CVPR2017PhotoLedig,zhang2018residual}, vision transformers (ViTs)~\cite{zhang2018image,liang2021swinir,wang2022uformer,chen2022cross,chen2023dual}, and their combinations.
Though achieving  inspiring results, these methods mostly fail in real-world scenarios.
This failure is attributed to assuming degradation as \textit{an ideal bicubic downsampling kernel}, way too different from the unknown and complex degradation in real world.
Therefore, it draws researchers increasing attention to reconstruct perceptually realistic HQ images in real-world scenarios.
Thereafter, this challenging and meaningful task is called real-world super-resolution (Real-SR)~\cite{gu2019blind,zhang2021designing,wang2021real,cai2019toward,yu2024scaling,wu2024seesr,sun2024coser}.

Recently, diffusion models (DMs) demonstrate remarkable performance in image generating tasks, particularly in perceptual quality.
The excellence of DM comes from its vast prior knowledge in modeling real-world objects, especially generating clear textures and mitigating artifacts and distortions.
The powerful realistic texture generation ability is inherently the same as the object of Real-SR problem, leading to plentiful breakthroughs~\cite{yang2025pixel,yu2024scaling,rombach2022high}.
However, the inference cost is still too high to run on edge devices.
Therefore, it's essential to further compress DMs to accelerate the inference, reduce storage cost, and minimize degradation.

Popular model compression techniques include pruning, distillation, and quantization, among which, 1-bit quantization (\ie binarization) gains significant effectiveness.
As an extreme quantization method, binarization compresses models' weights from 32-bit to only 1-bit, significantly reducing memory and computational cost.
However, applying naive binarization will lead to catastrophic model collapse.
Hence, additional structures are required to be designed. 

A popular solution is adding full-precision information, \ie skip connection branch.
However, due to UNet's frequent changes in resolution and dimension, skip connection faces the mismatch challenge.
To address this problem, we propose two auxiliary branches, namely low rank matrix branch (LRMB) and sparse matrix branch (SMB).
Inspired by LoRA, the proposed LRMB leverages low rank decomposition to achieve dimension shift.
We select the top $r$ singular values in SVD and utilize its corresponding components to initialize the LRMB.
As for SMB, we employ sparse matrix to absorb the top $k$ absolute values in full-precision matrix.
The weights of LRMB and SMB are subtracted from the binarized matrix branch (BMB) and it concentrates on restoring textures.
Three branches form the BiMaCoSR and provide excellent performance shown in Fig.~\ref{fig:intro-visual-comp}.

To sum up, the contributions of our work are as follows:
\begin{enumerate}
    \vspace{-3mm}
    \item We design BiMaCoSR, a new binarized one-step diffusion model for image super-resolution. 
    To the best of our knowledge, BiMaCoSR is the first binarized one-step diffusion model. 
    \vspace{-2mm}
    \item We propose LRMB, which leverages low rank decomposition and SVD initialization to carry low frequency information and decouple the effect of BMB.
    \vspace{-2mm}
    \item We propose SMB, which utilizes sparse matrix compression and extreme value absorption to deliver high rank features and achieve further decoupling.
    \vspace{-2mm}

    \item We conduct comprehensive comparison experiments to show the state-of-the-art performance of the proposed BiMaCoSR. 
    Besides, extensive ablation studies are conducted to prove the robustness and efficacy. 
\end{enumerate}


\section{Related Work}
\paragraph{Image Super-Resolution.} 
Deep learning based approaches have demonstrated striking power in the realm of SR~\cite{ECCV2014LearningDong,luo2022lattice,wang2021real,Lim_2017_CVPR_Workshops,chen2023dual}.
As a groundbreaking work, SRCNN~\cite{ECCV2014LearningDong} initiates the track of solving SR problem via deep learning based approach.
Thereafter, substantial contributions have been made to explore the best SR network architecture.
For example, RCAN~\cite{zhang2018image} leverages the residual in residual structure and deepens the network to more than 400 layers.
SwinIR~\cite{liang2021swinir} is based on vision transformer structure and utilizes spatial window self-attention to capture the overall structure information.
CAT~\cite{chen2022cross} combines the attention mechanism and the CNN structure to make the most of the local and the global information.
However, most of these conventional image super-resolution methods can not handle the Real-SR task because of the complex degradation in real world.

\vspace{-4mm}
\paragraph{Diffusion Model.} 
In recent years, the diffusion based methods have gained remarkable performance in many computer vision tasks and SR is no exception.
For instance, SR3~\cite{saharia2022image} restores the LQ by transforming the standard normal distribution into the empirical data distribution by learning a series of iterative refinement steps.
DiffBIR~\cite{lin2024diffbir} capitalizes on two restoration stage to seek the tradeoff of fidelity and quality.
SinSR~\cite{wang2024sinsr} effectively reduces the inference step to only one step via distillation and regularization.
Following SinSR, OSEDiff~\cite{wu2024one} modifies the distillation paradigm and novel losses are introduced to improve face restoration ability.
Despite the greatly improved inference speed, the model size remains the same and there is still room for further acceleration.



\vspace{-4mm}
\paragraph{Binarization.} 
As the most extreme form of quantization, binarization typically compresses the weight into only 1 bit.
In binarization, all the weights are seen as $\pm 1$ and the multiplications between weights and activations are converted to bit operation on sign bit of activation, allowing maximum compression and acceleration.
Binarization related researches are mainly about classification tasks initially~\cite{rastegari2016xnor,liu2020reactnet,qin2020forward,qin2022distribution}.
Recently, researchers begin to perform binarization on image restoration tasks.
Binary Latent Diffusion~\cite{wang2023binary} trains an auto-encoder with a binary latent space and mainly focus on the Bernoulli distribution instead of acceleration.
BiDM~\cite{zheng2024bidm} leverages timestep-friendly binary structure and space patched distillation to compress the diffusion model to 1 bit.
BI-DiffSR~\cite{chen2024binarized} designs several binary friendly modules and redistribute the activation of different time step.
However, the inference step remains the same.
Therefore, it is necessary to further compress the model to one step.


\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{figs/overview.pdf}
    \vspace{-7mm}
    \caption{Overview of our proposed BiMaCoSR which employs three different compressed matrix branches. (a) The structure of a convolution layer in BiMaCoSR after binarization. Two auxiliary branches, \ie LRMB and SMB, support BiMaCoSR's excellent performance. The linear layer can be regarded as 1$\times$1 convolution layer and is processed with the same pipeline. (b) Illustration of the initialization sequence and how the three branches solve the weakness of other branch. 
}
    \label{fig:method-overview}
    \vspace{-4.5mm}
\end{figure*}

\section{Methodology}\label{sec:method}
\vspace{-1mm}
In this section, we describe our proposed BiMaCoSR, shown in Fig.~\ref{fig:method-overview}.
First, we analyze three issues that have potential for further improvement when binarizing diffusion models.
Thereafter, we describe our proposed branches, \ie low rank matrix branch (LRMB) and sparse matrix branch (SMB), which could serve as auxiliary branch to binarized matrix branch (BMB).
Finally, we illustrate the initialization methods, shown in Fig.~\ref{fig:method-init}, and why our designs work.
\vspace{-3mm}
\subsection{Analysis}
\vspace{-1mm}
Binarized blocks inevitably suffer from representation degradation due to the extreme 1-bit compression.
Previous researches made remarkable progress~\cite{chen2024binarized,zheng2024bidm} on binarization, yet there still remains room for improvement.
We conclude three issues which deserve special attention in most diffusion models.

\textbf{\textit{Issue I: The specially severe degradation of linear layer.}}
Experimentally, we observe that with same size of parameters, linear layer suffers from more severe degradation in comparison to convolution layer.
This observation in SR is consistent with previous research~\cite{le2023binaryvit}.
With the advancing of DiT and binarization, this issue is gradually significant and additional process to quantized or binarized linear layer is in urgent need.

\textbf{\textit{Issue II: The frequent dimension changes.}}
It's necessary to leverage skip connection as an auxiliary branch in binarized network.
Skip connection could carry abundant feature information and bring negligible computation and storage overhead.
However, in UNet, frequent changes in dimension and resolution make two forms of skip connection (\ie addition and concatenation) not applicable.
In addition, the distributions before and after binarized block are greatly different.
Therefore, efficient module is critical to overcome the frequent changes and carry rich information.


\textbf{\textit{Issue III: The inadequate usage of pre-trained FP model.}}
The pre-trained FP model is vital to quantization methods from any perspective.
However, current QAT researches do not make the most of the pre-trained model, simply load the FP parameters and directly begin the training.
In binarization, it is a popular way to leverage additional modules to enhance the binarized network.
The impact of the modules could vary when different initialization methods are applied.

We conclude that the above issues are attributed to the insufficient ability of binarization.
\textbf{To be specific, binarization is just one of the matrix compression methods, inherently unsuitable for multifaceted challenges.}
Therefore, we propose following branches to compress matrix in different ways.
With the combination of various compressed matrices, the issues above can be appropriate addressed.

\vspace{-2mm}
\subsection{Low Rank Matrix Branch}
\vspace{-1mm}

To address \textbf{\textit{Issue I}}, \ie the specially severe degradation of linear layer, we identify that the quantized linear layer is strongly limited by the bit-width.
An FP linear layer usually serves two purposes, delivering the complete information of input and performing appropriate linear transformation.
With limited bit-width, the quantized linear layer cannot play two roles at the same time.
Therefore, we decouple the roles apart.
Though binarized, the weight matrix is usually high rank and contributes more to high frequency, \ie the detailed structure and texture in image.
Hence, we design low rank matrix branch (LRMB) to serve as a complementary branch, transmitting low frequency information.

The idea of LRMB is inspired by low rank approximation in matrix theory, which is a common strategy for matrix compression.
To transmit low frequency information, matrices with low rank are enough and efficient.
Mathematically, given an $m\times n$ matrix $\mathbf{W}$, it can be approximated with two low rank matrices, \ie $\mathbf{W} \approx \hat{\mathbf{W}} := \mathbf{B}\mathbf{A}$, where $\mathbf{B}$ and $\mathbf{A}$ are $m\times r$ and $r\times n$ matrices respectively, and $r \ll \min(m, n)$ is a hyper-parameters denoting the rank of $\hat{\mathbf{W}}$.
Therefore, the formula of LRMB is:
\begin{equation}
    \mathbf{x}_{\text{LRMB}}
    = \mathrm{LRMB}(\mathbf{x}_{\text{in}}) 
    := \mathbf{x}_{\text{in}}\mathbf{B}\mathbf{A},
\end{equation}
where $\mathbf{x}_{\text{in}}\in \mathbb{R}^{N\times m}$ and $\mathbf{x}_{\text{LRMB}}\in \mathbb{R}^{N\times n}$ are the input and output of $\mathrm{LRMB}(\cdot)$ respectively, and $N$ is the number of tokens.
Usually, $\mathbf{W}$ is a square matrix and $m=n$.

As for complexity, the storage overhead is:
\begin{equation}
    O_{s} = (m\times r + r \times n)B=rB(m+n)\ll mnB^{\prime},
\end{equation}
where $B=32 \text{ or } 16$ and $B^{\prime}=1$ are number of bits required by one element in LRMB branch and binarized branch respectively.
Meanwhile, the computation overhead is:
\begin{equation}
    O_{c} = N\times m \times r + N\times r \times n = Nr(m+n).
\end{equation}
This means that even saved with 32 bits, the storage and computation overhead of LRMB is neglectable.
In conclusion, with LRMB, \textbf{\textit{Issue I}} and \textbf{\textit{Issue II}} are partly solved.

\vspace{-2mm}
\subsection{Sparse Matrix Branch}\label{method-smb}
\vspace{-1mm}
LRMB is still not enough to replace the skip connection.
This is because skip connection can be represented as a full rank identity matrix while LRMB is low rank.
To compensate the missed ranks, we propose sparse matrix branch (SMB), which leverages sparse matrix compression and could efficiently deliver high rank information.

Sparse matrix is a matrix in which most of the elements are zero~\cite{yan2017efficient}.
A common criterion of a sparse matrix is that the number of non-zero elements is approximated to the number of rows or columns.
One way to save a sparse matrix is via the coordinate format (COO), where the non-zero elements are represented by a list of triples and it component is $(row, col, value)$.

Specifically, to form a sparse matrix, we select $k$ critical values from the weight matrix and the selection method will be described in Sec.~\ref{method-initialization}.
In the forward process, the SMB process could be equivalently expressed as
\begin{equation}
    \mathbf{x}_{\text{SMB}}=\mathrm{SMB}(\mathbf{x}_{\text{in}}):=\mathbf{x}_{\text{in}}\mathbf{W}_{\text{sparse}},
\end{equation}
where $\mathbf{W}_{\text{sparse}}$ is a matrix with $k$ non-zero elements.
During training, the coordinates will be fixed while their values will be updated by gradient descent.
With SMB, the high rank information could be delivered and the remaining parts of \textbf{\textit{Issue I}} and \textbf{\textit{Issue II}} could be solved.
\begin{figure}
    \centering
     \includegraphics[width=0.95\linewidth]{figs/initial.pdf}
     \vspace{-2.5mm}
    \caption{Initialization of different branch.$W_{res}$ represents the initial quantization error. In our method,  $\| W_{res} \|_F^2 = 0.1855$, while $\| W_{res} \|_F^2 =1.1275$ in direct binarization.}
    \vspace{-6mm}
    \label{fig:method-init}
\end{figure}
\vspace{-2mm}
\subsection{Pretrained-Friendly Initialization}\label{method-initialization}
\vspace{-1mm}
We propose the following initialization method to guarantee better use of the pre-trained model, allowing better restoration performance and faster convergence.


The purpose of LRMB is to carry the low frequency information.
Usually, compared with high frequency information, the magnitude of low frequency information is much greater.
Therefore, we leverage SVD and select the top $r$ components to initialize LRMB.
To be specific, we perform SVD on $\mathbf{W}$:
\begin{equation}
    \mathbf{W}=\mathbf{U}\Sigma\mathbf{V}^{T}=\mathbf{U}\text{diag}\{\sigma_1, \sigma_2, \dots, \sigma_n\} \mathbf{V}^{T}.
\end{equation}
Then, we truncate the top $r$ singular values and absorb the remaining singular value into $\mathbf{U}$, forming matrices $\mathbf{A}$ and $\mathbf{B}$. The process can be written as:
\begin{equation}
    \mathbf{W} \approx \underbrace{\mathbf{U}\text{diag}\{\sigma_1, \sigma_2, \dots, \sigma_r, 0, \dots, 0\}}_{B}  \underbrace{\mathbf{V}^{T}}_{A}
    =\mathbf{B}\mathbf{A}.
\end{equation}

Thereafter, to guarantee that the binarized matrix only serves as adding high frequency information, the low frequency counterpart will be subtracted.
The formula is:
\begin{equation}
    \mathbf{W}_{\text{BMB}}^{\prime}:=\mathbf{W}-\mathbf{B}\mathbf{A},
\end{equation}
where $\mathbf{W}_{\text{BMB}}^{\prime}$ is the matrix to be binarized after the initialization of LRMB.
Hence, the ability of weight matrix is decoupled into two components, the low frequency part ($\mathbf{B}\mathbf{A}$) and the high frequency part ($\mathbf{W}_{\text{BMB}}^{\prime}$).

Thereafter, the initialization of SMB also matters.
As described in Sec.~\ref{method-smb}, we select $k$ values from the weight matrix to generate the sparse matrix.
Due to the fixed coordinates of non-zero elements, random selection is apparently a good candidate.
Besides, binarization often ignores the outlier values in weight matrix, which are often huge but rare (less than 0.1\%). 
Though rare, these outliers play a crucial role in most models.
Considering their rareness and significance, we select $k$ elements with the greatest absolute value in $\mathbf{W}_{\text{BMB}}^{\prime}$.
Subsequently we subtract these values in $\mathbf{W}_{\text{BMB}}^{\prime}$ to decouple the overlapping effect of BMB and SMB.
The process can be formulated as:
\begin{equation}
\begin{aligned}
    \widetilde{w}_{ij}=\begin{cases}
w_{ij}, & \text{if } |w_{ij}^{\prime}|\geq t\\
0, & \text{otherwise}
\end{cases}
,\mathbf{W}_{\text{BMB}} := \mathbf{W}_{\text{BMB}}^{\prime}-\mathbf{W}_{\text{sparse}},
\end{aligned}
\end{equation}
where $\widetilde{w}_{ij}$ and $w_{ij}$ are the elements of $\mathbf{W}_{\text{sparse}}$ and $\mathbf{W}_{\text{BMB}}^{\prime}$ respectively, $t$ is the $k$-th largest absolute value in $\mathbf{W}_{\text{BMB}}^{\prime}$, $\mathbf{W}_{\text{BMB}}$ is the final matrix to be binarized.
\vspace{-2mm}
\subsection{Overall Structure}
\vspace{-2mm}
After adding LRMB and SMB and their initialization, the binarized matrix branch (BMB) is formed with $\mathbf{x}_{\text{BMB}}$.
To be specific, the input feature and weight matrix will be binarized with $\text{Sign}(\cdot)$, which can be written as:
\begin{equation}
\begin{cases}
\mathbf{x}_{\text{b}}=\text{Sign}(\mathbf{x}_{\text{in}}),\\
\mathbf{W}_{\text{b}}=\text{Sign}(\mathbf{W}_{\text{BMB}}),
\end{cases}
\text{Sign}(x) = \begin{cases}
    +1, & x\geq 0,\\
    -1, & x < 0,
\end{cases}
\end{equation}
where $\text{Sign}(\cdot)$ is performed element-wise.
Thereafter, the full-precision convolution and linear transform are replaced with efficient logical XNOR and bit-counting operations.
The process can be formulated as:
\begin{equation}
\begin{aligned}
\mathbf{x}_{\text{BMB}}^{\prime}=\text{bit-count}(\text{XNOR}(\mathbf{x}_{\text{b}}, \mathbf{W}_{\text{b}})),\\
\end{aligned}
\end{equation}
Finally, we follow the design of XNOR-Net~\cite{rastegari2016xnor} to compensate the precision loss, which is:
\begin{equation}
    \mathbf{x}_{\text{BMB}} = \mathbf{x}_{\text{BMB}}^{\prime} \odot ( \mathcal{A} \otimes \mathbf{k} ),
\end{equation}
where $\mathcal{A}_{H\times W}$ is the channel-wise absolute average of input activation, $\mathbf{k}$ is a vector whose element is the absolute average of correspond channel, $\odot$ is Hadamard product, and $(\mathcal{A}\otimes\mathbf{k})_{c,i,j}:=\mathcal{A}_{i,j}\mathbf{k}_{c}$ should be calculated first. 

We binarized every convolution layer and linear layer except the first and last convolution layers due to extremely severe degradation of quantization on head and tail.
Specifically, linear layer can be seen as the $1\times1$ convolution layer and therefore processed in the same way as convolution layer.

For one binarized layer, the output can be written as:
\begin{equation}
\mathbf{x}_{\text{out}}=\mathbf{x}_{\text{BMB}}+\mathbf{x}_{\text{LRMB}}+\mathbf{x}_{\text{SMB}}.
\end{equation}
In conclusion, we propose LRMB, SMB, and their corresponding initialization methods.
With these designs, our BiMaCoSR can significantly reduce the information loss caused by binarization and improve the restoration ability.
Besides, the storage and computation overhead caused by LRMB and SMB can be safely ignored.


\section{Experiments}
\vspace{-1mm}
\subsection{Experimental Settings}
\vspace{-1mm}
\textbf{Data.}
We take the models on the training set of ImageNet~\cite{russakovsky2015imagenet} and the LR images are generated by the same pipeline of RealESRGAN~\cite{wang2021real}.
We evaluate the models with three benchmark datasets: RealSR~\cite{cai2019toward}, DRealSR~\cite{wei2020component}, and DIV2K-Val~\cite{agustsson2017ntire}.
RealSR and DRealSR are real world benchmarks while DIV2K-Val employs Bicubic interpolation to generate LR images. 
The upscale ratio of training set and test set is $\times 4$.

\vspace{-0.5mm}
\textbf{Evaluation Metrics.}
The evaluation metrics are implemented with IQA-Pytorch~\cite{pyiqa} and are twofold to assess the restoration and compression ability.
\textbf{Firstly}, to thoroughly evaluate the restoration ability of our proposed BiMaCoSR, we employ the following quantitative metrics: full-reference metrics PSNR, SSIM~\cite{wang2004image}, and LPIPS~\cite{zhang2018unreasonable} and non-reference metrics DISTS~\cite{ding2020image}, FID~\cite{heusel2017gans}, NIQE~\cite{zhang2015feature}, MANIQA-pipal~\cite{yang2022maniqa}, MUSIQ~\cite{ke2021musiq}, and CLIPIQA+~\cite{wang2022exploring}.
PSNR and SSIM are distortion-based metrics and are calculated on the Y channel (\ie luminance) of the YCbCr space.
The rest metrics are all perceptual metrics and it is widely known that perceptual metrics are more aligned with human when rating the image quality. 

\textbf{Secondly}, to demonstrate binarization's extreme compression and acceleration ability, we use the total parameters and overall operations as key metrics.
Following previous work~\cite{xia2023basic,qin2023bibench}, the total parameters (\textbf{Params}) of the model are calculated as Params$=$Params$^b$$+$Params$^f$, and the overall operations (\textbf{OPs}) as OPs$=$OPs$^b$$+$OPs$^f$, where Params$^b$$=$Params$^f$$/$32 and OPs$^b$$=$OPs$^f$$/$64; the superscripts $f$ and $b$ denote full-precision and binarized modules, respectively.
The computational complexity is tested with the input size 3$\times$64$\times$64.

\vspace{-0.5mm}
\textbf{Implementation Details.}
We take SinSR~\cite{wang2024sinsr}, a one-step diffusion model, as the backbone.
We initialize with SinSR's weights and use ResShift~\cite{yue2024resshift} as the teacher model.
All the rest convolution and linear layers, except the head and tail layers, are binarized into BMB to guarantee the best compression ratio and LRMB and SMB are attached with BMB. 
We set the rank of LRMB as $r=8$ and the number of non-zero elements in SMB as $k=2\max(C_1, C_2)$, where $C_1$ and $C_2$ are the numbers of input and output channels respectively.
\begin{table*}[t]
    \centering
    \caption{Quantitative comparison with SOTA methods. We compare BiMaCoSR with full-precision models and current binarization methods. We employ 9 metrics commonly used in SR and test on three benchmarks. The best and second best value are marked with \textcolor{red}{red} and \textcolor{blue}{blue} respectively. In conclusion, our proposed BiMaCoSR achieves SOTA performance.}
    \vspace{-2mm}
    \label{tab:exp-comp-performance}
    \setlength{\tabcolsep}{1.6mm}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{l|c|c|ccccccccc}
    \hline
    \toprule[0.15em]
    \rowcolor{colorhead} Datasets & Methods & Bits (W/A) & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ & MUSIQ $\uparrow$ & MANIQA $\uparrow$ & DISTS $\downarrow$ & FID $\downarrow$ & NIQE $\downarrow$  & CLIP-IQA+ $\uparrow$ \\
\midrule[0.15em]
\multirow{9}{*}{RealSR}    & SinSR             &32/32& 26.51 & 0.7380 & 0.3635 & 57.87 & 0.5139 & 0.2193 & 56.36 & 5.826 & 0.5736  \\
                            & ResShift         &32/32& 25.45 & 0.7243 & 0.3731 & 56.23 & 0.5005 & 0.2344 & 58.14 & 7.353 & 0.5708  \\  \cline{2-12}
                            & XNOR              &1/1 & 26.48 & 0.7434 & 0.3968 & 43.56 & 0.3732 & 0.2609 & \textcolor{blue}{105.72} & \textcolor{blue}{6.014} & \textcolor{blue}{0.4380} \\
                            & ReActNet          &1/1 & \textcolor{blue}{26.60} & \textcolor{blue}{0.7530} & \textcolor{blue}{0.3834} & \textcolor{blue}{44.18} & \textcolor{blue}{0.3829} & \textcolor{blue}{0.2551} & 109.36 & 6.306 & 0.4361 \\
                            & BBCU              &1/1 & 26.43 & 0.7488 & 0.3902 & 43.70 & 0.3792 & 0.2575 & 108.32 & 6.058 & 0.4298 \\
                            & ReSTE             &1/1 & 26.26 & 0.7408 & 0.4184 & 41.04 & 0.3677 & 0.2719 & 113.86 & 6.174 & 0.4083 \\
                            & BiDM              &1/1 & 25.07 & 0.7036 & 0.5042 & 35.60 & 0.3517 & 0.3226 & 115.23 & 6.759 & 0.3935 \\
                            & BiMaCoSR          &1/1 & \textcolor{red}{26.84} & \textcolor{red}{0.7698} & \textcolor{red}{0.3375} & \textcolor{red}{49.01} & \textcolor{red}{0.4034} & \textcolor{red}{0.2183} & \textcolor{red}{86.09} & \textcolor{red}{5.856} & \textcolor{red}{0.4800}  \\
\midrule[0.15em]
\multirow{9}{*}{DRealSR}   & SinSR             &32/32& 27.89 & 0.7332 & 0.4499 & 30.81 & 0.4519 & 0.2209 & 16.56 & 5.789 & 0.6052  \\
                            & ResShift         &32/32& 26.64 & 0.7298 & 0.4478 & 31.09 & 0.4345 & 0.2337 & 18.12 & 6.959 & 0.5795  \\  \cline{2-12}
                            & XNOR              &1/1 & 29.03 & 0.8319 & 0.3712 & 26.19 & 0.3560 & 0.2447 & \textcolor{blue}{29.88} & \textcolor{blue}{6.229} & \textcolor{blue}{0.4449} \\
                            & ReActNet          &1/1 & \textcolor{red}{29.34} & \textcolor{red}{0.8431} & \textcolor{blue}{0.3571} & \textcolor{blue}{26.83} & \textcolor{blue}{0.3618} & \textcolor{blue}{0.2411} & 30.18 & 6.561 & 0.4380 \\
                            & BBCU              &1/1 & 29.00 & 0.8385 & 0.3643 & 26.37 & 0.3594 & 0.2433 & 30.94 & 6.337 & 0.4383 \\
                            & ReSTE             &1/1 & 28.91 & 0.8353 & 0.3899 & 25.12 & 0.3509 & 0.2641 & 33.64 & 6.459 & 0.4131 \\
                            & BiDM              &1/1 & 27.40 & 0.7942 & 0.4849 & 23.38 & 0.3529 & 0.3118 & 37.83 & 6.753 & 0.4307 \\
                            & BiMaCoSR          &1/1 & \textcolor{blue}{29.33} & \textcolor{blue}{0.8393} & \underline{\textcolor{red}{0.3400}} & \textcolor{red}{29.38} & \textcolor{red}{0.3802} & \textcolor{red}{0.2278} & \textcolor{red}{22.31} & \textcolor{red}{6.150} & \textcolor{red}{0.4867} \\
\midrule[0.15em]
\multirow{9}{*}{DIV2K-Val} & SinSR             &32/32& 27.75 & 0.7694 & 0.1903 & 64.62 & 0.5336 & 0.1029 &  6.27 & 4.308 & 0.6147   \\
                            & ResShift         &32/32& 27.18 & 0.7667 & 0.1775 & 65.04 & 0.5548 & 0.1016 &  7.54 & 5.121 & 0.6280   \\ \cline{2-12}
                            & XNOR              &1/1 & 26.44 & 0.7185 & 0.3727 & 49.10 & 0.3972 & 0.2204 & 55.77 & 5.320 & 0.4584 \\
                            & ReActNet          &1/1 & \textcolor{blue}{26.49} & \textcolor{blue}{0.7260} & \textcolor{blue}{0.3602} & \textcolor{blue}{50.29} & \textcolor{blue}{0.4078} & \textcolor{blue}{0.2111} & \textcolor{blue}{52.32} & 5.366  & \textcolor{blue}{0.4726} \\
                            & BBCU              &1/1 & 26.39 & 0.7221 & 0.3660 & 50.09 & 0.4035 & 0.2148 & 53.22 & \textcolor{blue}{5.263} & 0.4653 \\
                            & ReSTE             &1/1 & 26.07 & 0.7125 & 0.3916 & 46.95 & 0.3907 & 0.2295 & 61.52 & 5.399  & 0.4328 \\
                            & BiDM              &1/1 & 24.29 & 0.6725 & 0.4370 & 40.15 & 0.3747 & 0.2916 & 62.28 & 6.090  & 0.4112 \\
                            & BiMaCoSR          &1/1 & \textcolor{red}{27.35} & \textcolor{red}{0.7547} & \textcolor{red}{0.2999} & \textcolor{red}{53.38} & \textcolor{red}{0.4337} & \textcolor{red}{0.1806} & \textcolor{red}{27.99} & \textcolor{red}{4.987} & \textcolor{red}{0.5176} \\
    \bottomrule[0.15em]
    \end{tabular}
    } % resize box
\vspace{-4mm}
\end{table*}

\vspace{-0.5mm}
\textbf{Training Settings.}\label{sec:training-settings}
We utilize Adam optimizer~\cite{kingma2014adam} with $\beta_1 = 0.9$ and $\beta_2 = 0.99$, and set the learning rate as 2$\times$$10^{-5}$.
The batch size is set to 8, with 100K iterations. 
The input LR images are center-cropped to size 64$\times $64.
Our model is implemented based on Pytorch~\cite{paszke2019pytorch} with one NVIDIA RTX A6000.

\vspace{-2mm}
\subsection{Comparison with State-of-the-Art Methods}
\vspace{-2mm}
We compare our proposed BiMaCoSR with recent binarization methods, including XNOR~\cite{rastegari2016xnor}, ReActNet~\cite{liu2020reactnet}, BBCU~\cite{xia2023basic}, ReSTE~\cite{wu2023estimator}, and BiDM~\cite{zheng2024bidm}.
All binarization methods are implemented on SinSR~\cite{wang2024sinsr} and trained with the same settings.
As LRMB and SMB slightly increase the parameters, we additionally keep the first two and last two convolution layers as full-precision. 
We also compare BiMaCoSR with the full-precison model SinSR and ResShift~\cite{yue2024resshift}.
SinSR is the distilled version of ResShift.
The comparison results are exhibited in quantitative and qualitative aspects.


\begin{table*}[t]
    \centering
    \setlength{\tabcolsep}{2.0mm}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{l|cccccccc}
        \hline
        \toprule[0.15em]
        \rowcolor{colorhead}   & ResShift &   SinSR   & ReActNet  & BBCU    & ReSTE  & BiDM     & XNOR  & Ours        \\
        \midrule[0.15em]
        Inference Step         & 15 &  1   &  1   &   1  &  1      &  1  & 1  &  1 \\
        FLOPs (G)              & 753.45  &  50.23 &  5.83 & 5.83 & 5.83  & 11.60 & 5.83 & 1.83 \\
        \# Total Param (M)  & 118.59 &  118.59 & 4.95 & 4.95 & 4.95  &   18.69     &  4.95    &  4.98    \\
        PSNR/LPIPS   &25.45/0.3731 & 26.51/0.3635  & 26.60/0.3834    & 26.43/0.3902 & 26.26/0.4184 &  25.07/0.5042 & 26.48/0.3968 &  26.84/0.3375  \\
        \bottomrule[0.15em]
    \end{tabular}
    } % resize box
    \vspace{-3mm}
    \caption{Efficiency comparison on RealSR. BiMaCoSR takes least FLOPs while gains the best performance.}
    \vspace{-2mm}
    \label{tab:exp-comp-efficiency}
\end{table*}

\begin{figure*}[t]
\scriptsize
\centering
\scalebox{1.2}{
\begin{tabular}{cccc}
%% one row begins
\hspace{-6mm}
\begin{adjustbox}{valign=t}
\begin{tabular}{c}
\includegraphics[width=0.215\textwidth]{figs/visual/0809_HR_with_box.png}
\\
DIV2K-Val: 0809
\end{tabular}
\end{adjustbox}
\hspace{-0.46cm}
\begin{adjustbox}{valign=t}
\begin{tabular}{cccccc}
\includegraphics[width=0.149\textwidth]{figs/visual/0809_HR.png} \hspace{-4mm} &
\includegraphics[width=0.149\textwidth]{figs/visual/0809_sinsr.png} \hspace{-4mm} &
\includegraphics[width=0.149\textwidth]{figs/visual/0809_ReSTE.png} \hspace{-4mm} &
\includegraphics[width=0.149\textwidth]{figs/visual/0809_BiDM.png} \hspace{-4mm} 
\\
HR \hspace{-4mm} &
SinSR (FP) \hspace{-4mm} &
ReSTE \hspace{-4mm} &
BiDM \hspace{-4mm} &
\\
\includegraphics[width=0.149\textwidth]{figs/visual/0809_BBCU.png} \hspace{-4mm} &
\includegraphics[width=0.149\textwidth]{figs/visual/0809_XNOR.png} \hspace{-4mm} &
\includegraphics[width=0.149\textwidth]{figs/visual/0809_ReactNet.png} \hspace{-4mm} &
\includegraphics[width=0.149\textwidth]{figs/visual/0809_ours.png} \hspace{-4mm}
\\ 
BBCU \hspace{-4mm} &
XNOR \hspace{-4mm} &
ReactNet  \hspace{-4mm} &
BiMaCoSR (ours)  \hspace{-4mm}

\\
\end{tabular}
\end{adjustbox}
%% one row ends
\\
%% one row begins
\hspace{-6mm}
\begin{adjustbox}{valign=t}
\begin{tabular}{c}
\includegraphics[width=0.215\textwidth]{figs/visual/0831_HR_with_box.png}
\\
DIV2K-Val: 0831
\end{tabular}
\end{adjustbox}
\hspace{-0.46cm}
\begin{adjustbox}{valign=t}
\begin{tabular}{cccccc}
\includegraphics[width=0.149\textwidth]{figs/visual/0831_HR.png} \hspace{-4mm} &
\includegraphics[width=0.149\textwidth]{figs/visual/0831_sinsr.png} \hspace{-4mm} &
\includegraphics[width=0.149\textwidth]{figs/visual/0831_ReSTE.png} \hspace{-4mm} &
\includegraphics[width=0.149\textwidth]{figs/visual/0831_BiDM.png} \hspace{-4mm} 
\\
HR \hspace{-4mm} &
SinSR (FP) \hspace{-4mm} &
ReSTE \hspace{-4mm} &
BiDM \hspace{-4mm} &
\\
\includegraphics[width=0.149\textwidth]{figs/visual/0831_BBCU.png} \hspace{-4mm} &
\includegraphics[width=0.149\textwidth]{figs/visual/0831_XNOR.png} \hspace{-4mm} &
\includegraphics[width=0.149\textwidth]{figs/visual/0831_ReactNet.png} \hspace{-4mm} &
\includegraphics[width=0.149\textwidth]{figs/visual/0831_ours.png} \hspace{-4mm}
\\ 
BBCU \hspace{-4mm} &
XNOR \hspace{-4mm} &
ReactNet  \hspace{-4mm} &
BiMaCoSR (ours)  \hspace{-4mm}

\\
\end{tabular}
\end{adjustbox}
%% one row ends
\\
%% one row begins
\hspace{-6mm}
\begin{adjustbox}{valign=t}
\begin{tabular}{c}
\includegraphics[width=0.215\textwidth]{figs/visual/0834_HR_with_box.png}
\\
DIV2K-Val: 0834
\end{tabular}
\end{adjustbox}
\hspace{-0.46cm}
\begin{adjustbox}{valign=t}
\begin{tabular}{cccccc}
\includegraphics[width=0.149\textwidth]{figs/visual/0834_HR.png} \hspace{-4mm} &
\includegraphics[width=0.149\textwidth]{figs/visual/0834_sinsr.png} \hspace{-4mm} &
\includegraphics[width=0.149\textwidth]{figs/visual/0834_ReSTE.png} \hspace{-4mm} &
\includegraphics[width=0.149\textwidth]{figs/visual/0834_BiDM.png} \hspace{-4mm} 
\\
HR \hspace{-4mm} &
SinSR (FP) \hspace{-4mm} &
ReSTE \hspace{-4mm} &
BiDM \hspace{-4mm} &
\\
\includegraphics[width=0.149\textwidth]{figs/visual/0834_BBCU.png} \hspace{-4mm} &
\includegraphics[width=0.149\textwidth]{figs/visual/0834_XNOR.png} \hspace{-4mm} &
\includegraphics[width=0.149\textwidth]{figs/visual/0834_ReactNet.png} \hspace{-4mm} &
\includegraphics[width=0.149\textwidth]{figs/visual/0834_ours.png} \hspace{-4mm}
\\ 
BBCU \hspace{-4mm} &
XNOR \hspace{-4mm} &
ReactNet  \hspace{-4mm} &
BiMaCoSR (ours)  \hspace{-4mm}

\\
\end{tabular}
\end{adjustbox}
%% one row ends

\end{tabular}}
\vspace{-3.5mm}
\caption{\small{Visual comparison for image SR. We compare our proposed BiMaCoSR with current competitive binarization methods and the full-precision (FP) model. The visual results illustrate that BiMaCoSR gains rich details and reasonable textures.}}
\label{fig:sr_vs_2}
\vspace{-5mm}
\end{figure*}

\vspace{-0.5mm}
\textbf{Restoration Results.}
The quantitative comparison results are shown in Table~\ref{tab:exp-comp-performance}.
We can obtain the following observations.
(1) The results in Table~\ref{tab:exp-comp-performance} demonstrate clear advantage over competing methods in both full-reference and non-reference metrics on three benchmark datasets.
(2) In some situations, such as PSNR and LPIPS on RealSR, BiMaCoSR can even surpass SinSR and ResShift.
We attribute the excellence to the LRMB and SMB, which transmit the full-precision information in a parameter-efficient way.
(3) Most binarization methods outperform the baseline model and ReActNet presents competing performance especially on DRealSR.
(4) The full-precision models, \ie SinSR and ResShift, are excellent on perceptual metrics, such as DISTS and CLIP-IQA+.
Whereas, the multi-step models' performance on PSNR and SSIM is on the lower side.
This phenomenon means that compression on DM leads to disappearance of texture and details.
However, their performance on PSNR and SSIM is only slightly affected.

\vspace{-1mm}
\textbf{Efficiency Comparison.} Efficiency comparison results are provided in Table~\ref{tab:exp-comp-efficiency}.
After the distillation from multi-step to one-step, the FLOPs are significantly reduced.
Furthermore, compressing the model to 1 bit makes the model tiny and fast.
In comparison with SinSR, the compression ratio is 27.45$\times$ and the speedup ratio is 23.81$\times$.
Compared with current SOTA binarization methods, we keep almost the same parameters but take much less FLOPs on one-step DM, due to the calculation efficient design.
To conclude, our proposed BiMaCoSR gains remarkable compression ratio and speedup ratio and achieves outstanding performance.

\begin{table*}
\centering
\small
\subfloat[\small  Break down ablation. \label{tab:exp-abla-break}]{
    \scalebox{0.95}{
    \setlength{\tabcolsep}{3.2mm}
    % \resizebox{\textwidth}{!}{%
    \begin{tabular}{l|cccccccc}
        \hline
        \toprule[0.15em]
        \rowcolor{colorhead} Branch  & PSNR $\uparrow$   & SSIM $\uparrow$  & LPIPS $\downarrow$ & MANIQA $\uparrow$& FID $\downarrow$ & CLIP-IQA+ $\uparrow$ & FLOPs (G)        & Param (M)\\
        \midrule[0.15em]
                             BMB       & 26.41 & 0.7408 & 0.4141 & 0.3704 & 110.15 & 0.4325 & 0.78 & 3.69 \\
                             +LRMB     & 26.95 & 0.7718 & 0.3400 & 0.3937 & 88.72  & 0.4663 & 1.83 & 4.98 \\
                             +LRMB+SMB & 26.84 & 0.7698 & 0.3375 & 0.4034 & 86.09  & 0.4800 & 1.83 & 4.98 \\
        \bottomrule[0.15em]
    \end{tabular}
    \vspace{-2mm}
    % } % resize box
    }}\vspace{-0mm}\\

\subfloat[\small  Ablation study on losses. \label{tab:exp-abla-loss}]{
        \scalebox{0.95}{
        \setlength{\tabcolsep}{2.3mm}
        \begin{tabular}{l|cccc}
            \hline
            \toprule[0.15em]
            \rowcolor{colorhead} Loss  & PSNR $\uparrow$   & SSIM $\uparrow$  & LPIPS $\downarrow$ & CLIP-IQA+ $\uparrow$ \\
            \midrule[0.15em]
            Distill loss               & 26.83 & 0.7698 & 0.3375 & 0.4800 \\
            SinSR loss                 & 26.37 & 0.7466 & 0.4029 & 0.4273 \\
            \bottomrule[0.15em]
            \end{tabular}}}\hfill%\hspace{2.0mm}
\subfloat[\small  Ablation study on LRMB initialization. \label{tab:exp-abla-lorainit}]{
    \scalebox{0.95}{
    \setlength{\tabcolsep}{2.0mm}
    \begin{tabular}{l|cccc}
        \hline
        \toprule[0.15em]
        \rowcolor{colorhead} Initialization  & PSNR $\uparrow$   & SSIM $\uparrow$  & LPIPS $\downarrow$ & CLIP-IQA+ $\uparrow$ \\
        \midrule[0.15em]
                             Zero + Random   & 26.88 & 0.7660 & 0.3497 & 0.4674 \\
                             SVD             & 26.84 & 0.7698 & 0.3375 & 0.4800 \\
        \bottomrule[0.15em]
    \end{tabular}}}\hfill%\hspace{5.3mm}
\\
\vspace{-0mm}
\subfloat[\small  Ablation study on the rank of LRMB. \label{tab:exp-abla-rank}]{
    \scalebox{0.95}{
    \setlength{\tabcolsep}{1.0mm}
    \begin{tabular}{l|cccccc}
        \hline
        \toprule[0.15em]
        \rowcolor{colorhead} Rank  & PSNR $\uparrow$   & SSIM $\uparrow$  & LPIPS $\downarrow$ & CLIP-IQA+ $\uparrow$ & FLOPs (G)        & Param (M)        \\
        \midrule[0.15em]
        4  & 26.29 & 0.7383 & 0.4197 & 0.4411 & 1.31 & 4.37 \\
        8  & 26.84 & 0.7698 & 0.3375 & 0.4800 & 1.83 & 4.98 \\
        12 & 26.97 & 0.7695 & 0.3400 & 0.4766 & 2.32 & 5.60 \\
        16 & 26.72 & 0.7625 & 0.3442 & 0.4971 & 2.83 & 6.21 \\
        \bottomrule[0.15em]
        \end{tabular}}}
  \hfill%\hspace{5.3mm}
\subfloat[\small  Ablation study on SMB initialization. \label{tab:exp-abla-chanskipinit}]{
    \scalebox{0.95}{
    \setlength{\tabcolsep}{1.0mm}
    \begin{tabular}{l|cccc}
        \hline
        \toprule[0.15em]
        \rowcolor{colorhead} Initialization & PSNR $\uparrow$   & SSIM $\uparrow$  & LPIPS $\downarrow$ & CLIP-IQA+ $\uparrow$ \\
        \midrule[0.15em]
                            Zero initial & 26.72 & 0.7628 & 0.3561 & 0.4639 \\
                            Uni-shortcut & 25.69 & 0.7113 & 0.5105 & 0.4008 \\
                            Sparse skip  & 26.84 & 0.7698 & 0.3375 & 0.4800 \\
        \bottomrule[0.15em]
        \end{tabular}}}
\label{table:ablation}
\vspace{-3.5mm}
\caption{\small Ablation studies on branches, loss, rank of LRMB and initialization of LRMB and SMB. The experiments is tested on RealSR. The comprehensive results demonstrate the robustness and efficient performance of our proposed BiMaCoSR.}
\vspace{-5.5mm}
\end{table*}

\vspace{-1mm}
\textbf{Visual Results.}
We present visual comparison on challenging cases in Fig.~\ref{fig:sr_vs_2}.
One typical challenging case is the tiny and dense structures, such as hairs, grass, tiles, and faces.
It is struggling for previous binarization methods to restore image details especially in challenging cases.
On the contrary, our BiMaCoSR is able to recover results with sharper edges and richer textures.
For example, in 0809, BiMaCoSR reconstructs the hairs on the nose while other methods just output rough color blocks.
And in 0831, BiMaCoSR restores the woman's facial structures and expression while other methods smooth the organs to the same color.
In 0834, BiMaCoSR could successfully recover the tiles' texture and most binarization methods fail.
What's more, the difference between BiMaCoSR and the FP model (SinSR) exists but is minimal.
To conclude, our proposed BiMaCoSR surpasses other methods according to visual comparison.
Additionally, we provide more visual results and corresponding analysis in the supplementary material for further comparison.

\vspace{-2mm}
\subsection{Ablation Study}
\vspace{-2mm}
In this section, we adopt RealSR as the test set and other training settings are the same as Sec.~\ref{sec:training-settings}.
To test the effectiveness and robustness, we conduct five key ablation experiments, including the break down ablation, loss function, rank of low rank branch, initialization of LRMB, and initialization of SMB. 
What's more, detailed analysis is also provided in the following sections.
These ablation studies demonstrate the robustness and efficiency of our LRMB, SMB, and their corresponding initialization methods.


\textbf{Break Down Ablation.}
Table~\ref{tab:exp-abla-break} shows results of the break down ablation.
With only BMB, the model could be extremely compressed but the performance is somewhat on the low side.
With LRMB, though the number of parameters increases to 1.83G, the performance also improves on all metrics and the FLOPs only increase to 4.98M.
After adding SMB, the overhead of parameters and storage can be neglected.
The performance on all perceptual metrics improves while both PSNR and SSIM drop slightly. 


\textbf{Loss Function.}
The loss functions of SinSR play a critical role in FP model.
However, these loss functions are not suitable in the binarized situation.
After removing unnecessary parts from the SinSR loss, we leave only the distillation loss and the results are shown in Table~\ref{tab:exp-abla-loss}.
With only the distillation loss, BiMaCoSR gains improvement on both distortation metrics and perceptual metrics.

\textbf{Rank of LRMB.}
The rank of LRMB significantly influences the complexity while the performance is not consistent with increase of complexity.
As shown in Table~\ref{tab:exp-abla-rank}, model with rank of 16 performs best only on CLIP-IQA+ while model with rank of 8 performs best and takes acceptable parameters. 
We think this is because the LRMB with high rank may influence high frequency generated by BMB, making the training unstable and leading to worse results.
Therefore, we ultimately set the rank of LRMB to 8.

\textbf{Initialization of LRMB.}
Conventional LoRA's initialization method is set the first matrix with random values and the second matrix with zero values.
We compare this initialization method with our proposed SVD initialization and the result is shown in Table~\ref{tab:exp-abla-lorainit}.
To conclude, our method outperforms conventional methods on SSIM, LPIPS, and CLIP-IQA+.
We attribute it to the decoupling of transmitting low frequency information and generating high frequency.
To conclude, our proposed SVD allows better performance.


\textbf{Initialization of SMB.}
We search three popular ways to initialize SMB, including (1) random position and zero initialization, (2) Uni-shortcut~\cite{xu2022bimlp}, and (3) our proposed sparse skip.
The experiment results in Table~\ref{tab:exp-abla-chanskipinit} show that our sparse skip initialization consistently enjoys the best performance on all four metrics.
This improvement compared with other two initialization methods strongly supports the effectiveness of decoupling and absorption.

\begin{figure}
    \centering
     \includegraphics[width=1\linewidth]{figs/norm.pdf}
     \vspace{-5mm}
    \caption{The proportion of high frequency information generated by three branches. The high frequency information mainly comes from BMB, which obeys our assumption.}
    \label{fig:abla-branch-freq}
    \vspace{-8mm}
\end{figure}

\vspace{-3mm}
\subsection{Visualization Analysis.}
\vspace{-3mm}
In Sec.~\ref{sec:method}, we assume that LRMB and SMB are in charge of transmitting low frequency information while BMB generates high frequency after decoupling.
To validate this assumption, we visualize the proportion of high frequency generated by three branches in first 50 Conv layers and the result is shown in Fig.~\ref{fig:abla-branch-freq}.
On average, high frequency information is mainly generated by BMB, accounting for around 70\%.
With more parameters, LRMB also provides high frequency information in some layers but overall proportion is relative low.
SMB only absorbs the extreme values and indeed carries little high frequency information.
Fig.~\ref{fig:abla-branch-freq} shows that division of work is clear and our designs in LRMB, SMB and BMB obey our assumption.


\vspace{-2mm}
\section{Conclusion}
\vspace{-1mm}
In this paper, we propose the BiMaCoSR, a binarized SR diffusion model with only one inference step.
Detailedly, we first propose LRMB and SVD initialization to decouple the effect of binarized branch and deliver low frequency information.
Furthermore, we propose SMB and sparse initialization to absorb the extreme values and provide high rank representations.
Comprehensive comparison experiments demonstrate the SOTA restoration ability of the proposed BiMaCoSR.
Extensive ablation studies exhibit the efficiency and robustness of both LRMB and SMB.
In the future, we will focus on the combination of pruning and binarization on one-step diffusion models for further compression.

\bibliography{ref}
\bibliographystyle{icml2025}
\end{document}