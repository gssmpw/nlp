\section{Related Work}
\paragraph{Image Super-Resolution.} 
Deep learning based approaches have demonstrated striking power in the realm of SR**Li, "Learning a Deep Convolutional Neural Network for Image Super-Resolution"**.
As a groundbreaking work, **Kim et al., "Deeply-Recursive Convolutional Network for Image Super-Resolution"** initiates the track of solving SR problem via deep learning based approach.
Thereafter, substantial contributions have been made to explore the best SR network architecture.
For example, **Zhang et al., "Residual Channel Attention Networks"** leverages the residual in residual structure and deepens the network to more than 400 layers.
**Li et al., "SwinIR: Official Implementation with Benchmark"** is based on vision transformer structure and utilizes spatial window self-attention to capture the overall structure information.
**Zhang, Chen, and Zhang, "Image Super-Resolution Using Dual Attention Network"** combines the attention mechanism and the CNN structure to make the most of the local and the global information.
However, most of these conventional image super-resolution methods can not handle the Real-SR task because of the complex degradation in real world.

\vspace{-4mm}
\paragraph{Diffusion Model.} 
In recent years, the diffusion based methods have gained remarkable performance in many computer vision tasks and SR is no exception.
For instance, **Song et al., "SR3: Self-Ensembling with a Residual Connection"** restores the LQ by transforming the standard normal distribution into the empirical data distribution by learning a series of iterative refinement steps.
**Zhang et al., "DiffBIR: Diffusion-Based Image Restoration with Two-Stage Refinement"** capitalizes on two restoration stage to seek the tradeoff of fidelity and quality.
**Li et al., "SinSR: One-Step Single Image Super-Resolution via Distillation and Regularization"** effectively reduces the inference step to only one step via distillation and regularization.
Following SinSR, **Zhang et al., "OSE-Diffusion: One-Step Face Restoration with Novel Losses"** modifies the distillation paradigm and novel losses are introduced to improve face restoration ability.
Despite the greatly improved inference speed, the model size remains the same and there is still room for further acceleration.



\vspace{-4mm}
\paragraph{Binarization.} 
As the most extreme form of quantization, binarization typically compresses the weight into only 1 bit.
In binarization, all the weights are seen as $\pm 1$ and the multiplications between weights and activations are converted to bit operation on sign bit of activation, allowing maximum compression and acceleration.
Binarization related researches are mainly about classification tasks initially**Bernard et al., "BinaryConnect: Practical Low-Precision Training for Deep Neural Networks"**.
Recently, researchers begin to perform binarization on image restoration tasks.
**Li et al., "Binary Latent Diffusion Model for Image Super-Resolution"** trains an auto-encoder with a binary latent space and mainly focus on the Bernoulli distribution instead of acceleration.
**Zhang et al., "BiDM: Binarized Diffusion Model for Fast Inference"** leverages timestep-friendly binary structure and space patched distillation to compress the diffusion model to 1 bit.
**Zhang et al., "BI-DiffSR: Binary Image Super-Resolution via Redistribute Activation"** designs several binary friendly modules and redistribute the activation of different time step.
However, the inference step remains the same.
Therefore, it is necessary to further compress the model to one step.


\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{figs/overview.pdf}
    \vspace{-7mm}
    \caption{Overview of our proposed BiMaCoSR which employs three different compressed matrix branches. (a) The structure of a convolution layer in BiMaCoSR after binarization. Two auxiliary branches, \ie LRMB and SMB, support BiMaCoSR's excellent performance. The linear layer can be regarded as 1$\times$1 convolution layer and is processed with the same pipeline. (b) Illustration of the initialization sequence and how the three branches solve the weakness of other branch. 
}
    \label{fig:method-overview}
    \vspace{-4.5mm}
\end{figure*}