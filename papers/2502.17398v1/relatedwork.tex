\section{Related works}
Heterogeneous \glspl{soc}, often referred to as \glspl{apu}, early adopted shared virtual addressing via \gls{iommu} hardware units to enhance programmability and memory usage. Several works evaluated the key performance bottlenecks~\cite{amd_benchmarking_2016}~\cite{host_congestion_2022} induced by the additional IO page walking required to translate IO virtual addresses to physical addresses. For example, \glspl{gpu} typically rely on deep cache-hierarchy and latency tolerant \gls{simt} to maximize compute utilization. Upon L2-cache misses, \glspl{gpu} access \gls{dram} via a shared \gls{llc} with the \gls{cpu}. Conversely, \gls{mimd} accelerator clusters typically rely on scratchpad memories and \gls{dma}-engines to refill contiguous data chunks from \gls{dram}~\cite{hero_archi_2018}. In this scenario, different \gls{iommu} architectures have been proposed over the years, such as multi-level IO\glspl{tlb}~\cite{multi_lvl_tlb_2017}, enabling sharing of commonly accessed \glspl{pte} among different accelerators, significantly increasing performance on shared data access patterns.

On the other end of the spectrum, previous works also explored IOMMU-less solutions to enable shared virtual addressing to improve flexibility and reduce area and power consumption. A former study from Kurth et al.~\cite{hero_scalable_iommu_2018} proposes a hybrid architecture for heterogeneous systems-on-chip based on programmable accelerators, in which page table walking and prefetching are executed by software threads running on the accelerator cluster itself. This solution enables high versatility by tuning the proportion of page walker threads per compute thread for each kernel. Nevertheless, on systems as the one described in section~\ref{sec:platform}, software-based page table walking could waste precious computational resources such as the double-precision \gls{fpu} coupled to each \gls{pe}. To reduce energy consumption,~\cite{active_forwarding_2018} proposes a hardware-managed active forwarding from the host data cache to the accelerator's \gls{spm}. This solution is reported to reduce energy utilization significantly for simple dataflow accelerators. However, active forwarding is limited to problem sizes that can entirely fit within the (typically limited) on-chip accelerator scratchpad memory. While these limitations in terms of flexibility can be tolerated on certain \glspl{dsa} with highly predictable execution patterns, accelerators based on programmable \glspl{pe} justify the use of dedicated hardware \gls{iommu} between the \gls{pmca} and the system bus.

Within the context of \gls{iommu}-based approaches, Ben-Yehuda et al.~\cite{iommu_prize_2007} identified two types of performance overheads: \gls{cpu} utilization and memory bandwidth. In the case of low memory bandwidths, hardware solutions have been proposed to limit the IO\gls{tlb} wall. \gls{tlb} coalescing~\cite{colt_coalescing_2012} can be used in the host \gls{mmu} to reduce the number of \glspl{ptw} without the need of superpages. This method can be similarly applied to \glspl{iommu}. In~\cite{iotlb_coalescing_2018}, the authors propose to exploit the locality of IO page table entries in cache lines for efficient coalescing. To adress the problem of \gls{cpu} overhead. In this case, software solutions can be used to reduce the cost of mapping and un-mapping \gls{dma} buffers. In~\cite{iommu_allocator_2018}, the authors propose a \gls{dma} allocator specially designed to reuse \gls{iommu} mappings.

In this work, we focus on the RISC\nobreakdash-V \gls{iommu} proposed in~\cite{pinto_iommu_2023} coupled with a scratchpad-based programmable accelerator cluster with \gls{dma} engine. We benchmark the page translation overhead on the accelerator under different memory latencies to exhibit the high cost of shared virtual addressing. We also show that by integrating a last-level cache shared by the \gls{iommu} and the host, page walking overhead is highly reduced, making shared virtual addressing suitable without custom IO\gls{tlb} coalescing or prefetching. However, to avoid lowering the accelerator's bandwidth, the shared \gls{llc} must bypass \gls{dma} transactions. This bypass is implemented in platforms such as ESP~\cite{heterogeneous_esp_2022} to enable high bandwidth within network-on-chips. Our work shows that such memory hierarchy is also required to implement shared virtual addressing on heterogeneous platforms efficiently.

% ==========================================================================================
% ========== Conclusion ========================================================
% ==========================================================================================