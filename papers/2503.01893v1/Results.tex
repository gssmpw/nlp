\chapter{Evaluation and Results} 
%In what follows, we describe the evaluation process we performed for inflation prediction based on HRNNs. 
We evaluate HRNN and compare it with well-known baselines for inflation prediction  and covid19 as well as some alternative machine learning approaches.   
We use the following notation: Let $x_t$ be the CPI log-change rate at month $t$ or be the confirmed cases / deaths at day t.
We consider models for $\hat{x}_{t}$ - an estimate for $x_t$ based on historical values.
Additionally, we denote by $\varepsilon_{t}$ the estimation error at time $t$.
In all cases, the $h$-horizon forecasts were generated by recursively iterating the one-step forecasts forward. 
Hyper-parameters were set through a 10-fold cross-validation procedure.

\section{Baseline Models} \label{baslines}
We compare HRNN with the following CPI prediction and covid19 baselines:

\begin{enumerate}

\item{\bf Autoregression (AR) -} The AR($\rho$) estimates $\hat{x}_{t}$ based on the previous $\rho$ timestamps as follows: $\hat{x}_{t}= \alpha_0 + \left(\sum_{i=1}^{\rho} \alpha_{i} x_{t-i}  \right)+\varepsilon_{t}$, where $\{ \alpha_i \}_{i=0}^{\rho}$ are the model's parameters. 


\item{\bf Random Walk (RW) -} We consider the RW($\rho$) model of \cite{atkeson2002}. RW($\rho$) is a simple, yet effective, model that predicts next timestamps as an average of the last $\rho$ timestamps by:  $\hat{y}_{t}=\frac{1}{\rho} \sum_{i=1}^{\rho} x_{t-i}+\varepsilon_{t}$.


\item{\bf Random Forests (RF) - } The RF($\rho$) model is an ensemble learning method which builds a set of decision trees~\citep{song2015decision} in order to mitigate overfitting and improve generalization~\citep{breiman2001random}. 
At prediction time, the average prediction of the individual trees is returned. The inputs to the RF($\rho$) model are the last $\rho$ samples and the output is the predicted value for timestamp.

\item{\bf Gradient Boosted Trees (GBT) - } The GBT($\rho$) model \citep{friedman2002stochastic} is based on an ensemble of decision trees which are trained in a stage-wise fashion similar to other boosting models \citep{schapire1999brief}. Unlike RF($\rho$) which averages the prediction of several decision trees, the GBT($\rho$) trains each tree to minimize the remaining residual error of all previous trees. At prediction time, the sum of predictions of all the trees is returned.  The inputs to the GBT($\rho$) model are the last $\rho$ samples and the output is the predicted value for the next timestamps.


\item{\bf Fully Connected Neural Network (FC) -} The FC($\rho$) model is a fully connected neural network with one hidden layer and a ReLU activation~\citep{ActivationFunctions}. %given by $\hat{x}_{t}=\sigma(W X+b)+\varepsilon_{t}$, where $X$ is a vector of the last $\rho$ CPI samples, namely $X=\{ x_{t-1}, x_{t-1}, \dots, x_{t-\rho} \}^{\intercal}$, $W$ and $b$ are the model's parameters,  
%and $\sigma$ represents a non linearity function (i.e., the ReLU function \cite{ActivationFunctions}). 
The output layer employs no activation to formulate a regression problem with a squared loss optimization. 
The inputs to the FC($\rho$) model are the last $\rho$ samples and the output is the predicted value for the next timestamps.

\item{\bf Deep Neural Network (Deep-NN) - } 
The Deep-NN($\rho$) model is a deep neural network consisting of 10 layers with 100 neurons as in~\cite{olson2018modern}, which was shown to perform well for inflation prediction~\citep{goulet2020bag}. We used the original set-up of \cite{olson2018modern} and tuned its hyper-parameters as follows: learning rate was set to $lr=0.005$, training lasted 50 epochs (instead of 200), and the ELU activation functions~\citep{ELU} were replaced by ReLU activation functions. These changes yielded more accurate predictions, hence we decided to include them in all our evaluations.
The inputs to the Deep-NN($\rho$) model are the last $\rho$ samples and the output is the predicted value for the next timestamps.

%\noam{need to add explanation: RMSE FC 100 on 10 noramlize + new learning rate.  10 layers with 100 neurons in Wyner at al. 2016, which is well-performing for inflation in Goulet Coulombe (2020)}



\section{Ablation Models}
In order to demonstrate the contribution of hierarchical component of the HRNN model, we conducted an ablation study that considered ``simpler'' alternatives to HRNN based on GRUs without the hierarchical component: 
\begin{enumerate}

\item{\bf Single (S-GRU) -} The S-GRU($\rho$) is a single GRU unit that receives the last $\rho$ values as inputs in order to predict the next value. In GRU($\rho$), a single GRU is used for all the time series that comprise the CPI hierarchy. This baseline utilizes all the benefits of a GRU but assumes that the different components of the CPI behave similarly and a single unit is sufficient to model all the nodes.   %The model is given by the following formula: $\hat{y}_{t}=h_{t-1}+t \varepsilon_{t}$ where $h_{t-1}$ is the output value of a single scalar GRU layer with input shape of $\left[\begin{array}{ll}{d X} & {1}\end{array}\right]$  and $d$ is the time dimension. In other words the inputs are $x_{t-d}, x_{t-d+1} \ldots x_{t-1}$.


\item {\bf Independent GRUs (I-GRUs) -}
In I-GRUs($\rho$), we trained a different GRU($\rho$) unit for each CPI node. 
The S-GRU and I-GRU approaches represent two extremes: The first attempts to model all the CPI nodes with a single model, while the second treats each node separately. 
I-GRUs($\rho$) is equivalent to a variant of HRNN that ignores the hierarchy by setting the precision parameter $\tau_{\theta_n}=0 ; \; \forall n \in \mathcal{I}$. Namely, this is a simple variant of HRNN that trains independent GRUs, one for each index in the hierarchy.


\item{\bf K-Nearest Neighbors GRU (KNN-GRU) -}
In order to demonstrate the contribution of the hierarchical structure of HRNN, we devised the KNN-GRU($\rho$) baseline.
KNN-GRU attempts to utilize information from multiple Pearson-correlated CPI nodes without employing the hierarchical informative priors. Hence, KNN-GRU presents a ``simpler'' alternative to HRNN that replaces the hierarchical structure with elementary vector GRUs as follows:
First, the $k$ nearest neighbors of each CPI node were found using the Pearson correlation measure. 
Then, separate vector GRU($\rho$) units were trained for each CPI aggregate along its $k$ most similar nodes using the last $\rho$ values of node $n$ and its $k$-nearest nodes. 
%Formally, for a specific node $n$ in the CPI hierarchy, we denote by $n_i$ it's $i$'th most similar node according to the Pearson correlation measure.
%The vector GRU($\rho$) units were trained using an augmented vector time series $V^n=[X^n,X^{n_1},...,X^{n_k}]$, where $V^n$ is simply a concatenation of the top $k$ most similar nodes. 
% Formally, for each time series $X^n$, belonging to the $n$'th CPI node, we denote by $X^{n_i}$ its 
By doing so, the KNN-GRU($\rho$) baseline was able to utilize both the benefits of GRU units together with relevant information that comes from correlated nodes. 




%where $m_i$ is among the top $k$ indexes having the highest absolute Pearson Correlation Coefficient value with index $n$.$

%In this variant we learned a different GRU unit for each index $n$, however it receives time series of vectors, each vector in the form of $[x^t_{m_1},...,x^t_{m_k}]$, where $m_i$ is among the top $k$ indexes having the highest absolute Pearson Correlation Coefficient value with index $n$. This method models the relations between products that are not necessarily similar or share hierarchical relations.

\end{enumerate}
%Note that we have also experimented with ``deeper'' networks of more than one layer, but those did not yield a noticeable improvement. We attribute this to the relatively low number of training examples and features inherent to the CPI prediction problem.

\section{Evaluation Metrics}
Following \cite{faust2013forecasting} and \cite{AparicioBertolotto2020a}, we report results in terms of three evaluation metrics: 
\begin{enumerate}
    \item{\bf Root Mean Squared Error (RMSE) -} 
    The RMSE is given by:
    \begin{equation}
        RMSE=\sqrt{\frac{1}{T}\sum_{t=1}^T \left(x_t- \hat{x}_t \right)^2},
    \end{equation}
     where $x_t$ are the monthly change rate for month $t$, and $\hat{x}_t$ are the corresponding predictions.
    
    \item{\bf Pearson Correlation Coefficient -} The Pearson correlation coefficient $\phi$ is given by:
    \begin{equation}
        \phi = \frac{COV(X_T,\hat{X}_T)}{\sigma_{X} \times \sigma_{\hat{X}}},    
    \end{equation}
        where $COV(X_T,\hat{X}_T)$ is the covariance between the series of actual values and the predictions, and $\sigma_{X_T}$ and $\sigma_{\hat{X}_T}$ are the standard deviations of the actual values and the predictions, respectively.
    
\item{\bf Distance Correlation Coefficient -} 
In contrast to the Pearson correlation measure, which detects linear associations between two random variables, the distance correlation measure can also detect nonlinear correlations~\citep{SzekelyRizzoBakirov2007a,distanceCorrelation}. 
%Distance correlation  is a measure of dependence between two paired random vectors of arbitrary, not necessarily equal dimension. 
%which can only detect linear association between two random variables and Spearman's rho or Kendall's tau which both measure monotonic relationship but cannot capture all types of non-linear dependency. 
The distance correlation coefficient $r_d$ is given by:
    \begin{equation}
    \label{eq:distance_corr}
        r_d= \frac{\operatorname{dCov}(X_T, \hat{X}_T)}{\sqrt{ \operatorname{dVar}(X_T) \times \operatorname{dVar}(\hat{X}_T)}},
    \end{equation}
where $\operatorname{dCov}(X_T, \hat{X}_T)$ is the distance covariance between the series of actual values and the predictions, and $\operatorname{dVar}(X_T)$ and $\operatorname{dVar}(\hat{X}_T)$ are the distance variance of the actual values and the predictions, respectively.

    
\end{enumerate}

\section{Results} 
\label{subsec:eval} 
The HRNN model is unique in its ability to utilize information from higher levels to lower levels.
It is important to note that in this case, the HRNN model cannot utilize its hierarchical mechanism and has no advantage over the alternatives, so we do not expect it to outperform. 
In contrast, we see an advantage for the other deep learning models such as FC(4/14), Deep-NN(4) and GBT(14) that outperform the more ``traditional'' approaches.
\subsection{CPI Results}
In the CPI hierarchy in order to make predictions at lower levels. Therefore, we provide results for each level of the CPI hierarchy - overall 424 disaggregated indexes belonging to 8 different hierarchies.
For the sake of completion, we also provide results for the headline CPI index by itself
\input{TableFullResults}
\input{TableHeadResults}
Table~\ref{tab:allCPIResults} depicts the average results from all the disaggregated indexes in the CPI hierarchy. 
We present prediction results for horizons 0, 1, 2, 3, 4, and 8 months.
The results are relative to the $AR(1)$ model and normalized according to: $\frac{RMSE_{Model}}{RMSE_{AR\left( 1\right) }}$.
In HRNN we set $\alpha=1.5$, and the V-GRU($\rho$) models were based on $k=5$ nearest neighbors.
Table \ref{tab:allCPIResults}  demonstrate that different versions of the HRNN model consistently outperform the alternative models across all horizons in terms of CPI forecasting. Notably, HRNN's superiority over I-GRU highlights the importance of leveraging hierarchical information, proving HRNN's effectiveness over standard GRUs. Furthermore, the HRNN model also surpasses the various KNN-GRU models, emphasizing the significant role of informative priors based on the CPI hierarchy.
\input{TableResultsHRNN(4)ByHierarchy}

Table~\ref{tab:HRNNByHierarchy} depicts the results of HRNN(4), the best model, across all hierarchies ($1$-$8$, excluding the headline). Additionally, we included the results of the best ablation model, the I-GRU(4) model, for comparison. Results are averaged over all disaggregated components and normalized by the AR(1) model RMSE as before. As evident from Table~\ref{tab:HRNNByHierarchy}, the HRNN model shows the best relative performance at the lower levels of the hierarchy where the CPI indexes are more volatile and the hierarchical priors are most effective. 

Table ~\ref{tab:rmse-cpi-head}  summarizes these results. When considering only the prediction for the main index pricing. In this case, we do not observe much advantage for employing the HRNN model.

\subsection{Covid19 Results}
In Covid19 we provide results for each level of the covid19 - overall 222 contries belonging to 3 different hierachies: world, continent and country.
We also provide results for the world covid19 itself. 

\input{TableFullResultsCovid}
\input{TableHeadCovidResults}
Table~\ref{tab:allCovidResults} depicts the average results from all the disaggregated levels in the covid19 hierarchy. 
We present prediction results for horizons 0, 1, 2, 3, 7, and 14 days.
The results are relative to the $AR(1)$ model and normalized according to: $\frac{RMSE_{Model}}{RMSE_{AR\left( 1\right) }}$.
In HRNN we set $\alpha=0.5$, and the V-GRU($\rho$) models were based on $k=5$ nearest neighbors.
Table \ref{tab:allCovidResults} illustrate that the HRNN model similarly outperforms other models when applied to COVID-19 data, maintaining its superior performance at any horizon. The HRNN's advantages over I-GRU underscore the importance of incorporating hierarchical structures in the model. Additionally, HRNN's edge over the different KNN-GRU models highlights the effectiveness of using specific informative priors based on the COVID-19 hierarchy.

 Table \ref{tab:rmse-covid-head} summarizes these results. When considering only the prediction for the world level. In this case, it seems like the HRNN performance close to other models.

%\begin{figure}[htbp]
   % \centering
    {
        %\includegraphics[scale=0.6]{figures/FigsFromEliya/Tomatoes.png}
    }
    {

        %\includegraphics[scale=0.6]{figures/FigsFromEliya/Bread.png}
    }
    {

        %\includegraphics[scale=0.6]{figures/FigsFromEliya/Information technology commodities.png}
    }
%    \caption
    %{Examples of HRNN(4) predictions for disaggregated indexes.}
  %  \label{fig:examples}
%\end{figure}
