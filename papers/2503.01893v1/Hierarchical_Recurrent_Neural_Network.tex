\chapter{BiHRNN: Bi-Directional Hierarchical Recurrent Neural Network}

Hierarchical data refers to structured data organized into multiple levels, with each level representing a different degree of aggregation or detail. In the context of inflation, hierarchical data can range from the overall inflation index, such as the Consumer Price Index (CPI), down to disaggregated components like regional indices or individual product categories. This multi-level structure allows for analysis at varying levels of granularity, capturing relationships and dependencies across different layers. Such an approach is particularly valuable for inflation modeling, as changes at lower levels, such as specific goods or services, can propagate upward and influence aggregate measures, enabling more accurate and comprehensive forecasting.

\section{Hierarchical Recurrent Neural Networks}
Before delving into the specifics of the BiHRNN model, we provide a brief overview of its predecessor, the HRNN model \citep{BARKAN20231145}. The HRNN model is specifically designed to address the challenges of inflation forecasting in hierarchically structured datasets, where lower levels are often characterized by data sparsity and heightened volatility in change rates. To enhance predictions, the HRNN propagates information from parent categories to node categories within the hierarchy by employing hierarchical Gaussian priors. This approach connects each node’s parameters to its parent’s, allowing the model to share information across levels of aggregation. By leveraging parent-level information, the HRNN mitigates the effects of sparse or noisy data at finer levels and ensures consistency in forecasts across the hierarchy. Furthermore, it utilizes RNNs, specifically Gated Recurrent Units (GRUs) \citep{GRU}, which incorporate a feedback loop, enabling predictions to account for temporal dependencies. This combination of hierarchical priors and temporal modeling makes the HRNN particularly effective for capturing both cross-level interactions and dynamic patterns in inflation data \citep{RNNs_Book, RNN_evaluations}.

\subsection{Gated Recurrent Units (GRU)}
A Gated Recurrent Unit (GRU) is a type of Recurrent Neural Network (RNN) designed to capture long-term dependencies in sequential data by addressing the vanishing gradient problem common in traditional RNNs. GRUs use two gates—update and reset—to manage information flow. The update gate controls how much past information is passed along to future states, while the reset gate determines how much past information is forgotten, allowing GRUs to retain relevant information over longer sequences.

The following set of equations defines a GRU unit: 
\begin{equation}
\label{eq:gru}
\begin{split}
    z = & \sigma(x_tu^z + s_{t-1}w^z + b^z), \\
    r = & \sigma(x_tu^r + s_{t-1}w^r + b^r), \\
    v = & \tanh{(x_tu^v+(s_{t-1} \times r)w^v +b^v)}, \\
    s_t = & z \times v + (1-z)s_{t-1},
\end{split}
\end{equation}
where $u^z$, $w^z$ and $b^z$ are learned parameters governing the \emph{update gate} $z$, while $u^r$, $w^r$ and $b^r$ are the learned parameters for the \emph{reset gate} $r$. The candidate activation $v$ determined by the input $x_t$ and the previous output $s_{t-1}$, and is influenced by the learned parameters: $u^v$, $w^v$ and $b^v$. Finally, the output $s_t$ is a combination of the candidate activation $v$ and the previous state $s_{t-1}$ controlled by the \emph{update gate} $z$. Figure~\ref{fig:GRU} depicts an illustration of a GRU unit.

\begin{figure}[!htb]
    \centering
    %\subfigure
    {   \makebox[\textwidth]{
        \includegraphics[scale=0.6]{figures/GRU.pdf}}
    }
    \caption
    {An illustration of a GRU unit.}
    \label{fig:GRU}
     \floatfoot*{\textit{Each line represents a vector, connecting the output of one node to the inputs of others. Pink circles indicate point-wise operations, while yellow boxes represent learned neural network layers. When lines merge, it signifies concatenation; when a line forks, it means the vector is copied, with each copy directed to different destinations.}}
\end{figure}

GRUs form the foundational unit of the HRNN model, detailed in Section \ref{subsec:hrnn} as well as our BiHRNN model detailed in Section \ref{subsec:model}.

\subsection{HRNN}\label{subsec:hrnn}
Next, we proceed with a description of the HRNN model. 
The following notations apply to both HRNN and Bidirectional HRNN models:

Let \(\mathcal{I}=\{n\}_{n=1}^N\) represent dataset graph nodes, each associated with a parent \(\pi_n\). For node \(n\), \(x_t^n\) is its observed value at time \(t\), and \(X_t^n\) represents the sequence up to \(t\).
A parametric function \(g\) (a GRU node) predicts the next value in the sequence, learning parameters \(\theta_n\) to predict \(x_{t+1}^n\). Assuming Gaussian errors, the likelihood of the time series is modeled as a product of normal distributions, with \(\tau_n^{-1}\) as the error variance.
A hierarchical informative prior connects each node’s parameters to its parent’s, with the prior \( p(\theta_n|\theta_{\pi_n},\tau_{\theta_n}) \) using \(\tau_{\theta_n}\) as a precision parameter. Higher \(\tau_{\theta_n}\) values indicate stronger parameter connections between node \(n\) and its parent \(\pi_n\).
Instead of globally optimizing \(\tau_{\theta_n}\), the HRNN sets \(\tau_{\theta_n} = e^{\alpha + C_n}\), where \(\alpha\) is a hyperparameter and \(C_n\) is the Pearson correlation between node \(n\) and its parent \(\pi_n\). This ensures that node \(n\) stays close to \(\pi_n\) in parameter space, especially when correlation is high. For the root node, a non-informative Gaussian prior with zero mean and unit variance is used.

Let \(X=\{X_{T_n}^n\}_{n\in \mathcal{I}}\) represent all time series, \(\theta=\{\theta_n\}_{n\in \mathcal{I}}\) the GRU parameters, and \(\tau=\{\tau_n\}_{n\in \mathcal{I}}\) the precision parameters. Here, \(X\) is observed, \(\theta\) contains learned variables, and \(\tau\) is defined by \(\tau_{\theta_n}\).

Given the the likelihood of the observed time series and the priors aforementioned above, the posterior probability is then extracted and is formulated according to Equation~\eqref{eq:posterior}.
\begin{equation}
\label{eq:posterior}
\begin{split}
p(\theta|X,\tau) &= \frac{p(X|\theta,\tau)p(\theta)}{P(X)} \propto  
\prod_{n\in \mathcal{I}}\prod_{t=1}^{T_n}\mathcal{N}(x_t^n;g(\theta_n,X_{t-1}^n),\tau_n^{-1})\prod_{n\in \mathcal{I}}\mathcal{N}(\theta_n;\theta_{\pi_n},\tau_{\theta_n}^{-1}\mathbf{I}).
\end{split}
\end{equation}

HRNN optimization follows a \textit{Maximum A-Posteriori} (MAP) approach to find the optimal parameters \(\theta^*\) by maximizing the posterior probability.
\begin{equation}
\label{eq:obj}
\theta^*=\underset{ \theta}{\text{argmax}}\log p(\theta|X,\tau).
\end{equation}

The optimization is performed using stochastic gradient ascent on this objective. Figure~\ref{fig:HRNN} illustrates the HRNN architecture.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.25]{figures/Slide2.jpeg}
    \caption{Illustration of the HRNN Model.}
    \label{fig:HRNN}
    
\end{figure}


\section{Bidirectional HRNN Model}\label{subsec:model}

The Bidirectional HRNN (BiHRNN) builds upon the HRNN framework by enabling bidirectional information flow within hierarchical data structures, addressing a critical limitation of its predecessor. While the HRNN model effectively propagates information from parent categories to child categories, improving predictions for lower, more volatile levels, it does not leverage the potential benefits of propagating information in the reverse direction—from child categories back to their parents. Granular-level data often contains unique patterns or anomalies that can inform and refine higher-level predictions. By incorporating  bidirectional information flow, BiHRNN enhances the consistency and accuracy of predictions across all levels of the hierarchy.

The motivation for this extension lies in the interconnected nature of hierarchical data, particularly in the context of inflation forecasting. Economic indices at higher aggregation levels, such as national inflation rates, are directly influenced by fluctuations in disaggregated categories, such as specific goods, services, or regional indices. Without accounting for these influences, predictions at higher levels may miss critical insights embedded in lower-level data. By enabling information to flow upward, BiHRNN allows parent-level categories to benefit from the granularity and detail captured at the child level, thereby improving overall forecast accuracy and coherence across the entire hierarchy.

BiHRNN introduces a dual-constraint formulation, which integrates both top-down and bottom-up information; the constraints are applied during training to ensure structural consistency; and the enhanced loss function that governs its optimization, balancing accuracy across all hierarchical levels. These advancements make BiHRNN a more robust and versatile tool for forecasting in complex hierarchical datasets, particularly in domains like inflation modeling where cross-level interactions are critical.

\subsection{Bidirectional Information Flow}

BiHRNN is formulated as a risk minimization optimization problem. Instead of HRNN's informative prior, BiHRNN introduces two constraints on the models parameters. One ties the parameters of each time series to its parent's (similar to HRNN's prior) and the second ties the parameters of each of its' child series, with an appropriate weight:
\begin{itemize}
    \item \textbf{Parent-Node Constraint:} This constraint governs the relationship between the parameters of a node $n$ and its parent $\pi_n$. By aligning the parameters of node $n$ with those of its parent, this constraint ensures hierarchical consistency and allows top-down information to flow through the network.
    \item \textbf{Child-Node Constraint:} This constraint governs the relationship between node $n$ and its children $\eta_{i_n}$, where $\eta_{i_n}$ represents the $i$-th child of node $n$. This enables the node to aggregate information from its children, allowing bottom-up feedback to influence higher-level nodes.
\end{itemize}

By incorporating these two constraints, BiHRNN enables information to flow in both directions---\textit{downward} from parent to child and \textit{upward} from child to parent. This bidirectional approach significantly enhances the model’s ability to capture complex dependencies and interactions across hierarchical levels, leading to improved predictive performance.

\subsection{Customized Loss Function}

The information flow in BiHRNN is governed by a \textit{customized loss function} that balances prediction accuracy with hierarchical consistency. The loss function comprises three key components:

\begin{enumerate}
    \item \textbf{Mean Squared Error (MSE):}
    The primary objective of BiHRNN is to minimize prediction errors. This is achieved using the mean squared error:
    \begin{equation}
    \text{MSE} = \frac{1}{N} \sum \left( y - \hat{y} \right)^2
    \end{equation}
    where $y$ represents the observed values, $\hat{y}$ denotes the predicted values, and $N$ is the number of observations.
    
    \item \textbf{Parent Regularization ($l_{\text{parent}}$):}
    To ensure hierarchical coherence, the model penalizes the squared Euclidean distance between the parameters of a node $n$ and its parent $\pi_n$:
    \begin{equation}
    l_{\text{parent}} = \left( \theta_{\text{parent}} - \theta \right)^2
    \end{equation}
    This term enforces consistency between a node and its parent, ensuring that higher-level nodes influence lower-level nodes appropriately.
    
    \item \textbf{Child Regularization ($l_{\text{child}}$):}
    Similarly, the model incorporates the influence of child nodes through a weighted penalty:
    \begin{equation}
    l_{\text{child}} = \sum_{i \in \text{children}} w_i \left( \theta - \theta_{\text{child}} \right)^2
    \end{equation}
    where $w_i$ is a weight controlling the contribution of each child. This term allows the parameters of node $n$ to aggregate information from its children, enabling bottom-up information flow.
\end{enumerate}

The final loss function combines these components, weighted by hyperparameters $\lambda_1$ and $\lambda_2$ to control the relative importance of parent and child regularization:
\begin{equation}
\text{Loss}_{\text{BiHRNN}} = \frac{1}{N} \sum \left( y - \hat{y} \right)^2 
+ \lambda_1 \cdot l_{\text{parent}} + \lambda_2 \cdot l_{\text{child}}
\end{equation}

\subsection{Hyperparameter Tuning}

The hyperparameters $\lambda_1$ and $\lambda_2$ play a critical role in balancing the bidirectional information flow:
\begin{itemize}
    \item A higher $\lambda_1$ emphasizes top-down influence by prioritizing alignment with parent nodes.
    \item A higher $\lambda_2$ strengthens bottom-up feedback from child nodes.
\end{itemize}

Proper tuning of these hyperparameters is essential for optimizing the model’s performance while maintaining hierarchical consistency. To achieve this, Optuna\footnote{\url{https://optuna.org/}} was employed for hyperparameter tuning, utilizing its Tree-structured Parzen Estimator (TPE), a Bayesian optimization approach, to efficiently explore the hyperparameter space and ensure robust performance.

\subsection{Fixed Constraints During Training}

A key feature of the BiHRNN model is its use of \textit{fixed constraints} for the parent and child relationships throughout the training process. The procedure involves the following steps:
\begin{enumerate}
    \item \textbf{Pretraining the Base Model:} The HRNN (or a similar baseline model) is first trained independently for each category, and the learned weights are saved. These weights serve as the initial representations of the parent and child nodes.
    \item \textbf{Freezing the Weights:} Once the HRNN weights are trained, they are frozen and used as fixed constraints for the bidirectional model. This means that the weights representing parent and child relationships remain constant during the BiHRNN training process.
    \item \textbf{Stabilization and Regularization:} By leveraging frozen weights, the BiHRNN anchors predictions, ensuring that hierarchical relationships are preserved and reducing the risk of overfitting. This approach is particularly beneficial for datasets with limited samples or high variability, as it provides a stable foundation for the bidirectional model.
\end{enumerate}


\subsection{Summary}

The BiHRNN introduces a bidirectional approach to hierarchical modeling, leveraging dual constraints and a customized loss function to enable efficient information flow between nodes. By using fixed constraints and balancing top-down and bottom-up interactions, the model achieves superior forecasting accuracy and hierarchical coherence, establishing itself as a robust framework for hierarchical time series forecasting.

Figure ~\ref{fig:Bidirectional HRNN} below depicts the BiHRNN architechture.

\begin{figure}[H]
    \centering
    {   \makebox[\textwidth]{
        \includegraphics[scale=0.25]{figures/Slide1.jpeg}}
    }
        \caption
    {An illustration of our  BiHRNN Model.}
    \label{fig:Bidirectional HRNN}
\end{figure}

