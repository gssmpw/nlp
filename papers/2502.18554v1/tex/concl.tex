% \vspace{-2mm}

\section{Conclusion and Future Work}
\label{sec:conclusion}
% Longer version
% In this paper, we introduce {\pname}, an efficient framework for accelerating MPI collectives by leveraging error-bounded lossy-compression techniques. We carefully implement the compression-based collective operations (such as C-Allreduce, C-Scatter and C-Bcast) based on the {\pname} framework and perform a comprehensive evaluation using real-world scientific datasets and applications. 
% The experiments are conducted in terms of various problem/data sizes and execution scales.  
% We summarize our key observations as follows:
% \begin{itemize}
%      \item We utilize the accuracy-aware design and achieve the controlled data loss without sacrificing performance. In absolute terms, we can achieve up-to 2.8$\times$ performance improvement in the image stacking application and maintain excellent data quality.
%      \item We customize and optimize SZx to considerably mitigate the compression-related overhead and meet the needs of our collective framework.  
%      \item Experimental results demonstrate that our C-Allreduce operation outperforms the MPICH MPI\_Allreduce by up to \textbf{3.5$\times$}.
%      \item We demonstrate the generalizability of our approaches through C-Scatter and C-Bcast, which outperform the MPICH MPI\_Scatter and MPI\_Bcast by up to \textbf{7.7$\times$} and \textbf{9.7$\times$}, respectively. 
%  \end{itemize}

% In summary, our research addressed the issues of sub-optimal performance, lack of generality, and unbounded errors in lossy-compression-integrated MPI collectives, laying the foundation for future research in this area. Moving forward, we plan to expand our research by implementing more {\pname} based collectives and deploying our optimized frameworks on other hardware, such as GPUs and AI accelerators.
 
 %Our two proposed high-performance frameworks for compression-integrated MPI collectives, together with optimized and customized SZx, enable us to implement C-Allreduce, which outperforms the original Allreduce by up to \textbf{3.5$\times$} while preserving high data quality. We demonstrate the generalizability of our approaches through C-Scatter and C-Bcast, which outperform the original MPI\_Scatter and MPI\_Bcast by up to \textbf{7.7$\times$} and \textbf{9.7$\times$}, respectively. In summary, our research has addressed the issues of sub-optimal performance, lack of generality, and unbounded errors in lossy-compression-integrated MPI collectives, laying the foundation for future research in this area. Moving forward, we plan to expand our research by implementing more {\pname} based collectives and deploying our frameworks and optimizations on other hardware, such as GPUs and AI accelerators.


% In this paper, we introduce {\pname}, a novel design for lossy-compression-integrated MPI collectives that significantly improves performance with bounded errors. Our two proposed high-performance frameworks for compression-integrated MPI collectives, together with customized pipe-lined SZx, enable us to implement C-Allreduce, which outperforms the original Allreduce by up to \textbf{2.1$\times$} while preserving high data quality. We demonstrate the generalizability of our approaches through C-Scatter and C-Bcast, which outperform the original MPI\_Scatter and MPI\_Bcast by up to \textbf{1.8$\times$} and \textbf{2.7$\times$}, respectively. In summary, our research has addressed the issues of sub-optimal performance, lack of generality, and unbounded errors in lossy-compression-integrated MPI collectives, laying the foundation for future research in this area. Moving forward, we plan to expand our research by implementing more {\pname} based collectives and deploying our design on other hardware, such as GPUs and AI accelerators.

In this paper, we introduce {\pname}, a novel design for lossy-compression-integrated MPI collectives that significantly improves performance with bounded errors. Our two proposed high-performance frameworks for compression-integrated MPI collectives, together with optimized and customized pipe-lined {\fzlight}, enable us to implement Z-Allreduce, which outperforms the original Allreduce by up to \textbf{3.6$\times$} while preserving high data quality. We demonstrate the generalizability of our approaches through Z-Scatter and Z-Bcast, which outperform the original MPI\_Scatter and MPI\_Bcast by up to \textbf{5.4$\times$} and \textbf{8.9$\times$}, respectively. In summary, our research has addressed the issues of sub-optimal performance, lack of generality, and unbounded errors in lossy-compression-integrated MPI collectives, laying the foundation for future research in this area. Moving forward, we plan to expand our research by implementing more {\pname} based collectives and deploying our frameworks and optimizations on other hardware, such as GPUs and AI accelerators.