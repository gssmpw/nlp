% \vspace{-3mm}

\section{Introduction}
MPI collectives provide high-performance collective communication in distributed systems, making a significant impact on various research fields such as scientific applications, distributed machine learning, and others~\cite{awan2017s, wang2006pelegant, abadi2016tensorflow, ayala2019impacts, jain2019scaling, abdelmoniem2021efficient}. With the advent of exascale computing and deep learning applications, the demand for large-message MPI collectives has increased. For example, in image classification tasks, VGG19~\cite{simonyan2015very} and ResNet-50~\cite{he2016deep} have 143 million and 25 million parameters, respectively, with communication overheads of 83\% and 72\%~\cite{abdelmoniem2021efficient}. Therefore, optimizing MPI collectives for large messages has become essential~\cite{chunduri2018characterization, Bayatpour2018SALaR, patarasuk2009bandwidth}.


MPI collectives consist of both internode communication and intranode communication, and the former is often the major concern. The overall collective performance is usually limited by the efficiency of internode communication because of limited network bandwidth. Therefore, optimizing internode collective communication is critical to improving the overall performance of MPI collectives. This topic has been a focus of research for decades, with state-of-the-art algorithms achieving notable improvements. However, with the increasing demand for large-message MPI collectives, further optimization remains necessary~\cite{Alm05BlueGene, thakur2005optimization, patarasuk2009bandwidth}. Lossy compression \cite{Di2016SZ,Tao2017SZ,Zhao2020SZauto,Lindstrom2014ZFP} (rather than lossless compression \cite{Deutsch1996gzip, Gaillyzlib,Collet2015zstd}) is a promising solution to mitigate this MPI collective performance issue because of its ability to significantly reduce the message size.




Although lossy compression has been widely used to resolve many other scalability issues in high-performance computing, such as reducing memory footprint \cite{quant-compression}, reducing storage space \cite{nbody-compression,mdz}, and avoiding duplicated computation \cite{pastri}, only a few studies have explored its use in this direction, and all expose certain limitations. To elaborate, Zhou et al. \cite{Zhou2021GPUCOMPRESSION} proposed GPU-compression enhanced point-to-point communication by integrating MPC \cite{yang2015mpc} and 1D fixed-rate ZFP \cite{Lindstrom2014ZFP} into MVAPICH2 \cite{SHM-MVAPICH2}. Their approach, referred to as \textit{CPRP2P}, simply involved compressing the messages before transmission and decompressing them after reception, leading to significant performance overhead due to the non-negligible time required for compression and decompression. Meanwhile, Zhou et al. \cite{Zhou2022GPUCOMPRESSIONALLTOALL, Zhou2022HiPC} proposed several additional approaches to improve multiple MPI collectives using 1D fixed-rate ZFP \cite{Lindstrom2014ZFP} on GPUs. Their methods, however, focus on fixed-rate compression\footnote{Fixed-rate compression means that the lossy compression would be performed based on a user-specified fixed compression ratio.}, introducing two major limitations: (1) compression errors cannot be bounded, leading to an uncontrolled accuracy, and (2) the compression quality is considerably lower compared to the fixed-accuracy mode\footnote{Fixed-accuracy, also known as error-bounded lossy compression, compresses data based on a user-specified error bound.} in ZFP, as demonstrated by prior research~\cite{fraz, huang2023ccoll}.




The aforementioned limitations of compression-enabled MPI collective algorithms motivate us to develop a new efficient MPI collective framework which leverages lossy compression technique to significantly improve the MPI collective performance. However, this brings in three direct technical challenges. \circled{A} Devising a general framework that can effectively hide the communication cost and choose an appropriate timing to call lossy compression is non-trivial.
\circled{B} The data loss nature of the lossy compression brings up a critical concern on the accuracy of collective operations. \circled{C} Existing lossy compressors are not designed for the collective context, leading to suboptimal collective performance because of unnecessary overheads when they are directly applied in MPI collectives \cite{Di2016SZ, Tao2017SZ, Liang2018SZ, Yu2022SZx, Lindstrom2014ZFP}.


   
Our developed framework is named as compression-facilitated MPI collective framework ({\pname}), which can address the aforementioned limitations and challenges. To the best of our knowledge, this is the \textit{first-ever} framework that provides a general high-performance solution for compression-integrated MPI collectives. Moreover, this is the first accuracy-aware design, which ensures the accelerated collective performance with error-bounded lossy compression does not compromise data quality. To be more specific, our contributions include:
\begin{itemize}%[leftmargin=*]
    \item To address challenge \circled{A}, we introduce two efficient frameworks, which can significantly accelerate both types of MPI collectives. Specifically, the first framework notably diminishes the compression overhead in the collective data movement operations (e.g., Scatter, Bcast and All-gather), thereby achieving substantial performance improvement. The second one hides communication inside of compression in collective computation (e.g., Reduce-scatter), which in turn enhances the performance of collective computation. Moreover, these two frameworks can be combined together to speed up more advanced MPI collective operations such as All-reduce. 
    \item To address challenge \circled{B}, we devise several strategies to effectively control error propagation within the {\pname} framework. Specifically, we employ error-bounded lossy compression, regulate the number of compression operations, and develop an efficient method to resolve imbalances in collective communication caused by error-bounded lossy compression. Through in-depth mathematical analysis, we prove that our {\pname} framework achieves well-bounded data accuracy.
    \item To address challenge \circled{C}, we select the most suitable error-bounded lossy compressor for collective communication, considering factors such as compression throughput, compression ratio, and compression quality. We thoroughly analyze the newly developed {\fzlight} compressor~\cite{SZp} and compare it with SZx~\cite{Yu2022SZx}, which is used in the state-of-the-art C-Coll framework~\cite{huang2023ccoll}. After determining that {\fzlight} is the most suitable option, we customize it to meet the specific needs of collective communication. Specifically, we redesign the compression workflow of {\fzlight} and implement a pipelined version to overlap compression and communication. This optimization reduces the communication cost in our {\pname} framework by up to 3.3$\times$.
    \item To demonstrate the generality of our design, we integrate {\pname} into multiple collectives, including Allgather, Allreduce, Scatter, and Broadcast, and evaluate performance using real-world scientific application datasets. Experiments on 128 Intel Broadwell compute nodes from a supercomputer show that ZCCL-accelerated collectives achieve significant speedups over the CPRP2P and C-Coll baselines, outperforming MPI\_Allreduce, MPI\_Scatter, and MPI\_Bcast by up to 3.6$\times$, 5.4$\times$, and 8.9$\times$, respectively. Additionally, we validate the practical effectiveness of ZCCL-accelerated Z-Allreduce using a real-world use case (image stacking analysis), which shows up to 3.0$\times$ performance gain over MPI\_Allreduce while maintaining high data integrity and accuracy during collective operations.
    
\end{itemize}

The rest of the paper is organized as follows: we introduce background and related work in Section \ref{sec:background} and detail our design and optimization in Section \ref{sec-design_and_optimizations}. Evaluation results are presented in Section \ref{exp-setup-sec} followed by conclusion and future work in Section \ref{sec:conclusion}.
