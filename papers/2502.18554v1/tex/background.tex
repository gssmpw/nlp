\section{Background and Related Work}
\label{sec:background}
In this section, we discuss the background and related work. We first introduce MPI collective communication, followed by a discussion on high-speed lossy compressors and their integration with MPI implementations. The focus of our study is on lossy compression. This emphasis is due to the significantly lower compression ratios observed with lossless methods when applied to scientific datasets~\cite{Di2016SZ,Tao2017SZ,Huang-Exploring-Wavelet-Transform}.



% \vspace{-1mm}
\subsection{MPI Collective Communication}
\label{sec:MPI-Collectives}
There are many types of MPI collective operations, which can be divided into two sub-categories --- collective data movement and collective computation according to their communication patterns.
\subsubsection{Collective data movement}
Collective data movement includes gather, allgather, scatter, all-to-all, and so on. The gather operation collects the data from different processes and stores the collected data into the root process. In comparison, allgather stores the collected data to every participated process. As an opposite of gather, the scatter operation divides the data in the root process and sends the split data to all the processes. The all-to-all operation acts as the ``allscatter", which collectively scatters data on each process to each other. 


\subsubsection{Collective computation}
Allreduce/reduce are two popular collective computation operations. We use MPI\_SUM as an example to explain the working principle as it is frequently used. The reduce routine will sum up all the data entries from all the processes in the same communicator and store the sum into the root process. The Allreduce does the same thing but keeps a copy of the sum on every process in that communicator. Another widely-used operation is the reduce-scatter, which acts like a combination of the reduce and scatter: the reduced sum is scattered into all the processes. 



\subsection{High-speed Lossy Compressors}

Compression developers and scientific researchers have shown significant interest in high-speed lossy compression due to its ability to achieve high compression ratios. SZ~\cite{Di2016SZ, Tao2017SZ, Liang2018SZ} is an example of a fast, error-bounded lossy compressor, with performance comparable to other compressors like FPZIP~\cite{Lindstrom2006FPZIP} and SZauto~\cite{Zhao2020SZauto}. ZFP~\cite{Lindstrom2014ZFP} is another well-known compressor, offering relatively high compression ratios and even faster speeds than SZ. However, neither can match the speed of SZx~\cite{Yu2022SZx}, which achieves a compression throughput of 700-900 MB/s on CPUs~\cite{Yu2022SZx}. Recently, an ultra-fast compressor, {\fzlight}, has been proposed, but it has yet to be compared with the state-of-the-art SZx compressor used in the C-Coll framework~\cite{huang2023ccoll}. In Section \ref{sec:high-speed-compressor}, we thoroughly compare {\fzlight} with SZx in terms of compression throughput, ratio, and quality, demonstrating that {\fzlight} is the most suitable lossy compressor for MPI collective operations. Accordingly, we develop our customized compressor based on {\fzlight} for MPI collectives, which will be detailed in Section \ref{sec:Customize-fzl-reduce}.




% \vspace{-2mm}
\subsection{Lossy Compression-enabled MPI Implementations}
Researchers have shown interest in using lossy compression to improve MPI communication performance for years. Zhou et al. proposed GPU-compression enhanced point-to-point communication \cite{Zhou2021GPUCOMPRESSION}, and several optimized MPI collective operations \cite{Zhou2022GPUCOMPRESSIONALLTOALL, Zhou2022HiPC} using 1D fixed-rate ZFP~\cite{Lindstrom2014ZFP} on GPUs, but their solutions are either showcasing limited overlapping between compression and communication or subject to the fixed-rate mode compression, leading to the substandard compression quality, as demonstrated in~\cite{huang2023ccoll}. Conversely, our general framework can optimize the collective performance of all MPI collectives while maintaining controlled accuracy.


