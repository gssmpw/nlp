\section{Experimental Evaluation}\label{exp-setup-sec}

In this section, we present and discuss the evaluation results.


% \vspace{-2mm}

\subsection{Experimental Setup}
\label{sec:setup}

Since inter-node communication is the major bottleneck for collectives as discussed previously, we utilized a 128-node cluster with one process per node in our experiments. Each node is equipped with two Intel Xeon E5-2695v4 Broadwell processors. Furthermore, each NUMA node contains 64 GB of DDR4 memory, resulting in a total of 128 GB of memory per node. The nodes are interconnected via Intel Omni-Path Architecture (OPA), providing a maximum message rate of 97 million per second and a bandwidth of 100 Gbps.



\begin{table}[h]
\centering
\caption{Information of the application datasets.}
\label{tab:application-datasets}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{ccccc}
\hline
\textbf{Application} & \textbf{\# fields} & \textbf{Dims per field} & \textbf{Total Size} & \textbf{Domain} \\ \hline
\textbf{RTM~\cite{Kayum2020RTM}} & 151 & 849x849x235 & 95.3 GB & Seismic Wave \\
\textbf{NYX~\cite{nyx}} & 6 & 512x512x512 & 3.1 GB & Cosmology \\
\textbf{CESM-ATM~\cite{hurricane}} & 79 & 1800x3600 & 2.0 GB & Climate Simu. \\
\textbf{Hurricane~\cite{cesm}} & 13 & 100x500x500 & 1.3 GB & Weather Simu.\\\bottomrule 
\end{tabular}%
}
\end{table}



MPI collectives are common operations used in simulation analysis. For instance, generating stacking images in reverse time migration (essentially an Allreduce operation) is a typical real-world use case~\cite{Gurhem2021Kirchhoff}, which will be demonstrated in Section~\ref{sec-image-stacking}. We use the RTM application dataset for our evaluation, as it is the largest dataset we have, as shown in Table\ref{tab:application-datasets}. The information of our solutions and baselines are presented in Table \ref{tab:collective-implementations}. The compression error bound is set to 1E-4 by default. In our experiments, we adopt a two-stage approach, consisting of a warm-up stage and an execution stage. Each stage is run 10 times, and we report the average results of the execution stage to present the overall performance.

\begin{table}[ht]
\centering
\caption{Collective communication solutions.}
\label{tab:collective-implementations}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Solution} & \textbf{Description} \\ \midrule
\textbf{MPI (baseline)} & Orginal MPI collectives with no compression\\
\textbf{CPRP2P (baseline)} & Collectives implemented by CPRP2P with fZ-light\\
\textbf{C-Coll (baseline)} & The current SOTA compression-accelerated collectives~\cite{huang2023ccoll}\\
\textbf{{\pname} (single-thread)} & Single-thread mode of {\pname} \\
\textbf{{\pname} (multi-thread)} & Multi-thread mode of {\pname} \\ \bottomrule
\end{tabular}%
}
\end{table}




\subsection{Evaluating different compression-integrated baselines}
\label{sec:evaluate_different_compression-integrated_baselines}
We compare different compression-integrated CPRP2P baselines with the original MPI without compression using the Allreduce operation across 64 Broadwell nodes in Figure \ref{fig-original-naive}. The execution time of all the baselines is normalized based the running time of the original MPI\_Allreduce. We can notice that fZ-light has the highest performance in all CPRP2P baselines. This is because that fZ-light has the best compression throughput and compression ratio among all its counterparts. It is worth noting that, both the ZFP(ABS) and ZFP(FXR) demonstrate considerably worse performance than SZx and {\fzlight} when integrated into collective communication because of their relatively low compression throughput and ratio as shown in \cite{huang2023ccoll}. Compared to the original MPI, {\fzlight} integrated baseline shows much less communication time due to the significantly decreased communication volume by the compression technique. Since the {\fzlight} integrated baseline presents the highest performance in all the compression-enabled baselines, we use CPRP2P to represent it and compare our ZCCL with it in later experiments.       

\begin{figure}[ht]
    \centering
    % \vspace{1mm}
    {\includegraphics[width=0.8\linewidth]{./picture/direct_breakdown.pdf}}
    % \vspace{-2mm}
    \caption{Compare the normalized execution time of original MPI and the CPRP2P baselines with different compressors.} 
    \label{fig-original-naive}
    % \vspace{-3mm}
\end{figure}
\subsection{Step-wise Optimizations to Z-Allreduce with Performance Analysis}
\label{sec:evaluation}
In this section, we carry out step-by-step optimizations to our Z-Allreduce ({\pname}-enhanced Allreduce) integrated with {\fzlight} and demonstrate the performance on 64 Broadwell nodes. These optimizations are also applicable to Z-Allreduce when integrated with other compressors. All compression and decompression operations in this section are conducted in single-thread mode. The ring-based Allreduce that we implement contains a Reduce-scatter stage and an Allgather stage. Thus, we breakdown the performance of the two stages separately.



\subsubsection{Evaluating our collective data movement framework with Allgather}
\label{sec:eval-framework}


Figure \ref{fig-new-design} shows the performance improvement achieved by our novel design in the Allgather stage for data sizes ranging from 50 MB to 600 MB. In {\pname}, our collective data movement framework brings a considerable reduction in both compression and decompression time. Notably, at the data size of 300 MB, our {\pname} achieves a compression speed-up of 3.74$\times$ when compared to CPRP2P approach. Additionally, our balanced communication in {\pname} is up to 1.46$\times$ faster than the unbalanced communication in CPRP2P at 600MB. Overall, our {\pname} achieves the maximum speedup of 3.26$\times$ compared to CPRP2P at 250 MB. We analyze the key reason why our solution can obtain a significant performance improvement as follows. In fact, to overcome the compression bottleneck and balance MPI communication, we utilize our collective data movement framework that pre-compresses the data before transmission and decompresses it after all communication, rather than using expensive CPRP2P in collective routines. This novel design can significantly reduce the amount of compression required during collective communication. Using CPRP2P also brings unbalanced communication as the compressed data sizes may vary, but we can balance the communication with a fixed pipeline size in our new design because we do not need to compress the data every time before we send it. 
\begin{figure}[ht]
    \centering
    {\includegraphics[width=0.99\linewidth]{./picture/allgather_breakdown.pdf}}
    % \vspace{-5mm}
    \caption{Compare the Allgather performance of CPRP2P and our {\pname} from 50 MB to 600 MB.} 
    \label{fig-new-design}
\end{figure}


Note that using CPRP2P may accumulate errors during intensive collective communication, such as ring-based communication, as the same data is repeatedly passed from one process to another. Therefore, we have utilized our new framework to ensure that errors in the final results are bounded, which we will discuss in the application evaluation section \ref{sec-image-stacking}.



\subsubsection{Evaluating reduced communication overhead with our collective computation framework}
We demonstrate the effectiveness of our collective computation framework in this section. From Figure \ref{fig-Overlap}, it is evident that our {\pname} leads to significantly less communication in the Reduce\_scatter stage compared with the CPRP2P method, resulting in a performance boost of up to 3.32$\times$ for the data size of 300 MB. The rationale for this performance improvement is shown in the following text. To utilize our collective computation framework for hiding communication during compression, we design and implement PIPE-{\fzlight} (pipelined fZ-light), which could break the compression process into small chunks and allow us to overlap the compression with communication in a fine-grained pipelined manner. As a result, we can significantly reduce communication time in the Reduce\_scatter stage. Combining this optimized Reduce\_scatter with the previously optimized Allgather, we have obtained the final version of our Z-Allreduce and will evaluate it in the following parts.



\begin{figure}[ht]
    \centering
    % \vspace{-2mm}
    {\includegraphics[width=0.99\linewidth]{./picture/reduce_scatter_breakdown.pdf}}
    % \vspace{-5mm}
    \caption{Compare the Reduce\_scatter communication time of CPRP2P and {\pname} from 50 MB to 600 MB.} 
    \label{fig-Overlap}
    % \vspace{-6mm}
\end{figure}



\subsection{End-to-end Comparisons of Z-Allreduce with Baselines}


In this section, we compare the performance of our {\pname}-accelerated Z-Allreduce with four different baselines on various data sizes, node numbers, and datasets.



\begin{figure}[ht]
    \centering
    % \vspace{-1mm}
    {\includegraphics[width=0.99\linewidth]{./picture/end_to_end_size.pdf}}
    %\vspace{-7mm}
    \caption{Compare the performance of our {\pname}-accelerated Z-Allreduce and multiple baselines from 50 MB to 600 MB.} 
    \label{fig-64-sizes}
    % \vspace{-3mm}
\end{figure}


% \vspace{-3mm}

\subsubsection{Evaluating with different data sizes}\label{sec-128-sizes}

In this section, we present the performance of our Z-Allreduce, along with related baselines, using data sizes ranging from 50 MB to 600 MB on 64 Broadwell nodes. As shown in Figure \ref{fig-64-sizes}, {\pname} consistently outperforms or matches the state-of-the-art C-Coll, achieving up to 1.50$\times$ and 2.69$\times$ performance improvements in single-thread and multi-thread modes, respectively. Compared with the CPRP2P method, {\pname} achieves even greater performance enhancements, with up to 1.64$\times$ and 2.88$\times$ speedups in single-thread and multi-thread modes. Additionally, {\pname} is up to 1.91$\times$ and 3.46$\times$ faster than MPI in the two modes. This high performance of {\pname} stems from reduced compression and communication overhead, thanks to our collective data movement and computation frameworks. Moreover, we carefully select and customize the {\fzlight} compressor, leading to higher performance than the SZx-integrated baseline C-Coll.





\begin{figure}[ht]
    \centering
    % \vspace{2mm}
    {\includegraphics[width=0.99\linewidth]{./picture/end_to_end_scale.pdf}}
    % \vspace{-3mm}
    \caption{Compare the performance of our {\pname}-accelerated Z-Allreduce and multiple baselines from 2 to 128 nodes.} 
    \label{fig-128-nodes}
    % \vspace{-5mm}
\end{figure}



% \vspace{-2mm}
\subsubsection{Evaluating with different node counts}

To demonstrate the scalability of our approach, we compare the normalized execution time of our Z-Allreduce and four different baselines using a fixed data size of 678MB (the whole RTM dataset) across 2 to 128 nodes. As shown in Figure \ref{fig-128-nodes}, our {\pname} outperforms all the baselines across various node numbers. It can reach performance boosts of up to 1.56$\times$ and 3.56$\times$ in the single-thread and multi-thread versions compared to the MPI, respectively. Similar to our observation in Section \ref{sec-128-sizes}, we found that both the single-thread and multi-thread modes of {\pname} are outperforming the C-Coll framework, achieving 1.1$\times$ and 3.19$\times$ maximal performance improvements, respectively. Compared with the CPRP2P baseline, our {\pname} even reaches better speedups, with up-to 1.23$\times$ in the single-thread mode and 2.99$\times$ in the multi-thread mode. This scalability evaluation again confirms the high-performance of our designs and optimizations in the {\pname} framework.




\subsection{Generalizability Demonstration on Other MPI Collectives}


We have demonstrated the high performance of our {\pname}-accelerated Z-Allreduce, consisting of Z-Allgather and Z-Reduce-scatter. To showcase the generalizability of our frameworks and optimizations, we also present Z-Bcast and Z-Scatter, which utilize the ubiquitous binomial tree algorithm adopted by MPICH. We conduct experiments ranging from 50 MB to 600 MB using 64 Broadwell nodes.
\subsubsection{Broadcast}\label{sec-eval-bcast}

In Figure \ref{fig-portability-bcast}, we present the speedups of our Z-Bcast normalized against the original MPI\_Bcast. We also compare Z-Bcast with the C-Coll baseline. The experimental results show that {\pname} is 1.6$\times$ and 8.9$\times$ faster than MPI in single-thread and multi-thread modes, respectively. These performance improvements are originated from the reduced data transfer volume and minimized compression overheads provided by our framework. Additionally, {\pname} surpasses C-Coll, achieving up to 1.1$\times$ and 7.5$\times$ speedups in single-thread and multi-thread modes, respectively, demonstrating superior communication efficiency.
\begin{figure}[ht]
    \centering
    % \vspace{-2mm}
    {\includegraphics[width=0.99\linewidth]{./picture/bcast_size.pdf}}
    % \vspace{-5mm}
    \caption{Generalizability demonstration of our proposed framework and optimizations with Bcast from 50 MB to 600 MB.} 
    \label{fig-portability-bcast}
    % \vspace{-4mm}
\end{figure}
\subsubsection{Scatter}\label{sec-eval-scatter}

We evaluate our Z-Scatter against MPI and C-Coll in Figure \ref{fig-portability-scatter}, and observe that {\pname} achieves the highest performance among all counterparts. It demonstrates 1.5$\times$ and 5.4$\times$ performance improvements over MPI, and 1.2$\times$ and 4.5$\times$ enhancements over C-Coll. These speedups are even more pronounced compared to our Z-Allreduce, as collective data movement benefits more from our framework than collective computation.


\begin{figure}[ht]
    \centering
    % \vspace{-2mm}
    {\includegraphics[width=0.99\linewidth]{./picture/scatter_size.pdf}}
    % \vspace{-5mm}
    \caption{Generalizability demonstration of our proposed framework and optimizations with Scatter from 50 MB to 600 MB.} 
    \label{fig-portability-scatter}
    % \vspace{-4mm}
\end{figure}

% \vspace{-2mm}
\subsection{Evaluation of Image Stacking Performance and Accuracy}
\label{sec-image-stacking}

We use the image stacking application to evaluate both the performance and accuracy of our {\pname}. Image stacking is a widely used technique in scientific domains such as climate simulation and geology to generate high-quality images by combining multiple individual images. Researchers employ MPI to sum these images into final composite images \cite{Gurhem2021Kirchhoff}.





We show the performance results and validate the high quality of the stacked images generated under our compression-accelerated collective framework in the following texts. As shown in Table \ref{tab:image-stacking-perf}, for {\pname}, we observe speedups of 1.61$\times$ and 2.96$\times$ compared to MPI in single-thread and multi-thread modes, respectively. In contrast, the C-Coll baseline achieves only a 1.19$\times$ performance improvement, while the CPRP2P baseline lags behind the original MPI without compression. In CPRP2P, compression constitutes the majority of the overall runtime (63.12\%), highlighting its inefficiency in compression costs. This scenario is improved in the C-Coll framework, where compression takes a smaller proportion (53.47\%), and communication accounts for 34.24\%. In the single-thread mode of {\pname}, compression remains the largest contributor to runtime at 58.23\%, but this is still a sound improvement over CPRP2P, thanks to the significantly reduced compression time in {\pname}. Additionally, communication time decreases substantially compared to C-Coll, due to the higher compression ratio of {\fzlight} compared to SZx. In the multi-thread mode of {\pname}, compression costs are further reduced to only 23.18\%, with communication accounting for the largest proportion at 50.65\%. This is a result of the superior multi-thread compression performance of our {\pname} framework, boosted by the {\fzlight} compressor.

\begin{table}[]
\caption{Performance comparison and breakdown of image stacking (The speedup is based on MPI. The last four columns are performance breakdowns).}
\label{tab:image-stacking-perf}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}c|c|cccc@{}}
\toprule
                              & \textbf{Speedup} & \textbf{Compre.} & \textbf{Commu.} & \textbf{Comput.} & \textbf{Other} \\ \midrule
\textbf{CPRP2P (baseline)}               & 0.95             & 63.12\%          & 28.43\%         & 8.38\%           & 0.06\%         \\
\textbf{C-Coll (baseline)}               & 1.39             & 53.47\%          & 34.24\%         & 12.17\%          & 0.11\%         \\ \midrule
\textbf{{\pname} (single-thread)} & 1.61             & 58.23\%          & 27.57\%         & 14.05\%          & 0.15\%         \\
\textbf{{\pname} (multi-thread)}  & 2.96             & 23.18\%          & 50.65\%         & 25.87\%          & 0.30\%         \\ \bottomrule
\end{tabular}%
}
\end{table}

Apart from the performance analysis, we also evaluate the numeric and visual accuracy of our {\pname}. With an error bound of 1E-4, {\pname} achieves a Peak Signal-to-Noise Ratio (PSNR)~\cite{PSNR} of 49.1 and a Normalized Root Mean Square Error (NRMSE)~\cite{shcherbakov2013errormetrics} of 3.5E-3, demonstrating excellent data quality. In Figure \ref{fig-image-stacking-quality}, we compare the visual accuracy of the {\pname} framework against the original MPI without compression. The comparison shows no visual difference between the two methods, further confirming the high accuracy of our {\pname} framework. In summary, {\pname} achieves sound speedups over the baselines while maintaining high data accuracy.


\begin{figure}[ht]
% \vspace{-3mm}
    \centering
    %\hspace{-9mm}
    \subfloat[MPI (lossless)]{
        \includegraphics[scale=0.25]{./picture/Ori_vis.eps}
        \label{fig-image-stacking-ori}
    }
    \hspace{-2mm}
    \subfloat[{\pname}]{
        \includegraphics[scale=0.25]{./picture/ZCCL_vis.eps}
        \label{fig-image-stacking-lossy}
    }
    %\hspace{-10mm}
    % \vspace{-3mm}
    \caption{Visualization of image stacking application.}
    \label{fig-image-stacking-quality}
    % \vspace{-1mm}
\end{figure}

