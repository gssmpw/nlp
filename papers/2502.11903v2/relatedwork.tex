\section{Related Work}
\vspace{-0.1cm}
\textbf{Multimodal Large Language Model.}
Building on large language models, multimodal large language models (MLLMs) have exhibited remarkable capabilities~\citep{kilmllm,cui2024survey,qin2025survey}, achieving state-of-the-art performance across various downstream tasks, including visual grounding~\citep{li2024groundinggpt,xu2024mc}, object detection~\citep{zang2024contextual,wu2025dettoolchain}, visual question answering (VQA)~\citep{kuang2024natural,xu2024mlevlm,demllms}, and instruction following~\citep{li2023fine,sun2024parrot,wei2024demonstrative}. Their outstanding performance underscores their pivotal role in AGI~\cite{zhang2024improving}.

\begin{figure*}[!t]
 \centering
  \captionsetup{justification=raggedright, singlelinecheck=false}
  \includegraphics[width=0.91\textwidth]{dataset.pdf} 
  \vspace{-0.2cm}
  \caption{A sample from the \ours{}, featuring a multi-turn open-ended conversation with six human-annotated questions and answers, designed to assess the ability of MLLMs in open-ended conversations.}
  \vspace{-0.4cm}
  \label{fig:6ability}
\end{figure*}

\noindent \textbf{Benchmarks for Long-Term Conversation.}
MT-Bench~\citep{zheng2023judging} is a pioneering two-turn dialogue dataset generated by GPT, covering eight domain tasks. MT-Bench-101~\citep{bai2024mt} and Bench++~\citep{sun2024parrot} expand the dataset size and add more domains, enhancing evaluation depth. In parallel, Farm~\cite{xu2023earth}, EvalDial~\citep{park2024mitigating}, and MMR~\cite{liu2024seeing} examine model robustness in multi-turn dialogue scenarios using fixed dialogue formats. ConvBench~\citep{liu2024convbench} evaluates models' perception, reasoning, and creation abilities through structured three-turn dialogues, exploring their interrelations. DialogBench~\citep{ou2023dialogbench} and LongMemEval~\citep{wu2024longmemeval} focus on evaluating models' abilities in context understanding and memory retention during GPT-generated dialogues. MMDU~\citep{liu2024mmdu} evaluates the understanding and instruction-following abilities in GPT-generated multi-image, multi-turn dialogues. Table~\ref{tab:data_compare} compares \ours{} with previous works, highlighting its advantages in: (1) naturally open dialogue format with longer and more diverse conversations. (2) holistically covering critical abilities in memorization, recall, and reasoning in a uniquely challenging way (further examples in Fig.~\ref{fig:6ability}).

\vspace{-0.2cm}