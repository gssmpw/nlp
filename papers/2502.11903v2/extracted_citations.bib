@article{bai2024mt,
  title={Mt-bench-101: A fine-grained benchmark for evaluating large language models in multi-turn dialogues},
  author={Bai, Ge and Liu, Jie and Bu, Xingyuan and He, Yancheng and Liu, Jiaheng and Zhou, Zhanhui and Lin, Zhuoran and Su, Wenbo and Ge, Tiezheng and Zheng, Bo and others},
  journal={arXiv preprint arXiv:2402.14762},
  year={2024}
}

@inproceedings{cui2024survey,
  title={A survey on multimodal large language models for autonomous driving},
  author={Cui, Can and Ma, Yunsheng and Cao, Xu and Ye, Wenqian and Zhou, Yang and Liang, Kaizhao and Chen, Jintai and Lu, Juanwu and Yang, Zichong and Liao, Kuei-Da and others},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={958--979},
  year={2024}
}

@article{demllms,
  title={MLLMS KNOW WHERE TO LOOK: TRAINING-FREE PERCEPTION OF SMALL VISUAL DE-TAILS WITH MULTIMODAL LLMS},
  author={DE, SMALL VISUAL}
}

@inproceedings{kilmllm,
  title={MLLM-CompBench: A Comparative Reasoning Benchmark for Multimodal LLMs},
  author={Kil, Jihyung and Mai, Zheda and Lee, Justin and Chowdhury, Arpita and Wang, Zihe and Cheng, Kerrie and Wang, Lemeng and Liu, Ye and Chao, Wei-Lun},
  booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track}
}

@article{kuang2024natural,
  title={Natural Language Understanding and Inference with MLLM in Visual Question Answering: A Survey},
  author={Kuang, Jiayi and Shen, Ying and Xie, Jingyou and Luo, Haohao and Xu, Zhe and Li, Ronghao and Li, Yinghui and Cheng, Xianfeng and Lin, Xika and Han, Yu},
  journal={ACM Computing Surveys},
  year={2024},
  publisher={ACM New York, NY}
}

@inproceedings{li2023fine,
  title={Fine-tuning multimodal llms to follow zero-shot demonstrative instructions},
  author={Li, Juncheng and Pan, Kaihang and Ge, Zhiqi and Gao, Minghe and Ji, Wei and Zhang, Wenqiao and Chua, Tat-Seng and Tang, Siliang and Zhang, Hanwang and Zhuang, Yueting},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@inproceedings{li2024groundinggpt,
  title={Groundinggpt: Language enhanced multi-modal grounding model},
  author={Li, Zhaowei and Xu, Qi and Zhang, Dong and Song, Hang and Cai, Yiqing and Qi, Qi and Zhou, Ran and Pan, Junting and Li, Zefeng and Tu, Vu and others},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={6657--6678},
  year={2024}
}

@article{liu2024convbench,
  title={Convbench: A multi-turn conversation evaluation benchmark with hierarchical capability for large vision-language models},
  author={Liu, Shuo and Ying, Kaining and Zhang, Hao and Yang, Yue and Lin, Yuqi and Zhang, Tianle and Li, Chuanhao and Qiao, Yu and Luo, Ping and Shao, Wenqi and others},
  journal={arXiv preprint arXiv:2403.20194},
  year={2024}
}

@article{liu2024mmdu,
  title={MMDU: A Multi-Turn Multi-Image Dialog Understanding Benchmark and Instruction-Tuning Dataset for LVLMs},
  author={Liu, Ziyu and Chu, Tao and Zang, Yuhang and Wei, Xilin and Dong, Xiaoyi and Zhang, Pan and Liang, Zijian and Xiong, Yuanjun and Qiao, Yu and Lin, Dahua and others},
  journal={arXiv preprint arXiv:2406.11833},
  year={2024}
}

@article{liu2024seeing,
  title={Seeing Clearly, Answering Incorrectly: A Multimodal Robustness Benchmark for Evaluating MLLMs on Leading Questions},
  author={Liu, Yexin and Liang, Zhengyang and Wang, Yueze and He, Muyang and Li, Jian and Zhao, Bo},
  journal={arXiv preprint arXiv:2406.10638},
  year={2024}
}

@article{ou2023dialogbench,
  title={Dialogbench: Evaluating llms as human-like dialogue systems},
  author={Ou, Jiao and Lu, Junda and Liu, Che and Tang, Yihong and Zhang, Fuzheng and Zhang, Di and Gai, Kun},
  journal={arXiv preprint arXiv:2311.01677},
  year={2023}
}

@article{park2024mitigating,
  title={Mitigating dialogue hallucination for large multi-modal models via adversarial instruction tuning},
  author={Park, Dongmin and Qian, Zhaofang and Han, Guangxing and Lim, Ser-Nam},
  journal={arXiv preprint arXiv:2403.10492},
  year={2024}
}

@article{qin2025survey,
  title={A survey of multilingual large language models},
  author={Qin, Libo and Chen, Qiguang and Zhou, Yuhang and Chen, Zhi and Li, Yinghui and Liao, Lizi and Li, Min and Che, Wanxiang and Philip, S Yu},
  journal={Patterns},
  volume={6},
  number={1},
  year={2025},
  publisher={Elsevier}
}

@inproceedings{sun2024parrot,
  title={Parrot: Enhancing multi-turn instruction following for large language models},
  author={Sun, Yuchong and Liu, Che and Zhou, Kun and Huang, Jinwen and Song, Ruihua and Zhao, Wayne Xin and Zhang, Fuzheng and Zhang, Di and Gai, Kun},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={9729--9750},
  year={2024}
}

@inproceedings{wei2024demonstrative,
  title={Demonstrative Instruction Following in Multimodal LLMs via Integrating Low-Rank Adaptation with Ensemble Learning},
  author={Wei, Jingyu and Su, Yi and Xu, Kele and Zeng, Lingbin and Liu, Bo and Wang, Huaimin},
  booktitle={Proceedings of the 32nd ACM International Conference on Multimedia},
  pages={11435--11441},
  year={2024}
}

@article{wu2024longmemeval,
  title={Longmemeval: Benchmarking chat assistants on long-term interactive memory},
  author={Wu, Di and Wang, Hongwei and Yu, Wenhao and Zhang, Yuwei and Chang, Kai-Wei and Yu, Dong},
  journal={arXiv preprint arXiv:2410.10813},
  year={2024}
}

@inproceedings{wu2025dettoolchain,
  title={Dettoolchain: A new prompting paradigm to unleash detection ability of mllm},
  author={Wu, Yixuan and Wang, Yizhou and Tang, Shixiang and Wu, Wenhao and He, Tong and Ouyang, Wanli and Torr, Philip and Wu, Jian},
  booktitle={European Conference on Computer Vision},
  pages={164--182},
  year={2025},
  organization={Springer}
}

@article{xu2023earth,
  title={The Earth is Flat because...: Investigating LLMs' Belief towards Misinformation via Persuasive Conversation},
  author={Xu, Rongwu and Lin, Brian S and Yang, Shujian and Zhang, Tianqi and Shi, Weiyan and Zhang, Tianwei and Fang, Zhixuan and Xu, Wei and Qiu, Han},
  journal={arXiv preprint arXiv:2312.09085},
  year={2023}
}

@article{xu2024mc,
  title={Mc-bench: A benchmark for multi-context visual grounding in the era of mllms},
  author={Xu, Yunqiu and Zhu, Linchao and Yang, Yi},
  journal={arXiv preprint arXiv:2410.12332},
  year={2024}
}

@inproceedings{xu2024mlevlm,
  title={Mlevlm: Improve multi-level progressive capabilities based on multimodal large language model for medical visual question answering},
  author={Xu, Dexuan and Chen, Yanyuan and Wang, Jieyi and Huang, Yue and Wang, Hanpin and Jin, Zhi and Wang, Hongxing and Yue, Weihua and He, Jing and Li, Hang and others},
  booktitle={Findings of the Association for Computational Linguistics ACL 2024},
  pages={4977--4997},
  year={2024}
}

@article{zang2024contextual,
  title={Contextual object detection with multimodal large language models},
  author={Zang, Yuhang and Li, Wei and Han, Jun and Zhou, Kaiyang and Loy, Chen Change},
  journal={International Journal of Computer Vision},
  pages={1--19},
  year={2024},
  publisher={Springer}
}

@inproceedings{zhang2024improving,
  title={Improving Accuracy and Generalizability via Multi-Modal Large Language Models Collaboration},
  author={Zhang, Shuili and Mu, Hongzhang and Liu, Tingwen},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--8},
  year={2024},
  organization={IEEE}
}

@article{zheng2023judging,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={46595--46623},
  year={2023}
}

