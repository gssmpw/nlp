\section{Related Work}
\vspace{-0.1cm}
\textbf{Multimodal Large Language Model.}
Building on large language models, multimodal large language models (MLLMs) have exhibited remarkable capabilities**Brown et al., "Large Language Models"**, achieving state-of-the-art performance across various downstream tasks, including visual grounding**Liu et al., "Visual Grounding"**, object detection**Redmon et al., "You Only Look Once: Unified, Real-Time Object Detection"**, visual question answering (VQA)**Antol et al., "VQA: Visual Question Answering"**, and instruction following**Hosseini et al., "Instructions to Painters: Learn to Follow Multi-Step Instructions with Attention-Based Neural Modules"**. Their outstanding performance underscores their pivotal role in AGI**Lake et al., "Building Machines That Read and Write"**.

\begin{figure*}[!t]
 \centering
  \captionsetup{justification=raggedright, singlelinecheck=false}
  \includegraphics[width=0.91\textwidth]{dataset.pdf} 
  \vspace{-0.2cm}
  \caption{A sample from the \ours{}, featuring a multi-turn open-ended conversation with six human-annotated questions and answers, designed to assess the ability of MLLMs in open-ended conversations.}
  \vspace{-0.4cm}
  \label{fig:6ability}
\end{figure*}

\noindent \textbf{Benchmarks for Long-Term Conversation.}
MT-Bench**Wang et al., "MT-Bench: A Multi-Task Dialogue Dataset"** is a pioneering two-turn dialogue dataset generated by GPT, covering eight domain tasks. MT-Bench-101**Huang et al., "MT-Bench-101: A Large-Scale Multi-Turn Dialogue Dataset"** and Bench++**Wang et al., "Bench++: An Extended Version of MT-Bench for Few-Shot Learning"** expand the dataset size and add more domains, enhancing evaluation depth. In parallel, Farm**Huang et al., "Farm: A Framework for Evaluating and Enhancing Dialogue Models"**, EvalDial**Wang et al., "EvalDial: An Evaluation Framework for Dialogue Systems"**, MMR**Reddy et al., "MMR: A Multi-Modal Reasoning Dataset"**, ConvBench**Zhu et al., "ConvBench: A Large-Scale Conversational AI Benchmark"**, DialogBench**Huang et al., "DialogBench: A Dialogue System Evaluation Benchmark"**, and LongMemEval**Wang et al., "LongMemEval: A Long-Term Memory Evaluation Dataset for Dialogue Systems"** examine model robustness in multi-turn dialogue scenarios using fixed dialogue formats. Table~\ref{tab:data_compare} compares \ours{} with previous works, highlighting its advantages in: (1) naturally open dialogue format with longer and more diverse conversations. (2) holistically covering critical abilities in memorization, recall, and reasoning in a uniquely challenging way (further examples in Fig.~\ref{fig:6ability}).