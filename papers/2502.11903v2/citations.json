[
  {
    "index": 0,
    "papers": [
      {
        "key": "kilmllm",
        "author": "Kil, Jihyung and Mai, Zheda and Lee, Justin and Chowdhury, Arpita and Wang, Zihe and Cheng, Kerrie and Wang, Lemeng and Liu, Ye and Chao, Wei-Lun",
        "title": "MLLM-CompBench: A Comparative Reasoning Benchmark for Multimodal LLMs"
      },
      {
        "key": "cui2024survey",
        "author": "Cui, Can and Ma, Yunsheng and Cao, Xu and Ye, Wenqian and Zhou, Yang and Liang, Kaizhao and Chen, Jintai and Lu, Juanwu and Yang, Zichong and Liao, Kuei-Da and others",
        "title": "A survey on multimodal large language models for autonomous driving"
      },
      {
        "key": "qin2025survey",
        "author": "Qin, Libo and Chen, Qiguang and Zhou, Yuhang and Chen, Zhi and Li, Yinghui and Liao, Lizi and Li, Min and Che, Wanxiang and Philip, S Yu",
        "title": "A survey of multilingual large language models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "li2024groundinggpt",
        "author": "Li, Zhaowei and Xu, Qi and Zhang, Dong and Song, Hang and Cai, Yiqing and Qi, Qi and Zhou, Ran and Pan, Junting and Li, Zefeng and Tu, Vu and others",
        "title": "Groundinggpt: Language enhanced multi-modal grounding model"
      },
      {
        "key": "xu2024mc",
        "author": "Xu, Yunqiu and Zhu, Linchao and Yang, Yi",
        "title": "Mc-bench: A benchmark for multi-context visual grounding in the era of mllms"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "zang2024contextual",
        "author": "Zang, Yuhang and Li, Wei and Han, Jun and Zhou, Kaiyang and Loy, Chen Change",
        "title": "Contextual object detection with multimodal large language models"
      },
      {
        "key": "wu2025dettoolchain",
        "author": "Wu, Yixuan and Wang, Yizhou and Tang, Shixiang and Wu, Wenhao and He, Tong and Ouyang, Wanli and Torr, Philip and Wu, Jian",
        "title": "Dettoolchain: A new prompting paradigm to unleash detection ability of mllm"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "kuang2024natural",
        "author": "Kuang, Jiayi and Shen, Ying and Xie, Jingyou and Luo, Haohao and Xu, Zhe and Li, Ronghao and Li, Yinghui and Cheng, Xianfeng and Lin, Xika and Han, Yu",
        "title": "Natural Language Understanding and Inference with MLLM in Visual Question Answering: A Survey"
      },
      {
        "key": "xu2024mlevlm",
        "author": "Xu, Dexuan and Chen, Yanyuan and Wang, Jieyi and Huang, Yue and Wang, Hanpin and Jin, Zhi and Wang, Hongxing and Yue, Weihua and He, Jing and Li, Hang and others",
        "title": "Mlevlm: Improve multi-level progressive capabilities based on multimodal large language model for medical visual question answering"
      },
      {
        "key": "demllms",
        "author": "DE, SMALL VISUAL",
        "title": "MLLMS KNOW WHERE TO LOOK: TRAINING-FREE PERCEPTION OF SMALL VISUAL DE-TAILS WITH MULTIMODAL LLMS"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "li2023fine",
        "author": "Li, Juncheng and Pan, Kaihang and Ge, Zhiqi and Gao, Minghe and Ji, Wei and Zhang, Wenqiao and Chua, Tat-Seng and Tang, Siliang and Zhang, Hanwang and Zhuang, Yueting",
        "title": "Fine-tuning multimodal llms to follow zero-shot demonstrative instructions"
      },
      {
        "key": "sun2024parrot",
        "author": "Sun, Yuchong and Liu, Che and Zhou, Kun and Huang, Jinwen and Song, Ruihua and Zhao, Wayne Xin and Zhang, Fuzheng and Zhang, Di and Gai, Kun",
        "title": "Parrot: Enhancing multi-turn instruction following for large language models"
      },
      {
        "key": "wei2024demonstrative",
        "author": "Wei, Jingyu and Su, Yi and Xu, Kele and Zeng, Lingbin and Liu, Bo and Wang, Huaimin",
        "title": "Demonstrative Instruction Following in Multimodal LLMs via Integrating Low-Rank Adaptation with Ensemble Learning"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "zhang2024improving",
        "author": "Zhang, Shuili and Mu, Hongzhang and Liu, Tingwen",
        "title": "Improving Accuracy and Generalizability via Multi-Modal Large Language Models Collaboration"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "zheng2023judging",
        "author": "Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others",
        "title": "Judging llm-as-a-judge with mt-bench and chatbot arena"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "bai2024mt",
        "author": "Bai, Ge and Liu, Jie and Bu, Xingyuan and He, Yancheng and Liu, Jiaheng and Zhou, Zhanhui and Lin, Zhuoran and Su, Wenbo and Ge, Tiezheng and Zheng, Bo and others",
        "title": "Mt-bench-101: A fine-grained benchmark for evaluating large language models in multi-turn dialogues"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "sun2024parrot",
        "author": "Sun, Yuchong and Liu, Che and Zhou, Kun and Huang, Jinwen and Song, Ruihua and Zhao, Wayne Xin and Zhang, Fuzheng and Zhang, Di and Gai, Kun",
        "title": "Parrot: Enhancing multi-turn instruction following for large language models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "xu2023earth",
        "author": "Xu, Rongwu and Lin, Brian S and Yang, Shujian and Zhang, Tianqi and Shi, Weiyan and Zhang, Tianwei and Fang, Zhixuan and Xu, Wei and Qiu, Han",
        "title": "The Earth is Flat because...: Investigating LLMs' Belief towards Misinformation via Persuasive Conversation"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "park2024mitigating",
        "author": "Park, Dongmin and Qian, Zhaofang and Han, Guangxing and Lim, Ser-Nam",
        "title": "Mitigating dialogue hallucination for large multi-modal models via adversarial instruction tuning"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "liu2024seeing",
        "author": "Liu, Yexin and Liang, Zhengyang and Wang, Yueze and He, Muyang and Li, Jian and Zhao, Bo",
        "title": "Seeing Clearly, Answering Incorrectly: A Multimodal Robustness Benchmark for Evaluating MLLMs on Leading Questions"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "liu2024convbench",
        "author": "Liu, Shuo and Ying, Kaining and Zhang, Hao and Yang, Yue and Lin, Yuqi and Zhang, Tianle and Li, Chuanhao and Qiao, Yu and Luo, Ping and Shao, Wenqi and others",
        "title": "Convbench: A multi-turn conversation evaluation benchmark with hierarchical capability for large vision-language models"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "ou2023dialogbench",
        "author": "Ou, Jiao and Lu, Junda and Liu, Che and Tang, Yihong and Zhang, Fuzheng and Zhang, Di and Gai, Kun",
        "title": "Dialogbench: Evaluating llms as human-like dialogue systems"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "wu2024longmemeval",
        "author": "Wu, Di and Wang, Hongwei and Yu, Wenhao and Zhang, Yuwei and Chang, Kai-Wei and Yu, Dong",
        "title": "Longmemeval: Benchmarking chat assistants on long-term interactive memory"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "liu2024mmdu",
        "author": "Liu, Ziyu and Chu, Tao and Zang, Yuhang and Wei, Xilin and Dong, Xiaoyi and Zhang, Pan and Liang, Zijian and Xiong, Yuanjun and Qiao, Yu and Lin, Dahua and others",
        "title": "MMDU: A Multi-Turn Multi-Image Dialog Understanding Benchmark and Instruction-Tuning Dataset for LVLMs"
      }
    ]
  }
]