\section{Related Work}
\label{sec:connections}
\subsection{Function Space Optimization and Whitening}
The functional gradient descent framework is general and the 
application to neural networks described in ____ has connections to
multiple recent works. The LocoProp-S Algorithm 1 in ____
is mechanically equivalent to solving \Eqref{eq:OLS2} using gradient descent
instead of solving in closed form, whereas this work avoids constructing the targets altogether.
As the authors discuss in Appendix C,
there is a reduction from LocoProp-S to ProxProp ____.

The FOOF algorithm ____ is also motivated by performing gradient descent
in function space. For simplicity in implementation the authors analyzed functions
with no bias terms and Equation 6 in ____ is equivalent to
solving \Eqref{eq:normal}. In that work, an online estimate of the 
covariance matrix is computed but only infrequently inverted to update the
preconditioner. This work provides the following novel perspectives. First, 
we formulate the metric via inner-product of Jacobians. This approach
automatically handles, without specialized implementations, various forms of the neurons,
including if it has a bias term, is a convolutional layer, etc. 
Second, we make the equivalence to feature whitening explicit, which enables 
us to analytically identify a Cholesky preconditioner for fast convergence via
conjugate gradient. Lastly, this feature whitening perspective also
explains why FOOF converges in a single step in the example described in Appendix F:
it is performing a Newton step on squared loss, which is equivalent to performing one
gradient descent step on whitened features (via the reparameterized model).

Standardizing the inputs to neurons is an essential practitioner technique ____,
with BatchNorm ____ being one of the most prevalent
usages with a diagonal approximation. The reparameterized model interpretation discussed in
\S \ref{sec:WW} is equivalent to placing a BatchNorm layer before applying the \emph{weights},
as opposed to typical practice of placing BatchNorm before the
\emph{activation}. The preconditioner connection is also discussed in ____
where the preconditioner for CNNs is carefully constructed.
Related, PRONG ____ also computes the same first and
second moments (again, manually) to reparameterize the network, but misses the analytical interpretation
of the preconditioner matrix that enables easy adaption to arbitrary network topologies.

In summary, all aforementioned work will obtain the same result, and the primary difference
is implementation. The primary benefit of the LNB interpretation is that it can be easily
implemented for any differentiable network architecture and the trust-region connection
informs an adaptive step size.

\subsection{(Quasi-) Second-order methods}
The $M_i$ metric is very local in that it finds
an update vector that induces an $\eps$-sized squared norm in the differential of each neuron's output,
$\langle \delta\theta_i, \delta\theta_i \rangle_{M_i} =
\delta\theta_i^T \E[\frac{\partial x_i}{\partial \theta_i}^T\frac{\partial x_i}{\partial \theta_i}] \delta\theta_i
= \E[\delta x_i^T \delta x_i]$.
Intuitively, a better metric would compose the neuron's output differential with the change in the \emph{network's}
output differential, $\delta x_m$, to ensure that the cascaded output perturbations are small when the neuron's parameters
change; this corresponds to the Gauss-Newton (GN) matrix,
$\E[\frac{\partial x_i}{\partial \theta_i}^T \frac{\partial x_m}{\partial x_i}^T\frac{\partial x_m}{\partial x_i} \frac{\partial x_i}{\partial \theta_i}]
= \E[\frac{\partial x_m}{\partial \theta_i}^T\frac{\partial x_m}{\partial \theta_i}]$.
A seemingly better metric would measure how $\delta x_m$ locally varies in the landscape
of the loss function, i.e., the inner product of $\delta x_m$ along the eigenvectors of the 
loss' Hessian at $x_m$,
$\E[\frac{\partial x_m}{\partial \theta_i}^T \frac{\partial^2 \ell_y(x_m)}{\partial x_m \partial x_m}  \frac{\partial x_m}{\partial \theta_i}]$;
this is the Generalized GN matrix (GGN) ____.
As reviewed in ____,
in the special case when the network predictions, $x_m$, are the parameters of an exponential probability
distribution and $\ell(x_m)$ is log-loss, then the GGN matrix is equivalent to the FIM used in
natural gradient descent.

There are strong theoretical reasons to prefer the FIM in general
____; however,
computing the metric is typically expensive in practice and there is
extensive research for efficiently approximating it
____.
The functional gradient descent interpretation makes no attempt to approximate the FIM.
But, given that LNB uses a much simpler metric, it
provides an effective and efficient intermediary between the identity metric of gradient descent
and the full network Jacobian used in GGN and natural gradients.