\documentclass{article}
\pdfoutput=1
\input{math_commands}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{natbib}
\usepackage{microtype}
\usepackage{subfigure}

\usepackage{hyperref}

\usepackage{arxiv}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage{bbm}
\usepackage{url}
\usepackage{hyperref}

\title{Simple Linear Neuron Boosting}

\author{Daniel Munoz \\
        Independent Researcher \\
	\texttt{lnb@dmunoz.org}
}

\begin{document}
\maketitle

\begin{abstract}
Given a differentiable network architecture and loss function, we revisit optimizing the network's
neurons in function space using Boosted Backpropagation \citep{grub2010}, in contrast to optimizing
in parameter space. From this perspective, we reduce descent in the space of linear functions that optimizes
the network's backpropagated-errors to a preconditioned gradient descent algorithm. We show that this
preconditioned update rule is equivalent to reparameterizing the network to whiten each neuron's features,
with the benefit that the normalization occurs outside of inference. In practice, we use this equivalence to
construct an online estimator for approximating the preconditioner and we propose an online, matrix-free
learning algorithm with adaptive step sizes. The algorithm is applicable whenever autodifferentiation is
available, including convolutional networks and transformers, and it is simple to implement for both the
local and distributed training settings. We demonstrate fast convergence both in terms of epochs and wall
clock time on a variety of tasks and networks.
\end{abstract}

\input{1intro}
\input{2background}
\input{3lnboost}
\input{4related_work}
\input{5experiments}

\bibliography{lnb}

\end{document}
