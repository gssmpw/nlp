\section{Boosting over Linear Neurons}
\label{sec:lnb}

\subsection{Reduction}
For training general architectures we can forgo computing the 
regression targets, $\lambda_i$. Instead, we use
its definition that it is the gradient vector of the loss
\wrt the \emph{output} of $f_i$,
%
\begin{equation}
\label{eq:lambda}
\lambda_i^T
= \frac{\partial l_{y}(x_m)}{\partial f_i(x_{i-1})}
= \frac{\partial l_{y}(x_m)}{\partial x_i},
\end{equation}
%
where $x_{i-1} \in \gX_{i-1}$ are the forward-propagated \emph{input} features to the neuron $f_i$.
Note that $x_{i-1}$ and $\lambda_i$ both depend on the sample $(x,y)$ and current
network definition, $F_B$.

When projecting the functional gradient vector for $f_i$
over a linear hypothesis space, the weak-learners have the same
function definition as the neuron but differ in the parameters.
The scalar projection step (\Eqref{eq:scalarproj}) then corresponds to
solving the ordinary least squares (OLS) problem
%
\begin{equation}
\label{eq:OLS}
\hat{\theta}_i = \argmin_{\theta} \frac{1}{2} \E_{(x,y) \sim \gD} 
\Vert f_i(x_{i-1};\theta) - \lambda_i \Vert^2.
\end{equation}
%
Because this is an OLS problem, we know its solution
is the projection of the targets, $\{\lambda_i\}$,
onto the column space of the forward-propagated input features to $f_i$;
we denote this set of input features as $X_i = \{x_{i-1}\}$.

Using \Eqref{eq:lambda} and the definition
$x_i = f_i(x_{i-1};\theta)
= \frac{\partial x_i}{\partial \theta_i}\theta$
(because $f_i$ is linear),
we can rewrite \Eqref{eq:OLS} as
%
\begin{equation}
\label{eq:OLS2}
\hat{\theta}_i = \argmin_{\theta} \frac{1}{2} \E_{(x,y) \sim \gD}
\biggl \Vert \frac{\partial x_i}{\partial \theta_i}\theta - \frac{\partial l_{y}(x_m)}{\partial x_i}^T \biggr \Vert^2,
\end{equation}
%
which has the corresponding Normal Equation
%
\begin{align}
\E_{x_{i-1} \sim X_i} \biggl [ \frac{\partial x_i}{\partial \theta_i}^T\frac{\partial x_i}{\partial \theta_i} \biggr ] \hat{\theta}_i 
= \E_{(x,y) \sim \gD} \biggl [ \biggl (\frac{\partial l_{y}(x_m)}{\partial x_i} \frac{\partial x_i}{\partial \theta_i} \biggr )^T \biggr ].
\label{eq:normal}
\end{align}
%
The right-hand side is the component of $\nabla \bar\ell(\theta_B)$ corresponding to $\theta_i$, so the solution is
$\hat{\theta}_i = M_i^{-1}g_i$, where
 $M_i = \E_{x_{i-1} \sim X_i} [ \frac{\partial x_i}{\partial \theta_i}^T  \frac{\partial x_i}{\partial \theta_i}]$.
We can confirm that the norm in function space is equivalent to the
inner product under $M_i$ of \Eqref{eq:metricnorm}:
%
$$
\Vert f_i(\cdot; \hat{\theta}_i) \Vert^2_F
= \E \biggl [\frac{\partial x_i}{\partial \theta_i} \hat{\theta}_i \cdot \frac{\partial x_i}{\partial \theta_i} \hat{\theta}_i \biggr ]
= \hat{\theta}_i \cdot M_i \hat{\theta}_i
= \hat{\theta}_i \cdot g_i.
$$
%

Because the OLS problems are solved independently per neuron,
the final solution is equivalent to as if we constructed a block-diagonal metric, $M$,
containing each $M_i$ along the diagonal and then computing the preconditioned gradient
vector from \S \ref{sec:cgrad}. We denote this vector as $\hat{\theta}_B = \nabla_M \bar\ell(\theta_B)$,
which is composed of each neuron's $\hat{\theta}_i$ solution.
Note that the structure of the network is only needed to compute $\nabla \bar\ell(\theta_B)$ (via
autodifferentiation) and we can then solve \Eqref{eq:normal} for each neuron independently.

\subsection{Optimization}
To solve for $\hat{\theta}_i$ in \Eqref{eq:normal},
we can leverage any linear system solver that internally
uses matrix-vector-products (MVP) instead of instantiating
each metric, $M_i$.
First, we need to save the set of forward-propagated input features, $X_i$, to the neuron; we note
that these features are already computed and saved in
memory during backpropagation.
In practice, the MVP $M_i \theta$ can then be computed via a
VJP composed with a Jacobian-vector-product (JVP) of
the vectorized function $\mathbf{f}_{X_i}(\theta_i) = \text{vec}(\{f_i(x_{i-1};\theta_i)\}_{x_{i-1} \in X_i})$,
which maps $f_i(\cdot; \theta_i)$ over  every element (tensor) in dataset $X_i$, and then
%
\begin{equation}
\label{eq:mvp}
M_i(\theta) = 
\frac{1}{n_i} \frac{\partial \mathbf{f}_{X_i}}{\partial \theta_i}^T \frac{\partial \mathbf{f}_{X_i}}{\partial \theta_i} \theta,
\end{equation}
%
where $n_i$ is the number of samples that contribute to the
gradient\footnote{In general this value can be computed using the shape of $\gX_i$,
\eg, if $f_i(x;\theta) = \theta^Tx$, then $n_i$ is
the batch size; if $f_i$ is a strided convolution, then $n_i$ 
is the number of output pixels in the batch.}.
Note that since $\mathbf{f}_{X_i}$ is linear it need only be
linearized once (at any point) by the linear system solver.

After solving \Eqref{eq:normal} for each neuron, we can construct $\hat{\theta}_B$ and
compute the $\eps$-sized step under the full metric $M$ via \Eqref{eq:metricnorm}.
Lastly, we may wish to regularize our strong-learner in function space as discussed in 
\S \ref{sec:reg}. Because the learner is linear, this corresponds to a 
``weight decay'' of factor $1-\sqrt{\eps} \rho$; note that if a neuron contains
bias terms, these should also decay.
Algorithm \ref{algo1} summarizes the entire optimization algorithm, which
we refer to as Linear Neuron Boosting (LNB).

\begin{algorithm}[tb]
\caption{Linear Neuron Boosting}
\label{algo1}
\begin{algorithmic}[1]
\INPUT{Network $F$,
linear neurons $F_B \subseteq F$ with parameters $\theta_B$,
loss function $\bar\ell$,
step size schedule $\{\eps^{(t)}\}_{t=1}^T$,
weight decay $\rho$}
\STATE Randomly initialize $\theta_B$
\FOR{$\eps \in [\eps^{(1)}, \ldots, \eps^{(T)}]$}
\STATE Compute $\nabla \bar\ell(\theta_B)$ via backpropagation and save the 
inputs to each neuron, $X_B = \{X_i | f_i \in F_B\}$ \label{algo:bp}
   \FOR{$(f_i, g_i, X_i) \in (F_B, \nabla\bar\ell(\theta_B), X_B)$}
      \STATE Define the \Eqref{eq:mvp} MVP function, $M_i(\cdot)$, using the VJP and JVP of $\mathbf{f}_{X_i}$
      \STATE $\hat{\theta}_i \gets \texttt{linear\_solver}(A=M_i, b=g_i)$
   \ENDFOR
   \STATE $\theta_B \gets (1-\sqrt{\eps}\rho)\theta_B$  \hfill $\triangleright$ Weight decay
   \STATE $z = \hat{\theta}_B \cdot \nabla \bar \ell(\theta_B)$  \hfill $\triangleright$ Norm
\STATE $\theta_B \gets \theta_B - \sqrt{\frac{\eps}{z}} \hat{\theta}_B$ \hfill $\triangleright$ Adaptive step size
\ENDFOR
\OUTPUT{$\theta_B$}
\end{algorithmic}
\end{algorithm}

\subsection{Update Properties}
\label{sec:WW}
We now analyze the properties of the LNB update rule. For simplicity in explanation,
we use a neuron with scalar output, $f_i : \gX_{i-1} \to \sR$, and note that this is sufficient because
the OLS solution of \Eqref{eq:OLS} for a vector output is the same as independently solving for each component.

When $f_i(x;\theta) = \theta^Tx$ \emph{without} a bias term, the resulting metric is the \emph{uncentered}
second moment matrix, $M_i = \E_{x \sim X_i}[xx^T]$. Denoting the number of parameters as $d_i$,
when the features are linearly independent and $n_i > d_i$, then $M_i \succ 0$ and the
OLS solution is scale equivariant \wrt the features. Of course, satisfying both of these
conditions can be rare in practice and we discuss mitigations in \S \ref{sec:PSD}.

When $f_i(x;\theta) = \theta^T \acute x$, where $\acute x^T = [x^T, 1]$
the resulting metric has the block form
\begin{equation}
M_i = \begin{pmatrix}
\Sigma_i + \mu_i \mu_i^T & \mu_i\\
\mu_i^T & 1
\end{pmatrix},
\end{equation}
where $\mu_i = \E_{x \sim X_i}[x]$ and $\Sigma_i$ are the mean and covariance matrix of $X_i$, respectively.
We can analytically invert $M_i$ via the factorization $M_i=L_i^T L_i$, where
$L_i = \begin{pmatrix}
   \Sigma_i^{\frac{1}{2}} & 0\\
   \mu_i^T & 1
\end{pmatrix}$.
Then, $M_i^{-1} = W_i W_i^T$, where
$W_i = \begin{pmatrix}
   \Sigma_i^{-\frac{1}{2}} & 0\\
   -\mu_i^T \Sigma_i^{-\frac{1}{2}} & 1
\end{pmatrix}$. From \S \ref{sec:pcgd}, preconditioning $W_iW_i^T g_i$ is the same
as reparameterizing the model:
$$
f_i(x; W_i\theta) = (W_i \theta)^T \acute x = \theta^T W_i^T \acute x = \theta^T (\Sigma_i^{-\frac{1}{2}}(x-\mu_i)).
$$
To summarize, when running LNB with neurons that have bias parameters,
it is equivalent to optimizing a model where we 
added a whitening transformation step in the network to each $f_i$'s input features using the
respective statistics from $X_i$, \emph{before} applying the parameters.
We further discuss this relationship to Batch Normalization \citep{batchnorm} and other methods in \S \ref{sec:connections}.

When the neuron is a convolution layer, $f_i(x;\theta) = x * \theta$, the output is still linear in the parameters (filters)
and the interpretation reduces to either of the previous two cases, with the only difference that the
first and second moments are computed using the filter's spatial support features (and are
translationally equivariant). By representing the metric using Jacobians, we can use autodifferentiation
to compute the metric without requiring a specialized implementation for this case, \eg, $\texttt{unfold}$.

Other common forms of $f_i$ will likely result in an identity $M_i$. For example, in a vision
transformer \citep{vit} the corresponding metric for both the embedding function, $f(x;\theta) = x + \theta$, and the class-token
function, $f(x;\theta) = \texttt{concat}(\theta, x)$, is the identity. In these cases, the linear solver can be skipped
since $\hat{\theta}_i = g_i$ and there are no other changes to the LNB algorithm.

\subsection{Positive Semi-definite Metrics}
\label{sec:PSD}
Since $M_i$ is an inner product of a Jacobian with itself, $M_i \succeq 0$, in general.
To ensure $M_i \succ 0$ in practice, a common regularizer is to add a small constant, $\gamma$,
along the diagonal of $M_i$ (excluding the bias terms) when solving the linear system. 
In the boosting perspective this adds a $\gamma \Vert \theta \Vert^2$ penalty term to \Eqref{eq:OLS2}
(which is why one would typically exclude the bias terms).
Because this regularizes the norm of the parameters, the solution is no longer scale equivariant.
When the metric's eigenvalues are small it no longer informs a meaningful
step-size ($z_{\theta} \approx 0$). In this case, we can upperbound the maximum allowed step size
to avoid taking a large step. This is essentially equivalent to disabling the
preconditioning and following the typical gradient vector direction.

\begin{algorithm}[ht]
   \caption{Online Linear Neuron Boosting}
   \label{algo2}
   \begin{algorithmic}[1]
   \INPUT{Network $F$,
   linear neurons $F_B \subseteq F$ with parameters $\theta_B$,
   mini-batch loss function $\bar\ell$,
   step size schedule $\{\eps^{(t)}\}_{t=1}^T$,
   weight decay $\rho$, minimum norm $z_0$}
   \STATE Randomly initialize $\theta_B$
   \FOR{$\eps \in [\eps^{(1)}, \ldots, \eps^{(T)}]$}
   \STATE Compute $\nabla \bar\ell(\theta_B)$ via backpropagation and save the 
   inputs to each neuron, $X_B = \{X_i | f_i \in F_B\}$
   \STATE $g_B = \texttt{ema}(g_B, \nabla \bar\ell(\theta_B))$
      \FOR{$(f_i, g_i, X_i) \in (F_B, g_B, X_B)$}
         \STATE $\mu_i = \texttt{ema}(\mu_i, \texttt{mean}(X_i))$
         \STATE $\chi_i = \texttt{ema}(\chi_i, \texttt{mean}(X_i \odot X_i))$
         \STATE Create $P_i$ from $\mu_i$ and $\chi_i$ (\S \ref{sec:online})
         \STATE Define the \Eqref{eq:mvp} MVP function, $M_i(\cdot)$, using the VJP and JVP of $\mathbf{f}_{X_i}$
         \STATE $\hat{\theta}_i \gets \texttt{cg}(A=M_i, b=g_i, P=P_i, x_0=\hat{\theta}_i)$
      \ENDFOR
      \STATE $\theta_B \gets (1-\sqrt{\eps}\rho)\theta_B$  \hfill $\triangleright$ Weight decay
      \STATE $z = \max(z_0, \hat{\theta}_B \cdot g_B)$  \hfill $\triangleright$ Norm
   \STATE $\theta_B \gets \theta_B - \sqrt{\frac{\eps}{z}} \hat{\theta}_B$ \hfill $\triangleright$ Adaptive step size
   \ENDFOR
   \OUTPUT{$\theta_B$}
   \end{algorithmic}
\end{algorithm}

\subsection{Online Learning}
\label{sec:online}
As presented, each step of boosting should be performed using the entire dataset,
which is quite impractical. We can easily switch to using an online estimator
of the gradient vector, \eg, an exponential moving average (EMA) over minibatches.
However, computing an online estimate for $M_i$ in our setting is difficult because we
rely on MVPs and do not materialize it nor $M_i^{-1}$.
One alternative, at the expense of increased compute and memory during training,
is to estimate $M_i$ using samples from a much larger dataset, which has demonstrated
to help under the FIM \citep{pascanu-2014}. Because $M_i$ does not require labels
we can compute it via only forward passes.
A second option, when $M_i$ can be materialized in memory, is to use standard techniques for
online covariance estimation \citep{dasgupta}.
If eigendecomposition can be afforded, a third option is to initialize $M_i^{-1}$ with the identity and
perform k-rank updates via the Woodbury matrix identity; this could also be amortized across gradient updates.

Instead of the previous three options, we instead propose to use an online, approximate
estimate of the feature moments as a preconditioner for the linear system solver.
Specifically, we use the Conjugate Gradient algorithm where we precondition each conjugate
step with the approximate solution to facilitate quick convergence.
The form of the preconditioner, $P_i$, is dependent on if the respective $f_i$ has a bias term or not.
If $f_i$ does not have a bias term, we approximate $M_i^{-1}$ using its diagonal,
$P_i = \E[\texttt{diag}(xx^T)]^{-1}$. If $f_i$ has a bias term, 
we approximate $W_iW_i^T$ from \S \ref{sec:WW} with the incomplete Cholesky factorization
$P_i = \widetilde{W}_i\widetilde{W}_i^T$, where $\widetilde{W}_i$ approximates the respective $\Sigma_i$ term with its diagonal.
Note that this does \emph{not} approximate $M_i^{-1}$ with a diagonal matrix, since it still contains the
off-diagonal $\mu_i$ terms.

We construct the terms in $P_i$ using an EMA of the moments $\mu_i$ and $\chi_i = \E[x \odot x]$
across the minibatches\footnote{Note that these quantities are also straightforward to compute for convolutions:
$\mu_i$ is the mean over all pixels in the minibatch.}.
Then, $\E[\texttt{diag}(xx^T)] = \chi_i$ and
$\texttt{diag}(\Sigma_i)= \chi_i - \mu_i \odot \mu_i$.
When initializing conjugate gradient with the solution from the previous LNB step and using 
$P_i$ as the preconditioner, we found that we obtain diminishing returns
on the solution quality after only two iterations in practice. The online version of LNB is
summarized in Algorithm \ref{algo2}. Note that this approach is also amenable to the distributed
training setting where the sufficient statistics, $\mu$ and $\chi$, can be asynchronously updated and communicated
to different nodes (as with the gradient vectors) while each node runs its own conjugate gradient solver.

\subsection{Complexity}
The space complexity of computing a LNB step is the same as backpropagation since
it reuses the feature sets $X_B$. However, the actual amount of used memory is increased by
approximately the size of $X_B$ in order to evaluate the MVP. Additionally, conjugate
gradient holds additional copies of the parameters.

For each neuron, the time complexity to solve the linear system is $O(n_id_i^2 + d_i^3)$, in general.
However, in practice, due to the outer-product nature of the metric, one MVP evaluation is
$O(n_i d_i)$ and we perform only 2 steps of conjugate gradient.
