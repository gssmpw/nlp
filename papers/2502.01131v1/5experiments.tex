\begin{figure}[t]
    %
    \begin{center}
    \centerline{\includegraphics[width=0.5\columnwidth]{experiments/recht_loss.pdf}}
    %
    \caption{Matrix factorization via a 2-layer linear network}
    \label{recht}
\end{center}
\end{figure}

\section{Experiments}
\label{sec:experiments}
The prior works of FOOF, LocoProp, PRONG have shown to compare competitively
with other sophisticated optimizers such as K-FAC \citep{martens2015}.
Given their discussed equivalence with LNB, we focus on experimentally confirming
the contributions of this work: feature whitening via preconditioning the
gradient vector, and the applicability and effectiveness on
the realistic networks of ViT \citep{vit} and UNet \citep{unet}.

Due to its de facto status, we benchmark convergence with Adam in both
iteration and wall time.
In all experiments, the default EMA values were used ($\beta_1=0.9$, $\beta_2=0.999$)
for Adam while the learning rate was grid searched around $1e^{-4}$
All timings were recorded from a NVIDIA L4 GPU. Due to the deterministic nature of performing
2 conjugate gradient steps for LNB, the variance in reported times is negligible.


\subsection{Matrix factorization}
We start with the pathological example from \citet{recht}. This is a matrix
factorization problem formulated as a two-layer linear network:
$\sum_{i=1}^{n} \Vert W_1 W_2 x_i - y_i\Vert^2$,
where $y_i=Ax_i$ for a poorly conditioned matrix, $\kappa(A)=10^5$. Due to the conditioning
and the columns being correlated, it is known that gradient descent converges slowly for this
problem, whereas GN converges quickly. Because the LNB preconditioner is decorrelating
the feature space, we would also expect fast convergence.

We use the same initialization as in the notebook, but in order to magnify the differences,
we increase the dimensions by a factor of $10$: $n=10^4$,
$W_2 \in \sR^{60 \times 60}$, $W_1 \in \sR^{100 \times 60}$. Learning rates were tuned via
grid search to find fast and stable convergence for each method. The results are plotted in Figure \ref{recht}
and reproduce the prior reported slow convergence of Adam and demonstrate fast convergence with LNB.

\begin{figure}[t]
    %
    \begin{center}
    %
    \subfigure[Original pixels ($x$)]{\includegraphics[width=0.49\columnwidth]{experiments/mnist_acc.pdf}}
    %
    \subfigure[Inverted pixels ($1-x$)]{\includegraphics[width=0.49\columnwidth]{experiments/inv_mnist_acc.pdf}}
    %
    \vskip -0.1in
    \caption{MNIST test accuracy evolution trained on the original data (a) vs. inverted pixels (b).
    The step-size is parenthesized.}
    \label{fig:mnist}
    \end{center}
\end{figure}


\subsection{MLP}
We reproduce the MLP result in \citet{grub2010} that compares boosting and gradient
descent for a 2-layer MLP on MNIST using 800-node layers, $\tanh$ activation,
Glorot Normal initialization and a batch size of 1,000.

In Fig \ref{fig:mnist}-a., we plot the test accuracies w.r.t. epoch with the best two learning rates for Adam.
We first note that Adam and LNB obtain better than the prior reported accuracy of $98.3\%$.
Second, LNB achieved this performance with a fixed (ridge) regularizer $\lambda$, whereas prior
work heavily tuned this.
One explanation for the difference is that LNB is taking an adaptive step size according to the metric
and this was not derived before.
Third, there is little observed difference between boosting and gradient descent on this dataset.
This can be explained due to that most of the binary pixels in MNIST are zero, so the feature space
of the vectorized images is low rank, i.e., decorrelating provides little benefit.

\begin{figure}[t]
    \begin{center}
    \subfigure[\texttt{train} loss]{\includegraphics[width=0.49\columnwidth]{experiments/vit_loss.pdf}}
    \subfigure[\texttt{test} accuracy]{\includegraphics[width=0.49\columnwidth]{experiments/vit_acc.pdf}}
    %
    \caption{ViT performances on CIFAR10.}
    \label{fig:vit}
    \end{center}
\end{figure}

However, whitening does include a centering step. If we were to shift the feature space, we would expect
to get same performance. In Fig \ref{fig:mnist}-b, we plot the same models when trained and tested on pixels
$1-x$, where $x$ are the original binary pixel values used in Fig \ref{fig:mnist}-a. We observe that LNB gets very similar performance
while Adam (and other methods not invariant to affine reparameterizations) degrade. Although we could (and should)
simply normalize the features before training, this example illustrates a case of how feature scaling can
greatly affect convergence.

\subsection{Vision Transformer}
We train a vision transformer \cite{vit} using the notebook from Equinox \cite{eqx}
on CIFAR10.
The only modification we make is to not learn the affine terms in the LayerNorm
in order to speed up experimentation and we observed no performance benefit with it.
The train and test fold performances are show in \Figref{fig:vit}, where we observe
faster convergence and better generalization with LNB. Excluding JIT compilation time,
the duration per epoch for LNB and Adam is 1.26 min and 0.85 min, respectively. 

\begin{figure}[t]
    \begin{center}
    \subfigure[\texttt{train} loss]{\includegraphics[width=0.49\columnwidth]{experiments/voc_loss.pdf}}
    \subfigure[\texttt{val} accuracy]{\includegraphics[width=0.49\columnwidth]{experiments/voc_acc.pdf}}
    %
    \caption{UNet performances on VOC Segmentation.}
    \label{fig:unet}
    \end{center}
\end{figure}

\subsection{UNet}
We train a UNet \cite{unet} on the 2012 VOC Segmentation Challenge dataset \cite{voc}. The images are
pixelwise normalized into the range $[0,1]$ using ImageNet mean and variance R,G,B pixel values
and then zero-padded to $500 \times 500$ size and then downsized to $384 \times 384$.
No data augmentation is performed. We plot the results in \Figref{fig:unet} and
remark that LNB converges very quickly, using the same learning rate as with ViT, and
avoids overfitting. While both optimizers converge to comparable performance on the
\texttt{val} split, the rapid progress by LNB suggests it would be able to leverage
more data effectively.
However, excluding JIT compilation time,
the duration per epoch for LNB and Adam is 2.92 min and 1.27 min, respectively, and
this highlights the trade-off between convergence w.r.t. iterations vs. wall time.
While LNB converged marginally faster in wall time and is significantly
easier to implement to prior equivalent work, it is future work to better understand
in what deep networks does the whitening behavior lead to better generalization as
demonstrated in the other three experiments.
