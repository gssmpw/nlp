\section{Introduction}

First-order optimization techniques for training deep networks on large datasets continue
to yield impressive results on a variety of tasks. Typically, these networks are a composition
of linear functions with nonlinear activations and require multiple epochs over a dataset
that is as large as possible. We refer to these linear functions (\emph{without} the nonlinear activations)
as \newterm{neurons}. However, due to the deeply compositional structure of the network,
achieving fast convergence in practice can sometimes be difficult due to the relative scaling
of values between layers, \eg, vanishing and exploding gradients.
Due to these practical challenges there is wide research into techniques
such as adaptive gradient updates \citep{kingma2014, salimans, rmsprop, momentum},
and improved optimizers \citep{amari1998, Schraudolph2002, martens2020}.

One way to avoid the optimization issues related to feature scaling
(\eg, arbitrarily scaling a feature dimension by a large value typically induces a large gradient vector)
is to use non-parametric methods and forgo optimizing in parameter space, such as with 
Gaussian Processes and kernel methods \cite{kernelbook}.
Another alternative is to optimize in the space of functions, where the corresponding inner product
definition is independent of the parameterization \citep{mason2000,friedman2001}.
In the context of training compositional networks, this idea has
been investigated in \citet{grub2010} where they derive how gradient descent in function space can be used
to backpropagate errors to the respective nested functions; this is referred to as
\emph{boosted backpropagation}.
As will be reviewed in \S \ref{sec:bbp},
this approach no longer explicitly parameterizes each function, but instead operates
in the boosting regime and learns a set of strong-learners, each of which is additive
in a series of weak-learners.

In general, the weak-learners can be any function that is differentiable \wrt
its inputs, including a multilayer perception (MLP), for example. This leads to a design question of whether
to construct (a) a shallow network composed of highly nonlinear strong-learners, 
\eg, each strong-learner is a sum of wide MLP weak-learners,
or (b) a deep sequence of linear strong-learners composed with nonlinear activations. 
For the known state-of-the-art performance they can achieve, we study case (b) of learning
the linear neurons of nonlinear deep networks.

After reviewing the requisite definitions and notations in the next section,
in \S \ref{sec:lnb} we reduce boosted backpropagation for linear neurons to 
a covariant gradient descent algorithm with a metric (inner product space) that corresponds to the
column space of the features that are input to each respective neuron. We discuss how this reduction is equivalent to performing
feature whitening in the network, but the boosting perspective motivates a practical
training algorithm and informs regularization and adaptive step sizes.
Our primary contribution is an easy to implement algorithm to
perform linear boosted backpropagation that is applicable to any network architecture compatible with
autodifferentiation and does not require specialized compute kernels.
A secondary contribution is making the equivalence of this interpretation with prior work such as
Natural Neural Networks \citep{nnn}, FOOF \citep{foof}, and LocoProp \cite{locoprop}.
In addition to ease of implementation, we demonstrate in \S \ref{sec:experiments} that the
algorithm is also efficient with popular architectures in practical settings.
