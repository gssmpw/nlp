\section{Notation and Background}
We denote the derivative of a function $f: \gU \to \gV$
at $v = f(u)$ as the linear map $\frac{\partial f(u)}{\partial u}: \gU \to \gV$,
or equivalently as $\frac{\partial v}{\partial u}$.
In general, $u$ can represent a high dimensional tensor,
such as a feature map or the filter parameters in a convolutional neural network (CNN).
Similarly, we denote the transpose of the derivative as
$\frac{\partial v}{\partial u}^T: \gV \to \gU$.
Throughout this work we'll operate in vector spaces;
when $f$ is a scalar function, we interchangeably denote
the gradient vector at $u$, $\nabla f(u) \in \gU$, as the transpose of its derivative,
$\frac{\partial v}{\partial u}^T  \in \gU$.

\subsection{Preconditioned Gradient Descent}
\label{sec:pcgd}
We denote $\ell_{(x,y)}: \Theta \to \sR$ as the pointwise loss function for a labeled sample,
where $x$ and $y$ are the inputs and labels, respectively, and the 
samples are drawn from a data distribution, $\gD$.
Minimizing the expected loss, 
$\bar{\ell}(\theta) = \E_{(x,y) \sim \gD} [\ell_{(x,y)}(\theta)]$,
via gradient descent leads to the update rule
$\theta \leftarrow \theta - \alpha^{(t)} \nabla \bar{\ell}(\theta)$,
where $\alpha^{(t)}$ is the desired step size at iteration $t$.
Depending on the form of $\ell$, convergence can be improved by preconditioning
the gradient vector by a matrix $P$, \eg, using the inverse of the Hessian of $\bar\ell(\theta)$ when performing
Newton's method.
In general, when the preconditioner can be factored as $P=WW^T$, the update rule
of following the preconditioned gradient vector, $WW^T \nabla \bar{\ell}(\theta)$, is equivalent to
performing gradient descent on $\bar\ell(\acute\theta)$ with the reparameterized model, $\acute\theta = W\theta$.

%

\subsection{Natural Gradient Descent}
\label{sec:cgrad}
One way to view natural gradient descent \cite{amari1998} is as
a trust-region optimization problem to find a small step, \wrt some norm,
that is aligned in the direction of $\nabla\bar\ell(\theta)$:
%
\begin{align}
\argmin_{\delta \theta} & ~ \bar{\ell}(\theta) + \nabla \bar{\ell}(\theta) \cdot \delta\theta \nonumber\\
\text{s.t.} & ~ \frac{1}{2} \langle \delta\theta, \delta\theta\rangle_{M_{\theta}}  = \eps, \label{eq:trust}
\end{align}
%
where $\langle u, v\rangle_{M_{\theta}} > 0$ is the inner product
defined by the (Riemannian) metric $M_{\theta}$ in the local tangent space at $\theta$.
%
Solving for the stationary point of its Lagrange function results in an update step
$\delta \theta \propto M_{\theta}^{-1} \nabla \bar{\ell}(\theta)$.
When defining $M_\theta$ as the Fisher Information Matrix (FIM), the resulting
vector is referred to as the natural gradient vector; see \citet{kunster2019} for an informative review and discussion.
However, in general, we are free to choose any positive-definite metric, $M_{\theta} \succ 0$,
and we denote the resulting vector under the metric as $\nabla_{M} \ell(\theta) = M_{\theta}^{-1} \nabla \ell(\theta)$.
%
%
Note that taking a step solely proportional $\nabla_{M}\bar{\ell}(\theta)$
ignores the trust region constraint of \Eqref{eq:trust};
solving for it obtains the appropriate step size, $\alpha_{\theta}$,
in the original parameter space that induces an  $\eps$-sized step under the chosen metric:
$\alpha_{\theta} = \sqrt{\frac{\eps}{z_\theta}}$, where
%
\begin{equation}
\label{eq:metricnorm}
z_\theta
= \langle \nabla_{M} \bar{\ell}(\theta), \nabla_{M} \bar{\ell}(\theta) \rangle_{M_{\theta}}
= \nabla_{M} \bar{\ell}(\theta) \cdot \nabla \bar{\ell}(\theta).
\end{equation}
%
%
%
%
%

%
%
%
%
%

\subsection{Functional Gradient Descent}
\label{sec:fgrad}
In contrast to optimizing a function in the space of parameters,
an alternative is to optimize in the space of functions, $\gF$, which
can also be viewed as boosting \citep{mason2000, friedman2001}.
The following summarizes \S2 of \citet{grubbthesis} to introduce notation on this topic.
We denote $\bar{\gL}[f] = \E_{(x,y) \sim \gD}[l_{y}(f(x))]$ to be the loss functional
that computes the expected loss for a given function $f: \gX \to \gV$,
where $l_y: \gV \to \sR$ is loss function (using label $y$) in the image (output space) of $f$,
and $\gV$ is an application-specific vector
space\footnote{In general, each element in $\gV$ need not be a 1-D vector and each could
be a multi-dimensional tensor. 
For example, in $k$-class logistic regression, the image of $f$ is the predicted logits vector
($\gV = \sR^k$) and $l$ is pointwise log-loss.
Whereas for $k$-class semantic image segmentation, each the image of $f$ is a tensor
($\gV = \sR^{h \times w \times k}$) whose outer component is the per-pixel logits 
and $l$ is the per-pixel log-loss summed over the inner spatial components.}.
Letting $v_x = f(x)$, the functional gradient ``vector'' of $\bar{\gL}$ at $f$ is defined as the
\emph{function},
%
\begin{equation*}
\nabla_F \bar{\gL}[f] = \E_{(x,y) \sim \gD} [\nabla_F l_y(v_x)] = \E_{(x,y) \sim \gD} [ \lambda_{(x,y)} \mathbbm{1}_x],
\end{equation*}
%
where each $\lambda_{(x,y)} = \frac{\partial l_y(v_x)}{\partial v_x}^T \in \gV$
is a gradient vector, and $\mathbbm{1}_x: \gX \to \{0, 1\}$ is the Dirac delta function centered at $x$.
For brevity, let $\triangle_f = \nabla_F \tilde{\gL}[f]$.

Instead of the defining the strong-learner, $f$, additively in $\triangle_f \in \gF$,
each $\triangle_f$ is projected onto a smaller hypothesis space, $\gH \subseteq \gF$,
such as small decision trees or MLPs, in order to generalize.
The projection of $\triangle_f$ onto a hypothesis $h \in \gH$ is analogous to vector projection in Euclidean space:
$\frac{\langle \triangle_f, h \rangle_F}{\Vert h \Vert_F}\frac{h}{\Vert h \Vert_F}$,
where $\langle f,g \rangle_F = \mathbb{E}_x[f(x) \cdot g(x)]$ is the inner product in function
space with norm $\Vert h \Vert_F = \sqrt{\langle h,h \rangle_F}$.

When $\gH$ are regressors, the hypothesis, $\hat{h}$, that maximizes the scalar projection term is
equivalent \citep{friedman2001} to minimizing a least squares problem,
%
\begin{equation}
\label{eq:scalarproj}
\hat{h} = \argmax_{h \in \gH} \frac{\langle \triangle_f, h \rangle_F}{\Vert h \Vert_F}
= \argmin_{h \in \gH} \E_{x} [ \Vert h(x) -\triangle_f(x) \Vert^2 ].
\end{equation}
%
In practice, this translates to training a (vector-output) regressor
over the dataset $\{(x,\lambda_{(x,y)})\}$.
Finally, this leads to the update rule
$f \leftarrow f - \eps^{(t)} \frac{\hat{h}}{\Vert \hat{h} \Vert_F}$,
and we refer to each $\hat{h}$ as weak-learners.

\subsubsection{Regularization}
\label{sec:reg}
In addition to minimizing the incurred loss of $f$, we may also want to include
a regularization term in the objective $\tilde{\gL}[f] + \frac{\rho}{2} \Vert f \Vert_F^2$, where 
$\rho \geq 0$. The update rule with the regularized objective is
%
\begin{equation}
\label{eq:reg}
f \gets (1-\eps^{(t)}\rho) f -  \eps^{(t)} \frac{\hat{h}}{\Vert \hat{h} \Vert_F}.
\end{equation}
%
In practice, this is implemented by shrinking the existing weak-learner coefficients $\{\eps^{(j)}|j<t\}$
by the factor $0 \leq (1-\eps^{(t)}\rho) \leq 1$.

\subsection{Boosted Backpropagation}
\label{sec:bbp}

For simplicity in explanation, we begin with the problem of training a feed-forward
network (FFN), as done in \citet{grub2010}, and note that general architectures
will be discussed in the next section.
We represent a FFN as a composition of $m$ differentiable functions
$F = \{f_i:\gX_{i-1} \to \gX_i| 1 \leq i \leq m \}$, where a subset $F_B \subseteq F$
are neurons with parameters $\theta_i$ that we want to train, \eg,
$f_i(\cdot;\theta_i) \in F_B$ could be convolutional layer with filters $\theta_i$.
We denote $\theta_B$ as all the trainable parameters in the network.
The complement set of functions, $F \setminus F_B$, are fixed, \eg, activation functions, resizing layers, etc.
We denote the loss incurred for sample $(x,y)$ for the given network parameters
as $\ell_{(x,y)}(\theta_B) = (l_y \circ f_m \circ \ldots \circ f_1)(x)$,
where $l_y: \gX_m \to \sR$ computes the loss of the last layer's prediction vector with label $y$.
Backpropagation can be used to compute the gradient vector of the
expected loss, $\nabla \bar\ell(\theta_B)$,
and we denote $g_i$ to be the component of $\nabla \bar\ell(\theta_B)$
that corresponds to $\theta_i$, i.e., $g_i = \frac{\partial\bar\ell(\theta_B)}{\partial \theta_i}^T$.

Forgoing parameterizing the FFN neurons with weights,
\citet{grub2010} optimize the loss \wrt each neuron's \emph{function}
by using the adjoint state method for
the equivalent constrained minimization problem,
%
%
%
\begin{align}
\argmin_{F_B} ~& \E_{(x,y) \sim \gD} [l_y(x_{m})] \nonumber \\
\begin{split}
\label{eq:forward}
\text{s.t.} ~& x_0 = x, x_{i} = f_{i}(x_{i-1}), i \in [1, \ldots, m].
\end{split}
\end{align}
%
Using the necessary conditions for a stationary point of its Lagrangian,
they describe an algorithm to recursively compute the functional gradient vector for each neuron.
For $f_i \in F_B$ in a FFN, its functional gradient vector is $\lambda_i \mathbbm{1}_{x_{i-1}}$,
where
%
\begin{align}
\lambda_{m+1} &= \frac{\partial l_y(x_m)}{\partial x_m}^T, \nonumber \\
\lambda_{i} &= \frac{\partial x_i}{\partial x_{i-1}}^T \lambda_{i+1},  i \in [1, \ldots, m],
\label{eq:costates}
\end{align}
%
are the errors backpropagated from the loss layer via vector-Jacobian products (VJPs).
That it is, each training step is analogous to performing normal backpropagation
with the key difference of training a weak-learner using the dataset 
$\{(x_{i-1}, \lambda_i)\}$ for each $f_i \in F_B$,
instead of pulling-back the targets, $\lambda_i$, through respective neuron's local derivative.
That is, $f_i$'s component of $\nabla \bar\ell(\theta_B)$ is
$g_i = \frac{\partial x_i}{\partial \theta_i}^T \lambda_i$.

As presented, the time complexity for computing the unprojected
functional gradient vectors is the same backpropagation; however,
the storage complexity increases linearly with the dataset and dimensionality
due to aggregating the regression targets.
The projection operation then adds significant compute as it requires solving a
large vector regression problem for each neuron\footnote{\eg,
for a convolutional layer, each $\lambda_i$ represents a
$\sR^{h \times w \times d}$ feature map and there are $|\gD|$ of them.}.
Lastly, the presented algorithm is specific to a FFN and it is left
as an exercise to construct the corresponding recurrence relation of \Eqref{eq:costates}
for different network architectures, which can be error prone.
In the next section we address these concerns and provide a simple and efficient boosting algorithm
for any network architecture compatible with autodifferentiation.
