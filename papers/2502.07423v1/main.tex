




\documentclass[10pt,letterpaper]{article}

\usepackage{cogsci}

\cogscifinalcopy %


\usepackage{pslatex}
\usepackage{apacite}
\usepackage{float} %



\setlength\titlebox{4.5cm}



\usepackage[round]{natbib}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage[acronym]{glossaries}
\usepackage{array}
\usepackage{booktabs}
\usepackage[hidelinks,breaklinks]{hyperref}

\input{math_commands.tex}
\input{acronyms.tex}


\title{Towards a Formal Theory of the Need for Competence\\
via Computational Intrinsic Motivation}
 
\author{
{\large \bf Erik M.~Lintunen (erik.lintunen@aalto.fi),}\\
{\large \bf Nadia M.~Ady (nadia.ady@aalto.fi),}\\
{\large \bf Sebastian Deterding (s.deterding@imperial.ac.uk),}\\
{\large \bf Christian Guckelsberger (christian.guckelsberger@aalto.fi)}
}

\begin{document}

\maketitle


\begin{abstract}
Computational models offer powerful tools for formalising psychological theories, making them both testable and applicable in digital contexts.
However, they remain little used in the study of motivation within psychology.
We focus on the ``need for competence'', postulated as a key basic human need within Self-Determination Theory (SDT)---arguably the most influential psychological framework for studying intrinsic motivation (IM).
The need for competence is treated as a single construct across SDT texts. Yet, recent research has identified multiple, ambiguously defined facets of competence in SDT.
We propose that these inconsistencies may be alleviated by drawing on computational models from the field of artificial intelligence, specifically from the domain of reinforcement learning (RL).
By aligning the aforementioned facets of competence---effectance, skill use, task performance, and capacity growth---with existing RL formalisms, we provide a foundation for advancing competence-related theory in SDT and motivational psychology more broadly.
The formalisms reveal underlying preconditions that SDT fails to make explicit, demonstrating how computational models can improve our understanding of IM.
Additionally, our work can support a cycle of theory development by inspiring new computational models formalising aspects of the theory, which can then be tested empirically to refine the theory.
While our research lays a promising foundation, empirical studies of these models in both humans and machines are needed, inviting collaboration across disciplines.

\textbf{Keywords:}
need for competence;
self-determination theory;
intrinsic motivation;
motivational psychology;
theory development;
intrinsically motivated reinforcement learning;
computational modelling
\end{abstract}


\section{Introduction}

\Gls{sdt} \citep{deci1985intrinsic} is one of the most influential psychological theories of motivation, including \gls{im}. It has also inspired \gls{ai} research on computational \gls{im} (e.g., \citealp{oudeyer2007what}). Computational \gls{im} equips artificial agents with abilities for open-ended development, task-agnostic learning, and dealing with the potential sparsity of rewards \citep[p.~1161]{colas2022autotelic}. Notably, \gls{sdt}, like many psychological theories, is expressed in ``soft'' verbal propositions. In light of the current theory crisis \citep{oberauer2019addressing}, verbal theory has drawn increasing critique: theories that are not formally specified may fail to produce hypotheses that can be rigorously tested. While many psychology researchers have advocated for computational modelling as a paradigm for theory development (e.g., \citealp{marsella2010computational,oberauer2019addressing,robinaugh2021invisible}), formal specification in the form of precisely defined computational models remains largely absent in \gls{sdt}. Judging by the most recent integrative volume of the theory, the \emph{Oxford Handbook of Self-Determination Theory} \citep{ryan2023oxford}, methodological advances chiefly focus on new psychometric approaches and neuroscientific evidence.

Here, we make the case that computational models of \gls{im} can benefit the development of \gls{sdt}. More specifically, \citet{deterding2024why} have shown that the \emph{need for competence}, one of the basic psychological needs underpinning \gls{im} in \gls{sdt}, is poorly specified, with multiple definitions that lack clear operationalisation. They derive four distinct facets of competence by means of a detailed analysis of \gls{sdt} texts spanning many decades (\citeyear{deci1985intrinsic}--\citeyear{ryan2023oxford}), reproduced in \autoref{table:facets} \citep[pp.~7--10]{deterding2024why}. We focus on matching computational formalisms to each of these facets.

\begin{table}[h]
\centering
\caption{\textbf{The four distinct facets of competence} identified by \citet[pp.~7--10]{deterding2024why}.} 
\label{table:facets} 
\vskip 0.12in
\begin{tabular}{>{\raggedright\arraybackslash}p{0.45\linewidth}>{\raggedright\arraybackslash}p{0.45\linewidth}}
\toprule
\textbf{Effectance (C1)} & Observing that one's action causes consequences, which can be unintended. \\
\midrule
\textbf{Skill use (C2)} & Observing \textbf{(a)} an opportunity (an appropriate situation) to use a skill and/or \textbf{(b)} that one uses a particular capacity in the course of one's action. \\
\midrule
\textbf{Task performance (C3)} & Observing that one performs \textbf{(a)} well at an intended task, \textbf{(b)} to an extent that requires a certain skill or skill level. \\
\midrule
\textbf{Capacity growth (C4)} & Observing gain in the \textbf{(a)} strength and/or \textbf{(b)} range of one's skills. \\
\bottomrule
\end{tabular}
\end{table}

One reason computational models of \gls{im} offer a powerful paradigm for developing \gls{sdt} is that translating a concept like competence into an algorithmic description induces a different way of thinking about theory: the process forces us to specify abstract and informal verbal theories to the point that they can be turned into a system that can actually produce observable behaviour. Observing how a theory behaves can ``reveal implicit assumptions and hidden complexities'' in verbal theory, thus inspiring theory construction \citep[p.~4]{marsella2010computational}. Murayama's (\citeyear{murayama2022reward-learning}) recent adoption of \gls{rl} as an underpinning framework for a theory of curiosity is a good case in point, as is Ady et al.'s work on specific machine curiosity (\citeyear{ady2022five}), to name a few.

To advance formal development of \gls{sdt} and competence-related theory in motivational psychology more broadly, a key challenge is identifying formalisms that sufficiently approximate the theory. In this paper, we align each distinct competence facet with examples of formalisms from the computational literature. We then discuss how each example formalises aspects of the theory. These formalisms not only lay a foundation for specifying the need for competence but also unveil a wealth of existing implementations, simulators, and evaluation metrics from the \gls{ai} literature for use in psychological motivation research. In future work, these will allow us to test whether formalisations fitting the facets, either individually or in conjunction, produce empirical results that align with propositions made by the theory.


\section{Background: Computational IM \& RL}
\label{sec:imgcrl}

We begin by justifying why \gls{rl} is a suitable domain from which to draw computational models of competence. Additionally, to support understanding of the models we discuss, we introduce some key concepts from intrinsically motivated \gls{rl}---most notably, \emph{goals} and \emph{skills} and their relationship to the reward function---and recommend further reading.

The idea that \gls{im} might be driven by biological reward systems (\citealp[p.~113]{barto2004intrinsically}, in reference to \citealp{kakade2002dopamine,dayan2002reward})
has driven the choice of methods centrally used in computational \gls{im}: \gls{rl} \citep[p.~252]{baldassarre2022intrinsic}. Due to their usefulness, intrinsically motivated \gls{rl} methods have been studied extensively in the computational \gls{rl} literature at large (see reviews by \citealp{colas2022autotelic,aubret2023information,lidayan2024bamdp}). An assumption underlying much work in \gls{rl} is the \emph{reward hypothesis}: ``all of what we mean by goals and purposes can well be thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward)'' (\citealp[p.~53]{sutton2018reinforcement}; cf., \citealp[p.~4]{silver2021reward}). This hypothesis informs the formalisms used for \gls{cbim}, which tend to rely on the assumption that every goal can be sufficiently defined by a reward function. \emph{Intrinsically motivated} \gls{rl} is characterised by the use of task-agnostic intrinsic reward functions that collate or compare agent-internal variables independently of their semantics (\citealp[p.~3]{oudeyer2008how}; cf., \citealp[p.~246]{berlyne1965structure} and \citealp{oudeyer2007what}).

Many of our examples operate within the framework of \gls{gcrl} (see review by \citealp{liu2022goal}), which extends the standard definition of a reward function to be conditioned on goals. A \textit{goal}, or a \textit{goal-defining variable}, is a parameter to the reward function (cf., \citealp[p.~1165]{colas2022autotelic}; \citealp[p.~6]{aubret2023information}). Then, a \textit{skill} is a policy given a goal, optimising for the return according to the reward conditioned on that goal. Examples of goal-defining variables include indices \citep[e.g.,][]{eysenbach2019diversity} or elements drawn from a learned distribution \citep[e.g.,][]{nair2018visual}. Their role is simply to indicate which reward function the agent is aiming to maximise.

Goals can be viewed as ``a set of \emph{constraints} ... that the agent seeks to respect'' \citep[p.~1165, emphasis in original]{colas2022autotelic}. While the most immediate intuition of a goal is often as a desired state for the agent to reach (e.g., \citealp[p.~1]{kaelbling1993learning}; \citealp[p.~2]{schaul2015universal}), the formalism allows for a more general set of constraints on behaviour \citep[pp.~22--23]{lintunen2024advancing}. In effect, any behaviour that can be defined by attempting to maximise some reward function on the environment can be formulated as a goal--skill pairing.


\section{Aligning Computational Models With SDT}
\label{sec:alignmodelswithsdt}

Based on a review of classic \citep{oudeyer2007what} and state-of-the-art \citep{colas2022autotelic,aubret2023information} computational \gls{im}, we next turn our focus to matching computational formalisms to each of the competence facets (\autoref{table:facets}). Our matching is non-exhaustive, and instead we focus on exemplifying how each facet could be formalised.



\subsection{Effectance (C1)}
\label{sec:effectance}

Let us consider the case where competence in \gls{sdt} refers to \emph{effectance}, i.e., motivation by observing that one's own actions cause change in the environment. To recognise effectance, an agent needs a mechanism to distinguish whether a change in the environment is caused by the agent itself or some external factor. An example of a computational model fitting effectance is \gls{ride} \citep{raileanu2020ride}, as part of which the agent is rewarded for the amount of change it causes in its state observation: the ``impact'' of the agent's actions, measured between two consecutive observations. Specifically, the \emph{impact-driven reward} is defined as:
\begin{align}\label{eq:ride}
R_t(\rvs_t, \rvs_{t+1}) := \frac{\left\lVert\phi(\rvs_{t+1})-\phi(\rvs_t)\right\rVert_2}{\sqrt{N(\rvs_{t+1})}},
\end{align}
where $\phi(\rvs_t)$ and $\phi(\rvs_{t+1})$ are the learned representations of two consecutive states and $N(\rvs_{t+1})$ represents or approximates the number (count) of times the latter state has been visited \citep[pp.~4--5]{raileanu2020ride}.

In \gls{ride}, the agent computes its reward using the learned representation of the observation as opposed to the raw sensory space (e.g., pixels). Inspired by \citet{pathak2017curiosity}, the parameters of the model $\phi$ are optimised so that an agent learns the features it can control---that is, it learns to ignore aspects of the environment it cannot affect. Decomposing the reward function, the agent is rewarded for a combination of changes in those state features that it is able to consistently have some effect on (Eq.~\ref{eq:ride}, numerator) and the novelty of the state into which its action leads (Eq.~\ref{eq:ride}, denominator). The effect of the denominator is that the reward obtainable from particular states diminishes the more the agent observes them, even if the agent's action has the same effective outcome as previously observed. In recent \gls{sdt} texts, effectance motivation does not require novelty, but the original usage of the term required observations to be novel \citep[p.~322]{white1959motivation}. By having the reward given by \gls{ride} wane the more a particular situation is perceived, \gls{ride} offers a simple formalisation of this original novelty aspect of effectance motivation.


\subsection{Skill use (C2)}
\label{sec:skilluse}

We next consider the case in which competence refers to \emph{skill use} or the ``exercise $\ldots$ of one's capacities'' \citep[p.~86]{ryan2017self}. Following \citet[p.~6]{deterding2024why}, we cover the specific understanding of ``exercise'' as implementing or bringing something to bear \citep{webster2025}; an alternative interpretation as engaging a skill repeatedly in order to grow it will be treated under \nameref{sec:capacitygrowth}. As per \autoref{table:facets}, competence as skill-use motivation can be understood in two different ways: observing an opportunity (an appropriate situation) to use a skill (\hyperref[sec:c2a]{C2.a}) and that one uses a particular capacity in the course of one's action (\hyperref[sec:c2b]{C2.b}).


\subsubsection{An opportunity (an appropriate situation) to use a skill (C2.a)}
\phantomsection \label{sec:c2a}

Broadly, there are two sets of computational approaches used to model an agent learning about opportunities to use particular skills: those in which the agent ignores the current state of the world and those in which it does not. In both cases, hierarchy is used: an agent learns a distribution over its set of policies (i.e., a high-level policy) to determine which skill to use. In the former case, an agent follows the same policy regardless of its state observation---in this case, an ``appropriate'' opportunity can be thought of as a learning opportunity, determined not by observing the environment, but from information about how the learning is proceeding. In the latter case, identifying an opportunity to use a particular skill can also draw on observations of the environment.

Specifically, the first case of \hyperref[sec:c2a]{C2.a}, in which an agent is learning about opportunities to use a skill while ignoring the current state of the world, is well exemplified by computational approaches using \gls{lp}, which approximates some derivative of performance over time (initially proposed by \citealp[p.~7]{oudeyer2007intrinsic}; see also related earlier work by \citealp[p.~1461]{schmidhuber1991curious}). In systems employing \gls{lp}, it is typically used as a measure to support the selection of intermediate-difficulty goals with respect to the changing skills of an agent over time.
\Gls{lp}, as an intrinsic reward for goal selection, has been well studied across both machine and human reinforcement learners (e.g., \citealp{colas2019curious,ten2021humans,molinaro2024latent}), and could thus offer good starting points for formalising \hyperref[sec:c2a]{C2.a}.

While a reward function prioritises the choice of skills, an agent also needs a mechanism to select a skill from that prioritisation. One commonly used mechanism is Thompson sampling \citep{thompson1933on}, also known as probability matching, where an agent draws a ``belief''---choosing a skill according to a probability distribution (learned from some reward signal such as \gls{lp} values) over its set of skills---and then acts greedily with respect to the drawn skill until it has more ``evidence'' (e.g., a change in \gls{lp} values) and its ``belief'' (i.e., skill-selection policy) is updated. Thompson sampling has been shown to explain aspects of human exploration in certain tasks \citep{gershman2018deconstructing}, making it a candidate for a psychologically plausible formalism of human skill selection.

An example of a system formalising the second case of \hyperref[sec:c2a]{C2.a}, in which an agent learns about opportunities to use particular skills while accounting for the current state of the world, is \gls{vic}. In \gls{vic}, the appropriate skill for a given starting state is decided using a separate learned model of how empowering the skill has been in the past. We mean \emph{empowering} in the sense of its definition in the \gls{ai} literature,\footnote{``Empowerment'' has also been studied in the context of human exploration, e.g., as ``exploring options that enable the generation of as many more options as possible'' \citep[p.~1482]{brandle2023empowerment}.} as the combined ability of an agent to control its environment and to sense this control (cf., \citealp[p.~2]{salge2014empowerment}; \citealp[p.~2]{gregor2016variational}). The decision over skills is conditional on the state of the world, in that the policy for choosing skills is updated based on how empowering the skill turned out to be \emph{for that given situation}. The distribution over skills is updated during skill use, such that more empowering skills become more likely to be chosen \citep[p.~4]{gregor2016variational}, using the reward \citep[p.~2]{gregor2016variational}:
\begin{align}\label{eq:vicreward}
R_t(\rvs_0,\rvs_t,\rvg) := \underbrace{\log q_\rvphi(\rvg \mid \rvs_0,\rvs_t)}_\text{($\alpha$)} \underbrace{-\log p_\rvtheta(\rvg \mid \rvs_0)}_\text{($\beta$)}.
\end{align}
\begin{description}
  \item[($\alpha$)] The discriminator, $q_\rvphi$, is an arbitrary variational distribution parametrised by $\rvphi$ \citep[p.~2]{barber2003im}. Given the first ($\rvs_0$) and last ($\rvs_t$) observations induced by the skill corresponding to the goal being pursued, $q_\rvphi$ defines a probability distribution over goals. Successfully discriminating goals in the observation space requires the agent to observe distinct regions of the state space. If a goal is not discriminable based on observations, two or more skills are producing overlapping behaviours (and therefore, the skills lack diversity); conversely, if a goal is discriminable, then the corresponding skill is inducing trajectories unique to that skill. Thus, this reward term is maximised when there is no uncertainty in the prediction.
  \item[($\beta$)] The probability model, $p_\rvtheta(\rvg \mid \rvs_0)$, parametrised by $\rvtheta$, represents the goal-selection policy. As part of the reward, the purpose of this term is to reinforce high entropy: the agent should keep pursuing a wide range of goals, for each starting state, $\rvs_0$, as the reward is higher when the goal had a low probability of being selected.
\end{description}

Given a starting state, $\rvs_0$, a goal, $\rvg$, is selected according to the distribution defined by $q_\rvphi$, which the agent pursues until the termination state $\rvs_t$. Then, $p_\rvtheta$ is reinforced based on how empowering the corresponding skill was (estimated using Eq.~\ref{eq:vicreward}). The final state becomes the new starting state, $\rvs_0 \leftarrow \rvs_f$, and the process is repeated. This can motivate the agent to follow skills that lead to distinct end states ($\alpha$) and learn a wide range of skills for any given situation ($\beta$).


\subsubsection{That one uses a particular capacity in the course of one's action (C2.b)}
\phantomsection \label{sec:c2b}

While \hyperref[sec:c2a]{C2.a} is more about finding suitable opportunities to use particular skills, such as in the case of \gls{vic} learning a state-conditional policy over skills, \hyperref[sec:c2b]{C2.b} is more about an agent being motivated by observing that a skill is in use at all. The first term of the \gls{vic} reward function (Eq.~\ref{eq:vicreward}, $\alpha$) can be adapted to emphasise an agent's ability to distinguish that a particular skill is in use at a given time, by only conditioning on the current state. The reward for one such system, \gls{diayn} \citep{eysenbach2019diversity}, can be defined\footnote{See \citealp[Appendix~A, p.~12]{eysenbach2019diversity} for discussion on a constant term omitted here.} as:
\begin{align}\label{eq:diayn}
R_t(\rvs_{t+1}, \rvg) := \log q_\rvphi(\rvg \mid \rvs_{t+1}),
\end{align}
where $q_\rvphi$ is an arbitrary variational distribution parametrised by $\rvphi$ (refer back to the text following Eq.~\ref{eq:vicreward} for detail). An agent is rewarded for its ability to distinguish the current goal, $\rvg$; thus, the overall return is maximised when the agent can perfectly infer which of its skills is in use at any given time.

\Gls{diayn}, in further differentiation from \gls{vic}, ignores the state of the world in its choice of skill, instead choosing skills uniformly at random---in other words, learning skills that are empowering on average over the state space. For further examples, one can turn to other variational empowerment methods (see \citealp[p.~3]{choi2021variational}). The choice to fix the distribution over skills as uniform---although not an intuitive choice for a psychologically plausible model of goal selection, except maybe in special cases such as the absence of prior information---is common in the \gls{cbim} literature. This approach ensures that all skills receive, in expectation, an equal amount of training signal to improve. \Gls{diayn} learns a diverse set of skills across a wide array of situations, such as running, walking, hopping, flipping, or gliding to move through a space \citep[p.~5]{eysenbach2019diversity}. It thus shows that a formalism motivated by skill \emph{use} can drive skill \emph{learning}.


\subsection{Task performance (C3)}
\label{sec:taskperformance}

Next, we discuss computational models for two possible ways of understanding \emph{task performance}: observing that one performs well at an intended task (\hyperref[sec:c3a]{C3.a}) and performing to an extent that requires a certain skill or skill level (\hyperref[sec:c3b]{C3.b}).


\subsubsection{Observing that one performs well at an intended task (C3.a)}
\phantomsection \label{sec:c3a}

Given an agent has set itself a goal (task), if an agent has a mechanism to estimate its progress towards a chosen goal, it can use that estimate as a proxy for its own performance on the task. An example system of this kind is \gls{rig} \citep{nair2018visual}. \Gls{rig} encodes the agent's observations into a lower-dimensional latent space from which the agent samples goals to pursue. Defining the reward as the negative distance between the goal and the current state in latent space encourages the agent to move its observations closer to the goal:
\begin{align}\label{eq:rig}
R_t(\rvs_{t+1}, \rvg) := -\left\lVert\phi(\rvs_{t+1})-\phi(\rvg)\right\rVert_\rmA,
\end{align}
where $\phi(\rvs_{t+1})$ is the learned state representation of $\rvs_{t+1}$, $\phi(\rvg)$ denotes the goal drawn from the latent space, and $\rmA$ is a matrix that can give more importance to some dimensions of the latent space over others \citep[p.~5]{nair2018visual}.

In contrast to \gls{diayn}, in which the agent learns to master a fixed number of skills, in \gls{rig}, the agent selects from an infinite number of goals (the space is continuous). This results in a sort of smoothness of the space that the agent can benefit from: if one goal is sampled near another that the agent has already learned to achieve, it may be able to generalise and use some parts of an existing policy to more efficiently learn to achieve the new goal. In the \gls{rig} system, \emph{goals} take on the intuitive meaning as desired states for the agent to reach (discussed in \nameref{sec:imgcrl}), and ``performing well'' at an intended task is operationalised as minimising the semantic distance between the agent's observations and its self-generated goals. Over time, \gls{rig} agents learn to generate goals similar to the observations they make of their environment, while simultaneously learning the skills necessary to consistently perform well (reach the goals).


\subsubsection{Specifically to an extent that requires a certain skill or skill level (C3.b)}
\phantomsection \label{sec:c3b}

While \gls{rig} captures the idea of motivation by performance on a self-selected task (\hyperref[sec:c3a]{C3.a}), it does not take into account the level of skill needed to perform well on the task (\hyperref[sec:c3b]{C3.b}). For this, we can look to other system designs inspired by the idea of preferring to focus on goals or skills at an appropriate level of difficulty, given what the agent already knows. To identify skills of appropriate difficulty, such systems can make use of \gls{acl} (for a review see \citealp{portelas2021automatic}), enabling an agent to self-organise their goal selection. This can significantly reduce the number of training examples required for an agent to learn to reach its goals \citep[p.~1]{florensa18automatic}.

For example, in the \gls{curious} system by \citet{colas2019curious}, instead of considering task performance as distance from the goal, the system considers its capability to achieve its goals. Specifically, the agent is motivated to pursue goals in certain parts of the sensory space in which it believes to be increasingly or decreasingly successful at achieving its goals, estimated by measuring how successful it has been in the recent past---that is, its \gls{lp}. The underpinning logic of the system is that such goals will be of optimal difficulty, neither too easy nor too difficult. In the \gls{curious} experiments, an agent manipulates cubes by learning to control a robotic arm (\citealp[Supplementary p.~3]{colas2019curious}). Each observation includes information about the gripper and the objects of interest (e.g., their position and rotation), and each goal is a desired observation. The high-dimensional observation space makes learning challenging. Furthermore, some parts of the sensory space are irrelevant for achieving certain goals---for example, if the goal is to manipulate one of several cubes, the agent might not need information about the other cubes. To ignore such irrelevant information in its formulation of goals, \gls{curious} modularises the observation space, which is also the goal space, with each module being a specific, hand-defined subspace of the space. A \gls{curious} agent estimates its own \gls{lp} for each module, and uses the absolute values of the \gls{lp} estimates to compute a probability distribution for selecting the module to generate a goal within \citep[p.~4]{colas2019curious}. This mechanism helps the agent avoid modules containing goals that are too difficult or for which it already has sufficient skill, as well as to return to skills it is forgetting \citep[pp.~2--3]{colas2019curious}. In this way, the selection of different goals is driven by self-assessment of task performance.

The subjective \gls{lp}, for a given module, $LP(M_i)$, is defined as the derivative of \emph{subjective competence}, $C(M_i)$, with respect to time, where $C(M_i)$ is defined as the mean over results from self-evaluated trials performed by the agent (see \citealp[p.~4]{colas2019curious}). The results are stored in a queue of preset length, and each result is binary: either 1 for a successful trial or 0 for a failure. This can be viewed as the frequentist estimate of the probability of success. Finally, the probability of choosing a module is computed as:
\begin{align}\label{eq:curiouslpprob}
p(M_i) := \epsilon \times \frac{1}{N} + (1-\epsilon) \times \frac{|LP(M_{i})|}{\sum_{j=1}^N |LP(M_{j})|},
\end{align}
in which $\epsilon \in [0,1]$ is a hyperparameter that can be used to ensure that even poorly performing modules are occasionally selected, and $N$ corresponds to the total number of modules. As $\epsilon$ approaches 1, the probability distribution over modules resembles a uniform distribution. When $\epsilon$ is smaller than 1, the modules with the highest absolute \gls{lp} will have the highest probability of being selected. %

In the same vein, an emerging \gls{acl} paradigm, \gls{ued} (for a review see \citealp{rutherford2024no}), formalises the problem of automatically generating tasks for agents based on some criterion, for instance, the difference between the current agent and (in practice, an approximation of) an optimal agent \citep[p.~1]{rutherford2024no}. In \gls{ued}, environments can be thought of as goals for an agent to pursue (each associated with their own reward function). As part of one such system, \gls{omni-epic}, agents self-generate novel, interesting, and solvable goals, matched for an agent's current capabilities \citep[p.~2]{faldor2024omni-epic}.

While a significant proportion of \gls{acl} methods optimise task difficulty or competence progress with respect to current capabilities, \gls{acl} can also motivate agents to optimise goal selection based on other criteria that can be interpreted as proxies for ``skill level'' (from \hyperref[sec:c3b]{C3.b}). \citet{pong2020skew-fit} extend \gls{rig} (see \hyperref[sec:c3a]{C3.a}) by weighting the selection of previously visited states used in learning the goal-generation model, encouraging the agent to generate goals resembling less frequently visited states. This weighting can be seen as a heuristic for attending to tasks that are expected to be non-trivial for the agent. As a second example, \citet{lintunen2024diversity} introduce a method motivating agents to select goals based on how much the agent believes pursuing them will diversify its entire set of skills. As \gls{acl} systems often consider the level of skill needed to perform well on the task (where the \emph{task} could defined as learning a diverse set of skills), autocurricula methods offer a good starting point for formalising \hyperref[sec:c3b]{C3.b}.


\subsection{Capacity growth (C4)}
\label{sec:capacitygrowth}

Competence, understood as \emph{capacity growth}, can be interpreted in two different ways: observing gain in the strength (C4.a) and/or range (C4.b) of one’s skills. While we saw that a reward reflecting a derivative of some measure of performance (e.g., \gls{lp}) can direct the agent towards goals requiring a certain level of skill (\hyperref[sec:c3b]{C3.b}), the same derivative directly encourages capacity growth or improvement (C4.a). While these two facets appear to have different meanings, from a computational perspective, they can be closely aligned.


An example of a system that explores both C4.a and C4.b---both repeatedly engaging in skills and extending the agent's skill repertoire---is \gls{imrl} described by \citet{barto2004intrinsically} and \citet{singh2004intrinsically}. \Gls{imrl} extends the agent's repertoire of skills (C4.b): each time a ``salient'' event is observed, the agent adds a new skill associated with the salient event.\footnote{\citet{barto2004intrinsically} leave to future work what salient means and how to determine whether an event is salient or not. They simply hard-code events like turning on a light or ringing a bell as ``salient.''} After this first observation, the agent learns how to return to observe the salient event again from any other situation via experience over time---this is the new skill associated with the salient event. Strictly speaking, the skill repertoire expands as a matter of course, not because the agent is intrinsically rewarded for expanding it.

The intrinsic reward used in the \gls{imrl} system \citep[p.~4]{singh2004intrinsically} can be defined as:
\begin{align}
    R_t(\rvs_t,\rvs_{t+1},\rvg_{\rvs_{t+1}}) :\propto \left\{ \begin{array}{ll}
        1 - P^{\rvg_{\rvs_{t+1}}}(\rvs_{t+1} \mid \rvs_t ) & \text{if }\rvs_{t+1}\text{ is salient}\\
        0 & \text{otherwise}
        \end{array}\right.
\end{align}
where $P^{\rvg_{\rvs_{t+1}}}$ is what \citet{precup1997multi} call a \emph{multi-time model} for skill $\rvg_{\rvs_{t+1}}$. $P^{\rvg_{\rvs_{t+1}}}$ is not \textit{quite} a probability estimate for the transition from $\rvs_t$ to $\rvs_{t+1}$ given the agent is following skill $\rvg_{\rvs_{t+1}}$, but that is close to the correct intuition. %

The agent only receives a non-zero intrinsic reward when it reaches a salient state. The intrinsic reward here can be thought of as modelling how surprising it is that the agent reached $\rvs_{t+1}$ immediately from state $\rvs_t$---which it has just done---by following the skill $\rvg_{\rvs_{t+1}}$. If the agent expected, without uncertainty, to transition directly from $\rvs_t$ to $\rvs_{t+1}$, then $P^{\rvg_{\rvs_{t+1}}}$ is 1 and the reward is 0 (the transition is very unsurprising). If the agent did not expect it could transition from $\rvs_t$ to $\rvs_{t+1}$ by following skill $\rvg_{\rvs_{t+1}}$, then $P^{\rvg_{\rvs_{t+1}}}$ is 0 and the reward is 1 (maximally surprising). If the agent is uncertain, $P^{\rvg_{\rvs_{t+1}}}$ will be somewhere in between. However, if the agent expected that it could reach $\rvs_{t+1}$ from $\rvs_t$ by following skill $\rvg_{\rvs_{t+1}}$, just that it would take more than a single step, $P^{\rvg_{\rvs_{t+1}}}$ will be discounted based on the length of the path; this discounting is part of how the multi-time model is constructed.

Another example of C4.b, that is, being motivated by growing the number of skills in the agent's repertoire, is the aforementioned \gls{vic} (see \hyperref[sec:c2a]{C2.a}). The \gls{vic} reward function motivates the agent to grow its number of effective skills: in expectation, the second term of the reward function (Eq.~\ref{eq:vicreward}, $\beta$) increases as the distribution over skills becomes more uniform. In theory, this encourages the distribution over skills not to collapse to a small number of skills. In practice, since there are numerous other factors that affect how agents learn, this does not always happen (for empirical results see \citealp[Appendix E, pp.~17--18]{eysenbach2019diversity}).


\section{Advancing the Theory of Competence in SDT}

Why should human motivation researchers care about the computational models we have identified? By identifying distinct ways the different facets of the need for competence in \gls{sdt} (\autoref{table:facets}) can be formalised, we better understand the ways the theory is underspecified. For instance, \gls{lp}-based methods (discussed in \hyperref[sec:skilluse]{C2}, \hyperref[sec:taskperformance]{C3}, \hyperref[sec:capacitygrowth]{C4}) can exemplify skill use as an opportunity to use a particular skill (\hyperref[sec:c2a]{C2.a}), task performance that requires a certain skill or skill level (\hyperref[sec:c3b]{C3.b}), and capacity growth in the sense of observing gains in the strength of one's skills (\hyperref[sec:capacitygrowth]{C4.a}). Given that \gls{sdt} does not acknowledge distinct facets, finding such a formalism that fits multiple facets supports the theory. On the other hand, some facets are formally distinct. For example, it is unclear how effectance (\hyperref[sec:effectance]{C1}), being motivated by extending one's repertoire of skills (\hyperref[sec:capacitygrowth]{C4.b}), or being motivated by observing that one performs well at an intended task (\hyperref[sec:c3a]{C3.a}) relate to each other and other facets. Different interpretations of the theory might lead to drastically different operationalisations of the need for competence. Our research can alleviate this problem by uncovering ways the theory conflates formally distinct concepts, thus prompting review of the theory.

Another reason human motivation researchers should care about this work is that the computational formalisms reveal underlying preconditions that \gls{sdt} fails to make explicit, such as that a competence-motivated agent is able to recognise when actions cause effects (\hyperref[sec:effectance]{C1}) and differentiate between distinct skills (\hyperref[sec:skilluse]{C2}, \hyperref[sec:taskperformance]{C3}, \hyperref[sec:capacitygrowth]{C4}). Similarly, concepts from intrinsically motivated \gls{rl}, such as novelty, diversity, and progress towards goals, underlie competence-driven learning in computational contexts, yet are often overlooked in \gls{sdt}. Our research shines light on mechanisms used to computationally fulfil such preconditions, and these could be investigated for further insights. Additionally, our work can support a cycle of theory development by inspiring new computational models formalising aspects of the theory, which can then be tested empirically to refine the theory.



\section{Future Work and Conclusion}

For computational \gls{im} to be effective in informing theory on motivation, more empirical work is needed in the comparison and evaluation of computational \gls{im}. While some work exists comparing behaviour induced by different intrinsic reward functions (e.g., \citealp{santucci2012intrinsic,biehl2018expanding,linke2020adapting}), as does work studying variables such as the choice of representation (e.g., \citealp{burda2019large}), more research is needed on the properties of different models, such as how they behave in varying environments and bodies. Showing that \gls{rl} formalisms can be aligned with \gls{sdt} uncovers a wealth of resources for motivation research, such as existing implementations of models, simulators, and evaluation metrics. These resources can enable large-scale simulation studies to explore psychologically plausible computational \glspl{im} and how they may explain human data, thus contributing to formal competence-related theory. However, we need work to decide how to effectively compare artificial and biological agents (a project partially undertaken by researchers in \emph{representational alignment}; e.g., \citealp{sucholutsky2024getting}).

Further research is also required to understand the extent to which computational \gls{im} research intersects with intrinsically motivated learning as understood in psychology. Early work on computational \gls{im} emerged at the intersection of developmental psychology and robotics, attempting to model intrinsically motivated human behaviours, but a large portion of state-of-the-art intrinsically motivated \gls{rl} is not necessarily aligned with relevant goals of understanding the human mind or developing safe (human-aligned) \gls{ai} systems.

As most formalisms we identified relate to the pursuit of goals, we need to better understand how humans represent and generate goals \citep[p.~1]{colas2024what}. The topic has not been well studied for various reasons, including computational intractability \citep[p.~1]{byers2024modeling}. Excitingly, goals have recently emerged as a multidisciplinary area of research, cutting across \gls{ai}, cognitive science, and developmental psychology (e.g., \citealp{colas2022autotelic,molinaro2023a,chu2024in,davidson2024goals}). Recent work by \citet[p.~1150]{molinaro2023a} suggests that goals affect many agent dynamics, such as representing the environment, choosing relevant actions, and evaluating rewards.

In summary, we build on recent research arguing how the need for competence is at present poorly specified in \gls{sdt}. We do so by identifying computational formalisms to inspire empirical work to resolve this issue. We show that the four distinct facets of competence---effectance, skill use, task performance, and capacity growth---can be aligned with existing formalisms from intrinsically motivated \gls{rl}, highlighting the potential of computational models to provide deeper insights into competence-motivated behaviours. While this is a promising first step, further research is required to empirically study these computational models, inviting collaboration from motivation researchers across disciplines.



\section{Acknowledgments}

We thank the members of the Intrinsically Motivation Open-ended Learning (IMOL) community for engaging in the work and for providing feedback in Paris (2023) and Vancouver (2024). We also thank the members of the Autotelic Interaction Research (AIR) group for feedback throughout the project. EML and CG received financial support from the Research Council of Finland (NEXT-IM, grant 349036) and NMA from the Helsinki Institute for Information Technology.


\bibliographystyle{apalike}
\setlength{\bibleftmargin}{.125in}
\setlength{\bibindent}{-\bibleftmargin}
\bibliography{refs}

\end{document}
