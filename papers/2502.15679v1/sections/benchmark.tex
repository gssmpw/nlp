\section{The \bm Benchmark}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=1.0\textwidth]{imgs/bms.png} 
    \caption{This figure illustrates the three challenges of \bm where \pb occurs, with progressively increasing difficulty.\gb{Captions needs to be significantly expanded. I also feel like this figure does not look good. It has a lot of text and basic figures. Can you illustrate each challenge using concrete visual examples and fewer text? It would be more intuitive and look a lot better than the current figure. I also feel like the current figure has a lot going on in terms of arrows. You need to simplify it. For instance, I looked at it for ~30s and I still have no idea what some of these illustrations mean. Make it simpler and reduce the amount of text, arrows, and other potentially uncessary/overwhelming details}}
    \label{fig:bm_arch}
\end{figure*}




We introduce the \bm benchmark, designed to facilitate a comprehensive empirical study of the \pb problem across modern IL methods. To study this issue, we construct the environment (Section~\ref{subsec:bm_env}) using the Libero simulation platform~\cite{liu2024libero}. \bm includes three challenges (Section~\ref{subsec:bm_1}~\ref{subsec:bm_2}) designed not only to validate the existence of \pb but also to demonstrate its significant negative impact on the success of long-horizon tasks\gb{You also don't list any numbers overviewing your benchmark, which might be useful? How many tasks? How many trajectories, etc etc? From this overview, I don't have almost any idea what this benchmark looks like, what the scale of it is, etc.}. We also introduce four IL algorithms as baselines for evaluation on \bm (Section~\ref{subsec:basic_bl}). Additionally, we present a potential solution to \pb and the large-scale, diverse dataset generated as part of its preparation (Section~\ref{subsec:our_bl}).\gb{In my view, the solution shouldn't be a part of the benchmark section. It's not really related to the benchmark itself. Would it make sense to put it as another section?}


% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.4\textwidth, height=0.2\textheight]{example-image} 
%     \caption{example for picked tasks from Libero-100}
%     \label{fig:picked_tasks}
% \end{figure}

\subsection{Environment}
\label{subsec:bm_env}
% Introduce:
% 1. The based Libero Environment: basic + task customization (intro bddl)
% 2. Obs space and action space, control freq ...

We build \bm on the Libero platform~\cite{liu2024libero}, which, although originally designed for lifelong robot learning tasks, provides diverse manipulation scenes featuring a Franka Emika Panda robot arm, a wide range of tasks, and high flexibility for creating and customizing tasks. These features make Libero an excellent foundation for developing \bm, enabling us to adapt it specifically to study the \pb problem.

From Libero-100, the most comprehensive and diverse task suite in Libero, we select all 44 skill-level tasks spanning 12 diverse manipulation scenes, encompassing a wide range of object interactions and motor skills, which we denote as \taskoriginal. To form our base task set, we exclude multi-skill tasks (e.g., ``open the top drawer of the cabinet and put the bowl in it'') and include only single-skill tasks (e.g., ``open the bottom drawer of the cabinet'').\gb{Why? It seems that those would be even better suited to study the OSS problem} All three challenges are constructed from this refined set.\gb{Do we need a figure to highlight the main properties of the benchmark? Since this is one of our main contributions, I think it would be great to showcase that as a figure, especially if we want to emphasize particular things about the benchmark, i.e., a large number of tasks, etc?}
% , with selected examples shown in Fig.~\ref{fig:picked_tasks}.

For consistency, we define the observation space as a combination of third-person camera images, 7-DoF robot arm joint angles, and 2-DoF parallel gripper joint states. The action space is defined as a 7-dimensional relative Cartesian displacement (w.r.t. the gripper frame), and the control frequency is set to 20 Hz.\gb{Does this need to fo in the environment section? Seems like it breaks the flow of the benchmark description. This to me seems more fitting to some implementations details section}


\yy{For all challenges, May need to improve many statements to be more concise and accurate using math annotations when problem definition is improved.}

% \begin{table}[h!]
% \centering
% \caption{Language Examples Description of benchmark tasks in BM1, BM2, and BM3. \textcolor{red}{might delete later if we wanna add BMs language descriptions in Fig.2.}}
% \label{tb:bm_examples}
% \begin{tabular}{llll}
% \hline
%       & BM1 & BM2 & BM3 \\ \hline
% Exm-1 &     &     &     \\ \hline
% Exm-2 &     &     &     \\ \hline
% Exm-3 &     &     &     \\ \hline
% Exm-4 &     &     &     \\ \hline
% Exm-5 &     &     &     \\ \hline
% \end{tabular}
% \end{table}


\subsection{Challenge 1: Single-Step Modification\gb{Maybe Single-Step State Modification? From this name it's not clear what you are actually modifying}}
\label{subsec:bm_1}
% Intro - osm problem is incurred by transition between skills, so we for sure need to study how much effect the single-step modification can bring on the next skill.
% Briefly introduce bddl file - How I use it to create modification?
% metric - performance drop (PD).

% As discussed in Section~\ref{sec:problem_definition}, \pb occurs when skill-irrelevant environmental elements are altered in a manner that do not impact the dynamic preconditions of the next skill (i.e., the next skill is still physically capable of being completed), but do violate the visual preconditions. 

\gb{The section structure of this seems disjoint. You go from environment to Challenge 1 without any explanation or overview. Maybe instead you could have a separate section dedicated to all the challenges and then just list each challenge as a subsection? Also, before introducing each challenge maybe you could provide a brief overview of what each challenge entails? Right now the transition between the previous section and this section seems very abrupt and not very intuitive}

As discussed in Section~\ref{sec:problem_definition}, \pb occurs when skill-irrelevant predicates are altered in a manner that do not impact the completeness of current skill, but do disrupt the visuomotor skill policy execution.\gb{The writing of this sentence is somewhat cumbersome and difficult to follow. Maybe you could rewrite it to be more clear. Also, I'd encourage you to include concrete examples to illustrate this so that no reader would have any doubts about what you are trying to say with your verbal descriptions. Also, as I mentioned, it would be great if your figure illustrating three challenges included some visual examples. Then you could also reference to that figure in this text when reminding what the first challenge is} In the context of skill chaining, a foundational structure in long-horizon tasks, the preceding skill is the most likely source of \pb for the subsequent skill\gb{I don't fully understand what it means for the skill to be a source of OSS. I feel like the writing is unclear and ambiguous}. Therefore, we introduce a challenge, \bma, specifically designed to evaluate the impact of \pb caused by single-step transitions on the performance of baseline methods (see examples in Figure~\ref{fig:bm_arch}).

To investigate the impact of single-step \pb on baseline methods, we compare their performance on the next skill without \pb and with \pb\gb{Again this could be made more clear with some examples}. For the former, we evaluate baselines on tasks from \taskoriginal. For the latter, we modify tasks from \taskoriginal to simulate \pb caused by the preceding skill.\gb{I feel like your descriptions of this is confusing, at least to me. I don't know why we have to mention BOSS-44 at all. You can just say that we evaluate each method on each of the 44 tasks in a scenario where OSS exists and in a scenario where it doesn't. It's easy to understand that. I'm also generally confused why you even need this BOSS-44? It adds another name to keep track of and I personally don't understand how is this different than BOSS? Can we just have one name BOSS without creating these additional names, which there are already a lot of (e.g., three challenges, etc.) so that it would be easier for the reader to keep track of all of these details} Libero's customization flexibility plays a crucial role here, as its task environments are generated using Planning Domain Definition Language (PDDL) files\notezlf{you may mention PDDL when introducing task planning in formulation? you can mention it defines e.g., operators, and what you need to modify here}. This allows us to easily add new objects (e.g., placing a potato in a bowl or on a tabletop) or modify object states (e.g., closing an opened drawer). These modifications enable us to simulate single-step \pb efficiently and evaluate baseline performance on the altered tasks.

To quantify the impact of \pb, we introduce the metric ``\textbf{Performance Drop (PD)},'' defined as the difference between the success rate of a baseline on the original skill-level task and its success rate on the task with single-step \pb\notezlf{I don't think it's necessary to create these kind of abbreviations in general, people may get confused and this term not used much in your paper. If you really need, maybe explain in table title. Also, a better name may be performance delta, as it may be positive or negative}.\gb{I feel like this should go into the experimental section, next to the definition of evaluation metrics, not here}



\subsection{Challenge 2: Accumulated Modification\gb{Again, i feel like the challenge names could be more intuitive and self-explanatory}}
\label{subsec:bm_2}
Along a skill chain, \pb can occur in multiple skills\gb{can you give an example?}, with these modifications accumulating and affecting the current skill. Thus, it is valuable to study the differences between accumulated \pb and single-step \pb. To address this, we introduce another challenge, \bmb, specifically designed to analyze the impact of \pb accumulation across preceding skills (check examples in Figure~\ref{fig:bm_arch}). \gb{from this paragraph, to me it's not clear at all what this accumulation is referring to}

Similar to \bma, we evaluate baseline performance on the next skill under two conditions: without \pb and with accumulated \pb. Using PDDL description files, we create two sets of modified tasks: one with two modifications and another with three modifications\gb{why is such setup used? Seems like these are arbitrary choices}. However, multiple modifications can sometimes conflict. For example, a modification such as ``place an apple inside the small bowl'' could conflict with another that specifies ``place a potato inside the small bowl,'' potentially causing the Libero environment to break due to overlapping space constraints. To address this, we develop a \textbf{Rule-based Automatic Modification Generator (RAMG)} to ensure compatibility and scalability when generating modifications for a single task. We provide details of RAMG in Section~\ref{subsec:our_bl}. We use the same metric \textbf{PD} as \bma to measure the impact of accumulated \pb.\gb{It would be great to provide a detailed list of modifications that your system can implement. Similarly, for completenes/reporducibility, it would be great to provide a list of all the tasks included in your dataset along with the numbers for each task. This would emphasize the benchmark contribution and also make the paper more reproducible. Right now many details about your benchmark/dataset are missing.}




\subsection{Challenge 3: Real Long-Horizon Task}
\label{subsec:bm_3}
The ultimate goal of addressing the \pb problem is to improve the success of long-horizon robot tasks. To this end, we introduce a challenge, \bmc, consisting of 10 long-horizon tasks\gb{again, it would be nice to list the tasks somewhere for completeness/reproducibility}, each comprising a chain of three skills (check examples in Figure~\ref{fig:bm_arch}). \bmc serves as a straightforward way to demonstrate the impact of the \pb problem on long-horizon task performance.

To construct \bmc, we manually select and combine skills from \taskoriginal \gb{I'm confused what's the difference between BOSS and BOSS-44}, ensuring that \pb occurs in each skill while avoiding conflicts between modifications. We reset the robot to a neutral position after each skill to eliminate dynamic transition feasibility issues~\cite{chen2023sequential}, ensuring the challenge focuses solely on the impact of \pb\gb{is that a realistic thing to do in real-world settings? Sounds contrived/artificial to me}. For \bmc, the sequential execution of multiple skill policies requires accounting for all their original performance, making it unsuitable to rely solely on PD\gb{What is PD? Use the full phrase so that the reader wouldn't have to find for acronym definition. I also don't understand what this sentence means}. Instead, we introduce a slightly different metric to evaluate the impact of accumulated \pb: \textbf{Delta to Upper Bound (DUB)}. DUB measures the difference between the success rate of completing the entire long-horizon task (i.e., achieving success for all skills in the chain) and an upper bound (UB). We compute the UB as the product of the success rates for each skill in the chain when evaluated without \pb, representing the maximum success rate achievable if \pb had no effect on long-horizon tasks. \gb{Again, I feel like the evaluation metric definitions should go under the experimental setup, not the benchmark description}




\subsection{Baselines}
\label{subsec:basic_bl}

\gb{To me this should also go under the experimental setup section along with the evaluation metrics}
% Overview
We select four widely used imitation learning algorithms, representing diverse architectures and design approaches for baseline comparisons, to learn each skill and evaluate the impact of \pb on their performance in long-horizon tasks.  Among these, three are Behavioral Cloning approaches, while one is a vision-language-action model.


% BC-RNN
% BC-Transformer
% BC-Vilt
\textbf{Behavioral Cloning (BC)}: A set of three BC algorithms~\cite{torabi2018behavioral} from Libero is adopted: BC-RESNET-RNN~\cite{mandlekar2021matters}, BC-RESNET-T~\cite{zhu2023viola}, and BC-VIT-T~\cite{kim2021vilt}. These algorithms feature diverse neural network architectures for visual and language encoding. All three use BERT embeddings~\cite{devlin2018bert} to encode the language instructions for each skill. In BC-RESNET-RNN, ResNet~\cite{he2016identity} serves as the visual backbone for encoding per-step visual observations, while an LSTM processes the sequence of encoded visual embeddings as the temporal backbone. BC-RESNET-T employs the same visual encoder, ResNet, but replaces the LSTM with a transformer decoder~\cite{vaswani2017attention} for temporal processing. BC-VIT-T uses Vision Transformer (ViT)~\cite{dosovitskiy2020image} as the visual backbone and a transformer decoder as the temporal backbone. All these BC algorithms output a multi-modal distribution over manipulation actions using a Gaussian Mixture Model (GMM) output head~\cite{bishop1994mixture}, from which an action is sampled.


% % Diffusion Policy
% \textbf{Diffusion Policy} --- Generative models have achieved remarkable success in language and vision domains~\cite{openai2024chatgpt, ramesh2022hierarchical, yang2024annotated}, making their application to robot learning a natural progression. Unlike Generative Adversarial Imitation Learning (GAIL)-like methods~\cite{ho2016generative}, which use Generative Adversarial Networks (GANs) as their backbone, diffusion policy~\cite{chi2023diffusion} achieves superior performance in robot learning due to its diffusion model backbone, offering multi-modal action distributions, handling high-dimensional outputs, and ensuring stable training. As a result, we select diffusion policy as the representative generative IL method to evaluate the impact of \pb problem.

% OpenVLA
\textbf{OpenVLA}: \yy{need to improve to include better motivation - need to relate to some motivations in the Introduction section?} Applying large foundation models, such as Large Language Models (LLMs)~\cite{openai2024chatgpt} and Vision-Language Models (VLMs)~\cite{dubey2024llama}, to robot learning has become a growing trend, culminating in the development of vision-language-action (VLA) models. These models leverage strong representation capabilities, task generalization, contextual understanding, and common sense reasoning to improve decision-making in robotics. Among these, OpenVLA~\cite{kim2024openvla}, an open-sourced VLA model, outperforms other state-of-the-art methods, making it our choice for evaluating the \pb problem as a representative generalist policy. \gb{I don't feel like you need such lengthy description for this. You can just say that given popularity of this method in recent years, you also adopt it to your evaluations as well.}


\subsection{Additional Baseline - Data Augmentation}
\label{subsec:our_bl}
% We propose a simple baseline, \bl, which additionally provides a corresponding dataset as a valuable byproduct. As defined in Section~\ref{sec:problem_definition}, the \pb problem arises when visual changes occur due to previous skills. A straightforward solution is to collect a larger set of demonstrations, allowing algorithms to learn from a diverse observation space and become robust to novel visual modifications. To explore the effectiveness of this potential solution to \pb, we augments demonstrations and train all the aforementioned baselines based on it to provide guidance for future research on the \pb problem. 
\gb{Subsection title is not very informative and just generally sounds weird. It also doesn't fit well with the benchmark section. Lastly, I think it hurts your paper to first introduce the benchmark and then suggest/imply that some simple approach can provide a solution to your benchmark. It defeats the purpose of your benchmark. I would formulate this as some additional baselines rather than the solution}

As defined in Section~\ref{sec:problem_definition}, the \pb problem arises when visual changes caused by previous skills disrupt current skill execution. A straightforward solution is to collect a larger and more diverse set of demonstrations, enabling algorithms to learn from varied observation spaces and become more robust to novel visual modifications. To evaluate the effectiveness of this approach, we augment the demonstrations and train all the aforementioned baselines on an expanded dataset.\gb{You can just call this Baselines with Data Augmentation or something like that. This is essentially just data augmentation. Not sure why we need to use vague terminology such as "potential solution" to a technique that's already widely used in other fields} %, offering valuable insights for future research on addressing the \pb problem. 

For each skill-level task, we scalably generate modified tasks with \pb problem using a \textbf{Rule-based Automatic Modification Generator (RAMG)}.\gb{To me the flow of this is not great, i.e., that you start talking about this 2 sections ago, don't conclude it, then have a separate completely unrelated subsection, and then go back to this again.} RAMG is a systematic algorithm designed to enhance diversity in pre-defined environments through structured, rule-based modifications. Operating on PDDL files, RAMG iteratively alters object positions, introduces new objects, or modifies object states within the environment. It supports three types of modifications: (1) repositioning or adding external objects to specific regions, (2) changing the states of fixtures (e.g., opening or closing cabinets), and (3) placing small objects into designated containers. These modifications follow dynamic constraints (e.g., newly added objects do not create obstacles) and maintain logical consistency (e.g., a drawer cannot be simultaneously open and closed) to ensure they do not hinder the agent from achieving the task goal. By utilizing deterministic yet flexible rules, RAMG offers a scalable approach to generating task variations, creating up to 1,727 modified tasks from the 44 tasks in \taskoriginal.

For each modified task generated by RAMG, we replay the demonstrations in the corresponding environment, creating new demonstrations where the trajectory remains the same but \pb is introduced. This approach produces a substantially larger demonstration dataset for each skill-level task. We then pre-train or fine-tune all the baselines using this augmented demonstration set to evaluate their performance when facing new \pb problem.\gb{What does it mean new OSS problem?}

