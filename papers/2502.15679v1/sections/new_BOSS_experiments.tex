\section{The \bm Experimental Results}
\label{sec:bm_results}

\subsection{Experimental Setup}



\subsubsection{Baselines}
\label{subsec:basic_bl}

% Overview
We select four widely used imitation learning algorithms, representing diverse architectures and design approaches for baseline comparisons, to learn each skill and evaluate the impact of \pb on their performance in long-horizon tasks. Among these, three are Behavioral Cloning approaches from Libero, while one is a vision-language-action model.


% BC-RNN
% BC-Transformer
% BC-Vilt
\textbf{Behavioral Cloning (BC) in Libero}: A set of three BC algorithms~\cite{torabi2018behavioral} from Libero is adopted: BC-RESNET-RNN~\cite{mandlekar2021matters}, BC-RESNET-T~\cite{zhu2023viola}, and BC-VIT-T~\cite{kim2021vilt}. These algorithms feature diverse neural network architectures for visual and language encoding. All three use BERT embeddings~\cite{devlin2018bert} to encode the language instructions for each skill. In BC-RESNET-RNN, ResNet~\cite{he2016identity} serves as the visual backbone for encoding per-step visual observations, while an LSTM processes the sequence of encoded visual embeddings as the temporal backbone. BC-RESNET-T employs the same visual encoder, ResNet, but replaces the LSTM with a transformer decoder~\cite{vaswani2017attention} for temporal processing. BC-VIT-T uses Vision Transformer (ViT)~\cite{dosovitskiy2020image} as the visual backbone and a transformer decoder as the temporal backbone. All these BC algorithms output a multi-modal distribution over manipulation actions using a Gaussian Mixture Model (GMM) output head~\cite{bishop1994mixture}, from which an action is sampled.


% % Diffusion Policy
% \textbf{Diffusion Policy} --- Generative models have achieved remarkable success in language and vision domains~\cite{openai2024chatgpt, ramesh2022hierarchical, yang2024annotated}, making their application to robot learning a natural progression. Unlike Generative Adversarial Imitation Learning (GAIL)-like methods~\cite{ho2016generative}, which use Generative Adversarial Networks (GANs) as their backbone, diffusion policy~\cite{chi2023diffusion} achieves superior performance in robot learning due to its diffusion model backbone, offering multi-modal action distributions, handling high-dimensional outputs, and ensuring stable training. As a result, we select diffusion policy as the representative generative IL method to evaluate the impact of \pb problem.

% OpenVLA
\textbf{OpenVLA}: Applying large foundation models, such as Large Language Models (LLMs)~\cite{openai2024chatgpt} and Vision-Language Models (VLMs)~\cite{dubey2024llama}, to robotics has gained popularity, leading to vision-language-action (VLA) models. Among these, OpenVLA~\cite{kim2024openvla}, an open-sourced VLA model, outperforms other state-of-the-art methods, making it a natural choice for evaluating the \pb problem as a representative generalist policy. The language description, conditioned on each task and sourced from Libero, remains consistent, whether or not the task is affected by \pb.


\subsubsection{Metrics}
We mainly use two metrics in all the experiments: 

\textbf{Ratio Performance Delta (RPD)}: We define the metric ``Ratio Performance Delta'' as the relative change in skill success rate caused by \pb. It is calculated as the difference between a baseline's success rate on the original skill-level task and its success rate on the modified task where \pb occurs, normalized by the original success rate. A positive RPD indicates that \pb negatively impacts the current skill, while an RPD less than or equal to zero suggests no negative effect. Instances of negative RPD can occur due to the inherent randomness of IL models.

\textbf{Delta to Upper Bound Ratio (DUBR)}: Unlike \bma and \bmb, which evaluate the performance delta for individual skills, \bmc assesses the performance delta across an entire skill chain. To support this, we introduce a new metric. First, we define the ``Chain Upper Bound'' as the product of success rates for each skill in the chain when evaluated without \pb occurrence, representing the maximum achievable success rate in the absence of \pb. Then, we introduce the ``Delta to Upper Bound Ratio'', calculated as the difference between the actual success rate of completing the entire skill chain and the ``Chain Upper Bound'', divided by the ``Chain Upper Bound'', providing a normalized measure of the performance delta for the entire chain.


\subsubsection{Implementation Details}
For observation space design, we align with the baselines' default setups. OpenVLA uses only a third-person camera view as its observation space. To maintain consistency in visual observations, we define the observation space for BCs as a combination of third-person camera images, 7-DoF robot arm joint angles, and 2-DoF parallel gripper joint states, excluding only the wrist-camera view. For all the baselines, the action space is defined as a 7-dimensional relative Cartesian displacement (w.r.t. the gripper frame), and the control frequency is set to 20 Hz. For all the experiments, we use three random seeds and report only the averaged results across these runs.











\subsection{Results for \bma}
\label{subsec:results_bma}

% \input{tables/bm1_table}
\begin{figure*}[ht]
    \centering
    \includegraphics[width=1.0\textwidth]{imgs/ch1_results.png} 
    \caption{This figure presents the results for \bma. In each baseline subfigure, the majority of points lie below the diagonal line, representing tasks with a positive Ratio Performance Delta, indicating that single predicate modification negatively affects tasks performance.}
    \label{fig:ch1_results}
    \vspace{-1.5em}
\end{figure*}


We evaluate baseline methods, including BC-RESNET-RNN, BC-RESNET-T, BC-VIT-T, and OpenVLA, on the 44 selected tasks and their 44 modified counterparts, calculating the Ratio Performance Delta for each baseline. The results are presented in Figure~\ref{fig:ch1_results}, where the x-axis represents the success rate on tasks unaffected by \pb, and the y-axis represents the success rate on their modified counterparts. Each point corresponds to a task pair (unaffected and modified), with darker colors indicating higher Ratio Performance Delta values. A diagonal line separates tasks: points on or above the line have non-positive Ratio Performance Delta, indicating no negative impact from \pb, while points below the line signify tasks negatively affected by \pb. As shown in Figure~\ref{fig:ch1_results}, the percentage of tasks negatively affected by \pb (points below the diagonal) is 68\%, 66\%, 50\%, and 66\% for BC-RESNET-RNN, BC-RESNET-T, BC-VIT-T, and OpenVLA, respectively. Among these tasks, the average Ratio Performance Delta is 67\%, 35\%, 34\%, and 54\% for the respective baselines, highlighting the substantial performance degradation caused by \pb. These results demonstrate that \pb frequently occurs even in single-step transitions, emphasizing its potential to jeopardize the success of long-horizon tasks.

% As defined in Section~\ref{subsec:bm_1}, we use Performance Drop (PD) as the metric, calculated as the difference between the success rate on original tasks (SR-ORI) and the success rate on modified tasks (SR-MOD).\gb{Instead of repeating explanations about the evaluation metrics, just move them to the experiments section and create a separate paragraph/subsection for them} Table~\ref{tb:bm1_results} summarizes the average results across three random seeds, showing that \pb occurs in most tasks across all baselines. The ratios of positive PD (i.e., \pb occurrence) for BC-RESNET-RNN, BC-RESNET-T, BC-VIT-T, and OpenVLA are 68\%, 66\%, 50\%, and 66\%, respectively\gb{What is positive PD? It's very counterintuitive to put two words meaning opposite things, i.e., positive drop? What does that mean? I also can't relate to what you are saying to the things in the table as it's very overwhelming}. For tasks negatively affected by \pb, the average PD ratios are 29\%, 30\%, 28\%, and 44\%\gb{What are these ratios?}, respectively, underscoring the significant performance impact. These findings demonstrate that \pb frequently occurs even in single-step transitions, providing evidence that \pb can jeopardize the successful completion of long-horizon tasks. \gb{I feel like the definitions of you are evaluation metrics and the descriptions of these results + their connection to the table is not clear. I didn't understand much in this paragraph}


\subsection{Results for \bmb}
\label{subsec:results_bmb}
\vspace{-1.2em}

% \input{tables/bm2_table}
% \begin{figure*}[ht]
%     \centering
%     \includegraphics[width=1.0\textwidth]{imgs/ch2_results_final.png} 
%     \caption{This figure shows the results for \bmb. In each baseline subfigure, most points fall below the diagonal line, representing tasks with a positive Ratio Performance Delta and highlighting the negative impact of accumulated predicate modifications on task performance. Two bar charts beneath the scatterplot summarize (1) the average ratio of \pb occurrence across sets with varying numbers of modifications and (2) the average positive Ratio Performance Delta for sets with different numbers of modifications.\gb{I feel like there are so many things going on in this figure that's it's difficult to understand the main takeaways quickly. For instance, it's difficult to compare 1,2,3 modification settings in the figure because those points are all over the place. I feel like it would maybe be useful to plot these separately. Fig. 3 was great: easy to follow and clear. This figure is quite overwhelming at least for me.}
%     \yy{1. Enlarge bar chart font; 2. Find a better way to visualize (?)}}
%     \label{fig:ch2_results}
% \end{figure*}


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{imgs/ch2_results_final_new.png} 
    \caption{This figure shows the results for \bmb. Two bar charts summarize: (top) the average positive Ratio Performance Delta for sets with different numbers of modifications, and (bottom) the average ratio of \pb occurrence across sets with varying numbers of modifications. The upward trend of the red lines in both bar charts indicates that the accumulation of \pb progressively exacerbates its negative impact on long-horizon task completion, both in magnitude and frequency.}
    \vspace{-0.5em}
    \label{fig:ch2_results}
\end{figure}



% As detailed in Section~\ref{subsec:bm_2}, we use the proposed RAMG to generate two sets of modified tasks: one with two modifications and another with three, simulating scenarios of accumulated modifications. Similar to \bma, we evaluate baselines on the 44 unaffected tasks and their corresponding modified counterparts. Figure~\ref{fig:ch2_results} presents results for \bmb evaluations alongside \bma outcomes (i.e., single modification), providing totally three sets of results for comparison. In the scatterplot, similar to Figure~\ref{fig:ch1_results}, circles represent single-step \pb, while triangles and crosses correspond to accumulated \pb from two and three modifications, respectively. Most points fall below the diagonal line (i.e., Ratio Performance Delta is positive), reaffirming that \pb negatively impacts the current skill. Two bar charts below the scatterplot summarize: (1) the average positive Ratio Performance Delta for the three sets, and (2) the average ratio of \pb occurrence (i.e., Ratio Performance Delta is positive) across the three sets. In (1), tasks with multiple modifications display larger or comparable Ratio Performance Delta values than those with a single modification. In (2), the probability of \pb occurrence increases as the number of modifications grows.\gb{At least for me it's difficult to follow the analysis in the last two sentences. I wasn't sure what it means} Notably, across all baselines, the average Ratio Performance Delta for tasks with multiple modifications exceeds 50\%, indicating a large performance decline that is likely to result in the failure of long-horizon tasks. Since skills' effects accumulation is common in long-horizon tasks, with the number of modifications increasing along the skill chain, these results emphasize that \pb poses a significant challenge for achieving success in long-horizon tasks.


As detailed in Section~\ref{subsec:bm_2}, we use the proposed RAMG to generate two sets of modified tasks: one with two modifications and another with three, simulating scenarios of accumulated modifications. Similar to \bma, we evaluate baselines on the 44 unaffected tasks and their corresponding modified counterparts. Figure~\ref{fig:ch2_results} presents the evaluation results for \bmb alongside \bma (i.e., single modification), yielding three sets of results for comparison. Two bar charts summarize key findings: (top) the average positive Ratio Performance Delta across the three sets and (bottom) the average occurrence ratio of \pb (i.e., cases where Ratio Performance Delta is positive). Each bar chart consists of three grouped bars, where each group corresponds to sets with one, two, or three modifications, and within each group, bars represent different baselines. Additionally, a red line denotes the average performance across all baselines for each set. In both charts, this red line follows an increasing trend, indicating that as the number of modifications grows, the negative impact on task performance intensifies, both in magnitude and frequency. Notably, across all baselines, the average Ratio Performance Delta for sets with multiple modifications exceeds 50\% for all baselines (blue line in the top bar chart), signifying a substantial performance decline that is likely to cause failures in long-horizon tasks. Given that skill effect accumulation is inherent in long-horizon tasks, with modifications compounding along the skill chain, these results highlight that \pb presents a significant obstacle to achieving reliable performance.




% Table~\ref{tb:bm2_results} presents average results across three random seeds alongside \bma outcomes (single-step modifications) for straightforward comparison. Due to space constraints, we report only the average success rate (Avg. SR) and average performance drop (Avg. PD). The results reveal that tasks with multiple modifications experience greater performance degradation (i.e., higher PD) compared to single-step modifications\gb{Can you provide concrete numbers to back this up?}. Furthermore, as the number of modifications increases, the negative impact becomes more pronounced, indicating that \pb accumulation significantly exacerbates task completion difficulty\gb{Again would be nice to present some numbers here}. Since \pb accumulation can occur frequently in long-horizon tasks, with the number of modifications growing along the skill chain, these findings further validate that \pb poses a significant challenge for long-horizon task completion.


\subsection{Results for \bmc}
\label{subsec:results_bmc}

% \input{tables/bm3_table}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=1.0\textwidth]{imgs/ch3_results.png} 
    \caption{This figure presents the results for \bmc, where the ``Delta to Upper Bound Ratio'' (bars) is positive and notably high in most cases, highlighting the substantial negative impact of \pb on long-horizon task completion. 
    % \my{no legend, maybe add an avg column? Show the upper bound number as a horizontal line or it's weird to have many zeros?}
    }
    \vspace{-1.75em}
    \label{fig:ch3_results}
\end{figure*}

We include \bmc to evaluate the impact of \pb on skill chaining. As described in Section~\ref{subsec:bm_3}, we test baselines on 10 manually designed tasks, each comprising a skill chain of three skills, and use the ``Delta to Upper Bound Ratio'' metric to quantify \pb's effect on performance. Figure~\ref{fig:ch3_results} presents the results, the ``Delta to Upper Bound Ratio'' is shown as bar charts, where different baselines are shown in varied colors. The ``Delta to Upper Bound Ratio'' values are positive and high in most cases. Note that some bars for BC-RESNET-RNN show 0\% values because the ``Chain Upper Bound'' for those tasks is already 0\%, indicating that BC-RESNET-RNN fails even without \pb. These results further emphasize \pb's detrimental effect on long-horizon task completion.
% \my{explain task 4/5 that different baselines perform so different}