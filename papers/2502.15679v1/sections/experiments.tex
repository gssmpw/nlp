\section{Experimental Results}
% Overview
\yy{I feel it not nice to include all results with table, suggestions for possible replacements (e.g., results figure) are needed}
In this section, we present results from experiments that seek to quantify and further understand \pb problem. We evaluate the baseline algorithms described in \S\ref{subsec:basic_bl} across the three benchmarks in \bm (Sections~\ref{subsec:results_bma}---\ref{subsec:results_bmc}) and conduct two experiments to identify the key factors contributing to the \pb problem (Section~\ref{subsec:factors}). Finally, we investigate whether data augmentation serves as an effective and straightforward solution to the \pb problem (Section~\ref{subsec:results_bl}).

\notezlf{I agree with the comments that the tables look overwhelming, maybe try to compress them / use figures, then you can move this to appendix (in extended version)}

\subsection{Results for \bma\gb{I find these weird names, i.e., Challenge 1, 2, 3. They are too generic and feel disconnected to the overall BOSS benchmark. Can we come up with something better?}}
\label{subsec:results_bma}
% 1. Use language to explain results
% 2. do some analyses

\input{tables/bm1_table}

We evaluate baseline methods, including BC-RESNET-RNN, BC-RESNET-T, BC-VIT-T, and OpenVLA, on the 44 original tasks and their 44 modified counterparts in \taskoriginal\gb{again, I'm confused about the difference between BOSS and BOSS-44}. As defined in Section~\ref{subsec:bm_1}, we use Performance Drop (PD) as the metric, calculated as the difference between the success rate on original tasks (SR-ORI) and the success rate on modified tasks (SR-MOD).\gb{Instead of repeating explanations about the evaluation metrics, just move them to the experiments section and create a separate paragraph/subsection for them} Table~\ref{tb:bm1_results} summarizes the average results across three random seeds, showing that \pb occurs in most tasks across all baselines. The ratios of positive PD\notezlf{I'd suggest to go to performance delta, then you can define positive+negative, and you don't really need to abbreviate here as it confuse people} (i.e., \pb occurrence) for BC-RESNET-RNN, BC-RESNET-T, BC-VIT-T, and OpenVLA are 68\%, 66\%, 50\%, and 66\%, respectively\gb{What is positive PD? It's very counterintuitive to put two words meaning opposite things, i.e., positive drop? What does that mean? I also can't relate to what you are saying to the things in the table as it's very overwhelming}. For tasks negatively affected by \pb, the average PD ratios are 29\%, 30\%, 28\%, and 44\%\gb{What are these ratios?}, respectively, underscoring the significant performance impact. These findings demonstrate that \pb frequently occurs even in single-step transitions, providing evidence that \pb can jeopardize the successful completion of long-horizon tasks. \gb{I feel like the definitions of you are evaluation metrics and the descriptions of these results + their connection to the table is not clear. I didn't understand much in this paragraph}



\subsection{Results for \bmb}
\label{subsec:results_bmb}

\input{tables/bm2_table}

As described in Section~\ref{subsec:bm_2}, we use the proposed RAMG to generate two sets of modified tasks: one with two modifications and another with three, simulating scenarios where \pb accumulates. Similar to \bma, we evaluate baselines on these modified tasks. Table~\ref{tb:bm2_results} presents average results across three random seeds alongside \bma outcomes (single-step modifications) for straightforward comparison. Due to space constraints, we report only the average success rate (Avg. SR) and average performance drop (Avg. PD). The results reveal that tasks with multiple modifications experience greater performance degradation (i.e., higher PD) compared to single-step modifications\gb{Can you provide concrete numbers to back this up?}. Furthermore, as the number of modifications increases, the negative impact becomes more pronounced, indicating that \pb accumulation significantly exacerbates task completion difficulty\gb{Again would be nice to present some numbers here}. Since \pb accumulation can occur frequently in long-horizon tasks, with the number of modifications growing along the skill chain, these findings further validate that \pb poses a significant challenge for long-horizon task completion.



\subsection{Results for \bmc}
\label{subsec:results_bmc}

\input{tables/bm3_table}

We include \bmc to evaluate the impact of \pb on real long-horizon tasks. As described in Section~\ref{subsec:bm_3}, we test baselines on 10 manually designed long-horizon tasks and introduce the DUB metric to quantify \pb's effect on task performance. Table~\ref{tb:bm3_results} presents average results across three random seeds, along with the Leveled Success Rate (Level SR), which measures success rates progressively along the skill chain: L0 represents the success rate for completing the first skill, L1 for completing the first two skills, and L2 for completing all three.\gb{I'm confused by this} The results indicate that DUB is positive and high in most cases, highlighting \pb's negative impact. The low DUB for BC-RESNET-RNN is attributed to its poor overall performance, resulting in a low UB. These findings from \bmc provide the clearest validation of the detrimental effect \pb has on long-horizon tasks. \gb{All the different evaluation metrics makes it difficult to understand these analyses. I didn't understand the main takeaways from these results}

\gb{Maybe instead of tables a bar plot would be a better way to present the results for all the challenges, especially if you want to include the results for all the tasks? I feel like tables are too overwhelming and don't look good in this case}



\subsection{Factors Influencing \pb}
\label{subsec:factors}

\input{tables/abl1_table}
\input{tables/abl2_table}

In this section, we examine two potential factors influencing the severity of \pb: task difficulty and the magnitude of visual modifications. This analysis aims to provide insights to guide future research and improve the development of algorithms robust to \pb. %robust IL algorithms.

\textbf{Task Difficulty:} We begin by investigating task difficulty as a factor. Task difficulty is quantified using task success rate, under the assumption that demonstrations provided for all tasks are of similar quality. To measure the severity of \pb, we introduce Relative Performance Drop (RPD) as a metric, defined as the PD divided by the original success rate. To isolate task difficulty as the sole changing factor and better analyze its relationship with \pb, we select six sets of tasks where all tasks in each set are performed within the same scene (e.g., kitchen scene 2 in Libero). For each set, we apply identical modifications to all tasks. Since the modifications introduced do not affect task feasibility, the hypothesis implies that when evaluating baselines on the tasks within a set, we should observe a higher RPD for tasks with lower success rates compared to those with higher success rates. After evaluation, for each set and each baseline, we obtain two lists of results: one for success rates on original tasks and another for PD on modified tasks. We use Spearman's rank correlation coefficient ($\rho$)~\cite{spearman1961proof} to test for a decreasing monotonic relationship between task success rate and RPD. Table~\ref{tb:fac1_results} summarizes the results for the baselines (excluding OpenVLA due to time constraints). In the table, $\rho$ indicates the Spearman correlation (a negative value implies a decreasing monotonic relationship), and $p$ represents the p-value. Ultimately, none of the baselines exhibited a significant decreasing monotonic relationship (i.e., $\rho < 0$, $p < 0.05$), and  thus we did not find evidence to support the hypothesis that task difficulty would be a key factor influencing the severity of \pb. \gb{I also find these results difficult to interpret from the Table. The table has a lot of numbers/details that are not clearly explained. Could this be better presented as some sort of plot? If not, then you need to simplify the table. I also feel like this analysis is convoluted/overhwelming. In my view, it's possible to summarize the main findings in a much simpler way, and also more concisely.}

\textbf{Magnitude of Visual Modification:} The magnitude of visual modification is another factor that may influence the severity of \pb. We use PD as a metric to explore this potential relationship. To ensure that the magnitude of visual modification is the only variable changing, we follow a similar approach to \bmb by increasing the number of modifications applied to the same task. Unlike the random RAMG-generated modifications used in \bmb, we incrementally add new modifications based on existing ones, ensuring a clear correlation between the number of modifications and the size of the visual changes. Our hypothesis is that increasing the magnitude of visual modifications (i.e., the number of modifications) will result in a greater negative effect (i.e., higher PD) caused by \pb. We test this hypothesis using three baselines evaluated on 10 tasks for each modification level. Due to space constraints, Table~\ref{tb:fac2_results} presents the average results, which show that PD consistently increases for each baseline as the magnitude of visual modifications grows. These findings support the hypothesis that the magnitude of visual modifications may be a key factor influencing the severity of \pb.\gb{I'm a bit confused by this experiment. Are you showing something different here than you did for Challenge 2 where you also studied PD as a function of modifications? Also, your table for this is extremely uninformative. It's not clear what the values in the table actually measure. The table doesn't have any useful captions. If I just looked at the table, I would have no idea what I'm looking at.}

\subsection{Data Augmentation for \pb}
\label{subsec:results_bl}

\input{tables/da_table}

% intro the dataset we use for training
% intro the confusion matrix
% 1. pd is small
% 2. C - D is bad <= cuased by B's drop (even for OpenVLA)
% conclusion: it's not that straightforward to directly use data augmentation way to solve the \pb issue. Need special design.

% As stated in Section~\ref{subsec:our_bl}, we pre-train or finetune baselines on the large dataset generated via RAMG. To identify the effect of the simple data augmentation method, we compare the following 4 types of results:
% \begin{enumerate}[A.]
%     \item Trained on demonstrations of single environment-1, Evaluated on the same environment-1.
%     \item Trained on augmented mixtured demonstrations of diverse environment, Evaluated on the environment-1.
%     \item Trained on demonstrations of single environment-1, Evaluated on the modified environment-1.
%     \item Trained on augmented mixtured demonstrations of diverse environment, Evaluated on the modified environment-1.
% \end{enumerate}

% Table~\ref{tb:da_results} shows the above 4 types of results for baselines. When trained with augmented mistured demonstrations, although we could observe nearly 0 PD for all 3 BCs by comparing results-B and results-D, this does not prove that using more diverse training demonstrations could largely mitigate \pb problem. When comparing results-C and results-D where the baselines are both evaluated on modified environment but one (C) is trained only with original environment demonstration and the other one (D) is trained with augmented mixtured data, we could see that only BC-RESNET-RNN shows a bit improvement, and the other three baselines all present equivalent or even worse performance. By comparing results-A and results-B where baselines are both evaluated on the seen environment, we could find out that the current IL baselines, even for OpenVLA which is the generalist policy, will face performance drop when trained on larger and more diverse dataset. Therefore, we could conclude that simply trained on more diverse and larger dataset cannot bring clear improvement, the \pb problem is a severe problem that need some specific method design.



As described in Section~\ref{subsec:our_bl}, we pre-train or fine-tune baselines on the large and much more visual diverse dataset generated using RAMG. To analyze the impact of the data augmentation method, we compare the following four experimental setups, all designed for the same task but differing in the visual characteristics of the training and evaluation environments:
\begin{enumerate}[A.] 
    \item Trained on single-environment, noted as Env-X, demonstrations and evaluated on the same environment (i.e., Env-X). 
    \item Trained on augmented demonstrations from diverse environments and evaluated on Env-X. 
    \item Trained on Env-X and evaluated on the modified version of Env-X. 
    \item Trained on augmented demonstrations from diverse environments and evaluated on the modified version of Env-X. 
\end{enumerate} 
\gb{I understand these four settings but this is too complex and most readers will be lost/overwhelmed by these setups. You need to find a way to present these in a simpler manner}

Table~\ref{tb:da_results} presents average results across three random seeds for these setups across all baselines, with only the averaged results across all 44 tasks reported due to space constraints. Several key observations emerge from the results. Comparing Results-C and Results-D, where baselines are evaluated on the modified environment, shows that BC-RESNET-RNN exhibits slight improvement in D. However, other baselines, including BC-RESNET-T, BC-VIT-T, and OpenVLA, perform equivalently or worse, highlighting the limited effectiveness of data augmentation in mitigating \pb. A possible explanation emerges from comparing Results-A and Results-B, where baselines are evaluated on the same original environment. This comparison shows that training on a larger but more diverse dataset leads to performance drops across all baselines, including OpenVLA, the generalist policy. These findings indicate that simply training on larger and more diverse datasets is insufficient to address \pb effectively, emphasizing the need for specific algorithmic solutions.\gb{Do we really need baselines A and B? What do they contribute to this analysis? Generally, for me it was difficult to follow this analysis as well. }