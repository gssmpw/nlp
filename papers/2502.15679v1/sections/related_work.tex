\section{Related Work}

\subsection{Robot Learning Benchmarks}
% Many benchmarks (meta-world, maniskill, robosuite)

% Calvin - multitask learning, test generalization
% LoHoRavens (focus on language and not related to transition),  FurnitureBench [12] and CausalWorld [13] focus on real-world furniture assembly and 3D shape construction respectively (too focus on specific scenario, and not good for evaluating the transition issue) - not focus on transition; robocasa - focus on realistic and diverse human-centered environments with a focus on kitchen scenes and not focus on transition issue as well.
% Libero - focus on lifelong learning, but providing nice flexibility to adapt to long horizon tasks, so we build based on libero



In recent years, numerous benchmarks for robot learning research have been developed. For example, Meta-World~\cite{yu2020meta} targets meta-reinforcement learning and multi-task learning. RLBench~\cite{james2020rlbench} provides 100 simulated household tasks. D4RL~\cite{fu2020d4rl} focuses on offline reinforcement learning across diverse tasks. Robomimic~\cite{robomimic2021} emphasizes learning from human demonstrations. ManiSkill2~\cite{mu2021maniskill} supports a wide range of tasks incorporating both proprioceptive and visual data. However, none of these benchmarks are designed explicitly for long-horizon tasks.

On the other hand, CausalWorld~\cite{ahmed2020causalworld} includes long-horizon tasks but focuses on causal structures and transfer learning. Calvin~\cite{mees2022calvin} addresses language-conditioned multi-task policy learning. FurnitureBench~\cite{heo2023furniturebench} features realistic, long-horizon furniture assembly tasks. RoboCasa~\cite{nasiriany2024robocasa} offers a variety of long-horizon tasks set in kitchen environments, created using generative models. LoHoRavens~\cite{zhang2023lohoravens} is tailored for language-conditioned long-horizon tasks. Despite their significant contributions, these benchmarks do not focus on skill transitions and are thus not well-suited for studying the \pb problem, which demands diverse skill transitions within long-horizon tasks.
% Despite their significant contributions, none of these benchmarks are designed to address the Observation Space Shift (\pb) problem, which requires diverse skill transitions within long-horizon tasks\gb{I feel like you need a more clear discussion/explanation differentiating between these prior works and yours. If this problem is widespread in long-horizon tasks as you are claiming, then it should be present in their benchmarks too, no? Do these long-horizon tasks not involve sufficiently diverse skills? If so, say that explicitly} 
Libero~\cite{liu2024libero}, while primarily designed for lifelong robot learning, provides diverse tasks and customization capabilities, making it an ideal foundation for developing our \bm benchmark, which provides diverse and large-scale skill transitions, and specifically targets the \pb problem.
% \gb{again it would be useful to explain what does ths mean, i.e., focusing on diverse compositional skills, or what? When is this \pb problem most exposed if not in all long horizon tasks? Why is your dataset better than these other datasets? Also, what's the comparison in terms of scale?}


\subsection{Skill Chaining}

Skill chaining, a fundamental structure in HIL, sequentially executing pre-learned skill policies to complete long-horizon tasks~\cite{konidaris2009skill, garrett2021integrated, guo2023recent, ahn2022can, singh2023progprompt, du2023video}, where ensuring seamless transitions between adjacent skills is the primary challenge. One approach addresses this by learning transition policies to bridge the gap between the terminal state of the previous skill and the initial state of the next~\cite{lee2019composing, byun2021training, watahiki2022one}. Lee et al., 2019 proposed transition policies with proximity predictors to connect primitive skills for sequential skills~\cite{lee2019composing}, while adversarial inverse reinforcement learning is used to learn transition policies by matching state and action distributions~\cite{byun2021training}. Goal-conditioned policies have also been employed to imitate transitions from target demonstrations~\cite{watahiki2022one}. Another approach focuses on enforcing alignment between the terminal state set of the previous skill and the initial state set of the next. Expanding the initial state set of the next skill to include the terminal state set of the previous one is one method~\cite{clegg2018learning}, though it can lead to significant state space growth over time. Lee et al., 2021 address this issue by using an adversarial learning framework to regularize the terminal state distribution of the previous skill to match the initial state distribution of the next skill~\cite{lee2021adversarial}. Additionally, contrastive learning has been applied to learn auxiliary rewards that guide terminal states closer to the initial states of the next skill~\cite{li2024auxiliary}. Chen et al., 2023, optimized entire policy chains to ensure dynamic transition feasibility in dexterous tasks~\cite{chen2023sequential}. Despite these efforts, all methods rely on the two strong assumptions outlined in Section~\ref{sec:introduction}: the reachability of initial states for each skill and the feasibility of continuous policy fine-tuning. These assumptions significantly limit their ability to address the \pb problem effectively in general scenarios.

