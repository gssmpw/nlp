\section{Introduction}
\label{sec:introduction}

% Picture the future for general robot - need to complete many long horizon tasks (such as cooking). Show long horizon tasks are important. 
% Then show IL is a promising methods now, and could help with many tasks. So use IL to solve long horizon tasks is very natural. 
% However, directly use IL to learn from long demonstrations is hard due to covariate shift issue and lack flexibility.
% So HIL comes out and become a promising direction, where number of skills is relatively limited and combining skills together to complete a long-horizon task is human-like.


Recent advances in robotic learning have demonstrated its potential in diverse manipulation applications~\cite{ravichandar2020recent}, including manufacturing~\cite{liu2022robot}, sports~\cite{zaidi2023athletic, chen2021learning}, and household tasks~\cite{yang2024enhancing, yang2024arcade}. Imitation Learning (IL), which empowers end users to teach robot skills and behaviors through demonstrations, has become a prevalent approach in developing various skill controllers~\cite{chi2023diffusion, kim2024openvla}. However, IL algorithms remain limited to relatively short-horizon tasks due to covariate shift~\cite{ross2011reduction, chang2021mitigating}, where small discrepancies in action predictions accumulate over time. This challenge becomes even more pronounced in long-horizon tasks, as such errors propagate across multiple sequential steps, creating a significant barrier to achieving general-purpose robots. Task and Motion Planning (TAMP)~\cite{garrett2021integrated, guo2023recent} and recent advancements in large language model based planning~\cite{ahn2022can, singh2023progprompt, du2023video} offer a solution by integrating IL into a hierarchical framework~\cite{xu2023xskill, zhu2022bottom, shiarlis2018taco}, decomposing end-to-end long-horizon IL into high-level task planning and low-level visuomotor skill execution.

% has focused on Hierarchical Imitation Learning (HIL)\notezlf{yes, don't use "HIL"; you can mention "chaining/sequencing skills" "task planning", can briefly mention "TAMP" and cite their work; also cite George Konidaris's work as they have lots of skill chaining papers}, which decomposes end-to-end imitation learning into high-level task planning and low-level visuomotor skill execution.



\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{imgs/illustration_OSM.png} 
    \caption{The example illustrates how Observation Space Shift (\pb) occurs when chaining two pre-trained skills: \texttt{PlaceObject(potato, bowl)} and \texttt{MoveContainer(bowl, cabinet)}. During deployment, \pb arises in \texttt{MoveContainer(bowl, cabinet)} because the observation space changes, with a potato inside the bowl instead of the empty bowl scenario from \texttt{MoveContainer(bowl, cabinet)}'s pre-training. 
    % \gb{Good figure overall. I wonder if it would help to label the actions in the bottom subfigure as 1 and 2 to indicate the order more clearly. Currently, there are many arrows and it's not clear what the flow is. I would also remove the red arrow. Maybe you can place the red text on the left (instead of the right) closer to the second action and also remove the red arrow which is confusing. You can also highlight the read area (i.e., a potato in the bowl with a thicker red square. Currently, the highlighted area is hard to see. If you have space, you could even show a zoomed in version of a bowl with a potato and place the red description under the zoomed in visualization}
    % \gb{I think the figure needs to more clearly show that the two skills are trained independent of each other and then chained together. currently this can be a bit ambiguous. I think generally I would make the figure by contrasting the training and inference steps, i.e., in the training the skills are trained independently. During inference they re chained together and thus depends on another.}
    }
    \vspace{-2em}
    \label{fig:osm_illustration}
\end{figure}

% However, current HIL methods all assume same/known demos for tasks during training time, even for simple long-horizon tasks, such as skill chaining. (explain what is skill chaining). 
% Explain the reason is often caused by transition issue (cite some work), where terminal state distribution doesn't match initial state distribution. Previous methods solve this issue by [...].
% However, those methods all hold the assumption that the initial state of next skill is reachiable, but this is not true, especially for visual-servoing policies. explain \pb ...


Skill chaining, the sequential execution of pre-learned skills, is a simple yet powerful structure for tackling complex long-horizon tasks~\cite{bagaria2019option, konidaris2009skill}.
%
However, failures often arise in a skill chain when a skill encounters initial states that were not seen during training~\cite{lee2019composing, clegg2018learning}. 
%
Specifically, the terminal state of a preceding skill may fall outside the initial state distribution the next skill policy was trained on, leading to task failure. For instance, consider a household long-horizon task illustrated in Fig.~\ref{fig:osm_illustration}, which involves two skills: \texttt{PlaceObject(potato, bowl)} and \texttt{MoveContainer(bowl, cabinet)}.
% \gb{Would it make sense to describe these in normal text rather than coding functions? Maybe it's ok. I just thought that since the intro is high-level / focuses on intuitions to keep it less technical in terms of specification}
When training the visuomotor policy for \texttt{MoveContainer(bowl, cabinet)}, the initial state set excludes visual states (e.g., images) containing anything inside the bowl.
% \gb{This raises question why do they do that? Why not use data augmentations?}
However, the terminal state of \texttt{PlaceObject(potato, bowl)} places a potato in the bowl, creating a visual mismatch that can cause the \texttt{MoveContainer(bowl, cabinet)} policy to fail. We refer to this issue as \textbf{Observation Space Shift (\pb)}, a problem that frequently arises during skill transitions in visual-input long-horizon tasks. A formal definition of \pb is provided in Section~\ref{sec:problem_definition}.



As a result, ensuring smooth skill transitions is critical in skill chaining and serves as a key factor for completing long-horizon tasks~\cite{li2024auxiliary}.
% \notezlf{To perform smooth skill chaining, we need to make sure the effects of the previous skill is within the preconditions of the next skill during execution. However, this might not be true if we train the skills individually without the consideration of deployment time. Cite: Kondaris et al. IJRR 2018}
Previous researchers tackle the skill transition problem through two main strategies. The first line of offline approaches involves learning a transition policy to shift the state from the terminal state of one skill to the initial state of the next~\cite{watahiki2022one, byun2021training, lee2019composing}.
%
The second approach ensures that the initial state set of the next skill matches the terminal state set of the previous skill by fine-tuning the policy of the preceding skill, the next skill, or both in an online manner~\cite{li2024auxiliary, chen2023sequential, lee2021adversarial}. However, all these previous works design observation spaces that exclude visual inputs,
% \gb{Would it make sense to have a baseline that does this but doesn't exclude visual inputs?}
causing the above methods, whether using offline or online learning, to rely on assumptions that are often impractical for visuomotor IL policies. 1) Offline methods assume that the initial states for each skill are always reachable, which is often unreasonable for visual-input long-horizon tasks, as it may require undoing previously completed skills. For example, as shown in Figure~\ref{fig:osm_illustration}, offline methods would need to learn a transition policy to remove the potato from the bowl in order to enable the observation space to match the initial state of \texttt{MoveContainer(bowl, cabinet)} (i.e., an empty bowl), which is logically invalid. 2) Online approaches, on the other hand, assume that policies can be continuously fine-tuned to handle new initial states. This assumption is equally problematic for visual-input long-horizon tasks, where \pb is highly likely to happen at each skill transition. The frequent occurrence of \pb necessitates repeated fine-tuning, leading to substantial computational overhead and inefficiency.

% readily available, which is unrealistic without conducting online rollouts at scale considering that there exist countless combinations of visual changes. Like the example shown in Figure~\ref{fig:osm_illustration}, offline methods require knowing the bowl's contents in advance

% Online approaches, on the other hand, assume that policies can be continuously fine-tuned to handle new initial states. This assumption is equally problematic for long-horizon tasks with multiple skill transitions, as repeated fine-tuning introduces significant computational overhead and inefficiency. \yy{Such as the example,} Offline methods require knowing the bowl's contents in advance and training skill-2’s policy for specific initial state setups (e.g., ensuring the potato is already in the bowl before demonstration collection). This is impractical, particularly when long-horizon tasks involve many skills with diverse initial states. Similarly, online methods will need to fine-tune skill-2’s policy to include initial states where the potato is in the bowl, which is time-consuming and inefficient with multiple such adjustments. The problem is further compounded when the objects change (e.g., from a potato to an apple), necessitating additional fine-tuning. \yy{add the limitations are constrained when without visual input, but nowadays the arise of oss visual input case this could be quite frequent.}
























% previous-written
% In HIL, skill chaining, the sequential execution of pre-learned skills, is a simple yet powerful structure for tackling complex long-horizon tasks. However, robots often fail during the sequential execution of skills when a policy encounters states at the start of a skill (i.e., the initial states for that specific skill) that were not observed during training~\cite{lee2019composing, clegg2018learning}. Specifically, the terminal state of a preceding skill may fall outside the initial state distribution the next skill policy was trained on, leading to task failure.\gb{I think it would be a lot more clear if you gave a concrete example of this instead of just trying to explain it abstractly. Also, when doing that refer to your teaser figure} As a result, ensuring smooth skill transitions is critical in skill chaining and serves as a key factor for completing long-horizon tasks~\cite{li2024auxiliary}. Researchers tackle the skill transition problem through two main strategies. The first approach involves learning a transition policy to shift the state from the terminal state of one skill to the initial state of the next~\cite{watahiki2022one, byun2021training, lee2019composing}. The second approach ensures that the initial state set of the next skill matches the terminal state set of the previous skill by fine-tuning the policy of the preceding skill, the next skill, or both in an online manner~\cite{li2024auxiliary, chen2023sequential, lee2021adversarial}.

% \gb{Do you have any experiments with any of these approaches in your benchmark? To me this seems like the second approach should be very effective in solving this issue}


% 1. not practical (need to pre-know the long-horizon task), against goal of general robotics
% 2. many initial states are not reachiable. A. sth in the bowl, u cannot take the thing out -> cause task failure B. put sth into upper drawer but bottom drawer is open, it's super time-consuming and meaningless to close the bottom drawer first before closing the upper drawer.




% \textcolor{red}{Need to modify motivation of this paragraph. Need more focus on visual-servoing + behaviors are not the only issue causing skill transition, visual feature is also very important + not clear whether it's important, so need this work to verify}

% ass1: combination of skill transition is uncountable - need extra online training for each one before each deployment, which is impossible. ass2: assume initial state of next skill is always reachable by transition method or finetunine.
% example with fig 1. However, for ass1, there're many types of objects could be in the bowl. As proven in our experiments part, even generalist policy cannot work well. for ass2, 

% Based on training type, online or offline, the above methods rely on infeasible assumption.
% ass1: assume the initial states data are easy to obtain beforehand for offline learning, which is 1. actually impractical to know initial states for each skill without online rollouts. 2. countless initial states, so hardly to exhaustively collect all. 
% ass2: assume extra learning - online learning transition policy or finetuning is always feasible. But it's not practical to 1. finetuning several times or learn several transition policies if long-horizon task contains many skill transitions. 2. need to re-finetuning or learn new transition policy frequently even if small changes happen for the long-horizon task. 

% (change an object in the container will need a new finetuning).

% Later, the motivation for DA is - a straightforward solution is to collect more diverse and large set of demonstrations, based on which training policies can potentially force the policy focus only on task-relevant elements and overlook those irrelevant part.


% The above methods, whether using offline or online learning, rely on assumptions that are often impractical. Offline methods assume that the initial states for each skill in a long-horizon task are readily available, which is unrealistic without conducting online rollouts. Online approaches, on the other hand, assume that continuously fine-tuning policies is always feasible. This assumption becomes increasingly unrealistic in long-horizon tasks with multiple skill transitions, as the repeated need for fine-tuning introduces significant computational overhead and inefficiency. Furthermore, even minor changes to a single skill within the task would require additional fine-tuning, further complicating the process and reducing practicality. For instance, as illustrated in Fig.~\ref{fig:osm_illustration}, consider a simple household long-horizon task, which involves two skills: (1) placing the potato into a bowl, and (2) moving the bowl onto the cabinet. Offline methods' assumption requires knowing the bowl's contents beforehand and training skill-2 policy tailored to specific initial state setup (i.e., put the potato into the bowl before demonstration collection), which is not practical especially when long-horizon task contains many skills. Online method's assumption will require to finetune skill-2 policy to enlarge its initial state set to include initial states where potato is in the bowl, which could be time-consuming and thus impractical if multiple such finetunings are required. It would also require new finetuning when potato is changed to any other objects.


% previous-written
% The above methods, whether using online or offline learning, rely on assumptions that are often impractical, especially in long-horizon tasks. Offline methods assume that the initial states for each skill are readily available, which is unrealistic without conducting online rollouts at scale.
% \gb{is it really unrealistic if someone collects long-horizon training data ar scale?}
% Online approaches, on the other hand, assume that policies can be continuously fine-tuned to handle new initial states. This assumption is equally problematic for long-horizon tasks with multiple skill transitions, as repeated fine-tuning introduces significant computational overhead and inefficiency. For instance, consider a household long-horizon task illustrated in Fig.~\ref{fig:osm_illustration}, which involves two skills: (1) placing a potato into a bowl, and (2) moving the bowl onto the cabinet\gb{I feel like it would be more important to introduce this example to define the problem in the previous paragraph, instead of highlighting the weaknesses of existing methods in this paragraph}. Offline methods require knowing the bowl's contents in advance and training skill-2’s policy for specific initial state setups (e.g., ensuring the potato is already in the bowl before demonstration collection). This is impractical, particularly when long-horizon tasks involve many skills with diverse initial states. Similarly, online methods will need to fine-tune skill-2’s policy to include initial states where the potato is in the bowl, which is time-consuming and inefficient with multiple such adjustments. The problem is further compounded when the objects change (e.g., from a potato to an apple), necessitating additional fine-tuning. 




% The above methods rely on two strong assumptions. First, they assume that the framework has prior knowledge of the exact steps in the skill chain, an unrealistic expectation for practical deployment, and a barrier to achieving general robotics\gb{I'm confused why this is unrealistic. If I'm trying to solve a particular long-horizon task, i.e., set up a table, I think it's reasonable to assume that I'll know the steps needed to solve this task. I think this logic can be applied to many tasks. I'd say to me based on your descriptions it's more strange that these atomic skills are trained on data unrelated to long horizon tasks (at least if I understand correctly), which then is not surprising that these methods won't work in a different setting}. Second, they presume that the initial states of subsequent skills are always reachable\gb{I understand what you mean but this is not very clearly written. Also, again to me this seems very weird. Why would you train your atomic policies in this way if your goal is long-horizon planning? It's pretty obvious that they are not going to work well} using the proposed strategies or focusing solely on dynamic transition feasibility~\cite{chen2023sequential}, a limitation that is particularly problematic for visual-servoing\gb{what is this term?} skill policies. For instance, as illustrated in Fig.~\ref{fig:osm_illustration}, consider the task of heating a potato, which involves two skills: (1) placing the potato in a bowl, and (2) moving the bowl into an open microwave. Assumption 1 requires knowing the bowl's contents beforehand and training skill policies tailored to specific objects\gb{To me the solution seems to train with a wide range of objects}. Assumption 2 fails if the initial state of skill (2) (an empty bowl)\gb{This is unclearly written} during training mismatches its real use case (a bowl containing a potato), making it unreachable by transition policies or online fine-tuning. \gb{To me again this assumes a pretty crude policy that's trained only on one specific scenario. I guess to me this would be a lot more convincing if you said that even for modern large-scale VLAs which are trained with a wide range of conditions/variations, this is an issue. Otherwise, to me this seems like a narrow problem that applies to models that are trained on very narrow data/scenarios}

% These limitations highlight the need for a more practical and robust framework to successfully handle long-horizon tasks.


% [Optional] Say there're not proper benchmark designed for this problem.

% Thanks to open-source, introduce our benchmark \bm, tailing for this problem. 
% Introduce 3 task suites briefly.
% Introduce 2 ablations we conduct.
% Introduce the baseline we test on the \bm.

% contributions
% Before finding a framework, we need to do some verifications ...

% \yy{todo: start with introducing BOSS} These limitations motivate us to investigate a new research problem called \textbf{Observation Space Shift (\pb)}, which frequently arises during skill transitions in long-horizon tasks. A formal definition of \pb is provided in Section~\ref{sec:problem_definition}. 

These limitations motivate us to propose a dedicated \textbf{Benchmark for Observation Space Shift (\bm)}, to demonstrate the significant negative impact \pb has on long-horizon tasks, facilitate a deeper understanding of the problem, and inspire potential solutions (Section~\ref{sec:bm_challenge}). Built on the LIBERO simulator~\cite{liu2024libero}, the benchmark consists of evaluating three progressively challenges:
% \my{duplicate here, remove the sentence}
% that examine \pb's negative impact from three distinct perspectives, with increasing difficulty. 
1) \bma evaluates the robustness of skill policies against a single observation modification caused by the preceding skill. For example, as shown in Figure~\ref{fig:osm_illustration}, we evaluate the robustness of the \texttt{PlaceObject(potato, bowl)} policy under a single modification (i.e., a potato placed in the bowl). 
2) \bmb builds on this by evaluating the cumulative negative effects of multiple modifications introduced by several preceding skills, creating a more complex and challenging scenario than \bma. For instance, in the example from Figure~\ref{fig:osm_illustration}, adding a new skill \texttt{OpenDrawer(cabinet)} before \texttt{PlaceObject(potato, bowl)} results in accumulated modifications for the \texttt{MoveContainer(bowl, cabinet)} policy (i.e., the drawer is open, and a potato is placed in the bowl).
3) \bmc focuses on a realistic long-horizon task comprising a three-skill chain, testing policy performance across the entire sequence (e.g., \texttt{OpenDrawer(cabinet)} $\rightarrow$ \texttt{PlaceObject(potato, bowl)} $\rightarrow$ \texttt{MoveContainer(bowl, cabinet)}). 

We assess four popular IL algorithms using these benchmarks, offering detailed analyses of the results to generate insights and set baselines for future research (Section~\ref{sec:bm_results}). % In addition, we explore the impact of two potential factors, task difficulty and visual modification magnitude, on \pb. Our findings indicate that only the magnitude of visual modifications significantly affects \pb, as detailed in Section~\ref{sec:factors}.
In addition, we examine whether data augmentation can mitigate the \pb problem by expanding the existing Libero dataset with our Rule-based Automatic Modification Generator (RAMG). This generator produces a large, visually diverse dataset, serving as a valuable resource for the community. However, comprehensive experiments on the augmented data reveal that data augmentation alone is insufficient to address the \pb, providing key insights for future research (Section~\ref{sec:da}). In summary, our contributions of this work are three-fold:

% Finally, we explore whether scaling up training data with a larger and more diverse set of demonstrations offers a potential solution to \pb. We expand the existing dataset from Libero using our proposed Rule-based Automatic Modification Generator (RAMG), which generates a large and diverse dataset for the community as a valuable by-product. Through comprehensive experiments on the generated dataset, we test the effectiveness of this approach, offering insights for future research. \yy{need to add findings and refer to sections after i change the paper flow} \gb{In the last three sentences you list of what you do but don't include any findings. Not sure what the protocol is but I would also expect a summary/overview of what your findings from these experiments were} In summary, our contributions in this work are four-fold:

% Additionally, we propose a simple baseline solution, \bl, that not only highlights a promising direction\gb{If this is a data augmentation-based solution, I feel like it somewhat undermines the value of your benchmark, i.e., that such a simple and intuitive solution, which to me comes to my mind many times as I'm reading your intro, would basically solve the issue. I feel like you should also provide more details what this solution is} for addressing \pb but also generates a corresponding dataset as a valuable byproduct. Finally, we conduct two ablation studies to identify key factors contributing to the \pb problem. In summary, our contributions in this work are four-fold:

\begin{enumerate}%[leftmargin=*]
    % \item \my{May merge to three, put an example below} We formulate the \textbf{Observation Space Shift (\pb)} problem, which is crucial for successfully completing long-horizon robot tasks.    
    % \item We present a benchmark suite, \textbf{Benchmark for Observation Space Shift (\bm)}, comprising three challenges of increasing difficulty, and evaluating four popular IL methods on it.
    % \item We investigate how task difficulty and visual modification magnitude impact the severity of \pb and find that only visual modification magnitude significantly influences it, offering deeper insights into its challenges.
    % \item We generate a large, diverse dataset with the proposed Rule-based Automatic Modification Generator, but find that training IL methods on this augmented data fails to mitigate the \pb problem, highlighting the need for further study in algorithmic design.
    
    \item For the first time, we formulate the \textbf{Observation Space Shift (\pb)}, a critical problem in long-horizon robotic tasks.
    % , and reveal visual modifications are the primary factor affecting \pb rather than task difficulty.\yy{need to delete last sentence if deleting the sec VI.}
    
    \item We introduce \bm, a comprehensive benchmark that evaluates four IL methods across three increasingly challenging scenarios of \pb in long-horizon manipulation.
    
    \item By creating a large and diverse dataset with the proposed Rule-based Automatic Modification Generator, we demonstrate that data augmentation alone is insufficient to mitigate \pb, emphasizing the need and room for algorithmic solutions in future research.
\end{enumerate}

% \gb{Overall, my take is that you need a crisper/more clear definition for your OSS problem. Right now the way you are describing it in text is somewhat cumbersome and not very clear. For instance, if I didn't know what the project was about, I would have a lot of issues understanding what this problem actually refers to. I'd also suggest to use concrete examples to illustrate the problem.}
