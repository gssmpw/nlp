\section{Related Work}
\label{sec:formatting}
\begin{figure*}[t]
\centering
\includegraphics[width=2\columnwidth]{figures/method.pdf} % Reduce the figure size so that it is slightly narrower than the column.
\caption{The framework of AproMFL.}
\label{fig:method}
\end{figure*}
\noindent\textbf{Data-Heterogeneous Federated Learning.}
FL is a distributed machine learning framework where clients train locally on private data, and the server aggregates client models to update a global model. 
Throughout this process, data remains local, effectively preserving client privacy. 
Among FL algorithms, FedAvg~\cite{mcmahan2017communication} is one of the most representative. 
%However, FL models often perform poorly 
Some prior work has tried to address low performance due to data heterogeneity, where client data is non-independent and identically distributed (Non-IID)~\cite{li2023exploring}.
% low performance issue caused by data heterogeneity, i.e., client data are non-independent and identically distributed (Non-IID)~\cite{li2023exploring}. 
% client data are non-independent and identically distributed (Non-IID)~\cite{li2023exploring}. 
%To address the challenges posed by data heterogeneity, methods such as 
% For example, 
FedProx~\cite{li2020federated} and MOON~\cite{li2021model} introduce learning objectives to adjust local model training, while methods like FedAvgM~\cite{hsu2019measuring}, FedNova~\cite{wang2020tackling}, and FedMA~\cite{wang2020federated} mitigate heterogeneity's impact on model performance through aggregation.
% For example, FedProx~\cite{li2020federated} and MOON~\cite{li2021model} introduce learning objectives that adjust the direction of local model training, while other work, e.g., FedAvgM~\cite{hsu2019measuring}, FedNova~\cite{wang2020tackling}, and FedMA~\cite{wang2020federated}, mitigates the adverse effects of heterogeneity on model performance from the perspective of model aggregation. 
Other strategies to address data heterogeneity include meta-learning~\cite{fallah2020personalized}, hypernetworks~\cite{shamsian2021personalized}, multi-task learning~\cite{lu2024fedhca2}, and knowledge distillation~\cite{zhang2022fine}. 
However, existing methods generally assume that clients are unimodal.
%these methods generally assume that clients are unimodal. 
In MFL, clients may exhibit heterogeneity in modality, task, and statistics. Due to these differences in modality and task, existing FL methods for handling data heterogeneity cannot be directly applied to MFL.


\noindent\textbf{Multimodal Federated Learning.} MFL extends unimodal FL by enabling multimodal clients to participate in training. Zong et al.~\cite{zong2021fedcmr} introduced a framework for federated cross-modal retrieval, allowing multiple clients to collaboratively train a cross-modal retrieval model in a structure similar to FedAvg.
Li et al.~\cite{li2023prototype} proposed an unsupervised cross-modal hashing approach to enhance client privacy using prototype representations of local multimodal data. 
% Li et al.~\cite{li2023prototype} proposed an unsupervised cross-modal hashing approach that enhances client privacy by leveraging prototype representations of local multimodal data. 
While these studies provide foundations for MFL, they assume all clients possess identical multimodal capabilities. Modality heterogeneity is a critical challenge in MFL, where clients differ in modality types. 
Current modality heterogeneity mainly falls into two types: MFL with partial modality missing in some samples~\cite{xiong2023client,bao2023multimodal} and MFL with mixed modalities~\cite{peng2024fedmm,le2024cross}.
Existing research primarily focuses on cases with partial modality missing in some samples, while the study of mixed modalities has not been fully explored. To address the challenge of aligning modality knowledge in mixed modalities, Yu et al.~\cite{yu2023multimodal} proposed an MFL framework that distills knowledge from clients with different modality types into a unified global model via knowledge exchange through a public dataset. Similarly, Huy et al.~\cite{le2024cross} developed a multimodal joint cross-prototype learning method that enables classification training with missing client modalities. However, these approaches rely on strong assumptions, such as the availability of a public dataset or sufficient labeled data for each client.
