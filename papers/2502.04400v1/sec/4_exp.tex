\section{Experiments}
\label{sec:rationale}


\begin{figure*}[t]
  \centering

   \subfloat[Local (Image)]
   {\includegraphics[width=0.23\textwidth]{figures/flickr_local_pca_image_global3-proto20.pdf}\label{fig-mmalign:suba}}
  \hfil
  \subfloat[AproMFL (Image)]
  {\includegraphics[width=0.23\textwidth]{figures/flickr_our_pca_image_global3-proto20.pdf}\label{fig-mmalign:subb}}
  \hfil
  \subfloat[Local (Text)]
  {\includegraphics[width=0.23\textwidth]{figures/flickr_local_pca_text_global3-proto20.pdf}\label{fig-mmalign:subc}}
  \hfil
  \subfloat[AproMFL (Text)]
  {\includegraphics[width=0.23\textwidth]{figures/flickr_our_pca_text_global3-proto20.pdf}\label{fig-mmalign:subd}}

  \caption{Distribution of client representations under the Flickr30k dataset.}
  \label{fig-mmalign}
\end{figure*}






%\subsection{Experiment Configuration}
\noindent\textbf{Datasets.}
We evaluate the performance of AproMFL in heterogeneous client modality scenarios across via three datasets, including CIFAR-10~\cite{krizhevsky2009learning}, AG-NEWS~\cite{zhang2015character}, and Flickr30k~\cite{young2014image}. 
We allocate the CIFAR-10 dataset to two unimodal image clients, the AG-NEWS dataset to two unimodal text clients, and the Flickr30k to two multimodal clients. 
To simulate Non-IID data, we use the Dirichlet distribution for data partitioning~\cite{hsu2019measuring}, i.e., a smaller value of $\alpha$ indicates a higher-level data heterogeneity. 


\noindent\textbf{Implementation Details.}
We use the image encoder and text encoder of CLIP~\cite{radford2021learning} as encoders for different clients. 
The mapping module of each client is configured as a three-layer fully connected network.
In multimodal retrieval tasks, we test both image-to-text (i2t) and text-to-image (t2i) retrieval, measuring the top-1 and top-5 recall, denoted by R@1 and R@5, respectively.
In classification tasks, we test both top-1 and top-5 accuracy for CIFAR-10, denoted by P@1 and P@5, respectively. We also test both top-1 and top-2 accuracy for AG-NEWS, denoted by P@1 and P@2, respectively.
All experiments were conducted on an RTX 4090 GPU, with Python 3.9, PyTorch 2.2.2, and CUDA 11.8.

\noindent\textbf{Baselines.}
We compared AproMFL with existing MFL methods, covering followings. 
%specifically including the following approaches: 
(1) Local, a method that considers only local training on clients without modality knowledge sharing between them.
(2) MM-FedAvg, an extension of FedAvg~\cite{mcmahan2017communication} adapted to the multimodal federated setting. 
(3) MM-FedProx, an adaptation of FedProx~\cite{li2020federated}, originally designed to handle data heterogeneity in single-modal scenarios, here extended to the multimodal federated setting.




% \subsection{Experiment Evaluation}

\subsection{Performance Comparison}
Table \ref{baseline_sm} presents a comparison of the average precision or recall of client models across different methods under varying degrees of data heterogeneity.
Under both degrees of data heterogeneity, AproMFL outperforms the baselines in terms of both average precision and average recall.
Specifically, when 
\begin{table}[H]
	\centering
%    \renewcommand\arraystretch{1.2}
	\begin{tabular}{c|c|ccc}
		\bottomrule
        Datasets & Metrics & w/o GP & w/o GM & AproMFL \\ \hline
%		\multirow{2}*{Datasets} & \multirow{2}*{Metrics} &\multirow{2}*{\makecell[c]{ w/o GP}}& \multirow{2}*{\makecell[c]{w/o GM}} & \multirow{2}*{AproMFL} \\
%        & & & & \\
	  \multirow{2}*{CIFAR-10}& P@1& 59.58\%&58.05\%	&59.78\% \\
        & P@5& 68.09\%&65.62\%	&68.72\%\\
        \hline
      \multirow{2}*{AG-NEWS}&P@1& 58.78\%&59.21\%	&59.63\% \\
        & P@2& 65.08\%&66.61\%	&67.45\%\\
      \hline
       \multirow{2}*{Flickr30k}&{R@1}$_s$&84.59\% & 86.56\%& 87.58\%\\
       &R@5$_s$&148.32\% & 150.03\%& 151.72\%\\
       % &R@1&00.00 & 00.00& 00.00\\
       % &R@5&00.00 & 00.00& 00.00\\
      
       \toprule
       
	\end{tabular}
    \caption{The comparison of precision and recall between AproMFL w/o GP, AproMFL w/o GM, and AproMFL.}
 \label{ablation}
\end{table}
\noindent$\alpha = 0.1$, in the classification task, AproMFL achieves an average P@1 that is 1.52\% and 2.64\% higher than local on the CIFAR-10 and AG-NEWS datasets, respectively. 
In retrieval tasks, AproMFL achieves an average R@1 that is 1.1\% and 2.44\% higher than local on i2t and t2i, respectively. 
This indicates that our scheme effectively combines data from different clients to produce a better-performing model. 
Furthermore, AproMFL outperforms MM-FedAvg, MM-FedProx, and CreamFL in both classification and retrieval tasks, which 

\noindent evidence that our scheme effecitvely leverages knowledge from clients with different modalities, achieving a great improvement on downstream tasks. 






\subsection{Ablation Study}
Our method primarily facilitates knowledge sharing among different clients by leveraging global prototypes and global models to guide local model training. 
We investigate the impact of global prototypes and global models on model performance. 
AproMFL w/o GP refers to the AproMFL without guidance from global prototypes, while AproMFL w/o GM refers to the AproMFL without guidance from global models. 
R@1$_s$ denotes the sum of top-1 recall rates for the i2t and t2i tasks, while R@5$_s$ represents the sum of top-5 recall rates for these tasks.
Table \ref{ablation} presents the test precision and recall for three methods, AproMFL w/o GP, AproMFL w/o GM, and AproMFL. 
We compare AproMFL w/o GP and AproMFL w/o GM with Local and MM-FedAvg from Table \ref{baseline_sm}. 
It is observed that both AproMFL w/o GP and AproMFL w/o GM outperform Local in terms of precision and recall across all three datasets. This suggests that models guided by either global prototypes or global models perform better than those trained locally. Additionally, AproMFL w/o GP exhibits superior precision and recall compared to MM-FedAvg. This implies that global models derived through model-adaptive aggregation are more effective in minimizing discrepancies between clients and promoting knowledge sharing, compared to models obtained via FedAvg. 
The results above show that both the global prototype and the adaptive aggregation of global prototypes improve model precision and recall. Ultimately, AproMFL achieves the best performance, highlighting that the combined use of global prototypes and models to regularize local training enhances model performance.








\subsection{Performance of Modality Alignment}
We evaluate the alignment of modality knowledge in our approach using the Cifar10 and Flickr datasets.
Figure~\ref{fig-align} shows the modality knowledge alignment results of two image clients after completing training on the CIFAR-10 dataset.
Compared to the client-local training method (Local), we observe that our approach, AproMFL, effectively brings closer the knowledge alignment of the same-modality clients, improving the modelâ€™s generalization ability.
As shown in Figure~\ref{fig-mmalign}, our approach also achieves good alignment results in terms of knowledge representation across different modality client models.
From the local method, we observe significant representation differences for the same image and text samples across client models. However, AproMFL effectively aligns the representations of the image client, text client, and multimodal client.
AproMFL enables knowledge sharing and alignment between heterogeneous modality clients, thereby enhancing performance on downstream tasks.
\begin{figure}[t]
  \centering
   \subfloat[Local]
  {\includegraphics[width=0.24\textwidth]{figures/local_pca_image_global3-proto20.pdf}\label{fig-align:suba}}
  \hfil
  \subfloat[AproMFL]
  {\includegraphics[width=0.24\textwidth]{figures/our_pca_image_global3-proto20.pdf}\label{fig-align:subb}}

  \caption{Distribution of representation of image clients under the CIFAR-10 dataset.}
  \label{fig-align}
\end{figure}


\begin{table*}[t!]
	\centering
%    \resizebox{0.5\textwidth}{!}{
%    \renewcommand\arraystretch{1.2}
%	\begin{tabular}{0.8\linewidth}{c|cc|cc|cccc}
    	\begin{tabular}{c|cc|cc|cccc}
		\bottomrule
		\multirow{2}*{K} & \multicolumn{2}{c|}{CIFAR-10} & \multicolumn{2}{c|}{AG-NEWS} &\multicolumn{4}{c}{Flickr30k}\\
        \cline{2-9}
      & P@1 &P@5&P@1 &P@2&R@1 (i2t)&R@5 (i2t) &R@1 (t2i)&R@5 (t2i)\\
		\hline 
       10 & 59.87\% & 68.72\%& 59.64\%& 67.45\%& 49.10\% & 80.35\% & 38.48\% & 71.37\% \\
       20 & 59.56\% & 68.76\%& 58.82\%& 67.15\%& 48.90\% & 79.60\% & 37.95\% & 71.21\% \\
       40 & 60.18\% & 68.77\%& 59.38\%& 68.47\%& 48.95\% & 79.05\% & 37.72\% & 71.24\% \\
       60 & 60.26\% & 68.28\%& 59.30\%& 68.19\%& 48.85\% & 79.04\% & 38.05\% & 71.58\% \\
       80 & 62.50\% & 69.28\%& 59.31\%& 68.92\%& 48.09\% & 78.05\% & 37.65\% & 70.58\% \\
       \toprule       
	\end{tabular}
%    }
    \caption{Average precision or recall of the model under different numbers of prototypes.}
 \label{proto_num}
\end{table*}
\subsection{Parameter Analysis}
\noindent\textbf{Performance under Different Numbers of Prototypes.}
Table \ref{proto_num} presents the model's average precision and recall for different numbers of global prototype pairs. 
As shown in Table \ref{proto_num}, with the increase in the number of global prototypes, our method's precision on CIFAR-10 and AG-NEWS, as well as recall on Flickr30k, fluctuates within a certain range.
For multimodal clients, as the number of global prototypes increases, the changes in R@1 and R@5 for the i2t task are 1.01\% and 2.30\%, respectively, and for t2i, the changes are 0.83\% and 1.00\%. For unimodal clients, 
% when the number of global prototypes increases from 10 to 80, 
the changes in P@1 and P@5 for CIFAR-10 are 2.94\% and 1.00\%, respectively, and for AG-NEWS, the changes in P@1 and P@2 are 0.82\% and 1.77\%.
This shows that our scheme is robust to different numbers of global prototypes. 


\noindent\textbf{The Impact of Different Numbers of Clients.}
Figure~\ref{fig-clients} illustrates the impact of varying numbers of clients on the model's average accuracy and recall rates.
Figure~\ref{fig3:subc} presents our methodâ€™s performance in classification tasks with 6, 9, and 12 clients. For image classification, increasing the number of clients amplifies the effect of data distribution, significantly impacting accuracy. For text classification, the simpler dataset structure allows our method to perform consistently well across all client counts.
Figure~\ref{fig3:subd} illustrates our methodâ€™s performance in multimodal retrieval tasks across different client counts. On the Flickr30k dataset, we observe that as the number of clients increases, retrieval performance declines. This is because more clients distribute the same dataset samples, increasing training difficulty and reducing both performance and generalization.





\begin{figure}[t]
  \centering

   \subfloat[Classification]
  {\includegraphics[width=0.23\textwidth]{figures/new_sm_client_num-crop.pdf}\label{fig3:subc}}
  \hfil
  \subfloat[Multi-modal Retrieval]
  {\includegraphics[width=0.23\textwidth]{figures/new_mm_client_num-crop.pdf}\label{fig3:subd}}

  \caption{Average precision or recall of the model under different numbers of clients.}
  \label{fig-clients}
\end{figure}

\noindent\textbf{The Impact of Different Types of Mapping Modules.}
Table \ref{mapping} presents the precision or recall of our method on three datasets under different mapping modules. 
As shown in Table~\ref{mapping}, for the Flickr30k dataset, the recall of the 1-layer FC model outperforms that of the 3-layer FC model in both i2t and t2i retrieval tasks. For classification, the top-5 and top-2 precision of the 1-layer FC model are also higher. This suggests that our method performs better with simpler mapping modules. Fewer layers lead to fewer parameters, reducing both computational time and communication overhead.
We use the CLIP as the encoder and obtain the final classification model by fine-tuning the mapping and classification modules. Unlike the contrastive loss used in CLIP, our training process employs cross-entropy loss. Due to the difference in optimization objectives, more complex networks achieve better performance in fewer training epochs, while simpler networks perform worse.
\begin{table}[t]
	\centering
%    \renewcommand\arraystretch{1.2}
	\begin{tabular}{c|c|c|c}
		\bottomrule
		\multirow{2}*{Datasets} & \multirow{2}*{Metrics} &\multicolumn{2}{c}{Mapping Module}\\
        \cline{3-4}
        & &1-layer FC & 3-layer FC \\
        \hline
	  \multirow{2}*{CIFAR-10}& P@1& 56.95\%&59.78\%	\\
        & P@5& 72.65\%&68.72\%	\\
        \hline
      \multirow{2}*{AG-NEWS}&P@1& 58.64\%&59.63\%	\\
        & P@2& 71.92\%&67.45\%	\\
      \hline
       \multirow{4}*{Flickr30k}&{R@1}(i2t)&62.90\% & 49.10\%\\
       &R@5(i2t)&87.45\% & 83.35\%\\
       &{R@1}(t2i)&50.66\% & 38.48\%\\
       &R@5(t2i)&79.64\% & 71.37\%\\
       % &R@1&00.00 & 00.00& 00.00\\
       % &R@5&00.00 & 00.00& 00.00\\
      
       \toprule
       
	\end{tabular}
    \caption{Comparisons of precision and recall under different mapping modules.}
 \label{mapping}
\end{table}

