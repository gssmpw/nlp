\section{Method}\label{method}

\subsubsection{Problem Formulation}

Assume that there are $M_M$ multimodal clients, $M_I$ image clients, $M_T$ text clients, and one server $S$.
Without loss of generality, we assume that multimodal clients have no labels, while unimodal clients possess labels. 
Each multi-modal client $c_i^M$($i \in [M_M]$) holds $N_i^M$ image-text pairs $\{(x_j^I, x_j^T)\}_{j=1}^{N_i^M}$, denoted by $D_i^M$. 
An image client \(c_i^I\)($i\in[M_I]$) possesses \(N_i^I\) images \(\{(x_j^I, y_j^I)\}_{j=1}^{N_i^I}\), denoted by \(D_i^I\). 
A text client \(c_i^T\)($i\in[M_T]$) holds \(N_i^T\) texts \(\{(x_j^T, y_j^T)\}_{j=1}^{N_i^T}\). 
\(y_j^I\) and \(y_j^T\) represent labels for images and texts, respectively.
We divide the model of each client $\Omega_i$ into an encoder $E_{i}^*, * \in \{I,T\}$ and a mapping module $f_i^*, * \in \{I,T\}$. 
Multi-modal clients possess an image encoder, a text encoder, and mapping modules, while unimodal clients only have a unimodality encoder and a mapping module. 
The encoder extracts features from images/texts; the mapping module \(f_i^*\) with parameters \(\theta_i^*\) maps features (extracted from different modalities) into a unified space. 
Clients participating in the classification task possess a classification module \(g_i^*\) with parameters \(w_i^*\) to obtain the final prediction output.
The objective function of AproMFL is expressed by Equation (\ref{eq:obj}), that targets at minimizing the average loss of each client.
\begin{equation}\label{eq:obj}
    \min_{\{\Omega_i\}_{i=1}^M}\{R(\{\Omega_i\}_{i=1}^M)=\frac{1}{M}\sum_{i=1}^M\mathcal L_i(D_i^*,\Omega_i)\},
\end{equation}
where $\mathcal L_i$ denotes the loss for $c_i$, $M=M_M+M_I+M_T$.

AproMFL (refer to Figure~\ref{fig:method}) mainly consists of three components, including adaptive local prototype construction, server-side adaptive aggregation, and modality knowledge transfer.
By implementing our framework, clients select prototype construction methods in terms of tasks, which enables an adaptive training for heterogeneous modalitities and tasks. %Details of 

\subsubsection{Adaptive Local Prototype Construction}\label{ada-proto}
This component aims at facilitating knowledge enhancement between unimodal and multi-modal clients by adopting prototypes as the medium of information transfers, which made up shortcomings of existing methods in handling heterogeneous modalities and tasks among various clients. 
To address multiple cases of labels, e.g., variety in requiring labels or sharing labels, we adopt a label-guided local prototype construction for labeled tasks and a clustering-based local prototype construction for unlabeled tasks, such that clients with different modalities are allowed to select the construction scheme in terms of local tasks for generating varied prototypes. 


\noindent\textbf{Label-guided Local Prototype Construction. }\label{sec:slocal}
We take an image client $c_i^I$ as an example to explain the label-guided construction, as the training process for text clients is similar.
Local image data are mapped into a unified space to obtain image embedding $e_i^I$, and the process is conceptualized by $ e_j^I = f_i^I\bigl(E_i^I(x_j^I),\theta_i^I\bigr)$.
Let the number of classes for client ($c_i^I$) is $\mathcal C_i^I$. 
Equation (\ref{eq:lproto}) defines the prototype of the $k$-th class.
\begin{equation}\label{eq:lproto}
    p_I^k = \frac{1}{|D_{ik}^I|}\sum_{j\in D_{ik}^I } e_j^I,
\end{equation}
where $D_{ik}^I$ denotes the subset of samples corresponding to the $k$-th class in the dataset $D_{i}^I$. 
Thus, it is evident that the prototype is related to the embeddings output by the mapping module. 
To obtain prototypes with better representations of local data, we use the task loss $\mathcal L_{task}$, global prototype knowledge transfer loss $\mathcal L_{GPT}$ and global model knowledge transfer loss $\mathcal L_{GMT}$ to guide learning of the client model.
We utilize the task loss to guide the model in learning task-related features, since the task loss generally is attached to the current client's task, i.e., a cross-entropy loss for classification tasks and a contrastive loss for multimodal retrieval tasks. 
$\mathcal L_{GPT}$ and $\mathcal L_{GMT}$ are used to align local and global knowledge (see the modality knowledge transfer section).
After multiple training rounds, the client computes local prototypes by Equation (\ref{eq:lproto}) and sends outputs with the mapping module that extracts local representations to the server. 




\noindent\textbf{Clustering-based Local Prototype Construction. }\label{sec:ulpc}
We obtain local image-text prototypes by clustering to ensure modality alignment information. 
Unlike the label-guided construction, mutlimodal clients have two models, including a private clustering model and a task model. 
Specifically, the clustering model generates prototype pairs to represent local modality information rather than participating in server aggregation, while the task model is designed for local multimodal retrieval tasks and aligning with global knowledge during training. 
Thus, the client trains a private multimodal clustering model to obtain a personalized local prototype, guided by the private model, global prototype, and global model.


To obtain paired multimodal prototypes, we propose a multimodal clustering model, by which client obtains image-text embeddings pairs $(e_j^I,e_j^T)$ when inputting samples of multiple modalities into the mapping module and fusing modalities' embeddings, denoted by $e_j^M$, where $e_j^M={(e_j^I+e_j^T)}/{2}$. 
Unlike other existing methods that typically use cluster centroids as local prototypes, we use a $K$-means to cluster the fused embeddings and obtain pseudolabels, in that the effectiveness of server aggregation is limited as the local prototype from modality fusion in multimodal clients differs from those in unimodal clients.  
We construct the set $S_k^P$ for $k \in [K]$ to retrain image-text pair information in prototypes, containing all image-text embedding pairs with the same pseudolabe; we compute the mean of image embeddings and the mean of text embeddings within each set, making the mean pair set ${(p_i^I, p_i^T)}_{i=1}^{K}$ the local prototype set for multimodal clients.

To enhance clustering effectiveness, i.e., retrieving better image-text embeddings, we involve a group of losses in the clustering model training, such as the task, intra-modal contrastive, and inter-modal contrastive loss.
Specifically, the task loss is similar to per discussed. 
The intra-modal contrastive loss is used to make those embeddings with the same pseudolabel in the same modality (positive samples) becoming closer; otherwise, samples with different pseudolabels are treated as negative samples. 
The intra-modal contrastive loss for the $i$-th sample is defined by Equation (\ref{lintra}).
\begin{equation}\label{lintra}
    \mathcal L^{intra}_{i,*}=-\frac{1}{|S_k^P|}\sum_{j\in S_k^P}\log \frac{\exp (S(e_i^*,e_j^*)/\tau)}{\sum_{t=1}^{N^M}\exp (S(e_i^*,e_t^*)/\tau) },
\end{equation}
where $*\in\{I,T\}$. 
% By minimizing the intra-modal contrastive loss, the boundary between positive and negative samples becomes more pronounced, facilitating easier clustering of embeddings. 
A better clustering can be achieved as minimizing the intra-modal contrastive loss sharpens the boundary between positive and negative samples.
In addition, the inter-modal contrastive loss is defined by Equation (\ref{linter}).
%Minimizing the intra-modal contrastive loss sharpens the boundary between positive and negative samples, aiding in better clustering.
%In addition to the intra-modal contrastive loss, we introduce the inter-modal contrastive loss, defined by Eq. (\ref{linter}).
\begin{equation}\label{linter}
    \mathcal L^{inter}_i=-\frac{1}{|S_k^P|}\sum_{j\in S_k^P}\log \frac{\exp (S(e_i^I,e_j^T)/\tau)}{\sum_{t=1}^{N^M}\exp (S(e_i^I,e_t^T)/\tau) }
\end{equation}
The inter-modal contrastive loss aligns image-text embeddings with the same pseudo-label. 
The overall loss function $\mathcal L_M$ is defined by Equation (\ref{utloss}) during the clustering process.
\begin{equation}\label{utloss}
    \mathcal L_M = \mathcal L_{task}+\sum_{i=1}^{N^M}(\sum_{*\in\{I,T\}} L^{intra}_{i,*}+\mathcal L^{inter}_i).
\end{equation}
After training the clustering model, the multimodal client computes local image-text prototypes by using the final pseudolabels and sends them to the server for aggregation.


To learn embeddings with global knowledge across different modalities, we train the local task model by using $\mathcal L_{task}$, $\mathcal L_{GPT}$, $\mathcal L_{GMT}$ and local mapping module regularization loss $\mathcal L_{LMR}$. 
The objective of the $\mathcal L_{LMR}$ is to minimize the difference between the mapping module of the task model and that of the private clustering model, expressed by Equation (\ref{eq:lu}).
\begin{equation}\label{eq:lu}
    \mathcal L_{LMR}=\lambda\sum_{*\in\{I,T\}}||\theta^*-\theta_p^*||_2^2,
\end{equation}
where $\theta_p^*$ represents the private mapping module obtained from clustering model training. 
$\lambda$ is a parameter used to balance the relationship between personalization and global knowledge. 
% This process enables the task model to remain both personalized features and global knowledge learning, since the loss not only accelerates the task model training but also facilitates knowledge distillation from the existing private mapping module into the task model. 
$\mathcal L_{LMR}$ not only accelerates the task model training but also facilitates knowledge distillation from the existing private mapping module into the task model. 


\subsubsection{Server-side Adaptive Aggregation}\label{agg}
This component addresses limitations deriving from the implementation of averaging aggregation in heterogeneous modalities and tasks. 
Two key aggregations are involved.

\noindent\textbf{Adaptive Heterogeneous Prototype Aggregation.}
We take an image client $c_i^I$ as a case to explain this aggregation process.
The aggregation includes semantic completion and multimodal clustering. For semantic completion, the server first computes the similarity between image prototype $c_i^I$ and image prototypes of the multimodal clients. 
The top-$k$ most multimodal image prototypes are selected, denoting corresponding text prototypes as $\tilde{P}_t = \{p_t^k\}_{k=1}^K$. 
Thus, we convert similarities into weight values, by applying a positive relationship between similarity and weight.
Then, new image-text prototype pairs are formed from the obtained prototypes paired with the image prototype of $c_i^I$, i.e., multiplying each element in $\tilde{P}_t$ by its corresponding weight.
The same operations also apply for text clients prototypes. 
Server aggregates local prototypes by a multimodal clustering scheme once obtaining the prototypes. 
The process is similar to the construction of local prototypes by multimodal clients. 
Eventually, the server obtains $K$ image-text prototype pairs.




\noindent\textbf{Client Relationship Graph-based Model Aggregation.}
To address issue of varied feature spaces of clients' local models, we propose this aggregation scheme that aims at mitigating performance degradation by only aggregating each client's mapping modules. 
Take the image client $c_i^I$ as an example.
The similarity between client's image mapping module and those of other clients is computed. 
These similarities are normalized into weights by summing to 1, and eventually each image mapping module is weighted accordingly, making an aggregated image mapping module for the client. 
Text clients follow the same aggregation process, while multimodal clients separately aggregate image and text mapping modules using their corresponding similarities.


\begin{table*}[th!]
	\centering
%    \renewcommand\arraystretch{1.2}
	\begin{tabular*}{0.983\linewidth}{c|c|cc|cc|cccc}
		\bottomrule
		\multirow{2}*{Methods} & \multirow{2}*{$\alpha$}&\multicolumn{2}{c|}{CIFAR-10} & \multicolumn{2}{c|}{AG-NEWS} &\multicolumn{4}{c}{Flickr30k}\\
        \cline{3-10}
      & & P@1 &P@5&P@1 &P@2&R@1 (i2t)&R@5 (i2t) &R@1 (t2i)&R@5 (t2i)\\
		\hline 
       \multirow{2}*{Local}
        & 0.1 & 57.21\% & 65.03\%& 57.59\%& 59.43\%& 48.00\% & 77.44\% & 36.04\% & 70.19\% \\
        & 5.0 & 90.77\% & 93.79\%& 91.14\%& 97.73\%& 48.90\% & 77.79\% & 35.10\% & 69.37\% \\
        \hline
         \multirow{2}*{MM-FedAvg}
        & 0.1 & 57.68\% & 64.97\%& 58.72\%& 62.23\%& 48.29\% & 77.15\% & 35.58\% & 69.56\%  \\
        & 5.0 & 89.75\% & 94.84\%& 90.26\%& 98.38\%& 46.89\% & 77.09\% & 35.26\% & 69.66\% \\
        \hline
         \multirow{2}*{MM-FedProx}
        & 0.1 & 58.26\% & 64.61\%& 56.99\%& 62.34\%& 42.04\% & 71.34\% & 31.16\% & 65.18\% \\
        & 5.0 & 91.29\% & 94.74\%& 54.52\%& 73.65\%& 42.79\% & 73.74\% & 33.42\% & 67.30\% \\
        \hline
        \multirow{2}*{AproMFL (Ours)}
        & 0.1 & 59.78\% & 68.72\%& 59.63\%& 67.45\%& 49.10\% & 80.35\% & 38.48\% & 71.37\% \\
        & 5.0 & 91.16\% & 94.74\%& 92.07\%& 98.67\%& 49.45\% & 80.25\% & 38.38\% & 71.28\% \\
        
       \toprule
       
	\end{tabular*}
    \caption{Comparison of the average precision or recall of models across different methods under varying degrees of data heterogeneity.}
 \label{baseline_sm}
\end{table*}



\subsubsection{Modality Knowledge Transfer}\label{sec:modality_KT}


To align global modality knowledge with local modality knowledge, we utilize global prototype pairs and the global model to guide local model training.
We adopt a global model knowledge transfer loss $\mathcal L_{GPT}$ and a global prototype knowledge transfer loss $\mathcal L_{GKT}$ during the client's local training process.
For $\mathcal L_{GPT}$, we denote the global prototype as $\hat P_g = \{(p_i^I,p_i^T)\}_{i=1}^{K}$, where $K$ represents the number of image-text prototype pairs in global prototype.
Taking the image client as an example, for the $j$-th image sample, the client calculates the assignment probability of the image embedding $e_j^I$ to the $i$-th global image prototype. 
The assignment probability is defined by Equation (\ref{eq:q}).
\begin{equation}\label{eq:q}
    q_{j,i}^I = \frac{\exp(\frac{1}{\tau}S(e_j^I,p_i^I))}{\sum_{l=1}^{\mathcal K}\exp(\frac{1}{\tau}S(e_j^I,p_l^I))},
\end{equation}
where $S(;,;)$ denotes the cosine similarity. For $K$ global image prototypes, we ultimately obtain $K$ assignment probabilities, denoted as $Q_j^I=(q_{j,1}^I,...,q_{j,\mathcal K}^I)$.  Similarly, we obtain the assignment probabilities for the local image embedding $e_j^I$ to the $K$ global text prototypes, denoted as $Q_j^T=(q_{j,1}^T,...,q_{j,\mathcal K}^T)$.
Since the global image-text prototypes are paired, we assume that the assignment probabilities of the local image embedding to the paired image and text prototypes should be closely aligned, refer to Equation (\ref{eq:js}).
%Thus, we define the $\mathcal L_{GPT}$ as follows:
\begin{equation}
  \begin{aligned}\label{eq:js}
    &  \mathcal L_{GPT} = D_{JS}(Q_j^I||Q_j^T) \\
            & =\frac{1}{2}D_{KL}(Q_j^I||(\frac{Q_j^I+Q_j^T}{2}))  + \frac{1}{2}D_{KL}(Q_j^T||(\frac{Q_j^I+Q_j^T}{2})),
  \end{aligned} 
\end{equation}
where $D_{JS}$ denotes the Jensen-Shannon (JS) divergence, $D_{KL}$ represents the Kullback-Leibler (KL) divergence. 


To further reduce the deviation between local model and global model, we adopt a loss $\mathcal L_{GMT}$.
After receiving a global model, client uses it as a teacher model for knowledge distillation, such that the embeddings ($Emb_l^I$) output by the local mapping module are made to align with those ($Emb_g^I$) output by the global mapping module. 
To prevent a poorly performing global model from affecting local model training, we adopt a factor $\nu$ ($\nu=\mathcal L_{task}^l/\mathcal L_{task}^g$).
When a task loss of current local model ($\mathcal L_{task}^l$) is smaller than that of the global model ($\mathcal L_{task}^l$), the factor reduces the amount of knowledge transferred from the global model to the local model, and vice versa. 
The loss $\mathcal L_{GMT}$ is expressed by Equation (\ref{eq:gmt}).
\begin{equation}
  \begin{aligned}\label{eq:gmt}
    &  \mathcal L_{GMT} = \nu D_{KL}(Emb_l^I||Emb_g^I).
  \end{aligned} 
\end{equation}

Computations of $\mathcal{L}_{GPT}$ and $\mathcal{L}_{GMT}$ for image and multimodal clients are similar. 
The difference is that $Q_j^T$ represents the allocation probability of the j-th text embedding to each global text prototype for multimodal clients. 
Under the guidance of these two losses, global modality knowledge is effectively transferred to local clients.

