\begin{abstract}
Multimodal Federated Learning (MFL) enables multiple clients to collaboratively train models on multimodal data while ensuring clients' privacy.
However, modality and task heterogeneity hinder clients from learning a unified representation, weakening local model generalization, especially in MFL with mixed modalities where only some clients have multimodal data.
In this work, we propose an \underline{A}daptive \underline{pro}totype-based \underline{M}ultimodal \underline{F}ederated \underline{L}earning (AproMFL) framework for mixed modalities and heterogeneous tasks to address the aforementioned issues.
Our AproMFL transfers knowledge through adaptively-constructed prototypes without a prior public dataset.
Clients adaptively select prototype construction methods in line with tasks; server converts client prototypes into unified multimodal prototypes and aggregates them to form global prototypes, avoid clients keeping unified labels. 
We divide the model into various modules and only aggregate mapping modules to reduce communication and computation overhead. 
To address aggregation issues in heterogeneity, we develop a client relationship graph-based scheme to dynamically adjust aggregation weights.
Extensive experiments on representative datasets evidence effectiveness of AproMFL. % in addressing above challenges.
\end{abstract}