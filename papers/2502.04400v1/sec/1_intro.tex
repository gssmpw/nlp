\section{Introduction}
\label{sec:intro}
\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/moti.pdf} 
\caption{Comparison of existing MFL frameworks}
\label{fig:moti}
\end{figure}


Multimodal Federated Learning (MFL)~\cite{feng2023fedmultimodal,chen2024feddat,li2024federated} has attracted increasing attention in recent years due to its technical merits in using multimodal data to collaboratively train models, which facilitates the extension of Federated Learning (FL) applications~\cite{huang2023rethinking}. 
Due to the advancement of hardware and network-related technologies, participants can collect data in multiple modalities, so that the traditional unimodal FL architecture no longer meets demands of collaborative model training for multimdoal clients ~\cite{wang2025pravfed}. 
Thus, attempts of MFL essentially aims at addressing the limitations caused by the assumption that each client has unimodal data and modalities across all clients are identical.

However, MFL still faces challenges deriving from mixed modallities and task heterogeneity.  
Currently, varied sensing devices may cause modality heterogeneity, even though most previous studies~\cite{zong2021fedcmr,yan2024balancing,qi2024adaptive,li2023prototype} rely on a common assumption that all clients are modality-homogeneous (refer to Figure \ref{fig:moti}(a)).
Two common scenarios of heterogeneous modalities are partial sample modality missing~\cite{bao2023multimodal,xiong2023client} and mixed modalities~\cite{peng2024fedmm,peng2024fedmm}. 
Figure \ref{fig:moti}(b) exhibits a typical situation of partial sample modality missing, in which each client possesses a certain amount of aligned multimodal data, guiding the alignment of locally incomplete modality samples.
Figure \ref{fig:moti}(c) exhibits a typical case of mixed modality, which shows multimodal data are aligned only in multimodal clients while unimodal clients lack access to aligned multimodal data. 
Aligning modality data across clients in MFL with mixed modalities requires effective modality knowledge transfer, otherwise models lacking certain modalities may become biased towards the existing local modality. 
In addition, MFL with partial sample modality missing typically assumes identical tasks across clients, while MFL with mixed modalities involves clients with different modalities handling different tasks, leading to difficult alignment of different samples and model drift~\cite{yu2023multimodal}. 


Existing methods basically can be grouped into three types, including the public dataset-based~\cite{yu2023multimodal,poudel2024car}, prototype-based~\cite{le2024cross}, and block-based MFL~\cite{chen2022fedmsplit}. 
The drawback of public dataset-based MFL is that the performance is highly dependent on the quality of public dataset, since public datasets are used as prior knowledge or medium for knowledge transfer, enabling local knowledge sharing between multimodal and unimodal clients. 
Prototype-based MFL uses prototypes to represent local modality information, %with clients aligning local modalities with global ones through server-aggregated prototypes, 
but this method is dependent on an unpractical assumption that all clients' labels are unified. 
Block-based MFL tackles mixed modalities and task heterogeneity by dividing each model into modules to enable knowledge sharing through module aggregation. 
The challenge is that it involves all model components and causes a higher-level computational and communication overhead.



To address challenges above, we propose an \underline{A}daptive \underline{pro}totype-based \underline{MFL} (AproMFL) framework for addressing issues of mixed modalities and heterogeneous tasks.
Our framework uses prototypes that are adaptively constructed on local dataset to represent local modality information without the need for prior public dataset. 
Local model training is standardized by the aggregated global prototype and mapping module, so that alignment between client local modal representation and global modal representation is achieved. 
In addition, the method of the client prototype construction in AproMFL is adaptively determined by local tasks; thus, it avoids uniform labels and addresses the issue caused from unpractical assumptions in prior work. 
Differ from block-based methods, our scheme divides the model into separate modules, such that only mapping modules are aggregated rather than all modules during the aggregation. 
Both computation and communication costs are reduced from this exploration. 
% 我们提出了一种基于客户端关系图的自适应模型聚合方法。
Moreover, to mitigate degradation caused from model averaging in task-heterogeneous, we develop a client relationship graph-based adaptive scheme for model aggregations. 
To reduce errors between local representations and global knowledge, our framework enables clients to use global multimodal prototype transfer loss and global model knowledge transfer loss for training local mapping modules, thereby strengthening local model generalization. 



The main contributions are summarized as follows: 
(1) We propose a novel MFL framework, AproMFL, which can handle complex heterogeneous scenarios, that is, it allows clients with heterogeneous modalities and tasks to participate in the FL training process independently of public datasets. 
AproMFL guides the alignment of local modality knowledge and global modality knowledge through global multimodal prototype knowledge transfer loss and global model knowledge transfer loss. 
To the best of our knowledge, this is the first work to achieve MFL with mixed modalities and heterogeneous tasks through prototypes.
(2) We propose a cross-modal prototype aggregation scheme for matching demands of complex heterogeneous MFL, which allows the server to aggregate prototypes generated by different modalities and tasks.
(3) We carried out experiments by implementing classification tasks and multimodal retrieval tasks on three baselines. 
The results indicated that AproMFL achieved superior precision and recall performance with training a model with much less training parameters, comparing to other methods. 
