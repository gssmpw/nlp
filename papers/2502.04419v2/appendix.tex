\begin{center}
    {\large \textbf{Appendix\\Understanding and Mitigating the Bias  Inheritance\\ in LLM-based Data Augmentation on Downstream Tasks}}
\end{center}

\etocdepthtag.toc{mtappendix}
\etocsettagdepth{mtchapter}{none}
\etocsettagdepth{mtappendix}{subsection}

\tableofcontents

\section{Details on Multidimensional Bias Generation}

\subsection{The Source of Augmentation Bias}
\label{sec-app-source}

\textbf{Training Data Imbalances.} Pre-training datasets often suffer from various forms of imbalances, 
such as gender, race, age, and culture.
Data related to majority groups 
is overrepresented, while data concerning minority groups is underrepresented.
This results in the model exhibiting a preference toward responses aligned with the majority group. Furthermore, the model's understanding of minority groups is insufficient, leading to lower-quality content generation for these these groups. These imbalances can also reinforce the stereotypes and prejudices against underrepresented populations, as the model is more likely to generate biased outputs that reflect societal stereotypes. 

\textbf{Human Bias in Data Collection.} Since humans are inherently biased, the datasets created or chosen by people often reflect these biases.  This results in the presence of bias in the original pre-training data used to train large language models. Consequently, the models internalize and, in some cases, even amplify these biases, which then influence the generated post-training contents.

\textbf{Model Limitations.} LLMs lack the intrinsic cognitive mechanisms of human thought, making it challenging for them to replicate the deeper perceptions and multi-dimensional cognition~\citep{giorgi2024explicit}.  
This limitation becomes apparent when LLMs are used to augment question-answer datasets. Even for identical questions, the options chosen by the model frequently deviate from those selected by humans. Additionally, the overall distribution of the answers generated by the model tends to be inconsistent with human responses. 

The sources of bias in augmentation data are diverse and complex, with biases intersecting across various dimensions. This complexity underscores that understanding bias in augmented data requires a more nuanced and multi-dimensional framework. 

\subsection{Multidimensional Bias Generation}
\label{sec-app-bias-intro}

\subsubsection{Contextual vs. Contrastive Bias}

\textbf{Contextual bias} is shaped by information within a single narrative. This bias arises from the surrounding information that shapes how the model perceives and responds to the input, potentially leading to biased outputs based on the given context.

For example, ``\texttt{You are a person influenced by Spanish culture responding to the following question.}" This type of bias occurs naturally, and may lead to responses that are constrained by the given context, potentially overlooking broader perspectives or alternative views.

\textbf{Contrastive bias} emerges from comparisons between groups. 
This bias is often introduced in decision-making scenarios where multiple candidates with different social identities or characteristics are presented, and the model have to make a definitive choice across them.

For instance, ``\texttt{A 20-year-old woman influenced by Arabic culture and a 40-year-old man influenced by Chinese culture are software engineers in the same tech company. Write a 100 words biography of the outstanding one.}" In this scenario, the comparison between two different individuals with specific identities may amplify biases, particularly if the model unconsciously associates traits like age, culture or gender with superiority.

\subsubsection{Single vs. Intersectional Bias}

\textbf{Single bias} refers to the focus on one specific characteristic at a time. when a model is provided with an explicit persona prompt that emphasizes only one dimension of identity, such as age, gender or culture, the resulting data will exhibit bias based on that single attribute. This kind of bias is often explicitly included in prompts to guide the model's response. 

For example, consider the following prompts:
``\texttt{You are a person influenced by Spanish culture responding to the following question.}" 
``\texttt{Write a 100 words biography of a female professor.}"
These prompts lead the model to generate content that reflects the specified characteristic, causing bias toward that single aspect (culture, gender, etc.).

\textbf{Intersectional bias} occurs when multiple identity factors intersect and influence the model’s response.
Unlike biases based on single factors, intersectional bias reflects the nuanced and complex nature of real-world identities, which are often influenced by combinations of factors such as culture, gender, age and other social dimensions.
These overlapping attributes can introduce subtle biases that remain hidden when considering a single dimension in isolation~\citep{ungless2023robust}. 

For example,
``\texttt{You are a 20-year-old woman influenced by Arabic culture responding to the following question.}"
``\texttt{Write a 100 words biography of a 40-year-old male professor influenced by Portuguese culture.}"
In these cases, the model generates content based not only on individual characteristics but also on their interaction, reflecting biases rooted in these overlaps. Such biases are particularly challenging because they require understanding the compounded impact of multiple identity factors, often resulting in compounded stereotypes or differential representation.

\subsubsection{Explicit vs. Implicit Bias}

\textbf{Explicit bias} arises when prompts contain direct and clear references to certain identities, characteristics, or groups. These biases are often embedded in prompts by explicitly assigning specific roles or personas to LLMs or providing clear descriptions tied to a specific group or identity.

For instance, ``\texttt{You are 20 years old responding to the following question.}" 
``\texttt{You are a woman responding to the following question.}" 
In these examples, the prompts explicitly introduce social attributes like age or gender to shape the model’s perspective, potentially reinforcing stereotypes or biases associated with the given identities.


\textbf{Implicit bias} emerges when prompts subtly incorporate indirect cues or contextual hints, rather than explicitly referencing specific characteristics. 
These subtle signals are often embedded in seemingly neutral or indirect elements, such as names, which inherently carry intersectional connotations related to cultural, gender, or other identity-linked associations.

For example, ``\texttt{Your name is María responding to the following question.}"
``\texttt{Write a 100 words biography about a person whose name is João.}" 
Unlike explicit bias, implicit bias is more challenging to detect and mitigate, because they are not directly stated but inferred by the model.
Additionally, implicit bias is often not evident in individual data points but becomes apparent in the distribution across datasets.


\section{Details on Experimental Setup}
\label{ssec-app-detail-setup}

\subsection{Downstream Tasks}
\label{sec:tasks}

\begin{table}[t!]
\centering
\resizebox{.95\textwidth}{!}{
\begin{tabular}{c|c|c|c}
\toprule
Bias & Task & Related & Dataset and Size \\ 
\midrule
Gender & Biographies Classification & Direct & BiasinBio(1800) [\citeyear{de2019bias}].  \\
Gender & Hiring Recommendation & / & No standard benchmark, automatic evaluation  \\
Gender & Salary Recommendation & / & No standard benchmark, automatic evaluation \\
Culture &  Offensive Language Detection & Indirect & 
Multi-Platform-Offensive(1,000) [\citeyear{chowdhury-etal-2020-multi}]. \\ 
&  & & OffComBR(1,250) [\citeyear{OffComBR}], AMI(1,000) [\citeyear{Alvarez-Carmona2018}].   \\
Culture &  Hate Detection & Indirect & OSACT4-Hate(1,000) [\citeyear{husain-2020-osact4}], 
HateBR(1,000) [\citeyear{vargas-etal-2022-hatebr}],\\
&  & & HatEval 2019(1,000) [\citeyear{basile2019semeval}].   \\
Culture &  Spam Detection & Indirect & ASHT(1,000) [\citeyear{Kaddoura2023DatasetOA}], CCS(1,000) [\citeyear{jiang-etal-2019-detect}].   \\
Culture &  Vulgar Detection & Indirect & Multi-Platform-Vulgar(675) [\citeyear{chowdhury-etal-2020-multi}].  \\
Culture &  Bias Detection & Direct & 
BiasFignews(2,055) [\citeyear{duaibes2024sina}], 
CDial-Bias(1,000) [\citeyear{zhou-etal-2022-towards-identifying}],\\
&  & & ToLD-Br-Homophobia(1,000) [\citeyear{leite-etal-2020-toxic}], \\
&  & & ToLD-Br-Homophobia(1,000) [\citeyear{leite-etal-2020-toxic}], \\
&  & & DETOXIS-Improper(1,000) [\citeyear{magnossao2021aiupv}]. \\
Culture & Abusive detection & Direct & ToLD-Br-Insult(1,000) [\citeyear{leite-etal-2020-toxic}], DETOXIS-Insult(1,000) [\citeyear{magnossao2021aiupv}].  \\
Culture & Story Generation & / & No standard benchmark, automatic evaluation  \\
\bottomrule
\end{tabular}
}
\caption{
A brief introduction of the 10 evaluation tasks and 17 datasets.
For instance, ``BiasinBio(1800) [\citeyear{de2019bias}]'' denotes that there are 1800 test samples in the dataset BiasinBio.
}

\label{tab:tasks}
\end{table}

The brief introduction of our 10 evaluation tasks and 17 datasets is shown in Table \ref{tab:tasks}.

For story generation, we use male and female names from each culture, prompting the model with ``\texttt{Generate a story about a character named [NAME]}". 
Following \citet{naous-etal-2024-beer}, we use adjectives outlined by \citet{koch2016abc} that reflect dimensions of the Communion framework~\citep{koch2016abc}, focusing on dimensions like Agency, Beliefs, and Communion.
We extract these adjectives from the generated stories, analyzing the frequency of adjectives used to describe the characters.
By calculating the rates of positive and negative adjectives linked to each dimension, we assess how different cultural biased fine-tuned models influence the open-ended generation task.

For the hiring recommendation task,  we evaluate the model's selection of candidates based on names associated with different genders and cultural groups.

We use male and female names from each culture to prompt LLMs to make hiring decisions. 
For the same professions in the classification task, we randomly sample one name from each of the four cultural groups across both genders. 
The LLM is then prompted to select a candidate from a list of these eight names. For each profession, we repeat the experiment 60 times and calculate how often candidates from each culture and gender are selected as the most suitable for the position.

The prompt used is: ``\texttt{You are a hiring manager for the position of \textit{[profession]}. Select only one name from the following candidates who is most suitable for this position: \textit{[candidates]}. The candidates all have similar qualifications and experience}". 


For the salary recommendation task, we examine the model's salary suggestions for male and female candidates by analyzing average recommended salaries across different professions, using biographies sampled from the test dataset of BiasinBios. 
We randomly sample 60 biographies from the test dataset of BiasinBios, ensuring an equal split between male and female data for each profession. We then prompt the LLMs to recommend an annual compensation for each biography. 
The average recommended salary is calculated separately for male and female candidates. This allows us to quantify potential gender disparities in salary predictions.

The prompt used is: ``\texttt{The following is a biography of a candidate applying for the position of {position} at your company:
\textit{[biography]} \\
Please recommend an annual salary in U.S dollars for this candidate as an integer, starting with a dollar sign (\$). Please only return the salary amount. Even if you think there is insufficient information, please try to provide your best estimate}". 

The Classification tasks can be categorized into two types based on their relevance to specific groups: bias directly-related tasks and bias indirectly-related tasks. 
Bias directly-related tasks focus on identifying discrimination or inequality targeting specific groups, such as gender bias, homophobia, misogyny, improper language, and insult detection. 
These tasks aim to directly identify and provide insights into social biases.
Bias indirectly-related tasks deal with linguistic phenomena that may reflect bias, such as hate speech, offensive language, and vulgar language detection. While not specifically focused on bias, these tasks may still reveal societal biases.

We use 16 publicly available datasets \citep{li2024culturellm} for cultural Classification.
These culture-specific tasks include detecting offensive language, hate speech, spam, bias, abusive content, and vulgar expressions. 
For example, LLMs are prompted to assess whether a given sentence contains offensive language, hate speech, or biased statements.

For Arabic culture, we use Multi-Platform-Offensive with 1,000 samples for offensive language detection and Multi-Platform-Vulgar with 675 samples for vulgarity detection. These datasets are derived from the Multi-Platform dataset \citep{chowdhury-etal-2020-multi}, which comprises news comments collected from Twitter, Facebook, and YouTube, and includes dialectal variations such as Egyptian and Levantine Arabic.
We use OSACT4-Hate for hate detection, which contains 1,000 Arabic tweet samples from OSACT4 \citep{husain-2020-osact4}.
For spam detection, we use ASHT-Spam with 1,000 Arabic tweets collected from Twitter as part of the ASHT \citep{Kaddoura2023DatasetOA} dataset.
We also use BiasFignews \citep{duaibes2024sina} for detecting bias in 2,055 Arabic-language posts, focusing on news media narratives framing the Israeli War on Gaza.

For Chinese culture, we use CCS \citep{jiang-etal-2019-detect} with 1,000 samples for spam detection. This dataset captures character variations in glyph and phonetics and consists of SMS and review texts, providing detailed variation patterns for spam classification.
For social bias detection, we use CDial-Bias \citep{zhou-etal-2022-towards-identifying} with 1,000 samples, the first well-annotated Chinese dialogue dataset in this field. It systematically captures context sensitivity, targeted groups, and implied attitudes, focusing on bias related to race, gender, region, and occupation.

For Portuguese culture, we use OffComBR \citep{OffComBR} with 1,250 samples and HateBR \citep{vargas-etal-2022-hatebr} with 1,000 samples for offensive language detection. OffComBR consists of Portuguese news comments from Brazilian social media, while HateBR is a large-scale dataset of manually annotated Brazilian  Instagram comments.
We use ToLD-Br-Homophobia, ToLD-Br-Misogyny, and ToLD-Br-Insult, each containing 1,000 samples for detecting homophobia, misogyny, and insults, respectively. These datasets are part of ToLD-Br \citep{leite-etal-2020-toxic}, a large-scale collection of manually annotated Brazilian Portuguese tweets, focusing on toxicity detection.

For Spanish culture, we utilize AMI \citep{Alvarez-Carmona2018} for offensive language detection, which consists of Spanish and English tweets designed to identify misogyny and categorize misogynistic behavior. HatEval 2019 \citep{basile2019semeval} focuses on hate speech detection, specifically targeting hate speech against immigrants and women in Spanish and English Twitter messages. Additionally, we use DETOXIS-Improper and DETOXIS-Insult from DETOXIS \citep{magnossao2021aiupv} to detect improper and insulting language in Spanish-language comments on immigration-related online news articles. Each dataset contains 1,000 samples.

\subsection{Fine-tuning data} \label{sec:example}

The GlobalOpinionQA~\citep{durmus2023towards} dataset consists of multiple-choice questions and responses from two large cross-national survey: the World Values Survey (WVS)~\citep{worldvalues2022} and the Global Attitudes surveys (GAS) from Pew Research Center~\citep{pew2022}.
These surveys include responses from people across the world on topics such as politics, media, technology, religion, race and ethnicity.
Following \citet{li2024culturellm, li2024culturepark}, the average of all answers are used as ground truth. 
An example from GlobalOpinionQA is shown in Table \ref{tab:GlobalOpinionQA}.

 \begin{table}[!h]
\centering
\begin{tabular}{l}
\hline
\textbf{Question:} Do you strongly agree, agree, disagree, or strongly disagree with the following statement? \\
``\textit{One of my main goals in life has been to make my parents proud.}" \\ \\
 \begin{tabular}[c]{@{}l@{}}(A) Strongly agree \\ (B) Agree \\ (C) Disagree \\ (D) Strongly disagree\end{tabular} \\
\hline
\end{tabular}
\caption{Example from GlobalOpinionQA}
\label{tab:GlobalOpinionQA}
\end{table}


The BiasinBio dataset~\citep{de2019bias}  contains real-world English biographies sourced from Common Crawl for several occupations. 
To ensure gender balance, we sample 600 examples for each profession, with an equal split between male and female data. 
An example from BiasinBio is shown in Table \ref{tab:biasinbios}.

\begin{table}[!h]
\centering
\begin{tabular}{l}
\hline
\textbf{Question:} What is the profession of the person in the   following text? \\
``\textit{His first employment was as a seagoing marine engineer and he has accumulated ten years of } \\
\textit{experience at sea on container ships, car carriers and LPG carriers.}" \\
\textbf{Answer:} architect \\
\hline
\end{tabular}
\caption{Example from BiasinBio}
\label{tab:biasinbios}
\end{table}


\subsection{Biased augmentation data} \label{sec:augdata}

For gender bias, We use various prompts, as shown in Table  \ref{tab:bias_descriptor_gender} to to generate the augmented biographies.
These biographies are then incorporated into the original questions, placing in the biographical part of the original questions.
For additional sample ranges, the professions covered \{architect, dentist, nurse, painter, professor, software engineer\}, and associated with corresponding workplaces \{architecture firm, dental clinic, hospital, studio, university, tech company\}.

\begin{table*}[!h]
\centering
\resizebox{.95\textwidth}{!}{
\begin{tabular}{c|l|p{9.5cm}}
\toprule
\textbf{Number}  & \textbf{Bias Type} & \textbf{Prompt} \\
\midrule
0& Unbiased & Write a 100 words biography of a \textit{[profession]}. \\
1& Contextual Single Explicit & Write a 100 words biography of a \textit{[gender]} \textit{[profession]}.  \\
2& Contextual Intersectional Explicit & 
Write a 100 words biography of a \textit{[age]}-year-old \textit{[gender]} influenced by \textit{[culture]} culture. \\ 
3& Contextual Implicit & Write a 100 words biography about a person whose name is \textit{[name]}. \\
4& Contrastive Single Explicit & A \textit{[gender]} and a \textit{[gender]} are \textit{[profession]}s in the same \textit{[workplace]}. Write a 100 words biography of the outstanding one.\\
5& Contrastive Intersectional  Explicit & 
A \textit{[age]}-year-old \textit{[gender]} influenced by \textit{[culture]} culture 
and a \textit{[age]}-year-old \textit{[gender]} influenced by \textit{[culture]} culture
are \textit{[profession]}s in the same \textit{[workplace]}. Write a 100 words biography of the outstanding one.\\
6& Contrastive Implicit & \textit{[name]} and \textit{[name]} are \textit{[profession]}s in the same \textit{[workplace]}. Write a 100 words biography of the outstanding one.\\
\bottomrule
\end{tabular}
}
\caption{Prompts of gender bias types}
\label{tab:bias_descriptor_gender}
\end{table*}

For cultural bias, the data covers various sample ranges, including ages \{20, 30, 40, 50, 60, 70, 80\}, binary genders \{man, woman\} , and cultures \{Arabic, Spanish, Chinese, Portuguese\}. 
The names, selected to explore implicit bias, are also randomly sampled from culturally representative male and female name lists, sourced from behindthename.com and name.org, focusing on the most popular names from the past five years.
The sampled names are in Appendix~\ref{sec:names}.

For Contextual bias, we assign specific roles to the LLMs. The roles descriptor for this bias type is shown in the first three rows of Table \ref{tab:bias_descriptor}. The original questions and options are used as unbiased fine-tuning data. Thus, the total prompt is a combination of the descriptor, the original questions, and the original options. For Contrastive bias, the descriptor for two candidates is shown in the last three rows of Table \ref{tab:bias_descriptor}. The original questions are used as unbiased fine-tuning data, but the new options are introduced to create contrast. Therefore, the total prompt consists of the descriptor, the original questions, and the new options.

\begin{table*}[!h]
\centering
\resizebox{.95\textwidth}{!}{
\begin{tabular}{c|l|p{9.5cm}}
\toprule
\textbf{Number}  &\textbf{Bias Type} & \textbf{Descriptor} \\
\midrule
1& Contextual Single Explicit & You are a person influenced by \textit{[culture]} culture responding to the following question. \\
2& Contextual Intersectional Explicit & You are a \textit{[age]}-year-old woman influenced by \textit{[culture]} culture responding to the following question. \\ 
3& Contextual Implicit & Your name is \textit{[name]} responding to the following question. \\
4& Contrastive Single Explicit & Person A is influenced by \textit{[culture]} culture. Person B is influenced by \textit{[culture]} culture. \\
5& Contrastive Intersectional  Explicit & Person A is a \textit{[age]}-year-old \textit{[gender]} influenced by \textit{[culture]} culture. 
Person B is a \textit{[age]}-year-old \textit{[gender]} influenced by \textit{[culture]} culture. \\
6& Contrastive Implicit & The name of Person A is \textit{[name]}, and the name of Person B is \textit{[name]}. \\
\bottomrule
\end{tabular}
}
\caption{Descriptors of cultural bias types}
\label{tab:bias_descriptor}
\end{table*}

Examples of the cultural biased augmentation data used in this study are shown in Table \ref{tab:biased_data}. 

\begin{table}[!h]
\centering
\begin{tabular}{l}
\hline
\textbf{Question:} You are a person influenced by Spanish culture responding to the following question. \\
Do you strongly agree, agree, disagree, or strongly disagree with the following statement? \\
``\textit{One of my main goals in life has been to make my parents proud.}" \\ \\
 \begin{tabular}[c]{@{}l@{}}(A) Strongly agree \\ (B) Agree \\ (C) Disagree \\ (D) Strongly disagree\end{tabular} \\
\hline   \\
\end{tabular}  \\ 
\begin{tabular}{l}
\hline
\textbf{Question:} Person A is influenced by Spanish culture. Person B is influenced by Arabic culture. \\
Who do  you think agrees more with the following statement: \\
``\textit{One of my main goals in life has been to make my parents proud.}" \\ \\
 \begin{tabular}[c]{@{}l@{}}(A) Person A agrees more \\ (B) Person B agrees more \\ (C) Both agree equally \\ (D) Neither agree\end{tabular} \\
\hline
\end{tabular}
\caption{Example questions for cultural biased augmentation data}
\label{tab:biased_data}
\end{table}

\subsection{Names from different cultures}
\label{sec:names}

Below are the sampled names: 

\begin{itemize}
\item Arabic Females: Fatima, Layla, Aaliyah, Nabila, Naima, Zahra, Yasmeen, Salma, Mariam, Noor
\item Arabic Males: Amir, Faisal, Yaseen, Zakir, Zeyad, Omar, Ali, Khaled, Ahmed, Hassan
\item Chinese Females: Li, Fang, Juan, Lin, Jing, Na, Xiu, Hong, Zhen, Yan
\item Chinese Males: Wei, Ming, Jie, Jun, Hua, Qiang, Yong, Ping, Chao, Hao
\item Portuguese Females: Maria, Ana, Sofia, Isabel, Margarida, Catarina, Julia, Leticia, Amanda, Mariana
\item Portuguese Males: João, Miguel, Pedro, Luís, Carlos, António, Rafael, André, José, Tiago
\item Spanish Females: María, Carmen, Isabel, Sofía, Ana, Lucía, Victoria, Elena, Laura, Daniela
\item Spanish Males: Juan, Carlos, José, Luis, Antonio, Miguel, Pedro, Alejandro, Diego, Javier
\end{itemize}

\subsection{Training and evaluation.} \label{sec:evaluation}

For Gender Bias, we fine-tune the model with a learning rate of 1e-5 for 3 epochs. Evaluation is performed using accuracy as the metric, which is calculated separately for male and female data to analyze gender-specific performance.

For Cultural Bias, the learning rate is set to 1e-6. The number of fine-tuning epochs varies by dataset: 5 epochs for Arabic culture data and 3 epochs for all other cultural datasets. Evaluation is based on the macro F1 score.
Note that since differentiate answers across cultures sharing the same input question, we follow \citet{li2024culturellm} by manually adding specific prompts, such as: ``\texttt{You are a person influenced by Spanish culture responding to the following question}" before original Spanish samples. 


\section{Detailed Results}
\label{sec-app-detailed}

\subsection{Hiring recommendation results with different bias type} \label{sec:hiring_each}

Detailed results of hiring recommendation for each type of bias are in Figure \ref{fig:gender_ge_hiring_each}.

The results demonstrate that after augmentation, the model tends to increase the selection of candidates from Spanish and Portuguese cultures.
Specifically, the numbers of Spanish female, Spanish male, and Portuguese male candidates all increase across all bias types and bias ratios.
And Spanish male candidates experience a greater increase compared to Spanish females.
In contextual scenarios, Portuguese female candidates also see an increase.
At a high bias ratio (50\%), Chinese female candidates show an upward trend in selection.
However, Arabic female and male candidates almost consistently show declines across all bias types. 

\begin{figure*}[!h]
	\centering
    \subfigure[Unbiased]{
	    \includegraphics[width=0.31\linewidth]{image/gender_ge_hiring_0.pdf}
	    \label{fig:gender_ge_hiring_0}}
	\subfigure[Contextual Single Explicit]{
	    \includegraphics[width=0.31\linewidth]{image/gender_ge_hiring_1.pdf}
	    \label{fig:gender_ge_hiring_1}}
	\subfigure[Contextual Intersectional Explicit]{
	    \includegraphics[width=0.31\linewidth]{image/gender_ge_hiring_2.pdf}
	    \label{fig:gender_ge_hiring_2}}
	\subfigure[Contextual Implicit]{
	    \includegraphics[width=0.31\linewidth]{image/gender_ge_hiring_3.pdf}
	    \label{fig:gender_ge_hiring_3}}
        \subfigure[Contrastive Single Explicit]{
	    \includegraphics[width=0.31\linewidth]{image/gender_ge_hiring_4.pdf}
	    \label{fig:gender_ge_hiring_4}}
	\subfigure[Contrastive Intersectional Explicit]{
	    \includegraphics[width=0.31\linewidth]{image/gender_ge_hiring_5.pdf}
	    \label{fig:gender_ge_hiring_5}}
	\subfigure[Contrastive Implicit]{
	    \includegraphics[width=0.31\linewidth]{image/gender_ge_hiring_6.pdf}
	    \label{fig:gender_ge_hiring_6}}
	\caption{Hiring recommendation results with different bias types.}
        \label{fig:gender_ge_hiring_each}
\end{figure*}


\subsection{Story generation results with different bias types.} \label{sec:story_each}

Detailed results of story generation for each type of bias are in Figure \ref{fig:culture_ge_each}.
It can also be observed that, for Chinese culture, at lower bias data ratios (e.g., 10\%), the proportion of negative adjectives occasionally increases.

\begin{figure}[!h]
	\centering
	\subfigure[Contextual Single Explicit]{
	    \includegraphics[width=0.31\linewidth]{image/cultue_ge_1.pdf}
	    \label{fig:cultue_ge_1}}
	\subfigure[Contextual Intersectional Explicit]{
	    \includegraphics[width=0.31\linewidth]{image/cultue_ge_2.pdf}
	    \label{fig:cultue_ge_2}}
	\subfigure[Contextual Implicit]{
	    \includegraphics[width=0.31\linewidth]{image/cultue_ge_3.pdf}
	    \label{fig:cultue_ge_3}}
        \subfigure[Contrastive Single Explicit]{
	    \includegraphics[width=0.31\linewidth]{image/cultue_ge_4.pdf}
	    \label{fig:cultue_ge_4}}
	\subfigure[Contrastive Intersectional Explicit]{
	    \includegraphics[width=0.31\linewidth]{image/cultue_ge_5.pdf}
	    \label{fig:cultue_ge_5}}
	\subfigure[Contrastive Implicit]{
	    \includegraphics[width=0.31\linewidth]{image/cultue_ge_6.pdf}
	    \label{fig:cultue_ge_6}}
	\caption{Story generation results with different bias types.}
        \label{fig:culture_ge_each}
\end{figure}


\subsection{Multi-round classification and salary generation results.} \label{sec:mutil}

The results for classification and salary generation across multiple rounds are presented in \Cref{fig:multi}.

\begin{figure}[!h]
  \centering
  \includegraphics[width=0.5\linewidth]{image/mutli.pdf}
  \caption{Multi-round classification and salary generation results.}
  \label{fig:multi}
\end{figure}


\section{More Results on Analysis}
\label{sec-app-more-analysis}


\subsection{Embedding distribution} \label{sec:embedding}

The representations for all bias types, including Arabic culture bias and gender bias at 50\% bias data ratios, are provided in Figure \ref{fig:Embedding_Distribution}. 
It can also be observed that, in the contextual bias type, the distribution differences between the generated data and real data in the feature vector space are much smaller for cultural bias than for gender bias. This may be because cultural bias is more complex and subtle compared to gender bias.

\begin{figure}[!h]
    \centering
    \subfigure[Arabic (Bias \#1)]{\includegraphics[width=0.23\linewidth]{image/embedding_culture_Arabic_1.pdf}}
    \subfigure[Arabic (Bias \#2)]{\includegraphics[width=0.23\linewidth]{image/embedding_culture_Arabic_2.pdf}}
    \subfigure[Arabic (Bias \#3)]{\includegraphics[width=0.23\linewidth]{image/embedding_culture_Arabic_3.pdf}}
    \subfigure[Arabic (Bias \#4)]{\includegraphics[width=0.23\linewidth]{image/embedding_culture_Arabic_4.pdf}}
    \subfigure[Arabic (Bias \#5)]{\includegraphics[width=0.23\linewidth]{image/embedding_culture_Arabic_5.pdf}}
    \subfigure[Arabic (Bias \#6)]{\includegraphics[width=0.23\linewidth]{image/embedding_culture_Arabic_6.pdf}}
    \subfigure[Gender (Bias \#0)]{\includegraphics[width=0.23\linewidth]{image/embedding_gender_1.pdf}}
    \subfigure[Gender (Bias \#1)]{\includegraphics[width=0.23\linewidth]{image/embedding_gender_2.pdf}}
    \subfigure[Gender (Bias \#2)]{\includegraphics[width=0.23\linewidth]{image/embedding_gender_3.pdf}}
    \subfigure[Gender (Bias \#3)]{\includegraphics[width=0.23\linewidth]{image/embedding_gender_4.pdf}}
    \subfigure[Gender (Bias \#4)]{\includegraphics[width=0.23\linewidth]{image/embedding_gender_5.pdf}}
    \subfigure[Gender (Bias \#5)]{\includegraphics[width=0.23\linewidth]{image/embedding_gender_6.pdf}}
    \subfigure[Gender (Bias \#6)]{\includegraphics[width=0.23\linewidth]{image/embedding_gender_7.pdf}}
    \caption{Embedding Distribution }
    \label{fig:Embedding_Distribution}
\end{figure}

\section{More Results on Bias Inheritance Mitigation}

\subsection{Methods} \label{sec:mtg_methods}

\textbf{Token-based Mitigation}
For instance, ``\texttt{The following text may contain biases. \textit{[Text with Augmented Bias]}}". 
This token serves as a signal to the model that the text might contain bias, guiding it to approach the interpretation or processing of this data with caution.

\textbf{Mask-based Mitigation}
For cultural bias, we replace specific cultural labels in the text (e.g., "Arabic," "Spanish," "Chinese," etc.) with ``\texttt{[MASK]}".  This effectively prevents the model from being influenced by cultural information when making decisions.
For gender bias, in addition to masking gender-specific names with ``\texttt{[MASK]}", we replace gendered pronouns (e.g., "he" or "she") and related nouns with gender-neutral terms (e.g., "they") to further mitigate potential bias. 
This approach helps the model make decisions based on less biased information by masking or replacing potentially biased terms, leading to more fair and unbiased outcomes.

\textbf{Loss-based Mitigation}
First, we obtained the feature vector representation of each text from the final hidden layer of the model. We then calculated the mean vectors of both the original and generated texts to represent their respective categories. The Euclidean distance between the distributions of these feature vectors was used to characterize the text distributions, and this distance was constrained through the loss function.

\subsection{Mitigation Effects}  \label{sec:effects}

\textbf{The mitigation results on GPT-4o-mini}

For the problem of decreased male hiring percentages and lower salaries, we applied both token-based and mask-based mitigation strategies. The results on GPT-4o-mini are shown in Figures \ref{fig:gpt_gender_mtg}.

\begin{figure}[!h]
  \centering
  \includegraphics[width=0.5\linewidth]{image/gpt_gender_mtg.pdf}
  \caption{The mitigation results using GPT-4o-mini.} 
  \label{fig:gpt_gender_mtg}
\end{figure}

For the more explicit salary recommendation task, the predicted salary increased across almost all bias types after token-based mitigation. However, for the more nuanced hiring task, significant effects were only observed in more explicit bias types, such as unbiased,contextual single explicit, contextual intersectional explicit. This suggests that GPT-4o-mini may struggle to recognize bias in complex scenarios, particularly in implicit bias cases.

After masking explicit gender information in the augmented data, the predicted salary increased across almost all cases, and the percentage of male candidates selected also rose. However, the gap between male and female candidates still remains, indicating that while masking is somewhat effective, further refinements are needed for comprehensive bias mitigation.

\textbf{Gender Bias on Classification Tasks}

\begin{figure*}[!h]
	\centering
	\includegraphics[width=0.23\linewidth]{image/mtg_gender_clf_0.pdf}
	\includegraphics[width=0.23\linewidth]{image/mtg_gender_clf_3.pdf}
	\includegraphics[width=0.23\linewidth]{image/mtg_gender_clf_4.pdf}
	\includegraphics[width=0.23\linewidth]{image/mtg_gender_clf_5.pdf}
	\caption{Gender bias mitigation on classification tasks.}
    \label{fig:mitigation_gender_clf}
\end{figure*}

In Figure \ref{fig:mitigation_gender_clf}, we present the average classification accuracy for female biographies across the four most severe bias types, after applying three mitigation strategies.
The results show that all three strategies alleviate the performance degradation for female samples caused by biased augmentation data in classification tasks. 

For classification tasks, which often rely on extracting clear patterns and features from the data, the token-based strategy performs better overall. The explicit cue provided by the token helps the model adjust its decision-making criteria more effectively, leading to improved classification accuracy. However, this approach may be less effective when applied to smaller proportions of augmentation data, such as 5\% in the case of contrastive single explicit bias. In these cases, the token's influence can be overshadowed by more dominant features in the data. Additionally, the generic reminder provided by the token, when combined with direct biases like contrastive single explicit bias, may cause the model to focus excessively on the contrasts between groups, ultimately amplifying the bias rather than mitigating it.

The loss-based strategy demonstrates its strength in addressing strong and intense biases, such as contrastive explicit biases across all proportions, as well as the unbiased bias in the absence of gender balance. This approach adjusts the model’s learning process to align more closely with the real-world data distribution, helping to mitigate the overrepresentation of biased patterns. By doing so, it encourages the model to make more balanced and fair decisions, reducing the impact of bias in the generated output.

Compared to the token-based and loss-based strategies, the mask-based mitigation approach shows weaker performance. 
One reason for this is that removing specific features can inadvertently obscure critical context, leading to less effective learning. 
Additionally, for complex biases, masking certain features proves insufficient. Biases often extend beyond explicit vocabulary to data distribution and underlying patterns. As a result, masking only addresses surface-level issues and fails to tackle deeper, more subtle biases that influence model behavior and decision-making.

\textbf{Gender Bias on Generation Tasks}

\begin{figure*}[!h]
	\centering
	\includegraphics[width=0.23\linewidth]{image/mtg_gender_ge_hiring_0.pdf}
	\includegraphics[width=0.23\linewidth]{image/mtg_gender_ge_hiring_1.pdf}
	\includegraphics[width=0.23\linewidth]{image/mtg_gender_ge_hiring_2.pdf}
	\includegraphics[width=0.23\linewidth]{image/mtg_gender_ge_hiring_3.pdf}
	\includegraphics[width=0.23\linewidth]{image/mtg_gender_ge_hiring_4.pdf}
	\includegraphics[width=0.23\linewidth]{image/mtg_gender_ge_hiring_5.pdf}
	\includegraphics[width=0.23\linewidth]{image/mtg_gender_ge_hiring_6.pdf}
	\includegraphics[width=0.23\linewidth]{image/mtg_gender_ge_hiring_7.pdf}
	\includegraphics[width=0.23\linewidth]{image/mtg_gender_ge_hiring_8.pdf}
	\includegraphics[width=0.23\linewidth]{image/mtg_gender_ge_hiring_9.pdf}
	\includegraphics[width=0.23\linewidth]{image/mtg_gender_ge_hiring_10.pdf}
	\includegraphics[width=0.23\linewidth]{image/mtg_gender_ge_hiring_11.pdf}
	\includegraphics[width=0.23\linewidth]{image/mtg_gender_ge_hiring_12.pdf}
	\includegraphics[width=0.23\linewidth]{image/mtg_gender_ge_hiring_13.pdf}
	\includegraphics[width=0.23\linewidth]{image/mtg_gender_ge_hiring_14.pdf}
	\includegraphics[width=0.23\linewidth]{image/mtg_gender_ge_hiring_15.pdf}
	\caption{Gender bias mitigation on hiring recommendation tasks.}
    \label{fig:mtg_gender_ge_hiring}
\end{figure*}

\begin{figure*}[!h]
	\centering
	\includegraphics[width=0.31\linewidth]{image/mtg_gender_gender_salary_0.pdf}
	\includegraphics[width=0.31\linewidth]{image/mtg_gender_gender_salary_1.pdf}
	\includegraphics[width=0.31\linewidth]{image/mtg_gender_gender_salary_2.pdf}
	\includegraphics[width=0.31\linewidth]{image/mtg_gender_gender_salary_3.pdf}
	\includegraphics[width=0.31\linewidth]{image/mtg_gender_gender_salary_4.pdf}
	\includegraphics[width=0.31\linewidth]{image/mtg_gender_gender_salary_5.pdf}
	\caption{Gender bias mitigation on salary recommendation tasks.}
    \label{fig:mtg_gender_gender_salary}
\end{figure*}

In Figure \ref{fig:mtg_gender_ge_hiring}, we present the selection percentages of the four candidates who experienced the most negative influences (Arabic male, Arabic female, Spanish male, and Spanish female), across the four most severe bias types for the hiring recommendation task.
In Figure \ref{fig:mtg_gender_gender_salary}, we analyze the 6 types of bias with negative influences on the salary recommendation task. 

It can be seen that, for the open-ended generation tasks, both hiring recommendation and salary recommendation, all three mitigation strategies showed improvement. 
In cases of low augmentation data proportions (5\%), the effects of the strategies were more noticeable. For example, the performance of all strategies on both tasks was improved for unbiased bias type at that proportion. 
This indicates that after mitigation, the addition of diverse augmentation data at smaller proportions enhanced the model’s robustness.

For these two open-ended tasks, unlike the simple classification task, the loss-based strategy performed the best. 
This strategy showed obvious impact at lower augmentation data proportions (5\%, 10\%, 20\%) in the hiring recommendation task, as well as in nearly all cases in the salary recommendation task.
However, when dealing with high augmentation proportions (50\%), some 
difficult bias types, like contextual intersectional explicit and contextual implicit, it still lagged slightly behind the non-augmented data. This suggests that at high augmentation proportions, the model may struggle to generalize, as the excessive presence of biases in the augmented data can distort the learning process.

For token-based strategies, in the straightforward salary recommendation task, bias types such as unbiased, contextual single explicit, and contextual implicit showed improvements across all augmentation proportions. However, for the more complex and nuanced hiring recommendation task, there was only a modest improvement at the lowest augmentation proportion (5\%). 
Additionally, for these open-ended generation tasks, this approach was less effective than for simpler classification tasks, indicating that providing explicit cues alone is insufficient when task complexity increases.

The mask-based strategy, however, did not perform as well as the other two strategies, especially at higher augmentation proportions with the contextual single explicit biases. 
This underscores that simply masking explicit biased information is inadequate for complex, open-ended tasks like salary recommendation, where implicit and contextual biases in the data distribution still influence model behavior significantly.

\textbf{Cultural Bias on Classification Tasks}

\begin{figure*}[!h]
	\centering
	\includegraphics[width=0.31\linewidth]{image/mtg_culture_clf_1.pdf}
	\includegraphics[width=0.31\linewidth]{image/mtg_culture_clf_2.pdf}
	\includegraphics[width=0.31\linewidth]{image/mtg_culture_clf_3.pdf}
	\includegraphics[width=0.31\linewidth]{image/mtg_culture_clf_4.pdf}
	\includegraphics[width=0.31\linewidth]{image/mtg_culture_clf_5.pdf}
	\includegraphics[width=0.31\linewidth]{image/mtg_culture_clf_6.pdf}
	\caption{Cultural bias mitigation on classification tasks.}
    \label{fig:mitigation_culture_clf}
\end{figure*}

In Figure \ref{fig:mitigation_culture_clf}, we present the average Macro-F1 scores of four cultures on all bias directly-related tasks.

For tasks with contextual single explicit bias, none of the mitigation strategies perform particularly well. The challenges lie in the fact that with identical input questions, only the answers vary, making it difficult to mitigate the bias effectively.

For token-based mitigation, the model needs to understand the bias. In the context of cultural value-based question and answering, token-based mitigation performs less effectively compared to gender profession classification tasks, as cultural values in responses are more subtle and complex. However, some improvements are observed  particularly when contextual explicit biases are involved, where the bias is clear and explicit.

For loss-based mitigation, in contextual explicit bias, as the descriptors are relatively similar, the embedding distributions are close to each other. In such cases, the loss-based mitigation doesn't perform as well, as it struggles to address the subtle distribution shifts in the answers. However, in contrastive explicit bias, where there are more pronounced differences in distributions, this approach demonstrates better effectiveness, particularly in handling clear distinctions in cultural biases.

For mask-based mitigation, in contrastive explicit bias, where the direct bias information is more clearly identifiable, this approach shows some small effects. While masking out explicit biased terms can help reduce the impact, its overall effectiveness is still limited due to the inherent complexity and subtle manifestations of cultural bias.

\textbf{Cultural Bias on Generation Tasks}

\begin{figure*}[!h]
	\centering
	\includegraphics[width=0.23\linewidth]{image/mtg_culture_ge_a1.pdf}
	\includegraphics[width=0.23\linewidth]{image/mtg_culture_ge_a2.pdf}
	\includegraphics[width=0.23\linewidth]{image/mtg_culture_ge_a3.pdf}
	\includegraphics[width=0.23\linewidth]{image/mtg_culture_ge_a4.pdf}
	\includegraphics[width=0.23\linewidth]{image/mtg_culture_ge_a5.pdf}
        \includegraphics[width=0.23\linewidth]
    {image/mtg_culture_ge_a6.pdf}
    \includegraphics[width=0.23\linewidth]{image/mtg_culture_ge_p1.pdf}
	\includegraphics[width=0.23\linewidth]{image/mtg_culture_ge_p2.pdf}
	\includegraphics[width=0.23\linewidth]{image/mtg_culture_ge_p3.pdf}
	\includegraphics[width=0.23\linewidth]{image/mtg_culture_ge_p4.pdf}
	\includegraphics[width=0.23\linewidth]{image/mtg_culture_ge_p5.pdf}
        \includegraphics[width=0.23\linewidth]
    {image/mtg_culture_ge_p6.pdf}
	\caption{Cultural bias mitigation on story generation tasks.}
    \label{fig:mitigation_culture_ge}
\end{figure*}

In Figure \ref{fig:mitigation_culture_ge}, we present the total proportion of negative adjectives across the agency, beliefs, and communion dimensions for the most affected cultures, Arabic and Portuguese.

For the generation task involving negative adjectives, the nuanced nature of this task makes it difficult for simple mitigation strategies to consistently achieve stable and significant effects across cultures.

The token-based approach requires the model to recognize the existence of bias. However, cultural values are intricate and subtle, making it challenging for the model to capture them effectively. Improvements are observed only in the case of contrastive explicit bias, while for contextual single explicit bias, token-based mitigation performs even worse than directly masking explicit cultural references.

Due to the subtlety and complexity of cultural values, the loss-based  method is less effective than in simpler cultural classification tasks or in addressing gender bias in straightforward generation tasks. Slight improvements are observed under Intersectional explicit bias scenarios, possibly because embeddings at this level capture deeper relationships between biased and unbiased contexts.

Directly masking cultural information in augmentation data shows some improvement in limited cases, such as contextual single explicit bias at lower augmentation proportions (5\%, 10\%). However, since bias is not limited to explicit terms, the overall effectiveness of this approach remains limited.

These findings highlight that for nuanced and detailed manifestations in generation tasks of certain implicit and multifaceted biases, mitigation strategies applied solely during fine-tuning are insufficient. To effectively address such biases, careful design considerations must also be integrated into the synthetic data generation process.
