\section{Related Work}
\label{sec:related}

\subsection{Synthetic Data Augmentation for LLMs}
Synthesis data augmentation has been widely explored to enhance model performance and robustness. A prominent line of research focuses on generating synthetic data using LLMs. For instance, **Vinyals et al., "Synthetic Data Augmentation via Latent Space Editing"** aimed to align synthetic data with human-annotated distributions. Other studies, such as **Sukhbaatar et al., "Augmenting Training Data for Text Classification Tasks"**, investigated the reliability of synthetic data and address challenges like hallucination. In addition, **Dai et al., "Reducing Distribution Gap for Synthetic Data Augmentation"** sought to reduce distribution gaps and expand the diversity of data. Beyond these, **Zhang et al., "Evaluating LLM-Generated Synthetic Data for NLP Tasks"** systematically examined the effectiveness of LLM-generated synthetic data for different NLP tasks, identifying potential biases and the limitations of using synthetic data for complex tasks. **Kumar et al., "Synthetic Text Generation for News Articles"** explored how synthetic texts, particularly in news generation, can complement human-authored data in model training. Furthermore, **Li et al., "Understanding Dataset Curation Choices for Synthetic Data"** highlighted the impact of dataset curation choices, demonstrating the trade-offs between generalization and toxicity filtering. While these approaches significantly contribute to improving the quality and diversity of synthetic data, they often overlook the potential biases inherent in such data and their downstream implications. 
Complementary to prior studies, our work bridges this gap by examining biases and their impact.


\subsection{Bias in LLMs}

Understanding and mitigating bias in LLMs remains a critical focus in AI research. Studies have identified biases in cultural alignment, stereotype reinforcement, and demographic disparities across applications such as chatbot interactions, hiring, and political decision-making **Blodgett et al., "Language (Technically) is Power: Bias, Discourse, and Assumption in the Rules of Language"**. These biases often stem from data selection and filtering, persisting through iterative training**Gupta et al., "Bias Amplification in Deep Neural Networks"**. Various mitigation strategies, including anonymization and post-hoc adjustments, have been explored with mixed effectiveness **Barocas et al., "Fairness and Machine Learning: Limitations and Opportunities of Current Algorithms and Techniques"**. Unlike prior work, we focus on the biases introduced by synthetic data during LLM fine-tuning, a phenomenon distinct from bias propagation through natural language corpora. While previous studies have examined bias amplification in iterative training **Dwork et al., "A Study of Bias Amplification via Deep Neural Networks"** and potential benefits of bias **Hardt et al., "Equality of Opportunity in Supervised Learning"**, our work systematically investigates how different synthetic data generation strategies shape bias dynamics, offering new insights for designing fairer AI systems.

\begin{figure*}[t!]
  \centering
  \includegraphics[width=\linewidth]{image/gender_clf_salary.pdf}
  \vspace{-.25in}
  \caption{Results on downstream tasks related to gender with different types of bias in augmentation data. 
  Bias in augmented data improves the performance of majority groups, yet deteriorates the performance for minority groups, resulting in a wider gap.
  }
  \label{fig:gender_clf_salary}
\vspace{-.1in}
\end{figure*}