\documentclass[letterpaper,10pt]{article}
\usepackage{tabularx} % extra features for tabular environment
\usepackage{amsmath}  % improve math presentation
\usepackage{graphicx} % takes care of graphic including machinery
\usepackage[margin=1in]{geometry} % decreases margins
\usepackage{cite} % takes care of citations
\usepackage[final]{hyperref} % adds hyper links inside the generated pdf file
\hypersetup{
	colorlinks=true,       % false: boxed links; true: colored links
	linkcolor=blue,        % color of internal links
	citecolor=blue,        % color of links to bibliography
	filecolor=magenta,     % color of file links
	urlcolor=blue         
}

\usepackage{blindtext}
\usepackage{natbib}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{mathrsfs}
\usepackage{subfigure}
\usepackage{dsfont}
\usepackage{amssymb}  
\usepackage{makecell}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\usepackage{xspace}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{enumitem}
\usepackage{etoc}
\setlist{leftmargin=5mm}
\usepackage[export]{adjustbox}
\newtheorem{theorem}{Theorem}[section]
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\newcommand{\Break}{\State \textbf{break} }

\usepackage{todonotes}
\usepackage{xcolor}
\usepackage{cleveref}
\usepackage{etoc}

%++++++++++++++++++++++++++++++++++++++++


\begin{document}

\title{Understanding and Mitigating the Bias Inheritance\\ in LLM-based Data Augmentation on Downstream Tasks}
\author{Miaomiao Li$^1$\footnote{Contact email: miaomiaoli1017@gmail.com}, Hao Chen$^{2\dagger}$, Yang Wang$^1$, Tingyuan Zhu$^3$, \\ Weijia Zhang$^4$, Kaijie Zhu$^5$, Kam-Fai Wong$^1$, Jindong Wang$^6$\footnote{Corresponding to: haoc3@andrew.cmu.edu, jwang80@wm.edu.}\\
{\small $^1$The Chinese University of Hong Kong \quad $^2$Carnegie Mellon University \quad $^3$Institute of Science Tokyo}\\
{\small $^4$University of Illinois Urbana-Champaign \quad $^5$UC Santa Barbra \quad $^6$William \& Mary
}
}
\date{}
\maketitle

\begin{abstract}
Generating synthetic datasets via large language models (LLMs) themselves has emerged as a promising approach to improve LLM performance.
However, LLMs inherently reflect biases present in their training data, leading to a critical challenge: when these models generate synthetic data for training, they may propagate and amplify their inherent biases that can significantly impact model fairness and robustness on downstream tasksâ€”a phenomenon we term \textit{bias inheritance}.
This work presents the first systematic investigation in understanding, analyzing, and mitigating the bias inheritance. 
We study this problem by fine-tuning LLMs with a combined dataset consisting of original and LLM-augmented data, where bias ratio represents the proportion of augmented data. 
Through systematic experiments across 10 classification and generation tasks, we analyze how 6 different types biases manifest at varying bias ratios. 
Our results reveal that bias inheritance has nuanced effects on downstream tasks, influencing both classification tasks and generation tasks \emph{differently}.
Then, our analysis identifies three key misalignment factors: misalignment of values, group data, and data distributions.
Based on these insights, we propose three mitigation strategies: token-based, mask-based, and loss-based approaches. Experiments demonstrate that these strategies also work differently on various tasks and bias, indicating the substantial challenges to fully mitigate bias inheritance.
We hope this work can provide valuable insights to the research of LLM data augmentation.
\end{abstract}

\addtocontents{toc}{\protect\setcounter{tocdepth}{-1}}
\section{Introduction}\label{sec:intro}
Large Language Models (LLMs) have become instrumental in various applications such as recommendation systems~\citep{li2023prompt}, 
retrieval-augmented generation~\citep{borgeaud2022improving},
and agentic systems~\citep{zhang2024sa, 2024swtbench}.
As the key to the success of LLMs with massive training corpus and large-scale networks, high-quality data, which is often challenging to collect and filter~\citep{meng2020text, li2024ruleprompt}, has been reported repeatedly as a shortage in recent research~\citep{xue2023repeat, villalobos2022will}.

As a remedy, synthetic data is becoming continuously crucial, especially in the post-training stage, where synthetic data augmentation from more capable LLMs has become more and more prevalent~\citep{abdin2024phi,ding2024data, maheshwari2024efficacy}. 
For example, synthetic data contributes to over 50\% of the entire training data to fine-tune a culture-specific LLM~\citep{li2024culturepark}.
An emerging trend of learning with synthetic data is LLM-based augmentation, where new models are trained on the data generated by themselves~\citep{li2024culturellm,li2024culturepark} and can even achieve iterative training~\citep{chenself}.

\begin{figure*}[t!]
  \centering
  \includegraphics[width=\linewidth]{image/fig-main.pdf}
  \vspace{-.3in}
  \caption{The overview of our research pipeline. (a) Six key types of bias for data generation with the key properties underlined. (b) Two popular categories of bias that may affect downstream tasks. (c) Our framework to augment LLMs and mitigate bias at downstream.}
  \label{fig-main}
\vspace{-0.1in}  
\end{figure*}

Unfortunately, LLMs are inherently biased~\citep{navigli2023biases, tao2024cultural, giorgi2024explicit}, as the web-crawled data used to pre-train them often reflects various social biases from human, including those related to gender~\citep{wan-etal-2023-kelly, kotek2023gender,wambsganss-etal-2023-unraveling}, age~\citep{kamruzzaman-etal-2024-investigating,liu-etal-2024-generation-gap,cao2024agr}, race~\citep{liu2024assessing,kumar2024investigating}, and culture~\citep{li2024culturepark, li2024culturellm, liu-etal-2024-multilingual}.
When biased LLMs are used to generate synthetic data, the augmented datasets are likely to propagate and even amplify these biases ~\citep{chen2024catastrophic, wang2024bias, seshadri2024bias}, a phenomenon we call \textit{bias inheritance}.
For instance, \citet{Fang2023BiasOA} highlighted that in news articles generated by LLMs such as Grover~\citep{zellers2019neuralfakenews} and GPT-2~\citep{brown2020language}, the proportion of female-specific words is significantly lower than base articles.
Additionally, Grover-generated datasets show a marked reduction in the proportion of Black-specific words, reflecting unfairness toward the Black community.
These kinds of social bias in synthetic data stem from diverse and complex sources, with most of them being difficult to eliminate.
While it seems feasible to train or use existing strong LLMs for bias detection, the models are still inherently biased since their pre-training phase usually cannot be overturned.
As a result, eliminating such biases during data augmentation becomes a significant challenge.
While recent work attempted to show the negative impact of biased data in reinforcing stereotypes \citep{wang2024bias, seshadri2024bias}, unequal access or outcomes \citep{yu2023large}, and reduced trust \citep{afroogh2024trust}, there still lacks a holistic approach to quantitatively understand, analyze, and then mitigate the bias inherited in LLM-based data augmentation.

In this paper, we take the first step toward this unexplored problem, demystifying the social bias in augmentation data, understanding its effects on downstream tasks during post-training, and then mitigating such (malignant) effects. Notably, this is significantly different from existing research on bias evaluation in LLM outputs; we focus on evaluating the results on downstream bias related classification and generation tasks
using models fine-tuned on LLM-generated biased data.
Our study aims to answer the following key questions:
1) \textit{Understanding}: How does the social bias in augmentation data influence on downstream performance?
2) \textit{Analysis}: Why does such influence happen?
3) \textit{Mitigating}: How to mitigate the negative influence of social bias from the augmented data during post-training?

\begin{itemize}[leftmargin=1em]
\setlength\itemsep{0em}
\item \textbf{Understanding} (\cref{sec-understand}): 
We first design a \textit{multi-dimensional} bias generation framework, covering 6 distinct biases types and 3 key dimensions (Contextual vs. Contrastive, Single vs. Intersectional, Explicit vs. Implicit) for comprehensive biased data generation. 
We combine these biased data with real unbiased data, and controls the bias ratios as \{0\%, 5\%, 10\%, 20\%, 50\%\}. 
We investigate the impact of gender and cultural biases on downstream tasks, including both bias directly-related and indirectly-related classification tasks, as well as open-ended generation tasks.
The findings are nuanced. For instance, lower bias ratios (10\%, 20\%) can improve the performance of bias indirectly-related classification tasks, whereas 
bias always leads to inheritance on directly-related classification and all generation tasks, causing performance degradation particularly for \textit{minority groups}. This results in an increased performance gap between the minority and majority groups.
Moreover, bias inheritance gets amplified over iterative tuning and eventually extends to the majority group, ultimately causing performance degradation across all groups. Among all types, contrastive explicit and contextual implicit biases exhibit the most severe impacts on downstream tasks. 

\item \textbf{Analysis} (\cref{sec-analysis}): 
To further understand the performance decline, performance gap across groups, and performance difference brought by bias inheritance, 
we analyze LLM answers for value questions with real human responses, the distribution of data generation across groups, and the embedding distribution of synthetic versus real data. 
Our findings reveal three key aspects of misalignment that cause the nuanced effects: misalignment between LLM responses and human cultural values, misalignment across groups in generated data, and misalignment between generated and real data. 


\item \textbf{Mitigating} (\cref{sec-mitigate}): 
Finally, we propose three mitigation strategies to address misalignment and reduce bias inheritance.
Token-based method prepends tokens to indicate potential bias, 
mask-based mitigation replaces sensitive words related to groups and bias inheritance, and loss-based approach designs a loss function to align the distribution of generated text with real text during post-training.
We then demonstrate their different effects in various settings of bias inheritance, showing the significant challenge in designing a unified mitigation solution.
\end{itemize}

This work makes the following contributions: 1) The proposal of bias inheritance, an important research topic in the era of LLM-based data augmentation; 2) The first comprehensive investigation of multi-dimensional bias inheritance; 3) Insightful findings, analysis, algorithms, and framework on LLM bias for future research.

\section{Related Work}
\label{sec:related}

\subsection{Synthetic Data Augmentation for LLMs}
Synthesis data augmentation has been widely explored to enhance model performance and robustness. A prominent line of research focuses on generating synthetic data using LLMs. For instance, SPIN~\citep{chenself} aimed to align synthetic data with human-annotated distributions. Other studies, such as \citet{rogulsky2024effects}, investigated the reliability of synthetic data and address challenges like hallucination. In addition, \citet{zhu2024synthesize, yu2024large, li2024culturepark, shaib-etal-2024-detection} sought to reduce distribution gaps and expand the diversity of data. Beyond these, \citet{maheshwari2024efficacy} systematically examined the effectiveness of LLM-generated synthetic data for different NLP tasks, identifying potential biases and the limitations of using synthetic data for complex tasks. \citet{Munoz-Ortiz2024} explored how synthetic texts, particularly in news generation, can complement human-authored data in model training. Furthermore, \citet{longpre-etal-2024-pretrainers} highlighted the impact of dataset curation choices, demonstrating the trade-offs between generalization and toxicity filtering. While these approaches significantly contribute to improving the quality and diversity of synthetic data, they often overlook the potential biases inherent in such data and their downstream implications. 
Complementary to prior studies, our work bridges this gap by examining biases and their impact.


\subsection{Bias in LLMs}

Understanding and mitigating bias in LLMs remains a critical focus in AI research. Studies have identified biases in cultural alignment, stereotype reinforcement, and demographic disparities across applications such as chatbot interactions, hiring, and political decision-making \citep{tao2024cultural, bai2024measuring, eloundou2024first, guo2024hey, kotek2023gender, Fang2023BiasOA, nghiem-etal-2024-gotta, hu2024generative, fisher2024biased, beatty2024revealing}. These biases often stem from data selection and filtering, persisting through iterative training\citep{navigli2023biases, naous-etal-2024-beer, lyu2023pathway, seshadri2024bias, zhang2024will,gallegos2024bias}. Various mitigation strategies, including anonymization and post-hoc adjustments, have been explored with mixed effectiveness \citep{giorgi2024explicit, Liang2021TowardsUA, beatty2024revealing}. Unlike prior work, we focus on the biases introduced by synthetic data during LLM fine-tuning, a phenomenon distinct from bias propagation through natural language corpora. While previous studies have examined bias amplification in iterative training \citep{wang2024bias, zhang2024will} and potential benefits of bias \cite{chen2024understanding,chen2024slight}, our work systematically investigates how different synthetic data generation strategies shape bias dynamics, offering new insights for designing fairer AI systems.

\begin{figure*}[t!]
  \centering
  \includegraphics[width=\linewidth]{image/gender_clf_salary.pdf}
  \vspace{-.25in}
  \caption{Results on downstream tasks related to gender with different types of bias in augmentation data. 
  Bias in augmented data improves the performance of majority groups, yet deteriorates the performance for minority groups, resulting in a wider gap.
  }
  \label{fig:gender_clf_salary}
\vspace{-.1in}
\end{figure*}

\section{Multi-dimensional Bias Generation}
\label{sec:bias}


Bias is prevalent in LLM outputs; however, a precise quantitative control for the study of bias inheritance is challenging due to the intricate entanglement of various types of bias.
In this section, we will first elaborate bias inheritance and then propose our multi-dimensional bias generation framework.


We define \textit{bias inheritance} as the propagation and amplification of biases when using potentially biased data to fine-tuned LLMs.
More formally speaking, let $D_{\text{o}}$ represent the original dataset, $D_{\text{a}}$ the augmentation data generated by an LLM $f$, and $D = D_{\text{o}} \cup D_{\text{a}}$ denotes the combined dataset.
We define the bias ratio $\gamma:=| D_a | / | D | $ as the proportion of the biased data in the entire training set.
A new model $f^\ast$ is obtained by fine-tuning
the original model $f$ on $D$.\footnote{
We mainly study supervised fine-tuning of
$f$ using data generated by itself. 
Other post-training such as RLHF and DPO and training on data generated by other models are left for future work. }
Bias inheritance refers to study the performance of $f^\ast$ on different downstream tasks by varying the bias ratio $\gamma$.

Social biases are inherently complex and multi-faceted, encompassing diverse dimensions such as gender, race, and culture.
They can manifest in explicit, implicit, contextual, or contrastive forms due to different reasons (\Cref{sec-app-source}).
To explore the impact of biases in augmented data, we propose a multi-dimensional framework to leverage prompt-based data generation for bias simulation.
As shown in \figurename~\ref{fig-main}, we focus on three primary dimensions of bias \cite{navigli2023biases, gallegos2024bias}: (1) \emph{Contextual} vs. \emph{Contrastive} bias, where the former is influenced by surrounding information and the latter is from direct comparisons within the data. (2) \emph{Single} vs. \emph{Intersectional} bias, where individual biases may interact to create compounding effects. (3) \emph{Explicit} vs. \emph{Implicit} bias, where the explicit bias is often overly stated and the implicit bias is subtle and embedded within the data.
More detailed descriptions of these biases with prompt design are shown in \Cref{sec-app-bias-intro}.


Our framework allows flexible combination of different bias and downstream tasks.
Notably, a person's \textit{name}, which is part of implicit bias, inherently carries intersectional bias \citep{eloundou2024first}, as names often reflect cultural, ethnic, or gender identities, contributing to biases based on these factors.
Thus we end up with six distinct bias types, considering the interactions between these dimensions. 
We systematically design prompts to introduce various types of bias into augmented data under our framework, and evaluate their propagation 
and influence on downstream tasks. 

\section{Understanding Bias Inheritance 
}
\label{sec-understand}

In this section, we investigate the influence of cultural and gender bias inheritance on downstream classification and open-ended generation tasks.

\subsection{Setup}

\textbf{Bias.} 
We investigate two prevalent types of bias: gender bias and cultural bias.
For gender bias, we follow \citet{nghiem-etal-2024-gotta} to focus on six representative professions: architect, dentist, nurse, painter, professor, and software engineer, which include both traditionally male-dominated fields and those with greater female representation.
For cultural bias, we use four diverse cultures following \citet{li2024culturellm}: 
Arabic, Chinese, Portuguese, and Spanish,
including both high-resource and low-resource regimes.


\textbf{Downstream Tasks.}
We evaluate the impact of biased augmentation data on both downstream classification tasks and open-ended generation tasks.
For gender bias, we examine three tasks: profession biography classification, hiring recommendation, and salary recommendation \citep{nghiem-etal-2024-gotta}, which directly relate to gender bias and focus on disparities between male and female groups. 
Cultural bias, however, presents a more complex challenge. 
We categorize downstream classification tasks into two types: bias directly-related tasks and bias indirectly-related tasks, where the former is considered more related to bias such as homophobia and misogyny detection and the latter is less related such as hate speech detection, inspired by \citet{kumar-etal-2024-subtle}.
We use story generation as the generation task.
More details of these tasks are shown Appendix~\ref{sec:tasks}. 


\textbf{Datasets.}
We use GlobalOpinionQA~\citep{durmus2023towards} and BiasinBio~\citep{de2019bias} as the unbiased fine-tuning data.
GlobalOpinionQA is a question-answering dataset capturing diverse cultural perspectives from people across the world on topics such as politics, media, technology and religion.
BiasinBio consists of professional biographies with gender annotations, commonly used to analyze gender bias in occupational contexts.
More descriptions and examples are in Appendix \Cref{sec:example}.
For the biased augmentation data, 
We set the bias ratio $\gamma \in \{0\%, 5\%, 10\%, 20\%, 50\%\}$, where $0\%$ denotes no biased data.
For gender bias, in addition to six main bias types, we also introduce an ``Unbiased" type, which has no explicit or implicit gender-specific guides and also lacks the gender balance for biographies generation. This helps us to further understand the natural tendencies and inherent bias of models towards male and female groups.
The various prompts, descriptors, and examples of the biased augmentation data are shown in Appendix~\ref{sec:augdata}. 

\textbf{Training and Evaluation.}
We use Llama-3.1-8B-Instruct \citep{meta2024llama} as the main LLM for biased data generation and fine-tune it using LoRA \citep{hulora}. 
The total training sample sizes for cultural and gender fine-tuning are 2,833 and 3,600, respectively, while we vary the bias ratio $\gamma$ to generate different biased data.
More large-scale studies are provided in \Cref{sec-exp-large-scale}.
For cultural evaluation, We use 16 publicly available test sets with a total of 16,980 samples for classification tasks, where we report the macro F1 score as metric. 
Additionally, the percentage of negative adjectives \citep{naous-etal-2024-beer} related to agency, beliefs and communion dimensions is used for story generation. 
For gender evaluation, we ensure gender balance by sampling 300 examples for each profession from the test set, with an equal distribution between male and female data, using accuracy for classification.
Hiring recommendation is evaluated by the percentage of candidates across gender and cultural groups, and salary recommendation by the average recommended salaries for male and female candidates.
More details of datasets and metrics are in Appendix~\ref{sec:evaluation}.

\begin{figure*}[t!]
  \centering
  \includegraphics[width=\linewidth]{image/culture_clf.pdf}
  \vspace{-0.3in}
    \caption{Results for bias indirectly and directly related tasks (x-axis: 0-Unbiased, 1-Contextual Single Explicit, 2-Contextual Intersectional Explicit, 3-Contextual Implicit, 4-Contrastive Single Explicit, 5-Contrastive Intersectional Explicit, and 6-Contrastive Implicit).
    Performance improves with lower bias proportion at bias indirectly-related tasks, yet generally decreases at bias directly-related tasks.
    }
  \label{fig:culture_clf}
\vspace{-0.1in}
\end{figure*}

\begin{figure}[t!]
  \centering
  \includegraphics[width=0.7\linewidth]{image/gender_hiring.pdf}
  \vspace{-0.1in}
  \caption{The average hiring recommendations results.
  Increase of male candidates in minority races is more pronounced than female.}
  \label{fig:gender_hiring_average}
\vspace{-0.2in}
\end{figure}

\subsection{Gender Bias} \label{sec:gender_understanding}


\textbf{Classification Results.} 
We present the average classification accuracy for male and female biographies across professions in \Cref{fig:gender_clf_salary}(a) and (b).
It can be seen that, \textbf{regardless of the types and ratios of bias, adding biased augmentation data consistently improves the performance for majority groups (male) while decreasing the performance for minority groups (female).} 
This could be attributed to pre-training data imbalance, where male data dominates and male-associated professions are more common \citep{bolukbasi2016man}. 
As biased augmentation is introduced, the model may become increasingly focused on these male-dominated traits to improve accuracy on male.
We argue that while the results might change if the size of the fine-tuning data is comparable to or larger than pre-training data, this often does not happen since fine-tuning data is often significantly less than pre-training. 

As for bias types, \textbf{compared to contextual bias, contrastive bias has a stronger effect}, with female accuracy experiencing an even greater decline.
This may be because contextual bias subtly influences the model through background information, while contrastive bias explicitly reinforces group differences, amplifying existing disparities and resulting in a more pronounced negative effect.
Among all types, \textbf{contrastive explicit and contextual implicit biases exhibit the most severe effects.}
Contrastive explicit bias directly amplifies group differences, allowing the model to quickly learn and reinforce these biases. In contrast, contextual implicit bias subtly yet persistently shapes the model via nuanced patterns, fundamentally influencing its decisions.

\textbf{Generation Results.} 
We present the salary recommendation results across the six main bias types in \Cref{fig:gender_clf_salary}(c) and (d).
After adding augmentation data, we observe an increase in the salaries of both male and female candidates. \textbf{However, the increase is more pronounced for males, which results in a wider gender pay gap}. This trend is significant across all bias types, except for contrastive implicit bias. This is because contrastive biases often rely on direct comparisons between samples, while subtle implicit biases obscure differences between specific groups, lacking a strong enough pattern to continually reinforce the bias.

We present the average hiring recommendations for all types of bias in \Cref{fig:gender_hiring_average}.
Detailed results are in \Cref{sec:hiring_each}.
The results show that after augmentation, \textbf{the increase in the selection of Spanish male candidates is more pronounced than that of female}, particularly at high augmentation data proportions (20\%, 50\%).  This may reflect existing gender biases and could be attributed to the distribution and representation of genders in the pre-training data.  Spanish males may have more positive or balanced portrayals, which amplifies their selection after augmentation.
Additionally, the model tends to increase the selection of candidates from Spanish cultures, while Arabic candidates experience a consistent decline.  This suggests that \textbf{gender bias in the fine-tuning process can also influence other types of bias}, such as cultural bias, in downstream tasks, as these biases are often intersecting and can reinforce each other.
For this task, \textbf{the unbiased gender bias type exhibits the most pronounced disparities}, indicating that the lack of gender balance in the augmentation  data amplifies inherent biases in the model's decision-making process. 
Furthermore, among the six bias types, \textbf{contrastive explicit and contextual implicit biases also have the most severe impacts on hiring recommendations}. 

\subsection{Cultural Bias} \label{sec:culture_understanding}

\textbf{Classification Results}. 
\Cref{fig:culture_clf} shows the average Macro-F1 scores for four cultures across bias directly-related and indirectly-related tasks.
\textbf{Surprisingly, performance generally improves at lower proportions of bias data (10\%, 20\%) across all cultures on bias indirectly-related tasks.}
Additionally, for this kind of tasks, cultural variations are observed. Specifically, Spanish culture shows improvements even at higher proportions of bias data (50\%).
This may indicate that the additional biased data enhances the model's generalization ability, allowing it to better capture cultural nuances and improve performance.
However, \textbf{performance drops significantly even with smaller proportions of bias data on bias directly-related tasks.} Furthermore, performance continues to decrease as the proportion of bias data increases.  
This decline could be due to the model becoming overly influenced by the biased data, amplifying stereotypes and discrimination. In bias directly-related tasks, where the goal is to identify inequality targeting specific groups, the model may prioritize biased patterns, thereby diminishing its ability to accurately detect such inequality.

\begin{figure}[t!]
  \centering
\includegraphics[width=.7\textwidth]{image/combined_fig5.pdf}
  \vspace{-0.1in}
  \caption{The average story generation results and the multi-Round hiring recommendation results. Bias inheritance gets amplified over multiple rounds and eventually extends to majority groups.}
  \label{fig:culture_ge_average_mutli_hring}
\vspace{-0.1in}
\end{figure}

\textbf{Generation Results}. 
\Cref{fig:culture_ge_average_mutli_hring}(a) presents the total proportion of negative adjectives across the agency, beliefs, and communion dimensions. Detailed results of story generation results for each type of bias are in Appendix \ref{sec:story_each}. 
% It can be observed that 
After adding bias augmentation data,
for the Spanish culture, there is an overall decrease in the proportion of negative adjectives across all bias ratios.
% For the Chinese culture, at lower bias data ratios (e.g., 10\%), the proportion of negative adjectives occasionally increases.
For the Arabic culture, noticeable increases in the proportion of negative adjectives are observed at higher bias data ratios (e.g., 20\% and 50\%). 
This could also be attributed to the representation learned from the pre-training. Spanish culture may have more positive portrayals, so the added bias data counterbalances negative bias, reducing negative language. In contrast, for Arabic culture, the augmented data might reinforce existing negative stereotypes, leading to increased negative language use. 

\subsection{Multi-round Results}
\label{sec-exp-multi}

To investigate the long-term effects of biased inheritance, we conducted multi-round experiments focusing on gender bias. In each round, we sampled 3,600 unbiased real data points, which were then mixed with 50\% biased synthetic data of the unbiased bias type. 
The results for classification, salary generation, and hiring recommendation across multiple rounds are in \Cref{fig:culture_ge_average_mutli_hring}(b) and Appendix \ref{sec:mutil}.

\textbf{It is evident that bias inheritance not only persists but also amplifies over multiple rounds.}
For classification, performance declines across all demographic groups over multiple rounds.
For hiring recommendations, the proportion of Arabic candidates steadily decreases, while Spanish candidates become increasingly favored.
For salary recommendations, predicted salaries for male candidates rise over time, while those for female candidates decline, widening the gender pay gap over iterations.
These results also indicate that the model's bias toward minority groups accumulates and spreads over time. As the model may become overly reliant on certain features or past errors, \textbf{this bias extends to the majority group, leading to a decline in performance across all social groups}.

\subsection{Scaling Results}
\label{sec-exp-large-scale}

We further conducted large-scale experiments with proprietary GPT-4o-mini using the BiasinBio dataset. We sampled male and female biographies from the seven most popular professions as the unbiased real data, with each gender initially having 6,400 samples per profession, resulting in a total of 89,600 samples. We then replaced 50\% of the original data with augmented biased data to examine its impact.

As shown in Figure \ref{fig:gpt_gender}, \textbf{the augmentation process led to contrasting effects on male and female candidates.} 
In the salary recommendation task, the average predicted salary for male candidates decreased, while the average salary for female candidates increased.
Similarly, in the hiring recommendation task, the proportion of male candidates consistently decreased across all bias types, whereas the proportion of female candidates increased.
This observation could stem from the alignment tuning process
\citep{nghiem-etal-2024-gotta, street2024llm}
that many current LLMs experience. During this phase, models are fine-tuned to better reflect human values, such as fairness and inclusiveness. As a result, the GPT-4o-mini model may become more sensitive to gender bias, which could explain the increase in salary and hiring recommendations for female candidates. The biased augmentation data further reinforces this effect, amplifying the model's tendency to favor historically underrepresented groups.
The complicated effects of bias in post-training with aligned models is left as our future work for better understanding of how these biases evolve and influence model behavior over time.


\begin{figure}[t!]
  \centering
  \includegraphics[width=.7\textwidth]{image/gpt_gender.pdf}
  \vspace{-0.1in}
  \caption{Hiring and salary generation results using GPT-4o-mini.
  Same notation is used for x-axis as in \cref{fig:culture_clf}. 
  Bias leads to contrastive effects on the well-aligned GPT-4o-mini.
  % , possibly due to it is 
  }
  \label{fig:gpt_gender}
\vspace{-0.1in} 
\end{figure}

\section{Analysis of Bias Inheritance}
\label{sec-analysis}
This section presents several attempted analysis to further understand the rationale behind bias inheritance, which can be used for the subsequent mitigation.
While our analysis certainly cannot interpret all nuances in bias inheritance due to the large-scale or black-box nature of the pre-training data with increasingly complex network architecture, we provide several example-based analysis as attempts for explanation.
Our primary conjecture is that the biased augmented data causes \textit{misalignment} from different perspectives, including values, groups, and data distributions, which then result in the nuanced impacts on downstream tasks.

\textbf{Misalignment between LLM Responses and Human Cultural Values.}
As discussed in \cref{sec:culture_understanding}, 
the performance degraded across all cultures for bias directly-related classification tasks. 
Additionally, for story generation, the proportion of negative adjectives shows noticeable increases. 
To understand this, we analyzed the alignment between LLM-generated responses and real human responses to value-related questions.
In the cultural contextual bias type, LLMs answered the same value questions provided to individuals from different cultures. 
As shown in \Cref{fig:answer_imbalance}(a), the responses of LLMs significantly differ from those of humans in GlobalOpinionQA, particularly for subtle value questions.
The misalignment is more pronounced for Eastern cultures (e.g., Arabic, Chinese) than Western cultures (e.g., Portuguese, Spanish). 
This indicates that the modelâ€™s limited understanding of human value systems, especially those from Eastern contexts, contributes to the negative impacts.

\begin{figure}[t!]
	\centering
    \includegraphics[width=.7\textwidth]{image/combined_fig7.pdf}
        % \vspace{-0.25in}
	\caption{Misalignment of values and imbalanced generation.}
        \label{fig:answer_imbalance}
\vspace{-0.1in}
\end{figure}

\textbf{Misalignment across Groups in Generated Data.}
In \cref{sec:gender_understanding}, we demonstrated that the performance gap between male and female groups increased after data augmentation. For example, female samples experience a performance decline, whereas male samples consistently improve for gender biographies classification. Similarly, in the hiring recommendation task, Chinese female candidates are ranked higher than their male counterparts.
To investigate the causes of these disparities, we examine the gender distribution in augmented data under the unbiased type condition. Without explicit or implicit mechanisms to enforce gender balance, the generation process results in noticeable inconsistencies across groups.
\Cref{fig:answer_imbalance}(b) shows that Llama 3.1 generates more female-related biographies for most professions, including traditionally male-dominated fields like professor and software engineer, as well as female-dominated ones like nurse and painter. Architect is the only exception, where male biographies outnumber female ones. 
It reveal inherent misalignment in LLM-generated data across groups, likely stemming from imbalanced pre-training data.

\textbf{Misalignment between Generated and Real Data.}
In \cref{sec:gender_understanding,sec:culture_understanding}, we show that certain bias types have more severe impacts when combined with the original data. To better understand this, we analyzed the distribution differences between generated data and original data in the feature vector space. 
We obtained high-dimensional vector representations of both types of text in the model's hidden layers and performed dimensionality reduction. The three-dimensional representation of the reduced vectors corresponding to the contextual intersectional explicit and contrastive intersectional explicit bias type is shown in Figure \ref{fig:Embedding_2_5}, where red represents the vector representation of the original text, and blue represents the vector representation of the augmented text. 
The representation for all types of bias are provided in \Cref{sec:embedding}.
It is evident that there are significant distribution differences between the generated and real data in the feature space in the majority of cases. The only exception is the contextual explicit bias types, where the descriptors in the prompt are relatively similar for identical input questions, with only the answers varying between the models and humans. 
These findings suggest that the representation misalignment between generated and real data could be a key factor to performance degradation.

\begin{figure}[!t]
  \centering
  \includegraphics[width=.6\linewidth]{image/Embedding_Distribution.pdf}
  \vspace{-0.15in}
  \caption{Embedding Distribution.}
  \label{fig:Embedding_2_5}
\vspace{-0.2in}
\end{figure}

\begin{figure*}[!h]
  \centering
  \includegraphics[width=\linewidth]{image/mtg_aver.pdf}
  \vspace{-0.3in}
  \caption{The average mitigation results. All three mitigation strategies mitigate the malicious effects of bias at downstream. }
  \label{fig:mitigation}
\vspace{-0.1in}
\end{figure*}

\section{Mitigating Bias Inheritance}
\label{sec-mitigate}

In this section, inspired by previous results and analysis, we explore different approaches to mitigate bias inheritance.

\subsection{Mitigation Methods}

\textbf{Token-based Method.}
To address the observed misalignment between model-generated data and human understanding, we leverage the self-correction capabilities of current LLMs~\citep{madaan2024self, shinn2024reflexion} by prepending a token that indicates potential bias in the text. This allows the model to recognize the presence of bias, and potentially adjust its behavior accordingly.
For instance, ``\texttt{The following text may contain biases. \textit{[Text with Augmented Bias]}}". 
This token serves as a signal to the model that the text might contain bias, guiding it to approach the interpretation or processing of this data with caution \citep{allen2023physics}.

\textbf{Mask-based Method.}
To address the observed inconsistencies across groups in model-generated data, mask-based mitigation replaces sensitive words related to groups and bias inheritance with special placeholders (e.g., [MASK]) in the text to reduce bias learned by the model. The core idea is to ``mask'' out the potentially biased information in the text, preventing the model from making biased decisions based on those details.
For instance, for cultural bias, we replace specific cultural labels in the text (e.g., ``Arabic'' and ``Spanish'') with ``\texttt{[MASK]}'' to prevent the model from being influenced by cultural information when making
decisions.

\textbf{Loss-based Method.}
To address the distributional misalignment observed between generated and original data, we design a novel loss function to modify the optimization process, thereby mitigating bias during training.
We aligned the distribution of the generated text with the original text in a high-dimensional vector space to ensure that the generated text closely matches the semantics of the original text.
Specifically, denote $P_o$ and $P_a$ as the distribution of the original and augmented data, respectively.
Then, the alignment loss can be represented as:
\begin{equation*}
    \mathcal{L}_{align}=\left ( \mathbb{E}_{(x,y) \sim P_o} [\phi(x, y)] - \mathbb{E}_{(x,y) \sim P_a} [\phi(x, y)]\right)^2,
\end{equation*}
where $\phi(x,y)$ denotes the embedding of the question-answer pair and $\mathbb{E}$ is the expectation operator.
The loss is added to the standard fine-tuning loss.
The detailed implementation for these methods is provided in \Cref{sec:mtg_methods}.

\subsection{Mitigation Results}
\label{sec-method-result}

The average mitigation results of severe negative influences for each task and bias are shown in \Cref{fig:mitigation}.
A detailed analysis of the mitigation effects on gender and cultural biases across different tasks, as well as the results on GPT-4o-mini, are provided in \Cref{sec:effects}.
Our key conclusion is that, similar to the nuanced effects on downstream tasks, the effectiveness of different mitigation approaches is \emph{also nuanced}, depending on various factors such as different types of bias, downstream tasks, bias ratios, and more, highlighting the difficulty in devising a unified mitigation solution.

Specifically, token-based mitigation provides implicit cues and works best with \emph{simple biases and tasks}, as it depends on the model's own understanding. It is more effective for gender bias than complex cultural bias and performs better in classification tasks than in detailed generation tasks. For gender generation tasks, salary recommendation benefits more than complex hiring recommendation. More augmentation data (50\%) further enhance its performance.


Mask-based mitigation shows noticeable effects at \emph{lower bias ratios (5\%)},  especially in tasks like cultural classification. 
As at this ratio, explicit biased terms have a more direct and pronounced impact on the model's performance. 
By masking these terms, the model is less likely to rely on biased information or features that could skew its decision-making, thereby reducing bias inheritance. However, as bias ratios increase, the influence of more subtle and implicit biases grows, necessitating more complex mitigation strategies.
It also proves effective in \emph{contrastive explicit bias}, where direct bias information is more clearly identifiable.

Loss-based mitigation primarily depends on the distribution distance between the augmentation and the original data, showing significant effectiveness when the distance is large. It is more suitable for \emph{coarse-grained tasks}. For example, it works well in classification tasks where decisions often rely on broader patterns.
In generation tasks, coarse-grained contexts like salary recommendations perform better than finer-grained ones like hiring recommendation.  Additionally, smaller proportions (e.g., 5\% and 10\%) of augmentation data yield better results, by introducing subtle shifts that avoid overfitting to the augmented distribution while effectively influencing the original bias.

\section{Conclusion and Discussion}
\label{sec:conclusion}

In this paper, we took the first step towards understanding the impact of biases in LLM-based data augmentation, which we refer to as bias inheritance. To systematically study this issue, we proposed a multi-dimensional bias generation framework covering six main bias types. Focusing on gender and cultural bias, we investigated bias inheritance across various bias ratios on ten downstream classification and generation tasks.
We analyzed the negative influence of social bias from three  perspectives of misalignment.
We then proposed and evaluated three mitigation strategies.


This work has several limitations. 
First, our study focused primarily on gender and cultural biases, while other types of social biases (e.g., racial, socioeconomic) remain unexplored, which can be studied using our flexible pipeline.
Second, we mainly explored supervised fine-tuning, while other fine-tuning techniques such as RLHF and DPO can be studied in the future.
Third, our experiments are based on Llama 3.1 and GPT-4o-mini due to the large volume of experiments and costs.
Further explorations could be done on other models and datasets.
Finally, this paper can be extended to multimodal models in the future.

\section*{Impact Statement}

This study attempts to understand, analyze, and mitigate bias inheritance. In contemporary society, social fairness is of paramount importance.
Our approach provides a significant starting point for subsequent methodologies. Specifically, we generated biased synthetic data using LLaMA 3.1 and GPT-4o-mini to thoroughly investigate bias inheritance in LLM-based data augmentation. 
All generated biased data and pre-trained models based on this data are intended solely for research purposes. 
Throughout the paper, the authors remain neutral towards all cultural and gender perspectives, and respect their diversities.


%++++++++++++++++++++++++++++++++++++++++
% References section will be created automatically 
% with inclusion of "thebibliography" environment
% as it shown below. See text starting with line
% \begin{thebibliography}{99}
% Note: with this approach it is YOUR responsibility to put them in order
% of appearance.

\bibliographystyle{plainnat}
\bibliography{bias}

\newpage

\appendix
\input{appendix}


\end{document}
