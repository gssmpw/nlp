\section{Related Work}
\label{sec:related}

\subsection{Synthetic Data Augmentation for LLMs}
Synthesis data augmentation has been widely explored to enhance model performance and robustness. A prominent line of research focuses on generating synthetic data using LLMs. For instance, SPIN~\citep{chenself} aimed to align synthetic data with human-annotated distributions. Other studies, such as \citet{rogulsky2024effects}, investigated the reliability of synthetic data and address challenges like hallucination. In addition, \citet{zhu2024synthesize, yu2024large, li2024culturepark, shaib-etal-2024-detection} sought to reduce distribution gaps and expand the diversity of data. Beyond these, \citet{maheshwari2024efficacy} systematically examined the effectiveness of LLM-generated synthetic data for different NLP tasks, identifying potential biases and the limitations of using synthetic data for complex tasks. \citet{Munoz-Ortiz2024} explored how synthetic texts, particularly in news generation, can complement human-authored data in model training. Furthermore, \citet{longpre-etal-2024-pretrainers} highlighted the impact of dataset curation choices, demonstrating the trade-offs between generalization and toxicity filtering. While these approaches significantly contribute to improving the quality and diversity of synthetic data, they often overlook the potential biases inherent in such data and their downstream implications. 
Complementary to prior studies, our work bridges this gap by examining biases and their impact.


\subsection{Bias in LLMs}

Understanding and mitigating bias in LLMs remains a critical focus in AI research. Studies have identified biases in cultural alignment, stereotype reinforcement, and demographic disparities across applications such as chatbot interactions, hiring, and political decision-making \citep{tao2024cultural, bai2024measuring, eloundou2024first, guo2024hey, kotek2023gender, Fang2023BiasOA, nghiem-etal-2024-gotta, hu2024generative, fisher2024biased, beatty2024revealing}. These biases often stem from data selection and filtering, persisting through iterative training\citep{navigli2023biases, naous-etal-2024-beer, lyu2023pathway, seshadri2024bias, zhang2024will,gallegos2024bias}. Various mitigation strategies, including anonymization and post-hoc adjustments, have been explored with mixed effectiveness \citep{giorgi2024explicit, Liang2021TowardsUA, beatty2024revealing}. Unlike prior work, we focus on the biases introduced by synthetic data during LLM fine-tuning, a phenomenon distinct from bias propagation through natural language corpora. While previous studies have examined bias amplification in iterative training \citep{wang2024bias, zhang2024will} and potential benefits of bias \cite{chen2024understanding,chen2024slight}, our work systematically investigates how different synthetic data generation strategies shape bias dynamics, offering new insights for designing fairer AI systems.

\begin{figure*}[t!]
  \centering
  \includegraphics[width=\linewidth]{image/gender_clf_salary.pdf}
  \vspace{-.25in}
  \caption{Results on downstream tasks related to gender with different types of bias in augmentation data. 
  Bias in augmented data improves the performance of majority groups, yet deteriorates the performance for minority groups, resulting in a wider gap.
  }
  \label{fig:gender_clf_salary}
\vspace{-.1in}
\end{figure*}