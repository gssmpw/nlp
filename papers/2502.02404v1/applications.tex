%Placeholder for general introduction of the Accelerators and Applications theme sections. Themes of applications include machine-learning, bioinformatics, space applications, radio astronomy and weather simulations. Some of these references will overlap with other sections, e.g. when contributions are made on applying effective distributed computing for the purpose of weather forecasting.

FPGAs have emerged as powerful accelerators for a wide range of applications. In this section, we discuss FPGA-based solutions in machine learning (Section~\ref{sec:ml}), astronomy (Section~\ref{sec:astr}), particle physics experiments (Section~\ref{sec:phys}), quantum computing (Section~\ref{sec:quant}), space applications (Section~\ref{sec:space}), and bioinformatics (Section~\ref{sec:bio}).

\subsection{Machine learning}
\label{sec:ml}
% Three main parts, adapting an existing ML approach to hardware, designing hardware to accelerate an existing ML approach, (co-)design hardware for exotic ML approach.
% Main categories of evaluation are throughput, power, hardware area / resources, accuracy.
% \begin{itemize}
%     \item Accelerating existing ML models with new hardware design
%         \begin{itemize}
%             \item CNN acceleration (5)
%             \item TPU (1)
%             \item Benchmarking FPGAs (1)
%     \item Co-design existing ML models to hardware accelerate
%         \begin{itemize}
%             \item Pruning
%             \item Quantization / fixed point
%             \item Weight sharing
%             \item NAS adaptive to hardware
%         \end{itemize}
%     \item Design new hardware for exotic ML model
%         \begin{itemize}
%             \item Spiking / neuromorphic (7)
%             \item Bayesian (1)
%             \item Oscillating (2)
%         \end{itemize}
%     \end{itemize}
%     \item Hardware for 
% \end{itemize}
% \subsubsection{Background}
In the field of machine learning, and in particular deep learning, hardware acceleration plays a vital role. GPUs are the predominant method for hardware acceleration due to their high parallelism, but FPGA research is showing promising results. FPGAs enable inference at greater speed and better power efficiency when compared to GPUs \cite{hw-efficiency-compare} by designing model-specific accelerated pipelines \cite{ml-energy-efficient-cnn}. Through the co-design of machine learning models and machine learning hardware on FPGAs, models are accelerated without compromising on performance metrics and utilizing limited FPGA resources. In addition, the flexibility of the FPGA's architecture enables the realization of unconventional deep learning technology, such as Spiking Neural Networks (SNNs). 
%These networks can operate on a fraction of the power required by conventional networks on CPU or GPU.

%\subsubsection{Research topics}
\paragraph{Hardware acceleration} Ample research on hardware acceleration focuses on accelerating existing neural network architectures. One common class of architectures is convolutional neural networks (CNNs), which learn image filters in order to identify abstract image features. CNNs are often deployed in embedded applications which require real-time image processing and low energy consumption, making FPGAs a suitable candidate for CNN acceleration. \citet{ml-energy-efficient-cnn} propose an implementation of the LeNet architecture using Vitis HLS, pipelining the CNN layers, and outperforms other FPGA based implementations at a processing time of $70 \mu s$. One downside to this approach is the inflexibility of designing a specific model architecture in HLS which can be resolved by using partial reconfiguration \cite{ml-cnn-acclr-part-reconf}. To increase CNN throughput, further parallelization can be exploited, and in combination with the use of the high bandwidth OpenCAPI interface, can achieve a latency of less than $10 \mu s$ on the LeNet-5 model, streaming data from an HDMI interface \cite{ml-FPQNet}. In each of these implementations, fully pipelined CNNs are possible due to the limited number of parameters in small CNNs. As larger pipelined networks are deployed on FPGAs, parallelization puts strain on the available resources, and in particular the amount of on-chip-memory becomes a bottleneck. A proposed solution to this is using Frequency Compensated Memory Packing \cite{ml-mem-efficient-df-inf}.

In addition to CNN acceleration, general neural network acceleration has been developed by means of a programmable Tensor Processing Unit (TPU) as an overlay on an FPGA accelerator \cite{ml-agile-tuned-tpu}. Deep learning acceleration using FPGAs is also relevant to space technology research. Since the reprogrammability of FPGAs make them a suitable contender for deployment on space missions, FPGA implementations of existing deep learning models are being benchmarked for space applications \cite{ml-myriad-2-space-cnn} \cite{ml-mem-efficient-df-inf}.

\paragraph{Spiking neural networks} Spiking Neural Networks (SNNs) are computational models formed using spiking neuronal units that operate in parallel and mimic the basic operational principles of biological systems. These features endow SNNs with potentially richer dynamics than traditional artificial neural network models based on the McCulloch-Pitts point neurons or simple ReLU activation functions that do not incorporate timing information. Thus, SNNs excel in handling temporal information streams and are well-suited for innovative non-von-Neumann computer architectures, which differ from traditional sequential processing systems. SNNs are particularly well-suited for implementation in FPGAs due to their massive parallelism and requirement for significant on-chip memories with high-memory bandwidth for storing neuron states and synaptic weights. Additionally, SNNs use sparse binary communication, which is beneficial for low-latency operations because both computing and memory updates are triggered by events. FPGAs' inherent flexibility allows for reprogramming and customization, which enable reprogrammable SNNs in FPGAs, resulting in flexible, efficient, and low-latency systems~\cite{Corradi2021Gyro:Analytics,Irmak2021ADesigns,SankaranAnInference}. \citet{corradi2024accelerated} demonstrated the application of a Spiking Convolutional Neural Network (SCNN) to population genomics. The SCNN architecture achieved comparable classification accuracy to state-of-the-art CNNs while processing only about 59.9\% of the input data, reaching 97.6\% of CNN accuracy for classifying selective-sweep and recombination-hotspot genomic regions. This was enabled by % success is attributed to 
the SCNN's capability to temporize genetic information, allowing it to produce classification outputs without processing the entire genomic input sequence. Additionally, when implemented on FPGA hardware, the SCNN model exhibited over three times higher throughput and more than 100 times greater energy efficiency than a GPU implementation, markedly enhancing the processing of large-scale population genomics datasets.


\paragraph{Model/hardware co-design} Previous examples demonstrate that existing deep neural network models can be accelerated using FPGAs. Typically, research in this area focuses on designing an optimal hardware solution for an existing model. A more effective approach, however, is to co-design the model and the hardware accelerator simultaneously. However, simultaneous co-design of DNN models and accelerators is challenging. DNN designers often need more specialized knowledge to consider hardware constraints, while hardware designers may need help to maintain the quality and accuracy of DNN models. Furthermore, efficiently exploring the extensive co-design space is a significant challenge. This co-design methodology leads to better performance, leveraging FPGAs' flexibility and rapid prototyping capabilities. For example, \citet{Rocha2020BinaryWrist-PPG}, by co-designing the bCorNET framework, which combines binary CNNs and LSTMs, they were able to create an efficient hardware accelerator that processes HR estimation from PPG signals in real-time. The pipelined architecture and quantization strategies employed allowed for significant reductions in memory footprint and computational complexity, enabling real-time processing with low latency.

In SNNs, encoding information in spike streams is a crucial co-design aspect. SNNs primarily use two encoding strategies: rate-coding and time-to-first-spike (TTFS) coding. Rate coding is common in SNN models, encoding information based on the instantaneous frequency of spike streams. Higher spike frequencies result in higher precision but at the cost of increased energy consumption due to frequent spiking. While rate coding offers accuracy, it reduces sparsity. In FPGA implementations, rate coding is often used for its robustness, simplicity, ease of training through the conversion of analog neural networks to spiking neural networks, and practicality in multi-sensor data fusion, where it helps represent real values from various sensors (radars, cameras) even in the presence of jitter or imperfect synchronization~\cite{Corradi2021Gyro:Analytics}.
Conversely, TTFS coding has been demonstrated in SNNs implemented on FPGAs to enhance sparsity and has the potential of reducing energy consumption by encoding information in spike timing. For instance, Pes et al.~\cite{Pes2024ActiveNetworks} introduced a novel SNN model with active dendrites to address catastrophic forgetting in sequential learning tasks. Active dendrites enable the SNN to dynamically select different sub-networks for different tasks, improving continual learning and mitigating catastrophic forgetting. This model was implemented on a Xilinx Zynq-7020 SoC FPGA, demonstrating practical viability with a high accuracy of 80\% and an average inference time of 37.3 ms, indicating significant potential for real-world deployment in edge devices.

%To overcome this challenges, Cong et al in \textcolor{red}{\textcolor{red}{~\cite{FPGA/DNN Co-Design: An Efficient Design Methodology for IoT
%Intelligence on the Edge}}} introduced a co-design methodology for FPGAs and DNNs that integrates both bottom-up and top-down approaches, in which a bottom-up search for DNN models that prioritize high accuracy is paired with a top-down design of FPGA accelerators tailored to the specific characteristics of DNNs.
%Other methods leverage an automatic toolchain comprising  auto-DNN engine for hardware-aware DNN model optimization and an auto-HLS engine to generate FPGA-suitable synthesizable code, or hardware-aware Neural Architecture Search (NAS). When co-design is applied, it typicaly produces DNN models and FPGA accelerators that outperform state-of-the-art FPGA designs in various metrics, including accuracy, speed, power consumption, and energy efficiency \textcolor{red}{~\cite{When Neural Architecture Search Meets Hardware Implementation: from Hardware Awareness to Co-Design}.

%\textcolor{blue}{Co-design is critical in developing FPGA-based systems, merging hardware and software engineering from the initial design stages. This integrated method is essential for optimizing system performance, functionality, and cost-effectiveness. Co-design leverages the adaptable nature of FPGAs, tailoring the computing workload to meet specific hardware needs and adjusting the hardware to suit software demands. This synergy results in improved system performance and greater energy efficiency.}
%\textcolor{blue}{Many co-design examples exists in literature that demonstrate how clever distributed memory layouts can results in increased performances~\cite{}. }

%\paragraph{Novel hardware architecture} 

%\textcolor{blue}{Modern co-design methodologies allow the generation of hardware architectures and applications for advanced Reconfigurable Acceleration Devices (RAD) that go beyond traditional FPGA capabilities. These devices integrate FPGA fabric with other components like general-purpose processors, specialized accelerators, and high-performance networks-on-chip (NoCs) within a system-in-package framework. This integration enables complex data center applications to be handled more efficiently than conventional FPGAs. In particular, Boutrous et al in \cite{Architecture and Application Co-Design for Beyond-FPGA Reconfigurable Acceleration Devices} introduce RAD-Sim, an architecture simulator, to aid in the design space exploration of RADs. This allows for the study of interactions between different system components. Notably, they demonstrated mapping deep learning FPGA overlays to different RAD configurations, demonstrating how RAD-Sim can guide the adaptation of applications to exploit the novel features of RADs effectively.}


\subsection{Astronomy}
\label{sec:astr}
%\subsubsection{Introduction}

Astronomy is the study of everything in the universe beyond our Earth's atmosphere. Observations are done at different modalities and wavelengths, such as detection of a range of different particles (e.g., Cherenkov detector based systems such as KM3NeT \cite{KM3NeT:2009xxi}), gravitational waves, optical observations, gamma and x-ray observations and radio (e.g., WSRT \cite{van_Cappellen_2022}, LOFAR \cite{van_Haarlem_2013}, SKA \cite{book-SKA}). Observations can be done from space or from earth; in this section, we limit the scope to ground-based astronomy. A common denominator for instruments required for observation of the different modalities and different wave lengths is that the systems need to be very sensitive in order to observe very faint signals from outside the Earth's atmosphere. Instruments are typically large and/or distributed over a large area %in order 
to achieve %reach 
good sensitivity and resolution. Different modalities and wavelengths require distinct types of sensors, cameras, or antennas to convert observed phenomena into electrical signals. Each system is tailored to its specific modality and wavelength, necessitating specialized components to accurately capture and translate the data. %At different modalities and different wave lengths, the systems each require different kinds of sensors, camera's or antenna's that convert the observed phenomenon to an electrical signal. 
%The electrical signal is at some point in the signal chain converted to the digital domain and processed in various stages into an end product used by scientists. 
At a certain stage in the signal chain, the electrical signal is converted into the digital domain, where it undergoes multiple processing stages. This processed signal ultimately results in an end product that can be utilized by scientists for analysis and research purposes.
Systems can roughly be split into two parts, a front-end and a back-end. The front-end requires interfacing with and processing of data from the sensor; electronics commonly deployed in the front-end are constrained in space (size), temperature, power, cost, RFI, environmental conditions and serviceability. The back-end processes data produced by the front-end(s) either in an online or offline fashion, which is usually %typically be 
done with server infrastructure in a data center. % environment, either on site or centralized. 
In the back-end, the main challenges are the high data bandwidth and large data size coming from the front-ends. Although the environment is more flexible, systems are still constrained in space, power, and cost.

%\subsubsection{Background}

FPGAs have been used in astronomy instrumentation for quite some time, as they 
%FPGAs have since long found applications in astronomy instrumentation. 
%Typically FPGA's 
are %very 
efficient in interfacing with Analog to Digital Converters (ADCs), and well suited to the conditions faced in instrumentation front-ends (e.g. NCLE \cite{karapakula2024ncle}). Moreover, FPGA are also used further down the processing stages for various signal processing operations, both in the front-ends (e.g., Uniboard2 in LOFAR \cite{doi:10.1142/S225117171950003X}) as well as in the back-ends of systems (e.g., MeerKAT \cite{2022JATIS...8a1006V} and SKA \cite{SKA-CBF}). GPUs represent a good alternative in back-end processing (e.g., LOFAR's system COBALT \cite{Broekema_2018}) as well. The work by Veenboer et al. \cite{10.1007/978-3-030-29400-7_36} describes a trade-off between using a GPU and an FPGA accelerator in the implementation of an image processing operation in a radio telescope back-end.

%\subsubsection{Research topics}
%Dutch academia has contributed to several astronomy instruments:
%Often large international consortia, not immidiately clear what the role of the Dutch partners was. But also some work which is mainly done by Dutch institutes
\paragraph{Hardware Development for the Radio Neutrino Observatory in Greenland (RNO-G)}
The RNO-G \cite{Smith2022HardwareRNO-G} is a radio detection array for neutrinos. It consists of 35 autonomous stations deployed over a $5 \times 6$ km grid near the NSF Summit Station
in Greenland. Each station includes an FPGA-based phased trigger. The station has to operate in a 25 W power envelope. The implementation on FPGA seems to be favorable due to environmental conditions and operation constraints.

\paragraph{Implementation of a Correlator onto a Hardware Beam-Former to Calculate Beam-Weights}
The Apertif Phased Array Feed (PAF) \cite{van_Cappellen_2022} is a radio telescope front-end used in the WSRT system in the Netherlands. FPGAs are used for antenna read out as well as signal processing close to the antenna. Schoonderbeek et al. \cite{Schoonderbeek2020ImplementationBeam-Weights} describe the transformation and implementation of a beamformer algorithm on FPGA in order to build a more efficient system.

\paragraph{Near Memory Acceleration and Reduced-Precision Acceleration for High Resolution Radio Astronomy Imaging}
\citet{Corda2020NearImaging} describe the implementation of a 2D FFT on FPGA, leveraging Near-Memory Computing. The 2D FFT is applied to an image processing implementation on FPGA in the back-end of a radio telescope and compared with implementations on CPU and GPU. \citet{Corda2022Reduced-PrecisionHardware} explore %the concept of 
reduced-precision computation on an FPGA %is explored 
for the same image processing application. %They propose an implementation on an FPGA accelerator and compare with an implementation on CPU and GPU.

\paragraph{The MUSCAT Readout Electronics Backend: Design and Pre-deployment Performance}
The MUSCAT is a large single dish radio telescope with 1458 receives in the focal plane. The system uses FPGA based electronics to read out and pre-process the data from the receivers \cite{Rowe2023ThePerformance}. %\emph{Electronics Backend} in this case relates to the electronics close to the antenna, referred to as front-end in our description here.

% Small contribution by NL through SRON

\paragraph{Cherenkov Telescope Arrays}
Three different contributions have been made to three different Cherenkov based Telescope Arrays.
%\paragraph{A NECTAr-based upgrade for the Cherenkov cameras of the H.E.S.S. 12-meter telescopes}
Ashton et al.~\cite{Ashton2020ATelescopes} describe a system for the High Energy Stereoscopic System (H.E.S.S.) where a custom board with ARM CPU and an FPGA is used to read out and pre-process a custom designed NECTAr digitizer chip in the front-end of the system. After pre-processing, the data is distributed to a back-end over Ethernet.
%Anton Pannekoek Institute for Astronomy
%\paragraph{A White Rabbit-Synchronized Accurate Time-Stamping Solution for the Small-Sized Cameras of the Cherenkov Telescope Array}
Sánchez-Garrido et al.~\cite{Sanchez-Garrido2021AArray} present the design of a Zynq FPGA SoC based platform for White Rabbit time synchronization in the ZEN-CTA telescope array front-ends. Data captured and pre-processed at the front-ends is distributed over Ethernet to the back-end including the time stamp.
%\paragraph{Architecture and performance of the KM3NeT front-end firmware}
Aiello et al.~\cite{Aiello2021ArchitectureFirmware} outline the architecture and performance of the KM3Net front-end firmware. The KM3NeT telescope consists of two deep-sea three-dimensional sensor grids being deployed in the Mediterranean Sea. A central logic board with FPGA in the front-end serves as a Time to Digital Converter to record events and time at the sensors; the data is transmitted and further processed in a back-end on shore.
%S. Aiello et al., “KM3NeT front-end and readout electronics system:
%hardware, firmware, and software,” J. Astronomical Telescopes Inst.
%Syst., vol. 5, no. 4, pp. 1–15, 2019.

%\subsubsection{Future direction}

\vspace{0.4cm}
%In the works included in this survey, 
FPGA are mainly used for front-end sensor interfacing and pre-processing. \citet{Corda2020NearImaging, Corda2022Reduced-PrecisionHardware} underline that FPGAs are also still relevant in the back-end, providing improved performance over CPU and on-par performance with GPU accelerators. FPGA are expected to remain the dominant choice for platforms in astronomy instrumentation front-ends due to the strong interfacing capabilities and the adaptability and suitability to the constraints imposed by instrumentation front-ends. In the back-end, FPGAs provide a viable solution to application acceleration, but will have to compete with other accelerator architectures, e.g., GPUs~\cite{10.1007/978-3-030-29400-7_36}. 
An emerging new technology are the Artificial Intelligence Engines in the AMD Versal Adaptive SoC. The work from \citet{Versal-ACAP} evaluated the AI Engines for a signal processing application in radio astronomy. The flexibility and programmability of the AI Engines, combined with the interfacing capabilities of the FPGA can lead to a powerful platform for telescope front-ends.

\subsection{Particle physics experiments}
\label{sec:phys}
The Large Hadron Collider (LHC) features various particle accelerators to facilitate particle physics experiments. Experiments performed using particle accelerators can produce massive amounts of data that needs to be propagated and preprocessed at high speeds before the reduced relevant data is recorded for offline storage. FPGAs are widely employed throughout systems LHC particle accelerators, such as ATLAS and LHCb, for their high-bandwidth capabilities, and the flexibility that reconfigurable hardware offers without requiring hardware alterations to the system. Recently both the ATLAS and LHCb particle accelerators have been commissioned for upgrades. The Dutch Institute for Subatomic Physics (Nikhef) is one of the collaborating institutes working on the LHC accelerators.

%\subsubsection{Research topics}
%\paragraph{TODO - revise text into research topics}

LHCb is a particle accelerator that specializes in experiments that study the bottom quark. Major upgrades to the LHCb that enable handling a higher collision rate require new front-end and back-end electronics. To facilitate the back-end of the upgrade, the LHCb implements the custom PCIe40 board, which features an Intel Arria 10 FPGA. Four PCIe40 boards are dedicated for controlling part of the LHCb system, and 52 PCIe40 boards are used to read out each of the detector’s slices, producing an aggregated data rate of 2.85 Tb/s \cite{FernandezPrieto2020PhaseExperiment}.

ATLAS is one of the general particle accelerators of the LHC. ATLAS uses two trigger stages in order to record only the particle interactions of interest. In an upgrade to the ATLAS accelerator, ASIC-based calorimeter trigger preprocessor boards are replaced by FPGA-based hardware. Using FPGAs for this purpose allows implementing enhanced signal processing methods \cite{Aad2020PerformanceTrigger}. After the two trigger stages, FPGAs are deployed to process the triggers for tracking particles \cite{Aad2021TheSystem}.

Ongoing upgrades to the LHC particle accelerators, referred to as High Luminosity LHC (HL-LHC), will facilitate higher energy collisions. HL-LHC will produce increased background rates. To reduce false triggers due to background, the New Small Wheel checks for coinciding hits. Each trigger processor features Virtex-7, Kintex Ultrascale and Zynq FPGAs \cite{Iakovidis2023TheElectronics}. Interaction to and from front-end hardware is done through Front-End Link eXchange (FELiX) boards. As part of the HL-LHC upgrades, each FELiX board must facilitate a maximum throughput of 200 Gbps. To enable this, Remote Direct Memory Access (RDMA) over Converged Ethernet (RoCE) as part of the FELiX FPGA system is proposed \cite{Vasile2022FPGALHC, Vasile2023IntegrationLHC}. The performance of the FELiX upgrade in combination with an upgraded Software ReadOut Driver (SW ROD) satisfies the data transfer requirements of the upgraded ATLAS system \cite{Gottardo2020FEliXSystem}.




\subsection{Quantum computing}
\label{sec:quant}
Quantum computing promises to help solving many global challenges of our time such as quantum chemistry problems to design new medicines, the prediction of material properties for efficient energy storage, and the handling of big data needed for complex climate physics~\cite{Gibney-nat-2014}. The most promising quantum algorithms demand systems comprising thousands to millions of quantum bits~\cite{Meter-2013}, the quantum counterpart of a classical bit. A quantum processor comprising up to 50 qubits has been realized using solid-state superconducting qubits~\cite{Arute-nat-2019}, but its operation requires a combination of cryogenic temperatures below ~100 mK and hundreds of coaxial lines for qubit control and readout. %Furthermore, 
While in systems with a few qubits, this can be controlled using off-the-shelf electronic equipment, such approach becomes infeasible when scaling qubit systems toward thousands or millions of qubits that are required for a practical quantum computer. 

%\subsubsection{Research topics}

A means to tackle the foreseeable bottleneck in scaling the operation of qubit systems is to integrate FPGA technology in the control and readout of solid-state qubits. FPGAs have been used to generate highly-stable waveforms suitable for the control of quantum bits with latency significantly lower than software alternatives~\cite{Ireland-2020}. In systems of semiconductor spin qubits, FPGAs have provided in-hardware syncing of quantum dot control voltages with the signal acquisition and buffering and thus enabled the observation of real-time charge-tunneling events~\cite{Hartman-2023}. FPGAs have also been used to configure and synchronize a cryo-controller with an arbitrary waveform generator required to generate complex pulse shapes and perform quantum operations~\cite{Xue-nat-2021}. Such setup has enabled the demonstration of universal control of a quantum processor hosting six semiconductor spin qubits~\cite{Philips-nat-2022}. FPGAs have proven to be essential for implementing quantum error correction algorithms, which are critical for mitigating the effects of dephasing and decoherence in solid-state qubits. %FPGAs have also been shown to essential for the implementation of quantum error correction algorithms needed to mitigate the effects of dephasing and decoherence in solid-state qubits. 
In qubit systems based on superconducting quantum circuits, the first efficient demonstration of quantum error correction was made possible by a FPGA-controlled data acquisition system which provided dynamic real-time feedback on the evolution of the quantum system~\cite{Ofek-nat-2016}. It has been further predicted that FPGA can enable highly-efficient quantum error correction based on neural-network decoders~\cite{Overwater-2022}.

%\subsubsection{Future directions}

FPGA technology has proven invaluable in the development of the emerging research field of quantum computing.
However, the complexity of programming FPGA circuits hinders their implementation in quantum computing systems. Commercial efforts have been done toward providing graphical tools for designing FPGA programs, namely the Quantum Researchers Toolkit by Keysight Technologies and the FPGA-based multi-instrument platform Moku-Pro developed by Liquid Instruments. These tools are essential for implementing customized algorithms without the need for dedicated expertise in hardware description languages. Future research is also needed in integrating FPGAs in cryogenic platforms required to operate qubit systems. Such capability has already been demonstrated; commercial FPGAs can operate at temperatures below 4 K and be integrated in a cryogenic platform for qubit control~\cite{Homulle-2017}. These efforts provide evidence that FPGA technology is of great interest for enabling a scalable and practically applicable quantum computer. 


\subsection{Space}
\label{sec:space}
The flexibility of FPGA technology makes it a suitable platform for many applications on-board space missions. The European Space Research and Technology Centre (ESTEC), as part of the European Space Agency (ESA) actively explores FPGA technology for space applications, and has an extensive portfolio of FPGA Intellectual Property (IP) Cores~\cite{esa_ip}.

%\subsubsection{Research topics}

FPGAs can flexibly route its input and output ports, and can be configured to support many different communication protocols. This makes FPGAs good contenders as devices that communicate with the various hardware platforms and sensors on a space mission. FPGAs and have been implemented as interface devices in novel on-board machine learning and digital signal processing  implementations~\cite{Leon2021ImprovingSoC, Leon2021FPGABenchmarks, karapakula2024ncle}. 

An on-board task for which FPGAs are used is hyperspectral imaging. This type of on-board imaging produces vast amounts of data. To reduce transmission bandwidth requirements when transmitting the sensory data to earth, real-time on-board compression handling high data rates is required. FPGAs are well-suited for such tasks, and research has been done on using space-grade radiation-hardened FPGAs \cite{Barrios2020SHyLoCMissions} as well as commercial off-the-shelf (COTS) FPGAs \cite{Rodriguez2019ScalableCompression} for on-board hyperspectral image compression. COTS devices are generally cheaper than space-grade devices, but the higher susceptibility of these devices to radiation-induced effects makes them challenging to employ.

Communication between on-board systems often requires high data-rates and is susceptible to radiation induced effects. To deal with the unique constraints of space applications, dedicated communication protocols such as SpaceWire, and its successor, SpaceFibre have been developed. These protocols are available as FPGA IP implementations, and testing environments of SpaceFibre have been developed \cite{MystkowskaSimulationSpaceFibre, AnSection}. SpaceWire can interface with the common AXI4 protocol using a dedicated bridge \cite{RubattuASystems}, enabling its integration with SpaceWire interfaces. Direct Memory Access (DMA) allows peripherals to transfer data to and from an FPGA without going through a CPU. The application of DMA in space is being investigated, however its application as of now is limited since DMA is susceptibility to radiation-induced effects \cite{Portaluri2022Radiation-inducedDevices}.



\subsection{Bioinformatics}
\label{sec:bio}
FPGA technology has been extensively explored for accelerating Bioinformatics kernels. Bioinformatics is an interdisciplinary scientific field that combines biology, computer science, mathematics, and statistics to analyze and interpret biological data. The field primarily focuses on the development and application of methods, algorithms, and tools to handle, process, and analyze large sets of biological data, such as DNA sequences, protein structures, and gene expression patterns.Continuous advances in DNA sequencing technologies~\cite{hu2021next} have led to the rapid accumulation of biological data, creating an urgent need for high-performance computational solutions capable of efficiently managing increasingly larger datasets.

\citet{Shahroodi2022KrakenOnMem:Profiling} describe a hardware/software co-designed framework to accelerate and improve energy consumption of taxonomic profiling. In metagenomics, the main goal is to understand the role of each organism in our environment in order to
improve our quality of life, and taxonomic profiling involves the identification and categorization of the various types of organisms present in a biological sample by analyzing DNA or protein sequences from the sample to determine which species or taxa are represented. The study focuses on boosting performance of table lookup, which is the primary bottleneck in taxonomic profilers, by proposing a processing-in-memory hardware accelerator. Using large-scale simulations, the authors report an average of 63.1\% faster execution and orders of magnitude higher energy efficiency than the  widely used metagenomic analysis tool Kraken2~\cite{wood2019improved} executed on a 128-core server with AMD EPYC 7742 processors  operating at 2.25 GHz. An FPGA was used for prototyping and emulation purposes.

\citet{Corts2022AcceleratedFPGAs} employ FPGAs to accelerate the detection of traces of positive natural selection in genomes. The authors designed a hardware accelerator for the $\omega$ statistic~\cite{kim2004linkage}, which is extensively used in population genetics as an indicator of positive selection. In comparison with a single CPU core,
the FPGA accelerator can deliver up to $57.1\times$ faster
computation of the $\omega$ statistic, using the OmegaPlus~\cite{alachiotis2012omegaplus} software implementation as reference.


%\citet{Ahmad2022Communication-EfficientFlight}



\citet{Malakonakis2020ExploringRAxML} use FPGAs to accelerate the widely used phylogenetics software tool RAxML~\cite{stamatakis2014raxml}. The study implements the Phylogenetic Likelihood Function (PLF), which is used for evaluating phylogenetic trees, on a Xilinx ZCU102 development board and a cloud-based Amazon AWS EC2 F1 instance. The first system (ZCU102) can deploy two accelerator instances, operating at 250MHz, and delivers up to $7.7\times$ faster executions than sequential software execution on a AWS EC2 F1 instance. %Xeon processors. 
The AWS-based accelerated system is $5.2\times$ faster than the same software implementation. %In comparison with previous work by Alachiotis et al.~\cite{alachiotis2009exploring[7_12]}, the implementation on the Xilinx development board is about 2.35x faster. %, but the older technology should certainly be taken into consideration. 



\citet{Alachiotis2021AcceleratingCloud} also target the PLF implementation in RAxML, and propose an optimization method for data movement in PCI-attached accelerators using tree-search algorithms. They developed a software cache controller that leverages data dependencies between consecutive PLF calls to cache data on the accelerator card. In combination with double buffering over PCIe, this approach led to nearly $4\times$ improvement in the performance of an FPGA-based PLF accelerator. Executing the complete RAxML algorithm on an AWS EC2 F1 instance, the authors observed up to $9.2\times$ faster processing of protein data than a $2.7$ GHz Xeon processor in the same cloud environment.

With genomic datasets continuing to expand, bioinformatics analyses are likely to increasingly rely on cloud computing in the future. This shift will be supported by new programming models and frameworks designed to address the data-movement challenges posed by cloud-based hardware accelerators. These accelerators, such as FPGAs and GPUs, need data transfers from the host processor, which can significantly impact execution times and negate gains from computation improvements. Fortunately, similar data-movement concerns exist for both FPGAs and GPUs, and ongoing engineering efforts are likely to converge on common solutions~\cite{Corts2023AGenetics}. This will help bring optimized, hardware-accelerate processing techniques into more widespread use among computational biologists and bioinformaticians in the future.




