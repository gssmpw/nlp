% This section discusses models, tools, and techniques that facilitate porting of existing software to hardware accelerators. 

% \textcolor{blue}{Notes for Nikos/Sjoerd:
% \begin{itemize}
%     \item We don't have a lot of paper per subtopic, and inside each subtopic the papers are also quite different (in terms of goals). So we propose reducing the topic to just two: programming models/frameworks and performance prediction (or tools)
%     \item All the papers may have additional details embedded as latex comments
%     \item one of the select papers ( "Modeling FPGA-Based Systems via Few-Shot Learning") is actually a poster with a short abstract. We believe this is a subset of another paper ("LEAPER...") and therefore we suggest removing it.
%     \item Regarding the "Future direction": the current picture of the Dutch research in programming models and tools is quite scattered and with no clear (at least to us) direction. Out of the 14 papers, only 4 are led by Dutch institutions, and it is a bit hard to say on what Dutch research groups are leaders. At the moment, the corresponding subsections contain high-level overviews.
%     \item a comment that applies to both subtopics is that there are very few open-source tools, or even just publicly available artifacts, so we can make the call for action in this regard (probably it applies to other sections)
%     \item we noticed that there are some shared references with Sect. \ref{sec:big-data-processing-analytics} (Big data processing -- e.g., tydi) and Sect. \ref{sec:high-performance-computing} (High-Performance computing -- e.g., PERKS), so maybe it makes sense to link them together
%     \item you can remove these notes afterward
% \end{itemize}
% }


This section discusses approaches that boost developer productivity; Section~\ref{prog_models_frameworks} reviews programming models and frameworks that raise the abstraction level of describing hardware, while Section~\ref{perf_pred} presents 
techniques that predict performance of synthesized programs.

\subsection{Programming models and frameworks}
\label{prog_models_frameworks}
% This section presents developments of programming models and frameworks, including HLS-based, aimed at reducing development time for FPGA-based designs.

% \subsubsection{Background}
FPGA development is traditionally characterized by a steep learning curve, especially for non-experts. For this reason, High-Level Synthesis (HLS) tools and, more generally, high-level programming models and frameworks have been proposed to increase productivity by raising the abstraction level. 
HLS tools became commercial products in the early 2010s. Since then, they have been used in various application domains, including, but not limited to, deep learning, multimedia, graph processing, and genome sequencing ~\cite{Cong-2022}. HLS tools can reduce the average development time (up to two-thirds compared to RTL~\cite{Lahti-2019}). However, they still require considerable expertise to optimize the FPGA designs and achieve a Quality-of-Result that is on par with the one obtained through hardware description languages. For this reason, higher-level programming models and frameworks are now being proposed. They allow developers %user 
to describe hardware %write their 
%programs 
in a more convenient formalism (e.g., Clash~\cite{clash-2010,clash-website}, HeteroCL~\cite{heterocl-2019}, PyLog~\cite{pylog-2021}, and DaCe~\cite{dace-2021} which currently support Haskell, Python or a Python-embedded DSL), and %they %take care of 
automatically, or via user-provided hints, generate optimized %lower it into 
HLS/HDL descriptions. %and optimize the final design. 

%\subsubsection{Current research in the Netherlands}

\subsubsection*{\bf{Research topics}} Several papers have been published by Dutch organizations focusing on reducing development time for hardware design using HLS programming models and tools.

\paragraph{Abeto framework%: a Solution for Heterogeneous IP Management
}
% Reviewer: Tiziano
% Contribution:
%   - this is not a programming framework, rather a tool for IP management and workflow automation
%   - it should be general enough to be used for various application mains
%   - it is not open-source, probably still maintained
%   2. Advantages/Disadvantages, how to position this work related to state of the art: 
%       - in the paper the authors mention several other IP management applications, but they say these are usually "rigid" in the expected IP core format and require considerable effort

%Sanchez et al. propose Abeto 
\citet{Sanchez2022AbetoManagement} propose Abeto, a software tool for IP management and workflow automation. Historically, there has been no established standard for packing, documenting, and distributing IP core designs.  This prevents their re-usability, as each IP core has its unique learning curve and challenges for using them in an EDA toolchain. Abeto allows the user to operate in a unified manner with heterogeneous IP cores, and conveniently configure and launch the different stages of the IP workflow. To add an IP core, Abeto requires some auxiliary information to be provided: a database definition (containing information about the directory structure of the IP core) and a command dictionary (which includes the list of supported IP commands and how they must be executed). The tool has been validated against a subset of the ESA portfolio of IP cores\footnote{\url{https://www.esa.int/Enabling_Support/Space_Engineering_Technology/Microelectronics/ESA_HDL_IP_Cores_Portfolio_Overview}}, which constitutes a heterogeneous group of IP cores, demonstrating the tool's versatility.


\paragraph{%A Complete Open Source
Design flow for Gowin FPGAs}
% Reviewer: Christiaan
% Contributions
% - The contribution describes a framework to configure Gowin FPGAs using open-source tools
% - It is general purpose insofar that any application can make use of the new flow
% - It is an open source technology development, and contributions have been merged upstream
% - The work is analytical in that is describes the method for documenting the bitstream format.
%De Vos et. al.~
\citet{vos2020gowin} describe a method %and results 
to create an open-source design flow for the Gowin LittleBee family of FPGAs.
The design flow is based on well-known open-source tools such as Yosys and nextpnr, as well as the newly developed bitstream generator.
Architectural details of the FPGA family were documented using input fuzzing and comparing results from the existing closed-source vendor tool flow.
While the created open-source flow is capable of synthesizing a full RISC-V core, many aspects, such as DSPs, RAMs, and PLLs are currently unsupported. The authors report that documenting the bitstream format for all of these features is %will be 
the subject of future work.


%\paragraph{AEx: Automated High-Level Synthesis of Compiler Programmable Co-Processors -- \cite{Hirvonen2023AEx:Co-Processors}} 

\paragraph{AEx framework} 


% Reviewer: Tiziano
% Contributions:
% - a framework for overlays/FPGA design
% - seems general purpose but this is not clear
% - it is the result of a European project (https://fitoptivis.eu/) but there is no mention that this is opensource
\citet{Hirvonen2023AEx:Co-Processors} propose %in this paper 
AEx, a framework for automated High-Level synthesis of compiler programmable co-processors. AEx can be used to produce Application-Specific Instruction-Set (ASIP) architectures. ASIP processors have been proposed as a way to produce FPGA overlays starting from a software-programmable template. The program being executed can be easily changed, reducing design time and costs. The template being used by AEx is Transport Triggered Architecture (TTA).
% In case, a short explanation of TTAs
%TTAs are architectures in which programs have more low-level control of data transfers between processor functional units. This is in contrast with  directly control the internal transport buses of a processor. 
AEx includes heuristics for design space exploration and pruning, aimed at finding the best architecture able to satisfy real-time execution time and clock frequency constraints. The user can then choose the results that better fit their need (e.g., minimum resource utilization). Evaluation %The results 
shows how the tool is able to produce results in a reasonable amount of time, achieving %with 
performance close to that of the fixed-function implementations generated by HLS vendor tools such as AMD/Xilinx Vitis.


%\paragraph{Exploration of Synthesis Methods from Simulink Models to FPGA for Aerospace Applications}

\paragraph{Synthesis from Simulink Models to FPGA for Aerospace Applications}

% Reviewer: Tiziano
% Contributions:
% - a survey/analysis of methods to generate FPGA design from Matlab Simulink model (for space applications)
% - they considered a specific use case (Simulink)
% - nature of the work: comparative. It is not an actual framework, but rather a collection of suggestions
% - How to position this work wrt background? Not the first paper I believe to discuss the advantage of FPGA in aerospace. Maybe one of the few to discuss how to leverage HLS in a convenient way for aerospace engineers

Reconfigurable hardware is becoming an attractive solution for aerospace applications, thanks to its power efficiency and capabilities of in-flight configuration. Algorithms are usually expressed in model-based programming frameworks, e.g., Matlab Simulink, but turning them into low-level hardware description languages can be cumbersome. \citet{Curzel2023ExplorationApplications} analyze solutions to automatically synthesize Simulink models. Matlab already provides an automated method (HDL coder) to translate part of Simulink models into Verilog/VHDL, but this still requires a certain level of expertise. Therefore, the authors propose to apply HLS on the code generated by Matlab's Embedded Coder tool, further automatizing the design process. Experiments with three %different 
benchmarks show that this solution is more efficient than relying on HDL coder, and it does not require specific hardware expertise. % to the user.
% Future work: how to automatize HW/SW partitioning, quality of generated C code.

\paragraph{HLS optimizations for  post-quantum cryptography on Lattice FPGAs}
%\paragraph{Optimizing Lattice-based Post-Quantum Cryptography Codes for High-Level Synthesis}

% Reviewer: Tiziano:
% Contributions:
% - analysis and improvement of HLS based implementation of post-quantum crypto algorithms
% - they considered a specific use case
% - nature of the work: implementation. They claim they would open-sourced it, but I can not find it
% - How to position this paper: this paper does not introduce anything new, but applies known optimization to a specific application domain
%In this paper, Guerrieri et al.~
\citet{Guerrieri2022OptimizingSynthesis} discuss the process of porting Post-Quantum Cryptographic algorithms to an FPGA using HLS. While it can be reasonably straightforward %easy 
to port an existing CPU implementation to an FPGA, %the 
performance can be low and resource utilization %of resources 
is not optimal. The authors discuss how, applying well-known HLS-specific optimization techniques, the code can be rewritten to leverage the capabilities of HLS tools and produce more efficient designs, reducing the computation latency of up to two orders of magnitudes in specific cases.


%\paragraph{CONT Optimizing Industrial Applications for Heterogeneous HPC Systems: The OPTIMA Project Intermediate stage}
\paragraph{Optimizations for Heterogeneous HPC Systems (OPTIMA)}
% Reviewer: Christiaan
% Contributions
% - Documentation of the two HPC systems, and the results of porting certain applications
% - It's a case study for specific algorithms on specific HPC systems
% - The applications that are ported seem to be open-source and actively maintained
% - Positioning the paper: while the paper reports results, it does not demonstrate how these results compare to SOTA
%Theodoropoulos et al.~
\citet{Theodoropoulos2023optima} demonstrate the results of porting and optimizing industrial applications to two new heterogeneous HPC systems within the OPTIMA project. The results highlight the performance increase of using the available FPGA-based accelerators versus a pure software implementation running on the CPUs of the HPC system.


%\paragraph{The VINEYARD Framework for Heterogeneous Cloud Applications: The BrainFrame Case}

\paragraph{A Framework for Heterogeneous Cloud Applications (VINEYARD)}
% Reviewer: Christiaan
% Contributions:
% - A framework for deploying different parts of an application across multiple accelerators in a cloud environement
% - Though they consider multiple use-cases, they only work out a specific use case: spiking neural networks
% - Supposedly there is an open-source marketplace, http://www.accel-store.com/, but it returns error 503
% Positioning the paper: while the paper reports results, it does not demonstrate how these results compare to SOTA
%In this paper, Sidiropoulos et al.~
\citet{Sidiropoulos2018vineyard} describe a framework to accelerate different parts of an application across different accelerators, like GPUs and FPGAs. They demonstrate the utility of this framework by creating a platform for computational neuroscience, called BrainFrame. The BrainFrame platform allows one to simulate spiking neural networks, and depending on the number of neurons and their interconnectivity, certain combinations of accelerators achieved the shortest simulation times.

%\paragraph{Tydi: an open specification for complex data structures over hardware streams}
\paragraph{Mapping data structures to hardware streams (Tydi)}
%Pelterberg et.al.~
\citet{Peltenburg2020Tydi:Streams} describe a specification for mapping complex, dynamically sized data structures onto a fixed number of hardware streams.
%SV: see also data centre big data analytics section for more about Tidy

\subsubsection*{\bf{Future directions}}
Traditional FPGA programming has been done using Hardware Description Languages, which have a steep learning curve that does not favor adopting reconfigurable devices in the scientific and industry community. To address this issue, there is a collective effort to increase the level of abstraction for FPGA designs without compromising performance. Achieving this goal requires a multidisciplinary approach involving programming languages, compilers, and optimization techniques. HLS tools and high-level approaches are being used in various application domains.
Although the current direction in this field in the Netherlands is unclear, our analysis has pinpointed specific domains of interest within local research communities, such as aerospace and accelerated big data processing, that could benefit from more accessible programming methods for FPGA devices.

    % \item in the Netherlands:
    % \begin{itemize}
    % \item no winner application domain, even though aerospace is often considered (due to ESA)
    % \item TUD with Tydi (and the related lab research) seems to be investing a lot in architectures/hardware for accelerating big data processing
    % %SV: work is continued and implemented in Voltron Data ?
    % \end{itemize}
%\end{itemize}


\subsection{Performance prediction}
\label{perf_pred}
% This section describes tool to improve and speed up the performance prediction (e.g., area and efficiency) of FPGA designs.

% \subsubsection{Background}
FPGA design and development processes are time-consuming activities due to, among others, the very fine granularity reconfigurability of FPGA designs, which translates into a large design space and long synthesis time. For this reason, it is crucial to enable quick performance prediction of synthesized programs to improve early-stage design analysis and exploration, and performance debugging.
We can distinguish between two main types of performance prediction models: analytical and ML-based. Analytical models (such as HLscope+~\cite{hlscope-2017} and COMBA~\cite{comba-2020}) analyze the source code and use mathematical modeling to estimate performance and resource utilization. They are able to produce quick estimates at the cost of reduced accuracy. ML-Based models~\cite{oneal-2018, ustun-2020, Sun-2021}, on the other hand, %instead 
aim at improving prediction accuracy by considering device-specific features, but typically %often 
require long and expensive training procedures. 


%\subsubsection{Current research in the Netherlands}

\subsubsection*{\bf{Research topics}} Several papers have been published by Dutch organizations focusing on performance prediction of synthesized codes.

%\paragraph{LEAPER: Fast and Accurate FPGA-based System Performance Prediction via Transfer learning}

\paragraph{System Performance Prediction via Transfer learning (LEAPER)}

% Reviewr: Christiaan
% Contributions:
%  - The contribution describes a framework.
%  - It is general purpose insofar that any application can make use of the new flow, it does however only support C/C++ based entry.
%   - Technology development
%   - The work is analytical
%Singh et.al.~
\citet{Singha2022leaper} describe a method for predicting system performance and resource usage of FPGA accelerators using transfer learning.
They trained a performance predictor model for an edge/embedded FPGA, and used transfer learning so that the model can also be used for cloud/high-end FPGAs.
The method allows for design space exploration of mapping C/C++ programs to cloud/high-end FPGAs using HLS. The authors showed that it is 10x faster than the state of the art, achieving 85\% accuracy.

\paragraph{Modeling FPGA-Based Systems via Few-Shot Learning}
% Reviewer Tiziano
% This is a poster, and very few details are provided in the abstract
Machine learning based models are being proposed to provide fast and accurate performance predictions of FPGA-based designs. However, training these models is expensive, due to the time-consuming FPGA design cycle. %In this poster, 
%Singh et al.~
\citet{Singh2021ModelingLearning} propose a transfer-learning-based approach for FPGA-based systems, to adapt an existing ML model, trained for a specific device, to a new, unknown environment, reducing the training costs.


%\paragraph{CGRA-EAM—Rapid Energy and Area Estimation for Coarse-grained Reconfigurable Architectures}
\paragraph{%CGRA-EAM—Rapid 
Energy and Area Estimation for Coarse-grained Reconfigurable Architectures}
% Reviewer: Christiaan
% Contributions:
% - Analytical model for power and area of CGRAs, and the method to create said model
% - While the method to create the model is only applied to one CGRA architecture, the paper claims it should work for many different CGRAs as long as the RTL is available/can be generated.
% - It does not seem the code artifacts or models can be easily downloaded anywhere
% - Positioning the paper: This work is analytical vs a machine-learning approach, focus on CGRA (instead of FPGA or ASIC). 
Design space exploration is often required to achieve good Pareto points when creating reconfigurable architectures. %Wijtvliet et. al.~
\citet{Wijtvliet2021cgra} introduce the CGRA-EAM  model for energy and area estimation for CGRAs. It %which 
achieves a 15.5\% error for energy and 2.1\% error for area estimation for the Blocks~\cite{Wijtvliet2019Blocks:Efficiency} CGRA. %The novelty of the work lies on the focus on CGRAs and the that it works over multiple different application running on an CGRA.  
The novelty of this work lies in its focus on CGRAs and its ability to handle multiple different applications running on a CGRA.

\paragraph{Analytical Performance Estimation for Large-Scale Reconfigurable Dataflow Platforms}
% Reviewer: Christiaan & Steven

% Contributions:
% - framework for analytical model + statistics 
% - automatic approach for predicting performance on reconfigurable dataflow platforms
% - The novelty of the work lies in the fact that it is applicable to performance estimation for large-scale workloads on heterogeneous systems
% Keio University, Japan; Imperial College London, United Kingdom; University of Amsterdam, The Netherlands; Maxeler Technologies Ltd., United Kingdom
%The work from Yasudo et al., introduced in 
\citet{Yasudo2018PerformancePlatforms, Yasudo2021AnalyticalPlatforms} %and \citet{Yasudo2021perf} 
introduced and further expanded %in ~\cite{Yasudo2021perf}, proposes 
\emph{PERKS}, a performance estimation framework for reconfigurable dataflow platforms. The authors propose %In the work it is proposed 
that reconfigurable accelerators, such as FPGAs, will play an important role in future exascale computing platforms and that such a framework is essential in the efficient deployment of applications on heterogenous platforms with reconfigurable accelerators. The PERKS framework uses parameters from the target platform and the application to build an analytical model to predict the performance of multi-accelerator systems. Experimental results with different reconfigurable dataflow applications are presented, showing that the framework can predict the performance of current workloads with high accuracy.

%propose the PERKS performance estimation framework for reconfigurable dataflow programs in order to assess the feasibility of large-scale heterogeneous systems. Experimental show an above 91\% accuracy for execution time of five different applications for two reconfigurable dataflow processing platforms. It is an analytical model that adops profiling and statistical methods for calibration and improving accuracy. The novelty of the work lies in the fact that it is applicable to performance estimation for large-scale workloads on heterogeneous systems.


\paragraph{Memory and Communication Profiling for Accelerator-Based Platforms}
% Reviewer: Steven
% Contributions:
% - MCPROF profiling tool, open-source (last activity 7 years ago?)
% - Improvements compared to existing work include: faster, provinding better insights
% - Case study of image processing applications for FPGA and GPU, allowing to get insigt if application fits well to the architecture
% TU Delt + QuTech
% They descrive future work on "relating the datacommunication information generated by MCPROF with performance estimates generated by the profiling tools provided by Xilinx and Nvidia" and "utilization of the currently generated information by MCPROF to automatically generate SDSoC and OpenACC [53] pragmas for FPGA- and GPUbased accelerators, respectively".
% Looking at the publications from the main author of this work, it might have ended up in Quantum programming tooling? https://scholar.google.nl/citations?user=N_tCHwkAAAAJ&hl=nl
%The work by Ashraf et al. 
\citet{Ashraf2018MemoryPlatforms} present \emph{MCPROF}, an open-source memory-access and data-communication profiler. The tool provides a detailed profile of memory-access behavior for heterogeneous systems (CPU, GPU, and FPGA) for C/C++ applications, as well as data-communication-aware mapping of applications on these architectures. Comparison with the state of the art show that the proposed profiler has an order of magnitude, on average, lower overhead than state-of-the-art data-communication profilers over a wide range of benchmarks. A case study  with several image processing applications for heterogeneous multi-core platforms containing an FPGA and a GPU as accelerators was conducted. The authors also demonstrate that the tool can provide insights into whether %or not 
a specific %certain 
accelerator (GPU or FPGA) is a good fit 
for the application.

%\paragraph{Performance Estimation for Exascale Reconfigurable Dataflow Platforms}
% combined with: Analytical Performance Estimation for Large-Scale Reconfigurable Dataflow Platforms
% Reviewer: Steven
% Contributions:
% - 
% Keio University, Japan; Imperial College London, United Kingdom; University of Amsterdam, The Netherlands; Maxeler Technologies Ltd., United Kingdom
%
% CB: "Analytical Performance Estimation for Large-Scale Reconfigurable Dataflow Platforms" is _also_ introduces PERKS, is newer, and at the end of the introduction section it says:
% "This article extends the brief description of PERKS from Yasudo et al. [44] as follows. Section 2
% on related work is new. Section 3 covers a simpler model than the one described in the work of
% Yasudo et al. [44]. Section 4, which describes the automated method for performance estimation, is
% new. Section 5 includes further material on our experimental setup and on verifying the proposed
% model. Section 6 contains additional large-scale workload scenarios to illustrate our approach.
% Finally, Section 7 highlights the main take-home messages of this work."
% 
% CB: So perhaps these two works could be combined into one paragraph?

%This work by Yasudo et al. \cite{Yasudo2018PerformancePlatformsb} proposes \emph{PERKS}, a  performance estimation framework for reconfigurable dataflow platforms. In the work it is proposed that reconfigurable accelerators (such as FPGA) will play an important role in future exascale computing platforms and that such a framework is essential in the efficient deployment of applications on heterogenous platforms with reconfigurable accelerators. The PERKS framework uses parameters from the target platform and the application to build an analytical model to predict the performance of multi-accelerator systems. Experimental results with different reconfigurable dataflow applications are presented, showing that the framework can predict the performance of current workloads at high accuracy. 

\paragraph{Delay Prediction for ASIC HLS}%: Comparing Graph-Based and Nongraph-Based Learning Models}
% Reviewer: Christiaan
% Contributions
% - Thorough analysis of both graph and nongraph-based learning models for delay estimation
% - It's a general method tested against multiple applications including data and control oriented designs
% - It does not seem the code artifacts or models can be easily downloaded
% - Positioning the paper: existing work focuses mostly on FPGA, whilst the focus of this work is more on ASIC.
The delay estimates of HLS tools can often deviate significantly from results obtained from logic synthesis. %De et. al.~
\citet{De2023hls} propose %using 
a hybrid model by incorporating graph-based learning models, which can infer structural features from a design, into traditional non-graph-based learning models for delay estimates. The hybrid model improves delay prediction by 93\% in comparison to the delay prediction reported %given 
by a %the 
commercial HLS tool. 

\subsubsection*{\bf{Future directions}}
% \begin{itemize}
%     \item performance prediction for FPGA and, more in general, reconfigurable dataflow devices, is a challenging task. Traditionally, this was addressed by using analytical approaches, but more recently machine learning based approaches are becoming mainstream, and we can expect them to become a popular option
%     \item with CGRA-like architecture being commercialized, the interest in exploring these devices is increasing
%     \item we advocate the need for open-source tools (or at least reproducibility): performance prediction is a device-specific task, but very few of the papers reviewed release the code used. If we want to increase the chance of collaborations, and reproducibility, this has to change.
% \end{itemize}

Performance prediction for FPGAs and, more generally, reconfigurable dataflow devices is challenging. Traditionally, this was addressed with %by using 
analytical approaches, but more recently, machine-learning-based approaches are becoming mainstream, and we can expect them to become a popular option given their successes in recent research. With CGRA-like architectures being commercialized, the interest in exploring these devices is increasing, and we %should 
expect future work to focus more on the predictability of the 
performance of such devices. Performance prediction is a device-specific task, yet only a %fraction %very 
few 
of the reviewed papers favor the reproducibility of the presented results, or publicly release the % share the 
code used to generate these results. 
We should shift toward more reproducible and transparent research to foster %increase the chance of 
collaborations and facilitate general progress. Thus, we advocate for the need for open-source practices %also 
in performance prediction as well.
% Describe the future direction that this research, in the Netherlands, is likely to head into. This section should describe what ongoing research seems most promising and what we might expect in future research on this topic. This paragraph can also expand on potential applications that these developments can be applied to, giving insight into why this is important.
% - it is quite disjoint, so in case we can mention what's the trend in the world
% - it is also fine to say something about who is leading (e.g. work X was led by this dutch institution) or who contributed (in the last 5 years)
