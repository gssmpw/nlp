%This subsection contains all papers regarding FPGA use in dealing with big data, high performance computing and distributed computing.

%%%% This is a long text for intro, but we can polish it and provide a summary as it. Please feel free to revise the text and add/remove sentences:
\iffalse

In this section, we overview the research and activity related to FPGAs in HPC and data centers. In a broader view, even though GPUs are still the dominant accelerators, and AI-specific hardware is growing fast, but FPGAs also have recently gained attraction.

There are several reasons that FPGAs are emerging in HPC and data centers. First, in some cases, they are more cost and energy efficient compared to CPUs and GPUs, which is one of the most important factors in HPC centers nowadays.
Second, FPGAs are capable of direct I/O connection. For example they can directly attach to the network without host intervene through dedicated network stack implemented on FPGAs. 
Third, it enables spatial programming paradigm, e.g., data flow implementation, to reduce data movement (from memory to compute units) compared to the traditional control-based procedural programming~\cite{Licht2022PythonDesign}. 
Fourth, FPGAs as re-configurable hardware provide more flexibility to application developers to have HW-SW co-design and implement domain specific applications with their own constraint metrics. Although, there has been recently enormous effort to mitigate the drawback of programmability of these devices to software developers and end users, but in most cases it is still a challenge to extract good performance out of it and to yield an optimized implementation of an algorithm.

Major data centers such as Microsoft, Alibaba, Amazon, Baidu, Huawei, etc. benefit from FPGAs in their infrastructures~\cite{firestone2018azure,PutnamAServices,caulfield2016cloud,ernst2020competing,xilinx_alibaba}. Some of them only use FPGAs for their internal developed applications. For example in Microsoft Catapult project, FPGAs are used in Microsoft Bing search service by placing a re-configurable logic layer (i.e., FPGAs) between network switches and servers~\cite{PutnamAServices,caulfield2016cloud}. 
On the other hand, some of data centers expose FPGAs as a service to application developers, e.g., AWS. There is an overview in~\cite{Bobda2022TheCloud} of existing academic and commercial efforts of using FPGA acceleration in data centers. They discussed different aspects from architectures, scalability, abstractions to middleware, applications, security and vulnerability of these devices. 

On HPC side, we can also see that FPGAs are emerging as a different type of accelerators. For instance, Fugaku extends its supercomputer center with a scalable FPGA-cluster system~\cite{Sano2023ESSPER:Fugaku}. 
Under AMD university program~\cite{amd_hacc}, some research institutes such as Paderborn University, ETH Zurich, University of California, Los Angeles (UCLA), University of Illinois at Urbana Champagne (UIUC) and National University of Singapore (NUS) deploy Heterogeneous Accelerated Compute Clusters (HACCs).  All these clusters support adaptive computing by incorporating FPGAs in their compute nodes to accelerate scientific applications.   
On a different line of research, Intel and Vmware in collaboration with the University of Toronto, University of Texas at Austin, Carnegie Mellon University initiate Crossroads 3D-FPGA Academic Research Center~\cite{crossroads_fpga}. Their ambition is to define a role for FPGAs as a central function in future data center servers. All these activities indicate the importance role of FPGAs in the future of HPC ecosystem and data centers.

Along with the rapid adaptation of FPGA technology in HPC and data centers, whether it becomes a dominant accelerator is still an open question. One important factor will be the economic advantage, whether they provide more performance with less energy and hence cost for a variety of applications?


%%%%%%%%%%%%%%%%%%%%%%%%% Rephrasing above information with a story line:

In this section, we overview the research and activity related to FPGAs in HPC and data centers. The entrance of FPGAs in HPC and data centers bring a fundamental question: in which position FPGAs can play a significant role in the workflow of HPC and data centers?
%Where in the system design and in which role FPGAs can be beneficial in HPC and data centers workflow?
One historical position of FPGAs in this ecosystem is in the network and communication. This is due to the capability of these devices to have direct I/O connection (without host intervene) to attach to network components (e.g., switches and routers) through dedicated network stack implemented on FPGAs. For example in Microsoft Catapult project, FPGAs are used in Microsoft Bing search service by placing a re-configurable logic layer (i.e., FPGAs) between network switches and servers~\cite{PutnamAServices,caulfield2016cloud}. 

Another straight forward answer to the role of FPGAs is to be as a new type of accelerators sitting along with CPUs and GPUs in HPC and data centers compute nodes. This is due to re-configurability and flexibility of these devices which enables HW-SW co-design and implementation of domain specific applications with their own constraint metrics. Moreover, FPGAs as accelerators enable spatial programming paradigm, e.g., data flow implementation, to reduce data movement (from memory to compute units) compared to the traditional control-based procedural programming~\cite{Licht2022PythonDesign}. 
As an instance of FPGAs as accelerators, we can see that Fugaku extends its supercomputer center with a scalable FPGA-cluster system~\cite{Sano2023ESSPER:Fugaku}. 
Another example is AMD university program~\cite{amd_hacc} where some research institutes such as Paderborn University, ETH Zurich, University of California, Los Angeles (UCLA), University of Illinois at Urbana Champagne (UIUC) and National University of Singapore (NUS) deploy Heterogeneous Accelerated Compute Clusters (HACCs).  All these clusters support adaptive computing by incorporating FPGAs in their compute nodes to accelerate scientific applications.   
With all the efforts have been done to mitigate the drawback of programmability of these devices to software developers and end users, but in most cases it is still a challenge to extract good performance out of it and to yield an optimized implementation of an algorithm.

Intel and Vmware in collaboration with the University of Toronto, University of Texas at Austin, Carnegie Mellon University initiate Crossroads 3D-FPGA Academic Research Center to re-think and find a permanent solution for this question~\cite{crossroads_fpga}. Their ambition is to define a fix role for FPGAs as a central function in future data center servers. In their perspective, FPGAs will be at the heart of the servers as data movement and transformation engine between network, traditional compute units, accelerators and storage.

All these activities indicate the importance, but still ambiguous role of FPGAs in the future of HPC ecosystem and data centers. There is an overview in~\cite{Bobda2022TheCloud} of existing academic and commercial efforts of using FPGAs in data centers. Although this is still an open question, and different ad-hoc solutions have been proposed, but we can say with certainty that one important factor will be the economic advantage; i.e., whether they provide more performance with less energy and hence cost for a variety of applications?

In the rest of this section, we overview the landscape of the FPGA research within the Netherlands in four categories: big data processing and analytics, distributed computing, optical hardware communication and high performance computing.
\fi

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%A bit Shorter general background
In this section, we overview research and activities related to FPGAs in HPC and data centers. The use of FPGAs in HPC and data centers raises a fundamental question: in which position FPGAs can play a significant role in the workflow of HPC and data centers?
%Where in the system design and in which role FPGAs can be beneficial in HPC and data centers workflow?
One historical position of FPGAs in this ecosystem is in the network and communication. 
This is due to the direct I/O connection capabilities of these devices, allowing them to attach to network components (e.g., switches and routers) through a dedicated network stack directly implemented on FPGAs.
In the Microsoft Catapult project~\cite{caulfield2016cloud, PutnamAServices}, for instance, FPGAs are used in the Microsoft Bing search service as %by placing 
a re-configurable logic layer %(i.e., FPGAs) 
between network switches and servers. %~\cite{PutnamAServices,caulfield2016cloud}. 

Another straightforward answer to this question is to deploy %place 
FPGAs as dedicated %a new type of 
accelerators/co-processors. % along with CPUs and GPUs in HPC and data centers. 
Due to their re-configurability and flexibility, % of these devices, 
FPGAs enable hardware-software co-design and implementation of domain specific applications. Moreover, FPGAs as accelerators facilitate %the % enable 
spatial programming, % paradigm, 
e.g., dataflow implementations, to reduce data movement (from memory to compute units) compared to the traditional, control-based procedural programming~\cite{Licht2022PythonDesign}. 
As an instance of FPGAs as accelerators, Fugaku extends its supercomputer center with a scalable FPGA-cluster system~\cite{Sano2023ESSPER:Fugaku}. 
Another example is through the AMD university program~\cite{amd_hacc}, where some research institutes %all 
around the world deploy Heterogeneous Accelerated Compute Clusters (HACCs).  %All 
These clusters support adaptive computing by incorporating FPGAs in their compute nodes %in order 
to accelerate scientific applications.
Despite substantial efforts to improve the programmability of these devices for software developers and end users, achieving %extracting good 
high performance through %and achieving
an optimized implementation of an algorithm remains a significant challenge in most cases.
%Although there has been enormous effort to mitigate the programmability of these devices to software developers and end users, but in most cases it is still a challenge to extract good performance and to yield an optimized implementation of an algorithm.
Intel and Vmware, in collaboration with %some 
research institutes and universities, established the  Crossroads 3D-FPGA Academic Research Center~\cite{crossroads_fpga} to re-consider and find a permanent solution for this question. Their ambition is to define a fixed role for FPGAs as a central function in future data center servers. From %In 
their perspective, FPGAs will serve as %be at 
the core %heart 
of %the 
servers, acting as data movement and %data 
transformation engines between the network, traditional compute units, accelerators, and storage.


The aforementioned %All these 
activities indicate the important, yet %but still 
ambiguous role of FPGAs in the future of HPC ecosystems and data centers. ~\citet {Bobda2022TheCloud} provide %There is 
an overview %in 
of existing academic and commercial efforts of employing %using FPGAs 
in data centers. Among the commercial efforts, we observe that major data centers such as Microsoft, Alibaba, Amazon, Baidu, and Huawei %, etc. 
benefit from FPGAs in their infrastructures~\cite{firestone2018azure,PutnamAServices,caulfield2016cloud,ernst2020competing,xilinx_alibaba}. 
Although this is still an open question, and various %different 
ad-hoc solutions have been proposed, %but 
%can say with certainty that 
one important factor will be the economic advantage; %Specifically, 
it will depend on whether these solutions can deliver more performance with less energy consumption and lower costs across a range of applications.
%i.e., whether they provide more performance with less energy and hence cost for a variety of applications? 
The rest of this section presents an %, we 
overview of the FPGA research landscape %of the FPGA research with
in the Netherlands, organized into four categories: big data processing and analytics (\ref{sec:big-data-processing-analytics}), distributed computing (\ref{distcomp}), optical hardware communication (\ref{opthwcom}) and high performance computing (\ref{sec:high-performance-computing}).





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Other strong points of FPGAs:
% - Interfacing in general (e.g. Optical communication)
% - Network attached accelerators https://www.researchgate.net/publication/373405337_FPGA-Based_Network-Attached_Accelerators_-_An_Environmental_Life_Cycle_Perspective
% - SmartNICs https://www.intel.com/content/www/us/en/products/details/fpga/platforms/smartnic.html

\subsection{Big data processing and analytics} \label{sec:big-data-processing-analytics}
% This section discusses FPGAs as data center accelerators for big data processing and analytics. A relatively large amount of papers have been published on big data processing and analytics by Dutch organizations and in collaboration with Dutch organizations. 

% \paragraph{Background}
Several studies in Dutch academia 
 have assessed the domain of big data processing and analytics~\cite{Hoozemans2021FPGAOpportunities, Peltenburg2021GeneratingArrow, Rellermeyer2019TheProcessing, Fang2020In-memorySurvey}, identifying opportunities for FPGA accelerators %in this domain 
 and describing the challenges faced in the wide adoption of FPGA technology. \citet{Peltenburg2021GeneratingArrow} identify %the following %The 
 %main challenges: %identified
 %\cite{Peltenburg2021GeneratingArrow} include : 
 %\textbf{
 the programmability %} 
 of the accelerators, %\textbf{
 the portability %} 
 of the implementation, %\textbf{
 the interface design %} 
 to the data, and %\textbf{
 the infrastructure %} 
 for data movement to/from the accelerator and across % and between 
 %\textit{
 kernels %} 
 running on %in 
 the accelerator, as the main challenges. Solutions %are proposed 
 leveraging various existing technologies have been proposed, e.g., Apache Spark~\cite{ApacheSpark}, Apache Arrow Flight~\cite{ArrowFlight}, the IBM POWER architecture~\cite{7924241}, and OpenCAPI~\cite{OpenCAPI}, while applications 
%Application 
of FPGA accelerators in this domain %, in the Dutch research community, are found in the domains of 
involve database search~\cite{Fang2020In-memorySurvey}, real-time data analysis~\cite{Chrysos2019DataNode}, graph-based processing~\cite{Iosup2023GraphContinuum, Prodan2022TowardsEurope}, high-frequency trading~\cite{Chen2021FPGAAlgorithm}, DNA analysis~\cite{Voicu2019SparkJNI:Spark}, and machine learning~\cite{Rellermeyer2019TheProcessing}.

%: I/O challenges, data format specifications, (de-)compression, applications in graph processing, data base searches etc. 

%On fast retrieval of big data from memory and the communication of big data between hardware nodes through FPGAs. Also some applications of big data processing using FPGAs.

\subsubsection*{\bf{Research topics}}
Several challenges in using FPGAs effectively as accelerators for big data processing and analytics have been addressed by the Dutch research community.
% \begin{itemize}
%     \item \textbf{Interface design and Infrastructure}. Many data structures used in data bases are not well matched with the architecture of an FPGA, thus making processing on an FPGA inefficient. Apache Arrow Flight organizes data movement in a coherent and transparent way across various systems and applications. Fletcher \cite{Peltenburg2021GeneratingArrow}, \cite{Ahmad2022BenchmarkingMicroservices} extends Apache Arrow Flight towards FPGA and defines inter kernel infrastructure between processing kernels implemented in FPGA. Complementary work provides (on-line) data conversion from the often used Parquet \cite{Peltenburg2020BattlingFPGA} and JSON \cite{Peltenburg2021TensAccelerators} formats to Arrow. 
    
%     \item \textbf{Frameworks and tooling}. Addressing \textbf{Programmability}, \textbf{Portability}, \textbf{Interface design} and \textbf{Infrastructure} challenges. Several frameworks are developed to ease programming of FPGA accelerators for big data processing and analytics. \textit{Fletcher} integrates FPGA accelerators with tools and frameworks that use Apache Arrow in their back-ends \cite{Peltenburg2019Fletcher:Arrow}.
%     The open stream-oriented specification Tydi-spec \cite{Peltenburg2020Tydi:Streams} and language Tydi-lang \cite{Tian2022TydilangAL} are developed to specify and implement complex, dynamically sized data structures onto hardware streams.
%     \textit{SparkJNI} enables heterogeneous CPU - FPGA systems based on the Apache Spark unified engine for large-scale data analytics \cite{Voicu2019SparkJNI:Spark}.
%     The work by Abrahamse et al. \cite{Abrahamse2022Memory-DisaggregatedApplications} extends the ThymesisFlow \cite{9252003} memory disaggregration system with a framework leveraging IBM POWER9 and FPGA accelerators.
    
%     \item \textbf{Compression and Decompression}. Addressing the \textbf{Infrastructure} challenge. Both data storage size as well as data movement bandwidth from storage to data processor impose significant challenges in the efficient deployment of accelerators. Data compression is used to reduce both the data storage size and bandwidth requirements. However, compression and decompression of data requires a significant amount of resources. Efforts have been made to (de-)compress data on FPGA either to process the data directly on FPGA or to send them to another component in a system for further processing or storage. Implementations are made based on the Snappy \cite{Fang2019AModel}, LZ77 \cite{Fang2020AnLogic} and Zstd \cite{Chen2021FPGAAlgorithm} (de-)compression algorithms. Also, an energy-efficient co-processor, supporting a range of decompression algorithms, was designed and tested on FPGA \cite{Hoozemans2021EnergyASIP}.
% \end{itemize}
\paragraph{Interface design and infrastructure} Many data structures used in databases do %are 
not map well to the architecture of an FPGA, for example the alignment of data format or the method of data retrieval, %matched with the architecture of an FPGA, 
thus making processing on an FPGA inefficient. Apache Arrow Flight~\cite{ArrowFlight} organizes data movement in a coherent and transparent way across various systems and applications. Fletcher~ \cite{Peltenburg2021GeneratingArrow, Ahmad2022BenchmarkingMicroservices} extends Apache Arrow Flight with FPGA support %for %towards 
%FPGAs 
and defines inter-kernel infrastructure between 
processing kernels implemented in FPGA. Complementary work provides (on-line) data conversion from the widely %often 
used Parquet \cite{Peltenburg2020BattlingFPGA} and JSON \cite{Peltenburg2021TensAccelerators} formats to Arrow. 

\paragraph{Frameworks and tooling} %Addressing %\textbf{Programmability}, \textbf{Portability}, \textbf{Interface design} and \textbf{Infrastructure} challenges. 
Several frameworks have been %are 
developed to ease the programming of FPGA accelerators for big data processing and analytics. %\textit{
Fletcher %} 
integrates FPGA accelerators with tools and frameworks that use Apache Arrow as their back-end~\cite{Peltenburg2019Fletcher:Arrow}.
The open stream-oriented specification Tydi-spec \cite{Peltenburg2020Tydi:Streams} and language Tydi-lang \cite{Tian2022TydilangAL} are developed to specify and implement complex, dynamically sized data structures onto hardware streams.
%\textit{
SparkJNI %} 
enables heterogeneous CPU-FPGA systems based on the Apache Spark unified engine for large-scale data analytics \cite{Voicu2019SparkJNI:Spark}, while 
%The work by Abrahamse et al. 
\citet{Abrahamse2022Memory-DisaggregatedApplications} extend the ThymesisFlow \cite{9252003} memory disaggregration system with a framework leveraging IBM POWER9 and FPGA accelerators.

\paragraph{Compression and decompression} %Addressing the \textbf{Infrastructure} challenge. 

Both the data storage size and the bandwidth required to move data from/to storage %to the processor 
present significant challenges in efficiently deploying accelerators. Data compression is used to mitigate these challenges by reducing both storage size and bandwidth requirements. However, the processes of 
compression and decompression require considerable resources, and efforts have been undertaken to (de-)compress data on FPGAs, either to enable direct data processing on the FPGA itself, or to facilitate data transfers to another system component for further processing or storage. Solutions %Implementations are made 
based on various (de-)compression algorithms have been presented, such as Snappy~\cite{Fang2019AModel}, LZ77~\cite{Fang2020AnLogic} and Zstd~\cite{Chen2021FPGAAlgorithm}. % (de-)compression algorithms. 
%In addition, 
\citet{Hoozemans2021EnergyASIP} present an energy-efficient, FPGA-based co-processor that supports several %a range of 
decompression algorithms. %, was designed and tested on FPGA .

\subsubsection*{\bf{Future directions}}

Research to develop frameworks that enable the efficient use of FPGA accelerators for big data processing and analytics is ongoing. By adopting high-level workflows tailored to these tasks, FPGA accelerators are becoming increasingly applicable within general data center infrastructures and applications. 
%Many work is ongoing on frameworks enabling efficient use of FPGA accelerators for big data processing and analytics. With a higher level workflow suited to big data processing and analytics, FPGA accelerators are more likely to be applied in general data centre infrastructure and applications. 
We see the work referred to in section \ref{sec:big-data-processing-analytics} being continued, %especially the work from the Accelerated Big Data Systems group at TU Delft with IBM and AMD,
as well as being extended with other partners in the industry %, e.g.,  %such as Voltron Data
%for example in 
\cite{10.1145/3624062.3624541, 10305451, Reukers2023AnIR, groet2024leveraging}.
Furthermore, one can not overstep the current rise of ML and AI, which, when applied to big data processing and analytics \cite{Rellermeyer2019TheProcessing} can benefit from FPGA acceleration \cite{10.1145/3613963}. The above listed technologies being developed in the Netherlands can enable the use of FPGAs as accelerators for ML and AI in big data analytics. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Distributed computing}
\label{distcomp}
% This section discusses the topic of distributed and decentral computing to accelerate computations through the use of (multiple) FPGAs.

% \subsubsection{Background}
Distributed computing involves the deployment of multiple computing nodes in parallel to increase performance and solve large computational problems. %is a common approach where either we hit the resource limit within a compute node due to large problem size, or we aim at increasing the performance by using different compute resources in parallel. 
%Distributed computing is a natural approach to mitigate the challenge of resources limitation within a physical compute node. 
%When we encounter a situation where the size of the problem is much larger to keep in one compute node, distributing different computation elements to different compute nodes is an intuitive solution. Moreover, we can also distribute the computation workflow on different compute nodes in order to increase performance. 
%In contrast for CPUs and GPUs which use DRAM-based cache-hierarchy memory structure, FPGAs use various memory technologies such as LUTs, BRAM, URAM, HBM, etc. which have lower latency and higher bandwidth compared to DRAM. 
%Thus, FPGAs appear to be more efficient in memory-bound computation tasks. 
While %The field of 
research on distributed computing involving CPU and GPU nodes is well established, %for CPU, and also GPU nodes is not a new area. However, as 
the emergence of FPGAs %are emerging 
as a new type of computational resources and accelerators within data center infrastructures introduces a new and challenging area of research. %, the research in this direction is new and challenging. 
The Dutch academia has mainly focused on applications that use distributed multi-FPGA systems for %
%The applications that in Dutch academia are investigated using distributed multi-FPGAs are 
large-scale graph processing~\cite{Sahebi2023DistributedFPGAs} and deep neural networks (DNNs)~\cite{Alonso2021Elastic-DF:Partitioning}.  

\subsubsection*{\bf{Research topics}}
%There are 
Several research topics have been %that are 
investigated by Dutch researchers. 

% \begin{itemize}
%     \item \textbf{Communication overhead}. Reducing communication is a key factor in distributed computing, and in particular in multi-FPGA systems. By reducing communication overhead, computation time and latency reduces and efficiency increases. In order to reach this goal, researchers propose interconnection frameworks to establish flexible, reliable, efficient and custom communication protocols in multi-FPGA systems~\cite{salazar2020plasticnet,Salazar-Garcia2021PlasticNet+:Transceivers,Salazar-Garcia2022AApplications}. In addition to reducing latency, these proposed frameworks are designed to work with different topology schemes, and different FPGA technologies.      
    
%     \item \textbf{Partitioning and performance scaling}. 
%     In order to increase performance in multi-FPGAs systems, researchers propose an open-source distributed resource partitioning and allocator tool on FPGAs for data flow architectures targeting DNN inference which works in conjunction with FINN compiler. They demonstrate their methodology enables super-linear scaling of throughput, by benefiting from model parallelism and direct FPGA-FPGA communication~\cite{Alonso2021Elastic-DF:Partitioning}.
%     Another different research proposes a (multi-FPGA) framework to process large-scale graph processing. The framework uses an offline partitioning mechanism, and it uses Hadoop to map the graph into the underlying hardware. They show that graph partitioning using FPGA architecture results in better performance on large graphs included millions of vertices and billions of edges. Their results indicate a significant speed-up compared to the state-of-the art CPU, GPU and FPGA solutions~\cite{Sahebi2023DistributedFPGAs}.  


    
% \end{itemize}
\paragraph{Communication overhead} Reducing communication is a key factor in distributed computing, and in particular in multi-FPGA systems. By reducing communication overhead, computation time and latency reduces and efficiency increases. To reach this goal, researchers propose interconnection frameworks to establish flexible, reliable, efficient and custom communication protocols in multi-FPGA systems~\cite{salazar2020plasticnet,Salazar-Garcia2021PlasticNet+:Transceivers,Salazar-Garcia2022AApplications}. In addition to reducing latency, these %proposed 
frameworks are designed to work with various %different 
topology schemes and different FPGA technologies.

\paragraph{Partitioning and performance scaling} %In order 
To increase performance in multi-FPGA systems, \citet{Alonso2021Elastic-DF:Partitioning} %researchers have 
propose an open-source, distributed resource partitioning and allocator tool on FPGAs for data flow architectures targeting DNN inference; it %which 
works in conjunction with the FINN compiler~\cite{umuroglu2017finn}. The authors show that %demonstrate 
their methodology enables super-linear scaling of throughput by benefiting from model parallelism and direct FPGA-FPGA communication. %~\cite{Alonso2021Elastic-DF:Partitioning}.
\citet{Sahebi2023DistributedFPGAs} %Another different research 
propose a (multi-FPGA) framework for %to process 
large-scale graph processing. The framework uses an offline partitioning mechanism, and relies on %it uses 
Hadoop to map the graph into the underlying hardware. The authors show that graph partitioning using an FPGA architecture results in better performance on large graphs that include millions of vertices and billions of edges. Their results indicate %a 
significant speed-ups over %compared to the 
state-of-the art CPU, GPU, and FPGA solutions.


\subsubsection*{\bf{Future directions}}

There are several %different 
challenges in distributed computing using multi-FPGA systems, thereby %which 
necessitating %requiring 
further research in this direction. For instance, overcoming communication barriers and designing protocols for FPGA-FPGA communication is an ongoing research domain. Moreover, at the application level, developing (standard) MPI-like collective communication libraries for multi-FPGA systems would be beneficial. %desired. 
Also, %We also need 
more case studies are needed in order to investigate and design efficient partitioning and workload distribution schemes %and %assigning different 
%distribute computational tasks 
for FPGA resources. Therefore, to bring ease-of-use and automation for distributed computing on FPGAs, developing libraries and tools  is crucial. 


\iffalse
\subsubsection{PlasticNet: A low latency flexible network architecture for interconnected multi-FPGA systems; PlasticNet+: Extending multi-FPGA interconnect architecture via Gigabit transceivers}
The paper focuses on addressing the challenges of multi-FPGA system communication. They propose an extension over PlasticNet framework via flexible, efficient, reliable, custom protocol. PlasticNet framework is a FPGA interconnect architecture of processing units for both within a board or among neighboring FPGA boards. Their extended proposal improves PlasticNet over area, channel overhead and latency. They evaluate their approach using a ring-based topology of Zynq ZC706 FPGA boards. They report the best-case latency of 300 ns which is half the latency of an Ethernet 10G link. Another advantage of the proposed approach is the adaptability to a wider range of interconnect topologies.

\subsubsection{Elastic-DF: Scaling Performance of DNN Inference in FPGA Clouds through Automatic Partitioning}
Power dissemination and multi-tenancy are two important factors for data center operators. FPGAs gains attraction to data centers, in particular for Deep Neural Networks (DNN) inference applications, as they are well aligned with the two factors. However, it is challenging as DNN inference on FPGA requires the right choice of architecture and a set of implementation tools. Therefore, the authors proposed an open-source distributed resource partitioning and allocator tool on FPGAs for data flow architectures targeting DNN inference which works in conjunction with DFA compiler FINN. They experiment their approach on FPGA cluster at ETH Zurich. It includes four FPGA-equipped nodes; a mix of Alveo U250 and U280 (10 cards in total). The FPGAs are connected using 100 Gb/s interfaces. They use different FPGA accelerator deployment software: XRT, PYNQ, Jupyter Lab. Dask, and InAccel Coral. They evaluated several DNN accelerator implementations of FINN-generated DF accelerators for quantized MN and RN-50 classifiers. They demonstrate their methodology enables super-linear scaling of throughput, by benefiting from model parallelism and direct FPGA-FPGA communication over 100 G bps Ethernet connection. Concretely, they show 44\% latency decrease on U280 for ResNet-50, and 78\% throughput increase on U200 and U280 for MobileNetV1.

\subsubsection{A custom interconnection multi-FPGA framework for distributed processing applications}
One of the main challenges in FPGA clusters is to reduce communication overhead between network elements to reduce computation time and maximize efficiency of processing elements on FPGAs. Therefore, the authors of the paper propose a multi-FPGA interconnection framework targeting distributed applications. Thus, they build multi-FPGA systems included 5 Zynq ZC706 FPGAs over their custom network. They assume an application can be accelerated by decomposing its computation and distributed into different processing elements on a multi-FPGA machine. The authors show the effectiveness of their framework using matrix multiplication algorithm. With the aggregated bandwidth of 25 Gbps per FPGA, their framework shown the latency of 200 ns, an efficiency of 97.25\% and throughput of 21.4 GFLOPS. Another advantage of their approach is portability of the proposed network interconnect to newer generation of FPGAs.

\subsubsection{Distributed large-scale graph processing on FPGAs}
Large-scale graph processing is challenging and causes performance degradation due to irregular structure and memory access on both CPUs and GPUs. The authors propose a FPGA engine, as part of a framework to overlap and hide data transfers in order to maximize utilization of FPGA accelerator. The framework uses an offline partitioning mechanism, and it uses Hadoop to map the graph into the underlying hardware. They show that graph partitioning using FPGA architecture results in better performance on large graphs included millions of vertices and billions of edges. They benefit from the partitioning scheme in GridGraph library, but on FPGA instead. They use Xilinx Vivado HLS toolchain for their implementation on Alveo U250 card. Their optimized implementation of the PageRank for a single FPGA outperforms state-of-the art CPU, GPU and FPGA solutions: a speed up to GridGraph by 2x, Cugraph by 4.4x and VITIS LIB by 26x. Even when the size of graphs limit the performance of a FPGA, their approach shows a speed up about 12x using multi-FPGAs. 
\fi
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\subsection{Optical hardware communication}
\label{opthwcom}
% This section examines the use of FPGA-based systems in optical hardware communication, a field that is gaining traction for its potential to revolutionize data center network (DCN) infrastructures.

% \subsubsection{Background}
Optical hardware communication is at the forefront of addressing the critical challenges faced by contemporary data center network (DCN) infrastructures, such as bandwidth limitations, latency issues, and scalability concerns. Optical communication is a viable alternative to conventional electrical data pathways, offering significant improvements in terms of efficiency and performance. The integration of FPGAs into optical communication systems has been a key development, providing the necessary flexibility and speed for dynamic network reconfiguration and management.

\subsubsection*{\bf{Research topics}}
The exploration of optical hardware communication utilizing FPGAs encompasses a variety of innovative research topics covered by Dutch organizations.
\paragraph{Optical wireless datacenter networks}
%Implementing semiconductor optical amplifier (SOA)-based wavelength selectors and arrayed waveguide grating routers (AWGRs) controlled by fast FPGA-based switch schedulers. 
\citet{Zhang2022Low-LatencyRouter} have developed an optical wireless (OW)-DCN architecture that promises enhanced flexibility and scalability for DCNs, supporting high-speed optical packet-switching transmissions. FPGA-based switch schedulers are used for control of the implementation based on semiconductor optical amplifier (SOA)-based wavelength selectors and arrayed waveguide grating routers (AWGRs).

\paragraph{Disaggregated optical networks}
The DACON project~\cite{Guo2022DACON:Invited} introduces a Disaggregated, Application-Centric Optical Network that utilizes hybrid optical switches and FPGA-based controllers, resulting in improved application performance and reduced latency.

\paragraph{Low-latency edge networks}
The Electro-Optical Communication group at TU/e has proposed an edge data center network architecture that employs photonics and FPGA-based supervisory channels to achieve microsecond-time control and deterministic latency \cite{Santana2023SOA-BasedApplications}.

\paragraph{Nanosecond optical switching}
A novel optical switching and control system has been designed to address the bandwidth bottlenecks of electrical switching, featuring a distributed network architecture with optical label channels and the Optical Flow Control (OFC) protocol \cite{Xue2022NanosecondNetworks}.

\paragraph{Hybrid datacenter architectures} The HiFOST DCN architecture~\cite{Yan2018HiFOST:Switches} integrates flow-controlled fast optical switches with modified top-of-the-rack switches, offering substantial improvements in latency and cost efficiency.

\paragraph{Beyond 5G networks} \citet{Santana2022TransparentApplications} present a new Edge Cloud Network design %has been put forward, 
that uses %utilizing 
FPGA-based controllers for rapid reconfiguration of optical networks, catering to the low-latency requirements of 5G applications and beyond.  %\cite{Santana2022TransparentApplications}.

% \begin{itemize}
%     \item \textbf{Optical Wireless Data-Center Networks}. Implementing semiconductor optical amplifier (SOA)-based wavelength selectors and arrayed waveguide grating routers (AWGRs) controlled by fast FPGA-based switch schedulers. Researchers have developed an optical wireless (OW)-DCN architecture that promises enhanced flexibility and scalability for DCNs, supporting high-speed optical packet-switching transmissions.\cite{Zhang2022Low-LatencyRouter}

%     \item \textbf{Disaggregated Optical Networks}. The DACON project introduces a Disaggregated, Application-Centric Optical Network that utilizes hybrid optical switches and FPGA-based controllers, resulting in improved application performance and reduced latency.\cite{Guo2022DACON:Invited}

%     \item \textbf{Low-Latency Edge Networks}. The Electro-Optical Communumuroglu2017finnumuroglu2017finnication group at TU/e has proposed an edge data center network architecture that employs photonics and FPGA-based supervisory channels to achieve microsecond-time control and deterministic latency.\cite{Santana2023SOA-BasedApplications}

%     \item \textbf{Nanosecond Optical Switching}. A novel optical switching and control system has been designed to address the bandwidth bottlenecks of electrical switching, featuring a distributed network architecture with optical label channels and the Optical Flow Control (OFC) protocol.\cite{Xue2022NanosecondNetworks}

%     \item \textbf{Hybrid Data Center Architectures}. The HiFOST DCN architecture integrates flow-controlled fast optical switches with modified top-of-the-rack switches, offering substantial improvements in latency and cost efficiency.\cite{Yan2018HiFOST:Switches}

%     \item \textbf{Beyond 5G Networks}. A new Edge Cloud Network design has been put forward, utilizing FPGA-based controllers for rapid reconfiguration of optical networks, catering to the low-latency requirements of 5G and beyond applications.\cite{Santana2022TransparentApplications}

% \end{itemize}

\subsubsection*{\bf{Future directions}}
% The field of optical hardware communication is poised for significant advancements, with ongoing research directed towards:
% \begin{itemize}
%     \item \textbf{Visible Light Communications (VLC)}. Efforts to mitigate LED nonlinearity have led to the development of a Legendre-polynomials-based post-compensator optimized for FPGA implementation, enhancing the bit rate efficiency of high-speed VLC systems.\cite{Niu2021LEDCommunications}
%     \item \textbf{Real-Time LED Modeling}. The introduction of a real-time FPGA-based implementation of a nonlinear LED model and post-compensator marks a substantial contribution to VLC technology, enabling high data rates over bandwidth-limited LEDs.\cite{Deng2022Physics-BasedImplementation}
%     \item \textbf{Concurrency-Aware Mapping in HPC}. A concurrency-aware mapping technique has been developed to reduce optical packet collisions in Architecture-On-Demand  (AoD) network infrastructures, improving buffer utilization and execution time degradation in HPC systems.\cite{Meyer2018OpticalPerformance}
% \end{itemize}

Through various ongoing developments, the field of optical hardware communication is poised for significant advancements. Efforts to mitigate LED nonlinearity have led to the development of a Legendre-polynomials-based post-compensator optimized for FPGA implementation, enhancing the bit rate efficiency of high-speed Visible Light Communications (VLC) systems \cite{Niu2021LEDCommunications}. The introduction of a real-time FPGA-based implementation of a nonlinear LED model and post-compensator marks a substantial contribution to VLC technology, enabling high data rates over bandwidth-limited LEDs \cite{Deng2022Physics-BasedImplementation}. 
A concurrency-aware mapping technique has been developed to reduce optical packet collisions in Architecture-On-Demand  (AoD) network infrastructures, improving buffer utilization and execution time degradation in HPC systems \cite{Meyer2018OpticalPerformance}.


%\cite{mendely.bib key}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{High performance computing}\label{sec:high-performance-computing}
% In this section, we discuss the research of utilizing FPGAs in HPC ecosystem by Dutch researchers.

%\subsubsection{Background}
Benefiting from FPGAs in %for 
HPC applications is an active research area. % nowadays. 
Even though GPUs remain %are still 
the most prevalent %dominant 
accelerator technology in HPC, and AI-specific hardware is being  increasingly adopted, %growing fast, but 
FPGAs are also %have recently gained attraction, and even we can observe (experimental) FPGA deployments on 
increasingly employed in HPC centers. 

\subsubsection*{\bf{Research topics}}
Dutch institutes have been involved in European projects, e.g., %such as 
ExaNeSt~\cite{Katevenis2018NextDevelopment} and MANGO~\cite{Flich2018ExploringApproach}, to %in 
design %ing of 
large-scale heterogeneous compute systems. We can observe 
the important role of FPGAs in these projects, facilitating network communication or accelerating execution. % as either considering in the network or as accelerators: 

% \begin{itemize}
%     \item \textbf{Architecture and system design}.
\paragraph{Architecture and system design}
    The %At 
    ExaNeSt European project~\cite{Katevenis2018NextDevelopment} deploys FPGAs %are proposed 
    as accelerators in % for 
    a European 
    exascale supercomputer based on low-cost, low-power %many 
    ARM cores. They also employ an FPGA-based testbed for a low-latency, high bandwidth unified Remote Direct Memory Access (RDMA) interconnect, and present %hey design 
    a custom FPGA-based switch to support inner-cabinet communications.
   The MANGO project~\cite{Flich2018ExploringApproach} aims at addressing the PPP (Performance, Power, and Predictability) space in HPC %: Performance, Power and Predictability 
   by exploring %and investigating 
   customizabe and deeply heterogeneous accelerators. Their hardware concept consists of General-purpose compute Nodes (GNs) with %, having 
   commercial accelerators such as Xeon Phi and NVIDIA GPUs, along with Heterogeneous Nodes (HNs). HNs are clusters of many-core chips coupled with customized heterogeneous computing resources, including high-capacity clusters of FPGAs.
    
    % \item \textbf{Programming languages, tools and applications}. 
\paragraph{Programming languages, tools, and applications}
    Within the ExaNeSt project, \citet{Katevenis2018NextDevelopment} %, they 
    design a novel microarchitecture as Top-of-Rack switches. In one of their experiment, they port the OpenCL kernels of the molecular dynamics simulator LAMMPS~\cite{plimpton1995fast} to FPGAs using HLS tools. 
    %They report that the use of an FPGA improves %the 
    %performance compared to using ARM cores, more than a factor of two.
    They report that running the kernel on an FPGA requires 0.56 seconds while the 4 ARM cores requires 1.3 seconds. That is an improvement of more than a factor 2 in speed up.
    Within the MANGO project, ~\citet{Flich2018ExploringApproach} target three applications with significant QoS aspects: 1) online video transcoding, 2) rendering for medical imaging, and 3) error correcting codes in communication. The MANGO project relies on LLVM~\cite{lattner2004llvm} and their programming model is an extension of %the 
    existing languages and libraries (e.g., OpenCL~\cite{opencl}) for HPC by integrating the expression of new architectural features as well as QoS concerns and parameters. This is achieved %They do it 
    by augmenting the runtime library API with new functions, pragmas and keywords to the existing HPC languages (e.g., clang C/C++ frontend). 
    
    % \item \textbf{Performance Models}. 
\paragraph{Performance models}
    Combining the advantages of reconfigurability, dataflow computation, and heterogeneity results in %yields 
    Reconfigurable Dataflow Platforms (RDPs) as a promising building block in %the 
    next-generation, large-scale high-performance machines. RDPs rely on %include 
    Reconfigurable Dataflow Accelerators (RDAs) to realize multiple streaming pipelines, each % which each 
    comprising many parallel operations. Due to the % such 
    heterogeneous hierarchy, 
    performance prediction of RDPs is very challenging, in particular to detect bottlenecks within reasonable time and accuracy. %Therefore, 
\citet{Yasudo2021AnalyticalPlatforms,Yasudo2018PerformancePlatforms} %Dutch researchers have been involved in a project to 
propose a performance estimate framework for reconfigurable dataflow applications %(i.e., RDPs), 
    named Performance Estimation for Reconfigurable Kernels and Systems (PERKS). %~\cite{Yasudo2021AnalyticalPlatforms,Yasudo2018PerformancePlatforms}. 
    It %PERKS 
    automatically extracts specific parameters from the application, the hardware, and the platform to calibrate the model. They use eight applications for their evaluation: AdPredictor (an online machine learning algorithm), N-body simulation, Monte Carlo simulation, sequence alignment, Asian option pricing, Jacobi solver, and Regression/regularisation solver. Their results show that PERKS achieves %performs an 
    accuracy of 91\% on these applications.
% \end{itemize}

\subsubsection*{\bf{Future directions}}
\iffalse
Determining the role of FPGAs in HPC necessitates %demands 
more research and %it 
raises several %many 
questions, such as 
1) Do we have to find a permanent position for FPGAs in HPC ecosystems to maximize their impact? % the most? 
If so, what would that position be, %Where would be that position 
from both architectural system design and application workflow perspectives?,
2) How can we bridge %fill out 
the gap between software developers and FPGA programming models and tools? Should we focus on HLS approaches or compiler-specific tools, or a combination of both?,
%Through HLS approach or compiler specific tools?
3) What types of HPC applications can benefit from FPGAs?, and last but not least 4) Are FPGAs cost-efficient in terms of energy and performance to warrant a permanent position in future HPC centers? 
%Whether FPGAs would be cost efficient in terms of energy and performance to dictate a permanent position in HPC centers? 
Further research and case studies are required to obtain %gain 
more insights in order to answer %address 
these questions. This future direction and the corresponding outcomes will indicate how important FPGAs will be in the future of HPC and data centers. 
\fi

Determining the role of FPGAs in HPC necessitates more research from both data center architecture design and FPGA programming model.
From a data center design perspective, the positioning of FPGAs in the architecture of HPC centers needs more investigation. This also depends on the targeted application workflow and how FPGA can impact the most.    
From a user perspective, the programmability of these devices is an important factor. Therefore, the gap between software developers and FPGA programming models and tools should be reduced further to use FPGA as mainstream HPC devices.

\iffalse
\subsubsection{Next generation of Exascale-class systems: ExaNeSt project and the status of its interconnect and storage development}
ExaNeSt European project merges industry and academia in the area of system cooling, storage, network and interconnect, and HPC applications. They aim to develop system-level interconnect and distributed non-volatile memory storage for a European exascale supercomputer based on low cost and power many ARM cores and computing accelerators implemented in programmable components (FPGAs). In this paper they explain the project in terms of hardware architecture and software stack development. The breakdown of the components in this project is as follows: 1) A low-latency, high bandwidth unified RDMA interconnect. They use FPGA-based testbed for this purpose. 2) Providing low-latency inter-process communication as needed on HPC workflows. 3) A novel distributed storage architecture. 4) A set of exascale scientific applications such as MonetDB and LAMMPS 5) Packaging and advanced cooling system.
The ExaNeSt interconnect has three components: 1) Network interface which bridges the processes that run on ARM cores with the communication layer of the network. 2) Intra-rack network IP based on APEnet which provides switching and routing features and manages communications over links. 3) A novel micoarchitecture as Top-of-Rack switches. They design a custom FPGA-based switch to support inner-cabinet communications.
The unit of the system is the Xilinx Zynq UltraScale+ FPGA integrating four 64 bit ARMv8 Cortex-A53 hard-cores running 1.5 GHz. 
In one of their experiment, they port the OpenCL kernels of LAMMPS to FPGA using HLS tools. They report the use of FPGA improves the performance significantly compared to using ARM cores, more than a factor of 2. 

\subsubsection{Exploring manycore architectures for next-generation HPC systems through the MANGO approach}
The paper explains the main approach and architectural solution, application scenario and software stack in MONGO project. The MANGO project aims at addressing the PPP space in HPC: Performance, Power and Predictability by exploring and investigating customizabe and deeply heterogeneous accelerators. They target three applications with significant QoS aspects: 1) Online video transcoding 2) Rendering for medical imaging 3) Error correcting code in communication. Their hardware concept consists of General-purpose compute Nodes (GNs), having commercial accelerators such as Xeon Phi and NVIDIA GPUs, along with Heterogeneous Nodes (HNs). HNs are clusters of manycore chips coupled with customized heterogeneous computing resources. Their deployment platform consists of 16 GNn with standard processors, e.g., Intel Xeon E5 and Kepler GPUs, and 64 HNs consists of ASIC ARM cores and high-capacity cluster of FPGAs. GNs and HNs are connected via infiniband. Their programming model is an extension of the existing languages and libraries for HPC by providing new architectural features and QoS concerns and parameters. They do it by augmenting the runtime library API with new functions, pragmas and keywords to the language.

\subsubsection{Analytical Performance Estimation for Large-Scale Reconfigurable Dataflow Platforms; Performance Estimation for Exascale Reconfigurable Dataflow Platforms}
Combining the advantages of reconfigurability, dataflow computation and heterogeneity yields Reconfigurable Dataflow Platforms (RDPs) as promising building block in the next generation of large-scale high-performance machines. RDPs include Reconfigurable Dataflow Accelerators (RDAs). Various hardware, computation, storage and communication elements are costumized for a specific hardware design to implement an algorithm. As such performance prediction of RDPs becomes very challenging, in particular to detect bottlenecks within reasonable time and accuracy. The authors of the paper propose a performance estimate framework for reconfigurable dataflow applications (i.e., RDPs), named Performance Estimation for Reconfigurable Kernels and Systems (PERKS). PERKS automatically extract specific parameters from the application, hardware and platform to calibrate the model. PERKS allows to predict performance of multi-accelerator systems using analytical model along with machine and application parameters. Their experimental setup is RDPs from Maxeler, known as Maxeler DFEs. A node of 12-core Intel Xeon CPU connects to 4 DFEs (via PCI Express). Each DFE has a Xilinx v6-SXT475 FPGA and 48 GB of DRAM. They use 8 applications for their evaluation: AdPredictor (an online machine learning algorithm), N-body simulation, Monte Carlo simulation, sequence alignment, Asian option pricing, Jacobi solver, and Regression/regularisation solver. Their results show that PERKS performs an accuracy of 91\% on current reconfigurable workloads.
\fi

%1. One paragraph summary (of each paper):
%        What is the contribution
%            system design/architecture (FPGA position), e.g. PCIe device, network attached accelerator etc.
%            Application/case study/technology development
%            In case of accelerator, what is the workflow. In case of infrastructure 
%            Product/research? How does this relate to research topics?
%               If it's a product, which research groups did contribute to it
%               open source?, actively maintained? 
%           Is the research used somewhere?
%        Advantages/Disadvantages
%        Forward look, where do we see this going in the future?

% Paragraph:
% Publication title
% short answers to above questions

 

%2. Merge/combine all ones into one (sub)section: Deadline March 31

