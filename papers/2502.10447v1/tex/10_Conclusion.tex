\section{Conclusion}

In this study, we propose \ourmodel, a hierarchical MoE framework for AVSR, designed to enhance scalability and robustness. By training an inter-modal router that dynamically assigns weights to audio and visual expert groups, \ourmodel enables an adaptive group selection based on input context. Evaluations on robust AVSR benchmarks demonstrate its state-of-the-art performance, with superior noise resilience, further supported by flexible expert load distributions across diverse noisy conditions. This work establishes an adaptive modality-aware MoE paradigm, advancing larger-scale multimodal speech recognition systems.