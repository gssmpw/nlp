\section{Related Work}
\label{sec:related_work}

\subsection{Robustness of Audio-Visual Speech Recognition} 

The robustness of AVSR systems has significantly advanced by integrating auditory and visual cues to improve speech recognition, especially in noisy environments. Conventional ASR methods have evolved from relying solely on audio signals \cite{schneider2019wav2vec, gulati2020conformer, baevski2020wav2vec, hsu2021hubert, chen2022wavlm, chiu2022self, radford2023robust} to incorporating visual data from speech videos \citep{makino2019recurrent}.
The multimodal AVSR methods \citep{pan2022leveraging, shi2022learning, seo2023avformer, ma2023auto} have enhanced robustness under audio-corrupted conditions, leveraging visual details like speaker's face or lip movements as well as acoustic features of speech. These advancements have been driven by various approaches, including end-to-end learning frameworks \citep{dupont2000audio, ma2021end, hong2022visual, burchi2023audio} and self-supervised pretraining \citep{ma2021lira, qu2022lipsound2, seo2023avformer, zhu2023vatlm, kim2025multitask}, which focus on audio-visual alignment and the joint training of modalities~\citep{zhang2023self, lian2023av, haliassos2022jointly, haliassos2024braven}.


Furthermore, recent advancements in AVSR highlight the importance of visual understanding alongside audio \citep{dai2024study, kim2024learning}. While initial research primarily targeted audio disturbances \citep{shi2022robust, hu2023hearing, hu2023cross, chen2023leveraging}, latest studies increasingly focus on the visual robustness to address challenges such as real-world audio-visual corruptions~\citep{hong2023watch, wang2024restoring, kim2025multitask} or modality asynchrony~\citep{zhang2024visual, fu2024boosting, li2024unified}. These efforts remark a shift towards a more balanced use of audio and visual modalities. Yet, there has been limited exploration in scaling model capacity or introducing innovative architectural designs, leaving room for further developments in AVSR system that can meticulously balance audio and visual modalities.



\subsection{MoE for Language, Vision, and Speech Models}

Mixture-of-Experts (MoE), first introduced by \citet{jacobs1991adaptive}, is a hybrid structure incorporating multiple sub-models, \ie experts, within a unified framework. The essence of sparsely-gated MoE \cite{shazeer2017outrageously, lepikhin2021gshard, dai2022stablemoe} lies in its routing mechanism where a learned router activates only a subset of experts for processing each token, significantly enhancing computational efficiency. Initially applied within LLMs using Transformer blocks, this structure has enabled unprecedented scalability \cite{fedus2022switch, zoph2022st, jiang2024mixtral, guo2025deepseek} and has been progressively adopted in multimodal models, especially in large vision-language models (LVLMs) \cite{mustafa2022multimodal, lin2024moellava, mckinzie2025mm1}.
Among these multimodal MoEs, \citet{zhu2022uni, shen2023scaling, li2023pace, li2024uni} and \citet{lee2025moai} share the similar philosophy to ours, assigning specific roles to each expert and decoupling them based on distinct modalities or tasks. These models design an expert to focus on specialized segments of input and enhance the targeted processing.

Beyond its applications in LLMs and LVLMs, the MoE framework has also been applied for speech processing \cite{you2021speechmoe, you2022speechmoe2, hu2023mixture, wang2023language}, where it has shown remarkable effectiveness in multilingual and code-switching ASR tasks. In addition, MoE has been employed in audio-visual models \cite{cheng2024mixtures, wu2024robust}, although they primarily focus on general video processing and not specifically on human speech videos. These approaches leverage MoE to model interactions between audio and visual tokens without directly processing multimodal tokens.
Our research advances the application of the MoE framework to AVSR by designing a modality-aware hierarchical gating mechanism, which categorizes experts into audio and visual groups and effectively dispatches multimodal tokens to each expert group. 
This tailored design enhances the adaptability in managing audio-visual speech inputs, which often vary in complexity due to diverse noise conditions.
