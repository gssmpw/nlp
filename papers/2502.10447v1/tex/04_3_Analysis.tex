\subsection{Expert and Group Load Analysis}
\label{subsec:analysis_load}

\input{tex_table/muavic}

\paragraph{\ourmodel's expert load distribution.}
Figure\,\ref{fig:expert_load}(a) illustrates the expert load distribution of \ourmodel according to input types: audio-visual, audio-only, and video-only sequences. For audio-visual inputs, all experts from both the audio and visual groups are selected at similar frequencies, with some layer-dependent variations. In contrast, when processing audio-only sequences, the model predominantly activates the audio expert group, while for video-only sequences, the visual expert group is mainly utilized. This distribution validates the effectiveness of our load biasing loss in guiding the inter-modal router to assign appropriate weights based on input modality.

\vspace*{-10pt}
\paragraph{Expert group utilization in noisy AVSR.}
To analyze the effectiveness of hierarchical gating in AVSR, we first examine the limitations of hard routing (\S\ref{subsec:hard_routing}) under noisy conditions. Since hard routing relies on manually (de-)activating the audio and visual groups, for audio-visual inputs, we assign a fixed equal weight ($p^A, p^V = 0.5$) to both groups.
However, this equal weighting may not always be optimal in varying environments, such as noise type or intensity.

As shown in Figure\,\ref{fig:expert_load}(b), increasing reliance on the audio group under babble noise degrades performance, with an optimal weight for the audio group being $0.3$.
Unlike babble noise, which confuses the model with multiple overlapping speech signals, natural noise is more distinct from speech, leading to a higher reliance on the audio group ($p^A \ge 0.5$) preferable.
These results indicate that an ideal routing strategy for audio-visual data should be dynamically adjusted.

Figure\,\ref{fig:expert_load_noise} further illustrates \ourmodel's group load distribution across different noise levels. The model adaptively adjusts its reliance between the audio and visual expert groups---under high noise conditions (low SNRs), it shifts more tokens to the visual group, while in cleaner conditions (high SNRs), the audio group is more actively utilized. This behavior also adjusts to noise types, as observed with babble and natural noise, demonstrating the MoHAVEâ€™s adaptability and robustness.
%

\vspace{-5pt}
\subsection{Multilingual Audio-Visual Speech Tasks}
\vspace{-5pt}

MoEs have demonstrated effectiveness in multilingual speech tasks \citep{hu2023mixture, wang2023language}, as MoE is capable of enabling more diverse routing paths for different language tokens. To evaluate \ourmodel's multilingual capabilities, we train a multilingual model and assess its performance on the MuAViC benchmark~\citep{anwar2023muavic}, evaluating separately for each language.
%
Following \citet{han-etal-2024-xlavs}, we introduce multilingual babble noise at SNR 0\,dB to 50\% of the input samples during training, where the noise clips are sampled from the MuAViC train set. For inference, we apply beam search with a beam size of 5 and normalize text by punctuation removal and lower-casing before calculating WER. For AVS2TT evaluation, we use SacreBLEU\,\cite{post2018call} with its built-in \textit{13a} tokenizer. To simulate noisy test conditions, we inject babble noise sampled from the MuAViC test set.

Table\,\ref{tab:muavic} summarizes the results, where \ourmodel-\textsc{Large} achieves superior performance in both AVSR and AVS2TT. Whisper \cite{radford2023robust}, a leading multilingual ASR model, is known to perform poorly in noisy setup due to its lack of visual understanding for robustness. While multilingual AV-HuBERT \cite{anwar2023muavic} underperforms the state-of-the-art models like XLAVS-R \cite{han-etal-2024-xlavs}, we have re-implemented it using the pretrained model from \citet{choi2024av2av}, which has been pretrained on a significantly larger dataset including 7,000 hours of speech in 100+ languages. This model outperforms (38.9\% average WER) or remains competitive (18.5\% average BLEU) with much larger XLAVS-R 2B. When integrated with this version, \ourmodel further improves performance, achieving \textbf{37.4\% average WER} and \textbf{19.5\% average BLEU}, setting new benchmarks in almost every language being evaluated.
