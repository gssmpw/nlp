\section{Introduction}
\label{sec:intro}

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{tex_figure/figure1.pdf}
    \vspace{-23pt}
    \caption{Comparison of AVSR models based on standard Transformers (AV-HuBERT, \citealt{shi2022learning}), MoE, and MoHAVE, evaluated under babble noise.
    The MoE structure boosts the model capacity while maintaining the number of activations.
    \ourmodel-\textsc{Base}\,(359M) achieves similar performance to AV-HuBERT-\textsc{Large}\,(477M) while activating only 189M parameters.
    }
    \label{fig:figure1}
    \vspace{-5pt}
\end{figure}

%
Audio-visual speech recognition (AVSR)~\citep{noda2015audio, afouras2018deep, ma2021end, shi2022learning, hsu2022u, hu2023mir} has emerged as a pivotal technology in enhancing the robustness and accuracy of speech recognition systems, particularly in noisy environments. By integrating auditory and visual modalities, AVSR leverages the complementary information from both speech signals and lip movements~\citep{makino2019recurrent, ren2021learning, chen2023leveraging}, offering significant advantages over audio-only automatic speech recognition\,(ASR) approaches. This multimodal approach is indispensable in situations where auditory data alone is insufficient for reliable recognition.

%
Despite significant advances, AVSR systems have not kept pace with advancements in model scalability as seen in ASR \citep{radford2023robust} or large language models (LLMs) \citep{kaplan2020scaling, clark2022unified, achiam2023gpt}. Contemporary AVSR models, such as AV-HuBERT \citep{shi2022learning}, AV-data2vec \citep{lian2023av}, and Auto-AVSR \citep{ma2023auto}, generally employ fewer than 0.5B parameters, a stark contrast to large-scale ASR models like Whisper~\citep{radford2023robust} or Seamless~\citep{barrault2023seamlessm4t} which boasts up to 1.6B and 2.3B parameters, respectively. This disparity is not merely a matter of size but reflects a fundamental challenge in AVSR scalability: increasing the model size often disproportionately enhances audio semantic understanding without similarly improving visual processing capabilities~\citep{dai2024study, kim2024learning}. Moreover, the computational complexity and latency of larger models pose challenges for efficient deployment, especially in scenarios where AVSR users often require rapid processing and low latency. These factors make the integration of larger, more computationally intensive models impractical for many real-world applications.

To address the scalability challenges in AVSR systems, we leverage a sparse Mixture-of-Experts (MoE) architecture~\cite{shazeer2017outrageously, fedus2022switch} that activates only a subset of parameters (\ie experts) for efficient scaling of model capacity. Furthermore, recognizing the inherent bias in AVSR systems toward the audio modality, we find it essential to harness the full potential of both audio and video data. One approach is expert group specialization, also known as \textit{hard routing}~\cite{zhu2022uni, li2023pace, lee2025moai}, which assigns specific roles to expert groups and manually activates them based on input types. While effective, this fixed activation strategy lacks adaptability, making it sub-optimal for AVSR where noise conditions and modality reliability vary. A more flexible routing mechanism is needed to dynamically utilize expert groups.

We thus propose a novel MoE framework, \textbf{MoHAVE}\setcounter{footnote}{1}\footnote{MoHAVE is pronounced as \textit{Mojave} Desert.} (\underline{M}ixture \underline{o}f \underline{H}ierarchical \underline{A}udio-\underline{V}isual \underline{E}xperts), which employs a hierarchical gating mechanism with two-layer routers.
MoHAVE introduces an inter-modal router that makes effective decision on utilizing audio and visual expert groups (\S\ref{subsec:hierarchical_gating}). This dynamic routing adapts to input characteristics, specifically trained by our novel load biasing loss (\S\ref{subsec:load_biasing_loss}).
MoHAVE achieves state-of-the-art performance (\S\ref{sec:results}) on the noisy LRS3 benchmark\,\cite{afouras2018lrs3} and in multilingual tasks\,\cite{anwar2023muavic}. 
Empirical analysis shows that MoHAVE adaptively adjusts token distribution based on input context\,(\S\ref{subsec:analysis_load}), \eg visual expert group being more actively utilized under high auditory noise.

As shown in Figure\,\ref{fig:figure1}, MoHAVE capitalizes on its increased model capacity to significantly enhance performance while maintaining efficiency. Unlike simple MoE implementations, which yield only modest gains over Transformers, our innovative expert group strategy unlocks substantial advancements in adaptability and robustness.
%
Our main contributions are outlined as follows:
\vspace{-10pt}
\begin{itemize}[leftmargin=10pt, label={$\circ$}]
\setlength\itemsep{-0.1em}
    \vspace*{-2pt}
    \item \textbf{MoE architecture for scaling AVSR systems}: We present MoHAVE, a framework that integrates a sparse MoE architecture to efficiently scale AVSR model capacity and optimally process audio and visual data.
    \vspace*{-2pt}
    \item \textbf{Hierarchical gating for adaptive expert utilization}: MoHAVE features a novel hierarchical gating mechanism that dynamically adjusts the usage of audio and visual expert groups based on input context, significantly improving adaptability and robustness.
    \vspace*{-2pt}
    \item \textbf{Robust AVSR performance}: Our model showcases substantial improvements across robust AVSR benchmarks including multilingual tasks, delivering high accuracy while maintaining computational overhead.
\end{itemize}