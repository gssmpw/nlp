\subsection{Robust AVSR Benchmark Results}
\label{sec:results}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{tex_figure/figure4_new.pdf}
    \vspace{-22pt}
    \caption{(a) Expert load distribution in \ourmodel according to input modalities, with expert selection frequencies weighted by the inter-modal router’s output probability. (b) Performance of the hard routing strategy under different weight assignments to audio expert group. The visual expert group is weighted by $p^V = 1-p^A$.
    }
    \label{fig:expert_load}
\end{figure*}

\begin{figure*}[!t]
    \centering
    \vspace{-3pt}
    \includegraphics[width=\linewidth]{tex_figure/figure5_new.pdf}
    \vspace{-22pt}
    \caption{Expert load distribution in \ourmodel for the audio group (solid bars) and visual group (dashed bars) across noisy audio-visual sequences under babble (left) and natural (right) noise. Full layer-wise results are provided in Appendix\,\ref{appx:expert_group_usage}.
    }
    \label{fig:expert_load_noise}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Table\,\ref{tab:main_result} presents \ourmodel's robust performance on the AVSR benchmark under diverse noisy conditions, demonstrating exceptional robustness across different noise types and SNR levels: \textbf{N-WER of 5.8\% for \textsc{Base}} and \textbf{4.5\% for \textsc{Large}}. This substantiates the model's potential for effectively scaling AVSR systems without incurring significant computational costs.
%
The results also reveal that simple MoE implementations (AV-MoE in Table\,\ref{tab:main_result}), despite their larger capacity, fail to achieve remarkable gains. Instead, the key improvement stems from leveraging expert group specialization, as evidenced by the effectiveness of hard routing. By splitting experts into audio and visual groups, MoE is enabled with more targeted and effective processing of multimodal inputs, leading to substantial performance enhancements.
Without our load biasing loss, \ourmodel loses its group specialization capability, comparable to the performance of simple AV-MoEs.

Building upon this expert group strategy, \ourmodel enhances its adaptability through dynamically determining the usage of each group. This adaptive routing approach allows the model to flexibly adjust to varying audio-visual scenarios, contributing to consistent gains in robustness across the benchmark, as detailed in Table\,\ref{tab:main_result}.
An in-depth analysis of this hierarchical gating approach and its impact on token dispatching is discussed in \S\ref{subsec:analysis_load}, underscoring its critical role in advancing MoHAVE’s capabilities in various AVSR environments.


\vspace*{-12pt}
\paragraph{Comparison with state-of-the-art AVSR methods.}
Table\,\ref{tab:sota_comparison} shows how our \ourmodel decoder, when integrated with a range of audio-visual encoders, consistently improves performance compared to existing state-of-the-art methods. While BRAVEn \citep{haliassos2024braven} typically struggles in noisy multimodal scenarios---due to its original design focused on handling unimodal tasks---\ourmodel boosts its accuracy. Other recent approaches have advanced by utilizing the noise-augmented AVSR encoder\,\citep{shi2022learning}, such as additionally learning temporal dynamics with cross-modal attention modules\,(CMA) \citep{kim2024learning}. When paired with an AV-HuBERT encoder and trained through the CMA's self-supervised learning, \ourmodel achieves a remarkable performance: \textbf{N-WER of 4.2\%}.
