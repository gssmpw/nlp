\section{MoHAVE: Mixture of Hierarchical Audio-Visual Experts}
\label{sec:mohave}

Despite the benefits of hard routing in specializing expert groups according to decoupled input modality, it lacks the flexibility to autonomously determine the group utilization.
In practice, the optimal balance between audio and visual groups varies depending on ambient conditions such as noise type and intensity (more detailed in Figure\,\ref{fig:expert_load}(b)).
To address this limitation and enhance the model’s adaptability, we introduce an adaptive routing mechanism with hierarchical gating \cite{jordan1994hierarchical}, providing a more dynamic approach to manage multimodal inputs.

Our hierarchical model, \ourmodel, features a two-layer routing mechanism: \textit{inter-modal} and \textit{intra-modal} routers, where the inter-modal router learns to assign appropriate weights to each modality-specific group. Figure\,\ref{fig:routing}(c) presents the overview of \ourmodel's routing strategy.

\subsection{Hierarchical Gating Structure}
\label{subsec:hierarchical_gating}

The inter-modal router orchestrates the initial token distributions across expert groups. It generates logits through $u(x) = V_r \cdot x$ and determines the dispatch weights for group $i$ with $q_i(x) = \text{softmax}(u(x))_i$. This router dynamically selects the top-$m$ expert groups, and within those, the intra-modal routers select the top-$k$ experts, thus involving $m \times k$ experts in total. For practical efficiency, we set $k=1$ for each group and modify the intra-modal router's probability distribution to a Kronecker delta, $\tilde{p}_{ij} \rightarrow \delta_{j,\text{argmax}(p_i)}$.
The output from this layer integrates these selections:
\begin{align}
  y &= \!\!\!\!\sum_{i \in \text{top}m(G)} \!\!\tilde{q}_i(x)\!\! \sum_{j \in \text{top}k(E_i)} \!\!\tilde{p}_{ij} E_{ij}(x) \\
    &\rightarrow \!\!\!\!\sum_{i \in \text{top}m(G)} \!\!\tilde{q}_i(x)E_{ij}(x), ~~\text{where } j\!=\!\arg\max(p_i) 
\end{align}
where $\tilde{q}$ is the normalization of $q$ across top-$m$ probabilities, $G$ is the number of expert groups, and $E_{ij}(x)$ denotes the output from the $j$-th expert in the $i$-th group.

Focusing on audio-visual applications, we designate two expert groups: audio and visual. Each token $x$, regardless of its modality, is processed by the intra-modal routing networks of both groups, \ie $[h^A(x), h^V(x)] = [W_r^A, W_r^V] \cdot x$. The frequencies $f^{\{A,V\}}$ and probabilities $P^{\{A,V\}}$ for selecting experts are computed in the same manner as Eq.\,(\ref{eq:expert_frequency})--(\ref{eq:expert_probability}) for all $x \in \mathcal{B}$. Thus, the load balancing loss can be computed for both groups:
\vspace*{-5pt}
\begin{equation}
L_B = E^A \cdot \sum_{j=1}^{E^A} f_j^A \cdot P_j^A + E^V \cdot \sum_{j=1}^{E^V} f_j^V \cdot P_j^V
\end{equation}
where $f_j^A$ and $f_j^V$ denote the frequencies of token assignments to audio and visual experts, respectively.


\subsection{Group-level Load Biasing Loss}
\label{subsec:load_biasing_loss}

To autonomously manage the expert group loads without manual (de-)activation as hard routing, we introduce a biasing loss that directs the load towards a certain group. This load biasing loss encourages the inter-modal router to assign higher weights to $E^A$ experts for audio sequences and to $E^V$ experts for video sequences.
For audio sequences within a sub-batch $\mathcal{A}$, the frequency and average probability of selecting the $i$-th group is calculated as follows:
\begin{equation}
    g^A_i = \frac{1}{|\mathcal{A}|} \sum_{x\in\mathcal{A}} \mathbbm{1} \{\arg\max q(x) = i\},
\end{equation}
\begin{equation}
    Q^A_i = \frac{1}{|\mathcal{A}|} \sum_{x\in\mathcal{A}} q_i(x).
\end{equation}
Similar calculations for $g^V_i$ and $Q^V_i$ are made for video sequences $x\in\mathcal{V}$. We designate the first group as audio experts and the second group as video experts, then the load biasing loss is defined as:
\begin{equation}
    L_S = L_S^A + L_S^V = (1 - g^A_1 \cdot Q^A_1) + (1 - g^V_2 \cdot Q^V_2).
\end{equation}
Note that $L_S^A$ and $L_S^V$ are only computed over $x\in\mathcal{A}$ and $x\in\mathcal{V}$, respectively. 

For sequences containing both audio and video, we exclude them from the load biasing loss calculation but incorporate them into the load balancing.
Although these tokens are uniformly dispatched on average, the inter-modal router finds the optimal strategy for each token based on its characteristics.
Empirically, we find that \ourmodel learns to assign greater weight to the visual expert group for audio-visual inputs under high auditory noise, and to the audio expert group for less noisy inputs (see \S\ref{subsec:analysis_load} for details), demonstrating the model’s adaptability under various noisy conditions.

The overall loss function, combining the cross-entropy\,(CE) for token prediction, is formulated as:
\begin{equation}
L_{tot} = L_{CE} + c_B L_B + c_S L_S + c_Z L_Z.
\end{equation}
Here, $c_B$ and $c_Z$ are set to 1e-2 and 1e-3, respectively, in line with \citep{fedus2022switch, zoph2022st}, and $c_S$ is also set at 1e-2.