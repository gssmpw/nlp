\section{Additional Results}
\label{appx:additional_results}

\subsection{Expert Group Utilization}
\label{appx:expert_group_usage}

In the main paper (Figure\,\ref{fig:expert_load_noise}), we have presented expert load distribution for selected layers. Figure\,\ref{fig:appx_group_load} provides the distribution across all MoE layers, illustrating how \ourmodel dynamically adjusts expert groups based on noise conditions.

\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{tex_figure/appx/appx_group_load_new.pdf}
    \vspace{-20pt}
    \caption{Expert load distribution in \ourmodel for the audio group (solid bars) and visual group (dashed bars) across noisy audio-visual sequences under babble (first row) and natural (second row) noise. The frequency of each expert has been weighted by the inter-modal router's output probability.}
    \label{fig:appx_group_load}
\end{figure}


\subsection{Multilingual Tasks in Clean Environments}

Table\,\ref{tab:muavic_clean} outlines the MuAViC benchmark results in a clean environment, without auditory noise added. The experimental setup remains consistent with Table\,\ref{tab:muavic}, utilizing the same models. Unlike the noisy setting, we observe that \ourmodel does not yield significant performance improvements in clean speech tasks. This is primarily because \ourmodel enhances AVSR under noisy conditions by dynamically adjusting the utilization of audio and visual expert groups. 

Indeed, in clean speech recognition and translation tasks, encoder capacity---particularly when pretrained on large-scale audio data---plays a more crucial role than decoder-specific training methods. In addition, visual information is less essential in noise-free environments, as demonstrated by the strong ASR performance of the Whisper-large-v2 model \cite{radford2023robust}. Even the smaller ASR model, XLS-R 300M \cite{babu2022xls}, surpasses AVSR models such as mAV-HuBERT \cite{anwar2023muavic} or u-HuBERT \cite{hsu2022u} in this setting, underscoring that the advantage of using AVSR models emerges most clearly in robust speech recognition.

\input{tex_table/muavic_clean}


\subsection{Number of Activated Experts}

By default, \ourmodel selects one expert from each group---audio and visual---activating a total of two experts per token. This design is to match the compute of standard MoE implementations, which utilizes top-2 out of 8 experts. A natural question arises: \textit{does activating more experts improve performance, or does it simply increase computational costs without substantial gains?}

Table\,\ref{tab:num_experts} presents the results when activating more experts of \ourmodel-\textsc{Base}, where top-$k^A$ experts from the audio group and top-$k^V$ experts from the visual group are selected. Interestingly, increasing the number of audio experts significantly degrades performance, implying that the model might be confused by employing another sub-optimal expert. 

\input{tex_table/num_experts}
In contrast, activating two visual experts while keeping one audio expert improves performance (N-WER of 5.7\%) compared to the default setting of single visual expert. Particularly under the babble noise, WER has decreased from 9.6\% to 9.3\%. 
This suggests that adding an additional visual expert can be beneficial in noisy environments, likely due to the increased robustness from visual information in challenging audio conditions.


\subsection{Unimodal Task Results}

Table\,\ref{tab:asr_vsr} presents unimodal task results, evaluating model performance on video-only (VSR) sequences and audio-only (ASR) sequences. 
BRAVEn \cite{haliassos2024braven} and Llama-3.1-8B-AVSR \cite{cappellazzo2024large} models achieve the best VSR performance, as these models are specifically pretrained for the VSR task. While using an LLM decoder is highly effective in VSR, since LLMs are able to refine and correct recognition errors, ASR performance is largely determined by the encoder's pretraining strategy as BRAVEn and Whisper encoders.
As an adaptive audio-visual model, \ourmodel does not specialize in unimodal tasks but instead performs robustly in multimodal AVSR. It only exhibits a slight improvement in VSR over AV-HuBERT. These results indicate that unimodal performance is primarily influenced by the effectiveness of the encoder pretraining strategy rather than the MoE-based multimodal approach.

\input{tex_table/asr_vsr}


\clearpage
\subsection{Variations of MoHAVE Implementations}

\paragraph{\ourmodel in the encoder.}

We have implemented \ourmodel by integrating MoE into the decoder to facilitate text token processing while enhancing multimodal fusion. Since the AVSR decoder incorporates information from both audio and visual modalities along with text tokens, the decoder-based MoHAVE is expected to be the most effective strategy. An alternative approach is to apply MoHAVE within the encoder, by pretraining the encoder using the AV-HuBERT masked prediction strategy \cite{shi2022learning}. For this, we initialize the pretrained encoder (with standard Transformers) and convert the FFN layers into MoE layers by copying the FFN parameters into all the expert modules. Since the \textsc{Base} model consists of 12 encoder layers, we convert 6 of them alternatively to match the number of MoE layers in the decoder \ourmodel. During fine-tuning, all MoE layers in the encoder are also trained following the same procedure.

There are two options for pretraining strategies: (1) pretraining only the MoE layers initialized from the FFN parameters, and (2) pretraining the entire encoder with MoE layers. As shown in Table\,\ref{tab:encoder_mohave}, the latter approach significantly outperforms the former, suggesting that encoder \ourmodel requires full pretraining for effective learning.
However, even with full pretraining, encoder \ourmodel performs inferior to decoder \ourmodel. This is because the encoder only processes audio and visual tokens, whereas the decoder directly integrates audio-visual embeddings with text, finding optimal strategies for text token dispatching that best improves speech recognition. In addition, applying \ourmodel to both the encoder and decoder leads to degraded performance despite the increased computational cost.

\vspace{-8pt}
\paragraph{Decoder uptraining.}

We also explore a successive training strategy for the decoder, referred to as uptraining \cite{ainslie2023gqa}, where the decoder \ourmodel undergoes additional training after the fine-tuning phase of standard Transformers. However, as seen in Table\,\ref{tab:encoder_mohave}, uptraining does not yield further improvements compared to training from scratch, even after an additional 120K training steps. In fact, we observed the shorter uptraining steps leading to degraded performance. This suggests that training the decoder \ourmodel requires a comprehensive learning phase rather than incremental fine-tuning, as MoE  may fundamentally alter the processing pathways of tokens.

\input{tex_table/encoder_mohave}