\section{Experimental Setup}

\subsection{Model Description}
\label{appx:model_details}

As described in \S\ref{sec:implementation}, \ourmodel is implemented in two configurations: \textsc{Base} and \textsc{Large}, following the architecture of AV-HuBERT-\textsc{Base} and AV-HuBERT-\textsc{Large}, respectively. The encoder maintains the same structure as AV-HuBERT, while the decoder incorporates MoE layers by replacing every feed-forward network (FFN) layer with expert modules. Each expert in the MoE layers follows the same bottleneck structure with FFN, consisting of two fully-connected layers with an activation function. 

To encourage expert group specialization, we apply load biasing, where either audio or video is randomly dropped with a probability of 25\%. This allows the model to learn modality-aware expert utilization. For expert selection, a router network assigns tokens to a subset of experts, ensuring efficient computation. The router probabilities are always normalized as sum to 1 when computing the output $y$. The routing networks $V_r$ and $W_r$ are parameterized as matrices, with dimensions matching the hidden dimension size by the number of experts.

For comparison, we evaluate multiple MoE-based AVSR models:
\begin{itemize}[leftmargin=10pt, label={$\circ$}]
\setlength\itemsep{-0.1em}
\vspace{-10pt}
    \item AV-MoE: A simple MoE implementation over AV-HuBERT, activating top-2 out of 4 or 8 experts per token. We follow the same implementation of sparse MoE as \citep{dai2022stablemoe, jiang2024mixtral}.
    \item AV-MoE with Hard Routing: Uses top-2 out of 4 experts for unimodal inputs (audio-only or video-only). For multimodal (audio-visual) inputs, it activates top-1 from each expert group and averages their outputs. This model does not have an explicit router for groups, but within each group, there is an intra-modal router, \ie $W_r^A$ or $W_r^V$.
    \item \ourmodel: Employs top-1 expert per group, with an inter-modal router dynamically adjusting group weight assignments and an intra-modal router uniformly dispatching the tokens to modality-specific experts.
\end{itemize}


\subsection{Computation Cost}
\label{appx:computation_cost}

\input{tex_table/computation_cost}

Table\,\ref{tab:computation_cost} summarizes the parameter sizes and computational costs of \ourmodel. \ourmodel-\textsc{Base} contains 359M parameters, while the \textsc{Large} version expands to 1B parameters. Specifically, for \ourmodel-\textsc{Base}, the encoder accounts for 103M parameters, and the decoder 256M, whereas in \textsc{Large}, the encoder holds 325M, and the decoder 681M.
Despite its larger model capacity, \ourmodel maintains computational efficiency through sparse activation, where only around half of the total parameters are active per token. This results in 189M active parameters for \textsc{Base} and 553M for \textsc{Large}.

To assess actual computation costs when processing inputs, we measure floating point operations per second (FLOPs) using an audio-visual sequence of 500 frames with 50 text tokens. The entire compute cost for AV-HuBERT-\textsc{Base} and \ourmodel-\textsc{Base} are 12.1 GFLOPs and 14.8 GFLOPs, respectively, while for \textsc{Large}, the computes are 32.2 GFLOPs and 39.3 GFLOPs. This indicates a slight increase in FLOPs for \ourmodel, primarily due to the MoE layers replacing FFNs in the decoder. Although the MoE layers require roughly twice the computation cost of standard FFNs (refer to Compute\,/\,FFN), the encoder and attention layers in the decoder remain unchanged. Consequently, the overall computational cost remains comparable to AV-HuBERT counterparts, ensuring scalability without significant computation overhead.





\subsection{LRS3 Benchmark Experiments}
\label{appx:lrs3_benchmark}

We initialize our model using the pretrained checkpoint from \citep{shi2022learning} and fine-tune it on the LRS3 train set for 120K steps. The encoder remains frozen for the first 90K steps, allowing only the AVSR decoder to be trained, after which the entire model is fine-tuned for the remaining 30K steps. Our fine-tuning setup follows the configurations from \citep{shi2022robust}. We employ a sequence-to-sequence negative log-likelihood loss for predicting the next text token, without using connectionist temporal classification (CTC) decoding \citep{watanabe2017hybrid}. The Adam optimizer \cite{kingma2014adam} is used with a learning rate of 5e-4 and a polynomial decay schedule with an initial warmup. Each training step processes 8,000 audio-visual frames, equivalent to 320 seconds of speech data.

For inference, we use beam search with a beam size of 50. The AVSR performance is evaluated using word error rate (WER) across five signal-to-noise ratio (SNR) levels: $\{-10, -5, 0, 5, 10\}$ (lower value means higher noise level). We use audio noise sampled from MUSAN (babble, music, natural) and LRS3 speech noise, ensuring no speaker overlap between training and test sets. Since Table\,\ref{tab:main_result} presents SNR-averaged results for each noise type, we provide the full results across all SNR levels in Table\,\ref{tab:lrs3_full}.


\subsection{MuAViC Benchmark Experiments}
\label{appx:muavic_benchmark}

We evaluate \ourmodel on the MuAViC benchmark \cite{anwar2023muavic} for multilingual AVSR and X-to-English AVS2TT tasks. For multilingual AVSR, the dataset includes 8 non-English languages: Arabic (Ar), German (De), Greek (El), Spanish (Es), French (Fr), Italian (It), Portuguese (Pt), and Russian (Ru), encompassing approximately 700 hours of training data from 3,700 speakers. For X-En AVS2TT, the dataset covers 6 languages: Greek, Spanish, French, Italian, Portuguese, and Russian, where each sample includes audio-visual speech with corresponding English transcriptions.

A single multilingual model is trained for each task, capable of detecting the source language and generating target transcriptions accordingly. The evaluation is conducted on each language separately, as seen in Table\,\ref{tab:muavic}. Using the pretrained multilingual AV-HuBERT from \citep{choi2024av2av}, we fine-tune the model for 120K steps, unfreezing the encoder after 10K steps. Inference is performed with beam size of 5, and the samples with empty ground-truth transcriptions are removed from the evaluation set. 


\input{tex_table/lrs3_full}


\clearpage