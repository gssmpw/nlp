\subsection{Implementation Details}
\label{sec:implementation}

\input{tex_table/main_result}

\paragraph{Datasets.}
For the robust AVSR benchmark, we utilize the LRS3 dataset~\citep{afouras2018lrs3}, which consists of 433 hours of audio-visual speech from 5,000+ speakers. Following the experimental setup of \citet{shi2022robust}, we extract audio noise samples from the MUSAN~\citep{snyder2015musan} dataset, targeting different noise types such as \textit{babble}, \textit{music}, and \textit{natural} noises, along with \textit{speech} noise from LRS3. These noises are randomly augmented into the audio data, corrupting 25\% of the training set with a signal-to-noise ratio (SNR) sampled from $\mathcal{N}(0, 5)$. We measure performance using the word error rate (WER), primarily under noisy conditions with SNRs of \{$-$10, $-$5, 0, 5, 10\}\,dB, specifically N-WER\,\citep{kim2024learning} which highlights the significance of visual cues in noise-corrupted environments.


For multilingual evaluations, the MuAViC dataset \cite{anwar2023muavic} is used, featuring 1,200 hours of audio-visual content from 8,000+ speakers across 9 languages, sourced from LRS3-TED\,\cite{afouras2018lrs3} and mTEDx\,\cite{elizabeth2021multilingual}. We use 8 languages (excluding English) for multilingual AVSR and 6 languages for X-to-English audio-visual speech-to-text translation (AVS2TT) tasks. We assess the models using WER for transcription and the BLEU score\,\cite{papineni2002bleu} for translation.

\vspace*{-8pt}
\paragraph{\ourmodel model description.}

Our \ourmodel framework is developed in two configurations: \textsc{Base} and \textsc{Large}. The \textsc{Base} model consists of 12 Transformer~\citep{vaswani2017attention} encoder layers and 6 decoder layers, while the \textsc{Large} model incorporates 24 encoder layers and 9 decoder layers. Both modelsâ€™ audio-visual encoders are derived from the AV-HuBERT-\textsc{Base}/-\textsc{Large} models, pretrained on a noise-augmented corpus of LRS3 \citep{afouras2018lrs3} + VoxCeleb2 \citep{chung2018voxceleb2}. Our MoE implementation activates top-2 out of 8 experts in every FFN layer within the decoder~\citep{jiang2024mixtral}, while the hierarchical architecture engages the top-1 expert from each audio and visual group. To facilitate the expert group specialization, load biasing is used with audio or video randomly dropped in 25\% probability.

\input{tex_table/sota_comparison}

As summarized in Table\,\ref{tab:main_result}, the \textsc{Base} model of \ourmodel holds 359M parameters, and the \textsc{Large} configuration contains 1B. Despite its larger model capacity, due to the sparse activation of these parameters, only about half are active during token processing, amounting to 189M for \textsc{Base} and 553M for \textsc{Large} model. This setup ensures computational efficiency which is comparable to the smaller AV-HuBERT counterparts. For more details on the model description and computation cost, refer to Appendix\,\ref{appx:model_details} and \ref{appx:computation_cost}.
