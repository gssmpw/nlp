


\section{Introduction}

Transfer reinforcement learning (RL) has emerged as a promising solution to the fundamental challenge of sample inefficiency in RL. By leveraging knowledge from related tasks, transfer learning aims to accelerate policy learning and improve performance in new environments without requiring extensive data collection. This approach has shown empirical success across various domains, from robotics to game playing, yet theoretical understanding of how transfer provably benefits RL remains limited.

Consider autonomous vehicle training as an illustrative example: core driving dynamics -- including vehicle physics, road rules, and basic navigation -- remain consistent across different driving scenarios. However, specific environments (urban vs. highway driving, varying weather conditions, different traffic patterns) introduce distinct variations to these core dynamics. This naturally suggests modeling transition dynamics as a combination of shared low-rank structure capturing common elements, plus sparse components representing scenario-specific variations.

We propose a composite MDP framework that formalizes this intuition: transition dynamics are modeled as the sum of a low-rank component representing shared structure and a sparse component capturing task-specific deviations. This structure appears in many real-world applications beyond autonomous driving -- robotic manipulation with different objects, game playing across varying environments, and resource management under changing constraints all exhibit similar patterns of {\em core shared dynamics} with {\em sparse task-specific variations}.
%\lin{why is this natural?} 
%\r{The linear part can represent systematic or regular dynamics while sparse part captures rare behaviors and anomalies~\citep{chai2024structured}. This assumption relaxes the need for a strictly linear representation and simultaneously facilitates efficient estimation.}

Our approach extends existing work in several important directions. Prior transfer and multi-task RL research has primarily focused on pure low-rank MDPs \citep{agarwal2023provable,lu2021power,cheng2022provable} or made direct assumptions about value or reward function similarity \citep{calandriello2014sparse,du2024misspecified,chen2024data,chai2025deep}. While sparsity has been studied in the context of value function coefficients, theoretical analysis of sparse transition structures -- particularly in combination with low-rank components -- remains unexplored. This gap is significant because transition dynamics often more directly capture task similarity than value functions.

We begin by addressing single-task learning within this composite structure, introducing a variant of UCB-Q-learning tailored specifically for composite MDPs, which may involve a high-dimensional ambient space. In contrast to previous work, we consider the high-dimensional setting where the feature dimensions $p, q \gg$ number of trajectories $N$, and the transition core $M^*$ is no longer a low-rank matrix. This departure from low-rank structures makes existing algorithms designed for linear MDPs inapplicable. 
Similarly, methods built for low-rank MDPs fail in our context due to the absence of low-rank assumptions in $M^*$. 

Our work provides the first theoretical guarantees for this setting, demonstrating how the algorithm successfully learns both shared and task-specific components. These results extend and complement the existing body of work on low-rank MDPs by explicitly handling structured deviations from low-rank assumptions \citep{du2019good, lattimore2020learning}. Unlike the approach in \cite{foster2021statistical}, which introduced a Decision-Estimation Coefficient (DEC) to characterize the statistical complexity of decision-making across various scenarios, our framework relies on distinct structural assumptions. This necessitates the development of new techniques, as discussed in detail in Section \ref{sec:challenge-single}.

Building on this foundation, we propose UCB-TQL (Upper Confidence Bound Transfer Q-Learning) for transfer learning in composite MDPs. UCB-TQL strategically exploits shared dynamics while efficiently adapting to task-specific variations. Our theoretical analysis demonstrates that UCB-TQL achieves dimension-independent regret bounds that explicitly capture dependencies on both rank and sparsity, showing how structural similarities enable efficient knowledge transfer.
In particular, we construct a novel confidence region (CR) for the sparse difference, thereby reducing the target sample complexity in the online learning process, as discussed in detail in Section \ref{sec:challenges-ucb-tql}. 

Our primary contributions are as follows. 
\begin{itemize}
\item A novel composite MDP model that combines {\em low-rank shared structure} with {\em sparse task-specific components}, while allowing high-dimensional feature spaces. This framework better captures real-world task relationships and provides a foundation for future work in multi-task and meta-learning settings.
\item The first theoretical guarantees for single-task RL under the high-dimensional composite transition structure, demonstrating how algorithms can effectively learn and utilize both shared and task-specific components.
\item A transfer Q-learning algorithm with {\em provable regret bounds} that explicitly characterize how structural similarities enable efficient knowledge transfer across tasks.
\end{itemize}

This work represents a significant step toward bridging the gap between empirical success of transfer RL and theoretical understanding by providing a rigorous analysis of how structural similarities in transition dynamics enable efficient knowledge transfer. Our results suggest new directions for developing practical algorithms that can systematically leverage shared structure while accounting for task-specific variations.

\subsection{Related Work}

\noindent
\textbf{Transfer RL.}
\cite{agarwal2023provable} studied transfer via shared representations between source and target tasks. With generative access to source tasks, they showed that learned representations enable fast convergence to near-optimal policies in target tasks, matching performance as if ground truth features were known. \cite{cheng2022provable} proposed REFUEL for multitask representation learning in low-rank MDPs. They proved that learning shared representations across multiple tasks is more sample-efficient than individual task learning, provided enough tasks are available. Their analysis covers both online and offline downstream learning with shared representations.
\cite{chen2022transfer,chen2024data,chai2025deep} analyzed transfer $Q$-learning without transition model assumptions, focusing instead on reward function similarity and transition density. These works established convergence guarantees for both backward and iterative $Q$-learning approaches.

Our work differs by studying transition models with low-rank plus sparse structures. This setting presents {\em unique challenges} beyond purely low-rank models, as we must identify and leverage an unknown low-rank space while also accounting for sparse deviations.


\smallskip
\noindent
\textbf{Single task RL under structured MDPs.}
Single-task RL under structured MDPs has evolved through several key advances:
Linear MDPs with known representations were initially studied by \cite{yang2020reinforcement}, leading to provably efficient online algorithms \citep{sun2019model,jin2020provably,zanette2020learning,neu2020unifying,cai2020provably,wang2019optimism}.

Low-rank MDPs extend this by requiring representation learning. Major developments include FLAMBE \citep{agarwal2020flambe} for explore-then-commit transition estimation, and REP-UCB \citep{uehara2022representation} for balancing representation learning with exploration. Recent work has expanded to nonstationary settings \citep{cheng2023provably} and model-free approaches like MOFFLE \citep{modi2024model}.
Related structured models include block MDPs \citep{du2019provably,misra2020kinematic,zhang2022efficient}, low Bellman rank \citep{jiang2017contextual}, low witness rank \citep{sun2019model}, bilinear classes \citep{du2021bilinear}, and low Bellman eluder dimension \citep{jin2021bellman}.

Our work introduces the composite MDPs with high-dimensional feature space and low-rank plus sparse transition, extending beyond pure low-rank models. We provide the first theoretical guarantees for UCB Q-learning under this composite structure.

\smallskip
\noindent
\textbf{Multitask RL and Meta RL.}
Research in multitask and meta-RL has evolved through several key theoretical advances. Early work by \cite{calandriello2014sparse} examined multitask RL with linear Q-functions sharing sparse support, establishing sample complexity bounds that scale with the sparsity rather than ambient dimension. \cite{hu2021near} extended this framework by studying weight vectors spanning low-dimensional spaces, showing that sample efficiency improves when the rank is much smaller than both the ambient dimension and number of tasks. \cite{arora2020provable} demonstrated how representation learning reduces sample complexity in imitation learning settings, providing theoretical guarantees for learning shared structure across tasks. \cite{lu2022provable} further developed this direction by analyzing multitask RL with low Bellman error and unknown representations, establishing bounds that improve with task similarity.

Task distribution approaches offered another perspective. \cite{brunskill2013sample} proved sample complexity benefits when tasks are independently sampled from a finite MDP set, while \cite{pacchiano2022joint} and \cite{muller2022meta} extended these results to meta-RL for linear mixture MDPs, showing how learned structure transfers to new tasks. In parallel, research on shared representations by \cite{d2020sharing} established faster convergence rates for value iteration under common structure, and \cite{lu2021power} proved substantial sample efficiency gains in the low-rank MDP setting.

Our composite MDP structure advances this line of work by explicitly modeling deviations from low-rank similarity through a sparse component. This framework captures more realistic scenarios where tasks share core structure but maintain individual variations, opening new theoretical directions for multitask and meta-learning approaches.

\section{Problem Formulation} 

\noindent
\textbf{Episodic MDPs.} 
We consider an episodic Markov decision process (MDP) with finite horizon.
It is defined by a tuple $\mathcal{M} = (\mathcal{S}, \mathcal{A}, P, r, \mu, H)$, where $\mathcal{S}$ denotes the state space, $\mathcal{A}$ represents the action space, $H$ is the finite time horizon, $r: \mathcal{S} \times \mathcal{A} \to [0, 1]$ the reward function, $P$ is the state transition probability, and $\mu$ is the initial state distribution.
A policy $\pi : \mathcal{S} \times [H] \rightarrow \mathcal{A}$ maps each state-time pair to an action that the agent takes in the episode. 

For each time step $h \in [H]$, the value function $V^\pi_h : \mathcal{S} \rightarrow \mathbb{R}$ evaluates the expected cumulative reward from following policy $\pi$ starting from state $s$ at time $h$, defined as $V^\pi_h(s) = \mathbb{E}\left[\sum_{h'=h}^{H} r_{h'}(s_{h'}, \pi(s_{h'})) \mid s_h = s\right]$, and $V^\pi_{H+1}(s) = 0$, while the action-value function $Q^\pi_h : \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ evaluates the value of taking action $a$ in state $s$ at time $h$, given by
$Q^\pi_h(s, a) = r_h(s, a) + \mathbb{E}\left[\sum_{h'=h+1}^{H} r_{h'}(s_{h'}, \pi(s_{h'})) \mid s_h = s, a_h = a\right]$ and $Q^\pi_H(s, a) = r_h(s, a).$


%Defining $[P V_{h+1}](s, a) := \mathbb{E}_{s' \sim P(\cdot \mid s, a)}[V_{h+1}(s')],$  and denote its empirical counterpart for the n-th episode as $[P_{n,h} V_{h+1}](s_{n,h}, a_{n,h})$. 
The Bellman equation for $V^\pi_h$ and $Q^\pi_h$ can be expressed as:
\[V^\pi_h(s) = Q^\pi_h(s, \pi_h(s)), \ Q^\pi_h(s, a) = r_h(s, a) + [P V^\pi_{h+1}](s, a).\]
The Bellman optimally equations for the optimal value function 
and action-value function is as follows:
$$V^*_h(s) = \max_{a \in A}\left\{r_h(s, a) + [P V^*_{h+1}](s, a)\right\}, \quad V^*_{H+1}(s) = 0,$$
$$Q^*_h(s, a) = r_h(s, a) + [P V^*_{h+1}](s, a), \quad Q^*_H(s, a) = r_h(s, a).$$
The \textbf{cumulative regret} quantifies the performance discrepancy of an agent over episodes. Given an initial state $s_0\sim \mu$, for the $ n^{th} $ episode, the regret is the value difference of the optimal policy $ V^*(s_0) $ and the agent's chosen policy $ V^{\pi_n}(s_0) $ which based on its experience up to the beginning of the $ n^{th}$ episode and applied throughout the episode. Accumulating over $ N $ episodes, it is defined as:
\[Regret(N) = \sum_{n=1}^{N} \EE_\mu\left[ V^*(s_0) - V^{\pi_n}(s_0) \right] \]
The agent aims to learn a sequence of policies $ ( \pi_1, \ldots, \pi_N ) $ to minimize the cumulative regret. 
%In this paper, we assume that the reward is linear, i.e.,$\EE(r(s,a)\mid s,a)=\phi(s,a)\theta^*$, $\theta^*$ is unknown weight vector in $\RR^p$.
%The well-known LinUCB Algorithm in \citep{chu2011contextual} is proven to achieve minimax regret. 
%Assuming the linear feature representation of the reward function, the additional regret incurred by unknown reward is a lower-order term, not impacting the overall magnitude of the regret.
If the reward function has a linear feature representation, any additional regret from an unknown reward becomes a lower-order term and does not affect the regret's overall magnitude. For clarity of presentation, we assume the agent knows the reward function and focus primarily on estimating the transition probability.

\smallskip
\noindent
\textbf{Composite MDPs.}
Let $\phi(\cdot) \in \RR^p$ and $\psi(\cdot) \in \RR^q$ be feature functions where $p$ and $q$ can be large.
Consider probability transitions $\PP(s' | s, a)$ that can be fully embedded in the feature space via a core matrix $M^*$:
\[\PP(s' | s, a) = \phi(s, a)^\top \cdot M^* \cdot \psi(s').\]
Since feature dimensions $p$ and $q$ can be large, we need not know the exact feature functions - we can include many possible features to span the space. What matters is learning the structure of $M^*$ from data.

To capture how transition dynamics combine shared core elements with scenario-specific variations, we impose the following structured assumption on the transition matrix:
\begin{definition}[Composite MDPs] \label{ass:matrix}
A probability transition model $\PP:\cS\times\cA\rightarrow\Delta(\cA)$ can be fully embedded in the feature space characterized by two given feature functions $\phi(\cdot) \in \RR^p$ and $\psi(\cdot) \in \RR^q$ where both $p$ and $q$ can be large. The core matrix of the transition model decomposes as:
\[
\PP(s' | s, a) = \phi(s, a)^\top \cdot ( L^* +  S^*) \cdot \psi(s'),
\]
where  $ L^*$ is a low-rank incoherent matrix and $ S^*$ is a sparse matrix.
\end{definition}

\begin{remark}
The composite MDP model we propose differs from both the linear MDP and the low-rank MDP. While the linear MDP assumes a linear structure for the transition matrix and requires knowledge of the feature maps, the low-rank MDP does not demand such knowledge but constrains the feature map to a known function class. Our model allows high-dimensional feature space with a similar assumption to the low-rank MDP but augments it with a extra sparse component.
\end{remark}

% \begin{remark}
% The low-rank-plus-sparse model has been studied in \cite{negahban2012unified,chandrasekaran2011rank,chai2024structured}, which generalizes and robustifies  the rigid low-rank assumption.
% \end{remark}


\section{Single-Task UCB-$Q$-Learning under High-Dimensional Composite MDPs}
\label{sec:single-task}
This section introduces UCB-$Q$-Learning for composite MDPs with a high-dimensional feature space. Specifically, we consider the setting where the feature dimensions $p, q \gg N$, and the transition core $M^*$ is no longer a low-rank matrix. As a result, existing algorithms designed for linear MDPs are not applicable. Likewise, methods tailored for low-rank MDPs fail in our setting due to the absence of a low-rank structure in $M^*$. To address the challenges arising from our relaxed dimensionality constraints and the more complex MDP structure, novel algorithmic approaches are required.

For any tuples $(s_{i,h},a_{i,h})$ from episode $i$ and stage $h$: We define $\phi_{i,h}=\phi(s_{i,h},a_{i,h})$,  $\psi_{i,h}=\psi(s_{i,h})$, and $\bK_{\psi} := \sum_{s'\in\calS} \psi(s') \psi(s')^\top$. 
Our estimator is based on the following population-level equation at each step $h$, 
\begin{align} \label{eqn:population-regression}
&\EE\brackets{\psi_{i,h}^\top \bK_{\psi}^{-1} \mid s_{i,h}, a_{i,h}} = \sum_{s'} \PP(s' | s_{i,h}, a_{i,h}) \psi(s')^\top\bK_{\psi}^{-1} \nonumber \\
& = \sum_{s'} \phi_{i,h}^\top ( L^* +  S^*) \psi(s') \psi(s')^\top\bK_{\psi}^{-1} \nonumber \\
& =  \phi_{i,h}^\top ( L^* +  S^*).
\end{align}
This motivates us to use the sample-level counterpart of \eqref{eqn:population-regression} to estimate $L^*$ and  $S^*$. However, both $L^*$ and  $S^*$ are unknown. To recover the low-rank and sparse components, additional assumptions are required to ensure that the low-rank part can be separated from the sparse component. Below, we elaborate on the incoherence assumption and sufficient sparsity conditions.

\begin{assumption} \label{ass:low-rank-sparse}
Let \( L^* = U^* \Sigma^* V^{*T} \) be the singular value decomposition (SVD) of \( L^* \). 
We assume that:\\
(i) (Incoherence.) $\|U^*\|_{2,\infty},\|V^*\|_{2,\infty}\le \sqrt{\frac{\mu r}{p}}$.\\
(ii) (Sufficient sparsity.) Matrix $ S^*$ contains at most $s$ non-zero entries, where $s \leq \bar s:=\frac{\max\{p,q\}}{4C_S\mu r^3}$, for some constant $C_S$.
\end{assumption}

\begin{remark}  
The incoherence condition ensures that the singular vectors of a low-rank matrix are not overly concentrated in any single direction or entry, a property that is crucial for matrix completion \citep{candes2012exact}. In our setting, it also facilitates the separation of the sparse component from the low-rank matrix. When \( r \) and \( \mu \) are treated as constants, the maximum permissible sparsity level scales linearly with \( p \).  
Moreover, as shown in \citep{candes2010power}, the incoherence condition holds for a broad class of random matrices.  
\end{remark}

We consider the online learning setting and propose to estimate $L^*$ and $S^*$ in the composite MDP by optimizing the following hard-constrained least-square objective for each episode $n\in[N]$ with collected tuples $(s_{i,h},a_{i,h})$ from previous episode $i\in[n]$ and stage $h\in[H]$:
\begin{equation}
\begin{aligned}
(\hat{L}_n, \hat{S}_n) \in 
&\underset{L, S \in \mathbb{R}^{p \times q}}{\arg\min}\;  \sum_{i < n, h \leq H} \| \psi_{i,h}^\top \bK_{\psi}^{-1} -\phi_{i,h}^\top (L + S) \|_2^2  \\
&\text{s.t.}\quad L = U\Sigma V^T, \quad \|U\|_{2,\infty} \leq \sqrt{\frac{\mu r}{p}}, \\
&\hspace{2ex} \quad \|V\|_{2,\infty} \leq \sqrt{\frac{\mu r}{q}}, \|S\|_{0}\le s
\end{aligned}
\label{eq:rl-rank-sparse}
\end{equation}

\subsection{UCB-$Q$ Learning for High-Dimensional Composite MDPs}
Since the transition dynamics $P$ are typically unknown, we must leverage observed data to approximate the underlying model parameters.
To balance the exploration-exploitation trade-off, we adopt the optimism-in-the-face-of-uncertainty principle by employing an Upper Confidence Bound (UCB)-based algorithm.
We begin by constructing the confidence region:
\begin{align}
\label{def:confidence-region}
\mathcal{B}_n=\{( L, S) \mid \norm{ L-\hat L_n}_F^2+\norm{ S-\hat S_n}_F^2\leq \beta_n\}\end{align}
where $\hat L_n$ and $\hat S_n$ are estimated by ~\eqref{eq:rl-rank-sparse}, 
\begin{align} \label{def:beta_n}
\beta_{n}=\frac{C_\beta H\log(dNH)}{n}\left(r(C_\phi C'_{\psi})^2 +sC_{\phi\psi}^2\right), 
\end{align}
$d=\max\{p, q\}$ and $C_\phi,C_\psi,C'_\psi,C_{\phi\psi}$ are positive parameters defined in the regularity Assumption \ref{asp:regularity}, $C_\beta$ is a universal constant.
The optimistic value functions are given by:
\begin{gather}
\label{eq:updateQ}
Q_{n, h}(s, a) = r(s, a) + \max_{ L, S\in\calB_n}\phi(s, a)^{\top} ( L+ S)  \Psi^{\top} V_{n, h+1}, \\
Q_{n, H+1}(s, a) = 0,  
\nonumber
\end{gather}
where $V_{n, h}(s)=\Pi_{[0, H]}\left[\max _{a} Q_{n, h}(s, a)\right]$, with $\Pi_{[0, H]}$ truncating values to $[0, H]$.
Here, $\Psi \in |\mathcal{S}| \times q$ is feature matrix, where each row represents the  $q$-dimensional feature vector corresponding to a unique state in the state space $\mathcal{S}$.
The algorithm is summarized in Algorithm \ref{algo:UCBTQL}. 

\begin{algorithm}[tb]
\caption{UCB-$Q$ Learning for HD Composite MDPs}
\label{alg:UCBQL}
\begin{algorithmic}
\STATE {\bfseries Input:} Total number of episodes $N$, feature function $\phi\in \mathbb{R}^p, \psi\in \mathbb{R}^q$.
\FOR{{\bfseries episode} $n = 1,2,\ldots,N$}
\STATE Construct confidence region in \eqref{def:confidence-region} \STATE Calculate $Q_{n, h}(s, a)$ in \eqref{eq:updateQ}
\FOR{{\bfseries stage} $h = 1,2,\ldots,H$} 
\STATE Take action $a_{n, h}=\arg \max _{a \in \mathcal{A}} Q_{n, h}\left(s_{n, h}, a\right)$\;
\STATE Observe next state $s_{n, h+1}$ \;
\ENDFOR
\STATE Learn transition core estimator $\hat L_n,\hat S_n$ using \eqref{eq:rl-rank-sparse}.
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{Regret Analysis for UCB-Q-Learning under High-Dimensional Composite MDPs}
For the regret analysis, we impose certain regularity conditions on the features as outlined below.
\begin{assumption}
\label{asp:regularity}
Let $C_\phi,C_\psi,C'_\psi,C_{\phi\psi}$ be positive parameters such that
\begin{enumerate}
\item[(i)] $\forall (s,a),\quad \|\phi(s,a)\|_2\le C_\phi,\ \|\phi(s,a)\|_{\infty}\le C_\phi'$;
\item[(ii)] $\|\Psi^\top\|_{\infty,2}\le C_\psi$;
\item[(iii)] $\forall s',\quad \|\psi(s')^\top \bK_{\psi}^{-1}\|_2\le C'_\psi$;
\item[(iv)] $\forall (s,a,s'),\quad\|\phi(s,a)^\top \psi(s')\bK_{\psi}^{-1}\|_{\max}\le C_{\phi\psi}$.
\end{enumerate}
\end{assumption}

%\begin{theorem}[Transition Core Estimation Error] \label{thm:trans_core_error}\red Transition Core Estimation Error \end{theorem}

\begin{lemma}[Transition Estimation Error]
\label{lemma:estimation-error-single}
For composite MDPs in Definition \ref{ass:matrix}, under Assumption \ref{ass:low-rank-sparse} and \ref{asp:regularity}, the estimator obtained by solving program \eqref{eq:rl-rank-sparse} at the end of $n^{th}$-episode satisfies, with probability at least $1-1/(n^2H)$, that,
\[\norm{\hat L_N-L^*}_F^2+\norm{\hat S_N-S^*}_F^2\leq \beta_n,\]
where $\beta_n$ is defined in \eqref{def:beta_n}. 
\end{lemma}

\begin{remark}
The estimation error bound is minimax optimal with respect to $n$. This result has been established in the contexts of regression and matrix completion \cite{chai2024structured}.
\end{remark}

\begin{theorem}[Single-Task Regret Upper Bound]\label{thm:single-task-regret}
For composite MDPs in Definition \ref{ass:matrix}, under Assumption \ref{ass:low-rank-sparse} and \ref{asp:regularity}, let {\rm Regret(NH)}  be the accumulative regret of a total of $N$ episodes using the UCB-$Q$-Learning in Algorithm \ref{algo:UCBTQL}. 
We have that
\begin{align*}
{\rm Regret(NH)} 
&\le C_{\phi}C_{\psi}H^2\sum_{n=1}^N\sqrt{2\beta_n}+1\\
&\lesssim C_{reg}\sqrt{NH^5}
\end{align*}
where $d=\max\{p, q\}$ and
$$C_{reg}:=C_\phi C_\psi \sqrt{C_\beta\left(r(C_\phi C'_{\psi})^2 +sC_{\phi\psi}^2\right)\log(dNH)}.$$
\end{theorem}

\begin{remark}
This regret bound achieves optimal scaling with respect to both the number of trajectories $N$ and ambient dimension $d$, matching previous results in reinforcement learning \cite{yang2020reinforcement,jin2020provably}. In Section \ref{sec:transfer}, we demonstrate that transfer learning can substantially reduce both the dependence on ambient dimension $d$ and the scaling with $N$ by effectively utilizing additional trajectories from a source task.
\end{remark}

% \begin{remark}
% When $C_\phi,C_\psi,C_\beta,C_\psi',C_{\phi\psi},r,s=\calO(1)$, we have $C_{reg}=\tilde{\calO}(1)$, and hence the regret is of order $\tilde{\calO}(\sqrt{NH^5})$, improving the regret bound of \cite{yang2020reinforcement,jin2020provably}, etc.
% \end{remark}

\subsection{Challenge and Proof Sketch under the Composite Structure} \label{sec:challenge-single}

By optimality condition of \eqref{eq:rl-rank-sparse}, it holds that
\begin{align*}&\sum_{i < n, h \leq H} \| \psi_{i,h}^\top \bK_{\psi}^{-1} -\phi_{i,h}^\top (\hat L + \hat S) \|_2^2 \\&\le \sum_{i < n, h \leq H} \| \psi_{i,h}^\top \bK_{\psi}^{-1} -\phi_{i,h}^\top (L^* + S^*) \|_2^2\end{align*}
Expanding the inequality, we have
\begin{align*}&\sum_{i < n, h \leq H} \| \phi_{i,h}^\top (\hat L-L^*)\|^2+\|\phi_{i,h}^\top (\hat S-S^*) \|_2^2\le \\&2\sum_{i < n, h \leq H} \langle \phi_{i,h}^\top (L^* + S^*-\hat L - \hat S) ,\psi_{i,h}^\top \bK_{\psi}^{-1} -\phi_{i,h}^\top M^*\rangle\\&-2\sum_{i < n, h \leq H}\langle\phi_{i,h}^\top (\hat L-L^*),\phi_{i,h}^\top (\hat S-S^*)\rangle\end{align*}

Establishing Theorem \ref{thm:single-task-regret} presents several challenges and requires new techniques.
First, deriving a high-probability error bound for $\hat L$ and $\hat S$ is nontrivial due to the presence of cross terms at the end of the inequality. To address this, we adapt the separation lemma from \cite{chai2024structured}, which provides a way to control these cross terms effectively.

Second, ensuring the strong convexity of the linear operator is challenging due to high correlations across stages. To overcome this, we enforce the strong convexity property by incorporating a restart mechanism for each trajectory.

Thirdly, we must bound the error term $\sum_{i=1}^{n-1}\sum_{h=1}^H \phi_{i,h}\left(\psi_{i,h}^\top \bK_{\psi}^{-1}-\phi_{i,h}^\top   M^*\right)$. Since this term forms a martingale difference sequence, we apply matrix concentration techniques to control it effectively.

\section{Transition Transfer under Composite MDPs}\label{sec:transfer}

In this section, we consider transfer learning with target task $\calM^{*(1)}$ and source task $\calM^{*(0)}$. 
The transition probabilities of the target and source tasks are, respectively,
\begin{equation} \label{eqn:composite-MDP2}
\begin{aligned}
\PP^{(0)}(s' | s, a) &= \phi(s, a)^\top   M^{*(0)} \psi(s'), \quad\text{and}\quad \\
\PP^{(1)}(s' | s, a) &= \phi(s, a)^\top  M^{*(1)}  \psi(s'),   
\end{aligned}
\end{equation}
where the core transition matrices $M^{(1)}$ and $M^{(0)}$ are different. 

We propose modeling task similarity through their transition dynamics: similar tasks share a common low-rank structure capturing core dynamics, while differing only in sparse directions that represent task-specific variations.
%\footnote{For notational simplicity, we omit the `*' when discussing the transfer setting.}

\begin{assumption}[Transition Similarity]\label{assume:transition-similarity}
Consider the target and source tasks characterized by transition model \eqref{eqn:composite-MDP2}. 
The target and source tasks are different in that their core transition matrices $M^{*(1)} \ne M^{*(0)}$. 
However, their similarity is defined by:
\begin{equation}
  M^{*(0)} =  L^* +  S^{*(0)}, 
\quad\text{and}\quad
  M^{*(1)} =  L^* +  S^{*(1)},
\end{equation}
where both tasks share the same low-rank component $L^*$, $\lVert  S^{*(0)} \rVert_0 = s_0$,  $\lVert  S^{*(1)} \rVert_0 = s_1$ are task-specific sparse components, two tasks are similar in the sense that their difference $D^* = S^{(1)} - S^{(0)}$, called the ``sparsity difference'', is very sparse: $\norm{D^*}_0 = e \ll \max\{s_0, s_1\}$. 
\end{assumption}

%In words, the two core transition matrices $  M^{*(0)}$ and  $  M^{*(1)}$ share the same low-rank part and differs only in their respective sparse part $ S^{*(0)}$ and $ S^{*(1)}$. 

We have $N_0$ episodes for the source task and $N_1=N$ episodes for the target task. 
In practice, $N_0 \gg N$ and we would like to use the source task to enhance the performance of the target task. Since our primary focus is on the target data, we don't make specific data generating assumptions on the source data which can be both batch data or generated from certain online process.

For notation brevity, we use $i$ and $h$ to index episodes and time steps of the source task, with $i \in [N_0]$ and $h \in [H]$. For the target task, we use $j$ and $h$ to index its episodes and time steps, with $j \in [N]$ and $h \in [H]$.
We denote the following state-action-station transition triplet: $(s_{i, h}, a_{i, h}, s'_{i, h})$ from the target task and $(s_{j, h}, a_{j, h}, s'_{j, h})$ from the source task.
The associated features are 
\begin{equation}
\begin{aligned}
    \phi_{i, h}^{(0)} &:= \phi(s_{i, h}, a_{i, h}) \in \RR^p, & \psi_{i, h}^{(0)} &:= \psi(s'_{i, h}) \in \RR^{q}, \\
    \phi_{j, h}^{(1)} &:= \phi(s_{j, h}, a_{j, h}) \in \RR^p, & \psi_{j, h}^{(1)} &:= \psi(s'_{j, h}) \in \RR^{q}.
\end{aligned}
\end{equation}

Let $\bK_{\psi} := \sum_{s'\in\calS} \psi(s') \psi(s')^\top$. 
We have, at each step $h$ for the target task, 
\[\EE\brackets{\phi_{i,h}^{(1)}\psi_{i,h}^{(1)\top} \bK_{\psi}^{-1} \mid s_{i,h}, a_{i,h}} = \paran{\phi_{i,h}^{(1)} \phi_{i,h}^{(1)\top}} ( L^* +  S^{*(0)}).\]
    
Similarly, we have for the source task, 
\[\EE\brackets{\phi_{j,h}^{(0)}\psi_{j,h}^{(0)\top} \bK_{\psi}^{-1} \mid s_{j,h}, a_{j,h}}= \paran{\phi_{j,h}^{(0)} \phi_{j,h}^{(0)\top}} ( L^* +  S^{*(1)}).\]

%We denote $Y_{i,h}=\phi_{i,h}\psi_{i, h}^{\top}K_{\psi}^{-1}$, $X_{i,h}=\phi_{i,h}\phi_{i,h}^\top$ and $Y_{j,h}=\psi_{j, h}^{\top}K_{\psi}^{-1}$, $X_{j,h}=\phi_{j,h} \phi_{j,h}^\top$.

\subsection{UCB Transfer $Q$-Learning for High-Dimensional Composite MDPs}

Now we introduce the UCB Transfer $Q$-Learning (UCB-TQL) for HD Composite MDPs. The algorithm is summarized in Algorithm \ref{algo:UCBTQL}. We first introduce the optimization-based estimator in the following two steps, then proceed to construct the confidence region.

\noindent{\sc Step I.} Estimate the low-rank and sparse components of the source task by solving\footnote{
Note that for simplicity, we assume the sparsity $s_0$ appearing in the constraint is known. It can be replaced by an upper bound on $s_0$. 
%Note here that observations over $h \in [H]$ are not independent, one way to avoid this is to use non-stationary $\PP_h$, and thus $  M_h$, for each step. 
%For this moment, we will work with stationary and see if the proofs will go through with dependence over $h$. 
} 


\begin{equation}
\begin{aligned}
&(\hat{L}, \hat{S}^{(0)}) \in 
\underset{L, S \in \mathbb{R}^{p \times q}}{\arg\min}\;  \sum_{i\le N_0, h \le H}\left\|\psi_{i,h}^{(0)\top} \bK_{\psi}^{-1}-\phi_{i,h}^{(0)\top}(L+S)\right\|_{2}^{2}   \\
&\text{s.t.}\quad L= U\Sigma V^T,
\quad \|U\|_{2,\infty} \leq \sqrt{\frac{\mu r}{p}}, \\
&\qquad \|V\|_{2,\infty} \leq \sqrt{\frac{\mu r}{q}}, \| S\|_{0}\le s_0.
\end{aligned}
\label{eq:tl-step1}
\end{equation}

 
\noindent{\sc Step II.} Use target data to correct the bias of the sparse part in an online fashion. 
\begin{equation}
\begin{aligned}
&\hat  D_n\in \underset{  D \in\RR^{p\times q}}{\arg\min}\;  \sum_{j<n, h\le H}\norm{\psi_{j,h}^{(1)\top} \bK_{\psi}^{-1} - \phi_{j,h}^{(1)\top} (\hat L + \hat S^{(0)} +  D) }_2^2 \\
&\text{s.t.}\quad \|  D\|_{0}\le e
\end{aligned}
\label{eq:tl-step2}
\end{equation}
Then the target estimator for $n$ episode is given by \begin{equation}\label{def:transfer-estimator}(\hat L_n,\hat S_n^{(1)})=(\hat L,\hat  S^{(0)}+\hat   D_n).\end{equation}

To construct the confidence region, suppose at the first stage, we established $\|L^*-\hat L\|_F^2+\|S^{*(0)}-\hat S^{(0)}\|_F^2\le \beta_{N_0}$ with probability at least $1-1/(2N^2H)$, where we slightly abuse notation by again referring to $\beta_{N_0}$ as the confidence radius at initial stage of target learning. When the source samples come from the online UCB algorithm as described in Section \ref{sec:single-task}, we have 
\begin{align}\label{eqn:initial-radius}
\beta_{N_0}=\frac{C_\beta H\log(dN_0H)}{N_0}\left(r(C_\phi C'_{\psi})^2 +sC_{\phi\psi}^2\right),
\end{align} %as in \eqref{def:beta}.
$d=\max\{p, q\}$ and $C_\phi,C_\psi,C'_\psi,C_{\phi\psi}$ are positive parameters defined in the regularity Assumption \ref{asp:regularity}, $C_\beta$ is a un iversal constant.

The online confidence region at step $n$ is then constructed as 
\begin{equation}
\label{def:confidence-region-transfer}
\tilde{\calB}_n 
= {\scriptsize\left\{
\begin{aligned}
 (L,S,D)\,:\;& \|L - \hat L^{(0)}\|_F^2 + \|S - D - \hat S^{(0)}\|_F^2 \le \beta_{N_0},\\& \|D - \hat D_n\|_F^2 \le \beta_n^{(1)},\quad\|D\|_0 \le e
\end{aligned}
\right\}.}
\end{equation}
where we incorporate $D$ in the decision variables to put direct restriction on the sparsity of sparse difference.

Similarly, the optimistic value functions are calculated as follows.
\begin{gather}
\label{eq:updateQ-transfer}
Q_{n, h}(s, a) = r(s, a) + \max_{ L, S\in\tilde\calB_n}\phi(s, a)^{\top} ( L+ S)  \Psi^{\top} V_{n, h+1}, \\
Q_{n, H+1}(s, a) = 0,  
\nonumber
\end{gather}

\begin{algorithm}[tb]
\caption{UCB-TQL for Composite MDPs}\label{algo:UCBTQL}
\begin{algorithmic}
\STATE {\bfseries Input:} $N_0$ episodes of source data, feature function $\phi\in \mathbb{R}^p, \psi\in \mathbb{R}^q$, number of episodes $N$ of the target task.
% Estimate the low-rank and sparse components of source. 
\STATE Calculate pilot transition core estimators $\hat L,\hat S^{(0)}$ using \eqref{eq:tl-step1}.

\FOR{{\bfseries episode} $n = 1,2,\ldots,N$}
\STATE Construct confidence region \eqref{def:confidence-region-transfer}. \STATE Calculate $Q_{n, h}(s, a)$ in \eqref{eq:updateQ-transfer}. 

\FOR{{\bfseries stage} $h = 1,2,\ldots,H$} 
\STATE Take action $a_{n, h}=\arg \max _{a \in \mathcal{A}} Q_{n, h}\left(s_{n, h}, a\right)$\;
\STATE Observe $s_{n, h+1}$ from target domain $\mathcal{M}^{(1)}$\;
\ENDFOR
\STATE Learn transition core estimator using \eqref{def:transfer-estimator}. 
\ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{remark}
We focus on sparsity-constrained optimization, which can be extended to a Lasso-type $L_1$ penalty for improved computational efficiency. For brevity, we omit these details here.
\end{remark}

\subsection{Regret Analysis of UCB-TQL}
The following assumption is in parallel to Assumption \ref{ass:low-rank-sparse}. 
\begin{assumption}
Consider transfer RL setting with transition similarity defined in Assumption \ref{assume:transition-similarity}. Recall that $L^*=U^*\Sigma^* V^*$.
We assume that $\|U^*\|_{2,\infty},\|V^*\|_{2,\infty}\le \sqrt{\frac{\mu r}{p}}$ and that the sparsity of $ S^{*(0)}$ and $ S^{*(1)}$ satisfies $\max\{s_0,s_1\} \leq \bar s:=\frac{\max\{p,q\}}{4C_S\mu r^3}$, for some constant $C_S$.
\label{ass:low-rank-sparse-transfer}
\end{assumption}

The following theorem demonstrates the provable benefits of UCB-TQL for the target RL task. 

\begin{lemma}[Estimation Error]
Let $N_0$ denotes the number of episodes from the source task. 
Under Assumption \ref{asp:regularity}, \ref{assume:transition-similarity}, and \ref{ass:low-rank-sparse-transfer}, 
the estimator at the end of $n^{th}$-episode satisfies with probability at least $1-1/(n^2H)$ that,
\[\norm{\hat L_n-L^*}_F^2+\norm{\hat S_n-S^{*(0)}}_F^2\leq \beta_{N_0}+\frac{e C_{\phi\psi}^2H\log\left(dnH\right)}{n},\]
where $\beta_{N_0}$ is the initial confidence radius defined in \eqref{eqn:initial-radius}.
\end{lemma}

\begin{remark}
The estimation error bound is minimax optimal with respect to $N_0$, $n$, and $d$. 
We extend these existing results in the contexts of regression and matrix completion \cite{chai2024structured} to the settings of reinforcement learning and transfer learning.
\end{remark}

\begin{theorem}[Regret upper bound for UCB-TQL] \label{thm:regret-UCB-TQL}
Let $N_0$ and $N$ denotes the number of episodes from the source and target tasks, respectively.
Let {\rm Regret(NH)}  be the accumulative regret of a total of $N$ target episodes using the UCB-TQL in Algorithm \ref{algo:UCBTQL}. 
Under Assumption \ref{asp:regularity}, \ref{assume:transition-similarity}, and \ref{ass:low-rank-sparse-transfer}, it holds that
\begin{align*}
{\rm Regret(NH)} %&\lesssim C_{\phi}C_{\psi}H\sum_{n=1}^{N}\sqrt{\beta_{N_0}}+C_{\phi}'C_{\psi}H\sum_{n=1}^{N}\sqrt{4\beta_n^{(1)}e}\\
&\lesssim C'_{reg}N/\sqrt{N_0}
+\\&C_{\phi}'C_{\psi}H^2\sqrt{e C_{\phi\psi}^2NH\log\left(dNH\right)}
\end{align*}
where $s=\max\{s_0,s_1\}$ and 
\begin{equation}
\scriptsize
C'_{reg}:=\left(C_{\phi}+C_{\phi}'\sqrt{e}\right)C_{\psi}\sqrt{C_\beta H^5\log(dN_0H)\left(r(C_\phi C'_{\psi})^2 +sC_{\phi\psi}^2\right)}.
\end{equation}

\end{theorem}

\begin{remark}
Note that the first term represents the rate at which the source is learned, while the second term accounts for correcting the bias of the sparse component. 

When the source sample size $N_0$ is sufficiently larger than the target sample size, the regret is dominated by the second term. Specifically, when $N_0 \asymp N^2$, the regret bound simplifies to $\tilde{\mathcal{O}}(\sqrt{eH^5N})$ , which scales independently of the ambient dimension. Since $e\ll d$, this represents a significant improvement over the result in \cite{yang2020reinforcement}. 

We also characterize the phase transition. Specifically, when $N_0\ge N(rC_\phi^2+s)$, neglecting the logarithm terms, the regret bound becomes dominated by the second term, corresponding to estimation of the sparse difference.
%If we further assume $C_\phi',C_\psi,C_{\phi\psi}=\calO(1)$, the second term is of order $\tilde\calO(\sqrt{eN})$, neglecting the horizon factors. 
\end{remark}

\subsection{Challenges and Proof Sketch of UCB-TQL with High-Dimensional Composite MDPs.} \label{sec:challenges-ucb-tql}
A natural way to construct the confidence region is 
\begin{align}
\label{def:confidence-region-transfer-naive}
\mathcal{B}_n=\left\{(L,S) \mid \norm{L-\hat L_n}_F^2+\norm{S-\hat S_n}_F^2\leq \beta^{(1)}_n\right\}
\end{align}
where $\beta^{(1)}_n:=\beta_{N_0}+\frac{e C_{\phi\psi}^2H\log\left(dNH\right)}{n}$.

However, this confidence region is not tight in that we are not fully utilizing the sparse difference $D$. To be more specific,
plugging the value of $\beta_n^{(1)}$ in  \eqref{equ:regret-intermediate}, we have 
\[
\begin{aligned}
{\rm Regret(NH)}
   &\lesssim C_{\phi}C_{\psi}H \sum_{n=1}^{N}\sqrt{\beta_n^{(1)}} + 1\\
&\lesssim C_{\phi}C_{\psi}H\Bigl(N \sqrt{\beta_{N_0}}
     +\\& \sqrt{e\,C_{\phi\psi}^2\,N H \,\log\bigl(d N H\bigr)}\Bigr).
\end{aligned}
\]

In contrast to \eqref{def:confidence-region-transfer-naive}, we employ a more fine-grained confidence region \eqref{def:confidence-region-transfer}, where we directly restrict the sparsity of the sparse difference $D$ to be bounded, leading to improved rates.

In particular,  we have $(L^*,S^{*(0)},D^*)\in \tilde\calB_n$, indicating this CR is valid.
To bound the one-step error, let $(\widetilde{ L},\widetilde{ S},\widetilde{D})=\arg\max_{\substack{ L, S \in \tilde{\calB}_n}}\phi(s, a)^{\top} ( L+ S)  \Psi^{\top} V_{n, h+1}$, it holds that
\begin{equation}
     \begin{aligned}
     &Q_{n,h}(s_{n,h},a_{n,h})-\paran{r(s_{n,h},a_{n,h})+[P_h V_{n,h+1}](s_{n,h},a_{n,h})} \\
     &\leq \norm{\phi_{n, h}^{\top}(\widetilde{ L} -  L^{*})}_2\norm{ \Psi^{\top} V_{n, h+1}}_{2}+\\&\norm{\phi_{n, h}^{\top}\paran{\widetilde{ S}-\widetilde{D}-  S^*+D^*}}_2\norm{ \Psi^{\top} V_{n, h+1}}_{2} +\\&\left|\phi_{n, h}^{\top}\paran{\widetilde{D}- D^*} \Psi^{\top} V_{n, h+1}\right|
     \end{aligned}
    \end{equation}
The first two terms can be bounded similar to single-task case. From the constraint in the optimization problem \eqref{eq:tl-step1} and Assumption \ref{ass:low-rank-sparse-transfer}, we have $\|\tilde D_n\|_0\le e,\quad\|D^*\|_0\le e$, implying $\|\tilde D_n-D^*\|_0\le 2e$. This observation facilitates a tight bound on the third term. Combining these one-step error bounds then yields the final regret bound in \eqref{equ:regret-intermediate}.




\section{Discussion}

When employing low-rank and sparse structures as the core for transition probabilities, several directions for future exploration emerge. Firstly, alternative sparse structures, such as row sparsity, column sparsity, or group sparsity, could be further investigated to understand their impact on learning dynamics and efficiency. These alternative formulations may offer more nuanced or efficient ways to capture the underlying patterns in transition dynamics across different domains.

Secondly, our analysis reveals that the regret bounds of the UBC-TQL algorithm are significantly influenced by the error bounds derived from matrix recovery. Since the Upper Confidence Bound (UCB) is determined by the error bounds of matrix recovery, the regret bound is largely dictated by these errors. An extension goal is to achieve the current levels of regret under more relaxed assumptions. This could involve developing new theoretical frameworks or algorithms that either provide tighter error bounds or leverage additional structure in the transition dynamics that has not been fully exploited.