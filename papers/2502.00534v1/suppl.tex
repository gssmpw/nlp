
{\large\bf SUPPLEMENTARY MATERIAL of \\
	``\TITLE''}

\paragraph{Notation}
We use $[h] = \{1,2,\ldots,h\}$ for integers from 1 to $h$. In this paper, vectors are assumed to be column vectors. For a vector $\mathbf{v} \in \mathbb{R}^p$, the norms $\|\mathbf{v}\|_1$, $\|\mathbf{v}\|_2$, and $\|\mathbf{v}\|_\infty$ represent the 1-norm, Euclidean norm(or 2-norm), and infinity norm, respectively. 
For a matrix $M \in \mathbb{R}^{p \times q}$, we use the following notation for norms: $\|M\|_0$ denotes the number of non-zero elements, $\|M\|_1 = \sum_{i=1}^p \sum_{j=1}^q |M_{ij}|$ is the sum of the absolute values of all elements, and $\|M\|_{\max} = \max_{i,j} |M_{ij}|$ is the maximum absolute value among elements. The Frobenius norm is $\|M\|_F = \sqrt{\operatorname{Tr}(M^{\top} M)} = \sqrt{\sum_{j=1}^{d_1}\sum_{k=1}^{d_2} M_{jk}^2}$, which is also equivalent to $\sqrt{\sum_{j} \sigma_j(M)^2}$, where $\sigma_j(M)$ are the singular values of $M$. The nuclear norm is $\|M\|_* = \sum_{j} \sigma_j(M)$, and the operator norm is $\|M\|_{\text{op}} = \max_j \sigma_j(M)$. For two matrices, $\langle L, S \rangle$ represents the Euclidean inner product.
We use $a_n=\calO(b_n)$ or $a_n\lesssim b_n$ if there exists some $C>0$ such that $a_n\le C b_n$. $\tilde\calO(\cdot)$ is similarly defined, neglecting logarithmic factors.
Constants $c,C,c_0,\cdots$ may vary from line to line.

\section{Regret Analysis of the Single-Task UCB-$Q$-Learning with Composite MDP Structures}
We present below the proof of Theorem \ref{thm:single-task-regret}. The proof of Lemma \ref{lemma:estimation-error-single} emerges as an intermediate step along the way.
\paragraph{Proof of Theorem \ref{thm:single-task-regret}.}
We first sketch the proof as follows. 
First of all, we define the ``good event'' that the ground truth transition core matrix before episode $n$ lies in the confidence region as $\calE_n$, i.e., $(  L^*, S^*)\in \calB_{n'}$ for any $n'\le n-1$. We assume $\calE_n$ holds first and use concentration to prove that $\calE_n$ holds with high probability later. We denote $E_n=\mathds{1}_{\calE_n}$.


\begin{enumerate}
\item Under $\calE_n$, prove $Q_{n,h}\ge Q_h^*$ using induction. 

\item Bound $Q_{n,h}(s_{n,h},a_{n,h})-[r(s_{n,h},a_{n,h})+P(\cdot|s_{n,h},a_{n,h})^TV_{n,h+1}]$.

\item Bound the total regret by one-step errors derived in Step 2.
\end{enumerate}
We elaborate each step in the sequel.
\subsection{Upper confidence bound}
\begin{lemma}\label{lem:optimism}
Given any state-action pair $(s, a) \in \mathcal{S} \times \mathcal{A}$ , for each episode $n$ and decision step $h$, we have:
$$Q_{n,h}(s, a) \geq Q_h^*(s, a).$$
\end{lemma}
\begin{proof}
We use induction.
At $h=H$, we have $Q_{n,H}=Q_H^*(s, a)=r(s,a)$. Assuming the argument is true for $1<h'\le H$, it naturally extends to $h=h'-1$ that $V_{n,h}(s)\ge V_h^*(s)$, and hence 
\begin{align*}
     Q_{n, h}(s, a) 
     &= r(s, a) + \max_{\substack{ L, S \in \mathcal{B}_n}}\phi(s, a)^{\top} ( L+ S)  \Psi^{\top} V_{n, h+1}\\
     &\geq r(s, a) + \phi(s, a)^{\top} ( L^*+ S^*)  \Psi^{\top} V_{n, h+1}\\
     &\geq r(s, a) + [P V^*_{h+1}](s, a)=Q_h^*(s, a).
\end{align*}
\end{proof} 
\subsection{One-step bound}
\begin{lemma}\label{lem:boundQ} 
     For any $h \in [H]$ and $n \in [N]$, we have 
     \[Q_{n,h}(s_{n,h},a_{n,h})-\paran{r(s_{n,h},a_{n,h})+[P_h V_{n,h+1}](s_{n,h},a_{n,h})}
     \leq C_{\phi}C_{\psi}H\sqrt{2\beta_n}.\]
     \end{lemma}

\begin{proof}
Let $(\widetilde{ L},\widetilde{ S})=\arg\max_{\substack{ L, S \in \mathcal{B}_n}}\phi(s, a)^{\top} ( L+ S)  \Psi^{\top} V_{n, h+1}$, it holds that 
\begin{align*}
   Q_{n,h}(s_{n,h},a_{n,h})-\paran{r(s_{n,h},a_{n,h})+[P_h V_{n,h+1}](s_{n,h},a_{n,h})}
   =& \phi_{n, h}^{\top}\left[\left(\widetilde{ L} + \widetilde{ S}\right) - M^*\right]  \Psi^{\top} V_{n, h+1} \\
   = &\phi_{n, h}^{\top}\left[(\widetilde{ L}- L^*) + \left(\widetilde{ S}- S^{*}\right)\right]  \Psi^{\top} V_{n, h+1}.
    \end{align*}
        Applying HÃ¶lder's inequality, the triangle inequality, and Cauchy-Schwarz inequality, we deduce the following results:
     \begin{equation}
     \label{equ:one-step-analysis}
     \begin{aligned}
     &Q_{n,h}(s_{n,h},a_{n,h})-\paran{r(s_{n,h},a_{n,h})+[P_h V_{n,h+1}](s_{n,h},a_{n,h})} \\
     &\leq \norm{\phi_{n, h}^{\top}(\widetilde{ L} -  L^{*})}_2\norm{ \Psi^{\top} V_{n, h+1}}_{2}+\norm{\phi_{n, h}^{\top}\paran{\widetilde{ S} -  S^*}}_2\norm{ \Psi^{\top} V_{n, h+1}}_{2} \\
     & \leq H\paran{C_{\psi}\norm{\phi_{n, h}^{\top}(\widetilde{ L}-  L^*)}_2+C_{\psi}\norm{\phi_{n, h}^{\top}\paran{\widetilde{ S}-  S^{*}}}_2} \\
     & \leq H\paran{C_{\psi}\norm{\phi_{n, h}}_2\norm{\widetilde{ L} -  L^{*}}_F + C_{\psi}\norm{\phi_{n, h}}_2\norm{\widetilde{ S} -  S^{*}}_F} \\
     & \leq C_{\phi}C_{\psi}H\paran{\norm{\widetilde{ L} -  L^*}_F + \norm{\widetilde{ S} -  S^{*}}_F} \\
     & \leq C_{\phi}C_{\psi}H\sqrt{2\beta_n}.
     \end{aligned}
    \end{equation}
\end{proof}


\subsection{Regret decomposition}
We bound the regret by the sum of one-step errors. To set the stage, let
$\mathcal{F}_{n,h}$ be defined as the $\sigma$-field generated by all the random variables up until episode $n$, step $h$, essentially fixing the sequence $s_{1,1}, a_{1,1}, s_{1,2}, a_{1,2}, \ldots, s_{n,h}, a_{n,h}$. 
To proceed, let 
\[\delta_{n,h} := (V_{n,h}-V_{h}^{\pi_{n}})(s_{n,h}), \quad
\gamma_{n,h}:= Q_{n,h}(s_{n,h},a_{n,h}) - \left(r(s_{n,h},a_{n,h}) + [P_h V_{n,h+1}](s_{n,h},a_{n,h})\right).\]
And hence
\begin{align*}
\text{Regret}(NH) &= \mathbb{E}\left(\sum_{n=1}^{N} \left[ V^*(s_0) - V^{\pi_n}(s_0) \right]\right) 
\\
&\leq \mathbb{E}\left( \sum_{n=1}^{N}(V_{n,1}-V_{1}^{\pi_{n}})(s_0)\right) = \sum_{n=1}^{N}\mathbb{E}(\delta_{n,1}).
\end{align*}
We have
\[\mathbb{E}(\delta_{n,1}) = \mathbb{E}(\delta_{n,1}E_n+(1-E_n)\delta_{n,1}) 
\le\mathbb{E}(\delta_{n,1}E_n)+H\PP[E_n=0]\]
and
\begin{align*}
\mathbb{E}\paran{\delta_{n,1}E_n \mid \mathcal{F}_{n,1}} =& \paran{V_{n,1} - V_{1}^{\pi_{n}}}\paran{s_{n,1}}E_n \\
\leq &Q_{n,1}\paran{s_{n,1},a_{n,1}} - V_{1}^{\pi_{n}}\paran{s_{n,1}} \\
= &\gamma_{n,1} + \paran{r\paran{s_{n,1},a_{n,1}} +[PV_{n,2}]\paran{s_{n,1},a_{n,1}}} -V_{1}^{\pi_{n}}\paran{s_{n,1}} \\
\leq &\gamma_{n,1} + \mathbb{E}\paran{\paran{V_{n,2} - V_{2}^{\pi_{n}}}\paran{s_{n,2}}E_n \mid \mathcal{F}_{n,1}} 
\leq \cdots\\\leq &\sum_{h=1}^{H}\EE(\gamma_{n,h}\mid \mathcal{F}_{n,1})\\
\leq &C_{\phi}C_{\psi}H^2\sqrt{2\beta_n}.
\end{align*}
Therefore, we establish the regret bound with high probability that
\begin{align}
\label{equ:regret-intermediate}
\text{Regret}
(NH) &\leq C_{\phi}C_{\psi}H^2\sum_{n=1}^N\sqrt{2\beta_n}+NH\PP[E_N=0],
\end{align}
where we use $\calE_n\subseteq \calE_{n'}$ with $n>n'$.

\subsection{Confidence region}
In this subsection, we validate the confidence region. Recall the CR is defined in \eqref{def:confidence-region}.

Denote $X_n=\begin{bmatrix}
\phi_{1,1}^\top\\\cdots\\\phi_{1,H}^\top\\\cdots\\\phi^\top_{n-1,H}
\end{bmatrix}$and $Y_n=\begin{bmatrix}
\psi_{1,1}^\top  \bK_{\psi}^{-1}\\\cdots\\\psi_{1,H}^\top  \bK_{\psi}^{-1}\\\cdots\\\psi_{n-1,H}^\top  \bK_{\psi}^{-1}
\end{bmatrix}$, we have the observational model as follows:
\begin{align*}
Y_n=X_n(L^*+S^*)+W_n
\end{align*}
where $W_n=Y_n-X_n(L^*+S^*)$.


In view of \cite{chai2024structured}, we need the observation model to satisfy the restricted strong convexity condition in order for the low-rank part and sparse part to be separated.
\begin{align}
\label{cond:minimum-eigenvalue}\lambda_{\min}\left(\frac{X_n^\top X_n}{n-1}\right)\ge c_1.\end{align}
We will later give specific cases in which this inequality holds. In the sequel, we bound $\|X_n^\top W_n\|_2$ and $\|X_n^\top W_n\|_{\max}$.

In fact, we can express $X_n^\top W_n$ as 
\begin{align*}
X_n^\top W_n=\sum_{i=1}^{n-1}\sum_{h=1}^H \phi_{i,h}\left(\psi_{i,h}^\top \bK_{\psi}^{-1}-\phi_{i,h}^\top   M^*\right).
\end{align*}
Note that $\EE\left[\psi_{i,h}^\top \bK_{\psi}^{-1}-\phi_{i,h}^\top   M^*|\calF_{i,h}\right]=0$, and $X_n^\top W_n$ is a sum of martingale differences.
Let $Z_{i,h}=\phi_{i,h}\left(\psi_{i,h}^\top \bK_{\psi}^{-1}-\phi_{i,h}^\top   M^*\right)$, we have that 
\begin{align*}\|Z_{i,h}\|_2&=\|\phi_{i,h}\|\cdot \|\psi_{i,h}^\top \bK_{\psi}^{-1}-\phi_{i,h}^\top   M^*\|\\&=\|\phi_{i,h}\|\cdot \left(\|\psi_{i,h}^\top \bK_{\psi}^{-1}\|+\|\phi_{i,h}^\top   M^*\|\right)\\&\le 2C_\phi C'_\psi,
\end{align*}
where we used $\|\phi_{i,h}^\top   M^*\|=\Big\|\EE\left[\psi_{i,h}^\top \bK_{\psi}^{-1}|\calF_{i,h}\right]\Big\|\le C'_\psi$.

On the other hand,
\[\Big\|\sum_{i=1}^{n-1}\sum_{h=1}^H \EE[Z_{i,h}^{\top} Z_{i,h}|\calF_{i,h}]\Big\|\le 4nH(C_{\phi}C_{\psi}')^2\]
and 
\[\Big\|\sum_{i=1}^{n-1}\sum_{h=1}^H \EE[Z_{i,h} Z_{i,h}^{\top}|\calF_{i,h}]\Big\|\le 4nH(C_{\phi}C_{\psi}')^2.\]
By Matrix Freedman inequality (Corollary 1.3 in ~\cite{tropp2011freedman}), we have with probability at least $1-\delta$ that
\begin{align*}
\|X_n^\top W_n\|_2\lesssim C_{\phi}C'_{\psi}\log\left(\frac{d}{\delta}\right)+C_{\phi}C_{\psi}'\sqrt{nH\log\left(\frac{d}{\delta}\right)}.
\end{align*}

Similarly, each entry of $X_n^\top W_n$ is the sum of  martingale differences, almost surely bounded by $2C_{\phi\psi}$, and we have by Azuma-Hoeffding's inequality and a union bound over $d^2$ entries that, with probability at least $1-\delta$,
\begin{align*}
\|X_n^\top W_n\|_{\max}=\max_{i,j}|[X_n^\top W_n]_{ij}|\lesssim C_{\phi\psi}\sqrt{nH\log\left(\frac{d}{\delta}\right)}.
\end{align*}





\emph{Confidence Region}

By the optimality condition, we have
\begin{align*}
\|Y_n-X_n(\hat L_n+\hat S_n)\|_F^2\le \|Y_n-X_n( L^*+ S^*)\|_F^2.
\end{align*}
Expanding on both sides yields
\begin{align*}
\|X_n(\Delta_L+\Delta_S)\|_F^2&\le 2\langle W_n,X_n(\Delta_L+\Delta_S)\rangle\\
&=2\langle X_n^\top W_n,\Delta_L+\Delta_S\rangle\\&\le 2\|X_n^\top W_n\|_2\|\Delta_L\|_*+2\|X_n^\top W_n\|_{\max}\|\Delta_S\|_1\\&\le 2\langle X_n^\top W_n,\Delta_L+\Delta_S\rangle\\&\le 2\sqrt{2r}\|X_n^\top W_n\|_2\|\Delta_L\|_F+2\sqrt{2s}\|X_n^\top W_n\|_{\max}\|\Delta_S\|_F,
\end{align*}
where we denote $\Delta_L:=\hat  L_n- L^*$ and $\Delta_S:=\hat  S_n- S^*$.

On the other hand, by \eqref{cond:minimum-eigenvalue} and separation lemma in \cite{chai2024structured}, we have that 
\begin{align*}
\|X_n(\Delta_L+\Delta_S)\|_F^2&\ge c_1(n-1)\|\Delta_L+\Delta_S\|_F^2\\&\ge \frac{c_1(n-1)}{2}\left(\|\Delta_L\|_F^2+\|\Delta_S\|_F^2\right).
\end{align*}
Putting together, we have
\begin{align*}
\|\Delta_L\|_F^2+\|\Delta_S\|_F^2&\le \frac{4}{c_1(n-1)} \sqrt{2r}\|X_n^\top W_n\|_2\|\Delta_L\|_F+\sqrt{2s}\|X_n^\top W_n\|_{\max}\|\Delta_S\|_F
\\&\le \frac{4}{c_1(n-1)} \sqrt{2r\|X_n^\top W_n\|_2^2+2s\|X_n^\top W_n\|_{\max}^2}\cdot\sqrt{\|\Delta_L\|_F^2+\|\Delta_S\|_F^2}\end{align*}
which implies that 
\begin{align*}
\|\Delta_L\|_F^2+\|\Delta_S\|_F^2&\le \frac{32}{c_1^2(n-1)^2}\left(r\|X_n^\top W_n\|_2^2+s\|X_n^\top W_n\|_{\max}^2\right).
\end{align*}
Plugging in the aforementioned bounds of $\|X_n^\top W_n\|_2$ and $\|X_n^\top W_n\|_{\max}$, we deduce that $\calB_n$ is a valid $\delta$-confidence region if we take \begin{align}
\label{def:beta}\beta_n=\frac{C_\beta}{n^2}\left(r(C_\phi C'_{\psi})^2 nH\log(d/\delta)+sC_{\phi\psi}^2nH\log(d/\delta)\right)=\frac{C_\beta H\log(d/\delta)}{n}\left(r(C_\phi C'_{\psi})^2 +sC_{\phi\psi}^2\right)
\end{align} 
for some large enough $C_\beta$. In particular, we take $\delta=1/(N^2H)$ in the above display, then by the union bound, $\PP(E_N=0)\le N\cdot\frac{1}{N^2H}=\frac{1}{NH}$.

Combining the definition of $\beta_n$ with \eqref{equ:regret-intermediate}, we obtain that
\begin{align*}
\text{Regret}
(NH) &\le C_{\phi}C_{\psi}H^2\sum_{n=1}^N\sqrt{2\beta_n}+1\\
&\lesssim C_{reg}\sqrt{NH^5},
\end{align*}
where $C_{reg}:=C_\phi C_\psi \sqrt{C_\beta\left(r(C_\phi C'_{\psi})^2 +sC_{\phi\psi}^2\right)\log(dNH)}$.


As a byproduct, we have that by the end of the $N$ episode, with probability at least $1-1/(N^2H)$,
\begin{align*}
\norm{ L-\hat L_N}_F^2+\norm{ S-\hat S_N}_F^2\leq \beta_N.
\end{align*}



\subsection{Discussion on Condition \eqref{cond:minimum-eigenvalue}}
Now we provide an example where Condition $\eqref{cond:minimum-eigenvalue}$ holds. At a high level, $\phi_{i,h}$ may be highly correlated, across different steps and episodes. Nonetheless, note that each episodes starts at independent initial states, hence providing diversity to the linear operator $X_n$. In fact, Condition \eqref{cond:minimum-eigenvalue} holds when $\phi(s,a)$ depends mainly on $s$ and $a$ adds a perturbation effect. To be more concrete, consider the following lemma as an example.

\begin{lemma}
Suppose there exists some  function $\bar\phi$ such that  $\|\phi(s,a)-\bar\phi(s)\|_2\le \eta$. And 
\begin{align*}
\lambda_{\min}\left(\EE_\mu[\bar\phi(s)\bar\phi(s)^\top]\right)\ge c_{\min}.
\end{align*}
If $\eta(2C_\phi+\eta)\le \frac{c_{\min}}{4}$ and $n\ge C_e d$ for some constant $C_e$, then with probability at least $1-2e^{-c_en}$, Condition \eqref{cond:minimum-eigenvalue} holds with $c_1=c_{\min}/4$.
\end{lemma}
\begin{proof}
Denote $\EE_\mu[\bar\phi(s)\bar\phi(s)^\top]$ by $\Sigma_\mu$. As $|\bar\phi(s)^\top v|\le \|\bar\phi(s)\|\le C_\phi$ for any $v\in \mathbb{S}^d$, we have that $\bar\phi(s)$ is subGaussian with variance proxy $(C-\phi/2)^2$.
By (5.25) in \cite{vershynin2010introduction}, there exists some constants $C_e,c_e$ such that with probability at least $1-2e^{-c_en}$,
\begin{align*}
\frac{1}{n-1}\sum_{i=1}^{n-1}\bar\phi(s_{i,1})\bar\phi(s_{i,1})^\top\succeq\frac{1}{2}\Sigma_\mu\succeq \frac{c_{\min}}{2}I
\end{align*} as long as $n\ge C_ed$.

Let $\Delta=\frac{1}{n-1}\sum_{i=1}^{n-1}\bar\phi(s_{i,1})\bar\phi(s_{i,1})^\top-\frac{1}{n-1}\sum_{i=1}^{n-1}\phi_{i,1}\phi_{i,1}^\top$, we have for any $v\in \mathbb{S}^d$ that
\begin{align*}
v^\top \Delta v&=\frac{1}{n-1}\sum_{i=1}^{n-1}\left([\bar\phi(s_{i,1})^\top v]^2-[\phi_{i,1}^\top v]^2\right)\\&\le \frac{1}{n-1}\sum_{i=1}^{n-1}\Big\|\bar\phi(s_{i,1})-\phi_{i,1}\Big\|\cdot\Big\|\bar\phi(s_{i,1})+\phi_{i,1}\Big\|\\&\le \eta(2C_\phi+\eta).
\end{align*}
Hence $\|\Delta\|\le \eta(2C_\phi+\eta)\le \frac{c_{\min}}{4}$. It follows that
\begin{align*}
\frac{1}{n-1}\sum_{i=1}^{n-1}\phi_{i,1}\phi_{i,1}^\top\succeq \frac{c_{\min}}{4}I.
\end{align*}
To conclude, note that 
\begin{align*}
\frac{1}{n-1}\sum_{i=1}^{n-1}\sum_{h=1}^H\phi_{i,h}\phi_{i,h}^\top
\succeq\frac{1}{n-1}\sum_{i=1}^{n-1}\phi_{i,1}\phi_{i,1}^\top\succeq \frac{c_{\min}}{4}I.
\end{align*}
\end{proof}

\begin{remark}
The condition $n\ge C_e d$ requires a warm start. For $n\le C_e d$, one can use a fixed policy to generate samples. This will not affect the total regret as long as $dH$ is neglegible compared to $\sqrt{NH^5}$.
\end{remark}

\section{Regret Analysis for UCB-TQL under Composite MDPs}

In this section, we provide proof for Theorem \ref{thm:regret-UCB-TQL}. The pipeline is similar to the single-task setting.

We start by constructing the confidence region.
To that end, again denote
Denote $X_n^{(1)}=\begin{bmatrix}
\phi_{1,1}^{(1)\top}\\\cdots\\\phi_{1,H}^{(1)\top}\\\cdots\\\phi^{(1)\top}_{n-1,H}
\end{bmatrix}$and $Y_n^{(1)}=\begin{bmatrix}
\psi_{1,1}^{(1)\top}  \bK_{\psi}^{-1}\\\cdots\\\psi_{1,H}^{(1)\top}  \bK_{\psi}^{-1}\\\cdots\\\psi_{n-1,H}^{(1)\top} \bK_{\psi}^{-1}
\end{bmatrix}$, we have the observational model as follows:
\begin{align*}
Y_n^{(1)}=X_n^{(1)}(L^*+S^{*(0)}+D^*)+W_n
\end{align*}
where $W_n:=Y_n^{(1)}-X_n^{(1)}(L^*+S^{*(0)}+D^*)$.

By the optimality condition, we have 
\begin{align*}
\|Y_n^{(1)}-X_n^{(1)}(\hat L+\hat S^{(0)}-\hat D_n)\|_F^2\le \|Y_n^{(1)}-X_n^{(1)}(L^*+S^{*(0)}+D^*)\|_F^2.
\end{align*}

After some calculation, we obtain
\begin{align*}
\|X_n^{(1)}(\hat D_n-D^*)\|_F^2&\le 2\Big\langle Y_n^{(1)}-X_n^{(1)}(\hat L+\hat S^{(0)}-D^*),X_n^{(1)}(\hat D_n-D^*)\Big\rangle\\
&=2\Big\langle  
X_n^{(1)\top}\left(W_n+X_n^{(1)}(L^*-\hat L+S^{*(0)}-\hat S^{(0)})\right),\hat D_n-D^*
\Big\rangle\\&\le
2\Big\| 
X_n^{(1)\top}W_n
\Big\|_\infty \Big\|\hat D_n-D^*\Big\|_1+2\Big\|X_n^{(1)\top}X_n^{(1)}(L^*-\hat L+S^{*(0)}-\hat S^{(0)})\Big\|_F\Big\|\hat D_n-D^*\Big\|_F\\&\le
2\sqrt{2e}\Big\| 
X_n^{(1)\top}W_n
\Big\|_{\max} \Big\|\hat D_n-D^*\Big\|_F+2\Big\|X_n^{(1)\top}X_n^{(1)}(L^*-\hat L+S^{*(0)}-\hat S^{(0)})\Big\|_F\Big\|\hat D_n-D^*\Big\|_F.
\end{align*}

We have, similar as before, with probability at least $1-\delta$,
\begin{align*}
\| 
X_n^{(1)\top}W_n
\Big\|_{\max}\lesssim C_{\phi\psi}\sqrt{nH\log\left(\frac{d}{\delta}\right)}.
\end{align*}
If 
\begin{align*}
\lambda_{\min}\left(\frac{X_n^{(1)\top} X_n^{(1)}}{n-1}\right)\ge c_1,\\
\lambda_{\max}\left(\frac{X_n^{(1)\top} X_n^{(1)}}{n-1}\right)\le C_1,
\end{align*}
then it holds that 
\begin{align*}
\|\hat D_n-D^*\|_F^2\lesssim  \frac{eC_{\phi\psi}^2H\log\left(\frac{d}{\delta}\right)}{n}+\|L^*-\hat L\|_F^2+\|S^{*(0)}-\hat S^{(0)}\|_F^2.
\end{align*}

Suppose at the first stage, we established $\|L^*-\hat L\|_F^2+\|S^{*(0)}-\hat S^{(0)}\|_F^2\le \beta_{N_0}$ with probability at least $1-1/(2N^2H)$, where \begin{align*}\beta_{N_0}=\frac{C_\beta H\log(d/\delta)}{N_0}\left(r(C_\phi C'_{\psi})^2 +sC_{\phi\psi}^2\right),
\end{align*}as in \eqref{def:beta}. 

\paragraph{Naive CR}
Recall that we can construct a naive confidence region as 
\begin{align*}
\mathcal{B}_n=\left\{(L,S) \mid \norm{L-\hat L_n}_F^2+\norm{S-\hat S_n}_F^2\leq \beta^{(1)}_n\right\}
\end{align*}
where $\beta^{(1)}_n:=\beta_{N_0}+\frac{e C_{\phi\psi}^2H\log\left(dNH\right)}{n}$.

When using this confidence region to construct optimistic value functions,
we can plug the value of $\beta_n^{(1)}$ into  \eqref{equ:regret-intermediate}. It follows that
\begin{align*}
\text{Regret}
( N  H) &\lesssim C_{\phi}C_{\psi}H^2\sum_{n=1}^{ N}\sqrt{\beta_n^{(1)}}+1\\&\lesssim C_{\phi}C_{\psi}H^2\left( N \sqrt{\beta_{N_0}}+\sqrt{e C_{\phi\psi}^2 N H\log\left(dNH\right)}\right).
\end{align*}
\begin{remark}
When $N_0\gg  N $, this bound is dominated by the second term, which depends on $C_{\phi}$. 
\end{remark}

\paragraph{Tight CR}
As illustrated in the proof sketch, we can achieve better rate by constructing a more fine-grained confidence region as 
\begin{align*}
\tilde{\calB}_n=\left\{(L,S,D) \mid \norm{L-\hat L^{(0)}}_F^2+\norm{S-D-\hat S^{(0)}}_F^2\le \beta_{N_0},\ \norm{D-\hat D_n}_F^2\le \beta_n^{(1)},\ \norm{D}_0\le e\right\}.
\end{align*}
It is straightforward to show that $(L^*,S^{*(0)},D^*)\in \tilde\calB_n$.
We then carry out a more refined one-step analysis, similar in the vein of Section 3.2.2.
In particular, let $(\widetilde{ L},\widetilde{ S},\widetilde{D})=\arg\max_{\substack{ L, S \in \tilde{\calB}_n}}\phi(s, a)^{\top} ( L+ S)  \Psi^{\top} V_{n, h+1}$, it holds that
\begin{equation}
     \begin{aligned}
     &Q_{n,h}(s_{n,h},a_{n,h})-\paran{r(s_{n,h},a_{n,h})+[P_h V_{n,h+1}](s_{n,h},a_{n,h})} \\
     &\leq \norm{\phi_{n, h}^{\top}(\widetilde{ L} -  L^{*})}_2\norm{ \Psi^{\top} V_{n, h+1}}_{2}+\norm{\phi_{n, h}^{\top}\paran{\widetilde{ S}-\widetilde{D}-  S^*+D^*}}_2\norm{ \Psi^{\top} V_{n, h+1}}_{2} +\left|\phi_{n, h}^{\top}\paran{\widetilde{D}- D^*} \Psi^{\top} V_{n, h+1}\right| \\
     & \leq H\paran{C_{\psi}\norm{\phi_{n, h}^{\top}(\widetilde{ L} -  L^*)}_2+C_{\psi}\norm{\phi_{n, h}^{\top}\paran{\widetilde{ S}-\widetilde{D}-  S^*+D^*}}_2}+\sqrt{2e}HC_{\phi}'C_{\psi}\norm{\widetilde{D}-D^*}_F \\
     & \leq H\paran{C_{\psi}\norm{\phi_{n, h}}_2\norm{\widetilde{ L} -  L^{*}}_F + C_{\psi}\norm{\phi_{n, h}}_2\norm{\widetilde{ S}-\widetilde{D}-  S^*+D^*}_F}+\sqrt{2e}HC_{\phi}'C_{\psi}\norm{\widetilde{D}-D^*}_F \\
     & \leq C_{\phi}C_{\psi}H\paran{\norm{\widetilde{ L}-  L^*}_F + \norm{\widetilde{ S}-\widetilde{D}-  S^*+D^*}_F}+\sqrt{2e}HC_{\phi}'C_{\psi}\norm{\widetilde{D}-D^*}_F \\
     & \leq C_{\phi}C_{\psi}H\sqrt{2\beta_{N_0}}+C_{\phi}'C_{\psi}H\sqrt{4\beta_n^{(1)}e},
     \end{aligned}
    \end{equation}
where the sparsity constraint on $D$ is used in bound the third term in the second line.
Combined with the one-step error in the regret decomposition~\eqref{equ:regret-intermediate}, we obtain that
\begin{align*}
\text{Regret}
( N  H) &\lesssim C_{\phi}C_{\psi}H^2\sum_{n=1}^{ N }\sqrt{\beta_{N_0}}+C_{\phi}'C_{\psi}H^2\sum_{n=1}^{ N }\sqrt{4\beta_n^{(1)}e}\\&\lesssim 
\left(C_{\phi}+C_{\phi}'\sqrt{e}\right)C_{\psi}H^2 N \sqrt{\beta_{N_0}}+C_{\phi}'C_{\psi}H^2\sqrt{e C_{\phi\psi}^2 N H\log\left(dNH\right)}.
\end{align*}
Plugging the definition of $\beta_{N_0}$ completes the proof.

\begin{remark}
When $N_0\gg N$, this bound is dominated by the second term, which does not depend on $C_{\phi}$, but rather on $C_{\phi}'$. It is tighter than the rate of naive CR approach as $C_{\phi}\ge C_{\phi}'$.
\end{remark}