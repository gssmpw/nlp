\label{sec:user_study}

We conducted a user study with 12 participants to evaluate {\sc WhatELSE}. The goals of the study were to (1) understand their usage and perception of the Narrative Space Editor, (2) validate the effectiveness of Interactive Narrative Compiler in preserving authorial intent, and (3) explore the benefits and drawbacks of the AI-bridged IN creation workflow.

\subsection{Participants}
We recruited 12 participants (6 male, 6 female, between 20 to 34 years old, average 27 years) from within our institution \evaluation{by posting on an internal network channel. The demographic information of the participants was provided in the supplementary material.}
Participants were asked to self-report their experience in using generative AI (e.g., ChatGPT), digital games, and interactive narratives. Each was compensated with a \$75 gift card. All participants reported at least moderate experience in using generative AI. More than half of the participants used to experience interactive narratives. Five participants reported spending more than two hours a week playing digital games. 
Nine participants reported having no experience creating interactive narratives, while the other three reported having very limited experience.

\subsection{Task}
%The goal of this task is to evaluate the effectiveness of \textsc{WhatELSE} in supporting the AI-bridged interactive narrative creation workflow to generate play-time game plot execution. 
The task is to create a playable game plot using the systems provided.
Given a story domain and a narrative example with a story moral, participants were asked to generate an outline to guide the creation of a game plot that expresses the same moral as the given narrative instance.
The task uses the \presentation{\textit{"Fairytale Forest"} story domain (Figure~\ref{baseline}.3)}. It contains six characters (ant, dove, hunter, witch, cat, and deer), five locations (mountain, forest, village, brook, and witch house), and an action schema with six actions ("move to", "speak to", "kill", "attack", "think", "save") that defines what characters can perform. 
%It is a simple and easy-to-understand setting where characters have straightforward personas, making it suitable for novice users with limited writing experience to create interactive narratives. This domain also facilitates quick creation during the user study. 
Based on the story domain, we adapted two stories widely used in previous studies as narrative examples  ~\cite{suwartini2019culture,affendi2018elements}. These stories feature clear plots centered on emerging danger and the characters' decisions to help each other. The two narratives convey two distinct morals: \textit{``Kindness is never wasted''} and \textit{``Kindness is not always rewarded''}. Details of story domain and examples can be found in the supplementary material. We chose this story domain and narrative examples because they are simple and easy to follow, making it suitable for novice users with limited experience in IN. We used the pre-defined story moral as a representation of the authorial intent.

To complete the task, participants are encouraged to meet these criteria for their final outline: (1) The outline should consist of 3–4 sentences. (2) Each sentence must have at least one abstract element, allowing multiple events to fit the description. (3) The outline should convey the same moral as the provided narrative instance. (4) Participants are encouraged to make creative edits to the narrative instance, ensuring that their authorial intent is reflected in the outline. These criteria guide participants by providing clear objectives and controlling the process.


\begin{figure*}[h]
\centering
\includegraphics[width=\textwidth]{figures/baseline_interface_v2.png}
\vspace{-10pt}
\caption{\baseline{An illustration of the baseline system that uses a prompt-based approach for (1) the outline generation and (2) IN instantiation with (3) the pre-loaded Fairytale Forest story domain. (4) During runtime, LLM acts as a text adventure game engine, dynamically generating game plots based on the outline and responding to player input in the baseline system. }}
~\label{baseline}
\vspace{-10pt}
\end{figure*}

\subsection{Conditions}

\baseline{We compared \textsc{WhatELSE} with a baseline system that uses a conversational LLM-powered assistant in a counterbalanced within-subject design. This comparison aimed to evaluate the effectiveness of \textsc{WhatELSE} in supporting two features: (1) generating outlines from a narrative instance and (2) instantiating narratives from the outline (IN Compiler). Specifically, neither of the features is supported by traditional IN authoring tools such as Twine \cite{friedhoff2013untangling}. They are also not natively supported by a plain ChatGPT-like interface. Therefore, we implemented a baseline system with these two features empowered by LLMs for a fair comparison. 
% \zl{todo: these features are not supported by existing tools like twine. In order to do so, generative AI is needed. So...}
Participants could load a narrative instance and use the chatbox with a send button to prompt natural language queries. By pressing the ``Generate Outline'' button, an auto-generated outline would be displayed on the right. This outline generation used a prompt-based approach that loaded the chatbox history as requirements and outputted the outline in JSON format (Figure~\ref{baseline}.1). Participants can also directly edit the generated outline. To instantiate narratives from the outline, we implemented a prompt-based plot generation approach (Figure~\ref{baseline}.2, prompt in Appendix) similar to prior work \cite{ai_dungeon2}, with preloaded knowledge of the story domain on characters, locations and action schema (Figure~\ref{baseline}.3). During runtime, the LLM acts as a text adventure game engine, dynamically generating game plots based on the outline and responding to player input in the baseline system (Figure~\ref{baseline}.4). }

\subsection{Procedure} 

All 12 studies were hosted in person. After obtaining consent, an experimenter introduced the background of IN, including the concepts of narrative instance, narrative space, and playable game plots. \evaluation{Participants were oriented to complete the task by the experimenter, who provided instructions, answered questions, and offered help throughout the study}. They were informed of two systems. Each system has two stages: creating an outline and playing the game plot generated from it. 

In each condition, participants had five minutes to prepare a narrative instance with a story moral. An example was provided as a starting point. To prime them for a creator's mindset, participants were encouraged to edit the given story by highlighting interesting scenes or adding details, ensuring their edits still aligned with the story moral.
Upon editing the story, participants received a walk-through tutorial on the assigned system's features. Then, they had five minutes to practice and familiarize themselves with the tool. 
After the tutorial, participants used the assigned system to create a narrative outline for 15 minutes. They were encouraged to iteratively refine their outline until it met the task criteria. Upon completion, participants were asked to fill out a narrative editing questionnaire (Figure~\ref{fig:questionnaire}.a) rating their experience with the system. 

After filling out the questionnaire, participants played a turn-based interactive narrative game as the Dove in the Fairytale Forest. In each turn, they could choose their actions, with the plot developments guided by the outline they created. They then filled out a game plot questionnaire (Figure~\ref{fig:questionnaire}.b) rating their experience with the generated plots.  Finally, we conducted a 10-minute post-study semi-structured interview about their preferences for the two systems. The study took about 70 minutes in total.

%\subsection{Measure and Data Analysis} \qz{describe questionnaire here} We conducted the Wilcoxon signed rank test on the questionnaire results with significant values reported for $p<.05(*), p<.01(**),$ and $p<.001(***)$ respectively. Effect sizes are reported as partial eta squared ($\eta_p^2$) \qz{if any, otherwise remove this}. Numbers in brackets indicate Median ($Med$). We collected transcribed audio from the interview and analyzed the results using thematic clustering.

\evaluation{
\subsection{Measurement and Analysis}
We collected answers from a semi-structured interview and two questionnaires (narrative editing and game plot questionnaire). We also recorded their interaction activity in the interface. 
The narrative editing questionnaire focused on assessing the authoring experience. It has 11 questions, including 7 questions rating the controllability, expressivity, and perceived overall experience of the system, as well as 4 questions from the NASA Task Load Index (TLX) ~\cite{hart1988development} which measured participants' perceived workload while using the system. 
The game plot questionnaire focused on evaluating the player experience and their perceived quality of the generated game plots. This questionnaire included 9 questions rating controllability, expressivity, and the overall quality of the game plots.
To examine the potential differences between conditions, we conducted the Wilcoxon signed rank test on the questionnaire results. For the qualitative data collected in the interview, we transcribed audio from the interview and analyzed the results using thematic clustering ~\cite{blandford2016qualitative}. 
}


\subsection{Results}
We present questionnaire results (Figure~\ref{fig:questionnaire}), observation of participants' interactions\footnote{A recording error resulted in the loss of two participant’s interaction recordings.} (Figure~\ref{fig:behaviors}), and their interview responses to understand how \textsc{WhatELSE} helps people shape the narrative space and further unfold it into executable game events. Numbers in brackets indicate Median ($Med$).  

\begin{figure*}[t]
  \centering
  \subfloat[Outline Editing Questionnaire - Experience and Perceived Quality]{\includegraphics[width=0.98\textwidth]{figures/interface_eval_0902.png}\label{user_study:interface}}
  \hfill
  \subfloat[\evaluation{Outline Editing Questionnaire - NASA TLX Rating.}]{\includegraphics[width=0.9\textwidth]{figures/nasa_0902_v3.png}}\label{user_study:interface}
  \hfill
  \subfloat[Game Plot Questionnaire]{\includegraphics[width=0.98\textwidth]{figures/plot_eval_0902.png}\label{user_study:game_plot}}
  \vspace{-5pt}
  \caption{Questionnaire results comparing \textsc{WhatELSE} with the baseline.}
\label{fig:questionnaire}
\vspace{-10pt}
\end{figure*}

%\subsubsection{WhatELSE helps shaping the Narrative Space}

\paragraph{WhatELSE supports the creation of a narrative outline through authorial controls.} Participants reported to have more control in generating the outline using \textsc{WhatELSE} ($Med$: 7) than baseline ($Med$: 5, $p = 0.048$). They also reported that they were less restrained in creation using \textsc{WhatELSE} ($Med$: 7) than the baseline ($Med$: 6, $p = 0.049$). In the interview, most participants (9 out of 12) commented that the abstraction tools were helpful. As P4 noted, the ability to select the level of abstraction \textit{``allows users to have more control via clicking buttons.''} Similarly, P7 highlighted that the Abstraction Tooltip enhanced the creation process by providing \textit{``an easier interface to change anything you got."} Overall, participants were more satisfied with the outline generated using \textsc{WhatELSE} ($Med$: 7) compared to the baseline ($Med$: 6, $p = 0.047$). They felt more successful in completing the task of creating outlines with \textsc{WhatELSE} ($Med$: 6) compared to the baseline ($Med$: 5, $p = 0.025$). 


%\paragraph{Participants were more satisfied with the outline plot using \textsc{WhatELSE} over baseline.}  Overall, participants were more satisfied with the outline generated using \textsc{WhatELSE} (median rating: 7) compared to the baseline (median rating: 6, $p = 0.047$). Additionally, participants felt more successful in completing the task of creating outlines with \textsc{WhatELSE} (median rating: 6) compared to the baseline (median rating: 5, $p = 0.025$). 


\paragraph{WhatELSE helps participants perceive the Narrative Space} We found participants alternated between the narrative outline and narrative instance to perceive the narrative space. Nine participants alternated between Variant and Outline View to edit the narrative space, and two participants alternated between Pivot and Outline View (Figure~\ref{fig:behaviors}). While the Outline View gives a rough idea of the narrative space, participants found the Variant View offered more detailed information. For example, P2 found the Outline View useful but also commented it \textit{``very summarized though''}. They then switched to the Variant View for concrete examples, which provided them with \textit{``interesting variations''} (P2) that they found \textit{``very helpful for inspiration''}. We observed multiple participants having similar surprising reactions to the variants, indicating that people may \textit{underestimate} the scale of narrative space by working on the outline. Participants also used the Variants View to prevent deviation from their authorial intent. For example, P1 frequently checked the variants for their distance to the given story moral, stating, \textit{``I pretty much completely relied on that, right? OK, if anyone here (character) isn't following my vision for the story, then, you know, I can easily know what the problem is.''} This indicates that the visualization helped him quickly identify unexpected elements in the narrative space. 

%\subsubsection{Experiencing as the Player}

\paragraph{WhatELSE preserves the authorial intent in the gameplay. } While playing the game generated from the outline, participants reported that the progression of the game plot aligns better with the story moral using \textsc{WhatELSE} ($Med$: 6.0) compared to the baseline ($Med$: 4.0, $p = 0.020$). For instance, P10 expressed that in the baseline condition, the story moral in the game felt \textit{"some kind of lost"}, indicating a misalignment with their expectations. This contrasts with P10's experience in WhatELSE, where the moral was better preserved within the plot. P5's experience also showcases the effectiveness of the Interactive Narrative Compiler in preserving the authorial intent. P5 created an outline to express \textit{``kindness is not always rewarded''}. In the first round of gameplay, they killed both a weak character (an ant) and a threatening hunter instead of saving the ant. In the second round, the system adapted by having other characters (a deer and a cat) try to stop P5's killing behaviors, but P5 killed them too. In the final round, the system output a monologue that forced P5 to reflect on their actions, conveying the lesson that \textit{"kindness is not always rewarded"}, with kindness coming from NPCs rather than from the players. P5 praised \textsc{WhatELSE} for its adaptability and for effectively conveying the intended moral, describing the system as \textit{``smart''} for how it \textit{``tied my actions and packaged it to be its original directive''}, which was the story moral. 


\begin{figure}
    \centering
    \includegraphics[width=.95\columnwidth]{figures/result-behaviors.png}
    \caption{Percentage of participants' interaction time in each view using \textsc{WhatELSE} to create an executable interactive narrative.}
    \label{fig:behaviors}
    \Description{}
\end{figure}


\paragraph{WhatELSE provides engaging game events reacting to player actions. } Overall, participants were more satisfied with the game events generated by \textsc{WhatELSE} ($Med$: 6.5) over the baseline ($Med$: 4.5, $p = 0.014$). They felt that \textsc{WhatELSE} responded significantly to their actions, enhancing their sense of agency. For instance, in the baseline, P6 killed a character, but due to a lack of narrative planning, the supposedly dead character reappeared after several rounds. \presentation{However, in {\sc WhatELSE}-generated games, their actions had a greater impact}; when they tried to "kill" a character, the game responded appropriately. They also found the game plot to be more complete and they will spend less further work in editing narrative space ($Med$: 4.5) compared to the baseline ($Med$: 3.0, $p = 0.027$). 




%In this section, we report and analyze the difference in participants' experience and perceptions in our user study when using \textsc{WhatELSE} and the baseline tool. For brevity, we denote the participants in the user study as P1-P12.

%\subsubsection{Experience in Stage 1 of Sculpting Narrative Space} As described in the protocol, we conducted a questionnaire with 20 self-reported questions after participants completed stage 1 of the task, where they created an outline based on a linear narrative. The results of participants' ratings on their experience using different versions of systems are shown in Fig. ~\ref{user_study:interface}. The plot demonstrates a consistent trend where participants rated \textsc{WhatELSE} better than the baseline. To formally examine participants' evaluations from different perspectives, we conducted the Wilcoxon rank-sum test between people's ratings on different systems. Overall, participants were more satisfied with the outline generated using \textsc{WhatELSE} (median rating: 7) compared to the baseline (median rating: 6, $p = 0.047$). Additionally, participants felt more successful in completing the task of creating outlines with \textsc{WhatELSE} (median rating: 6) compared to the baseline (median rating: 5, $p = 0.025$). We found an essential advantage of \textsc{WhatELSE} is that participants felt that using it gives them more control in generating the outline (WhatELSE median rating: 7, baseline median rating: 5, $p = 0.048$). Such an advantage is also reflected in that they feel less restrained in creation using \textsc{WhatELSE} (median rating: 7) compared to baseline (median rating: 6, $p = 0.049$). Despite that participants also slightly rated higher for the efficiency and ease of use of WhatELSE, we did not detect a significant difference in their ratings (Easy to use: $ p = 0.055 $, efficiency: $p = 0.080$). From the angle of moral expression, participants consider that their outlines generated using both systems expressed the specified moral ($p > 0.05$), and progressed as they expected ($p > 0.05$).

%To understand why participants give higher ratings on WhatELSE, we dive deeper into our post-task interview and participants' comments on the systems during the task. We found that the higher ratings on \textsc{WhatELSE} could be attributed to [to do]. To begin with, most of the participants (9 out of 12) consider the abstraction tools (i.e., the abstraction ladder and the abstraction tooltip) to be helpful.  As P4 noted, the ability to select the level of abstraction "allows users to have more control via clicking buttons." From another perspective, P7 highlighted that the tooltip enhances the creation process by providing "an easier interface to change anything you got."

%In addition to the abstraction tools, half of the participants found the visualization of variants to be beneficial. Participants identified two main uses for this feature. For example, P1 relied heavily on the variants visualization, stating, "I pretty much completely relied on that, right? OK, if anyone (character) here isn't following my vision for the story, then, you know, I can easily know what the problem is." This indicates that the visualization helped him quickly identify unexpected elements in the outline. Meanwhile, P2 mentioned, "the variants are very helpful for inspiration," highlighting another use case where the visualization spurred creative ideas.


%Participants' feedback also highlights that, compared to fully free-form conversational assistants, \textsc{WhatELSE} is more helpful for laypeople in creating their games. As P12 noted, "I can explore what the different versions (abstraction levels) look like," and P6 added, "(WhatELSE) allows you to explore more." These comments point to a deeper reason behind the effectiveness of WhatELSE. Though conversational-based chatbot seems to provide infinite options to choose from in creation, it is overwhelming to laypeople. P1 has been acutely aware of this challenge, and left an opinion: "Maybe someone who is an experienced writer could be like, ok, I want to structure the plot this way. But as me, a person who isn't a writer, it's hard to really know what to do (with the baseline). " In contrast, the mixed-initiative design of \textsc{WhatELSE} does prime users to explore more in sculpting the narrative space in their iterative trials by implicitly guiding them in a professional workflow of outline generation, of which P14 named as "indicators of what I'm trying to do". P2, who has moderate experience in narrative writing, also noticed the design and said, "It's very close to how I think when I write a story, those levels, those metrics."

% Similar to the concept of the illusion of personalization. 

% As direct providing choices to users helps to create a mindset for participants to iteratively sculpt the narrative space by playing around with the representations. 

%\subsubsection{Quality Evaluation in Stage 2 of Interactivity} Again, we leveraged a questionnaire to evaluate participants' evaluation of the game plot that they experienced in Stage 2, with results reported in Fig. \ref{user_study:game_plot}. The figure shows that people generally rate game plots generated by \textsc{WhatELSE} better than the baseline. Specifically, the Wilcoxon test detects that people are significantly more satisfied with the game plots generated by \textsc{WhatELSE} (median rate: 6.5) compared to the baseline (median rate: 4.5, $p = 0.014$). Though participants felt that the game plots generated by both systems are aligned with the specified moral, and provide enough interactivity ($p > 0.05$), they prefer the game plots generated by \textsc{WhatELSE} as they felt the progression of the game plot aligns more with their intention (WhatELSE median rate: 6.0, baseline median rate: 4.0, $p = 0.020$). This also leads to participants considering the game plot to be complete and they will spend less further work in editing the game plot (median rate: 4.5) compared to the baseline (median rate: 3.0, $p = 0.027$). 

%The qualitative feedback from representative participants provides valuable insights into why the game plots generated by \textsc{WhatELSE} are generally preferred. For instance, P10 expressed that in the baseline game, the moral of the story felt "some kind of lost," indicating a misalignment with her expectations. This contrasts with her experience in WhatELSE, where the moral was better preserved within the plot. Additionally, P6 highlighted that her actions in WhatELSE-generated games had a more significant impact. She attempted to "kill" a character in the \textsc{WhatELSE} plot, and the game responded accordingly. In contrast, in the prompt-based game, her actions—such as asking a different person for help—were less influential. This increased sense of agency in \textsc{WhatELSE} enhanced her overall experience. P5's experience further illustrates the effectiveness of WhatELSE's narrative planning. He engaged in behaviors similar to P6, where he killed all the characters who had previously helped him in the forest. Despite this deviation from typical positive or role-play behavior, \textsc{WhatELSE} guided the narrative to an ending where his character remained alone in the forest, reflecting on the moral that "kindness sometimes is truly not rewarded." P5 highly rated \textsc{WhatELSE} for its ability to adapt to its actions while still conveying the intended moral, describing the system as "smarter" for how it "tied my actions and packaged it to be its original directive, which was the moral of the story."



% the game plots generated by \textsc{WhatELSE} advance in that the progression of the game is more 

