\section{Metastable Dynamics and Reasoning}
\label{sec:metastable}

\subsection{CoT as Markov Chains}

Our key insight to understanding inference-time search is to frame CoT reasoning as a metastable Markov process over an underlying linguistic model. Each state represents a logical assertion (e.g., a sentence or mathematical expression rather than a single token), and state transitions correspond to reasoning steps. The model distinguishes between \textbf{easy/trivial reasoning steps}, which form dense local clusters of roughly equivalent meaning, and \textbf{hard reasoning steps}, which form sparse connections between clusters of small probability $O(\ep)$. Reasoning paths sampled from this process typically spend a long time in each cluster before making a nontrivial jump to another cluster. This leads to a dynamical separation between fast and slow timescales, which we quantitatively study by tuning the degree $\ep$ of perturbation.

The setup is formalized as follows. Let $X^\ep=(X_t^\ep)_{t\ge 0}$ be a perturbed family of discrete-time stationary Markov chains on a (large but finite) state space $S$ with transition kernel $p^\ep$, such that $p^\ep$ uniformly converges to $p^0$ as $\ep\to 0$. We assume $X^\ep$ is recurrent for all $\ep\ge 0$ and irreducible for all $\ep>0$; also, $X^0$ is reducible and decomposes $S$ into $K$ disjoint $p^0$-ergodic components $C_1,\cdots,C_K$. We set $M:=\max_k |C_k|$ and assume that $\min_k|C_k| =\Theta(M)$ and $K\le\poly(M)$. Moreover, we denote the stochastic complement of $p^\ep$ corresponding to $C_k$ by the matrix $\bS_{kk}^\ep$; see Appendix~\ref{app:prelim} for definitions. The stationary distributions of $p^\ep,\bS_{kk}^\ep$ are denoted by $\pi^\ep,\pi_k^\ep$ and we set $\mu_k:=\pi_k^0$.

\begin{ass}[dense clusters]\label{ass:cluster}
For each $\bS_{kk}^\ep$, the pseudo-spectral gap $\gamma^\dagger(\bS_{kk}^\ep) \ge\gamma>0$ and the stationary measure $\pi_k^\ep$ satisfies $\pi_k^\ep(x)=\Theta(1/M)$ for all $x\in C_k$.
\end{ass}

We give verifiable conditions on the unperturbed kernel $p^0$ which guarantee Assumption~\ref{ass:cluster} in Proposition~\ref{thm:spectralgap}.

We further denote $E_0=\supp p^0$ and assume that $E=\supp p^\ep$ is fixed for all $\ep>0$. A reasoning path $X_{0:T}$ is termed \emph{valid} if $(X_{t-1},X_t)\in E$ for all $t\in[T]$. The set of sparse edges is denoted by $E_s=E\setminus E_0$. 

\begin{ass}[sparse edges]\label{ass:sparse}
There are at most $d_{\out}$ sparse edges from each of at most $n_{\out}$ sources in $C_k$, and there is at most one sparse edge between any two distinct clusters, with at least one sparse edge from each cluster. Moreover, $p^\ep(y|x)\propto\ep$ for each $(x,y)\in E_s$ with proportionality constant bounded above and below w.r.t. $M,K$, and $p^\ep(z|x)$ for $(x,z)\in E_0$ all decrease proportionally with $\ep$.
\end{ass}

\subsection{Reasoning Task}

The reasoner is given a pair of input and output states $(X_{\inn},X_{\out})$ sampled from a distribution $\DD$ on $S\times S$. The goal of the reasoner is to find a valid path from $X_{\inn}$ to $X_{\out}$. We are thus interested in the hitting time of CoT generation to understand inference-time computation. The overall difficulty of the task is measured by the minimum number of hard reasoning steps needed to reach $X_{\out}$ from $X_{\inn}$;  longer reasoning chains will require more sparse transitions. We assume the average difficulty of the task is lower bounded:

\begin{ass}\label{ass:sep}
For $(X_{\inn},X_{\out})\sim\DD$ and any valid path $X_{0:T}$ with $X_0=X_{\inn}$ and $X_T=X_{\out}$, it holds that
\begin{align*}
&\EE{\DD}{\min\abs{X_{0:T}\cap E_s}} =\Omega(K).
\end{align*}
\end{ass}

Note that $X_{\out}$ is already known in our setting. For example, for theorem proving, $X_{\inn}$ is the problem statement and $X_{\out}$ is the QED symbol; or when asked a ``why" question, $X_{\out}$ could be the conclusion, ``That is why..." Nonetheless, for many reasoning problems the answer is unknown and must be deduced or computed. We incorporate this aspect by introducing a `logical computation' task in Section~\ref{sec:hard}.


\subsection{Metastable Dynamics}

The \textit{hitting time} and \textit{return time} of $X^\ep$ to a set $A\subseteq S$ are defined as $\tau_A^\ep=\inf\{t\ge 0: X_t^\ep\in A\}$, $\bar{\tau}_A^\ep=\inf\{t> 0: X_t^\ep\in A\}$, respectively. Probabilities and expectations conditioned on the initial state $x$ are denoted as $\PP_x,\mathbb{E}_x$, etc.

In the context of perturbed Markov chains, a subset $M\subset S$ is defined as a \emph{metastable system} \citep{Bovier02} if
\begin{equation}\label{eq:bovier}
\lim_{\ep\to 0} \sup_{x\in M,y\notin M} \frac{\PP_x(\bar{\tau}_{M\setminus\{x\}}^\ep < \bar{\tau}_x^\ep)}{\PP_y(\bar{\tau}_M^\ep < \bar{\tau}_y^\ep)} = 0.
\end{equation}
That is, it is much easier to return to $M$ than to transition between different states in $M$. The following result, obtained from our perturbative analysis in Appendices~\ref{app:prelim}-\ref{app:perturb}, will motivate the distillation scheme described in Section~\ref{sec:distill}.

\begin{prop}\label{thm:bovier}
Any subset $S_\circ=\{x_1,\cdots,x_K\}\subset S$ of cluster representatives $x_k\in C_k$ constitutes a metastable system for $X^\ep$ in the sense of \eqref{eq:bovier} as $M\to\infty$.
\end{prop}

\paragraph{Meta-chain.} The coarse-grained dynamics of $X^\ep$ over long timescales is captured by its effective \textbf{metastable representation} $X_\star^\ep$ \citep{Wicks05,Betz16}, which acts as a compression of the full chain by only retaining information on inter-cluster dynamics. This `meta-chain' is defined on the set of clusters $S_\star = \{C_1,\cdots,C_K\}$ with transition kernel
\begin{equation}\label{eq:meta}
q_\star^\ep(C_\ell|C_k) = \sum_{x\in C_k} \mu_k(x)^2 \PP_x(\bar{\tau}_{C_\ell}^\ep < \bar{\tau}_x^\ep), \quad k\neq\ell,
\end{equation}
and $q_\star^\ep(C_k|C_k)$ such that the conditional probabilities sum to $1$. We emphasize that $X_\star^\ep$ faithfully characterizes cluster escape probabilities (see Proposition~\ref{thm:53}) but is \emph{not} a one-to-one copy of the cluster transitions of $X^\ep$, which generally cannot be uniquely defined as a Markov chain. For example, $q_\star^\ep$ is asymptotically reversible and always positive regardless of the actual arrangement of sparse edges (Proposition~\ref{thm:asymprev}). To provide further intuition, we state and discuss the following assumption.

\begin{ass}[uniform escape of $X_\star^\ep$]\label{ass:metamix}
For all $k\neq\ell$,
\begin{equation}\label{eq:metaprob}
q_\star^\ep(C_\ell|C_k)=\Omega(\ep/M).
\end{equation}
\end{ass}

Equation~\eqref{eq:metaprob} holds if there exists a sparse edge from $C_k$ to $C_\ell$ (Corollary~\ref{thm:qlower}), but it may well hold even if $C_k,C_\ell$ are not directly connected. For example, if the sparse edges are arranged as a cycle on $S_\star$, escaping $C_k$ implies that all other clusters $C_\ell$ will be hit before the process returns to $C_k$, and so Assumption~\ref{ass:metamix} is satisfied. Hence Assumption~\ref{ass:metamix} naturally guarantees that it is easy to explore the entire state space from any starting cluster.\footnote{On the other hand, if the meta-chain has poorly connected regions, then $X_\star^\ep$ itself is amenable to metastability analysis, leading to a hierarchy of metastable representations at increasingly faster timescales \citep{Wicks05}.}


\begin{algorithm}[t]
\caption{Two-stage Pretraining}
\label{alg:pre}
\begin{algorithmic}[1]
\STATE set $\bW^{(0)} = \boldsymbol{0}$, $\eta=O(KM)$,
\STATE $T_1=\widetilde{O}(KM^2\ep^{-2})$, $T_2=\widetilde{O}(KM\ep^{-2})$

\FOR{$t=1,\cdots, T_1$}
\STATE $\bW^{(t)} = \bW^{(t-1)} + \eta\nabla \EE{X_0,X_1}{\log\hat{p}_{\bW^{(t-1)}}(X_1|X_0)}$
\ENDFOR

\STATE $w_{ij}^{(T_1)} \gets -\infty$ if $\hat{p}_{ij}^{(T_1)} < c_{\thres}\ep$ \COMMENT{thresholding}

\FOR{$t-T_1=1,\cdots, T_2$}
\STATE $\bW^{(t)} = \bW^{(t-1)} + \eta\nabla \EE{X_0,X_1}{\log\hat{p}_{\bW^{(t-1)}}(X_1|X_0)}$
\ENDFOR
\end{algorithmic}
\end{algorithm}


\section{Search Improves the Pretrained Model}\label{sec:search}


\subsection{Pretraining the Base (World) Model}

We equate pretraining the base model with learning the underlying transition kernel $p^\ep$. Indeed, if the context window of an LLM is restricted to the tokens in the previous state, next-token prediction recursively defines a distribution over the following state, and further over reasoning chains of arbitrary length. We encode each state $x\in S$ as a one-hot vector in $\RR^{|S|}$ also denoted by $x$ and write $p_{ij}^\ep = p^\ep(e_j|e_i)$. For the model, we consider a simple linear softmax predictor:
\begin{equation*}
\hat{p}_\bW(\cdot|x) = \sm(\langle\bW,x\rangle), \quad \bW\in\RR^{|S|\times|S|}.
\end{equation*}
The pretraining data consists of random bigram samples $(X_0,X_1)$ where $X_1\sim p^\ep(\cdot|X_0)$; we allow $X_0$ to be either uniform over $S$ or distributed according to the stationary measure $\pi^\ep$ of $p^\ep$. The latter arises when generating samples $(X_{t-1},X_t)_{t\ge 1}$ from the observed transitions of the (unbounded) chain $(X_t^\ep)_{t\ge 0}$. The model is trained by gradient descent with cross-entropy loss, with an intermediate thresholding step to mask out edges determined to not lie in $E$. See Algorithm~\ref{alg:pre} for details and Theorem~\ref{thm:prefull} for the full statement.

\begin{thm}[convergence of pretraining]\label{thm:pre}
Let $X_0\sim\Unif(S)$ or $X_0\sim\pi^\ep$ and $X_1\sim p^\ep(\cdot|X_0)$ be random samples from $X^\ep$. Then for the gradient descent iterates $\bW^{(t)}$ from Algorithm \ref{alg:pre} w.r.t. cross-entropy loss
\begin{equation*}
L_{\pre}(\bW) = \EE{X_0,X_1}{-\log\hat{p}_{\bW}(X_1|X_0)},
\end{equation*}
the learned transition probabilities $\hat{p}_{ij}^{(T)} = \hat{p}_{\bW^{(T)}}(e_j|e_i)$ converge with error $\sup_{i,j}|\hat{p}_{ij}^{(T)} - p_{ij}^\ep|=O(\sqrt{KM^2/T})$ before thresholding. Moreover, after thresholding at time $T_1=\widetilde{O}(KM^2\ep^{-2})$, the error converges as $\exp(-\Omega(\ep^2T))$. Hence after $T_2=\widetilde{O}(KM\ep^{-2})$ additional steps, the output of Algorithm \ref{alg:pre} has error $\exp(-\Omega(|S|))$.
\end{thm}

Thus the base model $\hat{p}$ learns the underlying graph $E$ and all transition probabilities with exponentially small error. Under mild regularity conditions, all assumptions can be verified for $\hat{p}$ (see Propositions~\ref{thm:spectralgap} and \ref{thm:asymprev}); to simplify the discussion, we henceforth assume the base model is exact, $\hat{p}=p^\ep$. We remark that while the time to converge is quite long compared to the search, RL and distillation methods studied later, this is natural as pretraining is done on much longer timescales compared to test-time compute.
%The effective model size can be made $O(KM^2)$ via sparse encoding after the thresholding step, and even during the entirety of training by only recording valid edges.


\subsection{Learning Sparse Rewards via Search}

Having learned the underlying probabilities $p^\ep$, the base model now performs CoT reasoning by generating each step of the chain $(X_t^\ep)_{t\ge 0}$ in sequence starting from $X_0^\ep=X_{\inn}$. Since the reasoner has no prior knowledge of which steps it must take to progress towards $X_{\out}$, on average it will spend a long time trapped in each cluster before chancing upon a sparse edge (new idea) and moving to a new cluster. From our quantitative dynamical analysis, we are able to obtain a nearly tight characterization of the average hitting time.

\begin{thm}[expected hitting time]\label{thm:hitting}
Under Assumptions \ref{ass:cluster}-\ref{ass:metamix}, it holds for all $\ep \le\ep_{\max}:= \Theta(M^{-1}(\log M)^{-4})$ that
\begin{equation*}
\EE{(X_{\inn},X_{\out})\sim\DD}{\EE{X_{\inn}}{\tau_{X_{\out}}^\ep}} = \widetilde{\Theta}\left(\frac{KM}{\ep}\right).
\end{equation*}
\end{thm}

Intuitively, since each cluster is rapidly mixing, the chain will spend roughly $\Theta(1/M)$ of the time in states with outbound edges, from where it escapes with probability $\Theta(\ep)$. Such rare events are distributed approximately exponentially, and must be repeated $\Theta(K)$ times to reach the cluster containing $X_{\out}$, where the chain will mix fast and likely hit $X_{\out}$.

This result also illustrates a simple method to improve the hitting time: modifying the underlying probabilities to increase the denominator $\ep$. This corresponds to guiding CoT or fine-tuning the base model so that (correct) new, difficult reasoning steps are generated more often, ensuring a more efficient exploration of the solution space. However, this cannot be done by simply increasing the likelihood of low-probability edges, as there may be many low-probability edges within clusters as well; we want to only boost the generation of sparse edges to preserve the capabilities of the pretrained model. Indeed, we demonstrate in Theorem~\ref{thm:sqexp} that any updates based on local information is not enough to improve reasoning ability in a precise sense.

Instead, we run a simple \textit{tree search} protocol to identify sparse edges, detailed in Algorithm~\ref{alg:search}. The method consists of randomly sampling a state $X_0$ and rolling out $N$ random walks in parallel to construct an estimate $\hat{C}$ of the cluster containing $X_0$ for time $T_0$. After the cluster has been sufficiently explored we continue to simulate each walk until a transition outside $\hat{C}$ is detected, at which point the edge is marked as a sparse edge (added to $\hat{E}$) and the path is terminated. This continues until until all paths are terminated or a time horizon $T_{\max}$ is reached. Since we are not receiving signals from an external oracle but rather recording rare transitions, this is similar to intrinsic rewards such as curiosity or exploration bonuses \citep{Burda18,Burda19}.

We consider two versions of this process, \textbf{PRM mode} and \textbf{RL mode}, depending on whether the information gained from search is collected into an external reward model or used to fine-tune the base model. The benefits of both methods for reasoning is discussed in the next subsection.


\begin{algorithm}[t]
\caption{Sparse Edge Search}
\label{alg:search}
\begin{algorithmic}[1]
\REQUIRE pretrained model $\hat{p}_{\bW}$
\STATE set $R=\Theta(K\log K)$, $N=\Theta(\log K)$,\\ $T_0=\Theta(M(\log M)^2)$, $T_{\max}=\Theta(M/\ep)$, $\MM_s = \varnothing$
\FOR{$r=1,\cdots, R$}
\STATE set $\hat{C},\hat{C}^n,\hat{E}=\varnothing, A=[N]$
\STATE sample $X_0\sim \Unif(S)$ or $X_0\sim\pi^\ep$
\FOR{$t=1,\cdots,T_{\max}$}
\FOR{$n\in A$}
\STATE generate $X_t^{n,\ep} \sim \hat{p}_{\bW}(\cdot |X_{t-1}^{n,\ep})$
\IF[cluster search]{$t\le T_0$}
\STATE $\hat{C}^n\gets \hat{C}^n\cup\{X_t^{n,\ep}\}$
\ELSIF[edge search]{$t>T_0$, $X_t^{n,\ep}\notin \hat{C}^n$}
\STATE $\hat{E}\gets\hat{E}\cup\{(X_{t-1}^{n,\ep},X_t^{n,\ep})\}$
\STATE $A\gets A\setminus\{n\}$
\ENDIF
\ENDFOR
\IF{$t=T_0$}
\STATE $\hat{C}= \cap_{n=1}^N \hat{C}^n$
\ENDIF
\ENDFOR
\STATE run Algorithm \ref{alg:ppo} with $\hat{p}_{\bW},\hat{E}$ \COMMENT{if RL mode} 
\STATE $\MM_s\gets\MM_s\cup\hat{E}$ \COMMENT{if PRM mode}
\ENDFOR
\STATE return $\MM_s$

\end{algorithmic}
\end{algorithm}


\begin{algorithm}[t]
\caption{PPO-Clip}
\label{alg:ppo}
\begin{algorithmic}[1]
\REQUIRE pretrained model $\hat{p}_{\bW}$, subset of edges $\hat{E}$
\STATE set $\bW^{(0)}=\bW$, $T_{\PPO} = \Theta(\log\ep_{\max}/\ep)$, $\alpha=\Theta(KM)$
\STATE advantage function $\hat{A}(x,y) = 1_{\{(x,y)\in\hat{E}\}}$
\FOR{$t=1,\cdots,T_{\PPO}$}
\STATE $\bW^{(t)} = \bW^{(t-1)} + \alpha\sgn(\nabla L_{\PPO}(\bW^{(t-1)};\hat{A}))$
\ENDFOR
\end{algorithmic}
\end{algorithm}


\subsection{Improving the Base Model via RL}


\textbf{PRM mode} keeps an external process reward `model' (PRM) throughout the search process, which is simply the set $\MM_s$ which collects the estimated sparse edges over multiple iterations of the outer loop to reconstruct $E_s$. We prove that the PRM is strongly consistent:

\begin{prop}\label{thm:consistent}
PRM mode of Algorithm \ref{alg:search} returns $\MM_s = E_s$ with probability $1-\widetilde{O}(1/K)$.
\end{prop}

Then by increasing the likelihood of transitions $(x,y)\in\MM_s$ when the current state is $x$ by a factor of $\ep_{\max}/\ep$, \text{the PRM can guide CoT to follow $p^{\ep_{\max}}$ rather than $p^\ep$.} It is immediate from Theorem \ref{thm:hitting} that the expected hitting time decreases from $\widetilde{\Theta}(KM/\ep)$ to $\widetilde{\Theta}(KM/\ep_{\max})$. Moreover, the time complexity of Algorithm~\ref{alg:search} is $RT_{\max}=\widetilde{O}(KM/\ep)$, which is equal to the \text{time to solve a \emph{single} instance $(X_{\inn},X_{\out})$ \emph{without} search}, and the memory requirement is only $O(M+K)$. This demonstrates the effectiveness of utilizing search to guide CoT generation.


However, it is often desirable to use the information gained during search to directly fine-tune the pretrained model, so that maintaining an independent PRM is not necessary. \textbf{RL mode} performs online RL updates to $\hat{p}_{\bW}$ at each iteration of Algorithm \ref{alg:search}; while many policy gradient methods can be applied, we analyze the popular proximal policy optimization algorithm \citep[PPO-Clip,][]{Schulman17}. Based on the estimate $\hat{E}$ of sparse edges originating from the initialized cluster, we define the advantage function as $\hat{A}(x,y)=1$ if $(x,y)\in\hat{E}$ and $0$ otherwise. Similarly to pretraining, samples are generated as $X_0\sim\Unif(S)$ or $\pi^\ep$ and $X_1\sim p^\ep(\cdot|X_0)$. The objective of PPO-Clip to be maximized is
\citep{spinningup_ppo}
\begin{align*}
&L_{\PPO}(\bW;\hat{A})\\
&= \EEbig{X_0,X_1}{\min\left\{\frac{\hat{p}_{\bW}(X_1|X_0)}{p^\ep(X_1|X_0)}, c_{\clip}\right\} \hat{A}(X_0,X_1)}.
\end{align*}
The old policy is fixed to $p^\ep$ during Algorithm \ref{alg:search}. We use sign gradient descent for simplicity of analysis (ordinary gradient descent also guarantees convergence as long as $\ep\ge\ep_{\max}^2$).

\begin{prop}[convergence of PPO-Clip]
By running RL mode of Algorithm \ref{alg:search} with PPO-Clip, the base model $p^\ep$ is modified to $p^{\ep'}$ where $\ep'=(1-o(1))\ep_{\max}$ with probability $1-\widetilde{O}(1/K)$.
\end{prop}

The additional time complexity of running PPO-Clip is $\widetilde{O}(K\log(\ep_{\max}/\ep))$ which is small compared to the pretraining time or search process. In particular, it again follows from Theorem \ref{thm:hitting} that the expected hitting time is improved by the factor $\ep_{\max}/\ep$. At the same time, the \emph{magnitude} (total variation) of change to the pretrained model is negligible:
\begin{equation*}
\textstyle \sup_{x\in S}\norm{p^\ep(\cdot|x) - p^{\ep_{\max}}(\cdot|x)}_{\TV} \le o(1/M),
\end{equation*}
so the original capabilities of the base model are generally preserved. Hence {RL is also extremely efficient for fine-tuning the pretrained model to improve CoT.} 


\section{Distillation to a Smaller Model}\label{sec:distill}


A prominent innovation in the LLM development pipeline is to distill CoT of a powerful model into a smaller, more efficient model. This approach has been shown to significantly enhance reasoning ability, especially compared to directly training the smaller model with RL \citep{shridhar2022distilling,hsieh2023distilling,gandhi2024stream,guo2025deepseek}. In this section, we showcase an explicit distillation scheme for our CoT model that efficiently generates the hard reasoning steps to solve any task while faithfully capturing the metastable dynamics of the original system.


\subsection{Distilling Cluster Transitions}


The metastable chain $q_\star^\ep$ (Section \ref{sec:metastable}) provides a natural notion of compression for the nearly reducible system $X^\ep$ by collapsing each cluster into a single state. For many downstream tasks (including the logic task studied in Section \ref{sec:hard}) it may be satisfactory to retrieve only the hard reasoning steps connecting the clusters containing $X_{\inn},X_{\out}$. In particular, if the goal is to extract only the \emph{connectivity} of $S_\star$, it suffices to take the sparse edge estimate $\MM_s$ of Algorithm \ref{alg:search} and perform a uniform random walk to find a path between any two clusters. However, we want the distilled model to also preserve the underlying dynamics of the original chain as best as possible. To this end, we implement the following process, detailed in Algorithm \ref{alg:distill} in the appendix.

We first choose a set $S_\circ=\{x_1,\cdots,x_K\}$ of representatives $x_k$ of $C_k$ and assign each state to its representative via the map $\iota: S\to S_\circ$; this can be done by exploring each cluster similarly to the first $T_0$ steps of the search process.

\paragraph{Data collection.} The data for distillation is collected by continually running CoT and recording the frequency of transitions (or non-transitions) between $S_\circ$. The yields one datum per CoT step, and can also be implemented in parallel for an arbitrary number of independent chains.
\begin{enumerate}
\item If $X_t^\ep \in S_\star$ and the previous return to $S_\circ$ was $X_{t_{\prev}}^\ep$ then add $(X_{t_{\prev}}^\ep,X_t^\ep)$ to $D_{\dist}$.
\item If $X_t^\ep \notin S_\circ$ (no transition) add $(\iota(X_t^\ep), \iota(X_t^\ep))$ to $D_{\dist}$.
\end{enumerate}
This requires only $O(K^2)$ memory for frequency counts; no cache for $X^\ep$ is needed. The cluster labels $\iota$ and parameters $\bZ$ require $O(KM)$ and $O(K^2)$ memory, respectively. We suppose the process is run for arbitrarily long time so that we have access to the population distribution of $D_{\dist}$. We then one-hot embed $S_\circ$ in $\RR^K$ and use the collected data pairs to train a softmax model $\hat{q}_\bZ(\cdot|x) = \sm(\langle\bZ,x\rangle)$, $\bZ\in\RR^{K\times K}$ similarly to pretraining. Finally, we rescale time so that the non-diagonal entries sum to $\Theta(1)$, reducing redundant within-cluster transitions.

\paragraph{Equivalence with meta-chain.} The data $(Y_0,Y_1)\sim D_{\dist}$ has been constructed so that the distilled model learns the following kernel $q_\circ^\ep$ on $S_\circ$: $Y_0\sim\pi^\ep$, $Y_1\sim q_\circ^\ep(\cdot|Y_0)$ where
\begin{align*}
&q_\circ^\ep(x_\ell|x_k) := \pi_k^\ep(x_k)\PP_{x_k}(X_{\bar{\tau}_{S_\circ}^\ep}^\ep = x_\ell), \quad k\neq\ell,\\
&q_\circ^\ep(x_k|x_k) := 1-\textstyle\sum_{\ell\ne k} q_\circ^\ep(x_\ell|x_k).
\end{align*}
This kernel is a lazy version of the process obtained from $X^\ep$ by deleting all transitions to states outside $S_\circ$, with an additional time rescaling according to the stationary probability $\pi_k^\ep(x_k)$. This is slightly different from the construction given in \citet{Betz16}, as we do not presume access to the stationary distribution of the unperturbed chain $p_k^0$ and must sample directly from $\pi^\ep$. Moreover, $q_\circ^\ep$ is dependent on the choice of representatives and thus different from the `canonical' meta-chain $q_\star^\ep$ in general. Nonetheless, $q_\circ^\ep$ is faithful to the meta-chain in a rigorous sense:

\begin{prop}\label{thm:53}
Denote the return time of $q_\circ^\ep$ to $x_k$ as $\bar{\tau}_{\circ,x_k}^\ep$. For all $k,\ell\in[K]$ with $k\neq\ell$, it holds that
\begin{equation*}
\frac{\PP_{x_k}(\bar{\tau}_{\circ,x_\ell}^\ep < \bar{\tau}_{\circ,x_k}^\ep)}{q_\star^\ep(C_\ell|C_k)} = 1+o_M(1).
\end{equation*}
\end{prop}
That is, the escape probabilities of $q_\circ^\ep$ converge to $q_\star^\ep$ with uniformly vanishing relative error. This property is desirable as it shows the eventual likelihood of escaping to each cluster (i.e., reaching a certain idea) is consistent across different choices of $S_\circ$.

\subsection{CoT of Distilled Model}

To analyze the utility of the trained model $\hat{p}_{\bZ^+}$, we make the additional assumption:

\begin{ass}[inbound sparse edges]\label{ass:in}
All sparse edges leading to each cluster $C_k$ terminate at a fixed point $x_k$. For any sparse edge $(x',x_\ell)$ from $C_k$, there exists a path from $x_k$ to $x'$ in $C_k$ of probability bounded below.
\end{ass}

Then we may specify $S_\circ$ as the set of the points $x_k$. This ensures that representatives will not be skipped; otherwise, a CoT passing through $C_k,C_\ell,C_n$ in succession may miss $x_\ell$ and record the wrong transition $(x_k, x_n)$ (although the likelihood of this is $o(1)$ regardless).

Now, as with pretraining, the distilled model will converge to $q_\circ^\ep$ when trained with cross-entropy loss on $D_{\dist}$.

\begin{prop}[convergence of distillation]\label{thm:unchained}
For the gradient descent iterates $\bZ^{(t)}$ from Algorithm \ref{alg:distill}, the learned probabilities converge to $q_\circ^\ep$ after $T_{\dist} = \widetilde{O}(M^2\ep^{-2})$ as
\begin{equation*}
\textstyle \sup_{k,\ell}|\hat{q}_{\bZ^{(T_{\dist})}}(x_\ell|x_k) - q_\circ^\ep(x_\ell|x_k)|= K^{-\omega(1)}.
\end{equation*}
\end{prop}
We point out the time to convergence is much faster than pretraining time $\widetilde{O}(KM^2\ep^{-2})$ (Theorem \ref{thm:pre}), and also more computationally efficient since we are training a size $K^2$ model rather than size $(KM)^2$.

Finally, after time rescaling, the model $q_{\bZ^+}$ is capable of efficiently finding a path from the cluster containing $X_{\inn}$ to the cluster containing $X_{\out}$, with hitting time linear in $|S_\circ|$ and \textit{independent of the difficulty parameter} $\ep$.

\begin{thm}[hitting time of distilled CoT]\label{thm:soda}
For all $k\ne\ell$, $\hat{q}_{\bZ^+}(x_\ell|x_k) = \Theta(1)$ if there exists a sparse edge from $C_k$ to $C_\ell$ or $0$ if not. Moreover, the hitting time $\tau_{x_\ell}^+$ of $x_\ell\in S_\circ$ by $\hat{q}_{\bZ^+}$ satisfies $\EE{x_k}{\tau_{x_\ell}^+} = O(K)$.
\end{thm}

The returned sequence of clusters $C_{0:T}$ indicate the existence of a path from $X_{\inn}$ to $X_{\out}$ passing through precisely these clusters in order. Once $C_{0:T}$ is determined, a weaker reasoning agent (e.g., the base model $p^\ep$) may also efficiently resolve the fine-grained dynamics within each cluster.


\section{Logical Reasoning is Hard without Search}\label{sec:hard}


\subsection{Logical Reasoning Task}

In this section, we further investigate the benefits of search for reasoning by adding a quantitative `logic task' on top of the path-finding task. This provides two benefits. First, having a numerical answer allows us to evaluate the hardness of the task from a learning-theoretic perspective, separate from the previously obtained hitting time bounds. Second, by having the answer depend only on the sparse edges along a path, the reasoner is required to estimate which edges are sparse -- in other words, understand which reasoning steps are actually important -- in order to solve the task. Taking a proof problem for example, we expect an LLM with strong reasoning capability to not only \emph{generate} a plausible solution via next-token prediction but also \emph{understand} its own proof, so that it can correctly answer logical questions such as ``what are the key ideas of this proof?" or ``what happens if we replace step X with Y?" We attempt to formalize this notion using group actions (Definition~\ref{def:group}).

\paragraph{Logical actions.} Let $(G,\circ)$ be a finite group with identity $e_G$. The \emph{logical value} (or simply \emph{logic}) of a reasoning chain is an element of an abstract space $\calR$ equipped with a $G$-action $r\mapsto g\cdot r$. Each edge $e\in E$ is assigned a \emph{logical action} $\alpha(e)\in G$ which acts on the current logic when the edge is selected. To focus on learning hard steps, we assume that the logical action of edges not in $E_s$ are trivial, $\alpha|_{E_s^c}:=e_G$. Let $\psi:S\to\calR$ be an arbitrary embedding map. For a valid path $X_{0:T}\subseteq S$, we define the corresponding logic sequence $r_{0:T}\subseteq \calR$ as
\begin{equation*}
r_0 = \psi(X_0), \quad r_t = \alpha(X_{t-1},X_t) \cdot r_{t-1}.
\end{equation*}

For example, if each state is a Boolean expression being manipulated according to certain rules, $\calR=G=\ZZ_2$ could be used to encode the evaluation of the current expression by switching between $1$ (\texttt{True}) and $0$ (\texttt{False}) depending on the effect of each manipulation. $G$ could also be taken to be a space of functions with the evaluation action $g\cdot r=g(r)$, so that the logic computes a repeated composition of functions. When the chain terminates, the final logic $r_T=:r(X_{0:T})$ is returned. Note that logical values are not unique to states and $r_T$ depends on the entire path $X_{0:T}$.

\paragraph{Logic Task.} Given $(X_{\inn},X_{\out})\sim\DD$, the goal is to output both a valid path $X_{0:T}$ from $X_{\inn}$ to $X_{\out}$ and its logical value $r(X_{0:T})$. Since any path can be made simple by deleting loops, here we require valid paths to be simple.

To establish a rigorous distinction between the use of a search algorithm and lack thereof, we consider models consisting of a pretrained base model or \emph{linguistic} component $\MM_p$, responsible for learning $p$ and generating a valid CoT, and a \emph{reasoning} component $f_\theta$, which predicts the answer $r(X_{0:T})$ based on (limited) information from $\MM_p$.
As in Section \ref{sec:search}, we suppose $\MM_p$ has perfectly learned the kernel $p$ and can output arbitrary valid paths $\MM_p(X_{\inn},X_{\out})$, solving the first part of the task. Here we do not consider the time complexity of running $\MM_p$, which (as we have seen in Theorem \ref{thm:hitting}) can be quite long without a search-and-improvement protocol. Thus the main task of the reasoner is to execute logical computations along a generated CoT.


In this section, we assume a stronger \emph{uniform} lower bound in Assumption \ref{ass:sep} on the minimum number of hard steps; otherwise, querying a single sparse edge $(X_{\inn},X_{\out})\in E_s(p)$ could immediately reveal its (nontrivial) action.
\begin{manualtheorem}{3'}\label{ass:new}
For any $(X_{\inn},X_{\out})\sim\DD$ and any valid path $X_{0:T}$ with $X_0=X_{\inn}$ and $X_T=X_{\out}$, it holds that $\min\abs{X_{0:T}\cap E_s} =\Omega(K)$.
\end{manualtheorem}
We remark that this condition can be weakened to $\Omega(\log K)$ if $M\ge\Omega(K)$, in which case our results hold with $e^{-\Omega(K)}$ replaced by $K^{-\omega(1)}$.

\paragraph{Concept class.} Define $\calP$ the set of transition kernels on $S$ satisfying Assumptions \ref{ass:cluster}, \ref{ass:sparse} and denote the sparse edge set of $p\in\calP$ as $E_s(p)$. The logical action $\alpha$ can be seen as generated by sampling $\calA:S\times S\to G$ i.i.d. uniformly from $G$, then masking out all edges not in $E^s(p)$ by setting them to $e_G$. Thus $\calA$ can be regarded as a variable separate from the target $p$, and the logic is computed recursively as
\begin{align*}
&r_{\calA,p}(X_0) = \psi(X_0), \quad r_{\calA,p}(X_{0:t}) = \\
&\begin{cases}
\calA(X_{t-1},X_t)\cdot r_{\calA,p}(X_{0:(t-1)}) & (X_{t-1},X_t) \in E_s(p) \\
r_{\calA,p}(X_{0:(t-1)}) & (X_{t-1},X_t) \notin E_s(p).
\end{cases}
\end{align*}
Finally, the logic $r_{\calA,p}(X_{0:T})$ is mapped to a scalar output via a classifier $\phi:\calR\to\{+1,-1\}$. We assume that $\EE{g\in G}{\phi(g\cdot r)}=0$ for all $r\in\calR$. The concept class is thus
\begin{align*}
\HH = \big\{ &h_p\in S\times S\times G^{|S|\times |S|}: p\in\calP,\\
& h_p(X_{\inn},X_{\out},\calA) = \phi\circ r_{\calA,p}(\MM_p(X_{\inn},X_{\out})) \big\},
\end{align*}
equipped with inner product $\langle h_p,h_{p'}\rangle_{\HH}:= \EE{(X_{\inn},X_{\out})\sim\DD,\calA}{h_p(X_{\inn},X_{\out},\calA) h_{p'}(X_{\inn},X_{\out},\calA)}$.


\subsection{A Measure of Hardness with Restricted Access}

In previous sections, we have seen that pretraining $\MM_p = \hat{p}_{\bW}$ and running a search or distillation algorithm $f_\theta$ will correctly infer the underlying sparse structure. In this case, computing $r_{\calA,p}(\MM_p(X_{\inn},X_{\out}))$ is trivial by concatenating actions along the identified sparse edges. In contrast, we now restrict the reasoning component's access to $p$ by only allowing certain queries to $\MM_p$. This makes it difficult to infer the sparse structure and true logical actions.

To understand learning with this additional (restricted) information, we propose the following generalization of the statistical query dimension \citep{Kearns98,Feldman17}.

\begin{defn}[SDA: SQDIM with access]
Let $\calP$ be the set of ground truths and $\HH=\{h_p:\XX\to\{\pm 1\}\mid p\in\calP\}$ the associated concept class with inner product $\langle\cdot,\cdot\rangle_\HH$. Let $\calI_p$ be any value or any function on $\XX$ depending on $p$. Then the \emph{statistical query dimension of $\calP$ with access to $\calI$} and tolerance $\tau$ is defined as
\begin{align*}
&\SD_\tau(\calP;\calI) := \sup\{|\calP'|: \calP'\subseteq\calP,\\
&|\langle h_{p_1},h_{p_2}\rangle_\HH|\le\tau, \calI_{p_1} = \calI_{p_2}\,\forall p_1\ne p_2\in \calP'\}.
\end{align*}
\end{defn}
In this section, we consider $\tau=0$ and omit its notation. Extending classical analyses \citep[e.g.,][]{Shai17,Shamir18}, we prove a general limitation for gradient-based learning when additional information $\calI_p$ is provided.

\begin{thm}[SQ learning with additional information]\label{thm:sda}
Let $f_\theta$ be any parametric model of the form
\begin{equation*}
x\mapsto f_\theta(x,\calI_p(x)).
\end{equation*}
Let the loss function be $L(\theta;p) := \norm{h_p-f_\theta}_\HH^2$ and set $\delta:= (4\norm{\nabla f_\theta}_\HH^2/\SD(\calP,\calI))^{1/3}$. Then choosing $p$ randomly from a subset of $\calP$, any iterative algorithm $A(\theta)$ that makes at most $n$ queries to the $\delta$-corrupted gradient oracle $\nabla L$ has expected loss
\begin{equation*}
\EE{p}{L(A(\theta);p)} \ge 1-\SD(\calP,\calI)^{-1}
\end{equation*}
with probability at least $1-n\delta$.
\end{thm}
We only consider the squared loss in our formulation for simplicity. While squared loss only answers correlational queries, CSQ-learnability is equivalent to SQ-learnability for Boolean concepts \citep{Bshouty01}.

\subsection{Results on Hardness of Logical Task}

We consider four types of access to the pretrained model. Note that a \emph{local neighborhood} of a subset $S'\subset S$ in the weighted directed graph defined by $p$ is defined as the subgraph consisting of states reachable with a bounded number of steps from any state in $S'$.

\begin{enumerate}
\item\label{item1} \textbf{No pretraining}, $\calI_p\equiv\varnothing$: the learner $f_\theta(X_{\inn},X_{\out},\calA)$ has not been pretrained and does not receive any information on $p$.

\item\label{item2} \textbf{Path-only (no search)}, $\calI\equiv\MM$: the learner is allowed to depend on inputs $X_{\inn},X_{\out}$, and $\calA$, and also the generated path $\MM_p(X_{\inn},X_{\out})$. That is, the linguistic component (base model) will return a valid CoT for the input at hand, but we cannot simulate different chains from $p$ to execute some search policy or inference algorithm.

\item\label{item3} \textbf{Local search}, $\calI\equiv \nbd(\MM)$: the learner is allowed full access to a local neighborhood of $\MM_p(X_{\inn},X_{\out})$ in the graph of $p$, including connectivity information and transition probabilities. For instance, it can flag low-probability edges as more likely to be sparse, or run bounded-length CoT from $X_{\inn}$ or $X_{\out}$. 

\item\label{item4} \textbf{Full search}, $\calI\equiv\calP$: the learner is given full access to the entire graph of $p$ at all times. In this case, Algorithm \ref{alg:search} or \ref{alg:distill} can be used to infer $E_s(p)$ and generate CoT efficiently, and also perform the desired computation $h_p$.
\end{enumerate}

Our main negative result states that \ref{item1}-\ref{item3} \emph{cannot} solve the logic task with polynomial compute, and thus global search is necessary:

\begin{thm}\label{thm:sqexp}
$\SD(\calP;\calP)=1$ and
\begin{align*}
&\SD(\calP;\varnothing) \ge \SD(\calP;\MM) \ge \SD(\calP;\nbd(\MM)) \ge e^{\Omega(K)}.
\end{align*}
\end{thm}
\begin{rmk}
The necessity of global information for certain learning problems (\emph{globality barrier}) has been conjectured in \citet{abbe2024far}, where the hardness of a `cycle task' is proved. These results are also closely related to classical SQ-hard problems such as subset parity. The precise relationship between SDA, globality and learning is still open.
\end{rmk}

\begin{rmk}
While it suffices to lower bound the strictest term $\SD(\calP;\nbd(\MM))$, we exhibit different constructions for each of the three dimensions as they offer increasing levels of generality. In particular, $\SD(\calP;\varnothing)$ can be realized by $\calP'\subset\calP$ containing any prescribed $p\in\calP$ and for any $\DD$. Moreover, the difficulty is solely due to the logical part of the task; without pretraining, the reasoner will take exponentially many guesses to even produce a valid path.
\end{rmk}

\begin{cor}[hardness without global search]\label{thm:hard}
Suppose $f_\theta(\nbd(\MM_p(X_{\inn},X_{\out})),\calA)$ is any parametric model with polynomially bounded gradients, that can freely search a local neighborhood of the generated CoT. Then any iterative algorithm $A(\theta)$ that makes at most polynomial queries to the $e^{-\Omega(K)}$-corrupted gradient oracle $\nabla L$ satisfies
\begin{equation*}
\EE{p}{L(A(\theta);p)} \ge 1-e^{-\Omega(K)},
\end{equation*}
with probability $1-e^{-\Omega(K)}$ for $M$ sufficiently large.
\end{cor}

Hence $\HH$ cannot be even weakly learned in polynomial time if search is not long enough. The key intuition is that if the graph is locally isomorphic, local search cannot distinguish between sparse inter-cluster edges and low-probability but within-cluster edges as it cannot explore the whole cluster. This demonstrates the importance of spending sufficient inference-time compute for improving reasoning ability.



%Proof sketch: The hardness of learning a target in $\HH$ when the underlying kernel $p$ is unknown is intuitively similar to the hardness of learning sparse parities. However, while two parity functions are trivially uncorrelated when even a single bit differs, for two functions in $\HH$ to be uncorrelated the set of nontrivial actions along any valid path must be different for all possible $X_{\inn},X_{\out}$. Hence we must assume that very short paths are not queryable; otherwise, querying a single sparse edge $(X_{\inn},X_{\out})\in E_s(p)$ would immediately reveal its (nontrivial) action. At the same time, the sparse structure $E_s(p)$ must be as close to disjoint as possible while respecting the clusters $C_1,\cdots,C_K$. We prove the existence of a large such subset of $\calP$ by equating certain permutations with codewords of block length $K$ and employing the Gilbert-Varshamov bound in the high relative distance limiting regime.