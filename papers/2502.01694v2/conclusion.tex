\section{Conclusion}

We introduced a metastable Markov framework for modeling CoT reasoning, revealing the benefits of inference-time search, RL, and distillation. We showed that search can improve reasoning by identifying critical sparse transitions (hard steps), which can then be leveraged to fine-tune the pretrained model via RL or distilled into a more efficient representation, improving hitting times for path generation. We further established learning-theoretic limits on reasoning with restricted information and showed that logical reasoning tasks become intractable without global search.

\paragraph{Future directions.} We have studied a simple curiosity-based unsupervised reward model; it would be interesting to see how a more complex search process could be guided with outcome rewards. Our framework could also be used to study other inference-time methods such as CoT revision (e.g., backtracking to better locate sparse edges), as well as iterative finetuning of the pretrained model, and explore scaling laws for inference time compute.