% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}   

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)

\usepackage{subfigure}
\usepackage{array}
\usepackage{geometry}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{url}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{colortbl}
\usepackage{xcolor}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Unveiling Attractor Cycles in Large Language Models: A Dynamical Systems View of Successive Paraphrasing}
% Glimpsing Generation Momentum through the Lens of Recursive Paraphrasing
% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}


\author{ 
 Zhilin Wang$^{\spadesuit}\footnotemark[1]$\hspace{0.5mm},
 Yafu Li$^{\spadesuit}\footnotemark[1]$\hspace{0.5mm},
  Jianhao Yan$^{\diamondsuit \clubsuit}$\hspace{0.5mm} \\
\bf{Yu Cheng$^{\heartsuit}$\hspace{0.5mm}\hspace{0.5mm}, 
 Yue Zhang$^{\clubsuit }$}\hspace{0.2mm}\hspace{1.5mm} \\
$^\spadesuit$ Shanghai AI Laboratory\ \ \ \quad$^\clubsuit$Westlake University \\\quad$^\diamondsuit$  Zhejiang University \ \ \quad$^\heartsuit$Chinese University of Hong Kong \\
 % \texttt{yafuly@gmail.com} \quad \texttt{linzwcs@gmail.com} \quad 
 \texttt{\{linzwcs,yafuly,elliottyan37\}@gmail.com}  \\
 \quad\texttt{\{chengyu\}@cse.cuhk.edu.hk} 
 \quad\texttt{\{zhangyue\}@westlake.edu.cn}\\
}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{\ Equal contribution. Work was done during Zhilin Wang's internship at Shanghai AI Laboratory and Westlakenlp Lab.}
%\footnotetext[2]{\ Corresponding authors.}

 
% We demonstrate that periodicity can be mitigated through contextual interventions and perturbation strategies, but fully escaping these attractor states remains challenging. 

% Paraphrasing tools based on large language models (LLMs) have become widely used for rephrasing text. While some researchers focus on single-instance paraphrasing for tasks such as AI text detection and data augmentation, other work explores successive paraphrasing, which involves rephrasing a text multiple times in a sequence. In this study, we conduct an in-depth analysis of the characteristics of successive paraphrasing in current prevalent LLMs. 
% Our findings reveal that LLMs often regenerate paraphrases similar to earlier versions, rather than generating entirely new ones, demonstrating a phenomenon of 2-periodicity. This periodicity can be attributed to the self-reinforcing nature of LLMs, which is represented by three types of convergence in our work.
% This periodicity is not limited to paraphrasing but extends to other sequential tasks with similar characteristics. Additionally, the period can be adjusted by providing LLMs with contextual history.
% The results of our analysis experiments show that periodicity can be alleviated, but not eliminated, while preserving the original meaning. 
% The perturbation experiments further reveal that LLMs tend to perform synonym replacement rather than altering the text structure during successive paraphrasing. 
% Our work offers a comprehensive analysis of successive paraphrasing, identifies the underlying periodicity, and highlights the limited expressive capabilities of LLMs.

Dynamical systems theory provides a framework for analyzing iterative processes and evolution over time. Within such systems, repetitive transformations can lead to stable configurations, known as attractors, including fixed points and limit cycles. Applying this perspective to large language models (LLMs), which iteratively map input text to output text, provides a principled approach to characterizing long-term behaviors. Successive paraphrasing serves as a compelling testbed for exploring such dynamics, as paraphrases re-express the same underlying meaning with linguistic variation. Although LLMs are expected to explore a diverse set of paraphrases in the text space, our study reveals that successive paraphrasing converges to stable periodic states, such as 2-period attractor cycles, limiting linguistic diversity. This phenomenon is attributed to the self-reinforcing nature of LLMs, as they iteratively favour and amplify certain textual forms over others. This pattern persists with increasing generation randomness or alternating prompts and LLMs. These findings underscore inherent constraints in LLM generative capability, while offering a novel dynamical systems perspective for studying their expressive potential.





\end{abstract}



\input{article/introduction}

\input{article/Feature}
% \input{latex/article/ConnectionBetween2C}
\input{article/Generalization}
\input{article/related_work}
\input{article/Conclusion}


\bibliography{acl_latex}

\appendix
\label{sec:appendix}


\section{Data Statistics}
\label{app:data_stat}
We provide source information of our data in table \ref{table:dataset} and statistic information of data length in \ref{figs:data distribution}.
\begin{table*}[!h]
    \centering
    \begin{tabular}{lcccccc}
        \toprule
        \textbf{Dataset} & \textbf{TLDR} & \textbf{SQuAD} & \textbf{ROCT} & \textbf{Yelp} & \textbf{ELI5} & \textbf{Sci\_Gen} \\
        \midrule
        \textbf{Sentence/Paragraph} & 100/30 & 100/30 & 100/30 & 100/30 & 100/30 & 100/30 \\
        \midrule
        \textbf{Dataset} & \textbf{XSum} & \textbf{CMV} & \textbf{HSWAG} & \textbf{WP} & \textcolor{red}{\textbf{Wiki}} & \textcolor{red}{\textbf{WMT}} \\
        \midrule
        \textbf{Sentence/Paragraph} & 100/30 & 100/30 & 100/30 & 100/30 & 200/0 & 200/0 \\
        \bottomrule
    \end{tabular}
    \caption{Dataset Setup: Datasets marked in red indicate Chinese datasets, while others represent English datasets. The value indicates the number of extracted samples. For example, we extract 100 sentences and 30 paragraphs from the TLDR dataset.}
    \label{table:dataset}
\end{table*}
\begin{figure}[!h]

\centering
\includegraphics[width=0.45\textwidth]{article/figures/data_dist.pdf}
\caption{Statistical patterns of data length distribution.}
\label{figs:data distribution}
\end{figure}

\section{Change in similarity}
\label{app:similarity}
We measure the change in similarity between \(T_i\) and \(T_0\) across successive paraphrasing steps. The results are presented in Figure \ref{figs:similarity}.
As the number of paraphrasing steps increases, most LLMs maintain the similarity between paraphrases and their corresponding original texts, with the exception of an initial drop in similarity.
Meanwhile, it also exhibits aslight 2-periodicity in similarity.
By combining Figure \ref{figs:similarity} and Table \ref{figs:periodicity}, we found that models with higher periodicity also exhibit higher similarity.



\label{App:periodicity}
\begin{figure}[!h]
\centering
\includegraphics[width=0.5\textwidth]{article/figures/similarity.pdf}
\caption{Similarity changes during successive paraphrasing. Qwen2.5-72B is the best at preserving meaning, while all other LLMs experience slight degradation in similarity, except during the first paraphrasing step.}
\label{figs:similarity}
\end{figure}

\section{Generalization}
\label{app:generalization}
\label{App:Temperature}


\subsection{Task Extentions}
We propose four additional tasks beyond paraphrasing: polishing (\textbf{Pol.}), clarification (\textbf{Clar.}), informal-to-formal style transfer (\textbf{I/F.}), and forward/backward translation (\textbf{Trans.}).
The detailed prompts for these tasks are listed in Table \ref{tab:task_extension_prompts}. 
We perform these tasks on our paragraph dataset, calculate the textual difference of the paraphrase at each iteration with the initial text, and plot the results in Figure~\ref{figs:Other_tasks_div_trend}.
As the number of paraphrasing steps increases, the difference between \(T_i\) and \(T_{i-2}\) decreases. 
After 7 steps, there is little difference between \(T_i\) and \(T_{i-2}\).

% All of these tasks exhibit 2-periodicity, and the corresponding difference confusion matrices are shown in Figure \ref{figs:task_extensions}.




\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{article/figures/TaskExtentionTrend.pdf}
    \caption{The trend in normalized edit distance between $T_i$ and $T_{i-2}$ across various tasks during the repetition process using GPT-4o-mini.}    
    \label{figs:Other_tasks_div_trend}
\end{figure}


\label{App:tasks}
\begin{table}[!h]
    \centering
    \small
    \begin{tabular}{p{0.12\linewidth}p{0.78\linewidth}}
    \toprule
        \textbf{Pol.} &  Please polish the following text: \{text\}  \\
    \midrule
        \textbf{Clar.} &  Please rewrite the following text in a way that is simpler and easier to understand, using clear language and shorter sentences without losing the original meaning: \{text\}  \\
    \midrule
        \textbf{I/F.} & Transform the following text into an informal style:  \{text\} / Rewrite the following text in a formal style:  \{text\}  \\
    \midrule
        \textbf{Trans.} & Please translate the following English text into Chinese: \{text\} / Please translate the following English text into Chinese: \{text\}  \\
    \bottomrule
    \end{tabular}
    \caption{Four types of prompts for extension tasks. The last two tasks involve switching between different languages and styles, separated by a semicolon.}
    \label{tab:task_extension_prompts}
\end{table}


\subsection{Model and Prompt variation}

\label{App:prompt_var}

We continue to modify the models and prompts during paraphrasing.
The chosen model set includes GPT-4o-mini, GPT-4o, Qwen2.5-7B, and Llama3-8B.
Four variations of the paraphrasing prompts are provided in Table \ref{tab:prompts_var}. 


\begin{table}[!h]
    \centering
    \small
    \begin{tabular}{p{0.05\linewidth}p{0.88\linewidth}}
    \toprule
        \textbf{A:} &  Please paraphrase the following text: \{text\}  \\
    \midrule
        \textbf{B:} &  Please rephrase the text below: \{text\}  \\
    \midrule
        \textbf{C:} & Please rewrite the following text:  \{text\}  \\
    \midrule
        \textbf{D:} & Please polish the text below: \{text\}  \\
    \bottomrule
    \end{tabular}
    \caption{Four variations of paraphrasing prompts. In the prompt variation experiments, a prompt is randomly selected at each step to perform the paraphrasing.}
    \label{tab:prompts_var}
\end{table}

\subsection{Experiment with A Complex Prompt}
\label{app:complex_prompt}
We conduct experiments on our paragraph-level dataset using a more complex prompt, as presented in Table~\ref{tab:more_complex_prompt}. The resulting difference confusion matrix is shown in Figure~\ref{fig:more_complex_prompt}.

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{article/figures/complex.pdf}
    \caption{Difference confusion matrix for the complex prompt.}
    \label{fig:more_complex_prompt}
\end{figure}

\begin{table}[!h]
    \centering
    \small
    \begin{tabular}{p{0.95\linewidth}}
    \toprule
    Please rewrite the following paragraph with the goal of enhancing lexical and syntactical variety without changing the original meaning. Pay attention to employing diverse vocabulary, increasing the complexity and variation of sentence structures, using different conjunctions and clause constructions to make the expression more diverse and rich, while maintaining the core information and logical coherence of the original text. Specifically, avoid repetitive sentence patterns and try to express the same ideas in different ways.\\
    \bottomrule
    \end{tabular}
    \caption{A more complex prompt enhancing lexical and syntactical variety.}
    \label{tab:more_complex_prompt}
\end{table}

\subsection{Increasing Randomness}

We measure the impact of increasing randomness on periodicity by adjusting the generation temperature. We select four temperature values: 0.6, 0.9, 1.2, and 1.5. 
The results are shown in Figure \ref{figs:app_temperature}.
Although the temperature increases to a very high level, the 2-periodicity still persists.
Further increasing temperature will cause nonsense responses.

\begin{figure}[t!]
\centering
\includegraphics[width=0.45\textwidth]{article/figures/app_temperaturev2.pdf}
\caption{The difference confusion matrix for successive paraphrasing at different temperature settings, conducted by GPT-4o-mini.}
\label{figs:app_temperature}
\end{figure}


\begin{figure}[t!]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{article/figures/sim.pdf}
        \caption{The similarity between paraphrases and the original texts increases during the paraphrasing process.}
        \label{figs:sim}
    \end{minipage}
    \label{fig:overall}
\end{figure}

\subsection{Sample Selection Strategies}
\label{app:sample_selection}
We propose three strategies for successive paraphrasing and evaluate them across different LLMs. 
To assess the impact of these strategies on meaning preservation, we measure the similarity between the paraphrases \(T_i\) and their corresponding original texts \(T_0\) and demonstrate the result in Figure \ref{figs:sim}.
By combining Figure \ref{figs:sim} and Figure \ref{figs:strategy}, we suggest that the random strategy preserves meaning significantly better than the max strategy, while also effectively alleviating periodicity.



\section{Case Study}
We present part of the first case from our paragraph dataset for successive paraphrasing in Figure \ref{tab:case_study_1}.


%\begin{figure*}
%    \centering
%    \begin{minipage}{\textwidth}
%        \centering
%        \includegraphics[width=1\textwidth]{article/figures/app_task_ext.pdf}
%        \caption{The difference confusion matrix for four tasks beyond paraphrasing. 
%        Note that in translations, the difference between texts in two different languages is set to one.}
%        \label{figs:task_extensions}
%    \end{minipage}
%    \hfill
%    \begin{minipage}{1\textwidth}
%        \centering
%        \includegraphics[width=\linewidth]{article/figures/app_temperature.pdf}
%        \caption{The difference confusion matrix for successive paraphrasing at different temperature settings, conducted by GPT-4o-mini.}
%        \label{figs:app_temperature}
%    \end{minipage}
%    \label{fig:overall}
%\end{figure*}


%%%%%%%%%%%%%%%%%%%%% 代替换
\begin{table*}[t!]
    \setlength{\belowcaptionskip}{-0.cm}
    \centering
    \small
    \renewcommand{\arraystretch}{1.2} % Adds padding to the rows for better visualization
    \begin{tabular}{lp{0.8\linewidth}} % Adjust the 0.7\linewidth as needed
    \toprule
    \multicolumn{2}{c}{\textbf{Sample of successive paraphrasing}} \\
    \midrule
      $T_0$ &Australia has a strong tradition of an honest days pay for an honest days work. We have a strong union presence which leads to high wages backed by a strong resource economy. We also have a very multicultural society with many immigrants choosing to settle in Australia this leads to diverse shopping areas where goods originally made all over the world are made right here on our shores. The main reason Australia is the best country in the world is our heavily ingrained concept of a "fair go" meaning you give anybody a chance to prove themselves no matter who they are. This is not even mentioning our spectacular landscapes and beaches. All in all Australia is the best country in the world.  \\
    \midrule
      $ T_1$ & Australia has a robust tradition of fair compensation for hard work. The strong presence of unions contributes to high wages, supported by a solid resource-based economy. Additionally, our multicultural society, enriched by many immigrants, creates diverse shopping areas where products from around the globe are available right here in Australia. A key reason why Australia stands out as the best country in the world is our deep-rooted belief in a "fair go," which emphasizes giving everyone the opportunity to demonstrate their abilities, regardless of their background. Not to mention, our stunning landscapes and beautiful beaches add to our appeal. Overall, Australia truly is the best country in the world. \\
      \midrule
     $ T_2$ & Australia has a strong tradition of ensuring fair pay for hard work. The significant influence of unions leads to high wages, bolstered by a stable resource-driven economy. Furthermore, our multicultural society, enriched by a diverse population of immigrants, fosters vibrant shopping districts where global products are readily available. One of the main reasons Australia is considered the best country in the world is our fundamental belief in a "fair go," which highlights the importance of providing everyone with the chance to showcase their talents, regardless of their background. Additionally, our breathtaking landscapes and beautiful beaches enhance our country's appeal. Overall, Australia is truly the best place in the world.  \\
    \midrule
    $ T_3$ & Australia has a strong history of promoting fair compensation for hard work. The powerful presence of unions contributes to high wages, supported by a stable economy driven by natural resources. In addition, our multicultural society, enriched by a diverse immigrant population, creates lively shopping areas filled with international products. A key reason why Australia is often regarded as the best country in the world is our core belief in a "fair go," which emphasizes the importance of giving everyone the opportunity to demonstrate their abilities, regardless of their background. Moreover, our stunning landscapes and beautiful beaches add to the allure of our nation. Overall, Australia is undoubtedly one of the finest places in the world.  \\
    \midrule
   $T_4$ & Australia has a rich tradition of advocating for fair pay for hard work. The strong influence of unions helps maintain high wages, bolstered by a robust economy fueled by natural resources. Additionally, our multicultural society, enhanced by a varied immigrant community, fosters vibrant shopping districts filled with international goods. A significant reason Australia is often viewed as one of the best countries globally is our fundamental belief in a "fair go," which highlights the necessity of providing everyone with the chance to showcase their talents, regardless of their background. Furthermore, our breathtaking landscapes and beautiful beaches contribute to the appeal of our nation. Overall, Australia is truly one of the most exceptional places in the world.  \\
     \midrule
    $T_5$ & Australia has a rich history of promoting fair compensation for hard work. The strong presence of unions plays a crucial role in ensuring high wages, supported by a thriving economy driven by natural resources. Moreover, our diverse society, enriched by a wide-ranging immigrant population, creates lively shopping areas filled with products from around the globe. A key reason Australia is often regarded as one of the best countries in the world is our core belief in a "fair go," which emphasizes the importance of giving everyone the opportunity to demonstrate their abilities, regardless of their background. Additionally, our stunning landscapes and beautiful beaches enhance the allure of our nation. In summary, Australia is truly one of the most remarkable places in the world. \\
     \midrule
    $T_6$ & Australia has a notable history of advocating for fair pay for hard work. The significant influence of unions is essential in maintaining high wages, bolstered by a robust economy fueled by natural resources. Furthermore, our diverse society, enriched by a broad immigrant community, fosters vibrant shopping districts filled with international products. A major reason Australia is frequently seen as one of the best countries globally is our fundamental belief in a "fair go," which underscores the importance of providing everyone the chance to showcase their talents, irrespective of their background. Additionally, our breathtaking landscapes and beautiful beaches add to the appeal of our nation. In conclusion, Australia is indeed one of the most extraordinary places in the world.  \\
     \midrule
    $ T_7$ & Australia has a rich history of promoting fair compensation for hard work. The strong presence of unions plays a crucial role in sustaining high wages, supported by a thriving economy that benefits from its natural resources. Moreover, our multicultural society, enhanced by a diverse immigrant population, creates lively shopping areas filled with global products. A key reason Australia is often regarded as one of the best countries in the world is our core belief in a "fair go," which highlights the importance of giving everyone the opportunity to demonstrate their abilities, regardless of their background. Additionally, our stunning landscapes and picturesque beaches contribute to the allure of our nation. In summary, Australia is truly one of the most remarkable places on Earth.  \\
    \bottomrule
    \end{tabular}
    \caption{
    \label{tab:case_study_1}
    Case illustration of the first sample in our paragraph dataset. Due to the limited content, we just display \( T_0... T_{7}\).
    }


    
\end{table*}
\end{document}

