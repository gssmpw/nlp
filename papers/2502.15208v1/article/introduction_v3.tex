
\section{Introduction}
%In recent years, large language models (LLMs) have experienced rapid development. 
%Current LLMs demonstrate their impressive language abilities and have been widely used in people's daily lives, especially for polishing or rewriting articles, which has been extensively investigated in the research community and is referred to as paraphrasing ~\citep{bhagat2013what}.
%Using LLMs to paraphrase text is not only a great way to polish articles, especially for non-native speakers, but also an efficient method for data augmentation.
%However, some research also poses that potential misuse of paraphrasing might lead to plagiarism and the spread of fake news. 
%Besides, these texts paraphrased by LLMs might suffer from hallucinations\cite{} and factual errors\cite{}.

Paraphrases are sentences or phrases that convey the same meaning using different wording\citep{bhagat2013what}. 
Current large language models (LLMs) have demonstrated impressive ability in paraphrasing and have been widely used to polish articles and posts, especially for non-native speakers. 
In the research community, paraphrasing is also an efficient method for data augmentation\cite{}, helping solve the low resource problems\cite{}. 
However, some research also poses that potential misuse of paraphrasing might lead to plagiarism and the spread of fake news. 
Besides, these texts paraphrased by LLMs might suffer from hallucinations\cite{} and factual errors\cite{}.
Although there has been a considerable amount of work focusing on paraphrasing, most of it only considers text that has been paraphrased once.
In the real world, due to the widespread use of LLMs, any text you read on the internet might have been edited and paraphrased multiple times by various individuals for different reasons, a process we refer to as successive paraphrasing.
Based on our intuition and daily use of LLMs, we have noticed that successive paraphrasing exhibits some unique properties which raise our great interest.



We conduct our experiments using 2 language settings: English and Chinese. 
We collected 800 texts from \textbf{arXiv}, \textbf{Quora}, \textbf{Wikipedia}, and \textbf{CNN News} in equal proportions to form the original English dataset.
For the Chinese dataset, we sampled 800 texts from WMT2019\cite{}.
Each text in the datasets will be paraphrased 15 times by various LLMs (e.g., GPT-3.5-turbo, Llama3-8B-instruct, and GLM4-9b-chat). 
We calculate the BLEU score within each step paraphrase, and the result demonstrates that all LLMs exhibit periodicity during successive paraphrasing. 
The periodicity of successive paraphrases indicates that when you repeatedly paraphrase a text, each paraphrase is more similar to the one made two steps earlier. 
This implies that the nth paraphrase exhibits higher similarity with the (n-2)th, (n-4)th, (n-6)th, and so forth.



Moreover, through our comprehensive investigation, we reveal that successive paraphrasing tends to converge across various dimensions. 
To evaluate the diversity of potential paraphrased texts, we introduce a metric called sample space diversity. 
As the number of paraphrase steps increases, the resulting text becomes more certain, as indicated by the declining trend in sample space diversity (SSD). 
Meanwhile, the perplexity of paraphrases shows a decreasing trend and eventually converges to a specific value.
Besides, we show that there exists a strong positive correlation between SSD and perplexity in successive paraphrasing.
Furthermore, we argue that the periodicity observed in successive paraphrasing may be attributed to convergence phenomena. 
We believe that the low perplexity of the text being paraphrased is key to this periodicity.
In section 5, we will elaborate in detail how the convergence leads to periodicity.


While periodicity emerges during successive English paraphrasing by ChatGPT, the successive Chinese paraphrases show little periodicity. 
In our opinion, this is also the evidence supporting our hypothesis.
Since English occupies the largest proportion of the training corpus for ChatGPT, the model tends to have lower perplexity when processing English compared to Chinese.
To further support our assumption, we employed different LLMs to generate 10 samples at each iteration, selecting samples with the highest and lowest perplexity for the subsequent paraphrasing step.
The experiment's outcome shows the connection between perplexity and periodicity of successive paraphrasing.


Based on these properties, we discuss the potential use and harm of successive paraphrasing.
%%%%%


%%%%%%


Overall, in this paper, we deeply investigated the properties of successive paraphrasing. Our contributions are summarized as follows:
\begin{itemize}
    \item To our knowledge, we are the first to comprehensively study successive paraphrasing across a range of widely used large language models and identify the inherent periodicity and convergence.
    
    \item We propose a hypothesis that the periodicity observed in successive paraphrasing might be attributed to convergence phenomena and present experiments to support our view.
    \item We discuss the potential use and harm of successive paraphrasing.   
\end{itemize}


\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{article/figs_v1/intro.png}
    \caption{The similarity confusion matrix of successive paraphrasing is depicted in the following figures. Figures (a), (b), (c), and (d) illustrate the similarity confusion matrix for English successive paraphrasing, whereas figures (e) and (f) display the matrix for Chinese.}  
\end{figure}
%We discovered that successive paraphrases demonstrate both \textbf{periodicity} and \textbf{convergence}.
%The periodicity of successive paraphrases indicates that when you repeatedly paraphrase a text, each paraphrase is more similar to the one made two steps earlier. 
%This implies that the nth paraphrase exhibits higher similarity with the (n-2)th, (n-4)th, (n-6)th, and so forth.
%The convergence of successive paraphrases is that with the number of paraphrase steps increasing, the resulting text is more certain. 
%Meanwhile, the perplexity of paraphrases shows a decreasing trend and eventually converges to a specific value.




%The periodicity of successive paraphrases indicates that when you repeatedly paraphrase a text, the $n$th paraphrase has a higher similarity with the $(n-2)$th, $(n-4)$th, $(n-6)$th, and so on.


%we sometimes find that the paraphrased texts are only slightly refined, while at other times, they differ significantly from their original form.



%Successive paraphrases are iteratively paraphrased texts. 


%Paraphrase application
%Successive Paraphrase

%figure

%Current Research situation -> unique pattern of successive paraphrase,  interesting!

% In this paper