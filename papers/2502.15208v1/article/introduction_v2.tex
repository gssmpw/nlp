\section{Introduction}



% LLM advantage
% paraphrase generation -> successive
% what pattern?
% how to escape

Paraphrases are sentences or phrases that convey the same meaning using different wording\citep{bhagat2013what}.
With the advancement of large language models (LLMs) ~\citep{llama,gpt3,gpt4}, paraphrasing has gained widespread use, leading to the emergence of numerous online paraphrasing tools powered by these models. 
These tools have become particularly popular for refining articles and posts, especially among non-native speakers.
So far, many researchers have focused on the misuse of paraphrasing, including plagiarism \citep{plagiarism} and the spread of fake news \citep{detect-gpt}.
Besides, Some research shows that paraphrased texts also suffer from issues like hallucination \citep{llm_hallu,llm_hallu2} and bias \citep{llm_bias}.
Other researchers have explored a more practical scenario known as successive paraphrasing (SP), where a text is paraphrased multiple times by different individuals for various reasons. 
\cite{can_ai} investigates how SP can evade current AI text detection methods, while \cite{ship} discusses the impact of SP on determining authorship.
Our research stands out from existing work in the field of paraphrasing. 
By analyzing the effects of successive paraphrasing, we identify risks associated with the evolution of LLMs, which we refer to as \textbf{Digital Environment Homogenization} and the underlying \textbf{AI Homogenization}.


Digital environment homogenization (DEH) has recently become a significant concern.
As AI systems generate language based on pre-existing user data and interactions, there is a risk that they could perpetuate and even amplify existing linguistic norms and biases, resulting in the homogenization of language \citep{homogenization}.
\cite{anderson2024homogenization} shows that LLMs exert a stronger homogenizing effect on creative works compared to humans. 
Additionally, \cite{deepfake} highlights the distinct distribution of LLM-generated text in contrast to human-written text. 
%As large language models (LLMs) are increasingly used, the internet is gradually becoming saturated with machine-generated texts. 
As noted by \cite{shumailov2023curse}, the corpora mixed with these LLM-generated texts can lead to model collapse during the training process.
So, digital environment homogenization could have a profound impact on the development of next-generation LLMs.
Moreover, a digitally homogenized world could reshape the thinking of future generations, potentially passing on inherent stereotypes to them \citep{kotek2023gender,shrawgi-etal-2024-uncovering}.

%\textbf{Successive paraphrasing leading to DEH, and simulation experiments}
To clearly illustrate the homogenizing effect caused by the widespread use of paraphrasing, we conduct a simplified experiment that simulates the real digital environment in which people consistently produce paraphrased content by using LLMs.
We use several widely adopted commercial LLMs to paraphrase a subset of the corpus at each step, while concurrently adding new human-generated texts.
At each step, we calculate the standard deviation of the perplexity of the corpus.
Details of the simulation experiment setup can be found in Appendix \ref{Simulation}.
The results shown in \ref{intro} indicate that as the number of steps increases, the standard deviation of perplexity decreases significantly, implicitly revealing the potential homogenization issue in the future.
To understand the underlying causes of homogenization, we designed various experiments to explore successive paraphrasing and identified two interesting characteristics: periodicity and convergence.

%\textbf{Periodicity and Convergence}
Assuming a successive paraphrasing (SP) process denoted as \( s_{1}, \dots, s_{n} \), the periodicity of SP refers to the pattern observed when \( s_{i+1} \) shows significant grammatical similarity to \( s_{i-1} \) during the paraphrasing sequence. 
This suggests that LLMs have limited expressive diversity, which significantly contributes to \textbf{DEH}.
Additionally, we conduct a deeper analysis of the generation process and identify convergence phenomena occurring during the \textbf{SP} process. 
As the step number increases, the perplexity of paraphrase \( s_{i} \) conditioned on \( s_{i-1} \) or \( s_{i+1} \) decreases inversely. 
We identify an intrinsic connection between convergence and periodicity, and we discuss how convergence gives rise to periodicity from both an intuitive and mathematical perspective.
Based on our deduction, we propose a series of tasks that exhibit periodicity, which strongly supports our reasoning.
Furthermore, we conduct the \textbf{S.P.} under the model and prompt variations settings, and it still performs the periodicity characteristic.
It also indicates the underlying homogenization in current LLMs which we refer to \textbf{AIH}.


In the end, we examine the potential effects of \textbf{DEH} and \textbf{AIH}. 
 We consider AIH an inevitable challenge in LLM theory, a view strongly supported by S.P.'s experiments.
In the current theoretical model, improving LLMs could exacerbate the issue of homogenization. 
Therefore, We propose several potential solutions to reduce these risks and discuss the path toward developing more creative LLMs.



\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{article/figs_v1/intro.png}
    \caption{The similarity confusion matrix of successive paraphrasing is depicted in the following figures. Figures (a), (b), (c), and (d) illustrate the similarity confusion matrix for English successive paraphrasing, whereas figures (e) and (f) display the matrix for Chinese.} 
    \label{intro}
\end{figure}

