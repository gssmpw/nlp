\subsection{Deeper Understanding Periodicity}
\label{sec:Convergence2Periodicity}

%\textbf{Periodicity serves as an outward expression of convergence.}
The inherent characteristics of LLMs enable them to generate sequences with low perplexity, indicating a high probability of occurrence. 
During successive paraphrasing, both perplexity and reverse perplexity consistently decrease, resulting in diminished generation diversity and an increased likelihood of \(T_{i-1}\) appearing in the output space of \(T_i\). 
When the reverse perplexity approaches the perplexity, we can treat \(T_{i-1}\) and \(T_{i+1}\) as essentially equivalent. Additionally, low generation diversity suggests that the texts in \(T_{i+1}\) are highly similar to one another.
Therefore, the text in \(T_{i-1}\) may be virtually indistinguishable from the text in \(T_{i+1}\).

%In extreme cases, when \(T_{i-1}\) functions as a candidate paraphrase for \(T_i\) and the generation diversity of \(T_i\) is sufficiently low, the difference between \(T_{i-1}\) and \(T_{i+1}\) becomes minimal. 
%This is due to the reduction in reverse perplexity; as the reverse perplexity of \(T_{i-1}\) conditioned on \(T_i\) approaches that of \(T_{i+1}\) conditioned on \(T_i\), \(T_{i-1}\) qualifies as a candidate paraphrase for \(T_i\). 
%However, the intrinsic nature of paraphrasing requires LLMs to produce variations of the original text, resulting in \(T_{i+1}\) differing from \(T_i\). Combining these characteristics, LLMs perform a 2-periodicity in successive paraphrasing.




The decrease in reverse perplexity can be attributed to the invertibility of paraphrasing. 
Given a text and one of its paraphrases, we can consider the text as a paraphrase of its paraphrase, reinforcing the notion of invertibility of paraphrasing.
Building on this perspective, there are likely other tasks characterized by invertibility that exhibit similar periodicity to paraphrasing. We will extend our experiments to investigate this further in Section \ref{sec:beyond paraphrasing}.


%The nature of LLMs leads them to generate sequences with low perplexity, indicating a high likelihood at the same time.
%During successive paraphrasing, both perplexity and reverse perplexity consistently decrease, leading to a reduction in generation diversity and an increase in the likelihood of $T_{i-1}$ appearing in the output space of $T_i$.
%In an extreme situation, where the $T_{i-1}$ is a candidate paraphrase of \(T_i\) and the generation diversity of \(T_i\) is also low enough, there will be little difference between $T_{i-1}$ and $T_{i+1}$.
%Due to the decrease in reverse perplexity, when the reverse perplexity of \(T_{i-1}\) conditioned on \(T_i\) is near the perplexity of \(T_{i+1}\) conditioned on \(T_i\), T_{i-1} is in the candidate paraphrases of \(T_i\).



\iffalse

\subsection{The Mathematical Perspective}

We aim to explain that under certain conditions, the successive paraphrasing process using a LLM converges to an approximate period-2 behavior. Specifically, we incorporate the empirical findings that both forward and backward perplexities decrease as the iterative steps increase.
We represent the convergence of forward and backward perplexity as,
\[
\lim_{n\to\infty}\mathcal{L}(T_{n+1}|T_{n})=\mathcal{L}_f \geq 0    
\]
\[
\lim_{n\to\infty}\mathcal{L}(T_{n}|T_{n+1})=\mathcal{L}_b \geq 0    
\]
as both forward and backward perplexity decreases, \(P\) and \(P^{-1}\) becomes increasingly deterministic, 

Meanwhile, there exists \(\delta>0\) such that for all \(n\),
\[
d(T_{n+1},T_n) \geq \delta
\]
This ensures sufficient diversity between consecutive texts.


Under the above definitions and assumptions, the sequence \(\{T_n\}\) exhibits approximate period-2 behavior.
Specifically, there exists \(N \in Z^+\), such that for all \( n \geq N \),
\[
d(T_{n+2},d_{n}) \leq \epsilon,
\]
where \(\epsilon > 0\) is a small positive value indicating low diversity between \(T_n\) and \(T_{n+2}\). 
We will provide a brief explanation below.


\textbf{Step 1: Behavior of \(P\) and \(P^{-1}\) After Perplexity Convergence.}
Given \(P^{-1}\) represents the LLMâ€™s mapping from a paraphrase back to the original text.
As both forward and backward perplexities decrease, the functions 
\(P\) and \(P^{-1} \) become increasingly deterministic and invertible. 
For sufficiently large \(n\), we have:
\[
P(T_n) = T_{n+1}, P^{-1}(T_{n+1}) = T_n.
\]

\textbf{Step 2: Establishing that \(P\) is an Involution.}
We will show that \(P\) is an involution on the set of texts after convergence.
Since \(P^{-1}\) is the inverse of P, and both are deterministic:
\[P^{-1}=P\]
Therefore, \(P(P(T)) = T\), for all T after convergence, which means that \(P\) is an involutive function.



\textbf{Step 3: Implications of \(P\) Being an Involution.}
Because \(P\) is an involution, its mapping consists solely of fixed points and
transpositions (cycles of length 2). 
However, the minimal dissimilarity constraint rules out fixed points (cycles of length 1), since:
\[P(T) = T \rightarrow d(T, T) = 0 < \delta\]

\textbf{Step 4: The Sequence Must Cycle Between Exactly Two Texts.}
Given that \(P\) is an involution without fixed points, the only possible cycles
are of length 2. Let \(T_A\) and \(T_B\) be the texts such that:
\[
P(T_A)=T_B, P(T_B)=T_A
\]
Thus, the sequence alternates between TA and TB:
\begin{equation*}
T_n =
\begin{cases}
    T_A, & \text{if } n \text{ is even}, \\
    T_B, & \text{if } n \text{ is odd}.
\end{cases}
\end{equation*}


\textbf{Step 5: Verifying the Minimal Dissimilarity Constraint.}
The minimal dissimilarity constraint is satisfied since:
\[d(P(Tn), Tn) = d(Tn+1, Tn) \geq \delta\]
This holds for all \(n\), as \(T_n\) and \(T_{n+1}\) are distinct due to the absence of fixed points.

\textbf{Step 6: Bounding the Distance Between \(T_{n+2}\) and \(T_n\).}
In the deterministic case, we have:
\[
T_{n+2} = P(T_{n+1}) = P(P(T_n)) = T_n.
\]
As a result:
\[
d(T_{n+2}, T_n) = d(T_n, T_n) = 0.
\]
However, in practice, residual stochasticity introduces small deviations, so we consider:
\[
d(T_{n+2}, T_n) \leq \epsilon,
\]
where \(\epsilon\) is a small positive value. This adjustment links the theoretical result to empirical observations, such as the low diversity (e.g., low normalized edit distance scores) between \(T_n\) and \(T_{n+2}\).

\textbf{Step 7: Impossibility of Longer Periods.}
Assume, for the sake of contradiction, that there exists a period \(k > 2\) with distinct texts \(T_0, T_1, \dots, T_{k-1}\) such that: \(T_{n+k} = T_n\), and \(P(T_i) = T_{i+1}\) for \(i = 0, 1, \dots, k-1 \mod k\).

Since \(P\) is deterministic and invertible (an involution), we must have:
\[
P(P(T_i)) = T_i \quad \text{for all } i.
\]
This implies:
\[
T_i = P(P(T_i)) = P(T_{i+1}) = T_{i+2}.
\]
Therefore:
\[
T_{i+2} = T_i.
\]
This means the sequence repeats every 2 steps, contradicting the assumption that the period is \(k > 2\). Thus, longer periods are impossible under the given conditions.


As we illustrate above, the only possible behavior is an approximate period-2 cycle between \(T_A\) and \(T_B\). The minimal dissimilarity constraint eliminates
fixed points, and the involutive nature of \(P\) precludes longer cycles. 
\fi
