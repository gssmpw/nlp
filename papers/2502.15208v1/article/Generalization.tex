\section{Analysis}
\label{sec:Analysis}
% In this section, we will discuss the generalization of this phenomenon. 
% Considering content limitation, We put the respective confusion matrixes in Appendix \ref{app:generalization}.
In this section, we perform analytical experiments on paragraph-level paraphrase datasets to generalize our findings to longer texts.
We first demonstrate the extension of our findings to other task formats (Section~\ref{sec:beyond paraphrasing}). 
Then we go through a set of methods to try to escape from the attractor cycles in the remaining subsections.

\subsection{Beyond Paraphrase Generation}
\label{sec:beyond paraphrasing}

% \textbf{Effect of text length.} 
% A longer context can naturally be conveyed in many different ways. However, this does not hold true for LLMs.
% We extend our sentence-level experiments to the paraghraph-level.
% We utilize GPT-4o-mini to successively rephrase the text 15 times in our paragraph datasets and measure the periodicity and the differences between paraphrases.

% \textbf{Task Extension.} 

% According to our analysis in Section~\ref{sec:Convergence}, LLMs are expected to exhibit periodicity in invertible tasks. We further propose 4 tasks with the invertible characteristic, including polishing (\textbf{Pol.}), informal-to-formal style transfer (\textbf{I/F.}), clarify (\textbf{Clar.}), and translation (\textbf{Trans.}) using our paragraph dataset.
% The details of these tasks can be accessed in Appendix \ref{App:tasks}.
Our earlier results indicate that successive paraphrasing leads LLMs to settle into periodic attractors—specifically, 2-period limit cycles. According to the systems-theoretic perspective, such cycles should arise whenever the transformation is invertible, enabling a bidirectional mapping that makes prior states easily reproducible. To test this, we examine four additional invertible tasks at the paragraph level: polishing (Pol.), clarification (Clar.), informal-to-formal style transfer (I/F.), and forward/backward translation (Trans.). These tasks are defined in Appendix~\ref{App:tasks}.

\begin{figure}
\centering
\includegraphics[width=0.45\textwidth]{article/figures/app_task_extv2.pdf}
\caption{The difference confusion matrix for four tasks beyond paraphrasing. 
Note that in translations, the difference between texts in two different languages is set to one.}
\label{figs:task_extensions}
\end{figure}


% We plot the difference between \(T_i\) and \(T_{i-2}\) as the number of steps increases, and compare the periodicity of four tasks alongside paraphrasing. 
% As illustrated in Figures \ref{figs:Other_tasks_div_trend}, even at the paragraph level, LLMs tend to become trapped in specific stable states, exhibiting high 2-periodicity degrees. 
Figure~\ref{figs:task_extensions} shows that even for these varied tasks, LLMs repeatedly converge to stable states, exhibiting pronounced 2-periodicity. 
Table~\ref{table:task_extension} shows the degree of 2-periodicity across these tasks, with values ranging from 0.65 to 0.87. 
This finding reinforces the idea that invertibility fosters the emergence of limit cycles, as the model iterates the transformation and settles into an attractor. 
While paraphrasing is our primary lens, these findings confirm that stable attractor cycles are a broader characteristic of LLM behavior in iterative, invertible mappings.
% Confusion matrices of these tasks are presented in Appendix~\ref{App:tasks}. 



\begin{table}[!h]
    \centering
    \small
    \begin{tabular}{cccccc}
        \toprule
        \textbf{Tasks}& \textbf{Para.} & \textbf{Clar.} &\textbf{Pol.} & \textbf{I/F.} & \textbf{Trans.} \\
        \midrule
        \textbf{$\tau$} &0.80 & 0.83 &  0.86 & 0.65 & 0.87 \\
        \bottomrule
    \end{tabular}
    \caption{Impact of perturbations on periodicity compared to the original during paraphrasing.}
    \label{table:task_extension}
\end{table}

%\begin{figure}[!h]
%    \centering
%    \includegraphics[width=1\linewidth]{article/figures/TaskExtentionv2.pdf}
%    \caption{The periodicity of paraphrasing and other tasks on a paragraph-level English dataset using GPT-4o-mini, where a lower value indicates higher periodicity.} 
%    \label{figs:Other_tasks_Periodicity}
%\end{figure}




%
%Similar to paraphrasing, we iteratively perform these tasks on a paragraph-level dataset.
%We plot the difference between \(T_i\) and \(T_{i-2}\), and compare the 2-periodicity of four tasks %alongside paraphrasing.
%Figure \ref{figs:Other_tasks_div_trend} illustrates the decreasing trend of the four tasks, similar to that of paraphrasing, while Figure \ref{figs:Other_tasks_Periodicity} demonstrates their comparable periodicity to paraphrasing.





\subsection{Alternating Models and Prompts}
% We extend our experimental settings by introducing model and prompt variations in the process of successive paraphrasing.

% \textbf{Paraphrasing with prompt variation.}
% We design four distinct prompts that are used to rephrase text while preserving its original meaning, which can be accessed in (Appendix \ref{App:prompt_var}).
% GPT-4o-mini is then employed for successive paraphrasing, with a prompt randomly selected from the set for each iteration. 
% Although the prompt is changed during successive paraphrasing, it maintains a 2-periodicity, experiencing minimal degradation, as shown in Figure \ref{figs:model_prompt_var}.


% \textbf{Paraphrasing with model variation.}
% We selected a model set consisting of GPT-4o-mini, GPT-4o, Llama3-8B, and Qwen2.5-7B. 
% We randomly chose a different LLM from the set for each iteration. 
% Although each LLM exhibits its own paraphrasing style, contributing to the diversity of paraphrases, the 2-periodicity still persists.
% In other aspects, we calculated the perplexity of \(T_i\) conditioned on \(T_{i-1}\) using Llama3-8B, where both \(T_i\) and \(T_{i-1}\) were generated by different LLMs.
% As shown in Figure \ref{figs:PPL_conditioned_Llama3}, the perplexity of \(T_i\) decreases during successive paraphrasing, despite the fact that it was not generated by the LLM performing the calculation. 
% Both the existing convergence and 2-periodicity indicate the presence of statistically optimal patterns that exist across all LLMs.

One intuitive approach to escape an attractor is to introduce perturbations in the transformation itself. We attempt this by varying both models and prompts during successive paraphrasing. 
For \textbf{prompt variation}, we design four different paraphrasing prompts (refer to Appendix~\ref{App:prompt_var}) and randomly select one at each iteration. Despite regularly switching prompts, the 2-period cycle persists, as shown in Figure~\ref{figs:model_prompt_var}.

\begin{figure}[!hb]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{article/figures/app_model_prompt_var.pdf}
        \caption{The difference confusion matrices for model variation and prompt variation.}
        \label{figs:model_prompt_var}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
          \centering
    \includegraphics[width=1\linewidth]{article/figures/model_var_conditioned_llama3-7b.pdf}
    \caption{The perplexity of \(T_i\) conditioned on \(T_{i-1}\) calculated by Llama3-8B. Both \(T_i\) and \(T_{i-1}\) are generated by other LLMs. }    
    \label{figs:PPL_conditioned_Llama3}
    \end{minipage}
    \label{fig:overall}
\end{figure}

Similarly, we introduce \textbf{model variation} by alternating among GPT-4o-mini, GPT-4o, Llama3-8B, and Qwen2.5-7B during successive paraphrasing. 
Although each model brings its own stylistic biases, the fundamental attractor cycle remains intact. 
Interestingly, perplexity computed by a single model (e.g., Llama3-8B) on paraphrases generated by other models still decreases over iterations in Figure~\ref{figs:PPL_conditioned_Llama3}. 
This suggests that the attractor states are not confined to a single model’s parameter space.
Instead, they reflect a more general statistical optimum that multiple LLMs gravitate toward.



From a systems perspective, this findings suggest that randomizing the transformation function $P$ does not inherently break the attractor. 
The system remains in a basin of attraction shared across these varied modeling conditions, implying that the stable cycle is a robust property of the iterative transformation rather than a quirk of any particular prompt or model.


\subsection{Increasing Generation Randomness}
\label{sec:temperature}
% When LLMs become entrenched in what appears to be an optimal state, increasing the temperature has little effect in enabling them to escape this condition.
% We repeated our paraphrasing experiments using GPT-4o-mini with temperature settings of  0.6, 0.9, 1.2 and 1.5.

% As shown in Figure \ref{figs:Temperature}, while increasing the temperature from 0.6 to 1.5, The difference between paraphrases increases too.
% Nevertheless, the 2-periodicity phenomenon still remains. 
% Further elevation of the temperature leads to the generation of nonsensical text.
% In other words, LLMs cannot explore more valid expressions during paraphrasing.


\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{article/figures/Temperature.pdf}
    \caption{
    The difference between \(T_{15}\) and \(T_i\) generated by GPT-4o-mini. 
    By increasing the temperature, randomness is amplified, causing the differences to grow as well.
    }    
    \label{figs:Temperature}
\end{figure}
Another strategy is introducing more stochasticity in the generation process by increasing the generation temperature. 
Higher temperatures expand the immediate token selection space, potentially allowing trajectories to wander away from the attractor. However, as shown in Figure~\ref{figs:Temperature}, while higher temperatures do increase the difference between successive paraphrases, the system still exhibits a 2-period cycle. 
Further increases in temperature lead only to nonsensical outputs.
This outcome aligns with dynamical systems theory: a small increase in stochasticity may create local perturbations, but if the basin of attraction is strong, the system remains near the limit cycle. 
Excessive stochastic forcing can push the system out of meaningful regions of state space entirely, leading to “chaotic” or nonsensical behavior, rather than discovering a new stable attractor with richer linguistic diversity.



\subsection{Experiments with Complex Prompts}
Previous experiments were conducted using a simple paraphrase prompt, leading to existing limitations. To solve this, we experimented with a more complicated prompt, and the results indicated similar periodicity patterns. This prompt forces LLMs to enhance grammatical and syntactical variety. 
We used this prompt to instruct GPT-4o-mini to successively paraphrase the paragraph-level test set for 15 rounds. The empirical evaluation of periodicity (2-periodicity score) and convergence (PPL) of the successive paraphrasing with the complex prompt is listed below. Both the difference confusion matrix and the prompt are shown in  Appendix~\ref{app:complex_prompt}.

\begin{table}[h]
\centering
\small
\begin{tabular}{ccc}
\toprule
\textbf{Model} & \textbf{Periodicity} & \textbf{Convergence} \\ \midrule
Original & 0.80 & 1.19 \\
Complex  & 0.67 & 1.33 \\ \bottomrule
\end{tabular}
\caption{Periodicity and Convergence Table}
\end{table}

Although the sophisticated prompt alleviated the periodicity and convergence in some degree, the pattern of 2-period cycle remained strong. For context, a periodicity score of 0.67 implies an average edit distance of 0.33 between paraphrases two steps apart, whereas direct paraphrase exhibits an edit distance of 0.68. 


\subsection{Incoporating Local Perturbations}


% To account for perturbations that occur in real-world usage, we simulate human interference during successive paraphrasing.
We introduce local perturbations to mitigate the attractor cycle pattern. 
At the end of each iteration, we edit 5\% of the text by introducing perturbations using three methods: synonym replacement (S.R.), word swapping (W.S.), and random insertion or deletion (I./D.).
% According to Figure \ref{figs:3-periodicity}, synonym replacement results in minimal periodicity degradation, followed by random insertion or deletion. In contrast, word swapping leads to significant degradation.
As shown in Table~\ref{table:huamn_interven}, 
among these interventions, synonym replacement barely affects periodicity, suggesting that minor lexical changes do not move the system out of the attractor’s basin. 
It indicates that except during the first paraphrasing, LLMs primarily perform synonym replacements for words or phrases, as shown in Figure \ref{figs:intro}.
Word swapping, however, causes more significant disruption, lowering periodicity more effectively. 
From a dynamical standpoint, large structural perturbations are needed to shift the system’s state out of a stable cycle. 
Local lexical tweaks do not suffice because the attractor’s pull is strong and preserved at a deeper structural level.
\begin{table}[!h]
    \centering
    \small
    \begin{tabular}{cccc}
        \toprule
            \textbf{w/o Perturb.} & \textbf{S.R.} & \textbf{W.S.} & \textbf{I./D.} \\
        \midrule
             0.77 & 0.73 &  0.62 & 0.66 \\
        % \midrule
        % Difference & - & \textbf{-0.04} & -0.15 & -0.11 \\
        \bottomrule
    \end{tabular}
    \caption{Impact of different types of perturbations on 2-periodicity degrees $\tau$, compared to the original text during paraphrasing.}
    \label{table:huamn_interven}
\end{table}

% The minimal impact of synonym replacement aligns with our observations, as shown in Figure \ref{figs:intro}.
% These paraphrases, which involve synonym replacement, demonstrate differences similar to those observed without any perturbation at odd steps.
% It indicates that LLMs primarily perform synonym replacements for words or phrases, except during the first paraphrasing, which significantly alters the text's structure and language use.
% Although random insertion and deletion may lead to information loss, it does not significantly alter the text structure compared to word swapping, which causes substantial changes.



\subsection{Paraphrasing with History Paraphrases}


% We also examine the impact of providing historical context during paraphrasing. 
We consider a scenario where the transformation $\hat{P}$ depends on both $T_i$ and $T_{i-1}$.
This added historical context can alter the equilibrium states. 
In a scenario where we paraphrase \(T_{i}\) based on the reference \(T_{i-1}\), it is essential that \(T_{i+1}\) differs from both \(T_{i}\) and \(T_{i-1}\). This function can be expressed as: $T_{i+1} = \hat{P}(T_{i}, T_{i-1})$.
In this context, \(P_{i-1}\) emerges as a strong candidate for paraphrasing \(P(T_{i+1}, T_{i})\), as it aligns with the distribution of LLMs while maintaining difference from \(\hat{P}(T_{i+1}, T_{i})\), satisfying the task requirement.
As a result, this more complex cycle still represents a stable attractor, albeit of higher order, as shown in Figure~\ref{figs:3-periodicity}.
% Consequently, the periodicity in this scenario will be three.
% We also examine the impact of providing historical context during paraphrasing.
% In a scenario where we paraphrase \(T_{i}\) based on the reference \(T_{i-1}\), it is essential that \(T_{i+1}\) differs from both \(T_{i}\) and \(T_{i-1}\).
% This added historical context can alter the equilibrium states. In fact, incorporating the immediate history leads the system to settle into a 3-period cycle rather than a 2-period cycle, as shown in Figure~\ref{figs:3-periodicity}. 
% This more complex cycle still represents a stable attractor, albeit of higher order.
% It suggests that even with more elaborate conditioning, the model is not freed from limit cycles; instead, it can become locked into more intricate periodic behaviors.



\label{sec:history}
\begin{figure}[!h]
    \centering
    \subfigure{\includegraphics[width=0.6\linewidth]{article/figures/3-periodicity.pdf}} 
    %\includegraphics[width=\linewidth]{article/figures/perturbation_periodicity.pdf}
    \caption{When adding historical paraphrases, LLMs exhibit 3-periodicity in the paraphrasing task.}    
    \label{figs:3-periodicity}
\end{figure}

% To verify this, we utilize \(\hat{P}\) during successive paraphrasing. As shown in Figure \ref{figs:3-periodicity}, it indeed performs 3-periodicity, highlighting the limited expression capabilities of LLMs again.

\subsection{Sample Selection Strategies}

\label{sec:sampling_strat}
%\begin{figure}[h]
%    \centering
%    \includegraphics[width=1\linewidth]{article/figures/sim.pdf}
%    \caption{The similarity between paraphrases and the original texts increases during the paraphrasing process.} 
%    \label{figs:sim}
%\end{figure}



% Finally, we investigate methods to steer the system away from stable attractors at the least cost of generation quality. 
% Increasing temperature alone did not help, but controlling perplexity and selecting among multiple sampled paraphrases at each iteration shows promise. 
% By choosing a paraphrase with a higher perplexity, we partially weaken the attractor’s grip, reducing periodicity. 
% Rather than always taking the one with the highest or lowest perplexity. 


% However, as shown in Figures 8 and 14, this may come at the cost of semantic fidelity.

We investigate methods to steer the system away from stable attractors at the least cost of generation quality. 
Given the correlation between periodicity and perplexity, it is intuitive to mitigate this issue by increasing perplexity while maintaining generation quality.
To achieve this, we can randomly sample multiple paraphrases at each iteration and select the one based on perplexity.
We design three types of strategies: selecting the paraphrase with the maximum or minimum perplexity or randomly choosing one at each iteration.
Figures \ref{figs:strategy} illustrate that selecting a higher perplexity can reduce periodicity.
However, such diversity comes at the cost of semantic equivalence (Appendix~\ref{app:sample_selection}).
Considering both periodicity and meaning preservation, we recommend the random strategy, which effectively reduces periodicity while incurring minimal information loss compared to selecting the option with the lowest perplexity.


\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{article/figures/strategy.pdf}
    \caption{The periodicity of three strategies using different LLMs. } 
    \label{figs:strategy}
\end{figure}

\subsection{The Benefit of Mitigating Small-Size Cycles}
As a data augmentation method, paraphrase diversity should impact downstream tasks.
To figure out this, we conducted an experiment on domain classification using successive paraphrasing for data augmentation. We selected a commonly used dataset, AG News~\cite{zhang2016characterlevelconvolutionalnetworkstext} as the testbed, and trained BERT-based models using different data. For data augmentation, we conducted 5 rounds of successive paraphrasing under two different settings—min-strategy (Min.~Strat.) and max-strategy (Max.~Strat.), as detailed in Section~\ref{sec:sampling_strat}. Below are our results:

\begin{table}[h]
\centering
\small
\begin{tabular}{cccc}
\toprule
\textbf{Metric} & \textbf{w/o Aug.} & \textbf{Min. Strat.} & \textbf{ Max. Strat.} \\ \midrule
    Accuracy↑                       & 83.10        & 83.80        & 84.41        \\ 
2-periodicity↓    &       -     & 3.38          & 4.15          \\ \bottomrule
\end{tabular}
\caption{Performance on AG News across different data augmentation strategies.}
\end{table}

The table shows that our max-strategy, which more effectively mitigates the 2-periodicity cycle, yields more diverse paraphrases for data augmentation and therefore achieves higher classification accuracy (84.4\%) compared to both no augmentation and the min-strategy. We will add these findings to our revision to illustrate the beneficial impact of mitigating small-size cycles on downstream tasks.