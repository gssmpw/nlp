\section{The Connection Between Periodicity and Convergence}
Periodicity serves as an outward expression of convergence.
In this section, we will explore this idea from both an intuitive and mathematical perspective.
\label{Convergences2Periodicity}
\subsection{An intuitive perspective to understand the periodicity in SP}
The nature of LLMs leads them to generate sequences with low perplexity, indicating a high likelihood at the same time.
Recalling that an SP process can be represented as \((s_1, s_2, \dots, s_n)\), we define the set of all SP processes as \(\Omega = \{SP_1, SP_2, \dots, SP_n\}\). The paraphrased corpus at the \(i\)th step is defined as \(SP_j = \{SP_{ji} \mid SP_j \in \Omega\}\).
Let's consider an extreme case.
If the reverse condition perplexity and the condition perplexity converge at the same value \(\alpha\), we can think step corpus \(S_{i-1}\) and \(S_{i+1}\) follow the same distribution.
Specifically, this means that both \(S_{i-1}\) and \(S_{i+1}\) can be derived from \(S_i\) through paraphrasing. 
Since \(S_{i+1}\) is indeed derived from \(S_i\), we can rephrase the claim more concisely: \(S_{i-1}\) can also be derived from \(S_i\). 
If not, the subsequent corpus \(S_{i+1}\) derived from \(S_i\) must have lower conditional perplexity, as LLMs tend to generate sequences with low perplexity, which counters our assumption.
So, in this situation, during a successive paraphrase process, when \(i\) becomes sufficiently large, \(s_{i-1}\) and \(s_{i+1}\) are within the sample space of \(s_i\).
When \(\alpha\) is sufficiently low, the similarity between \(s_{i-1}\) and \(s_{i+1}\) is high, leading to a periodic pattern in the entire paraphrase corpus sequences.

\subsection{A mathematical perspective to understand the periodicity in SP}

In this section, we explain how convergence leads to periodicity during successive paraphrasing. We define the similarity function as \(\Omega\), the conditional perplexity function as \(\sigma\), and the distribution of successive paraphrasing as \(p\). The convergence of conditional perplexity during successive paraphrasing (SP) can be represented as:
\[
E_{t_{i+1}, t_{i} \sim p}[\sigma(t_{i+1}|t_{i})] < E_{t_{i}, t_{i-1} \sim p}[\sigma(t_{i}|t_{i-1})], \quad \lim_{i \to \infty} E_{t_{i}, t_{i-1} \sim p}[\sigma(t_{i}|t_{i-1})] = \alpha
\]
\[
\sigma(t_{i+1}, t_{i}) = \exp \left( -\frac{1}{T} \sum_{j=1}^{T} \log P(t_j \mid t_i, t_{i+1}^1, t_{i+1}^2, \ldots, t_{i+1}^{j-1}) \right)
\]
Meanwhile, We believe that the characteristics we have mentioned are inherent to LLMs. The key to periodicity is the task-specific decrease in reverse conditional perplexity, which can be represented as:

\[ E_{t_{i-1},t_{i} \sim p}[\sigma(t_{i-1}|t_{i})] - E_{t_{i+1},t_{i} \sim p}[\sigma(t_{i+1}|t_{i})]<\theta \\\lim_{i\to \infty} E_{t_{i-1},t_{i} \sim p}[\sigma(t_{i-1}|t_{i})]  < \alpha+\theta \]

As the conditional perplexity decreases, the similarity increases, showing a positive correlation with the conditional perplexity. Formally, this can be represented as:
\[
E_{t^1_{i+1}, t^2_{i+1}, t_i \sim \gamma}[\Omega(t^1_{i+1}, t^2_{i+1})] = 1 - kE_{t_{i+1}, t_{i} \sim p}[\sigma(t_{i+1}|t_{i})]
\]
This means that when the perplexity is low if you derive two paraphrases from a text, these two paraphrases will exhibit a high degree of similarity with each other. 
To simplify the deduction, while the conditional perplexity is low, we approximate this as:
\[ \forall t_{i+1}^1,t_{i+1}^2 \in M_p(*|t_{i}),\quad  \Omega(t^1_{i+1},t^2_{i+1})=1-k\alpha\]

We define all possible paraphrases of \(t_i\) as \(\hat{T}_{i+1}\) in the real world. The set \(T_{i+1}\) generated by LLMs should be a subset of \(\hat{T}_{i+1}\), containing elements that exhibit low conditional perplexity. Formally, we define:
\[
T_{i+1} = \{t_{i+1} \mid |\sigma(t_{i+1}, t_i) - \alpha| \le \epsilon, t_{i+1} \in \hat{T}_{i+1}\}
\]

While the number of step is high enough, Our goal is to prove that:
\[E_{t_{i-1}, t_{i+1} \sim p}[\Omega(t_{i-1}, t_{i+1})] > \Theta\]
where \(\Theta\) is sufficiently high, which means the \(T_{i+1}\) is very similar to \(T_{i-1}\).

It can be proven that:
\[E_{t_{i-1},t_{i+1} \sim p}[\Omega(t_{i-1},t_{i+1})] > (1-k\alpha)(1-\frac{\beta-\alpha}{\theta})\]
And we put the detailed proof in the Appendix \ref{}. It shows that while the reverse conditional perplexity is approaching the conditional perplexity, and the conditional perplexity is low, it will show the periodicity, the \(T_{i+1}\) is very similar to \(T_{i-1}\), which can properly explain our phenomenon.

%In this section, we will explain how the convergence causes the periodicity during successive paraphrasing.
%We define the similarity function as \(\Omega\), the condition perplexity function as \(\sigma\), and the distribution of successive paraphrasing as \(p\). The convergence of condition perplexity during SP can be represented as:
%\[
%E_{t_{i+1},t_{i} \sim p}[\sigma(t_{i+1}|t_{i})] < E_{t_{i},t_{i-1} \sim p}[\sigma(t_{i}|t_{i-1})],  \lim_{i \to \infty} E_{t_{i},t_{i-1} \sim p}[\sigma(t_{i}|t_{i-1})] =\alpha \\ 
%\]
%\[\sigma(t_{i+1},t_{i}) = \exp \left( -\frac{1}{T} \sum_{j=1}^{T} \log P(t_j \mid t_i, t_{i+1}^1, t_{i+1}^2, \ldots, t_{i+1}^{j-1}) \right)\]
%While the conditional perplexity decreases, the similarity also increases and shows a positive correlation with the conditional perplexity. Strictly, it can be represented as:
%\[E_{t^1_{i+1},t^2_{i+1},t_i\sim \gamma}[\Omega(t^1_{i+1},t^2_{i+1})]=1-kE_{t_{i+1},t_{i} \sim p}[\sigma(t_{i+1}|t_{i})\]
%This means while the perplexity is low if you derive 2 paraphrases from a text, the 2 paraphrases show a high similarity with each other.


\section{Generalization Analysis}

\subsection{Beyond Paraphrasing}

According to our deduction, periodicity should exist in tasks where the reverse conditional perplexity decreases as the number of steps increases. Intuitively, these tasks—where we assume \(t_i\) is derived from \(t_{i+1}\) and the concatenation of \(t_{i+1}\) and \(t_i\) is fluent—should exhibit periodicity.
To further support our theory, we designed several tasks and observed their periodicity. 
The tasks we designed are shown in Table \ref{table:other_taks}.

\begin{table*}[h!]
\centering
\begin{tabular}{|p{3cm}|p{3cm}|p{3cm}|p{3cm}|}
\hline
\textbf{Polish} & \textbf{Style Transfer} & \textbf{Formalize} & \textbf{Translation} \\ \hline
Please paraphrase the following text: & Transform the following formal text into an informal, conversational style; Rewrite the following informal text in a formal style & Rewrite the following informal text in a formal style & Please translate the following English to Chinese; Please translate the following English to Chinese;
\\ \hline
\end{tabular}
\label{table:other_taks}
\caption{Tasks}
\end{table*}

The results of these tasks are displayed in Appendix \ref{}. All of these tasks show the same periodicity, which strongly supports our deductions.



\subsection{Alternating Models and Prompts}


In this section, we extend our experiments to include variations in both the LLMs and the prompt settings.
During the \textbf{S.P.} process, for the LLM variation setting, we randomly select an LLM and apply a specific prompt. Conversely, for the prompt variation setting, we randomly choose a prompt and apply it to a specific LLM.
Despite the increased complexity in these settings, periodicity shown in figure \ref{} still remains evident.


Currently, there are several paraphrasing tools based on LLMs that primarily paraphrase the user's text with a fixed prompt. 
The periodicity observed in prompt varied \textbf{S.P.}, indicates the limited expressive range of these LLMs and illustrates how the widespread use of these paraphrasing tools can contribute to digital world homogenization.

Meanwhile, The periodicity observed in model variation \textbf{S.P.} suggests a homogenization within current LLMs. We've discussed that low perplexity is crucial for periodicity, and this experiment demonstrates that paraphrasing generated by one LLM still exhibits low perplexity on other LLMs. Given that LLMs generally produce low-perplexity text, this phenomenon implies that all LLMs share a similar output distribution in the \textbf{S.P.} task, implicitly pointing to the homogenization of LLMs.
This makes it extremely challenging to attribute AI-generated texts to the specific LLMs that produced them, and the homogenization of LLMs could further accelerate the homogenization of the digital environment.


% \subsection{Paragraph Paraphrase}


\subsection{Human Intervention}

