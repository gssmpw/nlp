\section{Successive Paraphrasing as System Function}

% \subsection{Task Definition}

% Successive paraphrasing involves repeatedly generating variations of a given text while maintaining semantic equivalence, where each iteration builds upon the previous output. 
% The ideal outcome of this process is to produce a diverse set of paraphrases that maintain the original meaning while exploring different linguistic structures, styles, and nuances. 
% Let $\mathcal{T}$ be a compact metric space representing all possible texts, $P: \mathcal{T} \rightarrow \mathcal{T}$ represents the LLM's mapping from a text to its paraphrase.
% Starting from an initial text $T_0 \in \mathcal{T}$, we define the sequence $\{ T_n \}_{n=0}^\infty$ recursively by:
% \begin{equation}
% T_{n+1} = P(T_n), \quad n = 0, 1, 2, \dots    
% \end{equation}
% The set $\mathcal{P}(T)$ denotes the complete text space for valid paraphrases of $T$, which is assumed as a finite space. 
% As the length of the paraphrasing sequence increases, the number of potential paraphrases, i.e., $|\mathcal{P}(T)|$, grows.
% Ideally, each iteration leverages the inherent knowledge embedded in in language models to introduce novel vocabulary, syntax, and phrasing in subsequent generations—thereby enhancing textual diversity.
% the language model draws upon its vast training data to produce varied outputs, which can lead to the discovery of unique perspectives and unexpected formulations.



In this section, we briefly introduce the theoretical framework of dynamical systems and applies it to understand the iterative process of successive paraphrasing. 
By viewing paraphrase generation as the repeated application of a transformation (the LLM’s paraphrasing function), we connect observed phenomena, e.g., periodicity and convergence, to well-studied concepts in systems theory. 
% This theoretical grounding will guide our interpretation of the empirical results in subsequent sections.

\subsection{Systems Theory Foundations}

Systems theory provides a broad mathematical and conceptual framework for analyzing how complex processes evolve over time~\cite{system1}. 
The core idea is modeling the state of a system and its evolution through deterministic or stochastic rules. 
In continuous or discrete time, systems can exhibit distinct behaviors, ranging from stable equilibria to oscillatory dynamics or even chaotic patterns.

A \textbf{dynamical system} is commonly defined as a set of states and a rule describing how those states vary under iteration. 
When a transformation repeatedly maps an initial state to a new state, one of several outcomes often emerges:
\textit{Fixed Points}: States that remain unchanged under the transformation, representing equilibrium;
\textit{Limit Cycles}: Closed loops of states that recur periodically, representing sustained oscillations;
\textit{More Complex Attractors}: Patterns to which the system’s trajectories converge, including chaotic attractors.

These attractors shape the long-term behavior of the system. If an initial state lies within the basin of attraction of a limit cycle, for example, the system will converge to that cycle regardless of small perturbations. Identifying such attractors offers valuable insights into the stability and variability of the system’s evolution.

\subsection{Framing Successive Paraphrasing as a Dynamical System}
Successive paraphrasing involves iteratively generating variations of a given text while maintaining semantic equivalence, where each iteration builds upon the previous output. 
We propose viewing successive paraphrasing as a discrete dynamical system. 
Let $\mathcal{T}$ be the space of all possible texts. 
Consider a large language model that defines a paraphrasing function: $P: T \rightarrow T$,
where $P(T)$ outputs a paraphrase of the input text $T$. 
Given an initial text $T_0 \in \mathcal{T}$, successive paraphrasing generates the sequence $\{ T_n \}_{n=0}^\infty$ recursively by:
\begin{equation}
T_{n+1} = P(T_n), \quad n = 0, 1, 2, \dots    
\end{equation}

The set $\mathcal{P}(T)$ denotes the complete text space for valid paraphrases of $T$, which is assumed as a finite space.
In theory, the space of potential paraphrases $\mathcal{P}(T)$ can be vast, especially as text length grows. 
Each new iteration can potentially explore fresh textual variations, e.g., new syntactic structures, vocabulary choices, and stylistic nuances, while maintaining semantic equivalence.
From a systems perspective, if the mapping $P$ is capable of diversifying output states, one might expect the generated text sequence to spread broadly through the space $\mathcal{P}(T)$, never stuck in repetitive patterns, resembling a system without stable attractors. 
In contrast, if the LLM’s internal biases lead to favouring certain textual forms, the sequence may enter a basin of attraction and converge onto a stable set of states. 
In other words, rather than exhibiting limitless variety, the system might find itself drawn to limit cycles, i.e., periodic attractors in the paraphrase space.



\begin{figure*}[t!]
    \centering
    \includegraphics[width=1\textwidth]{article/figures/periodicity.pdf}
    \caption{The difference confusion matrix for successive paraphrasing, where EN and ZH denotes English and Chinese sentence-level paraphrase generation accordingly. Both the x and y axes represent paraphrases at each step, and the value at the ($i$-th, $j$-th) grid position indicates the difference between the paraphrases at the $i$-th and $j$-th positions. 
    A darker color indicates a smaller difference value between two paraphrases. 
    The black arrow underlines the differences between \(T_i\) and \(T_{i-2}\), and averaging these values and subtracting the result from 1 gives our 2-period degree $\tau$.
    } 
    \label{figs:periodicity}
\end{figure*}



\section{Experiment Setup}
To systematically investigate this pattern, we first build dedicated testbeds and evaluation criteria.
\paragraph{Source Data Collection.}
We consider English and Chinese paraphrasing in this work.
For English paraphrase generation, we collect human-written source documents by sampling instances from the MAGE dataset~\cite{li2023mage}.
Specifically, we uniformly collect 1,000 sentences and 30 paragraphs from each domain in the dataset.
This results in a total of 1,000 sentences and 300 paragraphs for subsequent paraphrasing.
For Chinese, we source 200 sentences from WMT 2019 \citep{barrault-etal-2019-findings} and 200 sentences from Wikipedia \cite{wikidump}. 
Detailed data statistics is presented in Appendix~\ref{app:data_stat}.
The main experiments (Section~\ref{sec:main}) utilize sentence-level paraphrasing datasets, while analytic experiments employ paragraph-level datasets to demonstrate the generality of our findings (Section~\ref{sec:Analysis}).

% model setting
\paragraph{Paraphrase Generation.}
For English paraphrasing, we utilize Mistral-7B-Instruct-v0.3 ~\cite{jiang2023mistral7b}, Meta-Llama-3-8B-Instruct ~\cite{touvron2023llama2openfoundation}, Meta-Llama-3-70B-Instruct~\cite{touvron2023llama2openfoundation}, Qwen2.5-7B-Instruct, Qwen2.5-14B-Instruct, Qwen2.5-72B-Instruct~\cite{qwen2}, GPT-4o-mini and GPT-4o~\cite{openai2024gpt4technicalreport}.
For Chinese, we use  Qwen2.5-7B-Instruct, Qwen2.5-14B-Instruct, Qwen2.5-72B-Instruct, and GPT-4o-mini for paraphrase generation.
% inference hyper-parameter
By default, we set the temperature to 0.6 and p to 0.9 during the decoding process.
% TBD: steps? and beams?
We sample 10 different paraphrases at each step by setting the number of search beams to 10 and sequentially rephrasing each sample for 15 rounds. 
We select the candidate with the highest probability for the next paraphrasing iteration. 

% evaluation metrics
%For evaluation metrics, we employ the normalized edit distance function $d$ to measure the diversity between two paraphrases. 
%We define the costs of operations, including insertion, deletion, and replacement, as 1. 
%The formula for the \textbf{normalized edit distance} can be expressed as:
%\[
%d(A, B) = \frac{E(A, B)}{\max(|A|, |B|)}
%\]
%where \( E(A, B) \) represents the edit distance between strings \( A \) and \( B \), and \( |A| \) and \( |B| \) denote the lengths of strings \( A \) and \( B \), respectively.
\paragraph{Evaluation Metrics.}
We use the normalized Levenshtein edit distance function $d$ to quantify the textual differences between two paraphrases.
To provide a more intuitive of the attractor cycle, we propose a metric termed 2-periodicity degree to quantify and study the cyclic pattern in successive paraphrasing.
The 2-periodicity degree \(\tau\) is defined as $\tau = 1 - \frac{1}{M-2} \sum_{i=3}^{M} d(T_{i}, T_{i-2})$, which captures the average textual similarity between the current paraphrase and that from two steps prior. 
$M$ denotes the total number of paraphrasing iterations. 
A higher $\tau$ indicates stronger periodicity, i.e., similar between two paraphrases. 
For instance, if successive paraphrases exhibit perfect 2-periodicity such that \(d(T_{i}, T_{i-2}) = 0\), then \(\tau = 1\), indicating that the current paraphrase matches exactly with that from two steps earlier. 
To evaluate semantic equivalence, we employ cosine similarity on sentence embeddings~\footnote{https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2}~\cite{sentence_embed}.





%\begin{figure*}
%    \centering
%    \subfigure[\textcolor{blue}{\textbf{GPT-4o-mini}}]{\includegraphics[width=0.3\linewidth]{article/figures/periodicity/gpt4o-mini.pdf}}   
%    \subfigure[\textcolor{blue}{\textbf{Llama3-8B}}]{\includegraphics[width=0.3\linewidth]{article/figures/periodicity/llama3-8b.pdf}}   
%    \subfigure[\textcolor{red}{\textbf{GLM4-9B}}]{\includegraphics[width=0.3\linewidth]{article/figures/periodicity/glm4-zh.pdf}}   
%    \subfigure[\textcolor{blue}{\textbf{GPT-4o}}]{\includegraphics[width=0.3\linewidth]{article/figures/periodicity/gpt4o.pdf}}   
%    \subfigure[\textcolor{blue}{\textbf{Llama3-70B}}]{\includegraphics[width=0.3\linewidth]{article/figures/periodicity/llama3-70b.pdf}}   
%    \subfigure[\textcolor{red}{\textbf{GPT-4o-mini}}]{\includegraphics[width=0.3\linewidth]{article/figures/periodicity/gpt4o-mini-zh.pdf}} 
%    \caption{The diversity confusion matrix of successive paraphrasing. Figures (a), (b), (d), and (e) illustrate the diversity confusion matrix for English successive paraphrasing, whereas figures (c) and (f) display the matrix for Chinese.}  
%    \label{figs:periodicity}
%\end{figure*}

%\begin{table*}[!h]
%    \centering
%    \begin{tabular}{lccccc}
%        \toprule
%        \textbf{Model} &\textcolor{blue}{\textbf{GLM4-9B}} &\textcolor{blue}{\textbf{Llama3-8B}}& \textcolor{blue}{\textbf{Llama3-70B}} & \textcolor{red}{\textbf{GLM-9B}} \\
%        \midrule
%         & 0.25& 0.29 & 0.40 & 0.24 \\
%        \midrule
%        \textbf{Model} & \textcolor{blue}{\textbf{Mistral-7B}} & \textcolor{blue}{\textbf{GPT-4o-mini}} & \textcolor{blue}{\textbf{GPT-4o}} & \textcolor{red}{\textbf{GPT-4o-mini}} \\
%        \midrule
%          & 0.29 & 0.17 & 0.18& 0.12 \\
%        \bottomrule
%    \end{tabular}
    %\begin{tabular}{lccccc}
    %    \toprule
    %    Model & GLM-9B & Llama3-8B &Llama3-70B & \textcolor{red}{GLM-9B} \\
    %    \midrule
    %    Periodicity &0.2511 & 0.2839 &  0.4018 & 0.2418 \\
    %    \midrule
    %    Model & Mistral-7B & GPT-4o-mini & GPT-4o & \textcolor{red}{GPT-4o-mini} \\
    %    \midrule
    %    Periodicity &0.2897 & 0.1673 & 0.1847 & 0.1219 \\
    %    \bottomrule
    %\end{tabular}
%    \caption{The periodicity degree of different LLMs during successive paraphrasing. The models represented in blue indicate the periodicity degree on the English dataset, while those in red indicate it on the Chinese dataset.}
%    \label{table:periodicity}
%\end{table*}


%\begin{table*}[!h]
%    \centering
%    \begin{tabular}{l l l l l}
%        \toprule
%        \textbf{Experiments} & \textbf{4 Other Tasks} & \textbf{Model \& Prompt-Changing} & \textbf{Perturbation} & \textbf{Mitigation} \\ 
%        \midrule
%        \textbf{Iterative Number} & 10 & 15 & 10 & 15 \\ 
%        \textbf{Dataset} & Paragraph & Paragraph & Paragraph & Sentence Level \\ 
%        \textbf{Dataset} & gpt-4o-mini & models set 1 & gpt-4o-mini & en models(en) \\ 
%        \bottomrule
%    \end{tabular}
%    \caption{Summary of ANALYSIS Experiments, prompt-changing: gpt-4o-mini: model-changing: gpt-4o-mini, gpt-4o, glm4, llama3-8b}
%\end{table*}
\begin{table*}[t!]
    \centering
    \small
    \begin{tabular}{cccccc}
        \toprule
        \rowcolor{blue!20} \textbf{Mistral-7B} & \textbf{Llama3-8B} & \textbf{Llama3-70B} & \textbf{GPT-4o-mini} & \textbf{GPT-4o} & \textbf{Qwen2.5-7B} \\
        \midrule
        0.71 & 0.72 & 0.60 & 0.83 & 0.81 & 0.86 \\
        \midrule
        \cellcolor{blue!20} \textbf{Qwen2.5-14B} &\cellcolor{blue!20}  \textbf{Qwen2.5-72B} &  \cellcolor{red!20}  \textbf{Qwen2.5-7B} & \cellcolor{red!20}  \textbf{Qwen2.5-14B} &  \cellcolor{red!20} \textbf{Qwen2.5-72B} & \cellcolor{red!20} \textbf{ GPT-4o-mini} \\
        \midrule
        0.89 & 0.92 & 0.70 & 0.84 & 0.92 & 0.88 \\
        \bottomrule
    \end{tabular}
     \caption{The periodicity degree $\tau$ of different LLMs. The models represented in \colorbox{blue!20}{blue} denotes the English paraphrase generation, while\colorbox{red!20}{red} indicating Chinese paraphrasing.}
    \label{table:periodicity}
    \label{tab:model_comparison}
\end{table*}


\begin{figure*}[h]
    \centering
    \includegraphics[width=1\linewidth]{article/figures/ppl_div_mergev2.pdf}
    \caption{Convergence of perplexity, reverse perplexity, and generation diversity. The left and middle plots show that as the number of steps increases, both perplexity and reverse perplexity decrease steadily until they reach their lower bounds. The right plot shows that generation decreases as perplexity decreases.} 
    \label{figs:convergence}
\end{figure*}


\section{Results}
\label{sec:main}
%We start with our sentence-level dataset because most LLMs perform well on paraphrasing tasks at this level of granularity.
Building on the dynamical systems perspective introduced earlier, we now examine the empirical evidence that successive paraphrasing leads LLMs toward stable attractor cycles. 
We iteratively paraphrase sentences over 15 rounds within the sentence-level dataset and calculate the 2-periodicity degree.

\subsection{Periodicity}
\label{Periodicity}
We calculate the textual difference between $T_i$ and $T_{i-2}$ for paraphrases at each step. 
Arranging these differences into a confusion matrix (Figure~\ref{figs:periodicity}) reveals a pronounced 2-period cycle. 
For all LLMs, the matrix’s alternating light and dark patterns indicate that paraphrases generated at even iterations cluster together, and similarly, those at odd iterations form another cluster. 
This clear partitioning aligns with the behavior of a dynamical system converging onto a 2-period limit cycle—an attractor that draws the iterative process into a stable oscillation between two distinct states.

We also quantify this periodicity across different LLMs, as shown in Table~\ref{table:periodicity}. While all models exhibit some degree of 2-periodicity, Qwen2.5-72B shows a particularly strong and consistent cycle in both English and Chinese, whereas Llama3-70B displays relatively weaker periodic behavior. 
Models with higher periodicity tend to retain more semantic fidelity, suggesting that the recurring attractor states preserve core meaning even as they oscillate between two paraphrastic forms, as shown in Appendix~\ref{app:similarity}. 

While this periodicity can be viewed as an implicit repetition issue, it differs from explicit repetition of previously seen context. 
Instead, the model implicitly cycles through a limited set of paraphrastic forms without directly referencing prior iterations. 
In terms of systems theory, the model’s mapping function $P$ creates a dynamical environment in which the state space is not fully explored, with the trajectories settling into a 2-period attractor.


% Interestingly, even-step paraphrases are often closer to the original text than odd-step paraphrases, reinforcing the notion that the system alternates between two stable states—a hallmark of a limit cycle attractor.


% We measure the difference between paraphrases generated at different iterations from the same original sample during the process. 
% We then average the results across all samples to form an overall difference confusion matrix.
% Figure \ref{figs:periodicity} presents a selection of LLMs and their corresponding difference confusion matrices, highlighting the variation in paraphrases across successive steps during the paraphrasing process.
%Additional results are available in Appendix \ref{App:periodicity}. 

% We measure the semantic similarity between each paraphrase and its original text in our English setting. The results presented in Appendix \ref{figs:similarity} show that LLMs with higher periodicity tend to experience lower information loss. 
% Additionally, most LLMs exhibit 2-periodicity in semantic similarity.
% Notably, even-step paraphrases tend to be more similar to the original text compared to odd-step paraphrases.

% The periodicity of paraphrasing can also be viewed as a repetition problem, as seen in previous research. 
% However, unlike explicit text repetition with prior context, it represents an implicit repetition issue, where LLMs tend to reproduce the previous response without seeing the prior context.


%When we engage in successive paraphrasing, it raises the question of whether the subsequent paraphrases can resemble the original closely, or even mirror it in extreme cases. 
%To figure out this question, we calculate the BLEU score between paraphrases of different steps. 
%Specifically, a successive paraphrasing process can be represented as  $SP = s_0,s_1,s_2,\dots,s_n$, where $s_0$ represents the original text and the $s_i$ is derived from $s_{i-1}$ through paraphrasing.
%We then calculate the BLEU score between $s_i,s_j \in SP$ in a successive paraphrasing (SP) process to obtain the similarity confusion matrix. In our experimental setting, there are 800 original texts, which means 800 SP processes.
%After generating all the confusion matrices, we calculate the mean value to obtain the final similarity confusion matrix. 
%The result is shown in Figure 1.







%Figure 1 shows that for both English and Chinese settings, as the number of paraphrasing steps increases, all LLMs exhibit periodicity, which is reflected by the phenomenon that the $s_i$ is very similar to $s_{i-2}$.

%Figure 1 shows that in both English and Chinese settings, as the number of paraphrasing steps increases, all LLMs exhibit periodicity with a period of 2 in successive paraphrasing.
%This is reflected in the phenomenon where \( s_i \) is very similar to \( s_{i-2} \). 
%However, the different LLMs demonstrate varying degrees of periodicity in figures (a), (b), (c), and (d) for English, and figures (e) and (f) for Chinese. 
%To directly compare the degree of periodicity, we quantify it by measuring the distance between the actual and ideal similarity sequences.
%Ideally, \( s_i \) is equal to \( s_{i+2} \), making the similarity value between \( s_i \) and \( s_{i+2} \) equal to 1.0. 
%However, in reality, \( s_i \) is not always equal to \( s_{i+2} \). This discrepancy can be measured by subtracting the true similarity between \( s_i \) and \( s_{i+2} \) from 1.0.
%Let \( \kappa  \) be the similarity (BLEU) function. 
%We can evaluate the degree of periodicity by using the formula \( \sum_{i=3}^{n} \left(( 1.0 -\kappa(s_{i-2}, s_i))/{(n-2)} \right) \), where a lower value indicates a higher degree of periodicity, and the result is shown in table \ref{table:periodicity}.




\subsection{Convergence to Stable Attractor}
\label{sec:Convergence}

To probe the internal dynamics that lead to these attractor cycles, we explore generation determinism with successive paraphrasing unfolds.
We define a \textbf{conditioned perplexity} $\sigma(T_i \mid T_{i-1})$, reflecting the model’s confidence in generating $T_i$ given $T_{i-1}$, and a \textbf{reverse perplexity} $\hat{\sigma}(T_i \mid T_{i+1})$, indicating how easily $T_i$ could be reconstructed from $T_{i+1}$.

Figure~\ref{figs:convergence} demonstrates that as successive paraphrasing proceeds, both perplexity and reverse perplexity decrease. 
The forward direction (perplexity) quickly converges to a low boundary, while the reverse direction starts high, indicating that initially it is hard to ``go back'' from $T_{i+1}$ to $T_i$.
However, it drops fast as paraphrasing proceeds and aligns with the forward perplexity. 
Finally, the system evolves towards a state where generating $T_{i+1}$ from $T_i$ is nearly as deterministic and predictable as reconstructing $T_i$ from $T_{i+1}$.
This symmetry resembles a \textbf{stable attractor} in a dynamical system, where bidirectional predictability indicates that the system has ``locked in'' to a limit cycle.

We further quantify generation diversity by sampling multiple paraphrases at each iteration and computing the Vendi score~\cite{friedman2022vendi}. 
As shown in Figure~\ref{figs:convergence}, a low perplexity indicates a low generation diversity. 
A Vendi score of one indicates that all paraphrases in the beam are identical to each other.
%We quantify the generation diversity by applying the Vendi score~\cite{friedman2022vendi,pasarkar2023cousins} to each beam.
%The lower the Vendi score, the less diverse the generated paraphrases are, with a score of one indicating that all paraphrases in the beam are identical.
As both forward and reverse perplexity decreases, the model consistently produces similar paraphrases, leaving minimal room for alternative textual trajectories. 
From a systems viewpoint, the collapse into low perplexity and low diversity states corresponds to the model settling into the basin of attraction of a periodic orbit. 
Once inside the basin, the model’s generative behavior becomes nearly deterministic, causing the output sequence to cycle predictably.


The notion of invertibility, where each paraphrase can be treated as a paraphrase of its own paraphrase, further explains the robustness of periodicity. 
Invertibility places constraints on the mapping function $P$, effectively enabling a bidirectional relationship between states which encourages stable cycles. 
This insight suggests that tasks with similar invertible properties, e.g., translation, can also display limit cycle behavior, a hypothesis we will explore in Section~\ref{sec:beyond paraphrasing}.


% —just as a dynamical system settles into a limit cycle where each subsequent state is predetermined by the stable orbit.


%Inspired by previous research on repetitive text generation, we further investigate the inner feature of LLMs during successive paraphrasing.
%To simplify, we refer to all perplexity as conditioned perplexity and give it a function \(\sigma\).
%Thus, the perplexity of \(T_i\) conditioned on \(T_{i-1}\) can be represent as \(\sigma(T_{i}|T_{i-1})\).
%Meanwhile, we measure the perplexity of \(T_{i}\) generated by \(T_{i+1}\), and define it as reverse perplexity \(\hat{\sigma}(T_{i}|T_{i+1})\).
%To measure the generation diversity for each paraphrasing, we sample 10 sequences for each paraphrasing and utilize vendi score~\cite{pasarkar2023cousins,friedman2022vendi} to measure the diversity in the outputs.


%Additionally, we measure the perplexity of \(T_i\) generated by \(T_{i+1}\), defining it as reverse perplexity, represented by \(\hat{\sigma}(T_{i}|T_{i+1})\). 
%To assess the generation diversity of each paraphrasing, we sample 10 sequences for each paraphrase and use the Vendi score~\cite{pasarkar2023cousins,friedman2022vendi} to evaluate the diversity of the outputs.



% Inspired by previous research on repetitive text generation, we further investigate the internal features of LLMs during successive paraphrasing. To simplify, we refer to all perplexity measurements as conditioned perplexity and denote it by a function \(\sigma\). 
% Thus, the perplexity of \(T_i\) conditioned on \(T_{i-1}\) is represented as \(\sigma(T_{i}|T_{i-1})\).
% For each sample, we calculate the \(\sigma(T_{i}|T_{i-1})\) at each iteration, which results in the left plot of Figure \ref{figs:convergence}.
% Additionally, we measure the perplexity of \(T_i\) generated by \(T_{i+1}\), defining it as reverse perplexity, represented by \(\hat{\sigma}(T_{i}|T_{i+1})\). 
% Similar to perplexity, reverse perplexity reflects the probabilities of \(T_{i}\) generated by \(T_{i+1}\). 
% We apply reverse perplexity to each sample at each iteration, which results in the middle plot of Figure \ref{figs:convergence}.
% Furthermore,  we measure the generation diversity for each paraphrasing by sampling 10 paraphrases at each iteration and applying Vendi score~\cite{pasarkar2023cousins,friedman2022vendi} to assess the diversity in the outputs.
% Given a sample \(T_i\), we average the perplexity \(\sigma(T_{i+1}|T_{i})\) across the 10 paraphrases \(T_{i+1}\) and calculate the generation diversity of these 10 responses.
% The samples are then grouped into 20 buckets based on perplexity. For each bucket, we select the median perplexity and the average generation diversity to create the right-hand plot in Figure \ref{figs:convergence}.




%We pair $(T_{i-1}, T_i)$ for each same sample, calculate the average perplexity of $T_i$ conditioned on $T_{i-1}$, and visualize the evolution of perplexity across successive paraphrasing steps, shown in the left figure of Figure \ref{figs:convergence}. 
%Additionally, we measure the perplexity of \(T_i\) generated by \(T_{i+1}\), defining it as reverse perplexity, represented by \(\hat{\sigma}(T_{i}|T_{i+1})\).
%Given a text \( T_i \), we evaluate generation diversity following previous research~\cite{friedman2022vendi,pasarkar2023cousins}. 
%For each paraphrase, we compute the average perplexity of the next step and its corresponding generation diversity. The paraphrases are then grouped into 50 buckets based on perplexity. For each bucket, we select the median perplexity and the average generation diversity to create the right-hand plot in Figure \ref{figs:convergence}.

% As shown in Figure \ref{figs:convergence}, both perplexity and reverse perplexity decrease with the increasing number of paraphrase steps. Compared to reverse perplexity, the perplexity starts at a relatively low value and quickly converges to a lower boundary. In contrast, reverse perplexity begins at a much higher value, indicating the difficulty for LLMs to paraphrase \(T_1\) back to its original text. Over successive paraphrasing steps, reverse perplexity steadily decreases, eventually reaching a value close to that of perplexity.
% It reveals that as the paraphrasing progresses, the probabilities of paraphrasing \(T_i\) back to \(T_{i-1}\) continue to increase, eventually becoming comparable to the generation probability of the next step.
% Meanwhile, perplexity exhibits a positive correlation with generation diversity. 
% When the perplexity of paraphrases in the next step is low, the paraphrases tend to be more similar to each other.
% For models like Qwen2.5-72B, generation diversity can decrease to an extremely low level, representing a high similarity between next-step paraphrases.
% In fact, this reflects a limitation in the generation strategy of LLMs. Current autoregressive language models generate text token by token. 
% When the perplexity of a paraphrase approaches 1, the probabilities of each token converge towards 1, making it difficult for LLMs to sample alternative tokens simultaneously.
% This highlights the limited expressive capabilities of LLMs when the generation is in a low perplexity state. 
% The three characteristics mentioned above are referred to as convergence, and it is this convergence that leads to periodicity.

%In our opinion, the periodicity might be related to the self-enhancement nature of LLMs. 
%As the number of paraphrasing steps increases, LLMs become progressively more confident in their answers, which means their output distribution will become more focused. 
%To support this hypothesis, for an SP process, we utilize the LLMs to generate 10 paraphrases at each step and calculate the average condition perplexity of these paraphrases, which we refer to as sample space perplexity.
%We claim that a lower sample space perplexity value indicates a more centralized output distribution as explained in Appendix \ref{}.
%Figure \ref{figure:PPL}(a) illustrates that as the number of paraphrasing steps increases, the perplexity of the sample space decreases and ultimately converges to a constant value. 
%This phenomenon strongly supports our hypothesis. 



%While LLMs become more confident in their answers, their outputs will be more similar to each other.
%To measure output diversity, we employ the same sampling process used for sample space perplexity. 
%Instead of averaging conditional perplexity, we use the BLEU score to calculate the similarity between each paraphrase at each step. We then subtract the average similarity from 1.0 to represent the diversity of the outputs. 
%Figure \ref{figure:PPL}(b) shows that as the number of paraphrasing steps increases, the output diversity also decreases until it reaches a specific value.
%So, with the progress of the successive paraphrasing p rocess, the generation of LLMs becomes increasingly specific.


%In fact, lower sample space perplexity leads to a lower output diversity. 
%For all paraphrases, we can obtain their sample space perplexity and output diversity during the next-step paraphrasing.
%We divided these paraphrases into 50 buckets based on their sample space perplexity. 
%For each bucket, we calculated the average output diversity and the middle value of the sample space perplexity range.
%Figure \ref{figure:PPL}(c) illustrates that when the sample space perplexity is low, there is a positive correlation between sample space perplexity and output diversity.


%We believe these characteristics may be present in all large language models (LLMs). 
%However, these alone are insufficient to explain the periodicity of SP. 
%In fact, the inherent characteristics of SP itself also play a crucial role in its periodicity.
%If we paraphrase \(s_i\) as \(s_{i+1}\), it also makes sense to paraphrase \(s_{i+1}\) as \(s_i\).
%For LLMs, it means that the likelihood of \(s_{i}\) conditioned on \(s_{i+1}\) is not near zero.
%In an extreme case, let's assume \(s_{i+2}\) is the same as \(s_i\). 
%Therefore, while the perplexity of \(s_{i+2}\) conditioned on \(s_{i+1}\) converges after declining, the perplexity of \(s_i\) conditioned on \(s_{i+1}\) should exhibit the same trend.
%For this reason, we calculate the reverse conditional perplexity of SP at each step. 
%As shown in Figure \ref{figure:PPL}, the reverse conditional perplexity exhibits the same trend as the conditional perplexity, which strongly supports our guess.


%In our opinion, the three convergence characteristics mentioned above are the key factors contributing to the periodicity of SP. 
%We will explain how these convergence characteristics lead to periodicity in section \ref{Convergences2Periodicity}.



