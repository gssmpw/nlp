\section{Data Collection}
We conducted our experiment in both English and Chinese settings. 
We collected 800 English texts from arXiv, Quora, Wiki, and CNN news in equal proportions to form a original English text dataset.
For Chinese, We randomly selected 800 Chinese translations from WMT2019.
Both datasets were recursively paraphrased 15 times by the current widely used LLMs respectively. 
Additionally, we tested four different prompts to further explore the universality of our findings.
Table 1 shows the details of our experiment setting.
\begin{table*}[h!]
\centering
\begin{tabular}{lccccc}
\toprule
\textbf{Model} & \textbf{arXiv} & \textbf{Quora} & \textbf{Wiki} & \textbf{CNN} & \textbf{WMT2019} \\
\midrule
Original & 200 & 200 & 200 & 200 & 800 \\
chatgpt & 3000 & 3000 & 3000 & 3000 &- \\
llama3-8B-chat & 3000 & 3000 & 3000 & 3000 &- \\
Mistral & 3000 & 3000 & 3000 & 3000 &- \\
GLM4-chat & 3000 & 3000 & 3000 & 3000 & 12000 \\
Baichuan3-Turbo & 3000 & 3000 & 3000 & 3000 & 12000 \\
\bottomrule
\end{tabular}
\caption{Model and Text Number Information.The paraphrase text number is determined by multiplying the number of stages by the number of texts in each stage. }
\label{table:model_text_number_info}
\end{table*}
