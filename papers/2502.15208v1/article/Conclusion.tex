\section{Conclusion}

%summary
% In this work, we conducted a thorough investigation of successive paraphrasing.
% We found the phenomenon that during the generation process, LLMs frequently regenerate the same paraphrase rather than exploring new, diverse versions and tend to exhibit increased confidence in their responses, demonstrating the periodicity.
% This periodicity is driven by convergence, which also reflects the self-reinforcing nature of LLMs. 
% Interestingly, this phenomenon is not limited to paraphrasing but extends to other tasks as well.
% As shown in Section \ref{sec:Analysis}, this phenomenon can be alleviated, but it is difficult to completely eliminate. 
% The results of our perturbation experiments also reveal that LLMs tend to perform synonym replacement rather than altering the text structure. 
% Overall, our work provides a comprehensive analysis of successive paraphrasing, identifies the underlying periodicity, and highlights an implicit aspect of the self-reinforcing behavior in LLMs.


W reframed successive paraphrasing as a discrete dynamical system, offering a principled explanation for the emergence of stable periodic attractors in LLM-generated text. 
Our empirical findings revealed that instead of producing an expanding array of diverse paraphrases, LLMs rapidly settled into low-order limit cycles. 
These attractor states persisted even when we vary models, prompts, generation temperatures, and local kicak perturbations, indicating that they stem from a fundamental property of the system rather than superficial repetition or particular model idiosyncrasies.
Viewing iterative text generation through the lens of systems theory helps clarify why certain interventions fail to break these cycles and how others can weaken the attractor’s pull. 
Ultimately, recognizing and addressing these stable attractor cycles is crucial for unlocking more expressive and flexible language generation for large language models.
% In doing so, this work not only broadens our understanding of LLMs’ inherent limitations but also provides concrete directions for future efforts to enhance generative diversity. 
% 

\newpage
\section*{Limitations}

While this study provides valuable insights into successive paraphrasing, several limitations should be acknowledged. 
First, the paraphrasing is based on simple prompts, which may limit the generalizability of the findings to more complex or specific prompts. Second, although we have examined this phenomenon in the currently prevalent LLMs, other LLMs may not exhibit the same behavior. 
Finally, while we present the convergence of reverse perplexity in this work, the underlying reasons for this behavior still require further investigation.

\section*{Ethic Considerations}

We uphold the Code of Ethics and ensure that no private or non-public information is used in this work. We comply with the terms set by companies offering commercial LLM APIs and extend our gratitude to all collaborators for their invaluable support in utilizing these APIs.



%Addressing these limitations in future work could enhance the robustness.





%Our findings revealed limited expression of successive paraphrasing, and this constrained expressive capabilities of LLMs are closely associated with self-reinforcing nature of LLMs.


%We argue that this periodicity is indicative of the self-reinforcing nature of LLMs, distinguishing it from prior issues related to repetition.
%During the generation process, LLMs frequently regenerate the same paraphrase rather than exploring new, diverse versions and tend to exhibit increased confidence in their responses.

%Despite the widespread use of paraphrasing tools based on these models, we demonstrate their limitations in real-world usage. Our findings reveal that the constrained expressive capabilities of LLMs are closely associated with low perplexity. This raises a significant concern: a potential conflict between generation diversity and response quality. Specifically, if we aim for LLMs to excel in a series of factual tasks, their response diversity may be significantly restricted.
%Within the current theoretical framework, we believe that addressing this challenge is both important and valuable.
