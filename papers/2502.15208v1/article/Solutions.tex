
\subsection{Mitigating Generation Momentum}


% \section{Discussion}

% Understanding LLM Generation Momentum from the Lens of Successive Paraphrasing
% different models


% \textbf{Homogenization is the curse of LLMs.} 
% From the macro perspective, homogenization is lacking in the current statistic AI theory.
% In the current research community, the performance of an LLM is evaluated through a series of benchmarks.
% If one LLM outperforms another, it should achieve better results across these benchmarks, indicating that its responses are more accurate. However, focusing on improving response accuracy can reduce the richness and diversity of responses, leading to greater homogenization.
% Current LLMs aim to approximate real-world distributions based on a given corpus, consuming almost all available data on the internet. Given a specific corpus, we assume that there exists an optimal LLM within the current theoretical framework.
% From a micro perspective, the significant overlap in training datasets, similar training processes, model structures, and objectives will cause the LLMs to learn from similar distributions, leading to AI homogenization, which is the main contribution to the \textbf{DEH}.
% Furthermore, the limitations of the decoding strategy restrict the expressive capabilities of LLMs, intensifying the issue of \textbf{DEH}.

% \textbf{Relieve homogenization}


% \textbf{Relieve digital environment homogenization caused by S.P.} 
% do not fix a specific prompt, 
% adding noise to text
% adding system prompt


