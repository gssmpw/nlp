
\section{Related Work}
% Paraphrasing

\textbf{Paraphrase Generation.}
% enhance quality ...
Paraphrase generation has long been a significant focus in NLP research, with numerous studies dedicated to enhancing the quality of generated paraphrases.~\cite{li-etal-2018-paraphrase,roy-grangier-2019-unsupervised,lewis_pre-training_2020,lin-etal-2021-towards-document-level,hosking-etal-2022-hierarchical,xie-etal-2022-multi}.
Some studies also explore methods to control paraphrase generation by focusing on aspects such as syntactic and lexical diversity
~\cite{li-etal-2019-decomposable,goyal-durrett-2020-neural,huang_generating_2021,quality,dipper,yang-etal-2022-gcpg}.
% data augmentation...
Others investigate the application of paraphrase generation as a data augmentation technique to enhance model performance~\cite{jolly-etal-2020-data,bencke2024data,okur-etal-2022-data}.

% LLM-based paraphrase ...
Recently, advancements in LLMs have enabled LLM-based paraphrasing tools to generate stable, high-quality responses, making them widely used for refining materials like news articles, academic papers, and speeches~\cite{witteveen-andrews-2019-paraphrasing,roe2022automated,rani-etal-2023-factify}.
%Additionally, some research focuses on the societal impact of AI-paraphrased text.
However, their work primarily discusses single-step paraphrasing.
In contrast, another line of work involves LLMs iteratively rephrasing their own outputs over multiple iterations. 
\citet{can_ai} explores how repeated rephrasing can help evade AI text detectors, while \citet{ship} and \citet{huang_authorship_2024} discuss the implications for authorship after a document has undergone multiple rounds of paraphrasing.
Our research differs from those work. 
We investigate the inherent characteristics of paraphrasing when extended over multiple iterations.
% Following the trend of utilizing successive paraphrasing, we further investigate its characteristics and reveal the self-reinforcing nature of LLMs from a distinct perspective.
%In contrast to these works, we further investigate the characteristics of successive paraphrasing and reveal the self-reinforcing nature of LLMs from a distinct perspective.


\textbf{Self-Reinforcement in LLMs.}
%self-reinforce
Repetition, defined as the occurrence of repetitive text in natural language generation, has been widely explored in research community~\cite{Holtzman2020The,Welleck2020Neural,lin2021straightgradientlearninguse,see_get_2017,liu_text_2019,fu2020a}.
Several studies have observed repetition in text generation \cite{holtzman2020curiouscaseneuraltext,finlayson2023closingcuriouscaseneural} and proposed various sampling strategies to mitigate this issue. \citet{ivgi2025loopsoopsfallbackbehaviors} explores the connection between repetition and model size.
Additionally, \citet{xu_learning_2022} introduce the concept of self-reinforcement to elucidate this phenomenon, demonstrating that LLMs exhibit a tendency to repeat preceding sentences and reinforce this behavior during generation.
\citet{yan_understanding_2024} further explore the relationship between the self-reinforcement effect and the in-context learning capabilities of LLMs.
%While repetition investigated in these works is in single generation, Our research aligns with theirs but in multi-round generation in examining the self-reinforcement patterns of LLMs. 
While the repetition explored in these studies primarily focuses on single-round generation scenarios, our research reveals a similar phenomenon in multi-round generation. 
We specifically examine the self-reinforcement patterns of LLMs across successive paraphrasing tasks and concentrate on typical behaviors observed in them.
%mirroring LLMs' limitations in the exploration of text space.

% Compared to explicit repetition within a single generation, our work indicates the occurrence of implicit and unexpected repetition, where LLMs regenerate previous paraphrases without referencing them.



