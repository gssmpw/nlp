\section{Introduction}

Paraphrases are different expressions that carry the same meanings \citep{bhagat2013what}.
Paraphrasing tools are frequently used to enhance written material, especially by individuals who are not native speakers~\cite{motlagh_impact_nodate,roe_what_2022,kim_how_2024}.
Moreover, paraphrasing is crucial for data augmentation in situations with limited resources~\cite{okur_data_2022,sobrevilla_cabezudo_investigating_2024}.
The advancement of large language models (LLMs) ~\citep{touvron2023llama2openfoundation,alpaca,gpt3,gpt4} has enabled LLM-based paraphrasing to achieve comparable or even superior results to human performance. 
These models exhibit robust generalization capabilities across text lengths and domains.
\begin{figure}[t!]
    \centering
    \includegraphics[width=0.9\linewidth]{latex/article/figures/intro_v4.pdf}
    \caption{An illustration of successive paraphrasing using GPT-4o-mini is presented. Here, $T_0$ denotes the original human-written text, while $T_i$ indicates the i-th round of paraphrases. The nodes depicted in the lower section represent valid paraphrases for the input sentence, with distance reflecting textual variation. Successive paraphrases generated by LLMs are confined to alternating between two limited clusters, represented as blue and orange nodes.} 
    \label{figs:intro}
\end{figure}
% A few years ago, models like Parrots \cite{} and Dipper \cite{} primarily focused on sentence-level paraphrasing. However, when dealing with longer contexts, they needed to split the text into individual sentences, paraphrasing each one separately\cite{}. This approach often led to inconsistencies in their output. Moreover, in certain scenarios, their performance fell short of expectations\cite{}.
% Currently, with the advancement of large language models (LLMs) ~\citep{llama,gpt3,gpt4}, paraphrasing has entered a new era, with many paraphrase tools powered by LLMs emerging. The stability of paraphrase quality has greatly improved, and the scope of paraphrasing has expanded from the sentence level to the paragraph level \cite{}.

% Due to the enhanced stability of quality and the inherent nature of paraphrasing, it is intuitive to perform recursive paraphrasing (RP) on texts.
% \citeauthor{can_ai} investigates how RP can evade current AI text detection methods, while \citeauthor{ship} discusses the impact of RP on determining authorship. Compared to these works, we are more interested in the characteristics of RP itself.

Paraphrasing, however, can extend beyond a single transformation. 
A natural next step—successive paraphrasing—asks an LLM to iteratively rephrase its own outputs over multiple rounds~\cite{can_ai,ship}, as shown in Figure~\ref{figs:intro}.
Intuitively, this iterative setup should yield a growing tapestry of linguistic variety. 
With each iteration, the available textual space theoretically expands, promising an exploration of increasingly diverse phrasings.
As the sequence length increases, so does the textual space of potential paraphrases.
Analogous to how widening beam size expands a search space~\cite{Holtzman2020The,huang-etal-2023-affective,meister-etal-2023-locally}, successive paraphrasing can be viewed as a depth-first traversal that enriches textual diversity.




Yet, in practice, these expectations are not fully met. 
In this work, we leverage successive paraphrasing as a lens to examine a phenomenon we term \textbf{generation momentum}—\textit{the tendency of LLMs to self-reinforce and stabilize around particular outputs}. 
Despite the large theoretical space of valid paraphrases, empirical results demonstrate that LLMs repeatedly converge onto a limited set of solutions, as depicted in Figure~\ref{figs:intro}. 
Rather than spiraling outward into new linguistic territory, the iterative paraphrasing loop settles into a pattern of recurring outputs. 
Compared with explicit generation repetition~\cite{see_get_2017,liu_text_2019,fu2020a}, this effect is subtle: it may not manifest as obvious verbatim repetition, but it manifests as paraphrases that structurally and semantically recur in a cyclical pattern. 
Although earlier work has highlighted explicit forms of repetition and degeneracy in LLM outputs~\cite{xu_learning_2022,yan2024understanding}, what we find here is a more implicit and insidious form of repetition. 
By recursively paraphrasing their own paraphrases, LLMs reveal a hidden attractor—an oscillation between a small set of paraphrastic forms—exposing the generation momentum that constrains their creative space.



% In this work, we systematically study the behavior of LLMs in successive paraphrasing.
% Despite the theoretical potential for rich diversity, our empirical observations show that LLMs tend to reinforce their own prior outputs, leading to convergence and limited variation over successive iterations. 
% This observation is consistent with prior research indicating that LLMs have a tendency to produce explicitly repetitive content due to their self-reinforcing nature~\cite{xu_learning_2022,yan2024understanding}.
% %In contrast, we introduce the task of successive paraphrasing as a means to examine the generative momentum of large language models.
% In contrast, the phenomenon introduced here is the implicit repetition problem, which highlights the limited capabilities of LLMs in paraphrasing. 
% Understanding this behavior and mitigating its effects is crucial for tasks that demand high levels of linguistic diversity and creativity.

% data curation & exp


% To accomplish this, we first compile a diverse collection of human-authored texts across various writing tasks~\cite{li-etal-2024-mage}. We then use a set of open-source and commercial LLMs to iteratively paraphrase these texts over 15 successive rounds and quantify the text variation using normalized Levenshtein distance.
% % To quantify the variation between paraphrases, we apply the normalized Levenshtein distance. 
% Our empirical results reveal an increasingly evident cyclic pattern as paraphrasing progresses, with the generated paraphrases becoming more similar to those produced two iterations prior. 
% This 2-period cycle consistently appears across different models, text lengths, and tasks. 
% This unexpected finding challenges the intuition that longer texts (e.g., a 140-word paragraph) should theoretically allow for numerous valid paraphrases, far exceeding the mere two recurring options observed, as shown in Figure~\ref{figs:intro}.


To investigate this phenomenon, we first collect a diverse set of human-authored texts spanning multiple tasks and domains (Li et al., 2024). 
We then task a range of open-source and commercial LLMs with 15 rounds of successive paraphrasing and measure the textual variation using normalized Levenshtein distance. 
Surprisingly, the results show the emergence of a 2-period cycle, wherein each newly generated paraphrase closely resembles the one generated two steps earlier. This pattern persists across different models, text lengths, and tasks. The existence of such a low-order periodicity challenges our intuition: given the rich combinatorial possibilities of longer texts, one would expect a vast array of valid paraphrases rather than an iterative ping-pong between only two recurrent forms as shown in Figure~\ref{figs:intro}.


To further understand this cyclical convergence, we analyze the perplexity of LLMs as paraphrasing unfolds. We observe that with each iteration, the models become more certain about their next outputs, reinforcing prior paraphrases and narrowing the space of possibilities. This self-reinforcing loop—where outputs at one step guide the next step’s generation—creates a kind of gravitational pull. We call this self-sustaining pattern “generation momentum” because it describes how the paraphrasing process, once in motion, builds inertia that resists deviation from previously explored solutions. Under these conditions, the LLM’s “optimal” behavior is to stabilize around a small set of solutions, yielding the observed 2-periodicity.

We then probe the limits of this phenomenon through multiple angles. 
Introducing variability into the iterative process, such as alternating models or prompts, cannot mitigate periodicity. 
Adjusting temperature settings also influences the cycle, but simply increasing randomness does not suffice; overly high temperatures produce nonsensical outputs rather than unlocking sustained diversity. 
Moreover, the observed cyclical pattern extends beyond paraphrasing tasks. 
It appears in any invertible task—one where re-deriving a previous input from a given output is plausible—indicating that the revealed generation momentum is a more general characteristic of LLM behavior. 
% Additional experiments demonstrate that what variation does occur often focuses on swapping synonyms rather than altering deeper structural elements of the text (Section 5.5).
Finally, we propose a straightforward mitigation strategy that breaks this cycle while preserving semantic integrity.
% By addressing the root cause—LLMs’ self-reinforcing tendencies—such methods can restore the intended richness and variety of paraphrasing tasks.
In summary, this work uses successive paraphrasing as a proxy to illuminate the internal dynamics, or generation momentum, that shape LLM outputs. Although LLMs are lauded for their expansive capabilities, we find that when left to iterate on their own outputs, they gravitate towards periodic convergence rather than sustained exploration. Understanding this phenomenon and identifying ways to counteract it is crucial, not only for paraphrasing tasks but also for broader applications where genuine diversity and creativity are desired.










% Inspired by previous research on repetitive text generation~\cite{xu2022learningbreakloopanalyzing}, we investigate the perplexity of the LLMs during successive paraphrasing rounds.
% We observe that as the paraphrasing process continues, LLMs grow more confident when generating new paraphrases based on their previously generated outputs. 
% Concurrently, these models demonstrate heightened confidence in reproducing earlier paraphrases when conditioned on their current outputs.
% This self-reinforcing feedback loop leads to an increasing determinism in the LLMs' generations, thereby significantly constraining variation in paraphrasing.
% Given that paraphrasing inherently demands significant textual diversity, the model's optimal solution under this self-reinforcing behavior is to alternate between the two most likely paraphrases, thus resulting in the observed 2-periodicity.


%To further understand this phenomenon, We investigate the periodicity from many aspects.
%We introduce variability by randomly selecting LLMs or prompts at each step.
%The results show that introducing more variations during paraphrasing can alleviate the periodicity.
%Also, we measure the periodicity of successive paraphrasing in different temperature settings.
%However, raising the temperature alone does not completely prevent the periodicity; instead, it forces the model to produce nonsensical responses when the temperature is too high.
%Besides, This 2-periodicity phenomenon is not limited to paraphrasing, all tasks with an invertible nature, which ensures that the recurrence of the previous response in the next step is reasonable, could perform this in LLMs.
%Moreover, We confirm that rephrasing the paraphrases generated by LLMs always results in synonym substitution rather than altering the structure of the text, through a series of perturbation experiments in section \ref{sec:history}.
%Furthermore, this periodicity can also be changed manually by providing historical paraphrases.
%Finally, we provide an easy way to alleviate this periodicity, while keeping the meaning preserving well.

% To gain a deeper understanding of this phenomenon, we examine the periodicity from multiple perspectives. Specifically, we introduce variability by randomly selecting different LLMs or prompts at each step. The results demonstrate that increasing variability during paraphrasing can mitigate the observed periodicity. Additionally, we assess the periodicity of successive paraphrasing under varying temperature settings. However, merely increasing the temperature does not entirely eliminate periodicity; instead, excessively high temperatures lead the model to generate nonsensical responses. Furthermore, this 2-periodicity is not confined to paraphrasing tasks alone; it extends to any task with an invertible nature, which ensures that the recurrence of previous responses in subsequent steps is justifiable. Moreover, we establish that rephrasing paraphrases generated by LLMs predominantly results in synonym substitution, rather than altering the underlying structure of the text, as demonstrated through a series of perturbation experiments in Section \ref{sec:history}. Finally, we propose a simple approach to alleviate periodicity, while effectively preserving the meaning of the original text.

%Providing historical paraphrases can alter the period in paraphrasing. If we give one previous iteration and rephrase the current paraphrase, the next paraphrase should differ from both previous ones. In this case, the period becomes two, as shown in \ref{sec:history}.


%In Section \ref{sec:beyond paraphrasing}, we present four typical tasks—polishing, style transfer, translation, and clarification—each of which demonstrates the 2-periodicity phenomenon.

% Paraphrasing
%Rephrasing the paraphrases generated by LLMs always results in synonym substitution rather than altering the structure of the text.
%We independently apply three types of perturbations—synonym substitution, word swapping, and random insertion and deletion—throughout the entire generation process.
%The results indicate that synonym substitution has little impact on periodicity, compared to word swapping.
%Providing historical paraphrases can alter the period in paraphrasing. If we give one previous iteration and rephrase the current paraphrase, the next paraphrase should differ from both previous ones. In this case, the period becomes two, as shown in \ref{sec:history}.



%3-periodicity


% Generalization Analysis
%We further discuss this 2-periodicity in terms of text length, tasks, generation settings, perturbation, and finally the methods to mitigate this phenomenon.

%Further experiments demonstrate that the observed 2-periodicity is independent of text length. Even as text length increases, the 2-periodicity remains consistent with that of a single sentence.
%What's more, changing models or prompts have a slight impact on this phenomenon. In fact, our experiments show that these LLMs tend to generate similar paraphrases, as shown in figure\ref{figs:PPL_conditioned_Llama3}.
%Additionally, this phenomenon is prevalent in other invertible tasks, including polishing, translation, style transfer, and clarification, as detailed in Section \ref{sec:beyond paraphrasing}.
%Moreover, when LLMs become trapped in this cycle, increasing randomness has minimal impact on escaping it.
%Forcing it to generate more diverse paraphrases may result in nonsensical responses.
%This indicates a concentrated distribution when LLMs are caught in this state.
%Besides, after investigating three types of perturbations during paraphrasing—synonym replacement, word swapping, and random insertion and deletion, as described in \cite{wei-zou-2019-eda}—the results indicate that LLMs primarily perform synonym replacements for words or phrases, except for the initial paraphrasing.
%Expanding our discussion, we consider the impact of historical paraphrases and argue that providing a paraphrase from a previous step results in 3-periodicity in the current step, as confirmed by our experiments.
%Finally, we propose methods to mitigate this phenomenon based on our findings.
%----------------------------------------------------------



%----------------------------------------------------------









%Based on our analysis, we identify other tasks that exhibit the same periodicity as paraphrasing, including polishing, translation, style transfer, and clarification (see \ref{sec:beyond paraphrasing}). 

%We further explore the 2-periodicity phenomenon from multiple angles. 
%First, we demonstrate that text length has minimal impact on this phenomenon; even as text length increases, the 2-periodicity remains similar to that of a single sentence. 
%Next, we identify other tasks that exhibit the same periodicity as paraphrasing, including polishing, translation, style transfer, and clarification (see \ref{sec:beyond paraphrasing}). 
%In addition to text length and task influences, we find that increased randomness has little effect on this phenomenon, highlighting the sharp distribution of LLMs in the paraphrases. 
%We also investigate three types of perturbations during paraphrasing: synonym replacement, word swapping, and random insertion and deletion, as described in \cite{wei-zou-2019-eda}. The result indicates that LLMs primarily perform synonym replacements for words or phrases, except for the initial paraphrasing.
%Expanding our discussion, we consider the impact of historical paraphrases and discover that providing a paraphrase from a previous step results in 3-periodicity in the current step, as confirmed by our experiments.
%Finally, we propose methods to mitigate this phenomenon based on our findings.


%In summary, we discuss an interesting periodicity with the help of paraphrasing in LLMs.
%Unlike explicit text repetition in neural language generation, this periodicity reflects an implicit repetition, where LLMs regenerate previous paraphrases without it being provided as input.
%Meanwhile, this phenomenon is not limited to paraphrasing.
%It is the external manifestation of the self-reinforcing nature existing in LLMs.
%Compared to previous studies, our finding provides a novel perspective on this nature of LLMs, and reveals the limited capabilities of LLMs in generating diverse responses.


% In summary, we explore an intriguing phenomenon of paraphrasing in LLMs. During successive paraphrasing, LLMs often reproduce earlier paraphrases rather than generating novel variations. This tendency is not confined to paraphrasing alone but is prevalent across other invertible tasks. Our study identifies the root cause of this periodicity as the self-reinforcing nature of LLMs, raising concerns about their limited expressive capabilities.





% To further understand the reason behind this phenomenon, we analyze how conditional perplexity changes. 
% We find that as the number of paraphrasing steps increases, conditional perplexity and the diversity between sampled paraphrases decreases until they converges at a certain value, which we name convergence.
% We also observe a correlation between convergence and periodicity, and we believe that it is convergence leads to periodicity in recursive paraphrasing.
% Besides sufficient experiments, we give both intuitive and mathematical perspectives to support our opinion. Based on our findings, we expand our experiments to other tasks that we believe may exhibit similar characteristics to recursive paraphrasing. 
% The results from these tasks also strongly support our findings.

% findings

% extension


% What will happen if we recursively paraphrase a text multiple times? 
% Given a long text, there are many expressions that convey the same meaning. 
% While keeping the meaning unchanged, can LLMs explore all expressions during recursive paraphrasing, or do they simply remain unchanged as the number of paraphrase steps increases?
% To answer this question, We design a series of experiments and find an interesting phenomenon.
% When we conduct recursive paraphrasing, it appears that the paraphrases keep switching between two states. Specifically, the paraphrase at the current step is much more similar to the paraphrase from two steps prior, which we refer to as periodicity.




% It appears that LLMs have momentum when conducting recursive tasks, which leads them to become trapped in a suboptimal state reflected by convergence.
% To explore the stability of this phenomenon, we designed perturbation experiments that include alternating models and prompts, synonym replacement, as well as random swapping, insertion, and deletion during recursive paraphrasing. The results show that while alternating models and prompts still result in periodic behavior during recursive paraphrasing, random swapping, insertion, and deletion lead to periodicity degradation and loss of similarity. Synonym replacement, on the other hand, causes only slight periodicity degradation.
% Finally, we propose a method to mitigate generation momentum by sampling multiple paraphrases and selecting the one with the highest conditional perplexity.

% Humans continuously change, with each individual expressing themselves uniquely, shaped by their diverse life experiences. In comparison, LLMs exhibit limitations in maintaining diverse expressions while conveying the same meaning. Although there are various types of LLMs, they tend to produce similar expressions when performing recursive tasks.
% Consequently, we can more accurately determine whether the text was generated by LLMs if we provide the related question.
% We believe this represents a homogeneous problem, both in terms of generation and across different LLMs.
% However, does enhancing expressive diversity conflict with LLMs' ability to answer questions correctly?
% We consider this a valuable question that needs to be explored in the future.




















%Alternating Models and Prompts
%Human Intervention
%Mitigating Generation Momentum











%However, some research also poses that potential misuse of paraphrasing might lead to plagiarism and the spread of fake news. 
%Besides, these texts paraphrased by LLMs might suffer from hallucinations\cite{} and factual errors\cite{}.
%Although there has been a considerable amount of work focusing on paraphrasing, most of it only considers text that has been paraphrased once.
%In the real world, due to the widespread use of LLMs, any text you read on the internet might have been edited and paraphrased multiple times by various individuals for different reasons,    a process we refer to as successive paraphrasing.
%Based on our intuition and daily use of LLMs, we have noticed that successive paraphrasing exhibits some unique properties which raise our great interest.
