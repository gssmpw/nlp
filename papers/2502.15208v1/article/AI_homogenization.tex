
\section{AI homogenization Posed By Successive Paraphrasing with Diverse LLMs}
\label{SPwithDiverseModel}
In this section, we extend our experiments by incorporating a variety of large language models (LLMs). At each step, for every paraphrase, we randomly select an LLM to perform the paraphrasing, thereby generating the dataset for the next step.
In our view, current general LLMs are likely to follow a similar distribution due to the substantial overlap in their pre-training and instruction-tuning datasets, as well as their similar model structures.
Therefore, we anticipate that the periodicity we previously mentioned will also manifest in this task setting.

The results presented in Appendix \ref{} are consistent with our conjecture.
Although it is less obvious than the periodicity observed with single LLMs due to the noise introduced by using diverse LLMs, this phenomenon is still present.
This indicates that the text paraphrased by each LLM still maintains a low perplexity when evaluated against other LLMs.
The results reflect that current general LLMs indeed follow a similar distribution, which we refer to as AI homogenization.
The reason for model homogenization lies in the flaws inherent in current statistical AI theory.
We can summarize current statistical AI as the use of models to approximate the distribution of a given dataset.
The prevalent LLMs (such as Chatgpt, GLM4, and Llama3 etc.) have nearly exhausted all available corpus on the internet.
This implies that the training corpora of these powerful LLMs have significant overlap.
Additionally, most current general LLMs are based on transformers and share similar underlying structures, with the exception of recent models like RWKV and Mamba, which have not yet been widely adopted.
Moreover, these LLMs are trained in similar ways, using the same loss functions, training frameworks, and other methodologies.
All these factors contribute to model homogenization, which is a significant issue today. 
%This not only challenges the rights of the models but also accelerates corpus homogenization.
%In section \ref{}, we will comprehensively discuss the potential impact of AI homogenization.

AI homogenization raises serious philosophical and copyright issues concerning AI authorship.
Philosophically speaking, if two teams independently train two models that produce similar distributions and both models generate the same article, which model should be considered the author of that article?
Additionally, due to AI homogenization, texts generated by LLMs tend to be homogeneous, making it difficult to accurately determine which model generated them.
Besides the challenges and issues mentioned above, the homogeneous responses produced by current LLMs further accelerate the homogenization of the digital environment.





