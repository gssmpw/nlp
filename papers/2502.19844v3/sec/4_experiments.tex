\section{Experiments}
\label{sec: exp}


% ---------------------------------------------------- % 
%                     实验数据集的介绍
% ---------------------------------------------------- %
\noindent 
\textbf{Datasets.}
Following prompt tuning~\cite{CoOp, CoCoOp} and LLM-generated description works~\cite{DCLIP}, we evaluate our ProAPO on thirteen downstream tasks, including ImageNet-1K (IN-1K)~\cite{Imagenet}, Caltech101~\cite{caltech101} (object recognition), StandfordCars~\cite{Cars}, CUB200-2011~\cite{CUB} (bird classification), DTD~\cite{DTD} (Textures), EuroSAT (ESAT)~\cite{EuroSAT} (satellite images), FGVCAircraft~\cite{FGVC}, Flowers102~\cite{FLO}, Food101~\cite{Food101}, OxfordPets~\cite{oxford_pets}, Places365~\cite{Places365}, SUN397~\cite{SUN} (scene recognition), UCF101~\cite{UCF101} (human action).


% ---------------------------------------------------- % 
%                      核心实验表格
% ---------------------------------------------------- %
{
\begin{table*}[t]
  \centering
  \resizebox{0.86\linewidth}{!}
    {
    \begin{tabular}
        {l | c | ccccc ccccc ccc | c | c}
            
        \toprule
        \textbf{Module} & \textbf{TF} & \rotatebox{90}{\textbf{IN-1K}} & \rotatebox{90}{\textbf{Caltech}} & \rotatebox{90}{\textbf{Cars}} & \rotatebox{90}{\textbf{CUB}} & \rotatebox{90}{\textbf{DTD}}  & \rotatebox{90}{\textbf{ESAT}} & \rotatebox{90}{\textbf{FGVC}} & \rotatebox{90}{\textbf{FLO}} & \rotatebox{90}{\textbf{Food}}  &  \rotatebox{90}{\textbf{Pets}} & \rotatebox{90}{\textbf{Places}} & \rotatebox{90}{\textbf{SUN}} & \rotatebox{90}{\textbf{UCF}} & \rotatebox{90}{\textbf{Avg (11)}} & \rotatebox{90}{\textbf{Avg (13)}} \\
        \midrule

         \multicolumn{17}{c}{\textit{\textbf{\ccol{ResNet-50 Backbone}}}} \\
         \midrule
        CLIP (a photo of a \{\}) & \cmark & 57.9 & 84.5 & 53.9 & 44.7 & 38.8 & 28.6 & 15.9 & 60.2 & 74.0 & 83.2 & 38.2 & 58.0 & 56.9 & 55.6 & 53.4 \\ 

        % \rowcolor{gray!20} \multicolumn{16}{c}{\textbf{Fine-Tuning Approaches}}  \\
        \midrule

        \multicolumn{17}{l}{\textit{\textbf{prompt tuning methods}}} \\
        {CoOp}~\cite{CoOp} & \xmark & {57.2} & {87.5} & {55.6} & {-} & {44.4} & {50.6} & {9.6} & {68.1}  & {74.3} & {85.9} & {-} & {60.3} & {61.9} & {59.6} & {-} \\ 
        {PLOT}~\cite{PLOT} & \xmark & 59.5 &  \underline{89.8} & 56.6 & - & \underline{46.6} & \underline{54.1} & 17.9 & 71.7 & 77.7 & 87.5 & - & \underline{62.5} & 64.5  & 62.6 & - \\
        {ProGrad}~\cite{ProGrad} & \xmark & {57.8} & {88.7} & {\textbf{58.4}} & {-} & {46.1} & {\textbf{56.3}} & \underline{18.8} & {\underline{73.2}} & {76.0} & {\underline{88.4}} & {-} & {60.5} & \underline{65.6} & {62.7} & {-} \\
        
        \midrule

        \multicolumn{17}{l}{\textit{\textbf{automatic prompt optimization methods}}} \\
        
        PN~\cite{P_N} &  \cmark & \underline{59.6} & 89.1 & 56.2 & - & 44.8 & 49.0 & 18.1 & 67.2 & \underline{78.3} & 88.1 & - & 61.0 & 60.2 & 61.1 & - \\

        \highlight{\textbf{ProAPO} (ours)} & \highlight{\cmark} & \highlight{\textbf{61.5}} & \highlight{\textbf{90.3}} & \highlight{\underline{58.0}} & \highlight{\textbf{50.7}} & \highlight{\textbf{52.3}} & \highlight{51.7} & \highlight{\textbf{21.1}} & \highlight{\textbf{75.1}} & \highlight{\textbf{81.8}} & \highlight{\textbf{88.7}} & \highlight{\textbf{41.8}} & \highlight{\textbf{63.7}} & \highlight{\textbf{66.0}} & \highlight{\textbf{64.6}}  & \highlight{\textbf{61.8}} \\
        
        \midrule
        
        \multicolumn{17}{c}{\textit{\textbf{\ccol{ViT-B/32 Backbone}}}} \\
        
        \midrule

        CLIP (a photo of a \{\}) & \cmark & 62.1  & 91.2  & 60.4  & 51.7 & 42.9  & 43.9  & 20.2  & 66.0  & 83.2  & 86.8 & 39.9 & 62.1  & 60.9 & 61.8 & 59.3 \\ 

        \midrule

        \multicolumn{17}{l}{\textit{\textbf{hand-engineered methods}}} \\

        % CLIP-Hand~\cite{CLIP} & \cmark & 63.2 & 87.9 & 59.4 & 52.7 & 44.5 & 49.4 & 21.2 & 66.7 & 84.4 & 87.0 & 40.8 & 63.2 & 64.5 & 62.9 & 60.4 \\
        
        Template-80~\cite{CLIP} & \cmark & 63.5 & 91.6 & 60.4 & 51.2 & 42.8 & 52.6 & 19.5 & 66.1 & 84.2 & 87.4 & 41.6 & 63.5 & 62.9 & 63.1  & 60.6  \\ 
        FILIP-8~\cite{FILIP} & \cmark & 63.8 & 91.4 & 60.7 & 52.7 & 43.4 & 54.3 & 18.9 & 67.0 & 84.6 & 87.5 & 41.2 & 63.9 & 65.0 & 63.7  & 61.1 \\ 
        DEFILIP-6~\cite{DEFILIP} & \cmark & 62.5 & 91.0 & 59.9 & 51.1 & 41.3 & 46.4 & 18.8 & 66.5 & 84.3 & 87.5 & 40.2 & 62.3 & 63.6 & 62.2  & 59.6 \\      

        \midrule
        \multicolumn{17}{l}{\textit{\textbf{description-based methods}}} \\

        DCLIP~\cite{DCLIP} & \cmark & 63.3  & 92.7  & 59.4  & 52.7  & 44.1  & 38.4  & 19.4  & 66.1  & 83.9  & 88.1  & 41.2  & 65.0  & 65.8  & 62.4  & 60.0 \\
        Waffle~\cite{WaffleCLIP} & \cmark & 63.3 & 92.1 & 59.3 & 52.9 & 43.2 & 51.6 & 19.6 & 66.3 & \underline{84.9} & 87.7 & 41.5 & 65.0 & 64.5 & 63.4 & 60.9 \\ 
        CuPL~\cite{CuPL} &\cmark & \underline{64.4}  & 92.9  & 60.7  & 53.3  & \underline{50.6}  & 50.5  & 20.9  & 69.5  & 84.2  & 87.0  & \underline{43.1}  & \underline{66.3}  & 66.4  & 64.9  & 62.3 \\
        GPT4Vis~\cite{GPT4Vis} & \cmark & 63.5  & 93.1  & \underline{61.4}  & 52.7  & 48.5  & 47.0  & 21.4  & 69.8  & 84.3  & 88.1  & 42.7  & 64.2  & 65.7  & 64.3  & 61.7 \\ 
        AdaptCLIP~\cite{AdaptCLIP} & \cmark & 63.3  & 92.7  & 59.7  & 53.6  & 47.4  & 51.3  & 20.8  & 67.2  & 84.2  & 87.6  & 41.9  & 66.1  & 66.5  & 64.2  & 61.7 \\

        \midrule
        \highlight{\textbf{ProAPO} w/ DCLIP} & \highlight{\cmark} & \highlight{64.1} & \highlight{\underline{93.2}} & \highlight{60.6} & \highlight{\underline{53.6}} & \highlight{48.2} & \highlight{\underline{59.4}} & \highlight{\underline{22.6}} & \highlight{\underline{71.5}} & \highlight{84.2} & \highlight{\underline{88.7}} & \highlight{42.7} & \highlight{66.0} & \highlight{\underline{68.0}} & \highlight{\underline{66.0}}  & \highlight{\underline{63.3}} \\
        
        \highlight{\textbf{ProAPO} (ours)} & \highlight{\cmark} & \highlight{\textbf{64.7}} & \highlight{\textbf{94.4}} & \highlight{\textbf{61.7}} & \highlight{\textbf{55.4}} & \highlight{\textbf{53.5}} & \highlight{\textbf{63.0}} & \highlight{\textbf{23.0}} & \highlight{\textbf{74.3}} & \highlight{\textbf{85.3}} & \highlight{\textbf{91.0}} & \highlight{\textbf{43.3}} & \highlight{\textbf{66.6}} & \highlight{\textbf{69.0}} & \highlight{\textbf{67.9}}  & \highlight{\textbf{65.0}} \\

        % \midrule
        
        \bottomrule
    \end{tabular}
}
\vskip -6pt
  \caption{\textbf{Comparison of our ProAPO with SOTA textual prompt-based methods.} We report the top-1 accuracy (\%) on the test set. The best and second best results of the same backbone for each dataset are \textbf{bolded} and \underline{underlined}, respectively. \textbf{Avg (11)} and \textbf{Avg (13)} denote average results across 11 datasets (excluding CUB~\cite{CUB} and Places~\cite{Places365}) and all 13 datasets. \textbf{TF} denotes training-free approaches.}
  \label{tab: main_result}
  \vskip -0.15in
\end{table*}
}


% ---------------------------------------------------- % 
%                   补充细节的介绍
% ---------------------------------------------------- %
\noindent 
\textbf{Implementation details.}
In the default setting, we use template-80 pre-defined in CLIP~\cite{CLIP} as the template library and CuPL~\cite{CuPL} as the description library for searching the hyperparameters in a fixed language space. Dataset domain and synonym labels are generated by LLMs. This setting ensures a one-time query of LLMs, and no human intervention is required. If not explicitly stated, we set iteration times $T=4$ in both template and description optimization, generated number $M=N=8$, $\alpha = 1e3$, $n_{wst} = n_{sln} = \text{log}(|C|)$ for all datasets, where $|C|$ is the number of classes. All results are average with four seeds. Besides, our ProAPO is implemented in PyTorch and runs with an RTX 3090 GPU. More details are shown in Supp.4.

% Dataset domain and synonym labels are generated by LLMs.

% $T=8$ in template optimization and $T=4$ in description optimization

% Nvidia GeForce


% ---------------------------------------------------- % 
%                    4.1 核心实验
% ---------------------------------------------------- %
\subsection{Comparison with SOTA Methods}

% **************************** % 
%           对比的方法
% **************************** %
\noindent \textbf{Compared methods.}
We compare our results with state-of-the-art (SOTA) textual prompt-based methods, including vanilla CLIP with ``\texttt{a photo of a \{\}}'' template, prompt tuning methods~\cite{CoOp, PLOT, ProGrad}, hand-engineered methods (\textit{i.e.}, best templates released by~\cite{CLIP, FILIP, DEFILIP}), LLM-generated description methods~\cite{CuPL, DCLIP, WaffleCLIP, GPT4Vis, AdaptCLIP}, and automatic prompt optimization methods~\cite{P_N}. We test our ProAPO on two popular backbones (ResNet50~\cite{ResNet} and ViT-B/32~\cite{ViT}). Prompt tuning and automatic prompt optimization methods are evaluated under one-shot supervision.

% (\textit{i.e.}, fine-tuning prompts with continuous learnable tokens)

% **************************** % 
%             结果
% **************************** %
% \noindent 
\textbf{Results.}
In~\cref{tab: main_result}, we see that our ProAPO consistently outperforms previous prompt-based methods across diverse datasets on ResNet50 and ViT-B/32 backbones. Our optimized prompts improve vanilla CLIP by an average of 8.4\% (from 53.4\% to 61.8\%) on ResNet50 and 5.7\% (from 59.3\% to 65.0\%) on ViT-B/32. Moreover, our training-free ProAPO surpasses gradient-based prompt-tuning methods (CoOp, PLOT, ProGrad) by at least 1.9\% average accuracy in eleven datasets. It shows optimizing prompts in a natural language space is more effective in low-shot tasks. Compared to template-optimized methods (\textit{i.e.}, PN and hand-engineered methods), our ProAPO achieves remarkable performance on fine-grained datasets, \textit{e.g.}, CUB, FLO, ESAT, and UCF. This is because class-specific descriptions provide fine-grained discriminative details. Our ProAPO also outperforms description-based methods, even with DCLIP as initialization. We improve performances of DCLIP and CuPL by 3.3\% and 2.7\% on average across thirteen datasets. Notably, it improves DCLIP and CuPL by 5.4\% and 4.8\% on FLO and by 21\% and 12.5\% on ESAT, confirming that iterative optimization of class-specific prompts enhances fine-grained recognition.

% Notably, performance improvement of DCLIP and CuPL usually appears on fine-grained datasets, \textit{e.g.}, 5.4\% and 4.8\% increase on FLO, and 21\% and 12.5\% increase on ESAT.
% This confirms that iteratively optimizing class-specific prompts to remove ambiguous and retain discriminative ones can improve fine-grained performance.

% than continuous prompt-tuning methods 
% We also provide interpretable prediction results as stated in Section~\ref{sec: more_benefit}.

% **************************** % 
%       Critical Analysis
% **************************** %
% \noindent 
\textbf{Critical analysis.}
On datasets like ImageNet~\cite{Imagenet}, Caltech~\cite{caltech101}, Places~\cite{Places365}, and SUN~\cite{SUN}, performance gains are relatively small. We attribute this to two reasons: First, datasets like ImageNet and Caltech contain coarse-grained categories. The differences between categories have become clearer with LLM-generated descriptions, eliminating the need for our method. Second, fine-grained datasets, such as Places and SUN for scene recognition, are easily confusing between different images, limiting performance from only textual side optimization. In~\cref{sec: more_benefit}, we show that adapter-based methods with our optimized prompt improve performance again after addressing issues of image confusion. Moreover, we observe that the language search space impacts performance. Since CuPL has better descriptions than DCLIP, our ProAPO with CuPL performs well.

% compared to description methods

% We see that on some datasets, \textit{e.g.}, ImageNet~\cite{Imagenet}, Caltech~\cite{caltech101}, Places~\cite{Places365}, and SUN~\cite{SUN}, the performance gains are relatively smaller compared to description methods. 

% ---------------------------------------------------- % 
%               迁移到 Adapter-based 方法
% ---------------------------------------------------- %
{
\begin{figure}[t]
\centering
% \resizebox{0.9\linewidth}{!}
{
    \hfill
    \subfloat[Training-free methods.]{\includegraphics[width=0.23\textwidth]{Paper_Result/Average_Training-free_Results.pdf}}
    \hfill
    \subfloat[Fine-tuning methods.]{\includegraphics[width=0.23\textwidth]{Paper_Result/Average_Fine-Tuning_Results.pdf}}
}
\vspace{-7pt}
\caption{\textbf{Results of adapter-based methods with different initial prompts.} Solid and dotted lines denote prompt initialization with ProAPO and CuPL, respectively.
}
% \Description{}
\label{fig: improve_adapter_based}
\vspace{-5pt}
\end{figure}
}

% ---------------------------------------------------- % 
%           4.2 由 Optimal Prompts 带来的更多好处
% ---------------------------------------------------- %
\subsection{More Benefits by Optimal Prompts}
\label{sec: more_benefit}
% In this section, we show that our search prompts transfer effectively to adapter-based methods and different backbones.

% In this section, we show more benefits of our optimized prompts, which are highly transferable and generalized to adapter-based methods and different backbones.




% **************************** % 
%    迁移到 Adapter-based 方法
% **************************** %
\noindent 
\textbf{Transfer to adapter-based methods.}
In~\cref{fig: improve_adapter_based}, we show the results of popular adapter-based methods~\cite{Tip, Tip-X, APE, CLIP_Adapter} with different prompt initialization, \textit{i.e.}, SOTA method CuPL~\cite{CuPL} and our ProAPO. Adapter-based methods with ProAPO (solid lines) consistently surpass those with CuPL (dotted lines). It reveals that high-quality prompts make adapters perform better. Even in low shots, training with ProAPO achieves notable performance gains, which verifies its effectiveness. As the number of shots increases, adapters further improve results by enhancing image features.

% (including training-free and fine-tuning methods) 
% For adapter-based tuning, some lightweight modules are applied at the end of text and visual encoders to adjust features for downstream tasks. 

% ---------------------------------------------------- % 
%            迁移到不同 Backbones 的结果
% ---------------------------------------------------- %
{
\setlength{\tabcolsep}{4pt}
\begin{table}[t]
  \centering
  \resizebox{0.81\linewidth}{!}
    {
    \begin{tabular}
        {l | c | c | ccc }

        \toprule
         &  & {\textbf{Source}} & \multicolumn{3}{c}{\textbf{Target}} \\
        \cmidrule(lr){3-3} \cmidrule(lr){4-6}
        \textbf{Module} & \textbf{Shots} & RN50 & RN101 & ViT-B/32 & ViT-B/16  \\
        \midrule
        CLIP~\cite{CLIP} & 0 &  57.9 & 60.6 & 61.9 & 66.6  \\
        CoOp~\cite{CoOp} & 16 & \textbf{63.0} & 20.6 & 31.7 & 39.5 \\
        PN~\cite{P_N} & 1 & 59.9 & 60.7 & 62.2 & 67.0 \\
        % CuPL~\cite{CuPL} & 0 & 61.0 & 61.4 & 64.4 & 69.9  \\
       \highlight{\textbf{ProAPO}} & \highlight{1} & \highlight{61.5} & \highlight{\textbf{62.1}} & \highlight{\textbf{64.6}} & \highlight{\textbf{69.9}} \\
        \bottomrule
    \end{tabular}
}
\vspace{-4pt}
\caption{\textbf{Results of prompt transfer from ResNet-50 to other architectures.} We report the top-1 accuracy in ImageNet-1K.}
\vspace{-8pt}
\label{tab: transfer_backbone}
\end{table}
}


\begin{figure}[t]
\centering
\includegraphics[width=0.83\linewidth]{Paper_Result/transfer_to_different_backbones_avg.pdf}
\vspace{-8pt}
\caption{\textbf{Results of prompt transfer to different backbones.} The value denotes performance gains compared to vanilla VLMs. Our optimized prompts of ResNet50 and ViT-B/32 are reported.
}
\label{fig: transfer_to_backbones}
\vspace{-6pt}
\end{figure}


% **************************** % 
%       迁移到不同架构方法
% **************************** %
\noindent 
\textbf{Transfer to different backbones.}
In~\cref{tab: transfer_backbone}, we report accuracy in ImageNet across different backbones, with prompts optimized on a source backbone (ResNet50) adapted to target backbones (ResNet101, ViT-B/32, ViT-B/16).
We observe that CoOp~\cite{CoOp} obtains a significant drop in accuracy on target backbones while our ProAPO maintains performance. It verifies that discrete prompts searched in natural language spaces transfer better than continuous prompts.
ProAPO also outperforms PN, which reveals the effectiveness of class-specific optimization. Moreover, we achieve stable performance gains compared to CuPL~\cite{CuPL} from source to target models in~\cref{fig: transfer_to_backbones}. It further verifies that ProAPO transfers easily across different backbones.


% We further show that prompts searched by ProAPO transfer easily across different backbones.


% ---------------------------------------------------- % 
%                    性能提升分析
% ---------------------------------------------------- %

{
\begin{figure}[t]
\centering
% \resizebox{0.9\linewidth}{!}
{
    \hfill
    \subfloat[]{\includegraphics[height=0.16\textwidth]{Paper_Result/bar_analysis_performance_improve.pdf}}
    \hfill
    \subfloat[]{\includegraphics[height=0.16\textwidth]{Paper_Result/bar_improve_description_based_methods.pdf}}
}
\vspace{-10pt}
\caption{\textbf{Performance improvement analysis.} (a) Analysis of the effect of single vs. ensemble prompts. * denotes results evaluated in the test set. ATO is our automatic template optimization algorithm. (b) Results of previous description-based methods with prompt optimization by our ATO and ProAPO algorithms.}
% \Description{}
\label{fig: improve_description_methods}
\vspace{-5pt}
\end{figure}
}


\begin{figure}[t]
\centering
\includegraphics[width=1.0\linewidth]{Paper_Result/qualitative_results.pdf}
\vspace{-6pt}
\caption{\textbf{Qualitative analysis of class-specific prompt optimization by ProAPO.} Shaded \textbf{\textcolor{removed}{red}} and \textbf{\textcolor{retained}{blue}} words denote common and discriminative descriptions in two confused categories.
}
\label{fig: qualitative_result}
\vspace{-6pt}
\end{figure}


% ---------------------------------------------------- % 
%                   4.3 性能提升分析
% ---------------------------------------------------- %
\subsection{Performance Improvement Analysis}
In this section, we analyze the key reasons for the performance improvement of our ProAPO.

% **************************** % 
%    Single 和 Ensemble 的比较
% **************************** %
% \noindent 
\textbf{Prompt ensembling is better than a single prompt.}
Compared to PN~\cite{P_N}, we utilize prompt ensembling instead of a single prompt to optimize the template and description. To evaluate the effectiveness of prompt ensembling, we use Template-80~\cite{CLIP} as the template library and denote the prompts searched by the test set as the upper bound. As shown in~\cref{fig: improve_description_methods}(a), we observe that ensemble templates have a higher upper bound than the single template, consistent with prior work~\cite{FILIP, DEFILIP, DCLIP}. Similarly, we observe that our optimized templates achieve higher performance than PN~\cite{P_N}, even better than the best single template, further verifying the effectiveness of our method.

% conduct experiments on 13 datasets. We 

% even better than the best single template

% **************************** % 
% template 和 class-specific 的比较 (暂时先去掉咯, 在补充材料中完成)
% **************************** %
% \noindent 
% \textbf{Class-specific prompts provides more details than template.}



% **************************** % 
% Iterative Optimization 的作用
% **************************** %
% \noindent 
\textbf{Iterative optimization improves prompt quality.}
In \cref{fig: qualitative_result}, we show the changes in descriptions with our ProAPO. After iterative optimization, common descriptions such as ``\texttt{flight over ocean}'' and ``\texttt{long wings and hooked bill}'' are removed. Discriminative descriptions are also retained in candidate prompts, \textit{e.g.}, ``\texttt{white body}'' for Laysan Albatross, and ``\texttt{black underpants}'' for Sooty Albatross. As such, we see a notable improvement in description-based methods~\cite{DCLIP, AdaptCLIP, CuPL, GPT4Vis} with our ATO and ProAPO in~\cref{fig: improve_description_methods}(b) by at least 2.7\% average in thirteen datasets. It further verifies the effectiveness of our progressive optimization.

% Using descriptions in these methods as the class-specific library, we iteratively refine descriptions to find the optimal candidate prompt. Detailed results are shown in Supp.5.
 
% Iterative optimization, especially of class-specific prompts, is an urgent factor in our ProAPO.


% ---------------------------------------------------- % 
%                   GEN 和 EVO 算法的消融
% ---------------------------------------------------- %
{
% \setlength{\tabcolsep}{4pt}
\begin{table}[t]
  \centering
  \resizebox{0.9\linewidth}{!}
    {
    \begin{tabular}
        {l c c c c c | c c c }    
        \toprule
        \multicolumn{6}{c}{\textbf{Component}}  &   \\
        \cmidrule(lr){1-6} 
        & \texttt{Add} & \texttt{Del} & \texttt{Rep} & \texttt{Cross} & \texttt{Mut}  & \textbf{IN-1K} & {\textbf{Avg (11)}} & {\textbf{Avg (13)}} \\
        \midrule
       \multicolumn{5}{l}{CLIP (Baseline)} & & 62.1 & 61.8 & 59.3  \\ 
       \midrule
       \multicolumn{5}{l}{\textbf{\textit{edit-based generation}}} \\
       \texttt{a)} & \cmark & & & & & 63.8 & 66.0  & 63.3 \\ 
       \texttt{b)} & \cmark & \cmark & & & & 64.6 & 66.4  & 63.8 \\
       \texttt{c)} & \cmark &  & \cmark & & & 64.4 & 66.5  & 63.8 \\
       \texttt{d)} & \cmark & \cmark & \cmark & & & 64.6 & 66.7  & 64.0  \\ 
        \midrule
        \multicolumn{5}{l}{\textbf{\textit{evolution-based generation}}} \\
        \texttt{e)} & \cmark & \cmark & \cmark & \cmark & & 64.6 & 67.3  & 64.5 \\ 
        \texttt{f)} & \cmark & \cmark & \cmark & & \cmark &  64.7 & 67.1  & 64.3  \\ 
        \highlight{\texttt{g)}} & \highlight{\cmark} & \highlight{\cmark} & \highlight{\cmark} & \highlight{\cmark} & \highlight{\cmark} & \highlight{\textbf{64.7}} & \highlight{\textbf{67.9}}  & \highlight{\textbf{65.0}} \\
        \bottomrule
    \end{tabular}
}
\vspace{-6pt}
\caption{\textbf{Ablation of edit- and evolution-based operators.}}
\vspace{-5pt}
\label{tab: ablation_generate}
\end{table}
}


       % \texttt{c)} & & \cmark & \cmark & & & \\


% ---------------------------------------------------- % 
%                   4.4 Ablation Study
% ---------------------------------------------------- %
\subsection{Ablation Study}
\label{sec: ablation_study}


% **************************** % 
%      generate 操作的消融
% **************************** %
\noindent \textbf{Are both} \texttt{GEN} \textbf{and} \texttt{EVO} \textbf{algorithms necessary?}
In~\cref{tab: ablation_generate}, we ablate edit- and evolution-based operators. For edit-based operators, we observe that the model with add, delete, and replace operations achieves a higher result in row d). After introducing evolution-based operators, \textit{i.e.}, crossover operator to combine advantages of high-scoring candidates, and mutation operator to avoid locally optimal solutions, we see an increase in performance in rows e)-g). It confirms that evolution-based operators make the model search the optimal prompt faster with limited iterations.

% with the same setting

% This is because these operators can obtain all possible prompt candidates with enough iterations. 
% However, too many iterations consume more computation costs.

% To further explore whether each operator has a role in searching the optimal result, we show the number of each operator causing the new optimal candidate prompt during the iterations in Figure X.
% We see that each operator in iterative optimization may generate a better prompt.
% This further demonstrates each operator is useful in ProAPO.


% ---------------------------------------------------- % 
%                   对 Sampling 策略的消融
% ---------------------------------------------------- %
{
\renewcommand{\arraystretch}{1.02} 
\begin{table}[t]
  \centering
  \resizebox{0.88\linewidth}{!}
    {
    \begin{tabular}
        {l | c c c | c}  
        \toprule
        {\textbf{Module} (ViT-B/32)}  & \textbf{IN-1K} & {\textbf{Avg (11)}} & {\textbf{Avg (13)}} & \textbf{Times} \\
        \midrule
         
        \texttt{a)} w/o prompt sampling & 64.4 & 67.3 & 64.5 & 12 min \\
        \texttt{b)} w/o group sampling & \textbf{64.8} & \textbf{68.1} & \textbf{65.2} & \textbf{306 min} \\ 
        \texttt{c)} w/o sampling strategies & 64.5 & 67.2 & 64.4 & \underline{302 min} \\

        \midrule
        
        \highlight{\textbf{ProAPO} (full model)} & \highlight{\underline{64.7}} & \highlight{\underline{67.9}} & \highlight{\underline{65.0}} & \highlight{15 min} \\
        \bottomrule
    \end{tabular}
}
\vspace{-6pt}
  \caption{\textbf{Ablation of two sampling strategies.}}
\vspace{-8pt}
  \label{tab: ablation_sample}
\end{table}
}

% **************************** % 
%   Sample Strategies 的消融
% **************************** %
\noindent \textbf{Does sample strategies degrade performance?}
In~\cref{tab: ablation_sample}, we ablate two sampling strategies for description optimization. Without the prompt sampling, we see a slight decrease in times while results drop in row a). It verifies the effectiveness of the prompt sampling in finding a better initial point. Without the group sampling to select salient categories for optimization, we observe a notable increase in time costs (from 15 min to 300+ min, 20 times) yet similar results in row b) and the full model. It reveals that group sampling simultaneously improves performance and efficiency.

% We introduce a prompt sampling strategy to select partial descriptions from the original prompt library and a group sampling strategy to reduce traversed categories.
% We discuss whether these strategies lead to performance degradation.
% As shown in Figure X, the model with the prompt sampling strategy has better initialization accuracy.
% Thus, this model outperforms ones with full descriptions and without class-specific prompt initialization.
% In Figure X, we further show the performance as the number of iterations increases in different settings, including training with salient categories, randomly selected categories, and entire categories. We observe that the model with a group sampling strategy improves performance iteratively, while the other two settings show no significant change or decrease in performance. This demonstrates that selecting some salient categories for optimization does not lead to performance degradation and may even solve the issue of overfitting.

% **************************** % 
%     Score Function 的消融
% **************************** %
\noindent \textbf{Which score function is better?}
In~\cref{fig: score_result}, we compare the different score functions, \textit{i.e.}, accuracy (used in PN~\cite{P_N}) and our fitness score, to evaluate the quality of the candidate prompt. PCCs (Pearson Correlation Coefficients) are introduced to evaluate the linear relationship between training metrics and test performance. The high PCC values mean a strong correlation.
We see that the model with our fitness score achieves stable and high test results compared to previous score functions when achieving the best score. Besides, a higher PPC value further verifies that our fitness score effectively alleviates the overfitting problem.

% We see that the model with accuracy as a score function obtains multiple candidate prompts with the same training score while highly variable results in the test set.
% In contrast, our fitness score effectively alleviates overfitting and achieves higher PCC value.

% In the supplementary material, we consider the ablation of more metrics and demonstrate this simple combination is the optimal score function.



% ---------------------------------------------------- % 
%             Score Function 结果
% ---------------------------------------------------- %
{
\begin{figure}[t]
\centering
% \resizebox{0.9\linewidth}{!}
{
    \hfill
    \subfloat[Previous score function.]{\includegraphics[width=0.23\textwidth]{Paper_Result/score_result_top1.pdf}}
    \hfill
    \subfloat[Our fitness score]{\includegraphics[width=0.23\textwidth]{Paper_Result/score_result_fitness_score.pdf}}
}
\vspace{-7pt}
\caption{\textbf{Effect of different score functions}. Higher PCC values mean stronger correlations between training metrics and test results. Best Acc is the test result when achieving the best score.}
% \Description{}
\label{fig: score_result}
\vspace{-5pt}
\end{figure}
}

% PCCs indicate the linear relationship between training metrics and test performance, with 

% including (a) accuracy and (b) our fitness score

% ---------------------------------------------------- % 
%                   超参数影响结果图
% ---------------------------------------------------- %
{
\begin{figure}[t]
\centering
% \resizebox{0.9\linewidth}{!}
{
    \hfill
    \subfloat[Iteration times $T$.]{\includegraphics[width=0.154\textwidth]{Paper_Result/hyper_result_iterations.pdf}}
    \hfill
    \subfloat[Generated numbers]{\includegraphics[width=0.154\textwidth]{Paper_Result/hyper_result_generated_numbers.pdf}}
    \hfill
    \subfloat[Sampling groups $S$]{\includegraphics[width=0.154\textwidth]{Paper_Result/hyper_result_sample_groups.pdf}}
}
\vspace{-7pt}
\caption{\textbf{The effect of hyperparameter analysis}. The hyperparameter value set to 0 denotes the result of CuPL~\cite{CuPL}.
}
% \Description{}
\label{fig: hyper_result}
\vspace{-5pt}
\end{figure}
}

% ---------------------------------------------------- % 
%             4.5 Hyperparameter Sensitivity
% ---------------------------------------------------- %
\subsection{Hyperparameter Sensitivity}
\label{sec: hyper_ablation}

% **************************** % 
%       Epoch 轮数的消融
% **************************** %
\noindent \textbf{How many iterations of Alg.~\ref{alg: APO} to use?}
In~\cref{fig: hyper_result}(a), we show the effect of iteration times $T$ in \texttt{APO} algorithm. As the number of iterations increases, we see a consistent performance improvement compared to the baseline CuPL. It demonstrates that iterative optimization improves prompt quality. Stable results are achieved when $T \geq 4$.
% However, more iterations lead to a decrease in model performance,.
% Moreover, ProAPO performs better than ATO, which verifies that progressively optimizing is beneficial in searching for the optimal prompt.

% ATO and ProAPO improve performance as the number of iterations increases, and  

% **************************** % 
%        生成样本数的消融
% **************************** %
\noindent \textbf{How many generated prompts to use?}
In~\cref{fig: hyper_result}(b), we show the effect of generated numbers in \texttt{GEN} and \texttt{EVO} algorithm, where we set $M=N$. Similarly, progressive improvements are seen as generated numbers increase. We see a reliable result when $M = N \geq 8$.
It reveals the effectiveness of generating diverse candidates by our algorithm.

% We set $M$ and $N$ with the same value.
% in Alg.~\ref{alg: prompt_generate} 
% in Alg.~\ref{alg: prompt_evolution} 

% **************************** % 
%        Group 个数的消融
% **************************** %
\noindent \textbf{Are more sampling groups better?}
In~\cref{fig: hyper_result}(c), we show the effect of the number of salient groups $S$ in the group sampling strategy. We see a notable improvement when $S=2$. 
As the number of groups $S$ increases, it achieves stable results when $S \geq 8$. 
It verifies that optimizing several salient categories can achieve comparable performances with all categories and save iteration costs.



% **************************** % 
%        shot 个数的影响
% **************************** %
% \noindent \textbf{Are more shots better?}