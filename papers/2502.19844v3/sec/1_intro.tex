\section{Introduction}
\label{sec:intro}


% ---------------------------------------------------- % 
%            VLMs 背景 + 实现原理 + 挑战
% ---------------------------------------------------- %
In recent years, vision-language models (VLMs)~\cite{CLIP, Align, BLIP, Flamingo, SigLIP, SLIP, EVA-01, LLaVa_Grounding} pre-trained on large-scale paired image-text data, such as CLIP~\cite{CLIP}, have shown strong generalization on various image classification tasks. These models classify images by computing the similarity between the image and the prompt (a human-readable natural language) associated with a category. The prediction is the category with the highest similarity to the query image. Their performance highly relies on prompt quality, especially in fine-grained categories. Finding the optimal prompts for the downstream task becomes an urgent challenge~\cite{CoOp, CuPL, P_N}.


% ---------------------------------------------------- % 
%            引出找到最优文本表征的方法
% ---------------------------------------------------- %
Recent works have made efforts to improve prompt quality~\cite{CLIP, GPT3, prompt_influence_ACL_2021}.
\textbf{Manual prompt engineering}~\cite{CLIP, FILIP, DEFILIP} is a standard approach, writing several templates to include task-specific information, \textit{e.g.}, ``\texttt{a photo of a \{class\}, a type of bird.}'' for bird recognition. However, designing templates requires domain expertise, making it costly and challenging to scale~\cite{DCLIP, CuPL, P_N, CoOp, VDT_2023_ICCV}. Moreover, the template may lack details to recognize fine-grained categories as only the class name provides the distinct information in prompts~\cite{CoOp, VDT_2023_ICCV, CuPL}. 
\textbf{Prompt tuning} methods~\cite{CoOp, CoCoOp, PLOT, ProGrad} introduce a set of learnable tokens in the prompt to represent task-specific context, optimizing through gradient updates. While improving the performance of VLMs, they require additional training and lack interpretability~\cite{DCLIP, P_N, WaffleCLIP}. 
In contrast, \textbf{LLM-generated description} methods~\cite{DCLIP, CuPL, GPT4Vis, VDT_2023_ICCV, VDT_2023_NIPS_Hierarchical, AWT} leverage the implicit knowledge of large language models (LLMs)~\cite{GPT3, GPT4_Tech} to generate descriptions with class-specific details, enhancing generalization ability of VLMs. However, due to the hallucination in LLMs~\cite{Hallucination_in_LLM, Hallucination_in_LLM_EMNLP}, generated descriptions are suboptimal due to inaccurate, \textit{e.g.}, ``\texttt{feet}'' for the food Peking Duck, or lack discrimination for fine-grained recognition, \textit{e.g.}, the same descriptions ``\texttt{hooked bill}'' and ``\texttt{webbed feet}'' appear in Laysan Albatross and Sooty Albatross (see~\cref{fig: Problem}(a)), or exhibit non-visual descriptions, \textit{e.g.}, ``\texttt{strong smell}'' for Jackfruit. To this end, we propose the following key questions:

% making it costly and challenging to scale

% ---------------------------------------------------- % 
%                    Motivation 图
% ---------------------------------------------------- %
\begin{figure*}[t]
\centering
\includegraphics[width=0.85\linewidth]{Images/Problem.pdf}
\vspace{-7pt}
\caption{
\textbf{Issues of optimizing class-specific prompts.}
\textbf{(a)} Due to the hallucination in LLMs, generated descriptions may be inaccurate and lack discrimination between fine-grained categories (see \textbf{\textcolor{removed}{red words}}).
\textbf{(b)} Compared to task-specific templates, we see an explosion in the number of class-specific prompts (see \textbf{\textcolor{removed}{red rectangle}}). This leads to higher generation costs, iteration times, and the overfitting problem.
\textbf{(c)} Overfitting problem: Multiple candidate prompts have the same best training accuracy but variable and low test results (see \textbf{\textcolor{removed}{red circle}}).
}
\label{fig: Problem}
\vspace{-11pt}
\end{figure*}

% ---------------------------------------------------- % 
%                    引出要解决的问题
% ---------------------------------------------------- %
\textit{How can we find the optimal class-specific prompts that are visually discriminative for fine-grained categories with minimal supervision and no human intervention?}

% ---------------------------------------------------- % 
%          引出 class-specific 优化带来的问题
% ---------------------------------------------------- %
\noindent
Inspired by recent automatic prompt optimization (APO) methods~\cite{autoprompt, APE_NLP, GA_NLP, ProTeGi} in language tasks, we aim to optimize class-specific prompts by an evolution-based process, removing confused prompts while retaining discriminative ones. We consider an approximate zero-shot setting, \textit{i.e.}, one-shot classification, where minimal supervision is introduced to evaluate the prompt quality. However, previous APO methods focus solely on task-specific template optimization. In contrast, except for templates, a class-specific prompt also contains a description associated with visual details in a category, as shown in~\cref{fig: Problem}(b).
Compared with templates, class-specific optimization introduces new challenges due to the expanded search space (see~\cref{fig: Problem}(b)), which leads to:
(1) \textbf{High generation costs}. As the search space grows, it is costly and time-consuming to generate class-specific prompts exclusively with LLMs at each iteration.
(2) \textbf{Long iteration times}. Evaluating each candidate prompt in the search space may exponentially increase the iteration time, which is impractical in this scenario.
(3) \textbf{Overfitting problem}. As shown in~\cref{fig: Problem}(c), multiple candidate prompts achieve variable and low test results (from 63.6\% to 64.3\%) at the best training accuracy. 



% a class-specific description associated with visual details

% (a typical evaluation metric)

% Inspired by recent automatic prompt optimization methods~\cite{autoprompt, APE_NLP, GA_NLP, ProTeGi} in natural language processing

% Typical evaluation metric (\textit{i.e.}, accuracy) in previous methods~\cite{P_N, autoprompt, iCM} fails in this scenario.

% As shown in~\cref{fig: Problem}(b), a class-specific prompt typically contains a task-specific template, \textit{i.e.}, a template describes the dataset domain information, and a class-specific description, \textit{i.e.}, category name and its visual descriptions.

% We introduce minimal supervision, \textit{i.e.}, one-shot classification (almost nearly zero-shot setting) to evaluate the quality of prompts.

% ---------------------------------------------------- % 
%                      本文方法
% ---------------------------------------------------- %
To address these issues, we propose a \textbf{Pro}gressively \textbf{A}utomatic \textbf{P}rompt \textbf{O}ptimization (ProAPO) algorithm to iteratively optimize prompts from task-specific to class-specific levels. In each iteration, we use several edited-based (\textit{i.e.}, add, remove, and replace) and evolution-based operators (\textit{i.e.}, crossover, and mutation) to generate diverse candidate prompts from a prompt library. We query LLMs to generate this library in the initialization stage. Compared to querying LLMs at each iteration, these simple operations can effectively save generation costs. Then, we introduce a novel fitness score to evaluate generated candidate prompts and retain several top-scoring ones for the next iteration generation. An entropy constraint is added to the score to increase the soft prediction score and reduce overfitting. After several iterations, we return the best prompt for classification. To reduce iteration times in class-specific descriptions, we propose a prompt sampling strategy to find a better initial point in the search space and a group sampling strategy to explore a few salient classes instead of all for optimization. Our key contributions are: 
% ---------------------------------------------------- % 
%                      贡献
% ---------------------------------------------------- %
\begin{itemize}
    \item We propose an evolution-based algorithm to progressively optimize prompts from task-specific to class-specific levels with one-shot supervision and no human intervention. It solves issues of inaccuracy and lack of discrimination in LLM-generated descriptions.
    \item We address challenges in class-specific prompt optimization by an offline generation algorithm to reduce LLM querying costs, an entropy-constrained fitness score to prevent overfitting, and two sampling strategies to find an optimal initial point and reduce iteration times.
    \item Extensive experiments on thirteen datasets show that our proposed ProAPO consistently outperforms SOTA textual prompt-based methods and improves description-based methods in a challenging one-shot supervision. Moreover, our optimal prompts improve adapter-based methods and transfer effectively across different backbones.
\end{itemize}

% Experiments show that these strategies improve performance and efficiency. 

% Extensive experiments are conducted on thirteen datasets, and the results show that our proposed ProAPO outperforms existing prompt-based methods in a challenging one-shot classification setting. Moreover, we show that our optimal prompts improve adapter-based methods and transfer effectively across different backbones.

% We address challenges in class-specific prompt optimization by generating diverse candidate prompts offline to reduce LLM querying costs, introducing an entropy-constrained fitness score to reduce overfitting, and implementing two sampling strategies to find a better initial point and minimize iteration times. 

% We address challenges in class-specific prompt optimization, where several operations to generate diverse candidate prompts offline to reduce LLMs query costs, a entropy-constrainted fitness score to reduce overfitting, and two sampling strategies to find a better initial point and reduce iteration times.

% We consider challenges and propose solutions in class-specific prompt optimization, where several operations to generate diverse candidate prompts offline to reduce costs of querying LLMs, a novel fitness score with entropy constraints to reduce overfitting, and two sampling strategies to find a better initial point and reduce iteration times.

% time and


% For each iteration, we use several edited-based operators (\textit{i.e.}, add, remove, and replace) and evolution-based operators (\textit{i.e.}, crossover, and mutation) to generate diverse candidate prompts from a prompt library, which is obtained by one time LLM query. Compared to querying LLMs at each iteration, these simple operations can effectively save generation time and costs. 


% as shown in Fig. 2


% These simple operations can save generation time and costs effectively compared to querying LLMs at each iteration.
% to reduce the number of candidate prompts traversed


% To reduce iteration times in class-specific descriptions, we propose a prompt sampling strategy to find a better initial point in the search space. Moreover, a group sampling strategy is applied to explore a few salient categories instead of all for optimization. We empirically verify that this strategy can improve performance and save time. Our key contributions are as follows: 

% We empirically verify that these strategies can improve performance and save time. 