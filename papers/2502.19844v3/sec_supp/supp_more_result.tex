% ---------------------------------------------------- % 
%                不同 Backbones 的实验
% ---------------------------------------------------- %
{
\renewcommand{\arraystretch}{1.1} 
\setlength{\tabcolsep}{3.8pt}

\begin{table*}[htbp]
  \centering
  \resizebox{0.98\linewidth}{!}
    {
    \begin{tabular}
        {l | ccccc ccccc ccc | c | c}
            
        \toprule
        \textbf{Module} & \rotatebox{90}{\textbf{IN-1K}} & \rotatebox{90}{\textbf{Caltech}} & \rotatebox{90}{\textbf{Cars}} & \rotatebox{90}{\textbf{CUB}} & \rotatebox{90}{\textbf{DTD}}  & \rotatebox{90}{\textbf{ESAT}} & \rotatebox{90}{\textbf{FGVC}} & \rotatebox{90}{\textbf{FLO}} & \rotatebox{90}{\textbf{Food}}  &  \rotatebox{90}{\textbf{Pets}} & \rotatebox{90}{\textbf{Places}} & \rotatebox{90}{\textbf{SUN}} & \rotatebox{90}{\textbf{UCF}} & \rotatebox{90}{\textbf{Avg (11)}} & \rotatebox{90}{\textbf{Avg (13)}} \\
        \midrule

        CLIP~\cite{CLIP} - ResNet50 &  57.9 & 84.5 & 53.9 & 44.7 & 38.8 & 28.6 & 15.9 & 60.2 & 74.0 & 83.2 & 38.2 & 58.0 & 56.9 & 55.6 & 53.4 \\ 
        CuPL~\cite{CuPL} &   61.2 & 88.3 & 55.3 & 48.7 & 49.5  & 38.2  & 18.9  & 67.0  & 80.1& 86.1& 41.2& 63.1  & 63.3   & 61.1  & 58.5 \\
        {\textbf{ProAPO} (ours)} & \textbf{61.5}  & \textbf{90.3} & \textbf{58.0} & \textbf{50.7} & \textbf{52.3} & \textbf{51.7} & \textbf{21.1} & \textbf{75.1} & \textbf{81.8} & \textbf{88.7} & \textbf{41.8} & \textbf{63.7} & \textbf{66.0} & \textbf{64.6}  & \textbf{61.8} \\

        $\Delta$ & \textcolor{retained}{+ 3.6} & \textcolor{retained}{+ 5.8} & \textcolor{retained}{+ 4.1} & \textcolor{retained}{+ 6.0} & \textcolor{retained}{+ 13.5} & \textcolor{retained}{+ 23.1} & \textcolor{retained}{+ 5.2} & \textcolor{retained}{+ 14.9} & \textcolor{retained}{+ 7.8} & \textcolor{retained}{+ 5.5} & \textcolor{retained}{+ 3.6} & \textcolor{retained}{+ 5.7} & \textcolor{retained}{+ 9.1} & \textcolor{retained}{+ 9.0} & \textcolor{retained}{+ 8.4}   \\

        \midrule

        CLIP~\cite{CLIP} - ResNet101 & 61.4  & 89.9  & 63.3  & 49.6  & 40.3  & 31.7  & 18.3  & 64.3  & 83.4  & 86.9  & 37.9  & 59.0  & 61.2  & 60.0  & 57.5  \\ 
        CuPL~\cite{CuPL} &  61.4  & 91.0  & 61.2  & 45.3  & 49.7  & 28.7  & 18.6  & 59.0  & 82.7  & 86.6  & \textbf{40.6}  & 62.3  & 56.4  & 59.8  & 57.2  \\
        {\textbf{ProAPO} (ours)} & \textbf{63.6} & \textbf{92.3} & \textbf{64.4} & \textbf{52.2} & \textbf{51.6} & \textbf{45.9} & \textbf{21.2} & \textbf{69.6} & \textbf{84.9} & \textbf{89.6} & \textbf{40.6} & \textbf{63.5} & \textbf{64.0} & \textbf{64.6}  & \textbf{61.8} \\
        $\Delta$ & \textcolor{retained}{+ 2.2} & \textcolor{retained}{+ 2.4} & \textcolor{retained}{+ 1.1} & \textcolor{retained}{+ 2.6} & \textcolor{retained}{+ 11.3} & \textcolor{retained}{+ 14.2} & \textcolor{retained}{+ 2.9} & \textcolor{retained}{+ 5.3} & \textcolor{retained}{+ 1.5} & \textcolor{retained}{+ 2.7} & \textcolor{retained}{+ 2.7} & \textcolor{retained}{+ 4.5} & \textcolor{retained}{+ 2.8} & \textcolor{retained}{+ 4.6} & \textcolor{retained}{+ 4.3} \\


        \midrule

        CLIP~\cite{CLIP} - ViT-B/32 & 62.1  & 91.2  & 60.4  & 51.7 & 42.9  & 43.9  & 20.2  & 66.0  & 83.2  & 86.8 & 39.9 & 62.1  & 60.9 & 61.8 & 59.3  \\
        CuPL~\cite{CuPL} &  {64.4}  & 92.9  & 60.7  & 53.3  & {50.6}  & 50.5  & 20.9  & 69.5  & 84.2  & 87.0  & {43.1}  & {66.3}  & 66.4  & 64.9  & 62.3  \\
        {\textbf{ProAPO} (ours)} & {\textbf{64.7}} & {\textbf{94.4}} & {\textbf{61.7}} & {\textbf{55.4}} & {\textbf{53.5}} & {\textbf{63.0}} & {\textbf{23.0}} & {\textbf{74.3}} & {\textbf{85.3}} & {\textbf{91.0}} & {\textbf{43.3}} & {\textbf{66.6}} & {\textbf{69.0}} & {\textbf{67.9}}  & {\textbf{65.0}} \\

        $\Delta$ & \textcolor{retained}{+ 2.6} & \textcolor{retained}{+ 3.2} & \textcolor{retained}{+ 1.3} & \textcolor{retained}{+ 3.7} & \textcolor{retained}{+ 10.6} & \textcolor{retained}{+ 19.1} & \textcolor{retained}{+ 2.8} & \textcolor{retained}{+ 8.3} & \textcolor{retained}{+ 2.1} & \textcolor{retained}{+ 4.2} & \textcolor{retained}{+ 3.4} & \textcolor{retained}{+ 4.5} & \textcolor{retained}{+ 8.1} & \textcolor{retained}{+ 6.1} & \textcolor{retained}{+ 5.7} \\

        \midrule

        CLIP~\cite{CLIP} - ViT-B/16 & 66.9  & 93.2  & 65.5  & 55.3  & 44.3  & 51.0  & 24.4  & 70.6  & 88.4  & 89.0  & 40.8  & 62.5  & 67.7  & 65.8  & 63.0  \\
        CuPL~\cite{CuPL} & 69.6  & 94.3  & 66.1  & 57.2  & 53.8  & 55.7  & 26.6  & 73.9  & 88.9  & 91.2  & 43.4  & \textbf{69.0}  & 70.3  & 69.0  & 66.1  \\
        {\textbf{ProAPO} (ours)} & \textbf{69.9} & \textbf{95.2} & \textbf{67.7} & \textbf{59.0} & \textbf{55.8} & \textbf{65.3} & \textbf{28.3} & \textbf{82.7} & \textbf{89.5} & \textbf{92.7} & \textbf{43.8} & {68.9} & \textbf{73.1} & \textbf{71.7}  & \textbf{68.6} \\
        $\Delta$ & \textcolor{retained}{+ 3.0} & \textcolor{retained}{+ 2.0} & \textcolor{retained}{+ 2.2} & \textcolor{retained}{+ 3.7} & \textcolor{retained}{+ 11.5} & \textcolor{retained}{+ 14.3} & \textcolor{retained}{+ 3.9} & \textcolor{retained}{+ 12.1} & \textcolor{retained}{+ 1.1} & \textcolor{retained}{+ 3.7} & \textcolor{retained}{+ 3.0} & \textcolor{retained}{+ 6.4} & \textcolor{retained}{+ 5.4} & \textcolor{retained}{+ 5.9} & \textcolor{retained}{+ 5.6} \\


        \midrule 


        CLIP~\cite{CLIP} - ViT-L/14 &  73.5  & 95.1  & 76.8  & 62.5  & 52.1  & 61.5  & 33.4  & 79.5  & 93.1  & 93.3  & 40.7  & 67.6  & 75.0   & 72.8  & 69.5 \\ 
        CuPL~\cite{CuPL} & 76.7  & 96.2 & 77.6 &  61.4 & 62.6 & 62.4 & 36.1  & 79.7  & 93.4 &  93.8 & 43.8 &  73.2  & 78.3  & 75.5  & 71.9   \\
        {\textbf{ProAPO} (ours)} & \textbf{76.8} & \textbf{97.1} & \textbf{78.8} & \textbf{65.1} & \textbf{64.8} & \textbf{74.3} & \textbf{38.3} & \textbf{87.3} & \textbf{93.9} & \textbf{94.6} & \textbf{44.4} & \textbf{73.4} & \textbf{80.1} & \textbf{78.1}  & \textbf{74.5} \\
        $\Delta$ & \textcolor{retained}{+ 3.3} & \textcolor{retained}{+ 2.0} & \textcolor{retained}{+ 2.0} & \textcolor{retained}{+ 2.6} & \textcolor{retained}{+ 12.7} & \textcolor{retained}{+ 12.8} & \textcolor{retained}{+ 4.9} & \textcolor{retained}{+ 7.8} & \textcolor{retained}{+ 0.8} & \textcolor{retained}{+ 1.3} & \textcolor{retained}{+ 3.7} & \textcolor{retained}{+ 5.3} & \textcolor{retained}{+ 5.1} & \textcolor{retained}{+ 5.8} & \textcolor{retained}{+ 5.0} \\

        \midrule 

        OpenCLIP~\cite{OpenCLIP} - ViT-B/32 &  66.2  & 94.7  & 88.2  & 65.6  & 51.3  & 49.4  & 23.0  & 71.2 & 82.4 & 90.7 & 41.5 & 68.1 & 65.0  & 68.2  & 65.9  \\ 
        CuPL~\cite{CuPL} &  66.7  & 94.4  & 86.6  & 65.9  & 62.4  & 50.1  & 25.5  & 69.5  & 81.7  & 90.8  & 43.3  & 69.1  & 65.8  & 69.3  & 67.1   \\
        {\textbf{ProAPO} (ours)} & \textbf{67.0} & \textbf{95.8} & \textbf{88.7} & \textbf{67.3} & \textbf{65.1} & \textbf{66.0} & \textbf{27.5} & \textbf{81.8} & \textbf{83.2} & \textbf{91.9} & \textbf{43.4} & \textbf{69.7} & \textbf{70.2} & \textbf{73.3}  & \textbf{70.6} \\
        $\Delta$ & \textcolor{retained}{+ 0.8} & \textcolor{retained}{+ 1.1} & \textcolor{retained}{+ 0.5} & \textcolor{retained}{+ 1.7} & \textcolor{retained}{+ 13.8} & \textcolor{retained}{+ 16.6} & \textcolor{retained}{+ 4.5} & \textcolor{retained}{+ 10.6} & \textcolor{retained}{+ 0.8} & \textcolor{retained}{+ 1.2} & \textcolor{retained}{+ 1.9} & \textcolor{retained}{+ 1.6} & \textcolor{retained}{+ 5.2} & \textcolor{retained}{+ 5.1} & \textcolor{retained}{+ 4.7} \\


        \midrule 

        EVA02~\cite{EVA-02} - ViT-B/16 & 74.6  & \textbf{97.2}  & 79.2  & 60.8  & 49.7  & 68.0  & 24.6  & 75.6  & 89.5  & 92.2  & 42.9  & 70.7 & 68.6  & 71.8  & 68.7  \\ 
        CuPL~\cite{CuPL} &  75.4  & 96.7  & 79.2  & 61.8  & 59.1  & 61.7  & 27.5  & 75.2  & 89.3  & 92.1  & 44.0  & 72.5  & 71.9  & 72.8  & 69.7 \\
        {\textbf{ProAPO} (ours)} & \textbf{75.5} & 97.0 & \textbf{80.0} & \textbf{62.8} & \textbf{61.3} & \textbf{74.2} & \textbf{29.7} & \textbf{89.1} & \textbf{89.6} & \textbf{93.5} & \textbf{44.5} & \textbf{72.5} & \textbf{75.2} & \textbf{76.2}  & \textbf{72.7}  \\
        $\Delta$ & \textcolor{retained}{+ 0.9} & -0.2 & \textcolor{retained}{+ 0.8} & \textcolor{retained}{+ 2.0} & \textcolor{retained}{+ 11.6} & \textcolor{retained}{+ 6.2} & \textcolor{retained}{+ 5.1} & \textcolor{retained}{+ 13.5} & \textcolor{retained}{+ 0.1} & \textcolor{retained}{+ 1.3} & \textcolor{retained}{+ 1.6} & \textcolor{retained}{+ 1.8} & \textcolor{retained}{+ 6.6} & \textcolor{retained}{+ 4.4} & \textcolor{retained}{+ 4.0}  \\

        \midrule 

        SigLIP~\cite{SigLIP} - ViT-B/16 & 75.8  & 97.3  & 90.5  & 62.3 & 62.8  & 44.6 & 43.6 & 85.5 & 91.5  & 94.1  & 41.6  & 69.5  & 74.9  & 75.5  & 71.8 \\ 
        CuPL~\cite{CuPL} &  76.0 & 98.0 & 90.5 & 63.0 & 64.9 & 42.8 & 45.1 & 87.0 & 90.7 & 94.5 & 43.5 & 69.9 & 73.4 & 75.7 & 72.3 \\
        {\textbf{ProAPO} (ours)} & \textbf{76.4} & \textbf{98.3} & \textbf{91.7} & \textbf{66.2} & \textbf{69.1} & \textbf{55.8} & \textbf{47.1} & \textbf{93.3} & \textbf{92.2} & \textbf{94.9} & \textbf{44.3} & \textbf{71.7} & \textbf{75.9} & \textbf{78.8}  & \textbf{75.2} \\
        $\Delta$ & \textcolor{retained}{+ 0.6} & \textcolor{retained}{+ 1.0} & \textcolor{retained}{+ 1.2} & \textcolor{retained}{+ 3.9} & \textcolor{retained}{+ 6.3} & \textcolor{retained}{+ 11.2} & \textcolor{retained}{+ 3.5} & \textcolor{retained}{+ 7.8} & \textcolor{retained}{+ 0.7} & \textcolor{retained}{+ 0.8} & \textcolor{retained}{+ 2.7} & \textcolor{retained}{+ 2.2} & \textcolor{retained}{+ 1.0} & \textcolor{retained}{+ 3.3} & \textcolor{retained}{+ 3.4} \\
        
        \bottomrule
    \end{tabular}
}
\vskip -0.04in
  \caption{\textbf{Results of our ProAPO on different backbones.} \textbf{Avg (11)} and \textbf{Avg (13)} denote average results across 11 datasets (excluding CUB~\cite{CUB} and Places~\cite{Places365}) and all 13 datasets, respectively. $\Delta$ denotes performance gains compared to vanilla VLMs.}
  \label{supp_tab: results_different_backbones}
  \vskip -0.15in
\end{table*}
}

\section{Results on Different Backbones}
\label{sec_supp: different_backbone_result}

% **************************** % 
%           实验设置
% **************************** %
\textbf{Settings}. In~\cref{supp_tab: results_different_backbones}, we show results of our ProAPO in different backbones, including ResNet50, ResNet101, ViT-B/32, ViT-B/16, ViT-L/14 for CLIP~\cite{CLIP}, ViT-B/32 for OpenCLIP~\cite{OpenCLIP}, ViT-B/16 for EVA02~\cite{EVA-02}, and ViT-B/16 for SigLIP~\cite{SigLIP}. We compare our ProAPO with vanilla VLMs and the SOTA description method CuPL~\cite{CuPL}.


% **************************** % 
%             结果
% **************************** %
\textbf{Results}. We see that our ProAPO consistently improves vanilla CLIP and CuPL in thirteen datasets across all backbones. Compared to vanilla VLMs, our ProAPO enhances them by at least 3.4\% average accuracy in thirteen datasets. Moreover, we see notable performance improvement on several fine-grained datasets, such as DTD~\cite{DTD}, ESAT~\cite{EuroSAT}, FLO~\cite{FLO}, and UCF~\cite{UCF101}. It further verifies that class-specific descriptions provide helpful knowledge for fine-grained recognition. Besides, iterative optimization by our ProAPO also enhances the description method CuPL. 


% **************************** % 
%         更多有趣的发现
% **************************** %
\textbf{More interesting findings}.
We find that as the backbones of VLMs become larger, the performance improvement by ProAPO gradually decreases. For example, from ViT-B/32 to ViT-B/16 to ViT-L/14, the gain for CLIP is from 5.7\% to 5.6\% to 5.0\%. Moreover, similar results appear in different models with the same backbone, \textit{i.e.}, the vanilla model with better results achieves a lower performance increase. For example, from CLIP~\cite{CLIP} to OpenCLIP~\cite{OpenCLIP} on ViT-B/32 backbone, the gain is from 5.7\% to 4.7\%, and from CLIP~\cite{CLIP} to EVA02~\cite{EVA-02} to SigLIP~\cite{SigLIP}, the gain is from 5.6\% to 4.0\% to 3.4\%. We argue that the model with the higher result has more knowledge, which may be affected less by prompt quality. Overall, our ProAPO continues to improve the performance of VLMs.

% ---------------------------------------------------- % 
%                    更多比较的实验
% ---------------------------------------------------- %
\section{More Comparisons with SOTA Methods}
\label{supp_sec: more_comparison_with_sota_methods}
In this section, we compare our ProAPO with more SOTA prompt tuning methods. These methods adapt VLMs from both visual and textual views.

{
\renewcommand{\arraystretch}{1.1} 
\setlength{\tabcolsep}{3.8pt}

\begin{table*}[htbp]
  \centering
  \resizebox{0.75\linewidth}{!}
    {
    \begin{tabular}
        {l | ccccc ccccc c | c}
            
        \toprule
        \textbf{Module} (ViT-B/16) & \rotatebox{90}{\textbf{IN-1K}} & \rotatebox{90}{\textbf{Caltech}} & \rotatebox{90}{\textbf{Cars}} & \rotatebox{90}{\textbf{DTD}}  & \rotatebox{90}{\textbf{ESAT}} & \rotatebox{90}{\textbf{FGVC}} & \rotatebox{90}{\textbf{FLO}} & \rotatebox{90}{\textbf{Food}}  &  \rotatebox{90}{\textbf{Pets}} & \rotatebox{90}{\textbf{SUN}} & \rotatebox{90}{\textbf{UCF}} & \rotatebox{90}{\textbf{Avg (11)}} \\
        \midrule

        Vanilla CLIP~\cite{CLIP} & 66.9  & 93.2  & 65.5 & 44.3  & 51.0  & 24.4  & 70.6  & 88.4  & 89.0   & 62.5  & 67.7  & 65.8  \\

        \midrule
        \multicolumn{13}{c}{\textit{\textbf{\ccol{Test-Time Prompt Tuning Methods}}}} \\
        \midrule 

        TPT~\cite{TPT} & 69.0 & 94.2 & 66.9 & 47.8 & 42.4 & 24.8 & 69.0 & 84.7 & 87.8 & 65.5 &  68.0 & 65.5   \\ 
        DiffTPT~\cite{DiffTPT} & 70.3 & 92.5 & 67.0 & 47.0 & 43.1 & 25.6 & 70.1 & 87.2 &  88.2 & 65.7 & 68.2 & 65.9    \\ 
        PromptAlign~\cite{PromptAlign} & 71.4 & 94.0 & 68.5 & 47.2 & 47.9 & 24.8 & 72.4 & 86.7 &  90.8 & 67.5 & 69.5 & 67.3  \\
        Self-TPT-v~\cite{Self_TPT_v} & \textbf{73.0} & 94.7 &  68.8 & 49.4 & 51.9 & 27.6 & 71.8 &  85.4 & 91.3 & 68.2 &  69.5 & 68.3  \\
        
        \midrule 
        \multicolumn{13}{c}{\textit{\textbf{\ccol{Vector-based Prompt Tuning Methods}}}} \\
        \midrule 
        UPT~\cite{prompt_tuning_UPT} & 69.6 & 93.7 & 67.6 & 45.0 & 66.5 & 28.4 & 75.0 & 84.2 & 82.9 & 68.8 & 72.0 & 68.5   \\
        CoCoOp~\cite{CoCoOp} & 69.4 & 93.8 & 67.2 & 48.5 & 55.3 & 12.7 & 72.1 & 85.7 & 91.3 & 68.3 & 70.3 & 66.8  \\
        MaPLe~\cite{MaPLe}  & 69.6 & 92.6 & 66.6 & 52.1 & 71.8 & 26.7 & 83.3 & 80.5 &  89.1 & 64.8 & 71.8 & 69.9   \\
        ALIGN~\cite{prompt_tuning_align} & 69.8 & 94.0 & 68.3 & 54.1 & 53.2 & 29.6 & 81.3 & 85.3 & 91.4 & 69.1 & 74.4 & 70.1   \\ 
        PromptSRC~\cite{PromptSRC} & 68.1 & 93.7 & 69.4 & 56.2 & \underline{73.1} & 27.7 & \underline{85.9} & 84.9 & 92.0 & 69.7 & 74.8 & 72.3    \\ 
        
        \midrule 
        \multicolumn{13}{c}{\textit{\textbf{\ccol{Description-Based Methods}}}} \\
        \midrule 
        
        \multicolumn{13}{l}{\textit{\textbf{w/o adapters}}} \\
        % ----- 同 Test-Time Adaptation 的比较 ----- %
        CuPL~\cite{CuPL} & 69.6  & 94.3  & 66.1   & 53.8  & 55.7  & 26.6  & 73.9  & 88.9  & 91.2  & 69.0  & 70.3  & 69.0  \\

        AWT-text~\cite{AWT} & 68.9  & 95.2  & 66.0   & 52.0  & 52.6  & 26.1  & 74.5  & 89.4  & 91.2   & 68.4  & 69.8 & 68.6  \\
        
        \highlight{\textbf{ProAPO} (ours)} & \highlight{69.9} & \highlight{95.2} & \highlight{67.7} & \highlight{55.8} & \highlight{65.3} & \highlight{28.3} & \highlight{82.7} & \highlight{89.5} & \highlight{92.7} & \highlight{68.9} & \highlight{73.1} & \highlight{71.7}  \\

        \highlight{\textbf{ProAPO} w/ AWT-text} & \highlight{69.4} & \highlight{\underline{95.3}} & \highlight{67.8} &  \highlight{54.3} & \highlight{67.1} & \highlight{27.4} & \highlight{82.1} & \highlight{\underline{89.6}} & \highlight{\underline{93.2}} & \highlight{68.5} & \highlight{73.1} & \highlight{71.6} \\

        \midrule 

         \multicolumn{13}{l}{\textit{\textbf{w/ adapters}}} \\

        AWT-Adapter~\cite{AWT} & \underline{72.1} & 95.1 & \textbf{73.4} & \underline{59.4} & \textbf{76.3} & \textbf{33.9} & 85.6 &  85.9 & 92.9 & \textbf{72.7} & \textbf{78.4} & \underline{75.1} \\
        
        \highlight{\textbf{ProAPO} w/ APE~\cite{APE}} & \highlight{71.3} & \highlight{\textbf{95.8}} & \highlight{\underline{70.9}} & \highlight{\textbf{60.6}} & \highlight{72.4} & \highlight{\underline{33.2}} & \highlight{\textbf{91.4}} & \highlight{\textbf{89.9}} & \highlight{\textbf{93.4}} & \highlight{\underline{71.0}} & \highlight{\underline{77.6}}  & \highlight{\textbf{75.2}} \\

        \bottomrule
    \end{tabular}
}
\vskip -0.04in
  \caption{\textbf{Comparison of our ProAPO with more SOTA methods under one-shot supervision.} \textbf{Avg (11)} denote average results across 11 datasets. }
  \label{supp_tab: comparison_with_more_SOTA}
  % \vskip -0.15in
\end{table*}
}


% **************************** % 
%      同 TPT 方法的对比
% **************************** %
% \noindent 
\textbf{Comparison of test-time prompt tuning methods}.
In~\cref{supp_tab: comparison_with_more_SOTA}, our ProAPO outperforms SOTA test-time prompt tuning methods on 11 datasets. Notably, we adapt VLMs solely from the textual view, while TPT methods introduce textual and visual views (\textit{i.e.}, augmented images), which further verifies the effectiveness of our method.


% **************************** % 
%   同更多 vector-based prompt tuning 方法的对比
% **************************** %
\textbf{Comparison of vector-based prompt-tuning methods}
 Since recent prompt-tuning methods adapt VLMs using both visual and textual views, we combine ProAPO with an adapter (\textit{i.e.}, APE~\cite{APE}) for a fair comparison. 
 \textbf{(1) Higher performance in low-shot.} In~\cref{supp_tab: comparison_with_more_SOTA}, ProAPO consistently outperforms these methods, which verifies that optimizing prompts in natural language is more effective in low-shot tasks. 
 \textbf{(2) Better transferability and interpretability}. Unlike vector-based prompt-tuning methods that search in a continuous space, ProAPO benefits from the discrete nature of natural language, leading to better interpretability and easily transfers across different backbones (shown in~\cref{tab: transfer_backbone}).
 \textbf{(3) Lower performance in high-shot}.  However, in~\cref{supp_tab: shots_influence}, ProAPO shows a sub-optimal result compared to CoOp~\cite{CoOp} in high-shot settings. This is due to the limited language search space and iteration steps.

% **************************** % 
%   同 AWT 方法的比较 
% **************************** %
\textbf{Comparison of AWT~\cite{AWT}}.
First, since AWT uses augmented visual and textual views to adapt VLMs, we compare ProAPO with AWT under the augmented textual view for a fair comparison. In~\cref{supp_tab: comparison_with_more_SOTA}, the result shows our ProAPO improves AWT-text by 6.1\% on average, verifying that our progressive optimization improves prompt quality. In addition, we introduce a common adapter-based method to our ProAPO and compare it with AWT-Adapter in the one-shot setting. We see that our ProAPO achieves comparable results. These results suggest that ProAPO and AWT are complementary.



% **************************** % 
%   同 iCM 方法的比较 
% **************************** %
\textbf{Comparison of iCM~\cite{iCM}}. iCM is somewhat similar to ours, optimizing class-specific prompts with chat-based LLMs. However, it uses the whole validation set as supervision. In~\cref{supp_tab: comparison_of_iCM}, we see that our ProAPO outperforms iCM significantly even under the one-shot supervision. 
This is because our ProAPO address challenges in class-specific prompt optimization by an offline generation algorithm to reduce LLM querying costs, an entropy-constrained fitness score to prevent overfitting, and two sampling strategies to find an optimal initial point and reduce iteration times.

% we solve high generation costs, long iteration times, and overfitting in class-specific optimization.



{
% \vspace{-10pt}
% \renewcommand{\arraystretch}{1.0} 
\begin{table*}[h]
  \centering
  \resizebox{0.8\linewidth}{!}
    {
    \begin{tabular}
        {l | ccccc ccc | c}  
        \toprule
        
        \textbf{Module} (ViT-B/32) & {\textbf{IN-1K}} & {\textbf{Caltech}} & {\textbf{CUB}} & {\textbf{DTD}}  & {\textbf{ESAT}} & {\textbf{FLO}} & {\textbf{SUN}} & {\textbf{UCF}} & \textbf{Avg (8)} \\
        \midrule
        Vanilla CLIP & 62.1 & 91.2 & 51.7 & 42.9 & 43.9 &  66.0 & 62.1 & 60.9 & 60.1 \\
        \midrule
        \multicolumn{10}{c}{\textit{\textbf{\ccol{Automatic Prompt Optimization Methods}}}} \\
        \midrule
        
        iCM~\cite{iCM} (w/ validation set) & 64.5 & 92.7 & \textbf{56.1} & 51.4 & 56.3 & 72.2 & 66.2 & 67.0 & 65.8  \\
        \highlight{\textbf{ProAPO} (w/ 1-shot)} &  \highlight{\textbf{64.7}} & \highlight{\textbf{94.4}} & \highlight{55.4} & \highlight{\textbf{53.5}} & \highlight{\textbf{63.0}} & \highlight{\textbf{74.3}} & \highlight{\textbf{66.6}} & \highlight{\textbf{69.0}} & \highlight{\textbf{67.6}} \\

        \bottomrule
    \end{tabular}
}
% \vspace{-5pt}
  \caption{\textbf{Comparison of our ProAPO with iCM~\cite{iCM}.} Avg (8) denotes average results across 8 datasets.}
  \label{supp_tab: comparison_of_iCM}
\end{table*}
}



% ---------------------------------------------------- % 
%           对 Template 和 Description 优化的消融
% ---------------------------------------------------- %
{
\renewcommand{\arraystretch}{1.1} 
% \setlength{\tabcolsep}{4pt}

\begin{table*}[htbp]
  \centering
  \resizebox{0.99\linewidth}{!}
    {
    \begin{tabular}
        {l | lllll lllll lll | l | l}

            
        \toprule
        \textbf{Module} (ResNet50) & \rotatebox{90}{\textbf{IN-1K}} & \rotatebox{90}{\textbf{Caltech}} & \rotatebox{90}{\textbf{Cars}} & \rotatebox{90}{\textbf{CUB}} & \rotatebox{90}{\textbf{DTD}}  & \rotatebox{90}{\textbf{ESAT}} & \rotatebox{90}{\textbf{FGVC}} & \rotatebox{90}{\textbf{FLO}} & \rotatebox{90}{\textbf{Food}}  &  \rotatebox{90}{\textbf{Pets}} & \rotatebox{90}{\textbf{Places}} & \rotatebox{90}{\textbf{SUN}} & \rotatebox{90}{\textbf{UCF}} & \rotatebox{90}{\textbf{Avg (11)}} & \rotatebox{90}{\textbf{Avg (13)}} \\
        \midrule

        % \multicolumn{16}{c}{\textit{\textbf{\ccol{ViT-B/32 Backbone}}}} \\
        % \midrule

        Vanilla CLIP & 57.9 & 84.5 & 53.9 & 44.7 & 38.8 & 28.6 & 15.9 & 60.2 & 74.0 & 83.2 & 38.2 & 58.0 & 56.9 & 55.6 & 53.4 \\ 
        
        \midrule
        \multicolumn{16}{c}{\textit{\textbf{\ccol{Template Optimization Methods}}}} \\

        \midrule 
        PN~\cite{P_N} & 59.6 & 89.1 & 56.2 & - & 44.8 & \underline{49.0} & 18.1 & 67.2 & 78.3 & 88.1 & - & 61.0 & 60.2 & 61.1 & -  \\

        \highlight{\textbf{ATO} (w/o dataset domain)} & \highlight{60.4} & \highlight{88.9} & \highlight{56.8} & \highlight{47.0} & \highlight{45.0} & \highlight{43.7} & \highlight{17.9} & \highlight{67.4} & \highlight{79.9} & \highlight{87.8} & \highlight{40.0} & \highlight{61.2} & \highlight{61.5} & \highlight{61.0} & \highlight{58.3} \\
        
        \highlight{\textbf{ATO}} & \highlight{\underline{61.3}} & \highlight{89.4} & \highlight{57.4} & \highlight{49.2} &  \highlight{45.4} & \highlight{46.4} & \highlight{18.4} & \highlight{68.1} & \highlight{80.5} & \highlight{88.5} & \highlight{40.2} &  \highlight{61.8} & \highlight{63.9} & \highlight{61.9}  & \highlight{59.3} \\

        \midrule

        \multicolumn{16}{c}{\textit{\textbf{\ccol{Description Optimization Methods}}}} \\

        \midrule
        \highlight{\textbf{ProAPO} (w/o synonyms)} & \highlight{\textbf{61.5}} & \highlight{\underline{89.7}} & \highlight{\textbf{58.3}} & \highlight{\underline{49.7}} & \highlight{\underline{46.6}} & \highlight{46.8} & \highlight{\underline{20.5}} & \highlight{\underline{74.6}} & \highlight{\underline{81.0}} & \highlight{\textbf{88.8}} & \highlight{\underline{40.9}} & \highlight{\underline{62.3}} & \highlight{\underline{64.8}} & \highlight{\underline{63.2}} & \highlight{\underline{60.4}} \\        
        
        \highlight{\textbf{ProAPO} (ours)} & \highlight{\textbf{61.5}} & \highlight{\textbf{90.3}} & \highlight{\underline{58.0}} & \highlight{\textbf{50.7}} & \highlight{\textbf{52.3}} & \highlight{\textbf{51.7}} & \highlight{\textbf{21.1}} & \highlight{\textbf{75.1}} & \highlight{\textbf{81.8}} & \highlight{\underline{88.7}} & \highlight{\textbf{41.8}} & \highlight{\textbf{63.7}} & \highlight{\textbf{66.0}} & \highlight{\textbf{64.6}}  & \highlight{\textbf{61.8}} \\
        
        \bottomrule
    \end{tabular}
}
  
  \caption{\textbf{Ablation of template and description optimization.} 
  Avg (11) and Avg (13) denote average results across 11 datasets (excluding CUB~\cite{CUB} and Places~\cite{Places365}) and all 13 datasets, respectively. ATO denotes our automatic template optimization algorithm.}
  \vspace{-4pt}
  \label{supp_tab: ablate_template_and_description}
\end{table*}
}

% ---------------------------------------------------- % 
%           更多的消融结果
% ---------------------------------------------------- %
\section{More Ablation Results}
\label{sec_supp: more_ablation_result}

% ---------------------------------------------------- % 
%           对 Template 和 Description 优化的消融
% ---------------------------------------------------- %
\subsection{Ablation of Template and Description Optimization}
\label{supp_sec: ablation_template_and_description}

In~\cref{supp_tab: ablate_template_and_description}, we ablate key components in template and description optimization on the ResNet50 backbone. 

\textbf{(1) Ablation of Template Optimization}. In the main paper (Sec. 4.3), we show that prompt ensembling is better than a single prompt. Moreover, dataset domain information also plays a significant role in template optimization. Without domain information, we see a performance drop in our ATO by an average of 1.0\% (from 58.3\% to 59.3 \%) on thirteen datasets. This is because domain information provides contextual information, which can mitigate issues of semantic ambiguity caused by class names.

\textbf{(2) Ablation of Description Optimization}. Without label synonyms to increase description diversity, a performance degradation appears by an average of 1.4\% (from 60.4\% to 61.8\%) on thirteen datasets. It verifies the effectiveness of optimization class names, which are usually ignored in previous description methods~\cite{CuPL, DCLIP, AdaptCLIP, GPT4Vis}.

\textbf{(3) Template VS Description Optimization}. Compared with template optimization, we see a notable performance improvement with description optimization, especially in CUB~\cite{CUB}, DTD~\cite{DTD}, ESAT~\cite{EuroSAT}, FLO~\cite{FLO}, and UCF~\cite{UCF101} datasets. It demonstrates that optimizing class-specific prompts can find discriminative information for fine-grained classification.


% ---------------------------------------------------- % 
%                   对每个 Operator 操作的消融
% ---------------------------------------------------- %
\subsection{More Ablation of Operators}
\label{supp_sec: more_ablation_operator}
To further explore whether each operator has a role in searching the optimal result, we show the number of each operator causing the new optimal score during the iterations in~\cref{supp_tab: more_ablation_operator}. We see that each operator in iterative optimization may generate a better prompt. It further demonstrates that each operator is helpful in ProAPO. Notably, the crossover operator has the highest times to update the optimal score, which demonstrates that it makes the model search for the optimal prompt faster with limited iterations. 

{
\renewcommand{\arraystretch}{1.1} 
% \setlength{\tabcolsep}{4.pt}
\begin{table}[t]
  \centering
  \resizebox{0.96\linewidth}{!}
    {
    \begin{tabular}
        {l | c  c  c  c  c | c  }  
        \toprule
        {\textbf{Dataset}} & \texttt{Add} & \texttt{Del} & \texttt{Rep} & \texttt{Cross} & \texttt{Mut} & \textbf{Total} \\
        \midrule
        IN-1K~\cite{Imagenet} & 3 & 4 & 5 & 5 & 2 &  19 \\
        Caltech~\cite{caltech101} & 5 & 5 & 6 & 12 & 3 & 31 \\
        Cars~\cite{Cars} & 7 & 8 & 5 & 8 & 3 & 31 \\ 
        CUB~\cite{CUB} & 9 & 4 & 10 & 6 & 2 & 31 \\
        DTD~\cite{DTD} & 5 & 3 & 8 & 8 & 2 & 26 \\
        ESAT~\cite{EuroSAT} & 2 & 4 & 6 & 8 & 1 & 21 \\
        FGVC~\cite{FGVC} & 6 & 2 & 6 & 5 & 3 & 22 \\ 
        FLO~\cite{FLO} & 5 & 3 & 11 & 5 & 4 & 28 \\
        Food~\cite{Food101} & 5 & 3 & 4 & 5 & 2 & 19 \\ 
        Pets~\cite{oxford_pets} & 4 & 2 & 5 & 6 & 2 & 19 \\ 
        Places~\cite{Places365} & 3 & 2 & 8 & 12 & 4 & 29 \\ 
        SUN~\cite{SUN} & 4 & 2 & 3 & 5 & 2 & 16 \\ 
        UCF~\cite{UCF101} & 5 & 6 & 8 & 6 & 2 & 27 \\ 
        \midrule
        \textbf{Sum} & 63 & 48 & 85 & 91 & 32 & 319 \\ 
        \bottomrule
    \end{tabular}
}
% \vspace{-6pt}
  \caption{\textbf{Number of times for each operator that update the optimal score.} \textbf{Total} denotes the total number of iterations when achieving the highest score.}
% \vspace{-8pt}
  \label{supp_tab: more_ablation_operator}
\end{table}
}

% ---------------------------------------------------- % 
%                   对 Group Sampling 的消融
% ---------------------------------------------------- %
{
\renewcommand{\arraystretch}{1.1} 
% \setlength{\tabcolsep}{4.pt}
\begin{table*}[t]
  \centering
  \resizebox{0.99\linewidth}{!}
    {
    \begin{tabular}
        {l | ccccc ccccc ccc | c | c | c }  
        \toprule
        {\textbf{Module} (ViT-B/32)}  & \rotatebox{90}{\textbf{IN-1K}} & \rotatebox{90}{\textbf{Caltech}} & \rotatebox{90}{\textbf{Cars}} & \rotatebox{90}{\textbf{CUB}} & \rotatebox{90}{\textbf{DTD}}  & \rotatebox{90}{\textbf{ESAT}} & \rotatebox{90}{\textbf{FGVC}} & \rotatebox{90}{\textbf{FLO}} & \rotatebox{90}{\textbf{Food}}  &  \rotatebox{90}{\textbf{Pets}} & \rotatebox{90}{\textbf{Places}} & \rotatebox{90}{\textbf{SUN}} & \rotatebox{90}{\textbf{UCF}} & \rotatebox{90}{\textbf{Avg (11)}} & \rotatebox{90}{\textbf{Avg (13)}} & \textbf{Times}  \\
        \midrule
        CuPL & 64.4  & 92.9  & 60.7  & 53.3  & {50.6}  & 50.5  & 20.9  & 69.5  & 84.2  & 87.0  & \underline{43.1}  & {66.3}  & 66.4  & 64.9  & 62.3 & - \\
        \midrule
        
        \texttt{a)} w/ all categories in one group & 64.5 & 93.3 & 60.9 & 53.5 & \underline{51.6} & 52.2 & 22.2 & 70.8 & 84.5 & 87.9 & 42.3 & \textbf{66.7} & \textbf{69.4} & 65.8  & 63.1 & \textbf{20 min} \\
        \texttt{b)} w/ random selected group & 64.3 & 93.7 & \textbf{61.8} & \underline{55.2} & 48.7 & 59.5 & 22.6 & 72.9 & \underline{85.2} & \underline{90.8} & 42.6 & 65.4 & 68.4 & 66.7  & 63.9 & 15 min  \\
        \texttt{c)} w/ performance best group & 64.1 & 93.0 & 61.2 & 54.4 & 47.4 & 56.8 & 20.7 & 68.2 & 85.1 & 88.6 & 42.4 & 65.0 & 65.4 & 65.0  & 62.5 & 15 min \\
        \texttt{d)} w/ K-Means algorithm & \underline{64.6} & \underline{93.8} & \textbf{61.8} & 55.1 & 49.4 & \underline{59.6} & \underline{22.8} & \underline{74.0} & \textbf{85.3} & 90.7 & {42.7} & 65.4 & \underline{69.0} & \underline{67.0}  & \underline{64.2} & \underline{17 min} \\
        
        \midrule

        
        \highlight{\textbf{ProAPO} (full model)} & \highlight{\textbf{64.7}}  & \highlight{\textbf{94.4}} & \highlight{\underline{61.7}} & \highlight{\textbf{55.4}} & \highlight{\textbf{53.5}} & \highlight{\textbf{63.0}} & \highlight{\textbf{23.0}} & \highlight{\textbf{74.3}} & \highlight{\textbf{85.3}} & \highlight{\textbf{91.0}} & \highlight{\textbf{43.3}} & \highlight{\underline{66.6}} & \highlight{\underline{69.0}} & \highlight{\textbf{67.9}}  & \highlight{\textbf{65.0}} & \highlight{15 min} \\
        \bottomrule
    \end{tabular}
}
    \vspace{-5pt}
  \caption{\textbf{More ablation of group sampling strategy.} We ablate the ways for selecting salient groups. \textbf{Times} denotes the time that ProAPO runs on ImageNet with the default setting.}
  \vspace{-7pt}
  \label{supp_tab: more_ablation_group_sampling}
\end{table*}

}


\subsection{More Ablation of Group Sampling}
\label{supp_sec: more_ablation_group_sampling}

In~\cref{supp_tab: more_ablation_group_sampling}, we ablate how to select categories in the group sampling strategy. We consider the settings for optimizing all categories in one group, selecting random categories and the best categories with their misclassified categories in groups. In rows a)-c) of~\cref{supp_tab: more_ablation_group_sampling}, we see notable performance degradation compared to full ProAPO. It further demonstrates that optimizing salient and worst groups can achieve comparable results with all categories and save iteration costs. Moreover, we also consider replacing misclassified categories with a K-Means clustering algorithm. A performance drop appears in row d), which verifies the effectiveness of selecting misclassified categories in groups.



% ---------------------------------------------------- % 
%                   对 Computation 的消融
% ---------------------------------------------------- %
\subsection{Ablation of Cost Computation}
\label{supp_sec: ablation_of_cost_computation}

In~\cref{supp_tab: extra_computation_cost}, we detail the time each process consumes on ImageNet. Compared to previous LLM-generated description methods, we similarly query LLMs one-time to generate descriptions (\textit{i.e.}, process of building prompt library). In addition, we introduce iterative processes to refine prompts and two sampling strategies to save costs. With a few additional costs (15 min v.s. 60 min), our ProAPO improves previous methods by at least 2.7\% on average. This further verifies the efficiency of our method.

{
% \vspace{-11pt}
\renewcommand{\arraystretch}{1.0} 
\setlength{\tabcolsep}{3.8pt}
\begin{table}[!h]
  \centering
  \resizebox{1.0\linewidth}{!}
    {
    \begin{tabular}
        {l | c | c c c c }  
        \toprule
        \textbf{Process} & \textbf{Build Library} & \highlight{\textbf{Sample Strategy}} & \highlight{\textbf{Template Optim.}} & \highlight{\textbf{Description Optim.}} \\ 
        \midrule 
        \textbf{Times} & 60 min &  \highlight{3 min}  & \highlight{1.6 min} & \highlight{10.4 min} \\
        \bottomrule
    \end{tabular}
}
\vspace{-5pt}
  \caption{\textbf{Computation cost analysis in the ImageNet dataset.}}
% \vspace{-18pt}
  \label{supp_tab: extra_computation_cost}
\end{table}
}




% ---------------------------------------------------- % 
%                    超参数分析
% ---------------------------------------------------- %
\section{More Hyperparameter Analysis}
\label{sec_supp: more_hyper_analysis}




{
\renewcommand{\arraystretch}{1.1} 
\setlength{\tabcolsep}{4.pt}
\begin{table}[tbp]
  \centering
  \resizebox{0.99\linewidth}{!}
    {
    \begin{tabular}
        {l | l | c | c c c c c | c }

            
        \toprule
        {\textbf{Dataset}} & \textbf{Module} & {\textbf{TF}} & \multicolumn{5}{c}{\textbf{Number of training samples}} & \textbf{UB} \\
        \cmidrule(lr){4-8}
         & (RN50) & &  1 & 2 & 4 & 8 & 16\\
        \midrule
        \multirow{2}{*}{\textbf{Avg (11)}} & CoOp~\cite{CoOp} & \xmark & 59.6 & 62.3 & \textbf{66.8} & \textbf{69.9} & \textbf{73.4} & -  \\
         & {\textbf{ProAPO}} & \cmark & \textbf{64.6} & \textbf{65.0} & 65.4 & 65.8 & 66.1 & 67.2 \\
        \midrule
        
        \multirow{2}{*}{\textbf{IN-1K}} & CoOp~\cite{CoOp} & \xmark  & 57.2 & 57.8 & 60.0 & \textbf{61.6} & \textbf{63.0} & -  \\
        & {\textbf{ProAPO}} & \cmark & \textbf{61.5} & \textbf{61.6} & \textbf{61.5} & \textbf{61.6} & 61.6 & 61.7 \\
        \midrule
        
        \multirow{2}{*}{\textbf{Caltech}} & CoOp~\cite{CoOp} & \xmark  & 87.5 & 87.9 & 89.6 & 90.2 & 91.8 & -  \\
        & {\textbf{ProAPO}} & \cmark & \textbf{90.3} & \textbf{90.4} & \textbf{90.6} & \textbf{90.7} & \textbf{91.0} & 91.1 \\        
        \midrule
        
        \multirow{2}{*}{\textbf{Cars}} & CoOp~\cite{CoOp} & \xmark & 55.6 & 58.3 & \textbf{62.6} & \textbf{68.4} & \textbf{73.4} & -  \\
        & {\textbf{ProAPO}} & \cmark & \textbf{58.0} & \textbf{58.5} & 58.8 & 58.9 & 59.1 & 60.8  \\
        \midrule
        
        \multirow{2}{*}{\textbf{DTD}} & CoOp~\cite{CoOp} & \xmark & 44.4 & 45.2 & \textbf{53.5} & \textbf{60.0} & \textbf{63.6} & -  \\
        & {\textbf{ProAPO}} & \cmark & \textbf{52.3} & \textbf{52.7} & 53.0 & 53.4 & 53.6 \\
        \midrule
        
        \multirow{2}{*}{\textbf{ESAT}} & CoOp~\cite{CoOp} & \xmark & 50.6 & \textbf{61.5} & \textbf{70.2} & \textbf{76.7} & \textbf{83.5} & -  \\
        & {\textbf{ProAPO}} & \cmark & \textbf{51.7} & 53.5 & 55.6 & 57.4 & 58.3 & 62.2 \\
        \midrule
        
        \multirow{2}{*}{\textbf{FGVC}} & CoOp~\cite{CoOp} & \xmark & 9.6 & 18.7 & \textbf{21.9} & \textbf{26.1} & \textbf{31.3} & -  \\
        & {\textbf{ProAPO}} & \cmark & \textbf{21.1} & \textbf{21.0} & 21.2 & 21.2 & 21.3 & 21.5 \\
        \midrule
        
        \multirow{2}{*}{\textbf{FLO}} & CoOp~\cite{CoOp} & \xmark & 68.1 & \textbf{77.5} & \textbf{86.2} & \textbf{91.2} & \textbf{94.5} & -  \\
        & {\textbf{ProAPO}} & \cmark & \textbf{75.1} & 75.6 & 76.4 & 76.7 & 77.8 & 79.1 \\
        \midrule
        
        \multirow{2}{*}{\textbf{Food}} & CoOp~\cite{CoOp} & \xmark  & 74.3 & 72.5 & 73.3 & 71.8 & 74.7 & -  \\
        & {\textbf{ProAPO}} & \cmark & \textbf{81.8} & \textbf{82.0} & \textbf{82.1} & \textbf{82.2} & \textbf{82.3} & {82.9} \\
        \midrule
        
        \multirow{2}{*}{\textbf{Pets}} & CoOp~\cite{CoOp} & \xmark  & 85.9 & 82.6 & 86.7 & 85.3 & 87.0 & -  \\
        & {\textbf{ProAPO}} & \cmark & \textbf{88.7} & \textbf{89.4} & \textbf{89.5} & \textbf{89.8} & \textbf{89.9} & {91.0} \\
        \midrule
        
        \multirow{2}{*}{\textbf{SUN}} & CoOp~\cite{CoOp} & \xmark  & 60.3 & 59.5 & 63.5 & \textbf{65.5} & \textbf{69.3} & -  \\
        & {\textbf{ProAPO}} & \cmark & \textbf{63.7} & \textbf{63.8} & \textbf{63.8} & 63.8 & 63.9 & 64.5 \\
        \midrule
        
        \multirow{2}{*}{\textbf{UCF}} & CoOp~\cite{CoOp} & \xmark  & 61.9 & 64.1 & 67.0 & \textbf{71.9} & \textbf{75.7} & -  \\
        & {\textbf{ProAPO}} & \cmark &  \textbf{66.0} & \textbf{66.8} & \textbf{67.1} & 68.1 & 68.9 & 71.4 \\
        \bottomrule
    \end{tabular}
}
  \vspace{-5pt}
  \caption{\textbf{Scaling up to more shots.} \textbf{Avg (11)} denotes average results across 11 datasets. \textbf{TF} denotes training-free approaches. \textbf{UB} denotes upper bound evaluated on the test set.}
  % \vspace{-5pt}
  \label{supp_tab: shots_influence}
\end{table}
}



% ---------------------------------------------------- % 
%                    Shots 的影响
% ---------------------------------------------------- %
\subsection{Effect of Shot Numbers}
\label{supp_sec: effect_shots}
In~\cref{supp_tab: shots_influence}, we show the effect of the number of training samples per category.
Specifically, we conduct experiments with 1, 2, 4, 8, and 16 shots.
Moreover, we introduce the performance of the optimal prompt searched in the test set as the upper bound of ProAPO.
Compared with CoOp~\cite{CoOp}, ProAPO achieves remarkable performance when shots $\leq 2$, which demonstrates the effectiveness of our method under low-shot settings. Since we only adapt VLMs in a training-free way, the performance increases finitely as the training samples increase. We attribute two key directions for further performance improvement in high-shot settings. First, our result is still far from the upper bound (66.1 \% in 16 shots VS 67.2 \% for the upper bound). We need to improve the prompt generation algorithm and the score function to find better candidate prompts within the limited iterations. Second, the upper bound of our ProAPO is much smaller than the prompt tuning method. We need to use a larger natural language search space (\textit{e.g.}, more diverse descriptions, or more query times of LLMs) to further increase the upper bound of the optimal result.



% ---------------------------------------------------- % 
%                    Alpha 值的影响
% ---------------------------------------------------- %
\subsection{Effect of Scalar in Score Function}
\label{supp_sec: effect_alpha}

In~\cref{supp_tab: ablation_alpha},  we show the effect of $\alpha$ in~\cref{eq: score_function}. We see that performance improves as the $\alpha$ increases. This is because the entropy constraint provides more information to select better candidate prompts. We see a stable result when $\alpha \in [5e2, 5e3]$, which means a better trade-off between accuracy and entropy constraint. However, a high $\alpha$ may be biased to the train set, thus harming the performance. 

{
% \vspace{-10pt}
% \renewcommand{\arraystretch}{1.0} 
\begin{table}[h]
  \centering
  \resizebox{1.0\linewidth}{!}
    {
    \begin{tabular}
        {l | c c c c c c c c}  
        \toprule
        $\alpha$ & $0$ & $1e1$ & $1e2$ & $5e2$ & $1e3$ & $5e3$ & $1e4$ & $1e5$ \\
        \midrule
        \textbf{Avg (13)} & 62.3 & 63.4 & 64.4 & 64.9 & \textbf{65.0} & 64.8 & 63.7 & 63.1 \\
        \bottomrule
    \end{tabular}
}
% \vspace{-5pt}
  \caption{\textbf{Effect of $\alpha$ value in Eq.6 across 13 datasets.}}
  % \vspace{-5pt}
  \label{supp_tab: ablation_alpha}
\end{table}
}


% ---------------------------------------------------- % 
%                   T_sample 的影响
% ---------------------------------------------------- %
\subsection{Effect of Sampled Numbers in Prompt Sampling Strategy}
\label{supp_sec: effect_sampled_numbers}
In~\cref{supp_fig: effect_sampled_numbers}, we show the effect of sampled numbers $T_{sample}$ of Alg.~\ref{supp_alg: prompt_strategy}. The $T_{sample} = 0$ means that the prompt sampling strategy is not used. As the number of $T_{sample}$ increases, we see a slight performance gain when $T_{sample} < 4$. After $T_{sample} \geq 4$, a consistent improvement appears because the initial search point achieves a higher score than the baseline. We achieve stable results when $T_{sample} \geq 32$.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\linewidth]{Paper_Result_supp/hyper_result_Sampled_Numbers.pdf}
\vspace{-8pt}
\caption{\textbf{Effect of sampled numbers $T_{sample}$.} 
}
\label{supp_fig: effect_sampled_numbers}
\vspace{-6pt}
\end{figure}


% ---------------------------------------------------- % 
%                   Prompt Library 的影响
% ---------------------------------------------------- %
\subsection{Effect of Quality of Prompt Library}
\label{supp_sec: effect_of_quality_of_prompt_library}
In~\cref{supp_fig: effect_LLM_Query} and~\cref{supp_fig: effect_generated_descriptions}, we analyze two key factors affecting the prompt library: LLM-query prompts and generated descriptions. Our ProAPO improves prompt quality even under a small number of query prompts and descriptions, demonstrating its effectiveness in a limited prompt library. 




\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\linewidth]{Paper_Result_supp/LLM-query.pdf}
\vspace{-8pt}
\caption{\textbf{Effect of Number of LLM-query Prompts.} 
}
\label{supp_fig: effect_LLM_Query}
% \vspace{-6pt}
\end{figure}



\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\linewidth]{Paper_Result_supp/generated_descriptions.pdf}
\vspace{-8pt}
\caption{\textbf{Effect of Number of Generated Descriptions.} 
}
\label{supp_fig: effect_generated_descriptions}
% \vspace{-6pt}
\end{figure}




% ---------------------------------------------------- % 
%                     更多的可视化
% ---------------------------------------------------- %



\section{More Qualitative Results}
\label{sec_supp: more_qualitative_result}
In~\cref{supp_fig: qualitative_result}, we show more examples of the changes in descriptions with our ProAPO, including images of animals, flowers, and textures.
Similarly, we see that common descriptions are removed and discriminative ones are retained for fine-grained categories, which further verifies the effectiveness of our progressive optimization.


\begin{figure*}[htbp]
\centering
\includegraphics[width=0.8\linewidth]{Paper_Result_supp/qualitative_results_supp.pdf}
% \vspace{-8pt}
\caption{\textbf{Qualitative analysis of class-specific prompt optimization by ProAPO.} Shaded \textbf{\textcolor{removed}{red}} and \textbf{\textcolor{retained}{blue}} words denote common and discriminative descriptions in two confused categories.
}
\label{supp_fig: qualitative_result}
\vspace{20pt}
\end{figure*} 


% \newpage


