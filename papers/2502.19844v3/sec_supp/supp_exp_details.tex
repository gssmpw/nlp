\section{More Implementation Details}
\label{sec_supp: implement_details}


\subsection{Hyperparameter Settings}
In~\cref{supp_tab: exp_details}, we show the searched hyperparameter settings for thirteen datasets. All results are average with four seeds. Except for $1, 2, 3$ as seeds like CoOp~\cite{CoOp}, we add $42$ as our fourth seed to further evaluate the stability of our method. In the default setting, we use the same LLMs as the description methods, \textit{i.e.}, GPT-3~\cite{GPT3} for CuPL~\cite{CuPL} and DCLIP~\cite{DCLIP}, GPT-4~\cite{GPT4_Tech} for GPT4Vis~\cite{GPT4Vis} and AdaptCLIP~\cite{AdaptCLIP}. 

% Our codes are available at \href{https://anonymous.4open.science/r/ProAPO}{https://anonymous.4open.science/r/ProAPO}. We will release our code and optimized prompts after the review stage.


{
\renewcommand{\arraystretch}{1.1} 
% \setlength{\tabcolsep}{3.pt}
\begin{table}[htbp]
  \centering
  \resizebox{1.0\linewidth}{!}
    {
    \begin{tabular}
        {l | c | c | c | c | c | c | c  }  
        \toprule
        {\textbf{Dataset}}  & $T$ &  $M$ & $N$ & $\alpha$ & $n_{wst}$ & $n_{sln}$ & $T_{sample}$ \\
        \midrule
        IN-1K~\cite{Imagenet} & 4 & 8 & 8 & 1e3 & 4 & 4 & 32 \\
        Caltech~\cite{caltech101} & 2 & 8 & 8 & 1e2 & 2 & 2 & 32 \\
        Cars~\cite{Cars} & 4 & 8 & 8 & 1e4 & 4 & 4 & 32 \\ 
        CUB~\cite{CUB} & 4 & 8 & 8 & 1e2 & 4 & 4 & 32 \\
        DTD~\cite{DTD} & 4 &  8 & 8 & 1e3 & 4 & 4 & 32 \\
        ESAT~\cite{EuroSAT} & 4 &  8 &  8 & 1e3 & 3 & 3 & 32 \\
        FGVC~\cite{FGVC} & 4 & 8 & 8 & 1e3 & 4 & 4 & 32 \\ 
        FLO~\cite{FLO} & 4 & 8 & 8 & 1e3 & 4 & 4 & 32 \\
        Food~\cite{Food101} & 4 &  8 & 8 & 1e3 & 2 & 2 & 32 \\ 
        Pets~\cite{oxford_pets} & 2 & 8 &  8 & 1e4 & 2 & 2 & 32 \\ 
        Places~\cite{Places365} & 4  & 8 &  8 & 1e2 & 3 & 3 & 32 \\ 
        SUN~\cite{SUN} & 2 &  8 & 8 & 1e4 & 4 & 4 & 32 \\ 
        UCF~\cite{UCF101} & 4 &  8 & 8 & 1e3 & 3 & 3 & 32 \\ 
        \bottomrule
    \end{tabular}
}
\vspace{-6pt}
  \caption{\textbf{Hyperparameters settings for thirteen datasets.}}
\vspace{-10pt}
  \label{supp_tab: exp_details}
\end{table}
}


\subsection{More Related Work}
\noindent 
\textbf{Large-scale vision-language models}
like CLIP~\cite{CLIP} have shown promising performance on various tasks. They align visual and textual spaces to a joint space via training on millions of image-text pairs from the web. Other work~\cite{Align, DEFILIP, DeClip, FILIP, BLIP, Flamingo, SLIP, EVA-01, EVA-02} has furthered this paradigm to learn more accurate semantic alignment in joint space. 
In this work, we advance VLMs for downstream tasks by progressively learning optimal class-specific prompts with minimal supervision and no human intervention. 



