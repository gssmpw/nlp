\section{Details of Building Prompt Library}
\label{sec_supp: build_prompt_library}


% ---------------------------------------------------- % 
%             Template Library 的细节
% ---------------------------------------------------- %
\subsection{Details of Building Template Library}
The template library aims to collect a set of templates that provide task-specific contextual information, which can address issues of semantic ambiguity caused by class names. It contains processes for collecting templates, generating dataset domains, and adding dataset domains to templates.

% for subsequent prompt generation


% **************************** % 
% 直接借助 LLMs 生成 Template 的 Prompt
% **************************** %
\textbf{Collecting templates.} We utilize two ways to collect templates. First, pre-defined templates, such as Template-80~\cite{CLIP}, FILIP-8~\cite{FILIP}, and DEFILIP-6~\cite{DEFILIP} can be used. Second, similar to PN~\cite{P_N}, we query LLMs to create diverse templates by the following prompt:
\begin{quote}
    \makebox[\linewidth]{%
        \colorbox{lightblue}{%
            \hspace*{0mm} % Adjust left alignment
            \begin{minipage}{\dimexpr\linewidth+10\fboxsep\relax} % Adjust width
                % \fontsize{9pt}{10pt}\selectfont % Font settings
                ``Hi, ChatGPT! I would like your help to prompt for image classification using CLIP. As a human-level prompt engineer, your task is to create a set of Templates like the following for visual classification. For example: 
                \newline \newline
                a photo of a \{\}.''
            \end{minipage}%
        }%
    }
\end{quote}


% **************************** % 
%  生成 Dataset Type 的 Prompt
% **************************** %
\textbf{Generating dataset domain by LLMs.} Inspired by previous description-based methods~\cite{WaffleCLIP, VDT_2023_ICCV}, we query LLMs to generate dataset domain information to provide task-specific context. For this purpose, we use the prompt:
\begin{quote}
    \makebox[\linewidth]{%
        \colorbox{lightblue}{%
            \hspace*{0mm} % Adjust left alignment
            \begin{minipage}{\dimexpr\linewidth+10\fboxsep\relax} % Adjust width
                % \fontsize{9pt}{10pt}\selectfont % Font settings
                ``Hi, ChatGPT! I would like your help in generating dataset domain information for image classification based on the dataset paper. A few words are good. Please return directly without explanation. 
                \newline \newline
                \{\texttt{uploaded PDF}\}.''
            \end{minipage}%
        }%
    }
\end{quote}
Here, \{\texttt{uploaded PDF}\} represents the uploading of the paper of the dataset to LLMs. 
Generated dataset domain information is summarized in~\cref{supp_tab: domain_information}.


% **************************** % 
% 具体使用的 Dataset Domain
% **************************** %
{
\renewcommand{\arraystretch}{1.1} 
\begin{table}[htbp]
  \centering
  \resizebox{1.0\linewidth}{!}
    {
    \begin{tabularx}{0.56\textwidth}
        {l | X }  
        \toprule
        {\textbf{Dataset}}  & \textbf{Domain Information} \\
        \midrule
        IN-1K~\cite{Imagenet} & real scenario; natural scene \\
        Caltech~\cite{caltech101} & object; everyday objects; common items \\
        Cars~\cite{Cars} & car; vehicles; auto-mobile \\ 
        CUB~\cite{CUB} & bird; wildlife; ornithology  \\
        DTD~\cite{DTD} & textures; patterns; surface; material \\
        ESAT~\cite{EuroSAT} & land cover; remote sensing; satellite photo; satellite imagery; aerial or satellite images; centered satellite photo \\
        FGVC~\cite{FGVC} & aircraft; airplane; plane; airliner \\ 
        FLO~\cite{FLO} & flower; floral; botanical; bloom \\
        Food~\cite{Food101} & food; dishes; cuisine; nourishment \\ 
        Pets~\cite{oxford_pets} & pet; domestic animals; breed; dog or cat \\ 
        Places~\cite{Places365} & place; scene \\ 
        SUN~\cite{SUN} &  place; scene \\ 
        UCF~\cite{UCF101} & action; human action; human activities; person doing \\ 
        \bottomrule
    \end{tabularx}
}
\vspace{-6pt}
  \caption{\textbf{Generated dataset domain information.}}
% \vspace{-10pt}
  \label{supp_tab: domain_information}
\end{table}
}

% **************************** % 
% 将 Dataset Type 与 Template 融合的方式
% **************************** %
\textbf{Adding dataset domain to templates.} We supplement templates with dataset domain information in the following four ways: (1) Add ``a type of \{\texttt{domain}\}''. (2) Replace ``\{\texttt{class}\}'' with ``\{\texttt{domain}\}:\{\texttt{class}\}''. (3) Replace ``photo'' with ``\{\texttt{domain}\}''. (4) Replace ``photo'' with ``\{\texttt{domain}\} photo''. Taking ``a photo of a \{\texttt{class}\}'' as an example, we modify the templates with the above four ways to add dataset domain information as follows:
\begin{quote}
    \makebox[\linewidth]{%
        \colorbox{lightblue}{%
            \hspace*{0mm} % Adjust left alignment
            \begin{minipage}{\dimexpr\linewidth+10\fboxsep\relax} % Adjust width
                % \fontsize{9pt}{10pt}\selectfont % Font settings
                \begin{enumerate}
                    \item a photo of a \{\texttt{class}\}, a type of \{\texttt{domain}\}.
                    \item a photo of a \{\texttt{domain}\}: \{\texttt{class}\}.
                    \item a \{\texttt{domain}\} of a \{\texttt{class}\}.
                    \item a \{\texttt{domain}\} photo of a \{\texttt{class}\}.
                \end{enumerate}
            \end{minipage}%
        }%
    }
\end{quote}
Here, \{\texttt{class}\} and \{\texttt{domain}\} denote category name and dataset domain information, respectively.


% ---------------------------------------------------- % 
%             Description Library 的细节
% ---------------------------------------------------- %
\subsection{Details of Building Description Library}
\label{supp_sec: build_description_library}

Description Library aims to provide a set of visual descriptions for each category, enhancing visual semantics for fine-grained recognition in prompts. It contains processes for generating visual descriptions and category synonyms and integrating descriptions with the best templates.

% , including CuPL~\cite{CuPL}, DCLIP~\cite{DCLIP}, GPT4Vis~\cite{GPT4Vis}, and AdaptCLIP~\cite{AdaptCLIP}.


% **************************** % 
%      生成 Description 的 Prompt
% **************************** %
{
\renewcommand{\arraystretch}{1.1} 
\begin{table}[htbp]
  \centering
  \resizebox{1.0\linewidth}{!}
    {
    \begin{tabularx}{0.56\textwidth}
        {l | X }  
        \toprule
        {\textbf{Method}}  & \textbf{Prompts} \\
        \midrule
        DCLIP~\cite{DCLIP} & Q: What are useful visual features for distinguishing a \{\texttt{class}\} in a photo? \\
        & A: There are several useful visual features to tell there is a \{\texttt{class}\} in a photo: \\
        
        \midrule 
        CuPL-Base~\cite{CuPL} & Describe what a \{\texttt{class}\} looks like. \\ 
        & Describe a \{\texttt{class}\}. \\
        & What are the identifying characteristics of a \{\texttt{class}\}? \\
        
        \midrule

        CuPL-Full~\cite{CuPL} & Describe what a \{\texttt{class}\} looks like. \\ 
        & How can you identify a \{\texttt{class}\}? \\ 
        & What does a \{\texttt{class}\} look like? \\
        & Describe an image from the internet of a \{\texttt{class}\}\\
        & A caption of an image of a \{\texttt{class}\}: \\
        
        \midrule
        GPT4Vis~\cite{GPT4Vis} & I want you to act as an image description expert. I will give you a word and your task is to give me 20 sentences to describe the word. Your description must accurately revolve around this word and be as objective, detailed and diverse as possible. In addition, the subject of your description is a some kind of object photograph. Output the sentences in a json format which key is the the word and the value is a list composed of these sentences. Do not provide any explanations. The first word is ``\{\texttt{class}\}". \\ 

        \midrule 
        AdaptCLIP~\cite{AdaptCLIP} & What characteristics can be used to differentiate \{\texttt{class}\} from other \{\texttt{domain}\} based on just a photo? Provide an exhaustive list of all attributes that can be used to identify the \{\texttt{domain}\} uniquely. Texts should be of the form “\{\texttt{domain}\} with \{\texttt{characteristic}\}”. \\
        \bottomrule
    \end{tabularx}
}
% \vspace{-6pt}
  \caption{\textbf{Prompts for generating visual descriptions.}}
% \vspace{-10pt}
  \label{supp_tab: generate_description}
\end{table}
}


% **************************** % 
%      生成同义词的 Prompt
% **************************** %
\textbf{Generating category synonym}.
Except for descriptions, we also replace class names from the dataset with their synonyms to create diverse class-specific prompts. For this purpose, we use the following prompt to ask LLMs to generate category synonyms:
\begin{quote}
    \makebox[\linewidth]{%
        \colorbox{lightblue}{%
            \hspace*{0mm} % Adjust left alignment
            \begin{minipage}{\dimexpr\linewidth+10\fboxsep\relax} % Adjust width
                % \fontsize{9pt}{10pt}\selectfont % Font settings
                ``Hi, ChatGPT! I would like your help in generating category synonyms. As a \{\texttt{domain}\} expert, I will provide you with a category name. Your task is to provide synonyms for the current category. If it has subclasses, return them as well. Please return directly without explanation.
                \newline \newline 
                User: I want to give the synonyms of \{\texttt{class}\}. 
                \newline
                Assistant: ''
            \end{minipage}%
        }%
    }
\end{quote}

% **************************** % 
% 各个数据集使用的询问 LLMs 生成 Description 的 Prompt
% **************************** %
\textbf{Generating visual descriptions for each category}.
Similar to previous description methods~\cite{CuPL, DCLIP, GPT4Vis, AdaptCLIP}, we instruct LLM to generate visual descriptions for each category by several prompts, which are summarized in~\cref{supp_tab: generate_description}.


% **************************** % 
%  将 Description 和 Template 整合
% **************************** %
\textbf{Integrating descriptions with the best templates}.
We use the following prompt to integrate descriptions with templates: ``\{\texttt{template}\}. \{\texttt{description}.\}''. 


% **************************** % 
%  每个组迭代的描述库
% **************************** %
After the above processes, we collect diverse visual descriptions for each category $c$, denoted as $\text{VD}(c)$. For each group iteration, we select the descriptions for categories in the specific group as the description library. Moreover, the prompt sampling strategy also utilizes these descriptions for class-specific initialization.

% ---------------------------------------------------- % 
%              Prompt Sampling 策略的细节
% ---------------------------------------------------- %
\section{Details of Prompt Sampling Strategy}
\label{sec_supp: prompt_sampling} 
The detailed prompt sampling strategy is summarized in Alg.~\ref{supp_alg: prompt_strategy}. Visual descriptions of each class $\text{VD}(c)$ are collected by the above process (see~\cref{supp_sec: build_description_library}). We utilize the candidate prompt $P_t^*$ with the best templates as an initial point. The $\textsc{RandomSample}(\cdot)$ operator denotes randomly selecting a set of elements from a given set. We randomly sample descriptions for each category to create multiple candidate prompts (Lines 2-8). After $T_{sample}$-times steps, we select the candidate prompt $\hat{P}_0$ with the highest score for description initialization (Line 9). It ensures that subsequent optimization is around the optimal initial point. We set $T_{sample} = 32 $ for all datasets in the default setting.

% Notably, descriptions generated by LLMs in this strategy are also used as the specific-group description library.


% **************************** % 
%    Prompt Sampling 的具体算法
% **************************** %
\begin{algorithm}[htbp]
\caption{Prompt Sampling Strategy.}
\label{supp_alg: prompt_strategy}
\begin{algorithmic}[1]
\REQUIRE $\mathcal{D} \leftarrow \{{(x, y)}\}_n$: training samples, $F:  \mathcal{D} \times P \to \mathbb{R}$: score function, $\mathcal{C}$: class labels, $\text{VD}(c)$: visual descriptions of class $c$, $P_t^*$: the prompt candidate with the best template
\STATE $\mathcal{U} \leftarrow \{P_t^*\} $ 
\FOR{$i=1$ to $T_{sample}$}
    \STATE $P_i \leftarrow P_t^* $
    \FORALL{class $c \in \mathcal{C}$}
        \STATE $P_i \leftarrow P_i \cup \textsc{RandomSample}(\text{VD}(c))$
    \ENDFOR
    \STATE $\mathcal{U} \leftarrow \mathcal{U} \cup \{ P_i \} $ 
\ENDFOR
\STATE $\hat{P}_0 \leftarrow \arg\max_{{P} \in \mathcal{U}} F(\mathcal{D}, {P})$ 
\RETURN the candidate prompt with the highest score $\hat{P}_0$
\end{algorithmic}
\end{algorithm}


% $T_{repeat}$: repeated times, 


% **************************** % 
%    Group Sampling 的具体算法
% **************************** %
\begin{algorithm}[htbp]
\caption{Group Sampling Strategy.}
\label{supp_alg: group_strategy}
\begin{algorithmic}[1]
\REQUIRE $\mathcal{D} \leftarrow \{{(x, y)}\}_n$: training samples, $F:  \mathcal{D} \times P \to \mathbb{R}$: score function, $\mathcal{C}$: class labels, $\text{VD}(c)$: visual descriptions of class $c$, $P_t^*$: prompt candidate with the best template, $\text{pred}(x)$: prediction for image $x$
\FORALL{class $c \in \mathcal{C}$}
    \STATE $\textsc{MisClass}(c) \leftarrow \emptyset $
\ENDFOR
\FORALL{training sample $(x, y) \in \mathcal{D}$}
    \IF{$\text{pred}(x) \neq y $}
        \STATE $\textsc{MisClass}(y) \leftarrow \textsc{MisClass}(y) \cup \{\text{pred}(x)\} $
    \ENDIF
\ENDFOR

\FORALL{class $c \in \mathcal{C}$}
    \STATE \textbf{Select Class Images}: $\textsc{Data} (c) \leftarrow \{ (x, y) \; | \; y = c\}_{(x, y) \in \mathcal{D}}$
    \STATE \textbf{Compute Accuracy}: $\textsc{Acc} (c) \leftarrow F( \textsc{Data} (c), P^*_t )$
    \STATE \textbf{Add Descriptions}: $P_{c} \leftarrow P^*_t \cup \text{VD}(c) $
    \STATE \textbf{Compute Accuracy Gain}: $\textsc{AccGain} (c) \leftarrow F( \textsc{Data} (c), P_c ) -  \textsc{Acc} (c) $
\ENDFOR
\STATE \textbf{Sort Class by Accuracy}: $\mathcal{C}_{wst}$, retaining the classes with the lowest top-$n_{wst}$ accuracy
\STATE \textbf{Sort Class by Accuracy Gain}: $\mathcal{C}_{sln}$, retaining the classes with the top-$n_{sln}$ accuracy gain
\STATE \textbf{Initialize Group Set}: $\mathcal{G} \leftarrow \emptyset$
\FORALL{class $c \in \mathcal{C}_{wst}$}
    \STATE $\mathcal{G} \leftarrow \mathcal{G} \cup \{ \textsc{MisClass}(y) \}$
\ENDFOR
\FORALL{class $c \in \mathcal{C}_{sln}$}
    \STATE $\mathcal{G} \leftarrow \mathcal{G} \cup \{ \textsc{MisClass}(y) \}$
\ENDFOR
\RETURN sampled groups $\mathcal{G}$
\end{algorithmic}
\end{algorithm}



% ---------------------------------------------------- % 
%               Group Sampling 策略的细节
% ---------------------------------------------------- %
\section{Details of Group Sampling Strategy}
\label{sec_supp: group_sampling}
The detailed group sampling strategy is summarized in Alg.~\ref{supp_alg: group_strategy}.
It contains processes of obtaining misclassified categories and selecting the worst and salient groups.

% **************************** % 
% misclassified category 获取的算法
% **************************** %
\noindent \textbf{Obtaining misclassified categories}.
In Lines 1-8 of Alg.~\ref{supp_alg: group_strategy}, we collect misclassified set for each category by $\textsc{MisClass}(\cdot)$ operator. Given an image $x$, if the prediction $\text{pred}(x)$ is not its corresponding label $y$, we will add $\text{pred}(x)$ to the misclassified set for category $y$. In fact, we also ablate the K-means clustering algorithm to group categories (in~\cref{supp_sec: more_ablation_group_sampling}). Results show that the misclassified set achieves better performance than the K-means algorithm.

% **************************** % 
%       挑选最差组别的算法
% **************************** %
\noindent \textbf{Selecting the worst groups} aims to select categories with the lowest top-$n_{wst}$ accuracy and corresponding misclassified categories. We first compute the accuracy for each category in Line 11. Then, we sort the categories by accuracy and retain the top-$n_{wst}$ worst categories in Line 15. Finally, $n_{wst}$ groups are added to the set $\mathcal{G}$ in Lines 18-20.


% **************************** % 
%       挑选显著组别的算法
% **************************** %
\noindent \textbf{Selecting the salient groups} aims to select categories with the top-$n_{sln}$ performance gains and its misclassified categories after adding descriptions. In Line 13, we compute the accuracy gains after adding the descriptions. Then, we sort the categories by accuracy gain and retain the top-$n_{sln}$ accuracy gain categories in Line 16. At last, $n_{sln}$ groups are added to the set $\mathcal{G}$ in Lines 21-23.

Finally, we collect $S = n_{wst} + n_{sln}$ groups for subsequent description optimization.




