%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} % for professional tables
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{subcaption}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\input{math_commands}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\newcommand{\ours}{\textbf{AUKT}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{algpseudocode}
\usepackage{algorithm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\newcommand{\PG}[1]{\noindent\textbf{\color{blue} *Peng: #1}}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{}

\begin{document}

\twocolumn[
\icmltitle{AUKT: Adaptive Uncertainty-Guided Knowledge Transfer\\ with Conformal Prediction}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Rui Liu}{umd}
\icmlauthor{Peng Gao}{ncsu}
\icmlauthor{Yu Shen}{adobe}
\icmlauthor{Ming Lin}{umd}
\icmlauthor{Pratap Tokekar}{umd}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
\end{icmlauthorlist}

\icmlaffiliation{umd}{University of Maryland, College Park}
\icmlaffiliation{ncsu}{North Carolina State University}
\icmlaffiliation{adobe}{Adobe Research}

\icmlcorrespondingauthor{Rui Liu}{ruiliu@umd.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]
\newcommand{\PRT}[1]{{\footnotesize\color{red}[{\bf PRT:} \textsf{#1}]}} %Pratap's comments


% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Knowledge transfer between teacher and student models has proven effective across various machine learning applications. However, challenges arise when the teacher's predictions are noisy, or the data domain during student training shifts from the teacher's pretraining data. In such scenarios, blindly relying on the teacher's predictions can lead to suboptimal knowledge transfer. To address these challenges, we propose a novel and universal framework, Adaptive Uncertainty-guided Knowledge Transfer (\ours), which leverages Conformal Prediction (CP) to dynamically adjust the student's reliance on the teacher's guidance based on the teacher's prediction uncertainty. CP is a distribution-free, model-agnostic approach that provides reliable prediction sets with statistical coverage guarantees and minimal computational overhead. This adaptive mechanism mitigates the risk of learning undesirable or incorrect knowledge. We validate the proposed framework across diverse applications, including image classification, imitation-guided reinforcement learning, and autonomous driving. Experimental results consistently demonstrate that our approach improves performance, robustness and transferability, offering a promising direction for enhanced knowledge transfer in real-world applications.
\end{abstract}
% \PRT{would be nice to have some punchline numbers ``xy\% improvement'' here}

% : when the teacher is confident, the student is guided more heavily, while less reliance is placed on the teacher when the uncertainty is high, encouraging the student to explore and learn independently.
% \vspace{-15pt}
% \PRT{Should the pink boxes be student domain? I think in the bottom one, maybe you can add a switch that shows when the "uncertainty-guided teach" signal is activated and another one that says student loss or something. Kind of to suggest that the tacher loss is not always active}

\begin{figure}[htb]
    % \vspace{-5pt}
    \centering
    \includegraphics[width=\linewidth]{figs/moti.png}
    \vspace{-10pt}
    \caption{\textbf{Comparison of standard knowledge transfer and \ours.} $f_T$ and $f_S$ denote the teacher and student models, with $f_T$ pretrained on the teacher domain. The top panel shows standard knowledge transfer, where the student strictly follows the teacher, often leading to suboptimal performance under domain shifts. The bottom panel illustrates \ours, which leverages conformal prediction to adjust teacher guidance based on prediction uncertainty, balancing teacher knowledge exploitation with student exploration for improved transferability.}
    \label{fig:moti}
    \vspace{-20pt}
\end{figure}

\vspace{-15pt}
\section{Introduction}
\vspace{-5pt}
In many machine learning applications, knowledge transfer \cite{hinton2015distilling, romero2014fitnets, zagoruyko2016paying, passalis2018learning, kim2018paraphrasing, wang2023prototype, xue2022modality, huo2024c2kd, gu2023knowledge, jin2023multi, sun2024logit} from teacher models to student models is a valuable strategy that allows improved performance and practical deployment in real-world scenarios. The teacher models, composed of larger networks or access to multi-modalities, often exhibit superior performance but come with significant computational costs, making them unsuitable for resource-constrained environments. In contrast, student models are more lightweight \cite{gu2023knowledge} or designed to work with reduced data modalities \cite{shen2023auxiliary} during testing, making them practical for real-world applications. The teacher-student framework allows the teacher models to provide predictions or insights that guide the student models' learning process. This can involve transferring knowledge from a larger network to a smaller one \cite{hinton2015distilling, romero2014fitnets, zagoruyko2016paying}, adapting multimodal teacher models to unimodal student models \cite{shen2023auxiliary, wang2023prototype, xue2022modality, huo2024c2kd}, or using pretrained imitation learning (IL) policies to bootstrap reinforcement learning (RL) agents \cite{hu2023imitation, bhaskar2024planrl}. 

Traditional teacher-student frameworks often assume that the teacher’s predictions are always reliable, using them as guidance for the student's learning. However, a key challenge arises when domain shifts occur between the data used for pretraining the teacher and the data available for training the student. For instance, in transferring knowledge from large language models (LLMs) to student models for downstream tasks \cite{yang2024self, saad2023udapdr}, the student’s domain data may differ significantly from the pretraining data of the LLMs. Addressing this disparity typically requires retraining or fine-tuning the teacher model on the student’s domain \cite{yang2024self, saad2023udapdr}, but this process might be computationally expensive or impractical. For pretrained teacher models, domain shifts can introduce error in the teachers' predictions, undermining their reliability. Blindly relying on the teacher in these scenarios will lead to suboptimal knowledge transfer, as the student may overfit to incorrect or uncertain guidance. This over-reliance restricts the student's ability to explore independently and adapt effectively to new or noisy environments. These limitations are especially problematic in real-world applications, where dynamic and uncertain conditions are commonplace. Addressing such issues is crucial for improving the performance and transferability of knowledge transfer systems. 

% \PG{We can merge the following two paragraph into one to discuss the approach. In addition, we can move all references to above, but only discuss our approach below. The purpose is to show that AUKT is a totally new approach with new design, not a combination of two existing methods.}

Conformal prediction (CP) \cite{angelopoulos2020uncertainty, angelopoulos2021gentle, mossina2024conformal, karimi2023quantifying, tibshirani2019conformal, shafer2008tutorial, vovk2020conformal} is a non-parametric, distribution-free, and model-agnostic framework that provides reliable prediction sets with statistical coverage guarantees, enabling robust decision-making under data or model uncertainty. Leveraging CP, we propose a novel framework, Adaptive Uncertainty-Guided Knowledge Transfer (\ours), to address key challenges in knowledge transfer systems. We show a comparison between conventional knowledge transfer methods and \ours~in Fig. \ref{fig:moti}. \ours~quantifies the teacher’s prediction uncertainty and uses it to dynamically determine the extent to which the student should rely on the teacher’s guidance. Specifically, when the teacher exhibits high confidence in its predictions, the student prioritizes the teacher's guidance. Conversely, when the confidence is low, the student reduces its reliance on the teacher and explores the data more independently, potentially discovering new patterns or adapting better to noisy or domain-shifted data. \ours~is a general framework applicable to diverse teacher-student architectures, including model compression (larger to smaller networks), modality reduction (multimodal to unimodal networks), and imitation-to-reinforcement learning (pretrained policies to RL agents). We validate \ours~across diverse tasks, including image classification, imitation-guided reinforcement learning, and autonomous driving. 

Here, we ask the question: \textit{Can we improve the student model’s performance beyond standard knowledge transfer methods when there are domain shifts between the teacher’s pretraining data and the student’s training data, or when the teacher’s predictions are noisy?} Additionally, there can be cases of teacher underperformance where the pretrained teacher model performs worse than the student model trained from scratch on the student domain data, likely due to significant domain shifts. In such cases, we investigate: \textit{Can the teacher model still provide useful knowledge to improve the student model's performance beyond what is achievable when training from scratch?} This investigation especially differentiates \ours~from previous knowledge transfer methods, which often assume that the teacher's performance exceeds that of the student model.

% \PG{Maybe we can merge the following paragraph with the contributions, seem to discuss the same thing.} By adaptively adjusting the teacher's influence based on its confidence, \ours~ensures effective student learning without over-relying on potentially unreliable guidance, especially in noisy or domain-shifted scenarios. Leveraging conformal prediction for uncertainty quantification, our framework provides a robust and computationally efficient mechanism for guiding the student. Additionally, \ours~is universal and domain-agnostic, making it applicable to diverse tasks such as image classification, reinforcement learning, and autonomous driving. Notably, \ours~can still leverage useful "dark knowledge" to improve student performance, even when the teacher model performs worse than the student due to large domain shifts, as demonstrated in image classification experiments. This is especially valuable, as conventional knowledge transfer methods often assume that the teacher’s performance will always exceed that of the student. 

Unlike traditional teacher-student frameworks, our approach offers several unique advantages. We summarize the key contributions of our work as follows:
\begin{itemize}[left=0pt]
    \vspace{-10pt}
    \item We propose \ours, a novel framework that leverages conformal prediction to efficiently quantify the uncertainty in the teacher's predictions for knowledge transfer systems. By adaptively adjusting the teacher's influence based on its confidence, \ours~ensures effective student learning without over-relying on unreliable guidance, particularly in domain-shifted scenarios.
    \vspace{-6pt}
    \item \ours~demonstrates the ability to still leverage useful ``dark knowledge'' from the teacher model to improve student performance, even when the teacher is underperforming due to significant domain shifts. This is a notable distinction from conventional knowledge transfer methods, which assume that the teacher always performs better than the student.
    \vspace{-6pt}
    \item \ours~is universal and domain-agnostic, making it applicable to diverse tasks. We validate \ours~across a range of applications, including image classification, imitation-guided reinforcement learning and autonomous driving, demonstrating improvements in performance, as well as enhanced robustness and transferability compared to conventional knowledge transfer methods.%\PRT{give some punchline numbers}
\end{itemize}
% \PRT{I think you can highlight the last part a bit more in the intro somewhere. That AUKT is a general framework, we show how to use it in both supervised and reinforcement learning settings with minimal changes. }

% , regardless of whether the teacher-student structures are homogeneous or heterogeneous
% In real-world applications, effective knowledge transfer from a pretrained teacher model on a source domain to a student model on a target domain is highly valuable, particularly when deploying the teacher model is impractical due to computational or resource constraints, or when certain data modalities are unavailable during testing. 

% The proposed framework is designed to be universal and can handle diverse applications.

% This framework integrates uncertainty quantification with adaptive knowledge transfer, providing a principled approach for teacher-student learning.

% These scenarios underscore the value of uncertainty-guided knowledge transfer in addressing limitations of traditional approaches, which often assume consistently superior teacher performance. By systematically leveraging teacher uncertainty and conformal prediction, \ours~ensures that knowledge transfer remains effective even under challenging conditions.

% This is often the case when the teacher model is a larger network or benefits from access to additional data modalities.

% The novelty of this approach lies in its dynamic adjustment of teacher guidance based on the teacher’s uncertainty. This is particularly advantageous in scenarios where the teacher’s knowledge does not generalize well to all input domains or edge cases. By incorporating conformal prediction, the framework ensures robust and principled uncertainty quantification, while the adaptive loss formulation promotes efficient student learning even in diverse and challenging settings.

% In conclusion, this framework goes beyond traditional knowledge distillation by integrating uncertainty-aware modulation, which allows for a more nuanced and effective transfer of knowledge. The combination of robust uncertainty quantification and adaptive student learning creates a flexible and powerful approach for scenarios characterized by variability, domain shifts, or noisy data. By dynamically balancing teacher guidance and exploration, the framework establishes a foundation for more robust and versatile knowledge transfer methods in real-world applications.
% \vspace{-13pt}
\section{Related Work}
\subsection{Knowledge Transfer}
\vspace{-5pt}
Knowledge transfer methods aim to adapt information from one network, modality, or learning paradigm to another. For instance, \citet{hinton2015distilling, jin2023multi, sun2024logit} focused on transferring soft probabilities from the teachers' logits to guide student models, whereas \citet{romero2014fitnets, zagoruyko2016paying, passalis2018learning, kim2018paraphrasing} emphasized transferring intermediate features from the teacher to the student. These approaches primarily address knowledge transfer from larger teacher networks to smaller student networks. Cross-modality transfer has also been extensively explored. \citet{wang2023prototype} introduced a prototype-based distillation method to handle missing modalities in medical image segmentation, where a multi-modality teacher guides a single-modality student. Similarly, \citet{feng2023cekd} distilled knowledge from an RGB-Thermal teacher model to a Thermal-only student model for semantic scene understanding.  \citet{shen2023auxiliary} proposed Auxiliary Modality Learning (AML), where a teacher model with access to multiple modalities transfers knowledge to a student model that operates on reduced modalities during testing. Additionally, knowledge transfer across different learning paradigms has been investigated, such as in \cite{hu2023imitation, bhaskar2024planrl}, where pretrained imitation learning (IL) policies were used to guide reinforcement learning (RL) agents. 

A limitation of most existing knowledge transfer frameworks is their reliance on static guidance, which assumes the teacher’s predictions are consistently reliable and always superior to the student’s. This assumption often fails under significant domain shifts, where the teacher’s predictions may become unreliable, ultimately hindering the student’s learning. In contrast, \ours~introduces a principled approach to uncertainty-aware guidance that adaptively adjusts the teacher's influence based on its prediction uncertainty, ensuring that the student learns effectively without over-relying on potentially misleading guidance.

% \PRT{there is no work on knowledge transfer when teacher isn't perfect?}

% \vspace{-6pt}
\subsection{Uncertainty-Aware Learning}
\vspace{-5pt}
Quantifying uncertainty \cite{abdar2021review, kwon2020uncertainty, karimi2023quantifying} in machine learning systems has become a critical aspect, especially in safety-critical domains. For example, \citet{edupuganti2020uncertainty} quantified uncertainty in MRI reconstruction with deep learning models, while \citet{kwon2020uncertainty} employed Bayesian neural networks for uncertainty estimation in medical image classification. Similarly, \citet{michelmore2020uncertainty} evaluated uncertainty in end-to-end Bayesian controllers for autonomous driving, \cite{gao2023uncertainty, gao2021bayesian} leverage uncertainty quantification in human-robot interaction, and \citet{zhao2024leveraging} utilized Monte Carlo dropout for real-time uncertainty quantification in object detection for autonomous vehicles.

Despite the progress in uncertainty-aware learning, most prior works focus on quantifying uncertainty in standalone models, with less exploration of its integration into teacher-student knowledge transfer frameworks. In this context, uncertainty remains underexplored as a mechanism to modulate the interaction between teacher and student models dynamically. To bridge this gap, our approach leverages conformal prediction as a principled mechanism for uncertainty quantification. By integrating uncertainty awareness into the learning process, our method adaptively adjusts the teacher's guidance, enabling the student to learn effectively while reducing the risk of overfitting to unreliable teacher predictions.

% \vspace{-6pt}
\subsection{Conformal Prediction}
\vspace{-5pt}
Conformal prediction (CP) \cite{angelopoulos2020uncertainty, angelopoulos2021gentle, shafer2008tutorial} is a non-parametric, distribution-free, and model-agnostic framework designed to provide reliable prediction sets with statistical coverage guarantees. In machine learning systems, CP has primarily been utilized for post-hoc uncertainty calibration due to its computational efficiency. For instance, \citet{angelopoulos2020uncertainty} introduced an algorithm that adapts any image classifier to output predictive sets containing the true label with a user-specified probability. \citet{mossina2024conformal} proposed a computationally lightweight approach to quantify predictive uncertainty in semantic image segmentation using CP. Similarly, \citet{lu2022improving} applied CP to deep learning models for grading the severity of spinal stenosis in lumbar spine MRI, while \citet{karimi2023quantifying} leveraged CP to measure uncertainty in deep learning models.

Despite these advancements, the application of CP in dynamic decision-making frameworks, such as adaptive knowledge transfer, remains underexplored. In this work, we extend CP to the domain of knowledge transfer, utilizing it as a foundation for adaptive, uncertainty-guided learning. By quantifying the teacher model’s prediction uncertainty, we enable the student model to dynamically adjust its reliance on the teacher, effectively balancing the integration of pretrained knowledge with independent exploration.

\vspace{-5pt}
\section{Approach}
\subsection{Preliminaries} \label{sec:pre}
\vspace{-5pt}
In our framework, we leverage Conformal Prediction (CP) to quantify the teacher's prediction uncertainty and guide knowledge transfer. CP is a distribution-free method that provides prediction sets or intervals with guaranteed coverage levels, independent of the underlying model \cite{angelopoulos2021gentle, shafer2008tutorial}.

CP uses a nonconformity score $s$ to measure how unusual a prediction is for a new test input, based on a calibration set, a held-out dataset used to compute the empirical distribution of nonconformity scores. The score $s$ can be defined in various ways, such as residuals (e.g., $|\Bar{y} - \hat{y}(\Bar{x})|$ for regression) or confidence scores (e.g., $1 - p_{\hat{y}}(\Bar{x})$ for classification), where $\Bar{y}$ is the ground truth and $\hat{y}(\Bar{x})$ is the model's prediction for an input $\Bar{x}$ in the calibration set. Additional examples can be found in \cite{angelopoulos2021gentle, shafer2008tutorial}. From the calibration set, the $(1-\alpha)$ quantile $q_{1-\alpha}$ of the nonconformity scores is computed, representing the threshold below which $1-\alpha$ of the data falls, where $\alpha$ is the allowable error rate. A prediction set for a test input $x_{\text{test}}$ includes all labels $y$ satisfying $s(x_{\text{test}}, y) \leq q_{1-\alpha}$, ensuring a coverage probability of $1-\alpha$, assuming the calibration and test data are from the same distribution \cite{angelopoulos2021gentle}.

\vspace{-5pt}
\subsection{Problem Definition}
\vspace{-5pt}
We consider a knowledge transfer problem where a student model learns from a teacher model. Conventional knowledge transfer methods often assume consistently superior teacher performance. However, knowledge transfer becomes challenging when domain shifts occur, leading to uncertainty in the teacher model's prediction and suboptimal knowledge transfer.

% \PG{$D_T$ is never used in the approach section}
To address these challenges, we propose an Adaptive Uncertainty-Guided Knowledge Transfer (\ours) framework to improve the robustness and transferability of knowledge transfer systems. Formally, let the teacher domain data be denoted as $\mathcal{D}_T$ with distribution $P_T$, and the student domain data as $\mathcal{D}_S$ with distribution $P_S$. We represent the teacher model pretrained on $\mathcal{D}_T$ as $f_T: \mathcal{X} \rightarrow \mathcal{Y}$, and the student model as $f_S: \mathcal{X} \rightarrow \mathcal{Y}$, where $\mathcal{X}$ is the input space and $\mathcal{Y}$ is the output space. In this framework, we do not retrain or finetune the teacher model $f_T$ on the student domain $\mathcal{D}_S$, as this can be computationally prohibitive. Instead, we aim to transfer the knowledge from $f_T$ to $f_S$, with the goal of improving the performance of $f_S$ on $\mathcal{D}_S$ compared to conventional knowledge transfer approaches or training the student model from scratch. This is particularly important when $\mathcal{D}_S$ differs from $\mathcal{D}_T$ (i.e., $P_S\neq P_T$), making $f_T(x)$ uncertain for input $x\in \mathcal{D}_S$.

\vspace{-8pt}
\subsection{Adaptive Uncertainty-Guided Knowledge Transfer}
\vspace{-5pt}
Our approach aims to improve knowledge transfer by leveraging the prediction uncertainty of the teacher model to guide the student's learning process, especially in the presence of domain shifts. Given a student domain $\mathcal{D}_S$, we split it into three subsets: the student training set $\mathcal{D}_{\text{train}}^S$, used to train the student model $f_S$; the calibration set $\mathcal{D}_{\text{cal}}$, used to transform any heuristic measure of uncertainty from the pretrained teacher model $f_T$ into a rigorous one; and the testing set $\mathcal{D}_{test}$ for validate the model performance. The student training set $\mathcal{D}_{\text{train}}^S$ also serves as the testing set for $f_T$ in CP-based uncertainty quantification. This is because in the knowledge transfer process, both the teacher and student models take the same input data, with the student learning from the teacher's predictions. We assume that $\mathcal{D}_{\text{train}}^S$ and $\mathcal{D}_{\text{cal}}$ come from the same distribution, satisfying the assumption of CP. \ours~is a general framework, we show how to use it in both supervised and imitation-guided reinforcement learning settings with minimal changes.

% \PRT{might need a brief intro to CP that introduces what calibration set means before this section.}

\begin{figure}[htb]
    \vspace{-5pt}
    \centering
    \includegraphics[width=0.8\linewidth]{figs/app.png}
    % \vspace{-8pt}
    \caption{\textbf{AUKT Approach Pipeline.} 
    The figure demonstrates our approach, \ours, which dynamically adjusts the teacher's guidance based on its prediction uncertainty using conformal prediction. This method balances the exploitation of teacher knowledge with student exploration itself, enabling better transferability of knowledge under domain shifts.}
    \label{fig:app}
    \vspace{-10pt}
\end{figure}

% \PG{In the following sentence, we use 4 different terms including nonconformity score, residuals, confident score, quantile of scores, too many new terms without explanation. I suggest to explain them one by one or use consistent terms. Maybe add a preliminary to explain CP first.}
\vspace{-6pt}
\subsubsection{Supervised Learning} \label{sec:sup}
\vspace{-5pt}
Given a pretrained teacher model $f_T$ and the calibration set $\mathcal{D}_{\text{cal}}$, for each sample $(\Bar{x}_i, \Bar{y}_i) \in \mathcal{D}_{\text{cal}}$, we compute a nonconformity score $s_i$, where $s_i$ can be residuals (e.g., $|\Bar{y}_i - \hat{y}(\Bar{x}_i)|$ for regression) or confidence scores (e.g., $1 - p_{\hat{y}}(\Bar{x}_i)$ for classification), as described in Section \ref{sec:pre}, depending on the task. Using a predefined coverage level $1-\alpha$, we compute the $(1-\alpha)$-quantile of these scores, denoted as $q_{1-\alpha} = \text{Quantile}_{1-\alpha}(s_1, s_2, \ldots, s_{|\mathcal{D}_{\text{cal}}|})$, which serves as a threshold for constructing prediction sets. For a test input $x_{test}$, we define the prediction set for the teacher model $f_T$ as: $\mathcal{C}(x_{test}) = \{y \in \mathcal{Y} : s(x_{test}, y) \leq q_{1-\alpha}\}$, with the probability of the true label $y_{test}$ falling within $\mathcal{C}(x_{test})$ satisfies $P(y_{test}\in \mathcal{C}(x_{test})) \geq 1-\alpha$. To quantify uncertainty, we use the size of the prediction set $\mathcal{C}(x)$ for an input $x\in \gD_{train}^S$. Specifically, we define the teacher model's uncertainty as:
\vspace{-7pt}
\begin{equation} \label{eq: ut}
\vspace{-5pt}
    u_T(x) = g(|\mathcal{C}(x)|),
\vspace{-5pt}
\end{equation}
where $g$ is a function that maps the size of the prediction set (e.g., the number of classes for classification or the interval length for regression) to an uncertainty value. Then we define the loss function of the adaptive uncertainty-guided knowledge transfer, which is
\begin{equation} \label{eq: loss}
\vspace{-4pt}
    \gL = \lambda_1 \gL_S + w(x)\cdot\lambda_2 \gL_T,
\vspace{-4pt}
\end{equation}
where $\gL_S$ is the student's task loss (e.g., Cross Entropy loss), $\gL_T$ is the teacher guidance loss (e.g., KL divergence between student and teacher logits), $\lambda_1$ and $\lambda_2$ are the coefficients, and $w$ is an uncertainty-based weight derived from the teacher model’s uncertainty $u_T$: 
\vspace{-5pt}
\begin{equation} \label{eq: weight}
\vspace{-3pt}
    w(x) = h(u_T(x)),
\vspace{-3mm}
\end{equation}
where $h$ is a function mapping uncertainty to the weight $w$. $w$ approaches 1 when the teacher’s predictions are confident and decreases toward 0 as the uncertainty increases. We show the algorithm of \ours~in the Appendix \ref{app:algo}. We also provide a theoretical analysis to explain why \ours~outperforms standard knowledge transfer methods, please see the Appendix \ref{app:the}. 

Our approach dynamically adjusts the student's reliance on the teacher based on prediction uncertainty. When the teacher is confident, the student follows its guidance, but as uncertainty rises, the student explores more independently. This balance between exploiting teacher knowledge and encouraging exploration helps the student avoid over-reliance on uncertain predictions and generalize effectively, particularly in cases of domain shifts where the teacher's pretrained knowledge may not transfer well. This ensures robust and reliable learning outcomes.

% Our approach allows the student's reliance on the teacher to adapt dynamically based on the teacher's prediction uncertainty. When the teacher is confident, the student relies more heavily on the teacher's guidance. Conversely, as the teacher's uncertainty increases, the student is encouraged to explore and learn more independently. This adaptive mechanism effectively balances the exploitation of teacher knowledge and exploration by the student itself, ensuring that the student avoids over-reliance on uncertain predictions and can generalize effectively to challenging inputs. This is particularly critical in scenarios where domain shifts hinder the transferability of the teacher’s pretrained knowledge to the student’s domain, thereby ensuring robust and reliable learning outcomes.

\vspace{-3pt}
\subsubsection{Imitation-Guided Reinforcement Learning} \label{sec:rl}
\vspace{-5pt}
Consider a Markov Decision Process (MDP) defined by the tuple $\{\gS, \gA, \gP, \gR, \gamma\}$, where $\gS$ is the state space, $\gA$ is the action space, $\gP$ is the transition dynamics, $\gR$ is the reward function, and $\gamma$ is the discount factor. We focus on off-policy RL methods due to their higher sample efficiency, leveraging past experiences and expert demonstrations.

To quantify uncertainty, we define a nonconformity score $s(s,a) = -\log \pi(a|s)$, measuring how ``atypical" an action $a$ is in state $s$ for a policy $\pi$. Given a teacher model $f_T$ (e.g., a pretrained imitation policy), we first collect calibration set $\gD_{cal}=\{(s_i, a_i)\}_{i=1}^n$ by rolling out $f_T$ in the student domain. For a predefined coverage level $1-\alpha$, we compute the $(1-\alpha)$-quantile $q_{1-\alpha}$ of the nonconformity scores on $\gD_{cal}$. For a test state $s_{test}$, we construct a conformal prediction set $\mathcal{C}(s_{test}) = \{a \in \mathcal{A} : s(s_{test}, a) \leq q_{1-\alpha}\}$, ensuring $P(a_{test}\in \mathcal{C}(s_{test})) \geq 1-\alpha$. Unlike prior setups where only the teacher model is calibrated, here we calibrate both teacher model $f_T$ and student model $f_S$. Since $f_T$ is pretrained, its quantile $q_{1-\alpha}^T$ remains static, while the student's quantile $q_{1-\alpha}^S$ is updating online during training. Using the corresponding prediction sets, the teacher and student uncertainties are defined as $u_T(s) = g(|\mathcal{C}_T(s)|)$ and $u_S(s) = g(|\mathcal{C}_S(s)|)$, respectively.

Similar to supervised learning, the total loss function is defined as: $\gL = \gL_S + w(s)\cdot \gL_T$, where $\gL_S$ is the student’s task loss (e.g., $\mathbb{E}\left[ -\log f_S(a|s) \cdot A(s, a)\right]$ with $A(s, a)$ as the advantage function), which corresponds to the RL objective. $\gL_T = \E\left[\text{KL}(f_S(\cdot|s) \parallel f_T(\cdot|s)) \right]$ is the KL divergence between the student's policy and the teacher's, serving as the teacher guidance loss. The weight $w$, defined as: $w(s) = h(u_T(s), u_S(s))$, is similar to Eq. \ref{eq: weight} but incorporates the uncertainties of both teacher and student. 

% \PRT{I think since you are showing applications in supervised and in reinfrocement leanring, it woul dbe nice to add a bit more info on how equation 5 is used in both settings. in supervised applications, I'm assuming you first run calib, get alpha, then use some specific Ls Lt terms. In reinforcement, do you also first run calib or are quantiles computed online?, also in rl what are Ls Lt terms, you don't mention the connection with imitation learning. I think it would be nice to have a separate subsection showing how equation 5 gets adapted in both settings. Will also add a bit more meat to the paper}


% \section{Theoretical Analysis}
% A critical aspect of our approach is the interpretability and robustness afforded by conformal prediction. Unlike heuristic uncertainty measures, conformal prediction provides statistical guarantees, ensuring that the prediction sets C(x)C(x) achieve a desired coverage probability 1. This theoretical underpinning translates into reliable uncertainty estimates, which in turn drive the adaptive learning process.

% The combined loss ensures that the student optimally balances exploitation of the teacher’s knowledge and exploration of the data distribution. By using w(x)w(x) as a function of uT(x), we effectively solve the exploration-exploitation trade-off.

% To better understand the impact of uncertainty weighting, consider the gradient of the student loss with respect to the model parameters S. The total gradient is
% \PRT{table formatting is very hard to follow. Can you also add the color coding like in the earlier KT paper you showed me (give citation).. like we highlight in orange deltas greater than 0.3 following the protocol in []}

\begin{table*}[htb]
\caption{Top-1 accuracy (\%) of various knowledge transfer methods with and without \ours~on CIFAR-100 under homogeneous structure and domain shift Level 1. We use $\Delta$ to show performance gain relative to conventional knowledge transfer methods without \ours. We highlight in orange deltas greater than 0.15, indicating non-trivial enhancement following the protocol in \cite{sun2024logit}.}
\centering
\resizebox{0.8\textwidth}{!}{
\vspace{-5pt}
\begin{tabular}{lcccccc}
\hline \multirow{2}{*}{Teacher} & ResNet110 & ResNet56 & ResNet32 $\times 4$ & VGG13 & WRN-40-2 & WRN-40-2 \\
& 68.09 & 65.80 & 73.01 & 70.65 & 69.38 & 69.38 \\
Student & ResNet20 & ResNet20 & ResNet8 $\times 4$ & VGG8 & WRN-40-1 & WRN-16-2 \\
& 67.70 & 67.70 & 70.20 & 68.07 & 69.33 & 71.40 \\
\hline 
KD \cite{hinton2015distilling} & 67.85 & 67.58 & 70.27 & 69.34 & 69.42 & 70.80 \\
KD + \ours & \bf{68.33} & 67.65 & 70.87 & \bf{69.38} & \bf{70.47} & 71.79 \\
$\Delta$ & \color{orange}{0.46} & 0.07 & \color{orange}{0.60} & 0.04 & \color{orange}{1.05} & \color{orange}{0.89} \\
\hline 
FitNet \cite{romero2014fitnets} & 67.81 & 67.48 & 70.38 & 67.68 & 69.47 & 71.07 \\
FitNet + \ours & 68.22 & \bf{67.82} & \bf{71.22} & 68.00 & 69.51 & 71.32 \\
$\Delta$ & \color{orange}{0.41} & \color{orange}{0.34} & \color{orange}{0.84} & \color{orange}{0.32} & 0.04 & \color{orange}{0.25} \\
\hline
PKT \cite{passalis2018learning} & 67.50 & 67.33 & 70.51 & 67.90 & 69.46 & 71.29 \\
PKT + \ours & 67.89 & 67.64 & 71.11 & 68.08 & 70.03 & 71.56 \\
$\Delta$ & \color{orange}{0.39} & \color{orange}{0.31} & \color{orange}{0.60} & \color{orange}{0.18} & \color{orange}{0.57} & \color{orange}{0.27} \\
\hline
FT \cite{kim2018paraphrasing} & 67.37 & 67.25 & 70.09 & 67.90 & 69.64 & 71.23 \\
FT + \ours & 67.97 & 67.37 & 70.73 & 68.07 & 70.00 & \bf{71.84} \\
$\Delta$ & \color{orange}{0.60} & 0.12 & \color{orange}{0.64} & \color{orange}{0.17} & \color{orange}{0.36} & \color{orange}{0.61} \\
\hline
\end{tabular}
}
\label{tab:homo}%
\vspace{-10pt}
\end{table*}%

\begin{table*}[htb]
\caption{Top-1 accuracy (\%) of various knowledge transfer methods with and without \ours~on CIFAR-100 under homogeneous structure and domain shift Level 2 where the teacher models are underperforming. We use $\Delta$ to show performance gain relative to conventional knowledge transfer methods without \ours. We highlight in orange deltas greater than 0.15, indicating non-trivial enhancement following the protocol in \cite{sun2024logit}.}
\centering
\resizebox{0.8\textwidth}{!}{
\vspace{-5pt}
\begin{tabular}{lcccccc}
\hline \multirow{2}{*}{Teacher} & ResNet110 & ResNet56 & ResNet32 $\times 4$ & VGG13 & WRN-40-2 & WRN-40-2 \\
& 58.78 & 56.23 & 62.61 & 61.47 & 58.76 & 58.76 \\
Student & ResNet20 & ResNet20 & ResNet8 $\times 4$ & VGG8 & WRN-40-1 & WRN-16-2 \\
& 66.70 & 66.70 & 69.14 & 67.08 & 69.12 & 70.59 \\
\hline 
KD \cite{hinton2015distilling} & 65.96 & 65.56 & 68.06 & 66.51 & 68.80 & 69.06 \\
KD + \ours & 67.47 & \bf{67.39} & 68.92 & 67.31 & \bf{69.73} & 70.82 \\
$\Delta$ & \color{orange}{1.51} & \color{orange}{1.83} & \color{orange}{0.86} & \color{orange}{0.80} & \color{orange}{0.93} & \color{orange}{1.76} \\
\hline 
FitNet \cite{romero2014fitnets} & 66.60 & 66.20 & 68.97 & 66.59 & 69.03 & 70.15 \\
FitNet + \ours & 67.13 & 66.98 & 69.41 & 67.21 & 69.31 & 71.26 \\
$\Delta$ & \color{orange}{0.53} & \color{orange}{0.78} & \color{orange}{0.44} & \color{orange}{0.62} & \color{orange}{0.28} & 0.11 \\
\hline
PKT \cite{passalis2018learning} & 66.58 & 66.13 & 69.32 & 67.08 & 68.83 & 70.34 \\
PKT + \ours & \bf{67.58} & 66.68 & 69.66 & 67.27 & 69.28 & 70.65 \\
$\Delta$ & \color{orange}{0.20} & \color{orange}{0.55} & \color{orange}{0.34} & \color{orange}{0.19} & \color{orange}{0.45} & \color{orange}{0.31} \\
\hline
FT \cite{kim2018paraphrasing} & 66.62 & 66.56 & 69.04 & 67.43 & 68.63 & 70.14 \\
FT + \ours & 66.73 & 67.12 & \bf{69.66} & \bf{67.60} & 69.13 & \bf{71.33} \\
$\Delta$ & 0.11 & \color{orange}{0.27} & \color{orange}{0.62} & \color{orange}{0.17} & \color{orange}{0.50} & \color{orange}{1.19} \\
\hline
\end{tabular}
}
\label{tab:under}%
\vspace{-10pt}
\end{table*}%

% \vspace{-8pt}
\section{Experiments}
\vspace{-5pt}
To validate our approach, we conduct experiments across a diverse range of applications, including image classification, imitation-guided reinforcement learning, and autonomous driving.

% \vspace{-6pt}
\subsection{Image Classification}
\vspace{-5pt}
In image classification, we evaluate the effectiveness of our framework in enhancing predictive performance compared to traditional knowledge transfer approaches. This evaluation is performed under different levels of domain shifts and noise. Specifically, we aim to address the two primary research questions: (1) Can we improve the performance of the student model beyond standard knowledge transfer techniques? (2) When the teacher model is underperformance, can it still provide useful ``dark knowledge'' to enhance the performance of the student model beyond what is achievable by training from scratch?

\vspace{-5pt}
\paragraph{Experimental Settings.}
We conduct our experiments on the CIFAR-100 dataset \cite{krizhevsky2009learning}, which consists of 60K images, 50K for training and 10K for testing, across 100 distinct categories. In this section, we focus on knowledge transfer under two distinct settings: \textbf{Homogeneous Structure}, where both the teacher and student models share the same type of architecture (e.g., ResNet-32x4 and ResNet-8x4), and \textbf{Heterogeneous Structure}, where the teacher and student models are of different architectures (e.g., ResNet-32x4 and ShuffleNet-V1). We evaluate a wide range of neural network architectures, including ResNet \cite{he2016deep}, WRN \cite{zagoruyko2016wide}, VGG \cite{simonyan2014very}, ShuffleNet-V1 \cite{zhang2018shufflenet}/V2 \cite{ma2018shufflenet}, and MobileNet-V2 \cite{sandler2018mobilenetv2}. We introduce two levels of domain shifts. In Level 1, the domain shift is relatively mild with Gaussian noise of zero mean and a standard deviation
of 0.03. In Level 2, the shift is more pronounced with noise standard deviation increased to 0.05, which may lead to underperformance of the teacher model, where its performance is worse than that of the student model trained from scratch on the student training set. For further details on these two levels of domain shifts, please refer to the Appendix \ref{app:img_noise}.

% \PRT{I think full details in appendix is fine but you should give the noise value for Level 1 and 2 here. 0.03 and ??}

To quantify the uncertainty of the pretrained teacher model $f_T$ using conformal prediction, we utilize the RAPS algorithm as described in \cite{angelopoulos2020uncertainty} with an error rate of $\alpha=0.1$. For more details on the RAPS algorithm, please refer to \cite{angelopoulos2020uncertainty}. Given an input image $x$, we obtain the prediction set $|\mathcal{C}(x)|$ where the teacher model provides a set of candidate predictions. The prediction uncertainty of $f_T$ in Eq. \ref{eq: ut} is then defined as $u_T(x) = \frac{|\mathcal{C}(x)|-1}{K-1}$ \cite{vovk2016criteria}, where $K$ is the total number of classes, which is 100 for the CIFAR-100 dataset. From this uncertainty definition, $u_T$ takes values between 0 and 1. If $f_T$ is confident with its prediction with a single class, $u_T$ will be 0. Conversely, if the prediction is uncertain and contains multiple classes in the prediction set, $u_T$ is larger than 0. To incorporate this uncertainty into the knowledge transfer process, we define the uncertainty-based weight in Eq. \ref{eq: weight} as $w=1$ if $u_T=0$, $w=0$ if $u_T>0$ \cite{vovk2016criteria}.
% \begin{equation} \label{eq: aukt}
% \vspace{-2pt}
%     w = 
%     \begin{cases} 
%         1 & \text{if } u_T=0, \\
%         0 & u_T > 0.
%     \end{cases}
% \vspace{-2pt}
% \end{equation}
This weight helps adjust the influence of the teacher's prediction on the student model, ensuring that confident predictions are more heavily relied upon while uncertain predictions have less influence. We follow the same experimental settings as in previous work \cite{tian2019contrastive, sun2024logit} for the coefficients $\lambda_1$ and $\lambda_2$, as well as other training details. Please refer to the Appendix \ref{app:img_train} for more details. 

\vspace{-10pt}
\paragraph{Baselines.} 
We evaluate the Top-1 accuracy of multiple baseline methods, comparing their performance with and without the integration of \ours. The baselines include KD \cite{hinton2015distilling}, FitNet \cite{romero2014fitnets}, PKT \cite{passalis2018learning}, and FT \cite{kim2018paraphrasing}.

% For all knowledge transfer methods, we set the coefficients $\lambda_1$ and $\lambda_2$ in Eq. \ref{eq: loss} to 1, giving equal weight to the student task loss $\gL_S$ and the teacher guidance loss $\gL_T$. Here, $\gL_S$ corresponds to the cross-entropy loss, and $\gL_T$ varies based on the each knowledge transfer approach. 

\vspace{-8pt}
\paragraph{Experimental Results.}
We first analyze the results of the homogeneous teacher-student framework under domain shift Level 1. As observed in Table \ref{tab:homo}, the Top-1 accuracy of all knowledge transfer methods improves when combined with \ours, demonstrating the effectiveness of \ours~in enhancing model performance compared to conventional knowledge transfer techniques. Notably, in the ResNet56-ResNet20 case, the pretrained teacher performs worse than a student trained from scratch, leading to suboptimal performance after knowledge transfer. This highlights a key limitation of traditional knowledge transfer methods, which fail to account for teacher uncertainty, potentially resulting in degraded student performance. However, when combined with \ours, all methods achieve better results, with FitNet \cite{romero2014fitnets} even surpassing the performance of a student trained from scratch.

To further validate \ours, particularly in scenarios where the teacher model underperforms, we evaluate it under a more significant domain shift, Level 2. Traditional knowledge transfer methods typically assume that the teacher model is both reliable and superior to the student. However, as shown in Table \ref{tab:under}, when the teacher performs worse than the student under a larger domain shift, these methods often lead to degraded performance, resulting in students that perform worse than those trained from scratch. In contrast, integrating \ours~not only improves model performance but also enables the student to surpass the performance of a student trained from scratch. These findings highlight \ours’s effectiveness in enhancing knowledge transfer, even when the teacher is unreliable, and demonstrate its superior adaptability to domain shifts. By selectively leveraging useful knowledge while avoiding misleading supervision, \ours~ensures robust student learning, allowing for better transferability of knowledge in challenging scenarios.

Finally, as part of our ablation studies, we evaluate the performance of a heterogeneous teacher-student framework and present the results in Table \ref{tab:hete} in the Appendix. The table shows that, for all knowledge transfer methods, performance improves when combined with \ours, further validating the effectiveness of our approach across different teacher-student structures.

\vspace{-8pt}
\subsection{Imitation-Guided Reinforcement Learning}
% \vspace{-5pt}
For imitation-guided reinforcement learning, we demonstrate how \ours~improves policy learning efficiency and knowledge transferability in challenging and unseen environments when domain shifts occur compared to conventional knowledge transfer methods.

\vspace{-10pt}
\paragraph{Experimental Settings.}
We evaluate \ours~across three gridworld environment scenarios, as illustrated in Fig. \ref{fig:rl_env}. For the environment details, please refer to the Appendix \ref{app:rl}. We collect expert demonstration data for the Lava 1 and Door environments to train imitation learning (IL) models via behavior cloning. The Lava 2 environment represents a domain-shifted variant of Lava 1, featuring modified environmental configurations. Importantly, we do not collect expert demonstration data for Lava 2, and no IL model is trained on this environment. This setup highlights the evaluation of our framework's ability to generalize effectively under domain shifts, testing its generalization in unseen scenarios. After training the imitation learning (IL) models, we utilize them as teacher models, $f_T$, to guide the reinforcement learning (RL) agent during training, referred to as the student model, $f_S$. In this setup, we use the size of the prediction set to directly quantify model uncertainty—larger prediction sets correspond to higher uncertainty \cite{vovk2016criteria}. We use an error rate $\alpha=0.1$ to construct the prediction set. 

% In this experiment, we investigate a variant of \ours~where the decision-making process incorporates a comparison of uncertainties between the teacher model $f_T$ and the student model $f_S$, rather than relying solely on the teacher's uncertainty to guide student learning. 

For a given state $s$, the guidance weight, as described in Section \ref{sec:rl}, is defined as: $w(s)=1$ when $u_T(s) < u_S(s)$, otherwise $w(s)=0$. Based on $w(s)$, we dynamically decide which action to take: $a = a_T$ if $w(s)=1$, otherwise $a = a_S$. To encourage exploration by the RL agent, we define a probability $\epsilon=\min(0.5\frac{t}{S_{total}}+0.5\frac{e}{E_{total}}, 1)$, where $t$ is the current training step, $e$ is the current episode, $S_{total}$ and $E_{total}$ are the total training steps and episodes, respectively. At each step, if a random probability $p<\epsilon$, the agent takes an action from the RL policy, otherwise it takes an action based on \ours. As $\epsilon$ increases over time, the agent progressively shift to a learned RL policy, while initially it relies more on the IL model through \ours~to facilitate learning. Furthermore, we explore another soft variant of \ours, instead of taking \text{argmax} to compare teacher and student prediction uncertainties, we define $w(s)$ as a probability distribution informed by the relative uncertainties of the teacher and student models:
$w(s)=\frac{\exp(-u_T(s)/T)}{\exp(-u_T(s)/T) + \exp(-u_S(s)/T)}$,
where $T$ is the temperature to control the distribution sharpness with $T=1$. And we sample the action $a$ according to this distribution for $a\in\{a_T, a_S\}$. 

\vspace{-6pt}
\paragraph{Baselines.}
We compare \ours~and Soft \ours~against several baselines, including (1) \textbf{SAC} \cite{haarnoja2018soft}, a purely RL approach, (2) \textbf{IBRL} \cite{hu2023imitation}, which leverages a pretrained imitation learning (IL) model to bootstrap RL. During the training process, IBRL queries the target Q-network and selects actions by comparing the Q-values of two candidate actions, $a_T$ and $a_S$, and taking the action with the higher Q-value, and (3) \textbf{Soft IBRL} \cite{hu2023imitation}, a probabilistic variant of IBRL. Instead of selecting the action via a hard \text{argmax}, Soft IBRL samples the action according to a distribution proportional to the Q-values, $a\sim p(a) \propto \exp(\beta Q(s,a))$ for $a\in\{a_T, a_S\}$. 

\begin{figure}[hbt]
     \centering
     \begin{subfigure}[b]{0.495\linewidth}
         \centering
         \includegraphics[width=\textwidth]{figs/centerSquare6x6_1a_0_reward.png}
         \caption{Lava 1}
         \label{fig:reward_lava1}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.495\linewidth}
         \centering
         \includegraphics[width=\textwidth]{figs/centerSquare6x6_1a_1_reward.png}
         \caption{Lava 2}
         \label{fig:reward_lava2}
     \end{subfigure}
     \vfill
     \begin{subfigure}[b]{0.495\linewidth}
         \centering
         \includegraphics[width=\textwidth]{figs/appleDoor_b_1_reward.png}
         \caption{Door}
         \label{fig:reward_door}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\linewidth}
         \centering
         \includegraphics[width=\textwidth]{figs/centerSquare6x6_1a_0_unc.png}
         \caption{Prediction Uncertainty}
         \label{fig:rl_unc}
     \end{subfigure}
     \vspace{-15pt}
        \caption{\textbf{(a-c) Learning Curves.} We compare \ours~and Soft \ours~with other baselines, including SAC, IBRL, and Soft IBRL, and present their learning curves across three environments: (a) Lava 1, (b) Lava 2, and (c) Door. \ours~and Soft \ours~perform similarly, converging faster and achieving higher rewards than other baselines in all environments. \textbf{(d) Prediction Uncertainty.} We show the average prediction uncertainties of \ours~and Soft \ours, taking the Lava 1 environment as the example. Over time, their uncertainties shrink and approach that of the IL model, demonstrating the development of a well-learned RL policy.}
        \label{fig:rl_results}
        \vspace{-8pt}
\end{figure}

\vspace{-5pt}
\paragraph{Experimental Results.}
We present the experimental results in Fig. \ref{fig:rl_results}. First, we compare the learning curves of \ours~and Soft \ours~against other baselines across three environments: Lava 1, Lava 2, and Door. Both \ours~and Soft \ours~demonstrate similar performance, converging faster and achieving higher rewards than all other baselines. Before the agent reaches the goal, the reward function is defined as the negative Manhattan distance between the agent's current location and the goal, normalized by the maximum step limit of 100. Consequently, the accumulated episode rewards initially decrease as the agent explores the environment and accrues negative rewards but increase as it learns. The rewards of \ours~and Soft \ours~are consistently higher than those of other baselines and exhibit smaller initial decreases while converging faster. This can be attributed to the efficiency of \ours, as it compares the prediction uncertainties of teacher and student models instead of relying on Q-values. Methods like IBRL and Soft IBRL, which depend on Q-values to decide between IL or RL actions, may make suboptimal decisions initially due to poorly trained Q-networks. For instance, even if an IL action is superior, its Q-value might be lower than that of an RL action.

In the domain-shifted environment Lava 2, the overall rewards of the IL model are lower due to the domain shift. IBRL and Soft IBRL rewards eventually converge close to the IL model’s performance, as these methods are not uncertainty-aware. Blindly relying on a teacher underperforming due to domain shifts can lead to suboptimal knowledge transfer. In contrast, \ours~and Soft \ours~consider the teacher's prediction uncertainty, enabling significantly faster convergence and achieving rewards over six times higher than the best-performing baselines after convergence. When the teacher’s predictions are confident, the student relies more on them; otherwise, the student explores independently, enhancing knowledge transfer. Even though the teacher IL model's overall reward is not high, it still provides useful knowledge. This allows the RL agent to learn from the teacher and eventually achieve higher rewards than the IL model. We also show the average model prediction uncertainties of \ours~and Soft \ours, taking Lava 1 environment as the example, as illustrated in Fig. \ref{fig:rl_unc}. Over time, their uncertainties decrease and approach that of the IL model, demonstrating the progression toward a well-learned RL policy.

% \vspace{-5pt}
\subsection{Autonomous Driving}
\vspace{-5pt}
In this section, we evaluate the performance of \ours~in the context of autonomous driving against other knowledge transfer baselines under domain shifts and noisy sensor inputs. The task involves learning a steer prediction policy using a multimodal teacher and an RGB-only student.

\vspace{-10pt}
\paragraph{Experimental Settings.} 
We adopt mean accuracy (mAcc) as the evaluation metric for the regression task of steer prediction, following prior works \cite{shen2021gradient, shen2023auxiliary, shen2024task}. We use the real-world driving dataset SullyChen \cite{chen2018collection} for evaluation, which includes diverse driving scenarios with various road types and conditions. We use Nvidia PilotNet \cite{bojarski2016end} as the backbone for both the teacher and student models. The teacher model is a multimodal network that takes RGB images, depth and edge maps as input, while the student model is unimodal, relying solely on RGB images. For more details of experimental settings, please see the Appendix \ref{app:ad_set}. We first train the teacher model $f_T$ offline. Then we use it to guide the student model $f_S$ training through knowledge transfer, while the RGB images for $f_S$ training has domain shifts compared to the ones used for $f_T$ training. Detailed information about domain shifts and model training can be found in Appendices \ref{app:ad_data} and \ref{app:ad_train}, respectively.

\vspace{-6pt}
\paragraph{Baselines.}
We use following knowledge transfer approaches as baselines, comparing their performance with and without the integration of \ours. The baselines include KD \cite{hinton2015distilling}, FitNet \cite{romero2014fitnets}, PKT \cite{passalis2018learning}, and FT \cite{kim2018paraphrasing}.

% \begin{table}[htb]
% \vspace{-10pt}
% \caption{Steer prediction accuracy (\%) of different knowledge transfer methods with and without~\ours under various angle thresholds.}
% \centering
% \resizebox{\linewidth}{!}{
% \begin{tabular}{lccccc}
% \hline
% \multirow{2}{*}{Approach} & \multicolumn{5}{c}{Accuracy (\%) with various angle threshold $\tau$ (degree)} \\ \cline{2-6} 
%  & \multicolumn{1}{l}{$\tau=1.5$} & \multicolumn{1}{l}{$\tau=3.0$} & \multicolumn{1}{l}{$\tau=7.5$} & \multicolumn{1}{l}{$\tau=15.0$} & \multicolumn{1}{l}{$\bf mAcc$}\\ 
% \hline
% Teacher (RGB+Depth+Edge) & 52.2 & 73.8 & 91.0 & 97. & 78.50 \\
% Student (RGB) & 67.47 & 67.39 & 69.54 & 67.31 & 69.73 \\
% \hline
% KD \cite{hinton2015distilling} & 65.83 & 66.10 & 68.37 & 66.82 & 68.98 \\
% KD + \ours & 67.47 & 67.39 & 69.54 & 67.31 & 69.73 \\
% $\Delta$ & 1.64 & 1.29 & 1.17 & 0.49 & 0.75 \\
% \hline 
% FitNet \cite{romero2014fitnets} & 67.81 & 67.48 & 70.38 & 67.68 & 69.47 \\
% FitNet + \ours & 67.82 & 68.22 & 71.22 & 68.00 & 69.51\\
% $\Delta$ & 0.01 & 0.74 & 0.84 & 0.32 & 0.04 \\
% \hline
% PKT \cite{passalis2018learning} & 67.81 & 67.48 & 70.38 & 67.68 & 69.47 \\
% PKT + \ours & 67.82 & 68.22 & 71.22 & 68.00 & 69.51\\
% $\Delta$ & 0.01 & 0.74 & 0.84 & 0.32 & 0.04 \\
% \hline
% FT \cite{kim2018paraphrasing} & 67.81 & 67.48 & 70.38 & 67.68 & 69.47 \\
% FT + \ours & 67.82 & 68.22 & 71.22 & 68.00 & 69.51 \\
% $\Delta$ & 0.01 & 0.74 & 0.84 & 0.32 & 0.04 \\
% \hline
% \end{tabular}
% }
% \label{tab:steer}%
% \vspace{-5pt}
% \end{table}%

\vspace{-5pt}
\begin{table}[htb]
\vspace{-10pt}
\caption{Mean accuracy (\%) of steer prediction of different knowledge transfer methods with and without~\ours~under domain shifts.}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lccc}
\hline
\multirow{2}{*}{Approach} & \multicolumn{3}{c}{Mean Accuracy (\%)} \\ \cline{2-4} 
 & \multicolumn{1}{l}{without \ours} & \multicolumn{1}{l}{with \ours} & \multicolumn{1}{l}{$\Delta$} \\ 
\hline
KD \cite{hinton2015distilling} & 73.5 & 76.8 & 3.3\\

FitNet \cite{romero2014fitnets} & 72.4 & 76.2 & 3.8 \\

PKT \cite{passalis2018learning} & 72.8 & 75.9 & 3.1 \\

FT \cite{kim2018paraphrasing} & 73.1 & 76.4 & 3.3 \\
\hline
Teacher (RGB+Depth+Edge) & 78.5 &  &  \\
Student (RGB) & 71.8 &  & \\
\hline
\end{tabular}
}
\label{tab:steer}%
\vspace{-5pt}
\end{table}%

\vspace{-6pt}
\paragraph{Experimental Results.}
We present the mean accuracy of steer prediction for various knowledge transfer methods with and without \ours~under domain shifts in Table \ref{tab:steer}. As observed, incorporating \ours~consistently enhances the accuracy of all knowledge transfer methods. This demonstrates the effectiveness of \ours~in improving model performance, as its adaptive guidance mechanism strategically balances the exploitation of teacher knowledge with the student’s own exploration. By preventing over-reliance on uncertain teacher predictions, \ours~facilitates more reliable and transferable knowledge under domain shifts.

\vspace{-8pt}
\section{Conclusions}
\vspace{-5pt}
% We propose \ours, a novel framework that leverages conformal prediction to to efficiently quantify uncertainty in the teacher’s predictions for knowledge transfer systems. By dynamically adjusting the teacher’s influence based on its confidence, \ours~ensures effective student learning, particularly in domain-shifted scenarios where traditional knowledge transfer methods often lead to suboptimal performance due to their over-reliance on an uncalibrated teacher. Our approach allows the student to selectively utilize reliable guidance while avoiding misleading supervision. Additionally, \ours~ demonstrates the ability to still leverage useful “dark knowledge” from the teacher model to improve student performance, even when the teacher is underperforming due to significant domain shifts. This is a notable distinction from conventional knowledge transfer methods, which assume that the teacher always performs better than the student. Furthermore, \ours~is universal and domain-agnostic, making it applicable to diverse machine learning tasks. We validate \ours~on image classification, imitation-guided reinforcement learning, and autonomous driving, where it enhances performance, robustness, and transferability compared to existing methods. We also discuss some limitations and possible future work, please see the Appendix \ref{app:limit}.

We propose \ours, a novel framework that leverages conformal prediction to efficiently quantify uncertainty in the teacher’s predictions for knowledge transfer. By dynamically adjusting the teacher’s influence based on confidence, \ours~ensures effective student learning, especially under domain shifts where traditional methods struggle due to over-reliance on uncalibrated teachers. Our approach selectively utilizes reliable guidance while avoiding misleading supervision and can still extract useful “dark knowledge” even when the teacher underperforms, unlike conventional methods that assume a superior teacher. \ours~is universal and domain-agnostic, making it applicable across diverse tasks. We validate \ours~on image classification, imitation-guided reinforcement learning, and autonomous driving, demonstrating improved performance, robustness, and transferability. For limitations and future work, please see the Appendix \ref{app:limit}.

\section*{Impact Statement}
This paper presents work whose goal is to advance the field of machine learning, especially knowledge transfer with teacher-student structures. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\bibliography{reference}
\bibliographystyle{icml2025}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Appendix}
\subsection{Adaptive Uncertainty-Guided Knowledge Transfer (\ours) Algorithm} \label{app:algo}
\begin{algorithm}
\caption{Adaptive Uncertainty-Guided Knowledge Transfer (\ours)}
\begin{algorithmic}[1]
\Require
    \State Pretrained teacher model $f_T$, student model $f_S$
    \State Calibration set $\mathcal{D}_{cal}$ from the student domain
    \State Training set $\mathcal{D}_{train}^S$ from the student domain
\Ensure Trained student model $f_S$

\vspace{0.5em}
\State \textbf{Step 1: Calibrate Conformal Teacher Predictor}
\State Compute nonconformity scores $s_i = s(\Bar{x}_i, \Bar{y}_i)$ for each sample $(\Bar{x}_i, \Bar{y}_i)$ from the calibration set $\mathcal{D}_{cal}$
\State Compute the $(1-\alpha)$-quantile $q_{1-\alpha}$ of the nonconformity scores

\vspace{0.5em}
\State \textbf{Step 2: Train Student Model}
\For{each training sample $(x_j, y_j)\in\mathcal{D}_{train}^S$}
    \State Compute the teacher's prediction $f_T(x_j)$
    \State Construct the conformal prediction set $\mathcal{C}(x_j)$ and compute $w(x_j)$ as in Eq. \ref{eq: weight}
    \State Update the student model using the loss $\gL$ as in Eq. \ref{eq: loss}
\EndFor

\vspace{0.5em}
\State \Return Trained student model $f_S$
\end{algorithmic}
\end{algorithm}

% \subsection{Reinforcement Learning}
% \begin{algorithm}
% \caption{Adaptive Uncertainty-Guided Knowledge Transfer (\ours)}
% \begin{algorithmic}[1]
% \Require
%     \State Pretrained teacher model $f_T$, student model $f_S$
%     \State Calibration set $\mathcal{D}_{cal}$ from the student domain
%     \State Training set $\mathcal{D}_{train}^S$ from the student domain
% \Ensure Trained student model $f_S$

% \vspace{0.5em}
% \State \textbf{Step 1: Calibrate Conformal Predictor}
% \State Compute nonconformity scores $s_i = s(x_i, y_i)$ for each sample $(x_i, y_i)$ from the calibration set $\mathcal{D}_{cal}$
% \State Compute the $(1-\alpha)$-quantile $q_{1-\alpha}$ of the nonconformity scores

% \vspace{0.5em}
% \State \textbf{Step 2: Train Student Model}
% \For{each training sample $(x_j, y_j)\in\mathcal{D}_{train}^S$}
%     \State Compute the teacher's prediction $f_T(x_j)$
%     \State Construct the conformal prediction set $\mathcal{C}(x_j)$ and compute $w(x_j)$ as in Eq. \ref{eq: weight}
%     \State Update the student model using the loss $L$ as in Eq. \ref{eq: loss}
% \EndFor

% \vspace{0.5em}
% \State \Return Trained student model $f_S$
% \end{algorithmic}
% \end{algorithm}

\subsection{Theoretical Analysis} \label{app:the}
We provide a theoretical analysis comparing the proposed \ours~with standard knowledge transfer to explain why \ours~outperforms conventional methods, particularly under domain shifts. In \ours, we leverage CP to quantify the teacher's prediction uncertainty and dynamically adjust its guidance to the student based on its prediction uncertainty. The adaptive uncertainty-guided loss function is follows:
\begin{equation}
    \gL = \lambda_1 \gL_S + w(x)\cdot\lambda_2 \gL_T,
\end{equation}
where $\gL_S(f_S(x), y)$ is the student's supervised task loss (e.g., mean square error, cross-entropy), $\gL_T(f_S(x), f_T(x))$ is the teacher guidance loss (e.g., KL divergence, mean squared error), $\lambda_1$ and $\lambda_2$ are the coefficients, and $w$ is an uncertainty-based weight derived from the teacher model’s uncertainty $u_T$ as in Eq. \ref{eq: weight}. The weight $w$ approaches 1 when the teacher’s predictions are confident and decreases toward 0 as the uncertainty increases, ensuring that the student relies more on the teacher's predictions when the uncertainty is low, and less when the uncertainty is high.

From the perspective of gradient modulation, taking the gradient of $\gL$ with respect to the student model parameter $\theta_S$:
\begin{equation}
    \nabla_{\theta_S} \gL = \lambda_1 \nabla_{\theta_S}\gL_S + w\lambda_2\nabla_{\theta_S} \gL_T,
\end{equation}
when the teacher's prediction is confident, $w\rightarrow1$, the gradient from teacher guidance loss is backpropagated to update the student model. Conversely, when the teacher's prediction is noisy, $w\rightarrow0$, gradients from noisy $\gL_T$ are suppressed, preventing the student from learning incorrect patterns. However, standard knowledge transfer employs a fixed-weighted gradient update, $\nabla_{\theta_S} = \lambda_1 \nabla_{\theta_S}\gL_S + \lambda_2\nabla_{\theta_S} \gL_T$, even if the teacher's prediction is noisy, the gradient from teacher guidance is backpropogated to update the student model, leading to suboptimal knowledge transfer. 

Then we analyze from the perspective of critical point. To find the critical point of $f_S(x)$, we set the gradient of $\gL$ with respect to $f_S(x)$ to zero:
\begin{equation}
    \frac{\partial \gL}{\partial f_S(x)} = \lambda_1 \frac{\partial\gL_S}{\partial f_S(x)} + w \lambda_2 \frac{\partial\gL_T}{\partial f_S(x)} = 0.
\end{equation}
Rearranging, the critical point $f_S^*(x)$ satisfies:
\begin{equation}
    \frac{\partial\gL_S}{\partial f_S(x)} = -\frac{w\lambda_2}{\lambda_1} \frac{\partial\gL_T}{\partial f_S(x)}.
\end{equation}
This equation implicity defines $f_S^*(x)$ as a function of: the teacher's prediction $f_T(x)$, the ground truth label $y$, and the uncertainty-dependent weight $w$. When $w\rightarrow1$ (low teacher prediction uncertainty), the student's prediction $f_S^*(x)$ aligns more closely with the teacher's prediction $f_T(x)$. When $w\rightarrow0$ (high teacher prediction uncertainty), the student's prediction aligns more closely with the ground truth label $y$. However, in standard knowledge transfer, 
\begin{equation}
    \frac{\partial\gL_S}{\partial f_S(x)} = -\frac{\lambda_2}{\lambda_1} \frac{\partial\gL_T}{\partial f_S(x)},
\end{equation}
where the student consistently follows the teacher’s guidance, limiting its capability to explore and learn new knowledge independently. When the teacher's prediction is noisy or there are domain shifts, this rigid reliance can lead to suboptimal knowledge transfer. While \ours~can adaptively adjust the teacher's guidance based on its prediction uncertainty, which effectively balances teacher knowledge exploitation and student exploration. This adaptive mechanism enhances the student’s ability to generalize and improves the transferability of knowledge, particularly in domain-shifted environments.

Let's consider an example where $\gL_S$ and $\gL_T$ are mean square error (MSE) loss: $\gL_S=\E_{(x,y)}[(f_S(x)-y)^2]$, $\gL_T=\E_{(x,y)}[(f_S(x)-f_T(x))^2]$. The gradients:
\begin{equation}
    \frac{\partial\gL_S}{\partial f_S(x)} = 2(f_S(x)-y),
\end{equation}

\begin{equation}
    \frac{\partial\gL_T}{\partial f_S(x)} = 2(f_S(x)-f_T(x)).
\end{equation}

The critical point condition:
\begin{equation}
    \lambda_1(f_S(x)-y) = -w \lambda_2(f_S(x)-f_T(x)).
\end{equation}
Solving for $f_S(x)$:
\begin{equation}
    f_S(x) = \frac{\lambda_1 y+ w\lambda_2 f_T(x)}{\lambda_1 + w\lambda_2}.
\end{equation}
As we can see from this equation, when $w\rightarrow0$ (high teacher prediction uncertainty), $f_S(x)\rightarrow y$, the student model learns directly from the data and aims to approximate the ground truth $y$. Conversely, when $w \rightarrow1$ (low teacher prediction uncertainty), $f_S(x) \rightarrow \frac{\lambda_1 y + \lambda_2 f_T(x)}{\lambda_1 + \lambda_2}$, which is a interpolation between teacher prediction $f_T(x)$ and ground truth $y$. However, standard knowledge transfer enforces strict adherence to the teacher’s guidance, regardless of its accuracy, which can lead to suboptimal knowledge transfer, particularly when the teacher’s predictions are noisy or domain shifts occur.

% Uncertainty quantification helps balance the tradeoff between bias (teacher overconfidence leading to incorrect guidance) and variance (student over-exploration due to lack of structure).

% From an information-theoretic view, KL divergence between teacher and student distributions determines the knowledge transfer efficiency. A teacher model with high uncertainty has a broader posterior distribution, which means less informative supervision.

\subsection{Image Classification} \label{app:img}
\subsubsection{Different Levels of Domain Shifts} \label{app:img_noise}
We introduce two levels of domain shifts. In Level 1, we add Gaussian noise with zero mean and a standard deviation of 0.03 to $30\%$ of the training data of 50K images, where the noisy samples are selected uniformly at random across the entire dataset to ensure consistent noise distribution. Then we shuffle the dataset and randomly split it into a $90\%$ student training set $\mathcal{D}_{train}^S$, and a $10\%$ calibration set $D_{cal}$, ensuring that both sets are drawn from the same underlying distribution. Additionally, the same Gaussian noise is added to $30\%$ of the testing data of 10K images, to form the noisy test set $D_{test}$, which allows us to evaluate the performance of the models under noise conditions. In Level 2, the noise is increased to a zero mean with a standard deviation of 0.05, and we add it to $40\%$ of the training and testing images. After introducing the noise, we again randomly split the training data into a $90\%$ student training set $\mathcal{D}_{train}^S$, and a $10\%$ calibration set $D_{cal}$.

\subsubsection{Heterogeneous Teacher-Student Structure Results}
\vspace{-5pt}
\begin{table*}[htb]
\caption{Top-1 accuracy (\%) of various knowledge transfer methods with and without \ours~on CIFAR-100 under heterogeneous structure and domain shift Level 1. We use $\Delta$ to show performance gain relative to conventional knowledge transfer methods without \ours. We highlight in orange deltas greater than 0.15, indicating non-trivial enhancement following the protocol in \cite{sun2024logit}.}
\centering
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{lccccc}
\hline \multirow{2}{*}{Teacher} & ResNet50 & ResNet32 $\times 4$ & VGG13 & WRN-40-2 \\
& 72.99 & 73.01 & 70.65 & 69.38 \\
Student & ShuffleNet-V1 & MobileNet-V2 & MobileNet-V2 & ShuffleNet-V2 \\
& 65.66 & 57.22 & 57.22 & 66.91 \\
\hline 
KD \cite{hinton2015distilling} & 68.47 & 60.27 & 60.98 & 70.52 \\
KD + \ours & \bf{68.96} & \bf{60.94} & \bf{62.04} & \bf{70.70} \\
$\Delta$ & \color{orange}{0.49} & \color{orange}{0.67} & \color{orange}{1.06} & \color{orange}{0.18}\\
\hline 
FitNet \cite{romero2014fitnets} & 65.41 & 56.87 & 55.50 & 68.01 \\
FitNet + \ours & 66.17 & 57.83 & 56.39 & 68.22 \\
$\Delta$ & \color{orange}{0.76} & \color{orange}{0.96}& \color{orange}{0.89} & \color{orange}{0.21}\\
\hline
PKT \cite{passalis2018learning} & 65.16 & 57.33 & 56.22 & 67.20\\
PKT + \ours & 65.31 & 57.79 & 57.07 & 67.23 \\
$\Delta$ & \color{orange}{0.15} & \color{orange}{0.46} & \color{orange}{0.75} & 0.03 \\
\hline
FT \cite{kim2018paraphrasing} & 65.60 & 57.89 & 56.74 & 67.03 \\
FT + \ours & 66.04 & 58.26 & 58.00 & 67.52 \\
$\Delta$ & \color{orange}{0.44} & \color{orange}{0.37} & \color{orange}{1.26} & \color{orange}{0.49}\\
\hline
\end{tabular}
}
\label{tab:hete}%
\vspace{-5pt}
\end{table*}%

As part of our ablation studies, we evaluate the performance of a heterogeneous teacher-student framework and present the results in the following Table \ref{tab:hete}. The table shows that, for all knowledge transfer methods, performance improves when combined with \ours, further validating the effectiveness of our approach across different teacher-student structures.

\subsubsection{Training Details} \label{app:img_train}
For the experiments, we use the stochastic gradient descents (SGD) \cite{sutskever2013importance} as the optimizer with momentum 0.9 and weight decay $5e-4$. The epoch number is 240 and the batch size is 64. The initial learning rate is set to 0.01 for MobileNet \cite{sandler2018mobilenetv2}/ShuffleNet \cite{zhang2018shufflenet} architectures and 0.05 for other architectures. The model is trained on an Nvidia RTX 3090 GPU with AMD Ryzen 9 5900 CPU and 32 GB RAM.

\subsection{Imitation-Guided Reinforcement Learning}
The environment scenarios shown in Fig. \ref{fig:rl_env} are adapted from \cite{yu2024beyond} and developed using the Minigrid framework \cite{chevalier2024minigrid}. They are fully observable with discrete state and action spaces. In each environment, the agent's state corresponds to its \textit{\{x, y\}} coordinates on the map, and the action space comprises five discrete actions: \textit{left}, \textit{right}, \textit{up}, \textit{down}, and \textit{stay}. Each episode is capped at a maximum of 100 steps. In the Lava 1 and Lava 2 environments, the reward function is computed as the negative Manhattan distance between the agent's current position and the goal, normalized by the maximum step limit of 100. Upon reaching the goal, the agent receives a terminal reward of \(10-9\times\frac{\text{step count}}{\text{max step}}\). Stepping into the lava results in a reward of -1, and the episode terminates immediately. Similarly, in the Door environment, the reward structure follows the same formulation but without lava, encouraging the agent to minimize its distance to the goal, with a same terminal reward applied upon successful completion.

We collect expert demonstration data comprising state-action pairs for the Lava 1 and Door environments to train imitation learning (IL) models via behavior cloning \cite{torabi2018behavioral}. The demonstration data are inherently uncertain, as multiple valid actions may exist for the same state, as illustrated in \cite{yu2024beyond}, introducing ambiguity in the IL model's predictions.

\subsubsection{Environment Details} \label{app:rl}
\begin{figure}[hbt]
\vspace{-5pt}
     \centering
     \begin{subfigure}[bt]{0.22\linewidth}
         \centering
         \includegraphics[width=\linewidth]{figs/centerSquare6x6_1a_0.jpg}
         \caption{Lava 1}
         \label{fig:env_lava1}
     \end{subfigure}
     \hspace{0.02\textwidth}
     \begin{subfigure}[bt]{0.22\linewidth}
         \centering
         \includegraphics[width=\linewidth]{figs/centerSquare6x6_1a_1.jpg}
         \caption{Lava 2}
         \label{fig:env_lava2}
     \end{subfigure}
     \hspace{0.02\textwidth}
     \begin{subfigure}[bt]{0.22\linewidth}
         \centering
         \includegraphics[width=\linewidth]{figs/appleDoor_b_1.jpg}
         \caption{Door}
         \label{fig:env_door1}
     \end{subfigure}
        \caption{\textbf{Gridworld Environment Scenarios.} (a) \textbf{Lava 1}: An autonomous agent (red dot) must navigate to a target position (diagonal square) while avoiding lava regions. (b) \textbf{Lava 2}: A domain-shifted variant of Lava 1 with altered environment dynamics and layout. (c) \textbf{Door}: The agent must traverse a structured environment with doors and walls to reach the designated target position.}
        \label{fig:rl_env}
\end{figure}

% \begin{figure}[hbt] 
%     \centering
%     \includegraphics[width=0.25\linewidth]{figs/centerSquare6x6_policy0.jpg}
%     \caption{Illustration example of possible agent actions for each state in the demonstration data for the Lava 1 environment. Multiple valid actions may exist for the same state, making the learned IL model inherently uncertain. Figure adopted from \citet{yu2024beyond}.}
%     \label{fig:rl_action}
% \end{figure}

\subsubsection{Training Details}
For the imitation-guided reinforcement learning experiments, we employ a batch size of 512 and the Adam optimizer \citep{kingma2014adam} with an initial learning rate of $3e{-4}$. For each method and environment scenario, we train for 1000 episodes across 10 different random seeds. The model is trained on an Nvidia RTX 3090 GPU with AMD Ryzen 9 5900 CPU and 32 GB RAM.

\subsection{Autonomous Driving} 
\subsubsection{Experimental Settings} \label{app:ad_set}
We first define the accuracy with respect to a specific degree threshold $\tau$ as $acc_\tau = \text{count}(|\theta - \hat{\theta}| < \tau )/n$, following prior works \cite{shen2021gradient, shen2023auxiliary, shen2024task}, where $n$ is the number of test cases; $\theta$ and $\hat{\theta}$ represent the ground truth and the predicted steer angle, respectively, for $\tau\in\mathcal{T}=\{1.5, 3.0, 7.5, 15.0\}$. Then we compute the mean accuracy (mAcc) by averaging $acc_\tau$ across different thresholds. 

The SullyChen \cite{chen2018collection} dataset contains approximately 63,000 images, each with a resolution of 455 $\times$ 256, paired with a corresponding steer angle annotation. We show some sample images in Fig. \ref{fig:sully}. To generate edge maps from RGB images, we employ DexiNed \cite{soria2023dense}. To generate depth maps, we utilize DPT \cite{ranftl2021vision}. Following \cite{shen2023auxiliary}, we use channel-level attention to represent the importance of each modality. For the teacher model $f_T$, we combine the data from different modalities (RGB, depth, and edge) at the channel level and pass them through an Squeeze-and-Excitation (SE) block \cite{hu2018squeeze}, followed by a 1$\times$1 convolution layer to make the channel number to be the same as the main modality RGB. We first train the teacher model $f_T$ offline. Then we use it to guide the student model $f_S$ training through knowledge transfer, while the RGB images for $f_S$ training has domain shifts compared to the ones used for $f_T$ training. For the details of domain shift, please refer to the Appendix \ref{app:ad_data}.  

We use an error rate $\alpha=0.1$ and compute nonconformity score $s$ as the residuals between the predicted and true steer angles from the calibration set to get the quantile value $q_{1-\alpha}$. Then we use it to construct the prediction interval $\mathcal{C}(x)$ for a given input RGB image $x$. We define the teacher's uncertainty as the length of the prediction interval: $u_T(x)=|\mathcal{C}(x)|$. The dynamic weight is assigned as $w(x)=1$ if $u_T(x)<\tau$, otherwise $w(x)=0$, for $\tau \in \mathcal{T}$. For all knowledge transfer methods, we set the coefficients $\lambda_1$ and $\lambda_2$ in Eq. \ref{eq: loss} to 1, assigning equal importance to the student task loss $\gL_S$ and the teacher guidance loss $\gL_T$. Here, $\gL_S$ represents the MSE loss, while $\gL_T$ varies based on each knowledge transfer method.

\subsubsection{Data Processing} \label{app:ad_data}
After generating depth and edge maps, we split the dataset into $80\%$ training and $20\%$ testing. Then we train the multimodal teacher model on the training data offline. After training the teacher model, we introduce domain shift to the student training data compared to the teacher's pretraining data. Specifically, we add Gaussian noise with zero mean and a standard deviation of 0.1 to $30\%$ of the RGB images of the training data, where the noisy samples are selected uniformly at random across the entire training set to ensure consistent noise distribution. We do not add Gaussian noise to the generated depth and edge maps, as they are not used for the student model. Then we shuffle the training data and randomly split it into a $90\%$ student training set $\mathcal{D}_{train}^S$, and a $10\%$ calibration set $D_{cal}$, ensuring that both sets are drawn from the same underlying distribution. Additionally, the same Gaussian noise is added to $30\%$ of the testing data of RGB images, to form the noisy test set $D_{test}$, which allows us to evaluate the performance of the models under noise conditions.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/sullychen.png}
    % \vspace{-8pt}
    \caption{\textbf{Sample images of the real-world SullyChen dataset.} 
    SullyChen \cite{chen2018collection} is a real-world driving dataset which includes diverse driving scenarios with various road types and conditions.}
    \label{fig:sully}
\end{figure}

\subsubsection{Training Details}\label{app:ad_train}
For the experiments, we employ a batch size of 32 and the Adam optimizer \citep{kingma2014adam} with an initial learning rate of $1e{-3}$, and a weight decay of $1e-5$. The model is trained on an Nvidia RTX 3090 GPU with AMD Ryzen 9 5900 CPU and 32 GB RAM for 240 epochs.

\subsection{Limitations and Future Work} \label{app:limit}
Despite the advantages of \ours, there are some limitations. Currently, in our framework, the teacher model is pretrained and fixed during the knowledge transfer process, guiding the student without updates. Future work can explore more adaptive strategies where the teacher model is updated alongside the student, considering its prediction uncertainty. Such an extension could make the framework more flexible and better suited to handling domain shifts. Moreover, extending \ours~to multi-teacher settings, where multiple sources of knowledge with varying expertise guide the student, presents an promising opportunity to further improve robustness and transferability, especially in complex, real-world applications.

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
