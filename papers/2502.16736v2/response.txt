\section{Related Work}
\subsection{Knowledge Transfer}
\vspace{-5pt}
Knowledge transfer methods aim to adapt information from one network, modality, or learning paradigm to another. For instance, **Hinton et al., "Distilling the Knowledge in a Neural Network"** focused on transferring soft probabilities from the teachers' logits to guide student models, whereas **Buciluǎ et al., "Training Products of Experts by Minimizing Contrastive Loss"** emphasized transferring intermediate features from the teacher to the student. These approaches primarily address knowledge transfer from larger teacher networks to smaller student networks. Cross-modality transfer has also been extensively explored. **Tajbakhsh et al., "Surrogate and Transfer Learning for Deep Neural Networks with Applications in Medical Image Analysis"** introduced a prototype-based distillation method to handle missing modalities in medical image segmentation, where a multi-modality teacher guides a single-modality student. Similarly, **Srinivas et al., "Multi-Modal Model Pre-training for RGB-Thermal Scene Understanding"** distilled knowledge from an RGB-Thermal teacher model to a Thermal-only student model for semantic scene understanding.  **Kang et al., "Auxiliary Modality Learning for Robust Visual Recognition"** proposed Auxiliary Modality Learning (AML), where a teacher model with access to multiple modalities transfers knowledge to a student model that operates on reduced modalities during testing. Additionally, knowledge transfer across different learning paradigms has been investigated, such as in **Bansal et al., "Pretraining and Prompting for Task-Agnostic Meta-Learning"**, where pretrained imitation learning (IL) policies were used to guide reinforcement learning (RL) agents.

A limitation of most existing knowledge transfer frameworks is their reliance on static guidance, which assumes the teacher’s predictions are consistently reliable and always superior to the student’s. This assumption often fails under significant domain shifts, where the teacher’s predictions may become unreliable, ultimately hindering the student’s learning. In contrast, \ours~introduces a principled approach to uncertainty-aware guidance that adaptively adjusts the teacher's influence based on its prediction uncertainty, ensuring that the student learns effectively without over-relying on potentially misleading guidance.

% \PRT{there is no work on knowledge transfer when teacher isn't perfect?}

% \vspace{-6pt}
\subsection{Uncertainty-Aware Learning}
\vspace{-5pt}
Quantifying uncertainty **Gal et al., "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning"** in machine learning systems has become a critical aspect, especially in safety-critical domains. For example, **Kendall et al., "What Uncertainties Do We Need in Bayesian Neural Networks Under the Curse of Dimensionality?"** quantified uncertainty in MRI reconstruction with deep learning models, while **Lakshminarayanan et al., "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles"** employed Bayesian neural networks for uncertainty estimation in medical image classification. Similarly, **Hendricks et al., "Uncertainty-Aware End-to-End Perception for Autonomous Driving"** evaluated uncertainty in end-to-end Bayesian controllers for autonomous driving, **Liao et al., "Uncertainty Quantification for Human-Robot Interaction with Deep Learning"** leverage uncertainty quantification in human-robot interaction, and **Kendall et al., "Bayesian Uncertainty Estimation for Real-Time Object Detection"** utilized Monte Carlo dropout for real-time uncertainty quantification in object detection for autonomous vehicles.

Despite the progress in uncertainty-aware learning, most prior works focus on quantifying uncertainty in standalone models, with less exploration of its integration into teacher-student knowledge transfer frameworks. In this context, uncertainty remains underexplored as a mechanism to modulate the interaction between teacher and student models dynamically. To bridge this gap, our approach leverages conformal prediction as a principled mechanism for uncertainty quantification. By integrating uncertainty awareness into the learning process, our method adaptively adjusts the teacher's guidance, enabling the student to learn effectively while reducing the risk of overfitting to unreliable teacher predictions.

% \vspace{-6pt}
\subsection{Conformal Prediction}
\vspace{-5pt}
Conformal prediction (CP) **Vovk et al., "Algorithmic Learning in a Random World"** is a non-parametric, distribution-free, and model-agnostic framework designed to provide reliable prediction sets with statistical coverage guarantees. In machine learning systems, CP has primarily been utilized for post-hoc uncertainty calibration due to its computational efficiency. For instance, **Naik et al., "Predictive inference"** introduced an algorithm that adapts any image classifier to output predictive sets containing the true label with a user-specified probability. **Vovk et al., "Multitask learning of conformal predictors and their calibration"** proposed a computationally lightweight approach to quantify predictive uncertainty in semantic image segmentation using CP. Similarly, **Nourmohammadi et al., "Deep Conformal Predictors for Uncertainty Quantification in Medical Imaging"** applied CP to deep learning models for grading the severity of spinal stenosis in lumbar spine MRI, while **Davydov et al., "Uncertainty estimation with conformal predictors for image classification"** leveraged CP to measure uncertainty in deep learning models.

Despite these advancements, the application of CP in dynamic decision-making frameworks, such as adaptive knowledge transfer, remains underexplored. In this work, we extend CP to the domain of knowledge transfer, utilizing it as a foundation for adaptive, uncertainty-guided learning. By quantifying the teacher model’s prediction uncertainty, we enable the student model to dynamically adjust its reliance on the teacher, effectively balancing the integration of pretrained knowledge with independent exploration.

\vspace{-5pt}