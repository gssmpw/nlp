% First drafted by  A. Cichocki on 10.10.2024
%
\documentclass[onecolumn,technote,twoside]{IEEEtran}
%\documentclass{article}
%UTF8
%\UseRawInputEncoding
%\usepackage{clefval}
\usepackage[utf8]{inputenc}
%end(UTF8)
%\usepackage{amsmath,epsfig}
%%W%\usepackage{t1enc,amsmath,amsfonts,psfig,epsfig,graphicx,color}
\usepackage{t1enc,amsmath,amsfonts,epsfig,graphicx,color}
\usepackage[active]{srcltx} %SRC Specials for DVI Searching

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage[active]{srcltx} %SRC Specials for DVI Searching

%%sergio
\usepackage{bm}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}

\usepackage{empheq}
\usepackage{booktabs}
\newcommand*\widefbox[1]{\fbox{\hspace{2em}#1\hspace{2em}}}

\renewcommand{\baselinestretch}{1 } % vertical space for lines.
%\renewcommand{\baselinestretch}{2 } % vertical space for lines.

\newtheorem{assumption}{Assumption}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
%\newtheorem{proof}{Proof}
\newcommand{\dif}[0]{\mbox{d}}
\newcommand{\mbf}[1]{\boldmath #1}
%\newcommand{\bm}[1]{\boldmath #1}
\newcommand{\kron}[0]{\otimes}
\newcommand{\krao}[0]{\odot}
\newcommand{\V}[0]{\mbox{vec\ }}
%\newcommand{\U}[1]{\underline{#1}}
\newcommand{\tr}[0]{\mbox{tr\ }}
\newcommand{\bim}[1]{\mbox{\boldmath $#1$}}
\newcommand{\bis}[1]{\mbox{\small \boldmath $#1$}}
\newcommand{\bit}[1]{\mbox{\tiny \boldmath $#1$}}
\newcommand{\bi}[1]{\textbf{\textit{#1}}}

%\input{def1jml.set}


\input{def11.set}


\title{Generalized  Exponentiated Gradient Algorithms Using the Euler Two-Parameter Logarithm}

\author{{Andrzej CICHOCKI} $^1$}
%\author{\bf {Andrzej Cichocki}}\\


\begin{document}

\maketitle

%\centerline{\today} \hspace{1cm}

\begin{abstract}

In this paper we propose and investigate a new class of Generalized Exponentiated Gradient (GEG) algorithms using Mirror Descent (MD) approaches,  and applying  as a regularization function the Bregman divergence with two-parameter deformation of logarithm as a link function. This link function (referred to as the Euler logarithm) is associated with a wide class of generalized  entropies. In order to derive novel GEG/MD updates, we estimate generalized  exponential function, which closely approximates the inverse of the Euler two-parameter logarithm.  The characteristic/shape  and properties of  the Euler logarithm and its   inverse -- deformed exponential functions are tuned by two or even more hyperparameters. By learning these hyperparameters, we can adapt to distribution of training  data, and we can adjust them to achieve desired properties of gradient descent algorithms. The concept of generalized entropies and associated  deformed logarithms provide  deeper insight into novel gradient descent updates.
%
 In literature, there exist nowadays over fifty mathematically well-defined entropic functionals  and associated deformed logarithms, so impossible to investigate all of them in one research paper. Therefore, we focus  here on a wide-class of trace-form entropies and associated generalized logarithm. We applied the developed algorithms for Online Portfolio Selection (OPLS) in order to improve its performance and robustness. \footnote{Systems Research Institute of Polish Academy of Science, Newelska 6, 01-447 Warszawa, Poland,\\
Global Innovation Research Institute of Tokyo University of Agriculture and Technology, 2-24-16 Naka-cho, Koganei-shi, Tokyo, Japan,\\
Riken AIP, 103-0027 Tokyo, Nihonbashi, 1 chome-4-1, Japan, (e-mail: cichockiand@gmail.com)}

\end{abstract}

%\begin{keywords}%Enter key words or phrases in alphabetical order, separated by commas.
Keywords:  Generalized logarithms and deformed exponential functions, novel  generalized exponentiated gradient descent, Mirror Descent, generalized  multiplicative updates, Bregman Divergences with generalized logarithms as link functions, On-line Portfolio Selection (OLPS).
%\end{keywords}
%\begin{keywords}
%\end{keywords}

\section{Introduction}


Many fundamental formulations of gradient updates have been considered in the optimization, machine learning and AI including additive gradient descent (GD) \cite{Nemirowsky,EGSD}, its stochastic version (SGD), multiplicative updates (MU) \cite{Cia3,Cichocki_Cruces_Amari,CichZd_ICA06}, exponentiated gradient (EG) descent \cite{EG,MD2} and mirror descent (MD) updates \cite{Nemirowsky,MD1,Beck2003,EGSD,shalev2011}.
%
The (additive)  gradient descent and the  multiplicative (exponentiated) gradient  update method are  the most popular algorithms in machine learning.
The exponentiated gradient  descent updates belong  to the class of multiplicative gradient updates \cite{Cia3} and simultaneously to mirror descent (MD) algorithms (see e.g., \cite{MD1,EGSD}). EG algorithms were  proposed by Kivinen and Warmuth \cite{EG,KW1995} and  have been adopted for various applications by many researchers  \cite{Helmbold98}, \cite{herbsterwarmuth,EGnoise,Nock2023,Yang2022,Momentum,EGSD}.

Exponentiated Gradient (EG) descent  and Mirror descent (MD) \cite{Nemirowsky,Beck2003,MD1,MD2,EGSD,shalev2011} are becoming increasingly popular in a variety of settings in optimization and machine learning. One reason for their success is the fact that mirror descent can be adapted to fit the geometry of the optimization problem at hand by choosing suitable strictly convex potential functions (mirror maps). EG and MD  have been extensively studied for convex optimization problems and also in non-convex setting, and it is based on  the Bregman divergence associated to the mirror map \cite{Nemirowsky,Beck2003,MD1}.

The additive gradient descent updates are nowadays the most commonly used algorithms, but they are often not appropriate when all entries of the target weight vector have to be nonnegative. Moreover, in learning and optimization via additive gradient (GD), it incurs well-known problems like vanishing and exploding gradients, making careful and precise learning rate tuning essential for real-world applications \cite{Nemirowsky}.
%
The  EG weight updates may alleviate some of these problems without the need for precise learning rate tuning. Moreover, Exponentiated gradient (normalized-EGU and normalized-EG) updates can converge faster than GD, especially when target weight vectors are sparse \cite{EG,herbsterwarmuth,MD1}. It is also interesting to note, that recent findings about synapses of neurons in brains indicate that EG updates are more biologically plausible in learning process than additive GD updates \cite{Cornford24}.
%
The standard EG updates are typically viewed as appropriate for problems where the geometry of the optimization domain is described by the Kullback--Leibler divergence or relative entropy, as is often the case when optimizing over probability distributions. The main disadvantage of existing EG updates is that they do not have the possibility to adapt to data with various distributions due to the lack of hyperparameters which control convergence properties and performance of the algorithms.

We propose in this paper, that  generalized EG updates may arise under a broader principle: By exploiting two (or more) parameters link function which allow  us to adapt to data with various  geometry and probability distributions. Particularly, for the online portfolio selection problem, this allows the proposed algorithms to adapt to different market conditions and investor preferences more effectively than the existing standard EG algorithms \cite{Helmbold98,Momentum}.


 \section{Preliminaries: Mirror Descent and Standard Exponentiated Gradient (EG) Update}

 {\bf Notations}. Vectors are denoted by boldface lowercase letters, e.g., $\bw \in \Real^N$, where for any vector $\bw$, we denote its $i$-th entry by $w_i$. For any vectors $\bw, \bv \in \Real^N$.  The $N$-dimensional real vector space with nonnegative real numbers is denoted by $\Real^N_+$. We define the Hadamard product as $\bw \odot \bv = [w_1 v_1, \ldots, w_N v_N]^T$ and $\bw^{\alpha} = [w_1^{\alpha}, \ldots, w_N^{\alpha}]^T$. All operation for vectors like multiplications and additions are performed componentwise. The function of a vector is also applied for any entry of the vectors, e.g., $f(\bw) = [f(w_1),f(w_2),\ldots, f_(w_N)]^T$.   We let $\bw (t)$ denote the weight or parameter vector as a function of time $t$. The learning process advances in iterative steps, where during step $t$ we start with the weight vector $\bw (t) = \bw_t$ and update it to a new vector $\bw(t+1) = \bw_{t+1}$. We define $[x]_+ = \max\{0,x\}$, and the gradient of a differentiable cost function as $ \nabla_{\bi w} L(\bw) = \partial L(\bw)/\partial \bw = [\partial L(\bw)/\partial w_1, \ldots, \partial L(\bw)/\partial w_N]^T $. In contrast to generalized logarithms defined later the classical  natural logarithm will be denoted by $\ln (x)$.\\

{\bf Problem Statement:}
Suppose we want to minimize a loss/cost function $L(\bw)$  with respect the weight vector $\bw=[w_{1},\ldots,w_{N}]^T\in \mathbf{R}_+^N$, i.e., we want to solve the following optimization problem:
\be
	\bw_{t+1} = \argmin_{{\bi w} \in \mathbf{R}_+^N} \left\{ L(\bw )+ \frac{1}{\eta} D_f(\bw || \bw_t),
 \right\},
	\label{Eq-1a}
\ee
where  $L(\bw)$ is a differentiable loss function,  $\eta > 0$ is the learning rate and
$D_f(\bw || \bw_t)$ is the Bregman divergence \cite{Bregman1967}.

The Bregman divergence can be  defined as \cite{Bregman1967,MD1}
\be
D_f(\bw || \bw_t) = F(\bw) - F(\bw_t) -  (\bw-\bw_t)^T f(\bw_t),
\label{Bregman1}
\ee
where  generative function (mirror map) $F(\bw)$ is a continuously-differentiable, strictly convex function  defined on the convex domain, while $f(\bw)= \nabla_{\bi w} F(\bw)$,  which is often called the link function.
%
The Bregman divergence can be understood as the first-order Taylor expansion of $F$ around  $\bw$  evaluated at $\bw_t$.
The Bregman divergence $D_f(\bw || \bw_t)$  arising from generator function $F(\bw)$  referred here as mirror map can be viewed as a measure of curvature.

The Bregman divergence includes many well-known divergences commonly used
in practice, e.g., the squared Euclidean distance, Kullback-Leibler divergence (relative entropy), Itakura-Saito distance, beta divergence and many more \cite{Cia3}

The derivatives of  the Bregman divergence $D_f(\bw || \bw_t)$  w.r.t. the first and second arguments  yield
\be
\nabla_{\bw} D_f(\bw || \bw_t)= f(\bw) - f(\bw_t), \qquad \nabla_{\bi w} D_f(\bw || \bw_t)= -  \bH_f(\bw_t) (\bw - \bw_t),
\label{Bregman2}
\ee
where
\be
\bH_f(\bw_t) =\nabla^2_{\bi w_t} F(\bw_t) = \frac{\partial^2 F(\bw_t)}{\partial \bw^2_t}= \frac{\partial f(\bw_t)}{\partial \bw_t},
\ee
 is the Hessian of $F(\bw_t$ evaluated at $\bw_t$.  It should be noted that in this paper the Hessian matrix is a  diagonal positive-definite  matrix with  positive entries by proper selection the range of hyper-parameters.

Computing the gradient  for Eq. (\ref{Eq-1a}) and setting it  at $\bw_{t+1}$ at zero yields the so-called prox or implicit MD update
\be
	f(\bw_{t+1}) = f(\bw_t) - \eta \nabla_{\bi w} L(\bw_{t+1}),
	\ee
 or equivalently
\be
	\bw_{t+1} = f^{(-1)} \left[ f(\bw_t) - \eta \nabla_{\bi w} L(\bw_{t+1}\right],
\label{MDimplicit}
\ee
 where $f^{(-1)}(\bw)$ is inverse function of the link function $f(\bw)$.

In practice, assuming that
$\nabla_{\bi w} L(\bw_{t+1}) \approx \nabla_{\bi w} L(\bw_{t})$, the implicit iteration in \eqref{MDimplicit} can be approximated by the explicit update \cite{MD1,shalev2011}:
%
 \begin{empheq}[box=\fbox]{align}
\bw_{t+1} = f^{(-1)} \left[ f(\bw_t) - \eta \nabla_{\bi w} L(\bw_t)\right].
		\label{f-1fDT}
\end{empheq}
%
The  continuous time mirror descent update (mirror flow) can be represented by ordinary differential equation (ODE) (as $\Delta t \rightarrow 0$) \cite{MD1}
\begin{empheq}[box=\fbox]{align}
	\frac{d\,f\!\left(\bw(t)\right)}{d t}= -\mu \nabla_{\bi w} L(\bw(t))
\label{f-1fCT}
\end{empheq}
%\be
%
%\begin{empheq}[box=\fbox]{align}
% \frac{f(\bw_{t+1})-f(\bw_t)}{\Delta t} \approx	\frac{d\,f\!\left(\bw(t)\right)}{d t}= -\mu \nabla_{\bi w} L(\bw(t))
%\label{f-1fCT}
%\end{empheq}
%
where $\mu = \eta/\Delta t >0$ is the learning rate for continuous-time learning, and $f(\bw) = \nabla F(\bw)$ is a suitably chosen link function \cite{MD1,EGSD}.
Hence, we can obtain alternative form of continuous-time MD update in general form:
\be
 \frac{d\,\bw}{d t} =   - \mu \; [\nabla^2 F(\bw)]^{-1} \; \nabla_{\bi w} L(\bw)
\ee
%
%
%Remark: In MD, we map our primal point $\bw$ to the dual space (through the mapping $f(\bw)=\nabla F(\bw) $ and take a step in the direction given by the gradient of the function, then we map back to the primal space by using inverse function of the  link function. The advantage of using mirror descent (MD) over gradient descent is that it takes into account the geometry of the problem through the mirror map  function $F(\bw)$ or in other words suitable choice of a link function. In fact, we can consider mirror descent as a generalization of the projected gradient descent, which ordinarily is based on an assumed Euclidean geometry. Projected gradient descent is a special case of mirror descent using the mirror map $F(\bw) = ||\bw||^2  $.

%

In the special case,
 for $ F(\bw) = \sum_{i=1}^N w_i \ln w_i - w_i$  and corresponding (component-wise) link function $f(\bw) = \ln (\bw)$  we obtain (multiplicative) unnormalized Exponentiated Gradient (EGU) updates  \cite{EG}.
%
\be
		\frac{d \ln\!\left(\bw(t)\right)}{d t}= -\mu \nabla_{\bi w} L(\bw(t)), \quad \bw(t) > 0, \;\;\; \forall \, t .
		\label{MDEG1}
	\ee
%
In this sense, the unnormalized exponentiated gradient  update  (EGU) corresponds to the discrete-time version of the continuous ODE:
%
\be
	\bw_{t+1}
	&=& \exp \left( \ln \bw_t -\mu \Delta t\, \nabla_{\bi w} L(\bw_t) \right) \nonumber \\
	&=& \bw_t \odot \exp \left( - \eta \nabla_{\bi w} L(\bw_t) \right), \;\; \bw_t >0, \;\; \forall t,
\label{EGU}
\ee
where   $\odot$  and $\exp$ are component-wise multiplication and component-wise exponentiation respectively and $\eta = \mu \Delta t >0$ is the learning for discrete-time updates.

In many practical applications, for example, in on-line portfolio selection (OLPS), we  need to impose an additional constraint that the weights are not only nonnegative, i.e.,  $w_i \geq 0$ for $i=1,2,\ldots,N$, but also normalized to unit $\ell_1$-norm, i.e.,  $||\bw||_1=\sum_{i=1}^{N} w_i =1$ in each iteration step. In such case,  standard EG update can be derived by minimizing the following optimization problem \cite{EG,KW1995,Helmbold98}:
\be
	J(\bw_{t+1}) =  \hat{L}(\bw_{t})  + \frac{1}{\eta} D_{KL}(\bw_{t+1} \| \bw_{t}) + \lambda \left(\sum_{i=1}^{N} w_{i,t+1}-1\right),
\ee
where $\lambda >0$ is the Lagrange multiplier and the last term forces the normalization of the weight vector. The saddle point of this function leads to the standard EG algorithm, expressed in scalar form as \cite{EG,Helmbold98}
\be
	w_{i,t+1} = w_{i,t} \; \frac{\exp [- \eta \nabla_{w_{i,t}} L(\bw_{t})]}{\sum_{j=1}^N w_{j,t} \exp [- \eta \nabla_{w_{j,t}} L(\bw_{t})]}, \quad,   i=1,\ldots,N, \quad w_{i,t}>0, \quad \sum_{i=1}^{N}w_{i,t} =1, \;\; \forall i,t.
\ee
%
Alternatively, we can implement the normalized EG update as follows:
\be
	\tilde{\bw}_{t+1}
	&=& \bw_t \odot \exp \left( - \eta \nabla_{\bi w} L(\bw_t) \right), \;\; \bw_t >0, \;\; \forall t, \quad \text{(Multiplicative update)}\\
\bw_{t+1} &=&	\tilde{\bw}_{t+1} / ||\tilde{\bw}_{t+1}||_1, \qquad \qquad \text{(Unit simplex projection)}.
\label{EG}
\ee
%It is noteworthy that the aforementioned categories of gradient descent updates can be regarded as discrete approximations of continuous-time updates described by ordinary differential equations (ODE) \cite{MD1}.
%Alternatively, the normalized continuous-time EG update can be formulated as \cite{MD1}
%\be
%		\frac{d \ln\!\left(\bw(t)\right)}{d t}= -\mu [\nabla_{\bi w} L(\bw(t)) -{\bf 1} \; (\bw^T \nabla_{\bi w} L(\bw(t))], \quad \bw(t) > 0, \qquad \forall \, t, \quad {\bf 1} =[1,1, \ldots,1]^T,
%		\label{EGCT1}
%	\ee
%which corresponds to approximate discrete-time update
%\be
%\bw_{t+1} = \bw_t \odot \exp \left[-\eta (\bI -\bw_t \bw_t^T)  \nabla_{\bi w} L(\bw_t)\right].
%\label{EG2}
%\ee
%
There are many potential  choices of mirror map $F(\bw)$ or equivalently link function $f(\bw)$ that can adopt to the geometry of data for various optimization  problems and adopt to distribution of training data. Using mirror descent with an appropriately chosen link function, we can get a considerable improvement in performance and robustness in respect noise and outliers.

The key step in our  approach is  a suitable  choice of generalized two-parameter logarithm  as a flexible, parameterized link functions $f(\bw)$,  which allow us to adopt to various distributions  of training  data.
%
Although, many extensions and generalization of EGU and EG updates has been proposed included the Tsallis logarithm, however, to our best knowledge  the generalized two-parameter Euler logarithm has been not investigated neither applied  till now for EG/MD  updates.


\section{The Euler $(a,b)$-Logarithm and its Fundamental Properties}

In this paper we investigate the application of the following generalized logarithm as a link function in mirror descent:
%
%\be
\begin{empheq}[box=\fbox]{align}
 \log^{E}_{a,b}(x)= \frac{x^a-x^b}{a-b},  \;\; x>0, \;\; a\neq b, \; \; a<1,  \; 0<b<1, \;\; \text{or} \;\;  b< 1, \; 0<a<1,
\label{logab}
\end{empheq}
% \ee
which has been investigated firstly by Euler \cite{Euler1779} in 1779, who was inspired by Lambert research \cite{lambert1758},\cite{kaniadakiseditorial2004}.

{\bf Historical remarks}: The above function have long history. First of all,
the related algebraic  equation studied by Lambert was $x^n-x+q=0$,
 which Euler transform in \cite{lambert1758}, \cite{Euler1779}:
 \be
 x^a-x^b=(a-b)v x^{a+b},\;\; x>0,
 \ee
  where  he used here the following  function
 \be
v(x,a,b) = \frac{x^{-b}-x^{-a}}{a-b}, \;\; x>0.
 \ee
Euler considered also special cases, including $b=0$ and limiting case $a=b=0$ \cite{Euler1779,kaniadakiseditorial2004}.
%
  For $b=0$  and $a \neq 0$ he obtained $v (x,a)= (1 - x^{-a})/a$  and  its inverse function $v^{(-1)} (x,a) = [1- a x]_+^{-1/a}$, which in fact are closely  related to the Amari logarithm
  \cite{Amari2009}-\cite{Amari-PAN} and its inverse, investigated in  information geometry and implicitly  related to Harvda-Charvat \cite{harvda1967} and  Tsallis entropy (with $a=q-1$) \cite{tsallis1988,Tsallis1994}.
   %
   For the second case he assumed $ a=b+\lambda $ and  obtained as the limit the natural logarithm (for $b=0$:
   $\lim_{\lambda \rightarrow 0^+} = (x^{\lambda} -1)/\lambda =\ln (x)$.
   %
For these reasons, we will name  the generalized logarithm defined by Eq. (\ref{logab})  the Euler $(a,b)$-logarithm or simply the $(a,b)$-logarithm.

% Euler and Lambert investigated very similar function in   attempting to solve the following algebraic equation

The Euler logarithm has been independently  and implicitly re-introduced by Sharma-Taneja \cite{sharma1975,taneja1989} and Mittal \cite{mittal1975}  often called Sharma-Taneja-Mittal (STM)-entropy in the fields of information theory, and successively proposed by Borges-Roditi \cite{Borges1998} and Kaniadakis-Lissia-Scarfone (KLS)  to define  and investigate a wide class of  generalized trace-form entropies \cite{kaniadakis2004,kaniadakis2005}, in the fields of statistical physics and discussed  more recently by Wada and Scarfone \cite{wada2010}.

The generalized  Borges-Rodity (BR) entropy (which belongs to class of trace-form entropies) can be expressed by the Euler $(a,b)$-logarithm \cite{Borges1998}
\be
S^{BR} (\bp) = \sum_{i=1}^{W} p_i \frac{p_i^{-a} -p_i^{-b}}{a-b} = \sum_{i=1}^{W} p_i \log^{E}_{a,b} (1/p_i).
\ee
%  Furthermore, it is well-known that the generalized logarithm  can be  introduced by  exploiting finite-difference operators of  two-parameter functions \cite{chakrabarti1991} and discussed
%by discussed by Wada and Scarfone \cite{wada2010}.
%was discussed by  Chakrabarti and Jagannathan  using  generalization  of the Jackson derivative on the generating function \cite{chakrabarti1991}.

Although  the Euler $(a,b))$-logarithm has been investigated in statistical physics,  information theory and information geometry  to our best knowledge it has not been explored  so far in applications related to Mirror Descent, Exponentiated Gradient, machine learning or  artificial intelligence.

 It important to note that  such defined  generalized logarithm exists,  (in the sense classical logarithms) and  has desired concavity properties  for
 the following range of hyperparameters $a$ and $b$: $a<1, \; 0<b<1$ or $b<1, \; 0<a<1$ and strict
 convexity for $ 0 < (a,b) <1$ \cite{Borges1998,furuichi2010},\cite{Cantruk2018}.
 %Note that the set of parameters that are nonnegative simultaneously ($a <0$ and $b<0$) is forbidden, since the $(a,b)$-logarithm  does not increase monotonically there 

 %This  is a very general form of  deformed logarithm (called also group logarithm)
% related  to  the earlier works of Sharma-Taneja \cite{sharma1975} and Mittal \cite{mittal1975} and also discussed by Wada and Scarfone \cite{wada2010}.\\

 It  should be noted that, if we put $x=\exp(u)$, we can recover  form the Euler $(a,b)$-logarithm the function known in the mathematical literature as the Abel
exponential  \cite{tempesta2016}
 \be
G(u)= \exp_{\text{Abel}}(u,a,b) =\frac{e^{au} - e^{bu}}{a-b} = \frac{e^{r u}}{\kappa} \sinh (\kappa u),
\label{AbeG}
 \ee
 where $u= \ln(x)$,  $r=(a+b)/2$ and  $\kappa = |a-b|/2$.
Hence, we can express  the Euler $(a,b)$-logarithm as a function of the standard natural logarithm $\ln(x)$
%
\be
\log^{E}_{a,b}(x) =  \frac{\exp(r \; \ln(x))}{\kappa} \sinh (\kappa \ln (x)).
\ee
%
The Euler $(a,b)$-logarithm has the following  basic properties
(the super index E is neglected if it is not confused):

 \begin{itemize}

 \item

 Domain $\log_{a,b} (x)$:  $\Real^+ \rightarrow \Real$,\\

 \item

 Monotonicity: $\displaystyle \frac{d \log_{a,b}(x)}{dx} >0, \quad \text{for} \quad \frac{a}{a-b} >0, \;\; \frac{b}{a-b} <0$, \\

 \item

 Concavity:  $\displaystyle \frac{d^2 \log_{a,b}(x)}{dx^2} < 0$, \\

 \item

 Scaling and Normalization: $\log_{a.b} (1)=0$, \;\;  $\displaystyle \frac{d \log_{a,b}(x)}{dx}\vert_{x=1} =1$,\\

 \item

 Self-Duality:  $\log_{a,b}(1/x) = - \log_{-b,-a}(x)$.\\

 \end{itemize}

Furthermore, it is easy  to check the validity of the  following useful properties:
\be
\log_{a,b} (x) &=& \log_{b,a} (x) \\
\log_{a,b} (x^{\lambda}) &=& \lambda \; \log_{a\lambda,b\lambda} (x)\\
\log_{a+\lambda,b+\lambda} (x) &=& x^{\lambda} \; \log_{a,b} (x)\\
\log_{a,b} (x) &=& \log_{\lambda (w+1),\lambda (w-1)} (x)=\frac{x^{\lambda (w+1)} - x^{\lambda (w-1)}}{2 \lambda}.
\ee
%
It important to note that the  Euler $(a,b)$-logarithm can be represented approximately by the following power series
\be
\displaystyle \log^{E}_{a,b}(x) \approx \ln (x) + \frac{1}{2} (a+b) [\ln (x)]^2 + \frac{1}{6}  (a^2 +a b +b^2) [\ln(x)]^3 \cdots.
\ee
%
This a quite general form of generalized logarithm and associated   entropy.
%
In particular cases, we  have  well known deformed logarithms:

\begin{itemize}

\item

For $a \rightarrow 0$ and $b\rightarrow 0$, we obtain classical natural logarithm $\log^{E}_{0,0}(x)= \ln(x)$ and Boltzmann-Gibbs-Shannon (BGS) entropy $S^{BGS}(\bp) = \sum_i p_i \ln (1/p_i)$ \cite{shannon1948,tsallis1988,kaniadakis2002}.\\

\item

For $a = (\sigma)^{-1} -1 $ and $b= \sigma-1$, we obtain the Abe logarithm:
\be
\log^{Abe}_{\sigma} (x) = \frac{x^{\sigma^{-1}-1} - x^{\sigma-1}}{\sigma^{-1} - \sigma}, \;\; 1/2 < \sigma < 2, \;\; \sigma \neq 0,
\ee
%
and the associated the Abe  entropy \cite{abe1997}:
\be
S^{Abe}(\bp)=\sum_i p_i \frac{p_i^{1-\sigma^{-1}} - p_i^{1-\sigma}}{\sigma^{-1} - \sigma}  = \sum_i p_i \log^{Abe}(1/p_i),
\ee


\item

For $a = 0 $ and $b= -\alpha $, we obtain the Amari  $\alpha$-logarithm defined as \cite{Amari2009}-\cite{Amari-PAN}
\be
\log^{E}_{0,-\alpha}(x)= \log_{\alpha}^A (x)= \frac{1-x^{-\alpha}}{\alpha},
\ee
which is equivalent to Tsallis $q$-logarithm for $\alpha =q-1$.
Alternatively, for $a =1-q $ and $b=0$, we obtain  the Tsallis $q$-logarithm $\log^{E}_{1-q,0}(x)= \log_q^T(x)$ and the Harvda-Charvat \cite{harvda1967} and the Tsallis entropy \cite{tsallis1988,Tsallis1994}.\\

\item

For $a =\kappa $ and $b=-\kappa$, we obtain  $\kappa$-logarithm $\log^{E}_{\kappa,-\kappa}(x)= \log_{\kappa}^K(x)$ and the Kaniadakis  entropy \cite{kaniadakis2002}\\

\item

For $a = 2 \gamma$ and $b= -\gamma $, we obtain so called $ \gamma$-logarithm $\log_{\gamma}(x)= \frac{x^{2 \gamma}- x^{-\gamma}}{3  \gamma}$ for $x >0$ and  $-1/2 < \gamma < 1/2$ \cite{kaniadakis2005}.

\end{itemize}

Summarizing,  the Euler $(a,b)$-logarithm can be described  as follows (applying L'Hospital's rule for some singular cases):
\be
 \label{deflogEL}
	\log^{E}_{a,b}(x)=\left\{
	\begin{array}{cl}
&\displaystyle \frac{x^{a}- x^{b}}{a-b} = \frac{2}{|a-b|} x^{\frac{a+b}{2}} \sinh \left( \frac{|a-b|}{2} \ln(x) \right)\;\; \text{if} \;\; x>0 \;\;
\text{for} \;  a  < 1, \;\; 0 < b < 1,\\
% \;\; \text{or} \;\;   -1 \leq b \leq 0 \leq a \leq 1, \\
\\
&\displaystyle \log^{\text{Abe}}_{\sigma} = \frac{x^{\sigma^{-1}-1} - x^{\sigma-1}}{\sigma^{-1} - \sigma}, \;\;x>0,\;\; \text{for} \; a= \sigma^{-1}-1, \; b=\sigma-1, \;\; 1/2 < \sigma < 2, \; \sigma \neq 0\; \text{(Abe logarithm)},\\
\\
&\displaystyle \log^A_{\alpha}(x) = \frac{1-x^{-\alpha}}{\alpha}, \;\;  x>0 \;\;
\text{for} \; a= 0, \;\; b=-\alpha, \;\; \alpha \neq 0, \;\; \text{(Amari $\alpha$-logarithm)} \\
\\
&\displaystyle \log^T_{q}(x) = \frac{x^{1-q}-1}{1-q}, \;\;  x>0 \;\;
\text{for} \; a= 1-q, \;\; b=0, \;\; q \neq 1, \;\; \text{(Tsallis $q$-logarithm)} \\
\\
&\displaystyle \log^K_{\kappa} (x) = \frac{x^{\kappa} - x^{-\kappa}}{2 \kappa}, \quad  x>0 \;\;
\text{for} \; a=\kappa, \; b=-\kappa,\; \kappa \in [-1,1], \;\; \kappa \neq 0,  \;\; \text{(Kaniadakis $\kappa$-logarithm)}\\
\\
&\displaystyle \log_{\gamma}(x)= \frac{x^{2 \gamma}- x^{-\gamma}}{3  \gamma}  \quad x >0, \;\text{for} \;  a= 2 \gamma, \; b=\gamma, \;\; -1/2 < \gamma < 1/2,\;\; \gamma\neq 0,\;\;  \text{(gamma logarithm)}\\
\\
&\displaystyle x^a \ln(x),  \qquad x>0, \qquad \text{for} \qquad a=  b \neq 0,\\
\\
&\ln (x), \qquad x>0, \qquad \text{for} \qquad a=b=0.
	\end{array}
	\right.
\ee

\section{Properties of  Generalized $(a,b)$-Exponential}

In general, for arbitrary set of parameters, the Euler  $(a,b)$-logarithm cannot be inverted analytically, thus it is impossible to define explicitly the corresponding generalized exponential expressed by some basic functions (except of some important special cases discussed above).
However, we found least two solutions.

The  first approach, which we propose, is  to express  the generalized $(a,b)$- exponential functions in terms  of the Lambert-Tsallis $W_q$-functions \cite{lambert_tsallis}, which are the solution of equations $W_q(z)[ 1+(1-q) W_q(z)]_+^{1/(1-q)} = z$ :
%
\be
\displaystyle \exp_{a,b} (x) = \left(\frac{W_{\frac{\lambda+1}{\lambda}} \left(\lambda \tilde{x}^{-\lambda} \right)}{\lambda}\right)^{-1/(a \lambda)}
\ee
%
where $\lambda = (a-b)/a$, $\tilde x = (a-b) x$, $q=(\lambda+1)/\lambda=(2a-b)/(a-b)$ and $W_q$ is the Lambert-Tsallis function \cite{lambert_tsallis}.\\
%
The second alternative  approach, is to use Lagrange's inversion theorem around $1$.
Applying the Lagrange inversion,  we obtained the following  power series approximation
%
\be
\displaystyle \exp_{a,b} (x) &\approx& 1+x + \frac{1}{2} (1-a-b) x^2 + \left(\frac{1}{2}(1-a-b)^2 + \frac{1}{6} (-2 +3a -a^2+3b-a b-b^2)\right) x^3  +
\cdots  \nonumber\\
&=& 1+x + \frac{1}{2} (1-a-b) x^2 +  \frac{1}{6} \left(1-3a-3b+2 a^2+5ab+2 b^2)\right) x^3  +
\cdots  \nonumber \\
&=& \exp (x) - \frac{1}{2} \left(a+b\right) x^2  -
\frac{1}{6} \left(3a +3b - 2 a^2-5 a b - 2 b^2\right) x^3 +  O(x^4),
\ee
Summarizing, we can represent the generalized $(a,b)$-exponential function which is an inverse function of the Euler $(a,b)$-logarithm as follows:
%
\be
 \label{defexpEuler}
	\exp_{a,b}(x)=\left\{
	\begin{array}{cl}
&\approx \displaystyle \exp (x) - \frac{1}{2} \left(a+b\right) x^2  -
\frac{1}{6} \left(3a +3b - 2 a^2-5 a b - 2 b^2\right) x^3   \quad \text{for} \;\; |a| <1, \; 0 \leq b \leq 1,  \\
\\
&\displaystyle \exp^A_{\alpha} (x) = \left[ 1 - \alpha \; x \right]_+^{-1/\alpha} \quad
\text{for} \; a= 0, \;\; b=-\alpha, \;\; \alpha \neq 0, \\
\\
&\displaystyle \exp^T_{q} (x) = \left[1 + (1-q) x \right]_+^{1/(1-q)},  \quad
\text{for} \;\; a= 1-q, \;\; b=0 \;\;  q \neq 1, \\
\\
&\displaystyle \exp^K_{\kappa} (x) = \left[\kappa x + \sqrt{1+\kappa^2 x^2}\right]^{1/\kappa} \quad
\text{for} \;\; a=\kappa, \;\; b= -\kappa,\;\;  \kappa \in [-1,1], \;\;\kappa \neq 0, \\
\\
&\displaystyle \exp_{\gamma}(x) = \left[\left(\frac{1+\sqrt{1-4\gamma^3 x^3}}{2}\right)^{1/3} + \left(\frac{1-\sqrt{1-4\gamma^3 x^3}}{2}\right)^{1/3}\right]^{1/\gamma}, \; a =2 \gamma, \; b=-\gamma,\\
& -1/2 < \gamma < 1/2,  \;\; \gamma \neq 0, \\
\\
&\exp (x) \qquad \text{for} \;\;a=b=0.
	\end{array}
	\right.
\ee
%
The  $(a,b)$-exponential function should satisfy (at least approximately) the following fundamental properties
\be
\exp_{a,b}(log_{ab} (x))  &=& x,  \qquad x>0,\\
log_{a,b} (\exp_{a,b}(y))  &=& y, \quad (0 < \exp_{a,b}(y)< +\infty ).
\ee
Furthermore, we have the following useful properties.
\be
\frac{d \exp_{a,b}(x)}{dx}&>&0\\
\frac{d^2 \exp_{a,b}(x)}{dx^2}&>&0\\
\exp_{a,b}(0) &=& 1,\\
F(x) &=& \int \log_{a,b} (x) d x = \frac{1}{a-b}\left[\frac{x^{a+1}}{a+1}- \frac{x^{b+1}}{b+1}\right],\\
\int_{-\infty}^{0} \exp_{a,b} (x) dx &=& \int_{0}^{1} \log_{a,b} (x) dx =\frac{1}{1+ab +a+b}, \;\; \text{for} \;\; a \, b+ a+b > -1,\\
\int_{-\infty}^{0} \frac{1}{\exp_{a,b} (-x)} dx &=& \int_{0}^{1} \log_{a,b} (1/x) dx =\frac{1}{1 -(a+b +0.5 a^2 +0.5 b^2)}, \;\; \text{for} \;\;a+b +0.5(a^2+b^2) <1.
\ee
The above equations indicate that the generalized  $(a,b)$-exponential function $\exp_{a,b}(x)$ is a strictly increasing and convex function, normalized according to $\exp_{a,b}(0) =1$, and which goes to zero fast enough to be integrable for $x \rightarrow \pm \infty$ \cite{kaniadakis2005}.

\section{MD and GEG Algorithms}

\subsection{Unnormalized GEG Updates}

Let assume that the link function is defined as  $f(\bw)= \log_{a,b}(\bw)$ and its inverse  $ f^{(-1)} (\bw) = \exp_{a,b}(\bw)$, then
using a general MD formula (\ref{f-1fDT}), and fundamental properties described above, we obtain a generalized EG/MD update:
%\be
\begin{empheq}[box=\fbox]{align}
\displaystyle \bw_{t+1} = \exp_{a,b} \left[\log_{a,b}(\bw_t) - \eta_t \nabla L(\bw_t)\right] \\
= \bw_t \otimes_{a,b}  \exp_{a,b} \left[- \eta_t  \nabla L(\bw_t)\right], \quad \bw_t >0,
\end{empheq}
%
where the generalized  $(a,b)$-multiplication is defined/determined componentwise for two vectors $\bx$ and $\by$ as follows
\be
 \bx \; \otimes_{a,b} \; \by  &=& \exp_{a,b}  \left(\log_{a,b}(\bx) \; + \; \log_{a,b}(\by)\right) \;\; \text{for} \;\;x>0, \; y>0, \;\; \text{with} \;\; \bx \otimes_{a,b} {\bf 1} = x, \; {\bf 1} \otimes_{a,b} \by =\by\\
 \bx \; \otimes_{a,b} \; \exp_{a,b} (\by) &=& \exp_{a,b} \left(\log_{a,b} (x) +y\right), \;\; x>0.
\ee
%
In the special case, for $a=b=0$ the developed GEG update simplifies to the standard unnormalized EG (EGU) (\ref{EGU})
\be
\bw_{t+1} = \bw_t \odot  \exp \left[-\eta_t \nabla L(\bw_t)\right], \quad \bw_t >0,
\ee

\subsection{Normalized Generalized Exponentiated Gradient Updates}
\label{NABEG1}
%----------------------------------------------------------------------------
In the previous section, we derived a generalized unnoromalized  EG update (GEGU)  so weights can take any nonnegative values.
However, in most practical applications the weight vectors need to be normalized to unit $\ell_1$-norm.

 In this section, we present two step iterations and  two alternative variants of normalized GEG updates.  In one  variant in each iteration the unnormalized  solution $\tilde{\bw}_{t+1}$ is  scaled (normalized) after each iteration step as $\bw_{t+1} = \tilde{\bw}_{t+1}/||\tilde{\bw}_{t+1}||_1$. Alternative variant is simple projection of the vector $\bw_{t+1}$ onto $\ell_1$-norm unit simplex $\tilde{\bw}_{t+1}$ \cite{Cichocki2024}.

 Exponentiated Gradient Descent (EG) on a simplex refers to an optimization algorithm where the weights  constrained to be a unit simplex, meaning that the weights must sum to 1 and be non-negative, and the update rule  naturally keeping the solution within the simplex boundaries.

In order to make algorithm stable and to improve its convergence property we use normalized/scaling loss/cost function defined as (see for justification and detail \cite{Cichocki2024})
\be
\widehat{L}(\bw) = L(\bw/||\bw||_1).
\ee
%
Using theory and approach presented in the previous section,  we  can derive  the following novel  normalized generalized EG (GEG):
%
%\be
\begin{empheq}[box=\fbox]{align}
\left\{
	\begin{array}{l}
 \displaystyle \tilde{\bw}_{t+1} = \exp_{a,b} \left[\log_{a,b}(\bw_t) - \eta_t \nabla \widehat{L}(\bw_t)\right] \\
\qquad \;\;\;= \bw_t \otimes_{a,b} \exp_{a,b} \left(-\eta_t \nabla \widehat{L}(\bw_t)\right) \qquad \qquad  \qquad\text{(Generalized multiplicative update)} \\
 \bw_{t+1} = \frac{\tilde{\bw}_{t+1}}{||\tilde{\bw}_{t+1}||_1}, \\
 \text{or} \hspace{9.1cm} \text{(Unit simplex projection)}\\
 \bw_{t+1} = \; \text {projection on the  unit}\;\; \ell_1-\text{norm simplex} \;(\tilde{\bw}_{t+1}),
 \end{array}
	\right.
\end{empheq}
%\ee
where the gradients of loss/cost function  can  take  two forms \cite{Cichocki2024},
\begin{empheq}[box=\fbox]{align}
%\be
	 \nabla_{\bi w} \widehat{L}(\bw_t)=
\left\{
	\begin{array}{l}
		\displaystyle
	  \nabla_{\bi w} L(\bw_t)- (\bw_t^T \nabla_{\bi w} L(\bw_t))\, {\bf 1}= \nabla_{\bi w} L(\bw_t)-  \left(\sum_{i=1}^{N} w_{i,t} \frac{\partial L(\bw_t)}{\partial w_{i,t}}\right) {\bf 1} \,
	%\label{eq_gradLIc}
	\\ \\
\text{or}\\
\\
\displaystyle	 \nabla_{\bi w} L(\bw_t)- \frac{1}{N} ({\bf 1}^T \nabla_{\bi w} L(\bw_t))\, {\bf 1}=  \nabla_{\bi w} L(\bw_t)- \frac{1}{N} \left(\sum_{i=1}^{N} \frac{\partial L(\bw_t)}{\partial w_{i,t}}\right) {\bf 1}
	\, .
	\end{array}
	\right.
	\label{Grad-Lparc}
\end{empheq}
%\ee
%
In the special case, for $a=b=0$ the proposed update is simplified to the standard EG algorithm (\ref{EG}).
Furthermore, in another special case, for $a=1-q=\beta$ and $b=0$ and assuming that learning rate is
  vector with entries $\eta_{i,t} = \eta w_{i,t}^{\gamma}, \;\;\; i=1,2\ldots,N$, i.e., $\bm{\eta}_t = \eta \bw_t^{\gamma} \in \Real^N_+ $, we obtain the following update derived recently by us using quite different approach  using alpha-beta divergences  \cite{Cichocki2024,Cia3}
\be
\tilde{\bw}_{t+1} &=& \bw_{t} \odot \exp^T_{q}\left(-\eta \bw_t^{\gamma} \odot
	\nabla_{\bi w} \widehat{L}(\bw_{t})\right) \label{EGAB1}\\
	\bw_{t+1}   &=&  \frac{\tilde{\bw}_{t+1}}{\|\tilde{\bw}_{t+1}\|_1},
\label{EGAB2}
\ee
which can written in a scalar form as
\be
\tilde{w}_{i,t+1} &=& w_{i,t}  \exp^T_{q}\left(-\eta w_{i,t}^{\gamma} \frac{\partial \widehat{L}(\bw_{t})}{\partial w_{i,t}} \right) \label{EGAB1s}\\
	w_{i,t+1}   &=&  \frac{\tilde{w}_{i,t+1}}{\sum_{i=1}^{N} \tilde{w}_{i,t}}.
\label{EGAB2s}
\ee


\section{Application of Generalized Exponentiated Gradient Algorithm for Online Portfolio Selection}
%-----------------------------------------------------------------------


In this section, we propose a new  generalized EG (GEG) algorithms for Online Portfolio Selection (OLPS). The OLPS is a fundamental research problem in the area of computational finance \cite{Li2014}-\cite{OLPS}, \cite{Tsai2023}, which has been extensively investigated in both machine learning and computational finance communities, especially for high-frequency trading where it is necessary to use relatively fast and robust on-line algorithms. OLPS has become increasingly popular in recent years, particularly with the growth of online trading platforms and availability of real-time market data.  In general, the aim of portfolio selection is to determine combinations (mix) of assets like  stocks or bonds that are optimal with respect to performance measures,  typically,  capital gains, subject  to reducing risks. A portfolio selection model is a quantitative decision rule that tells us how to invest. In other words, the goal of portfolio selection is to find the optimal mix of assets that provides the highest expected total return  with limited  risk.
The online portfolio selection (OLPS) problem differs from classical portfolio model problems,
as it involves making sequential investment decisions.

We consider a self-financed, discrete-time, no-margin, and non-short investment
environment with $N$ assets for $T$ trading periods. This period can be
chosen arbitrarily, such as a fraction of seconds, minutes, hours, days, or weeks.  In the $t$-th period, the performance of assets can be described by a vector of price
relatives, denoted by $\bx_t=[x_{1,t},x_{2,t},\ldots,x_{N,t}]^T \in \mathbf{R}_+^N $, where $x_{i,t}, \;\; (i=1,2,\ldots,N)$ is the closing price ($p_{i,t}$) of the $i$-th asset in period $t$ divided by its closing price in the previous period ($p_{i,t-1}$),
\be
	x_{i,t}=p_{i,t}/p_{i,t-1}\, .
\ee
%
The portfolio, which reflects the investment decision in the $t$-th period, is
denoted by a weight vector $\bw_t= [w_{1,t},\ldots,w_{N,t}]^T \in \mathbf{R}_+^N $,
with the constraint
that  $w_{i,t}  \geq 0, \forall i,t$  and $\|\bw_t\|_1=1$.
The $i$-th element
of $\bw_t$ specifies the proportion of the total portfolio wealth invested in $i$-th
asset in the $t$-th period.

We assume that the cumulative return obtained at the end of the $t$-th
period (e.g., one day) is completely reinvested at the beginning of period $t+1$-th and no additional wealth can be taken into the portfolio.
Initially, we assume that the portfolio is uniformly allocated; that is,
$\bw_0 = \bu\equiv \tfrac{1}{N}{\bf 1}$. In the $t$-th period, portfolio $\bw_t$ is adopted and
the price relative vector $\bx_t$ occurs at the end of this period. The increase in wealth during this period is proportional to the convex combination of relative prices $\bw_t^T \bx_t = \sum_{i=1}^N w_{i,t}\, x_{i,t}$. In the absence of transaction costs, the final cumulative wealth is expressed as:
\be
	CW_T =  CW_0 \; \prod_{t=1}^{T} \bw_t^T\bx_t,
	\label{CW_T}
\ee
where $CW_0$ denotes the initial wealth, which, for simplicity, was be set to one (e.g., one thousand dollar for the initial investment).
%
Therefore for OLPS  as online loss function has been typically used linear $L_1(\bw) = - \bw^T \bx_t$ or logarithmic $L_2(\bw) = -\ln(\bw^T \bx_t$ functions.
%
In practice, in order to improve performance  and improve robustness the relative price is often preprocessed as follows \cite{Li2012},\cite{Li2015},\cite{RMR},\cite{Cichocki2024}:
%
\be
	\hat{\bx}_{t}=
	\left\{
	\begin{array}{l}
		\bx_{t} = \bp_t \oslash \bp_{t-1}  \;\; \text{no preprocessing}\\
\text{mean}(\bp_t,\ldots,\bp_{t-n})\oslash \bp_{t} \;\; \text{estimate online mean value}\\
\ell_1\text{-norm, median}(\bp_t,\ldots,\bp_{t-n})\oslash \bp_{t} \quad \text{estimate online median value}
	\end{array}
	\right.
\ee
where $\oslash$ refers to the component-wise division of vector elements. Such preprocessing has be applied, for example, in  OLMAR algorithm \cite{Li2015}, and RMR algorithm \cite{RMR}.

%During training, we can estimate $\hat{\bx}_{t+1}$ by $\bx_{t}$, and define
%the opposite of the daily  generalized log-return as

Since in a typical market the wealth grows exponentially fast (but with a various factor depending on a market conditions), the formal analysis of our
algorithm will be presented in terms of the normalized  generalized logarithm of the wealth achieved. We propose a novel cost/loss function to easier to adopt to real data and provide more flexibility and robustness to outliers:
\be
	L(\bw) = -\log^T_q(\bw^T \hat\bx_{t})= \frac{(\bw^T \hat\bx_{t})^{1-q}-1}{q-1}, \;\; \bw^T \hat\bx_{t} >0.
 \label{lossOLPS}
\ee
 Moreover, it can continuously interpolate between two, typically used loss functions: $L_1(\bw) = - \bw^T \bx_t$ and $L_2(\bw) = -\ln(\bw^T \bx_t$.

{\bf Remark}: Other generalized logarithms discussed in this paper can be applied including the most general Euler $(a,b)$-logarithm.\\

The OLPS problem is formulated as the following constrained  optimization problem:
%
\be
	J_t(\bw) = \widehat{L}(\bw) + \lambda D_f(\bw\|\bw_{t}), \quad \text{subject to} \quad \|\bw_t\|_1=1,\; w_{i,t} >0, \;\;\; \forall i,t
\ee
 where $ \widehat{L}(\bw) = L(\bw/||\bw||_1)$ is normalized loss function,  $\lambda =1/\eta$ is a regularizer or penalty parameter that controls the smoothness of the solution and it can takes positive and small negative values in our practical implementations and  $D_f(\bw\|\bw_{t})$ regulizer/penalty is the Bregman divergence:
 \be
 D_f(\bw\|\bw_{t})= F(\bw) - F(\bw_t) - (\bw-\bw_t)^T f(\bw_t)
 \ee
 with
 \be
 f(\bw_t)=\log_{a,b} (\bw_t), \quad (f_i(\bw_t)= (w_{i,t}^a - w_{i,t}^b)/(a-b), \;\; i=1,2,\ldots,N), \ee
   and
\be
F(\bw_t) =\sum_{i=1}^N F(w_{i,t}) = \frac{1}{a-b}\sum_{i=1}^N \left(\frac{w_{i,t}^{a+1}}{a+1}
 - \frac{w_{i,t}^{b+1}}{b+1}\right).
 \ee
 %
 Differentiation of the normalized loss $\widehat{L}\bw_{t}) =L(\bw/||\bw||_1)$, with the help of \eqref{Grad-Lparc} leads to the required gradients \cite{Cichocki2024}:
%
\be
	\nabla_{\bi w} \widehat{L}(\bw_{t})
	=\left\{
	\begin{array}{ll}
	\displaystyle- \frac{\left(\hat\bx_{t}- (\bw^T_t \hat{\bx}_t) {\bf 1} \right)}{(\bw^T_t \hat{\bx}_t)^{q}} &
	\\ \\
%
\text{or}\\ \\
%
	\displaystyle- \frac{\left(\hat{\bx}_{t}- (\frac{1}{N} \sum_{i=1}^{N} \hat{x}_{i,t}) {\bf 1} \right)}{(\bw^T_t \hat{\bx}_t)^{q}}  &
	\end{array}
	\right.
\ee
%where $\bu\dot=\tfrac{1}{N}{\bf 1}$ denote the uniform portfolio and
%\be
%	\tilde{x}_{t+1}= \bw_t^T \hat\bx_{t+1},
%	\qquad
%	\bar{x}_{t+1}= \bu^T\hat\bx_{t+1}.
%\ee
%
Based on the results of the previous sections and using the above expressions, we obtain
the generalized EG update for OLPS
%\be
\begin{empheq}[box=\fbox]{align}
\displaystyle	\tilde{\bw}_{t+1} &= \bw_{t} \otimes_{a,b} \exp_{a,b} \left(-\eta_t \nabla_{\bi w} \widehat{L}(\bw_{t})\right) \nonumber	\label{Iter1-8}\\
&= \exp_{a,b} \left( \log_{a,b} (\bw_t) - \eta_t \nabla_{\bi w} \widehat{L}(\bw_{t})\right)\\
\displaystyle	\bw_{t+1}   &=  \frac{\tilde{\bw}_{t+1}}{\|\tilde{\bw}_{t+1} \|_1} .
	\label{Iter2-8}
\end{empheq}
%\ee
The above update is controlled by four hyperparameters: $(a, b, q, \eta)$.  We can learn optimal or close to optimal range of parameters by several different approaches for detail see \cite{Cichocki2024}).  In general, to optimize the range of hyperparameters ($a, b, q, \eta$), we can use several methods of machine learning: grid search, random search, Bayesian optimization or evaluation algorithms. In grid search or random search, we first define a set of possible values for each hyperparameter within a specified range, then systematically or randomly evaluate different combinations of those values to find the best performing set of hyperparameters. To improve the robustness of our evaluation, we can use cross-validation to assess algorithm performance across different data splits. Since our algorithms are relatively simple, cross-validation combined with grid search or random search  is probably the simplest and most convenient to be used to identify the optimized hyperparameters. This involves defining a hyperparameter space, creating a grid of possible combinations, and evaluating each combination using cross-validation (see also for detail  \cite{Cichocki2024}.

In the special case, for $a=\beta, \; b=0$ and $\eta_t = \eta \bw_t^{\gamma}$  the update simplifies  to the following update, controlled by four hyperparameters $(\beta, \gamma, q, \eta)$
(derived in our previous work  for special case $q=1$, using different approach \cite{Cichocki2024})
%\be
\begin{empheq}[box=\fbox]{align}
\displaystyle	\tilde{\bw}_{t+1} &= \bw_{t} \odot \exp^T_{1-\beta}
	\left(\eta \, \bw_{t}^{\gamma} \odot
	\left(\frac{\hat{\bx}_{t}- (\frac{1}{N} \sum_{i=1}^{N} \hat{x}_{i,t}) {\bf 1} }{(\bw^T_t \hat{\bx}_t)^{q}} \right)\right), \label{GEGOLPS1} \\
\bw_{t+1} &= \; \text{projection onto the unit simplex} (\tilde{\bw}_{t+1}).
	\label{GEGOPLS2}
\end{empheq}
%\ee
%
{\bf Remark}: In order to reduce transactions costs, we usually sparsify considerably  the weights
$w_{i,t}$, by neglecting
the relatively small values (by approximating them by zero) and them implement unit simplex projections. In extreme case, we can take only one the largest weight and project it to one, while rest weights are  projected to zero.

The proposed GEG/MD algorithms for OLPS are not only generalizations of the well-known EG updates, but can also be considered as extensions of several existing OLPS algorithms. It is important to note that the developed updates can be applied not only for ``Follow The Loser'' (FTL) strategy, that is, the mean reversion strategy, but also for  ``Follow The Winner'' (FTW) strategy  by suitably adjusting the sign of the  parameter $\lambda =1/\eta$ \cite{Cichocki2024}.


\section{Conclusions}

In this paper, we have developed a new  generalized multiplicative  Mirror Descent, which can be considered as an extension and generalization of the standard  Exponentiated Gradient (EG) algorithms. The proposed algorithm  can be considered as a flexible and robust generalization of existing exponentiated gradient updates and alternative to additive gradient descent algorithms, especially for sparse representations. The developed generalized EG updates can take many different forms depending on selection of $(a,b)$ hyperparameters.  They may find applications in AI, especially in learning of deep neural networks  and in machine learning  for classification, clustering and predication. The  proposed updates unveil new perspectives on the applications of generalized logarithmic and exponential functions and associated generalized entropies, especially in  Mirror Descent and  gradient descent updates in a wide spectrum of applications formulated as optimization problems.
It is of great importance  to understand the mathematical structure of the generalized logarithm and its inverse generalized  exponential in order to obtain more insight into the proposed update schemes. Motivated by this fact, we systematically revised and extended fundamental properties of the generalized Euler logarithm  and its inverse, generalized exponentials.

\begin{thebibliography}{00}
%------------------------------------------------------------------------
	
	%% \bibitem{label}
	%% Text of bibliographic item


\bibitem{abe1997}
S.~Abe. (1997).
\newblock A note on the q-deformation-theoretic aspect of the generalized
  entropies in nonextensive physics.
\newblock {\em Physics Letters A}, 224(6):326--330.

%\bibitem{abel1823}
%Abel N.H. (1823).
%\newblock Methode generale pour trouver des fonctions d’une seule quantite variable,
%lorsqu'une propriete de ces fonctions est exprimee par une equation entre deux variables.
%\newblock {\em Mag. Nat.} 1, 216–229. [Reprinted in Oeuvres Complétes (eds L Sylow, S Lie), vol. 1. Christiania (1881)]


\bibitem{Amari2009}
Amari, S. (2009).
\newblock Alpha-divergence is unique, belonging to both f-divergence and
  {B}regman divergence classes.
\newblock {\em IEEE Transactions on Informations Theory}, {\em
  55},~4925--4931.


\bibitem{Amaribook}
Amari, S.; Nagaoka, H. (2000).
\newblock {\em {M}ethods of {I}nformation {G}eometry}.
\newblock Oxford University Press, New York.

\bibitem{Amari09}
Amari, S. (2009).
\newblock Information geometry and its applications: {C}onvex function and
  dually flat manifold.
\newblock In {\em Emerging Trends in Visual Computing}; Nielsen, F., Ed.
  Springer Lecture Notes in Computer Science, pp. 75--102.


\bibitem{Amari-PAN}
Amari, S.; Cichocki, A. (2010).
\newblock Information geometry of divergence functions.
\newblock {\em Bulletin of the Polish Academy of Science}, {\em
  58},~183--195.


\bibitem{MD1}
	Amid, E., and Warmuth, M.~K. (2020).
	\newblock Reparameterizing mirror descent as gradient descent.
	\newblock {\em In Proceedings of the 34th International Conference on Neural Information Processing Systems (NIPS'20)}, Curran Associates Inc., Red Hook, NY, USA, Article 706, 8430-8439.
	
	\bibitem{MD2}
	Amid, E.  and  Warmuth, M.~K. (2020).
	\newblock Winnowing with Gradient Descent.
	\newblock {\em In Proceedings of the 33rd International Conference on Algorithmic Learning Theory}, PMLR 125:163-182.

\bibitem{Beck2003}
Beck, A. and Teboulle, M. (2003).
\newblock Mirror descent and nonlinear projected subgradient methods for convex optimization.
\newblock {\em  Operations Research Letters}, 31(3), pp.167-175.

\bibitem{ben1989}
A.~Ben-Tal, A.~Charnes, and M.~Teboulle (1989).
\newblock Entropic means.
\newblock {\em Journal of Mathematical Analysis and Applications},
  139(2):537--551.

%\bibitem{Borges2004}
%Borges, E.P. (2004).
%\newblock  A possible deformed algebra and calculus inspired in nonextensive thermostatistics.
%\newblock {\em Physica A: Statistical Mechanics and its Applications},
%  volume={340},
%  number={1-3},
%  pages={95--101},
%  publisher={Elsevier}

%
%\bibitem{borges1998q}
%  title={On a q-generalization of circular and hyperbolic functions},
%  author={Borges, E.P.},
%  journal={Journal of Physics A: Mathematical and General},
%  volume={31},
%  number={23},
%  pages={5281},
%  year={1998},
%  publisher={IOP Publishing}


\bibitem{Borges1998}
E.P. Borges and I.~Roditi. (1998).
\newblock A family of nonextensive entropies.
\newblock {\em Physics Letters A}, 246(5):399--402.

	
%\bibitem{Box1964}
%Box, G. E. P. and  Cox, D. R. (1964) .
%	\newblock { An Analysis of Transformations},
%	\newblock {\em Journal of the Royal Statistical Society. Series B (Methodological)}, 26(2): 211-52.
%	\newblock{ http://www.jstor.org/stable/2984418. Accessed 11 July 2023.}

\bibitem{Bregman1967}
Bregman, L. (1967).
\newblock The relaxation method of finding a common point of convex sets and
  its application to the solution of problems in convex programming.
\newblock {\em Comp. Math. Phys., USSR}, {\em 7},~200--217.

\bibitem{Cantruk2018}
Canturk, B., Oikonomou, T., and Baris Bagci, G. (2018).
\newblock The parameter space and third law of thermodynamics for the Borges-Roditi, Abe and Sharma-Mittal entropies.
\newblock {\em International Journal of Modern Physics B} 32(24), 1850274.

\bibitem{chakrabarti1991}
R.~Chakrabarti and R.~Jagannathan. (1991).
\newblock A (p, q)-oscillator realization of two-parameter quantum algebras.
\newblock {\em Journal of Physics A: Mathematical and General}, 24(13):L711.


\bibitem{Cichocki2024}
Cichocki, A., Cruces, S., Sarmineto A., Tanaka T. (2024).
	\newblock Generalized Exponentiated Gradient Algorithms and Their Application to On-Line Portfolio Selection.
	\newblock {\em IEEE Access},
\newblock {https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10807168}
	

\bibitem{cichocki2010}
A.~Cichocki and S.I. Amari. (2010).
\newblock Families of $\alpha$-$\beta$-and $\gamma$-divergences: Flexible and
  robust measures of similarities.
\newblock {\em Entropy}, 12(6):1532--1568.


\bibitem{Cichocki_Cruces_Amari}
Cichocki, A. and Cruces, S. and Amari, S. I. (2011).
\newblock Generalized alpha-beta divergences and their application to robust nonnegative matrix factorization.
\newblock {\em Entropy}, 13(1), pp. 134-170.

\bibitem{Cia3}
	Cichocki, A., Zdunek, R., Phan, A. H. and Amari S.I. (2009).
	\newblock Nonnegative Matrix and Tensor Factorizations.
	\newblock{\em  John Wiley and Sons}, Chapter 3, pp. 131-202.
	\newblock{https://doi.org/10.1002/9780470747278.ch3}
	

\bibitem{CichZd_ICA06}
Cichocki, A.; Zdunek, R.; Amari, S. (2006).
\newblock Csisz\'ar's divergences for nonnegative matrix factorization: Family
  of new algorithms.
\newblock {\em Springer, LNCS-3889}, {\em 3889},~32--39.

%\bibitem{corcino2020}
%Corcino, C.B. and Corcino, R.B. (2020).
%\newblock Three-Parameter Logarithm and Entropy
% \newblock {\em Journal of Function Spaces},
%  volume={2020},
%  number={1},
%  pages={9791789},
%  publisher={Wiley Online Library}

  \bibitem{Cornford24}
	Cornford, J., Pogodin, R., Ghosh, A., Sheng, K., Bicknell, B., Codol, O., Clark, B.A., Lajoie, G. and Richards, B. (2024)
	\newblock Brain-like learning with exponentiated gradients.
	\newblock {\em bioRxiv }.
	\newblock{  https://doi.org/10.1101/2024.10.25.620272 }	

\bibitem{lambert_tsallis}
Da Silva, G. B., and R. V. Ramos. (2019).
\newblock The Lambert-Tsallis $W_q$ function.
\newblock {\em Physica A: Statistical Mechanics and its Applications}.
 525: 164-170.

\bibitem{Euler1779}
 L. Euler (1779).
 \newblock De serie Lambertina plurimisque eius insignibus proprietatibus.
 \newblock {\em Acta Academiae Scientiarum Petropolitanae
(1779: II, 1783) p. 29-51, Sankt Peterburg. Leonardi Euleri Opera Omnia, Series Prima Opera Mathematica, IV 1921 p.
350-369}; http://math.dartmouth.edu/~euler.docs/originals/E532.pdf)


\bibitem{furuichi2010}
S.~Furuichi. (2010).
\newblock An axiomatic characterization of a two-parameter extended relative
  entropy.
\newblock {\em Journal of mathematical physics}, 51(12):123302.

\bibitem{EGSD}
	Ghai, U., Hazan, E. and Singer, Y. (2020).
	\newblock Exponentiated Gradient Meets Gradient Descent.
	\newblock {\em In Proceedings of the 31st International Conference on Algorithmic Learning Theory}, PMLR 117:386-407.
	\newblock{https://doi.org/10.48550/arXiv.1902.01903}

\bibitem{gomez2021}
Gomez, I.S. and Borges, E.P., (2021).
\newblock Algebraic structures and position-dependent mass Schrodinger equation from group entropy theory.
\newblock {\em }Letters in Mathematical Physics, 111(2), p.43.

\bibitem{harvda1967}
J. Harvda and F. Charvat. (1967).
\newblock Quantification method of classification processes. Concept of structural a-entropy, \newblock {\em Kybernetica}, 3, 30-45 (1967).

	\bibitem{he2008explicit}
	He, W., and  Jiang, H. (2008).
	\newblock Explicit update vs implicit update.
	\newblock In {\em 2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence)}, Hong Kong, pp. 3441-3447.
	\newblock {https://doi.org/10.1109/IJCNN.2008.4634288}

\bibitem{Helmbold98}
	Helmbold, D.P., Schapire, R.E., Singer, Y. and Warmuth, M.~K. (1998).
	\newblock On-line Portfolio Selection Using Multiplicative Updates.
	\newblock {\em Mathematical Finance}, 8: 325-347.
	\newblock{https://doi.org/10.1111/1467-9965.00058}

\bibitem{herbsterwarmuth}
	Herbster, M. and Warmuth, M.~K. (1998).
	\newblock Tracking the Best Expert.
	\newblock {\em Machine Learning}, 32:151-178.
	\newblock{https://doi.org/10.1023/A:1007424614876}
	

	\bibitem{RMR}
	D. Huang, J. Zhou, B. Li, S. C. H. Hoi and S. Zhou (2016).
	\newblock{Robust Median Reversion Strategy for Online Portfolio Selection}
	in IEEE Transactions on Knowledge and Data Engineering, vol. 28(9): 2480-2493.
	\newblock{https://doi.org/10.1109/TKDE.2016.2563433.}

%\bibitem{jackson1909}
%F.H. Jackson.
%\newblock q-form of Taylor's theorem.
%\newblock {\em Mess. Math}, 3:57, 1909.
%
%\bibitem{jackson1910}
%F.H. Jackson.
%\newblock On q—definite integrals.
%\newblock {\em Pure Appl. Math}, 41:193--403, 1910.

\bibitem{kaniadakis2002}
G.~Kaniadakis. (2002(.
\newblock Statistical mechanics in the context of special relativity.
\newblock {\em Physical Review E}, 66(5):056125.

\bibitem{kaniadakiseditorial2004}
Kaniadakis, G.; Lissia, M. (2004).
\newblock Editorial on News and expectations in thermostatistics.
\newblock {\em Phys. A }, 340, XV-XIX.

\bibitem{kaniadakis2004}
Kaniadakis, G., Lissia, M. and Scarfone, A.M., (2004).
\newblock Deformed logarithms and entropies.
\newblock {\em Physica A: Statistical Mechanics and its Applications}, 340(1-3), pp.41-49.

\bibitem{kaniadakis2005}
G.~Kaniadakis, M.~Lissia, and A.M. Scarfone. (2005).
\newblock Two-parameter deformations of logarithm, exponential, and entropy: A
  consistent framework for generalized statistical mechanics.
\newblock {\em Physical Review E}, 71(4):046128.

\bibitem{kaniadakis2017}
Kaniadakis, G., Scarfone, A.M., Sparavigna, A. and Wada, T., (2017).
\newblock Composition law of $\kappa$-entropy for statistically independent systems. \newblock {\em Physical Review E}, 95(5), p.052112.

\bibitem{EG}
	Kivinen, J. and Warmuth, M.~K. (1997).
	\newblock Exponentiated Gradient versus Gradient Descent for Linear Predictors.
	\newblock{\em Information and Computation}, 132:1-63.
	\newblock{http://dx.doi.org/10.1006/inco.1996.2612}
	
	
	\bibitem{KW1995}
	Kivinen, J., Warmuth, M. K. (1995).
	\newblock{\em Additive versus exponentiated gradient updates for linear prediction}.
	\newblock  In Proceedings of the Twenty-seventh Annual ACM Symposium on Theory of Computing (pp. 209-218).
	\newblock{https://doi.org/10.1145/225058.225121}
	

\bibitem{kullback1951}
S.~Kullback and R.A. Leibler. (1951).
\newblock On information and sufficiency.
\newblock {\em The annals of mathematical statistics}, 22(1):79-86.

\bibitem{lambert1758}
J.H. Lambert (1758).
\newblock Observationes varie in mathesin puram.
\newblock {\em Acta Helvetica, Physico-mathematicoanatomico-botanico-medica},
Basel, 3, 128-168 (1758). http://www.kuttaka.org/~JHL/L1758c.pdf

%\bibitem{lanteri2020}
%H.~Lant{\'e}ri.
%\newblock Divergences. scale invariant divergences. applications to linear
%  inverse problems. nmf blind deconvolution.
%\newblock {\em arXiv preprint arXiv:2003.01411}, 2020.

%\bibitem{lanteri2023}
%H.~Lant{\'e}ri (2023).
%\newblock Generalization of divergences by application of the deformed logarithm--Applications to linear inverse problems--Inversion algorithms.
%\newblock {\em arXiv preprint arXiv:2304.01941}, 2023.


	\bibitem{Li2014}
	Li, B. and Hoi, S.C.H. (2014).
	\newblock{ Online portfolio selection: A survey},
	\newblock {\em  ACM Computing Surveys}, 46(3), Article No. 35:1-36.
	\newblock{https://doi.org/10.1145/2512962}.
	
	\bibitem{Li2012}
	Li, B.,  Zhao, P., Hoi, S.C.H. et al. (2012).
	\newblock{\em  PAMR: Passive aggressive mean reversion strategy for portfolio selection},
	\newblock Mach Learn 87:221-258.
	\newblock{https://doi.org/10.1007/s10994-012-5281-z}
	
	\bibitem{Li2015}
	Li, B., Hoi, S.C.H., Sahoo, D. and Liu, Z.Y. (2015).
	\newblock{Moving average reversion strategy for on-line portfolio selection}.
	\newblock {\em Artificial Intelligence}, 222:104-123.
	\newblock{https://doi.org/10.1016/j.artint.2015.01.006.}
	
	\bibitem{Li2023}
	B. Li, J. Luo and H. Xu (2023).
	\newblock{\em  A Portfolio Selection Strategy Based on the Peak Price Involving Randomness},
	\newblock IEEE Access 11: 52066-52074.
	\newblock{https://doi.org/10.1109/ACCESS.2023.3278980}

\bibitem{OLPS}
	Li, B., Sahoo, D. and Hoi, S.C.H. (2016).
	\newblock{OLPS: A Toolbox for On-Line Portfolio Selection},
	\newblock {\em  Journal of Machine Learning Research}, 17(35):1-5.
	\newblock{http://jmlr.org/papers/v17/15-317.html}

\bibitem{Momentum}
	Li, Y., Zheng, X., Chen, C., Wang, J., Xu, S. (2022).
	\newblock Exponential gradient with momentum for online portfolio selection.
	\newblock{\em Expert Systems with Applications}, 187, 115889.
	\newblock{https://doi.org/10.1016/j.eswa.2021.115889}

\bibitem{EGnoise}
	Majidi, N., Amid, E., Talebi, H. and Warmuth, M.~K. (2021).
	\newblock Exponentiated Gradient Reweighting for Robust Training Under Label Noise and Beyond.
	\newblock {\em ArXiv preprint} arXiv:2104.01493.

\bibitem{mcanally1995}
D.S. McAnally. (1995).
\newblock q-exponential and q-gamma functions. i. q-exponential functionsa.
\newblock {\em Journal of Mathematical Physics}, 36(1):546--573.

%\bibitem{Minka05}
%Minka, T.
%\newblock Divergence measures and message passing.
%\newblock {\em Microsoft Research Technical Report (MSR-TR-2005)} {\bf 2005}.

\bibitem{mittal1975}
D.P. Mittal. (1975).
\newblock On some functional equations concerning entropy, directed divergence
  and inaccuracy.
\newblock {\em Metrika}, 22(1):35--45, 1975.

\bibitem{naudts2002}
J.~Naudts. (2002).
\newblock Deformed exponentials and logarithms in generalized thermostatistics.
\newblock {\em Physica A: Statistical Mechanics and its Applications},
  316(1-4):323--334, 2002.

  \bibitem{Nemirowsky}
	A. Nemirovsky and D. Yudin. (1983).
	\newblock Problem Complexity and Method Efficiency in
Optimization.
	\newblock{\em  John Wiley and Sons},	
\newblock{ https://doi.org/10.1137/1027074}

  \bibitem{Nock2023}
	Nock, R., Amid, E., Warmuth, M. K. (2023).
	\newblock{\em Boosting with Tempered Exponential Measures.}
	\newblock arXiv preprint arXiv:2306.05487.
	\newblock{https://doi.org/10.48550/arXiv.2306.05487}

\bibitem{scarfone2009}
Scarfone, A.M., Suyari, H. and Wada, T. (2009).
\newblock Gauss law of error revisited in the framework of Sharma-Taneja-Mittal information measure.
\newblock {\em Central European Journal of Physics}, 7, pp.414-420.

\bibitem{shannon1948}
C.~E. Shannon. (1948).
\newblock A mathematical theory of communication.
\newblock {\em Bell system technical journal}, 27(3):379--423, 1948.

\bibitem{shalev2011}
Shalev-Shwartz, S. (2011).
\newblock Online learning and online convex optimization.
\newblock {\em Foundations and Trends in Machine Learning}, 4(2):107-194.


\bibitem{sharma1975}
B.D. Sharma and I.J. Taneja. (1975).
\newblock Entropy of type ($\alpha$, $\beta$) and other generalized measures in
  information theory.
\newblock {\em Metrika}, 22(1):205--215.

%\bibitem{schwammle2007}
%Schw{\"a}mmle, V. and Tsallis, C. (2007).
%\newblock Two-parameter generalization of the logarithm and exponential functions
%  and {B}oltzmann-{G}ibbs-{S}hannon entropy.
% \newblock {\em Journal of Mathematical Physics},
%  volume={48},
%  number={11},
%  publisher={AIP Publishing}

\bibitem{tempesta2016}
Tempesta, P. (2016).
\newblock Formal groups and Z-entropies.
\newblock {\em Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences}, 472(2195), 20160143.

\bibitem{taneja2001}
I.J. Taneja. (2001).
\newblock Generalized information measures and their applications. on-line
  book.
\newblock {\em URL www. mtm. ufsc. br/taneja/book/book. html}.

\bibitem{taneja1989}
I.J. Taneja. (1989).
\newblock On generalized information measures and their applications.
\newblock {\em Advances in Electronics and Electron Physics}, 76:327--413.

  \bibitem{Tsai2023}
	Tsai, C. E., Cheng, H. C., Li, Y. H. (2023).
	\newblock{\em Online self-concordant and relatively smooth minimization, with applications to Online Portfolio Selection and learning Quantum States.}
	\newblock In the International Conference on Algorithmic Learning Theory, PMLR 201:1481-1483.
	\newblock{https://doi.org/10.48550/arXiv.2210.00997}

\bibitem{tsallis1988}
C.~Tsallis. (1988).
\newblock Possible generalization of {B}oltzmann-{G}ibbs statistics.
\newblock {\em Journal of statistical physics}, 52(1):479--487.

\bibitem{Tsallis1994}
	Tasllis, C. (1994).
	\newblock What are the numbers that experiments provide.
\newblock {\em Quimica Nova}, 17,6, 468--471.

\bibitem{Ensemble}
	Xie, K., Yin, J., Yu, H., Fu, H., Chu, Y. (2024).
	\newblock Passive Aggressive Ensemble for Online Portfolio Selection.
	\newblock{\em Mathematics}, 12(7), 956.
	\newblock {https://doi.org/10.3390/math12070956}

\bibitem{Yang2022}
	Yang, X., He, J. A., Zhang, Y. (2022).
	\newblock{\em Aggregating exponential gradient expert advice for online portfolio selection.}
	\newblock Journal of the Operational Research Society, 73(3), 587-597.
	\newblock{https://doi.org/10.1080/01605682.2022.2122737}

%\bibitem{Yamano2002}
%Yamano, T. (2002).
%\newblock Some properties of q-logarithm and q-exponential functions in {T}sallis statistics.
% \newblock {\em Physica A: Statistical Mechanics and its Applications},
%  volume={305},
%  number={3-4},
%  pages={486--496},
%  publisher={Elsevier}


\bibitem{wada2010}
T.~Wada and A.M. Scarfone. (2010).
\newblock Finite difference and averaging operators in generalized entropies.
\newblock In {\em Journal of Physics: Conference Series}, volume 201, page
  012005. IOP Publishing.

  \bibitem{Aggreg}
	Zhang, Y., Li, J., Yang, X., Zhang, J. (2023).
	\newblock Competitive Online Strategy Based on Improved Exponential Gradient Expert and Aggregating Method.
	\newblock{\em Computational Economics}, 1-26.
	\newblock {https://doi.org/10.1007/s10614-023-10430-2}

\bibitem{EGMA}
	Zhang, Y., Lin, H., Zheng, L. et al. (2022).
	\newblock{Adaptive online portfolio strategy based on exponential gradient updates.}
	\newblock {\em J. Comb. Optim.} 43, 672-696.
	\newblock{https://doi.org/10.1007/s10878-021-00800-7}
	

	
\end{thebibliography}

\end{document}



