\section{Related Work}
\label{sec:relatedworks}

\keypoint{Pre-trained Vision-Language Models.}
Large-scale pre-training is essential for VLMs to develop a comprehensive understanding of the relationships between images and text. The effectiveness of pre-trained VLMs on downstream tasks largely depends on the design of pre-training tasks, which define what the model learns from the data \cite{du2022survey}. Among these tasks, image-text matching (ITM) and masked language modeling (MLM) are widely adopted due to their complementary roles \cite{li2019visualbert, kim2021vilt, li2021align, bao2022vlmo}. ITM enables models to focus on the global semantics of images and text, facilitating coarse-grained alignments. In contrast, MLM encourages models to extract information from objects within images to predict masked language tokens, thereby promoting fine-grained alignments. Meanwhile, transformers \cite{vaswani2017attention} play a key role in VLMs by serving as powerful contextualizers, enabling VLMs to model complex relationships between modalities. Among VLMs, CLIP \cite{radford2021clip} employs a dual-encoder architecture trained with contrastive learning. Despite being trained on noisy web data, it demonstrates effectiveness on various downstream tasks, including complex tasks such as monocular depth estimation \cite{auty2023learning}, visual question answering \cite{song2022clip}, and instance segmentation \cite{ding2022open}.

\keypoint{Fine-tuning VLMs for Downstream Tasks.} Fine-tuning all parameters in pre-trained VLMs on limited data of downstream tasks is prone to losing the rich representations learned by the model and overfitting. To address this issue, parameter-efficient fine-tuning methods have emerged as an alternative to conventional fine-tuning approaches. One such method, CLIP-Adapter \cite{gao2024clip}, trains small adapters that transform the output embeddings of CLIP encoder into task-useful features. Another approach is prompt tuning, which focuses on tailoring the modelâ€™s input. In CoOp \cite{zhou2022learning}, a set of learnable vectors is added to the input embeddings to guide the encoder to generate task-useful embeddings. CoCoOp \cite{zhou2022conditional} conditioned the prompts on image features, generating image-adaptive prompts that improve generalization to unseen classes. MaPLe \cite{khattak2023maple} and PromptSRC \cite{khattak2023promptsrc} extend adaptability to downstream tasks by adding prompts to both text and image inputs. In contrast to the above approaches, some fine-tuning methods focus on updating only specific parameters, such as bias and normalization terms \cite{song2022clip}.

\begin{figure*}[t]
    \setlength{\abovecaptionskip}{0.1cm}  
    \setlength{\belowcaptionskip}{-0.3cm} 
    \centering
    \includegraphics[width=1\linewidth]{fig/motivation.pdf} 
    \caption{Prompt generalization evaluation. \textbf{(Top)} Heatmap visualization of similarity distribution matrices computed over all (base+new) classes. From left to right: 1) $\mathbf{P}_{\mathtt{CoOp}}$, 2) $\mathbf{P}_{\mathtt{hand}}$, 3) produced by prompts learned by CoOp with SAR applied, and 4) produced by prompts learned by TCP \cite{yao2024tcp}. In class names, \textup{L.} and \textup{B.} are abbreviations of $Land$ and $Building$, respectively. An asterisk (*) before a class name indicates that it is a new class, which was not used during prompt training. \textbf{(Bottom)} t-SNE scatterplots of logits for test images from new classes. In CoOp, the logits points corresponding to images of $River$ and $Sea\ or\ Lake$ are broadly distributed, forming an ambiguous cluster boundary. In contrast, such issues are not observed in the logits visualization of CoOp with SAR applied, thank to the guiding of SAR.}
    \label{fig:motivation}
\end{figure*}

\keypoint{Regularization for Prompt Tuning.} Various regularization techniques have been explored to learn generalizable prompts. ProGrad \cite{zhu2023prompt} ensures that the gradient for prompt tuning does not conflict with the gradient used to preserve the general knowledge of CLIP. Specifically, if the angle between the two gradient vectors is obtuse, the gradient for prompt tuning is projected to be orthogonal to the other gradient, and is used for the update. KgCoOp \cite{yao2023kgcoop} minimizes the distance between text embeddings generated by learnable prompts and those generated by hand-crafted prompts, preserving the general knowledge captured by hand-crafted prompts. LASP \cite{bulat2023lasp} introduces a text-to-text cross-entropy loss to ensure that text embeddings generated by learnable prompts are correctly classified as those generated by hand-crafted prompts for the same class. DAPT \cite{cho2023distribution} enforces a uniform distribution of text embeddings on a hypersphere to minimize overlap while encouraging image embeddings of the same class to be positioned closer together, to achieve better alignment. TPR \cite{chentpr} maximize Pearson correlation coefficient computed between the pairwise cosine similarity matrices of the original CLIP text embeddings and the learned text embeddings, to preserve the class topology. In this process, they utilize the textual descriptions of both base and new classes in the dataset. LOBG \cite{ding2024lobg} preserves the structural topology of image embeddings captured by hand-crafted prompts, which is achieved by maintaining local angular relationships among image embeddings. Unlike these methods, our method SAR utilize the similarity distribution among text embeddings, to effectively capture the relational similarities among classes.