\section{Conclusion and Limitation}
In this paper, we found that prompts trained for base classes can disrupt the semantics of unseen classes, generating text embeddings with incorrect semantic relationships among classes. To address this issue, we proposed SAR, a method that regularizes learnable prompts to preserve the similarity relationships among classes generated by hand-crafted prompts. Our method utilizes ChatGPT-4o to generate novel classes that are semantically aligned with the base classes and uses them as potential unseen classes during prompt tuning. Extensive experiments across five baselines and 11 datasets demonstrate the effectiveness of SAR in improving generalization to unseen classes. Despite its effectiveness, SAR incurs additional memory and training time to compute text embeddings for novel classes. As future work, we aim to reduce the resource cost of SAR while exploring performance enhancement strategies that do not rely on hand-crafted prompts as supervison.

