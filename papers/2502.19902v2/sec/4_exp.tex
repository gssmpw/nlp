

\section{Experiments}
\subsection{Experiments Setting}
\textbf{Environment}. Following \cite{vpt,lifshitz2024steve}, we conduct experiments in the complex, open-world environment of
Minecraft on the MineRL \cite{guss2019minerl} platform. The agent interacts with the MineRL environment at 20 frames per second, generating low-level control signals for the mouse and keyboard. For each task execution, the agent is initialized in a randomized environment, allowing us to evaluate the agent's generalization across diverse environments. Please refer to \textbf{Sup. B} for more details about the Minecraft environment. 

\noindent\textbf{Implementation details}. For the planner, we follow Li et al. \cite{li2024optimus}, using a hybrid multimodal memory empowered GPT-4V \footnote{https://openai.com/index/gpt-4v-system-card} as the agent’s planner. As for the policy, we initialize GOAP with the weights of DeepSeek-VL-1.3B \cite{lu2024deepseek} as initialization. We train it on the MGOA dataset and the publicly available OpenAI Contractor Dataset \cite{vpt} through behavior cloning. All experiments were conducted on 8x NVIDIA L40 GPUs. Training details and hyperparameter setting can be found in \textbf{Sup. D}.

\input{figures/fig-3}
\noindent\textbf{Evaluation Tasks \& Metrics}. 
Evaluation tasks are categorized into three types: \textit{Atomic Tasks}, \textit{Long-Horizon Tasks}, and \textit{Open-Ended Instruction Tasks}. For each task, the environment is randomly reinitialized on each attempt, with a minimum of 30 executions per task to ensure robustness.
\begin{itemize}
    \item \textit{Atomic Tasks} represent short-term skills in Minecraft. We select ``chop a tree to get logs \includegraphics[width=0.3cm]{figures/logo/wood.pdf}'', ``collect seeds \includegraphics[width=0.3cm]{figures/logo/seeds.pdf}'', ``collect dirt \includegraphics[width=0.3cm]{figures/logo/dirt.pdf}'', and ``mine stone \includegraphics[width=0.3cm]{figures/logo/cobblestone.pdf} with a pickaxe'' as evaluation tasks. These tasks evaluate the policy's basic capabilities in Minecraft. We report the average rewards (number of items obtained) per task execution as an evaluation metric.
    \item \textit{Long-horizon Tasks} consist of an interdependent atomic tasks sequence, where the failure of any single atomic task results in the failure of the entire sequence. These long-horizon tasks are designed to evaluate the agent’s capability to execute a series of diverse tasks continuously within a complex environment. We follow the setup of Li et al. \cite{li2024optimus}, conducting experiments on long-horizon tasks comprising 67 tasks grouped into 7 categories. We report the average Success Rate (SR) as an evaluation metric.
    \item \textit{Open-Ended Instruction Tasks} are not limited to predefined text formats; rather, they involve flexible language directives that prompt the agent to accomplish long-horizon tasks. These tasks evaluate the agent’s capacity to interpret and execute instructions expressed in open-ended natural language. We selected the Torch \includegraphics[width=0.25cm]{figures/logo/torch.pdf} from the Stone Group, Rail \includegraphics[width=0.3cm]{figures/logo/rail.pdf} from the Iron Group, Golden Shovel \includegraphics[width=0.3cm]{figures/logo/Golden_Shovel.pdf} from the Gold Group, Diamond Pickaxe \includegraphics[width=0.3cm]{figures/logo/diamond_pickaxe.pdf} from the Diamond Group, and Compass \includegraphics[width=0.3cm]{figures/logo/compass.pdf} from the Redstone Group as evaluation tasks. Given a crafting relationship graph, we instructed GPT-4V and GLM-4V \cite{glm2024chatglm} to generate five open-ended instructions for each task. This allows us to evaluate the policies' understanding and execution capabilities regarding open-ended instructions. Task instructions are provided in the \textbf{Sup. E.1}.
\end{itemize}

\input{table/tb_ablation}
\noindent\textbf{Baseline}. For \textit{Atomic Tasks} and \textit{Open-ended Instruction Tasks}, we compare GOAP with existing goal-conditioned policies, including VPT \cite{vpt}, STEVE-1 \cite{lifshitz2024steve}, GROOT \cite{cai2023groot} and FSQ GROOT \cite{wang2024omnijarvis}. For \textit{Long-horizon Tasks}, we employ GPT-4V, DEPS \cite{wang2023describe}, Jarvis-1 \cite{wang2023jarvis}, and Optimus-1 \cite{li2024optimus} as baselines. We also introduce a human-level baseline \cite{li2024optimus} to evaluate the performance gap between existing agents and human capabilities.

\subsection{Experimental Results}
The experimental results for Optimus-2 compared to the baselines across \textit{Atomic Tasks}, \textit{Long-horizon Tasks}, and \textit{Open-ended Instruction Tasks} are presented in Table \ref{tb:main_atomic}, Table \ref{tb:main_horizon}, and Table \ref{tb:main_open}, respectively. 

\textbf{GOAP excels in Atomic Tasks}. Table \ref{tb:main_atomic} shows that proposed GOAP achieves improvements of 5\%, 4\%, 31\%, and 35\% over the current SOTA on the Logs \includegraphics[width=0.3cm]{figures/logo/wood.pdf}, Seeds \includegraphics[width=0.3cm]{figures/logo/seeds.pdf}, Dirt \includegraphics[width=0.3cm]{figures/logo/dirt.pdf}, and Stone \includegraphics[width=0.3cm]{figures/logo/cobblestone.pdf}, respectively. These results demonstrate that GOAP has successfully mastered a range of short-term skills across diverse environments, and can acquire items more effectively than existing policies.


%It demonstrates that GOAP has mastered various short-term skills across diverse environments, and can acquire items more efficiently than existing policies.

\textbf{Optimus-2 surpasses SOTA in Long-horizon Tasks}. Table \ref{tb:main_horizon} shows that Optimus-2 achieved the highest success rates across all seven task groups, particularly excelling in the challenging Diamond Group and Redstone Group with success rates of 13\% and 28\%, respectively. This indicates that Optimus-2 has effectively learned complex behavior patterns across atomic tasks, enabling it to sequentially execute multiple sub-goals and successfully complete long-horizon tasks within complex environments.

\input{figures/fig-6}
\textbf{GOAP outperforms in Open-ended Instruction Tasks}. As shown in Table \ref{tb:main_open}, GOAP achieved significantly higher success rates than existing agents across all tasks. Notably, on the challenging tasks of Golden Shovel \includegraphics[width=0.3cm]{figures/logo/Golden_Shovel.pdf}, Diamond Pickaxe \includegraphics[width=0.3cm]{figures/logo/diamond_pickaxe.pdf}, and Compass \includegraphics[width=0.3cm]{figures/logo/compass.pdf}, existing policies fail to complete these tasks, whereas GOAP achieves success rates of 13\%, 16\%, and 17\%, respectively. This advantage stems from GOAP’s superior comprehension of open-ended natural language instructions, whereas existing agents exhibit weaker instruction-following capabilities. Moreover, Figure \ref{fig:fig3} illustrates an example of different policies executing an open-ended goal. Due to the limited representation capability of their goal encoders, VPT \cite{vpt} and STEVE-1 \cite{lifshitz2024steve} fail to understand the goal, ``I need some iron ores, what should I do?'' In contrast, GOAP leverages the MLLM's understanding of open-ended instructions to effectively accomplish the goal (obtaining iron ore \includegraphics[width=0.3cm]{figures/logo/iron_ore.pdf}).


%\noindent\textbf{Atomic Tasks}. Table \ref{tb:main_atomic} show that Optimus-2 achieves improvements of xx\%, xx\%, xx\%, and xx\% over the current SOTA in the ``Logs'',``Seeds'', ``dirt'', ``stone'', and ``Iron\_ore'' tasks, respectively. It demonstrates that Optimus-2 has effectively mastered various atomic tasks in diverse environments. 

%\noindent\textbf{Long-horizon Tasks}. Table \ref{tb:main_horizon} show that Optimus-2 achieved the highest success rates across all seven task groups, particularly excelling in the challenging Diamond Group and Redstone Group with success rates of xx\% and xx\%, respectively. It indicates that Optimus-2 has effectively learned behavior patterns across various tasks, enabling it to sequentially execute multiple sub-goals to complete long-horizon tasks within complex environments. 

%\noindent\textbf{Open-ended Instruction Tasks}. As shown in Table \ref{tb:main_open}, Optimus-2 achieved significantly higher success rates than existing agents across all tasks, with an average improvement of xx\%. This advantage stems from Optimus-2’s strong comprehension of open-ended natural language instructions, whereas existing agents demonstrate weaker instruction-following capabilities. Figure \ref{fig:fig4} illustrates the differences in their ability to follow open-ended instructions.


\subsection{Ablation Study}
There are many unexplored questions around best practices for developing MLLM-based policy in Minecraft. In this section, we conduct an extensive ablation study and summarize our key findings. 

\input{figures/fig-4}
\textbf{The Action-guided Behavior Encoder plays a crucial role in task execution}. As shown in Table \ref{tb:ablation}, the removal of the Causal Perceiver leads to an average performance decline of 42\% across all tasks, highlighting the importance of capturing the causal relationship between observations and actions. Moreover, eliminating the History Aggregator and Memory Bank also results in an average performance decline of 36\% across all tasks. This emphasizes the crucial role of the History Aggregator in modeling observation-action sequences and the Memory Bank in dynamically storing long-sequence information.


\textbf{LLM significantly enhances policy's ability to understand open-ended instructions}. As shown in Figure \ref{fig:fig6}, replacing the LLM backbone with a Transformer-XL leads to a noticeable decline in performance. We attribute this to the LLM's pretraining on large-scale textual corpora, which endows it with a robust comprehension of open-ended language, a capability that Transformer-XL lacks.

\textbf{A pretrained action head improves performance in Minecraft}. As shown in Table \ref{tb:main_atomic}, replacing VPT with a 2-layer MLP projector as the action head leads to a noticeable decline in Optimus-2’s performance. While MLP-based action heads have shown promising results in other domains \cite{kim2024openvla,liu2024rdt}, this substitution is less effective in the Minecraft environment. We attribute this to VPT’s extensive pretraining on large-scale gameplay data, which equips it with substantial domain-specific knowledge critical for effective task execution in Minecraft.

%\textbf{The MGOA dataset is beneficial for training GOAP}. We conducted comparative experiments to evaluate the impact of different training datasets on performance. As shown in Figure \ref{fig:fig4}, using either the OpenAI Contractor Dataset (OCD) \cite{vpt} or MGOA alone results in performance declines across all \textit{Atomic Tasks}. For example, performance on the Logs \includegraphics[width=0.3cm]{figures/logo/wood.pdf} decreases by 85\% when only MGOA is used, while performance on the Stone \includegraphics[width=0.3cm]{figures/logo/cobblestone.pdf} drops by 89\% when only OCD is used. We attribute this to the fact that, while MGOA contains aligned vision-language-action pairs, it has limited task diversity. In contrast, OCD offers a wide variety of tasks but lacks high data quality. Therefore, combining these two datasets effectively balances task diversity and data quality, leading to improved performance.

\textbf{The MGOA datsaset is beneficial for training GOAP}. We conducted comparative experiments to evaluate the impact of different training datasets on performance. As shown in Figure \ref{fig:fig4}, training only with the current most commonly used dataset, OpenAI Contractor Dataset (OCD), results in suboptimal performance for GOAP on all \textit{Atomic Tasks}. For example, compared to training with a mixed dataset, its performance on Stone \includegraphics[width=0.3cm]{figures/logo/cobblestone.pdf} dropped by 89\%. We attribute this to the fact that OCD offers a wide variety of tasks but lacks high data quality. In contrast, using our MGOA dataset, performance on the four atomic tasks improved by an average of 70\% compared to using only the OCD data. We attribute this to the fact that MGOA contains high-quality aligned goal-observation-action pairs, which is beneficial for policy training. Further, we mix the two datasets to train the policy in order to balance task diversity and data quality, leading to improved performance.



\input{figures/fig-5}
\subsection{Visualization of Behavior Representation}
%As shown in Figure \ref{fig:fig5}, we aim to visualize the effectiveness of the proposed Action-guided Behavior Encoder in capturing distinct representations for diverse tasks. 

As shown in Figure \ref{fig:fig5}, we apply t-SNE \cite{tsne} to visualize observation features extracted by ViT \cite{dosovitskiy2020vit}, MineCLIP \cite{fan2022minedojo}, and the Action-guided Behavior Encoder for four tasks. From (a) and (b) in Figure \ref{fig:fig5}, it is evident that the behavior representations extracted by ViT and MineCLIP are highly mixed, making it challenging to delineate the boundaries between different tasks. This lack of clear distinction between task-specific behavior representations can hinder the model's ability to understand the unique behavior patterns associated with each task, potentially leading to task failure. In contrast, the visualization in (c) of Figure \ref{fig:fig5} reveals clear, distinct clusters for each task, demonstrating that the Action-guided Behavior Encoder effectively captures subtle differences in observation-action sequences, thereby learning robust behavior representations across tasks.
%It highlight the encoder’s capacity to discern and encode task-specific information, validating its role in facilitating reliable and interpretable behavior representation.