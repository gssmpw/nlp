\section{Related Work}
\input{figures/fig-2}
\noindent\textbf{Minecraft Agents}. Previous works \cite{pmlr-v70-oh17a,ding2023clip4mc,cai2023open,hafner2023mastering} have constructed policies in Minecraft using reinforcement learning or imitation learning. VPT \cite{vpt} was training on large-scale video data recorded by human players, using behavior cloning to mimic human behavior patterns. GROOT \cite{cai2023groot} employs a video encoder as a goal encoder to learn semantic information from videos. However, these policies rely solely on visual observations as input and cannot follow human instructions to accomplish specific tasks. MineCLIP \cite{fan2022minedojo} introduces a video-text contrastive learning module as a reward model for policy, and STEVE-1 \cite{lifshitz2024steve} builds on VPT \cite{vpt} by incorporating MineCLIP as goal encoder, enabling policy to follow natural language instructions. Despite these advancements, these policies are constrained by language understanding and reasoning capabilities. To address this, current agents \cite{wang2023voyager,qin2023mp5,wang2023jarvis,li2024auto,li2024optimus,wang2024omnijarvis} leverage MLLM's instruction following capabilities to decompose complex tasks into executable sub-goal sequences, which are then fed into a goal-conditioned policy \cite{lifshitz2024steve,cai2023groot} or formed as executable code \cite{liu2024rl,zhu2023ghost,liu2024odyssey,zhao2023see}. Despite significant progress, the performance of current policies remains constrained by their limited ability to understand sub-goals. In this paper, we aim to develop an MLLM-based goal-conditioned policy to enhance the policyâ€™s comprehension of open-ended sub-goals, thereby improving overall performance.

\noindent\textbf{Long-term Video Modeling}. Previous work \cite{vpt,lifshitz2024steve,fan2022minedojo,cai2023groot} have segmented videos into multiple clips for training to alleviate the challenges posed by long-sequence video inputs. However, this approach prevents the agent from learning comprehensive behavior representations from the entire video. To handle long-term video sequences \cite{li2025lion,zhang24aj,zhang2024flash}, existing studies employ temporal pooling \cite{maaz2023video}, querying transformers \cite{zhang2023video,he2024ma}, or token merging \cite{zhang2024token,song2024moviechat,jin2024chat} to integrate long-sequence visual tokens. Inspired by previous works \cite{chen2020memory,lee2018memory,lee2021video,wu2022memvit}, we propose a Q-former \cite{li2023blip,dai2023instructblip} structure with a memory bank \cite{he2024ma}, enabling effective long-term sequence modeling through interactions with historical queries. Unlike existing methods that model only the observation sequence, we focus on multimodal learning \cite{shao2019multi,shao2022open,shao2024detecting}. Moreover, different from previous work \cite{he2024ma} that primarily compress video features into fixed-length tokens, our Action-guided Behavior Encoder dynamically interacts with the historical sequence at each timestep, producing behavior tokens corresponding to the observation-action sequence from the start to the current timestep.


%\noindent\textbf{Long-term Video Modeling}. Previous work \cite{vpt,lifshitz2024steve,fan2022minedojo,cai2023groot} have segmented videos into multiple clips for training to alleviate the challenges posed by long-sequence video inputs. However, this approach prevents the agent from learning comprehensive behavior representations from the entire video. To handle long-term video sequences \cite{li2023videochat,li2023mvbench,zhang2024flash}, existing studies employ temporal pooling \cite{maaz2023video}, querying transformers \cite{zhang2023video,he2024ma}, or token merging \cite{zhang2024token,song2024moviechat,jin2024chat} to integrate long-sequence visual tokens. Inspired by previous works \cite{chen2020memory,lee2018memory,lee2021video,wu2022memvit}, we propose a Q-former \cite{li2023blip,dai2023instructblip} structure with a memory bank \cite{he2024ma}, enabling effective long-term sequence modeling through interactions with historical queries. Unlike existing methods that model only the observation sequence, the Action-guided Behavior Encoder models the relationship between observation and action. Moreover, different from previous work \cite{he2024ma} that primarily compress video features into fixed-length tokens, our Action-guided Behavior Encoder dynamically interacts with the historical sequence at each timestep, producing behavior tokens corresponding to the observation-action sequence from the start to the current timestep.