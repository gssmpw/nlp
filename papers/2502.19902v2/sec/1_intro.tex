
\section{Introduction}
\input{figures/fig-1}

Enabling agents to learn human behavioral patterns for completing complex tasks in open-world environments, is a long-standing goal in the field of artificial intelligence \cite{attribute-zhy,chen2024spa,shao2023detecting,li2023fine}. To effectively handle diverse tasks in an open-world environment like Minecraft \cite{qin2023mp5,li2024auto}, a prominent agent framework \cite{wang2023describe,wang2023jarvis,qin2023mp5,li2024optimus} integrates a task planner with a goal-conditioned policy. As illustrated in Figure \ref{fig:fig1} (left), this framework first utilizes the task planner’s language comprehension and visual perception abilities to decompose complex task instructions into sequential sub-goals. These sub-goals are then processed by a goal-conditioned policy to generate actions.

Although existing agents \cite{qin2023mp5,wang2023jarvis,li2024optimus} have made promising progress by using Multimodal Large Language Models (MLLM) \cite{chen2024lion,ye2024cat,shen2025mome} as planners, the current performance bottleneck for agents lies in the improvement of the goal-conditioned policy \cite{li2024optimus}. As the sub-goal serves as a natural language description of an observation-action sequence, the goal-conditioned policy needs to learn the crucial relationships among sub-goals, observations, and actions to predict actions. However, existing goal-conditioned policies exhibit the following limitations: \textbf{(1)} Existing policies neglect the modeling of the relationship between observations and actions. As shown in Figure \ref{fig:fig1}, they only model the relationship between the sub-goal and the current observation by adding the sub-goal embedding to the observation features \cite{lifshitz2024steve,cai2023groot,wang2024omnijarvis}. However, the current observation is generated by the previous action interacting with the environment. This implies a causal relationship between action and observation, which is neglected by current policies; \textbf{(2)} Existing policies struggle to model the relationship between open-ended sub-goals and observation-action sequences. As depicted in Figure \ref{fig:fig1}, existing policies primarily rely on either video encoders \cite{cai2023groot,wang2024omnijarvis} or conditional variational autoencoders (CVAE) \cite{lifshitz2024steve} as goal encoder to produce implicit goal embeddings. Such embeddings have limited representation ability \cite{wang2024omnijarvis}. Simply adding it to observation features is sub-optimal and unable to handle the complex relationship between sub-goals and observation-action sequences.


In this paper, we propose \textbf{Optimus-2}, a novel agent that incorporates an MLLM for planning, alongside a \textbf{G}oal-\textbf{O}bservation-\textbf{A}ction Conditioned \textbf{P}olicy (GOAP). To address the aforementioned challenges, we propose GOAP, which can better model the relationship among the observations, actions, and sub-goals in two aspects. 

\textbf{An Action-guided Behavior Encoder for observation-action sequence modeling.} To capture the relationship between observations and actions, the Action-guided Behavior Encoder first employs a Causal Perceiver to integrate action embeddings into observation features. It utilizes task-relevant action information as guidance to adjust the observation features, thereby providing fine-grained observation-action information for action prediction. Additionally, to model a long-term observation-action sequence without exceeding input length limitations, a History Aggregator is introduced to dynamically integrate current observation-action information with the historical sequence into fixed-length behavior tokens. Behavior tokens can capture the long-term dependencies of the observation-action sequence with a fixed and appropriate length. It enables the agent to predict actions that align with the logic of the observation-action sequence, rather than making isolated action predictions based solely on the current observation. 
%This approach ensures that the agent’s action prediction follow a coherent behavioral pattern over time, supporting sub-goal consistency across longer observation-action sequences.

\textbf{An MLLM to model the relationship between sub-goal and observation-action sequence.} To explicitly encode the semantics of sub-goals, we introduce an MLLM as the backbone of GOAP. It aligns the sub-goal with behavior tokens to predict subsequent actions auto-regressively. Leveraging the MLLM's language comprehension and multimodal perception capabilities, it can better integrate features from open-ended sub-goals and observation-action sequences, thereby enhancing the policy's action prediction ability. To the best of our knowledge, GOAP is the first effort to employ MLLM as the core architecture of a Minecraft policy, which demonstrates strong instruction comprehension capabilities for open-ended sub-goals.

Moreover, current Minecraft datasets either lack alignment among essential elements \cite{fan2022minedojo} or are not publicly accessible \cite{vpt}, resulting in a significant scarcity of high-quality observation-goal-action pairs necessary for policy training. To this end, we introduce an automated approach for constructing the \textbf{Minecraft Goal-Observation-Action (MGOA)} dataset. The MGOA dataset comprises 25,000 videos across 8 atomic tasks, providing approximately 30 million aligned observation-goal-action pairs. It will be made openly available to support advancements within the research community. We conducted comprehensive evaluations in the open-world environment of Minecraft, and the experimental results demonstrate that Optimus-2 achieves superior performance. Compared to previous SOTA, Optimus-2 achieves an average improvements of 27\%, 10\%, and 18\% on atomic tasks, long-horizon tasks, and open-ended sub-goal tasks, respectively.


In summary, our contributions are as follows:
\begin{itemize}
    \item We propose a novel agent Optimus-2, which consists of an MLLM for planning, and a policy for low-level control. The experimental results demonstrate that Optimus-2 exhibits superior performance on atomic tasks, long-horizon tasks, and open-ended sub-goal tasks.
    \item To better model the relationship among the observations, actions, and sub-goals, we propose Goal-Observation-Action Conditioned Policy, GOAP. It contains an Action-guided Behavior Encoder for observation-action sequence modeling, and an MLLM to model the relationship between sub-goal and observation-action sequence. 
    \item To address the scarcity of large-scale, high-quality datasets, we introduce the MGOA dataset. It comprises approximately 30 million aligned observation-goal-action pairs and is generated through an automated process without any manual annotations. The proposed dataset construction method and the released MGOA dataset can contribute to the community's efforts to train agents.
\end{itemize}




