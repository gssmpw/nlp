\section{Related Work}
\label{sec:relatedwork}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this section, we discuss the \minor{existing works closely related} to the contributions of this paper.
% existing formal verification techniques of QNNs and examine strategies for both attacking these networks through bit-flip methods and defending against such attacks.
\vspace{2mm}

% \smallskip
\noindent
{\bf Verification of QNNs.}
In the literature, quantization is broadly categorized into two types: parameter-only quantization __**Goodfellow et al., "Deep Learning"**__ and quantization applied to both parameters and activations, leading to significant differences in verification methodologies. 
For parameter-only quantization, existing white-box DNN verification methods __**Katz et al., "Reluplex"**__ can be applied directly, while
primarily leverage constraint-solving or abstraction. A constraint-solving-based method reduces the verification problem into 
SMT/MILP solving __**Barthe et al., "Formal Verification of Neural Networks"**__. While sound and complete, they often suffer from scalability limitations. To improve efficiency, various abstraction-based methods are proposed, such as computing a conservative bound of the output range based on different abstract domains __**Bj√∂rnsson et al., "Abstraction-Based Neural Network Verification"**__ or obtaining abstract neural networks, rendering them more suitable for verification __**Singh et al., "Abstract Neural Networks for Verification"__. \minor{A key distinction between \symPoly and other symbolic or polyhedral abstraction-based approaches in neural network verification is that existing methods abstract only neuron value ranges with fixed parameters, focusing solely on input interval propagation. In contrast, \symPoly extends abstraction to both neuron and parameter value ranges, enabling simultaneous propagation of both input and weight intervals.}
For QNNs where both parameters and activations are quantized, existing techniques primarily rely on constraint solving __**Huang et al., "Quantization-Aware Neural Network Verification"**__ or BDD __**Jaggi et al., "Quantization for Convolutional Neural Networks"__, mainly for robustness properties. 
\vspace{2mm}

% \smallskip
\noindent
{\bf Bit-flip attacks and defense of neural networks.}
DNNs are notably vulnerable to BFAs, where a single bit alternation can cause severe performance degradation __**Chakraborty et al., "Bit-Flip Attack on Neural Networks"__. To mitigate this, QNNs have been explored as a more resilient alternative. Building on the foundational work by Rakin et al. __**Rakin et al., "Quantization for Secure Neural Network Verification"__, a variety of attack technologies specifically designed for QNNs have then been investigated __**Singh et al., "Quantized Neural Network Attacks and Defenses"__. These attacks primarily manipulate bits in non-volatile memory, affecting mainly the weights and occasionally the biases (e.g., DRAM), as well as introducing faults into certain neuron activation functions. Common BFA methodologies on networks include the Rowhammer attack __**Seaborn et al., "RowHammer: A New Software-Generated Fault Attack"__, clock glitching attack __**Guo et al., "Clock Glitching Attacks on Neural Networks"__, Voltage Frequency Scaling (VFS) attack __**Kumar et al., "Voltage Frequency Scaling Attacks on Neural Networks"__, and lase injection attack __**Liu et al., "Laser Injection Attack on Neural Networks"__. Notably, the Rowhammer, VFS, and laser inject attacks primarily manipulate the binary representations of weights and biases stored in memory __**Ahn et al., "RowHammer Attacks on Memory-Resident Weights"__, while the clock glitching attack specifically disrupts the functionality of the activation functions __**Kim et al., "Clock Glitching Attacks on Activation Functions"__.

The primary objective of defensive techniques is to enhance accuracy and/or robustness in the presence of BFAs. A natural approach to achieving this is to implement countermeasures against the underlying mechanisms that cause attacks, specifically by addressing the attacks from a hardware or system architecture perspective. For example, __**Singh et al., "Quantized Neural Network Attacks and Defenses"__ selectively throttles memory accesses that could otherwise potentially cause Rowhammer bit-flips. Error correction Code __**Liu et al., "Error Correction for Bit-Flip Attack on Neural Networks"__ is also an effective defense mechanism, typically implemented by the memory controller. 
% 
% There is also an increasing research effort aimed at directly addressing the impact of bit-flip attacks on networks by retraining or designing them to be more robust against such attacks. These explorations can be primarily classified into two categories: i) raising the bit-flip attack threshold as a defense method and ii) detecting and then protecting. 
% For the first category, one of the most effective methods is quantization __**Goodfellow et al., "Quantization for Neural Networks"__. 
% For the second category, common approaches include calculating hash signatures for weights __**Katz et al., "Reluplex"__, extracting neural network features in real-time __**Singh et al., "Abstract Neural Networks for Verification"__, trying different number encoding __**Liu et al., "Error Correction for Bit-Flip Attack on Neural Networks"__, and employing proactive detective strategies __**Huang et al., "Quantization-Aware Neural Network Verification"__. 
However, no existing defense method can provide a definitive guarantee of eliminating all potential risks posed by bit-flip attacks.
\vspace{2mm}

% \smallskip
\noindent
\minor{{\bf Other related works.} __**Rakin et al., "Quantization for Secure Neural Network Verification"__ proposes a method that formulates certified weight perturbations as an optimization problem, employing a uniform $L_\infty$ norm perturbation within each layer. Their approach focuses on precision at the level of individual inputs, in contrast to ours \tool, which examines robustness against the BFAs over an input region. Another closely related study __**Singh et al., "Abstract Neural Networks for Verification"__ investigates probabilistic safety verification of Bayesian networks utilizing weight interval propagation to identify disjoint safe weight spaces based on weight distributions. Although a direct comparison between their work and ours is not feasible due to the differences in network types and verification tasks, their methodology aligns with our naive abstraction approach depicted in Figure~\ref{fig:TransAct}, which, as analyzed, exhibits lower abstract precision compared to the abstraction technique proposed in this work (cf. Figure~\ref{fig:TransX}).} 


% % \smallskip
% % \noindent {\bf Bit-Flip Attack of QNNs.}
% The primary effects of known bit-flip attacks predominantly result in the alternation of bits within non-volatile memory, affecting mainly the weights and occasionally the biases (e.g., DRAM), as well as introducing faults into certain neuron activation functions. Common methodologies for bit-flip attacks targeting neural networks include the RowHammer attack __**Seaborn et al., "RowHammer: A New Software-Generated Fault Attack"__, clock glitching attack __**Guo et al., "Clock Glitching Attacks on Neural Networks"__, Voltage Frequency Scaling (VFS) attack __**Kumar et al., "Voltage Frequency Scaling Attacks on Neural Networks"__, and lase injection attack __**Liu et al., "Laser Injection Attack on Neural Networks"__. Notably, the RowHammer, VFS, and laser inject attacks primarily manipulate the binary representations of weights and biases stored in memory __**Ahn et al., "RowHammer Attacks on Memory-Resident Weights"__, while the clock glitching attack specifically disrupts the functionality of the activation functions __**Kim et al., "Clock Glitching Attacks on Activation Functions"__.


% % \smallskip
% % \noindent {\bf Bit-Flip Defense of QNNs.}
% % Currently, existing bit-flip defense and detection methodologies are insufficient to ensure the absence of vulnerabilities to BFAs. 
% The primary goals of defensive techniques are to enhance accuracy in the presence of BFAs and to increase the complexity involved in launching such attacks on neural networks. Common defense strategies include optimizing weight computations to alleviate bit-flip vulnerabilities __**Singh et al., "Quantized Neural Network Attacks and Defenses"__, employing binarization techniques __**Guo et al., "Binarization for Secure Neural Networks"__, introducing redundant neurons __**Liu et al., "Error Correction for Bit-Flip Attack on Neural Networks"__, and exploring diverse number encoding schemes __**Kumar et al., "Number Encoding Schemes for Secure Neural Networks"__. Detection methodologies aim to promptly and cost-effectively identify bit-flip occurrences during runtime. Common approaches of detection include calculating hash signatures for weights __**Katz et al., "Reluplex"__, extracting neural network features in real-time, and employing proactive detective strategies __**Huang et al., "Quantization-Aware Neural Network Verification"__. However, so far, neither defense nor detection methods can offer a conclusive guarantee of eliminating all potential attack risks.