\section{Overview of the ACAS Xu Benchmark}\label{sec:app-bench}
Table~\ref{tab:bench_C} gives the details of the ACAS Xu Benchmark we used throughout Section~\ref{sec:exp}. We evaluate all 45 QNNs, denoted as $N_{i,j}$ with $1\le i\le 5,1\le j\le 9$, on the 10 properties, then select the successfully proved ones, resulting in a total of $8+9+8+9+7+14=55$ network-property pairs as our benchmarks. 

\begin{table}[h]
    \centering
    \caption{Benchmarks of properties and QNNs obtained via post-quantization training for ACAS Xu.}
    \setlength{\tabcolsep}{2pt}
    \scalebox{0.8}{
    \begin{tabular}{l|l|l}
        \toprule
         Property  & Description &  Network    \\ \midrule
         Prop\_3\_WL & \tabincell{l}{If the intruder is directly ahead and is moving towards the \\ ownership, a ``Weak Left'' maneuver is advised.} & \tabincell{l} {(8): $N_{2,6},N_{2,7},N_{2,8},N_{2,9},$ \\ $N_{4,6},N_{4,7},N_{4,8},N_{4,9}$} \\ \midrule
         
         Prop\_3\_WR & \tabincell{l}{If the intruder is directly ahead and is moving towards the \\ ownership, a ``Weak Right'' maneuver is advised.} & \tabincell{l}{(9): $N_{1,6},N_{3,6},N_{3,7},N_{3,8},$ \\ $N_{3,9},N_{5,6},N_{5,7},N_{5,8},N_{5,9}$ } \\ \midrule

         Prop\_3\_SL & \tabincell{l}{If the intruder is directly ahead and is moving towards the \\ ownership, a ``Strong Left'' maneuver is advised.} & \tabincell{l}{(8): $N_{2,2},N_{2,3},N_{2,4},N_{2,5},$ \\ $N_{4,2},N_{4,3},N_{4,4},N_{4,5}$} \\ \midrule

         
         Prop\_3\_SR & \tabincell{l}{If the intruder is directly ahead and is moving towards the \\ ownership, a ``Strong Right'' maneuver is advised.} & \tabincell{l}{(9): $N_{3,1},N_{3,3},N_{3,4},N_{3,5},$ \\ $N_{5,1}, N_{5,2},N_{5,3},N_{5,4},N_{5,5}$} \\ \midrule
         
         Prop\_5\_SR & \tabincell{l}{If the intruder is near and approaching from the left, a \\ ``Strong Right'' maneuver is advised.} & \tabincell{l}{(7): $N_{3,1},N_{3,2},N_{3,3},N_{5,2},$ \\ $N_{5,3},N_{5,4}, N_{5,5}$} \\ \midrule

         
         Prop\_10\_COC & \tabincell{l}{For a far away intruder, a ``Clear of Conflict'' maneuver is \\ advised.} & \tabincell{l}{(14): $N_{1,3},N_{1,4},N_{1,5},N_{1,6}$\\ $N_{3,2},N_{3,6},N_{3,7},N_{4,1},N_{4,2},$ \\ $N_{4,4},N_{4,5},N_{5,1},N_{5,4},N_{5,6}$} \\ 
        
        \bottomrule
    \end{tabular}}
    \label{tab:bench_C}
\end{table}

\section{Missing Proofs in Sections \ref{sec:pro} and~\ref{sec:method}}

\subsection{Proof of Theorem~\ref{the:npc}}

\begin{proof}
To show that the problem of checking whether $\mN\models^\rho_{\mm,\nn}\langle \phi,\psi \rangle$ holds is in NP, we can
\begin{enumerate}
  \item {\bf Step 1:} non-deterministically guess an input $\bs{x}\in \mathbb{R}^n$ and an $(k,\nn)$-attack vector $\rho=\{(\alpha_1,P_1),\cdots, (\alpha_k,P_k)\}$ for $k\leq \mm$;
  \item {\bf Step 2:} build a new neural network $\mN^\rho$ according to the $(k,\nn)$-fault attack vector $\rho$;
  \item {\bf Step 3:} compute $\mN^\rho(\bs{x})$  by feeding the values of the input $\bs{x}$ forward through the network;
  \item {\bf Step 4:} check if both $\phi(\bs{x})$ and $\psi(\mN^\rho(\bs{x}))$ hold, and return satisfiable if both $\phi(\bs{x})$ and $\psi(\mN^\rho(\bs{x}))$ hold.
\end{enumerate}
Since Steps 2--4 can be done in polynomial time, we conclude the proof.


The NP-hardness is proved by reducing from the satisfiability problem of the vanilla neural network verification problem $\mN\models \langle \phi,\psi\rangle$ which is NP-complete~\cite{GuyKatz2017ReluplexAE}.
Consider a vanilla neural network verification problem of checking whether $\mN\models \langle \phi,\psi\rangle$ holds.
Suppose the inputs and outputs of the neural network are $\bs{x}$ and $\bs{y} = \mN(\bs{x})$, respectively.
We construct a neural network  $\mN'$ as follows:
\begin{itemize}
  \item $\mN'$ comprises $\nn+1$ copies of the network verification $\mN$ in parallel,
  \item all the copies share the same inputs $\bs{x}$,
  \item the outputs of the $i$-th copy are renamed by $\bs{y}_i$,
  \item the weights of the edges between two neurons in two different copies are $0$,  ensuring that the neurons
  in the $i$-th copy are not affected by the neurons in other copies.
\end{itemize}
Let $\psi'=\bigvee_{i=1}^{\nn+1}\psi_i$, where $\psi_i$ is obtained from the property $\psi$ by renaming
the outputs $\bs{y}$ with the outputs $\bs{y}_i$.

{\bf Claim.} \textit{For any fixed constants $\mm$ and $\nn$,
$\mN' \models^\rho_{\mm,\nn} \langle \phi',\psi'\rangle$ holds
iff $\mN \models \langle \phi,\psi\rangle$ holds.}

$(\Leftarrow)$ Suppose the vanilla neural network verification $\mN\models\langle \phi,\psi\rangle$ holds,
then for any inputs $\bs{x}\in \mathbb{R}^n$ that satisfies the pre-condition $\phi$, $\bs{y}=N(\bs{x})$ also satisfies
the post-condition $\phi$. According to the construction of $N'$, 
for any $(k,\nn)$-fault attack vector $\rho$ with $k\leq \mm$, 
there exists a copy of $N$, say
the $i$-th copy of $N$, such that the outputs $\bs{y}_i$ are the same as the outputs $\bs{y}$.
It implies that $N'(\bs{x})$ satisfies $\psi_i$, hence $\psi'$.
Thus, $\mN'\models^\rho_{\mm,\nn}\langle \phi,\psi'\rangle$ holds.

$(\Rightarrow)$
Suppose the vanilla neural network verification problem $\mN\models \langle \phi,\psi\rangle$ does not hold, then 
there exists a counterexample $\bs{x}\in \mathbb{R}^n$ such that
$\bs{x}$ satisfies the pre-condition $\phi$ but $\bs{y}=\mN(\bs{x})$ does not satisfy
the post-condition $\phi$. According to the construction of $\mN'$, 
the outputs $\mN'(\bs{x})$ of $\mN'$ under an $(\mm,0)$-fault attack vector $\rho$ (i.e., no parameters can be changed)
are $\nn+1$ copies of $\bs{y}=\mN(\bs{x})$.
Thus, $\mN'(\bs{x})$ does not satisfy $\psi'=\bigvee_{i=1}^{\nn+1}\psi_i$,
i.e., $\mN'\models^\rho_{\mm,\nn}\langle \phi,\psi'\rangle$ does not hold.
\end{proof}



\subsection{Proof of Theorem~\ref{the:deepPolyR_weight}}

\begin{proof}

We first prove the soundness of the weighted-ReLU abstract transformer. Let $\bs{a}^{i}_{k,1}=\langle a^{i,\le}_{k,1},a^{i,\ge}_{k,1},l^{i}_{k,1},u^{i}_{k,1} \rangle$ be the abstract element of $\bs{x}^i_{k,1}$ and $\bs{a}^{i}_{k,2}=\langle a^{i,\le}_{k,2},a^{i,\ge}_{k,2},l^{i}_{k,2},u^{i}_{k,2} \rangle$ be the abstract element of $\bs{x}^i_{k,2}=\text{ReLU}(\bs{x}^i_{k,1})$. 
$\gamma(\bs{a}^i_{k,1}) = \{ x\in\mathbb{R}\mid a^{i,\le}_{k,1}\le x \le a^{i,\ge}_{k,1}\}$.         
Given $\overrightarrow{\bs{W}}^{i+1}_{j,k}\in[w_l,w_u]$, we prove the soundness by considering the following 5 cases:
\begin{itemize}
        %%%%%%%%% Case - 1 %%%%%%%%% 
        \item If $l^i_{k,1}\ge 0$: $\text{ReLU}(\bs{x}^i_{k,1})=\bs{x}^i_{k,1}$ for $\bs{x}^i_{k,1}\in\gamma(\bs{a}^i_{k,1})$ and $a^{i,\le}_{k,2}=a^{i,\ge}_{k,2}=\bs{x}^{i}_{k,1}$. Then, we have $w_l \cdot \bs{x}^i_{k,1}\le \overrightarrow{\bs{W}}^{i+1}_{j,k}\cdot \text{ReLU}(\bs{x}^i_{k,1})\le w_u\cdot \bs{x}^{i}_{k,1}$ and therefore
        \begin{displaymath}
        \begin{aligned}
            \overrightarrow{\bs{W}}^{i+1}_{j,k}\cdot \text{ReLU}(\gamma(\bs{a}^i_{k,1})) & \subseteq \{ x\in\mathbb{R} \mid w_l \cdot x' \le x \le w_u \cdot x' \wedge a^{i,\le}_{k,1} \le x'\le a^{i,\ge}_{k,1}\} \\
            & = \{ x\in\mathbb{R} \mid w_l \cdot a^{i,\le}_{k,2} \le x \le w_u\cdot a^{i,\ge}_{k,2}\} = \gamma(\bs{a}^{i,*}_{k,2})
        \end{aligned}
        \end{displaymath}
        
        %%%%%%%%% Case - 2 %%%%%%%%%
        \item If $u^i_{k,1}\le 0$: $\text{ReLU}(\bs{x}^i_{k,1})=0$ for $\bs{x}^i_{k,1}\in\gamma(\bs{a}^i_{k,1})$ and $a^{i,\le}_{k,2}=a^{i,\ge}_{k,2}=0$. Then, we have $0 \le \overrightarrow{\bs{W}}^{i+1}_{j,k}\cdot \text{ReLU}(\bs{x}^i_{k,1})\le 0$ and therefore 
        \begin{displaymath}
        \begin{aligned}
            \overrightarrow{\bs{W}}^{i+1}_{j,k}\cdot \text{ReLU}(\gamma(\bs{a}^i_{k,1})) & \subseteq \{ x\in\mathbb{R} \mid 0 \le x \le 0 \wedge a^{i,\le}_{k,1} \le x'\le a^{i,\ge}_{k,1}\} \\
            & = \{ x\in\mathbb{R} \mid w_l \cdot a^{i,\le}_{k,2} \le x \le w_u\cdot a^{i,\ge}_{k,2}\} = \gamma(\bs{a}^{i,*}_{k,2})
        \end{aligned}
        \end{displaymath}
        
        %%%%%%%%% Case - 3 %%%%%%%%%
        \item If $l^i_{k,1} u^i_{k,1}<0$: we have $\lambda \cdot \bs{x}^i_{k,1}\le \text{ReLU}(\bs{x}^i_{k,1})\le \frac{u^i_{k,1}}{u^i_{k,1}-l^i_{k,1}}(\bs{x}^i_{k,1}-l^i_{k,1})$ for $\bs{x}^i_{k,1}\in\gamma(\bs{a}^i_{k,1})$.   
        $a^{i,\le}_{k,2}=\lambda\cdot \bs{x}^{i}_{k,1}$ and $a^{i,\ge}_{k,2}=\frac{u^i_{k,1}}{u^i_{k,1}-l^i_{k,1}}(\bs{x}^i_{k,1}-l^i_{k,1})$.
        \begin{itemize}
            \item When $w_l \ge 0$: we have $w_l \cdot \lambda \cdot \bs{x}^i_{k,1}\le \overrightarrow{\bs{W}}^{i+1}_{j,k}\cdot \text{ReLU}(\bs{x}^i_{k,1})\le w_u\cdot \frac{u^i_{k,1}}{u^i_{k,1}-l^i_{k,1}}(\bs{x}^i_{k,1}-l^i_{k,1})$ and therefore
            \begin{displaymath}
            \begin{aligned}
                \overrightarrow{\bs{W}}^{i+1}_{j,k}\cdot \text{ReLU}(\gamma(\bs{a}^i_{k,1})) & \subseteq \{ x\in\mathbb{R} \mid w_l \cdot \lambda \cdot x' \le x \le w_u \cdot \frac{u^i_{k,1}}{u^i_{k,1}-l^i_{k,1}}(x'-l^i_{k,1})  \wedge a^{i,\le}_{k,1} \le x'\le a^{i,\ge}_{k,1}\} \\
                & = \{ x\in\mathbb{R} \mid w_l \cdot a^{i,\le}_{k,2} \le x \le w_u\cdot a^{i,\ge}_{k,2}\} \\
                & = \{x\in\mathbb{R} \mid \tilde{a}^{i,\le}_{k,2} \le x \le \tilde{a}^{i,\ge}_{k,2}\} = \gamma(\bs{a}^{i,*}_{k,2})
            \end{aligned}
            \end{displaymath}

            \item When $w_u \le 0$: we have $w_l \cdot \frac{u^i_{k,1}}{u^i_{k,1}-l^i_{k,1}}(\bs{x}^i_{k,1}-l^i_{k,1}) \le \overrightarrow{\bs{W}}^{i+1}_{j,k}\cdot \text{ReLU}(\bs{x}^i_{k,1})\le w_u \cdot \lambda \cdot \bs{x}^i_{k,1}$ and therefore
            \begin{displaymath}
            \begin{aligned}
                \overrightarrow{\bs{W}}^{i+1}_{j,k}\cdot \text{ReLU}(\gamma(\bs{a}^i_{k,1})) & \subseteq \{ x\in\mathbb{R} \mid w_l \cdot \frac{u^i_{k,1}}{u^i_{k,1}-l^i_{k,1}}(x'-l^i_{k,1}) \le x \le  w_u \cdot \lambda \cdot x' \wedge a^{i,\le}_{k,1} \le x'\le a^{i,\ge}_{k,1}\} \\
                & = \{ x\in\mathbb{R} \mid w_l \cdot a^{i,\ge}_{k,2} \le x \le w_u\cdot a^{i,\le}_{k,2}\} = \{x\in\mathbb{R} \mid \tilde{a}^{i,\le}_{k,2} \le x \le \tilde{a}^{i,\ge}_{k,2}\} = \gamma(\bs{a}^{i,*}_{k,2})
            \end{aligned}
            \end{displaymath}

            \item When $w_l < 0 < w_u$: if $\lambda=0$, we have $0\le \text{ReLU}(\bs{x}^i_{k,1})\le \frac{u^i_{k,1}}{u^i_{k,1}-l^i_{k,1}}(\bs{x}^i_{k,1}-l^i_{k,1})$. Hence, we have $ w_l \cdot \frac{u^i_{k,1}}{u^i_{k,1}-l^i_{k,1}}(\bs{x}^i_{k,1}-l^i_{k,1}) \le \overrightarrow{\bs{W}}^{i+1}_{j,k}\cdot \text{ReLU}(\bs{x}^i_{k,1})\le w_u \cdot \frac{u^i_{k,1}}{u^i_{k,1}-l^i_{k,1}}(\bs{x}^i_{k,1}-l^i_{k,1})$. If $\lambda=1$, we have $\bs{x}^i_{k,1} \le \text{ReLU}(\bs{x}^i_{k,1})\le \frac{u^i_{k,1}}{u^i_{k,1}-l^i_{k,1}}(\bs{x}^i_{k,1}-l^i_{k,1})$. Since $w_l<0$, we have $w_l\cdot \bs{x}^i_{k,1} \ge w_l\cdot \frac{u^i_{k,1}}{u^i_{k,1}-l^i_{k,1}}(\bs{x}^i_{k,1}-l^i_{k,1})$.
            Finally, we have $ w_l \cdot \frac{u^i_{k,1}}{u^i_{k,1}-l^i_{k,1}}(\bs{x}^i_{k,1}-l^i_{k,1}) \le \overrightarrow{\bs{W}}^{i+1}_{j,k}\cdot \text{ReLU}(\bs{x}^i_{k,1})\le w_u \cdot \frac{u^i_{k,1}}{u^i_{k,1}-l^i_{k,1}}(\bs{x}^i_{k,1}-l^i_{k,1})$ for $\lambda\in\{0,1\}$ and therefore             
            \begin{displaymath}
            \begin{aligned}
                \overrightarrow{\bs{W}}^{i+1}_{j,k}\cdot \text{ReLU}(\gamma(\bs{a}^i_{k,1})) & \subseteq \{ x\in\mathbb{R} \mid w_l \cdot \frac{u^i_{k,1}(x'-l^i_{k,1})}{u^i_{k,1}-l^i_{k,1}} \le x \le  w_u \cdot \frac{u^i_{k,1}(x'-l^i_{k,1})}{u^i_{k,1}-l^i_{k,1}} \wedge a^{i,\le}_{k,1} \le x'\le a^{i,\ge}_{k,1}\} \\
                & = \{ x\in\mathbb{R} \mid w_l \cdot a^{i,\ge}_{k,2} \le x \le w_u\cdot a^{i,\ge}_{k,2}\} = \{x\in\mathbb{R} \mid \tilde{a}^{i,\le}_{k,2} \le x \le \tilde{a}^{i,\ge}_{k,2}\} = \gamma(\bs{a}^{i,*}_{k,2})
            \end{aligned}
            \end{displaymath}
        \end{itemize}
    \end{itemize}
    
    Therefore, \textbf{our weighted-ReLU abstract transformer is sound} in all cases. We then prove that the invariant preserving. Let $\gamma (\bs{a}^{i,*}_{k,2})=\{x\in\mathbb{R}\mid \tilde{a}^{i,\le}_{k,2} \le x \le \tilde{a}^{i,\ge}_{k,2}\}$ and $l^i_{k,2}\le a^{i,\le}_{k,2} \le \bs{x}^i_{k,2} \le a^{i,\ge}_{k,2} \le u^i_{k,2}$:
    \begin{itemize}
        \item If $w_l\ge 0$, we have $ w_l \cdot l^i_{k,2} \le w_l \cdot a^{i,\le}_{k,2}$ and $ w_u \cdot a^{i,\ge}_{k,2} \le w_u \cdot u^i_{k,2}$. Then, we have $\tilde{l}^i_{k,2} = w_l \cdot l^i_{k,2} \le w_l \cdot a^{i,\le}_{k,2} = \tilde{a}^{i,\le}_{k,2} \le \tilde{\bs{x}}^i_{k,2} \le \tilde{a}^{i,\ge}_{k,2} = w_u \cdot a^{i,\ge}_{k,2} \le w_u\cdot u^i_{k,2} = \tilde{u}^i_{k,2}$;

        \item If $w_u\le 0$, we have $ w_l \cdot u^i_{k,2} \le w_l \cdot a^{i,\ge}_{k,2}$ and $ w_u \cdot a^{i,\le}_{k,2} \le w_u \cdot l^i_{k,2}$. Then, we have $\tilde{l}^i_{k,2} = w_l \cdot u^i_{k,2} \le w_l \cdot a^{i,\ge}_{k,2} = \tilde{a}^{i,\le}_{k,2} \le \tilde{\bs{x}}^i_{k,2} \le \tilde{a}^{i,\ge}_{k,2} = w_u \cdot a^{i,\le}_{k,2} \le w_u\cdot l^i_{k,2} = \tilde{u}^i_{k,2}$;

        \item If $w_l<0<w_u$, we have $ w_l \cdot u^i_{k,2} \le w_l \cdot a^{i,\ge}_{k,2}$ and $ w_u \cdot a^{i,\ge}_{k,2} \le w_u \cdot u^i_{k,2}$. Then, we have $\tilde{l}^i_{k,2} = w_l \cdot u^i_{k,2} \le w_l \cdot a^{i,\ge}_{k,2} = \tilde{a}^{i,\le}_{k,2} \le \tilde{\bs{x}}^i_{k,2} \le \tilde{a}^{i,\ge}_{k,2} = w_u \cdot a^{i,\ge}_{k,2} \le w_u\cdot u^i_{k,2} = \tilde{u}^i_{k,2}$;
    \end{itemize}
    
    Therefore, \textbf{our weight-ReLU abstract transformer preserves the invariant} in all cases, and we finish the proving.
\end{proof}



% \begin{theorem}\label{the:deepPolyR}
%     The affine abstract transformer for symbolic biases preserves both soundness and the invariant. \qed
% \end{theorem}

\subsection{Proof of Theorem~\ref{the:input}}
\begin{proof}
    \major{Consider $\bs{x}^2_{j,1}=\sum_{t\in [n_{2}]\backslash k}\bs{W}^{2}_{j,t}\bs{x}^1_{t}+\overrightarrow{\bs{W}}^{2}_{j,k}\bs{x}^1_k+\bs{b}^{2}_j$, we can inherit the proof of affine abstract transformer in \cite{SGPV19} directly on all input neurons $\bs{x}^i_t$ for $t\in[n_2]\backslash k$. To prove the theorem, it remains to demonstrate that the abstract transformer for the weighted input neuron $\overrightarrow{\bs{W}}^2_{j,k}\bs{x}^1_k$ is sound, i.e., to prove that the abstraction $\langle a^\le, a^\ge, l,u\rangle$ preserves all possible concrete values of $\overrightarrow{\bs{W}}^2_{j,k}\bs{x}^1_k$ with the input range of $\bs{x}^1_k$ as $[x_l,x_u]$ and value range of $\overrightarrow{\bs{W}}^2_{j,k}$ as $[w_l,w_u]$, where $a^\le = \kappa^\le \bs{x}^1_k -\eta$, $a^\ge=\kappa^\le \bs{x}^1_k +\eta$, and:
    \begin{itemize}
        \item If $x_l\ge 0$, then $\{\kappa^\le = w_l, \kappa^\ge=w_u, \eta=0\}$; % \bs{x}^1_k -\eta$
        \item If $x_u \le 0$, then $\{\kappa^\le=w_u,\kappa^\ge=w_l,\eta=0\}$;
        \item Otherwise, $\{\kappa^\le=\frac{w_u x_u - w_l x_l}{x_u-x_l}, \kappa^\ge = \frac{w_l x_u - w_u x_l}{x_u-x_l}, \eta = \frac{x_u x_l}{x_u-x_l}(w_l-w_u)\}$.
    \end{itemize}}

    \major{Intuitively, an intuitive sound abstract transformer for the function $y=\overrightarrow{\bs{W}}^2_{j,k} \bs{x}^1_k$ considering $\bs{x}^1_k \in[x_l,x_u]$ and $\overrightarrow{\bs{W}}^2_{j,k}\in[w_l,w_u]$ is the convex quadrilateral constructed by the four vertices: $A=(x_l, w_lx_l)$, $B=(x_u,w_lx_u)$, $C=(x_l, w_ux_l)$, and $D=(x_u,w_ux_u)$. The upper and lower boundaries of the quadrilateral are shown in Table~\ref{tab:affine_bound}.}
    \begin{table}[h]
    \centering
    \caption{\major{Boundaries of the quadrilateral constructed by the four vertices: $A=(x_l, w_lx_l)$, $B=(x_u,w_lx_u)$, $C=(x_l, w_ux_l)$, and $D=(x_u,w_ux_u)$, where $\overline{AB}$ denote the boundary is the line segment of AB.}}\label{tab:affine_bound}
    \setlength{\tabcolsep}{3pt}
    \scalebox{0.85}{
    \begin{tabular}{c|c|c|c|c|c|c|c|c|c}
    \toprule
      &  \multicolumn{3}{c|}{$w_l\ge 0$} & \multicolumn{3}{c|}{$w_u \le 0$} & \multicolumn{3}{c}{$w_l < 0 <w_u$} \\ \cline{2-10}
     & $x_l \ge 0$ & $x_u \le 0$ & $x_l <0<x_u $& $x_l \ge 0$ & $x_u \le 0$ & $x_l <0<x_u $& $x_l \ge 0$ & $x_u \le 0$ & $x_l <0<x_u $ \\ \midrule % \rowcolor{gray!20}

     Upper Boundary& $\overline{CD}$ & $\overline{AB}$ & $\overline{AD}$ & $\overline{CD}$ & $\overline{AB}$ & $\overline{AD}$ & $\overline{CD}$ & $\overline{AB}$ & $\overline{AD}$\\ 
     Lower Boundary& $\overline{AB}$ & $\overline{CD}$ & $\overline{BC}$ & $\overline{AB}$ & $\overline{CD}$ & $\overline{BC}$ & $\overline{AB}$ & $\overline{CD}$ & $\overline{BC}$ \\ 
     
    \bottomrule
    \end{tabular}}
\end{table}

\major{For any $x\in[x_l,x_u]$, the segment $\overline{AB}$ is given by $y=w_l x$, $\overline{CD}$ is given by  $y=w_u x$, $\overline{AD}$ is given by $y=w_lx_l+\frac{w_ux_u-w_lx_l}{x_u-x_l}(x-x_l)$, and $\overline{BC}$ is given by $y=w_lx_u+\frac{w_lx_u-w_ux_l}{x_u-x_l}(x-x_u)$. Since these expressions fully characterize the convex hull of the function over the given input interval, the theorem is thereby proved.}
\end{proof}

\subsection{Proof of Theorem~\ref{the:deepPolyR_bias}}

\begin{proof}
    Since the abstraction loses no precision, soundness and the invariant are preserved directly.
\end{proof}



\subsection{Proof of Theorem~\ref{the:sigmoid}}

\begin{proof}

\major{We prove it by construction. }

\major{Assuming that the abstract element of $g(x)$ obtained in \deepPoly is $\langle a^\le,a^\ge, l, u\rangle$, let $\langle \tilde{a}^\le, \tilde{a}^\ge, \tilde{l},\tilde{u} \rangle$ denote the abstract element of $w\cdot g(x)$. We use $g^\prime(x)$ and $g^{\prime\prime}(x)$ to denote the first and second derivatives of $g(x)$. Next, we prove the soundness of the abstract transformer demonstrated in Table~\ref{tab:log_AbT} by construction based on the existing proof ideas on the abstract transformers of the Sigmoid and Tanh activation functions in~\cite{SGPV19}, which can be illustrated as follows:
\begin{itemize}
    \item When $l \ge 0$, $a^\le$ is given by the line segment defined by two points $(l, g(l))$ and $(u, g(u))$, i.e., the slope is $\kappa = \frac{g(u)-g(l)}{u-l}$. This is because $g$ is concave on $[l, u]$; Otherwise, $a^\le$ is given by the function defined by the point $(l, g(l))$ and a minimum slope $\kappa^\prime = \text{min}(g^\prime(l), g^\prime(u))$, i.e., $y(x)=\kappa^\prime(x-l)+g(l)$.
    This is because $g^\prime$ is non-decreasing on $(l,0]$ and decreasing on $[0, u)$, then by setting the slope as  $\kappa^\prime$, we can always guarantee that $\kappa^\prime$ is the minimum derivative for any point on $g(x)$ with $x\in[l,u]$, hence for any point $x_c\in[l,u]$, $\kappa^\prime (x-x_c)+g(x_c)$ always lies below of $g(x)$ on $[x_c,u]$. Hence, when $x_c=l$, we have $\kappa^\prime (x-l)+g(l)$ always lies below of $g(x)$ on $[l,u]$.
% 
    \item When $u \le 0$, $a^\ge$ is given by the line segment defined by two points $(l, g(l))$ and $(u, g(u))$,  i.e., the slope is $\kappa = \frac{g(u)-g(l)}{u-l}$. This is because $g$ is convex on $[l, u]$; Otherwise, $a^\ge$ is given by the function defined by the point $(u,g(u))$ and the minimum slope $\kappa^\prime = \text{min}(g^\prime(l), g^\prime(u))$, i.e., $y(x)=\kappa^\prime(x-u)+g(u)$. This is because $g^\prime$ is non-decreasing on $(l,0]$ and decreasing on $[0, u)$, then by setting the slope as  $\kappa^\prime$, we can always guarantee that $\kappa^\prime$ is the minimum derivative for any point on $g(x)$ with $x\in[l,u]$, hence for any point $x_c\in[l,u]$, $\kappa^\prime (x-x_c)+g(x_c)$ always lies above of $g(x)$ on $[l, x_c]$. Hence, when $x_c=u$, we have $\kappa^\prime (x-u)+g(u)$ always lies above of $g(x)$ on $[l,u]$.
\end{itemize}}

\major{\bf We first prove the theorem when $g(x)=\text{Sigmoid}(x)$ by construction.} 

\major{Assuming that $a^\ge_{w_u}$ is the upper boundary of $w_u\cdot \text{Sigmoid}(x)$, then we have $a^\ge_{w_u}(x)\ge w_u\cdot \text{Sigmoid}(x) \ge w\cdot \text{Sigmoid}(x)$  with $w\in[w_l,w_u] \wedge x\in[l,u]$. Similarly, assuming $a^\le_{w_l}$ is the lower boundary of $w_l\cdot \text{Sigmoid}(x)$, then we have $a^\le_{w_l}(x)\le w_l\cdot \text{Sigmoid}(x) \le w\cdot \text{Sigmoid}(x)$  with $w\in[w_l,w_u] \wedge x\in[l,u]$. Hence, $a^\le_{w_l}$ and $a^\ge_{w_u}$ are sound lower and upper boundaries, i.e., $\tilde{a}^\le$ and $\tilde{a}^\ge$, of abstract domain $w\cdot \text{Sigmoid}(x)$ with $w\in[w_l,w_u]$ on $x\in[l,u]$, respectively. 
% 
By following the above abstract element construction idea from~\cite{SGPV19}, we obtain $\tilde{a}^\le = a^\le_{w_l}$ and $\tilde{a}^\ge = a^\ge_{w_u}$ as follows:}
\begin{figure}[h]
    \centering
    \subfigure[$0 \le w_l \le w_u$.]{\label{fig:sig_pos}
        \begin{minipage}[b]{0.3\textwidth}
            \includegraphics[width=1.0\textwidth]{figs/sig_pos.pdf}
        \end{minipage}	
    }\hspace{8mm}
        \subfigure[$w_l \le w_u \le 0$.]{\label{fig:sig_neg}
        \begin{minipage}[b]{0.3\textwidth}
            \includegraphics[width=1.0\textwidth]{figs/sig_neg.pdf}
        \end{minipage}	
    }
        % 
    \caption{$w\cdot \text{Sigmoid}(x)$ with $w\in\{w_l,w_u\}$, $w_l$ in red and $w_u$ in blue.}
    \Description{Weighted sigmoid function.}
    \label{fig:weight_sigmoid}
\end{figure}
    
\major{\begin{itemize}
        \item When $w_l\ge0$ (cf. Figure~\ref{fig:sig_pos}), we have $\tilde{l}=w_l g(l)$, $\tilde{u}=w_u g(u)$, and 
        \begin{itemize}
            \item If $l \ge 0$, $a^\le_{w_l}$ is given by the line segment defined by two points $(l, w_l g(l))$ and $(u, w_lg(u))$, i.e., $a^\le_{w_l}=w_l g(l) +\frac{w_l g(u)-w_l g(l)}{u-l}(x-l)=w_lg(l)+w_l\kappa(x-l)$; 
            % 
            Otherwise, $a^\le_{w_l}$ is given by the function defined by the point $(l, w_l g(l))$ and the minimum slope of $w_l g(x)$ on $[l,u]$, i.e., $a^\le_{w_l}= w_l g(l) + \text{min}((w_l g(x))^\prime\mid_{x=u}, (w_l g(x))^\prime\mid_{x=l})(x-l)=w_lg(l)+w_l\text{min}(g^\prime(u),g^\prime(l))(x-l) = w_lg(l)+w_l\kappa^\prime (x-l)$; 
            % 
            \item If $u \le 0$, $a^\ge_{w_u}$ is given by the line segment defined by two points $(l, w_u g(l))$ and $(u, w_u g(u))$, i.e., $a^\ge_{w_u}= w_u g(u) + \frac{w_u g(u)-w_u g(l)}{u-l}(x-u)=w_ug(u)+w_u\kappa(x-u)$; 
            % 
            Otherwise, $a^\ge_{w_u}$ is given by the function defined by the point $(u, w_u g(u))$ and the minimum slope of $w_u g(x)$ on $[l,u]$, i.e., $a^\ge_{w_u}= w_u g(u) + \text{min}((w_u g(x))^\prime\mid_{x=u}, (w_u g(x))^\prime\mid_{x=l})(x-u)=w_ug(u)+w_u\text{min}(g^\prime(u),g^\prime(l))(x-u) = w_ug(u)+w_u\kappa^\prime (x-u)$;
        \end{itemize}
        % 
        \item When $w_u\le 0$ (cf. Figure~\ref{fig:sig_neg}), we have $\tilde{l}= w_l g(u)$, $\tilde{u}=w_u g(l)$, and
        \begin{itemize}
            \item If $u\le 0$, $a^\le_{w_l}$ is given by the line segment defined by two points $(l,w_lg(l))$ and $(u,w_lg(u))$, i.e., $a^\le_{w_l}= w_l g(u) + \frac{w_l g(l)-w_l g(l)}{u-l}(x-u)=w_lg(u)+w_l\kappa(x-u)$;
            % 
            Otherwise, $a^\le_{w_l}$ is given by the function defined by the point $(u,w_l g(u))$ and the maximum slope of $w_l g(x)$ on $[l,u]$, i.e., $a^\le_{w_l}= w_l g(u) + \text{max}((w_l g(x))^\prime\mid_{x=u}, (w_u g(x))^\prime\mid_{x=l})(x-u)=w_lg(u)+w_l\text{min}(g^\prime(u),g^\prime(l))(x-u) = w_lg(u)+w_l\kappa^\prime (x-u)$; %Moreover, $\tilde{l}=$
                        % 
            \item If $l \ge 0$, $a^\ge_{w_u}$ is given by the line segment defined by two points $(l,w_ug(l))$ and $(u,w_ug(u))$, i.e., $a^\ge_{w_u}= w_u g(l) + \frac{w_u g(u)-w_u g(l)}{u-l}(x-l)=w_ug(l)+w_u\kappa(x-l)$; 
            % 
            Otherwise, $a^\ge_{w_u}$ is given by the function defined by the point $(l,w_u g(l))$ and the maximum slope of $w_u g(x)$ on $[l,u]$, i.e., $a^\ge_{w_u}= w_u g(l) + \text{max}((w_u g(x))^\prime\mid_{x=u}, (w_u g(x))^\prime\mid_{x=l})(x-l)=w_ug(l)+w_u\text{min}(g^\prime(u),g^\prime(l))(x-l) = w_ug(l)+w_u\kappa^\prime (x-l)$;
        \end{itemize}
    \end{itemize}}
    
\major{Finally, given the weighted Sigmoid function $\overrightarrow{\bs{W}}^{i+1}_{j,k}\cdot \text{Sigmoid}(\bs{x}^i_{k,1})$, we can construct and obtain its sound abstract element presented in Table~\ref{tab:log_AbT}. }

\major{\bf We next prove the theorem when $g(x)=\text{Tanh}(x)$ by construction.}
\major{Note that the sign of the value changes when crossing $x=0$ in the Tanh function. To ensure soundness, we construct the abstract element based on different values of $x$ and $w$ directly as follows:}
     
\begin{figure}[h]
    \centering
    \subfigure[$0 \le w_l \le w_u$.]{\label{fig:tanh_pos}
        \begin{minipage}[b]{0.3\textwidth}
            \includegraphics[width=1.0\textwidth]{figs/tanh_pos.pdf}
        \end{minipage}	
    }\hspace{8mm}
        \subfigure[$w_l \le w_u \le 0$.]{\label{fig:tanh_neg}
        \begin{minipage}[b]{0.3\textwidth}
            \includegraphics[width=1.0\textwidth]{figs/tanh_neg.pdf}
        \end{minipage}	
    }
        
    \caption{$w\cdot \text{Tanh}(x)$ with $w\in\{w_l,w_u\}$, $w_l$ in red and $w_u$ in blue.}
    \Description{Weighted tanh function.}
    \label{fig:weight_sigmoid}
\end{figure}

\major{\begin{itemize}
    \item When $w_l\ge0$ (cf. Figure~\ref{fig:tanh_pos}):
    \begin{itemize}
        \item If $l\ge 0$, then $\tilde{l}=w_l g(l)$, $\tilde{u}=w_u g(u)$, $\tilde{a}^\le=a^\le_{w_l}$, and $\tilde{a}^\ge = a^\ge_{w_u}$, where $a^\le_{w_l} = w_l g(l) + w_l \kappa (x-l)$ and $a^\ge_{w_u} = w_u g(u) + w_u \kappa^\prime (x-u)$;
% 
        \item If $u\le 0$, then $\tilde{l}=w_u g(l)$, $\tilde{u}=w_l g(u)$, $\tilde{a}^\le = a^\le_{w_u}$, and $\tilde{a}^\ge = a^\ge_{w_l}$, where $a^\le_{w_u} = w_u g(l) + w_u \kappa^\prime (x-l)$ and $a^\ge_{w_l}= w_l g(u) + w_l \kappa (x-u)$;
% 
        \item If $l<0<u$, then $\tilde{l}= w_u g(l)$, $\tilde{u}=w_u g(u)$. $\tilde{a}^\le$ is given by the point $(l, w_u g(l))$ and the minimum slope of all slopes of $w_l g(x)$ and $w_u g(x)$ on $[l,u]$, i.e., $\tilde{a}^\le = w_u g(l) + \text{min} ( w_u g^\prime(x)\mid_{x=u}, w_u g^\prime(x)\mid_{x=l}, w_l g^\prime(x)\mid_{x=l}, w_l g^\prime(x)\mid_{x=u}) (x-l) = w_u g(l) + \text{min}(w_l g^\prime(x)\mid_{x=u}, w_l g^\prime(x)\mid_{x=l})(x-l) = w_u g(l)+ w_l\kappa^\prime (x-l)$; Similarly, $\tilde{a}^\ge$ is given by the point $(u, w_u g(u))$ and the same minimum slope, i.e., $\tilde{a}^\ge = w_u g(u) + w_l \kappa^\prime (x-u)$.
    \end{itemize}    
    % 
    % 
    \item When $w_u \le 0$ (cf. Figure~\ref{fig:tanh_neg}):
    \begin{itemize}
        \item If $l\ge 0$, then $\tilde{l}=w_l g(u)$, $\tilde{u}=w_u g(l)$, $\tilde{a}^\le = a^\le_{w_l}$, and $\tilde{a}^\ge=a^\ge_{w_u}$. $a^\le_{w_l}$ is given by function defined by the point $(u, w_l g(u))$ and the maximum slope $\text{max}(w_l g^\prime(l), w_l g^\prime(u))= w_l \kappa^\prime$, i.e., $\tilde{a}^\le= w_l g(u) + w_l \kappa^\prime (x-u)$. $a^\ge_{w_u}$ is given by line segment defined by two points $(l, w_u g(l))$ and $(u, w_u g(u))$, i.e., $\tilde{a}^\ge = w_u g(l) + w_u \kappa (x-l)$;
% 
        \item If $u\le 0$, then $\tilde{l}= w_u g(u)$, $\tilde{u}=w_l g(l)$, $\tilde{a}^\le = a^\le_{w_u}$, and $\tilde{a}^\ge = a^\ge_{w_l}$. $a^\le_{w_u}$ is given by line segment by two points $(l, w_u g(l))$ and $(u, w_u g(u))$, i.e., $\tilde{a}^\le = w_u g(u) + w_u \kappa (x-u)$. $a^\ge_{w_l}$ is given by function defined by the point $(l, w_l g(l))$ and the maximum slope of $w_l g(x)$ on $[l,u]$, i.e., $\tilde{a}^\ge = w_l g(l) + w_l \kappa^\prime (x-l)$;
% 
        \item If $l < 0 < u$, then $\tilde{l}= w_l g(u)$, $\tilde{u}=w_l g(l)$. $\tilde{a}^\le$ is given by the point $u, w_l g(u)$ and the maximum slope of all slopes of $w_l g(x)$ and $w_u g(x)$ on $[l,u]$, i.e., $\tilde{a}^\le = w_l g(u) +  \text{max}(w_l g^\prime(x)\mid_{x=u}, w_l g^\prime(x)\mid_{x=l}, w_u g^\prime(x)\mid_{x=u}, w_u g^\prime(x)\mid_{x=l}) (x-u) = w_l g(u) + \text{max}(w_u g^\prime(x)\mid_{x=u}, w_u g^\prime(x)\mid_{x=l}) (x-u)$ $ = w_l g(u) + w_u \kappa^\prime (x-u)$; Similarly, $\tilde{a}^\ge$ is given by the point $(l, w_l g(l))$ and the same maximum slope, i.e., $\tilde{a}^\ge = w_l g(l) + w_u \kappa^\prime(x-l)$.
    \end{itemize}
\end{itemize}}

\major{Finally, given the weighted Tanh function $\overrightarrow{\bs{W}}^{i+1}_{j,k}\cdot \text{Tanh}(\bs{x}^i_{k,1})$, we can construct and obtain its sound abstract element presented in Table~\ref{tab:log_AbT}.}
\end{proof}


\section{\major{Illustration of Interval Partition Effectiveness}}\label{sec:app_ip_eff}
\major{This section presents an illustrative example (see Figure~\ref{fig:aff_illu}) to explain how the interval partition, as part of the binary search strategy, can enhance abstraction precision, albeit to a limited extent. Specifically, the blue-shaded region in Figure~\ref{fig:affine_0} represents the value domain of the abstract element when abstraction is directly applied considering $w_l\le w \le w_u$ with $w_l <0<w_u$. In contrast, the green and red regions, shown in Figures~\ref{fig:affine_0_1} and \ref{fig:affine_0_2}, respectively, illustrate the value domains of the abstract elements obtained by separately applying abstraction concerning $w_l\le w \le 0$ and $0\le w\le w_u$. 
By combining the attraction results after interval partition, we can find that the final union of value domains, as depicted in Figure~\ref{fig:affine_1}, is more precise (i.e., smaller) compared to that in Figure~\ref{fig:affine_0}. This demonstrates an improvement in abstraction precision achieved through the interval partition strategy. The ground truth of the abstraction in Figure~\ref{fig:affine_2}.}

\begin{figure}[h]
        \centering
        \subfigure[Abstraction concerning ${[w_l,w_u]}$.]{\label{fig:affine_0}
            \begin{minipage}[b]{0.15\textwidth}
                \includegraphics[width=1.0\textwidth]{figs/affine_0.pdf}
            \end{minipage}	
        } \hspace{2mm}
        \subfigure[Abstraction concerning ${[w_l,0]}$.]{\label{fig:affine_0_1}
            \begin{minipage}[b]{0.15\textwidth}
                \includegraphics[width=1.0\textwidth]{figs/affine_0_1.pdf}
            \end{minipage}	
        }\hspace{2mm}
        \subfigure[Abstraction concerning ${[0,w_u]}$.]{\label{fig:affine_0_2}
            \begin{minipage}[b]{0.15\textwidth}
                \includegraphics[width=1.0\textwidth]{figs/affine_0_2.pdf}
            \end{minipage}	
        }\hspace{2mm}
        \subfigure[The union of \ref{fig:affine_0_1} and \ref{fig:affine_0_2}.]{\label{fig:affine_1}
            \begin{minipage}[b]{0.15\textwidth}
                \includegraphics[width=1.0\textwidth]{figs/affine_1.pdf}
            \end{minipage}	
        }\hspace{2mm}
        \subfigure[The abstraction ground truth.]{\label{fig:affine_2}
            \begin{minipage}[b]{0.15\textwidth}
                \includegraphics[width=1.0\textwidth]{figs/affine_2.pdf}
            \end{minipage}	
        }
        \caption{\major{An illustration example explaining why interval partitions can enhance the abstraction precision when $w_l < 0 < w_u$ for the weighted input neuron $\overrightarrow{\bs{W}}^2_{j,k}\bs{x}^i_k$ with $\overrightarrow{\bs{W}}^2_{j,k}\in[w_l,w_u]$.}}
        \Description{Weighted tanh function.}
        \label{fig:aff_illu}
\end{figure}


\section{Additional Experimental Results}\label{sec:app_exp}
\major{This section presents the details of the experimental results by \tool without Binary Search in {\sf BFA\_RA}, which are omitted in Section~\ref{sec:exp_tool_small} and Section~\ref{sec:exp_tool_large}.}

\begin{table}[h]
    \centering
    \caption{\major{Verification results of \tool without Binary Search in BFA\_RA on ACAS Xu.}}\label{tab:tool_acas_wo_bs}
    % \setlength{\tabcolsep}{3pt}
    \scalebox{0.9}{
    \begin{tabular}{c|c|c|c|c|c|c|c}
    \toprule
      &  \multicolumn{2}{c|}{{\sf BFA\_RA}} & \multicolumn{2}{c|}{{\sf BFA\_MILP}} & \multicolumn{2}{c|}{AvgTime(s)} & \\ \cline{2-7}
     \multirow{-2}*{Property} & \#Safe\_Paras & \#Proved & \#Proved & \#Falsified & {\sf BFA\_RA} & {\sf BFA\_MILP}& \multirow{-2}*{\#TO} \\ \midrule \rowcolor{gray!20}

     Prop\_3\_WL & 99.7\% & 0 & 0 & 24 & 353.4 & 34.6 & 0\\ 
     Prop\_3\_WR & 99.8\% & 0 & 1 & 26 & 355.7 & 8.4 & 0\\ \rowcolor{gray!20}
     Prop\_3\_SL & 99.8\% & 0 & 0 & 24 & 356.5 & 15.3 & 0 \\
     Prop\_3\_SR & 99.5\% & 0 & 0 & 27 & 357.3 & 176.3 & 0\\ \rowcolor{gray!20}
     Prop\_5\_SR & 97.4\% & 0 & 0 & 18 & 379.0 & 376.3 & 3\\
     Prop\_10\_COC & 99.3\% & 9 & 1 & 10 & 361.7 & 84.7 & 22\\
     
    \bottomrule
    \end{tabular}}
\end{table}


\begin{table}[h]
    \centering
    \caption{\major{Verification results of \tool without Binary Search in BFA\_RA on MNIST for small networks when $(Q,r,\nn)=(8,0,1)$.}}\label{tab:tool_mnist_small}
    \setlength{\tabcolsep}{3pt}
    \scalebox{0.85}{
    \begin{tabular}{c|c|c|c|c|c|c|c}
    \toprule
      &  \multicolumn{2}{c|}{{\sf BFA\_RA}} & \multicolumn{2}{c|}{{\sf BFA\_MILP}} & \multicolumn{2}{c|}{AvgTime(s)} & \\ \cline{2-7}
     \multirow{-2}*{Network} & \#Safe\_Paras & \#Proved & \#Proved & \#Falsified & {\sf BFA\_RA} & {\sf BFA\_MILP} & \multirow{-2}*{\#TO} \\ \midrule \rowcolor{gray!20}

    3blk\_10 & 99.7\% & 0 & 0 & 20 & 30.8 & 0.2 & 0  \\ 
    3blk\_30 & 99.9\% & 11 & 0 & 9 & 103.4 & 0.4 & 0 \\ \rowcolor{gray!20}
    3blk\_50 & 99.9\% & 19 & 1 & 0 & 204.7 & 0.6 & 0 \\
    5blk\_10 & 99.2\% & 0 & 0 & 20 & 47.1 & 0.4 & 0 \\ \rowcolor{gray!20}
    5blk\_30 & 99.9\% & 17 & 3 & 0 & 171.4 & 0.4 & 0 \\
    5blk\_50 & 99.9\% & 0 & 0 & 20 & 349.5 & 0.8 & 0 \\
     
    \bottomrule
    \end{tabular}}
\end{table} 

\begin{table}[h]
    \centering
    \caption{\major{Detailed verification results of \tool without Binary Search in {\sf BFA\_RA} on 3blk\_100 and 5blk\_100 with $Q\in\{4,8\}$.}}\label{tab:tool_large_2}
    \setlength{\tabcolsep}{3pt}
    \scalebox{0.85}{
    \begin{tabular}{c|c|c|c|c|c|c|c|c|c}
    \toprule
     &  &  & \multicolumn{2}{c|}{{\sf BFA\_RA}} & \multicolumn{2}{c|}{{\sf BFA\_MILP}} & \multicolumn{2}{c|}{AvgTime(s)} & \\ \cline{4-9}
     \multirow{-2}*{ } & \multirow{-2}*{r} & \multirow{-2}*{$\nn$ } & \#Safe\_Paras & \#Proved & \#Proved & \#Falsified & {\sf BFA\_RA} & {\sf BFA\_MILP} & \multirow{-2}*{\#TO} \\ \midrule 

     &   & 1 & 100.0\% & 40 & 0 & 0 & 727.1 & 0 & 0\\\rowcolor{gray!20}
     \cellcolor{white} & 0 & 2 & 100.0\% & 40 & 0 & 0 & 747.7 & 0 & 0\\ 
     &   & 4 & 100.0\% & 40 & 0 & 0 & 760.8 & 0 & 0\\ \cline{2-10}

     &   & 1 & 100.0\% & 40 & 0 & 0 & 2025.1 & 0 & 0\\\rowcolor{gray!20}
     \cellcolor{white} $Q=4$ & 2 & 2 & 99.9\% & 38 & 1 & 1 & 2058.7 & 6.5 & 0\\ 
     &   & 4 & 99.9\% & 38 & 1 & 1 & 2034.2 & 6.8 & 0\\ \cline{2-10}

     &   & 1 & 99.8\% & 29 & 3 & 0 & 2037.0 & 1038.9 & 8 \\ \rowcolor{gray!20}
     \cellcolor{white}  & 4 & 2 & 99.7\% & 19 & 3 & 3 & 2044.2 & 694.5 & 15 \\ 
     &   & 4 & 99.8\% & 19 & 2 & 2 & 2032.4 & 103.0 & 17 \\ %\cline{1-10}

    \bottomrule
    \bottomrule

    & &  & \multicolumn{2}{c|}{{\sf BFA\_RA}} & \multicolumn{2}{c|}{{\sf BFA\_MILP}} & \multicolumn{2}{c|}{AvgTime(s)} & \\ \cline{4-9}
     \multirow{-2}*{} &  \multirow{-2}*{r} & \multirow{-2}*{$\nn$ } & \#Safe\_Paras & \#Proved & \#Proved & \#Falsified & {\sf BFA\_RA} & {\sf BFA\_MILP} & \multirow{-2}*{\#TO} \\ \midrule

      &  & 1 & 100.0\% & 40 & 0 & 0 & 717.6 & 0 & 0\\ \rowcolor{gray!20}
    \cellcolor{white}  & 0 & 2 & 100.0\% & 40 & 0 & 0 & 738.6 & 0 & 0\\ 
     &   & 4 & 99.9\% & 39 & 0 & 1 & 755.5 & 1.7 & 0\\ \cline{2-10}

     &   & 1 & 99.9\% & 38 & 1 & 1 & 2036.7 & 2.7 & 0\\ \rowcolor{gray!20}
     \cellcolor{white} $Q=8$ & 2 & 2 & 99.9\% & 37 & 1 & 2 & 2029.5 & 8.1 & 0\\ 
     &   & 4 & 99.9\% & 36 & 0 & 3 & 2027.9 & 4.6 & 1 \\ \cline{2-10}

     &   & 1 & 99.5\% & 22 & 3 & 1 & 2046.6 & 1419.0 & 14 \\ \rowcolor{gray!20}
     \cellcolor{white} & 4 & 2 & 99.3\% & 18 & 3 & 1 & 2036.1 & 858.4 & 18 \\ 
     &   & 4 & 99.4\% & 18 & 1 & 3 & 2067.3 & 295.6 & 18 \\
    
    \bottomrule
    \end{tabular}}
\end{table}
