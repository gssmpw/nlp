%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodology of \tool}\label{sec:method}
% \section{\deepPolyR: An Abstract Domain for Symbolic Parameters}\label{sec:method}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this work, we operate under the assumption that the adversary is limited to attacking a small number of parameters in a QNN, specifically targeting only one parameter by default ($\mm=1$). 
% The rationale behind the assumption is twofold: i) the adversaries cannot flip as many bits as they desire at any desired location. State-of-the-art BFA tools like DeepHammer~\cite{yao2020deephammer} can support only one bit flip in a 4KB space in memory, and ii) the recent research~\cite{1bitallyouneed} shows that an adversary can easily attack a QNN by flipping only one critical bit on average in the deployment stage. 
Note that even if the adversary is limited to flipping only one parameter, the number of possible $(1,\nn)$-attack vectors is still $K\cdot\sum_{i=1}^\nn\binom{Q}{i}$.
% 
Consider a QNN $\mN$ which is quantized by $Q$ and comprises $K$ parameters. The naive method introduced in Section~\ref{sec:proDef} can only verify each $(1,\nn)$-attack vector separately and invokes $K\cdot\sum_{i=1}^\nn\binom{Q}{i}$ times \deepPoly in total, which is highly inefficient. Our idea is to verify multiple attack vectors at one time. 



\subsection{Overview of \tool}

The overall verification procedure is given in Algorithm~\ref{alg:overall}. 
Given a QNN $\mN$, an input region $\mI$, a target class $g$, and the maximum number of bits to flip $\nn$, we firstly traverse each parameter $w$, 
performing a reachability analysis via function {\sf BFA\_RA}$(\cdot)$ independently (lines 3-4) to compute a sound output range for $\mN$ considering all potential $\sum_{i=1}^\nn\binom{Q}{i}$ attack vectors with respect to parameter $w$, and subsequently identify all parameters potentially susceptible to bit-flip attacks (line 5). 
If the set $\xi$ is empty, we return \texttt{True} which means all parameters are safe to BFA and the network $\mN$ is BFA-tolerant with respect to the region $\mI$ and class $g$. Otherwise, it implies the existence of at least one parameter for which the reachability analysis fails to confirm safety against such attacks. 
In this case, we reformulate the verification problem into an equivalent MILP problem based on the intermediate results (i.e., all susceptible parameters $\xi$) derived before, which can then be solved using off-the-shelf solvers. Therefore, the whole verification process \tool is sound, complete yet reasonably efficient. We remark that the MILP-based verification method is often more time-consuming and thus the first step allows us to quickly verify many tasks first or identify all vulnerable parameters soundly and formally.

Below, we present the details of functions {\sf BFA\_RA} and {\sf BFA\_MILP}. We first introduce an abstract domain designed for networks with symbolic parameters, which will be utilized throughout our reachability analysis procedure.

\begin{algorithm}[t]
    \SetKwProg{myproc}{Proc}{}{}

    \myproc{{\sf BFA\_Verifier}{$(\mN, \mI,g, \nn)$}}{
        $\xi = \emptyset$\;
    
        \ForEach{parameter $w$ in $\mN$}{
            % Compute the interval $[w_l,w_u]$ for $w$ w.r.t. flipping $\nn$ bits at most\;
            
            \If{{\sf BFA\_RA}$(\mN, \mI,g, w, \nn)$ = \texttt{Unknown}}{
                $\xi$.append($w$)\; 
            }
        }
        \If{$\xi==\emptyset$}{
            \Return{\texttt{True}};
        }
        \Else{
            \Return{{\sf BFA\_MILP$(\mN,\mI,g,\xi, \nn)$}};
        }
    }

    \caption{Overall Algorithm of {\sf BFAVerifier}}\label{alg:overall}

\end{algorithm}

\subsection{\symPoly: An Abstract Domain for Networks with Symbolic Parameters}\label{sec:deepPolyR}

In this section, we introduce a new abstract domain \symPoly designed for networks with symbolic parameters, equipped with abstract transformers tailored to our bit-flip attack setting to conduct a sound reachability analysis. 


Let us consider the $(i+1)$-th layer with neuron function $\bs{x}^{i+1}_j=\text{ReLU}(\bs{W}^{i+1}_{j,:}\bs{x}^{i}+\bs{b}^{i+1}_j)$ in a QNN $\mN$ such that $\bs{W}^{i+1}_{j,k}$ (for some $k\in[n_i]$) or/and $\bs{b}^{i+1}_j$ may be replaced by symbolic parameters.
Following \deepPoly, we first split each neuron (e.g., $\bs{x}^{i+1}_j$) into two nodes (e.g., $\bs{x}^{i+1}_{j,1}$ and $\bs{x}^{i+1}_{j,2}$) and reformulate the neuron function as follows: 
\[
\bs{x}^{i+1}_{j,1}= \sum_{t\in[n_i]} \bs{W}^{i+1}_{j,t}\bs{x}^{i}_{t,2}+\bs{b}^{i+1}_j, \quad \bs{x}^{i+1}_{j,2}= \text{ReLU}(\bs{x}^{i+1}_{j,1})
\]

\subsubsection{Abstract domain.} We inherit the abstract domain $\mA$ introduced in \deepPoly which consists of a set of polyhedral constraints, each relating one variable to a linear combination of the variables from preceding layers. 
% For clarity, we use $\dot{\bs{a}}$ (resp. $\bs{a}$) to denote abstract elements before (resp. after) the concrete parameters are replaced by symbolic ones and use an overrightarrow $\overrightarrow{\bs{W}}$ to differentiate between symbolic and concrete parameters. 
Formally, the abstract element of each neuron $\bs{x}^{i+1}_{j,s}$ ($s\in\{1,2\}$) in our abstract domain is represented as $\bs{a}^{i+1}_{j,s}=\langle a^{i+1,\le}_{j,s}, a^{i+1,\ge}_{j,s}, l^{i+1}_j, u^{i+1}_j\rangle \in \mA$, and it satisfy the following invariant: $\gamma(\bs{a}^{i+1}_{j,s})=\{x\in\mathbb{R}\mid a^{i+1,\le}_{j,s}\le x\le a^{i+1,\ge}_{j,s}\}\subseteq [l^{i+1}_{j,s},u^{i+1}_{j,s}]$. By repeatedly substituting each variable $\bs{x}^{i'}_{j'}$ in $a^{i+1,\le}_{j,s}$ (resp. $a^{i+1,\ge}_{j,s}$) using $a^{i',\le}_{j'}$ or $a^{i',\ge}_{j'}$ according to the coefficient of $\bs{x}^{i'}_{j'}$, until no further substitution is possible, $a^{i+1,\le}_{j,s}$ (resp. $a^{i+1,\ge}_{j,s}$) will be a linear combination over the input variables of the QNN.
% Note that if no concrete parameters are replaced by symbolic ones, our abstract domain is the same as that in \deepPoly.
% 
We next introduce our abstract transformation for functions in $\mN$.

\subsubsection{Affine abstract transformer for symbolic weights.}\label{sec:symWeight}
Without loss of generality, we consider the transformer for the case where there is only one concrete parameter is replaced by a symbolic one, e.g., $\overrightarrow{\bs{W}}^{i+1}_{j,k}$ for some $k\in[n_i]$.
% or $\overrightarrow{\bs{b}}^{i+1}_j$. 
For all nodes other than $\bs{x}^{i+1}_{j,1}$, we directly inherit the abstract transformers from \deepPoly.


In this work, we need to abstract affine functions with symbolic parameters and the ReLU function, both of which contribute to precision loss. To improve accuracy, we abstract them jointly as a symbolic weighted ReLU function with ReLU applied internally, as shown in Figure 3b. For the very first affine layer, we abstract the affine function solely, since there is no preceding ReLU, as given later at the end of this section. We remark that our abstract transformations can be compositionally applied to settings involving multiple symbolic parameters. 

% \smallskip
% \noindent
% {\bf Transformer for symbolic weights.}
\smallskip
\noindent
\minor{\bf Symbolic weights on hidden neurons.}
Consider a symbolic weight parameter $\overrightarrow{\bs{W}}^{i+1}_{j,k}$ constrained by an interval range $[w_l,w_u]$. Then, the updated neuron function for $\bs{x}^{i+1}_{j,1}$ is as follows: 
\begin{equation}\label{eq:symWeight}
    \bs{x}^{i+1}_{j,1}=\sum_{t\in [n_{i}]\backslash k}\bs{W}^{i+1}_{j,t}\bs{x}^i_{t,2}+\overrightarrow{\bs{W}}^{i+1}_{j,k}\bs{x}^i_{k,2}+\bs{b}^{i+1}_j
\end{equation}
% \[\bs{x}^{i+1}_{j,1}=\sum_{t\in [n_{i}]\backslash k}\bs{W}^{i+1}_{j,t}\bs{x}^i_{t,2}+\overrightarrow{\bs{W}}^{i+1}_{j,k}\bs{x}^i_{k,2}+\bs{b}^{i+1}_j\]

% % 
% For example, let the original abstract element of neuron $\bs{x}^{i}_{k,2}$ be $\bs{a}^{i}_{k,2}=\langle a^{i,\le}_{k,2}, a^{i,\ge}_{k,2}, l^i_{k,2}, u^i_{k,2} \rangle$ with $\gamma(\bs{a}^i_{k,2})=\{x\in\mathbb{R} \mid a^{i,\ge}_{k,2}\le  x\le a^{i,\le}_{k,2}\}$, then we have $\dot{\bs{a}}^i_{k,2}=\bs{a}^i_{k,2}$.

To perform abstract transformations on $\overrightarrow{\bs{W}}^{i+1}_{j,k}\bs{x}^i_{k,2}$, an intuitive idea is to directly make an affine transformation with respect to symbolic parameter $\overrightarrow{\bs{W}}^{i+1}_{j,k}$ on the abstract element of $\bs{x}^i_{k,2}$. However, it will lead to an over-approximate result compared to abstracting the symbolic-weighted ReLU function.
% 
To illustrate it, let us consider the setting where $(l^i_{k,2}<0<u^i_{k,2})\wedge(l^i_{k,2}+u^i_{k,2}>0)$ and $w_l\ge 0$. As shown in Figure~\ref{fig:TransAct}, the areas within the yellow boundaries and the green boundaries are captured by the weighted abstract elements $w_u\cdot \bs{a}^{i}_{k,2}$ and $w_l\cdot\bs{a}^{i}_{k,2}$, respectively, with $\gamma(w_u\cdot\bs{a}^{i}_{k,2})=\{ w_u  \cdot x \in\mathbb{R} \mid a^{i,\le}_{k,2}\le  x\le a^{i,\ge}_{k,2} \}$ and $\gamma(w_l\cdot\bs{a}^{i}_{k,2})=\{ w_l \cdot x \in\mathbb{R} \mid a^{i,\le}_{k,2}\le  x\le a^{i,\ge}_{k,2} \}$. 
It is obvious that the area captured by the dotted polyhedra in Figure~\ref{fig:TransAct} is larger than that in Figure~\ref{fig:TransX}, whose area is captured by directly abstracting the weighted ReLU function $\overrightarrow{\bs{W}}^{i+1}_{j,k}\cdot \text{ReLU}(\bs{x}^i_{k,1})$.


\begin{figure*}[t]
	\centering
        \subfigure[Areas obtained by abstracting $\overrightarrow{\bs{W}}^{i+1}_{j,k}\cdot \bs{a}^i_{k,2}$.]{\label{fig:TransAct}
		\begin{minipage}[b]{0.44\textwidth}
			\includegraphics[width=1.0\textwidth]{figs/TransAct.pdf}
		\end{minipage}	
	}\hspace{5mm}
        % 
        \subfigure[Areas obtained by abstracting $\overrightarrow{\bs{W}}^{i+1}_{j,k}\cdot\text{ReLU}(\bs{x}^i_{k,1})$.]{\label{fig:TransX}
		\begin{minipage}[b]{0.44\textwidth}
			\includegraphics[width=1.0\textwidth]{figs/TransX.pdf}
		\end{minipage}	
	}\vspace{-2mm}
        \caption{Convex approximations of $\overrightarrow{\bs{W}}^{i+1}_{j,k}\bs{x}^i_{k,2}=\overrightarrow{\bs{W}}^{i+1}_{j,k}\cdot \text{ReLU}(\bs{x}^i_{k,1})$ via different abstract transformations: (a) depicts the approximation derived by abstracting $\overrightarrow{\bs{W}}^{i+1}_{j,k}\cdot \bs{a}^i_{k,2}$, where $\bs{a}^i_{k,2}$ is an over-approximation of $\text{ReLU}(\bs{x}^i_{k,1})$ obtained from \deepPoly, and the yellow and green lines gives the boundaries of $w_u\cdot \bs{a}^i_{k,2}$ and $w_u\cdot \bs{a}^i_{k,2}$. (b) directly give the approximation by linear boundaries with the minimal area in the input-output plane of the function $\overrightarrow{\bs{W}}^{i+1}_{j,k}\cdot \text{ReLU}(\bs{x}^i_{k,1})$.}
        \Description{Convex approximations for $\overrightarrow{\bs{W}}^{i+1}_{j,k}\bs{x}^i_{k,2}$ via different abstract transforms.}
    \label{fig:abstrDomains}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.95\textwidth]{figs/ModifiedQNN.pdf}
    \caption{The construction of additional node $\tilde{\bs{x}}^{i}_{k,2}$ given a symbolic parameter $\overrightarrow{\bs{W}}^{i+1}_{j,k}$.}
    \Description{The construction of additional node $\tilde{\bs{x}}^{i}_{k,2}$ given a symbolic parameter $\overrightarrow{\bs{W}}^{i+1}_{j,k}$.}
    \label{fig:modQNN}
\end{figure*}

Therefore, our idea is to abstract the symbolic-weighted ReLU function directly. To achieve it, we initially introduce an additional node $\tilde{\bs{x}}^{i}_{k,2}$ for the symbolic parameter $\overrightarrow{\bs{W}}^{i+1}_{j,k}$ such that $\tilde{\bs{x}}^i_{k,2}=\text{ReLU}_w(\bs{x}^{i}_{k,1}) = \overrightarrow{\bs{W}}^{i+1}_{j,k} \cdot \text{ReLU}(\bs{x}^{i}_{k,1})$. After that, we set the weight as 1 between $\tilde{\bs{x}}^i_{k,2}$ and $\bs{x}^{i+1}_{j,1}$, and set the weight between $\bs{x}^i_{k,2}$ and $\bs{x}^{i+1}_{j,1}$ as 0. An illustration of the construction can be found in Figure~\ref{fig:modQNN}.
Then, given the abstract element of $\bs{x}^i_{k,2}$ as $\bs{a}^{i}_{k,2}=\langle a^{i,\le}_{k,2}, a^{i,\ge}_{k,2}, l^i_{k,2}, u^i_{k,2} \rangle$, we define the abstract element $\bs{a}^{i,\ast}_{k,2}=\langle \tilde{a}^{i,\le}_{k,2},\tilde{a}^{i,\ge}_{k,2}, \tilde{l}^i_{k,2}, \tilde{u}^i_{k,2} \rangle$ of neuron $\tilde{\bs{x}}^i_{k,2}$ as follows:
\begin{itemize}
    \item If $w_l\ge 0$: $\tilde{a}^{i,\le}_{k,2}=w_l\cdot a^{i,\le}_{k,2}$, $\tilde{a}^{i,\ge}_{k,2}=w_u\cdot a^{i,\ge}_{k,2}$, $\tilde{l}^i_{k,2}=w_l\cdot l^i_{k,2}$, and $\tilde{u}^i_{k,2}=w_u\cdot u^i_{k,2}$;
    
    \item If $w_u\le 0$: $\tilde{a}^{i,\le}_{k,2}=w_l\cdot a^{i,\ge}_{k,2}$, $\tilde{a}^{i,\ge}_{k,2}=w_u\cdot a^{i,\le}_{k,2}$, $\tilde{l}^i_{k,2}=w_l\cdot u^i_{k,2}$, and $\tilde{u}^i_{k,2}=w_u\cdot l^i_{k,2}$;
    
    \item If $w_l < 0 < w_u$: $\tilde{a}^{i,\le}_{k,2}=w_l\cdot a^{i,\ge}_{k,2}$, $\tilde{a}^{i,\ge}_{k,2}=w_u\cdot a^{i,\ge}_{k,2}$, $\tilde{l}^i_{k,2}=w_l\cdot u^i_{k,2}$, and $\tilde{u}^i_{k,2}=w_u\cdot u^i_{k,2}$.
\end{itemize}

An illustration of the above abstract transformer for the weighted-ReLU function can be found in Figure~\ref{fig:allCases}.
% , i.e., $\text{ReLU}_w(\cdot)$, can be found in Figure~\ref{fig:allCases}, where the first column of figures represents the setting where $l^i_{k,1}\ge 0$ with respect to the abstract element $\bs{a}^i_{k,1}$, and the second (resp. third) column indicates the setting when $(l^i_{k,1} <0<u^i_{k,1}) \wedge (l^i_{k,1}+u^i_{k,1}\le 0)$ (resp. $(l^i_{k,1} < 0 < u^i_{k,1}) \wedge (l^i_{k,1}+u^i_{k,1}>0)$).

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.85\textwidth]{figs/allCases.pdf}
    \caption{An illustration of the abstract transformer for the symbolic-weighted ReLU function $\overrightarrow{\bs{W}}^{i+1}_{j,k}\cdot \text{ReLU}(\bs{x}^i_{k,1})$, where the first column defines the range (interval) of values $[w_l, w_u]$ for $\overrightarrow{\bs{W}}^{i+1}_{j,k}$. The second column gives the abstraction when $l^i_{k,1}>0$ for the abstract element $\bs{a}^i_{k,1}$ of $\bs{x}^i_{k,1}$, and the third (resp. fourth) column shows the abstraction of when $(l^i_{k,1} <0<u^i_{k,1}) \wedge (l^i_{k,1}+u^i_{k,1}\le 0)$ (resp. $(l^i_{k,1} < 0 < u^i_{k,1}) \wedge (l^i_{k,1}+u^i_{k,1}>0)$).}
    \Description{An illustration of the abstract transformer for symbolic-weighted ReLU functions.}

    \label{fig:allCases}
\end{figure*}

\begin{theorem}\label{the:deepPolyR_weight}
    The weighted ReLU abstract transformer for neuron $\tilde{\bs{x}}^i_{k,2}$ in Figure~\ref{fig:modQNN} i) is sound
    % , i.e., $\overrightarrow{\bs{W}}^{i+1}_{j,k}\cdot\text{ReLU}(\gamma(\bs{a}^i_{k,1}))\subseteq \gamma (\bs{a}^{i,*}_{k,2})$, 
    % and ii)  
    and preserves the invariant $\gamma (\bs{a}^{i,*}_{k,2}) \subseteq [\tilde{l}^i_{k,2}, \tilde{u}^i_{k,2}]$. \qed
\end{theorem}


Finally, we can apply the affine abstract transformers introduced in \deepPoly (cf. Section~\ref{sec:naiveMethod}) to the remaining affine transformations within $\bs{x}^{i+1}_{j,1}$ in equation~(\ref{eq:symWeight}), i.e., $\sum_{t\in[n_i]\backslash k} \bs{W}^{i+1}_{j,t}\bs{x}^i_{t,2}+\bs{b}^{i+1}_j$. Hence, both soundness and the domain invariant are preserved in our affine abstract transformers on equation~(\ref{eq:symWeight}) considering symbolic weight parameters.

\smallskip
\noindent
\major{\bf Symbolic weights on input neurons.}
\major{Consider $\bs{x}^2_{j,1}=\sum_{t\in [n_{2}]\backslash k}\bs{W}^{2}_{j,t}\bs{x}^1_{t}+\overrightarrow{\bs{W}}^{2}_{j,k}\bs{x}^1_k+\bs{b}^{2}_j$ with a symbolic weight $\overrightarrow{\bs{W}}^{2}_{j,k}$ connected to the input neuron $\bs{x}^1_k$. Let $[x_l,x_u]$ be the input range of the neuron $\bs{x}^1_k$, then the abstract domain for $\bs{x}^2_{j,1}$ is
$\bs{a}^{2,\ast}_{j,1} = \langle \tilde{a}^{2,\le}_{j,1}, \tilde{a}^{2,\ge}_{j,1}, \tilde{l}^{2}_{j,1}, \tilde{u}^{2}_{j,1}\rangle$ for $\bs{x}^{2}_{j,1}=\sum_{t\in [n_{2}]\backslash k}\bs{W}^{2}_{j,t}\bs{x}^1_{t}+\overrightarrow{\bs{W}}^{2}_{j,k}\bs{x}^1_k+\bs{b}^{2}_j$ with $\tilde{a}^{2,\le}_{j,1}$ and $\tilde{a}^{2,\ge}_{j,1}$ set as follows:
\[
    \tilde{a}^{2,\le}_{j,1}= \sum_{t\in [n_{2}]\backslash k}\bs{W}^{2}_{j,t}\bs{x}^1_{t} + \bs{b}^2_j + \kappa^\le \bs{x}^1_k-\eta, \quad \tilde{a}^{2,\ge}_{j,1}= \sum_{t\in [n_{2}]\backslash k}\bs{W}^{2}_{j,t}\bs{x}^1_{t} + \bs{b}^2_j + \kappa^\ge \bs{x}^1_k+\eta
\]where if $0\le x_l$, then $\{\kappa^\le = w_l$, $ \kappa^\ge = w_u$, $\eta=0\}$; if $x_u\le 0$, then $\{\kappa^\le =w_u$, $\kappa^\ge = w_l$, $\eta =0\}$; Otherwise, $\{\kappa^\le=\frac{w_l x_u -w_u x_l}{x_u-x_l}$, $\kappa^\ge=\frac{w_u x_u -w_l x_l}{x_u-x_l}$, $\eta=\frac{x_u x_l}{x_u - x_l}(w_l-w_u)\}$.}

\major{$\tilde{l}^{2}_{j,1}$ and $\tilde{u}^{2}_{j,1}$ can be determined with corresponding lower/upper bounds computation methods (cf. Section~\ref{sec:naiveMethod}). Intuitively, $\kappa^\le \bs{x}^i_k -\eta$ (resp. $\kappa^\ge \bs{x}^i_k +\eta$) expresses the lower (resp. upper) boundary of the abstract domain of the weighted input neuron $\overrightarrow{\bs{W}}^{2}_{j,k}\bs{x}^1_{k}$.}



\begin{theorem}\label{the:input}
    The abstract transformer for symbolic weighted input neuron $\bs{x}^2_{j,2}$ is sound and preserves the invariant $\gamma(\bs{a}^{2,\ast}_{j,1})\subseteq [\tilde{l}^2_{j,1}, \tilde{u}^2_{j,1}]$. \hfill\qed
\end{theorem}

\subsubsection{Affine abstract transformer for symbolic biases.}
Similar to the affine transformer for symbolic weights, for all nodes other than $\bs{x}^{i+1}_{j,1}$, we adopt the abstract transformers from \deepPoly. Our abstract transformations can be compositionally applied to settings involving multiple symbolic parameters.
% \smallskip
% \noindent
% {\bf Symbolic biases}
% \paragraph{Abstract transformer for symbolic biases.}

Consider a symbolic bias parameter $\overrightarrow{\bs{b}}^{i+1}_j$ constrained by an interval range $[w_l,w_u]$. Then, the updated neuron function is $\bs{x}^{i+1}_{j,1}=\sum_{t\in[n_i]}\bs{W}^{i+1}_{j,t}\bs{x}^{i}_{t,2}+\overrightarrow{\bs{b}}^{i+1}_j$. Then, we define the abstract element 
$\bs{a}^{i+1}_{j,1}=\langle a^{i+1,\le}_{j,1}, a^{i+1,\ge}_{j,1},  l^{i+1}_{j,1}, u^{i+1}_{j,1}\rangle$ of neuron $\bs{x}^{i+1}_{j,1}$ as follows:
\[
      a^{i+1,\le}_{j,1}= \sum_{t\in[n_i]}\bs{W}^{i+1}_{j,t}\bs{x}^{i}_{t,2} + w_l, \quad  a^{i+1,\ge}_{j,1}= \sum_{t\in[n_i]}\bs{W}^{i+1}_{j,t}\bs{x}^{i}_{t,2} + w_u 
\] 
where $l^{i+1}_{j,1}$ and $u^{i+1}_{j,1}$ can be determined with corresponding lower/upper bounds computation methods (cf. Section~\ref{sec:naiveMethod}). 

% Intuitively, the soundness and the domain invariant are maintained.

\begin{theorem}\label{the:deepPolyR_bias}
    The affine abstract transformer for symbolic biases preserves both soundness and the invariant. \qed
\end{theorem}

% \begin{proof}
%     Since the transformer loses no precision, soundness and the invariant are preserved.
% \end{proof}

% \smallskip
\noindent
{\bf Other abstract transformers.}
In this work, for other network functions, such as the ReLU function and the maxpool operator, we directly adopt the corresponding abstract transformers from \deepPoly. Hence, \symPoly is sound.

\subsection{ Details of Function {\sf BFA\_RA}}\label{sec:bfa_ra}
In this section, we give the implementation details of our reachability analysis procedure {\sf BFA\_RA}. 
For any parameter $w$, we can always determine its concrete interval $[w_l,w_u]$ where any value obtained by flipping up to $\nn$ bits will invariably be contained. Based on this interval, given a QNN $\mN$, an input region $\mI$, and a target class $g$, we can use \symPoly to perform reachability analysis on a modified network $\mN^\ast$, where the concrete parameter $w$ is substituted with a symbolic parameter $\overrightarrow{w}$ constrained by the interval range $[w_l,w_u]$. Then, the analysis will yield two results: i) \emph{Proved}, indicating that the adversary cannot compromise $\mN$ (e.g., all inputs from the input region $\mI$ will be classified as the same class $g$) by altering at most $\nn$ bits in the two's complement representation of the parameter $w$; or ii) \emph{Unknown}, meaning that we cannot confirm the security of $w$ against such bit-flip attacks and there may exist some inputs within the region $\mI$ that will be misclassified by the attacked network into classes other than $g$.


We remark that not all bits of a parameter are equally important concerning BFAs. 
For example, given a quantized parameter $[v_Q;v_{Q-1};\cdots;v_1]$, flipping the signal bit $v_Q$ would always produce the most deviation than flipping any other bit $v_i$ (for $i\in[Q-1]$) since the former always results in the largest parameter interval by adding or subtracting $2^{Q-1}$ onto the original parameter value. Hence, if we consider all the bits at one time, we need to verify the neural network with a large interval for the parameter, and likely return \emph{Unknown}. 
Moreover, according to the abstract transformers defined for symbolic weight parameters in Section~\ref{sec:symWeight}, when the weight interval $[w_l,w_u]$ satisfies $w_l<0<w_u$, the abstract transformer for the $\text{ReLU}_w$ function would lead to a looser over-approximation compared to the other two settings ($w_l\ge 0$ or $w_u\le 0$).  
Hence, given a symbolic parameter with a constrained interval range $[w_l,w_u]$, to enhance the precision of our reachability analysis result, we first partition the parameter interval into two sub-intervals characterized by uniformly signed parameter values, either entirely positive or entirely negative. Then, we perform reachability analysis using \symPoly separately for each sub-interval. Note that, this division addresses the significant over-approximation precision loss that occurs when a symbolic weight parameter has a lower bound and upper bound with differing signs. 
An example illustrating how such an interval partition enhances the abstraction precision is given in Appendix~\ref{sec:app_ip_eff}.
Moreover, for parameter intervals that are too wide to be proved by \symPoly, we introduce a binary search method, which splits the parameter interval at its midpoint and independently verifies each resulting smaller interval iteratively.

\begin{algorithm}[t]
    % \SetAlgoNoLine
    \footnotesize
    \SetKwProg{myproc}{Proc}{}{}

    \myproc{{\sf BFA\_RA}{$(\mN, \mI, g, w, \nn)$}}{
        Compute two minimal sub-intervals $[w^+_l,w^+_u]$, $[w^-_l,w^-_u]$ for $w$ such that i) $w^+_l\ge 0 \wedge w^-_u\le 0$, and ii) $[w^+_l,w^+_u]\cup [w^-_l,w^-_u]$ consists of all possible values of $w$ that can be derived by flipped at most $\nn$ bits\;
        
        \If{{\sf binary\_RA}$(\mN,\mI, g, w, w^+_l,w^+_u)=\texttt{Unknown}$}   {
            \Return{ \texttt{Unknown} }\;

            % \Return{ {\sf binaryPolyR}$(\mN, \mI^r_\bs{u}, w^+_l,w^+_u)$}\;
        }
        \If{{\sf binary\_RA}$(\mN,\mI, g, w, w^-_l,w^-_u)=\texttt{Unknown}$}   {
            \Return{ \texttt{Unknown} }\;

            % \Return{ {\sf binaryPolyR}$(\mN, \mI^r_\bs{u}, w^-_l,w^-_u)$}\;
        }
        \Return{\texttt{Proved}}\;
    }

    \vspace{2mm}
    
    \myproc{{\sf binary\_RA}{$(\mN, \mI, g, w, w_l,w_u)$}}{

        \If{$\symPoly(\mN,\mI, g, w, w_l,w_u)=\texttt{Proved}$}{
            \Return{ \texttt{Proved}}\;
        }
        \If{$w_l==w_u$}{
            \Return{$\symPoly(\mN,\mI, g, w, w_l,w_u)$}\;
        }
        
        Split $[w_l,w_u]$ at the midpoint and get two minimal sub-intervals $[w_l, w_u']$, $[w_l',w_u]$ such that $[w_l, w_u'] \cup [w_l',w_u]$ encompasses all potential flipped values of $w$ as that in $[w_l,w_u]$\; 
        
        \If{{\sf binary\_RA}{$(\mN, \mI, g, w, w_l,w_u')=\texttt{Unknown}$ } }   {
            \Return{ \texttt{Unknown} }\;

            % \Return{ {\sf binary\_PolyR}$(\mN,\mI^r_\bs{u}, w_l,w_u')$}\;

        }\ElseIf{ {\sf binary\_RA}{$(\mN, \mI, g, w, w_l',w_u)=\texttt{ Unknown}$ } }{
            
            \Return{ \texttt{Unknown}}\;
            
        }\Else{
            \Return{\texttt{Proved}}\;
        }
        
    }
    
    \caption{{\sf BFA\_RA} function}\label{alg:verifyPolyR}

\end{algorithm}

The details of function {\sf BFA\_RA} can be found in Algorithm~\ref{alg:verifyPolyR}, where $\symPoly(\mN,\mI,g,w, w_l,w_u)$ represents that we conduct reachability analysis via \symPoly on the network $\mN$ equipped with a symbolic parameter $w$ constrained by the interval $[w_l,w_u]$ with respect to the input region $\mI$ and output class $g$.
% with each sub-interval characterized by uniformly signed parameter values, i.e., either positive or negative. Subsequently, we perform reachability analysis using \deepPolyR separately for each sub-interval. The details of function {\sf Verify\_DeepPolyR} are given in Algorithm~\ref{alg:verifyDPR}. 
% 
The algorithm works as follows. 
Given a neural network $\mN$, an input region $\mI$, a target class $g$, an attacked parameter $w$, and the bit flipping maximum number $\nn$, we first compute two intervals $[w^+_l,w^+_u]$ and $[w^-_l,w^-_u]$ for $w$ such that i) $w^+_l\ge 0$ and $w^-_u\le 0$, and ii) these intervals are designed to encompass any values obtained by flipping up to $\nn$ bits in the two's complement representation of $w$, which can be done as follows: 

\begin{itemize}
    \item If $w\ge 0$: $w^+_l$ (resp. $w^+_u$) is obtained by flipping the most significant $\nn$ bits via $1\rightarrow 0$ (resp. $0\rightarrow 1$), and $w^-_l$ (resp. $w^-_u$) is obtained by flipping the signal bit and the most significant $\nn-1$ bits via $1\rightarrow 0$ (resp. $0\rightarrow 1$);
    \item If $w<0$: $w^+_l$ (resp. $w^+_u$) is obtained by flipping the signal bits via $1\rightarrow 0$ and the most significant $\nn-1$ bits via $1\rightarrow 0$ (resp. $0\rightarrow 1$), and $w^-_l$ (resp. $w^-_u$) is obtained by the most significant $\nn$ bits via $1\rightarrow 0$ (resp. $0\rightarrow 1$).
\end{itemize}

For parameter intervals that are too wide to be verified by \symPoly (cf. line 13), we introduce a binary search method, which splits the parameter interval at its midpoint and independently verifies each resultant smaller interval iteratively. Smaller intervals are generally more likely to yield \texttt{Proved} results, thus enhancing the overall effectiveness and precision of our reachability analysis, which has been confirmed by our experiments (cf. Section~\ref{sec:RQ1}).


\subsection{ Details of Function {\sf BFA\_MILP}}\label{sec:milp}
If {\sf BFA\_RA} fails to prove the BFA-tolerant robustness property, we then encode the verification problem as an equivalent MILP problem w.r.t the set of unproved parameters as follows. 

\vspace{2mm}
\noindent
{\bf Encoding of input regions.}
We consider input regions that are expressible by polyhedra in this work, and they can be directly encoded by linear constraints. For example, for an input region defined by $L_\infty$-norm $\mI^r_u=\{\bs{x}\in\mathbb{R}^n \mid || \bs{x}-\bs{u}||_\infty \le r\}$, we can use the following constraint set $\Theta^\mI$ to encode the input condition $\phi(\bs{x}):=\bs{x}\in\mI^r_\bs{u}$:
\[
\Theta^\mI=\{ \text{max}(\bs{u}_i-r,0)\le \bs{x}_i \le \text{min}(\bs{u}_i+r,1) \mid i\in [n]\}
\]

For input regions defined by the cartesian product of intervals $\mI^{\bs{l}\times\bs{u}}=\{\bs{x}\in \mathbb{R}^n\mid \bs{x}_i \in [\bs{l}_i, \bs{u}_j]\}$, we can use the following constraint set to encode the input condition $\phi(\bs{x}):=\bs{x}\in\mI^{\bs{l}\times \bs{u}}$:
\[
\Theta^\mI=\{ \bs{l}_i \le \bs{x}_i \le \bs{u}_i \mid i\in [n]\}
\]

\vspace{2mm}
\noindent
{\bf Encoding of output properties.}
Let $\bs{y}$ denote the output vector of $\mN^\rho$ given any attack vector $\rho$. We encode the output condition, i.e., $\psi(\bs{y}):=\text{argmax}(\bs{y}) \neq \text{argmax}(\mN(\bs{u}))=g\in[s]$ into the following set of constraints based on a set of Boolean variables $\{\eta_{i}\mid i\in[s]\backslash g\}$:
\begin{itemize}
    \item If $i<g$: then $\bs{y}_i\ge \bs{y}_g \Leftrightarrow \eta_i=1$ which can be encoded as 
    $\Theta_{i,0}^g=\{\bs{y}_g + \textbf{M}\cdot (\eta_i-1) \le \bs{y}_i, \bs{y}_i < \bs{y}_g + \textbf{M}\cdot \eta_i\}$;
    \item If $i> g$: then $\bs{y}_i> \bs{y}_g \Leftrightarrow \eta_i=1$ which can be encoded as 
    $\Theta_{i,1}^g=\{\bs{y}_g + \textbf{M}\cdot (\eta_i-1) < \bs{y}_i, \bs{y}_i < \bs{y}_g + \textbf{M}\cdot \eta_i\}$.
\end{itemize}
Intuitively, $\Theta_{i,0}^g$ (resp. $\Theta_{i,1}^g$) ensures that the $i$-th entry of the output vector $\bs{y}$ for $i<g$ (resp. $i>g$) is no less than (resp. larger than) the $g$-th entry iff $\eta_i=1$. As a result, $\text{argmax}(\bs{y})\neq g$ iff the set of constraints $\Theta_g=\bigcup_{i<g}\Theta^g_{i,0} \cup \bigcup_{i>g}\Theta^g_{i,1}\cup\{\sum_{i\in[s]\backslash g} \eta \ge 1\}$ holds.

\vspace{2mm}
\noindent
{\bf Encoding of neural networks under BFAs.} 
Next, we present the MILP encoding of the neural network under BFAs based on the intermediate analysis results from function {\sf BFA\_RA}. 

Let $\xi=\{w_1,w_2,\cdots,w_m\}$ denote the vulnerable parameters set obtained from our reachability analysis (cf. Algorithm~\ref{alg:overall}). Each parameter is quantized by $Q$ bits. The adversary can only attack one parameter from the parameter set $\xi$ and flip $\nn$ bit at most on it. We first traverse all possible $(1,\nn)$-attack vectors for each parameter $w_i \in \xi$ ($1\le i\le|\xi|$), and get the flipped value set $F_{w_i}=\{w_i^1, w_i^2, \cdot, w_i^{\mathfrak{K}}\}$ for each $w_i$, where $\mathfrak{K}= \sum_{i=1}^\nn \binom{Q}{i}$. Then, we use the following constraint set $\Theta_\xi^{\nn}$ to encode the parameters in $\xi$, where $\delta^j_i$ for $i\in|\xi|$ and $j\in\mathfrak{K}$ are binary variables:
\begin{center}
    $\Theta_{\xi}^\nn= \left\{
        \begin{array}{c}
        
             \tilde{w}_i = w_i + (w_i^1 - w_i) \delta_i^1 + (w_i^2 -w_i) \delta_i^2 + \cdots + (w_i^\mathfrak{K}-w_i) \delta_i^\mathfrak{K}, \\
                    
            i\in |\xi|,  \qquad \sum_{i=1}^{|\xi|} \sum_{j=1}^{\mathfrak{K}} \delta_i^j = 1 
        
        \end{array}\right\}$

\end{center}

Intuitively, for each parameter $w_i$, the binary variable $\delta^j_i=1$ indicates that the adversary attacks the parameter $w_i$ and alters it into a new value $w_i^j$. If $\sum_{j=1}^\mathfrak{K} \delta_i^j=0$, then it means that the adversary does not attack the parameter $w_i$. The constraint $\sum_{i=1}^{|\xi|} \sum_{j=1}^{\mathfrak{K}} \delta_i^j = 1 $ ensures that only one parameter is altered, while no more than $\nn$ bits are subject to modification.

Then, we follow the existing MILP encoding method~\cite{LomuscioM17} to encode $\mN^\rho$ into a set of mixed-integer linear constraints $\Theta_{\mN^\rho}$, where for each vulnerable parameter $w_i\in\xi$, we use $\tilde{w}_i$ in $\Theta_{\xi}^\nn$ to replace $w_i$ in the encoding of the affine function. 
% 
Finally, the BFA-tolerant robustness verification problem is equivalent to the solving of the constraint set: $\Theta_P= \Theta_{\mN^\rho} \cup \Theta_{\xi}^\nn \cup \Theta^\mI \cup \Theta_{g}$.

\begin{theorem}\label{the:complete}
    $\mN\models^\rho_{\mm,\nn}$ holds iff $\Theta_P$ is unsatisfiable.  \hfill \qed
\end{theorem}

% \begin{proof}
%     \red{TBD.}
% \end{proof}

\minor{Overall, the complexity of {\sf BFA\_RA} is polynomial in the network size when $\mm=1$, whereas {\sf BFA\_MILP} remains NP-complete even when $\mm=1$.}

\subsection{\major{Extension to Other Networks}}
\major{This work primarily focuses on feedforward neural networks with ReLU activations. In this section, we demonstrate the extensibility of our framework to other networks, including those with sigmoid or tanh activations and architectures incorporating convolutional layers.}


\subsubsection{\major{Other activation functions}} \major{Following the idea of symbolic weights on hidden neurons in Section~\ref{sec:symWeight} and the abstract transformers proposed in \deepPoly for sigmoid and tanh, for an activation function $g(x)$ that is continuous and twice-differentiable such that 
 the first derivative $g^\prime(x)>0$ and the second derivative $g^{\prime\prime}\ge 0 \Leftrightarrow x\le 0$, we also construct an additional node $\tilde{\bs{x}}^i_{k,2}$ (the same as in Figure~\ref{fig:modQNN}) and study its abstract domain according to 
$\tilde{\bs{x}}^i_{k,2}=\overrightarrow{\bs{W}}^{i+1}_{j,k} \cdot g(\bs{x}^i_{k,1})$.
The corresponding abstract transformers for Sigmoid and Tanh considering the node $\tilde{\bs{x}}^i_{k,2}$ are given in Table~\ref{tab:log_AbT}. For the other network functions with constant parameters, we can reuse the corresponding abstract transformers from \deepPoly directly.}


\begin{table}[t]
    \centering
    \caption{\major{The abstract domain $\bs{a}^{i,\ast}_{k,2}=\langle  \tilde{a}^{i,\le}_{k,2}, \tilde{a}^{i,\ge}_{k,2}, \tilde{l}^i_{k,2}, \tilde{u}^i_{k,2} \rangle$ of $\tilde{\bs{x}}^i_{k,2}=\overrightarrow{\bs{W}}^{i+1}_{j,k}\cdot g(\bs{x}^i_{k,1})$, where $l^i_{k,1}$ and $u^i_{k,1}$ are the lower and upper bounds of $\bs{x}^i_{k,1}$, $\kappa = \frac{g(u^i_{k,1})-g(l^i_{k,1})}{u^i_{k,1}-l^i_{k,1}}$, and $\kappa^\prime = \text{min}(g^\prime (l^i_{k,1}), g^\prime(u^i_{k,1}))$}}
    \setlength{\tabcolsep}{2pt}
    \scalebox{0.8}{
    \begin{tabular}{c|c|c|c}
        \toprule
         $g(x)$ & Bounds of $\bs{x}^i_{k,1}$ & $w_l\ge 0$ & $w_u\le 0$  \\ \midrule

         
        & $l^i_{k,1} \ge 0$ &  \tabincell{l}{$\tilde{a}^{i,\le}_{k,2}=w_l g(l^i_{k,1})+w_l\kappa (\bs{x}^i_{k,1}-l^i_{k,1})$ \\ $\tilde{a}^{i,\ge}_{k,2}= w_u g(u^i_{k,1})+w_u\kappa^\prime (\bs{x}^i_{k,1}-u^i_{k,1})$ \\ $\tilde{l}^{i}_{k,2}= w_l g(l^i_{k,1})$, $\tilde{u}^i_{k,2}= w_u g(u^i_{k,1})$} & 
        \tabincell{l}{$\tilde{a}^{i,\le}_{k,2}= w_l g(u^i_{k,1})+w_l\kappa^\prime (\bs{x}^i_{k,1}-u^i_{k,1})$ \\ $\tilde{a}^{i,\ge}_{k,2}=w_u g(l^i_{k,1})+w_u\kappa (\bs{x}^i_{k,1}-l^i_{k,1})$ \\ $\tilde{l}^{i}_{k,2}= w_l g(u^i_{k,1})$, $\tilde{u}^i_{k,2}= w_u g(l^i_{k,1})$} \\  \cline{2-4}

        $\text{Sigmoid}(x)$ & $u^i_{k,1} \le 0$ & \tabincell{l}{$\tilde{a}^{i,\le}_{k,2}= w_l g(l^i_{k,1})+w_l\kappa^\prime (\bs{x}^i_{k,1}-l^i_{k,1})$ \\ $\tilde{a}^{i,\ge}_{k,2}=w_u g(u^i_{k,1})+w_u\kappa (\bs{x}^i_{k,1}-u^i_{k,1})$ \\ $\tilde{l}^{i}_{k,2}= w_l g(l^i_{k,1})$, $\tilde{u}^i_{k,2}= w_u g(u^i_{k,1})$} & \tabincell{l}{$\tilde{a}^{i,\le}_{k,2}= w_l g(u^i_{k,1})+w_l\kappa (\bs{x}^i_{k,1}-u^i_{k,1})$ \\ $\tilde{a}^{i,\ge}_{k,2}=w_u g(l^i_{k,1})+w_u\kappa^\prime (\bs{x}^i_{k,1}-l^i_{k,1})$ \\ $\tilde{l}^{i}_{k,2}= w_l g(u^i_{k,1})$, $\tilde{u}^i_{k,2}= w_u g(l^i_{k,1})$} \\ \cline{2-4}

        & $l^i_{k,1} < 0 < u^i_{k,1}$ &  \tabincell{l}{$\tilde{a}^{i,\le}_{k,2}= w_l g(l^i_{k,1})+w_l\kappa^\prime (\bs{x}^i_{k,1}-l^i_{k,1})$ \\ $\tilde{a}^{i,\ge}_{k,2}=w_u g(u^i_{k,1})+w_u\kappa^\prime (\bs{x}^i_{k,1}-u^i_{k,1})$\\ $\tilde{l}^{i}_{k,2}= w_l g(l^i_{k,1})$, $\tilde{u}^i_{k,2}= w_u g(u^i_{k,1})$} & 
         \tabincell{l}{$\tilde{a}^{i,\le}_{k,2}= w_l g(u^i_{k,1})+w_l\kappa^\prime (\bs{x}^i_{k,1}-u^i_{k,1})$ \\ $\tilde{a}^{i,\ge}_{k,2}=w_u g(l^i_{k,1})+w_u\kappa^\prime (\bs{x}^i_{k,1}-l^i_{k,1})$\\ $\tilde{l}^{i}_{k,2}= w_l g(u^i_{k,1})$, $\tilde{u}^i_{k,2}= w_u g(l^i_{k,1})$} \\  \midrule


         & $l^i_{k,1} \ge 0$ &  \tabincell{l}{$\tilde{a}^{i,\le}_{k,2}= w_l g(l^i_{k,1})+w_l\kappa (\bs{x}^i_{k,1}-l^i_{k,1})$ \\ $\tilde{a}^{i,\ge}_{k,2}=w_u g(u^i_{k,1})+w_u\kappa^\prime (\bs{x}^i_{k,1}-u^i_{k,1})$ \\ $\tilde{l}^{i}_{k,2}= w_l g(l^i_{k,1})$, $\tilde{u}^i_{k,2}= w_u g(u^i_{k,1})$} & 
        \tabincell{l}{$\tilde{a}^{i,\le}_{k,2}= w_l g(u^i_{k,1})+w_l\kappa^\prime (\bs{x}^i_{k,1}-u^i_{k,1})$ \\ $\tilde{a}^{i,\ge}_{k,2}=w_u g(l^i_{k,1})+w_u\kappa (\bs{x}^i_{k,1}-l^i_{k,1})$ \\ $\tilde{l}^{i}_{k,2}= w_l g(u^i_{k,1})$, $\tilde{u}^i_{k,2}= w_u g(l^i_{k,1})$} \\  \cline{2-4}

        $\text{Tanh}(x)$ & $u^i_{k,1} \le 0$ & \tabincell{l}{$\tilde{a}^{i,\le}_{k,2}= w_u g(l^i_{k,1})+w_u\kappa^\prime (\bs{x}^i_{k,1}-l^i_{k,1})$ \\ $\tilde{a}^{i,\ge}_{k,2}=w_l g(u^i_{k,1})+w_l\kappa (\bs{x}^i_{k,1}-u^i_{k,1})$ \\ $\tilde{l}^{i}_{k,2}= w_u g(l^i_{k,1})$, $\tilde{u}^i_{k,2}= w_l g(u^i_{k,1})$} & \tabincell{l}{$\tilde{a}^{i,\le}_{k,2}= w_u g(u^i_{k,1})+w_u\kappa (\bs{x}^i_{k,1}-u^i_{k,1})$ \\ $\tilde{a}^{i,\ge}_{k,2}=w_l g(l^i_{k,1})+w_l\kappa^\prime (\bs{x}^i_{k,1}-l^i_{k,1})$ \\ $\tilde{l}^{i}_{k,2}= w_u g(u^i_{k,1})$, $\tilde{u}^i_{k,2}= w_l g(l^i_{k,1})$} \\ \cline{2-4}

        & $l^i_{k,1} < 0 < u^i_{k,1}$ &  \tabincell{l}{$\tilde{a}^{i,\le}_{k,2}= w_u g(l^i_{k,1})+w_l\kappa^\prime (\bs{x}^i_{k,1}-l^i_{k,1})$ \\ $\tilde{a}^{i,\ge}_{k,2}=w_u g(u^i_{k,1})+w_l\kappa^\prime (\bs{x}^i_{k,1}-u^i_{k,1})$\\ $\tilde{l}^{i}_{k,2}= w_u g(l^i_{k,1})$, $\tilde{u}^i_{k,2}= w_u g(u^i_{k,1})$} & 
         \tabincell{l}{$\tilde{a}^{i,\le}_{k,2}= w_l g(u^i_{k,1})+w_u\kappa^\prime (\bs{x}^i_{k,1}-u^i_{k,1})$ \\ $\tilde{a}^{i,\ge}_{k,2}=w_l g(l^i_{k,1})+w_u\kappa^\prime (\bs{x}^i_{k,1}-l^i_{k,1})$\\ $\tilde{l}^{i}_{k,2}= w_l g(u^i_{k,1})$, $\tilde{u}^i_{k,2}= w_l g(l^i_{k,1})$} \\
         
        \bottomrule
    \end{tabular}}
    \label{tab:log_AbT}
\end{table}



\begin{theorem}\label{the:sigmoid}
    \major{Both the weighted Sigmoid and the weighted Tanh abstract transformers are sound and preserve the invariant $\gamma(\bs{a}^{i,\ast}_{k,2})\subseteq[\tilde{l}^i_{k,2}, \tilde{u}^i_{k,2}]$. \hfill \qed}
\end{theorem}

\major{For the MILP encoding of other activation functions, the piecewise linear approximation can be employed to encode the sigmoid and tanh functions using linear constraints. We argue that such an approximation-based MILP encoding approach is sound, however, not incomplete,
Therefore, for these logistic activation functions, we can only claim that
our approach is sound but incomplete, and limit our focus to the {\sf BFA\_RA} component.}


%The corresponding MILP problem under various activation functions, the primary distinction lies in how the activation function is encoded, as compared to the implementation of {\sf BFA\_MILP} in Section~\ref{sec:milp}.
%While piecewise linear approximation can be employed to encode the sigmoid or tanh functions using linear constraints, we argue that such an approximation-based MILP encoding approach is inconsistent with the notion of a ``mathematically rigorous'' guarantee in formal verification, especially given that we require \tool to return sound and/or complete verification results. Therefore, for these logistic activation functions, we limit our focus to the {\sf BFA\_RA} component, recognizing it as a sound but incomplete verifier.
% \red{While it is not possible to precisely encode the sigmoid or tanh functions using linear constraints, certain MILP solvers, such as Gurobi, offer built-in support for logistic functions through piecewise linear approximation. The specifics of this approximation can be managed by adjusting inherent attributes provided within the Gurobi framework.}

\subsubsection{\major{Other network architectures}}
\major{This work focuses on feed-forward network architectures, however, our approach can be generalized to shared-parameter architectures—such as convolutional networks—without additional technical challenges. Figure~\ref{fig:conv2aff} illustrates how a convolutional layer subjected to bit-flip attacks on the parameter $w$ can be transformed into an equivalent affine layer, to which \tool can be directly applied. Note that, in Figure~\ref{fig:conv2aff_BFA}, although multiple copies are under bit-flip attack, they share the same parameter $w$ in the original convolutional layer, and consequently, the attack effect is identical. Therefore, no additional combinatorial explosion occurs and the computational complexity remains equivalent to the case when $\mm=1$.} 

\major{Although no additional technical challenges occurs, \tool may suffer from significant abstraction precision loss on CNNs, compared to DNNs, due to multiple abstractions in weighted activation function and input neurons even when $\mm=1$ (in contrast to only single abstraction in feedforward networks), both contributing to higher loss than in cases without symbolic weighting.}

\begin{figure*}[t]
	\centering
        \subfigure[A convolutional layer example under BFAs.]{\label{fig:conv_BFA}
		\begin{minipage}[b]{0.44\textwidth}
			\includegraphics[width=1.0\textwidth]{figs/conv.pdf}
		\end{minipage}	
	}\hspace{6mm}
        % 
        \subfigure[The transformed affine layer under BFAs.]{\label{fig:conv2aff_BFA}
		\begin{minipage}[b]{0.44\textwidth}
			\includegraphics[width=1.0\textwidth]{figs/conv2aff.pdf}
		\end{minipage}	
	}\vspace{-3mm}
        \caption{\major{An example of affine transformation from a convolutional computation under bit-flip attacks. Given a $3\times 3$ feature layer $\bs{x}^i$ and a $2\times 2$ convolution filter with one parameter attacked (denoted as $w$) and the other three parameters to be 1 in Figure~\ref{fig:conv_BFA}, we can always get an equivalent affine layer as shown in Figure~\ref{fig:conv2aff_BFA}.}}\vspace{-2mm}
        \Description{The affine transformation from a convolutional computation under bit-flip attacks.}
    \label{fig:conv2aff}
\end{figure*}

\subsubsection{\major{Other quantization schemes and network precisions}}
\major{\tool can be adapted to support other quantization schemes. For instance, when addressing a mixed-precision quantization scheme, only the weight interval associated with each symbolic parameter under bit-flip attacks needs to be adjusted for the BFA\_RA procedure, while no modifications are required for BFA\_MILP. }

\major{For floating-point neural networks (FPNNs), parameters are typically stored as IEEE 754 32-bit single-precision floating-point numbers, where flipping the exponent bit can cause drastic value changes (e.g., altering 0.078125 to $1.25\times 2^{124}$). \tool can be adapted to FPNNs by adjusting the parameter interval derived from bit-flipping, but the performance remains uncertain and is left for future work. 
% 
Indeed, studies~\cite{1bitallyouneed,liyes} have shown that FPNNs are highly vulnerable to BFAs. Given this inherent vulnerability, we argue that verifying BFAs on QNNs is more reasonable, as FPNNs can be almost always compromised. Instead of formal verification, demonstrating vulnerabilities through attacks or testing is a more practical and insightful approach for FPNNs.}

\smallskip
\noindent
{\bf Clarification.}  Our method, \tool, is primarily designed to: i) effectively and efficiently prove desirable property of a given neural network under BFA and ii) or identify a relatively tight superset of vulnerable parameters that must be protected to avoid BFA. Note that the latter allows existing model integrity protection methods to be applied in a more cost-effectively way, i.e., by protecting only the identified vulnerable parameters (e.g., 0.01\% of parameters in a network) rather than all parameters. Additionally, we consider the robustness of input regions against BFA, which is more interesting yet more challenging than the robustness of individual inputs. Note that it is virtually impossible to enumerate and verify all attack vectors for an input region and attackers may use \tool to identify critical bits/parameters to flip as well as an individual input.
