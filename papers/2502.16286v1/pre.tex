%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Preliminary}\label{sec:pre}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We denote by $\mathbb{R}$ (resp. $\mathbb{N}$) the set of real (resp. integer) numbers.
Given a positive integer $n$,  we denote by $[n]$ the set of positive integers $\{1,2, \ldots, n\}$.
We use $x,x',\ldots$ to denote scalars, $\bs{x}, \bs{x}',\ldots$ to denote vectors, and $\bs{W}, \bs{W}',\ldots$ to denote matrices. We denote by $\bs{W}_{i,:}$ and $\bs{W}_{:,j}$ to denote the $i$-th row and $j$-th column of the matrix $\bs{W}$, and use $\bs{x}_i$ to denote the $i$-th entry of the vector $\bs{x}$.


\subsection{Neural Network and Quantization}\label{sec:quant}
In this section, we provide the minimal necessary background on neural networks and the quantization scheme considered in this work. Specifically, we focus on feedforward deep neural networks (DNNs) used for classification problems.

\smallskip
\noindent
{\bf Neural networks.}
% 
% \paragraph{Neural Networks.} 
A DNN consists of an input layer, multiple hidden layers, and an output layer. Each layer contains neurons connected via weighted edges to the neurons in the subsequent layer. Specifically, each neuron in a non-input layer is additionally linked with a bias term. Given an input, a DNN computes an output by propagating it through the network layer by layer and gets the classification result by identifying the dimension with the highest value in the output vector.

A DNN with $d$ layers can be represented by a non-linear multivariate function $\mN: \mathbb{R}^n \rightarrow \mathbb{R}^s$. For any input $\bs{x}\in\mathbb{R}^n$, let $\bs{x}=\bs{x}^1$, the output $\mN(\bs{x})=\bs{W}^d \bs{x}^{d-1}+\bs{b}^d$ can be obtained via the recursive definition $\bs{x}^{i}=\text{ReLU}(\bs{W}^{i} \bs{x}^{i-1}+\bs{b}^{i}) \text{ for } i\in\{2,3,\ldots, d-1\}$, where $\bs{W}^i$ and $\bs{b}^i$ (for $2\le i\le d$) are the weight matrix and bias vector of the $i$-th layer. We refer to $\bs{x}^i_j$ as $j$-th neuron in the $i$-th layer and use $n_i$ to denote the dimension of the $i$-th layer. $n=n_1$ and $s=n_d$.

\smallskip
\noindent
{\bf Quantization.}
% \paragraph{Quantization.}
% 
Quantization is the process of converting high-precision floating-point values into a finite range of lower-precision ones, i.e., fixed-point numbers, without significant accuracy loss. 
A quantized neural network (QNN) is structurally similar to a DNN, except that the parameters and/or activation values are quantized into fixed-pointed numbers, e.g., 
4-bit or 8-bit integers. 
In this work, we adopt the symmetric quantization scheme widely utilized in prior research concerning bit-blip attack (BFA) strategies on QNNs~\cite{1bitallyouneed}, where only parameters are quantized to reduce the memory requirements~\cite{HanMD15,zhou2022incremental,zhang2023post}. During inference, we assume that the parameters are de-quantized and all operations within the quantized networks are executed using floating-point arithmetic.

Given the weight matrix $\bs{W}^i$ and the bias vector $\bs{b}^i$, their signed integer counterparts $\widehat{\bs{W}}^i$ and $\hat{\bs{b}}^i$ with respect to quantization bit-width $Q$ are respectively defined as follows. For each $j,k$,
\[
\widehat{\bs{W}}^i_{j,k} = \lfloor \bs{W}^i_{j,k}/\Delta w^i \rceil, \quad 
        \hat{\bs{b}}^i_{j} = \lfloor \bs{b}^i_{j}/\Delta w^i \rceil
\]
where $\Delta w^i=\text{maxAbs}(\bs{W}^i,\bs{b}^i)/(2^{Q-1}-1)$ is the quantization step size of the $i$-th layer and the \text{max} function returns the maximal value of $\bs{W}^i$ and $\bs{b}^i$. $\lfloor \cdot \rceil$ is the rounding operator, maxAbs$(\bs{W}^i,\bs{b}^i)$ means finding the maximum absolute value among all the entries from $\bs{W}^i$ and $\bs{b}^i$. 



\begin{figure*}[t]
	\centering
	\subfigure[DNN.]{\label{fig:dnnDemo}
		\begin{minipage}[b]{0.43\textwidth}
			\includegraphics[width=1.0\textwidth]{figs/DNNDemo-new.pdf}
		\end{minipage}	
	}\hspace{3mm}
	\subfigure[QNN.]{\label{fig:qnnDemo}
		\begin{minipage}[b]{0.43\textwidth}
			\includegraphics[width=1.0\textwidth]{figs/QNNDemo-new.pdf}
		\end{minipage}	
	}
	\caption{A 3-layer DNN \minor{with ReLU activations} and its quantized version.}
	\Description{A 3-layer DNN and its quantized version.}
    \label{fig:nnDemo}
\end{figure*}

Once quantized into an integer, the parameter will be stored as the two's complement format in the memory. In the forward pass, the parameters will be de-quantized by multiplying the step size $\Delta w^i$. Taking a quantized parameter $\widehat{\bs{W}}^{i}_{j,k}$ as an example and let $\vec{v}(\cdot)$ denote the operation that converts an integer into its two's complement expressions. Assume that $\vec{v}(\widehat{\bs{W}}^i_{j,k})=[v_{Q};v_{Q-1};\cdots;v_1]$, then the de-quantized version $\widetilde{\bs{W}}^i_{j,k}$ can be calculated as follows with $\widetilde{\bs{W}}^i_{j,k}\approx \bs{W}^i_{j,k}$:
\[
\widetilde{\bs{W}}^i_{j,k}=\widehat{\bs{W}}^i_{j,k} \cdot \Delta w^i =\big( -2^{Q-1}\cdot v_{Q}+\sum_{q=1}^{Q-2} 2^{q-1}\cdot v_{q} \big) \cdot \Delta w^i
\]

\begin{example}\label{eg:dnnDemo}
    Consider the DNN shown in Figure~\ref{fig:dnnDemo}. It contains three layers: one input layer, one hidden layer, and one output layer. The weights are associated with the edges and all the biases are 0 and the quantization bit-width $Q=4$. Then, the step size of the parameter quantizer for each non-input layer is $\Delta w^2 = 0.7/(2^3-1) = 0.1$, $\Delta w^3 = 1/(2^3-1) = 1/7$. 
    The integer counterparts of weight parameters are associated with the edges in Figure~\ref{fig:qnnDemo}. 
    
    Take the hidden layer for instance, we obtain their quantized weights, two's complement counterparts, and de-quantized versions as follows: 
    \begin{itemize}
        \item $\widehat{\bs{W}}^2_{1,1}=\lfloor -0.7/\Delta w^2 \rceil=\lfloor -0.7*10 \rceil =-7$, $\vec{v}(\widehat{\bs{W}}^{2}_{1,1})=[1001]$, and $\widetilde{\bs{W}}^2_{1,1}=-0.7$;
        
        \item $\widehat{\bs{W}}^2_{1,2}=\lfloor -0.3/\Delta w^2  \rceil=\lfloor -0.3*10  \rceil=-3$, $\vec{v}(\widehat{\bs{W}}^{2}_{1,2})=[1101]$, and  $\widetilde{\bs{W}}^2_{1,2}=-0.3$;

        \item $\widehat{\bs{W}}^2_{2,1}=\lfloor 0.3/\Delta w^2  \rceil =\lfloor 0.3*10 \rceil =3$, $\vec{v}(\widehat{\bs{W}}^{2}_{2,1})=[0011]$, and $\widetilde{\bs{W}}^2_{2,1}=0.3$;
        
        \item $\widehat{\bs{W}}^2_{2,2}=\lfloor 0.5/\Delta w^2 \rceil=\lfloor 0.7*10 \rceil =7$, $\vec{v}(\widehat{\bs{W}}^{2}_{2,2})=[0111]$, and $\widetilde{\bs{W}}^2_{2,2}=0.7$;
    \end{itemize}
    
    Similarly, for the output layer, we have 
     \begin{itemize}
        \item $\widehat{\bs{W}}^3_{1,1}=\lfloor -1/\Delta w^3 \rceil=\lfloor -1*7 \rceil =-7$, $\vec{v}(\widehat{\bs{W}}^{3}_{1,1})=[1001]$, and $\widetilde{\bs{W}}^3_{1,1}=-1$;
        \item $\widehat{\bs{W}}^3_{1,2}=\lfloor 0/\Delta w^3  \rceil=\lfloor 0*7  \rceil=0$, $\vec{v}(\widehat{\bs{W}}^{3}_{1,2})=[0000]$, and $\widetilde{\bs{W}}^3_{1,2}=0$;
        \item $\widehat{\bs{W}}^3_{2,1}=\lfloor 0.8/\Delta w^3  \rceil=\lfloor 0.8*7 \rceil =6$, $\vec{v}(\widehat{\bs{W}}^{3}_{2,1})=[0110]$, and $\widetilde{\bs{W}}^2_{1,1}=0.8571$;
        \item $\widehat{\bs{W}}^3_{2,2}=\lfloor -0.2/\Delta w^3  \rceil=\lfloor -0.2*7 \rceil =-1$, $\vec{v}(\widehat{\bs{W}}^{3}_{2,2})=[1111]$, and $\widetilde{\bs{W}}^2_{1,1}=-0.1429$.
    \end{itemize}
    
\end{example}


\subsection{Bit-Flip Attacks}
\major{
Bit-flip attacks (BFAs) are a class of fault-injection attacks that were originally proposed to breach cryptographic primitives~\cite{BarenghiBKN12,BihamS97,BonehDL97}. Recently, BFAs have been ported to neural networks.}
%demonstrated that can crush a neural network by maliciously flipping extremely small amounts of bits (often a single bit) within its parameters storage memory systems (i.e. DRAM).}

\smallskip
\noindent
\major{{\bf Attack scenarios and threat model.}
Recent studies~\cite{kim2014flipping,yao2020deephammer,rakin2022deepsteal} have revealed vulnerabilities in DRAM chips, which act as a crucial memory component in hardware systems. 
Specifically, an adversary can induce bit-flips in memory by repeatedly accessing the adjacent memory rows in DRAM, without \emph{direct} access to the victim model's memory, known as Rowhammer attack~\cite{kim2014flipping}. Such attacks exploit an unintended side effect in DRAM, where memory cells interact electrically by leaking charges, potentially altering the contents of nearby memory rows that were not originally targeted in the memory access. 
Although such attacks do not grant adversaries full control over the number or precise location of bit-flips and the most prevalent BFA tools such as DeepHammer~\cite{yao2020deephammer} can typically induce only a single bit-flip,
the recent study~\cite{1bitallyouneed} has demonstrated that an adversary can effectively attack a QNN by flipping, on average, just one critical bit during the deployment stage. While indirectly flipping multiple bits is theoretically feasible, achieving this would require highly sophisticated techniques that are both extremely time-intensive and have a low likelihood of success in practice~\cite{rakin2022deepsteal}.
Therefore, in this study, we assume that the adversary can \emph{indirectly} manipulate only a minimal number of parameters in a QNN, by default 1. More powerful attacks that can \emph{directly} manipulate memory go beyond the scope of this work.
On the other hand, though most of the existing BFAs target weights only~\cite{liyes,1bitallyouneed,HONG_USENIX19,BFAICCV19}, in this work, we consider a more general setting where all parameters (weights and biases) of QNNs are vulnerable to BFAs~\cite{randomDNN}.}




\begin{example}
    Consider the QNN given in Example~\ref{eg:dnnDemo}. 
    Suppose a bit-flip attacker can alter any single bit in the memory cell storing parameters and we use two dots ``$\cdot\cdot$'' to represent a parameter that is targeted for such attacks.
    Take the parameter $\widehat{\bs{W}}^3_{2,2}$ with $\vec{v}(\widehat{\bs{W}}^3_{2,2})=[1111]$ for example. Its potential attacked representations are  $\vec{v}(\ddot{\widehat{\bs{W}}}^3_{2,2})\in\{[0111],[1011],[1101],[1110]\}$, thus we have $\ddot{\widehat{\bs{W}}}^3_{2,2}\in\{7,-5,-3,-2\}$ and $\ddot{\widetilde{\bs{W}}}^3_{2,2}\in\{1,-0.7143,-0.4286, -0.2857\}$. Given an input $\bs{x}^1=(1,1)$, after de-quantizing integer parameters during the inference, we can get the output of each non-input layer as $\bs{x}^2=(0,1)$ and $\bs{x}^3=(0,-0.1429)$.
    % Similarly, we can get the attacked values for other parameters as follows: 
    % \begin{itemize}
    %     \item $\ddot{\widehat{\bs{W}}}^2_{1,2}\in\{-3,-6,-8,4\}$,
    %             $\ddot{\widehat{\bs{W}}}^2_{2,1}=\{5,6,0,-4\}$,          
    %             $\ddot{\widehat{\bs{W}}}^2_{2,2}=\{6,5,3,-1\}$,
    %             $\ddot{\widehat{\bs{W}}}^3_{1,1}=\{-8,-5,-2,1\}$, $\ddot{\widehat{\bs{W}}}^3_{1,2}=\{1,2,4,-8\}$, $\ddot{\widehat{\bs{W}}}^3_{2,1}=\{7,4,2,-2\}$, $\ddot{\widehat{\bs{W}}}^3_{2,2}=\{-4,-1,-7,5\}$;
    % \item $\ddot{\widetilde{\bs{W}}}^2_{1,2}\in\{-3,-6,-8,4\}$,
    %             $\ddot{\widetilde{\bs{W}}}^2_{2,1}=\{5,6,0,-4\}$,          
    %             $\ddot{\widetilde{\bs{W}}}^2_{2,2}=\{6,5,3,-1\}$,
    %             $\ddot{\widetilde{\bs{W}}}^3_{1,1}=\{-8,-5,-2,1\}$, $\ddot{\widetilde{\bs{W}}}^3_{1,2}=\{1,2,4,-8\}$, $\ddot{\widetilde{\bs{W}}}^3_{2,1}=\{7,4,2,-2\}$, $\ddot{\widetilde{\bs{W}}}^3_{2,2}=\{-4,-1,-7,5\}$;
    % \end{itemize}
    
    
    % $h(\widehat{W}^2_{1,2})=\{1101,1110,1000,0100\}$,
    % $h(\widehat{W}^2_{2,1})=\{0101,0110,0000,1100\}$,
    % $h(\widehat{W}^2_{2,2})=\{0110,0101,0011,1111\}$,
    % $h(\widehat{W}^2_{1,2})=\{-8,-5,-2,1\}$, 
    % $h(\widehat{W}^2_{1,2})=\{0001,0010,0100,1000\}$, 
    % $h(\widehat{W}^2_{1,2})=\{0111,0100,0010,1110\}$, 
    % $h(\widehat{W}^2_{1,2})=\{1100,1111,1001,0101\}$. 
    
    Now, suppose that the attacker flips the fourth bit (i.e., sign bit) of the parameter $\widehat{\bs{W}}^3_{2,2}$, 
    then we have $\ddot{\widehat{\bs{W}}}^3_{2,2}=7$ and $\ddot{\widetilde{\bs{W}}}^3_{2,2}=1$. Finally, the network output after the attack is $\bs{x}^3=(0,1)$, resulting in an altered classification outcome. % for the same input. 
\end{example}


% To better formalize the problem considered in this work, We define an attack vector that characterizes a specific bit-flip attack on a quantized neural network as follows.
\begin{definition}[Attack Vector]
    Given a QNN $\mN$ with quantization bit-width $Q$, and two integers $\mm$ and $\nn$ such that an adversary can attack any $\mm$ parameters by flipping $\nn$ bits at most within each parameter ($\nn\le Q$). 
    % 
    An $(\mm,\nn)$-attack vector $\rho$ is a set of pairs $\{(\alpha_i, P_i) \mid i\le \mm \}$ where $\alpha_i$ is a parameter (weight or bias) of $\mN$ and $P_i$ is a set of bit positions with $|P_i|\leq \nn$. 
    % 
    We use $\mN^\rho$ to denote the resulting network by invoking the attack vector $\rho$ on $\mN$. 
\end{definition}

An $(\mm,\nn)$-attack vector defines the vulnerable parameters and bits that are flipped by the adversary during a specific BFA. 
% Intuitively, to achieve a bit-flip attack vector $\rho=\{(\alpha_i, P_i) \mid i\le \mm \}$, $\sum_{i=1}^{|\rho|} |P_i|$ bit-flips should be injected.
\begin{example}
    Consider the QNN given in Example~\ref{eg:dnnDemo}. Let $\mm=\nn=2$ and an attack vector $\rho=\{(\widehat{\bs{W}}^2_{1,1}, \{2,4\}),(\widehat{\bs{W}}^2_{1,2}, \{3\})\}$. Intuitively, $\rho$ describes a specific bit-flip attack that the 2nd and 4th bits in $\vec{v}(\widehat{\bs{W}}^2_{1,1})=[1001]$ and the 3rd bit in $\vec{v}(\widehat{\bs{W}}^2_{1,2})=[1101]$ are flipped. Then, we have the two's complement representations of attacked parameters as $\vec{v}(\ddot{\widehat{\bs{W}}}^2_{1,1})=[0011]$ and $\vec{v}(\ddot{\widehat{\bs{W}}}^2_{1,2})=[1001]$.
    % , and get the corresponding attacked parameters as $\ddot{\widehat{\bs{W}}}^2_{1,1}=3$ and $\ddot{\widehat{\bs{W}}}^2_{1,2}=-7$.
\end{example}

Note that for clarity and convenience, given a QNN, the de-quantized parameters before (resp. after) BFAs $\widetilde{\bs{W}}^i_{j,k}$ (resp. $\ddot{\widetilde{\bs{W}}}^i_{j,k}$) may be directly represented by $\bs{W}^i_{j,k}$ (resp. $\ddot{\bs{W}}^i_{j,k}$) when it is clear from the context in the subsequent sections.


% Note that even if the adversary is limited to flipping only one parameter, the number of possible $(1,\nn)$-attack vectors is still $K\cdot\sum_{i=1}^\nn\binom{Q}{i}$.


% Considering the documented high susceptibility (nearly 99\% susceptible parameters) of DNNs to bit-flip attacks, quantization has been proposed as a viable defensive technique. On the other hand, quantization has emerged as a promising method for compressing DNNs and facilitating their deployment on resource-constrained devices. Consequently, current research on bit-flip attacks and their defense is increasingly centered on QNNs. Despite these measures, QNNs continue to exhibit vulnerability to BFAs. Current defense techniques lack the provision of formal security assurances against such attacks. This persistent vulnerability highlights the urgent necessity to develop a rigorous verification method that can conclusively ascertain the absence of BFAs in QNNs. Such a method would significantly enhance the integrity and reliability of QNNs, particularly in applications where security is paramount.



