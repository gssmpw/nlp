%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Implementation and Evaluation}\label{sec:exp}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



To validate the effectiveness of our method, we aim to answer the following research questions:

\begin{enumerate}[label=\textbf{RQ\arabic*. },itemindent=*,leftmargin=*,itemsep=0pt]%\setlength{\itemsep}{1pt}
    \item How effective and efficient is {\sf BFA\_RA} for providing the sound verification result on potential attack vectors, compared with the naive baseline method (cf. Section~\ref{sec:naiveMethod})?
    
    \item Can the absence of BFAs be verified with a conclusive result for a specific network using \tool, and how effective is the MILP-based method for providing a sound and complete verification result, as a complementary approach to {\sf BFA\_RA}?
    
    \item How efficient and effective is \tool for verifying the absence of BFAs on larger networks with various activation functions?
\end{enumerate}




\noindent
{\bf Implementation.} We implemented our verification method as an end-to-end tool \tool with Gurobi~\cite{Gurobi} as the back-end MILP solver. The \symPoly \minor{component} is built upon GPUPoly~\cite{gpupoly}, an open-source GPU implementation~\cite{muellch2023eth} of \deepPoly. The quantization follows the scheme mentioned in Section~\ref{sec:quant}. During the network inference procedure and the verification process in \tool, quantized (fixed-point) parameters are stored in the IEEE 754~\cite{FPIEEE754} floating-point number format for arithmetic operation. The floating-point number soundness flag in the implementation~\cite{muellch2023eth} of GPUPoly \cite{gpupoly} is turned on for both \deepPoly and \symPoly.

\smallskip
\noindent
{\bf Datasets.} We use MNIST~\cite{MNIST} and ACAS Xu~\cite{julian2019deep} as the datasets in our experiments. MNIST contains 60,000 grayscale handwritten digits (from 0 to 9) of the size of $28\times 28$. ACAS Xu is a safety-critical system designed to provide collision avoidance advisories for unmanned aircraft.


\smallskip
\noindent
{\bf Networks.} For the MNIST dataset, we train 12$\times$2 QNNs following the post-training quantization scheme~\cite{gholami2021survey,li2024investigating} on the MNIST~\cite{MNIST} dataset, which is a common practice in prior research to improve the robustness against bit-flip attack \cite{HARDeNN,randomDNN}. We evaluate 12 architectures with varying model sizes and 2 quantization bit-widths $Q\in\{4,8\}$, using ReLU activations by default. The details of the QNNs are listed in Tables~\ref{tab:bench_A} and \ref{tab:bench_B}. The first row shows the architecture of each QNN, where $x$blk\_$y$ means that the network has $x$ hidden layers with each hidden layer containing $y$ neurons. Row 2 shows the number of parameters in these networks and Rows 3-4 give the accuracy of these networks under different quantization bit-width, i.e., $Q=4$ and $Q=8$.
Moreover, we consider two additional networks of architecture $3\text{blk}\_100$ with Sigmoid and Tanh activation functions.

For the ACAS Xu dataset, although the authors in~\cite{KBDJK17} provide 45 neural networks trained on this dataset along with 10 safety properties. We find that few of these properties can be proved via \deepPoly on these networks. Hence, in this work, we adopt retrained ones instead of the original networks from~\cite{longNew} as our benchmark. These retrained 45 networks adopt the same architecture as~\cite{KBDJK17}, i.e., 6blk\_50, and maintain comparable accuracy to the original networks (86.6\% on average). These networks output a score for five different actions: clear-of-conflict (COC), weak left (WL), weak right (WR), strong left (SL), and strong right (SR). Based on these, we built 45 QNNs following a post-training quantization scheme, setting the quantization bit-width as $Q=8$.

% \begin{table*}[t]
%     \centering
%     \caption{\yd{QNNs obtained via quantization-aware training on MNIST with small network architectures.}}\label{tab:bench_A}
%     \setlength{\tabcolsep}{3pt}
%     \scalebox{0.88}{
%     \begin{tabular}{c|c|c|c|c|c|c|c|c|c}
%     \toprule
%     % \multicolumn{2}{c|}{~} & \multicolumn{6}{c}{Small Networks} \\ \cline{3-8}
%     \multicolumn{2}{c|}{ArchSmall} & 3blk\_10 & 3blk\_30 & 3blk\_50 & 3blk\_50$^{\text{sig}}$ & 3blk\_50$^{\text{tan}}$ & 5blk\_10 & 5blk\_30 & 5blk\_50 \\ \midrule

%     \multicolumn{2}{c|}{\# Param.} & 8.28k & 26.62k & 47.36k & 47.36k & 47.36k & 8.5k & 28.48k & 52.46k \\ \midrule
    
%     & $Q=4$ & 81.09\% & 94.40\% & 95.96\% & 96.50\% & 96.19\% & 86.80\% & 95.63\% & 96.44\% \\
%     \multirow{-2}*{Acc.} & $Q=8$ & 92.83\% & 96.38\% & 96.38\% & 96.76\% & 97.16\% &  92.87\% & 96.95\% & 97.20\%  \\

      
%     \bottomrule
%     \end{tabular}}
% \end{table*}

\begin{table*}[t]
    \centering
    \caption{QNNs obtained via quantization-aware training on MNIST with small network architectures.}\label{tab:bench_A}
    \setlength{\tabcolsep}{3pt}
    \scalebox{0.88}{
    \begin{tabular}{c|c|c|c|c|c|c|c}
    \toprule
    % \multicolumn{2}{c|}{~} & \multicolumn{6}{c}{Small Networks} \\ \cline{3-8}
    \multicolumn{2}{c|}{ArchSmall} & 3blk\_10 & 3blk\_30 & 3blk\_50 & 5blk\_10 & 5blk\_30 & 5blk\_50 \\ \midrule

    \multicolumn{2}{c|}{\# Param.} & 8.28k & 26.62k & 47.36k & 8.5k & 28.48k & 52.46k \\ \midrule
    
    & $Q=4$ & 81.09\% & 94.40\% & 95.96\% & 86.80\% & 95.63\% & 96.44\% \\
    \multirow{-2}*{Acc.} & $Q=8$ & 92.83\% & 96.38\% & 96.38\% & 92.87\% & 96.95\% & 97.20\%  \\

      
    \bottomrule
    \end{tabular}}
\end{table*}

\begin{table*}[t]
    \centering
    \caption{QNNs obtained via post-training quantization on MNIST with large network architectures.}\label{tab:bench_B}
    \setlength{\tabcolsep}{3pt}
    \scalebox{0.88}{
    \begin{tabular}{c|c|c|c|c|c|c|c|c|c}
    \toprule
    % \multicolumn{2}{c|}{~} & \multicolumn{6}{c|}{Small Networks} & \multicolumn{6}{c}{Large Networks} \\ \cline{3-14}
    \multicolumn{2}{c|}{ArchLarge} & 3blk\_100& 3blk\_100$^\text{sigmoid}$ & 3blk\_100$^\text{tan}$ & 3blk\_512 & 3blk\_1024 & 5blk\_100 & 5blk\_512 & 5blk\_1024  \\ \midrule

    \multicolumn{2}{c|}{\# Param.} & 109.7k & 109.7k & 109.7k & 1,195k & 3,962k & 129.9k & 1,720k & 6,061k \\ \midrule
    
    & $Q=4$ 
      & 97.03\%  % from PTQ_mnist_3blk_100_100_100_qu_4.txt
      & 97.24\%  % from PTQ_mnist_3blk_100_100_100_qu_4_relu_1.txt
      & 97.43\%  % from PTQ_mnist_3blk_100_100_100_qu_4_relu_2.txt
      & 97.85\%  % from PTQ_mnist_3blk_512_512_512_qu_4.txt
      & 97.72\%  % from PTQ_mnist_3blk_1024_1024_1024_qu_4.txt
      & 97.31\%  % from PTQ_mnist_5blk_100_100_100_100_100_qu_4.txt
      & 97.62\%  % from PTQ_mnist_5blk_512_512_512_512_512_qu_4.txt
      & 97.97\%  % from PTQ_mnist_5blk_1024_1024_1024_1024_1024_qu_4.txt
      \\ 
    \multirow{-2}*{Acc.} 
    & $Q=8$ 
      & 97.53\%  % from PTQ_mnist_3blk_100_100_100_qu_8.txt
      & 97.71\%  % from PTQ_mnist_3blk_100_100_100_qu_8_relu_1.txt
      & 97.56\%  % from PTQ_mnist_3blk_100_100_100_qu_8_relu_2.txt
      & 98.18\%  % from PTQ_mnist_3blk_512_512_512_qu_8.txt
      & 97.84\%  % from PTQ_mnist_3blk_1024_1024_1024_qu_8.txt
      & 97.39\%  % from PTQ_mnist_5blk_100_100_100_100_100_qu_8.txt
      & 97.74\%  % from PTQ_mnist_5blk_512_512_512_512_512_qu_8.txt
      & 98.06\%  % from PTQ_mnist_5blk_1024_1024_1024_1024_1024_qu_8.txt
      \\
      
    \bottomrule
    \end{tabular}}
\end{table*}

% \begin{table*}[t]
%     \centering
%     \caption{\yd{QNNs obtained via post-training quantization on MNIST with large network architectures.}}\label{tab:bench_B}
%     \setlength{\tabcolsep}{3pt}
%     \scalebox{0.88}{
%     \begin{tabular}{c|c|c|c|c|c|c|c}
%     \toprule
%     % \multicolumn{2}{c|}{~} & \multicolumn{6}{c|}{Small Networks} & \multicolumn{6}{c}{Large Networks} \\ \cline{3-14}
%     \multicolumn{2}{c|}{ArchLarge} & 3blk\_100& 3blk\_512 & 3blk\_1024 & 5blk\_100 & 5blk\_512 & 5blk\_1024  \\ \midrule

%     \multicolumn{2}{c|}{\# Param.} & 109.7k & 1,195k & 3,962k & 129.9k & 1,720k & 6,061k \\ \midrule
    
%     & $Q=4$ 
%       & 97.03\%  % from PTQ_mnist_3blk_100_100_100_qu_4.txt
%       % & 97.24\%  % from PTQ_mnist_3blk_100_100_100_qu_4_relu_1.txt
%       % & 97.43\%  % from PTQ_mnist_3blk_100_100_100_qu_4_relu_2.txt
%       & 97.85\%  % from PTQ_mnist_3blk_512_512_512_qu_4.txt
%       & 97.72\%  % from PTQ_mnist_3blk_1024_1024_1024_qu_4.txt
%       & 97.31\%  % from PTQ_mnist_5blk_100_100_100_100_100_qu_4.txt
%       & 97.62\%  % from PTQ_mnist_5blk_512_512_512_512_512_qu_4.txt
%       & 97.97\%  % from PTQ_mnist_5blk_1024_1024_1024_1024_1024_qu_4.txt
%       \\ 
%     \multirow{-2}*{Acc.} 
%     & $Q=8$ 
%       & 97.53\%  % from PTQ_mnist_3blk_100_100_100_qu_8.txt
%       % & 97.71\%  % from PTQ_mnist_3blk_100_100_100_qu_8_relu_1.txt
%       % & 97.56\%  % from PTQ_mnist_3blk_100_100_100_qu_8_relu_2.txt
%       & 98.18\%  % from PTQ_mnist_3blk_512_512_512_qu_8.txt
%       & 97.84\%  % from PTQ_mnist_3blk_1024_1024_1024_qu_8.txt
%       & 97.39\%  % from PTQ_mnist_5blk_100_100_100_100_100_qu_8.txt
%       & 97.74\%  % from PTQ_mnist_5blk_512_512_512_512_512_qu_8.txt
%       & 98.06\%  % from PTQ_mnist_5blk_1024_1024_1024_1024_1024_qu_8.txt
%       \\
      
%     \bottomrule
%     \end{tabular}}
% \end{table*}

% \begin{table}[t]
%     \centering
%     \caption{Benchmarks of properties and QNNs obtained via post-quantization training for ACAS Xu.}
%     \setlength{\tabcolsep}{2pt}
%     \scalebox{0.8}{
%     \begin{tabular}{l|l|l}
%         \toprule
%          Property  & Description &  Network    \\ \midrule
%          Prop\_3\_WL & \tabincell{l}{If the intruder is directly ahead and is moving towards the \\ ownership, a ``Weak Left'' maneuver is advised.} & \tabincell{l} {(8): $N_{2,6},N_{2,7},N_{2,8},N_{2,9},$ \\ $N_{4,6},N_{4,7},N_{4,8},N_{4,9}$} \\ \midrule
         
%          Prop\_3\_WR & \tabincell{l}{If the intruder is directly ahead and is moving towards the \\ ownership, a ``Weak Right'' maneuver is advised.} & \tabincell{l}{(9): $N_{1,6},N_{3,6},N_{3,7},N_{3,8},$ \\ $N_{3,9},N_{5,6},N_{5,7},N_{5,8},N_{5,9}$ } \\ \midrule

%          Prop\_3\_SL & \tabincell{l}{If the intruder is directly ahead and is moving towards the \\ ownership, a ``Strong Left'' maneuver is advised.} & \tabincell{l}{(8): $N_{2,2},N_{2,3},N_{2,4},N_{2,5},$ \\ $N_{4,2},N_{4,3},N_{4,4},N_{4,5}$} \\ \midrule

         
%          Prop\_3\_SR & \tabincell{l}{If the intruder is directly ahead and is moving towards the \\ ownership, a ``Strong Right'' maneuver is advised.} & \tabincell{l}{(9): $N_{3,1},N_{3,3},N_{3,4},N_{3,5},$ \\ $N_{5,1}, N_{5,2},N_{5,3},N_{5,4},N_{5,5}$} \\ \midrule
         
%          Prop\_5\_SR & \tabincell{l}{If the intruder is near and approaching from the left, a \\ ``Strong Right'' maneuver is advised.} & \tabincell{l}{(7): $N_{3,1},N_{3,2},N_{3,3},N_{5,2},$ \\ $N_{5,3},N_{5,4}, N_{5,5}$} \\ \midrule

         
%          Prop\_10\_COC & \tabincell{l}{For a far away intruder, a ``Clear of Conflict'' maneuver is \\ advised.} & \tabincell{l}{(14): $N_{1,3},N_{1,4},N_{1,5},N_{1,6}$\\ $N_{3,2},N_{3,6},N_{3,7},N_{4,1},N_{4,2},$ \\ $N_{4,4},N_{4,5},N_{5,1},N_{5,4},N_{5,6}$} \\ 
        
%         \bottomrule
%     \end{tabular}}
%     \label{tab:bench_C}
% \end{table}


\smallskip
\noindent
{\bf Experimental setup.} For the BFA-tolerant robustness verification problem defined in Section~\ref{sec:proDef}, we randomly selected 20 inputs from the MNIST dataset for each network. We considered 3 different attack radii, $r\in\{0,2,4\}$, for each input, resulting in 60 input regions for each network. Note that all these input regions are robust to the corresponding QNNs until the bit-flip attacks.
% Note that all the 20 inputs are correctly classified by the QNNs in Tables~\ref{tab:bench_A} and \ref{tab:bench_B} until the bit-flip attacks. 
For the ACAS Xu benchmark, we test all 45 QNNs on the 10 properties and select the successfully proved 55 network-property pairs as our benchmarks. The details are given in Table \ref{tab:bench_C} in Appendix~\ref{sec:app-bench}.
We set the maximum number of bit flips as $\nn\in \{1,2,4\}$. 
% 
Unless otherwise noted, each BFA\_RA task is conducted on an NVIDIA Tesla V100 accelerator and each BFA\_MILP task is conducted with 30 threads on a computer equipped with AMD EPYC 7742 64-core processors. The time limit for each verification task is 1 hour by default, \minor{considering the large number of tasks (thousands) in the subsequent experiments.}



% \begin{table}[t]
%     \centering
%     \caption{\yd{The computation time (in GPU hours) of the three methods.}}
%     % \setlength{\tabcolsep}{5pt}
%     \scalebox{0.88}{
%     \begin{tabular}{l|c|c}
%         \toprule
%            & $Q=4$ & $Q=8$   \\ \midrule
%          Naive Method  & {$\approx$72.9h} &  {$\approx$488.2h} \\ \midrule
%          {\sf BFA\_RA} w.o. Binary Search & {$\approx $13.6h} & {$\approx $14.0h}  \\ \midrule
%          {\sf BFA\_RA} & {$\approx$14.2h} & {$\approx $14.8h} \\ 

%         \bottomrule
%     \end{tabular}}
%     \label{tab:RA_time}%\vspace{-1mm}
% \end{table}


\subsection{The Effectiveness and Efficiency of {\sf BFA\_RA}}\label{sec:RQ1}
To answer \textbf{RQ1}, for each network listed in Tables~\ref{tab:bench_A} and~\ref{tab:bench_B}, we randomly select 100 weight parameters and up to 100 bias parameters per layer for manipulation by the attacker, considering the huge amount of parameters in these networks.
For each verification task of network with architecture $x$blk\_$y$ and quantized by $Q$ bits, there are $K\cdot\sum_{i=1}^{\nn}{Q \choose i}$ $(1,\nn)$-attack vectors, where $K=100(x+1)+x\min(y,100)+10$. In total, we have $28\times 60\times 3=5040$ verification tasks (28 networks, 60 input regions per network, and 3 different values of $\nn$) for MNIST.

% We compare the performance of the sound verification procedure {\sf BFA\_RA} against the naive baseline method mentioned in Section~\ref{sec:naiveMethod}, which traverses all possible attack vectors and utilizes \deepPoly to conduct robustness analysis on the resulting attacked network. 
% Furthermore, to analyze the effectiveness of the binary search strategy proposed in Algorithm~\ref{alg:verifyPolyR}, we also implement a binary-search-free version of {\sf BFA\_RA}, where {\sf binary\_RA}$(\mN,\mI, g, w,w^+_l,w^+_u)$ (resp. {\sf binary\_RA}$(\mN,\mI,g,w,w^-_l,w^-_u)$) at line 3 (resp. line 5) in Algorithm~\ref{alg:verifyPolyR} is replaced by the function $\symPoly(\mN,\mI,g,w,w^+_l,w^+_u)$ (resp. $\symPoly(\mN,\mI,g,w, w^-_l,w^-_u)$).

\begin{table}[t]
    \centering
    \caption{\minor{Verification results of {\sf BFA\_RA}, {\sf BFA\_RA} w.o binary search, and the naive method. Each entry shows
    the proportion of parameters that are proved to be safe/unknown by two compared methods. For example, the entry in the top left corner indicates that when $r=0, \nn=1$, there are 98.55\% parameters across all verification tasks on all QNNs that are both proved as safe by the naive method and the {\sf BFA\_RA} method.
    The bottom left corner entry indicates 0.03\% of parameters are proven as unknown by the naive method but proved as safe by BFA\_RA, considering 8-bit quantization and robustness radius 4.}}
    \label{tab:RA_effect}
    \setlength{\tabcolsep}{2pt}
{
    \scalebox{0.85}{
\begin{tabular}{cc|ccc|ccc|ccc|ccc}
\toprule
&  & \multicolumn{6}{c|}{\sf BFA\_RA} & \multicolumn{6}{c}{\sf BFA\_RA w.o. Binary Search}  \\ 
\cline{3-14}
\multicolumn{2}{c|}{$Q=4$} 
& \multicolumn{3}{c|}{\#Safe\_Paras} 
& \multicolumn{3}{c|}{\#Unknown\_Paras} 
& \multicolumn{3}{c|}{\#Safe\_Paras} 
& \multicolumn{3}{c}{\#Unknown\_Paras} \\

& & $\nn=1$  & $\nn=2$ & $\nn=4$  & $\nn=1$ & $\nn=2$  & $\nn=4$
& $\nn=1$  & $\nn=2$ & $\nn=4$  & $\nn=1$ & $\nn=2$  & $\nn=4$  \\ 
\midrule

\multirow{3}{*}{\makecell{\minor{Naive Method}\\ \minor{\#Safe\_Paras}}}    & $r=0$ 
& 98.55\% & 98.13\% & 98.07\%
& 0.00\% & 0.00\% & 0.00\%  
& 98.51\% & 98.00\% & 97.89\% 
& 0.04\% & 0.14\% & 0.17\% \\
%\rowcolor{gray!20}


 & $r=2$
& 97.98\% & 97.48\% & 97.42\%
& 0.00\% & 0.00\% & 0.00\% 
& 97.92\% & 97.33\% & 97.22\%
& 0.06\% & 0.16\% & 0.20\% \\

                & $r=4$
& 95.94\% & 95.19\% & 95.06\%
& 0.00\% & 0.00\% & 0.00\% 
& 95.70\% & 94.71\% & 94.51\%
& 0.24\% & 0.48\% & 0.55\% \\
\midrule

\multirow{3}{*}{\makecell{\minor{Naive Method}\\ \minor{\#Unknown\_Paras}}}    & $r=0$ 
& 0.00\% & 0.00\% & 0.00\%
& 1.45\% & 1.87\% & 1.93\% 
& 0.00\% & 0.00\% & 0.00\%
& 1.45\% & 1.87\% & 1.93\% \\ 

%\rowcolor{gray!20}
& $r=2$
& 0.00\% & 0.01\% & 0.01\%
& 2.02\% & 2.51\% & 2.57\% 
& 0.00\% & 0.00\% & 0.00\%
& 2.02\% & 2.52\% & 2.58\% \\ 

                & $r=4$
& 0.02\% & 0.06\% & 0.08\%
& 4.05\% & 4.75\% & 4.85\% 
& 0.02\% & 0.04\% & 0.04\%
& 4.05\% & 4.77\% & 4.89\% \\ 
\bottomrule
\bottomrule

&  & \multicolumn{6}{c|}{\sf BFA\_RA} & \multicolumn{6}{c}{\sf BFA\_RA w.o. Binary Search}  \\ 
\cline{3-14}
\multicolumn{2}{c|}{$Q=8$} 
& \multicolumn{3}{c|}{\#Safe\_Paras} 
& \multicolumn{3}{c|}{\#Unknown\_Paras} 
& \multicolumn{3}{c|}{\#Safe\_Paras} 
& \multicolumn{3}{c}{\#Unknown\_Paras} \\
        
& & $\nn=1$  & $\nn=2$ & $\nn=4$  & $\nn=1$ & $\nn=2$  & $\nn=4$
& $\nn=1$  & $\nn=2$ & $\nn=4$  & $\nn=1$ & $\nn=2$  & $\nn=4$  \\ 
\midrule

\multirow{3}{*}{\makecell{\minor{Naive Method}\\ \minor{\#Safe\_Paras}}}    & $r=0$

& 99.03\% & 98.59\% & 98.41\%
& 0.00\% & 0.00\% & 0.00\% 
& 99.02\% & 98.54\% & 98.33\%
& 0.01\% & 0.06\% & 0.08\% \\ 

%\rowcolor{gray!20}
  & $r=2$
& 98.41\% & 97.89\% & 97.66\%
& 0.00\% & 0.00\% & 0.00\% 
& 98.39\% & 97.83\% & 97.57\%
& 0.02\% & 0.06\% & 0.09\% \\

                & $r=4$
& 96.19\% & 95.43\% & 95.11\%
& 0.00\% & 0.00\% & 0.00\% 
& 95.97\% & 94.95\% & 94.53\%
& 0.21\% & 0.48\% & 0.59\% \\
\midrule

\multirow{3}{*}{\makecell{\minor{Naive Method}\\ \minor{\#Unknown\_Paras}}}    & $r=0$
& 0.00\% & 0.00\% & 0.00\%
& 0.97\% & 1.41\% & 1.59\% 
& 0.00\% & 0.00\% & 0.00\%
& 0.97\% & 1.41\% & 1.59\% \\

%\rowcolor{gray!20} \#Unknown\_Paras 
& $r=2$
& 0.00\% & 0.00\% & 0.01\%
& 1.59\% & 2.11\% & 2.33\% 
& 0.00\% & 0.00\% & 0.01\%
& 1.59\% & 2.11\% & 2.34\% \\

                & $r=4$
& 0.03\% & 0.08\% & 0.11\%
& 3.78\% & 4.49\% & 4.78\% 
& 0.02\% & 0.05\% & 0.06\%
& 3.79\% & 4.52\% & 4.82\% \\
\bottomrule
\end{tabular}
}
}

\end{table}

\begin{table}[t]
    \centering
    \caption{The computation time (in GPU hours) of the three methods.}
    % \setlength{\tabcolsep}{5pt}
    \scalebox{0.88}{
    \begin{tabular}{c|c|c|c}
        \toprule
           & Naive Method & {\sf BFA\_RA} & {\sf BFA\_RA} w.o. Binary Search  \\ \midrule
          $Q=4$ & {$\approx$72.9h} & {$\approx $14.2h} &  {$\approx$13.6h} \\ \midrule
        $Q=8$ & {$\approx $488.2h} & {$\approx $14.8h} & {$\approx $14.0h} \\ 

        \bottomrule
    \end{tabular}}
    \label{tab:RA_time}%\vspace{-1mm}
\end{table}

We compare the performance of {\sf BFA\_RA} with the naive method mentioned in Section~\ref{sec:naiveMethod}. Recall that, given a parameter, the naive method performs one reachability analysis for each $(1,\nn)$-attack vector
to check robustness, whereas {\sf BFA\_RA} performs one reachability analysis for all the possible $(1,\nn)$-attack vectors. Thus, {\sf BFA\_RA} is expected to reduce execution time significantly. To evaluate the trade-off between efficiency and effectiveness in {\sf BFA\_RA} compared to the naive method, we analyze the effectiveness loss/gain in Table~\ref{tab:RA_effect} and the efficiency gain of {\sf BFA\_RA} in Table~\ref{tab:RA_time}, \minor{where
% in Table~\ref{tab:RA_effect}:
\begin{itemize}
   \item Parameters proved as safe by the naive method but unknown by BFA\_RA indicate an \textbf{effectiveness loss} of {\sf BFA\_RA}, i.e., row \#Safe\_Paras and column \#Unknown\_Paras.
    \item Parameters proved as safe by both methods demonstrate the \textbf{effectiveness maintenance} of {\sf BFA\_RA},
    i.e., row \#Safe\_Paras and column \#Safe\_Paras.
    \item Parameters proved as safe by {\sf BFA\_RA} but unknown by the naive method represent an \textbf{effectiveness gain} of {\sf BFA\_RA}, i.e., row \#Unknown\_Paras and column \#Safe\_Paras.
    \item Parameters proved as unknown by both methods reveals the \textbf{limitations} of reachability analysis by both methods, i.e., row \#Unknown\_Paras and column \#Unknown\_Paras.
\end{itemize}}
% 
Furthermore, to assess the effectiveness of the binary search strategy proposed in Section~\ref{sec:bfa_ra} for {\sf BFA\_RA}, we implement a variant that excludes binary search, referred to as {\sf BFA\_RA} w.o. Binary Search, where {\sf binary\_RA}$(\mN,\mI, g, w,w^+_l,w^+_u)$ (resp. {\sf binary\_RA}$(\mN,\mI,g,w,w^-_l,w^-_u)$) at line 3 (resp. line 5) in Algorithm~\ref{alg:verifyPolyR} is replaced by $\symPoly(\mN,\mI,g,w,w^+_l,w^+_u)$ (resp. $\symPoly(\mN,\mI,g,w, w^-_l,w^-_u)$). The corresponding experimental results are also given in Tables~\ref{tab:RA_effect} and \ref{tab:RA_time}.

% In Table~\ref{tab:RA_Q_4}, \#Proved\_Paras (resp. \#Unknown\_Paras) indicates the proportion of parameters that have been proved (remain unknown) by the corresponding method. For example, the top right corner value 0\% indicates no parameters are proved as ``Safe'' by the naive method but ``Unknown'' by BFA\_RA, considering 4-bit quantization and robustness radius 0, and the bottom left corner value 0.2\% indicates 0.2\% of parameters are proved as ``Unknown'' by the naive method but as ``Safe'' by BFA\_RA without Binary Search, considering 8-bit quantization and robustness radius 4. 
% % \yd{For example, the data in the top left corner indicates that when $r=0,\nn=1$, there are $90.4\%$ parameters that are both proved by the naive method and the {\sf BFA\_RA} function without the binary search strategy.} 
% We remark that for $r=0$, \#Unknown\_Paras by the naive method represents the proportion of parameters that are determined definitively unsafe to BFAs. This occurs because when $r=0$, the input region consists of a single input, thus \deepPoly invariably returns a definitive result, either proving or falsifying the BFA-freeness of the parameter. Table~\ref{tab:RA_time} reports the computation time (in wall-clock time) in total.



\subsubsection{\minor{Effectiveness of {\sf BFA\_RA}}} 
By analyzing the experimental results reported in Table~\ref{tab:RA_effect}, we observe that {\sf BFA\_RA} consistently achieve an effectiveness gain over the naive method across all settings, including various radii of input perturbation $r$ and maximal numbers of bit to flip $\nn$. Specifically, in cases where the naive method reports parameters as unknown (\#Unknown\_Paras), {\sf BFA\_RA} successfully verifies additional parameters as safe (\#Safe\_Paras), albeit with a relatively modest gain (up to 0.11\% on average). \major{Indeed, it is reasonable that {\sf BFA\_RA} achieves a relatively small effectiveness gain. In the worst case, {\sf BFA\_RA} performs a binary search over each potential flipped weight value, similar to exhaustive traversal, making it at least as effective as the naive method. However, unlike the naive method, {\sf BFA\_RA} initially treats each symbolic weight as a range and then partitions it using a binary approach. This introduced weight range may affect the reachability analysis of non-input neurons, leading to different abstract elements, i.e., distinct value domains obtained for each neuron between the naive method and \symPoly. In this setting, if a counterexample (a neuron value leading to a successful BFA) falls within the abstract element domain obtained by \deepPoly in the naive method but not within that of \symPoly, then {\sf BFA\_RA} may exhibit an effectiveness gain. However, we argue that such cases should be rare, leading to a limited overall effectiveness gain of {\sf BFA\_RA} over the naive method.}

Furthermore, we also find that the binary-search-free variant of {\sf BFA\_RA} demonstrates a reduction in effectiveness compared to {\sf BFA\_RA}, as certain parameters that are proved as safe by both the naive method and {\sf BFA\_RA} are proved as unknown by the variant. 
For example, when $Q=8$, $r=4$, and $\nn=4$, a total of 0.59\% of the parameters that are verified as safe by both the naive method and {\sf BFA\_RA} are proved as unknown by the binary-search-free variant of {\sf BFA\_RA}.

\vspace{1mm}
\noindent
\setlength{\fboxsep}{3pt}%box to content distance
\setlength{\fboxrule}{1pt}%thickness of box
\fcolorbox{gray!90}{gray!05}{%
    \parbox{0.97\columnwidth}{
        \textbf{Result 1:} {\sf BFA\_RA} demonstrates an effectiveness gain over both the naive method and its binary-search-free variant, albeit with a relatively modest improvement.
    }
}
\vspace{1mm}

By comparing the experimental results between the naive method and binary-search-free variant of {\sf BFA\_RA}, we find that only a small proportion of parameters (up to 0.55\% for $Q=4$ and 0.59\% for $Q=8$) are verified as safe by the naive method but remain unknown by the binary-search-free variant. Recall that the binary-search-free variant performs reachability analysis based on two intervals $[w^+_l,w^+_u]$ and $[w^-_l,w^-_u]$ (cf. line 2 in Algorithm~\ref{alg:verifyPolyR}), to approximate the reachability analysis result under bit-flip attack. The observed comparison results indicate that the abstract domain proposed in \symPoly effectively captures the impact of bit-flip operations on the corresponding parameters with high accuracy.  

\vspace{1mm}
\noindent
\setlength{\fboxsep}{3pt}%box to content distance
\setlength{\fboxrule}{1pt}%thickness of box
\fcolorbox{gray!90}{gray!05}{%
    \parbox{0.97\columnwidth}{
        \textbf{Result 2:} The abstract domain proposed in \symPoly is relatively accurate in approximating the bit-flip operations.
    }
}
% \vspace{1mm}

\subsubsection{Efficiency of {\sf BFA\_RA}}
By comparing the computation time of {\sf BFA\_RA} and the naive method, as given in Table~\ref{tab:RA_time}, we find that {\sf BFA\_RA} consumes significantly less time than the naive method (up to 30x faster). It is noteworthy that the execution time for each query of \deepPoly and \symPoly is nearly identical. Therefore, the number of queries serves as a critical determinant of the overall efficiency of various methods. To illustrate it, we show the total number of queries invoked by the two methods in Figure~\ref{fig:queryNum} (we take $Q=8$ for example) and it can be observed that the naive method invokes an enormous amount of queries of \deepPoly, attributable to the fact that there are up to $K\cdot\sum_{i=1}^{\nn}{Q \choose i}$ queries for each verification task.
On the other hand, although the binary search strategy \minor{slightly} increases the number of \symPoly queries, as shown in Figure~\ref{fig:queryNum}, the execution time of {\sf BFA\_RA} remains comparable to that of its binary-search-free variant (cf. Table~\ref{tab:RA_time}). 




\vspace{1mm}
\noindent
\setlength{\fboxsep}{3pt}%box to content distance
\setlength{\fboxrule}{1pt}%thickness of box
\fcolorbox{gray!90}{gray!05}{%
    \parbox{0.97\columnwidth}{
        \textbf{Result 3:} {\sf BFA\_RA} is significantly more efficient than the naive method, achieving up to a 30x speedup. Moreover, it demonstrates comparable efficiency to its binary-search-free variant. 
    }
}
% \vspace{2mm}




% Such a comparison result indicates that the abstract domain proposed in \symPoly is relatively accurate in abstracting the bit-flip operations on the corresponding parameter. The result also suggests that we can effectively protect a neural network from BFAs by protecting only a few important bits - using techniques such as trusted computing.


\begin{figure*}[t]
	\centering
	% \subfigure[$(Q,\nn)=(4,1)$.]{\label{fig:query_4_1}
	% 	\begin{minipage}[b]{0.25\textwidth}
	% 		\includegraphics[width=1.0\textwidth]{figs/Q4_n1.pdf}
	% 	\end{minipage}	
	% }%\hspace{5mm}
 %        \subfigure[$(Q,\nn)=(4,2)$.]{\label{fig:query_4_2}
	% 	\begin{minipage}[b]{0.25\textwidth}
	% 		\includegraphics[width=1.0\textwidth]{figs/Q4_n2.pdf} 
	% 	\end{minipage}	
	% }
 %        \subfigure[$(Q,\nn)=(4,4)$.]{\label{fig:query_4_4}
	% 	\begin{minipage}[b]{0.25\textwidth}
	% 		\includegraphics[width=1.0\textwidth]{figs/Q4_n4.pdf}
	% 	\end{minipage}	
	% }%\hspace{5mm}
        
        \subfigure[$(Q,\nn)=(8,1)$.]{\label{fig:query_8_1}
		\begin{minipage}[b]{0.3\textwidth}
			\includegraphics[width=1.0\textwidth]{figs/Q8_n1.pdf}
		\end{minipage}	
	}
        \subfigure[$(Q,\nn)=(8,2)$.]{\label{fig:query_8_2}
		\begin{minipage}[b]{0.3\textwidth}
			\includegraphics[width=1.0\textwidth]{figs/Q8_n2.pdf}
		\end{minipage}	
	}
        \subfigure[$(Q,\nn)=(8,4)$.]{\label{fig:query_8_4}
		\begin{minipage}[b]{0.3\textwidth}
			\includegraphics[width=1.0\textwidth]{figs/Q8_n4.pdf}
		\end{minipage}	
	}
        
	\caption{The total number of queries with respect to \deepPoly or \symPoly in the three methods  when $Q=8$.}%\vspace{-4mm}
	\Description{The total number of queries with respect to \deepPoly or \deepPolyR in the three methods when $Q=8$.}
    \label{fig:queryNum}
\end{figure*}



\subsection{Verifying BFAs with \tool}\label{sec:exp_tool_small}
To answer \textbf{RQ2}, in this section, we use \tool to verify the BFA-tolerant robustness property across all small networks outlined in Table~\ref{tab:bench_A}  and all properties listed in Table~\ref{tab:bench_C}. This results in a total of $12\times 60\times 3=2160$ (12 networks, 60 input regions per network, and 3 different values of $\nn$) verification tasks for the MNIST dataset and $55\times 3 =165$ (55 network-property pairs and 3 different values of $\nn$) verification tasks for the ACAS Xu dataset. It is important to note that, for each verification task, we assume that all model parameters, including weights and biases, are vulnerable to bit-flip attacks. Furthermore, we consider each attack to affect only a single parameter at a time, with the attacker potentially altering up to $\nn\in\{1,2,4\}$ bits per attack.
% 
% 
In the following, we define a task as successfully proved by {\sf BFA\_RA} when all parameters in the corresponding network are proved as safe by {\sf BFA\_RA}. Additionally, we consider a task as successfully solved by \tool when it is either proved by {\sf BFA\_RA} or proved/falsified by {\sf BFA\_MILP}.

\begin{table}[t]
    \centering
    \caption{Verification results of \tool on ACAS Xu}\label{tab:tool_acas}
    \setlength{\tabcolsep}{2pt}
    \scalebox{0.8}{
    \begin{tabular}{c|c|c|c|c|c|c|c}
    \toprule
      &  \multicolumn{2}{c|}{{\sf BFA\_RA}} & \multicolumn{2}{c|}{{\sf BFA\_MILP}} & \multicolumn{2}{c|}{AvgTime(s)} & \\ \cline{2-7}
     \multirow{-2}*{Property} & \#Safe\_Paras & \#Proved & \#Proved & \#Falsified & {\sf BFA\_RA} & {\sf BFA\_MILP}& \multirow{-2}*{\#TO} \\ \midrule \rowcolor{gray!20}

     Prop\_3\_WL & 99.8\% & 0  & 0  & 24  & 356.0 & 4.2  & 0 \\ 
     Prop\_3\_WR & 99.9\%  & 0  & 1 & 26  & 354.7  & 4.0  & 0\\ \rowcolor{gray!20}
     Prop\_3\_SL & 99.9\%  & 0 & 0 & 23  & 354.8  & 44.2 & 1 \\
     Prop\_3\_SR & 99.7\%  & 0 & 0  & 27  & 357.5  & 5.9  & 0 \\ \rowcolor{gray!20}
     Prop\_5\_SR & 98.0\%  & 0  & 0  & 20  & 381.2  & 45.3 & 1 \\
     Prop\_10\_COC & 99.5\%  & 11 & 2  & 13  & 365.2  & 227.7  & 16\\
     
    \bottomrule
    \end{tabular}}%\vspace{-2mm}
\end{table}

% \begin{table}[t]
%     \centering
%     \caption{\yd{Verification results of \tool on ACAS Xu}}\label{tab:tool_acas}
%     % \setlength{\tabcolsep}{3pt}
%     \scalebox{0.9}{
%     \begin{tabular}{c|c|c|c|c|c|c|c}
%     \toprule
%       &  \multicolumn{2}{c|}{{\sf BFA\_RA}} & \multicolumn{2}{c|}{{\sf BFA\_MILP}} & \multicolumn{2}{c|}{AvgTime(s)} & \\ \cline{2-7}
%      \multirow{-2}*{Property} & \#Proved\_Paras & \#Proved & \#Proved & \#Falsified & {\sf BFA\_RA} & {\sf BFA\_MILP}& \multirow{-2}*{\#TO} \\ \midrule \rowcolor{gray!20}

%      Prop\_3\_WL & 99.8\% (99.7\%) & 0 (0) & 0 (0) & 24 (24) & 356.0 (351.5) & 4.2 (9.3) & 0 (0)\\ 
%      Prop\_3\_WR & 99.9\% (99.8\%) & 0 (0) & 1 (1) & 26 (26) & 354.7 (350.8) & 4.0 (4.2) & 0 (0)\\ \rowcolor{gray!20}
%      Prop\_3\_SL & 99.9\% (99.8\%) & 0 (0) & 0 (0) & 23 (24) & 354.8 (347.3) & 44.2 (96.8) & 1 (0) \\
%      Prop\_3\_SR & 99.7\% (99.5\%) & 0 (0) & 0 (0) & 27 (27) & 357.5 (346.0) & 5.9 (26.1) & 0 (0)\\ \rowcolor{gray!20}
%      Prop\_5\_SR & 98.0\% (97.4\%) & 0 (0) & 0 (0) & 20 (18) & 381.2 (348.1) & 45.3 (99.5) & 1 (3)\\
%      Prop\_10\_COC & 99.5\% (99.3\%) & 11 (9) & 2 (1) & 13 (10) & 365.2 (351.4) & 227.7 (67.1) & 16 (22)\\
     
%     \bottomrule
%     \end{tabular}}
% \end{table}


\begin{figure}[t]
	\centering
	\subfigure[Vulnerable weights.]{\label{fig:acas_weight}
		\begin{minipage}[b]{0.35\textwidth}
			\includegraphics[width=1.0\textwidth]{figs/acas_weight.pdf}
		\end{minipage}	
	}\hspace{5mm}
        \subfigure[Vulnerable biases.]{\label{fig:acas_bias}
		\begin{minipage}[b]{0.35\textwidth}
			\includegraphics[width=1.0\textwidth]{figs/acas_bias.pdf}
		\end{minipage}	
	}
        
	\caption{The number of vulnerable parameters detected by {\sf BFA\_RA} in each layer on ACAS Xu.}\vspace{-2mm}
	\Description{The distribution of vulnerable parameters on ACAS Xu.}
    \label{fig:alloPara_acas}
\end{figure}

\subsubsection{ACAS Xu.}\label{sec:exp_tool_small_acas} The results for ACAS Xu are shown in Table~\ref{tab:tool_acas} and Figure~\ref{fig:alloPara_acas}. In Table~\ref{tab:tool_acas}, Column 1 shows the property verified. Column 2 shows the average proportion of parameters that are proved to be safe by {\sf BFA\_RA} across all corresponding networks and three different values of $\nn$. Column 3 gives the number of verification tasks that can be successfully verified by {\sf BFA\_RA}. Columns 4 and 5 display the verification results by {\sf BFA\_MILP}. Columns 6 and 7 give the average computation time for the two methods, and the last column gives the number of verification tasks that run out of time within 1 hour.
% 
We can observe that for ACAS Xu, \tool successfully solved 147 tasks, with 18 tasks running out of time within 1 hour. Among all of these, {\sf BFA\_RA} proves 11 tasks independently. \major{Note that when {\sf BFA\_RA} w.o. binary search is used instead of {\sf BFA\_RA}, the total number of proved tasks via pure reachability analysis decreases by 2, and the total number of timeout tasks (by {\sf BFA\_MILP}) increases by 7. It is because that binary search strategy enables {\sf BFA\_RA} to consistently obtain a tighter value range for each vulnerable parameter, leading to a more compact MILP model (or a reduced solution space) for the {\sf BFA\_MILP} procedure and improving the overall efficiency. Detailed experimental results are presented in Table~\ref{tab:tool_acas_wo_bs} in Appendix~\ref{sec:app_exp}.}


Figures~\ref{fig:acas_weight} and~\ref{fig:acas_bias} show the detailed distribution of vulnerable weights and biases across all 7 non-input layers within the ACAS Xu networks, respectively. On average, we find that the proportion of vulnerable parameters in the earlier layers of the ACAS Xu networks is higher than that in the later layers. This observation suggests that enhanced protection measures should be prioritized for the parameters in the preceding layers to effectively mitigate the impact of bit-flip attacks. One possible reason behind this phenomenon is that the earlier layers of the ACAS Xu networks play a crucial role in feature extraction, making their parameters more susceptible to perturbations caused by BFAs. Since these layers directly influence the representations propagated through the network, any disruption in their parameters can have a cascading effect on the overall network performance, thereby increasing their vulnerability.

\begin{table}[t]
    \centering
    \caption{Verification results of \tool on MNIST for small networks when $(Q,r,\nn)=(8,0,1)$.}\label{tab:tool_mnist_small}
    \setlength{\tabcolsep}{3pt}
    \scalebox{0.85}{
    \begin{tabular}{c|c|c|c|c|c|c|c}
    \toprule
      &  \multicolumn{2}{c|}{{\sf BFA\_RA}} & \multicolumn{2}{c|}{{\sf BFA\_MILP}} & \multicolumn{2}{c|}{AvgTime(s)} & \\ \cline{2-7}
     \multirow{-2}*{Network} & \#Safe\_Paras & \#Proved & \#Proved & \#Falsified & {\sf BFA\_RA} & {\sf BFA\_MILP} & \multirow{-2}*{\#TO} \\ \midrule \rowcolor{gray!20}

    3blk\_10 & 99.7\% & 0 & 0 & 20 & 30.4 & 0.2 & 0  \\ 
    3blk\_30 & 99.9\% & 11 & 0 & 9 & 103.6 & 0.4 & 0 \\ \rowcolor{gray!20}
    3blk\_50 & 99.9\% & 19 & 1 & 0 & 203.6 & 0.7 & 0 \\
    % 3blk\_50$^\text{sig}$ & \% &  & N/A & N/A &  & N/A & 0 \\ \rowcolor{gray!20}
    % 3blk\_50$^\text{tanh}$ & \% &  & N/A & N/A &  & N/A & 0  \\

    5blk\_10 & 99.2\% & 0 & 0 & 20 & 47.2 & 0.4 & 0 \\ \rowcolor{gray!20}
    5blk\_30 & 99.9\% & 17 & 3 & 0 & 171.9 & 0.4 & 0 \\
    5blk\_50 & 99.9\% & 0 & 0 & 20 & 352.2 & 0.8 & 0 \\
     
    \bottomrule
    \end{tabular}}\vspace{-2mm}
\end{table} 

\begin{figure}[t]
	\centering
	\subfigure[Vulnerable Weights.]{\label{fig:mnist_weight}
		\begin{minipage}[b]{0.35\textwidth}
			\includegraphics[width=1.0\textwidth]{figs/mnist_weight.pdf}
		\end{minipage}	
	}\hspace{5mm}
        \subfigure[Vulnerable Biases.]{\label{fig:mnist_bias}
		\begin{minipage}[b]{0.35\textwidth}
			\includegraphics[width=1.0\textwidth]{figs/mnist_bias.pdf}
		\end{minipage}	
	}
        
	\caption{The distribution of vulnerable parameters in each layer on MNIST when $(Q,r,\nn)=(8,0,1)$.}\vspace{-2mm}
	\Description{The distribution of vulnerable parameters in each layer on MNIST dataset when $(Q,r,\nn)=(8,0,1)$.}
    \label{fig:alloPara_mnist}
\end{figure}

\subsubsection{MNIST}
For the MNIST benchmark, \tool successfully verifies the absence of BFAs for all 2160 tasks across all 12 QNNs. Due to space limitations,  
rather than presenting the average results across all possible values of $Q$, $r$, and $\nn$, this section provides detailed verification results for a specific configuration of $(Q,r,\nn)=(8,0,1)$ as an illustrative example. 

The results are presented in Table~\ref{tab:tool_mnist_small} and Figure~\ref{fig:alloPara_mnist}. 
In Table~\ref{tab:tool_mnist_small}, the first column lists the network architecture, while the remaining columns display the same types of results as those shown in Table~\ref{tab:tool_acas}. From Table~\ref{tab:tool_mnist_small}, we can observe that for small networks, {\sf BFA\_RA} achieves relatively high precision, as the majority of tasks (69 out of 73) that fail to be proved by {\sf BFA\_RA} are indeed not robust to bit-flip attacks and subsequently falsified by {\sf BFA\_MILP}.

Figures~\ref{fig:mnist_weight} and~\ref{fig:mnist_bias} give the average number of vulnerable weights and biases in each layer within the 8 networks, respectively. We observe a distinct phenomenon compared to ACAS Xu: in these small networks, the parameters in the middle layers exhibit greater vulnerability to bit-flip attacks. This suggests that greater attention should be given to protecting the middle layer to mitigate the impact of BFAs effectively.
% 
One possible explanation is that given the high input dimensions in these small networks, middle layers play a crucial role in transforming extracted features into high-level representations with fewer redundancy mechanisms to compensate for errors, making them more susceptible to BFAs. 
% For example, in the output layer, the ten output features determine the final output classification directly, making the corresponding parameters much more vulnerable. 
Furthermore, we find that the bias parameters of these small QNNs for MNIST exhibit significantly greater robustness against BFAs compared to those of QNNs for ACAS Xu.





\vspace{2mm}
\noindent
\setlength{\fboxsep}{3pt}%box to content distance
\setlength{\fboxrule}{1pt}%thickness of box
\fcolorbox{gray!90}{gray!05}{%
    \parbox{0.97\columnwidth}{
        \textbf{Result 4:} \tool can verify the absence of the bit-flip attacks, either prove the BFA-freeness or return a counter-example, and {\sf BFA\_MILP} is effective as a complementary method to {\sf BFA\_RA.}
    }
}
\vspace{2mm}

\subsection{\tool on Larger Networks with Various Activation Functions}\label{sec:exp_tool_large}

%\red{3blk\_512 r=2 one input: 14hours.} 
To answer RQ3, in this subsection, we evaluate the performance of \tool on the larger networks listed in Table~\ref{tab:bench_B} 
% with those under the same architecture.  but different activation functions (sigmoid and tanh activations), 
resulting in a total of 
\minor{$16 \times 60 \times 3=2880$ (16 networks, 60
% $12\times 3 \times 60 \times 3=8640$ (12$\times$3=48 networks, 60 
input regions per network, and 3 different values of $\nn$)} verification tasks. Note that, although {\sf BFA\_RA} is more efficient (in polynomial time to the network size) compared to the MILP-based method (which is NP-hard), it is still possible for the MILP-based method to effectively and efficiently verify BFA-tolerant robustness properties when the size of the input region and the vulnerable parameter set, i.e., $|\xi|$ in Algorithm~\ref{alg:overall}, are limited.

\begin{table}[t]
    \centering
    \caption{\major{Verification results of \tool on the MNIST dataset for large networks, where each network undergoes 360 verification tasks in total.}}\label{tab:tool_mnist_diffAct}
    \setlength{\tabcolsep}{3pt}
    \scalebox{0.85}{
    \begin{tabular}{c|c|c|c|c|c}
    \toprule
      \multirow{2}*{} &  {\sf BFA\_RA} & \multicolumn{2}{c|}{{\sf BFA\_MILP}} & \multirow{2}*{\#TO} & \multirow{2}*{\#Solved}\\ \cline{2-4} 
     & \#Proved & \#Proved & \#Falsified & \\ \midrule \rowcolor{gray!20}
    3blk\_100 & 302 & 14 & 10 & 34 & 360 \\ 
    3blk\_100$^\text{sigmoid}$ & 347 & N/A & N/A & N/A & 347 \\ \rowcolor{gray!20}
    3blk\_100$^\text{tanh}$ & 311 & N/A & N/A & N/A & 311 \\
    5blk\_100 & 290 & 6 & 10 & 54 & 306\\ %\rowcolor{gray!20}
    % 5blk\_100$^\text{sig}$ & & & & & \\ 
    % 5blk\_100$^\text{tanh}$ & & & & & \\ 
    \bottomrule
    \end{tabular}}
\end{table} 


\begin{table}[t]
    \centering
    \caption{Detailed verification results of \tool on 3blk\_100 and 5blk\_100 with $Q\in\{4,8\}$.}\label{tab:tool_large_1}
    \setlength{\tabcolsep}{2pt}
    \scalebox{0.8}{
    \begin{tabular}{c|c|c|c|c|c|c|c|c|c}
    \toprule
     &  &  & \multicolumn{2}{c|}{{\sf BFA\_RA}} & \multicolumn{2}{c|}{{\sf BFA\_MILP}} & \multicolumn{2}{c|}{AvgTime(s)} & \\ \cline{4-9}
     \multirow{-2}*{ } & \multirow{-2}*{r} & \multirow{-2}*{$\nn$ } & \#Safe\_Paras & \#Proved & \#Proved & \#Falsified & {\sf BFA\_RA} & {\sf BFA\_MILP} & \multirow{-2}*{\#TO} \\ \midrule 

     &   & 1 & 100.0\% & 40 & 0 & 0 & 731.9 & 0 & 0\\\rowcolor{gray!20}
     \cellcolor{white} & 0 & 2 & 100.0\% & 40 & 0 & 0 & 749.8 & 0 & 0\\ 
     &   & 4 & 100.0\% & 40 & 0 & 0 & 761.9 & 0 & 0\\ \cline{2-10}

     &   & 1 & 100.0\% & 40 & 0 & 0 & 2031.5 & 0 & 0\\\rowcolor{gray!20}
     \cellcolor{white} $Q=4$ & 2 & 2 & 99.9\% & 38 & 1 & 1 & 2035.3 & 6.5 & 0\\ 
     &   & 4 & 99.9\% & 38 & 1 & 1 & 2035.7 & 6.8 & 0\\ \cline{2-10}

     &   & 1 & 99.8\% & 29 & 3 & 0 & 2035.4 & 685.4 & 8 \\ \rowcolor{gray!20}
     \cellcolor{white}  & 4 & 2 & 99.8\% & 19 & 3 & 3 & 2064.1 & 449.6 & 15 \\ 
     &   & 4 & 99.8\% & 19 & 3 & 2 & 2038.3 & 783.0 & 16 \\ %\cline{1-10}

    \bottomrule
    \bottomrule

    & &  & \multicolumn{2}{c|}{{\sf BFA\_RA}} & \multicolumn{2}{c|}{{\sf BFA\_MILP}} & \multicolumn{2}{c|}{AvgTime(s)} & \\ \cline{4-9}
     \multirow{-2}*{} &  \multirow{-2}*{r} & \multirow{-2}*{$\nn$ } & \#Safe\_Paras & \#Proved & \#Proved & \#Falsified & {\sf BFA\_RA} & {\sf BFA\_MILP} & \multirow{-2}*{\#TO} \\ \midrule

      &  & 1 & 100.0\% & 40 & 0 & 0 & 722.2 & 0 & 0\\ \rowcolor{gray!20}
    \cellcolor{white}  & 0 & 2 & 100.0\% & 40 & 0 & 0 & 745.0 & 0 & 0\\ 
     &   & 4 & 99.9\% & 39 & 0 & 1 & 754.2 & 1.6 & 0\\ \cline{2-10}

     &   & 1 & 99.9\% & 38 & 1 & 1 & 2032.5 & 2.8 & 0\\ \rowcolor{gray!20}
     \cellcolor{white} $Q=8$ & 2 & 2 & 99.9\% & 37 & 1 & 2 & 2038.6 & 6.9 & 0\\ 
     &   & 4 & 99.9\% & 37 & 0 & 3 & 2046.4 & 2.6 & 0\\ \cline{2-10}

     &   & 1 & 99.6\% & 22 & 3 & 1 & 2048.4 & 854.0 & 14 \\ \rowcolor{gray!20}
     \cellcolor{white} & 4 & 2 & 99.5\% & 18 & 3 & 2 & 2091.6 & 1482.3 & 17 \\ 
     &   & 4 & 99.4\% & 18 & 1 & 3 & 2173.7 & 297.7 & 18 \\
    
    \bottomrule
    \end{tabular}}\vspace{-2mm}
\end{table}



% \begin{table}[t]
%     \centering
%     \caption{\yd{Verification results of \tool on 3blk\_100 and 5blk\_100 with $Q=4$.}}\label{tab:tool_large_1}
%     % \setlength{\tabcolsep}{3pt}
%     \scalebox{0.9}{
%     \begin{tabular}{c|c|c|c|c|c|c|c|c}
%     \toprule
%       &  & \multicolumn{2}{c|}{{\sf BFA\_RA}} & \multicolumn{2}{c|}{{\sf BFA\_MILP}} & \multicolumn{2}{c|}{AvgTime(s)} & \\ \cline{3-8}
%      \multirow{-2}*{r} & \multirow{-2}*{$\nn$ } & \#Proved\_Paras & \#Proved & \#Proved & \#Falsified & {\sf BFA\_RA} & {\sf BFA\_MILP} & \multirow{-2}*{\#TO} \\ \midrule 

%         & 1 & 100.0\% & 40 & 0 & 0 & 719.7 & 0 & 0\\
%       0 & 2 & 100.0\% & 40 & 0 & 0 & 727.4 & 0 & 0\\ \rowcolor{gray!20}
%         & 4 & 100.0\% & 40 & 0 & 0 & 740.2 & 0 & 0\\ \midrule

%         & 1 & 100.0\% & 40 & 0 & 0 & 2039.7 & 0 & 0\\
%       2 & 2 & 99.9\% & 38 & 1 & 1 & 1999.8 & 5.4 & 0\\ \rowcolor{gray!20}
%         & 4 & 99.9\% & 38 & 1 & 1 & 2057.9 & 4.7 & 0\\ \midrule

%         & 1 & 99.9\% & 29 & 2 & 0 & 2042.5 & 789.9 & 9\\
%       4 & 2 & 99.9\% & 19 & 4 & 3 & 2017.7 & 858.7 & 14\\ \rowcolor{gray!20}
%         & 4 & 99.9\% & 19 & 3 & 3 & 2066.4 & 534.7 & 15\\

     
%     \bottomrule
%     \end{tabular}}
% \end{table}

% \begin{table}[t]
%     \centering
%     \caption{\yd{Verification results of \tool on 3blk\_100 and 5blk\_100 with $Q=8$.}}\label{tab:tool_large_2}
%     % \setlength{\tabcolsep}{3pt}
%     \scalebox{0.9}{
%     \begin{tabular}{c|c|c|c|c|c|c|c|c}
%     \toprule
%       &  & \multicolumn{2}{c|}{{\sf BFA\_RA}} & \multicolumn{2}{c|}{{\sf BFA\_MILP}} & \multicolumn{2}{c|}{AvgTime(s)} & \\ \cline{3-8}
%      \multirow{-2}*{r} & \multirow{-2}*{$\nn$ } & \#Proved\_Paras & \#Proved & \#Proved & \#Falsified & {\sf BFA\_RA} & {\sf BFA\_MILP} & \multirow{-2}*{\#TO} \\ \midrule 

%         & 1 & 100.0\% & 40 & 0 & 0 & 709.7 & 0 & 0\\ \rowcolor{gray!20}
%       0 & 2 & 100.0\% & 40 & 0 & 0 & 737.0 & 0 & 0\\
%         & 4 & 99.9\% & 39 & 0 & 1 & 737.7 & 1.7 & 0\\ \midrule
%         & 1 & 99.9\% & 38 & 1 & 1 & 2012.1 & 3.3 & 0\\ \rowcolor{gray!20}
%       2 & 2 & 99.9\% & 37 & 1 & 2 & 2006.3 & 6.1 & 0\\
%         & 4 & 99.9\% & 37 & 0 & 3 & 2037.8 & 3.8 & 0\\ \midrule
%         & 1 & 99.6\% & 22 & 3 & 1 & 2047.7 & 1681.7 & 14\\ \rowcolor{gray!20}
%       4 & 2 & 99.7\% & 18 & 3 & 1 & 2298.9 & 758.3 & 18\\
%         & 4 & 99.7\% & 18 & 1 & 2 & 3155.1 & 45.1 & 19\\

     
%     \bottomrule
%     \end{tabular}}
%     \label{tab:tool_large_2}
% \end{table}

We observe that, with the exceptions of networks under the  3blk\_100 and 5blk\_100 architectures,
all other large networks listed in Table~\ref{tab:bench_B} cannot be successfully verified by \tool within 1 hour. \minor{For instance, a verification task for the network of architecture 3blk\_512\_512\_512 with $(Q,r,\nn)=(8,2,1)$ requires approximately 14 hours to complete.} Such failure is largely attributable to the challenges posed by i) the vast number of potential attack vectors and ii) the substantial model sizes when utilizing the MILP-based method, mirroring the issues found in existing MILP-based verification techniques in the context of vanilla network verification problems~\cite{GuyKatz2017ReluplexAE,huang2024towards}.

We report the experimental results in Tables~\ref{tab:tool_mnist_diffAct} and~\ref{tab:tool_large_1}.
% give the detailed verification result of the networks of architectures 3blk\_100 and 5blk\_100 in Table~\ref{tab:tool_large_1}.
% Note that, when {\sf BFA\_RA} w.o. binary search is used instead of {\sf BFA\_RA}, the total number of proved tasks via pure reachability analysis decreases by 1, and the total number of timeout tasks (by {\sf BFA\_MILP}) increases by 3. Detailed experimental results are presented in Table~\ref{tab:tool_large_2} in Appendix~\ref{sec:app_exp}.
% 
\major{We find that \tool successfully solves the majority of verification tasks for the four networks in Table~\ref{tab:tool_mnist_diffAct}. For networks with logistic-based activations (3blk\_100$^\text{sig}$ and 3blk\_100$^\text{tanh}$), although \tool can only provide sound verification results by exclusively utilizing {\sf BFA\_RA}, \tool is still able to solve most tasks within the given time limit.} 
\minor{We also find that, compared to small networks, larger networks appear to exhibit greater robustness against BFAs. Specifically, when $r=0$ (cf. Table~\ref{tab:tool_large_1}), almost all BFA-tolerant properties can be successfully verified by {\sf BFA\_RA} solely (except for one property when $(Q,r,\nn)=(8,0,4)$), indicating enhanced resistance to bit-flip attacks within larger network architectures. Moreover, we observe that networks quantized with a lower bit-width tend to exhibit greater robustness against BFAs. This suggests that reduced bit-width quantization may inherently increase the difficulty of executing bit-flip attacks, a finding that aligns with the existing work~\cite{he2020defending}}. 


\vspace{2mm}
\noindent
\setlength{\fboxsep}{3pt}%box to content distance
\setlength{\fboxrule}{1pt}%thickness of box
\fcolorbox{gray!90}{gray!05}{%
    \parbox{0.97\columnwidth}{
        \textbf{Result 5:} \tool demonstrates generalizability across various activation functions and scales to $8$-bit QNNs with a 5blk\_100 architecture, completing verification of BFAs involving up to four bit flips within 1 hour.
    }
}
\vspace{2mm}

\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{figs/cacuts.pdf}
    \caption{\major{The impact of time limit on the number of solved tasks by {\sf BFA\_MILP} for experiments in Table \ref{tab:tool_large_1}.}}
    \label{fig:table10_cactus}%\vspace{-2mm}
\end{figure}


\major{For the unsolved tasks in Table~\ref{tab:tool_large_1}, we further investigate how the number of verified tasks evolves with extended time limits (up to 4 hours). The results are illustrated in Figure~\ref{fig:table10_cactus}. We observe that as the time limit increases, both the number of proved and the number of falsified tasks by {\sf BFA\_MILP} increase,  with the number of proved tasks exhibiting a more significant growth.} 