\documentclass[runningheads]{llncs}

% ---------------------------------------------------------------
% Include basic ECCV package
 
% TODO REVIEW: Insert your submission number below by replacing '*****'
% TODO FINAL: Comment out the following line for the camera-ready version
\usepackage[review,year=2024,ID=7885]{eccv}
% TODO FINAL: Un-comment the following line for the camera-ready version
% \usepackage{eccv}

% OPTIONAL: Un-comment the following line for a version which is easier to read
% on small portrait-orientation screens (e.g., mobile phones, or beside other windows)
%\usepackage[mobile]{eccv}


% ---------------------------------------------------------------
% Other packages

% Commonly used abbreviations (\eg, \ie, \etc, \cf, \etal, etc.)
\usepackage{eccvabbrv}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{booktabs}

% The "axessiblity" package can be found at: https://ctan.org/pkg/axessibility?lang=en
\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Recommended (but not required) packages
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amssymb}            % Defines common symbols like \mathbb R
\usepackage{mathtools}          % Extends amsmath, providing common math tools
\usepackage{mathrsfs}           % Enables \mathscr, which can work in cases that \mathcal does not
% \mathtoolsset{showonlyrefs}     % Only number equations that are referenced (optional)
\usepackage{graphicx}           % For including images
\usepackage{subcaption}         % Allows for the use of subfigures and subcaptions
\usepackage[space]{grffile}     % For spaces in image names
\usepackage{url}                % For displaying urls
\usepackage{booktabs} % For professional looking tables
\usepackage{multirow} % For multirow feature
\usepackage{adjustbox}
% \usepackage{caption}
\usepackage[font=scriptsize,labelfont=bf]{caption}
\usepackage{comment}
% ---------------------------------------------------------------
% Hyperref package

% It is strongly recommended to use hyperref, especially for the review version.
% Please disable hyperref *only* if you encounter grave issues.
% hyperref with option pagebackref eases the reviewers' job, but should be disabled for the final version.
%
% If you comment hyperref and then uncomment it, you should delete
% main.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).

% TODO FINAL: Comment out the following line for the camera-ready version
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
% TODO FINAL: Un-comment the following line for the camera-ready version
%\usepackage{hyperref}

% Support for ORCID icon
\usepackage{orcidlink}

\newcommand{\smalltablefont}{\fontsize{9pt}{14pt}\selectfont}
\newcommand{\smallertablefont}{\fontsize{6pt}{8pt}\selectfont}
\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

\begin{document}

% ---------------------------------------------------------------
% TODO REVIEW: Replace with your title
% \title{Beyond Imitation: Masked Humanoid Controller for Under-specified Control and Planning} 
% \title{Beyond Imitation: Generating Realistic and Directable Human Motions with Masked Adversarial Imitation} 
% \title{Generating Physically Simulated Humanoid Behavior from Multi-Modal Under-Specified Commands}
\title{Generating Physically Realistic and Directable Human Motions from Multi-Modal Inputs}
% \title{The Masked Humanoid Controller (MHC) for Generating Physically Realistic and Directable Human Motions from Multi-Modal Inputs}


% TODO REVIEW: If the paper title is too long for the running head, you can set
% an abbreviated paper title here. If not, comment out.
% \titlerunning{Abbreviated paper title}

% TODO FINAL: Replace with your author list. 
% Include the authors' OCRID for the camera-ready version, if at all possible.
\author{Aayam Shrestha^{\ast}\inst{1} \and
Pan Liu^{\ast}\inst{2} \and
Kai Yuan\inst{2}  \and
German Ros^{\dagger}\inst{3}  \and
Alan Fern\inst{1}
}

% TODO FINAL: Replace with an abbreviated list of authors.
\authorrunning{A. Shrestha et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.

% TODO FINAL: Replace with your institution list.
\institute{Oregon State University \and
Intel Labs \and
NVIDIA\\
\blfootnote{$\ast$ Equal contribution\\
$\dagger$ Work done when working at Intel Labs}
}

\maketitle

\begin{abstract}
This work focuses on generating realistic, physically-based human behaviors from multi-modal inputs, which may only partially specify the desired motion.
For example, the input may come from a VR controller providing arm motion and body velocity, partial key-point animation, computer vision applied to videos, or even higher-level motion goals.  
This requires a versatile low-level humanoid controller that can handle such sparse, under-specified guidance, seamlessly switch between skills, and recover from failures. Current approaches for learning humanoid controllers from demonstration data capture some of these characteristics, but none achieve them all. To this end, we introduce the Masked Humanoid Controller (MHC), a novel approach that applies multi-objective imitation learning on augmented and selectively masked motion demonstrations. The training methodology results in an MHC that exhibits the key capabilities of \textit{catch-up} to out-of-sync input commands, \textit{combining} elements from multiple motion sequences, and \textit{completing} unspecified parts of motions from sparse multimodal input. We demonstrate these key capabilities for an MHC learned over a dataset of 87 diverse skills and showcase different multi-modal use cases, including integration with planning frameworks to highlight MHCâ€™s ability to solve new user-defined tasks without any finetuning. \footnote{
Anonymous project webpage: \url{https://mhc404.github.io/mhc404/}. \\Source code will be released upon acceptance of the paper.}

  \keywords{Humanoid Motion Generation \and Multimodal \and Imitation}
\end{abstract}


\section{Introduction}
\label{sec:intro}

\begin{figure}[tb]
  \centering
  \includegraphics[width=1.0\linewidth]{assets/overview.png}
  \caption{Showcases generated human motions from multi-modal inputs: (A) VR device, (B) joystick controller, (C) video, and (D) text. Our proposed method can generate physically realistic motions from a wide variety of muli-modal directives. 
  }
  \label{fig:hero}
\end{figure}



\begin{figure}[tb]
  \centering
  \includegraphics[width=1.0\linewidth]{assets/fig_1_mhc_v9.png}
  \caption{ Shows generated motions that illustrate the CCC capabilities. From left to right: the generated motion is able to (1) adjust and \emph{catcup} from an out-of-sync pose, (2) imitate a target motion that \emph{combines} upper and lower body sub-segments from different motions, (3) \emph{complete} the motion from under-specified directives as indicated by missing target outlines.
  }
  \label{fig:overview}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{assets/fig_2_mhc_v9.png}
  \caption{Illustrates the architecture and training details of the MHC framework, which consists of a controller and an ensemble of discriminators. Here the controller is trained to follow an augmented set of masked directives derived from the provided MoCap Dataset. The controller gets feedback via Tracking objective and Style Rewards generated by the ensemble of discrimintors. Together they enable a directable policy to generate physically realistic motions capable of catching up, combining primitives, and completing motions from sparse guidance signals.
  }
  \label{fig:block_diagram}
\end{figure}


\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth,clip]{assets/fig_3_mhc_v9b.png}
  \caption{ Highlights the potential applications of the Masked Humanoid Controller (MHC). [Top] The selective masking of target directive allows MHC to represent various modalities of motion data under a single framework. These multi-modal inputs include MoCAP, full or occluded video, joystick, VR controller among others. [Bottom] Selective masking of look-ahead also allows us to treat guidance itself as abstract actions. This allows for straightforward integration with Finite State Machines and Data driven planning to allow for zero-shot motion generation for higher-level tasks.}
  \label{fig:downstream}
\end{figure}

\begin{figure}[tb]
  \centering
  \includegraphics[width=1.0\linewidth]{assets/imitation.png}
  \caption{Illustrates generated motions that relating to key CCC capabilities of MHC. The simulation(left) displays key-frames of humanoid following different motion directives(right). (A) follows an imitation target, (B) transitions to catch-up to target directive from a falling-down position, (C) imitates motion directive that combines upper-body and lower-body movements from distinct motion (D) completes the motion using only the 3D positions of the head, hands and feets.
  }
  \label{fig:imitation}
\end{figure}





\begin{table}[t]
\caption{Results for ASE Baseline, MHC, and its Key ablation for Imitation and Catchup and Combine tasks. We ablate style rewards, random pose initialization and data augmentation for Imitation and Catchup and Combine tasks respectively. MHC outperforms ASE baselines for all tasks across Reallusion Dataset $\mathcal{M}_{Train}$ and ASE rollout dataset $\mathcal{M}_{Test}$.}
\label{tab:ccc_results}
\centering
\tiny
\renewcommand{\arraystretch}{2} % Adjusts the row spacing
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c}
\hline
\multirow{3}{*}{Method} & \multicolumn{4}{c|}{Imitation} & \multicolumn{4}{c|}{Catchup} & \multicolumn{4}{c}{Combine} \\
\cline{2-13}
 & \multicolumn{2}{c|}{$\mathcal{M}_{Train}$} & \multicolumn{2}{c|}{$\mathcal{M}_{Test}$} & \multicolumn{2}{c|}{$\mathcal{M}_{Train}$} & \multicolumn{2}{c|}{$\mathcal{M}_{Test}$} & \multicolumn{2}{c|}{$\mathcal{M}_{Train}$} & \multicolumn{2}{c}{$\mathcal{M}_{Test}$} \\
\cline{2-13}
 & \(mpjpe \downarrow \) & \(Suc \uparrow \) & \(mpjpe \downarrow\) & \( \uparrow\) 
 & \(mpjpe \downarrow \) & \(Suc \uparrow \) & \(mpjpe \downarrow\) & \(Suc \uparrow\) & \(mpjpe \downarrow\) & \(Suc \uparrow\) & \(mpjpe \downarrow\) & \(Suc \uparrow\) \\
\hline
ASE \cite{Peng2022ASE} &
123.51 & 0.6 & 100.28 & 0.77 & 
125.96 & 0.55 & 102.5 & 0.7 &  
210.75 & 0.17 & 197.94 & 0.25 \\
MHC (abl) &
552.25 & 0.0 & 555.7 & 0.0& 
66.23 & 0.3 & 87.1 & 0.17 &
103.73 & 0.44 & 69.68 & \textbf{0.78}\\
MHC (Ours) &
\textbf{ 51.05} & \textbf{0.92} & \textbf{56.23} & \textbf{0.97} &
\textbf{59.24} & \textbf{0.89 }& \textbf{63.46} & \textbf{0.98} &
\textbf{95.09} &\textbf{ 0.48} & \textbf{60.95} & \textbf{0.78} \\
\hline
\end{tabular}
\end{table}

% Include the image with the \includegraphics command
\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{assets/mpjpe_jl_channel.png}
\caption{Comparative Analysis of MPJPE Across different channel and joint level masking. (left) shows that MHC can retain its imitation performance across different variants of channel masks in contrast to MHC trained without masking. (Right) shows performance under progressively increasing probability of masking joints (0 to 0.75) in the motion directive. We see that MHC can track the joint masked directives better than the unmasked version of ASE baseline as well as MHC trained without Joint Level (JL) masking.}
\label{fig:underspecification_results}
\end{figure}


\begin{figure}
  \centering
  \includegraphics[width=1.0\linewidth]{assets/dac_fsm_fig_v9b.png}
  \caption{Visualization of case studies on zero-shot motion generation for higher-level tasks. Panel (C) shows key-frames of FSM rollouts for Go-To-Location. FSM 1 (Red) is designed to generate a motion walking towards the goal position while doing right sword slashes and ultimately falling down and recovering on reaching the goal. FSM 1 (Blue) is derived to crouched-walk towards the goal location while facing away from it and swinging the sword along the way. The remaining panels A,B and D depicts Go-To-Location Task under various DAC-MDP Objectives. Panel B compares trajectories optimized for the original environment reward (Green) versus the negative of that reward (Blue). Panel C demonstrates the impact of discount factors - the agent with a small factor (Blue) myopically prefers the immediate sword swing reward over long-term gains, while the higher discount agent (Red) chooses superior cumulative rewards. Finally, Panel D shows how a safety-aware DAC-MDP solved with noisy state transition dynamics (Blue) will avoid unsafe region where sword swings are penalized, unlike solving with the noiseless dynamics(Red). 
  }
  \label{fig:planning}
\end{figure}


\begin{figure}[tb]
  \centering
  \includegraphics[width=1.0\linewidth]{assets/multi-modality.png}
  \caption{Illustrates qualitative results using keyframes for motion generation under multi-modal inputs such as (A) VR headset and controllers, (B) joystick controllers, (C) 3D joint positions derived video and (D) text-to-motion generator. This highlights the versatility of MHC and its potential applications for motion generation with various modalities of noisy, under-specified directives.
  }
  \label{fig:modality}
\end{figure}


% \bibliographystyle{splncs04}
% \bibliography{main}

\end{document}
