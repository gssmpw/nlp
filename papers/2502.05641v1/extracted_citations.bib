@article{Bae2023PMPLT,
  title={PMP: Learning to Physically Interact with Environments using Part-wise Motion Priors},
  author={Jinseok Bae and Jungdam Won and Donggeun Lim and Cheol-Hui Min and Y. Kim},
  journal={ACM SIGGRAPH 2023 Conference Proceedings},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:258547073}
}

@article{Cern2023ANM,
  title={A Novel Multi-Modal Teleoperation of a Humanoid Assistive Robot with Real-Time Motion Mimic},
  author={Julio C. Cer{\'o}n and Md. Samiul Haque Sunny and Brahim Brahmi and Luis Miguel Mendez and Raouf Fareh and Helal Uddin Ahmed and Mohammad Habibur Rahman},
  journal={Micromachines},
  year={2023},
  volume={14},
  url={https://api.semanticscholar.org/CorpusID:256964199}
}

@article{Chen2022ExecutingYC,
  title={Executing your Commands via Motion Diffusion in Latent Space},
  author={Xin Chen and Biao Jiang and Wen Liu and Zilong Huang and Bin Fu and Tao Chen and Jingyi Yu and Gang Yu},
  journal={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2022},
  pages={18000-18010},
  url={https://api.semanticscholar.org/CorpusID:254408910}
}

@article{Chentanez2018PhysicsbasedMC,
  title={Physics-based motion capture imitation with deep reinforcement learning},
  author={Nuttapong Chentanez and Matthias M{\"u}ller and Miles Macklin and Viktor Makoviychuk and Stefan Jeschke},
  journal={Proceedings of the 11th ACM SIGGRAPH Conference on Motion, Interaction and Games},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:53237567}
}

@article{Dou2023CASELC,
  title={CÂ·ASE: Learning Conditional Adversarial Skill Embeddings for Physics-based Characters},
  author={Zhiyang Dou and Xuelin Chen and Qingnan Fan and Taku Komura and Wenping Wang},
  journal={ArXiv},
  year={2023},
  volume={abs/2309.11351},
  url={https://api.semanticscholar.org/CorpusID:262064161}
}

@article{Du2023AvatarsGL,
  title={Avatars Grow Legs: Generating Smooth Human Motion from Sparse Tracking Inputs with Diffusion Model},
  author={Yuming Du and Robin Kips and Albert Pumarola and Sebastian Starke and Ali K. Thabet and Artsiom Sanakoyeu},
  journal={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2023},
  pages={481-490},
  url={https://api.semanticscholar.org/CorpusID:258187221}
}

@inproceedings{Ho2016GenerativeAI,
  title={Generative Adversarial Imitation Learning},
  author={Jonathan Ho and Stefano Ermon},
  booktitle={Neural Information Processing Systems},
  year={2016},
  url={https://api.semanticscholar.org/CorpusID:16153365}
}

@article{Huang2022PhysicallyPA,
  title={Physically Plausible Animation of Human Upper Body from a Single Image},
  author={Ziyuan Huang and Zhengping Zhou and Yung-Yu Chuang and Jiajun Wu and C. Karen Liu},
  journal={2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
  year={2022},
  pages={930-939},
  url={https://api.semanticscholar.org/CorpusID:254535979}
}

@article{Juravsky2022PADLLP,
  title={PADL: Language-Directed Physics-Based Character Control},
  author={Jordan Juravsky and Yunrong Guo and Sanja Fidler and Xue Bin Peng},
  journal={SIGGRAPH Asia 2022 Conference Papers},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:254070640}
}

@article{Lee2022HumaninspiredVI,
  title={Human-inspired Video Imitation Learning on Humanoid Model},
  author={Chun Hei Lee and Nicole Chee Lin Yueh and Kam Tim Woo},
  journal={2022 Sixth IEEE International Conference on Robotic Computing (IRC)},
  year={2022},
  pages={345-352},
  url={https://api.semanticscholar.org/CorpusID:256243027}
}

@article{Lee2022LearningVC,
  title={Learning Virtual Chimeras by Dynamic Motion Reassembly},
  author={Seyoung Lee and Jiye Lee and Jehee Lee},
  journal={ACM Transactions on Graphics (TOG)},
  year={2022},
  volume={41},
  pages={1 - 13},
  url={https://api.semanticscholar.org/CorpusID:254097260}
}

@article{Luo2023UniversalHM,
  title={Universal Humanoid Motion Representations for Physics-Based Control},
  author={Zhen-Ge Luo and Jinkun Cao and Josh Merel and Alexander Winkler and Jing Huang and Kris Kitani and Weipeng Xu},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.04582},
  url={https://api.semanticscholar.org/CorpusID:263829555}
}

@article{Merel2017LearningHB,
  title={Learning human behaviors from motion capture by adversarial imitation},
  author={Josh Merel and Yuval Tassa and TB Dhruva and Sriram Srinivasan and Jay Lemmon and Ziyun Wang and Greg Wayne and Nicolas Manfred Otto Heess},
  journal={ArXiv},
  year={2017},
  volume={abs/1707.02201},
  url={https://api.semanticscholar.org/CorpusID:40186317}
}

@article{Merel2018NeuralPM,
  title={Neural probabilistic motor primitives for humanoid control},
  author={Josh Merel and Leonard Hasenclever and Alexandre Galashov and Arun Ahuja and Vu Pham and Greg Wayne and Yee Whye Teh and Nicolas Manfred Otto Heess},
  journal={ArXiv},
  year={2018},
  volume={abs/1811.11711},
  url={https://api.semanticscholar.org/CorpusID:53831933}
}

@article{Peng2022ASE,
  title={Ase: Large-scale reusable adversarial skill embeddings for physically simulated characters},
  author={Peng, Xue Bin and Guo, Yunrong and Halper, Lina and Levine, Sergey and Fidler, Sanja},
  journal={ACM Transactions On Graphics (TOG)},
  volume={41},
  number={4},
  pages={1--17},
  year={2022},
  publisher={ACM New York, NY, USA}
}

@article{Shrestha2020DeepAveragersOR,
  title={DeepAveragers: Offline Reinforcement Learning by Solving Derived Non-Parametric MDPs},
  author={Aayam Shrestha and Stefan Lee and Prasad Tadepalli and Alan Fern},
  journal={ArXiv},
  year={2020},
  volume={abs/2010.08891},
  url={https://api.semanticscholar.org/CorpusID:224703455}
}

@article{Sun2023PromptPP,
  title={Prompt, Plan, Perform: LLM-based Humanoid Control via Quantized Imitation Learning},
  author={Jingkai Sun and Qiang Zhang and Yiqun Duan and Xiaoyang Jiang and Chong Cheng and Renjing Xu},
  journal={ArXiv},
  year={2023},
  volume={abs/2309.11359},
  url={https://api.semanticscholar.org/CorpusID:261974876}
}

@article{Tessler2023CALMCA,
  title={CALM: Conditional Adversarial Latent Models  for Directable Virtual Characters},
  author={Chen Tessler and Yoni Kasten and Yunrong Guo and Shie Mannor and Gal Chechik and Xue Bin Peng},
  journal={ACM SIGGRAPH 2023 Conference Proceedings},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:258461220}
}

@article{Wang2020SynthesizingL3,
  title={Synthesizing Long-Term 3D Human Motion and Interaction in 3D Scenes},
  author={Jiashun Wang and Huazhe Xu and Jingwei Xu and Sifei Liu and X. Wang},
  journal={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020},
  pages={9396-9406},
  url={https://api.semanticscholar.org/CorpusID:228083961}
}

@article{Won2020ASA,
  title={A scalable approach to control diverse behaviors for physically simulated characters},
  author={Jungdam Won and Deepak Edakkattil Gopinath and Jessica K. Hodgins},
  journal={ACM Transactions on Graphics (TOG)},
  year={2020},
  volume={39},
  pages={33:1 - 33:12},
  url={https://api.semanticscholar.org/CorpusID:219569865}
}

@article{Won2022PhysicsbasedCC,
  title={Physics-based character controllers using conditional VAEs},
  author={Jungdam Won and Deepak Edakkattil Gopinath and Jessica K. Hodgins},
  journal={ACM Transactions on Graphics (TOG)},
  year={2022},
  volume={41},
  pages={1 - 12},
  url={https://api.semanticscholar.org/CorpusID:250956798}
}

@article{Xu2021AGA,
  title={A GAN-Like Approach for Physics-Based Imitation Learning and Interactive Control},
  author={Pei Xu and Ioannis Karamouzas},
  journal={Proceedings of the ACM on Computer Graphics and Interactive Techniques},
  year={2021},
  volume={4},
  pages={1 - 22},
  url={https://api.semanticscholar.org/CorpusID:235125922}
}

@article{Xu2023CompositeML,
  title={Composite Motion Learning with Task Control},
  author={Pei Xu and Xiumin Shang and Victor B. Zordan and Ioannis Karamouzas},
  journal={ACM Transactions on Graphics (TOG)},
  year={2023},
  volume={42},
  pages={1 - 16},
  url={https://api.semanticscholar.org/CorpusID:258547111}
}

@article{Yao2023MoConVQUP,
  title={MoConVQ: Unified Physics-Based Motion Control via Scalable Discrete Representations},
  author={Heyuan Yao and Zhenhua Song and Yuyang Zhou and Tenglong Ao and Baoquan Chen and Libin Liu},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.10198},
  url={https://api.semanticscholar.org/CorpusID:264146020}
}

@misc{luo_perpetual_2023,
	title = {Perpetual Humanoid Control for Real-time Simulated Avatars},
	url = {http://arxiv.org/abs/2305.06456},
	doi = {10.48550/arXiv.2305.06456},
	abstract = {We present a physics-based humanoid controller that achieves high-fidelity motion imitation and fault-tolerant behavior in the presence of noisy input (e.g. pose estimates from video or generated from language) and unexpected falls. Our controller scales up to learning ten thousand motion clips without using any external stabilizing forces and learns to naturally recover from fail-state. Given reference motion, our controller can perpetually control simulated avatars without requiring resets. At its core, we propose the progressive multiplicative control policy ({PMCP}), which dynamically allocates new network capacity to learn harder and harder motion sequences. {PMCP} allows efficient scaling for learning from large-scale motion databases and adding new tasks, such as fail-state recovery, without catastrophic forgetting. We demonstrate the effectiveness of our controller by using it to imitate noisy poses from video-based pose estimators and language-based motion generators in a live and real-time multi-person avatar use case.},
	number = {{arXiv}:2305.06456},
	publisher = {{arXiv}},
	author = {Luo, Zhengyi and Cao, Jinkun and Winkler, Alexander and Kitani, Kris and Xu, Weipeng},
	urldate = {2023-08-14},
	date = {2023-05-24},
	eprinttype = {arxiv},
	eprint = {2305.06456 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/Users/bobby/Zotero/storage/U2N4WJ4X/Luo et al. - 2023 - Perpetual Humanoid Control for Real-time Simulated.pdf:application/pdf;arXiv.org Snapshot:/Users/bobby/Zotero/storage/VMNYWLF5/2305.html:text/html},
}

@misc{merel_catch_2020,
	title = {Catch \& Carry: Reusable Neural Controllers for Vision-Guided Whole-Body Tasks},
	url = {http://arxiv.org/abs/1911.06636},
	shorttitle = {Catch \& Carry},
	abstract = {We address the longstanding challenge of producing flexible, realistic humanoid character controllers that can perform diverse whole-body tasks involving object interactions. This challenge is central to a variety of fields, from graphics and animation to robotics and motor neuroscience. Our physics-based environment uses realistic actuation and first-person perception -- including touch sensors and egocentric vision -- with a view to producing active-sensing behaviors (e.g. gaze direction), transferability to real robots, and comparisons to the biology. We develop an integrated neural-network based approach consisting of a motor primitive module, human demonstrations, and an instructed reinforcement learning regime with curricula and task variations. We demonstrate the utility of our approach for several tasks, including goal-conditioned box carrying and ball catching, and we characterize its behavioral robustness. The resulting controllers can be deployed in real-time on a standard {PC}. See overview video, https://youtu.be/2rQAW-8gQQk .},
	number = {{arXiv}:1911.06636},
	publisher = {{arXiv}},
	author = {Merel, Josh and Tunyasuvunakool, Saran and Ahuja, Arun and Tassa, Yuval and Hasenclever, Leonard and Pham, Vu and Erez, Tom and Wayne, Greg and Heess, Nicolas},
	urldate = {2023-03-20},
	date = {2020-06-16},
	eprinttype = {arxiv},
	eprint = {1911.06636 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/Users/bobby/Zotero/storage/UU4P8SED/Merel et al. - 2020 - Catch & Carry Reusable Neural Controllers for Vis.pdf:application/pdf;arXiv.org Snapshot:/Users/bobby/Zotero/storage/M8849959/1911.html:text/html},
}

@article{peng_amp_2021,
  title={Amp: Adversarial motion priors for stylized physics-based character control},
  author={Peng, Xue Bin and Ma, Ze and Abbeel, Pieter and Levine, Sergey and Kanazawa, Angjoo},
  journal={ACM Transactions on Graphics (ToG)},
  volume={40},
  number={4},
  pages={1--20},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{peng_deepmimic_2018,
  title={Deepmimic: Example-guided deep reinforcement learning of physics-based character skills},
  author={Peng, Xue Bin and Abbeel, Pieter and Levine, Sergey and Van de Panne, Michiel},
  journal={ACM Transactions On Graphics (TOG)},
  volume={37},
  number={4},
  pages={1--14},
  year={2018},
  publisher={ACM New York, NY, USA}
}

@misc{ren_diffmimic_2023,
	title = {{DiffMimic}: Efficient Motion Mimicking with Differentiable Physics},
	url = {http://arxiv.org/abs/2304.03274},
	shorttitle = {{DiffMimic}},
	abstract = {Motion mimicking is a foundational task in physics-based character animation. However, most existing motion mimicking methods are built upon reinforcement learning ({RL}) and suffer from heavy reward engineering, high variance, and slow convergence with hard explorations. Specifically, they usually take tens of hours or even days of training to mimic a simple motion sequence, resulting in poor scalability. In this work, we leverage differentiable physics simulators ({DPS}) and propose an efficient motion mimicking method dubbed {DiffMimic}. Our key insight is that {DPS} casts a complex policy learning task to a much simpler state matching problem. In particular, {DPS} learns a stable policy by analytical gradients with ground-truth physical priors hence leading to significantly faster and stabler convergence than {RL}-based methods. Moreover, to escape from local optima, we utilize a Demonstration Replay mechanism to enable stable gradient backpropagation in a long horizon. Extensive experiments on standard benchmarks show that {DiffMimic} has a better sample efficiency and time efficiency than existing methods (e.g., {DeepMimic}). Notably, {DiffMimic} allows a physically simulated character to learn Backflip after 10 minutes of training and be able to cycle it after 3 hours of training, while the existing approach may require about a day of training to cycle Backflip. More importantly, we hope {DiffMimic} can benefit more differentiable animation systems with techniques like differentiable clothes simulation in future research.},
	number = {{arXiv}:2304.03274},
	publisher = {{arXiv}},
	author = {Ren, Jiawei and Yu, Cunjun and Chen, Siwei and Ma, Xiao and Pan, Liang and Liu, Ziwei},
	urldate = {2023-04-25},
	date = {2023-04-06},
	eprinttype = {arxiv},
	eprint = {2304.03274 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bobby/Zotero/storage/SHDIELAP/2304.html:text/html;Full Text PDF:/Users/bobby/Zotero/storage/GPJQBFR3/Ren et al. - 2023 - DiffMimic Efficient Motion Mimicking with Differe.pdf:application/pdf},
}

@misc{wagener_mocapact_2023,
	title = {{MoCapAct}: A Multi-Task Dataset for Simulated Humanoid Control},
	url = {http://arxiv.org/abs/2208.07363},
	doi = {10.48550/arXiv.2208.07363},
	shorttitle = {{MoCapAct}},
	abstract = {Simulated humanoids are an appealing research domain due to their physical capabilities. Nonetheless, they are also challenging to control, as a policy must drive an unstable, discontinuous, and high-dimensional physical system. One widely studied approach is to utilize motion capture ({MoCap}) data to teach the humanoid agent low-level skills (e.g., standing, walking, and running) that can then be re-used to synthesize high-level behaviors. However, even with {MoCap} data, controlling simulated humanoids remains very hard, as {MoCap} data offers only kinematic information. Finding physical control inputs to realize the demonstrated motions requires computationally intensive methods like reinforcement learning. Thus, despite the publicly available {MoCap} data, its utility has been limited to institutions with large-scale compute. In this work, we dramatically lower the barrier for productive research on this topic by training and releasing high-quality agents that can track over three hours of {MoCap} data for a simulated humanoid in the dm\_control physics-based environment. We release {MoCapAct} (Motion Capture with Actions), a dataset of these expert agents and their rollouts, which contain proprioceptive observations and actions. We demonstrate the utility of {MoCapAct} by using it to train a single hierarchical policy capable of tracking the entire {MoCap} dataset within dm\_control and show the learned low-level component can be re-used to efficiently learn downstream high-level tasks. Finally, we use {MoCapAct} to train an autoregressive {GPT} model and show that it can control a simulated humanoid to perform natural motion completion given a motion prompt. Videos of the results and links to the code and dataset are available at https://microsoft.github.io/{MoCapAct}.},
	number = {{arXiv}:2208.07363},
	publisher = {{arXiv}},
	author = {Wagener, Nolan and Kolobov, Andrey and Frujeri, Felipe Vieira and Loynd, Ricky and Cheng, Ching-An and Hausknecht, Matthew},
	urldate = {2023-03-18},
	date = {2023-01-13},
	eprinttype = {arxiv},
	eprint = {2208.07363 [cs, eess]},
	keywords = {Computer Science - Graphics, Computer Science - Machine Learning, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
	file = {arXiv Fulltext PDF:/Users/bobby/Zotero/storage/NISW4846/Wagener et al. - 2023 - MoCapAct A Multi-Task Dataset for Simulated Human.pdf:application/pdf;arXiv.org Snapshot:/Users/bobby/Zotero/storage/WBWZLKED/2208.html:text/html},
}

