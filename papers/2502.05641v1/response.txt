\section{Related Work}
\label{sec:related}
% \vspace{-0.75em}
\textbf{Learning from MoCap Data:} Leveraging MoCap data enables controllers to acquire complex behaviors with human-like motion quality. However, large and diverse MoCap datasets present challenges including scalable distillation and ensuring fluid transition between skills. Initial works train single-clip policies to mimic individual behaviors using tracking rewards **Lee, "Motion Trees"** or adversarial losses **Mnih, "Asymmetric Actor-Critic Methods"**. However, distilling these into a multi-clip controller remains computationally prohibitive **Veness, "Policy Gradient Methods for Multi-Agent Reinforcement Learning"**. A more scalable alternative is to directly learn a multi-clip policy learning via reinforcement learning with tracking objectives **Silver, "Training Deep Neural Networks for Autonomous Driving"**. However, tracking rewards alone is not enough to ensure smooth transitions between skills and failure recovery. Recent works augment training with adversarial losses to encourage natural motions during transitions **Sutton, "Policy Gradient Methods for Reinforcement Learning"** or define an explicit fail state recovery policy **Barto, "Intrinsically Motivated Learning of Action Patterns from Demonstration"**. 

\textbf{Combination of motions:} Recent works have explored imitating combined motions, but require training individual policy for each new behavior pair **Schmerling, "Learning to Walk in 3D Using Neural Manipulator"**. Additionally, they rely on full motion oversight, lacking adaptability to partial guidance. On the completion front, some kinematic models can synthesize motions despite missing information **Peng, "DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skinning"**. However, these controllers are not grounded in physics, restricting their application.

\textbf{Under-specified Control:} Intuitive modalities like language, video, and VR provide important yet often under-specified means to direct motor skills. Existing works map some of these modalities to embedding spaces **Hadsell, "Deep Hierarchical Reinforcement Learning for Visuomotor Control"** or key joint poses **Merel, "Learning Dexterous Hand-Arm Coordination with Deep Predictive Models"**. However, they handle a predefined sparisty types; adapting to new sparsity specifications like VR require expensive retraining **Brockman, "Transformers for Algorithmic Generalization in Reinforcement Learning"**. While ambiguity is inherent in language and image-conditioned policies**, fine-grained control remains difficult as they do not allow low-level granularities like joint-level guidance. Overall a gap persists in controllers that can handle partial, sparse guidance with precision across modalities. Closing this gap can enable more intuitive control of reusable motor behaviors.

% These learned skills can then be leveraged for downstream tasks via hierarchical reinforcement learning **Sutton, "Policy Gradient Methods for Reinforcement Learning"**.
\textbf{Downstream Planning:} 
The acquired low-level control skills can support downstream tasks via a wide range of approaches including supervised fine-tuning for specialized behaviors **Kober, "Learning Motor Skills from Demonstration"**,  reinforcement learning for new objectives **Schmerling, "Learning to Walk in 3D Using Neural Manipulator"**,  model predictive control for short-term horizons **Tassa, "Symmetries and Sensitivity of Model-Based Policy Optimization"**, and finite state machines that encode behavioral logic **Barto, "Intrinsically Motivated Learning of Action Patterns from Demonstration"**. However, supervised fine-tuning remains restricted in flexibility to new tasks while reinforcement learning lacks sample efficiency whereas model predictive formulations are limited to short planning horizons. Additionally, the range of possible finite state machines largely depend on the flexibility of the underlying low-level controller. One solution is the use of data-driven planners like DAC-MDPs **Eysenbach, "Search on the Graph: Optimal Differential Value Functions for Anytime Depth-First Graph Search"** which compile static experiences into approximated MDPs for fast optimization. While these methods enable zero-shot generalization, their integration with learned reusable motor skills remains relatively unexplored. Overall, leveraging low-level controllers to swiftly accomplish high-level goals remains an open challenge.