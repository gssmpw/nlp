
@article{Yuan2020ResidualFC,
  title={Residual Force Control for Agile Human Behavior Imitation and Extended Motion Synthesis},
  author={Ye Yuan and Kris Kitani},
  journal={ArXiv},
  year={2020},
  volume={abs/2006.07364},
  url={https://api.semanticscholar.org/CorpusID:219636238}
}

@article{Park2019LearningPP,
  title={Learning predict-and-simulate policies from unorganized human motion data},
  author={Soohwan Park and Hoseok Ryu and Seyoung Lee and Sunmin Lee and Jehee Lee},
  journal={ACM Transactions on Graphics (TOG)},
  year={2019},
  volume={38},
  pages={1 - 11},
  url={https://api.semanticscholar.org/CorpusID:207997808}
}

@article{Mahmood2019AMASSAO,
  title={AMASS: Archive of Motion Capture As Surface Shapes},
  author={Naureen Mahmood and Nima Ghorbani and Nikolaus F. Troje and Gerard Pons-Moll and Michael J. Black},
  journal={2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2019},
  pages={5441-5450},
  url={https://api.semanticscholar.org/CorpusID:102351100}
}


@article{Reallusion2022,
  title={3D Animation and 2D Cartoons Made Simple. },
  author={Reallusion},
  url={http://www.reallusion.com}
}

@article{Shrestha2020DeepAveragersOR,
  title={DeepAveragers: Offline Reinforcement Learning by Solving Derived Non-Parametric MDPs},
  author={Aayam Shrestha and Stefan Lee and Prasad Tadepalli and Alan Fern},
  journal={ArXiv},
  year={2020},
  volume={abs/2010.08891},
  url={https://api.semanticscholar.org/CorpusID:224703455}
}

@article{Yuan2021SimPoESC,
  title={SimPoE: Simulated Character Control for 3D Human Pose Estimation},
  author={Ye Yuan and Shih-En Wei and Tomas Simon and Kris Kitani and Jason M. Saragih},
  journal={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021},
  pages={7155-7165},
  url={https://api.semanticscholar.org/CorpusID:232478309}
}


@misc{ren_diffmimic_2023,
	title = {{DiffMimic}: Efficient Motion Mimicking with Differentiable Physics},
	url = {http://arxiv.org/abs/2304.03274},
	shorttitle = {{DiffMimic}},
	abstract = {Motion mimicking is a foundational task in physics-based character animation. However, most existing motion mimicking methods are built upon reinforcement learning ({RL}) and suffer from heavy reward engineering, high variance, and slow convergence with hard explorations. Specifically, they usually take tens of hours or even days of training to mimic a simple motion sequence, resulting in poor scalability. In this work, we leverage differentiable physics simulators ({DPS}) and propose an efficient motion mimicking method dubbed {DiffMimic}. Our key insight is that {DPS} casts a complex policy learning task to a much simpler state matching problem. In particular, {DPS} learns a stable policy by analytical gradients with ground-truth physical priors hence leading to significantly faster and stabler convergence than {RL}-based methods. Moreover, to escape from local optima, we utilize a Demonstration Replay mechanism to enable stable gradient backpropagation in a long horizon. Extensive experiments on standard benchmarks show that {DiffMimic} has a better sample efficiency and time efficiency than existing methods (e.g., {DeepMimic}). Notably, {DiffMimic} allows a physically simulated character to learn Backflip after 10 minutes of training and be able to cycle it after 3 hours of training, while the existing approach may require about a day of training to cycle Backflip. More importantly, we hope {DiffMimic} can benefit more differentiable animation systems with techniques like differentiable clothes simulation in future research.},
	number = {{arXiv}:2304.03274},
	publisher = {{arXiv}},
	author = {Ren, Jiawei and Yu, Cunjun and Chen, Siwei and Ma, Xiao and Pan, Liang and Liu, Ziwei},
	urldate = {2023-04-25},
	date = {2023-04-06},
	eprinttype = {arxiv},
	eprint = {2304.03274 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bobby/Zotero/storage/SHDIELAP/2304.html:text/html;Full Text PDF:/Users/bobby/Zotero/storage/GPJQBFR3/Ren et al. - 2023 - DiffMimic Efficient Motion Mimicking with Differe.pdf:application/pdf},
}

@article{Merel2018NeuralPM,
  title={Neural probabilistic motor primitives for humanoid control},
  author={Josh Merel and Leonard Hasenclever and Alexandre Galashov and Arun Ahuja and Vu Pham and Greg Wayne and Yee Whye Teh and Nicolas Manfred Otto Heess},
  journal={ArXiv},
  year={2018},
  volume={abs/1811.11711},
  url={https://api.semanticscholar.org/CorpusID:53831933}
}

@article{Merel2018HierarchicalVC,
  title={Hierarchical visuomotor control of humanoids},
  author={Josh Merel and Arun Ahuja and Vu Pham and Saran Tunyasuvunakool and Siqi Liu and Dhruva Tirumala and Nicolas Manfred Otto Heess and Greg Wayne},
  journal={ArXiv},
  year={2018},
  volume={abs/1811.09656},
  url={https://api.semanticscholar.org/CorpusID:53792719}
}

@article{Yu2022MaskbasedLR,
  title={Mask-based Latent Reconstruction for Reinforcement Learning},
  author={Tao Yu and Zhizheng Zhang and Cuiling Lan and Zhibo Chen and Yan Lu},
  journal={ArXiv},
  year={2022},
  volume={abs/2201.12096},
  url={https://api.semanticscholar.org/CorpusID:246411446},
  abstract = {This work proposes a simple yet effective self-supervised method, Mask-based Latent Reconstruction (MLR), to predict complete state representations in the latent space from the observations with spatially and temporally masked pixels, and significantly improves the sample efficiency in RL and outperforms the state-of-the-art sample-efficient RL methods on multiple continuous and discrete control benchmarks.}
}

@article{Zhu2020MaskedCR,
  title={Masked Contrastive Representation Learning for Reinforcement Learning},
  author={Jinhua Zhu and Yingce Xia and Lijun Wu and Jiajun Deng and Wen-gang Zhou and Tao Qin and Houqiang Li},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2020},
  volume={45},
  pages={3421-3433},
  url={https://api.semanticscholar.org/CorpusID:222377666}
}
@article{Yin2007SIMBICONSB,
  title={SIMBICON: simple biped locomotion control},
  author={KangKang Yin and Kevin Loken and Michiel van de Panne},
  journal={ACM SIGGRAPH 2007 papers},
  year={2007},
  url={https://api.semanticscholar.org/CorpusID:13954731}
}

@article{Bachmann2022MultiMAEMM,
  title={MultiMAE: Multi-modal Multi-task Masked Autoencoders},
  author={Roman Bachmann and David Mizrahi and Andrei Atanov and Amir Roshan Zamir},
  journal={ArXiv},
  year={2022},
  volume={abs/2204.01678},
  url={https://api.semanticscholar.org/CorpusID:247939918}
}

@article{Chen2022ExecutingYC,
  title={Executing your Commands via Motion Diffusion in Latent Space},
  author={Xin Chen and Biao Jiang and Wen Liu and Zilong Huang and Bin Fu and Tao Chen and Jingyi Yu and Gang Yu},
  journal={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2022},
  pages={18000-18010},
  url={https://api.semanticscholar.org/CorpusID:254408910}
}

@article{Wang2020SynthesizingL3,
  title={Synthesizing Long-Term 3D Human Motion and Interaction in 3D Scenes},
  author={Jiashun Wang and Huazhe Xu and Jingwei Xu and Sifei Liu and X. Wang},
  journal={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020},
  pages={9396-9406},
  url={https://api.semanticscholar.org/CorpusID:228083961}
}

@article{Reza2023RobustML,
  title={Robust Multimodal Learning with Missing Modalities via Parameter-Efficient Adaptation},
  author={Md Kaykobad Reza and Ashley Prater-Bennette and M. Salman Asif},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.03986},
  url={https://api.semanticscholar.org/CorpusID:263828735}
}

@article{Maheshwari2023MissingMR,
  title={Missing Modality Robustness in Semi-Supervised Multi-Modal Semantic Segmentation},
  author={Harsh Maheshwari and Yen-Cheng Liu and Zsolt Kira},
  journal={ArXiv},
  year={2023},
  volume={abs/2304.10756},
  url={https://api.semanticscholar.org/CorpusID:258291773}
}

@inproceedings{Song2023MA2CLMA,
  title={MA2CL: Masked Attentive Contrastive Learning for Multi-Agent Reinforcement Learning},
  author={Haolin Song and Ming Feng and Wen-gang Zhou and Houqiang Li},
  booktitle={International Joint Conference on Artificial Intelligence},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:259076469}
}

@inproceedings{Ho2016GenerativeAI,
  title={Generative Adversarial Imitation Learning},
  author={Jonathan Ho and Stefano Ermon},
  booktitle={Neural Information Processing Systems},
  year={2016},
  url={https://api.semanticscholar.org/CorpusID:16153365}
}

@article{Lee2022HumaninspiredVI,
  title={Human-inspired Video Imitation Learning on Humanoid Model},
  author={Chun Hei Lee and Nicole Chee Lin Yueh and Kam Tim Woo},
  journal={2022 Sixth IEEE International Conference on Robotic Computing (IRC)},
  year={2022},
  pages={345-352},
  url={https://api.semanticscholar.org/CorpusID:256243027}
}

@article{peng_amp_2021,
  title={Amp: Adversarial motion priors for stylized physics-based character control},
  author={Peng, Xue Bin and Ma, Ze and Abbeel, Pieter and Levine, Sergey and Kanazawa, Angjoo},
  journal={ACM Transactions on Graphics (ToG)},
  volume={40},
  number={4},
  pages={1--20},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{Peng2019MCPLC,
  title={MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies},
  author={Xue Bin Peng and Michael Chang and Grace Zhang and P. Abbeel and Sergey Levine},
  journal={ArXiv},
  year={2019},
  volume={abs/1905.09808},
  url={https://api.semanticscholar.org/CorpusID:162184474}
}


@article{Peng2018SFVRL,
  title={SFV: Reinforcement Learning of Physical Skills from Videos},
  author={Xue Bin Peng and Angjoo Kanazawa and Jitendra Malik and P. Abbeel and Sergey Levine},
  journal={ACM Trans. Graph.},
  year={2018},
  volume={37},
  pages={178},
  url={https://api.semanticscholar.org/CorpusID:52937281}
}

@article{Todorov2012MuJoCoAP,
  title={MuJoCo: A physics engine for model-based control},
  author={Emanuel Todorov and Tom Erez and Yuval Tassa},
  journal={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  year={2012},
  pages={5026-5033},
  url={https://api.semanticscholar.org/CorpusID:5230692}
}

@article{Makoviychuk2021IsaacGH,
  title={Isaac Gym: High Performance GPU-Based Physics Simulation For Robot Learning},
  author={Viktor Makoviychuk and Lukasz Wawrzyniak and Yunrong Guo and Michelle Lu and Kier Storey and Miles Macklin and David Hoeller and N. Rudin and Arthur Allshire and Ankur Handa and Gavriel State},
  journal={ArXiv},
  year={2021},
  volume={abs/2108.10470},
  url={https://api.semanticscholar.org/CorpusID:237277983}
}

@article{Bae2023PMPLT,
  title={PMP: Learning to Physically Interact with Environments using Part-wise Motion Priors},
  author={Jinseok Bae and Jungdam Won and Donggeun Lim and Cheol-Hui Min and Y. Kim},
  journal={ACM SIGGRAPH 2023 Conference Proceedings},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:258547073}
}

@article{Petrovich2021ActionConditioned3H,
  title={Action-Conditioned 3D Human Motion Synthesis with Transformer VAE},
  author={Mathis Petrovich and Michael J. Black and G{\"u}l Varol},
  journal={2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2021},
  pages={10965-10975},
  url={https://api.semanticscholar.org/CorpusID:233210075}
}


@article{Zhao2020BayesianAH,
  title={Bayesian Adversarial Human Motion Synthesis},
  author={Rui Zhao and Hui Su and Qiang Ji},
  journal={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020},
  pages={6224-6233},
  url={https://api.semanticscholar.org/CorpusID:219964627}
}


@inproceedings{hasenclever_comic_2020,
	title = {{CoMic}: Complementary Task Learning \& Mimicry for Reusable Skills},
	url = {https://proceedings.mlr.press/v119/hasenclever20a.html},
	shorttitle = {{CoMic}},
	abstract = {Learning to control complex bodies and reuse learned behaviors is a longstanding challenge in continuous control. We study the problem of learning reusable humanoid skills by imitating motion capture data and joint training with complementary tasks. We show that it is possible to learn reusable skills through reinforcement learning on 50 times more motion capture data than prior work. We systematically compare a variety of different network architectures across different data regimes both in terms of imitation performance as well as transfer to challenging locomotion tasks. Finally we show that it is possible to interleave the motion capture tracking with training on complementary tasks, enriching the resulting skill space, and enabling the reuse of skills not well covered by the motion capture data such as getting up from the ground or catching a ball.},
	eventtitle = {International Conference on Machine Learning},
	pages = {4105--4115},
	booktitle = {Proceedings of the 37th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Hasenclever, Leonard and Pardo, Fabio and Hadsell, Raia and Heess, Nicolas and Merel, Josh},
	urldate = {2023-03-20},
	date = {2020-11-21},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	file = {Full Text PDF:/Users/bobby/Zotero/storage/MAIINSB9/Hasenclever et al. - 2020 - CoMic Complementary Task Learning & Mimicry for R.pdf:application/pdf;Supplementary PDF:/Users/bobby/Zotero/storage/6QZFSVEJ/Hasenclever et al. - 2020 - CoMic Complementary Task Learning & Mimicry for R.pdf:application/pdf},
}

@article{Shi2023ControllableMD,
  title={Controllable Motion Diffusion Model},
  author={Yi Shi and Jingbo Wang and Xuekun Jiang and Bo Dai},
  journal={ArXiv},
  year={2023},
  volume={abs/2306.00416},
  url={https://api.semanticscholar.org/CorpusID:258999716}
}

@article{Xu2023CompositeML,
  title={Composite Motion Learning with Task Control},
  author={Pei Xu and Xiumin Shang and Victor B. Zordan and Ioannis Karamouzas},
  journal={ACM Transactions on Graphics (TOG)},
  year={2023},
  volume={42},
  pages={1 - 16},
  url={https://api.semanticscholar.org/CorpusID:258547111}
}


@article{Tevet2022HumanMD,
  title={Human Motion Diffusion Model},
  author={Guy Tevet and Sigal Raab and Brian Gordon and Yonatan Shafir and Daniel Cohen-Or and Amit H. Bermano},
  journal={ArXiv},
  year={2022},
  volume={abs/2209.14916},
  url={https://api.semanticscholar.org/CorpusID:252595883}
}

@article{Coros2010GeneralizedBW,
  title={Generalized biped walking control},
  author={Stelian Coros and Philippe Beaudoin and Michiel van de Panne},
  journal={ACM SIGGRAPH 2010 papers},
  year={2010},
  url={https://api.semanticscholar.org/CorpusID:17559393}
}

@article{Sridhar2023NoMaDGM,
  title={NoMaD: Goal Masked Diffusion Policies for Navigation and Exploration},
  author={Ajay Kumar Sridhar and Dhruv Shah and Catherine Glossop and Sergey Levine},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.07896},
  url={https://api.semanticscholar.org/CorpusID:263909572}
}


@article{peng_deepmimic_2018,
  title={Deepmimic: Example-guided deep reinforcement learning of physics-based character skills},
  author={Peng, Xue Bin and Abbeel, Pieter and Levine, Sergey and Van de Panne, Michiel},
  journal={ACM Transactions On Graphics (TOG)},
  volume={37},
  number={4},
  pages={1--14},
  year={2018},
  publisher={ACM New York, NY, USA}
}



@article{Du2023AvatarsGL,
  title={Avatars Grow Legs: Generating Smooth Human Motion from Sparse Tracking Inputs with Diffusion Model},
  author={Yuming Du and Robin Kips and Albert Pumarola and Sebastian Starke and Ali K. Thabet and Artsiom Sanakoyeu},
  journal={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2023},
  pages={481-490},
  url={https://api.semanticscholar.org/CorpusID:258187221}
}

@article{Zhu2023NeuralCP,
  title={Neural Categorical Priors for Physics-Based Character Control},
  author={Qing Zhu and He Zhang and Mengting Lan and Lei Han},
  journal={ArXiv},
  year={2023},
  volume={abs/2308.07200},
  url={https://api.semanticscholar.org/CorpusID:260886980}
}

@article{Winkler2022QuestSimHM,
  title={QuestSim: Human Motion Tracking from Sparse Sensors with Simulated Avatars},
  author={Alexander W. Winkler and Jungdam Won and Yuting Ye},
  journal={SIGGRAPH Asia 2022 Conference Papers},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:252383239}
}

@article{Zhao2023SynthesizingDH,
  title={Synthesizing Diverse Human Motions in 3D Indoor Scenes},
  author={Kaifeng Zhao and Yan Zhang and Shaofei Wang and Thabo Beeler and Siyu Tang},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.12411},
  url={https://api.semanticscholar.org/CorpusID:258833123}
}

@article{Zhang2023LearningPS,
  title={Learning Physically Simulated Tennis Skills from Broadcast Videos},
  author={Haotian Zhang and Ye Yuan and Viktor Makoviychuk and Yunrong Guo and Sanja Fidler and Xue Bin Peng and Kayvon Fatahalian},
  journal={ACM Transactions on Graphics (TOG)},
  year={2023},
  volume={42},
  pages={1 - 14},
  url={https://api.semanticscholar.org/CorpusID:259266921}
}

@article{Huang2022PhysicallyPA,
  title={Physically Plausible Animation of Human Upper Body from a Single Image},
  author={Ziyuan Huang and Zhengping Zhou and Yung-Yu Chuang and Jiajun Wu and C. Karen Liu},
  journal={2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
  year={2022},
  pages={930-939},
  url={https://api.semanticscholar.org/CorpusID:254535979}
}

@article{Braun2023PhysicallyPF,
  title={Physically Plausible Full-Body Hand-Object Interaction Synthesis},
  author={Jona Braun and Sammy Christen and Muhammed Kocabas and Emre Aksan and Otmar Hilliges},
  journal={ArXiv},
  year={2023},
  volume={abs/2309.07907},
  url={https://api.semanticscholar.org/CorpusID:261823135}
}

@article{Xiao2023UnifiedHI,
  title={Unified Human-Scene Interaction via Prompted Chain-of-Contacts},
  author={Zeqi Xiao and Tai Wang and Jingbo Wang and Jinkun Cao and Wenwei Zhang and Bo Dai and Dahua Lin and Jiangmiao Pang},
  journal={ArXiv},
  year={2023},
  volume={abs/2309.07918},
  url={https://api.semanticscholar.org/CorpusID:261822943}
}
@article{Juravsky2022PADLLP,
  title={PADL: Language-Directed Physics-Based Character Control},
  author={Jordan Juravsky and Yunrong Guo and Sanja Fidler and Xue Bin Peng},
  journal={SIGGRAPH Asia 2022 Conference Papers},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:254070640}
}

@article{Sun2023PromptPP,
  title={Prompt, Plan, Perform: LLM-based Humanoid Control via Quantized Imitation Learning},
  author={Jingkai Sun and Qiang Zhang and Yiqun Duan and Xiaoyang Jiang and Chong Cheng and Renjing Xu},
  journal={ArXiv},
  year={2023},
  volume={abs/2309.11359},
  url={https://api.semanticscholar.org/CorpusID:261974876}
}

@inproceedings{
jiang2023hgap,
title={H-{GAP}: Humanoid Control with a Generalist Planner},
author={zhengyao jiang and Yingchen Xu and Nolan Wagener and Yicheng Luo and Michael Janner and Edward Grefenstette and Tim Rockt{\"a}schel and Yuandong Tian},
booktitle={NeurIPS 2023 Foundation Models for Decision Making Workshop},
year={2023},
url={https://openreview.net/forum?id=xbOtG1P723}
}

@misc{
anonymous2023reason,
title={Reason to Behave: Achieving Human-Like Task Execution for Physics-Based Characters},
author={Anonymous},
year={2023},
url={https://openreview.net/forum?id=Y6PVsnkKVV}
}

@article{Bergamin2019DReConDR,
  title={DReCon: data-driven responsive control of physics-based characters},
  author={Kevin Bergamin and Simon Clavet and Daniel Holden and James Richard Forbes},
  journal={ACM Trans. Graph.},
  year={2019},
  volume={38},
  pages={206:1-206:11},
  url={https://api.semanticscholar.org/CorpusID:207998110}
}

@article{Wang2020UniConUN,
  title={UniCon: Universal Neural Controller For Physics-based Character Motion},
  author={Tingwu Wang and Yunrong Guo and Maria Shugrina and Sanja Fidler},
  journal={ArXiv},
  year={2020},
  volume={abs/2011.15119},
  url={https://api.semanticscholar.org/CorpusID:227239348}
}

@article{Robinson2022RoboticVF,
  title={Robotic Vision for Human-Robot Interaction and Collaboration: A Survey and Systematic Review},
  author={Nicole L. Robinson and Brendan Tidd and Dylan Campbell and Dana Kuli{\'c} and Peter Corke},
  journal={ACM Transactions on Human-Robot Interaction},
  year={2022},
  volume={12},
  pages={1 - 66},
  url={https://api.semanticscholar.org/CorpusID:254247688}
}
@article{Arumbakkam2010AMA,
  title={A multi-modal architecture for human robot communication},
  author={Arjun Arumbakkam and Taizo Yoshikawa and Behzad Dariush and Kikuo Fujimura},
  journal={2010 10th IEEE-RAS International Conference on Humanoid Robots},
  year={2010},
  pages={639-646},
  url={https://api.semanticscholar.org/CorpusID:18911576}
}

@article{Yao2023MoConVQUP,
  title={MoConVQ: Unified Physics-Based Motion Control via Scalable Discrete Representations},
  author={Heyuan Yao and Zhenhua Song and Yuyang Zhou and Tenglong Ao and Baoquan Chen and Libin Liu},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.10198},
  url={https://api.semanticscholar.org/CorpusID:264146020}
}

@article{Cern2023ANM,
  title={A Novel Multi-Modal Teleoperation of a Humanoid Assistive Robot with Real-Time Motion Mimic},
  author={Julio C. Cer{\'o}n and Md. Samiul Haque Sunny and Brahim Brahmi and Luis Miguel Mendez and Raouf Fareh and Helal Uddin Ahmed and Mohammad Habibur Rahman},
  journal={Micromachines},
  year={2023},
  volume={14},
  url={https://api.semanticscholar.org/CorpusID:256964199}
}

@article{Dou2023CASELC,
  title={C·ASE: Learning Conditional Adversarial Skill Embeddings for Physics-based Characters},
  author={Zhiyang Dou and Xuelin Chen and Qingnan Fan and Taku Komura and Wenping Wang},
  journal={ArXiv},
  year={2023},
  volume={abs/2309.11351},
  url={https://api.semanticscholar.org/CorpusID:262064161}
}

@article{Silva2008SimulationOH,
  title={Simulation of Human Motion Data using Short‐Horizon Model‐Predictive Control},
  author={Marco da Silva and Yeuhi Abe and Jovan Popovi{\'c}},
  journal={Computer Graphics Forum},
  year={2008},
  volume={27},
  url={https://api.semanticscholar.org/CorpusID:8928978}
}

@article{Peng2015DynamicTT,
  title={Dynamic terrain traversal skills using reinforcement learning},
  author={Xue Bin Peng and Glen Berseth and Michiel van de Panne},
  journal={ACM Transactions on Graphics (TOG)},
  year={2015},
  volume={34},
  pages={1 - 11},
  url={https://api.semanticscholar.org/CorpusID:17966128}
}


@article{Coros2009RobustTC,
  title={Robust task-based control policies for physics-based characters},
  author={Stelian Coros and Philippe Beaudoin and Michiel van de Panne},
  journal={ACM SIGGRAPH Asia 2009 papers},
  year={2009},
  url={https://api.semanticscholar.org/CorpusID:1833501}
}

@misc{yuan_residual_2020,
	title = {Residual Force Control for Agile Human Behavior Imitation and Extended Motion Synthesis},
	url = {http://arxiv.org/abs/2006.07364},
	abstract = {Reinforcement learning has shown great promise for synthesizing realistic human behaviors by learning humanoid control policies from motion capture data. However, it is still very challenging to reproduce sophisticated human skills like ballet dance, or to stably imitate long-term human behaviors with complex transitions. The main difficulty lies in the dynamics mismatch between the humanoid model and real humans. That is, motions of real humans may not be physically possible for the humanoid model. To overcome the dynamics mismatch, we propose a novel approach, residual force control ({RFC}), that augments a humanoid control policy by adding external residual forces into the action space. During training, the {RFC}-based policy learns to apply residual forces to the humanoid to compensate for the dynamics mismatch and better imitate the reference motion. Experiments on a wide range of dynamic motions demonstrate that our approach outperforms state-of-the-art methods in terms of convergence speed and the quality of learned motions. Notably, we showcase a physics-based virtual character empowered by {RFC} that can perform highly agile ballet dance moves such as pirouette, arabesque and jet{\textbackslash}'e. Furthermore, we propose a dual-policy control framework, where a kinematic policy and an {RFC}-based policy work in tandem to synthesize multi-modal infinite-horizon human motions without any task guidance or user input. Our approach is the first humanoid control method that successfully learns from a large-scale human motion dataset (Human3.6M) and generates diverse long-term motions. Code and videos are available at https://www.ye-yuan.com/rfc.},
	number = {{arXiv}:2006.07364},
	publisher = {{arXiv}},
	author = {Yuan, Ye and Kitani, Kris},
	urldate = {2023-03-20},
	date = {2020-10-22},
	eprinttype = {arxiv},
	eprint = {2006.07364 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning, Computer Science - Robotics, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/bobby/Zotero/storage/D6D3NKVK/Yuan and Kitani - 2020 - Residual Force Control for Agile Human Behavior Im.pdf:application/pdf;arXiv.org Snapshot:/Users/bobby/Zotero/storage/QE5F6RXU/2006.html:text/html},
}



@article{Won2020ASA,
  title={A scalable approach to control diverse behaviors for physically simulated characters},
  author={Jungdam Won and Deepak Edakkattil Gopinath and Jessica K. Hodgins},
  journal={ACM Transactions on Graphics (TOG)},
  year={2020},
  volume={39},
  pages={33:1 - 33:12},
  url={https://api.semanticscholar.org/CorpusID:219569865}
}

@article{Chentanez2018PhysicsbasedMC,
  title={Physics-based motion capture imitation with deep reinforcement learning},
  author={Nuttapong Chentanez and Matthias M{\"u}ller and Miles Macklin and Viktor Makoviychuk and Stefan Jeschke},
  journal={Proceedings of the 11th ACM SIGGRAPH Conference on Motion, Interaction and Games},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:53237567}
}

@article{Xu2021AGA,
  title={A GAN-Like Approach for Physics-Based Imitation Learning and Interactive Control},
  author={Pei Xu and Ioannis Karamouzas},
  journal={Proceedings of the ACM on Computer Graphics and Interactive Techniques},
  year={2021},
  volume={4},
  pages={1 - 22},
  url={https://api.semanticscholar.org/CorpusID:235125922}
}

@article{Holden2017PhasefunctionedNN,
  title={Phase-functioned neural networks for character control},
  author={Daniel Holden and Taku Komura and Jun Saito},
  journal={ACM Transactions on Graphics (TOG)},
  year={2017},
  volume={36},
  pages={1 - 13},
  url={https://api.semanticscholar.org/CorpusID:7261259}
}


@article{Merel2017LearningHB,
  title={Learning human behaviors from motion capture by adversarial imitation},
  author={Josh Merel and Yuval Tassa and TB Dhruva and Sriram Srinivasan and Jay Lemmon and Ziyun Wang and Greg Wayne and Nicolas Manfred Otto Heess},
  journal={ArXiv},
  year={2017},
  volume={abs/1707.02201},
  url={https://api.semanticscholar.org/CorpusID:40186317}
}

@article{Yao2022ControlVAE,
  title={ControlVAE},
  author={Heyuan Yao and Zhenhua Song and Bao Xin Chen and Libin Liu},
  journal={ACM Transactions on Graphics (TOG)},
  year={2022},
  volume={41},
  pages={1 - 16},
  url={https://api.semanticscholar.org/CorpusID:252846386}
}

@misc{merel_catch_2020,
	title = {Catch \& Carry: Reusable Neural Controllers for Vision-Guided Whole-Body Tasks},
	url = {http://arxiv.org/abs/1911.06636},
	shorttitle = {Catch \& Carry},
	abstract = {We address the longstanding challenge of producing flexible, realistic humanoid character controllers that can perform diverse whole-body tasks involving object interactions. This challenge is central to a variety of fields, from graphics and animation to robotics and motor neuroscience. Our physics-based environment uses realistic actuation and first-person perception -- including touch sensors and egocentric vision -- with a view to producing active-sensing behaviors (e.g. gaze direction), transferability to real robots, and comparisons to the biology. We develop an integrated neural-network based approach consisting of a motor primitive module, human demonstrations, and an instructed reinforcement learning regime with curricula and task variations. We demonstrate the utility of our approach for several tasks, including goal-conditioned box carrying and ball catching, and we characterize its behavioral robustness. The resulting controllers can be deployed in real-time on a standard {PC}. See overview video, https://youtu.be/2rQAW-8gQQk .},
	number = {{arXiv}:1911.06636},
	publisher = {{arXiv}},
	author = {Merel, Josh and Tunyasuvunakool, Saran and Ahuja, Arun and Tassa, Yuval and Hasenclever, Leonard and Pham, Vu and Erez, Tom and Wayne, Greg and Heess, Nicolas},
	urldate = {2023-03-20},
	date = {2020-06-16},
	eprinttype = {arxiv},
	eprint = {1911.06636 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/Users/bobby/Zotero/storage/UU4P8SED/Merel et al. - 2020 - Catch & Carry Reusable Neural Controllers for Vis.pdf:application/pdf;arXiv.org Snapshot:/Users/bobby/Zotero/storage/M8849959/1911.html:text/html},
}

@misc{wagener_mocapact_2023,
	title = {{MoCapAct}: A Multi-Task Dataset for Simulated Humanoid Control},
	url = {http://arxiv.org/abs/2208.07363},
	doi = {10.48550/arXiv.2208.07363},
	shorttitle = {{MoCapAct}},
	abstract = {Simulated humanoids are an appealing research domain due to their physical capabilities. Nonetheless, they are also challenging to control, as a policy must drive an unstable, discontinuous, and high-dimensional physical system. One widely studied approach is to utilize motion capture ({MoCap}) data to teach the humanoid agent low-level skills (e.g., standing, walking, and running) that can then be re-used to synthesize high-level behaviors. However, even with {MoCap} data, controlling simulated humanoids remains very hard, as {MoCap} data offers only kinematic information. Finding physical control inputs to realize the demonstrated motions requires computationally intensive methods like reinforcement learning. Thus, despite the publicly available {MoCap} data, its utility has been limited to institutions with large-scale compute. In this work, we dramatically lower the barrier for productive research on this topic by training and releasing high-quality agents that can track over three hours of {MoCap} data for a simulated humanoid in the dm\_control physics-based environment. We release {MoCapAct} (Motion Capture with Actions), a dataset of these expert agents and their rollouts, which contain proprioceptive observations and actions. We demonstrate the utility of {MoCapAct} by using it to train a single hierarchical policy capable of tracking the entire {MoCap} dataset within dm\_control and show the learned low-level component can be re-used to efficiently learn downstream high-level tasks. Finally, we use {MoCapAct} to train an autoregressive {GPT} model and show that it can control a simulated humanoid to perform natural motion completion given a motion prompt. Videos of the results and links to the code and dataset are available at https://microsoft.github.io/{MoCapAct}.},
	number = {{arXiv}:2208.07363},
	publisher = {{arXiv}},
	author = {Wagener, Nolan and Kolobov, Andrey and Frujeri, Felipe Vieira and Loynd, Ricky and Cheng, Ching-An and Hausknecht, Matthew},
	urldate = {2023-03-18},
	date = {2023-01-13},
	eprinttype = {arxiv},
	eprint = {2208.07363 [cs, eess]},
	keywords = {Computer Science - Graphics, Computer Science - Machine Learning, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
	file = {arXiv Fulltext PDF:/Users/bobby/Zotero/storage/NISW4846/Wagener et al. - 2023 - MoCapAct A Multi-Task Dataset for Simulated Human.pdf:application/pdf;arXiv.org Snapshot:/Users/bobby/Zotero/storage/WBWZLKED/2208.html:text/html},
}

@article{nakaoka_learning_2007,
	title = {Learning from Observation Paradigm: Leg Task Models for Enabling a Biped                 Humanoid Robot to Imitate Human Dances},
	volume = {26},
	issn = {0278-3649},
	url = {https://doi.org/10.1177/0278364907079430},
	doi = {10.1177/0278364907079430},
	shorttitle = {Learning from Observation Paradigm},
	abstract = {This paper proposes a framework that achieves the Learning from Observation paradigm for learning dance motions. The framework enables a humanoid robot to imitate dance motions captured from human demonstrations. This study especially focuses on leg motions to achieve a novel attempt in which a biped-type robot imitates not only upper body motions but also leg motions including steps. Body differences between the robot and the original dancer make the problem difficult because the differences prevent the robot from straightforwardly following the original motions and they also change dynamic body balance. We propose leg task models, which play a key role in solving the problem. Low-level tasks in leg motion are modelled so that they clearly provide essential information required for keeping dynamic stability and important motion characteristics. The models divide the problem of adapting motions into the problem of recognizing a sequence of the tasks and the problem of executing the task sequence. We have developed a method for recognizing the tasks from captured motion data and a method for generating the motions of the tasks that can be executed by existing robots including {HRP}-2. {HRP}-2 successfully performed the generated motions, which imitated a traditional folk dance performed by human dancers.},
	pages = {829--844},
	number = {8},
	journaltitle = {The International Journal of Robotics Research},
	author = {Nakaoka, Shin'ichiro and Nakazawa, Atsushi and Kanehiro, Fumio and Kaneko, Kenji and Morisawa, Mitsuharu and Hirukawa, Hirohisa and Ikeuchi, Katsushi},
	urldate = {2023-03-18},
	date = {2007-08-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Ltd {STM}},
	file = {SAGE PDF Full Text:/Users/bobby/Zotero/storage/QX8K5JWN/Nakaoka et al. - 2007 - Learning from Observation Paradigm Leg Task Model.pdf:application/pdf},
}

@article{kelly_introduction_2017,
	title = {An Introduction to Trajectory Optimization:  How to Do Your Own Direct Collocation},
	volume = {59},
	issn = {0036-1445},
	url = {https://epubs.siam.org/doi/10.1137/16M1062569},
	doi = {10.1137/16M1062569},
	shorttitle = {An Introduction to Trajectory Optimization},
	abstract = {The direct transcription or collocation method has demonstrated notable success in the solution of trajectory optimization and optimal control problems. This approach combines a sparse nonlinear programming algorithm with a discretization of the trajectory dynamics. A challenging class of optimization problems occurs when the spacecraft trajectories are characterized by thrust levels that are very low relative to the vehicle weight. Low thrust trajectories are demanding because realistic forces, due to oblateness, and third-body perturbations often dominate the thrust. Furthermore, because the thrust is so low, significant changes to the orbits require very long duration trajectories. When a collocation method is applied to a problem of this type, the resulting nonlinear program is very large, because the trajectories are long, and very nonlinear because of the perturbing forces.This paper describes the application of the transcription method to compute an optimal low thrust transfer from an Earth orbit to a specified lunar mission orbit. It is representative of the {SMART}-1 or "Small Missions for Advanced Research in Technology" of the {ESA} scientific program [J. Schoenmaekers, J. Pulido, and R. Jehn, Tech. report S1-{ESC}-{RP}-5001, European Space Agency, 1998]. The spacecraft is deployed from an Ariane-5 into an elliptic Earth centered park orbit. The goal is to insert the spacecraft into a lunar orbit that is polar and elliptic and has its pericenter above the south pole. The spacecraft utilizes a solar electric propulsion system. The goal is to compute the optimal steering during the orbit transfer, which takes over 200 days, so that the fuel consumption is minimized. The vehicle dynamics are defined using a modified set of equinoctial coordinates, and the trajectory modeling is described using these dynamics. A solution is presented that requires the solution of a sparse optimization problem with 211031 variables and 146285 constraints. The trajectory we present requires two very long thrust arcs, and, consequently, the overall mission duration is much shorter than multiburn trajectories. Issues related to the numerical conditioning and problem formulation are discussed.},
	pages = {849--904},
	number = {4},
	journaltitle = {{SIAM} Rev.},
	author = {Kelly, Matthew},
	urldate = {2023-03-18},
	date = {2017-01},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	file = {Full Text PDF:/Users/bobby/Zotero/storage/LVVASWF4/Kelly - 2017 - An Introduction to Trajectory Optimization  How t.pdf:application/pdf},
}

@inproceedings{xu_prompting_2022,
	title = {Prompting Decision Transformer for Few-Shot Policy Generalization},
	url = {https://proceedings.mlr.press/v162/xu22g.html},
	abstract = {Human can leverage prior experience and learn novel tasks from a handful of demonstrations. In contrast to offline meta-reinforcement learning, which aims to achieve quick adaptation through better algorithm design, we investigate the effect of architecture inductive bias on the few-shot learning capability. We propose a Prompt-based Decision Transformer (Prompt-{DT}), which leverages the sequential modeling ability of the Transformer architecture and the prompt framework to achieve few-shot adaptation in offline {RL}. We design the trajectory prompt, which contains segments of the few-shot demonstrations, and encodes task-specific information to guide policy generation. Our experiments in five {MuJoCo} control benchmarks show that Prompt-{DT} is a strong few-shot learner without any extra finetuning on unseen target tasks. Prompt-{DT} outperforms its variants and strong meta offline {RL} baselines by a large margin with a trajectory prompt containing only a few timesteps. Prompt-{DT} is also robust to prompt length changes and can generalize to out-of-distribution ({OOD}) environments. Project page: {\textbackslash}href\{https://mxu34.github.io/{PromptDT}/\}\{https://mxu34.github.io/{PromptDT}/\}.},
	eventtitle = {International Conference on Machine Learning},
	pages = {24631--24645},
	booktitle = {Proceedings of the 39th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Xu, Mengdi and Shen, Yikang and Zhang, Shun and Lu, Yuchen and Zhao, Ding and Tenenbaum, Joshua and Gan, Chuang},
	urldate = {2023-05-31},
	date = {2022-06-28},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	file = {Full Text PDF:/Users/bobby/Zotero/storage/R5GABGAT/Xu et al. - 2022 - Prompting Decision Transformer for Few-Shot Policy.pdf:application/pdf},
}


@misc{humphreys_large-scale_2022,
	title = {Large-Scale Retrieval for Reinforcement Learning},
	url = {http://arxiv.org/abs/2206.05314},
	abstract = {Effective decision making involves flexibly relating past experiences and relevant contextual information to a novel situation. In deep reinforcement learning ({RL}), the dominant paradigm is for an agent to amortise information that helps decision making into its network weights via gradient descent on training losses. Here, we pursue an alternative approach in which agents can utilise large-scale context sensitive database lookups to support their parametric computations. This allows agents to directly learn in an end-to-end manner to utilise relevant information to inform their outputs. In addition, new information can be attended to by the agent, without retraining, by simply augmenting the retrieval dataset. We study this approach for offline {RL} in 9x9 Go, a challenging game for which the vast combinatorial state space privileges generalisation over direct matching to past experiences. We leverage fast, approximate nearest neighbor techniques in order to retrieve relevant data from a set of tens of millions of expert demonstration states. Attending to this information provides a significant boost to prediction accuracy and game-play performance over simply using these demonstrations as training trajectories, providing a compelling demonstration of the value of large-scale retrieval in offline {RL} agents.},
	number = {{arXiv}:2206.05314},
	publisher = {{arXiv}},
	author = {Humphreys, Peter C. and Guez, Arthur and Tieleman, Olivier and Sifre, Laurent and Weber, Théophane and Lillicrap, Timothy},
	urldate = {2023-06-02},
	date = {2022-12-16},
	eprinttype = {arxiv},
	eprint = {2206.05314 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bobby/Zotero/storage/ARIIFQH8/2206.html:text/html;Full Text PDF:/Users/bobby/Zotero/storage/E4C7KKNR/Humphreys et al. - 2022 - Large-Scale Retrieval for Reinforcement Learning.pdf:application/pdf},
}

@misc{luo_embodied_2022,
	title = {Embodied Scene-aware Human Pose Estimation},
	url = {http://arxiv.org/abs/2206.09106},
	abstract = {We propose embodied scene-aware human pose estimation where we estimate 3D poses based on a simulated agent's proprioception and scene awareness, along with external third-person observations. Unlike prior methods that often resort to multistage optimization, non-causal inference, and complex contact modeling to estimate human pose and human scene interactions, our method is one-stage, causal, and recovers global 3D human poses in a simulated environment. Since 2D third-person observations are coupled with the camera pose, we propose to disentangle the camera pose and use a multi-step projection gradient defined in the global coordinate frame as the movement cue for our embodied agent. Leveraging a physics simulation and prescanned scenes (e.g., 3D mesh), we simulate our agent in everyday environments (library, office, bedroom, etc.) and equip our agent with environmental sensors to intelligently navigate and interact with the geometries of the scene. Our method also relies only on 2D keypoints and can be trained on synthetic datasets derived from popular human motion databases. To evaluate, we use the popular H36M and {PROX} datasets and achieve high quality pose estimation on the challenging {PROX} dataset without ever using {PROX} motion sequences for training. Code and videos are available on the project page.},
	number = {{arXiv}:2206.09106},
	publisher = {{arXiv}},
	author = {Luo, Zhengyi and Iwase, Shun and Yuan, Ye and Kitani, Kris},
	urldate = {2023-06-02},
	date = {2022-10-13},
	eprinttype = {arxiv},
	eprint = {2206.09106 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/bobby/Zotero/storage/CSSNIX9I/2206.html:text/html;Full Text PDF:/Users/bobby/Zotero/storage/L75P5NYN/Luo et al. - 2022 - Embodied Scene-aware Human Pose Estimation.pdf:application/pdf},
}

@article{Tessler2023CALMCA,
  title={CALM: Conditional Adversarial Latent Models  for Directable Virtual Characters},
  author={Chen Tessler and Yoni Kasten and Yunrong Guo and Shie Mannor and Gal Chechik and Xue Bin Peng},
  journal={ACM SIGGRAPH 2023 Conference Proceedings},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:258461220}
}

@article{Lee2022LearningVC,
  title={Learning Virtual Chimeras by Dynamic Motion Reassembly},
  author={Seyoung Lee and Jiye Lee and Jehee Lee},
  journal={ACM Transactions on Graphics (TOG)},
  year={2022},
  volume={41},
  pages={1 - 13},
  url={https://api.semanticscholar.org/CorpusID:254097260}
}

@misc{hassan_synthesizing_2023,
	title = {Synthesizing Physical Character-Scene Interactions},
	url = {http://arxiv.org/abs/2302.00883},
	abstract = {Movement is how people interact with and affect their environment. For realistic character animation, it is necessary to synthesize such interactions between virtual characters and their surroundings. Despite recent progress in character animation using machine learning, most systems focus on controlling an agent's movements in fairly simple and homogeneous environments, with limited interactions with other objects. Furthermore, many previous approaches that synthesize human-scene interactions require significant manual labeling of the training data. In contrast, we present a system that uses adversarial imitation learning and reinforcement learning to train physically-simulated characters that perform scene interaction tasks in a natural and life-like manner. Our method learns scene interaction behaviors from large unstructured motion datasets, without manual annotation of the motion data. These scene interactions are learned using an adversarial discriminator that evaluates the realism of a motion within the context of a scene. The key novelty involves conditioning both the discriminator and the policy networks on scene context. We demonstrate the effectiveness of our approach through three challenging scene interaction tasks: carrying, sitting, and lying down, which require coordination of a character's movements in relation to objects in the environment. Our policies learn to seamlessly transition between different behaviors like idling, walking, and sitting. By randomizing the properties of the objects and their placements during training, our method is able to generalize beyond the objects and scenarios depicted in the training dataset, producing natural character-scene interactions for a wide variety of object shapes and placements. The approach takes physics-based character motion generation a step closer to broad applicability.},
	number = {{arXiv}:2302.00883},
	publisher = {{arXiv}},
	author = {Hassan, Mohamed and Guo, Yunrong and Wang, Tingwu and Black, Michael and Fidler, Sanja and Peng, Xue Bin},
	urldate = {2023-06-02},
	date = {2023-02-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2302.00883 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Graphics, Computer Science - Machine Learning},
	file = {Hassan et al. - 2023 - Synthesizing Physical Character-Scene Interactions.pdf:/Users/bobby/Zotero/storage/R25JCUZ8/Hassan et al. - 2023 - Synthesizing Physical Character-Scene Interactions.pdf:application/pdf},
}

@article{Fu2022DeepWC,
  title={Deep Whole-Body Control: Learning a Unified Policy for Manipulation and Locomotion},
  author={Zipeng Fu and Xuxin Cheng and Deepak Pathak},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.10044},
  url={https://api.semanticscholar.org/CorpusID:252968218}
}

@article{Schulman2017ProximalPO,
  title={Proximal Policy Optimization Algorithms},
  author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
  journal={ArXiv},
  year={2017},
  volume={abs/1707.06347},
  url={https://api.semanticscholar.org/CorpusID:28695052}
}

@article{Luo2023UniversalHM,
  title={Universal Humanoid Motion Representations for Physics-Based Control},
  author={Zhen-Ge Luo and Jinkun Cao and Josh Merel and Alexander Winkler and Jing Huang and Kris Kitani and Weipeng Xu},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.04582},
  url={https://api.semanticscholar.org/CorpusID:263829555}
}

@article{zhang_learning_nodate,
	title = {Learning Physically Simulated Tennis Skills from Broadcast Videos},
	volume = {42},
	number = {4},
	author = {Zhang, Haotian and Yuan, Ye and Makoviychuk, Viktor and Guo, Yunrong and Fidler, Sanja and Peng, Xue Bin and Fatahalian, Kayvon},
	langid = {english},
	file = {Zhang et al. - Learning Physically Simulated Tennis Skills from B.pdf:/Users/bobby/Zotero/storage/LK3ECA8S/Zhang et al. - Learning Physically Simulated Tennis Skills from B.pdf:application/pdf},
}

@article{rempe_trace_nodate,
	title = {Trace and Pace: Controllable Pedestrian Animation via Guided Trajectory Diffusion},
	abstract = {We introduce a method for generating realistic pedestrian trajectories and full-body animations that can be controlled to meet user-defined goals. We draw on recent advances in guided diffusion modeling to achieve test-time controllability of trajectories, which is normally only associated with rule-based systems. Our guided diffusion model allows users to constrain trajectories through target waypoints, speed, and specified social groups while accounting for the surrounding environment context. This trajectory diffusion model is integrated with a novel physics-based humanoid controller to form a closed-loop, full-body pedestrian animation system capable of placing large crowds in a simulated environment with varying terrains. We further propose utilizing the value function learned during {RL} training of the animation controller to guide diffusion to produce trajectories better suited for particular scenarios such as collision avoidance and traversing uneven terrain. Video results are available on the project page.},
	author = {Rempe, Davis and Luo, Zhengyi and Peng, Xue Bin and Yuan, Ye and Kitani, Kris and Kreis, Karsten and Fidler, Sanja},
	langid = {english},
	file = {Rempe et al. - Trace and Pace Controllable Pedestrian Animation .pdf:/Users/bobby/Zotero/storage/FUV5YRYN/Rempe et al. - Trace and Pace Controllable Pedestrian Animation .pdf:application/pdf},
}

@article{li_robust_nodate,
	title = {Robust and Versatile Bipedal Jumping Control through Multi-Task Reinforcement Learning},
	abstract = {This work aims to push the limits of agility for bipedal robots by enabling a torque-controlled bipedal robot to perform robust and versatile dynamic jumps in the real world. We present a multi-task reinforcement learning framework to train the robot to accomplish a large variety of jumping tasks, such as jumping to different locations and directions. To improve performance on these challenging tasks, we develop a new policy structure that encodes the robot’s long-term input/output (I/O) history while also providing direct access to its short-term I/O history. In order to train a versatile multi-task policy, we utilize a multi-stage training scheme that includes different training stages for different objectives. After multi-stage training, the multi-task policy can be directly transferred to Cassie, a physical bipedal robot. Training on different tasks and exploring more diverse scenarios leads to highly robust policies that can exploit the diverse set of learned skills to recover from perturbations or poor landings during real-world deployment. Such robustness in the proposed multi-task policy enables Cassie to succeed in completing a variety of challenging jump tasks in the real world, such as standing long jumps, jumping onto elevated platforms, and multi-axis jumps.},
	author = {Li, Zhongyu and Peng, Xue Bin and Abbeel, Pieter and Levine, Sergey and Berseth, Glen and Sreenath, Koushil},
	langid = {english},
	file = {Li et al. - Robust and Versatile Bipedal Jumping Control throu.pdf:/Users/bobby/Zotero/storage/6KNAVWIR/Li et al. - Robust and Versatile Bipedal Jumping Control throu.pdf:application/pdf},
}

@article{Luo2022FromUH,
  title={From Universal Humanoid Control to Automatic Physically Valid Character Creation},
  author={Zhengyi Luo and Ye Yuan and Kris M. Kitani},
  journal={ArXiv},
  year={2022},
  volume={abs/2206.09286},
  url={https://api.semanticscholar.org/CorpusID:249890452}
}


@article{Won2019LearningBS,
  title={Learning body shape variation in physics-based characters},
  author={Jungdam Won and Jehee Lee},
  journal={ACM Transactions on Graphics (TOG)},
  year={2019},
  volume={38},
  pages={1 - 12},
  url={https://api.semanticscholar.org/CorpusID:207997891}
}

@inproceedings{Luo2021DynamicsRegulatedKP,
  title={Dynamics-Regulated Kinematic Policy for Egocentric Pose Estimation},
  author={Zhengyi Luo and Ryo Hachiuma and Ye Yuan and Kris Kitani},
  booktitle={Neural Information Processing Systems},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:235390810}
}


@misc{luo_perpetual_2023,
	title = {Perpetual Humanoid Control for Real-time Simulated Avatars},
	url = {http://arxiv.org/abs/2305.06456},
	doi = {10.48550/arXiv.2305.06456},
	abstract = {We present a physics-based humanoid controller that achieves high-fidelity motion imitation and fault-tolerant behavior in the presence of noisy input (e.g. pose estimates from video or generated from language) and unexpected falls. Our controller scales up to learning ten thousand motion clips without using any external stabilizing forces and learns to naturally recover from fail-state. Given reference motion, our controller can perpetually control simulated avatars without requiring resets. At its core, we propose the progressive multiplicative control policy ({PMCP}), which dynamically allocates new network capacity to learn harder and harder motion sequences. {PMCP} allows efficient scaling for learning from large-scale motion databases and adding new tasks, such as fail-state recovery, without catastrophic forgetting. We demonstrate the effectiveness of our controller by using it to imitate noisy poses from video-based pose estimators and language-based motion generators in a live and real-time multi-person avatar use case.},
	number = {{arXiv}:2305.06456},
	publisher = {{arXiv}},
	author = {Luo, Zhengyi and Cao, Jinkun and Winkler, Alexander and Kitani, Kris and Xu, Weipeng},
	urldate = {2023-08-14},
	date = {2023-05-24},
	eprinttype = {arxiv},
	eprint = {2305.06456 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/Users/bobby/Zotero/storage/U2N4WJ4X/Luo et al. - 2023 - Perpetual Humanoid Control for Real-time Simulated.pdf:application/pdf;arXiv.org Snapshot:/Users/bobby/Zotero/storage/VMNYWLF5/2305.html:text/html},
}
@article{Won2022PhysicsbasedCC,
  title={Physics-based character controllers using conditional VAEs},
  author={Jungdam Won and Deepak Edakkattil Gopinath and Jessica K. Hodgins},
  journal={ACM Transactions on Graphics (TOG)},
  year={2022},
  volume={41},
  pages={1 - 12},
  url={https://api.semanticscholar.org/CorpusID:250956798}
}

@inproceedings{zhang2023generating,
  title={Generating Human Motion From Textual Descriptions With Discrete Representations},
  author={Zhang, Jianrong and Zhang, Yangsong and Cun, Xiaodong and Zhang, Yong and Zhao, Hongwei and Lu, Hongtao and Shen, Xi and Shan, Ying},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14730--14740},
  year={2023}
}

@article{sarandi2021metrabs,
  title={{MeTRAbs:} Metric-Scale Truncation-Robust Heatmaps for Absolute 3{D} Human Pose Estimation},
  author={S\'ar\'andi, Istv\'an and Linder, Timm and Arras, Kai O. and Leibe, Bastian},
  journal={IEEE Transactions on Biometrics, Behavior, and Identity Science},
  year={2021},
  volume={3},
  number={1},
  pages={16-30},
  doi={10.1109/TBIOM.2020.3037257}
}

@article{Peng2022ASE,
  title={Ase: Large-scale reusable adversarial skill embeddings for physically simulated characters},
  author={Peng, Xue Bin and Guo, Yunrong and Halper, Lina and Levine, Sergey and Fidler, Sanja},
  journal={ACM Transactions On Graphics (TOG)},
  volume={41},
  number={4},
  pages={1--17},
  year={2022},
  publisher={ACM New York, NY, USA}
}