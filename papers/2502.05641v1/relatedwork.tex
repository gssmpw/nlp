\section{Related Work}
\label{sec:related}
% \vspace{-0.75em}
\textbf{Learning from MoCap Data:} Leveraging MoCap data enables controllers to acquire complex behaviors with human-like motion quality. However, large and diverse MoCap datasets present challenges including scalable distillation and ensuring fluid transition between skills. Initial works train single-clip policies to mimic individual behaviors using tracking rewards \cite{peng_deepmimic_2018, Chentanez2018PhysicsbasedMC, ren_diffmimic_2023} or adversarial losses \cite{peng_amp_2021, Ho2016GenerativeAI, Lee2022HumaninspiredVI, Merel2017LearningHB, Xu2021AGA}. However, distilling these into a multi-clip controller remains computationally prohibitive \cite{wagener_mocapact_2023}. A more scalable alternative is to directly learn a multi-clip policy learning via reinforcement learning with tracking objectives \cite{Won2022PhysicsbasedCC, Won2020ASA, luo_perpetual_2023, Merel2018NeuralPM, Dou2023CASELC, wagener_mocapact_2023, Peng2022ASE}. However, tracking rewards alone is not enough to ensure smooth transitions between skills and failure recovery. Recent works augment training with adversarial losses to encourage natural motions during transitions \cite{Peng2022ASE, Tessler2023CALMCA} or define an explicit fail state recovery policy \cite{luo_perpetual_2023}. 

\textbf{Combination of motions:} Recent works have explored imitating combined motions, but require training individual policy for each new behavior pair \cite{Bae2023PMPLT, Xu2023CompositeML, Lee2022LearningVC}. Additionally, they rely on full motion oversight, lacking adaptability to partial guidance. On the completion front, some kinematic models can synthesize motions despite missing information \cite{Wang2020SynthesizingL3, Chen2022ExecutingYC}. However, these controllers are not grounded in physics, restricting their application.

\textbf{Under-specified Control:} Intuitive modalities like language, video, and VR provide important yet often under-specified means to direct motor skills. Existing works map some of these modalities to embedding spaces \cite{Juravsky2022PADLLP, Yao2023MoConVQUP, Lee2022HumaninspiredVI, Sun2023PromptPP} or key joint poses \cite{luo_perpetual_2023, Luo2023UniversalHM}. However, they handle a predefined sparisty types; adapting to new sparsity specifications like VR require expensive retraining \cite{Cern2023ANM, Du2023AvatarsGL}. While ambiguity is inherent in language and image-conditioned policies\cite{Huang2022PhysicallyPA, Yao2023MoConVQUP}, fine-grained control remains difficult as they do not allow low-level granularities like joint-level guidance. Overall a gap persists in controllers that can handle partial, sparse guidance with precision across modalities. Closing this gap can enable more intuitive control of reusable motor behaviors.

% These learned skills can then be leveraged for downstream tasks via hierarchical reinforcement learning \cite{merel_catch_2020, Peng2022ASE, Luo2023UniversalHM}. 
\textbf{Downstream Planning:} 
The acquired low-level control skills can support downstream tasks via a wide range of approaches including supervised fine-tuning for specialized behaviors \cite{Yao2023MoConVQUP},  reinforcement learning for new objectives \cite{merel_catch_2020, Luo2023UniversalHM, Peng2022ASE, Dou2023CASELC},  model predictive control for short-term horizons \cite{jiang2023hgap}, and finite state machines that encode behavioral logic \cite{Tessler2023CALMCA}. However, supervised fine-tuning remains restricted in flexibility to new tasks while reinforcement learning lacks sample efficiency whereas model predictive formulations are limited to short planning horizons. Additionally, the range of possible finite state machines largely depend on the flexibility of the underlying low-level controller. One solution is the use of data-driven planners like DAC-MDPs \cite{Shrestha2020DeepAveragersOR} which compile static experiences into approximated MDPs for fast optimization. While these methods enable zero-shot generalization, their integration with learned reusable motor skills remains relatively unexplored. Overall, leveraging low-level controllers to swiftly accomplish high-level goals remains an open challenge.