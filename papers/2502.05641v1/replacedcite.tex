\section{Related Work}
\label{sec:related}
% \vspace{-0.75em}
\textbf{Learning from MoCap Data:} Leveraging MoCap data enables controllers to acquire complex behaviors with human-like motion quality. However, large and diverse MoCap datasets present challenges including scalable distillation and ensuring fluid transition between skills. Initial works train single-clip policies to mimic individual behaviors using tracking rewards ____ or adversarial losses ____. However, distilling these into a multi-clip controller remains computationally prohibitive ____. A more scalable alternative is to directly learn a multi-clip policy learning via reinforcement learning with tracking objectives ____. However, tracking rewards alone is not enough to ensure smooth transitions between skills and failure recovery. Recent works augment training with adversarial losses to encourage natural motions during transitions ____ or define an explicit fail state recovery policy ____. 

\textbf{Combination of motions:} Recent works have explored imitating combined motions, but require training individual policy for each new behavior pair ____. Additionally, they rely on full motion oversight, lacking adaptability to partial guidance. On the completion front, some kinematic models can synthesize motions despite missing information ____. However, these controllers are not grounded in physics, restricting their application.

\textbf{Under-specified Control:} Intuitive modalities like language, video, and VR provide important yet often under-specified means to direct motor skills. Existing works map some of these modalities to embedding spaces ____ or key joint poses ____. However, they handle a predefined sparisty types; adapting to new sparsity specifications like VR require expensive retraining ____. While ambiguity is inherent in language and image-conditioned policies____, fine-grained control remains difficult as they do not allow low-level granularities like joint-level guidance. Overall a gap persists in controllers that can handle partial, sparse guidance with precision across modalities. Closing this gap can enable more intuitive control of reusable motor behaviors.

% These learned skills can then be leveraged for downstream tasks via hierarchical reinforcement learning ____. 
\textbf{Downstream Planning:} 
The acquired low-level control skills can support downstream tasks via a wide range of approaches including supervised fine-tuning for specialized behaviors ____,  reinforcement learning for new objectives ____,  model predictive control for short-term horizons ____, and finite state machines that encode behavioral logic ____. However, supervised fine-tuning remains restricted in flexibility to new tasks while reinforcement learning lacks sample efficiency whereas model predictive formulations are limited to short planning horizons. Additionally, the range of possible finite state machines largely depend on the flexibility of the underlying low-level controller. One solution is the use of data-driven planners like DAC-MDPs ____ which compile static experiences into approximated MDPs for fast optimization. While these methods enable zero-shot generalization, their integration with learned reusable motor skills remains relatively unexplored. Overall, leveraging low-level controllers to swiftly accomplish high-level goals remains an open challenge.