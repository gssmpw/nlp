%
%

%
%
%

\chapter{\label{ch:1-intro}Introduction} 

\minitoc

The ability to make well-informed decisions is crucial across a variety of domains. 
Whether it is a doctor prescribing the most effective treatment for a patient or a company launching a marketing campaign that resonates with its target audience \citep{xu2020contextual,li2010contextual,bastani2019online}, we constantly strive to take actions that lead to desirable outcomes. 
%
However, achieving this goal becomes increasingly challenging in the face of uncertainty. Real-world data is often noisy and incomplete, and the systems we interact with are complex and constantly evolving. As machine learning models become more integrated into critical applications, the need for robust decision-making under these challenging conditions becomes paramount.


This thesis explores the key challenges of robust decision-making in machine learning, specifically focusing on \emph{off-policy evaluation} \citep{uncertainty5, adaptive-ope, uncertainty2, uncertainty3, uncertainty4, doubly-robust}. Consider the example of a doctor who wants to assess a new treatment for a disease. Ideally, they would conduct a randomized controlled trial \citep{tsiatis2019dynamic} where patients are randomly assigned the new treatment or a standard one. However, such trials can be expensive, time-consuming or worse, ethically problematic. Off-policy evaluation (OPE) offers a compelling alternative by allowing us to evaluate the performance of a new decision-making policy (the new treatment) using data collected under a different policy (the standard treatment). This eliminates the need for costly experimentation and allows for quicker implementation of potentially more effective strategies.

However, off-policy evaluation presents its own set of challenges. These challenges stem from two main sources of uncertainty:

\begin{itemize}
    \item \textbf{Statistical uncertainty:} This arises from the inherent randomness in the data we have access to and the limitations of the models we use to represent the real world. 
    For instance, the doctor might have a limited number of patients in their historical dataset, and their model might not perfectly capture a patient's response to treatment due to model misspecification.
    In these circumstances, the conventional OPE methods may suffer from high variance and/or bias, thereby potentially resulting in misleading conclusions \citep{saito2021evaluating,su2020doubly,saito2022off}. 
    \item \textbf{Causal unidentifiability:} In many cases, it may be impossible to definitively establish the causal effects of actions even if we had access to infinite data. This arises due to factors like confounding variables, which can influence both the treatment and the outcome. Imagine the existence of some unmeasured factors, such as a patient's pre-existing conditions, that can influence both their initial treatment and their response to the treatment. This makes it challenging to isolate the true causal effect of the new treatment from the influence of these confounding variables \citep{tsiatis2019dynamic,kallus2018confounding,namkoong2020offpolicy}.
\end{itemize}
This thesis tackles these challenges head-on, proposing novel methods for off-policy evaluation that address both statistical and causal uncertainties.
Before we go into the specifics of these challenges, we introduce the problem of off-policy evaluation in contextual bandits which forms the basis of the setting considered in Chapters \ref{ch:3-mr} and \ref{ch:4-copp}. 

%
%

\section{Off-policy evaluation in contextual bandits}
\subsection{Contextual bandits}
Contextual bandits \citep{Lattimore_Szepesv√°ri_2020} provide a powerful framework for tackling decision-making problems where the effectiveness of an action depends on the specific context in which it is chosen. 
For instance, in medical decision-making, the optimal treatment for a patient might depend on various factors such as their age, medical history, and current symptoms. 
Contextual bandits allow us to model these complex decision-making scenarios by incorporating the notion of context.

In this setting, we use covariates $X \in \mathcal{X}$ to denote features which encapsulate the contextual information such as the patient's age and medical history. We use $A \in \mathcal{A}$ to represent the action chosen by some real-world agent (such as a doctor), and $Y \in \mathcal{Y}$ to denote the outcome/reward observed as a result of taking action $A$. For example, $Y \in \{0, 1\}$ might represent whether a patient survives ($Y=1$) or not ($Y=0$). The goal of a learner in contextual bandits is to choose actions $A$ for a context $X$ which maximises the reward $Y$. 

\subsection{Off-policy evaluation}
Off-policy evaluation (OPE) tackles a crucial challenge in decision-making: assessing the performance of a new policy using data collected under a different policy \citep{swaminathan2015counterfactual, wang2017optimal, farajtabar2018more, su2019continuous, metelli2021subgaussian, liu2019triply, sugiyama2012machine, swaminathan2017off}. This is particularly valuable when conducting controlled experiments with the new policy is impractical or unethical. Here, we formally define the OPE problem in contextual bandits which will set up the challenges tackled in Chapters \ref{ch:3-mr} and \ref{ch:4-copp} of this thesis. 

%
To be more concrete, let $\D\coloneqq \{(x_i, a_i, y_i)\}_{i=1}^n$ be a historically logged dataset with $n$ observations, generated by a (possibly unknown) \emph{behaviour policy} $\beh(a\mid x)$, i.e. the conditional distribution of agent's actions is $A\mid X=x \sim \beh(\,\cdot\mid x)$.
Next, suppose that we are given a different \emph{target policy}, which we denote by $\tar(a\mid x)$. Our goal is to estimate what the expected outcome \emph{would} be if actions were instead sampled from this target policy $\tar$.

\begin{importantresultwithtitle}[title=Off-policy evaluation (OPE)]\noindent
    The main objective of off-policy evaluation (OPE) is to estimate the expectation of the outcome $Y$ under a given target policy $\tar$ using only the logged data $\D$.
\end{importantresultwithtitle}

The key challenge of OPE arises from the fact that we do not
have access to samples from the target distribution which makes the estimation of off-policy value non-trivial in general.
To tackle this problem, the standard OPE methods make the following assumption.
%
%

\begin{assumption}[No unmeasured confounding]\label{assum:no-unmeasured-confounding}
    The agent's action in the observational data $A$ depends only on the context $X$ and possibly additional randomness independent of everything else. This means that when choosing the action, the agent does not rely on additional information relevant to the outcome which is not captured in the context. For instance, in a medical context, this assumption means that all of the information that clinicians use to make treatment decisions is captured in the data. This assumption is also referred to as the \emph{strong ignorability assumption} \citep{tsiatis2019dynamic}.
\end{assumption}

Then, under Assumption \ref{assum:no-unmeasured-confounding}, the off-policy value can be estimated using importance-sampling-based methods \citep{horvitz1952generalization}. 
However, these estimators come with their own set of limitations, which are described in the following section. 
%

\section{Limitations of existing OPE methods}
\subsection{High variance of OPE estimators}\label{subsec:high-variance}
The conventional off-policy value estimators use policy ratios $\rho(a, x) \coloneqq \tar(a\mid x)/\beh(a\mid x)$ as importance weights. 
In cases where the two policies are significantly different, the policy ratios $\rho(a, x)$ attain extreme values leading to a high variance in the OPE estimators. 
To alleviate this high variance, \cite{dudik2014doubly} proposed a Doubly Robust (DR) estimator for OPE which uses a control variate to decrease the variance of conventional OPE estimators. 
%
%
%
%
%
%
%
%
%
%
%
However, DR still relies on policy ratios as importance weights and as a result, also suffers from high variance when the policy shift is large. 
This problem is further exacerbated as the sizes of the action and context spaces grow \citep{sachdeva2020off, saito2022off}.
Chapter \ref{ch:3-mr} of this thesis specifically focuses on this limitation of OPE. 

Besides using control variates (as in DR estimator), several techniques have been proposed to address the variance issues associated with importance weights. 

\paragraph{Trading off variance for bias}
\cite{swaminathan2015counterfactual, swaminathan2015the, chaudhuri2019london} attempt to bound the importance weights within a certain range to prevent them from becoming excessively large. 
Besides this, the Direct Method (DM) \citep{Beygelzimer2008Offset} avoids the use of importance-sampling by estimating the reward function from observational data.
Similarly, Switch-DR \citep{wang2017optimal} aims to circumvent the high variance in conventional DR estimator by switching to DM when the importance weights are large.
However, these approaches introduce a bias-variance trade-off, as clipping the weights or using the learned reward function can introduce bias into the estimates. 
%


\paragraph{Marginalization-based techniques}
Several works explore marginalisation techniques for variance reduction. For example, \cite{saito2022off} propose Marginalized Inverse Propensity Score (MIPS), which considers the marginal shift in the distribution of a lower dimensional embedding of the action space, denoted by $E$, instead of considering the shift in the policies explicitly. 
While this approach reduces the variance, we show in Chapter \ref{ch:3-mr} that MIPS relies on an additional assumption regarding the action embeddings $E$ which does not hold in general.

In addition, various marginalisation ideas have also been proposed in the context of Reinforcement Learning (RL). For example, \cite{liu2018breaking, xie2019advances, kallus2020off} use methods which consider the shift in the marginal distribution of the states, and apply importance weighting with respect to this marginal shift rather than the trajectory distribution. Similarly, \cite{Fujimoto2021deep} use marginalisation for OPE in deep RL, where the goal is to consider the shift in marginal distributions of state and action. Although marginalization is a key trick of these estimators, these techniques are aimed at resolving the curse of horizon, a problem specific to RL.



\subsection{Lack of uncertainty quantification}\label{subsec:uncertainty-quantification}
Most techniques for OPE in contextual bandits focus on evaluating policies based on their \emph{expected} outcomes \citep{uncertainty5, adaptive-ope, uncertainty2, uncertainty3, uncertainty4, doubly-robust}. However, this can be problematic as
methods that are only concerned with the average outcome do not take into account any notions of
uncertainty in the outcome. Therefore, in risk-sensitive settings such as econometrics, where we want
to minimize the potential risks, metrics such as CVaR (Conditional Value at Risk) might be more
appropriate \citep{keramati2020being}. Additionally, when only small sample sizes of observational data are available, the average outcomes under finite data can be misleading, as they are prone to outliers and hence, metrics such as medians or quantiles are more robust in these scenarios \citep{altschuler2019best}. Next, we outline some recent works which tackle this challenge by developing methodologies to account for the uncertainty in off-policy performance using available data. 

\paragraph{Off-policy risk assessment in contextual bandits}
Instead of estimating bounds on the expected outcomes, \cite{risk-assessment, chandak2021universal} establish finite-sample bounds for a general class of metrics (e.g., Mean, CVaR, CDF) on the outcome. Their methods can be used to estimate quantiles of the outcomes under the target policy and are therefore robust to outliers. 
For example, \cite{chandak2021universal} proposed a non-parametric Weighted Importance Sampling (WIS) estimator for the empirical CDF of $Y$ under $\pi^*$,
which can be used to construct predictive intervals on the outcome under target policy. This can help us quantify the range of plausible outcomes $Y$ that are likely to occur if actions are chosen according to target policy $\tar$. However, the resulting bounds do not depend on the context $X$ (i.e., are not adaptive w.r.t. $X$). This can lead to overly conservative intervals, which may not be very informative. In Chapter \ref{ch:4-copp}, we circumvent this problem by proposing a methodology of constructing predictive intervals on $Y$ under target policy $\tar$ which are adaptive w.r.t. context $X$ and are therefore considerably more informative.

%
%
%
%
%
%

\section{Causal considerations for sequential decisions}
Having outlined some of the key limitations of OPE methods in contextual bandits, we now move on to the causal considerations for off-policy decision-making which will set up our contribution in Chapter \ref*{ch:5-causal}. 
Before we dive deeper into this topic, we introduce the sequential decision setting which generalises the contextual bandits framework.

\subsection{Sequential decision setting}
Contextual bandits encapsulate the single-decision regimes where, for each observed context, we take a single action and observe the resulting outcome. This is analogous to a doctor choosing a single treatment for a patient based on their current state. However, many real-world decision-making scenarios involve multiple interventions over time, where each action not only affects the immediate outcome but also influences the context for future decisions. To capture this complexity, we introduce the sequential decision setting in this section. This setting extends the framework of contextual bandits to handle sequential decision-making problems, allowing us to model more complex scenarios where interventions unfold over time and the context evolves dynamically.

We consider a setting with a fixed number of decisions per episode (i.e., a fixed time horizon) $T \in \{1, 2, \ldots\}$. For each $t\in \{0, \ldots, T\}$, we assume that the process gives rise to an observation at time $t$, denoted by $X_t$ which takes values in some space $\mathcal{X}_t \coloneqq \R^{\Xspacedim_\tx}$. 
%
Moreover, at time $t\in  \{1, \ldots, T\}$ a real-world agent (such as a doctor) chooses an action $A_t$ which takes values in some space $\mathcal{A}_t$. The agent's choice of $A_t$ may depend on the historical observations $(X_0, \ldots, X_{t-1})$ or any additional information not captured in historical observations that the agent can access. For example, in a medical context, the observations may consist of a patient's vital signs, and the actions may consist of possible treatments or interventions that the doctor chooses based on patient history.
%
The actions taken up to time $t$, i.e. $(A_1, A_2, \ldots, A_t)$ can influence the future observations $(X_t, X_{t+1}, \ldots, X_T)$. 
This describes the sequential decision setting, of which the contextual bandits are a special case when $T=1$. 
%

\subsection{Causal unidentifiability under unmeasured confounding}\label{subsec:unmeasured-confounding}
Most of the standard OPE methods for contextual bandits can be straightforwardly extended to sequential decision settings \citep{uehara2022reviewoffpolicyevaluationreinforcement}. 
However, like in contextual bandits, these estimators assume no unmeasured confounding (outlined in Assumption \ref{assum:no-unmeasured-confounding}) in the available observational data. 
Informally, in the sequential decision setting, this assumption holds when each action $\A_\tx$ is chosen by the behavioural agent solely on the basis of the information available at time $\tx$ that is actually recorded in the dataset, namely $\X_{0}, \A_1, \X_1, \ldots, \A_{\tx-1}, \X_{\tx-1}$, as well as possibly some additional randomness that is independent of the real-world process, such as the outcome of a coin toss. 
Unobserved confounding is present whenever this does not hold, i.e.\ whenever some unmeasured factor simultaneously influences both the agent's choice of action and the observation produced by the real-world process.
%
This can happen when the real-world agent has access to more information than is captured in the data. 
In such circumstances, the causal effect of a given action sequence may be unidentifiable from the available observational data, making it impossible to accurately estimate the value of a target policy.
To make this concrete, we provide an intuitive illustration of this phenomenon below using a toy example where the available observational data suffers from unmeasured confounding.


\begin{importantresultwithtitle}[title=Toy example: Unmeasured confounding in medical decision-making]\noindent
Suppose that we are interested in estimating the effect of a drug on the weight of patients in a certain population. 
Moreover, assume that this drug interacts with an enzyme that is only present in part of the population.
Denote by $U \in \{0, 1\}$ the presence or absence of the enzyme in a patient, and assume that when $U = 1$ the patient's weight increases after action the drug is administered, and that when $U = 0$ the drug has no effect.
Additionally, suppose that, among the patients whose data we have obtained, the drug was only prescribed to those for whom $U = 1$, perhaps on the basis of some initial lab reports available to the prescriber.
Finally, suppose that these lab results were \emph{not} included in the context $X$ captured in the observational dataset $\D$, so that the value of $U$ for each patient cannot be determined from the data we have available. 

In this setup, since the drug was only administered to patients with $U=1$, it would appear from the data that the drug causes patient weight to increase. 
However, when the drug is administered to the general population, i.e.\ regardless of the value of $U$, we would observe that the drug has no effect on patients for whom $U=0$. Figure \ref{fig:syn_ex_intro} illustrates this discrepancy under a toy model for
this scenario. In this example, since the data $\D$ contains no information about the presence or absence of the enzyme in patients, $U$, it is impossible to determine using the data $\D$ alone how the drug will affect a given population of patients. 

%
    
\end{importantresultwithtitle}

\begin{figure}[t]
    \centering
    \includegraphics[height=3.3cm]{figures/causal/synthetic_example_newest2.pdf}
    \caption{The discrepancy between observational data and interventional behaviour in the presence of unmeasured confounding: the range of outcomes observed in the data for patients who were administered the drug (blue) differs from what \emph{would} be observed if the drug were administered to the general population (red).}
    \label{fig:syn_ex_intro}
\end{figure}

%
%
%
%
%
%
%
%
%
%
%

%


In certain contexts it may be reasonable to assume that the data are unconfounded. For example, in certain situations it may be possible to gather data in a way that specifically guarantees there is no confounding.
Randomised controlled trials, which ensure that each $\A_\tx$ is chosen via a carefully designed randomisation procedure \citep{lavori2004dynamic,murphy2005experimental}, constitute a widespread example of this approach. However, for typical datasets, it is widely acknowledged that the assumption of no unmeasured confounding will rarely hold, and so OPE procedures based on this assumption may yield unreliable results in practice \citep{murphy2003optimal,tsiatis2019dynamic}. 
This is formalised in a foundational result from the causal inference literature, often referred to as the \emph{fundamental problem of causal inference} \citep{holland1986statistics}.
%
\begin{mainresultwithtitle}[title=Fundamental problem of causal inference (informal statement)]\noindent
    The causal effect of an action is not uniquely identified by the observational data distribution without additional assumptions.
\end{mainresultwithtitle}

%
%
%

\paragraph{Partial identification}
Since the precise identifiability of causal effects is not possible in the presence of unmeasured confounding, a notable line of work instead explores partial identification techniques \citep{manski,manski1989anatomy, manski2003partial}. Instead of the point identification of causal effects which may require strong unconfounding assumptions, partial identification typically considers the range of causal effects which may occur in the presence of confounding. For example, \cite{manski} constructs sharp bounds on the causal effects which can be readily estimated using the available observational data. While these bounds do not require any strong assumptions, they can be conservative. 

\paragraph{Sensitivity analysis}
Slightly stronger assumptions yield inferences that may be more powerful but less credible. To this end, \cite{rosenbaum2002observational} proposes a classical model of confounding for a single binary decision setting which posits that the unobserved confounders have a limited influence on the agent's actions in the real world.  \cite{namkoong2020offpolicy} extend this model to the multi-action sequential decision-making setting, and subsequently use this to obtain bounds on the off-policy value.

The Rosenbaum model is also closely related to (albeit different from) the marginal sensitivity model introduced by \cite{tan2006distributional} which also assumes bounds on the strength of unmeasured confounding on agent's actions. Subsequently, \cite{kallus2020minimax} use the marginal sensitivity model to develop a policy learning algorithm which remains robust to unmeasured confounding. However, these models impose assumptions on the strength of unmeasured confounding which can be impossible to verify using observational data alone, and therefore the inferences obtained may be misleading in many cases. 

\paragraph{Proxy causal learning}
This comprises methodologies for estimating the causal effect of actions on outcomes in the presence of unobserved confounding, using \emph{proxy variables} which contain relevant side information about the unmeasured confounders \citep{xu2021deep, tchetgen2020introduction, xu2024kernel}. This usually involves a two-stage regression. First, the relationship between action and proxies is modelled and subsequently, this model is used to learn the causal effect of actions on the outcomes. \cite{kuroki2014measurement} outline the necessary conditions on proxy variables to obtain the true causal effects. While proxy causal learning may be effective in cases where such proxy variables are available, in many real-world settings the available proxy variables may not satisfy the necessary conditions for identification of true causal effects.

Chapter \ref{ch:5-causal} of this thesis considers the challenges posed by unmeasured confounding in sequential decision setting. We propose a set of novel bounds on the causal effects in this setting, which remain valid in the presence of arbitrary unmeasured confounding and rely on minimal assumptions making them highly applicable to a wide variety of real-world settings. 


%
%
%
%
%
%
%
%

%
%
%
%

%
%
%

%

%
%
%
%
%

%

%
%
%
%
%
%
%
%

%
%
%

%
%
%
%
%

%
%

%
%


%

%
%

%
%

%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%

%
%
%
%

%
%
%

%
%
%

%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%


%
%

%
%
%
%
%
%
%

%


%
%
%
%
%
%
%


%
%
%

%



%
%
%
%
%
%

%
%

%
%
%
%
%
%


%
%
%
%

%
%
%
%
%
%

%
%
%
%
%
%

%
%
%

%
    
%


%
%

%
%
%
%
%
%
%
%
%
%


%
%
%
%
%
%

%
%

%
%

%
%

%

%
%

%


%
%

%

%

%

%


%

%

%


%

%

%
%

%

%






%

%

%


%
%
%
%
%
%

\section{Contributions and thesis outline}
Having outlined some of the key challenges associated with off-policy evaluation, we dedicate the rest of this thesis to addressing each of these individually.
Specifically, this thesis is organised as follows:

\paragraph*{Chapter \ref*{ch:3-mr}: Variance reduction \citep{taufiq2023marginal}}
The first challenge we consider is that of high variance in existing OPE estimators based on importance sampling. 
As we mentioned in Section \ref{subsec:high-variance}, this variance is exacerbated in cases where there is low overlap between behaviour and target policies, or where the action or context space is high-dimensional.
To address this challenge, we propose a novel OPE estimator for contextual bandits, the Marginal Ratio (MR) estimator, which uses a marginalisation technique to focus on the shift in the marginal distribution of outcomes $Y$
directly, instead of the policies themselves. 
Unlike the conventional approaches which use policy ratios as importance weights, intuitively, our proposed estimator treats actions $A$ and contexts $X$ as latent variables.
Consequently, the resulting estimator is significantly more robust to the overlap between policies and the sizes of action and/or context spaces.
This chapter also includes extensive theoretical and empirical analyses demonstrating the benefits of the MR estimator compared to the state-of-the-art OPE estimators for contextual bandits.
%

%
%

\paragraph*{Chapter \ref*{ch:4-copp}: Uncertainty quantification \citep{taufiq2022conformal}}
As explained in Section \ref{subsec:uncertainty-quantification}, most OPE methods have focused on the expected outcome of a policy which does not capture the variability of the outcome $Y$.
In addition, many of these methods provide only asymptotic guarantees of validity at best. 
In this chapter, we address these limitations by considering a novel application of conformal prediction \citep{vovk2005algorithmic} to contextual bandits. 
Given data collected under a behavioral policy, we propose \emph{conformal off-policy prediction} (COPP), which can output reliable predictive intervals for the outcome under a new target policy. We provide theoretical finite-sample guarantees without making any additional assumptions beyond the standard contextual bandit setup, and empirically demonstrate the utility of COPP compared with existing methods on synthetic and real-world data.

\paragraph*{Chapter \ref*{ch:5-causal}: Causal considerations \citep{cornish2023causalfalsificationdigitaltwins}}
In this chapter we consider the sequential decision setting, where the available observational data may suffer from unmeasured confounding. 
As mentioned in Section \ref{subsec:unmeasured-confounding}, fundamental results from causal inference mean that in this setting the causal effect of interventions is unidentifiable from the observational distribution.  
To address this challenge, we provide a novel set of longitudinal causal bounds that remain valid under arbitrary unmeasured confounding.

Chapter \ref*{ch:5-causal} focuses on the application of these 
%
bounds for assessing the accuracy of \emph{digital twin models}.
These models are virtual systems designed to predict how a real-world process will evolve in response to interventions. 
To be considered accurate, these models must correctly capture the true causal effects of interventions.
Unfortunately, the causal unidentifiability results mean that observational data cannot be used to certify a twin in this sense if the data are confounded.
To circumvent this, we instead use our proposed causal bounds to find situations in which the twin \emph{is not} correct, and present a general-purpose statistical procedure for doing so.
Our approach yields reliable and actionable information about the twin under only the assumption of an i.i.d.\ dataset of observational trajectories, and remains sound even if the data are confounded.

%
%
%

\paragraph*{Chapter \ref*{ch:6-conclusion}: Conclusion} 
Finally, we conclude by summarising the main findings of the works presented in this thesis. 
In this chapter, we also discuss some of the limitations of our proposed methodologies and mention some interesting avenues for future research arising from these works. 

\section{An overview of work conducted during the DPhil}
In this section, we provide an overview of the research conducted during the doctoral studies by listing the papers which are included in this thesis, as well those which have been omitted.

\subsection{Works included in the thesis}
This is an integrated thesis where each chapter comprises a paper and therefore is self-contained.
These papers are listed here in chronological order for completeness.
\begin{enumerate}
    \item \textbf{Muhammad Faaiz Taufiq}*, Jean-Francois Ton*, Rob Cornish, Yee Whye Teh, and Arnaud Doucet.
    Conformal Off-Policy Prediction in Contextual Bandits. In \textit{\textcolor{purple}{Advances in Neural Information Processing
    Systems, 2022}}. \citep{taufiq2022conformal}
    \item Rob Cornish*, \textbf{Muhammad Faaiz Taufiq}*, Arnaud Doucet, and Chris Holmes. Causal Falsification
    of Digital Twins, 2023. \textit{Preprint}. \citep{cornish2023causalfalsificationdigitaltwins}
    \item \textbf{Muhammad Faaiz Taufiq}, Arnaud Doucet, Rob Cornish, and Jean-Francois Ton. Marginal Density Ratio for Off-Policy Evaluation in Contextual Bandits. In 
    \textit{\textcolor{purple}{Advances in Neural Information Processing Systems, 2023}}. \citep{taufiq2023marginal}
\end{enumerate}

\subsection{Works omitted from the thesis}
For the purposes of coherence and conciseness, several works which were part of the doctoral research have been omitted from this thesis. 
Here, we list these papers along with a brief description in chronological order for completeness. 
\begin{enumerate}
    \item \textbf{Muhammad Faaiz Taufiq}, Patrick Bl√∂baum, and Lenon Minorics. Manifold Restricted
    Interventional Shapley Values. In \textit{\textcolor{purple}{International Conference on Artificial Intelligence and
    Statistics, 2023}}.  \citep{taufiq2023manifold}
    \item  \textbf{Muhammad Faaiz Taufiq}, Jean-Francois Ton, and Yang Liu. Achievable Fairness on your Data
    with Utility Guarantees. \textit{Preprint}. \citep{taufiq2024achievablefairnessdatautility}
    \item Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo, Hao Cheng, Yegor
    Klochkov, \textbf{Muhammad Faaiz Taufiq}, and Hang Li. 
    Trustworthy LLMs: A Survey and Guideline for Evaluating Large Language Models' Alignment, 2023. 
    In \textit{\textcolor{purple}{NeurIPS 2023 Workshop on Socially Responsible Language Modelling Research (SoLaR)}}.    
    \citep{liu2024trustworthyllmssurveyguideline}
\end{enumerate}
In \cite{taufiq2023manifold}, we consider the robustness of Shapley values, which are model-agnostic methods for explaining model predictions.
Many commonly used methods of computing Shapley values, known as off-manifold methods, are sensitive to model behaviour outside the data distribution.
This makes Shapley explanations highly sensitive to off-manifold perturbations of models, resulting in misleading explanations.
To circumvent this problem, we propose \emph{ManifoldShap}, which respects the model‚Äôs domain of validity by restricting model evaluations to the data manifold.
We show, theoretically and empirically, that ManifoldShap is robust to off-manifold perturbations of the model and leads to
more accurate and intuitive explanations than existing state-of-the-art Shapley methods.

Beyond this, \cite{taufiq2024achievablefairnessdatautility} considers fairness within the context of machine learning models. 
In this setting, training models that minimize disparity across different sensitive groups often leads to diminished accuracy, a phenomenon known as the fairness-accuracy tradeoff. 
The severity of this trade-off inherently depends on dataset characteristics such as dataset
imbalances or biases and therefore, using a uniform fairness requirement across diverse datasets
remains questionable. To address this, we present a computationally efficient approach to approximate the fairness-accuracy trade-off curve tailored to individual datasets, backed by rigorous
statistical guarantees.  Crucially, we introduce a novel methodology for quantifying uncertainty in our
estimates, thereby providing practitioners with a robust framework for auditing model fairness
while avoiding false conclusions due to estimation errors.

Finally, \cite{liu2024trustworthyllmssurveyguideline} presents a comprehensive survey of key dimensions that are crucial to consider when assessing the trustworthiness of Large Language Models (LLMs). 
The survey covers seven major categories of LLM trustworthiness: reliability, safety, fairness, resistance to misuse, explainability and reasoning, adherence to social norms, and robustness. 
%
The empirical results presented in this study indicate that, in general, more aligned models tend to perform better in terms of overall trustworthiness. However, the effectiveness of alignment varies across the different trustworthiness categories considered. This highlights the importance of conducting more fine-grained analyses, testing, and making continuous improvements on LLM alignment. 
%