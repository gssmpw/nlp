\section{Discussion}
%
%
%
%
%

In this paper, we proposed an OPE method for contextual bandits called marginal ratio (MR) estimator, which considers only the shift in the marginal distribution of the outcomes resulting from the policy shift. Our theoretical and empirical analysis showed that MR achieves better variance and MSE compared to the current state-of-the-art methods and is more data efficient overall. Additionally, we demonstrated that MR applied to ATE estimation provides more accurate results than most commonly used methods. Next, we discuss limitations of our methodology and possible avenues for future work.

\myparagraph{Limitations}
The MR estimator requires the additional step of estimating $\hat{w}(y)$ which may introduce an additional source of bias in the value estimation. However, $\hat{w}(y)$ can be estimated by solving a simple 1d regression problem, and as we show empirically in Appendix \ref{app:experiments}, MR achieves the smallest bias among all baselines considered in most cases. Most notably, our ablation study in Appendix \ref{subsec:mips-empirical} shows that even when the training data is reasonably small, MR outperforms the baselines considered. 
%


\myparagraph{Future work}
The MR estimator can also be applied to policy optimisation problems, where the data collected using an `old' policy is used to learn a new policy. This approach has been used in Proximal Policy Optimisation (PPO) \citep{schulman2017proximal} for example, which has gained immense popularity and has been applied to reinforcement learning with human feedback (RLHF) \citep{lambert2022illustrating}. We believe that the MR estimator applied to these methodologies could lead to improvements in the stability and convergence of these optimisation schemes, given its favourable variance properties.

