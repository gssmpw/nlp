\section{Related work}
Off-policy evaluation is a central problem both in contextual bandits \citep{dudik2014doubly, wang2017optimal, liu2018breaking, farajtabar2018more, su2019continuous, su2020doubly, kallus2021optimal, metelli2021subgaussian, saito2020open} and in RL \citep{thomas2016data, xie2019advances, kallus2020off, liu2020understanding}. 
Existing OPE methodologies can be broadly categorised into Direct Method (DM), Inverse Probability Weighting (IPW), and Doubly Robust (DR). 
While DM typically has a low variance, it suffers from high bias when the reward model is misspecified \citep{voloshin2021empirical}. 
On the other hand, IPW \citep{horvitz1952generalization} and DR \citep{dudik2014doubly, wang2017optimal, su2020doubly} use policy ratios as importance weights when estimating policy value and suffer from high variance as overlap between behaviour and target policies increases or as the action/context space gets larger \citep{sachdeva2020off, saito2022off}. To circumvent this problem, techniques like weight clipping or normalisation \citep{swaminathan2015counterfactual, swaminathan2015the, chaudhuri2019london} are often employed, however, these can often increase bias.

%



%

%

In contrast to these approaches, \cite{saito2022off} propose MIPS, which considers the marginal shift in the distribution of a lower dimensional embedding of the action space. While this approach reduces the variance associated with IPW, we show in Section \ref{subsec:mips-comparison} that the MR estimator achieves a lower variance than MIPS while not requiring any additional assumptions (like Assumption \ref{assum:indep-mips}).
%
%

In the context of Reinforcement Learning (RL), various marginalisation techniques of importance weights have been used to propose OPE methodologies.
%
%
%
\cite{liu2018breaking, xie2019advances, kallus2020off} use methods which considers the shift in the marginal distribution of the states, and applies importance weighting with respect to this marginal shift rather than the trajectory distribution. Similarly, \cite{Fujimoto2021deep} use marginalisation for OPE in deep RL, where the goal is to consider the shift in marginal distributions of state and action. Although marginalization is a key trick of these estimators, these techniques do not consider the marginal shift in reward as in MR and are aimed at resolving the curse of horizon, a problem specific to RL. Apart from this, \cite{rowland2020conditional} propose a general framework of OPE based on conditional expectations of importance ratios for variance reduction. While their proposed framework includes reward conditioned importance ratios, this is not the main focus and there is little theoretical and empirical comparison of their proposed methodology with existing state-of-the-art methods like DR. 

Finally we note that the idea of approximating the ratio of intractable marginal densities via leveraging the fact that this ratio can be reformulated as the conditional expectation of a ratio of tractable densities is a standard idea in computational statistics \citep{meng1996simulating} and has been exploited more recently to perform likelihood-free inference \citep{brehmer2020mining}. In particular, while  \cite{meng1996simulating} typically approximates this expectation through Markov chain Monte Carlo, \cite{brehmer2020mining} uses regression instead, however without any theory.
%

%
%

%

\begin{comment}
\begin{itemize}
    \item IPW
    \item Double robust methods
    \item Debiased methods 
    \item See related works in \cite{saito2022off}
\end{itemize}
\subsection{Baselines in Contextual Bandits setting}
\begin{itemize}
    \item \cite{thomas2016data, saito2021evaluating, saito2022off,wang2017optimal}
    \item Using ideas from \cite{saito2022off},  we can show that our methodology requires common support of $\pbeh(Y\mid X)$ and $\ptar(Y\mid X)$ which is weaker than requiring common supports of $\beh(A\mid X)$ and $\tar(A\mid X)$, which is needed for IPW estimator. We can show that our estimator has a smaller variance than the methodology proposed by \cite{saito2022off}. Moreover, we should show empirically that, as a consequence of this, our estimator is more robust to heavy-tailed policy ratios. The baselines considered in this paper are the ones to compare against.
    \item \cite{wang2017optimal} proposes a SWITCH estimator for policy evaluation, and show that this estimator is minimax optimal when the hyperparameter $\tau$ is chosen appropriately. Moreover, this estimator is more robust to large (or heavy-tailed) importance weights. It also includes an expression for the variance of the DR estimator which could be useful.
    \item \cite{saito2021evaluating}: In this work, the authors develop Interpretable Evaluation for Offline Evaluation (IEOE), an experimental procedure to evaluate OPE estimatorsâ€™ robustness to changes in hyperparameters and/or evaluation policies in an interpretable manner. This is more of a review paper.
    \item \cite{liu2019triply}: Could be an interesting baseline. The background section of this paper seems interesting. 
\end{itemize}
\subsection{Marginalised Importance Sampling in Reinforcement Learning}
The main difference between this line of work and our idea is that these works consider the marginal distribution over the states. Instead, we propose considering the marginal distribution over rewards directly. 
\begin{itemize}
    \item \cite{liu2018breaking}: The paper originally proposed avoiding the long horizon problem by computing MIS over state distributions rather than entire trajectories. The marginal ratio in these works is different to ours.  
    \item \cite{Fujimoto2021deep}: Applies the MIS idea to deep RL setting: Marginalized importance sampling (MIS), which measures the density ratio between the state-action occupancy of a target policy and that of a sampling distribution, is a promising approach for off-policy evaluation. However, current state-of-the-art MIS methods rely on complex optimization tricks and succeed mostly on simple toy problems. This paper bridges the gap between MIS and deep reinforcement learning by observing that the density ratio can be computed from the successor representation of the target policy. 
    \item \cite{kallus2020off}: The related works section of this paper is very useful. In this work, the authors propose the use of a marginal importance sampling estimator which is somewhat similar to ours (although not the same). Would be useful to read this in depth.
    \item \cite{xie2019advances}: Avoiding the long horizon problem by computing IS over state distributions rather than trajectories - was already introduced in \cite{liu2018breaking}, which the authors cite sufficiently often in the text. However, the approach the authors take to leveraging this idea is original. The estimation technique for the ratios is also different -- the authors propose a recursive estimation technique to estimate the marginal distribution ratios over states. 
    \item \cite{liu2020understanding} also references MIS estimators. 
    \item \cite{rowland2020conditional}: This paper proposes conditional importance sampling for variance reduction in RL setting. The paper considers a general conditioning strategy, based on conditioning importance weights on different statistics. They also mention reward conditional importance sampling. \textbf{Important paper.} 
\end{itemize}
\end{comment}
