\section{Comparison with extensions of the doubly robust estimator}\label{sec:dr-extensions}
In this section, we theoretically investigate the variance of MR against the commonly used extensions of the DR estimator, namely Switch-DR \citep{wang2017optimal} and DR with Optimistic Shrinkage (DRos) \citep{su2020doubly}. At a high level, these estimators seek to reduce the variance of the vanilla DR estimator by considering modified importance weights, thereby trading off the variance for additional bias.
Below, we provide the explicit definitions of these estimators for completeness.

\paragraph{Switch-DR estimator}
The original DR estimator can still have a high variance when the importance weights are large due to a large policy shift. Switch-DR \citep{wang2017optimal} aims to circumvent this problem by switching to DM when the importance weights are large:
\[
\thetaswitch \coloneqq \frac{1}{n} \sum_{i=1}^n \rho(a_i, x_i)\,(y_i - \hat{\mu}(a_i, x_i))\ind(\rho(a_i, x_i) \leq \tau) + \hat{\eta}(\tar),
\]
where $\tau \geq 0$ is a hyperparameter, $\hat{\mu}(a, x) \approx \E[Y \mid X=x, A=a]$ is the outcome model, and 
$$
\hat{\eta}(\tar) = \frac{1}{n} \sum_{i=1}^n \sum_{a'\in \Aspace} \hat{\mu}(a', x_i) \tar(a'\mid x_i) \approx \E_{\tar}[\hat{\mu}(A, X)]
$$
where $a_i^* \sim \tar(\cdot \mid x_i)$.

\paragraph{Doubly Robust with Optimal Shrinkage (DRos)}
DRos proposed by \citep{su2020doubly} uses new weights $\hat{\rho}_\lambda(a_i, x_i)$ which directly minimises sharp bounds on the MSE of the resulting estimator,
\[
\thetadros \coloneqq \frac{1}{n} \sum_{i=1}^n \hat{\rho}_\lambda(a_i, x_i)\,(y_i - \hat{\mu}(a_i, x_i)) + \hat{\eta}(\tar),
\]
where $\lambda \geq 0$ is a pre-defined hyperparameter and $\hat{\rho}_\lambda$ is defined as
\[
\hat{\rho}_\lambda(a, x) \coloneqq \frac{\lambda}{\rho^2(a, x) + \lambda}\, \rho(a, x).
\]
When $\lambda = 0$, $\hat{\rho}_\lambda(a, x) = 0$ leads to DM, whereas as $\lambda \rightarrow \infty$, $\hat{\rho}_\lambda(a, x) \rightarrow \rho(a, x)$ leading to DR.

More generally, both of these estimators can be written as follows:
\[
\hat{\theta}_{\textup{DR}}^{\tilde{\rho}} \coloneqq \frac{1}{n} \sum_{i=1}^n \tilde{\rho}(a_i, x_i)\,(y_i - \hat{\mu}(a_i, x_i)) + \hat{\eta}(\tar).
\]
Here, when $\tilde{\rho}(a, x) = \rho(a, x)\ind(\rho(a_i, x_i) \leq \tau)$, we recover the Switch-DR estimator and likewise when $\tilde{\rho}(a, x) = \hat{\rho}_\lambda(a, x)$, we recover DRos. 

\subsection{Variance comparison with the DR extensions}
Next, we provide a theoretical result comparing the variance of the MR estimator with these DR extension methods.
\begin{proposition}\label{prop:var_dr_extensions}
    When the weights $w(y)$ are known exactly and the outcome model is exact, i.e., $\hat{\mu}(a, x) = \mu(a, x) = \E[Y \mid X=x, A=a]$ in the DR estimator $\hat{\theta}_{\textup{DR}}^{\tilde{\rho}}$ defined above,
    \begin{align*}
    &\Vbeh[\hat{\theta}_{\textup{DR}}^{\tilde{\rho}}] - \Vbeh[\thetamr]\\
    &\qquad \qquad \qquad  \geq \frac{1}{n} \Ebeh \left[ \Vbeh\left[ \rho(A, X) \mid Y \right]\, Y^2 -  \Vbeh\left[ \rho(A, X)\mu(A, X) \mid X \right] \right] - \Delta,
\end{align*}
where $\Delta \coloneqq \frac{1}{n}\Ebeh\left[(\rho^2(A, X) - \tilde{\rho}^2(A, X))\,\V[Y\mid X, A]\right]$. 
\end{proposition}

\begin{proof}[Proof of Proposition \ref{prop:var_dr_extensions}]
Using the fact that $\hat{\mu}(a, x) = \mu(a, x) $ and the law of total variance, we get that
\begin{align*}
    n\,\Vbeh[\hat{\theta}_{\textup{DR}}^{\tilde{\rho}}] &= \Vbeh[\tilde{\rho}(A, X)\,(Y -\hat{\mu}(A, X)) + \sum_{a'\in \Aspace}\hat{\mu}(a', X)\tar(a'\mid X) ]\\
    &= \Vbeh[\tilde{\rho}(A, X)\,(Y -\hat{\mu}(A, X)) + \Etar[\hat{\mu}(A, X)\mid X] ]\\
    &= \Vbeh[\tilde{\rho}(A, X)\,(Y -\mu(A, X)) + \Etar[\mu(A, X)\mid X] ]\\
    &= \Vbeh[\Ebeh[\tilde{\rho}(A, X)\,(Y -\mu(A, X)) + \Etar[\mu(A, X)\mid X] \mid X, A]] \\
    &\qquad+ \Ebeh[\Vbeh[\tilde{\rho}(A, X)\,(Y -\mu(A, X)) + \Etar[\mu(A, X)\mid X]\mid X, A]]\\
    &= \Vbeh[\Etar[\mu(A, X)\mid X]] + \Ebeh[\tilde{\rho}^2(A, X)\V[Y\mid X, A]]\\
    &= \Vbeh[\Etar[\mu(A, X)\mid X]] + \Ebeh[\rho^2(A, X)\,\V[Y\mid X, A]] \\
    &\qquad+ \underbrace{\Ebeh[(\tilde{\rho}^2(A, X) - \rho^2(A, X))\,\V[Y\mid X, A]]}_{-n\,\Delta}\\
    &= \Vbeh[\Ebeh[\rho(A, X)\,\mu(A, X)\mid X]] + \Ebeh[\rho^2(A, X)\,\V[Y\mid X, A]] - n\,\Delta.
\end{align*}
    Again, using the law of total variance we can rewrite the second term on the RHS above as,
    \begin{align*}
        &\Ebeh[\rho^2(A, X)\,\V[Y\mid X, A]] \\
        &\quad= \Vbeh[\rho(A, X)\, Y] - \Vbeh[\rho(A, X)\,\mu(A, X)]\\
        &\quad= \Vbeh[\Ebeh[\rho(A, X)\mid Y]\, Y] + \Ebeh[\Vbeh[\rho(A, X)\mid Y]\,Y^2] \\
        &\qquad- \Vbeh[\rho(A, X)\,\mu(A, X)]\\
        &\quad= \Vbeh[w(Y)\, Y] + \Ebeh[\Vbeh[\rho(A, X)\mid Y]\,Y^2] - \Vbeh[\rho(A, X)\,\mu(A, X)]\\
        &\quad= n\,\Vbeh[\thetamr] + \Ebeh[\Vbeh[\rho(A, X)\mid Y]\,Y^2] - \Vbeh[\rho(A, X)\,\mu(A, X)].
    \end{align*}
    Putting this together, we get that
    \begin{align*}
        &n\,\Vbeh[\hat{\theta}_{\textup{DR}}^{\tilde{\rho}}] \\
        &\quad= n\,\Vbeh[\thetamr] + \Ebeh[\Vbeh[\rho(A, X)\mid Y]\,Y^2] - \Vbeh[\rho(A, X)\,\mu(A, X)] \\
        &\qquad+ \Vbeh[\Ebeh[\rho(A, X)\,\mu(A, X)\mid X]] - n\,\Delta\\
        &\quad= n\,\Vbeh[\thetamr] + \Ebeh[\Vbeh[\rho(A, X)\mid Y]\,Y^2] - \Ebeh[\Vbeh[\rho(A, X)\,\mu(A, X)\mid X]] - n\,\Delta,
    \end{align*}
    where in the last step above, we again use the law of total variance. Rearranging the above leads us to the result. 
\end{proof}
\paragraph{Intuition}
Note that for both of the DR extensions under consideration, the modified ratios $\tilde{\rho}(a, x)$ satisfy $0\leq \tilde{\rho}(a, x)\leq \rho(a, x)$ and hence $\Delta \geq 0$ (using the definition of $\Delta$ in Proposition \ref{prop:var_dr_extensions}).
When the modified ratios $\tilde{\rho}(a, x)$ are `close' to the true policy ratios $\rho(a, x)$, then using the definition of $\Delta$, we have that $\Delta \approx 0$. In this case, the result above provides a similar intuition to Proposition \ref{prop:var_dr} in the main text. Specifically, in this case we have that if $\Vbeh\left[ \rho(A, X)\,Y \mid Y \right]$ is greater than $\Vbeh\left[ \rho(A, X)\,\mu(A, X) \mid X \right]$ on average, the variance of the MR estimator will be less than that of the DR extension under consideration. 
%
%
%
Intuitively, this will occur when the dimension of context space $\Xspace$ is high because in this case the conditional variance over $X$ and $A$, $\Vbeh\left[\rho(A, X)\,Y \mid Y \right]$ is likely to be greater than the conditional variance over $A$, $\Vbeh\left[ \rho(A, X)\,\mu(A, X) \mid X \right]$.

In contrast if the modified ratios $\tilde{\rho}(a, x)$ differ substantially from $\rho(a, x)$, then $\Delta$ will be large and the variance of MR may be higher than that of the resulting DR extension. However, this comes at the cost of significantly higher bias in the DR extension and consequently MSE of the DR extension will be high in this case.
