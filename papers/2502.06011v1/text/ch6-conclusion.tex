\chapter{\label{ch:6-conclusion}Conclusion and Future Work} 

\minitoc

\section{Discussion}
%

Before deploying a decision-making policy to production, it is usually important to understand the
plausible range of outcomes that it may produce. However, due to resource or ethical constraints, it is
often not possible to obtain this understanding by testing the policy directly in the real-world. In such
cases we have to rely on off-policy evaluation (OPE), which uses observational data collected under a different behavioural policy to evaluate the target policy in some way. 
In this thesis, we have considered some of the challenges posed by the current OPE methodologies and proposed novel solutions to each of these individually.
We have also demonstrated the practical utility of these solutions by applying them to a range of real-world problems involving large scale datasets.
To be more specific, below we provide a brief summary of the challenges tackled in this thesis:
\begin{itemize}
    \item In Chapter \ref*{ch:3-mr} we consider the problem of variance reduction in OPE estimators. To address this, we propose the Marginal Ratio (MR) estimator, which uses a marginalization technique to provide a more efficient and robust estimator for contextual bandits, and may also be of interest in other domains such as causal inference. 
    \item Next, in Chapter \ref*{ch:4-copp} we provide a novel methodology of uncertainty quantification in off-policy outcomes based on conformal prediction \citep{vovk2005algorithmic}. Our proposed technique can help practitioners quantify the plausible range of outcomes that are likely to occur under the target policy, and comes with sound finite-sample probabilistic guarantees.
    \item Finally, in Chapter \ref*{ch:5-causal} we explore the case when the assumption of no unmeasured confounding (needed for the existing OPE methodologies) is violated. 
    We provide a set of novel causal bounds which remain valid in this case, and subsequently use these bounds to develop a procedure for robust assessment of digital twin models using observational data which remains valid under only the assumption of an i.i.d. dataset of observational trajectories. 
    %
\end{itemize}
%
%

\section{Limitations}
Here, we outline some of the limitations of the methodologies described in this thesis. 

\paragraph*{Distributional shift in data generating mechanism}
Our methodologies highlighted in this thesis implicitly assume that the data generating process remains unchanged between testing and deployment times. 
Technically, for contextual bandits this assumption means that the conditional distribution of $Y$ given $(X, A)$ does not shift when the target policy is deployed. 
If this distribution changes at deployment time, then so too does the distribution of outcomes that \emph{would} be observed under the target policy. 
If this shift is significant enough, our methodologies may yield misleading results.  

\paragraph*{Estimation errors}
The techniques mentioned in Chapters \ref*{ch:3-mr} and \ref*{ch:4-copp} involve an additional estimation of marginal density ratios for importance sampling. 
While we outline straightforward regression methodologies for estimating these importance ratios directly, this step may still introduce an additional source of bias in the value estimation. 

\paragraph*{Scalability limitations}
Increasing data dimensionality may pose additional challenges for our solutions, especially those described in Chapters \ref*{ch:4-copp} and \ref*{ch:5-causal}. 
For example, in Chapter \ref*{ch:4-copp}, the estimation of importance ratios for our conformal off-policy prediction (COPP) algorithm may become more challenging when $(X, A)$ is high-dimensional, thereby yielding biased results.
Likewise in Chapter \ref*{ch:5-causal}, the procedure we used in our case study to choose the hypothesis parameters was ad hoc, which may not scale to high-dimensional datasets.
For scalability, it would likely be necessary to obtain these parameters via a more automated procedure.

\section{Directions for future work}
Our work in this thesis opens up several interesting avenues for future research. We highlight some of these below.

\paragraph*{Off-policy learning}
This thesis has largely focused on robust off-policy assessment methodologies. However, our findings are highly applicable to robust policy optimisation problems, where the data collected using an `oldâ€™ policy is used to learn a new policy. 
Proximal Policy Optimisation \citep{schulman2017proximal} is one such approach which has gained immense popularity and has been applied to reinforcement learning with human feedback \citep{lambert2022illustrating}. 
We believe that our MR estimator proposed in Chapter \ref*{ch:3-mr} applied to these methodologies could lead to improvements in the stability and convergence
of these optimisation schemes, given its favourable variance properties. 
Similarly, our conformal off-policy prediction (COPP) algorithm when applied to off-policy learning could be a
step towards robust policy learning by optimising the worst-case outcome \citep{stutz2021learning}.

\paragraph*{Addressing the curse of horizon in sequential decision-making}
Chapters \ref*{ch:3-mr} and \ref*{ch:4-copp} of this thesis specifically consider OPE in contextual bandits. 
This setting offers a strong foundational framework for conducting rigorous theoretical and empirical analyses, 
however, it would be interesting to extend the application of these methodologies to sequential decision frameworks.
While some follow-up works have attempted to apply our COPP algorithm to Markov Decision Processes \citep{foffano2023conformal, zhang2023conformal, kuipers2024conformal}, the obtained confidence sets become increasingly conservative with increasing time horizon. 
It is worth exploring methodologies for obtaining intervals which remain valid and informative even in sequential decision settings with large time horizons.  

%

\paragraph*{Application to transfer learning}
Finally, our solutions in Chapters \ref*{ch:3-mr} and \ref*{ch:4-copp} involves learning importance ratios which may also be of interest in other domains beyond OPE. 
One such area is \emph{transfer learning} which considers cases where the testing data distribution is different from the training data distribution. 
Classical transfer learning methods rely on importance weighting to handle the distribution mismatch \citep{SHIMODAIRA2000Improving, sugiyama2007covariate, huang2007correcting, sugiyama2008direct,lu2021rethinking}. 
Our proposed regression techniques may be of interest for obtaining these weights efficiently in high-dimensional datasets in this setting.  

%