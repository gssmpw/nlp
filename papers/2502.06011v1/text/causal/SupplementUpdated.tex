\section{Notation} \label{sec:notation}

\begin{tabularx}{\linewidth}{l X}
$z_{\tx:\tx'}$ & The sequence of elements $(z_\tx, \ldots, z_{\tx'})$ (or the empty sequence when $\tx > \tx'$) \\
$\mathcal{Z}_{\tx:\tx'}$ (where each $\mathcal{Z}_{i}$ is a set) & The cartesian product $\mathcal{Z}_{\tx} \times \cdots \times \mathcal{Z}_{\tx'}$ (or the empty set when $\tx > \tx'$) \\
$Z_{\tx:\tx'}(\ax_{1:\tx'})$ & The sequence of potential outcomes $Z_\tx(\ax_{1:\tx}), \ldots, Z_{\tx'}(\ax_{1:\tx'})$ (or the empty sequence when $\tx > \tx'$) \\
$\Law[Z]$ & The distribution of the random variable $Z$ \\
$\Law[Z \mid M]$ & The conditional distribution of $Z$ given $M$, where $M$ is either an event or a random variable \\
%
$Z \eqas Z'$ & The random variables $Z$ and $Z'$ are almost surely equal, i.e.\ $\Prob(Z = Z') = 1$ \\
$Z \ci Z'$ & The random variables $Z$ and $Z'$ are independent \\
$Z \ci Z' \mid Z''$ & The random variables $Z$ and $Z'$ are conditionally independent given the random variable $Z''$ \\
$\ind(E)$ & Indicator function of some event $E$ %
%
%
%
%
%
%
%
\end{tabularx}

%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\section{Proof of Proposition \ref{prop:interventional-correctness-alternative-characterisation} (unconditional form of interventional correctness)} \label{sec:unconditional-interventional-correctness-proof}

%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%

\begin{proof}
Fix any choice of $\ax_{1:\T} \in \Aspace_{1:\T}$.
By disintegrating $\Law[\X_0, \Xt_{1:\T}(\X_0, \ax_{1:\T})]$ and $\Law[\X_{0:\T}(\ax_{1:\T})]$ along their common $\Xspace_0$-marginal (which is namely $\Law[\X_0]$), it holds that
\begin{equation} \label{eq:interventional-correctness-alternative}
    \Law[\X_0, \Xt_{1:\T}(\X_0, \ax_{1:\T})] = \Law[\X_{0:\T}(\ax_{1:\T})]
\end{equation}
if and only if
\begin{equation} \label{eq:joint-interventional-correctness-proof-intermediate-step}
    \Law[\Xt_{1:\T}(\X_0, \ax_{1:\T}) \mid \X_0 = \xx_0] = \Law[\X_{1:\T}(\ax_{1:\T}) \mid \X_0 = \xx_0]
\end{equation}
for $\Law[\X_0]$-almost all $\xx_0 \in \Xspace_0$.
%
But now, our definition of $\Xt_{1:\T}(\xx_0, \ax_{1:\T})$ in terms of $\twinfunction_\tx$ and $\twinnoise_{1:\tx}$ means we can write $\Xt_{1:\T}(\X_0, \ax_{1:\T}) = \boldsymbol{\twinfunction}(\X_0, \ax_{1:\T}, \twinnoise_{1:\T})$,
where
\[
    \boldsymbol{\twinfunction}(\xx_0, \ax_{1:\T}, \ux_{1:\T}) \coloneqq (\twinfunction_1(\xx_0, \ax_1, \ux_1), \ldots, \twinfunction_\T(\xx_0, \ax_{1:\T}, \ux_{1:\T})).
\]
%
For all $\xx_0 \in \Xspace_0$ and measurable $\B_{1:\T} \subseteq \Xspace_{1:\T}$, we then have
\begin{align*}
    \Law[\Xt_{1:\T}(\xx_0, \ax_{1:\T})](\B_{1:\T}) &= \E[\ind(\boldsymbol{\twinfunction}(\xx_0, \ax_{1:\T}, \twinnoise_{1:\T}) \in \B_{1:\T})] \\
    &= \int \ind(\boldsymbol{\twinfunction}(\xx_0, \ax_{1:\T}, \ux_{1:\T}) \in \B_{1:\T}) \, \Law[\twinnoise_{1:\T}](\dee \ux_{1:\T}).
\end{align*}
It is standard to show that the right-hand side is a Markov kernel in $\xx_0$ and $\B_{1:\T}$.
Moreover, for any measurable $\B_0 \subseteq \Xspace_0$, we have
\begin{align*}
    &\int_{\B_0} \Law[\Xt_{1:\T}(\xx_0, \ax_{1:\T})](\B_{1:\T}) \, \Law[\X_0](\dee \xx_0) \\
        &\qquad= \int_{\B_0} \left[\int \ind(\boldsymbol{\twinfunction}(\xx_0, \ax_{1:\T}, \ux_{1:\T}) \in \B_{1:\T}) \, \Law[\twinnoise_{1:\T}](\dee \ux_{1:\T}) \right] \, \Law[\X_0](\dee \xx_0) \\
        &\qquad= \int \ind(\xx_0 \in \B_0, \boldsymbol{\twinfunction}(\xx_0, \ax_{1:\T}, \ux_{1:\T}) \in \B_{1:\T}) \, \Law[\X_0, \twinnoise_{1:\T}](\dee \xx_0, \dee \ux_{1:\T}) \\
        &\qquad= \Law[\X_0, \Xt_{1:\T}(\X_0, \ax_{1:\T})](\B_{0:\T}),
\end{align*}
where the second step follows because $\X_0 \ci \twinnoise_{1:\T}$.
It therefore follows that $(\xx_0, \B_{1:\T}) \mapsto \Law[\Xt_{1:\T}(\xx_0, \ax_{1:\T})](\B_{1:\T})$ is a regular conditional distribution of $\Xt_{1:\T}(\X_0, \ax_{1:\T})$ given $\X_0$, i.e.\
\[
    \Law[\Xt_{1:\T}(\xx_0, \ax_{1:\T})] = \Law[\Xt_{1:\T}(\X_0, \ax_{1:\T}) \mid \X_0 = \xx_0] \qquad \text{for $\Law[\X_0]$-almost all $\xx_0 \in \Xspace_0$.}
\]
Substituting this into \eqref{eq:joint-interventional-correctness-proof-intermediate-step}, we see that \eqref{eq:interventional-correctness-alternative} holds if and only if
\[
    \Law[\Xt_{1:\T}(\xx_0, \ax_{1:\T})] = \Law[\X_{1:\T}(\ax_{1:\T}) \mid \X_0 = \xx_0]
\]
for $\Law[\X_0]$-almost all $\xx_0 \in \Xspace_0$.
The result now follows since $\ax_{1:\T}$ was arbitrary.
\end{proof}

\section{Online prediction} \label{sec:online-prediction}

\subsection{Correctness in the online setting}

A distinguishing feature of many digital twins is their ability to integrate real-time information obtained from sensors in their environment \citep{barricelli2019survey}.
It is therefore relevant to consider a setting in which a twin is used repeatedly to make a sequence of predictions over time, each time taking all previous information into account.
One way to formalize this is to instantiate our model for the twin at each timestep.
For example, we could represent the predictions made by the twin at $\tx = 0$ after observing initial covariates $\xx_0$ as potential outcomes $(\Xt^1_{1:\T}(\xx_0, \ax_{1:\T}) : \ax_{1:\T} \in \Aspace_{1:\T})$, similar to what we did in the main text.
We could then represent the predictions made by the twin after some action $\ax_1$ is taken and an additional observation $\xx_1$ is made via potential outcomes $(\Xt^2_{2:\T}(\xx_{0:1}, \ax_{1:\T}) : \ax_{2:\T} \in \Aspace_{2:\T})$.
More generally, for $\tx \in \{1, \ldots, \T\}$, we could introduce potential outcomes $(\Xt^{\tx}_{\tx:\T}(\xx_{0:\tx-1}, \ax_{1:\T}) : \ax_{\tx:\T} \in \Aspace_{\tx:\T})$ to represent the predictions that the twin would make at time $\tx$ after the observations $\xx_{0:\tx-1}$ are made and the actions $\ax_{1:\tx-1}$ are taken.

This extended model requires a new definition of correctness than our Definition \ref{eq:interventional-correctness} from the main text.
%
%
%
%
%
%
A natural approach is to say that the twin is correct in this new setting if
\begin{equation}
    \Law[\Xt_{\tx:\T}^\tx(\xx_{0:\tx-1}, \ax_{1:\T})]
        = \Law[\X_{\tx:\T}(\ax_{1:\T}) \mid \X_{0:\tx-1}(\ax_{1:\tx-1}) = \xx_{0:\tx-1}] \label{eq:online-interventional-correctness-def}
\end{equation}
for all $\tx \in \{1, \ldots, \T\}$, $\ax_{1:\T} \in \Aspace_{1:\T}$, and $\Law[\X_{0:\tx-1}(\ax_{1:\tx-1})]$-almost all $\xx_{0:\tx-1} \in \Xspace_{0:\tx-1}$.
%
A twin with this property would at each step be able to accurately simulate the future in light of previous information, use this to choose a next action to take, observe the result of doing so, and then repeat.
It is possible to show that \eqref{eq:online-interventional-correctness-def} holds if and only if we have
\begin{align*}
    \Law[\Xt^1_{1:\T}(\xx_0, \ax_{1:\T})] &= \Law[\X_{1:\T}(\ax_{1:\T}) \mid \X_{0}=\xx_0] \\
    \Law[\Xt^\tx_{\tx:\T}(\xx_{0:\tx-1}, \ax_{1:\T})]
        &= \Law[\Xt^1_{\tx:\T}(\xx_0, \ax_{1:\T}) \mid \Xt_{1:\tx-1}^1(\xx_0, \ax_{1:\tx-1}) = \xx_{1:\tx-1}]
\end{align*}
for all $\tx \in \{1, \ldots, \T\}$, $\ax_{1:\T} \in \Aspace_{1:\T}$, $\Law[\X_0]$-almost all $\xx_0 \in \Xspace_0$, and $\Law[\Xt_{1:\tx-1}^1(\xx_0, \ax_{1:\tx-1})]$-almost all $\xx_{1:\tx-1} \in \Xspace_{1:\tx-1}$.
The first condition here says that $\Xt^1_{1:\T}(\xx_0, \ax_{1:\T})$ must be interventionally correct in the sense of Definition \ref{eq:interventional-correctness} from the main text.
The second condition says that the predictions made by the twin across different timesteps must be internally consistent with each other insofar as their conditional distributions must align.
This holds automatically in many circumstances, such as if the predictions of the twin are obtained from a Bayesian model (for example), and otherwise could be checked numerically given the ability to run simulations from the twin, without the need to obtain data or refer to the real-world process in any way.
As such, the problem of assessing the correctness of the twin in this new sense primarily reduces to the problem of assessing the correctness of $\Xt^1_{1:\T}(\xx_0, \ax_{1:\T})$ in the sense of Definition \ref{eq:interventional-correctness} in the main text, which motivates our focus on that condition.
%

%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%

%

%

%
%

%
%
%
%
%
%
%
%

%

\subsection{Alternative notions of online correctness} \label{sec:online-correctness-alternative-notion-supp}

An important and interesting subtlety arises in this context that is worth noting.
In general it does not follow that a twin correct in the sense of \eqref{eq:online-interventional-correctness-def} satisfies
\begin{equation}
    \Law[\Xt_{\tx:\T}^\tx(\xx_{0:\tx-1}, \ax_{1:\T})] \
        = \Law[\X_{\tx:\T}(\ax_{1:\T}) \mid \X_{0:\tx-1}(\ax_{1:\tx-1}) = \xx_{0:\tx-1}, \A_{1:\tx-1} = \ax_{1:\tx-1}] \label{eq:online-interventional-correctness-alt}
\end{equation}
for all $\ax_{1:\T} \in \Aspace_{1:\T}$, and $\Law[\X_{0:\tx-1}(\ax_{1:\tx-1}) \mid \A_{1:\tx-1} = \ax_{1:\tx-1}]$-almost all $\xx_{0:\tx-1} \in \Xspace_{0:\tx-1}$, 
since in general it does not hold that 
\begin{multline*}
    \Law[\X_{\tx:\T}(\ax_{1:\T}) \mid \X_{0:\tx-1}(\ax_{1:\tx-1}) = \xx_{0:\tx-1}]
        = \Law[\X_{\tx:\T}(\ax_{1:\T}) \mid \X_{0:\tx-1}(\ax_{1:\tx-1}) = \xx_{0:\tx-1}, \A_{1:\tx-1} = \ax_{1:\tx-1}].
\end{multline*}
for all $\ax_{1:\T} \in \Aspace_{1:\T}$ and $\Law[\X_{0:\tx-1}(\ax_{1:\tx-1}) \mid \A_{1:\tx-1} = \ax_{1:\tx-1}]$-almost all $\xx_{0:\tx-1} \in \Xspace_{0:\tx-1}$ unless the actions $\A_{1:\tx-1}$ are unconfounded.
(Here as usual $\A_{1:\T}$ denotes the actions of a behavioural agent; see Section \ref{sec:data-driven-twin-assessment} of the main text.)
In other words, a twin that is correct in the sense of \eqref{eq:online-interventional-correctness-def} will make accurate predictions at time $\tx$ when every action taken before time $\tx$ was unconfounded (as occurs for example when the twin is directly in control of the decision-making process), but in general not when certain taken actions before time $\tx$ were chosen by a behavioural agent with access to more context than is available to the twin (as may occur for example when the twin is used as a decision-support tool).
However, should it be desirable, our framework could be extended to encompass the alternative condition in \eqref{eq:online-interventional-correctness-alt} by relabelling the observed history $(\X_{0:\tx-1}(\A_{1:\tx-1}), \A_{1:\tx-1})$ as $\X_0$, and then assessing the correctness of the potential outcomes $\Xt_{\tx:\T}^\tx(\xx_{0:\tx-1}, \ax_{1:\T})$ in the sense of Definition \ref{eq:interventional-correctness} from the main text.

%
%

%

%
%

%
%

%
%

Overall, the ``right'' notion of correctness in this online setting is to some extent a design choice.
We believe our causal approach to twin assessment provides a useful framework for formulating and reasoning about these possibilities, and consider the investigation of assessment strategies for additional usage regimes to be an interesting direction for future work.

\section{Proof of Theorem \ref{prop:nonidentifiability} (interventional distributions are not identifiable)} \label{sec:non-identifiability-result-proof-supp}

It is well-known in the causal inference literature that the interventional behaviour of the real-world process cannot be uniquely identified from observational data.
For completeness, we now provide a self-contained proof of this result in our notation.
Our statement here is lengthier than Theorem \ref{prop:nonidentifiability} in the main text in order to clarify what is meant by ``uniquely identified'': intuitively, the idea is that there always exist distinct families of potential outcomes whose interventional behaviours differ and yet give rise to the same observational data.
%
%

%
\begin{theorem}
    Suppose we have $\ax_{1:\T} \in \Aspace_{1:\T}$ such that $\Prob(\A_{1:\T} \neq \ax_{1:\T}) > 0$.
    %
    Then there exist potential outcomes $(\tilde{\X}_{0:\T}(\ax_{1:\T}') : \ax_{1:\T}' \in \Aspace_{1:\T})$ such that %
    \begin{equation} \label{eq:nonidentifiability-proof-almost-sure-equality}
        (\tilde{\X}_{0:\T}(\A_{1:\T}), \A_{1:\T}) \eqas (\X_{0:\T}(\A_{1:\T}), \A_{1:\T}).
    \end{equation}
    but for which $\Law[\tilde{\X}_{0:\T}(\ax_{1:\tx})] \neq \Law[\X_{0:\T}(\ax_{1:\tx})]$.
\end{theorem}
%

\begin{proof}
    Our assumption that $\Prob(\A_{1:\T} \neq \ax_{1:\T}) > 0$ means there must exist some $\tx \in \{1, \ldots, \T\}$ such that $\Prob(\A_{1:\tx} \neq \ax_{1:\tx}) > 0$.
    Since $\Xspace_\tx = \R^{\Xspacedim_\tx}$, we may also choose some $\xx_\tx \in \Xspace_\tx$ with $\Prob(\X_\tx(\ax_{1:\tx}) = \xx_\tx \mid \A_{1:\tx} \neq \ax_{1:\tx}) \neq 1$.
    %
    %
    Then, for each $\sx \in \{0, \ldots, \T\}$ and $\ax_{1:\sx}' \in \Aspace_{1:\sx}$, define
    \[
         \tilde{\X}_{\sx}(\ax_{1:\sx}') \coloneqq \begin{cases}
            \ind(\A_{1:\tx} = \ax_{1:\tx}) \, \X_{\tx}(\ax_{1:\tx}) + \ind(\A_{1:\tx} \neq \ax_{1:\tx}) \, \xx_\tx & \text{if $\sx = \tx$ and $\ax_{1:\sx}' = \ax_{1:\tx}$} \\
            \X_{\sx}(\ax_{1:\sx}') & \text{otherwise},
         \end{cases}
    \]
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    It is then easily checked that \eqref{eq:nonidentifiability-proof-almost-sure-equality} holds, but
    \begin{align*}
        \Law[\tilde{\X}_{\tx}(\ax_{1:\tx})] &= \Law[\tilde{\X}_{\tx}(\ax_{1:\tx}) \mid \A_{1:\tx} = \ax_{1:\tx}] \, \Prob(\A_{1:\tx} = \ax_{1:\tx}) + \Law[\tilde{\X}_{\tx}(\ax_{1:\tx}) \mid \A_{1:\tx} \neq \ax_{1:\tx}] \, \Prob(\A_{1:\tx} \neq \ax_{1:\tx}) \\
        &= \Law[X_{\tx}(\ax_{1:\tx}) \mid \A_{1:\tx} = \ax_{1:\tx}] \, \Prob(\A_{1:\tx} = \ax_{1:\tx}) + \mathrm{Dirac}(\xx_\tx) \, \Prob(\A_{1:\tx} \neq \ax_{1:\tx}) \\
        &\neq \Law[X_{\tx}(\ax_{1:\tx}) \mid \A_{1:\tx} = \ax_{1:\tx}] \, \Prob(\A_{1:\tx} = \ax_{1:\tx}) + \Law[\X_{\tx}(\ax_{1:\tx}) \mid \A_{1:\tx} \neq \ax_{1:\tx}] \, \Prob(\A_{1:\tx} \neq \ax_{1:\tx}) \\
        &= \Law[X_{\tx}(\ax_{1:\tx})],
    \end{align*}
    from which the result follows.
    %
    %
    %
    %
    %
    %
    %
    %
\end{proof}

%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\section{Deterministic potential outcomes are unconfounded} \label{eq:deterministic-potential-outcomes-are-unconfounded}

In this section we expand on our earlier claim that, if the real-world process is deterministic, then the observational data is unconfounded.
We first make this claim precise.
By ``deterministic'', we mean that there exist measurable functions $\gx_\tx$ for $\tx \in \{1, \ldots, \T\}$ such that
\begin{equation} \label{eq:potential-outcomes-are-deterministic}
    \X_\tx(\ax_{1:\tx}) \eqas \gx_\tx(\X_{0:\tx-1}(\ax_{1:\tx-1}), \ax_{1:\tx}) \qquad \text{for all $\tx \in \{1, \ldots, \T\}$ and $\ax_{1:\tx} \in \Aspace_{1:\tx}$.}
\end{equation}
By ``unconfounded'', we mean that the \emph{sequential randomisation assumption (SRA)} introduced by Robins \citep{robins1986new} holds, i.e.\
\begin{equation} \label{eq:actions-are-unconfounded}
    (\X_{\sx}(\ax_{1:\sx}) : \sx \in \{1, \ldots, \T\}, \ax_{1:\sx} \in \Aspace_{1:\sx}) \ci \A_\tx \mid \X_{0:\tx-1}(\A_{1:\tx-1}), \A_{1:\tx-1} \qquad \text{for all $\tx \in \{1, \ldots, \T\}$},
\end{equation}
where $\ci$ denotes conditional independence.
Intuitively, this says that, apart from the historical observations $(\X_{0:\tx-1}(\A_{1:\tx-1}), \A_{1:\tx-1})$, any additional factors that influence the agent's choice of action $\A_\tx$ are independent of the behaviour of the real-world process.
The SRA provides a standard formulation of the notion of unconfoundedness in longitudinal settings such as ours (see \cite[Chapter 5]{tsiatis2019dynamic} for a review).

It is now a standard exercise to show that \eqref{eq:potential-outcomes-are-deterministic} implies \eqref{eq:actions-are-unconfounded}.
We include a proof below for completeness.
Key to this is the following straightforward Lemma.

\begin{lemma}\label{lem:determinism_conditional_independence}
Suppose $U$ and $V$ are random variables such that, for some measurable function $g$, it holds that $U \eqas g(V)$.
Then, for any other random variable $W$, we have
\[
    U \ci W \mid V.
\]
\end{lemma}

\begin{proof}
By standard properties of conditional expectations, for any measurable sets $S_1$ and $S_2$, we have almost surely
\begin{align*}
    \Prob(U \in S_1, W \in S_2 \mid V) &= \E[\ind(g(V) \in S_1) \, \ind(W \in S_2) \mid V] \\
    %
    &= \ind(g(V) \in S_1) \, \E[\ind(W \in S_2) \mid V] \\
    &= \E[\ind(U \in S_1) \mid V] \, \Prob(W \in S_2 \mid V) \\
    &= \Prob(U \in S_1 \mid V) \, \Prob(W \in S_2 \mid V),
\end{align*}
which gives the result.
\end{proof}

It is now easy to see that \eqref{eq:potential-outcomes-are-deterministic} implies \eqref{eq:actions-are-unconfounded}.
Indeed, by recursive substitution, it is straightforward to show that there exist measurable functions $\tilde{g}_\tx$ for $\tx \in \{1, \ldots, \T\}$ such that
\[
    \X_\tx(\ax_{1:\tx}) \eqas \tilde{g}_\tx(\X_{0}, \ax_{1:\tx}) \qquad \text{for all $\tx \in \{1, \ldots, \T\}$ and $\ax_{1:\tx} \in \Aspace_{1:\tx}$},
\]
and so
\[
    (\X_{\sx}(\ax_{1:\sx}) : \sx \in \{1, \ldots, \T\}, \ax_{1:\sx} \in \Aspace_{1:\sx})
        = (\tilde{g}_\tx(\X_{0}, \ax_{1:\sx}) : \sx \in \{1, \ldots, \T\}, \ax_{1:\sx} \in \Aspace_{1:\sx}).
\]
The right-hand side is now seen to be a measurable function of $\X_0$ and hence certainly of $(\X_{0:\tx-1}(\A_{1:\tx-1}), \A_{1:\tx-1})$, so that the result follows by Lemma \ref{lem:determinism_conditional_independence}.

%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\input{text/causal/ToyExample}

\section{Causal bounds} \label{sec:causal-bounds-proofs}

%

%
%
%
%
%
%
%

%
%
%
%
%
%
%
%




\subsection{Proof of Theorem \ref{thm:causal-bounds}}

%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\begin{proof}
We prove the lower bound; the upper bound is analogous.
It is easily checked that 
\begin{multline} \label{eq:causal-bounds-proof-first-step}
    \E[\Y(\ax_{1:\tx}) \mid \X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx}] \\
        = \E[\Y(\ax_{1:\tx}) \mid \X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx}, \A_{1:\tx} = \ax_{1:\tx}] \, \Prob(\A_{1:\tx} = \ax_{1:\tx} \mid \X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx}) \\
            + \E[\Y(\ax_{1:\tx}) \mid \X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx}, \A_{1:\tx} \neq \ax_{1:\tx}] \, \Prob(\A_{1:\tx} \neq \ax_{1:\tx} \mid \X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx}).
\end{multline}
If $\Prob(\A_{1:\tx} = \ax_{1:\tx} \mid \X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx}) > 0$, then
\begin{align*}
    \E[\Y(\ax_{1:\tx}) \mid \X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx}, \A_{1:\tx} = \ax_{1:\tx}]
        &= \E[\Y(\A_{1:\tx}) \mid \X_{0:\tx}(\A_{1:\tx}) \in \B_{0:\tx}, \A_{1:\tx} = \ax_{1:\tx}] \\
        &= \E[\Y(\A_{1:\N}) \mid \X_{0:\N}(\A_{1:\N}) \in \B_{0:\N}, \A_{1:\tx} = \ax_{1:\tx}],
\end{align*}
where the second step follows because $\Prob(\N = \tx \mid \A_{1:\tx} = \ax_{1:\tx}) = 1$.
Similarly, if $\Prob(\A_{1:\tx} \neq \ax_{1:\tx} \mid \X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx}) > 0$, then \eqref{eq:Y-boundedness-assumption} implies
\[
    \E[\Y(\ax_{1:\tx}) \mid \X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx}, \A_{1:\tx} \neq \ax_{1:\tx}]
        \geq \ylo.
\]
Substituting these results into \eqref{eq:causal-bounds-proof-first-step}, we obtain
\begin{multline} \label{eq:causal-bounds-proof-second-step}
    \E[\Y(\ax_{1:\tx}) \mid \X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx}] \\
    \geq \E[\Y(\A_{1:\tx}) \mid \X_{0:\N}(\A_{1:\N}) \in \B_{0:\N}, \A_{1:\tx} = \ax_{1:\tx}] \, \Prob(\A_{1:\tx} = \ax_{1:\tx} \mid \X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx}) \\
            + \ylo \, \Prob(\A_{1:\tx} \neq \ax_{1:\tx} \mid \X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx}).
\end{multline}
Now observe that the right-hand side of \eqref{eq:causal-bounds-proof-second-step} is a convex combination with mixture weights $\Prob(\A_{1:\tx} = \ax_{1:\tx} \mid \X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx})$ and $\Prob(\A_{1:\tx} \neq \ax_{1:\tx} \mid \X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx})$.
We can bound
\begin{align}
    \Prob(\A_{1:\tx} = \ax_{1:\tx} \mid \X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx})
        &= \frac{\Prob(\X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx}, \A_{1:\tx} = \ax_{1:\tx})}{\Prob(\X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx})} \notag \\
        &\geq \frac{\Prob(\X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx}, \A_{1:\tx} = \ax_{1:\tx})}{\Prob(\X_{0:\N}(\ax_{1:\N}) \in \B_{0:\N})} \notag \\
        &= \frac{\Prob(\X_{0:\N}(\A_{1:\N}) \in \B_{0:\N}, \A_{1:\tx} = \ax_{1:\tx})}{\Prob(\X_{0:\N}(\A_{1:\N}) \in \B_{0:\N})} \notag \\
        &= \Prob(\A_{1:\tx} = \ax_{1:\tx} \mid \X_{0:\N}(\A_{1:\N}) \in \B_{0:\N}), %
\end{align}
where the inequality holds because $\tx \geq \N$ almost surely, and the second equality holds because the definition of $\N$ means
\[
    \X_{0:\N}(\ax_{1:\N}) \eqas \X_{0:\N}(\A_{1:\N}).
\]
As such, we can bound the convex combination in \eqref{eq:causal-bounds-proof-second-step} from below by replacing its mixture weights with $\Prob(\A_{1:\tx} = \ax_{1:\tx} \mid \X_{0:\N}(\A_{1:\N}) \in \B_{0:\N})$ and $\Prob(\A_{1:\tx} \neq \ax_{1:\tx} \mid \X_{0:\N}(\ax_{1:\N}) \in \B_{0:\N})$, which shifts weight from the $\E[\Y(\A_{1:\tx}) \mid \A_{1:\tx} = \ax_{1:\tx}, \X_{0:\N}(\A_{1:\N}) \in \B_{0:\N}]$ term onto the $\ylo$ term.
This yields
\begin{align*}
    &\E[\Y(\ax_{1:\tx}) \mid \X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx}] \\
        &\qquad\qquad\geq \E[\Y(\A_{1:\tx}) \mid \X_{0:\N}(\A_{1:\N}) \in \B_{0:\N}, \A_{1:\tx} = \ax_{1:\tx}] \, \Prob(\A_{1:\tx} = \ax_{1:\tx} \mid \X_{0:\N}(\ax_{1:\N}) \in \B_{0:\N}) \\
        &\qquad\qquad\qquad+ \ylo \, \Prob(\A_{1:\tx} \neq \ax_{1:\tx} \mid \X_{0:\N}(\ax_{1:\N}) \in \B_{0:\N}) \\
        &\qquad\qquad= \E[\Y(\A_{1:\tx}) \, \ind(\A_{1:\tx} = \ax_{1:\tx}) + \ylo \, \ind(\A_{1:\tx} \neq \ax_{1:\tx}) \mid \X_{0:\N}(\A_{1:\N}) \in \B_{0:\N}] \\
        &\qquad\qquad= \E[\Ylo \mid \X_{0:\N}(\A_{1:\N}) \in \B_{0:\N}].
\end{align*}
\end{proof}

\subsection{Proof of Proposition \ref{prop:our-bounds-vs-manskis}}

\begin{proof}
%
%
%
%
%
%
%
%
%
%
%
%
From the definition of $\Yup$, we have straightforwardly
\begin{multline*}
    \Qup = \E[\Y(\A_{1:\tx}) \mid \X_{0:\N}(\A_{1:\N}) \in \B_{0:\N}, \A_{1:\tx} = \ax_{1:\tx}] \, \Prob(\A_{1:\tx} = \ax_{1:\tx} \mid \X_{0:\N}(\A_{1:\N}) \in \B_{0:\N}) \\
            +  \yup \, \Prob(\A_{1:\tx} \neq \ax_{1:\tx} \mid \X_{0:\N}(\A_{1:\N}) \in \B_{0:\N}).
\end{multline*}
A similar expression holds for $\Qlo$.
Subtracting these two expressions yields
\[
    \Qup - \Qlo = (\yup - \ylo) \, (1 - \Prob(\A_{1:\tx}=\ax_{1:\tx} \mid \X_{0:\N}(\A_{1:\N}) \in \B_{0:\N})). 
\]
Similar manipulations show that
\[
    \E[\Yup] - \E[\Ylo] = (\yup - \ylo) \, (1 - \Prob(\A_{1:\tx}=\ax_{1:\tx})),
\]
and the result now follows.

%
%
%
%
%
%
%


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%

%



%
%

%
%
%
%
%

%
%
%
%
%
%
%
%
\end{proof}

\subsection{Proof of Proposition \ref{prop:sharpness-of-bounds} and discussion} \label{sec:sharpness-of-bounds-supplement}

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


\begin{proof}
    We consider the case of the lower bound; the case of the upper bound is analogous.
    Choose $\xx_{1:\T} \in \B_{1:\T}$ arbitrarily.
    (Certainly some choice is always possible, since each $\B_{\sx}$ has positive measure and is therefore nonempty.)
    Define
    \begin{align*}
        \tilde{\X}_0 &\coloneqq \X_0 \\
        \tilde{\X}_{\sx}(\ax_{1:\sx}') &\coloneqq \ind(\A_{1:\sx} = \ax_{1:\sx}') \, \X_{\sx}(\ax_{1:\sx}') + \ind(\A_{1:\sx} \neq \ax_{1:\sx}') \, \xx_{\sx} \qquad \text{for each $\sx \in \{0, \ldots, \T\}$ and $\ax_{1:\sx}' \in \Aspace_{1:\sx}$},
        %
        %
    \end{align*}
    and similarly let
    \[
        %
        \tilde{\Y}(\ax_{1:\tx}') = \ind(\A_{1:\tx} = \ax_{1:\tx}') \, \Y(\ax_{1:\tx}') + \ind(\A_{1:\tx} \neq \ax_{1:\tx}')\, \ylo \qquad \text{for all $\ax_{1:\tx}' \in \Aspace_{1:\tx}$}.
    \]
    It is easy to check that $(\tilde{\X}_{0:\T}(\A_{1:\T}), \tilde{\Y}(\A_{1:\tx}), \A_{1:\T}) \eqas (\X_{0:\T}(\A_{1:\T}), \Y(\A_{1:\tx}), \A_{1:\T})$.
    But now we have directly $\tilde{\Y}(\ax_{1:\tx}) = \Ylo$.
    Moreover, it is easily checked from the definition of $\N$ and $\tilde{\X}_{0:\tx}(\ax_{1:\tx})$ that
    \[
        \tilde{\X}_{0:\tx}(\ax_{1:\tx}) \eqas (\X_{0:\N}(\A_{1:\N}), \xx_{\N+1:\tx}),
    \]
    so that
    \begin{align*}
        \ind(\tilde{\X}_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx})
        &\eqas \ind(\tilde{\X}_{0:\N}(\ax_{1:\N}) \in \B_{0:\N}, \xx_{\N+1:\tx} \in \B_{\N+1:\tx}) \\
        &\eqas \ind(\X_{0:\N}(\A_{1:\N}) \in \B_{0:\N})
    \end{align*}
    since each $\xx_{\sx} \in \B_\sx$.
    Consequently,
    \begin{align*}
        \E[\tilde{\Y}(\ax_{1:\tx}) \mid \tilde{\X}_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx}]
        &= \E[\Ylo \mid \tilde{\X}_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx}] \\
        &= \E[\Ylo \mid \X_{0:\N}(\A_{1:\N}) \in \B_{0:\N}],
    \end{align*}
    which gives the result.
\end{proof}

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
\subsection{Bounds on the conditional expectation given specific covariate values} \label{sec:impossibility-of-bounds-for-continuous-data}

%
%
%
%
%

Theorem \ref{thm:causal-bounds} provides a bound on $\E[\Y(\ax_{1:\tx}) \mid \X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx}]$, i.e.\ the conditional expectation given the \emph{event} $\{\X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx}\}$, which is assumed to have positive probability.
We consider here the prospect of obtaining bounds on $\E[\Y(\ax_{1:\tx}) \mid \X_{0:\tx}(\ax_{1:\tx})]$, i.e.\ the conditional expectation given the \emph{value} of $\X_{0:\tx}(\ax_{1:\tx})$.
%
For falsification purposes, this would provide a means for determining that twin is incorrect when it outputs specific values of $\Xt_{0:\tx}(\ax_{1:\tx})$, rather than just that it is incorrect on average across all runs that output values $\Xt_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx}$.

%
%
%
%
When $\X_{0:\tx}(\ax_{1:\tx})$ is discrete, Theorem \ref{thm:causal-bounds} yields measurable functions $\lo{\gx}, \up{\gx} : \Xspace_{0:\tx} \to \R$ such that
\begin{equation} \label{eq:psi-lo-up-defining-property}
    \lo{\gx}(\X_{0:\tx}(\ax_{1:\tx}))
        \leq \E[\Y(\ax_{1:\tx}) \mid \X_{0:\tx}(\ax_{1:\tx})] \leq \up{\gx}(\X_{0:\tx}(\ax_{1:\tx})) \qquad \text{almost surely}.
\end{equation}
In particular, $\lo{\gx}(\xx_{0:\tx})$ is obtained as the value of $\E[\Ylo \mid \X_{0:\N}(\A_{1:\N}) \in \B_{0:\N}]$ for $\B_{0:\tx} \coloneqq \{\xx_{0:\tx}\}$, and similarly for $\up{\gx}(\xx_{0:\tx})$.
Moreover, since the constants $\ylo, \yup \in \R$ in Theorem \ref{thm:causal-bounds} were allowed to depend on $\B_{0:\tx}$, and hence here on each choice of $\xx_{0:\tx} \in \Xspace_{0:\tx}$, we may think of these now as measurable functions $\ylo, \yup : \Xspace_{0:\tx} \to \R$ satisfying
\begin{equation} \label{eq:y-boundedness-functional-assumption}
    \ylo(\X_{0:\tx}(\ax_{1:\tx})) \leq \Y(\ax_{1:\tx}) \leq \yup(\X_{0:\tx}(\ax_{1:\tx})) \qquad \text{almost surely}.
\end{equation}
In other words, when $\X_{0:\tx}(\ax_{1:\tx})$ is discrete, Theorem \ref{thm:causal-bounds} provides bounds on the conditional expectation of $\Y(\ax_{1:\tx})$ given the value of $\X_{0:\tx}(\ax_{1:\tx})$ whenever we have $\ylo$ and $\yup$ such that \eqref{eq:y-boundedness-functional-assumption} holds.

%

%


%
%
%


%

When $\Prob(\X_{1:\tx}(\ax_{1:\tx}) \in \B_{1:\tx}) > 0$, a fairly straightforward modification of the proof of Theorem \ref{thm:causal-bounds} yields bounds of the following form:
\begin{align}
    \E[\Ylo \mid \X_0, \X_{1:\N}(\A_{1:\N}) \in \B_{1:\N}] 
        &\leq \E[\Y(\ax_{1:\tx}) \mid \X_0, \X_{1:\tx}(\ax_{1:\tx}) \in \B_{1:\tx}] \notag \\
        &\qquad\qquad\leq \E[\Yup \mid \X_0, \X_{1:\N}(\A_{1:\N}) \in \B_{1:\N}] \qquad \text{almost surely}. \label{eq:bareinboim-style-bounds-with-continuous-initial-covariates}
\end{align}
In particular, this holds regardless of whether or not $\X_0$ is discrete.
In turn, if $\X_{1:\tx}(\ax_{1:\tx})$ is discrete, then by a similar argument as was given in the previous subsection, this yields almost sure bounds on $\E[\Y(\ax_{1:\tx}) \mid \X_{0:\tx}(\ax_{1:\tx})]$ of the form in \eqref{eq:psi-lo-up-defining-property}, provided \eqref{eq:y-boundedness-functional-assumption} holds.
Alternatively, by taking $\B_{1:\tx} \coloneqq \Xspace_{1:\tx}$, \eqref{eq:bareinboim-style-bounds-with-continuous-initial-covariates} yields bounds of the form
\[
    \E[\Ylo \mid \X_0] \leq \E[\Y(\ax_{1:\tx}) \mid \X_0] \leq \E[\Yup \mid \X_0].
\]
If the action sequence $\ax_{1:\tx}$ is thought of as a single choice of an action from the extended action space $\Aspace_{1:\tx}$, then this recovers the bounds originally proposed by \citet{manski}, which allowed conditioning on potentially continuous pre-treatment covariates corresponding to our $\X_0$.


%


%

\subsection{Proof of Theorem \ref{thm:no-causal-bounds-for-continuous-data} and discussion}

%
%

%
%

%
%
%
%

%
%
%
%
%
%
%
%
%
%

%
%
%


\begin{proof}
    Suppose we have a permissible $\lo{\gx}$.
    (The case of $\up{\gx}$ is analogous).
    Choose $\xx_{1:\T} \in \Xspace_{1:\T}$ arbitrarily, and define new potential outcomes
    \begin{align*}
        \tilde{\X}_0 &\coloneqq \X_0 \\
        \tilde{\X}_r(\ax_{1:r}') &\coloneqq %
            %
            \ind(\A_{1:r} = \ax_{1:r}') \, \X_{r}(\ax_{1:r}') + \ind(\A_{1:r} \neq \ax_{1:r}') \, \xx_{r} \qquad \text{for $r \in \{1, \ldots, \T\}$ and $\ax_{1:r}' \in \Aspace_{1:r}$}.
        %
    \end{align*}
    Similarly, define
    \begin{align*}
        \tilde{\Y}(\ax_{1:\tx}') &\coloneqq \ind(\A_{1:\tx} = \ax_{1:\tx}') \, \Y(\ax_{1:\tx}') + \ind(\A_{1:\tx} \neq \ax_{1:\tx}') \, \ylo(\tilde{\X}_{0:\tx}(\ax_{1:\tx}')) \qquad \text{for all $\ax_{1:\tx}' \in \Aspace_{1:\tx}$}.
    \end{align*}
    It immediately follows that
    \[
        (\tilde{\X}_{0:\T}(\A_{1:\T}), \tilde{\Y}(\A_{1:\tx}), \A_{1:\T}) \eqas (\X_{0:\T}(\A_{1:\T}), \Y(\A_{1:\tx}), \A_{1:\T}).
    \]
    Moreover, it is easily checked that
    \[
        \ylo(\tilde{\X}_{0:\tx}(\ax_{1:\tx})) \leq \tilde{\Y}(\ax_{1:\tx}) \leq \yup(\tilde{\X}_{0:\tx}(\ax_{1:\tx})) \qquad \text{almost surely}.
    \]
    As such, since $\lo{\gx}$ is permissible, we must have, almost surely,
    \begin{align}
        \lo{\gx}(\tilde{\X}_{0:\tx}(\ax_{1:\tx})) &\leq \E[\tilde{\Y}(\ax_{1:\tx}) \mid \tilde{\X}_{0:\tx}(\ax_{1:\tx})] \notag \\
         &= \begin{multlined}[t]
            \E[\tilde{\Y}(\A_{1:\tx}) \mid \tilde{\X}_{0:\tx}(\ax_{1:\tx}), \A_{1:\tx} = \ax_{1:\tx}] \, \Prob(\A_{1:\tx} = \ax_{1:\tx} \mid \tilde{\X}_{0:\tx}(\ax_{1:\tx})) \\
                + \underbrace{\E[\tilde{\Y}(\ax_{1:\tx}) \mid \tilde{\X}_{0:\tx}(\ax_{1:\tx}), \A_{1:\tx} \neq \ax_{1:\tx}]}_{=\ylo(\tilde{\X}_{0:\tx}(\ax_{1:\tx}))} \, \Prob(\A_{1:\tx} \neq \ax_{1:\tx} \mid \tilde{\X}_{0:\tx}(\ax_{1:\tx})).
        \end{multlined} \label{eq:no-continuous-bounds-proof-convex-combination}
    \end{align}
    Now, by our definition of $\tilde{\X}_{0:\tx}(\ax_{1:\tx})$, we have almost surely
    \begin{align*}
        \ind(\A_{1} \neq \ax_{1}) \, \Prob(\A_{1:\tx} = \ax_{1:\tx} \mid \tilde{\X}_{0:\tx}(\ax_{1:\tx}))
            &= \ind(\A_{1} \neq \ax_{1}, \tilde{\X}_{\sx}(\ax_{1:\sx}) = \xx_\sx) \, \Prob(\A_{1:\tx} = \ax_{1:\tx} \mid \tilde{\X}_{0:\tx}(\ax_{1:\tx})) \\
            &= \ind(\A_{1} \neq \ax_{1}) \, \E[\ind(\A_{1:\tx} = \ax_{1:\tx}, \tilde{\X}_{\sx}(\ax_{1:\sx}) = \xx_\sx) \mid \tilde{\X}_{0:\tx}(\ax_{1:\tx})] \\
            &= \ind(\A_{1} \neq \ax_{1}) \, \E[\ind(\A_{1:\tx} = \ax_{1:\tx}, \X_{\sx}(\A_{1:\sx}) = \xx_\sx) \mid \tilde{\X}_{0:\tx}(\ax_{1:\tx})] \\
            &= 0,
    \end{align*}
    where the last step follows by our assumption that $\Prob(\X_{\sx}(\A_{1:\sx}) = \xx_{\sx}) = 0$.
    Combining this with \eqref{eq:no-continuous-bounds-proof-convex-combination}, we get, almost surely,
    \begin{align}
        \ind(\A_{1} \neq \ax_{1}) \, \lo{\gx}(\X_{0}, \xx_{1:\tx}) &=  \ind(\A_{1} \neq \ax_{1}) \, \lo{\gx}(\tilde{\X}_{0:\tx}(\ax_{1:\tx})) \notag \\
            &\leq \ind(\A_{1} \neq \ax_{1}) \, \ylo(\tilde{\X}_{0:\tx}(\ax_{1:\tx})) \notag \\
            &= \ind(\A_{1} \neq \ax_{1}) \, \ylo(\X_{0}, \xx_{1:\tx}). \label{eq:no-continuous-bounds-intermediate-step}
    \end{align}
    Now let $\xx_0 \in \Xspace_0$ be the value such that $\Prob(\X_0 = \xx_0) = 1$.
    Using our assumption that $\Prob(\A_1 \neq \ax_1) > 0$ and the fact that $\xx_{1:\tx}$ was arbitrary, we obtain
    \[
        \lo{\gx}(\xx_{0:\tx}) \leq \ylo(\xx_{0:\tx}) \qquad \text{for all $\xx_{1:\tx} \in \Xspace_{1:\tx}$}.
    \]
    The result now follows.
\end{proof}


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%

%
%
%
%


To gain intuition for the phenomenon underlying Theorem \ref{thm:no-causal-bounds-for-continuous-data}, consider a simplified model consisting of $\Xspace$-valued potential outcomes $(\X(\ax') : \ax \in \Aspace)$, $\R$-valued potential outcomes $(\Y(\ax') : \ax \in \Aspace)$, and an $\Aspace$-valued random variable $\A$ representing the choice of action.
(This constitutes a special case of our setup with $\T = 1$ and $\Xspace_0$ taken to be a singleton set.)
Suppose moreover that the following conditions hold:
\begin{align*}
    %
    \Prob(\X(\A) = \xx) &= 0 \qquad \text{for all $\xx \in \Xspace$} \\
    \Prob(\A = \ax) &< 1.
\end{align*}
We then have
\begin{equation} \label{eq:no-continuous-bounds-toy-example}
    \E[\Y(\ax) \mid \X(\ax)]
        \eqas \E[\Y(\A) \mid \X(\A), \A = \ax] \, \Prob(\A = \ax \mid \X(\ax)) + \E[\Y(\ax) \mid \X(\ax), \A \neq \ax] \, \Prob(\A \neq \ax \mid \X(\ax)).
\end{equation}
But now, since the behaviour of $\X(\ax)$ is only observed on $\{\A = \ax\}$, for any given value of $\xx \in \Xspace$, we cannot rule out the possibility that
\[
    \X(\ax) = \ind(\A = \ax) \, \X(\A) + \ind(\A \neq \ax) \, \xx \qquad \text{almost surely}.
\]
In turn, since $\Prob(\A = \ax) > 0$, this would imply $\Prob(\X(\ax) = \xx) > 0$, and, since $\Prob(\X(\A) = \xx) = 0$, that $\Prob(\A = \ax \mid \X(\ax) = \xx) = 0$.
From \eqref{eq:no-continuous-bounds-toy-example}, this would yield
\[
    \E[\Y(\ax) \mid \X(\ax) = \xx] = \E[\Y(\ax) \mid \X(\ax) = \xx, \A \neq \ax].
\]
But now, since the behaviour of $\Y(\ax)$ is unobserved on $\{\A \neq \ax\}$, intuitively speaking, the observational distribution does not provide any information about the value of the right-hand side, and therefore about the behaviour of $\E[\Y(\ax) \mid \X(\ax)]$ more generally since $\xx \in \Xspace$ was arbitrary.

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


\section{Hypothesis testing methodology} 

\subsection{Validity of testing procedure} \label{sec:hyp-testing-supplement}

%
%
%
%
%
%
%
%
%

%
%
%
%
%

We show here that our procedure for testing $\Qt \geq \Qlo$ based on the one-sided confidence intervals $\qlo{\alpha}$ and $\qt{\alpha}$ has the correct probability of type I error, provided $\qlo{\alpha}$ and $\qt{\alpha}$ have the correct coverage probabilities.
In particular, the result below (which applies a standard union bound argument) shows that if $\Qt \geq \Qlo$, then our test rejects (i.e.\ $\qt{\alpha} < \qlo{\alpha}$) with probability at most $\alpha$.
An analogous result is easily proven for testing $\Qt \leq \Qup$ also, with $\qlo{\alpha}$ replaced by a one-sided upper $(1-\alpha/2)$-confidence interval for $\Qup$, and $\qt{\alpha}$ replaced by a one-sided lower $(1 - \alpha/2)$-confidence interval for $\Qt$.

\begin{proposition}
Suppose that for some $\alpha \in (0, 1)$ we have random variables $\qt{\alpha}$ and $\qlo{\alpha}$ satisfying
\begin{align}
    \Prob(\Qlo \geq \qlo{\alpha}) &\geq 1 - \frac{\alpha}{2} \label{eq:qlo-confidence-interval-guarantee} \\
    \Prob(\Qt \leq \qt{\alpha}) &\geq 1 - \frac{\alpha}{2} \label{eq:qt-confidence-interval-guarantee}.
\end{align}
If $\Qt \geq \Qlo$, then $\Prob(\qt{\alpha} < \qlo{\alpha}) \leq \alpha$.
\end{proposition}

\begin{proof}
If $\Qt \geq \Qlo$, then we have
\[
    \{\qt{\alpha} < \qlo{\alpha}\} \subseteq \{\Qt > \qt{\alpha}\} \cup \{\Qlo < \qlo{\alpha}\}.
\]
To see this, note that
\[
    (\{\Qt > \qt{\alpha}\} \cup \{\Qlo < \qlo{\alpha}\})^c
        = \{\Qt > \qt{\alpha}\}^c \cap \{\Qlo < \qlo{\alpha}\}^c
        = \{\Qt \leq \qt{\alpha}\} \cap \{\Qlo \geq \qlo{\alpha}\}
        \subseteq \{\qlo{\alpha} \leq \qt{\alpha}\}.
\]
As such,
\begin{align*}
    \Prob(\qt{\alpha} < \qlo{\alpha}) \leq \Prob(\{\Qt > \qt{\alpha}\} \cup \{\Qlo < \qlo{\alpha}\}) \leq \Prob(\Qt > \qt{\alpha}) + \Prob(\Qlo < \qlo{\alpha}) \leq \alpha/2 + \alpha/2 = \alpha.
\end{align*}
\end{proof}

%
%
%
%
%
%

\subsection{Unbiased sample mean estimates of $\Qlo$, $\Qt$, and $\Qup$} \label{sec:confidence-intervals-methodology-supplement}

We use our data to obtain one-sided confidence intervals $\qlo{\alpha}$ and $\qt{\alpha}$ satisfying \eqref{eq:qlo-confidence-interval-guarantee} and \eqref{eq:qt-confidence-interval-guarantee} as required by our procedure for testing $\Qt \geq \Qlo$.
We use an analogous procedure to obtain confidence intervals for testing $\Qt \leq \Qup$.
We tried two techniques for this: an exact method based on Hoeffding's inequality, and an approximate method based on bootstrapping.
%
Conceptually, both are based on obtaining unbiased sample mean estimates of $\Qlo$ and $\Qt$, which we describe now, before giving the particulars of each method in the next two subsections.
%

%

%

We begin with our sample mean estimator of $\Qlo$.
Recall that we assume access to a dataset $\D$ consisting of i.i.d.\ copies of observational trajectories of the form
\[
    \X_0, \A_1, \X_1(\A_1), \ldots, \A_\T, \X_\T(\A_{1:\T}).
\]
Let $\D(\ax_{1:\tx}, \B_{0:\tx})$ be the subset of trajectories in $\D$ for which $\X_{0:\N}(\A_{1:\N})\in\B_{0:\N}$.
Obtaining $\D(\ax_{1:\tx}, \B_{0:\tx})$ is possible since the only random quantity that $\N = \max\{0 \leq \sx \leq \tx \mid \A_{1:\sx} = \ax_{1:\sx}\}$ depends on is $\A_{1:\tx}$, which is included in the data.
We denote the cardinality of $\D(\ax_{1:\tx}, \B_{0:\tx})$ by $\nx \coloneqq \abs{\D(\ax_{1:\tx}, \B_{0:\tx})}$.
We then denote by $\Ylo^{(i)}$ for $i \in \{1, \ldots, \nx\}$ the corresponding values of
\begin{align*}
    \Ylo &= \ind(\A_{1:\tx} = \ax_{1:\tx}) \, \fx(\X_{0:\tx}(\A_{1:\tx})) + \ind(\A_{1:\tx} \neq \ax_{1:\tx}) \, \ylo
\end{align*}
obtained from each trajectory in $\D(\ax_{1:\tx}, \B_{0:\tx})$.
This is again possible since both terms only depends on the observational quantities $(\X_{0:\tx}(\A_{1:\tx}), \A_{1:\tx})$.
It is easily seen that the values of $\Ylo^{(i)}$ are i.i.d.\ and satisfy $\E[\Ylo^{(i)}] = \Qlo$.
    %
    %
As a result, the sample mean
\begin{equation} \label{eq:YClmean-definition-supplement}
    \YClmean \coloneqq \frac{1}{\nx} \sum_{i=1}^{\nx} \Ylo^{(i)} 
    %
    %
    %
    %
\end{equation}
is an unbiased estimator of $\Qlo$.

%

We obtain an unbiased sample mean estimate of $\Qt$ in a similar fashion as for $\Qlo$.
Recall that we assume access to a dataset $\Dt(\ax_{1:\tx})$ consisting of i.i.d.\ copies of
\[
    \X_0, \Xt_1(\X_0, \ax_1), \ldots, \Xt_t(\X_0, \ax_{1:\tx}).
\]
Let $\Dt(\ax_{1:\tx}, \B_{0:\tx})$ denote the subset of twin trajectories in $\Dt(\ax_{1:\tx})$ for which $(\X_0, \Xt_{\tx}(\X_0, \ax_{1:\tx})) \in \B_{0:\tx}$, and denote its cardinality by $\widehat{\nx} \coloneqq \abs{\Dt(\ax_{1:\tx}, \B_{0:\tx})}$.
%
Then denote by $\Yt^{(i)}$ for $i \in \{1 \ldots, \widehat{\nx}\}$ the corresponding values of
\[
    \Yt = \fx(\X_0, \Xt_{1:\tx}(\X_0, \ax_{1:\tx}))
\]
obtained from each trajectory in $\Dt(\ax_{1:\tx}, \B_{0:\tx})$.
It is easily seen that the values $\Yt^{(i)}$ are i.i.d.\ (since the entries of $\Dt(\ax_{1:\tx})$ are) and satisfy $\E[\Yt^{(i)}] = \Qt$.
As a result, the sample mean
\[
    \Ytmean \coloneqq \frac{1}{\widehat{\nx}} \sum_{i=1}^{\widehat{\nx}} \Yt^{(i)}
\]
is an unbiased estimator of $\Qt$.


\subsection{Exact confidence intervals via Hoeffding's inequality}

Recall that we assume in Section \ref{sec:hypotheses-from-causal-bounds-setup} that $\Y(\ax_{1:\tx})$ has the form $\Y(\ax_{1:\tx}) = \fx(\X_{0:\tx}(\ax_{1:\tx}))$, and that moreover
\begin{equation} \label{eq:f-boundedness-assumption-hoeffding-proof}
    \ylo \leq \fx(\xx_{0:\tx}) \leq \yup \qquad \text{for all $\xx_{0:\tx} \in \B_{0:\tx}$.}
\end{equation}
This means $\Yt^{(i)}$ is almost surely bounded in $[\ylo, \yup]$, and so $\Ytmean$ gives rise to one-sided confidence intervals via an application of Hoeffding's inequality.
The exact form of these confidence intervals is as follows:

%
%
%

%


\begin{proposition} \label{prop:hoeffding-confidence-bounds-supp}
If \eqref{eq:f-boundedness-assumption-hoeffding-proof} holds, then for each $\alpha \in (0, 1)$, letting
\[
    \CIlen \coloneqq (\yup - \ylo) \, \sqrt{\frac{1}{2 \nx} \, \log \frac{2}{\alpha}}  \qquad \text{and} \qquad \widehat{\CIlen} \coloneqq (\yup - \ylo) \, \sqrt{\frac{1}{2 \widehat{\nx}} \, \log \frac{2}{\alpha}},
\]
and similarly
\begin{align*}
    \qlo{\alpha} \coloneqq \YClmean - \CIlen  \qquad \text{and} \qquad
    \qt{\alpha} \coloneqq \Ytmean + \widehat{\CIlen},
\end{align*}
it follows that
\begin{align*}
    \Prob(\Qlo \geq \qlo{\alpha}) \geq 1 - \frac{\alpha}{2} \qquad \text{and} \qquad \Prob(\Qt \leq \qt{\alpha}) \geq 1 - \frac{\alpha}{2}.
\end{align*}
\end{proposition}

\begin{proof}
We only prove the result for $\qlo{\alpha}$; the other statement can be proved analogously.
Recall that $\YClmean$ is the empirical mean of i.i.d.\ samples $\Ylo^{(i)}$ for $i\in \{1, \ldots, \nx\}$ with $\E[\Ylo^{(i)}]=\Qlo$ (see \eqref{eq:YClmean-definition-supplement}).
Moreover, by \eqref{eq:f-boundedness-assumption-hoeffding-proof}, $\Ylo^{(i)}$ is almost surely bounded in $[\ylo, \yup]$.
Hoeffding's inequality then implies that
\begin{align*}
    \Prob(\YClmean - \Qlo > \CIlen) &\leq \exp\left(- \frac{2 \nx \CIlen^2}{(\yup - \ylo)^2 } \right).
\end{align*}
In turn, some basic manipulations yield
\begin{align*}
    \Prob(\Qlo \geq \qlo{\alpha}) &= 1 - \Prob(\Qlo < \YClmean - \CIlen) \\
    &\geq 1 - \exp\left(- \frac{2 \nx \CIlen^2}{(\yup - \ylo)^2 } \right) \\
    &= 1- \frac{\alpha}{2}.
\end{align*}
\end{proof}

\subsection{Approximate confidence intervals via bootstrapping} \label{subsec:bootstrapping}

While Hoeffding's inequality yields the probability guarantees in \eqref{eq:qlo-confidence-interval-guarantee} and \eqref{eq:qt-confidence-interval-guarantee} exactly, the confidence intervals obtained can be conservative.
Consequently, our testing procedure may have lower probability of falsifying certain twins that in fact do not satisfy the causal bounds.
To address this, we also consider an approximate approach based on bootstrapping that can produce tighter confidence intervals.
%
While other schemes are possible, bootstrapping provides a general-purpose approach that is straightforward to implement and works well in practice.

At a high level, our approach here is again to construct one-sided level $1 - \alpha/2$ confidence intervals via bootstrapping \citep{efron1979bootstrap} on $\Qlo$ and $\Qt$.
Many bootstrapping procedures for obtaining confidence intervals have been proposed in the literature \citep{tibshirani1993introduction,davison1997bootstrap,hesterberg2015what}. 
Our results reported below were obtained via the \emph{reverse percentile} bootstrap (see \cite{hesterberg2015what} for an overview).
(We also tried the \emph{percentile} bootstrap method, which obtained nearly indistinguishable results.)
In particular, this method takes
\[
    \qlo{\alpha} \coloneqq 2 \YClmean - \Delta 
    \qquad \qt{\alpha} \coloneqq 2 \widehat{\mu} - \widehat{\Delta},
\]
where $\Delta$ and $\widehat{\Delta}$ correspond to the approximate $1 - \alpha / 2$ and $\alpha / 2$ quantiles of the distributions of 
%
\[
    \frac{1}{\nx} \sum_{i=1}^{\nx} \Ylo^{(i^\ast)} 
    \qquad\text{and}\qquad
    \frac{1}{\widehat{\nx}} \sum_{i=1}^{\widehat{\nx}} \Yt^{(i^\ast)},
\]
where each $\Ylo^{(i^\ast)}$ and $\Y^{(i^\ast)}$ is obtained by sampling uniformly with replacement from among the values of $\Ylo^{(i)}$ and $\Y^{(i)}$.
In our case study, as is typically done in practice, we approximated $\Delta$ and $\widehat{\Delta}$ via Monte Carlo sampling.
It can be shown that the confidence intervals produced in this way obtain a coverage level that approaches the desired level of $1 - \alpha/2$ as $\nx$ and $\widehat{\nx}$ grow to infinity under mild assumptions \citep{hall1988theoretical}.

%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%


\section{Experimental Details} \label{sec:experiments-supplement}

%

\subsection{MIMIC preprocessing} \label{sec:mimic-preprocessing-supp}

For data extraction and preprocessing, we re-used the same procedure as \cite{ai-clinician} with minor modifications.
For completeness, we describe the pre-processing steps applied in \cite{ai-clinician} and subsequently outline our modifications to these.

%

Following \cite{ai-clinician}, we extracted adult patients fulfilling the sepsis-3 criteria \citep{sepsis-criteria}. 
Sepsis was defined as a suspected infection (as indicated by prescription of antibiotics and sampling of bodily fluids for microbiological culture) combined with evidence of organ dysfunction, defined by a SOFA score $\geq 2$ \citep{sepsis-criteria, seymour2016assessment}.  
%

Following \cite{ai-clinician}, we excluded patients for whom any of the following was true: their age was less than $18$ years old at the time of ICU admission; their mortality not documented; their IV fluid/vasopressors intake was not documented; their treatment was withdrawn.

We made the following modifications to the preprocessing code of \cite{ai-clinician} for our experiment.
First, instead of extracting physiological quantities (e.g.\ heart rate) every 4 hours, we extracted these every hour.
Additionally, we excluded patients with any missing hourly vitals during the first 4 hours of their ICU stay.

We then extracted a total of 19 quantities of interest listed in Table \ref{tab:mimic-features}.
Of these, 17 were physiological quantities associated with the patient, including static demographic quantities (e.g.\ age), patient vital signs (e.g.\ heart rate), and patient lab values (e.g.\ potassium blood concentration).
All of these were continuous values, apart from sex.
These were chosen as the subset of physiological quantities extracted from MIMIC by \cite{ai-clinician} that are also modelled by Pulse, and were used to define our observation spaces $\Xspace_\tx$ as described next.
The remaining 2 quantities (intravenous fluids and vasopressor doses) were chosen since they correspond to treatments that the patient received, and were used to define our action spaces $\Aspace_\tx$ as described below.

\subsection{Sample splitting}

Before proceeding further, we randomly selected 5\% of the extracted our trajectories (583 trajectories, denoted as $\D_0$) to use for preliminary tasks such as choosing the parameters of our hypotheses.
We reserved the remaining 95\% (11,094 trajectories, denoted as $\D$) for the actual testing.
By a standard sample splitting argument \citep{cox1975note}, the statistical guarantees of our testing procedure established above continue to apply even when our hypotheses are defined in this data-dependent way.

%

\begin{table}[t]%
\centering
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
\begin{footnotesize}
\begin{tabular}{lll}
%
Category &  Physiological quantity \\
%
\midrule
Demographic & Age \\
& Sex  \\
& Weight \\
\midrule
%
Vital Signs & Heart rate (HR)  \\
& Systolic blood pressure (SysBP)  \\
& Diastolic blood pressure (DiaBP)  \\
& Mean blood pressure (MeanBP) \\
& Respiratory Rate (RR) \\
& Skin Temperature (Temp) \\
%
\midrule
Lab Values & Potassium Blood Concentration (Potassium)  \\
& Sodium Blood Concentration (Sodium)  \\
& Chloride Blood Concentration (Chloride)  \\
& Glucose Blood Concentration (Glucose)  \\
& Calcium Blood Concentration (Calcium)  \\
& Bicarbonate Blood Concentration ($\textup{HCO}_3$)  \\
& Arterial $\textup{O}_2$ Pressure ($\textup{PaO}_2$)  \\
& Arterial $\textup{CO}_2$ Pressure ($\textup{PaCO}_2$) \\
\midrule
%
Treatments & Intravenous fluid (IV) dose \\
& Vasopressor dose \\
\bottomrule
\end{tabular}
\end{footnotesize}
\caption{Physiological quantities and treatments extracted from MIMIC}\label{tab:mimic-features}
\end{table}

\subsection{Observation spaces} \label{sec:observation-space-definition-supp}

Our $\Xspace_0$ consisted of the following features: age, sex, weight, heart rate, systolic blood pressure, diastolic blood pressure and respiration rate.
We chose $\Xspace_0$ in this way because, out of the 17 physiological quantities we extracted from MIMIC, these were the quantities that can be initialised to user-provided values before starting a simulation in the version of Pulse we considered (4.x).
(In contrast, Pulse initialises the other 10 features to default values.)
%
For the remaining observation spaces, we used the full collection of the 17 physiological quantities we extracted to define $\Xspace_1 = \cdots = \Xspace_4$.
We encoded all features in $\Xspace_\tx$ numerically, i.e.\ $\Xspace_0 = \R^7$, and $\Xspace_\tx = \R^{17}$ for $\tx \in \{1, 2, 3, 4\}$.


%
%

\subsection{Action spaces} \label{sec:action-space-definition-supp}

Following \cite{ai-clinician}, we constructed our action space using 2 features obtained from MIMIC, namely intravenous fluid (IV) and vasopressor doses.
To obtain discrete action spaces suitable for our framework, we used the same discretization procedure for these quantities as was used by \cite{ai-clinician}.
Specifically, we divided the hourly doses of intravenous fluids and vasopressors into 5 bins each, with the first bin corresponding to zero drug dosage, and the remaining 4 bins based on the quartiles of the non-zero drug dosages in our held-out observational dataset $\D_0$.
%
From this we obtained action spaces $\Aspace_1 = \cdots = \Aspace_{4}$ with $5 \times 5 = 25$ elements. 
Table \ref{tab:act_space} shows the dosage bins constructed in this way, as well as the frequency of each bin's occurrence in the observational data.

%

%


\begin{table}[t]%
    \centering
    \begin{footnotesize}
\begin{tabular}{ll|ccccc}
%
\multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{5}{c}{Vasopressor dose ($\mu$g/kg/min)}\\
%
\multicolumn{1}{c}{} & \multicolumn{1}{c}{} &      0 &  0.0 - 0.061 &  0.061 - 0.15 &  0.15 - 0.313 &  $>0.313$ \\
\midrule
\multirow{5}{*}{IV dose (mL/h)} & 0        &  16659 &          329 &           256 &           152 &      145 \\
& 0 - 20   &   5840 &          428 &           351 &           244 &      145 \\
& 20 - 75  &   6330 &          297 &           378 &           383 &      309 \\
& 75 - 214 &   6232 &          176 &           175 &           197 &      273 \\
& $>214$    &   5283 &          347 &           488 &           544 &      747 \\
%
\end{tabular}
    \end{footnotesize}
\caption{Action space with frequency of occurrence in observational data} \label{tab:act_space}
\end{table}

\subsection{Hypothesis parameters} \label{sec:hypothesis-parameters-supplement}

We used our held-out observational dataset $\D_0$ to obtain a collection of hypothesis parameters $(\tx, \fx, \ax_{1:\tx}, \B_{0:\tx})$.
%
Specifically, for each physiological quantity of interest (e.g.\ heart rate) in the list of `Vital Signs' and `Lab Values' given in Table \ref{tab:mimic-features}, we did the following.
First, for each $\tx \in \{0, \ldots, 4\}$, we obtained 16 choices of $\B_{\tx}$ by discretizing the patient space $\Xspace_{\tx}$ into 16 subsets based on the values of certain features as follows: 2 bins corresponding to sex; 4 bins corresponding to the quartiles of the ages of patients in $\D_0$; 2 bins corresponding to whether or not the value of the chosen physiological quantity of interest at time $\tx$ was above or below its median value in $\D_0$.

Next, for each $\tx \in \{1, \ldots, 4\}$, $\ax_{1:\tx} \in \Aspace_{1:\tx}$, and sequence $\B_{0:\tx}$ with each $\B_{\tx'}$ as defined in the previous step, let $\D_0(\tx, \ax_{1:\tx}, \B_{0:\tx})$ denote the subset of $\D_0$ corresponding to $(\tx, \ax_{1:\tx}, \B_{0:\tx})$, i.e.\
\[
    \D_0(\tx, \ax_{1:\tx}, \B_{0:\tx}) \coloneqq \{\X_{0:\tx}(\A_{1:\tx}) \mid \text{$(\X_{0:\T}(\A_{1:\T}), \A_{1:\T}) \in \D_0$ with $\A_{1:\tx} = \ax_{1:\tx}$ and $\X_{0:\tx}(\A_{1:\T}) \in \B_{0:\tx}$}\}.
\]
We then selected the set of all triples $(\tx, \ax_{1:\tx}, \B_{0:\tx})$ such that $\D_0(\tx, \ax_{1:\tx}, \B_{0:\tx})$ contained at least one trajectory.
This meant the number of combinations of hypotheses parameters that we considered was limited to a tractable quantity, which had benefits both computationally, and also by ensuring that we did not sacrifice too much power when adjusting for multiple testing.

Finally, for each selected triple $(\tx, \ax_{1:\tx}, \B_{0:\tx})$, we chose a corresponding $\fx$ as follows.
First, we let $i \in \{1, \ldots, \Xspacedim_\tx\}$ denote the index of the physiological quantity of interest in $\Xspace_\tx = \R^{\Xspacedim_\tx}$.
We then set $\ylo, \yup$ to be the .2 and the .8 quantiles of the values in
\[
    \{(\X_\tx(\A_{1:\tx}))_i \mid \X_{0:\tx}(\A_{1:\tx}) \in \D_0(\tx, \ax_{1:\tx}, \B_{0:\tx})\}
\]
We then obtained $\fx : \Xspace_{0:\tx} \to \R$ as the function that extracts the physiological quantity of interest from $\Xspace_\tx$ and clips its value to between $\ylo$ and $\yup$, i.e.\ $\fx(\xx_{0:\tx}) \coloneqq \min(\max(\xx_{\tx})_{i}, \ylo), \yup)$.

Overall, accounting for all physiological quantities of interest, we obtained 721 distinct choices of $(\tx, \fx, \ax_{1:\tx}, \B_{0:\tx})$ in this way.
Figure \ref{fig:n_histograms} shows the amount of non-held out observational and twin data that we subsequently used for testing each hypothesis, i.e.\ the values of $n$ and $\widehat{n}$ as defined in Section \ref{sec:confidence-intervals-methodology-supplement} above.
(We describe how we generated our dataset of twin trajectories in Section \ref{sec:pulse-trajectories-supplement}.)

\subsection{Generating twin trajectories using the Pulse Physiology Engine}\label{sec:pulse-trajectories-supplement}

The Pulse Physiology Engine is an open source comprehensive human physiology simulator that has been used in medical education, research, and training. The core engine of Pulse is C++ based with APIs available in different languages, including python. Detailed documentation is available at: \href{https://pulse.kitware.com/}{pulse.kitware.com}.
Pulse allows users to initialize patient trajectories with given age, sex, weight, heart rate, systolic blood pressure, diastolic blood pressure and respiration rate and medical conditions such as sepsis, COPD, ARDS, etc. Once initialised, users have the ability to advance patient trajectories by a given time step (one hour in our case), and administer actions (e.g. administer a given dose of IV fluids or vasopressors).

In Algorithm \ref{algo:twin-data-generation} we describe how we generated the twin data to test the chosen hypotheses. Note that we sampled $\X_0$ without replacement as it ensures that each $\X_0$ is chosen at most once and consequently twin trajectories in $\Dt(\ax_{1:\tx})$ are i.i.d. 
%
%
Additionally, Algorithm \ref{algo:twin-data-generation} can be easily parallelised to improve efficiency.
%
Figure \ref{fig:n_histograms} shows histograms of the number of twin trajectories $\widehat{\nx}$ (as defined in Section \ref{sec:confidence-intervals-methodology-supplement} above) obtained in this way across all hypotheses.

\begin{algorithm}
\SetAlgoLined
\textbf{Inputs:} Action sequence $\ax_{1:\tx}$; Observational dataset $\D$.\\
\textbf{Output:} Twin data $\Dt(\ax_{1:\tx})$ of size $m$.\\
\For{$i = 1, \dots, m$}{
Sample $\X_0$ without replacement from $\D$\;
$\Xt_0 \leftarrow  \X_0$ i.e., initialize the Pulse trajectory with the information of $\X_0$\;
\For{$\tx' = 1, \dots, \tx$}{
Administer the median doses of IV fluids and vasopressors in action bin $\ax_{\tx'}$\;
\If{$\tx' \equiv 0$ \textup{(mod 3)}}{
Virtual patient in Pulse consumes nutrients and water, and urinates\;
}
Advance the twin trajectory by one hour\;
}
Add the trajectory $\Xt_{0:\tx}(\ax_{1:\tx})$ to $\Dt(\ax_{1:\tx})$\;
}
\textbf{Return} $\Dt(\ax_{1:\tx})$
\caption{Generating Twin data $\Dt(\ax_{1:\tx})$.}
\label{algo:twin-data-generation}
\end{algorithm}

\begin{figure}[t]
    \centering
    \includegraphics[height=21cm]{figures/causal/latest_experimental_results/nhistograms_nogray.pdf}
    \caption{Histograms of $n$ and $\widehat{n}$ (as defined in Section \ref{sec:confidence-intervals-methodology-supplement}) across all hypothesis parameters corresponding to each physiological quantity of interest.}
    \label{fig:n_histograms}
\end{figure}

%
%
%
%
%
%
%
%
%
%


\begin{figure}[t]%
    \centering
    \includegraphics[height=8cm]{figures/causal/latest_experimental_results/p_vals_hoeff_nogray.pdf}
    \caption{Boxenplots showing distributions of $-\log_{10}{p_\textup{lo}}$ and $-\log_{10}{p_\textup{up}}$ for different physiological quantities obtained via Hoeffding's inequality. Higher values indicate greater evidence in favour of rejection.}
    \label{fig:p_values_hoeff_complete}
\end{figure}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\begin{table}%
    \centering
\begin{footnotesize}
\begin{tabular}{l|cc|cc}
%
& \multicolumn{2}{c}{Ours} & \multicolumn{2}{c}{Manski} \\
                   Physiological quantity &  Rejs. &  Hyps. &  Rejs. &  Hyps. \\
\midrule
  Chloride Blood Concentration (Chloride) &            24 &            94 & 1 & 46 \\
      Sodium Blood Concentration (Sodium) &            21 &            94 & 9 & 46 \\
Potassium Blood Concentration (Potassium) &            13 &            94 & 0 & 46 \\
                  Skin Temperature (Temp) &            10 &            86 & 9 & 46 \\
    Calcium Blood Concentration (Calcium) &             5 &            88 & 0 & 46 \\
    Glucose Blood Concentration (Glucose) &             5 &            96 & 1 & 46 \\
      Arterial CO$_2$ Pressure (paCO$_2$) &             3 &            70 & 0 & 46 \\
Bicarbonate Blood Concentration (HCO$_3$) &             2 &            90 & 1 & 46 \\
       Systolic Arterial Pressure (SysBP) &             2 &           154 & 0 & 46 \\
        Arterial O$_2$ Pressure (paO$_2$) &             0 &            78 & 1 & 46 \\
                Arterial pH (Arterial\_pH) &             0 &            80 & 0 & 46 \\
      Diastolic Arterial Pressure (DiaBP) &             0 &            72 & 0 & 46 \\
          Mean Arterial Pressure (MeanBP) &             0 &            92 & 0 & 46 \\
                    Respiration Rate (RR) &             0 &           172 & 0 & 46 \\
                          Heart Rate (HR) &             0 &           162 & 0 & 46 \\
\bottomrule
\end{tabular}
\end{footnotesize}
    \caption{Total hypotheses (Hyps.) and rejections (Rejs.) per physiological quantity obtained using Hoeffding's inequality} \label{tab:hypotheses_hoeffding_full}
\end{table}

\begin{table}[t]
\centering
\begin{footnotesize}
\begin{tabular}{l|cc|cc}
%
& \multicolumn{2}{c}{Ours} & \multicolumn{2}{c}{Manski} \\
                   Physiological quantity &  Rejs. &  Hyps. &  Rejs. &  Hyps. \\
\midrule
  Chloride Blood Concentration (Chloride) &            47 &            94 & 1 & 46 \\
      Sodium Blood Concentration (Sodium) &            46 &            94 & 12 & 46 \\
Potassium Blood Concentration (Potassium) &            33 &            94 & 0 & 46 \\
                  Skin Temperature (Temp) &            43 &            86 & 13 & 46 \\
    Calcium Blood Concentration (Calcium) &            44 &            88 & 0 & 46 \\
    Glucose Blood Concentration (Glucose) &            19 &            96 & 0 & 46 \\
      Arterial CO$_2$ Pressure (paCO$_2$) &            13 &            70 & 0 & 46 \\
Bicarbonate Blood Concentration (HCO$_3$) &             8 &            90 & 0 & 46 \\
       Systolic Arterial Pressure (SysBP) &             8 &           154 & 0 & 46 \\
        Arterial O$_2$ Pressure (paO$_2$) &             4 &            78 & 1 & 46 \\
                Arterial pH (Arterial\_pH) &             0 &            80 & 0 & 46 \\
      Diastolic Arterial Pressure (DiaBP) &             0 &            72 & 0 & 46 \\
          Mean Arterial Pressure (MeanBP) &             3 &            92 & 0 & 46 \\
                    Respiration Rate (RR) &            12 &           172 & 0 & 46 \\
                          Heart Rate (HR) &             1 &           162 & 0 & 46 \\
\bottomrule
                                    %
%
\end{tabular}
\end{footnotesize}
\caption{Total hypotheses (Hyps.) and rejections (Rejs.) per physiological quantity obtained using the reverse percentile bootstrap}  \label{tab:hypotheses_rev_percentile}
\end{table}

\subsection{Bootstrapping details} \label{sec:boostrapping-details-supplement}

In addition to Hoeffding's inequality, we also used reverse percentile bootstrap method (see e.g.\ \cite{hesterberg2015what}) to obtain our confidence intervals on $\Qlo$ and $\Qup$ as described in Section \ref{sec:confidence-intervals-methodology-supplement}.
We used 100 bootstrap samples for each confidence interval.
To avoid bootstrapping on small numbers of data points, we did not reject any hypothesis where either the number of observational trajectories $n$ or twin trajectories $\widehat{n}$ was less than 100, and returned a $p$-value of 1 in each such case.

Table \ref{tab:hypotheses_rev_percentile} shows the number of rejected hypotheses for each physiological quantity using this approach.
We observed a similar trend as in our results obtained using Hoeffding's inequality (Table \ref{tab:hypotheses_hoeffding_full}).
For example, we obtained high number of rejections for Sodium, Chloride and Potassium blood concentrations but few rejections for Arterial Pressure and Heart Rate.
Overall, bootstrapping increased the number of rejected hypotheses by a factor of roughly 3.3 compared with Hoeffding's inequality (281 vs.\ 85 rejections in total).
Like we described for Hoeffding's inequality in the main text, we also ran this analysis with each hypothesis obtained using the unconditional bounds of \cite{manski}, and again obtained substantially fewer rejections compared with our approach based on Theorem \ref{thm:causal-bounds}.


%

%
%
%
%
%
%


%

%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%

\begin{figure}[t]
    \centering
        \begin{subfigure}[b]{0.26\textwidth}
    \includegraphics[height=3.7cm]{figures/causal/latest_experimental_results/Glucose_hyp_1_with_hoeff_onesided_loFalse_nogray.pdf}
    \subcaption{Not rejected}
    \label{fig:glucosea-supp}
    \end{subfigure}\hspace{1cm}%
    \begin{subfigure}[b]{0.26\textwidth}
    \includegraphics[height=3.7cm]{figures/causal/latest_experimental_results/Glucose_hyp_5_with_hoeff_onesided_loFalse_nogray.pdf}
    \subcaption{Rejected}
    \label{fig:glucoseb-supp}
    \end{subfigure}\\      
    \begin{subfigure}[b]{0.26\textwidth}
    \includegraphics[height=3.7cm]{figures/causal/latest_experimental_results/Glucose_hyp_8_with_hoeff_onesided_loFalse_nogray.pdf}
    \subcaption{Not rejected}
    \label{fig:chloridea}
    \end{subfigure}\hspace{1cm}%
    \begin{subfigure}[b]{0.26\textwidth}
    \includegraphics[height=3.7cm]{figures/causal/latest_experimental_results/Glucose_hyp_6_with_hoeff_onesided_loFalse_nogray.pdf}
    \subcaption{Rejected}
    \label{fig:chlorideb}
    \end{subfigure}\\
    \begin{subfigure}[b]{0.26\textwidth}
    \includegraphics[height=3.7cm]{figures/causal/latest_experimental_results/Glucose_hyp_46_with_hoeff_onesided_loFalse_nogray.pdf}
    \subcaption{Not rejected}
    \label{fig:potassiuma}
    \end{subfigure}\hspace{1cm}%
    \begin{subfigure}[b]{0.26\textwidth}
    \includegraphics[height=3.7cm]{figures/causal/latest_experimental_results/Glucose_hyp_7_with_hoeff_onesided_loFalse_nogray.pdf}
    \subcaption{Rejected}
    \label{fig:potassiumb}
    \end{subfigure}\\
    \begin{subfigure}[b]{0.26\textwidth}
    \includegraphics[height=3.7cm]{figures/causal/latest_experimental_results/Glucose_hyp_47_with_hoeff_onesided_loFalse_nogray.pdf}
    \subcaption{Not rejected}
    \label{fig:paco2a}
    \end{subfigure}\hspace{1cm}%
    \begin{subfigure}[b]{0.26\textwidth}
    \includegraphics[height=3.7cm]{figures/causal/latest_experimental_results/Glucose_hyp_10_with_hoeff_onesided_loFalse_nogray.pdf}
    \subcaption{Rejected}
    \label{fig:paco2b}
    \end{subfigure}
    %
    %
    %

    \caption{Raw observational data values conditional on $\A_{1:\tx}=\ax_{1:\tx}$ and $\X_{0:\tx}(\A_{1:\tx})\in \B_{0:\tx}$, and from the output of the twin conditional on $\Xt_{0:\tx}(\ax_{1:\tx})\in \B_{0:\tx}$.
    Each row shows two distinct choices of $(\B_{0:\tx}, \ax_{1:\tx})$.
    Below each figure are shown 95\% Hoeffding confidence intervals for $\Qt$ and $\Qup$.
    Unlike Figure \ref{fig:histograms} from the main text, the horizontal axes of the histograms are not truncated, and the first row is in particular an untruncated version of Figure \ref{fig:histograms} from the main text.
    Note however that the scales of the horizontal axes of the confidence intervals differ from those of the histograms, since it is visually more difficult to determine whether or not the confidence intervals overlap when fully zoomed out.} \label{fig:histograms-supplement}
\end{figure}

\subsection{Tightness of bounds and number of data points per hypothesis}
    In this section, we show empirically how both the tightness of the bounds $[\Qlo, \Qup]$ and the number of data points per hypothesis relate to the number of falsifications obtained in our case study.
    Recall that the tightness of $[\Qlo, \Qup]$ is determined by the value of $\Prob(\A_{1:\tx} = \ax_{1:\tx} \mid \X_{0:\N}(\A_{1:\N}) \in \B_{0:\N})$, since we have
    \begin{equation} \label{eq:N-propensity-tightness-relation}
        \frac{\Qup - \Qlo}{\yup - \ylo} = 1 - \Prob(\A_{1:\tx} = \ax_{1:\tx} \mid \X_{0:\N}(\A_{1:\N}) \in \B_{0:\N}).
    \end{equation}
    Here the left-hand side is a number in $[0, 1]$ that quantifies the tightness of the bounds $[\Qlo, \Qup]$ relative to the trivial worst-case bounds $[\ylo, \yup]$, with smaller values meaning tighter bounds. The equation above shows that the higher the value of $\Prob(\A_{1:\tx} = \ax_{1:\tx} \mid \X_{0:\N}(\A_{1:\N}) \in \B_{0:\N})$, the tighter the bounds are.
    
    Figure \ref{fig:scatter-plot} shows the bounds are often informative in practice, with $\Prob(\A_{1:\tx} = \ax_{1:\tx} \mid \X_{0:\N}(\A_{1:\N}) \in \B_{0:\N})$ being reasonably large (and hence the bounds tight, by \eqref{eq:N-propensity-tightness-relation} above) for a significant number of hypotheses we consider.
    However, rejections still occur even when the bounds are reasonably loose (e.g.\ $\Prob(\A_{1:\tx} = \ax_{1:\tx} \mid \X_{0:\N}(\A_{1:\N}) \in \B_{0:\N}) \approx 0.3$), which shows our method can still yield useful information even in this case.
    We moreover observe rejections across a range of different numbers of observational data points used to test each hypothesis, which shows that our method is not strongly dependent on the size of the dataset obtained. 

    \subsection{Sensitivity to $\ylo$ and $\yup$} \label{sec:sensitity-analysis-appendix}

    We investigated the sensitivity of our methodology with respect to our choices of the values $\ylo$ and $\yup$.
    Specifically, we repeated our procedure with the intervals $[\ylo, \yup]$ replaced with $[\ylo\, (1- \Delta/2), \yup\,(1 + \Delta/2)]$ for a range of different values of $\Delta \in \R$.
    Figure \ref{fig:sensitivity-plot-rejections} plots the number of rejections for different values of $\Delta$.
    We observe that for significantly larger $[\ylo, \yup]$ intervals, we do obtain fewer rejections, although this is to be expected since the widths of our both the bounds $[\Qlo, \Qup]$ and our confidence intervals $\qlo{\alpha}$ and $\qup{\alpha}$ obtained using Hoeffding's inequality (see Proposition \ref{prop:hoeffding-confidence-bounds-supp}) grow increasingly large as the width of $[\ylo, \yup]$ grows.
    However, we observe that the number of rejections per outcome is stable for a moderate range of widths of $[\ylo, \yup]$, which indicates that our method is reasonably robust to the choice of $\ylo, \yup$ parameters.

    %

    \begin{figure}[t]
        \centering
        \includegraphics[height=6cm]{figures/causal/latest_experimental_results/propensity-plots_nogray.pdf}
        \caption{Sample mean estimate of $\Prob(\A_{1:\tx} = \ax_{1:\tx} \mid \X_{0:\N}(\A_{1:\N}) \in \B_{0:\N})$ for each pair of hypotheses $(\Hlo, \Hup)$ corresponding to the same set of parameters $(\tx, \fx, \ax_{1:\tx}, \B_{0:\tx})$ that we tested, along with the corresponding number of observational data points used to test each hypothesis.
        Red points indicate that either $\Hlo$ or $\Hup$ were rejected, while blue points indicate that both $\Hlo$ and $\Hup$ were not rejected.}
        \label{fig:scatter-plot}
    \end{figure}


    \begin{figure}[t]
    \centering
\includegraphics[width=0.6\textwidth]{figures/causal/latest_experimental_results/sensitivity-plots-rejections_nogray.pdf}
    \caption{Rejections obtained as the width of the $[\ylo, \yup]$ interval changes. Here, the interval is increased (or decreased) symmetrically on each side.}
    \label{fig:sensitivity-plot-rejections}
\end{figure}

%
%
