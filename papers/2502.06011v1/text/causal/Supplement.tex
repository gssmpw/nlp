\section{Notation} \label{sec:notation}

\begin{tabularx}{\linewidth}{l X}
$z_{\tx:\tx'}$ & The sequence of elements $(z_\tx, \ldots, z_{\tx'})$ (or the empty sequence when $\tx > \tx'$) \\
$\mathcal{Z}_{\tx:\tx'}$ (where each $\mathcal{Z}_{i}$ is a set) & The cartesian product $\mathcal{Z}_{\tx} \times \cdots \times \mathcal{Z}_{\tx'}$ (or the empty set when $\tx > \tx'$) \\
$Z_{\tx:\tx'}(\ax_{1:\tx'})$ & The sequence of potential outcomes $Z_\tx(\ax_{1:\tx}), \ldots, Z_{\tx'}(\ax_{1:\tx'})$ (or the empty sequence when $\tx > \tx'$) \\
$\Law[Z]$ & The distribution of the random variable $Z$ \\
$\Law[Z \mid M]$ & The conditional distribution of $Z$ given $M$, where $M$ is either an event or a random variable \\
%
$Z \eqas Z'$ & The random variables $Z$ and $Z'$ are almost surely equal, i.e.\ $\Prob(Z = Z') = 1$ \\
$Z \ci Z'$ & The random variables $Z$ and $Z'$ are independent \\
$Z \ci Z' \mid Z''$ & The random variables $Z$ and $Z'$ are conditionally independent given the random variable $Z''$ \\
$\ind(E)$ & Indicator function of some event $E$ %
%
%
%
%
%
%
%
\end{tabularx}

%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\section{Proof of Proposition \ref{prop:interventional-correctness-alternative-characterisation} (unconditional form of interventional correctness)} \label{sec:unconditional-interventional-correctness-proof}

In this section we prove Proposition \ref{prop:interventional-correctness-alternative-characterisation} from the main text.
To account for measure-theoretic technicalities, we first clarify slightly our earlier definition of interventional correctness.
%
Since each $\Xspace_\tx = \R^{\Xspacedim_\tx}$ is real-valued, $\X_{0:\T}(\ax_{1:\T})$ admits a regular conditional distribution given $\X_0$ \citep[Theorem 6.3]{kallenberg1997foundations}.
%
By interventional correctness, we then mean that, for all $\ax_{1:\T} \in \Aspace_{1:\T}$, the map $(\xx_0, \B_{1:\T}) \mapsto \Law[\Xt_{1:\T}(\xx_0, \ax_{1:\T})](\B_{1:\T})$ is a version of this conditional distribution, i.e.\ it is a Markov kernel such that
\begin{equation} \label{eq:interventional-correctness-precise-form}
    \Law[\Xt_{1:\T}(\xx_0, \ax_{1:\T})] = \Law[\X_{1:\T}(\ax_{1:\T}) \mid \X_0 = \xx_0] \qquad \text{for $\Law[\X_0]$-almost all $\xx_0 \in \Xspace_0$}.
\end{equation}
The result is then as follows.

\begin{manualproposition}{\ref{prop:interventional-correctness-alternative-characterisation}}
    The twin is interventionally correct if and only if, for all $\ax_{1:\T} \in \Aspace_{1:\T}$, it holds that
    \begin{equation} \label{eq:interventional-correctness-alternative}
        \Law[\X_0, \Xt_{1:\T}(\X_0, \ax_{1:\T})] = \Law[\X_{0:\T}(\ax_{1:\T})]
    \end{equation}
    %
\end{manualproposition}

\begin{proof}
Fix any choice of $\ax_{1:\T} \in \Aspace_{1:\T}$.
Since both $\Law[\X_0, \Xt_{1:\T}(\X_0, \ax_{1:\T})]$ and $\Law[\X_{0:\T}(\ax_{1:\T})]$ have the same $\Xspace_0$-marginal, namely $\Law[\X_0]$, \eqref{eq:interventional-correctness-alternative} holds if and only if
\begin{equation} \label{eq:joint-interventional-correctness-proof-intermediate-step}
    \Law[\Xt_{1:\T}(\X_0, \ax_{1:\T}) \mid \X_0 = \xx_0] = \Law[\X_{1:\T}(\ax_{1:\T}) \mid \X_0 = \xx_0] \qquad \text{for $\Law[\X_0]$-almost all $\xx_0 \in \Xspace_0$}.
\end{equation}
%
But now, our definition of $\Xt_{1:\T}(\xx_0, \ax_{1:\T})$ in terms of $\twinfunction_\tx$ and $\twinnoise_{1:\tx}$ means we can write
\[
    \Xt_{1:\T}(\X_0, \ax_{1:\T}) = \boldsymbol{\twinfunction}(\X_0, \ax_{1:\T}, \twinnoise_{1:\T}),
\]
where $\boldsymbol{\twinfunction}(\xx_0, \ax_{1:\T}, \ux_{1:\T}) \coloneqq (\twinfunction_1(\xx_0, \ax_1, \ux_1), \ldots, \twinfunction_\T(\xx_0, \ax_{1:\T}, \ux_{1:\T}))$.
%
For all $\xx_0 \in \Xspace_0$ and measurable $\B_{1:\T} \subseteq \Xspace_{1:\T}$, we then have
\begin{align*}
    \Law[\Xt_{1:\T}(\xx_0, \ax_{1:\T})](\B_{1:\T}) &= \E[\ind(\boldsymbol{\twinfunction}(\xx_0, \ax_{1:\T}, \twinnoise_{1:\T}) \in \B_{1:\T})] \\
    &= \int \ind(\boldsymbol{\twinfunction}(\xx_0, \ax_{1:\T}, \ux_{1:\T}) \in \B_{1:\T}) \, \Law[\twinnoise_{1:\T}](\dee \ux_{1:\T}).
\end{align*}
It is standard to show that the right-hand side is a Markov kernel in $\xx_0$ and $\B_{1:\T}$.
Moreover, for any measurable $\B_0 \subseteq \Xspace_0$, we have
\begin{align*}
    &\int_{\B_0} \Law[\Xt_{1:\T}(\xx_0, \ax_{1:\T})](\B_{1:\T}) \, \Law[\X_0](\dee \xx_0) \\
        &\qquad= \int_{\B_0} \left[\int \ind(\boldsymbol{\twinfunction}(\xx_0, \ax_{1:\T}, \ux_{1:\T}) \in \B_{1:\T}) \, \Law[\twinnoise_{1:\T}](\dee \ux_{1:\T}) \right] \, \Law[\X_0](\dee \xx_0) \\
        &\qquad= \int \ind(\xx_0 \in \B_0, \boldsymbol{\twinfunction}(\xx_0, \ax_{1:\T}, \ux_{1:\T}) \in \B_{1:\T}) \, \Law[\X_0, \twinnoise_{1:\T}](\dee \xx_0, \dee \ux_{1:\T}) \\
        &\qquad= \Law[\X_0, \Xt_{1:\T}(\X_0, \ax_{1:\T})](\B_{0:\T}),
\end{align*}
where the second step follows because $\X_0 \ci \twinnoise_{1:\T}$.
It therefore follows that $(\xx_0, \B_{1:\T}) \mapsto \Law[\Xt_{1:\T}(\xx_0, \ax_{1:\T})](\B_{1:\T})$ is a regular conditional distribution of $\Xt_{1:\T}(\X_0, \ax_{1:\T})$ given $\X_0$, i.e.\
\[
    \Law[\Xt_{1:\T}(\xx_0, \ax_{1:\T})] = \Law[\Xt_{1:\T}(\X_0, \ax_{1:\T}) \mid \X_0 = \xx_0] \qquad \text{for $\Law[\X_0]$-almost all $\xx_0 \in \Xspace_0$.}
\]
Substituting this into \eqref{eq:joint-interventional-correctness-proof-intermediate-step}, we see that \eqref{eq:interventional-correctness-alternative} holds if and only if \eqref{eq:interventional-correctness-precise-form} does.
The result now follows since $\ax_{1:\T}$ was arbitrary.
\end{proof}

\section{Online prediction} \label{sec:online-prediction}

A distinguishing feature of many digital twins is their ability to integrate real-time information obtained from sensors in their environment \cite{barricelli2019survey}.
It is therefore relevant to consider a setting in which a twin is used repeatedly to make a sequence of predictions over time, each time taking all previous information into account.
One way to formalise this is to instantiate our model for the twin at each timestep.
For example, we could represent the predictions made by the twin at $\tx = 0$ after observing initial covariates $\xx_0$ as potential outcomes $(\Xt^1_{1:\T}(\xx_0, \ax_{1:\T}) : \ax_{1:\T} \in \Aspace_{1:\T})$, similar to what we did in the main text.
We could then represent the predictions made by the twin after some action $\ax_1$ is taken and an additional observation $\xx_1$ is made via potential outcomes $(\Xt^2_{2:\T}(\xx_{0:1}, \ax_{1:\T}) : \ax_{2:\T} \in \Aspace_{2:\T})$.
More generally, for $\tx \in \{1, \ldots, \T\}$, we could introduce potential outcomes $(\Xt^{\tx}_{\tx:\T}(\xx_{0:\tx-1}, \ax_{1:\T}) : \ax_{\tx:\T} \in \Aspace_{\tx:\T})$ to represent the predictions that the twin would make at time $\tx$ after the observations $\xx_{0:\tx-1}$ are made and the actions $\ax_{1:\tx-1}$ are taken.

\subsection{Correctness in the online setting}

This extended model requires a new definition of correctness than our Definition \ref{eq:interventional-correctness} from the main text.
%
%
%
%
%
%
A natural approach is to say that the twin is correct in this new setting if
\begin{equation}
    \Law[\Xt_{\tx:\T}^\tx(\xx_{0:\tx-1}, \ax_{1:\T})]
        = \Law[\X_{\tx:\T}(\ax_{1:\T}) \mid \X_{0:\tx-1}(\ax_{1:\tx-1}) = \xx_{0:\tx-1}] \label{eq:online-interventional-correctness-def}
\end{equation}
for all $\tx \in \{1, \ldots, \T\}$, $\ax_{1:\T} \in \Aspace_{1:\T}$, and $\Law[\X_{0:\tx-1}(\ax_{1:\tx-1})]$-almost all $\xx_{0:\tx-1} \in \Xspace_{0:\tx-1}$.
%
A twin with this property would at each step be able to accurately simulate the future in light of previous information, use this to choose a next action to take, observe the result of doing so, and then repeat.
It is possible to show that \eqref{eq:online-interventional-correctness-def} holds if and only if we have
\begin{align*}
    \Law[\Xt^1_{1:\T}(\xx_0, \ax_{1:\T})] &= \Law[\X_{1:\T}(\ax_{1:\T}) \mid \X_{0}=\xx_0] \\
    \Law[\Xt^\tx_{\tx:\T}(\xx_{0:\tx-1}, \ax_{1:\T})]
        &= \Law[\Xt^1_{\tx:\T}(\xx_0, \ax_{1:\T}) \mid \Xt_{1:\tx-1}^1(\xx_0, \ax_{1:\tx-1}) = \xx_{1:\tx-1}]
\end{align*}
for all $\tx \in \{1, \ldots, \T\}$, $\ax_{1:\T} \in \Aspace_{1:\T}$, $\Law[\X_0]$-almost all $\xx_0 \in \Xspace_0$, and $\Law[\Xt_{1:\tx-1}^1(\xx_0, \ax_{1:\tx-1})]$-almost all $\xx_{1:\tx-1} \in \Xspace_{1:\tx-1}$.
The first condition here says that $\Xt^1_{1:\T}(\xx_0, \ax_{1:\T})$ must be interventionally correct in the sense of Definition \ref{eq:interventional-correctness} from the main text.
The second condition says that the predictions made by the twin across different timesteps must be internally consistent with each other insofar as their conditional distributions must align.
This holds automatically in many circumstances, such as if the predictions of the twin are obtained from a Bayesian model (for example), and otherwise could be checked numerically given the ability to run simulations from the twin, without the need to obtain data or refer to the real-world process in any way.
As such, the problem of assessing the correctness of the twin in this new sense primarily reduces to the problem of assessing the correctness of $\Xt^1_{1:\T}(\xx_0, \ax_{1:\T})$ in the sense of Definition \ref{eq:interventional-correctness} in the main text, which motivates our focus on that condition.
%

%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%

%

%

%
%

%
%
%
%
%
%
%
%

%

\subsection{Alternative notions of online correctness} \label{sec:online-correctness-alternative-notion-supp}

An important and interesting subtlety arises in this context that is worth noting.
In general it does not follow that a twin correct in the sense of \eqref{eq:online-interventional-correctness-def} satisfies
\begin{equation}
    \Law[\Xt_{\tx:\T}^\tx(\xx_{0:\tx-1}, \ax_{1:\T})] \
        = \Law[\X_{\tx:\T}(\ax_{1:\T}) \mid \X_{0:\tx-1}(\ax_{1:\tx-1}) = \xx_{0:\tx-1}, \A_{1:\tx-1} = \ax_{1:\tx-1}] \label{eq:online-interventional-correctness-alt}
\end{equation}
for all $\ax_{1:\T} \in \Aspace_{1:\T}$, and $\Law[\X_{0:\tx-1}(\ax_{1:\tx-1}) \mid \A_{1:\tx-1} = \ax_{1:\tx-1}]$-almost all $\xx_{0:\tx-1} \in \Xspace_{0:\tx-1}$, 
since in general it does not hold that 
\begin{multline*}
    \Law[\X_{\tx:\T}(\ax_{1:\T}) \mid \X_{0:\tx-1}(\ax_{1:\tx-1}) = \xx_{0:\tx-1}]\\
        = \Law[\X_{\tx:\T}(\ax_{1:\T}) \mid \X_{0:\tx-1}(\ax_{1:\tx-1}) = \xx_{0:\tx-1}, \A_{1:\tx-1} = \ax_{1:\tx-1}].
\end{multline*}
for all $\ax_{1:\T} \in \Aspace_{1:\T}$ and $\Law[\X_{0:\tx-1}(\ax_{1:\tx-1}) \mid \A_{1:\tx-1} = \ax_{1:\tx-1}]$-almost all $\xx_{0:\tx-1} \in \Xspace_{0:\tx-1}$ unless the actions $\A_{1:\tx-1}$ are unconfounded.
(Here as usual $\A_{1:\T}$ denotes the actions of a behavioural agent; see Section \ref{sec:data-driven-twin-assessment} of the main text.)
In other words, a twin that is correct in the sense of \eqref{eq:online-interventional-correctness-def} will make accurate predictions at time $\tx$ when every action taken before time $\tx$ was unconfounded (as occurs for example when the twin is directly in control of the decision-making process), but in general not when certain taken actions before time $\tx$ were chosen by a behavioural agent with access to more context than is available to the twin (as may occur for example when the twin is used as a decision-support tool).
However, should it be desirable, our framework could be extended to encompass the alternative condition in \eqref{eq:online-interventional-correctness-alt} by relabelling the observed history $(\X_{0:\tx-1}(\A_{1:\tx-1}), \A_{1:\tx-1})$ as $\X_0$, and then assessing the correctness of the potential outcomes $\Xt_{\tx:\T}^\tx(\xx_{0:\tx-1}, \ax_{1:\T})$ in the sense of Definition \ref{eq:interventional-correctness} from the main text.

%
%

%

%
%

%
%

%
%

Overall, the ``right'' notion of correctness in this online setting is to some extent a design choice.
We believe our causal approach to twin assessment provides a useful framework for formulating and reasoning about these possibilities, and consider the investigation of assessment strategies for additional usage regimes to be an interesting direction for future work.

\section{Proof of Theorem \ref{prop:nonidentifiability} (interventional distributions are not identifiable)} \label{sec:non-identifiability-result-proof-supp}

It is well-known in the causal inference literature that the interventional behaviour of the real-world process cannot be uniquely identified from observational data.
For completeness, we now provide a self-contained proof of this result in our notation.
Our statement here is lengthier than Theorem \ref{prop:nonidentifiability} in the main text in order to clarify what is meant by ``uniquely identified'': intuitively, the idea is that there always exist distinct families of potential outcomes whose interventional behaviours differ and yet give rise to the same observational data.
%
%

\begin{manualtheorem}{\ref{prop:nonidentifiability}}
    Suppose we have $\ax_{1:\T} \in \Aspace_{1:\T}$ such that $\Prob(\A_{1:\T} \neq \ax_{1:\T}) > 0$.
    %
    Then there exist potential outcomes $(\tilde{\X}_{0:\T}(\ax_{1:\T}') : \ax_{1:\T}' \in \Aspace_{1:\T})$ such that %
    \begin{equation} \label{eq:nonidentifiability-proof-almost-sure-equality}
        (\tilde{\X}_{0:\T}(\A_{1:\T}), \A_{1:\T}) \eqas (\X_{0:\T}(\A_{1:\T}), \A_{1:\T}).
    \end{equation}
    but for which $\Law[\tilde{\X}_{0:\T}(\ax_{1:\tx})] \neq \Law[\X_{0:\T}(\ax_{1:\tx})]$.
\end{manualtheorem}

\begin{proof}
    Our assumption that $\Prob(\A_{1:\T} \neq \ax_{1:\T}) > 0$ means there must exist some $\tx \in \{1, \ldots, \T\}$ such that $\Prob(\A_{1:\tx} \neq \ax_{1:\tx}) > 0$.
    Since $\Xspace_\tx = \R^{\Xspacedim_\tx}$, we may also choose some $\xx_\tx \in \Xspace_\tx$ with $\Prob(\X_\tx(\ax_{1:\tx}) = \xx_\tx \mid \A_{1:\tx} \neq \ax_{1:\tx}) \neq 1$.
    %
    %
    Then, for each $\sx \in \{0, \ldots, \T\}$ and $\ax_{1:\sx}' \in \Aspace_{1:\sx}$, define
    \[
         \tilde{\X}_{\sx}(\ax_{1:\sx}') \coloneqq \begin{cases}
            \ind(\A_{1:\tx} = \ax_{1:\tx}) \, \X_{\tx}(\ax_{1:\tx}) + \ind(\A_{1:\tx} \neq \ax_{1:\tx}) \, \xx_\tx & \text{if $\sx = \tx$ and $\ax_{1:\sx}' = \ax_{1:\tx}$} \\
            \X_{\sx}(\ax_{1:\sx}') & \text{otherwise},
         \end{cases}
    \]
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    It is then easily checked that \eqref{eq:nonidentifiability-proof-almost-sure-equality} holds, but
    \begin{align*}
        &\Law[\tilde{\X}_{\tx}(\ax_{1:\tx})]\\
        &\qquad= \Law[\tilde{\X}_{\tx}(\ax_{1:\tx}) \mid \A_{1:\tx} = \ax_{1:\tx}] \, \Prob(\A_{1:\tx} = \ax_{1:\tx}) + \Law[\tilde{\X}_{\tx}(\ax_{1:\tx}) \mid \A_{1:\tx} \neq \ax_{1:\tx}] \, \Prob(\A_{1:\tx} \neq \ax_{1:\tx}) \\
        &\qquad= \Law[X_{\tx}(\ax_{1:\tx}) \mid \A_{1:\tx} = \ax_{1:\tx}] \, \Prob(\A_{1:\tx} = \ax_{1:\tx}) + \mathrm{Dirac}(\xx_\tx) \, \Prob(\A_{1:\tx} \neq \ax_{1:\tx}) \\
        &\qquad\neq \Law[X_{\tx}(\ax_{1:\tx}) \mid \A_{1:\tx} = \ax_{1:\tx}] \, \Prob(\A_{1:\tx} = \ax_{1:\tx}) + \Law[\X_{\tx}(\ax_{1:\tx}) \mid \A_{1:\tx} \neq \ax_{1:\tx}] \, \Prob(\A_{1:\tx} \neq \ax_{1:\tx}) \\
        &\qquad= \Law[X_{\tx}(\ax_{1:\tx})],
    \end{align*}
    from which the result follows.
    %
    %
    %
    %
    %
    %
    %
    %
\end{proof}

%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\section{Deterministic potential outcomes are unconfounded} \label{eq:deterministic-potential-outcomes-are-unconfounded}

In this section we expand on our earlier claim that, if the real-world process is deterministic, then the observational data is unconfounded.
We first make this claim precise.
By ``deterministic'', we mean that there exist measurable functions $\gx_\tx$ for $\tx \in \{1, \ldots, \T\}$ such that
\begin{equation} \label{eq:potential-outcomes-are-deterministic}
    \X_\tx(\ax_{1:\tx}) \eqas \gx_\tx(\X_{0:\tx-1}(\ax_{1:\tx-1}), \ax_{1:\tx}) \qquad \text{for all $\tx \in \{1, \ldots, \T\}$ and $\ax_{1:\tx} \in \Aspace_{1:\tx}$.}
\end{equation}
By ``unconfounded'', we mean that the \emph{sequential randomisation assumption (SRA)} introduced by Robins \cite{robins1986new} holds, i.e.\
\begin{equation} \label{eq:actions-are-unconfounded}
    (\X_{\sx}(\ax_{1:\sx}) : \sx \in \{1, \ldots, \T\}, \ax_{1:\sx} \in \Aspace_{1:\sx}) \ci \A_\tx \mid \X_{0:\tx-1}(\A_{1:\tx-1}), \A_{1:\tx-1} \qquad \text{for all $\tx \in \{1, \ldots, \T\}$},
\end{equation}
where $\ci$ denotes conditional independence.
Intuitively, this says that, apart from the historical observations $(\X_{0:\tx-1}(\A_{1:\tx-1}), \A_{1:\tx-1})$, any additional factors that influence the agent's choice of action $\A_\tx$ are independent of the behaviour of the real-world process.
The SRA provides a standard formulation of the notion of unconfoundedness in longitudinal settings such as ours (see \cite[Chapter 5]{tsiatis2019dynamic} for a review).

It is now a standard exercise to show that \eqref{eq:potential-outcomes-are-deterministic} implies \eqref{eq:actions-are-unconfounded}.
We include a proof below for completeness.
Key to this is the following straightforward Lemma.

\begin{lemma}\label{lem:determinism_conditional_independence}
Suppose $U$ and $V$ are random variables such that, for some measurable function $g$, it holds that $U \eqas g(V)$.
Then, for any other random variable $W$, we have
\[
    U \ci W \mid V.
\]
\end{lemma}

\begin{proof}
By standard properties of conditional expectations, for any measurable sets $S_1$ and $S_2$, we have almost surely
\begin{align*}
    \Prob(U \in S_1, W \in S_2 \mid V) &= \E[\ind(g(V) \in S_1) \, \ind(W \in S_2) \mid V] \\
    %
    &= \ind(g(V) \in S_1) \, \E[\ind(W \in S_2) \mid V] \\
    &= \E[\ind(U \in S_1) \mid V] \, \Prob(W \in S_2 \mid V) \\
    &= \Prob(U \in S_1 \mid V) \, \Prob(W \in S_2 \mid V),
\end{align*}
which gives the result.
\end{proof}

It is now easy to see that \eqref{eq:potential-outcomes-are-deterministic} implies \eqref{eq:actions-are-unconfounded}.
Indeed, by recursive substitution, it is straightforward to show that there exist measurable functions $\tilde{g}_\tx$ for $\tx \in \{1, \ldots, \T\}$ such that
\[
    \X_\tx(\ax_{1:\tx}) \eqas \tilde{g}_\tx(\X_{0}, \ax_{1:\tx}) \qquad \text{for all $\tx \in \{1, \ldots, \T\}$ and $\ax_{1:\tx} \in \Aspace_{1:\tx}$},
\]
and so
\[
    (\X_{\sx}(\ax_{1:\sx}) : \sx \in \{1, \ldots, \T\}, \ax_{1:\sx} \in \Aspace_{1:\sx})
        = (\tilde{g}_\tx(\X_{0}, \ax_{1:\sx}) : \sx \in \{1, \ldots, \T\}, \ax_{1:\sx} \in \Aspace_{1:\sx}).
\]
The right-hand side is now seen to be a measurable function of $\X_0$ and hence certainly of $(\X_{0:\tx-1}(\A_{1:\tx-1}), \A_{1:\tx-1})$, so that the result follows by Lemma \ref{lem:determinism_conditional_independence}.

%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\input{text/causal/ToyExample}

\section{Causal bounds} \label{sec:causal-bounds-proofs}

%

%
%
%
%
%
%
%

%
%
%
%
%
%
%
%




\subsection{Proof of Theorem \ref{thm:causal-bounds}}

%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\begin{manualtheorem}{\ref{thm:causal-bounds}} \label{thm:causal-bounds-supp}
    Suppose $(\Y(\ax_{1:\tx}) : \ax_{1:\tx} \in \Aspace_{1:\tx})$ are real-valued potential outcomes, and that for some $\tx \in \{1, \ldots, \T\}$, $\ax_{1:\tx} \in \Aspace_{1:\tx}$, measurable $\B_{0:\tx} \subseteq \Xspace_{0:\tx}$, and $\ylo, \yup \in \R$ we have
    \begin{gather}
        \Prob(\X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx}) > 0 \label{eq:causal-bounds-proof-positivity} \\
        \Prob(\ylo \leq \Y(\ax_{1:\tx}) \leq \yup \mid \X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx}) = 1.  \label{eq:causal-bounds-proof-boundedness}
    \end{gather}
    Then it holds that
    \[
        \E[\Ylo \mid \X_{0:\N}(\A_{1:\N}) \in \B_{0:\N}]
        \leq \E[\Y(\ax_{1:\tx}) \mid \X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx}]
        \leq \E[\Yup \mid \X_{0:\N}(\A_{1:\N}) \in \B_{0:\N}]
    \]
    where we define the following random variables:
    \begin{align*}
        \N &\coloneqq \max \{0 \leq \sx \leq \tx \mid \A_{1:\sx} = \ax_{1:\sx}\} \\
        \Ylo &\coloneqq \ind(\A_{1:\tx} = \ax_{1:\tx}) \, \Y(\A_{1:\tx}) + \ind(\A_{1:\tx} \neq \ax_{1:\tx}) \, \ylo \\
        \Yup &\coloneqq \ind(\A_{1:\tx} = \ax_{1:\tx}) \, \Y(\A_{1:\tx}) + \ind(\A_{1:\tx} \neq \ax_{1:\tx}) \, \yup.
    \end{align*}
\end{manualtheorem}

\begin{proof}
We prove the lower bound; the upper bound is analogous.
It is easily checked that 
\begin{multline} \label{eq:causal-bounds-proof-first-step}
    \E[\Y(\ax_{1:\tx}) \mid \X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx}]\\
        = \E[\Y(\ax_{1:\tx}) \mid \X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx}, \A_{1:\tx} = \ax_{1:\tx}] \, \Prob(\A_{1:\tx} = \ax_{1:\tx} \mid \X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx}) \\
            \quad + \E[\Y(\ax_{1:\tx}) \mid \X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx}, \A_{1:\tx} \neq \ax_{1:\tx}] \, \Prob(\A_{1:\tx} \neq \ax_{1:\tx} \mid \X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx}).
\end{multline}
If $\Prob(\A_{1:\tx} = \ax_{1:\tx} \mid \X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx}) > 0$, then
\begin{align*}
    \E[\Y(\ax_{1:\tx}) \mid \X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx}, \A_{1:\tx} = \ax_{1:\tx}]
        &= \E[\Y(\A_{1:\tx}) \mid \X_{0:\tx}(\A_{1:\tx}) \in \B_{0:\tx}, \A_{1:\tx} = \ax_{1:\tx}] \\
        &= \E[\Y(\A_{1:\N}) \mid \X_{0:\N}(\A_{1:\N}) \in \B_{0:\N}, \A_{1:\tx} = \ax_{1:\tx}],
\end{align*}
where the second step follows because $\Prob(\N = \tx \mid \A_{1:\tx} = \ax_{1:\tx}) = 1$.
Similarly, if $\Prob(\A_{1:\tx} \neq \ax_{1:\tx} \mid \X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx}) > 0$, then \eqref{eq:causal-bounds-proof-boundedness} implies
\[
    \E[\Y(\ax_{1:\tx}) \mid \X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx}, \A_{1:\tx} \neq \ax_{1:\tx}]
        \geq \ylo.
\]
Substituting these results into \eqref{eq:causal-bounds-proof-first-step}, we obtain
\begin{multline} \label{eq:causal-bounds-proof-second-step}
    \E[\Y(\ax_{1:\tx}) \mid \X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx}]\\ 
    \geq \E[\Y(\A_{1:\tx}) \mid \X_{0:\N}(\A_{1:\N}) \in \B_{0:\N}, \A_{1:\tx} = \ax_{1:\tx}] \, \Prob(\A_{1:\tx} = \ax_{1:\tx} \mid \X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx}) \\
            + \ylo \, \Prob(\A_{1:\tx} \neq \ax_{1:\tx} \mid \X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx}).
\end{multline}
Now observe that the right-hand side of \eqref{eq:causal-bounds-proof-second-step} is a convex combination with mixture weights $\Prob(\A_{1:\tx} = \ax_{1:\tx} \mid \X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx})$ and $\Prob(\A_{1:\tx} \neq \ax_{1:\tx} \mid \X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx})$.
We can bound
\begin{align}
    \Prob(\A_{1:\tx} = \ax_{1:\tx} \mid \X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx})
        &= \frac{\Prob(\X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx}, \A_{1:\tx} = \ax_{1:\tx})}{\Prob(\X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx})} \notag \\
        &\geq \frac{\Prob(\X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx}, \A_{1:\tx} = \ax_{1:\tx})}{\Prob(\X_{0:\N}(\ax_{1:\N}) \in \B_{0:\N})} \notag \\
        &= \frac{\Prob(\X_{0:\N}(\A_{1:\N}) \in \B_{0:\N}, \A_{1:\tx} = \ax_{1:\tx})}{\Prob(\X_{0:\N}(\A_{1:\N}) \in \B_{0:\N})} \notag \\
        &= \Prob(\A_{1:\tx} = \ax_{1:\tx} \mid \X_{0:\N}(\A_{1:\N}) \in \B_{0:\N}), %
\end{align}
where the inequality holds because $\tx \geq \N$ almost surely, and the second equality holds because the definition of $\N$ means
\[
    \X_{0:\N}(\ax_{1:\N}) \eqas \X_{0:\N}(\A_{1:\N}).
\]
As such, we can bound the convex combination in \eqref{eq:causal-bounds-proof-second-step} from below by replacing its mixture weights with $\Prob(\A_{1:\tx} = \ax_{1:\tx} \mid \X_{0:\N}(\A_{1:\N}) \in \B_{0:\N})$ and $\Prob(\A_{1:\tx} \neq \ax_{1:\tx} \mid \X_{0:\N}(\ax_{1:\N}) \in \B_{0:\N})$, which shifts weight from the $\E[\Y(\A_{1:\tx}) \mid \A_{1:\tx} = \ax_{1:\tx}, \X_{0:\N}(\A_{1:\N}) \in \B_{0:\N}]$ term onto the $\ylo$ term.
This yields
\begin{align*}
    &\E[\Y(\ax_{1:\tx}) \mid \X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx}] \\
        &\qquad\qquad\geq \E[\Y(\A_{1:\tx}) \mid \X_{0:\N}(\A_{1:\N}) \in \B_{0:\N}, \A_{1:\tx} = \ax_{1:\tx}] \, \Prob(\A_{1:\tx} = \ax_{1:\tx} \mid \X_{0:\N}(\ax_{1:\N}) \in \B_{0:\N}) \\
        &\qquad\qquad\qquad+ \ylo \, \Prob(\A_{1:\tx} \neq \ax_{1:\tx} \mid \X_{0:\N}(\ax_{1:\N}) \in \B_{0:\N}) \\
        &\qquad\qquad= \E[\Y(\A_{1:\tx}) \, \ind(\A_{1:\tx} = \ax_{1:\tx}) + \ylo \, \ind(\A_{1:\tx} \neq \ax_{1:\tx}) \mid \X_{0:\N}(\A_{1:\N}) \in \B_{0:\N}] \\
        &\qquad\qquad= \E[\Ylo \mid \X_{0:\N}(\A_{1:\N}) \in \B_{0:\N}].
\end{align*}
\end{proof}

\subsection{Proof of Proposition \ref{prop:sharpness-of-bounds} (sharpness of bounds)} \label{sec:sharpness-of-bounds-supplement}

%
%


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


\begin{manualproposition}{\ref{prop:sharpness-of-bounds}}
    Under the same setup as in Theorem \ref{thm:causal-bounds}, there always exists potential outcomes $(\tilde{\X}_{0:\T}(\ax_{1:\T}'), \tilde{\Y}(\ax_{1:\tx}') : \ax_{1:\T}' \in \Aspace_{1:\T})$ with
    \[
        \Prob(\ylo \leq \tilde{\Y}(\ax_{1:\tx}) \leq \ylo \mid \tilde{\X}_{0:\tx} \in \B_{0:\tx}) = 1
    \]
    and moreover
    \begin{equation} \label{eq:sharpness-of-bounds-proof-new-outcomes-indistinguishable}
        (\tilde{\X}_{0:\T}(\A_{1:\T}), \tilde{\Y}(\A_{1:\tx}), \A_{1:\T}) \eqas (\X_{0:\T}(\A_{1:\T}), \Y(\A_{1:\tx}), \A_{1:\T}),
    \end{equation}
    but for which
    \[
        \E[\tilde{\Y}(\ax_{1:\tx}) \mid \tilde{\X}_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx}] = \Qlo.
    \]
    The corresponding statement is also true for $\Qup$.
\end{manualproposition}


\begin{proof}
    We consider the case of the lower bound; the case of the upper bound is analogous.
    Choose $\xx_{1:\T} \in \B_{1:\T}$ arbitrarily.
    (Certainly some choice is always possible, since each $\B_{\sx}$ has positive measure and is therefore nonempty.)
    Define
    \begin{align*}
        \tilde{\X}_0 &\coloneqq \X_0 \\
        \tilde{\X}_{\sx}(\ax_{1:\sx}') &\coloneqq \ind(\A_{1:\sx} = \ax_{1:\sx}') \, \X_{\sx}(\ax_{1:\sx}') + \ind(\A_{1:\sx} \neq \ax_{1:\sx}') \, \xx_{\sx} \quad \text{for each $\sx \in \{0, \ldots, \T\}$ and $\ax_{1:\sx}' \in \Aspace_{1:\sx}$},
        %
        %
    \end{align*}
    and similarly let
    \[
        %
        \tilde{\Y}(\ax_{1:\tx}') = \ind(\A_{1:\tx} = \ax_{1:\tx}') \, \Y(\ax_{1:\tx}') + \ind(\A_{1:\tx} \neq \ax_{1:\tx}')\, \ylo \qquad \text{for all $\ax_{1:\tx}' \in \Aspace_{1:\tx}$}.
    \]
    It is easy to check that \eqref{eq:sharpness-of-bounds-proof-new-outcomes-indistinguishable} holds.
    But now, we have directly
    \[
        \tilde{\Y}(\ax_{1:\tx}) = \Ylo.
    \]
    Moreover, it is easily checked from the definition of $\N$ and $\tilde{\X}_{0:\tx}(\ax_{1:\tx})$ that
    \[
        \tilde{\X}_{0:\tx}(\ax_{1:\tx}) \eqas (\X_{0:\N}(\A_{1:\N}), \xx_{\N+1:\tx}),
    \]
    so that
    \begin{align*}
        \ind(\tilde{\X}_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx})
        &\eqas \ind(\tilde{\X}_{0:\N}(\ax_{1:\N}) \in \B_{0:\N}, \xx_{\N+1:\tx} \in \B_{\N+1:\tx}) \\
        &\eqas \ind(\X_{0:\N}(\A_{1:\N}) \in \B_{0:\N})
    \end{align*}
    since each $\xx_{\sx} \in \B_\sx$.
    Consequently,
    \begin{align*}
        \E[\tilde{\Y}(\ax_{1:\tx}) \mid \tilde{\X}_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx}]
        &= \E[\Ylo \mid \tilde{\X}_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx}] \\
        &= \E[\Ylo \mid \X_{0:\N}(\A_{1:\N}) \in \B_{0:\N}],
    \end{align*}
    which gives the result.
\end{proof}

\subsubsection{Our case} \label{sec:our-case-sharpness}

Proposition \ref{prop:sharpness-of-bounds} considered $(\Y(\ax_{1:\tx}') : \ax_{1:\tx}' \in \Aspace_{1:\tx})$ to be arbitrary potential outcomes defined jointly on the same probability space as $(\X_{0:\tx}(\ax_{1:\tx}') : \ax_{1:\tx}' \in \Aspace_{1:\tx})$.
In contrast, our falsification methodology (Section \ref{sec:hypotheses-from-causal-bounds} of the main text) assumes the particular form
\[
    \Y(\ax_{1:\tx}') \eqas \fx(\X_{0:\tx}(\ax_{1:\tx}')),
\]
%
%
%
%
for some known measurable function $\fx : \Xspace_{0:\tx} \to \R$.
Proposition \ref{prop:sharpness-of-bounds} does not directly imply that the bounds in Theorem \ref{thm:causal-bounds-supp} are sharp under this additional assumption, but this nevertheless remains true for many cases of interest in practice.
In particular, if $\fx$ depends only on the final observation space $\Xspace_{\tx}$ (which is true for example throughout our case study), i.e.\ we have
\[
    \Y(\ax_{1:\tx}') \eqas \fx(\X_\tx(\ax_{1:\tx}')),
\]
then it still holds that the bounds can be achieved, provided the worst-case values are chosen sensibly as
\[
    \ylo = \min_{\xx_t \in \B_\tx} \fx(\xx_\tx) \qquad \yup = \max_{\xx_\tx \in \B_\tx} \fx(\xx_\tx).
\]
This follows straightforwardly by modifying the proof of Proposition \ref{prop:sharpness-of-bounds} to define $\xx_\tx$ as either the minimiser or maximiser of $\fx$ on $\B_\tx$.

%
\subsection{Bounds on the conditional expectation given specific covariate values} \label{sec:impossibility-of-bounds-for-continuous-data}

%
%
%
%
%

Theorem \ref{thm:causal-bounds-supp} provides a bound on $\E[\Y(\ax_{1:\tx}) \mid \X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx}]$, i.e.\ the conditional expectation given the \emph{event} $\{\X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx}\}$, which is assumed to have positive probability.
We consider here the prospect of obtaining bounds on $\E[\Y(\ax_{1:\tx}) \mid \X_{0:\tx}(\ax_{1:\tx})]$, i.e.\ the conditional expectation given the \emph{value} of $\X_{0:\tx}(\ax_{1:\tx})$.
%
For falsification purposes, this would provide a means for determining that twin is incorrect when it outputs specific values of $\Xt_{0:\tx}(\ax_{1:\tx})$, rather than just that it is incorrect on average across all runs that output values $\Xt_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx}$.

\subsubsection{Discrete covariates}

%
%
%
%
When $\X_{0:\tx}(\ax_{1:\tx})$ is discrete, Theorem \ref{thm:causal-bounds-supp} yields measurable functions $\lo{\gx}, \up{\gx} : \Xspace_{0:\tx} \to \R$ such that
\begin{equation} \label{eq:psi-lo-up-defining-property}
    \lo{\gx}(\X_{0:\tx}(\ax_{1:\tx}))
        \leq \E[\Y(\ax_{1:\tx}) \mid \X_{0:\tx}(\ax_{1:\tx})] \leq \up{\gx}(\X_{0:\tx}(\ax_{1:\tx})) \qquad \text{almost surely}.
\end{equation}
In particular, $\lo{\gx}(\xx_{0:\tx})$ is obtained as the value of $\E[\Ylo \mid \X_{0:\N}(\A_{1:\N}) \in \B_{0:\N}]$ for $\B_{0:\tx} \coloneqq \{\xx_{0:\tx}\}$, and similarly for $\up{\gx}(\xx_{0:\tx})$.
Moreover, since the constants $\ylo, \yup \in \R$ in Theorem \ref{thm:causal-bounds-supp} were allowed to depend on $\B_{0:\tx}$, and hence here on each choice of $\xx_{0:\tx} \in \Xspace_{0:\tx}$, we may think of these now as measurable functions $\ylo, \yup : \Xspace_{0:\tx} \to \R$ satisfying
\begin{equation} \label{eq:y-boundedness-functional-assumption}
    \ylo(\X_{0:\tx}(\ax_{1:\tx})) \leq \Y(\ax_{1:\tx}) \leq \yup(\X_{0:\tx}(\ax_{1:\tx})) \qquad \text{almost surely}.
\end{equation}
In other words, when $\X_{0:\tx}(\ax_{1:\tx})$ is discrete, Theorem \ref{thm:causal-bounds-supp} provides bounds on the conditional expectation of $\Y(\ax_{1:\tx})$ given the value of $\X_{0:\tx}(\ax_{1:\tx})$ whenever we have $\ylo$ and $\yup$ such that \eqref{eq:y-boundedness-functional-assumption} holds.

%

%


%
%
%


\subsubsection{Continuous initial covariates} \label{sec:continuous-initial-covariates-supplement}

When $\Prob(\X_{1:\tx}(\ax_{1:\tx}) \in \B_{1:\tx}) > 0$, a fairly straightforward modification of the proof of Theorem \ref{thm:causal-bounds-supp} yields bounds of the following form:
\begin{align}
    \E[\Ylo \mid \X_0, \X_{1:\N}(\A_{1:\N}) \in \B_{1:\N}] 
        &\leq \E[\Y(\ax_{1:\tx}) \mid \X_0, \X_{1:\tx}(\ax_{1:\tx}) \in \B_{1:\tx}] \notag \\
        &\qquad\qquad\leq \E[\Yup \mid \X_0, \X_{1:\N}(\A_{1:\N}) \in \B_{1:\N}] \quad \text{almost surely}. \label{eq:bareinboim-style-bounds-with-continuous-initial-covariates}
\end{align}
In particular, this holds regardless of whether or not $\X_0$ is discrete.
In turn, if $\X_{1:\tx}(\ax_{1:\tx})$ is discrete, then by a similar argument as was given in the previous subsection, this yields almost sure bounds on $\E[\Y(\ax_{1:\tx}) \mid \X_{0:\tx}(\ax_{1:\tx})]$ of the form in \eqref{eq:psi-lo-up-defining-property}, provided \eqref{eq:y-boundedness-functional-assumption} holds.
Alternatively, by taking $\B_{1:\tx} \coloneqq \Xspace_{1:\tx}$, \eqref{eq:bareinboim-style-bounds-with-continuous-initial-covariates} yields bounds of the form
\[
    \E[\Ylo \mid \X_0] \leq \E[\Y(\ax_{1:\tx}) \mid \X_0] \leq \E[\Yup \mid \X_0].
\]
If the action sequence $\ax_{1:\tx}$ is thought of as a single choice of an action from the extended action space $\Aspace_{1:\tx}$, then this recovers the bounds originally proposed by Manski \cite{manski}, which allowed conditioning on potentially continuous pre-treatment covariates corresponding to our $\X_0$.


%


\subsubsection{Proof of Theorem \ref{thm:no-causal-bounds-for-continuous-data} (no bounds for continuous subsequent covariates)} \label{sec:continuous-subsequent-covariates}

We now give a proof of Theorem \ref{thm:no-causal-bounds-for-continuous-data} from the main text, which shows that, unlike in the examples just given, bounds on $\E[\Y(\ax_{1:\tx}) \mid \X_{0:\tx}(\ax_{1:\tx})]$ analogous to Theorem \ref{thm:causal-bounds-supp} cannot be obtained without further assumptions.
We refer the reader to the main text for a full explanation, including a definition of ``permissible''.

%
%

%
%
%
%

\begin{manualtheorem}{\ref{thm:no-causal-bounds-for-continuous-data}}
    Suppose $\X_0$ is almost surely constant, $\Prob(\A_1 \neq \ax_1) > 0$, and for some $\sx \in \{1, \ldots, \tx\}$ we have
    \begin{equation} \label{eq:potential-outcomes-continuity-assumption-9}
        \text{$\Prob(\X_{\sx}(\A_{1:\sx}) = \xx_{\sx}) = 0$ for all $\xx_{\sx} \in \Xspace_{\sx}$}.
    \end{equation}
    Then $\lo{\gx}, \up{\gx} : \Xspace_{0:\tx} \to \R$ are permissible bounds only if they are trivial, i.e.\ we have
    \[
        \lo{\gx}(\X_{0:\tx}(\ax_{1:\tx})) \leq \ylo(\X_{0:\tx}(\ax_{1:\tx})) \quad \text{and} \quad \up{\gx}(\X_{0:\tx}(\ax_{1:\tx})) \geq \yup(\X_{0:\tx}(\ax_{1:\tx})) \qquad \text{almost surely.}
    \]

    %
\end{manualtheorem}


\begin{proof}
    Suppose we have a permissible $\lo{\gx}$.
    (The case of $\up{\gx}$ is analogous).
    Choose $\xx_{1:\T} \in \Xspace_{1:\T}$ arbitrarily, and define new potential outcomes
    \begin{align*}
        \tilde{\X}_0 &\coloneqq \X_0 \\
        \tilde{\X}_r(\ax_{1:r}') &\coloneqq %
            %
            \ind(\A_{1:r} = \ax_{1:r}') \, \X_{r}(\ax_{1:r}') + \ind(\A_{1:r} \neq \ax_{1:r}') \, \xx_{r} \qquad \text{for $r \in \{1, \ldots, \T\}$ and $\ax_{1:r}' \in \Aspace_{1:r}$}.
        %
    \end{align*}
    Similarly, define
    \begin{align*}
        \tilde{\Y}(\ax_{1:\tx}') &\coloneqq \ind(\A_{1:\tx} = \ax_{1:\tx}') \, \Y(\ax_{1:\tx}') + \ind(\A_{1:\tx} \neq \ax_{1:\tx}') \, \ylo(\tilde{\X}_{0:\tx}(\ax_{1:\tx}')) \qquad \text{for all $\ax_{1:\tx}' \in \Aspace_{1:\tx}$}.
    \end{align*}
    It immediately follows that
    \[
        (\tilde{\X}_{0:\T}(\A_{1:\T}), \tilde{\Y}(\A_{1:\tx}), \A_{1:\T}) \eqas (\X_{0:\T}(\A_{1:\T}), \Y(\A_{1:\tx}), \A_{1:\T}).
    \]
    Moreover, it is easily checked that
    \[
        \ylo(\tilde{\X}_{0:\tx}(\ax_{1:\tx})) \leq \tilde{\Y}(\ax_{1:\tx}) \leq \yup(\tilde{\X}_{0:\tx}(\ax_{1:\tx})) \qquad \text{almost surely}.
    \]
    As such, since $\lo{\gx}$ is permissible, we must have, almost surely,
    \begin{align}
        \lo{\gx}(\tilde{\X}_{0:\tx}(\ax_{1:\tx})) &\leq \E[\tilde{\Y}(\ax_{1:\tx}) \mid \tilde{\X}_{0:\tx}(\ax_{1:\tx})] \notag \\
         &= \begin{multlined}[t]
            \E[\tilde{\Y}(\A_{1:\tx}) \mid \tilde{\X}_{0:\tx}(\ax_{1:\tx}), \A_{1:\tx} = \ax_{1:\tx}] \, \Prob(\A_{1:\tx} = \ax_{1:\tx} \mid \tilde{\X}_{0:\tx}(\ax_{1:\tx})) \\
                + \underbrace{\E[\tilde{\Y}(\ax_{1:\tx}) \mid \tilde{\X}_{0:\tx}(\ax_{1:\tx}), \A_{1:\tx} \neq \ax_{1:\tx}]}_{=\ylo(\tilde{\X}_{0:\tx}(\ax_{1:\tx}))} \, \Prob(\A_{1:\tx} \neq \ax_{1:\tx} \mid \tilde{\X}_{0:\tx}(\ax_{1:\tx})).
        \end{multlined} \label{eq:no-continuous-bounds-proof-convex-combination}
    \end{align}
    Now, by our definition of $\tilde{\X}_{0:\tx}(\ax_{1:\tx})$, we have almost surely
    \begin{align*}
        \ind(\A_{1} \neq \ax_{1}) \, \Prob(\A_{1:\tx} = \ax_{1:\tx} \mid \tilde{\X}_{0:\tx}(\ax_{1:\tx}))
            &= \ind(\A_{1} \neq \ax_{1}, \tilde{\X}_{\sx}(\ax_{1:\sx}) = \xx_\sx) \, \Prob(\A_{1:\tx} = \ax_{1:\tx} \mid \tilde{\X}_{0:\tx}(\ax_{1:\tx})) \\
            &= \ind(\A_{1} \neq \ax_{1}) \, \E[\ind(\A_{1:\tx} = \ax_{1:\tx}, \tilde{\X}_{\sx}(\ax_{1:\sx}) = \xx_\sx) \mid \tilde{\X}_{0:\tx}(\ax_{1:\tx})] \\
            &= \ind(\A_{1} \neq \ax_{1}) \, \E[\ind(\A_{1:\tx} = \ax_{1:\tx}, \X_{\sx}(\A_{1:\sx}) = \xx_\sx) \mid \tilde{\X}_{0:\tx}(\ax_{1:\tx})] \\
            &= 0,
    \end{align*}
    where the last step follows by our assumption in \eqref{eq:potential-outcomes-continuity-assumption-9}.
    Combining this with \eqref{eq:no-continuous-bounds-proof-convex-combination}, we get, almost surely,
    \begin{align}
        \ind(\A_{1} \neq \ax_{1}) \, \lo{\gx}(\X_{0}, \xx_{1:\tx}) &=  \ind(\A_{1} \neq \ax_{1}) \, \lo{\gx}(\tilde{\X}_{0:\tx}(\ax_{1:\tx})) \notag \\
            &\leq \ind(\A_{1} \neq \ax_{1}) \, \ylo(\tilde{\X}_{0:\tx}(\ax_{1:\tx})) \notag \\
            &= \ind(\A_{1} \neq \ax_{1}) \, \ylo(\X_{0}, \xx_{1:\tx}). \label{eq:no-continuous-bounds-intermediate-step}
    \end{align}
    Now let $\xx_0 \in \Xspace_0$ be the value such that $\Prob(\X_0 = \xx_0) = 1$.
    Using our assumption that $\Prob(\A_1 \neq \ax_1) > 0$ and the fact that $\xx_{1:\tx}$ was arbitrary, we obtain
    \[
        \lo{\gx}(\xx_{0:\tx}) \leq \ylo(\xx_{0:\tx}) \qquad \text{for all $\xx_{1:\tx} \in \Xspace_{1:\tx}$}.
    \]
    The result now follows.
\end{proof}


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%

%
%
%
%


\paragraph{Illustrative example}

To gain intuition for the phenomenon underlying Theorem \ref{thm:no-causal-bounds-for-continuous-data}, consider a simplified model consisting of $\Xspace$-valued potential outcomes $(\X(\ax') : \ax \in \Aspace)$, $\R$-valued potential outcomes $(\Y(\ax') : \ax \in \Aspace)$, and an $\Aspace$-valued random variable $\A$ representing the choice of action.
(This constitutes a special case of our setup with $\T = 1$ and $\Xspace_0$ taken to be a singleton set.)
Suppose moreover that the following conditions hold:
\begin{align*}
    %
    \Prob(\X(\A) = \xx) &= 0 \qquad \text{for all $\xx \in \Xspace$} \\
    \Prob(\A = \ax) &< 1.
\end{align*}
We then have
\begin{multline}
\label{eq:no-continuous-bounds-toy-example}
    \E[\Y(\ax) \mid \X(\ax)]
        \eqas \E[\Y(\A) \mid \X(\A), \A = \ax] \, \Prob(\A = \ax \mid \X(\ax))\\ 
        + \E[\Y(\ax) \mid \X(\ax), \A \neq \ax] \, \Prob(\A \neq \ax \mid \X(\ax)).
\end{multline} 
But now, since the behaviour of $\X(\ax)$ is only observed on $\{\A = \ax\}$, for any given value of $\xx \in \Xspace$, we cannot rule out the possibility that
\[
    \X(\ax) = \ind(\A = \ax) \, \X(\A) + \ind(\A \neq \ax) \, \xx \qquad \text{almost surely}.
\]
In turn, since $\Prob(\A = \ax) > 0$, this would imply $\Prob(\X(\ax) = \xx) > 0$, and, since $\Prob(\X(\A) = \xx) = 0$, that $\Prob(\A = \ax \mid \X(\ax) = \xx) = 0$.
From \eqref{eq:no-continuous-bounds-toy-example}, this would yield
\[
    \E[\Y(\ax) \mid \X(\ax) = \xx] = \E[\Y(\ax) \mid \X(\ax) = \xx, \A \neq \ax].
\]
But now, since the behaviour of $\Y(\ax)$ is unobserved on $\{\A \neq \ax\}$, intuitively speaking, the observational distribution does not provide any information about the value of the right-hand side, and therefore about the behaviour of $\E[\Y(\ax) \mid \X(\ax)]$ more generally since $\xx \in \Xspace$ was arbitrary.

\paragraph{Our case}

The discussion in this subsection considered $(\Y(\ax_{1:\tx}') : \ax_{1:\tx}' \in \Aspace_{1:\tx})$ to be arbitrary potential outcomes defined jointly on the same probability space as $(\X_{0:\tx}(\ax_{1:\tx}') : \ax_{1:\tx}' \in \Aspace_{1:\tx})$.
In contrast, our falsification methodology (Section \ref{sec:hypotheses-from-causal-bounds} of the main text) assumes the particular form
\[
    \Y(\ax_{1:\tx}) \eqas \fx(\X_{0:\tx}(\ax_{1:\tx})),
\]
which means $\E[\Y(\ax_{1:\tx}) \mid \X_{0:\tx}(\ax_{1:\tx})] \eqas \fx(\X_{0:\tx}(\ax_{1:\tx}))$ is known trivially.
In this context, an alternative quantity to consider is $\E[\Y(\ax_{1:\tx}) \mid \X_{0:r}(\ax_{1:r})]$ with $r \in \{0, \ldots, \tx-1\}$, which in general will be unknown and therefore still interesting to bound.
In the discrete case, Theorem \ref{thm:causal-bounds-supp} yields a bound on this quantity obtained by taking $\B_{0:r} = \{\xx_{0:r}\}$ with $\Prob(\X_{0:r}(\ax_{1:r}) = \xx_{0:r}) > 0$ and then $\B_{r+1:\tx} \coloneqq \Xspace_{r+1:\tx}$, and with $\ylo$ and $\yup$ in \eqref{eq:y-boundedness-functional-assumption} now replaced by
\[
    \min_{\xx_\tx \in \Xspace_\tx} \fx(\xx_\tx)  \qquad \text{and} \qquad
    \max_{\xx_\tx \in \Xspace_\tx} \fx(\xx_\tx)
\]
respectively.
However, in the continuous case, the same issues described above continue to apply in many cases of interest.
For example, when $\fx$ is a function of $\Xspace_\tx$ only (which holds for example throughout our case study), then under the assumptions of the previous result, the most informative almost sure lower bound on $\E[\Y(\ax_{1:\tx}) \mid \X_{0:r}(\ax_{1:r})]$ is
\[
    \min_{\xx_\tx \in \Xspace_\tx} \fx(\xx_\tx),
\]
which is already known trivially.
%
%
%
%
%
%
%
Roughly, this follows by modifying the proof of Theorem \ref{thm:no-causal-bounds-for-continuous-data} so that $\tilde{\X}_\tx(\ax_{1:\tx}')$ becomes
\[
    \tilde{\X}_\tx(\ax_{1:\tx}') \coloneqq \ind(\A_{1:\tx} = \ax_{1:\tx}') \, \X_{\tx}(\ax_{1:\tx}') + \ind(\A_{1:\tx} \neq \ax_{1:\tx}') \, \argmin_{\xx_\tx \in \Xspace_\tx} \fx(\xx_\tx),
\]
and $\tilde{\Y}(\ax_{1:\tx}')$ becomes
\[
    \tilde{\Y}(\ax_{1:\tx}') \coloneqq \fx(\tilde{\X}_\tx(\ax_{1:\tx})).
\]
The remainder of the argument is then unchanged.
An analogous result holds for the upper bound also.

%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


\section{Hypothesis testing methodology} 

\subsection{Validity of testing procedure} \label{sec:hyp-testing-supplement}

%
%
%
%
%
%
%
%
%

%
%
%
%
%

We show here that our procedure for testing $\Qt \geq \Qlo$ based on the one-sided confidence intervals $\qlo{\alpha}$ and $\qt{\alpha}$ has the correct probability of type I error, provided $\qlo{\alpha}$ and $\qt{\alpha}$ have the correct coverage probabilities.
In particular, the result below (which applies a standard union bound argument) shows that if $\Qt \geq \Qlo$, then our test rejects (i.e.\ $\qt{\alpha} < \qlo{\alpha}$) with probability at most $\alpha$.
An analogous result is easily proven for testing $\Qt \leq \Qup$ also, with $\qlo{\alpha}$ replaced by a one-sided upper $(1-\alpha/2)$-confidence interval for $\Qup$, and $\qt{\alpha}$ replaced by a one-sided lower $(1 - \alpha/2)$-confidence interval for $\Qt$.

\begin{proposition}
Suppose that for some $\alpha \in (0, 1)$ we have random variables $\qt{\alpha}$ and $\qlo{\alpha}$ satisfying
\begin{align}
    \Prob(\Qlo \geq \qlo{\alpha}) &\geq 1 - \frac{\alpha}{2} \label{eq:qlo-confidence-interval-guarantee} \\
    \Prob(\Qt \leq \qt{\alpha}) &\geq 1 - \frac{\alpha}{2} \label{eq:qt-confidence-interval-guarantee}.
\end{align}
If $\Qt \geq \Qlo$, then $\Prob(\qt{\alpha} < \qlo{\alpha}) \leq \alpha$.
\end{proposition}

\begin{proof}
If $\Qt \geq \Qlo$, then we have
\[
    \{\qt{\alpha} < \qlo{\alpha}\} \subseteq \{\Qt > \qt{\alpha}\} \cup \{\Qlo < \qlo{\alpha}\}.
\]
To see this, note that
\[
    (\{\Qt > \qt{\alpha}\} \cup \{\Qlo < \qlo{\alpha}\})^c
        = \{\Qt > \qt{\alpha}\}^c \cap \{\Qlo < \qlo{\alpha}\}^c
        = \{\Qt \leq \qt{\alpha}\} \cap \{\Qlo \geq \qlo{\alpha}\}
        \subseteq \{\qlo{\alpha} \leq \qt{\alpha}\}.
\]
As such,
\begin{align*}
    \Prob(\qt{\alpha} < \qlo{\alpha}) \leq \Prob(\{\Qt > \qt{\alpha}\} \cup \{\Qlo < \qlo{\alpha}\}) \leq \Prob(\Qt > \qt{\alpha}) + \Prob(\Qlo < \qlo{\alpha}) \leq \alpha/2 + \alpha/2 = \alpha.
\end{align*}
\end{proof}

%
%
%
%
%
%

\subsection{Methodology for obtaining confidence intervals} \label{sec:confidence-intervals-methodology-supplement}

In this section, we describe concretely how we use our data to obtain one-sided confidence intervals $\qlo{\alpha}$ and $\qt{\alpha}$ satisfying \eqref{eq:qlo-confidence-interval-guarantee} and \eqref{eq:qt-confidence-interval-guarantee} as required by our procedure for testing $\Qt \geq \Qlo$.
We use an analogous procedure to obtain confidence intervals for testing $\Qt \leq \Qup$.
We consider two techniques: an exact method based on Hoeffding's inequality, and an approximate method based on bootstrapping.
%
Conceptually, both are based on obtaining unbiased sample mean estimates of $\Qlo$ and $\Qt$.
We therefore describe this first, and then describe the particulars of each technique separately.

%

\subsubsection*{Unbiased sample mean estimate of $\Qlo$}

First, we describe how to obtain an unbiased sample mean estimate of $\Qlo$.
Recall that we assume access to a dataset $\D$ consisting of i.i.d.\ copies of observational trajectories of the form
\[
    \X_0, \A_1, \X_1(\A_1), \ldots, \A_\T, \X_\T(\A_{1:\T}).
\]
Let $\D(\ax_{1:\tx}, \B_{0:\tx})$ be the subset of trajectories in $\D$ for which $\X_{0:\N}(\A_{1:\N})\in\B_{0:\N}$.
Obtaining $\D(\ax_{1:\tx}, \B_{0:\tx})$ is possible since the only random quantity that $\N = \max\{0 \leq \sx \leq \tx \mid \A_{1:\sx} = \ax_{1:\sx}\}$ depends on is $\A_{1:\tx}$, which is included in the data.
We denote the cardinality of $\D(\ax_{1:\tx}, \B_{0:\tx})$ by $\nx \coloneqq \abs{\D(\ax_{1:\tx}, \B_{0:\tx})}$.
We then denote by $\Ylo^{(i)}$ for $i \in \{1, \ldots, \nx\}$ the corresponding values of
\begin{align*}
    \Ylo &= \ind(\A_{1:\tx} = \ax_{1:\tx}) \, \fx(\X_{0:\tx}(\A_{1:\tx})) + \ind(\A_{1:\tx} \neq \ax_{1:\tx}) \, \ylo
\end{align*}
obtained from each trajectory in $\D(\ax_{1:\tx}, \B_{0:\tx})$.
This is again possible since both terms only depends on the observational quantities $(\X_{0:\tx}(\A_{1:\tx}), \A_{1:\tx})$.
It is easily seen that the values of $\Ylo^{(i)}$ are i.i.d.\ and satisfy $\E[\Ylo^{(i)}] = \Qlo$.
    %
    %
As a result, the sample mean
\begin{equation} \label{eq:YClmean-definition-supplement}
    \YClmean \coloneqq \frac{1}{\nx} \sum_{i=1}^{\nx} \Ylo^{(i)} 
    %
    %
    %
    %
\end{equation}
is an unbiased estimator of $\Qlo$.

\subsubsection*{Unbiased sample mean estimate of $\Qt$}

We obtain an unbiased sample mean estimate of $\Qt$ in a similar fashion as for $\Qlo$.
Recall that we assume access to a dataset $\Dt(\ax_{1:\tx})$ consisting of i.i.d.\ copies of
\[
    \X_0, \Xt_1(\X_0, \ax_1), \ldots, \Xt_t(\X_0, \ax_{1:\tx}).
\]
Let $\Dt(\ax_{1:\tx}, \B_{0:\tx})$ denote the subset of twin trajectories in $\Dt(\ax_{1:\tx})$ for which $(\X_0, \Xt_{\tx}(\X_0, \ax_{1:\tx})) \in \B_{0:\tx}$, and denote its cardinality by $\widehat{\nx} \coloneqq \abs{\Dt(\ax_{1:\tx}, \B_{0:\tx})}$.
%
Then denote by $\Yt^{(i)}$ for $i \in \{1 \ldots, \widehat{\nx}\}$ the corresponding values of
\[
    \Yt = \fx(\X_0, \Xt_{1:\tx}(\X_0, \ax_{1:\tx}))
\]
obtained from each trajectory in $\Dt(\ax_{1:\tx}, \B_{0:\tx})$.
It is easily seen that the values $\Yt^{(i)}$ are i.i.d.\ (since the entries of $\Dt(\ax_{1:\tx})$ are) and satisfy $\E[\Yt^{(i)}] = \Qt$.
As a result, the sample mean
\[
    \Ytmean \coloneqq \frac{1}{\widehat{\nx}} \sum_{i=1}^{\widehat{\nx}} \Yt^{(i)}
\]
is an unbiased estimator of $\Qt$.


\subsubsection*{Exact confidence intervals via Hoeffding's inequality}

Recall that we assume in our methodology that $\Y(\ax_{1:\tx})$ has the form
\[
    \Y(\ax_{1:\tx}) = \fx(\X_{0:\tx}(\ax_{1:\tx})),
\]
and that moreover
\begin{equation} \label{eq:f-boundedness-assumption-hoeffding-proof}
    \ylo \leq \fx(\xx_{0:\tx}) \leq \yup \qquad \text{for all $\xx_{0:\tx} \in \B_{0:\tx}$.}
\end{equation}
This means that $\Yt^{(i)}$ is almost surely bounded in $[\ylo, \yup]$, and so $\Ytmean$ gives rise to one-sided confidence intervals via an application of Hoeffding's inequality.
The exact form of these confidence intervals is as follows:

%
%
%

%


\begin{proposition} \label{prop:hoeffding-confidence-bounds-supp}
If \eqref{eq:f-boundedness-assumption-hoeffding-proof} holds, then for each $\alpha \in (0, 1)$, letting
\[
    \CIlen \coloneqq (\yup - \ylo) \, \sqrt{\frac{1}{2 \nx} \, \log \frac{2}{\alpha}}  \qquad \text{and} \qquad \widehat{\CIlen} \coloneqq (\yup - \ylo) \, \sqrt{\frac{1}{2 \widehat{\nx}} \, \log \frac{2}{\alpha}},
\]
and similarly
\begin{align*}
    \qlo{\alpha} \coloneqq \YClmean - \CIlen  \qquad \text{and} \qquad
    \qt{\alpha} \coloneqq \Ytmean + \widehat{\CIlen},
\end{align*}
it follows that
\begin{align*}
    \Prob(\Qlo \geq \qlo{\alpha}) \geq 1 - \frac{\alpha}{2} \qquad \text{and} \qquad \Prob(\Qt \leq \qt{\alpha}) \geq 1 - \frac{\alpha}{2}.
\end{align*}
\end{proposition}

\begin{proof}
We only prove the result for $\qlo{\alpha}$; the other statement can be proved analogously.
Recall that $\YClmean$ is the empirical mean of i.i.d.\ samples $\Ylo^{(i)}$ for $i\in \{1, \ldots, \nx\}$ with $\E[\Ylo^{(i)}]=\Qlo$ (see \eqref{eq:YClmean-definition-supplement}).
Moreover, by \eqref{eq:f-boundedness-assumption-hoeffding-proof}, $\Ylo^{(i)}$ is almost surely bounded in $[\ylo, \yup]$.
Hoeffding's inequality then implies that
\begin{align*}
    \Prob(\YClmean - \Qlo > \CIlen) &\leq \exp\left(- \frac{2 \nx \CIlen^2}{(\yup - \ylo)^2 } \right).
\end{align*}
In turn, some basic manipulations yield
\begin{align*}
    \Prob(\Qlo \geq \qlo{\alpha}) &= 1 - \Prob(\Qlo < \YClmean - \CIlen) \\
    &\geq 1 - \exp\left(- \frac{2 \nx \CIlen^2}{(\yup - \ylo)^2 } \right) \\
    &= 1- \frac{\alpha}{2}.
\end{align*}
\end{proof}

\subsubsection{Approximate confidence intervals via bootstrapping}
\label{subsec:bootstrapping}
While Hoeffding's inequality yields the probability guarantees in \eqref{eq:qlo-confidence-interval-guarantee} and \eqref{eq:qt-confidence-interval-guarantee} exactly, the confidence intervals obtained can be conservative.
Consequently, our testing procedure may have lower probability of falsifying certain twins that in fact do not satisfy the causal bounds.
To address this, we also consider an approximate approach based on bootstrapping that can produce tighter confidence intervals.
%
While other schemes are possible, bootstrapping provides a general-purpose approach that is straightforward to implement and works well in practice.

At a high level, our approach here is again to construct one-sided level $1 - \alpha/2$ confidence intervals via bootstrapping \cite{efron1979bootstrap} on $\Qlo$ and $\Qt$.
Many bootstrapping procedures for obtaining confidence intervals have been proposed in the literature \citep{tibshirani1993introduction,davison1997bootstrap,hesterberg2015what}. 
Our results reported below were obtained via the \emph{reverse percentile} bootstrap (see \citet{hesterberg2015what} for an overview).
(We also tried the \emph{percentile} bootstrap method, which obtained nearly indistinguishable results.)
In particular, this method takes
\[
    \qlo{\alpha} \coloneqq 2 \YClmean - \Delta 
    \qquad \qt{\alpha} \coloneqq 2 \widehat{\mu} - \widehat{\Delta},
\]
where $\Delta$ and $\widehat{\Delta}$ correspond to the approximate $1 - \alpha / 2$ and $\alpha / 2$ quantiles of the distributions of 
%
\[
    \frac{1}{\nx} \sum_{i=1}^{\nx} \Ylo^{(i^\ast)} 
    \qquad\text{and}\qquad
    \frac{1}{\widehat{\nx}} \sum_{i=1}^{\widehat{\nx}} \Yt^{(i^\ast)},
\]
where each $\Ylo^{(i^\ast)}$ and $\Y^{(i^\ast)}$ is obtained by sampling uniformly with replacement from among the values of $\Ylo^{(i)}$ and $\Y^{(i)}$.
In our case study, as is typically done in practice, we approximated $\Delta$ and $\widehat{\Delta}$ via Monte Carlo sampling.
It can be shown that the confidence intervals produced in this way obtain a coverage level that approaches the desired level of $1 - \alpha/2$ as $\nx$ and $\widehat{\nx}$ grow to infinity under mild assumptions \cite{hall1988theoretical}.

\subsection{Testing with two-sided confidence intervals}
\label{sec:two-sided-intervals-supplement}
Although we do not consider it in our case study, it is possible to replace the one-sided confidence intervals for $\Qlo$ and $\Qt$ that we use with two-sided intervals.
This would allow us to define a procedure that does the following:
\begin{enumerate}
    \item If the interval for $\Qlo$ lies completely below the interval for $\Qt$ (without overlap), then infer that $\Qt \geq \Qlo$;
    \item If the interval for $\Qlo$ lies completely above the interval for $\Qt$ (without overlap), then infer that $\Qt < \Qlo$;
    \item Otherwise, draw no inference.
\end{enumerate}
In particular, notice that this procedure is now able to infer that $\Qt \geq \Qlo$ is true (if the first case occurs), as well as to infer that $\Qt \geq \Qlo$ is false as previously.
By a closed testing argument \citep{marcus1976method}, this procedure can be shown to have at most probability $\alpha$ of drawing a false inference about the twin.
A similar approach can be used for the hypothesis $\Qt \leq \Qup$ by obtaining two-sided intervals for $\Qup$ and $\Qt$.

Since each of $\Qlo$, $\Qup$, and $\Qt$ are identifiable from the observational distribution, it is straightforward to obtain two-sided confidence intervals whose widths will shrink to zero as the size of the observational dataset grows large.
(For instance, this holds for confidence intervals obtained from Hoeffding's inequality and from bootstrapping.)
Consequently, with a sufficiently large dataset, with high probability, only one of the first two cases above will be observed to occur.
As such, when it does occur, the third case would indicate that insufficient data has been collected to draw an appropriate conclusion about the twin, which could be useful information for practitioners.
On the other hand, this procedure comes at an expense as it leads to a more conservative upper bound for $\Qlo$ and lower bound for $\Qt$ when testing the hypothesis $\Qt \geq \Qlo$ (and analogously for $\Qt \leq \Qup$), and so may result in fewer falsifications than the method we consider based on one-sided confidence intervals instead.


\section{Experimental Details} \label{sec:experiments-supplement}

In this section, we provide additional experimental details relating to our case study.

\subsection{MIMIC preprocessing} \label{sec:mimic-preprocessing-supp}

For data extraction and preprocessing, we re-used the same procedure as \cite{ai-clinician} with minor modifications.
For completeness, we describe the pre-processing steps applied in \cite{ai-clinician} and subsequently outline our modifications to these.

\subsubsection*{Patient cohorts}

Following \cite{ai-clinician}, we extracted adult patients fulfilling the sepsis-3 criteria \cite{sepsis-criteria}. 
Sepsis was defined as a suspected infection (as indicated by prescription of antibiotics and sampling of bodily fluids for microbiological culture) combined with evidence of organ dysfunction, defined by a SOFA score $\geq 2$ \cite{sepsis-criteria} \citep{seymour2016assessment}.  

\subsubsection*{Exclusion criteria}

Following \cite{ai-clinician}, we excluded patients for whom any of the following was true:
\begin{itemize}
    \item Age $<18$ years old at the time of ICU admission
    \item Mortality not documented
    \item IV fluid/vasopressors intake not documented
    \item Withdrawal of treatment
\end{itemize}

\subsubsection*{Our modifications}

We made the following modifications to the pre-processing code of \cite{ai-clinician} for our experiment:
\begin{itemize}
    \item Instead of extracting physiological quantities (e.g.\ heart rate) every 4 hours, we extracted these every hour.
    \item We excluded patients with any missing hourly vitals during the first 4 hours of their ICU stay.
\end{itemize}
We then extracted a total of 19 quantities of interest listed in Table \ref{tab:mimic-features}.
Of these, 17 were physiological quantities associated with the patient, including static demographic quantities (e.g.\ age), patient vital signs (e.g.\ heart rate), and patient lab values (e.g.\ potassium blood concentration).
These were chosen as the subset of physiological quantities extracted from MIMIC by \cite{ai-clinician} that are also modelled by Pulse, and were used to define our observation spaces $\Xspace_\tx$ as described next.
The remaining 2 quantities (intravenous fluids and vasopressor doses) were chosen since they correspond to treatments that the patient received, and were used to define our action spaces $\Aspace_\tx$ as described below.

\paragraph{Sample splitting}

Before proceeding further, we randomly selected 5\% of the extracted our trajectories (583 trajectories, denoted as $\D_0$) to use for preliminary tasks such as choosing the parameters of our hypotheses.
We reserved the remaining 95\% (11,094 trajectories, denoted as $\D$) for the actual testing.
By a standard sample splitting argument \cite{cox1975note}, the statistical guarantees of our testing procedure established above continue to apply even when our hypotheses are defined in this data-dependent way.

%

\begin{table}[t]%
\centering
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
\begin{footnotesize}
\begin{tabular}{lll}
\toprule
Category &  Physiological quantity & Type \\
\midrule
1. Demographic & Age & Continuous \\
& Sex & Binary  \\
& Weight & Continuous \\
\midrule
2. Vital Signs & Heart rate (HR) & Continuous  \\
& Systolic blood pressure (SysBP) & Continuous  \\
& Diastolic blood pressure (DiaBP) & Continuous  \\
& Mean blood pressure (MeanBP) & Continuous \\
& Respiratory Rate (RR) & Continuous \\
& Skin Temperature (Temp) & Continuous \\
\midrule
3. Lab Values & Potassium Blood Concentration (Potassium) & Continuous  \\
& Sodium Blood Concentration (Sodium) & Continuous  \\
& Chloride Blood Concentration (Chloride) & Continuous  \\
& Glucose Blood Concentration (Glucose) & Continuous  \\
& Calcium Blood Concentration (Calcium) & Continuous  \\
& Bicarbonate Blood Concentration ($\textup{HCO}_3$) & Continuous  \\
& Arterial $\textup{O}_2$ Pressure ($\textup{PaO}_2$) & Continuous  \\
& Arterial $\textup{CO}_2$ Pressure ($\textup{PaCO}_2$) & Continuous \\
\midrule
4. Treatments & Intravenous fluid (IV) dose & Continuous \\
& Vasopressor dose & Continuous \\
\bottomrule
\end{tabular}
\end{footnotesize}
\caption{Physiological quantities extracted from MIMIC}\label{tab:mimic-features}
\end{table}

\subsection{Observation spaces} \label{sec:observation-space-definition-supp}

Our $\Xspace_0$ consisted of the following features: age, sex, weight, heart rate, systolic blood pressure, diastolic blood pressure and respiration rate.
We chose $\Xspace_0$ in this way because, out of the 17 physiological quantities we extracted from MIMIC, these were the quantities that can be initialised to user-provided values before starting a simulation in the version of Pulse we considered (4.x).
(In contrast, Pulse initialises the other 10 features to default values.)
%
For the remaining observation spaces, we used the full collection of 17 physiological quantities we extracted (i.e.\ those listed in Categories 1-3 of Table \ref{tab:mimic-features}) to define $\Xspace_1 = \cdots = \Xspace_4$.
We encoded all features in $\Xspace_\tx$ numerically, i.e.\ $\Xspace_0 = \R^7$, and $\Xspace_\tx = \R^{17}$ for $\tx \in \{1, 2, 3, 4\}$.


%
%

\subsection{Action spaces} \label{sec:action-space-definition-supp}

Following \cite{ai-clinician}, we constructed our action space using 2 features obtained from MIMIC, namely intravenous fluid (IV) and vasopressor doses.
To obtain discrete action spaces suitable for our framework, we used the same discretization procedure for these quantities as was used by \cite{ai-clinician}.
Specifically, we divided the hourly doses of intravenous fluids and vasopressors into 5 bins each, with the first bin corresponding to zero drug dosage, and the remaining 4 bins based on the quartiles of the non-zero drug dosages in our held-out observational dataset $\D_0$.
%
From this we obtained action spaces $\Aspace_1 = \cdots = \Aspace_{4}$ with $5 \times 5 = 25$ elements. 
Table \ref{tab:act_space} shows the dosage bins constructed in this way, as well as the frequency of each bin's occurrence in the observational data.

%

%


\begin{table}[t]%
    \centering
    \begin{footnotesize}
\begin{tabular}{l|l|lllll}
\cline{3-7}
\multicolumn{1}{c}{} & \multicolumn{1}{c|}{} & \multicolumn{5}{c}{Vasopressor dose ($\mu$g/kg/min)}\\
\cline{3-7}
\multicolumn{1}{c}{} & \multicolumn{1}{c|}{} &      0 &  0.0 - 0.061 &  0.061 - 0.15 &  0.15 - 0.313 &  $>$ 0.313 \\
\midrule
\multirow{5}{*}{IV dose (mL/h)} & 0        &  16659 &          329 &           256 &           152 &      145 \\
& 0 - 20   &   5840 &          428 &           351 &           244 &      145 \\
& 20 - 75  &   6330 &          297 &           378 &           383 &      309 \\
& 75 - 214 &   6232 &          176 &           175 &           197 &      273 \\
& $>$ 214    &   5283 &          347 &           488 &           544 &      747 \\
\bottomrule
\end{tabular}
    \end{footnotesize}
\caption{Action space with frequency of occurrence in observational data} \label{tab:act_space}
\end{table}

\subsection{Hypothesis parameters} \label{sec:hypothesis-parameters-supplement}

We used our held-out observational dataset $\D_0$ to obtain a collection of hypothesis parameters $(\tx, \fx, \ax_{1:\tx}, \B_{0:\tx})$.
%
Specifically, for each physiological quantity of interest (e.g.\ heart rate) in the list of `Vital Signs' and `Lab Values' given in Table \ref{tab:mimic-features}, we did the following:
\begin{itemize}
    \item For each $\tx \in \{0, \ldots, 4\}$, we obtained 16 choices of $\B_{\tx}$ by discretizing the patient space $\Xspace_{\tx}$ into 16 subsets based on the values of certain features as follows:
    \begin{itemize}
        \item 2 bins corresponding to sex
        \item 4 bins corresponding to the quartiles of the ages of patients in $\D_0$
        \item 2 bins corresponding to whether or not the value of the chosen physiological quantity of interest at time $\tx$ was above or below its median value in $\D_0$.
    \end{itemize}
    \item For each $\tx \in \{1, \ldots, 4\}$, $\ax_{1:\tx} \in \Aspace_{1:\tx}$, and sequence $\B_{0:\tx}$ with each $\B_{\tx'}$ as defined in the previous step, let $\D_0(\tx, \ax_{1:\tx}, \B_{0:\tx})$ denote the subset of $\D_0$ corresponding to $(\tx, \ax_{1:\tx}, \B_{0:\tx})$, i.e.\
    \[
        \D_0(\tx, \ax_{1:\tx}, \B_{0:\tx}) \coloneqq \{\X_{0:\tx}(\A_{1:\tx}) \mid \text{$(\X_{0:\T}(\A_{1:\T}), \A_{1:\T}) \in \D_0$ with $\A_{1:\tx} = \ax_{1:\tx}$ and $\X_{0:\tx}(\A_{1:\T}) \in \B_{0:\tx}$}\}.
    \]
    We then selected the set of all triples $(\tx, \ax_{1:\tx}, \B_{0:\tx})$ such that $\D_0(\tx, \ax_{1:\tx}, \B_{0:\tx})$ contained at least one trajectory.
    This meant the number of combinations of hypotheses parameters that we considered was limited to a tractable quantity, which had benefits both computationally, and also by ensuring that we did not sacrifice too much power when adjusting for multiple testing.
    \item For each selected triple $(\tx, \ax_{1:\tx}, \B_{0:\tx})$, we chose a corresponding $\fx$ as follows:
    \begin{itemize}
        \item Let $i \in \{1, \ldots, \Xspacedim_\tx\}$ denote the index of the physiological quantity of interest in $\Xspace_\tx = \R^{\Xspacedim_\tx}$. We set $\ylo, \yup$ to be the .2 and the .8 quantiles of the values in
        \[
            \{(\X_\tx(\A_{1:\tx}))_i \mid \X_{0:\tx}(\A_{1:\tx}) \in \D_0(\tx, \ax_{1:\tx}, \B_{0:\tx})\}
        \]
        \item We defined $\fx : \Xspace_{0:\tx} \to \R$ as the function that extracts the physiological quantity of interest from $\Xspace_\tx$ and clips its value to between $\ylo$ and $\yup$, i.e.\
        \begin{align}
            \fx(\xx_{0:\tx}) \coloneqq \clip((\xx_{\tx})_{i}, \ylo, \yup). \label{eq:our-fx-in-experiments}
        \end{align}
        where $\clip(z, a, b) \coloneqq \min(\max(z, a), b)$.
    \end{itemize}
\end{itemize}
Overall, accounting for all physiological quantities of interest, we obtained 721 distinct choices of $(\tx, \fx, \ax_{1:\tx}, \B_{0:\tx})$ in this way.
Figure \ref{fig:n_histograms} shows the amount of non-held out observational and twin data that we subsequently used for testing each hypothesis, i.e.\ the values of $n$ and $\widehat{n}$ as defined in Section \ref{sec:confidence-intervals-methodology-supplement} above.
(We describe how we generated our dataset of twin trajectories in Section \ref{sec:pulse-trajectories-supplement}.)

%
%
%
%
%
%
%
%
%
%
%
%
%
%


\subsubsection{Implications of a falsification for clipped and unclipped outcomes} 

For a given index $i \in \{1, \ldots, \Xspacedim_\tx\}$ of some physiological quantity in $\Xspace_\tx = \R^{\Xspacedim_\tx}$, denote the corresponding \emph{unclipped} potential outcomes by
\begin{align*}
    \Z(\ax_{1:\tx}) &\coloneqq (\X_\tx(\ax_{1:\tx}))_i \\
    \Zt(\ax_{1:\tx}) &\coloneqq (\Xt_\tx(\X_0, \ax_{1:\tx}))_i,
\end{align*}
so that our choice of $\fx$ in \eqref{eq:our-fx-in-experiments} gives
\begin{align*}
    \Q &= \E[\clip(\Z(\ax_{1:\tx}), \ylo, \yup) \mid \X_{0:\tx}(\ax_{1:\tx}) \in \B_{0:\tx}] \\
    \Qt &= \E[\clip(\Zt(\ax_{1:\tx}), \ylo, \yup) \mid \X_0 \in \B_0, \Xt_{1:\tx}(\X_0, \ax_{1:\tx}) \in \B_{1:\tx}].
\end{align*}
Falsifying $\Hup$ immediately yields the following inference about the twin:
\begin{equation} \label{eq:hup-inference-in-terms-of-clipped-outcomes}
    \text{$\clip(\Zt(\ax_{1:\tx}), \ylo, \yup)$ is on average too large, conditional on $\{\X_0 \in \B_0, \Xt_{1:\tx}(\X_0, \ax_{1:\tx}) \in \B_{1:\tx}\}$}.
\end{equation}
When $\Hlo$ is false, we may similarly infer that $\clip(\Zt(\ax_{1:\tx}), \ylo, \yup)$ is on average too small, conditional on the same event.
%
%
%
%
However, these statements are expressed in terms of the clipped outcomes $\clip(\Zt(\ax_{1:\tx}), \ylo, \yup)$.
In practice, it may be of interest to draw some inference about the twin that is expressed more directly in terms of the unclipped outcomes $\Z(\ax_{1:\tx})$ and $\Zt(\ax_{1:\tx})$.
This is indeed possible as we explain now.

%

For notational simplicity, we will consider the case where $\B_{0:\tx} = \Xspace_{0:\tx}$ is trivial, which means $\Q$ and $\Qt$ simplify as
\begin{align*}
    \Q &= \E[\clip(\Z(\ax_{1:\tx}), \ylo, \yup)] \\
    \Qt &= \E[\clip(\Zt(\ax_{1:\tx}), \ylo, \yup)].
\end{align*}
However, our considerations here can easily be adapted to the more general case.
We then have the following result for $\Hup$ (an analogous result holds for $\Hlo$):

\begin{proposition}\label{prop:Hup-rejection-implications}
If $\Hup$ is false (i.e.\ $\Qt > \Qup$), then one of the following must hold:
\begin{align}
    \Prob(\Zt(\ax_{1:\tx}) \geq \yup) &> \Prob(\Z(\ax_{1:\tx}) \geq \yup) \label{eq:Hup-rejection-case-1} \\
    %
    \Prob(\Zt(\ax_{1:\tx}) > \ylo) &> \Prob(\Z(\ax_{1:\tx}) > \ylo) \label{eq:Hup-rejection-case-2} \\
    \E[\Zt(\ax_{1:\tx}) \mid \ylo < \Zt(\ax_{1:\tx}) < \yup] &> \E[\Z(\ax_{1:\tx}) \mid \ylo < \Z(\ax_{1:\tx}) < \yup]. \label{eq:Hup-rejection-case-3} 
\end{align}
\end{proposition}

\begin{proof}
    We will use the following straightforward fact: given real numbers $a \leq b \leq c$ and discrete probability vectors $(p, q, r)$ and $(p', q', r')$ (i.e.\ $p, q, r \in [0, 1]$ with $p + q + r = 1$, and similarly for $(p', q', r')$), if it holds that $r \leq r'$ and $p \geq p'$, then
    \begin{align}
        p \, a + q \, b + r \, c &\leq p' \, a + (q + p - p') \, b + r \, c \notag \\
        &\leq p' \, a + (q + p - p' - (r' - r)) \, b + r' \, c \notag \\
        &= p' \, a + q' \, b + r' \, c. \label{eq:unclipped-outcomes-inference-intermediate-step}
    \end{align}
    Intuitively, we first move $p' - p$ units of mass from $a$ to $b$, and then $r' - r$ units from $b$ to $c$, and both steps can only increase the expected value since $a \leq b \leq c$.

    Now assume that \eqref{eq:Hup-rejection-case-1}, \eqref{eq:Hup-rejection-case-2}, and \eqref{eq:Hup-rejection-case-3} do \emph{not} hold.
    We will show that in this case $\Hup$ must be \emph{true}, i.e.\ $\Qt \leq \Qup$.
    Indeed, we have
    \begin{align*}
        \Qt &= \Prob(\Zt(\ax_{1:\tx}) \leq \ylo) \, \ylo + \Prob(\ylo < \Zt(\ax_{1:\tx}) < \yup) \, \E[\Zt(\ax_{1:\tx}) \mid \ylo < \Zt(\ax_{1:\tx}) < \yup]\\
        &\quad+ \Prob(\Zt(\ax_{1:\tx}) \geq \yup) \, \yup \\
        &\leq \Prob(\Zt(\ax_{1:\tx}) \leq \ylo) \, \ylo + \Prob(\ylo < \Zt(\ax_{1:\tx}) < \yup) \, \E[\Z(\ax_{1:\tx}) \mid \ylo < \Z(\ax_{1:\tx}) < \yup]  \\
        &\quad + \Prob(\Zt(\ax_{1:\tx}) \geq \yup) \, \yup \\
        &\leq \Prob(\Z(\ax_{1:\tx}) \leq \ylo) \, \ylo + \Prob(\ylo < \Z(\ax_{1:\tx}) < \yup) \, \E[\Z(\ax_{1:\tx}) \mid \ylo < \Z(\ax_{1:\tx}) < \yup]\\
        &\quad+ \Prob(\Z(\ax_{1:\tx}) \geq \yup) \, \yup \\ %
        &= \Q,
    \end{align*}
    where the first inequality follows since we assumed \eqref{eq:Hup-rejection-case-3} is false, and the second from \eqref{eq:unclipped-outcomes-inference-intermediate-step}, where we note that \eqref{eq:Hup-rejection-case-2} is false precisely when $\Prob(\Zt(\ax_{1:\tx}) \leq \ylo) \leq \Prob(\Z(\ax_{1:\tx}) \leq \ylo)$.
\end{proof}

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

As a result, when $\Hup$ is falsified, Proposition \ref{prop:Hup-rejection-implications} yields the following inference about the behaviour of these outcomes:
\begin{equation} \label{eq:hup-false-inference-in-terms-of-unclipped-outcomes}
    \text{One of \eqref{eq:Hup-rejection-case-1}, \eqref{eq:Hup-rejection-case-2}, or \eqref{eq:Hup-rejection-case-3} is false.}
\end{equation}
Unlike \eqref{eq:hup-inference-in-terms-of-clipped-outcomes}, this inference is expressed directly in terms of the unclipped outcomes $\Z(\ax_{1:\tx})$ and $\Zt(\ax_{1:\tx})$, since each of \eqref{eq:Hup-rejection-case-1}, \eqref{eq:Hup-rejection-case-2}, and \eqref{eq:Hup-rejection-case-3} are.
%
%
%
Intuitively, we may still interpret \eqref{eq:hup-false-inference-in-terms-of-unclipped-outcomes} as saying that the output $\Zt(\ax_{1:\tx})$ produced by the twin is typically too large: either the twin places too much probability mass in one of the upper tails $(\ylo, \infty)$ or $[\yup, \infty)$ (i.e.\ \eqref{eq:Hup-rejection-case-1} or \eqref{eq:Hup-rejection-case-2}), or else it is on average too large in the region $(\ylo, \yup)$ (i.e.\ \eqref{eq:Hup-rejection-case-3}).
%
The situation is analogous when $\Hlo$ is false, and can be interpreted as saying that the output of the twin $\Zt(\ax_{1:\tx})$ is typically too small.

In some situations, it may be desirable to obtain a more granular conclusion about the outcomes of the twin by identifying \emph{which} case of \eqref{eq:Hup-rejection-case-1}, \eqref{eq:Hup-rejection-case-2}, and \eqref{eq:Hup-rejection-case-3} holds.
This can be achieved by an appropriate choice of $\fx$ or $\B_{0:\tx}$.
For instance, when $\fx(\xx_{0:\tx}) = \ind((\xx_\tx)_i \geq \yup)$, if $\Hup$ is false, then \eqref{eq:Hup-rejection-case-1} must hold.
Similarly, when $\B_{\tx} = \{\xx_\tx \in \Xspace_\tx \mid (\xx_\tx)_i \in (\ylo, \yup)\}$, if $\Hup$ is false, then \eqref{eq:Hup-rejection-case-3} must hold.


\begin{figure}[t]
    \centering
    \includegraphics[height=21cm]{figures/causal/latest_experimental_results/nhistograms_nogray.pdf}
    \caption{Histograms of $n$ and $\widehat{n}$ (as defined in Section \ref{sec:confidence-intervals-methodology-supplement}) across all hypothesis parameters corresponding to each physiological quantity of interest.}
    \label{fig:n_histograms}
\end{figure}

\subsection{Generating twin trajectories using the Pulse Physiology Engine}\label{sec:pulse-trajectories-supplement}
The Pulse Physiology Engine is an open source comprehensive human physiology simulator that has been used in medical education, research, and training. The core engine of Pulse is C++ based with APIs available in different languages, including python. Detailed documentation is available at: \href{https://pulse.kitware.com/}{pulse.kitware.com}.
Pulse allows users to initialise patient trajectories with given age, sex, weight, heart rate, systolic blood pressure, diastolic blood pressure and respiration rate and medical conditions such as sepsis, COPD, ARDS, etc. Once initialised, users have the ability to advance patient trajectories by a given time step (one hour in our case), and administer actions (e.g. administer a given dose of IV fluids or vasopressors).

In Algorithm \ref{algo:twin-data-generation} we describe how we generated the twin data to test the chosen hypotheses. Note that we sampled $\X_0$ without replacement as it ensures that each $\X_0$ is chosen at most once and consequently twin trajectories in $\Dt(\ax_{1:\tx})$ are i.i.d. 
%
%
Additionally, Algorithm \ref{algo:twin-data-generation} can be easily parallelised to improve efficiency.
%
Figure \ref{fig:n_histograms} shows histograms of the number of twin trajectories $\widehat{\nx}$ (as defined in Section \ref{sec:confidence-intervals-methodology-supplement} above) obtained in this way across all hypotheses.

\begin{algorithm}
\SetAlgoLined
\textbf{Inputs:} Action sequence $\ax_{1:\tx}$; Observational dataset $\D$.\\
\textbf{Output:} Twin data $\Dt(\ax_{1:\tx})$ of size $m$.\\
\For{$i = 1, \dots, m$}{
Sample $\X_0$ without replacement from $\D$\;
$\Xt_0 \leftarrow  \X_0$ i.e., initialise the Pulse trajectory with the information of $\X_0$\;
\For{$\tx' = 1, \dots, \tx$}{
Administer the median doses of IV fluids and vasopressors in action bin $\ax_{\tx'}$\;
\If{$\tx' \equiv 0$ \textup{(mod 3)}}{
Virtual patient in Pulse consumes nutrients and water, and urinates\;
}
Advance the twin trajectory by one hour\;
}
Add the trajectory $\Xt_{0:\tx}(\ax_{1:\tx})$ to $\Dt(\ax_{1:\tx})$\;
}
\textbf{Return} $\Dt(\ax_{1:\tx})$
\caption{Generating Twin data $\Dt(\ax_{1:\tx})$.}
\label{algo:twin-data-generation}
\end{algorithm}


\begin{figure}[t]
    \centering
    \includegraphics[height=12cm]{figures/causal/latest_experimental_results/p_vals_revperc_monocolored_nogray.pdf}
    \caption{Boxenplots showing distributions of $-\log_{10}{p_\textup{lo}}$ and $-\log_{10}{p_\textup{up}}$ for different physiological quantities obtained via the reverse percentile method.
    Higher values of $-\log_{10}{p}$ indicate greater evidence in favour of rejection.
    Note that we computed these $p$-values numerically by determining the lowest level at which each hypothesis was rejected over a grid of values in $(0, 1)$, with the smallest such level being $10^{-6}$.
    In cases where a hypothesis was rejected at every level tested, we defined the $p$-value to be $10^{-6}$, and so the horizontal axis here is truncated to between $0$ and $6$.
    In some cases, e.g.\ $\Hlo$ for Chloride, every hypothesis obtained a $p$-value of $10^{-6}$ in this way.}
    \label{fig:p_values_revperc}
\end{figure}


\begin{figure}[t]%
    \centering
    \includegraphics[height=12cm]{figures/causal/latest_experimental_results/p_vals_hoeff_nogray.pdf}
    \caption{Boxenplots showing distributions of $-\log_{10}{p_\textup{lo}}$ and $-\log_{10}{p_\textup{up}}$ for different physiological quantities obtained via Hoeffding's inequality. Higher values indicate greater evidence in favour of rejection.}
    \label{fig:p_values_hoeff_complete}
\end{figure}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\begin{table}%
    \centering
\begin{footnotesize}
\begin{tabular}{lll}
\toprule
                   Physiological quantity &  \# Rejections &  \# Hypotheses \\
\midrule
  Chloride Blood Concentration (Chloride) &            24 &            94 \\
      Sodium Blood Concentration (Sodium) &            21 &            94 \\
Potassium Blood Concentration (Potassium) &            13 &            94 \\
                  Skin Temperature (Temp) &            10 &            86 \\
    Calcium Blood Concentration (Calcium) &             5 &            88 \\
    Glucose Blood Concentration (Glucose) &             5 &            96 \\
      Arterial CO$_2$ Pressure (paCO$_2$) &             3 &            70 \\
Bicarbonate Blood Concentration (HCO$_3$) &             2 &            90 \\
       Systolic Arterial Pressure (SysBP) &             2 &           154 \\
        Arterial O$_2$ Pressure (paO$_2$) &             0 &            78 \\
                Arterial pH (Arterial\_pH) &             0 &            80 \\
      Diastolic Arterial Pressure (DiaBP) &             0 &            72 \\
          Mean Arterial Pressure (MeanBP) &             0 &            92 \\
                    Respiration Rate (RR) &             0 &           172 \\
                          Heart Rate (HR) &             0 &           162 \\
\bottomrule
\end{tabular}
\end{footnotesize}
    \caption{Total hypotheses and rejections per physiological quantity obtained using Hoeffding's inequality.} \label{tab:hypotheses_hoeffding_full}
\end{table}

\begin{table}[t]
\centering
\begin{footnotesize}
\begin{tabular}{lll}
\toprule
                    Physiological quantity &  \# Rejections &  \# Hypotheses \\
\midrule
  Chloride Blood Concentration (Chloride) &            47 &            94 \\
      Sodium Blood Concentration (Sodium) &            46 &            94 \\
Potassium Blood Concentration (Potassium) &            33 &            94 \\
                  Skin Temperature (Temp) &            43 &            86 \\
    Calcium Blood Concentration (Calcium) &            44 &            88 \\
    Glucose Blood Concentration (Glucose) &            19 &            96 \\
      Arterial CO$_2$ Pressure (paCO$_2$) &            13 &            70 \\
Bicarbonate Blood Concentration (HCO$_3$) &             8 &            90 \\
       Systolic Arterial Pressure (SysBP) &             8 &           154 \\
        Arterial O$_2$ Pressure (paO$_2$) &             4 &            78 \\
                Arterial pH (Arterial\_pH) &             0 &            80 \\
      Diastolic Arterial Pressure (DiaBP) &             0 &            72 \\
          Mean Arterial Pressure (MeanBP) &             3 &            92 \\
                    Respiration Rate (RR) &            12 &           172 \\
                          Heart Rate (HR) &             1 &           162 \\
\bottomrule
                                    %
%
\end{tabular}
\end{footnotesize}
\caption{The total number of hypotheses per outcome, along with rejections obtained using the reverse percentile bootstrap.} \label{tab:hypotheses_rev_percentile}
\end{table}
\subsection{Bootstrapping details} \label{sec:boostrapping-details-supplement}

In addition to Hoeffding's inequality, we also used reverse percentile bootstrap method (see e.g.\ \cite{hesterberg2015what}) to test the chosen hypotheses as described in Section \ref{sec:hyp-testing-supplement}. 
We used 100 bootstrap samples to construct the confidence intervals on $\Qlo$, $\Qt$ and $\Qup$ for each hypothesis.
To avoid bootstrapping on small numbers of data points, we did not reject any hypothesis where either the number of observational trajectories $n$ or twin trajectories $\widehat{n}$ was less than 100, and returned a $p$-value of 1 in each such case.

\subsubsection*{Hypothesis rejections}
Table \ref{tab:hypotheses_rev_percentile} shows the number of rejected hypotheses for each physiological quantity.
We observed similar trends between results obtained using Hoeffding's inequality (Table \ref{tab:hypotheses_hoeffding_full}) and bootstrapping (Table \ref{tab:hypotheses_rev_percentile}).
For example, we obtained high number of rejections for Sodium, Chloride and Potassium blood concentrations but few rejections for Arterial Pressure and Heart Rate.
Overall, bootstrapping increased the number of rejected hypotheses by a factor of roughly 3.3 compared with Hoeffding's inequality (281 vs.\ 85 rejections in total).

\subsubsection*{$p$-value plots}

Figure \ref{fig:p_values_revperc} shows the distributions of $-\log_{10} \plo$ and $-\log_{10}\pup$ for different physiological quantities, with higher values indicating greater evidence in favour of rejection.
We again saw the same trends between $p$-value plots for bootstrapping (Figure \ref{fig:p_values_revperc}) and Hoeffding's inequality (Figure \ref{fig:p_values_hoeff_complete}).
Specifically, we can see that the $p$-values for Sodium, Chloride, Potassium blood concentrations and Skin Temperature are often low, suggesting that the twin simulation of these quantities is not accurate.
Additionally, we again observed that for each physiological quantity, we either obtain low values for $\plo$, or low values of $\pup$, but not both.
Moreover, for each quantity, whether we obtain low values for $\plo$ or low values for $\pup$ remained consistent between Figures \ref{fig:p_values_revperc} and \ref{fig:p_values_hoeff_complete}. 
For example, both plots suggest that Calcium and Sodium blood concentrations are over-estimated by the twin whereas Skin Temperature and Chloride blood concentrations are under-estimated.


\subsubsection*{Bootstrapping vs.\ Hoeffding's inequality for hypothesis testing}

Overall, it can be seen that we obtained lower $p$-values and consequently more rejections when using bootstrapping, as compared to Hoeffding's inequality.
This happens because the finite sample guarantee in Hoeffding's inequality comes at the cost of more conservative intervals.
%
%
We confirmed this empirically in Figure \ref{fig:len_ratio_histograms}, which plots the histograms of the ratios 
\[
\frac{\textup{Hoeffding's interval length}}{\textup{Bootstrapping interval length}},
\]
for each physiological quantity.
We observed that bootstrapping yielded confidence intervals that were typically between 7.5 to 15 times smaller than those produced by Hoeffding's inequality.

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\begin{figure}[t]
    \centering
    \includegraphics[height=21cm]{figures/causal/latest_experimental_results/ratio_histograms_nogray.pdf}
    \caption{
    Histograms of $L^{\textup{h}}_{\textup{lo}}/L^{\textup{b}}_{\textup{lo}}$ and $L^{\textup{h}}_{\textup{up}}/L^{\textup{b}}_{\textup{up}}$ for different hypotheses and outcomes $Y$.
    Here, $L^{\textup{h}}_{\textup{lo}}$ and $L^{\textup{b}}_{\textup{lo}}$ denote the lengths of intervals $[\ylo, \qup{\alpha}]$ obtained using Hoeffding's inequality and bootstrapping respectively. Likewise, $L^{\textup{h}}_{\textup{up}}$ and $L^{\textup{b}}_{\textup{lo}}$ correspond to the lengths of $[\qlo{\alpha}, \yup]$.}
    \label{fig:len_ratio_histograms}
\end{figure}

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%

\begin{figure}[t]
    \centering
        \begin{subfigure}[b]{0.26\textwidth}
    \includegraphics[height=3.7cm]{figures/causal/latest_experimental_results/Glucose_hyp_1_with_hoeff_onesided_loFalse_nogray.pdf}
    \subcaption{Not rejected}
    \label{fig:glucosea-supp}
    \end{subfigure}%
    \begin{subfigure}[b]{0.26\textwidth}
    \includegraphics[height=3.7cm]{figures/causal/latest_experimental_results/Glucose_hyp_5_with_hoeff_onesided_loFalse_nogray.pdf}
    \subcaption{Rejected}
    \label{fig:glucoseb-supp}
    \end{subfigure}\\      
    \begin{subfigure}[b]{0.26\textwidth}
    \includegraphics[height=3.7cm]{figures/causal/latest_experimental_results/Glucose_hyp_8_with_hoeff_onesided_loFalse_nogray.pdf}
    \subcaption{Not rejected}
    \label{fig:chloridea}
    \end{subfigure}%
    \begin{subfigure}[b]{0.26\textwidth}
    \includegraphics[height=3.7cm]{figures/causal/latest_experimental_results/Glucose_hyp_6_with_hoeff_onesided_loFalse_nogray.pdf}
    \subcaption{Rejected}
    \label{fig:chlorideb}
    \end{subfigure}\\
    \begin{subfigure}[b]{0.26\textwidth}
    \includegraphics[height=3.7cm]{figures/causal/latest_experimental_results/Glucose_hyp_46_with_hoeff_onesided_loFalse_nogray.pdf}
    \subcaption{Not rejected}
    \label{fig:potassiuma}
    \end{subfigure}%
    \begin{subfigure}[b]{0.26\textwidth}
    \includegraphics[height=3.7cm]{figures/causal/latest_experimental_results/Glucose_hyp_7_with_hoeff_onesided_loFalse_nogray.pdf}
    \subcaption{Rejected}
    \label{fig:potassiumb}
    \end{subfigure}\\
    \begin{subfigure}[b]{0.26\textwidth}
    \includegraphics[height=3.7cm]{figures/causal/latest_experimental_results/Glucose_hyp_47_with_hoeff_onesided_loFalse_nogray.pdf}
    \subcaption{Not rejected}
    \label{fig:paco2a}
    \end{subfigure}%
    \begin{subfigure}[b]{0.26\textwidth}
    \includegraphics[height=3.7cm]{figures/causal/latest_experimental_results/Glucose_hyp_10_with_hoeff_onesided_loFalse_nogray.pdf}
    \subcaption{Rejected}
    \label{fig:paco2b}
    \end{subfigure}
    %
    %
    %

    \caption{Raw observational data values conditional on $\A_{1:\tx}=\ax_{1:\tx}$ and $\X_{0:\tx}(\A_{1:\tx})\in \B_{0:\tx}$, and from the output of the twin conditional on $\Xt_{0:\tx}(\ax_{1:\tx})\in \B_{0:\tx}$.
    Each row shows two distinct choices of $(\B_{0:\tx}, \ax_{1:\tx})$.
    Below each figure are shown 95\% Hoeffding confidence intervals for $\Qt$ and $\Qup$.
    Unlike Figure \ref{fig:histograms} from the main text, the horizontal axes of the histograms are not truncated, and the first row is in particular an untruncated version of Figure \ref{fig:histograms} from the main text.
    Note however that the scales of the horizontal axes of the confidence intervals differ from those of the histograms, since it is visually more difficult to determine whether or not the confidence intervals overlap when fully zoomed out.} \label{fig:histograms-supplement}
\end{figure}

\subsection{Tightness of bounds and number of data points per hypothesis}
    In this section, we show empirically how both the tightness of the bounds $[\Qlo, \Qup]$ and the number of data points per hypothesis relate to the number of falsifications obtained in our case study.
    Recall that the tightness of $[\Qlo, \Qup]$ is determined by the value of $\Prob(\A_{1:\tx} = \ax_{1:\tx} \mid \X_{0:\N}(\A_{1:\N}) \in \B_{0:\N})$, since we have
    \begin{equation} \label{eq:N-propensity-tightness-relation}
        \frac{\Qup - \Qlo}{\yup - \ylo} = 1 - \Prob(\A_{1:\tx} = \ax_{1:\tx} \mid \X_{0:\N}(\A_{1:\N}) \in \B_{0:\N}).
    \end{equation}
    Here the left-hand side is a number in $[0, 1]$ that quantifies the tightness of the bounds $[\Qlo, \Qup]$ relative to the trivial worst-case bounds $[\ylo, \yup]$, with smaller values meaning tighter bounds. The equation above shows that the higher the value of $\Prob(\A_{1:\tx} = \ax_{1:\tx} \mid \X_{0:\N}(\A_{1:\N}) \in \B_{0:\N})$, the tighter the bounds are.
    
    Figure \ref{fig:scatter-plot} shows the bounds are often informative in practice, with $\Prob(\A_{1:\tx} = \ax_{1:\tx} \mid \X_{0:\N}(\A_{1:\N}) \in \B_{0:\N})$ being reasonably large (and hence the bounds tight, by \eqref{eq:N-propensity-tightness-relation} above) for a significant number of hypotheses we consider.
    However, rejections still occur even when the bounds are reasonably loose (e.g.\ $\Prob(\A_{1:\tx} = \ax_{1:\tx} \mid \X_{0:\N}(\A_{1:\N}) \in \B_{0:\N}) \approx 0.3$), which shows our method can still yield useful information even in this case.
    We moreover observe rejections across a range of different numbers of observational data points used to test each hypothesis, which shows that our method is not strongly dependent on the size of the dataset obtained. 

    \subsection{Sensitivity to $\ylo$ and $\yup$} \label{sec:sensitity-analysis-appendix}

    We investigated the sensitivity of our methodology with respect to our choices of the values $\ylo$ and $\yup$.
    Specifically, we repeated our procedure with the intervals $[\ylo, \yup]$ replaced with $[\ylo\, (1- \Delta/2), \yup\,(1 + \Delta/2)]$ for a range of different values of $\Delta \in \R$.
    Figure \ref{fig:sensitivity-plot-rejections} plots the number of rejections for different values of $\Delta$.
    We observe that for significantly larger $[\ylo, \yup]$ intervals, we do obtain fewer rejections, although this is to be expected since the widths of our both the bounds $[\Qlo, \Qup]$ and our confidence intervals $\qlo{\alpha}$ and $\qup{\alpha}$ obtained using Hoeffding's inequality (see Proposition \ref{prop:hoeffding-confidence-bounds-supp}) grow increasingly large as the width of $[\ylo, \yup]$ grows.
    However, we observe that the number of rejections per outcome is stable for a moderate range of widths of $[\ylo, \yup]$, which indicates that our method is reasonably robust to the choice of $\ylo, \yup$ parameters.

    %

    \begin{figure}[t]
        \centering
        \includegraphics[height=7cm]{figures/causal/latest_experimental_results/propensity-plots_nogray.pdf}
        \caption{Sample mean estimate of $\Prob(\A_{1:\tx} = \ax_{1:\tx} \mid \X_{0:\N}(\A_{1:\N}) \in \B_{0:\N})$ for each pair of hypotheses $(\Hlo, \Hup)$ corresponding to the same set of parameters $(\tx, \fx, \ax_{1:\tx}, \B_{0:\tx})$ that we tested, along with the corresponding number of observational data points used to test each hypothesis.
        Red points indicate that either $\Hlo$ or $\Hup$ were rejected, while blue points indicate that both $\Hlo$ and $\Hup$ were not rejected.}
        \label{fig:scatter-plot}
    \end{figure}


    \begin{figure}[t]
    \centering
\includegraphics[width=0.7\textwidth]{figures/causal/latest_experimental_results/sensitivity-plots-rejections_nogray.pdf}
    \caption{Rejections obtained as the width of the $[\ylo, \yup]$ interval changes. Here, the interval is increased (or decreased) symmetrically on each side.}
    \label{fig:sensitivity-plot-rejections}
\end{figure}
