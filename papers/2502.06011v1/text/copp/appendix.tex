\section{Proofs}\label{sec:proofs}
\subsection{Proof of Proposition \ref{coverage_theorem}} 
This proof is a direct adaptation of \cite[Lemma 3]{tibshirani2020conformal}, and has only been included for the sake of completeness.

In this proof, we use the notion of \textit{weighted exchangeability} as defined in Section 3.2 of \cite{tibshirani2020conformal}.
\begin{definition}[Weighted exchangeability]\label{def:weighted_exch}
Random variables $V_1, \dots, V_n$ are said to be \textit{weighted exchangeable} with weight functions $w_1, \dots, w_n$, if the density $f$ of their joint distribution can be factorized as
\begin{align}
    f(v_1, \dots, v_n) = \prod_{i=1}^n w_i(v_i) g(v_1, \dots, v_n)
\end{align}
where $g$ is any function that does not depend on the ordering of its inputs, i.e. $g(v_{\sigma(1)}, \dots, v_{\sigma(n)}) = g(v_1, \dots, v_n)$ for any permutation $\sigma$ of $1, \dots, n$.
\end{definition}

\begin{lemma}\label{exchangeability_lemma}
Let $Z_i = (X_i, Y_i) \in \mathbb{R}^d \times \mathbb{R}$, $i=1,...,n+1$, be such that $\{(X_i, Y_i)\}_{i=1}^n \overset{\textup{i.i.d.}}{\sim}P^{\pi^b}_{X,Y}$ and $(X_{n+1}, Y_{n+1}) \sim P^{\pi^*}_{X,Y}$. Then $Z_1, \dots, Z_{n+1}$ are weighted exchangeable with weights $w_i \equiv 1$, $i\leq n$ and $w_{n+1}(X,Y) = \mathrm{d}P^{\pi^{*}}_{X,Y}/\mathrm{d}P^{\pi^{b}}_{X,Y}(X,Y)$.
\end{lemma}

\begin{proof}
The proof below is merely a verification that our proposed weights still retain the coverage guarantees and is mainly taken from \cite{tibshirani2020conformal}. Hence, we follow the same strategy as in \cite{tibshirani2020conformal}, with the exception that we have the weights as in Lemma \ref{exchangeability_lemma}, hence inducing a lot of simplifications. As in \cite{tibshirani2020conformal}, we assume for simplicity that $V_1, \dots, V_{n+1}$ are distinct almost surely, however the result holds in general case as well. We define $f$ as the joint distribution of the random variables $\{X_i, Y_i\}_{i=1}^{n+1}$. We also denote $E_z$ as the event of $\{Z_1, \dots, Z_{n+1}\}$ = $\{z_1, \dots, z_{n+1}\}$ and let $v_i = s(z_i) = s(x_i, y_i)$, then for each $i$:
\begin{align}
    \mathbbm{P}\{V_{n+1} = v_i| E_z\} = \mathbbm{P}\{Z_{n+1}=z_i|E_z\} = \frac{\sum_{\sigma:\sigma(n+1)=i}f(z_{\sigma(1)}, \dots, z_{\sigma(n+1)})}{\sum_{\sigma}f(z_{\sigma(1)}, \dots, z_{\sigma(n+1)})}
\end{align}

Now using the fact that $Z_1, \dots, Z_{n+1}$ are weighted exchangeable:

\begin{align}
    \frac{\sum_{\sigma:\sigma(n+1)=i}f(z_{\sigma(1)}, \dots, z_{\sigma(n+1)})}{\sum_{\sigma}f(z_{\sigma(1)}, \dots, z_{\sigma(n+1)})}&= \frac{\sum_{\sigma:\sigma(n+1)=i}\prod_{j=1}^{n+1}w_{j}(z_{\sigma(j)})g(z_{\sigma(1)}, \dots, z_{\sigma(n+1)})}{\sum_{\sigma}\prod_{j=1}^{n+1} w_{j}(z_{\sigma(j)})g(z_{\sigma(1)}, \dots, z_{\sigma(n+1)})}\label{eq:simply}\\ 
    &= \frac{w_{n+1}(z_i)g(z_{1}, \dots, z_{n+1})}{\sum_{j=1}^{n+1} w_{n+1}(z_{j})g(z_{1}, \dots, z_{n+1})}\nonumber\\
    &= p_i^w(z_{n+1}) \nonumber
\end{align}
where we recall that
\begin{align}
    p_i^{w}(x, y) \coloneqq \frac{w(X_i, Y_i)}{\sum_{j=1}^n w(X_j, Y_j) + w(x, y)}. \nonumber
\end{align}
We get simplifications in \eqref{eq:simply} due to the weights defined in Lemma \ref{exchangeability_lemma}, i.e. $w_i \equiv 1$ for $i \leq n$ and $w_{n+1}(x, y) = w(x, y) = \mathrm{d}P^{\pi^{*}}_{X,Y}/\mathrm{d}P^{\pi^{b}}_{X,Y}(x,y)$.
Next, just as in \cite{tibshirani2020conformal} we can view:
\begin{align}
    V_{n+1} = v_i| E_z \sim \sum_{i=1}^{n+1}p_i^w(z_{n+1}) \delta_{v_i}
\end{align}
which implies that:
$$
\mathbbm{P}\{V_{n+1} \leq \text{Quantile}_{\beta}(\sum_{i=1}^{n+1}p_i^w(z_{n+1}) \delta_{v_i}) | E_z\} \geq \beta.
$$
This is equivalent to 
$$
\mathbbm{P}\{V_{n+1} \leq \text{Quantile}_{\beta}(\sum_{i=1}^{n+1}p_i^w(Z_{n+1}) \delta_{v_i}) | E_z\} \geq \beta
$$
and, after marginalizing, one has
$$
\mathbbm{P}\{V_{n+1} \leq \text{Quantile}_{\beta}(\sum_{i=1}^{n+1}p_i^w(Z_{n+1}) \delta_{v_i})\} \geq \beta
$$
This is equivalent to the claim in Proposition \ref{coverage_theorem}.
\end{proof}

\subsection{Proof of Proposition \ref{prop2}}
The following proof is an adaptation of \cite[Proposition 1]{lei2020conformal} to our setting.

Before detailing the main proof, we introduce a preliminary result which will be used in the proof of Proposition \ref{prop2}.

%
%
%
%
%
%

%
%
%
%
%
%

\begin{lemma}\label{Aevent}
Let $\hat{w}(x,y)$ be an estimate of the weights $w(x,y) = \mathrm{d}P^{\pi^{*}}_{X,Y}/\mathrm{d}P^{\pi^{b}}_{X,Y}(x,y)$, and $$(\expb[\hat{w}(X,Y)^r])^{1/r} \leq M_r < \infty$$ for some $r \geq 2$. Let $(X_i, Y_i) \overset{\textup{i.i.d.}}{\sim} P^{\pi^b}_{X,Y}$ and $\mathcal{A}$ denote the event that 
\[
\sum_{i=1}^n \hat{w}(X_i, Y_i) \leq n/2.
\]
Then, 
\[
\mathbb{P}(\mathcal{A}) \leq \frac{c_1 M_r^2}{n}
\]
where $c_1$ is an absolute constant, and the probability is taken over $\{X_i, Y_i\}_{i=1}^n  \overset{\textup{i.i.d.}}{\sim} P^{\pi^b}_{X,Y}$.
\end{lemma}

\subsubsection*{Proof of Lemma \ref{Aevent}}
The condition $\expb[\hat{w}(X, Y)^r] < \infty \implies \behprob(\hat{w}(X, Y)< \infty) = 1$ and $\expb[\hat{w}(X, Y)]<\infty$. WLOG assume $\expb[\hat{w}(X, Y)] = 1$. Recall that $p_{i}^{\hat{w}}(x, y) \coloneqq \frac{\hat{w}(X_i, Y_i)}{\sum_{i=1}^n \hat{w}(X_i, Y_i) + \hat{w}(x,y)}$, and therefore, $p_i^{\hat{w}}(x,y)$ are invariant to weight scaling. Since $\expbcal[\hat{w}(X_i, Y_i)]^2 \leq M_r^2$ and $\expbcal(\hat{w}(X_i, Y_i)) = 1$, using Chebyshev's inequality %
%
%
%
%
%
%
%
%
%
\begin{align}
    \p\left( \sum_{i=1}^n  \hat{w}(X_i, Y_i) \leq n/2 \right) =& \p\left( \sum_{i=1}^n  (\hat{w}(X_i, Y_i)-1) \leq -n/2 \right) \nonumber\\
    \leq& \p\left( |\sum_{i=1}^n  (\hat{w}(X_i, Y_i) -1)| \geq n/2 \right) \nonumber \\
    \leq& \frac{4}{n^2}\E \Bigg[\left( \sum_{i=1}^n \hat{w}(X_i, Y_i) - \E[\hat{w}(X_i, Y_i)] \right)^2\Bigg] \nonumber \\
    =& \frac{4}{n^2} \left\{ n \E |\hat{w}(X_1, Y_1) - \E[\hat{w}(X_1, Y_1)] |^2\right\}  \label{holder1}\\ 
   %
    \leq& \frac{16}{n^2}  n \E |\hat{w}(X_1, Y_1)|^2  \label{holder2} \\
    \leq& \frac{c_1 M_r^2}{n} \nonumber
\end{align}
where to get from (\ref{holder1}) to (\ref{holder2}) we use:
%
%
%
\begin{align}
    \E |\hat{w}(X_1, Y_1) - \E[\hat{w}(X_1, Y_1)] |^2 &\leq 2 \E \left[\hat{w}(X_1, Y_1)^2 + \E[\hat{w}(X_1, Y_1)]^2 \right] \nonumber\\
    &\leq 4\E[\hat{w}(X_1, Y_1)^2]. \nonumber
\end{align}
\qed


We can now prove Proposition \ref{prop2}.
\begin{proof}
The condition $\expb[\hat{w}(X, Y)^r] < \infty \implies \behprob(\hat{w}(X, Y)< \infty) = 1$ and $\expb[\hat{w}(X, Y)]<\infty$. WLOG assume $\expb[\hat{w}(X, Y)] = 1$. Let $\tilde{P}^{\pi^*}_{X, Y}$ be a probability measure with 
\[
    \mathrm{d}\tilde{P}^{\pi^*}_{X, Y}(x,y) \coloneqq \hat{w}(x,y) \mathrm{d}P^{\pi^b}_{X, Y}(x,y)
\]
and $(\tilde{X}, \tilde{Y}) \sim \tilde{P}^{\pi^*}_{X,Y}$ that is independent of the data. By H\"older's inequality, 
\begin{align}
    \expatt[\hat{w}(\tilde{X}, \tilde{Y})] =& \int_{\tilde{x}, \tilde{y}} \frac{\mathrm{d}\tilde{P}^{\pi^*}(\tilde{x}, \tilde{y})}{\mathrm{d}P^{\pi^b}(\tilde{x}, \tilde{y})}\mathrm{d}\tilde{P}^{\pi^*}(\tilde{x}, \tilde{y}) \nonumber \\
    =& \expb[\hat{w}(X, Y)^2] \leq M_r^2 < \infty \nonumber 
\end{align}
Note using Proposition \ref{coverage_theorem} with $(\tilde{X}, \tilde{Y})$ denoting $(X_{n+1}, Y_{n+1})$ for simplicity 
\begin{align}
    &\mathbb{P}(\tilde{Y} \in \hat{C}(\tilde{X}, \tilde{Y})) \nonumber \\
    &\quad= \exptt \left[\mathbb{P}\left(s(\tilde{X}, \tilde{Y}) \leq \text{Quantile}_{1-\alpha}\left(\sum_{i=1}^n p_i^{\hat{w}}(\tilde{X}, \tilde{Y})\delta_{V_i} + p_{n+1}^{\hat{w}}(\tilde{X}, \tilde{Y})\delta_\infty \right)\mid \mathcal{E}(\tilde{V})\right)\right] \label{eq4}
\end{align}
where $\mathcal{E}(\tilde{V})$ denotes the unordered set of $V_1, \dots, V_{n+1}$. Marginalising over $\{(X_i, Y_i)\}_{i=1}^n$, we obtain
%
%
%
\begin{align}
    (\ref{eq4}) \leq \E\left(1-\alpha + \max_{i \in [n+1]} p_i^{\hat{w}}(\tilde{X}, \tilde{Y}) \right) \label{eq5}
\end{align}
where the expectation is over $\{(X_i, Y_i)\}_{i=1}^n \overset{\textup{i.i.d.}}{\sim} P^{\pi^b}_{X, Y}$ and $(\tilde{X}, \tilde{Y}) \sim \tilde{P}^{\pi^*}_{X, Y}$. Let $\mathcal{A}$ denote the event that 
\[
\sum_{i=1}^n \hat{w}(X_i, Y_i) \leq n/2.
\]
using Lemma \ref{Aevent} and $\E[\hat{w}(\tilde{X}, \tilde{Y})] \leq M_r^2 $, we get that
%
%
%
%
%
%
%
%
%
\begin{align}
    \E\left[\max_{i \in [n+1]} p_i^{\hat{w}}(\tilde{X}, \tilde{Y})\right] =& \E\left[ \frac{\max \{\hat{w}(\tilde{X}, \tilde{Y}), \max_i \hat{w}(X_i, Y_i) \}}{\hat{w}(\tilde{X}, \tilde{Y}) + \sum_{i=1}^n \hat{w}(X_i, Y_i) } \right] \nonumber \\
    \leq& \E\left[ \frac{\max \{\hat{w}(\tilde{X}, \tilde{Y}), \max_i \hat{w}(X_i, Y_i) \}}{\hat{w}(\tilde{X}, \tilde{Y}) + \sum_{i=1}^n \hat{w}(X_i, Y_i) } \mathbbm{1}_{\mathcal{A}^C} \right] + \mathbb{P}(\mathcal{A}) \nonumber \\
    \leq& \E\left[ \frac{2\max \{\hat{w}(\tilde{X}, \tilde{Y}), \max_i \hat{w}(X_i, Y_i) \}}{n} \mathbbm{1}_{\mathcal{A}^C} \right] + \frac{c_1 M_r^2}{n} \nonumber \\
    \leq& \frac{2}{n} \left( \E[\hat{w}(\tilde{X}, \tilde{Y})] + \E \max_i \hat{w}(X_i, Y_i) 
    \right) + \frac{c_1 M_r^2}{n} \nonumber \\
    \leq& \frac{2}{n}\left( \E[\hat{w}(\tilde{X}, \tilde{Y})] + \left(\sum_{i=1}^n \E[\hat{w}(X_i, Y_i)^r]\right)^{1/r}\right) + \frac{c_1 M_r^2}{n} \nonumber \\
    \leq& \frac{2}{n}\left(M_r^2 + n^{1/r}M_r \right) + \frac{c_1 M_r^2}{n}.  \nonumber
\end{align} 
This implies that 
\[
\ttar(Y \in \hat{C}(X)) \leq 1-\alpha + cn^{1/r-1}
\]
for some constant $c$ that only depends on $M_r$ and $r$.
Note that 
\begin{align}
    | \ttar(Y \in \hat{C}(X)) - \tarprob(Y \in \hat{C}(X)) | \leq d_{\textup{TV}}(\tilde{P}^{\pi^*}, P^{\pi^*})  \label{eq6}
\end{align}
where $d_{\textup{TV}}$ is the total variation norm which satisfies
\begin{align}
    d_{\textup{TV}}(\tilde{P}^{\pi^*}, P^{\pi^*}) =& \frac{1}{2} \int | \hat{w}(x,y)\mathrm{d}P^{\pi^b}(x,y) - \mathrm{d}P^{\pi^*}(x,y) | \nonumber \\
    =& \frac{1}{2} \int | \hat{w}(x,y)\mathrm{d}P^{\pi^b}(x,y) - w(x,y) \mathrm{d}P^{\pi^b}(x,y) | \nonumber\\
    =& \frac{1}{2} \expb[|\hat{w}(X,Y) - w(X,Y) |] = \Delta_w. \label{eq7}
\end{align}
Putting together (\ref{eq6}) and (\ref{eq7}), we get
\begin{align}
    \tarprob(Y \in \hat{C}(X)) \leq 1-\alpha + \Delta_w + cn^{1/r-1}.
\end{align}
For the lower bound, using Proposition \ref{coverage_theorem} we get that 
\begin{align}
    \p_{(\tilde{X}, \tilde{Y}) \sim \tilde{P}^{\pi^*}_{X,Y}}(\tilde{Y} \in \hat{C}(\tilde{X}, \tilde{Y})) =& \mathbb{P}\left(s(\tilde{X}, \tilde{Y}) \leq \text{Quantile}_{1-\alpha}\left(\sum_{i=1}^n p_i^{\hat{w}}(\tilde{X}, \tilde{Y})\delta_{V_i} + p_{n+1}^{\hat{w}}(\tilde{X}, \tilde{Y})\delta_\infty \right)\right) \nonumber\\
    \geq 1-\alpha.
\end{align}
Using (\ref{eq6}) we thus obtain 
\begin{align}
    \tarprob(Y \in \hat{C}(X)) \geq& \ttar(Y \in \hat{C}(X)) - d_{TV}(\tilde{P}^{\pi^*}, P^{\pi^*}) \nonumber \\
    \geq& 1-\alpha - \Delta_w.
\end{align}
\end{proof}

\subsection{Proof of Proposition \ref{conditional-res}}
For notational convenience, we suppress the subscripts $m$ and $n$ in $\hat{q}, \hat{w}, \hat{C}$. Moreover, we use $\hat{w}_i$ to denote $\hat{w}(X_i, Y_i)$ and $\eta(x, y)$ to denote $\textup{Quantile}_{1-\alpha}(\sum_{i=1}^n \hat{p}_i(x, y)\delta_{V_i} + \hat{p}_{n+1}(x, y) \delta_{\infty})$.

%
%
%
%
%
%
%
\begin{proof}


We use $(\tilde{X}, \tilde{Y}) \sim P^{\pi^*}_{X,Y}$ in place of $(X_{n+1}, Y_{n+1})$ and let $\epsilon < r/2$. By the definition of $\hat{C}(\tilde{X})$, we directly have
\begin{align}
    \p(\tilde{Y} \in \hat{C}(\tilde{X}) \mid \tilde{X}) =& \p(s(\tilde{X}, \tilde{Y}) \leq \eta(\tilde{X}, \tilde{Y}) \mid \tilde{X}) \nonumber \\
    \geq& \p(s^*(\tilde{X}, \tilde{Y}) \leq \eta(\tilde{X}, \tilde{Y}) - H(\tilde{X}) \mid \tilde{X}) \label{dr-e1}
\end{align}
where $ s^*(\tilde{X}, \tilde{Y}) \coloneqq \max \{\tilde{Y} - q_{\alpha_{hi}}(\tilde{X}), q_{\alpha_{lo}}(\tilde{X}) - \tilde{Y} \}$ and the probability is taken over $\{(X_i, Y_i)\}_{i=1}^n\overset{\textup{i.i.d.}}{\sim} P^{\pi^b}_{X, Y}$ and $\tilde{Y} \sim P^{\pi^*}_{Y|X=\tilde{X}}$. We then get 
\begin{align}
    \eqref{dr-e1} \geq& \p(s^*(\tilde{X}, \tilde{Y}) \leq -\epsilon - H(\tilde{X}) \mid \tilde{X}) - \p(\eta(\tilde{X}, \tilde{Y}) < -\epsilon \mid \tilde{X}) \nonumber \\
    \geq& \p(s^*(\tilde{X}, \tilde{Y}) \leq -\epsilon - H(\tilde{X}) \mid \tilde{X}) \left(\mathbbm{1}(H(\tilde{X}) \leq \epsilon) + \mathbbm{1}(H(\tilde{X}) > \epsilon)\right) - \p(\eta(\tilde{X}, \tilde{Y}) < -\epsilon \mid \tilde{X}) \label{dr-eq2} \\
    \geq& \left(\p(s^*(\tilde{X}, \tilde{Y}) \leq 0 \mid \tilde{X}) - b_2 \{ \epsilon + H(\tilde{X})\}\right)\mathbbm{1}(H(\tilde{X}) \leq \epsilon)\nonumber \\ 
    & + \p(s^*(\tilde{X}, \tilde{Y}) \leq -\epsilon - H(\tilde{X}) \mid \tilde{X})\mathbbm{1}(H(\tilde{X}) > \epsilon) - \p(\eta(\tilde{X}, \tilde{Y}) < -\epsilon \mid \tilde{X})   \label{dr-eq3} \\
    \geq& \p(s^*(\tilde{X}, \tilde{Y}) \leq 0 \mid \tilde{X})\mathbbm{1}(H(\tilde{X}) \leq \epsilon) - b_2 \{ \epsilon + H(\tilde{X})\mathbbm{1}(H(\tilde{X}) \leq \epsilon)\} \nonumber \\ 
    &+ \left(\p(s^*(\tilde{X}, \tilde{Y}) \leq 0 \mid \tilde{X}) - \p(s^*(\tilde{X}, \tilde{Y}) \in (-\epsilon - H(\tilde{X}), 0))\right)\mathbbm{1}(H(\tilde{X}) > \epsilon) \nonumber\\
    &- \p(\eta(\tilde{X}, \tilde{Y}) < -\epsilon \mid \tilde{X})\nonumber\\
    \geq& \p(s^*(\tilde{X}, \tilde{Y})\leq 0 \mid \tilde{X} ) - b_2 \{ \epsilon + H(\tilde{X}) \mathbbm{1}(H(\tilde{X}) \leq \epsilon ) \} - \mathbbm{1}(H(\tilde{X}) > \epsilon)\nonumber\\
    &- \p(\eta(\tilde{X}, \tilde{Y}) < -\epsilon \mid \tilde{X}) \label{dr-eq4-add}
\end{align}
where, to get from \eqref{dr-eq2} to \eqref{dr-eq3}, we use the condition $2\epsilon < r$ and Assumption 2

\begin{align}
    \eqref{dr-eq4-add} \geq& \p(s^*(\tilde{X}, \tilde{Y})\leq 0 \mid \tilde{X} ) -  b_2 \{ \epsilon + H(\tilde{X})\} - \mathbbm{1}(H(\tilde{X}) > \epsilon) - \p(\eta(\tilde{X}, \tilde{Y}) < -\epsilon \mid \tilde{X}) \label{dr-eq4} \\
    =& 1 - \alpha -  b_2 \{ \epsilon + H(\tilde{X})\} - \mathbbm{1}(H(\tilde{X}) > \epsilon) - \p(\eta(\tilde{X}, \tilde{Y}) < -\epsilon \mid \tilde{X}). \label{dr-eq5}
\end{align}
Next, we derive an upper bound on $\p(\eta(\tilde{X}, \tilde{Y}) < -\epsilon \mid \tilde{X})$. Let $G$ denote the CDF of the random distribution $\sum_{i=1}^n \hat{p}_i(x, y)\delta_{V_i} + \hat{p}_{n+1}(x, y) \delta_{\infty}$. Then, $\eta(\tilde{X}, \tilde{Y}) < -\epsilon$ implies $G(-\epsilon) \geq 1-\alpha$ and thus $\p(\eta(\tilde{X}, \tilde{Y}) < -\epsilon \mid \tilde{X}) \leq \p(G(-\epsilon) \geq 1-\alpha \mid \tilde{X})$ a.s.
Moreover, we have
\begin{align}
    \p(G(-\epsilon) \geq 1 - \alpha \mid \tilde{X}) =& \p \left( \frac{\sum_{i=1}^n \hat{w}_i \mathbbm{1}(V_i \leq - \epsilon) }{\sum_{i=1}^n \hat{w}_i + \hat{w}(\tilde{X}, \tilde{Y}) } \geq 1 - \alpha \mid \tilde{X} \right) \nonumber \\
    \leq& \p \left( \frac{\sum_{i=1}^n \hat{w}_i \mathbbm{1}(V_i \leq - \epsilon) }{\sum_{i=1}^n \hat{w}_i} \geq 1 - \alpha \mid \tilde{X} \right) \label{dr-eq6} \\ 
    =& \p \left( \frac{\sum_{i=1}^n \hat{w}_i \mathbbm{1}(V_i \leq - \epsilon) }{\sum_{i=1}^n \hat{w}_i} \geq 1 - \alpha \right) \label{dr-eq1}
\end{align}
where, to get from \eqref{dr-eq6} to \eqref{dr-eq1} we use the independence of $\{(X_i, Y_i)\}_{i=1}^n$ and $\tilde{X}$.
Now we observe that
\begin{align}
    \frac{\sum_{i=1}^n \hat{w}_i \mathbbm{1}(V_i \leq - \epsilon) }{n} =& \frac{\sum_{i=1}^n (\hat{w}_i - w_i) \mathbbm{1}(V_i \leq - \epsilon) }{n} + \frac{\sum_{i=1}^n w_i \mathbbm{1}(V_i \leq - \epsilon) }{n}. \nonumber
\end{align}
As $n\rightarrow \infty$, the strong law of large numbers yields
\begin{align}
    \left| \frac{\sum_{i=1}^n (\hat{w}_i - w_i) \mathbbm{1}(V_i \leq - \epsilon) }{n}\right| &\overset{a.s.}{\longrightarrow} \left|\expb\left[ (\hat{w}(X, Y) - w(X,Y))\mathbbm{1}(s(X,Y) \leq -\epsilon)\right]\right| \nonumber \\
    &\leq \expb\left[ |\hat{w}(X, Y) - w(X,Y)|\mathbbm{1}(s(X,Y) \leq -\epsilon)\right] \nonumber \\
    &\leq \expb\left[ |\hat{w}(X, Y) - w(X,Y)|\right] \overset{m \rightarrow \infty}{\longrightarrow} 0
\end{align}
from Assumption 1 and
\begin{align}
    \frac{\sum_{i=1}^n w_i \mathbbm{1}(V_i \leq - \epsilon) }{n} \overset{a.s.}{\longrightarrow} \expb[w(X,Y) \mathbbm{1}(s(X,Y) \leq -\epsilon)] = \tarprob(s(X,Y) \leq -\epsilon).
\end{align}
Using the triangle inequality,
\begin{align}
    \tarprob(s(X,Y) \leq -\epsilon) &\leq \tarprob(s^*(X,Y) \leq -\epsilon/2) + \p(H(X) \geq \epsilon/2) \label{dr-eq7} \\
    &\leq \tarprob(s^*(X,Y) \leq 0) - \epsilon b_1/2 + 2^k\mathbb{E}[H^k(X)]/\epsilon^k \label{dr-eq8} \\
    &= 1- \alpha - \epsilon b_1/2 + 2^k\mathbb{E}[H^k(X)]/\epsilon^k \overset{m \rightarrow \infty}{\longrightarrow} 1- \alpha - \epsilon b_1/2. \nonumber
\end{align}
To get from \eqref{dr-eq7} to \eqref{dr-eq8}, we use Assumption 2 and Markov's inequality. Similarly, we have

\begin{align}
    \frac{\sum_{i=1}^n \hat{w}_i}{n} =& \frac{\sum_{i=1}^n (\hat{w}_i - w_i)}{n} + \frac{\sum_{i=1}^n w_i }{n} \nonumber
\end{align}
so, as $n \rightarrow \infty$, 
\begin{align}
    \left| \frac{\sum_{i=1}^n (\hat{w}_i - w_i) }{n}\right| &\overset{a.s.}{\longrightarrow} \left|\expb\left[ (\hat{w}(X, Y) - w(X,Y))\right]\right| \nonumber\\
    &\leq \expb\left[ |\hat{w}(X, Y) - w(X,Y)|\right] \overset{m \rightarrow \infty}{\longrightarrow} 0,
\end{align}
and
\begin{align}
    \frac{\sum_{i=1}^n w_i }{n} \overset{a.s.}{\longrightarrow} \expb[w(X,Y)] = 1.
\end{align}

Putting this all together using the continuous mapping theorem, we get that, almost surely,

\begin{align}
\lim_{m \rightarrow \infty} \lim_{n \rightarrow \infty} \frac{\sum_{i=1}^n \hat{w}_i \mathbbm{1}(V_i \leq - \epsilon) }{\sum_{i=1}^n \hat{w}_i} = \lim_{m \rightarrow \infty} \lim_{n \rightarrow \infty} \frac{\sum_{i=1}^n \hat{w}_i \mathbbm{1}(V_i \leq - \epsilon)/n }{\sum_{i=1}^n \hat{w}_i/n} = 1- \alpha - \epsilon b_1/2.
\end{align}
Since convergence almost surely implies convergence in probability, we have
\begin{align}
    \lim_{m \rightarrow \infty} \lim_{n \rightarrow \infty} \p \left( \frac{\sum_{i=1}^n \hat{w}_i \mathbbm{1}(V_i \leq - \epsilon) }{\sum_{i=1}^n \hat{w}_i} \geq 1 - \alpha \right) = 0.
\end{align}
This implies that, for any $\epsilon > 0$, $\lim_{m \rightarrow \infty} \lim_{n \rightarrow \infty} \p(\eta(\tilde{X}, \tilde{Y}) < -\epsilon \mid \tilde{X}) = 0$ almost surely.


Using Markov's inequality and Assumption 3
\begin{align}
    \p(H(X) > \epsilon) \leq \mathbb{E}[H^k(X)]/\epsilon^k \overset{m \rightarrow \infty}{\longrightarrow} 0.
\end{align}
So as $m\rightarrow \infty$, $H(X)\overset{\mathcal{P}}{\rightarrow} 0$. Similarly, $\mathbbm{1}(H(X) > \epsilon) \overset{\mathcal{P}}{\rightarrow} 0$ as $m\rightarrow \infty$.

Recall (using \ref{dr-eq5}) that, for any $\epsilon \in (0, r/2)$, almost surely,
\begin{align}
    \p(\tilde{Y} \in \hat{C}(\tilde{X}) \mid \tilde{X}) - (1-\alpha -b_2 \epsilon) \geq - b_2 H(\tilde{X}) - \mathbbm{1}(H(\tilde{X}) > \epsilon) - \p(\eta(\tilde{X}, \tilde{Y}) < -\epsilon \mid \tilde{X}).
\end{align}
For given $t > 0$, pick $\epsilon < \min(r/2, t/2b_2)$. Then,
\begin{align}
    \p(\tilde{Y} \in \hat{C}(\tilde{X}) \mid \tilde{X}) - (1-\alpha -t/2) \geq - b_2 H(\tilde{X}) - \mathbbm{1}(H(\tilde{X}) > \epsilon) - \p(\eta(\tilde{X}, \tilde{Y}) < -\epsilon \mid \tilde{X}). \label{dr-eq10}
\end{align}
Each term on the right hand side of \eqref{dr-eq10} converges in probability to 0 as $m, n \rightarrow \infty$, and therefore using continuous mapping theorem  
$$ 
b_2 H(\tilde{X}) + \mathbbm{1}(H(\tilde{X}) > \epsilon) + \p(\eta(\tilde{X}, \tilde{Y}) < -\epsilon \mid \tilde{X}) \overset{\mathcal{P}}{\rightarrow} 0.
$$
This implies
\begin{align}
    &\p(\p(\tilde{Y} \in \hat{C}(\tilde{X}) \mid \tilde{X}) \leq 1-\alpha -t) \nonumber\\
    &\quad \leq \p(b_2 H(\tilde{X}) + \mathbbm{1}(H(\tilde{X}) > \epsilon) + \p(\eta(\tilde{X}, \tilde{Y}) < -\epsilon \mid \tilde{X}) \geq t/2) \rightarrow 0. \nonumber
\end{align}
Therefore, 
\begin{align}
    \lim_{m \rightarrow \infty} \lim_{n \rightarrow \infty} \p(\p(\tilde{Y} \in \hat{C}(\tilde{X}) \mid \tilde{X}) \leq 1-\alpha -t) = 0.
\end{align}
\end{proof}

\newpage

\section{Conformal Off-Policy Prediction (COPP)}

\subsection{Further comments on the differences between \cite{lei2020conformal} and COPP}\label{sec:comp_lc}
In this subsection, we elaborate on the differences between our work and \cite{lei2020conformal}.


Firstly, \cite{lei2020conformal} consider a setup in which the distribution of $X$ is shifted, and construct intervals on the outcome under a specific (deterministic) action, i.e.\ $Y(a)$. In contrast, we consider a setup in which the distribution of $Y|X$ is shifted due to a change in the policy which is non trivial, and construct bounds on the outcome under this new policy (which could be stochastic). Additionally, since the theory in our methodology relies on the ratio of the joint distribution $P_{X,Y}$, our framework can be straightforwardly extended to the case where both, the conditional $P_{Y|X}$ and the covariate distribution $P_X$ shift.

Secondly, as already mentioned in section \ref{sec:related_work}, \cite{lei2020conformal} can only be applied to the case where we have a deterministic target policy and a discrete action space, whereas COPP generalizes to the stochastic policy and continuous action space. This limitation of \cite{lei2020conformal} can be partially addressed by employing the ``\textit{union method}'' as described in the main text, which consists of constructing CP intervals for each action separately before taking the union of the intervals. However, we showed in our experiments that this leads to overly conservative intervals i.e. coverage above the required $1-\alpha$ in Table \ref{tab:coverage_toy}. This is because the predictive interval does not depend on the target policy, since every action is treated identically when taking the union. This approach is moreover unsuitable for continuous action spaces, whereas COPP applies without modification.

Thirdly, as stated in in section \ref{sec:related_work}, even in the case when we only consider deterministic target policies, there is an important methodological difference between COPP and \cite{lei2020conformal}. \cite{lei2020conformal} construct the intervals on $Y(a)$ by only using calibration data with $A=a$ (see eq. 3.4 in \cite{lei2020conformal}). In contrast, it can be shown that COPP uses the entire calibration data when constructing intervals on $Y(a)$. This is a consequence of integrating out the actions in the weights $w(x, y)$ (sec \ref{sec:weights}). This empirically leads to smaller variance in coverage compared to \cite{lei2020conformal} as evidenced by the experimental results in \ref{subsec:comp_lc}.

Finally, in our paper we are \emph{not} interested in a linear combination of the $Y(a)$ as in \cite{lei2020conformal}, who consider the linear combination of the form $Y(1)-Y(0)$. Instead, as described in section \ref{sec:problem_setup}, we are interested in the outcome $Y$ under the new target policy $\pi^*$ (sometimes denoted as $Y(\pi^*)$ in the literature), which cannot be expressed as a linear combination of $Y(a)$. As a result, there does not appear to be a straightforward application of \cite[Section 4.3]{lei2020conformal} to our setup which relies on the linear combination assumption to be applicable.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.45\textwidth, height=0.3\textwidth]{figures/copp/outcome_1.png}
    \includegraphics[width=0.45\textwidth, height=0.3\textwidth]{figures/copp/outcome_2.png}\\
    \includegraphics[width=0.45\textwidth, height=0.3\textwidth]{figures/copp/outcome_3.png}
    \includegraphics[width=0.45\textwidth, height=0.3\textwidth]{figures/copp/outcome_4.png}
    \caption{Results for synthetic data experiment with $\pi^b = \pi_{0.3}$ and deterministic target policies.}
    \label{fig:comp_lc}
\end{figure*}
\subsection{Comparison with \cite{lei2020conformal} on deterministic target policies.} \label{subsec:comp_lc}
In order to further clarify the distinction between COPP and \cite{lei2020conformal}, we conducted additional experiments when the target policy is deterministic i.e. $\pi^*(A|X) = \mathbbm{1}\{A=a\}$. In the main text we modified \cite{lei2020conformal} to our setting of stochastic policies by constructing the conformal intervals through the union of the CP sets across the actions. Here we aim to apply COPP to the setting of \cite{lei2020conformal}, i.e. deterministic target policy.

As mentioned in in the main text, given that we are integrating out the action in Eq. \ref{weight-est}, we are essentially able to use the full dataset when constructing the CP intervals. To see this explicitly, consider the case where $Y \mid X, A$ is a normal random variable (as in our toy experiment). In this case, it can be straightforwardly shown that the weights $w(x_i, y_i)$ will be non-zero, and therefore, when constructing the COPP intervals using \eqref{score-dist-pshift}, we are able to use all the calibration datapoints.

This is contrary to \cite{lei2020conformal}, who only consider calibration data with $A=a$, when constructing the CP intervals for $Y(a)$. Below, we use the same experimental setup as our toy experiment in section \ref{sec:exp_toy} (see section \ref{sec:toy_experiments_descrip} for more details) with the difference here that we now consider deterministic target policies. In figure \ref{fig:comp_lc} we plot the coverage for given deterministic target policies against the number of calibration datapoints. In this figure, we refer to the methodology of \cite{lei2020conformal} as \emph{LC21}. Here, we use the behavioural policy $\pi_{0.3}$ and a deterministic target policy which takes a single fixed action $a \in \{1, 2, 3, 4\}$ at test time. In the title of each subfigure, $Y(a)$ corresponds to the outcome for the target policy $\pi^*(A=a\mid X) = \mathbbm{1}(A=a)$.

\textbf{Results}: We first note in Figure \ref{fig:comp_lc} that the coverage of COPP intervals has a lower variance than \cite{lei2020conformal}. Given that COPP is able to use all the data when constructing the CP intervals, as opposed to \cite{lei2020conformal} which only uses a subset, our bounds have lower variance while also attaining the coverage guarantees. We observe this difference particularly in the case when we have little calibration data. Given that \cite{lei2020conformal} have to split the data into $4$ different splits (we have 4 different actions), the calibration data for each action is relatively small, whereas we are able to use the whole dataset to construct our CP intervals.


\subsection{Motivation of using stochastic policies for bandits}
One of the key difference between our method and that of \cite{lei2020conformal} is that our method can be applied to the setting where the target policy is stochastic. In many settings, deterministic target policies might not be applicable such as in the settings of recommendation systems or RL where exploration is needed \citep{swaminathan2016off, su2020doubly}. For example, COPP can be used to compare different recommendation systems given some logged data. We explore this application in our MSR experiments where the target policies correspond to different recommendation systems which are, by default, stochastic. Other applications which also make use of stochastic policies bandit problems can be found in \cite{su2020doubly, farajtabar2018more}.



%
%
%
%

%
%

%

%
%
%

%


%







\subsection{COPP for Group-balanced coverage}\label{sec:grp-bal}
As \cite{conf-bates} point out, we may want predictive intervals that have same error rates across multiple different groups. Using our example of a recommendation system, we may want the predictive intervals to have same coverage across male and female users. 

Formally, this problem can be expressed as follows. Let $\Omega = \{\Omega_1, \cdots, \Omega_k \}$ be subsets of $\mathcal{X} \times \mathcal{Y}$ with $\tarprob((X,Y) \in \Omega_j) > 0$ for $j\in \{1, \dots, k\}$. We would like to construct  predictive intervals $\hat{C}_n^\Omega$ which satisfy 
\begin{align}
    \tarprob(Y \in \hat{C}_n^\Omega(X) \mid (X, Y) \in  \Omega_j) \geq 1-\alpha \hspace{0.2cm} \textup{for all $j\in \{1, \dots, k\}$.} \nonumber
\end{align}
CP offers us the ability to construct such intervals $\hat{C}_n^\Omega$, by simply running algorithm \ref{cp_covariate_shift} (main text) on each group separately. This has been visualized in figure \ref{fig:grps}. 

\begin{figure}[!htp]
    \centering
    \includegraphics[height=0.25\textwidth]{figures/copp/grp.png}
    \caption{Figure taken from \cite{conf-bates}. To achieve group-balanced coverage, we simply run conformal prediction separately on each group.}
    \label{fig:grps}
\end{figure}
Formally, this procedure can be described as follows. We group scores into different groups according to each subset.
\begin{align}
    \{(X_i^{\Omega_j}, Y_i^{\Omega_j})\}_{i = 1}^{n_j} &\coloneqq \{(X_i, Y_i): (X_i, Y_i) \in \Omega_j\}_{i = 1}^{n} \hspace{0.2cm} \textup{and,} \nonumber \\
    V_i^{\Omega_j} &\coloneqq (X_i^{\Omega_j}, Y_i^{\Omega_j}) \nonumber
\end{align}
Then, within each subset, we calculate the conformal quantile, 
\begin{align}
    \eta^{\Omega_j}(x, y) \coloneqq \textup{Quantile}_{1-\alpha}( \hat{F}^{\Omega_j}_n(x, y)) \nonumber
\end{align}
where,
\begin{align}
    \hat{F}^{\Omega_j}_n(x, y) &\coloneqq \sum_{i=1}^{n_j} p_i^{\Omega_j} (x, y)\delta_{V_i^{\Omega_j}} + p_{n+1}^{\Omega_j}(x,y)\delta_\infty  \hspace{0.2cm} \textup{where,} \nonumber \\
    p_i^{\Omega_j} (x, y) &\coloneqq \frac{w(X^{\Omega_j}_i, Y^{\Omega_j}_i)}{\sum_{i=1}^{n_j} w(X^{\Omega_j}_i, Y^{\Omega_j}_i) + w(x,y)} \nonumber\\
    p_{n+1}^{\Omega_j} (x, y) &\coloneqq \frac{w(x,y)}{\sum_{i=1}^{n_j} w(X^{\Omega_j}_i, Y^{\Omega_j}_i) + w(x,y)} \nonumber
\end{align}
Next, we construct the set $\hat{C}_n^{\Omega}$ as follows:
\begin{align}
    \hat{C}_n^{\Omega}(x^{test}) &\coloneqq \bigcup_{j=1}^k \hat{C}_n^{\Omega_j}(x^{test}) \hspace{0.2cm} \textup{where,} \nonumber \\
    \hat{C}_n^{\Omega_j}(x^{test}) &\coloneqq \{y:  (x^{test}, y) \in \Omega_j \textup{ and } s(x^{test},y) \leq \eta^{\Omega_j}(x^{test}, y)  \}. \nonumber \\
\end{align}
\begin{proposition}[Coverage guarantee for class-balanced conformal prediction]\label{prop:grp-balanced-cp}
Let $\Omega = \{\Omega_1, \cdots, \Omega_k \}$ be subsets of $\mathcal{X} \times \mathcal{Y}$ with $\tarprob((X,Y) \in \Omega_j) > 0$ for $j\in \{1, \dots, k\}$. Then, the set $\hat{C}_n^{\Omega}$ defined above satisfies the coverage guarantee 
\begin{align}
    \tarprob(Y \in \hat{C}_n^\Omega(X) \mid (X, Y) \in  \Omega_j) \geq 1-\alpha \hspace{0.2cm} \textup{for all $j\in \{1, \dots, k\}$.} \nonumber
\end{align}
\end{proposition}

\paragraph{Proof of Proposition \ref{prop:grp-balanced-cp}}

\begin{align}
    &\tarprob(Y \in \hat{C}_n^\Omega(X) \mid (X, Y) \in  \Omega_j) \nonumber\\
    &\geq \tarprob(Y \in \hat{C}_n^{\Omega_j}(X) \mid (X, Y) \in  \Omega_j) \nonumber \\
    &\geq \tarprob( (X, Y) \in \Omega_j: s(X,Y) \leq  \eta^{\Omega_j}(X, Y) \mid (X, Y) \in \Omega_j) \label{eq:prop-gp-bal}  
\end{align}
Define the measure $P^{j}_{X, Y}$ by restricting $P^{\pi^*}_{X, Y}$ to $\Omega_j$, i.e.
\[
P^j_{X, Y}(x, y) \propto P^{\pi^*}_{X, Y}(x, y) \mathbbm{1}((x, y) \in \Omega_j)
\]
Then, \eqref{eq:prop-gp-bal} can be written as
\begin{align}
    \eqref{eq:prop-gp-bal} =& \p_{(X, Y) \sim P^{j}_{X, Y}}(s(X,Y) \leq  \eta^{\Omega_j}(X, Y)) \label{eq1:prop-gp-bal}
\end{align}
Moreover, for $(x, y) \in \Omega_j$ we have
\begin{align}
    w(x, y) = \frac{P^{\pi^*}_{X, Y}(x, y)}{P^{\pi^b}_{X, Y}(x, y)} \propto \frac{P^{j}_{X, Y}(x, y)}{P^{\pi^b}_{X, Y}(x, y)} \nonumber
\end{align}
Since $p_i^{\Omega_j}(x, y)$ is invariant to scaling of weights $w(x, y)$, replacing the weights by $\tilde{w}(x, y) = \frac{P^{j}_{X, Y}(x, y)}{P^{\pi^b}_{X, Y}(x, y)}$ keeps the conformal sets unchanged. 

Therefore, using Proposition \ref{coverage_theorem}, the conformal sets constructed will provide coverage guarantees under the measure $P^{j}_{X, Y}$, i.e.
\begin{align}
    \p_{(X, Y) \sim P^{j}_{X, Y}}(s(X,Y) \leq  \eta^{\Omega_j}(X, Y)) \geq 1-\alpha \nonumber
\end{align}
Using \eqref{eq1:prop-gp-bal}, we get that
\begin{align}
    \tarprob(Y \in \hat{C}_n^\Omega(X) \mid (X, Y) \in  \Omega_j) \geq \p_{(X, Y) \sim P^{j}_{X, Y}}(s(X,Y) \leq  \eta^{\Omega_j}(X, Y)) \geq 1-\alpha \nonumber
\end{align}
\qed

\subsubsection{COPP for class-balanced coverage}
\begin{algorithm}
\SetAlgoLined
\textbf{Inputs:} Observational data $\mathcal{D}_{obs}=\{X_i, A_i, Y_i\}_{i=1}^{n_{obs}}$, conf. level $\alpha$, a score function $s(x,y)\in\mathbb{R}$, new data point $x^{test}$, target policy $\pi^*$ \;
\textbf{Output:} $\hat{C}^{\mathcal{Y}}_n(x^{test})$ with coverage guarantee \eqref{label_cond}\;
Split $\mathcal{D}_{obs}$ into training data ($\mathcal{D}_{tr}$) and calibration data ($\mathcal{D}_{cal}$) of sizes $m$ and $n$ respectively\;
Use $\mathcal{D}_{tr}$ to estimate weights $\hat{w}(\cdot, \cdot)$\;
\For{$y \in\mathcal{Y}$}{
Let $\{X_j^y, Y_j^y\}_{j=1}^{n_y}$ be the following subset of calibration data: $\{(X_i, Y_i): Y_i = y\}$\;
Let $V_j^y \coloneqq s(X_j^y, Y_j^y)$, for $j = 1, \dots, n_y$\;
Define $\hat{F}_{n}^{x, y} = \sum_{i=1}^{n_y} p_i^w(x, y)\delta_{V^y_i} + p_{n+1}^w(x,y)\delta_\infty$\;
where, $p_{i}^w(x, y) \coloneqq \frac{w(X^y_i, Y^y_i)}{\sum_{i=1}^{n_y} w(X^y_i, Y^y_i) + w(x,y)}$, $p_{n+1}^w(x, y) \hspace{-0.1cm} \coloneqq \hspace{-0.1cm} \frac{w(x,y)}{\sum_{i=1}^{n_y} w(X^y_i, Y^y_i) + w(x,y)}$\;
$\eta(x, y) \coloneqq \text{Quantile}_{1-\alpha}( \hat{F}_{n}^{x, y})$
%
}
Define $\hat{C}^{\mathcal{Y}}_n(x^{test}) \coloneqq  \{y: s(x^{test},y) \leq \eta(x^{test}, y) \} $\;
\textbf{Return} $\hat{C}^{\mathcal{Y}}_n(x^{test})$
\caption{COPP for class-balanced coverage}
  \label{cp_label_conditioned}
\end{algorithm}
In the case when $Y$ is discrete, we construct predictive sets, $\hat{C}^{\mathcal{Y}}_n(x)$, which offer label conditioned coverage guarantees using the methodology described above,
\begin{align}
    \tarprob(Y \in \hat{C}^{\mathcal{Y}}_n(X) \mid Y = y) \geq 1- \alpha, \hspace{0.2cm} \textup{for all $y\in \mathcal{Y}$} \label{label_cond}
\end{align}
This is a strictly stronger guarantee than marginal coverage, i.e. $\tarprob(Y \in \hat{C}_n(X)) \geq 1- \alpha$. To understand what \eqref{label_cond} means, consider our running example of recommendation systems, where the outcome $Y$ is whether the recommendation is relevant (0) or not (1) to the user. Then, Eq. \eqref{label_cond} ensures that out of the users who received irrelevant recommendations, the predictive sets contain `not relevant' (1) at least $100\cdot(1-\alpha)\%$ of the times. This can be thought of as controlling the false negative rate of irrelevant recommendations at $100\cdot\alpha\%$. The same is true for users who receive relevant recommendations. This is particularly useful when data is imbalanced, for example, when the majority of the users in observational receive relevant recommendations. 
\subsection{Weights estimation $\hat{w}(x, y)$}\label{sec:weights_estimation_app}
\subsubsection{Consistent estimation of the weights does not imply consistent estimation of $\hat{P}(y| x, a)$}
In Proposition \ref{coverage_theorem}, we assume to have consistent estimator of $w(x, y)$ which begs the following question: In general, does a consistent estimate of $w(x, y)$ imply that we also obtain a consistent estimate of $P(y|x, a)$? In particular, one could then just use the estimate of $\hat{P}(y|x, a)$ to construct the predictive interval. However, we answer the above question with the negative by supplying a counter-example. 


\paragraph{Counter-example}
Let $X \in [1, + \infty), a\in \mathbbm{R} \textup{ s.t. } |a| < K$ for $K \in \mathbbm{R}_{>0}$.

Let $Y|X, a \sim \mathcal{N}((KX^2+a)^{0.5}, (KX^2-a))$.

We have $\mathbbm{E}[Y^2|X, a] = Var(Y|X, a) + \mathbbm{E}[Y|X, a]^2 = KX^2 +a + KX^2 -a = 2KX^2$ (independent of $a$)

Next let 
\begin{align}
  \hat{P}(y|x, a) \coloneqq \frac{y^2P(y|x, a)}{2Kx^2}. \label{def:pyxhat} 
\end{align}
Recall that 
\begin{align}
    w(x, y) = \frac{\int P(y|x, a) \pi^*(a|x) \mathrm{d}a}{\int P(y| x, a) \pi^b(a|x) \mathrm{d}a}
\end{align}
Using the above definition of $\hat{P}(y|x, a)$ we have:
\begin{align}
    \hat{w}(x, y) &= \frac{\int \hat{P}(y|x, a) \pi^*(a|x) \mathrm{d}a}{\int \hat{P}(y| x, a) \pi^b(a|x) \mathrm{d}a} \nonumber\\ 
     &= \frac{\int P(y|x, a) \frac{Y^2}{2KX^2} \pi^*(a|x) \mathrm{d}a}{\int P(y| x, a) \frac{Y^2}{2KX^2} \pi^b(a|x) \mathrm{d}a} \nonumber\\ 
    &= w(x, y).\nonumber
\end{align}
Hence, $w(x, y) \equiv \hat{w}(x, y) \centernot\implies \hat{P}(y|x, a) \equiv P(y|x, a)$.
\qed 

More generally, if there exists a function $\Phi: \mathcal{X}\times \mathcal{Y} \rightarrow \mathbb{R}_{\geq0}$ such that 
\begin{enumerate}
    \item $\Phi(x, y)$ is not constant in $y$
    \item $0<\E[\Phi(X,Y) \mid X, A]< \infty$, and does not depend on $A$
\end{enumerate}
Then, we can define $\tilde{P}(y|x, a) \coloneqq P(y|x, a) \Phi(x, y)/\E[\Phi(X,Y) \mid X, A]$, and the weights computed using $\tilde{P}(y|x, a)$ will be the equal to $w(x, y)$ even though $\tilde{P}(y|x, a) \ne P(y|x, a)$.
\subsubsection{Alternative ways to estimate $\hat{w}(x, y)$ without estimating $\hat{P}(y| x, a)$}\label{sec:alternate_weights_est}
%
%
%
In this section, we show how we could estimate $w(x, y)$ without having to estimate $\hat{P}(y|x, a)$. One way to obtain an estimate $\hat{w}(x, y)$ is by taking a closer look at the definition of $w(x, y)$ and rewriting the ratio.
\begin{align}
    w(x,y)&=\frac{P_{X, Y}^{\pi^*}(x,y)}{P_{X, Y}^{\pi^b}(x,y)} \nonumber\\
    &=\int \frac{P_{X,A, Y}^{\pi^*}(x,a,y)}{P_{X,A, Y}^{\pi^b}(x,a,y)}P_{A|X,Y}^{\pi^b}(a|x,y)\mathrm{d}a \nonumber \\
    &=\int \frac{\pi^{\ast}(a|x)}{\pi^b (a|x)}P_{A|X,Y}^{\pi^b}(a|x,y)\mathrm{d}a \nonumber\\
    &= \E_{A \sim P^{\pi^b}_{A\mid X=x,Y=y}}\Big[ \frac{\pi^{\ast}(A|x)}{\pi^b (A|x)} \Big]. \label{eq:ratioidentity}
\end{align}

\begin{lemma}\label{prop:weights-est}
Let $w(x,y)=\frac{P_{X, Y}^{\pi^*}(x,y)}{P_{X, Y}^{\pi^b}(x,y)}$, then
\begin{align}
    w(x,y) = \arg\min_{f} \mathbb{E}_{X,A,Y \sim P^{\pi^b}_{X,A,Y}} \Big[\Big|\Big|\frac{\pi^{\ast}(A|X)}{\pi^b (A|X)}-f(X,Y)\Big|\Big|^2\Big]. \label{eq:weights-obj}
\end{align}
\end{lemma}
\paragraph{Proof of Lemma \ref{prop:weights-est}}
This follows directly from the identity (\ref{eq:ratioidentity}). We prove it here for sake of completeness.
\begin{align}
    &\mathbb{E}_{X,A,Y \sim P^{\pi^b}_{X,A,Y}} \Big[\Big|\Big|\frac{\pi^{\ast}(A|X)}{\pi^b (A|X)}-f(X,Y)\Big|\Big|^2\Big] \nonumber \\
    &= \mathbb{E}_{X,Y \sim P^{\pi^b}_{X,Y}} \Big[\E_{A \sim P^{\pi^b}_{A\mid X,Y}} \Big|\Big|\frac{\pi^{\ast}(A|X)}{\pi^b (A|X)}-f(X,Y)\Big|\Big|^2\Big] \nonumber \\
    &= \mathbb{E}_{X,Y \sim P^{\pi^b}_{X,Y}} \Big[\textup{Var}_{A \sim P^{\pi^b}_{A\mid X,Y}}\Big[ \frac{\pi^{\ast}(A|X)}{\pi^b (A|X)} \Big] + \left(\E_{A \sim P^{\pi^b}_{A\mid X,Y}}\Big[ \frac{\pi^{\ast}(A|X)}{\pi^b (A|X)} \Big] - f(X,Y) \right)^2 \Big].
     \label{eq:w-reg}
\end{align}
Where, \eqref{eq:w-reg} is minimized if $f(x, y) = \E_{A \sim P^{\pi^b}_{A\mid X=x,Y=y}}\Big[ \frac{\pi^{\ast}(A|x)}{\pi^b (A|x)} \Big] = w(x,y)$.
\qed

Using Lemma \ref{prop:weights-est}, we can thus approximate $w(x,y)$ by minimizing the loss
\begin{align}
    \hat{w}(x, y) =\arg \min_{f_\theta} \mathbb{E}_{X,A,Y \sim P^{\pi^b}_{X,A,Y}} \Big[\Big|\Big|\frac{\pi^{\ast}(A|X)}{\pi^b (A|X)}-f_\theta(X,Y)\Big|\Big|^2\Big] \label{eq:weights-loss}
\end{align}
Hence we see that the ratio estimation problem can be rewritten as a regression problem where $f_\theta(x,y)$ is for example a neural network. This allows one to estimate directly, without the need for estimating $\hat{P}(y\mid x, a)$ first.

%
%

%
%
%
%
%
%
%
%
%
%
%
%
%

%

%


%
%
%
%
%
%
%
%
%
%
%
%

\newpage
\section{Estimation of the quantiles of the target distribution}\label{sec:estimating_target_quantiles}
As mentioned in Section \ref{sec:cond_cov}, we present here a way to estimate the quantiles of the target distribution $P_{X,Y}^{\pi^*}$ consistently when the ground truth weight function $w(x, y)$ is known. As we are interested in the quantiles, we will be using the pinball loss to train our model $\hat{f}_{\theta}$ defined by
\begin{align}
    L_{\alpha}(\theta, x, y) = \begin{cases}
         \alpha (\hat{f}_{\theta}(x)-y) \qquad &\text{ if } (\hat{f}_{\theta}(x)-y) > 0, \\
         (1-\alpha) (y -\hat{f}_{\theta}(x)) \qquad &\text{ if } (\hat{f}_{\theta}(x)-y) < 0.
    \end{cases} \nonumber
\end{align}
Then we have the following objective to optimize:
\begin{align}
\expt[L_\alpha(\theta, X, Y)] &= \int_{X,Y} L_\alpha(\theta, x, y) P_{X,Y}^{\pi^*}(\mathrm{d}x,\mathrm{d}y) \nonumber \\
&= \int_{X,Y} L_\alpha(\theta, x, y) \frac{\mathrm{d}P_{X, Y}^{\pi^*}(x, y)}{\mathrm{d}P_{X,Y}^{\pi^b}(x, y)} P_{X,Y}^{\pi^b}(\mathrm{d}x,\mathrm{d} y) \nonumber \\
&= \int_{X,Y} L_\alpha(\theta, x, y) w(x, y) P_{X,Y}^{\pi^b}(\mathrm{d}x,\mathrm{d}y)\nonumber\\
&= \expb[L_\alpha(\theta, X, Y) w(X, Y)]. \nonumber
\end{align}
The above holds true if the true weight function is known. However in the case where we only have a consistent estimator of $w(x, y)$, it remains to be proven that the above objective will also yield a consistent estimator of the quantiles under $\pi^*$. We leave this for future work to prove as we are simply providing a possible avenue to relax the assumptions in Proposition \ref{sec:cond_cov}. 



\newpage
\section{Experiments}\label{sec:exps_app}
The code for our experiments is available at \url{https://anonymous.4open.science/r/COPP-75F5} and we ran all our experiments on Intel(R) Xeon(R) CPU E5-2690 v4 @ 2.60GHz with 8GB RAM per core. We were able to use 100 CPUs in parallel to iterate over different configurations and seeds. However, we would like to note that our algorithms only requires 1 CPU and at most 10 mins to run, as our networks are relatively small.
\subsection{Toy Experiment}\label{sec:toy_experiments_descrip}
\subsubsection{Synthetic data experiments setup}
\paragraph{Model.}
The observational data distribution is defined as follows:
\begin{align}
    & X_i \overset{\textup{i.i.d.}}{\sim} \mathcal{N}(0,9) \nonumber \\
    & A_i \mid x_i \sim \pi^b(\cdot \mid x_i) \hspace{0.2cm} \textup{where $\pi^b$ has been defined below} \nonumber \\
    & Y_i \mid x_i, a_i \sim \mathcal{N}(a_i * x_i, 1) \nonumber
\end{align}

\paragraph{Behaviour and Target Policies.}
We define a family of policies $\pi_\epsilon(a \mid x)$ as follows:
\begin{align}
&\pi_\epsilon(a|x) \coloneqq
     \begin{cases}
          \epsilon\mathbbm{1}(a \in \{1,2,3\}) + (1-3\epsilon)\mathbbm{1}(a=4) &  \textup{if } |x|\in (3, \infty)\\
          \epsilon\mathbbm{1}(a \in \{1,2,4\}) + (1-3\epsilon)\mathbbm{1}(a=3) & \textup{if } |x|\in (2, 3]\\
          \epsilon\mathbbm{1}(a \in \{1,3,4\}) + (1-3\epsilon)\mathbbm{1}(a=2) & \textup{if } |x|\in (1, 2]\\
          \epsilon\mathbbm{1}(a \in \{2,3,4\}) + (1-3\epsilon)\mathbbm{1}(a=1) & \textup{if } |x|\in [0, 1]\\
          \end{cases} \nonumber
\end{align}
We use the parameter $\epsilon \in (0,1/3)$ to control the policy shift between target and behaviour policies. For the behaviour policy $\pi^b$, we use $\epsilon^b = 0.3$, and for target policies $\pi^*$, we use $\epsilon^* \in \{0.1, 0.2, 0.3\}$. Here we use $m=1000$ training datapoints.

\paragraph{Neural Network Architectures}
\begin{itemize}
    \item To approximate the behaviour policy $\pi^b$, we use a neural network with 2 hidden layers and 16 nodes in each hidden layer, and ReLU activation function.
    \item To approximate $P(y|x, a)$, we use $\mathcal{N}(\mu(x, a), \sigma(x, a))$, where $\mu$ and $\sigma$ are neural networks with one-hidden layer, 32 nodes in the hidden layer, and ReLU activation function.
    \item For the score function, we train the quantiles $\hat{q}_{\alpha/2}$ and $\hat{q}_{1 - \alpha/2}$ using quantile regression, each of which are modelled using neural networks with one-hidden layer, 32 nodes in the hidden layer, and ReLU activation functions.
\end{itemize}

\paragraph{Results: Coverage as a function of increase calibration data}\label{app:N-cal_exp_toy}
As mentioned in the main text, we have also performed experiments to investigate how much calibration data is needed for COPP as well as other methods to converge to the required $90\%$ coverage. In the below figure \ref{fig:Toy_GT} we have plotted the coverage as a function of $n$ calibration data points. Our proposed method is converging much faster to the required coverage compared to the competing methods.

\begin{figure*}[htp!]
    \centering
    \includegraphics[width=0.45\textwidth, height=0.3\textwidth]{figures/copp/cov-n_cal.png}
    \includegraphics[width=0.45\textwidth, height=0.3\textwidth]{figures/copp/ncal-abl.png}
    \caption{Results for synthetic data experiment with $\pi^b = \pi_{0.3}$ and the target policy is $\pi^* = \pi_{0.1}$. \textbf{Left:} our proposed method is able to converge to the required coverage rather quickly compared to the competing methods. \textbf{Right:} here we see that our method is on par with using the GT weights. Due to estimation error, COPP with estimated weights has slightly higher variance in terms of coverage}
    \label{fig:Toy_GT}
\end{figure*}

\paragraph{Additional experimental baseline using weighted quantile regression.}
In order to add an additional baseline that is also covariate dependent, we have added some experiments using the weighted quantile regression (WQR) as described in Sec. \ref{sec:estimating_target_quantiles} on our toy experiments from Sec. \ref{sec:exp} in the main text. Below in Table \ref{tab:coverage_toy_app} and Table \ref{tab:length_toy_app} we see the complete coverage table with the respective interval lengths. Note also that WQR does not seem to perform well as it does not have any statistical guarantees and heavily relies on good estimation of the ratio. We have added these experiments here in the appendix for completeness and did not add it in the main text as the results were not comparable to other baselines.

\begin{table}[t]
      \centering
      \caption{Mean Coverage as a function of policy shift with 2 standard errors over 10 runs. We have added weighted quantile regression (WQR) for completeness and note that it does not seem to perform well.}\label{tab:coverage_toy_app}
      \resizebox{0.7\columnwidth}{!}{%
        \begin{tabular}{lccc}
\toprule
Coverage &  $\Delta_{\epsilon}=0.0$ &  $\Delta_{\epsilon}=0.1$ &  $\Delta_{\epsilon}=0.2$ \\
\midrule
COPP (Ours)            &                    \textbf{0.90 $\pm$ 0.01}&                    \textbf{0.90 $\pm$ 0.01}&                    \textbf{0.91 $\pm$ 0.01}\\
WIS                  &                    \textbf{0.89 $\pm$ 0.01}&                     \textbf{0.91 $\pm$ 0.02}&                     0.94 $\pm$ 0.02\\
SBA                  &                     \textbf{0.90 $\pm$ 0.01}&                     0.88 $\pm$ 0.01&                     0.87 $\pm$ 0.01\\
\midrule
\midrule
COPP (GT weights Ours)      &                     \textbf{0.90 $\pm$ 0.01}&                     \textbf{0.90 $\pm$ 0.01}&                     \textbf{0.90 $\pm$ 0.01}\\
CP (no policy shift) &                     \textbf{0.90 $\pm$ 0.01}&                     0.87 $\pm$ 0.01&                     0.85 $\pm$ 0.01\\
CP (union) &                      0.96 $\pm$ 0.01 &         0.96 $\pm$ 0.01 &         0.96 $\pm$ 0.01 \\
\red{WQR}          &         \red{0.82 $\pm$ 0.04} &         \red{0.76 $\pm$ 0.03} &          \red{0.70 $\pm$ 0.03} \\
\bottomrule
\end{tabular}
}
\end{table}



\begin{table}[h!]
      \centering
      \caption{Mean Interval Length as a function of policy shift with 2 standard errors over 10 runs. We have added weighted quantile regression (WQR) for completeness and note that it does not seem to perform well.}\label{tab:length_toy_app}
      \resizebox{0.7\columnwidth}{!}{%
        \begin{tabular}{lccc}
\toprule
Interval Lengths &  $\Delta_{\epsilon}=0.0$ &  $\Delta_{\epsilon}=0.1$ &  $\Delta_{\epsilon}=0.2$ \\
\midrule
COPP (Ours)           &                     9.08 $\pm$ 0.10&                     9.48 $\pm$ 0.22&                     9.97 $\pm$ 0.38\\
WIS                  &                    \red{24.14 $\pm$ 0.30}&               \red{32.96 $\pm$ 1.80}&             \red{43.12 $\pm$ 3.49}\\
SBA                  &                     8.78 $\pm$ 0.12&                     8.94 $\pm$ 0.10&                     8.33 $\pm$ 0.09\\
\midrule
\midrule
COPP (GT weights Ours)      &                     8.91 $\pm$ 0.09&                     9.25 $\pm$ 0.12&                     9.59 $\pm$ 0.20\\
CP (no policy shift) &                     9.00 $\pm$ 0.10&                     9.00 $\pm$ 0.10&                     9.00 $\pm$ 0.10\\
CP (union) &                     10.66 $\pm$ 0.18 &         11.04 $\pm$ 0.2 &         11.4 $\pm$ 0.26 \\
\red{WQR}         &         \red{8.55 $\pm$ 0.50} &         \red{8.61 $\pm$ 0.52} &          \red{8.70 $\pm$ 0.55} \\
\bottomrule
\end{tabular}
}
\end{table}







\newpage
\subsubsection{Experiments with continuous action space}\label{subsec:cts_act}
As mentioned in the main text and also in Sec. \ref{sec:comp_lc}, our proposed method, contrary to the work of \cite{lei2020conformal} is able to also handle continuous action space. Given that we are integrating out the actions when computing the weights in Eq. \ref{weight-est} our method trivially extends to the continuous action space, whereas \cite{lei2020conformal} is only applicable for discrete action spaces, as they compute conformal intervals conditioned on a given action.

\paragraph{Model.}
The observational data distribution is defined as follows:
\begin{align}
    & X_i \overset{\textup{i.i.d.}}{\sim} \mathcal{N}(0,4) \nonumber \\
    & A_i \mid x_i \sim \mathcal{N}(x_i/4, 1) \hspace{0.2cm} \nonumber \\
    & Y_i \mid x_i, a_i \sim \mathcal{N}(a_i + x_i, 1) \nonumber
\end{align}
\paragraph{Target Policies.}
We define a family of policies $\pi_\epsilon(a \mid x)$ as follows:
\begin{align}
    \pi_\epsilon(a \mid x) = \mathcal{N}(x/4 + \epsilon, 1). \label{tar_pols}
\end{align}
In our experiments, for the target policy $\pi^*$, we use $\pi^* = \pi_{\epsilon^*}$ for $\epsilon^* \in \{0, 0.5, 1, 1.5, 2, 2.5\}$.

\paragraph{Results.}
Table \ref{tab:cov_cts} shows the coverages of different methods as the policy shift $\epsilon^*$ increases. The behaviour policy $\pi^b = \pi_{0}$ is fixed and we use $n=5000$ calibration datapoints and $m=1000$ training points, across 10 runs. Table \ref{tab:cov_cts} shows, how COPP stays very close to the required coverage of $90\%$ across all target policies with $\epsilon^* \leq 2.0$, compared to WIS and SBA. Both, WIS intervals and SBA intervals suffer from under-coverage i.e. below the required coverage. These results again support our hypothesis from Sec. \ref{sec:weights}, which stated that COPP is less sensitive to estimation errors of $\hat{P}(y|x, a)$ compared to directly using $\hat{P}(y|x, a)$ for the intervals i.e. SBA.

Next, Table \ref{tab:len_cts} shows the mean interval lengths and even though WIS intervals are under-covered, the average interval length is huge compared to COPP. Additionally, for $\epsilon^* \in \{0, 0.5, 1, 1.5\}$, COPP with estimated weights produces results which are close to COPP intervals with ground truth weights. This shows that when the behaviour and target policies have reasonable overlap, the effect of weights estimation error on COPP results is limited. However, as $\epsilon^*$ increases to $2.0$ and $2.5$, the overlap between behaviour and target policies becomes low. We empirically note that this leads to high weights estimation error and consequently under-coverage in COPP with estimated weights. In contrast, COPP with ground truth weights still achieves required coverage, even though it becomes conservative when the overlap is low. Figure \ref{fig:pols_cts_acs} visualises how the overlap between target and behaviour policies decreases with increasing $\epsilon^*$. It can be seen that $\epsilon^* \in \{2, 2.5\}$ leads to very low overlap between the behaviour and target data.



\begin{table}[htp!]
\begin{center}
\caption{Mean Coverage as a function of policy shift with 2 standard errors over 10 runs.}
\label{tab:cov_cts}
\resizebox{0.9\columnwidth}{!}{
\begin{tabular}{lllllll}
\toprule
Coverage &              $\epsilon^*=0.0$ &              $\epsilon^*=0.5$ &              $\epsilon^*=1.0$ &              $\epsilon^*=1.5$ &              $\epsilon^*=2.0$ &              $\epsilon^*=2.5$ \\
\midrule
COPP (Ours)                   &   \textbf{0.90 $\pm$ 0.01} &  \textbf{0.91 $\pm$ 0.01} &  0.92 $\pm$ 0.01 &  \textbf{0.91 $\pm$ 0.01} &  \textbf{0.89 $\pm$ 0.02} &  0.85 $\pm$ 0.02 \\
WIS                           &  0.87 $\pm$ 0.01 &  0.87 $\pm$ 0.01 &  0.87 $\pm$ 0.01 &  0.87 $\pm$ 0.02 &  \textbf{0.89 $\pm$ 0.02} &  0.83 $\pm$ 0.02 \\
SBA                           &  0.86 $\pm$ 0.01 &  0.86 $\pm$ 0.01 &  0.86 $\pm$ 0.01 &  0.86 $\pm$ 0.01 &  \textbf{0.89 $\pm$ 0.02} &  0.83 $\pm$ 0.02 \\
\midrule
\midrule
COPP (GT Weights Ours)             &   \textbf{0.90 $\pm$ 0.01} &  \textbf{0.91 $\pm$ 0.01} &  \textbf{0.91 $\pm$ 0.01} &   \textbf{0.90 $\pm$ 0.01} &  0.96 $\pm$ 0.02 &   0.93 $\pm$ 0.02 \\
CP (no policy shift) &   \textbf{0.90 $\pm$ 0.01} &  0.88 $\pm$ 0.01 &  0.82 $\pm$ 0.01 &  0.73 $\pm$ 0.01 &   0.60 $\pm$ 0.01 &  0.46 $\pm$ 0.01\\
\bottomrule
\end{tabular}
}
\end{center}
\end{table}

\begin{table}[h!]
\begin{center}
\caption{Mean Interval Length as a function of policy shift with 2 standard errors over 10 runs.}
\label{tab:len_cts}
\resizebox{0.9\columnwidth}{!}{
\begin{tabular}{lllllll}
\toprule
Interval Lengths &              $\epsilon^*=0.0$ &              $\epsilon^*=0.5$ &              $\epsilon^*=1.0$ &              $\epsilon^*=1.5$  & $\epsilon^*=2.0$ & $\epsilon^*=2.5$\\
\midrule
COPP (Ours)                   &  4.75 $\pm$ 0.04 &  5.08 $\pm$ 0.09 &  5.89 $\pm$ 0.14 &  6.92 $\pm$ 0.18 &  7.82 $\pm$ 0.41 &  8.45 $\pm$ 0.44\\
WIS                           &   9.55 $\pm$ 0.1 &  9.56 $\pm$ 0.12 &  9.56 $\pm$ 0.27 &  9.44 $\pm$ 0.38 &   9.40 $\pm$ 0.59 &  9.08 $\pm$ 0.64 \\
SBA                           &  4.38 $\pm$ 0.03 &  4.37 $\pm$ 0.03 &  4.36 $\pm$ 0.04 &  4.34 $\pm$ 0.07 &   4.31 $\pm$ 0.1 &  4.28 $\pm$ 0.14 \\
\midrule
\midrule
COPP (GT Weights Ours)             &  4.73 $\pm$ 0.05 &  5.07 $\pm$ 0.09 &  5.87 $\pm$ 0.14 &  6.82 $\pm$ 0.13 &  7.57 $\pm$ 0.19 &  8.07 $\pm$ 0.22 \\
CP (no policy shift) &   4.70 $\pm$ 0.05 &   4.70 $\pm$ 0.05 &   4.70 $\pm$ 0.05 &   4.70 $\pm$ 0.05 &   4.70 $\pm$ 0.05 &   4.70 $\pm$ 0.05 \\
\bottomrule
\end{tabular}
}
\end{center}
\end{table}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.45\textwidth, height=0.3\textwidth]{figures/copp/pol0.png}
    \includegraphics[width=0.45\textwidth, height=0.3\textwidth]{figures/copp/pol0.5.png}\\
    \includegraphics[width=0.45\textwidth, height=0.3\textwidth]{figures/copp/pol1.png}
    \includegraphics[width=0.45\textwidth, height=0.3\textwidth]{figures/copp/pol1.5.png}\\
    \includegraphics[width=0.45\textwidth, height=0.3\textwidth]{figures/copp/pol2.png}
    \includegraphics[width=0.45\textwidth, height=0.3\textwidth]{figures/copp/pol2.5.png}
    \caption{Plots of $A$ against $X$, where $X \sim \mathcal{N}(0, 4)$ and $A\mid X$ is sampled from behaviour and target policies. Here, target policies are defined in \eqref{tar_pols} for $\epsilon^* \in \{0, 0.5, 1, 1.5, 2, 2.5\}$.}
    \label{fig:pols_cts_acs}
\end{figure*}


\newpage



\subsection{Experiments on Microsoft Ranking Dataset}\label{sec:MSR_experiments_decrip}

\paragraph{Dataset details.}
The dataset contains relevance scores for websites recommended to different users, and comprises of $30,000$ user-website pairs. For a user $i$ and website $j$, the data contains a $136$-dimensional feature vector $u_i^j$, which consists of user $i$'s attributes corresponding to website $j$, such as length of stay or number of clicks on the website. Furthermore, for each user-website pair, the dataset also contains a relevance score, i.e. how relevant the website was to the user.

First, given a user $i$ we sample (with replacement) $5$ websites,  $\{u_i^j\}_{j=1}^5$, from the data. Next, we reformulate this into a contextual bandit where $A \in \{1,2,3,4,5\}$ corresponds to the website we recommend to a user. For a user $i$, we define $X$ by combining the $5$ feature vectors corresponding to the user, i.e. $X \in \mathbb{R}^{5 \times 136}$, where $x_i = (u^1_{i},u^2_{i},u^3_{i},u^4_{i}, u^5_{i})$. In addition, $Y \in\{0,1,2,3,4\}$ corresponds to the relevance score for the $A$'th website, i.e. the recommended website. The goal is to construct prediction sets that are guaranteed to contain the true relevance score with a probability of $90\%$. Here we use $m=5000$ training data points.

\paragraph{Behaviour and Target Policies.}
We first train a Neural Network (NN) classifier model mapping each 136-dimensional feature vector to the
softmax scores for each relevance score class, $\hat{f}_\theta:\mathcal{U} \rightarrow [0,1]^5$. We use this trained model $\hat{f}_\theta$ to define a family of policies such that we pick the most relevant website as predicted by $\hat{f}_\theta$ with probability $\epsilon$ and the rest uniformly with probability $(1-\epsilon)/4$. Formally, this has been expressed as follows. We use $\hat{f}^{\textup{label}}_\theta$ to denote the relevance class predicted by $\hat{f}_\theta$, i.e. $\hat{f}^{\textup{label}}_\theta(u) \coloneqq \argmax_i\{\hat{f}_\theta(u)_i\}$. 

Then,
\begin{align}
    \pi_\epsilon (a\mid X=(u^1, u^2,u^3,u^4,u^5)) \coloneqq& \epsilon \mathbbm{1}(a = \argmax_j\{ \hat{f}^{\textup{label}}_\theta(u^j) \}) \nonumber \\
    &+ (1-\epsilon)/4 \mathbbm{1}(a \neq \argmax_j\{ \hat{f}^{\textup{label}}_\theta(u^j) \}) \nonumber
\end{align}

\paragraph{Estimation of ratios, $\hat{w}(X, Y)$.}
To estimate the $\hat{P}(y \mid x, a)$ we use the trained model $\hat{f}_\theta$ as follows:
\[
\hat{P}(y \mid x = (u^1, u^2,u^3,u^4,u^5), a) = \hat{f}_\theta(u^a)_y
\]
where $\hat{f}_\theta(u^a)_y$ corresponds to the softmax prediction of $u^a$ for label $y$ under the model $\hat{f}_\theta$. To estimate the behaviour policy $\hat{\pi}^b$, we train a classifier model $\mathcal{X} \rightarrow \mathcal{A}$ using a neural network. We use \eqref{weight-est} to estimate the weights $\hat{w}(x, y)$.


\paragraph{Neural Network Architectures}
\begin{itemize}
    \item To approximate the behaviour policy, we use a neural network with 2 hidden layers and 25 nodes in each hidden layer, ReLU activations and softmax output.
    \item To approximate $\hat{f}_{\theta}$, we use a neural network with 2 hidden layers with 64 nodes each and ReLU activations.
\end{itemize}

\paragraph{Results: Coverage as a function of increase calibration data.}\label{app:N-cal_exp_msr}
As mentioned in the main text, we have also performed experiments to investigate how much calibration data is needed for COPP as well as other methods to converge to the required $90\%$ coverage. In the below plot we have plotted the coverage as a function of $n$ calibration data points. We observe that our proposed method is converging much faster to the required coverage compared to the competing methods.

\begin{figure*}[htp!]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/copp/cov-ncal-msr.png} %
    \label{fig:ncal-msr}
    \caption{Results of Microsoft Ranking Dataset experiment with behaviour policy $\pi^b = \pi_{0.5}$ and the target policy is $\pi^* = \pi_{0.2}$. Our proposed method is able to converge to the required coverage rather quickly compared to the competing methods}
    \label{fig:msr}
\end{figure*}
\newpage
\subsubsection{Results: COPP for Class-balanced coverage}\label{sec:results_class_bal_coverage}

%
Table \ref{tab:label-cond} shows the coverages of COPP predictive sets ($\hat{C}_n$ with marginal coverage guarantee constructed using algorithm \ref{cp_covariate_shift}) and COPP intervals with label conditioned coverage ($\hat{C}^{\mathcal{Y}}_n$ satisfying \eqref{label_cond} constructed using algorithm \ref{cp_label_conditioned}). Extensions of WIS and SBA to the conditional case are not straightforward and hence have not been included. For $\hat{C}_n$, while the overall coverage is very close to the required coverage of $90\%$, we see that there is under-coverage for $Y = 0,1,2,3$. This can be explained by the data imbalance -- the number of test data points with $Y = 0,1,2,3$ is significantly lower than $Y=4$. 

This under-coverage problem disappears in $\hat{C}^{\mathcal{Y}}_n$. Instead, in cases where number of data points is small, ($Y = 0,1,2,3$), the predictive sets $\hat{C}^{\mathcal{Y}}_n$ are conservative (i.e. have coverage $> 90\%$). As a result, the overall coverage increases to 0.941. This is a price to be paid for label conditioned coverage -- the overall coverage may increase, however, being conservative in safety-critical settings is better than being overly optimistic.

\begin{table}[t]
\begin{center}
\caption{Coverages for COPP with and without label conditioned coverage, $\hat{C}^{\mathcal{Y}}_n$ and $\hat{C}_n$ respectively. Overall coverage refers to marginal coverage while $Y=y$ refers to coverage conditioned on $Y=y$. Here $n_{test}$ corresponds to the number of test data points ($\sim P^{\pi^*}$).}
\label{tab:label-cond}
\resizebox{0.5\columnwidth}{!}{
\begin{tabular}{lccccr}
\toprule
   & $n_{test}$  & $\hat{C}_n$ Cov      & $\hat{C}^{\mathcal{Y}}_n$ Cov      \\
\midrule
Overall & 5000  & 0.896 $\pm$ 0.005  & 0.941 $\pm$ 0.003 \\
$Y=0$ & 266  & \red{0.700 $\pm$ 0.020}  & 1.000 $\pm$ 0.000  \\
$Y=1$ & 293  & \red{0.526  $\pm$ 0.019} & 1.000 $\pm$ 0.000  \\
$Y=2$ & 228  & \red{0.772 $\pm$  0.018} & 0.990 $\pm$ 0.029 \\
$Y=3$ & 320  & \red{0.852 $\pm$  0.015} & 0.964 $\pm$ 0.035 \\
$Y=4$ & 3893 & 0.950 $\pm$ 0.006 & 0.928 $\pm$ 0.003 \\
\bottomrule
\end{tabular}
}
\end{center}
\end{table}

\subsection{UCI Dataset experiments}\label{sec:UCI}
Following \cite{risk-assessment, doubly-robust, adaptive-ope} we apply COPP on UCI classification datasets. We can pose classification as contextual bandits by defining the covariates $\mathcal{X}$ as the features, the action space $\mathcal{A} =\mathcal{K}$, where $\mathcal{K}$ is the set of labels, and the outcomes are binary, i.e. $\mathcal{Y}= \{0,1\}$, defined by $Y \mid X, A = \mathbbm{1}(X \textup{ belongs to class }A)$. Here we use $m=1000$ training data points.

\paragraph{Behaviour and Target Policies.}
First we train a neural network classifier mapping each covariate to the softmax scores for each class, $\hat{f}_\theta: \mathcal{X} \rightarrow [0,1]^{|\mathcal{K}|}$. We use this trained model $\hat{f}_\theta$ to define a family of policies such that we pick the most likely label as predicted by $\hat{f}_\theta$ with probability $\epsilon$ and the rest uniformly with probability. Formally, this can be expressed as follows:
\begin{align}
    &\pi_\epsilon (a\mid x) \coloneqq \epsilon \mathbbm{1}(a = \argmax_{k \in \mathcal{K}}\{ \hat{f}_\theta(x)_{k} \}) + (1-\epsilon)/(|\mathcal{K}|-1)\mathbbm{1}(a \neq \argmax_{k \in \mathcal{K}}\{ \hat{f}_\theta(x)_{k} \}) \nonumber
\end{align}
Like other experiments, we use $\epsilon$ to control the shift between behaviour and target policies. For $\pi^b$, we use $\epsilon^b = 0.5$ and for $\epsilon^* \in \{0.05, 0.3, 0.4, 0.5, 0.6, 0.7, 0.95\}$. Using this behaviour policy $\pi^b$, we generate an observational dataset $\mathcal{D}_{obs} = \{x_i, a_i, y_i\}_{i=1}^{n_{obs}}$ which is then split into training $\mathcal{D}_{tr}$ and calibration datasets $\mathcal{D}_{cal}$, of sizes $m$ and $n$ respectively.

\paragraph{Estimation of ratios, $\hat{w}(X, Y)$.}
To estimate the $\hat{P}(y \mid x, a)$ we use the trained model $\hat{f}_\theta$ as follows:
\[
\hat{P}(Y = 1 \mid x, a) = \hat{f}_\theta(x)_a
\]
where $\hat{f}_\theta(x)_a$ corresponds the softmax prediction of $x$ for label $a$ under the model $\hat{f}_\theta$. To estimate the behaviour policy $\hat{\pi}^b$, we train a classifier model $\mathcal{X} \rightarrow \mathcal{A}$ using a neural network. We use \eqref{weight-est} in main text to estimate weights $\hat{w}(x, y)$.


\paragraph{Score.} We define $\hat{P}^{\pi^b}(y \mid x) = \sum_{i \in \mathcal{K}} \hat{\pi}^b(A = i|x) \hat{P}(y|x, A = i)$. Using similar formulation as in \cite{conf-bates}, we define the score as 
\[
s(x, y) = \sum_{y' = 0, 1} \hat{P}^{\pi^b}(y' \mid x) \mathbbm{1}(\hat{P}^{\pi^b}(y' \mid x) \geq \hat{P}^{\pi^b}(y \mid x))
\]


\paragraph{Neural Network Architectures}
\begin{itemize}
    \item To approximate the behaviour policy, we use a neural network with 2 hidden layers and 64 nodes in each hidden layer, ReLU activations and softmax output.
    \item To approximate $\hat{f}_{\theta}$, we use a neural network with 2 hidden layers with 64 nodes each and ReLU activations.
\end{itemize}


\paragraph{Results.} Tables \ref{tab:yeast}-\ref{tab:satimage} show the coverages across varying target policies for different classification datasets. The behaviour policy $\pi^b = \pi_{0.5}$ is fixed and we use $n=5000$ calibration datapoints, across 10 runs with $m=5000$ training data. The tables show that COPP is able to provide the required coverage of 90\% across all target policies. Moreover, compared to COPP, SBA and WIS are overly conservative. WIS estimates are not adaptive w.r.t. $X$, and as a result, the predictive sets produced are uninformative (i.e. contain all outcomes) in these experiments where the outcome is binary. 

We have also included a comparison of COPP using estimated behaviour policy with COPP using GT behaviour policy. The latter provides more accurate coverage, and using estimated behaviour policy provides slightly over-covered predictive sets comparatively in most cases. This can be explained by policy estimation error. Additionally, we observe that using standard CP leads to predictive sets which are not adaptive to policy shift. As a result, the standard CP predictive sets get overly conservative (optimistic) as $\Delta_\epsilon$ becomes more negative (positive).

\newpage
\begin{table}[h!]
\begin{center}
\caption{Yeast dataset results}
\label{tab:yeast}
\resizebox{0.8\columnwidth}{!}{
\begin{tabular}{llllllll}
\toprule
& $\Delta_{\epsilon}=-0.45$ & $\Delta_{\epsilon}=-0.2$ & $\Delta_{\epsilon}=-0.1$ & $\Delta_{\epsilon}=0.0$ & $\Delta_{\epsilon}=0.1$ & $\Delta_{\epsilon}=0.2$ & $\Delta_{\epsilon}=0.45$ \\
\midrule
COPP (Ours)            &              0.92$\pm$0.00 &             0.92$\pm$0.00 &             0.92$\pm$0.00 &            0.92$\pm$0.00 &            0.92$\pm$0.00 &            0.92$\pm$0.00 &             0.91$\pm$0.00 \\
WIS                  &             0.99$\pm$0.01 &              1.00$\pm$0.00 &              1.00$\pm$0.00 &             1.00$\pm$0.00 &             1.00$\pm$0.00 &             1.00$\pm$0.00 &              1.00$\pm$0.00 \\
SBA                  &              0.98$\pm$0.00 &              1.00$\pm$0.00 &              1.00$\pm$0.00 &             1.00$\pm$0.00 &             1.00$\pm$0.00 &             1.00$\pm$0.00 &              1.00$\pm$0.00 \\
\midrule
\midrule
COPP (GT behav policy) &              0.91$\pm$0.00 &             0.91$\pm$0.00 &              0.90$\pm$0.00 &             0.90$\pm$0.00 &             0.90$\pm$0.00 &             0.90$\pm$0.00 &              0.90$\pm$0.00 \\
CP (no policy shift) &              0.97$\pm$0.00 &             0.93$\pm$0.00 &             0.92$\pm$0.00 &             0.90$\pm$0.00 &            0.89$\pm$0.00 &            0.87$\pm$0.00 &             0.83$\pm$0.00 \\
\bottomrule
\end{tabular}
}
\end{center}
\end{table}

\begin{table}[h!]
\begin{center}
\begin{small}
\begin{sc}
\caption{Ecoli dataset results}
\label{tab:ecoli}
\resizebox{0.8\columnwidth}{!}{
\begin{tabular}{llllllll}
\toprule
 & $\Delta_{\epsilon}=-0.45$ & $\Delta_{\epsilon}=-0.2$ & $\Delta_{\epsilon}=-0.1$ & $\Delta_{\epsilon}=0.0$ & $\Delta_{\epsilon}=0.1$ & $\Delta_{\epsilon}=0.2$ & $\Delta_{\epsilon}=0.45$ \\

\midrule
COPP (Ours)            &              0.92$\pm$0.00 &             0.91$\pm$0.00 &             0.91$\pm$0.00 &             0.90$\pm$0.00 &             0.90$\pm$0.00 &             0.90$\pm$0.00 &              0.90$\pm$0.00 \\
WIS                  &               1.00$\pm$0.00 &              1.00$\pm$0.00 &              1.00$\pm$0.00 &             1.00$\pm$0.00 &             1.00$\pm$0.00 &             1.00$\pm$0.00 &              1.00$\pm$0.00 \\
SBA                  &               1.00$\pm$0.00 &              1.00$\pm$0.00 &              1.00$\pm$0.00 &             1.00$\pm$0.00 &             1.00$\pm$0.00 &             1.00$\pm$0.00 &              1.00$\pm$0.00 \\
\midrule
\midrule
COPP (GT behav policy) &              0.91$\pm$0.00 &              0.90$\pm$0.00 &              0.90$\pm$0.00 &             0.90$\pm$0.00 &             0.90$\pm$0.00 &             0.90$\pm$0.00 &             0.90$\pm$0.01 \\
CP (no policy shift) &              0.92$\pm$0.00 &             0.91$\pm$0.00 &             0.91$\pm$0.00 &             0.90$\pm$0.00 &             0.90$\pm$0.00 &            0.89$\pm$0.00 &             0.88$\pm$0.00 \\
\bottomrule
\end{tabular}
}
\end{sc}
\end{small}
\end{center}
\end{table}

\begin{table}[h!]
\begin{center}
\begin{small}
\begin{sc}
\caption{Letter dataset results}
\label{tab:letter}
\resizebox{0.8\columnwidth}{!}{
\begin{tabular}{llllllll}
\toprule
 & $\Delta_{\epsilon}=-0.45$ & $\Delta_{\epsilon}=-0.2$ & $\Delta_{\epsilon}=-0.1$ & $\Delta_{\epsilon}=0.0$ & $\Delta_{\epsilon}=0.1$ & $\Delta_{\epsilon}=0.2$ & $\Delta_{\epsilon}=0.45$ \\

\midrule
COPP (Ours)            &              0.95$\pm$0.00 &             0.93$\pm$0.00 &             0.93$\pm$0.00 &            0.92$\pm$0.00 &            0.92$\pm$0.00 &            0.92$\pm$0.00 &             0.91$\pm$0.00 \\
WIS                  &               1.00$\pm$0.00 &              1.00$\pm$0.00 &              1.00$\pm$0.00 &             1.00$\pm$0.00 &             1.00$\pm$0.00 &             1.00$\pm$0.00 &              1.00$\pm$0.00 \\
SBA                  &              0.97$\pm$0.00 &              1.00$\pm$0.00 &              1.00$\pm$0.00 &             1.00$\pm$0.00 &             1.00$\pm$0.00 &             1.00$\pm$0.00 &              1.00$\pm$0.00 \\
\midrule
\midrule
COPP (GT behav policy) &              0.92$\pm$0.00 &             0.91$\pm$0.00 &             0.91$\pm$0.00 &             0.90$\pm$0.00 &            0.89$\pm$0.00 &            0.89$\pm$0.00 &             0.88$\pm$0.00 \\
CP (no policy shift) &              0.99$\pm$0.00 &             0.94$\pm$0.00 &             0.92$\pm$0.00 &             0.90$\pm$0.00 &            0.88$\pm$0.00 &            0.86$\pm$0.00 &             0.81$\pm$0.00 \\
\bottomrule
\end{tabular}
}
\end{sc}
\end{small}
\end{center}
\end{table}

\begin{table}[h!]
\begin{center}
\begin{small}
\begin{sc}
\caption{Optdigits dataset results}
\label{tab:optdigits}
\resizebox{0.8\columnwidth}{!}{
\begin{tabular}{llllllll}
\toprule
 & $\Delta_{\epsilon}=-0.45$ & $\Delta_{\epsilon}=-0.2$ & $\Delta_{\epsilon}=-0.1$ & $\Delta_{\epsilon}=0.0$ & $\Delta_{\epsilon}=0.1$ & $\Delta_{\epsilon}=0.2$ & $\Delta_{\epsilon}=0.45$ \\
\midrule
COPP (Ours)            &              0.93$\pm$0.00 &             0.93$\pm$0.00 &             0.93$\pm$0.00 &            0.93$\pm$0.00 &            0.93$\pm$0.00 &            0.93$\pm$0.00 &             0.93$\pm$0.00 \\
WIS                  &             0.99$\pm$0.01 &              1.00$\pm$0.00 &              1.00$\pm$0.00 &             1.00$\pm$0.00 &             1.00$\pm$0.00 &             1.00$\pm$0.00 &              1.00$\pm$0.00 \\
SBA                  &              0.97$\pm$0.00 &              1.00$\pm$0.00 &              1.00$\pm$0.00 &             1.00$\pm$0.00 &             1.00$\pm$0.00 &             1.00$\pm$0.00 &             0.99$\pm$0.00 \\
\midrule
\midrule
COPP (GT behav policy) &              0.91$\pm$0.00 &              0.90$\pm$0.00 &              0.90$\pm$0.00 &             0.90$\pm$0.00 &             0.90$\pm$0.00 &            0.89$\pm$0.00 &             0.89$\pm$0.00 \\
CP (no policy shift) &              0.97$\pm$0.00 &             0.93$\pm$0.00 &             0.91$\pm$0.00 &             0.90$\pm$0.00 &            0.88$\pm$0.00 &            0.87$\pm$0.00 &             0.83$\pm$0.00 \\
\bottomrule
\end{tabular}
}
\end{sc}
\end{small}
\end{center}
\end{table}

\begin{table}[h!]
\begin{center}
\begin{small}
\begin{sc}
\caption{Pendigits dataset results}
\label{tab:pendigits}
\resizebox{0.8\columnwidth}{!}{
\begin{tabular}{llllllll}
\toprule
 & $\Delta_{\epsilon}=-0.45$ & $\Delta_{\epsilon}=-0.2$ & $\Delta_{\epsilon}=-0.1$ & $\Delta_{\epsilon}=0.0$ & $\Delta_{\epsilon}=0.1$ & $\Delta_{\epsilon}=0.2$ & $\Delta_{\epsilon}=0.45$ \\
\midrule
COPP (Ours)            &              0.92$\pm$0.00 &             0.92$\pm$0.00 &             0.92$\pm$0.00 &            0.92$\pm$0.00 &            0.92$\pm$0.00 &            0.92$\pm$0.00 &             0.91$\pm$0.00 \\
WIS                  &               1.00$\pm$0.00 &              1.00$\pm$0.00 &              1.00$\pm$0.00 &             1.00$\pm$0.00 &             1.00$\pm$0.00 &             1.00$\pm$0.00 &              1.00$\pm$0.00 \\
SBA                  &              0.97$\pm$0.00 &              1.00$\pm$0.00 &              1.00$\pm$0.00 &             1.00$\pm$0.00 &             1.00$\pm$0.00 &             1.00$\pm$0.00 &             0.99$\pm$0.00 \\
\midrule
\midrule
COPP (GT behav policy) &              0.91$\pm$0.00 &              0.90$\pm$0.00 &              0.90$\pm$0.00 &             0.90$\pm$0.00 &             0.90$\pm$0.00 &            0.89$\pm$0.00 &             0.89$\pm$0.00 \\
CP (no policy shift) &              0.99$\pm$0.00 &             0.94$\pm$0.00 &             0.92$\pm$0.00 &             0.90$\pm$0.00 &            0.88$\pm$0.00 &            0.86$\pm$0.00 &             0.81$\pm$0.00 \\
\bottomrule
\end{tabular}
}
\end{sc}
\end{small}
\end{center}
\end{table}

\begin{table}[h!]
\begin{center}
\begin{small}
\begin{sc}
\caption{Satimage dataset results}
\label{tab:satimage}
\resizebox{0.8\columnwidth}{!}{
\begin{tabular}{llllllll}
\toprule
 & $\Delta_{\epsilon}=-0.45$ & $\Delta_{\epsilon}=-0.2$ & $\Delta_{\epsilon}=-0.1$ & $\Delta_{\epsilon}=0.0$ & $\Delta_{\epsilon}=0.1$ & $\Delta_{\epsilon}=0.2$ & $\Delta_{\epsilon}=0.45$ \\

\midrule
COPP (Ours)            &              0.92$\pm$0.00 &             0.91$\pm$0.00 &             0.91$\pm$0.00 &            0.91$\pm$0.00 &            0.91$\pm$0.00 &            0.91$\pm$0.00 &             0.91$\pm$0.00 \\
WIS                  &               1.00$\pm$0.00 &              1.00$\pm$0.00 &              1.00$\pm$0.00 &             1.00$\pm$0.00 &             1.00$\pm$0.00 &             1.00$\pm$0.00 &              1.00$\pm$0.00 \\
SBA                  &              0.98$\pm$0.00 &              1.00$\pm$0.00 &              1.00$\pm$0.00 &             1.00$\pm$0.00 &             1.00$\pm$0.00 &             1.00$\pm$0.00 &             0.99$\pm$0.00 \\
\midrule
\midrule
COPP (GT behav policy) &               0.90$\pm$0.00 &              0.90$\pm$0.00 &              0.90$\pm$0.00 &             0.90$\pm$0.00 &             0.90$\pm$0.00 &             0.90$\pm$0.00 &             0.89$\pm$0.00 \\
CP (no policy shift) &              0.97$\pm$0.00 &             0.93$\pm$0.00 &             0.92$\pm$0.00 &             0.90$\pm$0.00 &            0.88$\pm$0.00 &            0.87$\pm$0.00 &             0.83$\pm$0.00 \\
\bottomrule
\end{tabular}
}
\end{sc}
\end{small}
\end{center}
\end{table}

\clearpage

\section{How the miscoverage depends on $\hat{P}(y\mid x, a)$}
\begin{proposition}
Let
\begin{align*}
    \tilde{w}(x, y) \coloneqq \frac{\int \hat{P}(y\mid x, a)\pi^*(a\mid x)\mathrm{d}a}{\int \hat{P}(y\mid x, a)\pi^b(a\mid x)\mathrm{d}a}.
\end{align*}
Assume that
$\hat{P}(y\mid x, a)/P(y\mid x, a) \in [1/\Gamma, \Gamma]$ for some $\Gamma \geq 1$.
Then, $$\Delta_w \coloneqq \tfrac{1}{2}\expb \mid \tilde{w}(X,Y) - w(X,Y)\mid \leq \Gamma^2 - 1.$$
\end{proposition}
\begin{proof}
In this proof, we investigate the error of the weights as a function of the error in $\hat{P}(y\mid x, a)$. Therefore, to isolate this effect we ignore the Monte Carlo error, and assume known behavioural policy $\pi^b$.

Under the assumption above, we have that

\begin{align*}
    \frac{1/ \Gamma \int P(y\mid x, a)\pi^*(a\mid x)\mathrm{d}a}{\Gamma \int P(y\mid x, a)\pi^b(a\mid x)\mathrm{d}a} \leq &\tilde{w}(x, y) \leq \frac{\Gamma \int P(y\mid x, a)\pi^*(a\mid x)\mathrm{d}a}{1/\Gamma \int P(y\mid x, a)\pi^b(a\mid x)\mathrm{d}a}.\\
    \implies \frac{1}{\Gamma^2} w(x, y) \leq &\tilde{w}(x, y) \leq \Gamma^2 w(x, y)
\end{align*}
This means that, 

\begin{align*}
    \left(\frac{1}{\Gamma^2}-1 \right) w(x, y) \leq &\tilde{w}(x, y) - w(x, y) \leq (\Gamma^2 - 1) w(x, y)
\end{align*}
So, 
\begin{align*}
    \mid \tilde{w}(x, y) - w(x, y)\mid \leq (\Gamma^2 - 1) w(x, y)
\end{align*}
And therefore, 
\begin{align*}
    \expb \mid \tilde{w}(X,Y) - w(X,Y)\mid \leq (\Gamma^2 - 1) \expb[w(X, Y)] = \Gamma^2 - 1
\end{align*}
\end{proof}

%
