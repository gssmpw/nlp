
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage[backref=page]{hyperref}
% \usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{adjustbox}
\usepackage{cleveref}
\usepackage{booktabs}
\usepackage{textcomp}

% \title{Object-Centric Latent Action Pretraining for Robust World Models in Distracting Environments}
\title{Object-Centric Latent Action Learning}
% \title{Latent Action Learning with Object-Centric Representations}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.
% klepach@airi.net

\author{
Albina Klepach\thanks{Correspondence to: \href{mailto:klepach@airi.net}{klepach@airi.net}. Work done by \href{https://dunnolab.ai}{dunnolab.ai}.} \\ AIRI \And 
Alexander Nikulin \\ AIRI, MIPT \And 
Ilya Zisman \\ AIRI, Skoltech \And 
Denis Tarasov \\ AIRI, ETH Zürich \And 
Alexander Derevyagin \\ AIRI \And 
Andrei Polubarov \\ AIRI, Skoltech \And 
Nikita Lyubaykin \\ AIRI, Innopolis University \And 
Vladislav Kurenkov \\ AIRI, Innopolis University
% Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
% about author (webpage, alternative address)---\emph{not} for acknowledging
% funding agencies.  Funding acknowledgements go at the end of the paper.} \\
% Department of Computer Science\\
% Cranberry-Lemon University\\
% Pittsburgh, PA 15213, USA \\
% \texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
% \And
% Ji Q. Ren \& Yevgeny LeNet \\
% Department of Computational Neuroscience \\
% University of the Witwatersrand \\
% Joburg, South Africa \\
% \texttt{\{robot,net\}@wits.ac.za} \\
% \AND
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Leveraging vast amounts of internet video data for Embodied AI is currently bottle-necked by the lack of action annotations and the presence of action-correlated distractors. We propose a novel object-centric latent action learning approach, based on VideoSaur and LAPO, that employs self-supervised decomposition of scenes into object representations and annotates video data with proxy-action labels. This method effectively disentangles causal agent-object interactions from irrelevant background noise and reduces the performance degradation of latent action learning approaches caused by distractors. Our preliminary experiments with the Distracting Control Suite show that latent action pretraining based on object decompositions improve the quality of inferred latent actions by $\mathbf{x2.7}$ and efficiency of downstream fine-tuning with a small set of labeled actions, increasing return by $\mathbf{x2.6}$ on average.
\end{abstract}


\section{Introduction}

% The ability to learn robust World Models from uncurated video data is critical for scaling embodied AI to real-world scenarios. However, data on the internet has no labels. So we need to prepare them on our own. The methods to prepare action labels for raw data include LAPO. And some others [which ones?, LAPA], with their own peculiar properties and limitations [which ones?]. Lapo is the most promising, but there is a fundamental issue with lapo. Lapo can not work in presentence of action-correlated distractors. What are them? Nearly everything what is not related to the solution of the main task and  can become implicit bias on which our model will definitelly overfit on. So we need some way to cope with them, better without any supervision. And here it is -- object-centric learning. It can help to find the direction in this all distractor noise, as it offers a pathway implicitly separating entities from distractors by decomposing scenes into spatio-temporal representations, object slots. 

In recent years, the scaling of model and data sizes has led to the creation of powerful and general foundation models \citep{bommasani2021opportunities} that have enabled many breakthroughs in understanding and generation of natural language \citep{achiam2023gpt, brown2020language} and images \citep{dehghani2023scaling, radford2021learning}. On the other hand, the field of embodied AI has generally remained behind in terms of generalization and emergent abilities \citep{guruprasad2024benchmarking}, being mostly limited by the lack of diverse data for pre-training \citep{lin2024data}. The vast amount of video data on the Internet, covering a wide variety of human-related activities, can potentially fulfill the current data needs \citep{mccarthy2024towards}. Unfortunately, videos cannot be used directly since they do not have action labels, which is necessary for imitation and reinforcement learning algorithms.

In order to compensate for the lack of action labels, approaches based on Latent Action Models (LAM) \citep{schmidt2024learningactactions, ye2024latentactionpretrainingvideos, cui2024dynamo, bruce2024genie}, aim to infer latent proxy-actions between consecutive observations. Such actions can be later used for imitation learning to obtain behavioral policy prior from large unlabeled datasets. A significant challenge in this endeavor is the presence of action-correlated distractors—dynamic backgrounds, incidental object motions, camera shifts, and other nuisances—that falsely correlate with agent actions, and may lead models to overfit to non-causal patterns \citep{wang2024ad3, misra2024towards, mccarthy2024towards}. Existing methods for learning latent actions from videos, such as Latent Action Pretraining (LAPA) \citep{ye2024latentactionpretrainingvideos}, often assume curated, distractor-free datasets or rely on costly annotations. Although effective in controlled settings, this reliance on clean data limits their scalability and applicability in real-world scenarios.

In this preliminary work, we propose object-centric latent action learning in order to improve applicability to real-world data. By decomposing scenes into spatio-temporal object slots \citep{locatello2020objectcentriclearningslotattention}, our method provides the structural priors needed to disentangle causal agent-object representations from distractors. Object-centric representations \citep{zadaianchuk2023objectcentriclearningrealworldvideos} isolate entities into slots through self-supervised feature similarity losses, inherently filtering noise like static backgrounds or incidental motion. This enables Latent Action Models to focus on the dynamics of task-relevant objects while ignoring spurious correlations. Using the Distracting Control Suite (DCS) \citep{stone2021distractingcontrolsuite}, we empirically demonstrate that latent action learning based on self-supervised object-centric decomposition from VideoSAUR \citep{zadaianchuk2023objectcentriclearningrealworldvideos} improves the quality of latent actions by $\mathbf{x2.7}$ and downstream performance after fine-tuning with small amount of ground-truth actions by $\mathbf{x2.6}$ on average. 

In a concurrent work to ours, \citet{villarcorrales2025playslotlearninginverselatent} also explores the application of object-centric representations for latent action learning and video prediction, showing positive results in the robotic tabletop simulations. In contrast to our work, \citet{villarcorrales2025playslotlearninginverselatent} does not explore the limitations of such an approach to latent action learning in the presence of distractors, similar to existing research. Moreover, instead of VideoSAUR \citep{zadaianchuk2023objectcentriclearningrealworldvideos} employed in our work, their method uses SAVi \citep{kipf2021conditional}  for object-centric learning, which, as we show (see \Cref{app:steve}) does not work well in more complex environments with real-world distractors.

% We summarize our main contributions as follows:
% \begin{itemize}
%     \item \textbf{Integration of object-centric decomposition with latent action learning}. By combining VideoSAUR’s \citep{zadaianchuk2023objectcentriclearningrealworldvideos} self-supervised object-centric decomposition with latent action learning, enabling robust representation learning from pure video data containing distractors.
% 	\item \textbf{Empirical Validation on Noisy Observations}. We demonstrate improved action prediction quality and downstream performance on the Distracting Control Suite (DCS), highlighting the effectiveness of our approach in handling complex, distractor-laden environments.
% \end{itemize}

% , such as those from VideoSAUR 

% This challenge mirrors a broader theme: \textit{how to extract meaningful, causal representations from high-dimensional observations}. Object-centric learning \citep{zadaianchuk2023objectcentriclearningrealworldvideos} offers pathway implicitly separating entities from distractors by decomposing scenes into spatio-temporal representations, object slots. However, translating these representations into actionable dynamics for World Models remains unsolved. Specifically, raw videos lack explicit action labels.
% , which can enable the model to reason about interactions between independent components. However, their potential for real-world World Modeling is still underexplored.
% We address this gap by proposing \textit{object-centric latent action learning}, a step toward scalable World Models. Our key insight is that object-centric representations provide an inductive bias for causal structure: by decomposing observations into entity-level slots (via VideoSAUR), we enable models to (1) suppress distractors and (2) infer latent actions directly from object dynamics. This approach eliminates the need for curated data or manual annotations, bridging the divide between raw video inputs and actionable World Model training.


% \section{Related work}

% ocl for rl
% An Investigation into Pre-Training Object-Centric Representations  for Reinforcement Learning -- Our results provide empirical evidence for valuable insights into the effectiveness of OCR pre-training for RL and the potential limitations of its use in certain scenarios. Additionally, this study also examines the critical aspects of incorporating OCR pre-training in RL, including performance in a visually complex environment and the appropriate pooling layer to aggregate the object representations.

% ocl for model-based rl
% SOLD: Reinforcement Learning with Slot Object-Centric Latent Dynamics не рассматривают дистракторы
% DREAM TO MANIPULATE: COMPOSITIONAL WORLD MODELS EMPOWERING ROBOT IMITATION LEARNING WITH IMAGINATION
% Dreamweaver: Learning Compositional World Models from Pixels (Recurrent Block-Slot Unit (RBSU))
% OBJECTS MATTER: OBJECT-CENTRIC WORLD MODELS IMPROVE REINFORCEMENT LEARNING IN VISUALLY COMPLEX ENVIRONMENTS

% ocl for video prediction
% Generative Omnimatte: Learning to Decompose Video into Layers

% ocl for learnging from videos
% ???
% Object-centric Video Representation for Long-term Action Anticipation: This study focuses on building object-centric representations for long-term action anticipation in videos. It proposes an approach to extract task-specific object-centric representations from general-purpose pretrained models without finetuning, highlighting the importance of objects in recognizing and predicting human-object interactions. (arxiv.org)


% \begin{figure}[h]
% \begin{center}
% %\framebox[4.0in]{$\;$}
% % \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
% % \includegraphics[width=5.0in]{plots/tmp.pdf}
% % \includegraphics[width=4.0in]{plots/image.pdf}
% \includegraphics[width=3.0in]{plots/vsaur_short.pdf}
% % \includegraphics[width=5.5in]{plots/vsaur_short_2.pdf}
% \end{center}
% \caption{Examples of slot decoder masks on Disctractor Control Suite.}
% \label{fig:slot_examples}
% \end{figure}

\section{Background}

% Here we describe the topics which we combine in this work: object centric learning and lapo.
% \textbf{selected method} here we used videosaur, it worked better than STEVE. just look on the slots [figure]
%  
% Self-supervised Object-Centric Learning for Videos https://arxiv.org/pdf/2310.06907 -- other ocl for video
% Learning to Compose: Improving Object Centric Learning by Injecting Compositionality https://openreview.net/forum?id=HT2dAhh4uV
% SAVi++: Towards End-to-End Object-Centric Learning from Real-World Videos https://slot-attention-video.github.io/savi++/
% Slot-BERT: Self-supervised Object Discovery in Surgical Video https://arxiv.org/pdf/2501.12477
% https://github.com/Jun-Pu/Awesome-object-centric-learning
% Object Scene Representation Transformer https://arxiv.org/abs/2206.06922
% dinosaur \citep{seitzer2023bridginggaprealworldobjectcentric}
% dinov2 \citep{oquab2023dinov2}

\textbf{Learning from observations.} Learning solely from observations (LfO) has emerged recently \citep{mccarthy2024towards} to mimic the success of large-scale pretraining in different domains, like text, as a pathway for scalable embodied foundation models. Early efforts like Video PreTraining (VPT) \citep{Baker2022VideoP} demonstrated the potential of pretraining on internet-scale video data (e.g., Minecraft gameplay) to recover latent policies. However, VPT requires costly action labeling via human demonstrations, limiting its scalability. In \citet{ghosh2023reinforcementlearningpassivedata} the authors proposed modeling latent intentions, representations of various outcomes an agent might aim to achieve, to learn useful features from observations data. 

\begin{wrapfigure}{r}{0.4\textwidth}
\vskip -0.2in
    \centering
    \includegraphics[width=0.4\textwidth]{plots/vsaur_short.pdf}
    \caption{Examples of slot decoder masks on the Distraction Control Suite utilized in our Object-Centric Latent Action Learning pipeline. From top to bottom, the rows correspond to different tasks: cheetah-run, walker-run, hopper-hop, humanoid-walk. From left to right: the distracted observation (background video, color, and camera position variations), the non-distracted observation, the mixture of slot decoder masks obtained after object-centric pretraining, and the main object slot decoder mask selected after object-centric pretraining.}
    \label{fig:slot_examples}
\vskip -0.2in
\end{wrapfigure}
    
Subsequent work shifted to Latent Action Policies (LAPO) \citep{schmidt2024learningactactions}, a combination of forward-dynamics model (FDM) $f_\mathrm{FDM}(\boldsymbol{o}_{t+1}|\boldsymbol{o}_{t}, \boldsymbol{a}_{t})$, which predict future states from current observations, and inverse-dynamics model (IDM) $f_\mathrm{IDM}(\boldsymbol{a}_t|\boldsymbol{o}_t, \boldsymbol{o}_{t+1})$, which infer latent actions from state transitions. FDM and IDM are jointly learned to minimize the next state prediction and further used to label the trajectory of observations with latent actions. As \citet{schmidt2024learningactactions} show,  obtained latent actions can recover ground true actions, however, they assume distractor-free environments, a brittle assumption for real-world video data.

% Learning solely from observations is very interesting but started not so long ago, as behavior cloning approaches required actoion labels. Among first tries was \citep{Baker2022VideoP} ( Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos -- learning from minecraft videos). Then there came up an idea to learn forward-dynamics model [] [], such methods showed promicing results in ... but then some limitations ... were found for them.

% Towards Principled Representation Learning from Videos for  Reinforcement Learning - misra2024principledrepresentationlearningvideos

% Reinforcement Learning from Passive Data via Latent Intentions

\textbf{Learning in noisy setting.} 
Recent work by \citet{wang2023denoisedmdpslearningworld} argues that optimal World Models should suppress such distractors by isolating controllable, reward-relevant factors. There are papers \citep{efroni2022provablerlexogenousdistractors} proving the discovery of true latent states from offline trajectories of observations and actions. Real-world videos inherently contain action-correlated distractors: environmental dynamics (e.g., moving backgrounds, camera jitter) that spuriously correlate with agent actions. However, existing LfO methods lack mechanisms to disentangle distractors \citep{misra2024principledrepresentationlearningvideos}, leading to overfitting in noisy settings.

% tang2020neuroevolution

% [However, this requires access to interactive environments or curated datasets with action/reward labels, which are impractical for web-scale video.] [...more about learning in presentence of distractors...] 

% Guaranteed Discovery of Control-Endogenous Latent States with Multi-Step Inverse Models


% Denoised MDPs: Learning World Models Better Than the World Itself https://arxiv.org/pdf/2206.15477 claims that Optimal control only relies on information that is both controllable and reward-relevant. Good world models should ignore other factors as noisy distractors. However, to identify controllability and reward-relevance the agent should have acess to intercative MDP or at least a curated dataset of trajectories labeled with actions and rewards, which can be improssible or coslty for large-scale datasets.
% Reinforcement Learning under Latent Dynamics: Toward Statistical and Algorithmic Modularity

\textbf{Object-Centric Pretraining for Videos.} Object-centric learning aims to decompose visual scenes into structured, entity-level representations that capture independent objects, i.e. slots. During learning, slots compete with each other in describing the image. Such procedure is named Slot Attention and firstly it was suggested for images \citep{locatello2020objectcentriclearningslotattention}. It was later enhanced in SAVi \citep{kipf2022conditionalobjectcentriclearningvideo} and STEVE \cite{singh2022simpleunsupervisedobjectcentriclearning} to work with videos by iteratively applying slot attention to consecutive frames to ensure stable temporal order of slots through time. However, naive SlotAttention \citep{locatello2020objectcentriclearningslotattention} approaches usually do not scale well to real-world videos. VideoSAUR \citep{zadaianchuk2023objectcentriclearningrealworldvideos} utilizes a large-scale pretrained DIVOv2 \citep{oquab2023dinov2} features, which helps to capture deep concepts instead of low-level RGB and introduces a temporal feature similarity loss, which encodes semantic and temporal correlations between image patches. Such changes bias the model toward discovering moving objects while filtering static distractors. %This enforces motion-aware slot consistency, addressing limitations of recurrent architectures like STEVE , which uses a transformer decoder but lacks explicit temporal regularization due to their reliance on low-level RGB or locally trained vae features. Fig. \Cref{fig:slot_examples}.

% https://arxiv.org/pdf/2204.07756 visual attention review

\section{Method} \label{section:oc-lapo}

% The pipeline for our approach to integrating object-centric representations into latent action pretraining, aiming to enhance robustness against action-correlated distractors in raw video data:

% \subsection{Object-centric latent action pretraining}

% \begin{figure}[t]
% \label{image:schema-of-ocl}
% \begin{center}
% % \framebox[4.0in]{$\;$}
% \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
% % \includegraphics[width=4.0in]{plots/action_probe_loss.pdf}
% % \includegraphics[width=5.0in]{/}
% \end{center}
% \caption{Scheme of our approach.}
% \end{figure}

\textbf{Object-Centric Representation Learning.} We employ the VideoSAUR \citep{zadaianchuk2023objectcentriclearningrealworldvideos} to decompose input video frames into spatio-temporal object slots. This self-supervised model isolates individual entities within a scene, providing structured representations that are less susceptible to background noise and incidental motion. % $$\texttt{here can be some notations from VideoSAUR}$$
In the end of this step we obtain an encoder $F^s$, which directly maps a trajectory of observations $\boldsymbol{O}$ into the trajectory of slots $\boldsymbol{S}$. The number of slots $K$ for each observation $o_t$ in $\boldsymbol{S_t}$ is hyperparameter, fixed at the beginning of the training. The resulting representation $\boldsymbol{S}_t$ of the observation $\boldsymbol{o}_{i}$ is a combination of slot vectors $\boldsymbol{s}_{t}^{k}$, i.e. 
$F^{s}(\boldsymbol{O}) = \boldsymbol{S}$. %\quad F^{m}(\boldsymbol{O})= \Pr {}_{\boldsymbol{O}}\ \boldsymbol{S} = \boldsymbol{M},$$
Due to having a transformer-based slot decoder, slots $\boldsymbol{s}_t^k$ can be projected to initial observation $\boldsymbol{o}_t$ utilizing attention maps as alpha masks, to obtain object masks $\boldsymbol{m}_t^k$. Further we will denote them as masks (see \Cref{fig:slot_examples,plot:videosaur-VS-steve,plot:video-saur-main-object-slots} for a visualization). % $\boldsymbol{M}_t$.
% $\boldsymbol{m}_{t}^k = \mathrm{proj}_{\boldsymbol{o}_t}\boldsymbol{s}_t^k$
%In the end of this step we obtain a function $F_{enc}$, which directly maps a trajectory of observations $\boldsymbol{O}_i = [\boldsymbol{o}_{i1}, \boldsymbol{o}_{i2}, \dots,  \boldsymbol{o}_{iL}]$ into the trajectory of slots $\boldsymbol{S}_i= [\boldsymbol{S}_i^1, \boldsymbol{S}_i^2, \dots,  \boldsymbol{S}_i^L],$
% $$F^{s}(\boldsymbol{O}_i) = \boldsymbol{S}_i ,\quad i\in [0, N],$$
% where the number of slots $K$ in $\boldsymbol{S}_i^j$ is hyperparameter, fixed at the beginning of the training. The resulting representation $\boldsymbol{S}_i^j = [\boldsymbol{s}_{ij}^0, \boldsymbol{s}_{ij}^1, \dots,  \boldsymbol{s}_{ij}^K]$ of the observation $\boldsymbol{o}_{ij}$ is a combination of slot vectors $\boldsymbol{s}_{ij}^{k}$.
%Also, due to having a transformer-based slot decoder we are able to project slots to initial image utilizing attention maps as alpha masks $\boldsymbol{m}_{ij}^k = \Pr_{\boldsymbol{o_{ij}}}\boldsymbol{s}_{ij}^k$. So we are also able to get the similar encoding function for them, further we will denote them as masks $\boldsymbol{M}_i= [\boldsymbol{M}_i^1, \boldsymbol{M}_i^2, \dots,  \boldsymbol{M}_i^L],$ where $\boldsymbol{M}_i^j = [\boldsymbol{m}_{ij}^0, \boldsymbol{m}_{ij}^k, \dots,  \boldsymbol{m}_{ij}^K]$.
% $$F^{m}(\boldsymbol{O}_i)= \Pr {}_{\boldsymbol{O_{i}}}\ \boldsymbol{S}_{i} = \boldsymbol{M}_i ,\quad i\in [0, N],$$

\textbf{Slot Selection.} From the generated object slots, we identify and select those relevant to the agent’s interactions. We plan to address the automatic selection of control-related slots in the future. In the DCS's the control-related objects are the main agent (cheetah, walker, hopper or humanoid) and the floor. Depending on the number of slots and the environment, they can arise in the same or different slots, so if needed, slots can be combined during this stage by concatenation over selected slots or taking mean over selected masks. In the current pipeline slot selection among slot vectors $\boldsymbol{s}_{ij}^*$ is performed based on the corresponding masks $\boldsymbol{m}_{ij}^*$. Due to fixed slots initialization (see details in \Cref{appendix:fixed-init}) it can be done only once for the whole dataset.

\textbf{Latent Action Modeling.} Utilizing the selected object-centric representations, we train a latent action model inspired by the LAPO \citep{schmidt2024learningactactions}. The inverse-dynamics model $\boldsymbol{z}_t \sim f^{s}_\mathrm{IDM}(\cdot|\boldsymbol{s}_t, \boldsymbol{a}_{t+1})$ and the forward-dynamics model $\boldsymbol{\hat{s}}_{t+q} \sim f^{s}_\mathrm{FDM}(\cdot|\boldsymbol{s}_{t}, \boldsymbol{z}_{t})$ are trained to reconstruct the trajectory of slots ${\vert \vert \boldsymbol{\hat{s}}_{t+1}-\boldsymbol{s}_{t+1}\vert \vert}^2$  (or masks). We denote this as \emph{lapo-slots} and, accordingly, \emph{lapo-masks} for masks representations. Thus, lapo-slots reconstruct next observations in latent space, while lapo-masks reconstruct images, which were filtered based on selected slots masks (see \Cref{fig:slot_examples,plot:video-saur-main-object-slots}).

\textbf{Behavior Cloning and Finetuning.} The inferred latent actions serve as proxies for actual action labels. We train a behavior cloning (BC) agent to predict these latent actions, using the same dataset as for latent action learning. To evaluate the pre-training effectiveness, as a final stage, we fine-tune the resulting agent on a limited set of trajectories with ground-truth action labels, in line with \citep{schmidt2024learningactactions, ye2024latentactionpretrainingvideos}. The scores of the BC agent depending on number of the labeled trajectories are presented on the  \Cref{plots:eval_returns}. %This step ensures that the agent refines its policy based on accurate action information, leading to improved task execution.	


\section{Experiments}
% Proof of concept setup

% Here we describe our preliminar problem setting for the task of object-centric latent action learning from raw video and demonstrate quantitative results of it application.

% \subsection{Task formulation} 
% The challenge of distractors. Why we test on DistractorControlSuite? 
% \label{subsec:task}

% As a first milestone for learning agents from raw video data we took The Distracting Control Suite (DCS) \citep{stone2021distractingcontrolsuite}, which is an extension over DM Control environments with 3 kinds of visual distractions: background, color and camera pose. As for the distracting backgrounds, we were using the dataset containing 60 different videos from DAVIS 17 dataset \citep{ponttuset20182017davischallengevideo}. The level of variations of color and camera was adjusted to remain difficult for lapo and still solvable by a behavior cloning strategy trained on the expert labels from PPO algorithm (which is learned on minimal states).

\textbf{Task formulation.} To address the robustness to action-correlated distractors, a critical challenge in learning from raw video data, we evaluate on Distracting Control Suite (DCS) \citep{stone2021distractingcontrolsuite} environment. DSC is an extension of DM Control with three types of real-world visual noise: (1) dynamic backgrounds: 60 diverse videos from DAVIS 2017 \citep{ponttuset20182017davischallengevideo}, simulating realistic environmental clutter; (2) color variations: hue/saturation shifts that preserve object semantics but degrade low-level features; (3) camera perturbations: randomized pose adjustments mimicking handheld camera noise. The scale of color and camera variations is $0.1$. As for the tasks: we selected 4 (in the order of increasing complexity): cheetah-run, walker-run, hopper-hop, humanoid-walk. We trained expert policy optimization agents on trained on privileged state information and collected the dataset with observation-action tuples. Importantly, the level of distractions is tuned so the behavior cloning (BC) agent, trained on privileged true actions, is able to achieve expert score. More details on dataset collection can be found in \Cref{appendix:dataset-collection}. 

% \begin{itemize}
%     \item LAPO: The primary baseline, LAPO trained on distracted observations, represents current limitations of latent action pretraining.
%     \item Clean-LAPO: Upper bound for our method, trained on non-distracted observation data, illustrating the performance gap caused by distractors.
% \end{itemize}

%% ====================================================================

% \subsection{Training details}

\textbf{Training details.} To conduct the experiments 4 models were trained: \emph{lapo}, \emph{lapo-no-distractors}, \emph{lapo-slots} and \emph{lapo-masks}. The baseline model is LAPO, which is trained on observations with distractors, following \citet{schmidt2024learningactactions} procedure (\emph{lapo} on the figures). We use it as a baseline to demonstrate the currently existing limitations of latent action pretraining. We additionally trained LAPO on data without distractors as our second baseline to illustrate the performance gap caused by distractors (\emph{lapo-no-distractors} on the figures). LAPO-slots and LAPO-masks are the models following the object-centric latent action pretraining training pipeline described in the \Cref{section:oc-lapo} (respectively, \emph{lapo-slots} and \emph{lapo-masks} on the figures). All models were trained on the same datasets. More details about training can be found in the \Cref{appendix:training-details}

\subsection{Latent Action Quality}
\label{subsec:lam-quality}

\begin{figure}[t]
    \vskip 0.2in
    \begin{center}
        \centerline{\includegraphics[width=\textwidth]{plots/action_probe_loss.pdf}}
        \caption{
        Mean action probes MSE for varying dimensions of latent actions. 
        TL;DR: Object-centric learning improves action probes for all tasks, especially in cheetah-run and hopper-hop.
        The plots are arranged from left to right in order of increasing complexity of correspoding task. The curves represent the following approaches: \emph{lapo}: baseline latent action pretraining (LAPO) \citep{schmidt2024learningactactions} trained on the Distraction Control Suite (DCS); \emph{lapo-no-distractors}: LAPO trained on non-distracted observation data; \emph{lapo-slots} and \emph{lapo-masks}: models following the object-centric latent action pretraining pipeline described in \Cref{section:oc-lapo}. The result are averaged among 3 random seeds.
        }
        \label{plots:action_probe}
    \end{center}
    \vskip -0.4in
\end{figure}

\begin{table}[t]
\caption{Mean action probes MSE for different tasks. 
TL;DR:  LAPO's performance drops by x5.3 in the presence of distractors. Object-centric learning, using slots and masks, reduces this gap, with notable improvements in tasks like cheetah-run and hopper-hop.
The rows represent corresponding tasks from the Distraction Control Suite (DSC), with the last row showing the average action probe MSE across all tasks. The columns represent the following approaches: \emph{lapo}: baseline latent action pretraining (LAPO) \citep{schmidt2024learningactactions} trained on the DCS; \emph{lapo-no-distractors}: LAPO trained on non-distracted observation data; \emph{lapo-slots} and \emph{lapo-masks}: models following the object-centric latent action pretraining pipeline described in \Cref{section:oc-lapo}. Each value represents the average MSE across 5 different latent action dimensions and 3 random seeds.
}
\label{table:mean-action-probe}
\begin{center}
% \setlength{\tabcolsep}{4pt}
\adjustbox{max width=\textwidth}{
\begin{tabular}{l|r|rr|rr|rrrr}
\toprule
\multicolumn{1}{c}{\bf Task} 
    &\multicolumn{1}{c}{\bf lapo} &\multicolumn{2}{c}{\bf lapo-masks (ours)} 
    &\multicolumn{2}{c}{\bf lapo-slots (ours)} &\multicolumn{2}{c}{\bf lapo-no-distractors} 
\\ 
\midrule
cheetah-run & 11.2 $\pm$ 6.9 &2.8 $\pm$ 1.0       & \colorbox{cyan!20}{x4.0}& 3.1 $\pm$ 0.9  &\colorbox{cyan!20}{x3.6} & 1.0 $\pm$ 0.6     & x12.0 \\
walker-run & 7.0 $\pm$ 3.2 &4.5 $\pm$ 1.7         & \colorbox{cyan!20}{x1.6} &5.3 $\pm$ 2.2   &\colorbox{cyan!20}{x1.3}  & 2.7 $\pm$ 1.3    & x2.6 \\ 
hopper-hop & 11.2 $\pm$ 4.9 & 3.5 $\pm$ 1.2       & \colorbox{cyan!20}{x3.2} &4.4 $\pm$ 1.8   &\colorbox{cyan!20}{x2.5}  & 1.9 $\pm$ 1.2    & x5.8 \\ 
humanoid-walk & 0.20 $\pm$ 0.04 & 0.18 $\pm$ 0.04 & \colorbox{cyan!20}{x1.1} &0.16 $\pm$ 0.04 &\colorbox{cyan!20}{x1.3}  &  0.13 $\pm$ 0.03 & x1.6 \\ 
\midrule
average & 7.4 & 2.7 & \colorbox{cyan!20}{x2.7} & 3.3 & \colorbox{cyan!20}{x2.5} & 1.4 & x5.3 \\
\bottomrule
\end{tabular}}
\end{center}
\vskip -0.1in
\end{table}

% \begin{table}[t]
% \caption{Mean action probes MSE for different tasks. The rows represent corresponding tasks from the Distraction Control Suite (DSC), with the last row showing the average action probe MSE across all tasks. The columns represent the following approaches: 'lapo': baseline latent action pretraining (LAPO) \citep{schmidt2024learningactactions} trained on the  DCS; 'lapo-no-distractors': LAPO trained on non-distracted observation data; 'lapo-slots' and 'lapo-masks': models following the object-centric latent action pretraining pipeline described in \Cref{section:oc-lapo}.
% Each value represents the average MSE across 5 different latent action dimensions and 3 random seeds.
% }
% \label{table:mean-action-probe}
% \begin{center}
% % \setlength{\tabcolsep}{4pt}
% \adjustbox{max width=\textwidth}{
% \begin{tabular}{l|r|rr|rr|rrrr}
% \toprule
% \multicolumn{1}{c}{\bf Task} 
%     &\multicolumn{1}{c}{\bf lapo} &\multicolumn{2}{c}{\bf lapo-masks (ours)} 
%     &\multicolumn{2}{c}{\bf lapo-slots (ours)} &\multicolumn{2}{c}{\bf lapo-no-distractors} 
% \\ 
% \midrule
% %            task_  masks-vs-lapo  slots-vs-lapo  no-distraction-vs-lapo
% % 0    cheetah-run       4.006120       3.590286               11.650915
% % 3     walker-run       1.569831       1.317752                2.581003
% % 1     hopper-hop       3.190084       2.517685                5.783559
% % 2  humanoid-walk       1.133421       1.264283                1.594804
% cheetah-run & 11.2 $\pm$ 6.9 &2.8 $\pm$ 1.0       & \scalebox{0.8}{$\downarrow$} \colorbox{cyan!20}{x4.0}& 3.1 $\pm$ 0.9  &\scalebox{0.8}{$\downarrow$} \colorbox{cyan!20}{x3.6} & 1.0 $\pm$ 0.6     &\scalebox{0.8}{$\downarrow$} x12.0 \\
% walker-run & 7.0 $\pm$ 3.2 &4.5 $\pm$ 1.7         & \scalebox{0.8}{$\downarrow$} \colorbox{cyan!20}{x1.6} &5.3 $\pm$ 2.2   &\scalebox{0.8}{$\downarrow$} \colorbox{cyan!20}{x1.3}  & 2.7 $\pm$ 1.3    &\scalebox{0.8}{$\downarrow$} x2.6 \\ 
% hopper-hop & 11.2 $\pm$ 4.9 & 3.5 $\pm$ 1.2       & \scalebox{0.8}{$\downarrow$} \colorbox{cyan!20}{x3.2} &4.4 $\pm$ 1.8   &\scalebox{0.8}{$\downarrow$} \colorbox{cyan!20}{x2.5}  & 1.9 $\pm$ 1.2    &\scalebox{0.8}{$\downarrow$} x5.8 \\ 
% humanoid-walk & 0.20 $\pm$ 0.04 & 0.18 $\pm$ 0.04 & \scalebox{0.8}{$\downarrow$} \colorbox{cyan!20}{x1.1} &0.16 $\pm$ 0.04 &\scalebox{0.8}{$\downarrow$} \colorbox{cyan!20}{x1.3}  &  0.13 $\pm$ 0.03 &\scalebox{0.8}{$\downarrow$} x1.6 \\ 
% \midrule
% average & 7.4 & 2.7 & \scalebox{0.8}{$\downarrow$} \colorbox{cyan!20}{x2.7} & 3.3 & \scalebox{0.8}{$\downarrow$} \colorbox{cyan!20}{x2.5} & 1.4 & \scalebox{0.8}{$\downarrow$} x5.3 \\
% \bottomrule
% %       mean_orig_mean  mean_orig_std  mean_masks_mean  mean_masks_std  \
% % mean        7.402232       3.782005         2.740569        0.993732   
% % std         5.179797       2.910699         1.842492        0.702957   
% %       masks-vs-lapo  mean_slots_mean  mean_slots_std  slots-vs-lapo  \
% % mean       2.474864         3.265205        1.250060       2.172501   
% % std        1.350906         2.262229        0.981822       1.108260   
% %       mean_vanilla_mean  mean_vanilla_std  no-distraction-vs-lapo  
% % mean           1.437362          0.766513                5.402570  
% % std            1.133342          0.587233                4.533111 
% % cheetah-run & 11.2 $\pm$ 6.9 &2.8 $\pm$ 1.0 &       +82\%& 3.1 $\pm$ 0.9 &    +79\% & 1.0 $\pm$ 0.6  \\
% % walker-run & 7.0 $\pm$ 3.2   &4.5 $\pm$ 1.7 &       +59\% &5.3 $\pm$ 2.2 &    +39\% & 2.7 $\pm$ 1.3 \\ 
% % hopper-hop & 11.2 $\pm$ 4.9  & 3.5 $\pm$ 1.2 &      +83\% &4.4 $\pm$ 1.8 &    +73\% & 1.9 $\pm$ 1.2  \\ 
% % humanoid-walk & 0.20 $\pm$ 0.04 & 0.18 $\pm$ 0.04 & +32\%&  0.16 $\pm$ 0.04 & +56\%&  0.13 $\pm$ 0.03 \\ 
% \end{tabular}}
% \end{center}
% \end{table}

To quantify the quality of obtaining latent actions we employed linear probing of latent actions against ground-truth expert actions. Such probe quantifies how well latent actions capture causal dynamics. The quantitative results of mean action probes are presented on the \Cref{plots:action_probe} and in the \Cref{table:mean-action-probe}. The results are averaged among 3 seeds.

\textbf{Object-slots reduce the gap caused by the distractors.}
The experiments showed a significant $\mathbf{x5.3}$ gap between the quality of latent actions on non-distracted and distracted observations. On all tasks the gap is reduced by the integration of object-centric leaning at least twice on slots or masks. The best improvements are observed on cheetah-run, 78\% on slots  and 82\% on masks, and hopper-hop, 73\% on slots and 83\% on masks. Moreover, on the average, object-centric learning reduced mean action probes by $\mathbf{x2.7}$ on masks and $\mathbf{x2.5}$ on slots.

% \textbf{Task-Specific Insights}:
% Masks perform better than slots on all tasks except humanoid, probably, some information is lost in slots. On humanoid-walk: slots outperform masks by 14\%, highlighting the need for explicit object decomposition in complex scenes.

% Probably this also shows that our action probe slighly overfits on masks.

%% -------------------------------------

\subsection{Downstream Performance} \label{subsec:eval-returns}

\begin{figure}[t]
    \vskip 0.2in
    \begin{center}
        \centerline{\includegraphics[width=\textwidth]{plots/histogram-eval.pdf}}
        \caption{
        Normalized evaluation returns of the BC agent trained on latent actions for varying numbers of fine-tuning labeled trajectories. 
        TL;DR: Object-centric learning improves evaluation returns for all tasks, showing high sample efficiency on tasks like cheetah-run and hopper-hop.
        The plots are arranged from left to right in order of increasing task complexity. The curves represent the following approaches: \emph{lapo}: baseline latent action pretraining (LAPO) \citep{schmidt2024learningactactions} trained on the Distraction Control Suite (DCS); \emph{lapo-no-distractors}: LAPO trained on non-distracted observation data; \emph{lapo-slots} and \emph{lapo-masks}: models following the object-centric latent action pretraining pipeline described in \Cref{section:oc-lapo}. The horizontal lines labeled \emph{lapo-slots limit} and \emph{lapo-masks limit} denote the scores of the corresponding BC agents fine-tuned on the entire training dataset of 5k trajectories. The values are averaged across three random seeds.
        The BC agent trained with access to the full dataset of ground-truth actions would return a score of 1 for each task. 
        }
        \label{plots:eval_returns}
    \end{center}
    \vskip -0.2in
\end{figure}

\begin{table}[t]
\caption{Normalized evaluation returns of the BC agent trained on latent actions for different tasks. 
TL;DR:  LAPO's performance drops by x3.9 in the presence of distractors. Object-centric learning, using slots and masks, reduces this gap.
The rows represent corresponding tasks from the Distraction Control Suite (DSC), with the last row showing the average action probe MSE across all tasks. The columns represent the following approaches: \emph{lapo}: baseline latent action pretraining (LAPO) \citep{schmidt2024learningactactions} trained on the Distraction Control Suite (DCS); \emph{lapo-no-distractors}: LAPO trained on non-distracted observation data; \emph{lapo-slots} and \emph{lapo-masks}: models following the object-centric latent action pretraining pipeline described in \Cref{section:oc-lapo}. Each value represents the average MSE across three different amounts of fine-tuning labeled trajectories and three random seeds. The values are scaled by 100, so the BC agent trained with access to the full dataset of ground-truth actions would achieve a score of 100 for each task. }
\label{table:eval-returns}
\begin{center}
    \adjustbox{max width=\textwidth}{
    \begin{tabular}{l|r|rr|rr|rrrr}
    \toprule
    \multicolumn{1}{c}{\bf Task} 
        &\multicolumn{1}{c}{\bf lapo} &\multicolumn{2}{c}{\bf lapo-masks (ours)} 
        &\multicolumn{2}{c}{\bf lapo-slots (ours)} &\multicolumn{2}{c}{\bf lapo-no-distractors} \\
    \midrule
    cheetah-run & 23.0 $\pm$ 6.0 & 39.0 $\pm$ 1.0 & \colorbox{cyan!20}{x1.7} & 50 $\pm$ 1.0    & \colorbox{cyan!20}{x2.2} & 69.0 $\pm$ 14.0   & x3.1\\
    walker-run & 4.0 $\pm$ 0.7 &  7.4 $\pm$ 2.8  & \colorbox{cyan!20}{x1.8} & 8.4 $\pm$ 3.0 & \colorbox{cyan!20}{x2.1} & 22.0 $\pm$ 19.0   & x5.5\\
    hopper-hop & 3.2 $\pm$ 1.5 & 10.8 $\pm$ 0.6  & \colorbox{cyan!20}{x3.4} & 16 $\pm$ 3.0 & \colorbox{cyan!20}{x5.0} & 25.0 $\pm$ 9.0    & x7.7\\
    humanoid-walk & 0.8 $\pm$ 0.4 & 5.9 $\pm$ 0.7& \colorbox{cyan!20}{x7.0} & 6.0 $\pm$ 0.2 & \colorbox{cyan!20}{x7.0} & 3.8 $\pm$ 3.1 & x4.5\\
    \midrule
    average & 7.7 & 16 & \colorbox{cyan!20}{x2.0} & 20 & \colorbox{cyan!20}{x2.6} & 30 & x3.9 \\
    \bottomrule
    \end{tabular}}
\end{center}
\vskip -0.1in
\end{table}

% \begin{table}[t]
% \caption{Normalized evaluation returns of the BC agent trained on latent actions for different tasks. The rows represent corresponding tasks from the Distraction Control Suite (DSC), with the last row showing the average action probe MSE across all tasks. The columns represent the following approaches: 'lapo', which is the baseline latent action pretraining (LAPO) \citep{schmidt2024learningactactions} trained on the DCS; 'lapo-no-distractors', which is LAPO trained on non-distracted observation data; and 'lapo-slots' and 'lapo-masks', which are models following the object-centric latent action pretraining pipeline described in Section \ref{section:oc-lapo}. Each value represents the average MSE across three different amounts of fine-tuning labeled trajectories and three random seeds. The values are scaled by 100, so the BC agent trained with access to the full dataset of ground-truth actions would return a score of 100 for each task.}
% \label{table:eval-returns}
% \begin{center}
%     \adjustbox{max width=\textwidth}{
%     \begin{tabular}{l|r|rr|rr|rrrr}
%     \toprule
%     \multicolumn{1}{c}{\bf Task} 
%         &\multicolumn{1}{c}{\bf lapo} &\multicolumn{2}{c}{\bf lapo-masks (ours)} 
%         &\multicolumn{2}{c}{\bf lapo-slots (ours)} &\multicolumn{2}{c}{\bf lapo-no-distractors} \\
%     \midrule
%     cheetah-run & 23.0 $\pm$ 6.0 & 39.0 $\pm$ 1.0 & \scalebox{0.8}{$\uparrow$} \colorbox{cyan!20}{x1.7} & 50 $\pm$ 1.0    & \scalebox{0.8}{$\uparrow$} \colorbox{cyan!20}{x2.2} & 69.0 $\pm$ 14.0   & \scalebox{0.8}{$\uparrow$} x3.1\\
%     walker-run & 4.0 $\pm$ 0.7 &  7.4 $\pm$ 2.8  & \scalebox{0.8}{$\uparrow$} \colorbox{cyan!20}{x1.8} & 8.4 $\pm$ 3.0 & \scalebox{0.8}{$\uparrow$} \colorbox{cyan!20}{x2.1} & 22.0 $\pm$ 19.0   & \scalebox{0.8}{$\uparrow$} x5.5\\
%     hopper-hop & 3.2 $\pm$ 1.5 & 10.8 $\pm$ 0.6  & \scalebox{0.8}{$\uparrow$} \colorbox{cyan!20}{x3.4} & 16 $\pm$ 3.0 & \scalebox{0.8}{$\uparrow$} \colorbox{cyan!20}{x5.0} & 25.0 $\pm$ 9.0    & \scalebox{0.8}{$\uparrow$} x7.7\\
%     humanoid-walk & 0.8 $\pm$ 0.4 & 5.9 $\pm$ 0.7& \scalebox{0.8}{$\uparrow$} \colorbox{cyan!20}{x7.0} & 6.0 $\pm$ 0.2 & \scalebox{0.8}{$\uparrow$} \colorbox{cyan!20}{x7.0} & 3.8 $\pm$ 3.1 & \scalebox{0.8}{$\uparrow$} x4.5\\
%     \midrule
%     average & 7.7 & 16 & \scalebox{0.8}{$\uparrow$} \colorbox{cyan!20}{x2.0} & 20 & \scalebox{0.8}{$\uparrow$} \colorbox{cyan!20}{x2.6} & 30 & \scalebox{0.8}{$\uparrow$} x3.9 \\
%     \bottomrule
%     \end{tabular}}
% \end{center}
% \end{table}

% \begin{table}[t]
% \caption{Eval returns. The corresponding BC agent return for each task would be 100.}
% \label{table:eval-returns}
% \begin{center}
%     \begin{tabular}{l|r|rr|rr|rrrr}
%     \toprule
%     \multicolumn{1}{c}{\bf Task} 
%         &\multicolumn{1}{c}{\bf lapo} &\multicolumn{2}{c}{\bf lapo-masks (ours)} 
%         &\multicolumn{2}{c}{\bf lapo-slots (ours)} &\multicolumn{2}{c}{\bf lapo-no-distractors} \\
%     \midrule
%     cheetah-run & 23.0 $\pm$ 6.0 & 39.0 $\pm$ 1.0  & x1.7 & 50.0 $\pm$ 1.0  & x2.2 & 69.0 $\pm$ 14.0   & x3.1\\
%     walker-run & 4.0 $\pm$ 0.7 &  7.4 $\pm$ 2.8  & x1.8 & 8.4 $\pm$ 3.0 & x2.1 & 22.0 $\pm$ 19.0   & x5.5\\
%     hopper-hop & 3.2 $\pm$ 1.5 & 10.8 $\pm$ 0.6  & x3.4 & 16.0 $\pm$ 3.0& x5.0 & 25.0 $\pm$ 9.0    & x7.7\\
%     humanoid-walk & 0.8 $\pm$ 0.4 & 5.9 $\pm$ 0.7& x7.0 & 6.0 $\pm$ 0.2 & x7.0 & 3.8 $\pm$ 3.1 & x4.5\\
%     \midrule
%     average & 7.7 $\pm$ 10.0 & 16.0 $\pm$ 16.0 & x3.5 & 20.0 $\pm$ 20.0 & x4.1 & 30.0 $\pm$ 28.0 & x5.2 \\
%     \bottomrule
%     \end{tabular}
% \end{center}
% \end{table}

To evaluate the quality of the resulting agents we measure episodic return of a behavior cloning agent, trained on the obtained latent actions and fine-tuned on 32–128 trajectories (1k transitions) with ground-truth actions. Small size of the action-labeled sample is critical for real-world deployment. Each agent is evaluated over 25 episodes in the environment.
The scores on the \Cref{plots:eval_returns} are normalized by the performance of BC trained on the full datasets with all action labels revealed (see \Cref{table:ppo-bc-scores} in the Appendix). The scores on the \Cref{table:eval-returns} are scaled by 100. The quantitative results of evaluation retuns are presented on the \Cref{plots:eval_returns} and in the \Cref{table:eval-returns}. The result are averaged among 3 random seeds.

% \textbf{RL-agent Eval Return for different amount of labeled trajectories}

\textbf{LAPO struggle in the precense of distractors.} The gap between LAPO trained in distracted and non-distracted setting is $\mathbf{x3.9}$ among all tasks, which shows the importance a technique, that can filter out the distraction. 

% The experiments showed a significant $\mathbf{x5.4}$ gap between the quality of latent actions on non-distracted and distracted observations. On all tasks the gap is reduced by the integration of object-centric leaning at least twice on slots or masks. The best improvements are observed on cheetah-run, 78\% on slots  and 82\% on masks, and hopper-hop, 73\% on slots and 83\% on masks. Moreover, on the average, object-centric learning reduced mean action probes by $\mathbf{x2.2}$ on masks and $\mathbf{x2.7}$ on slots.
\textbf{Object-slots reduce the gap caused by distractors.} Following the results in the \Cref{subsec:lam-quality} slots and masks outperform baseline lapo for all 4 tasks: by at least $\mathbf{x2.1}$ for slots ($\mathbf{x2.6}$ on the average), and at least $\mathbf{x1.7}$ for masks ($\mathbf{x2.0}$ on the average). The maximal improvement os observed on humanoid-walk: $\mathbf{x7}$ for both slots and masks, while improvement between non-distracted and distracted lapo is $\mathbf{x4.5}$.

\textbf{For easy tasks slots show higher sample efficiency}. Horizontal lines on \Cref{plots:eval_returns} denote the evaluation score of an average corresponding BC agent finetuned on the whole training dataset of trajectories. The figure shows that for cheetah-run only 32 trajectories and 128 trajectories for hopper-hop are enough to achieve this limit of possible evaluation score. This explicitly shows the applicability of the object-centric pretraining for LAM in real-world scenario of limited access to labeled datasets.

\textbf{Slots perfrom better than masks.} Even though \Cref{plots:action_probe} shows that latent action quality for masks is slightly better than for slots (2.7 for masks versus 3.3 for slots), evaluation in the environment shows that slots were able to produce latent actions more amenable to pretraining, resulting in 20 \% better returns, than masks (16 for masks versus 20 for slots). 

% Comparing the result of action probes to the result of eval returns (\Cref{plots:eval_returns}) of learned agent we have to admit the sad truth -- even though slots are much better than our baseline everywhere, except walker, they are currently not able to reach the performance of clean lapo, which is also quite low.


\section{Conclusion \& Limitations}
% \section{Conclusion \& Future work}
% \section{Discussion \& Future work}
% \section{Conclusion}

% We found out that slots contain meaningful information that is naturally isolated from distractors


% Slots definitely help, but are them enough? How to select the needed slot? Seems that we will never overcome a need to clean-up the data, but we can try to make this process easier and cheeper. The cleaner data -- the easier model and less data is needed, more data -- worse data -- more cleaning is needed, but more generalization, more foundation and more opportunity. Very promising thing about slots is the fact that their representations are meaningful, causal and interpretable.
% Future work: the model able to naturally isolate task-relevant objects and to reason about interactions between independent components.

Our preliminary results demonstrate that object-centric representations significantly mitigate the impact of distractors for learning latent actions from videos. By disentangling scene elements into meaningful, interpretable slots, latent actions focus on causal dynamics rather than spurious correlations. This could provide a strong inductive bias for more effective Latent Action Models in noisy environments.

However, challenges remain. While object slots reduce noise, selecting task-relevant slots is still ambiguous. Without explicit supervision, models may focus on irrelevant elements. This points to a fundamental limitation in current object-centric methods: the difficulty in dynamically attending to the “right” objects across varying tasks and environments. Another limitation is the dependency on the quality of training data. Cleaner, well-curated datasets naturally lead to more robust representations, whereas large-scale uncurated video data necessitates additional mechanisms for filtering out noise. This presents a trade-off between data volume, model complexity, and data cleaning efforts. While curated data simplifies training, it limits generalization and scalability. On the other hand, noisier datasets require more sophisticated models but unlock broader generalization capabilities for embodiead AI.

% One particularly promising finding is the inherent interpretability and causal nature of slot-based representations. This opens new opportunities for building transparent and debuggable world models, allowing practitioners to understand and intervene in the learned dynamics.

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\appendix
\newpage

% \section{Appendix}
% You may include other additional sections here.

\section{Comparing STEVE and VideoSAUR slot projections}
\label{app:steve}

We started from STEVE \citep{singh2022simpleunsupervisedobjectcentriclearning} model, as one of widely used and promising approaches. However, STEVE wasn't able to select the hopper on the images. You can compare STEVE's and VideoSAUR's slot projections on the  \Cref{plot:videosaur-VS-steve}. We also note, that STEVE is mostly identical to SAVi \citep{kipf2022conditionalobjectcentriclearningvideo}, differentiated only by the fact that it uses transformer-based decoder instead of pixel-mixture decoder.

\begin{figure}[h]
\begin{center}
% \framebox[4.0in]{$\;$}
% \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
% \includegraphics[width=3.9in]{plots/vsaur_steve.pdf}
\includegraphics[width=4.0in]{plots/vsaur_steve_1.pdf}
\caption{Examples of slot projections on DCS hopper-hop. From left to right: clean image without distractors, image with distractors, STEVE's slots projections, the main object's slot projection by STEVE, VideoSAUR's slot projections, the main object's slot by VideoSAUR}
\label{plot:videosaur-VS-steve}
\end{center}
\end{figure}


% \section{Examples of VideoSAUR slot projections (all slots)}

% \begin{figure}[h]
% \label{plot:video-saur-all-slots}
% \begin{center}
% % \framebox[4.0in]{$\;$}
% \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
% % \includegraphics[width=4.0in]{plots/action_probe_loss.pdf}
% % \includegraphics[width=5.0in]{plots/action_probe_loss.pdf}
% \end{center}
% \caption{Examples of VideoSAUR slot projections (all slots)}
% \end{figure}


\section{Dataset collection} \label{appendix:dataset-collection}
% The dataset of the observation-state-action tuples of 5k trajectories of 1k steps length $\left(o, s, a\right)$, expert trajectories is collected by a policy optimization agent (SAC for humanoid, PPO for other tasks), trained on privileged state information. DCS provides a controlled yet challenging benchmark: on the observations the distractions are tuned to be solvable by a behavior cloning agent trained on the true actions labels but catastrophic for methods like LAPO that lack explicit noise suppression. See the scores of bc and ppo on table table \Cref{table:ppo-bc-scores} in appendix.
The datasets were collected via experts policies trained on Disctracting Control Suite via PPO (for cheetah-run, walker-run and hopper-hop) and SAC (for humanoid-walk). The scores of the experts are presented on the \Cref{table:ppo-bc-scores}. The final dataset of transitions for each tasks consists of 5k trajectories (1k transitions each). The observations in the dataset have height and width 64px.


\begin{table}[h]
\caption{Comparison of the Performance of Algorithms. \emph{Expert} denotes  the policy used to collect the dataset trained on privileged information about minimal state of the observation, PPO (for cheetah-run, walker-run and hopper-hop) and SAC (for humanoid-walk). \emph{BC Vanilla} denotes the scores of behavior cloning agents (BC) trained on full expert dataset to imitate expert policy on the privileged for our method true action labels and non-distracted observations. \emph{BC} denotes the scores of BC agents trained on full expert dataset to imitate expert policy on the distracted observations and privileged for our method true action labels.}
\label{table:ppo-bc-scores}
\begin{center}
\begin{tabular}{l|l|l|l}
\toprule
\bf Task & \bf Expert & \bf BC no-distraction &\ \bf BC
\\ 
\midrule
cheetah-run & 838 & 840 & 823 \\
walker-run & 740 & 735 & 749 \\
hopper-hop & 307 & 300 & 253 \\
humanoid-walk & 617 & 601 & 428 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}


\section{Training details} \label{appendix:training-details}

All experiments were run in H100 GPU. The models are trained in bfloat16 precision. Traning duration is shown on \Cref{table:training-duration}

Object-centric learning pretraining codebased was adopted from VideoZaur \citet{zadaianchuk2023objectcentriclearningrealworldvideos}. It utilizes DINOv2 \citep{oquab2023dinov2} pretrained encoder vit-base-patch8-224-dino from Timm \citep{rw2019timm} models hub. The images in the dataset are upscaled for dino encoder up to 224px. The hyperparameters for object-centric pretraining can be found on \Cref{table:hyper-ocl}

Latent action learning model for images and object-centric masks is formed from IDM and FDM models based on ResNet-like CNN encoder and decoder. The hyperparameters for latent action learning from images (used for lapo, lapo-masks) can be found on \Cref{table:hyper-lapo-masks}

Latent action learning model for object-centric slots is formed from IDM and FDM based on 3-layer MLP blocks with residual connections and GeLU activations to effectively process vector representations. The hyperparameters for latent action learning from representations (used for lapo-slots) can be found on \Cref{table:hyper-lapo-masks}



\begin{table}[h]
\caption{Amount of parameters for different models. The rows represent the following approaches:
\emph{ocp}: denotes the number of parameters of the object-centric model;
\emph{lapo-slots}: denotes the number of parameters for latent action learning from vector representations; utilizing object-centric representations from a precollected dataset;
\emph{lapo}: denotes the number of parameters for latent action learning from images;
\emph{lapo-masks}: denotes the number of parameters for latent action learning from images, utilizing object-centric masks from a precollected dataset (using the same model as \emph{lapo});
\emph{bc}: Denotes the number of parameters of the behavior cloning agent trained on latent actions.
}
\label{table:training-duration}
\begin{center}
\begin{tabular}{l|r}
\toprule
\bf Method & \bf Number of parameters
\\ 
\midrule
ocp (total) & 92,149,776 \\
ocp (trainable)&  6,343,440 \\
\midrule
lapo-slots & 89,186,432\\
lapo-masks & 211,847,849\\
lapo & 211,847,849  \\
\midrule
bc & 107,541,504\\
\bottomrule
\end{tabular}
\end{center}
\end{table}


\begin{table}[h]
\caption{Average training duration of the methods. The row represent the following approaches: \emph{ocp} denotes the time spent on object-centric pretraining stage which is common for both slots and masks; \emph{lapo-masks} and \emph{lapo-slots} denote the time spent on training latent action model, reading object-centric representations from a precollected dataset; \emph{lapo} denote the time spent on training latent action model, \emph{ocp + lapo-masks} and \emph{ocp + lapo-slots} denote the time for full pipeline of object-centric latent action learning}
\label{table:training-duration}
\begin{center}
\begin{tabular}{l|r}
\toprule
\bf Method & \bf Training Duration\\ 
\midrule
ocp & $\sim$ 6 h 35 m  \\
% lapo-slots & $\sim$ 5 h 31 m\\
% lapo-masks & $\sim$ 10 h 40 m\\
lapo-slots & $\sim$ 2 h 29 m\\
lapo-masks & $\sim$ 7 h 38 m\\
\midrule
lapo & $\sim$ 7 h 38 m  \\
% lapo & $\sim$ 10 h 40 m  \\
% ocp + lapo-slots & $\sim$ 12 h 6 m\\
% ocp + lapo-masks & $\sim$ 17 h 15 m\\
ocp + lapo-slots & $\sim$ 9 h 4 m\\
ocp + lapo-masks & $\sim$ 14 h 13 m\\
\midrule
bc + finetuning & $\sim$ 3 h 2 m\\
\bottomrule
\end{tabular}
\end{center}
\end{table}

% \textbf{Amounts of parameters of different models}


\section{Fixed Initialization for Slot Stability} \label{appendix:fixed-init}
To mitigate slot permutation variance across predictions, we introduce a fixed slot initialization scheme that learns deterministic initial slot vectors while preserving robustness. Unlike standard Gaussian initialization, which samples slots stochastically at each step, our approach learns per-slot parameters (mean $\boldsymbol{\mu}_k \in \mathbb{R}^d$ and variance $\boldsymbol{\sigma}_k \in \mathbb{R}^d$) during training. During training, we inject controlled noise scaled by the learned variance into the slot initializations, acting as a regularizer to encourage robust feature disentanglement. At inference, slots are initialized deterministically using the learned means, ensuring consistent slot-object assignments. This hybrid strategy bridges the gap between training stability and inference consistency: the noise-augmented training phase prevents overfitting to fixed initializations, while the deterministic inference phase enables efficient object-wise slot selection via decoder masks as visual priors. %See examples in appendix ...
$$
\texttt{Train:} \quad \boldsymbol{s}_k^{\text{(init)}} = \boldsymbol{\mu}_k + \boldsymbol{\sigma}_k \odot \boldsymbol{\epsilon}, \quad \boldsymbol{\epsilon} \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol{I}), \quad \texttt{Inference:} \quad
\boldsymbol{s}_k^{\text{(inference)}} = \boldsymbol{\mu}_k.
$$

\newpage
\section{Examples of VideoSAUR slot projections}
\begin{figure}[h]
\begin{center}
% \framebox[4.0in]{$\;$}
% \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
% \includegraphics[width=4.0in]{plots/action_probe_loss.pdf}
\includegraphics[width=3.7in]{plots/vsaur_long.pdf}
\caption{Examples of VideoSAUR slot projections on DCS for 4 tasks: (from upper to lower) cheetah-run, hopper-hop, walker-run, humanoid-walk. From left to right: distracted observation, clean observation, observation with segments of slot projections, slot projection (we call it "mask") of the main object}
\label{plot:video-saur-main-object-slots}
\end{center}
\end{figure}

% \begin{table}
% \caption{helo}
% \label{table:training-duration}
% \begin{center}
% \begin{tabular}{l|l}
% \toprule
% \bf Hyperparameter & \bf Training Duration\\ 
% \midrule
%  Method & Training Duration\\ 
% \bf Method & \bf Training Duration\\ 
% \bf Method & \bf Training Duration\\ 
% \bottomrule
% \end{tabular}
% \end{center}
% \end{table}

\begin{table}
\caption{Object-centric pretraining hyperparams. Number of slots varies across tasks: 4 for cheetah-run, walker-run, hopper-hop and 8 for humanoid-walk}
\label{table:hyper-ocl}
\begin{center}
\begin{tabular}{l|l}
\toprule
\bf Hyperparameter & \bf Value\\ 
\midrule
Episode Length & 3 \\ \hline
Image Size Dataset & 64 \\ \hline
Image Size Resize & 224 \\ \hline
Max Steps & 100000 \\ \hline
Number of Slots & 4 \\ \hline
Batch Size & 256 \\ \hline
Warmup Steps & 2500 \\ \hline
Weight Decay & 0 \\ \hline
Max Video Length & 1000 \\ \hline
Gradient Clip Value & 0.05 \\ \hline
Slot Dimension & 128 \\ \hline
Vision Transformer Model & vit\_base\_patch8\_224\_dino \\ \hline
Feature Dimension & 768 \\ \hline
Number of Patches & 784 \\ \hline
Batch Size per GPU & 128 \\ \hline
Total Batch Size  & 128 \\ \hline
Similarity Temperature  & 0.075 \\ \hline
Similarity Weight  & 0.1 \\ \hline
Base Learning Rate  & 0.0001 \\ \hline
Learning Rate & 0.0003 \\ \hline
Learning Rate Scheduler & exp\_decay\_with\_warmup \\ \hline
Warmup Steps & 2500 \\ \hline
Decay Steps & 100000 \\ \hline
\bottomrule
\end{tabular}
\end{center}
\end{table}


\begin{table}
\caption{Hyperparameters for latent action learning from vector representations (used for \emph{lapo-slots}).}
\label{table:hyper-lapo-slots}
\begin{center}
\begin{tabular}{l|l}
\toprule
\bf Hyperparameter & \bf Value\\ 
\midrule
\multicolumn{2}{c}{\bf Latent Action Learning} \\ \hline
Batch Size & 8192 \\ \hline
Hidden Dimension & 1024 \\ \hline
Number of Epochs & 30 \\ \hline
Frame Stack & 1 \\ \hline
Weight Decay & 0 \\ \hline
Learning Rate & 0.00005 \\ \hline
Warmup Epochs & 3 \\ \hline
Future Observation Offset & 10 \\ \hline
Latent Action Dimension & 8192 \\ \hline
\midrule
\multicolumn{2}{c}{\bf BC} \\ \hline
Dropout & 0 \\ \hline
Use Augmentation & False \\ \hline
Evaluation Seed  & 0 \\ \hline
Batch Size & 512 \\ \hline
Number of Epochs & 10 \\ \hline
Frame Stack & 3 \\ \hline
Encoder Deep & False \\ \hline
Weight Decay & 0 \\ \hline
Encoder Scale & 32 \\ \hline
Evaluation Episodes & 5 \\ \hline
Learning Rate & 0.0001 \\ \hline
Warmup Epochs & 0 \\ \hline
Encoder Number of Residual Blocks & 2 \\ \hline
\midrule
\multicolumn{2}{c}{\bf BC finetuning} \\ \hline
Use Augmentation & False \\ \hline
Batch Size & 512 \\ \hline
Hidden Dimension & 256 \\ \hline
Weight Decay & 0 \\ \hline
Evaluation Episodes & 25 \\ \hline
Learning Rate & 0.0003 \\ \hline
Total Updates & 2500 \\ \hline
Warmup Epochs & 0 \\ \hline
\bottomrule
\end{tabular}
\end{center}
\end{table}




\begin{table}
\caption{Hyperparameters for latent action learning from images (used for \emph{lapo}, \emph{lapo-masks}).}
\label{table:hyper-lapo-masks}
\begin{center}
\begin{tabular}{l|l}
\toprule
\bf Hyperparameter & \bf Value\\ 
\midrule
\multicolumn{2}{c}{\bf Latent Action Learning} \\ \hline
Batch Size & 512 \\ \hline
Number of Epochs & 10 \\ \hline
Frame Stack & 3 \\ \hline
Encoder Deep & False \\ \hline
Weight Decay & 0 \\ \hline
Encoder Scale & 6 \\ \hline
Learning Rate & 0.00005 \\ \hline
Warmup Epochs & 3 \\ \hline
Future Observation Offset & 10 \\ \hline
Latent Action Dimension & 1024 \\ \hline
Encoder Number of Residual Blocks & 2 \\ \hline
\midrule
\multicolumn{2}{c}{\bf BC} \\ \hline
Dropout & 0 \\ \hline
Use Augmentation & False \\ \hline
Evaluation Seed & 0 \\ \hline
Batch Size & 512 \\ \hline
Number of Epochs & 10 \\ \hline
Frame Stack & 3 \\ \hline
Encoder Deep & False \\ \hline
Weight Decay & 0 \\ \hline
Encoder Scale & 32 \\ \hline
Evaluation Episodes & 5 \\ \hline
Learning Rate & 0.0001 \\ \hline
Warmup Epochs & 0 \\ \hline
Encoder Number of Residual Blocks & 2 \\ \hline
\midrule
\multicolumn{2}{c}{\bf BC finetuning} \\ \hline
Use Augmentation & False \\ \hline
Evaluation Seed & 0 \\ \hline
Batch Size & 512 \\ \hline
Hidden Dimension & 256 \\ \hline
Weight Decay & 0 \\ \hline
Evaluation Episodes & 25 \\ \hline
Learning Rate & 0.0003 \\ \hline
Total Updates & 2500 \\ \hline
Warmup Epochs & 0 \\ \hline
\bottomrule
\end{tabular}
\end{center}
\end{table}


\end{document}

