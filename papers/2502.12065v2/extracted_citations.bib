@misc{azerbayev2023proofnet,
      title={ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics}, 
      author={Zhangir Azerbayev and Bartosz Piotrowski and Hailey Schoelkopf and Edward W. Ayers and Dragomir Radev and Jeremy Avigad},
      year={2023},
      eprint={2302.12433},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{lu-etal-2023-toward,
    title = "Toward Human-Like Evaluation for Natural Language Generation with Error Analysis",
    author = "Lu, Qingyu  and
      Ding, Liang  and
      Xie, Liping  and
      Zhang, Kanjian  and
      Wong, Derek F.  and
      Tao, Dacheng",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.324",
    doi = "10.18653/v1/2023.acl-long.324",
    pages = "5892--5907",
}

@inproceedings{lu-etal-2024-error,
    title = "Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models",
    author = "Lu, Qingyu  and
      Qiu, Baopu  and
      Ding, Liang  and
      Zhang, Kanjian  and
      Kocmi, Tom  and
      Tao, Dacheng",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.520",
    doi = "10.18653/v1/2024.findings-acl.520",
    pages = "8801--8816",
}

@misc{lu2024processdrivenautoformalizationlean4,
      title={Process-Driven Autoformalization in Lean 4}, 
      author={Jianqiao Lu and Yingjia Wan and Zhengying Liu and Yinya Huang and Jing Xiong and Chengwu Liu and Jianhao Shen and Hui Jin and Jipeng Zhang and Haiming Wang and Zhicheng Yang and Jing Tang and Zhijiang Guo},
      year={2024},
      eprint={2406.01940},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.01940}, 
}

@inproceedings{quan-etal-2024-enhancing,
    title = "Enhancing Ethical Explanations of Large Language Models through Iterative Symbolic Refinement",
    author = "Quan, Xin  and
      Valentino, Marco  and
      Dennis, Louise  and
      Freitas, Andre",
    editor = "Graham, Yvette  and
      Purver, Matthew",
    booktitle = "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = mar,
    year = "2024",
    address = "St. Julian{'}s, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.eacl-long.1/",
    pages = "1--22",
    abstract = "An increasing amount of research in Natural Language Inference (NLI) focuses on the application and evaluation of Large Language Models (LLMs) and their reasoning capabilities. Despite their success, however, LLMs are still prone to factual errors and inconsistencies in their explanations, offering limited control and interpretability for inference in complex domains. In this paper, we focus on ethical NLI, investigating how hybrid neuro-symbolic techniques can enhance the logical validity and alignment of ethical explanations produced by LLMs. Specifically, we present an abductive-deductive framework named Logic-Explainer, which integrates LLMs with an external backward-chaining solver to refine step-wise natural language explanations and jointly verify their correctness, reduce incompleteness and minimise redundancy. An extensive empirical analysis demonstrates that Logic-Explainer can improve explanations generated via in-context learning methods and Chain-of-Thought (CoT) on challenging ethical NLI tasks, while, at the same time, producing formal proofs describing and supporting models' reasoning. As ethical NLI requires commonsense reasoning to identify underlying moral violations, our results suggest the effectiveness of neuro-symbolic methods for multi-step NLI more broadly, opening new opportunities to enhance the logical consistency, reliability, and alignment of LLMs."
}

@inproceedings{quan-etal-2024-verification,
    title = "Verification and Refinement of Natural Language Explanations through {LLM}-Symbolic Theorem Proving",
    author = "Quan, Xin  and
      Valentino, Marco  and
      Dennis, Louise A.  and
      Freitas, Andre",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.172/",
    doi = "10.18653/v1/2024.emnlp-main.172",
    pages = "2933--2958",
    abstract = "Natural language explanations represent a proxy for evaluating explanation-based and multi-step Natural Language Inference (NLI) models. However, assessing the validity of explanations for NLI is challenging as it typically involves the crowd-sourcing of apposite datasets, a process that is time-consuming and prone to logical errors. To address existing limitations, this paper investigates the verification and refinement of natural language explanations through the integration of Large Language Models (LLMs) and Theorem Provers (TPs). Specifically, we present a neuro-symbolic framework, named Explanation-Refiner, that integrates TPs with LLMs to generate and formalise explanatory sentences and suggest potential inference strategies for NLI. In turn, the TP is employed to provide formal guarantees on the logical validity of the explanations and to generate feedback for subsequent improvements. We demonstrate how Explanation-Refiner can be jointly used to evaluate explanatory reasoning, autoformalisation, and error correction mechanisms of state-of-the-art LLMs as well as to automatically enhance the quality of explanations of variable complexity in different domains."
}

@misc{wang2024largelanguagemodelsfail,
      title={Where Do Large Language Models Fail When Generating Code?}, 
      author={Zhijie Wang and Zijie Zhou and Da Song and Yuheng Huang and Shengmai Chen and Lei Ma and Tianyi Zhang},
      year={2024},
      eprint={2406.08731},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2406.08731}, 
}

@inproceedings{zhang-etal-2024-consistent,
    title = "Consistent Autoformalization for Constructing Mathematical Libraries",
    author = "Zhang, Lan  and
      Quan, Xin  and
      Freitas, Andre",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.233",
    pages = "4020--4033",
}

@inproceedings{zheng2022miniff,
    title={miniF2F: a cross-system benchmark for formal Olympiad-level mathematics},
    author={Kunhao Zheng and Jesse Michael Han and Stanislas Polu},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=9ZPegFuFTFv}
}

