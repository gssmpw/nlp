@inproceedings{
wu2022autoformalization,
title={Autoformalization with Large Language Models},
author={Yuhuai Wu and Albert Qiaochu Jiang and Wenda Li and Markus Norman Rabe and Charles E Staats and Mateja Jamnik and Christian Szegedy},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=IUikebJ1Bf0}
}

@inproceedings{meadows-etal-2024-symbolic,
    title = "A Symbolic Framework for Evaluating Mathematical Reasoning and Generalisation with Transformers",
    author = "Meadows, Jordan  and
      Valentino, Marco  and
      Teney, Damien  and
      Freitas, Andre",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.84/",
    doi = "10.18653/v1/2024.naacl-long.84",
    pages = "1505--1523",
    abstract = "This paper proposes a methodology for generating and perturbing detailed derivations of equations at scale, aided by a symbolic engine, to evaluate the generalisability of Transformers to out-of-distribution mathematical reasoning problems. Instantiating the framework in the context of sequence classification tasks, we compare the capabilities of GPT-4, GPT-3.5, and a canon of fine-tuned BERT models, exploring the relationship between specific operators and generalisation failure via the perturbation of reasoning aspects such as symmetry and variable surface forms. Surprisingly, our empirical evaluation reveals that the average in-distribution performance of fine-tuned models surpasses GPT-3.5, and rivals GPT-4. However, perturbations to input reasoning can reduce their performance by up to 80 F1 points. Overall, the results suggest that the in-distribution performance of smaller open-source models may potentially rival GPT by incorporating appropriately structured derivation dependencies during training, and highlight a shared weakness between BERT and GPT involving a relative inability to decode indirect references to mathematical entities. We release the full codebase, constructed datasets, and fine-tuned models to encourage future progress in the field."
}

@inproceedings{quan-etal-2024-enhancing,
    title = "Enhancing Ethical Explanations of Large Language Models through Iterative Symbolic Refinement",
    author = "Quan, Xin  and
      Valentino, Marco  and
      Dennis, Louise  and
      Freitas, Andre",
    editor = "Graham, Yvette  and
      Purver, Matthew",
    booktitle = "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = mar,
    year = "2024",
    address = "St. Julian{'}s, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.eacl-long.1/",
    pages = "1--22",
    abstract = "An increasing amount of research in Natural Language Inference (NLI) focuses on the application and evaluation of Large Language Models (LLMs) and their reasoning capabilities. Despite their success, however, LLMs are still prone to factual errors and inconsistencies in their explanations, offering limited control and interpretability for inference in complex domains. In this paper, we focus on ethical NLI, investigating how hybrid neuro-symbolic techniques can enhance the logical validity and alignment of ethical explanations produced by LLMs. Specifically, we present an abductive-deductive framework named Logic-Explainer, which integrates LLMs with an external backward-chaining solver to refine step-wise natural language explanations and jointly verify their correctness, reduce incompleteness and minimise redundancy. An extensive empirical analysis demonstrates that Logic-Explainer can improve explanations generated via in-context learning methods and Chain-of-Thought (CoT) on challenging ethical NLI tasks, while, at the same time, producing formal proofs describing and supporting models' reasoning. As ethical NLI requires commonsense reasoning to identify underlying moral violations, our results suggest the effectiveness of neuro-symbolic methods for multi-step NLI more broadly, opening new opportunities to enhance the logical consistency, reliability, and alignment of LLMs."
}

@inproceedings{quan-etal-2024-verification,
    title = "Verification and Refinement of Natural Language Explanations through {LLM}-Symbolic Theorem Proving",
    author = "Quan, Xin  and
      Valentino, Marco  and
      Dennis, Louise A.  and
      Freitas, Andre",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.172/",
    doi = "10.18653/v1/2024.emnlp-main.172",
    pages = "2933--2958",
    abstract = "Natural language explanations represent a proxy for evaluating explanation-based and multi-step Natural Language Inference (NLI) models. However, assessing the validity of explanations for NLI is challenging as it typically involves the crowd-sourcing of apposite datasets, a process that is time-consuming and prone to logical errors. To address existing limitations, this paper investigates the verification and refinement of natural language explanations through the integration of Large Language Models (LLMs) and Theorem Provers (TPs). Specifically, we present a neuro-symbolic framework, named Explanation-Refiner, that integrates TPs with LLMs to generate and formalise explanatory sentences and suggest potential inference strategies for NLI. In turn, the TP is employed to provide formal guarantees on the logical validity of the explanations and to generate feedback for subsequent improvements. We demonstrate how Explanation-Refiner can be jointly used to evaluate explanatory reasoning, autoformalisation, and error correction mechanisms of state-of-the-art LLMs as well as to automatically enhance the quality of explanations of variable complexity in different domains."
}

@InProceedings{szegedy2020,
author="Szegedy, Christian",
editor="Benzm{\"u}ller, Christoph
and Miller, Bruce",
title="A Promising Path Towards Autoformalization and General Artificial Intelligence",
booktitle="Intelligent Computer Mathematics",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="3--20",
isbn="978-3-030-53518-6"
}

@inproceedings{zheng2022miniff,
    title={miniF2F: a cross-system benchmark for formal Olympiad-level mathematics},
    author={Kunhao Zheng and Jesse Michael Han and Stanislas Polu},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=9ZPegFuFTFv}
}

@inproceedings{zhang-etal-2024-consistent,
    title = "Consistent Autoformalization for Constructing Mathematical Libraries",
    author = "Zhang, Lan  and
      Quan, Xin  and
      Freitas, Andre",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.233",
    pages = "4020--4033",
}

@inproceedings{lu-etal-2023-toward,
    title = "Toward Human-Like Evaluation for Natural Language Generation with Error Analysis",
    author = "Lu, Qingyu  and
      Ding, Liang  and
      Xie, Liping  and
      Zhang, Kanjian  and
      Wong, Derek F.  and
      Tao, Dacheng",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.324",
    doi = "10.18653/v1/2023.acl-long.324",
    pages = "5892--5907",
}

@inproceedings{lu-etal-2024-error,
    title = "Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models",
    author = "Lu, Qingyu  and
      Qiu, Baopu  and
      Ding, Liang  and
      Zhang, Kanjian  and
      Kocmi, Tom  and
      Tao, Dacheng",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.520",
    doi = "10.18653/v1/2024.findings-acl.520",
    pages = "8801--8816",
}

@misc{shao2024deepseekmath,
      title={DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models}, 
      author={Zhihong Shao and Peiyi Wang and Qihao Zhu and Runxin Xu and Junxiao Song and Xiao Bi and Haowei Zhang and Mingchuan Zhang and Y. K. Li and Y. Wu and Daya Guo},
      year={2024},
      eprint={2402.03300},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.03300}, 
}

@misc{grattafiori2024llama3herdmodels,
      title={The Llama 3 Herd of Models}, 
      author={Llama Team, AI @ Meta},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}

@misc{openai2024gpt4o,
      title={GPT-4o System Card}, 
      author={OpenAI},
      year={2024},
      eprint={2410.21276},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.21276}, 
}

@misc{openai2024gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}


@inproceedings{
li2024autoformalize,
title={Autoformalize Mathematical Statements by Symbolic Equivalence and Semantic Consistency},
author={Zenan Li and Yifan Wu and Zhaoyu Li and Xinming Wei and Xian Zhang and Fan Yang and Xiaoxing Ma},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=8ihVBYpMV4}
}

@inproceedings{
tarrach2024more,
title={More Details, Please: Improving Autoformalization with More Detailed Proofs},
author={Guillem Tarrach and Albert Q. Jiang and Daniel Raggi and Wenda Li and Mateja Jamnik},
booktitle={AI for Math Workshop @ ICML 2024},
year={2024},
url={https://openreview.net/forum?id=AkJvzpYMvK}
}

@inproceedings{
jiang2023draft,
title={Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs},
author={Albert Qiaochu Jiang and Sean Welleck and Jin Peng Zhou and Timothee Lacroix and Jiacheng Liu and Wenda Li and Mateja Jamnik and Guillaume Lample and Yuhuai Wu},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=SMa9EAovKMC}
}

@misc{azerbayev2023proofnet,
      title={ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics}, 
      author={Zhangir Azerbayev and Bartosz Piotrowski and Hailey Schoelkopf and Edward W. Ayers and Dragomir Radev and Jeremy Avigad},
      year={2023},
      eprint={2302.12433},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wang2024largelanguagemodelsfail,
      title={Where Do Large Language Models Fail When Generating Code?}, 
      author={Zhijie Wang and Zijie Zhou and Da Song and Yuheng Huang and Shengmai Chen and Lei Ma and Tianyi Zhang},
      year={2024},
      eprint={2406.08731},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2406.08731}, 
}

@inproceedings{
wei2022chain,
title={Chain of Thought Prompting Elicits Reasoning in Large Language Models},
author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and brian ichter and Fei Xia and Ed H. Chi and Quoc V Le and Denny Zhou},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=_VjQlMeSB_J}
}

@article{Moschkovich2003WhatCA,
  title={What Counts as Mathematical Discourse},
  author={Judit N. Moschkovich},
  journal={International Group for the Psychology of Mathematics Education},
  year={2003},
  volume={3},
  pages={325-332},
  url={https://api.semanticscholar.org/CorpusID:173294493}
}
@Book{Nipkow-Paulson-Wenzel:2002,
  author	= {Tobias Nipkow and Lawrence C. Paulson and Markus Wenzel},
  title		= {Isabelle/HOL --- A Proof Assistant for Higher-Order Logic},
  publisher	= {Springer},
  series	= {LNCS},
  volume	= 2283,
  year		= 2002}

@inproceedings{survey2020,
author = {Kaliszyk, Cezary and Rabe, Florian},
title = {A Survey of Languages for Formalizing Mathematics},
year = {2020},
isbn = {978-3-030-53517-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-53518-6_9},
doi = {10.1007/978-3-030-53518-6_9},
booktitle = {Intelligent Computer Mathematics: 13th International Conference, CICM 2020, Bertinoro, Italy, July 26–31, 2020, Proceedings},
pages = {138–156},
numpages = {19},
location = {Bertinoro, Italy}
}


@article{meadows2023symbolic,
  title={A Symbolic Framework for Systematic Evaluation of Mathematical Reasoning with Transformers},
  author={Meadows, Jordan and Valentino, Marco and Teney, Damien and Freitas, Andre},
  journal={arXiv preprint arXiv:2305.12563},
  year={2023}
}

@article{balazevic2019multi,
  title={Multi-relational poincar{\'e} graph embeddings},
  author={Balazevic, Ivana and Allen, Carl and Hospedales, Timothy},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{geirhos2020shortcut,
  title={Shortcut learning in deep neural networks},
  author={Geirhos, Robert and Jacobsen, J{\"o}rn-Henrik and Michaelis, Claudio and Zemel, Richard and Brendel, Wieland and Bethge, Matthias and Wichmann, Felix A},
  journal={Nature Machine Intelligence},
  volume={2},
  number={11},
  pages={665--673},
  year={2020},
  publisher={Nature Publishing Group UK London}
}

@article{hitzler2022neuro,
  title={Neuro-symbolic artificial intelligence: The state of the art},
  author={Hitzler, Pascal and Sarker, Md Kamruzzaman},
  year={2022},
  publisher={IOS Press}
}

@article{hochreiter1996lstm,
  title={LSTM can solve hard long time lag problems},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Advances in neural information processing systems},
  volume={9},
  year={1996}
}

@inproceedings{mishra2022numglue,
  title={NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks},
  author={Mishra, Swaroop and Mitra, Arindam and Varshney, Neeraj and Sachdeva, Bhavdeep and Clark, Peter and Baral, Chitta and Kalyan, Ashwin},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={3505--3523},
  year={2022}
}

@inproceedings{ferreira2020premise,
  title={Premise selection in natural language mathematical texts},
  author={Ferreira, Deborah and Freitas, Andr{\'e}},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={7365--7374},
  year={2020}
}

@inproceedings{welleck2021naturalproofs,
  title={NaturalProofs: Mathematical Theorem Proving in Natural Language},
  author={Welleck, Sean and Liu, Jiacheng and Le Bras, Ronan and Hajishirzi, Hannaneh and Choi, Yejin and Cho, Kyunghyun},
  booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)},
  year={2021}
}

@article{hamilton2017inductive,
  title={Inductive representation learning on large graphs},
  author={Hamilton, Will and Ying, Zhitao and Leskovec, Jure},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{li2021survey,
  title={A survey of convolutional neural networks: analysis, applications, and prospects},
  author={Li, Zewen and Liu, Fan and Yang, Wenjie and Peng, Shouheng and Zhou, Jun},
  journal={IEEE transactions on neural networks and learning systems},
  year={2021},
  publisher={IEEE}
}

@inproceedings{paliwal2020graph,
  title={Graph representations for higher-order logic and theorem proving},
  author={Paliwal, Aditya and Loos, Sarah and Rabe, Markus and Bansal, Kshitij and Szegedy, Christian},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={03},
  pages={2967--2974},
  year={2020}
}

@article{yu2019review,
  title={A review of recurrent neural networks: LSTM cells and network architectures},
  author={Yu, Yong and Si, Xiaosheng and Hu, Changhua and Zhang, Jianxun},
  journal={Neural computation},
  volume={31},
  number={7},
  pages={1235--1270},
  year={2019},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@inproceedings{kipf2016semi,
  title={Semi-Supervised Classification with Graph Convolutional Networks},
  author={Kipf, Thomas N and Welling, Max},
  booktitle={International Conference on Learning Representations},
  year={2016}
}

@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group UK London}
}

@article{meadows2021similarity,
  title={Similarity-based equational inference in physics},
  author={Meadows, Jordan and Freitas, Andr{\'e}},
  journal={Physical Review Research},
  volume={3},
  number={4},
  pages={L042010},
  year={2021},
  publisher={APS}
}

@article{eichhorn2022question,
  title={Question Answering in STACK Applying String Similarity},
  author={Eichhorn, Achim and Helfrich-Schkarbanenko, Andreas},
  journal={International Journal of Emerging Technologies in Learning (Online)},
  volume={17},
  number={23},
  pages={56},
  year={2022},
  publisher={International Association of Online Engineering (IAOE)}
}

@article{minervini2018towards,
  title={Towards neural theorem proving at scale},
  author={Minervini, Pasquale and Bosnjak, Matko and Rockt{\"a}schel, Tim and Riedel, Sebastian},
  journal={arXiv preprint arXiv:1807.08204},
  year={2018}
}

@article{meurer2017sympy,
  title={SymPy: symbolic computing in Python},
  author={Meurer, Aaron and Smith, Christopher P and Paprocki, Mateusz and {\v{C}}ert{\'\i}k, Ond{\v{r}}ej and Kirpichev, Sergey B and Rocklin, Matthew and Kumar, AMiT and Ivanov, Sergiu and Moore, Jason K and Singh, Sartaj and others},
  journal={PeerJ Computer Science},
  volume={3},
  pages={e103},
  year={2017},
  publisher={PeerJ Inc.}
}

@article{valentino2023multi,
  title={Multi-Relational Hyperbolic Word Embeddings from Natural Language Definitions},
  author={Valentino, Marco and Carvalho, Danilo S and Freitas, Andr{\'e}},
  journal={arXiv preprint arXiv:2305.07303},
  year={2023}
}

@inproceedings{welleck2022symbolic,
  title={Symbolic brittleness in sequence models: on systematic generalization in symbolic mathematics},
  author={Welleck, Sean and West, Peter and Cao, Jize and Choi, Yejin},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={8},
  pages={8629--8637},
  year={2022}
}

@article{meadows2023introduction,
  title={Introduction to Mathematical Language Processing: Informal Proofs, Word Problems, and Supporting Tasks},
  author={Meadows, Jordan and Freitas, Andr{\'e}},
  journal={Transactions of the Association for Computational Linguistics},
  volume={11},
  pages={1162--1184},
  year={2023},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@inproceedings{lee2019mathematical,
  title={Mathematical Reasoning in Latent Space},
  author={Lee, Dennis and Szegedy, Christian and Rabe, Markus and Loos, Sarah and Bansal, Kshitij},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{schick2023toolformer,
  title={Toolformer: Language models can teach themselves to use tools},
  author={Schick, Timo and Dwivedi-Yu, Jane and Dess{\`\i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  journal={arXiv preprint arXiv:2302.04761},
  year={2023}
}

@article{zhang2022hgen,
  title={HGEN: Learning hierarchical heterogeneous graph encoding for math word problem solving},
  author={Zhang, Yi and Zhou, Guangyou and Xie, Zhiwen and Huang, Jimmy Xiangji},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={30},
  pages={816--828},
  year={2022},
  publisher={IEEE}
}

@article{lecun2022path,
  title={A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27},
  author={LeCun, Yann},
  journal={Open Review},
  volume={62},
  year={2022}
}

@article{mikolov2013efficient,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}

@article{bordes2013translating,
  title={Translating embeddings for modeling multi-relational data},
  author={Bordes, Antoine and Usunier, Nicolas and Garcia-Duran, Alberto and Weston, Jason and Yakhnenko, Oksana},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}

@article{saparov2023testing,
  title={Testing the General Deductive Reasoning Capacity of Large Language Models Using OOD Examples},
  author={Saparov, Abulhair and Pang, Richard Yuanzhe and Padmakumar, Vishakh and Joshi, Nitish and Kazemi, Seyed Mehran and Kim, Najoung and He, He},
  journal={arXiv preprint arXiv:2305.15269},
  year={2023}
}

@article{meadows2023generating,
  title={Generating Mathematical Derivations with Large Language Models},
  author={Meadows, Jordan and Valentino, Marco and Freitas, Andre},
  journal={arXiv preprint arXiv:2307.09998},
  year={2023}
}

@inproceedings{kim2014convolutional,
  title={Convolutional Neural Networks for Sentence Classification},
  author={Kim, Yoon},
  booktitle={Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={1746--1751},
  year={2014}
}

@article{chen2022program,
  title={Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks},
  author={Chen, Wenhu and Ma, Xueguang and Wang, Xinyi and Cohen, William W},
  journal={arXiv preprint arXiv:2211.12588},
  year={2022}
}

@inproceedings{saxton2018analysing,
  title={Analysing Mathematical Reasoning Abilities of Neural Models},
  author={Saxton, David and Grefenstette, Edward and Hill, Felix and Kohli, Pushmeet},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{lample2019deep,
  title={Deep learning for symbolic mathematics},
  author={Lample, Guillaume and Charton, Fran{\c{c}}ois},
  journal={arXiv preprint arXiv:1912.01412},
  year={2019}
}

@article{teney2020value,
  title={On the value of out-of-distribution testing: An example of goodhart's law},
  author={Teney, Damien and Abbasnejad, Ehsan and Kafle, Kushal and Shrestha, Robik and Kanan, Christopher and Van Den Hengel, Anton},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={407--417},
  year={2020}
}

@article{hupkes2020compositionality,
  title={Compositionality decomposed: How do neural networks generalise?},
  author={Hupkes, Dieuwke and Dankers, Verna and Mul, Mathijs and Bruni, Elia},
  journal={Journal of Artificial Intelligence Research},
  volume={67},
  pages={757--795},
  year={2020}
}

@article{henderson2017efficient,
  title={Efficient natural language response suggestion for smart reply},
  author={Henderson, Matthew and Al-Rfou, Rami and Strope, Brian and Sung, Yun-Hsuan and Luk{\'a}cs, L{\'a}szl{\'o} and Guo, Ruiqi and Kumar, Sanjiv and Miklos, Balint and Kurzweil, Ray},
  journal={arXiv preprint arXiv:1705.00652},
  year={2017}
}

@article{shen2021towards,
  title={Towards out-of-distribution generalization: A survey},
  author={Shen, Zheyan and Liu, Jiashuo and He, Yue and Zhang, Xingxuan and Xu, Renzhe and Yu, Han and Cui, Peng},
  journal={arXiv preprint arXiv:2108.13624},
  year={2021}
}

@misc{lu2024processdrivenautoformalizationlean4,
      title={Process-Driven Autoformalization in Lean 4}, 
      author={Jianqiao Lu and Yingjia Wan and Zhengying Liu and Yinya Huang and Jing Xiong and Chengwu Liu and Jianhao Shen and Hui Jin and Jipeng Zhang and Haiming Wang and Zhicheng Yang and Jing Tang and Zhijiang Guo},
      year={2024},
      eprint={2406.01940},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.01940}, 
}