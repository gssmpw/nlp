%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
%documentclass[sigconf,authordraft]{acmart}
\documentclass[sigconf]{acmart}
\makeatletter
\def\@ACM@checkaffil{% Only warnings
    \if@ACM@instpresent\else
    \ClassWarningNoLine{\@classname}{No institution present for an affiliation}%
    \fi
    \if@ACM@citypresent\else
    \ClassWarningNoLine{\@classname}{No city present for an affiliation}%
    \fi
    \if@ACM@countrypresent\else
        \ClassWarningNoLine{\@classname}{No country present for an affiliation}%
    \fi
}
\makeatother
\usepackage{caption}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{pgfplots} 
\pgfplotsset{compat=1.16} 
\usepackage{balance}
\usepackage{bigstrut,array,multirow,tabularx}
\usepackage{caption}
\usepackage{appendix}
% \usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
% \usepackage{subcaption}
\usepackage{dsfont}
\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
% \usepackage[dvipsnames]{xcolor}
%Table
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{makecell} 
\setlength{\textfloatsep}{5pt}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}[theorem]
%% NOTE that a single column version may be required for 
%% submission and peer review. This can be done by changing
%% the \doucmentclass[...]{acmart} in this template to 
%% \documentclass[manuscript,screen]{acmart}
%% 
%% To ensure 100% compatibility, please check the white list of
%% approved LaTeX packages to be used with the Master Article Template at
%% https://www.acm.org/publications/taps/whitelist-of-latex-packages 
%% before creating your document. The white list page provides 
%% information on how to submit additional LaTeX packages for 
%% review and adoption.
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.
%%
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

\newcommand{\jm}[1]{\textcolor{black}{#1}}%

\newcommand{\jd}[1]{\textcolor{blue}{#1}}%

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
% \copyrightyear{2025}
% \acmYear{2025}
% \setcopyright{acmlicensed}\acmConference[WWW '25]{Proceedings of the 
% ACM Web Conference}{April 28 -- May 02, 2025}{Sydney, Australia} 
% \acmBooktitle{Proceedings of the ACM Web Conference 25 (WWW '25), April 28 -- May 02, 2025, Sydney, Australia}
% \acmPrice{15.00}
% \acmDOI{XXXXXXX.XXXXXXX}
% \acmISBN{XXXXXX}


\copyrightyear{2025}
\acmYear{2025}
\setcopyright{rightsretained}
\acmConference[WWW '25]{Proceedings of the ACM Web Conference 2025}{April 28-May 2, 2025}{Sydney, NSW, Australia}
\acmBooktitle{Proceedings of the ACM Web Conference 2025 (WWW '25), April 28-May 2, 2025, Sydney, NSW, Australia}
\acmDOI{10.1145/3696410.3714799}
\acmISBN{979-8-4007-1274-6/25/04}
%% These commands are for a PROCEEDINGS abstract or paper.

%
%  Uncomment \acmBooktitle if th title of the proceedings is different
%  from ``Proceedings of ...''!
%
%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%  June 03--05, 2018, Woodstock, NY} 
% \acmPrice{15.00}
% \acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
%% \BibTeX command to typeset BibTeX logo in the docs \AtBeginDocument{%  \providecommand\BibTeX{{%

\copyrightyear{2025}
\acmYear{2025}
%% \setcopyright{cc}
%% \setcctype{CC-BY}
\setcopyright{rightsretained}
\acmConference[WWW '25]{Proceedings of the ACM Web Conference 2025}{April 28-May 2, 2025}{Sydney, NSW, Australia}
\acmBooktitle{Proceedings of the ACM Web Conference 2025 (WWW '25), April 28-May 2, 2025, Sydney, NSW, Australia}
\acmDOI{10.1145/3696410.3714799}
\acmISBN{979-8-4007-1274-6/25/04}

% The following includes the CC license icon appropriate for your paper.
% Download the image from www.scomminc.com/pp/acmsig/4ACM-CC-by-88x31.eps
% and place within your figs or figures folder

\makeatletter
\gdef\@copyrightpermission{
 \begin{minipage}{0.2\columnwidth}
  \href{https://creativecommons.org/licenses/by/4.0/}{\includegraphics[width=0.90\textwidth]{figures/4ACM-CC-by-88x31.pdf}}
 \end{minipage}\hfill
 \begin{minipage}{0.8\columnwidth}
  \href{https://creativecommons.org/licenses/by/4.0/}{This work is licensed under a Creative Commons Attribution International 4.0 License.}
 \end{minipage}
 \vspace{5pt}
}
\makeatother
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Criteria-Aware Graph Filtering: \\ Extremely Fast Yet Accurate Multi-Criteria Recommendation}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

\author{Jin-Duk Park}
\affiliation{%
  \institution{Yonsei University}
  \city{Seoul}
  \country{Republic of Korea}
}
\email{jindeok6@yonsei.ac.kr}


\author{Jaemin Yoo}
\affiliation{%
  \institution{KAIST}
  \city{Daejeon}
  \country{Republic of Korea}
}
\email{jaemin@kaist.ac.kr}


\author{Won-Yong Shin}
\authornote{Corresponding author}
\affiliation{%
  \institution{Yonsei University \& POSTECH}
  \city{Seoul}
  \country{Republic of Korea}
}
\email{wy.shin@yonsei.ac.kr}

% \author{Cong Tran}
% \affiliation{%
%   \institution{Posts and Telecommunications Institute of Technology}
% %   \city{Hanoi}
% %   \country{Vietnam}
% }
% \email{congtt@ptit.edu.vn}

% \author{Won-Yong Shin}
% \authornote{Corresponding author}
% \affiliation{%
%   \institution{Yonsei University}
% %   \city{Seoul}
% %   \country{Republic of Korea}
% }
% \email{wy.shin@yonsei.ac.kr}

% \author{Xin Cao}
% \affiliation{%
%   \institution{The University of New South Wales Australia}
% %   \city{Sydney}
% %   \country{Australia}
% }
% \email{xin.cao@unsw.edu.au}



% \renewcommand{\shortauthors}{Jin-Duk Park, Siqing Li, Xin Cao, \& Won-Yong Shin}


%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
%% \renewcommand{\shortauthors}{Jin-Duk Park et al.}
%%
%% The abstract is a short summary of the work to be presented in the
%% article.
% \begin{abstract}
% The multi-criteria recommender system (MCRS) aims at providing more accurate recommendations by utilizing multi-criteria rating information. Our study is motivated by the insight that there exists \textit{criteria preference} when a user consumes items having multi-criteria ratings. However, prior studies do not have the capacity to explicitly capture the criteria preference of users. In this study, we propose \textsf{CA-GF}, a novel criteria preference-aware light graph convolution model that captures the criteria preference of users along with capturing the collaborative signal in complex high-order connectivities between users and items via graph neural networks (GNNs). Specifically, our \textsf{CA-GF} first constructs a \textit{multi-criteria expansion graph (MCEG)} that transforms user-item multi-criteria interactions into an expanded bipartite graph to capture the collaborative signal in complex high-order connectivities that exist in multi-criteria ratings between users and items. Next, to strengthen the capability of capturing the criteria preference of each user, we incorporate newly characterized embeddings, including {\em user criteria-preference embeddings} and {\em criterion embeddings}, into our CPA-LGC model. Extensive empirical evaluation quantitatively demonstrates the superiority of \textsf{CA-GF} as well as the effectiveness of using multi-criteria rating information compared to single-criterion recommendation circumstances.
% \end{abstract}

\begin{abstract}

  Multi-criteria (MC) recommender systems, which utilize MC rating information for recommendation, are increasingly widespread in various e-commerce domains. However, the MC recommendation using training-based collaborative filtering, requiring consideration of multiple ratings compared to single-criterion counterparts, often poses practical challenges in achieving state-of-the-art performance along with scalable model training. To solve this problem, we propose \textsf{CA-GF}, a {\it training-free} MC recommendation method, which is built upon {\it criteria-aware} graph filtering for {\it efficient yet accurate} MC recommendations. Specifically, first, we construct an item--item similarity graph using an MC user-expansion graph. Next, we design \textsf{CA-GF} composed of the following key components, including 1) {\it criterion-specific} graph filtering where the optimal filter for each criterion is found using various types of polynomial low-pass filters and 2) {\it criteria preference-infused aggregation} where the smoothed signals from each criterion are aggregated. We demonstrate that \textsf{CA-GF} is \textbf{(a) efficient}: providing the computational efficiency, offering the extremely fast runtime of less than \underline{{\it 0.2 seconds}} even on the largest benchmark dataset, \textbf{(b) accurate}: outperforming benchmark MC recommendation methods, achieving substantial accuracy gains up to 24\% compared to the best competitor, and \textbf{(c) interpretable}: providing interpretations for the contribution of each criterion to the model prediction based on visualizations.
\end{abstract}

% Recently, graph neural networks (GNNs) become the mainstream for recommender systems (RS). However, such an approach is truly underexplored in multi-criteria recommender system (MCRS), where a user leaves ratings with multiple aspects ({\it i.e.}, multi-criteria). because it is not straightforward to adopts conventional GNN-based approaches when MCRS circumstance is concerned.

%%
%% The code below is g


%t http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
% \begin{CCSXML}
% <ccs2012>
%    <concept>
%        <concept_id>10010147.10010257.10010293.10010319</concept_id>
%        <concept_desc>Computing methodologies~Learning latent representations</concept_desc>
%        <concept_significance>300</concept_significance>
%        </concept>
%  </ccs2012>
% \end{CCSXML}
% ------------------- Removed for submission
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10002951.10003260.10003261.10003269</concept_id>
<concept_desc>Information systems~Collaborative filtering</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}
\ccsdesc[500]{Information systems~Collaborative filtering}
% ------------------- Removed
%% Keywords. he author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Collaborative filtering; criteria preference; graph filtering; low-pass filter; multi-criteria recommender system.}
\maketitle
\section{Introduction}
\label{sec 1}
 Multi-criteria (MC) recommender systems, which leverage detailed {\it criteria} ratings for each item, have become increasingly important across various online service areas, including travel, restaurants, hotels, movies, and music \cite{wang2011latent, shambour2021deep,jannach2014leveraging,nassar2020novel, li2019latent,mcauley2012learning}. MC recommender systems generally excel in recommending relevant items to users compared to single-criterion recommender systems, as they capture user preferences more exquisitely \cite{fan2021predicting, nassar2020novel, shambour2021deep, zheng2019utility, li2019latent,jannach2014leveraging}. For example, as illustrated in Figure \ref{fig:intro_1a}, in a restaurant domain, a user can provide four criteria ratings, which include an \textit{overall} rating as well as other MC ratings for {\it food}, {\it service}, and {\it location}.

% -------------- START int ------------------------- %
\begin{figure}[t]
        \centering
        \begin{subfigure}[c]{0.29\columnwidth}
                \includegraphics[width=0.92\columnwidth]{figures/MCRS.png}
                \caption{}
                \label{fig:intro_1a}
        \end{subfigure}        
        \begin{subfigure}[c]{0.65\columnwidth}
                \includegraphics[width=1\columnwidth]{figures/intro.png}
                \caption{}
                \label{fig:intro_1b}
        \end{subfigure}
\vspace{-3mm}
        \caption{ An illustration showing (a) four criteria ratings in a restaurant domain and (b) a comparison of the training time for 100 epochs between using single-criterion ratings ({\it i.e.}, overall ratings) and MC ratings across three benchmark MC recommendation methods on the TripAdvisor dataset. Additionally, the processing time is measured for \textsf{CA-GF} that does not need any training process.}
        \label{dist_plot} 
\end{figure}
% -------------- END int ------------------------- %

Nonetheless, the inclusion of MC ratings in MC recommender systems often significantly increases computational demands compared to single-criterion counterparts, due to the complexity of processing and analyzing multi-dimensional user feedback \cite{li2020learning,wu2019deep}. For instance, given $N$ user--item interactions with $8$ criteria, training-based collaborative filtering (CF) methods having a computational complexity of $O(N^2)$ increase the training time by approximately $64$ times over the single-criterion case. As shown in Figure \ref{fig:intro_1b}, the time required for identical epochs in training substantially escalates when MC ratings are incorporated into three benchmark deep neural network (DNN)-based MC recommendation methods, including AEMC \cite{shambour2021deep}, ExtendedSAE \cite{tallapally2018user}, and CPA-LGC \cite{park2023criteria}. Acknowledging that user preferences evolve quickly due to trends, personal circumstances, and exposure to new content, such latency in training time may not be desirable \cite{ju2015using,pereira2018analyzing, chang2021sequential,park2024turbo}.

Unlike earlier studies on learning models such as graph convolution for recommendations in a parametric manner \cite{zheng2018spectral,wang2020disentangled,wang2019neural,he2020lightgcn,he2017neural}, our study is inspired by recent advances in non-parametric, {\it i.e.}, training-free, CF for single-criterion recommender systems. More precisely, we leverage the concept of graph signal processing, namely \textit{graph filtering} \cite{shen2021powerful,xia2022fire,liu2023personalized}, for CF. While graph filtering offers an affordable solution to the computational challenges in single-criterion recommender systems, it remains open how such methodology can be seamlessly and effectively applied to MC recommender systems without causing computational demands associated with MC ratings. In our study, we aim to develop an innovative graph filtering-based MC recommendation method, which is efficient yet accurate. To this end, we start by outlining three design {\bf goals (G1--G3)} to attain a successful MC recommender system.

\begin{itemize}[leftmargin=*]

    \item \textbf{G1. Capability of capturing inter-criteria relations:} Canonical MC recommendation methods \cite{fan2021predicting,fan2023improving,zheng2019utility,nassar2020novel,shambour2021deep}, which simply extend rather popular single-criterion recommendation methods to MC circumstances, often fail to fully grasp the collaborative signal in complex semantics across MC ratings ({\it i.e.}, inter-criteria relations) \cite{park2023criteria}. In this context, when MC recommender systems are built upon graph filtering, a natural challenge lies in how to capture {\it inter-criteria} relations.
    
    \item \textbf{G2. Awareness of criterion-specific characteristics:} While inter-criteria relation information is exploited in designing MC recommender systems, it is also crucial to preserve the unique characteristics inherent to each criterion. This viewpoint motivates us to comprehensively grasp the \textit{distinctiveness of each criterion}, thus leading to accurate MC recommendations.
    
    \item \textbf{G3. Fast runtime:}  Conventional DNN-based MC recommendation methods are not without the increased training cost caused by making use of MC ratings, compared to the single-criterion counterparts (see Figure \ref{fig:intro_1b}). While graph filtering-based CF methods are known to circumvent the computational burden to some extent, it is yet technically challenging how to maximize computational efficiency in devising a graph filtering-based method when MC ratings are concerned.
\end{itemize}

To effectively achieve \textbf{G1}-\textbf{G3} at once, we propose \textsf{CA-GF}, a \textit{training-free} MC recommendation method, which accommodates \textit{criteria-aware} graph filters for both efficient and accurate MC recommendations. Specifically, in \textsf{CA-GF}, we initially construct an item--item similarity graph using an MC user-expansion graph to capture complex contextual semantics across the MC ratings (\textbf{G1}). Then, based on the item--item similarity graph, we perform {\it criterion-specific} graph filtering where the optimal filter for each criterion is found using various types of polynomial low-pass filters (LPFs) (\textbf{G2}), which do not necessitate costly matrix decomposition in conventional graph filtering-based CF methods \cite{shen2021powerful,xia2022fire,liu2023personalized} (\textbf{G3}). Finally, we perform {\it criteria preference-infused aggregation} to judiciously aggregate the smoothed signals from each criterion (\textbf{G2}). 
% Surprisingly, it is demonstrated that \textsf{CA-GF} takes only \jd{\textbf{\underline{0.2 seconds runtime}} on the largest dataset ({\it i.e.}, BeerAdvocate (BA)).}

Our main contributions are three-fold and summarized as follows:

\begin{enumerate}[leftmargin=*]
    \item \textbf{Simply understandable yet effective methodology:} We address the challenge of increased training time caused by MC ratings. To this end, we device \textsf{CA-GF}  built upon \textit{polynomial graph filters}. By harnessing criteria awareness using different polynomial LPFs for different criteria, \textsf{CA-GF} is capable of making full use of modern computer hardware (i.e., CPU and GPU), achieving the extremely fast runtime of \textbf{less than 0.2 seconds} even on the largest benchmark dataset ({\it i.e.}, BeerAdvocate).
    \item \textbf{Superior performance:} Through the sophisticated integration of each intra-criterion information as well as inter-criteria relation information into the graph filtering process, \textsf{CA-GF} attains state-of-the-art performance. Our approach consistently outperforms the best competitor by up to \textbf{24\% in terms of the NDCG@5}.
    \item \textbf{Comprehensive empirical studies:} Extensive experiments validate the efficacy of \textsf{CA-GF}, demonstrating its computational efficiency and accuracy. Moreover, unlike traditional DNN-based recommender systems, \textsf{CA-GF} offers clear insights into the model’s predictions, providing substantial interpretability benefits.
\end{enumerate}

% \begin{enumerate}
%     \item \textbf{Computational efficiency:} Owing to the implementation of polynomial filters in \textsf{CA-GF} that allows us to make full use of modern computer hardware components (\textit{i.e.}, CPU and GPU), a runtime of \textbf{\underline{less than 0.2 seconds}} is achieved on the largest benchmark dataset.
%     \item \textbf{Accuracy:} By sophisticatedly integrating additional MC ratings into our filtering process, \textsf{CA-GF} achieves state-of-the-art performance with significant margins over the best competitor up to \textbf{\underline{24\% in terms of the NDCG@5}}.
%     \item \textbf{Interpretability:} Contrary to canonical DNN-aided black-box recommender systems, \textsf{CA-GF} provides substantial benefits in terms of model interpretability. This facilitates a comprehensive understanding of the processes behind each prediction.
% \end{enumerate}
% Moreover, we provide rigorous theoretical analyses and comprehensive empirical evaluations to validate the efficacy of \textsf{CA-GF}.
For reproducibility, the source code of this study is available at \underline{\smash{\url{https://github.com/jindeok/CA-GF}}}.



% \begin{itemize}
%     \item MCRS introduction. Fig1-a
%     \item CF and its recent methods
%     \item motivation:  incorporating MC ratings boosts computational burden
%     \item For instance, O(r**2) algorithm takes xC+1**2 times. Fig1-b illustrates well
%     \item To this end, motivated by recent advances in non-parametric CF (graph filtering), we devise CA-GF. (* do not specify to GF-CF)
%     \item even though the non-parametric method partly relieves problems, there are two practical challenges (3.1 -> recall C1, C2 w/o description)
%     \item C1. blabla
%     \item C2. blabla
%     \item To this end, we propose CA-GF
%     \item CA-GF is training and decomposition-free, using polynomial graph filters.
%     \item Main contributions are follows:
%     \item 1) outrageously efficient MCRS: x??? faster
%     \item 2) Yet accurate method: \% gain in Recall
%     \item 3) Interpretable: 
%     \item 4) Comprehensive analysis \& evaluation:
% \end{itemize}

\section{{Preliminaries}}
\label{section 2}
\subsection{Problem Definition}
We formally define the top-$K$ MC recommendation, along with basic notations. Let $u \in \mathcal{U}$ and $i\in\mathcal{I}$ denote a user and an item, respectively, where $\mathcal{U}$ and $\mathcal{I}$ denote the sets of all users and all items, respectively. $\mathcal{N}_u \subset \mathcal{I}$ denotes a set of items interacted by user $u$. Compared to single-criterion recommender systems, MC recommender systems comprise of a number of rating criteria. We denote $R_c\in \mathbb{R}^{|\mathcal{U}| \times |\mathcal{I}|}$ as the rating matrix ({\it i.e.}, the user--item interaction matrix) for criterion $c$. In particular, $R_0\in \mathbb{R}^{|\mathcal{U}| \times |\mathcal{I}|}$ refers to the overall rating matrix. Then, the top-$K$ MC recommendation problem is formally defined as follows:
\begin{definition}
    (Top-$K$ MC recommendation) \cite{park2023criteria}: Given $u \in \mathcal{U}$ and $i\in\mathcal{I}$, and $C+1$ user--item rating matrices $R_0 \times R_1 \times ... \times R_C $ including an overall rating matrix $R_0$, the top-$K$ MC recommendation aims to recommend top-$K$ items that user $u\in\mathcal{U}$ is most likely to prefer among his/her non-interacted items in $\mathcal{I} \setminus \mathcal{N}_u$ {\it w.r.t.} the {\it overall} rating by using all $C+1$ user--item MC ratings.
\end{definition}


\subsection{Graph Signal Processing}
 We introduce basic concepts of graph signal processing. First, we consider a weighted undirected graph $G = (V, E)$, represented by an adjacency matrix $A \in\mathbb{R}^{|V|\times |V|}$. A graph signal is a function $f: V \to \mathbb{R}^d$ that encodes the set of nodes into a $d$-dimensional vector $\mathbf{x}=[x_1, x_2, \ldots, x_{|V|}]^T$, where $x_i$ represents the signal strength of node $i$. The smoothness of $\mathbf{x}$ on $G$ is quantified by the graph quadratic form, a measure based on the graph Laplacian $L = D - A$,\footnote{Here, we note that the graph Laplacian $L$ can also be defined as its normalized version $L = I - \tilde{A}$, where $\tilde{A}=D^{-1/2}AD^{-1/2}$.} where $D=\text{diag}(A\mathbf{1})$ is the degree matrix.\footnote{We denote the all-ones vector of any dimension as ${\bf 1}$ for notational simplicity.} The smoothness measure $S({\bf x})$ is formally expressed as follows \cite{shuman2013emerging, shen2021powerful}:
\begin{equation}
    S({\bf x}) = \sum_{i,j}A_{i,j}(x_i-x_j)^2 =  \mathbf{x}^T L \mathbf{x}.
\end{equation}
The smaller the value of $S(x)$, the smoother the signal $\mathbf{x}$ is on the graph. Next, we formally define the graph Fourier transform (GFT) for a graph signal $\mathbf{x}$ as:
    \begin{equation}
        \hat{\mathbf{x}} = U^T \mathbf{x},
    \end{equation} 
where $U\in\mathbb{R}^{|V|\times |V|}$ is the GFT basis whose $i$-th column is the eigenvector $u_i$ of $L$ in the eigen-decomposition of $L = U \Lambda U^T$ for $\Lambda=\text{diag}(\lambda_1, \ldots, \lambda_{V})$ and ordered eigenvalues $\lambda_1 \le \cdots \le \lambda_{|V|}$. Here, the signal $\mathbf{x}$ is considered smooth if the dot product of the eigenvectors corresponding to smaller eigenvalues of $L$ is high. As the GFT is a linear orthogonal transform, the inverse GFT is given by $\mathbf{x}=U\hat{\mathbf{x}}$. Finally, the graph filter and the graph convolution are formally defined as follows:
\begin{definition}
    (Graph filter) \cite{shuman2013emerging, shen2021powerful, xia2022fire,ortega2018graph}: Given a graph Laplacian matrix $L$, a graph filter $H(L)\in\mathbb{R}^{|V|\times |V|}$ is defined as 
\begin{equation}
H(L) = U \text{diag}(h(\lambda_1), \ldots, h(\lambda_{|V|})) U^T,
\end{equation}
where $h:\mathbb{C} \rightarrow \mathbb{R}$ is the frequency response function that maps eigenvalues $\{\lambda_1,\cdots,\lambda_{|V|}\}$ of $L$ to $\{h(\lambda_1),\cdots,h(\lambda_{|V|}\}$.
\end{definition}
\begin{definition}
\label{graph_conv_def}
    (Graph convolution) \cite{shuman2013emerging, shen2021powerful, xia2022fire}: The convolution of a graph signal $\mathbf{x}$ and a graph filter $H(L)$ is given by
\begin{equation}
    H(L) \mathbf{x} = U \text{diag}(h(\lambda_1), \ldots, h(\lambda_{|V|})) U^T\mathbf{x},
\end{equation}
\end{definition}
\noindent which first transforms $\mathbf{x}$ by the GFT and then transforms its filtered signal with $H(L)$ by the inverse GFT.

In signal processing, signals are typically characterized by their smoothness and low-frequency components, whereas noise is usually non-smooth and dominates at high frequencies \cite{shen2021powerful}. In this context, a significant category of filters is LPFs, which enhance the smoothness of graph signals, thereby aiding noise reduction. We refer to Appendix \ref{app:LPF} for the formal definition of LPFs.
% For instance, in the semi-supervised node classification task aiming at predicting the labels of unlabeled nodes on graphs, LPFs facilitate smooth label propagation for limited labeled nodes \cite{li2019label}. 





\subsection{Graph Filtering for Single-Criterion Recommendation}
LPFs play a crucial role in CF by promoting signal smoothness and reducing high-frequency noise \cite{liu2023personalized,shen2021powerful}. This is because CF relies on users' ratings, which inherently exhibit low-frequency patterns \cite{liu2023personalized}. Such patterns represent consistent preferences or trends among groups of users. By filtering out noise and smoothing graph signals, LPFs enhance the clarity of the underlying low-frequency patterns, thus leading to more accurate and reliable CF-aided recommendations \cite{liu2023personalized}. 
 
 Conventional graph filtering-based CF methods \cite{shen2021powerful,xia2022fire,liu2023personalized,park2024turbo} in single-criterion recommender systems first construct an item--item similarity graph $\tilde{P}$ as in the following:
\begin{equation}
\label{conventional_graph}
\begin{aligned}
    \tilde{P} = \tilde{R}_0^T\tilde{R}_0,
\end{aligned}
\end{equation}
where $\tilde{R}_0 = D^{-1/2}_UR_0D^{-1/2}_I$. Here, $R_0 \in \mathbb{R}^{|\mathcal{U}| \times |\mathcal{I}|}$ is the rating matrix; $\tilde{R}_0$ is the normalized rating matrix; and $D_U=\text{diag}(R_0\mathbf{1})$ and $D_I = \text{diag}(\mathbf{1}^TR_0)$.

Graph filtering-based CF methods \cite{liu2023personalized,shen2021powerful,xia2022fire} typically employ both linear and ideal LPFs. 
Representative work includes GF-CF \cite{shen2021powerful}, whose graph convolution is formulated as follows:
\begin{equation}
\label{gfcf}
    {\bf s}_u = {\bf r}_u(\tilde{P} + \alpha D^{-1/2}_U\bar{U}\bar{U}^TD^{-1/2}_I),
\end{equation}
where ${\bf s}_u \in \mathbb{R}^{|\mathcal{I}|}$ is the predicted preferences for user $u$; ${\bf r}_u \in \mathbb{R}^{|\mathcal{I}|}$ is the ratings of $u$, which serves as graph signals to be smoothed; $\bar{U}\in \mathbb{R}^{|\mathcal{I}| \times k}$ is the top-$k$ singular vectors of $\tilde{R}_0$; $\tilde{P}$ is the linear LPF in Eq. \eqref{conventional_graph}; $D^{-1/2}_U\bar{U}\bar{U}^TD^{-1/2}_I$ is the ideal LPF of $\tilde{P}$; and $\alpha$ is a hyperparameter balancing between the two filters. 

A primary benefit of such graph filtering-based CF methods is their non-parametric nature. These approaches bypass the need for intricate and time-intensive model training, relying instead on efficient matrix operations to derive a closed-form solution corresponding to recommendation scores  ({\it i.e.}, predicted preferences of users, denoted as ${\bf s}_u$).

% \subsection{Motivation}
% \label{sec 2.3}

%  Nonetheless, those approaches may encounter major {\bf practical challenges} when MC recommender systems are concerned:
% \begin{itemize}
%     \item \textbf{C1. Incapability of capturing semantics in MC ratings:} It is validated that effectively incorporating MC ratings help produce more personalized and accurate recommendations \cite{park2023criteria, shambour2021deep,jannach2014leveraging,nassar2020novel, li2019latent,mcauley2012learning}. However, the aforementioned graph filtering-based CF methods were originally developed for single-criterion recommender systems, where complex contextual semantics existing among MC ratings cannot be captured and exploited;
%     \item \textbf{C2. Computational burden on the matrix decomposition :} The ideal LPF in Eq. \eqref{gfcf} requires matrix decomposition to obtain $\bar{U}$, which may require expensive computing costs for its implementation especially when the matrix dimension increases by a factor of the number of criteria in MC recommender systems.\footnote{This includes the case where we perform the matrix decomposition on the MC user-expansion graph mentioned in Section 3.1. Although GF-CF uses the generalized power method \cite{journee2010generalized} for efficient top-$K$ eigenvector computation, it is shown to be efficient only for sufficiently small $K$, which still has limitations in designing diverse and expressive LPFs.}
% \end{itemize} 

% The objective of our study is to judiciously incorporate MC ratings into graph filtering without losing its computational efficiency. Motivated by the fact that the ideal LPF in Eq. \eqref{gfcf} necessitates costly matrix decomposition, we propose \textsf{CA-GF}, a not only {\it training-free} but also \textit{matrix decomposition-free} graph filtering method, where \textit{criteria-aware} graph filters built upon an \textit{MC user-expansion graph} are accommodated to effectively capture the collaborative signal in complex contextual semantics across MC ratings for accurate recommendations.



\section{Proposed Method: \textsf{CA-GF}}
\label{section 3}

\subsection{Overview}
\label{sec 2.3}

% We recall and further specify the \jd{three {\bf goals (G1-G3)}} when designing a graph filtering-based MC recommendation method:
% \jd{
% \begin{itemize}
%     \item \textbf{G1. Capability of capturing inter-criteria relations:} The mere adoption of existing CF methods only using single ratings in MC settings often produces insufficient accuracy;
%     \item \textbf{G2. Awareness of criterion-specific characteristics:} While considering inter-criteria information, it is crucial to design criterion-specific filters that preserve each criterion's unique characteristics;
%     \item \textbf{G3. Fast runtime capabilities:} Even with the use of graph filtering techniques, the ideal LPF in Eq. \eqref{gfcf} is realized through costly matrix decomposition to obtain $\bar{U}$, which naturally requires additional computing costs especially when MC ratings are concerned. 
% \end{itemize} }


The objective of our study is to judiciously incorporate MC ratings into graph filtering without losing its computational efficiency. To this end, we propose \textsf{CA-GF}, a not only {\it training-free} but also \textit{matrix decomposition-free} graph filtering method. In particular, in \textsf{CA-GF}, \textit{criteria-aware} graph filters built upon an MC user-expansion graph are accommodated to effectively capture the collaborative signal in complex contextual semantics across MC ratings. % for accurate recommendations.

To achieve the aforementioned goals \textbf{G1--G3} in Section \ref{sec 1}, the proposed \textsf{CA-GF} consists of the following components:

\begin{enumerate}[leftmargin=*]
\item \textbf{Graph construction:} To accomplish \textbf{G1}, we initially construct an MC user-expansion graph that enables us to capture complex contextual semantics in MC ratings. Next, we construct an item–-item similarity graph to design a new graph filtering method with regulated edge weights (see Section \ref{sec 3.2}). 
\item \textbf{Graph filtering harnessing criteria awareness:} To accomplish \textbf{G2}, due to the fact that the optimal filter can be found differently for each criterion, we propose {\it criterion-specific} graph filtering (see Section \ref{sec 3.3.1}). Moreover, we perform \textit{criteria preference-infused aggregation} that combines the smoothed signals from each criterion to enrich the criteria awareness (see Section \ref{sec 3.3.3}). 
\item \textbf{Polynomial graph filtering:} To accomplish \textbf{G3}, we propose to use \textit{polynomial graph filtering} \cite{park2024turbo}, which is performed without costly matrix decomposition to achieve extremely fast recommendation when accommodating multiple graph filters (see Section \ref{sec 3.3.2}).
\end{enumerate}

We elaborate on the technical details of the proposed \textsf{CA-GF} method in the following subsections, where the schematic overview is illustrated in Figure \ref{overview}.

% The schematic overview of \textsf{CA-GF} is illustrated in Figure \ref{overview} (refer to Appendix \ref{app:pseudo-code} for the pseudocode of \textsf{CA-GF}).  First, an MC user-expansion graph is constructed based on MC ratings (Section \ref{sec 3.2.1}). Then, the item--item similarity graph is constructed based on the MC user-expansion graph and is adjusted differently according to graph filters (Section \ref{sec 3.2.2}). Next, criterion-specific graph filtering is performed depending on each criterion (Section \ref{sec 3.3.1}). Finally, all smoothed signals are aggregated for the final prediction by infusing the criteria preferences of each user (Section \ref{sec 3.3.3}). We elaborate on each step of \textsf{CA-GF} in the following subsections.

\subsection{Graph Construction}
\label{sec 3.2}
\subsubsection{MC user-expansion graph construction} \label{sec 3.2.1}
As stated in Section \ref{sec 2.3}, graph filtering-based CF methods in single-criterion recommender systems \cite{shen2021powerful,xia2022fire,liu2023personalized, choi2023blurring} typically utilize the rating matrix to construct the item--item similarity graph. However, as long as MC ratings are associated, it is not straightforward how to construct an item--item similarity graph. As the primary component of our study, the first step of \textsf{CA-GF} is to create an \textit{MC user-expansion graph}, where each user is expanded to $C+1$ different \textit{criterion-user nodes} to capture complex semantics inherent in MC ratings. Precisely, the rating matrix $R_{MC} \in \mathbb{R}^{(C+1)|\mathcal{U}|\times |\mathcal{I}|}$ for our MC user-expansion graph is designed by concatenating the $C+1$ rating matrices as follows:
\begin{equation}
    R^T_{MC} = R^T_{0}||R^T_{1}||\ldots||R^T_{C},
\end{equation}
where the operator $||$ denotes the concatenation of matrices, which enables us to explore complex high-order connectivity among criterion-user nodes and item nodes \cite{park2023criteria}. Then, we normalize $R_{MC}$ according to the degree of nodes in the graph as in \cite{{shen2021powerful,xia2022fire,liu2023personalized}}:
\begin{equation}
\label{tild_r_mc}
    \tilde{R}_{MC} = D^{-1/2}_UR_{MC}D^{-1/2}_I,
\end{equation}
where $D_U$ and $D_I$ are the diagonal matrices of criterion-user nodes and item nodes, respectively, defined as $D_U=\text{diag}(R_{MC}\mathbf{1})$ and $D_I = \text{diag}(\mathbf{1}^TR_{MC})$.
% Note that, although such MC user-expansion graph construction was originally introduced in \cite{park2023criteria}, our study differs from \cite{park2023criteria} in that, instead of expanding each item node to $C+1$ criterion-user nodes, our MC user-expansion graph expands each \textit{user node} to $C+1$ criterion-item nodes, which enables us to prevent the problem of high dimensionality of the item--item similarity graph to be used for performing graph filtering.
% Figure \ref{overview} illustrates how to construct the MC user-expansion graph $\tilde{R}_{MC}$ based on MC ratings.

\subsubsection{Item--item similarity graph with adjustment} \label{sec 3.2.2} To perform graph filtering, we construct the normalized item--item similarity graph $ \tilde{P}_\text{MC}\in \mathbb{R}^{|\mathcal{I}|\times|\mathcal{I}|}$ using $\tilde{R}_{MC}$ in Eq. \eqref{tild_r_mc} as follows:
\begin{equation}
     \tilde{P}_\text{MC} = \tilde{R}^T_{MC}\tilde{R}_{MC},
\end{equation}
which represents the degree of similarity between each pair of items. Note that, compared to the case of expanding each item node to $C+1$ criterion-item nodes \cite{park2023criteria}, our approach effectively prevents the high dimensionality problem of the item--item similarity graph, while achieving \textbf{G1} by constructing a unified graph structure across MC ratings towards graph filtering. Next, according to the type of graph filters that we use based on $ \tilde{P}_\text{MC}$, the corresponding filtered signals are over-smoothed or under-smoothed, depending on the intensity of connections between nodes in $ \tilde{P}_\text{MC}$. For instance, using a linear LPF (\textit{i.e.}, $ \tilde{P}_\text{MC}$) would be less prone to over-smoothing since it is associated only with the first-order connectivities. Thus, we aim to adjust the filtered signals differently for each graph filter $f(\cdot)$. To this end, similarly as in \cite{park2024turbo}, we employ an additional adjustment process for the graph $ \tilde{P}_\text{MC}$. This process utilizes the Hadamard power $ \tilde{P}_\text{MC}^{\circ s_{f}}$ by raising each edge weight of $ \tilde{P}_\text{MC}$ to the power of $s_{f}$, which is formulated as
\begin{equation}
\label{adjustment_criterion}
    (\bar{P}_f)_{ij} =  (\tilde{P}_{\text{MC}})^{s_f}_{ij},
\end{equation}
where $(\bar{P}_f)_{ij}$ is the $(i,j)$-th element of matrix $\bar{P}_f$ and $s_{f}$ is the adjustment parameter for graph filter $f(\cdot)$, which will be specified in Section \ref{sec 3.3}.





\subsection{Graph Filtering Harnessing Criteria Awareness}
\label{sec 3.3}
We describe how to perform graph filtering based on the adjusted item--item similarity graph ${\bar{P}_f}$. 
\begin{figure}[t]
    \centering
    \includegraphics[width=1\columnwidth]{figures/Overview_revision.pdf}
    \vspace{-3mm}
    \caption{The schematic overview of \textsf{CA-GF}.}
    \label{overview}
    
\end{figure}

\subsubsection{Criterion-Specific Graph Filtering} \label{sec 3.3.1} In single-criterion recommender systems, it is required to discover only a single optimal LPF that promotes smoothness of the graph signals for denoising \cite{shen2021powerful}. In MC recommendations, however, the optimal LPF for each criterion is often different. For instance, for relatively subjective criteria (\textit{spec.}, \textit{price} in a hotel domain), low-frequency components should be utilized less than those in other criteria. In our study, to attain {\bf G2}, we propose \textit{criterion-specific} graph filtering, which applies \textit{diverse LPFs} for different criteria to smooth out graph signals as shown in Figure \ref{overview}. The predicted rating $s_{u,c}\in \mathbb{R}^{|\mathcal{I}|}$ of user $u$ for criterion $c$ is then characterized as:
\begin{equation}
    \label{criteria-aware graph filtering}
    \textbf{s}_{u,c} = {\mathbf r}_{u,c}f(\bar{P}_f, c),
\end{equation}
where ${\mathbf r}_{u,c}$ is the $u$-th row of $R_c$, which will be used as graph signals of user $u$ for criterion $c$; and $f(\bar{P}_f, c)$ is the graph filter \textit{specific to criterion} $c$ for graph $\bar{P}_f$. Here, Eq. \eqref{criteria-aware graph filtering} is the signal ${\mathbf r}_{u,c}$ convolved with $f(\bar{P}_f, c)$ (refer to Definition \ref{graph_conv_def}). 
% Finally, one can simply aggregate by summing up the smoothed signals in Eq.\eqref{criteria-aware graph filtering} for each criterion:
% \begin{equation}
%     \label{simple_agg}
%     s_{u} = \frac{1}{C+1}\sum_{c=0}^{C}{s_{u,c}},
% \end{equation}
% where $s_{u}$ is the final predicted scores for user $u$, whose $i$-th element indicates preference score of user $u$ on the item $i$.
% Specifically, we denote the normalized Laplacian matrix of $\bar{P}_f$ as $L = I - \bar{P}_f$. Then, since $L$ is a symmetric positive semi-definite matrix, there exists orthogonal eigendecomposition $L = U\Lambda U^T$, where $U$ is the orthogonal matrix of eigenvectors and $\Lambda$ is the diagonal matrix for eigenvalues. Since $U$ is orthogonal (\textit{i.e.}, $UU^T=U^TU =I$), the matrix polynomial of $L$ maps each eigenvalue to the arbitrary polynomial function while maintaining its eigenvectors, without the need for the costly matrix decomposition process.

\subsubsection{Polynomial Graph Filtering} \label{sec 3.3.2}
We now specify the criterion-specific graph filter $f(\bar{P}_f, c)$ in Eq. \eqref{criteria-aware graph filtering} that is decided depending on each criterion $c$. As stated in \textbf{G3}, using multiple filters for MC ratings naturally produces additional computation costs caused by a matrix decomposition process in Eq. \eqref{gfcf}. To bypass the high computation overhead, a recent study \cite{park2024turbo} proposed to use {\it polynomial graph filters} for CF. Inspired by this, we also employ multiple polynomial graph filters, applying a distinct polynomial LPF for each criterion.  The polynomial graph filter up to the $K$-th order can be expressed as
\begin{equation}
\label{matrix_polynomial}
    f(\bar{P}_f, c) = \sum_{k=1}^K{a_{c,k}}\bar{P}_f^k,
\end{equation}
where $f(\bar{P}_f, c)$ is the polynomial graph filter specific to criterion $c$; $a_{c,k}$ is the coefficient of a matrix polynomial; and $K$ is the maximum order of the matrix polynomial basis. Note that, using Eq. \eqref{matrix_polynomial}, any LPFs can be designed by adjusting $a_{c,k}$. To design universal polynomial LPFs, we establish the following lemma.
\begin{lemma}
     \label{thm_poly} \cite{park2024turbo, shen2021powerful}:
     The matrix polynomial $ \sum_{k=1}^K{a_{c,k}}\bar{P}_f^k$ is a graph filter for graph $\bar{P}_f$, with the frequency response function of $h(\lambda) = \sum_{k=1}^K{a_{c,k}}(1 - \lambda)^k$. 
\end{lemma} 
According to Lemma \ref{thm_poly}, one can find the optimal polynomial LPF by extensively searching for $\{a_{c,k}\}_{k=1}^K$ using the validation set, which however comes at the expensive computation costs and thus violates our design goal {\bf G3}. Alternatively, we present three representative (\textit{i.e.}, predefined) polynomial LPFs, namely linear ($\bar{P}_f$), inward ($\bar{P}_f^2$), and outward ($2\bar{P}_f-\bar{P}_f^2$) LPFs, which essentially embrace a broad set of LPFs. These three types of graph filters are implemented within second-order polynomials (\textit{i.e.}, $K=2$). As depicted in Figure \ref{overview}, the three polynomial LPFs behave differently, affecting how much low-frequency components are exploited compared to the rest of the spectrum. We note that one can use other types of LPFs with matrix polynomials based on one's own design choice. Using Lemma \ref{thm_poly}, we theoretically show how each polynomial LP graph filter has its unique frequency response function as follows. All proofs are provided in Appendix \ref{app:proof}.

\begin{corollary}
\textbf{(Linear LPF)} \cite{shen2021powerful, liu2023personalized}:
    \label{thm_linear}
     The matrix $\bar{P}_f$ is equivalent to a linear LPF with the following frequency response function
    \begin{equation}
    \label{eq_thm_linear}
    h(\lambda) = 1 - \lambda.
    \end{equation}
    
\end{corollary} 
    

    \begin{corollary} \textbf{(Inward LPF)}
    \label{thm_inward}
    The matrix $\bar{P}_f^2$ is equivalent to an inward LPF of the item-item similarity graph $\bar{P}_f$ with the following frequency response function
    \begin{equation}
    \label{eq_thm_inward}
    h(\lambda) = \lambda^2 - 2\lambda + 1.
    \end{equation}
    \end{corollary}

    \begin{corollary} \textbf{(Outward LPF)}
    \label{thm_outward}
    The matrix $2\bar{P}_f-\bar{P}_f^2$ is equivalent to an outward LPF of the item-item similarity graph $\bar{P}_f$ with the following frequency response function
    \begin{equation}
    \label{eq_thm_outward}
    h(\lambda) = 1 - \lambda^2.
    \end{equation}
    \end{corollary}
From Corollaries \ref{thm_linear}--\ref{thm_outward}, we pay attention to choosing optimal $f(\bar{P}_f, c)$ out of the three polynomial LPFs for each criterion $c$. Here, it is worth noting that our polynomial LPFs can be implemented through simple matrix calculations, thereby allowing us to more effectively leverage well-optimized machine learning and computation frameworks such as PyTorch \cite{paszke2019pytorch} and CUDA \cite{sanders2010cuda} to enhance computational speed via parallel computation. As shown in Figure \ref{fig:intro_1b}, such operation makes \textsf{CA-GF} immune to the computational burden as the number MC ratings increases, resulting in successfully achieving {\bf G3}. Moreover, thanks to such rapid computation of polynomial graph filtering, the optimal filter for each criterion is easily found using the validation set. For the detailed analysis of computational complexity of \textsf{CA-GF}, we refer to Appendix \ref{app:complexity}.


\subsubsection{Criteria Preference-Infused Aggregation}
\label{sec 3.3.3}
One can aggregate the smoothed signals in Eq. \eqref{criteria-aware graph filtering} by simply summing up the smoothed signals for all criteria. However, users often exhibit different criteria preferences \cite{park2023criteria, sreepada2017multi}. For instance, in a hotel domain, a user tends to make decisions based on the cleanliness aspect of a hotel, while another user decides based on the service aspect. To accommodate such personalized information into our \textsf{CA-GF} method, we additionally devise a novel {\it criteria preference-infused aggregation} technique for elaborately capturing the criteria preferences of each user. From the fact that the number of ratings often differs from each criterion \cite{park2023criteria}, we claim that, if a user gave more and/or higher ratings on a certain criterion, then he/she tends to reveal a higher preference for the criterion, which will be empirically validated via ablation studies in Section \ref{section 4.4}. Based on this claim, we formalize our aggregation technique as follows:
\begin{equation}
\label{pref_mat}
\begin{aligned}
   \hat{C} &= \tilde{X}\bar{T}; \bar{T}_{ij} = T^{s_f}_{ij};T = \tilde{X}^T\tilde{X};\\ \tilde{X} &= XD^{-1}_X X = (R_0\textbf{1}) || (R_1\textbf{1}) || \cdots || (R_C\textbf{1});   
\end{aligned}
\end{equation}
where $X \in \mathbb{R}^{|U|\times (C+1)}$ represents the sum-rating matrix for each criterion whose $(u,c)$-th element refers to the sum of ratings given by a user $u$ to all relevant items for criterion $c$; $\tilde{X}$ is the normalized matrix of $X$ along $D^{-1}_X= \text{diag}(X{\bf 1})$; $T = \tilde{X}^T\tilde{X} \in \mathbb{R}^{(C+1)\times (C+1)}$ is the criterion--criterion similarity graph; $\bar{T}_{ij}$ is the $(i,j)$-th element of the adjacency matrix of criterion--criterion similarity graph $\bar{T}$ adjusted with the parameter $s_f$; and $\hat{C}\in \mathbb{R}^{|U|\times (C+1)}$ is the criteria preference matrix in which $\tilde{X}$ serves as signals for graph filtering. Then, the matrix $\hat{C}$ is used for weights during aggregation to infuse the criteria preferences of users. Finally, as illustrated in Figure \ref{overview}, the predicted rating of user $u$ after the criteria preference-infused aggregation is expressed as
\begin{equation}
    \label{cri_pref_agg}
    {\bf s}_{u} = \frac{1}{C+1}\sum_{c=0}^{C}\hat{C}_{u,c}{{\bf s}_{u,c}} = \frac{1}{C+1}\sum_{c=0}^{C}\hat{C}_{u,c}{\mathbf r}_{u,c}f(\bar{P}_f, c),
\end{equation}
where $\hat{C}_{u,c}$ is the preference of user $u$ on the criterion $c$ in $\hat{C}$. 


% \subsubsection{Matrix polynomial low pass filters}



% \begin{theorem} \cite{liu2023personalized}
% \label{thm_linear}
%  The item-item similarity matrix $\tilde{P}$ is a linear low-pass filter with a frequency response function
% \begin{equation}
% h(\lambda) = 1 - \lambda.
% \end{equation}
% \end{theorem}

% \begin{proof}
% First, we have:
% \begin{equation}
% L = U \Lambda U^T = U\text{diag}(\lambda_1, \lambda_2, \ldots, \lambda_m)U^T,
% \end{equation}
% which means that $L$ is a graph filter with a frequency response function of $h(\lambda) = \lambda$.

% Then, we have: 
% \begin{equation}
% L = I - \tilde{P}.
% \end{equation}
% Suppose $U_i$ is the eigenvector of $L$ corresponds to the eigenvalue $\lambda_i$, the derivation is as follows:
% \begin{equation}
% L U_i = \lambda_iU_i =  (I - \tilde{P}) U_i = U_i - \tilde{P} U_i.
% \end{equation}

% So, we have:
% \begin{equation}
% \tilde{P} U_i = (1 - \lambda_i) U_i,
% \end{equation}
% which means that $L$ and $\tilde{P}$ have the same eigenvectors and the corresponding eigenvalues have the relationship:
% \begin{equation}
% (\lambda_{\tilde{P}})_i = 1 - \lambda_i,
% \end{equation}
% where $(\lambda_{\tilde{P}})_i$ is the $i$-th largest eigenvalue of $\tilde{P}$. Suppose:
% \begin{equation}
% \Lambda_{{\tilde{P}}} = \text{diag}((\lambda_{\tilde{P}})_1,(\lambda_{\tilde{P}})_2, \ldots, (\lambda_{\tilde{P}})_m)
% \end{equation}
% then we have:
% \begin{equation}
% \tilde{P} = U \Lambda_{{\tilde{P}}} U^T = U\text{diag}(1 - \lambda_1, 1 - \lambda_2, \ldots, 1 - \lambda_n)U^T,
% \end{equation}
% which means that $L$ is a low-pass filter with a frequency response function of
% \begin{equation}
% h(\lambda) = 1 - \lambda. 
% \end{equation}
% \end{proof}


% \begin{theorem}
% The matrix $2\tilde{P}-\tilde{P}^2$ is a low-pass filter of the item-item similarity matrix $\tilde{P}$ with a frequency response function
% \begin{equation}
% h(\lambda) = 1 - \lambda^2.
% \end{equation}
% \end{theorem}

% \begin{proof}
% Given symmetric matrix $\tilde{P}$ can be decomposed as $\tilde{P} = U \Lambda_{{\tilde{P}}} U^T$. Then, we have:
% \begin{equation}
% \label{eq1.2-1}
% 2\tilde{P}-\tilde{P}^2 = 2U \Lambda_{{\tilde{P}}} U^T - U \Lambda_{{\tilde{P}}}^2 U^T = U(2\Lambda_{{\tilde{P}}} -\Lambda_{{\tilde{P}}}^2)U^T,
% \end{equation}
% %where $\Lambda = \text{diag}(\lambda_1, \lambda_2, \ldots, \lambda_m)$.

% Then, considering $\Lambda_{{\tilde{P}}}=I-\Lambda$ by Theorem \ref{thm_linear}, \eqref{eq1.2-1} produce:


% \begin{equation}
%  U(2\Lambda_{{\tilde{P}}} -\Lambda_{{\tilde{P}}}^2)U^T = U(2(I-\Lambda) - (I-\Lambda)^2)U^T =U(I-\Lambda^2)U^T
% \end{equation}
% Therefore,
% \begin{equation}
% 2\tilde{P}-\tilde{P}^2 = U(I-\Lambda^2)U^T,
% \end{equation}
% where $2\tilde{P}-\tilde{P}^2$ has the same effect of mapping each eigenvalue $\lambda_i$ of ${\bf L}$ to $1-\lambda^2_i$. Finally, this shows that $2\tilde{P}-\tilde{P}^2$ is a low-pass filter of $\tilde{P}$ with a frequency response function of
% \begin{equation}
% h(\lambda) = 1 - \lambda^2.
% \end{equation}
% \end{proof}

% \begin{theorem}
% The matrix $e^{k(\tilde{P} - I)}$ is a low-pass filter of the item-item similarity matrix $\tilde{P}$ with a frequency response function
% \begin{equation}
% h(\lambda) = e^{-k\lambda}.
% \end{equation}
% \end{theorem}

% \begin{proof}
% Symmetric matrix \( \tilde{P} \) can be decomposed as \( \tilde{P} = U \Lambda_{\tilde{P}} U^T \). According to Theorem \ref{thm_linear}, we have \( \Lambda_{\tilde{P}} = I - \Lambda \). Then, we can write:
% \begin{equation}
% e^{k(\tilde{P} - I)} = e^{k(U (I - \Lambda) U^T - I)} = e^{k(U (I - \Lambda) U^T - UU^T)} = e^{-kU{\Lambda}U^T}.
% \end{equation}

% Using the property of matrix exponentials for a diagonalizable matrix \( A = VDV^{-1} \), where \( e^{A} = Ve^{D}V^{-1} \), and noting that \( U \) is orthogonal (i.e., \( U^T = U^{-1} \)), we get:
% \begin{equation}
% e^{k(\tilde{P} - I)}= e^{-kU\LambdaU^T} = U e^{-k\Lambda} U^T
% \end{equation}
% where $e^{k(\tilde{P} - I)}$ has the same effect of mapping each eigenvalue $\lambda_i$ of ${\bf L}$ to $e^{-k\lambda_i}$. Finally, this shows that $e^{k(\tilde{P} - I)}$ is a low-pass filter of $\tilde{P}$ with a frequency response function of
% \begin{equation}
% h(\lambda) = e^{-k\lambda}.
% \end{equation}
% \end{proof}

% \subsubsection{Criteria preference filtering}


\section{Experimental Evaluation}
\label{section 4}

In this section, we systematically conduct extensive experiments to address the key research questions (RQs) outlined below:

\begin{itemize}    
    \item \textbf{RQ1 (Efficiency):} How fast is \textsf{CA-GF} compared to benchmark MC recommendation methods in terms of runtime?
    \item \textbf{RQ2 (Accuracy):} How much does \textsf{CA-GF} improve top-$K$ recommendation accuracy over benchmark recommendation methods?
    \item \textbf{RQ3 (Ablation study):} How does each component in \textsf{CA-GF} contribute to the recommendation accuracy?  
    \item \textbf{RQ4 (Sensitivity):} How do key parameters affect the performance of \textsf{CA-GF}?
    \item \textbf{RQ5 (Interpretability):} How precisely does \textsf{CA-GF} provide interpretations relavent to the criteria?
\end{itemize}

\subsection{Experimental Settings}
\label{section 4.1}
% Dataset Table S
\begin{table}[t!]
\footnotesize
\caption{Statistics of the three datasets used in our experiments.}
\begin{tabular}{ccccccc}
\hline
\multicolumn{1}{c}{\textbf{}} & {\begin{tabular}[c]{@{}c@{}} \textbf{\# of} \\ \textbf{users}\end{tabular}} & {\begin{tabular}[c]{@{}c@{}} \textbf{\# of} \\ \textbf{items}\end{tabular}} & {\begin{tabular}[c]{@{}c@{}} \textbf{\# of} \\ \textbf{overall ratings}\end{tabular}} & {\begin{tabular}[c]{@{}c@{}} \textbf{\# of} \\ \textbf{MC ratings}\end{tabular}} & {\begin{tabular}[c]{@{}c@{}}$C$  \end{tabular}}& {\begin{tabular}[c]{@{}c@{}} \textbf{Density}  \end{tabular}}  \\ \hline
\multirow{1}{*}{\begin{tabular}[c]{@{}c@{}} \textbf{TA}\end{tabular}}   & 5,132 & 7,205 &  41,638 &  280,521 & 7 & 0.11\%\\\hline
\multirow{1}{*}{\begin{tabular}[c]{@{}c@{}} \textbf{YM}\end{tabular}}   & 1,827 & 1,471 &  46,239 &  231,195 & 4 & 1.72\% \\\hline
\multirow{1}{*}{\begin{tabular}[c]{@{}c@{}} \textbf{BA}\end{tabular}}   & 10,726 & 10,832 &  626,995 &  3,134,981 & 4 & 0.54\% \\\hline
\end{tabular}
\label{datasettable}
\end{table}
\noindent\textbf{Datasets.} We carry out experiments on three public datasets, which are widely used in studies on MC recommendation \cite{park2023criteria, fan2021predicting, tallapally2018user,nassar2020novel, shambour2021deep, li2020learning}: TripAdvisor (TA), Yahoo!Movie (YM), and BeerAdvocate (BA). Statistics of the three datasets are summarized in Table \ref{datasettable}.


\noindent\textbf{Competitors.} To comprehensively demonstrate the superiority of \textsf{CA-GF}, we present six benchmark MC recommendation methods (including ExtandedSAE \cite{tallapally2018user}, AEMC \cite{shambour2021deep}, DMCF \cite{nassar2020novel}, CFM \cite{fan2023improving, fan2021predicting}, CPA-LGC \cite{park2023criteria}, and $\text{GF-CF}_\text{MC}$). Additionally, to observe the potential benefits of MC ratings, we include four representative single-criterion recommendation methods (namely NGCF \cite{wang2019neural}, LightGCN \cite{he2020lightgcn}, GF-CF \cite{shen2021powerful}, and DiffRec \cite{wang2023diffusion}), where only overall ratings are used due to the incapability of using MC ratings as input. In our study, to show the results by a na\"ive extension of GF-CF to MC settings, we additionally introduce a variant of GF-CF, termed $\text{GF-CF}_\text{MC}$. This variant employs GF-CF \cite{shen2021powerful} to each of $C+1$ item--item similarity graphs constructed from MC ratings, and then aggregates the output through summation for the final prediction. We refer to Appendix \ref{app:gfcf_mc} for further details of $\text{GF-CF}_\text{MC}$.

\noindent\textbf{Evaluation protocols.} We randomly select 80\% of the interactions of each user for the training set and the remaining 20\% as the test set. From the training set, we randomly select 10\% of interactions as the validation set for hyperparameter tuning. To evaluate the accuracy of top-$K$ MC recommendation, we use benchmark metrics that are widely used in literature \cite{berg2017graph, ying2018graph,zheng2018spectral,wang2020disentangled,wang2019neural,he2020lightgcn,he2017neural,he2016vbpr,hou2024collaborative,park2024cf}, such as \textit{recall} and \textit{normalized discounted cumulative gain (NDCG)}, where $K$ is set to 5 and 10 by default. In the test phase, we treat user--item interactions with {\it overall ratings} that are higher than the median rating in the test set as positive, following the protocols in other studies on the MC recommendation \cite{park2023criteria, shambour2021deep, nassar2020novel, tallapally2018user}. For each metric, we report the average taken over 10 independent runs except for deterministic methods ({\it i.e.}, GF-CF, $\text{GF-CF}_\text{MC}$, and \textsf{CA-GF}).

\noindent\textbf{Implementation details.} We use the best hyperparameters of competitors obtained by extensive hyperparameter tuning on the validation set. Due to space limitation, we specify default hyperparameters of $\textsf{CA-GF}$ in Appendix \ref{app:hyper_cagf}. Unless otherwise stated, for all training-based methods, we set the dimensionality of the embedding to $64$ and use the Adam optimizer \cite{kingma2015adam}, where the batch size is set to 128. All experiments are carried out with the same device for a fair comparison: Intel (R) 12-Core (TM) i7-9700K CPUs @ 3.60 GHz and GPU of NVIDIA GeForce RTX A6000.

% In RQ2--RQ4, we provide experimental results on all datasets. For RQ1 and RQ5, we show here only the results on BA and TA, respectively, due to space limitation. The results on the other datasets are included in Appendix \ref{app:results_dataset}. We evaluate the performance in terms of the Recall@10 in RQ3--RQ5. 

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.7\columnwidth]{figures/time_baseline.pdf}
%     \vspace{-2mm}
%     \caption{Recall@10 versus runtime among \textsf{CA-GF} and four competitors on BA.}
%     \label{rq2figure}
% \end{figure}

\vspace{-2mm}
\subsection{RQ1: Efficiency Analysis} 


Table \ref{runtime_table} showcases the computational efficiency on the BA dataset, the largest dataset with more than $3M$ MC ratings.\footnote{We refer to Appendix \ref{app:results_dataset} for the empirical analysis of efficiency on other datasets.} Moreover, to validate the scalability of \textsf{CA-GF}, we present a runtime comparison on different device configurations, \textit{i.e.}, cases without GPU ({\it i.e.}, CPU only) and with GPU, using various synthetic datasets with $C=4$ in Figure \ref{rq2figure};\footnote{$\text{GF-CF}_\text{MC}$ was not implemented on GPU as utilizing GPU for calculating ideal LPFs is not straightforward.} in this experiment, we generated seven synthetic datasets whose sparsity level is controlled to $98.5\%$, where the numbers of $($users, items, MC ratings$)$ are set to $\{(1.5K, 3K, 0.3M)$, $(2.5K,5.5K,1M)$, $(4K,6K,1.8M)$, $(5K,9K,3.4M)$, $(8K,10K,6M)$,\newline $(10K,15K,11M)$, $(25K,20K,38M)\}$. Our findings are as follows: 
\vspace{0mm}

\begin{enumerate}[leftmargin=*, label=(\roman*)]
    \item Notably, Table \ref{runtime_table} demonstrates that training-free methods, $\text{GF-CF}_\text{MC}$ and \textsf{CA-GF}, outperform training-based methods such as ExtendedSAE, AEMC, and CPA-LGC in terms of runtime efficiency. Specifically, \textsf{CA-GF} achieves the {\it runtime of} {\it 0.2 seconds}, whereas other training-based methods require over $1,000$ seconds for the model convergence.
    
    \item Furthermore, Table \ref{runtime_table} shows that, although $\text{GF-CF}_\text{MC}$ alleviates the computational demands of matrix decomposition using the generalized power method \cite{journee2010generalized, shen2021powerful}, \textsf{CA-GF} is over $2,160 \times$ faster than $\text{GF-CF}_\text{MC}$ on the BA dataset. This is owing to the use of matrix decomposition-free polynomial filters in \textsf{CA-GF}, which efficiently utilize GPU resources for achieving \textbf{G3}. Moreover, Figure \ref{rq2figure} demonstrates that, while $\text{GF-CF}_\text{MC}$ is a training-free solution, \textsf{CA-GF} consistently and significantly outperforms $\text{GF-CF}_\text{MC}$ in terms of runtime across the datasets of various sizes.

    \item Figure \ref{rq2figure} reveals the scalability of \textsf{CA-GF}. By fully utilizing the parallel computing capabilities of GPU, \textsf{CA-GF} runs consistently within 2 seconds for datasets containing up to $6M$ MC ratings. Even when the size of the loaded data exceeds GPU memory limits, \textsf{CA-GF} demonstrates robust performance on CPU, maintaining runtime below 2 minutes for datasets exceeding $38M$ MC ratings. This highlights the computational efficiency of \textsf{CA-GF}, especially in handling large-scale datasets.

\end{enumerate}

\begin{table}
\centering
\small
\caption{Runtime (in seconds) and recommendation accuracy (in NDCG@10) on the largest dataset (BA).}
\label{runtime_table}
\begin{tabular}{lccc}
\hline
\textbf{Method} & \textbf{Training} & \textbf{NDCG@10} & \textbf{Runtime (s)} \\ \hline
\textbf{ExtendedSAE} & \textcolor{black}{\cmark} & 0.0052 & 11,520 \\
\textbf{AEMC} & \textcolor{black}{\cmark} & 0.0752 & 1,022 \\
\textbf{CPA-LGC} & \textcolor{black}{\cmark} & 0.1396 & 10,020 \\
\textbf{$\text{GF-CF}_\text{MC}$} & \textcolor{orange}{\xmark} & 0.1344 & 274 \\
\textbf{\textsf{CA-GF}} & \textcolor{orange}{\xmark} & \textbf{0.1477} & \textbf{0.2} \\
\hline
\end{tabular}
\vspace{-1mm}
\end{table}



% \begin{table}
% \centering
% \footnotesize
% \caption{Runtime in seconds on the BA dataset.}
% \label{runtime_table}
% \begin{tabular}{lcccccc}
% \hline
% \textbf{} & \textbf{ExtendedSAE}   & \textbf{AEMC} & \textbf{$\text{GF-CF}_\text{MC}$} & \textbf{CPA-LGC} & \textbf{\textsf{CA-GF}} \\ \hline
% Runtime (s) & 11,520 & 1,022 & 274 &  10,020 & \textbf{0.2} \\
% Training & \textcolor{black}{\cmark} &\textcolor{black}{\cmark} &  \textcolor{black}{\xmark} &\textcolor{black}{\cmark} & \textcolor{black}{\xmark}  \\
% \hline
% \end{tabular}
% \end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\columnwidth]{figures/CAGF_scal.pdf}
    \vspace{-4mm}
    \caption{Log-scaled runtime comparison of CA-GF (with and without GPU) and $\text{GF-CF}_\text{MC}$ (CPU) using various synthetic datasets, where OOM (GPU) indicates out-of-memory issues on GPU.}
    \label{rq2figure}
    \vspace{-1mm}
\end{figure}



\subsection{RQ2: Recommendation Accuracy}


We compare the accuracy of \textsf{CA-GF} against the competitors specified in Section \ref{section 4.1}. For the methods that were originally designed for single-criterion recommender systems (NGCF, LightGCN, GF-CF, and DiffRec), we use single ratings ({\it i.e.}, overall ratings). Table \ref{accuracy_table} shows the results of all the competitors and \textsf{CA-GF}. Our findings are as follows:
\begin{enumerate}[leftmargin=*, label=(\roman*)]
    \item  \textsf{CA-GF} consistently outperforms all the competitors regardless of the performance metrics and datasets. In particular, \textsf{CA-GF} achieves state-of-the-art performance with significant gains up to 24\% in terms of the NDCG@5 on TA.

    \item The use of MC ratings remarkably boosts the performance, with CPA-LGC and \textsf{CA-GF} surpassing their single-criterion counterparts, namely LightGCN and GF-CF, respectively.

    \item  MC recommendation methods that are built upon DNNs or matrix factorization (ExtendedSAE, AEMC, DMCF, and CFM) show comparatively inferior performance to that of GCN (CPA-LGC) or graph filtering ($\text{GF-CF}_\text{MC}$ and \textsf{CA-GF}). This implies that explicitly exploiting the complex structural information for MC recommendations is indeed beneficial for accurate recommendations.
    
    \item Meanwhile, performance comparison between \textsf{CA-GF} and $\text{GF-CF}_\text{MC}$ reveals that, while graph filtering-based methods generally yield superior results, the gain of \textsf{CA-GF} over $\text{GF-CF}_\text{MC}$ is also significant. This underscores that the mere adoption of graph filtering for MC recommendations is insufficient to achieve optimal accuracy.
\end{enumerate}
\begin{table}[t]
\centering
\footnotesize
\caption{Performance comparison among \textsf{CA-GF} and all competitors for the three benchmark
datasets. Here, MC represents the methods using MC ratings. The best and second-best performers are highlighted in bold and underline, respectively.}
\label{accuracy_table}
\begin{tabular}{lcccccc}
\hline
\textbf{Method} & \textbf{MC} & \textbf{Metric} & \textbf{K} & \textbf{TA} & \textbf{YM} & \textbf{BA} \\ \hline
\multirow{4}{*}{\textbf{NGCF}} & \multirow{4}{*}{-} & \multirow{2}{*}{Recall@K} & 5 & 0.0370 & 0.0855 & 0.0551 \\
 & &  & 10 & 0.0513 & 0.1352 & 0.0925 \\ \cline{3-7}
 & & \multirow{2}{*}{NDCG@K} & 5 & 0.0310 & 0.1056 & 0.1255 \\
 & &  & 10 & 0.0363 & 0.1183 & 0.1275 \\ \hline
 \multirow{4}{*}{\textbf{LightGCN}} & \multirow{4}{*}{-} & \multirow{2}{*}{Recall@K} & 5 & 0.0550 & 0.0970 & 0.0566 \\
 & &  & 10 & 0.0724 & 0.1502 & 0.0937 \\ \cline{3-7}
 & & \multirow{2}{*}{NDCG@K} & 5 & 0.0512 & 0.1154 & 0.1320 \\
 & &  & 10 & 0.0586 & 0.1325 & 0.1298 \\ \hline
 \multirow{4}{*}{\textbf{GF-CF}} & \multirow{4}{*}{-} & \multirow{2}{*}{Recall@K} & 5 & 0.0596 & 0.1217 & 0.0645 \\
 & &  & 10 & 0.0728 & 0.1753 & 0.1034 \\ \cline{3-7}
 & & \multirow{2}{*}{NDCG@K} & 5 & 0.0570 & 0.1440 & 0.1333 \\
 & &  & 10 & 0.0612 & 0.1574 & 0.1344 \\ \hline
 \multirow{4}{*}{\textbf{DiffRec}} & \multirow{4}{*}{-} & \multirow{2}{*}{Recall@K} & 5 & 0.0574 & 0.0834 & 0.0695 \\
 & &  & 10 & 0.0637 & 0.1003 & 0.1030 \\ \cline{3-7}
 & & \multirow{2}{*}{NDCG@K} & 5 & 0.0527 & 0.1123 & 0.1230 \\
 & &  & 10 & 0.0637 & 0.1420 & 0.1392 \\ \hline
\multirow{4}{*}{\textbf{ExtendedSAE}} & \multirow{4}{*}{\checkmark} & \multirow{2}{*}{Recall@K} & 5 & 0.0024 & 0.0766 & 0.0018 \\
 & &  & 10 & 0.0048 & 0.1150 & 0.0044 \\ \cline{3-7}
 & & \multirow{2}{*}{NDCG@K} & 5 & 0.0012 & 0.0912 & 0.0026 \\
 & &  & 10 & 0.0025 & 0.1001 & 0.0052 \\ \hline
 \multirow{4}{*}{\textbf{AEMC}} & \multirow{4}{*}{\checkmark} & \multirow{2}{*}{Recall@K} & 5 & 0.0538 & 0.0802 & 0.0353 \\
 & &  & 10 & 0.0664 & 0.1112 & 0.0588 \\ \cline{3-7}
 & & \multirow{2}{*}{NDCG@K} & 5 & 0.0530 & 0.0909 & 0.0737 \\
 & &  & 10 & 0.0574 & 0.0990 & 0.0752 \\ \hline
 \multirow{4}{*}{\textbf{DMCF}} & \multirow{4}{*}{\checkmark} & \multirow{2}{*}{Recall@K} & 5 & 0.0312 & 0.0333 & 0.0307 \\
 & &  & 10 & 0.0388 & 0.0470 & 0.0411 \\ \cline{3-7}
 & & \multirow{2}{*}{NDCG@K} & 5 & 0.0334 & 0.0541 & 0.0312 \\
 & &  & 10 & 0.0401 & 0.0614 & 0.0401 \\ \hline
  \multirow{4}{*}{\textbf{CFM}} & \multirow{4}{*}{\checkmark} & \multirow{2}{*}{Recall@K} & 5 & 0.0411 & 0.0420 & 0.0375 \\
 & &  & 10 & 0.0501 & 0.0613 & 0.0531 \\ \cline{3-7}
 & & \multirow{2}{*}{NDCG@K} & 5 & 0.0357 & 0.0392 & 0.0891 \\
 & &  & 10 & 0.0398 & 0.0538 & 0.0920 \\ \hline
\multirow{4}{*}{\textbf{CPA-LGC}} & \multirow{4}{*}{\checkmark} & \multirow{2}{*}{Recall@K} & 5 & \underline{0.0630} & 0.1211 & 0.0625 \\
 & &  & 10 & \underline{0.0830} & 0.1725 & 0.0966 \\ \cline{3-7}
 & & \multirow{2}{*}{NDCG@K} & 5 & 0.0550 & 0.1392 & \underline{0.1388} \\
 & &  & 10 & \underline{0.0650} & 0.1532 & \underline{0.1396} \\ \hline
\multirow{4}{*}{\textbf{$\text{GF-CF}_\text{MC}$}} & \multirow{4}{*}{\checkmark} & \multirow{2}{*}{Recall@K} & 5 & 0.0617 & \underline{0.1223} & \underline{0.0643} \\
 & &  & 10 & 0.0753 & \underline{0.1755} & \underline{0.1038} \\ \cline{3-7}
 & & \multirow{2}{*}{NDCG@K} & 5 & \underline{0.0595} & \underline{0.1457} & 0.1329 \\
 & &  & 10 & 0.0645 & \underline{0.1588} & 0.1344 \\ \hline
\multirow{4}{*}{\textbf{CA-GF}} & \multirow{4}{*}{\checkmark} & \multirow{2}{*}{Recall@K} & 5 & \textbf{0.0750} & \textbf{0.1224} & \textbf{0.0704} \\
 & &  & 10 & \textbf{0.0854} & \textbf{0.1765} & \textbf{0.1144} \\ \cline{3-7}
 & & \multirow{2}{*}{NDCG@K} & 5 & \textbf{0.0738} & \textbf{0.1476} & \textbf{0.1464} \\
 & &  & 10 & \textbf{0.0774} & \textbf{0.1608} & \textbf{0.1477} \\
\hline
\end{tabular}
\vspace{-3mm}
\end{table}
\subsection{RQ3: Ablation Studies}
\label{section 4.4}
\begin{table}[t]
\small
\centering
\caption{The performance comparison among \textsf{CA-GF} and its four variants in terms of the Recall@10. Here, the best performer is highlighted in bold.}
\label{ablation_table}
\begin{tabular}{lccc}
\hline
\textbf{} & \textbf{TA} & \textbf{YM} & \textbf{BA} \\ \hline
\textsf{CA-GF} & \textbf{0.0854} & \textbf{0.1765} & \textbf{0.1147} \\
\textsf{CA-GF-m} & 0.0718 & 0.1751 & 0.1031 \\
\textsf{CA-GF-s} & 0.0713 & 0.1698 & 0.1076 \\
\textsf{CA-GF-f} & 0.0725 & 0.1751 & 0.1077 \\
\textsf{CA-GF-p} & 0.0840 & 0.1760 & 0.1143 \\
\hline
\end{tabular}
\end{table}
\vspace{-0mm}

To analyze the contribution of each component in \textsf{CA-GF}, we conduct an ablation study in comparison with four variants depending on which components are taken into account for designing the end-to-end \textsf{CA-GF} method. The performance comparison among the four methods is presented in Table \ref{ablation_table} {\it w.r.t.} the Recall@10 using three datasets.

\begin{itemize}[leftmargin=*]
\item \textsf{CA-GF}: corresponds to the original \textsf{CA-GF} method without removing any components;
\item \textsf{CA-GF-m}: removes the MC user-expansion graph. That is, only a single criterion (\textit{i.e.}, overall ratings) is used for graph construction; 
\item \textsf{CA-GF-s}: replaces all $s_{f}$'s by 1. That is, the adjustment parameter is ablated;
\item \textsf{CA-GF-f}: sets all $f(\bar{P}_f, c)$'s to the linear LPF in Eq. \eqref{eq_thm_linear}. That is, only a single polynomial LPF is used;
\item \textsf{CA-GF-p}: removes criteria preference-infused aggregation. Instead, all the smoothed signals from each criterion are evenly aggregated by simple summation.

\end{itemize}
Our observations are as follows: 
\begin{enumerate}[leftmargin=*, label=(\roman*)]
    \item All four modules in \textsf{CA-GF} plays a crucial role in the success of the proposed \textsf{CA-GF} method.

    \item The performance gap between \textsf{CA-GF} and \textsf{CA-GF-f} reveals that using diverse polynomial LPFs is important for accurate MC recommendation.
    
    \item The performance gap between \textsf{CA-GF} and \textsf{CA-GF-s} tends to be much higher than that between \textsf{CA-GF} and other variants except for the BA dataset. This finding indicates that the use of proper adjustment parameters for graph filter $f(\bar{P}_f, c)$ is crucial for accurate recommendations.

    \item Albeit slightly, the gain of \textsf{CA-GF} over \textsf{CA-GF-p} manifests a positive impact of criteria preference-infused aggregation on the recommendation accuracy for all the datasets.
\end{enumerate}

\subsection{RQ4: Sensitivity Analysis}

We investigate the impact of key parameters in \textsf{CA-GF} on the recommendation accuracy, which include the selection of graph filters $f(\bar{P}_f, c)$ for criterion $c$ and the adjustment parameter $s_{f}$ for each  $f(\bar{P}_f, c)$. For notational simplicity, we denote the three polynomial LPFs (\textit{i.e.}, linear, inward, and outward LPFs) as L, I, and O, respectively. When each parameter varies so that its effect is revealed, other parameters are set to the pivot values specified in Appendix \ref{app:hyper_cagf}. 
\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.32\linewidth}
        \includegraphics[width=\linewidth]{figures/filter_TA_bar.pdf}
        \caption{TA}
        \label{fig:L}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\linewidth}
        \includegraphics[width=\linewidth]{figures/filter_YM_bar.pdf}
        \caption{YM}
        \label{fig:I}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\linewidth}
        \includegraphics[width=\linewidth]{figures/filter_BA_bar.pdf}
        \caption{BA}
        \label{fig:O}
    \end{subfigure}
    \vspace{-2mm}
    \caption{The effect of three polynomial LPFs $f(\bar{P}_f, c)$ (L, I, and O) for each criterion on the Recall@10.}
    \label{fc_plot}
    \vspace{-1mm}
    
\end{figure}


{\bf (Effect of $f(\bar{P}_f, c)$)}
From Figure \ref{fc_plot}, it is seen that using different filters for each criterion $c$ produces different recommendation accuracies, which confirms that each criterion needs its specific optimal graph filter (see Section \ref{sec 3.3.1}). Here, we again note that the optimal filter for each criterion is found quite promptly, thanks to the minimal computation time required for \textsf{CA-GF}.

{\bf (Effect of $s_{f}$)}
Figure \ref{sc_plot} shows how the Recall@10 behaves according to different values of $s_{f}$ for each of three polynomial LPFs (L,I, and O). Our findings are as follows:
\begin{enumerate}[leftmargin=*, label=(\roman*)]
    \item The performance is not uniformly sensitive across different $s_{f}$'s. Some regimes of $s_{f}$ exhibit high sensitivity, whereas others show low sensitivity.
    \item For L, the optimal $s_{f}$ is found in $s_{f}<1$ except for YM, which is the densest dataset. For I and O, the opposite pattern is observed while the optimum lies mostly in the range of $s_{f}>1$. This is because I and O, implemented using the second-order polynomial of $\tilde{P}$, are more prone to over-smoothing, where predicted ratings become overly similar by exploring the collaborative signal in higher-order connectivities. On the other hand, since L employs only the first-order polynomial of $\tilde{P}$, it is less prone to over-smoothing.
    \item This variation is partly due to the use of different orders of $\tilde{P}$ for each polynomial LPF. Hence, in performance optimization, it is of utmost importance to select an appropriate value of $s_{f}$ for each $f(\bar{P}_f, c)$. For example, for I and O, it is desirable to use $s_{f}>1$ to effectively mitigate the over-smoothing problem by enhancing value distinctions.
    %\footnote{For $0<a<b<1$, the value of $(\frac{b}{a})^{s_{f}}$ becomes large with increasing $s_{f}$.}
\end{enumerate} 
\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.32\linewidth}
        \includegraphics[width=\linewidth]{figures/plot_L.pdf}
        \caption{L}
        \label{fig:L}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\linewidth}
        \includegraphics[width=\linewidth]{figures/plot_I.pdf}
        \caption{I}
        \label{fig:I}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\linewidth}
        \includegraphics[width=\linewidth]{figures/plot_O.pdf}
        \caption{O}
        \label{fig:O}
    \end{subfigure}
\vspace{-2mm}
    \caption{The effect of adjustment parameter $s_{f}$ for three polynomial LPFs (L, I, and O) on the Recall@10.}
    \label{sc_plot}
\end{figure}

\vspace{-3mm}
\subsection{RQ5: Interpretability}

Unlike canonical black-box recommendation methods based on DNNs, \textsf{CA-GF} offers significant advantages in model interpretability, allowing an in-depth understanding of each prediction process. For instance, we generate an attribution map that quantifies the influence of MC on the model predictions, Specifically, the attribution map of the user--item interaction $(u, i)$ visualizes the $i$-th component of $\hat{C}_{u,c}{{\bf s}_{u,c}}\in\mathbb{R}^{|\mathcal{I}|}$ in Eq. \eqref{cri_pref_agg} for different $c$'s. It represents each user's criteria preferences for certain items, which provides insight into the importance of each criterion. For instance, Figure \ref{interpret_plot} illustrates two attribution maps for the two different interactions of (user, item), including $(1,7)$ and $(1844,1)$ on the TA dataset. The following observations are made. 
\begin{enumerate}[leftmargin=*, label=(\roman*)]
\vspace{-1mm}
\item These two instances display different patterns, which verifies that each criterion makes a different contribution to the model prediction.
\item It is likely that the two criteria \textit{check-in} and \textit{rooms} contribute less to the prediction of \textsf{CA-GF}, compared to the other criteria.
\end{enumerate}
This level of interpretability is invaluable, not just for comprehending the model's functionality but also for guiding strategic business decisions, thus enabling model refinement and enhancement.
\begin{figure}[t]
\vspace{-4mm}
    \centering
    \begin{subfigure}[b]{0.8\linewidth}
        \includegraphics[width=\linewidth]{figures/TA_u1_i7.png}
        \caption{(user 1, item 7)}
        \label{fig:L}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.8\linewidth}
        \includegraphics[width=\linewidth]{figures/TA_u1844_i1.png}
        \caption{(user 1844, item 1)}
        \label{fig:I}
    \end{subfigure}
    \hfill
    
    \caption{Attribution maps that visualize the contribution of each criterion to \textsf{CA-GF}'s predictions for the TA dataset.}
    \vspace{0mm}
    \label{interpret_plot}
\end{figure}
\vspace{-2mm}
\section{Related Work}
\label{section 5}
\noindent{\bf Graph filtering-based approaches.} Within the realm of graph signal processing, GCN is regarded as a parameterized graph convolutional filter \cite{DBLP:conf/iclr/KipfW17, shen2021powerful}. As a representative approach, NGCF \cite{wang2019neural} was introduced by learning appropriate LPFs while capturing high-order collaborative signals inherent in user--item interactions \cite{shen2021powerful}. LightGCN \cite{he2020lightgcn} achieved convincing performance by eliminating linear transformation and non-linear activation from the GCN layers in NGCF. By closing the gap between LightGCN and graph filtering methods alongside a closed-form solution for the infinite-dimensional LightGCN, GF-CF \cite{shen2021powerful} stood out for achieving accurate recommendation performance while significantly reducing time consumption with its parameter-free nature. PGSP \cite{liu2023personalized} made use of a mixed-frequency filter that combines a linear LPF with an ideal LPF. Turbo-CF \cite{park2024turbo} introduced polynomial graph filtering for CF, enabling diverse LPF designs without costly matrix decomposition. In a subsequent study, to show its efficacy in another recommendation scenario, Group-GF \cite{kim2025leveraging} was developed by presenting multi-view polynomial graph filtering that offers a holistic view of complex user--group dynamics in the group recommendation task. 

\noindent\textbf{MC recommender systems.} Efforts have consistently been made to incorporate MC ratings to enhance the accuracy of recommendations. In one of the initial endeavors, a support vector regression-based approach \cite{jannach2012accuracy} was introduced to assess the relative importance of individual criteria ratings. CFM \cite{fan2021predicting} was formulated by collectively employing matrix factorization for MC rating matrices. DTTD \cite{chen2021deep} was developed by integrating cross-domain knowledge alongside side information. Moreover, in light of the extensive adoption of deep learning, there has been a continuous endeavor to develop DNN-based MC recommender systems. For instance, ExtendedSAE \cite{tallapally2018user} was introduced to capture the relationship between MC ratings using stacked autoencoders. LatentMC \cite{li2019latent} was designed with variational autoencoders to map user reviews into latent vectors, constituting latent MC ratings. DMCF \cite{nassar2020novel} was devised for predicting MC ratings using a DNN, with the predicted ratings being aggregated by another DNN. AEMC \cite{shambour2021deep} employed deep autoencoders, capturing non-linear relationships between users' preferences for criteria. As pioneer work on integrating light graph convolution into MC recommender systems, CPA-LGC \cite{park2023criteria} was devised to capture complex semantics in MC ratings. 
\vspace{-3mm}

 
\section{Conclusions and future work}
\label{section 6}
In this paper, we explored how fast and accurate graph filtering can be developed in MC recommender systems. To this end, we introduced \textsf{CA-GF}, the first attempt to design a graph filtering-based MC recommendation method that is not only training-free but also matrix decomposition-free, thereby circumventing the problem of the computational burden that MC ratings entail. Through extensive experiments on three benchmark datasets, we demonstrated the impact and benefits of \textsf{CF-GF} from various perspectives, including (a) the extraordinarily computational efficiency with the runtime of less than 0.2 seconds on BA, the largest dataset, (b) the superior accuracy over other competing MC recommendation methods, (c) the impact of using different optimal LPFs for each criterion, (d) the effectiveness of each component, and (e) the interpretability via visualizing each user’s criteria preferences for certain items.
Potential avenues of our future research include the design of an adaptive graph filter such that the optimal LPF for each criterion is found more effectively.
\vspace{-3mm}
\section*{Acknowledgments}
This research was supported by the National Research Foundation
of Korea (NRF) grant funded by the Korea government (MSIT) (No. RS-2021-NR059723, No. RS-2023-00220762).





\bibliographystyle{ACM-Reference-Format}
\balance
\bibliography{0.0.citation_list}

%%
%% If your work has an appendix, this is the place to put it.
\newpage





% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \begin{table}[]
% \begin{tabular}{|l|llllllll|lllll|lllll|}
% \hline
%                    & \multicolumn{8}{l|}{T}                                                                                                                                                           & \multicolumn{5}{l|}{Y}                                                                                & \multicolumn{5}{l|}{R}                                                                                \\ \hline
% \multirow{2}{*}{f} & \multicolumn{1}{l|}{0} & \multicolumn{1}{l|}{1} & \multicolumn{1}{l|}{2} & \multicolumn{1}{l|}{3} & \multicolumn{1}{l|}{4} & \multicolumn{1}{l|}{5} & \multicolumn{1}{l|}{6} & 7 & \multicolumn{1}{l|}{0} & \multicolumn{1}{l|}{1} & \multicolumn{1}{l|}{2} & \multicolumn{1}{l|}{3} & 4 & \multicolumn{1}{l|}{0} & \multicolumn{1}{l|}{1} & \multicolumn{1}{l|}{2} & \multicolumn{1}{l|}{3} & 4 \\ \cline{2-19} 
%                    & \multicolumn{1}{l|}{}  & \multicolumn{1}{l|}{}  & \multicolumn{1}{l|}{}  & \multicolumn{1}{l|}{}  & \multicolumn{1}{l|}{}  & \multicolumn{1}{l|}{}  & \multicolumn{1}{l|}{}  &   & \multicolumn{1}{l|}{}  & \multicolumn{1}{l|}{}  & \multicolumn{1}{l|}{}  & \multicolumn{1}{l|}{}  &   & \multicolumn{1}{l|}{}  & \multicolumn{1}{l|}{}  & \multicolumn{1}{l|}{}  & \multicolumn{1}{l|}{}  &   \\ \hline
% \multirow{2}{*}{s} & \multicolumn{8}{l|}{}                                                                                                                                                            & \multicolumn{5}{l|}{}                                                                                 & \multicolumn{5}{l|}{}                                                                                 \\ \cline{2-19} 
%                    & \multicolumn{1}{l|}{}  & \multicolumn{1}{l|}{}  & \multicolumn{1}{l|}{}  & \multicolumn{1}{l|}{}  & \multicolumn{1}{l|}{}  & \multicolumn{1}{l|}{}  & \multicolumn{1}{l|}{}  &   & \multicolumn{1}{l|}{}  & \multicolumn{1}{l|}{}  & \multicolumn{1}{l|}{}  & \multicolumn{1}{l|}{}  &   & \multicolumn{1}{l|}{}  & \multicolumn{1}{l|}{}  & \multicolumn{1}{l|}{}  & \multicolumn{1}{l|}{}  &   \\ \hline
% \end{tabular}
% \end{table}


\appendix

\section{Definition of LPF}
We formally define the LPF as follows:
\label{app:LPF}
\begin{definition}
    (LPF) \cite{shen2021powerful,ramakrishna2020user,wai2019blind}: 
For $k=1,\cdots,|V|$ and $\lambda_1\le\cdots\le\lambda_{|V|}$, the graph filter $H(L)$ is $k$-low-pass if and only if $\eta_k \in [0,1]$, where
\begin{equation}
\eta_k:=\frac{max\{|h(\lambda_{k+1})|, \cdots, |h(\lambda_{|V|})|\}}{min\{|h(\lambda_{1})|, \cdots, |h(\lambda_{k})|\}}.
\end{equation}
\end{definition}
\vspace{2mm}
\section{Pseudocode of \textsf{CA-GF}}
\label{app:pseudo-code}

We summarize the end-to-end process of \textsf{CA-GF} in
Algorithm \ref{cpalgc_pseudo}.

\begin{algorithm}
\caption{\textsf{CA-GF}}
\label{cpalgc_pseudo}
\begin{algorithmic}[1]
  \renewcommand{\algorithmicrequire}{\textbf{Input:}}
  \renewcommand{\algorithmicensure}{\textbf{Output:}}
\REQUIRE MC ratings $R_0 \times R_1 \times ... \times R_C$, set of users $\mathcal{U}$, set of items $\mathcal{I}$
\ENSURE ${\bf s}_{u}$ for $u \in \mathcal{U}$
\\
\STATE $R^T_{MC} = R^T_{0}||R^T_{1}||\ldots||R^T_{C}$
\STATE $\tilde{R}_{MC} = D^{-1/2}_UR_{MC}D^{-1/2}_I$
\STATE $\tilde{P} = \tilde{R}^T_{MC}\tilde{R}_{MC}$
\STATE Calculate $\bar{P}_f$ for each $f(\cdot)$ using Eq. \eqref{adjustment_criterion}
% \STATE $\tilde{X} = XD^{-1}_X X = (R_0\textbf{1}) || (R_1\textbf{1}) || \cdots || (R_C\textbf{1})$ 
\STATE Calculate $\hat{C}$ using Eq. \eqref{pref_mat}
\\
\FOR {$u \leftarrow 0$ to $|\mathcal{U}|$}
\FOR {$c \leftarrow 0$ to $C$}
\STATE $\textbf{s}_{u,c} = {\mathbf r}_{u,c}f(\bar{P}_f, c)$ 
\ENDFOR
\STATE ${\bf s}_{u} = \frac{1}{C+1}\sum_{c=0}^{C}\hat{C}_{u,c}{{\bf s}_{u,c}}$
\ENDFOR
\RETURN ${\bf s}_{u}$ for $u \in \mathcal{U}$
\end{algorithmic}
\end{algorithm}
% \vspace{2mm}

\section{Proofs of Corollaries}
\label{app:proof}
\noindent\textbf{Proof of Corollary 4.1.} Although the proof of Corollary 4.1 is presented in \cite{liu2023personalized}, we provide the full proof for completeness. First, the Laplacian matrix $L$ can be decomposed as
\begin{equation}
L = U \Lambda U^T = U\text{diag}(\lambda_1, \lambda_2, \ldots, \lambda_m)U^T,
\end{equation}
which indicates that $L$ is a graph filter with the frequency response function of $h(\lambda) = \lambda$. Then, we have
\begin{equation}
\label{eq21}
L = I -\bar{P}_f.
\end{equation}
Suppose that $U_i$ is the eigenvector of $L$ corresponding to the eigenvalue $\lambda_i$,  Then, using Eq. \eqref{eq21}, we have
\begin{equation}
L U_i = \lambda_iU_i =  (I -\bar{P}_f) U_i,
\end{equation}
resulting in
\begin{equation}
\bar{P}_f U_i = (1 - \lambda_i) U_i.
\end{equation}
This means that $L$ and $\bar{P}_f$ have the same eigenvectors and the corresponding eigenvalues have the following relationship:
\begin{equation}
(\lambda_{\bar{P}_f})_i = 1 - \lambda_i,
\end{equation}
where $(\lambda_{\bar{P}_f})_i$ is the $i$-th largest eigenvalue of $\bar{P}_f$. Suppose
\begin{equation}
\Lambda_{{\bar{P}_f}} = \text{diag}((\lambda_{\bar{P}_f})_1,(\lambda_{\bar{P}_f})_2, \ldots, (\lambda_{\bar{P}_f})_m).
\end{equation}
Then, we have
\begin{equation}
\bar{P}_f = U \Lambda_{{\bar{P}_f}} U^T = U\text{diag}(1 - \lambda_1, 1 - \lambda_2, \ldots, 1 - \lambda_n)U^T,
\end{equation}
which means that $\bar{P}_f$ is the polynomial LPF with the frequency response function of
\begin{equation}
h(\lambda) = 1 - \lambda,
\end{equation}
which completes the proof of the Corollary \ref{thm_linear}.

\vspace{20pt}

\noindent\textbf{Proof of Corollary 4.2.} The symmetric matrix \(\bar{P}_f\) can be decomposed as \(\bar{P}_f = U \Lambda_{\bar{P}_f} U^T\), where \(\Lambda_{\bar{P}_f}\) is a diagonal matrix of the eigenvalues of \(\bar{P}_f\). Now, due to the fact that $U^T=U^{-1}$, $\bar{P}_f^2$ becomes
\begin{equation}
\bar{P}_f^2 = (U \Lambda_{\bar{P}_f} U^T)(U \Lambda_{\bar{P}_f} U^T) = U \Lambda_{\bar{P}_f}^2 U^T.
\end{equation}
Using the relation \(\Lambda_{\bar{P}_f} = I - \Lambda\) from Corollary \ref{thm_linear}, we have
\begin{equation}
\label{eq28}
\bar{P}_f^2 = U (I - \Lambda)^2 U^T.
\end{equation}
Using the expansion
\begin{equation}
\begin{aligned}
    (I - \Lambda)^2 = I - 2\Lambda + \Lambda^2,
\end{aligned}
\end{equation}
Eq. \eqref{eq28} can be rewritten as
\begin{equation}
\bar{P}_f^2 = U(I - 2\Lambda + \Lambda^2)U^T,
\end{equation}
which indicates that \(\bar{P}_f^2\) plays a role of mapping each eigenvalue \(\lambda_i\) of \(\bar{P}_f\) to \(\lambda_i^2 - 2\lambda_i + 1\). Hence, the frequency response function of $\bar{P}_f^2$ is given by
\begin{equation}
h(\lambda) = \lambda^2 - 2\lambda + 1,
\end{equation}
which completes the proof of Corollary \ref{thm_inward}.

\vspace{20pt}

\noindent\textbf{Proof of Corollary 4.3.} Given $\bar{P}_f = U \Lambda_{{\bar{P}_f}} U^T$, we have
\begin{equation}
\label{eq1.2-1}
\begin{aligned}
2\bar{P}_f-\bar{P}_f^2 &= 2U \Lambda_{{\bar{P}_f}} U^T - U \Lambda_{{\bar{P}_f}}^2 U^T \\
&= U(2\Lambda_{{\bar{P}_f}} -\Lambda_{{\bar{P}_f}}^2)U^T.
\end{aligned}
\end{equation}
Using the relation $\Lambda_{{\bar{P}_f}}=I-\Lambda$ from Collorary \ref{thm_linear}, we have
\begin{equation}
 2\Lambda_{{\bar{P}_f}} -\Lambda_{{\bar{P}_f}}^2 = 2(I-\Lambda) - (I-\Lambda)^2 =I-\Lambda^2.
\end{equation}
Thus, Eq. \eqref{eq1.2-1} can be rewritten as
\begin{equation}
2\bar{P}_f-\bar{P}_f^2 = U(I-\Lambda^2)U^T,
\end{equation}
which indicates that $2\bar{P}_f-\bar{P}_f^2$ contributes to mapping each eigenvalue $\lambda_i$ of $L$ to $1-\lambda^2_i$. Hence, $2\bar{P}_f-\bar{P}_f^2$ is the polynomial LPF of $\bar{P}_f$ with the frequency response function of
\begin{equation}
h(\lambda) = 1 - \lambda^2,
\end{equation}
which completes the proof of Corollary \ref{thm_outward}.


\section{Complexity Analysis}
\label{app:complexity}
We provide a theoretical analysis of \textsf{CA-GF}'s computational complexity. Although the complexities of sparse matrix multiplication can vary, we base our analysis on the complexity of $O(\text{nnz})$ \cite{yavits2014sparse}, where nnz represents the non-zero components. \textsf{CA-GF} comprises two primary steps: graph construction and filtering. First, for the graph construction step $ P = R_{MC}^TR_{MC} $, the nnz of $ R_{MC} $ is equal to the number of MC ratings, denoted as $ n_{mc}$, hence $O(n_{mc})$.
Next, in the graph filtering stage $R_{MC}P $, the nnz of $ R_{MC} $ is $n_{mc} $, and while the nnz of $P =R^T_{MC}R_{MC} $ is not explicitly expressible to $n_{mc} $, it is equivalent to the total number of user pairs sharing common items, denoted as $ n_{P} $. Typically, $ n_{mc} < n_{P} $, and $n_{P}$ is not significantly large in scale compared to $ n_{mc} $; therefore, we assert that the final computational complexity for graph filtering is $ O(n_{P}) $. 

In consequence, the computational complexity of \textsf{CA-GF} is \textbf{\underline{linear}} in nnz in the polynomial graph filter used, which is generally not large due to the high sparsity characteristic of recommendation datasets. Additionally, using parallel GPU computation for matrix multiplication, \textsf{CA-GF} achieves constant runtime regardless of the dataset size, provided sufficient GPU VRAM is available.

\section{Details of experiments}
\subsection{Dataset Description}
\label{app:dataset}

We describe the details of the datasets used in our experiments.
% , and the number of ratings for each criterion is shown in Table \ref{app:data_rating_table}

\noindent\textbf{TripAdvisor (TA)}: The TA dataset, released by \cite{wang2011latent}, comprises hotel rating information, including an overall rating as well as ratings for seven comprehensive criteria: \textit{business}, \textit{check-in quality}, \textit{cleanliness}, \textit{location}, \textit{rooms}, \textit{service}, and \textit{value}. The ratings are on a scale of 1 to 5 for all criteria.

\noindent\textbf{Yahoo!Movie (YM)}: The YM dataset, first introduced by \cite{jannach2014leveraging}, comprises movie rating information, including an overall rating as well as ratings for four specific criteria: \textit{story}, \textit{acting}, \textit{direction}, and \textit{visuals}. The ratings are on a scale of 1 to 5 for all criteria. 

\noindent\textbf{BeerAdvocate (BA)}: The BA dataset, released by \cite{mcauley2012learning, mcauley2013amateurs}, comprises beer rating information, including an overall rating as well as ratings for four specific criteria: \textit{appearance}, \textit{aroma}, \textit{taste}, and \textit{palate}. The ratings range from 1 to 5 for all criteria.

\subsection{Details of $\text{GF-CF}_\text{MC}$}
\label{app:gfcf_mc}
Figure \ref{fig:gfcf_mc} illustrates the schematic overview of $\text{GF-CF}_\text{MC}$. Given MC rating matrices $R_c$ for criterion $c \in \{0,1,\cdots,C\}$, $\text{GF-CF}_\text{MC}$ first separately construct $C+1$ bipartite graphs, and performs GF-CF \cite{shen2021powerful} on each of $C+1$ different criteria. Then, the predicted ratings $\hat{s}_{u,c}$ for each criterion are aggregated by simple summation for the final prediction. This approach differs from \textsf{CA-GF}, which uses an integrated graph, called the MC user-expansion graph, for graph filtering. Moreover,  while $\text{GF-CF}_\text{MC}$ uses the same graph filter across the criteria, \textsf{CA-GF} leverages a different optimal graph filter depending on the criterion, which effectively harnesses criteria awareness.

\begin{table}
\centering
\footnotesize
\caption{Runtime on the three benchmark datasets.}
\label{app:runtime_table}
\begin{tabular}{lcccccc}
\hline
\textbf{} & \textbf{ExtendedSAE} & \textbf{AEMC} & \textbf{CPA-LGC} & \textbf{$\text{GF-CF}_\text{MC}$} & \textbf{\textsf{CA-GF}} \\ \hline
TA & 2,657 & 871 & 2,345 & 316 & \textbf{0.2} \\
YM & 45 & 27 & 731 & 34 & \textbf{0.03} \\
BA & 11,520 & 1,022 & 10,020 & 274 & \textbf{0.2} \\
Training & \textcolor{black}{\cmark} & \textcolor{black}{\cmark} & \textcolor{black}{\cmark} & \textcolor{orange}{\xmark} & \textcolor{orange}{\xmark} \\
\hline
\end{tabular}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/GF-CF_MC.pdf}
    \caption{The schematic overview of $\text{GF-CF}_\text{MC}$.}
    \label{fig:gfcf_mc}
\end{figure}

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.85\linewidth}
        \includegraphics[width=\linewidth]{figures/YM_u1_i25.png}
        \includegraphics[width=\linewidth]{figures/YM_u165_i855.png}
        \caption{(user 1, item 125) and (user 165, item 1855) on YM}
        \label{fig:L}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.85\linewidth}
        \includegraphics[width=\linewidth]{figures/BA_u10_i544.png}
        \includegraphics[width=\linewidth]{figures/BA_u4895_i382.png}
        \caption{(user 10, item 1544) and (user 4895, item 1382) on BA}
        \label{fig:O}
    \end{subfigure}   
    \caption{Attribution maps that visualize the contribution of each criterion to \textsf{CA-GF}'s predictions for the (a) YM and (b) BA datasets.}
    \label{app:interpret_plot}
\end{figure}

\subsection{Default Hyperparameters of \textsf{CA-GF}}
\label{app:hyper_cagf}
We describe default hyperparameters of \textsf{CA-GF}, each of which is tuned on the validation set. The hyperparameters of \textsf{CA-GF} are listed as follows: 1) the polynomial LPF $f(\bar{P}_f, c)$; 2) the adjustment parameter $s_{f}$ used for the item--item similarity graph $\tilde{P}_\text{MC}$; and 3) the adjustment parameter $s$ used for the criterion--criterion similarity graph $\bar{T}$. First, the polynomial LPF $f(\bar{P}_f, c)$ optimally found for all criteria on each dataset is as follows:
\begin{itemize}
    \item TA: \{\textit{overall}: O, \textit{business}: L, \textit{check-in}: I, \textit{cleanliness}: L, \textit{location}: L, \textit{rooms}: I, \textit{service}: L, \textit{value}: L\};
    \item YM: \{\textit{overall}: O, \textit{acting}: I, \textit{direction}: I, \textit{story}: O, \textit{visuals}: L \};
    \item BA: \{\textit{overall}: L, \textit{appearance}:O, \textit{aroma}: I, \textit{palate}: I, \textit{taste}: L\}.
\end{itemize}  
Second, the adjustment parameter $s_{f}$ for each graph filter is set as 
\begin{itemize}
    \item TA: \{L: 0.1, I: 1, O: 1.2\};
    \item YM: \{L: 1, I: 1, O: 1.8\};
    \item BA: \{L: 0.6, I: 0.85, O: 1.5\}.
\end{itemize}
Lastly, the adjustment parameter $s_f$ for the criterion--criterion similarity graph is set as \{TA: 2, YM: 4, BA: 2\}.
 For the graph construction of each dataset in GNN-based competitors (NGCF, LightGCN, and CPA-LGC), edges are included if their ratings are higher than the median value.

\subsection{Experimental Results on Other Datasets}
\label{app:results_dataset}
First, Table \ref{app:runtime_table} showcases the (average) runtime on all the datasets including BA. Here, the runtime of training-based methods (ExtendedSAE, AEMC, and CPA-LGC) refers to the total time spent on model training for 100 epochs, whose duration is empirically determined to be sufficient for model convergence. Second, in Figure \ref{app:interpret_plot}, we show the experimental results corresponding to RQ5 on the YM and BA datasets, along with the attribution maps visualizing the contribution of each criterion to the model prediction.

\subsection{Further Ablation Studies and Comparative Analyses}

\begin{table}[t]
\centering
\caption{Performance comparison of \textsf{CA-GF} and \textsf{CA-GF}$_{MC}$ in terms of Recall@10.}
\label{tab:ablation-study}
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{TA} & \textbf{YM} & \textbf{BA} \\
\midrule
\textsf{CA-GF}         & \textbf{0.0854} & \textbf{0.1765} & \textbf{0.1147} \\
\textsf{CA-GF}$_{MC}$  & 0.0802          & 0.1759          & 0.1077          \\
\bottomrule
\end{tabular}
\end{table}

% \begin{table}[h]
% \centering
% \caption{Performance comparison of \textsf{CA-GF}, its variant \textsf{CA-GF}-m, and single-criterion recommendation methods on the TA dataset.}
% \label{tab:single-criterion-comparison}
% \begin{tabular}{lcc}
% \toprule
% \textbf{Method}   & \textbf{Recall@10} & \textbf{Runtime (s)} \\
% \midrule
% LightGCN          & 0.0724             & 3242                 \\
% DiffRec           & 0.0637             & 827                  \\
% GF-CF             & 0.0728             & 77                   \\
% \textsf{CA-GF}-m           & 0.0718             & \textbf{0.1}                  \\
% \textsf{CA-GF}            & \textbf{0.0854}    & 0.2        \\
% \bottomrule
% \end{tabular}
% \end{table}




To further evaluate the effectiveness of \textsf{CA-GF}, we perform an ablation study and a comparative analysis with single-criterion recommendation methods. Table~\ref{tab:ablation-study} compares \textsf{CA-GF} to its variant, \textsf{CA-GF}$_{MC}$, which applies graph filtering by constructing individual graphs for each criterion rating. \textsf{CA-GF} consistently outperforms \textsf{CA-GF}$_{MC}$ across all datasets, highlighting the importance of modeling inter-criteria relationships via the MC user-expansion graph. Additionally, Table~\ref{tab:single_criterion} compares \textsf{CA-GF} to single-criterion recommendation methods, including \textsf{CA-GF}-m, a variant of \textsf{CA-GF}, that removes the MC user-expansion graph from \textsf{CA-GF} and thus is regarded as another single-criterion GF method. It is demonstrated that \textsf{CA-GF} outperforms state-of-the-art single-criterion recommendation methods like LightGCN and DiffRec in both accuracy and runtime efficiency across all the datasets while being slightly inferior to \textsf{CA-GF}-m in terms of runtime. These findings manifest \textsf{CA-GF}'s ability to effectively and efficiently achieve outstanding performance via aggregating information from MC ratings.
\begin{table}[t]
\centering
\scriptsize
\caption{Performance comparison of \textsf{CA-GF} and single-criterion recommendation methods in terms of Recall@10 and runtime (in seconds).}
\label{tab:single_criterion}
\begin{tabular}{lcccccc}
\toprule
\multirow{2}{*}{Method} & \multicolumn{2}{c}{TA} & \multicolumn{2}{c}{YM} & \multicolumn{2}{c}{BA} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
 & Recall@10 & Runtime & Recall@10 & Runtime & Recall@10 & Runtime \\
\midrule
LightGCN   & 0.0724 & 3242 & 0.1502 & 1154 & 0.0937 & 6770 \\
DiffRec    & 0.0637 & 827  & 0.1003 & 300  & 0.1030 & 1725 \\
GF-CF      & 0.0728 & 77   & 0.1753 & 27   & 0.1034 & 161 \\
\textsf{CA-GF}-m & 0.0718 & \textbf{0.1} & 0.1751 & \textbf{0.02} & 0.1031 & \textbf{0.17} \\
\textsf{CA-GF}   & \textbf{0.0854} & 0.2 & \textbf{0.1765} & 0.03 & \textbf{0.1144} & 0.2 \\
\bottomrule
\end{tabular}
\end{table}


% Second, in Figure \ref{app:interpret_plot}, we show the experimental results corresponding to RQ5 on the YM and BA datasets, along with the attribution maps visualizing the contribution of each criterion to the model prediction.

% \begin{table}[t]
% % \setcounter{}{7}
% \centering
% \small
% \caption{Runtime on the TA and YM datasets.}
% \label{app:runtime_table}
% \begin{tabular}{lccccc}
% \hline
% \textbf{} & \textbf{ExtendedSAE}   & \textbf{CPA-LGC} & \textbf{$\text{GF-CF}_\text{MC}$} & \textbf{\textsf{CA-GF}} \\ \hline
% TA & 44m17s & 39m5s & 2m47s  & \textbf{0.2s} \\
% YM & 45s  & 12m11s & 17s & \textbf{0.1s} \\
% Training & \textcolor{black}{\cmark}  & \textcolor{black}{\cmark} & \textcolor{black}{\xmark} & \textcolor{black}{\xmark}  \\
% \hline
% \end{tabular}
% \end{table}
% -------------------------------


% \begin{table}[t]
% \centering
% \footnotesize
% \caption{Number of ratings for each criterion in the three datasets}
% \label{app:data_rating_table}
% \vspace{-4mm}
% \begin{subtable}{\linewidth}
%   \centering
%   \caption{TA}
%   \begin{tabular}{lcccccc}
%   \hline
%   \textbf{overall} & \textbf{business} & \textbf{checkin} & \textbf{cleanliness} & \textbf{location} & \textbf{rooms} & \textbf{service} \\
%   \hline
%   41,638 & 40,879 & 40,434 & 41,393 & 38,261 & 41,430 & 22,624 \\
%   \hline
%   \end{tabular}
% \end{subtable}

% \begin{subtable}{\linewidth}
%   \centering
%   \caption{YM}
%   \begin{tabular}{lcccccc}
%   \hline
%   \textbf{overall} & \textbf{acting} & \textbf{direction} & \textbf{story} & \textbf{visuals} \\
%   \hline
%   46,239 & 46,239 & 46,239 & 46,239 & 46,239 \\
%   \hline
%   \end{tabular}
% \end{subtable}

% \begin{subtable}{\linewidth}
%   \centering
%   \caption{BA}
%   \begin{tabular}{lccccc}
%   \hline
%   \textbf{overall} & \textbf{appearance} & \textbf{aroma} & \textbf{palate} & \textbf{taste} \\
%   \hline
%   626,995 & 626,995 & 626,997 & 626,997 & 626,997 \\
%   \hline
%   \end{tabular}
% \end{subtable}

% \end{table}
% \begin{table}[t]
% % -------------------------------
% \setcounter{}{7}




\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
