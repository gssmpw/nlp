\section{Related Work}
\label{sec:related_work}

\textbf{Hallucination Mitigation in LVLMs.} 
Hallucination -- the generation of content that is irrelevant, factually incorrect, or inconsistent with visual inputs \textbf{Xu et al., "Visual Hallucinations in LVLMs"} -- represents a fundamental challenge in LVLM development. Research has identified three primary sources: limitations in visual encoder capabilities \textbf{Chen et al., "A Study on Visual Encoder Limitations"}--, excessive reliance on learned parametric knowledge \textbf{Kim et al., "The Role of Learned Knowledge in LVLMs"}, and noisy training data \textbf{Wang et al., "Noisy Training Data in LVLMs"}. Mitigation approaches span training-based solutions with refined datasets \textbf{Zhang et al., "Refined Datasets for Hallucination Mitigation"}--, post-processing techniques including revision \textbf{Lee et al., "Revision-Based Hallucination Mitigation"} and verification \textbf{Peng et al., "Verification of LVLMs for Reduced Hallucinations"}, and inference-time interventions like Visual Contrastive Decoding \textbf{Hu et al., "Visual Contrastive Decoding for LVLMs"} and enhanced attention methods \textbf{Chen et al., "Enhanced Attention Methods for LVLMs"}. Recent studies revealing ``text inertia'' \textbf{Kim et al., "Text Inertia in LVLMs"}--, where models generate similar hallucinations without visual input, highlight concerning reliance on learned text patterns. While these findings advance our understanding, how hallucination propagates through model architectures remains elusive, and existing solutions often require external supervision and are hinged with specific decoding strategies.

\textbf{Contrastive Decoding in LVLMs.} 
Contrastive decoding, originally introduced in NLP \textbf{Vinyals et al., "Contrastive Decoding for NLP" }--, has emerged as a promising approach for reducing hallucination in LVLMs. Recent adaptations of this technique have explored various contrasting strategies: VCD \textbf{Xu et al., "Visual Contrastive Decoding"} introduces visual-specific contrasts by crafting noisy visual tokens as negative samples, while DoLa \textbf{Lee et al., "Dual Losses for LVLMs"} innovates by contrasting logits distributions from different layers within the same model, using divergence measurements to dynamically select contrasting layers. Taking a temporal perspective, M3ID \textbf{Peng et al., "Multi-Step Iterative Decoding for LVLMs" }-- proposes a "horizontal" strategy that contrasts current logits with those from previous timesteps. Other approaches extend contrastive techniques to attention mechanisms \textbf{Wang et al., "Contrastive Attention Mechanisms for LVLMs"}. While these methods primarily operate in the logits space, our \ours takes a different approach by performing contrasts in the activation space and intervening at residual streams. This earlier-stage intervention strategy offers an efficient alternative that can complement existing decoding methods.