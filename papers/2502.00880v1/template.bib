%% ANNOTATED BIBLIOGRAPHY SOURCES %%

@article{Tam_Framework2006,
title = {A framework for asynchronous change awareness in collaborative documents and workspaces},
journal = {International Journal of Human-Computer Studies},
volume = {64},
number = {7},
pages = {583-598},
year = {2006},
note = {Theoretical and empirical advances in groupware research},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2006.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S1071581906000218},
author = {James Tam and others},
keywords = {Change awareness, Asynchronous awareness},
abstract = {Change awareness is the ability of individuals to track the asynchronous changes made to a collaborative document or graphical workspace by other participants over time. We develop a framework that articulates what change awareness information is critical if people are to track and maintain change awareness. Information elements include: knowing who changed the artifact, what those changes involve, where changes occur, when changes were made, how things have changed and why people made the changes. The framework accounts for people's need to view these changes from different perspectives: an artifact-based view, a person-based view and a workspace-based view. Each information element is further broken down into distinguishing features and matched against these perspectives, e.g., location history within the where category prompts the questions ‘where was this artifact when I left’ in the artifact-based view, ‘where in the workspace has a person visited’ in the person-based view and ‘where have people been in the workspace’ in the workspace-based view. The framework can be used both to inform and critique change awareness tools.}
}

@InProceedings{Kim_PilotVRTours2021,
author="Kim, Jeffrey",
title="A Pilot of Student-Guided Virtual Reality Tours",
booktitle="Collaboration and Integration in Construction, Engineering, Management and Technology",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="251--257",
abstract="Construction management programs that contain technology courses often teach students how to create building information models (BIM). The problem is that creating these models for the first few times is a difficult process. This learning process pushes students' spatial abilities to the limit as they try to understand how buildings come together while trying to perfect their final product. However, when students can view their models at a 1:1 scale instead of on a flat computer screen, they start seeing things differently. Recently, nearly 60 students took part in completing their regularly assigned BIM project in a construction technology class. With the aid of an Oculus Go® virtual reality headset, students walked in their finished product and were able to critique their work from a different perspective. Furthermore, this pilot placed the student's in their model along with their classmates. This way, the author of the model could take their classmates on a virtual tour of their work, allowing multiple people to review and critique the finished product. This paper describes a pilot inquiry into the use of collaborative virtual reality in a 4-year construction management classroom to improve student's building information modeling skills. This paper presents the students' feedback about the experience and documents the researcher's observations in preparation for a plenary study on collaborative virtual reality in the classroom.",
isbn="978-3-030-48465-1"
}

@inproceedings{Wang_AgainTogether2020,
author = {Wang, Cheng Yao and others},
title = {Again, Together: Socially Reliving Virtual Reality Experiences When Separated},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376642},
doi = {10.1145/3313831.3376642},
abstract = {To share a virtual reality (VR) experience remotely together, users usually record videos from an individual's point of view and then co-watch these videos. However, co-watching recorded videos limits users to reliving their memories from the perspective from which the video was captured. In this paper, we describe ReliveInVR, a new time-machine-like VR experience sharing method. ReliveInVR allows multiple users to immerse themselves in the relived experience together and independently view the experience from any perspective. We conducted a 1x3 within-subject study with 26 dyads to compare ReliveInVR with (1) co-watching 360-degree videos on desktop, and (2) co-watching 360-degree videos in VR. Our results suggest that participants reported higher levels of immersion and social presence in ReliveInVR. Participants in ReliveInVR also understood the shared experience better, discovered unnoticed things together and found the sharing experience more fulfilling. We discuss the design implications for sharing VR experiences over time and space.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {immersion, presence, replay, shared experience, social, virtual reality},
location = {<conf-loc>, <city>Honolulu</city>, <state>HI</state>, <country>USA</country>, </conf-loc>},
series = {CHI '20}
}

@InProceedings{Mayer_AsyncManualWork2022,
author="Mayer, Anjela and others",
title="Asynchronous Manual Work in Mixed Reality Remote Collaboration",
booktitle="Extended Reality",
year="2022",
publisher="Springer Nature Switzerland",
address="Cham",
pages="17--33",
abstract="Research in Collaborative Virtual Environments (CVEs) is becoming more and more significant with increasing accessibility of Virtual Reality (VR) and Augmented Reality (AR) technology, additionally reinforced by the increasing demand for remote collaboration groupware. While the research is focusing on methods for synchronous remote collaboration, asynchronous remote collaboration remains a niche. Nevertheless, future CVEs should support both paradigms of collaborative work, since asynchronous collaboration has as well its benefits, for instance a more flexible time-coordination. In this paper we present a concept of recording and later playback of highly interactive collaborative tasks in Mixed Reality (MR). Furthermore, we apply the concept in an assembly training scenario from the manufacturing industry and test it during pilot user experiments. The pilot study compared two modalities, the first one with a manufacturing manual, and another using our concept and featuring a ghost avatar. First results revealed no significant differences between both modalities in terms of time completion, hand movements, cognitive workload and usability. Some differences were not expected, however, these results and the feedback brought by the participants provide insights to further develop our concept.",
isbn="978-3-031-15553-6"
}

@inproceedings{Frecon_BuildingDistVEs1998,
author = {Fr\'{e}con, Emmanuel and others},
title = {Building distributed virtual environments to support collaborative work},
year = {1998},
isbn = {1581130198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/293701.293715},
doi = {10.1145/293701.293715},
booktitle = {Proceedings of the ACM Symposium on Virtual Reality Software and Technology},
pages = {105–113},
numpages = {9},
keywords = {3D collaborative tools, CSCW, CVE, DIVE, room metaphor},
location = {Taipei, Taiwan},
series = {VRST '98}
}

@inproceedings{Fender_CausalityAsync2022,
author = {Fender, Andreas Rene and others},
title = {Causality-preserving Asynchronous Reality},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3501836},
doi = {10.1145/3491102.3501836},
abstract = {Mixed Reality is gaining interest as a platform for collaboration and focused work to a point where it may supersede current office settings in future workplaces. At the same time, we expect that interaction with physical objects and face-to-face communication will remain crucial for future work environments, which is a particular challenge in fully immersive Virtual Reality. In this work, we reconcile those requirements through a user’s individual Asynchronous Reality, which enables seamless physical interaction across time. When a user is unavailable, e.g., focused on a task or in a call, our approach captures co-located or remote physical events in real-time, constructs a causality graph of co-dependent events, and lets immersed users revisit them at a suitable time in a causally accurate way. Enabled by our system AsyncReality, we present a workplace scenario that includes walk-in interruptions during a person’s focused work, physical deliveries, and transient spoken messages. We then generalize our approach to a use-case agnostic concept and system architecture. We conclude by discussing the implications of Asynchronous Reality for future offices.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {634},
numpages = {15},
keywords = {Mixed Reality, Interruption in workplaces, Immersive workspaces, Collaboration, Camera networks, Asynchronous communication},
location = {<conf-loc>, <city>New Orleans</city>, <state>LA</state>, <country>USA</country>, </conf-loc>},
series = {CHI '22}
}

@article{Chow_ChallengesAsyncCollabVR2019,
author = {Chow, Kevin and others},
title = {Challenges and Design Considerations for Multimodal Asynchronous Collaboration in VR},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359142},
doi = {10.1145/3359142},
abstract = {Studies on collaborative virtual environments (CVEs) have suggested capture and later replay of multimodal interactions (e.g., speech, body language, and scene manipulations), which we refer to as multimodal recordings, as an effective medium for time-distributed collaborators to discuss and review 3D content in an immersive, expressive, and asynchronous way. However, there exist gaps of empirical knowledge in understanding how this multimodal asynchronous VR collaboration (MAVRC) context impacts social behaviors in mediated-communication, workspace awareness in cooperative work, and user requirements for authoring and consuming multimedia recording. This study aims to address these gaps by conceptualizing MAVRC as a type of CSCW and by understanding the challenges and design considerations of MAVRC systems. To this end, we conducted an exploratory need-finding study where participants (N = 15) used an experimental MAVRC system to complete a representative spatial task in an asynchronously collaborative setting, involving both consumption and production of multimodal recordings. Qualitative analysis of interview and observation data from the study revealed unique, core design challenges of MAVRC in: (1) coordinating proxemic behaviors between asynchronous collaborators, (2) providing traceability and change awareness across different versions of 3D scenes, (3) accommodating viewpoint control to maintain workspace awareness, and (4) supporting navigation and editing of multimodal recordings. We discuss design implications, ideate on potential design solutions, and conclude the paper with a set of design recommendations for MAVRC systems.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {40},
numpages = {24},
keywords = {virtual reality, speech, spatial task, proxemics, presence, pointing, multimodal recording, immersion, gesture, body language, asynchronous collaboration, 3D}
}

%author={Irlitti, Andrew and Smith, Ross T. and Von Itzstein, Stewart and Billinghurst, Mark and Thomas, Bruce H.},
@INPROCEEDINGS{Irlitti_ChallengesAsync2016,
  author={Irlitti, Andrew and others},
  booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, 
  title={Challenges for Asynchronous Collaboration in Augmented Reality}, 
  year={2016},
  volume={},
  number={},
  pages={31-35},
  keywords={Collaboration;Electronic mail;Augmented reality;Context;Stakeholders;Resists;Production;H.5.3 [Information interfaces and presentation (e.g.;HCI)]: Group and Organization Interfaces—Asynchronous interaction—Computer-supported cooperative work; I.3.6 [Computer Graphics]: Methodology and Techniques—Interaction Techniques},
  doi={10.1109/ISMAR-Adjunct.2016.0032}
}

% author="Mayer, Anjela and Chardonnet, Jean-R{\'e}my and H{\"a}fner, Polina and Ovtcharova, Jivka",
@Inbook{Mayer_CollaborativeWorkImmersive2023,
author="Mayer, Anjela and others",
title="Collaborative Work Enabled by Immersive Environments",
bookTitle="New Digital Work: Digital Sovereignty at the Workplace",
year="2023",
publisher="Springer International Publishing",
address="Cham",
pages="87--117",
abstract="Digital transformation facilitates new methods for remote collaboration while shaping a new understanding of working together. In this chapter, we consider global collaboration in the context of digital transformation, discuss the role of Collaborative Virtual Environments (CVEs) within the transformation process, present an overview of the state of CVEs and go into more detail on significant challenges in CVEs by providing recent approaches from research.",
isbn="978-3-031-26490-0",
doi="10.1007/978-3-031-26490-0_6",
url="https://doi.org/10.1007/978-3-031-26490-0_6"
}

@Article{Garcia_CrossDeviceARAnnotations2021,
AUTHOR = {García-Pereira, Inma and others},
TITLE = {Cross-Device Augmented Reality Annotations Method for Asynchronous Collaboration in Unprepared Environments},
JOURNAL = {Information},
VOLUME = {12},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {519},
URL = {https://www.mdpi.com/2078-2489/12/12/519},
ISSN = {2078-2489},
ABSTRACT = {Augmented Reality (AR) annotations are a powerful way of communication when collaborators cannot be present at the same time in a given environment. However, this situation presents several challenges, for example: how to record the AR annotations for later consumption, how to align virtual and real world in unprepared environments or how to offer the annotations to users with different AR devices. In this paper we present a cross-device AR annotation method that allows users to create and display annotations asynchronously in environments without the need for prior preparation (AR markers, point cloud capture, etc.). This is achieved through an easy user-assisted calibration process and a data model that allows any type of annotation to be stored on any device. The experimental study carried out with 40 participants has verified our two hypotheses: we are able to visualize AR annotations in indoor environments without prior preparation regardless of the device used and the overall usability of the system is satisfactory.},
DOI = {10.3390/info12120519}
}

@inproceedings{Pausch_Aladdin1996,
author = {Pausch, Randy and others},
title = {Disney's Aladdin: first steps toward storytelling in virtual reality},
year = {1996},
isbn = {0897917464},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/237170.237257},
doi = {10.1145/237170.237257},
booktitle = {Proceedings of the 23rd Annual Conference on Computer Graphics and Interactive Techniques},
pages = {193–203},
numpages = {11},
series = {SIGGRAPH '96}
}

@INPROCEEDINGS{Zhang_InteractivityStory2019,
  author={Zhang, Lei and others},
  booktitle={2019 11th International Conference on Virtual Worlds and Games for Serious Applications (VS-Games)}, 
  title={Exploring Effects of Interactivity on Learning with Interactive Storytelling in Immersive Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={1-8},
  doi={10.1109/VS-Games.2019.8864531}
}

@inproceedings{Galyean_GuidedNavigation1995,
author = {Galyean, Tinsley A.},
title = {Guided Navigation of Virtual Environments},
year = {1995},
isbn = {0897917367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/199404.199421},
doi = {10.1145/199404.199421},
abstract = {This paper presents a new methodology for navigating virtual environments called “The River Analogy.” This analogy provides a new way of thinking about the user's relationship to the virtual environment; guiding the user's continuous and direct input within both space and time allowing a more narrative presentation. The paper then presents the details of how this analogy was applied to a VR experience that is now part of the permanent collection at the Chicago Museum of Science and Industry.},
booktitle = {Proceedings of the 1995 Symposium on Interactive 3D Graphics},
pages = {103–ff.},
location = {Monterey, California, USA},
series = {I3D '95}
}

@article{Best_MuseumTours2012,
author = {Katie Best},
title = {Making museum tours better: understanding what a guided tour really is and what a tour guide really does},
journal = {Museum Management and Curatorship},
volume = {27},
number = {1},
pages = {35--52},
year = {2012},
publisher = {Routledge},
doi = {10.1080/09647775.2012.644695},
URL = {https://doi.org/10.1080/09647775.2012.644695},
eprint = {https://doi.org/10.1080/09647775.2012.644695}
}

@Article{Duc_MemoryEffectVideo2019,
AUTHOR = {Nguyen Duc, Tho and others},
TITLE = {Modeling of Cumulative QoE in On-Demand Video Services: Role of Memory Effect and Degree of Interest},
JOURNAL = {Future Internet},
VOLUME = {11},
YEAR = {2019},
NUMBER = {8},
ARTICLE-NUMBER = {171},
URL = {https://www.mdpi.com/1999-5903/11/8/171},
ISSN = {1999-5903},
ABSTRACT = {The growing demand on video streaming services increasingly motivates the development of a reliable and accurate models for the assessment of Quality of Experience (QoE). In this duty, human-related factors which have significant influence on QoE play a crucial role. However, the complexity caused by multiple effects of those factors on human perception has introduced challenges on contemporary studies. In this paper, we inspect the impact of the human-related factors, namely perceptual factors, memory effect, and the degree of interest. Based on our investigation, a novel QoE model is proposed that effectively incorporates those factors to reflect the user’s cumulative perception. Evaluation results indicate that our proposed model performed excellently in predicting cumulative QoE at any moment within a streaming session.},
DOI = {10.3390/fi11080171}
}

@INPROCEEDINGS{Zaman_MRMAC2023,
  author={Zaman, Faisal and others},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={MRMAC: Mixed Reality Multi-user Asymmetric Collaboration}, 
  year={2023},
  volume={},
  number={},
  pages={591-600},
  keywords={Telepresence;Three-dimensional displays;Collaboration;Mixed reality;Teleportation;Video conferencing;User experience;Mixed Reality;Telecollaboration;Telepresence},
  doi={10.1109/ISMAR59233.2023.00074}
}

@Article{Tsiropoulou_ExpMuseumTour2017,
author={Tsiropoulou, Eirini Eleni and others},
title={Quality of Experience-based museum touring: a human in the loop approach},
journal={Social Network Analysis and Mining},
year={2017},
month={Jul},
day={28},
volume={7},
number={1},
pages={33},
abstract={This paper introduces a human in the loop approach toward proposing a physical, personal and interest-aware museum touring in order to maximize visitor's perceived Quality of Experience (QoE). The most influential parameters on visitors QoE are identified and more importantly quantified via performing an empirical study based on a questionnaire answered by experts in the field of arts and museums. Individual QoE functions with respect to each of the identified parameters are formulated for different museum visitors' styles, toward capturing visitors' perceived utility in a formal manner and providing them a customized and personalized experience. A social recommendation and personalization approach is designed toward creating visitors' profiles, exploiting common characteristics and interests among them and assisting in recommending a set of exhibits to be visited, through a ranking system according to visitor's interests. A Museum Visitor QoE Routing problem is formulated as an optimization problem considering the various QoE-related characteristics. The latter is solved via a graph-based approach determining both an optimal and a heuristic but less complex solution. Detailed numerical results are provided toward illustrating the applicability of the proposed framework under different scenarios and topologies.},
issn={1869-5469},
doi={10.1007/s13278-017-0453-2},
url={https://doi.org/10.1007/s13278-017-0453-2}
}

@INPROCEEDINGS{Marques_RemoteAsyncCollab2021,
  author={Marques, Bernardo and others},
  booktitle={2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={Remote Asynchronous Collaboration in Maintenance scenarios using Augmented Reality and Annotations}, 
  year={2021},
  volume={},
  number={},
  pages={567-568},
  keywords={Industries;Three-dimensional displays;Annotations;Conferences;Collaboration;Maintenance engineering;User interfaces;Human-centered computing;Human computer interaction (HCI);Collaborative interaction;Mixed / augmented reality},
  doi={10.1109/VRW52623.2021.00166}
}

@article{Jonides_MindMemory2008,
  title={The mind and brain of short-term memory},
  author={Jonides, John and others},
  journal={Annu. Rev. Psychol.},
  volume={59},
  pages={193--224},
  year={2008},
  publisher={Annual Reviews}
}

@article{Pearce_TouristGuideInteraction1984,
  title={Tourist-guide interaction},
  author={Pearce, Philip L},
  journal={Annals of Tourism Research},
  volume={11},
  number={1},
  pages={129--146},
  year={1984},
  publisher={Elsevier}
}

@inproceedings{Hong_VirtualVoyage1997,
author = {Hong, Lichan and others},
title = {Virtual voyage: interactive navigation in the human colon},
year = {1997},
isbn = {0897918967},
publisher = {ACM Press/Addison-Wesley Publishing Co.},
address = {USA},
url = {https://doi.org/10.1145/258734.258750},
doi = {10.1145/258734.258750},
booktitle = {Proceedings of the 24th Annual Conference on Computer Graphics and Interactive Techniques},
pages = {27–34},
numpages = {8},
keywords = {camera control, endoscopy, interactive rendering, physically-based navigation, potential field, virtual colonoscopy, virtual environment, visibility},
series = {SIGGRAPH '97}
}

@INPROCEEDINGS{Letellier_VizInteractionGuidedTours2019,
  author={Letellier, Julien and others},
  booktitle={2019 10th IEEE International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications (IDAACS)}, 
  title={Visualization and Interaction Techniques in Virtual Reality for Guided Tours}, 
  year={2019},
  volume={2},
  number={},
  pages={1041-1045},
  keywords={Three-dimensional displays;Buildings;Solid modeling;Virtual reality;Visualization;User interfaces;Cultural differences;virtual reality;human computer interaction;user interfaces;multimedia guides},
  doi={10.1109/IDAACS.2019.8924392}
}

@INPROCEEDINGS{Wang_VRReplay2019,
  author={Wang, Cheng Yao and others},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={VR-Replay: Capturing and Replaying Avatars in VR for Asynchronous 3D Collaborative Design}, 
  year={2019},
  volume={},
  number={},
  pages={1215-1216},
  keywords={Avatars;Tools;Three-dimensional displays;Task analysis;Teamwork;Prototypes;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism;Virtual reality},
  doi={10.1109/VR.2019.8797789}
}

@INPROCEEDINGS{Ghamandi_CollaborationTaxonomy2023,
  author={Ghamandi, Ryan K. and others},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={What And How Together: A Taxonomy On 30 Years Of Collaborative Human-Centered XR Tasks}, 
  year={2023},
  volume={},
  number={},
  pages={322-335},
  keywords={Productivity;Social computing;Systematics;Design methodology;Bibliographies;Taxonomy;Collaboration;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Collaborative and social computing design and evaluation methods;HCI design and evaluation methods},
  doi={10.1109/ISMAR59233.2023.00047}
}

@inproceedings{Marques_AsyncNotifications2022,
author = {Marques, Bernardo and others},
title = {Which notification is better? Comparing Visual, Audio and Tactile Cues for Asynchronous Mixed Reality (MR) Remote Collaboration: A User Study},
year = {2022},
isbn = {9781450398206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568444.3570587},
doi = {10.1145/3568444.3570587},
abstract = {Although Mixed Reality (MR) has been explored to support scenarios of remote collaboration, the majority of studies reported in literature have focused on synchronous use-cases, where all team-members can interact in real-time. Hence, an opportunity exist to explore asynchronous situations, in which collaborative actions take place at different times. Despite the physical distance and different time zones, remote experts can generate MR-instructions to assist with emerging problems, suggesting to on-site collaborators where to act, and what to do. This work explored three distinct notification methods for such scenarios: C1 - visual cue; C2 - audio cue; C3 - tactile cue, aiming to comprehend which one captures the attention of on-site team-members easily. A user study with 12 participants was conducted to evaluate these conditions during an asynchronous scenario of remote maintenance. We report participants insights, suggesting tactile cues represent the predominant condition given the scenario context.},
booktitle = {Proceedings of the 21st International Conference on Mobile and Ubiquitous Multimedia},
pages = {276–278},
numpages = {3},
keywords = {Audio and Tactile Cues, Mixed Reality, Notification Cues, Remote Collaboration, User Study, Visual},
location = {<conf-loc>, <city>Lisbon</city>, <country>Portugal</country>, </conf-loc>},
series = {MUM '22}
}

@InProceedings{Pidel_CollabSystematic2020,
author="Pidel, Catlin and others",
editor="De Paolis, Lucio Tommaso
and Bourdot, Patrick",
title="Collaboration in Virtual and Augmented Reality: A Systematic Overview",
booktitle="Augmented Reality, Virtual Reality, and Computer Graphics",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="141--156",
abstract="This paper offers a systematic overview of collaboration in virtual and augmented reality, including an assessment of advantages and challenges unique to collaborating in these mediums. In an attempt to highlight the current landscape of augmented and virtual reality collaboration (AR and VR, respectively), our selected research is biased towards more recent papers (within the last 5 years), but older work has also been included when particularly relevant. Our findings identify a number of potentially under-explored collaboration types, such as asynchronous collaboration and collaboration that combines AR and VR. We finally provide our key takeaways, including overall trends and opportunities for further research.",
isbn="978-3-030-58465-8"
}

@article{Schafer_SyncSurveyXR2022,
author = {Sch\"{a}fer, Alexander and others},
title = {A Survey on Synchronous Augmented, Virtual, andMixed Reality Remote Collaboration Systems},
year = {2022},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3533376},
doi = {10.1145/3533376},
abstract = {Remote collaboration systems have become increasingly important in today’s society, especially during times when physical distancing is advised. Industry, research, and individuals face the challenging task of collaborating and networking over long distances. While video and teleconferencing are already widespread, collaboration systems in augmented, virtual, and mixed reality are still a niche technology. We provide an overview of recent developments of synchronous remote collaboration systems and create a taxonomy by dividing them into three main components that form such systems: Environment, Avatars, and Interaction. A thorough overview of existing systems is given, categorising their main contributions to help researchers working in different fields by providing concise information about specific topics such as avatars, virtual environment, visualisation styles, and interaction. The focus of this work is clearly on synchronised collaboration from a distance. A total of 87 unique systems for remote collaboration are discussed, including more than 100 publications and 25 commercial systems.},
journal = {ACM Comput. Surv.},
month = {dec},
articleno = {116},
numpages = {27},
keywords = {literature review, distant cooperation, remote assistance, collaboration, mixed reality, augmented reality, Virtual reality}
}

%author = {Barrett Ens and Joel Lanir and Anthony Tang and Scott Bateman and Gun Lee and Thammathip Piumsomboon and Mark Billinghurst},
@article{Ens_RevisitingCollabMR2019,
title = {Revisiting collaboration through mixed reality: The evolution of groupware},
journal = {International Journal of Human-Computer Studies},
volume = {131},
pages = {81-98},
year = {2019},
note = {50 years of the International Journal of Human-Computer Studies. Reflections on the past, present and future of human-centred technologies},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2019.05.011},
url = {https://www.sciencedirect.com/science/article/pii/S1071581919300606},
author = {Barrett Ens and others},
keywords = {Collaborative mixed reality, Mixed reality, Augmented reality, Computer supported cooperative work, Collaborative technology},
abstract = {Collaborative Mixed Reality (MR) systems are at a critical point in time as they are soon to become more commonplace. However, MR technology has only recently matured to the point where researchers can focus deeply on the nuances of supporting collaboration, rather than needing to focus on creating the enabling technology. In parallel, but largely independently, the field of Computer Supported Cooperative Work (CSCW) has focused on the fundamental concerns that underlie human communication and collaboration over the past 30-plus years. Since MR research is now on the brink of moving into the real world, we reflect on three decades of collaborative MR research and try to reconcile it with existing theory from CSCW, to help position MR researchers to pursue fruitful directions for their work. To do this, we review the history of collaborative MR systems, investigating how the common taxonomies and frameworks in CSCW and MR research can be applied to existing work on collaborative MR systems, exploring where they have fallen behind, and look for new ways to describe current trends. Through identifying emergent trends, we suggest future directions for MR, and also find where CSCW researchers can explore new theory that more fully represents the future of working, playing and being with others.}
}

%% END ANNOTATED BIBLIOGRAPHY SOURCES %%

@INPROCEEDINGS{johansen:1989,
  author={Johansen, R.},
  booktitle={1989 IEEE Global Telecommunications Conference and Exhibition 'Communications Technology for the 1990s and Beyond'}, 
  title={Groupwise and collaborative systems-a big picture view}, 
  year={1989},
  volume={},
  number={},
  pages={1217-1220},
  doi={10.1109/GLOCOM.1989.64148}
}

@article{Churchill_CollabVEs1998,
  title={Collaborative virtual environments: an introductory review of issues and systems},
  author={Churchill, Elizabeth F and others},
  journal={virtual reality},
  volume={3},
  pages={3--15},
  year={1998},
  publisher={Springer}
}

@misc{Tijssen_ResearchCollab2012,
  doi = {10.48550/ARXIV.1203.4194},
  url = {https://arxiv.org/abs/1203.4194},
  author = {Tijssen, Robert J. W. and others},
  keywords = {Digital Libraries (cs.DL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Research collaboration and the expanding science grid: Measuring globalization processes worldwide},
  publisher = {arXiv},
  year = {2012},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@incollection{Steed_collaboration2015,
  title={Collaboration in immersive and non-immersive virtual environments},
  author={Steed, Anthony and others},
  booktitle={Immersed in Media},
  pages={263--282},
  year={2015},
  publisher={Springer}
}

@InCollection{Shapiro_EmbodiedCog2024,
	author       =	{Shapiro, Lawrence and others},
	title        =	{{Embodied Cognition}},
	booktitle    =	{The {Stanford} Encyclopedia of Philosophy},
	editor       =	{Edward N. Zalta and Uri Nodelman},
	howpublished =	{\url{https://plato.stanford.edu/archives/sum2024/entries/embodied-cognition/}},
	year         =	{2024},
	edition      =	{{S}ummer 2024},
	publisher    =	{Metaphysics Research Lab, Stanford University}
}

@article{Obrien_UES2018,
title = {A practical approach to measuring user engagement with the refined user engagement scale (UES) and new UES short form},
journal = {International Journal of Human-Computer Studies},
volume = {112},
pages = {28-39},
year = {2018},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2018.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S1071581918300041},
author = {Heather L. O’Brien and others},
keywords = {User engagement, Questionnaires, Measurement, Reliability, Validity},
abstract = {User engagement (UE) and its measurement have been of increasing interest in human-computer interaction (HCI). The User Engagement Scale (UES) is one tool developed to measure UE, and has been used in a variety of digital domains. The original UES consisted of 31-items and purported to measure six dimensions of engagement: aesthetic appeal, focused attention, novelty, perceived usability, felt involvement, and endurability. A recent synthesis of the literature questioned the original six-factors. Further, the ways in which the UES has been implemented in studies suggests there may be a need for a briefer version of the questionnaire and more effective documentation to guide its use and analysis. This research investigated and verified a four-factor structure of the UES and proposed a Short Form (SF). We employed contemporary statistical tools that were unavailable during the UES’ development to re-analyze the original data, consisting of 427 and 779 valid responses across two studies, and examined new data (N=344) gathered as part of a three-year digital library project. In this paper we detail our analyses, present a revised long and short form (SF) version of the UES, and offer guidance for researchers interested in adopting the UES and UES-SF in their own studies.}
}

@article{Kennedy_SSQ1993,
author = {Robert S. Kennedy and others},
title = {Simulator Sickness Questionnaire: An Enhanced Method for Quantifying Simulator Sickness},
journal = {The International Journal of Aviation Psychology},
volume = {3},
number = {3},
pages = {203--220},
year = {1993},
publisher = {Taylor \& Francis},
doi = {10.1207/s15327108ijap0303_3},
URL = { https://doi.org/10.1207/s15327108ijap0303_3 },
eprint = { https://doi.org/10.1207/s15327108ijap0303_3 }
}

@article{Hart_NASATLX2006,
author = {Sandra G. Hart},
title ={Nasa-Task Load Index (NASA-TLX); 20 Years Later},
journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
volume = {50},
number = {9},
pages = {904-908},
year = {2006},
doi = {10.1177/154193120605000909},
URL = {https://doi.org/10.1177/154193120605000909},
eprint = {https://doi.org/10.1177/154193120605000909}
}

@ARTICLE{Giovannelli_Gestures2023,
  author={Giovannelli, Alexander and others},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={Gestures vs. Emojis: Comparing Non-Verbal Reaction Visualizations for Immersive Collaboration}, 
  year={2023},
  volume={29},
  number={11},
  pages={4772-4781},
  keywords={Avatars;Visualization;Emojis;Collaboration;Task analysis;Color;Behavioral sciences;Human-computer interaction (HCI);virtual humans and avatars;telepresence;collaborative interfaces},
  doi={10.1109/TVCG.2023.3320254}
}

@INPROCEEDINGS{Klacanksy_InspectionAM2022,
  author={Klacansky, Pavol and others},
  booktitle={2022 IEEE 15th Pacific Visualization Symposium (PacificVis)}, 
  title={Virtual Inspection of Additively Manufactured Parts}, 
  year={2022},
  volume={},
  number={},
  pages={81-90},
  keywords={Solid modeling;Three-dimensional displays;Design automation;Computed tomography;Computational modeling;Volume measurement;Virtual reality;Human-centered computing—Visualization—Visualization systems and tools;Human-centered computing—Visualization—Visualization application domains—Scientific visualization;Human-centered computing—Visualization—Empirical studies in visualization},
  doi={10.1109/PacificVis53943.2022.00017}
}

@article{Sharpe_ChiSquare2015,
  title={Your chi-square test is statistically significant: now what?.},
  author={Sharpe, Donald},
  journal={Practical assessment, research \& evaluation},
  volume={20},
  number={8},
  pages={n8},
  year={2015},
  publisher={ERIC}
}

@INPROCEEDINGS{Lisle_ISTSense2021,
  author={Lisle, Lee and others},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Sensemaking Strategies with Immersive Space to Think}, 
  year={2021},
  volume={},
  number={},
  pages={529-537},
  keywords={Visualization;Three-dimensional displays;Layout;Virtual reality;Organizations;User interfaces;Feature extraction;Human-centered computing- Visualization- Visualization techniques-;Human-centered computing-Human Computer Interaction (HCI)-Interaction Paradigms-Virtual Reality},
  doi={10.1109/VR50410.2021.00077}
}

@inproceedings{Zhang_VRGit2023,
author = {Zhang, Lei and others},
title = {VRGit: A Version Control System for Collaborative Content Creation in Virtual Reality},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581136},
doi = {10.1145/3544548.3581136},
abstract = {Immersive authoring tools allow users to intuitively create and manipulate 3D scenes while immersed in Virtual Reality (VR). Collaboratively designing these scenes is a creative process that involves numerous edits, explorations of design alternatives, and frequent communication with collaborators. Version Control Systems (VCSs) help users achieve this by keeping track of the version history and creating a shared hub for communication. However, most VCSs are unsuitable for managing the version history of VR content because their underlying line differencing mechanism is designed for text and lacks the semantic information of 3D content; and the widely adopted commit model is designed for asynchronous collaboration rather than real-time awareness and communication in VR. We introduce VRGit, a new collaborative VCS that visualizes version history as a directed graph composed of 3D miniatures, and enables users to easily navigate versions, create branches, as well as preview and reuse versions directly in VR. Beyond individual uses, VRGit also facilitates synchronous collaboration in VR by providing awareness of users’ activities and version history through portals and shared history visualizations. In a lab study with 14 participants (seven groups), we demonstrate that VRGit enables users to easily manage version history both individually and collaboratively in VR.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {36},
numpages = {14},
keywords = {Collaboration, Version Control System, Virtual Reality},
location = {Hamburg, Germany},
series = {CHI '23}
}

@INPROCEEDINGS{Tahmid_ColtCollab2023,
  author={Tahmid, Ibrahim A. and others},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={CoLT: Enhancing Collaborative Literature Review Tasks with Synchronous and Asynchronous Awareness Across the Reality-Virtuality Continuum}, 
  year={2023},
  volume={},
  number={},
  pages={831-836},
  keywords={Productivity;Technological innovation;Three-dimensional displays;Telepresence;Portable computers;Bibliographies;Collaboration;Human-centered computing;Virtual Reality;Augmented Reality;Reality-Virtuality Continuum;Synchronous Collaboration;Asynchronous Collaboration;Remote Collaboration;Colocated Collaboration},
  doi={10.1109/ISMAR-Adjunct60411.2023.00183}
}

@INPROCEEDINGS{Thomas_HybridFw2023,
  author={Thomas, Jerald and others},
  booktitle={2023 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={A Communication-Focused Framework for Understanding Immersive Collaboration Experiences}, 
  year={2023},
  volume={},
  number={},
  pages={301-304},
  keywords={Social computing;Solid modeling;Three-dimensional displays;Conferences;Computational modeling;Collaboration;Virtual reality;Human-centered computing-Human computer interaction (HCI)-HCI theory;concepts and models;Human-centered computing-Collaborative and social computing theory-Theory;concepts and paradigms;Human-centered computing-Collaborative and social computing theory-Systems and tools},
  doi={10.1109/VRW58643.2023.00070}
}

@book{Verlinden_VirtualAnnotation1993,
  title={Virtual annotation: Verbal communication in virtual reality},
  author={Verlinden, Jouke C and others},
  year={1993},
  publisher={Delft University of Technology, Faculty of Technical Mathematics and Informatics}
}

@INPROCEEDINGS{Harmon_AnnotationSys1996,
  author={Harmon, R. and others},
  booktitle={Proceedings of the IEEE 1996 Virtual Reality Annual International Symposium}, 
  title={The virtual annotation system}, 
  year={1996},
  volume={},
  number={},
  pages={239-245},
  keywords={Data visualization;Organizing;Information analysis;Space technology;Virtual environment;Data analysis;Design engineering;Buildings;Process design;Navigation},
  doi={10.1109/VRAIS.1996.490533}
}

@article{Wither_OutdoorAR2009,
title = {Annotation in outdoor augmented reality},
journal = {Computers \& Graphics},
volume = {33},
number = {6},
pages = {679-689},
year = {2009},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2009.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0097849309000867},
author = {Jason Wither and others},
keywords = {Augmented reality, Annotation, Taxonomy, Online content creation},
abstract = {Annotation, the process of adding extra virtual information to an object, is one of the most common uses for augmented reality. Although annotation is widely used in augmented reality, there is no general agreed-upon definition of what precisely constitutes an annotation in this context. In this paper, we propose a taxonomy of annotation, describing what constitutes an annotation and outlining different dimensions along which annotation can vary. Using this taxonomy we also highlight what styles of annotation are used in different types of applications and areas where further work needs to be done to improve annotation. Through our taxonomy we found two primary categories into which annotations in current applications fall. Some annotations present information that is directly related to the object they are annotating, while others are only indirectly related to the object that is being annotated. We also found that there are very few applications that enable the user to either edit or create new annotations online. Instead, most applications rely on content that is created in various offline processes. There are, however, many advantages to online annotation. We summarize and formalize our recent work in this field by presenting the steps needed to build an online annotation system, looking most closely at techniques for placing annotations from a distance.}
}

@inproceedings{Bowman_IRVE2003,
author = {Bowman, Doug A. and others},
title = {Information-rich virtual environments: theory, tools, and research agenda},
year = {2003},
isbn = {1581135696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1008653.1008669},
doi = {10.1145/1008653.1008669},
abstract = {Virtual environments (VEs) allow users to experience and interact with a rich sensory environment, but most virtual worlds contain only sensory information similar to that which we experience in the physical world. Information-rich virtual environments (IRVEs) combine the power of VEs and information visualization, augmenting VEs with additional abstract information such as text, numbers, or graphs. IRVEs can be useful in many contexts, such as education, medicine, or construction. In our work, we are developing a theoretical foundation for the study of IRVEs and tools for their development and evaluation. We present a working definition of IRVEs, a discussion of information display and interaction in IRVEs. We also describe a software framework for IRVE development and a testbed enabling evaluation of text display techniques for IRVEs. Finally, we present a research agenda for this area.},
booktitle = {Proceedings of the ACM Symposium on Virtual Reality Software and Technology},
pages = {81–90},
numpages = {10},
keywords = {information visualization, information-rich virtual environments},
location = {Osaka, Japan},
series = {VRST '03}
}

@inproceedings{Hansen_UbiAnnotation2006,
author = {Hansen, Frank Allan},
title = {Ubiquitous annotation systems: technologies and challenges},
year = {2006},
isbn = {1595934170},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1149941.1149967},
doi = {10.1145/1149941.1149967},
abstract = {Ubiquitous annotation systems allow users to annotate physical places, objects, and persons with digital information. Especially in the field of location based information systems much work has been done to implement adaptive and context-aware systems, but few efforts have focused on the general requirements for linking information to objects in both physical and digital space. This paper surveys annotation techniques from open hypermedia systems, Web based annotation systems, and mobile and augmented reality systems to illustrate different approaches to four central challenges ubiquitous annotation systems have to deal with: anchoring, structuring, presentation, and authoring. Through a number of examples each challenge is discussed and HyCon, a context-aware hypermedia framework developed at the University of Aarhus, Denmark, is used to illustrate an integrated approach to ubiquitous annotations. Finally, a taxonomy of annotation systems is presented. The taxonomy can be used both to categorize system based on the way they present annotations and to choose the right technology for interfacing with annotations when implementing new systems.},
booktitle = {Proceedings of the Seventeenth Conference on Hypertext and Hypermedia},
pages = {121–132},
numpages = {12},
keywords = {annotation, context-aware computing, mobile computing, ubiquitous hypermedia},
location = {Odense, Denmark},
series = {HYPERTEXT '06}
}

@inproceedings{Guerreiro_BeyondPostIt2014,
  title={Beyond Post-It: Structured Multimedia Annotations for Collaborative VEs.},
  author={Guerreiro, Tiago Joao and others},
  booktitle={ICAT-EGVE},
  pages={55--62},
  year={2014}
}

@inproceedings{Pick_PosAnnotation2010,
author = {Pick, S. and others},
title = {Automated positioning of annotations in immersive virtual environments},
year = {2010},
isbn = {9783905674309},
publisher = {Eurographics Association},
address = {Goslar, DEU},
abstract = {The visualization of scientific data sets can be enhanced by providing additional information that aids the data analysis process. This information is represented by so called annotations, which contain descriptive meta data about the underlying visualization. The meta data results from diverse sources like previous analysis sessions (e.g. ideas, comments, or sketches) or automated meta data extraction (e.g. descriptive statistics). Visually integrating annotations into an existing data visualization while maintaining easy data access and a clear overview over all visible annotations is a non-trivial task. Several automated annotation positioning algorithms have been proposed that specifically target single-screen display systems and hence cannot be applied to immersive multiscreen display systems commonly used in Virtual Reality. In this paper, we propose a new automated annotation positioning algorithm specifically designed for such display systems. Our algorithm is based on an analogy to the well-known shadow volume technique, which is used to determine occlusion relations. A force-based approach is used to update annotation positions. The whole algorithm is independent of the specific annotation contents and considers well-established quality criteria to build an annotation layout. We evaluate our algorithm by means of performance measurements and a structured expert walkthrough.},
booktitle = {Proceedings of the 16th Eurographics Conference on Virtual Environments \& Second Joint Virtual Reality},
pages = {1–8},
numpages = {8},
location = {Stuttgart, Germany},
series = {EGVE - JVRC'10}
}

@inproceedings{Bell_ViewMgmt2001,
author = {Bell, Blaine and others},
title = {View management for virtual and augmented reality},
year = {2001},
isbn = {158113438X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502348.502363},
doi = {10.1145/502348.502363},
abstract = {We describe a view-management component for interactive 3D user interfaces. By view management, we mean maintaining visual constraints on the projections of objects on the view plane, such as locating related objects near each other, or preventing objects from occluding each other. Our view-management component accomplishes this by modifying selected object properties, including position, size, and transparency, which are tagged to indicate their constraints. For example, some objects may have geometric properties that are determined entirely by a physical simulation and which cannot be modified, while other objects may be annotations whose position and size are flexible.We introduce algorithms that use upright rectangular extents to represent on the view plane a dynamic and efficient approximation of the occupied space containing the projections of visible portions of 3D objects, as well as the unoccupied space in which objects can be placed to avoid occlusion. Layout decisions from previous frames are taken into account to reduce visual discontinuities. We present augmented reality and virtual reality examples to which we have applied our approach, including a dynamically labeled and annotated environment.},
booktitle = {Proceedings of the 14th Annual ACM Symposium on User Interface Software and Technology},
pages = {101–110},
numpages = {10},
keywords = {annotation, augmented reality, environment management, labeling, view management, virtual environments, wearable computing},
location = {Orlando, Florida},
series = {UIST '01}
}

@INPROCEEDINGS{Clergeaud_AnnotationSys2017,
  author={Clergeaud, Damien and others},
  booktitle={2017 3DTV Conference: The True Vision - Capture, Transmission and Display of 3D Video (3DTV-CON)}, 
  title={Design of an annotation system for taking notes in virtual reality}, 
  year={2017},
  volume={},
  number={},
  pages={1-4},
  keywords={Task analysis;XML;Tools;Collaboration;Industries;Virtual environments;Virtual Reality;Annotation System;Interaction Technique;Design;Tangible User Interface},
  doi={10.1109/3DTV.2017.8280398}
}

@book{Argyle_Social1981,
  title={Social situations},
  author={Argyle, Michael and others},
  year={1981},
  publisher={Cambridge University Press}
}

@INPROCEEDINGS{Tahmid_ISTClusters2022,
  author={Tahmid, Ibrahim A. and others},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Evaluating the Benefits of Explicit and Semi-Automated Clusters for Immersive Sensemaking}, 
  year={2022},
  volume={},
  number={},
  pages={479-488},
  keywords={Semantics;Manuals;Task analysis;Artificial intelligence;Augmented reality;Virtual Reality;Human AI Collaboration;Semantic Interaction;Immersive Analytics;Clustering},
  doi={10.1109/ISMAR55827.2022.00064}
}

@ARTICLE{Ribarsky_VRSpeech1994,
  author={Ribarsky, W. and others},
  journal={IEEE Computer Graphics and Applications}, 
  title={Visualization and analysis using virtual reality}, 
  year={1994},
  volume={14},
  number={1},
  pages={10-12},
  keywords={Virtual reality;Data visualization;Usability;Application software;Computer graphics;Optical sensors;Data analysis;Humans;Computer interfaces;Information technology},
  doi={10.1109/38.250911}
}

@INPROCEEDINGS{Irlitti_InteractAsync2013,
  author={Irlitti, Andrew and others},
  booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Tangible interaction techniques to support asynchronous collaboration}, 
  year={2013},
  volume={},
  number={},
  pages={1-6},
  keywords={Collaboration;Prototypes;Mobile communication;Context;Australia;Augmented reality;Wearable computers;Augmented Reality;Asynchronous Collaboration;Spatial Annotations;Tangible Interaction},
  doi={10.1109/ISMAR.2013.6671840}
}

@InCollection{Thagard_CogSci2023,
	author       =	{Thagard, Paul},
	title        =	{{Cognitive Science}},
	booktitle    =	{The {Stanford} Encyclopedia of Philosophy},
	editor       =	{Edward N. Zalta and Uri Nodelman},
	howpublished =	{\url{https://plato.stanford.edu/archives/win2023/entries/cognitive-science/}},
	year         =	{2023},
	edition      =	{{W}inter 2023},
	publisher    =	{Metaphysics Research Lab, Stanford University}
}

@article{Lovreglio_Training2021,
  title={Comparing the effectiveness of fire extinguisher virtual reality and video training},
  author={Lovreglio, Ruggiero and others},
  journal={Virtual Reality},
  volume={25},
  number={1},
  pages={133--145},
  year={2021},
  publisher={Springer}
}

@inproceedings{Lin_Basketball2021,
author = {Lin, Tica and others},
title = {Towards an Understanding of Situated AR Visualization for Basketball Free-Throw Training},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445649},
doi = {10.1145/3411764.3445649},
abstract = {We present an observational study to compare co-located and situated real-time visualizations in basketball free-throw training. Our goal is to understand the advantages and concerns of applying immersive visualization to real-world skill-based sports training and to provide insights for designing AR sports training systems. We design both a situated 3D visualization on a head-mounted display and a 2D visualization on a co-located display to provide immediate visual feedback on a player’s shot performance. Using a within-subject study design with experienced basketball shooters, we characterize user goals, report on qualitative training experiences, and compare the quantitative training results. Our results show that real-time visual feedback helps athletes refine subsequent shots. Shooters in our study achieve greater angle consistency with our visual feedback. Furthermore, AR visualization promotes an increased focus on body form in athletes. Finally, we present suggestions for the design of future sports AR studies.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {461},
numpages = {13},
keywords = {Augmented Reality, Data Visualization, Immersive Analytics, Situated Analytics, SportsXR},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{Chheang_AMInspect2024,
author = {Chheang, Vuthea and others},
title = {A Virtual Environment for Collaborative Inspection in Additive Manufacturing},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3650730},
doi = {10.1145/3613905.3650730},
abstract = {Additive manufacturing (AM) techniques have been used to enhance the design and fabrication of complex components for various applications in the medical, aerospace, energy, and consumer products industries. A defining feature for many AM parts is the complex internal geometry enabled by the printing process. However, inspecting these internal structures requires volumetric imaging, i.e., X-ray CT, leading to the well-known challenge of visualizing complex 3D geometries using 2D desktop interfaces. Furthermore, existing tools are limited to single-user systems making it difficult to jointly discuss or share findings with a larger team, i.e., the designers, manufacturing experts, and evaluation team. In this work, we present a collaborative virtual reality (VR) for the exploration and inspection of AM parts. Geographically separated experts can virtually inspect and jointly discuss data. It also supports VR and non-VR users, who can be spectators in the VR environment. Various features for data exploration and inspection are developed and enhanced via real-time synchronization. We followed usability and interface verification guidelines using Nielsen’s heuristics approach. Furthermore, we conducted exploratory and semi-structured interviews with domain experts to collect qualitative feedback. Results reveal potential benefits, applicability, and current limitations. The proposed collaborative VR environment provides a new basis and opens new research directions for virtual inspection and team collaboration in AM settings.},
booktitle = {Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {26},
numpages = {7},
keywords = {Additive Manufacturing, Collaborative VR, Digital Twins, Virtual Inspection, Virtual Reality},
location = {
},
series = {CHI EA '24}
}

% ART ANOVA Citation
@inproceedings{Wobbrock_ArtAnova2011,
author = {Wobbrock, Jacob O. and others},
title = {The Aligned Rank Transform for Nonparametric Factorial Analyses Using Only Anova Procedures},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1978963},
doi = {10.1145/1978942.1978963},
abstract = {Nonparametric data from multi-factor experiments arise often in human-computer interaction (HCI). Examples may include error counts, Likert responses, and preference tallies. But because multiple factors are involved, common nonparametric tests (e.g., Friedman) are inadequate, as they are unable to examine interaction effects. While some statistical techniques exist to handle such data, these techniques are not widely available and are complex. To address these concerns, we present the Aligned Rank Transform (ART) for nonparametric factorial data analysis in HCI. The ART relies on a preprocessing step that "aligns" data before applying averaged ranks, after which point common ANOVA procedures can be used, making the ART accessible to anyone familiar with the F-test. Unlike most articles on the ART, which only address two factors, we generalize the ART to N factors. We also provide ARTool and ARTweb, desktop and Web-based programs for aligning and ranking data. Our re-examination of some published HCI results exhibits advantages of the ART.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {143–146},
numpages = {4},
keywords = {f-test, nonparametric data, analysis of variance, anova, factorial analysis, statistics},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

% ART-C Citation
@inproceedings{Elkin_ArtC2021,
author = {Elkin, Lisa A. and others},
title = {An Aligned Rank Transform Procedure for Multifactor Contrast Tests},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474784},
doi = {10.1145/3472749.3474784},
abstract = {Data from multifactor HCI experiments often violates the assumptions of parametric tests (i.e., nonconforming data). The Aligned Rank Transform (ART) has become a popular nonparametric analysis in HCI that can find main and interaction effects in nonconforming data, but leads to incorrect results when used to conduct post hoc contrast tests. We created a new algorithm called ART-C for conducting contrast tests within the ART paradigm and validated it on 72,000 synthetic data sets. Our results indicate that ART-C does not inflate Type I error rates, unlike contrasts based on ART, and that ART-C has more statistical power than a t-test, Mann-Whitney U test, Wilcoxon signed-rank test, and ART. We also extended an open-source tool called ARTool with our ART-C algorithm for both Windows and R. Our validation had some limitations (e.g., only six distribution types, no mixed factorial designs, no random slopes), and data drawn from Cauchy distributions should not be analyzed with ART-C.},
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {754–768},
numpages = {15},
keywords = {experiments, nonparametric statistics, aligned rank transform., Statistical methods, data analysis, quantitative methods},
location = {Virtual Event, USA},
series = {UIST '21}
}

@article{Stefik_Wysiwis1987,
author = {Stefik, M. and others},
title = {WYSIWIS revised: early experiences with multiuser interfaces},
year = {1987},
issue_date = {April 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/27636.28056},
doi = {10.1145/27636.28056},
abstract = {WYSIWIS (What You See Is What I See) is a foundational abstraction for multiuser interfaces that expresses many of the characteristics of a chalkboard in face-to-face meetings. In its strictest interpretation, it means that everyone can also see the same written information and also see where anyone else is pointing. In our attempts to build software support for collaboration in meetings, we have discovered that WYSIWIS is crucial, yet too inflexible when strictly enforced. This paper is about the design issues and choices that arose in our first generation of meeting tools based on WYSIWIS. Several examples of multiuser interfaces that start from this abstraction are presented. These tools illustrate that there are inherent conflicts between the needs of a group and the needs of individuals, since user interfaces compete for the same display space and meeting time. To help minimize the effect of these conflicts, constraints were relaxed along four key dimensions of WYSIWIS: display space, time of display, subgroup population, and congruence of view. Meeting tools must be designed to support the changing needs of information sharing during process transitions, as subgroups are formed and dissolved, as individuals shift their focus of activity, and as the group shifts from multiple parallel activities to a single focused activity and back again.},
journal = {ACM Trans. Inf. Syst.},
month = {apr},
pages = {147–167},
numpages = {21}
}

@article{Zhao_effectiveness2020,
  title={The effectiveness of virtual reality-based technology on anatomy teaching: a meta-analysis of randomized controlled studies},
  author={Zhao, Jingjie and others},
  journal={BMC medical education},
  volume={20},
  pages={1--10},
  year={2020},
  publisher={Springer}
}

@article{Roussou_LearningByDoing2004,
author = {Roussou, Maria},
title = {Learning by doing and learning through play: an exploration of interactivity in virtual environments for children},
year = {2004},
issue_date = {January 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
url = {https://doi.org/10.1145/973801.973818},
doi = {10.1145/973801.973818},
abstract = {The development of interactive, participatory, multisensory
environments that combine the physical with the virtual comes as a
natural continuation to the computer game industrys constant race
for more exciting user experiences. Specialized theme parks and
various other leisure and entertainment centers worldwide are
embracing the interactive promise that games have made users
expect. This is not a trend limited to the entertainment domain;
non-formal learning environments for children are also following
this path, backed up by a theoretical notion of play as a core
activity in a childs development. In this article we explore a
central thread in learning, play, as well as an essential
characteristic of virtual reality environments: interactivity. A
critical review of examples of immersive virtual reality worlds
created for children, with particular attention given to the role
and nature of interactivity, is attempted. Interactivity is
examined in relation to learning, play, narrative, and to
characteristics inherent in virtual reality, such as immersion,
presence, and the creation of illusion.},
journal = {Comput. Entertain.},
month = jan,
pages = {10},
numpages = {1}
}

@inproceedings{Milgram_RealityVirtuality1995,
author = {Paul Milgram and others},
title = {{Augmented reality: a class of displays on the reality-virtuality continuum}},
volume = {2351},
booktitle = {Telemanipulator and Telepresence Technologies},
editor = {Hari Das},
organization = {International Society for Optics and Photonics},
publisher = {SPIE},
pages = {282 -- 292},
year = {1995},
doi = {10.1117/12.197321},
URL = {https://doi.org/10.1117/12.197321}
}

@INPROCEEDINGS{Lee_EnhancingInstructionVideos2020,
  author={Lee, Gun A. and others},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Enhancing First-Person View Task Instruction Videos with Augmented Reality Cues}, 
  year={2020},
  volume={},
  number={},
  pages={498-508},
  keywords={Training;Performance evaluation;Visualization;Prototypes;Task analysis;Augmented reality;Videos;Augmented Reality;instruction video;task guide},
  doi={10.1109/ISMAR50242.2020.00078}}

@inproceedings{Wang_GesturAR2021,
author = {Wang, Tianyi and others},
title = {GesturAR: An Authoring System for Creating Freehand Interactive Augmented Reality Applications},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474769},
doi = {10.1145/3472749.3474769},
abstract = {Freehand gesture is an essential input modality for modern Augmented Reality (AR) user experiences. However, developing AR applications with customized hand interactions remains a challenge for end-users. Therefore, we propose GesturAR, an end-to-end authoring tool that supports users to create in-situ freehand AR applications through embodied demonstration and visual programming. During authoring, users can intuitively demonstrate the customized gesture inputs while referring to the spatial and temporal context. Based on the taxonomy of gestures in AR, we proposed a hand interaction model which maps the gesture inputs to the reactions of the AR contents. Thus, users can author comprehensive freehand applications using trigger-action visual programming and instantly experience the results in AR. Further, we demonstrate multiple application scenarios enabled by GesturAR, such as interactive virtual objects, robots, and avatars, room-level interactive AR spaces, embodied AR presentations, etc. Finally, we evaluate the performance and usability of GesturAR through a user study.},
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {552–567},
numpages = {16},
keywords = {immersive authoring, embodied demonstration, Freehand interactions, Augmented Reality},
location = {Virtual Event, USA},
series = {UIST '21}
}

@inproceedings{Wang_CAPturAR2020,
author = {Wang, Tianyi and others},
title = {CAPturAR: An Augmented Reality Tool for Authoring Human-Involved Context-Aware Applications},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415815},
doi = {10.1145/3379337.3415815},
abstract = {Recognition of human behavior plays an important role in context-aware applications. However, it is still a challenge for end-users to build personalized applications that accurately recognize their own activities. Therefore, we present CAPturAR, an in-situ programming tool that supports users to rapidly author context-aware applications by referring to their previous activities. We customize an AR head-mounted device with multiple camera systems that allow for non-intrusive capturing of user's daily activities. During authoring, we reconstruct the captured data in AR with an animated avatar and use virtual icons to represent the surrounding environment. With our visual programming interface, users create human-centered rules for the applications and experience them instantly in AR. We further demonstrate four use cases enabled by CAPturAR. Also, we verify the effectiveness of the AR-HMD and the authoring workflow with a system evaluation using our prototype. Moreover, we conduct a remote user study in an AR simulator to evaluate the usability.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {328–341},
numpages = {14},
keywords = {augmented reality, context-aware application, embodied authoring, end-user programming tool, in-situ authoring, ubiquitous computing},
location = {Virtual Event, USA},
series = {UIST '20}
}

@article{Cho_RealityReplay2023,
author = {Cho, Hyunsung and others},
title = {RealityReplay: Detecting and Replaying Temporal Changes In Situ Using Mixed Reality},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
url = {https://doi.org/10.1145/3610888},
doi = {10.1145/3610888},
abstract = {Humans easily miss events in their surroundings due to limited short-term memory and field of view. This happens, for example, while watching an instructor's machine repair demonstration or conversing during a sports game. We present RealityReplay, a novel Mixed Reality (MR) approach that tracks and visualizes these significant events using in-situ MR visualizations without modifying the physical space. It requires only a head-mounted MR display and a 360-degree camera. We contribute a method for egocentric tracking of important motion events in users' surroundings based on a combination of semantic segmentation and saliency prediction, and generating in-situ MR visual summaries of temporal changes. These summary visualizations are overlaid onto the physical world to reveal which objects moved, in what order, and their trajectory, enabling users to observe previously hidden events. The visualizations are informed by a formative study comparing different styles on their effects on users' perception of temporal changes. Our evaluation shows that RealityReplay significantly enhances sensemaking of temporal motion events compared to memory-based recall. We demonstrate application scenarios in guidance, education, and observation, and discuss implications for extending human spatiotemporal capabilities through technological augmentation.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {90},
numpages = {25},
keywords = {Augmented Reality, Computational Interaction, Mixed Reality}
}
