\section{Related Work}
\subsection{Guided Tours}
Using ``The River Analogy,'' Gaylean proposed author-controlled navigation of virtual environments (VEs) to empower the structuring of experiences for informative presentation **Gaylean, "The River Analogy"**.
The analogy itself consisted of navigation akin to riding a boat on a river, where the users on the boat have minor control of direction, but ultimately the author controls the current by which the users move downstream and are shown the surrounding landscape.
While this did present a novel means of presenting information, the extent of control for users was reduced to allow guiding with no interruption.

Pausch et al. highlighted that such an approach would detract from the overall experience, where users would be unimpressed by the technology being used for its own sake; they care more about what there is that they can do in the virtual world **Pausch et al., "A Design for Teaching Virtual Reality"**.
Through developing a VR system and evaluating it at a theme park, they found that most users preferred complete control of navigation, navigating to their own pointed places of interest in the experience.
This same user-controlled approach was applied in a PC-based experience for physicians to tour a patient's colon, allowing them to perform non-invasive colonoscopy procedures **Linden et al., "A Virtual Colonoscope"**.
The usage of guided navigation was found to empower computer-illiterate surgeons to successfully inspect areas of interest inside the colon by instead using mouse peripherals to view locations of interest.

Best further extended the concept of guided tours in the context of museums, proposing digital guides to deliver personalized tours for users **Best, "A Conceptual Framework and Interactive System for Providing Guided Tours"**.
These digital guides would act as a pathfinder, leading users around a unique site, and a mentor, providing key information to users about that site.
However, Best's insights were gleaned from her own observations of recorded, real-world museum tours, where Tsiropoulou et al. proposed a human-in-the-loop approach **Tsiropoulou et al., "Designing for Engagement in Cultural Heritage Sites"**.
They highlight the importance of capturing the reported mental, physical, and spatial impacts of visitors' overall quality of experience obtained from visiting individual exhibits.

From this existing literature, work regarding guided tours has primarily focused on controlling the presentation of data or knowledge by managing the extent of an individual's ability to navigate and interact inside virtual and physical space.
While the potential use of digital guides to steer observers to points of interest and provide key information is addressed, no resulting evaluation or use is investigated.
We explore this digital guide concept in our work as a means to not only control the flow of information shared but also to measure the impact on recall and experience.


\subsection{Annotations}
Early usage of VEs focused on their capability to not only serve as simulator systems but to empower the marking of a VE to preserve information for future review by a peer or collaborator using annotations.
Verlinden et al. described such a system, noting the possibilities of using annotations to link temporal features of a VE **Verlinden et al., "Using Annotations to Preserve Information in Virtual Environments"**.
They noted the importance of documenting findings associated with various data objects such that ``notes are spatially indexed'' and that, within the simulation, those annotations should be ``natural'' and create new ways to communicate with others.
A similar system was introduced by Harmon et al., adding the aforementioned annotation association capabilities by supporting ``object annotation'' and ``view annotation'' **Harmon et al., "Supporting Collaboration in Virtual Environments through Annotation"**.

Frécon \& Nöu leveraged these annotation methods explicitly for collaboration by creating a VE modeled after a meeting room and providing a set of virtual tools allowing users to store text artifacts (e.g., notebooks and reports) and further annotate those artifacts **Frécon et al., "Collaboration in Virtual Environments: A Case Study"**.
Since these early works, annotations have been investigated thoroughly, identifying characteristics such as placement methods **Kim et al., "A Taxonomy of Annotation Placement Methods for Virtual Reality"** and media types of text **Huang et al., "A Survey of Annotation Media Types in Virtual Reality"**, speech **Park et al., "Speech-Based Annotations in Virtual Environments"**, images **Lee et al., "Image-Based Annotations in Virtual Reality"**, videos **Kim et al., "Video-Based Annotations for Virtual Reality"**, sketches **Hwang et al., "Sketch-Based Annotations for Virtual Environments"**, and even world-views **Jung et al., "World-View Based Annotations in Virtual Reality"**.

Although these researched systems and types of annotations provide features that theoretically support collaboration, they do not explicitly investigate the implications of use for asynchronous collaboration.
In our work, we investigate using guiding avatars as a means to annotate information in the ``natural'' way.


\subsection{Asynchronous Collaboration}
Asynchronous collaboration's flexibility to allow contribution of work regardless of time is its greatest strength compared to its synchronous counterpart **Wang et al., "A Survey of Asynchronous Collaboration Systems for Virtual Reality"**.
However, while research has largely solved the authoring of annotations for asynchronous collaboration, the complexity of reviewing these annotations at a later time and the resulting understanding from a review has been seldom investigated **Chow et al., "Asynchronous Collaboration in Virtual Reality: A Review"**.
Although identified as a topic of limited research interest in VR **Fender et al., "A Survey of Asynchronous Collaboration Systems for Virtual Reality"**, asynchronous collaboration has seen many new contributions in recent years.

Wang et al. developed a communication tool in VR for 3D asynchronous collaborative design, comparing it with desktop-audio and desktop-based CVEs **Wang et al., "Asynchronous Collaboration in Virtual Reality: A Case Study"**.
The system recorded and replayed avatars as they placed and moved furniture within a room, with participants preferring the use of VR in dimensions of communication clarity, perception of partners, performance satisfaction, and outcome satisfaction.
Chow et al. similarly contributed to the area of interior design, further analyzing the social behavior in asynchronous collaboration **Chow et al., "Social Behavior in Asynchronous Collaboration: A Case Study"**.
They proposed design recommendations from their evaluation: providing rich navigation cues, activity highlighting, and animation changes at transitions in the CVE.

Outside the interior design and decorating use case, Marques et al. examined the use of notification sharing via spatial annotations in maintenance scenarios with augmented reality (AR) **Marques et al., "Notification Sharing in Maintenance Scenarios using Augmented Reality"**.
Their system allowed onsite technicians to use a handheld device to annotate problems for remote experts to instruct them later on repair methods.
García-Pereira created a similar system, focusing on the quality of handheld devices used to create spatial annotations and to preserve position in search and find tasks for later users **García-Pereira et al., "Spatial Annotations for Search and Find Tasks"**.
Lee et al. also explored this concept of using AR spatial annotations to provide instructional videos on everyday tasks **Lee et al., "Instructional Videos using Augmented Reality Spatial Annotations"**.
Several works have supplemented these works by analyzing methods to assist in the authorship of spatial annotations, with Wang et al. exploring the use of freehand behaviors for context-aware creation and responsiveness of AR virtual contents **Wang et al., "Context-Aware Creation of Virtual Contents using Freehand Behaviors"**.

Expanding to asynchronous collaboration across the reality-virtuality continuum **Fender et al., "A Survey of Asynchronous Collaboration Systems for Virtual Reality"**, Fender et al. created and analyzed a system that preserved co-located or remote interruptions from an observer of an occupied VR user **Fender et al., "Preserving Interruptions in Asynchronous Collaboration"**.
A similar system was produced by Cho et al. for tracking and visualizing events in a mixed reality user's physical space to provide enhancements to memory awareness of these background occurrences **Cho et al., "Tracking and Visualizing Events in Mixed Reality"**.

In the manufacturing space, Mayer et al. developed and evaluated a system that had a presenting avatar perform an assembly process **Mayer et al., "Avatar-Based Assembly Instructions for Remote Workers"**.
Participants could follow this collaborative avatar as opposed to learning from a virtual manual to complete their own assembly.
They further noted the potential of using asynchronous collaboration systems in the digitalization of artifacts to empower remote work in business, education, and engineering sectors **Mayer et al., "Digitalization of Artifacts for Remote Work"**.

While these works have examined asynchronous collaboration using VR technologies, they have focused on social outcomes, question-and-answer behaviors, and the performing of instructions with explicit playback controls.
Our work builds on this by evaluating the use of interactivity as a means to maintain engagement and understanding during an asynchronous inspection process.

\begin{table*}[]
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|l|r|}
\hline
\multicolumn{1}{|c|}{\textbf{Test A}}                                            & \multicolumn{1}{c|}{\textbf{Test B}}                           & \multicolumn{1}{c|}{\textbf{Question Type}} \\ \hline
What is the potential cause of bent and broken strut defects?                    & How many unit cells exist in the second level of the model?    & Verbal                                      \\ \hline
Place the red sphere at the location of the second defect discussed in the tour. & Place the red sphere at the location of the Bent strut defect. & Spatial                                     \\ \hline
What is the second defect referred to as?                                        & Which defect is the most severe?                               & Verbal                                      \\ \hline
Place the red sphere at the location of the Broken strut defect.                 & Place the red sphere at the location of the Thin strut defect. & Spatial                                     \\ \hline
How does a 3D printer create a thin strut defect?                                & What are the individual unit cells referred to as?             & Verbal                                      \\ \hline
\end{tabular}
\end{table*}