@inproceedings{Bell_ViewMgmt2001,
author = {Bell, Blaine and others},
title = {View management for virtual and augmented reality},
year = {2001},
isbn = {158113438X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502348.502363},
doi = {10.1145/502348.502363},
abstract = {We describe a view-management component for interactive 3D user interfaces. By view management, we mean maintaining visual constraints on the projections of objects on the view plane, such as locating related objects near each other, or preventing objects from occluding each other. Our view-management component accomplishes this by modifying selected object properties, including position, size, and transparency, which are tagged to indicate their constraints. For example, some objects may have geometric properties that are determined entirely by a physical simulation and which cannot be modified, while other objects may be annotations whose position and size are flexible.We introduce algorithms that use upright rectangular extents to represent on the view plane a dynamic and efficient approximation of the occupied space containing the projections of visible portions of 3D objects, as well as the unoccupied space in which objects can be placed to avoid occlusion. Layout decisions from previous frames are taken into account to reduce visual discontinuities. We present augmented reality and virtual reality examples to which we have applied our approach, including a dynamically labeled and annotated environment.},
booktitle = {Proceedings of the 14th Annual ACM Symposium on User Interface Software and Technology},
pages = {101–110},
numpages = {10},
keywords = {annotation, augmented reality, environment management, labeling, view management, virtual environments, wearable computing},
location = {Orlando, Florida},
series = {UIST '01}
}

@article{Best_MuseumTours2012,
author = {Katie Best},
title = {Making museum tours better: understanding what a guided tour really is and what a tour guide really does},
journal = {Museum Management and Curatorship},
volume = {27},
number = {1},
pages = {35--52},
year = {2012},
publisher = {Routledge},
doi = {10.1080/09647775.2012.644695},
URL = {https://doi.org/10.1080/09647775.2012.644695},
eprint = {https://doi.org/10.1080/09647775.2012.644695}
}

@inproceedings{Bowman_IRVE2003,
author = {Bowman, Doug A. and others},
title = {Information-rich virtual environments: theory, tools, and research agenda},
year = {2003},
isbn = {1581135696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1008653.1008669},
doi = {10.1145/1008653.1008669},
abstract = {Virtual environments (VEs) allow users to experience and interact with a rich sensory environment, but most virtual worlds contain only sensory information similar to that which we experience in the physical world. Information-rich virtual environments (IRVEs) combine the power of VEs and information visualization, augmenting VEs with additional abstract information such as text, numbers, or graphs. IRVEs can be useful in many contexts, such as education, medicine, or construction. In our work, we are developing a theoretical foundation for the study of IRVEs and tools for their development and evaluation. We present a working definition of IRVEs, a discussion of information display and interaction in IRVEs. We also describe a software framework for IRVE development and a testbed enabling evaluation of text display techniques for IRVEs. Finally, we present a research agenda for this area.},
booktitle = {Proceedings of the ACM Symposium on Virtual Reality Software and Technology},
pages = {81–90},
numpages = {10},
keywords = {information visualization, information-rich virtual environments},
location = {Osaka, Japan},
series = {VRST '03}
}

@article{Cho_RealityReplay2023,
author = {Cho, Hyunsung and others},
title = {RealityReplay: Detecting and Replaying Temporal Changes In Situ Using Mixed Reality},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
url = {https://doi.org/10.1145/3610888},
doi = {10.1145/3610888},
abstract = {Humans easily miss events in their surroundings due to limited short-term memory and field of view. This happens, for example, while watching an instructor's machine repair demonstration or conversing during a sports game. We present RealityReplay, a novel Mixed Reality (MR) approach that tracks and visualizes these significant events using in-situ MR visualizations without modifying the physical space. It requires only a head-mounted MR display and a 360-degree camera. We contribute a method for egocentric tracking of important motion events in users' surroundings based on a combination of semantic segmentation and saliency prediction, and generating in-situ MR visual summaries of temporal changes. These summary visualizations are overlaid onto the physical world to reveal which objects moved, in what order, and their trajectory, enabling users to observe previously hidden events. The visualizations are informed by a formative study comparing different styles on their effects on users' perception of temporal changes. Our evaluation shows that RealityReplay significantly enhances sensemaking of temporal motion events compared to memory-based recall. We demonstrate application scenarios in guidance, education, and observation, and discuss implications for extending human spatiotemporal capabilities through technological augmentation.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {90},
numpages = {25},
keywords = {Augmented Reality, Computational Interaction, Mixed Reality}
}

@article{Chow_ChallengesAsyncCollabVR2019,
author = {Chow, Kevin and others},
title = {Challenges and Design Considerations for Multimodal Asynchronous Collaboration in VR},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359142},
doi = {10.1145/3359142},
abstract = {Studies on collaborative virtual environments (CVEs) have suggested capture and later replay of multimodal interactions (e.g., speech, body language, and scene manipulations), which we refer to as multimodal recordings, as an effective medium for time-distributed collaborators to discuss and review 3D content in an immersive, expressive, and asynchronous way. However, there exist gaps of empirical knowledge in understanding how this multimodal asynchronous VR collaboration (MAVRC) context impacts social behaviors in mediated-communication, workspace awareness in cooperative work, and user requirements for authoring and consuming multimedia recording. This study aims to address these gaps by conceptualizing MAVRC as a type of CSCW and by understanding the challenges and design considerations of MAVRC systems. To this end, we conducted an exploratory need-finding study where participants (N = 15) used an experimental MAVRC system to complete a representative spatial task in an asynchronously collaborative setting, involving both consumption and production of multimodal recordings. Qualitative analysis of interview and observation data from the study revealed unique, core design challenges of MAVRC in: (1) coordinating proxemic behaviors between asynchronous collaborators, (2) providing traceability and change awareness across different versions of 3D scenes, (3) accommodating viewpoint control to maintain workspace awareness, and (4) supporting navigation and editing of multimodal recordings. We discuss design implications, ideate on potential design solutions, and conclude the paper with a set of design recommendations for MAVRC systems.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {40},
numpages = {24},
keywords = {virtual reality, speech, spatial task, proxemics, presence, pointing, multimodal recording, immersion, gesture, body language, asynchronous collaboration, 3D}
}

%author={Irlitti, Andrew and Smith, Ross T. and Von Itzstein, Stewart and Billinghurst, Mark and Thomas, Bruce H.}

@INPROCEEDINGS{Clergeaud_AnnotationSys2017,
  author={Clergeaud, Damien and others},
  booktitle={2017 3DTV Conference: The True Vision - Capture, Transmission and Display of 3D Video (3DTV-CON)}, 
  title={Design of an annotation system for taking notes in virtual reality}, 
  year={2017},
  volume={},
  number={},
  pages={1-4},
  keywords={Task analysis;XML;Tools;Collaboration;Industries;Virtual environments;Virtual Reality;Annotation System;Interaction Technique;Design;Tangible User Interface},
  doi={10.1109/3DTV.2017.8280398}
}

@inproceedings{Fender_CausalityAsync2022,
author = {Fender, Andreas Rene and others},
title = {Causality-preserving Asynchronous Reality},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3501836},
doi = {10.1145/3491102.3501836},
abstract = {Mixed Reality is gaining interest as a platform for collaboration and focused work to a point where it may supersede current office settings in future workplaces. At the same time, we expect that interaction with physical objects and face-to-face communication will remain crucial for future work environments, which is a particular challenge in fully immersive Virtual Reality. In this work, we reconcile those requirements through a user’s individual Asynchronous Reality, which enables seamless physical interaction across time. When a user is unavailable, e.g., focused on a task or in a call, our approach captures co-located or remote physical events in real-time, constructs a causality graph of co-dependent events, and lets immersed users revisit them at a suitable time in a causally accurate way. Enabled by our system AsyncReality, we present a workplace scenario that includes walk-in interruptions during a person’s focused work, physical deliveries, and transient spoken messages. We then generalize our approach to a use-case agnostic concept and system architecture. We conclude by discussing the implications of Asynchronous Reality for future offices.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {634},
numpages = {15},
keywords = {Mixed Reality, Interruption in workplaces, Immersive workspaces, Collaboration, Camera networks, Asynchronous communication},
location = {<conf-loc>, <city>New Orleans</city>, <state>LA</state>, <country>USA</country>, </conf-loc>},
series = {CHI '22}
}

@inproceedings{Frecon_BuildingDistVEs1998,
author = {Fr\'{e}con, Emmanuel and others},
title = {Building distributed virtual environments to support collaborative work},
year = {1998},
isbn = {1581130198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/293701.293715},
doi = {10.1145/293701.293715},
booktitle = {Proceedings of the ACM Symposium on Virtual Reality Software and Technology},
pages = {105–113},
numpages = {9},
keywords = {3D collaborative tools, CSCW, CVE, DIVE, room metaphor},
location = {Taipei, Taiwan},
series = {VRST '98}
}

@inproceedings{Galyean_GuidedNavigation1995,
author = {Galyean, Tinsley A.},
title = {Guided Navigation of Virtual Environments},
year = {1995},
isbn = {0897917367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/199404.199421},
doi = {10.1145/199404.199421},
abstract = {This paper presents a new methodology for navigating virtual environments called “The River Analogy.” This analogy provides a new way of thinking about the user's relationship to the virtual environment; guiding the user's continuous and direct input within both space and time allowing a more narrative presentation. The paper then presents the details of how this analogy was applied to a VR experience that is now part of the permanent collection at the Chicago Museum of Science and Industry.},
booktitle = {Proceedings of the 1995 Symposium on Interactive 3D Graphics},
pages = {103–ff.},
location = {Monterey, California, USA},
series = {I3D '95}
}

@Article{Garcia_CrossDeviceARAnnotations2021,
AUTHOR = {García-Pereira, Inma and others},
TITLE = {Cross-Device Augmented Reality Annotations Method for Asynchronous Collaboration in Unprepared Environments},
JOURNAL = {Information},
VOLUME = {12},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {519},
URL = {https://www.mdpi.com/2078-2489/12/12/519},
ISSN = {2078-2489},
ABSTRACT = {Augmented Reality (AR) annotations are a powerful way of communication when collaborators cannot be present at the same time in a given environment. However, this situation presents several challenges, for example: how to record the AR annotations for later consumption, how to align virtual and real world in unprepared environments or how to offer the annotations to users with different AR devices. In this paper we present a cross-device AR annotation method that allows users to create and display annotations asynchronously in environments without the need for prior preparation (AR markers, point cloud capture, etc.). This is achieved through an easy user-assisted calibration process and a data model that allows any type of annotation to be stored on any device. The experimental study carried out with 40 participants has verified our two hypotheses: we are able to visualize AR annotations in indoor environments without prior preparation regardless of the device used and the overall usability of the system is satisfactory.},
DOI = {10.3390/info12120519}
}

@inproceedings{Guerreiro_BeyondPostIt2014,
  title={Beyond Post-It: Structured Multimedia Annotations for Collaborative VEs.},
  author={Guerreiro, Tiago Joao and others},
  booktitle={ICAT-EGVE},
  pages={55--62},
  year={2014}
}

@inproceedings{Hansen_UbiAnnotation2006,
author = {Hansen, Frank Allan},
title = {Ubiquitous annotation systems: technologies and challenges},
year = {2006},
isbn = {1595934170},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1149941.1149967},
doi = {10.1145/1149941.1149967},
abstract = {Ubiquitous annotation systems allow users to annotate physical places, objects, and persons with digital information. Especially in the field of location based information systems much work has been done to implement adaptive and context-aware systems, but few efforts have focused on the general requirements for linking information to objects in both physical and digital space. This paper surveys annotation techniques from open hypermedia systems, Web based annotation systems, and mobile and augmented reality systems to illustrate different approaches to four central challenges ubiquitous annotation systems have to deal with: anchoring, structuring, presentation, and authoring. Through a number of examples each challenge is discussed and HyCon, a context-aware hypermedia framework developed at the University of Aarhus, Denmark, is used to illustrate an integrated approach to ubiquitous annotations. Finally, a taxonomy of annotation systems is presented. The taxonomy can be used both to categorize system based on the way they present annotations and to choose the right technology for interfacing with annotations when implementing new systems.},
booktitle = {Proceedings of the Seventeenth Conference on Hypertext and Hypermedia},
pages = {121–132},
numpages = {12},
keywords = {annotation, context-aware computing, mobile computing, ubiquitous hypermedia},
location = {Odense, Denmark},
series = {HYPERTEXT '06}
}

@INPROCEEDINGS{Harmon_AnnotationSys1996,
  author={Harmon, R. and others},
  booktitle={Proceedings of the IEEE 1996 Virtual Reality Annual International Symposium}, 
  title={The virtual annotation system}, 
  year={1996},
  volume={},
  number={},
  pages={239-245},
  keywords={Data visualization;Organizing;Information analysis;Space technology;Virtual environment;Data analysis;Design engineering;Buildings;Process design;Navigation},
  doi={10.1109/VRAIS.1996.490533}
}

@inproceedings{Hong_VirtualVoyage1997,
author = {Hong, Lichan and others},
title = {Virtual voyage: interactive navigation in the human colon},
year = {1997},
isbn = {0897918967},
publisher = {ACM Press/Addison-Wesley Publishing Co.},
address = {USA},
url = {https://doi.org/10.1145/258734.258750},
doi = {10.1145/258734.258750},
booktitle = {Proceedings of the 24th Annual Conference on Computer Graphics and Interactive Techniques},
pages = {27–34},
numpages = {8},
keywords = {camera control, endoscopy, interactive rendering, physically-based navigation, potential field, virtual colonoscopy, virtual environment, visibility},
series = {SIGGRAPH '97}
}

@INPROCEEDINGS{Irlitti_ChallengesAsync2016,
  author={Irlitti, Andrew and others},
  booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, 
  title={Challenges for Asynchronous Collaboration in Augmented Reality}, 
  year={2016},
  volume={},
  number={},
  pages={31-35},
  keywords={Collaboration;Electronic mail;Augmented reality;Context;Stakeholders;Resists;Production;H.5.3 [Information interfaces and presentation (e.g.;HCI)]: Group and Organization Interfaces—Asynchronous interaction—Computer-supported cooperative work; I.3.6 [Computer Graphics]: Methodology and Techniques—Interaction Techniques},
  doi={10.1109/ISMAR-Adjunct.2016.0032}
}

% author="Mayer, Anjela and Chardonnet, Jean-R{\'e}my and H{\"a}

@INPROCEEDINGS{Irlitti_InteractAsync2013,
  author={Irlitti, Andrew and others},
  booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Tangible interaction techniques to support asynchronous collaboration}, 
  year={2013},
  volume={},
  number={},
  pages={1-6},
  keywords={Collaboration;Prototypes;Mobile communication;Context;Australia;Augmented reality;Wearable computers;Augmented Reality;Asynchronous Collaboration;Spatial Annotations;Tangible Interaction},
  doi={10.1109/ISMAR.2013.6671840}
}

@INPROCEEDINGS{Lee_EnhancingInstructionVideos2020,
  author={Lee, Gun A. and others},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Enhancing First-Person View Task Instruction Videos with Augmented Reality Cues}, 
  year={2020},
  volume={},
  number={},
  pages={498-508},
  keywords={Training;Performance evaluation;Visualization;Prototypes;Task analysis;Augmented reality;Videos;Augmented Reality;instruction video;task guide},
  doi={10.1109/ISMAR50242.2020.00078}}

@INPROCEEDINGS{Lisle_ISTSense2021,
  author={Lisle, Lee and others},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Sensemaking Strategies with Immersive Space to Think}, 
  year={2021},
  volume={},
  number={},
  pages={529-537},
  keywords={Visualization;Three-dimensional displays;Layout;Virtual reality;Organizations;User interfaces;Feature extraction;Human-centered computing- Visualization- Visualization techniques-;Human-centered computing-Human Computer Interaction (HCI)-Interaction Paradigms-Virtual Reality},
  doi={10.1109/VR50410.2021.00077}
}

@inproceedings{Marques_AsyncNotifications2022,
author = {Marques, Bernardo and others},
title = {Which notification is better? Comparing Visual, Audio and Tactile Cues for Asynchronous Mixed Reality (MR) Remote Collaboration: A User Study},
year = {2022},
isbn = {9781450398206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568444.3570587},
doi = {10.1145/3568444.3570587},
abstract = {Although Mixed Reality (MR) has been explored to support scenarios of remote collaboration, the majority of studies reported in literature have focused on synchronous use-cases, where all team-members can interact in real-time. Hence, an opportunity exist to explore asynchronous situations, in which collaborative actions take place at different times. Despite the physical distance and different time zones, remote experts can generate MR-instructions to assist with emerging problems, suggesting to on-site collaborators where to act, and what to do. This work explored three distinct notification methods for such scenarios: C1 - visual cue; C2 - audio cue; C3 - tactile cue, aiming to comprehend which one captures the attention of on-site team-members easily. A user study with 12 participants was conducted to evaluate these conditions during an asynchronous scenario of remote maintenance. We report participants insights, suggesting tactile cues represent the predominant condition given the scenario context.},
booktitle = {Proceedings of the 21st International Conference on Mobile and Ubiquitous Multimedia},
pages = {276–278},
numpages = {3},
keywords = {Audio and Tactile Cues, Mixed Reality, Notification Cues, Remote Collaboration, User Study, Visual},
location = {<conf-loc>, <city>Lisbon</city>, <country>Portugal</country>, </conf-loc>},
series = {MUM '22}
}

@INPROCEEDINGS{Marques_RemoteAsyncCollab2021,
  author={Marques, Bernardo and others},
  booktitle={2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={Remote Asynchronous Collaboration in Maintenance scenarios using Augmented Reality and Annotations}, 
  year={2021},
  volume={},
  number={},
  pages={567-568},
  keywords={Industries;Three-dimensional displays;Annotations;Conferences;Collaboration;Maintenance engineering;User interfaces;Human-centered computing;Human computer interaction (HCI);Collaborative interaction;Mixed / augmented reality},
  doi={10.1109/VRW52623.2021.00166}
}

@InProceedings{Mayer_AsyncManualWork2022,
author="Mayer, Anjela and others",
title="Asynchronous Manual Work in Mixed Reality Remote Collaboration",
booktitle="Extended Reality",
year="2022",
publisher="Springer Nature Switzerland",
address="Cham",
pages="17--33",
abstract="Research in Collaborative Virtual Environments (CVEs) is becoming more and more significant with increasing accessibility of Virtual Reality (VR) and Augmented Reality (AR) technology, additionally reinforced by the increasing demand for remote collaboration groupware. While the research is focusing on methods for synchronous remote collaboration, asynchronous remote collaboration remains a niche. Nevertheless, future CVEs should support both paradigms of collaborative work, since asynchronous collaboration has as well its benefits, for instance a more flexible time-coordination. In this paper we present a concept of recording and later playback of highly interactive collaborative tasks in Mixed Reality (MR). Furthermore, we apply the concept in an assembly training scenario from the manufacturing industry and test it during pilot user experiments. The pilot study compared two modalities, the first one with a manufacturing manual, and another using our concept and featuring a ghost avatar. First results revealed no significant differences between both modalities in terms of time completion, hand movements, cognitive workload and usability. Some differences were not expected, however, these results and the feedback brought by the participants provide insights to further develop our concept.",
isbn="978-3-031-15553-6"
}

@Inbook{Mayer_CollaborativeWorkImmersive2023,
author="Mayer, Anjela and others",
title="Collaborative Work Enabled by Immersive Environments",
bookTitle="New Digital Work: Digital Sovereignty at the Workplace",
year="2023",
publisher="Springer International Publishing",
address="Cham",
pages="87--117",
abstract="Digital transformation facilitates new methods for remote collaboration while shaping a new understanding of working together. In this chapter, we consider global collaboration in the context of digital transformation, discuss the role of Collaborative Virtual Environments (CVEs) within the transformation process, present an overview of the state of CVEs and go into more detail on significant challenges in CVEs by providing recent approaches from research.",
isbn="978-3-031-26490-0",
doi="10.1007/978-3-031-26490-0_6",
url="https://doi.org/10.1007/978-3-031-26490-0_6"
}

@inproceedings{Milgram_RealityVirtuality1995,
author = {Paul Milgram and others},
title = {{Augmented reality: a class of displays on the reality-virtuality continuum}},
volume = {2351},
booktitle = {Telemanipulator and Telepresence Technologies},
editor = {Hari Das},
organization = {International Society for Optics and Photonics},
publisher = {SPIE},
pages = {282 -- 292},
year = {1995},
doi = {10.1117/12.197321},
URL = {https://doi.org/10.1117/12.197321}
}

@inproceedings{Pausch_Aladdin1996,
author = {Pausch, Randy and others},
title = {Disney's Aladdin: first steps toward storytelling in virtual reality},
year = {1996},
isbn = {0897917464},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/237170.237257},
doi = {10.1145/237170.237257},
booktitle = {Proceedings of the 23rd Annual Conference on Computer Graphics and Interactive Techniques},
pages = {193–203},
numpages = {11},
series = {SIGGRAPH '96}
}

@inproceedings{Pick_PosAnnotation2010,
author = {Pick, S. and others},
title = {Automated positioning of annotations in immersive virtual environments},
year = {2010},
isbn = {9783905674309},
publisher = {Eurographics Association},
address = {Goslar, DEU},
abstract = {The visualization of scientific data sets can be enhanced by providing additional information that aids the data analysis process. This information is represented by so called annotations, which contain descriptive meta data about the underlying visualization. The meta data results from diverse sources like previous analysis sessions (e.g. ideas, comments, or sketches) or automated meta data extraction (e.g. descriptive statistics). Visually integrating annotations into an existing data visualization while maintaining easy data access and a clear overview over all visible annotations is a non-trivial task. Several automated annotation positioning algorithms have been proposed that specifically target single-screen display systems and hence cannot be applied to immersive multiscreen display systems commonly used in Virtual Reality. In this paper, we propose a new automated annotation positioning algorithm specifically designed for such display systems. Our algorithm is based on an analogy to the well-known shadow volume technique, which is used to determine occlusion relations. A force-based approach is used to update annotation positions. The whole algorithm is independent of the specific annotation contents and considers well-established quality criteria to build an annotation layout. We evaluate our algorithm by means of performance measurements and a structured expert walkthrough.},
booktitle = {Proceedings of the 16th Eurographics Conference on Virtual Environments \& Second Joint Virtual Reality},
pages = {1–8},
numpages = {8},
location = {Stuttgart, Germany},
series = {EGVE - JVRC'10}
}

@InProceedings{Pidel_CollabSystematic2020,
author="Pidel, Catlin and others",
editor="De Paolis, Lucio Tommaso
and Bourdot, Patrick",
title="Collaboration in Virtual and Augmented Reality: A Systematic Overview",
booktitle="Augmented Reality, Virtual Reality, and Computer Graphics",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="141--156",
abstract="This paper offers a systematic overview of collaboration in virtual and augmented reality, including an assessment of advantages and challenges unique to collaborating in these mediums. In an attempt to highlight the current landscape of augmented and virtual reality collaboration (AR and VR, respectively), our selected research is biased towards more recent papers (within the last 5 years), but older work has also been included when particularly relevant. Our findings identify a number of potentially under-explored collaboration types, such as asynchronous collaboration and collaboration that combines AR and VR. We finally provide our key takeaways, including overall trends and opportunities for further research.",
isbn="978-3-030-58465-8"
}

@ARTICLE{Ribarsky_VRSpeech1994,
  author={Ribarsky, W. and others},
  journal={IEEE Computer Graphics and Applications}, 
  title={Visualization and analysis using virtual reality}, 
  year={1994},
  volume={14},
  number={1},
  pages={10-12},
  keywords={Virtual reality;Data visualization;Usability;Application software;Computer graphics;Optical sensors;Data analysis;Humans;Computer interfaces;Information technology},
  doi={10.1109/38.250911}
}

@INPROCEEDINGS{Tahmid_ColtCollab2023,
  author={Tahmid, Ibrahim A. and others},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={CoLT: Enhancing Collaborative Literature Review Tasks with Synchronous and Asynchronous Awareness Across the Reality-Virtuality Continuum}, 
  year={2023},
  volume={},
  number={},
  pages={831-836},
  keywords={Productivity;Technological innovation;Three-dimensional displays;Telepresence;Portable computers;Bibliographies;Collaboration;Human-centered computing;Virtual Reality;Augmented Reality;Reality-Virtuality Continuum;Synchronous Collaboration;Asynchronous Collaboration;Remote Collaboration;Colocated Collaboration},
  doi={10.1109/ISMAR-Adjunct60411.2023.00183}
}

@INPROCEEDINGS{Tahmid_ISTClusters2022,
  author={Tahmid, Ibrahim A. and others},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Evaluating the Benefits of Explicit and Semi-Automated Clusters for Immersive Sensemaking}, 
  year={2022},
  volume={},
  number={},
  pages={479-488},
  keywords={Semantics;Manuals;Task analysis;Artificial intelligence;Augmented reality;Virtual Reality;Human AI Collaboration;Semantic Interaction;Immersive Analytics;Clustering},
  doi={10.1109/ISMAR55827.2022.00064}
}

@Article{Tsiropoulou_ExpMuseumTour2017,
author={Tsiropoulou, Eirini Eleni and others},
title={Quality of Experience-based museum touring: a human in the loop approach},
journal={Social Network Analysis and Mining},
year={2017},
month={Jul},
day={28},
volume={7},
number={1},
pages={33},
abstract={This paper introduces a human in the loop approach toward proposing a physical, personal and interest-aware museum touring in order to maximize visitor's perceived Quality of Experience (QoE). The most influential parameters on visitors QoE are identified and more importantly quantified via performing an empirical study based on a questionnaire answered by experts in the field of arts and museums. Individual QoE functions with respect to each of the identified parameters are formulated for different museum visitors' styles, toward capturing visitors' perceived utility in a formal manner and providing them a customized and personalized experience. A social recommendation and personalization approach is designed toward creating visitors' profiles, exploiting common characteristics and interests among them and assisting in recommending a set of exhibits to be visited, through a ranking system according to visitor's interests. A Museum Visitor QoE Routing problem is formulated as an optimization problem considering the various QoE-related characteristics. The latter is solved via a graph-based approach determining both an optimal and a heuristic but less complex solution. Detailed numerical results are provided toward illustrating the applicability of the proposed framework under different scenarios and topologies.},
issn={1869-5469},
doi={10.1007/s13278-017-0453-2},
url={https://doi.org/10.1007/s13278-017-0453-2}
}

@book{Verlinden_VirtualAnnotation1993,
  title={Virtual annotation: Verbal communication in virtual reality},
  author={Verlinden, Jouke C and others},
  year={1993},
  publisher={Delft University of Technology, Faculty of Technical Mathematics and Informatics}
}

@inproceedings{Wang_CAPturAR2020,
author = {Wang, Tianyi and others},
title = {CAPturAR: An Augmented Reality Tool for Authoring Human-Involved Context-Aware Applications},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415815},
doi = {10.1145/3379337.3415815},
abstract = {Recognition of human behavior plays an important role in context-aware applications. However, it is still a challenge for end-users to build personalized applications that accurately recognize their own activities. Therefore, we present CAPturAR, an in-situ programming tool that supports users to rapidly author context-aware applications by referring to their previous activities. We customize an AR head-mounted device with multiple camera systems that allow for non-intrusive capturing of user's daily activities. During authoring, we reconstruct the captured data in AR with an animated avatar and use virtual icons to represent the surrounding environment. With our visual programming interface, users create human-centered rules for the applications and experience them instantly in AR. We further demonstrate four use cases enabled by CAPturAR. Also, we verify the effectiveness of the AR-HMD and the authoring workflow with a system evaluation using our prototype. Moreover, we conduct a remote user study in an AR simulator to evaluate the usability.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {328–341},
numpages = {14},
keywords = {augmented reality, context-aware application, embodied authoring, end-user programming tool, in-situ authoring, ubiquitous computing},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{Wang_GesturAR2021,
author = {Wang, Tianyi and others},
title = {GesturAR: An Authoring System for Creating Freehand Interactive Augmented Reality Applications},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474769},
doi = {10.1145/3472749.3474769},
abstract = {Freehand gesture is an essential input modality for modern Augmented Reality (AR) user experiences. However, developing AR applications with customized hand interactions remains a challenge for end-users. Therefore, we propose GesturAR, an end-to-end authoring tool that supports users to create in-situ freehand AR applications through embodied demonstration and visual programming. During authoring, users can intuitively demonstrate the customized gesture inputs while referring to the spatial and temporal context. Based on the taxonomy of gestures in AR, we proposed a hand interaction model which maps the gesture inputs to the reactions of the AR contents. Thus, users can author comprehensive freehand applications using trigger-action visual programming and instantly experience the results in AR. Further, we demonstrate multiple application scenarios enabled by GesturAR, such as interactive virtual objects, robots, and avatars, room-level interactive AR spaces, embodied AR presentations, etc. Finally, we evaluate the performance and usability of GesturAR through a user study.},
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {552–567},
numpages = {16},
keywords = {immersive authoring, embodied demonstration, Freehand interactions, Augmented Reality},
location = {Virtual Event, USA},
series = {UIST '21}
}

@INPROCEEDINGS{Wang_VRReplay2019,
  author={Wang, Cheng Yao and others},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={VR-Replay: Capturing and Replaying Avatars in VR for Asynchronous 3D Collaborative Design}, 
  year={2019},
  volume={},
  number={},
  pages={1215-1216},
  keywords={Avatars;Tools;Three-dimensional displays;Task analysis;Teamwork;Prototypes;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism;Virtual reality},
  doi={10.1109/VR.2019.8797789}
}

@article{Wither_OutdoorAR2009,
title = {Annotation in outdoor augmented reality},
journal = {Computers \& Graphics},
volume = {33},
number = {6},
pages = {679-689},
year = {2009},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2009.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0097849309000867},
author = {Jason Wither and others},
keywords = {Augmented reality, Annotation, Taxonomy, Online content creation},
abstract = {Annotation, the process of adding extra virtual information to an object, is one of the most common uses for augmented reality. Although annotation is widely used in augmented reality, there is no general agreed-upon definition of what precisely constitutes an annotation in this context. In this paper, we propose a taxonomy of annotation, describing what constitutes an annotation and outlining different dimensions along which annotation can vary. Using this taxonomy we also highlight what styles of annotation are used in different types of applications and areas where further work needs to be done to improve annotation. Through our taxonomy we found two primary categories into which annotations in current applications fall. Some annotations present information that is directly related to the object they are annotating, while others are only indirectly related to the object that is being annotated. We also found that there are very few applications that enable the user to either edit or create new annotations online. Instead, most applications rely on content that is created in various offline processes. There are, however, many advantages to online annotation. We summarize and formalize our recent work in this field by presenting the steps needed to build an online annotation system, looking most closely at techniques for placing annotations from a distance.}
}

