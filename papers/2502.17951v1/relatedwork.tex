\section{Related Work}
\subsection{Generative Adversarial Networks for Polyp Synthesis}

Synthetic polyp generation plays a pivotal role in addressing the scarcity of annotated datasets, a persistent challenge in colonoscopy-related deep learning tasks. Early methods predominantly relied on generative adversarial networks (GANs) due to their ability to synthesize visually realistic images. For example, Shin et al.~\cite{shin2018abnormal} utilized conditional GANs to enhance the realism of synthetic polyps by incorporating edge maps and masks, providing additional structural details to the generated images. Sasmal et al.~\cite{sasmal2020improved} adopted DCGANs to expand polyp datasets, demonstrating improvements in downstream classification performance. Similarly, Qadir et al.~\cite{qadir2022simple} proposed mask-based conditional GANs to manipulate polyp appearances, while He et al.~\cite{he2021colonoscopic} developed adversarial techniques to produce false-negative samples, significantly enhancing the robustness of polyp detection models by challenging classifiers with hard-to-identify cases.

Despite these advances, GAN-based methods face inherent challenges, including convergence instability, limited image diversity, and artifact generation~\cite{dhariwal2021diffusion}. Further studies by Frid-Adar et al.~\cite{frid2018gan} and Yoon et al.~\cite{yoon2022colonoscopic} explored GANs in medical image synthesis, highlighting both their potential and limitations, particularly in colonoscopy.

\subsection{Diffusion Models for Controlled Polyp Generation}

Diffusion models have emerged as robust alternatives to GANs, providing greater stability in training and generating more diverse and realistic synthetic images. Machacek et al.~\cite{machavcek2023mask} introduced a latent diffusion model conditioned on segmentation masks, marking a significant advancement in synthetic polyp generation by focusing on accurate structural details. Du et al.~\cite{du2023arsdm} further developed this concept with ArSDM, incorporating adaptive mechanisms to enhance lesion-specific focus and employing external models to refine alignment accuracy between synthetic polyps and ground truth masks. These refinements led to improved performance in segmentation and detection tasks. Sharma et al.~\cite{Sharma_2024_CVPR} expanded the scope of diffusion-based methods with ControlPolypNet, a framework designed to generate more realistic images by controlling background details and spatial attributes, such as polyp size, shape, and location, resulting in substantial segmentation performance gains.

Despite these advancements, existing frameworks often overlook the wealth of clinical information, focusing on isolated attributes and limiting their ability to achieve comprehensive control over diverse and clinically significant polyp characteristics. This narrow focus restricts their potential to address real-world challenges such as inter-hospital variability and domain shifts.
Our approach builds on these advancements by integrating semantically rich polyp annotations into a unified diffusion-based framework. By enabling joint control across multiple granularity levels, we achieve the modulation of spatial and semantic features during polyp generation, addressing limitations in existing methods and improving model robustness and adaptability for diverse clinical scenarios.