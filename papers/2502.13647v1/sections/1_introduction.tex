\section{Introduction}
%\textbf{WHAT IS THE IMPORTANCE OF THIS DATASET? WHY TO FOCUS ON GOVSET?}
%Fixed, lower the tone on Gov focu


Instruction tuning enhances large language models (LLMs) by fine-tuning them on structured prompts, improving their ability to follow human instructions across various tasks such as question answering and summarization \cite{ouyang2022instructgpt}. While extensive instruction-tuning datasets exist for English, such as FLAN \cite{longpre2023flan}, P3 \cite{sanh2021multitask}, and Dolly \cite{conover2023dolly}, efforts in low-resource languages remain limited. This gap is particularly evident in domain-specific applications where multilingual LLMs often provide generic or inaccurate responses due to a lack of localized training data \cite{li2023bactrian}.


A key challenge in adapting LLMs to underrepresented languages is the scarcity of high-quality instruction data \cite{li2023bactrian}. Multilingual models may process low-resource languages at a technical level \cite{openai2024gpt4o}, but their practical effectiveness is often constrained by an incomplete understanding of region-specific socio-political structures and cultural contexts. For example, when asked about administrative procedures like obtaining a passport in a particular country, models tend to default to well-documented cases rather than providing precise, localized information. Similarly, cultural narratives—such as folklore, literature, and traditions—are often missing from instruction datasets \cite{conover2023dolly}, limiting the models’ ability to generate contextually appropriate responses. While prior work relied on translation \cite{sengupta2023jais} or template-based techniques \cite{cahyawijaya-etal-2024-cendol} to build instruction-tuning datasets, it does not fully reflect the actual local context, as direct translations often fail to capture the nuances of regional governance, customs, and linguistic variations.

Building instruction datasets from scratch is costly, making large-scale manual data collection impractical for many low-resource languages. To address this, we adopt an LLM-assisted dataset generation approach \cite{liu-etal-2022-wanli,cahyawijaya2023instructalign,zhang-etal-2024-llm-assisted}, followed by full human validation. Specifically, we use a single-prompt method where LLMs process high-quality unlabeled text from public government and cultural sources to extract both factual information and corresponding instructions. These domains are highly relevant for real-world applications, but remain underexplored for instruction-tuning, particularly in the context of government data.


\begin{figure*}[h!]
    \centering
    \includegraphics[width=1\textwidth]{images/main_flowchart.pdf} % Change width as needed
    \caption{End-to-end process of dataset construction. The English translation is for illustrative purposes.}
    \label{fig:pdf-image}
\end{figure*}

To demonstrate the effectiveness of this approach, we introduce an instruction-tuning dataset for Kazakh that integrates both institutional\footnote{Our study incorporates administrative, procedural, legal, structural, and other government-related types of information.} (\texttt{GovSet}) and cultural (\texttt{CultSet}) domains. We choose Kazakh as our case study because it remains underrepresented in NLP \citep{joshi-etal-2020-state}, despite having approximately 20 million speakers. Prior research on Kazakh NLP has primarily focused on classic tasks such as named entity recognition \cite{yeshpanov-etal-2022-kaznerd} and sentiment analysis \cite{yeshpanov2024kazsandra}, leaving more advanced applications like instruction tuning, largely unexplored. 
% 
Our contributions are as follows:%\todo{Preslav: These should be refined a bit. I have merged two, but it still feel imperfect. What you should focus on is not what you did, but why does it matter! What should I remeber about this work. Nobody cares what you did. People care what they learn from this. Is there a new dataset? a new task formulation? A new model? A new finding? What should I remember about this paper. If it is just a documentation about a study done, then why keep reading it?}
\begin{compactitem}
    
    % \item We compare the efficacy of open-weight and closed-weight LLMs in LLM-assisted dataset construction for low-resource languages and underrepresented cultures. Our study focuses on Kazakh, specifically in the domains of institutional data and culture, which are highly relevant for real-world applications.
    % \item Our datasets undergo full manual verification to ensure quality, making them a reliable evaluation benchmark for the Kazakh language and culture.
    % \item We fine-tune models such as Qwen \cite{qwen2.5}, Falcon \cite{Falcon3}, and Gemma \cite{gemma2} on our dataset, observing consistent improvements in both multiple-choice and generative tasks. These results highlight the impact of incorporating localized knowledge into instruction tuning and demonstrate the potential of LLM-assisted approaches for expanding instruction datasets in other low-resource languages.
    \item We create an open-source a high-quality manually verified large scale (10K samples) IFT dataset, which covers both cultural, and institutional knowledge, relevant to Kazakhstan.
    \item We contribute new domain knowledge on essential institutional topics, including procedural, legal, structural, and other key aspects of public governance, enhancing LLMs' understanding of these critical areas.
    %We explore the role of institutional knowledge in instruction fine-tuning, highlighting the lack of its use in prior IFT research, with a focus on governmental data for low-resource languages.
    \item We compare the efficacy of open-weight and closed-weight LLMs in LLM-assisted dataset construction for low-resource languages and underrepresented cultures.
    \item We demonstrate that fine-tuning on our dataset, results in consistent improvements in both multiple-choice and generative tasks. These results highlight the impact of incorporating localized knowledge into instruction tuning and demonstrate the potential of LLM-assisted approaches for expanding instruction datasets in other low-resource languages.
\end{compactitem}


%We evaluate multiple LLMs for dataset generation and select GPT-4 due to its superior performance in producing high-quality instructions. All generated instances undergo full manual verification to maintain reliability. We then fine-tune models such as Qwen \cite{qwen2.5}, Falcon \cite{Falcon3}, and Gemma \cite{gemma2} on our dataset, observing consistent improvements in both multiple-choice and generative tasks. Our results highlight the impact of incorporating localized knowledge into instruction tuning and demonstrate the potential for LLM-assisted approaches to expand instruction datasets in other low-resource languages.


%Instruction tuning refers to the process of fine-tuning large language models (LLMs) to perform specific tasks by providing them with clear, structured instructions. This enhances the model's ability to understand and execute tasks based on human guidance, making them more effective in various applications such as answering questions, summarizing, and other task-specific operations. %With the widespread adoption of LLMs in everyday use, instruction tuning has gained significant popularity, becoming crucial for optimizing model performance in real-world applications. Datasets like FLAN, the Public Pool of Prompted Tasks (P3), and Dolly have been created to meet this demand \citep{longpre2023flan, sanh2021multitask, conover2023dolly}. %However, the majority of these efforts have been focused on English, leaving a gap for other languages.

%Despite advancements in instruction tuning, current LLMs still struggle with domain-specific and context-dependent tasks~\cite{ghosh2024closerlooklimitationsinstruction}. Requests directed at such LLMs regarding procedural matters, such as obtaining a passport in a specific country (e.g., country X), frequently produce generalized responses or default explanations based on more widely documented cases, such as the United States. This phenomenon highlights a critical limitation of multilingual models: while they possess the technical capability to process low-resource languages, their practical utility in these contexts remains severely constrained. Specifically, such models often lack a nuanced understanding of the cultural, bureaucratic, and socio-political frameworks unique to underrepresented regions, making them ineffective for localized informational tasks.

%This underscores the necessity of instruction-tuning datasets that integrate region-specific knowledge to enhance LLMs' capacity for handling localized tasks. Likewise, cultural dimensions, including national identity, traditions, folklore, and literature, should be well-represented in the model's knowledge base.

%To bridge these gaps, we present a case study on Kazakh and introduce the first instruction-tuning dataset for the language, encompassing both cultural (CultSet) and sociopolitical (GovSet) domains. Our dataset comprises a diverse selection of texts that reflect Kazakh-specific linguistic patterns and contextual intricacies, with a strong focus on public administration, civic services, and fundamental societal operations. To construct this dataset, we propose an innovative instruction-generation approach that integrates fact extraction with the reverse instruction method introduced by \cite{koksal2024muri}. Specifically, we first extract structured factual information before applying the reverse instruction technique to generate corresponding prompts. Empirical evaluations indicate that this hybrid approach outperforms the reverse instruction method alone, as evidenced by higher human evaluation scores, underscoring its efficacy in developing high-quality instruction-tuning datasets.

 %For low-resource languages like Kazakh, this challenge is compounded by the lack of datasets tailored to their unique linguistic and cultural characteristics. While existing multilingual large language models (LLMs), such as Aya \cite{2024ayamodel} and LLaMA \cite{touvron2023llama}, provide partial support for Kazakh \cite{maxutov-etal-2024-llms}, they are often biased toward high-resource languages and fail to address Kazakh-specific syntax, semantics, and contextual nuances, especially in instruction-based tasks.

 %While existing multilingual large language models (LLMs), such as GPT-4o \cite{openai2024gpt4o}, Claude \cite{anthropic2024claude} can become somewhat proficient in speaking low-resource languages, their understanding of the specific cultures, traditions, and societal contexts behind these languages remains limited. A model may easily understand and generate Kazakh text but might struggle to accurately answer questions about the day-to-day lives of Kazakhstani people. %For instance, if you ask how to obtain a specific document or about a famous TV host, the model might respond in Kazakh but reference Western equivalents rather than providing local, contextually accurate answers. For instance, if you ask an LLM how to get a passport in country X, it often provides a generic response or explains the procedure for obtaining a passport in the USA. This poses a significant challenge for low-resource languages like Kazakh, where it has its own specific procedures and requirements that the model fails to capture.%This gap in contextual and cultural understanding highlights a significant issue when applying LLMs in real-world settings, especially for low-resource languages like Kazakh.


%There are three main methods \cite{koksal2024muri} for creating instruction tuning datasets: human annotation, templatized NLP tasks, and synthetic data generation using LLMs \cite{conover2023dolly, ouyang2022instructgpt, kopf2023openassistant}. For low-resource languages, these methods have significant challenges. Human annotation is expensive and finding native speakers is difficult. Templatized tasks limit dataset diversity and often lack annotated data for low-resource languages \cite{imanigooghari2023glot500}. Synthetic data generation depends on the language capabilities of existing models and often produces less valid and creative outputs \cite{wang2023self}. Both template-based and synthetic methods heavily rely on translation, which can lead to "translationese" \cite{gellerstam1986translationese} artifacts, such as simplified grammar, unidiomatic phrasing, and loss of cultural context. These issues can negatively affect model training by reducing linguistic and cultural alignment \cite{yu2022translate}.

%In addition to linguistic challenges, the development of Kazakh NLP resources must address culturally relevant instructions and the specific needs of government data processing. government data, which forms a cornerstone of public life in Kazakhstan, requires accurate and culturally aligned language understanding to ensure reliable NLP applications. These gaps highlight the critical need for datasets curated to reflect Kazakh linguistic structures and functional requirements. Despite the increasing development of instruction tuning datasets, very few focus on government processes, even for high-resource languages like English. Most instruction tuning datasets prioritize general tasks such as open-domain question answering, summarization, or reasoning, while structured domains like legal and government data remain largely underrepresented. This gap is even more pronounced for low-resource languages like Kazakh, where no dedicated instruction tuning dataset exists for government processes.

%The absence of instruction-tuned government data is particularly problematic for languages like Kazakh, where citizenship applications, legal procedures, taxation, and public services require precise language understanding. Developing instruction-tuned models for these domains is essential for improving access to government information, automating bureaucratic tasks, and enhancing citizen-government interactions. Addressing this gap requires a dataset specifically designed to align with Kazakh linguistic structures and functional government requirements.

%In addition to linguistic challenges, the development of Kazakh NLP resources must address culturally relevant instructions and the specific needs of government data processing. Cultural knowledge is deeply embedded in language, shaping how information is conveyed, interpreted, and understood. Instruction-tuned datasets must capture these nuances to ensure that models can accurately process cultural references, traditions, and historical contexts in Kazakh. Similarly, government data, which forms a cornerstone of public life in Kazakhstan, requires accurate and culturally aligned language understanding to ensure reliable NLP applications. These gaps highlight the critical need for datasets curated to reflect Kazakh linguistic structures and functional requirements.

%Despite the increasing development of instruction tuning datasets, very few focus on either cultural or government processes, even for high-resource languages like English. 
%Despite this challenge, most instruction tuning datasets prioritize general tasks such as open-domain question answering, summarization, or reasoning, while structured domains like legal, government, and cultural knowledge remain largely underrepresented. 


% This underscores the need for instruction tuning datasets that incorporate region-specific knowledge to enhance LLMs' ability to handle localized tasks. Therefore, the absence of such datasets is particularly problematic for countries like Kazakhstan, where citizenship applications, legal procedures, taxation, and public services require precise language understanding. Similarly, cultural topics such as national identity, traditions, folklore, and literature demand contextually aware processing.
%Developing instruction-tuned models for these domains is essential for improving access to government information, automating bureaucratic tasks, and preserving cultural knowledge in NLP applications. 

%To address these gaps, we introduce the first instruction-tuning dataset for Kazakh, focusing on both cultural and government domains. The dataset includes a diverse collection of texts that capture Kazakh-specific linguistic structures and contextual nuances, with a particular emphasis on government processes and public services. 


%Furthermore, we propose a novel approach for instruction generation that combines fact extraction with the reverse instruction method proposed by \cite{koksal2024muri}. Specifically, we first employ a fact extraction process to generate structured inputs and then apply the reverse instruction technique to generate corresponding instructions. Our empirical analysis demonstrates that this combined approach achieves higher scores in human evaluation compared to using reverse instruction alone, highlighting its effectiveness for generating instruction-tuning datasets. 

% To address these gaps, we introduce the first instruction-tuning dataset for Kazakh, focusing on both cultural (CultSet) and government (GovSet) domains. The dataset includes a diverse collection of texts that capture Kazakh-specific linguistic structures and contextual nuances, with a particular emphasis on government processes and public services. We propose a novel approach for instruction generation that combines fact extraction with the reverse instruction method proposed by \cite{koksal2024muri}. Specifically, we first employ a fact extraction process to generate structured inputs and then apply the reverse instruction technique to generate corresponding instructions. Our empirical analysis demonstrates that this combined approach achieves higher scores in human evaluation compared to using reverse instruction alone, highlighting its effectiveness for generating instruction-tuning datasets.









%Moreover, to our knowledge, creating instruction fine-tuning (IFT) datasets specifically focused on government processes has previously been unaddressed.
%In general, there are no instruction fine-tuning (IFT) datasets specifically focused on government processes.

%However, LEGALBENCH \cite{guha2023legalbench} is a notable dataset designed to evaluate legal reasoning in large language models. It includes 162 tasks across categories like rule-recall, application, and interpretation, sourced from diverse legal datasets and contributions by legal experts. While focused on American law, LEGALBENCH demonstrates the potential of domain-specific benchmarks in assessing structured and specialized tasks.


