\section{Proposed method}

\subsection{Factual Instruction Pair Generation}
To develop an instruction-tuning dataset for Kazakh, we utilized GPT-4o to generate factual instruction pairs based on Kazakh content sourced from Kazakh Wikipedia and governmental data. This approach enables us to create instructions that are both informative and culturally relevant, ensuring that the dataset accurately reflects Kazakh-specific knowledge. We expanded the dataset to approximately 9339 instruction pairs, covering a broad range of topics and knowledge domains.

We used a specialized collection of Kazakh Wikipedia pages focused on topics related to Kazakh culture. Starting with an initial set of 8,247 texts, we parsed the content using Beautiful Soup in Python to extract the main text. To ensure the quality and relevance of the dataset, we filtered out texts shorter than 500 characters, resulting in a refined collection of 4,400 texts. This cultural focus ensures that the dataset captures unique aspects of Kazakh heritage, language, and customs, supporting culturally aligned instruction-tuning applications. For governmental data, we manually collected 1300 texts from \textbf{gov.kz\footnote{\url{https://www.gov.kz}}}, prioritizing information relevant to Kazakh public life and services.

As a preliminary study, we sampled 30 texts from Kazakh Wikipedia and 20 texts from \textbf{gov.kz}. To generate instruction pairs based on Kazakh texts, we employed two distinct prompting approaches tailored to our specific needs: the one-step approach and the two-step approach. In both cases, instruction pairs were generated using the reverse instruction method. The one-step approach applied reverse instruction directly to each text, producing 465 instruction pairs. In contrast, the two-step approach involved first extracting individual facts from each text, followed by applying reverse instruction to generate questions, resulting in 428 pairs. Together, these methods produced a total of 893 instruction pairs.

In the \textbf{one-step approach}, we instructed GPT-4o to directly generate instruction pairs in reverse, based on the provided text as described in the \cite{koksal2024muri}. The exact prompt used is shown below:
\begin{quote} \textit{ You are given a title and a text. I want you to create instruction tuning pairs based on the given text in \textbf{Kazakh}. The instructions MUST incorporate the provided context where relevant to make the instructions more specific and meaningful. The pairs may fall into the following categories, but you are free to use other relevant categories if appropriate: \begin{itemize} \item Is it true that ... \item Explain ... \item Describe ... \item List the steps ... \end{itemize} For each category, provide: \begin{enumerate} \item A clear instruction. \item Relevant input (only if necessary). \item A detailed answer. \end{enumerate}} \end{quote}

In the \textbf{two-step approach}, we first instructed GPT-4o to extract individual facts from the text and then generate instruction pairs in reverse for each fact. The prompt for this method is as follows:

\begin{quote}
\textit{
You are given a title,  text. I want to extract all the facts from the given text. Based on the extracted facts, I want you to create instruction fine-tuning pairs in \textbf{Kazakh}. The instructions MUST incorporate the provided context where relevant to make the instructions more specific and meaningful. The pairs may fall into the following categories, but you are free to use other relevant categories if appropriate: \begin{itemize} \item Is it true that ... \item Explain ... \item Describe ... \item List the steps ... \end{itemize} For each category, provide: \begin{enumerate} \item A clear instruction. \item Relevant input (only if necessary). \item A detailed answer. \end{enumerate}} \end{quote}


To evaluate the quality of the generated instruction pairs, we asked three native Kazakh speakers majoring in World Languages and Literature (WLL) to assess the pairs based on the following criteria:

\textbf{Correctness (0-1 scale)}:
This measures whether the response accurately fulfills the instruction or task. A score of 1 indicates that the response is factually and contextually correct, while a score of 0 reflects incorrect or irrelevant content. Correctness ensures that the model delivers reliable and 
accurate outputs.


\textbf{Fluency (1-5 scale)}:
This evaluates the naturalness and readability of the response in Kazakh. Scores range from 1 (poor fluency, significant grammatical errors, awkward phrasing) to 5 (excellent fluency, grammatically correct, and stylistically appropriate). Fluency is crucial for assessing the modelâ€™s ability to produce responses that align with native language standards.


\textbf{Ambiguity (0-1 scale)}:
This criterion measures whether the response contains vague or unclear language. A score of 1 indicates that the response is free from ambiguity and provides clear and precise answers, while a score of 0 reflects ambiguous phrasing or lack of clarity in addressing the instruction.
Each response was independently evaluated for these criteria by native Kazakh speakers. 
\par The results can be seen in Table~\ref{tab:evaluation}.




Following the preliminary results, where both approaches were tested, we chose the two-step approach for generating the full dataset due to its effectiveness in producing higher-quality question-answer pairs. Using this approach, we expanded the dataset to a total of 9339 instruction pairs.


\begin{table}[ht]
\centering
\renewcommand{\arraystretch}{1.3} % Adjust row height
\resizebox{0.35\textwidth}{!}{ % Resize to 90% of the text width
\begin{tabular}{lcc}
\toprule
\textbf{Criteria} & \textbf{One-Step} & \textbf{Two-Step} \\ 
\midrule
Correctness (0/1) & 0.95 & 0.93 \\ 
Fluency (1-5)     & 4.65 & 4.70 \\ 
Ambiguity (0/1)   & 0.44 & 0.38 \\ 
\bottomrule
\end{tabular}
}
\caption{Evaluation on sampled data for Instruction Pair Generation}
\label{tab:evaluation}
\end{table}


\begin{table}[ht]
\centering
\renewcommand{\arraystretch}{1.3} % Adjust row height
\resizebox{0.5\textwidth}{!}{ % Resize to fit within the text width
\begin{tabular}{lcc}
\toprule
\textbf{}       & \textbf{Wikipedia} & \textbf{Governmental Data} \\ 
\midrule
Texts                   & 4,439              & 1,300                     \\ 
Generated Instruction Pairs  & 4439                & 5900                       \\ 
\bottomrule
\end{tabular}
}
\caption{Statistics on Wikipedia and Governmental Data for Instruction Pair Generation}
\label{tab:statistics_wikipedia_governmental}
\end{table}


\subsection{Dataset Analysis}
\subsubsection{Wikipedia Data for Kazakh Culture}
For the Wikipedia-based portion of our dataset, which focuses on Kazakh culture, including topics such as traditions, historical events, and notable figures, we generated 4,400 instruction pairs that reflect the cultural and linguistic nuances of the Kazakh context. Through content screening and evaluation, we observed that the generated instruction pairs were generally of high quality, exhibiting a good level of specificity with minimal ambiguity.

\subsubsection{Governmental Data}
The governmental dataset consisted of 1,300 collected texts, a smaller number compared to the 4,439 texts from Wikipedia. To address this imbalance and ensure sufficient representation of governmental topics in the overall dataset, we instructed the model to generate 4-5 instruction pairs per text. However, during content evaluation, we observed a significant level of ambiguity in the generated pairs, often due to unclear references or insufficient context. To improve the quality of these pairs, we refined our prompts by providing a general topic and context for each text, ensuring the instructions became more specific, accurate, and aligned with the intended meaning of the source material.

\par
The final results of the collected texts and their corresponding generated instruction pairs are summarized in Table~\ref{tab:statistics_wikipedia_governmental}.
\par

An example of an ambiguous question is provided in Appendix Figure ~\ref{fig:ambiguity}. The initial instruction, "What rights do foreigners and stateless persons have?", lacks specificity and context, which can lead to multiple interpretations. For instance,  it does not specify the geographical or legal framework being referred to, leaving the instruction open-ended and prone to misinterpretation. To resolve this ambiguity, additional context was incorporated into the instruction. The final instruction, refines the original prompt by specifying the group of individuals (permanent residents) and the jurisdiction (Republic of Kazakhstan). This ensures that the instruction is precise and aligned with the content of the associated output, improving clarity and coherence in the instruction-output pair.


\begin{figure}[h]
\centering
\includegraphics[width=0.45\textwidth]{images/piechart.png} % Adjust width as needed
\caption{Distribution of instruction types in the dataset.}
\label{fig:piechart}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.45\textwidth]{images/wordcount.png} % Adjust width as needed
\caption{ Instruction vs. Output Word Count Distribution.}
\label{fig:instruction_distribution}
\end{figure}





\par
The diversity of instruction types in the dataset is illustrated in Figure~\ref{fig:piechart}, showcasing a wide range of categories that emphasize explanatory and informative tasks.  Figure~\ref{fig:instruction_distribution} shows instructions with both short (around 5 words) and medium length (around 15 words) can generate long outputs. This shows that concise instructions can still elicit detailed responses, while longer instructions do not always correspond to proportionally longer outputs.


\subsection{Annotation Process}
For all human evaluation aspects of this project, we asked three students from Nazarbayev University majoring in World Languages and Literature (WLL) to serve as annotators.
