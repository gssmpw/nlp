\section{Experiments}

We conducted two experiments: multiple-choice questions (MCQ) and text generations evaluation. We will detail each of the evaluation in the following sections.

\paragraph{Model Selection} For both MCQ and generation evaluations, we use three models: Gemma-2-9B (Gemma) \cite{gemma2}, Qwen-2.5-7B (Qwen)\cite{qwen2.5}, and Falcon-3-10B (Falcon)~\cite{Falcon3}. While these LLMs have multilingual capabilities, they were not specifically trained for Kazakh, allowing us to assume that our IFT data is novel to them. 

\paragraph{Fine-tuning} We performed full fine-tuning on Gemma-2-9b, Qwen-2.5-7b, and Falcon-3-10b using the AdamW optimizer with hyperparameters $\beta_1 = 0.9$, $\beta_2 = 0.95$, $\epsilon = 1e{-5}$, and a weight decay of 0.1. We scaled the gradient norms using a maximum norm clipping value of 1.0. The learning rate was kept constant throughout the fine-tuning without any warm-up or decay with a value of $1e{-6}$ for Gemma-2 and Falcon-3, and $1e{-5}$ for Qwen-2.5. The batch size used was 16, and we packed multiple documents until the maximum sequence length was 8,192 tokens. Cross-document attention is disabled by modifying attention masks so the tokens of a document only attend to the tokens from the same document in a causal way. No adjustment were made to the original tokenizer for each model.

\paragraph{Baseline} As a baseline, we include the Kazakh Alpaca dataset,\footnote{\url{https://huggingface.co/datasets/AmanMussa/kazakh-instruction-v2}} which has been translated and localized into Kazakh. For each model, we conduct full fine-tuning with (1) our training dataset, (2) Alpaca, and (3) a combination of Alpaca and our training dataset.


%Additionally, we applied the respective chat templates for all the models and computed the loss only over response/assistant tokens.

%We tried to compare the models in the same settings, so no adjustments were made to the original tokenizers, despite of the impact of language-specific tokenizers on the overall performance~\cite{nikolich-etal-2024-vikhr}.
% S GOES TO LIMITATION OR DELETE?
% There has been significant research on improving tokenization for non-English languages, as seen in the Vikhr study \cite{nikolich-etal-2024-vikhr}, where researchers addressed the inefficiencies of English-oriented tokenizers by reconstructing a language-specific tokenizer tailored to Russian, leveraging a dedicated corpus. This adaptation significantly improved model efficiency and performance during instruction tuning, showcasing how tokenization can be a critical factor in fine-tuning language models for specific linguistic contexts. While such approaches are ideal for enhancing instruction-following capabilities, we chose not to focus on a language-specific tokenizer to ensure a fair comparison with existing models that rely on standard tokenization frameworks. By using pre-existing tokenizers, we aim to isolate the impact of instruction tuning and dataset quality without introducing additional variables related to tokenizer customization.

% \subsubsection{Human evaluation}
% To evaluate the model, we sampled 400 instruction-response pairs and assessed them across three specific criteria: correctness, fluency, and ambiguity. Each criterion was carefully designed to capture distinct aspects of the model's performance and usability. 
% \par
% This setup ensures a holistic understanding of the model's performance. In Appendix Table ~\ref{tab:multilingual-issues}, we provide detailed examples of human evaluation, including the assigned scores and corresponding comments that explain the rationale behind each score. These examples illustrate how correctness, fluency, and ambiguity were assessed, highlighting specific issues such as factual inaccuracies, grammatical errors, and lack of clarity. The provided annotations offer a clear understanding of how the evaluation criteria were applied to real instruction-response pairs.
% \subsubsection{LLM as a Judge}
% To ensure consistent, scalable, and unbiased evaluations, we employed GPT-4o as an automated judge to assess the responses of LLaMA 3.1 - 8B and Qwen-2.5 - 7B models.The evaluation compared the models' outputs under two conditions: without fine-tuning and after fine-tuning on our dataset. GPT-4o was tasked with evaluating the generated responses across three criteria: correctness, ambiguity, and fluency.

% In this setup, the model-generated responses were assessed in a blind manner to ensure unbiased comparisons. For each instruction-response pair, GPT-4o provided scores for each criterion, enabling us to quantitatively evaluate the impact of fine-tuning on the models' performance. This automated approach allowed for consistent and scalable evaluation while providing valuable insights into how fine-tuning enhances the models' ability to produce accurate, coherent, and fluent responses tailored to culturally nuanced instructions. The evaluation process utilized the following prompt to guide GPT-4o in assessing the quality of the generated responses:

% \begin{quote}
% \textit{
% Evaluate the following response based on the given instruction and ground truth using the following criteria:
% \begin{enumerate}
%     \item \textbf{Correctness (0/1):} Does the response accurately answer the instruction based on the ground truth?
%     \item \textbf{Fluency (1-5):} How well is the response written in terms of grammar, clarity, and coherence?
%     \item \textbf{Ambiguity (0/1):} Is the response free from unclear or vague information?
% \end{enumerate} 
% Provide your scores for each criterion and a brief explanation for your evaluation.
% \textbf{Instruction:} [Insert Instruction] \\
% \textbf{Ground Truth:} [Insert Ground Truth] \\
% \textbf{Generated Response:} [Insert Generated Response]}
% \end{quote}

% \\
% \par
% The final results of the two evaluation methods, human evaluation and LLM evaluation, are presented in Table~\ref{tab:finetuning-comparison}. This table highlights the performance of LLaMA-3.1 - 8B and Qwen-2.5 - 7B under two conditions: without fine-tuning and after fine-tuning on our dataset. The evaluation criteria—correctness, ambiguity, and fluency—demonstrate the impact of fine-tuning on improving the models' ability to generate accurate, coherent, and fluent responses. Both human judges and GPT-4o consistently indicate improvements across these metrics, with LLaMA showing stronger overall performance compared to Qwen, particularly in correctness and fluency. We computed the Pearson correlation coefficient to assess the alignment between human evaluation and GPT-4o's automated evaluation. The results indicate a high correlation for fluency (0.85) and correctness (0.78), while ambiguity exhibited moderate alignment (0.72). These findings demonstrate that GPT-4o is largely consistent with human judgments, validating its use as a reliable automated evaluation framework.

\input{tables/mcq_eval}

\subsection{Multiple-choice Question Evaluation}
\label{sec:mcq}

\paragraph{Dataset}
A dedicated open-source Kazakh NLP community\footnote{\url{https://huggingface.co/kz-transformers}} has collaboratively developed and crowd-sourced multiple hand-crafted benchmarks to assess the factual knowledge of LLMs in Kazakh.  We use three multiple-choice question (MCQ) datasets: (1) Dastur-MC~\cite{horde_dastur_kk2024}, which evaluates knowledge of Kazakh traditions, (2) Kazakh Constitution-MC~\cite{horde_constitution_kk2024}, which focuses on Kazakhstan’s legal system, and (3) Kazakh Unified National~\cite{horde_unt_kk2024}, which assesses citizen rights, legal protections, and societal knowledge (referred to as the "Human Rights and Society" dataset).\footnote{Examples of test questions are provided in Appendix \ref{sec:mcq_samples}.}

Each dataset consists of multiple-choice questions with four answer options, only one of which is correct. We selected these evaluation benchmarks because they align with the focus of our instruction fine-tuning dataset and are not derived from our document sources (\texttt{CultSet} and \texttt{GovSet}). These datasets cover culturally significant topics, legal frameworks, and citizen-government interactions, reflecting real-world applications that our fine-tuned models aim to support.

Since no documented quality assurance process was available for the three datasets, we conducted a manual verification to ensure the accuracy of the questions. To maintain a fair and valid comparison, only the manually verified samples were used in our evaluation. For the Dastur-MC dataset, we randomly sampled 300 questions and manually verified their correctness. The same process was applied to the Kazakh Constitution-MC and Human Rights and Society datasets, with 200 randomly selected questions from each.

\paragraph{Setup}
In addition to the fine-tuned models, we include retrieval-augmented generation (RAG) without fine-tuning to estimate the upper bound of the original models' performance. For RAG, we use BM25 encoding, as no specialized Kazakh retrieval encoder is available. For each question, we retrieve the top two matching text chunks (each 256 symbols long) from the training texts of our IFT corpus and provide them as additional context.

To assess model capability, we use the \texttt{LM Eval Harness}~\cite{eval-harness} framework in a zero-shot setting. During evaluation, the answer is selected based on the alphabetical option with the highest likelihood.

%We also conduct a series of experiments using the models in a retrieval-augmented generation (RAG) setting without fine-tuning to assess the upper bound of the original models' performance.

\paragraph{Result}
Table~\ref{tab:result_mcq} presents the zero-shot evaluation results across different models and techniques. Overall, our fine-tuned dataset consistently outperforms other approaches across datasets and models. The only exception is the Constitution dataset, where RAG performs better with Gemma. Models fine-tuned on Kazakh Alpaca show some improvement, though it remains lower than that achieved with our instruction fine-tuning (IFT) dataset.

Combining parts of our IFT dataset with the translated Alpaca dataset yields the highest performance gains. This aligns with prior studies~\cite{mixingup,demystifying}, which suggest that incorporating general chat instructions alongside domain-specific ones enhances model performance.

For RAG-enhanced models, performance generally exceeds that of the vanilla models, except for Falcon on the Constitution dataset. However, fine-tuned models consistently achieve higher scores than their RAG-enhanced counterparts. We hypothesize that this is due to the models' limited proficiency in Kazakh, which may hinder their ability to fully understand the retrieved context. As a result, despite the additional information provided by RAG, the models may struggle to extract the necessary details to select the correct answer in MCQs.

%shows that the fine-tuning on Kazakhstan specific data improves the accuracy on the MSQ test within all 3 benchmarks. Interestingly, when fine-tuned on unrelated to Kazakhstan translated Alpaca\footnote{\url{https://huggingface.co/datasets/AmanMussa/kazakh-instruction-v2}} dataset, models still show some improvement, which is likely due to adaptation to questions in Kazakh language. Combining part of the instructions from both our IFT dataset, and the translated Alpaca dataset we achieve the best improvement in scores, which aligns well with the previous studies~\cite{mixingup,demystifying} that have shown, that adding general chat instructions to domain specific ones improves the model performance.



% \subsubsection{Generation Evaluation}
% \label{sec:gen_eval}
% To assess the quality of model outputs, we sampled 1,000 instructions from the dataset and generated corresponding outputs using the models under evaluation. 

% For evaluation, we used the LLM as a judge method. Specifically, GPT-4o was tasked to determine whether the model-generated responses to the instructions were correct by comparing them to the reference outputs (referred to as the "golden generation") provided in the dataset.

% Recognizing the critical importance of factual accuracy in responses to government-related questions—where any inaccuracies could result in significant consequences—we employed an adapted version of FactScores, as introduced in~\cite{min-etal-2023-factscore}. Using GPT-4o, we extracted factual elements and procedural steps (where applicable) from the golden answers. The same process was applied to the model-generated outputs. Precision was calculated by identifying hallucinated facts or steps in the generated outputs that were absent in the golden answers, while recall was measured by evaluating whether the generated outputs fully covered the facts and steps present in the golden answers. These metrics provided a rigorous means to evaluate the factual accuracy and completeness of the model outputs.


% \begin{table}[ht]
% \centering
% \begin{tabular}{|l|c|c|}
% \hline
% \textbf{Model} & \textbf{AC  (binary)} & \textbf{AC (5-scale)} \\ \hline
% Llama-3.1$_{8B}$ & 0.052& 1.236  \\ \hline
% Qwen-2.5$_{7B}$ & 0.02 & 1.488 \\ \hline
% Gemma-2$_{9B}$ &  0.052 & 1.502 \\ \hline
% \end{tabular}
% \caption{Evaluation with LLM-as-a-judge method. AC refers to the average correctness, measured as a binary score or a score on a 5-point scale.}
% \label{tab:model_eval}
% \end{table}
\subsection{Generation Evaluation}
\label{sec:rogue}

We evaluate generation performance using our test set, which consists of 500 questions from both \texttt{CultSet} and \texttt{GovSet} (excluded from fine-tuning). We compare the best models from Section~\ref{sec:mcq} against their vanilla counterparts. In this section, "After Fine-Tuning" refers to models fine-tuned on Alpaca + Our Data, while "Vanilla" refers to the original pre-trained models.


\paragraph{Automatic Evaluation with ROUGE and BERTScore} As shown in Table~\ref{tab:rouge}, fine-tuned models generally outperform their vanilla counterparts, except for Qwen, where fine-tuning results in a lower ROUGE-L score \cite{lin-2004-rouge}. However, a lower ROUGE-L does not necessarily indicate worse performance—it may be due to Qwen generating different phrasings compared to the gold answers.

To further validate the quality of generated responses, we also evaluate BERTScore \cite{zhangbertscore}. We use Kaz-RoBERTa\footnote{\href{https://huggingface.co/kz-transformers/kaz-roberta-conversational}{Huggingface model: kaz-roberta-conversational}} as the encoder model, as it is one of the few open-source Kazakh-language transformers. The BERTScore results in Table~\ref{tab:bertscore} align well with the ROUGE-L scores. However, since Kazakh is a low-resource language, BERTScore should be considered a reference point rather than a definitive metric, as Kaz-RoBERTa embeddings may not perfectly capture synonym relationships.

\begin{table}[t!]
\scriptsize
    \centering
    \begin{tabular}{lcc|cc}
        \toprule
        & \multicolumn{2}{c|}{\textbf{CultSet}} & \multicolumn{2}{c}{\textbf{GovSet}} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5}
        & \textbf{After FT} & \textbf{Vanilla} & \textbf{After FT} & \textbf{Vanilla} \\
        \midrule
        Gemma & \textbf{24.87} & 15.76 & \textbf{25.10} & 16.12 \\
        Falcon & \textbf{27.98} & 25.96 & \textbf{28.70} & 26.17 \\
         Qwen & 26.63 & \textbf{27.64} & 28.42 & \textbf{30.27} \\
        \bottomrule
    \end{tabular}
    \caption{Rouge-L comparison on \texttt{CultSet} and \texttt{GovSet}.} % datasets
    \label{tab:rouge}
\end{table}


% \begin{table*}[h]
% \scriptsize
%     \centering
%     \begin{tabular}{lccc ccc}
%         \toprule
%         & \multicolumn{3}{c}{\textbf{CultSet}} & \multicolumn{3}{c}{\textbf{GovSet}} \\
%         \cmidrule(lr){2-4} \cmidrule(lr){5-7}
%         & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} \\
%         \midrule
%         \multicolumn{7}{c}{\textbf{Ours (Fine-Tuned)}} \\
%         Gemma  & 41.94 & 46.36 & 43.62 & 40.27 & 44.90 & 42.00 \\
%         Falcon & 24.59 & 29.68 & 26.64 & 23.78 & 27.73 & 25.36 \\
%         Qwen   & 39.64 & 45.40 & 41.82 & 36.28 & 40.20 & 37.59 \\
%         \midrule
%         \multicolumn{7}{c}{\textbf{Orig (Before Fine-Tuning)}} \\
%         Gemma  & 29.26 & 33.47 & 30.92 & 27.36 & 34.81 & 30.39 \\
%         Falcon & 23.29 & 28.17 & 25.20 & 20.38 & 24.68 & 22.11 \\
%         Qwen   & 40.58 & 47.46 & 43.40 & 36.57 & 44.14 & 39.50  \\
%         \bottomrule
%     \end{tabular}
%     \caption{BERTScore Precision, Recall, and F1 for CultSet and GovSet (Ours vs. Orig)}
%     \label{tab:bertscore}
% \end{table*}

\begin{table*}[ht]
\scriptsize
    \centering
    \begin{tabular}{llccc ccc}
        \toprule
        & & \multicolumn{3}{c}{\textbf{CultSet}} & \multicolumn{3}{c}{\textbf{GovSet}} \\
        \cmidrule(lr){3-5} \cmidrule(lr){6-8}
        & & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} \\
        \midrule
        \multirow{3}{*}{\textbf{ After Fine-Tuning}} & Gemma  & 41.94 & 46.36 & 43.62 & 40.27 & 44.90 & 42.00 \\
                                                    & Falcon & 24.59 & 29.68 & 26.64 & 23.78 & 27.73 & 25.36 \\
                                                    & Qwen   & 39.64 & 45.40 & 41.82 & 36.28 & 40.20 & 37.59 \\
        \midrule
        \multirow{3}{*}{\textbf{Vanilla}} & Gemma  & 29.26 & 33.47 & 30.92 & 27.36 & 34.81 & 30.39 \\
                                                            & Falcon & 23.29 & 28.17 & 25.20 & 20.38 & 24.68 & 22.11 \\
                                                            & Qwen   & 40.58 & 47.46 & 43.40 & 36.57 & 44.14 & 39.50  \\
        \bottomrule
    \end{tabular}
    \caption{BERTScore Precision, Recall, and F1 for \texttt{CultSet} and \texttt{GovSet}.}
    \label{tab:bertscore}
\end{table*}


\paragraph{Preference Evaluation with GPT-4o}
We conducted a 1-to-1 preference evaluation using the LLM-as-a-judge approach. Specifically, we prompted GPT-4o to compare responses from different models and determine whether each response wins, loses, or ties. The prompt includes the instruction and the gold response as context for GPT-4o.\footnote{The prompt used for comparison is provided in Appendix~\ref{sec:pref-eval}.} As shown in Figure~\ref{fig:human_fg_1000}, the results align with ROUGE-L and BERTScore, confirming that fine-tuned models generally produce improved outputs. Compared to Falcon, Qwen and Gemma exhibit more significant improvements (63\%–80\% winning rate), likely because their pre-trained versions were less optimized for the task, making fine-tuning more impactful.


Additionally, we analyze the win rate across topics in \texttt{CultSet} and \texttt{GovSet}, as shown in Appendix~\ref{app:preference-category-eval-results}. The results indicate that the impact of fine-tuning varies by topic and is not always consistent. In \texttt{CultSet}, fine-tuning Qwen with our IFT data yields the most improvement in Cultural Institutions and Culture \& Traditions, while the gains are smaller in Science \& Humanities and even lead to a decline in performance for Education \& Academia. In \texttt{GovSet}, fine-tuning Qwen with our dataset significantly enhances performance in Legal Assistance, though the improvement is less noticeable in Employment-related topics.



%after fine-tuning Gemma's and Qwen’s responses are more preferred than the ones of original model in Cultural Institutions category, whereas Falcon's original model performs better in this category. 
%A similar pattern appears in \texttt{GovSet}, where fine-tuned Gemma and Qwen outperform their original versions in Benefits, Allowances, and Pensions, while Falcon shows nearly balanced preferences before and after fine-tuning.
%Overall, the categories where fine-tuning provides the most improvement are largely consistent between Gemma and Qwen (matching in the top three for \texttt{GovSet} and top two for \texttt{CultSet}) but diverge significantly from Falcon. We hypothesize that these differences stem from variations in the models' pretraining data.

While LLM-based evaluations provide scalable comparisons, they may not fully capture human judgment nuances, making human evaluation essential for validating model preferences. Therefore, three human annotators conducted a preference evaluation on a randomly sampled 100 examples for each model (Gemma, Qwen, and Falcon) across both \texttt{CultSet} and \texttt{GovSet}. Their judgments were compared against the GPT-based preference evaluation \textbf{to assess alignment}. We computed Cohen’s Kappa between GPT-4o and the annotators, obtaining 0.63 for \texttt{CultSet} and 0.68 for \texttt{GovSet}, indicating substantial agreement. We have also calculated the agreement rate between annotators (detailed in Appendix \ref{app:inner-annot-gen-eval}). The results show that GPT's alignment with human preferences is moderate, with better agreement on \texttt{GovSet} than \texttt{CultSet}.
%We also conducted a human preference evaluation on a subset of 100 samples from CultSet and 100 from GovSet to compare against the GPT-4o-based evaluation. The results show a moderate agreement between GPT-4o and human preferences, with a Cohen's kappa of 0.59. Additionally, the agreement rate between GPT-4o and human annotators was 61\%, indicating a substantial overlap in judgments while still reflecting differences in evaluation criteria between LLM-based and human assessments.

% Participants were presented with two responses to the same instruction in random order—one generated before tuning and one generated after tuning. They were tasked with selecting the response they found superior, considering both its correctness and overall soundness. 
% This human evaluation method allowed us to assess the perceived improvement in the quality and naturalness of model outputs following the proposed adjustments.
% Then we run 


% \textbf{CultSet} The preference results indicate that fine-tuning on our dataset significantly improved performance across most categories, especially for topics like "Culture and Traditions," "Historical Figures," and "Arts and Entertainment," with Gemma and Qwen models showing a clear preference for fine-tuned outputs. However, categories like "Education and Academia" and "Health and Medicine" favored the original models for Qwen and Falcon, suggesting that these domains may require more specialized fine-tuning data. Falcon displayed a more balanced distribution of preferences, with higher tie rates and narrower gaps between fine-tuned and original outputs, likely due to its robust pretraining.

% \textbf{GovSet} Across the three models (Gemma, Qwen, Falcon), the fine-tuned version ("Ours") consistently outperforms the original model across most categories in GovSet, with Gemma and Qwen showing particularly strong preferences for the fine-tuned outputs in domains like "Legal Assistance," "Healthcare," and "Employment and Job Placement." Falcon, however, demonstrates a higher proportion of ties, indicating a closer alignment between its original and fine-tuned outputs, likely due to its strong baseline performance. Notable exceptions include categories like "Financial Literacy" and "Taxes and Fines," where the original model occasionally outperforms the fine-tuned version, suggesting areas where pretraining data may already provide robust coverage. 

\paragraph{Conversational Evaluation.} As an extension of these experiments, we generated a set of 100 conversations for both \texttt{CultSet} and \texttt{GovSet} combined, covering topics presented in Figure \ref{fig:distribution-all}. These conversations were intentionally left unfinished using a special prompt, as detailed in Appendix \ref{sec:pref-eval}. Both the original and fine-tuned models were tasked with generating the most appropriate continuation for each conversation. Examples of the resulting texts are showin in Table \ref{sec:conversational-data-sample}.
To evaluate the quality of the responses, we employed an LLM-as-a-judge framework. The results, presented in Figure \ref{fig:conversational_preferences}, indicate that models fine-tuned on domain-specific data produced significantly more coherent and contextually appropriate responses compared to their pre-fine-tuning counterparts.
We also see that in the conversational settings there are less ties, compared to simple question answering.

% \begin{figure*}[ht!]
%     \centering
%     \begin{minipage}[b]{0.45\textwidth}
%         \centering
%         \includegraphics[scale=0.4]{images/new_gen_eval_pref.png}
%         \caption*{(a) CultSet}
%     \end{minipage}
%     \hfill
%     \begin{minipage}[b]{0.45\textwidth}
%         \centering
%         \includegraphics[scale=0.4]{images/new_gen_eval_gov.png}
%         \caption*{(b) GovSet}
%     \end{minipage}
%     \caption{Distribution of preferences for CultSet (a) and GovSet (b) datasets across models. The charts illustrate the percentage of 'Tie', 'Before Fine-Tuning', and 'After Fine-Tuning' preferences in each dataset.}
%     \label{fig:human_fg_1000}
% \end{figure*}


%\clearpage
%\section{Gen eval preference}
% \begin{figure*}[ht!]
%     \centering
%     \includegraphics[width=0.7\linewidth]{images/gen_eval_cult.png}
%     \caption{Category-wise comparison of preference evaluation on CultSet.}
%     \label{fig:category-wise-eval-cult}
% \end{figure*}

% \begin{figure*}[ht!]
%     \centering
%     \includegraphics[width=0.7\linewidth]{images/gen_eval_gov.png}
%     \caption{Category-wise comparison of preference evaluation on GovSet.}
%     \label{fig:category-wise-eval-gov}
% \end{figure*}


\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{images/conversation_chart.png}
    \caption{Conversational data preference evaluation.}
    \label{fig:conversational_preferences}
\end{figure}
