
\section{Conclusion and Future Work}
We introduced a culturally and institutionally aligned instruction-tuning dataset for Kazakh, % with a strong focus on 
aiming to enhance practical knowledge representation and address the specific needs of public governmental data processing in Kazakh.
Through a carefully designed data collection pipeline,
% that included web-scraping, 
we generated instruction-tuning examples using GPT-4o and ensured their quality via a manual correction and localization to capture Kazakh linguistic and cultural nuances accurately.

The evaluation results show that this approach substantially improved the model's factual knowledge, and the understanding of low-resource languages.
It also shows that after such fine-tuning, the model's responses are much better in terms of correctness and soundness, as assessed by native speakers and LLM as a judge.
% To evaluate the impact of fine-tuning, we outlined a comprehensive evaluation process involving native Kazakh speakers, who assessed the models on informativeness, coherence, and fluency. This evaluation demonstrates the effectiveness of instruction tuning in low-resource languages and establishes a benchmark for future Kazakh NLP models. Our approach highlights the critical importance of cultural and linguistic alignment in instruction-tuning datasets, particularly for languages with limited existing resources.

In future work, we plan to apply this methodology to other languages and dialects. We further aim to work towards streamlining and automating the process as much as possible. 
We will also focus more on the modeling part of the experiments, and open-source culturally and institutionally relevant models for low-resource languages including Kazakh.
