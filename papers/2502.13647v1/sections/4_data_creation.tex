

\section{Data}
\subsection{Document Source}

\paragraph{\texttt{GovSet}} We manually collected 1,376 texts from the official Kazakhstan e-Government portal (gov.kz\footnote{\url{https://www.gov.kz}}), the primary and most comprehensive platform for all public services, governmental processes, and administrative resources in the country. As the central hub for Kazakhstan's digital governance, \texttt{gov.kz} consolidates a wide range of essential information into a single system, covering diverse aspects of public administration, legal frameworks, citizen services, and governmental initiatives. 
% REPETITION The collected texts cover various aspects of Kazakh public life and services, reflecting the country’s ongoing efforts to digitalize and streamline public administration through its e-government framework. 
By incorporating these texts, we ensure that the dataset captures essential institutional aspects of life in Kazakhstan, including its governmental structure and public services. This enrichment enhances instruction-tuning applications, making them more linguistically appropriate and contextually informed.


\paragraph{\texttt{CultSet}} We automatically collected 4,400 texts from Kazakh Wikidata,\footnote{\href{https:_/_/kk.wikipedia.org}{kk.wikipedia.org}} specifically focusing on pages related to Kazakh culture. These pages were identified based on metadata that explicitly indicated their relevance to Kazakh cultural topics. 
The parsed texts include various aspects of Kazakh traditions, heritage, arts, and historical practices, providing a rich source of culturally relevant content.
This ensures that the dataset reflects the depth and diversity of Kazakh culture, making it suitable for instruction-tuning tasks that require a culturally grounded perspective.

%To ensure the uniqueness and quality of the dataset, we performed a MinHash deduplication.

\subsection{LLM-assisted Data Generation}

We benchmark one open-weight LLM: LLaMA 3.1-70B \cite{touvron2023llama}, and three closed-weight LLMs: GPT-4o \cite{openai2024gpt4o}, Gemini-1.5 \cite{google2024gemini15}, and Claude-3.5-Sonnet \cite{anthropic2024claude}, to assess their effectiveness in assisting dataset creation. These models were selected based on their strong performance in multilingual benchmarks. However, their capability in generating instruction datasets specific to Kazakh government and cultural data remains uncertain.

%For this study, we used the following LLMs: LLaMA 3.1-70B \cite{touvron2023llama}, GPT-4o \cite{openai2024gpt4o}, Gemini-1.5 \cite{google2024gemini15}, and Claude-3.5-sonnet \cite{anthropic2024claude}.  We selected these LLMs because of their advanced capabilities and widespread use, making them suitable for evaluating instruction generation across various tasks. 

We design a prompt (see Appendix~\ref{sec:prompts}) that instructs LLMs to first extract factual information from a given Kazakh document and then generate an instruction dataset based on the extracted content. Table~\ref{tab:ds_stats} provides detailed statistics on the source documents and the resulting instruction fine-tuning (IFT) dataset using GPT-4o. Specifically, we use 4,400 Kazakh cultural Wikipedia documents and 1,376 Kazakh government data sources, generating a total of 10,600 IFT instances. Of these, 58\% belong to the government public data category (\texttt{GovSet}), while the remaining samples are derived from Wikipedia (\texttt{CultSet}). The example of generated IFT data can be found in Table~\ref{tab:example_govset} and Table~\ref{tab:example_cultset}.

\paragraph{Human Evaluation Across LLMs}  
For each LLM, we sampled 100 generated IFT instances, drawn from 25 randomly selected \texttt{GovSet} and 25 \texttt{CultSet} documents. Additionally, we randomly sampled 100 instances from MURI~\cite{koksal2024muri}, which also includes Kazakh IFT data, to provide a comparative quality assessment. Two native Kazakh speakers were recruited to manually evaluate the generated data based on the following criteria:
\begin{compactitem}
    \item \textbf{Correctness}: The factual accuracy and alignment with the original text. A high score indicates that the generated pair adheres closely to the source material without introducing errors or inaccuracies.
    \item \textbf{Fluency}: The grammatical and stylistic quality of the generated text. A higher score reflects well-structured, natural, and polished language.
    \item \textbf{Completeness}: The degree to which the instruction-response pair is clear, contextually grounded, and free from ambiguity. High scores indicate that the pair is fully self-contained, with enough context to make it understandable. % and actionable.
\end{compactitem}
All criteria were rated on a Likert scale from 1 to 5, with 5 representing the highest quality. A detailed evaluation rubric is provided in Table~\ref{tab:multilingual-issues}.

%Each response was evaluated on a -pofiveint scale, assessing its alignment with predefined criteria (a detailed rubric is provided in Table \ref{tab:multilingual-issues}).
Table~\ref{tab:llm-instruction} presents the quality assessment of various LLMs in generating IFT data for Kazakh. The inter-annotator agreement, measured using Pearson correlation, is high (ranging from 0.68 to 0.70) across correctness, completeness, and fluency, indicating strong reliability in the evaluation process (see Appendix \ref{app:inner-annot-prelim} for further details).

Among the evaluated models, GPT-4o achieved the highest performance across all three criteria. In contrast, LLaMA-3.1 (70B) lagged significantly, scoring nearly 0.8–1 point lower in all aspects. Notably, MURI's quality was lower than GPT-4o despite both relying on OpenAI models. This discrepancy is likely due to MURI’s reliance on machine translation, where Kazakh text is first translated into English before generating instructions, followed by a final back-translation into Kazakh. This multi-step translation process can introduce errors due to cumulative translation inaccuracies. Additionally, MURI is entirely LLM-generated without human validation, further affecting its quality. 


%We additionally evaluated 100 samples, randomly selected from the publicly available MURI~\cite{koksal2024muri} dataset, as a quality baseline, since as mentioned by its authors, no additional cleaning of this dataset was done. 
%Examples of MURI instructions and their issues are given in Appendix \ref{appx:muri}.

%25 GovSet texts and 25 CultSet. Using these texts, we sampled 100 generated IFT pairs.%, ensuring a diverse set of instructions and responses.
%To assess the quality of the generated IFT pairs, we employed a manual evaluation framework where three annotators evaluated each pair based on the following criteria:


\input{tables/preliminary_studies_table}





%The results of this evaluation are summarized in Table \ref{tab:llm-instruction}. 




% More importantly, the Fact Extraction with Reverse Generation approach yielded the best performance across all criteria, making it the preferred method for subsequent experiments.
%This preliminary study helps us identify effective methods for generating high-quality instruction-tuning data tailored to Kazakh cultural and governmental contexts.
%\input{tables/preliminary_studies_table}

\begin{table}[t]
%\scriptsize
\centering
\renewcommand{\arraystretch}{1.2} % Adjust row height
 \resizebox{\linewidth}{!}{
\begin{tabular}{lrr}
\toprule
 & \textbf{CultSet} & \textbf{GovSet} \\ \midrule
Collected text & 4,400 & 1,376 \\ 
Avg. lengths (\#char) of collected text & 245 & 179\\
\cdashline{1-3} % Dashed line

Generated IFT pairs & 4,400 & 6,200 \\
Avg. lengths (\#char) of instruction & 85 & 76\\
% 10.21 & 8.64 \\
Avg. length (\#) of output & 453 & 215\\
% 56.60 & 26.36 \\
\# of unique tokens & 62,449 & 24,304\\
\bottomrule
\end{tabular}}
\caption{Overall statistics of GPT-4o generated IFT dataset.}
\label{tab:ds_stats}
\end{table}




\begin{table}[t]
\small
\centering
\resizebox{0.7\linewidth}{!}{
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lcc}
    \toprule
    \multirow{2}{*}{\textbf{Error Type}} & \multicolumn{2}{c}{\textbf{\% of Questions}} \\
    \cmidrule(lr){2-3}
    & \textbf{CultSet} & \textbf{GovSet} \\
    \midrule
    No error & 28.32\% & 19.47\% \\
    \hdashline
    Wrong language & 0.07\% & 0.14\% \\
    \textbf{Structural} & \textbf{28.45\%} & \textbf{33.58\%} \\
    Grammatical & 25.24\% & 28.73\% \\
    Lexical & 17.92\% & 18.08\% \\
    \bottomrule
\end{tabular}}
\caption{Distribution of error types in GPT-4o-generated IFT data from \texttt{CultSet} and \texttt{GovSet}, identified during manual post-editing.}
\label{tab:error_analysis}
\end{table}


\subsection{Manual Post-Editing}

Given GPT-4o’s strong performance, we use it for large-scale IFT data generation while ensuring quality through full human verification. We employ 12 expert annotators, all native Kazakh speakers with advanced degrees in World Languages, Literature, or Political Science from top Kazakhstani universities. Their extensive experience—having lived in Kazakhstan for over 25 years—equips them with the necessary linguistic and cultural expertise.

To maintain consistency, annotators received detailed guidelines outlining task objectives, evaluation criteria, and examples of high-quality IFT pairs (see Appendix \ref{app:annot-guide-appendix}). They were responsible for manually reviewing and correcting errors in the generated data. Before starting the main annotation process, all candidates completed a pilot task to assess their understanding of project requirements and their ability to refine IFT pairs accurately. Only those who met the evaluation criteria were selected. Each annotator's workload was equivalent to five full working days, and they were compensated fairly based on Kazakhstan’s monthly minimum wage. To accommodate flexibility, annotators were given up to one month to complete the task while working part-time.

Table~\ref{tab:error_analysis} summarizes the error types identified during manual post-editing of GPT-4o-generated data across the two document sources. Annotators found that \texttt{CultSet} had a higher proportion of "No error" cases (28.32\%) compared to \texttt{GovSet} (19.47\%), suggesting variations in data quality.

Structural errors were the most common in both datasets, accounting for over 28\% in \texttt{CultSet} and 33\% in \texttt{GovSet}. These errors involve grammatically correct but poorly structured responses, including issues with logical flow, organization, and unnatural phrasing for a Kazakh speaker. Additionally, grammatical and lexical errors were frequently observed, with annotators noting that GPT-4o occasionally replaces Kazakh words with Russian equivalents, even when the correct Kazakh term is explicitly provided in the original text. For a detailed breakdown of annotator observations, see Appendix~\ref{app:annot-comments}.

%Throughout the process, annotators were asked to document their findings as notes, focusing on patterns of errors they observed in the generated pairs. Annotators observe linguistic issues, inconsistent verb usage, unnatural phrasing, and reliance on Russian borrowings instead of existing Kazakh translations. Grammar errors, such as incorrect sentence structure and case application, are prevalent, along with formatting inconsistencies in names, punctuation, and noun linkage. For more details, in the key findings from their observations, please refer to Appendix~\ref{app:annot-comments}.

%Both CultSet and GovSet exhibit recurring linguistic issues, including inconsistent verb usage, unnatural phrasing, and reliance on Russian borrowings instead of existing Kazakh translations. Grammar errors, such as incorrect sentence structure and case application, are prevalent in both datasets. Additionally, formatting inconsistencies, particularly in names, punctuation, and noun linkage, affect the overall quality.
%However, some differences emerge. CultSet is notably affected by capitalization errors and inconsistent suffix application, whereas GovSet struggles more with active vs. passive constructions, improper word order, and inaccuracies in domain-specific terminology. Moreover, GovSet contains frequent issues with missing or redundant question particles, which are less represented in CultSet.
%Despite these variations, both datasets highlight the need for improved consistency in Kazakh text generation, particularly in reducing Russian borrowings and refining grammatical accuracy.

% \textbf{CultSet.}
% The generated IFT pairs exhibit several recurring issues, including inconsistent verb forms, incorrect use of grammar and sentence structure, inconsistent application of suffixes, capitalization errors, and reliance on Russian borrowings instead of existing Kazakh translations. Additionally, formatting inconsistencies in names, years, and noun linkage, as well as unnatural phrasing, were observed. 

% \textbf{GovSet.}
% The GovSet IFT pairs demonstrate several common issues, including inconsistent verb usage, incorrect case application, and improper use of active versus passive constructions. Errors in sentence phrasing, unnatural word order, and missing or redundant question particles were observed. Additionally, reliance on Russian words over their Kazakh equivalents and inaccuracies in translating specific terminologies were prevalent.
% Formatting inconsistencies in punctuation and improper differentiation between similar words further impacted the quality of the generated data. 


\subsection{Final Data Overview}




%which predominantly covers topics related to Kazakh literature, traditions, and media. It contains information on legal assistance in Kazakhstan, the healthcare system, laws related to real estate, and education  Additionally, it covers different aspects of Kazakh culture, including historical figures, academia, handicrafts, fashion, and more. 
As shown in Table~\ref{tab:ds_stats}, the final dataset consists of 4,400 \texttt{CultSet} and 6,200 \texttt{GovSet} IFT instances, totaling 10,600 high-quality samples. We split the dataset into 90\% training and 10\% test, where the training data is used for full fine-tuning of LLMs, and the test set is used for generation evaluation in our experiments.

Since both \texttt{CultSet} and \texttt{GovSet} are topic-based, we include their respective topics as metadata in the final IFT dataset (see Table~\ref{tab:category_exp_gov} and Table~\ref{tab:category_exp_culture} for topic definitions). Figure~\ref{fig:distribution-all} illustrates the topic distribution of the dataset. The most common topics in \texttt{CultSet} include Kazakh literature, traditions, and media, while \texttt{GovSet} primarily covers legal assistance, the healthcare system, real estate laws, and education in Kazakhstan. Examples of GPT-4o-generated IFT data can be found in Table~\ref{tab:example_govset} and Table~\ref{tab:example_cultset}.

Table~\ref{tab:ds_stats} further highlights a notable difference between the two subsets: the average output length in \texttt{CultSet} is significantly longer and contains more unique tokens than \texttt{GovSet}. This difference stems from the nature of \texttt{GovSet} responses, which are strictly factual and concise, whereas \texttt{CultSet} responses tend to be more diverse and expressive.
 


%\textbf{Appendix}
%The generated IFT pairs exhibit several recurring issues, including inconsistent verb forms (e.g., \foreignlanguage{russian}{\textit{түсіндір}} instead of \foreignlanguage{russian}{\textit{түсіндіріңіз}}) and incorrect use of \foreignlanguage{russian}{\textit{туралы}} in instructions. Phrases such as "\foreignlanguage{russian}{\textit{... қалай сипатталады?}}" imply reliance on external text, requiring adjustment. Russian borrowings (e.g., \foreignlanguage{russian}{\textit{награда}}, \foreignlanguage{russian}{\textit{карьера}}, \foreignlanguage{russian}{\textit{музей}}) are sometimes used despite existing Kazakh translations. Words like \foreignlanguage{russian}{\textit{әртүрлі}} and suffixes like \foreignlanguage{russian}{\textit{-мен/-пен}} are inconsistently applied, and shortened names lack spacing (e.g., \foreignlanguage{russian}{\textit{Ы.Алтынсарин}}). Errors in year formatting (e.g., \foreignlanguage{russian}{\textit{1973-78}}), noun capitalization (e.g., \foreignlanguage{russian}{\textit{ру}}), and verb usage (e.g., improper linkage with \foreignlanguage{russian}{\textit{-ып/-іп}}) persist. Additionally, phrases like \foreignlanguage{russian}{\textit{рөл ойнады}} reflect direct translations from Russian and require correction to \textit{\foreignlanguage{russian}{рөл сомдады}}. The translator overuses \foreignlanguage{russian}{\textit{ол}} and occasionally writes Latin-alphabet words. Missteps such as \foreignlanguage{russian}{\textit{пайдаланады деп түсіндіріңіз}} instead of \foreignlanguage{russian}{\textit{пайдаланатынын түсіндіріңіз}} and confusion with \foreignlanguage{russian}{\textit{жылдары}} formatting remain common. While generally faithful to the original text, these issues necessitate ongoing revision for consistency and grammatical accuracy.


\begin{figure*}[h!]
    \centering
    \begin{minipage}[b]{0.45\textwidth}
        %\centering
        \includegraphics[scale=0.35]{images/wikipedia_topics_distribution.png}
        \caption*{(a) CultSet}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        \includegraphics[scale=0.35]{images/category_gov_barchart.png}
        \caption*{(b) GovSet}
    \end{minipage}
    \caption{Topic distribution of GPT-4 generated IFT dataset in \texttt{CultSet} and \texttt{GovSet}.}
    \label{fig:distribution-all}
\end{figure*}