\section{Related Work}
\subsection{Instruction-Tuning Datasets in English}

%Instruction tuning has become a widely adopted method for improving the instruction-following capabilities of LLMs, as demonstrated in various studies \cite{sanh2021multitask, muennighoff2023crosslingual}. Unlike traditional language modeling, which focuses solely on predicting the next token in a sequence, instruction tuning involves training models on datasets that explicitly pair instructions with appropriate responses. This approach allows models to better interpret and respond to a wide range of user requests, effectively transforming them from passive predictive tools into interactive assistants.

There are three main strategies for creating English instruction-tuning datasets: human-curated datasets, templatized NLP tasks, and synthetic data generation using LLMs.

Human-curated datasets, such as Open Assistant \cite{kopf2023openassistant} and Dolly \cite{conover2023dolly}, rely heavily on human annotation. While this approach ensures high-quality data, it is expensive and difficult to scale across multiple languages. To reduce costs, datasets like the Public Pool of Prompts (P3) \cite{sanh2021multitask}, SuperNatural Instructions \cite{wang2022super}, and FLAN \cite{longpre2023flan} reformat existing NLP tasks into instruction-based formats. However, these datasets primarily focus on specific NLU tasks rather than general-purpose instruction following, limiting their applicability.

Prior work on instruction dataset creation has largely relied on generating data from existing language models without incorporating external real-world knowledge. Self-Instruct \cite{wang2023self} expands an initial set of human-written instructions by iteratively generating new tasks using the model’s own outputs, while \citet{honovich2022unnatural} created instruction-tuning datasets by conditioning on a few example instructions. These methods rely solely on sampling data from language models or predefined topics, rather than grounding them in external knowledge, making them less suitable for capturing domain-specific or culturally relevant understanding. Unlike these approaches, our work focuses on Kazakh, a low-resource language, and constructs an instruction dataset by leveraging external, factual sources such as governmental and cultural texts, ensuring alignment with real-world contexts.

%Synthetic datasets, such as Self-Instruct \cite{wang2023self} and Unnatural Instructions \cite{honovich2022unnatural}, aim to address these limitations by using LLMs to generate tasks, which introduces task diversity. However, this approach requires additional validation.  

\subsection{Instruction Tuning Datasets for Medium- to Low-Resource Languages}


%Another common approach is translating English instruction datasets into other languages using machine translation. \citet{sengupta2023jais} applied this method to develop JAIS, an Arabic-centric language model, by translating various English instruction datasets into Arabic. Similarly, \citet{li2023bactrian} translated Alpaca \cite{taori2023stanford} into 52 languages. While this method is scalable, translation quality remains inconsistent for low-resource languages, as noted by \citet{li2023bactrian}. Additionally, machine-translated datasets often introduce Anglocentric biases, limiting their ability to capture culturally and linguistically diverse perspectives.


%The development of instruction tuning datasets for low-resource languages presents unique challenges due to the scarcity of annotated data, linguistic diversity, and cultural nuances. Traditional approaches, such as human-curated datasets, are often cost-prohibitive and require access to native speakers, limiting their scalability. 

%Recent advancements, such as the CIDAR dataset \cite{CIDAR2024}, highlight the importance of culturally aligned instruction datasets. CIDAR represents the first open Arabic instruction-tuning dataset, containing 10,000 instruction-output pairs that reflect Arabic linguistic structures and cultural norms. Models fine-tuned on CIDAR have demonstrated superior cultural alignment and performance compared to models trained on larger English-centric datasets, underscoring the value of tailored datasets for non-English languages.

While human-curated datasets are often expensive and require native speakers, prior work has explored automatic dataset generation using machine translation for medium- to low-resource languages. \citet{sengupta2023jais} applied this approach to develop JAIS, an Arabic-centric language model, by translating various English instruction datasets into Arabic. Similarly, \citet{li2023bactrian} translated Alpaca \cite{taori2023stanford} into 52 languages. More recently, \citet{CIDAR2024} introduced an Arabic instruction dataset by translating Alpaca into Arabic and performing manual edits and localization to ensure relevance to the Arabic context. Although this method is scalable, the quality of translation remains inconsistent for low-resource languages, as noted by \citet{li2023bactrian}. Additionally, machine-translated datasets often introduce Anglocentric biases, limiting their ability to capture culturally diverse perspectives.

Several frameworks have been proposed to improve instruction tuning for low-resource languages. MURI \cite{koksal2024muri} generates multilingual instruction datasets using reverse instruction generation and translation, but none of its data have been validated by native speakers. \citet{li-etal-2024-x} improve upon translation-based methods by prompting LLMs in English while requiring responses in low-resource languages (e.g., Urdu), allowing models to leverage their internal knowledge of the target language's local context. However, our approach differs in that we do not rely on the model’s internal knowledge but instead use factual information from reliable sources such as government data. Additionally, our dataset undergoes full human validation to ensure accuracy and relevance. Meanwhile, \citet{cahyawijaya2023instructalign} focused on the linguistic aspects of instruction tuning by denoising low-resource language text and prompting models to reconstruct complete sentences. While this enhances fluency, it differs from our approach, which prioritizes grounding instructions in externally verified, domain-specific knowledge rather than refining linguistic quality alone.


%The Multilingual Reverse Instructions (MURI) framework offers an innovative approach to overcoming these challenges \cite{koksal2024muri}. By combining reverse instruction generation and translation pipelines, MURI creates high-quality instruction datasets without requiring pre-trained multilingual models or native annotators. Its dataset, MURI-IT, spans over 200 languages and includes more than 2 million instruction-output pairs. Evaluation results have shown that MURI improves model performance in both natural language understanding and open-ended generation tasks, particularly for low-resource languages.

%Another notable contribution is InstructAlign, which addresses the limitations of adapting high-resource instruction-tuned models to low-resource languages \cite{cahyawijaya2023instructalign}. InstructAlign employs continual crosslingual instruction tuning to align high- and low-resource languages while mitigating catastrophic forgetting. By leveraging alignment-based crosslingual instruction tuning and experience replay, it achieves significant performance improvements for low-resource languages without degrading multitask or multilingual capabilities. Experiments on Indonesian local languages demonstrate that InstructAlign enhances language understanding while preserving zero-shot generalization to related languages.

%X-Instruction \cite{li-etal-2024-x} introduces a novel cross-lingual instruction tuning pipeline to address challenges in low-resource languages. Unlike approaches relying on translations or distillations, which often miss linguistic and cultural nuances, it uses self-curated instruction-output pairs generated through alignment with English instructions. By employing iterative evaluation and clustering, it ensures diverse and culturally relevant data, demonstrating notable performance improvements over existing methods in low-resource language settings.

\subsection{Existing datasets in Kazakh}
While there has been significant progress in developing Kazakh datasets, the majority of high-quality Kazakh datasets are related to speech~\cite{kaz_asr, mussakhojayeva22_interspeech, mussakhojayeva21_interspeech}. In terms of textual data, existing resources primarily focus on question answering and reading comprehension rather than instruction tuning. For instance, KazQAD \cite{kazqad2024} is a Kazakh open-domain question answering (ODQA) dataset that can be used in both reading comprehension and full ODQA settings, as well as for information retrieval experiments. Similarly, Belebele \cite{bandarkar2023belebele} is another dataset that, while useful for multilingual machine reading comprehension, is not explicitly designed for instruction tuning. Belebele covers 122 languages, including Kazakh, and comprises 900 multiple-choice questions associated with 488 distinct passages from the Flores-200 dataset. 


%KazQAD includes nearly 6,000 unique questions with short answers and approximately 12,000 passage-level relevance judgments. To create KazQAD, a combination of machine translation, Wikipedia search, and in-house manual annotation was used, ensuring efficient and high-quality annotation. The questions were sourced from two primary datasets: translated items from the Natural Questions (NQ) dataset, used solely for training, and the original Kazakh Unified National Testing (UNT) exam, which is used for development and testing. The accompanying corpus consists of over 800,000 passages from Kazakh Wikipedia.
% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=\textwidth]{images/workflow.png} % Replace 'filename.png' with your actual file name
%     \caption{Diagram of the Kazakh data collection workflow}
%     \label{fig:label_name} % Replace 'label_name' with a unique label for referencing the image
% \end{figure*}

%Initially, the multiple-choice questions and answers were manually created in English, using passages from Flores. The questions and answers were subsequently translated into other languages and aligned with corresponding passages from Flores-200. Each question offers four answer options, with one correct answer, giving a random guessing accuracy of 0.25. Unlike instruction-tuning datasets, Belebele is solely intended for testing and lacks a training component. 

Despite progress in Kazakh NLP resources, no existing instruction-tuning dataset incorporates cultural or domain-specific knowledge, focuses on real-world applications, and undergoes full human validation. This limits the ability of LLMs to process Kazakh-language instructions effectively in practical and locally relevant contexts, which we aim to address in this work.

%While multilingual instruction datasets such as MURI include Kazakh, they do not include public government data, nor do they ensure human validation of all samples. 


