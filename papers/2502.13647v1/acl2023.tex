% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.

% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[final]{article}

% Remove the "review" option to generate the final version.
\usepackage[final]{ACL2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
%\usepackage[utf8]{inputenc}
%\usepackage{utf8}
%\setcode{utf8}
\usepackage{tikz}
\usepackage{xcolor}% Adjust margins for more space
\usepackage{xcolor}
\usepackage{lipsum} % For dummy text, remove if unnecessary

\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{placeins}

% and will typically save some space.
\usepackage{microtype}
%\usepackage{lmodern} % For modern fonts
\usepackage{listings} 
% This is also not strictly necessary and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{import}
\usepackage{microtype}
\usepackage{layout}
\usepackage{tabularx, makecell}
\usepackage{booktabs}
\usepackage{mathrsfs}
\usepackage{amssymb} 
\usepackage{url}
\usepackage{graphicx}
\usepackage{xspace,paralist}
\usepackage{times,latexsym}
\usepackage{amsmath}
\usepackage{appendix}
\usepackage{comment} 
\usepackage{enumitem}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{tablefootnote}
\usepackage{enumitem}  % More control
\usepackage{lipsum}
\usepackage{xspace}
\usepackage{bbding}
\usepackage{pifont}
\usepackage{arydshln}
\usepackage{array}
\usepackage{cleveref}

\definecolor{darkgreen}{rgb}{0.0, 0.5, 0.0}
%\usepackage[T2A]{fontenc}
%\usepackage[T1]{fontenc}
%\usepackage[T2A]{fontenc}   % Set Cyrillic font encoding

\usepackage[russian,english]{babel}

\newcommand{\eqnref}[1]{Eq~\eqref{#1}\xspace}
\newcommand{\tabref}[2][]{Table#1~\ref{#2}\xspace}
\newcommand{\figref}[1]{Figure~\ref{#1}\xspace}
\newcommand{\secref}[1]{Section~\ref{#1}\xspace}
\newcommand{\appref}[1]{Appendix~\ref{#1}\xspace}

\newcommand{\errortype}[1]{\textit{#1}\xspace}
\newcommand{\ftype}{\errortype{Type1}}
\newcommand{\stype}{\errortype{Type2}}
\newcommand{\ttype}{\errortype{Type3}}

\DeclareMathOperator{\inff}{inf}
\newcommand{\metric}[1]{\textsc{#1}\xspace}
\newcommand{\ece}{\metric{ECE}}
\newcommand{\nlpd}{\metric{NLPD}}
\newcommand{\kl}{\metric{KL}}
\newcommand{\NA}{\textsc{n/a}}
\newcommand{\sd}[1]{\smaller{\ensuremath{\pm {#1}}}}

\newcommand{\dataset}[1]{\text{#1}\xspace}
\newcommand{\nrisk}{\text{61}\xspace}

\newcommand{\model}[1]{\text{#1}\xspace}
\newcommand{\mcdropout}{\model{MC-Dropout}}
\newcommand{\chatgpt}{\model{ChatGPT}}
\newcommand{\gptfour}{\model{GPT-4}}
\newcommand{\instructgpt}{\model{InstructGPT}}
\newcommand{\gptthree}{\model{GPT-3}}
\newcommand{\bard}{\model{Bard}}
\newcommand{\llama}{\model{LLaMA}}
\newcommand{\llamatwo}{\model{LLaMA-2}}
\newcommand{\chatglm}{\model{ChatGLM2}}
\newcommand{\claude}{\model{Claude}}
\newcommand{\alpaca}{\model{Alpaca}}
\newcommand{\vicuna}{\model{Vicuna}}
\newcommand{\longformer}{\model{Longformer}}
\newcommand{\bert}{\model{BERT}}
\newcommand{\repo}{\url{https://github.com/Libr-AI/do-not-answer}}
%\newcommand{\cn}[1]{\begin{CJK*}{UTF8}{gbsn}#1\end{CJK*}}
\usepackage{CJKutf8}
% \newcommand{\repo}{\url{https://anonymous.for.review}}


% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage{float}
\restylefloat{table}

\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets
\usepackage{arydshln}
% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{CJK}
\usepackage[T2A,T1]{fontenc}
\AtBeginDocument{%
  \DeclareFontFamilySubstitution{T2A}{\familydefault}{Tempora-TLF}%
}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% old title
% \title{Instruction Tuning on Public Government and Cultural Data: \\A Case Study in a Low-Resource Language}

% Preslav's title
\title{Instruction Tuning on Public Government and Cultural Data\\ for Low-Resource Language: a Case Study in Kazakh}

%\title{Instruction Tuning for Low-Resource Languages: Enhancing Practical and Cultural Knowledge in Kazakh
%}

\author{
  Nurkhan Laiyk$^1$\thanks{\hspace{0.2cm}These authors contributed equally.}  \quad 
  Daniil Orel$^{1*}$ \quad  
  Rituraj Joshi$^2$ \quad  
  Maiya Goloburda$^1$ \\  
  \textbf{Yuxia Wang}$^1$ \quad  
  \textbf{Preslav Nakov}$^1$ \quad  
  \textbf{Fajri Koto}$^1$ \\  
  \textsuperscript{1}Department of Natural Language Processing, MBZUAI \\  
  \textsuperscript{2}Cerebras Systems \\  
  \texttt{\small \{nurkhan.laiyk,daniil.orel\}@mbzuai.ac.ae} \\
}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}


\usepackage{graphicx}
\begin{document}
\maketitle

\begin{abstract}

% \todo{Preslav: Why not mention Kazakh in the title? See my proposed title.}
% yes, looks good to me.

Instruction tuning in low-resource languages remains underexplored due to limited text data, particularly in government and cultural domains. To address this, we introduce and open-source a large-scale (10,600 samples) instruction-following (IFT) dataset, covering key institutional and cultural knowledge relevant to Kazakhstan. Our dataset enhances LLMs' understanding of procedural, legal, and structural governance topics.
We employ LLM-assisted data generation, comparing open-weight and closed-weight models for dataset construction, and select GPT-4o as the backbone. Each entity of our dataset undergoes full manual verification to ensure high quality. We also show that fine-tuning Qwen, Falcon, and Gemma on our dataset leads to consistent performance improvements in both multiple-choice and generative tasks, demonstrating the potential of LLM-assisted instruction tuning for low-resource languages.

% Instruction tuning in low-resource languages remains underexplored due to the scarcity of text data, particularly in domains such as government and culture. Public government data, while valuable for real-world applications, is often difficult to access or underrepresented in existing datasets. Similarly, cultural data plays a crucial role in ensuring that models align with local contexts, yet it remains largely untapped for instruction tuning. In this work, we investigate the adaptation of LLMs to these underrepresented domains, using Kazakh as a case study. Our approach leverages high-quality unlabeled texts to facilitate LLM-assisted dataset creation through a single-prompt method that extracts both factual information and corresponding questions. To ensure data quality, we evaluate multiple LLMs for dataset construction and ultimately select GPT-4o as the backbone. Every instance in our dataset undergoes full manual verification to maintain high standards. Our dataset consists of 10,600 instruction-following (IFT) pairs, and we assess the impact of instruction tuning by fine-tuning Qwen, Falcon, and Gemma on our dataset, observing consistent performance improvements in both multiple-choice and generative tasks.


%Instruction tuning has emerged as a prominent method for training Large Language Models (LLMs) to effectively interpret and follow instructions. However, most existing instruction datasets are heavily focused on English or developed using LLMs biased toward English-speaking contexts, often resulting in a Western cultural perspective. This cultural bias can affect the syntactic and semantic accuracy of non-English languages, such as Kazakh, which has unique grammatical structures and cultural nuances. To address this limitation, we present "Instruction Tuning for Low-Resource Languages: Enhancing Practical and Cultural Knowledge in Kazakh"â€”an instruction-tuning dataset specifically designed for Kazakh and culturally validated by native speakers.

%For dataset collection, we gathered 4,400 texts from Kazakh Wikipedia and 1,300 from government sources using a combination of manual and automated methods. Additionally, we utilized GPT-4o to generate instruction pairs based on this content, expanding the dataset to a total of 9,300 instruction-following question-answer pairs. These datasets were then fine-tuned on Llama-3.1-8B and Qwen-2.5-7B to better capture the linguistic and cultural nuances of the Kazakh language. Our dataset is publicly available at \url{https://github.com/nurkhanl5/KazakhIFT}.

\end{abstract}



\input{sections/1_introduction}

\input{sections/2_related_work}
\input{sections/3_background}
\input{sections/4_data_creation}

%\input{sections/methodology}
\input{sections/5_dataset_analysis}
\input{sections/5_experiments_evaluation}
% \input{sections/6_discussion}
\input{sections/7_conclusion}

\input{sections/8_limitations}

\bibliography{acl2023}
\bibliographystyle{acl_natbib}
\input{sections/9_appendix}
%\input{tables/prompts_appendix}
%\input{tables/mcq_eval}
%\input{tables/prelim_appendix_rubric}

%\input{tables/annot_example_appendix}
%\input{tables/annot_findings_culture}
%\input{tables/annot_findings_gov}


\input{sections/conversation_example}
%\input{tables/category_explanation}
% \input{tables/category_cultural}
%\input{tables/sampled_df_table}

%\include{tables/cohen_appendix.tex}
%\include{sections/appendix}





\end{document}
