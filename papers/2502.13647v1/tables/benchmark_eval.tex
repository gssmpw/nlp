\begin{table*}[ht!]
\centering
\renewcommand{\arraystretch}{1.9} % Adjust row height
%\resizebox{\textwidth}{!}{
\scalebox{0.75}{
\begin{tabular}{lcccccccccc}
\toprule
\textbf{Model name} & \multicolumn{3}{c}{\textbf{Reverse QA}} & \multicolumn{3}{c}{\textbf{Reverse Instruction Generation}} & \multicolumn{3}{c}{\textbf{Fact Extraction with Reverse Generation}} \\  
\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
& \textbf{Correctness} & \textbf{Completeness} & \textbf{Fluency} 
& \textbf{Correctness} & \textbf{Completeness} & \textbf{Fluency} 
& \textbf{Correctness} & \textbf{Completeness} & \textbf{Fluency} \\ 
\midrule
 Llama 3.1 - 70B & 0.57 & 0.69 & 3.77 & 0.41 & 0.72 & 3.12 & 0.50 & 0.65 & 3.45 \\
GPT-4o & \textbf{0.61 $\uparrow$} & \underline{\textbf{0.65} $\downarrow$} & \textbf{3.84 $\uparrow$} 
& \textbf{0.47 $\uparrow$} & \textbf{0.69 $\downarrow$} & \textbf{3.23 $\uparrow$} 
& \textbf{0.53 $\uparrow$} & \textbf{0.62 $\downarrow$} & \textbf{3.52 $\uparrow$} \\ 
Claude\\
Gemini 1.5
\\
\midrule
\end{tabular}}
\caption{Comparison of Fine-tuning Conditions Across Models}
\label{tab:finetuning-comparison}
\end{table*}