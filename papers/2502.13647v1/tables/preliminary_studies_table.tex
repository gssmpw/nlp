%\begin{table*}[htbp!]
%\centering
%\renewcommand{\arraystretch}{1} % Adjust row height
%%\resizebox{\textwidth}{!}{
%\scalebox{0.75}{
%\begin{tabular}{lcccccccccc}
%\toprule
%\textbf{Model name} & \multicolumn{3}{c}{\textbf{Reverse Instruction Generation}} & \multicolumn{3}{c}{\textbf{Fact Extraction with Reverse Generation}} \\  
%\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
%
%
%
%& \textbf{Correctness} & \textbf{Completeness} & \textbf{Fluency} 
%& \textbf{Correctness} & \textbf{Completeness} & \textbf{Fluency} \\ 
%\midrule
% Llama 3.1 - 70B  & 3.45 & 3.21 & 2.87 & 3.54 & 3.45 & 3.07\\
%GPT-4o & \textbf{4.05}  & \textbf{3.97} & \textbf{3.84} 
%& \textbf{4.38 $\uparrow$} & \underline{\textbf{4.29 $\uparrow$}} & \textbf{4.04$\uparrow$}  \\ 
%Claude & 3.63 & 3.41 & 2.98 & 3.74 & 3.48 & 3.09\\
%Gemini 1.5  & 3.54 & 3.53 & 3.2 & 3.85 & 3.64 & 3.32
%\\
%\midrule
%\end{tabular}}
%\caption{Comparison of Fine-tuning Conditions Across Models}
%\label{tab:finetuning-comparison}
%\end{table*}




\begin{table}[t]
\centering
%\renewcommand{\arraystretch}{1} % Adjust row height
 \resizebox{\linewidth}{!}{
    \begin{tabular}{lccc}
    \toprule
    \textbf{Model} & \textbf{Correctness} & \textbf{Completeness} & \textbf{Fluency} \\ 
    %\multicolumn{3}{c}{\textbf{LLM Instruction Generation Analysis}} \\  
    %\cmidrule(lr){2-4} 
    \midrule
    Llama 3.1 (70B)  & 3.54 & 3.45 & 3.07 \\
    Claude & 3.74 & 3.48 & 3.09 \\
    Gemini 1.5  & 3.85 & 3.64 & 3.32 \\
    GPT-4o & \textbf{4.38} & {\textbf{4.29}} & \textbf{4.04}  \\ 
    % \midrule
    \hdashline
    MURI & 3.87 & 3.52 & 3.41\\
    \bottomrule
    \end{tabular} 
} % Close scalebox correctly
\caption{Human evaluation on LLM-generated instruction datasets.}


\label{tab:llm-instruction}
\end{table}

