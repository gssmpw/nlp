\raggedbottom
\begin{abstract}
% Trying  a new edit 

Legal services rely heavily on text processing. While large language models (LLMs) show promise, their application in legal contexts demands higher accuracy, repeatability, and transparency. Logic programs, by encoding legal concepts as structured rules and facts, offer reliable automation, but require sophisticated text extraction. We propose a neuro-symbolic approach that integrates LLMs’ natural language understanding with logic-based reasoning to address these limitations.

As a legal document case study, we applied neuro-symbolic AI to coverage-related queries in insurance contracts using both closed and open-source LLMs. While LLMs have improved in legal reasoning, they still lack the accuracy and consistency required for complex contract analysis. In our analysis, we tested three methodologies to evaluate whether a specific claim is covered under a contract: a vanilla LLM, an unguided approach that leverages LLMs to encode both the contract and the claim, and a guided approach that uses a framework for the LLM to encode the contract. We demonstrated the promising capabilities of LLM + Logic in the guided approach.



%\hl{In contrast, the LLM+Logic approach improved response accuracy by x and consistency by y, while ensuring full explainability and reproducibility, demonstrating its potential for legal applications.}


%Legal services predominantly depend on text processing. While large language models (LLMs) offer promising advancements, their deployment in legal contexts requires higher levels of accuracy, repeatability, and transparency. Alternatively, logic programs — by formalizing legal concepts as structured code through rules and facts — reliably automate legal reasoning but may depend on sophisticated information extraction from texts. We propose a neuro-symbolic approach that combines LLMs' natural language understanding with logic-based reasoning to mitigate drawbacks. 

%Driven by the complexity of health insurance contracts and the consumer need for clarity, we applied neuro-symbolic AI to coverage-related questions, leveraging current closed and open-source LLM base models. We demonstrate that current LLM reasoning capabilities have substantially improved capabilities on this task but still lack the necessary accuracy and consistency to address complex and crucial contract queries. In contrast, the LLM+Logic approach provided on average \hl{x} improvement in response accuracy and \hl{y} improvement in consistency while achieving full explainability and reproducibility compared to a vanilla LLM, highlighting the future potential of the neuro-symbolic approach. 
% TO BE ADDED: conclusion






%Legal services rely mainly on text-processing capabilities, which can enormously benefit from new advancements in large language models (LLM). However, applying LLMs in the legal domain demands higher accuracy, repeatability, and transparency to achieve a transformative impact. Alternatively, logic programs, having proven useful for formally representing legal concepts as structured code, offer a solution to this ambiguity by reliably automating legal reasoning. Since logic programming fundamentally relies on the interplay of rules and facts, developing computable legal reasoning may depend on a complex information extraction process from written documents. A neuro-symbolic AI approach of combining LLMs' natural language capabilities with a logic-based reasoning system could eventually offset LLMs' limiting drawbacks to achieve correct, consistent, and explainable text analysis, generation, and manipulation of legalese. This paper demonstrates how to integrate LLMs with logic programming to enhance their reasoning ability in the legal domain. We aim to demonstrate the effect of prompting LLMs on legal terms, which have been transformed into logic programs compared to applying solely base model LLMs to query specific legal cases. Driven by the complexity of health insurance for consumers and the corresponding business need, we applied neuro-symbolic AI to coverage-related questions, leveraging current closed and open-source LLM base models. We demonstrate that recent advances in LLM reasoning capabilities (e.g., Open AI o1, DeepSeek R1) have substantially improved their capabilities on this task but still lack the necessary accuracy and consistency for the given domain. Our results showed x improvement in response accuracy and y improvement in consistency while achieving full explainability and reproducibility compared to a vanilla LLM, highlighting the potential of the neuro-symbolic AI approach. 
% TO BE ADDED: performance distinction between LLMs, conclusion



%\textbf{Manav draft:} A majority of Americans faced problems with their health insurance contracts over the course of a year, with many of these issues stemming from confusion regarding \emph{what medical care was covered under the contract}. Logic programs, having proven useful for formally representing complex legal contracts as structured code (i.e. \say{computable contracts}), offer a computational solution to this ambiguity by reliably automating the determination of coverage. Since logic programming consists of the establishment of relations between entities, computable contract development is akin to an information extraction task. However, contract complexity and length make this process difficult to scale. LLMs (large language models), on the other hand, offer a scalable contract analysis solution, but are much less reliable than deterministic logic programs. We take a \say{best of both worlds} neuro-symbolic approach, demonstrating a workflow for leveraging LLMs (large language models) to automate the process of encoding health insurance contracts as logic programs, finding that recent advances in reasoning capabilities (e.g. OpenAI o1, DeepSeek R1) have substantially improved their capabilities on this task. We work with a recently deployed computable contract, the CodeX Insurance Analyst, which helps Stanford University students and staff determine coverage on their university-wide Cardinal Care health care plan.


%$3^\text{rd}$ draft:
%Legal services can enormously benefit from new advancements in LLM. However, we believe that achieving transformative legal AI requires more than just language models. Legal reasoning, a key capability of expert attorneys, goes beyond understanding text—it involves systematic analysis, consistent rule application, and accurate, traceable decision-making. This is why we are exploring neuro-symbolic AI, which combines the natural language capabilities of LLMs with logic-based reasoning systems. Driven by the complexity of health insurance for consumers, we applied neuro-symbolic AI to coverage-related questions, leveraging both closed and open-source models. Our results showed XYZ (placeholder: TK%) improvement in response accuracy compared to a vanilla LLM, highlighting the potential of this approach.

%This is an unfortunate reflection on the ever-present challenges faced by non-legal experts in interpreting logically complex, legalese-laden contracts (e.g., privacy policies, building codes, insurance contracts). In this paper, we introduce a neuro-symbolic approach combining LLMs (large language models) with logic programs to develop computational contract analysis capabilities that are broadly and equitably accessible. While LLMs exhibit strong linguistic capabilities, hallucinations and the cost of compute make these tools unideal for contract analysis, a field which requires precise, transparent, reproducible, and cost-effective solutions. Logic programs, on the other hand, which are deterministic (meaning they cannot hallucinate), interpretable, and require little compute, have proven useful for formally representing complex legal contracts as code. The production of these programs (called computable contracts), however, is difficult to scale because of contract complexity and length, as well as the specific expertise required for the task. We take the best of both worlds, leveraging recent advancements in LLM reasoning capabilities (e.g. OpenAI o1-preview onwards) to scale the production of computable contracts. We demonstrate this approach by applying both open- and closed-source LLMs to encode insurance coverages, finding a significant increase in capabilities on this task from traditional LLMs to recent, reasoning-based LLMs.

%We frequently encounter complex insurance contracts, such as health insurance agreements and privacy policies, which are often laden with legalese. This complexity makes it challenging for everyone from end users to claims managers to interpret these documents effectively. With the advancements in large language models (LLMs), there is significant potential to aid in the interpretation of these contracts, particularly in determining the coverage of specific claims. However, deploying AI and LLMs in legal contexts demands consistent and reliable System 2 reasoning. In this paper, we explore a neuro-symbolic approach that combines LLMs with logic programming to improve their reasoning capabilities and convert legal texts into "machine understandable" formats that are easily understood and utilized by humans. We demonstrate this approach by applying both open and closed source LLMs to encode and evaluate three types of insurance contracts.

%A majority of Americans had problems with their health insurance contracts over the course of a year, an unfortunate reflection on the ever-present challenges faced by non-legal experts in interpreting logically complex, legalese-laden contracts (e.g. privacy policies, building codes, insurance contracts). In this paper, we introduce a neuro-symbolic approach combining LLMs (large language models) with logic programs to develop computational contract analysis capabilities that are broadly and equitably accessible. While LLMs exhibit strong linguistic capabilities, hallucinations and the cost of compute make these tools unideal for contract analysis, a field which requires precise, cost-effective solutions. Logic programs, on the other hand, which are deterministic (meaning they cannot hallucinate) and require little compute, have proven useful for formally representing complex legal contracts as code. The production of these programs (called computable contracts), however, is difficult to scale because of contract complexity and length, as well as the specific expertise required for the task. We take the best of both worlds, leveraging recent advancements in LLM reasoning capabilities (e.g. OpenAI o1-preview onwards) to scale the production of computable contracts. We demonstrate this approach by applying both open- and closed-source LLMs to encode insurance coverages, finding a significant increase in capabilities on this task from traditional LLMs to recent, reasoning-based LLMs.

%We frequently encounter complex insurance contracts, such as health insurance agreements and privacy policies, which are often laden with legalese. This complexity makes it challenging for everyone from end users to claims managers to interpret these documents effectively. With the advancements in large language models (LLMs), there is significant potential to aid in the interpretation of these contracts, particularly in determining the coverage of specific claims. However, deploying AI and LLMs in legal contexts demands consistent and reliable System 2 reasoning. In this paper, we explore a neuro-symbolic approach that combines LLMs with logic programming to improve their reasoning capabilities and convert legal texts into "machine understandable" formats that are easily understood and utilized by humans. We demonstrate this approach by applying both open and closed source LLMs to encode and evaluate three types of insurance contracts.

%The costs and complexity of the American judicial system limit access to legal solutions for many Americans. Large language models (LLMs) hold great potential to improve access to justice. However, a major challenge in applying AI and LLMs in legal contexts, where consistency and reliability are crucial, is the need for System 2 reasoning. In this paper, we explore the integration of LLMs with logic programming to enhance their ability to reason, bringing their strategic capabilities closer to that of a skilled lawyer. Our objective is to translate laws and contracts into logic programs that can be applied to specific legal cases, with a focus on insurance contracts. We demonstrate that while GPT-4o fails to encode a simple health insurance contract into logical code, the recently released OpenAI o1-preview model succeeds, exemplifying how LLMs with advanced System 2 reasoning capabilities can expand access to justice.
\end{abstract}