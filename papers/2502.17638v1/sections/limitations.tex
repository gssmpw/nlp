\section{Limitations}

Our current approach addresses only a limited scope and serves as an initial step in a novel directionâ€”combining LLMs and logic programs to form a neuro-symbolic AI for legal analysis. The experiments in this work are constrained in terms of problem space, architectural design, datasets, logic interpreters used, prompt tuning, measurement metrics, and LLMs experimented with.

Our long-term ambition is to apply a neuro-symbolic approach more broadly in the legal domain. Currently, we focus only on health insurance-related coverage questions and answers. Further application areas, such as reasoning about civil and corporate legal terms, remain out of scope but present an exciting direction for future work.

The architectural design for combining LLMs with logic programs is demonstrated solely through LLM-generated logic programs, their execution via logic interpreters, and manual evaluation. This paper does not address post-training fine-tuning, adapter layers, retrieval-augmented generation, knowledge injection, or reinforcement learning, which remain areas for future work.

The experiments process only a narrow set of policies, questions, and answers in terms of data. In future work, a broader range of cases should be explored to gain deeper insights into the performance of the demonstrated approaches. Additionally, we included only a subset of available LLMs in the analysis and focused solely on Prolog as a logic interpreter. Future work should incorporate a wider variety of models and interpreters to enhance generalizability and robustness.

Regarding prompt tuning, we applied only an explicit \textit{Chain-of-logic} Prolog encoding, derived from learning on the Stanford CodeX \textit{Insurance Analyst}. Future work should explore additional encoding strategies, such as self-ask decomposition-based reasoning, iterative refinement, or reinforcement learning with thought tracing.

In this paper, we focused on accuracy and consistency measurements while qualitatively highlighting certain aspects of explainability and auditability without formal evaluation. Future work should incorporate a broader set of metrics to provide a more comprehensive assessment of the performance gains in neuro-symbolic AI designs.