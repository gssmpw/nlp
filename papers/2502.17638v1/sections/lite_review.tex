\section{Related Work}
\subsection{Evaluation of LLM in the Legal Domain}
Recent evaluations of LLMs in the legal domain have revealed promising advances and critical limitations. \citet{blairstanek2025llms} show that state-of-the-art LLMs exhibit considerable output instability when answering legal questions, with models yielding divergent decisions even under controlled settings. In parallel, \citet{hu2025finetuning} address the prevalent issue of hallucinations in legal question answering by proposing a fine-tuning framework that integrates behavior cloning with a sample-aware iterative direct preference optimization strategy, thereby enhancing factual consistency. \citet{peoples2025ai} further underscores that, although LLMs are capable of performing basic legal analysis through a typical chain of thoughts approach such as Issue, Rule, Analysis, and Conclusion (IRAC), their brief and sometimes unreliable outputs raise concerns regarding their adequacy for high-stakes legal reasoning and education. 

Complementing these findings, an evaluation reported in the \textit{Journal of Legal Analysis} \cite{oup2025} highlights persistent transparency, ethical compliance, and reliability challenges when deploying LLMs for legal research in practice. The heterogeneous nature of legal language across different jurisdictions often leads to inconsistencies in model outputs, thereby questioning the ability to generalize with the necessary level of accuracy. The study demonstrated that legal hallucinations are pervasive and disturbing: hallucination rates range from 59\% to 88\% in response to specific legal queries. 

Comprehensively the \textit{LegalBench} benchmark introduced by Guha et al. \cite{legalbench-2023} provides a collaboratively built suite of tasks that systematically measures various facets of legal reasoning, emphasizing the necessity for domain-specific evaluation metrics. These studies illustrate that while LLMs hold potential for legal applications, careful and targeted methodological improvements are essential to ensure their dependable integration into legal practice. Thus, while reported accuracy metrics are encouraging, they must be evaluated alongside limitations in consistency and transparency to assess the actual applicability of LLMs in the legal domain.
%TO BE REVIEWED

%\subsection{Demand for Explainability and Transparency of AI}
%The sensitive nature of legal queries necessitates a system whose decisions are - besides being correct - traceable and interpretable. In the United States, oversight of AI-driven systems is intensifying, with the \citet{BipartisanAI2024} emphasizing in its final report the need for transparency and explainability to prevent deceptive practices and ensure consumer protection. 

%In parallel, sector-specific supervision in the insurance domain, as our case study, is evolving; for example, the \citet{IAIS2024} recently published its Draft Application Paper on the Supervision of AI, which calls for rigorous auditability and interpretability standards for AI-driven contract analytics. Under the General Data Protection Regulation in Europe, data subjects must be provided “meaningful information about the logic” underlying automated decision-making processes \cite{GDPR2016}. This requirement ensures that individuals can understand, challenge, or seek human intervention regarding algorithmic decisions. 

%Similarly, the proposed EU AI Act mandates that high-risk AI systems be designed with explainability and traceability, ensuring stakeholders can reasonably comprehend the system’s functioning and outputs \cite{EUAIAct2023}. These legal perspectives clarify that opaque, black-box AI models are unacceptable in legal contexts.
%TO BE REVIEWED

\subsection{Advances in Neuro-Symbolic AI}
Recent advances in legal language processing have increasingly focused on integrating LLMs with symbolic reasoning to balance the flexibility of neural architectures with the rigor of formal logic. \citet{ssrn5023212} demonstrated that embedding logical rules into neural frameworks can enhance the interpretability and robustness of legal text analysis. \citet{servantez2024} introduced the \textit{Chain of Logic} prompting method, which decomposes legal reasoning into independent logical steps and recomposes them to form coherent conclusions for rule‐based legal evaluation. Similarly,  \citet{InsurLE} presented \textit{InsurLE}. This domain-specific controlled natural language codifies insurance contracts by preserving key syntactic nuances while exposing the underlying formal logic for a computable representation. 

\citet{wei2005} proposed a hybrid neural-symbolic framework that synergizes neural representations with explicit logical rules, thereby improving the rigor of legal reasoning in automated systems. \citet{patil2025} systematically surveyed methods to enhance reasoning in LLMs and highlighted modular reasoning and retrieval-augmented techniques as promising approaches for bolstering logical consistency in legal applications. \citet{colelough2025} provided a comprehensive review of neuro-symbolic AI in the legal domain, identifying substantial progress in learning and inference while noting significant gaps in explainability and understanding derived logic programs. 

\citet{calanzone2024} developed a neuro-symbolic integration approach that enforces logical consistency by incorporating external constraint sets into LLM outputs. \citet{sun2024} introduced a framework that explicitly learns case-level and law-level logic rules to generate faithful and interpretable explanations for legal case retrieval. \citet{tan2024} enhanced LLM reasoning through a self-driven Prolog-based chain-of-thought mechanism that iteratively refines logical inferences in legal tasks. Lastly, \citet{Vakharia_2024} proposed \textit{ProSLM}, a Prolog-based language model that validates LLM outputs against a domain-specific legal knowledge base, ensuring higher factual accuracy and interpretability in legal question answering.

Collectively, these studies chart a clear trajectory toward AI systems that harness the complementary strengths of deep learning and logical inference to address the nuanced challenges inherent in legal reasoning.
%TO BE REVIEWED



