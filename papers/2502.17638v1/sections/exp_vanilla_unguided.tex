\section{Preliminary Experiments}
\label{sec:exp_vanilla_unguided}
In this section, we evaluate a range of state-of-the-art reasoning models to benchmark their capabilities in answering coverage-related `yes/no' claim questions about an insurance policy.

We selected seven LLMs, including O1-preview, DeepSeek-R1, Llama-3.1-405B-Instruct, Claude-3.5-Sonnet, Mistral-Large-Latest, Gemini-1.5-Pro, and GPT-4o-2024-08-06\footnote{We included GPT-4o-2024-08-06 along with all aforementioned state-of-the-art reasoning models to assess its performance in legal and contract analysis tasks, given its strong contextual comprehension and broad reasoning ability.}. These models excel in long-context reasoning, mathematical problem-solving, multi-step reasoning, logical consistency, and following policy rules, making them well-suited for analyzing insurance contracts and legal texts. For all models, we set both the temperature and top-p parameters to $1$.

The insurance contract used in the experiments in this section is the Simplified Chubb Hospital Cash Benefit policy, referred to as Chubb hereafter. The task is to determine whether nine given claims are covered under this insurance policy. The Chubb contract and nine claim queries are provided in Appendix \ref{app:simplified_chubb} and  \ref{app:queries_and_answers}, respectively.

We describe the vanilla LLM approach in \S\ref{sec:vanilla_llm} where we directly ask the LLM to answer `yes/no' claim questions. In Section \ref{sec:exp_unguided}, we task the LLM with generating Prolog encodings of the insurance contract and claim queries, which we then manually evaluate using the help of SWISH Prolog interpreter \cite{swish}.

\subsection{Vanilla LLM Approach} 
\label{sec:vanilla_llm}
We prompt the selected LLMs to answer nine claim questions about the Chubb insurance policy. We then evaluate the performance of these models across 10 trials and report their average accuracies and standard errors in Figure~\ref{fig:vanilla_unguided_chubb}. The prompt used for the LLMs is provided in Appendix \ref{app:prompt_vanilla}.


The results show that models such as Mistral-large-latest, Gemini-1.5-pro, Claude-3.5-sonnet, Llama-3.1-405B-instruct, and GPT-4o-2024-08-06 achieved a consistent accuracy of $0.78$ across all 10 trials, with no variance, even at a temperature setting of $1.0$. The error bars in Figure~\ref{fig:vanilla_unguided_chubb} represent the Standard Error of the Mean (SEM)\footnote{The SEM is calculated by dividing the standard deviation of accuracy scores from 10 trials by the square root of the number of trials.}, indicating the variability of the model across trials.

All models consistently failed to correctly answer questions 5 and 9 (see Appendix \ref{app:queries_and_answers}). Question 5 asks whether a self-harm injury is covered if all other conditions are met, while Question 9 concerns coverage for a police officer injured outside of duty (see Appendix \ref{app:queries_and_answers} for the exact wording). Clause 1.1 of the policy specifies that hospitalization must result from sickness or accidental injury, meaning the claim in Question 5 is not covered. In Question 9, although "Service in the police" is excluded if the injury arises from it, the injury in this case occurred when the officer’s son bit him in the ankle outside of duty. In both cases, the vanilla LLM models struggled to distinguish between being a police officer and being injured outside of service, as well as failing to recognize that punching someone in the face is not classified as sickness or accidental injury.

In contrast, DeepSeek-R1 achieved an average accuracy of $0.81$ with an SEM of $0.02$, correctly answering Question 5 in 3 out of 10 trials, though it still missed Question 9 in all trials. The O1-preview model performed better, achieving an average accuracy of $0.88$ with an SEM of $\pm0.02$. It correctly answered Question 9 in 9 out of 10 trials but failed to answer Question 5 correctly in 9 out of 10 trials.

As observed, the vanilla LLM approach alone cannot provide answers to claim questions with 100\% accuracy and consistency across trials, even when using state-of-the-art reasoning models. Next, we aim to enhance LLMs by leveraging the benefits of logic programming. We will ask them to generate Prolog encodings of the policy and claims and then answer the claim questions by evaluating the generated Prolog encodings.
%
% \begin{figure*}[ht]
%     \centering
%    \includegraphics[width=0.75\textwidth]{Papers-Reasoning/ACL2025/figures/vanilla_unguided_accuracy.pdf}
%    \caption{LLM models' average accuracy. Error bars represent the standard error of the mean across 10 trials. Specific models used are Llama-3.1-405B-instruct, Mistral-large-latest, Gemini-1.5-pro, Claude-3.5-sonnet, GPT-4o-2024-08-06, DeepSeek-R1, and O1-preview.}
%    \label{fig:vanilla_unguided_accuracy}
% \end{figure*}

% \begin{table*}[ht]
% \centering
% \begin{minipage}{0.48\textwidth}
% \centering
% \begin{tabular}{|c|c|}
% \hline
% \textbf{Model} & \textbf{Accuracy $\pm$ SEM} \\ \hline
% Mistral-large-latest & 0.78 ± 0.00 \\ \hline
% Gemini-1.5-pro & 0.78 ± 0.00 \\ \hline
% Claude-3.5-sonnet & 0.78 ± 0.00 \\ \hline
% Llama-3.1-405B-instruct & 0.78 ± 0.00 \\ \hline
% GPT-4o-2024-08-06 & 0.78 ± 0.00 \\ \hline
% DeepSeek-R1 & 0.81 ± 0.02 \\ \hline
% O1-preview & 0.88 ± 0.02 \\ \hline
% \end{tabular}
% \end{minipage}
% \hfill
% \begin{minipage}{0.48\textwidth}
% \centering
% \begin{tabular}{|c|c|}
% \hline
% \textbf{Model} & \textbf{Accuracy $\pm$ SEM} \\ \hline
% Mistral-large-latest & 0.50 ± 0.06 \\ \hline
% Gemini-1.5-pro & 0.56 ± 0.05 \\ \hline
% Claude-3.5-sonnet & 0.73 ± 0.03 \\ \hline
% Llama-3.1-405B-instruct & 0.41 ± 0.04 \\ \hline
% GPT-4o-2024-08-06 & 0.60 ± 0.07 \\ \hline
% DeepSeek-R1 & 0.63 ± 0.03 \\ \hline
% O1-preview & 0.89 ± 0.02 \\ \hline
% \end{tabular}
% \end{minipage}
% \caption{Average accuracy of LLMs on the Chubb insurance claim coverage dataset. The $\pm$ values represent the Standard Error of the Mean (SEM) across 10 trials. Left: Vanilla LLM; Right: Unguided Prolog Generation.}
% \label{tab:vanilla_unguided_chubb}
% \end{table*}
%
\begin{figure*}[ht]
    \centering
    \begin{minipage}{1\textwidth}
    \centering                  \includegraphics[width=0.77\textwidth]{figures/vanilla_unguided_accuracy.pdf}
    \end{minipage}
    
    \begin{minipage}{0.48\textwidth}
    \centering
    \begin{tabular}{|c|c|}
    \hline
    \multicolumn{2}{|c|}{\textbf{Vanilla LLM Approach}} \\ \hline
    Model & Accuracy $\pm$ SEM \\ \hline
    Mistral-large-latest & 0.78 $\pm$ 0.00 \\ \hline
    Gemini-1.5-pro & 0.78 $\pm$ 0.00 \\ \hline
    Claude-3.5-sonnet & 0.78 $\pm$ 0.00 \\ \hline
    Llama-3.1-405B-instruct & 0.78 $\pm$ 0.00 \\ \hline
    GPT-4o-2024-08-06 & 0.78 $\pm$ 0.00 \\ \hline
    DeepSeek-R1 & 0.81 $\pm$ 0.02 \\ \hline
    O1-preview & 0.88 $\pm$ 0.02 \\ \hline
    \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
    \centering
    \begin{tabular}{|c|c|}
    \hline
    \multicolumn{2}{|c|}{\textbf{Unguided Approach}} \\ \hline
    Model & Accuracy $\pm$ SEM \\ \hline
    Mistral-large-latest & 0.50 $\pm$ 0.06 \\ \hline
    Gemini-1.5-pro & 0.56 $\pm$ 0.05 \\ \hline
    Claude-3.5-sonnet & 0.73 $\pm$ 0.03 \\ \hline
    Llama-3.1-405B-instruct & 0.41 $\pm$ 0.04 \\ \hline
    GPT-4o-2024-08-06 & 0.60 $\pm$ 0.07 \\ \hline
    DeepSeek-R1 & 0.63 $\pm$ 0.03 \\ \hline
    O1-preview & 0.89 $\pm$ 0.02 \\ \hline
    \end{tabular}
    \end{minipage}
    
    \caption{LLM models' average accuracy on the Chubb insurance claim coverage dataset. 
    The plot (top) visualizes the models' average accuracy with error bars representing the Standard Error of the Mean (SEM) across 10 trials. 
    The tables (bottom) provide the corresponding raw numerical accuracy values: the left table represents the Vanilla LLM approach, while the right table corresponds to the Unguided Prolog Generation approach.}    
    \label{fig:vanilla_unguided_chubb}
\end{figure*}

\subsection{Unguided LLM-generated Prolog}
\label{sec:exp_unguided}
We prompted the selected LLMs (from \S\ref{sec:exp_vanilla_unguided}) to generate Prolog encodings of the Chubb policy contract and nine claims. We then manually evaluated whether the insurance covered the claims by evaluating the Prolog encodings and using the help of SWISH Prolog interpreter\footnote{The generated encodings often failed to run on SWISH due to its ambiguity. In such cases, we manually reasoned through the policy encodings and evaluated the claims.}. 

This process was repeated for 10 trials. In every trial, each LLM generated a policy encoding from the prompt in Appendix~\ref{app:prompt_unguided}, and then translated nine claim questions (given in \ref{app:queries_and_answers}) into Prolog queries based on the policy encoding. We manually evaluated the policy and claim encodings and recorded the number of correct responses. When the encodings were unambiguous, we confirmed our evaluation using SWISH.
We report average accuracies and the standard error of the mean over these 10 trials in Figure~\ref{fig:vanilla_unguided_chubb} and provide a qualitative analysis of each LLM for this task below.

The O1-preview model achieved an average accuracy of $0.89 \pm 0.02$, slightly improving on its vanilla approach in \S\ref{sec:vanilla_llm}. O1-preview answered Question 9 correctly in all trials, showing improved reliability over the vanilla approach in distinguishing between an injury caused by a son biting the claimant while the claimant was a police officer—an explicitly covered scenario. 

However, similar to its vanilla approach, the O1-preview struggled with Question 5, missing it in 9 out of 10 trials. This question involved self-harm, where the claimant was injured due to a face punch. O1-preview failed to determine whether the scenario fell under an exclusion correctly. While it correctly excluded activities like skydiving, firefighting, and police service (where injuries during these activities are not covered), it failed to identify the primary cause of the claim—whether it was sickness or accidental injury, which are addressed indirectly in the contract. Additionally, O1-preview missed Question 4 in only one trial due to ambiguity in encoding time-based conditions. The vanilla LLM, in comparison, had a slightly lower average accuracy of $0.88 \pm 0.02$, with errors mostly in Question 5, and it consistently failed to answer Questions 9 and 4. 

The DeepSeek-R1 model achieved an average accuracy of $0.63 \pm 0.03$. In several trials, it generated incorrect logic encodings, particularly in how it handled exclusions. For instance, it sometimes treated exclusions as conjunctive conditions (e.g., both general activity exclusions, such as serving as a firefighter, and the age > 80 condition had to hold simultaneously). However, the policy contract (see Appendix \ref{app:simplified_chubb}) specifies that exclusions should be interpreted with an OR operator: coverage is denied if sickness or accidental injury results from a listed activity (e.g., skydiving, military service) or if the claimant is 80 years or older at the time of hospitalization. This misinterpretation led to inaccuracies in claim assessments. Some queries were ambiguous, preventing DeepSeek-R1 from determining a final coverage decision. Compared to O1-preview, which achieved $0.89 \pm 0.02$, DeepSeek-R1 not only had a lower average accuracy ($0.63 \pm 0.03$) but also exhibited more significant variability across trials.

% GPT-4o-2024-08-06 achieved an average accuracy of $5.4 \pm 0.62$. GPT-4o demonstrated errors in encoding policy rules in some trials, and its claim encodings often lacked sufficient information. This led to ambiguity, preventing the determination of essential predicate values required for accurate evaluation. There were frequent instances where a final answer could not be determined due to this ambiguity. Additionally, the model exhibited significant inconsistencies across trials, producing logic encodings of varying quality. In some cases, inaccurate encodings—such as confusion between the claim date and the wellness visit time limit or inconsistent predicate parameters (e.g., passing the claim date instead of the claimant's age to an age exclusion predicate; see Appendix \ref{app:simplified_chubb})—led to a low accuracy of 1 out of 9 correct answers in one trial, while in another, it correctly answered 8 out of 9 questions.

GPT-4o-2024-08-06 achieved an average accuracy of $0.60 \pm 0.07$. GPT-4o demonstrated errors in encoding policy rules in some trials, and its claim encodings often lacked sufficient information. This led to ambiguity, preventing the determination of essential predicate values required for accurate evaluation. Frequently, a final answer could not be determined due to this ambiguity. Additionally, the model exhibited significant inconsistencies across trials, producing logic encodings of varying quality. In some cases, inaccurate encodings—such as confusion between the claim date and the wellness visit time limit or inconsistent predicate parameters (e.g., passing the claim date instead of the claimants' age to an age exclusion predicate; see Appendix \ref{app:simplified_chubb})—led to a low accuracy of 1 out of 9 correct answers in one trial, while in another, it correctly answered 8 out of 9 questions.

The encodings of the remaining models were often ambiguous, resulting in inaccurate responses to several questions. The Mistral-large-latest model had an average accuracy of $0.5 \pm 0.06$, with variability and significant struggles in determining coverage due to ambiguous logic encodings. The Gemini-1.5-Pro model also had an average accuracy of $0.56 \pm 0.05$ and faced logic ambiguity issues. 
The Claude-3.5-Sonnet model achieved an average accuracy of $0.73 \pm 0.03$, with its main challenges primarily in Questions 5 and 9. Finally, the Llama-3.1-405B-instruct model, with an average accuracy of $0.41 \pm 0.04$, faced frequent ambiguities. All these models performed worse than their vanilla versions, which had a consistent accuracy of $0.78$. Overall, the ambiguity and inaccuracy in their encodings led to challenges in accurately responding to claim queries. As the next step, we propose expert-guided Prolog encoding generation in \S\ref{sec:expert-informed} to improve accuracy and consistency in LLMs when answering such claims.

% The \textbf{Mistral-large-latest} model achieved an average accuracy of 4.5 ± 0.56, with substantial variability across trials. The O1-preview model often struggled to determine coverage questions based on Mistral’s encodings, frequently returning “CD” (Could Not Determine) for certain cases due to Mistral’s logic encodings’ ambiguity. This was particularly notable for some questions, such as Q3 and Q8, where ambiguity may have made reasoning more difficult. Performance fluctuated significantly across trials, with a minimum accuracy of 2/9 and a maximum of 8/9, highlighting a lack of consistency. 

% The \textbf{Gemini-1.5-Pro} model achieved an average accuracy of 5.0 ± 0.45. We observe significant variation across trials. O1-preview was unable to respond to some questions due to ambiguity in the logic encodings generated by Gemini-1.5-Pro, particularly for Questions 2 and 5. Additionally, Gemini-1.5-Pro struggled to produce correct responses across multiple questions, performing worse than its vanilla version, which had a consistent accuracy of 7 across trials with no deviation.

% The \textbf{Claude-3.5-Sonnet} model achieved an average accuracy of 6.7 ± 0.26. Claude’s encodings were the least ambiguous so far, as O1-preview successfully returned the final answer using Claude’s encodings, failing to determine a response only twice across all 10 trials for Question 3. The model struggled the most with Question 9, answering incorrectly in 9 out of 10 trials, and with Question 5, which it got wrong in 7 out of 10 trials. It also missed Questions 2, 3, and 4 in 1–2 trials but answered them correctly in the remaining trials. Despite its structured encodings, Claude-3.5-Sonnet performed worse than its vanilla version.

% The \textbf{Llama-3.1-405B-instruct} model achieved an average accuracy of 3.7 ± 0.40, with significant variations across trials. There were also frequent instances of Llama’s encodings being ambiguous, leading to O1-preview failing to determine a final solution. Llama-3.1-405B-instruct performed substantially worse than its vanilla version, which had a mean accuracy of 7.0 with zero standard error, indicating a lack of trial-to-trial variability in the vanilla model. Additionally, its worst-performing trial resulted in only 2/9 correct responses, and even in its best trial, it only achieved 6/9 correct responses, making it one of the least reliable logic encoders observed.








