\section{Conclusion and Future Work}
% \subsection{Conclusion}
% \subsection{Future Work}
We are on the cusp of an exciting era where AI enhances legal access through human-like thinking, such as planning and reasoning. While LLMs show promise, their probabilistic nature, inconsistency, and potential for hallucination make their application in the legal domain risky. 

We propose a neuro-symbolic approach that combines LLMs with logic programming and provide a comparative analysis of a use case involving health insurance coverage questions. First, we experimented with a vanilla LLM approach, without logic programs, prompting the LLM to respond to coverage claims based on the contract. Our key observation is that advancements in foundational models allow a vanilla LLM to reasonably assess whether a claim is covered. However, it lacks full accuracy and consistency, which are essential in legal use cases.

We then prompted LLMs to convert a legal contract into a logical encoding and evaluate coverage queries with the help of a logic interpreter. In the unguided approach, where the LLM received only the contract and claim queries without additional guidance, we observed poor-quality encodings, often exhibiting ambiguity and inaccuracy, though some LLMs performed better than others. 

The results of the unguided approach were worse than those of the vanilla LLM. This poor encoding quality is expected, as these models are not specifically trained for such tasks. To address these issues, we introduced a structured framework containing essential information a human encoder would use, providing guidance to the LLMs. Our findings suggest that this guided approach significantly improves the quality, accuracy, and consistency of the generated encodings.

Beyond using LLMs for logical representations through the unguided and guided methods presented in this work, we propose exploring additional approaches in future work.

Our first proposal involves fine-tuning foundational models with high-quality, human-generated logic encodings. Generating a logic encoding for a legal segment or contract is similar to writing code. However, current foundational models have been trained on significantly more high-quality code than on logic encodings. Fine-tuning with curated logic encodings could improve LLMs’ ability to generate accurate and structured representations.

We see an opportunity to enhance the accuracy of LLM-generated Prolog encodings using agentic AI. This includes automating both LLM-generated logic encoding and evaluation within a Prolog interpreter like SWISH. Our experiments revealed frequent syntax errors in LLM-generated encodings, but this approach enables automatic error detection and correction, improving accuracy. Another approach in this direction involves using multiple LLMs: one to encode legal terms and queries, a cost-efficient model to execute them, and another to evaluate the outcomes. 
However, whether this method guarantees accurate and reliable results remains uncertain.

Our third proposal is to improve LLMs’ ability to generate accurate Prolog encodings by incorporating reinforcement learning (RL) with synthetic data and feedback from a Prolog interpreter. One potential approach is to use RL with reward modeling, where the LLM generates logical encodings, executes them in a Prolog interpreter like SWISH, and receives a reward signal based on correctness, consistency, and execution success. 

This iterative process would enable the model to refine its encoding strategy over multiple training cycles. Additionally, an RL-based framework could optimize generalization by training the model on diverse logical structures, helping it adapt to different legal contracts and reasoning tasks. Combining RL with human-in-the-loop evaluation could further enhance reliability, ensuring that generated encodings align with legal reasoning principles.


