\raggedbottom
\section{Expert-Guided Experiments}

\input{sections/flowchart}

\label{sec:expert-informed}
In what follows, we demonstrate a workflow for leveraging LLMs \emph{through expert guidance} to automate the process of encoding health insurance policies as logic programs (also called computable contracts). 

%This is a \say{best of both worlds} neuro-symbolic approach in which we harness LLMs' rapidly improving reasoning capabilities in developing accurate, interpretable, and verifiable policy representations on which insurees can rely. Some recently released reasoning LLMs supersede their more traditional predecessors on this task, especially for logically complex coverage rules.

 We prompted LLMs to encode Prolog rules representing three insurance coverages. The first coverage was the simplified Chubb policy, described in the previous experiment in \S\ref{sec:exp_vanilla_unguided} (see Appendix \ref{app:simplified_chubb}). The latter two have been derived from the Stanford CodeX \textit{Insurance Analyst} \cite{insurance-analyst-2025}, a deployed, expert-encoded computable contract representing the Stanford Cardinal Care Aetna Student Health Insurance Plan \cite{aetna2024} \cite{goodenough2023}. 
 
 Specifically, we evaluated LLMs' ability to encode the \textit{Advanced Reproductive Technology} (ART) and the \textit{Comprehensive Infertility} (CI) coverage rules from the \textit{Insurance Analyst}. Each coverage rule in the \textit{Insurance Analyst} evaluates claims to reach coverage decisions, calling helper rules from other parts of the code base in the process (see Figure \ref{fig:insurance-analyst-functionality}). The LLMs were prompted to encode their own versions of these coverage rules with $1)$ the coverage text from the Cardinal Care policy $2)$ documentation defining a valid claim to the rule, and $3)$ documentation defining the relevant helper rules which can be called from other parts of the code base (see Figure \ref{fig:llm-coverage-rule-generation}, Appendix \ref{app:art-prompt}). The documentation provided to the LLM constitutes guidance given by an expert. We had each LLM generate an encoding of each coverage $5$ times, testing each coverage encoding by querying it with claims and evaluating whether the outputted decisions were correct (see Figure \ref{fig:generated-coverage-rule-testing}).

\subsection{Guided LLM-generated Prolog for a simplified policy}
\label{sec:expert-informed-chubb}
On the Chubb policy, we prompted LLMs with the text of the policy and documentation about the facts provided in any valid claim (e.g., \verb|claim_hospitalization_reason|, \verb|claim_misrepresentation_occurred|) to be used in generating a representative computable contract. Since this policy is stand-alone, its encoding does not need to integrate into a more extensive code base. Thus, no helper rules (from other parts of the code base) needed to be included in the prompt.

Three of the four LLMs performed well on this task (see Table \ref{tab:chubb-simplified}), with each of their $5$ generated encodings perfectly answering all $9$ test queries used for evaluation. These test queries were Prolog translations of the natural language queries used to assess the previous approach (see Appendix \ref{app:queries_and_answers}). DeepSeek-R1, however, produced one encoding with a syntactic error due to an unclosed parenthesis. This, along with some failed test cases in another one of its encodings, resulted in a lower accuracy rate than the other models.

Figure \ref{fig:vanilla_unguided_guided_chubb_comparisons} illustrates a comparison of three approaches—Vanilla LLM, the Unguided approach described in \S\ref{sec:exp_vanilla_unguided}, and the Guided approach described in \S\ref{sec:expert-informed-chubb}—on the Chubb contract using three models: DeepSeek, GPT-4o, and OpenAI o1. As observed, both the Vanilla and Unguided approaches struggled to achieve 100\% accuracy and consistency in all models. In contrast, the Guided approach using GPT-4o and OpenAI o1 models achieved 100\% accuracy with no variations across all trials examined (zero standard error). Among the three models, OpenAI o1 performed best across all approaches.
%
\begin{table}
  \centering
  %\textbf{Simplified Chubb Automated Encoding Evaluation} \\[5pt]
  \begin{tabular}{|c|c|}
    \hline
    Model & Accuracy ± SEM \\
    \hline
    \verb|GPT-4o|     & $1.00 \pm  0.00$         \\ \hline
    \verb|OpenAI o1|     & $1.00 \pm  0.00$          \\ \hline
    \verb|OpenAI o3-mini|     & $1.00 \pm  0.00$          \\ \hline
    \verb|DeepSeek-R1|     & $0.73 \pm  0.17$                   \\\hline
  \end{tabular}
  \caption{Evaluation of the Guided LLM-generated logic program encodings of simplified Chubb contract.}
  \label{tab:chubb-simplified}
\end{table}
%
\begin{figure}[ht]
    \centering
   \includegraphics[angle=0, width=0.48\textwidth]{figures/chubb_all.pdf}
   \caption{Average accuracy for the simplified Chubb contract across three approaches—Vanilla LLM, Unguided, and Guided—and three models, GPT-4o, o1-preview, and DeepSeek-R1, with error bars representing the standard error of the mean across 10 trials.}
\label{fig:vanilla_unguided_guided_chubb_comparisons}
\end{figure}
%
\subsection{Guided LLM-generated Prolog for coverages in a larger policy}
The Stanford Cardinal Care health insurance policy comprises many individual \say{coverages}. For a claim to be covered under the policy, it must be covered under one of these coverages. Thus, while the \textit{Insurance Analyst} has an overarching \say{covered} rule (which should be satisfied exactly when a claim is covered under the Cardinal Care policy), it also contains many rules associated with specific coverages, one of which must be satisfied for the overarching one to be satisfied. The code undergirding the \textit{Insurance Analyst}, written in Epilog (a logic programming language similar to Prolog), is available in its public code repository \cite{insurance-analyst-github-2025}. We used a Prolog-translated version of this code for consistency with our simplified Chubb experiments, asking LLMs to encode the ART and CI coverages, testing the accuracy of these encodings through $20$ test cases from the \textit{Insurance Analyst}'s publicly available code repository.
% We asked LLMs to encode the ART and CI coverages, testing the accuracy of these encodings through $20$ test cases from the \textit{Insurance Analyst}'s publicly available code repository.

OpenAI o1 was substantially more successful at encoding these Cardinal Care coverages than GPT-4o, OpenAI o3-mini, and DeepSeek-R1. As shown in Table \ref{tab:art}, OpenAI o1's ART encodings achieved an average accuracy of $95\%$, far superseding the $50-60\%$ accuracies of the encodings generated by the other models. Similarly, as shown in Table \ref{tab:comprehensive}, OpenAI o1's encodings on CI coverages had an $87\%$ accuracy significantly outperformed that of the other models.

Since the ART and CI coverages are longer and more logically complex than the simplified Chubb policy, they serve as better differentiators of the logical capabilities of the tested LLMs. As an example of the difference in logical correctness between the logic programs written by OpenAI o1 and GPT-4o, consider the following excerpt from the ART coverage:

\begin{quote}
For women $39$ years of age and older, ovarian responsiveness is determined by measurement of day $3$ FSH obtained within the prior $6$ months. For women who are less than $40$ years of age, the day $3$ FSH must be less than $19$ mIU/mL in their most recent laboratory test to use their own eggs. For women $40$ years of age and older, their unmedicated day $3$ FSH must be less than $19$ mIU/mL in all prior tests to use their own eggs.
\end{quote}

Note that there are two age-based boundaries specified in this excerpt. Firstly, women who are at least $39$ years of age must have had an FSH test within the prior $6$ months, whereas this condition does not apply to younger women. Secondly, women who are at least $40$ will have \emph{all FSH tests} past age $40$ examined, whereas younger women will only have the most recent test looked at.

GPT-4o, in its first trial, encoded the FSH criteria in the rule \verb|validate_day_3_fsh(C)| (see Appendix \ref{app:4o-fsh}). This rule correctly checks for the strictness criterion with a boundary at age $40$, but there is no sign of the recency criterion with a boundary at age $39$. By contrast, consider the analogous encoding generated by OpenAI o1 in \emph{its} first trial of the rule \verb|meets_fsh_criteria(C)| (see Appendix \ref{app:o1-fsh}), which correctly delineates \emph{both} age-based boundaries--at age $40$ as well as $39$. Unlike the encoding generated by GPT-4o, it ensures that the most recent FSH test for women who are at least $39$ years of age was conducted no more than $6$ months ago.

This and other examples demonstrate the significant gap in logical ability between OpenAI o1 and GPT-4o, explaining the former's significantly higher accuracy in representing insurance coverages in a logical form.

OpenAI o3-mini and DeepSeek also performed worse than OpenAI o1 on ART due to logical errors and syntactical mistakes. One major issue was the misapplication of the premature ovarian failure (POF) exception. OpenAI o3-mini wrongly applied this exception to \emph{all} women aged $40+$ (not just ones with POF), allowing some to qualify when they should not have. Both models also made syntax errors that prevented their encodings from running. OpenAI o3-mini referenced the nonexistent rule \texttt{day\_3\_fsh\_tests\_since\_age\_40} where it should have been referring to \texttt{day\_3\_fsh\_tests\_since\_age\_40\_in\_claim}, while DeepSeek introduced an unclosed parenthesis, making its Prolog code invalid and thus impossible to evaluate.

Since the CI coverage is even longer and more complex than ART, three of the four models performed worse on encoding this coverage (see Table \ref{tab:comprehensive}). However, OpenAI o1 still led the pack with $87\%$ accuracy rate by producing logically and syntactically superior Prolog encodings. These results show that strong reasoning LLMs such as OpenAI o1 could play a critical role in developing computable contracts that provide reliable, interpretable, and auditable coverage decisions. 

Finally, Figure~\ref{fig:guided_all_coverages_accuracy} consolidates the results of the Guided approach presented in Tables~\ref{tab:chubb-simplified}, \ref{tab:art}, and \ref{tab:comprehensive} for all three contracts—Chubb, ART, and CI. As shown, the OpenAI o1 model outperformed the others across all contracts, achieving 100\% accuracy on Chubb and demonstrating higher accuracy than other models on the more complex contracts, ART and CI.
%
\begin{table}
  \centering
 % \textbf{Advanced Reproductive Technology Automated Encoding Evaluation} \\[5pt]
  \begin{tabular}{|c|c|}
    \hline
    Model & Accuracy ± SEM \\
    \hline
    \verb|GPT-4o|     & $0.56 \pm  0.09$         \\ \hline
    \verb|OpenAI o1|     & $0.95 \pm  0.00$          \\ \hline
    \verb|OpenAI o3-mini|     & $0.58 \pm  0.13$      \\ \hline
    \verb|DeepSeek-R1|     & $0.72 \pm 0.16$
        \\\hline
  \end{tabular}
  \caption{Evaluation of the Guided LLM-generated Cardinal Care ART coverage logic program encodings.}
  \label{tab:art}
\end{table}

\begin{table}
  \centering
  %\textbf{Comprehensive Infertility Automated Encoding Evaluation} \\[5pt]
  \begin{tabular}{|c|c|}
    \hline
     Model & Accuracy ± SEM \\
    \hline
    \verb|GPT-4o|     & $0.37 \pm  0.10$         \\ \hline
    \verb|OpenAI o1|     & $0.87 \pm  0.04$          \\ \hline
    \verb|OpenAI o3-mini|     & $0.72 \pm  0.04$    \\ \hline
    \verb|DeepSeek-R1|     & $0.47 \pm 0.18$
        \\\hline
  \end{tabular}
  \caption{Evaluation of the Guided LLM-generated Cardinal Care CI coverage logic program encodings.}
  \label{tab:comprehensive}
\end{table}

\begin{figure*}[ht]
    \centering
   \includegraphics[angle=0, width=0.7\textwidth]{figures/guided_all_contracts.pdf}
   \caption{LLMs' average accuracy for Chubb, ART, and CI coverages using Guided approach. Error bars represent the standard error of the mean across 10 trials. Models used are Deepseek-R1, GPT-4o, OpenAI o1, and o3-mini.}
   \label{fig:guided_all_coverages_accuracy}
\end{figure*}


% \begin{figure*}[ht]
%     \centering
%    \includegraphics[angle=0, width=0.75\textwidth]{Papers-Reasoning/ACL2025/figures/all_coverages_guided.png}
%    \caption{LLM models' average accuracy for ART and CI coverages. Error bars represent the standard error of the mean across 10 trials. Specific models used are Deepseek-R1, GPT-4o, O1, and O3-mini.}
%    \label{fig:guided_all_coverages_accuracy}
% \end{figure*}
