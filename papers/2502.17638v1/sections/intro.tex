\section{Introduction}
\subsection{Importance of Trustworthy Legal AI}
Legal systems rely on rigorous reasoning, explainability, and transparency to ensure fairness and accountability. Unlike many other AI applications, legal decision-making directly affects individuals' rights, obligations, and access to justice. Consequently, AI-driven legal solutions must go beyond surface-level predictions and provide structured, interpretable reasoning.

Expert attorneys engage in complex reasoning beyond pattern recognition. Legal analysis requires System 2 thinking — deliberate and logical reasoning that evaluates statutes, case law, and contracts. Attorneys dissect legal texts, identify principles, and construct arguments based on precedent. Their decisions involve weighing interpretations, assessing nuances, and considering broader implications. Additionally, legal professionals must articulate their reasoning clearly, ensuring their conclusions are defendable against scrutiny from courts, clients, and the opposition. 

The sensitive nature of legal queries requires a system that is both correct and interpretable. In the U.S., oversight of AI systems is intensifying, with the \citet{BipartisanAI2024} highlighting the need for transparency to prevent deceptive practices and ensure consumer protection. Every legal argument must reference laws, precedents, or contractual clauses, ensuring accountability. Unlike black-box AI, legal reasoning must be auditable, allowing stakeholders to trace conclusions. Without this level of explainability, AI legal tools risk undermining trust and reliability in decision-making.

In parallel, sector-specific supervision in the insurance domain, as in our case study, is evolving; for example, the \citet{IAIS2024} recently published its Draft Application Paper on the Supervision of AI, which calls for rigorous auditability and interpretability standards for AI-driven contract analytics. Under Europe's General Data Protection Regulation, data subjects must be provided “meaningful information about the logic” underlying automated decision-making processes \cite{GDPR2016}. This requirement ensures that individuals can understand, challenge, or seek human intervention regarding algorithmic decisions. Similarly, the proposed EU AI Act mandates that high-risk AI systems be designed with explainability and traceability, ensuring stakeholders can reasonably comprehend the system’s functioning and outputs \cite{EUAIAct2023}. 

As AI increasingly integrates into legal workflows, the need for trustworthy solutions that embody human-like reasoning, transparency, and explainability becomes more critical. AI must assist in analyzing legal texts and provide justifications that align with established legal reasoning practices. The challenge lies in designing AI systems that generate plausible answers and engage in structured, interpretable decision-making, ensuring they can be trusted in high-stakes legal contexts.

%Legal systems rely on rigorous reasoning, explainability, and transparency to ensure fairness and accountability. Unlike many other AI applications, legal decision-making directly affects individuals' rights, obligations, and access to justice. Consequently, AI-driven legal solutions must go beyond surface-level predictions and provide structured, interpretable reasoning.

%Expert attorneys engage in complex reasoning processes that go far beyond pattern recognition. Legal analysis requires System 2 thinking — deliberate, logical, and rule-based reasoning that carefully evaluates statutes, case law, and contractual language. Attorneys break down legal texts, identify underlying principles, and construct arguments based on precedent and legal doctrines. Their decisions are not just about matching facts to rules but involve weighing competing interpretations, assessing nuances, and considering broader implications. Furthermore, legal professionals must articulate their reasoning clearly, ensuring their conclusions are explainable and withstand scrutiny from courts, clients, and opposing parties. 

%Transparency is another cornerstone of legal practice. Every legal argument and decision must be justified with references to laws, precedents, or contractual clauses, ensuring accountability and consistency. Unlike black-box AI models, which often provide opaque responses, legal reasoning must be auditable, allowing stakeholders to trace how a conclusion was reached. Without this level of explainability, AI-driven legal tools risk undermining trust and reliability in legal decision-making.

%As AI increasingly integrates into legal workflows, the need for trustworthy solutions that embody human-like reasoning, transparency, and explainability becomes more critical. AI must assist in analyzing legal texts and provide justifications that align with established legal reasoning practices. The challenge lies in designing AI systems that generate plausible answers and engage in structured, interpretable decision-making, ensuring they can be trusted in high-stakes legal contexts.

%On a KFF (formerly Kaiser Family Foundation) survey of $3,605$ American adults, $58\%$ of insured adults said they had experienced a problem with their health insurance in the past year \citep{kff-2023}. Many reported issues were related to a lack of understanding of what medical care was covered (and to what degree), with $27\%$ complaining that their insurance paid less than they had expected for a bill and $18\%$ that their insurance did not pay for the care they thought was covered. The legal complexity of insurance contracts harms customers in several ways \cite{mcnulty-2024}. A clearer understanding of insurance coverage could help customers in $1)$ deciding which policy (or policies) to buy, $2)$ determining whether medical care would be affordable under their current policies, and $3)$ appealing wrongfully denied coverage. When individuals can comprehend the terms and implications of contracts, they are better equipped to negotiate their rights, avoid exploitative situations, and seek legal redress when necessary. In this sense, fostering contract literacy can significantly empower individuals and enhance overall access to justice. However, legal services that could help to overcome these challenges are often expensive, underscoring the need for cost-effective, efficient computational solutions.
%TO BE REVIEWED

%\subsection{Demand for Explainability and Transparency}
%The sensitive nature of these legal queries necessitates a system whose decisions are - besides being correct - traceable and interpretable. In the United States, oversight of AI-driven systems is intensifying, with the \citet{BipartisanAI2024} emphasizing in its final report the need for transparency and explainability to prevent deceptive practices and ensure consumer protection. In parallel, sector-specific supervision in the insurance domain is evolving; for example, the \citet{IAIS2024} recently published its Draft Application Paper on the Supervision of AI, which calls for rigorous auditability and interpretability standards for AI-driven contract analytics. Under the General Data Protection Regulation in Europe, data subjects must be provided “meaningful information about the logic” underlying automated decision-making processes \cite{GDPR2016}. This requirement ensures that individuals can understand, challenge, or seek human intervention regarding algorithmic decisions. Similarly, the proposed EU AI Act mandates that high-risk AI systems be designed with explainability and traceability, ensuring stakeholders can reasonably comprehend the system’s functioning and outputs \cite{EUAIAct2023}. These legal perspectives clarify that opaque, black-box AI models are unacceptable in legal contexts.
%TO BE REVIEWED

\subsection{Challenges in Legal Text Processing}
Legal services rely mainly on text-processing capabilities, which can enormously benefit from new advancements in large language models (LLM). Several scientific studies and business initiatives have highlighted the potential and limitations of LLMs in the legal domain. Nevertheless, LLM hallucinations have manifested in critical errors, such as generating nonexistent case law citations and misinterpreting contractual provisions. 

A prominent example is \textit{Mata v. Avianca}, where an attorney unknowingly submitted a brief containing fictitious judicial opinions produced by ChatGPT \citep{Aidid2024}. This event underscores the risks of using LLMs without robust verification mechanisms. 

Applying LLMs in the legal domain demands higher accuracy, repeatability, and transparency to achieve a transformative impact. The LLM reasoning abilities have traditionally been too weak to understand the complex logic associated with legal contracts. Considerable progress is still required before these technologies deliver consistent and transparent solutions. 

While human lawyers can articulate the reasoning behind their decisions and strategies, LLMs lack this capability to a sufficient degree. Despite progress in methodologies such as retrieval augmented generation - which guides LLMs to retrieve information from credible sources - hallucinations can and do occur, including for citations in the legal domain \cite{magesh-2024}. The auto-regressive nature of these models, which pushes them into greedily generating responses word-by-word rather than upfront planning, may contribute to this limitation \cite{boraz-2024}. 

The recent release of OpenAI o1, which achieved substantially better results on reasoning-based tasks than its predecessors, can change this situation. The subsequent releases of DeepSeek R1 and OpenAI o3-mini, which achieved similar results to OpenAI o1 at substantially lower costs, have demonstrated the potential for \say{reasoning} LLMs to revolutionize task automation. 

Despite these advancements, LLMs (including reasoning models such as OpenAI o1) still have a penchant for hallucinating on tasks that involve applying and interpreting complex rules. OpenAI o1 achieved a score of $77.6\%$ on LegalBench \cite{legalbench-2023, vals-2024}, a benchmark comprising a diverse set of tasks on various legal domains, leaving much room for improvement. 

\subsection{Proposed Neuro-Symbolic Approach}
Unlike LLMs, logic programs, which have proven helpful for formally representing legal concepts as structured code, offer a solution to this ambiguity by reliably automating legal reasoning. Since logic programming fundamentally relies on the interplay of rules and facts, developing computable legal reasoning may depend on a complex information extraction process from written documents \cite{wang-pan-2020, aitken-2002}. 

A neuro-symbolic AI approach of combining LLMs' natural language capabilities with a logic-based reasoning system could eventually offset LLMs' limiting drawbacks to achieve correct, consistent, and explainable text analysis, generation, and manipulation of legalese. Applying this approach raises new questions about a) architecture - how to combine LLMs with logic programs, b) performance - what is the improvement in accuracy and consistency, and c) explainability - is the reasoning more understandable for humans, compared to plain vanilla LLMs. 

This paper demonstrates how integrating LLMs with logic programming, particularly by prompting LLMs on legal terms transformed into logic programs, could outperform vanilla LLMs on targeted legal queries. Furthermore, we evaluate the performance gain by measuring the effect of prompting LLMs on legal terms transformed into logic programs compared to applying solely LLMs to query specific legal cases.

The described experiments are based on a predefined and validated set of insurance claim coverage questions and answers from two US health insurance policies: $1)$ a simplified Chubb Hospital Cash Benefit Policy (see Appendix \ref{app:simplified_chubb}) and $2)$ more complex, a Stanford Cardinal Care Aetna Student Health Insurance Plan \cite{aetna2024}.

We tested three approaches. In the \textit{vanilla LLM} approach, LLMs answered coverage questions without any guidance on how to derive the answers. In the \textit{unguided} approach, different LLMs were tasked with converting the insurance contract and claims into logic encoding (Prolog). We then determined claim coverage by conducting manual evaluations and utilizing a Prolog interpreter (SWISH). Finally, in the \textit{guided} approach, we provided the LLM with a structured framework containing basic facts and information necessary for logic encoding.

%The described experiments are based on a predefined and validated set of insurance claim coverage questions and answers on US health insurance contracts (Chubb, Aetna reference). We test sets of questions and answers on a)\textit{vanilla LLMs} in whihc we used LLMs to answer the coverage questions without any guidance on how to derive the answers, b)\textit{unguided} approach where we asked different LLMs to covert the insurance contract and the claim to logic encoding (prolog). We, then, used a prolog interpreter (SWISH), to answer the claim coverage, and c)\textit{guided} approach, where we provided a framework to LLM on basic facts and information needed for the logic encoding. 


%WIP




 %Driven by the complexity of health insurance for consumers and the corresponding business need, we applied neuro-symbolic AI to coverage-related questions, leveraging current closed and open-source LLM base models. We demonstrate that recent advances in LLM reasoning capabilities (e.g., Open AI o1, DeepSeek R1) have substantially improved their capabilities on this task but still lack the necessary accuracy and consistency for the given domain.


%Logic programs, on the other hand, are deterministic models which do not hallucinate. Furthermore, their relational nature makes them more conducive to representing complex contracts than more common procedural ones. Effectively, logic programming creates a knowledge base relating relevant entities, making it akin to an information extraction task }.

%Given the multifaceted nature of legal practices, we propose that a combination of probabilistic and deterministic AI solutions is required to effectively address legal planning and reasoning. This raises the next logical question: Which AI algorithms and relational frameworks are most suitable for developing reliable legal assistance? In the following sections of this article, we outline our current approach to addressing these challenges, supported by an experiment that illustrates our findings in the context of contracts. We will then discuss our broader strategy and the future directions we intend to explore.