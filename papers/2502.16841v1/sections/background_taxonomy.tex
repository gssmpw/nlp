\section{Background and Taxonomy}
This section delineates the fundamental principles underlying fairness and foundation models.

\subsection{Fairness}
\textbf{Principles of Trustworthy AI.} The development of medical image analysis AI systems, to achieve trustworthiness, is guided by six fundamental principles: fairness, universality, traceability, usability, robustness, and explainability \cite{lekadir_future-ai_2025}. Among these, fairness, which ensures non-discriminatory outcomes across diverse patient populations, represents a critical determinant of ethical AI implementation. Our analysis centers on fairness and examines its interdependencies with other principles, specifically how it interacts with robustness to ensure reliable system performance and with traceability to maintain systematic accountability, thus demonstrating how these principles collectively contribute to trustworthy AI in clinical settings.

\textbf{Healthcare Disparities.} Differences in national healthcare delivery capacities result in variable health outcomes across populations \cite{world_health_organization_conceptual_2010, chen_ethical_2021}. Within countries that maintain public healthcare systems, disparities persist because of multiple factors, including race, gender, age, ethnicity, body mass index, education, insurance status, and geographic location \cite{bailey_structural_2017, williams_understanding_2019}. These determinants influence an individual's capacity to access treatment, obtain quality care, and achieve favorable health outcomes.

\textbf{AI Bias Manifestation.} AI models frequently assimilate and reproduce biases inherent in their training data, subsequently reflecting and intensifying societal inequalities. These models often depend on spurious correlations, employing computational shortcuts that amplify existing biases \cite{geirhos_shortcut_2020, zou_implications_2023, glocker_algorithmic_2023}. A well-designed fair algorithm produces impartial decisions by ensuring equitable outcomes across demographic groups without discrimination based on sensitive attributes such as race, gender, or age. Despite the implementation of controlled datasets and balanced groups, bias can manifest through various factors, including image complexity and labeling inconsistencies \cite{cui_classes_nodate}. Consequently, unintended biases may emerge even under optimal conditions, highlighting the necessity for continuous evaluation and mitigation procedures to maintain algorithmic fairness.


\textbf{Bias Mitigation Strategies.} Previous research has established three primary classifications for bias mitigation strategies: pre-processing, in-processing, and post-processing \cite{du_fairness_2021, chen_algorithmic_2023, mehrabi_survey_2021, xu_addressing_2024}. Pre-processing approaches modify datasets through demographic representation balancing, sensitive feature removal (such as race and gender), or synthetic data augmentation to enhance diversity \cite{burlina_addressing_2021, noseworthy_assessing_2020, calmon_optimized_2017}. In-processing methods alter the training process by integrating fairness constraints into the loss function or implementing adversarial training to prevent bias acquisition \cite{celis_improved_2019, wissel_investigation_2019, zafar_fairness_2017}. Post-processing techniques adjust model predictions after training through methods such as prediction calibration to satisfy fairness criteria \cite{Kamishima_Fairness-Aware_2012, pleiss_fairness_2017, chouldechova_case_2018}.

\textbf{Framework Integration.} Our research extends traditional bias mitigation strategies (pre-processing, in-processing, and post-processing) into a comprehensive framework encompassing all stages of FMs development. Through the integration of bias mitigation efforts across all phases and the inclusion of policymakers in technical discussions, as illustrated in Figure 1, this framework systematically addresses biases throughout the development process. The approach responds to significant concerns regarding the potential of these models to amplify global economic inequalities. The incorporation of policymakers ensures the integration of ethical considerations and regulatory measures into FMs development, thus mitigating potential adverse effects on global economic disparities.

\subsection{Foundation Models}

\textbf{Technical Foundations.} FMs are trained on extensive datasets that can be efficiently adapted to multiple downstream tasks through fine-tuning, thereby eliminating the necessity of training specialized models from scratch. The Vision Transformer (ViT) \cite{dosovitskiy_image_2021} serves as a leading FMs architecture in computer vision for extracting fundamental image features. The integration of self-supervised learning techniques, such as Masked Autoencoder (MAE) \cite{he_masked_2021} and contrastive learning methods like SimCLR \cite{chen_simple_2020}, enables these models to learn directly from large volumes of unlabeled data, thus largely mitigating the need for costly manual labeling processes. The Image-based Joint-Embedding Predictive Architecture (I-JEPA) introduces a self-supervised learning approach that predicts semantic embeddings directly, offering enhanced efficiency compared to pixel-reconstruction methods \cite{assran_self-supervised_2023}.

\textbf{Capabilities and Applications.} Through the utilization of massive unlabeled datasets, advanced architectures, and self-supervised learning techniques, FMs acquire comprehensive image representations within their training domains. The fine-tuning process facilitates task-specific adaptation through minimal adjustments and limited labeled data requirements. Several key capabilities establish their essential role in scalable, real-world applications: rapid task adaptation, robustness to distribution shifts, and efficient utilization of labeled data \cite{zhou_foundation_2023, azizi_robust_2023, lu_towards_2023, tu_towards_2023}.

\textbf{Multimodal Integration.} These models facilitate training across diverse data types within a unified architectural framework. Vision-Language Models (VLMs) \cite{radford_learning_2021} demonstrate this capability by integrating visual and linguistic processing within a single computational structure. This architectural integration enables simultaneous interpretation and generation of information across multiple modalities, thereby establishing new possibilities for applications that require image and textual data.

\textbf{Domain Specialization.} Domain-specific FMs constitute specialized architectures tailored to address the distinctive characteristics of particular domains, ranging from natural image processing \cite{oquab_dinov2_2024} to medical imaging applications \cite{lu_towards_2023, azizi_robust_2023}. In contrast to general-purpose models, these specialized frameworks concentrate on the unique data structures and analytical challenges inherent to their respective domains. Medical imaging presents a notable example, where image characteristics vary substantially in resolution, ranging from whole-organ visualization to cellular-level structures, with data originating from diverse acquisition modalities including X-ray radiography, computed tomography (CT), magnetic resonance imaging (MRI), and ultrasonography.

\textbf{Medical FMs Categories.}  The inherent diversity of medical imaging modalities presents significant challenges in developing a comprehensive global model. To address this complexity, researchers have established a systematic categorization of medical image FMs into distinct segments: general Medical Image FMs for broad applications, Modality-specific FMs that address particular imaging techniques, Organ-specific FMs that target anatomical structures, and Task-specific FMs that focus on specialized medical procedures such as segmentation or classification \cite{zhang_challenges_2023}.