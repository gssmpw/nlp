\section{Data Documentation}

This section delineates the essential components of data documentation, encompassing both the data creation phase and subsequent curation processes.

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/map.png}
    \caption{\textbf{Global distribution of medical imaging data:} Geographic visualization of dataset volumes across countries (excluding datasets from Table \ref{tab:medical-datasets} that span multiple locations).}
    \label{fig:map}
\end{figure*}

\begin{table*}[t]
\small
\setlength{\tabcolsep}{5pt}
\caption{\textbf{Medical imaging datasets and their characteristics.} Overview of publicly available medical imaging datasets, ordered by size and annotated with key attributes including computational framework, anatomical region, and data accessibility parameters.}
\label{tab:medical-datasets}
\begin{tabular}{l l l l r l l l}
\toprule
Dataset & Model & Region & Modality & \multicolumn{1}{c}{Images} & Demographics & Origin & License \\
\midrule
Moorfields BioResource 001 \cite{health_data_research_innovation_gateway_moorfields_2024} & Vision & Ocular & Multimodal & 26,548,820 & A & UK & Restricted \\
MedTrinity-25M\cite{xie_medtrinity-25m_2024} & Multimodal & Multiple & Multimodal & 25,000,000 & -- & Multiple & Group \\
PMC-15M\cite{zhang_biomedclip_2025} & Multimodal & Multiple & Multimodal & 15,000,000 & -- & Multiple & CC BY-SA \\
BRATS24-MICCAI\cite{verdier_2024_2024} & Vision & Cerebral & MRI & 2,535,132 & -- & USA & CC BY 4.0 \\
PMC-OA\cite{lin_pmc-clip_2023} & Multimodal & Multiple & Multimodal & 1,600,000 & A & Multiple & OpenRAIL \\
RadImageNet\cite{mei_radimagenet_2022} & Vision & Multiple & Multimodal & 1,350,000 & -- & USA & CC BY 4.0 \\
ISIC\cite{zawacki_siim-isic_2020} & Vision & Dermal & Dermoscopy & 1,162,456 & A, S & Multiple & CC BY-NC-SA \\
TCGA\cite{kawai_large-scale_2023} & Vision & Dermal & Histology & 1,142,221 & A, R, S & USA & CC BY-NC-SA \\
HyperKvasir\cite{borgli_hyperkvasir_2020} & Vision & Colon & Endoscopy & 1,000,000 & -- & Norway & CC BY 4.0 \\
BRATS-ISBI\cite{karargyris_federated_2023} & Vision & Cerebral & MRI & 987,340 & -- & Multiple & CC BY 4.0 \\
BHX\cite{reis_brain_nodate} & Vision & Cerebral & MRI & 973,908 & -- & India & CC BY 4.0 \\
LDPolypVideo\cite{ma_ldpolypvideo_2021} & Vision & Colon & Endoscopy & 901,666 & -- & China & -- \\
MIMIC-CXR-JPG\cite{johnson_mimic-cxr-jpg_2019} & Multimodal & Pulmonary & Radiograph & 370,955 & A, R, S & USA & PHDL \\
CheXpert\cite{irvin_chexpert_2019} & Multimodal & Pulmonary & Radiograph & 222,793 & A, R, S & USA & RUA \\
PadChest\cite{bustos_padchest_2020} & Multimodal & Pulmonary & Radiograph & 160868 & A, S & Spain & RUA \\
SUN-SEG\cite{ji_video_2022} & Vision & Colon & Endoscopy & 158,690 & A, S & Japan & MIT \\
NIH-CXR14\cite{wang_chestx-ray8_2017} & Vision & Pulmonary & Radiograph & 112,120 & A, S & USA & CC0 \\
Kermany et al.\cite{kermany_identifying_2018} & Vision & Ocular & OCT & 108,312 & -- & USA & CC BY 4.0 \\
EyePACS\cite{gulshan_development_2016} & Vision & Ocular & Fundus & 92,501 & -- & India & CC BY 4.0 \\
BRAX\cite{reis_brax_2022} & Vision & Pulmonary & Radiograph & 40,967 & A, S & Brazil & PHDL \\
MURA\cite{rajpurkar_mura_2018} & Vision & Multiple & Radiograph & 40,561 & -- & USA & CC BY 4.0 \\
ASU-Mayo\cite{tajbakhsh_automated_2016} & Vision & Colon & Endoscopy & 19,400 & -- & USA & -- \\
BRSET\cite{nakayama_brset_2024} & Vision & Ocular & Fundus & 16,266 & A, S, N & Brazil & PHDL \\
Harvard-FairVLMed\cite{luo_fairclip_2024} & Multimodal & Ocular & SLO & 10,000 & A, R, S, E & USA & CC BY-NC-SA \\
\bottomrule
\multicolumn{8}{p{16cm}}{\footnotesize Demographics: A, Age; R, Race; S, Sex; N, Nationality; E, Ethnicity; --, None reported} \\
\multicolumn{8}{p{16cm}}{\footnotesize Licenses: CC, Creative Commons (BY, Attribution; NC, NonCommercial; SA, ShareAlike); PHDL, PhysioNet Health Data License; RUA, Research Use Agreement} \\
\end{tabular}
\end{table*}

\subsection{Data Creation}
The first step in FMs development involves comprehensive data collection. Healthcare facilities generate medical images through specialized imaging equipment that captures various anatomical structures. The pre-training phase utilizes extensive unlabeled datasets, enabling FMs to acquire general representations and discover underlying patterns without annotation requirements. For subsequent fine-tuning processes, labeled data plays an essential role in enabling models to develop task-specific capabilities through the refinement of learned representations. Both unlabeled and labeled datasets must maintain high quality and diversity standards, as these characteristics fundamentally influence FMs performance and generalization capabilities \cite{deitke_molmo_2024, oquab_dinov2_2024}.

\textbf{Criteria for Inclusion.} Our investigation of large-scale datasets employed in FMs followed a systematic approach, establishing a minimum threshold of 10,000 images for inclusion (Table \ref{tab:medical-datasets}). The geographic distribution of dataset origins appears in Figure \ref{fig:map}. While not exhaustive, this methodological approach enabled the systematic curation of 74,184,415 medical images, representing a comprehensive cross-section of diverse imaging modalities and anatomical regions. The comprehensive review encompassed unimodal imaging and multimodal image-text paired datasets, spanning diverse imaging modalities and anatomical regions. This methodological documentation framework facilitates a systematic assessment of the current medical imaging data ecosystem while elucidating opportunities for enhanced demographic representation in FMs development.

\textbf{Pre-training.} The utilization of large unlabeled datasets for FMs training significantly reduces dependence on costly and time-intensive labeled data collection. Advanced learning methodologies, particularly self-supervised learning approaches \cite{chen_simple_2020, he_masked_2021, assran_self-supervised_2023}, facilitate effective utilization of unlabeled data while minimizing labeling bias. This methodology proves especially valuable in medical contexts, where individual clinician annotation preferences, influenced by patient attributes, may introduce systematic biases \cite{berhane_patients_2015}. Using unlabeled data enables FMs to identify generalizable patterns while simultaneously reducing both cost constraints and annotation-induced biases.

The development of robust datasets for FMs presents significant challenges in ensuring comprehensive representation across populations, imaging modalities, equipment specifications, and disease classifications. The accumulation of large-scale data frequently amplifies existing biases and imbalances that reflect underlying disparities in healthcare access and infrastructure. As illustrated in Figure \ref{fig:map}, the predominant source of available datasets resides in developed nations, which substantially constrains the diversity of documented diseases and patient demographics. This geographic concentration of data resources is particularly evident in regions such as Africa, where large-scale medical imaging datasets remain notably absent.

\textbf{Fine-tuning.} Smaller, more accurate, and more representative datasets are ideal for specializing FMs for specific tasks. This specialization is achieved through supervised learning on labeled datasets. Due to their data efficiency and generalization capabilities \cite{zhou_foundation_2023, azizi_robust_2023, lu_towards_2023, tu_towards_2023}, FMs require less data than training supervised models from scratch. Consequently, fine-tuning FMs offers significant advantages, as it requires fewer computational resources and enables countries and institutions to adapt these models to their specific tasks with greater accessibility.

Precise labels and comprehensive patient information, including gender, age, and race, facilitate the evaluation of model performance and enable systematic bias identification \cite{reis_brax_2022, nakayama_brset_2024, irvin_chexpert_2019, groh_evaluating_2021}. This demographic information proves essential for model evaluation, as it enables more rigorous identification and mitigation of potential biases (discussed in the evaluation section \ref{Model Evaluation}). The availability of labeled data supports dataset balancing across specific attributes, thereby enhancing model fairness and performance. This represents a significant advantage over unlabeled data, which presents greater challenges in achieving these objectives \cite{ashurst_fairness_2023}.

\textbf{Generative Models.} Generative models constitute a driving force behind recent advancements in AI and its applications. These models learn to approximate the probability distribution of output features conditioned on input features, enabling the generation of novel data instances that closely approximate the characteristics of the training data. Advanced techniques, including generative adversarial networks (GANs) \cite{liu_radimagegan_2023}, variational autoencoders (VAEs) \cite{pesteie_adaptive_2019}, and diffusion models \cite{pinaya_brain_2022}, have significantly enhanced the capabilities of generative AI systems.

Data augmentation through generative approaches facilitates both pre-training and fine-tuning stages. These models enable dataset balancing across sensitive attributes through synthetic data generation for underrepresented patient groups while simultaneously enhancing model robustness through the generation of challenging cases \cite{cui_classes_nodate}. Empirical studies demonstrate that fine-tuning models with augmented data improve both fairness and robustness, resulting in enhanced performance across diverse populations \cite{ktena_generative_2023}. However, generative models may inadvertently amplify biases present in training data, such as generating synthetic medical images that systematically underrepresent darker skin tones \cite{zhang_fairskin_2024}, which necessitates robust bias mitigation strategies when utilizing synthetic data for augmentation.

\subsection{Data Curation}
Although increasing data volume in parallel with neural network size can enhance model performance \cite{goyal_self-supervised_2021, goyal_vision_2022}, the relationship between data quantity and model improvement is not consistently linear. Recent research demonstrates that systematic data curation plays a critical role in both natural language processing and computer vision tasks \cite{deitke_molmo_2024, oquab_dinov2_2024}. Ensuring dataset fairness requires careful attention to multiple dimensions: diversity, global representativeness, equipment variability, disease representation, gender balance, and age distribution. Furthermore, data deduplication serves as an essential process for eliminating near-duplicate images, thereby reducing redundancy and enhancing dataset diversity and representativeness \cite{oquab_dinov2_2024, lee_deduplicating_2022}.

\textbf{Sensitive attributes.} The acquisition of metadata and labels in medical imaging presents substantial challenges, primarily due to the resource-intensive nature of manual curation given data volume and complexity. While fairness research frequently presumes the availability of demographic data, such information often remains inaccessible due to legal, ethical, and practical constraints \cite{andrus_demographic-reliant_2022}. This limited access to demographic data creates significant obstacles for both data curation processes and the development of unbiased models \cite{queiroz_using_2025}. Current methodologies attempting to address fairness without demographic metadata face fundamental limitations, including systematic biases, accuracy limitations, technical barriers, and compromised transparency, thus emphasizing the critical need for rigorous evaluation protocols and explicit methodological guidelines \cite{ashurst_fairness_2023}.

\textbf{Scalable methods without sensitive attributes.} FMs serve as highly effective feature extractors, enabling clustering-based approaches for automatic data curation that do not require sensitive attributes. This capability facilitates the enhancement of diversity and balance in large datasets \cite{vo_automatic_2024, queiroz_using_2025}. When compared to uncurated data, features derived from these automatically curated datasets demonstrate superior performance \cite{vo_automatic_2024}. The clustering techniques developed through this approach serve a dual purpose: they not only improve data quality but also provide a systematic method for identifying potential biases in models \cite{queiroz_using_2025}. These findings establish foundation models as versatile tools that excel not only in feature extraction but also in crucial data management tasks, including bias assessment and automated curation.


\textbf{Data curation alone is insufficient.} While data balancing contributes to model fairness, it does not fully eliminate inherent biases in AI systems. The integration of multiple data domains, particularly the combination of textual and visual data, can potentially introduce or intensify biases beyond those present in unimodal models \cite{hall_vision-language_2023, hutchinson_underspecification_2022, booth_bias_2021}. Consequently, achieving fair downstream behavior requires comprehensive mitigation strategies beyond data balancing alone \cite{alabdulmohsin_clip_2024}. In the context of unimodal models, performance disparities in downstream tasks frequently correlate with class difficulty, where more challenging classes exhibit higher misclassification rates and decreased performance metrics \cite{cui_classes_nodate}.