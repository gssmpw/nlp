\section{Enviromental Impact}

This section examines the environmental considerations throughout FMs development, encompassing training requirements, model evaluation protocols, and deployment strategies.

\subsection{Training}
\textbf{Fairness.} Self-supervised learning models leveraging diverse, unlabeled datasets demonstrate enhanced fairness and inclusivity compared to supervised approaches, yielding outcomes characterized by increased robustness and reduced bias \cite{goyal_vision_2022}. However, FMs retain inherent biases despite these advantages \cite{glocker_risk_2023, vaidya_demographic_2024, luo_fairclip_2024}. The mitigation of such biases necessitates targeted interventions within the training loop. This optimization process faces dual challenges: the substantial computational requirements and the inherent difficulty of acquiring large-scale datasets that exclude sensitive attributes.

\textbf{Computational Challenges.} The substantial computational requirements of FMs arise from two primary factors: the necessity of extensive datasets during pre-training and the complexity of architectures involving numerous parameters. These resource demands create significant entry barriers, constraining many institutions to fine-tuning existing pre-trained models rather than developing their own. Additionally, as established in the Data Creation section, this computational divide disproportionately affects certain countries, potentially exacerbating existing biases and societal inequities.

\textbf{Bias Amplification in VLMs.} VLMs face heightened challenges in bias mitigation due to their requirement for concurrent training on both visual and textual data modalities. Contemporary research demonstrates that multimodal training not only maintains existing societal biases \cite{hutchinson_underspecification_2022}, as evidenced in CLIP (Contrastive Language-Image Pretraining) architectures \cite{hall_vision-language_2023}, but also intensifies these biases relative to single-modality systems \cite{booth_bias_2021}.

\subsubsection{Pre-training}
\textbf{Sensitive Attributes.} The computational challenges inherent in vision and language models stem from power-law scaling relationships, wherein incremental performance improvements necessitate exponential increases in computational resources \cite{goyal_self-supervised_2021, goyal_vision_2022}. As established in the Data Creation section, the prevalent absence of metadata in large-scale datasets introduces significant complexities for training loop implementation in pre-trained FMs. These constraints fundamentally limit the application of fairness techniques during the training process. Furthermore, fairness methodologies developed for pre-trained FMs must demonstrate efficient scalability across expansive datasets and sophisticated architectural frameworks.

\textbf{Data selection.} The incorporation of systematic data curation into the training loop can be achieved through active selection strategies, wherein computational priority is assigned to data elements that maximize task performance contributions \cite{schaul_prioritized_2015}. Research demonstrates that small curated models can facilitate the training of larger models through systematic identification of both straightforward and challenging image cases \cite{evans_bad_2024, evans_data_2024}. Implementation of these selection methodologies yields dual benefits: reduced training duration for pre-trained models and enhanced quality of resultant outputs.
 
\textbf{Loss.} The Reducible Holdout Loss (RHO) employs a secondary model to identify three categories of data points: those that are learnable, those worth dedicating computational resources to learn, and those not yet acquired by the model \cite{mindermann_prioritized_2022}. One established approach for enhancing model robustness and fairness involves the integration of sensitive attributes into loss functions, thereby promoting equitable outcomes across demographic groups \cite{mandal_ensuring_2020}. However, as examined in the Data Documentation section, the acquisition of sensitive attributes presents significant practical challenges. To address this limitation, alternative methodologies utilizing clustering-based data curation \cite{vo_automatic_2024} offer potential proxy measures for sensitive attributes \cite{queiroz_using_2025}.

\textbf{Large Concept Models.} Contemporary research in robust and unbiased model development prioritizes the prediction of representations within embedding spaces over traditional token-level forecasting approaches (e.g., word or pixel prediction) \cite{lecun_path_nodate}. Within this framework, a novel class of FMs, termed Large Concept Models (LCM), enables simultaneous training across multiple languages and modalities while maintaining scalability and minimizing bias \cite{team_large_2024}. In the visual domain, the Image-based Joint-Embedding Predictive Architecture (I-JEPA) implements this embedding-centric approach, yielding substantial improvements in robustness, scalability, and computational efficiency relative to Masked Autoencoders (MAE) \cite{assran_self-supervised_2023, littwin_how_2024}.

\subsubsection{Fine-tuning}
\label{sec:fine-tuning}
\textbf{Efficacy of Fine-tuning for Fairness Enhancement.} Fine-tuning serves as a critical mechanism for aligning pre-trained FMs with targeted objectives. These models demonstrate exceptional capability in adapting to novel data distributions with minimal sample requirements \cite{azizi_robust_2023}. Fine-tuning effectively mitigates bias through increased model sensitivity to training distribution, particularly when employing balanced and curated datasets \cite{alabdulmohsin_clip_2024}.

\textbf{Resource Optimization in Fine-tuning.} In contrast to the computationally intensive pre-training phase, fine-tuning procedures can be executed with substantially reduced resource requirements, rendering this approach particularly advantageous for resource-constrained initiatives. The utilization of domain-specific pre-trained FMs, especially those optimized for medical imaging applications, further enhances computational efficiency in settings with limited infrastructure.

\textbf{Parameter-efficient Fine-tuning.} Methodologies such as LoRa \cite{hu_lora_2021} and QLoRa \cite{dettmers_qlora_2023} enhance the efficiency of low-resource fine-tuning through selective parameter modification, targeting only specific subsets within the base model architecture. The impact of these optimization techniques on bias mitigation remains an active area of investigation, with current research providing inconclusive evidence regarding their effects on model fairness \cite{ding_fairness_2024, jin_fairmedfm_2024}. Notably, alternative computational optimization strategies, including model pruning and differentially private training approaches, demonstrate increased bias manifestation within specific demographic subgroups \cite{tran_pruning_2022, bagdasaryan_differential_2019}.

\textbf{Mitigating Bias.} Despite extensive research into bias mitigation strategies within deep learning frameworks, conventional fairness interventions demonstrate variable efficacy when applied to FMs \cite{jin_fairmedfm_2024}. The limitations of these approaches extend beyond FMs applications, with traditional deep learning implementations often achieving only modest improvements in fairness metrics \cite{zong_medfair_2023}. Advanced data augmentation strategies, including AutoAug, Mixup, and CutMix methodologies, demonstrate promise in addressing challenging FMs scenarios \cite{cui_classes_nodate}. Furthermore, the integration of synthetically generated data during the fine-tuning process presents compelling evidence for enhanced fairness outcomes \cite{ktena_generative_2023}.

\textbf{Blackbox FMs.} The increasing commercialization of FMs and proliferation of their APIs has resulted in a distribution model where access is frequently restricted to embedding outputs, limiting direct interaction with model architectures. This constrained accessibility presents distinct challenges for bias mitigation in medical imaging applications. Recent research demonstrates, however, that effective bias elimination techniques can be implemented without requiring access to internal model parameters, offering viable solutions for both open-source and proprietary model frameworks \cite{jin_universal_2024}.


\subsection{Model Evaluation}
\label{Model Evaluation}
\textbf{Fairness Considerations and Metrics.} Within deep learning research, fairness has become a critical consideration, with particular significance in medical imaging applications \cite{chen_algorithmic_2023, xu_addressing_2024, ricci_lara_addressing_2022, shi_survey_2024, vaidya_demographic_2024}. The evaluation of fairness encompasses two fundamental methodological approaches: individual fairness, which requires consistent model outputs for similar inputs, and group fairness, which assesses model performance across demographic categories defined by sensitive attributes such as race and gender. Despite the widespread adoption of group fairness metrics in practical applications, a significant number of influential FMs studies in medical imaging have conducted evaluations without incorporating explicit fairness metrics \cite{azizi_robust_2023, zhou_foundation_2023, lu_visual-language_2024, zhao_foundation_2024}, thus leaving critical questions about potential biases and their clinical implications unexplored.

\textbf{Utility-Fairness Trade-offs.} The pursuit of bias minimization can inadvertently impact primary predictive performance, particularly affecting non-protected demographic groups, thereby introducing considerations of beneficence in model development. Establishing an optimal equilibrium between utility and fairness represents a fundamental challenge in FMs implementation \cite{barocas_fairness_2023}. Contemporary research introduces quantitative methodologies for evaluating these trade-offs within specific prediction tasks and group fairness frameworks \cite{dehdashtian_utility-fairness_2024}. Empirical evidence suggests that the optimization of utility-fairness balance depends predominantly on model adaptation strategies rather than initial model selection; furthermore, certain adaptation approaches demonstrate the potential to concurrently enhance both utility metrics and fairness measures \cite{jin_fairmedfm_2024}.

\textbf{Robustness.} A fundamental challenge in medical imaging applications lies in ensuring that fairness-aware models maintain their equitable performance when transitioning between different data distributions (A to B), particularly given the dynamic nature of population characteristics and deployment contexts \cite{schrouff_diagnosing_2023}. Although adversarial training techniques enhance overall model robustness, the improvements typically manifest unevenly across different classes \cite{xu_be_2021, ma_tradeoff_2022}. Research indicates that fairness enhancement strategies can simultaneously strengthen model robustness \cite{mou_fairness_2024}. However, contemporary deep learning systems frequently demonstrate inadequate robustness to sensitive demographic attributes, specifically sex and gender variables \cite{zong_medfair_2023}. Within FMs frameworks, the direct deployment of models without fine-tuning procedures further compromises robustness to these demographic attributes, thus constraining their efficacy across diverse application scenarios \cite{queiroz_using_2025}.


\textbf{Data-Efficient Generalization.} FMs demonstrate exceptional capabilities in data-efficient generalization, facilitating fine-tuning processes for medical imaging tasks with minimal labeled data requirements \cite{azizi_robust_2023}. This characteristic proves particularly advantageous for resource-constrained institutions implementing model deployment within their specific data environments. Nevertheless, substantial uncertainty persists regarding the implications of limited labeled data usage on fairness metrics in FMs applications. The development of ethical frameworks and bias mitigation strategies assumes critical importance in these contexts, especially given that institutions operating under resource constraints typically depend on restricted labeled data availability.

\textbf{Benchmark.} Benchmarks are essential for assessing fairness in FMs development, providing teams with systematic evaluation tools. While fairness benchmarks exist for deep learning in medical imaging \cite{zong_medfair_2023, zhang_improving_2022, zhou_radfusion_2021, dutt_fairtune_2024}, comprehensive benchmarks and libraries specifically designed for FMs remain lacking \cite{jin_fairmedfm_2024, khan_how_2023}. Developing such resources is crucial for holistic fairness evaluation, encompassing metrics, utility, utility-fairness trade-offs, robustness, and data-efficient generalization. Furthermore, FMs should undergo testing on unbiased data and pipelines, with evaluations conducted on distributions that reflect real-world applications to ensure valid and applicable fairness metrics \cite{longpre_responsible_2024}.

\subsection{Deployment}
\textbf{Documentation.} Comprehensive documentation constitutes a fundamental requirement for the ethical implementation of FMs \cite{longpre_responsible_2024, bommasani_foundation_2023}. The documentation must encompass detailed specifications of training data sources, testing benchmark methodologies, and explicit deployment guidelines that minimize risk and bias \cite{mitchell_model_2019}. Moreover, the documentation should present information in a stratified manner, ensuring accessibility across varying levels of technical expertise while maintaining rigorous detail. This multi-level documentation approach enables stakeholders to comprehend the model's inherent risks, biases, and operational constraints. Such transparency serves as a crucial mechanism for fostering trust and promoting responsible model deployment in clinical settings.

\textbf{License.} Open FMs represent a strategic approach to addressing global bias mitigation, enhancing transparency, and facilitating equitable power distribution \cite{bommasani_considerations_2024, matheny_artificial_2025}. By providing unrestricted access to data, code, and model weights, this approach enables resource-constrained institutions to engage in model development and adaptation, thereby fostering innovation through reduced computational barriers. As established in Section \ref{sec:fine-tuning}, while closed models present substantial impediments to achieving fairness and impartial outcomes, open-weight architectures facilitate superior downstream task adaptation. Nevertheless, ensuring adherence to intended model applications remains crucial for preventing bias propagation and maintaining equity. The implementation of a Responsible AI License framework provides a structured mechanism for guiding ethical and equitable model utilization.

\textbf{Monitoring.} In medical settings, the prevalence of the disease and the distribution of patients accessing a specific hospital can change over time. While assessing fairness and bias during pretraining and fine-tuning is critical for FMs, continuous monitoring of deployed models in real-world scenarios is equally important. Despite its importance, such monitoring is not widely practiced; in the United States, only 44\% of institutions reported conducting local evaluations for bias \cite{nong_current_2025}.