\section{Introduction}
\begin{figure*}[htb]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/overview.png}
    \caption{\textbf{Conceptual framework}. Sequential phases of FMs development that structure this work.}
    \label{fig:overview}
\end{figure*}
AI in medicine offers transformative potential through improving access to diagnostics and enhancing the speed and quality of medical care \cite{moor_foundation_2023, zhang_challenges_2023}. AI tools extend healthcare services, particularly in resource-limited regions, thereby making care more efficient and accessible \cite{widner_lessons_2023}. However, these advancements raise critical ethical concerns that emphasize the need for a fair and equitable distribution of benefits across all populations \cite{rajkomar_ensuring_2018, ricci_lara_addressing_2022, mccradden_ethical_2020}. We must develop and apply these technologies responsibly to uphold bioethical principles of justice, autonomy, beneficence, and non-maleficence to prevent discrimination and promote inclusive healthcare for all \cite{beauchamp_principles_1994}.

Governments worldwide are establishing regulatory frameworks for artificial intelligence across critical sectors. The EU AI Act \cite{european_parliament_and_council_regulation_2024}, the first comprehensive regulation of its kind, has introduced a risk-based classification system for AI applications. In parallel, the U.S. Office for Civil Rights has enacted specific protections against algorithmic bias in healthcare through the Affordable Care Act \cite{united_states_national_archives_and_records_administration_office_of_the_federal_register_act_2010}. These regulatory initiatives reflect a coordinated global effort to implement evidence-based guidelines that ensure the fairness, safety, and equity of AI systems \cite{high-level_expert_group_on_artificial_intelligence_ethics_2019, longpre_responsible_2024, obermeyer_algorithmic_nodate, lekadir_future-ai_2025, vasey_reporting_2022, collins_tripodai_2024, tejani_checklist_2024, liu_reporting_2020}.

FMs serve as essential components in AI by enabling diverse tasks across text, image, video, audio, and other domains through their versatility and scalability \cite{bommasani_opportunities_2022}. Through training on massive datasets, these models capture broad patterns within and across domains. However, significant challenges persist in developing ethical models that can effectively identify and reduce inherent biases \cite{khan_how_2023, glocker_risk_2023, li_empirical_2024}. While bias persists across AI systems, FMs demonstrate significant potential for their mitigation \cite{vaidya_demographic_2024, queiroz_using_2025}, thus creating opportunities for unified models that equitably serve diverse populations while driving greater inclusion in AI.

The production of FMs requires substantial resources, including specialized labor, large datasets, and significant computational infrastructure. These high costs restrict development capabilities to select countries, institutions, and companies, thereby increasing the risk of global inequalities. Stakeholders in regions such as Africa have indicated their inability to develop such models, which potentially widens the disparity between populations that benefit from AI and those that do not \cite{ade-ibijola_artificial_2023}. Addressing this challenge requires developing strategies that facilitate the creation of globally accessible FMs while enabling broad participation in their production.

Bias mitigation must occur throughout the proposed development process of FMs, as illustrated in Figure \ref{fig:overview}. This process requires thorough data documentation during the curation and creation phases to ensure dataset diversity and representativeness. The consideration of environmental impacts during training, model evaluation, and deployment phases enhances fairness. Policymakers fulfill a crucial role through the enactment of laws and the allocation of resources that support equitable AI practices. Through the integration of bias mitigation strategies at each developmental stage, researchers can develop inclusive and responsible AI models that serve diverse populations effectively.

A substantial body of research has empirically demonstrated inherent biases and established initial frameworks for evaluating fairness in these systems \cite{jin_fairmedfm_2024, glocker_risk_2023, khan_how_2023, li_empirical_2024}. While comprehensive investigations of trustworthiness in medical imaging FMs have been conducted \cite{shi_survey_2024}, these studies primarily address fairness as one component within broader trustworthiness considerations. Additionally, existing analyses provide thorough technical perspectives on medical imaging FMs \cite{zhang_challenges_2023, li_progress_2024} but lack systematic examination of fairness considerations. Current reviews of AI fairness in healthcare \cite{chen_algorithmic_2023, du_fairness_2021, mehrabi_survey_2021, ricci_lara_addressing_2022, xu_addressing_2024} have not adequately addressed the distinct challenges posed by FMs. To our knowledge, no study has comprehensively reviewed the field regarding challenges and approaches to ensuring fairness in FMs, underscoring the need for systematic interventions throughout their development lifecycle.

Our contributions are:
\begin{itemize}
\item An investigation of potentials and gaps in bias mitigation methods across all stages of FMs development.
\item A global assessment of dataset distribution patterns reveals inequalities in representation across different regions.
\item An analysis of potential policy interventions for ensuring and promoting the development of more equitable FMs.
\end{itemize}