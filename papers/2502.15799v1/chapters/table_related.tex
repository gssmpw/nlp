\begin{table*}[!t]
\centering

%\scriptsize
 \resizebox{\textwidth}{!}{%
\begin{tabular}{llllllc}\toprule
\textbf{Paper} &\textbf{Models} &\textbf{Methods} &\textbf{Bits Range } &\textbf{Datasets} &\textbf{Evaluation}& \textbf{New Datatset} \\\midrule
\multirow{2}{*}{\citealp{li2024evaluating}} &LLaMA2-7B, LLaMA2-70B, &AWQ$^6$, SmoothQuant$^3$, &W8, W4, W3, W2, W8A8, &\textbf{Ethics}: Adversarial GLUE, & Multiple-choice questions.& \ding{55} \\
&Mistral-7B, Mixtral-8x7B &KV Cache qantization$^1$ &W4A8, W8A4, W4A4 & \textbf{Hallucinations}: TruthfulQA.& \\
\hline

\multirow{2}{*}{\citealp{liu2024evaluating}} &LLaMA2-7B &GPTQ$^4$, SpQR$^2$, AWQ$^6$,  &W2A16, W4A8, W3A8 &\textbf{Toxicity}: Implicit Hate, & Multiple-choice questions.& \ding{55} \\
 &  &SmoothQuant$^3$ &W2A16, W4A8, W3A8 & ToxiGen, BOSS.& \\
\hline

\multirow{2}{*}{\citealp{jin2024comprehensive}} &Qwen-7B-Chat, Qwen-14B-Chat, &SpQR$^2$, GPTQ$^4$,  &W8, W4, W3, W2 &\textbf{Hallucinations}: TruthfulQA, & Multiple-choice questions.& \ding{55} \\
 &Qwen-72B-Chat &LLM.int8()$^2$ & &\textbf{Social biases}: BBQ.&\\

\hline

\multirow{2}{*}{\citealp{belkhiter2024harmlevelbench}}  &Vicuna 13B &AWQ$^6$, GPTQ$^4$ & Not specified & \textbf{Safety}: HarmLevelBench. & Experts and & \ding{52} \\
 &&& &  & LLM-as-a-judge.& \\
\hline

\multirow{4}{*}{\citealp{xu2024beyond}} &LLaMa2-7B, TÜLU2-7B,& LLM.int8()$^2$, GPTQ$^4$, &W8, W4 & \textbf{Toxicity}: RealToxicityPrompts, ToxiGen, & Rule based +\\

& TÜLU2-13B  & AWQ$^6$ & & AdvPromptSet.  \textbf{Bias and Stereotypes:}   &  Model evaluation& \ding{55}    \\

&  & & & BOLD, HolisticBiasR,  BBQ.  & (OpenAI moderation API).& \\

&  & & &  \textbf{Hallucinations}: TruthfulQA.  & &\\

\hline

\multirow{2}{*}{\citealp{yang2024llmcbench}} & LLaMA2, LLaMa3-7B &GPTQ$^4$, SmoothQuant$^3$,  &W8A16, W8A8 & \textbf{Robustness}: AdvGLUE.  & Rule-based.& \ding{55} \\

& &AWQ$^6$, OmniQuant$^1$ & & \textbf{Hallucinations}: TruthfulQA.& \\

\hline

\multirow{3}{*}{\textbf{OUR}} &LLaMa3.1-8B, &AQLM$^1$, QUIK$^1$, &W4, W2 & \textbf{Safety:} XSAFETY, OpenSafetyMini, & Human Evaluation, &  \\
 &Mistral-7B v0.2,& QUIP$^1$, AWQ$^6$ & & SafetyBench.  &  multiple-choice questions,&  \ding{52} \\
 &LLaMa3 Abliterated &  & & \textbf{Hallucinations}: HotPotQA. & AlignScore, LLM as a Judge.& \\



\bottomrule
\end{tabular}}
\caption{Review of previous benchmarks in relation to safety, hallucination, and trustworthiness of quantized LLMs, including \textbf{OUR} contributions. Notation: $W[\cdot]$ - specifies precision for model weights, $A[\cdot]$ specifies precision for model activations (defaults to FP16 if unspecified). Superscript signifies in how many papers a method was evaluated.}
\label{tab:review}
\end{table*}