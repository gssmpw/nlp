\section{Introduction}

The modern state of artificial intelligence (AI) is built on the scaling paradigm, initially focusing on increasing model size~\cite{hoffmann2022trainingcomputeoptimallargelanguage} and later shifting toward test-time compute scaling~\cite{snell2024scalingllmtesttimecompute,geiping2025scalingtesttimecomputelatent}. These paradigms demand substantial computational resources, particularly for inference involving longer meta-reasoning~\cite{gao2024metareasoninglargelanguage}. To democratize access to these computationally expensive systems and enable deployment on local devices while reducing costs and improving efficiency, quantization has emerged as a powerful technique, significantly reducing model size~\cite{lin2024awq,ashkboos2023towards}. 

Quantization maps weight matrices to lower precision, improving computational efficiency by leveraging faster integer arithmetic and reducing memory consumption. However, the performance of quantized models is typically assessed using closed-book benchmarks, with limited evaluation of their safety and trustworthiness. This oversight hinders real-world deployment, as AI systems inherently present safety and reliability challenges, potentially leading to harmful outcomes~\cite{zhang2023safetybench,ren2024safetywashing}. 
%Deploying quantized models without assessment of their safety and trustworthiness poses significant risks, as misaligned AI systems may pursue unintended objectives, potentially leading to harmful outcomes.

Previous studies on the safety evaluation of quantized models primarily focused on older architectures~\cite{li2024salad}, quantization techniques~\cite{xu2024beyond}, and bit ranges~\cite{belkhiter2024harmlevelbench}, as well as outdated datasets that are insufficiently challenging for modern models~\cite{liu2024evaluating,yang2024llmcbench}. Furthermore, existing evaluations rely on either multiple-choice assessments or the LLM-as-a-Judge paradigm~\cite{xu2024beyond}, which may not align well with human judgment~\cite{bavaresco2024llms}.

To address this gap, we introduce a novel challenging dataset \textbf{OpenSafetyMini}, curated with human assessments to enhance specificity in evaluating quantized model performance in open-ended generation. We further demonstrate that the LLM-as-a-Judge approach exhibits high alignment with human judgment.

Finally, we conduct 24 rigorous evaluations by applying 4 state-of-the-art quantization techniques to 2 modern models across 3 precision ranges~\footnote{Here, "precision" refers to the arithmetic precision, encompassing formats from floating-point to integers.}. These evaluations span 4 diverse benchmarks, covering both open-ended and multiple-choice assessments of safety and trustworthiness, with additional human assessments to ensure reliability and alignment with real-world judgment. Our findings reveal that quantized models exhibit unsafe behavior when rigorously tested, with 4-bit precision and 2-bit vector quantization delivering the most safe and trustworthy results.

Our contributions are as follows:
\begin{itemize}
    \item We introduce \textbf{OpenMiniSafety}, a challenging human-curated safety dataset comprising 1,067 questions.
    \item We release a dataset containing human evaluations of safety for 4 models, both quantized and full precision, totaling in 4,268 annotated question-answer pairs.
    \item We demonstrate that {LLM-as-a-Judge} exhibits high agreement with human evaluators in assessing safety.
    \item We conduct a {comprehensive evaluation} of 24 different settings across 4 diverse benchmarks, incorporating human assessments to uncover the impact of modern quantization techniques on safety and trustworthiness. Our results demonstrate model-specific behavior of quantization methods, which highlights importance of the future work in this direction. 
    % that there is no clear winner among methods on evaluated benchmarks, precisions and models highlighting necessity of development safety-aware compression methods. 
\end{itemize}

We make data publicly available.\footnote{\href{https://github.com/On-Point-RND/OpenSafetyMini-Investigating-the-Impact-of-Quantization-Methods-on-the-Safety-and-Reliability-of-LLM/}{repository}}

% Our contributions are as follows: 
% \textbf{Our contribution: } First, we introduce a cleaner version of a previously released dataset for model safety evaluation~\cite{wang2023all}. Since this dataset contains open questions, we perform human evaluation for several settings, including 4000 prompts in total with four possible answers: safe, unsafe, response contains an error, and ambiguous (difficult to determine safe or unsafe). The obtained annotations have a Krippendorff's $\alpha$ of $0.62$ and are used for prompt calibration in the LLM as Judge scenario. We release the annotated dataset. Finally, we extensively evaluate safety of popular quantization methods for 2-bit and 4-bit models on our dataset and SafetyBench~\cite{zhang2023safetybench}.


% \textbf{Potential Risks of LLMs.} While the rapid growth of Large Language Models (LLMs) brings new tools and possibilities to everyday life, it also introduces potential risks such as bias, discrimination, misinformation, harmful content, and legal and ethical concerns. This work focuses on LLM safety, defined as any interaction with an LLM that may cause short-term or long-term harm to users.

% \textbf{Safety and Efficiency Trade-offs.} Significant efforts are being made to ensure LLMs are safe for user interaction, often involving training models with pre-defined safety objectives \cite{wei2024jailbroken, wang2023self, fu2024cross, yuan2024refuse}, even though, some-times unsafe behaviour can not be eliminated~\cite{hubinger2024sleeper}.  Another line of work aims to improve LLMs' computational efficiency through methods like quantization, which makes models faster and less memory-intensive. Unfortunately, these two approaches can conflict, as quantization \cite{egiazarian2024extreme, chee2024quip, ashkboos2023towards, lin2024awq} may degrade model quality, making LLMs more prone to errors and unsafe outputs.

% \textbf{Evaluation of Quantized Models.} Most studies on model quantization assess performance using perplexity metrics and zero-shot benchmarks like MMLU~\cite{hendrycks2021measuringmassivemultitasklanguage}, ARC-e~\cite{Clark2018ThinkYH}, and BoolQ~\cite{Clark2019BoolQET}. These evaluations focus on the accuracy of multi/single choice tasks rather than generating detailed responses, failing to cover all aspects of LLM use.

% \textbf{Assessing LLM Safety.} Ensuring that models do not generate harmful or undesirable outputs with real-world negative consequences is crucial. Existing safety benchmarks validate models across different scenarios \cite{li2024salad, sun2023safety, zhang2023safetybench, li2024salad, sun2023safety, zhang2023safetybench, wang2023all}. 

% \textbf{Our contribution: } First, we introduce a cleaner version of a previously released dataset for model safety evaluation~\cite{wang2023all}. Since this dataset contains open questions, we perform human evaluation for several settings, including 4000 prompts in total with four possible answers: safe, unsafe, response contains an error, and ambiguous (difficult to determine safe or unsafe). The obtained annotations have a Krippendorff's $\alpha$ of $0.62$ and are used for prompt calibration in the LLM as Judge scenario. We release the annotated dataset. Finally, we extensively evaluate safety of popular quantization methods for 2-bit and 4-bit models on our dataset and SafetyBench~\cite{zhang2023safetybench}.



