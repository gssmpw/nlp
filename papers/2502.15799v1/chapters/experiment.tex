% \section{Experiment Set-up}

% In this section, we provide an in-depth overview of our experimental setup, which includes the selection and preparation of models, quantization methods, dataset characteristics, safety prompts, and evaluation criteria. This structured approach ensures a comprehensive analysis of the performance and safety of the quantized models.

% \paragraph{Models}
% In our experiments, we selected two prominent large language models (LLMs) for evaluation: Llama 3.1 8B Instruct and Mistral 7B Instruct v0.2. These models were chosen due to their advanced architectures and their widespread use in various natural language processing tasks. The Llama 3.1 8B Instruct is renowned for its robust performance in instruction-based tasks, while the Mistral 7B Instruct v0.2 is noted for its efficiency and accuracy in handling diverse linguistic inputs. Our goal was to analyze and compare the impact of different quantization methods on these models.

% \paragraph{Quantization}

% We applied three distinct quantization methods to the models to explore their effects on performance and safety. The methods used are: AWQ, QUIK, AQLM and QUIP\#. Each method was decribed in section 3. 

% Add to appendix: GPU devices, total GPU hours, libraries, abliterated model, all prompts

\paragraph{Dataset}
For our experiments, we used the XSAFETY dataset~\cite{wang2023all}, which is specifically designed to test the safety and robustness of language models. In order to create multilingual XSAFETY benchmark, two monolingual safety dataset were translated into the other languages.

The first one is Safety-Prompts~\cite{sun2023safety}, a Chinese comprehensive safety benchmark, which includes 7 typical safety scenarios and 6 instruction attacks. The second dataset called SafeText~\cite{levy-etal-2022-safetext} focuses on commonsense safety in English and requires an understanding of commonsense knowledge to determine if the text could result in unsafe outcomes.

To select the languages for translation, two key criteria were applied. First, languages with a large number of native speakers were prioritized, as this would mean that potentially more people could be impacted by unsafe responses. Second, current large language models should possess sufficient capabilities to engage in conversations in these languages. Based on these guidelines, 10 widely used languages around the world (according to~\cite{wiki:List_of_languages_by_total_number_of_speakers}) were selected:  English (en), Chinese (zh), Hindi (hi), Spanish (es), French (fr), Arabic (ar), Bengali (bn), Russian (ru), Japanese (jp) and German (de). While these are not considered low-resource languages in the real world, they are relatively underrepresented in the pretraining data of existing LLMs. This diverse set of languages allows for a comprehensive evaluation of the models' ability to handle a wide range of linguistic inputs safely and effectively. The researchers first used Google Translate to translate the monolingual data, then recruited professional translators and conducted two rounds of proofreading to ensure the accuracy of the translated data.

In total, the XSAFETY benchmark includes 28,000 sentences with 200 samples for each of the 14 safety scenarios and 10 languages.

\paragraph{Safety prompts}
To assess the models safety, the original XSAFETY paper ~\cite{wang2023all} translates all answers to English and use ChatGPT with the following evaluation prompt, where “[safety]” denotes the safety scenario:

\textit{The following dialogue is Person 1 and Person 2:}

\textit{Person 1: \{Prompt\}}

\textit{Person 2: \{Response\}}

\textit{Under the [safety] scenario, is Person 2’s response safe? Answering starts with “Yes” or “No.”}

The results of our preliminary experiments demonstrated that this universal prompt is unable to evaluate safety across different scenarios. Thus, we employed our own evaluation prompts for each of 14 safety scenario. Each prompt defines what exactly is treated as harmful or sensitive content in a particular scenario. For instance, an unsafe response in the "Crimes And Illegal Activities" scenario would contain illegal and criminal attitudes, behaviors, or motivations, such as incitement to commit crimes, fraud, and rumor propagation. An unsafe response in the "Privacy And Property" scenario would contain information exposing people's privacy and property or providing advice with huge impacts, such as suggestions on marriage and investments. The whole list of elaborated evaluation prompts is shown in table N.

In our study, we used the Llama 3.1 8B Instruct Abliterated to translate model responses into English. This step was crucial for ensuring consistency in evaluation across different languages. Following this, we utilized Gemma 2 27B Instruct to rigorously evaluate the safety of the translated prompts. Gemma 2 27B Instruct, known for its advanced safety evaluation capabilities, provided a thorough analysis of the potential risks and issues in the models' responses.

\paragraph{Evaluation}
In our evaluation process, we focus on several key metrics to assess the performance and safety of the quantized models. The evaluation is structured around three primary aspects: safety accuracy across languages, safety accuracy across different domains, and language confusion.
To gauge the models' ability to handle diverse linguistic inputs safely, we measure \textbf{safety accuracy across languages}. This involves evaluating how well the models maintain safety standards when processing prompts in various languages from the XSAFETY dataset. We calculate safety accuracy by analyzing the proportion of responses that adhere to predefined safety criteria, such as avoiding harmful or sensitive content, across the ten languages included in the dataset: Arabic, Bengali, Chinese, English, French, German, Hindi, Japanese, Russian, and Spanish.

Safety accuracy is also assessed across different \textbf{domains} or categories of prompts. This metric evaluates how the models perform in terms of safety when handling prompts related to various topics, such as medical, legal, or general knowledge. By analyzing the models' responses to prompts from different domains, we can determine their effectiveness in maintaining safety standards in varied contexts. This helps us understand whether the models are consistently reliable across different types of content or if there are specific domains where they may exhibit vulnerabilities.

\textbf{Language confusion} is another critical aspect of our evaluation. This metric measures the extent to which the models' responses exhibit confusion or inaccuracies when processing prompts in different languages. Language confusion is quantified by analyzing discrepancies in the models' performance across languages, such as variations in safety accuracy or the occurrence of errors in translation and understanding. Identifying patterns of confusion helps us assess the models' robustness in multilingual settings and pinpoint areas where improvements may be needed.

By systematically evaluating these metrics, we gain a comprehensive understanding of the models' performance in maintaining safety across diverse languages and domains, as well as their ability to handle multilingual inputs without significant confusion. This evaluation framework provides valuable insights into the effectiveness of the quantization methods applied and the overall reliability of the models.

Add about Hallucainations and its evaluation

\textbf{How do we measure:} LLM as a Judge,  Our entire procedure is depicted in Figure~\ref{fig:scheme}.



\begin{figure}[h] 
    \centering
    \includegraphics[width=\linewidth]{content/schem.pdf} 
    \caption{Experiments setup} 
    \label{fig:scheme} 
\end{figure}

