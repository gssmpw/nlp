\section{Results}


% In this section, we present and analyze the results of our experiments on the safety and trustworthiness of quantized models. 

\input{chapters/safetybench_table}


\subsection{Open-Ended Safety}

In this section, we discuss the safety of open-ended models using {XSafety} and \textbf{OpenSafetyMini} dataset, incorporating human evaluations and LLM-as-a-Judge. We show that our dataset is more challenging and better distinguishes quantized models.


\subsubsection{Human Evaluation}

The results in Table~\ref{tab:safe_human} present human evaluations of safety for LLaMA models. As expected, the Abliterated LLaMA model is the least safe. Notably, QUIK int4 demonstrates strong robustness, with less than a $0.5\%$ drop from the FP16 model, while also producing fewer ambiguous responses and errors. At the same time we observe a lower performance with 2 bit precision for QUIP\#, accompanied by a significant increase in errors. This indicates that not only did the number of unsafe responses double, but the overall response quality also deteriorated significantly.


\subsubsection{Automatic Evaluation}

Results are presented in Table~\ref{tab:llm_judge_safety} for LLaMA and Mistral models for both XSafety and \textbf{OpenSafetyMini}.



\paragraph{Int4.}  
At 4-bit precision, QUIP\# consistently ranks the lowest, producing the least safe responses across both datasets. While QUIK and AWQ perform similarly on \textbf{XSafety}, their behavior diverges on \textbf{OpenSafetyMini}, with AWQ experiencing a larger drop in safety while QUIK maintains nearly the same quality.

Interestingly, this trend is completely reversed for Mistral, where AWQ achieves the safest performance, while QUIK ranks as the most unsafe, showing a much greater drop in safety on \textbf{OpenSafetyMini}.


\paragraph{Int2.}  
For LLaMA at 2-bit precision, we observe the overall stability of vector quantization with AQLM, maintaining safety across both \textbf{XSafety} and \textbf{OpenSafetyMini}, even outperforming some Int4 settings. Meanwhile, QUIP\# continues to show a decline in performance relative to Int4.  

For Mistral, the trend remains generally consistent, though, as with Int4, the performance drop is more pronounced on \textbf{OpenSafetyMini}.



\paragraph{Mistral vs. LLaMA.}  
Our results indicate that LLaMA consistently demonstrates higher safety levels across quantized settings and on \textbf{OpenSafetyMini}. Notably, \textbf{OpenSafetyMini} reveals Mistral’s lower safety even at FP16, highlighting its inherent vulnerabilities.  

While Mistral exhibits a more significant safety drop at 2-bit quantization, the trend is less consistent at 4-bit, where AWQ and QUIP\# experience a larger decline for LLaMA. We hypothesize that this may be related to QUIK’s approach of preserving salient weights from quantization, which could retain some of the LLaMA safety-related capabilities.




\paragraph{OpenSafetyMini Advantages}  
For the LLaMA model, the Abliterated variant consistently yields the lowest safety scores across both \textbf{XSafety} and \textbf{OpenSafetyMini}, with a significantly larger drop on \textbf{OpenSafetyMini}. This trend extends across all settings, where models experience a more pronounced decline in safety ratio on \textbf{OpenSafetyMini}.  

Notably, FP16 performance remains stable for LLaMA, indicating that models subjected to safety alignment do not experience degradation at full precision when evaluated with our dataset. Additionally, by comparing results to FP16, we observe that \textbf{OpenSafetyMini} provides a more fine-grained distinction between quantization methods in terms of safety. For instance, the drop in safety for Int4 LLaMA AWQ highlights its vulnerabilities, while Int4 QUIP\# for Mistral demonstrates a similar trend.  

These findings underscore that \textbf{OpenSafetyMini} enables a more precise assessment of safety in models, offering insights that would be unattainable using \textbf{XSafety} alone.
  

% \begin{table}[!ht]
%     \centering
%     \caption{Human safety evaluation results of Llama-3.1-8B-Instruct model}
%     \begin{tabular}{ccccccc}
%     \toprule
%        Method & \textbf{Unsafe} & \textbf{Safe} & \textbf{Error} & \textbf{Ambiguous} \\ 
%         \midrule
%         FP 16 & 0.0736 & 0.9059 & 0.0069 & 0.0134 \\ 
%         Abliterated & 0.3703 & 0.5867 & 0.0153 & 0.0275 \\ 
%         QUIK 4 bit & 0.0777 & 0.9075 & 0.0052 & 0.0093 \\ 
%         QUIP\# 2 bit & 0.1345 & 0.7881 & 0.0628 & 0.0144 \\ 
%         \bottomrule
%     \end{tabular}
%     \label{tab:safe_human}
% \end{table}




% \begin{table}[!ht]
%     \centering
%      \caption{LLM as a Judge safety evaluation.}
%      \resizebox{\textwidth}{!}{%
%     \begin{tabular}{ccccc|ccccc}
%     \toprule
%      \multicolumn{5}{c}{\textbf{Llama-3.1-8B-Instruct}} & \multicolumn{5}{c}{\textbf{Mistral-7B-Instruct-v0.2}} \\
%      \midrule
%         \textbf{Method} & \textbf{Unsafe} & \textbf{Safe} & \textbf{Error} & \textbf{Ambiguous} & \textbf{Method} & \textbf{Unsafe} & \textbf{Safe} & \textbf{Error} & \textbf{Ambiguous} \\ 
%         \midrule
%         FP 16 & 0.0665 & 0.9306 & 0 & 0.0028 & FP 16 & 0.1499 & 0.8481 & 0 & 0.0018 \\ 
%         Abliterated & 0.3636 & 0.6326 & 0 & 0.0037 & & & &  \\
        
%         AWQ 4 bit & 0.0993 & 0.8950 & 0 & 0.0056 &  AWQ 4 bit & 0.1649 & 0.8313 & 0 & 0.0037 \\ 
%         QUIK 4 bit & 0.0618 & 0.9325 & 0 & 0.0056 &  QUIK 4 bit & 0.1883 & 0.7638 & 0.0243 & 0.0234 \\ 
%         QUIP\# 4 bit & 0.1443 & 0.8444 & 0 & 0.0112 & QUIP\# 4 bit & 0.1977 & 0.7947 & 0 & 0.0065 \\ 
%         QUIP\# 2 bit & 0.1490 & 0.8425 & 0 & 0.0084 & QUIP\# 2 bit & 0.2895 & 0.7010 & 0 & 0.0092 \\ 
%         AQLM 2 bit 1x16 & 0.3786 &	0.4686	& 0.0196	& 0.1330  & AQLM 2 bit 1x16 & 0.2155 & 0.7788 & 0 & 0.0056 \\ 
%         \bottomrule
%         \label{tab:llm_judge_safety}
%     \end{tabular}}
% \end{table}



% -------------------------------------------------

% \begin{table}[!ht]
%     \centering
%     \caption{Results on original XSAFETY on English subset, $\Delta \%$ change from SafetyMini Evaluated with LLM}

%         \begin{tabular}{c|c|c}
%             \toprule
%             \textbf{Method} & \textbf{Llama-3.1-8B-Instruct} & \textbf{Mistral-7B-Instruct-v0.2} \\
%             \midrule
%             & \multicolumn{2}{c}{\textbf{Safe and $\Delta \%$ change  }}  \\ 
%             \midrule
%             FP 16 & 0.8779 \textcolor{red}{\faArrowDown\ -6.0\%} & 0.9011 \textcolor{green}{\faArrowUp\ +5.9\%} \\


%             AWQ 4 bit & 0.8500 $\Delta -5.2\%$ & 0.8896 $\Delta +6.9\%$ \\ 
%             QUIP\# 4 bit & 0.7825 $\Delta -7.9\%$ & 0.7929 $\Delta -0.2\%$\\ 
%             QUIP\# 2 bit & 0.7529 $\Delta -12\%$ & 0.6896 $\Delta -1.6\%$ \\ 
%             AQLM 2 bit 1x16 & 0.5411  $\Delta +13\%$ & 0.4379 $\Delta -78\%$  \\
%             \bottomrule
%         \end{tabular}
    
%     \label{tab:ll_judge_xsafety_full}
% \end{table}





\subsection{Multiple-Choice Safety}
\label{sec:safetybench_res}

The results are presented in Table~\ref{tab:safetybench}, showcasing the performance of various quantized models.  

\paragraph{Int4.}  
Interestingly, Int4 models outperform FP16 models on average for both LLaMA and Mistral. Consistent with open-ended safety evaluations, QUIK demonstrates the strongest performance for LLaMA, while AWQ leads for Mistral, with QUIK being the weakest for Mistral. However, despite its lower overall performance, QUIK for Mistral exhibits notable strengths in Unfairness and Bias, while QUIP\# shows strong capabilities in handling Illegal Activities.  

\paragraph{Int2.}
We observe a significant drop in performance for the LLaMA model under lower precision conditions, while the Mistral model exhibits only a mild decline. Notably, the overall trend remains consistent with open-ended safety benchmarks, where vector quantization methods such as AQLM consistently outperform QUIP\#.


\paragraph{Benchmark Specificity.}  
Overall, we observe differences between quantized and full-precision models on the multi-choice benchmark, which align with the trends identified in open-ended benchmarks. However, we note that this benchmark, while consistent in trends, may lack specificity. Specifically, it fails to distinguish {Abliterated LLaMA} model from regular FP16. We also observe a significant performance improvement for Abliterated, such as in tasks related to {Illegal Activities}. This suggests that marginally reducing a model's safety constraints can enhance its reasoning capabilities on ethically challenging topics. Additionally, we highlight the performance gap between {LLaMA} and {Mistral}, which is only evident in the \textbf{OpenSafetyMini} benchmark and not in \textit{XSafety}. This further underscores the advantages of \textbf{OpenSafetyMini} in capturing nuanced model behaviors.


\subsection{Trustworthiness}

\begin{table}[t!]
    \centering

    \resizebox{0.48\textwidth}{!}{%
        \begin{tabular}{ll cc}
        \toprule
        \textbf{Precision} & \textbf{Method} & \textbf{AlignScore} & \textbf{In-accuracy} \\
        \midrule
        \multicolumn{4}{c}{\textbf{Llama-3.1-8B-Instruct}} \\
        \midrule
        bfloat16 & FP 16 & 0.588 & 0.684 \\
        & Abliterated & 0.444 & 0.587 \\
        \midrule
        int4 & AWQ & 0.599 & 0.672 \\
        & QUIK & 0.550 & 0.655 \\
        & QUIP\# & 0.564 & 0.666 \\
        \midrule
        int2 & QUIP\# & 0.511 & 0.617 \\
        & AQLM & 0.547 & 0.653 \\
        \midrule
        \multicolumn{4}{c}{\textbf{Mistral-7B-Instruct-v0.2}} \\
        \midrule
        bfloat16 & FP 16 & 0.549 & 0.718 \\
        \midrule
        int4 & AWQ & 0.555 & 0.708 \\
        & QUIK & 0.537 & 0.519 \\
        & QUIP\# & 0.540 & 0.698 \\
        \midrule
        int2 & QUIP\# & 0.543 & 0.686 \\
        & AQLM & 0.560 & 0.679 \\
        \bottomrule
        \end{tabular}
        }
    \caption{Evaluation of models trustworthiness with HotPotQA.}
    \label{tab:hotpotqa_results}
\end{table}

We further investigate model trustworthiness, with results detailed in Table~\ref{tab:hotpotqa_results}.

\paragraph{Int4.}
For Int4 models, distinctions are less pronounced compared to safety benchmarks. LLaMA shows different trends, with QUIK underperforming in both automated and rule-based metrics, while QUIP\# delivers acceptable results. For Mistral, QUIK low performance aligns with safety benchmarks, whereas AWQ dominates, showing minimal decline in In-accuracy and even higher AlignScore results.

\paragraph{Int2.}
For Int2 models, LLaMA maintains AQLM stability and higher factuality, while Mistral exhibits a slight decline, with AQLM outperforming QUIP\# only in AlignScore, however, the difference in In-accuracy is negligible.

Notably, the Abliterated version of LLaMA experiences a significant loss in factuality, which is unexpected since uncensuring typically targets safety rather than factual accuracy. This suggests a potential connection between safety mechanisms and trustworthiness.

% \subsection{Comparison of quantization methods}


% The results indicate that quantization affects the safety and performance of language models differently depending on the method and bit precision used. AWQ seems to have the least impact on model performance, while AQLM, especially at 2-bit precision, results in significant degradation. QUIP\# shows mixed results, with data for Llama 3.1 8B Instruct missing but demonstrating a clear performance drop for Mistral 7B Instruct v0.2.

%\subsection{Multilingual Safety after quantization}



% \begin{table*}[t!]
% 	\centering
% 	\begin{tabular}{ccccccc}
%         Language &  Llama FP 16 & Llama AWQ 4-bit  & Mistral FP 16 & Mistral AWQ 4-bit\\
  
% 		\cmidrule(r){1-5}
  
%         Arabic &  0.62 & 0.64 & 0.38 & 0.36  \\
%         Bengali &  0.49 & 0.45 & 0.33 & 0.36 \\
%         Chinese &  \textbf{0.71} & \textbf{0.76} & 0.80 & 0.80 \\
%         English &  0.88 & 0.85 & 0.90 & 0.89 \\
%         French &  0.83 & 0.79 & 0.83 & 0.80 \\
%         German &  0.83 & 0.79  & 0.74 & 0.72 \\
%         Hindi &  \textbf{0.69} & \textbf{0.60} & 0.31 & 0.31  \\
%         Japanese &  0.56 & 0.60 & 0.62 & 0.60 \\
%         Russian &  0.80 & 0.78 & \textbf{0.79} & \textbf{0.76} \\
%         Spanish &  0.85 & 0.81 & 0.74 & 0.72 \\
%         Average &  0.73 & 0.71 & 0.65 & 0.64 \\
	
    
% 		\bottomrule
%     \\[1em]
% 	\end{tabular}
%     \caption{Multilingual quantization}
% 	\label{tab:table}
% \end{table*}




% \begin{table*}[t!]
% 	\centering
% 	\begin{tabular}{ccccccc}
%         Language &  Llama FP 16 & Llama AWQ 4-bit  & Mistral FP 16 & Mistral AWQ 4-bit\\
  
% 		\cmidrule(r){1-5}
  
%         Commonsense &  0.82 & 0.77 & 0.82 & 0.78  \\
%         Crimes and Illegal Activities &  0.80 & 0.83 & 0.66 & 0.65 \\
%         Ethics and Morality &  0.71 & 0.70 & 0.71 & 0.71 \\
%         Goal Hijacking &  0.66 & 0.67 & \textbf{0.52} & \textbf{0.49} \\
%         Inquiry with Unsafe Opinion &  0.76 & 0.70 & 0.66 & 0.65 \\
%         Insult &  0.73 & 0.79  & 0.53 & 0.51 \\
%         Mental Health &  0.80 & 0.81 & 0.75 & 0.76  \\
%         Physical Harm &  0.76 & 0.74 & 0.68 & 0.66 \\
%         Privacy and Property &  0.81 & 0.84 & 0.78 & 0.77 \\
%         Prompt Leaking &  \textbf{0.34} & \textbf{0.21} & 0.32 & 0.33\\
%         Reverse Exposure &  0.68 & 0.67 & 0.73 & 0.72 \\
% 	Role Play Instruction &  \textbf{0.81} & \textbf{0.71} & 0.59 & 0.59 \\
%  	Unfairness and Discrimination &  0.71 & 0.73 & 0.70 & 0.71 \\	
%   Unsafe Instruction Topic &  0.76 & 0.74 & \textbf{0.61} & \textbf{0.58} \\
%     Average &  0.73 & 0.71 & 0.65 & 0.64 \\
    
% 		\bottomrule
%     \\[1em]
% 	\end{tabular}
%     \caption{Common sense quantization}
% 	\label{tab:table}
% \end{table*}



% \begin{table*}[H]
% 	\centering
% 	\begin{tabular}{ccccccc}
%         Language &  LLama FP 16 & Llama AWQ 4-bit  & Mistral FP 16 & Mistral AWQ 4-bit\\
  
% 		\cmidrule(r){1-5}
  
%         Arabic &  0.95 & 1.00 & 0.78 & 0.69  \\
%         Bengali &  0.98 & 0.99 & 0.42 & 0.34 \\
%         Chinese &  \textbf{0.67} & \textbf{0.76} & 0.67 & 0.67 \\
%         English &  0.98 & 0.98 & 0.99 & 0.98 \\
%         French &  0.98 & 0.99 & 0.91 & 0.86 \\
%         German &  0.99 & 0.99  & 0.92 & 0.89 \\
%         Hindi &  0.98 & 0.98 & \textbf{0.55} & \textbf{0.36}  \\
%         Japanese &  \textbf{0.84} & \textbf{0.91} & 0.78 & 0.74 \\
%         Russian &  0.97 & 0.99 & 0.93 & 0.89 \\
%         Spanish &  0.98 & 0.98 & \textbf{0.56} & \textbf{0.66} \\
%         Average &  0.93 & 0.96 & 0.75 & 0.71 \\
    
% 		\bottomrule
%     \\[1em]
% 	\end{tabular}
%     \caption{Language confusion line-level pass rate}
% 	\label{tab:table}
% \end{table*}


