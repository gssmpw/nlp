


% \section{Quantization}


% Quantization maps a range of values to a discrete finite set, typically requiring 4 or 8 bits for storage. This process improves computational efficiency by enabling faster integer arithmetic and reducing memory usage. However, quantization can limit model expressivity and degrade quality due to quantization noise or error.
% Two primary factors affect model inference time: arithmetic and memory bottlenecks. Large language models (LLMs) involve substantial matrices, making the transfer between different memory types computationally challenging. Consequently, recent efforts have focused on addressing memory bottlenecks in LLMs by reducing weight precision and quantizing weights-only to Int-8, Int-4, and even Int-2 or lower, while maintaining activations in floating-point format~\cite{egiazarian2024extreme, chee2024quip, ashkboos2023towards, lin2024awq}.
% There are two common quantization paradigms: Quantization Aware Training (QAT) and Post Training Quantization (PTQ). QAT has been shown to yield better results than PTQ for low-resource models. However, applying QAT to arbitrary large models is often computationally infeasible. As a result, recent quantization efforts for LLMs have primarily focused on PTQ methods.
% One of the most common approaches is \textbf{linear uniform quantization}, which is computationally efficient but may lack precision, as regions with high and low densities are quantized using the same step size. To address this issue, weights can be transformed to "smooth" them, making them more amenable to quantization. This approach, known as \textbf{companding}, requires additional projection operations. Another method is \textbf{vector quantization}, which theoretically reduces quantization error~\cite{gray1984vector, gray1998quantization}. However, vector quantization necessitates a lookup table to recover the original vector, potentially introducing additional computational overhead.
% In this work, we study PTQ methods from each category: linear quantization, companding, and vector quantization, focusing on 4-bit and 2-bit precision for weight-only quantization. This focus is due to the fact that 8-bit quantization is more widely studied and typically does not significantly degrade model performance~\cite{li2024evaluating, liu2024evaluating, jin2024comprehensive}.

% \begin{itemize}
%     \item \textbf{AWQ:}~\cite{lin2024awq} This method employs linear quantization with weight scaling, allowing for both 8-bit and 4-bit representations.
%     \item \textbf{QUIK:}~\cite{ashkboos2023towards} Similar to AWQ, QUIK uses linear quantization but incorporates non-quantizable salient vectors, also supporting 8-bit and 4-bit formats.
%     \item \textbf{AQLM:}~\cite{egiazarian2024extreme} This approach utilizes additive vector quantization, focusing on 4-bit and lower (including 2-bit) precision.
%     \item \textbf{QUIP\#:}~\cite{chee2024quip} This method applies vector quantization combined with a Hadamard Transform to smooth the weights, allowing for 4-bit and lower (including 2-bit) precision.
% \end{itemize}
    
% Details and description of each quantization method can be found in Appendix~\ref{app:methods}.


\section{Safety Alignment and root of the problem}
The unsafe behavior of models often stems from training on uncurated datasets containing harmful examples. To address this, LLMs undergo alignment procedures to ensure they operate in line with human values. Misalignment is a primary source of potential harm in AI systems. To prevent generating content that conflicts with human values, LLMs undergo supervised fine-tuning after pre-training, followed by alignment training using reinforcement learning from human feedback (RLHF)\cite{10.5555/3600270.3602281} or direct preference optimization (DPO)~\cite{Rafailov2023DirectPO}. However, studies show that safely aligned LLMs can still be subverted through fine-tuning to generate harmful content~\cite{yang2023shadow, lermen2023lora}. We evaluate an open-source model, LLaMA3 Abliterated, as an unsafe baseline. Since quantization alters model weights and often involves fine-tuning, it is reasonable to assume quantization impacts model safety, as supported by recent works~\cite{liu2024evaluating, belkhiter2024harmlevelbench}. Finally, as suggested in~\cite{ren2024safetywashing} overall model performance drop caused by quantization would correlate with model safety.