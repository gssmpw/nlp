\section{Related Work}



Quantization has been widely studied for efficiency gains, but its impact on safety remains an evolving research area. Our work expands on prior studies by introducing new datasets and evaluation methodologies, reflected in Table~\ref{tab:review}.

\paragraph{Quantization and Model Robustness.}
\citet{liu2024evaluating} found that quantizing weights to 3-4 bits generally preserves performance across tasks, but sensitivity varies by dataset, requiring task-specific optimization. Meanwhile, \citet{li2024evaluating} found no clear link between adversarial robustness and quantization, whereas \citet{belkhiter2024harmlevelbench} observed that quantized models showed increased resistance to complex jailbreaking attempts.
\citet{jin2024comprehensive} showed that social biases largely remain post-quantization, but truthfulness drops significantly at 2-bit precision using GPTQ. Similarly, \citet{xu2024beyond} found that extreme quantization introduces unpredictable representational harm, disproportionately affecting protected groups.


% \paragraph{Multilingual Considerations.}
% \citet{marchisio2024does} analyzed quantization across 23 languages, revealing significant degradation in non-Latin script languages, particularly in mathematical reasoning tasks. Their findings suggest that automatic metrics underestimate performance loss. \citet{liu2024evaluating} studied quantization across 40 datasets but did not examine its impact on LLM safety.

\paragraph{Post-Training Quantization and Safety}
Most recent efforts focus on post-training quantization (PTQ) due to the computational infeasibility of quantization-aware training (QAT) for large models. Linear uniform quantization remains common but struggles with precision loss. Alternative methods, such as companding and vector quantization, attempt to mitigate these issues by modifying weight distributions or leveraging lookup-based recovery mechanisms~\cite{gray1984vector, gray1998quantization}. Our work evaluates PTQ techniques across these categories, specifically targeting 4-bit and 2-bit weight-only quantization~\cite{li2024evaluating, liu2024evaluating, jin2024comprehensive}.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{content/safe.png}
    \caption{A schematic overview of the \textbf{OpenSafetyMini} dataset construction process. First, we extract questions from XSafety and estimate their {deflection score} using GPT-4o. We then select questions with a {deflection score > 50\%} and further refine them through human assessment to create the final dataset.  Questions with a {deflection score > 80\%} are highlighted in orange, while those with {< 10\%} appear in blue.}
    \label{fig:schema}
\end{figure*}


\paragraph{Alignment and Safety Considerations}
Model alignment strategies like reinforcement learning from human feedback (RLHF)~\cite{10.5555/3600270.3602281} and direct preference optimization (DPO)~\cite{Rafailov2023DirectPO} seek to reduce harmful outputs, but quantization may affect alignment properties. \citet{ren2024safetywashing} suggest that performance degradation due to quantization correlates with increased safety risks. We investigate this hypothesis by evaluating two models—one aligned and one unaligned—to assess quantization’s impact on safety.



%% OLD 
% While effect of quantization on safety of LLMs was studied previously, in Table~\ref{tab:review} we demonstrare that our work complement previous research in this direction in terms of datasets and methods.

% \cite{liu2024evaluating} found that most methods can achieve comparable performance when quantizing weights to 3-4 bits precision for most tasks. However, sensitivity to quantization varies significantly across tasks and datasets, highlighting the need for task-specific optimization strategies. The relationship between quantization and model robustness is complex. \cite{li2024evaluating} observed no clear connection between adversarial attacks and quantization effects. However, \cite{belkhiter2024harmlevelbench} found that quantized models exhibited increased robustness against complex jailbreaking methods compared to direct attacks on the original model, suggesting potential protective benefits. \cite{jin2024comprehensive} found no consistent trend in social bias changes when LLMs are quantized. However, they observed a significant decrease in truthfulness when LLMs were quantized to 2 bits using GPTQ. \cite{xu2024beyond} noted that while quantization methods mostly preserve a model's bias, toxicity, and performance at moderate compression rates, higher compression levels led to divergent representational harm against different protected groups with no clear pattern.

% \paragraph{Effect of Quantization on Safety in Multilingual Models.} In~\cite{marchisio2024does}, the authors analyze the impact of quantization on LLMs across 23 languages using 4-bit and 8-bit techniques. They find that non-Latin script languages experience the most degradation, particularly in mathematical reasoning tasks. Their evaluation combines human assessments with LLM-as-a-Judge methods, revealing that automatic metrics significantly underestimate performance drops. In~\cite{liu2024evaluating}, the authors assess quantization effects across 40 datasets using two LLMs and four algorithms at 4-bit and 2-bit precision. Both studies highlight that while quantization can yield benefits, certain tasks and languages are more sensitive to its effects. However, neither paper explores the impact of model quantization on LLM safety.


% \begin{tcolorbox}[
%     colback=blue!5!white, % Background color
%     colframe=blue!75!black, % Border color
%     title=Takeaway, % Caption/title of the box
%     fonttitle=\bfseries, % Bold font for the caption
%     boxrule=0.5pt, % Border thickness
%     arc=4pt, % Rounded corners
%     boxsep=5pt, % Space between text and box edges
%     left=6pt, right=6pt, top=6pt, bottom=6pt % Padding
% ]
% The relationship between quantization and evaluated safety dimensions is complex. Thus, context-specific evaluation is required.
% \end{tcolorbox}



% Findings from~\cite{li2024evaluating}: The performance of the quantized model consistently declines, resembling the overall phenomena discussed. Overall, in our experiments, we do not observe a clear connection between the effects of adversarial attacks and quantization.

% Findings from~\cite{liu2024evaluating}:  1) Tasks vary significantly in their sensitivity to quantization, and even the same tasks exhibit different sensitivities on different datasets. Overall, when most methods quantize weights to 3-4 bits precision, models can still achieve performance close to that of full precision models on most tasks. 

% Findings from~\cite{jin2024comprehensive}: The experimental results, as illustrated in Figure 6, reveal no consistent trend of increase or decrease in social bias when LLMs are quantized to fewer bits.
% Furthermore, Figure 5a demonstrates that the truthfulness of LLMs can
% also be influenced by quantization. Specifically, when LLMs are quantized to 2 bits using GPTQ, there is a significant decrease in the truthfulness of the quantized LLMs. In conclusion, our findings suggest that in addition to commonly evaluated dimensions such as knowledge & capacity and efficiency, the alignment of LLMs with human values, which is a dimension often overlooked in previous studies of LLM quantization, deserves greater attention.


% Findings from~\cite{belkhiter2024harmlevelbench}: We introduce HarmLevelBench, a novel dataset comprising queries across 7 harmful topics, each further categorized into 8 distinct levels of severity, enabling a fine-grained analysis of model responses. Second, we conduct a comprehensive performance comparison of 7 state-of-the-art jailbreaking techniques applied to this dataset, offering
% insights into their effectiveness across various harm levels
%  Our findings indicate a general trend toward increased robustness in quantized models for more complex jailbreaking methods compared to direct attacks on the original model

% Findings from~\cite{xu2024beyond}: With higher compression, the representational harm against different protected groups diverges, and such changes show no clear pattern. Pretrained language models have dialect biases, and model compression maintains such biases.  Quantization methods mostly preserve model’s bias, toxicity and performance at a moderate compression rate (e.g. 50\%), while pruning methods show significant degradation at the same compression rate.





% In~\cite{egashira2024exploitingllmquantization}, the adverse effects of the LLM quantization are studied from a security perspective for the first time. The authors demonstrate that commonly used quantization methods can be manipulated to create a harmful quantized LLM, despite the full-precision version seeming harmless. It could mislead users into deploying the malicious quantized model. The experimental evidence for the feasibility and seriousness of this attack is provided in three distinct scenarios: vulnerable code generation, content injection, and over-refusal attacks. According to~\cite{kumar2024finetuningquantizationllmsnavigating}, quantization has an impact on the vulnerability of LLMs against jailbreak attacks, but high variability is observed. Quantization of some models has a significant effect, resulting in the attack success rate (ASR) more than doubling, while for others the impact is minimal. These studies indicate the importance of detailed safety evaluation for quantized LLMs.

%Please add the following packages if necessary:
%\usepackage{booktabs, multirow} % for borders and merged ranges
%\usepackage{soul}% for underlines
%\usepackage{xcolor,colortbl} % for cell colors
%\usepackage{changepage,threeparttable} % for wide tables
%If the table is too wide, replace \begin{table}[!htp]...\end{table} with
%\begin{adjustwidth}{-2.5 cm}{-2.5 cm}\centering\begin{threeparttable}[!htb]...\end{threeparttable}\end{adjustwidth}



%\subsection{Evaluation of LLM safety}
% \textbf{TODO add separate sections for benchmakrs and safety training \cite{wei2024jailbroken, wang2023self, fu2024cross, yuan2024refuse, hubinger2024sleeper}. }




 % \colorbox{orange}{1) What conclusions are made in these papers?} \\
 % \colorbox{orange}{Why models are not unsafe, 2) What conclusions are made in the papers on Quantizaion and Safety}
