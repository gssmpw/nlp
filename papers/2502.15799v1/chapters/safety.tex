\section{Evaluation of LLM Safety}
Large Language Models (LLMs) are increasingly deployed across diverse applications, each requiring domain-specific validation procedures. Consequently, LLM safety evaluation has emerged as a distinct research area. Many usage scenarios demand safe user interactions (\textbf{SUI}), a context-dependent concept influenced by factors such as application domain, cultural norms, and user demographics. For instance, definitions of "safe" interactions vary significantly between educational chatbots and healthcare advisory systems. Beyond \textbf{SUI}, LLM safety encompasses broader risks, including malicious exploitation, privacy leakage, copyright violations, and adversarial attacks~\cite{chao2024jailbreakingblackboxlarge,zou2023universal,10.1145/3605764.3623985,10.1145/3447548.3467390}.

This work focuses exclusively on \textbf{SUI}. However, operationalizing safety within this scope remains non-trivial. To address this, researchers have developed specialized datasets targeting specific risk factors: Social bias~\cite{dhamala2021bold,wan2023biasasker}, Toxicity~\cite{hartvigsen2022toxigen}, Jailbreak vulnerabilities~\cite{shen2023anything}, Empathy and ethics~\cite{huang2023emotionally}.

Recent efforts further categorize safety into granular subdomains—physical harm, mental health, ethics, privacy, and illegal activities—supported by benchmarks such as SALAD-Bench~\cite{li2024salad}, Safety-Prompts~\cite{sun2023safety}, and SafetyBench~\cite{zhang2023safetybench}. However, these benchmarks are predominantly limited to English and Chinese, hindering research on multilingual safety alignment. The XSAFETY benchmark~\cite{wang2023all} addresses this gap as the first multilingual safety evaluation dataset. Its findings reveal that LLMs generate significantly more unsafe responses for non-English queries compared to English, underscoring the urgent need for cross-lingual safety alignment.

% While safety categories could theoretically proliferate (e.g., as combinations of application, language, and cultural contexts), this work investigates the feasibility of automated category creation to mitigate manual categorization. A detailed analysis of safety benchmarks~\cite{ren2024safetywashing} further reveals that many correlate with upstream model capabilities, suggesting that improvements in general performance may inadvertently enhance—or degrade—safety metrics. This implies that compression techniques, such as quantization, could impact safety outcomes. We therefore explore two key questions: \textbf{Q1:} How distinct quantization methods affect LLM safety? And \textbf{Q2}: Do results from automatically generated safety categories correlate with established benchmarks?



% LLMs are rapidly finding applications in a very diverse areas and for a variety of tasks. Often, each area and a task requires a domain specif validation procedure, LLM evaluation is becoming a distinct area on its own.  Many applications need to meet specific safety requirements for \textit{safe-user-interactions - SUI}. However, it is difficult to give a strict definition of \textit{SUI} since it is context dependent. For example, based on application, culture and a user safe interaction would mean different things. 



% Apart from\textit{SUI} there are other safety areas such as malicious exploitation, privacy leakage, copyright violations, adversarial attacks and much more~\cite{chao2024jailbreakingblackboxlarge, 10.1145/3605764.3623985, 10.1145/3447548.3467390, zou2023universal}. But in our work, we focus only on \textit{SUI}. However, even in this limited context it is difficult to define safety. Therefore, as a mean of defining safety researches focused on creating specifically tailored datasets emphasizing particular risk factors: social bias~\cite{dhamala2021bold, wan2023biasasker}, toxicity~\cite{hartvigsen2022toxigen}, empathy ability~\cite{huang2023emotionally}, jailbreak~\cite{shen2023anything}. Other researchers focused on even deeper detalization bringin more safety categories: including physical harm, mental health, ethics and morality, privacy and property, illegal activities, prompt leaking, and others, these datasets are: SALAD-Bench~\cite{li2024salad}, Safety-Prompts~\cite{sun2023safety}, SafetyBench~\cite{zhang2023safetybench}.  However, the above mentioned datasets are limited to one or two languages (either English, Chinese, or both), which impedes research on multilingual safety. This issue is addressed by the first multilingual safety benchmark for LLMs called XSAFETY~\cite{wang2023all}. The study's findings indicate that all the examined LLMs generate notably more unsafe responses for non-English queries compared to English ones. It highlights the necessity of establishing safety alignment for non-English languages.


 % The findings in~\cite{li2024salad} reveal differing performance levels among models and underscore specific areas that may require additional focus to improve the safety and reliability of LLMs. According to~\cite{sun2023safety}, the safety scores for Instruction attacks are consistently lower than that of typical scenarios, confirming that Instruction attacks effectively expose safety issues in LLMs more readily. In~\cite{zhang2023safetybench}, the authors show that open-source LLMs demonstrate a notable performance disparity when compared to GPT-4, highlighting considerable opportunities for future enhancements.

% Even though, these datasets did not capture the broad range of potentially harmful outputs that LLMs could produce. There have been proposed new benchmarks which surpass traditional ones due to their large scale, rich diversity and versatile functionalities: for example, SALAD-Bench~\cite{li2024salad}, Safety-Prompts~\cite{sun2023safety}, SafetyBench~\cite{zhang2023safetybench}. They cover various safety scenarios including physical harm, mental health, ethics and morality, privacy and property, illegal activities, prompt leaking, and others. The findings in~\cite{li2024salad} reveal differing performance levels among models and underscore specific areas that may require additional focus to improve the safety and reliability of LLMs. According to~\cite{sun2023safety}, the safety scores for Instruction attacks are consistently lower than that of typical scenarios, confirming that Instruction attacks effectively expose safety issues in LLMs more readily. In~\cite{zhang2023safetybench}, the authors show that open-source LLMs demonstrate a notable performance disparity when compared to GPT-4, highlighting considerable opportunities for future enhancements.

% However, the above mentioned datasets are limited to one or two languages (either English, Chinese, or both), which impedes research on multilingual safety. This issue is addressed by the first multilingual safety benchmark for LLMs called XSAFETY~\cite{wang2023all}. The study's findings indicate that all the examined LLMs generate notably more unsafe responses for non-English queries compared to English ones. It highlights the necessity of establishing safety alignment for non-English languages.

% These concerns have spurred research focused on AI alignment. The goal of AI alignment is to ensure that AI systems operate in line with human intentions and values. Misalignment, or failures in this regard, is one of the primary sources of potential harm associated with AI. To avoid generating content that conflicts with human values, LLMs undergo a supervised fine-tuning phase following their pre-training. This is complemented by an alignment training phase that utilizes reinforcement learning from human feedback (RLHF)~\cite{10.5555/3600270.3602281} or direct preference optimization (DPO)~\cite{Rafailov2023DirectPO}.