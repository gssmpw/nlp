\section{Conclusion}
In this paper, we introduce DNPO, a robust post-training framework that enhances LLMs with self-generated synthetic data. DNPO divides into Dynamic Sample Labeling (DSL) and Noise Preference Optimization (NPO): DSL dynamically reassign training target, effectively suppressing harmful supervision from human-annotated preference pairs. NPO introduces trainable noise into the optimization process, simultaneously fine-tuning both LLMs and the introduced noise to overcome model stagnation. Our extensive experiments demonstrate that DNPO consistently boosts model performance across iterations. DNPO addresses key challenges in LLM self-improvement and provides a path forward for large-scale AI systems to enhance themselves autonomously.