\section{Experiments}
\subsection{Experimental setup}

We use Mistral-7B \citep{jiang2023mistral7b} as the base model in our experiments, which is fine-tuned on the UltraChat-200k \citep{ding2023enhancingchatlanguagemodels} dataset into Zephyr-7B-SFT. Then, we conduct post-training alignment with DNPO on a 20k sample from the UltraChat dataset. It's crucial that both SFT and DNPO are trained on the same dataset to ensure self-improvement. During the DSL stage, GPT4o-mini is used for evaluation, with the prompt template provided in Appendix \ref{appendix b}. On a 1k sample set, preference pairs predicted by GPT scores reached 95\% accuracy compared to human judgments. The noise generator in the NPO stage is parameterized as $ \theta_\sigma = { [\mathbf{W}_\sigma \in \mathbb{R}^{4096 \times 32000}, \mathbf{b}_\sigma \in \mathbb{R}^{32000} ]} $. In the initial iteration ($k=0$), we do not perform label sampling or noise addition, as the SFT model is yet unaligned with preference knowledge. Instead, we use the SPIN method for initialization, ensuring alignment with the ground truth data. This can be seen as a warm-up process, allowing the model to acquire basic preference information. Key training hyper-parameters and the evaluation metrics are detailed in Appendix \ref{appendix c}.

\subsection{Main Results}

\begin{figure}[t!]
    \centering
    \begin{minipage}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Figures/Exp/benchmark_avg.png}
    \caption{Comparison of average benchmark scores across iterations for DNPO and SPIN. DNPO consistently improves over iterations while SPIN stagnates after the first iteration.}
    \label{fig5}
    \end{minipage}
    \hspace{0.04\textwidth}
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/Exp/llm_reward/gpt4omini_reward.png}
        \caption{Average GPT4o-mini scores comparison across iterations for generated data of DNPO and SPIN, alongside the ground truth performance.}
        \label{fig6}
    \end{minipage}

\end{figure}



\begin{table}[t]
\centering
\small
\caption{Performance of Mistral-7B on various benchmarks. Performance is compared between different iterations of SPIN and DNPO, starting from the Zephyr-7B-SFT.}
\begin{tabular}{cccccccc}
    \toprule
    \textbf{Iteration} & \textbf{ARC} & \textbf{TruthfulQA} & \textbf{Winogrande} & \textbf{GSM8K} & \textbf{HellaSwag} & \textbf{MMLU} & \textbf{Average} \\
    \midrule
    Zephyr-7B-SFT   & 0.704 & 0.340 & 0.762 & 0.318 & 0.810 & 0.588 & 0.587 \\
    SPIN-Iter. 0 & 0.709 & 0.393 & 0.768 & 0.289 & 0.826 & 0.590 & 0.596 \\
    SPIN-Iter. 1 & 0.702 & 0.362 & 0.760 & 0.316 & 0.817 & 0.585 & 0.590 \\
    \cellcolor{gray!20} DNPO-Iter. 1 (Ours) & \cellcolor{gray!20} 0.734 & \cellcolor{gray!20} 0.381 & \cellcolor{gray!20} 0.766 & \cellcolor{gray!20} 0.334 & \cellcolor{gray!20} 0.827 & \cellcolor{gray!20} 0.583 & \cellcolor{gray!20} \textbf{0.604} \\
    SPIN-Iter. 2 & 0.707 & 0.370 & 0.761 & 0.276 & 0.820 & 0.585 & 0.586 \\
    \cellcolor{gray!20} DNPO-Iter. 2 (Ours) & \cellcolor{gray!20} 0.735 & \cellcolor{gray!20} 0.397 & \cellcolor{gray!20} 0.765 & \cellcolor{gray!20} 0.323 & \cellcolor{gray!20} 0.828 & \cellcolor{gray!20} 0.587 & \cellcolor{gray!20} \textbf{0.606} \\
    SPIN-Iter. 3 & 0.703 & 0.383 & 0.756 & 0.275 & 0.818 & 0.579 & 0.586 \\
    \cellcolor{gray!20} DNPO-Iter. 3 (Ours) & \cellcolor{gray!20} 0.737 & \cellcolor{gray!20} 0.417 & \cellcolor{gray!20} 0.766 & \cellcolor{gray!20} 0.336 & \cellcolor{gray!20} 0.827 & \cellcolor{gray!20} 0.586 & \cellcolor{gray!20} \textbf{0.612} \\
    \bottomrule
\end{tabular}
\label{table1}
\end{table}


Figures \ref{fig5} and \ref{fig6} compare DNPO and SPIN using two metrics: average benchmark scores and GPT4o-mini scores. Figure \ref{fig5} shows DNPO steadily improving in average benchmark scores, reaching 0.612 in iteration 3, while SPIN gets stuck around 0.586. In Figure \ref{fig6}, DNPO consistently outperforms SPIN in GPT4o-mini scores across all iterations, peaking at 84.86 in iteration 2, compared to SPIN's best of 82.66. These results demonstrate DNPO's superior and consistent improvement over SPIN across iterations.


Table \ref{table1} provides a detailed comparison of DNPO, SPIN, and SFT model across various benchmarks. On average, DNPO achieves a 2.5\% improvement over the SFT model and a peak improvement of 2.6\% over SPIN in iteration 3. Notably, on the TruthfulQA benchmark, DNPO shows a substantial improvement of 7.7\% over the SFT model and 3.4\% over SPIN. This benchmark best reflects the model's performance because both UltraChat and TruthfulQA are question-answering datasets with similar data formats, focusing on generating accurate, truthful conversational data. This significant gain indicates that DNPO effectively enhances the model's ability to generate high-quality responses. Similarly, DNPO outperforms SPIN on ARC with a gain of 3.3\% and outperforms the SFT model by 3.4\%. These results further highlight the effectiveness of DNPO in improving model performance across a wide range of benchmarks. 

\begin{minipage}{0.56\textwidth}
Figure \ref{fig7} compares the win, tie, and loss rates of data generated by DNPO and SPIN over three iterations, using GPT4o-mini scores as the evaluation metric. DNPO consistently outperforms SPIN in win rate, with the largest gap in iteration 3 (57.51\% vs. 28.07\%, a 29.4\% gap). On average, the win-loss rate gap is 24.56\% across iterations, highlighting DNPO's superior ability to generate higher-quality data. Additionally, Appendix \ref{appendix d} presents two examples comparing data generated by DNPO and SPIN. Furthermore, Appendix \ref{appendix e} and \ref{appendix f} provide additional evaluation results using various LLMs and traditional metrics, further demonstrating the robustness and reliability of DNPO across diverse evaluation methods.

\end{minipage}
\hfill
\begin{minipage}{0.4\textwidth}
    \centering
        \includegraphics[width=\textwidth]{Figures/Exp/win_rate/exp_win_rate-zephyr_gpt4omini.png}
        \captionof{figure}{Win rate comparison of DNPO vs. SPIN, where DNPO consistently outperforms SPIN across all iterations.}
        \label{fig7}
\end{minipage}

\subsection{Comparison Between DNPO and Dynamic Data Mixing Approaches}
In our study, we also compared DNPO, which involves training with fixed training data at each iteration, with several dynamic data mixing approaches. Specifically, we evaluated two approaches:

\begin{enumerate}
    \item \textbf{PPO}: This method leverages a reward model \footnote{We use \href{https://huggingface.co/OpenAssistant/reward-model-deberta-v3-large-v2}{\texttt{reward-model-deberta-v3-large-v2}} in our experiments.} and the Proximal Policy Optimization (PPO) algorithm \cite{schulman2017proximalpolicyoptimizationalgorithms} to train the model. Unlike DNPO, the training data is not fixed; instead, new data is dynamically generated online throughout the training process.
    
    \item \textbf{$\alpha$-SPIN}: $\alpha$-SPIN \cite{alami2024investigatingregularizationselfplaylanguage} introduces diversity by mixing training data from previous iterations. For iteration $k$, the training data is a 50:50 mix of data generated by models from iterations $k-1$ and $k-2$.
\end{enumerate}

We evaluated these methods on six benchmarks. The results are summarized in Table~\ref{table2}. While both PPO and $\alpha$-SPIN helped introduce greater data diversity, neither method outperformed DNPO in terms of average performance.

\begin{table}[ht]
\small
\centering
\caption{Performance comparison of DNPO, PPO, and $\alpha$-SPIN on six benchmarks.}
\label{table2}
\begin{tabular}{lccccccc}
\toprule
\textbf{Method} & \textbf{ARC} & \textbf{TruthfulQA} & \textbf{Winogrande} & \textbf{GSM8K} & \textbf{Hellaswag} & \textbf{MMLU} & \textbf{Average} \\
\midrule
PPO & 0.700 & 0.351 & 0.762 & 0.282 & 0.817 & 0.584 & 0.583 \\     
$\alpha$-SPIN  & 0.714 & 0.352 & 0.754 & 0.271 & 0.788 & 0.567 & 0.574 \\
DNPO           & 0.735 & 0.360 & 0.770 & 0.300 & 0.830 & 0.590 & 0.604 \\
\bottomrule
\end{tabular}
\end{table}



\subsection{Ablation studies}

The SPIN-iteration $k$ model is used as the baseline for each iteration in the ablation study, with DSL, NPO, and DNPO applied separately to validate their effectiveness. Figure \ref{fig9} compares the SPIN model with the addition of DSL, NPO, and DNPO across three iterations. Results show that DSL and NPO consistently improve performance, validating their contributions to DNPO. \textbf{In iteration 1}, the largest gains are achieved by NPO, which effectively addresses model stagnation and boosts early-stage performance. \textbf{In iteration 2}, DSL shows the highest impact, as the win rate of generated data over SFT ground truth peaks, leading to the most incorrect preference pairs. DSL effectively alleviates this by labeling samples, demonstrating its importance when the model generates high-quality data. \textbf{In iteration 3}, performance gains result from the combined effects of DSL and NPO. Despite nearing the performance ceiling, the continued improvements highlight the robustness of this approach. Detailed benchmark accuracy is in Appendix \ref{appendix f}, with Appendix \ref{appendix g} comparing fixed vs. trainable noise, showing the benefits of learning noise parameters.


\begin{figure}[h]
    \centering
    % First subfigure
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/Exp/ablation/ablation-iter1.png}
        \caption{Iteration 1}
    \end{subfigure}
    %\hfill
    \hspace{0.02\textwidth}
    % Second subfigure
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/Exp/ablation/ablation-iter2.png}
        \caption{Iteration 2}
    \end{subfigure}
    %\hfill
    \hspace{0.02\textwidth}
    % Third subfigure
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/Exp/ablation/ablation-iter3.png}
        \caption{Iteration 3}
    \end{subfigure}

    \caption{Comparing the performance of the SPIN Iter. $k$ model as the base model combined with different methods—SPIN, SPIN + DSL, SPIN + NPO, and SPIN + DNPO across various benchmarks from iteration 1 to 3.}
    \label{fig9}
\end{figure}



\subsection{Analysis}

\begin{minipage}[h]{0.5\textwidth}  
Figure \ref{fig10} illustrates the behavior of model loss and noise loss during iteration 1, corresponding to the two terms in Obj.~\ref{obj:final}. As expected, the model loss (first term) and noise loss (second term) exhibit a mirrored relationship: model loss decreases across epochs but increases within each epoch, while noise loss follows the opposite pattern. This behavior suggests that the model is influenced by noise within each epoch but improves overall as training progresses. At the same time, noise loss steadily decreases within each epoch, indicating that the noise itself is learning and becoming more refined throughout the training process. Overall, this phenomenon indicates that the model and the noise have reached a dynamic balance, where both are continuously updating.

\end{minipage}
\hfill
\begin{minipage}[h]{0.48\textwidth} 
    \centering
    \includegraphics[width=\textwidth]{Figures/Exp/loss-zephyr.png}
    \captionof{figure}{Evolution of model loss and noise loss over iteration 1.} 
    \label{fig10}
\end{minipage}

Figure \ref{fig11} presents the evolving log probability distributions of positive samples, negative samples, and generated data across three iterations of DNPO, highlighting the model's continuous updates. A notable phenomenon is the increasing overlap between positive and negative samples, which leads the model to update its parameters with larger gradients when maximizing the margin between positive and negative samples, making the training process less prone to stagnation. Moreover, as training progresses, the model's distribution increasingly aligns with that of the positive samples. These findings demonstrate that the combination of DSL and NPO not only keeps the model actively learning but also drives it toward the desired distribution, ensuring more effective and targeted improvements throughout the iterative training process.


\begin{figure}[h]
    \centering

    % First subfigure
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/Exp/analysis/KL-noise-zephyr-iter1.png}
        \caption{Iteration 1}
    \end{subfigure}
    %\hfill
    \hspace{0.02\textwidth}
    % Second subfigure
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/Exp/analysis/KL-noise-zephyr-iter2.png}
        \caption{Iteration 2}
    \end{subfigure}
    %\hfill
    \hspace{0.02\textwidth}
    % Third subfigure
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/Exp/analysis/KL-noise-zephyr-iter3.png}
        \caption{Iteration 3}
    \end{subfigure}

    \caption{The figure illustrates log probability distributions of positive sample, negative sample in iteration $k$ and generated data of iteration $k+1$ model during DNPO training. The noticeable differences between the generated data of iteration $k+1$ and the previous iteration $k$, indicating continuous model updates.}
    \label{fig11}
\end{figure}
















