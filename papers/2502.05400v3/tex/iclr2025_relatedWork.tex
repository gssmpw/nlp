\section{Related Work}
\textbf{RL with AI Feedback.} Reinforcement Learning from AI Feedback (RLAIF) \citep{bai2022traininghelpfulharmlessassistant} builds upon the principles of Reinforcement Learning from Human Feedback (RLHF) \citep{ouyang2022traininglanguagemodelsfollow, christiano2023deepreinforcementlearninghuman} and has gained considerable traction. Extending beyond established methods like PPO \citep{schulman2017proximalpolicyoptimizationalgorithms} and DPO \citep{rafailov2024directpreferenceoptimizationlanguage}, which align language models to human preferences using human-annotated data, \citep{lee2024rlaifvsrlhfscaling} demonstrates that AI-generated preferences can match or surpass human feedback-based reward models across diverse policies. Furthermore, LLMs have been leveraged to generate high-quality training data, including datasets based on human preferences \citep{cui2024ultrafeedbackboostinglanguagemodels} and conversational interactions \citep{ding2023enhancingchatlanguagemodels}.


\textbf{Self-play in LLMs with Generated Data.} 
% \ting{self-improvement of LLM with generated data}
The pioneering work of AlphaGo Zero \citep{silver2017mastering} inspired self-play fine tuning (SPIN) \citep{chen2024selfplayfinetuningconvertsweak} to explore self-play schemes in LLM fine-tuning, where the model iteratively distinguishes target data from self-generated responses without requiring a separate reward model. Similarly, Self-rewarding Language Model \citep{yuan2024selfrewardinglanguagemodels} demonstrates consistent improvement through self-annotated rewards. This self-improvement paradigm has been successfully applied to various LLM-based reasoning tasks like Werewolf \citep{xu2024languageagentsreinforcementlearning} and Adversarial Taboo \citep{cheng2024selfplayingadversariallanguagegame}. Notably, CICERO \citep{meta2022human} employs self-play to train a RL policy, achieving human-level performance in Diplomacy gameplay. Recently, \citep{shumailov2024ai} observes diminishing tail content distribution in resulting models when iteratively trained on self-generated data. Aligning with this finding, we see notable stagnation in model updates during post-training, and propose an innovative method to reactivate effective updates.



\textbf{Noise Introduction in Language Modeling.} A substantial amount of research has explored the benefits of incorporating noise during training to enhance language model performance. \citep{zhu2020freelbenhancedadversarialtraining} demonstrates that injecting adversarial perturbations into input embeddings can improve masked language modeling. Similarly, \citep{miyato2021adversarialtrainingmethodssemisupervised} show that adversarial training can improve text classification performance. Furthermore, \citep{wu2022noisytunelittlenoisehelp} achieves consistent gains in downstream fine-tuning tasks through a matrix-wise perturbation approach. Gaining popularity recently, NEFTune \citep{jain2023neftunenoisyembeddingsimprove} leverages noisy input embeddings to improve instruction fine-tuning, attaining notable improvement in conversational capabilities. 


