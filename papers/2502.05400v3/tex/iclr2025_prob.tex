\section{Limitations of the Current Approaches}

Previous work \citep{chen2024selfplayfinetuningconvertsweak, alami2024investigatingregularizationselfplaylanguage}, improves LLM alignment by treating human-annotated data as positive examples ($y_i$) and model-generated data as negative examples ($y_i'$). The model is updated to maximize the margin between these examples through an optimization process with Obj.~\ref{eq1}. However, we observed that these methods fail to produce consistent performance improvements across iterations.
To address this, we take SPIN \citep{chen2024selfplayfinetuningconvertsweak} as a case study to examine the following two problems:

\begin{equation}
\label{eq1}
\min_{\theta \in \Theta} \sum_{i \in [N]} \ell \left( \lambda \log \frac{p_{\theta}(y^+_i \,|\, x_i)}{p_{\theta_t}(y^+_i \,|\, x_i)} - \lambda \log \frac{p_{\theta}(y^-_i \,|\, x_i)}{p_{\theta_t}(y^-_i \,|\, x_i)} \right).
\end{equation}



\begin{wrapfigure}{r}{0.4\textwidth}
\begin{center}
\includegraphics[width=0.38\textwidth]{Figures/Problem/spin_win_rate-zephyr.png}
\end{center}
\caption{Win rate comparison of generated data versus human-annotated data, based on GPT4o-mini's evaluation. A win indicates that generated data scored higher than human-annotated data.}
~\label{fig1}
\end{wrapfigure}

\textbf{Is human-annotated data truly better?} One potential issue is that, as the model continues to improve, the human-annotated data may not always be of higher quality than the generated data. As illustrated in Figure \ref{fig1}, we used GPT-4o-mini \citep{openai2024gpt4omni} to compare the generated data produced by SPIN iteration $k$ applied on Zephyr-7b during each iteration and the human-annotated data. In each iteration, around 30\% of the generated data is of equal or higher quality compared to the human-annotated data. This indicates that the assumption of human-annotated data being inherently superior to generated data will introduce about 30\% preference noise in every round, leading to performance fluctuation and potential degradation  \citep{gao2024impactpreferencenoisealignment}.

\textbf{Why does model update stagnation occur?} The stagnation of model updates is demonstrated in Figure \ref{fig2}. After the initial SPIN iteration, model-generated data shows nearly identical log probability distributions between iterations $k$ and $k+1$ across multiple iterations. This resemblance suggests a lack of significant learning progress, as the model struggles to meaningfully adjust its distribution with each iteration. Additionally, model-generated data remains noticeably distant from the distribution of positive samples, suggesting that the model is trapped in a suboptimal state, unable to make further improvements or move toward an optimal solution.

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/Problem/spin-process-zephyr/KL-zephyr-iter0.png}
        \caption{Iteration 0}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/Problem/spin-process-zephyr/KL-zephyr-iter1.png}
        \caption{Iteration 1}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/Problem/spin-process-zephyr/KL-zephyr-iter2.png}
        \caption{Iteration 2}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/Problem/spin-process-zephyr/KL-zephyr-iter3.png}
        \caption{Iteration 3}
    \end{subfigure}
    \caption{This figure illustrates the log probability distributions of positive samples, negative samples in iteration $k$, and the generated data from the iteration $k+1$ model during SPIN training. The minimal differences between the generated data of iteration $k+1$ and the previous iteration $k$ indicate model stagnation during training.}
    \label{fig2}
\end{figure}