\section{Introduction}
Large Language Models (LLMs) have demonstrated remarkable capabilities in various domains. Despite this success, training these models requires vast amounts of human-annotated data, and the limited availability of such data has become a bottleneck for further scaling LLMs \citep{kaplan2020scaling, villalobos2024rundatalimitsllm}. 
This has led to a growing interest on synthetic data generation techniques to supplement human-generated data. 
However, prior research suggests that using self-generated data for pre-training can easily lead to model collapse \citep{shumailov2024ai}. In contrast, leveraging self-generated data for post-training alignment (fine-tuning) appears to be a more practical and manageable approach \citep{chen2024selfplayfinetuningconvertsweak, alami2024investigatingregularizationselfplaylanguage}.

How can we trust synthetic data? Can it be treated the same as human-annotated data, which is often regarded as the gold standard in RLHF methods for training explicit or implicit reward models? Moreover, can we fully trust human-annotated data itself? In reality, human data is susceptible to uncontrollable factors and inevitable errors, which can introduce noise and inconsistencies into the training process.

Surprisingly, we found that synthetic data has the potential to outperform human-annotated data in specific instances. In about 30\% of our experimental cases, we observed that the modelâ€™s self-generated data was of higher quality than the human-annotated data, which challenges the assumption that human-annotated data is always superior. However, even human-annotated data is not flawless, synthetic data cannot be treated identically to it. Self-generated synthetic data poses unique challenges, such as minimal variation between iterations, may lead to model stagnation. Without sufficient diversity in generated samples, the model struggles to consistently improve, reinforcing the need for careful handling of both data types.

To address these issues, we propose Dynamic Noise Preference Optimization (DNPO), a novel framework that enhances both the data labeling and preference optimization processes, enabling the self-improvement of LLMs through synthetic data. Our method introduces a dynamic sample labeling (DSL) mechanism that constructs preference pairs based on data quality by selecting high-quality examples from both LLM-generated and human-annotated data. Also, we proposes the noise preference optimization (NPO), which introduces a trainable noise into the optimization process, resulting in a min-max problem.
This optimization process maximizes the margin between positive and negative samples of the preference pairs, while simultaneously updates the noise parameters to minimize the margin.
Our approach can effectively prevent stagnation, ensuring continuous model improvement with each iteration and increased robustness throughout the self-improvement process.
Our main contributions can be summarized as follows:
\begin{itemize}[leftmargin=8pt]
% \item \textbf{Investigated why LLMs fail to improve consistently:} We pinpoint two key reasons why LLMs struggle to self-improve across iterations: (1) assuming the human-annotated data are always better will introduce noise in preference labeling, as generated data may outperform it (2) model stagnation occurs during updates across iterations.
\item \textbf{Challenges in Consistent Self-Improvement:} We identified two key reasons why current methods struggle to achieve consistent self-improvement in LLMs across iterations: (1) the assumption that human-annotated data is always superior, which introduces noise in preference labeling since generated data may sometimes surpass it, and (2) the lack of variation in generated data across iterations, leading to stagnation during model updates.

\item \textbf{Introducing DNPO with DSL and NPO:} We propose DNPO, a framework that enables LLMs to self-improve using synthetic data via two components: (1) DSL dynamically adjusts sample labels based on data quality, ensuring the model learns from appropriate preference pairs; (2) NPO incorporates trainable noise into the preference data, promoting exploration and reducing stagnation across iterations.

\item \textbf{Demonstrating Improved Performance with DNPO:} Our experiments reveal that DNPO consistently enhances model performance, making it particularly effective for self-generated data, especially as human-annotated data becomes increasingly limited.
\end{itemize}










%{our solution}









