\section{Methodology}\label{sec:model}
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{Figures/Figure3.pdf}
    \caption{This diagram illustrates the iterative training process of DNPO. There are two core components: Dynamic Sample Labeling (DSL) and Noise Preference Optimization (NPO). In each iteration $k$, DSL is responsible for generating new data from the model and labeling it by comparing it with SFT ground truth data using an evaluation model, forming preference pairs. These pairs are then passed to the NPO, which computes a probability ratio between the SFT ground truth and the generated data. NPO applies a noise-tuning strategy, where the model is frozen and the noise component is trained to minimize the margin between positive and negative sample pairs. In the following step, the noise is frozen while optimizing the model to maximize this margin. This leads to an updated model for the next iteration $k+1$.}
    \label{fig3}
\end{figure*}

\subsection{Overview}
As shown in Figure \ref{fig3}, our proposed method, DNPO, effectively addresses two critical issues in iterative model training: preference noise and model update stagnation.

First, to tackle the challenge of preference noise, which arises from the assumption that human-annotated data is always superior to model-generated data, Dynamic Sample Labeling (DSL) is introduced to reduces the noise in the training process. In each iteration, DSL leverages an evaluation model to dynamically compare data generated by LLMs with SFT ground truth, forming preference pairs based on the scores of evaluation model, which ensures that the selection between model-generated and human-annotated data is based on their actual quality, rather than assuming one is inherently better. By dynamically forming preference pairs, this approach eliminates the rigid assumption that human annotations are always preferable. 

Second, to address the issue of model update stagnation, Noise Preference Optimization (NPO) mechanism is employed. NPO works by calculating a probability ratio between the SFT ground truth and the model-generated data, setting an optimization target to minimize or maximize the margin between these two distributions. Specifically, when the model is frozen, noise is fine-tuned to minimize the margin between SFT ground truth and generated data, ensuring that the margin is small enough to provide sufficient incentive for the model to update in the subsequent steps. Conversely, when the noise is frozen, the model is fine-tuned to maximize the margin, allowing the model to capitalize on the diversity introduced by the noise.
By alternating between these two processes, NPO ensures that the model evolves consistently over iterations, avoiding the pitfall of local optima and enhancing long-term performance.


\subsection{Dynamic Sample Labeling}
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{Figures/Figure2.pdf}
    \caption{
    Comparison between a human-annotated response from UltraChat-200k and a model-generated answer from Zephyr-7B after a single SPIN iteration. The ground truth misinterprets the user's intent and refuses to respond on clothes reviews. However, Zephyr-7B generates a detailed and descriptive review of a recently purchased blouse, highlighting aspects such as fit, fabric quality, color, and style.}
    \label{fig4}
\end{figure*}
As shown in Figure \ref{fig4}, in certain instances, we observe that model-generated responses can surpass the quality of the original human-annotated responses for specific prompts (additional examples are provided in Appendix \ref{appendix a}). This observation motivates a dynamic sample labeling (DSL) mechanism.
Before each iteration, DSL selects positive and negative samples based on model evaluation, thereby enhancing the contrastive learning process.
Specifically, For a dataset consisting of input prompts \( \{x_i\} \) and corresponding human-annotated data \( \{y_i\} \), at iteration \( k \), we utilize the current model \( M_{\theta^{(k)}} \) to generate new responses \( y_i' \) for each \( x_i \): $y_i' \sim M_{\theta^{(k)}}(\cdot|x_i)$.

We then evaluate both the human-annotated response \( y_i \) and the generated response \( y_i' \) using a more powerful evaluation model \( M_{\text{eval}} \) with promoting method, which will return their respective scores: $s_i = M_{\text{eval}}(x_i, y_i)$  and $ s_i' = M_{\text{eval}}(x_i, y_i') $. Based on the evaluation, The higher-scoring example becomes the positive sample and the lower-scoring example becomes the negative sample. 
The the optimization object at iteration $k$ is defined as:
\begin{align}
\label{eq2}
    \min_{\theta} \sum_{i=1}^N \ell \Bigg[ & \mathbbm{1}\{ s_i \geq s_i' \} \lambda \left( \log \frac{p_{\theta_t}(y_i \mid x_i)}{p_{\theta}(y_i \mid x_i)} - \log \frac{p_{\theta_t}(y_i' \mid x_i)}{p_{\theta}(y_i' \mid x_i)} \right) \nonumber \\
& + \mathbbm{1}\{ s_i' > s_i \} \lambda \left( \log \frac{p_{\theta_t}(y_i' \mid x_i)}{p_{\theta}(y_i' \mid x_i)} - \log \frac{p_{\theta_t}(y_i \mid x_i)}{p_{\theta}(y_i \mid x_i)} \right) \Bigg]
\end{align}

where $\ell$ is a negative log-sigmoid function, \( \theta \) are the model parameters of \( M_{\theta^{(k)}} \) and \( \theta_t \) represents the parameters of a reference model, initialized with \( M_{\theta^{(k)}} \) and keep frozen,


Through iterative application of this method, the model's performance improves by selectively exploiting human-annotated responses and high-quality LLM-generated data. The dynamic sample labeling mechanism selects higher-quality data as positive samples, thereby increasing label accuracy.



\subsection{Noise Preference Optimization}
Figure \ref{fig2} indicates a large initial margin between positive and negative samples since Iteration 0. This substantial margin results in minimal loss during iterative updates (as shown in Obj.~\ref{eq1}), weakening the gradient's magnitude, in turn, reducing the model's incentive to update its parameters effectively. To counter this, we introduce noise to shrink the initial margin, thereby reinvigorating the model's learning dynamics.



We designate all positive samples as $y_i^+$ and all negative samples as $y_i^-$ after sample labeling. Hence, we can rewrite the Obj.~\ref{eq2} into

\begin{equation}
\label{eq3}
\min_{\theta} \sum_{i=1}^N \ell \left( \lambda \log \frac{p_{\theta}(y^+_i \,|\, x_i)}{p_{\theta_t}(y^+_i \,|\, x_i)} - \lambda \log \frac{p_{\theta}(y^-_i \,|\, x_i)}{p_{\theta_t}(y^-_i \,|\, x_i)} \right).
\end{equation}


We aim to utilize noise to reduce the margin between positive and negative samples and rewrite Obj.\ref{eq3} as Obj.\ref{eq4} to analyze which terms should have noise added. Noise is not added to the first two terms in Obj.~\ref{eq4}, as this could degrade generation quality during inference. Adding noise to the fourth term would increase the margin, whereas adding noise to $\log p_{\theta_t}(\mathbf{y}_i^- \mid \mathbf{x}_i)$ reduces the margin, which aligns with the objective. By introducing noise to this term, the reference model's confidence in negative samples is reduced, effectively narrowing the margin between positive and negative samples.



\begin{equation}
\label{eq4}
        \min_{\theta} \sum_{i=1}^N \ell \Bigg( \lambda \bigg( \Big( \log p_{\theta}(\mathbf{y}_i^+ \mid \mathbf{x}_i) - \log p_{\theta}(\mathbf{y}_i^- \mid \mathbf{x}_i) \Big) + \Big( \underbrace{\log p_{\theta_t}(\mathbf{y}_i^- \mid \mathbf{x}_i)}_{\text{margin $\downarrow$ when add noise}} - \underbrace{\log p_{\theta_t}(\mathbf{y}_i^+ \mid \mathbf{x}_i)}_{ \text{margin $\uparrow$ when add noise}} \Big) \bigg) \Bigg)
\end{equation}

The vocabulary size is often large for LLMs, for example, Mistral~\citep{jiang2023mistral7b} has a vocabulary size of 32,000. In this high-dimensional space, adding random noise cannot effectively minimize the margin. We then propose to add trainable noise generator with zero mean to the logits of the negative samples in the reference model \( p_{\theta_t} \). Specifically, the variance of the noise is modeled using a fully connected layer. For the last hidden state \( \mathbf{h}_i \) of the reference model, the variance \( \boldsymbol{\sigma}_i^2 \) is predicted as follows:

\begin{equation}~\label{eq:noise-generation}
\log \boldsymbol{\sigma}_i = \mathbf{W}_{\sigma} \mathbf{h}_i + \mathbf{b}_{\sigma},
\end{equation}

where \( \mathbf{W}_{\sigma} \) is the weight matrix, \( \mathbf{b}_{\sigma} \) is the bias vector. The parameters for the noise generator are denoted as $\theta_{\sigma} = [\mathbf{W}_{\sigma}, \mathbf{b}_{\sigma}]$.


Noise \( \boldsymbol{\epsilon}_i \) is sampled from a zero-mean, unit-variance Gaussian distribution  $\boldsymbol{\epsilon}_i \sim \mathcal{N}\left( \mathbf{0}, \mathbf{1} \right) $, and the reparameterization trick~\citep{kingma2022autoencodingvariationalbayes} is employed to add the noise to the logits \( \mathbf{z}_i \) corresponding to the negative samples in the reference model: $ \mathbf{z}_i' = \mathbf{z}_i + \exp(\log\boldsymbol{\sigma}_i) \boldsymbol{\epsilon}_i = \mathbf{z}_i + \boldsymbol{\sigma}_i\boldsymbol{\epsilon}_i $. Using the logits \( \mathbf{z}_i' \) with added noise, the modified probability of the negative sample is computed as:



\begin{equation}
p_{\theta_t,\theta_{\sigma}}^{\text{noise}}(y_i^- \mid x_i) = \text{Softmax}(\mathbf{z}_i')
\end{equation}

Incorporating the trainable noise into the optimization function, we obtain a bi-level optimization problem:
\begin{align}~\label{obj:bilevel}
&\min_{\theta} \sum_{i=1}^N \ell \left( \lambda \log \frac{p_{\theta}(y^+_i \,|\, x_i)}{p_{\theta_t}(y^+_i \,|\, x_i)} - \lambda \log \frac{p_{\theta}(y^-_i \,|\, x_i)}{p_{\theta_t,\theta^{*}_{\sigma}}^{\text{noise}}(y_i^- \mid x_i)} \right) \nonumber \\
s.t.\ \theta_{\sigma}^* = &\arg\max_{\theta_{\sigma}}\sum_{i=1}^N \ell \left( \lambda \log \frac{p_{\theta}(y^+_i \,|\, x_i)}{p_{\theta_t}(y^+_i \,|\, x_i)} 
    - \lambda \log \frac{p_{\theta}(y^-_i \,|\, x_i)}{p_{\theta_t,\theta_{\sigma}}^{\text{noise}}(y_i^- \mid x_i)} \right),\ \boldsymbol{\sigma}^2_i < \varepsilon
\end{align}
Where the inner problem is to minimize the margin between positive and negative sample pairs by optimizing $\theta_{\sigma}$, the outer problem is to maximize the margin between sample pairs by optimizing $\theta$ given the optimal noise model parameters $\theta_{\sigma}^*$, and $\varepsilon$ is a constant to prevent the variance of the added noise from being too large and producing meaningless results.  Minimizing $\theta$ requires finding the optimal parameters  for noise $\theta_{\sigma}^*$, which can be computationally expensive. Alternatively, Obj.~\ref{obj:bilevel} can be converted into a min-max problem to avoid the costly inner update: 
\begin{equation}
\min_{\theta}\max_{\theta_{\sigma}}\ \sum_{i=1}^N \ell \left( \lambda \log \frac{p_{\theta}(y^+_i \,|\, x_i)}{p_{\theta_t}(y^+_i \,|\, x_i)} 
    - \lambda \log \frac{p_{\theta}(y^-_i \,|\, x_i)}{p_{\theta_t,\theta_{\sigma}}^{\text{noise}}(y_i^- \mid x_i)} \right),\ \boldsymbol{\sigma}^2_i < \varepsilon
\end{equation}
To save computational costs further, we do not perform iterative updates for the min-max problem. Instead, we update both $\theta$ and $\theta_{\sigma}$ in a single iteration by minimizing the following object function:
\begin{align}~\label{obj:final}
\min_{\theta, \theta_{\sigma}} \mathcal{L}(\theta, \theta_{\sigma}):= 
& \underbrace{\sum_{i=1}^N \ell\left( \lambda \left[ \log \frac{p_{\theta}(y_i^+ \mid x_i)}{p_{\theta_t}(y_i^+ \mid x_i)} - \log \frac{p_{\theta}(y_i^- \mid x_i)}{p_{\theta_t,\theta_{\sigma}}^{\text{noise}}(y_i^- \mid x_i)'} \right] \right)}_{\text{first term: freeze $\theta_{\sigma}$, maximize positive negative pair margin}} \nonumber \\
& \underbrace{- \sum_{i=1}^N \ell\left( \lambda \left[ \log \frac{p_{\theta}(y_i^+ \mid x_i)}{p_{\theta_t}(y_i^+ \mid x_i)} - \log \frac{p_{\theta}(y_i^- \mid x_i)}{p_{\theta_t,\theta_{\sigma}}^{\text{noise}}(y_i^- \mid x_i)} \right] \right)}_{\text{second term: freeze $\theta$, minimize positive negative pair margin}}
+ \alpha \frac{1}{N}\sum_{i=1}^N \boldsymbol{\sigma}_i^2 
\end{align}
Where $\alpha$ is a hyper-parameter to control the magnitude of the variance. Note that many computations of the first term and the second term of Obj.~\ref{obj:final} are shared, eliminating the need to recompute everything. More specifically, we first compute the first term and store the results of $p_{\theta}(y_i^+ \mid x_i)$, $p_{\theta}(y_i^- \mid x_i)$ and $p_{\theta_t}(y_i^+ \mid x_i)$. For the second term, the feature of the last layer $h_i$ can be reused and only Eq.~\ref{eq:noise-generation} needs to be recomputed. Thus, the overhead of the Obj.~\ref{obj:final} is trivial. Additionally, the noise in $\mathbf{z}_i'$ for $p_{\theta_t,\theta_{\sigma}}^{\text{noise}}(y_i^- \mid x_i)'$ in the first term and for $p_{\theta_t,\theta_{\sigma}}^{\text{noise}}(y_i^- \mid x_i)$ in the second term is independently sampled to better explore the noise space.


Adding trainable noise encourages more creativity in the model's optimization process. It makes the model more robust throughout the self-improvement process and smooths the optimization landscape.

