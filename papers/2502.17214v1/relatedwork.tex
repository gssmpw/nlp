\section{Related Works}
\subsection{Uncertainty Quantification in LLMs}
Prior efforts to quantify uncertainty and confidence in LLMs can be categorized into four main approaches. The first approach is to derive calibrated confidence by examining agreement across multiple sampled responses \citep{malinin2021uncertainty, kuhn2023semantic, manakul2023selfcheckgpt, tian2023fine}. However, as \citet{qiu2024semantic} recently pointed out, these methods primarily quantify prompt-wise rather than response-wise uncertainty. While \citet{qiu2024semantic} provides a method for response-wise uncertainty, it still relies on generating multiple response samples, making it computationally inefficient. The second approach is to leverage LLM's own ability to evaluate the confidence of its responses, often through self-probing techniques \citep{kadavath2022language, tian2023just, xiong2023can}. The third approach is to aggregate token probabilities of its generated response, which includes adopting traditional UQ methods \citep{xiao2022uncertainty, ye2024benchmarking} and assigning importance weights to tokens \citep{duan2024shifting, bakman2024mars}. However, the above two approaches still suffer from overconfidence due to the model’s inherent bias to trust its own outputs \citep{mielke2022reducing, lin2022teaching}. The fourth approach is to fine-tune the original LLM to calibrate its confidence \citep{lin2022teaching, kapoor2024large}. However, the model-specific tuning has limited their applications to new scenarios. In contrast to these four approaches, the proposed CoT-UQ is a response-wise UQ method that does not require response sampling or model-specific tuning. Instead, it leverages the LLM’s inherent reasoning abilities to calibrate uncertainty/confidence scores, making it readily generalizable to new tasks and models. 

\subsection{Chain of Thought Reasoning in LLMs}
To equip LLMs with capabilities to solve more complex and reasoning tasks, \citet{wei2022chain} extended in-context learning by introducing the concept of Chain of Thought (CoT) through a step-by-step reasoning process. \citet{kojima2022large} found that simply adding a leading sentence “Let’s think step by step” to a cue allowed LLMs to perform zero-shot logical reasoning without any additional human prompts \citep{chu2023survey}. Subsequently, CoT-SC~\citep{wang2022self} introduces a self-consistency strategy to replace the greedy decoding strategy. \citet{feng2024towards} further reveals the underlying mechanisms behind CoT through a theoretical perspective. \citet{liu2024era} refines CoT by capturing relationships between entities to aid LLMs in understanding context. Although these studies highlight the importance of CoT in enhancing LLMs’ reasoning abilities in various situations, a recent study \citep{fu2025multiple} observes that CoT can exacerbate the overconfident issues in LLMs when only measuring the final answer. To the best of our knowledge, CoT-UQ is the first approach to integrate reasoning knowledge into the UQ process for LLMs.