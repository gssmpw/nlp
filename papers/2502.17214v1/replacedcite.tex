\section{Related Works}
\subsection{Uncertainty Quantification in LLMs}
Prior efforts to quantify uncertainty and confidence in LLMs can be categorized into four main approaches. The first approach is to derive calibrated confidence by examining agreement across multiple sampled responses ____. However, as ____ recently pointed out, these methods primarily quantify prompt-wise rather than response-wise uncertainty. While ____ provides a method for response-wise uncertainty, it still relies on generating multiple response samples, making it computationally inefficient. The second approach is to leverage LLM's own ability to evaluate the confidence of its responses, often through self-probing techniques ____. The third approach is to aggregate token probabilities of its generated response, which includes adopting traditional UQ methods ____ and assigning importance weights to tokens ____. However, the above two approaches still suffer from overconfidence due to the model’s inherent bias to trust its own outputs ____. The fourth approach is to fine-tune the original LLM to calibrate its confidence ____. However, the model-specific tuning has limited their applications to new scenarios. In contrast to these four approaches, the proposed CoT-UQ is a response-wise UQ method that does not require response sampling or model-specific tuning. Instead, it leverages the LLM’s inherent reasoning abilities to calibrate uncertainty/confidence scores, making it readily generalizable to new tasks and models. 

\subsection{Chain of Thought Reasoning in LLMs}
To equip LLMs with capabilities to solve more complex and reasoning tasks, ____ extended in-context learning by introducing the concept of Chain of Thought (CoT) through a step-by-step reasoning process. ____ found that simply adding a leading sentence “Let’s think step by step” to a cue allowed LLMs to perform zero-shot logical reasoning without any additional human prompts ____. Subsequently, CoT-SC____ introduces a self-consistency strategy to replace the greedy decoding strategy. ____ further reveals the underlying mechanisms behind CoT through a theoretical perspective. ____ refines CoT by capturing relationships between entities to aid LLMs in understanding context. Although these studies highlight the importance of CoT in enhancing LLMs’ reasoning abilities in various situations, a recent study ____ observes that CoT can exacerbate the overconfident issues in LLMs when only measuring the final answer. To the best of our knowledge, CoT-UQ is the first approach to integrate reasoning knowledge into the UQ process for LLMs.