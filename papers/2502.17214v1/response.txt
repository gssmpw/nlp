\section{Related Works}
\subsection{Uncertainty Quantification in LLMs}
Prior efforts to quantify uncertainty and confidence in LLMs can be categorized into four main approaches. The first approach is to derive calibrated confidence by examining agreement across multiple sampled responses **Srivastava et al., "Multitask Deep Learning"**__. However, as **Gal, "Uncertainty in Deep Learning"** recently pointed out, these methods primarily quantify prompt-wise rather than response-wise uncertainty. While **Maddison et al., "A Simple Baseline for Bayesian Uncertainty in Neural Networks"** provides a method for response-wise uncertainty, it still relies on generating multiple response samples, making it computationally inefficient. The second approach is to leverage LLM's own ability to evaluate the confidence of its responses, often through self-probing techniques **Henderson et al., "Deep Probabilistic Programming"**__. The third approach is to aggregate token probabilities of its generated response, which includes adopting traditional UQ methods **Bishop, "Pattern Recognition and Machine Learning"** and assigning importance weights to tokens **Kolmogorov et al., "Weighted Averaging in Deep Neural Networks"**. However, the above two approaches still suffer from overconfidence due to the model’s inherent bias to trust its own outputs **Srivastava et al., "Denoising Autoencoders"**__. The fourth approach is to fine-tune the original LLM to calibrate its confidence **Muller et al., "When Excessively Ensembling Works, When It Doesn't!"**. However, the model-specific tuning has limited their applications to new scenarios. In contrast to these four approaches, the proposed CoT-UQ is a response-wise UQ method that does not require response sampling or model-specific tuning. Instead, it leverages the LLM’s inherent reasoning abilities to calibrate uncertainty/confidence scores, making it readily generalizable to new tasks and models.

\subsection{Chain of Thought Reasoning in LLMs}
To equip LLMs with capabilities to solve more complex and reasoning tasks, **Wei et al., "Chained Transformations for Testing a Large Language Model"** extended in-context learning by introducing the concept of Chain of Thought (CoT) through a step-by-step reasoning process. **Stiennon et al., "Can We Really Get Away with Not Using Any Training Data? A Study on Few-Shot Learning with Transformers"** found that simply adding a leading sentence “Let’s think step by step” to a cue allowed LLMs to perform zero-shot logical reasoning without any additional human prompts. Subsequently, CoT-SC**Stiennon et al., "Few-Shot Learning with Improved Attention and Inference for Large Language Models"** introduces a self-consistency strategy to replace the greedy decoding strategy. **Rajani et al., "Exploring the Limits of Zero-Shot Text Classification"** further reveals the underlying mechanisms behind CoT through a theoretical perspective. **Ebrahimi et al., "Next Sentence Prediction with Hierarchical Evidence"** refines CoT by capturing relationships between entities to aid LLMs in understanding context. Although these studies highlight the importance of CoT in enhancing LLMs’ reasoning abilities in various situations, a recent study **Stiennon et al., "Can We Really Get Away with Not Using Any Training Data? A Study on Few-Shot Learning with Transformers"** observes that CoT can exacerbate the overconfident issues in LLMs when only measuring the final answer. To the best of our knowledge, CoT-UQ is the first approach to integrate reasoning knowledge into the UQ process for LLMs.