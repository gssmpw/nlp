\section{Related Work}
\label{sec:related_work}
In this section, we review existing works relevant to W2SD.

\paragraph{Weak-to-Strong Mechanism} The concept of improving weak models into strong models originates from the AdaBoost~\citep{hogsgaard2023adaboost}, which constructs a more accurate classifier by aggregating multiple weak classifiers. Building upon this, ~\citet{green2022optimal,hogsgaard2023adaboost} introduced a provably optimal weak-to-strong learner, establishing a robust theoretical foundation for this weak-to strong paradigm. In the field of LLMs, several studies~\citep{chenself,burns2023weak} have utilized weak models as supervisory signals to facilitate the alignment of LLMs. This paradigm of weak-to-strong generation during training has similarly been investigated in the context of diffusion model training~\citep{chen2025pixart}. And~\citet{sugiyama2013density} recommended directly estimating the density difference between weak and strong models instead of separate estimations. In this work, we propose W2SD, a general framework that, for the first time, implements weak-to-strong enhancement in diffusion inference through a reflective mechanism.

\paragraph{Diffusion Inference Enhancement}
The study of inference scaling laws in diffusion models has recently become a prominent focus within the research community~\citep{ma2025inference, yetfg,liu2024alignment}. This line of work can be traced back to Re-Sampling~\citep{lugmayr2022repaint}, which iteratively refines latent variables by injecting random Gaussian noise, effectively reverting the noise level to a previous scale. This iterative paradigm has been utilized in subsequent works, including universal conditional control~\citep{bansal2023universal}, video generation~\citep{wu2023tune}, and protein design~\citep{jumper2021highly}, to enhance inference performance. However, it has primarily been treated as a heuristic trick, with its underlying mechanisms remaining underexplored. Z-Sampling~\citep{bai2024zigzag} extended this paradigm by replacing random noise injection with inversion operations and identified the guidance difference between denoising and inversion as a critical factor. This phenomenon has also been validated in subsequent studies~\citep{zhou2024golden, shao2024iv, ahn2024noise}. In our work, we systematically unify these inference enhancement methods, demonstrating that their essence lies in approximating the strong-to-ideal difference via the weak-to-strong difference, and integrate them into a unified reflective framework, W2SD, through theoretical and empirical analysis.