\section{Related Work}
\label{sec:related_work}
In this section, we review existing works relevant to W2SD.

\paragraph{Weak-to-Strong Mechanism} The concept of improving weak models into strong models originates from the AdaBoostFreund, "Experiments on Boosting and NETtalk"__, which constructs a more accurate classifier by aggregating multiple weak classifiers. Building upon this, Breiman, "Prediction Games and Arcing Algorithms"__ introduced a provably optimal weak-to-strong learner, establishing a robust theoretical foundation for this weak-to strong paradigm. In the field of LLMs, several studiesChen et al., "Meta-Learning for Few-Shot Text Classification with Differentiable Neural Computer"__, Qin et al., "Improving Language Understanding by Generative Domains Adaptation"__ have utilized weak models as supervisory signals to facilitate the alignment of LLMs. This paradigm of weak-to-strong generation during training has similarly been investigated in the context of diffusion model trainingHuang et al., "Diffusion-based Deep Unsupervised Learning"__. And Wu et al., "Density Estimation for Unnormalized Models via a Class of Divergences"__ recommended directly estimating the density difference between weak and strong models instead of separate estimations. In this work, we propose W2SD, a general framework that, for the first time, implements weak-to-strong enhancement in diffusion inference through a reflective mechanism.

\paragraph{Diffusion Inference Enhancement}
The study of inference scaling laws in diffusion models has recently become a prominent focus within the research communityLi et al., "Inversion-Induced Sampling"__. This line of work can be traced back to Re-SamplingSong et al., "Re-Sampling: Iterative Refining for Latent Variables"__, which iteratively refines latent variables by injecting random Gaussian noise, effectively reverting the noise level to a previous scale. This iterative paradigm has been utilized in subsequent works, including universal conditional controlChen et al., "Universal Conditional Control via Inversion-Induced Sampling"__, video generationHuang et al., "Video Generation with Diffusion-Based Neural Process"__ and protein designRajput et al., "Diffusion-Based Protein Design through Iterative Refining"__, to enhance inference performance. However, it has primarily been treated as a heuristic trick, with its underlying mechanisms remaining underexplored. Z-SamplingWu et al., "Z-Sampling: Inversion-Induced Sampling for Efficient Inference"__ extended this paradigm by replacing random noise injection with inversion operations and identified the guidance difference between denoising and inversion as a critical factor. This phenomenon has also been validated in subsequent studiesChen et al., "Inversion-Induced Sampling for Efficient Inference"__. In our work, we systematically unify these inference enhancement methods, demonstrating that their essence lies in approximating the strong-to-ideal difference via the weak-to-strong difference, and integrate them into a unified reflective framework, W2SD, through theoretical and empirical analysis.