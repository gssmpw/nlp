@article{ahn2024noise,
  title={A Noise is Worth Diffusion Guidance},
  author={Ahn, Donghoon and Kang, Jiwon and Lee, Sanghyun and Min, Jaewon and Kim, Minjae and Jang, Wooseok and Cho, Hyoungwon and Paul, Sayak and Kim, SeonHwa and Cha, Eunju and others},
  journal={arXiv preprint arXiv:2412.03895},
  year={2024}
}

@article{bai2024zigzag,
  title={Zigzag Diffusion Sampling: Diffusion Models Can Self-Improve via Self-Reflection},
  author={Bai, Lichen and Shao, Shitong and Zhou, Zikai and Qi, Zipeng and Xu, Zhiqiang and Xiong, Haoyi and Xie, Zeke},
  journal={arXiv preprint arXiv:2412.10891},
  year={2024}
}

@inproceedings{bansal2023universal,
  title={Universal guidance for diffusion models},
  author={Bansal, Arpit and Chu, Hong-Min and Schwarzschild, Avi and Sengupta, Soumyadip and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={843--852},
  year={2023}
}

@article{burns2023weak,
  title={Weak-to-strong generalization: Eliciting strong capabilities with weak supervision},
  author={Burns, Collin and Izmailov, Pavel and Kirchner, Jan Hendrik and Baker, Bowen and Gao, Leo and Aschenbrenner, Leopold and Chen, Yining and Ecoffet, Adrien and Joglekar, Manas and Leike, Jan and others},
  journal={arXiv preprint arXiv:2312.09390},
  year={2023}
}

@inproceedings{chen2025pixart,
  title={PIXART-\(\Sigma\): Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation
},
  author={Chen, Junsong and Ge, Chongjian and Xie, Enze and Wu, Yue and Yao, Lewei and Ren, Xiaozhe and Wang, Zhongdao and Luo, Ping and Lu, Huchuan and Li, Zhenguo},
  booktitle={European Conference on Computer Vision},
  pages={74--91},
  year={2025},
  organization={Springer}
}

@inproceedings{chenself,
  title={Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models},
  author={Chen, Zixiang and Deng, Yihe and Yuan, Huizhuo and Ji, Kaixuan and Gu, Quanquan},
  booktitle={Forty-first International Conference on Machine Learning}
}

@article{green2022optimal,
  title={Optimal weak to strong learning},
  author={Green Larsen, Kasper and Ritzert, Martin},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={32830--32841},
  year={2022}
}

@inproceedings{hogsgaard2023adaboost,
  title={Adaboost is not an optimal weak to strong learner},
  author={H{\o}gsgaard, Mikael M{\o}ller and Larsen, Kasper Green and Ritzert, Martin},
  booktitle={International Conference on Machine Learning},
  pages={13118--13140},
  year={2023},
  organization={PMLR}
}

@article{jumper2021highly,
  title={Highly accurate protein structure prediction with AlphaFold},
  author={Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v{Z}}{\'\i}dek, Augustin and Potapenko, Anna and others},
  journal={nature},
  volume={596},
  number={7873},
  pages={583--589},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{liu2024alignment,
  title={Alignment of diffusion models: Fundamentals, challenges, and future},
  author={Liu, Buhua and Shao, Shitong and Li, Bao and Bai, Lichen and Xu, Zhiqiang and Xiong, Haoyi and Kwok, James and Helal, Sumi and Xie, Zeke},
  journal={arXiv preprint arXiv:2409.07253},
  year={2024}
}

@inproceedings{lugmayr2022repaint,
  title={Repaint: Inpainting using denoising diffusion probabilistic models},
uthor={Lugmayr, Andreas and Danelljan, Martin and Romero, Andres and Yu, Fisher and Timofte, Radu and Van Gool, Luc},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={11461--11471},
  year={2022}
}

@article{ma2025inference,
  title={Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps},
  author={Ma, Nanye and Tong, Shangyuan and Jia, Haolin and Hu, Hexiang and Su, Yu-Chuan and Zhang, Mingda and Yang, Xuan and Li, Yandong and Jaakkola, Tommi and Jia, Xuhui and others},
  journal={arXiv preprint arXiv:2501.09732},
  year={2025}
}

@article{shao2024iv,
  title={IV-Mixed Sampler: Leveraging Image Diffusion Models for Enhanced Video Synthesis},
  author={Shao, Shitong and Zhou, Zikai and Bai, Lichen and Xiong, Haoyi and Xie, Zeke},
  journal={arXiv preprint arXiv:2410.04171},
  year={2024}
}

@article{sugiyama2013density,
  title={Density-difference estimation},
  author={Sugiyama, Masashi and Kanamori, Takafumi and Suzuki, Taiji and Du Plessis, Marthinus Christoffel and Liu, Song and Takeuchi, Ichiro},
  journal={Neural Computation},
  volume={25},
  number={10},
  pages={2734--2775},
  year={2013},
  publisher={MIT Press}
}

@inproceedings{wu2023tune,
  title={Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation},
  author={Wu, Jay Zhangjie and Ge, Yixiao and Wang, Xintao and Lei, Stan Weixian and Gu, Yuchao and Shi, Yufei and Hsu, Wynne and Shan, Ying and Qie, Xiaohu and Shou, Mike Zheng},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={7623--7633},
  year={2023}
}

@inproceedings{yetfg,
  title={TFG: Unified Training-Free Guidance for Diffusion Models},
  author={Ye, Haotian and Lin, Haowei and Han, Jiaqi and Xu, Minkai and Liu, Sheng and Liang, Yitao and Ma, Jianzhu and Zou, James and Ermon, Stefano},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems}
}

@article{zhou2024golden,
  title={Golden noise for diffusion models: A learning framework},
  author={Zhou, Zikai and Shao, Shitong and Bai, Lichen and Xu, Zhiqiang and Han, Bo and Xie, Zeke},
  journal={arXiv preprint arXiv:2411.09502},
  year={2024}
}

