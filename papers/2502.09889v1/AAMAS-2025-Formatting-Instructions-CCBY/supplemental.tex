%%%% For anonymized submission, use this
\documentclass[sigconf,anonymous,nonacm]{aamas} 

%%%% For camera-ready, use this
% \documentclass[sigconf,nonacm]{aamas} 

%%% Load required packages here (note that many are included already).
\usepackage{multirow}
\usepackage{dsfont}

\usepackage{balance} % for balancing columns on the final page

%%% == IMPORTANT ==
%%% Use this command to specify your EasyChair submission number.
%%% In anonymous mode, it will be printed on the first page.
%%% Use this command to specify the title of your paper.

\title{Supplementary Material for\\Evaluating and Improving Graph-based Explanation Methods for Multi-Agent Coordination}

%%% Provide names, affiliations, and email addresses for all authors.

%%% Use this environment to specify a short abstract for your paper.

% \begin{abstract}

% \end{abstract}

%%% The code below was generated by the tool at http://dl.acm.org/ccs.cfm.
%%% Please replace this example with code appropriate for your own paper.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Include any author-defined commands here.
         
\newcommand{\BibTeX}{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em\TeX}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%% The following commands remove the headers in your paper. For final 
%%% papers, these will be inserted during the pagination process.

\pagestyle{fancy}
\fancyhead{}

%%% The next command prints the information defined in the preamble.

\maketitle 

\section{Environment Training Details}

\subsection{Navigation}
We train all policies for the navigation task with the hyperparameters in Table \ref{table:navigation-hyperparameters}. We set the weight of the attention entropy term to 10.
% All trained policies follow the architecture described in Table [].
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Hyperparameter} & \textbf{Value} \\ \hline
Learning rate & 0.0003 \\ \hline
Gamma & 0.99999 \\ \hline
Lambda & 0.9 \\ \hline
Entropy epsilon & 0.0001 \\ \hline
Clip epsilon & 0.2 \\ \hline
Critic loss type & Smooth L1 \\ \hline
Normalize advantage & False \\ \hline
Number of epochs & 30 \\ \hline
Max gradient norm & 1.0 \\ \hline
Minibatch size & 800 \\ \hline
Collector iterations & 150 \\ \hline
Frames per batch & 18000 \\ \hline
\end{tabular}
\caption{Navigation Hyperparameters}
\label{table:navigation-hyperparameters}
\end{table}

\subsection{Passage}
We train all policies for the passage task with the hyperparameters in Table \ref{table:passage-hyperparameters}. We set the weight of the attention entropy term to 50.
% All trained policies follow the architecture described in Table [].
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Hyperparameter} & \textbf{Value} \\ \hline
Learning rate & 0.00005 \\ \hline
Gamma & 0.99999 \\ \hline
Lambda & 0.9 \\ \hline
Entropy epsilon & 0.0001 \\ \hline
Clip epsilon & 0.2 \\ \hline
Critic loss type & Smooth L1 \\ \hline
Normalize advantage & False \\ \hline
Number of epochs & 30 \\ \hline
Max gradient norm & 1.0 \\ \hline
Minibatch size & 800 \\ \hline
Collector iterations & 100 \\ \hline
Frames per batch & 60000 \\ \hline
\end{tabular}
\caption{Passage Hyperparameters}
\label{table:passage-hyperparameters}
\end{table}

% \begin{table}[h!]
% \centering
% \begin{tabular}{|c|c|}
% \hline
% Encoder depth & 4 \\ \hline
% Encoder number of cells & 32 \\ \hline
% GNN heads & 1 \\ \hline
% GNN layers & 1 \\ \hline
% GNN convolution layer & GATv2Conv \\ \hline
% GNN hidden dimension & 32 \\ \hline
% Decoder depth & 4 \\ \hline
% Decoder number of cells & 64 \\ \hline
% Critic depth & 2 \\ \hline
% Critic number of cells & 64 \\ \hline
% \end{tabular}
% \caption{Passage Policy Architecture}
% \label{table:passage-policy-arch}
% \end{table}

\subsection{Discovery}
We train all policies for the discovery task with the hyperparameters in Table \ref{table:discovery-hyperparameters}. We set the weight of the attention entropy term to 50.
% All trained policies follow the architecture described in Table \ref{table:discovery-policy-arch}.
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Hyperparameter} & \textbf{Value} \\ \hline
Learning rate & 0.0007 \\ \hline
Gamma & 0.9999 \\ \hline
Lambda & 0.95 \\ \hline
Entropy epsilon & 0.0001 \\ \hline
Clip epsilon & 0.05 \\ \hline
Critic loss type & Smooth L1 \\ \hline
Normalize advantage & True \\ \hline
Number of epochs & 5 \\ \hline
Max gradient norm & 10 \\ \hline
Minibatch size & 10000 \\ \hline
Collector iterations & 1000 \\ \hline
Frames per batch & 10000 \\ \hline
\end{tabular}
\caption{Discovery Hyperparameters}
\label{table:discovery-hyperparameters}
\end{table}

% \begin{table}[h!]
% \centering
% \begin{tabular}{|c|c|}
% \hline
% Encoder depth & 1 \\ \hline
% Encoder number of cells & 64 \\ \hline
% GNN heads & 3 \\ \hline
% GNN layers & 1 \\ \hline
% GNN convolution layer & GATv2Conv \\ \hline
% GNN hidden dimension & 64 \\ \hline
% Decoder depth & 2 \\ \hline
% Decoder number of cells & 64 \\ \hline
% Critic depth & 2 \\ \hline
% Critic number of cells & 128 \\ \hline
% \end{tabular}
% \caption{Discovery Policy Architecture}
% \label{table:discovery-policy-arch}
% \end{table}

% \section{Locality Analysis of Minima for $D(\alpha)$}
% We can provide a rudimentary insight into the neighborhood of the minima of $D(\alpha) = \sum_{i, j} \left| 2\alpha_{i,j} - 1 \right|$. To do this, consider the quantity shown below.
% \begin{equation}
%     \alpha' = [\frac{1}{N} + \epsilon, \frac{1}{N} - \epsilon, \frac{1}{N}, \ldots, \frac{1}{N}]
% \end{equation}
% We can compute $D(\alpha')$ as shown below.
% \begin{align}
%     D(\alpha') &= \left| 2 \left(\frac{1}{N} + \epsilon \right) - 1 \right| + \left| 2 \left(\frac{1}{N} - \epsilon \right) - 1 \right| + (N^2 - 2)\left|\frac{2}{N} - 1 \right| \\
%     &= \left| \frac{2}{N} + 2\epsilon - 1 \right| + \left| \frac{2}{N} - 2\epsilon - 1 \right| + (N^2 - 2)\left|\frac{2}{N} - 1 \right|
% \end{align}
% We can solve for what values of $\epsilon$ does the quantity $D(\alpha') = \left|2N - N^2\right|$ as shown below.
% \begin{align}
%     \left| \frac{2}{N} + 2\epsilon - 1 \right| + \left| \frac{2}{N} - 2\epsilon - 1 \right| + (N^2 - 2)\left|\frac{2}{N} - 1 \right| &= \left|2N - N^2\right|
% \end{align}
% We find that the above equality holds for $\frac{2-N}{2N} \leq \epsilon \leq \frac{N-2}{2N}$ for $N \geq 2$
% This analysis does not hold for large $N$, but it provides insight into the fact that area near the local minima is flat (i.e. it yields a 0 gradient). As a result, optimizing $D(\alpha)$ directly will not yield improvement unless the task reward gradients eventually yield attention values that are outside the $\epsilon$-neighborhood of the local minima. As a result, it would likely be better to optimize for a surrogate function that has non-zero gradients defined throughout (such as entropy for example).

% \section{Brief Statement Regarding Node Embedding Separation}
% We note that a convex function over a convex set 

\section{Positive Fidelity}
\figureautorefname~\ref{fig:positiveFidelity} shows the positive fidelity measures.

\begin{figure*}
    \centering
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-navigation-3_agent-PositiveFidelity.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-navigation-4_agent-PositiveFidelity.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-navigation-5_agent-PositiveFidelity.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-passage-3_agent-PositiveFidelity.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-passage-4_agent-PositiveFidelity.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-passage-5_agent-PositiveFidelity.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-discovery-3_agent-PositiveFidelity.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-discovery-4_agent-PositiveFidelity.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-discovery-5_agent-PositiveFidelity.png}
    \caption{Positive Fidelity ($\uparrow$) of explanations generated by three explanation methods with and without proposed regularization, across three team sizes. \textit{Top row}: Blind navigation task, \textit{Middle row}: Passage task, \textit{Bottom row}: Discovery task. Higher is better.}
    \label{fig:positiveFidelity}
\end{figure*}

\section{Negative Fidelity}
\figureautorefname~\ref{fig:negativeFidelity} shows the negative fidelity measures.

\begin{figure*}
    \centering
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-navigation-3_agent-NegativeFidelity.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-navigation-4_agent-NegativeFidelity.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-navigation-5_agent-NegativeFidelity.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-passage-3_agent-NegativeFidelity.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-passage-4_agent-NegativeFidelity.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-passage-5_agent-NegativeFidelity.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-discovery-3_agent-NegativeFidelity.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-discovery-4_agent-NegativeFidelity.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-discovery-5_agent-NegativeFidelity.png}
    \caption{Negative Fidelity ($\downarrow$) of explanations generated by three explanation methods with and without proposed regularization, across three team sizes. \textit{Top row}: Blind navigation task, \textit{Middle row}: Passage task, \textit{Bottom row}: Discovery task. Lower is better.}
    \label{fig:negativeFidelity}
\end{figure*}

% \section{CL Analysis}
% \tableautorefname~\ref{table:CLPerformance} shows the performance comparison and \figureautorefname~\ref{fig:CL} shows explanation quality when training larger agent teams with curriculum learning.
% \begin{table*}[!ht]
% \centering
% \begin{tabular}{|c|c|c|c|c|c|c|}
% \hline
% \multicolumn{5}{|c|}{\textbf{Blind Navigation}} \\
% \hline
% \multirow{2}{*}{Performance Metric} & \multicolumn{2}{c|}{N = 5} & \multicolumn{2}{c|}{N = 6}\\ % & \multicolumn{2}{c|}{N = 7} \\
%     \cline{2-5}
%      &  No Att. Ent. Min. & Att. Ent. Min. & No Att. Ent. Min. & Att. Ent. Min. \\ %& No Att. Ent. Min. & Att. Ent. Min. \\
% \hline
% Reward ($\uparrow$) & 3.68 $\pm$ 0.68 & 3.89 $\pm$ 0.645 & 4.02 $\pm$ 1.689 & 4.453 $\pm$ 0.74 \\ %& 4.93 $\pm$ 1.37 & 4.77 $\pm$ 1.19 \\
% \hline
% Success Rate ($\uparrow$) & 0.975 $\pm$ 0.158 & 0.975 $\pm$ 0.158 &  0.9 $\pm$ 0.30 & 0.875 $\pm$ 0.33 \\ %& 0.67 $\pm$ 0.47 & 0.9 $\pm$0.30 \\
% \hline
% No Agent Coll. Rate ($\uparrow$) & 0.95 $\pm$ 0.22 & 1.0 $\pm$ 0.0 & 0.775 $\pm$ 0.42 & 0.875 $\pm$ 0.33 \\ %& 0.925 $\pm$ 0.27 & 0.9 $\pm$ 0.30 \\
% \hline
% % No Object Coll. Rate ($\uparrow$) & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 \\
% % \hline
% Makespan ($\downarrow$) & 111.4 $\pm$ 113.27 & 108.775 $\pm$ 78.33 & 127.325 $\pm$ 105.24 & 144.95 $\pm$ 121.008 \\ % & 217.75 $\pm$ 137.53 & 138.9 $\pm$ 99.74 \\
% \hline
% \end{tabular}
% \caption{Performance comparison for navigation when policy trained using simple curriculum learning.}
% \label{table:CLPerformance}
% \end{table*}

% \begin{figure*}
%     \centering
%     \includegraphics[width=0.24\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig_old/boxplot_analysis-navigation-5_agent_CL-v1-PositiveFidelity.png}
%     \includegraphics[width=0.24\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig_old/boxplot_analysis-navigation-5_agent_CL-v1-NegativeFidelity.png}
%     \includegraphics[width=0.24\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig_old/boxplot_analysis-navigation-5_agent_CL-v1-DeltaFidelity.png}
%     \includegraphics[width=0.24\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig_old/boxplot_analysis-navigation-5_agent_CL-v1-Unfaithfulness.png}
%     \includegraphics[width=0.24\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig_old/boxplot_analysis-navigation-6_agent_CL-v2-PositiveFidelity.png}
%     \includegraphics[width=0.24\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig_old/boxplot_analysis-navigation-6_agent_CL-v2-NegativeFidelity.png}
%     \includegraphics[width=0.24\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig_old/boxplot_analysis-navigation-6_agent_CL-v2-DeltaFidelity.png}
%     \includegraphics[width=0.24\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig_old/boxplot_analysis-navigation-6_agent_CL-v2-Unfaithfulness.png}
%     % \includegraphics[width=0.24\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig_old/boxplot_analysis-navigation-7_agent_CL-PositiveFidelity.png}
%     % \includegraphics[width=0.24\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig_old/boxplot_analysis-navigation-7_agent_CL-NegativeFidelity.png}
%     % \includegraphics[width=0.24\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig_old/boxplot_analysis-navigation-7_agent_CL-DeltaFidelity.png}
%     % \includegraphics[width=0.24\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig_old/boxplot_analysis-navigation-7_agent_CL-Unfaithfulness.png}
%     \caption{Explanation Quality metrics of explanations generated by three explanation methods with and without proposed regularization, across two larger team sizes trained using curriculum learning with $N=4$ policy without attention entropy minimization as its initial weights}
%     \label{fig:CL}
% \end{figure*}

% Given that minimizing attention entropy leads to a larger separation between $G_S(\alpha)$ and $G \setminus G_S(\alpha)$, assuming a static $\phi$, we show that the bound on the distance between the embeddings obtained under $G_S(\alpha)$ and $G \setminus G_S(\alpha)$ is greater when minimizing attention entropy. Without loss of generality, let $\alpha_i$ be the vector of attention applied to each neighbor of node $i$ obtained without attention entropy minimization, and $\alpha_i^{a}$ be the vector of attention obtained with attention entropy minimization. It follows that,
% \begin{align}
%     \sum_{j \in \mathcal{N}(i)}|\alpha_{i,j} - (1-\alpha_{i,j})| \leq \sum_{j \in \mathcal{N}(i)}|\alpha_{i,j}^a - (1-\alpha_{i,j}^a)|
% \end{align}
% The distance between the node embeddings obtained under $G_S(\alpha_i)$ and $G \setminus G_S(\alpha_i)$ can be written as,
% \begin{align}
%    \norm{\sum_{j \in \mathcal{N}(i)} \alpha_{ij} \cdot \phi_{\theta} h_j - (1-\alpha_{ij}) \cdot \phi_{\theta} h_j}
% \end{align}
% The distance between the node embeddings obtained under $G_S(\alpha^a_i)$ and $G \setminus G_S(\alpha^a_i)$ can be written as,
% \begin{align}
%    \norm{\sum_{j \in \mathcal{N}(i)} \alpha^a_{ij} \cdot \phi_{\theta} h_j - (1-\alpha^a_{ij}) \cdot \phi_{\theta} h_j}
% \end{align}
% By the triangle inequality, the upper bound on the embedding distance without attention entropy minimization is,
% \begin{align}
%    \sum_{j \in N(i)} \norm{\alpha_{ij} \cdot \phi_{\theta} h_j - (1-\alpha_{ij}) \cdot \phi_{\theta} h_j}
% \end{align}
% and the upper bound on the embedding distance with attention entropy minimization is given below.
% \begin{align}
%    \sum_{j \in N(i)} \norm{\alpha_{ij}^a \cdot \phi_{\theta} h_j - (1-\alpha_{ij}^a) \cdot \phi_{\theta} h_j}
% \end{align}
% Since $\alpha_{ij}$ is a scalar, we simplify the upper bound without attention entropy minimization to the following.
% \begin{align}
%    \sum_{j \in N(i)} |(\alpha_{ij} - (1-\alpha_{ij}))| \norm{\phi_{\theta} h_j}
% \end{align}
% and similarly the upper bound on the embedding distance with attention entropy minimization,
% \begin{align}
%    \sum_{j \in N(i)} |(\alpha_{ij}^a - (1-\alpha_{ij}^a))| \norm{\phi_{\theta} h_j}
% \end{align}
% From (1), it follows that
% \begin{align}
%     \sum_{j \in N(i)} |(\alpha_{ij} - (1-\alpha_{ij}))| \norm{\phi_{\theta} h_j} \leq \sum_{j \in N(i)} |(\alpha_{ij}^a - (1-\alpha_{ij}^a))| \norm{\phi_{\theta} h_j}
% \end{align}

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%