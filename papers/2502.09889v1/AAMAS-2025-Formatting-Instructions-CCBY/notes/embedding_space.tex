%  This LaTeX template is based on T.J. Hitchman's work which is itself based on Dana Ernst's template.  
% 
% --------------------------------------------------------------
% Skip this stu ff, and head down to where it says "Start here"
% --------------------------------------------------------------
 
\documentclass[12pt]{article}

\usepackage{xcolor}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb}
\usepackage{graphicx}
\newenvironment{statement}[2][Statement]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\begin{document}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\newcommand\sk[1]{{\color{red}{#1}}}
\newcommand\sj[1]{{\color{blue}{#1}}}
% --------------------------------------------------------------
%
%                         Start here
%
% --------------------------------------------------------------
 
\title{Attention Entropy Minimization and the Graph Attention Layer} % replace with the problem you are writing up
% \author{Your Name} % replace with your name
\maketitle
We attempt to present an intuition around the effect of attention entropy minimization on the node embedding produced by a graph attention layer. We hypothesize that attention entropy minimization induces node embeddings to be further apart in the embedding space, which encourages a separation in the predicted action distributions under the explanation sub-graph and its complement, resulting in a higher delta fidelity. \\
\newline
Consider the Graph Attention Layer's node embedding update for node $i$.
\begin{align*}
    h_i^{'} = \sk{\sigma}\left( \sum_{j \in \mathcal{N}(i)} \alpha_{ij} \cdot \phi_{\theta} h_j \right)
\end{align*}
\newline
\sk{I think as long as $\sigma$ is a monotonic (pointwise nonlinear) operator, then doing analysis on the internal component will suffice.}
We assume a fully connected graph $\mathcal{G(V,E)}$, and the edges are weighted by an attention function $e(h_i, h_j) = \alpha_{i,j}$. Let (1) denote the difference between $h_i^{'}$ under the graph $\mathcal{G}$ and its complement $\overline{\mathcal{G}}$
\begin{align}
   \norm{\sum_{j \in N(i)} \alpha_{ij} \cdot \phi_{\theta} h_j -\sum_{j \in N(i)} (1-\alpha_{ij}) \cdot \phi_{\theta} h_j}
\end{align}
Next, let (2) denote the difference between $h_i^{'}$ under the graph $\mathcal{G}$ and its complement $\overline{\mathcal{G}}$ when attention entropy minimization is applied,
\begin{align}
   \norm{\sum_{j \in N(i)} \alpha_{ij}^a \cdot \phi_{\theta} h_j - \sum_{j \in N(i)} (1-\alpha_{ij}^a) \cdot \phi_{\theta} h_j}
\end{align}
where the superscript $a$ indicates that attention entropy minimization was applied during the training process.\\
\newline
Let $\alpha_i$ be the vector of attention applied to each neighbor of $i$ obtained without attention entropy minimization, and $\alpha_i^{a}$ be the vector of attention obtained with attention entropy minimization. We expect that if attention entropy minimization is totally successful, then for any $j \in N(i)$ \sk{This seems to be true but I cannot figure out a way to show this property exists from entropy minimization (namely $H(\alpha) \geq H(\alpha^a)$. Also, I believe this does not hold for any $j$, but may hold in an aggregate sense such as sum or mean across j.} \sj{This makes sense. I originally used this assumption because I couldn't determine a way to analyze the bounds when making the assumption of the aggregate sense you mentioned, and the lowest entropy attention distribution would just be a one-hot vector, in which case this assumption is true (I think). I left my current idea for showing the same result for the sum across j in blue.}
\sk{The original statement may hold true if the attention values are sorted?} \sj{TODO: add Siva's analysis about minimizing attention entropy being equivalent to maximizing the distance between $\alpha$ and $(1-\alpha)$ to justify (3)}.
\begin{align}
    |\alpha_{i,j} - (1-\alpha_{i,j})| \leq |\alpha_{i,j}^a - (1-\alpha_{i,j}^a)|
\end{align}
\sj{
\begin{align*}
    \sum_{j \in \mathcal{N}(i)}|\alpha_{i,j} - (1-\alpha_{i,j})| \leq \sum_{j \in \mathcal{N}(i)}|\alpha_{i,j}^a - (1-\alpha_{i,j}^a)|
\end{align*}
\newline
}
since attention entropy minimization encourages attention concentration (i.e. the opposite of a uniform distribution), the attention values tend toward 0 or 1. Simplifying the difference of sums in (1), (2), we obtain (4) and (5) respectively.
\begin{align}
   \norm{\sum_{j \in \mathcal{N}(i)} \alpha_{ij} \cdot  \phi_{\theta} h_j - (1-\alpha_{ij}) \cdot \phi_{\theta} h_j}
\end{align}

\begin{align}
   \norm{\sum_{j \in \mathcal{N}(i)} \alpha_{ij}^a \cdot  \phi_{\theta} h_j - (1-\alpha_{ij}^a) \cdot \phi_{\theta} h_j}
\end{align}
By the triangle inequality, we can analyze the upper bounds on the embedding distances with and without entropy. The upper bound on the embedding distance (4) without attention to entropy minimization is given below.
\begin{align}
   \norm{\sum_{j \in N(i)} \alpha_{ij} \cdot \phi_{\theta} h_j - (1-\alpha_{ij}) \cdot  \phi_{\theta} h_j} \leq \sum_{j \in N(i)} \norm{\alpha_{ij} \cdot \phi_{\theta} h_j - (1-\alpha_{ij}) \cdot \phi_{\theta} h_j}
\end{align}
and the upper bound on the embedding distance with attention entropy minimization is given below.
\begin{align}
   \norm{\sum_{j \in N(i)} \alpha_{ij}^a \cdot \phi_{\theta} h_j - (1-\alpha_{ij}^a) \cdot \phi_{\theta} h_j} \leq \sum_{j \in N(i)} \norm{\alpha_{ij}^a \cdot \phi_{\theta} h_j - (1-\alpha_{ij}^a) \cdot \phi_{\theta} h_j}
\end{align}
\sj{
Given that $\alpha_{ij}$ is a scalar, we can further simplify the upper bound without attention entropy minimization to,
\begin{align}
   \norm{\sum_{j \in N(i)} \alpha_{ij} \cdot \phi_{\theta} h_j - (1-\alpha_{ij}) \cdot \phi_{\theta} h_j} &\leq \sum_{j \in N(i)} \norm{(\alpha_{ij} - (1-\alpha_{ij})) \cdot \phi_{\theta} h_j} \\
   &\leq  \sum_{j \in N(i)} |(\alpha_{ij} - (1-\alpha_{ij}))| \norm{\phi_{\theta} h_j}
\end{align}
and similarly the upper bound on the embedding distance with attention entropy minimization,
\begin{align}
   \norm{\sum_{j \in N(i)} \alpha_{ij}^a \cdot \phi_{\theta} h_j - (1-\alpha_{ij}^a) \cdot \phi_{\theta} h_j} &\leq \sum_{j \in N(i)} \norm{(\alpha_{ij}^a - (1-\alpha_{ij}^a)) \cdot \phi_{\theta} h_j} \\
   &\leq  \sum_{j \in N(i)} |(\alpha_{ij}^a - (1-\alpha_{ij}^a))| \norm{\phi_{\theta} h_j}
\end{align}
}
Assuming (3), it follows that the upper bound on the embedding distance with attention entropy minimization (7) is greater than or equal to the embedding distance without attention entropy minimization (6). Thus, attention entropy minimization can be interpreted as a stronger bias for node embeddings to be further apart in the embedding space.
\newline
\sj{
An idea for pushing up the lower bound, assuming the norm of the embeddings is 1: we want to show that
\begin{align*}
    max_k \left(|2\alpha_{ik} - 1| - \sum_{j \neq k} |2 \alpha_{ij} - 1| \right) \leq max_k \left(|2\alpha^a_{ik} - 1| - \sum_{j \neq k} |2 \alpha^a_{ij} - 1| \right)
\end{align*}
Under attention entropy minimization, we assume the following  (idk how to show this but it seems reasonable that the maximum value of attention would be greater),
\begin{align}
    |2\alpha^a_{ik} - 1| > |2\alpha_{ik} - 1|
\end{align}
Next, we know that, (without loss of generality to $\alpha^a$)
\begin{align}
    \sum_{j \in \mathcal{N}(i)} \alpha_{ij} = 1 \\
    \sum_{j \in \mathcal{N}(i)} 1 - \alpha_{ij} = N - 1
\end{align}
Where $N = |\mathcal{N}(i)|$. Therefore,
\begin{align}
    \sum_{j \in \mathcal{N}(i)} |2\alpha_{ij} - 1| &\geq  \left| \sum_{j \in \mathcal{N}(i)} 2\alpha_{ij} - 1 \right| \\
    &\geq \left| N - 2 \right| \\
    &\geq  N - 2,~\forall N \geq 2
\end{align}
Assuming (12) and knowing that  $\sum_{j \neq k} |2 \alpha_{ij} - 1|$ is bounded from below by (17), it follows that
\begin{align*}
    max_k \left(|2\alpha_{ik} - 1| - \left| \sum_{j \neq k} 2 \alpha_{ij} - 1 \right| \right) \leq max_k \left(|2\alpha^a_{ik} - 1| - \left| \sum_{j \neq k} 2 \alpha^a_{ij} - 1 \right| \right)
\end{align*}
}


 








% --------------------------------------------------------------
%     You don't have to mess with anything below this line.
% --------------------------------------------------------------
 
\end{document}