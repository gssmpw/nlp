%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

\usepackage{balance} % for balancing columns on the final page


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

%%% Load required packages here (note that many are included already).
\usepackage{multirow}
\usepackage{dsfont}

\usepackage{balance} % for balancing columns on the final page

\usepackage{xcolor}
\usepackage[font=small,skip=3pt]{caption}
% \newcommand\sk[1]{{\color{red}{SK:#1}}}
\newcommand\sk[1]{{\color{black}{#1}}}
\newcommand\sj[1]{{\color{blue}{SJ:#1}}}
\newcommand\harish[1]{{\color{purple}{HR:#1}}}

\newcommand{\rulesep}{\unskip\ \vrule\ }

\setlength{\textfloatsep}{5pt}
\setlength{\dbltextfloatsep}{5pt}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Evaluating and Improving Graph-based Explanation Methods for Multi-Agent Coordination}

\begin{document}

\twocolumn[
\icmltitle{Evaluating and Improving Graph-based Explanation Methods \\ for Multi-Agent Coordination}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Siva Kailas}{yyy}
\icmlauthor{Shalin Jain}{yyy}
\icmlauthor{Harish Ravichandar}{yyy}
% \icmlauthor{Firstname4 Lastname4}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
% %\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{School of Interactive Computing, College of Computing, Georgia Institute of Technology, Atlanta, Georgia, United States of America}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Siva Kailas}{skailas3@gatech.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
% This document provides a basic paper template and submission guidelines.
% Abstracts must be a single paragraph, ideally between 4--6 sentences long.
% Gross violations will trigger corrections at the camera-ready phase.
Graph Neural Networks (GNNs), developed by the graph learning community, have been adopted and shown to be highly effective in multi-robot and multi-agent learning. 
% In particular, GNNs have enabled complex multi-agent coordination via communication even under partially observable environments. 
% Inspired by this successful cross-pollination, we explore whether one could adopt GNN explanation methods to explain multi-agent coordination. Specifically, we investigate and characterize the suitability of existing GNN explanation methods for graph attention network (GAT) based policies in multi-agent coordination. 
Inspired by this successful cross-pollination, we investigate and characterize the suitability of existing GNN explanation methods for explaining multi-agent coordination.
We find that these methods have the potential to identify the most-influential communication channels that impact the team's behavior. 
Informed by our initial analyses, we propose an attention entropy regularization term that renders GAT-based policies more amenable to existing graph-based explainers. 
Intuitively, minimizing attention entropy incentivizes agents to limit their attention to the most influential or impactful agents, thereby easing the challenge faced by the explainer. 
% in identifying the most-influential interactions.
We theoretically ground this intuition by showing that minimizing attention entropy increases the disparity between the explainer-generated subgraph and its complement. 
% allows the learned node embeddings produced by the explainer-generated attention-based subgraph and its complement to move further apart.
% an inverse relationship between attention entropy and the distance between the learned node embeddings produced by the attention-based subgraph and its complement. 
Evaluations across three tasks and three team sizes i) provides insights into the effectiveness of existing explainers, and ii) demonstrates that our proposed regularization consistently improves explanation quality without sacrificing task performance. 
% with negligible impact on task performance.
\end{abstract}

\section{Introduction}
\label{related:GNN_MAC}
% \sk{Outline:
% \begin{enumerate}
%     \item Growing body of MARL literature to enable cooperative AI
%     \item Scalability issue arises when training with NNs
%     \item Policy trained with typical NN architecture only compatible with that team size
%     \item GNNs introduced from graph learning community and applied in MARL case
%     \item Led to scalable policies that allow for one architecture for multiple team sizes
%     \item Generating explanations about agent decisions is important
%     \item Existing work on XAI focus either on single-agent or multi-agent with typical NN architectures
%     \item Analogous to how GNNs were transposed over from large graph data/supervised learning to multi-agent learning, can GNN explanation methods transpose similarly?
%     \item We study this and derive observations and useful training modifications to better improve explanation generation for multi-agent GNN policies
%     \item Contributions:
%     \begin{enumerate}
%         \item Empirical of subgraph explanation quality extracted from GNN-based RL policy for multi-agent coordination
%         \item Attention entropy minimization as a means of improving explanation fidelity and faithfulness of continuous subgraph explanations for GNN-based multi-agent policies
%     \end{enumerate}
% \end{enumerate}
% }

Graph neural networks (GNNs) were originally developed to analyze complex relational data~\cite{wu2020comprehensive}. However, they were quickly adopted by various other communities due to their ability to capture structural information and reason over non-euclidean spaces while remaining invariant to certain distractors. The fields of multi-agent and multi-robot learning were among the beneficiaries of these powerful techniques, enabling scalable policies that encode team size-invariant strategies for inter-robot communication and coordination. Indeed, researchers have demonstrated that GNNs are effective in solving a variety of core challenges in multi-agent coordination \cite{coordination-graphs}, such as information aggregation~\cite{nayak2023scalable}, decentralization~\cite{ji2021decentralized}, and learning to communicate~\cite{sheng2022learning}.
This has instantiated into adoption of RL trained GNN-based policies with overall similar design choices in the multi-robot community to tackle practical applications such as cooperative navigation \cite{porok-path-plan, magat}, coverage control \cite{coverage-control}, autonomous driving \cite{gat-auto-drive}, and real-world multi-robot coordination \cite{passageProrok}.
% (see Sec. \ref{related:GNN_MAC}). 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/XAI-GNN-MAS-block_diagram.png}
    \caption{We systematically evaluate existing graph-based explainers when used to explain multi-agent coordination, and propose a regularizer to improve the explainability of GNN-based policies.}
    \label{fig:blockdiagram}
\end{figure}

Despite the impressive strides in multi-agent learning fueled by GNNs, the inner workings of the learned policies remain opaque to most users. GNN-based coordination policies are hard to introspect and analyze, giving rise to unexplained agent behaviors and decision making strategies. In contrast, other fields that analyze and study graph-structured data (e.g., material science~\cite{reiser2022graph} and chemistry~\cite{yang2021predicting}) enjoy improved transparency and explainability, thanks to recent developments in graph-based explanation methods~\cite{gnnexplainer}.
These aforementioned works, and the graph learning community in general, have been pursuing explanations of GNNs with respect to supervised learning, and typically do so on large graph data. Thus, analogous to how the multi-agent/multi-robot community has evaluated and transposed the use of GNNs from the graph learning community for learning coordination for different tasks (as evidenced by the aforementioned related works), we seek to do a similar evaluation with GNN-based post-hoc explainers. 
% Thus, we adopt the core elements across these aforementioned works and evaluate the explanation quality generated by these methods. 
This evaluation, combined with the insights presented to potentially improve the explanation quality, can be used to address a much-needed explanation paradigm for multi-agent coordination (see \cite{brandao2022explainability}.
and \sectionautorefname~\ref{sec:related_works}).

In this work, we explore whether one could adopt existing GNN explanation methods to explain multi-agent coordination. We systematically investigate and characterize the suitability of existing GNN explanation methods for graph attention network (GAT) based policies in multi-agent coordination. If we could explain GNN-based coordination policies, users can effectively debug the learning algorithm by comparing observed coordination strategies against their expectations. Further, explanations would help non-experts gain insights into learned coordination policies. 

We study GNN explainers for multi-agent coordination since they estimate parsimonious yet representative subgraphs as a means to explain complex decision making over graphs. 
% These subgraphs reveal valuable information about inter-agent influences on collective decision making. 
% As these methods have the potential to identify the most influential interactions that can effectively approximate and distill coordination strategies learned over the entire graph. 
When translated to multi-agent coordination, extracting such subgraphs can help identify the most influential inter-agent interactions that can effectively approximate and distill coordination strategies learned across the entire team. 
In fact, identifying such influential interactions was found to be a key challenge in explaining coordination strategies by a recent user study focused on multi-agent navigation~\cite{brandao2022explainability}. Further, these explanation methods are agnostic to the learning algorithm or paradigm used to train GNNs and do not interfere with training or task performance.
% Given that graph-based explainers estimate parsimonious yet representative subgraphs as explanations, they are viable candidates to identify the most influential inter-agent interactions.

% In particular, we study three prominent post-hoc subgraph-based explanation methods: i) Graph Mask~\cite{graphmask}, ii) GNN-Explainer~\cite{gnnexplainer} and iii) Attention Explainer~\cite{??}. These approaches are post-hoc as they are deployed after the GNNs are trained and thus do not impact its training.

In particular, we analyzed three prominent post-hoc graph-based explainers (Graph Mask~\cite{graphmask}, GNN-Explainer~\cite{gnnexplainer} and Attention Explainer~\cite{Fey/Lenssen/2019}) across three multi-agent tasks (blind navigation, constrained navigation, and search and rescue) and measured the quality of their explanations using established explanation metrics (e.g., fidelity~\cite{amara2022graphframex} and faithfulness~\cite{agarwal2023evaluating}). 

Our findings suggest that graph-based explainers have the potential to explain learned inter-agent influences in GNN-based coordination policies. 
% These approaches essentially help identify the most influential communication channels between agents impacting the team's behavior. 
However, our analysis also revealed that there is room for improvement in the quality of explanations generated by these approaches.
% might not be very effective when directly employed without modifications.

In addition to our systematic analyses, we propose a simple regularization technique for training graph-based coordination policies in an effort to make learned policies more amenable to existing explanation methods. Specifically, we introduce an attention entropy minimization objective that can be used as a regularizer when training graph attention networks (GATs). Intuitively, minimizing attention entropy will ``narrow" each agent's attention to the most influential or impactful neighbors. In turn, this more focused attention simplifies the core problem faced by graph-based explainers: identifying the most-influential inter-agent interactions.

Our theoretical analysis shows that minimizing attention entropy increases the disparity between the subgraph generated based on attention values and its complement. This aligns well with intuition and helps explain the proposed regularizer's potential to improve explanation quality.

% Rigorous evaluation of the impact of the proposed attention entropy minimization. 
% on the three existing graph-based explainers and across the three multi-agent coordination tasks. 
Rigorous empirical evaluations across three tasks and differing team sizes suggest that our regularization approach is remarkably effective on Attention Explainer, transforming a simple explanation method into the best-performing method in terms of explanation quality across all tasks.
We find that pairing minimizing attention entropy with the other two explainers tends to improve at least one aspect of their explanation quality.
% Our results suggest that the fidelity and faithfulness of generated explanations are considerably improved by the proposed regularization technique across all three tasks, with different explanation methods yielding different levels of improvement. 
Further, we find our regularization's improvements to explanation quality come with negligible impact on task performance. 
Our theoretical and empirical analyses provide a strong foundation and take the first steps towards interpretable and explainable graph-based policies for multi-agent coordination.

In summary, our contributions include: i) insights from a systematic evaluation of existing post-hoc graph-based explainers when used to explain GNN-based multi-agent coordination, and ii) a theoretically-grounded attention entropy regularization scheme that is shown to improve the explainability of learned GNN-based policies.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related Works}
\label{sec:related_works}
% \sk{Outline:
% \begin{enumerate}
%     % \item Graph Neural Networks
%     % \begin{itemize}
%     %     \item Graph Convolution Networks (https://arxiv.org/abs/1609.02907)
%     %     \item Graph Attention Networks (https://arxiv.org/abs/1710.10903, https://arxiv.org/abs/2105.14491)
%     % \end{itemize}
%     \item Multi-agent explanation methods
%     \begin{itemize}
%         \item https://arxiv.org/pdf/2311.11955, https://arxiv.org/pdf/2204.12568, https://arxiv.org/pdf/2305.10378 operate using policy abstraction and typically grounded in the task at hand. Do not focus on explaining agent-agent influences for team decisions
%         \item https://arxiv.org/pdf/2008.01508 not peer-reviewed, but this along with https://arxiv.org/pdf/2204.12568, https://arxiv.org/pdf/2305.10378 are specific to MARL
%         \item MAPF-specific/motion planning-specific (and not doing model analysis): https://mortezalahijanian.com/papers/AAMAS2020.pdf, https://arxiv.org/abs/2202.09930, https://etdm2020.github.io/abstracts/RSS\_2020\_ETDM\_Kottinger\_Explainable.pdf
%     \end{itemize}
%     \item GNN-based explanation methods
%     \begin{itemize}
%         \item GNNExplainer (http://arxiv.org/abs/1903.03894)
%         \item GraphMask (https://arxiv.org/abs/2010.00577)
%         \item PGExplainer (https://arxiv.org/abs/2011.04573)
%         \item SubgraphX (https://arxiv.org/abs/2102.05152)
%         \item PGM-Explainer (https://proceedings.neurips.cc/paper/2020/hash/8fb134f258b1f7865a6ab2d935a897c9-Abstract.html)
%         \item XGNN (https://dl.acm.org/doi/abs/10.1145/3394486.3403085)
%         \item CF-GNNExplainer (https://proceedings.mlr.press/v151/lucic22a.html)
%     \end{itemize}
%     \item GNN-based policies for multi-agent systems (all of these works use a agent-agent network as the graph input)
%     \begin{itemize}
%         \item https://arxiv.org/pdf/2111.01777 (passage task)
%         \item https://arxiv.org/pdf/1912.06095, https://arxiv.org/pdf/2011.13219 (Prorok gnn path planning)
%         \item https://ieeexplore.ieee.org/abstract/document/9676458 (collaborative perception)
%         \item https://ieeexplore.ieee.org/abstract/document/9811854 (Coverage control)
%         \item https://www.ifaamas.org/Proceedings/aamas2021/pdfs/p764.pdf (Coordination in benchmark MARL tasks)
%     \end{itemize}
% \end{enumerate}
% }

% Below, we discuss different categories of related work and contextualize our contribution within each category.

\textbf{Explainable multi-agent coordination}:
% \label{xaimacRelatedWorks}
Since explainable multi-agent coordination was proposed as a new research direction recently \cite{kraus2020ai}, only a few works have explored this problem. 
One line of work investigates explanations via policy abstraction using a multi-agent MDP~\cite{boggess2022toward, boggess2023explainable}. Similarly, policy summarization and the use of landmarks to condition and convey the high-level strategy is also being explored~\cite{pandya2024multi}. These works rely on text and visual modalities, and are focused more on high-level explanations. As a result, they do not focus on explaining or distilling key interactions among agents.
\sk{\textbf{In fact, explanations that capture critical and affected agents (and agent-agent influences) were found to be desirable in a user-study by \cite{brandao2022explainability} for multi-agent/multi-robot navigation-based tasks.}}
While a different line of work has proposed generating easily verifiable multi-agent path finding plans as explainable~\cite{almagor2020explainable, kottinger2022conflict, kottingerexplainable}, they inherently do not capture key agents or interactions as desired and are limited to specific problem representations of multi-agent navigation. These gaps in the literature strongly motivate the idea of identifying subgraphs of agents to outline the most relevant agents and their interactions among one another. Toward this, we investigate and improve the utility of graph-based explanations of GNN-based multi-agent policies.
% could be a reasonable strategy for fulfilling this desired explanation paradigm. 

% \subsection{GNN-based multi-agent coordination}
% \label{related:GNN_MAC}
% \sk{
% Graph Neural Networks have become prominent in MARL due to their scalability and demonstrated success in facilitating inter-agent communication. They have been utilized to overcome partial observability and enable generalization to varying team sizes in tasks such as cooperative navigation \cite{porok-path-plan, magat}, coverage control \cite{coverage-control}, autonomous driving \cite{gat-auto-drive}, and real-world multi-robot coordination \cite{passageProrok}. Further, GNNs have been shown to be more effective than feed-forward architectures at integrating information from surrounding agents, leading to improved performance in benchmark MARL tasks such as Predator-Prey, Traffic Junction, and Star-Craft \cite{coordination-graphs}.

% % Across these works, an agent-agent graph and a representation of each agent's local observations are provided as inputs to the GNN as the adjacency matrix and the node features. Single or several rounds of message passing are conducted through graph convolutional layers, allowing agents to integrate information from surrounding agents into their own node representation (such as local observations), analogous to communication channels. This node representation is then utilized for task completion, typically via a local decoder structure such as a multi-layer perceptron with shared parameters/weights. This effectively conditions the action of the ego agent on the observations of surrounding agents in addition to the ego agent.

% Since the decisions of one agent are influenced by the information communicated by neighboring agents, it is challenging to interpret GNN-based policies without the help of explanations. However, the graph learning community has been pursuing explanations of GNNs with respect to supervised learning on large graph data recently. \textbf{Analogous to how the multi-agent/multi-robot community has evaluated and transposed the use of GNNs from the graph learning community for learning coordination for different tasks (as evidenced by the aforementioned related works), we seek to do a similar evaluation with GNN-based post-hoc explainers. 
% % Thus, we adopt the core elements across these aforementioned works and evaluate the explanation quality generated by these methods. 
% This evaluation, combined with the insights presented to potentially improve the explanation quality, can be used to address a much-needed explanation paradigm for multi-agent coordination \cite{brandao2022explainability}.
% % and \sectionautorefname~\ref{xaimacRelatedWorks}.
% }
% }

\textbf{Graph-based explanation methods}:
% \label{related:GNN_XAI}
The prevalent approach to explaining GNNs is to identify the subgraph most relevant to its prediction~\cite{graphmask, gnnexplainer, luo2020parameterized, yuan2022explainability}. In addition to these methods, other approaches include generating probabilistic graphical models to explain GNN predictions \cite{vu2020pgm}, identifying graph patterns and motifs \cite{yuan2020xgnn}, and generating counterfactual subgraphs representing the minimal perturbation such that the GNN prediction changes \cite{lucic2022cf}. These methods have demonstrated success in explaining the predictions of GNNs on synthetic graph datasets such as Tree-Cycles and BA-shapes \cite{gnnexplainer}, and large real-world graph datasets such as MUTAG \cite{debnath1991structure}. However, their ability to explain graph-based multi-agent policies remains unexplored. Motivated by the potential of subgraph explanations to identify the most influential channels of communication within a team, we focus on subgraph identification methods. To explore differing subgraph explanations, we use GraphMask to obtain binary mask subgraphs \cite{graphmask}, GNN-Explainer to obtain continuous mask subgraphs \cite{gnnexplainer}, and Attention-Explainer to obtain attention-based subgraphs for Graph Attention Networks \cite{Fey/Lenssen/2019}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Preliminaries}
% In this section, we briefly discuss the relevant background for this study.
% \sk{
% \begin{enumerate}
%     \item Preliminaries
%     \begin{enumerate}
%         \item Description of MARL, multi-agent POMDP
%         \item Description of post-hoc explanation paradigm, including some notion that graph and graph complement separability is important
%     \end{enumerate}
%     \item Methodology
%     \begin{enumerate}
%         \item Policy parameterized using Graph Attention with action decoder and optional observation encoder
%         \item We choose GATv2 as it offers attention values (which can be interpreted as a form of explanation) and also to allow for attention entropy mechanism
%         \item We use MAPPO with critic parameterized using MLP
%         \item We use VMAS environment and train from scratch policies for 3, 4, and 5 agents
%         \item Additionally, we leverage a curriculum that involves model reuse + a simple recipe to pre-train the critic with frozen actor to scale to larger number of agents with lower sample complexity
%     \end{enumerate}
%     \item Metrics
%     \begin{enumerate}
%         \item Delta Fidelity
%         \item Unfaithfulness
%         \item Stability
%         \item Sparsity
%         \item Reward
%         \item Success Rate
%         \item Makespan (if applicable)
%         \item Collision Rate (if applicable)
%     \end{enumerate}
% \end{enumerate}
% }

\textbf{Multi-Agent Coordination}:
We consider a class of multi-agent coordination problems that can be modeled as Decentralized Partially Observable Markov Decision Processes (POMDP). They are described by the tuple $(\mathcal{D, S, A, T, R, O})$, where $\mathcal{D}$ is the set of N agents, $\mathcal{S}$ is the set global states, $\mathcal{A}$ is the set of actions each agent can take, $\mathcal{T}$ is the state transition model, $\mathcal{R}$ is the global reward function, and $\mathcal{O}$ is the observation model. We aim to learn decentralized action policies $\pi(a_i|o_{\mathcal{N}(i)})$ that maximize the expected return $\mathds{E}[\sum^T_{t=0} r_t]$, where each agent's policy is conditioned its own observations and the messages received from its neighbors through a graph network. We include the ego agent in its own set of neighbors.

\textbf{Graph Neural Networks}:
We consider policies that contain a convolutional Graph Neural Network (GNN). GNNs consist of $L$ layers of graph convolutions, followed by a nonlinear activation~\cite{gcn}. GNNs facilitate learned communication between agents over the communication graph $G\mathcal{(V, E)}$, where $\mathcal{V} = \{v_1, ... v_N\}$ is the set of $N$ nodes representing agents and $(v_i, v_j) \in \mathcal{E}$ indicates a communication link between agents $i$ and $j$. A single Graph Convolution Layer is given by $h_i^{'} = \sum_{j \in \mathcal{N}(i)} \frac{1}{\sqrt{deg(i)deg(j)}}\phi_\theta h_j$
% \begin{align*}
%     h_i^{'} = \sum_{j \in \mathcal{N}(i)} \phi_\theta h_j
% \end{align*}
where $h_j$ is the node feature of $v_j$, $\phi_\theta$ is a learned transformation of the node feature parameterized by $\theta$, and $\mathcal{N}(i) = \{j~|~(v_i, v_j) \in \mathcal{E}\}$.
\sk{Early works in multi-robot coordination using GNNs adopted this base architecture (e.g., \cite{porok-path-plan}). However, this architecture implicitly assumes that all agents exhibit equivalent influence through the message passing/communication channels.}
\sk{
This is generally not the case, and more recent works have pivoted to adopting graph attention layers into their architecture in lieu of standard GNN layers due to the improved expressivity and performance achieved in multi-agent/multi-robot tasks (e.g., \cite{magat, gat-auto-drive, he2023multi}).
In this work, every policy contains a single Graph Attention Layer \cite{gatv2}, which extends the graph convolution layer with a learned attention function that weights each edge $(v_i, v_j) \in \mathcal{E}$. A single graph attention layer is given by $h_i^{'} = \sum_{j \in \mathcal{N}(i)} \alpha_{ij} \cdot \phi_\theta h_j$, where $\alpha_{ij}$ is the edge weight computed by the attention function introduced by \citet{gatv2}.
}

\textbf{GNN-based Multi-Agent Coordination}:
There are fundamental design choices that have been adopted throughout the GNN-based multi-agent coordination literature (see Sec. ~\ref{sec:related_works}). Most importantly, an agent-agent graph and a representation of each agent's local observations are provided as inputs to the GNN as the adjacency matrix and the node features. Within the GNN, single or several rounds of message passing are conducted through graph convolutional layers, allowing agents to integrate information from surrounding agents into their own node representation (such as local observations), analogous to communication channels. This node representation is then utilized for task completion, typically via a local decoder structure such as a multi-layer perceptron with shared parameters/weights. This effectively conditions the action of the ego agent on the observations of surrounding agents in addition to the ego agent. We adopt these fundamental design aspects in our model architecture, along with task-specific design choices informed by prior works~\cite{bettini2024benchmarl, bettini2022vmas, passageProrok}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Evaluating Graph Explainers}
Below, we discuss the graph explainers we investigate along with the tasks and metrics we use to evaluate both explanation quality and task performance. 
% for evaluating explanation quality and task performance when training a GNN-based policy with or without attention entropy regularization. 
% Aside from the attention entropy regularization, all other training details, which are informed by common choices associated with GNN-based policy training, can be found in the appendix.

\subsection{Graph-based Explainers}
\label{subsec:explainers}
We investigate the following GNN explainers in terms of their ability to explain GNN-based coordination policies.

% \subsubsection{Graph Mask}
\textbf{Graph Mask:} Graph Mask \cite{graphmask} searches for the minimal-edge subgraph $G_s \subseteq G$ such that a measure of the divergence between the output under the subgraph and the original graph is below a threshold: $D(f(G_s, \mathcal{X} || f(G, \mathcal{X})) < \beta$. The minimal-edge subgraph $G_s$ is then a binary-valued mask over the original graph.

% \subsubsection{GNN-Explainer}
\label{gnnExplainerPrelim}
\textbf{GNN-Explainer:} GNN-Explainer \cite{gnnexplainer} explains GNNs by searching for the subgraph $G_s \subseteq G$ and node feature subset $\mathcal{X}_s \subseteq \mathcal{X}$ having the maximal mutual information with the prediction under the original graph and node feature set.
% GNNExplainer optimizes for this mutual information objective in hopes of identifying a subgraph $G_s$ that minimizes the uncertainty (i.e., variance) of the model output and maximizes the probability of producing the original GNN output when limiting the GNN inference to $G_s$.
In this work, we are most interested in the subgraph identification component, which is a continuous-valued edge mask over the original graph, incentivized to be discrete through regularization.

% \subsubsection{Attention Explainer} 
\textbf{Attention Explainer:} Given the prominence of graph attention networks in policies for multi-agent coordination \cite{magat, gat-auto-drive, collab-perception}, we also investigate Attention Explainer~\cite{Fey/Lenssen/2019} which presents the attention values generated by the Graph Attention Layer as the explanation. Intuitively, these values represent the importance of each channel within the communication graph, with a higher attention value indicating greater importance and influence.

\subsection{Tasks}
We evaluate the three graph-based explainers introduced above on three multi-agent coordination tasks implemented using the VMAS simulator~\cite{bettini2022vmas} across three team sizes.
\sk{
These tasks are all the same or harder versions of the tasks in BenchMARL~\cite{bettini2024benchmarl}, a new MARL benchmark. Since most tasks in BenchMARL and VMAS are unsolved with GNNs (or in general), we adopted the model architecture and training parameters of prior works for tasks that have been solved using GNNs, and designed GNNs for other tasks based on related work.
}
% In all tasks, the agents only have access to their local information and observations (i.e. the agents have no direct access to the observations, states, or other information regarding the other agents except via communication). This ensures that coordination and communication are necessary for task success, and prohibits trivial solutions. 
% For all tasks, we adopt the standard encoder-message passing-decoder network architecture that is ubiquitous in MARL~\cite{passageProrok}, however the network hyperparameters were modified to benefit each task. 
% (i.e. number of layers, number of units, etc.) were modified to benefit each task. 
% We describe each task, the corresponding local observations for each agent, the specific network parameters used, and associated training hyperparameters.

% \subsection{Tasks}
% \subsubsection{Blind Navigation}
\textbf{Blind Navigation:} We adapted this task from \cite{bettini2022vmas}. The original task considers a team of $N$ agents, each equipped with LIDAR and required to navigate to its assigned goal location while minimizing collisions with other agents. Both initial and goal locations are randomized.
% are spawned in random locations, and are assigned random goal locations. The agents are required to reach their assigned goal positions while minimizing collisions with each other.
% (penalty of -1). 
Our \textit{blind} navigation task differs from the original version in that there are no LIDAR sensors equipped on the agents. This means that the agents only observe their own position, velocity, and goal location, and that each agent has no information about the other agents from local observations alone. Thus, effective communication is required to solve our blind version of this task. We consider three team sizes ($N={3,4,5}$).
% (in contrast, in the original navigation task \cite{bettini2022vmas}, the use of LIDAR allows the agents to solve the task reasonably well without communication). 
For all team sizes, we design a common policy with an Identity layer for the encoder, a 32-dim GATv2 layer with TanH activation, and a 2-layer decoder with 256 units each and TanH activation. 
% We make no modifications to the policy architecture across team sizes.
\sk{
We use the dense reward structure from \cite{bettini2022vmas,bettini2024benchmarl}.
}

% \subsubsection{Passage}
% \harish{Constrained Navigation?}
\textbf{Blind Passage:} We adapted this task from \cite{passageProrok, bettini2022vmas}. The original passage task considers a team of 5 agents that start in a cross-shaped formation on one side of a wall and need to reach a destination on the other side with the same formation after traversing a narrow corridor or passage and minimizing collisions with each other and with the wall.
% reconfigure on the other side of a wall by proceeding through a narrow corridor or passage. 
% The agents are required to reach their goal positions while minimizing collisions with both each other (penalty of -10) and with the wall (penalty of -1). 
% The environment and agent dynamics are borrowed from \cite{bettini2022vmas}. 
The original passage task is hard-coded to 5 agents. We modify the environment to also consider 3- and 4-agent teams by having the 3 or 4 agents randomly spawn in any of the 5 positions in the cross-shaped formation. We use the same reward structure and policy architecture (a 4 layer encoder with 32 units each and ReLU activation, a 32-dim GATv2 layer with TanH activation, and a 4 layer decoder with 64 units each and ReLU activation) as the original version, and make no modifications across team sizes.
\sk{
We use the dense reward structure from \cite{passageProrok}.
}

% \subsubsection{Discovery}
\textbf{Blind Discovery:} This task is a specific instantiation of the Discovery task proposed in \cite{bettini2022vmas}, with modifications applied to the observation space. The task considers a team of $N$ agents, which are spawned randomly and must explore the environment to discover a single landmark.
% and perform a rescue operation. 
The landmark is considered discovered when at least 2 agents converge on its position simultaneously. 
% Once 2 agents converge to the landmark, the team receives a shared reward of 1, and the task is completed. There is no penalty for inter-agent collisions. 
We modify the agent observation space with respect to the landmark to have a sensing radius instead of a simulated 12-point LIDAR for training efficiency. Agents can only observe their own position, velocity, and a binary value indicating if the landmark is within their sensing radius. For all team sizes, we design a common policy with a 1-layer encoder with 64 units and a ReLU activation, 64 GATv2 layer with ReLU activation, and a 2-layer decoder with 64 units each and a ReLU activation. 
% We make no modifications to the network architecture across team sizes.
\sk{
We use the sparse reward structure from \cite{bettini2022vmas}.
}

% \subsection{Metrics}
% \subsection{Metrics for Task Performance}
% % We consider five task performance metrics: reward, success rate, no agent collision rate, no object collision rate, and makespan. We describe each of these metrics below for a single episode or rollout. Each metric is averaged across multiple episodes or rollouts, each with an random initial state. 
% We consider the following task performance metrics.

% % \subsubsection{Reward}
% \textbf{Reward:} This is the average reward accrued across all agents, computed as $\frac{1}{N}\sum^N_{n=1}\sum^T_{t=0} r_t(n)$ where $n \in N$ is agent index and $t \in T$ is the time index within an episode. 
% % For the blind navigation and discovery task, the reward is as defined by \cite{bettini2022vmas}. For the passage task, the collision reward is defined by \cite{bettini2022vmas}, while the waypoint reward is based on \cite{passageProrok}.
% % We calculate this as shown in \eqref{rewardMetric}.

% % \begin{equation}
% %     \Bar{R} = \frac{1}{K}\frac{1}{N}\sum_k\sum_n\sum_t r_t
% %     \label{rewardMetric}
% % \end{equation}

% % \subsubsection{Success Rate}
% % This is the percentage of episodes that were successfully completed. 
% \textbf{Success Rate:} For the blind navigation and passage task, the success rate of an episode is defined by the fraction of the total number of agents who reach their assigned goals. 
% % $\frac{\text{number of agents that reach their goal}}{\text{total number of agents in the team}}$. 
% An episode of the discovery task is considered successful if the landmark is discovered before the episode terminates.
% % $\mathds{1}$[target located by a pair of agents simultaneously] where $\mathds{1}$ is an indicator function that returns 1 if the condition is satisfied and 0 otherwise.

% % \subsubsection{No Agent Collision Rate}
% \textbf{No Agent Collision Fraction:} Across all tasks, this metric is defined as the fraction of episodes that result in no agent-agent collisions, averaged across random initial states. 
% % In an episode, the no agent collision rate is defined as $\mathds{1}$[if no agents collided with each other]. 
% % This metric holds for all tasks, as all tasks seek to avoid agent-agent collisions (thus necessitating communication and coordination).

% % \subsubsection{No Object Collision Rate}
% \textbf{No Object Collision Fraction:} This is the fraction of episodes in which no agent collides with an obstacle, averaged across multiple random initial states. 
% % In an episode, the no object collision rate is defined as $\mathds{1}$[if no agents collided with a static obstacle] where $\mathds{1}$ is an indicator function that returns 1 if the condition is satisfied and 0 otherwise. 
% % This is averaged across multiple random initial states. 
% This metric is only used in the passage task, since 
% % there is a wall in the environment that the agents are seeking to avoid collision with. For the blind navigation and search and rescue task, 
% there are no obstacles in blind navigation and discovery tasks.

% % \subsubsection{Makespan}
% \textbf{Makespan:} This is the number of environment steps it takes for the team to complete the task, with a maximum value defined by episode length.
% % Note that there is an imposed maximum number of steps in the environment, after which the rollout or episode terminates. 
% % Thus, the makespan for an episode or rollout can be defined as $\min$(number of steps to complete task, imposed maximum number of steps). 
% We consider the blind navigation and passage tasks complete when the last agent reaches its goal location. 
% % the number of steps to complete the task is the number of steps it takes for the last agent to reach its goal location. 
% We consider the discovery task complete when the landmark is discovered.
% % the number of steps to complete the task is the time it takes for two agents to find the target simultaneously.

\subsection{Metrics for Explanation Quality}

We consider four metrics to quantify both the fidelity~\cite{amara2022graphframex} and faithfulness~\cite{agarwal2023evaluating} of generated explanations.
% which are positive fidelity, negative fidelity, delta fidelity, and unfaithfulness. 
% The fidelity-based metrics were adopted from \cite{amara2022graphframex}, and the unfaithfulness metric was adopted from \cite{agarwal2023evaluating}. 
These metrics and their variants have been studied and used extensively in the graph learning community to characterize the quality of subgraph-based explanations~\cite{kosangnnx,liu2021dig,Fey/Lenssen/2019, pope2019explainability, bajaj2021robust, yuan2022explainability}. 
In the definitions below, $F(\cdot)$ refers to the GNN-based model and $G^t = (X^t,A^t)$ refers to the original graph input at timestep $t$, where $A^t$ is the adjacency matrix at timestep $t$ of $G^t$ and $X^t$ is set of all node features at timestep $t$ in $G^t$. We use $G_S^t$ to refer to the explanation subgraph at timestep $t$ with $A_S^t$ as its adjacency matrix at timestep $t$. Since $G^t$ is a fully connected graph at every timestep in our experiments, $A^t = \mathbf{1},\,\forall t$ where $\mathbf{1}$ is a matrix with each entry equal to $1$. 
We define each of explanation quality metric for a single timestep, and we average across all timesteps and across multiple episodes, each with a random initial state, during the evaluation in \sectionautorefname~\ref{empiricalResultsDiscussion}.

\textbf{Positive Fidelity $(\uparrow)$}: 
$Fid_+$ is defined as the difference between the original prediction $F(G)$ and the prediction generated using the complement of the explanation subgraph $F(G \setminus G_S)$ \cite{pope2019explainability, bajaj2021robust}. The adjacency matrix of the complement of the subgraph $G \setminus G_S$ is $A - A_S = \mathbf{1} - A_S$. Formally, $Fid_+$ at timestep $t$ is $Fid_+^t \triangleq |F(G^t) - F(G \setminus G_S^t)|$,
% \begin{align}
%     Fid_+^t &\triangleq |F(G^t) - F(G \setminus G_S^t)|
%     % \\ &= |F(X^t, \mathbf{1}) - F(X^t, \mathbf{1} - A_S)| \text{ (in this study)}
%     \label{eqn:fidPlus}
% \end{align}
and measures the \textit{necessity} of the explanation subgraph. The more necessary the subgraph, the larger the change to the prediction when it's removed~\cite{amara2022graphframex}. Larger $Fid_+$ are preferred.
% Larger $Fid_+^t$ are preferred.
% in that the magnitude of change to model prediction when the subgraph is removed from the initial graph \cite{amara2022graphframex}. 
% Here, a large positive fidelity score indicates a more necessary explanation subgraph.

\textbf{Negative Fidelity $(\downarrow)$}:
$Fid_-$ is defined as the difference between the original prediction $F(G)$ and the prediction under the explanation subgraph $F(G_S)$ \cite{yuan2022explainability}. Formally, $Fid_-$ at timestep $t$ is $Fid_-^t \triangleq |F(G^t) - F(G_S^t)|$,
% in a rollout or episode as shown in \eqref{eqn:fidMinus}.
% \begin{align}
%     Fid_-^t &\triangleq |F(G^t) - F(G_S^t)|
%     % \\ &= |F(X^t, \mathbf{1}) - F(X^t, A_S^t)| \text{ (in this study)}
%     \label{eqn:fidMinus}
% \end{align}
and measures the \textit{sufficiency} of the explanation subgraph. The more sufficient the explanation subgraph, the closer its prediction to that of the initial graph~\cite{amara2022graphframex}. Smaller $Fid_-^t$ are preferred.
% in that a subgraph is sufficient if the explanation subgraph leads by its own to the initial prediction of the model \cite{amara2022graphframex}. Here, a small negative fidelity score indicates a more sufficient explanation subgraph.

\textbf{Delta Fidelity  $(\uparrow)$}:
We define $Fid_{\Delta}$ as the difference between positive fidelity ($Fid_+$) and negative fidelity ($Fid_-$). Formally, $Fid_\Delta$ at timestep $t$ is defined as $Fid_{\Delta} \triangleq Fid_+ - Fid_-$. Since we desire an explanation subgraph that results in a large $Fid_+$ and a small $Fid_-$, a high fidelity explanation subgraph $G_S$ should maximize $Fid_{\Delta}$.

\textbf{Unfaithfulness $(\downarrow)$}:
$GEF$ measures how unfaithful the subgraph explanations are to an underlying GNN model \cite{agarwal2023evaluating}. Formally, $GEF$ at timestep $t$ is $GEF^t \triangleq  1 - \exp{(-KL(F(G^t) || F(G_S^t)))}$, 
% \begin{align}
%     GEF^t & \triangleq  1 - \exp{(-KL(F(G^t) || F(G_S^t)))}
%     % \\ &= 1 - \exp{(-KL(F(X^t, \mathbf{1}) || F(X^t, A_S^t)))} \text{ (in this study)}
%     \label{eqn:unfaithfulness}
% \end{align}
where $KL(\cdot || \cdot)$ is the Kullbackâ€“Leibler divergence. $GEF^t$ is 1 when $KL(F(G^t) || F(G_S^t))$ tends to $\infty$ and is 0 when $KL(F(G^t) || F(G_S^t))$ tends to $0$. As a result, a smaller unfaithfulness measure is
% generally
preferred.
% However, we note that if $F(G_S^t)$ results in a similar prediction as $F(G)$ but with a lower variance than $F(G)$, then the $GEF^t$ is larger than if the converse was true due to $KL$'s asymmetry.


% \subsection{Training Details}
% \sk{
% In alignment with many MARL-based works \cite{nayak2023scalable, blumenkamp2021emergence, passageProrok, yang2023learning, he2023multi, escudie2024attention}, we adopt a centralized training decentralized execution (CTDE) paradigm to train policies. Specifically, we use multi-agent proximal policy optimization (MAPPO) \cite{yu2022surprising} with a centralized critic and homogeneous agents, which has been used in \cite{nayak2023scalable, yang2023learning, he2023multi, escudie2024attention}. The critic is parameterized as a 2-layer fully-connected multilayer perceptron (MLP) with 32 units in each layer for the blind navigation task and the passage task, which we borrow from \cite{passageProrok}, and 128 units in each layer for the discovery task, which we found worked well across all team sizes without attention entropy regularization (see \sectionautorefname~\ref{subsec:entropy_reg}). Following \cite{yu2022surprising}, we incorporate generalized advantage estimation \cite{Schulmanetal_ICLR2016} and parameter sharing for the agents (though the policies are decentralized). We optimize the MAPPO loss term using ADAM optimizer \cite{kingma2014adam}, and implement the MARL training and evaluation using PyTorch \cite{paszke2019pytorch} and PyTorch Geometric \cite{Fey/Lenssen/2019}.

% \textbf{
% To ensure an unbiased evaluation of entropy regularization and different explanation methods, 
% % It is important to note that 
% all our design choices for training (actor, critic, hyperparameters, etc.) are motivated by prior works or empirical performance of the policy without attention entropy minimization (from \sectionautorefname~\ref{subsec:entropy_reg}). In addition, we do not inform these design choices based on explanation quality either.
% }
% }

\section{Improving Explanations}
\label{subsec:entropy_reg}

After analyzing the quality of explanations generated by existing explanation results (see Sec. \ref{empiricalResultsDiscussion}), we observed that they could be improved by modifying how we train GNNs -- by minimizing the entropy of the attention values.

Attention entropy minimization can be motivated from two perspectives, one from a multi-agent coordination perspective and one from a graph learning perspective. From a multi-agent collaboration perspective, an agent who collaborates with other agents will intuitively desire to (1) filter out useless information and (2) focus on the information that is most crucial to the task at hand. This is akin to how humans use selective attention to filter unnecessary information and focus on the truly important information for decision-making~\cite{moerel2024selective}. Attention in GNNs serve a similar purpose when the nodes are agents and the edges are communication channels composed of agents providing information to other agents. Minimizing attention entropy in a multi-agent setting yields an attention distribution that is further from pure uniform attention, resulting in a set of agents that are given more focus than other agents.

From a graph learning perspective, minimizing attention entropy in conjunction with task objective can be seen as a form of denoising for node-level learning tasks (e.g., node classification or regression). By incentivizing a set of attention values that have lower entropy, the model is likely to learn a stronger filter that starts removing extraneous or noisy information.
% that contributes no to little utility for the task at hand. 
The intuition behind minimizing attention entropy can be connected to more formal notions of information bottleneck over graphs \cite{yugraph}. But, optimizing such objectives tends to be computationally intensive and challenging, which will be likely exacerbated when combined with multi-agent learning. In contrast, integrating attention entropy minimization into learning GNNs is much simpler, especially within MARL frameworks like MAPPO \cite{yu2022surprising}. Formally, let $\mathcal{L}^{PPO}_t(\theta)$ be the standard clip PPO loss used in MAPPO \cite{yu2022surprising, schulman2017proximal} to update the policy weights $\theta$ and let $\alpha_t(\theta)$ be the attention values generated from the model parameters $\theta$. We can define the new regularized loss as
% shown in \eqref{eqn:attnEntPPO}.

% \vspace{-2em}

\begin{equation}
    \mathcal{L}^{PPO + ATTN}_t = \mathds{E}_t [\mathcal{L}^{PPO}_t(\theta) + \lambda \mathcal{H}(\alpha_t(\theta))]
    \label{eqn:attnEntPPO}
\end{equation}

where $\mathcal{H}(p) = -\sum_i p_i \log (p_i)$ denotes the entropy of $p$. 
% Entropy is a convex function over a convex set (namely, the probability simplex). As a result, it can be shown that entropy is maximized when the categorical distribution is uniform (i.e., $p_i = p_j \forall i, j$) and is minimized at the extreme points of the simplex.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{figure}[ht]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=\columnwidth]{icml_numpapers}}
% \caption{Historical locations and number of accepted papers for International
% Machine Learning Conferences (ICML 1993 -- ICML 2008) and International
% Workshops on Machine Learning (ML 1988 -- ML 1992). At the time this figure was
% produced, the number of accepted papers for ICML 2008 was unknown and instead
% estimated.}
% \label{icml-historical}
% \end{center}
% \vskip -0.2in
% \end{figure}


\subsection{Theoretical Analysis}

% While the discussion above provides a high-level intuition for minimizing attention entropy, we theoretically characterize its effect in this section.

Let $G$ be the fully-connected input graph of $N$ nodes or agents (with $A = \mathbf{1}$ as its adjacency matrix) to a GNN-based model that learns a set of attention values $\alpha_{ij}$ for each pair of vertices $i, j$ in $G$. Moreover, let $G_S(\alpha)$ be the subgraph generated by Attention Explainer using an attention-based adjacency matrix $A_S(\alpha)$ whose $ij$th element is given by $\alpha_{ij}$.
% the attention values $\alpha_{ij}$ as the adjacency matrix (which we denote as $A_S(\alpha)$ such that $A_S(\alpha)_{i, j} = \alpha_{i,j}$). 
% This would be the explanation subgraph when using an Attention Explainer. 
The complement of $G_S(\alpha)$ is $G \setminus G_S(\alpha)$ with the corresponding adjacency matrix $\mathbf{1} - A_S(\alpha)$. Below, we show that minimizing the attention entropy corresponds to maximizing the dissimilarity of the subgraph $G_S$ and its complement $G \setminus G_S$ under a global measure characterized by an entry-wise matrix norm. 

Distances between two graphs $G_1$ and $G_2$ are best described in terms of the adjacency matrix $A_{G_1 - G_2} = A_{G_1} - A_{G_2}$, and mismatch norms are matrix norms applied to $A_{G_1 - G_2}$~\cite{gervens2022graph}. It turns out that global norms such as edit distance and its direct generalizations correspond to entrywise matrix norms, and that the entry-wise matrix $1$-norm $||A||_1 = (\sum_{i, j} |A_{i,j}||)$ is a suitable mismatch norm~\cite{gervens2022graph}. As such, the distance between $G_S(\alpha)$ and $G \setminus G_S(\alpha)$ (denoted as $D(\alpha)$) 
can be represented as the entry-wise 1-norm of $A_S(\alpha) - (\mathbf{1} - A_S(\alpha))$, yielding 
% or $D(\alpha) = ||A_S(\alpha)_{i, j} - (\mathbf{1} - A_S(\alpha)_{i, j})||_1$, and reformulate it as shown in \eqref{eqn:reductionStep}.
{\scriptsize
\begin{align}
    D(\alpha) &= \left|\left|A_S(\alpha)_{i, j} - (\mathbf{1} - A_S(\alpha)_{i, j})\right|\right|_1\\ 
    &= \sum_{i, j} \left|A_S(\alpha)_{i,j} - (\mathbf{1} - A_S(\alpha)_{i, j})\right| \\
    &= \sum_{i, j} \left|\alpha_{i,j} - (1 - \alpha_{i, j})\right| = \sum_{i, j} \left| 2\alpha_{i,j} - 1 \right|
    \label{eqn:reductionStep}
\end{align}
}
Note that $D(\alpha)$ is convex as it is a sum of convex functions. As such, we can compute its lower as shown below
% in \eqref{eqn:jensensDalpha} 
using Jensen's inequality~\cite{mcshane1937jensen} and the fact that $\sum_j \alpha_{i, j} = 1,\, \forall i$ due to softmax normalization. 
% that occurs on the attention scores for each node $i$.
% \begin{align}
%     D(\alpha) = \sum_{i, j} |2\alpha_{i,j} - 1| &\geq N^2 \left| 2\cdot\frac{\sum_{i, j}\alpha_{i, j}}{N^2}-1 \right| \\&= N^2 \left| 2\cdot\frac{N}{N^2}-1 \right| \\
%     &= \left|2N - N^2\right|
%     \label{eqn:jensensDalpha}
% \end{align}
\begin{equation*}
    \scriptstyle
    D(\alpha) = \sum_{i, j} |2\alpha_{i,j} - 1| \geq N^2 \left| 2\cdot\frac{\sum_{i, j}\alpha_{i, j}}{N^2}-1 \right| = N^2 \left| 2\cdot\frac{N}{N^2}-1 \right|
    = \left|2N - N^2\right|
    % \label{eqn:jensensDalpha}
\end{equation*}

Now, suppose $\alpha_{i, j} = \frac{1}{N}, \, \forall i, j$ (that is, a uniform attention distribution is learned for all nodes in $G$). We can determine 
% what $D(\alpha = \left[\frac{1}{N}, \ldots, \frac{1}{N}\right])$ is as shown in \eqref{eqn:lowerBoundDalpha}.

% \begin{align}
%     D \left(\alpha = \left[\frac{1}{N}, \ldots, \frac{1}{N}\right] \right) &= \sum_{i, j} \left |2 \cdot \frac{1}{N} - 1 \right| \\&= N^2 \left|\frac{2}{N} - 1 \right| \\
%     &= \left|2N - N^2\right|
%     \label{eqn:lowerBoundDalpha}
% \end{align}
\vspace{-1.5em}
\begin{equation}
    \scriptstyle
    D \left(\alpha = \left[\frac{1}{N}, \ldots, \frac{1}{N}\right] \right) = \sum_{i, j} \left |2 \cdot \frac{1}{N} - 1 \right| 
    \ = N^2 \left|\frac{2}{N} - 1 \right|
    \ = \left|2N - N^2\right|
    \label{eqn:lowerBoundDalpha}
\end{equation}
\vspace{-1.5em}

From the above, we can see that a uniform attention distribution realizes the lower bound of $D(\alpha)$, implying that a uniform attention distribution yields the lowest distance between the induced subgraph explanation $G_S(\alpha)$ and the complement of the subgraph $G \setminus G_S(\alpha)$ (i.e., a global minima under simplex constraint on $\alpha_{i, j=1:N}$). We can additionally analyze what happens to $D(\alpha)$ at the extreme points of the simplex constraint on $\alpha_{i, j=1:N}$ (w.l.o.g., let $\boldsymbol{\alpha} = \alpha_{i, j=1:N} = \left[1, 0, \ldots 0\right], \, \forall i$) as follows

% \begin{align}
%     D \left(\boldsymbol{\alpha} = \left[1, 0, \ldots, 0\right], \, \forall i \right) &= \sum_{i, j} 1 = N^2 \\&> \left|2N - N^2\right|, \, \forall N \geq 2
%     \label{eqn:extremePointEval}
% \end{align}
\vspace{-1.75em}

\begin{equation}
\scriptstyle
    D \left(\boldsymbol{\alpha}\ =\ \left[1, 0, \ldots, 0\right], \, \forall i \right) \ =\ \sum_{i, j} 1 \ =\ N^2 \ > \  \left|2N - N^2\right|, \, \forall N \geq 2
    \label{eqn:extremePointEval}
\end{equation}

\vspace{-0.5em}

From this, we can see that the minima is not persistent throughout the simplex on $\alpha_{i, j=1:N}$. Note that through a nearly-identical analysis of negative attention entropy $-\mathcal{H}(\alpha)$, the uniform distribution achieves a global minima under simplex constraint on $\alpha_{i, j=1:N}$ for $-\mathcal{H}(\alpha_{i, j})$ (i.e., the uniform distribution of attention results in the largest entropy). Moreover, as $-\mathcal{H}$ is \textit{strictly} convex, this minima is unique to the uniform distribution. 

Given that $D(\alpha)$ and $-\mathcal{H}(\alpha)$ are both convex functions that share a global minima and that minima is not persistent over the simplex constraint on $\alpha_{i, j=1:N}$, we can conclude that minimizing attention entropy widens the separation between $G_S(\alpha)$ and $G \setminus G_S(\alpha)$. 

We hypothesize that increasing the distance $D(\alpha)$ is a desirable trait to have within the GNN-based policy structure for explainability.
\sk{
Intuitively, increasing $D(\alpha)$ increases the correlation of the original model output $F(G)$ and the model output using the subgraph induced by the attention values $F(G_S(\alpha))$. This can be attributed to attention entropy minimization pruning noisy and detractive edges when jointly guided by the task reward. This improved alignment should manifest as an improvement in negative fidelity, as supported by empirical results.

Conversely, increasing $D(\alpha)$ decreases the correlation of the original model output $F(G)$ and the model output using the complement of the subgraph induced by the attention values $F(G \setminus G_S(\alpha))$. Since the distance $D(\alpha)$ between $G_S(\alpha)$ and $G \setminus G_S(\alpha)$ is maximized under the regularization, coupled with the fact that $G \setminus G_S(\alpha)$ will likely have the noisy and detractive edges as mentioned earlier, it is likely that $F(G)$ and $F(G \setminus G_S(\alpha))$ will diverge, improving positive fidelity (also supported by our empirical experiments).
In effect, the derived relationship between $D(\alpha)$ and attention entropy minimization (i.e. minimizing attention entropy $\rightarrow$ maximizing $D(\alpha)$) suggests that our regularization likely incentivizes the attention-induced subgraph to be both necessary and sufficient via the above mechanisms.
}
% Intuitively, increasing $D(\alpha)$ amounts to ensuring that the explanation's subgraph $G_S(\alpha)$ is getting closer to the original graph $G(\alpha)$.
% This observation can be extend to understand the impact of increasing $D(\alpha)$ on fidelity measures.

\sk{
To summarize, since negative fidelity is proportional to how much the subgraph explanation approximates the original graph's predictions, we will likely see a reduction in negative fidelity when $D(\alpha)$ increases.
Similarly, since positively fidelity is proportional to how much useful information is taken away when the subgraph explanation is deleted from the original graph, we will likely see an improvement in positive fidelity when $D(\alpha)$ increases.
}
% To convey this intuition, consider $F(G_S)$ from negative fidelity. 
% When constructing a graph input based on the attention values, there tends to be a stronger preservation of the edge weight along the more important message passing edge, and this preservation is reinforced when the entropy of the attention distribution is low. 
% When we minimize attention entropy, the edges that are more important (as identified by large attention weights) will likely increase in importance, in turn increasing the chance that the subgraph generated based on attention values effectively encodes all the key influences. 
% A such, a model that is trained with attention entropy minimization, when provided a subgraph that is generated from its attention values as input, will retain a higher edge importance for the edges of the graph that were inherently deemed important. Consequently, we will likely see a reduction in negative fidelity. 

% Now consider $F(G \setminus G_S)$ from positive fidelity. When constructing a graph input based on the complement of the attention values, there tends to be a counteracting effect due to the interaction between $\alpha$ in the model and $1 - \alpha$ in the subgraph. Thus, the edges that were deemed by the model to be important will receive a deprioritization when fed a subgraph generated from the complement of the attention values as input. Moreover, this deprioritization or decay of importance is exacerbated when the original attention values had a low entropy. This can be seen more evidently from the fact that the subgraph and its complement have a higher mismatch norm or distance. Going even further, given that the distance between the subgraph and its complement is maximized under attention entropy, and given that the resultant node embeddings are a function of the attention values affected by entropy minimization, we conjecture that there could be an increase in the distance between the corresponding node embeddings.

Finally, upon inspection of the objective $D(\alpha)$, we find that the gradient within an $\epsilon$-neighborhood of the uniform distribution (i.e., the minima) has a gradient of 0. As such, unless the gradient produced by the learning objective guides the parameters to leave this neighborhood, the distance between $G_S$ and $G \setminus G_S$ will not increase. Under the assumption that this property is desirable (which we confirm in our empirical analysis), a surrogate objective such as entropy $\mathcal{H}$ will provide a better gradient signal due to its strict concavity.
% that not only is attention entropy is a suitable surrogate objective, but also

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Empirical Results and Discussion}
\label{empiricalResultsDiscussion}

% \sk{\begin{enumerate}
%     \item Results
%     \begin{enumerate}
%         \item Explanation-based Results
%         \item Box and Whisker plots for Fidelity, Unfaithfulness
%     \end{enumerate}
% \end{enumerate}}

Our empirical evaluation has a twofold purpose. First, to characterize and compare the performance of state-of-the-art graph-based explainers when deployed in cooperative MARL settings as opposed to large graph data and supervised learning settings as done in prior work.
% This provides some initial insights into how existing state-of-the-art post-hoc explainers perform in this domain relative to one another. 
Second, to investigate how minimizing attention entropy within GAT-based policies affects both explanation quality and task performance. For all evaluations, we run 50 rollouts in each environment for each independent variable of interest.
% verify the validity of the aforementioned intuitions, and whether minimizing attention entropy within GAT-based multi-agent policies (1) improves explanation quality and (2) affects task performance significantly. 
% Generally, there is often a trade-off between model explainability/interpretability and model performance, and this evaluation seeks to understand the significance of this trade-off.
All relative comparisons made below were validated via Mann-Whitney U-tests with Bonferroni correction.

\begin{figure}[t]
    \centering
    \includegraphics[trim={0.1cm 0.65cm 1.55cm 0.8cm},clip,width=0.325\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig_no_legend_big_font/boxplot_analysis-navigation-3_agent-DeltaFidelity.png}
    \includegraphics[trim={0.5cm 0.65cm 1.55cm 0.8cm},clip,width=0.317\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig_no_legend_big_font/boxplot_analysis-navigation-4_agent-DeltaFidelity.png}
    \includegraphics[trim={0.5cm 0.65cm 1.55cm 0.8cm},clip,width=0.317\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig_no_legend_big_font/boxplot_analysis-navigation-5_agent-DeltaFidelity.png}
    % \rulesep
    % \includegraphics[trim={0.45cm 0.65cm 1.55cm 0.9cm},clip,width=0.16\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-navigation-3_agent-Unfaithfulness.png}
    % \includegraphics[trim={0.45cm 0.65cm 1.55cm 0.9cm},clip,width=0.16\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-navigation-4_agent-Unfaithfulness.png}
    % \includegraphics[trim={0.45cm 0.65cm 1.55cm 0.9cm},clip,width=0.16\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-navigation-5_agent-Unfaithfulness.png}
    \includegraphics[trim={0.1cm 0.65cm 1.55cm 0.8cm},clip,width=0.325\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig_no_legend_big_font/boxplot_analysis-passage-3_agent-DeltaFidelity.png}
    \includegraphics[trim={0.5cm 0.65cm 1.55cm 0.8cm},clip,width=0.317\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig_no_legend_big_font/boxplot_analysis-passage-4_agent-DeltaFidelity.png}
    \includegraphics[trim={0.5cm 0.65cm 1.55cm 0.8cm},clip,width=0.317\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig_no_legend_big_font/boxplot_analysis-passage-5_agent-DeltaFidelity.png}
    % \rulesep
    % \includegraphics[trim={0.45cm 0.65cm 1.55cm 0.9cm},clip,width=0.16\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-passage-3_agent-Unfaithfulness.png}
    % \includegraphics[trim={0.45cm 0.65cm 1.55cm 0.9cm},clip,width=0.16\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-passage-4_agent-Unfaithfulness.png}
    % \includegraphics[trim={0.45cm 0.65cm 1.55cm 0.9cm},clip,width=0.16\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-passage-5_agent-Unfaithfulness.png}
    \includegraphics[trim={0.1cm 0.65cm 1.55cm 0.8cm},clip,width=0.325\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig_no_legend_big_font/boxplot_analysis-discovery-3_agent-DeltaFidelity.png}
    \includegraphics[trim={0.5cm 0.65cm 1.55cm 0.8cm},clip,width=0.317\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig_no_legend_big_font/boxplot_analysis-discovery-4_agent-DeltaFidelity.png}
    \includegraphics[trim={0.5cm 0.65cm 1.55cm 0.8cm},clip,width=0.317\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig_no_legend_big_font/boxplot_analysis-discovery-5_agent-DeltaFidelity.png}
    % \rulesep
    % \includegraphics[trim={0.45cm 0.65cm 1.55cm 0.9cm},clip,width=0.16\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-discovery-3_agent-Unfaithfulness.png}
    % \includegraphics[trim={0.45cm 0.65cm 1.55cm 0.9cm},clip,width=0.16\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-discovery-4_agent-Unfaithfulness.png}
    % \includegraphics[trim={0.45cm 0.65cm 1.55cm 0.9cm},clip,width=0.16\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-discovery-5_agent-Unfaithfulness.png}
    
    \caption{Delta Fidelity ($\uparrow$, \sectionautorefname~\ref{fidelityResults}) of explanations generated by three explainers \textcolor{blue}{\textbf{without (blue)}} and \textcolor{orange}{\textbf{with (orange)}} proposed regularization across three tasks (rows) and three team sizes (columns). Higher Delta Fidelity is better.
    % \textit{Top row}: Blind navigation task, \textit{Middle row}: Passage task, \textit{Bottom row}: Discovery task. Higher is better.
    }
    \label{fig:deltaFidelity}
\end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.495\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-navigation-3_agent-DeltaFidelity.png}
%     \includegraphics[width=0.495\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-navigation-3_agent-Unfaithfulness.png}
%     \includegraphics[width=0.495\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-navigation-4_agent-DeltaFidelity.png}
%     \includegraphics[width=0.495\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-navigation-4_agent-Unfaithfulness.png}
%     \includegraphics[width=0.495\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-navigation-5_agent-DeltaFidelity.png}
%     \includegraphics[width=0.495\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-navigation-5_agent-Unfaithfulness.png}
%     \caption{Navigation}
%     \label{fig:navigation-violin}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.495\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-passage-3_agent-DeltaFidelity.png}
%     \includegraphics[width=0.495\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-passage-3_agent-Unfaithfulness.png}
%     \includegraphics[width=0.495\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-passage-4_agent-DeltaFidelity.png}
%     \includegraphics[width=0.495\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-passage-4_agent-Unfaithfulness.png}
%     \includegraphics[width=0.495\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-passage-5_agent-DeltaFidelity.png}
%     \includegraphics[width=0.495\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-passage-5_agent-Unfaithfulness.png}
%     \caption{Passage}
%     \label{fig:passage-violin}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.495\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-discovery-3_agent-DeltaFidelity.png}
%     \includegraphics[width=0.495\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-discovery-3_agent-Unfaithfulness.png}
%     \includegraphics[width=0.495\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-discovery-4_agent-DeltaFidelity.png}
%     \includegraphics[width=0.495\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-discovery-4_agent-Unfaithfulness.png}
%     \includegraphics[width=0.495\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-discovery-5_agent-DeltaFidelity.png}
%     \includegraphics[width=0.495\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-discovery-5_agent-Unfaithfulness.png}
%     \caption{Discovery}
%     \label{fig:discovery-violin}
% \end{figure}

% \begin{table*}[h!]
% \centering
% \begin{tabular}{|c|c|c|c|c|c|c|}
% \hline
% \multicolumn{7}{|c|}{\textbf{Blind Navigation}} \\
% \hline
% \multirow{2}{*}{Performance Metric} & \multicolumn{3}{c|}{No Attention Entropy} & \multicolumn{3}{c|}{Attention Entropy} \\
%     \cline{2-7}
%      &  N = 3 & N = 4 & N = 5 & N = 3 & N = 4 & N = 5 \\
% \hline
% Reward ($\uparrow$) & 2.32 $\pm$ 0.644 & 3.045 $\pm$ 0.815 & 3.489 $\pm$ 1.1589 & 2.287 $\pm$ 0.58 & 3.03 $\pm$ 0.7019 & 3.45 $\pm$ 1.6569 \\
% \hline
% Success Rate ($\uparrow$) & 0.99 $\pm$ 0.0997 & 0.955 $\pm$ 0.2078 & 0.435 $\pm$ 0.497 & 0.99 $\pm$ 0.0997 & 0.94 $\pm$ 0.238 & 0.535 $\pm$ 0.50002 \\
% \hline
% No Agent Collision Rate ($\uparrow$) & 1 $\pm$ 0.0 & 0.98 $\pm$ 0.14035 & 0.97 $\pm$ 0.1710 & 0.995 $\pm$ 0.0707 & 0.995 $\pm$ 0.0707 & 0.955 $\pm$ 0.2078 \\
% \hline
% No Object Collision Rate ($\uparrow$) & 1 $\pm$ 0.0 & 1 $\pm$ 0.0 & 1 $\pm$ 0.0 & 1 $\pm$ 0.0 & 1 $\pm$ 0.0 & 1 $\pm$ 0.0 \\
% \hline
% Makespan ($\downarrow$) & 48.445 $\pm$ 24.901 & 61.405 $\pm$ 27.469 & 129.615 $\pm$ 29.922 & 48.985 $\pm$ 23.469 & 64.18 $\pm$ 30.685 & 123.255 $\pm$ 33.927 \\
% \hline
% \hline
% \multicolumn{7}{|c|}{\textbf{Passage}} \\
% \hline
% \multirow{2}{*}{Performance Metric} & \multicolumn{3}{c|}{No Attention Entropy} & \multicolumn{3}{c|}{Attention Entropy} \\
%     \cline{2-7}
%      &  N = 3 & N = 4 & N = 5 & N = 3 & N = 4 & N = 5 \\
% \hline
% Reward ($\uparrow$) & 54.5 $\pm$ 12.7 & 44.5 $\pm$ 17.3 & 37.9 $\pm$ 24.3 & 55.6 $\pm$ 23.1 & 32.1 $\pm$ 27.1 & 33.2 $\pm$ 21.7 \\
% \hline
% Success Rate ($\uparrow$) & 1.0 $\pm$ 0.0 & 0.68625 $\pm$ 0.3 & 0.914 $\pm$ 0.24 & 1.0 $\pm$ 0.0 & 0.9975 $\pm$ 0.02 & 0.991 $\pm$ 0.08 \\
% \hline
% No Agent Collision Rate ($\uparrow$) & 0.295 $\pm$ 0.5 & 0.575 $\pm$ 0.5 & 0.105 $\pm$ 0.307 & 0.685 $\pm$ 0.5 & 0.315 $\pm$ 0.5 & 0.215 $\pm$ 0.41 \\
% \hline
% No Object Collision Rate ($\uparrow$) & 0 $\pm$ 0.0 & 0 $\pm$ 0.0 & 0 $\pm$ 0.0 & 0.06 $\pm$ 0.2 & 0 $\pm$ 0.0 & 0 $\pm$ 0.0 \\
% \hline
% Makespan ($\downarrow$) & 45.445 $\pm$ 21.8 & 119.505 $\pm$ 42.4 & 84.885 $\pm$ 37.7 & 49.015 $\pm$ 14.4 & 119.675 $\pm$ 35.3 & 100.2 $\pm$ 33.26 \\
% \hline
% \hline
% \multicolumn{7}{|c|}{\textbf{Discovery}} \\
% \hline
% \multirow{2}{*}{Performance Metric} & \multicolumn{3}{c|}{No Attention Entropy} & \multicolumn{3}{c|}{Attention Entropy} \\
%     \cline{2-7}
%      &  N = 3 & N = 4 & N = 5 & N = 3 & N = 4 & N = 5 \\
% \hline
% Reward ($\uparrow$) & 0.9225 $\pm$ 0.27 & 0.985 $\pm$ 0.12 & 0.9225 $\pm$ 0.29 & 0.8775 $\pm$ 0.345 & 0.955 $\pm$ 0.23 & 0.9475 $\pm$ 0.23 \\
% \hline
% Success Rate ($\uparrow$) & 0.92 $\pm$ 0.27 & 0.985 $\pm$ 0.12 & 0.915 $\pm$ 0.28 & 0.87 $\pm$ 0.337 & 0.95 $\pm$ 0.218 & 0.945 $\pm$ 0.23 \\
% \hline
% No Agent Collision Rate ($\uparrow$) & 1 $\pm$ 0.0 & 1 $\pm$ 0.0 & 1 $\pm$ 0.0 & 1 $\pm$ 0.0 & 1 $\pm$ 0.0 & 1 $\pm$ 0.0 \\
% \hline
% No Object Collision Rate ($\uparrow$) & 1 $\pm$ 0.0 & 1 $\pm$ 0.0 & 1 $\pm$ 0.0 & 1 $\pm$ 0.0 & 1 $\pm$ 0.0 & 1 $\pm$ 0.0 \\
% \hline
% Makespan ($\downarrow$) & 112.38 $\pm$ 135.85 & 95.155 $\pm$ 103.32 & 113.245 $\pm$ 133.33 & 145.19 $\pm$ 150.275 & 134.79 $\pm$ 120.83 & 108.95 $\pm$ 116.55 \\
% \hline
% \end{tabular}
% \caption{Performance Comparison for All Tasks}
% \label{table:AllTasksPerformance}
% \end{table*}

% \begin{table*}[h!]
% \centering
% \begin{tabular}{|c|c|c|c|c|c|c|}
% \hline
% \multirow{2}{*}{Performance Metric} & \multicolumn{3}{c|}{No Attention Entropy} & \multicolumn{3}{c|}{Attention Entropy} \\
%     \cline{2-7}
%      &  N = 3 & N = 4 & N = 5 & N = 3 & N = 4 & N = 5 \\
% \hline
% Reward ($\uparrow$) & 2.32 $\pm$ 0.644 & 3.045 $\pm$ 0.815 & 3.489 $\pm$ 1.1589 & 2.287 $\pm$ 0.58 & 3.03 $\pm$ 0.7019 & 3.45 $\pm$ 1.6569 \\
% \hline
% Success Rate ($\uparrow$) & 0.99 $\pm$ 0.0997 & 0.955 $\pm$ 0.2078 & 0.435 $\pm$ 0.497 & 0.99 $\pm$ 0.0997 & 0.94 $\pm$ 0.238 & 0.535 $\pm$ 0.50002 \\
% \hline
% No Agent Collision Rate ($\uparrow$) & 1 $\pm$ 0.0 & 0.98 $\pm$ 0.14035 & 0.97 $\pm$ 0.1710 & 0.995 $\pm$ 0.0707 & 0.995 $\pm$ 0.0707 & 0.955 $\pm$ 0.2078 \\
% \hline
% No Object Collision Rate ($\uparrow$) & 1 $\pm$ 0.0 & 1 $\pm$ 0.0 & 1 $\pm$ 0.0 & 1 $\pm$ 0.0 & 1 $\pm$ 0.0 & 1 $\pm$ 0.0 \\
% \hline
% Makespan ($\downarrow$) & 48.445 $\pm$ 24.901 & 61.405 $\pm$ 27.469 & 129.615 $\pm$ 29.922 & 48.985 $\pm$ 23.469 & 64.18 $\pm$ 30.685 & 123.255 $\pm$ 33.927 \\
% \hline
% \end{tabular}
% \caption{Performance Comparison for Blind Navigation Task}
% \label{table:NavigationPerformance}
% \end{table*}

% \begin{table*}[h!]
% \centering
% \begin{tabular}{|c|c|c|c|c|c|c|}
% \hline
% \multirow{2}{*}{Performance Metric} & \multicolumn{3}{c|}{No Attention Entropy} & \multicolumn{3}{c|}{Attention Entropy} \\
%     \cline{2-7}
%      &  N = 3 & N = 4 & N = 5 & N = 3 & N = 4 & N = 5 \\
% \hline
% Reward ($\uparrow$) & 54.5 $\pm$ 12.7 & 44.5 $\pm$ 17.3 & 37.9 $\pm$ 24.3 & 55.6 $\pm$ 23.1 & 32.1 $\pm$ 27.1 & 33.2 $\pm$ 21.7 \\
% \hline
% Success Rate ($\uparrow$) & 1.0 $\pm$ 0.0 & 0.68625 $\pm$ 0.3 & 0.914 $\pm$ 0.24 & 1.0 $\pm$ 0.0 & 0.9975 $\pm$ 0.02 & 0.991 $\pm$ 0.08 \\
% \hline
% No Agent Collision Rate ($\uparrow$) & 0.295 $\pm$ 0.5 & 0.575 $\pm$ 0.5 & 0.105 $\pm$ 0.307 & 0.685 $\pm$ 0.5 & 0.315 $\pm$ 0.5 & 0.215 $\pm$ 0.41 \\
% \hline
% No Object Collision Rate ($\uparrow$) & 0 $\pm$ 0.0 & 0 $\pm$ 0.0 & 0 $\pm$ 0.0 & 0.06 $\pm$ 0.2 & 0 $\pm$ 0.0 & 0 $\pm$ 0.0 \\
% \hline
% Makespan ($\downarrow$) & 45.445 $\pm$ 21.8 & 119.505 $\pm$ 42.4 & 84.885 $\pm$ 37.7 & 49.015 $\pm$ 14.4 & 119.675 $\pm$ 35.3 & 100.2 $\pm$ 33.26 \\
% \hline
% \end{tabular}
% \caption{Performance Comparison for Passage Task}
% \label{table:PassagePerformance}
% \end{table*}

% \begin{table*}[h!]
% \centering
% \begin{tabular}{|c|c|c|c|c|c|c|}
% \hline
% \multirow{2}{*}{Performance Metric} & \multicolumn{3}{c|}{No Attention Entropy} & \multicolumn{3}{c|}{Attention Entropy} \\
%     \cline{2-7}
%      &  N = 3 & N = 4 & N = 5 & N = 3 & N = 4 & N = 5 \\
% \hline
% Reward ($\uparrow$) & 0.9225 $\pm$ 0.27 & 0.985 $\pm$ 0.12 & 0.9225 $\pm$ 0.29 & 0.8775 $\pm$ 0.345 & 0.955 $\pm$ 0.23 & 0.9475 $\pm$ 0.23 \\
% \hline
% Success Rate ($\uparrow$) & 0.92 $\pm$ 0.27 & 0.985 $\pm$ 0.12 & 0.915 $\pm$ 0.28 & 0.87 $\pm$ 0.337 & 0.95 $\pm$ 0.218 & 0.945 $\pm$ 0.23 \\
% \hline
% No Agent Collision Rate ($\uparrow$) & 1 $\pm$ 0.0 & 1 $\pm$ 0.0 & 1 $\pm$ 0.0 & 1 $\pm$ 0.0 & 1 $\pm$ 0.0 & 1 $\pm$ 0.0 \\
% \hline
% No Object Collision Rate ($\uparrow$) & 1 $\pm$ 0.0 & 1 $\pm$ 0.0 & 1 $\pm$ 0.0 & 1 $\pm$ 0.0 & 1 $\pm$ 0.0 & 1 $\pm$ 0.0 \\
% \hline
% Makespan ($\downarrow$) & 112.38 $\pm$ 135.85 & 95.155 $\pm$ 103.32 & 113.245 $\pm$ 133.33 & 145.19 $\pm$ 150.275 & 134.79 $\pm$ 120.83 & 108.95 $\pm$ 116.55 \\
% \hline
% \end{tabular}
% \caption{Performance Comparison for Discovery Task}
% \label{table:DiscoveryPerformance}
% \end{table*}

%%%%%%%%%%%%%%%%%% NO ATTENTION ENTROPY TABLE RAW DATA - THESE WILL BE COMMENTED OUT %%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{table*}[h!]
% \centering
% \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
% \hline
% \multirow{2}{*}{Performance Metric} & \multicolumn{3}{c|}{Blind Navigation} & \multicolumn{3}{c|}{Passage} & \multicolumn{3}{c|}{Search-Rescue} \\
%     \cline{2-10}
%      &  N = 3 & N = 4 & N = 5 & N = 3 & N = 4 & N = 5 & N = 3 & N = 4 & N = 5 \\
% \hline
% Reward & 2.3225897401571274 $\pm$ 0.6442990345920702 & 3.0451605182886126 $\pm$ 0.8150683707259007 & 3.48919437661767 $\pm$ 1.1589072553500306 & 54.51285989522934 $\pm$ 12.695167926031512 & 44.49774878501892 $\pm$ 17.333921599607248 & 37.92856542218477 $\pm$ 24.375521661548277 & 0.9225 $\pm$ 0.27499428957067895 & 0.985 $\pm$ 0.12185748327926266 & 0.9225 $\pm$ 0.2883739720201764 \\
% \hline
% Success Rate & 0.99 $\pm$ 0.09974842727441166 & 0.955 $\pm$ 0.20782433633689137 & 0.435 $\pm$ 0.49700105658967797 & 1.0 $\pm$ 0.0 & 0.68625 $\pm$ 0.34637162704254587 & 0.914 $\pm$ 0.2435169450214149 & 0.92 $\pm$ 0.2719739863410307 & 0.985 $\pm$ 0.12185748327926266 & 0.915 $\pm$ 0.27958152504364203 \\
% \hline
% No Agent Collision Rate & 1 $\pm$ 0.0 & 0.98 $\pm$ 0.14035131799278394 & 0.97 $\pm$ 0.17101529509309654 & 0.295 $\pm$ 0.4571871613491706 & 0.575 $\pm$ 0.4955835096887071 & 0.105 $\pm$ 0.3073226906378774 & 1 $\pm$ 0.0 & 1 $\pm$ 0.0 & 1 $\pm$ 0.0 \\
% \hline
% No Object Collision Rate & N/A & N/A & N/A & 0 $\pm$ 0.0 & 0 $\pm$ 0.0 & 0 $\pm$ 0.0 & N/A & N/A & N/A \\
% \hline
% Makespan & 48.445 $\pm$ 24.900747198734347 & 61.405 $\pm$ 27.468775786628722 & 129.615 $\pm$ 29.922122956285293 & 45.445 $\pm$ 21.824263271338243 & 119.505 $\pm$ 42.351484225235865 & 84.885 $\pm$ 37.70357506741664 & 112.38 $\pm$ 135.8538810079588 & 95.155 $\pm$ 103.32293608199012 & 113.245 $\pm$ 133.3339592845441 \\
% \hline
% \end{tabular}
% \caption{No Attention Entropy}
% \label{table:noAttnEntFull}
% \end{table*}


% %%%%%%%%%%%%%%%%%% ATTENTION ENTROPY TABLE RAW DATA - THESE WILL BE COMMENTED OUT %%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{table*}[h!]
% \centering
% \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
% \hline
% \multirow{2}{*}{Performance Metric} & \multicolumn{3}{c|}{Blind Navigation} & \multicolumn{3}{c|}{Passage} & \multicolumn{3}{c|}{Search-Rescue} \\
%     \cline{2-10}
%      &  N = 3 & N = 4 & N = 5 & N = 3 & N = 4 & N = 5 & N = 3 & N = 4 & N = 5 \\
% \hline
% Reward & 2.286789354085922 $\pm$ 0.5802445826590419 & 3.030672726333141 $\pm$ 0.7019182355574164 & 3.452301803827286 $\pm$ 1.65694840785181 & 55.62005806386471 $\pm$ 23.11759590934693 & 32.104694254398346 $\pm$ 27.08503079649114 & 33.2062195456028 $\pm$ 21.718051160248145 & 0.8775 $\pm$ 0.34545687467657454 & 0.955 $\pm$ 0.2252302004973587 & 0.9475 $\pm$ 0.23186755446605795 \\
% \hline
% Success Rate & 0.99 $\pm$ 0.09974842727441166 & 0.94 $\pm$ 0.23808279460185092 & 0.535 $\pm$ 0.5000251249968752  & 1.0 $\pm$ 0.0 & 0.9975 $\pm$ 0.024937106818602915 & 0.991 $\pm$ 0.08094293541730402 & 0.87 $\pm$ 0.33714735904132886 & 0.95 $\pm$ 0.21849186132974538 & 0.945 $\pm$ 0.22855235921889455 \\
% \hline
% No Agent Collision Rate & 0.995 $\pm$ 0.07071067811865475 & 0.995 $\pm$ 0.07071067811865475 & 0.955 $\pm$ 0.20782433633689137 & 0.685 $\pm$ 0.4656815397698432 & 0.315 $\pm$ 0.4656815397698432 & 0.215 $\pm$ 0.41185326947578 & 1 $\pm$ 0.0 & 1 $\pm$ 0.0 & 1 $\pm$ 0.0 \\
% \hline
% No Object Collision Rate & N/A & N/A & N/A & 0.06 $\pm$ 0.23808279460185092 & 0 $\pm$ 0.0 & 0 $\pm$ 0.0 & N/A & N/A & N/A \\
% \hline
% Makespan & 48.985 $\pm$ 23.469102429454374 & 64.18 $\pm$ 30.684610583045792 & 123.255 $\pm$ 33.926908947821474 & 49.015 $\pm$ 14.402148604361454 & 119.675 $\pm$ 35.256410016281876 & 100.2 $\pm$ 33.26564316341076 & 145.19 $\pm$ 150.27456914146606 & 134.79 $\pm$ 120.83023474338161 & 108.95 $\pm$ 116.55471896308109 \\
% \hline
% \end{tabular}
% \caption{Attention Entropy}
% \label{table:attnEntFull}
% \end{table*}

% \subsection{Comparing Existing Explainers}

\subsection{Results on Explanation Fidelity}
\label{fidelityResults}
\figureautorefname~\ref{fig:deltaFidelity} shows the delta fidelity results for each of the three team sizes and each of the three tasks. Note that a higher delta fidelity is desirable, as this indicates that the explanation subgraph is both necessary and sufficient.

\textit{Comparing explainers}: We first analyze each explanation method without attention entropy with respect to delta fidelity (blue violin plots in \figureautorefname~\ref{fig:deltaFidelity}). We observe that GraphMask tends to have the least delta fidelity, followed by AttentionExplainer, with GNNExplainer yielding the highest delta fidelity. The low delta fidelity of GraphMask is likely due to its sole reliance on hard binary masks, which constrains the space of possible subgraph explanations. 
% In contrast, GNNExplainer and AttentionExplainer use soft continuous masks for the edges, allowing for a greater range of possible subgraph explanations. 
\sk{
GNNExplainer and AttentionExplainer likely outperform than GraphMask since they produce more expressive subgraphs and allow for varying degrees of inter-agent influences as they use soft edge masks (c.f. binary hard-masks in GraphMask). Unlike AttentionExplainer, GNNExplainer is compatible with any GNN (i.e. GCN, GAT, etc) and does not rely on the model being self-interpretable. Without attention entropy regularization, the attention values are more noisy and likely cause AttentionExplainer to perform worse than GNNExplainer. 
}
% GraphMask, however, had the lowest variance in the delta fidelity measures, followed by GNNExplainer, and then by AttentionExplainer with the highest variance. 
These observations suggest that GNNExplainer provides the best explanations with respect to delta fidelity when employed out-of-the-box. 


% \textit{With regularization}:  Now, we examine each explanation method with the addition of attention entropy minimization. We observe that GraphMask still tends to have the worst delta fidelity measures. 
% % However, we see that AttentionExplainer outperforms GNNExplainer when attention entropy is minimized. 
% % The addition of attention entropy likely enables Attention Explainer to produce the explanations with respect to delta fidelity. 
% % However, we note that GNNExplainer still tends to have lower variance, and GraphMask has the lowest variance out of the three methods with respect to delta fidelity measures.
% \sk{
% GNNExplainer and AttentionExplainer still perform better than GraphMask since they produce more expressive subgraphs and allow for varying degrees of inter-agent influences as they use soft edge masks (c.f. binary hard-masks in GraphMask). 
% However, unlike without attention entropy minimization, AttentionExplainer now outperforms GNNExplainer. 
% % The attention entropy minimization results in a comparably performing policy that is more self-interpretable. 
% The inclusion of attention entropy boosts the correlation between the attention values (which can be interpreted as a subgraph) and the GNN model behavior, in line with the insights from the theoretical analysis.
% }

% \textit{Impact of regularization}: When analyzing the impact of entropy regularization on fidelity, we immediately note that the delta fidelity of the Attention Explainer improves across all team sizes and tasks, further supporting the intuitions and theoretical analysis derived from the previous sections. GraphMask has no discernible improvement or deterioration with respect to delta fidelity when using attention entropy minimization. GNNExplainer, on the other hand, has mixed results. Of the 9 experimental configurations, 5 of them yield a marginal improvement in delta fidelity, 3 of them yield a marginal deterioration in delta fidelity, and 1 yields no improvement or deterioration in delta fidelity.
% \sk{
% The lack of improvement or deterioration in explanation fidelity by GraphMask is because GraphMask gains an improvement in negative fidelity, but also incurs a deterioration in positive fidelity (see appendix). This means that the explanations generated by GraphMask on a model trained with attention entropy minimization contain a larger subset of the salient edges but do not capture all the salient edges due to being restricted to hard binary masks. While this results in no discernible improvement in delta fidelity, the benefits of GraphMask being able to capture a larger subset of the salient edges when the model is trained using attention entropy minimization can be observed in the improvement in negative fidelity (see appendix) and, more importantly, the improvement in unfaithfulness (see \sectionautorefname~\ref{unfaithResults}).

% GNNExplainer likely obtains mixed results with respect to explanation fidelity improvement due to the difficulty of the optimization problem formulated in \cite{gnnexplainer}. In \cite{gnnexplainer}, the original objective is intractable to solve, so the authors make assumptions that tend to work well for the graph datasets that they evaluate on. Moreover, the authors acknowledge that the assumptions they make and the objective they solve for lead to a good local minima, implying that the strategy employed by GNNExplainer likely misses better local minima. Thus, given the need of GNNExplainer to be generalizable for different GNN-based architectures, the inclusion of attention entropy minimization may not inherently improve the optimization landscape that GNNExplainer attempts to solve despite the fact that the subgraphs induced by the attention values themselves better represent the underlying model.
% }

\textit{Impact of regularization}: Now we examine the impact of attention entropy minimization on each explainer's fidelity measures and also compare them with one another. We observe that the regularization has had no discernible impact on GraphMask as it still tends to have the worst delta fidelity measures. This is because GraphMask gains an improvement in negative fidelity, but also incurs a deterioration in positive fidelity (see appendix). This means that the explanations generated by GraphMask on a model trained with attention entropy minimization contain a larger subset of the salient edges but do not capture all the salient edges due to the restriction of hard binary masks. GNNExplainer and AttentionExplainer continue to perform better than GraphMask after the regularization. 

However, unlike without attention entropy minimization, AttentionExplainer now outperforms GNNExplainer. 
This is likely because inclusion of attention entropy boosts the correlation between the attention values (which can be interpreted as a subgraph) and the GNN model behavior, in line with the insights from the theoretical analysis. 
Further, entropy minimization likely obtains mixed results for GNNExplainer with respect to fidelity due to the difficulty of the optimization problem \cite{gnnexplainer}. The original objective of GNNExplainer is intractable to solve, requiring assumptions that tend to work well for the graph datasets for which it was designed. 
% Moreover, the authors acknowledge that the assumptions they make and the objective they solve for lead to a good local minima, implying that the strategy employed by GNNExplainer likely misses better local minima. 
Thus, the inclusion of attention entropy minimization may not inherently improve the optimization landscape that GNNExplainer attempts to solve despite the fact that the subgraphs induced by the attention values themselves better represent the underlying model. 

\begin{figure}[tb]
    \centering
    \includegraphics[trim={0.1cm 0.65cm 1.55cm 0.8cm},clip,width=0.325\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig_no_legend_big_font/boxplot_analysis-navigation-3_agent-Unfaithfulness.png}
    \includegraphics[trim={0.5cm 0.65cm 1.55cm 0.8cm},clip,width=0.317\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig_no_legend_big_font/boxplot_analysis-navigation-4_agent-Unfaithfulness.png}
    \includegraphics[trim={0.5cm 0.65cm 1.55cm 0.8cm},clip,width=0.317\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig_no_legend_big_font/boxplot_analysis-navigation-5_agent-Unfaithfulness.png}
    
    \includegraphics[trim={0.1cm 0.65cm 1.55cm 0.8cm},clip,width=0.325\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig_no_legend_big_font/boxplot_analysis-passage-3_agent-Unfaithfulness.png}
    \includegraphics[trim={0.5cm 0.65cm 1.55cm 0.8cm},clip,width=0.317\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig_no_legend_big_font/boxplot_analysis-passage-4_agent-Unfaithfulness.png}
    \includegraphics[trim={0.5cm 0.65cm 1.55cm 0.8cm},clip,width=0.317\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig_no_legend_big_font/boxplot_analysis-passage-5_agent-Unfaithfulness.png}
    
    \includegraphics[trim={0.1cm 0.65cm 1.55cm 0.8cm},clip,width=0.325\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig_no_legend_big_font/boxplot_analysis-discovery-3_agent-Unfaithfulness.png}
    \includegraphics[trim={0.5cm 0.65cm 1.55cm 0.8cm},clip,width=0.317\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig_no_legend_big_font/boxplot_analysis-discovery-4_agent-Unfaithfulness.png}
    \includegraphics[trim={0.5cm 0.65cm 1.55cm 0.8cm},clip,width=0.317\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig_no_legend_big_font/boxplot_analysis-discovery-5_agent-Unfaithfulness.png}
    \caption{Unfaithfulness ($\downarrow$, \sectionautorefname~\ref{unfaithResults}) of explanations generated by three explainers \textcolor{blue}{\textbf{without (blue)}} and \textcolor{orange}{\textbf{with (orange)}} proposed regularization across three tasks (rows) and three team sizes (columns). Lower unfaithfulness is better.
    % \textit{Top row}: Blind navigation task, \textit{Middle row}: Passage task, \textit{Bottom row}: Discovery task. Lower is better.
    }
    \label{fig:unfaithfulness}
\end{figure}

\begin{table*}[bt]
\scriptsize
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\multicolumn{7}{|c|}{\textbf{Blind Navigation}} \\
\hline
\multirow{2}{*}{Performance Metric} & \multicolumn{2}{c|}{N = 3} & \multicolumn{2}{c|}{N = 4} & \multicolumn{2}{c|}{N = 5} \\
    \cline{2-7}
     &  No Att. Ent. Min. & Att. Ent. Min. & No Att. Ent. Min. & Att. Ent. Min. & No Att. Ent. Min. & Att. Ent. Min. \\
\hline
Reward ($\uparrow$) & 2.32 $\pm$ 0.64 & 2.29 $\pm$ 0.58 & 3.05 $\pm$ 0.82 & 3.03 $\pm$ 0.70 & 4.75 $\pm$ 1.10 & 4.64 $\pm$ 2.04 \\
\hline
Success Rate ($\uparrow$) & 0.99 $\pm$ 0.10 & 0.99 $\pm$ 0.10 & 0.96 $\pm$ 0.21 & 0.94 $\pm$ 0.24 & 0.80 $\pm$ 0.40 & 0.90 $\pm$ 0.30 \\
\hline
No Agent Coll. Rate ($\uparrow$) & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.07 & 0.98 $\pm$ 0.14 & 1.0 $\pm$ 0.07 & 0.94 $\pm$ 0.22 & 0.94 $\pm$ 0.24 \\
\hline
% No Object Coll. Rate ($\uparrow$) & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 \\
% \hline
Makespan ($\downarrow$) & 48.45 $\pm$ 24.90 & 48.99 $\pm$ 23.47 & 61.41 $\pm$ 27.47 & 64.18 $\pm$ 30.69 & 200.92 $\pm$ 116.44 & 175.86 $\pm$ 104.04 \\
\hline
\hline
\multicolumn{7}{|c|}{\textbf{Passage}} \\
\hline
\multirow{2}{*}{Performance Metric} & \multicolumn{2}{c|}{N = 3} & \multicolumn{2}{c|}{N = 4} & \multicolumn{2}{c|}{N = 5} \\
    \cline{2-7}
     &  No Att. Ent. Min. & Att. Ent. Min. & No Att. Ent. Min. & Att. Ent. Min. & No Att. Ent. Min. & Att. Ent. Min. \\
\hline
Reward ($\uparrow$) & 54.5 $\pm$ 12.7 & 55.6 $\pm$ 23.1 & 44.5 $\pm$ 17.3 & 32.1 $\pm$ 27.1 & 37.9 $\pm$ 24.3 & 33.2 $\pm$ 21.7 \\
\hline
Success Rate ($\uparrow$) & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 0.69 $\pm$ 0.3 & 1.0 $\pm$ 0.02 & 0.91 $\pm$ 0.24 & 0.99 $\pm$ 0.08 \\
\hline
No Agent Coll. Rate ($\uparrow$) & 0.30 $\pm$ 0.50 & 0.69 $\pm$ 0.50 & 0.58 $\pm$ 0.50 & 0.32 $\pm$ 0.50 & 0.11 $\pm$ 0.31 & 0.22 $\pm$ 0.41 \\
\hline
No Object Coll. Rate ($\uparrow$) & 0.0 $\pm$ 0.0 & 0.06 $\pm$ 0.2 & 0.0 $\pm$ 0.0 & 0.0 $\pm$ 0.0 & 0.0 $\pm$ 0.0 & 0.0 $\pm$ 0.0 \\
\hline
Makespan ($\downarrow$) & 45.445 $\pm$ 21.8 & 49.015 $\pm$ 14.4 & 119.505 $\pm$ 42.4 & 119.675 $\pm$ 35.3 & 84.885 $\pm$ 37.7 & 100.2 $\pm$ 33.26 \\
\hline
\hline
\multicolumn{7}{|c|}{\textbf{Discovery}} \\
\hline
\multirow{2}{*}{Performance Metric} & \multicolumn{2}{c|}{N = 3} & \multicolumn{2}{c|}{N = 4} & \multicolumn{2}{c|}{N = 5} \\
    \cline{2-7}
     &  No Att. Ent. Min. & Att. Ent. Min. & No Att. Ent. Min. & Att. Ent. Min. & No Att. Ent. Min. & Att. Ent. Min. \\
\hline
Reward ($\uparrow$) & 0.92 $\pm$ 0.27 & 0.88 $\pm$ 0.35 & 0.99 $\pm$ 0.12 & 0.96 $\pm$ 0.23 & 0.92 $\pm$ 0.29 & 0.95 $\pm$ 0.23 \\
\hline
Success Rate ($\uparrow$) & 0.92 $\pm$ 0.27 & 0.87 $\pm$ 0.34 & 0.99 $\pm$ 0.12 & 0.95 $\pm$ 0.22 & 0.92 $\pm$ 0.28 & 0.95 $\pm$ 0.23 \\
\hline
% No Agent Coll. Rate ($\uparrow$) & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1 $\pm$ 0.0 & 1.0 $\pm$ 0.0 \\
% \hline
% No Object Coll. Rate ($\uparrow$) & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 \\
% \hline
Makespan ($\downarrow$) & 112.38 $\pm$ 135.85 & 145.19 $\pm$ 150.28 & 95.16 $\pm$ 103.32 & 134.79 $\pm$ 120.83 & 113.25 $\pm$ 133.33 & 108.95 $\pm$ 116.55 \\
\hline
\end{tabular}
\caption{Attention entropy minimization results in little to no degradation in task performance across all tasks and team sizes.}
\label{table:AllTasksPerformance}
\end{table*}

\subsection{Results on Explanation Faithfulness}
\label{unfaithResults}

\figureautorefname~\ref{fig:unfaithfulness} shows the unfaithfulness results for each of the three team sizes and each of the three tasks. Note that a \textit{lower} unfaithfulness is generally desirable, as this indicates that the explanation subgraph is more faithful to the underlying prediction distribution.

\textit{Comparing explainers}: We first analyze each explanation method without attention entropy with respect to unfaithfulness (blue violin plots in \figureautorefname~\ref{fig:unfaithfulness}). We observe that, out of the three explanation methods, GraphMask tends to have the worst unfaithfulness measures. Of the 9 experimental configurations, 5 configurations show AttentionExplainer having marginally better unfaithfulness measures compared to GNNExplainer, and the remaining 4 configurations show the opposite. Once again, the high unfaithfulness measures from GraphMask are likely due to its use of hard binary masks. 
% In contrast, GNNExplainer and AttentionExplainer use soft continuous masks for the edges, allowing for a greater space of subgraph explanations to choose from.
\sk{
Especially when the attention values are diffuse, GNNExplainer and AttentionExplainer are more likely to be able to capture varying degrees of inter-agent influences via soft edges. On the other hand, GraphMask either preserves or drops an edge, making it difficult to capture more diffuse inter-agent influences, which are more likely to occur when the model is trained without regularization. 
% These observations suggest that both GNNExplainer and AttentionExplainer are preferable to GraphMask with respect to faithfulness when employed out-of-the-box.
}

% \textit{With regularization}: Now, we examine each explanation method with the addition of attention entropy minimization to policy training. We observe that, out of the three explanation methods, GraphMask still tends to have the highest unfaithfulness measures. However, unlike before, we now see that AttentionExplainer generally has the lowest unfaithfulness measures out of the three. The addition of attention entropy enables Attention Explainer to produce the highest quality of explanation with respect to unfaithfulness. 
% \sk{
% Once again, this can likely be attributed to the fact that the minimizing attention entropy boosts the correlation between the subgraph induced by the attention values and the inter-agent influences considered by the underlying model. This also supports the insights from our theoretical analysis.
% }

% \textit{Impact of regularization}: When analyzing the impact of entropy regularization on fidelity, we immediately note that the unfaithfulness of the Attention Explainer decreases across all team sizes and tasks, further supporting the intuitions and theoretical analysis derived from the previous sections. Moreover, we see a similar decrease in unfaithfulness measures across all team sizes and tasks for GraphMask as well, indicating that GraphMask produces more faithful explanations when the policy is trained using attention entropy minimization. GNNExplainer, however, has a mixed response with respect to attention entropy minimization. Of the 9 experimental configurations, 5 of them result in a lower unfaithfulness measure (indicating a more faithful explanation generated), while the remaining 4 of them result in a higher unfaithfulness measure (indicating a less faithful explanation generated).
% \sk{
% As previously mentioned, attention entropy minimization results in a comparably performing policy that is more self-interpretable. The inclusion of attention entropy boosts the correlation between the attention values (which can be interpreted as a subgraph) and the GNN model behavior, which we attribute to the insights derived from the theoretical analysis. This can be observed by the improvement in the explanation faithfulness of AttentionExplainer. However, we also note an improvement in explanation faithfulness for GraphMask. This can be likely attributed to the fact that the attention distribution is sparser, resulting in agent-agent influences that are closer to a subgraph composed of only a binary adjacency matrix. As such, GraphMask can capture more of the salient edges, and those salient edges correspond more closely with the actual importance of the edge in terms of weight. Because of this, the explanation faithfulness from GraphMask improves, as well as the negative fidelity of the explanations from GraphMask. However, since the attention values are still continuous and real-valued, GraphMask is still limited in its ability to capture the edge instances that are more diffuse, leading to a decrease in positive fidelity. This is why while the explanation faithfulness improves, the explanation delta fidelity remains the same.
% }

\textit{Impact of regularization}: Now we examine the impact of attention entropy minimization on each explainer's faithfulness and also compare them with one another. The regularization consistently reduces GraphMask's unfaithfulness. This is likely due to the regularization making the attention distribution sparser, resulting in agent-agent influences that are closer to a subgraph composed of only a binary adjacency matrix. As such, GraphMask can capture more of the salient edges even when employing a hard mask. However, this boost from regularization seems to be insufficient to overcome all the limitations of hard masks since GraphMask continues to underperform the other two explainers with respect to faithfulness. In contrast, the regularization significantly improves the faithfulness of AttentionExplainer across all team sizes and tasks, enabling it to now consistently outperform GNNExplainer. Once again, this can likely be attributed to the fact that the minimizing attention entropy boosts the correlation between the subgraph induced by the attention values and the inter-agent influences considered by the underlying model. Similar to what we observed for fidelity, minimizing attention entropy seems to have a mixed impact on GNNExplainer's faithfulness.



% There are a couple instances where the delta fidelity improves, yet the unfaithfulness increases, and vice versa. We believe this may result from the objective that GNNExplainer optimizes for, and what sort of explanations GNNExplainer attempts to produce. As previously mentioned in \sectionautorefname~\ref{gnnExplainerPrelim}, GNNExplainer seeks to find a subgraph that maximizes the probability of the original GNN output, which includes minimizing the variance or uncertainty around that original GNN output. This implies that GNNExplainer will prune edges that are not only contributing zero informativeness or entropy to the GNN output, but also any edges that are increasing the entropy (i.e., detracting from the original GNN output). However, due to the unsymmetric nature of the $KL$ divergence, this can be problematic, as this can lead to a large $KL$ divergence. Essentially, an explanation that yields a similar prediction to the original model but with lower variance can yield a large $KL$, causing a poor faithfulness measure despite a great fidelity measure. Similarly, an explanation that yields a slightly worse point estimate prediction but causes a large probability over that area (i.e. similar variance) can yield better faithfulness measure at the expense of a worse fidelity measure. 

\subsection{Impact on Task Performance}
We also measured the task performance comparison between the presence and absence of attention entropy minimization when training the policy (see Appendix for metrics). Generally, it is common to expect some performance loss in pursuit of explainability, but a model that is more explainable but does not perform the task well is not useful. Moreover, a potential concern of augmenting the RL objective with attention entropy is the reduction in policy performance. Reassuringly, \tableautorefname~\ref{table:AllTasksPerformance} shows that, in general, little to no task performance is lost when including attention entropy minimization into the learning objective.

\subsection{Additional Experiments}
We conduct two additional experimental studies in conjunction with the above evaluation. The first study looks at the explanation quality over the course of model training, results can be found in \ref{subsec:explanation-quality-training}. The second study assesses the explanation quality in zero-shot deployment of trained policies on larger team sizes than the training team size, results can be found in \ref{subsec:explanation-quality-zero-shot}. The trends and findings from these studies are generally similar to the trends and findings of the aforementioned results. In particular, we continue to observe that GraphMask improves in faithfulness and AttentionExplainer improve in overall explanation quality while GNNExplainer garners mixed improvements when the model is trained using attention entropy minimization.

% \section{Limitations}
% In our empirical analysis, we attempted to cover a wide variety of potential factors by considering multiple tasks in multi-robot coordination and architectural hyperparameters associated with multi-robot coordination. However, there are certain aspects not included in this work that can be further studied in future work. The first is other learning paradigms, such as imitation learning. The current study only focuses on reinforcement learning as finding an expert or demonstration dataset for multi-agent systems is challenging, especially for the chosen tasks. The second is alternative neural architecture paradigms. We focus on the most representative structure of encoder-GNN-decoder, but we focused mostly on MLPs as opposed to other types of neural architectures. Finally, there are other potential ways to generate sparse attention or edge weights besides attention entropy minimization. Especially for GraphMask, one could try sampling the attention weights as a Bernoulli trial to generate hard edge masks, which could be more in-distribution for GraphMask. These are all left for future work and study.

\section{Conclusion}
Our work lays the foundation for the adoption of GNN explainers to explain graph-based multi-agent policies. We conducted the first empirical evaluation of GNN explanation methods in explaining policies trained on established multi-agent tasks across varying team sizes. We find that existing GNN explanation methods have the potential to identify the most influential communication channels impacting the team's decisions. Further, we proposed attention entropy minimization as an effective regularization to improve explanation quality for graph attention-based policies, with theoretical motivations grounded in maximizing the separation between the subgraph and its complement. Future work could improve GNN explanation methods for MARL settings and the interpretability of subgraph explanations with natural language summarization.

% \section{Electronic Submission}
% \label{submission}

% Submission to ICML 2025 will be entirely electronic, via a web site
% (not email). Information about the submission process and \LaTeX\ templates
% are available on the conference web site at:
% \begin{center}
% \textbf{\texttt{http://icml.cc/}}
% \end{center}

% The guidelines below will be enforced for initial submissions and
% camera-ready copies. Here is a brief summary:
% \begin{itemize}
% \item Submissions must be in PDF\@. 
% \item If your paper has appendices, submit the appendix together with the main body and the references \textbf{as a single file}. Reviewers will not look for appendices as a separate PDF file. So if you submit such an extra file, reviewers will very likely miss it.
% \item Page limit: The main body of the paper has to be fitted to 8 pages, excluding references and appendices; the space for the latter two is not limited in pages, but the total file size may not exceed 10MB. For the final version of the paper, authors can add one extra page to the main body.
% \item \textbf{Do not include author information or acknowledgements} in your
%     initial submission.
% \item Your paper should be in \textbf{10 point Times font}.
% \item Make sure your PDF file only uses Type-1 fonts.
% \item Place figure captions \emph{under} the figure (and omit titles from inside
%     the graphic file itself). Place table captions \emph{over} the table.
% \item References must include page numbers whenever possible and be as complete
%     as possible. Place multiple citations in chronological order.
% \item Do not alter the style template; in particular, do not compress the paper
%     format by reducing the vertical spaces.
% \item Keep your abstract brief and self-contained, one paragraph and roughly
%     4--6 sentences. Gross violations will require correction at the
%     camera-ready phase. The title should have content words capitalized.
% \end{itemize}

% \subsection{Submitting Papers}

% \textbf{Anonymous Submission:} ICML uses double-blind review: no identifying
% author information may appear on the title page or in the paper
% itself. \cref{author info} gives further details.

% \medskip

% Authors must provide their manuscripts in \textbf{PDF} format.
% Furthermore, please make sure that files contain only embedded Type-1 fonts
% (e.g.,~using the program \texttt{pdffonts} in linux or using
% File/DocumentProperties/Fonts in Acrobat). Other fonts (like Type-3)
% might come from graphics files imported into the document.

% Authors using \textbf{Word} must convert their document to PDF\@. Most
% of the latest versions of Word have the facility to do this
% automatically. Submissions will not be accepted in Word format or any
% format other than PDF\@. Really. We're not joking. Don't send Word.

% Those who use \textbf{\LaTeX} should avoid including Type-3 fonts.
% Those using \texttt{latex} and \texttt{dvips} may need the following
% two commands:

% {\footnotesize
% \begin{verbatim}
% dvips -Ppdf -tletter -G0 -o paper.ps paper.dvi
% ps2pdf paper.ps
% \end{verbatim}}
% It is a zero following the ``-G'', which tells dvips to use
% the config.pdf file. Newer \TeX\ distributions don't always need this
% option.

% Using \texttt{pdflatex} rather than \texttt{latex}, often gives better
% results. This program avoids the Type-3 font problem, and supports more
% advanced features in the \texttt{microtype} package.

% \textbf{Graphics files} should be a reasonable size, and included from
% an appropriate format. Use vector formats (.eps/.pdf) for plots,
% lossless bitmap formats (.png) for raster graphics with sharp lines, and
% jpeg for photo-like images.

% The style file uses the \texttt{hyperref} package to make clickable
% links in documents. If this causes problems for you, add
% \texttt{nohyperref} as one of the options to the \texttt{icml2025}
% usepackage statement.


% \subsection{Submitting Final Camera-Ready Copy}

% The final versions of papers accepted for publication should follow the
% same format and naming convention as initial submissions, except that
% author information (names and affiliations) should be given. See
% \cref{final author} for formatting instructions.

% The footnote, ``Preliminary work. Under review by the International
% Conference on Machine Learning (ICML). Do not distribute.'' must be
% modified to ``\textit{Proceedings of the
% $\mathit{42}^{nd}$ International Conference on Machine Learning},
% Vancouver, Canada, PMLR 267, 2025.
% Copyright 2025 by the author(s).''

% For those using the \textbf{\LaTeX} style file, this change (and others) is
% handled automatically by simply changing
% $\mathtt{\backslash usepackage\{icml2025\}}$ to
% $$\mathtt{\backslash usepackage[accepted]\{icml2025\}}$$
% Authors using \textbf{Word} must edit the
% footnote on the first page of the document themselves.

% Camera-ready copies should have the title of the paper as running head
% on each page except the first one. The running title consists of a
% single line centered above a horizontal rule which is $1$~point thick.
% The running head should be centered, bold and in $9$~point type. The
% rule should be $10$~points above the main text. For those using the
% \textbf{\LaTeX} style file, the original title is automatically set as running
% head using the \texttt{fancyhdr} package which is included in the ICML
% 2025 style file package. In case that the original title exceeds the
% size restrictions, a shorter form can be supplied by using

% \verb|\icmltitlerunning{...}|

% just before $\mathtt{\backslash begin\{document\}}$.
% Authors using \textbf{Word} must edit the header of the document themselves.

% \section{Format of the Paper}

% All submissions must follow the specified format.

% \subsection{Dimensions}




% The text of the paper should be formatted in two columns, with an
% overall width of 6.75~inches, height of 9.0~inches, and 0.25~inches
% between the columns. The left margin should be 0.75~inches and the top
% margin 1.0~inch (2.54~cm). The right and bottom margins will depend on
% whether you print on US letter or A4 paper, but all final versions
% must be produced for US letter size.
% Do not write anything on the margins.

% The paper body should be set in 10~point type with a vertical spacing
% of 11~points. Please use Times typeface throughout the text.

% \subsection{Title}

% The paper title should be set in 14~point bold type and centered
% between two horizontal rules that are 1~point thick, with 1.0~inch
% between the top rule and the top edge of the page. Capitalize the
% first letter of content words and put the rest of the title in lower
% case.

% \subsection{Author Information for Submission}
% \label{author info}

% ICML uses double-blind review, so author information must not appear. If
% you are using \LaTeX\/ and the \texttt{icml2025.sty} file, use
% \verb+\icmlauthor{...}+ to specify authors and \verb+\icmlaffiliation{...}+ to specify affiliations. (Read the TeX code used to produce this document for an example usage.) The author information
% will not be printed unless \texttt{accepted} is passed as an argument to the
% style file.
% Submissions that include the author information will not
% be reviewed.

% \subsubsection{Self-Citations}

% If you are citing published papers for which you are an author, refer
% to yourself in the third person. In particular, do not use phrases
% that reveal your identity (e.g., ``in previous work \cite{langley00}, we
% have shown \ldots'').

% Do not anonymize citations in the reference section. The only exception are manuscripts that are
% not yet published (e.g., under submission). If you choose to refer to
% such unpublished manuscripts \cite{anonymous}, anonymized copies have
% to be submitted
% as Supplementary Material via OpenReview\@. However, keep in mind that an ICML
% paper should be self contained and should contain sufficient detail
% for the reviewers to evaluate the work. In particular, reviewers are
% not required to look at the Supplementary Material when writing their
% review (they are not required to look at more than the first $8$ pages of the submitted document).

% \subsubsection{Camera-Ready Author Information}
% \label{final author}

% If a paper is accepted, a final camera-ready copy must be prepared.
% %
% For camera-ready papers, author information should start 0.3~inches below the
% bottom rule surrounding the title. The authors' names should appear in 10~point
% bold type, in a row, separated by white space, and centered. Author names should
% not be broken across lines. Unbolded superscripted numbers, starting 1, should
% be used to refer to affiliations.

% Affiliations should be numbered in the order of appearance. A single footnote
% block of text should be used to list all the affiliations. (Academic
% affiliations should list Department, University, City, State/Region, Country.
% Similarly for industrial affiliations.)

% Each distinct affiliations should be listed once. If an author has multiple
% affiliations, multiple superscripts should be placed after the name, separated
% by thin spaces. If the authors would like to highlight equal contribution by
% multiple first authors, those authors should have an asterisk placed after their
% name in superscript, and the term ``\textsuperscript{*}Equal contribution"
% should be placed in the footnote block ahead of the list of affiliations. A
% list of corresponding authors and their emails (in the format Full Name
% \textless{}email@domain.com\textgreater{}) can follow the list of affiliations.
% Ideally only one or two names should be listed.

% A sample file with author names is included in the ICML2025 style file
% package. Turn on the \texttt{[accepted]} option to the stylefile to
% see the names rendered. All of the guidelines above are implemented
% by the \LaTeX\ style file.

% \subsection{Abstract}

% The paper abstract should begin in the left column, 0.4~inches below the final
% address. The heading `Abstract' should be centered, bold, and in 11~point type.
% The abstract body should use 10~point type, with a vertical spacing of
% 11~points, and should be indented 0.25~inches more than normal on left-hand and
% right-hand margins. Insert 0.4~inches of blank space after the body. Keep your
% abstract brief and self-contained, limiting it to one paragraph and roughly 4--6
% sentences. Gross violations will require correction at the camera-ready phase.

% \subsection{Partitioning the Text}

% You should organize your paper into sections and paragraphs to help
% readers place a structure on the material and understand its
% contributions.

% \subsubsection{Sections and Subsections}

% Section headings should be numbered, flush left, and set in 11~pt bold
% type with the content words capitalized. Leave 0.25~inches of space
% before the heading and 0.15~inches after the heading.

% Similarly, subsection headings should be numbered, flush left, and set
% in 10~pt bold type with the content words capitalized. Leave
% 0.2~inches of space before the heading and 0.13~inches afterward.

% Finally, subsubsection headings should be numbered, flush left, and
% set in 10~pt small caps with the content words capitalized. Leave
% 0.18~inches of space before the heading and 0.1~inches after the
% heading.

% Please use no more than three levels of headings.

% \subsubsection{Paragraphs and Footnotes}

% Within each section or subsection, you should further partition the
% paper into paragraphs. Do not indent the first line of a given
% paragraph, but insert a blank line between succeeding ones.

% You can use footnotes\footnote{Footnotes
% should be complete sentences.} to provide readers with additional
% information about a topic without interrupting the flow of the paper.
% Indicate footnotes with a number in the text where the point is most
% relevant. Place the footnote in 9~point type at the bottom of the
% column in which it appears. Precede the first footnote in a column
% with a horizontal rule of 0.8~inches.\footnote{Multiple footnotes can
% appear in each column, in the same order as they appear in the text,
% but spread them across columns and pages if possible.}

% \begin{figure}[ht]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=\columnwidth]{icml_numpapers}}
% \caption{Historical locations and number of accepted papers for International
% Machine Learning Conferences (ICML 1993 -- ICML 2008) and International
% Workshops on Machine Learning (ML 1988 -- ML 1992). At the time this figure was
% produced, the number of accepted papers for ICML 2008 was unknown and instead
% estimated.}
% \label{icml-historical}
% \end{center}
% \vskip -0.2in
% \end{figure}

% \subsection{Figures}

% You may want to include figures in the paper to illustrate
% your approach and results. Such artwork should be centered,
% legible, and separated from the text. Lines should be dark and at
% least 0.5~points thick for purposes of reproduction, and text should
% not appear on a gray background.

% Label all distinct components of each figure. If the figure takes the
% form of a graph, then give a name for each axis and include a legend
% that briefly describes each curve. Do not include a title inside the
% figure; instead, the caption should serve this function.

% Number figures sequentially, placing the figure number and caption
% \emph{after} the graphics, with at least 0.1~inches of space before
% the caption and 0.1~inches after it, as in
% \cref{icml-historical}. The figure caption should be set in
% 9~point type and centered unless it runs two or more lines, in which
% case it should be flush left. You may float figures to the top or
% bottom of a column, and you may set wide figures across both columns
% (use the environment \texttt{figure*} in \LaTeX). Always place
% two-column figures at the top or bottom of the page.

% \subsection{Algorithms}

% If you are using \LaTeX, please use the ``algorithm'' and ``algorithmic''
% environments to format pseudocode. These require
% the corresponding stylefiles, algorithm.sty and
% algorithmic.sty, which are supplied with this package.
% \cref{alg:example} shows an example.

% \begin{algorithm}[tb]
%    \caption{Bubble Sort}
%    \label{alg:example}
% \begin{algorithmic}
%    \STATE {\bfseries Input:} data $x_i$, size $m$
%    \REPEAT
%    \STATE Initialize $noChange = true$.
%    \FOR{$i=1$ {\bfseries to} $m-1$}
%    \IF{$x_i > x_{i+1}$}
%    \STATE Swap $x_i$ and $x_{i+1}$
%    \STATE $noChange = false$
%    \ENDIF
%    \ENDFOR
%    \UNTIL{$noChange$ is $true$}
% \end{algorithmic}
% \end{algorithm}

% \subsection{Tables}

% You may also want to include tables that summarize material. Like
% figures, these should be centered, legible, and numbered consecutively.
% However, place the title \emph{above} the table with at least
% 0.1~inches of space before the title and the same after it, as in
% \cref{sample-table}. The table title should be set in 9~point
% type and centered unless it runs two or more lines, in which case it
% should be flush left.

% % Note use of \abovespace and \belowspace to get reasonable spacing
% % above and below tabular lines.

% \begin{table}[t]
% \caption{Classification accuracies for naive Bayes and flexible
% Bayes on various data sets.}
% \label{sample-table}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{lcccr}
% \toprule
% Data set & Naive & Flexible & Better? \\
% \midrule
% Breast    & 95.9$\pm$ 0.2& 96.7$\pm$ 0.2& $\surd$ \\
% Cleveland & 83.3$\pm$ 0.6& 80.0$\pm$ 0.6& $\times$\\
% Glass2    & 61.9$\pm$ 1.4& 83.8$\pm$ 0.7& $\surd$ \\
% Credit    & 74.8$\pm$ 0.5& 78.3$\pm$ 0.6&         \\
% Horse     & 73.3$\pm$ 0.9& 69.7$\pm$ 1.0& $\times$\\
% Meta      & 67.1$\pm$ 0.6& 76.5$\pm$ 0.5& $\surd$ \\
% Pima      & 75.1$\pm$ 0.6& 73.9$\pm$ 0.5&         \\
% Vehicle   & 44.9$\pm$ 0.6& 61.5$\pm$ 0.4& $\surd$ \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}

% Tables contain textual material, whereas figures contain graphical material.
% Specify the contents of each row and column in the table's topmost
% row. Again, you may float tables to a column's top or bottom, and set
% wide tables across both columns. Place two-column tables at the
% top or bottom of the page.

% \subsection{Theorems and such}
% The preferred way is to number definitions, propositions, lemmas, etc. consecutively, within sections, as shown below.
% \begin{definition}
% \label{def:inj}
% A function $f:X \to Y$ is injective if for any $x,y\in X$ different, $f(x)\ne f(y)$.
% \end{definition}
% Using \cref{def:inj} we immediate get the following result:
% \begin{proposition}
% If $f$ is injective mapping a set $X$ to another set $Y$, 
% the cardinality of $Y$ is at least as large as that of $X$
% \end{proposition}
% \begin{proof} 
% Left as an exercise to the reader. 
% \end{proof}
% \cref{lem:usefullemma} stated next will prove to be useful.
% \begin{lemma}
% \label{lem:usefullemma}
% For any $f:X \to Y$ and $g:Y\to Z$ injective functions, $f \circ g$ is injective.
% \end{lemma}
% \begin{theorem}
% \label{thm:bigtheorem}
% If $f:X\to Y$ is bijective, the cardinality of $X$ and $Y$ are the same.
% \end{theorem}
% An easy corollary of \cref{thm:bigtheorem} is the following:
% \begin{corollary}
% If $f:X\to Y$ is bijective, 
% the cardinality of $X$ is at least as large as that of $Y$.
% \end{corollary}
% \begin{assumption}
% The set $X$ is finite.
% \label{ass:xfinite}
% \end{assumption}
% \begin{remark}
% According to some, it is only the finite case (cf. \cref{ass:xfinite}) that is interesting.
% \end{remark}
% %restatable

% \subsection{Citations and References}

% Please use APA reference format regardless of your formatter
% or word processor. If you rely on the \LaTeX\/ bibliographic
% facility, use \texttt{natbib.sty} and \texttt{icml2025.bst}
% included in the style-file package to obtain this format.

% Citations within the text should include the authors' last names and
% year. If the authors' names are included in the sentence, place only
% the year in parentheses, for example when referencing Arthur Samuel's
% pioneering work \yrcite{Samuel59}. Otherwise place the entire
% reference in parentheses with the authors and year separated by a
% comma \cite{Samuel59}. List multiple references separated by
% semicolons \cite{kearns89,Samuel59,mitchell80}. Use the `et~al.'
% construct only for citations with three or more authors or after
% listing all authors to a publication in an earlier reference \cite{MachineLearningI}.

% Authors should cite their own work in the third person
% in the initial version of their paper submitted for blind review.
% Please refer to \cref{author info} for detailed instructions on how to
% cite your own papers.

% Use an unnumbered first-level section heading for the references, and use a
% hanging indent style, with the first line of the reference flush against the
% left margin and subsequent lines indented by 10 points. The references at the
% end of this document give examples for journal articles \cite{Samuel59},
% conference publications \cite{langley00}, book chapters \cite{Newell81}, books
% \cite{DudaHart2nd}, edited volumes \cite{MachineLearningI}, technical reports
% \cite{mitchell80}, and dissertations \cite{kearns89}.

% Alphabetize references by the surnames of the first authors, with
% single author entries preceding multiple author entries. Order
% references for the same authors by year of publication, with the
% earliest first. Make sure that each reference includes all relevant
% information (e.g., page numbers).

% Please put some effort into making references complete, presentable, and
% consistent, e.g. use the actual current name of authors.
% If using bibtex, please protect capital letters of names and
% abbreviations in titles, for example, use \{B\}ayesian or \{L\}ipschitz
% in your .bib file.

% \section*{Accessibility}
% Authors are kindly asked to make their submissions as accessible as possible for everyone including people with disabilities and sensory or neurological differences.
% Tips of how to achieve this and what to pay attention to will be provided on the conference website \url{http://icml.cc/}.

% \section*{Software and Data}

% If a paper is accepted, we strongly encourage the publication of software and data with the
% camera-ready version of the paper whenever appropriate. This can be
% done by including a URL in the camera-ready copy. However, \textbf{do not}
% include URLs that reveal your institution or identity in your
% submission for review. Instead, provide an anonymous URL or upload
% the material as ``Supplementary Material'' into the OpenReview reviewing
% system. Note that reviewers are not required to look at this material
% when writing their review.

% Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include acknowledgements.  Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.

\section*{Impact Statement}

% Authors are \textbf{required} to include a statement of the potential 
% broader impact of their work, including its ethical aspects and future 
% societal consequences. This statement should be in an unnumbered 
% section at the end of the paper (co-located with Acknowledgements -- 
% the two may appear in either order, but both must be before References), 
% and does not count toward the paper page limit. In many cases, where 
% the ethical impacts and expected societal implications are those that 
% are well established when advancing the field of Machine Learning, 
% substantial discussion is not required, and a simple statement such 
% as the following will suffice:

This paper presents work whose goal is to advance the field of 
Machine Learning. In particular, the goal is to advance the multi-agent and multi-robot communities by providing an evaluation and improvement of a desirable explanation paradigm for multi-agent coordination.
There are many potential societal consequences 
of our work, none which we feel must be specifically highlighted here.

% The above statement can be used verbatim in such cases, but we 
% encourage authors to think about whether there is content which does 
% warrant further discussion, as this statement will be apparent if the 
% paper is later flagged for ethics review.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}
\balance
\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Appendix}
\subsection{Training Details}
\sk{
In alignment with many MARL-based works \cite{nayak2023scalable, blumenkamp2021emergence, passageProrok, yang2023learning, he2023multi, escudie2024attention}, we adopt a centralized training decentralized execution (CTDE) paradigm to train policies. Specifically, we use multi-agent proximal policy optimization (MAPPO) \cite{yu2022surprising} with a centralized critic and homogeneous agents, which has been used in \cite{nayak2023scalable, yang2023learning, he2023multi, escudie2024attention}. The critic is parameterized as a 2-layer fully-connected multilayer perceptron (MLP) with 32 units in each layer for the blind navigation task and the passage task, which we borrow from \cite{passageProrok}, and 128 units in each layer for the discovery task, which we found worked well across all team sizes without attention entropy regularization (see \sectionautorefname~\ref{subsec:entropy_reg}). Following \cite{yu2022surprising}, we incorporate generalized advantage estimation \cite{Schulmanetal_ICLR2016} and parameter sharing for the agents (though the policies are decentralized). We optimize the MAPPO loss term using ADAM optimizer \cite{kingma2014adam}, and implement the MARL training and evaluation using PyTorch \cite{paszke2019pytorch} and PyTorch Geometric \cite{Fey/Lenssen/2019}.

\textbf{To ensure an unbiased evaluation of entropy regularization and different explanation methods, 
% It is important to note that 
all our design choices for training (actor, critic, hyperparameters, etc.) are motivated by prior works or empirical performance of the policy without attention entropy minimization (from \sectionautorefname~\ref{subsec:entropy_reg}). In addition, we do not inform these design choices based on explanation quality either.
}
}
% \section{Environment Training Details}

% \subsection{Metrics}
\subsection{Metrics for Task Performance}
% We consider five task performance metrics: reward, success rate, no agent collision rate, no object collision rate, and makespan. We describe each of these metrics below for a single episode or rollout. Each metric is averaged across multiple episodes or rollouts, each with an random initial state. 
We consider the following task performance metrics.

% \subsubsection{Reward}
\textbf{Reward:} This is the average reward accrued across all agents, computed as $\frac{1}{N}\sum^N_{n=1}\sum^T_{t=0} r_t(n)$ where $n \in N$ is agent index and $t \in T$ is the time index within an episode. 
% For the blind navigation and discovery task, the reward is as defined by \cite{bettini2022vmas}. For the passage task, the collision reward is defined by \cite{bettini2022vmas}, while the waypoint reward is based on \cite{passageProrok}.
% We calculate this as shown in \eqref{rewardMetric}.

% \begin{equation}
%     \Bar{R} = \frac{1}{K}\frac{1}{N}\sum_k\sum_n\sum_t r_t
%     \label{rewardMetric}
% \end{equation}

% \subsubsection{Success Rate}
% This is the percentage of episodes that were successfully completed. 
\textbf{Success Rate:} For the blind navigation and passage task, the success rate of an episode is defined by the fraction of the total number of agents who reach their assigned goals. 
% $\frac{\text{number of agents that reach their goal}}{\text{total number of agents in the team}}$. 
An episode of the discovery task is considered successful if the landmark is discovered before the episode terminates.
% $\mathds{1}$[target located by a pair of agents simultaneously] where $\mathds{1}$ is an indicator function that returns 1 if the condition is satisfied and 0 otherwise.

% \subsubsection{No Agent Collision Rate}
\textbf{No Agent Collision Fraction:} Across all tasks, this metric is defined as the fraction of episodes that result in no agent-agent collisions, averaged across random initial states. 
% In an episode, the no agent collision rate is defined as $\mathds{1}$[if no agents collided with each other]. 
% This metric holds for all tasks, as all tasks seek to avoid agent-agent collisions (thus necessitating communication and coordination).

% \subsubsection{No Object Collision Rate}
\textbf{No Object Collision Fraction:} This is the fraction of episodes in which no agent collides with an obstacle, averaged across multiple random initial states. 
% In an episode, the no object collision rate is defined as $\mathds{1}$[if no agents collided with a static obstacle] where $\mathds{1}$ is an indicator function that returns 1 if the condition is satisfied and 0 otherwise. 
% This is averaged across multiple random initial states. 
This metric is only used in the passage task, since 
% there is a wall in the environment that the agents are seeking to avoid collision with. For the blind navigation and search and rescue task, 
there are no obstacles in blind navigation and discovery tasks.

% \subsubsection{Makespan}
\textbf{Makespan:} This is the number of environment steps it takes for the team to complete the task, with a maximum value defined by episode length.
% Note that there is an imposed maximum number of steps in the environment, after which the rollout or episode terminates. 
% Thus, the makespan for an episode or rollout can be defined as $\min$(number of steps to complete task, imposed maximum number of steps). 
We consider the blind navigation and passage tasks complete when the last agent reaches its goal location. 
% the number of steps to complete the task is the number of steps it takes for the last agent to reach its goal location. 
We consider the discovery task complete when the landmark is discovered.
% the number of steps to complete the task is the time it takes for two agents to find the target simultaneously.

\subsection{Navigation Environment Training Details}
We train all policies for the navigation task with the hyperparameters in Table \ref{table:navigation-hyperparameters}. We set the weight of the attention entropy term to 10.
% All trained policies follow the architecture described in Table [].
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Hyperparameter} & \textbf{Value} \\ \hline
Learning rate & 0.0003 \\ \hline
Gamma & 0.99999 \\ \hline
Lambda & 0.9 \\ \hline
Entropy epsilon & 0.0001 \\ \hline
Clip epsilon & 0.2 \\ \hline
Critic loss type & Smooth L1 \\ \hline
Normalize advantage & False \\ \hline
Number of epochs & 30 \\ \hline
Max gradient norm & 1.0 \\ \hline
Minibatch size & 800 \\ \hline
Collector iterations & 150 \\ \hline
Frames per batch & 18000 \\ \hline
\end{tabular}
\caption{Navigation Hyperparameters}
\label{table:navigation-hyperparameters}
\end{table}

\subsection{Passage Environment Training Details}
We train all policies for the passage task with the hyperparameters in Table \ref{table:passage-hyperparameters}. We set the weight of the attention entropy term to 50.
% All trained policies follow the architecture described in Table [].
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Hyperparameter} & \textbf{Value} \\ \hline
Learning rate & 0.00005 \\ \hline
Gamma & 0.99999 \\ \hline
Lambda & 0.9 \\ \hline
Entropy epsilon & 0.0001 \\ \hline
Clip epsilon & 0.2 \\ \hline
Critic loss type & Smooth L1 \\ \hline
Normalize advantage & False \\ \hline
Number of epochs & 30 \\ \hline
Max gradient norm & 1.0 \\ \hline
Minibatch size & 800 \\ \hline
Collector iterations & 100 \\ \hline
Frames per batch & 60000 \\ \hline
\end{tabular}
\caption{Passage Hyperparameters}
\label{table:passage-hyperparameters}
\end{table}

% \begin{table}[h!]
% \centering
% \begin{tabular}{|c|c|}
% \hline
% Encoder depth & 4 \\ \hline
% Encoder number of cells & 32 \\ \hline
% GNN heads & 1 \\ \hline
% GNN layers & 1 \\ \hline
% GNN convolution layer & GATv2Conv \\ \hline
% GNN hidden dimension & 32 \\ \hline
% Decoder depth & 4 \\ \hline
% Decoder number of cells & 64 \\ \hline
% Critic depth & 2 \\ \hline
% Critic number of cells & 64 \\ \hline
% \end{tabular}
% \caption{Passage Policy Architecture}
% \label{table:passage-policy-arch}
% \end{table}

\subsection{Discovery Environment Training Details}
We train all policies for the discovery task with the hyperparameters in Table \ref{table:discovery-hyperparameters}. We set the weight of the attention entropy term to 50.
% All trained policies follow the architecture described in Table \ref{table:discovery-policy-arch}.
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Hyperparameter} & \textbf{Value} \\ \hline
Learning rate & 0.0007 \\ \hline
Gamma & 0.9999 \\ \hline
Lambda & 0.95 \\ \hline
Entropy epsilon & 0.0001 \\ \hline
Clip epsilon & 0.05 \\ \hline
Critic loss type & Smooth L1 \\ \hline
Normalize advantage & True \\ \hline
Number of epochs & 5 \\ \hline
Max gradient norm & 10 \\ \hline
Minibatch size & 10000 \\ \hline
Collector iterations & 1000 \\ \hline
Frames per batch & 10000 \\ \hline
\end{tabular}
\caption{Discovery Hyperparameters}
\label{table:discovery-hyperparameters}
\end{table}

% \begin{table}[h!]
% \centering
% \begin{tabular}{|c|c|}
% \hline
% Encoder depth & 1 \\ \hline
% Encoder number of cells & 64 \\ \hline
% GNN heads & 3 \\ \hline
% GNN layers & 1 \\ \hline
% GNN convolution layer & GATv2Conv \\ \hline
% GNN hidden dimension & 64 \\ \hline
% Decoder depth & 2 \\ \hline
% Decoder number of cells & 64 \\ \hline
% Critic depth & 2 \\ \hline
% Critic number of cells & 128 \\ \hline
% \end{tabular}
% \caption{Discovery Policy Architecture}
% \label{table:discovery-policy-arch}
% \end{table}

% \section{Locality Analysis of Minima for $D(\alpha)$}
% We can provide a rudimentary insight into the neighborhood of the minima of $D(\alpha) = \sum_{i, j} \left| 2\alpha_{i,j} - 1 \right|$. To do this, consider the quantity shown below.
% \begin{equation}
%     \alpha' = [\frac{1}{N} + \epsilon, \frac{1}{N} - \epsilon, \frac{1}{N}, \ldots, \frac{1}{N}]
% \end{equation}
% We can compute $D(\alpha')$ as shown below.
% \begin{align}
%     D(\alpha') &= \left| 2 \left(\frac{1}{N} + \epsilon \right) - 1 \right| + \left| 2 \left(\frac{1}{N} - \epsilon \right) - 1 \right| + (N^2 - 2)\left|\frac{2}{N} - 1 \right| \\
%     &= \left| \frac{2}{N} + 2\epsilon - 1 \right| + \left| \frac{2}{N} - 2\epsilon - 1 \right| + (N^2 - 2)\left|\frac{2}{N} - 1 \right|
% \end{align}
% We can solve for what values of $\epsilon$ does the quantity $D(\alpha') = \left|2N - N^2\right|$ as shown below.
% \begin{align}
%     \left| \frac{2}{N} + 2\epsilon - 1 \right| + \left| \frac{2}{N} - 2\epsilon - 1 \right| + (N^2 - 2)\left|\frac{2}{N} - 1 \right| &= \left|2N - N^2\right|
% \end{align}
% We find that the above equality holds for $\frac{2-N}{2N} \leq \epsilon \leq \frac{N-2}{2N}$ for $N \geq 2$
% This analysis does not hold for large $N$, but it provides insight into the fact that area near the local minima is flat (i.e. it yields a 0 gradient). As a result, optimizing $D(\alpha)$ directly will not yield improvement unless the task reward gradients eventually yield attention values that are outside the $\epsilon$-neighborhood of the local minima. As a result, it would likely be better to optimize for a surrogate function that has non-zero gradients defined throughout (such as entropy for example).

% \section{Brief Statement Regarding Node Embedding Separation}
% We note that a convex function over a convex set 

\subsection{Positive Fidelity and Negative Fidelity}
\figureautorefname~\ref{fig:positiveFidelity} shows the positive fidelity measures and \figureautorefname~\ref{fig:negativeFidelity} shows the negative fidelity measures. When the model is trained with attention entropy minimization, GraphMask tends to improve in negative fidelity, but regresses in positive fidelity, yielding little net benefit in delta fidelity. GNNExplainer has mixed improvements for both positive and negative fidelity, which yields mixed results in delta fidelity. AttentionExplainer consistently improves in positive and negative fidelity, which accounts for the large improvement in delta fidelity that is observed.

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-navigation-3_agent-PositiveFidelity.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-navigation-4_agent-PositiveFidelity.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-navigation-5_agent-PositiveFidelity.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-passage-3_agent-PositiveFidelity.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-passage-4_agent-PositiveFidelity.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-passage-5_agent-PositiveFidelity.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-discovery-3_agent-PositiveFidelity.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-discovery-4_agent-PositiveFidelity.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-discovery-5_agent-PositiveFidelity.png}
    \caption{Positive Fidelity ($\uparrow$) of explanations generated by three explanation methods with and without proposed regularization, across three team sizes. \textit{Top row}: Blind navigation task, \textit{Middle row}: Passage task, \textit{Bottom row}: Discovery task. Higher is better.}
    \label{fig:positiveFidelity}
\end{figure}

% \subsection{Negative Fidelity}
% \figureautorefname~\ref{fig:negativeFidelity} shows the negative fidelity measures.

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-navigation-3_agent-NegativeFidelity.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-navigation-4_agent-NegativeFidelity.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-navigation-5_agent-NegativeFidelity.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-passage-3_agent-NegativeFidelity.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-passage-4_agent-NegativeFidelity.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-passage-5_agent-NegativeFidelity.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-discovery-3_agent-NegativeFidelity.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-discovery-4_agent-NegativeFidelity.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-discovery-5_agent-NegativeFidelity.png}
    \caption{Negative Fidelity ($\downarrow$) of explanations generated by three explanation methods with and without proposed regularization, across three team sizes. \textit{Top row}: Blind navigation task, \textit{Middle row}: Passage task, \textit{Bottom row}: Discovery task. Lower is better.}
    \label{fig:negativeFidelity}
\end{figure}

\subsection{Explanation Quality Over Training Iterations}
\label{subsec:explanation-quality-training}
We analyze the explanation quality across training iterations to evaluate what post-hoc explainers are best suited for mediocre performing policies or for debugging models that are still training. \figureautorefname~\ref{fig:explanationQualityOverTraining} shows the delta fidelity and unfaithfulness measures for 10\%, 50\%, and 90\% training for a 3 agent GNN-based policy for the blind navigation task. This provides some insight into the evolution of the explanation quality as task reward optimization and attention entropy minimization occurs.

\subsubsection{Delta Fidelity}
As seen before, GraphMask does not gain any improvement in Delta Fidelity due to the sole reliance on hard binary masks and the approximately equal trade-off between Positive and Negative Fidelity that occurs when GraphMask is able to capture a larger subset of important agent-agent influences but still misses the other remaining important agent-agent influences. GNNExplainer and AttentionExplainer both improve over the course of training. Moreover, both GNNExplainer and AttentionExplainer benefit from attention entropy minimization, with AttentionExplainer inhereting a larger improvement due to attention entropy minimization. Finally, while GraphMask provides the least fidelity explanation regardless of the presence or absence of regularization, GNNExplainer performs the best across models without attention entropy minimization while AttentionExplainer performs the best across models with attention entropy minimization. AttentionExplainer, evaluated on a model trained with attention entropy minimization, yields the highest fidelity explanations overall.

\subsubsection{Unfaithfulness}
As seen before, GraphMask improves in faithfulness when the model is trained with attention entropy minimization, which is likely due to attention entropy minimization making the distribution sparser, resulting in agent-agent influences that are closer to a subgraph composed of only a binary adjacency matrix. Still, GraphMask does not achieve as faithful of explanations as GNNExplainer and AttentionExplainer. This is once again attributed to the limited representation capacity of GraphMask subgraphs since they are constrained to be hard binary masks, and attention entropy minimization can only ameliorate this, to an extent. In addition, we observe that the faithfulness of AttentionExplainer improves due to attention entropy minimization, while GNNExplainer has negligible to slight improvements in faithfulness due to attention entropy minimization. Finally, while GraphMask provides the least faithful explanations overall regardless of the presence or absence of regularization, GNNExplainer performs the best across models without attention entropy minimization while AttentionExplainer performs the best across models with attention entropy minimization. AttentionExplainer, evaluated on a model trained with attention entropy minimization, yields the most faithful (least unfaithful) explanations overall.

\subsubsection{Task Performance}
\tableautorefname~\ref{table:TrainingIterationPerformance} presents the task performance metrics for each of the aforementioned model checkpoints. Overall, the performance of the models trained with attention entropy minimization ranges from being comparable to outperforming the models trained without attention entropy minimization.

\begin{figure}[!h]
    \centering
    % \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/symm_kl_30/boxplot_analysis-navigation-3_agent_half_trained-symm_kl_30-PositiveFidelity.png}
    % \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-navigation-3_agent_half_trained-symm_kl_75-PositiveFidelity.png}
    % \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/symm_kl_120/boxplot_analysis-navigation-3_agent_half_trained-symm_kl_120-PositiveFidelity.png}
    % \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/symm_kl_30/boxplot_analysis-navigation-3_agent_half_trained-symm_kl_30-NegativeFidelity.png}
    % \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-navigation-3_agent_half_trained-symm_kl_75-NegativeFidelity.png}
    % \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/symm_kl_120/boxplot_analysis-navigation-3_agent_half_trained-symm_kl_120-NegativeFidelity.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/symm_kl_30/boxplot_analysis-navigation-3_agent_half_trained-symm_kl_30-DeltaFidelity.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-navigation-3_agent_half_trained-symm_kl_75-DeltaFidelity.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/symm_kl_120/boxplot_analysis-navigation-3_agent_half_trained-symm_kl_120-DeltaFidelity.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/symm_kl_30/boxplot_analysis-navigation-3_agent_half_trained-symm_kl_30-Unfaithfulness.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/boxplot_analysis-navigation-3_agent_half_trained-symm_kl_75-Unfaithfulness.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/symm_kl_120/boxplot_analysis-navigation-3_agent_half_trained-symm_kl_120-Unfaithfulness.png}
    \caption{Delta Fidelity (top, $\uparrow$) and Unfaithfulness (bottom, $\downarrow$) of explanations generated by three explanation methods with and without proposed regularization at 20\% (left), 50\% (middle), and 80\% (right) training completion.}
    \label{fig:explanationQualityOverTraining}
\end{figure}

%% TABLE TO PUT VALUES, NOT DONE YET
\begin{table*}[bt]
\scriptsize
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\multicolumn{7}{|c|}{\textbf{Blind Navigation}} \\
\hline
\multirow{2}{*}{Performance Metric} & \multicolumn{2}{c|}{20\% Training Completed} & \multicolumn{2}{c|}{50\% Training Completed} & \multicolumn{2}{c|}{80\% Training Completed} \\
    \cline{2-7}
     &  No Att. Ent. Min. & Att. Ent. Min. & No Att. Ent. Min. & Att. Ent. Min. & No Att. Ent. Min. & Att. Ent. Min. \\
\hline
Reward ($\uparrow$) & 2.34 $\pm$ 0.69 & 2.11 $\pm$ 0.77 & 2.37 $\pm$ 0.65 & 2.37 $\pm$ 0.60 & 2.20 $\pm$ 0.81 & 2.39 $\pm$ 0.53 \\
\hline
Success Rate ($\uparrow$) & 0.22 $\pm$ 0.41 & 0.75 $\pm$ 0.43 & 0.97 $\pm$ 0.15 & 0.95 $\pm$ 0.21 & 0.97 $\pm$ 0.15 & 0.97 $\pm$ 0.15 \\
\hline
No Agent Coll. Rate ($\uparrow$) & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 0.97 $\pm$ 0.15 & 1.0 $\pm$ 0.0 \\
\hline
% No Object Coll. Rate ($\uparrow$) & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 \\
% \hline
Makespan ($\downarrow$) & 128.3 $\pm$ 40.87 & 72.37 $\pm$ 47.74 & 50.8 $\pm$ 28.40 & 58.45 $\pm$ 27.94 & 47.5 $\pm$ 25.28 & 55.6 $\pm$ 28.05 \\
\hline
\end{tabular}
\caption{Performance of each model checkpoint.}
\label{table:TrainingIterationPerformance}
\end{table*}

\subsection{Zero-Shot Deployment for Larger Team Sizes}
\label{subsec:explanation-quality-zero-shot}
One major benefit of GNN-based policies is the ability to directly deploy them to different team sizes, which has been done in works such as \cite{yu2022learning, hu2023graph, yang2023masp}. We analyze the explanation quality when deploying the trained policy on larger team sizes for the blind navigation task. This analysis can also be used to observe the explanation quality on out-of-distribution team sizes.

\subsubsection{Delta Fidelity}
As seen before, GraphMask does not gain any improvement in Delta Fidelity due to the sole reliance on hard binary masks and the approximately equal trade-off between Positive and Negative Fidelity that occurs when GraphMask is able to capture a larger subset of important agent-agent influences but still misses the other remaining important agent-agent influences. Also, both GNNExplainer and AttentionExplainer benefit from attention entropy minimization, with AttentionExplainer inhereting a larger improvement due to attention entropy minimization. However, note that, in general, as the test team size deviates further from the train team size (i.e. the deployed team size is more out-of-distribution), GNNExplainer performs more similarly to AttentionExplainer both in terms of fidelity and faithfulness. Thus, while GNNExplainer performs the best across models without attention entropy minimization, AttentionExplainer and GNNExplainer are interchangeably the best performers across the out-of-distribution evaluations. This can likely be due to the fact that while GNNExplainer has a difficult optimization to undertake and makes assumptions that work better on the graph datasets more common in the graph learning community, it is still not biased by the model itself, and treats the model as a black-box. On the other hand, AttentionExplainer is based solely on the attention values. Since the attention values are not trained to minimize the attention entropy on these out-of-distribution teams, even though there is evidently some entropy minimization effect occurring that transfers over to the out-of-distribution teams, it is likely less pronounced. Since the effect of attention entropy minimization likely deteriorates with more out-of-distribution teams, AttentionExplainer becomes less superior to GNNExplainer in terms of fidelity. 

\subsubsection{Unfaithfulness}
As seen before, GraphMask improves in faithfulness when the model is trained with attention entropy minimization, which is likely due to attention entropy minimization making the distribution sparser, resulting in agent-agent influences that are closer to a subgraph composed of only a binary adjacency matrix. Still, GraphMask does not, on average, achieve as faithful of explanations as GNNExplainer and AttentionExplainer. This is once again attributed to the limited representation capacity of GraphMask subgraphs since they are constrained to be hard binary masks, and attention entropy minimization can only ameliorate this, to an extent. In addition, we observe that the faithfulness of AttentionExplainer improves due to attention entropy minimization, while GNNExplainer has mixed improvement results in faithfulness due to attention entropy minimization. GNNExplainer performs the best across models without attention entropy minimization. With models trained with attention entropy, Attention Explainer performs better when the team is less out-of-distribution (i.e., the number of agents of the test team is closer to the number of agents of the train team). As the team size grows more out-of-distribution, the performance of AttentionExplainer and GNNExplainer becomes more interchangeable. This can likely be attributed to the fact that the model has not directly optimized for attention entropy minimization on these out-of-distribution team sizes, so the effect of attention entropy minimization deteriorates the more out-of-distribution the team size is. On the other hand, GNNExplainer treats the model as a black-box model and is not directly influenced by the attention values, but still has to solve a fairly difficult optimization problem and relies on assumptions that are more useful in the graph datasets that are more common in the graph learning community.

\subsubsection{Task Performance}
\tableautorefname~\ref{table:ZeroShotPerformance} presents the task performance metrics for each zero-shot train-test combination. Overall, the performance of the models trained with attention entropy minimization ranges from being comparable to outperforming the models trained without attention entropy minimization.

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/4_agent_zero_shot_from_3_policy/boxplot_analysis-navigation-4_agent_zero_shot_from_3_policy-DeltaFidelity.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/5_agent_zero_shot_from_3_policy/boxplot_analysis-navigation-5_agent_zero_shot_from_3_policy-DeltaFidelity.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/6_agent_zero_shot_from_3_policy/boxplot_analysis-navigation-6_agent_zero_shot_from_3_policy-DeltaFidelity.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/5_agent_zero_shot_from_4_policy/boxplot_analysis-navigation-5_agent_zero_shot_from_4_policy-DeltaFidelity.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/6_agent_zero_shot_from_4_policy/boxplot_analysis-navigation-6_agent_zero_shot_from_4_policy-DeltaFidelity.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/7_agent_zero_shot_from_4_policy/boxplot_analysis-navigation-7_agent_zero_shot_from_4_policy-DeltaFidelity.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/6_agent_zero_shot_from_5_policy/boxplot_analysis-navigation-6_agent_zero_shot_from_5_policy-DeltaFidelity.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/7_agent_zero_shot_from_5_policy/boxplot_analysis-navigation-7_agent_zero_shot_from_5_policy-DeltaFidelity.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/8_agent_zero_shot_larger_env_from_5_policy/boxplot_analysis-navigation-8_agent_zero_shot_larger_env_from_5_policy-DeltaFidelity.png}
    \caption{Delta Fidelity ($\uparrow$) of explanations generated by three explanation methods with and without proposed regularization. Each row represents the team size the policy was originally trained on (Top = 3, Middle = 4, Bottom = 5). Each column represents the larger team size the policy was deployed on (Left = +1 team size, Middle = +2 team size, Right = +3 team size)}
    \label{fig:explanationQualityOOD}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/4_agent_zero_shot_from_3_policy/boxplot_analysis-navigation-4_agent_zero_shot_from_3_policy-Unfaithfulness.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/5_agent_zero_shot_from_3_policy/boxplot_analysis-navigation-5_agent_zero_shot_from_3_policy-Unfaithfulness.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/6_agent_zero_shot_from_3_policy/boxplot_analysis-navigation-6_agent_zero_shot_from_3_policy-Unfaithfulness.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/5_agent_zero_shot_from_4_policy/boxplot_analysis-navigation-5_agent_zero_shot_from_4_policy-Unfaithfulness.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/6_agent_zero_shot_from_4_policy/boxplot_analysis-navigation-6_agent_zero_shot_from_4_policy-Unfaithfulness.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/7_agent_zero_shot_from_4_policy/boxplot_analysis-navigation-7_agent_zero_shot_from_4_policy-Unfaithfulness.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/6_agent_zero_shot_from_5_policy/boxplot_analysis-navigation-6_agent_zero_shot_from_5_policy-Unfaithfulness.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/7_agent_zero_shot_from_5_policy/boxplot_analysis-navigation-7_agent_zero_shot_from_5_policy-Unfaithfulness.png}
    \includegraphics[width=0.33\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig/8_agent_zero_shot_larger_env_from_5_policy/boxplot_analysis-navigation-8_agent_zero_shot_larger_env_from_5_policy-Unfaithfulness.png}
    \caption{Unfaithfulness ($\downarrow$) of explanations generated by three explanation methods with and without proposed regularization. Each row represents the team size the policy was originally trained on (Top = 3, Middle = 4, Bottom = 5). Each column represents the larger team size the policy was deployed on (Left = +1 team size, Middle = +2 team size, Right = +3 team size)}
    \label{fig:explanationQualityOOD}
\end{figure}

%% TABLE TO PUT VALUES, NOT DONE YET - FIRST TABLE DONE!!!
\begin{table*}[bt]
\scriptsize
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\multicolumn{7}{|c|}{\textbf{Blind Navigation Trained on N=3}} \\
\hline
\multirow{2}{*}{Performance Metric} & \multicolumn{2}{c|}{Tested on N=4} & \multicolumn{2}{c|}{Tested on N=5} & \multicolumn{2}{c|}{Tested on N=6} \\
    \cline{2-7}
     &  No Att. Ent. Min. & Att. Ent. Min. & No Att. Ent. Min. & Att. Ent. Min. & No Att. Ent. Min. & Att. Ent. Min. \\
\hline
Reward ($\uparrow$) & 2.68 $\pm$ 1.28 & 2.79 $\pm$ 0.69 & 4.74 $\pm$ 1.21 & 5.02 $\pm$ 0.99 & 3.98 $\pm$ 1.26 & 4.08 $\pm$ 1.68 \\
\hline
Success Rate ($\uparrow$) & 1.0 $\pm$ 0.0 & 0.93 $\pm$ 0.26 & 0.82 $\pm$ 0.37 & 0.95 $\pm$ 0.21 & 0.9 $\pm$ 0.29 & 0.88 $\pm$ 0.33 \\
\hline
No Agent Coll. Rate ($\uparrow$) & 0.93 $\pm$ 0.26 & 0.93 $\pm$ 0.26 & 0.92 $\pm$ 0.26 & 0.97 $\pm$ 0.15 & 0.67 $\pm$ 0.47 & 0.75 $\pm$ 0.43 \\
\hline
% No Object Coll. Rate ($\uparrow$) & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 \\
% \hline
Makespan ($\downarrow$) & 61.57 $\pm$ 25.16 & 60.82 $\pm$ 33.83 & 154.57 $\pm$ 121.44 & 120.5 $\pm$ 75.15 & 116.6 $\pm$ 103.4 & 129.4 $\pm$ 110.33 \\
\hline
\hline
\multicolumn{7}{|c|}{\textbf{Blind Navigation Trained on N=4}} \\
\hline
\multirow{2}{*}{Performance Metric} & \multicolumn{2}{c|}{Tested on N=5} & \multicolumn{2}{c|}{Tested on N=6} & \multicolumn{2}{c|}{Tested on N=7} \\
    \cline{2-7}
     &  No Att. Ent. Min. & Att. Ent. Min. & No Att. Ent. Min. & Att. Ent. Min. & No Att. Ent. Min. & Att. Ent. Min. \\
\hline
Reward ($\uparrow$) & 5.08 $\pm$ 0.93 & 5.03 $\pm$ 0.70 & 6.55 $\pm$ 1.52 & 6.33 $\pm$ 1.62 & 5.14 $\pm$ 0.95 & 5.12 $\pm$ 0.99 \\
\hline
Success Rate ($\uparrow$) & 0.92 $\pm$ 0.26 & 1.0 $\pm$ 0.0 & 0.87 $\pm$ 0.33 & 0.95 $\pm$ 0.21 & 0.80 $\pm$ 0.40 & 0.80 $\pm$ 0.40 \\
\hline
No Agent Coll. Rate ($\uparrow$) & 0.92 $\pm$ 0.26 & 0.98 $\pm$ 0.15 & 0.8 $\pm$ 0.40 & 0.88 $\pm$ 0.33 & 0.85 $\pm$ 0.35 & 0.75 $\pm$ 0.43 \\
\hline
% No Object Coll. Rate ($\uparrow$) & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 \\
% \hline
Makespan ($\downarrow$) & 125.17 $\pm$ 88.26 & 103.87 $\pm$ 58.46 & 149.82 $\pm$ 102.37 & 135.32 $\pm$ 82.67 & 158.62 $\pm$ 125.71 & 174.47 $\pm$ 130.42 \\
\hline
\hline
\multicolumn{7}{|c|}{\textbf{Blind Navigation Trained on N=5}} \\
\hline
\multirow{2}{*}{Performance Metric} & \multicolumn{2}{c|}{Tested on N=6} & \multicolumn{2}{c|}{Tested on N=7} & \multicolumn{2}{c|}{Tested on N=8} \\
    \cline{2-7}
     &  No Att. Ent. Min. & Att. Ent. Min. & No Att. Ent. Min. & Att. Ent. Min. & No Att. Ent. Min. & Att. Ent. Min. \\
\hline
Reward ($\uparrow$) & 6.03 $\pm$ 2.00 & 6.41 $\pm$ 1.61 & 7.33 $\pm$ 3.18 & 8.70 $\pm$ 2.30 & 5.04 $\pm$ 4.22 & 5.82 $\pm$ 3.48 \\
\hline
Success Rate ($\uparrow$) & 0.52 $\pm$ 0.49 & 0.72 $\pm$ 0.44 & 0.42 $\pm$ 0.49 & 0.52 $\pm$ 0.49 & 0.2 $\pm$ 0.40 & 0.45 $\pm$ 0.49 \\
\hline
No Agent Coll. Rate ($\uparrow$) & 0.87 $\pm$ 0.33 & 0.95 $\pm$ 0.21 & 0.6 $\pm$ 0.48 & 0.85 $\pm$ 0.35 & 0.52 $\pm$ 0.49 & 0.72 $\pm$ 0.44 \\
\hline
% No Object Coll. Rate ($\uparrow$) & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 \\
% \hline
Makespan ($\downarrow$) & 282.7 $\pm$ 123.71 & 248.17 $\pm$ 114.98 & 333.72 $\pm$ 90.45 & 296.85 $\pm$ 108.79 & 362.7 $\pm$ 80.47 & 341.57 $\pm$ 80.20 \\
\hline
\end{tabular}
\caption{Performance of zero-shot deployment of policies trained on smaller team size on larger size test teams.}
\label{table:ZeroShotPerformance}
\end{table*}

\subsection{Limitations}
In our empirical analysis, we attempted to cover a wide variety of potential factors by considering multiple tasks in multi-robot coordination and architectural hyperparameters associated with multi-robot coordination. However, there are certain aspects not included in this work that can be further studied in future work. The first is other learning paradigms, such as imitation learning. The current study only focuses on reinforcement learning as finding an expert or demonstration dataset for multi-agent systems is challenging, especially for the chosen tasks. The second is alternative neural architecture paradigms. We focus on the most representative structure of encoder-GNN-decoder, but we focused mostly on MLPs as opposed to other types of neural architectures. Finally, there are other potential ways to generate sparse attention or edge weights besides attention entropy minimization. Especially for GraphMask, one could try sampling the attention weights as a Bernoulli trial to generate hard edge masks, which could be more in-distribution for GraphMask. These are all left for future work and study.

% \section{CL Analysis}
% \tableautorefname~\ref{table:CLPerformance} shows the performance comparison and \figureautorefname~\ref{fig:CL} shows explanation quality when training larger agent teams with curriculum learning.
% \begin{table*}[!ht]
% \centering
% \begin{tabular}{|c|c|c|c|c|c|c|}
% \hline
% \multicolumn{5}{|c|}{\textbf{Blind Navigation}} \\
% \hline
% \multirow{2}{*}{Performance Metric} & \multicolumn{2}{c|}{N = 5} & \multicolumn{2}{c|}{N = 6}\\ % & \multicolumn{2}{c|}{N = 7} \\
%     \cline{2-5}
%      &  No Att. Ent. Min. & Att. Ent. Min. & No Att. Ent. Min. & Att. Ent. Min. \\ %& No Att. Ent. Min. & Att. Ent. Min. \\
% \hline
% Reward ($\uparrow$) & 3.68 $\pm$ 0.68 & 3.89 $\pm$ 0.645 & 4.02 $\pm$ 1.689 & 4.453 $\pm$ 0.74 \\ %& 4.93 $\pm$ 1.37 & 4.77 $\pm$ 1.19 \\
% \hline
% Success Rate ($\uparrow$) & 0.975 $\pm$ 0.158 & 0.975 $\pm$ 0.158 &  0.9 $\pm$ 0.30 & 0.875 $\pm$ 0.33 \\ %& 0.67 $\pm$ 0.47 & 0.9 $\pm$0.30 \\
% \hline
% No Agent Coll. Rate ($\uparrow$) & 0.95 $\pm$ 0.22 & 1.0 $\pm$ 0.0 & 0.775 $\pm$ 0.42 & 0.875 $\pm$ 0.33 \\ %& 0.925 $\pm$ 0.27 & 0.9 $\pm$ 0.30 \\
% \hline
% % No Object Coll. Rate ($\uparrow$) & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 1.0 $\pm$ 0.0 \\
% % \hline
% Makespan ($\downarrow$) & 111.4 $\pm$ 113.27 & 108.775 $\pm$ 78.33 & 127.325 $\pm$ 105.24 & 144.95 $\pm$ 121.008 \\ % & 217.75 $\pm$ 137.53 & 138.9 $\pm$ 99.74 \\
% \hline
% \end{tabular}
% \caption{Performance comparison for navigation when policy trained using simple curriculum learning.}
% \label{table:CLPerformance}
% \end{table*}

% \begin{figure*}
%     \centering
%     \includegraphics[width=0.24\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig_old/boxplot_analysis-navigation-5_agent_CL-v1-PositiveFidelity.png}
%     \includegraphics[width=0.24\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig_old/boxplot_analysis-navigation-5_agent_CL-v1-NegativeFidelity.png}
%     \includegraphics[width=0.24\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig_old/boxplot_analysis-navigation-5_agent_CL-v1-DeltaFidelity.png}
%     \includegraphics[width=0.24\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig_old/boxplot_analysis-navigation-5_agent_CL-v1-Unfaithfulness.png}
%     \includegraphics[width=0.24\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig_old/boxplot_analysis-navigation-6_agent_CL-v2-PositiveFidelity.png}
%     \includegraphics[width=0.24\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig_old/boxplot_analysis-navigation-6_agent_CL-v2-NegativeFidelity.png}
%     \includegraphics[width=0.24\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig_old/boxplot_analysis-navigation-6_agent_CL-v2-DeltaFidelity.png}
%     \includegraphics[width=0.24\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig_old/boxplot_analysis-navigation-6_agent_CL-v2-Unfaithfulness.png}
%     % \includegraphics[width=0.24\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig_old/boxplot_analysis-navigation-7_agent_CL-PositiveFidelity.png}
%     % \includegraphics[width=0.24\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig_old/boxplot_analysis-navigation-7_agent_CL-NegativeFidelity.png}
%     % \includegraphics[width=0.24\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig_old/boxplot_analysis-navigation-7_agent_CL-DeltaFidelity.png}
%     % \includegraphics[width=0.24\linewidth]{AAMAS-2025-Formatting-Instructions-CCBY/fig_old/boxplot_analysis-navigation-7_agent_CL-Unfaithfulness.png}
%     \caption{Explanation Quality metrics of explanations generated by three explanation methods with and without proposed regularization, across two larger team sizes trained using curriculum learning with $N=4$ policy without attention entropy minimization as its initial weights}
%     \label{fig:CL}
% \end{figure*}

% Given that minimizing attention entropy leads to a larger separation between $G_S(\alpha)$ and $G \setminus G_S(\alpha)$, assuming a static $\phi$, we show that the bound on the distance between the embeddings obtained under $G_S(\alpha)$ and $G \setminus G_S(\alpha)$ is greater when minimizing attention entropy. Without loss of generality, let $\alpha_i$ be the vector of attention applied to each neighbor of node $i$ obtained without attention entropy minimization, and $\alpha_i^{a}$ be the vector of attention obtained with attention entropy minimization. It follows that,
% \begin{align}
%     \sum_{j \in \mathcal{N}(i)}|\alpha_{i,j} - (1-\alpha_{i,j})| \leq \sum_{j \in \mathcal{N}(i)}|\alpha_{i,j}^a - (1-\alpha_{i,j}^a)|
% \end{align}
% The distance between the node embeddings obtained under $G_S(\alpha_i)$ and $G \setminus G_S(\alpha_i)$ can be written as,
% \begin{align}
%    \norm{\sum_{j \in \mathcal{N}(i)} \alpha_{ij} \cdot \phi_{\theta} h_j - (1-\alpha_{ij}) \cdot \phi_{\theta} h_j}
% \end{align}
% The distance between the node embeddings obtained under $G_S(\alpha^a_i)$ and $G \setminus G_S(\alpha^a_i)$ can be written as,
% \begin{align}
%    \norm{\sum_{j \in \mathcal{N}(i)} \alpha^a_{ij} \cdot \phi_{\theta} h_j - (1-\alpha^a_{ij}) \cdot \phi_{\theta} h_j}
% \end{align}
% By the triangle inequality, the upper bound on the embedding distance without attention entropy minimization is,
% \begin{align}
%    \sum_{j \in N(i)} \norm{\alpha_{ij} \cdot \phi_{\theta} h_j - (1-\alpha_{ij}) \cdot \phi_{\theta} h_j}
% \end{align}
% and the upper bound on the embedding distance with attention entropy minimization is given below.
% \begin{align}
%    \sum_{j \in N(i)} \norm{\alpha_{ij}^a \cdot \phi_{\theta} h_j - (1-\alpha_{ij}^a) \cdot \phi_{\theta} h_j}
% \end{align}
% Since $\alpha_{ij}$ is a scalar, we simplify the upper bound without attention entropy minimization to the following.
% \begin{align}
%    \sum_{j \in N(i)} |(\alpha_{ij} - (1-\alpha_{ij}))| \norm{\phi_{\theta} h_j}
% \end{align}
% and similarly the upper bound on the embedding distance with attention entropy minimization,
% \begin{align}
%    \sum_{j \in N(i)} |(\alpha_{ij}^a - (1-\alpha_{ij}^a))| \norm{\phi_{\theta} h_j}
% \end{align}
% From (1), it follows that
% \begin{align}
%     \sum_{j \in N(i)} |(\alpha_{ij} - (1-\alpha_{ij}))| \norm{\phi_{\theta} h_j} \leq \sum_{j \in N(i)} |(\alpha_{ij}^a - (1-\alpha_{ij}^a))| \norm{\phi_{\theta} h_j}
% \end{align}
% \section{You \emph{can} have an appendix here.}

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
