\section{Related Work}
\label{sec:related}
\noindent\textbf{Large Language Models.}
In recent years, language models (LMs) such as BERT~\cite{devlin2019bert}, RoBERTa~\cite{liu2019roberta}, and DistilBERT~\cite{sanh2019distilbert} have evolved to large language models (LLMs) with multi-billion parameter architectures.~\footnote{We distinguish LMs (e.g., BERT), which are smaller and fine-tunable with academic resources, from LLMs (e.g., GPT-4), which are larger and generally infeasible to fine-tune in academic settings.}
LLMs, including GPT-4~\cite{achiam2023gpt}, LLaMA-2~\cite{touvron2023llama}, and PaLM~\cite{anil2023palm}, are trained on massive text corpora and have demonstrated impressive performance in various natural language tasks such as translation, summarization, and question answering.
These models possess extensive domain knowledge and exhibit zero-shot generalization capability, enabling them to perform tasks without specific training on those tasks~\cite{yang2024harnessing,brown2020language,kojima2022large}. 
Additionally, they exhibit emergent abilities such as arithmetic, multi-step reasoning, and instruction following, which LLMs were not explicitly trained for~\cite{wei2022emergent}. 
Their performance can be further enhanced through in-context learning, where a few input-label pairs are provided as demonstrations~\cite{brown2020language,min2022rethinking,liu2021makes}.
Their versatility has enabled adoption across various fields, including computer vision~\cite{koh2023grounding,guo2023images,pan2023retrieving,tsimpoukelli2021multimodal}, tabular data analysis~\cite{hegselmann2023tabllm,narayan2022can}, and audio processing~\cite{deshmukh2024training,tang2024extending}.

\noindent\textbf{LLMs and Time Series.}
Recent advancements in LLMs have attracted attention to their integration into time series analysis.
Approaches include training LLMs (or LMs) from scratch~\cite{ansari2024chronos,nie2022time,zhang2022crossformer} or fine-tuning pre-trained LLMs~\cite{zhou2024one,chang2023llm4ts}, using time series data.
Another approach is prompt tuning, where time series data is parameterized and input into either frozen LLMs~\cite{jin2023time,sun2023test} or trainable LLMs~\cite{cao2023tempo}.
These approaches bridge the gap between time series and LLMs by either integrating LLMs directly with time series data (LLM-for-time series) or aligning time series data with the LLM embedding spaces (time series-for-LLM)~\cite{sun2023test}.
Some studies use pre-trained LLMs without additional training (i.e., zero-shot prompting)~\cite{gruver2024large,liu2023large,xue2023promptcast}. 
For example, PromptCast~\cite{xue2023promptcast} textualizes time series inputs into prompts with basic contextual information.
For a more comprehensive overview, refer to recent surveys~\cite{jin2023large,jiang2024empowering,zhang2024large}.

%\vspace{1pt}
\noindent\textit{\textbf{Our Work.}} Existing methods have focused on leveraging LLMs as direct predictors using time series through (fine-) tuning or soft/hard prompting. 
In this work, we utilize LLMs for two additional purposes beyond their typical role as a predictor.
Specifically, LLMs in \method play a role as a \textit{contextualizer} of time series data, providing a high-quality \textit{augmentation} that further enhances prediction performance.