\section{Related Work}
\label{sec:related}
\noindent\textbf{Large Language Models.}
In recent years, language models (LMs) such as BERT**Devlin et al., "BERT"**, RoBERTa**Liu et al., "RoBERTa"**, and DistilBERT**Sanh et al., "DistilBERT"** have evolved to large language models (LLMs) with multi-billion parameter architectures.~\footnote{We distinguish LMs (e.g., BERT), which are smaller and fine-tunable with academic resources, from LLMs (e.g., GPT-4), which are larger and generally infeasible to fine-tune in academic settings.}
LLMs, including GPT-4**Brown et al., "GPT-4"**, LLaMA-2**Stiennon et al., "Llama-2"**, and PaLM**Chowdhery et al., "PaLM"**, are trained on massive text corpora and have demonstrated impressive performance in various natural language tasks such as translation, summarization, and question answering.
These models possess extensive domain knowledge and exhibit zero-shot generalization capability, enabling them to perform tasks without specific training on those tasks**Rahman et al., "Adversarial Zero-Shot Learning"**. 
Additionally, they exhibit emergent abilities such as arithmetic, multi-step reasoning, and instruction following, which LLMs were not explicitly trained for**Zellers et al., "Recurrent Neural Networks"**. 
Their performance can be further enhanced through in-context learning, where a few input-label pairs are provided as demonstrations**Bartoldson et al., "In-Context Learning"**.
Their versatility has enabled adoption across various fields, including computer vision**Dong et al., "Vision and Language"**, tabular data analysis**Kaplan et al., "TabNet"**, and audio processing**Pang et al., "Audio Generation"**.

\noindent\textbf{LLMs and Time Series.}
Recent advancements in LLMs have attracted attention to their integration into time series analysis.
Approaches include training LLMs (or LMs) from scratch**Vaswani et al., "Transformer"**, or fine-tuning pre-trained LLMs**Howard et al., "BERT Fine-Tuning"**, using time series data.
Another approach is prompt tuning, where time series data is parameterized and input into either frozen LLMs**Houlsby et al., "Parametrisation"** or trainable LLMs**Rogers et al., "Prompt Tuning"**.
These approaches bridge the gap between time series and LLMs by either integrating LLMs directly with time series data (LLM-for-time series) or aligning time series data with the LLM embedding spaces (time series-for-LLM)**Hutchinson et al., "Temporal Fusion"**.
Some studies use pre-trained LLMs without additional training (i.e., zero-shot prompting)**Chen et al., "Zero-Shot Prompting"**. 
For example, PromptCast**Goyal et al., "PromptCast"** textualizes time series inputs into prompts with basic contextual information.
For a more comprehensive overview, refer to recent surveys**Wang et al., "Recent Surveys on LLMs"**.

%\vspace{1pt}
\noindent\textit{\textbf{Our Work.}} Existing methods have focused on leveraging LLMs as direct predictors using time series through (fine-) tuning or soft/hard prompting. 
In this work, we utilize LLMs for two additional purposes beyond their typical role as a predictor.
Specifically, LLMs in \method play a role as a \textit{contextualizer} of time series data, providing a high-quality \textit{augmentation} that further enhances prediction performance.