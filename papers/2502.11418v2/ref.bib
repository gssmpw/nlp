@inproceedings{devlin2019bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={NAACL},
  year={2019}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={NeurIPS},
  year={2017}
}

@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={TMLR},
  year={2022}
}

@article{anil2023palm,
  title={Palm 2 technical report},
  author={Anil, Rohan and Dai, Andrew M and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and others},
  journal={arXiv preprint arXiv:2305.10403},
  year={2023}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@inproceedings{zhang2023prompting,
  title={Prompting large language model for machine translation: A case study},
  author={Zhang, Biao and Haddow, Barry and Birch, Alexandra},
  booktitle={ICML},
  year={2023}
}

@inproceedings{wang2023document,
  title={Document-Level Machine Translation with Large Language Models},
  author={Wang, Longyue and Lyu, Chenyang and Ji, Tianbo and Zhang, Zhirui and Yu, Dian and Shi, Shuming and Tu, Zhaopeng},
  booktitle={EMNLP},
  year={2023}
}

@inproceedings{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  booktitle={NeurIPS},
  year={2020}
}

@article{yang2024harnessing,
  title={Harnessing the power of llms in practice: A survey on chatgpt and beyond},
  author={Yang, Jingfeng and Jin, Hongye and Tang, Ruixiang and Han, Xiaotian and Feng, Qizhang and Jiang, Haoming and Zhong, Shaochen and Yin, Bing and Hu, Xia},
  journal={TKDD},
  volume={18},
  number={6},
  pages={1--32},
  year={2024}
}

@inproceedings{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  booktitle={NeurIPS},
  year={2022}
}

@inproceedings{koh2023grounding,
  title={Grounding language models to images for multimodal inputs and outputs},
  author={Koh, Jing Yu and Salakhutdinov, Ruslan and Fried, Daniel},
  booktitle={ICML},
  year={2023}
}

@inproceedings{guo2023images,
  title={From images to textual prompts: Zero-shot visual question answering with frozen large language models},
  author={Guo, Jiaxian and Li, Junnan and Li, Dongxu and Tiong, Anthony Meng Huat and Li, Boyang and Tao, Dacheng and Hoi, Steven},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{pan2023retrieving,
  title={Retrieving-to-answer: Zero-shot video question answering with frozen large language models},
  author={Pan, Junting and Lin, Ziyi and Ge, Yuying and Zhu, Xiatian and Zhang, Renrui and Wang, Yi and Qiao, Yu and Li, Hongsheng},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{tsimpoukelli2021multimodal,
  title={Multimodal few-shot learning with frozen language models},
  author={Tsimpoukelli, Maria and Menick, Jacob L and Cabi, Serkan and Eslami, SM and Vinyals, Oriol and Hill, Felix},
  booktitle={NeurIPS},
  year={2021}
}

@inproceedings{hegselmann2023tabllm,
  title={Tabllm: Few-shot classification of tabular data with large language models},
  author={Hegselmann, Stefan and Buendia, Alejandro and Lang, Hunter and Agrawal, Monica and Jiang, Xiaoyi and Sontag, David},
  booktitle={AISTATS},
  year={2023}
}

@article{narayan2022can,
  title={Can Foundation Models Wrangle Your Data?},
  author={Narayan, Avanika and Chami, Ines and Orr, Laurel and R{\'e}, Christopher},
  journal={PVLDB},
  volume={16},
  number={4},
  pages={738--746},
  year={2022}
}

@inproceedings{dinh2022lift,
  title={Lift: Language-interfaced fine-tuning for non-language machine learning tasks},
  author={Dinh, Tuan and Zeng, Yuchen and Zhang, Ruisu and Lin, Ziqian and Gira, Michael and Rajput, Shashank and Sohn, Jy-yong and Papailiopoulos, Dimitris and Lee, Kangwook},
  booktitle={NeurIPS},
  year={2022}
}

@inproceedings{fathullah2024prompting,
  title={Prompting large language models with speech recognition abilities},
  author={Fathullah, Yassir and Wu, Chunyang and Lakomkin, Egor and Jia, Junteng and Shangguan, Yuan and Li, Ke and Guo, Jinxi and Xiong, Wenhan and Mahadeokar, Jay and Kalinli, Ozlem and others},
  booktitle={ICASSP},
  year={2024}
}

@inproceedings{deshmukh2024training,
  title={Training audio captioning models without audio},
  author={Deshmukh, Soham and Elizalde, Benjamin and Emmanouilidou, Dimitra and Raj, Bhiksha and Singh, Rita and Wang, Huaming},
  booktitle={ICASSP},
  year={2024}
}

@inproceedings{tang2024extending,
  title={Extending Large Language Models for Speech and Audio Captioning},
  author={Tang, Changli and Yu, Wenyi and Sun, Guangzhi and Chen, Xianzhao and Tan, Tian and Li, Wei and Lu, Lu and Ma, Zejun and Zhang, Chao},
  booktitle={ICASSP},
  year={2024}
}

@inproceedings{nie2022time,
  title={A Time Series is Worth 64 Words: Long-term Forecasting with Transformers},
  author={Nie, Yuqi and Nguyen, Nam H and Sinthong, Phanwadee and Kalagnanam, Jayant},
  booktitle={ICLR},
  year={2023}
}

@inproceedings{liuitransformer,
  title={iTransformer: Inverted Transformers Are Effective for Time Series Forecasting},
  author={Liu, Yong and Hu, Tengge and Zhang, Haoran and Wu, Haixu and Wang, Shiyu and Ma, Lintao and Long, Mingsheng},
  booktitle={ICLR},
  year={2024}
}

@inproceedings{yi2024frequency,
  title={Frequency-domain MLPs are more effective learners in time series forecasting},
  author={Yi, Kun and Zhang, Qi and Fan, Wei and Wang, Shoujin and Wang, Pengyang and He, Hui and An, Ning and Lian, Defu and Cao, Longbing and Niu, Zhendong},
  booktitle={NeurIPS},
  year={2024}
}

@inproceedings{jin2023time,
  title={Time-LLM: Time Series Forecasting by Reprogramming Large Language Models},
  author={Jin, Ming and Wang, Shiyu and Ma, Lintao and Chu, Zhixuan and Zhang, James Y and Shi, Xiaoming and Chen, Pin-Yu and Liang, Yuxuan and Li, Yuan-Fang and Pan, Shirui and others},
  booktitle={ICLR},
  year={2024}
}

@article{xue2023promptcast,
  title={Promptcast: A new prompt-based learning paradigm for time series forecasting},
  author={Xue, Hao and Salim, Flora D},
  journal={TKDE},
  volume={36},
  number={11},
  pages={6851-6864},
  year={2023}
}

@inproceedings{gruver2024large,
  title={Large language models are zero-shot time series forecasters},
  author={Gruver, Nate and Finzi, Marc and Qiu, Shikai and Wilson, Andrew G},
  booktitle={NeurIPS},
  year={2023}
}

@article{ansari2024chronos,
  title={Chronos: Learning the language of time series},
  author={Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Sundar and Arango, Sebastian Pineda and Kapoor, Shubham and others},
  journal={arXiv preprint arXiv:2403.07815},
  year={2024}
}

@article{jiang2024empowering,
  title={Empowering Time Series Analysis with Large Language Models: A Survey},
  author={Jiang, Yushan and Pan, Zijie and Zhang, Xikun and Garg, Sahil and Schneider, Anderson and Nevmyvaka, Yuriy and Song, Dongjin},
  journal={arXiv preprint arXiv:2402.03182},
  year={2024}
}

@article{jin2023large,
  title={Large models for time series and spatio-temporal data: A survey and outlook},
  author={Jin, Ming and Wen, Qingsong and Liang, Yuxuan and Zhang, Chaoli and Xue, Siqiao and Wang, Xue and Zhang, James and Wang, Yi and Chen, Haifeng and Li, Xiaoli and others},
  journal={arXiv preprint arXiv:2310.10196},
  year={2023}
}

@article{zhang2024large,
  title={Large Language Models for Time Series: A Survey},
  author={Zhang, Xiyuan and Chowdhury, Ranak Roy and Gupta, Rajesh K and Shang, Jingbo},
  journal={arXiv preprint arXiv:2402.01801},
  year={2024}
}

@inproceedings{mirchandani2023large,
  title={Large Language Models as General Pattern Machines},
  author={Mirchandani, Suvir and Xia, Fei and Florence, Pete and Ichter, Brian and Driess, Danny and Arenas, Montserrat Gonzalez and Rao, Kanishka and Sadigh, Dorsa and Zeng, Andy},
  booktitle={CoRL},
  year={2023}
}

@article{wang2023enhancing,
  title={Enhancing recommender systems with large language model reasoning graphs},
  author={Wang, Yan and Chu, Zhixuan and Ouyang, Xin and Wang, Simeng and Hao, Hongyan and Shen, Yue and Gu, Jinjie and Xue, Siqiao and Zhang, James Y and Cui, Qing and others},
  journal={arXiv preprint arXiv:2308.10835},
  year={2023}
}

@article{chu2023leveraging,
  title={Leveraging large language models for pre-trained recommender systems},
  author={Chu, Zhixuan and Hao, Hongyan and Ouyang, Xin and Wang, Simeng and Wang, Yan and Shen, Yue and Gu, Jinjie and Cui, Qing and Li, Longfei and Xue, Siqiao and others},
  journal={arXiv preprint arXiv:2308.10837},
  year={2023}
}

@article{lievin2024can,
  title={Can large language models reason about medical questions?},
  author={Li{\'e}vin, Valentin and Hother, Christoffer Egeberg and Motzfeldt, Andreas Geert and Winther, Ole},
  journal={Patterns},
  volume={5},
  number={3},
  pages={100943},
  year={2024}
}

@inproceedings{sun2022black,
  title={Black-box tuning for language-model-as-a-service},
  author={Sun, Tianxiang and Shao, Yunfan and Qian, Hong and Huang, Xuanjing and Qiu, Xipeng},
  booktitle={ICML},
  year={2022}
}

@article{shi2023replug,
  title={Replug: Retrieval-augmented black-box language models},
  author={Shi, Weijia and Min, Sewon and Yasunaga, Michihiro and Seo, Minjoon and James, Rich and Lewis, Mike and Zettlemoyer, Luke and Yih, Wen-tau},
  journal={arXiv preprint arXiv:2301.12652},
  year={2023}
}

@inproceedings{kamalloo2023evaluating,
  title={Evaluating Open-Domain Question Answering in the Era of Large Language Models},
  author={Kamalloo, Ehsan and Dziri, Nouha and Clarke, Charles and Rafiei, Davood},
  booktitle={ACL},
  year={2023}
}

@article{zheng2023lmsys,
  title={Lmsys-chat-1m: A large-scale real-world llm conversation dataset},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Li, Tianle and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Li, Zhuohan and Lin, Zi and Xing, Eric and others},
  journal={arXiv preprint arXiv:2309.11998},
  year={2023}
}

@inproceedings{qin2023chatgpt,
  title={Is ChatGPT a General-Purpose Natural Language Processing Task Solver?},
  author={Qin, Chengwei and Zhang, Aston and Zhang, Zhuosheng and Chen, Jiaao and Yasunaga, Michihiro and Yang, Diyi},
  booktitle={EMNLP},
  year={2023}
}

@article{schneider1974climate,
  title={Climate modeling},
  author={Schneider, Stephen H and Dickinson, Robert E},
  journal={Reviews of Geophysics},
  volume={12},
  number={3},
  pages={447--493},
  year={1974}
}

@inproceedings{liu2023sadi,
  title={Sadi: A self-adaptive decomposed interpretable framework for electric load forecasting under extreme events},
  author={Liu, Hengbo and Ma, Ziqing and Yang, Linxiao and Zhou, Tian and Xia, Rui and Wang, Yi and Wen, Qingsong and Sun, Liang},
  booktitle={ICASSP},
  year={2023}
}

@article{li2015trend,
  title={Trend modeling for traffic time series analysis: An integrated study},
  author={Li, Li and Su, Xiaonan and Zhang, Yi and Lin, Yuetong and Li, Zhiheng},
  journal={T-ITS},
  volume={16},
  number={6},
  pages={3430--3439},
  year={2015}
}

@inproceedings{cao2023tempo,
  title={TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting},
  author={Cao, Defu and Jia, Furong and Arik, Sercan O and Pfister, Tomas and Zheng, Yixiang and Ye, Wen and Liu, Yan},
  booktitle={ICLR},
  year={2023}
}

@article{liu2023large,
  title={Large language models are few-shot health learners},
  author={Liu, Xin and McDuff, Daniel and Kovacs, Geza and Galatzer-Levy, Isaac and Sunshine, Jacob and Zhan, Jiening and Poh, Ming-Zher and Liao, Shun and Di Achille, Paolo and Patel, Shwetak},
  journal={arXiv preprint arXiv:2305.15525},
  year={2023}
}

@inproceedings{xue2022leveraging,
  title={Leveraging language foundation models for human mobility forecasting},
  author={Xue, Hao and Voutharoja, Bhanu Prakash and Salim, Flora D},
  booktitle={SIGSPATIAL},
  year={2022}
}

@inproceedings{sawhney2020deep,
  title={Deep attentive learning for stock movement prediction from social media text and company correlations},
  author={Sawhney, Ramit and Agarwal, Shivam and Wadhwa, Arnav and Shah, Rajiv},
  booktitle={EMNLP},
  year={2020}
}

@inproceedings{wu2021autoformer,
  title={Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting},
  author={Wu, Haixu and Xu, Jiehui and Wang, Jianmin and Long, Mingsheng},
  booktitle={NeurIPS},
  year={2021}
}

@inproceedings{zhang2022crossformer,
  title={Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting},
  author={Zhang, Yunhao and Yan, Junchi},
  booktitle={ICLR},
  year={2022}
}

@inproceedings{sun2023test,
  title={TEST: Text prototype aligned embedding to activate LLM's ability for time series},
  author={Sun, Chenxi and Li, Yaliang and Li, Hongyan and Hong, Shenda},
  booktitle={ICLR},
  year={2024}
}

@inproceedings{zhou2024one,
  title={One fits all: Power general time series analysis by pretrained lm},
  author={Zhou, Tian and Niu, Peisong and Sun, Liang and Jin, Rong and others},
  booktitle={NeurIPS},
  year={2024}
}

@article{chang2023llm4ts,
  title={Llm4ts: Two-stage fine-tuning for time-series forecasting with pre-trained llms},
  author={Chang, Ching and Peng, Wen-Chih and Chen, Tien-Fu},
  journal={arXiv preprint arXiv:2308.08469},
  year={2023}
}

@inproceedings{zhou2021informer,
  title={Informer: Beyond efficient transformer for long sequence time-series forecasting},
  author={Zhou, Haoyi and Zhang, Shanghang and Peng, Jieqi and Zhang, Shuai and Li, Jianxin and Xiong, Hui and Zhang, Wancai},
  booktitle={AAAI},
  year={2021}
}

@inproceedings{wu2022timesnet,
  title={Timesnet: Temporal 2d-variation modeling for general time series analysis},
  author={Wu, Haixu and Hu, Tengge and Liu, Yong and Zhou, Hang and Wang, Jianmin and Long, Mingsheng},
  booktitle={ICLR},
  year={2022}
}

@inproceedings{zeng2023transformers,
  title={Are transformers effective for time series forecasting?},
  author={Zeng, Ailing and Chen, Muxi and Zhang, Lei and Xu, Qiang},
  booktitle={AAAI},
  year={2023}
}

@inproceedings{min2022rethinking,
  title={Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?},
  author={Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  booktitle={EMNLP},
  year={2022}
}

@article{liu2021makes,
  title={What Makes Good In-Context Examples for GPT-$3 $?},
  author={Liu, Jiachang and Shen, Dinghan and Zhang, Yizhe and Dolan, Bill and Carin, Lawrence and Chen, Weizhu},
  journal={arXiv preprint arXiv:2101.06804},
  year={2021}
}

@inproceedings{berndt1994using,
  title={Using dynamic time warping to find patterns in time series},
  author={Berndt, Donald J and Clifford, James},
  booktitle={KDD},
  year={1994}
}

@book{chowdhury2010introduction,
  title={Introduction to modern information retrieval},
  author={Chowdhury, Gobinda G},
  year={2010},
  publisher={Facet publishing}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{chentsmixer,
  title={TSMixer: An All-MLP Architecture for Time Series Forecasting},
  author={Chen, Si-An and Li, Chun-Liang and Arik, Sercan O and Yoder, Nathanael Christian and Pfister, Tomas},
  journal={Transactions on Machine Learning Research},
  year={2023}
}
