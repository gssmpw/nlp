[
  {
    "index": 0,
    "papers": [
      {
        "key": "devlin2019bert",
        "author": "Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina",
        "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "liu2019roberta",
        "author": "Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin",
        "title": "Roberta: A robustly optimized bert pretraining approach"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "sanh2019distilbert",
        "author": "Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas",
        "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "achiam2023gpt",
        "author": "Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others",
        "title": "Gpt-4 technical report"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "touvron2023llama",
        "author": "Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others",
        "title": "Llama 2: Open foundation and fine-tuned chat models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "anil2023palm",
        "author": "Anil, Rohan and Dai, Andrew M and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and others",
        "title": "Palm 2 technical report"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "yang2024harnessing",
        "author": "Yang, Jingfeng and Jin, Hongye and Tang, Ruixiang and Han, Xiaotian and Feng, Qizhang and Jiang, Haoming and Zhong, Shaochen and Yin, Bing and Hu, Xia",
        "title": "Harnessing the power of llms in practice: A survey on chatgpt and beyond"
      },
      {
        "key": "brown2020language",
        "author": "Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others",
        "title": "Language models are few-shot learners"
      },
      {
        "key": "kojima2022large",
        "author": "Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke",
        "title": "Large language models are zero-shot reasoners"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "wei2022emergent",
        "author": "Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others",
        "title": "Emergent abilities of large language models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "brown2020language",
        "author": "Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others",
        "title": "Language models are few-shot learners"
      },
      {
        "key": "min2022rethinking",
        "author": "Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke",
        "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?"
      },
      {
        "key": "liu2021makes",
        "author": "Liu, Jiachang and Shen, Dinghan and Zhang, Yizhe and Dolan, Bill and Carin, Lawrence and Chen, Weizhu",
        "title": "What Makes Good In-Context Examples for GPT-$3 $?"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "koh2023grounding",
        "author": "Koh, Jing Yu and Salakhutdinov, Ruslan and Fried, Daniel",
        "title": "Grounding language models to images for multimodal inputs and outputs"
      },
      {
        "key": "guo2023images",
        "author": "Guo, Jiaxian and Li, Junnan and Li, Dongxu and Tiong, Anthony Meng Huat and Li, Boyang and Tao, Dacheng and Hoi, Steven",
        "title": "From images to textual prompts: Zero-shot visual question answering with frozen large language models"
      },
      {
        "key": "pan2023retrieving",
        "author": "Pan, Junting and Lin, Ziyi and Ge, Yuying and Zhu, Xiatian and Zhang, Renrui and Wang, Yi and Qiao, Yu and Li, Hongsheng",
        "title": "Retrieving-to-answer: Zero-shot video question answering with frozen large language models"
      },
      {
        "key": "tsimpoukelli2021multimodal",
        "author": "Tsimpoukelli, Maria and Menick, Jacob L and Cabi, Serkan and Eslami, SM and Vinyals, Oriol and Hill, Felix",
        "title": "Multimodal few-shot learning with frozen language models"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "hegselmann2023tabllm",
        "author": "Hegselmann, Stefan and Buendia, Alejandro and Lang, Hunter and Agrawal, Monica and Jiang, Xiaoyi and Sontag, David",
        "title": "Tabllm: Few-shot classification of tabular data with large language models"
      },
      {
        "key": "narayan2022can",
        "author": "Narayan, Avanika and Chami, Ines and Orr, Laurel and R{\\'e}, Christopher",
        "title": "Can Foundation Models Wrangle Your Data?"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "deshmukh2024training",
        "author": "Deshmukh, Soham and Elizalde, Benjamin and Emmanouilidou, Dimitra and Raj, Bhiksha and Singh, Rita and Wang, Huaming",
        "title": "Training audio captioning models without audio"
      },
      {
        "key": "tang2024extending",
        "author": "Tang, Changli and Yu, Wenyi and Sun, Guangzhi and Chen, Xianzhao and Tan, Tian and Li, Wei and Lu, Lu and Ma, Zejun and Zhang, Chao",
        "title": "Extending Large Language Models for Speech and Audio Captioning"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "ansari2024chronos",
        "author": "Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Sundar and Arango, Sebastian Pineda and Kapoor, Shubham and others",
        "title": "Chronos: Learning the language of time series"
      },
      {
        "key": "nie2022time",
        "author": "Nie, Yuqi and Nguyen, Nam H and Sinthong, Phanwadee and Kalagnanam, Jayant",
        "title": "A Time Series is Worth 64 Words: Long-term Forecasting with Transformers"
      },
      {
        "key": "zhang2022crossformer",
        "author": "Zhang, Yunhao and Yan, Junchi",
        "title": "Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "zhou2024one",
        "author": "Zhou, Tian and Niu, Peisong and Sun, Liang and Jin, Rong and others",
        "title": "One fits all: Power general time series analysis by pretrained lm"
      },
      {
        "key": "chang2023llm4ts",
        "author": "Chang, Ching and Peng, Wen-Chih and Chen, Tien-Fu",
        "title": "Llm4ts: Two-stage fine-tuning for time-series forecasting with pre-trained llms"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "jin2023time",
        "author": "Jin, Ming and Wang, Shiyu and Ma, Lintao and Chu, Zhixuan and Zhang, James Y and Shi, Xiaoming and Chen, Pin-Yu and Liang, Yuxuan and Li, Yuan-Fang and Pan, Shirui and others",
        "title": "Time-LLM: Time Series Forecasting by Reprogramming Large Language Models"
      },
      {
        "key": "sun2023test",
        "author": "Sun, Chenxi and Li, Yaliang and Li, Hongyan and Hong, Shenda",
        "title": "TEST: Text prototype aligned embedding to activate LLM's ability for time series"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "cao2023tempo",
        "author": "Cao, Defu and Jia, Furong and Arik, Sercan O and Pfister, Tomas and Zheng, Yixiang and Ye, Wen and Liu, Yan",
        "title": "TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "sun2023test",
        "author": "Sun, Chenxi and Li, Yaliang and Li, Hongyan and Hong, Shenda",
        "title": "TEST: Text prototype aligned embedding to activate LLM's ability for time series"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "gruver2024large",
        "author": "Gruver, Nate and Finzi, Marc and Qiu, Shikai and Wilson, Andrew G",
        "title": "Large language models are zero-shot time series forecasters"
      },
      {
        "key": "liu2023large",
        "author": "Liu, Xin and McDuff, Daniel and Kovacs, Geza and Galatzer-Levy, Isaac and Sunshine, Jacob and Zhan, Jiening and Poh, Ming-Zher and Liao, Shun and Di Achille, Paolo and Patel, Shwetak",
        "title": "Large language models are few-shot health learners"
      },
      {
        "key": "xue2023promptcast",
        "author": "Xue, Hao and Salim, Flora D",
        "title": "Promptcast: A new prompt-based learning paradigm for time series forecasting"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "xue2023promptcast",
        "author": "Xue, Hao and Salim, Flora D",
        "title": "Promptcast: A new prompt-based learning paradigm for time series forecasting"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "jin2023large",
        "author": "Jin, Ming and Wen, Qingsong and Liang, Yuxuan and Zhang, Chaoli and Xue, Siqiao and Wang, Xue and Zhang, James and Wang, Yi and Chen, Haifeng and Li, Xiaoli and others",
        "title": "Large models for time series and spatio-temporal data: A survey and outlook"
      },
      {
        "key": "jiang2024empowering",
        "author": "Jiang, Yushan and Pan, Zijie and Zhang, Xikun and Garg, Sahil and Schneider, Anderson and Nevmyvaka, Yuriy and Song, Dongjin",
        "title": "Empowering Time Series Analysis with Large Language Models: A Survey"
      },
      {
        "key": "zhang2024large",
        "author": "Zhang, Xiyuan and Chowdhury, Ranak Roy and Gupta, Rajesh K and Shang, Jingbo",
        "title": "Large Language Models for Time Series: A Survey"
      }
    ]
  }
]