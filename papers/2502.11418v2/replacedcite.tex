\section{Related Work}
\label{sec:related}
\noindent\textbf{Large Language Models.}
In recent years, language models (LMs) such as BERT____, RoBERTa____, and DistilBERT____ have evolved to large language models (LLMs) with multi-billion parameter architectures.~\footnote{We distinguish LMs (e.g., BERT), which are smaller and fine-tunable with academic resources, from LLMs (e.g., GPT-4), which are larger and generally infeasible to fine-tune in academic settings.}
LLMs, including GPT-4____, LLaMA-2____, and PaLM____, are trained on massive text corpora and have demonstrated impressive performance in various natural language tasks such as translation, summarization, and question answering.
These models possess extensive domain knowledge and exhibit zero-shot generalization capability, enabling them to perform tasks without specific training on those tasks____. 
Additionally, they exhibit emergent abilities such as arithmetic, multi-step reasoning, and instruction following, which LLMs were not explicitly trained for____. 
Their performance can be further enhanced through in-context learning, where a few input-label pairs are provided as demonstrations____.
Their versatility has enabled adoption across various fields, including computer vision____, tabular data analysis____, and audio processing____.

\noindent\textbf{LLMs and Time Series.}
Recent advancements in LLMs have attracted attention to their integration into time series analysis.
Approaches include training LLMs (or LMs) from scratch____ or fine-tuning pre-trained LLMs____, using time series data.
Another approach is prompt tuning, where time series data is parameterized and input into either frozen LLMs____ or trainable LLMs____.
These approaches bridge the gap between time series and LLMs by either integrating LLMs directly with time series data (LLM-for-time series) or aligning time series data with the LLM embedding spaces (time series-for-LLM)____.
Some studies use pre-trained LLMs without additional training (i.e., zero-shot prompting)____. 
For example, PromptCast____ textualizes time series inputs into prompts with basic contextual information.
For a more comprehensive overview, refer to recent surveys____.

%\vspace{1pt}
\noindent\textit{\textbf{Our Work.}} Existing methods have focused on leveraging LLMs as direct predictors using time series through (fine-) tuning or soft/hard prompting. 
In this work, we utilize LLMs for two additional purposes beyond their typical role as a predictor.
Specifically, LLMs in \method play a role as a \textit{contextualizer} of time series data, providing a high-quality \textit{augmentation} that further enhances prediction performance.