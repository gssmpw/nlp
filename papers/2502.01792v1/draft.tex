\section{Introduction} 

Two-sided platforms, where some individuals view content or information provided by other individuals, are ubiquitous in real-world decisions, e.g., video streaming, job matching, and online ads~\citep{boutilier2023modeling}. 
In such applications, viewers and providers may co-evolve and mutually influence each other: providers increase their content production if they receive more attention from viewers (i.e., exposure), and the platform gains more viewers if viewers receive high-quality and favored content (i.e., satisfaction). 
These effects are mediated by the platform's recommendation algorithm.
Considering such \textit{non-stationarity} and \textit{two-sided dynamics} is crucial, as the viewers and providers are affected by each others' population in self-reinforcing feedback loops.

\begin{example}[Video recommendation]
    When a platform has many videos about sports, we can expect that top sports videos have high quality (e.g., production and intellect). In contrast, if a platform is popular among sports lovers, creators will produce more sports videos to gain more views.
\end{example}

\begin{example}[Job matching] 
    When a platform has many applicants from a target category, companies looking to fill a specific role can identify more highly skilled applicants. 
    On the other hand, if a platform has more openings for a specific job type, more applicants from target categories will register for the service. 
\end{example}

\begin{figure*}[t]
\centering
\includegraphics[clip, width=0.95\linewidth]{figs/synthetic/concave_increase_dynamics_representative.png}
    \caption{\textbf{Comparing the myopic-greedy policy, the uniform random policy, and the long-term policy in a synthetic simulation.} 
    As shown, the myopic-greedy policy loses the provider population due to concentrated exposure allocation, resulting in the negative impact on the viewer welfare in the long-run.
    The ``long-term'' policy is based on the algorithm proposed in Section~\ref{sec:proposal} (Eq.~\eqref{eq:look_ahead_policy}), and the experiment setting follows Section~\ref{sec:synthetic_experiment} (with a small initial population).
  } 
  \label{fig:example}
\end{figure*}

The \textit{``population effects''} in the aforementioned examples strongly affect viewer utility and their long-term satisfaction.
However, their implications for recommendation policy design have been under-explored. 
The conventional formulation of recommendation follows (contextual) bandits~\citep{li2010contextual} and assumes that viewers and providers are static across timesteps. 
Some recent work studies content provider departures~\citep{mladenov2020optimizing, huttenlocher2023matching} and the (negative) impacts on viewer welfare.
However, the viewer population is modeled as static.
In contrast, existing works which consider dynamic viewer populations assume that provider population is fixed~\citep{hashimoto2018fairness, dean2022emergent}.
Therefore, we cannot tell how we should optimize a policy, particularly in the initial launch of the platform when two-sided dynamics exist. Finally, existing works considering strategic content providers~\citep{hron2022modeling, jagadeesan2022supply, yao2023rethinking, yao2024exploration, prasad2023content} model the (strategic) evolution of provider features, assuming that the total number of viewers and providers are fixed. These works cannot tell if a platform can ``grow the pie'' (i.e., viewer and provider populations) to improve long-term welfare.

In response to this gap, \textbf{we study the dynamics of \textit{``population effects''} on two-sided platforms}. Specifically, we consider viewer and provider participation dynamics which operate as follows: (1) the population of providers increases as their exposure increases, (2) the population of viewers increases as their satisfaction increases, and (3) the potential for viewer satisfaction increases as provider populations increase. 
We assume that these effects follow an arbitrary monotonically increasing function, and the immediate utility (in the form of exposure or satisfaction) is observed. 
The key consequence of setting is that the default approach to recommendation can perform much worse in the long term than even a uniform random policy.
Figure~\ref{fig:example} illustrates this shortcoming of the default approach, a myopic-greedy policy that recommends providers to viewers on the basis of immediate utility.

We examine success and failure cases of the myopic-greedy policy through control- and game-theoretic analyses. Our primary results are the following three points. 
First, by analyzing the convergence conditions of the dynamics, we argue that concentrated exposure allocation among provider groups can easily cause polarization of viewer and provider populations, potentially resulting in a smaller pie (i.e., populations) and long-term social welfare compared to an exposure-distributing policy.
These findings highlight the importance of provider-side awareness such as exposure fairness~\citep{singh2018fairness} for the long-term success of two-sided platforms under population dynamics. 
Second, we analyze the linear case to show that the myopic-greedy policy is guaranteed to be optimal only if the population effects (i.e., utility gain by the population growth) are homogeneous across provider groups. 
Third, we explain the shortcomings of the myopic-greedy policy by decomposing the welfare sub-optimality into two terms:
the \textit{``policy regret''} and \textit{``population regret''}. The former comes from the difference caused between the policy and the myopic-optimal policy at each timestep given the current population, while the latter comes from the difference between the current population and the population under the optimal policy. By definition, the myopic-greedy policy minimizes only the policy regret (i.e., short-term objective).
Because the myopic-policy ignores the population regret (i.e., long-term impact on the dynamics), the myopic-greedy policy fails when the scale of the long-term utility gain from the population growth is large. 

Finally, we propose a simple algorithm that balances the policy and population regrets by projecting the long term population that will result from the current viewer satisfaction and provider exposure. Our proposed \textbf{``look-ahead'' policy optimizes the utility at the projected long term population} instead of the immediate population. The synthetic and real-data experiments using the KuaiRec dataset~\citep{gao2022kuairec} demonstrate that the proposed algorithm works better than both myopic-greedy and uniform random policies in multiple configurations by better trading off the long and short term goals accounting for the population growth.

Our contributions are summarized as follows:
\begin{itemize}
    \item We formulate the ``population effects'' in two-sided platforms where viewer and provider populations evolve.
    \item We find that the myopic-greedy policy can fall short when the population effects are heterogeneous.
    \item We find that an exposure-guaranteeing policy is useful for growing populations and minimizing the population regret.
    \item We propose a simple algorithm that considers the long term population and demonstrate its effectiveness in the synthetic and real-data experiments.
\end{itemize}

\input{section/problem_formulation}
\input{section/stability}
\input{section/linear_case}

\section{Optimizing the long-term social welfare} \label{sec:proposal}

The key observation from the previous sections is that myopic-greedy policy fails by ignoring the \textit{population regret}, which comes from the difference between the population of the current policy and that of the optimal one. Therefore, we first establish a policy learning method that optimizes for the \textit{population regret} and later consider balancing this with the policy regret. 
However, one difficulty in minimizing the population regret $\Delta R(\blambda_t^{\ast}, \blambda_t)$ (Theorem~\ref{prop:regret}) is that the population $\blambda_{t}$ depends on the past choices of policy $\bpi$. This means that when optimizing the policy, we should take into account its future influence on the population. 
Because we know that the population gradually changes towards the reference population $\bar{\lambda}(\cdot)$, we consider the following \textbf{Look-ahead policy}:
\begin{align}
    \bpi_t^{(d)} \; := \; {\arg\max}_{\bpi \in \Pi} \; R(\bar{\bpi}_t^{1}(\bpi); \bar{\blambda}_t(\bpi)) \label{eq:look_ahead_policy}
\end{align}
Above, $\bar{\blambda}_t(\bpi)$ is the reference population at timestep $t$ given the viewer satisfaction and provider exposure realized by the policy $\bpi$ at population $\blambda_t$, i.e., $\bar{\lambda}_k(s_{t, k})$ and $\bar{\lambda}_l(e_{t, l})$. 
$\bar{\bpi}_t^{1}(\bpi)$ is the myopic-greedy policy at the reference population $\bar{\blambda}_t(\bpi)$. 
Thus, the lookahead policy focuses on reaching reference populations which enable high user satisfaction.

The look-ahead policy's optimization problem is potentially nonconvex.
To make it differential, one can consider the following softmax policy as the approximation of $\bar{\bpi}_t^{1}$:
\begin{align*}
    \bar{\pi}_{t,k,l}^{1} = \frac{\exp(\gamma \cdot (b_{k,l} + f_{k,l}(\bar{\lambda}_l(e_l)))}{\sum_{l' \in [L]} \exp(\gamma \cdot (b_{k,l'} + f_{k,l'}(\bar{\lambda}_{l'}(e_{l'})))} 
\end{align*}
where $\gamma > 0$ is the inverse temperature parameter. Then, we can optimize the objective function in Eq.~\eqref{eq:look_ahead_policy} via gradient ascent, where we present the exact gradient in Appendix~\ref{app:gradient}.

Once we obtain the look-ahead policy, we can interpolate between the look-ahead policy and the myopic-greedy policy to balance the population and policy regrets as follows:
\begin{align}
    \bpi_t = \beta \bpi_t^{(d)} + (1 - \beta) \bpi_t^{(m)} \label{eq:interpolation}
\end{align}
where $\beta \in [0, 1]$ is the interpolation hyperparameter and $\bpi_t^{(m)}$ is the myopic-greedy policy. 
$\beta$ can be determined by the platform's desire to focus on short vs. long term goals.

\subsection{Estimation the dynamics} \label{sec:dyanmics_estimation}
 
In practice, there may be situations in which we need to estimate the dynamics function ($\bar{\lambda}$ and $f$) using some function approximation. In such case, we can use the following Explore-then-Commit style estimation:
\begin{enumerate}
    \item For $t \leq T_b$, deploy some epsilon-greedy policy and collect the data of $D_t := (s_k, e_l, q_{k,l}, \lambda_k, \lambda_l),  \forall (k, l) \in [K] \times [L]$. Then, update the dataset as $\mathcal{D}_t = \mathcal{D}_{t-1} \cup D_t$ where $\mathcal{D}_0 := \phi$ (empty set). $T_b$ is a burn-in period.
    \item For $t > T_b$, update the dynamics models ($\widehat{q}_{l,k}(\lambda_{l}), \widehat{\lambda}(s_{k}), \widehat{\lambda}(e_{l})$) via supervised learning. 
    Note that the true ``reference population'' is obtained from data as $\bar{\lambda}_{t'} = \eta^{-1} (\lambda_{t'+1} - \lambda_{t'}) + \lambda_{t'}$ from  Eqs.~\eqref{eq:user_dynamics} and \eqref{eq:content_dynamics}.
    Then, optimize the policy using Eqs.~\eqref{eq:look_ahead_policy} and \eqref{eq:interpolation} using the estimated functions $\widehat{\lambda}$ and $\widehat{q}$, collect the interaction data $D_t$, and add them to the dataset $\mathcal{D}_{t}$.
\end{enumerate}

\section{Synthetic Experiment} \label{sec:synthetic_experiment}

We first study the dynamics and the performance of the proposed method in a synthetic experiment. In this task, we use $K = L = 20$ subgroups. To define the base utility, we first sample 20-dimensional binary feature vectors ($u_k, c_l$) from a Bernoulli distribution for each viewer and provider group and let their inner products be the base utility $b_{k,l} = u_k^{\top} c_l, \forall (k,l) \in [K] \times [L]$. Then we simulate the following concave dynamics:
\begin{align}
    \bar{\lambda}_k(z) 
        &= \lambda_k^{(max)} (\sigma(z / \tau_k^{(\lambda)}) - 0.5),  \label{eq:experiment_population_dynamics}
\end{align}
where $\sigma(z) := 1 / (1 + \exp(-z))$ is the sigmoid function, and $\bar{\lambda}(\cdot)$ follows the upper half of the sigmoid function.

Next, to simulate a \textit{heterogeneous} population effect, we further take inner products between viewer embeddings $u_k$ and the vector of population-dependent quality as follows. 
\begin{align}
    f_{k,l}(\lambda_l) = u_k^{\top} [\bar{f}_l^{(1)}(\lambda_l), \bar{f}_l^{(2)}(\lambda_l), \cdots, \bar{f}_l^{(d)}(\lambda_l)], \label{eq:synthetic_population_effect}
\end{align} 
where its $i$-th quality element follows the upper half of the sigmoid, i.e., $\bar{f}_{l}^{(i)}(z) = F_{l}^{(i), (max)} (\sigma(z / \tau_{l}^{(i), (F)}) - 0.5)$.
We use $d=20$. With this model, each provider group has different improvements in  quality of content, e.g., visuals, humor, and technical depth, and each viewer group has different preferences on these aspects of quality. We visualize the population effect in Figure~\ref{fig:synthetic_population_effect} in the Appendix.

We initialized the subgroups populations by sampling values from the normal distribution, so we have the majority and minority subgroups at $t=0$. Specifically, we use two initializations: (1) a small population ($\lambda \sim \mathcal{N}(20, 10^2)$) and (2) a large one ($\lambda \sim \mathcal{N}(100, 30^2)$) to see how policies perform in both increasing and decreasing dynamics.

\begin{figure*}
\begin{minipage}{0.99\hsize}
  \centering
  \includegraphics[clip, width=0.98\linewidth]{figs/synthetic/concave_increase_dynamics_main.png} 
  \vspace{1mm}
\end{minipage}
\begin{minipage}{0.99\hsize}
  \centering
  \includegraphics[clip, width=0.98\linewidth]{figs/synthetic/concave_decrease_dynamics_main.png} 
  \caption{\textbf{Comparing the total welfare, and the viewer and provider populations with varying values of interpolation hyperparam, i.e., $\beta$.} ``uniform'' represents the uniform random policy. We report the result with a large initial population in Figure~\ref{fig:synthetic_two_comparison} in the Appendix.} \label{fig:synthetic_dynamics}
  \vspace{3mm}
\end{minipage}
\end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[clip, width=0.95\linewidth]{figs/synthetic/concave_increase_reward_matrix_representitive.png}
    \caption{\textbf{Comparing the utility matrix of the myopic ($\beta=0.0$), long-term ($\beta=1.0$), and uniform random policies at the final timestep and the initial utility matrix.} For the initial utility matrix, we use the one with a small initial population. 
  } 
  \label{fig:synthetic_reward_matrix}
\end{figure*}

\textbf{Compared methods}. \;
We compare the proposed look-ahead policy with varying interpolation hyperparam $\beta \in [0.0, 0.2, \ldots, 1.0]$.
We also compare the uniform random policy as a reference. When computing the look-ahead policy, we assume access to the dynamics and population effect functions.
The lookahead policy is computed with gradient ascent on the objective (Eq.~\eqref{eq:look_ahead_policy}) 100 iterations. 

\textbf{Results. } \;
We run the compared methods for 200 timesteps and report the results in Figure~\ref{fig:synthetic_dynamics}. 
The results demonstrate that the long-term (look-ahead) policy performs better than the myopic-greedy policy, as the reward gain from the population effects is large
in this setting. 
Specifically, 
we observe that the pure look-ahead policy ($\beta=1.0$) increases the provider populations while the myopic-greedy policy decreases the provider populations. 
These population changes immediately affect the total welfare, 
suggesting that guaranteeing high population among multiple subgroups via balanced exposure allocation is crucial when population effects matter. 
Indeed, we also observe a different distribution of the utility matrix at the final timestep across compared methods in Figure~\ref{fig:synthetic_reward_matrix}. 
Interestingly, while the uniform policy has the largest provider population at the final timestep in Figure~\ref{fig:synthetic_dynamics}, the look-ahead policy ($\beta=1.0$) achieves better total welfare (and population regret). This is because the look ahead policy allocates exposure more efficiently than the uniform policy to ensure both high viewer satisfaction and high provider exposure among multiple subgroups, empirically demonstrating the effectiveness of our approach.

\begin{figure*}[t]
\centering
\includegraphics[clip, width=0.95\linewidth]{figs/real/kuairec_main_population_effect_main.png}
    \caption{\textbf{Visualization of the (true) population effects in the real-world experiment.} The population effects are based on the spline function~\citep{reinsch1967smoothing} fitted on the empirical population effect (dotted points) observed in the KuaiRec~\citep{gao2022kuairec} dataset. Figures~\ref{fig:estimation_population_effect} and \ref{fig:estimation_population_dynamics} in the Appendix also report the population effects and dynamics learned by the long-term policy, following Section~\ref{sec:dyanmics_estimation}.}
  \label{fig:real_population_effect}
\end{figure*}

\begin{figure*}
\begin{minipage}{0.99\hsize}
  \centering
  \includegraphics[clip, width=0.98\linewidth]{figs/real/kuairec_main_dynamics_main.png} 
  \vspace{1mm}
\end{minipage}
\begin{minipage}{0.99\hsize}
  \centering
  \includegraphics[clip, width=0.98\linewidth]{figs/real/kuairec_main_regret_long_short_tradeoff.png} 
  \caption{\textbf{Comparing the total welfare, viewer and provider populations, and regrets in the real-world experiment.} 
  Cumulative regret is the sum of total regret by the timestep $t$, and the total regret is decomposed to the and policy regrets. Note that the true optimal policies that minimize the total regret and population regret are not accessible. Thus, we report the empirical regrets by letting one of the compared policies as the optimal baseline.}
  \label{fig:real_dynamics}
  \vspace{3mm}
\end{minipage}
\end{figure*}


\section{Real-data Experiment}

This section studies the empirical behavior of the proposed method using 
the KuaiRec~\citep{gao2022kuairec} dataset.

\textbf{Datasets.} \; KuaiRec (dense)~\citep{gao2022kuairec} is a viewer-provider interaction dataset consisting of 4,676,570 data samples with 1,411 viewers and 3,326 videos (i.e., providers). The data contains ``watch ratio'' (i.e., play duration divided by the video duration) as the viewers feedback signal. We clip the maximum watch ratio by 10 and learn the viewer-provider base utility $b(u,c)$ using a neural collaborative filtering (CF) model~\citep{he2017neural}. The base utility is calculated as individual level ($(u, c) \in \mathcal{U} \times \mathcal{C}$), where $u$ and $c$ are viewer and provider embeddings.

\textbf{Simulation.} \; 
We simulate the subgroup of viewers and providers, following the procedure presented in \citep{bose2023initializing}. Specifically, we cluster viewers and providers into $K = L = 20$ subgroups respectively, based on the viewer and provider embeddings learned by the neural CF model~\citep{he2017neural}. 
We use the same initialization and dynamics of the population as described in Section~\ref{sec:synthetic_experiment}.
Then, we simulate the utility and population effects as follows.

\begin{enumerate}
    \item Let $u_k := \mathbb{E}[u \,|\, u \in \mathcal{U}_k]$ be the mean embeddings of viewer group $k$. We define the group-wise base utility as $b_{l,k} = \mathbb{E}[b(u_k,c) \,|\, c \in\mathcal{C}_l]$ (i.e.,  mean utility that viewer group $k$ receives providers in group $l$).
    \item Next, to simulate a population effect, we generate a random permutation of providers within each provider group. Given the current provider population $\lambda_l$, we  let first $\lambda_l$ samples in the permutation as the the set of providers in the subgroup $l$ used at timestep $t$. We denote this subset as $\mathcal{C}_{t,l}$.
    Then, we define the utility from the provider group $l$ as $q(u_k, c_l) := {\arg\max}_{c \in \mathcal{C}_{t,l}} b(u_k,c)$. Therefore, the population effects are defined as 
    \begin{align*}
        f_{k,l}(\lambda_l) := {\arg\max}_{c \in \mathcal{C}_{t,l}(\lambda_l)} b(u_k,c) - b_{k,l},
    \end{align*}
    which increases monotonically as provider population ($\lambda_l$) increases. 
\end{enumerate}
To obtain a smooth population effect function,
we generate 10 different random permutation in Step 2. Then, we take the average of 10 population effects and fit spline functions~\citep{reinsch1967smoothing} implemented in SciPy~\citep{virtanen2020scipy}. The resulting population effects are in Figure~\ref{fig:real_population_effect}.

\textbf{Estimation of the dynamics.} \; In this experiment, we estimate the dynamics functions $\bar{\lambda}$ and $f$ using regression. 
We use the model $F(z) = a_0 (1 - \exp(- a_1 (x - a_2))) + a_3$ due to its concavity and flexibility, and fit the params $(a_0, \cdots, a_3)$
from interaction data as described in Section~\ref{sec:dyanmics_estimation}. 
Note that we add perturbations in the population dynamics $\xi_t$ sampled from a normal distribution as $\xi_t \sim \mathcal{N}(0.0, 0.01^2) \times \blambda_t$ (i.e., the scale of perturbation is proportional to the population) to account for the difficulty in learning the real-world dynamics. During the burn-in period (10 steps), we deploy epsilon-greedy with the corresponding value of $\beta \, (= \epsilon)$. 

\textbf{Results.} \;
Figure~\ref{fig:real_dynamics} report the population dynamics, total welfare, and the regret. Unlike synthetic experiment, the myopic-greedy policy performs better than the uniform random policy, while the proposed look-ahead policy is competitive to the myopic-policy in the total welfare after converging to the NE. However, we observe some tradeoff between the myopic-greedy and look-ahead policy. Specifically, Figure~\ref{fig:real_dynamics} (Bottom) suggests that the myopic-greedy policy ($\beta=0.0$) has some population regret compared to the look-ahead policy ($\beta=1.0$), while the look-ahead policy retains some policy regret. As the result, an interpolated policy with $\beta=0.6$ is the best among the compared methods, while all interpolated policies with various values of $\beta$ perform quite well. It is also worth mentioning that the look-ahead policy ($\beta=1.0$) maintains high total welfare even though achieving almost the same level of the provider population as the uniform policy. This suggests that the proposed look-ahead policy is able to allocate exposure efficiently by considering the long-term population effects.

Together with the synthetic experiment results, we observe that the proposed look-ahead policy adaptively behaves (near-)optimally in terms of total welfare, while also guaranteeing a high provider population through provider-fair exposure allocation. This minimizes the effort to tune the hyperparam $\beta$ as the look-ahead policy ($\beta=1.0$) works reasonably well in practical situations. 

\section{Related work} \label{app:related_work}

This section summarizes the important related work.

\textbf{Policy optimization under the population departure.} \quad The most relevant existing works to ours are \citet{mladenov2020optimizing} and \citet{huttenlocher2023matching}, which consider the population dynamics by modeling the departure of viewers and providers. Specifically, \citet{mladenov2020optimizing} assume that a provider will leave the platform if the provider cannot receive adequate exposure (i.e., exposure is below some given threshold). Then, \citet{mladenov2020optimizing} solves the constrained optimization problem as linear integer programming and demonstrates that provider fairness is crucial to maintain a high viewer welfare. To extend, \citet{huttenlocher2023matching} additionally consider the departure of viewers who receive less utility than given thresholds. \citet{huttenlocher2023matching} also formulate a matching problem to determine which viewers and providers to keep in the platform to achieve high long-term social welfare. However, both works ignore the possible growth of the platform, and how a policy design affects the ``growing-the-pie'' behavior has remained underexplored. Our work complements these existing work by finding that provider fairness is important to ensure high ``population effects'' in a generalized formulation.

\textbf{Policy optimization under strategic content providers.} \quad
Another related literature is the policy optimization under strategic content providers~\citep{hron2022modeling, jagadeesan2022supply,  yao2023rethinking}. These works often formulate content providers as ``selfish'' agents who maximize only their own utility defined by the amount of exposure minus the cost of content generation. As described in Section~\ref{sec:game_formulation}, our problem setting can also be seen as a variant of policy optimization under strategic viewers and content providers. However, our formulation is distinctive in modeling the increase and decrease of the total population, while existing works assume that the total number of viewers and providers are fixed. This difference results in novel findings: while \citet{hron2022modeling} find that more explorative (i.e., stochastic) policy can be against producing high-quality ``niche'' contents when the population is fixed, provider-fair policy (or stochastic allocation) can be beneficial when taking the population growth of multiple groups into account.

\textbf{Provider fairness, provider diversity, and viewer welfare.}
Fairness and diversity among providers have been considered as necessary metrics or constraints when optimizing policies in two-sided platforms~\citep{singh2018fairness, wang2021user, boutilier2023modeling}. While provider-fairness is initially considered important from provider-side perspectives~\citep{singh2018fairness}, recent works considers the impacts of provider fairness on viewer welfare. Specifically, provider fairness turned out important to maintain provider diversity~\citep{yao2023rethinking, hron2022modeling}, and provider diversity helps maintain viewer welfare in the long-run~\citep{su2023value, mladenov2020optimizing}. Our findings align with these works in pointing out that provider-fairness is important for long-term viewer satisfaction, but from a different viewpoint, suggesting that ``population effect'' should also be taken into account to invest future growth of populations.

\section{Conclusion}
This paper studied the 
policy design in two-sided platforms where viewer and provider populations matter. Through the control- and game-theoretic analyses, we found that the myopic-greedy policy is guaranteed optimal only when the population effects are linear and homogeneous among provider groups, and otherwise may fall short by ignoring the 
``population effects''. To take such long-term effects into account, we proposed a simple algorithm that guarantees both viewer satisfaction and provider exposure for future population growth.
We believe our work provides a cornerstone to build dynamics-aware allocation policies in two-sided platforms where multiple stakeholders engage.

