\section{Stability and sub-optimality}

This section provides theoretical analyses\footnote{All proofs are provided in Appendix~\ref{app:proofs}.} of the stability and sub-optimality under the two-sided dynamics.


\subsection{Stability} 
An important question to ask about the two-sided dynamics is on stability: \textbf{\textit{Under what conditions do the dynamics converge to a fixed point?}} The following Theorem \ref{thrm:condition_eq} provides an affirmative answer, demonstrating that the two-sided dynamics always converge to a stable fixed point, which is also a Nash Equilibrium (NE) \citep{nash1950equilibrium} of the corresponding game instance.\footnote{Definition~\ref{def:fixed_point} and \ref{def:nash_eq} in the Appendix formally define the two concepts of fixed point and Nash equilibrium.}

\begin{theorem}\label{thm:ne_exist}
    For any continuous functions $f, \bar{\lambda}$ with bounded first-order derivatives, consider the environment defined by the game instance $\G(\bpi, B, f, \bar{\lambda})$. We have:
    \begin{enumerate}[leftmargin=12pt]
        \item The NE of $\G$ always \textbf{exists}, but is not necessarily unique;
        \item The two-sided dynamics (Eqs.~\eqref{eq:user_dynamics} and \eqref{eq:content_dynamics}) always \textbf{converge to} one of $\G$'s NE, provided that $\eta_k, \eta_l$ are smaller than a constant that depends on the game parameters.
    \end{enumerate}
\end{theorem}

Theorem \ref{thm:ne_exist} establishes a general stability result for the two-sided dynamics, showing that as long as the reactiveness hyperparams are sufficiently small, it always converges to some fixed point corresponding to a NE of $\G$. This result allows us to model dynamics under various reference functions $\bar{\lambda}$, population effect $f$, and recommendation policies $\bpi$, without introducing restrictive assumptions.

Moreover, the following sufficient condition indicates an interesting relationship between the policy design and the stability of fixed points.

\begin{proposition}[Sufficient condition for stability] \label{coro:sufficient_eq}
Suppose that the first-order derivative of dynamics functions are bounded as $(\nabla_{e_l} \bar{\lambda}_l) (\nabla_{\lambda_l} f_l) \leq C_1, \forall l \in [L]$ and $(\nabla_{s_k} \bar{\lambda}_k) \leq C_2, \forall k \in [K]$ at some fixed point $\blambda_{eq}$. Also, suppose $\eta_k \leq \eta, \forall k \in [K]$. Then, $\blambda_{eq}$ is stable when 
\begin{align}
    \textstyle\sum_{k=1}^{K} \pi_{l,k} \leq \frac{4 \eta^{-1}}{C_1 C_2}. \label{eq:stable_saficient}
\end{align}
\end{proposition}

Proposition~\ref{coro:sufficient_eq} suggests that an exposure-fair policy guarantees in a balanced equilibrium when both $f$ and $\bar{\lambda}$ are monotonically increasing concave 
functions, while the exposure-concentrated policy can allow a polarized equilibrium. This is due to the following reasons. First, the upper bound (i.e., RHS of the inequality) becomes more restrictive when the first order derivative of the dynamics (i.e., $C_1$ and $C_2$) is large, which is true when viewer satisfaction ($s_k$), provider exposure ($e_l$), and provider population ($\lambda_l$) are small. While an exposure-fair policy can exclude such equilibrium due to the violation of Ineq.~\eqref{eq:stable_saficient}, exposure-concentrated policy may include polarized equilibrium with winners and losers.

Consequently, the reduced subgroup population may negatively impact the long-term viewer satisfaction, as we have seen in Figure~\ref{fig:example}. We formally discuss such impacts through the regret analysis in the next subsection.

\subsection{Sub-optimality} 
Our next question is: \textbf{\textit{How does the ``population effect'' affect the policy design when the dynamics converge?}}
To answer the question, we introduce the following notion of sub-optimality, called \textit{regret}, to measure the performance difference between the optimal (static) policy\footnote{{$\bpi^{\ast} := {\arg\max}_{\bpi \in \Pi} \sum_{t=1}^T R(\bpi; \blambda_t)$}}  $\bpi^{\ast}$ and a given (posibly time-varying) policy $\bpi_t$:
\begin{align*}
    \text{Regret}(\bpi)
    &= \frac{1}{T} \textstyle\sum_{t=1}^T \left( R(\bpi^{\ast}; \blambda_t^{\ast}) - R(\bpi_t; \blambda_t) \right)
\end{align*}
where 
$\blambda_t^{\ast}$ is the population at timestep $t$ under the policy $\bpi^{\ast}$ and $\blambda_t$ is that of $\bpi$. $T$ is the total horizon of the timesteps. 
Assuming that the policy $\bpi_t$ converges to within $\delta$ of a static policy $\bpi$, the above regret can be decomposed into two factors as shown in the following Proposition~\ref{prop:regret}.

\begin{theorem}[Regret decomposition] \label{prop:regret}
The (total) regret is decomposed into two main factors:
\begin{align*}
    \mathrm{Regret}(\bpi)
    &= \underbrace{\frac{1}{T} \sum_{t=1}^T \Delta R (\blambda_t^{\ast},  \blambda_t^{\pi})}_{(1)} + \underbrace{\frac{1}{T} \sum_{t=1}^T \Delta R (\bpi_t^{1}, \bpi_t)}_{(2)} \\
    & \quad \quad + \mathcal{O} \left(\delta / T \right) + \text{const.}
\end{align*} 
We call (1) as ``population regret'' and (2) as ``policy regret''. Each component is defined as follows.
\begin{align*}
    \Delta R (\blambda_t^{\ast},  \blambda_t) &:= R(\bpi_t^{1,\ast}; \blambda_t^{\ast}) - R(\bpi_t^{1}; \blambda_t) \\ 
    \Delta R (\bpi_t^{1}, \bpi_t) &:= R(\bpi_t^{1}; \blambda_t) - R(\bpi_t; \blambda_t),
\end{align*}
where $\bpi_t^{1}$ is the one-step myopic-greedy policy at timestep $t$ given population $\blambda_t$, and $\bpi_t^{1,\ast}$ is that of under $\blambda_t^{\ast}$. 
\end{theorem}

The population regret refers to the sub-optimality caused by the difference of population ($\blambda_t$ and $\blambda_t^{\ast}$) at timestep $t$, while the policy regret refers to the sub-optimality caused by the difference of the policy ($\bpi_t$ and $\bpi_t^{1}$). This suggests that the myopic policy makes the policy regret small, but completely ignores the population regret. 
This presents the reason why we have observed that the uniform random policy outperformed the myopic policy in the toy example presented 
in the Introduction. 