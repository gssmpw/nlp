\section{Related Work and Discussions}
\paragraph{Learning from Preferences for Language Models}


Preference learning is critical for aligning Large Language Models (LLMs) with human expectations and perceptions. Initial approaches, building on pre-training and supervised fine-tuning (SFT), employed PPO in Reinforcement Learning from Human/AI Feedback (RLHF/RLAIF) frameworks \citep{PPO,christiano2017deep,ouyang2022training,MCTSDPO}. These approaches typically involve training a reward model on preference pairs and subsequently optimizing the LLM to maximize the learned reward. However, PPO's instability and inefficiency motivated alternative approaches like DPO \citep{DPO}, which directly optimizes a policy from paired preference data.
Subsequent research has addressed various challenges. ORPO \citep{ORPO} integrates alignment into SFT, KTO \citep{KTO} leverages pointwise data, simplifying data acquisition process. Other efforts focus on finer-grained optimization, such as Step-DPO \citep{StepDPO} and Cross-DPO \citep{yang2024supercorrect} that targets intermediate reasoning or reflection steps. SPO \citep{MinMaxPPO} employs game-theoretic concepts to address non-transitive preferences, while Multi-turn DPO \citep{DMPO} extends optimization to conversations. 
% Synthetic data generation, exemplified by MCTS-DPO \citep{MCTSDPO}, offers another avenue for improvement. 
However, existing methods often rely on instance or step-level reward units, potentially failing to capture and reward the higher-level cognitive processes inherent in human problem-solving process. 
To this end, we introduce hierarchical RL-based optimization, a novel preference learning approach that encourages the model to configure a series of high-level thought templates that can handle diverse sub-tasks for complex problems, thereby promoting more human-like problem-solving strategies in LLMs.


\paragraph{Retrieval-Augmented Generation for Language Models}
% % Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models
Retrieval-augmented Language Models (RALMs) have become a powerful approach to mitigating hallucinations and enhancing the factual accuracy of LLMs \citep{asai2023retrieval,mialon2023augmented,shi2023replug,gao2023retrieval,zhao2024retrieval}. By retrieving relevant documents from a large-scale external knowledge source \citep{borgeaud2022improving} to inform response generation, RALMs have demonstrated superior performance in question-answering, often with fewer parameters than traditional LLMs \citep{mialon2023augmented}. Their versatility is further evidenced by successful applications across diverse tasks, including multi-modal generation and biomedical applications \citep{yasunaga2023retrieval,izacard2023atlas,wang2022retrieval,zhao2024retrieval,borgeaud2022improving,yang2023prompt}.  However, RALMs face challenges in complex reasoning tasks, such as math and code, where retrieving relevant guidelines or templates via standard embedding similarity search proves difficult. While methods like RAFT \citep{RAFT} have attempted to address this by improving retrieval relevance, respectively, their effectiveness decrease as the document size grows.  To overcome these limitations, we design a structured and compact template library for efficient and accurate retrieval, specifically targeting complex reasoning problems.




% \begin{figure*}[tp]
%     \centering
    
%     \includegraphics[width=\textwidth]{figs/training.pdf}
%     \vspace{-0.3in}
%     \caption{Training framework for \method.}
%     \vspace{-0.2in}
%     \label{pic-training}
% \end{figure*}

\begin{figure*}[tp]
    \centering
    \includegraphics[width=\textwidth]{figs/inference.pdf}
    % \vspace{-0.3in}
    \caption{\textbf{New inference scaling system based on hierarchical reasoning.} We retrieve a series of high-level thought templates for complex problems, and gradually conduct instantiated reasoning for a sequence of sub-problems. }
    \vspace{-0.2in}
    \label{pic-inference}
\end{figure*}


\paragraph{Inference Scaling for LLM Reasoning}





The auto-regressive nature of LLMs suggests that solving more complex problems inherently requires generating more tokens. Early work, such as CoT \citep{CoT}, used prompting techniques like "Let's think step by step" to break down complex reasoning tasks into simpler sub-problems, thus enhancing reasoning performance. Building on this, ToT \citep{ToT} and GoT \citep{GoT} employed different data structures to expand the reasoning space, allowing LLMs to explore multiple solution paths. Recent research \citep{inferenceScaling,snell2024scaling} has formalized the concept of inference scaling laws, which examine the trade-offs between the generation of additional tokens, and the use of various inference strategies. For instance, majority voting and best-of-N methods \citep{wang2022self,li2023making} generate multiple candidate solutions and select the best based on frequency among all the results or the reward model's evaluation.  Similarly, approaches using Monte Carlo Tree Search (MCTS) \citep{zhang2023planning,liu2024don,choi2023kcts,zhou2023language} leverage greater search and computation to improve accuracy. To enhance search accuracy, Process Reward Models (PRMs) have been introduced to select high-quality reasoning paths, with studies \citep{setlur2024rewarding,snell2024scaling,lightman2023let,luo2024improve,wang2024math} demonstrating their effectiveness, particularly in complex reasoning tasks. 
More recently, methods like BoT \citep{BoT} utilize thought templates from past reasoning processes to guide exploration, significantly improving efficiency. However, a deeper understanding of the exploration-exploitation trade-off \citep{tang2024code,setlur2024rewarding} for these template-based approaches remains an open challenge. Our work addresses this challenge by scaling an hierarchical template-augmented reasoning paradigm that significantly enhances reasoning accuracy, especially for complex tasks, while strategically balancing exploration and exploitation.
%%% Inference scaling

% The auto-regressive nature of LLMs implies that solving complex problems inherently requires generating more tokens. Early work like Chain-of-Thought (CoT) \citep{CoT} demonstrated that prompting techniques, such as "Let's think step by step," can decompose complex tasks into simpler sub-problems, improving reasoning performance. Subsequent methods like Tree-of-Thoughts (ToT) \citep{ToT} and Graph-of-Thoughts (GoT) \citep{GoT} expanded the reasoning space by exploring alternative data structures, allowing LLMs to consider multiple solution paths.  These approaches implicitly touch upon the concept of *inference scaling laws*, which formally characterize the relationship between computational resources (including generated tokens) and performance gains during inference.** Recent work \citep{inferenceScaling} has explicitly brought these scaling laws to the forefront, analyzing trade-offs between model size, inference strategies, and the generation of additional tokens. For instance, methods employing Monte Carlo Tree Search (MCTS) \citep{zhang2023planning,liu2024don,choi2023kcts,zhou2023language}, majority voting, and best-of-N strategies \citep{wang2022self,li2023making} directly manipulate the inference process to improve accuracy, often at the cost of increased computation. While these methods explore the inference landscape, they often do so in a relatively unstructured manner.  More recently, approaches like the Bag of Thoughts (BoT) \citep{BoT} have introduced the use of templates extracted from past reasoning, guiding exploration more effectively and improving efficiency. **However, optimizing the exploration-exploitation trade-off within these template-guided inference strategies, a crucial aspect of inference scaling, remains an open challenge.** Our work addresses this gap by proposing an automated template-augmented reasoning paradigm that aims to enhance reasoning accuracy, particularly for complex tasks, while strategically balancing exploration and exploitation, contributing to a deeper understanding of inference scaling laws for template-based reasoning.