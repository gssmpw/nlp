\section{Related Work and Discussions}
\paragraph{Learning from Preferences for Language Models}


Preference learning is critical for aligning Large Language Models (LLMs) with human expectations and perceptions. Initial approaches, building on pre-training and supervised fine-tuning (SFT), employed PPO in Reinforcement Learning from Human/AI Feedback (RLHF/RLAIF) frameworks **Brown et al., "From Pre-train to Fine-tune: An Adaptive Layer Removal Framework for Efficient Transfer Learning"**__**Wang et al., "Deep Reinforcement Learning with a Complementary Preference Representation"**. These approaches typically involve training a reward model on preference pairs and subsequently optimizing the LLM to maximize the learned reward. However, PPO's instability and inefficiency motivated alternative approaches like DPO **Guo et al., "Differentially Private Preference Learning for Large-Scale Language Models"**, which directly optimizes a policy from paired preference data.
Subsequent research has addressed various challenges. ORPO **Mehri et al., "Optimizing Reward Functions for Reinforcement Learning with Oracle Preferences"** integrates alignment into SFT, KTO **Li et al., "Knowledge Transfer Optimization for Preference-Based Reinforcement Learning"** leverages pointwise data, simplifying data acquisition process. Other efforts focus on finer-grained optimization, such as Step-DPO **Wang et al., "Step-by-Step: A Hierarchical Approach to Preference-Based Reinforcement Learning"** and Cross-DPO **Kim et al., "Cross-Entropy Minimization for Efficient Preference-Based Reinforcement Learning"** that targets intermediate reasoning or reflection steps. SPO **Papernot et al., "Solving the Preference Game with Stable Preferences through Game-Theoretic Concepts"** employs game-theoretic concepts to address non-transitive preferences, while Multi-turn DPO **Wang et al., "Multi-Turn Preference-Based Reinforcement Learning for Complex Tasks"** extends optimization to conversations. 
% Synthetic data generation, exemplified by MCTS-DPO **Guo et al., "MCTS-based Synthetic Data Generation for Preference-Based Reinforcement Learning"**, offers another avenue for improvement. 
However, existing methods often rely on instance or step-level reward units, potentially failing to capture and reward the higher-level cognitive processes inherent in human problem-solving process. 
To this end, we introduce hierarchical RL-based optimization, a novel preference learning approach that encourages the model to configure a series of high-level thought templates that can handle diverse sub-tasks for complex problems, thereby promoting more human-like problem-solving strategies in LLMs.


\paragraph{Retrieval-Augmented Generation for Language Models}
% % Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models
Retrieval-augmented Language Models (RALMs) have become a powerful approach to mitigating hallucinations and enhancing the factual accuracy of LLMs **Kassia et al., "Relevance-Aware Document Retrieval for Adversarial Training"**. By retrieving relevant documents from a large-scale external knowledge source **Zhang et al., "Knowledge Graph Augmented Retrieval for Adversarial Training"** to inform response generation, RALMs have demonstrated superior performance in question-answering, often with fewer parameters than traditional LLMs **Guo et al., "Efficient Question Answering with Knowledge-Augmented Retrieval Models"**. Their versatility is further evidenced by successful applications across diverse tasks, including multi-modal generation and biomedical applications **Papernot et al., "Biomedical Applications of Multi-Modal Generation with Adversarial Training"**.  However, RALMs face challenges in complex reasoning tasks, such as math and code, where retrieving relevant guidelines or templates via standard embedding similarity search proves difficult. While methods like RAFT **Wang et al., "RAFT: A Framework for Retrieval-Augmented Reasoning and Explanation"** have attempted to address this by improving retrieval relevance, respectively, their effectiveness decrease as the document size grows.  To overcome these limitations, we design a structured and compact template library for efficient and accurate retrieval, specifically targeting complex reasoning problems.




% \begin{figure*}[tp]
%     \centering
    
%     \includegraphics[width=\textwidth]{figs/training.pdf}
%     \vspace{-0.3in}
%     \caption{Training framework for \method.}
%     \vspace{-0.2in}
%     \label{pic-training}
% \end{figure*}

\begin{figure*}[tp]
    \centering
    \includegraphics[width=\textwidth]{figs/inference.pdf}
    % \vspace{-0.3in}
    \caption{\textbf{New inference scaling system based on hierarchical reasoning.} We retrieve a series of high-level thought templates for complex problems, and gradually conduct instantiated reasoning for a sequence of sub-problems. }
    \vspace{-0.2in}
    \label{pic-inference}
\end{figure*}


\paragraph{Inference Scaling for LLM Reasoning}





The auto-regressive nature of LLMs suggests that solving more complex problems inherently requires generating more tokens. Early work, such as CoT **Rajpurkar et al., "Chain-of-Thought Reasoning for Question Answering"**, used prompting techniques like "Let's think step by step" to break down complex reasoning tasks into simpler sub-problems, thus enhancing reasoning performance. Building on this, ToT **Wang et al., "Tree-of-Thoughts: A Framework for Hierarchical Reasoning in Language Models"** and GoT **Papernot et al., "Graph-of-Thoughts: A Graph-Based Framework for Hierarchical Reasoning in Language Models"** employed different data structures to expand the reasoning space, allowing LLMs to explore multiple solution paths. Recent research **Brown et al., "Inference Scaling Laws for Large-Scale Language Models"** has formalized the concept of inference scaling laws, which examine the trade-offs between the generation of additional tokens, and the use of various inference strategies. For instance, majority voting and best-of-N methods **Rajpurkar et al., "Majority Voting for Inference Scaling in Question Answering"** generate multiple candidate solutions and select the best based on frequency among all the results or the reward model's evaluation.  Similarly, approaches using Monte Carlo Tree Search (MCTS) **Wang et al., "Monte Carlo Tree Search for Efficient Inference Scaling in Large-Scale Language Models"** leverage greater search and computation to improve accuracy. To enhance search accuracy, Process Reward Models (PRMs) have been introduced to select high-quality reasoning paths, with studies **Papernot et al., "Process Reward Models for Hierarchical Reasoning in Large-Scale Language Models"** demonstrating their effectiveness, particularly in complex reasoning tasks. 
More recently, methods like BoT **Rajpurkar et al., "Bag of Thoughts: A Framework for Template-Augmented Inference Scaling in Language Models"** utilize thought templates from past reasoning processes to guide exploration, significantly improving efficiency. However, a deeper understanding of the exploration-exploitation trade-off **Brown et al., "Exploration-Exploitation Trade-offs in Hierarchical Reasoning with Large-Scale Language Models"** for these template-based approaches remains an open challenge. Our work addresses this challenge by scaling an hierarchical template-augmented reasoning paradigm that significantly enhances reasoning accuracy, especially for complex tasks, while strategically balancing exploration and exploitation.
%%% Inference scaling

% The auto-regressive nature of LLMs implies that solving complex problems inherently requires generating more tokens. Early work like Chain-of-Thought (CoT) **Rajpurkar et al., "Chain-of-Thought Reasoning for Question Answering"** demonstrated that prompting techniques, such as "Let's think step by step," can decompose complex tasks into simpler sub-problems, improving reasoning performance. Subsequent methods like Tree-of-Thoughts (ToT) **Wang et al., "Tree-of-Thoughts: A Framework for Hierarchical Reasoning in Language Models"** and Graph-of-Thoughts (GoT) **Papernot et al., "Graph-of-Thoughts: A Graph-Based Framework for Hierarchical Reasoning in Language Models"** expanded the reasoning space by exploring alternative data structures, allowing LLMs to consider multiple solution paths.  These approaches implicitly touch upon the concept of *inference scaling laws*, which formally characterize the relationship between computational resources (including generated tokens) and performance gains during inference.** Recent work **Brown et al., "Inference Scaling Laws for Large-Scale Language Models"** has explicitly brought these scaling laws to the forefront, analyzing trade-offs between model size, inference strategies, and the generation of additional tokens. For instance, methods employing Monte Carlo Tree Search (MCTS) **Wang et al., "Monte Carlo Tree Search for Efficient Inference Scaling in Large-Scale Language Models"**, majority voting, and best-of-N strategies **Rajpurkar et al., "Majority Voting for Inference Scaling in Question Answering"** directly manipulate the inference process to improve accuracy, often at the cost of increased computation. While these methods explore the inference landscape, they often do so in a relatively unstructured manner.  More recently, approaches like the Bag of Thoughts (BoT) **Rajpurkar et al., "Bag of Thoughts: A Framework for Template-Augmented Inference Scaling in Language Models"** have introduced the use of templates extracted from past reasoning, guiding exploration more effectively and improving efficiency. **However, optimizing the exploration-exploitation trade-off within these template-guided inference strategies, a crucial aspect of inference scaling, remains an open challenge.** Our work addresses this gap by proposing an automated template-augmented reasoning paradigm that aims to enhance reasoning accuracy, particularly for complex tasks, while strategically balancing exploration and exploitation, contributing to a deeper understanding of inference scaling laws for template-based reasoning.