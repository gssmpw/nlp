[
  {
    "index": 0,
    "papers": [
      {
        "key": "PPO",
        "author": "Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg",
        "title": "Proximal policy optimization algorithms"
      },
      {
        "key": "christiano2017deep",
        "author": "Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario",
        "title": "Deep reinforcement learning from human preferences"
      },
      {
        "key": "ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "key": "MCTSDPO",
        "author": "Xie, Yuxi and Goyal, Anirudh and Zheng, Wenyue and Kan, Min-Yen and Lillicrap, Timothy P and Kawaguchi, Kenji and Shieh, Michael",
        "title": "Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "DPO",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea",
        "title": "Direct preference optimization: Your language model is secretly a reward model"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "ORPO",
        "author": "Hong, Jiwoo and Lee, Noah and Thorne, James",
        "title": "Orpo: Monolithic preference optimization without reference model"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "KTO",
        "author": "Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe",
        "title": "Kto: Model alignment as prospect theoretic optimization"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "StepDPO",
        "author": "Lai, Xin and Tian, Zhuotao and Chen, Yukang and Yang, Senqiao and Peng, Xiangru and Jia, Jiaya",
        "title": "Step-dpo: Step-wise preference optimization for long-chain reasoning of llms"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "yang2024supercorrect",
        "author": "Yang, Ling and Yu, Zhaochen and Zhang, Tianjun and Xu, Minkai and Gonzalez, Joseph E and Cui, Bin and Yan, Shuicheng",
        "title": "Supercorrect: Supervising and correcting language models with error-driven insights"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "MinMaxPPO",
        "author": "Swamy, Gokul and Dann, Christoph and Kidambi, Rahul and Wu, Zhiwei Steven and Agarwal, Alekh",
        "title": "A minimaximalist approach to reinforcement learning from human feedback"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "DMPO",
        "author": "Shi, Wentao and Yuan, Mengqi and Wu, Junkang and Wang, Qifan and Feng, Fuli",
        "title": "Direct Multi-Turn Preference Optimization for Language Agents"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "MCTSDPO",
        "author": "Xie, Yuxi and Goyal, Anirudh and Zheng, Wenyue and Kan, Min-Yen and Lillicrap, Timothy P and Kawaguchi, Kenji and Shieh, Michael",
        "title": "Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "asai2023retrieval",
        "author": "Asai, Akari and Min, Sewon and Zhong, Zexuan and Chen, Danqi",
        "title": "Retrieval-based language models and applications"
      },
      {
        "key": "mialon2023augmented",
        "author": "Mialon, Gr{\\'e}goire and Dessi, Roberto and Lomeli, Maria and Nalmpantis, Christoforos and Pasunuru, Ramakanth and Raileanu, Roberta and Roziere, Baptiste and Schick, Timo and Dwivedi-Yu, Jane and Celikyilmaz, Asli and others",
        "title": "Augmented Language Models: a Survey"
      },
      {
        "key": "shi2023replug",
        "author": "Shi, Weijia and Min, Sewon and Yasunaga, Michihiro and Seo, Minjoon and James, Rich and Lewis, Mike and Zettlemoyer, Luke and Yih, Wen-tau",
        "title": "Replug: Retrieval-augmented black-box language models"
      },
      {
        "key": "gao2023retrieval",
        "author": "Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Haofen",
        "title": "Retrieval-augmented generation for large language models: A survey"
      },
      {
        "key": "zhao2024retrieval",
        "author": "Zhao, Penghao and Zhang, Hailin and Yu, Qinhan and Wang, Zhengren and Geng, Yunteng and Fu, Fangcheng and Yang, Ling and Zhang, Wentao and Cui, Bin",
        "title": "Retrieval-Augmented Generation for AI-Generated Content: A Survey"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "borgeaud2022improving",
        "author": "Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George Bm and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and others",
        "title": "Improving language models by retrieving from trillions of tokens"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "mialon2023augmented",
        "author": "Mialon, Gr{\\'e}goire and Dessi, Roberto and Lomeli, Maria and Nalmpantis, Christoforos and Pasunuru, Ramakanth and Raileanu, Roberta and Roziere, Baptiste and Schick, Timo and Dwivedi-Yu, Jane and Celikyilmaz, Asli and others",
        "title": "Augmented Language Models: a Survey"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "yasunaga2023retrieval",
        "author": "Yasunaga, Michihiro and Aghajanyan, Armen and Shi, Weijia and James, Richard and Leskovec, Jure and Liang, Percy and Lewis, Mike and Zettlemoyer, Luke and Yih, Wen-Tau",
        "title": "Retrieval-Augmented Multimodal Language Modeling"
      },
      {
        "key": "izacard2023atlas",
        "author": "Izacard, Gautier and Lewis, Patrick and Lomeli, Maria and Hosseini, Lucas and Petroni, Fabio and Schick, Timo and Dwivedi-Yu, Jane and Joulin, Armand and Riedel, Sebastian and Grave, Edouard",
        "title": "Atlas: Few-shot Learning with Retrieval Augmented Language Models"
      },
      {
        "key": "wang2022retrieval",
        "author": "Wang, Zichao and Nie, Weili and Qiao, Zhuoran and Xiao, Chaowei and Baraniuk, Richard and Anandkumar, Anima",
        "title": "Retrieval-based Controllable Molecule Generation"
      },
      {
        "key": "zhao2024retrieval",
        "author": "Zhao, Penghao and Zhang, Hailin and Yu, Qinhan and Wang, Zhengren and Geng, Yunteng and Fu, Fangcheng and Yang, Ling and Zhang, Wentao and Cui, Bin",
        "title": "Retrieval-Augmented Generation for AI-Generated Content: A Survey"
      },
      {
        "key": "borgeaud2022improving",
        "author": "Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George Bm and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and others",
        "title": "Improving language models by retrieving from trillions of tokens"
      },
      {
        "key": "yang2023prompt",
        "author": "Yang, Ling and Huang, Zhilin and Zhou, Xiangxin and Xu, Minkai and Zhang, Wentao and Wang, Yu and Zheng, Xiawu and Yang, Wenming and Dror, Ron O and Hong, Shenda and others",
        "title": "Prompt-based 3d molecular diffusion models for structure-based drug design"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "RAFT",
        "author": "Zhang, Tianjun and Patil, Shishir G and Jain, Naman and Shen, Sheng and Zaharia, Matei and Stoica, Ion and Gonzalez, Joseph E",
        "title": "Raft: Adapting language model to domain specific rag"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "CoT",
        "author": "Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others",
        "title": "Chain-of-thought prompting elicits reasoning in large language models"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "ToT",
        "author": "Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik",
        "title": "Tree of thoughts: Deliberate problem solving with large language models"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "GoT",
        "author": "Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Podstawski, Michal and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Niewiadomski, Hubert and Nyczyk, Piotr and others",
        "title": "Graph of thoughts: Solving elaborate problems with large language models"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "inferenceScaling",
        "author": "Wu, Yangzhen and Sun, Zhiqing and Li, Shanda and Welleck, Sean and Yang, Yiming",
        "title": "Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models"
      },
      {
        "key": "snell2024scaling",
        "author": "Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral",
        "title": "Scaling llm test-time compute optimally can be more effective than scaling model parameters"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "wang2022self",
        "author": "Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny",
        "title": "Self-consistency improves chain of thought reasoning in language models"
      },
      {
        "key": "li2023making",
        "author": "Li, Yifei and Lin, Zeqi and Zhang, Shizhuo and Fu, Qiang and Chen, Bei and Lou, Jian-Guang and Chen, Weizhu",
        "title": "Making language models better reasoners with step-aware verifier"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "zhang2023planning",
        "author": "Zhang, Shun and Chen, Zhenfang and Shen, Yikang and Ding, Mingyu and Tenenbaum, Joshua B and Gan, Chuang",
        "title": "Planning with large language models for code generation"
      },
      {
        "key": "liu2024don",
        "author": "Liu, Jiacheng and Cohen, Andrew and Pasunuru, Ramakanth and Choi, Yejin and Hajishirzi, Hannaneh and Celikyilmaz, Asli",
        "title": "Don't throw away your value model! Generating more preferable text with Value-Guided Monte-Carlo Tree Search decoding"
      },
      {
        "key": "choi2023kcts",
        "author": "Choi, Sehyun and Fang, Tianqing and Wang, Zhaowei and Song, Yangqiu",
        "title": "KCTS: knowledge-constrained tree search decoding with token-level hallucination detection"
      },
      {
        "key": "zhou2023language",
        "author": "Zhou, Andy and Yan, Kai and Shlapentokh-Rothman, Michal and Wang, Haohan and Wang, Yu-Xiong",
        "title": "Language agent tree search unifies reasoning acting and planning in language models"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "setlur2024rewarding",
        "author": "Setlur, Amrith and Nagpal, Chirag and Fisch, Adam and Geng, Xinyang and Eisenstein, Jacob and Agarwal, Rishabh and Agarwal, Alekh and Berant, Jonathan and Kumar, Aviral",
        "title": "Rewarding progress: Scaling automated process verifiers for llm reasoning"
      },
      {
        "key": "snell2024scaling",
        "author": "Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral",
        "title": "Scaling llm test-time compute optimally can be more effective than scaling model parameters"
      },
      {
        "key": "lightman2023let",
        "author": "Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl",
        "title": "Let's verify step by step"
      },
      {
        "key": "luo2024improve",
        "author": "Luo, Liangchen and Liu, Yinxiao and Liu, Rosanne and Phatale, Samrat and Lara, Harsh and Li, Yunxuan and Shu, Lei and Zhu, Yun and Meng, Lei and Sun, Jiao and others",
        "title": "Improve Mathematical Reasoning in Language Models by Automated Process Supervision"
      },
      {
        "key": "wang2024math",
        "author": "Wang, Peiyi and Li, Lei and Shao, Zhihong and Xu, Runxin and Dai, Damai and Li, Yifei and Chen, Deli and Wu, Yu and Sui, Zhifang",
        "title": "Math-shepherd: Verify and reinforce llms step-by-step without human annotations"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "BoT",
        "author": "Yang, Ling and Yu, Zhaochen and Zhang, Tianjun and Cao, Shiyi and Xu, Minkai and Zhang, Wentao and Gonzalez, Joseph E and Cui, Bin",
        "title": "Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "tang2024code",
        "author": "Tang, Hao and Hu, Keya and Zhou, Jin Peng and Zhong, Sicheng and Zheng, Wei-Long and Si, Xujie and Ellis, Kevin",
        "title": "Code Repair with LLMs gives an Exploration-Exploitation Tradeoff"
      },
      {
        "key": "setlur2024rewarding",
        "author": "Setlur, Amrith and Nagpal, Chirag and Fisch, Adam and Geng, Xinyang and Eisenstein, Jacob and Agarwal, Rishabh and Agarwal, Alekh and Berant, Jonathan and Kumar, Aviral",
        "title": "Rewarding progress: Scaling automated process verifiers for llm reasoning"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "CoT",
        "author": "Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others",
        "title": "Chain-of-thought prompting elicits reasoning in large language models"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "ToT",
        "author": "Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik",
        "title": "Tree of thoughts: Deliberate problem solving with large language models"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "GoT",
        "author": "Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Podstawski, Michal and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Niewiadomski, Hubert and Nyczyk, Piotr and others",
        "title": "Graph of thoughts: Solving elaborate problems with large language models"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "inferenceScaling",
        "author": "Wu, Yangzhen and Sun, Zhiqing and Li, Shanda and Welleck, Sean and Yang, Yiming",
        "title": "Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "zhang2023planning",
        "author": "Zhang, Shun and Chen, Zhenfang and Shen, Yikang and Ding, Mingyu and Tenenbaum, Joshua B and Gan, Chuang",
        "title": "Planning with large language models for code generation"
      },
      {
        "key": "liu2024don",
        "author": "Liu, Jiacheng and Cohen, Andrew and Pasunuru, Ramakanth and Choi, Yejin and Hajishirzi, Hannaneh and Celikyilmaz, Asli",
        "title": "Don't throw away your value model! Generating more preferable text with Value-Guided Monte-Carlo Tree Search decoding"
      },
      {
        "key": "choi2023kcts",
        "author": "Choi, Sehyun and Fang, Tianqing and Wang, Zhaowei and Song, Yangqiu",
        "title": "KCTS: knowledge-constrained tree search decoding with token-level hallucination detection"
      },
      {
        "key": "zhou2023language",
        "author": "Zhou, Andy and Yan, Kai and Shlapentokh-Rothman, Michal and Wang, Haohan and Wang, Yu-Xiong",
        "title": "Language agent tree search unifies reasoning acting and planning in language models"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "wang2022self",
        "author": "Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny",
        "title": "Self-consistency improves chain of thought reasoning in language models"
      },
      {
        "key": "li2023making",
        "author": "Li, Yifei and Lin, Zeqi and Zhang, Shizhuo and Fu, Qiang and Chen, Bei and Lou, Jian-Guang and Chen, Weizhu",
        "title": "Making language models better reasoners with step-aware verifier"
      }
    ]
  },
  {
    "index": 29,
    "papers": [
      {
        "key": "BoT",
        "author": "Yang, Ling and Yu, Zhaochen and Zhang, Tianjun and Cao, Shiyi and Xu, Minkai and Zhang, Wentao and Gonzalez, Joseph E and Cui, Bin",
        "title": "Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models"
      }
    ]
  }
]