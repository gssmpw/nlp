\section{Related Work and Discussions}
\paragraph{Learning from Preferences for Language Models}


Preference learning is critical for aligning Large Language Models (LLMs) with human expectations and perceptions. Initial approaches, building on pre-training and supervised fine-tuning (SFT), employed PPO in Reinforcement Learning from Human/AI Feedback (RLHF/RLAIF) frameworks ____. These approaches typically involve training a reward model on preference pairs and subsequently optimizing the LLM to maximize the learned reward. However, PPO's instability and inefficiency motivated alternative approaches like DPO ____, which directly optimizes a policy from paired preference data.
Subsequent research has addressed various challenges. ORPO ____ integrates alignment into SFT, KTO ____ leverages pointwise data, simplifying data acquisition process. Other efforts focus on finer-grained optimization, such as Step-DPO ____ and Cross-DPO ____ that targets intermediate reasoning or reflection steps. SPO ____ employs game-theoretic concepts to address non-transitive preferences, while Multi-turn DPO ____ extends optimization to conversations. 
% Synthetic data generation, exemplified by MCTS-DPO ____, offers another avenue for improvement. 
However, existing methods often rely on instance or step-level reward units, potentially failing to capture and reward the higher-level cognitive processes inherent in human problem-solving process. 
To this end, we introduce hierarchical RL-based optimization, a novel preference learning approach that encourages the model to configure a series of high-level thought templates that can handle diverse sub-tasks for complex problems, thereby promoting more human-like problem-solving strategies in LLMs.


\paragraph{Retrieval-Augmented Generation for Language Models}
% % Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models
Retrieval-augmented Language Models (RALMs) have become a powerful approach to mitigating hallucinations and enhancing the factual accuracy of LLMs ____. By retrieving relevant documents from a large-scale external knowledge source ____ to inform response generation, RALMs have demonstrated superior performance in question-answering, often with fewer parameters than traditional LLMs ____. Their versatility is further evidenced by successful applications across diverse tasks, including multi-modal generation and biomedical applications ____.  However, RALMs face challenges in complex reasoning tasks, such as math and code, where retrieving relevant guidelines or templates via standard embedding similarity search proves difficult. While methods like RAFT ____ have attempted to address this by improving retrieval relevance, respectively, their effectiveness decrease as the document size grows.  To overcome these limitations, we design a structured and compact template library for efficient and accurate retrieval, specifically targeting complex reasoning problems.




% \begin{figure*}[tp]
%     \centering
    
%     \includegraphics[width=\textwidth]{figs/training.pdf}
%     \vspace{-0.3in}
%     \caption{Training framework for \method.}
%     \vspace{-0.2in}
%     \label{pic-training}
% \end{figure*}

\begin{figure*}[tp]
    \centering
    \includegraphics[width=\textwidth]{figs/inference.pdf}
    % \vspace{-0.3in}
    \caption{\textbf{New inference scaling system based on hierarchical reasoning.} We retrieve a series of high-level thought templates for complex problems, and gradually conduct instantiated reasoning for a sequence of sub-problems. }
    \vspace{-0.2in}
    \label{pic-inference}
\end{figure*}


\paragraph{Inference Scaling for LLM Reasoning}





The auto-regressive nature of LLMs suggests that solving more complex problems inherently requires generating more tokens. Early work, such as CoT ____, used prompting techniques like "Let's think step by step" to break down complex reasoning tasks into simpler sub-problems, thus enhancing reasoning performance. Building on this, ToT ____ and GoT ____ employed different data structures to expand the reasoning space, allowing LLMs to explore multiple solution paths. Recent research ____ has formalized the concept of inference scaling laws, which examine the trade-offs between the generation of additional tokens, and the use of various inference strategies. For instance, majority voting and best-of-N methods ____ generate multiple candidate solutions and select the best based on frequency among all the results or the reward model's evaluation.  Similarly, approaches using Monte Carlo Tree Search (MCTS) ____ leverage greater search and computation to improve accuracy. To enhance search accuracy, Process Reward Models (PRMs) have been introduced to select high-quality reasoning paths, with studies ____ demonstrating their effectiveness, particularly in complex reasoning tasks. 
More recently, methods like BoT ____ utilize thought templates from past reasoning processes to guide exploration, significantly improving efficiency. However, a deeper understanding of the exploration-exploitation trade-off ____ for these template-based approaches remains an open challenge. Our work addresses this challenge by scaling an hierarchical template-augmented reasoning paradigm that significantly enhances reasoning accuracy, especially for complex tasks, while strategically balancing exploration and exploitation.
%%% Inference scaling

% The auto-regressive nature of LLMs implies that solving complex problems inherently requires generating more tokens. Early work like Chain-of-Thought (CoT) ____ demonstrated that prompting techniques, such as "Let's think step by step," can decompose complex tasks into simpler sub-problems, improving reasoning performance. Subsequent methods like Tree-of-Thoughts (ToT) ____ and Graph-of-Thoughts (GoT) ____ expanded the reasoning space by exploring alternative data structures, allowing LLMs to consider multiple solution paths.  These approaches implicitly touch upon the concept of *inference scaling laws*, which formally characterize the relationship between computational resources (including generated tokens) and performance gains during inference.** Recent work ____ has explicitly brought these scaling laws to the forefront, analyzing trade-offs between model size, inference strategies, and the generation of additional tokens. For instance, methods employing Monte Carlo Tree Search (MCTS) ____, majority voting, and best-of-N strategies ____ directly manipulate the inference process to improve accuracy, often at the cost of increased computation. While these methods explore the inference landscape, they often do so in a relatively unstructured manner.  More recently, approaches like the Bag of Thoughts (BoT) ____ have introduced the use of templates extracted from past reasoning, guiding exploration more effectively and improving efficiency. **However, optimizing the exploration-exploitation trade-off within these template-guided inference strategies, a crucial aspect of inference scaling, remains an open challenge.** Our work addresses this gap by proposing an automated template-augmented reasoning paradigm that aims to enhance reasoning accuracy, particularly for complex tasks, while strategically balancing exploration and exploitation, contributing to a deeper understanding of inference scaling laws for template-based reasoning.