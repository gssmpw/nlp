\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{stfloats}
\usepackage{lipsum}
\usepackage{amsfonts}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{doi}
\usepackage{pifont}
\usepackage{etoolbox}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

\begin{document}

\title{Towards Consistent and Controllable Image Synthesis for Face Editing}

\author{Mengting Wei, 
      Tuomas Varanka,
      Yante Li,
      Xingxun Jiang,
      Huai-Qian Khor,
      Guoying Zhao$^{\ast}$,~\IEEEmembership{Fellow,~IEEE}% <-this % stops a space

        % <-this % stops a space
   \thanks{M. Wei, T. Varanka, H. Khor and G. Zhao are with the Center for Machine Vision and Signal Analysis, Faculty of Information Technology and Electrical Engineering, University of Oulu, Oulu, FI-90014, Finland. E-mail: mengting.wei@oulu.fi, tuomas.varanka@student.oulu.fi, yante.li@oulu.fi, huai.khor@oulu.fi, guoying.zhao@oulu.fi.}
   \thanks{X. Jiang is with the Key Laboratory of Child Development and Learning Science of Ministry of Education, School of Biological Sciences and Medical Engineering, Southeast University, Nanjing 210096, China, and is also with the Center for Machine Vision and Signal Analysis, Faulty of Information Technology and Electrical Engineering, University of Oulu, Oulu, FI-90014, Finland (e-mail:jiangxingxun@seu.edu.cn).}
   \thanks{*Corresponding author}}

% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

\IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

\begin{abstract}
Face editing methods, essential for tasks like virtual avatars, digital human synthesis and identity preservation, have traditionally been built upon GAN-based techniques, while recent focus has shifted to diffusion-based models due to their success in image reconstruction. However, diffusion models still face challenges in controlling specific attributes and preserving the consistency of other unchanged attributes especially the identity characteristics. To address these issues and facilitate more convenient editing of face images, we propose a novel approach that leverages the power of Stable-Diffusion (SD) models and crude 3D face models to control the lighting, facial expression and head pose of a portrait photo. We observe that this task essentially involves the combinations of target background, identity and face attributes aimed to edit. We strive to sufficiently disentangle the control of these factors to enable consistency of face editing. Specifically, our method, coined as RigFace, contains: 1) A Spatial Attribute Encoder that provides presise and decoupled conditions of background, pose, expression and lighting; 2) A high-consistency FaceFusion method that transfers identity features from the Identity Encoder to the denoising UNet of a pre-trained SD model; 3) An Attribute Rigger that injects those conditions into the denoising UNet. Our model achieves comparable or even superior performance in both identity preservation and photorealism compared to existing face editing models. Code is publicly available at https://github.com/weimengting/RigFace.
\end{abstract}

\begin{IEEEkeywords}
Face editing, 3D morphable model, diffusion model
\end{IEEEkeywords}

\section{Introduction}
\label{sec:intro}

Lifelikely varying the lighting, expression, head pose and other attributes of a portrait while keeping the identity and high-frequency facial characteristics has been a topic of enduring interest in face editing. Such capabilities are essential for applications in virtual avatars, digital human synthesis, and identity-preserving facial modifications. This task requires complete disentangling and fine-grained control over identity, background, head pose and other face attributes. Current techniques for this mainly rely on GAN-based models \cite{bounareli2023hyperreenact,huang2024interactive,li2024toward}. However, the efficacy of GAN-based methods in editing real images is constrained by their dependence on GAN inversion to translate real images into a semantic latent space \cite{wang2022high,zhou2022hrinversion,cheng2022inout}. This can cause unintended change in the resulting images especially when dealing with face images that requires greater variations. 


Recent research has sought to overcome these challenges by utilizing the potent generative potential of diffusion models. However, these methods fail to efficiently preserve identity details and have not been successful in editing certain specific attributes. For instance, Diffusion Autoencoders (DAE) \cite{xiang2023denoising} has the ability to interpolate between expressions, such as from smiling to a poker face, once semantic labels are applied to determine the direction of editing. Nevertheless, it can only edit with binary semantic labels, while 3D lighting, head pose, and more flexible expressions are hard to be expressed by only two directions. Face-Adapter \cite{han2024face} instead leverages the power of large pre-trained diffusion models by introducing an adapter plugin. Although this can reduce the training cost, the generated outcomes are not satisfactory enough. Using adapters involves only a subset of parameters released, which can lead to the edited images resembling cartoons and struggling to deviate from the generative style of the original diffusion model, which are insufficiently close to lifelike images found in the real world. Some methods also proposed face editing plugins for large pre-trained diffusion models \cite{wang2024instantid,ye2023ip}. However, these approaches mainly emphasize attribute editing through text, which inevitably compromises spatial control to maintain text-driven editability. 

\begin{figure*}[t] 
\centering 
\includegraphics[width=1.0\textwidth]{figs/demo.pdf}
\caption{\textbf{Consistent and controllable face editing results given identity images.} Our approach is capable of editing arbitrary identities with new facial expression, pose and lighting, generating clear and stable results while maintaining consistency with the attributes unintended to change.}
\label{fig:demo}
\end{figure*}

\IEEEpubidadjcol

To resolve the above challenges, we are committed to developing an effective model that allows us to photorealisticly edit the head pose, lighting and facial expression of a given photo with the help of pre-trained latent diffusion models. Our method is inspired the success of works that inherit the parameters and architectures while keeping all parameters trainable of Stable-Diffusion (SD) models, which has achieved better performance in adapting knowledge from SD models to specific tasks. The design motivation of our model is twofold: (1) Fully disentangling of the lighting, background, pose and facial expressions to enable consistent and independent edit from three conditions; (2) Leveraging a large SD model by inheriting its parameters as well as architecture to enable high-quality editing results. 




Specifically, the proposed model, coined as RigFace, comprises three components: 1) Spatial Attribute Provider is designed to automatically generate 3D renderings, the mask of dilated foreground area and expression parameters, which provides disentangled and precise guidance for controlled generation of lighting, head pose and facial expressions. Additionally, this strategy mitigates potential issues that may arise when extracting only the background from the identity image, such as inconsistencies caused by variations in the target background present in the training data. 2) FaceFusion that transfers the face features from the Identity Encoder through layer-by-layer transmission of transformer blocks to the denoising UNet. The way these features transmitted to the denoising UNet significantly enhances the consistency of the identity details in the generated images. 3) Attribute Rigger integrates all the conditions from identity to maintain background consistency, collect clues about global lighting and spatial reference, and generate facial expression in harmony with the expression parameters. Our model adheres to the given conditions, and meanwhile preserves most attributes of the identity image that are not intended to change. Fig. \ref{fig:demo} showcases some examples of the results edited by our model. 

Our contributions are summarized as follows:





\begin{itemize}
  \item We introduce RigFace, a face editing model to facilitate precise control over head pose, lighting and facial expressions for a given identity. This model proficiently enables face editing, surpassing previous state-of-the-art GAN-based and diffusion-based methods.
  \item We propose an innovative Spatial Attribute Provider that separately generate 3D renderings, target dilated background, and expression parameters as the disentangled conditions. This allows the model to learn the mapping from specific conditions to edited images.
  \item We propose FaceFusion to efficiently align the identity features with the target face in the self-attention of transformer blocks without redundant warping.
  \item RigFace is powerful by inheriting the knowledge from pre-trained latent diffusion models. Releasing all parameters can unleash the full potential of the model, enabling the SD model to achieve better adaptability to specific tasks.
\end{itemize}

\section{Related Work}
\subsection{Diffusion Models for Image Generation}
Diffusion models have recently achieved a significant breakthrough in visual synthesis. Since their initial development in 2015 \cite{sohl2015deep}, diffusion models have seen major improvements in the quality and variety of their outputs, propelled by advancements in training and sampling methods. As a result, these models are now extensively used across various applications and modalities, including the synthesis of images, videos, audio, and text. The Latent Diffusion Model \cite{rombach2022high} introduces a method to denoise in the latent space, effectively balancing efficiency with performance to cut down on computational demands. ControlNet \cite{zhang2023adding} and T2I-Adapter \cite{mou2024t2i} explore enhanced control over visual generation by integrating extra encoding layers, which assist in the controlled production of images under specific conditions like pose, mask, edge, and depth. However, these methods fail to retain the details of identity conditions, which is one of the main challenges we will resolve in this work.



\subsection{Facial Appearance Editing}
Conditional generation and editing is a key research area that focuses on guiding generative models through various modalities, including text, segmentation masks, and audio. Notable methods such as StyleCLIP \cite{patashnik2021styleclip} and DiffusionCLIP \cite{kim2022diffusionclip} have demonstrated impressive results in text-driven face generation and editing. Beyond text-based approaches, face editing tasks like face swapping and reenactment \cite{han2024face,ding2023diffusionrig,bounareli2023hyperreenact,zeng2023face} often leverage 3D Morphable Face Models (3DMM) as conditioning inputs due to their well-defined and disentangled parameter space for representing 3D facial geometry \cite{blanz2023morphable}. Common 3DMMs, such as FLAME \cite{li2017learning} and BFM \cite{paysan20093d}, enable precise control over shape, pose, and expressions. However, while these models provide a compact and physically meaningful representation of facial structure, they lack the ability to model appearance of face images. To bridge this gap, some image reconstruction methods integrate 3DMMs with Lambertian reflectance and Spherical Harmonics (SH) lighting to capture facial appearance. Trained on large-scale datasets, these models can infer albedo, SH lighting, and FLAME parameters from a single portrait image, enhancing realism and editability \cite{deng2019accurate,feng2021learning}. Inspired by these works, we employ 3D reconstruction to provide our model with precise conditioning on pose, expression and lighting, enabling accurate and controllable edits.
 


\subsection{Personalization of Pretrained Diffusion Models}
Personalization refers to adapting pre-trained T2I diffusion models to suit specific tasks. Earlier approaches \cite{gal2022image,ruiz2023dreambooth} achieved this by employing optimization or fine-tuning techniques. Later studies \cite{gao2024adaptive,yan2023towards,huang2024detail,que2024denoising,papa2024d4d,kang2024image} introduced coarse spatial control, enabling multi-subject generation and regional attribute editing through text. However, these methods often require extensive fine-tuning of the majority of model parameters. More recent methods, such as IP-adapter \cite{ye2023ip} and InstantID \cite{wang2024instantid}, streamline this process by fine-tuning only a limited number of parameters. Since most of the attention layer parameters in the original diffusion model are frozen, training efficiency of IP-Adapter is improved; however, the generated images tend to retain the original visual style of the stable-diffusion model. The latter method ensures strong identity preservation. However, as a tradeoff for text-based editability, InstantID offers only limited spatial control, making it less effective for handling fine movements. To solve this problem, RigFace allocates more computational resources to release all the parameters of the pre-trained diffusion model, allowing personalization to better adapt to face editing task. 

\section{Method}

\subsection{Preliminaries}

\noindent\textbf{3D Morphable Face Models.} 3D Morphable Face Models (3DMMs) are parametric models that utilize a compact and entirely disentangled latent space, either handcrafted or learned from scans, to encode attributes such as head pose, facial geometry, and expressions \cite{tu2021image,xin2023learning,zhu2024dfie3d,taha2020learned}. In this paper, we adopt FLAME and BFM, two widely used 3DMMs that employ standard vertex-based linear blend skinning with corrective blendshapes, constructing facial meshes through pose, shape, and expression parameters. While these models offer a compact and physically meaningful representation of facial geometry, it lacks the description for appearance from face images. To address this, DECA \cite{feng2021learning} builds upon FLAME by incorporating Lambertian reflectance and Spherical Harmonics (SH) lighting to model facial appearance. In our work, we leverage DECA to generate rough 3D representations, enabling flexible "rigging" of pose and lighting by modifying FLAME parameters. For facial expression rigging, we did not use FLAME because its 3D template ($\sim5,000$ vertices) lacks the ability to capture detailed facial variations. Instead, we opted for Deep3DRecon, which is based on the BFM template ($\sim35,000$ vertices) to extract expression parameters from images. As illustrated in Fig. \ref{fig:results}, Deep3DRecon indeed performs better in expressing facial expressions than DECA, and there is a noticeable realism gap between rendered images and real photos, highlighting the need for post-editing adjustments.


\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{figs/framework.pdf}
    \caption{\textbf{Overview pipeline of our proposed method} consisting three modules: 1) The Spatial Attribute Provider adapts the foreground mask and predict 3D rendering as well as expression coefficients, offering decoupled and more clear guidance for controlled generation. 2) The FaceFusion transfers spatial features of face from Identity Encoder to the denoising UNet to enable high identity consistency. 3) The Attribute Rigger integrates the image conditions into the denoising UNet to fill in the attributes aimed to edit.}
    \label{fig:framework}
\end{figure*}

\noindent\textbf{Stable Diffusion.} In this paper, we develop our method based on the recent T2I diffusion model, specifically Stable Diffusion (SD) \cite{rombach2022high}. SD is a latent diffusion model (LDM) that consists of an autoencoder and a UNet denoiser. The autoencoder encodes an image $\mathbf{x}_0$ into the latent space as $\mathbf{z}_0$, which can then be reconstructed. The diffusion process takes place within the latent space using a modified UNet denoiser. The optimization process is formally defined by the following equation:

\begin{equation}
\mathcal{L}=\mathbb{E}_{\mathbf{z}_t, \mathbf{C}, \epsilon, t}\left(\left\|\epsilon-\epsilon_\theta\left(\mathbf{z}_t, t, \mathbf{C}\right)\right\|_2^2\right)
\end{equation}

Here, $\textbf{z}_t$ represents the noised latent at timestep $t$, while $\mathbf{C}$ denotes the conditional text embedding generated by the pre-trained CLIP \cite{radford2021learning} text encoder. The function $\epsilon_\theta$ refers to the UNet denoiser. During the sampling process, the latent 
$\textbf{z}_t$ is progressively denoised from an initial random Gaussian noise using $\epsilon_\theta$, which is conditioned on both $\textbf{C}$ and $t$. Finally, the denoised latent is decoded into an image by the autoencoder’s decoder.





\subsection{Model Overview}

The structure of the proposed method is illustrated in Fig. \ref{fig:framework}, which aims to assemble identity clues with new pose, expression and lighting attributes provided by the spatial attribute provider. 

As the ground truth of the edited image is unknown, we form image pairs with the same identity but varying background, pose and expression to train. Therefore, the model's task would be to change the source image into the target image by the expression, pose or lighting conditions injected to the model. As shown in Fig. \ref{fig:framework}, $\boldsymbol{I}_{sor}$ and $\boldsymbol{I}_{tar}$ denote the identity image to be edited and the target image after editing, respectively, for training the model. In the training stage, all the conditions are parsed from the target image, while in the inference stage, the background will be parsed from the identity image. For editing under specific conditions, Spatial Attribute Provider only needs to adjust the conditions intended accordingly, while other conditions will keep consistent with the identity image for generation.




\subsection{Spatial Attribute Provider}

To provide a precise guidance for subsequent controllable generation, we design a Spatial Attribute Provider (SAP) to predict varying background area, rendering, and expression coefficients which represent decoupled pose, lighting and expression conditions.

\noindent \textbf{Expression Predictor.} We utilize Deep3DRecon \cite{deng2019accurate} to extract expression coefficients $\boldsymbol{\psi}$ of the target faces. Although this model provides other coefficients related to pose and lighting, we find that it doesn't work to change the pose and lighting if these coefficients are applied, we hypothesize this is because that pose and lighting require more intuitive and low-level guidance while coefficients are over high-level to make model understand, which we will validate in Sec. \ref{sec:abl}.


\noindent \textbf{Lighting Predictor.} As mentioned above, to provide intuitive guidance about lighting, we use the rendering image from DECA \cite{feng2021learning} which is pixel-aligned with the target face. Specifically, DECA produces the physical coefficients of lighting (spherical harmonics) from the target image. We then use the Lambertian reflectance to render these physical coefficients into the Lambertian rendering.

\noindent \textbf{Pose \& Background Predictor.} To address the issue of background changes from the source image to the target image, the model needs to be explicitly informed of current background and pose of the target. Incorporating the background as a constraint greatly reduces the complexity of the model's learning process, shifting it from generating entirely new images to performing conditional inpainting. As a result, the model becomes better adapted to maintaining background consistency and producing content that integrates smoothly with it.

Specifically, we use a pre-trained face parsing model \cite{yu2021bisenet} to obtain the head region. It is important to note that directly applying the mask of the target image's head to the identity image is impractical when no pose adjustment is required. To address this, we extract the head regions from both the source and target images, combine them into a new image, and then apply dilation to create a more adaptable area for the pose. The result image is then used to mask the foreground of the target image. This result in one of the conditions sent into Attribute Rigger in Fig. \ref{fig:framework}.


\subsection{FaceFusion}

To preserve the facial details of the identity image, we need to integrate the face information from the source image into the diffusion U-Net. Although many SD-based methods use CLIP as the reference image encoder, we find that it fails to preserve many detailed identity features, as validated in Sec. \ref{sec:abl}. Previous works \cite{hu2024animate,xu2024ootdiffusion} have demonstrated that self-attention effectively preserves reference image content through information fusion. Inspired by this, we design FaceFusion to efficiently learn the detail features of the face characteristics from the Identity Encoder. The left side of Fig. \ref{fig:framework} shows the architecture of the Identity Encoder, which is essentially identical to the denoising UNet of SD. It only inherits the weights in the SD model for initialization, and during the training all parameters are released. Given the source image $\mathbf{I}_{sor}$, a frozen VAE encoder $\mathcal{E}$ compresses the image into a low dimension feature representation and then sent it into the Indentity Encoder. The denoising UNet is also inherited from the SD model with all parameters released. As demonstrated in the FaceFusion blocks of Fig. \ref{fig:framework}, in each transformer blocks of these two models, suppose feature $\mathbf{x}_{id}\in \mathbb{R}^{h \times w \times c}$ from the Identity Encoder and $\mathbf{x}_{dn} \in \mathbb{R}^{h \times w \times c}$ from the denoising UNet, $\mathbf{x}_{id}$ is first concatenated with $\mathbf{x}_{dn}$ along the $w$ dimension. Then the self-attention layer takes in it and only half feature of the output along the $w$ dimension is kept and sent into the cross-attention layer. Note that the Identity Encoder only extracts features in the entire adding noise and denoising process, so in the inference stage it won't cause extreme increase in time cost. 



% In text-to-image tasks, the alignment between text and images is semantic, which indicates that most approaches using CLIP image encoder tend to extract high-level features of the identity. However, our task requires to keep detailed features of the identity image, so we design the identity encoder to enable consistent control.




\subsection{Attribute Rigger}

Although ControlNet \cite{zhang2023adding} has shown highly conditional generation capabilities for diffusion models, it needs a copy of the UNet which could induce a significant increase in computational complexity. Instead, we design the Attribute Rigger to be lightweight by utilizing a convolution layer with 8 channels, $4\times4$ kernels and $2\times2$ strides. To be specific, the VAE encoder $\mathcal{E}$ in SD compresses the spatial conditions into latent features and then the Attribute Rigger takes in the features and integrates them into the denoising UNet $\boldsymbol\phi_{SD}$. The expression coefficients $\boldsymbol{\psi}$ are mapped by a linear layer to have the same dimension with the time embedding of the denoising UNet and then added to the time embedding. As a result, given all the conditions and identity features, the whole framework is optimized by minmizing the following:

\begin{equation}
\mathcal{L}=\mathbb{E}_{\mathbf{z}_t, \mathbf{g}, \mathbf{y}, \boldsymbol{\psi}, \epsilon, t}[\|\epsilon-\boldsymbol\phi_{SD}(\mathbf{z}_t, t, \boldsymbol\phi_{id}(\mathbf{g}), \boldsymbol{\psi}, \boldsymbol\phi_{col}(\mathbf{y}))\|_2^2],
\end{equation}

\noindent where $\mathbf{z}_t$ is the noised latent at timestep $t$, $\mathbf{g}=\mathcal{E}(\boldsymbol{I}_{sor})$ represents the latent feature sent to the Identity Encoder, $\mathbf{y}=\mathcal{E}(\boldsymbol{C})$ represents the latent feature sent to the Attribute Rigger, $\boldsymbol{C}$ denotes conditional images, $\boldsymbol\phi_{id}$ represents the Identity Encoder, $\boldsymbol\phi_{col}$ represents the Attribute Rigger and $\epsilon$ is the predicted noise, respectively.

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=1.\textwidth]{figs/results.pdf}
    \caption{RigFace achieves convincing editing results on facial expressions, poses and lightings while preserving the individual's identity and other attributes.}
    \label{fig:results}
\end{figure*}


\section{Experiments}
\subsection{Experimental Setup}
\textbf{Dataset.} Since no dedicated dataset exists for training our task, we preprocess the Aff-Wild \cite{kollias2019deep} video dataset to meet our training requirements. Aff-Wild is a large-scale in-the-wild dataset consisting of videos with a variety of subjects in different emotional states, head poses and illumination conditions. Each video segment typically features a single individual. We randomly sample 30K images from the videos as the source image, and accordingly sampled another 30K images in the same video as the target image. During the evaluation, we sampled 20 identities from the videos and operated 3 editings for expression, pose and lighting, which leads to a number of 180 generated images. The videos used in training and test are different to ensure reliability. For preprocessing of the sampled images, we follow FOMM \cite{siarohin2019first} to crop the face, preserve partial backgrounds, and resize them into $512\times512$ to fit in the resolution of Stable-Diffusion. 


\noindent\textbf{Evaluation Criteria.} We use the ID feature similarity calculated by a pre-trained model \footnote{https://github.com/ageitgey/face\_recognition} to measure identity preservation. We use SSIM \cite{wang2004image} to evaluate the background similarity between generated and real images. Note that for fair evaluation we mask the face and only preserve the background area for evaluation of this score. For expression edit evaluation, we compute the emotion discrepancy between features from the edited and source image as in EMOCA \cite{danvevcek2022emoca}. Following the work of \cite{ding2023diffusionrig}, we use FLAME coefficients to evaluate the accuracy of edited pose and lighting. We compute the pose coefficients of FLAME \cite{li2017learning} faces and then compute Root Mean Squared Error (RMSE) for pose edit evaluation. For lighting, we use DECA to infer the spherical harmonics and then compute RMSE.

\noindent\textbf{Implementation Details.} We train our model for 100,000 steps on 2×AMD MI250x GPUs with a constant learning rate of 1e-5. The batch size on each GPU is 4. The Identity Encoder and the denoising UNet inherit Stable-Diffusion 1-5 base \footnote{https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5}. 



\begin{table}[h!]
\caption{\textbf{Quantitative comparison for face editing of facial expression, lighting and pose.} The best and second best results are reported in \textbf{bold} and [square brackets], respectively.} 
\label{tab:comp}
\begin{tabular}{lccccc}
\toprule
       Method   & ID$\downarrow$   & SSIM$\uparrow$ & Exp.$\downarrow$ & Light$\downarrow$ & Pose$\downarrow$\\
\midrule
DECA\cite{feng2021learning}           & 0.65    &0.55           &[0.11]        &\textbf{0.11}        &\textbf{0.03} \\  
GIF\cite{ghosh2020gif}                & 0.62    &0.65           &0.12          &[0.12]         &[0.05] \\
HeadNerf\cite{hong2022headnerf}       & 0.69    &0.70           &0.15          &0.23         &0.06 \\
Deep3DRecon\cite{deng2019accurate}    & 0.58    &0.77           &0.12          &0.25         &N/A \\
DiffusionRig\cite{ding2023diffusionrig}    & 0.45    &0.82      &0.13          &0.15         &[0.05] \\
FaceAdapter\cite{han2024face}           &[0.41]    &[0.88]      &\textbf{0.08}   &0.28    &[0.05] \\ 
Ours                                   &\textbf{0.34}  &\textbf{0.95}   &[0.11]   &0.14    &\textbf{0.03} \\

\bottomrule
\end{tabular}
\end{table}

\subsection{Experimental Results}
\textbf{Comparison with SoTA methods.} We compare with SoTA methods quantitatively and quantitatively on the test set, including DECA \cite{feng2021learning}, GIF \cite{ghosh2020gif}, HeadNerf \cite{hong2022headnerf}, Deep3DRecon \cite{deng2019accurate}, DiffusionRig\cite{ding2023diffusionrig} and FaceAdapter\cite{han2024face}. DECA, HeadNerf and Deep3DRecon are 3DMM-based techniques, GIF is a GAN-based technique, while DiffusionRig and FaceAdapter are diffusion-based techniques. 

Fig. \ref{fig:results} visually shows some example results of our method and other face editing methods on some identities. We observe that compared with other methods, RigFace consistently achieves the best edit effects for various individuals and backgrounds. Compared with 3DMM-based methods like DECA and HeadNerf, our method is able to preserve the background of the original image with minimal loss. Although Deep3DRecon uses rendering to reattach the face to the original background image, when the head pose is edited, filling in the background becomes a challenge that rendering alone cannot resolve. More importantly, our method accurately preserves and transfers facial details from the original image. GIF tends to change the identity and other facial attributes, despite its decent control over generating specific expressions, lighting, and poses. DiffusionRig alters certain facial features, and the overall image generation quality decreases. The face appears smoother compared with the input image. FaceAdapter performs better in preserving identity details, but some edits are entangled: it can cause expression variation when editing the lighting. In contrast, our model not only generates realistic images while preserveing most of the individual details, it also enable the decoupled editing of specific controls.

Tab. \ref{tab:comp} presents the quantitative evaluation results on different methods. We can observe that our method presents a significant improvement in preserving identity and background of the input image, as depicted by the large increase in ID and SSIM scores. Although some of the 3DMM-based methods do not show a significant performance gap in editing facial expressions and poses since they render images directly using FLAME coefficients, our method surpasses them by achieving higher identity preservation, finer detail transfer, and more precise controllability.




\begin{table}[htbp]
\centering
\caption{\textbf{Quantitative evaluation of generalization ability of RigFace.} $\text{RigFace}^{\dagger}$ denotes the pipeline is directly evaluated on the small comic dataset after being train on AffWild, and $\text{RigFace}^{\ddagger}$ represents the pipeline is further fine-tuned the small comic dataset.}
\label{tab:out}
\begin{tabular}{lccccc}
\toprule
       Method   & ID$\downarrow$   & SSIM$\uparrow$ & Exp.$\downarrow$ & Light$\downarrow$ & Pose$\downarrow$\\
\midrule
$\text{RigFace}^{\dagger}$    & 0.43              &\textbf{0.95}                 &0.16            &0.22                        &\textbf{0.05} \\  
$\text{RigFace}^{\ddagger}$   & \textbf{0.40}    &\textbf{0.95}                 &\textbf{0.13}          &\textbf{0.17}         &\textbf{0.05} \\

\bottomrule
\end{tabular}
\end{table}


\noindent\textbf{Generalization Ability.} It is also worth highlighting that RigFace can generalize to out-of-domain identity images of unseen styles with unexpectedly strong control over appearance, even without additional fine-tuning on the target domain. Fig \ref{fig:dom}. compares the zero-shot results of applying HeadNerf \cite{hong2022headnerf}, DiffusionRig \cite{ding2023diffusionrig}, FaceAdapter \cite{han2024face} and RigFace to out-of-domain identities, whose visual style is distinct from corresponding training data of the real-human face images. We observe that most of existing methods are trained on real-human datasets, so we test our method on more comic characters. The comparison results show that RigFace generalizes surprisingly well to new character images even though it has never been trained on such data. Furthermore, we include more comic images as a small dataset which contains 1000 images by collecting them on a website \footnote{https://fi.pinterest.com/} to fine-tune our model, which we dub the small dataset as \textbf{Comic-S}. As shown in Tab. \ref{tab:out}, higher generation quality can also be achieved through fine-tuning on specific datasets, further enhancing identity consistency and editability based on the zero-shot capability.


\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.98\textwidth]{figs/out_domain.pdf}
    \caption{\textbf{Comparison of zero-shot pose, expression and lighting editing on out-of-domain images.} Our method faithfully reconstructs the background and facial details.}
    \label{fig:dom}
\end{figure*}


\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.98\textwidth]{figs/reena.pdf}
    \caption{\textbf{Face reenactment qualitative comparison results.} The top two rows represent the same-identity reenactment results, and the bottom two rows represent the cross-identity reenactment results.}
    \label{fig:ree}
\end{figure*}

\begin{table}[h!]
\centering
\caption{\textbf{Quantitative comparison for face reenactment evaluation.} The best and second best results are reported in \textbf{bold} and [square brackets], respectively.} 
\label{tab:reena}
\begin{tabular}{lcccc}
\toprule
       Method   & ID$\downarrow$   & SSIM$\uparrow$ & Exp.$\downarrow$ & Pose$\downarrow$\\
\midrule
TPSM\cite{zhao2022thin}                   &0.48               &0.79                &0.16                    &0.14 \\  
FADM\cite{zeng2023face}                     & [0.38]           &0.82                 &0.17                   &0.06 \\
HyperReenact\cite{bounareli2023hyperreenact}    & 0.60       &  0.75           & [0.12]                      &\textbf{0.03} \\
DiffusionRig\cite{ding2023diffusionrig}    & 0.48           &  0.80           & \textbf{0.11}                &\textbf{0.03} \\
FaceAdapter\cite{han2024face}               & 0.40           & [0.84]        &0.16                            &[0.05] \\ 
Ours                                        &\textbf{0.34}     &\textbf{0.89}   &\textbf{0.11}             &\textbf{0.03} \\

\bottomrule
\end{tabular}
\end{table}










\noindent \textbf{Comparison with Face Reenactment Methods.} Since our method can be applied to edit pose, expression, and lighting together, it can effectively perform face reenactment. Fig. \ref{fig:ree} compares with SoTA methods on face reenactment, including TPSM \cite{zhao2022thin}, FADM \cite{zeng2023face}, HyperReenact \cite{bounareli2023hyperreenact}, DiffusionRig \cite{ding2023diffusionrig}, and FaceAdapter \cite{han2024face}. Tab. \ref{tab:reena} reports the results tested on the same 20 identities for face reenactment task. Note that due to changes in pose, background computation may introduce interference for this task. Therefore, it is necessary to combine SSIM and pose metrics to evaluate background preservation. We can observe that we achieve comparable or even optimal results in identity consistency and editing precision. Owing to the Attribute Rigger, integrating the target background area into the condition helps prevent interference from background motion. During inference, the model only needs to copy a large portion of background from the source, which significantly simplifies background generation and improves background consistency. 




\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.48\textwidth]{figs/color.pdf}
    \caption{\textbf{Ablation study for different colors used to mask for the background image.} Black foreground is easy to clash with the color of the original images.}
    \label{fig:color}
\end{figure}



\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.98\textwidth]{figs/abl.pdf}
    \caption{\textbf{Ablation study for different conditions and identity encoder architectures.} The third and fourth rows represent the ablation results of the Disentangled and Pixel-Aligned Image Condition, while the fifth to eighth rows show the ablation results of the Model Architecture. Better viewed when zoomed in.}
    \label{fig:abl}
\end{figure*}




\subsection{Ablation Study}
\label{sec:abl}
\textbf{Disentangled and pixel-aligned image condition.} To demonstrate the effectiveness of using disentangled conditions and of using image conditions for pose and lighting conditioning, we explore alternative designs, including 1) We don't use image condition. Instead, we concatenate the expression, pose, and lighting coefficients together into a vector to make the conditions entangled. Then we embed this vector through a single linear layer and then add the output into the time embedding of the denoising UNet (\textbf{\textit{No-disent.}}) 2) We still use the coefficients instead of rendered images for pose and lighting condition, and we separate these coefficients to deal with them in different linear layers respectively. After that, we concatenate the ouputs from those linear layers into a vector and add it to the time embedding (\textbf{\textit{Coef.-sep.}}). As shown in the third and fourth rows of Fig. \ref{fig:abl}, visualizations illustrate that decoupled conditions outperform entangled conditions. Soleing relying on 3DMM coefficients as conditions does not work on pose and lighting editing, which can also cause pose variation when it is not intended to edit. Quantitative results are also presented in Tab. \ref{tab:abl}, demonstrating the superiority of our design.






\begin{table}[h!]
\centering
\caption{\textbf{Quantitative comparison of our model under different ablative configurations.} The best and second best results are reported in \textbf{bold} and [square brackets], respectively.} 
\label{tab:abl}
\begin{center}
\begin{tabular}{lccccc}
\toprule
       Method   & ID$\downarrow$   & SSIM$\uparrow$ & Exp.$\downarrow$ & Light$\downarrow$ & Pose$\downarrow$\\
\midrule 
No-disent.  & [0.36]       &0.81                    &0.14                 &0.32        &0.18 \\  
Coef.-sep.   & [0.36]      &0.79                    &0.13                 &0.34         &0.11 \\
Control.    & 0.49        &0.77                    &0.18                 &0.38         &0.14 \\
CLIP-ID    &0.43          &[0.96]                    &\textbf{0.11}      &[0.28]    &[0.09] \\ 
CLIP-light    &0.36       &\textbf{0.97}            &\textbf{0.11}       &0.30    &0.12 \\
Conv-ID    &\textbf{0.34}       &\textbf{0.97}        &\textbf{0.11}     &[0.28]    &[0.09] \\ 
Ours        &\textbf{0.34}    &0.95               &\textbf{0.11}        &\textbf{0.14}    &\textbf{0.03} \\

\bottomrule
\end{tabular}
\end{center}
\end{table}







\noindent\textbf{Model Architecture.} To demonstrate the effectiveness of Indentity encoder design, we conduct experiments 1) Replacing the whole architecture with ControlNet (\textbf{\textit{Control.}}); 2) Replacing the Identity Encoder with CLIP \cite{radford2021learning} image encoder (\textbf{\textit{CLIP-ID}}); 3) Replacing the Identity Encoder with a trainable Conv layer (\textbf{\textit{Conv-ID}}); 4) Using CLIP image encoder to embed lignting conditions (\textbf{\textit{CLIP-light}}). As shown in the bottom five rows of Fig. \ref{fig:abl}, ControlNet model will largely change the appearance of the input images and even fails on editing expressions. Using CLIP features as identity image features can preserve image similarity but fails to fully transfer details. Moreover, CLIP is unable to extract lighting features from rendering conditions, making the light editing in the seventh row less effective. A trainable Conv layer performs much better than CLIP in transfering facial details, but its performance in editing lighting and head pose is not effective enough. We speculate that this may be due to the identity condition and editing condition being placed together in the first layer of the UNet, increasing the burden on the first layer. Quantitative results are presented in Tab. \ref{tab:abl} likewise, which is coherent with our qualitative observations. 


\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.98\textwidth]{figs/vis.pdf}
    \caption{\textbf{Visualization of self-attention maps (the 1st) row and cross-attention maps (the 2nd) row.} We made expression and pose edits here to generate the predicted image.}
    \label{fig:vis}
\end{figure*}


\noindent\textbf{Foreground Color.} We find that the color of the masked region has a significant impact on the experimental performance. As demonstrated in Fig. \ref{fig:color}, a black foreground can easily blend with a person's hair, clothing, or even the original image background, leading to errors in the model's background identification. This not only affects the preservation of the source image background but also impacts the final editing of the person. To solve this problem, we use gray to mask the human region, as it is a color that rarely clashes with the image background or the person.



\noindent\textbf{Visualization.} Fig. \ref{fig:vis} presents a visualization of the attention maps learned in both the self-attention and cross-attention layers of the denoising UNet. At the beginning of the denoising process, the values in the self-attention map are generally low across the entire image. However, as the denoising progresses, the self-attention increasingly focuses on the identity itself. This indicates that self-attention focuses more on the generation of the edited content. 

For cross-attention, since our text prompt (\textit{a closeup of a person}) remains fixed and unchanged in both the training as well as testing stage, the model initially focuses on outlining the person's silhouette. As the denoising process progresses, the cross-attention gradually shifts towards incorporating the content from the Attribute Rigger, contributing to the reconstruction of the entire image.

\noindent\textbf{Limitations.} Since RigFace relies on DECA to get render conditions, it will be affected by DECA's limited capability. Minor changes in lighting will cause different DECA renderings, but this may be insensitive to our model. Additionally, our model may struggle to be faithful to the original background when used to edit dramatic pose variation, as this involves large area of background inpainting which is beyond our model's topic. Additionally, training two Stable Diffusion models simultaneously requires a significant amount of computational resources.



\section{Conclusion}


In this paper, we present RigFace, a LDM-based model for image-based editing of facial appearance. We introduce a Spatial Attribute Provider to produce decoupled conditions, which unleashes the generative capabilities of pretrained diffusion models. We introduce Indentity Encoder, which highly preserves intricate ID appearances and achieve efficient controllability from editing conditions. This model effectively addresses the tasks in face editing of pose, expression and lighting, surpassing previous state-of-theart GAN-based and diffusion-based methods. Extensive qualitative and quantitative experiments demonstrate the superiority of our method.


\noindent\textbf{Potential Impact.} This work explores a framework based on diffusion for high-quality riggable of face editing, which offers greater practical value while enhancing the quality of the generated content. However, the potential misuse of RigFace poses risks such as privacy violations, the spread of misinformation, and ethical concerns. To address these issues, both visible and invisible digital watermarks can be implemented to verify the origin and authenticity of the content. On the other hand, RigFace can also support advancements in forgery detection \cite{guo2023ldfnet,zhang2024face,zhang2023bi,yu2024distilling}, strengthening the ability to identify and counter deepfakes.





\bibliographystyle{IEEEtran}
\bibliography{tcsvt_references}

\end{document}


