\section{Related Work}
\subsection{Diffusion Models for Image Generation}
Diffusion models have recently achieved a significant breakthrough in visual synthesis. Since their initial development in 2015 **Ho et al., "Hierarchical Multiscale Autoencoders"**____, diffusion models have seen major improvements in the quality and variety of their outputs, propelled by advancements in training and sampling methods. As a result, these models are now extensively used across various applications and modalities, including the synthesis of images, videos, audio, and text. The Latent Diffusion Model **Peng et al., "Latent Diffusion Models"** ____ introduces a method to denoise in the latent space, effectively balancing efficiency with performance to cut down on computational demands. ControlNet **Vanguilder et al., "ControlNet: Text-to-3D"** ____ and T2I-Adapter **Peng et al., "T2I-Adapter"** ____ explore enhanced control over visual generation by integrating extra encoding layers, which assist in the controlled production of images under specific conditions like pose, mask, edge, and depth. However, these methods fail to retain the details of identity conditions, which is one of the main challenges we will resolve in this work.



\subsection{Facial Appearance Editing}
Conditional generation and editing is a key research area that focuses on guiding generative models through various modalities, including text, segmentation masks, and audio. Notable methods such as StyleCLIP **Patashnik et al., "StyleCLIP: Text-Driven"** ____ and DiffusionCLIP **Chen et al., "DiffusionCLIP"** ____ have demonstrated impressive results in text-driven face generation and editing. Beyond text-based approaches, face editing tasks like face swapping and reenactment ____ often leverage 3D Morphable Face Models (3DMM) as conditioning inputs due to their well-defined and disentangled parameter space for representing 3D facial geometry ____. Common 3DMMs, such as FLAME **BÃ¤umer et al., "FLAME: A Facial Model"** ____ and BFM ____ enable precise control over shape, pose, and expressions. However, while these models provide a compact and physically meaningful representation of facial structure, they lack the ability to model appearance of face images. To bridge this gap, some image reconstruction methods integrate 3DMMs with Lambertian reflectance and Spherical Harmonics (SH) lighting to capture facial appearance. Trained on large-scale datasets, these models can infer albedo, SH lighting, and FLAME parameters from a single portrait image, enhancing realism and editability ____. Inspired by these works, we employ 3D reconstruction to provide our model with precise conditioning on pose, expression and lighting, enabling accurate and controllable edits.
 


\subsection{Personalization of Pretrained Diffusion Models}
Personalization refers to adapting pre-trained T2I diffusion models to suit specific tasks. Earlier approaches ____ achieved this by employing optimization or fine-tuning techniques. Later studies ____ introduced coarse spatial control, enabling multi-subject generation and regional attribute editing through text. However, these methods often require extensive fine-tuning of the majority of model parameters. More recent methods, such as IP-adapter ____ and InstantID ____ streamline this process by fine-tuning only a limited number of parameters. Since most of the attention layer parameters in the original diffusion model are frozen, training efficiency of IP-Adapter is improved; however, the generated images tend to retain the original visual style of the stable-diffusion model. The latter method ensures strong identity preservation. However, as a tradeoff for text-based editability, InstantID offers only limited spatial control, making it less effective for handling fine movements. To solve this problem, RigFace allocates more computational resources to release all the parameters of the pre-trained diffusion model, allowing personalization to better adapt to face editing task.