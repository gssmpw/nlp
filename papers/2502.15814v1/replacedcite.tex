\section{Related Work}
\newpara{Efficient training.} Enhancing the efficiency of neural network training has been extensively studied____. ____ examined the impact of data selection on \ac{LLM} training and introduced efficient data selection methods. ____ proposed using structured sparse gradients to enhance compute efficiency in \ac{LLM} training, while ____ explored the potential of leveraging smaller language models to improve the training efficiency of larger \ac{LLM}s. ____ investigated the use of low-dimensional projections for attention parameters to enhance training efficiency. Meanwhile, ____ proposed applying LayerDrop as a technique to optimise neural network training.

More closely related to our work, ____ propose a training strategy for developing \ac{LLM}s within a $100k\$$ budget. ____ introduce ModernBERT, an efficient training pipeline for optimising BERT models, while ____ outline a method for training a BERT model in $24$ hours using $8$ GPUs. The most relevant work to ours is Cramming ____, where the authors conduct an in-depth analysis of masked LM training on a single GPU in one day.

While these studies offer valuable insights, they primarily focus on training text models, such as \ac{LLM}s and masked LMs. In the speech domain, similar research has been conducted on self-supervised representation models____, but not on \slms. In this work, we address this gap by focusing on efficient \slm training.

\newpara{Generative speech language models} were explored under various setups____.  
____ were the first to show how raw and uncurated speech data can be leveraged into building a \ac{GSLM} system. Next,____ proposed a cascade version using both coarse and fine speech tokens. Such a modelling framework opened up a new and promising research direction for processing and modelling spoken data, such as speech resynthesis____, speaking style conversion____, dialogue modelling____, speech-to-speech translation____, etc. 
____ proposed augmenting a text \ac{LM} with continuous speech data to improve spoken question-answering tasks. Recently, ____ proposed \slm based on state-space models____ to further push long context-efficient modelling, while ____ proposed to fine-tune \slms using direct preference optimisation____ obtained from text LLM rankings. 

Similar to text \ac{LLMs}, training \slms often demands large-scale datasets. For instance, Moshi____ was trained on $7$ million hours of speech data, SpiritLM____ utilized $560k$ hours, and TWIST____ was trained on approximately $150k$. Recently, ____ introduced the first scaling laws for \slms, suggesting that achieving comparable performance to text LMs requires three times more tokens. In this work, we focus on reducing the computational demands while maintaining performance comparable to leading \slms.