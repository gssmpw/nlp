\section{Setup}
In this study, we explore decoder-only generative \slms, which aim at maximising the likelihood of speech samples represented as discrete tokens. We examine both purely speech-based \slms trained on speech tokens and joint speech-text \slms using interleaving strategies~\citep{spiritlm}. Similarly to ~\citet{twist, gslm}, we obtain speech tokens by quantising continuous latent representations of a self-supervised speech representation model using the k-means algorithm, often known as \emph{semantic tokens}. Specifically, we utilise a multilingual HuBERT~\citep{hubert} model running at $25$ Hz, as employed in ~\citet{twist}. We then train \slms by minimising the negative log-likelihood of the input segments.

Unless mentioned otherwise, all \slms are trained using \textbf{a single $A5000$ GPU ($24GB$ VRAM)} along with $16$ CPU cores for $24$ hours. We deliberately focus on this constrained compute budget, assuming that most academic labs can access similar resources, thereby ensuring the accessibility of our research. The training data is pre-processed, i.e. extracting HuBERT units and dividing data into chunks, and stored prior to model training. As a result, this pre-processing time is excluded from the compute budget. This approach, aligned with \citet{geiping2023cramming}, is practical since many research experiments utilise the same pre-processed data. We additionally do not count the time for running validation and visualisations as they are not used as part of the optimisation pipeline and only used for demonstration purposes.

\newpara{Evaluation metrics.} We assess all \slms using four distinct evaluation metrics. The first three are based on likelihood evaluation, while the fourth is a generative metric. For likelihood based modelling we consider sBLIMP~\cite{dunbar2021zero}, \emph{Spoken Story Cloze} (\ssc)), and \emph{Topic Story-Cloze} (\tsc)~\cite{twist}. For modelling-likelihood metrics, we evaluate the likelihood assigned by the \slms to pairs of speech utterances, consisting of a positive example and a distractor. We calculate the percent of pairs in which the \slm assigns higher likelihood to the positive sample. sBLIMP focuses on grammatical abilities thus the negative is ungrammatical version of the positive. \ssc and \tsc focus on semantic modelling abilities. In \ssc, the distractor suffix is taken from the original textual StoryCloze dataset~\citep{mostafazadeh2016corpus}, allowing to assess fine-grained semantic speech understanding. In \tsc, however, the distractor suffix is drawn from a different topic, enabling us to evaluate the modelâ€™s ability to understand the overall semantic concept. 

Finally, to assess the generative abilities of \slms, we compute \emph{generative perplexity} (GenPPL). Following the approach of ~\cite{gslm, twist}, we provide the \slm with a short speech prompt and generate speech tokens continuation. We use unit-vocoder with duration prediction to convert the tokens into speech~\citep{polyak2021speech, twist}. The generated speech is then transcribed, and its \ac{PPL} is evaluated using a pre-trained text \ac{LLM}. To minimise the impact of token repetition on \ac{PPL} measurements, we ground the generated text using diversity metrics derived from the auto-BLEU score~\citep{gslm}. Similarly to ~\citet{lin2024alignslm} we use bigram auto-BLEU. In other words, we ensure that all models achieve similar auto-BLEU scores, allowing for a fair comparison of \ac{PPL}. Specifically, we transcribe speech segments using Whisper-large-v$3$-turbo model~\citep{radford2023robust} and measure \ac{PPL} using Llama-$3.2$-$1$B model~\citep{grattafiori2024llama3herdmodels}. 

\newpara{Software efficiency.} To maximise performance within $24$ hours of model training, we leverage multiple efficient implementations. Through extensive performance testing, we found that using bfloat$16$ ~\cite{kalamkar2019study} alongside FlashAttention$2$ \citep{dao2023flashattention2fasterattentionbetter} and data packing provided the most efficient compute performance in our setup. We also experimented with model compilation using \texttt{torch.compile} \citep{ansel2024pytorch}, but it lacked native compatibility with FlashAttention$2$ at the time of our study, and its performance without FlashAttention$2$ was subpar. Future work could investigate this further with more efficient attention implementations~\cite{FA3, li2024flexattention}.

To enable rapid and scalable experimentation, we developed a specialised library for \slm training that supports various model architectures, training objectives, and evaluation metrics. This library accommodates both TWIST-style training, text-speech interleaving, preference optimisation, etc. We will open-source this package along all models weights and training recipes, aiming to empower the community to further explore \slms.