\appendix

\section{Full \method Recipe} \label{sec:app_recipe}
We provide below the full training recipe, including hyperparameters for the best, \method recipe. In Table ~\ref{tab:slam-dpo-recipe} we see the \method (-DPO) pre-training recipe and in Table ~\ref{tab:dpo-recipe} we see the \method DPO training recipe.


\begin{table}[ht]
  \centering
  \caption{\method (-DPO) Pre Training Recipe}
  \resizebox{\columnwidth}{!}{
  \begin{tabular}{l|c}
    \toprule
    Parameter & Value \\ 
    \midrule
    Text Base Model   & Qwen2.5-0.5B  \\
    TWIST initialisation & True \\ 
    Data & Librilight + Librispeech + sTinyStories \\
    Train Time  & $23.5$ hours $\simeq17625$  steps  \\ 
    RoPE theta   & $10000$  \\ 
    Context length   & $1024$  \\ 
    Per device Batch Size   & $8$  \\
    Gradient Accumulation & $16$ \\ 
    Base Learning Rate & $1e-3$ \\ 
    Warmup Ratio & $1\%$ \\ 
    Optimizer   & AdamW  \\ 
    Learning Rate Scheduler   & cosine with min $5e-5$  \\
    Max Grad Norm & $0.5$ \\
    Dtype & bfloat16 \\
    \bottomrule
  \end{tabular}
  }
  \label{tab:slam-dpo-recipe}
\end{table}

\begin{table}[ht]
  \centering
  \caption{\method  DPO Training Recipe}
  \resizebox{\columnwidth}{!}{
  \begin{tabular}{l|c}
    \toprule
    Parameter & Value \\ 
    \midrule
    Initial Model & \method (-DPO) \\
    Data & SpokenSwag with auto-bleu smaller than 0.3 \\
    Train Time  & $0.5$ hour $\simeq813$  steps  \\ 
    RoPE theta   & $10000$  \\ 
    Context length   & $1024$  \\ 
    Per device Batch Size   & $4$  \\
    Gradient Accumulation & $16$ \\
    Base Learning Rate & $5e-5$ \\ 
    Optimizer   & AdamW  \\ 
    Learning Rate Scheduler   & inverse sqrt  \\
    Max Grad Norm & $0.5$ \\
    Dtype & bfloat16 \\
    DPO $\beta$ & $0.1$ \\
    \bottomrule
  \end{tabular}
  }
  \label{tab:dpo-recipe}
\end{table}




\section{Model Sizes}\label{sec:model_sizes}

\begin{table}[h]
  \caption{Model names and parameter counts after changing vocabulary to speech only units (500).}
  \centering
  \resizebox{\columnwidth}{!}{
  \begin{tabular}{l|c}
    \toprule
    Model Name & Number of Params\\
    \midrule
    MobileLLM-125M \cite{mobilellm} & 106,492,608 \\
    MobileLLM-350M \cite{mobilellm} & 315,117,120 \\
    OPT-125M \cite{opt} & 87,015,936 \\
    OPT-350M \cite{opt} & 305,714,176 \\
    QWEN2.5-0.5B \cite{qwen2} & 358,347,904 \\
    SmolLM2-135M \cite{smollm2} & 106,492,608 \\
    SmolLM2-360M \cite{smollm2} & 315,117,120 \\
    Pythia-160M \cite{pythia}  & 85,827,072  \\
    Pythia-410M \cite{pythia}  & 303,339,520  \\
    \bottomrule
  \end{tabular}
  \label{tab:model_sizes}
  }
\end{table}



As mentioned, we use the original names of the text LMs used for clarity and consistency, but note that the actual parameter counts after resizing the vocabulary to speech-units only can be very different. In Table \ref{tab:model_sizes} we provide an extensive list of models and sizes.

\section{Dataset Statistics}
We use and synthesise several datasets. In this section we give exact details of number of samples, splits used, domains etc. 

For pre-training we use Libri-Light \cite{ll} and LibriSpeech \cite{ls}. For Libri-Light we randonly select one percent of samples as validation, whereas for LibriSpeech we use the original \emph{dev-clean} and \emph{dev-other} splits. Both of these datasets are English speech only, focused in the audio-book domain. We also synthesise sTinyStories for pre-training which consists of synthetically generated English short stories. We use the official train split for training. Full dataset sizes are in Table \ref{tab:data_stats}.

We also investigate diverse datasets for pre-training: SWC \cite{swc}, Tedlium \cite{tedlium}, PeopleSpeech \cite{people} and VoxPopuli \cite{vp}. We only take English subsets for all datasets, yet they can still contain diverse accents. These datasets are in the following domains SWC - read Wikipedia articles, Tedlium - short lectures, PeopleSpeech - diverse data including many local council gatherings etc, VoxPopuli - from European Parliament meetings. For SWC specifically, we use the text alignment to create chunks, remove silence from the audio and remove mis-aligned chunks. We use full training splits where provided, otherwise splitting 99\% for training. The dataset sizes are described in \ref{tab:data_stats}.

\begin{table}[h]
  \caption{Dataset train set sizes that we use.}
  \centering
  \resizebox{\columnwidth}{!}{
  \begin{tabular}{l|cc}
    \toprule
    Dataset & Number of Hours & Number of Tokens\\
    \midrule
    Libri-Light \cite{ll} & $50K$ & $3.5B$ \\
    LibriSpeech \cite{ls} & $960$ & $67M$ \\
    SWC \cite{swc} & $750$ & $19M$ \\
    Tedlium \cite{tedlium} & $1.6K$ & $110M$ \\
    PeopleSpeech \cite{people} & $7K$ & $480M$ \\
    VoxPopuli \cite{vp} & $24K$ & $1.64B$ \\
    sTinyStories & $30K$ & $2.2B$ \\
    \bottomrule
  \end{tabular}
  \label{tab:data_stats}
  }
\end{table}

For DPO we synthesise SpokenSwag based on the SWAG \cite{swag} dataset. We use only the official train set and filter only the gold standard labels. We end up with 47k sample pairs which end up to be $\sim 4.5M$ tokens.

\section{AI Tool Usage}
AI based tools may have been used in writing parts of the code for this study, or para-phrasing some of the writing within the paper, yet all the content was thoroughly checked by the authors, with these only being used as assistive tools.
