\section{Conclusion}
In this work we show that training high quality \slms with a very modest compute budget, is feasible. We give these main guidelines: (i) \textbf{Do not skimp on the model} - not all model families are born equal and the TWIST initialisation exaggerates this, thus it is worth selecting a stronger / bigger text-LM even if it means less tokens. we found Qwen$2.5$ to be a good choice; (ii) \textbf{Utilise synthetic training data} - pre-training on data generated with TTS helps a lot; (iii) \textbf{Go beyond next token prediction} - we found that DPO boosts performance notably even when using synthetic data, and as little as $30$ minutes training massively improves results; (iv) \textbf{Optimise hyper-parameters} - as researchers we often dis-regard this stage, yet we found that tuning learning rate schedulers and optimising code efficiency can improve results notably. We hope that these insights, and open source resources will be of use to the research community in furthering research into remaining open questions in \slms.