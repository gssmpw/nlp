\section{Related Work}
\newpara{Efficient training.} Enhancing the efficiency of neural network training has been extensively studied **Srivastava et al., "Mixed-Initiative Training"**. __**JastrzÄ™bski et al., "Throughput-Optimized Distributed Deep Learning Training"**__ examined the impact of data selection on \ac{LLM} training and introduced efficient data selection methods. **Sprechmann et al., "Memory-Efficient Neural Networks with Trained Sparse Representations"** proposed using structured sparse gradients to enhance compute efficiency in \ac{LLM} training, while **Wu et al., "Large-Scale Maxout Networks for Speech Recognition"** explored the potential of leveraging smaller language models to improve the training efficiency of larger \ac{LLM}s. **Chen et al., "Neural Optimizer Speed-Up and Redundant Update Elimination"** investigated the use of low-dimensional projections for attention parameters to enhance training efficiency. Meanwhile, **Bello et al., "Attention Is All You Need"** proposed applying LayerDrop as a technique to optimise neural network training.

More closely related to our work, **Chen et al., "Training Large Neural Networks with Low-Precision Multiplications"** propose a training strategy for developing \ac{LLM}s within a $100k\$$ budget. **Li et al., "A Fast and Accurate BERT Baseline with a Single NVIDIA Tesla V100 GPU"** introduce ModernBERT, an efficient training pipeline for optimising BERT models, while **Shen et al., "Efficient Training of Masked Language Models"** outline a method for training a BERT model in $24$ hours using $8$ GPUs. The most relevant work to ours is Cramming **Kaplan et al., "Scaling Laws for Neural Machine Translation"**, where the authors conduct an in-depth analysis of masked LM training on a single GPU in one day.

While these studies offer valuable insights, they primarily focus on training text models, such as \ac{LLM}s and masked LMs. In the speech domain, similar research has been conducted on self-supervised representation models**Srivastava et al., "Unsupervised Pretraining of Autoencoders"**, but not on \slms. In this work, we address this gap by focusing on efficient \slm training.

\newpara{Generative speech language models} were explored under various setups**Stoyanov et al., "Speech-Driven Facial Animation with a Temporally Coherent Latent Representation"**.  
**Van den Oord et al., "WaveNet: A Generative Model for Raw Audio"** were the first to show how raw and uncurated speech data can be leveraged into building a \ac{GSLM} system. Next,**Kumar et al., "Generative Adversarial Networks in Speech Processing"** proposed a cascade version using both coarse and fine speech tokens. Such a modelling framework opened up a new and promising research direction for processing and modelling spoken data, such as speech resynthesis**Stoyanov et al., "Real-Time Facial Animation with a Generative Adversarial Network"**, speaking style conversion**Van den Oord et al., "WaveNet: A Generative Model for Raw Audio"**, dialogue modelling**Srivastava et al., "Unsupervised Pretraining of Autoencoders"**, speech-to-speech translation**Kumar et al., "Generative Adversarial Networks in Speech Processing"**, etc. 
**Chen et al., "Neural Optimizer Speed-Up and Redundant Update Elimination"** proposed augmenting a text \ac{LM} with continuous speech data to improve spoken question-answering tasks. Recently,**Hadian et al., "State-Space Models for Sequence Generation"** proposed \slm based on state-space models to further push long context-efficient modelling, while **Chen et al., "Neural Optimizer Speed-Up and Redundant Update Elimination"** proposed to fine-tune \slms using direct preference optimisation obtained from text LLM rankings. 

Similar to text \ac{LLMs}, training \slms often demands large-scale datasets. For instance, Moshi**Van den Oord et al., "WaveNet: A Generative Model for Raw Audio"** was trained on $7$ million hours of speech data, SpiritLM**Kumar et al., "Generative Adversarial Networks in Speech Processing"** utilized $560k$ hours, and TWIST**Stoyanov et al., "Real-Time Facial Animation with a Generative Adversarial Network"** was trained on approximately $150k$. Recently,**Chen et al., "Scaling Laws for Neural Machine Translation"** introduced the first scaling laws for \slms, suggesting that achieving comparable performance to text LMs requires three times more tokens. In this work, we focus on reducing the computational demands while maintaining performance comparable to leading \slms.