\section{Related Work}

\newpara{Efficient training.} Enhancing the efficiency of neural network training has been extensively studied~\citep{shen2023efficient}. \citet{hajimolahoseini2023swiftlearn, wang2024greats} examined the impact of data selection on \ac{LLM} training and introduced efficient data selection methods. \citet{muhamed2024grass} proposed using structured sparse gradients to enhance compute efficiency in \ac{LLM} training, while \citet{rawat2024little} explored the potential of leveraging smaller language models to improve the training efficiency of larger \ac{LLM}s. \citet{lv2024scalable} investigated the use of low-dimensional projections for attention parameters to enhance training efficiency. Meanwhile, \citet{neiterman2024layerdropback} proposed applying LayerDrop as a technique to optimise neural network training.

More closely related to our work, \citet{li2023flm} propose a training strategy for developing \ac{LLM}s within a $100k\$$ budget. \citet{warner2024modernbert} introduce ModernBERT, an efficient training pipeline for optimising BERT models, while \citet{izsak2021train} outline a method for training a BERT model in $24$ hours using $8$ GPUs. The most relevant work to ours is Cramming \cite{geiping2023cramming}, where the authors conduct an in-depth analysis of masked LM training on a single GPU in one day.

While these studies offer valuable insights, they primarily focus on training text models, such as \ac{LLM}s and masked LMs. In the speech domain, similar research has been conducted on self-supervised representation models~\citep{liu2024efficient}, but not on \slms. In this work, we address this gap by focusing on efficient \slm training.

\newpara{Generative speech language models} were explored under various setups~\citep{gslm, kharitonov2021text}.  
\citet{gslm} were the first to show how raw and uncurated speech data can be leveraged into building a \ac{GSLM} system. Next,~\citet{borsos2023audiolm} proposed a cascade version using both coarse and fine speech tokens. Such a modelling framework opened up a new and promising research direction for processing and modelling spoken data, such as speech resynthesis~\citep{polyak2021speech}, speaking style conversion~\citep{kreuk2021textless,maimon2023dissc}, dialogue modelling~\cite{nguyen2022generative}, speech-to-speech translation~\citep{popuri2022enhanced, peng2024mslm}, etc. 
\citet{nachmani2023spoken} proposed augmenting a text \ac{LM} with continuous speech data to improve spoken question-answering tasks. Recently, \citet{park2024long} proposed \slm based on state-space models~\citep{gu2021efficiently} to further push long context-efficient modelling, while ~\citet{lin2024alignslm} proposed to fine-tune \slms using direct preference optimisation~\citep{rafailov2024dpo} obtained from text LLM rankings. 

Similar to text \ac{LLMs}, training \slms often demands large-scale datasets. For instance, Moshi~\cite{defossez2024moshi} was trained on $7$ million hours of speech data, SpiritLM~\cite{spiritlm} utilized $560k$ hours, and TWIST~\cite{twist} was trained on approximately $150k$. Recently, \citet{cuervo2024scaling} introduced the first scaling laws for \slms, suggesting that achieving comparable performance to text LMs requires three times more tokens. In this work, we focus on reducing the computational demands while maintaining performance comparable to leading \slms. 