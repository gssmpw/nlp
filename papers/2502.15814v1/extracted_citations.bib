@article{borsos2023audiolm,
  title={Audiolm: a language modeling approach to audio generation},
  author={Borsos, Zal{\'a}n and Marinier, Rapha{\"e}l and Vincent, Damien and Kharitonov, Eugene and Pietquin, Olivier and Sharifi, Matt and Roblek, Dominik and Teboul, Olivier and Grangier, David and Tagliasacchi, Marco and others},
  journal={IEEE/ACM transactions on audio, speech, and language processing},
  volume={31},
  pages={2523--2533},
  year={2023},
  publisher={IEEE}
}

@article{cuervo2024scaling,
  title={Scaling Properties of Speech Language Models},
  author={Cuervo, Santiago and Marxer, Ricard},
  journal={arXiv preprint arXiv:2404.00685},
  year={2024}
}

@article{defossez2024moshi,
  title={Moshi: a speech-text foundation model for real-time dialogue},
  author={D{\'e}fossez, Alexandre and Mazar{\'e}, Laurent and Orsini, Manu and Royer, Am{\'e}lie and P{\'e}rez, Patrick and J{\'e}gou, Herv{\'e} and Grave, Edouard and Zeghidour, Neil},
  journal={arXiv preprint arXiv:2410.00037},
  year={2024}
}

@inproceedings{geiping2023cramming,
  title={Cramming: Training a Language Model on a single GPU in one day.},
  author={Geiping, Jonas and Goldstein, Tom},
  booktitle={International Conference on Machine Learning},
  pages={11117--11143},
  year={2023},
  organization={PMLR}
}

@article{gslm,
  title={On generative spoken language modeling from raw audio},
  author={Lakhotia, Kushal and Kharitonov, Eugene and Hsu, Wei-Ning and Adi, Yossi and Polyak, Adam and Bolte, Benjamin and Nguyen, Tu-Anh and Copet, Jade and Baevski, Alexei and Mohamed, Abdelrahman and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={1336--1354},
  year={2021},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{gu2021efficiently,
  title={Efficiently modeling long sequences with structured state spaces},
  author={Gu, Albert and Goel, Karan and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2111.00396},
  year={2021}
}

@article{hajimolahoseini2023swiftlearn,
  title={SwiftLearn: A Data-Efficient Training Method of Deep Learning Models using Importance Sampling},
  author={Hajimolahoseini, Habib and Awad, Omar Mohamed and Ahmed, Walid and Wen, Austin and Asani, Saina and Hassanpour, Mohammad and Javadi, Farnoosh and Ahmadi, Mehdi and Ataiefard, Foozhan and Liu, Kangling and others},
  journal={arXiv preprint arXiv:2311.15134},
  year={2023}
}

@article{izsak2021train,
  title={How to train BERT with an academic budget},
  author={Izsak, Peter and Berchansky, Moshe and Levy, Omer},
  journal={arXiv preprint arXiv:2104.07705},
  year={2021}
}

@article{kharitonov2021text,
  title={Text-free prosody-aware generative spoken language modeling},
  author={Kharitonov, Eugene and others},
  journal={arXiv preprint arXiv:2109.03264},
  year={2021}
}

@article{kreuk2021textless,
  title={Textless speech emotion conversion using decomposed and discrete representations},
  author={Kreuk, Felix and others},
  journal={arXiv preprint arXiv:2111.07402},
  year={2021}
}

@article{li2023flm,
  title={Flm-101b: An open llm and how to train it with \$100 k budget},
  author={Li, Xiang and Yao, Yiqun and Jiang, Xin and Fang, Xuezhi and Meng, Xuying and Fan, Siqi and Han, Peng and Li, Jing and Du, Li and Qin, Bowen and others},
  journal={arXiv preprint arXiv:2309.03852},
  year={2023}
}

@article{lin2024alignslm,
  title={Align-SLM: Textless Spoken Language Models with Reinforcement Learning from AI Feedback},
  author={Lin, Guan-Ting and Shivakumar, Prashanth Gurunath and Gourav, Aditya and Gu, Yile and Gandhe, Ankur and Lee, Hung-yi and Bulyko, Ivan},
  journal={arXiv preprint arXiv:2411.01834},
  year={2024}
}

@inproceedings{liu2024efficient,
  title={Efficient Training of Self-Supervised Speech Foundation Models on a Compute Budget},
  author={Liu, Andy T and Lin, Yi-Cheng and Wu, Haibin and Winkler, Stefan and Lee, Hung-yi},
  booktitle={2024 IEEE Spoken Language Technology Workshop (SLT)},
  pages={961--968},
  year={2024},
  organization={IEEE}
}

@article{lv2024scalable,
  title={Scalable Efficient Training of Large Language Models with Low-dimensional Projected Attention},
  author={Lv, Xingtai and Ding, Ning and Zhang, Kaiyan and Hua, Ermo and Cui, Ganqu and Zhou, Bowen},
  journal={arXiv preprint arXiv:2411.02063},
  year={2024}
}

@inproceedings{maimon2023dissc,
  title={Speaking Style Conversion in the Waveform Domain Using Discrete Self-Supervised Units},
  author={Maimon, Gallil and Adi, Yossi},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={8048--8061},
  year={2023}
}

@article{muhamed2024grass,
  title={Grass: Compute efficient low-memory llm training with structured sparse gradients},
  author={Muhamed, Aashiq and Li, Oscar and Woodruff, David and Diab, Mona and Smith, Virginia},
  journal={arXiv preprint arXiv:2406.17660},
  year={2024}
}

@inproceedings{nachmani2023spoken,
  title={Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM},
  author={Nachmani, Eliya and others},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@article{neiterman2024layerdropback,
  title={LayerDropBack: A Universally Applicable Approach for Accelerating Training of Deep Networks},
  author={Neiterman, Evgeny Hershkovitch and Ben-Artzi, Gil},
  journal={arXiv preprint arXiv:2412.18027},
  year={2024}
}

@article{nguyen2022generative,
  title={Generative Spoken Dialogue Language Modeling},
  author={Nguyen, Tu Anh and others},
  journal={arXiv preprint arXiv:2203.16502},
  year={2022}
}

@article{park2024long,
  title={Long-Form Speech Generation with Spoken Language Models},
  author={Park, Se Jin and Salazar, Julian and Jansen, Aren and Kinoshita, Keisuke and Ro, Yong Man and Skerry-Ryan, RJ},
  journal={arXiv preprint arXiv:2412.18603},
  year={2024}
}

@article{peng2024mslm,
  title={MSLM-S2ST: A Multitask Speech Language Model for Textless Speech-to-Speech Translation with Speaker Style Preservation},
  author={Peng, Yifan and others},
  journal={arXiv preprint arXiv:2403.12408},
  year={2024}
}

@article{polyak2021speech,
  title={Speech resynthesis from discrete disentangled self-supervised representations},
  author={Polyak, Adam and others},
  journal={arXiv preprint arXiv:2104.00355},
  year={2021}
}

@article{popuri2022enhanced,
  title={Enhanced Direct Speech-to-Speech Translation Using Self-supervised Pre-training and Data Augmentation},
  author={Popuri, Sravya and others},
  journal={arXiv preprint arXiv:2204.02967},
  year={2022}
}

@article{rafailov2024dpo,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{rawat2024little,
  title={A Little Help Goes a Long Way: Efficient LLM Training by Leveraging Small LMs},
  author={Rawat, Ankit Singh and Sadhanala, Veeranjaneyulu and Rostamizadeh, Afshin and Chakrabarti, Ayan and Jitkrittum, Wittawat and Feinberg, Vladimir and Kim, Seungyeon and Harutyunyan, Hrayr and Saunshi, Nikunj and Nado, Zachary and others},
  journal={arXiv preprint arXiv:2410.18779},
  year={2024}
}

@article{shen2023efficient,
  title={On efficient training of large-scale deep learning models: A literature review},
  author={Shen, Li and Sun, Yan and Yu, Zhiyuan and Ding, Liang and Tian, Xinmei and Tao, Dacheng},
  journal={arXiv preprint arXiv:2304.03589},
  year={2023}
}

@article{spiritlm,
  title={Spirit-lm: Interleaved spoken and written language model},
  author={Nguyen, Tu Anh and Muller, Benjamin and Yu, Bokai and Costa-Jussa, Marta R and Elbayad, Maha and Popuri, Sravya and Ropers, Christophe and Duquenne, Paul-Ambroise and Algayres, Robin and Mavlyutov, Ruslan and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={13},
  pages={30--52},
  year={2025},
  publisher={MIT Press 255 Main Street, 9th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@article{twist,
  title={Textually pretrained speech language models},
  author={Hassid, Michael and Remez, Tal and Nguyen, Tu Anh and Gat, Itai and Conneau, Alexis and Kreuk, Felix and Copet, Jade and Defossez, Alexandre and Synnaeve, Gabriel and Dupoux, Emmanuel and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{wang2024greats,
  title={GREATS: Online Selection of High-Quality Data for LLM Training in Every Iteration},
  author={Wang, Jiachen T and Wu, Tong and Song, Dawn and Mittal, Prateek and Jia, Ruoxi},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024}
}

@article{warner2024modernbert,
  title={Smarter, better, faster, longer: A modern bidirectional encoder for fast, memory efficient, and long context finetuning and inference},
  author={Warner, Benjamin and Chaffin, Antoine and Clavi{\'e}, Benjamin and Weller, Orion and Hallstr{\"o}m, Oskar and Taghadouini, Said and Gallagher, Alexis and Biswas, Raja and Ladhak, Faisal and Aarsen, Tom and others},
  journal={arXiv preprint arXiv:2412.13663},
  year={2024}
}

