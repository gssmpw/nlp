[
  {
    "index": 0,
    "papers": [
      {
        "key": "shen2023efficient",
        "author": "Shen, Li and Sun, Yan and Yu, Zhiyuan and Ding, Liang and Tian, Xinmei and Tao, Dacheng",
        "title": "On efficient training of large-scale deep learning models: A literature review"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "hajimolahoseini2023swiftlearn",
        "author": "Hajimolahoseini, Habib and Awad, Omar Mohamed and Ahmed, Walid and Wen, Austin and Asani, Saina and Hassanpour, Mohammad and Javadi, Farnoosh and Ahmadi, Mehdi and Ataiefard, Foozhan and Liu, Kangling and others",
        "title": "SwiftLearn: A Data-Efficient Training Method of Deep Learning Models using Importance Sampling"
      },
      {
        "key": "wang2024greats",
        "author": "Wang, Jiachen T and Wu, Tong and Song, Dawn and Mittal, Prateek and Jia, Ruoxi",
        "title": "GREATS: Online Selection of High-Quality Data for LLM Training in Every Iteration"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "muhamed2024grass",
        "author": "Muhamed, Aashiq and Li, Oscar and Woodruff, David and Diab, Mona and Smith, Virginia",
        "title": "Grass: Compute efficient low-memory llm training with structured sparse gradients"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "rawat2024little",
        "author": "Rawat, Ankit Singh and Sadhanala, Veeranjaneyulu and Rostamizadeh, Afshin and Chakrabarti, Ayan and Jitkrittum, Wittawat and Feinberg, Vladimir and Kim, Seungyeon and Harutyunyan, Hrayr and Saunshi, Nikunj and Nado, Zachary and others",
        "title": "A Little Help Goes a Long Way: Efficient LLM Training by Leveraging Small LMs"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "lv2024scalable",
        "author": "Lv, Xingtai and Ding, Ning and Zhang, Kaiyan and Hua, Ermo and Cui, Ganqu and Zhou, Bowen",
        "title": "Scalable Efficient Training of Large Language Models with Low-dimensional Projected Attention"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "neiterman2024layerdropback",
        "author": "Neiterman, Evgeny Hershkovitch and Ben-Artzi, Gil",
        "title": "LayerDropBack: A Universally Applicable Approach for Accelerating Training of Deep Networks"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "li2023flm",
        "author": "Li, Xiang and Yao, Yiqun and Jiang, Xin and Fang, Xuezhi and Meng, Xuying and Fan, Siqi and Han, Peng and Li, Jing and Du, Li and Qin, Bowen and others",
        "title": "Flm-101b: An open llm and how to train it with \\$100 k budget"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "warner2024modernbert",
        "author": "Warner, Benjamin and Chaffin, Antoine and Clavi{\\'e}, Benjamin and Weller, Orion and Hallstr{\\\"o}m, Oskar and Taghadouini, Said and Gallagher, Alexis and Biswas, Raja and Ladhak, Faisal and Aarsen, Tom and others",
        "title": "Smarter, better, faster, longer: A modern bidirectional encoder for fast, memory efficient, and long context finetuning and inference"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "izsak2021train",
        "author": "Izsak, Peter and Berchansky, Moshe and Levy, Omer",
        "title": "How to train BERT with an academic budget"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "geiping2023cramming",
        "author": "Geiping, Jonas and Goldstein, Tom",
        "title": "Cramming: Training a Language Model on a single GPU in one day."
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "liu2024efficient",
        "author": "Liu, Andy T and Lin, Yi-Cheng and Wu, Haibin and Winkler, Stefan and Lee, Hung-yi",
        "title": "Efficient Training of Self-Supervised Speech Foundation Models on a Compute Budget"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "gslm",
        "author": "Lakhotia, Kushal and Kharitonov, Eugene and Hsu, Wei-Ning and Adi, Yossi and Polyak, Adam and Bolte, Benjamin and Nguyen, Tu-Anh and Copet, Jade and Baevski, Alexei and Mohamed, Abdelrahman and others",
        "title": "On generative spoken language modeling from raw audio"
      },
      {
        "key": "kharitonov2021text",
        "author": "Kharitonov, Eugene and others",
        "title": "Text-free prosody-aware generative spoken language modeling"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "gslm",
        "author": "Lakhotia, Kushal and Kharitonov, Eugene and Hsu, Wei-Ning and Adi, Yossi and Polyak, Adam and Bolte, Benjamin and Nguyen, Tu-Anh and Copet, Jade and Baevski, Alexei and Mohamed, Abdelrahman and others",
        "title": "On generative spoken language modeling from raw audio"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "borsos2023audiolm",
        "author": "Borsos, Zal{\\'a}n and Marinier, Rapha{\\\"e}l and Vincent, Damien and Kharitonov, Eugene and Pietquin, Olivier and Sharifi, Matt and Roblek, Dominik and Teboul, Olivier and Grangier, David and Tagliasacchi, Marco and others",
        "title": "Audiolm: a language modeling approach to audio generation"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "polyak2021speech",
        "author": "Polyak, Adam and others",
        "title": "Speech resynthesis from discrete disentangled self-supervised representations"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "kreuk2021textless",
        "author": "Kreuk, Felix and others",
        "title": "Textless speech emotion conversion using decomposed and discrete representations"
      },
      {
        "key": "maimon2023dissc",
        "author": "Maimon, Gallil and Adi, Yossi",
        "title": "Speaking Style Conversion in the Waveform Domain Using Discrete Self-Supervised Units"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "nguyen2022generative",
        "author": "Nguyen, Tu Anh and others",
        "title": "Generative Spoken Dialogue Language Modeling"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "popuri2022enhanced",
        "author": "Popuri, Sravya and others",
        "title": "Enhanced Direct Speech-to-Speech Translation Using Self-supervised Pre-training and Data Augmentation"
      },
      {
        "key": "peng2024mslm",
        "author": "Peng, Yifan and others",
        "title": "MSLM-S2ST: A Multitask Speech Language Model for Textless Speech-to-Speech Translation with Speaker Style Preservation"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "nachmani2023spoken",
        "author": "Nachmani, Eliya and others",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "park2024long",
        "author": "Park, Se Jin and Salazar, Julian and Jansen, Aren and Kinoshita, Keisuke and Ro, Yong Man and Skerry-Ryan, RJ",
        "title": "Long-Form Speech Generation with Spoken Language Models"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "gu2021efficiently",
        "author": "Gu, Albert and Goel, Karan and R{\\'e}, Christopher",
        "title": "Efficiently modeling long sequences with structured state spaces"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "lin2024alignslm",
        "author": "Lin, Guan-Ting and Shivakumar, Prashanth Gurunath and Gourav, Aditya and Gu, Yile and Gandhe, Ankur and Lee, Hung-yi and Bulyko, Ivan",
        "title": "Align-SLM: Textless Spoken Language Models with Reinforcement Learning from AI Feedback"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "rafailov2024dpo",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea",
        "title": "Direct preference optimization: Your language model is secretly a reward model"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "defossez2024moshi",
        "author": "D{\\'e}fossez, Alexandre and Mazar{\\'e}, Laurent and Orsini, Manu and Royer, Am{\\'e}lie and P{\\'e}rez, Patrick and J{\\'e}gou, Herv{\\'e} and Grave, Edouard and Zeghidour, Neil",
        "title": "Moshi: a speech-text foundation model for real-time dialogue"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "spiritlm",
        "author": "Nguyen, Tu Anh and Muller, Benjamin and Yu, Bokai and Costa-Jussa, Marta R and Elbayad, Maha and Popuri, Sravya and Ropers, Christophe and Duquenne, Paul-Ambroise and Algayres, Robin and Mavlyutov, Ruslan and others",
        "title": "Spirit-lm: Interleaved spoken and written language model"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "twist",
        "author": "Hassid, Michael and Remez, Tal and Nguyen, Tu Anh and Gat, Itai and Conneau, Alexis and Kreuk, Felix and Copet, Jade and Defossez, Alexandre and Synnaeve, Gabriel and Dupoux, Emmanuel and others",
        "title": "Textually pretrained speech language models"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "cuervo2024scaling",
        "author": "Cuervo, Santiago and Marxer, Ricard",
        "title": "Scaling Properties of Speech Language Models"
      }
    ]
  }
]