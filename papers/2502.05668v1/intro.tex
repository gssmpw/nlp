Modern deep learning architectures are typically \emph{overparameterized} relative to the number of training samples, leading to infinitely many weight configurations that interpolate the training data. Traditional learning theory suggests such solutions should generalize poorly \citep{vapnik, belkin2019reconciling}. Yet, empirical evidence shows that interpolating solutions found through training often generalize well \citep{zhang2021understanding}.
This observation has led to the \emph{implicit bias} hypothesis, which posits that gradient-based optimization algorithms act as implicit regularizers, guiding models toward parameters that generalize well \citep{NeyshaburTS14,GunasekarLSS18}. Understanding this bias is key to explaining the success of deep learning and remains an active research area.

While a general theoretical characterization for deep neural networks remains out of reach, progress has been made in simplified settings. One such setting is the class of positively homogeneous neural networks (NNs), where a network $\Phi(\cdot; w)$ satisfies $\Phi(x; \lambda w) = \lambda^L \Phi(x; w)$ for any $\lambda > 0$, weights $w$, and input features $x$. This class encompasses feedforward NNs of depth $L$ without biases and with piecewise linear activation functions, such as MLPs and CNNs.
%Modern deep learning architectures are typically highly \emph{overparameterized} with respect to the number of training samples.
%While this implies that there is usually infinitely many solutions that perform well on the training data (i.e. the train loss is small or zero), empirically we often observe that solutions found during the training perform well on the unseen data.

%In recent years, this has lead to a belief, termed as an \emph{implicit bias} phenomenon, that the choice of a gradient-based algorithm acts as an implicit regularizer, enforcing the convergence to parameters that generalize well. Characterizing, if possible, such bias sheds light on the effectiveness of deep learning models and has been, in recent years, a subject of extensive research.

%While characterizing such bias in full generality seems for now an unattainable task, a number of researchers focused on simplified settings where some characterization is possible. One particularly fruitful setting is the case of positively homogeneous neural networks (NN). Denoting $\Phi(w;x)$ the output of such network, where $w$ stands for the parameter, $x$ for the input, it is characterized by the fact that for all $\lambda >0$ and all $w,x$, $\Phi(\lambda w; x) = \lambda^{L} \Phi(w;x)$. This class includes any feedforward or convolutional neural network, without biases, with $L$ being the depth of the NN.
In the spirit of early works by \cite{Lyu_Li_maxmargin,ji2020directional}, this paper focuses on binary classification problem with such networks. Given a labeled dataset $((x_i,y_i))_{i \leq n}$ in $\mathbb{R}^p \times \{-1, 1\}$ and a decreasing non-negative loss function $l$ (e.g., logistic or exponential loss), the objective is to minimize the empirical risk,
\begin{equation}\label{def:emp_risk_intro}
  \cL(w) \coloneqq \frac{1}{n}\sum_{i=1}^N l(y_i \Phi(x_i; w))\, .
\end{equation}
The quality of a prediction fora  given weight vector $w$ is measured by the margin,
\begin{equation}\label{def:margin_intro}
  \sm(w) \coloneqq \min_{1 \leq i \leq n} y_i \Phi(x_i;w)\, .
\end{equation}
Notably, the training set is correctly classified if $\sm(w){>}0$.
Two key observations follow. First, if some weights $w$ sastify $\sm(w){>}0$, then for any $\lambda{>}1$, $\cL(\lambda w){<}\cL(w)$. Consequently, $\inf_{w}\cL(w) = 0$, and reaching this infimum requires the norm of the weights  to grow to infinity. Second, due to homogeneity, prediction quality depends only on the direction $\bar{w} := w / \norm{w}$. This naturally raises the question of how $\bar{w}$ evolves during training and what properties characterize its limit points.

A step toward answering this question was taken by \cite{Lyu_Li_maxmargin}, who analyzed the dynamics of subgradient flow (GF) and, under additional smoothness assumptions, gradient descent (GD) on the empirical risk. Assuming that at some point during training $\sm(w) > 0$ (i.e., the training error is zero), they showed that the parameter norm grows to infinity and that the direction $\bar{w}$ converges to the set of \emph{KKT points} of a constrained optimization problem related to margin maximization. Later, \cite{ji2020directional} proved  $\bar{w}$ actually converges to a \emph{unique} KKT point considering the GF dynamics. These results highlights a clear implicit bias: even after achieving zero training error, the dynamics continue evolving until reaching a form of optimality in the normalized margin.

%While the works of \citet{Lyu_Li_maxmargin,ji2020directional} provide key inspiration for this paper, they leave an important open question: 
While the works of \cite{Lyu_Li_maxmargin,ji2020directional} provide key insights into the training dynamics of homogeneous networks, they leave open an important question: does the same implicit bias phenomenon extend to (stochastic) subgradient descent (SGD)? GF describes the limiting continuous-time dynamics of SGD, but the discrete-time analysis in \cite{Lyu_Li_maxmargin} does not account for stochasticity or nonsmoothness. As a result, their framework does not extend to neural networks with $\ReLU$ or $\LeakyReLU$ activations, which are widely used in practice \citep{goodfellow2016deep,fleuret2023little}.

\paragraph{Main contributions} We address this open question by identifying a setting in which the limit points of the normalized directions $u_k := w_k / \norm{w_k}$ can be characterized, where $(w_k)$ is generated by SGD with a \emph{constant step-size} on the empirical risk~\eqref{def:emp_risk_intro} for $L$-homogeneous \emph{nonsmooth} networks. Our analysis covers both the exponential and logistic losses. Since we are interested in the limit points of the normalized directions, we focus the late-stage training dynamics, \textit{i.e.} after the training data is correctly classified. We formalize this stage as the regime in which the normalized margin remains positive (see Assumption~\ref{hyp:marg_lowb} and related discussions in Section~\ref{sec:sett}). Our main contributions can be summarized as follows:

\begin{itemize}
\item[1.] We show that the dynamics of $(u_k)$ can be interpreted as a Euler-like discretization (or stochastic approximation) of a 
%``reversed gradient flow inclusion" 
gradient flow inclusion, $\dot{\su}(t) \in  \bar{D}_s(\su(t))$, where $\bar{D}_s$ is a conservative set-valued field of the margin \emph{restricted} to the unit sphere $\sm_{|\bbS^{d-1}}$. This gradient-like object, recently introduced by \cite{bolte2021conservative}, naturally appears in the analysis.
 In particular, we show in Proposition~\ref{prop:stoch_approx_exp_log} that
\begin{equation}\label{eq:sto_appro_intro}
  u_{k+1} \in  u_k + \bgamma_k \bar{D}_s(u_k) + \bgamma_k e_k\, ,
\end{equation}
where $(e_k)$ is a sequence of (stochastic) perturbations, that diminishes as $k \rightarrow + \infty$.
\item[2.] As a consequence, leveraging recent results on stochastic approximations of differential inclusions (\cite{benaim2006dynamics,dav-dru-kak-lee-19,bolte2021conservative}), we establish in Theorem~\ref{thm:main} that $(u_k)$ converges to the set of $\bar{D}_s$-critical points $\cZ_s := \{u \in \bbS^{d-1}: \bar{D}_s(u) = 0 \}$. In our setting, $\cZ_s$ coincides \emph{exactly} with the KKT points of \cite{Lyu_Li_maxmargin}.
\end{itemize}
From a mathematical perspective, our techniques differ from those of \cite{Lyu_Li_maxmargin,ji2020directional}. 
Rather than constructing a smooth approximation of the margin, we directly consider the margin and interpret the dynamics of the normalized SGD iterates as a stochastic approximation. This, in contrast to these previous works, allows us to incorporate both stochasticity and nonsmoothness in our analysis. Finally, although we focus on exponential-type losses, we also present a more general setting in Appendix~\ref{app:gen_sett} where our proof techniques apply.

\paragraph{Organization of the paper} We begin by reviewing related works in Section~\ref{sec:rw}. Then, in Section~\ref{sec:preli}, we introduce key technical concepts for nonsmooth analysis. Section~\ref{sec:sett} presents the main setting we consider. Our main result and its proof are detailed in Sections~\ref{sec:main} and~\ref{pf:sto_app_explog}, respectively. Finally, additional proofs and supplementary materials are provided in the appendices.