\label{sec:cons_field}

%In this section, we provide some background on semialgebraic functions and conservative set-valued field, which we both prove useful for the analysis of non-smooth, non-convex objectives. 

Although feedforward and convolutional NNs with ReLU or LeakyReLU activations are inherently nonsmooth—making their analysis more challenging—they exhibit some form of regularity. In particular, they belong to the important class of semialgebraic functions, which we now introduce.

%While functions encountered in optimization might be nonsmooth they often exhibit a high degree of regularity. In particular, the output of a feedforward or a CNN neural network with $\ReLU$ or $\LeakyReLU$ activations is an example of a semialgebraic function.

\paragraph{Semialgebraic functions.}
A set $A \subset \bbR^d$ is semialgebraic if it can be expressed as a finite union of intersections of sets of the form $\{ Q(x) \leq 0\}$, where $Q : \bbR^d \rightarrow \bbR$ is a polynomial. A (set-valued) function is semialgebraic if its graph is a semialgebraic set. Examples include piecewise polynomials, rational functions and functions such as $x \mapsto x^{q}$, where $q$ is any rational number.

Semialgebraic functions are closed under composition and remain stable under a finite number of operations, including  $\{+, -, \times, \max, \min, \circ, \circ^{-1}\}$. From these properties it is clear that feedforward and convolutional NNs with piecewise linear activation functions such as $\ReLU$ are semialgebraic functions.  Notably, the class of semialgebraic functions is a particular case of functions definable in an o-minimal structure. We discuss o-minimality in Appendix~\ref{app:omin} and emphasize that every statement of the paper which involves semialgebraicity might be replaced by definability.

Although semialgebraic functions exhibit some regularity, they are not necessarily smooth. To analyze their behavior in optimization settings, we rely on a more general notion of differentiation--- \emph{conservative set-valued fields}, which we now define.


\paragraph{Conservative set-valued fields.}
Conservative set-valued fields were introduced in \cite{bolte2021conservative} as an elegant framework for describing the output of automatic differentiation provided by numerical libraries such as TensorFlow and PyTorch (\cite{tensorflow2015-whitepaper,paszke2017automatic}). 
They serve as a fundamental tool for analyzing first-order methods in non-smooth settings (see, e.g., \cite{bolte2023subgradient}).

\begin{definition}[\cite{bolte2021conservative}]\label{def:cons_f}
  A graph-closed, locally bounded set-valued map\footnote{A map $D: \bbR^d \rightrightarrows \bbR^d$ is set-valued if, for every $w \in \bbR^d$, $D(w)$ is a set in $\bbR^d$. See Appendix~\ref{app:interp} for various definitions related to such maps.} $D: \bbR^d \rightrightarrows \bbR^d$ with nonempty values is a \emph{conservative field} for the \emph{potential} $\cL: \bbR^d \rightarrow \bbR$, if for any absolutely continuous curve $\sw: [0, 1] \rightarrow \bbR^d$, it holds for almost every $t \in [0,1]$ that 
  \begin{equation*}
    \frac{\dif}{\dif t} \cL(\sw(t)) = \scalarp{v}{\dot{\sw}(t)} \quad \textrm{ for all $v \in D(\sw(t))$} \, .
  \end{equation*} 
  Functions that are potentials of some conservative field are called \emph{path differentiable}.
\end{definition}
Given a continuously differentiable potential $\cL$, the set-valued map $w \rightrightarrows \{ \nabla \cL(w)\}$ is in obvious example of a conservative field. 
For semialgebraic functions, two important examples of conservative set-valued fields are the Clarke subgradient and the output of backpropagation.

\paragraph{Clarke subgradients.}
Semialgebraic functions always admit a conservative field, the Clarke subgradient. This result, initially proven in \cite{drusvyatskiy2015curves}, builds on the work of \cite{bolte2007clarke}.

\begin{definition}[Clarke subgradient {\cite{cla-led-ste-wol-livre98}}]
  Let $\cL: \bbR^d \rightarrow \bbR$ be a locally Lipschitz function. The Clarke subgradient of $\cL$ at $w \in\bbR^d$ is defined as
  \begin{equation}\label{eq:def_clarke}
    \partial \cL(w) := \conv \{ v \in \bbR^d: \textrm{ there exist $w_k \rightarrow w$, with $\cL$ differentiable at $w_k$ and $\nabla \cL(w_k) \rightarrow v$}\}\, ,
  \end{equation}
  where we denote by $\conv(A)$ the convex closure of a set $A$.
\end{definition}
For semialgebraic function $\cL$, $\partial \cL$ is the minimal convex-valued semialgebraic conservative field (\cite{drusvyatskiy2015curves, bolte2021conservative}). That is, for any conservative set-valued field $D$ with semialgebraic potential $\cL$, it holds for all $w$ that $\partial \cL(w) \subset \conv(D(w))$\footnote{Note that if $D$ is a conservative field, then the mapping $w \rightrightarrows \conv D(w)$ is also conservative.}. 

\begin{comment}
  For semialgebraicfunction, $\partial \cL$ is the minimal convex-valued conservative field. 
\begin{proposition}[\cite{drusvyatskiy2015curves, dav-dru-kak-lee-19,bolte2021conservative}]
  Let $\cL$ be semialgebraic potential of $D: \bbR^{d} \rightrightarrows \bbR^d$. Then, $\partial \cL$ is a semialgebraic conservative field for $\cL$ and for all $w \in \bbR^d$, 
  \begin{equation*}
    \partial \cL(w) \subset \conv D(w)\, .
  \end{equation*}
\end{proposition}
\end{comment}

\paragraph{Backpropagation.} When applied to nonsmooth functions, the backpropagation algorithm formally applies the chain rule, replacing the derivative with an element of the Clarke subgradient when necessary. Although the Clarke subgradient of a composition $f \circ g$ is not necessarily equal to $\partial f \times \partial g$, the product of conservative mappings remains conservative (see Proposition~\ref{pr:comp_cons} in Appendix~\ref{app:conserv}).
As a result, \cite[Section 5]{bolte2021conservative} show that if a (possibly nonsmooth) semialgebraic function is defined through a computational graph (such as in a neural network), backpropagation produces an element of a conservative field. Consequently, the training a neural network via (S)GD is actually a (stochastic) conservative field descent.