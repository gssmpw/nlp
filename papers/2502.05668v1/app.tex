\appendix

\newpage

\section{o-minimal structures}\label{app:omin}

In every statement of this paper \emph{semialgebraic} can be replaced by \emph{definable} in a fixed \emph{o-minimal structure}. We collect here some useful facts about definable sets and maps. In particular, Proposition~\ref{prop:var_strat_cons}, that provides a variational description of a definable conservative set-valued field of a definable potential, will be used in Appendix~\ref{pf:main_th} to prove Theorem~\ref{thm:main}.

For more details on o-minimal structure we refer to the monographs \cite{cos02,van1998tame,van96}. A nice review of their importance in optimization is \cite{iof08}.

The definition of an o-minimal structure is inspired by properties that are satisfied by semialgebraic sets.
\begin{definition}
  We say that $\cO:=(\cO_n)$, where for each $n \in \bbN$, $\cO_n$ is a collection of sets in $\bbR^n$, is an o-minimal structure if the following holds. 
  \begin{enumerate}[label=\roman*)]
    \item If $Q: \bbR^n \rightarrow \bbR$ is a polynomial, then $\{x \in \bbR^n : Q(x) = 0 \} \in \cO_n$.
    \item For each $n \in \bbN$, $\cO_n$ is a boolean algebra: if $A, B \in \cO_n$, then $A \cup B, A \cap B$ and $A^c$ are in $\cO_n$.
    \item If $A \in \cO_n$ and $B \in \cO_m$, then $A \times B \in \cO_{n+m}$.
    \item If $A \in \cO_{n+1}$, then the projection of $A$ onto its first $n$ coordinates is in $\cO_n$.
    \item Every element of $\cO_1$ is exactly a finite union of intervals and points of $\bbR$. 
  \end{enumerate}
\end{definition}



Sets contained in $\cO$ are called \emph{definable}. We call a map
$f : \bbR^d \rightarrow \bbR^m$ definable if its graph is definable. Similarly, $D: \bbR^d \rightrightarrows \bbR^d$ is definable if $\Graph D = \{(w,v): v \in D(w) \}$ is definable.
Definable sets and maps have remarkable stability
properties. For instance, if $f$ and $A$ are definable, then $f(A)$
and $f^{-1}(A)$ are definable and definability is stable by most of the common operators such as  $\{+, -, \times, \circ, \circ^{-1}\}$. Let us look at some examples of o-minimal structures.

\textbf{Semialgebraic.} Semialgebraic sets form an o-minimal structure. This follows from the celebrated result of Tarski \cite{tarski1951decision}.

\textbf{Globally subanalytic.} There is an o-minimal structure that contains, for every $n \in \bbN$, sets of the form $\{ (x,t) : t = f(x)\}$, where $f : [-1, 1]^n \rightarrow \bbR$ is an analytic function. This comes from the fact that subanalytic sets are stable by projection, which was established by Gabrielov \cite{gabrielov1968projections, gabrielov1996complements}. The sets belonging to this structure are called globally subanalytic (see \cite{bier_semi_sub} for more details).\\
\textbf{Log-exp.} There is an o-minimal structure that contains, semialgebraic sets, globally sub-analytic sets as well as the graph of the exponential
and the logarithm (see \cite{wilkie1996model, van1994elementary}).


With these examples in mind it is usually easy to verify that a function is definable. This will be the case as soon as the function is constructed by a finite number of definable operations on definable functions. From this, we see that \emph{most of neural networks architectures} are definable in the structure \emph{Log-exp}.

Let us record here a striking fact. The domain of a definable function can be always partitioned in manifolds such that $f$ is differentiable on each set of this partition. Before stating this result we briefly introduce to the reader some basics of differential geometry. We refer to the monographs \cite{Lafontaine_2015,boumal2023intromanifolds} for a more detailed introduction on these notions.

\paragraph{Submanifolds, tangent spaces and Riemannian gradients.}


A set $\cX \subset \bbR^d$ is said to be a $k$-dimensional $C^p$-manifold if for every $x \in \cX$, there is a neighborhood $\cU \subset \bbR^d$ of $x$ and a $C^p$ function $g: \cU \rightarrow \bbR^{d-k}$ such that the Jacobian of $g$ at every point of $\cU$ is of full rank. The tangent space of $\cX$ at $x$ is $\cT_{\cX}(x) =\ker \Jac g(x)$. Equivalently, it is the set of vectors $v \in \bbR^d$ such that there is $\varepsilon >0$ and a $C^1$ curve $\su : (- \varepsilon, \varepsilon) \rightarrow \cX$, such that $(\su(0), \dot{\su}(0)) = (x,v)$.

We say that a function $f: \cX \rightarrow \bbR$ is $C^p$ if around every $x$ there is a neighborhood $\cU \subset \bbR^d$ and a $C^p$ function $\tilde{f} : \cU\rightarrow \bbR$ such that $\tilde{f}_{|\cU} = f_{|\cU}$. For such $f$ and $x \in \cX$, the Riemannian gradient (here, we implicitly induce the Riemannian structure from the ambient space) of $f$ at $x$, is
\begin{equation}\label{eqdef:riem_grad}
\nabla_{\cX}f(x) :=   P_{\cT_{\cX}(x)} \nabla \tilde{f}(x)\, ,
\end{equation} 
where $P_{\cT_{\cX}(x)}$ is the orthogonal projection onto $\cT_{\cX}(x)$. This definition is independent of the smooth representative $\tilde{f}$.


For the following two propositions we fix a definable structure $\cO$ (e.g., semialgebraic sets) and definable will mean definable in $\cO$.
\begin{proposition}[{\cite[4.8]{van1998tame}}]\label{prop:def_strat}
  Consider  a definable $f: \bbR^d \rightarrow \bbR$. For every $p>0$, there is $(\cX_i)_{1 \leq i \leq l}$ a finite partition \footnote{This partition also satisfies some important properties and is usually called a stratification.} of $\bbR^d$ into $C^p$ submanifolds such that for every $i$, $f_{|\cX_i}$ is $C^p$, in the sense of differential geometry. Moreover, if $\cM \subset \bbR^d$ is definable, then this partition can be chosen such that there is $\cI \subset \{1, \ldots, l \}$ for which $\cM = \bigcup_{i \in \cI} \cX_i$.
\end{proposition}
Furthermore, the partition can be chosen in a way that along the tangent directions $D(w)$ is simply the Riemannian gradient of $f$. This will be useful in our proof of Theorem~\ref{thm:main}.
\begin{proposition}[{\cite[Theorem 4]{bolte2021conservative}}]\label{prop:var_strat_cons}
  Consider a definable, locally Lipschitz continuous $f: \bbR^d \rightarrow \bbR$ a potential of a definable conservative set-valued field $D: \bbR^d \rightrightarrows \bbR^d$. For any $p >0$, there is a partition $(\cX_i)_{1 \leq i \leq l}$ such that the result of Proposition~\ref{prop:def_strat} holds and, moreover, for any $w \in \cX_i$, 
  \begin{equation*}
    P_{\cT_{\cX_i}(w)} D(w) = \{\nabla_{\cX_i} f(w)\}\footnote{Note the similarity with~\eqref{eqdef:riem_grad}.}\, .
  \end{equation*}
  Moreover, if $\cM \subset \bbR^d$ is definable, then this partition can be chosen such that there is $\cI \subset \{1, \ldots, l \}$, for which $\cM = \bigcup_{i \in \cI} \cX_i$.
\end{proposition}

\begin{remark}
  The last point of Proposition~\ref{prop:var_strat_cons} is not formally stated in \cite[Theorem 4]{bolte2021conservative}. Nevertheless, in its proof, the first stage of the construction of $(\cX_i)$ is based on Proposition~\ref{prop:def_strat}. Thus, at this stage we can use the last point of Proposition~\ref{prop:def_strat} (i.e. the one of \cite[4.8]{van1998tame}) and obtain the stated result.
\end{remark}


\section{Conservative mappings}\label{app:conserv}

The purpose of this section is to prove the fact that \emph{i)}$\bar{D}$ from Equation~\eqref{def:riem_cons} is a conservative set-valued field of $\sm$, \emph{ii)} Lemma~\ref{lm:loc_max}.


First, we introduce a generalization of conservative fields to vector-valued functions.

\begin{definition}[\cite{bolte2021conservative}]\label{def:cons_map}
  For a locally Lipschitz function $f = (f_1, \ldots, f_m): \bbR^d \rightarrow \bbR^m$, a set-valued map $J: \bbR^d \rightrightarrows \bbR^{m \times d}$ is said to be a \emph{conservative mapping} for $f$ if for every $ 1\leq i \leq m$ the $i$-th row of $J$ is a conservative field for $f_i$. 
\end{definition}


One of the issue of the Clarke subgradient is that it is not closed under composition. That is to say, if $f, g : \bbR \rightarrow \bbR$ are locally Lipschitz we do not necessarily have $\partial (f\circ g) = \partial f \times \partial g$. On the contrast, product of conservative mapping remains conservative. This is the main reason why the backpropagation algorithm outputs an element of a conservative set-valued field.
\begin{proposition}[{\cite[Lemma 5]{bolte2021conservative}}]\label{pr:comp_cons}
  Consider $f : \bbR^d \rightarrow \bbR^m$, $g: \bbR^m \rightarrow \bbR^{l}$ two locally Lipschitz continuous functions with $D_f: \bbR^{d} \rightrightarrows \bbR^{m \times d}$, $D_g : \bbR^m \rightrightarrows \bbR^{l \times m}$ two corresponding conservative mappings. The set-valued map
  \begin{equation*}
    D_{g \circ f} : w \mapsto \{J \in \bbR^{l \times m} : \textrm{$J = J_2 J_1$, with $J_f \in D_f(w)$ and $J_g \in D_{g}(f(w))$}\}\, 
  \end{equation*} 
  is a conservative mapping of $g \circ f$.
\end{proposition}

\begin{comment}
\begin{lemma}
    Let $\cL_1, \ldots, \cL_n : \bbR^d \rightarrow \bbR$ be locally Lipschitz continuous potentials with conservative gradients $D_1, \ldots, D_n: \bbR^d \rightrightarrows \bbR^d$. Then $\sum_{i=1}^nD_i$ is a conservative gradient of $\sum_{i=1}^n\cL_i$
\end{lemma}
\begin{proof}
  Immediate from Definition~\ref{def:cons_f}.
\end{proof}
\end{comment}

 In the context of the paper, $p_1, \ldots, p_n : \bbR^d \rightarrow \bbR$ are semialgebraic potentials of semialgebraic conservative set-valued fields $D_1, \ldots, D_n$. Recall that $\sm(w) = \min_{i} p_i(w)$ and the definition $\bar{D}$ in~\eqref{eq:avg_consfiel}. We now have the tools to prove that $\bar{D}$ is a conservative set-valued field of $\sm$.

\begin{lemma}\label{lm:max_consgrad}
  The map $\bar{D}: \bbR^d \rightrightarrows \bbR^d$ is a semialgebraic conservative set-valued field for the potential $\sm$.
\end{lemma}
\begin{proof}
  The fact that $\Graph\bar{D}$ is semialgebraic comes from the fact that it is constructed from finite semialgebraic operations involving only semialgebraic sets. To prove that $\bar{D}$ is a conservative set-valued field note that $\sm$ can be written as a composition of semialgebraic functions:
  \begin{equation*}
    w \overset{\varphi_1}{\mapsto} (p_1(w), \ldots, p_n(w)) \overset{\varphi_2}{\mapsto} \min(p_1(w), \ldots, p_n(w))
  \end{equation*}
   Then, $D_p(w) = [D_1(w), \ldots, D_n(w)]^{\top}$ is a conservative mapping of $\varphi_1$ and $(x_1, \ldots, x_n) \rightrightarrows \conv\{ e_i : x_i = \min(x_1, \ldots, x_n) \}$, where $e_i \in \bbR^n$ is the $i$-the element of the canonical basis, is the Clarke subgradient (and thus a conservative gradient) of $\varphi_2$. The claim follows from Proposition~\ref{pr:comp_cons} and the fact that conservativity is stable by convexity.
\end{proof}

\begin{remark}\label{rmk:max_subg}
  Even if $D_1, \ldots, D_n$ are Clarke subgradients, $\bar{D}$ is not necessarily a Clarke subgradient. For instance, if $n=2$, $p_1(w) = \min(0,w)$ and $p_2(w) = \min(0, -w)$, then $\sm(w) = 0$ and for any $w \neq 0$, $\bar{D}(w) = \{0\}$. Nevertheless, $\bar{D}(0) = [-1,1]$. Thus, conservative set-valued fields appear naturally in the analysis of our problem, independently of the use of backpropagation.
\end{remark}

We finish this section by a proof of Lemma~\ref{lm:loc_max}. Note that 
$\bbS^{d-1} = \{ u \in\bbR^d: \norm{u} =1\} = g^{-1}(0)$, with $g(w) = \norm{w}^2$. Therefore, $\bbS^{d-1}$ is a $d-1$ dimensional $C^{\infty}$-manifold and for every $u \in \bbS^{d-1}$, 
\begin{equation}\label{eq:tang_sphere}
\cT_{\bbS^{d-1}}(u) = \{v - \scalarp{v}{u} : v \in \bbR^d \}\, .
\end{equation}


\begin{proof}[Proof of Lemma~\ref{lm:loc_max}]
  Assume the contrary and consider $v_s \in \argmin \{ \norm{v_s'}: v_s' \in \bar{D}_s(u^*)\}$. Since $v_s$ is in the tangent space of $\bbS^{d-1}$ at $u^*$, there is a $C^1$ curve $\su: (-\varepsilon, \varepsilon) \rightarrow \bbS^{d-1}$ such that $\su(0) = u^*$ and $\dot{\su}(0) = v_s$. Moreover, since $\bar{D}_s(u^*)$ is convex and $v_s$ is its element of minimal norm, for any $v_s'\in D(u^*)$, $\scalarp{v_s}{v_s'} \geq \norm{v_s}^2 >0$. 
  
  Since $\Graph \bar{D}_s$ is closed, it also holds, for $t \geq 0$ small enough, that for any $v_s' \in \bar{D}_s(\su(t))$, $2\scalarp{v_s}{v_s'} \geq \norm{v_s}^2$ and, thus, for $t$ small enough, $4\scalarp{v_s'}{\dot{\su}(t)} \geq \norm{v_s}^2$. 
  
  In particular, for almost every $t \in [0,\delta]$, for $\delta$ small enough, and every $v \in \bar{D}(\su(t))$, 
  \begin{equation*}
    \frac{\dif}{\dif t}\sm(\su(t)) = \scalarp{v}{\dot{\su}(t)} = \scalarp{ v - \scalarp{v}{\su(t)}\su(t)}{\dot{\su}(t)} \geq \frac{1}{4}\norm{v_s}^2 >0 \, ,
  \end{equation*}
  where we have used that $v - \scalarp{v}{\su(t)}\su(t) \in \bar{D}_s(\su(t))$ and that $\scalarp{\su(t)}{\dot{\su}(t)} = 0$.
  Therefore, $\sm$ strictly increase on $[0, \delta]$, which contradicts the fact that $u^*$ is a local maximum.
\end{proof}




\section{Discretization of differential inclusions}\label{app:interp}

\paragraph{Differential inclusions.}
Consider $B \subset \bbR^d$. We say that $\sH$ is a set-valued map from $B$ to $\bbR^d$, denoted $\sH: B \rightrightarrows \bbR^d$, if for every $w \in B$, $\sH(w)$ is a subset of $\bbR^d$. We say that $\sH$ is graph-closed if $\Graph H := \{(w,v): v \in \sH(w) \}$ is closed. It is said to have nonempty (respectively convex) values if for every $w \in B$, $\sH(w)$ is nonempty (respectively convex). It is said to be locally bounded, if every $w \in \bbR^d$ admits a neighborhood $\cU$ of $w$ and $M>0$, such that for every $w' \in \cU$, and $v \in \sH(w')$, $\norm{v} \leq M$.

\paragraph{Differential inclusions and Lyapunov functions.}
To every such $\sH$ we can associate a differential inclusion (DI):
\begin{equation}\label{eq:DI}
    \dot{\su}(t) \in \sH(\su(t)) \, .
\end{equation}
We say that $\su: \bbR_{+} \rightarrow B$ is a solution to~\eqref{eq:DI}, if Equation~\eqref{eq:DI} is satisfied for almost every $t \geq 0$.
A continuous function $\Lambda: B \rightarrow \bbR$ is said to be a Lyapunov function of~\eqref{eq:DI} for a set $\cZ \subset B$, if for every solution $\su$ of~\eqref{eq:DI},
\begin{equation*}
  \Lambda(\su(t)) \leq \Lambda(\su(0))\, ,
\end{equation*}
with strict inequality as soon as $\su(0) \not \in \cZ$. 

In the context of our paper, we are of course interested in the case where $B  = \bbS^{d-1}$ and $\sH = \bar{D}_s$. In such case, as explained before the statement of Theorem~\ref{thm:main}, $-\sm$ is a Lyapunov function for the set $\cZ_s$.


\paragraph{Discretization of differential inclusions.}
Consider a $\bbR^d$-valued sequence $(u_k)$, satisfying the following recursion:
\begin{equation*}
  u_{k+1} = u_k + \gamma_k v_k + \gamma_k e_k\, ,
\end{equation*}
where $(\gamma_k)$ is a sequence of positive stepsizes, where $v_k,e_k\in \bbR^d$. In practice, $v_k$ will be chosen close to an element of $\sH(u_k)$, with $\sH$ some set-valued map (in our setting $\sH = \bar{D}_s$), and $(e_k)$ will represent some (stochastic) perturbations that will vanish when $k \rightarrow + \infty$. Hence, one can view this algorithm as an Euler-like discretization of the DI associated to $\sH$. 

Under mild assumptions, under a presence of a Lyapunov function, we can characterize the accumulation points of $(u_k)$.
The first such result was established in the seminal work of \cite{benaim_05_DI_1}.
Similar statements were lately proved in \cite{duchi2018stochastic,borkar2008stochastic,dav-dru-kak-lee-19}. Here we use a version of the result proved in \cite[Theorem 3.2]{dav-dru-kak-lee-19}. 

To state the proposition, for every $T>0$ and $k \in \bbN$, we define
\begin{equation*}
  \sn(k, T) = \sup \left\{ l \geq k: \sum_{i=k}^l \gamma_i \leq T \right\}\, .
\end{equation*}

\begin{proposition}\label{pr:stoch_approx_our}
  Let $B \subset \bbR^d$ be closed and let $\sH: B \rightrightarrows \bbR^d$ be a graph-closed set-valued map, with convex and nonempty values. Assume the following.
  \begin{enumerate}[label=\roman*)]
    \item The sequences $(u_k), (v_k)$ are bounded and any accumulation point of $(u_k)$ lies in $B$.
    \item $(\gamma_k)$ is a sequence of positive numbers such that $\sum_{k} \gamma_k = +\infty$ and $\gamma_k \rightarrow 0$.
    \item\label{hyp:perturb_zero} For every $T>0$, 
    \begin{equation*}
     \lim_{k \rightarrow + \infty} \sup\left\{ \norm{\sum_{i=k}^l \gamma_i e_i} : k \leq l \leq \sn(k,T)\right\}  = 0 \, .
    \end{equation*}
    \item \label{hyp:drus_lyap} There is $\Lambda : B \rightarrow \bbR$, a Lyapunov function associated with~\eqref{eq:DI} and the set $\cZ = \{u \in B: 0 \in \sH(u)\}$, such that $\Lambda(\cZ)$ is of empty interior.
     \item\label{hyp:drus_conv} For any unbounded sequence $(k_j)$, such that $(u_{k_j})$ converges to some $u\in B$, it holds that 
    \begin{equation}\label{eq:conv_setv_seq}
     \lim_{j \rightarrow + \infty} \dist(\sH(u), v_{k_j}) =   0
    \end{equation}
   \end{enumerate}
Then the limit points of $(u_k)$ are in $\cZ$ and the sequence $\Lambda(u_k)$ converges.
\end{proposition}
The statement of \cite[Theorem 3.2]{dav-dru-kak-lee-19} have slightly different assumptions on $\sH$, $(e_k)$ and $(\gamma_k)$, and we devote the end of this section to describe how to adapt their proof to obtain our statement.

 The only difference between Proposition~\ref{pr:stoch_approx_our} and \cite[Theorem 3.2]{dav-dru-kak-lee-19} are the following.

\begin{enumerate}
  \item In \cite[Theorem 3.2]{dav-dru-kak-lee-19} there are no assumptions on $\sH$ and~\eqref{eq:conv_setv_seq} is replaced by 
  \begin{equation*}
   \lim_{N \rightarrow + \infty} \dist\left(\sH(u), \frac{1}{N} \sum_{j=1}^N v_{k_j}\right) = 0\, .
  \end{equation*}
  \item In \cite[Theorem 3.2]{dav-dru-kak-lee-19} there is a requirement that $\sum_k \gamma_k^2 < + \infty$ and that $\sum_{k}\gamma_k e_k$ converges.
\end{enumerate}

The only part in the proof of \cite[Theorem 3.2]{dav-dru-kak-lee-19} where these two points are needed are in their use of \cite[Theorem 3.1]{dav-dru-kak-lee-19} which corresponds to \cite[Theorem 3.7]{duchi2018stochastic}. To state the result let us first define $\tau_k = \sum_{i=0}^k \gamma_i$ and the linearly interpolated processes $\sU$ as 
 \begin{equation*}
  \sU(t) = u_k + \frac{t - \tau_k}{\gamma_{k+1}} (u_{k+1} - u_k) \quad \textrm{ if $t \in [\tau_k, \tau_{k+1}]$}\, .
 \end{equation*}
 \begin{proposition}[ {\cite[Theorem 4.2]{benaim_05_DI_1}\cite[Theorem 3.7]{duchi2018stochastic}}]\label{prop:interp_proc}
  \phantom{=}\\For any $T>0$ and any sequence $t_k \rightarrow + \infty$, we can extract a subsequence $(k_j)$ such that there is a continuous $\su : [0,T] \rightarrow \bbR^d$, for which, 
  \begin{equation*}
    \lim_{j \rightarrow + \infty}\sup_{h \in [0, T]} \norm{\sU(t_{k_j} +h) -\su(h)} = 0 \, , 
  \end{equation*}
  and $\su$ is a solution to~\eqref{eq:DI}.
 \end{proposition}
 This result holds under our assumptions. The idea of the proof is standard and goes back to \cite[Proof of Theorem 4.2]{benaim_05_DI_1}.
 
 Indeed, define piecewise constant processes $\sV, \sE : \bbR_{+} \rightarrow \bbR^d$ as
 \begin{equation*}
 \textrm{for $t \in [\tau_k, \tau_{k+1})$}\, , \quad  \sV(t) = v_k\, ,  \quad \sE(t) = e_{k}\, .
 \end{equation*}
Note that for any $t, h>0$,
 \begin{equation*}
  \sU(t+h) = \sU(t) + \int_{t}^{t+h} (\sV(t')+\sE(t')) \dif t'\, . 
 \end{equation*}
Moreover, from our assumptions on $(\gamma_k), (e_k)$, it holds that
\begin{equation*}
 \lim_{t \rightarrow + \infty} \norm{\sup_{h \in [0,T]} \int_{t}^{t+h} (\sE(t')) \dif t'} = 0 \, .
\end{equation*}
 Therefore, fixing $t_{k} \rightarrow + \infty$, and proceeding as in \cite[Proof of Theorem 4.2]{benaim_05_DI_1}, there is $\sv, \su : [0, T] \rightarrow + \infty$ and an extracted sequence $(k_j)$ such that
 \begin{equation*}
 \lim_{j \rightarrow + \infty} \sup_{h \in [0,T]}\norm{\sU(t_{k_j} + h) - \su(h)} = 0 \, ,
 \end{equation*}
 and for any $h \in [0,T]$,
 \begin{equation*}
  \su(h) = \su(0) + \int_{0}^h \sv(t') \dif t'\, ,
 \end{equation*}
 Moreover, for almost every $t' \in[0,T]$, there is a sequence $(\tilde{v}_{k_j})$ such that for each $j$, $\tilde{v}_{k_j}$ is a finite convex combination of elements in $\{\sV(t_{k_{j'}} + t'): j' \geq j \}$ and such that
\begin{equation*}
  \sv(t') = \lim_{j \rightarrow + \infty} \tilde{v}_{k_j} \in \sH(\su(t'))\, ,
\end{equation*}
where the last inclusion comes from~\eqref{hyp:drus_conv} and the fact that $\sH$ is convex-valued.
Consequently, we obtain for almost every $t' \in [0,T]$, $\dot{\su}(t') = \sv(t')\in  \sH(\su(t'))$, which shows the statement of Proposition~\ref{prop:interp_proc} and thus the one of Proposition~\ref{pr:stoch_approx_our}.


\section{Proof of Theorem~\ref{thm:main}}\label{pf:main_th}
To obtain Theorem~\ref{thm:main}, we only need to check the assumptions of Proposition~\ref{pr:stoch_approx_our}, with $\sH = \bar{D}_s$, $\Lambda = -\sm$, $v_k = \bg_k^s$ and $e_k = \eta_{k+1} + \gamma_k r_k$.

\paragraph{Boundedness of $(u_k)$ and $(\bg_k^s)$.} For each $k$, $\norm{u_k} = 1$. The fact that $(\bg_k^s)$ is bounded comes from the fact that $\lim_{k}\dist(\bar{D}_s(\bbS^{d-1}), g_k^s) = 0$ (or we could already see it from the way $\bg_k^s$ is constructed in the proof of Proposition~\ref{prop:stoch_approx_exp_log}).
\paragraph{Assumption on $(\bg_k^s)$ and $(\bgamma_k)$.} Immediate by Proposition~\ref{prop:stoch_approx_exp_log}.

\paragraph{Assumption on $\bar{D}_s$.} As established before Proposition~\ref{prop:stoch_approx_exp_log}, $-\sm$ is a Lyapunov function to the DI associated with $D_s$ and the set $\cZ_s$. We now prove that $\sm(\cZ_s)$ is of zero-measure. It is a Sard's type result and is a simple adaptation of {\cite[Theorem 5.]{bolte2021conservative}}.
  

  We first apply Proposition~\ref{prop:var_strat_cons} to $\sm: \bbR^d \rightarrow \bbR$, $\bar{D}$ and $\cM = \bbS^{d-1}$, with $p \geq d$, obtaining $(\cX_i)$ a partition of $\bbR^d$ into $C^p$ manifolds that is compatible with $\bbS^{d-1}$. Then, noting that for any $u \in \cX_i \subset \bbS^{d-1}$, it holds that $\cT_{\cX_i}(u) \subset \cT_{\bbS^{d-1}}(u) = \{v - \scalarp{v}{u} : v \in \bbR^d \}$, we obtain
  \begin{equation*}
   \{ \nabla_{\cX_i} \sm(u)\} = P_{\cT_{\cX_i}(u)}(\bar{D}(u)) =P_{\cT_{\cX_i}(u)}\circ P_{\cT_{\bbS^{d-1}}(u)}(\bar{D}(u)) = P_{\cT_{\cX_i}(u)}(\bar{D}_s(u))\, ,
  \end{equation*}
   which implies
   \begin{equation*}
     \cZ_s \subset \bigcup_{i \in \cI} \{ u \in \cX_i: \nabla_{\cX_i} \sm(u) = 0\}\, .
   \end{equation*}
   Hence,
   \begin{equation*}
     \sm(\cZ_z) \subset \bigcup_{i \in \cI} \sm_{|\cX_i}\left(\{ w \in \cX_i: \nabla_{\cX_i} \sm(w) = 0\}\right)\, .
   \end{equation*}
   By Sard's theorem (\cite{sard1942measure}) every set in the union has zero-measure. Therefore, $\sm(\cZ_s)$ has zero-measure and the assumption on $\bar{D}_s$ holds.

  \paragraph{Assumption on $e_k = \eta_{k+1} + \gamma_k r_k$.} First, note that for any $k\leq l \leq \sn(k,T)$,
  \begin{equation*}
    \norm{\sum_{i=k}^l \bgamma_i^2 r_i} \leq \left(\sup_{k \leq j \leq l} \norm{\bgamma_j r_j}\right)\sum_{i=k}^l \bgamma_i \leq \left(\sup_{k \leq j \leq l} \norm{\bgamma_j r_j}\right)\sum_{i=k}^{\sn(k,T)} \bgamma_i \leq T \sup_{k \leq j \leq l} \norm{\bgamma_j r_j}\, ,
  \end{equation*}
and the right-hand side goes to zero, when $k \rightarrow + \infty$. Thus, we only need to prove that the following quantity goes to zero
\begin{equation*}
 \sC(k):= \sup\left\{ \norm{\sum_{i=k}^l \bgamma_i \bar{\eta}_{i+1}} : k \leq l \leq \sn(k,T)\right\} \, .
\end{equation*}

To prove that $\sC(k)$ goes to zero we want to apply \cite[Proposition 4.2, Remarks 4.3]{benaim2006dynamics}, which states that it is the case if there is a filtration $(\cF_k)$ such that \emph{i)} $(\bgamma_k)$ and $(\bar{\eta}_{k})$ are adapted to $\cF_k$, \emph{ii)} there is $q \geq 2$ such that $\sup_{k} \bbE[\norm{\bar{\eta}_{k+1}}^q] < + \infty$, \emph{iii)} and $\sum_{k} \bbE[\bgamma_k^{1+q/2}] <  +\infty$.

Note that the first two points hold by Proposition~\ref{prop:stoch_approx_exp_log} (for any $q$) but for the last point, even though we have that almost surely $\sum_{k}\bgamma_k^{1 + 2/c_3} \leq \sum_{k} k^{-2} < + \infty$, $c_3$ is a random variable, and we do not necessarily such convergence in expectation for a fixed $q$. 

To address this issue, for every $b \in \bbQ$ define $\hat{\gamma}_k(b) = \min(\bgamma_k, k^{-b})$ and 
\begin{equation*}
  \tilde{\sC}(k,b):= \sup\left\{ \norm{\sum_{i=k}^l \hat{\gamma}_i(b)\eta_{i+1}} : k \leq l \leq \sn(k,T)\right\} \, .
 \end{equation*}
Next, for any $k_0 \in \bbN$, define $A_{k_0, b}$ as the following event 
\begin{equation*}
  A_{k_0, b} = [\forall k \geq k_0\, , \bgamma_k = \hat{\gamma}_k(b)]\, .
\end{equation*}
Note that by construction (since $b$ is fixed) assumptions of \cite[Proposition 4.2, Remarks 4.3]{benaim2006dynamics} are satisfied for $\bar{\eta}_{k+1},\hat{\gamma}_k(b)$. Therefore, $ \tilde{\sC}(k,b) \rightarrow 0$. Moreover, on $A_{k_0, b}$, for $k \geq k_0$, $\tilde{\sC}(k,b) = \sC(k)$. Therefore, on $A_{k_0, b}$, $\sC(k)$ converges to zero.

Finally, by the assumptions on $(\bgamma_k)$, $\bigcup_{k_{0}, b\in \bbN}A_{k_{0},b}$ is of full measure, hence $\sC(k,b)$ converges to zero almost surely.

This completes the proof of the assumption on $e_k = \eta_{k+1} + \gamma_k r_k$ and thus the proof of Theorem~\ref{thm:main}. 

\section{Proof of Proposition~\ref{prop:log_wk}}\label{sec:pf_logwk}

Note that for the exponential loss $l(q)=  e^{-q}$ and the logistic loss $l(q) = \log(1+ e^{-q})$, it holds 
\begin{align}\label{lm:exp_log}
\frac{e^{-q}}{2}\leq -l'(q) \leq e^{-q}, \quad \text{ for all } q \geq 0.
\end{align}
In the following proofs, we assume $k$ is sufficiently large so that $\sm(w_{k}) \geq \varepsilon \norm{w_k}^L$, for some $\varepsilon >0$. Taking for instance $\varepsilon = \liminf \sm(u_k)/2$, this is always possible by Assumption~\ref{hyp:marg_lowb}. We now establish that $(\norm{w_k})_k$ strictly increases to infinity.
\begin{lemma}\label{lm:wk_infty}
  Under Assumptions~\ref{hyp:loss_exp_log}--\ref{hyp:marg_lowb}, it holds that $(\norm{w_k})$ is a strictly increasing sequence that diverges to infinity.
\end{lemma}
\begin{proof}
  Using the fact that $l'(q) <0$, we obtain
  \begin{equation*}
  \begin{split}
    \norm{w_{k+1}}^2 &= \norm{w_k}^2 - \frac{2\gamma}{n_b} \sum_{i\in B_k} l'(p_i(w_k)) \scalarp{\sa_i(w_k)}{w_k} + \frac{\gamma^2}{n_b^2} \norm{\sum_{i\in B_k}l'(p_i(w_k)) \sa_i(w_k)}^2 \\
    &\geq \norm{w_k}^2 - L\frac{2\gamma}{n_b} \sum_{i\in B_k} l'(p_i(w_k)) p_i(w_k) \\
    &\geq \norm{w_k}^2 - L \frac{2\gamma}{n_b} \sum_{i\in B_k} l'(p_i(w_k)) m(w_k) \\
    &\geq \norm{w_k}^2 - \varepsilon L \frac{2\gamma}{n_b} \sum_{i \in B_k} l'(p_i(w_k))\norm{w_k}^L\, ,
  \end{split}
\end{equation*}
where for the second inequality we used Assumption~\ref{hyp:conserv} and for the last the homogeneity of the margin.
As a result, $(\norm{w_k})_k$ is a strictly increasing sequence. Assume by contradiction that $\sup_{k \geq k_0} \norm{w_k} \leq M$. Then, there exists $\delta > 0$ such that $\inf_{k \geq k_0\, , i \leq n} (-l'(p_i(w_k))) \geq \delta >0$. In particular, it implies that for all $k \geq k_0$,
\begin{equation*}
  \norm{w_{k+1}}^2 \geq \norm{w_k} + 2\varepsilon L \gamma \delta \norm{w_{k_0}}^L\, ,
\end{equation*}
which contradicts the fact that $\sup_{k \geq k_0} \norm{w_k} \leq M$. Therefore, $\norm{w_k} \rightarrow + \infty$.
\end{proof}
We now turn to the proof of the first part of Proposition~\ref{prop:log_wk}.
\begin{proposition}
  Let Assumptions~\ref{hyp:loss_exp_log}--\ref{hyp:marg_lowb} hold. Almost surely, there exist constants $c_1,c_2 >0$ and $k_2 \in \bbN$ such that for all $k \geq k_2$, 
  \begin{equation*}
   c_1 \log(k) \leq  \norm{w_k}^L \leq c_2 \log(k)\, .
  \end{equation*}
\end{proposition}
In the following proof, $ C, C', C_1, C_2, \dots $ denote positive constants that may vary from line to line. Additionally, we repeatedly use the following crude estimate, which follows from Taylor expansion: for $a > 0$ and sufficiently small $ x \in \mathbb{R} $, there exist constants $C, C' > 0$ such that  
\begin{equation*}
  1 + C' x \leq (1 + x)^a \leq 1 + Cx \, .
\end{equation*}  
\begin{proof}
  By Assumption~\ref{hyp:conserv}, for every $k$, $\scalarp{\sa_i(w_k)}{w_k} = Lp_i(w_k) =L \norm{w_k}^L p_i(u_k)$. Therefore,
  \begin{equation}\label{eq:est_wk}
    \begin{split}
      \norm{w_{k+1}}^2 &= \norm{w_k}^2 - 2\frac{\gamma}{n_b} \sum_{i\in B_k} l'(p_i(w_k)) \scalarp{\sa_i(w_k)}{w_k} + \frac{\gamma^2}{n_b^2} \norm{\sum_{i\in B_k}l'(p_i(w_k)) \sa_i(w_k)}^2\\
      &=\norm{w_k}^2 - 2L \norm{w_k}^L \frac{\gamma}{n_b} \sum_{i \in B_k} l'(p_i(w_k)) p_i(u_k) + \frac{\gamma^2}{n_b^2} \norm{\sum_{i\in B_k}l'(p_i(w_k)) \sa_i(w_k)}^2\, .
    \end{split}
  \end{equation}
  Since each $D_i$ is locally bounded, it is bounded on $\bbR^{d-1}$, which implies that there is a constant $C >0$ such that for every $1 \leq i \leq n$, $u \in \bbS^{d-1}$ and $v \in D_i(u)$,
  \begin{equation*}
    \norm{v} + |p_i(u)| \leq C\, .
  \end{equation*} 
  In particular, by Assumption~\ref{hyp:conserv}, $\sa_i(w_k) \leq C \norm{w_k}^{L-1}$.
  Therefore, using Equation~\eqref{lm:exp_log} and the fact that $p_i(u_k) \geq \sm(u_k) \geq \varepsilon$,
  \begin{equation*}
    \begin{split}
      \norm{w_{k+1}}^2 \leq \norm{w_k}^2 +  2CL\norm{w_k}^L \frac{\gamma}{n_b}\sum_{i \in B_k} e^{-\varepsilon \norm{w_k}^L}p_i(u_k)  + C''\frac{\gamma^2\norm{w_k}^{2(L-1)}}{n_b^2} \sum_{i\in B_k} e^{-2\varepsilon \norm{w_k}^L} \,.
    \end{split}
  \end{equation*}
Since $\norm{w_k} \rightarrow + \infty$ and $p_i(u_k) \geq \varepsilon$, there exists a constant $C>0$
  \begin{equation*}
    \norm{w_{k+1}}^2 \leq \norm{w_k}^2 + C \gamma\norm{w_k}^L e^{-\varepsilon \norm{w_k}^L}\, .
  \end{equation*}
  Thus, using the Taylor's expansion of $(1+x)^{L/2}$ near zero, for $k$ large enough,
  \begin{equation*}
    \begin{split}
    \norm{w_{k+1}}^L &\leq \norm{w_k}^L \left( 1 + C\norm{w_k}^{L-2} e^{-\varepsilon\norm{w_k}^L } \right)^{L/2}\\
    &\leq \norm{w_k}^L \left(1 + C' \norm{w_k}^{L-2}e^{-\varepsilon\norm{w_k}^L } \right)\\
    &\leq \norm{w_k}^L + C'' e^{-\varepsilon\norm{w_k}^L /2}\, . 
  \end{split}
  \end{equation*}
 This implies
  \begin{equation*}
    e^{\frac{\varepsilon}{2} \norm{w_k}^L}\left(\norm{w_{k+1}}^L  - \norm{w_k}^L\right)\leq C \, .
  \end{equation*}
  Denote $m_k:=\norm{w_{k+1}}^L$. Since $0 \leq m_{k+1} - m_k \rightarrow 0$, $ m_k \leq t \leq m_{k+1}$ implies $t \leq 2m_k$ for $k$ large enough. Therefore,
  \begin{equation*}
    \int_{m_k}^{m_{k+1}} e^{\varepsilon t/4}\dif t \leq   e^{\varepsilon \norm{w_k}^L/2}\left(\norm{w_{k+1}}^L  - \norm{w_k}^L\right)  \leq C \, .
  \end{equation*}
  In particular, for any $k_0$, such that for all $k \geq k_0$, all the preceding equations hold, we obtain,
  \begin{equation*}
    \frac{4(e^{m_k} -e^{m_{k_0}}) }{\varepsilon} = \int_{m_{k_0}}^{m_{k}} e^{\varepsilon t/4}\dif t  \leq C(k-k_0)\, .
  \end{equation*}
  Therefore, for $k$ large enough,
  \begin{equation*}
    m_{k}\leq C_1 + \log(C_2(k-k_0)) \leq C_3 \log(k)\, ,
  \end{equation*}
  which proves the upper bound.

Establishing the lower bound is similar. First, using Equation~\eqref{eq:est_wk}, we obtain
  \begin{equation*}
    \begin{split}
      \norm{w_{k+1}}^2 &\geq \norm{w_k}^2 + \frac{L \varepsilon\gamma \norm{w_k}^L}{n_b}\sum_{i\in B_k}e^{-\norm{w_k}^Lp_i(u_k)}\\
  &\geq \norm{w_k}^2 + L \varepsilon \gamma  \norm{w_k}^L e^{-C\norm{w_k}^L}\, ,
    \end{split}
  \end{equation*}
  where $C>0$ is such that for every $i \leq n$ and $u \in \bbS^{d-1}$, $|p_i(u)| \leq C$.


Therefore, for $k$ large enough, 
\begin{equation*}
  \begin{split}
    \norm{w_{k+1}}^L &\geq \norm{w_k}^L(1 + C_1 \norm{w_k}^{L-2} e^{-C\norm{w_k}^L})^{L/2} \\
    &\geq \norm{w_k}^L\left( 1 + C_2 \norm{w_k}^{L-2}e^{-C\norm{w_k}^L}\right)\\
    &\geq \norm{w_k}^L + C_3 e^{-C \norm{w_k}^L}\, .
  \end{split}
\end{equation*}
Thus,
\begin{equation*}
  e^{ C\norm{w_k}^L} \left(\norm{w_{k+1}}^L - \norm{w_k}^L\right)\geq C_3\, .
\end{equation*}
Recalling that $m_k = \norm{w_k}^L$, similarly to previous computations, for $m_{k}\leq t\leq m_{k+1} $, we obtain 
\begin{equation*}
  \int_{m_k}^{m_{k+1}}e^{Ct} \dif t\geq C_3\, .
\end{equation*}
Therefore, for a fixed, large enough $k_0$ and $k \geq k_0$,  
\begin{equation*}
\frac{e^{C m_{k}}}{ C}\geq \int_{m_{k_0}}^{m_{k}} e^{C t} \dif t \geq C_3(k - k_0)\, ,
\end{equation*}
which implies, for $C_2$ small enough and $k$ large enough,
\begin{equation*}
  m_{k} \geq \frac{\log(C_3  C(k-k_0))}{C} \geq C_2 \log(k)\, .
\end{equation*}
\end{proof}

To finish the proof of Proposition~\ref{prop:log_wk} it is enough to notice that for $k$ large enough,
\begin{equation*}
  \cL(w_k) = \frac{1}{n}\sum_{i=1}^n e^{-\norm{w_k}^L p_i(u_k)} \leq e^{-\sm(u_k) \norm{w_k}^L} \leq e^{-\varepsilon \norm{w_k}^L } \leq \frac{1}{k^{\varepsilon c_1}}\, .
\end{equation*}

\section{General convergence setting}\label{app:gen_sett}
The purpose of this section is to present a general convergence setting under which it is possible to apply the stochastic approximation ideas that were used in the proof of Proposition~\ref{prop:stoch_approx_exp_log} and Theorem~\ref{thm:main}. 

Mainly, our setting applies to more general losses than the exponential or logistic and allows to treat the case where we do not have an a priori control of the form $\liminf \sm(u_k) >0$. Due to the generality of the approach, we do not aim to push its limits and believe that better guarantees (such as an equivalent version of Proposition~\ref{prop:log_wk}) could be obtained on a case-by-case basis.


We still analyze~\eqref{eq:sgd_new} but now allow step-sizes to decrease to zero:
\begin{equation}\label{eq:sgd_gen}
  w_{k+1} = w_k -  \frac{\gamma_k}{n_b}\sum_{i\in B_k} l'(p_i(w_k)) \sa_i(w_k)\, .
\end{equation}
Note that~\eqref{eq:sgd_gen} encompasses the deterministic setting, in which $n_b = 0$, and note the presence of $(\gamma_k)$.

We first state our assumptions that are, mostly, only mild modifications of Assumptions~\ref{hyp:loss_exp_log}--\ref{hyp:marg_lowb}.
\begin{assumption}\label{Hgen:loss}\phantom{=}
  \begin{enumerate}[label=\roman*)]
    \item There exists a positive integer $L \in \mathbb{N}^*$,  such that, for every $1 \leq i \leq n$, the function $p_i$ is $L$-homogeneous, locally Lipschitz continuous and semialgebraic.
    \item\label{Hgen:loss_gen} The loss function $l$ is $C^1$, with $l'(q) <0$ on some interval $[q_0, + \infty)$ and is such that for any two sequences $a_k,b_k \rightarrow + \infty$,
    \begin{equation*}
        \limsup \frac{b_k}{a_k}< 1 \implies \frac{l'(a_k)}{l'(b_k)} \rightarrow 0 \, .
    \end{equation*}
  \end{enumerate}
\end{assumption}

\begin{assumption}\phantom{=}\label{Hgen:cons}
  For every $i \leq n$, $\sa_i$ is measurable and $D_i$ is semialgebraic. Moreover, for every $w \in \bbR^d$ and $\lambda \geq 0$, $\sa_i(w)  \in D_i(w)$ and
  \begin{equation*}
    D_i(\lambda w) = \lambda^{L-1} D_i(w)\, .
  \end{equation*}
\end{assumption}


\begin{assumption}[Deterministic setting]\label{Hgen:det}
  The batch-size $n_b = n$ and there is $\gamma>0$ such that $\gamma_k \leq \gamma$ and $\sum \gamma_k = + \infty$.
\end{assumption}
\begin{assumption}[Stochastic setting]\label{Hgen:sto}
 The sequence $(\gamma_k)$ is a sequence of strictly positive step-sizes such that $\sum_{k} \gamma_k = + \infty$ and there is $q \geq 2$ such that $\sum_{k} \gamma_k^{1+q/2} < + \infty$.
\end{assumption}

\begin{assumption}\label{Hgen:wk_div}
  Almost surely, $\norm{w_k} \rightarrow + \infty$, $\norm{w_k}^{L-2} \sum_{i=1}^n \ell'(p_i(w_k)) \rightarrow 0$.
  Moreover, almost surely, there is $k_0 \in\bbN$ such that $\inf_{k \geq k_0}\sm(w_k)\geq q_0$, where $q_0$ was defined in Assumption~\ref{Hgen:loss}.
\end{assumption}

\paragraph{Comments on Assumptions~\ref{Hgen:loss}--\ref{Hgen:wk_div}.} These assumptions are similar to Assumptions~\ref{hyp:loss_exp_log}--\ref{hyp:marg_lowb}. The main differences are the following. 

\begin{itemize}
  \item Assumption~\ref{Hgen:loss} allows us to consider more general losses such as $l(q) = e^{-q^a}$ or $l(q) =\log (1+e^{-q^a})$, for $a>0$, already considered in \cite{Lyu_Li_maxmargin}.
  \item Differently to the main setting of the paper, to treat the stochastic case we require in Assumption~\ref{Hgen:sto} vanishing stepsizes. In fact it could be alleviated, if, similarly to Proposition~\ref{prop:log_wk} we have control on the growth of $\norm{w_k}$. In the deterministic setting of Assumption~\ref{Hgen:det} one could still choose a constant stepsize.
  \item In Assumption~\ref{Hgen:wk_div}, differently to the main setting of the paper, we \emph{assume} $\norm{w_k} \rightarrow + \infty$. Note that, similarly to Lemma~\ref{lm:wk_infty}, we could ensure this growth if we assumed $\liminf \sm(u_k) >q_0$. However, the second point of Assumption~\ref{Hgen:wk_div} allows us to treat the case, where, a priori, we do not have such control on the growth of the normalized margin. For instance, if $l(q) = e^{-q}$, then Assumption~\ref{Hgen:wk_div} requires that $\norm{w_k}^{L-2}e^{-\norm{w_k}^L \sm(u_k)} \rightarrow 0$. Thus, it would hold as soon as $\sm(u_k) \geq \norm{w_k}^{-L}\log(\norm{w_k}^{L-1})$ a much softer requirement than $\liminf \sm(u_k) >0$. 
\end{itemize}

Under these new assumptions we have a version of Proposition~\ref{prop:stoch_approx_exp_log}. Recall that $\cF_k$ denotes the sigma algebra generated by $\{w_0, \ldots, w_k \}$.

\begin{proposition}\label{Pgen:stoch_approx}
  Let either Assumptions~\ref{Hgen:loss}--\ref{Hgen:det},~\ref{Hgen:wk_div} or Assumptions~\ref{Hgen:loss},\ref{Hgen:cons} and \ref{Hgen:sto},\ref{Hgen:wk_div} hold. There are sequences $(\bg_k), (r_k), (\bgamma_k), (\bar{\eta}_{k+1})$ such that
  \begin{equation}\label{Egen:stoch_app_u}
    u_{k+1} = u_k + \bgamma_k(\bg_k - \scalarp{\bg_k}{u_k}u_k) + \bgamma_k \bar{\eta}_{k+1} + \bgamma_k^2 r_k\, ,
  \end{equation}
  and the following holds.
  \begin{enumerate}
    \item\label{Gpr_res:rk} The sequence $(r_k)$ is such that almost surely $\sup_{k}\norm{r_k} < + \infty$.
    \item\label{Gpr_res:gammak} The sequence $(\bgamma_k)$ is adapted to $(\cF_k)$ and almost surely, there is $k_0 \in \bbN$, such that for all $k \geq k_0$, $\bgamma_k >0$. Moreover, $\bgamma_k \rightarrow 0$ and $\sum_{k} \bar{\gamma}_k = + \infty$. Under Assumption~\ref{Hgen:sto}, we additionally have that $\sup_k \bgamma_k / \gamma_k <  +\infty$.
    \item\label{Gpr_res:etak} Under Assumption~\ref{Hgen:det}, $\bar{\eta}_{k} \equiv 0$. Otherwise, under Assumption~\ref{Hgen:sto}, the sequence $(\bar{\eta}_{k})$ is adapted to $(\cF_k)$, 
    \begin{equation*}
    \bbE[\bar{\eta}_{k+1} |\cF_k] = 0 \, ,
    \end{equation*}
    and there is a deterministic constant $c_1>0$ such that $\sup_{k} \norm{\bar{\eta}_{k+1}} < c_6$.
    \item\label{Gpr_res:barD} For any unbounded sequence $(k_j)_{j \geq 0}$, such that $u_{k_j}$ converges to $u \in \bbS^{d-1}$, it holds that $\dist(\bar{D}(u), \bg_{k_j}) \rightarrow 0$. 
  \end{enumerate}
\end{proposition}

\begin{proof}
  The proof goes as the one of Proposition~\ref{prop:stoch_approx_exp_log} in Section~\ref{pf:sto_app_explog} but with $\gamma$ replaced by $\gamma_k$. The claim on $(\bgamma_k)$ follows from the fact that 
  \begin{equation*}
    \bgamma_k = -\gamma_k \norm{w_k}^{L-2} \sum_{j=1}^n l'(p_j(w_k))\, .
  \end{equation*}
  Indeed, for $k$ large enough, $p_j(w_k) \geq q_0$. Hence, $\bgamma_k >0$ by Assumption~\ref{Hgen:loss}-\ref{Hgen:loss_gen}. Moreover, it goes to zero by Assumption~\ref{Hgen:wk_div}. From the same assumption and under Assumption~\ref{Hgen:sto} we indeed have $\limsup \bgamma_k/\gamma_k < + \infty$. 
  The claim on $\bar{D}_s$ is obtained from Assumption~\ref{Hgen:loss}-\ref{Hgen:loss_gen}. The other claims are obtained \emph{exactly as} in the proof of  Proposition~\ref{prop:stoch_approx_exp_log}.
\end{proof}

As a result, we have a version of Theorem~\ref{thm:main}.
\begin{theorem}\label{Tgen:main}
  In the setting of Proposition~\ref{Pgen:stoch_approx}, almost surely $\sm(u_k)$ converges to a nonnegative limit and 
  \begin{equation}\label{Egen:conv_uk}
    \dist(u_k, \cZ_s) \xrightarrow[k \rightarrow + \infty]{} 0 \, .
  \end{equation}
\end{theorem}
\begin{proof}
  The proof goes as the proof of Theorem~\ref{thm:main}, with Proposition~\ref{Pgen:stoch_approx} playing the role of Proposition~\ref{prop:stoch_approx_exp_log}.
\end{proof}

Let us finish this section with a brief comment on KKT points of~\eqref{def:prob2} considered in \cite{Lyu_Li_maxmargin}.
A point $w \in \bbR^d$ is a KKT point of~\eqref{def:prob2} if there is $\alpha_i, \ldots, \alpha_n \geq 0$ and $v_1, \ldots, v_n$, with $v_i \in \partial p_i(w)$ and 
\begin{equation*}
  w = \sum_{i=1}^n \alpha_i v_i \quad \textrm{ and for all $i$, } \quad \lambda_i(p_i(w)- 1) = 0 \, .
\end{equation*}
Denoting $\lambda_i = \alpha_i/ \sum_{j=1}^n \alpha_j$ and $u = w/ \norm{w}$, we can successively observe that \emph{i)} $\lambda_i \neq 0$ only if $p_i(u) = \sm(u)$ and $v_i = \norm{w}^L g_i$, with $g_i \in\partial p_i(u)$, \emph{ii)} $\sum_{i=1}^n \lambda_i g_i \in \bar{D}(u)$ \emph{iii)} therefore, $u= \sum_{i=1}^n \lambda_i g_i$ and finally $0\in \bar{D}_s(u)$. 
Conversely, if $u \in \cZ_s$ and \emph{if additionally} $\sm(u) >0$, then $w = u/\sm(u)^{1/L}$ is a KKT point of~\eqref{def:prob2}.

Nevertheless, due to the slow decrease rate required by Assumption~\ref{Hgen:wk_div}, we do not, a priori, have that $\lim \sm(u_k) > 0$. Therefore, in the setting of Theorem~\ref{Tgen:main} there could be two scenarios. First, in which $\lim \sm(u_k) >0$ and in that case the limit points of $(u_k)$ are \emph{exactly} (scaled) KKT directions of~\eqref{def:prob2}. Second, in which $\lim \sm(u_k) =0$, and we only have that any limit point of $(u_k)$ is in $\cZ_s$.

Note however, that without any discussion on KKT points in all cases being in $\cZ_s$ is a meaningful description of optimality of maximization of the margin, which, furthermore, naturally extends to settings, where $D_i$ are different of the Clarke subgradients of $p_i$.


