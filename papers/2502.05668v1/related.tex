%The implicit regularization of first-order optimization methods, such as gradient descent, has been widely studied. It is considered one of the most promising explanations for the remarkable generalization ability of overparameterized models, including deep neural networks. 
We review related works  on the implicit bias of neural networks trained with gradient-based algorithms and their connection to margin maximization. The discussion follows an increasing order of flexibility, starting with logistic regression--viewed as a one-layer network-- then progressing to linear and homogeneous networks and finally addressing non-homogeneous architectures. For a comprehensive survey on the topic, we refer the reader to \cite{vardi2023implicit}.


\paragraph{Logistic regression} Training a logistic regression model can be viewed as training a one-layer neural network or fine-tuning the final linear layer of a deep network, making it a natural starting point for studying the training dynamics of deep networks. Motivated by this perspective, a first line of research has explored the properties of (stochastic) gradient descent iterates for linear logistic regression.
\cite{soudry2018implicit} showed that when learning parameters  $w$  using gradient descent on the logistic loss--or more generally, on exponentially-tailed losses--over separable data, the direction of $w$  converges to the $\ell_2$-max margin solution, while its norm grows to infinity. \cite{nacson2019stochastic} extended this result to stochastic gradient descent with a fixed learning rate, demonstrating similar behavior. \cite{ji2018risk} further considered non-linearly separable data, showing that the direction of $w$ converges to the maximum margin predictor of the largest linearly separable subset.

\paragraph{Linear networks} Linear networks, composed of stacked linear layers $W_D \times \cdots \times W_1$ without activation functions, represent linear functions, but their parameterization strongly influences learning dynamics and implicit bias.  
\cite{gunasekar2018implicit} analyzed gradient descent on fully connected and convolutional linear networks, showing convergence in direction to the maximum-margin solution. \cite{ji2018gradient} extended this result by proving predictor convergence under weaker assumptions and establishing asymptotic singular vector alignment. \cite{nacson2019convergence} examined conditions on the loss function ensuring that gradient descent iterates converge to the $\ell_2$-maximum margin solution. \cite{yun2020unifying} later developed a unified framework for gradient flow in linear tensor networks, covering fully connected, diagonal, and convolutional architectures. 

\paragraph{Homogeneous networks} Linear networks cannot model architectures with nonlinear activations, limiting their applicability. Homogeneous networks, including fully connected and convolutional architectures without bias terms and with ReLU activations, have been considered to address this limitation.  
In this setting, \cite{du2018algorithmic} proves that gradient flow enforces the differences between squared norms across different layers to remain invariant without any explicit regularization.
%\cite{wei2019regularization} proved that the global minimizer of a regularized cross-entropy loss corresponds to the maximum normalized margin solution. 
Assuming training error converges to zero and parameters converge in direction, \cite{nacson2019lexicographic} established that rescaled parameters reach a first-order KKT point of a maximum-margin problem.  
\cite{Lyu_Li_maxmargin} showed that for networks trained with exponential or logistic loss, gradient flow converges to a KKT point of the maximum-margin problem in parameter space. \cite{vardi2022margin} examined the (local) optimality of these KKT points, identifying conditions for local or global optimality. \cite{ji2020directional} proved that the direction of gradient flow converges to a unique point. Finally, \cite{wang2021implicit} analyzed margin maximization in homogeneous networks trained with adaptive methods such as Adam.

\paragraph{Beyond homogeneous networks}
\cite{kunin2022asymmetric, le2022training} extend maximum-margin bias and alignment results from linear and homogeneous to quasi-homogeneous networks, allowing to consider networks with biases and residual connections.
