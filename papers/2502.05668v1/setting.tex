We study (stochastic) gradient descent on the empirical risk
\begin{equation*}
\cL(w) = \frac{1}{n}\sum_{i=1}^n l(p_i(w))\, ,
\end{equation*}
where the loss function $l$ and the functions  $(p_i)_{i=1}^n$  are specified in the following assumptions. Note that the empirical risk for binary classification from Equation~\eqref{def:emp_risk_intro} is a special case of the above objective.

\begin{assumption}\label{hyp:loss_exp_log}\phantom{=}
  \begin{enumerate}[label=\roman*)]
    \item The loss is either the exponential loss, $l(q) = e^{-q}$, or the logistic loss, $l(q) = \log(1{+}e^{-q})$.
    \item There exists an integer $L \in \mathbb{N}^*$  such that, for all $1 \leq i \leq n$, the function $p_i$ is $L$-homogeneous\footnote{We recall that a mapping $f : \mathbb{R}^d \rightarrow \mathbb{R}$ is positively $L$-homogeneous if $f(\lambda w) = \lambda^L f(w)$ for all $w \in \mathbb{R}^d$ and $\lambda >0$.}, locally Lipschitz continuous and semialgebraic.
  \end{enumerate}
\end{assumption}
If the $p_i$'s were differentiable with respect to $w$, the chain rule would guarantee that
\begin{align*}
\nabla \mathcal{L}(w) = \frac{1}{n}\sum_{i=1}^n  l'(p_i(w)) \nabla p_i(w)\enspace.
\end{align*}
However, we only assume that the $p_i$'s are semialgebraic. While we could consider Clarke subgradients, the Clarke subgradient of operations on functions (e.g., addition, composition, and minimum) is only contained within the composition of the respective Clarke subgradients. This, as noted in Section~\ref{sec:cons_field}, implies that the output of backpropagation is usually not an element of a Clarke subgradient but a selection of some conservative set-valued field.
Consequently, for $1\leq i \leq n$, we consider $D_i : \bbR^d \rightrightarrows\bbR^d$, a conservative set-valued field of $p_i$, and a function $\sa_i : \bbR^d \rightarrow \bbR^d$ such that for all $w \in \bbR^d$, $\sa_i(w) \in D_i(w)$. Given a step-size $\gamma >0$, gradient descent (GD)\footnote{More precisely, this refers to conservative gradient descent. We use the term GD for simplicity, as conservative gradients behave similarly to standard gradients.} is then expressed as
\begin{equation*}\label{eq:gd_new}\tag{GD}
  w_{k+1} = w_k - \frac{\gamma}{n} \sum_{i=1}^n l'(p_i(w_k))\sa_i(w_k)\,.
\end{equation*}
For its stochastic counterpart, stochastic gradient descent (SGD), we fix a batch-size $1\leq n_b \leq n$. At each iteration $k \in \bbN$, we randomly and uniformly draw a batch $B_k \subset \{1, \ldots, n \}$ of size $n_b$. The update rule is then given by 
\begin{equation*}\label{eq:sgd_new}\tag{SGD}
  w_{k+1} = w_k -  \frac{\gamma}{n_b}\sum_{i\in B_k} l'(p_i(w_k)) \sa_i(w_k)\, .
\end{equation*}
The considered conservative set-valued fields will satisfy an Euler lemma-type assumption.
%\nic{Smoother transition}
\begin{assumption}\phantom{=}\label{hyp:conserv}
  For every $i \leq n$, $\sa_i$ is measurable and $D_i$ is semialgebraic. Moreover, for every $w \in \bbR^d$ and $\lambda \geq 0$, $\sa_i(w)  \in D_i(w)$,
  \begin{equation*}
    D_i(\lambda w) = \lambda^{L-1} D_i(w)\, , \textrm{ and } \quad   L p_i(w) = \scalarp{\sa_i(w)}{w}\, .
  \end{equation*}
\end{assumption}
%\nic{Smoother transition}
Having in mind the binary classification setting, in which $p_i(w) = y_i \Phi(x_i, w)$, we define the margin
\begin{equation}\label{def:marg}
  \sm: \bbR^d \rightarrow \bbR, \quad \sm(w) = \min_{1\leq i \leq n} p_i(w)\, .
\end{equation}
It quantifies the quality of a prediction rule $\Phi(\cdot, w)$. In particular,  the training data is perfectly separated when $\sm(w) >0$. A binary prediction for $x$ is given by the sign of $\Phi(x, w)$, and under the homogeneity assumption, it depends only on the normalized direction $w / \norm{w}$. Consequently, we will focus on the sequence of directions $u_k := w_k / \norm{w_k}$. Our final assumption ensures that the normalized directions $(u_k)$ have stabilized in a region where the training data is correctly classified.

\begin{assumption}\label{hyp:marg_lowb}
  Almost surely, $\liminf \sm(u_k) >0$.
\end{assumption}
Before presenting our main result, we comment on our assumptions.

\paragraph{On Assumption~\ref{hyp:loss_exp_log}.} As discussed in the introduction, the primary example we consider is when $p_i(w) = y_i \Phi(x_i;w)$ is the signed prediction of a feedforward neural network without biases and with piecewise linear activation functions on a labeled dataset $((x_i,y_i))_{i \leq n}$. In this case,
\begin{equation}\label{eq:NN}
 p_i(w) = y_i \Phi(w;x_i) = y_i V_L(W_L) \sigma(V_{L-1}(W_{L-1}) \sigma(V_{L-1}(W_{L-2}) \ldots \sigma(V_{1}(W_1 x_i))))\, ,
\end{equation}
where $w = [W_1, \ldots, W_L]$, $W_i$ represents the weights of the $i$-th layer, $V_i$ is a linear function in the space of matrices (with $V_i$ being the identity for fully-connected layers) and $\sigma$ is a coordinate-wise activation function such as $z \mapsto \max(0,z)$ ($\ReLU$), $z \mapsto \max(az, z)$ for a small parameter $a>0$ (LeakyReLu) or $z \mapsto z$. Note that the mapping $w \mapsto p_i(w)$ is semialgebraic and $L$-homogeneous for any of these activation functions. Regarding the loss functions, the logistic and exponential losses are among the most commonly studied and widely used. In Appendix~\ref{app:gen_sett}, we extend our results to a broader class of losses, including $l(q) = e^{-q^a}$ and $l(q) = \ln (1 + e^{-q^a})$ for any $a \geq 1$.

\paragraph{On Assumption~\ref{hyp:conserv}.} Assumption~\ref{hyp:conserv} holds automatically  if $D_i$ is the Clarke subgradient of $p_i$. Indeed, at any vector $w \in \bbR^d$, where $p_i$ is differentiable it holds that $p_i(\lambda w) = \lambda^{L} p_i(w)$. Differentiating relatively to $w$ and $\lambda$ (noting that $p_i$ remains differentiable at $\lambda w$ due to homogeneity), we obtain $\lambda \nabla p_i(\lambda w) = \lambda^{L} \nabla p_i(w)$ and $\scalarp{\nabla p_i(\lambda w)}{w} = L \lambda^{L-1} p_i(w)$. The expression for any element of the Clarke subgradient then follows from~\eqref{eq:def_clarke}. 

However, for an arbitrary conservative set-valued field, Assumption~\ref{hyp:conserv} does not necessarily hold. For instance, $D(x) = \mathds{1}(x \in \mathbb{N})$ is a conservative set-valued field for $p \equiv 0$, which does not satisfy Assumption~\ref{hyp:conserv}. Nevertheless, in practice, conservative set-valued fields naturally arise from a formal application of the chain rule. For a non-smooth but homogeneous activation function $\sigma$, one selects an element $e \in \partial \sigma (0)$, and computes $\sa_i(w)$ via backpropagation. Whenever a gradient candidate of $\sigma$ at zero is required (i.e., in~\eqref{eq:NN}, for some $j$, $V_j(W_j)$ contains a zero entry), it is replaced by $e$. 
Since $V_j(W_j)$ and $V_j(\lambda W_j)$ have the same zero elements, it follows that for every such $w$, $
\sa_i(\lambda w) = \lambda^L \sa_i(w)$. The conservative set-valued field $D_i$ is then obtained by associating to each $w$ the set of all possible outcomes of the chain rule, with $e$ ranging over all elements of $\partial \sigma(0)$. Thus, for such fields, Assumption~\ref{hyp:conserv} holds.


\paragraph{On Assumption~\ref{hyp:marg_lowb}.} Training typically continues even after the training error reaches zero.
Assumption~\ref{hyp:marg_lowb} characterizes this late-training phase, where our result applies. 
As noted earlier, since $\sm$ is $L$-homogeneous, the classification rule is determined by the direction of the  iterates $u_k=w_k/\norm{w_k}$. Assumption~\ref{hyp:marg_lowb} then states that, beyond some iteration, the normalized margin remains positive. 
This assumption is natural in the context of studying the implicit bias of SGD: we \emph{assume} that we reached the phase in which the dataset is correctly classified and \emph{then} characterize the limit points. A similar perspective was taken in  \cite{nacson2019lexicographic}, where the implicit bias of GF was analyzed under the assumption that the sequence of directions and the loss converge. However, unlike their approach, ours does not require assuming such convergence a priori.

Earlier works such as \cite{ji2020directional,Lyu_Li_maxmargin}, which analyze subgradient flow or smooth GD, establish convergence by assuming the existence of a single iterate $w_{k_0}$ satisfying $\sm(w_{k_0}) > \varepsilon$ and then proving that $\lim \sm(u_{k}) > 0$. Their approach relies on constructing a smooth approximation of the margin, which increases during training, ensuring that $\sm(u_k) > 0$ for all iterates with $k \geq k_0$. This is feasible in their setting, as they study either subgradient flow or GD with smooth $p_i$â€™s, allowing them to leverage the descent lemma.

In contrast, our analysis considers a nonsmooth and stochastic setting, in which, even if an iterate $w_{k_0}$ satisfying $\sm(w_{k_0}) > \varepsilon$ exists, there is no a priori assurance that subsequent iterates remain in the region where Assumption~\ref{hyp:marg_lowb} holds. From this perspective, Assumption~\ref{hyp:marg_lowb} can be viewed as a stability assumption, ensuring that iterates continue to classify the dataset correctly. Establishing stability for stochastic and nonsmooth algorithms is notoriously hard, and only partial results in restrictive settings exist \cite{borkar2000ode,ramaswamy2017generalization,josz2024global}.

%Finally, note that Assumption~\ref{hyp:marg_lowb} only needs to hold almost surely. Specifically, with probability 1, there exist $k_0$ and $\varepsilon$ such that for all $k \geq k_0$, $\sm(u_k) \geq \varepsilon > 0$. In the case of~\eqref{eq:sgd_new}, $k_0$ and $\delta$ are random variables and may take different values across different realizations. 

%\paragraph{On constant stepsizes.}
%We allow the step size to be a constant of arbitrary magnitude, subject to the stability Assumption~\ref{hyp:marg_lowb}. This may seem surprising in a nonsmooth and stochastic setting, where a vanishing step size is typically required to ensure convergence (see, e.g., \cite{majewski2018analysis, dav-dru-kak-lee-19, bolte2023subgradient, le2024nonsmooth}).