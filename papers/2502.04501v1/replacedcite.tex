\section{Related Work}
\textbf{Parameter-efficient fine-tuning.}
With the rapid growth of pretrained neural networks, researchers have investigated parameter-efficient fine-tuning methods that update only a small set of parameters while maintaining high performance. One straight way is to tune specific components of the model. For example, BitFit____ updates only the bias terms, and LayerNorm tuning____ only trains the layer-norm parameters.
Another line of work involves introducing and training small, task-specific non-linear modules, such as Adapters____ and AdapterDrop____. Other methods steer the activation representations either globally or locally____.

Two other prominent paradigms are low-rank adaptation (LoRA; ____, ____) and prompt tuning methods____, which are more related to our work. They will be further elaborated in the subsequent sections. 


\textbf{Low-rank adaptation.} ____ assume that weight updates can be approximated by low-rank matrices and propose a low-rank adaptation (LoRA) method for fine-tuning a model.  Building upon this foundational work, many extensions have been developed to enhance LoRAâ€™s performance. For example, ReLoRA____ iteratively trains and merges low-rank adapters to achieve high-rank updates. 
____ propose learning low-rank matrices with different learning rates.
____ explore training a mixture of LoRA modules and leverage dynamic routing mechanisms for different task distributions or domains.

However, for large models, LoRA still requires a considerable number of trainable parameters. To address this limitation, several works have explored the use of random projection____ to further improve parameter efficiency. For example, \textsc{Flora}____ updates the pretrained matrices with randomly projected gradients, while VeRA____ uses random projections combined with two trainable scaling vectors to approximate each update matrix. 

\textbf{Prompt tuning.} 
____ introduce the concept of learning prompt tokens to elicit knowledge from LLMs. Subsequently, ____ extend this idea to continuous prompt tuning, where prompt embeddings are optimized through gradient descent while keeping the LLM frozen. Building on this, ____ further generalize prompt embeddings to a multi-layer setting. ____ re-parameterize prompt tuning by incorporating a feed-forward neural network with residual connections.  ____ observe that redistributing parameters to learn offsets for input token embeddings can enhance performance. On the other hand, multi-task prompt tuning has been explored, where the learned prompt parameters are reused across different tasks____. Closely with our work, ____ decompose the prompt embedding matrix into two low-rank components: a low-dimensional prompt matrix and a learnable up-projection matrix. By contrast, our ULPT method freezes the up-projection matrix, so that we are able to achieve high performance with much fewer trainable parameters, supported by random-projection theory____. Overall, our approach is able to function well with an ultra-low dimension, making it practical to customize millions of LLMs and perform continual learning in an ever-changing environment.