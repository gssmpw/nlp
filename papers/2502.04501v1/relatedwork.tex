\section{Related Work}
\textbf{Parameter-efficient fine-tuning.}
With the rapid growth of pretrained neural networks, researchers have investigated parameter-efficient fine-tuning methods that update only a small set of parameters while maintaining high performance. One straight way is to tune specific components of the model. For example, BitFit~\cite{ben-zaken-etal-2022-bitfit} updates only the bias terms, and LayerNorm tuning~\cite{zhao2024tuning} only trains the layer-norm parameters.
Another line of work involves introducing and training small, task-specific non-linear modules, such as Adapters~\cite{pmlr-v97-houlsby19a} and AdapterDrop~\cite{ruckle-etal-2021-adapterdrop}. Other methods steer the activation representations either globally or locally~\cite{wu2024reft, yin2024lofit}.

Two other prominent paradigms are low-rank adaptation (LoRA; \citeauthor{hu2022lora}, \citeyear{hu2022lora}) and prompt tuning methods~\cite{lester-etal-2021-power}, which are more related to our work. They will be further elaborated in the subsequent sections. 


\textbf{Low-rank adaptation.} \citet{hu2022lora} assume that weight updates can be approximated by low-rank matrices and propose a low-rank adaptation (LoRA) method for fine-tuning a model.  Building upon this foundational work, many extensions have been developed to enhance LoRAâ€™s performance. For example, ReLoRA~\cite{lialin2024relora} iteratively trains and merges low-rank adapters to achieve high-rank updates. 
\citet{hayou2024lora} propose learning low-rank matrices with different learning rates.
\citet{wu2024mixture} explore training a mixture of LoRA modules and leverage dynamic routing mechanisms for different task distributions or domains.

However, for large models, LoRA still requires a considerable number of trainable parameters. To address this limitation, several works have explored the use of random projection~\cite{10.1145/502512.502546} to further improve parameter efficiency. For example, \textsc{Flora}~\cite{hao2024flora} updates the pretrained matrices with randomly projected gradients, while VeRA~\cite{kopiczko2024vera} uses random projections combined with two trainable scaling vectors to approximate each update matrix. 

\textbf{Prompt tuning.} 
\citet{shin-etal-2020-autoprompt} introduce the concept of learning prompt tokens to elicit knowledge from LLMs. Subsequently, \citet{lester-etal-2021-power} extend this idea to continuous prompt tuning, where prompt embeddings are optimized through gradient descent while keeping the LLM frozen. Building on this, \citet{li-liang-2021-prefix} further generalize prompt embeddings to a multi-layer setting. \citet{razdaibiedina-etal-2023-residual} re-parameterize prompt tuning by incorporating a feed-forward neural network with residual connections.  \citet{shi2024dept} observe that redistributing parameters to learn offsets for input token embeddings can enhance performance. On the other hand, multi-task prompt tuning has been explored, where the learned prompt parameters are reused across different tasks~\cite{wang2023multitask}. Closely with our work, \citet{xiao-etal-2023-decomposed} decompose the prompt embedding matrix into two low-rank components: a low-dimensional prompt matrix and a learnable up-projection matrix. By contrast, our ULPT method freezes the up-projection matrix, so that we are able to achieve high performance with much fewer trainable parameters, supported by random-projection theory~\cite{10.1145/502512.502546}. Overall, our approach is able to function well with an ultra-low dimension, making it practical to customize millions of LLMs and perform continual learning in an ever-changing environment.