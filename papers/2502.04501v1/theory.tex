We first show that an ultra low-dimensional space can capture the structure of the original embeddings (i.e., expressiveness). We then show the convergence of gradient descent with our random projection (i.e., optimization).

\paragraph{Expressiveness.}

Our low-dimensional parameterization approximately captures high-dimensional structure with high confidence. To show this, we first state the following lemma.

\begin{lemma}\label{lem:im}
      Sample a random matrix $\mA \in \sR^{r \times m}$ such that each element follows the standard Gaussian distribution. Let $\epsilon \in (0, 1/2]$ and $r \in \sN_+$. There exists a constant $c$ such that \begin{align}
        \Pr\left( \left| \frac{(1/{\sqrt{r}}) \|  \mA\vx \| - \|\vx \|}{\|\vx\|} \right| \ge \epsilon \right) \le \frac{2}{ \exp\left(\epsilon^2r\right/c)}
    \end{align}
    for any $\vx \in \sR^d$. 
\end{lemma}

This result is adapted from \citet{indyk1998approximate}. Essentially, the lemma characterizes the high-probability bound of the well known Johnsonâ€“Lindenstrauss lemma~\cite{dasgupta2003elementary, matouvsek2008variants}. 
Based on this, we formally show the expressiveness of our ultra low-dimensional embeddings in the following theorem.

\begin{restatable}{theorem}{expr}\label{thm:exsitance}   
Let $\ve_1,\cdots, \ve_n\in\mathbb R^d$ be the embedding vectors in the high-dimensional space. Let $\mP \in \sR^{r \times d}$ be a random projection matrix with each element $p_{i,j}\sim\mathcal N(0,1/r)$. There exists a set of low-dimensional vectors $\vz_1,\cdots, \vz_n\in \mathbb R^r$ such that with confidence at least $1-\delta$ we have \begin{align}
        (1-\epsilon) \| \ve_i - \ve_j \| \le \| \vz_i - \vz_j \| \le (1+\epsilon) \| \ve_i - \ve_j \|
    \end{align}
    for all $i,j \in [n]$, as long as  $r\ge 2 c \epsilon^{-2} \log (2n/\delta)$.
\end{restatable}




\begin{proof}
See Appendix~\ref{apx:proof:expr}.
\end{proof}

In essence, our theorem asserts that there exists a set of low-dimensional vectors such that the pair-wise $L^2$ distances of the original high-dimensional vectors are preserved for all $(i, j)$ pairs. More importantly, the projected dimension $r$ only grows logarithmically  with the original dimension $n$, demonstrating a favorable property of scaling. It should be noted that, although our theorem uses $L^2$ as the metric, it can be easily extended to the dot-product metric as well by noticing that $\|\vx - \vy\|^2 =   \|\vx\|^2 + \|\vy\|^2 - 2 \vx \cdot \vy$. 


% \begin{lemma}\label{lem:identity}
% Let $\mP \in \sR^{r \times d}$ be a random matrix where each element is sampled from $\gN_{0,1}$. We have 
% \begin{align}
%     \frac{1}{r}\E_{\mP}[\mP^\top \mP] = \mI,
% \end{align}
% where $\mI \in \sR^{d\times d}$ is the identity matrix.
% \end{lemma}

% \begin{proof}
%     For each element of $\mP^\top \mP$, we have \begin{align}
%         [\mP^\top \mP]_{i,j} = \begin{cases}
%            \sum_{k=1}^r  p_{k,i}^2, &\text{ if } i = j, \\
%            \sum_{k=1}^r p_{k,i} a_{k,j}, &\text{ otherwise, }
%         \end{cases}
%     \end{align}
%     where each element $p_{i,k}$ is an independent random variable following $\gN(0, 1)$.

%     For $z_{i,i} := \sum_{k=1}^r  p_{k,i}^2$, it follows the $\chi^2(r)$ distribution.
    
%     % By the standard Laurent-Massart bounds~\cite{laurent2000adaptive}, we obtain \begin{align}
%     %     \delta_{i,i} := \Big| \frac{z_{i,i}}{r} - 1 \Big| \le 2\sqrt{\frac{\log(2/\delta_1')}{r}}+2 \frac{\log(2/\delta_1')}{r} \label{eq:diag-delta}
%     % \end{align}
%     % with probability at least $1-\delta_1'$.

%     For $z_{i,j} := \sum_{k=1}^r p_{k,i} p_{k,j}$ where $i \not= j$, we can rewrite it as $z_{i,j} = \sum_{k=1}^r [(\frac{p_{k,i} + p_{k,j}}{2})^2  - (\frac{p_{k,i} - p_{k,j}}{2})^2]$. In addition, all $(\frac{p_{k,i} + p_{k,j}}{2})^2$ and $(\frac{p_{k,i} - p_{k,j}}{2})^2$ are i.i.d. $\chi^2(1)$ distributions. Define \begin{align}
%         & z^+_{i,j}  := \sum_{k=1}^r \bigg(\frac{p_{k,i} + p_{k,j}}{2}\bigg)^2  \\ \text{and} & \quad
%         z^-_{i,j} := \sum_{k=1}^r \bigg(\frac{p_{k,i} - p_{k,j}}{2}\bigg)^2,
%     \end{align}
%     it is easy to see that $z^+_{i,j}$ and $z^-_{i,j}$ are i.i.d. $\chi^2(r)$ distributions. In addition, $z_{i,j} = z^+_{i,j} - z^-_{i,j}$.
    
%     Therefore, we have
%     \begin{align}
%         \delta_{i,j} :=& \Big| \frac{z_{i,j}}{r} \Big| = \bigg| \Big( \frac{z_{i,j}^+}{r}-1\Big) - \Big(\frac{z_{i,j}^-}{r} -1 \Big) \bigg| \\
%         \le& \Big| \frac{z_{i,j}^+}{r} - 1\Big| + \Big| \frac{z_{i,j}^-}{r} - 1\Big| \\
%         \le& 4\sqrt{\frac{\log(4/\delta_2')}{r}}+4 \frac{\log(4/\delta_2')}{r} \label{eq:off-diag-delta}
%     \end{align}
%     with probability at least $1-\delta_2'$.
    
%     By using a union bound upon Equations~\eqref{eq:diag-delta} and \eqref{eq:off-diag-delta}, we can obtain \begin{align}
%         \delta_{i,j} \le& 4\sqrt{\frac{2\log(2m/\delta)}{r}}+ 4 \frac{2\log(2m/\delta)}{r} 
%     \end{align}
%     for all $i,j$ with probability at least $1-\delta$. Under this condition, we further have \begin{align}
%         \delta_{i,j} =& 4\sqrt{\frac{2\log(2m/\delta)}{ 128  \log (2m/\delta) \epsilon^{-2}}}+ 4 \frac{2 \log(2m/\delta)}{128 \log (2m/\delta) \epsilon^{-2}} \tag{Let $r = 128  \log (2m/\delta) \epsilon^{-2}$}\\
%         =& \frac{1}{2} (\epsilon + \epsilon^2) \\
%         \le& \epsilon, \tag{$\epsilon^2 \le \epsilon \le 1$}
%     \end{align}
%     concluding the proof by noticing $\delta_{i,j} = \Big| [(1/r)\mP^\top \mP - \mI]_{i,j} \Big|$.
% \end{proof}

\paragraph{Optimization.} The above theorem shows the existence of an expressive low-dimensional space. We assert in the following theorem that, given a random up-projection matrix, the optimal low-dimensional embeddings can be learned by gradient descent under mild assumptions.

\begin{restatable}{theorem}{optim}\label{thm:optim}
Assume the original loss function $\gL$  is Polyak--Lojasiewic and element-wise Lipschitz on the original $d$-dimensional embeddings. Let $\mP \in \sR^{r\times d}$ be a given full-rank  random Gaussian matrix (i.e., rank $r$), and our parametrization be $\hat\ve_i = \diag(\vs) \mP^\top \vz_i + \vb$. With a proper learning rate schedule $\eta_1, \eta_2, \dots$, our parameters $\vx = [\vb, \vs, \vz_1, \dots, \vz_n]$ converge to the global optimum with gradient descent if $\vs$ is always non zero.
\end{restatable}
\begin{proof}
    See Appendix~\ref{apx:proof:optim}.
\end{proof}

Theorem~\ref{thm:optim} shows that, even with the na\"ive gradient descent, the fixed random matrix $\mP$ does not hinder the optimization procedure. By combining Theorem~\ref{thm:exsitance}, we theoretically justifies our overall practice of ULPT.