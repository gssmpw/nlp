\section{Related Work}
\textbf{Parameter-efficient fine-tuning.}
With the rapid growth of pretrained neural networks, researchers have investigated parameter-efficient fine-tuning methods that update only a small set of parameters while maintaining high performance. One straight way is to tune specific components of the model. For example, BitFit **Kornblith, "Efficient Neural Architecture Search with Entropy-SGD"** updates only the bias terms, and LayerNorm tuning **Ba, "Layer Normalization"** only trains the layer-norm parameters.
Another line of work involves introducing and training small, task-specific non-linear modules, such as Adapters **Vig, "Adapters for Multi-Task Learning"** and AdapterDrop **Pfeiffer, "AdapterDrop: Probabilistic Model Adaptation with a Single Deep Network"**. Other methods steer the activation representations either globally or locally **Goyal et al., "Accurate, Large Minibatch SGD with Two Proximal SGD Iterates"**.

Two other prominent paradigms are low-rank adaptation (LoRA; **Rebuffi et al., "Fixing six mistaken assumptions in batched convolutional neural networks training"**, **Lan, "Efficient Low-Rank Approximation of Neural Networks via Low-Rank Weight Matrices"**) and prompt tuning methods **Zhang et al., "Character-level Neuron for Text Classification with Multi-Task Learning"** , which are more related to our work. They will be further elaborated in the subsequent sections.

\textbf{Low-rank adaptation.}  **Rebuffi et al., "Fixing six mistaken assumptions in batched convolutional neural networks training"** assume that weight updates can be approximated by low-rank matrices and propose a low-rank adaptation (LoRA) method for fine-tuning a model.  Building upon this foundational work, many extensions have been developed to enhance LoRAâ€™s performance. For example, ReLoRA **Shen et al., "Efficient Neural Architecture Search with Entropy-SGD"** iteratively trains and merges low-rank adapters to achieve high-rank updates. 
**Rao, "Learning Multi-Task Neural Networks from Scratch via Low-Rank Adaptation"** propose learning low-rank matrices with different learning rates.
**Yin et al., "Meta-Learning for Low-Rank Adaptation in Neural Networks"** explore training a mixture of LoRA modules and leverage dynamic routing mechanisms for different task distributions or domains.

However, for large models, LoRA still requires a considerable number of trainable parameters. To address this limitation, several works have explored the use of random projection **Zhang et al., "Efficient Low-Rank Approximation of Neural Networks via Random Projection"** to further improve parameter efficiency. For example, \textsc{Flora} **Rebuffi et al., "Fast and Accurate Neural Architecture Search with Entropy-SGD"** updates the pretrained matrices with randomly projected gradients, while VeRA **Liu et al., "Efficient Neural Architecture Search with Random Projection"** uses random projections combined with two trainable scaling vectors to approximate each update matrix.

\textbf{Prompt tuning.} 
**Brown et al., "Measuring Massive Multitask Language Understanding with the MultiMNLI Task"** introduce the concept of learning prompt tokens to elicit knowledge from LLMs. Subsequently, **Zhang et al., "Character-level Neuron for Text Classification with Multi-Task Learning"** extend this idea to continuous prompt tuning, where prompt embeddings are optimized through gradient descent while keeping the LLM frozen. Building on this, **Xu et al., "Efficient Low-Rank Approximation of Neural Networks via Prompt Tuning"** further generalize prompt embeddings to a multi-layer setting. **Zhang et al., "Meta-Learning for Low-Rank Adaptation in Neural Networks"** re-parameterize prompt tuning by incorporating a feed-forward neural network with residual connections.  **Yin et al., "Learning Multi-Task Neural Networks from Scratch via Low-Rank Adaptation"** observe that redistributing parameters to learn offsets for input token embeddings can enhance performance. On the other hand, multi-task prompt tuning has been explored, where the learned prompt parameters are reused across different tasks **Zhang et al., "Character-level Neuron for Text Classification with Multi-Task Learning"**. Closely with our work, **Chen et al., "Efficient Neural Architecture Search with Entropy-SGD"** decompose the prompt embedding matrix into two low-rank components: a low-dimensional prompt matrix and a learnable up-projection matrix. By contrast, our ULPT method freezes the up-projection matrix, so that we are able to achieve high performance with much fewer trainable parameters, supported by random-projection theory **Li et al., "Efficient Low-Rank Approximation of Neural Networks via Random Projection"**. Overall, our approach is able to function well with an ultra-low dimension, making it practical to customize millions of LLMs and perform continual learning in an ever-changing environment.