\documentclass{article}
\usepackage{arxiv}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amssymb}
\usepackage{amsthm,bm}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{wrapfig}
\usepackage{diagbox}
\usepackage{pdfpages}
\usepackage{thmtools}

\usepackage{colortbl}
\usepackage{arydshln}
\usepackage{multirow}

\input{math_commands.tex}

\usepackage[round]{natbib}

\title{\Large ULPT: Prompt Tuning with Ultra-Low-Dimensional Optimization}

\author{
Zijun Wu$^{1}$, Yongchang Hao$^{1}$, Lili Mou$^{1\dagger}$\\
$^{1}$Dept. Computing Science, Alberta Machine Intelligence Institute (Amii), University of Alberta \\ 
$^{\dagger}$Canada CIFAR AI Chair, Amii \\
\texttt{\{zijun4, yongcha1\}@ualberta.ca}, \texttt{doublepower.mou@gmail.com}\\
}

\begin{document}
\maketitle

\begin{abstract}
Large language models achieve state-of-the-art performance but are costly to fine-tune due to their size. Parameter-efficient fine-tuning methods, such as prompt tuning, address this by reducing trainable parameters while maintaining strong performance. However, prior methods tie prompt embeddings to the model’s dimensionality, which may not scale well with larger LLMs and more customized LLMs. In this paper, we propose Ultra-Low-dimensional Prompt Tuning (ULPT), which optimizes prompts in a low-dimensional space (e.g., 2D) and use a random but frozen matrix for the up-projection. To enhance alignment, we introduce learnable shift and scale embeddings. ULPT drastically reduces the trainable parameters, e.g., 2D only using 2\% parameters compared with vanilla prompt tuning while retaining most of the performance across $21$ NLP tasks. Our theoretical analysis shows that random projections can capture high-rank structures effectively, and experimental results demonstrate ULPT’s competitive performance over existing parameter-efficient methods.\footnote{Our code is available at \url{https://github.com/MANGA-UOFA/ULPT}}
\end{abstract}


\section{Introduction}

Fine-tuning large language models (LLMs) is essential for adapting them to specific tasks and controlling their outputs~\citep{raffel2020exploring, wei2022finetuned}. However, the enormous size of LLMs makes full fine-tuning prohibitively resource intensive, as it involves updating millions or even billions of parameters. To address this challenge, parameter-efficient fine-tuning methods have emerged as practical solutions, such as low-rank adaptation~(LoRA; \citeauthor{hu2022lora}, \citeyear{hu2022lora}) and prompt tuning~\citep{lester-etal-2021-power, li-liang-2021-prefix}. These methods drastically reduce the number of tunable parameters, offering an efficient alternative while achieving performance comparable to full fine-tuning.

Prompt tuning introduces learnable prompt embeddings exclusively in the input layer of the model~\citep{lester-etal-2021-power, LIU2024208}, automating prompt engineering by gradient descent to guide the frozen LLM in producing task-specific outputs~\citep{petrov2024when, petrov2024prompting}. By contrast, LoRA modifies the model by injecting low-rank weight matrices into its layers, causing the number of trainable parameters to scale with model's depth~\citep{hu2022lora}. Given that LLMs encode substantial knowledge during pretraining~\citep{NEURIPS2020_1457c0d6, NEURIPS2022_8bb0d291} and that both in-context learning and expertly crafted prompts can achieve remarkable results~\citep{NEURIPS2022_9d560961, dong-etal-2024-survey}, prompt tuning offers a more efficient and effective alternative to LoRA in many scenarios~\citep{shi2024dept}. 

Despite its advantages, most existing prompt tuning approaches couple the dimensionality of prompt embeddings with the hidden size of the model~\citep{lester-etal-2021-power, li-liang-2021-prefix, liu-etal-2022-p, choi-etal-2023-smop, razdaibiedina-etal-2023-residual}. As the size of the model increases, the dimensionality of the prompt embedding space also increases~\cite{raffel2020exploring, touvron2023llama}. This scaling leads to unnecessary complexity, as full dimensionality is often not required for task adaptation~\citep{aghajanyan-etal-2021-intrinsic, qin2022exploringuniversalintrinsictask}. Consequently, optimizing in this expanded space becomes inefficient in parameter's usage and may also increase the risk of overfitting, especially for less complex tasks or with limited training data.

In this paper, we propose \textbf{U}ltra-\textbf{L}ow-Dimensional \textbf{P}rompt \textbf{T}uning (ULPT), a method that decouples the prompt/model dimensions and enables prompt tuning in an ultra-low-dimensional space (e.g., 2D). A na\"ive attempt is to jointly optimize the ultra-low-dimensional embeddings with an up-projection matrix~\citep{xiao-etal-2023-decomposed, guo2024loptlowrankprompttuning}, but the learnable up-projection matrix may result in more trainable parameters than vanilla prompt tuning, therefore offsetting the gains in parameter efficiency. 
As shown in Figure~\ref{fig: ulpt diagram}a, our ULPT eliminates this overhead by using a \textit{random} but \textit{frozen} matrix for the up-projection. We further introduce learnable \textit{shift} and \textit{scale} embedding vectors to better align the up-projected embeddings and the model’s prompt space~\citep{wu2024zeroshot}. In addition, we provide theoretical analysis for our ULPT, which not only proves its convergence but also shows that a low-dimensional space with random projection can effectively approximate high-rank information. 

\begin{figure}[!t]
\centering
\includegraphics[width=0.55\textwidth]{images/ULPT_digram.pdf} 
\caption{Overview of our approach. (a) ULPT up-projects ultra-low-dimensional embeddings with a random but fixed matrix, followed by a learnable alignment mechanism shared across all up-projected embeddings. (b) ULPT can significantly reduce parameters usage for LLM customizations.}
\label{fig: ulpt diagram}
\end{figure}

The ultra-low-dimensional nature of our ULPT is particularly suitable for scenarios requiring massive LLM customizations~\cite{peft} and continual learning~\cite{Wang_2022_CVPR}, as shown in Figure~\ref{fig: ulpt diagram}b. 
For example, in a typical prompt tuning approach, each task might require 100K real-valued parameters, which can add up significantly when scaling to millions of customized LLMs or tasks. By contrast, our ULPT with 2D prompt embeddings can reduce this to just 2K parameters per task, presenting a dramatic parameter saving to just 2\% of the original usage.

We conducted experiments across $21$ NLP datasets to evaluate our ULPT. The results demonstrate that ULPT can extend a few-token vanilla prompt tuning setup to a $100$-token configuration without increasing the number of trainable parameters, while matching the performance of a fully parameterized $100$-token prompt tuning setup. With appropriate settings of the dimension, ULPT outperforms existing prompt tuning-based methods, while requiring much fewer trainable parameters. 

In summary, our main contributions include:
\begin{itemize}
    \item We introduce ULPT (Ultra-Low-Dimensional Prompt Tuning), which optimizes prompts in a low-dimensional space with a random up-projection and learnable shift and scale vectors, drastically reduces trainable parameters while maintaining performance.
    \item We provide theoretical analysis showing ULPT's ability to capture high-rank structures effectively and ensure convergence. 
    \item Across $21$ NLP tasks, ULPT delivers comparable performance to vanilla prompt tuning while reducing trainable parameters 2\% of the original usage. When scaling to higher dimensions, it outperforms existing prompt tuning-based methods with much fewer trainable parameters.
\end{itemize}

\section{Related Work}

\textbf{Parameter-efficient fine-tuning.}
With the rapid growth of pretrained neural networks, researchers have investigated parameter-efficient fine-tuning methods that update only a small set of parameters while maintaining high performance. One straight way is to tune specific components of the model. For example, BitFit~\cite{ben-zaken-etal-2022-bitfit} updates only the bias terms, and LayerNorm tuning~\cite{zhao2024tuning} only trains the layer-norm parameters.
Another line of work involves introducing and training small, task-specific non-linear modules, such as Adapters~\cite{pmlr-v97-houlsby19a} and AdapterDrop~\cite{ruckle-etal-2021-adapterdrop}. Other methods steer the activation representations either globally or locally~\cite{wu2024reft, yin2024lofit}.

Two other prominent paradigms are low-rank adaptation (LoRA; \citeauthor{hu2022lora}, \citeyear{hu2022lora}) and prompt tuning methods~\cite{lester-etal-2021-power}, which are more related to our work. They will be further elaborated in the subsequent sections. 


\textbf{Low-rank adaptation.} \citet{hu2022lora} assume that weight updates can be approximated by low-rank matrices and propose a low-rank adaptation (LoRA) method for fine-tuning a model.  Building upon this foundational work, many extensions have been developed to enhance LoRA’s performance. For example, ReLoRA~\cite{lialin2024relora} iteratively trains and merges low-rank adapters to achieve high-rank updates. 
\citet{hayou2024lora} propose learning low-rank matrices with different learning rates.
\citet{wu2024mixture} explore training a mixture of LoRA modules and leverage dynamic routing mechanisms for different task distributions or domains.

However, for large models, LoRA still requires a considerable number of trainable parameters. To address this limitation, several works have explored the use of random projection~\cite{10.1145/502512.502546} to further improve parameter efficiency. For example, \textsc{Flora}~\cite{hao2024flora} updates the pretrained matrices with randomly projected gradients, while VeRA~\cite{kopiczko2024vera} uses random projections combined with two trainable scaling vectors to approximate each update matrix. 

\textbf{Prompt tuning.} 
\citet{shin-etal-2020-autoprompt} introduce the concept of learning prompt tokens to elicit knowledge from LLMs. Subsequently, \citet{lester-etal-2021-power} extend this idea to continuous prompt tuning, where prompt embeddings are optimized through gradient descent while keeping the LLM frozen. Building on this, \citet{li-liang-2021-prefix} further generalize prompt embeddings to a multi-layer setting. \citet{razdaibiedina-etal-2023-residual} re-parameterize prompt tuning by incorporating a feed-forward neural network with residual connections.  \citet{shi2024dept} observe that redistributing parameters to learn offsets for input token embeddings can enhance performance. On the other hand, multi-task prompt tuning has been explored, where the learned prompt parameters are reused across different tasks~\cite{wang2023multitask}. Closely with our work, \citet{xiao-etal-2023-decomposed} decompose the prompt embedding matrix into two low-rank components: a low-dimensional prompt matrix and a learnable up-projection matrix. By contrast, our ULPT method freezes the up-projection matrix, so that we are able to achieve high performance with much fewer trainable parameters, supported by random-projection theory~\cite{10.1145/502512.502546}. Overall, our approach is able to function well with an ultra-low dimension, making it practical to customize millions of LLMs and perform continual learning in an ever-changing environment.



\section{Methodology}

\subsection{Problem Formulation}
Prompt tuning introduces learnable token embeddings in the input layer of a language model~\cite{lester-etal-2021-power}. These embeddings are optimized via gradient descent based on the task-specific loss signals. During optimization, the model weights remain frozen, while the gradient is backpropagated to the input layer to update the learnable embeddings. Typically, learnable prompt embeddings  $\bm{e}_1, \cdots, \bm{e}_n \in \sR^d$ serve as a prefix~\cite{li-liang-2021-prefix}, followed by the text prompt, which is tokenized and represented by token embeddings $\bm{x}_1, \cdots, \bm{x}_l \in \sR^d$. Overall, the LLM has an input in the form of
\begin{align}
     (\bm{e}_1, \bm{e}_2, \cdots, \bm{e}_n, \bm{x}_1, \bm{x}_2, \cdots, \bm{x}_l)
\end{align}
where $n$ is a predefined prompt length and $l$ represents the length of the tokenized text.
The objective is to optimize the prompt embedding matrix $\mE \in \sR^{n \times d}$ over a given dataset \( \mathcal{D} \) based on the conditional log-likelihood:
\begin{align}
    \underset{\mE}{\arg\max} \sum_{(x, y) \in \mathcal{D}} \log P( y \mid \mE, x)
\end{align}
where \( (x, y) \in \mathcal{D} \) represents input--output pairs in a dataset.

\begin{figure}[!t]
\centering
\includegraphics[width=0.55\textwidth]{images/dim_distribution.pdf} 
\caption{Distribution of prompt embedding values over 100 prompt tokens. We randomly selected $20$ dimensions from the original prompt embeddings, which have 768 dimensions as in the T5-base model. The mean, 25/75 percentiles, and min/max are shown for the embedding values learned in the CoLA and SST-2 tasks (details explained in \S\ref{sec: experimental settings}).}
\label{fig: dims distribution}
\end{figure}


\subsection{Our Ultra-Low-Dimensional Prompt Tuning}               
The learnable prompt embeddings do not inherently need to match the model dimension $\sR^d$ due to the low intrinsic dimensionality of downstream tasks~\cite{aghajanyan-etal-2021-intrinsic, qin2022exploringuniversalintrinsictask}. Inspired by low-rank adaptation~\cite{hu2022lora}, the prompt embedding matrix $\mE$ can be decomposed into the product of two matrices: $\mE = \mZ  \mP$, where $\mZ \in \sR^{n \times r}$ represents the prompt embeddings in an ultra-low $r$-dimensional space, and $\mP \in \sR^{r \times d}$ is a projection matrix that maps the low-dimensional embeddings back to the model's embedding space. 

A na\"ive implementation of this decomposition treats both $\mZ$ and $\mP$ as learnable parameters~\cite{xiao-etal-2023-decomposed}, which reduces the number of trainable parameters to $nr + rd$. This, unfortunately, scales poorly for larger models, as an $r\times d$ up-projection matrix should be learned and stored, which undermines the savings of learnable parameters.

To address this limitation, we propose an ultra-low-dimensional prompt tuning (ULPT) method that only learns $r$-dimensional prompt embeddings $\bm Z$, while keeping the projection $\mP$ \textit{randomly initialized} and \textit{frozen} during training, denoted by $\tilde{\mP} \in \mathbb{R}^{r \times d}$. 
In implementation, we only need to store one single number---the random seed of a random number generator---to reconstruct $\tilde{\mP}$ when an LLM is loaded. 

In this way, we completely eliminate the need for storing the up-project matrix. This not only largely reduces the learnable parameters from $nr+rd$ to $nr$ (plus one extra random seed), but also combats overfitting especially when the fine-tuning dataset is small.

In our pilot study, we observe that typical prompt embeddings $\mE$, even without low-rank treatment, exhibit significant variation across different dimensions, as shown in Figure~\ref{fig: dims distribution}. These variations may cause difficulty during training, therefore, we further introduce a learnable \textit{shift} embedding $\bm{s} \in \mathbb{R}^d$ and a learnable \textit{scale} embedding $\bm{b} \in \mathbb{R}^d$ to adjust the projected embeddings to ensure better alignment with the varying distributions across dimensions. Notice that the shift and scale embeddings are shared across different prompt token positions, but may vary for different tasks.

Specifically, an entry $\hat e_{ij}$ in the up-projected embedding matrix $\hat{\bm E}$ has the following form:
\begin{align}\label{eqn: reparameterization}
    \hat{e}_{ij} = \left( \sum_{k=1}^r z_{ik} \tilde {p}_{kj} \right) {s}_j + {b}_j,
\end{align}
where $z_{ik}$ and $\tilde p_{kj}$ are an entry in $\mZ$ and $\tilde\mP$ matrices, respectively; $s_j$ and $b_j$ are an entry in $\bm s$ and $\bm j$ vectors, respectively. 

Such a treatment introduces two $d$-dimensional vectors, resulting in the total number of trainable parameters being $nr + 2d$. This is significantly more parameter-efficient than full-dimension prompt tuning with $nd$-many parameters~\cite{lester-etal-2021-power} and vanilla low-rank prompt tuning with $(nr+rd)$-many parameters~\cite{xiao-etal-2023-decomposed}.


\subsection{Theoretical Analyses}
\input{theory.tex}


\section{Experiments}
\subsection{Experimental Settings}\label{sec: experimental settings}
\textbf{Datasets.} We evaluate the proposed ULPT method across $21$ NLP tasks following prior work~\cite{asai-etal-2022-attempt, wang2023multitask, shi2024dept}. Those tasks are grouped into 4 categories: (1) \textbf{GLUE}~\cite{wang-etal-2018-glue} is a benchmark suite consisting of various language understanding tasks, such as MNLI~\cite{williams-etal-2018-broad}, QQP~\cite{wang-etal-2018-glue}, QNLI~\cite{demszky2018transformingquestionansweringdatasets}, SST-2~\cite{socher-etal-2013-recursive}, STS-B~\cite{cer-etal-2017-semeval}, MRPC~\cite{dolan-brockett-2005-automatically}, RTE~\cite{giampiccolo-etal-2007-third} and CoLA~\cite{warstadt2019neural_cola}. (2) \textbf{SuperGLUE}~\cite{wang2019superglue} extends GLUE with more challenging tasks with limited training data, consisting of MultiRC~\cite{khashabi-etal-2018-looking}, BoolQ~\cite{clark-etal-2019-boolq}, WiC~\cite{pilehvar-camacho-collados-2019-wic}, WSC~\cite{levesque2012winograd_wnli}, and CB~\cite{de2019commitmentbank}. (3) The \textbf{MRQA} 2019 Shared Tasks~\cite{fisch-etal-2019-mrqa} are a set of QA tasks to test LLM generation capabilities, consisting of Natural Questions~\cite{kwiatkowski-etal-2019-natural}, HotpotQA~\cite{yang-etal-2018-hotpotqa}, SearchQA~\cite{dunn2017searchqa} and NewsQA~\cite{trischler-etal-2017-newsqa}. (4) \textbf{Other tasks} beyond the above test suites are also considered, including WinoGrande~\cite{10.1145/3474381}, Yelp-2~\cite{10.5555/2969239.2969312}, SciTail~\cite{Khot_Sabharwal_Clark_2018}, and PAWS-Wiki~\cite{zhang-etal-2019-paws}. Further details on these datasets are provided in Table~\ref{tab: datasets details} in Appendix~\ref{appendix: details in datasets}.

\textbf{Baselines.} We compare our ULPT against a wide range of baselines to demonstrate its effectiveness and parameter efficiency. First, we evaluate against full-model fine-tuning, which optimizes all model parameters for downstream task adaptation, serving as a strong but parameter-intensive baseline. Second, we include state-of-the-art parameter-efficient methods such as Adapter~\cite{pmlr-v97-houlsby19a}, AdapterDrop~\cite{ruckle-etal-2021-adapterdrop}, BitFit~\cite{ben-zaken-etal-2022-bitfit}, HyperFormer~\cite{mahabadi2021parameter}, HyperDecoder~\cite{ivison-peters-2022-hyperdecoders}, LoRA~\cite{hu2022lora}, and Ladder Side-Tuning (LST;~\citeauthor{sunglst}, \citeyear{sunglst}). Third, we compare ULPT with vanilla prompt tuning (PT) and its variants, including DePT~\cite{shi2024dept}, which learns offsets to the input token embeddings while using a separate learning rate for the prompt embeddings, and DPT~\cite{xiao-etal-2023-decomposed}, which is closely related to ULPT and decomposes prompt embeddings into low-rank matrices. Finally, we compare ULPT with transfer or multi-task learning methods, including SPoT~\cite{vu-etal-2022-spot}, ATTEMPT~\cite{asai-etal-2022-attempt}, and MPT~\cite{wang2023multitask}. 


\textbf{Implementation details.} In our pilot study (Figure~\ref{fig: dims distribution}), we perform vanilla prompt tuning on the T5-base model~\cite{raffel2020exploring} with CoLA and SST-2, using $n =100$ prompt embeddings, each having a dimensionality of $d = 768$. We randomly select $20$ dimensions and report the mean, $25$th/$75$th percentiles, and the minimum/maximum values for each dimension.

In our main experiment, we use the T5-base model with $d=768$. Consistent with prior work~\cite{shi2024dept, xiao-etal-2023-decomposed}, we set the number of prompt tokens $n =100$ for the prompt embeddings $\mQ \in \sR^{n \times r}$ of our ULPT. For the rank $r$, we evaluate four configurations:  $r = 2$, $16$, $64$, and $256$, ranging from an ultra-low-dimensional setup to a more expressive configuration of $1/3$ of the original prompt dimension. All experiments use a batch size of $16$ and a default learning rate of $6e{-1}$ with AdamW. The learning rate follows a linear schedule, warming up for $500$ steps and then decaying linearly to $0$. We set a maximum sequence length of $256$ for most tasks, except for SuperGLUE-MultiRC being $348$ and MRQA being $512$. ULPT is trained on all tasks for up to $100,000$ steps. Performance is evaluated every $1,000$ steps, with the best checkpoint selected based on the validation set. 

In our analysis, T5-small ($d=512$) and T5-large model ($d=1024$) are considered to evaluate the generality of ULPT across different model sizes. We also vary the number of prompt tokens from $10$ to $100$ under different rank configurations. Further details are provided in \S\ref{sec: analysis}.


\begin{table*}[!t]
\centering
\caption{Performance on GLUE and SuperGLUE benchmarks based on the T5-base model. We report standard evaluation metrics, namely, Pearson correlation for STS-B, F1 for MultiRC, and accuracy for other tasks. 
\textsuperscript{\dag}We replicate prompt tuning (PT; \citeauthor{lester-etal-2021-power}, \citeyear{lester-etal-2021-power}) and DPT~\cite{xiao-etal-2023-decomposed} using their default configurations. Our replicated PT results slightly exceed those reported in previous studies. All other baseline results are directly sourced from \citet{shi2024dept}.  \textsuperscript{\ddag}The suggested rank for DPT is r=10 based on \citet{xiao-etal-2023-decomposed}; we replicate DPT with r=64 for controlled comparison with our ULPT.   \textsuperscript{t}Transfer learning methods.  \textsuperscript{m}Multi-task learning methods, whose ``\#param/task'' scores are calculated based on the GLUE benchmark.}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lrccccccccccccccc}
\toprule
 & \multirow{2}{*}{\raisebox{-6mm}{\shortstack{\textbf{\#Param/} \\ \textbf{Task}}}} & \multicolumn{9}{c}{\textbf{GLUE}} & \multicolumn{6}{c}{\textbf{SuperGLUE}} \\
\cmidrule(l){3-11} \cmidrule(l){12-17}
\textbf{Method} & & \textbf{MNLI} & \textbf{QQP} & \textbf{QNLI} & \textbf{SST-2} & \textbf{STS-B} & \textbf{MRPC} & \textbf{RTE} & \textbf{CoLA} & \cellcolor{gray!20}\textbf{Avg.} & \textbf{MultiRC\!\!\!\!} & \textbf{Bool} & \textbf{WiC} & \textbf{WSC} & \textbf{CB} & \textbf{\cellcolor{gray!20}Avg.} \\
\midrule
\multicolumn{17}{c}{\textbf{Single-Task Learning}} \\
\midrule
Fine-tuning & 220M & 86.8 & 91.6 & 93.0 & 94.6 & 89.7 & 90.2 & 71.9 & 61.8 & \cellcolor{gray!20}84.9 & 72.8 & 81.1 & 70.2 & 59.6 & 85.7 & \cellcolor{gray!20}73.9 \\
Adapter & 1.9M & 86.5 & 90.2 & 93.2 & 93.8 & 90.7 & 85.3 & 71.9 & 64.0 & \cellcolor{gray!20}84.5 & 75.9 & 82.5 & 67.1 & 67.3 & 85.7 & \cellcolor{gray!20}75.7 \\
AdapterDrop & 1.1M & 86.3 & 90.2 & 93.2 & 93.6 & 91.4 & 86.3 & 71.2 & 62.7 & \cellcolor{gray!20}84.4 & 72.9 & 82.3 & 68.3 & 67.3 & 85.7 & \cellcolor{gray!20}75.3 \\
BitFit & 280K & 85.3 & 90.1 & 93.0 & 94.2 & 90.9 & 86.8 & 67.6 & 58.2 & \cellcolor{gray!20}83.3 & 74.5 & 79.6 & 70.0 & 59.6 & 78.6 & \cellcolor{gray!20}72.5 \\
LoRA & 3.8M & 86.3 & 89.0 & 93.2 & 94.3 & 90.0 & 90.1 & 75.5 & 63.3 & \cellcolor{gray!20}85.3 & 72.6 & 81.3 & 68.3 & 67.3 & 92.9 & \cellcolor{gray!20}76.5 \\
LST & 3.8M & 85.6 & 88.8 & 93.3 & 94.0 & 90.7 & 90.4 & 71.9 & 58.1 & \cellcolor{gray!20}84.1 & -- & -- & -- & -- & -- & \cellcolor{gray!20}-- \\
\hdashline
PT\textsuperscript{\dag} & 76.8K & 84.6 & 90.2 & 93.3 & 94.4 & 90.5 & 88.7 & 77.7 & 59.5 & \cellcolor{gray!20}84.9 & 72.3 & 80.4 & 67.7 & 67.3 & 78.6 & \cellcolor{gray!20}73.3 \\
DePT & 76.8K & 85.0 & 90.4 & 93.2 & 94.2 & 90.8 & 90.7 & 79.1 & 63.8 & \cellcolor{gray!20}85.9 & 74.3 & 79.3 & 68.7 & 67.3 & 92.9 & \cellcolor{gray!20}76.5 \\
DPT\textsuperscript{\dag}(r=10) & 9.0K & 84.4 & 90.2 & 93.3 & 94.6 & 91.2 & 87.7 & 77.7 & 57.8 & \cellcolor{gray!20}84.6 & 74.5 & 78.7 & 66.8  & 67.3 & 71.4 & \cellcolor{gray!20}71.7 \\
DPT\textsuperscript{\ddag}(r=64) & 55.6K & 85.2 & 90.3 & 92.9 & 93.6 & 90.4 & 88.2 & 79.1 & 63.5 & \cellcolor{gray!20}85.4 & 73.2 & 80.1 & 63.0  & 67.3 & 85.7  & \cellcolor{gray!20}73.9 \\
\hdashline
ULPT (r=2) & 1.7K & 81.9 & 90.3 & 92.3 & 92.9 & 89.8 & 89.2 & 76.3 & 59.5 & \cellcolor{gray!20}84.0 & 73.4 & 76.7 & 67.4 & 67.3 & 71.4 & \cellcolor{gray!20}71.2 \\
ULPT (r=16) & 3.1K & 82.9 & 90.0 & 93.1 & 93.8 & 90.5 & 89.2 & 80.6 & 54.3 & \cellcolor{gray!20}84.3 & 72.6 & 77.7 & 66.1 & 67.3 & 89.3 & \cellcolor{gray!20}74.6 \\
ULPT (r=64) & 7.9K & 84.9 & 90.3 & 93.1 & 93.5 & 90.7 & 90.2 & 81.3 & 63.7 & \cellcolor{gray!20}\textbf{86.0} & 73.1 & 78.2 & 69.0 & 67.3 & 96.4 & \cellcolor{gray!20}\textbf{76.8} \\
ULPT (r=256) & 27.1K & 85.5 & 90.3 & 92.8 & 94.3 & 90.6 & 90.7 & 76.3 & 63.7 & \cellcolor{gray!20}85.5 & 74.3 & 79.9 & 63.3 & 67.3 & 89.3 & \cellcolor{gray!20}74.8 \\
\midrule
\multicolumn{17}{c}{\textbf{Multi-Task Learning \& Transfer Learning}} \\
\midrule
Fine-tuning\textsuperscript{m} & 28M & 85.7 & 91.1 & 92.0 & 92.5 & 88.8 & 90.2 & 75.4 & 54.9 & \cellcolor{gray!20}83.8 & 74.4 & 81.1 & 70.0 & 71.2 & 85.7 & \cellcolor{gray!20}76.1 \\
Adapter\textsuperscript{m} & 1.8M & 86.3 & 90.5 & 93.2 & 93.0 & 89.9 & 90.2 & 70.3 & 61.5 & \cellcolor{gray!20}84.4 & 72.6 & 82.3 & 66.5 & 67.3 & 89.3 & \cellcolor{gray!20}75.6 \\
HyperFormer\textsuperscript{m} & 638K & 85.7 & 90.0 & 93.0 & 93.0 & 89.7 & 87.2 & 75.4 & 63.7 & \cellcolor{gray!20}84.8 & 72.9 & 82.5 & 69.0 & 67.3 & 85.7 & \cellcolor{gray!20}75.4 \\
HyperDecoder\textsuperscript{m} & 1.8M & 86.0 & 90.5 & 93.4 & 94.0 & 90.5 & 87.7 & 71.7 & 55.9 & \cellcolor{gray!20}83.7 & 70.4 & 78.8 & 67.1 & 61.5 & 82.1 & \cellcolor{gray!20}72.0 \\
SPoT\textsuperscript{t} & 76.8K & 85.4 & 90.1 & 93.0 & 93.4 & 90.0 & 79.7 & 69.8 & 57.1 & \cellcolor{gray!20}82.3 & 74.0 & 77.2 & 67.0 & 50.0 & 46.4 & \cellcolor{gray!20}62.9 \\
ATTEMPT\textsuperscript{t} & 232K & 84.3 & 90.3 & 93.0 & 93.0 & 89.7 & 85.7 & 74.3 & 57.4 & \cellcolor{gray!20}83.4 & 74.4 & 78.8 & 66.8 & 53.8 & 78.6 & \cellcolor{gray!20}70.5 \\
MPT\textsuperscript{t} & 77.6K & 85.9 & 90.3 & 93.1 & 93.8 & 90.4 & 89.1 & 79.4 & 62.4 & \cellcolor{gray!20}85.6 & 74.8 & 79.6 & 69.0 & 67.3 & 79.8 & \cellcolor{gray!20}74.1 \\
ATTEMPT\textsuperscript{t+m} & 96K & 83.8 & 90.0 & 93.1 & 93.7 & 90.8 & 86.1 & 79.9 & 64.3 & \cellcolor{gray!20}85.2 & 74.4 & 78.5 & 66.5 & 69.2 & 82.1 & \cellcolor{gray!20}74.1 \\
MPT\textsuperscript{t+m} & 10.5K & 84.3 & 90.0 & 93.0 & 93.0 & 90.4 & 89.2 & 82.7 & 63.5 & \cellcolor{gray!20}85.8 & 74.8 & 79.6 & 70.2 & 67.3 & 89.3 & \cellcolor{gray!20}76.1 \\
\bottomrule
\end{tabular}%
}
\label{tab:glue-superglue}
\end{table*}


\begin{table*}[!t]
\centering
\caption{Performance on MRQA and other benchmarks using the T5-base model. The standard metrics reported are the F1 score for MRQA tasks and accuracy for other datasets. ${}^\dagger$Results are obtained based on our replication using default configurations. Other baseline results are sourced from \citet{shi2024dept}.}
\resizebox{0.83\textwidth}{!}{%
\begin{tabular}{lccccccccccc}
\toprule
 &  & \multicolumn{5}{c}{\textbf{MRQA}} & \multicolumn{5}{c}{\textbf{Others}} \\
\cmidrule(l){3-7} \cmidrule(l){8-12}
\textbf{Method} & \textbf{\#Param} & \textbf{NQ} & \textbf{HQA} & \textbf{SQA} & \textbf{NewsQA} & \cellcolor{gray!20}\textbf{Avg.} & \textbf{WG} & \textbf{Yelp} & \textbf{SciTail} & \textbf{PAWS} & \cellcolor{gray!20}\textbf{Avg.} \\
\midrule
Fine-tuning & 220M & 75.1 & 77.5 & 81.1 & 65.2 & \cellcolor{gray!20}\textbf{74.7} & 61.9 & 96.7 & 95.8 & 94.1 & \cellcolor{gray!20}\textbf{87.1} \\
Adapter & 1.9M & 74.2 & 77.6 & 81.4 & 65.6 & \cellcolor{gray!20}74.7 & 59.2 & 96.9 & 94.5 & 94.3 & \cellcolor{gray!20}86.2 \\
BitFit & 280K & 70.7 & 75.5 & 77.7 & 64.1 & \cellcolor{gray!20}72.0 & 57.2 & 94.7 & 94.7 & 92.0 & \cellcolor{gray!20}84.7 \\
LoRA & 3.8M & 72.4 & 62.3 & 72.5 & 56.9 & \cellcolor{gray!20}66.0 & 58.2 & 97.1 & 94.7 & 94.0 & \cellcolor{gray!20}86.0 \\
\hdashline
SPoT & 76.8K & 68.2 & 74.8 & 75.3 & 58.2 & \cellcolor{gray!20}69.1 & 50.4 & 95.4 & 91.2 & 91.1 & \cellcolor{gray!20}82.0 \\
ATTEMPT & 232K & 70.4 & 75.2 & 78.5 & 62.8 & \cellcolor{gray!20}71.4 & 57.6 & 96.7 & 93.1 & 92.1 & \cellcolor{gray!20}84.9 \\
PT\textsuperscript{\dag} & 76.8K & 70.0 & 74.7 & 75.3 & 63.0 & \cellcolor{gray!20}70.8 & 49.6 & 95.6 & 92.0  & 57.9 & \cellcolor{gray!20}73.8 \\
DPT\textsuperscript{\dag}(r=10) & 9.0K & 71.3 & 75.5 & 76.3 & 63.5 & \cellcolor{gray!20}71.7 & 49.6 & 96.1 & 95.6 & 92.2 & \cellcolor{gray!20}83.4 \\
DPT (r=256) & 222K & 71.4 & 76.0 & 77.6 & 64.2 & \cellcolor{gray!20}72.3 & 49.6 & 96.3 & 95.2 & 55.8 & \cellcolor{gray!20}74.2 \\
DePT & 76.8K & 73.2\textsubscript{0.3} & 76.0\textsubscript{0.2} & 77.6\textsubscript{0.2} & 64.4\textsubscript{0.1} & \cellcolor{gray!20}73.0 & 59.0\textsubscript{0.2} & 96.8\textsubscript{0.1} & 95.6\textsubscript{0.2} & 93.7\textsubscript{0.1} & \cellcolor{gray!20}86.3 \\
MPT & 77.6K & 72.0\textsubscript{0.1} & 75.8\textsubscript{0.1} & 77.2\textsubscript{0.1} & 63.7\textsubscript{0.1} & \cellcolor{gray!20}72.2 & 56.5\textsubscript{0.9} & 96.4\textsubscript{0.0} & 95.5\textsubscript{0.3} & 93.5\textsubscript{0.1} & \cellcolor{gray!20}85.5 \\
\hdashline
ULPT (r=2) & 1.7K & 67.2\textsubscript{0.2} & 74.0\textsubscript{0.1} & 71.7\textsubscript{0.2} & 61.4\textsubscript{0.1} & \cellcolor{gray!20}68.6 & 49.5\textsubscript{0.2} & 95.6\textsubscript{0.1} & 93.0\textsubscript{0.9} & 90.4\textsubscript{0.2} & \cellcolor{gray!20}82.1\\
ULPT (r=16) & 3.1K & 68.0\textsubscript{0.3} & 74.3\textsubscript{0.0} & 72.9\textsubscript{0.1} & 61.3\textsubscript{0.5} & \cellcolor{gray!20}69.1 & 52.3\textsubscript{0.9} & 95.6\textsubscript{0.2} & 93.1\textsubscript{0.7} & 90.5\textsubscript{0.3} & \cellcolor{gray!20}82.9 \\
ULPT (r=64) & 7.9K & 70.7\textsubscript{0.3} & 75.3\textsubscript{0.1} & 75.3\textsubscript{0.1} & 62.9\textsubscript{0.5} & \cellcolor{gray!20}71.1 & 56.6\textsubscript{0.9} & 96.2\textsubscript{0.1} & 94.4\textsubscript{0.9} & 91.7\textsubscript{0.4} & \cellcolor{gray!20}84.7 \\
ULPT (r=256) & 27.1K & 72.6\textsubscript{0.2} & 76.5\textsubscript{0.1} & 77.9\textsubscript{0.1} & 64.2\textsubscript{0.2} & \cellcolor{gray!20}72.8 & 57.6\textsubscript{0.8} & 96.6\textsubscript{0.2} & 96.2\textsubscript{0.1} & 93.0\textsubscript{0.1} & \cellcolor{gray!20}85.9 \\
\bottomrule
\end{tabular}%
}
\label{tab:mrqa and others}
\end{table*}


% main results
\subsection{Main Results}
\textbf{Performance on GLUE and SuperGLUE.} As shown in Table~\ref{tab:glue-superglue}, our ULPT achieves similar or higher performance on GLUE and SuperGLUE benchmark datasets compared with previous methods, while maintaining remarkable parameter efficiency. 

Profoundly, the ultra-low-rank configuration of $r = 2$ retains at least $97\%$ performance of vanilla prompt tuning (PT), achieving average accuracy points of $84.0$ on GLUE and $71.2$ on SuperGLUE with only 2\% of the parameters. This highlights the capability of ULPT and its advantage in large-scale LLM customization.

With a moderate rank of $r=64$, our ULPT outperforms that with $r=256$ and other state-of-the-art models, showing that our approach not only reduces the number of parameters but also alleviates the overfitting problem. Specifically, the DPT model~\cite{xiao-etal-2023-decomposed} learns an up-projection matrix, resulting in lower performance and 7x more parameters when the rank is controlled; even with the best rank $r=10$ suggested by the original paper~\cite{xiao-etal-2023-decomposed}, DPT is inadmissible as it is worse than our ULPT (with $r=64$) in both parameter efficiency and performance. 


Our ULPT also has significant advantages in multi-task setups. A transfer learning method initializes a model by task mixtures and then adapts it to a specific task; therefore, it cannot save parameters. Example studies of transfer learning include SPoT~\cite{vu-etal-2022-spot} and ATTEMPT~\cite{asai-etal-2022-attempt}. Our ULPT approach outperforms them in terms of accuracy and parameter efficiency, while offering a simpler training pipeline.
Multi-task learning, on the other hand, shares certain parameters across different tasks~\cite{mahabadi2021parameter,ivison-peters-2022-hyperdecoders,wang2023multitask}, and thus, the parameter efficiency is measured on a per-task basis. Despite this, our ULPT still outperforms multi-task prompt tuning methods in both accuracy and per-task parameter efficiency due to its ultra-low-dimensional nature.


\textbf{Performance on MRQA and other NLP tasks.} Table~\ref{tab:mrqa and others} presents the results on the MRQA dataset and four additional tasks in the ``Others'' category. Following the standard practice on these benchmarks \cite{wang2023multitask, shi2024dept}, we run ULPT three times with different seeds and report the mean and standard deviation. 

Unlike GLUE and SuperGLUE performance, ULPT exhibits consistent improvement when the rank is higher. This is probably because these tasks are more challenging, which aligns with the observation that full-model fine-tuning  outperforms parameter-efficient methods on these tasks. Nevertheless, ULPT achieves competitive performance (slightly worse than the best-performing DePT approach), while saving parameters by multiple folds.


\subsection{In-Depth Analyses}\label{sec: analysis}

\begin{figure}[!t]
\centering
    \begin{minipage}[t]{0.27\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pilot_sst_training_loss.pdf}
    \end{minipage}
    \hspace{0.1cm}
    \begin{minipage}[t]{0.27\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pilot_sst_eval_acc.pdf}
    \end{minipage}
\caption{\textbf{Left:} Training loss curves on SST2 comparing ULPT with and without learnable shift and scale embeddings across different rank configurations. \textbf{Right:} Evaluation accuracy curves on SST2. For clarity, we present the case $r=2$, where our ULPT is at a disadvantage. The trend for other configurations is similar.}
\label{fig: pilot sst2}
\end{figure}

\begin{figure}[!t]
\centering
\includegraphics[width=0.54\textwidth]{images/similarity_matrix.pdf}
\caption{Pairwise similarities of the learned shift (\textbf{left}) and scale (\textbf{right}) embeddings for various rank configurations on SST-2.}
\label{fig: similarity matrix}
\end{figure}

\textbf{Ablation study on shift and scale embeddings.}
We conduct an ablation study on the learnable \textit{shift} embedding $\bm{b} \in \mathbb{R}^d$ and \textit{scale} embedding $\bm{s} \in \mathbb{R}^d$, using the SST-2 dataset\footnote{Our preliminary experiments show that prompt tuning on SST-2, a smaller dataset, leads to faster convergence, making it suitable for validating the effectiveness of the ablated models. However, for the rest of the analysis, we use MNLI and Natural Questions, to better test the expressiveness of the ablated models and reduce the risk of overfitting observed in our main experiment.} with the T5-base model as the testbed, where we set the token number to be $n=100$. The results are shown in Figure~\ref{fig: pilot sst2}. As seen, the dotted lines correspond to removing both shift and scale embeddings; their training loss remains high, suggesting that na\"ively freezing the projection matrix $\Tilde{\mP}$ hinders the optimization process and consequently lowers the model performance. Introducing a learnable shift embedding $\bm{b}$  provides a substantial improvement (dashed lines), particularly in the low-dimensional configuration of $r=2$. A learnable scale embedding \textit{scale} $\bm{s}$ further improves the training process and performance (solid lines). The ablation study shows that, although shift and scale embeddings are additional $2d$-many parameters, they play an important role in ultra-low-dimensional prompt tuning.

To further investigate the behavior of these embeddings, we analyze the pairwise cosine similarities of the shift $\bm{s}$ and scale $\bm{b}$ embeddings under different rank configurations, visualized in Figure~\ref{fig: similarity matrix}. We see that they exhibit interesting patterns: the learned shift embeddings have consistently high similarity scores with different rank configurations, indicating their primary role as an alignment mechanism after up-projection. By contrast, the scale embeddings show near-zero pairwise similarities, as they depend on the sampled (and frozen) random projection matrix $\tilde{\bm P}$.

\begin{figure}[!t]
\centering
    \begin{minipage}[t]{0.27\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/plot_control_p_l_mnli.pdf}
    \end{minipage}
    \hspace{0.1cm}
    \begin{minipage}[t]{0.27\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/plot_control_p_l_nq.pdf}
    \end{minipage}
\caption{{Results on MNLI and Natural Questions with the T5-base model. The number of prompt tokens for both ULPT and na\"ive prompt tuning varies from $10$ to $100$.}}
\label{fig: control ULPT length}
\end{figure}

\begin{figure}[!t]
\centering
    \begin{minipage}[t]{0.27\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/plot_control_param_mnli.pdf}
    \end{minipage}
    \hspace{0.1cm}
    \begin{minipage}[t]{0.27\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/plot_control_param_nq.pdf}
    \end{minipage}
\caption{Results on MNLI and Natural Questions with controlled numbers of trainable parameters, comparing ULPT and prompt tuning across three T5 model sizes (small, base, and large).}
\label{fig: control parameters}
\end{figure}

\textbf{Analysis of prompt lengths and dimensions.} Recall that Table~\ref{tab:glue-superglue} has analyzed our ULPT performance with different ranks. We now vary the number of prompt tokens and plot the trend in Figure~\ref{fig: control ULPT length}. We see that our ULPT exhibits a similar trend as vanilla prompting, where the performance increases with a longer prompt. With an appropriate rank configuration, our ULPT consistently outperforms vanilla prompt tuning under different lengths.

Our low-rank ULPT provides a trade-off between the prompt length and dimension. We compare ULPT with vanilla prompt tuning when the learnable parameters are controlled. For our ULPT, we keep the prompt token number as $100$ and vary the rank from $2$ to $256$; for vanilla full-dimensional prompt tuning, we vary the token number from $2$ to $50$. This analysis is also conducted with three model sizes: T5-small, T5-base, and T5-large. 

Figure~\ref{fig: control parameters} illustrates the results, showing that our low-dimensional ULPT with more tokens (solid lines) always outperform vanilla full-dimensional prompt tuning with fewer tokens (dashed lines). The analysis suggests that, when the number of learnable parameters is controlled, a longer prompt with a lower dimension offers more flexibility due to the additional Transformer steps. 


\begin{table}[!t]
\centering
\caption{Results on MNLI and Natural Questions for training either $\mP$ or  $\mZ$ with T5-base. Numbers in the brackets refer to the rank $r$ given the controlled number of parameters.}
\resizebox{0.55\textwidth}{!}{%
\begin{tabular}{lccllll}
\toprule
& \multicolumn{2}{c}{\textbf{Train?}} & \multicolumn{4}{c}{\textbf{\#Trainable Parameters}} \\
\cmidrule(l){2-3} \cmidrule(l){4-7}
\textbf{Dataset} & $\mZ$ & $\mP$ & 1.7K & 3.1K & 7.9K & 27.1K \\
\midrule
\multirow{2}{*}{MNLI}& \checkmark & & \textbf{81.9} (2) & \textbf{82.9} (16) & \textbf{84.9} (64) & \textbf{85.5} (256) \\
& & \checkmark & -- & \textbf{82.9} (2) & 84.5 (8) & 85.3 (33) \\
\midrule
\multirow{2}{*}{NQ} & \checkmark & & $\textbf{67.2}_{0.2}$ (2) & $\textbf{68.0}_{0.4}$ (16) & $\textbf{70.7}_{0.3}$ (64) & $\textbf{72.6}_{0.2}$ (256) \\
& & \checkmark & -- & 66.9 (2) & 70.0 (8) & 72.0 (33) \\
\bottomrule
\end{tabular}}
\label{tab: tune P or Qresults}
\end{table}


\textbf{Comparison with an alternative method of tuning $\mP$.} The low-rank decomposition $\mE=\mZ \mP$ allows an alternative approach that freezes $\mZ$ and tunes $\mP$, which contrasts with our approach that freezes $\mP$ and tunes $\mZ$. The comparison is shown in Table~\ref{tab: tune P or Qresults}.
The alternative setup (tuning $\mP$) can be viewed as learning an up-projection from a set of random but frozen low-dimensional vectors. However, a key drawback of making $\mP$ trainable is the rapid growth in the number of parameters when the rank $r$ increases, since $d \gg n$ in most practical scenarios. To ensure a fair comparison, we control the number of parameters by varying the rank $r$ for both methods.

As seen, tuning $\mP$ fails to be feasible in the 1.7K-parameter setup. Even if we set $r=2$, tuning $\mP$ results in 3.1K parameters, equivalent to our $r=16$ setup.
With a larger budget, tuning $\mP$ achieves slightly worse performance than our ULPT which tunes $\mZ$. This analysis verifies the expressiveness of random projections; it also shows that our ULPT is superior to the alternative approach.


\begin{table*}[!t]
\centering
\caption{Results on Bloomz, a decoder model with varying sizes (560M, 1.7B, and 3B) and hidden dimensions (1024, 2048, and 2560). We compare ULPT with prompt tuning by conditioning on the same number of trainable parameters.}
\resizebox{0.99\textwidth}{!}{%
\begin{tabular}{lccccccccccccccccc}
\toprule
\textbf{Model} & \textbf{Method} & SST-2 & HQA & WG & \cellcolor{gray!20}\textbf{Avg.} & SST-2 & HQA & WG & \cellcolor{gray!20}\textbf{Avg.} & SST-2 & HQA & WG & \cellcolor{gray!20}\textbf{Avg.} & SST-2 & HQA & WG & \cellcolor{gray!20}\textbf{Avg.} \\
\midrule
 &  & \multicolumn{4}{c}{\#Param=2K, ULPT (r=2)} & \multicolumn{4}{c}{\#Param=4K, ULPT (r=16)} & \multicolumn{4}{c}{\#Param=8K, ULPT (r=64)} & \multicolumn{4}{c}{\#Param=28K, ULPT (r=256)} \\
\cmidrule(l){3-6} \cmidrule(l){7-10} \cmidrule(l){11-14} \cmidrule(l){15-18}
\multirow{2}{*}{Bloomz-560M} & PT & 89.8 & 42.9 & 48.6  & \cellcolor{gray!20}60.4 & 91.1 & 53.0 & 52.0 & \cellcolor{gray!20}65.4 & 91.9 & 57.2 & 50.0 & \cellcolor{gray!20}66.4 & 92.2 & 60.6 & 52.2  & \cellcolor{gray!20}68.3 \\
& ULPT & 90.2 & 52.4 & 51.7 & \cellcolor{gray!20}\textbf{64.8} & 92.2 & 55.7 & 53.1 & \cellcolor{gray!20}\textbf{67.0} & 91.8 & 59.3 & 53.1 & \cellcolor{gray!20}\textbf{68.1} & 92.6 & 62.5 & 52.2 & \cellcolor{gray!20}\textbf{69.1} \\
\midrule
 &  & \multicolumn{4}{c}{\#Param=4K, ULPT (r=2)} & \multicolumn{4}{c}{\#Param=6K, ULPT (r=16)} & \multicolumn{4}{c}{\#Param=10K, ULPT (r=64)} & \multicolumn{4}{c}{\#Param=30K, ULPT (r=256)} \\
\cmidrule(l){3-6} \cmidrule(l){7-10} \cmidrule(l){11-14} \cmidrule(l){15-18}
\multirow{2}{*}{Bloomz-1.7B} & PT & 93.2 & 64.6 & 50.1 & \cellcolor{gray!20}69.3  & 93.5 & 66.1 & 51.5 & \cellcolor{gray!20}70.4  & 94.0 & 67.3 & 55.3 & \cellcolor{gray!20}72.2 & 94.7 & 69.1 & 55.4  & \cellcolor{gray!20}73.1 \\
& ULPT & 94.4 & 65.6 & 54.6 & \cellcolor{gray!20}\textbf{71.5} & 93.9 & 66.3 & 55.6 & \cellcolor{gray!20}\textbf{71.9} & 94.3 & 68.0 & 55.2 & \cellcolor{gray!20}\textbf{72.5} & 95.1 & 69.3 & 57.4  & \cellcolor{gray!20}\textbf{73.9} \\
\midrule
 &  & \multicolumn{4}{c}{\#Param=5K, ULPT (r=2)} & \multicolumn{4}{c}{\#Param=8K, ULPT (r=16)} & \multicolumn{4}{c}{\#Param=13K, ULPT (r=64)} & \multicolumn{4}{c}{\#Param=31K, ULPT (r=256)} \\
\cmidrule(l){3-6} \cmidrule(l){7-10} \cmidrule(l){11-14} \cmidrule(l){15-18}
\multirow{2}{*}{Bloomz-3B} & PT & 93.2 & 66.1 & 50.5 & \cellcolor{gray!20}69.9 &  94.5 & 69.0 &56.0 & \cellcolor{gray!20}73.2 & 94.9 & 69.1 & 58.9 & \cellcolor{gray!20}74.3 & 94.9 & 71.5 & 60.0 & \cellcolor{gray!20}75.5 \\
& ULPT & 94.0 & 68.1 & 53.5 & \cellcolor{gray!20}\textbf{71.9}  & 94.4 & 68.9 & 57.1  &\cellcolor{gray!20} \textbf{73.5}  & 94.7 & 70.9  & 58.5 & \cellcolor{gray!20} \textbf{74.7} & 95.0 & 71.8 & 60.7 & \cellcolor{gray!20} \textbf{75.8}\\
\bottomrule
\end{tabular}}
\label{tab: detailed decoder}
\end{table*}

\subsection{Results on Decoder Models}
In our main experiments, we use the  encoder--decoder T5 model~\cite{raffel2020exploring}, following most previous work on prompt tuning~\cite{lester-etal-2021-power,wang2023multitask,shi2024dept}.

We extend the evaluation of ULPT to Bloomz~\cite{muennighoff-etal-2023-crosslingual}, a decoder-only model with three difference sizes: 560M, 1.7B, and 3B, having hidden dimensions of 1024, 2048, and 2560 respectively. 
For evaluation diversity, we select three mid-sized tasks from each task group: SST-2, HotpotQA, and Winogrande, providing assessment across classification, multi-hop reasoning, and coreference reasoning.
Since Bloomz models are larger than the T5 series, we train up to 30K steps with a batch size of $4$, while keeping other hyperparamters the same as our main experiment.

We consider comparing ULPT with prompt tuning under different parameter budgets for text generation. Specifically, we vary the rank of ULPT from $2$ to $256$ while fixing the length $n=100$. For full-dimensional prompt tuning, the token number is adjusted to match the parameter count. 

Results in Table~\ref{tab: detailed decoder} show that ULPT consistently outperforms prompt tuning across all model sizes and tasks given a fixed parameter budget. These findings align with our earlier analysis (\S\ref{sec: analysis}), confirming that ULPT can be applied to different model architectures. 


\section{Conclusion}
In this paper, we propose Ultra-Low-Dimensional Prompt Tuning (ULPT), a novel parameter-efficient prompt tuning method that achieves superior performance across diverse NLP tasks with significantly fewer trainable parameters. ULPT decouples prompt embeddings from the model’s dimensionality, optimizing in a low-dimensional space and projecting into the model’s embedding space by a frozen random projection. 

Our research offers future opportunities for large-scale LLM customizations, as efficient storage of task-specific models is increasingly critical.



\bibliographystyle{plainnat}
\bibliography{main}

\newpage
\appendix
\section{Appendix}


\section{Dataset Details}\label{appendix: details in datasets}
\begin{table}[!t]
\centering
\caption{Dataset information and statistics.}
\resizebox{0.92\textwidth}{!}{%
\begin{tabular}{lrrrrrll}
\toprule
\textbf{Dataset} & \textbf{Source Length} & \textbf{Target Length} & \textbf{\#Train} & \textbf{\#Valid} & \textbf{\#Test} & \textbf{Type} & \textbf{Size}\\ 
\midrule
\multicolumn{7}{c}{\textbf{GLUE Benchmark}} \\
\midrule
MNLI   & 31.8  & 1.0 & 392,702 & 9,832  & 9,815  & Natural language inference     & Large          \\
QQP    & 24.1  & 1.0 & 362,846 & 1,000  & 40,431 & Paraphrasing   & Large     \\
QNLI   & 38.4  & 1.0 & 103,743 & 1,000  & 5,463  & Natural language inference     & Large        \\
SST-2  & 10.4  & 1.0 & 66,349  & 1,000  & 872    & Sentiment analysis   & Medium      \\
STS-B  & 21.9  & 1.0 & 5,749   & 750    & 750    & Sentence similarity & Small \\
MRPC   & 45.9  & 1.0 & 3,668   & 204    & 204    & Paraphrasing   &Small     \\
RTE    & 54.4  & 1.0 & 2,490   & 138    & 139    & Natural language inference       &Small        \\
CoLA   & 8.7   & 1.0 & 8,551   & 521    & 522    & Acceptability  &Small    \\
\midrule
\multicolumn{7}{c}{\textbf{SuperGLUE Benchmark}} \\
\midrule
MultiRC & 286.1 & 1.0 & 27,243  & 2,424  & 2,424  & Question answering & Medium   \\
BoolQ   & 108.3 & 1.0 & 9,427   & 1,635  & 1,635  & Question answering  & Small  \\
WiC     & 18.4  & 1.0 & 5,428   & 319    & 319    & Word sense disambiguation & Small \\
WSC     & 28.1  & 1.0 & 554     & 52     & 52     & Commonsense reasoning  & Small\\
CB      & 64.6  & 1.0 & 250     & 28     & 28     & Natural language inference  & Small             \\
% ReCoRD  & 210.7 & 1.5 & 137,484 & 1,370  & 15,176 & Common Sense Reason.  \\
\midrule
\multicolumn{7}{c}{\textbf{MRQA 2019 Shared Task}} \\
\midrule
NaturalQuestions & 242.7 & 4.5 & 103,071 & 1,000  & 12,836 & Question answering & Large\\
HotpotQA         & 225.7 & 2.6 & 71,928  & 1,000  & 5,901  & Question answering & Medium\\
SearchQA         & 942.8 & 2.0 & 116,384 & 1,000  & 16,980 & Question answering & Large\\
NewsQA           & 615.5 & 5.1 & 73,160  & 1,000  & 4,212  & Question answering & Medium\\
\midrule
\multicolumn{7}{c}{\textbf{Other Datasets}} \\
\midrule
WinoGrande      & 23.8  & 1.0 & 39,398  & 1,000  & 1,267  & Commonsense reasoning & Medium\\
YelpPolarity    & 134.0 & 1.0 & 100,000 & 1,000  & 38,000 & Sentiment analysis  & Large       \\
SciTail         & 30.8  & 1.0 & 23,596  & 652    & 652    & Natural language inference & Medium \\
PAWS            & 44.7  & 1.0 & 49,401   & 8,000  & 8,000  & Sentence Similarity  & Medium  \\
\bottomrule
\end{tabular}}
\label{tab: datasets details}
\end{table}

We present detailed information for the 21 NLP tasks in Table~\ref{tab: datasets details}. Following previous work~\citep{wang2023multitask, shi2024dept}, we preprocess the labels for classification and multiple-choice tasks into a single-token label (e.g., 0, 1, 2, …) to simplify evaluation. For MRQA, the model generates an answer containing a sequence of tokens.

Based on the training set size, the tasks can be roughly categorized into three scales: small ($<$10K samples), medium (10--100K samples), and large ($>$100K samples). Notably, SuperGLUE contains small training sets, and is generally considered more challenging than GLUE, making it more susceptible to overfitting due to its limited samples. By contrast, MRQA and the tasks in the ``Others'' category consist of more complex tasks, likely requiring more parameters to capture their difficulty.


\input{theory_apx}


\end{document}


