\allowdisplaybreaks
\section{Theoretical Results}\label{apx:proof}


\subsection{Proof of Theorem~\ref{thm:exsitance}}\label{apx:proof:expr}
\expr*
\begin{proof}

Setting $\vz_i = \mP \ve_i$, we have
\begin{align}
        \Pr\left( \left| \frac{ \| \vz_i - \vz_j \| - \|\ve_i - \ve_j\|}{\|\ve_i - \ve_j\|} \right| \ge \epsilon \right)
        =& \Pr\left( \left| \frac{ \| \mP \left( \ve_i - \ve_j \right) \| - \|\ve_i - \ve_j\|}{\|\ve_i - \ve_j\|}  \right| \ge \epsilon \right) \\
        \le& \frac{2}{ \exp\left(\epsilon^2r/c\right)},
    \end{align}
    for any $i,j \in [n]$. The last inequality is a direction application of Lemma~\ref{lem:im}. Further, Boole's inequality suggests
    \begin{align}
        \Pr\left(\text{any } i,j \in [n] : \left| \frac{ \| \vz_i - \vz_j \| - \|\ve_i - \ve_j\|}{\|\ve_i - \ve_j\|} \right| \ge \epsilon \right) \le n^2 \frac{2}{ \exp\left(\epsilon^2r/c\right)},\label{eq:hp}
    \end{align}
    where $n^2$ comes from counting all $(i,j)$ pairs. By setting $\delta > 0$ to any value smaller than $\frac{2n^2}{ \exp\left(\epsilon^2r/c\right)}$, we have $r \ge 2c\epsilon^{-2} \cdot \log(2n/\delta)$. Therefore, Eqn.~\eqref{eq:hp} can be rewritten as follows: with confidence at least $1-\delta$, we have \begin{align}
        (1-\epsilon) \| \ve_i - \ve_j \| \le \| \vz_i - \vz_j \| \le (1+\epsilon) \| \ve_i - \ve_j \|
    \end{align}
    for all $i,j \in [n]$, as long as $r\ge 2c\epsilon^{-2}  \log(2n/\delta)$.
\end{proof}

\subsection{Proof of 
Theorem~\ref{thm:optim}}\label{apx:proof:optim}

We first formally explain our assumptions.

\begin{assumption}
The loss function $\gL$ is $\beta$ element-wise Lipschitz w.r.t.~embeddings. Specifically, we have \begin{align}
     | \nabla \gL(x_i) - \nabla \gL(y_i) | \le \beta | x_i - y_i | 
\end{align}
for any $\vx, \vy \in \sR^{nd}$ being unrolled from $n \times d$ embedding matrices. $x_i$ and $y_i$ are elements in the vectors.
\end{assumption}

\begin{assumption}
The loss function $\gL$ is  $\mu$-PL (Polyak--Lojasiewic) w.r.t.~embeddings, meaning that \begin{align}
   \frac{1}{2} \| \nabla \gL(\vx) \|_2^2 \ge \mu\left( \gL(\vx) - \gL(\vx^*)\right)
\end{align}
for any $\vx \in \sR^{nd}$, where $\vx$ is embedding parameters and $\vx^*$ is any finite minimizer of $\gL$.
\end{assumption}

These are the common assumptions used to show the optimization process in deep learning~\cite{,}. In addition, we also impose an assumptions on the projection matrix and the scaling vector $\vs$.

\begin{assumption}
    The projection matrices $\mP \in \sR^{r\times d}$ has a rank of $r$. In addition, we assume $\vs$ is not a zero vector during optimization.
\end{assumption}

Based on these assumptions, we first provide the essential lemmas for our proof.

\begin{lemma}\label{lem:element-to-global}
    If $\gL: \sR^d \to \sR$ is $\beta$-Lipschitz in each element, then $\gL$ is $\beta$-Lipschitz.
\end{lemma}
\begin{proof}
    Let $\nabla \gL(x_i)$ be the partial derivative of $\mathcal L$ w.r.t.~$x_i$. We have \begin{align}
        | \nabla \gL(x_i) - \nabla \gL(y_i) | \le \beta | x_i - y_i|
    \end{align}
    for every $x_i,y_i\in\mathbb R$.
    Therefore, \begin{align}
        \| \nabla \gL(\vx) - \nabla \gL(\vy) \|^2
        =& \sum_{i=1}^d \left|\nabla \gL(x_i) - \nabla \gL(y_i)\right|^2 \\
        \le& \sum_{i=1}^d \beta^2 | x_i - y_i|^2 \\
        =& \beta^2 \|\vx - \vy\|^2.
    \end{align}
    We complete the proof by taking the square root on both sides.
\end{proof}

\begin{lemma}\label{lem:lip}
    Let $\hat\gL(\hat \vx)$ be the loss function with our ULPT approach, where $\hat\vx\in\mathbb R^{nr+2d}$ is the concatenation of shift/scale embeddings and the ultra-low-dimensional prompt embeddings.  $\hat\gL(\hat \vx)$ is $\beta'$-Lipschitz w.r.t.~$\hat\vx$ for some $\beta'>0$. 
\end{lemma}
\begin{proof} We prove the Lipschitz condition of $\mathcal L$ w.r.t.~the ultra-low-dimensional prompt embeddings, scale embedding, and shift embedding separately. Then, Lemma~\ref{lem:element-to-global} suggests the Lipschitz condition of $\mathcal L$ w.r.t to $\hat {\bm x}$. Without loss of generality, we assume the layout of parameters is $\hat {\bm x}=[\vb, \vs, \vz_1, \vz_2, \dots, \vz_n]$, where $n$ is the number of prompt tokens. 

We first calculate partial derivatives as follows \begin{align}
       & \frac{\partial \gL}{\partial \vb}
        = \sum_{i=1}^n\left(\frac{\partial \hat\ve_i}{\partial \vb}\right)^\top \frac{\partial \gL}{\partial \hat \ve_i}  = \sum_{i=1}^n \frac{\partial \gL}{\partial \hat\ve_i}, \label{eq:db}\\
        & \frac{\partial \gL}{\partial \vs} = \left(\frac{\partial \hat\ve_i}{\partial \vs}\right)^\top \frac{\partial \gL}{\partial \hat \ve_i}  = \sum_{i=1}^n \diag( \mP^\top \vz_i)  \frac{\partial \gL}{\partial \hat\ve_i},  \text{and } \label{eq:ds}\\
        & \frac{\partial \gL}{\partial \vz_i}        = \left(\frac{\partial \hat\ve_i}{\partial \vz_i}\right)^\top \frac{\partial \gL}{\partial \hat \ve_i}  = \mP \diag(\vs) \frac{\partial \gL}{\partial \hat\ve_i}. \label{eq:dqi}
    \end{align}
    % This suggests that the gradient of $\vz_i$ is the random down projection of the original gradient $\frac{\partial \gL}{\partial \hat\ve_i}$.Without loss of generality, we assume the layout of parameters is $[\vb, \vs, \vz_1, \vz_2, \dots, \vz_n] \in \sR^{2d + nr}$. 

 Our proof of the Lipschitz condition starts with checking $\vb$. For any element $b_k$, where $k=1,\cdots, d$, we have \begin{align}
        \left|\nabla \hat\gL(b_k^{(1)}) - \nabla \hat\gL(b_k^{(2)} )  \right| =&  
        % \sum_{i=1}^n \frac{\partial \gL}{\partial \hat\ve_i} \\
        \left| \sum_{i=1}^n \left( \nabla \gL(\hat e_{i,k}^{(1)}) - \nabla \gL(\hat e_{i,k}^{(2)}) \right) \right| \\
        \le&  \sum_{i=1}^n \left|  \nabla \gL(\hat e_{i,k}^{(1)}) - \nabla \gL(\hat e_{i,k}^{(2)}) \right| \\
        \le&  L \sum_{i=1}^n |\hat e_{i,k}^{(1)} - \hat e_{i,k}^{(2)}| \\
        =& nL | b_k^{(1)} - b_k^{(2)} |
    \end{align}
    where superscripts (1) and (2) indicate two values in the Lipschitz condition. $\hat e_{i,k}$ refers to the $i$th prompt token and its $k$th dimension. Here, the first equation is due to Eqn.~\eqref{eq:db}.

 For the scale embedding $\vs$, we also consider the $k$th dimension for $k=1,\cdots, d$: \begin{align}
        \left|\nabla \hat\gL(s_k^{(1)}) - \nabla \hat\gL(s_k^{(2)})\right|
        =& \left| \sum_i \left(\vz_i^\top \mP_{:,k} \nabla \gL(\hat e_{i,k}^{(1)}) - \vz_i^\top\mP_{:,k}  \nabla \gL(\hat e_{i,k}^{(2)})  \right) \right| \\
        =&  \left|\sum_i \left( \vz_i^\top \mP_{:,k} \right) \left(\nabla \gL(\hat e_{i,k}^{(1)}) - \nabla \gL(\hat e_{i,k}^{(2)})  \right)\right| \\
        \le&  \sqrt{\sum_i \left( \vz_i^\top \mP_{:,k} \right)^2} \sqrt{\sum_i \left(\nabla \gL(\hat e_{i,k}^{(1)}) - \nabla \gL(\hat e_{i,k}^{(2)}) \right)^2 } \label{eq:cs}\\
        % \le&  \sum_i \left| \left( \vz_i^\top \mP_{:,k} \right) \left(\nabla \gL^{s_k}(x) - \nabla \gL^{s_k}(y)  \right)\right| \\
        % \le&  \sum_i \left| \vz_i^\top \mP_{:,k} \right| \left|\nabla \gL^{s_k}(x) - \nabla \gL^{s_k}(y) \right| \\
        \le& \sum_i\|\vz_i\| \|\mP_{:,k}\| L \sqrt{\sum_i \left(\hat e_{i,k}^{(1)} - \hat e_{i,k}^{(2)}\right)^2} \\
        \le& L n  \sigma_{\max}(\mZ) \sigma_{\max}(\mP) \sqrt{\sum_i  (\vz_i^\top \mP_{:,k})^2 (\hat s_{k}^{(1)} - \hat s_{k}^{(2)})^2 } 
        \label{eq:norm}\\
        \le& L n  \sigma_{\max}(\mZ) \sigma_{\max}(\mP) \sqrt{\sum_i  (\vz_i^\top \mP_{:,k})^2} |\hat s_{k}^{(1)} - \hat s_{k}^{(2)}|  \\
        \le&  L n \sigma_{\max}^2(\mZ) \sigma_{\max}^2(\mP) \left| \hat s_{k}^{(1)} - \hat s_{k}^{(2)}\right|,
    \end{align}
where $\mP_{:,k}$ is the $k$th column of the $\mP$ matrix (as a column vector), and $\sigma_{\max}(\cdot)$ is the maximum singular value of a matrix. Here, Line~\eqref{eq:cs} is obtained by applying the  Cauchy--Schwartz inequality. Line~\eqref{eq:norm} is based on matrix norm inequalities. 
    
    Finally, we examine $z_{i,k}$, which is the $k$th dimension ($k=1,\cdots, r$) of the $i$th token of our ultra-low-dimensional embeddings: \begin{align}
        \left|\nabla \hat\gL(z_{i,k}^{(1)}) - \nabla \hat\gL(z_{i,k}^{(2)})\right|
        =& \left| \mP_{k,:} \diag(\vs) 
        \frac{\partial \gL}{\partial \hat \ve_i^{(1)}}
         - \mP_{k,:} \diag(\vs) \frac{\partial \gL}{\partial \hat \ve_i^{(2)}} \right| \\
        =& \left| \sum_j p_{k,j} s_j \left(\nabla \gL(\hat e_{ij}^{(1)}) -  \nabla \gL(\hat e_{ij}^{(2)})  \right)\right| \\
        \le& \left\| \mP_{k,:} \diag(\vs) \right\| \left\| \frac{\partial \gL}{\partial \hat \ve_i^{(1)}} -  \frac{\partial \gL}{\partial \hat \ve_i^{(2)}} \right\| \\
        \le& \sigma_{\max}(\mP) \sigma_{\max}(\vs) L \| \hat\ve_i^{(1)} - \hat\ve_i^{(2)} \| \\
        \le&  \sigma_{\max}(\mP) \sigma_{\max}(\vs) L \left\| \diag(\vs) \mP^\top \left(\vz_i^{(1)} - \vz_i^{(2)}\right) \right\| \\
        \le& L  \sigma_{\max}^2(\mP) \sigma_{\max}^2(\vs)  \| \vz_i^{(1)} - \vz_i^{(2)}\| \\
        =& L  \sigma_{\max}^2(\mP) \sigma_{\max}^2(\vs)  |z_{i,k}^{(1)} - z_{i,k}^{(2)}|. \label{eq:last}
    \end{align}
where Eqn.~\eqref{eq:last} holds because we examine one element $z_{i,k}$ at a time, so $z_{i,k'}^{(1)}=z_{i,k'}^{(2)}$ for $k'\ne k$.

    With these element-wise properties, we can have the full-parameter Lipschitz condition by using Lemma~\ref{lem:element-to-global}.
    % \begin{align}
    %     & \| \nabla \gL(\hat \vx) - \nabla \gL(\hat \vy) \|^2 \\
    %     =& \left\|\frac{\partial \gL}{\partial \vb^\vx} - \frac{\partial \gL}{\partial \vb^\vy}\right\|^2  + \left\|\frac{\partial \gL}{\partial \vs^\vx} - \frac{\partial \gL}{\partial \vs^\vy}\right\|^2 + \sum_{i=1}^n \left\|\frac{\partial \gL}{\partial \vz_i^\vx} - \frac{\partial \gL}{\partial \vz_i^\vy}\right\|^2  \\
    %     =& \left\| \sum_{i=1}^n \left( \frac{\partial \gL}{\partial \hat\ve_i^\vx} - \frac{\partial \gL}{\partial \hat\ve_i^\vy} \right) \right\|^2 + \left\| \sum_{i=1}^n \left( \diag(\vz_i^\vx) \mP \frac{\partial \gL}{\partial \hat\ve_i^\vx} - \diag(\vz_i^\vy) \mP  \frac{\partial \gL}{\partial \hat\ve_i^\vy} \right) \right\|^2 + \sum_{i=1}^n \left\|\mP \left(\frac{\partial \gL}{\partial \hat\ve_i^\vx}  - \frac{\partial \gL}{\partial \hat\ve_i^\vy} \right)\right\|^2  \\
    %     \le& n  \sum_{i=1}^n  \left\|\left( \frac{\partial \gL}{\partial \hat\ve_i^\vx} - \frac{\partial \gL}{\partial \hat\ve_i^\vy} \right) \right\|^2 + n \sum_{i=1}^n \left\|  \left( \diag(\vz_i^\vx) \mP \frac{\partial \gL}{\partial \hat\ve_i^\vx} - \diag(\vz_i^\vy) \mP  \frac{\partial \gL}{\partial \hat\ve_i^\vy} \right) \right\|^2 + \| \mP \|^2 \sum_{i=1}^n  \left\| \left(\frac{\partial \gL}{\partial \hat\ve_i^\vx}  - \frac{\partial \gL}{\partial \hat\ve_i^\vy} \right)\right\|^2  \\
    %     =& \left(n + \|\mP\|^2\right) \sum_{i=1}^n  \left\| \left(\frac{\partial \gL}{\partial \hat\ve_i^\vx}  - \frac{\partial \gL}{\partial \hat\ve_i^\vy} \right)\right\|^2 \\
    %     =& \left(n + \|\mP\|^2\right) \| \nabla \gL(\vx) - \nabla \gL(\vy) \|^2 \\
    %     \le& \left(n + \|\mP\|^2 \right) \beta^2 \| \vx - \vy \|^2 \\
    %     =& \left(n + \|\mP\|^2\right) \beta^2 \left( \sum_i \| \mP \vz_i^\vx - \mP \vz_i^\vy \|^2  \right) \\
    %     \le& \left(n + \|\mP\|^2\right) \beta^2 \| \mP\|^2 \left( \sum_i \| \vz_i^\vx - \vz_i^\vy \|^2  \right) \\
    %     \le& \left(n + \|\mP\|^2\right) \beta^2 \| \mP\|^2 \left( \sum_i \| \vz_i^\vx - \vz_i^\vy \|^2  + \| \vb^\vx - \vb^\vy \|^2 \right) \\
    %     =& \left(n + \|\mP\|^2\right) \beta^2 \| \mP\|^2 \| \hat \vx - \hat \vy \|^2 \\
    %     =& \left(n + \sigma_{\max}^2\right) \beta^2 \sigma_{\max}^2 \| \hat \vx - \hat \vy \|^2
    % \end{align}
    % Let $\beta':=\sqrt{\left(n + \sigma_{\max}^2\right)} \beta \sigma_{\max}$,
    % we can see $\gL$ is $\beta'$-Lipschitz w.r.t.~our parameterization by taking square root of both sides.
\end{proof}

\begin{lemma}\label{lem:pl}
    The loss function $\hat \gL$ is  $\mu'$-PL (Polyak--Lojasiewic) w.r.t.~$\hat\vx \in {\sR^{nr+d}}$ for some $\mu'$.
\end{lemma}
\begin{proof}
\begin{align}
     \frac{1}{2} \| \nabla \hat\gL(\hat \vx) \|^2
     =& \frac{1}{2}  \left\| \frac{\partial \gL}{\partial \vb} \right\|^2 + \frac{1}{2}  \left\| \frac{\partial \gL}{\partial \vs} \right\|^2  + \frac{1}{2}  \sum_{i=1}^n \left\|\frac{\partial \gL}{\partial \vz_i} \right\|^2 \\
     =& \frac{1}{2}  \left\| \sum_{i=1}^n \frac{\partial \gL}{\partial \hat \ve_i} \right\|^2 + \frac{1}{2}  \left\| \sum_{i=1}^n \diag( \mP^\top \vz_i) \frac{\partial \gL}{\partial \hat \ve_i} \right\|^2 + \frac{1}{2}  \sum_{i=1}^n \left\| \mP \diag(\vs) \frac{\partial \gL}{\partial \hat \ve_i} \right\|^2 \\
     \ge& \frac{1}{2}  \sum_{i=1}^n \left\| \mP \diag(\vs) \frac{\partial \gL}{\partial \hat \ve_i} \right\|^2 \\
    \ge& \frac{1}{2}  \sigma_{\min}^2(\mP)\sigma_{\min}^2(\vs) \sum_{i=1}^n \left\| \frac{\partial \gL}{\partial \hat \ve_i} \right\|^2 \\
    =& \frac{1}{2}  \sigma_{\min}^2(\mP)\sigma_{\min}^2(\vs) \| \nabla \gL(\hat\vx) \|^2 \\
    \ge& \sigma_{\min}^2(\mP)\sigma_{\min}^2(\vs)\mu\left( \gL(\hat\vx) - \gL(\vx^*)\right) \\
    \ge& \sigma_{\min}^2(\mP)\sigma_{\min}^2(\vs)\mu\left( \gL(\hat\vx) - \gL(\hat\vx^*)\right),
\end{align}
where $\hat \vx^*$ is the minimizer under our parameterization. This suggests that $\gL$ is $\mu'$-PL for some $\mu'$.
\end{proof}

\optim*
\begin{proof}
At each iteration $t$, gradient descent produces \begin{align}
    \vx_{t+1} \gets \vx_{t} - \eta_t \nabla  \gL(\vx_t),
\end{align}
where $\gL$ is the loss function under our parametrization. 
For each iteration, we choose $\eta_t = 1/\beta'(\vx_t)$, where $\beta'(\vx_t)$ is the Lipschitz coefficient in Lemma~\ref{lem:lip} depending on $\vx_t$: \begin{align}
    \gL(\vx_{t+1}) \le& \gL(\vx_t) +  \left(\nabla \gL(\vx_t) \right)^\top (\vx_{t+1} - \vx_t) + \frac{\beta'(\vx_t)}{2} \| \vx_{t+1} - \vx_{t} \|^2 \\
    =& \gL(\vx_t) - \frac{1}{2\beta'(\vx_t)} \|\nabla \gL(\vx_t) \|^2 \\
    \le& \gL(\vx_t) - \frac{\mu'(\vx_t)}{\beta'(\vx_t)}(\gL(\vx_t) - \gL(\vx^*)).
\end{align}
where $\mu'(\vx_t)$ is the PL coefficient in Lemma~\ref{lem:pl}, which also depends on $\vx_t$.
By rearranging the terms, we obtain\begin{align}
        \gL(\vx_{t+1}) - \gL(\vx^*) \le \left(1 - \frac{\mu'(\vx_t)}{\beta'(\vx_t)}\right) ( \gL(\vx_t) - \gL(\vx^*)),
    \end{align}
    suggesting that the excessive loss $\gL(\vx) - \gL(\vx^*)$ converges to 0.
\end{proof}
Note that our Lipschitz and PL conditions are non-uniform (i.e., depending on the parameters according to the lemmas above). Therefore, a proper learning schedule $\eta_t = 1/\beta(\vx_t)$ is needed in the theoretical analysis.