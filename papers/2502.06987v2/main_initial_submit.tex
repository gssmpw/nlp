\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage[table]{xcolor}
\usepackage{amssymb}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{balance}

\usepackage{graphicx} % For including the ORCID icon
\usepackage{hyperref} % For clickable ORCID links
\usepackage{cleveref}
% updated with editorial comments 8/9/2021

% Command to include ORCID icon
\newcommand{\orcidicon}{\includegraphics[scale=0.6]{Figures/ORCID-iD_icon_16x16.png}} % Adjust the scale as needed

% Command to link ORCID with the icon
\newcommand{\orcidlink}[1]{\href{https://orcid.org/#1}{\orcidicon}}

\begin{document}

\title{Universal Vessel Segmentation for Multi-Modality Retinal Images}
\author{Bo Wen \raisebox{1.5pt}{\orcidlink{0009-0001-5747-4739}}, Anna Heinke \raisebox{1.5pt}{\orcidlink{0000-0002-7115-8767}}, Akshay Agnihotri \raisebox{1.5pt}{\orcidlink{0009-0001-4624-8250}}, \\ Dirk-Uwe Bartsch \raisebox{1.5pt}{\orcidlink{0000-0003-0955-8708}}, William Freeman \raisebox{1.5pt}{\orcidlink{0000-0001-9979-2500}}, Truong Nguyen \raisebox{1.5pt}{\orcidlink{0000-0002-5022-063X}}, \IEEEmembership{IEEE Fellow}, Cheolhong An \raisebox{1.5pt}{\orcidlink{0000-0003-2821-7386}} 
% \thanks{Manuscript created October, 2020; This work was developed by the IEEE Publication Technology Department. This work is distributed under the \LaTeX \ Project Public License (LPPL) ( http://www.latex-project.org/ ) version 1.3. A copy of the LPPL, version 1.3, is included in the base \LaTeX \ documentation of all distributions of \LaTeX \ released 2003/12/01 or later. The opinions expressed here are entirely that of the author. No warranty is expressed or implied. User assumes all risk.}
}

\definecolor{lightblue}{rgb}{0.8,0.9,1}
\definecolor{lightgreen}{rgb}{0.8,1,0.9}
\definecolor{lightred}{rgb}{0.9,0.6,0.6}

% The paper headers
% \markboth{To replace with manuscript ID (after registration, TBD)}%
% {Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

% \IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

\crefname{table}{Table}{Tables}
\crefname{figure}{Fig.}{Figs.}

\begin{abstract}
We identify two major limitations in the existing studies on retinal vessel segmentation: (1) Most existing works are restricted to one modality, i.e, the Color Fundus (CF). However, multi-modality retinal images are used every day in the study of retina and retinal diseases, and the study of vessel segmentation on the other modalities is scarce; (2) Even though a small amount of works extended their experiments to limited new modalities such as the Multi-Color Scanning Laser Ophthalmoscopy (MC), these works still require finetuning a separate model for the new modality. The finetuning will require extra training data, which is difficult to acquire. In this work, we present a foundational universal vessel segmentation model (UVSM) for multi-modality retinal images. Not only do we perform the study on a much wider range of modalities, but we also propose a universal model to segment the vessels in all these commonly-used modalities. Despite being much more versatile comparing with existing methods, our universal model still demonstrates comparable performance with the state-of-the-art finetuned methods. To the best of our knowledge, this is the first work that achieves cross-modality retinal vessel segmentation and also the first work to study retinal vessel segmentation in some novel modalities. \footnote{Our codes and model will be available at the time of publication. We are also actively looking for approaches to release the three new vessel segmentation datasets. However, since retinal images and retinal vessels contain sensitive personal information, we can not make promise at this moment.}
\end{abstract}

\begin{IEEEkeywords}
retina, retinal images, vessel segmentation, image translation, topology-preserving image segmentation, foundation model.
\end{IEEEkeywords}

\section{Background and Introduction}
\IEEEPARstart{R}{etinal} vessel segmentation is a fundamental task in retinal image processing. It has multiple useful applications, including retinal image registration \cite{Registration-Wang-TIP, Registration-Zhang-TIP}, human biometric identification \cite{Biometric-Alex}, retinal artery-vein classification \cite{AVCls-Estrada} and tree topology analysis in digital images \cite{TreeTopology-Estrada}. Clinically, the evaluation of retinal vessels is also important since the vessel features are predictive in several systemic diseases \cite{Clinical-Ikram, Clinical-Kawasaki, Clinical-McGee} and vessel segmentation is the key towards the automation of the evaluation. However, most works in retinal vessel segmentation focus on only the Color Fundus (CF) images. CF uses white visible light to illuminate the retina and the reflected light is used for generating the image. Nevertheless, although CF was the most widely-used modality in the past, it requires dilation before the imaging and is uncomfortable for the patients. Additionally, the pathological information provided by CF is limited. Therefore, in more recent years, CF is used less frequently and emerging new imaging techniques become increasingly popular. These modalities include (\cref{fig:intro}): (1) Multi-Color Scanning Laser Ophthalmoscopy (MC), which uses more intense laser beam and allow exposing structures in deeper layers of the retina; (2) Florescence Angiography (FA), which uses florescence dye to illuminate the retinal vessels and are commonly used in the diagnosis of diseases related to neovascularization; (3) Fundus AutoFlorescence (FAF), which does not rely on external light source but uses natural fluorescence of the retina and provides insights into the health and metabolism of the retinal pigment epithelium (RPE) and photoreceptor cells; (4) Infrared/Near-Infrared Reflectance (IR/NIR), which uses Infrared or Near-Infrared lights to illuminate the image and allows seeing deeper features on the retina such as RPE abnormalities. (5) Optical Coherence Tomography Angiography (OCTA), which uses low-coherence interferometry to measure the backscattered light from retinal layers. It is primarily used to image the micro-vascular structure on the retina. Nevertheless, OCTA has a much smaller imaging area than the other modalities and has significantly different retinal vessel morphology and topology. Therefore, in this work we focus on the first four modalities and the CF.

\begin{figure*}[ht]
\centering
\includegraphics[width=1\textwidth]{Figures/Intro_small.png}
\vspace{-0.7cm}
\caption{Overview of multi-modality retinal images and the segmentation results from our UVSM. In each image, the upper half is the original image and the lower half is the segmentation. In the middle, the camera (left) and modality (right) of the image is labeled. `Heidel' refers to Heidelberg Spectralis.}
\label{fig:intro}
\vspace{-0.5cm}
\end{figure*}

One major challenge that impedes the development of machine learning-based retinal vessel segmentation algorithms is the lack of data. Due to the complexity of the retinal vessels and the existence of many fine details, labeling is very time-consuming. Existing retinal vessel segmentation datasets (especially considering high label quality) are scarce and small even for the CF modality, which usually only have 20-40 images. In addition, to the best of our knowledge, there are no publicly available vessel segmentation datasets for FAF and IR. For FA, there is only a very small public dataset with 8 images \cite{Dataset-Vampire}. In this work, we prepare three high-quality datasets for FA, FAF and IR, respectively. Each dataset has 40 images and the vessels are carefully labeled by retina experts. Nevertheless, despite the availability of the new datasets, in this work, we only use them for evaluation purpose. The reason is that: we believe that there can never be enough annotated images to allow training a powerful universal model. Instead, we need to train the model with limited existing data and seek to overcome the challenge through innovations in the algorithm. In fact, our segmentation network is trained with only 19 annotated CF images from the widely-used DRIVE \cite{Dataset_DRIVE} dataset. 

There are limited previous works which are extended beyond CF modality \cite{MMVesseg-Rodri, MMVesseg-Zhang, MMVesseg-StyleTransferPeng}. In addition to lacking comprehensive modalities (compared to our approach), one still needs to finetune a separate model on the target modality where each model only works for one image modality. We instead aim to build a single universal model that achieves robust segmentation for all the commonly-used modalities. The model will serve as an important tool in cross-modality retinal image study, which becomes increasingly important in modern research of the retina. To achieve the universal segmentation with limited labeled data, we develop our model in two major steps (\cref{fig:pipeline}). Firstly, we propose to use image translation as a preliminary domain adaptation method and train a model that translate input images from arbitrary modalities and cameras into a uniform Topcon (The Topcon TRC-50DX series is explicitly referred to as the gold standard in color fundus photography) CF image modality. We accessed abundant of de-identified multi-modality retinal images from multiple widely-used cameras (Topcon, Heidelberg Spectralis, Optos) at the Jacobs Retina Center at Shiley Eye Institute, University of California, San Diego (UCSD). Those images allow us to train a useful image translation model in a self-supervised approach. Furthermore, we adopt data augmentation to account for the slight domain difference with the other CF cameras which our center does not have, including Canon and Zeiss. Secondly, we train a segmentation model on the Topcon CF domain to perform the segmentation. However, image translation can not fully eliminate the domain gaps between different modalities given such a difficult many-to-one translation task. To solve this problem, we propose to use a topology-aware segmentation method that learns topological features of the vessels in addition to their conventional pixel-level features. The topology-aware segmentation methods were initially proposed to improve the topological accuracy of the segmentation of tubular structures within a certain type of image. We extend these methods to our task and found that they improve the topological accuracy in the segmentation (e.g., preserving vessel continuity, branching and loop features), as well as they force the segmentation to be topologically correct so when images with slight domain gap are inputted, the model can still make the correct segmentation.

Our universal vessel segmentation model is evaluated on 7 diverse datasets (including 3 new datasets we prepared for this work) of the 5 most commonly-used retinal image modalities (CF, FA, FAF, MC, IR/NIR) taken from 5 most commonly-used cameras (Topcon, Canon, Zeiss, Heidelberg Spectralis, Optos). Our universal model is compared with state-of-the-art methods which are finetuned on each of the 7 datasets, respectively, and is shown to achieve comparable performance. Extensive ablation studies were also performed to investigate how different design choices in the proposed pipeline affects the performance of the universal vessel segmentation. The main contributions of this paper are:
\begin{itemize}
    \item We propose a foundational universal vessel segmentation model (UVSM) for multi-modality retinal images which can perform robust vessel segmentation for all the commonly-used modalities and cameras using one single model without providing any modality information.
    \item We investigate image-to-image translation between retinal images from different modalities. We explore different Generative Adversarial Nets (GAN)-based and Diffusion Model-based image translation methods and evaluate their performance on retinal images.
    \item We propose to use a topology-aware image segmentation method to train our segmentation model to allow more robust segmentation across modalities. Furthermore, we discover an additional merit of the topology-aware segmentation loss functions in segmenting images with domain gaps in addition to their conventional applications.
    \item We collect and annotate three high-quality vessel segmentation datasets for the FA, FAF and IR retinal images, respectively.
    \item We provide a refined version of the widely-used DRIVE dataset with better topological accuracy in the vessel labels (to be discussed in \cref{sssec:segtrain_dataset}). 
\end{itemize}
 


\section{Related Work}

\subsection{Retinal Vessel Segmentation}
Retinal vessel segmentation classifies each pixel in the retinal images into foreground (vessel) and background (non-vessel). Current works focus on segmenting vessels in the CF images, where various machine learning-based or non-machine learning-based methods were proposed. A majority of the early methods are based on the matched filter \cite{Vesseg-MF-Chaudhuri, Vesseg-MF-Kovacs, Dataset-HRF}, which convolve retinal images with predefined or learned kernels and then adaptively threshold the image to obtain the vessel segmentation map. Another type of methods uses tracking algorithms to mathematically estimate the growth of the retinal vessel trees from seed points \cite{Vesseg-Tracking-Delibasis, Vesseg-Tracking-Lin}. The segmentation can then be derived by extending around the estimated vessel centerlines using the estimated vessel diameters. Some methods also adopt morphological image processing methods such as the top-hat operation to extract the vessel or as a post-processing step \cite{Vesseg-Morph-Graz, Vesseg-Morph-Imani}. Furthermore, numerous traditional machine learning-based methods were also applied in retinal vessel segmentation such as the support vector machine \cite{Vesseg-SVM-Tang}, random forest \cite{Vesseg-RF-Wang} and Adaboost \cite{Vesseg-Adaboost-Memari} for supervised methods, and Gaussian mixture model \cite{Vesseg-GMM-Roy}, Fuzzy C-Means \cite{Vesseg-FCM-Neto} for unsupervised methods. In more recent years, deep learning-based methods dominate the literature. They mainly rely on the Convolutional Neural Networks (CNN) as a backbone model due to the existence of vast fine details and the spread-out characteristic of the vessels. An introductory work is the Deep Retinal Image Understanding (DRIU) \cite{Vesseg-CNN-DRIU}, which adopts a VGG \cite{Others-VGG} network structure as encoder and assigns specialized output layers to the embedded features to segment the vessels and the optic disc from the retina. Based on a widely-used U-Net \cite{Others-U-Net} network structure and to seek further improvement for retinal vessel segmentation, \cite{Vesseg-CNN-CENet, Vesseg-CNN-CS2Net, Vesseg-CNN-GlobalTransformer, Vesseg-CNN-MCDAUNet, Vesseg-CNN-MCGNet} proposed to add various global reasoning modules at the bottom of the network. \cite{Vesseg-CNN-MCDAUNet, Vesseg-CNN-MCGNet, Vesseg-CNN-RCARUNet, Vesseg-CNN-WaveNet, Vesseg-CNN-GlobalTransformer} proposed to add feature aggregation modules in the residual connection at each level of the network. \cite{Vesseg-CNN-DUNet, Topo-TCLoss} proposed to use deformable convolution where the convolution kernel shapes are learned to adapt to the elongated morphology of the retinal vessels. With similar motivation, \cite{Vesseg-CNN-WSDMF} proposed an orientation-selective convolution to learn selective receptive fields of the kernel to adapt to the shape of the vessels. \cite{Vesseg-CNN-GlobalTransformer} proposed to use improved feature fusion methods to better preserve the fine details of vessels in the segmentation. In addition to a single U-Net-like segmentation network, several works \cite{Vesseg-CNN-WNet, Vesseg-CNN-WSDMF, Vesseg-CNN-FANet} proposed to use cascaded networks or a single looped network to iteratively refine the segmentation. 

\subsection{Unpaired Image-to-Image Translation} 
Image-to-image translation aims to transfer a source image to a target image domain while preserving its structural information. However, it is difficult to prepare paired retinal images from the source and target domain for training. Therefore, in this work we look into the unpaired image-to-image translation methods where the projection between the two domains can be learned with unpaired images with no structural correspondences. A representative and most-commonly used method in this task is CycleGAN \cite{Trans-CycleGAN}, which introduces a cycle consistency that projects the translated image back to the source domain to force retaining the structural information in the translated image. Following works further introduce geometric consistency \cite{Trans-CcGAN}, mutual information regularization \cite{Trans-Mutual} and contrastive unpaired translation \cite{Trans-CUT} to improve the computational efficiency over the cycle consistency-based methods during the training phrase. \cite{Trans-MUIST, Trans-StarGAN} proposed one-to-many transfer schemes which allow translating images to multiple domains. While the above works are based on GAN backbone and achieved good image translation performance, diffusion models \cite{Others-Diffusion, Others-DDPM, Others-DDIM} have recently gained more research attention in this topic for easier training and superior image quality. However, the methods \cite{Trans-DDIB, Trans-ENOT, Trans-SynDiff} are computationally intensive and were only applied on small images. For example, \cite{Trans-SynDiff} combines diffusion model with adversarial learning and formulate the training in a cycle-consistency framework, which requires training two diffusion models at the same time. Most recently, \cite{Trans-UNSB} proposed an Unpaired Neuron Schrodinger Bridge (UNSB) with a novel scheme to simulate the Schrodinger Bridge, which is a random process that interpolates between two image domains, via a diffusion process. It has higher computational efficiency and makes application to higher-resolution images (e.g., retinal images) possible.


Image style transfer \cite{Trans-NAAS, Trans-PerceptualLoss, Trans-EFDM, Trans-StyTr2, Trans-StyleDiffusion} is another method which can be seen as a special type of unpaired image-to-image translation but with a perceptual inductive bias. The majority of the methods use a perceptual feature space to compute a style loss and inject the style of a reference image to the input content image, while using another self-comparison content loss to retain the content information of the input in the transferred image. We also explore methods in this topic since style transfer is used for non-CF retinal vessel segmentation in \cite{MMVesseg-Zhang, MMVesseg-StyleTransferPeng}. 


\subsection{Topology-Aware Image Segmentation}
Topology-aware image segmentation aims to preserve the topological accuracy in the segmentation of tubular and net-like structures. The literature mainly considers a topological loss function that extracts topological features from the ground truth and segmentation and minimizes the difference between them. One type of methods extracts certain features from the segmentation which are proximity to the actual topological features of the foreground objects. A pioneer work \cite{Topo-PTLoss} uses an ImageNet-pretrained VGG \cite{Others-ImageNet, Others-VGG} network after the segmentation network. The perceptual features extracted from different layers of the pretrained VGG network is shown to be approximations of the topological features. The topological loss is then computed between the ground truth and prediction perceptual features. Another topological segmentation loss function \cite{Topo-GlandSeg} was proposed exclusively for the segmentation of glands. The loss is computed according to the iterations needed to derive the gland skeletons by the image erosion operation. \cite{Topo-clDice} proposed to approximate the topological features of tubular structures by a relationship between the foregrounds in the ground truth and prediction and their soft-skeletons. Using such a relationship as a loss function is shown to strengthen the connectivity along the tubular structure thus providing better topological accuracy. 

Another type of methods uses strict topological features of the segmentation which are based on the computational topology theory and received more research attentions in recent years. The topological features are mainly based on the persistent homology theory \cite{Topo-WTLoss, Topo-WTLoss3D, Topo-CMR,  Topo-TPAMI, Topo-TCLoss, Topo-MLLoss, Topo-BMLoss, Topo-SATLoss}. A revolutionary work that first introduce this concept in image segmenation is \cite{Topo-WTLoss}, which computes the persistent features from the ground truth and prediction, matching the features using a Wasserstein distance \cite{Topo-Wassersteinforpersistence} and using the distance as the loss function. The following works share similar idea. \cite{Topo-WTLoss3D} also uses the Wasserstein distance but apply the method on 3D point cloud data. \cite{Topo-MLLoss} proposed to match the longest persistent barcodes in the prediction to the ground truth. \cite{Topo-TCLoss} proposed to use a Hausdorff distance \cite{Topo-Hausdorff_distance} to match the persistent diagrams to improve the sensitivity of the matching to the outliers. \cite{Topo-SATLoss} and \cite{Topo-BMLoss} both aim to compute more precise persistent feature matching, as in earlier methods the actual matching results are highly inaccurate due to only considering the global topological features. \cite{Topo-SATLoss} and \cite{Topo-BMLoss} instead allow the loss to act more selectively on the topological features thus improve the efficiency of the topological loss function. \cite{Topo-BMLoss} proposed to use an Induced Matching \cite{Topo-Induced_matching} method which constructs a common ambient space between the ground truth and prediction and maps the persistent features in between to find a better matching. However, the computational cost of this method is very high, especially for large images, where typical retinal images usually have high resolutions. Alternatively, \cite{Topo-SATLoss} proposed to leverage the spatial locations of the persistent features referencing their critical cell locations for more faithful persistent feature matching and is computationally much more efficient than \cite{Topo-BMLoss}.

\begin{figure*}[ht]
\centering
\includegraphics[width=1\textwidth]{Figures/Pipeline.png}
\vspace{-0.7cm}
\caption{Pipeline for our proposed universal vessel segmentation model, which includes the 2-stage training and the inference.}
\label{fig:pipeline}
\vspace{-0.6cm}
\end{figure*}


\section{Universal Retinal Vessel Segmentation Model}
\label{sec:overview}

\subsection{Overview}
We discuss the overall training and inference pipeline (\cref{fig:pipeline}) in this sub-section and provide more details about the major components in the pipeline in the following sub-sections. Given multi-modality retinal images, we first adopt a simple yet widely-used pre-processing method that takes only the green channel from the RGB image. All of the training and inference are then performed on the green-channel images. 

Following the pre-processing, we train an domain adaptation model for multi-modality retinal images. Although several domain adaptation methods were proposed for retinal vessel segmentation, such as using the pseudo-label methods \cite{Vesseg-CNN-WNet}, adversarial learning \cite{Vesseg-CNN-MCGNet} and asymmetrical maximum classifier discrepancy \cite{UA-Vesseg-AMCD}, they were proposed only to address the domain gap between different datasets (from different cameras) in CF images. Moreover, these methods still require unsupervised finetuning on the target domain images, which deviate from our goal of universal segmentation. Therefore, in this work we propose to use image translation as the domain adaptation method, which allows us to train a single model for all the modalities.

The image translation model aims to translate images from all the modalities to a uniform Topcon CF modality, and is trained between equivalent number of non-CF images (FA, FAF, MC, IR/NIR) and the Topcon CF images, i.e., during the training, we treat all the non-CF modalities as one type of image. More information of the training dataset is provided in \cref{sssec:trans_dataset}. Furthermore, the translation network is trained with an identity loss (will be discussed in detail in \cref{subsec:image_translation}) which allows us to use a single translation network to transfer all the images to the Topcon CF domain without the need to specify whether they are CF or non-CF. In addition, the translation model also translates CF images from other cameras to Topcon.

After the image translation model is trained, we use it to translate our segmentation training set (19 images from the DRIVE \cite{Dataset_DRIVE} training set with vessel labels) to the Topcon CF domain. The translated images are combined with the original images as an augmented training set to train the downstream segmentation network. Nevertheless, since we are training a difficult many-to-one (FA, FAF, MC, IR/NIR to CF) translation, the translation model cannot perfectly address the domain gap between the non-CF images and the CF images (\cref{fig:ab_transfer_examples}). We therefore propose to further solve this problem by introducing a topology-aware segmentation method. In addition to a conventional pixel-wise loss function (BCELoss), the segmentation network is jointly trained with two topological loss functions.

At inference, we can disregard the modality of the image. Given a retinal image from an arbitrary modality, we simply need to (1) extract the green channel; (2) apply the translation network to the green-channel image; (3) apply the topology-aware segmentation network to the translated image to obtain the final segmentation. 




\subsection{Image Translation Network}
\label{subsec:image_translation}

Multiple GAN-based or Diffusion model-based image translation methods are explored for our task. For Diffusion-based method, we experiment with StyleDiffusion \cite{Trans-StyleDiffusion} and Unpaired Neuron Schrodinger Bridge (UNSB) \cite{Trans-UNSB}. For GAN-based method, we experiment with CycleGAN \cite{Trans-CycleGAN}. We will show in \cref{sssec:ab_image_translation} that CycleGAN, despite being a classic method, outperforms the other two most recent methods in most datasets. Therefore, we pick CycleGAN as the backbone of our image translation model. Consequently, the following sections discuss the implementation based on CycleGAN and we refer the readers to \cite{Trans-StyleDiffusion, Trans-UNSB} for details about the other two methods and to \cref{sssec:ab_image_translation} for our implementation details.

\begin{figure}[]
\centering
\includegraphics[width=0.45\textwidth]{Figures/CycleGAN.png}
\vspace{-0.3cm}
\caption{Overview of our image translation network, which is based on CycleGAN \cite{Trans-CycleGAN}. $G$ refers to the GAN generators and $D$ for the discriminators. The gray dotted lines illustrate how loss functions are computed between different images.}
\label{fig:CycleGAN}
\vspace{-0.6cm}
\end{figure}

To train the translation network, we treat all the non-CF images (FA, FAF, MC, IR/NIR) as image type A and the Topcon CF images as image type B. At each iteration, a pair of image A batch and image B batch is drawn from the training set. As illustrated by \cref{fig:CycleGAN}, generator A is trained to translate all the type A images (non-CF) to a fake type B (CF) and generator B is used to translate type B to a fake type A, which is a neutral non-CF domain. Both generator A and B are learned with adversarial learning \cite{Others-GAN, Others-LSGAN}, where the generator and discriminator are alternatively updated. At each iteration, given the image batch $I_{A}$, the generators (A+B) are first updated with the objectives (discriminators frozen):
\begin{equation}
    \textit{L}_{G_{A}} = \textit{L}_{adv.-G_{A}} + \lambda_{c}\textit{L}_{cycle-A} + \lambda_{i}\textit{L}_{identity-A}
\end{equation}
where the adversarial loss defined as:
\begin{equation}
    \textit{L}_{adv.-G_{A}} =  \left | D_{A}(G_{A}(I_{A})) - 1 \right |^{2}
\end{equation}
Furthermore, the translated fake CF images are again fed into the opposite generator to reconstruct the non-CF images. The reconstruction is supervised by a cycle consistency loss:
\begin{equation}
    \textit{L}_{cycle-A} =  \left | I_{A} - G_{B}(G_{A}(I_{A})) \right |
\end{equation}
and is controlled by a weight $\lambda_{c}$. Finally, the real images are also passed through their opposite generator to generate the image itself. This step is supervised with an identity loss:
\begin{equation}
    \textit{L}_{identity-A} =  \left | I_{A} - G_{B}(I_{A}) \right |
\end{equation}
and is controlled by a weight $\lambda_{i}$. The identity loss was initially proposed to correct the tint in the translated image \cite{Trans-CycleGAN}. However, we find it especially useful in our task since it allows us to use the generator $G_{A}$ to map the CF images to itself. As a result, at inference we only need to apply one generator $G_{A}$ to all the input images regardless of their modalities. If the image is non-CF, the translation model will convert it to CF. If the image is already CF, the output image will remain a CF image. It allows our universal model to work without the need to provide any modality information about the input.



After the generator is updated, the discriminator is updated with the adversarial loss (generators frozen):
\begin{equation}
    \textit{L}_{adv.-D_{A}} =  \frac{1}{2}\left | D_{A}(I_{B}) - 1 \right |^{2} + \frac{1}{2}\left | D_{A}(G_{A}(I_{A})) \right |^{2}
\end{equation}

For the opposite image batch $I_{B}$, the generators (B+A) and discriminator (B) are updated in the same way, as illustrated in \cref{fig:CycleGAN}. In addition, we first update the generators for both $I_{A}$ and $I_{B}$, and then update the discriminators.





% \subsubsection{Data Augmentation}
% \label{sssec:translation_augmentation}
% Data augmentation is applied to improve the robustness of the translation model to new images, especially to images from new cameras which our training set does not include. For each batch of images, with a probability of 0.5 we apply the following strategies in a random order: (1) scaling with a random factor between $[0.8, 1.2]$; (2) sharpness adjustment with a random factor between $[0.3, 3.0]$; (3) contrast adjustment with a random factor between $[0.3, 3.0]$; (4) gamma adjustment with a random factor between $[0.5, 2.0]$. Additionally, since different camera manufacturers apply different shapes of mask to the image, we preprocessed the training set that we randomly apply the masks shown in \cref{fig:masks} to the original images to prevent the model from generating artifacts out of the effective imaging area.



% \subsubsection{Network Structures}
% The balance between the size of the generator and discriminator is important to stably train the CycleGAN for our task. For the generator, we use a 5-level U-Net \cite{Others-U-Net} structure where the levels have a depth of $[64, 128, 256, 512, 512]$ from the outermost to the innermost and in each deeper level the image is downsampled by 2. The generator has in total 16.66M trainable parameters. The discriminator is a 4-layer MLP with depth $[64, 128, 256, 512]$ and use a final convolution layer to convert the depth from 512 to 1 for output. The discriminator has in total 2.77M trainable parameters.\footnote{More details on the network structures we use for translation and segmentation will be provided in our codes.}

% \subsubsection{Training Settings}
% We use a batch size of 4 and train the model for 100 epochs on the 4000 images. We start with an initial learning rate of $2e-4$ and start decaying the learning rate linearly to 0 from epoch 50 to 100. Adam optimizer is used to train both the generators and the discriminators, with a momentum of $(0.5, 0.999)$ and no weight decay. The cycle consistency loss has a weight $\lambda_{c}$ of 10.0 and the identity loss weight $\lambda_{i}$ we use is 0.5. We also use a image buffer of size 50 in the CycleGAN training, such that, when updating the discriminator, by a 50\% chance the fake generated image is substituted with a random fake image in the pool and the current fake image is inserted to the pool.


\subsection{Topology-aware Segmentation Network}
\label{subsec:topological_segmentation}

The downstream segmentation network is trained with a Perceptual Topological Loss $\mathcal{L}_{tc}$ and a Persistent Topological Loss $\mathcal{L}_{ts}$, introduced below accordingly.

\subsubsection{Perceptual Topological Loss}
\label{sssec:perceptual_topological_loss}
We first propose to use a perceptual topological loss \cite{Topo-PTLoss} where the topological information is approximated by the perceptual features encoded by an ImageNet \cite{Others-ImageNet} pretrained VGG-16 \cite{Others-VGG} network. As illustrated by \cref{fig:topo_perceptual}, image embeddings $\mathcal{E}_{l}(\cdot)$ are computed from the predicted vessel probability map $I_{pred}$ and the ground truth $I_{GT}$, where $l$ denotes different layers in the VGG-16 network. In practice, we take the output of the $relu2\_2$, $relu3\_3$ and $relu4\_3$ layers. Then, the perceptual topological loss function $\mathcal{L}_{tc}$ is defined as:
\begin{equation}
    \mathcal{L}_{tc} = \sum_{l}^{N_{l}}\left | \mathcal{E}_{l}(I_{pred}) - \mathcal{E}_{l}(I_{GT}) \right |^{2}
\end{equation}
\vspace{-0.3cm}
\begin{figure}[ht]
\centering
\includegraphics[width=0.4\textwidth]{Figures/Perceptual_topo.png}
\vspace{-0.3cm}
\caption{Illustration for the perceptual topological loss function.}
\label{fig:topo_perceptual}
\vspace{-0.1cm}
\end{figure}


\subsubsection{Persistent Topological Loss}
\label{ssec:persistent_topological_loss}

% \begin{figure*}[ht]
% \centering
% \includegraphics[width=1.0\textwidth]{Figures/SegNet_training.png}
% \caption{Left: Design and training of the segmentation network. Right: Illustration for the perceptual topological loss function.}
% \label{fig:segnet}
% \end{figure*}


In 2D digital images, the topology is described by their 0- and 1-homology features, i.e., the connected components and the loops. The persistent homology is a method to describe a process where homology features are created and destroyed in a sequence (called `filtration') of cubical complex \cite{Topo-cc} (which encodes an image and the higher-dimensional relationships between its pixels to structured cells) and its sub-complexes determined by the values of cells. Although it deviates from the definition and how persistent homology is computed, a simpler way to interpret persistent homology in 2D digital images is by thresholding the image from its highest pixel value to its lowest pixel value and stop at every pixel value that appears in the image. Each thresholded image is a binary image where we can determine the connected components and the loops inside. As demonstrated by the example in \cref{fig:PH}, when the threshold value decreases, new pixels are added to the binary image, leading to creation of new homology features or destruction of old features. A homology feature that `lives' through the decreasing thresholds is then called a `persistent feature'. We refer the readers to \cite{Topo-CompTopoBook} for more details and strict definitions. 



\begin{figure}[]
\centering
\includegraphics[width=0.47\textwidth]{Figures/persistent_homology.png}
\vspace{-0.3cm}
\caption{A toy example visualizing persistent homology in images: (a) a gray-scale image; (b) the filtration visualized by thresholded binary images. Under each image, the digits represent the filtration value (threshold value), the number of 0-homology features and the number of 1-homology features; (c) the 0-persistent diagram; (d) the 1-persistent diagram.}
\label{fig:PH}
\vspace{-0.6cm}
\end{figure}

The persistent homology of an image can be described by a persistent diagram, where each point on the diagram represent one persistent feature, whose coordinate is (1$-$Creation Value, 1$-$Destruction Value). 

We propose to use the Spatial-Aware Topological Loss Function (SATLoss) \cite{Topo-SATLoss} as the persistent topological loss function. After computing persistent diagrams $\mathcal{D}(I_{pred})$ and $\mathcal{D}(I_{GT})$, the persistent features are matched using a spatially-weighted Wasserstein distance \cite{Topo-Wassersteinforpersistence}:

\begin{align}
     & W_{q}(\mathcal{D}(I_{pred}),\mathcal{D}(I_{GT}))= \\
     & [\inf_{\eta :\mathcal{D}(I_{pred})\rightarrow \mathcal{D}(I_{GT})}\sum_{p\in \mathcal{D}(I_{pred})}^{} \notag s_{p} \left \|p - \eta(p))  \right \|^{q}]^{\frac{1}{q}}
\label{eq:matching}
\end{align}
where $s_{p}=\left \|c_{b}(p)-c_{b}(\eta(p))  \right \|^{q}$ is a spatial weight that is computed between the creation cells ($c_{b}(\cdot)$) of the persistent features. $p$ and $\eta(p)$ are a persistent feature from $I_{pred}$ and a candidate persistent feature from $I_{GT}$ to be matched with. $q$ is the norm and in practice we take $q=2$. After finding the optimal matching by solving the optimal transport problem, the persistent topological loss function is computed as (reformulating $p$ and $\eta(p)$ by their creation and destruction values):
\begin{equation}
  \mathcal{L}_{ts} = \sum_{p \in \mathcal{D}(I_{pred})}^{}s^{*}_{p}([b(p) - b(\eta^{*}(p))]^{2}+[d(p) - d(\eta^{*}(p))]^{2})
\label{eq:satloss}
\end{equation}
where $\eta^{*}(p)$ is the optimal matching of $p$ in $\mathcal{D}(I_{GT})$ and $s^{*}_{p}$ is the corresponding spatial weight. $b(\cdot)$ and $d(\cdot)$ are the creation and destruction values of the topological features. The values are taken from $I_{pred}, I_{GT}$ instead of $\mathcal{D}(I_{pred}), \mathcal{D}(I_{GT})$ to make the loss differentiable \cite{Topo-SATLoss}. 

Furthermore, for $p \in \mathcal{D}(I_{pred})$ which are not matched with a point in $\mathcal{D}(I_{GT})$, we match it to the closest point on the diagonal of the persistent diagram, i.e., $b(\eta^{*}(p))$ and $d(\eta^{*}(p))$ equals to 1, 0 if the matched $\eta^{*}(p)$ is a point in $\mathcal{D}(I_{GT})$, otherwise:
\begin{equation}
    b(\eta^{*}(p)) = d(\eta^{*}(p)) = \frac{1}{2}(b(p)+d(p))
\end{equation}
The afore-hand matching process is based on all the candidate matchings, including matching with points in $\mathcal{D}(I_{GT})$ and matching with the diagonal.



\begin{figure}[ht]
\centering
\includegraphics[width=0.48\textwidth]{Figures/SegNet_2.png}
\vspace{-0.3cm}
\caption{Overview of the segmentation network.}
\label{fig:segnet}
\vspace{-0.4cm}
\end{figure}

\subsubsection{Loss Formulation}
\label{sssec:loss_formulation}
Finally, the persistent topological loss and the perceptual topological loss are used with a conventional pixel-wise BCELoss:

\begin{equation}
\mathcal{L}_{bce} = \frac{1}{n}\sum_{i=1}^{n}y_{i}log(x_{i})+(1-y_{i})log(1-x_{i})
\end{equation}
where $x_{i}$ and $y_{i}$ are the per-pixel predicted likelihood value and the ground truth value. $n$ is the total number of pixels in the image. The topological loss functions are controlled by weights $\lambda_{tc}$, $\lambda_{ts}$:

\begin{equation}
\mathcal{L}_{total} = \mathcal{L}_{bce} + \lambda_{tc}\mathcal{L}_{tc} + \lambda_{ts}\mathcal{L}_{ts}
\end{equation}
to jointly optimize the segmentation network.

\begin{figure}[]
\centering
\includegraphics[width=0.48\textwidth]{Figures/masks.png}
\caption{Different types of masks randomly applied to the retinal images for training both the translation and segmentation networks.}
\label{fig:masks}
\end{figure}

\subsubsection{Segmentation Network}
\label{sssec:segnet}
We use a basic U-Net \cite{Others-U-Net} structure with reduced model size for the segmentation network. In addition, as shown in \cref{fig:segnet}, we adopt a `predict-refine' strategy which first make a coarse prediction of the vessels and feed the initial prediction and the input image to the network again to refine it. This method has been shown useful in both topology-aware image segmentation \cite{Topo-PTLoss} and multiple works in deep-learning-based retinal vessel segmentation \cite{Vesseg-CNN-WSDMF, Vesseg-CNN-WNet, Vesseg-CNN-FANet}. In our method, the prediction and refinement networks share the same weight. Additionally, at both the initial prediction stage and the refinement stage, the loss functions are computed to optimize the network. 

% \subsection{Implementation Details}
% \label{ssec:seg_details}

% \subsubsection{Data Augmentation}
% In addition to the data augmentation strategies we used to train the image translation network, we also use the following methods to train the segmentation network: (1) By a 50\% chance we randomly compute the negative film of the image followed by a random contrast or sharpness adjustment. This is essential to allow the universal segmentation model to work for the FA modality as the vessels appears to be brighter than the background and we found no image translation method capable of stably inverting the color of the vessels; (2) To better learn the morphological feature of the vessels, image rotation is used with a random angle in 0, 90, 180, 270 degrees; (3) image shift with a random shift magnitude between 0 and 0.1 times the width and height of the image; (4) random Gaussian noise.

% \subsubsection{Training Settings}
% We start with an learning rate of 1e-3 and train for 150 epochs, and decay the weight to 1e-4 to train for another 250 epochs. Adam optimizer is used with a weight decay of 1e-3 and momentum of $(0.9, 0.999)$. We train with a batch size of 4. The weight $\lambda_{tc}$ and $\lambda_{ts}$ we use are 0.05 and 0.0002, respectively. The implementation of both image translation and the segmentation parts are under the PyTorch framework.

% \subsubsection{Inference and Evaluation}
% \label{sssec:inference_resize}
% At inference we first pad all the images to square if they are rectangular. Then, the images are resized to 768$\times$768 to feed into the image translation network and the vessel segmentation network. The predicted likelihoods are then interpolated back to the original size and binarized to compare with the ground truth and compute the evaluation metrics.

% \subsubsection{Computation of Persistent Homology}
% In a higher level, the computation of persistent homology mainly consists of three steps: (1) Constructing a cubical complex from the image; (2) Sorting the cubical complex by the filtration values; (3) Iterative over the sorted cubical complex and compute the creation and death of the persistent features via a Union and Find algorithm. Readers can refer to \cite{Topo-Persistence_compute, Topo-Persistence_compute2} for more detailed information. In our implementation, we ultilize the GUDHI (Graphical Understanding of High Dimensional Data) library to compute the persistent features. GUDHI is a C++ library and provide a python interface. The persistent features are adapted back to Python to compute the topological loss function.



\section{Experiments}
\label{sec:experiments}

\begin{table*}[ht]
  \caption{Comparison between our UVSM with SOTA finetuned methods. A green box indicates the best result and a blue box indicates the second-best result. Heidel refers to Heidelberg Spectralis.}
  \vspace{-0.1cm}
  \footnotesize
  \label{tab:main_tab}
  \centering
  
  
  \begin{tabular}{@{\hspace{2pt}}c@{\hspace{2pt}}ccc|c|cccccccccc@{}}
    \toprule
     Dataset & Modality & Camera & \#img & Method & Acc. & Dice & Sp & Se & Pr & F1 & MCC & clDice & \(\beta_{0}\downarrow\) & \(\beta_{1}\downarrow\) \\

        \midrule
    
    \multirow{7}{*}{\(STARE\)} & \multirow{7}{*}{\(CF\)} & \multirow{7}{*}{Topcon} & \multirow{7}{*}{20}
          & W-Net & 97.61 & 81.52 & 98.80 & 81.59 & 82.36 & 81.52 & 80.51 & 85.56 & 75.6 & 13.1 \\
    & & & & WS-DMF & 96.01 & 71.77 & 97.36 & 77.28 & 68.55 & 71.77 & 70.30 & 78.94 & 193.3 & 17.3 \\
    & & & & GT-DLA-dsHff & 95.59 & 66.17 & 97.70 & 66.15 & 66.71 & 66.17 & 63.96 & 66.44 & 84.0 & 12.2 \\
    & & & & U-Net & 97.25 & 78.32 & 98.67 & 77.27 & 80.48 & 78.32 & 77.17 & 81.59 & 132.7 & 14.3 \\
    & & & & CS$^{2}$-Net & \cellcolor{lightblue}{\textcolor{black}{97.80}} & \cellcolor{lightblue}{\textcolor{black}{82.86}} & \cellcolor{lightgreen}{\textcolor{black}{98.91}} & \cellcolor{lightblue}{\textcolor{black}{82.85}} & \cellcolor{lightgreen}{\textcolor{black}{83.85}} & \cellcolor{lightblue}{\textcolor{black}{82.86}} & \cellcolor{lightblue}{\textcolor{black}{81.97}} & \cellcolor{lightblue}{\textcolor{black}{86.41}} & 84.9 & 12.8 \\
    & & & & FA-Net & \cellcolor{lightgreen}{\textcolor{black}{97.85}}  & \cellcolor{lightgreen}{\textcolor{black}{83.57}} & \cellcolor{lightblue}{\textcolor{black}{98.84}} & \cellcolor{lightgreen}{\textcolor{black}{84.51}} & \cellcolor{lightblue}{\textcolor{black}{83.17}} & \cellcolor{lightgreen}{\textcolor{black}{83.57}} & \cellcolor{lightgreen}{\textcolor{black}{82.58}} & \cellcolor{lightgreen}{\textcolor{black}{87.63}} & \cellcolor{lightblue}{\textcolor{black}{69.1}} & \cellcolor{lightgreen}{\textcolor{black}{10.1}} \\
    & & & & \textbf{Our UVSM} & 97.48 & 80.51 & 98.69 & 80.77 & 80.78 & 80.52 & 79.31 & 85.21 & \cellcolor{lightgreen}{\textcolor{black}{35.1}} & \cellcolor{lightblue}{\textcolor{black}{12.6}} \\

        \midrule
    
    \multirow{7}{*}{\(ChaseDB1\)} & \multirow{7}{*}{\(CF\)} & \multirow{7}{*}{Zeiss} & \multirow{7}{*}{28}
          & W-Net & 97.08 & 78.87 & \cellcolor{lightblue}{\textcolor{black}{98.75}} & 76.35 & \cellcolor{lightgreen}{\textcolor{black}{82.32}} & 78.87 & 77.58 & 78.46 & \cellcolor{lightgreen}{\textcolor{black}{54.09}} & 9.0 \\
    & & & & WS-DMF & 95.54 & 59.62 & \cellcolor{lightgreen}{\textcolor{black}{98.81}} & 50.31 & 78.94 & 59.62 & 59.92 & 58.15 & 123.6 & 24.1 \\
    & & & & GT-DLA-dsHff & 96.40 & 74.26 & 98.24 & 72.87 & 76.11 & 74.26 & 72.46 & 73.06 & 81.1 & 7.6 \\
    & & & & U-Net & 96.81 & 76.84 & 98.60 & 74.23 & 80.36 & 76.84 & 75.39 & 76.01 & 83.7 & 8.2 \\
    & & & & CS$^{2}$-Net & \cellcolor{lightgreen}{\textcolor{black}{97.19}} & \cellcolor{lightblue}{\textcolor{black}{80.05}} & 98.65 & \cellcolor{lightblue}{\textcolor{black}{78.89}} & \cellcolor{lightblue}{\textcolor{black}{81.65}} & \cellcolor{lightblue}{\textcolor{black}{80.05}} & \cellcolor{lightblue}{\textcolor{black}{78.67}} & \cellcolor{lightblue}{\textcolor{black}{80.77}} & 84.9 & 8.6 \\
    & & & & FA-Net & \cellcolor{lightblue}{\textcolor{black}{97.18}} & \cellcolor{lightgreen}{\textcolor{black}{80.21}} & 98.52 & \cellcolor{lightgreen}{\textcolor{black}{80.39}} & 80.72 & \cellcolor{lightgreen}{\textcolor{black}{80.21}} & \cellcolor{lightgreen}{\textcolor{black}{78.89}} & \cellcolor{lightgreen}{\textcolor{black}{81.29}} & \cellcolor{lightblue}{\textcolor{black}{68.4}} & \cellcolor{lightgreen}{\textcolor{black}{7.1}} \\
    & & & & \textbf{Our UVSM} & 96.55 & 77.34 & 98.05 & 77.81 & 75.38 & 76.34 & 74.63 & 78.34 & 70.1 & \cellcolor{lightblue}{\textcolor{black}{7.5}} \\

        \midrule
    
    \multirow{7}{*}{\(HRF\)} & \multirow{7}{*}{\(CF\)} & \multirow{7}{*}{Canon} & \multirow{7}{*}{35}
          & W-Net & 96.60 & 64.39 & \cellcolor{lightblue}{\textcolor{black}{98.48}} & 61.70 & \cellcolor{lightblue}{\textcolor{black}{68.04}} & 64.39 & 62.88 & 60.09 & 260.4 & 59.8 \\
    & & & & WS-DMF & 94.79 & 58.02 & 96.02 & \cellcolor{lightblue}{\textcolor{black}{71.78}} & 49.75 & 58.03 & 56.84 & 56.54 & 945.1 & \cellcolor{lightblue}{\textcolor{black}{51.7}} \\
    & & & & GT-DLA-dsHff & 95.49 & 53.17 & 97.86 & 51.16 & 55.84 & 53.17 & 50.99 & 45.92 & 421.0 & 60.0 \\
    & & & & U-Net & 96.58 & 64.22 & 98.42 & 61.82 & 67.40 & 64.22 & 62.64 & 59.63 & 297.7 & 53.1 \\
    & & & & CS$^{2}$-Net & \cellcolor{lightgreen}{\textcolor{black}{96.68}} & 65.17 & \cellcolor{lightgreen}{\textcolor{black}{98.51}} & 62.37 & \cellcolor{lightgreen}{\textcolor{black}{68.82}} & 65.18 & 63.67 & 60.82 & 334.7 & \cellcolor{lightgreen}{\textcolor{black}{51.1}} \\
    & & & & FA-Net & \cellcolor{lightblue}{\textcolor{black}{96.67}} & \cellcolor{lightblue}{\textcolor{black}{65.80}} & 98.40 & 64.36 & 67.87 & \cellcolor{lightblue}{\textcolor{black}{65.80}} & \cellcolor{lightblue}{\textcolor{black}{64.23}} & \cellcolor{lightblue}{\textcolor{black}{61.97}} & \cellcolor{lightblue}{\textcolor{black}{257.7}} & 47.5 \\
    & & & & \textbf{Our UVSM} & 96.60 & \cellcolor{lightgreen}{\textcolor{black}{68.50}} & 97.78 & \cellcolor{lightgreen}{\textcolor{black}{74.70}} & 63.87 & \cellcolor{lightgreen}{\textcolor{black}{68.50}} & \cellcolor{lightgreen}{\textcolor{black}{67.15}} & \cellcolor{lightgreen}{\textcolor{black}{68.26}} & \cellcolor{lightgreen}{\textcolor{black}{142.8}} & 55.5 \\

        \midrule
    
    \multirow{7}{*}{\(IOSTAR\)} & \multirow{7}{*}{\(MC\)} & \multirow{7}{*}{Heidel} & \multirow{7}{*}{30}
          & W-Net & \cellcolor{lightblue}{\textcolor{black}{97.09}} & \cellcolor{lightblue}{\textcolor{black}{80.66}} & \cellcolor{lightgreen}{\textcolor{black}{98.70}} & 78.54 & \cellcolor{lightgreen}{\textcolor{black}{83.50}} & \cellcolor{lightblue}{\textcolor{black}{80.66}} & \cellcolor{lightblue}{\textcolor{black}{79.30}} & \cellcolor{lightblue}{\textcolor{black}{87.46}} & \cellcolor{lightblue}{\textcolor{black}{43.2}} & 9.4 \\
    & & & & WS-DMF & 95.89 & 71.49 & 98.29 & 67.53 & 77.54 & 71.49 & 69.88 & 76.92 & 140.9 & 22.0 \\
    & & & & GT-DLA-dsHff & 96.91 & 79.63 & 98.52 & 78.27 & 81.67 & 79.63 & 78.16 & 86.99 & 43.4 & \cellcolor{lightblue}{\textcolor{black}{7.8}} \\
    & & & & U-Net & 97.01 & 80.50 & 98.52 & \cellcolor{lightblue}{\textcolor{black}{79.72}} & 81.80 & 80.50 & 79.04 & 87.08 & 57.4 & 8.7 \\
    & & & & CS$^{2}$-Net & 97.05 & 80.59 & \cellcolor{lightblue}{\textcolor{black}{98.59}} & 79.27 & \cellcolor{lightblue}{\textcolor{black}{82.49}} & 80.59 & 79.17 & 87.06 & 67.2 & 10.3 \\
    & & & & FA-Net & \cellcolor{lightgreen}{\textcolor{black}{97.13}} & \cellcolor{lightgreen}{\textcolor{black}{81.38}} & 98.52 & \cellcolor{lightgreen}{\textcolor{black}{81.30}} & 82.23 & \cellcolor{lightgreen}{\textcolor{black}{81.38}} & \cellcolor{lightgreen}{\textcolor{black}{80.06}} & \cellcolor{lightgreen}{\textcolor{black}{88.08}} & \cellcolor{lightgreen}{\textcolor{black}{38.3}} & 10.9 \\
    & & & & \textbf{Our UVSM} & 96.70 & 78.38 & 98.36 & 77.53 & 79.72 & 78.38 & 76.73 & 85.08 & 69.1 & \cellcolor{lightgreen}{\textcolor{black}{6.3}} \\

        \midrule
    
    \multirow{7}{*}{\(JRCFA\)} & \multirow{7}{*}{\(FA\)} & \multirow{7}{*}{\shortstack{Heidel \\ Optos}} & \multirow{7}{*}{40}
          & W-Net & 97.83 & 83.49 & \cellcolor{lightblue}{\textcolor{black}{99.03}} & 81.73 & \cellcolor{lightgreen}{\textcolor{black}{86.11}} & 83.49 & 82.57 & 84.68 & 162.0 & 13.7 \\
    & & & & WS-DMF & 97.20 & 78.20 & 98.69 & 76.90 & 81.84 & 78.20 & 77.35 & 78.60 & 209.2 & 14.8 \\
    & & & & GT-DLA-dsHff & 96.38 & 73.14 & 98.17 & 72.25 & 74.44 & 73.14 & 71.31 & 72.16 & 279.1 & 13.2 \\
    & & & & U-Net & 97.89 & 83.95 & 99.14 & 81.65 & 87.02 & 83.95 & 83.04 & 84.86 & 160.5 & 13.9 \\
    & & & & CS$^{2}$-Net & \cellcolor{lightgreen}{\textcolor{black}{97.98}} & \cellcolor{lightgreen}{\textcolor{black}{84.88}} & 99.00 & \cellcolor{lightgreen}{\textcolor{black}{84.43}} & 85.79 & \cellcolor{lightgreen}{\textcolor{black}{84.88}} & \cellcolor{lightgreen}{\textcolor{black}{83.93}} & \cellcolor{lightgreen}{\textcolor{black}{86.07}} & 157.6 & \cellcolor{lightblue}{\textcolor{black}{11.2}} \\
    & & & & FA-Net & \cellcolor{lightblue}{\textcolor{black}{97.94}} & \cellcolor{lightblue}{\textcolor{black}{84.47}} & \cellcolor{lightgreen}{\textcolor{black}{99.03}} & \cellcolor{lightblue}{\textcolor{black}{83.51}} & \cellcolor{lightblue}{\textcolor{black}{85.94}} & \cellcolor{lightblue}{\textcolor{black}{84.47}} & \cellcolor{lightblue}{\textcolor{black}{83.51}} & \cellcolor{lightblue}{\textcolor{black}{85.72}} & \cellcolor{lightblue}{\textcolor{black}{126.3}} & \cellcolor{lightgreen}{\textcolor{black}{10.1}} \\
    & & & & \textbf{Our UVSM} & 96.47 & 75.00 & 97.91 & 78.40 & 72.89 & 74.89 & 73.43 & 76.37 & \cellcolor{lightgreen}{\textcolor{black}{37.7}} & 15.2 \\

        \midrule
    
    \multirow{7}{*}{\(JRCFAF\)} & \multirow{7}{*}{\(FAF\)} & \multirow{7}{*}{\shortstack{Heidel \\ Optos}} & \multirow{7}{*}{40}
          & W-Net & 97.93 & 77.78 & \cellcolor{lightgreen}{\textcolor{black}{99.21}} & 74.01 & \cellcolor{lightgreen}{\textcolor{black}{82.59}} & 77.77 & 76.98 & 81.36 & 48.0 & 8.1  \\
    & & & & WS-DMF & 97.48 & 72.19 & 99.05 & 67.88 & 78.94 & 72.19 & 71.53 & 75.63 & 109.1 & 10.6 \\
    & & & & GT-DLA-dsHff & 97.24 & 70.71 & 98.80 & 67.71 & 74.57 & 70.71 & 69.50 & 74.61 & 62.8 & 7.5 \\
    & & & & U-Net & 97.89 & 77.66 & 99.13 & 74.94 & 81.29 & 77.66 & 76.81 & 81.03 & 68.6 & 7.2 \\
    & & & & CS$^{2}$-Net & \cellcolor{lightblue}{\textcolor{black}{97.93}} & \cellcolor{lightblue}{\textcolor{black}{78.11}} & \cellcolor{lightblue}{\textcolor{black}{99.14}} & 75.34 & \cellcolor{lightblue}{\textcolor{black}{81.64}} & \cellcolor{lightblue}{\textcolor{black}{78.12}} & \cellcolor{lightblue}{\textcolor{black}{77.24}} & 81.46 & 63.4 & \cellcolor{lightblue}{\textcolor{black}{6.9}} \\
    & & & & FA-Net & \cellcolor{lightgreen}{\textcolor{black}{97.95}} & \cellcolor{lightgreen}{\textcolor{black}{78.59}} & 99.09 & \cellcolor{lightblue}{\textcolor{black}{76.69}} & 81.17 & \cellcolor{lightgreen}{\textcolor{black}{78.69}} & \cellcolor{lightgreen}{\textcolor{black}{77.70}} & \cellcolor{lightgreen}{\textcolor{black}{82.55}} & \cellcolor{lightblue}{\textcolor{black}{42.4}} & 7.3 \\
    & & & & \textbf{Our UVSM} & 97.30 & 75.49 & 97.96 & \cellcolor{lightgreen}{\textcolor{black}{85.35}} & 68.19 & 75.49 & 74.78 & \cellcolor{lightblue}{\textcolor{black}{81.89}} & \cellcolor{lightgreen}{\textcolor{black}{21.6}} & \cellcolor{lightgreen}{\textcolor{black}{4.5}} \\

        \midrule
    
    \multirow{7}{*}{\(JRCIR\)} & \multirow{7}{*}{\textit{\shortstack{IR \\ NIR}}} & \multirow{7}{*}{Heidel} & \multirow{7}{*}{40}
          & W-Net & 97.07 & 79.79 & \cellcolor{lightgreen}{\textcolor{black}{98.85}} & 75.25 & \cellcolor{lightgreen}{\textcolor{black}{83.35}} & 78.80 & 77.52 & 79.19 & 55.3 & 7.0 \\
    & & & & WS-DMF & 96.32 & 74.49 & 98.10 & 74.37 & 75.87 & 74.49 & 72.88 & 75.67 & 107.9 & 9.8 \\
    & & & & GT-DLA-dsHff & 96.30 & 73.68 & 98.30 & 71.36 & 76.64 & 73.68 & 71.88 & 74.46 & 88.5 & 5.9 \\
    & & & & U-Net & 97.14 & 79.92 & 98.66 & \cellcolor{lightblue}{\textcolor{black}{78.74}} & 81.84 & 79.92 & 78.60 & 80.93 & 79.4 & 5.6 \\
    & & & & CS$^{2}$-Net & \cellcolor{lightblue}{\textcolor{black}{97.22}}
 & \cellcolor{lightblue}{\textcolor{black}{80.28}} & \cellcolor{lightblue}{\textcolor{black}{98.77}} & 78.30 & \cellcolor{lightblue}{\textcolor{black}{83.01}} & \cellcolor{lightblue}{\textcolor{black}{80.28}} & \cellcolor{lightblue}{\textcolor{black}{79.01}} & \cellcolor{lightblue}{\textcolor{black}{80.79}} & 86.9 & 5.6 \\
    & & & & FA-Net & \cellcolor{lightgreen}{\textcolor{black}{97.24}} & \cellcolor{lightgreen}{\textcolor{black}{80.48}} & 98.75 & \cellcolor{lightgreen}{\textcolor{black}{78.79}} & 82.84 & \cellcolor{lightgreen}{\textcolor{black}{80.48}} & \cellcolor{lightgreen}{\textcolor{black}{79.19}} & \cellcolor{lightgreen}{\textcolor{black}{81.30}} & \cellcolor{lightblue}{\textcolor{black}{49.6}} & \cellcolor{lightgreen}{\textcolor{black}{5.0}} \\
    & & & & \textbf{Our UVSM} & 96.10 & 74.66 & 97.84 & 75.31 & 72.85 & 73.66 & 71.81 & 76.30 & \cellcolor{lightgreen}{\textcolor{black}{28.8}} & \cellcolor{lightblue}{\textcolor{black}{5.5}} \\

    % \cellcolor{lightgreen}{\textcolor{black}{}}
    % \cellcolor{lightblue}{\textcolor{black}{}}

  \bottomrule
  \end{tabular}
  \vspace{-0.5cm}
\end{table*}

\begin{figure*}[htbp]
\centering
\includegraphics[width=0.99\textwidth]{Figures/main_quali_4.png}
\vspace{-0.3cm}
\caption{Qualitative comparison between our UVSM and finetuned SOTA models on multi-modality retinal images (better zoom in to view). Red, blue orange and green box include the original image, the ground truth label, finetuned FA-Net segmentation and the segmentation from our UVSM. From left to right, up to down: Topcon CF, Canon CF, Zeiss CF, Heidelberg MC, Optos FA, Heidelberg FA, Optos FAF, Heidelberg FAF, Heidelberg IR. Bottom right shows the qualitative results for Optos MC and Canon FA (no ground truth available to compare with and to train the baseline finetuned models).}
\label{fig:main_quali}
\vspace{-0.6cm}
\end{figure*}

% \begin{figure*}[htbp]
% \centering
% \includegraphics[width=0.95\textwidth]{Figures/main_quali_CF.png}
% \caption{Qualitative comparison between our universal model and finetuned SOTA models on CF modality from different cameras.}
% \label{fig:main_quali_CF}
% \end{figure*}

\subsection{Datasets}
\subsubsection{Image Translation Training}
\label{sssec:trans_dataset}
We prepared a multi-modality database with over 150k de-identified images from the database of the Jacobs Retina Center at UCSD. However, in our previous numerous attempts we found it very difficult and time-consuming to tune the training of the image translation network (regardless of the methods) for a large dataset. Therefore, in this work, we use a 4k images (2k CF, 500 FA, FAF, MC, IR/NIR, respectively) dataset from the larger 150k database. Each image has a resolution of 768 $\times$ 768. All the CF images are from the 30-degree (field-of-view) Topcon camera and all the IR/NIR images are from the Heidelberg camera. For FA, FAF and MC, 250 images are from Heidelberg camera. For Optos camera, the original images are ultra-wide-filed 135-degree images, which are out of the scope of this work (will be dicussed in \cref{ssec:limitations}). Therefore, we crop random regions from the center area of the retina to get 30-40-degree conventional field-of-view images and apply a commonly-used circular mask (\cref{fig:masks}(a)). In the dataset, there are 250 cropped and masked Optos image for FA, MC, FAF modalities, respectively.

\subsubsection{Segmentation Training}
\label{sssec:segtrain_dataset}
We train the segmentation network using only the 20 training images from the public DRIVE \cite{Dataset_DRIVE} CF dataset. In practice, we discard image \#14 since the vessel pattern is unusual. In addition, for all the images we resize them to 768$\times$768 for training. We also resized the ground truth annotation and manually corrected the error caused by interpolation. In addition, according to our examination, the original DRIVE ground truth has multiple topological errors such as small redundant holes in the vessel foreground. In our revised ground truth, we correct these errors and allow learning more accurate vessel topological features.

\subsubsection{Evaluation}
Seven diverse datasets are used for evaluation. For the CF modality, we use the public STARE \cite{Dataset-STARE}, ChaseDB1 \cite{Dataset-ChaseDB1} and HRF \cite{Dataset-HRF} datasets. For MC modality, we use the public IOSTAR \cite{Dataset-IOSTAR} dataset. For FA, FAF and IR/NIR, we prepared three new datasets: JRCFA, JRCFAF and JRCIR. Each dataset has 40 768$\times$768 images, which is more than most of the commonly-used public retinal vessel segmentation (CF) datasets. The JRCFA and JRCFAF datasets both consist 20 images from Heidelberg and 20 from Optos, respectively. The JRCIR dataset consists 40 Heidelberg (the only camera for this modality) images. All the vessel annotations are either labeled or carefully examined and refined by the retina experts in the UCSD Jacobs Retina Center (JRC). More information on the 7 datasets can be found in \cref{tab:main_tab}. In addition to the seven vessel segmentation dataset, we also evaluate our model qualitatively on the Optos MC images using images from our 150K database and on the Canon FA images on the CF-FA registration dataset \cite{Dataset-CFFA}. We provide preliminary insights in this paper (\cref{fig:intro}, \cref{fig:main_quali}) yet full evaluation requires the vessel labels (unavailable) and is left as future work.

\subsection{Implementation Details}
\subsubsection{Translation Network}
The balance between the size of the generator and discriminator is important to stably train the CycleGAN for our task. We use a U-Net \cite{Others-U-Net} generator with 16.66M parameters and an NLP discriminator with 2.77M parameters\footnote{More details on the network structures we use for translation and segmentation will be provided in our codes.}. For training, we use a batch size of 4 and train the model for 100 epochs on the 4000 images. We start with an initial learning rate of 2e-4 and start decaying the learning rate linearly to 0 from epoch 50 to 100. Adam optimizer is used to train both the generators and the discriminators, with a momentum of $(0.5, 0.999)$ and no weight decay. $\lambda_{c}=$10.0 and $\lambda_{i}=$0.5.

\subsubsection{Segmentation Network}
An initial learning rate of 1e-3 is used to train the model for 150 epochs, then decayed to 1e-4 to train for another 250 epochs. Adam optimizer is used with a weight decay of 1e-3 and momentum of $(0.9, 0.999)$. We train with a batch size of 4. The weights $\lambda_{tc}$ and $\lambda_{ts}$ are 0.05 and 0.0002, respectively. All the testing images are first padded to square and resized to 768$\times$768 to pass through the translation and the segmentation network. Then the prediction is resized back to the original image size to compute the metrics with the ground truth labels. The implementation of both image translation and the segmentation parts are under the PyTorch framework. The computation of persistent homology relies on the GUDHI library \cite{gudhi:urm}.


\subsubsection{Data Augmentation}
Extensive data augmentations are used for training both the translation and the segmentation network. For translation network, random scaling, sharpness, contrast and gamma adjustment are used. Additionally, since different camera manufacturers apply different shapes of mask to the image, we randomly apply the masks shown in \cref{fig:masks} to the images to prevent the model from generating artifacts out of the effective imaging area. For segmentation network, in addition to the aforementioned methods, we also apply random negative film followed by a contrast or sharpness adjustment. This is essential to allow the universal segmentation model to work for the FA modality as the vessels appears to be brighter than the background and we found no image translation method capable of stably inverting the color of the vessels. In addition, random image rotation, shift and Gaussian noise are applied.

\begin{figure}[]
\centering
\includegraphics[width=0.47\textwidth]{Figures/ablation_step.png}
\vspace{-0.3cm}
\caption{Ablation studies on how each proposed step in the pipeline improves the performance of the 
universal vessel segmentation model.}
\label{fig:ab_step}
\vspace{-0.4cm}
\end{figure}

\begin{figure}[]
\centering
\includegraphics[width=0.47\textwidth]{Figures/ablation_step_qualitative.png}
\vspace{-0.3cm}
\caption{Qualitative comparison for how each proposed step imporves the segmentation quality (better zoom in to view). From top to bottom: CF, MC, FA, FAF, IR modality.}
\label{fig:ab_step_examples}
\vspace{-0.6cm}
\end{figure}

\subsection{Comparison with Finetuned SOTA Methods}
To the best of our knowledge, this work is the first universal retinal vessel segmentation model and there is no previous method that we can fairly compare with. Therefore, we compared with state-of-the-art (SOTA) methods which are finetuned individually on each of the 7 datasets in a fully-supervised fashion. Since we use three new datasets and the works in the retinal vessel segmentation literature have inconsistent dataset choice, dataset split, and inconsistent choice of evaluation metrics, we prepare a new benchmark where all the methods are evaluated in a consistent way for fair comparison. The baseline methods are run using respective recommended training details and in a five-fold experiment that splits the whole dataset by 5 groups. Each fold uses one group for testing and the other four for training. Eventually, all images are used for testing so as to make sure that the average score is computed on the same images with our universal model (which is directly tested on all the images). The baseline methods include U-Net \cite{Others-U-Net}, CS$^{2}$-Net \cite{Vesseg-CNN-CS2Net}, W-Net \cite{Vesseg-CNN-WNet}, FA-Net \cite{Vesseg-CNN-FANet}, GT-DLA-dsHff \cite{Vesseg-CNN-GlobalTransformer} and WS-DMF \cite{Vesseg-CNN-WSDMF}. The training and testing of the baselines are done in original image size except for the HRF dataset. The images in HRF dataset are too large to be trained on our 24GB GPUs. Therefore, we first pad the image and ground truth to square, then downsampling them to 768$\times$768 for training and testing. The predicted vessel likelihood is then upsampled to the original image size to compare with the original ground truth and compute the metrics. 

Since the use of evaluation metrics is inconsistent in the literature, in our work we use most of the commonly-used metrics, including the pixel-wise accuracy (Acc.), the Dice score, the specificity (Sp), the sensitivity (Se), the Precision (Pr), the F1 score and the Mathew Correlation Coefficients (MCC). In addition, since topological learning is an important part of our work and the topological accuracy is important in retinal vessel segmentation, we also use three topological metrics which are commonly used in the segmentation of more general tubular structures. These metrics include the clDice score \cite{Topo-clDice} and the 0-/1-Betti errors ($\beta_{0}, \beta_{1}$) \cite{Topo-WTLoss}. All of the metrics are the larger the better except for the 0-/1- Betti errors, which are the smaller the better. For all of them reported metrics, we compute them at the original image size and averaged over all test images.

The results in \cref{tab:main_tab}, \cref{fig:main_quali} show that our proposed model, despite allowing universal segmentation, still possesses comparable performance with SOTA finetuned methods in a general sense. However, for some modalities there still exist non-neglectable performance gap, especially for the FA. As shown by an example in the second row of \cref{fig:main_quali}, the FA modality (particularly from the Optos camera) has very high contrast between the vessel and the background. As a result, more ending vanishing vessels are visible on the image and the universal model often fails to identify them. The reasons are two-fold: (1) The image translation introduces non-avoidable detail loss; (2) The topological loss function makes the model more conservative in predicting a pixel as vessel.

\begin{figure*}[ht]
\centering
\includegraphics[width=0.98\textwidth]{Figures/ablation_translation_4.png}
\vspace{-0.3cm}
\caption{Comparison of the final segmentation performance between using different methods for image translation (reporting pixel-wise accuracy).}
\label{fig:ab_transfer_chart}
\vspace{-0.5cm}
\end{figure*}

\begin{figure}[ht]
\centering
\includegraphics[width=0.48\textwidth]{Figures/ab_transfer3.png}
\vspace{-0.3cm}
\caption{Qualitative examples of the translated images using different image translation methods. (best zoom in to view)}
\label{fig:ab_transfer_examples}
\vspace{-0.6cm}
\end{figure}



\subsection{Ablation Studies}
\subsubsection{Pipeline}
\label{sssec:pipeline}
In this part, we study how the main components in the proposed pipeline improve the segmentation quality. All the experiments in the ablation studies adopt the same training configurations, data augmentations and network structure, unless otherwise stated. We first run a baseline experiment where no image translation nor topological learning is applied. In another word, we simply train a segmentation model on the DRIVE training set and apply the model to the multi-modality retinal images. As shown by \cref{fig:ab_step} and \cref{fig:ab_step_examples}, the performance of the model is very poor. After applying the image translation (CycleGAN), the segmentation quality notably improves, especially for the non-CF modalities. However, we still observe some results where the vessel boundary is not clear or adjacent vessels are mixing up (\cref{fig:ab_step_examples}). After further introducing the topological learning, the model makes clean and accurate segmentation.




\subsubsection{Image Translation Method}
\label{sssec:ab_image_translation}
We compare CycleGAN with two most recent Diffusion model-based methods (StyleDiffusion \cite{Trans-StyleDiffusion} and UNSB \cite{Trans-UNSB}). The 19 translated DRIVE images are from the respective translation method to train the downstream segmentation network.

For StyleDiffusion, we randomly pick only one CF image as the style target and reduce the training set size to the same as that in \cite{Trans-StyleDiffusion} because we find that on larger training set, StyleDiffusion generates severe artifacts despite our efforts to finetune the training hyperparameters for our images. In addition, data augmentation cannot be implemented in StyleDiffusion for its two-diffusion-stage design. In general, the final segmentation performance by StyleDiffusion is the poorest (\cref{fig:ab_transfer_chart}). Furthermore, similar with the observation in \cite{MMVesseg-Zhang}, where style transfer method is also used for retinal vessel segmentation, the transferred images tend to include many vessel-like artifacts (\cref{fig:ab_transfer_examples}) introduced by the perceptual inductive bias, which will later result in many artifacts in the segmentation.



For UNSB, the training is performed on the same dataset for training CycleGAN. We use the recommended training settings and network structure in \cite{Trans-UNSB} except for tuning the number of the diffusion steps to 6 to adapt to our data. 

As shown in \cref{fig:ab_transfer_chart}, when comparing the final segmentation performance, CycleGAN demonstrates the best performance for most datasets. It is also observed that the CycleGAN translation results are visually more alike with the target CF images where the sharpness and contrast of the images are weaker than in the other modalities (\cref{fig:ab_transfer_examples}). Furthermore, for FAF where the original images are very noisy, CycleGAN also performs better in reducing the noise and generating a smoother translated image.

\begin{figure}[]
\centering
\includegraphics[width=0.47\textwidth]{Figures/ab_topo.png}
\vspace{-0.3cm}
\caption{Qualitative results for \cref{sssec:ab_topo}, From top to bottom: CF, MC, FA, FAF, IR.}
\label{fig:ab_topo}
\vspace{-0.6cm}
\end{figure}

\begin{table}[ht]
  \caption{Comparison between different choices of topological loss functions. Light blue-conventional pixel-wise metrics, light red-topological metrics, light green-hybrid metric.}
  \vspace{-0.1cm}
  \footnotesize
  \label{tab:topo_loss}
  \centering
  
  
  % \begin{tabular}{@{}c|cc|ccccc@{}}
  \begin{tabular}{@{}c|cc|>{\columncolor{lightblue!50}}c>{\columncolor{lightblue!50}}c>{\columncolor{lightgreen!50}}c>{\columncolor{lightred!20}}c>{\columncolor{lightred!20}}c}
    \toprule
    \rowcolor{white} Dataset & $\mathcal{L}_{tc}$ & $\mathcal{L}_{ts}$ & Acc. & Dice & clDice & \(\beta_{0}\) & \(\beta_{1}\) \\

    % \midrule
    \hline
    \multirow{4}{*}{\(STARE\)}
      & - & - & 97.28 & 79.25 & 83.62 & 65.5 & 16.00 \\
      & $\checkmark$ & - & 97.29 & 78.83 & 83.14 & 114.7 & 16.2 \\
      & - & $\checkmark$ & 97.08 & 78.20 & 82.86 & \textbf{10.7} & 15.4 \\
      & $\checkmark$ & $\checkmark$ & \textbf{97.48} & \textbf{80.51} & \textbf{85.21} & 35.1 & \textbf{12.6} \\

    % \midrule
    \hline
    \multirow{4}{*}{\(ChaseDB1\)}
      & - & - & \textbf{96.57} & 77.03 & 78.34 & 92.7 & 10.5 \\
      & $\checkmark$ & - & 96.56 & 76.62 & 77.62 & 151.6 & 30.8 \\
      & - & $\checkmark$ & 96.34 & 75.32 & 77.22 & \textbf{19.1} & \textbf{6.4} \\
      & $\checkmark$ & $\checkmark$ & 96.55 & \textbf{77.34} & \textbf{78.34} & 70.1 & 7.5 \\

    % \midrule
    \hline
    \multirow{4}{*}{\(HRF\)}
      & - & - & 96.52 & 67.56 & 66.55 & 160.5 & 66.7 \\
      & $\checkmark$ & - & 96.59 & 67.79 & 66.61 & 168.5 & 57.5 \\
      & - & $\checkmark$ & 95.99 & 63.43 & 61.67 & \textbf{7.4} & \textbf{50.9} \\
      & $\checkmark$ & $\checkmark$ & \textbf{96.60} & \textbf{68.50} & \textbf{68.26} & 70.6 & 55.5 \\

    % \midrule
    \hline
    \multirow{4}{*}{\(IOSTAR\)}
      & - & - & 96.48 & 77.51 & 84.61 & 87.6 & 8.0 \\
      & $\checkmark$ & - & 96.56 & 77.49 & 83.56 & 160.7 & 6.3 \\
      & - & $\checkmark$ & 96.28 & 76.15 & 83.01 & \textbf{15.6} & 6.6 \\
      & $\checkmark$ & $\checkmark$ & \textbf{96.70} & \textbf{78.38} & \textbf{85.08} & 69.1 & \textbf{6.3} \\

    % \midrule
    \hline
    \multirow{4}{*}{\(JRCFA\)}
      & - & - & 90.85 & 40.15 & 36.35 & 233.4 & 17.1 \\
      & $\checkmark$ & - & 94.00 & 58.73 & 60.22 & 325.4 & 98.8 \\
      & - & $\checkmark$ & 95.41 & 69.78 & 72.91 & \textbf{15.6} & 16.0 \\
      & $\checkmark$ & $\checkmark$ & \textbf{96.70} & \textbf{75.00} & \textbf{76.37} & 37.7 & \textbf{15.2} \\

    % \midrule
    \hline
    \multirow{4}{*}{\(JRCFAF\)}
      & - & - & 95.90 & 64.45 & 69.03 & 203.9 & 8.9 \\
      & $\checkmark$ & - & 97.03 & 73.46 & 77.88 & 149.7 & 8.8 \\
      & - & $\checkmark$ & 96.52 & 71.00 & 79.07 & \textbf{17.1} & 5.6 \\
      & $\checkmark$ & $\checkmark$ & \textbf{97.30} & \textbf{75.49} & \textbf{81.89} & 21.6 & \textbf{4.5} \\

    % \midrule
    \hline
    \multirow{4}{*}{\(JRCIR\)}
      & - & - & 95.36 & 69.77 & 71.34 & 144.0 & 6.7 \\
      & $\checkmark$ & - & 96.01 & 73.48 & 74.66 & 149.2 & 14.6 \\
      & - & $\checkmark$ & 95.69 & 72.09 & 74.69 & \textbf{21.0} & \textbf{5.4} \\
      & $\checkmark$ & $\checkmark$ & \textbf{96.10} & \textbf{74.66} & \textbf{76.30} & 28.8 & 5.6 \\
      
    % \cellcolor{lightgreen}{\textcolor{black}{}}
    % \cellcolor{lightblue}{\textcolor{black}{}}

  \bottomrule
  \end{tabular}
  \vspace{-0.6cm}
\end{table}

\subsubsection{Topological Segmentation Loss Function}
\label{sssec:ab_topo}
In this part we compare how the combinations of topological segmentation loss functions impact the performance of the UVSM. \cref{tab:topo_loss} and \cref{fig:ab_topo} show that, both $\mathcal{L}_{tc}$ and $\mathcal{L}_{ts}$ helps with overcoming the domain gap of the non-CF modalities. It is observed that $\mathcal{L}_{tc}$ is unable to address the inverted vessel color in FA images and tend to instead predict the boundary of the vessel, which results in a `sandwich' pattern. In addition, using only $\mathcal{L}_{tc}$ adds saw artifacts and discontinuities to the segmentation. $\mathcal{L}_{ts}$ is able to address the problem of FA modality and improves the discontinuities along the vessel, however, it is observed that if only $\mathcal{L}_{ts}$ is used, the vessel boundary will be smoothed and the morphological quality of the segmentation will be poor. With a combination of $\mathcal{L}_{tc}$ and $\mathcal{L}_{ts}$, the saw and smoothing artifacts counteracts, and the challenge in predicting FA vessels is solved, producing the optimal results.



\vspace{-0.1cm}


\section{Conclusions}
\label{sec:Conclusions}
\vspace{-0.2cm}

\subsection{Limitations}
\label{ssec:limitations}
As demonstrated and discussed in the experiment section, our current universal segmentation has several limitations to improve upon. Firstly, although the primary vessels are well predicted, the model tends to overprune the vessels, which leads to thin vanishing vessels appearing at the peripheral area of the retina or near the macula are missing in the segmentation. This is caused by both the non-neglectable detail loss of the image translation process and the effect of topological loss function. Secondly, our current work only addresses the vessel segmentation problem for multi-modality retinal images in a most commonly-used 30-60 degree field-of-view. Our work does not include the ultra-wide-field 135 degrees retina images. Moreover, we does not consider the commonly-used 10-15 degree OCTA image, as its imaging principle is different. These two types of images are considerably different with the conventional modalities and requires further research to be included into the universal vessel segmentation model.

\subsection{Conclusion and Future Work}
\label{ssec:conclusion}
In this paper, we propose a foundational universal vessel segmentation model for multi-modality retinal images. Compared with the past literature of retinal vessel segmentation where (1) at most time only Color Fundus modality is studied and; (2) separate models need to be trained for different datasets (from different cameras) to achieve good performance, our method is able to perform robust vessel segmentation for all commonly-used modalities from all commonly-used cameras using only a single model and achieved comparable performance with SOTA finetuned methods. Although the current model still has limitations and we are working to further improve it, we believe this work is a milestone in the community of retinal image processing and understanding and marks a new direction in the research of retinal vessel segmentation. Our future work includes improving the universal vessel segmentation model to allow them to work for a wider range of field of views and to better predict the fine vanishing vessels. 

\vspace{-0.3cm}

\bibliographystyle{IEEEtran}
\bibliography{ref}


\vfill

\end{document}


