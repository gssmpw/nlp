\section{Related Work}
\subsection{Retinal Vessel Segmentation}
Retinal vessel segmentation classifies each pixel in the retinal images into foreground (vessel) and background (non-vessel). Current works focus on segmenting vessels in the CF images, where various machine learning-based or non-machine learning-based methods were proposed. A majority of the early methods are based on the matched filter \cite{Vesseg-MF-Chaudhuri, Vesseg-MF-Kovacs, Dataset-HRF}, which convolve retinal images with predefined or learned kernels and then adaptively threshold the image to obtain the vessel segmentation map. Another type of methods uses tracking algorithms to mathematically estimate the growth of the retinal vessel trees from seed points \cite{Vesseg-Tracking-Delibasis, Vesseg-Tracking-Lin}. The segmentation can then be derived by extending around the estimated vessel centerlines using the estimated vessel diameters. Some methods also adopt morphological image processing methods such as the top-hat operation to extract the vessel or as a post-processing step \cite{Vesseg-Morph-Graz, Vesseg-Morph-Imani}. Furthermore, numerous traditional machine learning-based methods were also applied in retinal vessel segmentation such as the support vector machine \cite{Vesseg-SVM-Tang}, random forest \cite{Vesseg-RF-Wang} and Adaboost \cite{Vesseg-Adaboost-Memari} for supervised methods, and Gaussian mixture model \cite{Vesseg-GMM-Roy}, Fuzzy C-Means \cite{Vesseg-FCM-Neto} for unsupervised methods. In more recent years, deep learning-based methods dominate the literature. They mainly rely on the Convolutional Neural Networks (CNN) as a backbone model due to the existence of vast fine details and the spread-out characteristic of the vessels. An introductory work is the Deep Retinal Image Understanding (DRIU) \cite{Vesseg-CNN-DRIU}, which adopts a VGG \cite{Others-VGG} network structure as encoder and assigns specialized output layers to the embedded features to segment the vessels and the optic disc from the retina. Based on a widely-used U-Net \cite{Others-U-Net} network structure and to seek further improvement for retinal vessel segmentation, \cite{Vesseg-CNN-CENet, Vesseg-CNN-CS2Net, Vesseg-CNN-GlobalTransformer, Vesseg-CNN-MCDAUNet, Vesseg-CNN-MCGNet} proposed to add various global reasoning modules at the bottom of the network. \cite{Vesseg-CNN-MCDAUNet, Vesseg-CNN-MCGNet, Vesseg-CNN-RCARUNet, Vesseg-CNN-WaveNet, Vesseg-CNN-GlobalTransformer} proposed to add feature aggregation modules in the residual connection at each level of the network. \cite{Vesseg-CNN-DUNet, Topo-TCLoss} proposed to use deformable convolution where the convolution kernel shapes are learned to adapt to the elongated morphology of the retinal vessels. With similar motivation, \cite{Vesseg-CNN-WSDMF} proposed an orientation-selective convolution to learn selective receptive fields of the kernel to adapt to the shape of the vessels. \cite{Vesseg-CNN-GlobalTransformer} proposed to use improved feature fusion methods to better preserve the fine details of vessels in the segmentation. In addition to a single U-Net-like segmentation network, several works \cite{Vesseg-CNN-WNet, Vesseg-CNN-WSDMF, Vesseg-CNN-FANet} proposed to use cascaded networks or a single looped network to iteratively refine the segmentation. 

\subsection{Unpaired Image-to-Image Translation} 
Image-to-image translation aims to transfer a source image to a target image domain while preserving its structural information. However, it is difficult to prepare paired retinal images from the source and target domain for training. Therefore, in this work we look into the unpaired image-to-image translation methods where the projection between the two domains can be learned with unpaired images with no structural correspondences. A representative and most-commonly used method in this task is CycleGAN \cite{Trans-CycleGAN}, which introduces a cycle consistency that projects the translated image back to the source domain to force retaining the structural information in the translated image. Following works further introduce geometric consistency \cite{Trans-CcGAN}, mutual information regularization \cite{Trans-Mutual} and contrastive unpaired translation \cite{Trans-CUT} to improve the computational efficiency over the cycle consistency-based methods during the training phrase. \cite{Trans-MUIST, Trans-StarGAN} proposed one-to-many transfer schemes which allow translating images to multiple domains. While the above works are based on GAN backbone and achieved good image translation performance, diffusion models \cite{Others-Diffusion, Others-DDPM, Others-DDIM} have recently gained more research attention in this topic for easier training and superior image quality. However, the methods \cite{Trans-DDIB, Trans-ENOT, Trans-SynDiff} are computationally intensive and were only applied on small images. For example, \cite{Trans-SynDiff} combines diffusion model with adversarial learning and formulate the training in a cycle-consistency framework, which requires training two diffusion models at the same time. Most recently, \cite{Trans-UNSB} proposed an Unpaired Neuron Schrodinger Bridge (UNSB) with a novel scheme to simulate the Schrodinger Bridge, which is a random process that interpolates between two image domains, via a diffusion process. It has higher computational efficiency and makes application to higher-resolution images (e.g., retinal images) possible.


Image style transfer \cite{Trans-NAAS, Trans-PerceptualLoss, Trans-EFDM, Trans-StyTr2, Trans-StyleDiffusion} is another method which can be seen as a special type of unpaired image-to-image translation but with a perceptual inductive bias. The majority of the methods use a perceptual feature space to compute a style loss and inject the style of a reference image to the input content image, while using another self-comparison content loss to retain the content information of the input in the transferred image. We also explore methods in this topic since style transfer is used for non-CF retinal vessel segmentation in \cite{MMVesseg-Zhang, MMVesseg-StyleTransferPeng}. 


\subsection{Topology-Aware Image Segmentation}
Topology-aware image segmentation aims to preserve the topological accuracy in the segmentation of tubular and net-like structures. The literature mainly considers a topological loss function that extracts topological features from the ground truth and segmentation and minimizes the difference between them. One type of methods extracts certain features from the segmentation which are proximity to the actual topological features of the foreground objects. A pioneer work \cite{Topo-PTLoss} uses an ImageNet-pretrained VGG \cite{Others-ImageNet, Others-VGG} network after the segmentation network. The perceptual features extracted from different layers of the pretrained VGG network is shown to be approximations of the topological features. The topological loss is then computed between the ground truth and prediction perceptual features. Another topological segmentation loss function \cite{Topo-GlandSeg} was proposed exclusively for the segmentation of glands. The loss is computed according to the iterations needed to derive the gland skeletons by the image erosion operation. \cite{Topo-clDice} proposed to approximate the topological features of tubular structures by a relationship between the foregrounds in the ground truth and prediction and their soft-skeletons. Using such a relationship as a loss function is shown to strengthen the connectivity along the tubular structure thus providing better topological accuracy. 

Another type of methods uses strict topological features of the segmentation which are based on the computational topology theory and received more research attentions in recent years. The topological features are mainly based on the persistent homology theory \cite{Topo-WTLoss, Topo-WTLoss3D, Topo-CMR,  Topo-TPAMI, Topo-TCLoss, Topo-MLLoss, Topo-BMLoss, Topo-SATLoss}. A revolutionary work that first introduce this concept in image segmenation is \cite{Topo-WTLoss}, which computes the persistent features from the ground truth and prediction, matching the features using a Wasserstein distance \cite{Topo-Wassersteinforpersistence} and using the distance as the loss function. The following works share similar idea. \cite{Topo-WTLoss3D} also uses the Wasserstein distance but apply the method on 3D point cloud data. \cite{Topo-MLLoss} proposed to match the longest persistent barcodes in the prediction to the ground truth. \cite{Topo-TCLoss} proposed to use a Hausdorff distance \cite{Topo-Hausdorff_distance} to match the persistent diagrams to improve the sensitivity of the matching to the outliers. \cite{Topo-SATLoss} and \cite{Topo-BMLoss} both aim to compute more precise persistent feature matching, as in earlier methods the actual matching results are highly inaccurate due to only considering the global topological features. \cite{Topo-SATLoss} and \cite{Topo-BMLoss} instead allow the loss to act more selectively on the topological features thus improve the efficiency of the topological loss function. \cite{Topo-BMLoss} proposed to use an Induced Matching \cite{Topo-Induced_matching} method which constructs a common ambient space between the ground truth and prediction and maps the persistent features in between to find a better matching. However, the computational cost of this method is very high, especially for large images, where typical retinal images usually have high resolutions. Alternatively, \cite{Topo-SATLoss} proposed to leverage the spatial locations of the persistent features referencing their critical cell locations for more faithful persistent feature matching and is computationally much more efficient than \cite{Topo-BMLoss}.

\begin{figure*}[ht]
\centering
\includegraphics[width=1\textwidth]{Figures/Pipeline.png}
\vspace{-0.7cm}
\caption{Pipeline for our proposed universal vessel segmentation model, which includes the 2-stage training and the inference.}
\label{fig:pipeline}
\vspace{-0.6cm}
\end{figure*}