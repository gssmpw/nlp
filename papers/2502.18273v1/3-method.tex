\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figure/introduction.pdf}
    \caption{The illustration of shortcut learning, wherein the model exploits spurious correlations between questions and answers (\emph{e.g., answer length}) rather than genuinely understanding the underlying reasoning, ultimately results in poor generalization ability.}
    \label{fig:introduction}
\end{figure}
\section{Preliminary}
This paper investigates the generalization capabilities of transformer-based language models when applied to compound tasks characterized by dynamic dependencies among sub-tasks. Unlike standard sequential processing, compound tasks involve inter-related actions where the state of each action depends on a varying subset of previous actions. These dependencies are not static but evolve based on the task's progress, posing a unique challenge for models trained on sequential text. For example, consider preparing a complex meal with multiple dishes. The cooking state of pasta, for instance, depends on whether the sauce is freshly made or pre-made, demonstrating how dynamic dependencies can arise. This scenario reflects the intricate structure of many real-world tasks where a simple sequential reading fails to capture the underlying relationships between actions.

We formalize compound tasks through a state transition framework with dynamic dependencies, motivated by real-world scenarios like software development: subtask states (e.g., database setup, API integration) evolve through context-dependent interactions.
\subsection{Compound Task Structure}  
A compound task organizes complex operations through a hierarchical tree of subtasks. The structure comprises atomic actions at leaf nodes and interconnected subtasks at higher levels, with dynamic dependencies governing their execution flow. As illustrated in \ref{fig:disbution_shift}, the system processes these tasks sequentially, evaluating dependencies and aggregating results from completed subtasks to achieve the overall objective. This flexible architecture allows for modification and reorganization of subtasks to accommodate evolving requirements.


\begin{figure}[t]
    \centering
    % \begin{subfigure}{0.45\linewidth}
        % \centering
        \includegraphics[width=0.9\linewidth]{figure/distribution_shift.pdf}
        % \caption{CoT alleviates distribution shift}
        % \label{fig:COT dis shift}
    % \end{subfigure}
    % \hfill
    % \begin{subfigure}{0.45\linewidth}
    %     \centering
    %     \includegraphics[width=\linewidth]{figure/cp.pdf}
    %     \caption{Compound task}
    %     \label{fig:introcp}
    % \end{subfigure}
    % \vspace{-1em}
    \caption{ Chain-of-thought alleviates distribution shift by breaking down complex problems into simpler, familiar sub-problems.}
    \label{fig:disbution_shift}
    %\vspace{-1.5em}
\end{figure}

To accommodate transformer models' input format requirements, we define the compound task at the token level. Figure \ref{fig:cp2} provides a step-by-step illustration demonstrating how this definition integrates into the task structure.
\begin{definition}
\label{def:CP}
Given the input sequence $S = (s_1, \ldots, s_n)$ and subtask state sequence $Q = (q_1, \ldots, q_n)$ where $Q^i = (q_1, \ldots, q_i)$ denotes the prefix subsequence contains the first $i$ states of $Q$, CP($n$) is defined:
\setlength{\itemsep}{-1pt}
\item \textbf{Dynamic Graph Evolution:} At step $i+1$, the dependency graph updates with a function $B: S^{i+1} \times \mathbb{N} \to \mathcal{P}(\mathbb{N})$:
\begin{align}
    G_{i + 1} = G(q_1, \ldots, q_i \mid s_1, \ldots, s_{i+1})
    \notag\\= \{q_k \mid k \in B(s_1,\ldots,s_{i+1}, i+1)\}
\end{align}
\vspace{-0.1em}
where $B(s_1,\ldots,s_i, j)$ returns the set of indices that determine the dynamic dependencies based on the current state sequence and global semantic constraints.
\vspace{-0.4em}
\item \textbf{State Transition Function} $F: \mathcal{P}(Q^i) \times Q^i \to q$ computes the next state using selected predecessors from the prefix subsequence $Q^i$:
\begin{equation*}
    \begin{split}
        q_{i+1} &= F(G(q_1, \ldots, q_i \mid s_1, \ldots, s_{i+1}), s_{i+1}) \\
        &\text{where} \quad F(\emptyset) = \text{Constant}
    \end{split}
\end{equation*}
\vspace{-1em}
\item \textbf{Final Result Operation:} The system's final result is computed through a sequence of operations $L_i$ that aggregate states progressively: $L: q \times q \rightarrow R$
\begin{equation*}
\begin{split}
    L_1 &= H(\emptyset, q_1) = q_1 \\
    L_i &= H(L_{i-1}, q_i) \quad \text{for } i = 2,\ldots,N
\end{split}
\end{equation*}
\vspace{-0.2em}
where $H$ is an aggregation function like maximum, minimum, or summation function and $H(\emptyset, q) = q$ that can be specialized as needed.


\end{definition}
\vspace{-\baselineskip}
\paragraph{Problem Setting}
To investigate the generalization capabilities of transformer-based language models, we establish a comprehensive training and evaluation framework across different complexity levels. Our framework involves training the model on problems of complexity levels $n_1$ and $n_2$, while evaluating on problems of complexity $n_3$, where $n_3$ is strategically chosen between $n_1$ and $n_2$. Formally, let $X^{n} = (s_1, s_2, ..., s_n)$ denote the input question text for CP($n$) problems and $Y^{n} = (y_1, y_2, ..., y_n)$ represent their corresponding output answers. The transformer model is trained on the mappings $X^{n_{1}} \rightarrow Y^{n_{1}}$ and $X^{n_{2}} \rightarrow Y^{n_{2}}$, and subsequently evaluated on $X^{n_{3}} \rightarrow Y^{n_{3}}$, where $n_1 \neq n_2 \neq n_3$.
The strategic selection of $n_3$ within the interval $(n_1, n_2)$ is informed by the token distribution patterns observed in transformer training. Since CP($n_2$) problems typically encompass the token vocabulary required for CP($n_3$), this ensures that the model's token embeddings are adequately trained for the evaluation tasks. This approach helps minimize the impact of unseen tokens during OOD evaluation, enabling us to focus on assessing the transformer's fundamental reasoning capabilities rather than its ability to handle novel vocabulary.
In our experiments, we utilize two types of training data:
%\vspace{-\baselineskip}
\begin{itemize} \setlength{\itemsep}{-1pt}
    \item Question-Answer pairs (Q-A): Constructed using $X^n$ as input and $Y^n$ containing only the final answer token (e.g., the length of the longest increasing sequence)
    \item Question-CoT pairs (Q-CoT): Constructed using $X^n$ as input and $Y^n$ containing the complete Chain of Thought tokens as defined in Section \ref{def:CoT}
\end{itemize}

\section{Theoretical Analysis}
In order to explain how CoT training mitigates shortcut learning and distribution shifts in compound tasks. We perform theoretical analysis in three stages: (1) characterizing transformer shortcut learning mechanisms, (2) deriving structural conditions for effective CoT sequences, and (3) quantifying CoT's generalization benefits through distribution shift theory. We will present details as follows:
% First of all, we will construct a trivial shortcuts solution to elaborate it's for transformer trapped in shortcuts when training on simimliar problem settings.
% Second, we will introduction recap condition which is a guidance to write better CoT. It's told that the token should be placed on a specfic way. And finaly, we will make a important statement that the reason why CoT helps in generalization is: CoT can decrease OOD distribution shift from training set on such tasks from a KL divergence's point of view.


\subsection{Shortcuts via Transformer}
Shortcut learning \cite{geirhos2020shortcut,robinson2021can}, wherein models exploit spurious correlations instead of learning robust, generalizable features, poses a significant challenge to OOD generalization. In the context of compound tasks, transformers, despite their capacity for complex computations, can be susceptible to learning trivial shortcuts, particularly when training data exhibits systematic patterns. We can easily construct a trivial shortcut example in the compound task setting.
\newline
For a compound task with two patterns having different token lengths $n_1$, $n_2$, transformers can learn shortcuts for classification and memorization. Let $T: n \to {0,1}$ be the binary classifier that identifies pattern type based on sequence length.
The shortcut solution consists of two steps:

1.  Pattern Classification: Using $O(1)$ attention heads to distinguish sequence lengths by uniform attention over all positions.


2.  Pattern Memorization: After identifying the pattern type, the transformer uses $O(\max(n_1, n_2))$ layers with attention heads to capture position-dependent features of the input sequence.

The total complexity is $O(\max(n_1, n_2))$ layers, with each layer requiring attention heads to learn position-specific patterns. This provides a more efficient solution compared to memorizing all possible question-answer pairs in the training set, as it leverages the structural property of sequence length for classification.


% Assume F is only fundamental operatation, which is one of following $\{+, -, sum, mean, min, max\}$.


\subsection{Recap Conditions for Effective Chain of Thought} \label{sec:recap-theory}

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[scale=0.7, transform shape,
        token/.style={draw, minimum width=1cm, minimum height=1cm, align=center},
        % Style for tokens that are "recapped" or serve as dependency highlights
        recapped token/.style={draw=red, fill=red!20, minimum width=1cm, minimum height=1cm, align=center},
        window/.style={draw=blue, thick, rounded corners},
        arrow/.style={->, >=Stealth, thick, shorten >=2pt},
        recap arrow/.style={->, >=Stealth, line width=2pt, red, shorten >=2pt, out=270, in=120}
    ]
    
    % --- BEFORE RECAP ---
    \node[anchor=north] at (0,2) {\textbf{Before Recap}};
    \foreach \x/\t in {1/s2,2/s3,3/s4,5/s6,6/s7,7/s8,8/s9,9/s10,10/s11,11/s12} {
        \node[token] at (\x*1.2,1) {\t};
    }
    % Draw window covering tokens at positions 3,4,5,6,7 (i.e. from 3.6 to 8.4)
    \draw[window] (1.8,0.4) rectangle (7.8,1.6);
    \node[anchor=north] at (5,0.2) {Window (size 5)};
    \node[recapped token] at (0*1.2,1) {s1};
    \node[recapped token] at (4*1.2,1) {s5};
    % Dependency arrow (s6 depends on s1)
    %\node[anchor=north] at (0, -0.1) {s6 requires for s1};
    
    % --- AFTER RECAP ---
    \node[anchor=north] at (0,-1) {\textbf{After Recap}};
    % First, draw all tokens except the recapped one
    \foreach \x/\t in {0/s2,1/s3,2/s4,5/s6,6/s7,7/s8,8/s9,9/s10,10/s11,11/s12} {
        \node[token] at (\x*1.2,-2) {\t};
    }
    \node[recapped token] at (3*1.2,-2) {s1};
    \node[recapped token] at (4*1.2,-2) {s5};
    % Draw window covering tokens at positions 3,4,5,6,7 (i.e. from 3.6 to 8.4)
    \draw[window] (1.8,-1.4) rectangle (7.8,-2.6);
    \node[anchor=north] at (5,-0.9) {Window (size 5)};

    % Now insert the recapped s1 (brought inside the window) at position 4
    %\node[recapped token] at (4*1.2,-2) {s1};
    % Recap arrow from the original s1 (at index 0 in BEFORE) to the recapped s1 (inside the window)
    \draw[recap arrow] (0*1.2,0.8) to (3*1.2,-2);
    \node[anchor=north] at (6,-3) {Recapped s1 inside window};
    
    
    \end{tikzpicture}
    \caption{Outside Window Recap Condition: Recapping a token from outside the attention window, s5 depends on s1}
    \end{figure}
    
    %---------------------------------------------------------------------
    
    % Figure 2: Inside Window Recap
    \begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[scale=0.7, transform shape,
        token/.style={draw, minimum width=1cm, minimum height=1cm, align=center},
        recapped token/.style={draw=red, fill=red!20, minimum width=1cm, minimum height=1cm, align=center},
        window/.style={draw=blue, thick, rounded corners},
        arrow/.style={->, >=Stealth, thick, shorten >=2pt},
        recap arrow/.style={->, >=Stealth, line width=2pt, red, shorten >=2pt, out=270, in=120}
    ]
    
    % --- BEFORE RECAP ---
    \node[anchor=north] at (0,2.2) {\textbf{Before Recap}};
    % Draw all tokens except the dependent s5 (which will be colored)
    \foreach \x/\t in {1/s2,3/s4,4/s5,5/s6,6/s7,7/s8,8/s9,9/s10,10/s11} {
        \node[token] at (\x*1.2,1) {\t};
    }
    % Insert the dependent token s5 (highlighted) at position 5
    \node[recapped token] at (0*1.2,1) {s1};
    \node[recapped token] at (2*1.2,1) {s3};
    \node[recapped token] at (5*1.2,1) {s6};
    % Draw window covering tokens 3 to 7 (i.e. x from 3.6 to 8.4)
    \draw[window] (-0.6,0.4) rectangle (12.6,1.6);
    \node[anchor=north] at (6,0.2) {Window (size 11)};
    
    %\node[anchor=north] at (1.2,-0.1) {s5 depends on s1 and s3};
    
    % --- AFTER RECAP ---
    \node[anchor=north] at (0,-1) {\textbf{After Recap}};
    % Draw tokens that are not recapped
    \foreach \x/\t in {0/s2,1/s4,2/s5,3/s4,5/s6,6/s7,7/s8,8/s9,9/s10,10/s11} {
        \node[token] at (\x*1.2, -2) {\t};
    }
    % Insert the recapped tokens s1 and s3 and the dependent token s5 at positions 5, 6 and 7 respectively.
    %\node[anchor=north] at (4,-0.8) {copy};
    \node[recapped token] at (3*1.2,-2) {s1};
    \node[recapped token] at (4*1.2,-2) {s3};
    \node[recapped token] at (5*1.2,-2) {s6};
    \draw[window] (-0.6,-1.4) rectangle (12.6,-2.6);
    \node[anchor=north] at (6,-0.8) {Window (size 11)};
    % Recap arrows from the original s1 and s3 (in BEFORE) to the recapped tokens (in AFTER)
    \draw[recap arrow] (0*1.2,0.8) to (3*1.2,-2);
    \draw[recap arrow] (2*1.2,0.8) to (4*1.2,-2);
    \node[anchor=north] at (6,-3) {Recapped s1 and s3 before s6};
    
    \end{tikzpicture}
    \caption{Inside Window Recap Condition: Recapping tokens to preserve causal order and skip the irrelevant token, s6 depends on s1 and s3}
    \end{figure}


Before introducing the Chain of Thought \cite{wei2022chain} approach, we establish two fundamental conditions that enable more effective reasoning chains:\newline
1. \textbf{Outside Window Recap Condition} Due to finite precision constraints, only tokens with the top-k attention scores can be effectively recalled. When the distance between the target token position and current position exceeds a threshold, the attention scores become negligible within the given precision, preventing recall of tokens outside this window. This necessitates strategic recapping of tokens within the window to maintain information flow. See \ref{thm: Recap 1} for formal analysis.
\newline
2. \textbf{Inside Window Recap Condition:} For optimal model performance, tokens within the attention window should be organized following the ground truth causal order, avoiding irrelevant intermediary tokens that could impede convergence. As proven in \ref{thm: Recap 2}, including irrelevant tokens in the sequence slows down model convergence. Therefore, we must carefully structure the token sequence within the window (e.g., through techniques like reverse ordering or selective token repetition) to align with the underlying causal structure.
\newline
Combined with these two recap conditions, for each inference step in a compound task, we must ensure all dependent tokens are both compactly positioned and arranged in their causal order. This strategy enables effective information flow while maintaining computational efficiency. In implementation, since tokens may have multiple dependencies, it is preferable to copy tokens rather than move them.


\subsection{Chain of Thought Suppresses the Shortcuts}
Inspired by recent work \cite{li2024chainthoughtempowerstransformers, feng2023revealingmysterychainthought} and building upon our recap conditions, we now introduce a CoT formalization specifically designed to capture the dynamic state transitions in compound tasks. Rather than allowing the model to learn superficial shortcuts, our CoT format explicitly tracks the evolving dependencies and state transitions that characterize the ground truth recurrent solution.

\begin{definition}[Chain of Thought]
  \vspace{0em}
    \small\begin{align*}
    \vspace{-0.5em}
        \text{Input:} & \; s_1 \mid \cdots \mid s_N \\[1ex]
        \text{CoT Steps:} & \; \langle\text{sep}\rangle \; s_2 \mid G_2 \mid q_2 \mid L_1 \mid L_2 \\
        & \; \langle\text{sep}\rangle \; \cdots \\
        & \; \langle\text{sep}\rangle \; s_{N} \mid G_N \mid q_N \mid L_{N-1} \mid L_N
    \end{align*}
    \label{def:CoT}
\end{definition}
\vspace{-\baselineskip}
This format structures each step to track dependencies ($G_i$), states ($q_i$), and intermediate results ($L_i$), guiding the model toward learning true solution dynamics. Remarkably, this principled approach can be implemented with minimal architectural requirements:

\begin{proposition}
\label{thm:CoT}
For any compound problem satisfying Definition \ref{def:CP}, and for any input length bound $n \in \mathbb{N}$, there exists an autoregressive Transformer with:
\begin{itemize}
\item Constant depth $L$
\item Constant hidden dimension $d$
\item Constant number of attention heads $H$
\end{itemize}
where $L$, $d$, and $H$ are independent of $n$, such that the Transformer correctly generates the Chain-of-Thought solution defined in Definition \ref{def:CoT} for all input sequences of length at most $n$. Furthermore, all parameter values in the Transformer are bounded by $O(\text{poly}(n))$.
\end{proposition}

\subsection{Distribution Shift Analysis for Q-A vs.\ Q-CoT}
Understanding the distribution shift between traditional Q-A and CoT approaches is crucial for analyzing OOD generalization. This section examines how CoT's intermediate reasoning steps influence the alignment between training and evaluation distributions.
We analyze a dynamic state-transition system where problems of different lengths are processed through either direct Q-A or through CoT reasoning \ref{fig:disbution_shift}. Specifically, we compare:
1. \textbf{Q-A training/evaluation} on problem lengths \(\{n_1, n_2\}\) (train) and \(n_3\) (eval).
2. \textbf{Q-CoT training/evaluation} on problem lengths \(\{n_1, n_2\}\) (train) and \(n_3\) (eval).
We assume \(n_{1} < n_{3} < n_{2}\), and let
\[
   \mathcal{D}_{\text{train}}^{\text{Q-A}} 
   \;=\;
   \Bigl\{
   (X^{n_1}, Y^{n_1}),\, 
   (X^{n_2}, Y^{n_2})
   \Bigr\}, 
   \quad \]
\[
   \mathcal{D}_{\text{eval}}^{\text{Q-A}}
   \;=\;
   \Bigl\{(X^{n_3}, Y^{n_3})\Bigr\},
\]
be the respective datasets in the Q-A setup.  Analogously, in the Q-CoT setup, each problem instance is expanded into subproblem–subsolution pairs (the chain-of-thought states).  Denote:
\[
   \mathcal{D}_{\text{train}}^{\text{Q-CoT}} 
   \;=\;
   \Bigl\{
   \bigl(X^{n_1}, \{q_i^{(n_1)}\}, Y^{n_1}\bigr),\, 
   \bigl(X^{n_2}, \{q_i^{(n_2)}\}, Y^{n_2}\bigr)
   \Bigr\},
   \quad \]
\[
   \mathcal{D}_{\text{eval}}^{\text{Q-CoT}}
   \;=\;
   \Bigl\{\bigl(X^{n_3}, \{q_i^{(n_3)}\}, Y^{n_3}\bigr)\Bigr\}.
\]
Here \(\{q_i^{(n)}\}\) denotes the chain-of-thought states or subsolutions for a length-\(n\) problem. 

We let \(P_{\text{train}}^{\text{Q-A}}\), \(P_{\text{eval}}^{\text{Q-A}}\), \(P_{\text{train}}^{\text{Q-CoT}}\), and \(P_{\text{eval}}^{\text{Q-CoT}}\) be the corresponding data or model-induced distributions over Q-A setting or Q-CoT setting.

Our analysis begins with a fundamental result about the structure of CoT sequences in our dynamic system.


% \medskip

%=============================================================================%
\subsubsection{Prefix-Substructure Theorem in a Dynamic State-Transition System}
%=============================================================================%

A key property of chain-of-thought sequences is that prefix relationships between input sequences carry over to their states, enabling shorter problems CP($n_3$) to embed within longer ones CP($n_2$).

\begin{lemma}[Prefix Substructure]
\label{thm:prefix-substructure}
Let \(S^{short} = (s_1, s_2, \ldots, s_{n_2})\) and let \(S^{long} = (s_1, s_2, \ldots, s_{n_3})\) be its prefix, with \(n_3 < n_2\).  Suppose their chain-of-thought sequences under the same \((G,F)\) are
\[
   Q^{short} \;=\;
   \bigl(q_1^{short}, q_2^{short}, \ldots, q_{n_2}^{short}\bigr)
   \quad\text{and}
\]
\[\quad
   Q^{long} \;=\;
   \bigl(q_1^{long}, q_2^{long}, \ldots, q_{n_3}^{long}\bigr).
\]
Then for all \(1 \le i \le n_3\), we have
\[
   q_i^{long} 
   \;=\; 
   q_i^{short}.
\]
Hence \(\bigl(q_1^{long},\ldots,q_{n_3}^{long}\bigr)\) is exactly the prefix of \(\bigl(q_1^{short},\ldots,q_{n_2}^{short}\bigr)\).
\end{lemma}

This property suggests CoT's intermediate steps provide natural bridges between problems of different lengths, potentially easing distribution shift concerns.
\subsubsection{Training Size Effects on Distribution Shift Through Prefix Coverage}
Building upon the prefix-substructure property, we now quantify the distribution shift between training and evaluation sets for both Q-A and Q-CoT approaches. This analysis reveals how CoT's intermediate reasoning steps can potentially mitigate distribution shift effects.
\begin{theorem}[KL Divergence Reduction with Training Size]
\label{thm:kl-reduction}
Let $m_1$ and $m_2$ be the number of training sequences of length $n_1$ and $n_2$ respectively, with total training size $M = m_1 + m_2$. Let $n_1 < n_3 < n_2$ where $n_3$ is the evaluation length and $m_3$ is the number of sequences in evaluation set. Then:
1. The coverage probability $P_{\text{cover}}(M)$ for prefixes of length $n_3$ is:
\[
   P_{\text{cover}}(M) = \frac{m_2}{m_3 k^{n_3}}
\]
where $k$ is the size of the input vocabulary $m_2 \leq m_3 k^{n_3}$.

2. The KL divergence between evaluation and training distributions under Q-CoT satisfies:
\[
   D_{\mathrm{KL}}\!\Bigl(
      P_{\text{eval}}^{\text{Q-CoT}}
      \,\big\|\,
      P_{\text{train}}^{\text{Q-CoT}}
   \Bigr)
   \;\leq\;
   (1 - P_{\text{cover}}(M)) \cdot D_{\mathrm{KL}}\!\Bigl(
      P_{\text{eval}}^{\text{Q-A}}
      \,\big\|\,
      P_{\text{train}}^{\text{Q-A}}
   \Bigr)
\]
\vspace{0em}
3. When $m_2 = m_3  k^{n_3}$:
\[
    D_{\mathrm{KL}}\!\Bigl(
      P_{\text{eval}}^{\text{Q-CoT}}
      \,\big\|\,
      P_{\text{train}}^{\text{Q-CoT}}
   \Bigr) = 0
\]
\end{theorem}

\begin{remark}[Relationship with Training Size]
The relationship between distribution shift and training size $M$ is characterized by several interconnected aspects:

First, $P_{\text{cover}}(M)$ exhibits monotonic growth with training size $M$, reflecting increased coverage of prefix patterns in the training data. This directly influences the distribution shift reduction.

Second, the term $D_{\mathrm{KL}}\!\Bigl(P_{\text{eval}}^{\text{Q-A}}\,\big\|\,P_{\text{train}}^{\text{Q-A}}\Bigr)$ represents a fundamental structural difference between Q-A and Q-CoT approaches. This term remains constant regardless of training size $M$ and equals the Q-A distribution shift by construction.

The theorem implies that Q-CoT's out-of-distribution (OOD) performance improves primarily through enhanced prefix coverage as training size increases. However, in practical scenarios, missing intermediate steps in the training data's chain-of-thought sequences can significantly impact performance:
\begin{itemize}
\item Incomplete chains reduce $P_{\text{cover}}(M)$ due to corrupted prefix structures
\item This reduction leads to a higher upper bound on the KL divergence $D_{\mathrm{KL}}\Bigl(P_{\text{eval}}^{\text{Q-CoT}}\,\big\|\,P_{\text{train}}^{\text{Q-CoT}}\Bigr)$
\item Consequently, OOD performance with incomplete chains underperforms relative to complete Q-CoT settings
\end{itemize}

This analysis demonstrates that Chain-of-Thought's intermediate reasoning steps effectively mitigate distribution shift between training and evaluation sets through improved prefix coverage, leading to enhanced out-of-distribution performance compared to traditional Q-A approaches.
\end{remark}
























% Interesting Thing:

% \subsection{Problem Setting}
% To investigate the generalization ability of transformer-based language models, we establish a training and evaluation framework that spans different complexity levels. We train the model on problems of length $n_1$ and $n_2$ while evaluating on problems of length $n_3$. Specifically, let $X^{n} = (s_1^{n}, s_2^{n}, ..., s_n^{n})$ represent the input question text of compound problem CP(n) and $Y^{n} = (y_1^{n}, y_2^{n}, ..., y_n^{n})$ represent the corresponding output answer text. A transformer-based language model is trained on data mappings $X^{n_{1}} \rightarrow Y^{n_{1}}$ and $X^{n_{2}} \rightarrow Y^{n_{2}}$, and evaluated on $X^{n_{3}} \rightarrow Y^{n_{3}}$, where $n_1 < n_3 < n_2$.

% \subsection{Shortcuts via Transformer}
% For a compound task with training examples of two different input token lengths $n_{1}$ and $n_{2}$, transformers may learn to exploit input length as a shortcut for classification.  Specifically, a transformer could learn a function $F(\text{input}) = \text{pattern type}$ using a shallow network (e.g., one layer) by primarily relying on the input length $n$ to identify the pattern type.

% A trivial shortcut solution for a transformer is to memorize question-answer pairs from the training set for both problem types $CP(n_{1})$ and $CP(n_{2})$. By classifying the input pattern type based on its length ($n_1$ or $n_2$), the transformer could potentially output the correct answer with a relatively shallow network (e.g., using $O(n_2)$ layers and $O(n_2)$ attention heads), effectively bypassing the need for deeper reasoning. This shortcut relies on the assumption that input length is a strong indicator of the desired output pattern in the training data.


% \subsection{Recap Condition}

% \begin{theorem}[Outside Window Recap Condition]
% \label{thm:Recap 1}
% Consider an attention mechanism with RoPE where:
% \begin{itemize}
%     \item Queries and keys are structured as:
%     \[
%     \mathbf{q}_i = \text{sbin}_s(m) \parallel \text{sbin}_s(i) \parallel \mathbf{1}_s,\quad
%     \]
%     \[\mathbf{k}_j = \text{sbin}_s(n) \parallel B_s \cdot (\text{sbin}_s(j) \parallel (-\mathbf{1}_s))
%     \]
%     \item $B_s = 2^s - 2^{-s}$ amplifies positional terms
%     \item Inner products are computed with $s$-bit precision: $\langle \cdot, \cdot \rangle_s$
% \end{itemize}

% For hard attention and relative position $d = |i - j| > \tau$, the attention score satisfies:
% \[
% \langle \text{Rot}(\mathbf{q}_i, i), \text{Rot}(\mathbf{k}_j, j) \rangle_s < \max_{|i'-j'| \leq \tau} \langle \text{Rot}(\mathbf{q}_{i'}, i'), \text{Rot}(\mathbf{k}_{j'}, j') \rangle_s
% \]
% Thus, the mechanism attends incorrectly within $[i-\tau, i+\tau]$ when $|i-j| > \tau$.
% \end{theorem}

% \begin{proof}
% We analyze the rotated inner product structure through these steps:

% \emph{Step 1: Decompose Query and Key Vectors}\\
% Let $\mathbf{q}_i = [\mathbf{c}_m \parallel \mathbf{p}_i \parallel \mathbf{1}_s]$ and $\mathbf{k}_j = [\mathbf{c}_n \parallel B_s\mathbf{p}_j \parallel -B_s\mathbf{1}_s]$ where:
% \begin{itemize}
%     \item $\mathbf{c}_m = \text{sbin}_s(m)$, $\mathbf{c}_n = \text{sbin}_s(n)$ (content embeddings)
%     \item $\mathbf{p}_i = \text{sbin}_s(i)$, $\mathbf{p}_j = \text{sbin}_s(j)$ (position embeddings)
% \end{itemize}

% \emph{Step 2: Apply Rotary Position Encoding}\\
% For each dimension pair $(2t-1, 2t)$ with frequency $\omega_t$, apply rotation matrix:
% \[
% \mathbf{R}_t(d) = \begin{bmatrix}
% \cos(d\omega_t) & -\sin(d\omega_t) \\
% \sin(d\omega_t) & \cos(d\omega_t)
% \end{bmatrix}, \quad d = i - j
% \]
% The full rotation operation gives:
% \[
% \langle \text{Rot}(\mathbf{q}_i, i), \text{Rot}(\mathbf{k}_j, j) \rangle = \sum_{t=1}^s \left[\mathbf{R}_t(d)\begin{pmatrix}c_{m,t} \\ p_{i,t}\end{pmatrix}\right]^\top \begin{pmatrix}c_{n,t} \\ B_s p_{j,t}\end{pmatrix}\] \[+ \mathbf{1}_s^\top(-B_s\mathbf{1}_s)
% \]

% \emph{Step 3: Expand Inner Product Terms}\\
% The inner product expands to three components:
% \[
% \underbrace{\sum_{t=1}^s \mathbf{c}_m^\top \mathbf{R}_t(-d) \mathbf{c}_n}_{\text{Content-Content }(A)} + \underbrace{B_s \sum_{t=1}^s \mathbf{p}_i^\top \mathbf{R}_t(-d) \mathbf{p}_j}_{\text{Position-Position }(B)} \underbrace{- sB_s}_{\text{Bias }(C)}
% \]

% \emph{Step 4: Bound Each Component}\\

% \textbf{Content-Content Term (A):}
% \[
% |A| \leq \sum_{t=1}^s |\cos(d\omega_t)c_{m,t}c_{n,t} + \sin(d\omega_t)(c_{m,t}p_{j,t} - p_{i,t}c_{n,t})| \leq s(1 + 2^{-s})
% \]
% For $d > \tau$, phase terms $\cos(d\omega_t)$, $\sin(d\omega_t)$ become incoherent, causing cancellation.

% \textbf{Position-Position Term (B):}
% \[
% B = B_s \sum_{t=1}^s [p_{i,t}p_{j,t}\cos(d\omega_t) + (p_{i,t}^2 - p_{j,t}^2)\sin(d\omega_t)]
% \]
% Given $p_{i,t}, p_{j,t} \in \{-1,1\}$, maximum constructive alignment occurs at $d=0$:
% \[
% B_{\text{max}} = B_s \sum_{t=1}^s 1 = sB_s
% \]
% For $d > \tau$, terms oscillate destructively. Let $\omega_{\min} = \min_t \omega_t$:
% \[
% |B| \leq sB_s|\cos(d\omega_{\min})| \quad \text{(Worst-case bound)}
% \]

% \textbf{Bias Term (C):}
% Constant negative offset:
% \[
% C = -sB_s
% \]

% \emph{Step 5: Analyze Score for Different $d$}\\

% \textbf{Case 1: Target within window ($|d| \leq \tau$)}\\
% Position-term alignment preserves constructive interference:
% \[
% \text{Score}_{\text{in}} \geq (s(1 + 2^{-s}) + sB_s\cos(\tau\omega_{\min})) - sB_s
% \]
% \[
% = s(1 + 2^{-s}) + sB_s(\cos(\tau\omega_{\min}) - 1)
% \]

% \textbf{Case 2: Target outside window ($|d| > \tau$)}\\
% Destructive interference dominates:
% \[
% \text{Score}_{\text{out}} \leq s(1 + 2^{-s}) + sB_s|\cos(d\omega_{\min})| - sB_s
% \]
% \[
% = s(1 + 2^{-s}) - sB_s(1 - |\cos(d\omega_{\min})|)
% \]

% \emph{Step 6: Establish Threshold $\tau$}\\
% There exists $\tau$ where for $|d| > \tau$:
% \[
% sB_s(1 - |\cos(\tau\omega_{\min})|) > s(1 + 2^{-s})
% \]
% \[
% \Rightarrow 1 - |\cos(\tau\omega_{\min})| > \frac{1 + 2^{-s}}{B_s}
% \]
% Given $B_s = 2^s - 2^{-s} \approx 2^s$, the RHS becomes negligible for $s \geq 8$. Thus:
% \[
% \tau = \left\lceil \frac{\arccos(1 - \epsilon)}{\omega_{\min}} \right\rceil, \quad \epsilon > \frac{1 + 2^{-s}}{B_s}
% \]

% \emph{Step 7: Hard Attention Behavior}\\
% For $|d| > \tau$, $\text{Score}_{\text{out}} < \text{Score}_{\text{in}}$ because:
% \[
% -sB_s(1 - |\cos d\omega_{\min}|) \ll sB_s(\cos \tau\omega_{\min} - 1)
% \]
% Hence, maximum attention score occurs within the window $[i-\tau, i+\tau]$, proving incorrect attention to distant targets.

% \end{proof}

% \begin{theorem}[Inside window recap condition]
% \label{thm: Recap 2}
% \textbf{Assume:} The true function is $s_2 = {w^{\circ}}^{\top} e(s_1) + \epsilon$, with $s_2 \perp s_x$ in expectation.
% \textbf{Consider two linear models:}
% \begin{itemize}
% \item $M_s$: $y_s = {w_s}^{\top} e(s_1)$
% \item $M_l$: $y_l = {w_1}^{\top} e(s_1) + {w_2}^{\top} e(s_x)$
% \end{itemize}
% \textbf{Under mean-squared-error training:}
% \begin{itemize}
% \item $g_s = \frac{\partial L_s}{\partial w_s}$
% \item $(g_1, g_2) = \left(\frac{\partial L_l}{\partial w_1}, \frac{\partial L_l}{\partial w_2}\right)$
% \end{itemize}
% \textbf{Then,} in finite-data regimes or with random noise, the gradient component of $g_1$ along $(w^{\circ} - w_1)$ is typically smaller than the corresponding component of $g_s$ along $(w^{\circ} - w_s)$
% Formally,
% \[
% \mathbb{E}\left[ \left\langle \mathbf{g}_1, \mathbf{w}^\circ - \mathbf{w}_1 \right\rangle \right] < \mathbb{E}\left[ \left\langle \mathbf{g}_s, \mathbf{w}^\circ - \mathbf{w}_s \right\rangle \right],
% \]
% leading to slower convergence for \( \mathcal{M}_{l} \) when \( s_{\x} \) is irrelevant. This theorem suggests that models attempting to incorporate potentially irrelevant shortcut features ($s_x$) might experience slower and less efficient learning compared to models focused on relevant features ($s_1$).
% \end{theorem}

% Combined with two types of recap condition, we'd better to:
% For each inference in compound task, make all dependent token in causal order and compact located.


% \subsection{Chain of Thought to Mitigate Shortcuts}

% \begin{definition}[Chain of Thought (CoT) for Compound Problems]
% For a compound problem instance represented by an input sequence $S = (s_1, \dots, s_N)$, a Chain of Thought solution is a structured sequence of intermediate reasoning steps, explicitly represented in text tokens, that leads to the final answer.  A CoT sequence can be viewed as a decomposition of the problem into sub-problems, with each step representing progress towards the solution.  For instance, a CoT could take the form:

% $$ \text{Input:} \quad s_1 \, | \, \cdots \, | \, s_N $$
% $$ \text{CoT Steps:} \quad <\text{step}> \, (\text{Step 1 Question}) \, | \, (\text{Step 1 Answer}) \, | \, <\text{step}> \, (\text{Step 2 Question}) \, | \, (\text{Step 2 Answer}) \, | \, \cdots $$
% $$ \text{Final Answer:} \quad <\text{answer}> \, A $$

% where `<step>`, `<answer>` are special separator tokens, and $(\text{Step j Question})$ and $(\text{Step j Answer})$ are token sequences representing the sub-problem and its solution at each step.  The goal of CoT is to force the model to explicitly perform and output intermediate reasoning, rather than directly mapping input to output via potential shortcuts.
% \end{definition}


% \begin{theorem}[Potential of CoT to Reduce Shortcut Learning (Hypothesis)]
% \label{thm:CoT}
% For compound problems (CP(n) as defined in Problem Setting), we hypothesize that training Transformer models to generate Chain of Thought solutions can mitigate the learning of undesirable shortcut strategies, such as those based solely on input length.  By requiring the model to produce explicit reasoning steps, CoT encourages the model to learn more robust and generalizable problem-solving strategies that are less reliant on superficial correlations present in the training data.  Whether specific Transformer architectures and training methodologies are necessary to effectively realize this potential is a subject of further investigation.
% \end{theorem}

% \subsection{Quantitative Analysis of CoT Accuracy Degradation}
% \begin{theorem}[Simplified Model of CoT Accuracy Degradation - Illustrative Example]
% \label{thm:cot_accuracy_drop}
% To illustrate the potential impact of errors in individual Chain of Thought steps on the final answer accuracy, we present a simplified probabilistic model.  Let $s_{\text{input}}$ be the input text, $s_{\text{ans}}$ be the unique correct answer, and $s_1, \dots, s_k$ be the \textit{exact required sequence} of perfect Chain-of-Thought (CoT) tokens. We make the following highly simplifying assumptions:
%     \begin{enumerate}
%         \item \textbf{Completeness}: $P(s_{\text{ans}} \mid s_1, \dots, s_k, s_{\text{input}}) = 1$ (If the correct CoT is generated, the answer is guaranteed).
%         \item \textbf{Uniqueness}: No other token sequence produces $s_{\text{ans}}$ (For simplicity, assume a unique correct CoT path for each input).
%         \item \textbf{Conditional Independence (Extremely Strong Assumption)}: We assume, for illustrative purposes, that the generation of each CoT token is conditionally independent given the input: $P(s_1, \dots, s_k \mid s_{\text{input}}) = \prod_{i=1}^k P(s_i \mid s_{\text{input}})$.  \textbf{This is a drastic oversimplification and is not representative of actual CoT models, where steps are highly dependent.}
%         \item \textbf{Uniform Error Probability (Simplified Assumption)}:  For each CoT token $s_j$, we assume a constant probability $\epsilon$ of generating it incorrectly, independently for each step.  Thus, $P(\text{incorrect } s_j \mid s_{\text{input}}) = \epsilon$ and $P(\text{correct } s_j \mid s_{\text{input}}) = 1 - \epsilon$.
%     \end{enumerate}
% Under these extremely strong and unrealistic simplifying assumptions, if $l$ out of $k$ CoT steps are performed incorrectly (leading to "lost/mishandled" tokens), and assuming each incorrect step independently leads to failure, the probability of generating the correct final answer is approximated by:
%     \[
%     P(s_{\text{ans}} \mid s_{\text{input}}) \approx (1 - \epsilon)^l
%     \]
% \textbf{Limitations and Interpretation:** This theorem is \textbf{not intended to be a realistic model of CoT accuracy}.  It is a highly simplified, illustrative example to demonstrate that even with a small per-step error probability $\epsilon$, the cumulative error over multiple CoT steps can significantly degrade the final answer accuracy, especially as the number of reasoning steps $k$ (and thus potentially $l$) increases.  The conditional independence assumption is particularly unrealistic, as errors in earlier CoT steps are likely to propagate and influence subsequent steps in a complex manner.  Therefore, this result should be interpreted with extreme caution and only as a qualitative indication of potential error propagation in multi-step reasoning, \textbf{not as a precise quantitative prediction of CoT performance.}
% \end{theorem}


% \subsection{Distribution Shift Theorem for Non-CoT vs.\ CoT}

% \subsubsection*{1. Prefix-Substructure Theorem in the Context of Compound Problems CP(n)}

% To formalize the idea that shorter compound problems are "substructures" of longer ones when using Chain-of-Thought, we introduce the concept of a Chain-of-Thought generating function.

% Assume that for a compound problem CP(n) with input $X^n$, there exists a deterministic function $G$ that generates a Chain-of-Thought sequence $Q^n = G(X^n) = (q_1^{(n)}, q_2^{(n)}, \ldots, q_{n}^{(n)})$.  We assume this function $G$ represents a "dynamic state-transition system" where each $q_i^{(n)}$ is a state in the reasoning process.

% \begin{theorem}[Prefix Substructure for Chain-of-Thought in CP(n)]
% \label{thm:prefix-substructure}
% Consider two compound problems $CP(n_2)$ and $CP(n_3)$ with input question texts $X^{n_2} = (s_1, s_2, \ldots, s_{n_2})$ and $X^{n_3} = (s_1, s_2, \ldots, s_{n_3})$, where $X^{n_3}$ is a prefix of $X^{n_2}$ (meaning the first $n_3$ tokens are identical), and $n_3 < n_2$.  Let their Chain-of-Thought sequences generated by the function $G$ be:
% \[
%    Q^{(2)} \;=\; G(X^{n_2}) \;=\;
%    \bigl(q_1^{(2)}, q_2^{(2)}, \ldots, q_{n_2}^{(2)}\bigr)
%    \quad\text{and}
% \]
% \[\quad
%    Q^{(3)} \;=\; G(X^{n_3}) \;=\;
%    \bigl(q_1^{(3)}, q_2^{(3)}, \ldots, q_{n_3}^{(3)}\bigr).
% \]
% If the Chain-of-Thought generation process $G$ is such that the reasoning steps for the common prefix input are identical, then for all $1 \le i \le n_3$, we have:
% \[
%    q_i^{(3)}
%    \;=\;
%    q_i^{(2)}.
% \]
% Therefore, the Chain-of-Thought sequence for $CP(n_3)$, $Q^{(3)}$, is a prefix of the Chain-of-Thought sequence for $CP(n_2)$, $Q^{(2)}$.
% \end{theorem}

% \noindent\textbf{Assumption for Prefix Substructure:** This theorem relies on the assumption that the Chain-of-Thought generation function $G$ exhibits a prefix property: if the input is a prefix, the initial steps of the generated CoT are also a prefix of the CoT for the longer input.  This is a reasonable assumption for many compositional reasoning processes, where initial reasoning steps are determined by the initial part of the input problem.


% \subsubsection*{2. Distribution Shift Theorem for Non-CoT vs.\ CoT in CP(n) Generalization}

% \begin{theorem}[Strictly Smaller Distribution Shift Under CoT for CP(n)]
% \label{thm:cot-shift}
% Let $n_1, n_2, n_3$ be integers with $n_1 < n_3 < n_2$.  Suppose we have training datasets for compound problems CP($n_1$) and CP($n_2$), and an evaluation dataset for CP($n_3$).

% In the \textbf{non-CoT} setting, we have training dataset $\mathcal{D}_{\text{train}}^{(\text{non-CoT})} = \Bigl\{ (X^{n_1}, Y^{n_1}),\, (X^{n_2}, Y^{n_2}) \Bigr\}$ and evaluation dataset $\mathcal{D}_{\text{eval}}^{(\text{non-CoT})} = \Bigl\{(X^{n_3}, Y^{n_3})\Bigr\}$. Let $P_{\text{train}}^{(\text{non-CoT})}$ and $P_{\text{eval}}^{(\text{non-CoT})}$ be the empirical distributions derived from these datasets over (input, output) pairs.  The distribution shift is measured by $D_{\mathrm{KL}}\!\Bigl( P_{\text{eval}}^{(\text{non-CoT})} \,\big\|\, P_{\text{train}}^{(\text{non-CoT})} \Bigr)$.

% In the \textbf{CoT} setting, we have training dataset $\mathcal{D}_{\text{train}}^{(\text{CoT})} = \Bigl\{ \bigl(X^{n_1}, Q^{n_1}, Y^{n_1}\bigr),\, \bigl(X^{n_2}, Q^{n_2}, Y^{n_2}\bigr) \Bigr\}$ (where $Q^{n}$ is the CoT for $X^n$) and evaluation dataset $\mathcal{D}_{\text{eval}}^{(\text{CoT})} = \Bigl\{\bigl(X^{n_3}, Q^{n_3}, Y^{n_3}\bigr)\Bigr\}$. Let $P_{\text{train}}^{(\text{CoT})}$ and $P_{\text{eval}}^{(\text{CoT})}$ be the empirical distributions derived from these datasets over (input, CoT, output) tuples. The distribution shift is measured by $D_{\mathrm{KL}}\!\Bigl( P_{\text{eval}}^{(\text{CoT})} \,\big\|\, P_{\text{train}}^{(\text{CoT})} \Bigr)$.

% Then, assuming the Prefix Substructure Theorem (Theorem~\ref{thm:prefix-substructure}) holds for the Chain-of-Thought generation process for CP(n), we have:
% \[
%    D_{\mathrm{KL}}\!\Bigl(
%       P_{\text{eval}}^{(\text{CoT})}
%       \,\big\|\,
%       P_{\text{train}}^{(\text{CoT})}
%    \Bigr)
%    \;<\;
%    D_{\mathrm{KL}}\!\Bigl(
%       P_{\text{eval}}^{(\text{non-CoT})}
%       \,\big\|\,
%       P_{\text{train}}^{(\text{non-CoT})}
%    \Bigr).
% \]
% In other words, the distribution shift at problem length $n_3$ is strictly smaller when using Chain-of-Thought training compared to non-CoT training.

% \noindent\textbf{Rationale for Reduced Shift with CoT:}  Non-CoT training only exposes the model to full problems of lengths $n_1$ and $n_2$.  When evaluating on length $n_3$, the model is seeing problems of a length not directly present in the training data.  However, with CoT training, the model is exposed to the Chain-of-Thought sequences for both $n_1$ and $n_2$.  Due to the Prefix Substructure Theorem, the CoT sequences for $n_2$ problems contain, as prefixes, the complete CoT sequences for problems of length $n_3$ (when the input prefixes align).  Therefore, CoT training effectively includes examples of $n_3$-length reasoning within the training data derived from $n_2$-length problems, thus reducing the distribution shift when evaluating on $n_3$. This inclusion of intermediate length reasoning steps in the training distribution is what reduces the KL divergence.
% \end{theorem}



% Interesting Thing:

% \subsection{Problem Setting}
% To investigate the generalization ability of transformer-based language models, we establish a training and evaluation framework that spans different complexity levels. We train the model on problems of length $n_1$ and $n_2$ while evaluating on problems of length $n_3$. Specifically, let $X^{n} = (s_1^{n}, s_2^{n}, ..., s_n^{n})$ represent the input question text of compound problem CP(n) and $Y^{n} = (y_1^{n}, y_2^{n}, ..., y_n^{n})$ represent the corresponding output answer text. A transformer-based language model is trained on data mappings $X^{n_{1}} \rightarrow Y^{n_{1}}$ and $X^{n_{2}} \rightarrow Y^{n_{2}}$, and evaluated on $X^{n_{3}} \rightarrow Y^{n_{3}}$, where $n_1 < n_3 < n_2$.

% \subsection{Shortcuts via Transformer}
% For a compound task with training examples of two different input token lengths $n_{1}$ and $n_{2}$, transformers may learn to exploit input length as a shortcut for classification.  Specifically, a transformer could learn a function $F(\text{input}) = \text{pattern type}$ using a shallow network (e.g., one layer) by primarily relying on the input length $n$ to identify the pattern type.

% A trivial shortcut solution for a transformer is to memorize question-answer pairs from the training set for both problem types $CP(n_{1})$ and $CP(n_{2})$. By classifying the input pattern type based on its length ($n_1$ or $n_2$), the transformer could potentially output the correct answer with a relatively shallow network (e.g., using $O(n_2)$ layers and $O(n_2)$ attention heads), effectively bypassing the need for deeper reasoning. This shortcut relies on the assumption that input length is a strong indicator of the desired output pattern in the training data.


% \subsection{Recap Condition}

% \begin{theorem}[Outside window recap condition] \label{thm:Recap 1}
% Consider an attention mechanism with RoPE (Rotary Position Embedding). Let $S_{query}$ denote the current input token at position $i_{query}$, and $S_{target}$ denote the previous input / output token at position $i_{target}$. If the absolute positional difference $|i_{target} - i_{query}|$ exceeds a given threshold $\tau$, then the attention mechanism will attend more strongly to tokens within the window $[i_{query} - \tau, i_{query} + \tau]$ rather than the intended target token.
% \end{theorem}
% \input{proof_outside_recap}

% \begin{theorem}[Inside window recap condition]
% \label{thm: Recap 2}
% \textbf{Assume:} The true function is $s_2 = {w^{\circ}}^{\top} e(s_1) + \epsilon$, with $s_2 \perp s_x$ in expectation.
% \textbf{Consider two linear models:}
% \begin{itemize}
% \item $M_s$: $y_s = {w_s}^{\top} e(s_1)$
% \item $M_l$: $y_l = {w_1}^{\top} e(s_1) + {w_2}^{\top} e(s_x)$
% \end{itemize}
% \textbf{Under mean-squared-error training:}
% \begin{itemize}
% \item $g_s = \frac{\partial L_s}{\partial w_s}$
% \item $(g_1, g_2) = \left(\frac{\partial L_l}{\partial w_1}, \frac{\partial L_l}{\partial w_2}\right)$
% \end{itemize}
% \textbf{Then,} in finite-data regimes or with random noise, the gradient component of $g_1$ along $(w^{\circ} - w_1)$ is typically smaller than the corresponding component of $g_s$ along $(w^{\circ} - w_s)$
% Formally,
% \[
% \mathbb{E}\left[ \left\langle \mathbf{g}_1, \mathbf{w}^\circ - \mathbf{w}_1 \right\rangle \right] < \mathbb{E}\left[ \left\langle \mathbf{g}_s, \mathbf{w}^\circ - \mathbf{w}_s \right\rangle \right],
% \]
% leading to slower convergence for \( \mathcal{M}_{l} \) when \( s_{\x} \) is irrelevant. This theorem suggests that models attempting to incorporate potentially irrelevant shortcut features ($s_x$) might experience slower and less efficient learning compared to models focused on relevant features ($s_1$).
% \end{theorem}

% Combined with two types of recap condition, we'd better to:
% For each inference in compound task, make all dependent token in causal order and compact located.


% \subsection{Chain of Thought to Mitigate Shortcuts}

% \begin{definition}[Chain of Thought (CoT) for Compound Problems]
% For a compound problem instance represented by an input sequence $S = (s_1, \dots, s_N)$, a Chain of Thought solution is a structured sequence of intermediate reasoning steps, explicitly represented in text tokens, that leads to the final answer.  A CoT sequence can be viewed as a decomposition of the problem into sub-problems, with each step representing progress towards the solution.  For instance, a CoT could take the form:

% $$ \text{Input:} \quad s_1 \, | \, \cdots \, | \, s_N $$
% $$ \text{CoT Steps:} \quad <\text{step}> \, (\text{Step 1 Question}) \, | \, (\text{Step 1 Answer}) \, | \, <\text{step}> \, (\text{Step 2 Question}) \, | \, (\text{Step 2 Answer}) \, | \, \cdots $$
% $$ \text{Final Answer:} \quad <\text{answer}> \, A $$

% where `<step>`, `<answer>` are special separator tokens, and $(\text{Step j Question})$ and $(\text{Step j Answer})$ are token sequences representing the sub-problem and its solution at each step.  The goal of CoT is to force the model to explicitly perform and output intermediate reasoning, rather than directly mapping input to output via potential shortcuts.
% \end{definition}


% \begin{theorem}[Potential of CoT to Reduce Shortcut Learning (Hypothesis)]
% \label{thm:CoT}
% For compound problems (CP(n) as defined in Problem Setting), we hypothesize that training Transformer models to generate Chain of Thought solutions can mitigate the learning of undesirable shortcut strategies, such as those based solely on input length.  By requiring the model to produce explicit reasoning steps, CoT encourages the model to learn more robust and generalizable problem-solving strategies that are less reliant on superficial correlations present in the training data.  Whether specific Transformer architectures and training methodologies are necessary to effectively realize this potential is a subject of further investigation.
% \end{theorem}

% \subsection{Quantitative Analysis of CoT Accuracy Degradation}
% \begin{theorem}[Simplified Model of CoT Accuracy Degradation - Illustrative Example]
% \label{thm:cot_accuracy_drop}
% To illustrate the potential impact of errors in individual Chain of Thought steps on the final answer accuracy, we present a simplified probabilistic model.  Let $s_{\text{input}}$ be the input text, $s_{\text{ans}}$ be the unique correct answer, and $s_1, \dots, s_k$ be the \textit{exact required sequence} of perfect Chain-of-Thought (CoT) tokens. We make the following highly simplifying assumptions:
%     \begin{enumerate}
%         \item \textbf{Completeness}: $P(s_{\text{ans}} \mid s_1, \dots, s_k, s_{\text{input}}) = 1$ (If the correct CoT is generated, the answer is guaranteed).
%         \item \textbf{Uniqueness}: No other token sequence produces $s_{\text{ans}}$ (For simplicity, assume a unique correct CoT path for each input).
%         \item \textbf{Conditional Independence (Extremely Strong Assumption)}: We assume, for illustrative purposes, that the generation of each CoT token is conditionally independent given the input: $P(s_1, \dots, s_k \mid s_{\text{input}}) = \prod_{i=1}^k P(s_i \mid s_{\text{input}})$.  \textbf{This is a drastic oversimplification and is not representative of actual CoT models, where steps are highly dependent.}
%         \item \textbf{Uniform Error Probability (Simplified Assumption)}:  For each CoT token $s_j$, we assume a constant probability $\epsilon$ of generating it incorrectly, independently for each step.  Thus, $P(\text{incorrect } s_j \mid s_{\text{input}}) = \epsilon$ and $P(\text{correct } s_j \mid s_{\text{input}}) = 1 - \epsilon$.
%     \end{enumerate}
% Under these extremely strong and unrealistic simplifying assumptions, if $l$ out of $k$ CoT steps are performed incorrectly (leading to "lost/mishandled" tokens), and assuming each incorrect step independently leads to failure, the probability of generating the correct final answer is approximated by:
%     \[
%     P(s_{\text{ans}} \mid s_{\text{input}}) \approx (1 - \epsilon)^l
%     \]
% \textbf{Limitations and Interpretation:** This theorem is \textbf{not intended to be a realistic model of CoT accuracy}.  It is a highly simplified, illustrative example to demonstrate that even with a small per-step error probability $\epsilon$, the cumulative error over multiple CoT steps can significantly degrade the final answer accuracy, especially as the number of reasoning steps $k$ (and thus potentially $l$) increases.  The conditional independence assumption is particularly unrealistic, as errors in earlier CoT steps are likely to propagate and influence subsequent steps in a complex manner.  Therefore, this result should be interpreted with extreme caution and only as a qualitative indication of potential error propagation in multi-step reasoning, \textbf{not as a precise quantitative prediction of CoT performance.}
% \end{theorem}


% \subsection{Distribution Shift Theorem for Non-CoT vs.\ CoT}

% \subsubsection*{1. Prefix-Substructure Theorem in the Context of Compound Problems CP(n)}

% To formalize the idea that shorter compound problems are "substructures" of longer ones when using Chain-of-Thought, we introduce the concept of a Chain-of-Thought generating function.

% Assume that for a compound problem CP(n) with input $X^n$, there exists a deterministic function $G$ that generates a Chain-of-Thought sequence $Q^n = G(X^n) = (q_1^{(n)}, q_2^{(n)}, \ldots, q_{n}^{(n)})$.  We assume this function $G$ represents a "dynamic state-transition system" where each $q_i^{(n)}$ is a state in the reasoning process.

% \begin{theorem}[Prefix Substructure for Chain-of-Thought in CP(n)]
% \label{thm:prefix-substructure}
% Consider two compound problems $CP(n_2)$ and $CP(n_3)$ with input question texts $X^{n_2} = (s_1, s_2, \ldots, s_{n_2})$ and $X^{n_3} = (s_1, s_2, \ldots, s_{n_3})$, where $X^{n_3}$ is a prefix of $X^{n_2}$ (meaning the first $n_3$ tokens are identical), and $n_3 < n_2$.  Let their Chain-of-Thought sequences generated by the function $G$ be:
% \[
%    Q^{(2)} \;=\; G(X^{n_2}) \;=\;
%    \bigl(q_1^{(2)}, q_2^{(2)}, \ldots, q_{n_2}^{(2)}\bigr)
%    \quad\text{and}
% \]
% \[\quad
%    Q^{(3)} \;=\; G(X^{n_3}) \;=\;
%    \bigl(q_1^{(3)}, q_2^{(3)}, \ldots, q_{n_3}^{(3)}\bigr).
% \]
% If the Chain-of-Thought generation process $G$ is such that the reasoning steps for the common prefix input are identical, then for all $1 \le i \le n_3$, we have:
% \[
%    q_i^{(3)}
%    \;=\;
%    q_i^{(2)}.
% \]
% Therefore, the Chain-of-Thought sequence for $CP(n_3)$, $Q^{(3)}$, is a prefix of the Chain-of-Thought sequence for $CP(n_2)$, $Q^{(2)}$.
% \end{theorem}

% \noindent\textbf{Assumption for Prefix Substructure:** This theorem relies on the assumption that the Chain-of-Thought generation function $G$ exhibits a prefix property: if the input is a prefix, the initial steps of the generated CoT are also a prefix of the CoT for the longer input.  This is a reasonable assumption for many compositional reasoning processes, where initial reasoning steps are determined by the initial part of the input problem.


% \subsubsection*{2. Distribution Shift Theorem for Non-CoT vs.\ CoT in CP(n) Generalization}

% \begin{theorem}[Strictly Smaller Distribution Shift Under CoT for CP(n)]
% \label{thm:cot-shift}
% Let $n_1, n_2, n_3$ be integers with $n_1 < n_3 < n_2$.  Suppose we have training datasets for compound problems CP($n_1$) and CP($n_2$), and an evaluation dataset for CP($n_3$).

% In the \textbf{non-CoT} setting, we have training dataset $\mathcal{D}_{\text{train}}^{(\text{non-CoT})} = \Bigl\{ (X^{n_1}, Y^{n_1}),\, (X^{n_2}, Y^{n_2}) \Bigr\}$ and evaluation dataset $\mathcal{D}_{\text{eval}}^{(\text{non-CoT})} = \Bigl\{(X^{n_3}, Y^{n_3})\Bigr\}$. Let $P_{\text{train}}^{(\text{non-CoT})}$ and $P_{\text{eval}}^{(\text{non-CoT})}$ be the empirical distributions derived from these datasets over (input, output) pairs.  The distribution shift is measured by $D_{\mathrm{KL}}\!\Bigl( P_{\text{eval}}^{(\text{non-CoT})} \,\big\|\, P_{\text{train}}^{(\text{non-CoT})} \Bigr)$.

% In the \textbf{CoT} setting, we have training dataset $\mathcal{D}_{\text{train}}^{(\text{CoT})} = \Bigl\{ \bigl(X^{n_1}, Q^{n_1}, Y^{n_1}\bigr),\, \bigl(X^{n_2}, Q^{n_2}, Y^{n_2}\bigr) \Bigr\}$ (where $Q^{n}$ is the CoT for $X^n$) and evaluation dataset $\mathcal{D}_{\text{eval}}^{(\text{CoT})} = \Bigl\{\bigl(X^{n_3}, Q^{n_3}, Y^{n_3}\bigr)\Bigr\}$. Let $P_{\text{train}}^{(\text{CoT})}$ and $P_{\text{eval}}^{(\text{CoT})}$ be the empirical distributions derived from these datasets over (input, CoT, output) tuples. The distribution shift is measured by $D_{\mathrm{KL}}\!\Bigl( P_{\text{eval}}^{(\text{CoT})} \,\big\|\, P_{\text{train}}^{(\text{CoT})} \Bigr)$.

% Then, assuming the Prefix Substructure Theorem (Theorem~\ref{thm:prefix-substructure}) holds for the Chain-of-Thought generation process for CP(n), we have:
% \[
%    D_{\mathrm{KL}}\!\Bigl(
%       P_{\text{eval}}^{(\text{CoT})}
%       \,\big\|\,
%       P_{\text{train}}^{(\text{CoT})}
%    \Bigr)
%    \;<\;
%    D_{\mathrm{KL}}\!\Bigl(
%       P_{\text{eval}}^{(\text{non-CoT})}
%       \,\big\|\,
%       P_{\text{train}}^{(\text{non-CoT})}
%    \Bigr).
% \]
% In other words, the distribution shift at problem length $n_3$ is strictly smaller when using Chain-of-Thought training compared to non-CoT training.

% \noindent\textbf{Rationale for Reduced Shift with CoT:}  Non-CoT training only exposes the model to full problems of lengths $n_1$ and $n_2$.  When evaluating on length $n_3$, the model is seeing problems of a length not directly present in the training data.  However, with CoT training, the model is exposed to the Chain-of-Thought sequences for both $n_1$ and $n_2$.  Due to the Prefix Substructure Theorem, the CoT sequences for $n_2$ problems contain, as prefixes, the complete CoT sequences for problems of length $n_3$ (when the input prefixes align).  Therefore, CoT training effectively includes examples of $n_3$-length reasoning within the training data derived from $n_2$-length problems, thus reducing the distribution shift when evaluating on $n_3$. This inclusion of intermediate length reasoning steps in the training distribution is what reduces the KL divergence.
% \end{theorem}
