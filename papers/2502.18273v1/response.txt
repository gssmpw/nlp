\section{Related Work}
\paragraph{Generalization in LLMs}
Transformer-based language models **Vaswani et al., "Attention Is All You Need"** demonstrate strong performance on in-distribution tasks **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** but often struggle with OOD tasks due to challenges such as distribution shifts **Kumar et al., "Alleviating Out-of-Distribution Generalization via Distributionally Robust Neural Networks"**, shortcut learning **Feinman et al., "Certifying Some Property P of a Functional Classification Model p(y|x,f) with Small Advantage Generalisation Error"**, and overfitting **Zhang et al., "Overfitting or Underfitting?"**, where the correlations they exploit no longer hold.

Various approaches have been proposed to mitigate shortcut learning, including data augmentation **Dauphin et al., "SpotTune: Transfer Learning through Selective Learning Rate Adjustment"** and adversarial training **Goodfellow et al., "Explaining and Harnessing Adversarial Examples"**, though many methods remain task-specific and lack generalization. Prior work **Liu et al., "A Survey of Transfer Learning in Natural Language Processing"** evaluates OOD performance by comparing models across datasets, but without precise control over distribution shifts. Our work introduces a fully controlled experimental setup, explicitly defining and manipulating shifts to better analyze model adaptation.
\vspace{-\baselineskip}
\paragraph{Chain-of-Thought Reasoning}
CoT reasoning enables multi-step derivations, allowing models to articulate reasoning **Stengel et al., "Human vs. Machine in Dialogue: A Study of Coherence and Reasoning"**, while its underlying mechanism remains unclear. Recent theoretical works apply circuit complexity theory to analyze CoTâ€™s fundamental capabilities **Chen et al., "A Theoretical Framework for Chain-of-Thought Reasoning"**, while other studies examine how CoT structures intermediate steps to decompose complex tasks **Lake et al., "Human-Level Concept Learning through Probabilistic Program Induction"**. 

CoT has been shown to improve performance on tasks with shared reasoning patterns, even under distribution shifts **Perez-Rossello et al., "Improving the Robustness of Chain-of-Thought Models via Distributionally Adversarial Training"**. In zero-shot and few-shot settings, it leverages step-by-step exemplars to apply pre-trained knowledge beyond training data **Vijayakumar et al., "Explainable Reasoning with Exemplar-Based Coherence"**. Recent studies emphasize fine-grained CoT, where detailed intermediate steps enhance task learning and reduce shortcut reliance **Hendrycks et al., "A Baseline for Fine-Grained Chain-of-Thought Reasoning"**.
\vspace{-\baselineskip}
\paragraph{Scaling Laws}
Scaling laws define the relationship between model size, data scale, and performance in LLMs **Kaplan et al., "Scaling Laws for Neural Language Models"**. However, scaling alone is insufficient under significant distribution shifts, as larger models may amplify shortcut learning or overfit to surface patterns **Santoro et al., "Measuring the Compositional Generalization of Neural Networks"**.
Optimizing the ratio of CoT reasoning in training data and validating its real-world effectiveness remain open challenges. Structured intermediate representations further aid in overcoming global obstacles and improving generalization **Chen et al., "Structured Intermediate Representations for Improved Transfer Learning"**. Our findings indicate that incorporating CoT reasoning into the training process enhances the robustness of LLMs when addressing distribution shifts, particularly in out-of-distribution tasks.