\section{Related Work}
\paragraph{Generalization in LLMs}
Transformer-based language models ____ demonstrate strong performance on in-distribution tasks ____ but often struggle with OOD tasks due to challenges such as distribution shifts ____, shortcut learning ____, and overfitting ____, where the correlations they exploit no longer hold ____.

Various approaches have been proposed to mitigate shortcut learning, including data augmentation ____ and adversarial training ____, though many methods remain task-specific and lack generalization. Prior work ____ evaluates OOD performance by comparing models across datasets, but without precise control over distribution shifts. Our work introduces a fully controlled experimental setup, explicitly defining and manipulating shifts to better analyze model adaptation.
\vspace{-\baselineskip}
\paragraph{Chain-of-Thought Reasoning}
CoT reasoning enables multi-step derivations, allowing models to articulate reasoning ____, while its underlying mechanism remains unclear. Recent theoretical works apply circuit complexity theory to analyze CoTâ€™s fundamental capabilities ____, while other studies examine how CoT structures intermediate steps to decompose complex tasks ____. 

CoT has been shown to improve performance on tasks with shared reasoning patterns, even under distribution shifts ____. In zero-shot and few-shot settings, it leverages step-by-step exemplars to apply pre-trained knowledge beyond training data ____. Recent studies emphasize fine-grained CoT, where detailed intermediate steps enhance task learning and reduce shortcut reliance ____.
\vspace{-\baselineskip}
\paragraph{Scaling Laws}
Scaling laws define the relationship between model size, data scale, and performance in LLMs ____. However, scaling alone is insufficient under significant distribution shifts, as larger models may amplify shortcut learning or overfit to surface patterns ____.
Optimizing the ratio of CoT reasoning in training data and validating its real-world effectiveness remain open challenges. Structured intermediate representations further aid in overcoming global obstacles and improving generalization ____. Our findings indicate that incorporating CoT reasoning into the training process enhances the robustness of LLMs when addressing distribution shifts, particularly in out-of-distribution tasks.