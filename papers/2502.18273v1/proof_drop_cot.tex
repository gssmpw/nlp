\section{Quantitation Analysis Drop of CoT}
    \begin{theorem}[CoT Accuracy Degradation]
    Let $s_{\text{input}}$ be the input text, $s_{\text{ans}}$ be the unique correct answer, and $s_1, \dots, s_k$ be the \textit{exact required sequence} of perfect Chain-of-Thought (CoT) tokens where:
    \begin{enumerate}
        \item \textbf{Completeness}: $P(s_{\text{ans}} \mid s_1, \dots, s_k, s_{\text{input}}) = 1$
        \item \textbf{Uniqueness}: No other token sequence produces $s_{\text{ans}}$
        \item \textbf{Conditional Independence}: $P(s_1, \dots, s_k \mid s_{\text{input}}) = \prod_{i=1}^k P(s_i \mid s_{\text{input}})$
        \item \textbf{Training Deficiency}: For any CoT token $s_j$ excluded during training, $P(s_j \mid s_{\text{input}})$ drops from 1 to $1 - \epsilon$
    \end{enumerate}
    When $l < k$ CoT tokens are lost/mishandled during inference, the final answer accuracy satisfies:
    \[
    P(s_{\text{ans}} \mid s_{\text{input}}) = (1 - \epsilon)^l
    \]
    \label{thm:drop_CoT}
\end{theorem}

\input{proof_CoT_token_loss}