\section{Related Work}
\paragraph{Generalization in LLMs}
Transformer-based language models \cite{vaswani2017attention} demonstrate strong performance on in-distribution tasks \cite{minaee2024largelanguagemodelssurvey,naveed2024comprehensiveoverviewlargelanguage,Xu_2024} but often struggle with OOD tasks due to challenges such as distribution shifts \cite{anil2022exploringlengthgeneralizationlarge,zhang2022delving}, shortcut learning \cite{liu2022transformers,geirhos2020shortcut}, and overfitting \cite{li2023transformers}, where the correlations they exploit no longer hold \cite{qian2022limitationslanguagemodelsarithmetic,nogueira2021investigatinglimitationstransformerssimple}.

Various approaches have been proposed to mitigate shortcut learning, including data augmentation \cite{hendrycks2020pretrainedtransformersimproveoutofdistribution,zhang2023unveilingtransformerslegosynthetic} and adversarial training \cite{jiang2019avoiding,taori2020when}, though many methods remain task-specific and lack generalization. Prior work \cite{chen2024alphamathzeroprocesssupervision} evaluates OOD performance by comparing models across datasets, but without precise control over distribution shifts. Our work introduces a fully controlled experimental setup, explicitly defining and manipulating shifts to better analyze model adaptation.
\vspace{-\baselineskip}
\paragraph{Chain-of-Thought Reasoning}
CoT reasoning enables multi-step derivations, allowing models to articulate reasoning \cite{wei2022chain,kojima2022large,zhu2024deductive,fu2022complexity,kim2024transformers}, while its underlying mechanism remains unclear. Recent theoretical works apply circuit complexity theory to analyze CoTâ€™s fundamental capabilities \cite{li2024chainthoughtempowerstransformers,abbe2024fartransformersreasonglobality,feng2023revealingmysterychainthought}, while other studies examine how CoT structures intermediate steps to decompose complex tasks \cite{ton2024understandingchainofthoughtllmsinformation,kudo2024thinktotalktalktothinkllmscome,yu2025llmsreallythinkstepbystep,anonymous2025chainofthought}. 

CoT has been shown to improve performance on tasks with shared reasoning patterns, even under distribution shifts \cite{li2024how,hu2024unveilingstatisticalfoundationschainofthought}. In zero-shot and few-shot settings, it leverages step-by-step exemplars to apply pre-trained knowledge beyond training data \cite{kim2023cot}. Recent studies emphasize fine-grained CoT, where detailed intermediate steps enhance task learning and reduce shortcut reliance \cite{nguyen2023cof,chu-etal-2025-towards}.
\vspace{-\baselineskip}
\paragraph{Scaling Laws}
Scaling laws define the relationship between model size, data scale, and performance in LLMs \cite{kaplan2020scalinglawsneurallanguage,hoffmann2022trainingcomputeoptimallargelanguage,chowdhery2022palm}. However, scaling alone is insufficient under significant distribution shifts, as larger models may amplify shortcut learning or overfit to surface patterns \cite{micelibarone2022distributionallyrobustrecurrentdecoders}.
Optimizing the ratio of CoT reasoning in training data and validating its real-world effectiveness remain open challenges. Structured intermediate representations further aid in overcoming global obstacles and improving generalization \cite{abbe2024fartransformersreasonglobality}. Our findings indicate that incorporating CoT reasoning into the training process enhances the robustness of LLMs when addressing distribution shifts, particularly in out-of-distribution tasks.