

\section{CoT simulates the target solution}

\begin{definition}[Chain of Thought]
  \vspace{0em}
    \small\begin{align*}
    \vspace{-0.5em}
        \text{Input:} & \; s_1 \mid \cdots \mid s_N \\[1ex]
        \text{CoT Steps:} & \; \langle\text{sep}\rangle \; s_2 \mid G_2 \mid q_2 \mid L_1 \mid L_2 \\
        & \; \langle\text{sep}\rangle \; \cdots \\
        & \; \langle\text{sep}\rangle \; s_{N} \mid G_N \mid q_N \mid L_{N-1} \mid L_N
    \end{align*}
    \label{def:CoT_appendix}
\end{definition}

\begin{proposition}
\label{thm:CoT_implementation}
For any compound problem satisfying Definition \ref{def:CP}, and for any input length bound $n \in \mathbb{N}$, there exists an autoregressive Transformer with:
\begin{itemize}
\item Constant depth $L$
\item Constant hidden dimension $d$
\item Constant number of attention heads $H$
\end{itemize}
where $L$, $d$, and $H$ are independent of $n$, such that the Transformer correctly generates the Chain-of-Thought solution defined in Definition \ref{def:CoT} for all input sequences of length at most $n$. Furthermore, all parameter values in the Transformer are bounded by $O(\text{poly}(n))$.
\end{proposition}

\subsection{Constructive Proof}
We prove this theorem by constructing a Transformer architecture with 4 blocks, where each block contains multiple attention heads and feed-forward networks (FFNs). The key insight is that we can simulate each step of the Chain-of-Thought solution using a fixed number of attention heads and a fixed embedding dimension.
The attention mechanism is primarily used to select and retrieve relevant elements from the input and previous computations, while the FFNs approximate the required functions $G$, $B$, etc. By maintaining constant depth, width, and number of heads per layer, we ensure the Transformer's architecture remains independent of the input length, while still being able to generate arbitrarily long Chain-of-Thought solutions.
The parameter complexity of $O(\text{poly}(n))$ arises from the need to handle inputs and intermediate computations of length $n$, but importantly, this only affects the parameter values and not the model architecture itself.

\subsection{Embedding Structure}
For position $k$, define the input embedding:
\begin{equation*}
    x^{(0)}_k=(e^{\text{isInput}}_k, e^{\text{isState}}_k, e^{\text{isDependence}}, e^{\text{isL}}, e^{\text{q}}_k, e^{\text{d}}_{k}, e^{\text{L}}_k, e^{\text{sep}}_k, e^{\text{step}}_k, k,1)
\end{equation*}
where:
\begin{itemize}
    \item $e^{\text{isInput}}_k \in \{0,1\}$: Input token indicator
    \item $e^{\text{isState}}_k \in \{0,1\}$: State position indicator
    \item $e^{\text{isDependence}} \in \{0,1\}$: Dependency marker
    \item $e^{\text{isL}} \in \{0,1\}$: Aggregation result indicator
    \item $e^{\text{q}}_k \in \mathbb{R}^{d_q}$: State value embedding
    \item $e^{\text{d}}_{k} \in \mathbb{R}^{d_d}$: Dependency graph embedding
    \item $e^{\text{L}}_k \in \mathbb{R}^{d_L}$: Aggregation value embedding
    \item $e^{\text{sep}}_k \in \{0,1\}$: Step separator indicator
    \item $e^{\text{step}}_k \in \mathbb{N}$: Current step index
    \item $k \in \mathbb{N}$: Position encoding
    \item $1$: Bias term
\end{itemize}

\subsection{Block Constructions}

\subsubsection{Block 1: Input Processing and State Identification}
Define attention heads $A^{(1)}_1, A^{(1)}_2, A^{(1)}_3$ with parameters:
\begin{align*}
    Q^{(1)}_1 &= W^q_1[e^{\text{isInput}}_k] \\ 
    K^{(1)}_1 &= W^k_1[e^{\text{isInput}}_j]_{j<k} \\
    V^{(1)}_1 &= W^v_1[j]_{j<k}
\end{align*}

The second head tracks state positions:
\begin{align*}
    Q^{(1)}_2 &= W^q_2[e^{\text{isState}}_k] \\
    K^{(1)}_2 &= W^k_2[e^{\text{isState}}_j]_{j<k} \\
    V^{(1)}_2 &= W^v_2[j]_{j<k}
\end{align*}

The third head tracks step indices through separators:
\begin{align*}
    Q^{(1)}_3 &= W^q_3[e^{\text{sep}}_k] \\
    K^{(1)}_3 &= W^k_3[e^{\text{sep}}_j]_{j<k} \\
    V^{(1)}_3 &= W^v_3[\text{count}(e^{\text{sep}}_j)]_{j<k}
\end{align*}

\begin{lemma}
The first block correctly identifies positions through attention scoring:
\begin{enumerate}
    \item For input positions, $A^{(1)}_1$ scoring gives:
    \begin{equation*}
        \text{score}_1(q_k, k_j) = \begin{cases}
        1 & \text{if } e^{\text{isInput}}_j = 1 \\
        0 & \text{otherwise}
        \end{cases}
    \end{equation*}
    Thus $V^{(1)}_1$ returns positions of input tokens

    \item For state positions, $A^{(1)}_2$ scoring gives:
    \begin{equation*}
        \text{score}_2(q_k, k_j) = \begin{cases}
        1 & \text{if } e^{\text{isState}}_j = 1 \\
        0 & \text{otherwise}
        \end{cases}
    \end{equation*} 
    Thus $V^{(1)}_2$ returns positions of states

    \item For step indices, $A^{(1)}_3$ counts separators up to position k:
    \begin{equation*}
        \text{count}(e^{\text{sep}}_j) = \sum_{l \leq j} e^{\text{sep}}_l
    \end{equation*}
    Thus $V^{(1)}_3$ returns the current step index
\end{enumerate}
\end{lemma}

\subsubsection{Block 2: Dependency Graph Construction} 
Define three attention heads $A^{(2)}_1, A^{(2)}_2, A^{(2)}_3$ implementing dependency selection:
\begin{align*}
    A^{(2)}_1&: Q^{(2)}_1 = W^q_2[e^{\text{step}}_k] \\
    &K^{(2)}_1 = W^k_2[e^{\text{input}}_j]_{j<k} \\
    &V^{(2)}_1 = W^v_2[j]_{j<k} \\
    A^{(2)}_2&: Q^{(2)}_2 = W^q_3[e^{\text{step}}_k] \\
    &K^{(2)}_2 = W^k_3[e^{\text{step}}_j]_{j<k} \\
    &V^{(2)}_2 = W^v_3[B(s_1,\ldots,s_{i+1}, i+1)]_{j<k} \\
    A^{(2)}_3&: Q^{(2)}_3 = W^q_4[e^{\text{step}}_k] \\
    &K^{(2)}_3 = W^k_4[j]_{j<k} \\
    &V^{(2)}_3 = W^v_4[e^{\text{q}}_j]_{j<k}
\end{align*}

\begin{lemma}
Block 2 correctly implements $G_{i+1} = \{q_k | k \in B(s_1,\ldots,s_{i+1}, i+1)\}$ through:

1. First attention head $A^{(2)}_1$ gathers input sequence up to current step i+1:
\begin{equation*}
    z^{(2)}_1 = \{s_j | j \leq i+1\}
\end{equation*}

2. Second attention head $A^{(2)}_2$ computes indices from B using gathered inputs:
\begin{equation*}
    z^{(2)}_2 = B(z^{(2)}_1, i+1)
\end{equation*}

3. Third attention head $A^{(2)}_3$ selects states using computed indices:
\begin{equation*}
    z^{(2)}_3 = \{e^{\text{q}}_j | j \in z^{(2)}_2\}
\end{equation*}

Therefore, the composition $z^{(2)}_3(z^{(2)}_2(z^{(2)}_1))$ correctly implements $G_{i+1}$ by:
\begin{enumerate}
    \item Gathering relevant input sequence 
    \item Computing dependency indices using B
    \item Selecting corresponding states
\end{enumerate}

The correctness follows from attention scoring:
\begin{align*}
    \text{score}_1(q_k, k_j) &= \begin{cases}
        1 & \text{if } j \leq i+1 \\
        0 & \text{otherwise}
    \end{cases} \\
    \text{score}_2(q_k, k_j) &= \begin{cases}
        1 & \text{if } j \in B(s_1,\ldots,s_{i+1}, i+1) \\
        0 & \text{otherwise}
    \end{cases} \\
    \text{score}_3(q_k, k_j) &= \begin{cases}
        1 & \text{if } j \in z^{(2)}_2 \\
        0 & \text{otherwise}
    \end{cases}
\end{align*}
\end{lemma}

\subsubsection{Block 3: State Transition}
Define attention mechanism implementing $F$:
\begin{align*}
    A^{(3)}_1&: Q^{(3)}_1 = W^q_3[e^{\text{isState}}_k] \\
    &K^{(3)}_1 = W^k_3[e^{\text{isDependence}}_j]_{j<k} \\
    &V^{(3)}_1 = W^v_3[e^{\text{q}}_j]_{j<k} \\
    A^{(3)}_2&: Q^{(3)}_2 = W^q_4[e^{\text{isState}}_k] \\
    &K^{(3)}_2 = W^k_4[e^{\text{isInput}}_j]_{j<k} \\
    &V^{(3)}_2 = W^v_4[e^{\text{input}}_j]_{j<k}
\end{align*}

\begin{lemma}
The state transition function $F$ is correctly computed through:
\begin{equation*}
    q_{i+1} = F(G_{i+1}, s_{i+1}) = \text{FFN}(z^{(3)}_1, z^{(3)}_2)
\end{equation*}
where $z^{(3)}_1 = A^{(3)}_1(e^{\text{q}}_j \mid j \in B(s_1,\ldots,s_{i+1}, i+1))$ represents the states selected by $G_{i+1}$ from Block 2, and $z^{(3)}_2 = A^{(3)}_2(s_{i+1})$ represents the current input token.
\end{lemma}

\subsubsection{Block 4: Result Aggregation}
Define two attention heads $A^{(4)}_1, A^{(4)}_2$ for implementing $H$:
\begin{align*}
    A^{(4)}_1&: Q^{(4)}_1 = W^q_4[e^{\text{isL}}_k] \\
    &K^{(4)}_1 = W^k_4[e^{\text{isL}}_j]_{j<k} \\
    &V^{(4)}_1 = W^v_4[e^{\text{L}}_j]_{j<k} \\
    A^{(4)}_2&: Q^{(4)}_2 = W^q_5[e^{\text{isL}}_k] \\
    &K^{(4)}_2 = W^k_5[e^{\text{isState}}_j]_{j<k} \\
    &V^{(4)}_2 = W^v_5[e^{\text{q}}_j]_{j<k}
\end{align*}

\begin{lemma}
Block 4 correctly implements the aggregation function $H$ through:

1. For $i=1$ (base case):
\begin{equation*}
    \text{score}_1(q_k, k_j) = 0, \quad \text{score}_2(q_k, k_j) = \begin{cases}
        1 & \text{if } e^{\text{isState}}_j = 1 \\
        0 & \text{otherwise}
    \end{cases}
\end{equation*}
Therefore $L_1 = H(\emptyset, q_1) = q_1$ since only $A^{(4)}_2$ activates to select $q_1$

2. For $i>1$:
\begin{equation*}
    \text{score}_1(q_k, k_j) = \begin{cases}
        1 & \text{if } e^{\text{isL}}_j = 1 \text{ and j is the latest L position} \\
        0 & \text{otherwise}
    \end{cases}
\end{equation*}
\begin{equation*}
    \text{score}_2(q_k, k_j) = \begin{cases}
        1 & \text{if } e^{\text{isState}}_j = 1 \text{ and j corresponds to } q_i \\
        0 & \text{otherwise}
    \end{cases}
\end{equation*}

Therefore:
\begin{align*}
    z^{(4)}_1 &= A^{(4)}_1(e^{\text{L}}_k) = L_{i-1} \text{ (previous aggregation result)} \\
    z^{(4)}_2 &= A^{(4)}_2(e^{\text{q}}_k) = q_i \text{ (current state)} \\
    L_i &= \text{FFN}(z^{(4)}_1, z^{(4)}_2) = H(L_{i-1}, q_i)
\end{align*}

The FFN is constructed to implement the specific aggregation operation of $H$ (e.g., max, min, or sum).
\end{lemma}

\begin{proposition}[Block Transitions]
The blocks connect sequentially where:
\begin{enumerate}
    \item Block 1 output provides input positions, state positions and step indices
    \item Block 2 implements dependency function $G$ to gather required states
    \item Block 3 uses gathered dependencies and current input to compute new states via $F$
    \item Block 4 implements $H$ to aggregate states into final result
\end{enumerate}
Each transition preserves information through residual connections.
\end{proposition}
