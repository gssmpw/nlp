[
  {
    "index": 0,
    "papers": [
      {
        "key": "vaswani2017attention",
        "author": "Vaswani, A",
        "title": "Attention is all you need"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "minaee2024largelanguagemodelssurvey",
        "author": "Shervin Minaee and Tomas Mikolov and Narjes Nikzad and Meysam Chenaghlu and Richard Socher and Xavier Amatriain and Jianfeng Gao",
        "title": "Large Language Models: A Survey"
      },
      {
        "key": "naveed2024comprehensiveoverviewlargelanguage",
        "author": "Humza Naveed and Asad Ullah Khan and Shi Qiu and Muhammad Saqib and Saeed Anwar and Muhammad Usman and Naveed Akhtar and Nick Barnes and Ajmal Mian",
        "title": "A Comprehensive Overview of Large Language Models"
      },
      {
        "key": "Xu_2024",
        "author": "Xu, Xingcheng and Pan, Zihao and Zhang, Haipeng and Yang, Yanqing",
        "title": "It Ain\u2019t That Bad: Understanding the Mysterious Performance Drop in OOD Generalization for Generative Transformer Models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "anil2022exploringlengthgeneralizationlarge",
        "author": "Cem Anil and Yuhuai Wu and Anders Andreassen and Aitor Lewkowycz and Vedant Misra and Vinay Ramasesh and Ambrose Slone and Guy Gur-Ari and Ethan Dyer and Behnam Neyshabur",
        "title": "Exploring Length Generalization in Large Language Models"
      },
      {
        "key": "zhang2022delving",
        "author": "Zhang, Chongzhi and Zhang, Mingyuan and Zhang, Shanghang and Jin, Daisheng and Zhou, Qiang and Cai, Zhongang and Zhao, Haiyu and Liu, Xianglong and Liu, Ziwei",
        "title": "Delving deep into the generalization of vision transformers under distribution shifts"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "liu2022transformers",
        "author": "Liu, Bingbin and Ash, Jordan T and Goel, Surbhi and Krishnamurthy, Akshay and Zhang, Cyril",
        "title": "Transformers learn shortcuts to automata"
      },
      {
        "key": "geirhos2020shortcut",
        "author": "Geirhos, Robert and Jacobsen, J{\\\"o}rn-Henrik and Michaelis, Claudio and Zemel, Richard and Brendel, Wieland and Bethge, Matthias and Wichmann, Felix A",
        "title": "Shortcut learning in deep neural networks"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "li2023transformers",
        "author": "Li, Yingcong and Ildiz, Muhammed Emrullah and Papailiopoulos, Dimitris and Oymak, Samet",
        "title": "Transformers as algorithms: Generalization and stability in in-context learning"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "qian2022limitationslanguagemodelsarithmetic",
        "author": "Jing Qian and Hong Wang and Zekun Li and Shiyang Li and Xifeng Yan",
        "title": "Faith and Fate: Limits of Transformers on Compositionality"
      },
      {
        "key": "nogueira2021investigatinglimitationstransformerssimple",
        "author": "Rodrigo Nogueira and Zhiying Jiang and Jimmy Lin",
        "title": "Investigating the Limitations of Transformers with Simple Arithmetic Tasks"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "hendrycks2020pretrainedtransformersimproveoutofdistribution",
        "author": "Dan Hendrycks and Xiaoyuan Liu and Eric Wallace and Adam Dziedzic and Rishabh Krishnan and Dawn Song",
        "title": "Pretrained Transformers Improve Out-of-Distribution Robustness"
      },
      {
        "key": "zhang2023unveilingtransformerslegosynthetic",
        "author": "Yi Zhang and Arturs Backurs and S\u00e9bastien Bubeck and Ronen Eldan and Suriya Gunasekar and Tal Wagner",
        "title": "Unveiling Transformers with LEGO: a synthetic reasoning task"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "jiang2019avoiding",
        "author": "Jiang, Yichen and Bansal, Mohit",
        "title": "Avoiding reasoning shortcuts: Adversarial evaluation, training, and model development for multi-hop QA"
      },
      {
        "key": "taori2020when",
        "author": "Rohan Taori and Achal Dave and Vaishaal Shankar and Nicholas Carlini and Benjamin Recht and Ludwig Schmidt",
        "title": "When Robustness Doesn{\\textquoteright}t Promote Robustness: Synthetic vs. Natural Distribution Shifts on ImageNet"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "chen2024alphamathzeroprocesssupervision",
        "author": "Guoxin Chen and Minpeng Liao and Chengxi Li and Kai Fan",
        "title": "AlphaMath Almost Zero: Process Supervision without Process"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "wei2022chain",
        "author": "Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others",
        "title": "Chain-of-thought prompting elicits reasoning in large language models"
      },
      {
        "key": "kojima2022large",
        "author": "Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke",
        "title": "Large language models are zero-shot reasoners"
      },
      {
        "key": "zhu2024deductive",
        "author": "Zhu, Tinghui and Zhang, Kai and Xie, Jian and Su, Yu",
        "title": "Deductive beam search: Decoding deducible rationale for chain-of-thought reasoning"
      },
      {
        "key": "fu2022complexity",
        "author": "Fu, Yao and Peng, Hao and Sabharwal, Ashish and Clark, Peter and Khot, Tushar",
        "title": "Complexity-based prompting for multi-step reasoning"
      },
      {
        "key": "kim2024transformers",
        "author": "Kim, Juno and Suzuki, Taiji",
        "title": "Transformers Provably Solve Parity Efficiently with Chain of Thought"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "li2024chainthoughtempowerstransformers",
        "author": "Zhiyuan Li and Hong Liu and Denny Zhou and Tengyu Ma",
        "title": "Chain of Thought Empowers Transformers to Solve Inherently Serial Problems"
      },
      {
        "key": "abbe2024fartransformersreasonglobality",
        "author": "Emmanuel Abbe and Samy Bengio and Aryo Lotfi and Colin Sandon and Omid Saremi",
        "title": "How Far Can Transformers Reason? The Globality Barrier and Inductive Scratchpad"
      },
      {
        "key": "feng2023revealingmysterychainthought",
        "author": "Guhao Feng and Bohang Zhang and Yuntian Gu and Haotian Ye and Di He and Liwei Wang",
        "title": "Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "ton2024understandingchainofthoughtllmsinformation",
        "author": "Jean-Francois Ton and Muhammad Faaiz Taufiq and Yang Liu",
        "title": "Understanding Chain-of-Thought in LLMs through Information Theory"
      },
      {
        "key": "kudo2024thinktotalktalktothinkllmscome",
        "author": "Keito Kudo and Yoichi Aoki and Tatsuki Kuribayashi and Shusaku Sone and Masaya Taniguchi and Ana Brassard and Keisuke Sakaguchi and Kentaro Inui",
        "title": "Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in Multi-Step Reasoning"
      },
      {
        "key": "yu2025llmsreallythinkstepbystep",
        "author": "Yijiong Yu",
        "title": "Do LLMs Really Think Step-by-step In Implicit Reasoning?"
      },
      {
        "key": "anonymous2025chainofthought",
        "author": "Anonymous",
        "title": "Chain-of-Thought Provably Enables Learning the (Otherwise) Unlearnable"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "li2024how",
        "author": "Hongkang Li and Meng Wang and Songtao Lu and Xiaodong Cui and Pin-Yu Chen",
        "title": "How Do Nonlinear Transformers Acquire Generalization-Guaranteed CoT Ability?"
      },
      {
        "key": "hu2024unveilingstatisticalfoundationschainofthought",
        "author": "Xinyang Hu and Fengzhuo Zhang and Siyu Chen and Zhuoran Yang",
        "title": "Unveiling the Statistical Foundations of Chain-of-Thought Prompting Methods"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "kim2023cot",
        "author": "Kim, Seungone and Joo, Se June and Kim, Doyoung and Jang, Joel and Ye, Seonghyeon and Shin, Jamin and Seo, Minjoon",
        "title": "The cot collection: Improving zero-shot and few-shot learning of language models via chain-of-thought fine-tuning"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "nguyen2023cof",
        "author": "Nguyen, Hoang H and Liu, Ye and Zhang, Chenwei and Zhang, Tao and Yu, Philip S",
        "title": "CoF-CoT: Enhancing large language models with coarse-to-fine chain-of-thought prompting for multi-domain NLU tasks"
      },
      {
        "key": "chu-etal-2025-towards",
        "author": "Chu, Zheng  and\nChen, Jingchang  and\nWang, Zhongjie  and\nTang, Guo  and\nChen, Qianglong  and\nLiu, Ming  and\nQin, Bing",
        "title": "Towards Faithful Multi-step Reasoning through Fine-Grained Causal-aware Attribution Reasoning Distillation"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "kaplan2020scalinglawsneurallanguage",
        "author": "Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei",
        "title": "Scaling Laws for Neural Language Models"
      },
      {
        "key": "hoffmann2022trainingcomputeoptimallargelanguage",
        "author": "Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre",
        "title": "Training Compute-Optimal Large Language Models"
      },
      {
        "key": "chowdhery2022palm",
        "author": "Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others",
        "title": "Palm: Scaling language modeling with pathways"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "micelibarone2022distributionallyrobustrecurrentdecoders",
        "author": "Antonio Valerio Miceli-Barone and Alexandra Birch and Rico Sennrich",
        "title": "Distributionally Robust Recurrent Decoders with Random Network Distillation"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "abbe2024fartransformersreasonglobality",
        "author": "Emmanuel Abbe and Samy Bengio and Aryo Lotfi and Colin Sandon and Omid Saremi",
        "title": "How Far Can Transformers Reason? The Globality Barrier and Inductive Scratchpad"
      }
    ]
  }
]