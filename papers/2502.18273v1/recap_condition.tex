\section{Code and Data}
We provide an anonymous page, you can following the instruction to generate data, training model and evaluation.
https://github.com/physicsru/Scaling-Curves-of-CoT-Granularity-for-Language-Model-Generalization

\section{Recap Condition Analysis}

\begin{theorem}[Outside Token Recap Condition under RoPE]
\label{thm: Recap 1}
Consider a transformer with Rotary Positional Embedding (RoPE) using angles $\theta_j = 10000^{-2j/d_{\text{model}}}$. Given finite computational precision $s$ and minimum resolvable attention score $\epsilon$, there exists a threshold distance $\tau > 0$ such that for all positional distances $d > \tau$:
\[
|A(d)| < \epsilon,
\]
where $A(d)$ is the attention score between tokens at distance $d$. Consequently, tokens beyond $[i_{\text{current}} - \tau, i_{\text{current}} + \tau]$ cannot be recalled.

\begin{proof}
\textbf{Step 1: Attention Score Formulation}  
The RoPE attention score between positions $m$ and $n$ (distance $d = |m-n|$) is:
\[
A(d) = \text{Re}\left[\sum_{j=0}^{d_{\text{model}}/2-1} h_j e^{\mathbf{i}d\theta_j}\right], \quad h_j := q_{[2j:2j+1]}\mathbf{k}^*_{[2j:2j+1]}
\]
where $h_j$ encodes query-key interactions for the $j$-th dimension pair.

\textbf{Step 2: Abel Transformation\cite{men2024baseropeboundscontext}}  
Let $S_{j+1} = \sum_{k=0}^j e^{\mathbf{i}d\theta_k}$ with $S_0 = 0$. Using summation by parts:
\[
\sum_{j=0}^{d_{\text{model}}/2-1} h_j e^{\mathbf{i}d\theta_j} = \sum_{j=0}^{d_{\text{model}}/2-1} (h_j - h_{j+1}) S_{j+1}
\]
Taking absolute values:
\[
|A(d)| \leq \sum_{j=0}^{d_{\text{model}}/2-1} |h_{j+1} - h_j| \cdot |S_{j+1}|
\]

\bigskip
\textbf{Step 3: Bounding Query-Key Differences.}  
Assume that the query and key representations are uniformly bounded so that $\|q\|,\,\|k\| \le C$. In particular, since each $h_j$ results from a dot product between sub-vectors from $q$ and $k$, we have $|h_j| \le C^2$. Moreover, if we assume that the embeddings vary smoothly with the index $j$ (as expected from the continuity of underlying network nonlinearities and weight matrices), then the mean value theorem implies that the difference
\[
|h_{j+1} - h_j|
\]
is bounded by a Lipschitz constant. That is, there exists a constant $M = \mathcal{O}(C^2)$ such that for every $j$
\[
|h_{j+1} - h_j| \le M.
\]
A more refined analysis might track this difference in terms of the networkâ€™s smoothness, but the key point is that each difference is uniformly bounded by a constant depending on $C$.

\bigskip
\textbf{Step 4: Analyzing Oscillatory Sums.}  
We now study the partial sum
\[
S_{j+1} = \sum_{k=0}^j e^{\mathbf{i}d\theta_k}.
\]
Two regimes are considered:

\emph{(i) Low-frequency regime ($k \le j_0$):}  
For sufficiently small indices $k$, we have $\theta_k$ being relatively large so that
\[
d\theta_k \gg 1.
\]
In this regime the phases $e^{\mathbf{i}d\theta_k}$ change rapidly with $k$, leading to cancellations among the terms. A standard bound for such oscillatory sums yields
\[
\Big|\sum_{k=0}^{j_0} e^{\mathbf{i}d\theta_k}\Big| \le \frac{2}{\left|1 - e^{\mathbf{i}d\theta_0}\right|} = \mathcal{O}(1),
\]
since the denominator remains bounded away from zero when $d\theta_0$ is large.

\emph{(ii) High-frequency regime ($k > j_0$):}  
For larger $k$, the angle $\theta_k$ becomes very small (because $\theta_k$ decays exponentially with $k$), so that $d\theta_k \ll 1$. In this case we can use the first-order Taylor expansion:
\[
e^{\mathbf{i}d\theta_k} \approx 1 + \mathbf{i}d\theta_k.
\]
Thus, for indices $j>j_0$, the sum becomes approximately
\[
\sum_{k=j_0+1}^j e^{\mathbf{i}d\theta_k} \approx (j-j_0) + \mathbf{i}d\sum_{k=j_0+1}^j \theta_k.
\]
While the real part grows (almost) linearly in the number of terms, the alternating phases and the small magnitude of $d\theta_k$ imply that additional cancellation occurs when the two regimes are combined. In the worst case one may bound
\[
|S_{j+1}| \le \mathcal{O}(\log d)
\]
by appealing to harmonic series type estimates; this bound is loose but captures the fact that the cancellation improves with larger $d$.

\bigskip
\textbf{Step 5: Combining the Results.}  
Returning to the bound from Step~2, we have
\[
|A(d)| \le \sum_{j=0}^{d_{\text{model}}/2-1} |h_{j+1} - h_j|\, |S_{j+1}|
\]
and using the bounds from Steps~3 and 4,
\[
|A(d)| \le M \sum_{j=0}^{d_{\text{model}}/2-1} \mathcal{O}(\log d) = \mathcal{O}(M\, d_{\text{model}} \log d).
\]
In practice, the alternating signs in the summands produce even stronger decay in $d$, and empirical observations suggest a polynomial decay of the form
\[
|A(d)| \le \mathcal{O}\Big(\frac{M}{\sqrt{d}}\Big).
\]

\bigskip
\textbf{Step 6: Precision Threshold.}  
For a given minimum resolvable attention score $\epsilon$, we require
\[
\frac{M}{\sqrt{d}} < \epsilon \quad \Longrightarrow \quad d > \left(\frac{M}{\epsilon}\right)^2.
\]
Defining
\[
\tau = \left(\frac{M}{\epsilon}\right)^2,
\]
we conclude that for any $d > \tau$, it holds that $|A(d)| < \epsilon$. That is, tokens beyond the window indexed by $[i_{\text{current}}-\tau,\, i_{\text{current}}+\tau]$ effectively contribute an attention score below the resolvable threshold.

\end{proof}

\textbf{Interpretation:}
\begin{itemize}
\item The $\theta_j$ schedule creates frequency-dependent decay: high frequencies (small $j$) attenuate rapidly
\item Window size $\tau \propto (M/\epsilon)^2$ explains memory limitations in long contexts
\item Practical implementations must balance $d_{\text{model}}$ and precision $s$ for optimal $\tau$
\end{itemize}
\end{theorem}

\begin{theorem}[Inside window recap condition under Rope]
\label{thm: Recap 2}
Assume: The true function is $s_2 = {w^{\circ}}^{\top} e(s_1) + \epsilon$, with $s_2 \perp s_x$ in expectation.
Consider two linear models:
\begin{itemize}
\item $M_s$: $y_s = {w_s}^{\top} e(s_1)$
\item $M_l$: $y_l = {w_1}^{\top} e(s_1) + {w_2}^{\top} e(s_x)$
\end{itemize}
Under mean-squared-error training:
\begin{itemize}
\item $g_s = \frac{\partial L_s}{\partial w_s}$
\item $(g_1, g_2) = \left(\frac{\partial L_l}{\partial w_1}, \frac{\partial L_l}{\partial w_2}\right)$
\end{itemize}
Then in finite-data regimes or with random noise, the gradient component of $g_1$ along $(w^{\circ} - w_1)$ is typically smaller than the corresponding component of $g_s$ along $(w^{\circ} - w_s)$
Formally,
\[
\mathbb{E}\left[ \left\langle \mathbf{g}_1, \mathbf{w}^\circ - \mathbf{w}_1 \right\rangle \right] < \mathbb{E}\left[ \left\langle \mathbf{g}_s, \mathbf{w}^\circ - \mathbf{w}_s \right\rangle \right],
\]
leading to slower convergence for \( \mathcal{M}_{l} \) when \( s_{x} \) is irrelevant.
\end{theorem}
%\input{proof_inside_recap}

\begin{proof}
We analyze the gradients of both models and demonstrate how irrelevant features in \( \mathcal{M}_l \) reduce the effective gradient signal.

\noindent \textbf{Step 1: Express Gradients for Both Models}

For model \( \mathcal{M}_s \), the loss is:
\[
L_s = \mathbb{E}\left[(s_2 - \mathbf{w}_s^\top \mathbf{e}(s_1))^2\right]
\]
The gradient becomes:
\begin{equation}
\mathbf{g}_s = -2 \mathbb{E}\left[\mathbf{e}(s_1)\mathbf{e}(s_1)^\top\right](\mathbf{w}^\circ - \mathbf{w}_s)
\label{eq:grad_s}
\end{equation}

For model \( \mathcal{M}_l \), the gradient for \( \mathbf{w}_1 \) is:
\begin{equation}
\mathbf{g}_1 = -2 \mathbb{E}\left[\mathbf{e}(s_1)\mathbf{e}(s_1)^\top\right](\mathbf{w}^\circ - \mathbf{w}_1) + 2 \mathbb{E}\left[\mathbf{w}_2^\top \mathbf{e}(s_x)\mathbf{e}(s_1)\right]
\label{eq:grad_l}
\end{equation}

\noindent \textbf{Step 2: Compare Gradient Components}

The inner product for \( \mathcal{M}_l \) contains two terms:
\begin{equation}
\begin{aligned}
    \mathbb{E}\left[\langle \mathbf{g}_1, \mathbf{w}^\circ - \mathbf{w}_1 \rangle \right] 
    &= \underbrace{-2 (\mathbf{w}^\circ - \mathbf{w}_1)^\top \mathbb{E}\left[\mathbf{e}(s_1)\mathbf{e}(s_1)^\top\right] (\mathbf{w}^\circ - \mathbf{w}_1)}_{\text{Matches } \mathcal{M}_s} \\
    &\quad + \underbrace{2 \mathbb{E}\left[\mathbf{w}_2^\top \mathbf{e}(s_x)\mathbf{e}(s_1)^\top (\mathbf{w}^\circ - \mathbf{w}_1)\right]}_{\text{Additional term}}
\end{aligned}
\end{equation}


\noindent \textbf{Step 3: Effect of Irrelevant Features}

Since \( s_x \) is irrelevant (\( s_2 \perp s_x \)):
\begin{itemize}
\item Population truth: \( \mathbf{w}_2 = \mathbf{0} \)
\item Finite data allows \( \mathbf{w}_2^\epsilon \) to fit noise \( \epsilon \)
\item Induces spurious correlation: \( \mathbb{E}\left[\mathbf{e}(s_x)\mathbf{e}(s_1)^\top\right] \neq \mathbf{0} \)
\end{itemize}

This makes the additional term:
\[
2 \mathbb{E}\left[\mathbf{w}_2^\top \mathbf{e}(s_x)\mathbf{e}(s_1)^\top (\mathbf{w}^\circ - \mathbf{w}_1)\right] \neq 0
\]
which \textit{reduces} the magnitude of the gradient component.

\noindent \textbf{Step 4: Convergence Comparison}

For \( \mathcal{M}_s \):
\[
\mathbb{E}\left[\langle \mathbf{g}_s, \mathbf{w}^\circ - \mathbf{w}_s \rangle \right] = -2 (\mathbf{w}^\circ - \mathbf{w}_s)^\top \mathbb{E}\left[\mathbf{e}(s_1)\mathbf{e}(s_1)^\top\right] (\mathbf{w}^\circ - \mathbf{w}_s)
\]

For \( \mathcal{M}_l \), the additional term in Equation~\ref{eq:grad_l} creates:
\[
\mathbb{E}\left[ \left\langle \mathbf{g}_1, \mathbf{w}^\circ - \mathbf{w}_1 \right\rangle \right] < \mathbb{E}\left[ \left\langle \mathbf{g}_s, \mathbf{w}^\circ - \mathbf{w}_s \right\rangle \right]
\]

\noindent \textbf{Conclusion} \\
The irrelevant features in \( \mathcal{M}_l \) reduce the gradient component in the direction of \( \mathbf{w}^\circ \), leading to slower convergence compared to \( \mathcal{M}_s \).
\end{proof}