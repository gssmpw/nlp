\section{Proof for Theorem \ref{thm:kl-reduction}}


\begin{proof}
We prove the KL divergence bound by decomposing the distributions over covered and uncovered prefixes. Let $P_{\text{train}}^{\text{Q-CoT}}$ and $P_{\text{eval}}^{\text{Q-CoT}}$ denote the training and evaluation distributions under Q-CoT, respectively. 

\vspace{0.5em}

\noindent \textbf{Step 1: Event Space Partitioning}

Define two disjoint events for any evaluation sample $x = (X^{n_3}, \{q_i^{(n_3)}\}, Y^{n_3})$:
\begin{itemize}
    \item $\mathcal{E}_{\text{cover}}$: The prefix $\{q_i^{(n_3)}\}_{i=1}^{n_3}$ exists in some length-$n_2$ training sample.
    \item $\mathcal{E}_{\text{uncover}}$: The prefix $\{q_i^{(n_3)}\}_{i=1}^{n_3}$ is absent from all training samples.
\end{itemize}
By Lemma \ref{thm:prefix-substructure}, $\mathcal{E}_{\text{cover}}$ occurs when the evaluation prefix matches at least one length-$n_2$ training sequence's prefix. The probabilities satisfy:
\[
P_{\text{cover}} = \mathbb{P}(\mathcal{E}_{\text{cover}}), \quad 1 - P_{\text{cover}} = \mathbb{P}(\mathcal{E}_{\text{uncover}}).
\]
where $P_{\text{cover}}$ is calculated as:
\[
P_{\text{cover}} = \frac{m_2}{m_3 k^{n_3}}
\]
\textit{Derivation}: Each length-$n_2$ training sample contains a unique prefix of length $n_3$ (Lemma \ref{thm:prefix-substructure}). With $m_2$ samples, we can cover $m_2$ distinct prefixes. The total number of possible prefixes is $m_3 k^{n_3}$ ($m_3$ evaluation problems, each with $k^{n_3}$ possible prefixes). Thus, the coverage probability follows the ratio.

\vspace{0.5em}

\noindent \textbf{Step 2: Distributional Decomposition}

Using the law of total probability, we express:
\[
P_{\text{eval}}^{\text{Q-CoT}} = P_{\text{cover}} \cdot P_{\text{eval}|\mathcal{E}_{\text{cover}}} + (1-P_{\text{cover}}) \cdot P_{\text{eval}|\mathcal{E}_{\text{uncover}}}
\]
\[
P_{\text{train}}^{\text{Q-CoT}} = P_{\text{cover}} \cdot P_{\text{train}|\mathcal{E}_{\text{cover}}} + (1-P_{\text{cover}}) \cdot P_{\text{train}|\mathcal{E}_{\text{uncover}}}
\]
where:
\begin{itemize}
    \item $P_{\text{eval}|\mathcal{E}_{\text{cover}}}$: Evaluation distribution restricted to covered prefixes
    \item $P_{\text{train}|\mathcal{E}_{\text{cover}}}$: Training distribution restricted to covered prefixes
    \item $P_{\text{eval}|\mathcal{E}_{\text{uncover}}}$: Evaluation distribution for uncovered prefixes
    \item $P_{\text{train}|\mathcal{E}_{\text{uncover}}}$: Training distribution for uncovered prefixes
\end{itemize}

\vspace{0.5em}

\noindent \textbf{Step 3: KL Divergence Expansion with Total Expectation}

From the KL divergence definition:
\[
D_{\mathrm{KL}}\left(P_{\text{eval}}^{\text{Q-CoT}} \,\big\|\, P_{\text{train}}^{\text{Q-CoT}}\right) = \mathbb{E}_{x \sim P_{\text{eval}}^{\text{Q-CoT}}} \left[ \log \frac{P_{\text{eval}}^{\text{Q-CoT}}(x)}{P_{\text{train}}^{\text{Q-CoT}}(x)} \right]
\]
Apply the law of total expectation by conditioning on $\mathcal{E}_{\text{cover}}$ and $\mathcal{E}_{\text{uncover}}$:
\[
= \mathbb{P}(\mathcal{E}_{\text{cover}}) \cdot \mathbb{E}_{x|\mathcal{E}_{\text{cover}}} \left[ \log \frac{P_{\text{eval}}^{\text{Q-CoT}}(x|\mathcal{E}_{\text{cover}})}{P_{\text{train}}^{\text{Q-CoT}}(x|\mathcal{E}_{\text{cover}})} \right] + \mathbb{P}(\mathcal{E}_{\text{uncover}}) \cdot \mathbb{E}_{x|\mathcal{E}_{\text{uncover}}} \left[ \log \frac{P_{\text{eval}}^{\text{Q-CoT}}(x|\mathcal{E}_{\text{uncover}})}{P_{\text{train}}^{\text{Q-CoT}}(x|\mathcal{E}_{\text{uncover}})} \right]
\]

\vspace{0.5em}

\noindent \textbf{Step 4: Handling Covered Cases}

Under $\mathcal{E}_{\text{cover}}$, Lemma \ref{thm:prefix-substructure} guarantees that the CoT states $\{q_i^{(n_3)}\}$ in evaluation samples exactly match those in training samples. This implies:
\[
P_{\text{eval}|\mathcal{E}_{\text{cover}}}(x) = P_{\text{train}|\mathcal{E}_{\text{cover}}}(x), \quad \forall x \in \mathcal{E}_{\text{cover}}
\]
Therefore:
\[
\mathbb{E}_{x|\mathcal{E}_{\text{cover}}} \left[ \log \frac{P_{\text{eval}|\mathcal{E}_{\text{cover}}}}{P_{\text{train}|\mathcal{E}_{\text{cover}}}} \right] = \mathbb{E}_{x|\mathcal{E}_{\text{cover}}} [\log 1] = 0
\]

\vspace{0.5em}

\noindent \textbf{Step 5: Uncovered Cases Reduce to Q-A}

For $x = (X^{n_3}, \{q_i^{(n_3)}\}, Y^{n_3}) \in \mathcal{E}_{\text{uncover}}$, the absence of matching prefixes in training data implies the model cannot leverage CoT states $\{q_i^{(n_3)}\}$ during inference. We formally analyze this degradation:

%\vspace{0.5em}

%\noindent **5.1 Conditional Distribution Decomposition**

Under Q-CoT, the generation process factors as:
\[
P^{\text{Q-CoT}}(Y|X) = \sum_{\{q_i\}} P(Y|X, \{q_i\}) P(\{q_i\}|X)
\]
where:
\begin{itemize}
    \item $P(\{q_i\}|X)$: Probability of generating CoT states $\{q_i\}$ given input $X$
    \item $P(Y|X, \{q_i\})$: Probability of answer $Y$ given $X$ and CoT states
\end{itemize}

%\vspace{0.5em}

%\noindent **5.2 Uncovered Case Analysis**

When $\{q_i^{(n_3)}\}$ is uncovered ($\mathcal{E}_{\text{uncover}}$), the model lacks training data to estimate either:
\begin{itemize}
    \item The CoT state distribution $P(\{q_i\}|X)$
    \item The answer likelihood $P(Y|X, \{q_i\})$ 
\end{itemize}

Thus, the model \textit{cannot} utilize the CoT decomposition and must marginalize over all possible $\{q_i\}$:
\[
P^{\text{Q-CoT}}(Y|X) = \mathbb{E}_{\{q_i\} \sim P(\{q_i\}|X)} \left[ P(Y|X, \{q_i\}) \right]
\]

%\vspace{0.5em}

%\noindent **5.3 Degeneration to Q-A**

Without CoT supervision on $\{q_i^{(n_3)}\}$, two condition assumes:
\begin{enumerate}
    \item \textbf{Untrained CoT States}: If $\{q_i^{(n_3)}\}$ never appears in training, $P(\{q_i\}|X)$ becomes a \textit{uniform prior} over possible CoT sequences (by maximum entropy principle).
    
    \item \textbf{Uninformative Likelihood}: The answer likelihood $P(Y|X, \{q_i\})$ reduces to $P^{\text{Q-A}}(Y|X)$ because the model cannot associate $\{q_i\}$ with $Y$ without training signals.
\end{enumerate}

Thus:
\[
P^{\text{Q-CoT}}(Y|X) = \sum_{\{q_i\}} \underbrace{P^{\text{Q-A}}(Y|X)}_{\text{Uninformative}} \cdot \underbrace{\frac{1}{k^{n_{3}}}}_{\text{Uniform } P(\{q_i\}|X)} = P^{\text{Q-A}}(Y|X)
\]

with expansion of KL divergence of Q-A
\[
D_{\mathrm{KL}}\left(P_{\text{eval}}^{\text{Q-A}} \,\big\|\, P_{\text{train}}^{\text{Q-A}}\right) = \mathbb{E}_{x \sim P_{\text{eval}}^{\text{Q-A}}} \left[ \log \frac{P_{\text{eval}}^{\text{Q-A}}(x)}{P_{\text{train}}^{\text{Q-A}}(x)} \right]
\] 
\[
= \mathbb{P}(\mathcal{E}_{\text{cover}}) \cdot \mathbb{E}_{x|\mathcal{E}_{\text{cover}}} \left[ \log \frac{P_{\text{eval}}^{\text{Q-A}}(x|\mathcal{E}_{\text{cover}})}{P_{\text{train}}^{\text{Q-A}}(x|\mathcal{E}_{\text{cover}})} \right] + \mathbb{P}(\mathcal{E}_{\text{uncover}}) \cdot \mathbb{E}_{x|\mathcal{E}_{\text{uncover}}} \left[ \log \frac{P_{\text{eval}}^{\text{Q-A}}(x|\mathcal{E}_{\text{uncover}})}{P_{\text{train}}^{\text{Q-A}}(x|\mathcal{E}_{\text{uncover}})} \right]
\]
Notice that
\[
\mathbb{E}_{x|\mathcal{E}_{\text{cover}}} \left[ \log \frac{P_{\text{eval}}^{\text{Q-A}}(x|\mathcal{E}_{\text{cover}})}{P_{\text{train}}^{\text{Q-A}}(x|\mathcal{E}_{\text{cover}})} \right]
\leq
\mathbb{E}_{x|\mathcal{E}_{\text{uncover}}} \left[ \log \frac{P_{\text{eval}}^{\text{Q-A}}(x|\mathcal{E}_{\text{uncover}})}{P_{\text{train}}^{\text{Q-A}}(x|\mathcal{E}_{\text{uncover}})} \right]
\]
since covered prefix will decrease the KL divergence via probability decomposition
\[
D_{\mathrm{KL}}\left(P_{\text{eval}}^{\text{Q-A}} \,\big\|\, P_{\text{train}}^{\text{Q-A}}\right) \geq 
\mathbb{E}_{x|\mathcal{E}_{\text{uncover}}} \left[ \log \frac{P_{\text{eval}}^{\text{Q-A}}(x|\mathcal{E}_{\text{uncover}})}{P_{\text{train}}^{\text{Q-A}}(x|\mathcal{E}_{\text{uncover}})} \right] = D_{\mathrm{KL}}\left(P_{\text{eval}|\mathcal{E}_{\text{uncover}}}^{\text{Q-A}} \,\big\|\, P_{\text{train}|\mathcal{E}_{\text{uncover}}}^{\text{Q-A}}\right)
\]

Therefore, for $x \in \mathcal{E}_{\text{uncover}}$:
\[
D_{\mathrm{KL}}\left(P_{\text{eval}|\mathcal{E}_{\text{uncover}}} \,\big\|\, P_{\text{train}|\mathcal{E}_{\text{uncover}}}\right) = 
D_{\mathrm{KL}}\left(P_{\text{eval}|\mathcal{E}_{\text{uncover}}}^{\text{Q-A}} \,\big\|\, P_{\text{train}|\mathcal{E}_{\text{uncover}}}^{\text{Q-A}}\right) \leq 
D_{\mathrm{KL}}\left(P_{\text{eval}}^{\text{Q-A}} \,\big\|\, P_{\text{train}}^{\text{Q-A}}\right) = \mathrm{KL}_{\text{base}}
\]

\vspace{0.5em}

\noindent \textbf{Step 6: Final Inequality}

Combining all terms:
\[
D_{\mathrm{KL}}\left(P_{\text{eval}}^{\text{Q-CoT}} \,\big\|\, P_{\text{train}}^{\text{Q-CoT}}\right) = \underbrace{P_{\text{cover}} \cdot 0}_{\text{Covered term}} + \underbrace{(1-P_{\text{cover}}) \cdot \mathrm{KL}_{\text{base}}}_{\text{Uncovered term}}
\]
Hence:
\[
D_{\mathrm{KL}}\left(P_{\text{eval}}^{\text{Q-CoT}} \,\big\|\, P_{\text{train}}^{\text{Q-CoT}}\right) \leq (1 - P_{\text{cover}}) \cdot \mathrm{KL}_{\text{base}}
\]
The equality holds when $P_{\text{cover}} \in [0,1]$. When $m_2 = m_3 k^{n_3}$, we have $P_{\text{cover}} = 1$, making the KL divergence zero.
\end{proof}

