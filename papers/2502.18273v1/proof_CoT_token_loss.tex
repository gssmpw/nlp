\begin{proof}
By the uniqueness condition, only the full sequence $s_1, \dots, s_k$ guarantees $s_{\text{ans}}$. Let $\mathcal{L}$ be the set of $l$ compromised tokens. The probability of maintaining correctness is:

\[
P(s_{\text{ans}} \mid s_{\text{input}}) = \underbrace{\prod_{j \in \mathcal{L}} P(s_j \mid s_{\text{input}})}_{\text{Lost tokens}} \cdot \underbrace{\prod_{i \notin \mathcal{L}} P(s_i \mid s_{\text{input}})}_{\text{Preserved tokens}}
\]

For preserved tokens ($i \notin \mathcal{L}$), full training ensures $P(s_i \mid s_{\text{input}}) = 1$. For lost tokens ($j \in \mathcal{L}$), training deficiency gives $P(s_j \mid s_{\text{input}}) = 1 - \epsilon$. Thus:

\[
P(s_{\text{ans}} \mid s_{\text{input}}) = (1 - \epsilon)^l \cdot 1^{k-l} = (1 - \epsilon)^l
\]

This equality holds because any deviation from the exact CoT sequence (due to lost tokens) eliminates the chance of correctness by the uniqueness condition.
\end{proof}