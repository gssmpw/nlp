@inproceedings{Xu_2024, series={IJCAI-2024},
   title={It Ain’t That Bad: Understanding the Mysterious Performance Drop in OOD Generalization for Generative Transformer Models},
   url={http://dx.doi.org/10.24963/ijcai.2024/727},
   DOI={10.24963/ijcai.2024/727},
   booktitle={Proceedings of the Thirty-ThirdInternational Joint Conference on Artificial Intelligence},
   publisher={International Joint Conferences on Artificial Intelligence Organization},
   author={Xu, Xingcheng and Pan, Zihao and Zhang, Haipeng and Yang, Yanqing},
   year={2024},
   month=aug, pages={6578–6586},
   collection={IJCAI-2024} }

@misc{abbe2024fartransformersreasonglobality,
      title={How Far Can Transformers Reason? The Globality Barrier and Inductive Scratchpad}, 
      author={Emmanuel Abbe and Samy Bengio and Aryo Lotfi and Colin Sandon and Omid Saremi},
      year={2024},
      eprint={2406.06467},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.06467}, 
}

@misc{anil2022exploringlengthgeneralizationlarge,
      title={Exploring Length Generalization in Large Language Models}, 
      author={Cem Anil and Yuhuai Wu and Anders Andreassen and Aitor Lewkowycz and Vedant Misra and Vinay Ramasesh and Ambrose Slone and Guy Gur-Ari and Ethan Dyer and Behnam Neyshabur},
      year={2022},
      eprint={2207.04901},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2207.04901}, 
}

@misc{chen2024alphamathzeroprocesssupervision,
      title={AlphaMath Almost Zero: Process Supervision without Process}, 
      author={Guoxin Chen and Minpeng Liao and Chengxi Li and Kai Fan},
      year={2024},
      eprint={2405.03553},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.03553}, 
}

@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@inproceedings{chu-etal-2025-towards,
    title = "Towards Faithful Multi-step Reasoning through Fine-Grained Causal-aware Attribution Reasoning Distillation",
    author = "Chu, Zheng  and
      Chen, Jingchang  and
      Wang, Zhongjie  and
      Tang, Guo  and
      Chen, Qianglong  and
      Liu, Ming  and
      Qin, Bing",
    editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven",
    booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
    month = jan,
    year = "2025",
    address = "Abu Dhabi, UAE",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.coling-main.157/",
    pages = "2291--2315",
    abstract = "Despite the remarkable reasoning capabilities demonstrated by large language models (LLM), the substantial computational overhead limits their practices. Some efforts have been directed toward distilling multi-step reasoning capabilities into smaller models through chain-of-thought (CoT). While CoT facilitates multi-step reasoning, the dependencies between reasoning steps are not always clearly discernible, which may lead to inconsistent reasoning. In this paper, we introduce fine-grained attribution reasoning distillation (FARD), which incorporates grounded citations to consolidate the relationships between reasoning steps. Specifically, FARD distills attribution reasoning rationales from LLMs to substitute CoT reasonings, which clarifies the dependencies among reasoning steps. Besides, we regularize the model`s attention pattern by leveraging the causal dependencies between reasoning steps, thereby enhancing the consistency of reasoning. Grounded attribution reasoning also enhances interpretability and verifiability, thereby facilitating faithful reasoning. We evaluate FARD on mathematical and general reasoning benchmarks. The experimental results indicate that FARD outperforms CoT distillation methods in mathematical reasoning, demonstrating its effectiveness. Furthermore, the small models trained with FARD have shown outstanding performance in out-of-distribution reasoning, proving strong generalization capabilities."
}

@misc{feng2023revealingmysterychainthought,
      title={Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective}, 
      author={Guhao Feng and Bohang Zhang and Yuntian Gu and Haotian Ye and Di He and Liwei Wang},
      year={2023},
      eprint={2305.15408},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.15408}, 
}

@inproceedings{fu2022complexity,
  title={Complexity-based prompting for multi-step reasoning},
  author={Fu, Yao and Peng, Hao and Sabharwal, Ashish and Clark, Peter and Khot, Tushar},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@article{geirhos2020shortcut,
  title={Shortcut learning in deep neural networks},
  author={Geirhos, Robert and Jacobsen, J{\"o}rn-Henrik and Michaelis, Claudio and Zemel, Richard and Brendel, Wieland and Bethge, Matthias and Wichmann, Felix A},
  journal={Nature Machine Intelligence},
  volume={2},
  number={11},
  pages={665--673},
  year={2020},
  publisher={Nature Publishing Group UK London}
}

@misc{hendrycks2020pretrainedtransformersimproveoutofdistribution,
      title={Pretrained Transformers Improve Out-of-Distribution Robustness}, 
      author={Dan Hendrycks and Xiaoyuan Liu and Eric Wallace and Adam Dziedzic and Rishabh Krishnan and Dawn Song},
      year={2020},
      eprint={2004.06100},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2004.06100}, 
}

@misc{hoffmann2022trainingcomputeoptimallargelanguage,
      title={Training Compute-Optimal Large Language Models}, 
      author={Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},
      year={2022},
      eprint={2203.15556},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2203.15556}, 
}

@misc{hu2024unveilingstatisticalfoundationschainofthought,
      title={Unveiling the Statistical Foundations of Chain-of-Thought Prompting Methods}, 
      author={Xinyang Hu and Fengzhuo Zhang and Siyu Chen and Zhuoran Yang},
      year={2024},
      eprint={2408.14511},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2408.14511}, 
}

@article{jiang2019avoiding,
  title={Avoiding reasoning shortcuts: Adversarial evaluation, training, and model development for multi-hop QA},
  author={Jiang, Yichen and Bansal, Mohit},
  journal={arXiv preprint arXiv:1906.07132},
  year={2019}
}

@misc{kaplan2020scalinglawsneurallanguage,
      title={Scaling Laws for Neural Language Models}, 
      author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
      year={2020},
      eprint={2001.08361},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2001.08361}, 
}

@article{kim2023cot,
  title={The cot collection: Improving zero-shot and few-shot learning of language models via chain-of-thought fine-tuning},
  author={Kim, Seungone and Joo, Se June and Kim, Doyoung and Jang, Joel and Ye, Seonghyeon and Shin, Jamin and Seo, Minjoon},
  journal={arXiv preprint arXiv:2305.14045},
  year={2023}
}

@article{kim2024transformers,
  title={Transformers Provably Solve Parity Efficiently with Chain of Thought},
  author={Kim, Juno and Suzuki, Taiji},
  journal={arXiv preprint arXiv:2410.08633},
  year={2024}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}

@misc{kudo2024thinktotalktalktothinkllmscome,
      title={Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in Multi-Step Reasoning}, 
      author={Keito Kudo and Yoichi Aoki and Tatsuki Kuribayashi and Shusaku Sone and Masaya Taniguchi and Ana Brassard and Keisuke Sakaguchi and Kentaro Inui},
      year={2024},
      eprint={2412.01113},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.01113}, 
}

@inproceedings{li2023transformers,
  title={Transformers as algorithms: Generalization and stability in in-context learning},
  author={Li, Yingcong and Ildiz, Muhammed Emrullah and Papailiopoulos, Dimitris and Oymak, Samet},
  booktitle={International Conference on Machine Learning},
  pages={19565--19594},
  year={2023},
  organization={PMLR}
}

@misc{li2024chainthoughtempowerstransformers,
      title={Chain of Thought Empowers Transformers to Solve Inherently Serial Problems}, 
      author={Zhiyuan Li and Hong Liu and Denny Zhou and Tengyu Ma},
      year={2024},
      eprint={2402.12875},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.12875}, 
}

@article{liu2022transformers,
  title={Transformers learn shortcuts to automata},
  author={Liu, Bingbin and Ash, Jordan T and Goel, Surbhi and Krishnamurthy, Akshay and Zhang, Cyril},
  journal={arXiv preprint arXiv:2210.10749},
  year={2022}
}

@misc{micelibarone2022distributionallyrobustrecurrentdecoders,
      title={Distributionally Robust Recurrent Decoders with Random Network Distillation}, 
      author={Antonio Valerio Miceli-Barone and Alexandra Birch and Rico Sennrich},
      year={2022},
      eprint={2110.13229},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2110.13229}, 
}

@misc{minaee2024largelanguagemodelssurvey,
      title={Large Language Models: A Survey}, 
      author={Shervin Minaee and Tomas Mikolov and Narjes Nikzad and Meysam Chenaghlu and Richard Socher and Xavier Amatriain and Jianfeng Gao},
      year={2024},
      eprint={2402.06196},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.06196}, 
}

@misc{naveed2024comprehensiveoverviewlargelanguage,
      title={A Comprehensive Overview of Large Language Models}, 
      author={Humza Naveed and Asad Ullah Khan and Shi Qiu and Muhammad Saqib and Saeed Anwar and Muhammad Usman and Naveed Akhtar and Nick Barnes and Ajmal Mian},
      year={2024},
      eprint={2307.06435},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.06435}, 
}

@article{nguyen2023cof,
  title={CoF-CoT: Enhancing large language models with coarse-to-fine chain-of-thought prompting for multi-domain NLU tasks},
  author={Nguyen, Hoang H and Liu, Ye and Zhang, Chenwei and Zhang, Tao and Yu, Philip S},
  journal={arXiv preprint arXiv:2310.14623},
  year={2023}
}

@misc{nogueira2021investigatinglimitationstransformerssimple,
      title={Investigating the Limitations of Transformers with Simple Arithmetic Tasks}, 
      author={Rodrigo Nogueira and Zhiying Jiang and Jimmy Lin},
      year={2021},
      eprint={2102.13019},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2102.13019}, 
}

@misc{qian2022limitationslanguagemodelsarithmetic,
      title={Faith and Fate: Limits of Transformers on Compositionality}, 
      author={Jing Qian and Hong Wang and Zekun Li and Shiyang Li and Xifeng Yan},
      year={2022},
      eprint={2208.05051},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2208.05051}, 
}

@misc{ton2024understandingchainofthoughtllmsinformation,
      title={Understanding Chain-of-Thought in LLMs through Information Theory}, 
      author={Jean-Francois Ton and Muhammad Faaiz Taufiq and Yang Liu},
      year={2024},
      eprint={2411.11984},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.11984}, 
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@misc{yu2025llmsreallythinkstepbystep,
      title={Do LLMs Really Think Step-by-step In Implicit Reasoning?}, 
      author={Yijiong Yu},
      year={2025},
      eprint={2411.15862},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.15862}, 
}

@inproceedings{zhang2022delving,
  title={Delving deep into the generalization of vision transformers under distribution shifts},
  author={Zhang, Chongzhi and Zhang, Mingyuan and Zhang, Shanghang and Jin, Daisheng and Zhou, Qiang and Cai, Zhongang and Zhao, Haiyu and Liu, Xianglong and Liu, Ziwei},
  booktitle={Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition},
  pages={7277--7286},
  year={2022}
}

@misc{zhang2023unveilingtransformerslegosynthetic,
      title={Unveiling Transformers with LEGO: a synthetic reasoning task}, 
      author={Yi Zhang and Arturs Backurs and Sébastien Bubeck and Ronen Eldan and Suriya Gunasekar and Tal Wagner},
      year={2023},
      eprint={2206.04301},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2206.04301}, 
}

@article{zhu2024deductive,
  title={Deductive beam search: Decoding deducible rationale for chain-of-thought reasoning},
  author={Zhu, Tinghui and Zhang, Kai and Xie, Jian and Su, Yu},
  journal={COLM},
  year={2024}
}

