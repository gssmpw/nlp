
\vspace{-1em}
\section{Introduction}

Transformer-based language models (LMs) \cite{brown2020language,chowdhery2022palm,touvron2023llama} have demonstrated unprecedented capabilities in knowledge retrieval \cite{kojima2022large,wei2022chain,wei2022emergent}  and reasoning \cite{hendrycks2020measuring,cobbe2021training}, driven by large-scale pretraining on diverse text corpora \cite{ouyang2022training,wei2021finetuned,longpre2023flan}. While these models excel at tasks with clear input-output mappings, their ability to generalize to compound tasks—those requiring dynamic planning, multi-step reasoning, and adaptation to evolving contexts—remains a critical challenge. Real-world applications such as GUI automation \cite{wang2024gui,yang2024aria, shen2024falcon, qinghong2024showui, verma2024adaptagent}, textual puzzle solving \cite{giadikiaroglou2024puzzle,saha2024language}, and strategic gameplay like poker \cite{guo2023suspicion,zhang2024agent,huang2024pokergpt} demand not only procedural knowledge but also the capacity to handle \textbf{distribution shift} between training and deployment environments. 

A central challenge lies in the misalignment between training data and deployment environments. For instance, consider a medical diagnosis system trained on historical patient records: it may falter when encountering novel symptom combinations during a disease outbreak or misalign with updated clinical guidelines (e.g., revised thresholds for "high-risk" biomarkers \cite{welsh2016prediction}). Such distribution shifts can significantly degrade the performance of LMs trained on limited training datasets, underscoring the importance of generalization for reliable deployment.


\begin{figure}
    \centering
    % \includegraphics[1.0 * \textwidth]{}
        \includegraphics[width=\linewidth]{figure/plot_combined_avg_id_ood.pdf}
    
    \caption{The illustration of the impact of the granularity of Chain-of-Thought on In-Distribution (IID) and Out-of-Distribution (OOD) performance. Left: IID performance. Right: OOD performance. Results are averaged over four compound tasks.  While models trained without CoT achieve high IID accuracy (~80\%), they exhibit a substantially poor generalization performance (~10\%) on OOD data.
}
    \label{fig:average}
\end{figure}

In this paper, we investigate how data collection impacts the generalization of LMs in compound tasks. Traditional approaches that prioritize direct instruction-result pairs (analogous to question-answering frameworks) optimize for task-specific accuracy but inadequately prepare models for compositional reasoning in unseen contexts. Recent advances in Chain-of-Thought (CoT) prompting \cite{wei2022chain}, which elicit intermediate reasoning steps, suggest that explicit process supervision can enhance generalization. However, acquiring high-quality CoT annotations at scale remains prohibitively expensive \cite{lightman2023let,kim2023cot}, raising a pivotal question: \textbf{How do different data collection strategies, \emph{e.g.}, result-oriented Q-A pairs and CoT—affect the generalization of LMs to novel compound tasks?}

To address this, we conduct controlled experiments using synthetic compound tasks that systematically introduce distribution shifts between training and evaluation environments. We evaluate LMs trained on two data paradigms: (1) result-oriented Q-A pairs and (2) CoT sequences of varying granularity. Our analysis reveals three critical insights:  (i) \textbf{Generalization Gap:} While both Q-A and CoT-trained LMs achieve near-perfect in-distribution accuracy, Q-A models exhibit severe performance degradation under distribution shifts—even with 10000k training examples. (ii) \textbf{Granularity-Generalization Tradeoff:} The granularity of CoT data strongly correlates with generalization performance; finer-grained CoT data leads to better generalization. (iii) \textbf{Sample Efficiency:} LMs trained with fine-grained CoT data demonstrate strong sample efficiency, achieving comparable generalization to Q-A pair training with substantially less data. These results suggest that even small amounts of high-quality CoT data can significantly enhance LM generalization, despite the challenges associated with its collection.


Inspired by the recent work \cite{liu2022transformers}, we further theoretically demonstrate that compound tasks inherently contain shortcuts—superficial patterns in Q-A data that enable high in-distribution accuracy but misalign with underlying reasoning principles. CoT training mitigates this by decomposing tasks into structured subtasks, forcing models to internalize valid reasoning paths. Leveraging transformer positional embeddings, we further show that explicitly emphasizing subtask conditions within CoT sequences further reduces shortcut reliance, especially in the long CoT setting.

 In summary, our paper makes the following key contributions: (1) We establish a controlled experimental framework to quantify the impact of data collection strategies on LM generalization under distribution shifts. Based on it, we demonstrate that LMs trained directly on question-answer pairs can achieve high accuracy on in-distribution data for new tasks but exhibit poor generalization, even when trained on substantial datasets (e.g., 10000k examples). (2) Through systematic scaling experiments, we demonstrate that fine-grained CoT data enhances generalization and sample efficiency, even with limited training data. (3) We provide theoretical insights into how CoT helps to mitigate shortcut learning, thus improving generalization and further demonstrate that, with longer CoT chains, repeating the sub-task condition can further improve LMs generalization capability. We believe our findings provide a reliable guide for data collection practices when leveraging LMs for novel tasks.
