%=============================================================================%


\section{Experiment}

In order to investigate how data collection methods impact LMs' generalization ability, we conducted controlled studies on three compound reasoning tasks with systematic distribution shifts. Our experiments address three key questions: (1) How does direct QA training compare to CoT in OOD generalization? (2) Can increasing data help direct QA training to generalize to OOD data? (3) Can repeat the conditions for each subtask of one compound task help generalization?
\subsection{Setting}
\subsubsection{Task and Experiment setting}
We evaluate the generalization ability of LMs through three compound reasoning tasks: (1) Longest Increasing Subsequence, (2) Multi-Step Path Counting, and (3) Equation Restoration with Variable Calculation. 
\begin{itemize}
    \item \textbf{Longest Increasing Subsequence (LIS):} Given an input sequence $X^n$ of integers, compute the length of the longest increasing subsequence. The model is trained on sequences of complexity levels $n_1=4$ and $n_2=16$, and evaluated on sequences of complexity $n_3=10$, examining the model's ability to generalize across different sequence lengths and positional dependencies.
    
    \item \textbf{Multi-Step Path Counting (MPC):} Given an input sequence $X^n$ specifying the available steps (where 0 indicates a forbidden step, 1 means an available step), compute the number of unique paths to reach position n using steps of size \{1,2,3\}. The model is trained on problems of complexity levels $n_1=20$ and $n_2=40$, and evaluated on problems of complexity $n_3=30$, testing the model's ability to generalize across different path constraints and combinatorial patterns.
    
    \item \textbf{Equation Restoration and Variable Computation (ERVC):} Given a dataset of observations containing $n$ variables and their values, recover $m$ underlying linear system of equations, and then use these recovered relationships to compute target variables under new value assignments. The model trains on systems of ERVC21-41($n_1=2$, $m_1=1$), ($n_2=4$, $m_2=1$) variables and equations for the first setting, and ERVC21-43($n_1=2$, $m_1=1$) and ($n_2=4$, $m_2=3$) for the second setting, while testing on ($n_3=3$, $m_3=1$) and ($n_3=3$, $m_3=2$) respectively, where distribution shifts arise from varying numbers of variables (n) and equations (m) in the system.
    
\end{itemize}

\subsubsection{Training Datasets Preparation}
For each task, we generated datasets follow the two formats:
%\vspace{-\baselineskip}
\begin{itemize}
    \item \textbf{Question-Answer (Q-A):} Datasets consist of input questions paired directly with the final answer, representing direct supervision without explicit reasoning steps. This corresponds to a CoT dropout rate of 0\% (CoT-0\%).
    \item \textbf{Question-Chain-of-Thought (Q-CoT):} Datasets include input questions paired with step-by-step Chain-of-Thought explanations leading to the final answer, providing process supervision. This represents a CoT dropout rate of 100\% (CoT-100\%).
\end{itemize}

To investigate the impact of partial CoT supervision, we also conducted ablation studies using probabilistic CoT dropout. In these settings, each step within a complete CoT demonstration has a probability of being retained in the training data. We explored dropout rates of 30\%, 50\%, and 70\% (CoT-30\%, CoT-50\%, CoT-70\%).  We focus on probabilistic dropout as preliminary experiments with fixed-portion CoT dropout showed significant OOD performance degradation.

For scaling experiments, we varied the dataset size from 1k to 30k samples for both Q-A and Q-CoT regimes. Models were trained on the ID data splits and evaluated on both ID and OOD splits to measure generalization gaps.

\subsubsection{Model Training and Evaluation}
For the Longest Increasing Subsequence and Multi-Step Path Counting tasks, we implemented a 6-layer transformer architecture trained from scratch, featuring an embedding size of 256/512 and 16 attention heads. The Equation Restoration tasks were approached differently, utilizing the Phi-3.5-mini-instruct model as the foundation for task-specific fine-tuning. All model variants incorporate Rotary Position Embedding.
In evaluating model performance, we employed different strategies for Q-A and Q-CoT data. For Q-A data in both the Longest Increasing Subsequence and Multi-Step Path Counting tasks, correctness is determined by comparing the token immediately preceding the \texttt{<eos>} token. For Q-CoT data, our evaluation extends beyond the final answer, incorporating intermediate computational tokens (such as $Q$ and $L$) that contribute to the solution. This comprehensive evaluation approach helps mitigate cases where incorrect reasoning processes might accidentally yield correct final answers through guessing.


\subsection{Results Analysis}

\begin{table}[htbp]
\vspace{-1em}
\caption{Performance comparison across MPC, LIS, ERVC tasks, where ID denotes the in-distribution task, OOD denotes out-of-distribution task, ID-OOD denotes the different between ID and OOD performance. }
% \vspace{-0.5em}
    \centering
    \resizebox{\columnwidth}{!}{
        \begin{tabular}{ccccccccc}
            \hline
            \multicolumn{1}{c}{\multirow{2}{*}{Task}} & \multicolumn{2}{c}{MPC} & \multicolumn{2}{c}{LIS} & \multicolumn{2}{c}{ERVC21-43} & \multicolumn{2}{c}{ERVC21-41} \\ \cline{2-9} 
            \multicolumn{1}{c}{} & Q-A & Q-CoT & Q-A & Q-CoT & Q-A & Q-CoT & Q-A & Q-CoT \\ \hline
            ID $\uparrow$ & 0.83 & 1.0 & 0.97 & 0.995 & 0.39 & 1.0 & 0.49 & 0.95 \\
            OOD $\uparrow$ & 0.21 & 0.81 & 0.14 & 0.75 & 0.02 & 0.959 & 0.126 & 0.905 \\
            ID-OOD $\downarrow$ & 0.62 & 0.19 & 0.83 & 0.245 & 0.37 & 0.041 & 0.364 & 0.045 \\ \hline
        \end{tabular}
    }
    \label{tab:performance gap}
     \vspace{-1em}
\end{table}

\subsubsection{Generalization Gap and Scaling Curves}

\begin{figure}
    \centering
    \subfigure[Multi-Step Path Count (MPC)]{
        \includegraphics[width=0.86\linewidth]{figure/plot_avg_ID_ood_3steps.pdf}
        \label{fig:ratio_3step}
    }\vfill
    \vspace{-0.5em}
    \subfigure[Longest Increasing Sequence (LIS)]{
        \includegraphics[width=0.86\linewidth]{figure/plot_lis_avg_ID_ood_LIS.pdf}
        \label{fig:lis_analysis}
    }\vfill
    \vspace{-0.5em}
    \subfigure[Equation Restoration and Variable Computation (ERVC21-43), number of equation varies.]{
        \includegraphics[width=0.86\linewidth]{figure/plot_2141_ID_ood_df.pdf}
        \label{fig:dec24}
    }\vfill
    \vspace{-0.5em}
    \subfigure[Equation Restoration and Variable Computation (ERVC21-41), number of variables in each equation varies.]{
        \includegraphics[width=0.86\linewidth]{figure/plot_2123_ID_ood_df.pdf}
        \label{fig:dec14}
    }
    \vspace{-1em}
    \caption{Impact of CoT granularity on In-Distribution and Out-of-Distribution performance across four tasks. Left panels show ID accuracy, right panels show OOD accuracy. While non-CoT models achieve high ID accuracy, they demonstrate significantly lower OOD generalization performance.}
    \vspace{-2em}
    \label{fig:combined}
\end{figure}

\begin{figure}[]
\centering
\subfigure[Data Scaling Curves in MPC task]{
    \includegraphics[width=1\linewidth]{figure/scaling_plot_3steps_scaling.pdf}
}\hfill
\subfigure[Data Scaling Curves in LIS task]{
    \includegraphics[width=1 \linewidth]{figure/scaling_plot_LIS_scaling.pdf}
    \label{fig:lis}
}   
\caption{The illustration of the impact of data scale on ID and OOD performance in the MPC and LIS tasks. Left: ID performance. Right: OOD performance. We can clearly see that even when the training data increases to 10M, LMs trained without CoT can exhibit near-perfect performance on training tasks, yet they still achieve only about ~30\% accuracy on OOD data.}
\label{fig:scaling_results}
\end{figure}

Figure \ref{fig:scaling_results} and Table \ref{tab:performance gap} presents the accuracy scaling curves for MPC and LIS tasks as a function of dataset size, comparing different CoT ratios (0\%, 30\%, 50\%, 70\%, and 100\%).  Consistent with the theoretical predictions derived from Theorem \ref{thm:kl-reduction} (referenced in the main paper), models trained with higher CoT percentages (CoT-70\%, CoT-100\%) exhibit significantly improved OOD generalization compared to direct QA models (CoT-0\%). This is manifested in both faster initial performance gains and higher sustained accuracy levels as dataset size increases.

Specifically, for Q-A (CoT-0\%), OOD performance plateaus rapidly with increasing data, indicating limited generalization. In contrast, Q-CoT models demonstrate continued OOD performance improvement with larger datasets, effectively mitigating the generalization gap.  The observed performance gap between Q-CoT and Q-A is qualitatively consistent with the upper bound on KL divergence predicted by Theorem \ref{thm:kl-reduction}, which suggests that CoT supervision reduces distribution shift by enhancing prefix coverage.

Furthermore, the degraded OOD performance observed in partial CoT settings (CoT-30\%, CoT-50\%) empirically supports the theoretical argument that incomplete reasoning chains diminish prefix coverage, leading to increased KL divergence and reduced generalization.

These empirical findings strongly validate the theoretical framework, demonstrating that CoT supervision enhances OOD generalization by encoding explicit reasoning steps. This structured reasoning process appears to transform novel OOD problems into more familiar, in-distribution-like problems, enabling better generalization.

\subsubsection{Granularity-Generalization Tradeoff}
Figure \ref{fig:combined} demonstrates a clear relationship between CoT step coverage and OOD generalization performance across tasks. Specifically, models trained with more complete CoT steps exhibit superior generalization capabilities on OOD samples. To formally characterize this empirical observation, we develop a theoretical framework in with a simple linear model that quantitatively explains how the omission of CoT steps during training systematically degrades model performance. Our analysis reveals that the degradation follows an exponential decay pattern with respect to the number of missing steps, highlighting the critical importance of maintaining granular reasoning steps for robust generalization. 

\subsubsection{Data Scaling Experiments}
To investigate the effect of training data scaling on generalization, we examined the performance trends as dataset size increased. As depicted in Figure~\ref{fig:scaling_results}, a key observation emerges: models trained without CoT supervision exhibit limited improvement in OOD generalization \textbf{despite substantial increases in training data size (up to 10,000k samples)}. While ID accuracy improves with larger datasets for these models, their OOD performance plateaus, \textbf{indicating that even the increased data cannot help generalization to novel compound tasks with direct Q-A training.} In stark contrast, models trained with CoT supervision demonstrate a distinct capacity to translate larger datasets into improved OOD generalization. 

% \vspace{-0.1em}
\subsubsection{Sample Efficiency}
As shown in Figure \ref{fig:scaling_results}, models trained with a higher percentage of CoT examples demonstrate superior sample efficiency in both MPC and LIS scaling experiments. Specifically, models with $70\%$ and $100\%$ CoT (CoT-$70\%$ and CoT-$100\%$) achieve a higher OOD accuracy with significantly smaller dataset sizes compared to models trained with lower CoT percentages. This effect is particularly pronounced in the small data regime (1k-10k samples), where CoT-$100\%$ maintains a relatively robust OOD performance (~0.55-0.75 accuracy) while models with less CoT supervision struggle (below 0.4 accuracy). This suggests that incorporating more reasoning steps through CoT not only improves overall performance but also enables more efficient learning from limited training data.
% \vspace{-0.1em}
\subsubsection{Recap Condition Ablation}
We investigate the impact of the recap condition on model performance in Equation Restoration tasks in Table \ref{tab:model-comparison}. For ERVC21-43, models trained with the recap condition achieve a high training accuracy of 0.974 and maintain this level of performance on the OOD set (0.974).  In contrast, models trained without the recap condition exhibit significantly lower accuracy on both the training set (0.436) and the OOD set (0.126). A similar trend is observed for ERVC21-41. The model with the recap condition achieves near-perfect training accuracy (0.997) and strong OOD performance (0.952).  Conversely, the model without the recap condition shows substantially reduced accuracy in both training (0.638) and OOD settings (0.558). These results strongly suggest that the recap condition of each subtask plays a critical role in the generalization of LMs and and validate our theoretical propositions outlined in Section \ref{sec:recap-theory}
\begin{table}[]
\vspace{-1em}
\caption{Model performance comparisons on the with and without recap condition on the Equation Restoration Task, where }
% \vspace{-0.5em}
    \centering
    \resizebox{0.8\columnwidth}{!}{ % 
\begin{tabular}{ccccc}
\hline
Task                & \multicolumn{2}{c}{ERVC21-43} & \multicolumn{2}{c}{ERVC21-41} \\ \hline
Dataset               & Training     & OOD       & Training     & OOD       \\ \hline
w/ recap condition  & 0.974        & 0.947     & 0.997        & 0.952     \\
w/o recap condition & 0.436        & 0.126     & 0.638        & 0.558     \\ \hline
\end{tabular}
 }
\label{tab:model-comparison}
 \vspace{-1.em}
\end{table}



