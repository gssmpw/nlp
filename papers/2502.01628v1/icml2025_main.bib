




@article{olah2020zoom,
  author = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  title = {Zoom In: An Introduction to Circuits},
  journal = {Distill},
  year = {2020},
  note = {https://distill.pub/2020/circuits/zoom-in},
  doi = {10.23915/distill.00024.001}
}

@article{power2022grokking,
  title={Grokking: Generalization beyond overfitting on small algorithmic datasets},
  author={Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
  journal={arXiv preprint arXiv:2201.02177},
  year={2022}
}

@article{todd2023function,
  title={Function vectors in large language models},
  author={Todd, Eric and Li, Millicent L and Sharma, Arnab Sen and Mueller, Aaron and Wallace, Byron C and Bau, David},
  journal={arXiv preprint arXiv:2310.15213},
  year={2023}
}

@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{li2024enhancing,
  title={Enhancing hydrological extremes prediction accuracy: Integrating diverse loss functions in Transformer models},
  author={Li, Xue and Sun, Qi-Liang and Zhang, Yanfei and Sha, Jian and Zhang, Man},
  journal={Environmental Modelling \& Software},
  volume={177},
  pages={106042},
  year={2024},
  publisher={Elsevier}
}

@article{olah2020circuits,
  title={Zoom in: An introduction to circuits},
  author={Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  journal={Distill},
  volume={5},
  number={3},
  pages={e00024--001},
  year={2020}
}

@article{bom2023wind,
  title={Hybrid wind speed forecasting using ICEEMDAN and transformer model with novel loss function},
  author={Bommidi, Bala Saibabu and Teeparthi, Kiran and Kosana, Vishalteja},
  journal={Energy},
  volume={265},
  pages={126383},
  year={2023},
  publisher={Elsevier}
}


@inproceedings{bosco2024cardio,
  title={Echocardiographic Image Segmentation with Vision Transformers: A Comparative Analysis of Different Loss Functions},
  author={Bosco, Edoardo and Magenes, Giovanni and Matrone, Giulia},
  booktitle={2024 IEEE International Symposium on Medical Measurements and Applications (MeMeA)},
  pages={1--6},
  year={2024},
  organization={IEEE}
}

@article{seber2024protein,
  title={Predicting O-GlcNAcylation Sites in Mammalian Proteins with Transformers and RNNs Trained with a New Loss Function},
  author={Seber, Pedro},
  journal={arXiv preprint arXiv:2402.17131},
  year={2024}
}

% specific loss papers

@inproceedings{sal2017tversky,
  title={Tversky loss function for image segmentation using 3D fully convolutional deep networks},
  author={Salehi, Seyed Sadegh Mohseni and Erdogmus, Deniz and Gholipour, Ali},
  booktitle={International workshop on machine learning in medical imaging},
  pages={379--387},
  year={2017},
  organization={Springer}
}

@inproceedings{demir2023topo,
  title={Topology-Aware Focal Loss for 3D Image Segmentation},
  author={Demir, Andac and Massaad, Elie and Kiziltan, Bulent},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={580--589},
  year={2023}
}

@inproceedings{sudre2017dice,
  title={Generalised dice overlap as a deep learning loss function for highly unbalanced segmentations},
  author={Sudre, Carole H and Li, Wenqi and Vercauteren, Tom and Ourselin, Sebastien and Jorge Cardoso, M},
  booktitle={Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support: Third International Workshop, DLMIA 2017, and 7th International Workshop, ML-CDS 2017, Held in Conjunction with MICCAI 2017, Qu{\'e}bec City, QC, Canada, September 14, Proceedings 3},
  pages={240--248},
  year={2017},
  organization={Springer}
}

@article{lin2017focal,
  title={Focal Loss for Dense Object Detection},
  author={Lin, T},
  journal={arXiv preprint arXiv:1708.02002},
  year={2017}
}

@article{olsson2022context,
  title={In-context learning and induction heads},
  author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and others},
  journal={Transformer Circuits Thread},
  year={2022}
}
@article{ding2024survival,
  title={Survival of the Fittest Representation: A Case Study with Modular Addition},
  author={Ding, Xiaoman Delores and Guo, Zifan Carl and Michaud, Eric J and Liu, Ziming and Tegmark, Max},
  journal={arXiv preprint arXiv:2405.17420},
  year={2024}
}

@article{zhong2024clock,
  title={The clock and the pizza: Two stories in mechanistic explanation of neural networks},
  author={Zhong, Ziqian and Liu, Ziming and Tegmark, Max and Andreas, Jacob},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{liu2023seeing,
  title={Seeing is believing: Brain-inspired modular training for mechanistic interpretability},
  author={Liu, Ziming and Gan, Eric and Tegmark, Max},
  journal={Entropy},
  volume={26},
  number={1},
  pages={41},
  year={2023},
  publisher={MDPI}
}

@misc{templeton2024claude,
  title={Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet. Transformer Circuits Thread},
  author={Templeton, Adly and Conerly, Tom and Marcus, Jonathan and Lindsey, Jack and Bricken, Trenton and Chen, Brian and Pearce, Adam and Citro, Craig and Ameisen, Emmanuel and Jones, Andy and others},
  journal={Transformer Circuits Thread},
  year={2024}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@article{bejani2021systematic,
  title={A systematic review on overfitting control in shallow and deep neural networks},
  author={Bejani, Mohammad Mahdi and Ghatee, Mehdi},
  journal={Artificial Intelligence Review},
  pages={1--48},
  year={2021},
  publisher={Springer}
}

@book{roberts2022principles,
  title={The principles of deep learning theory},
  author={Roberts, Daniel A and Yaida, Sho and Hanin, Boris},
  year={2022},
  publisher={Cambridge University Press Cambridge, MA, USA}
}

@article{halverson2021neural,
  title={Neural networks and quantum field theory},
  author={Halverson, James and Maiti, Anindita and Stoner, Keegan},
  journal={Machine Learning: Science and Technology},
  volume={2},
  number={3},
  pages={035002},
  year={2021},
  publisher={IOP Publishing}
}

@article{kunin2020neural,
  title={Neural mechanics: Symmetry and broken conservation laws in deep learning dynamics},
  author={Kunin, Daniel and Sagastuy-Brena, Javier and Ganguli, Surya and Yamins, Daniel LK and Tanaka, Hidenori},
  journal={arXiv preprint arXiv:2012.04728},
  year={2020}
}

@inproceedings{gao2020free,
  title={A free-energy principle for representation learning},
  author={Gao, Yansong and Chaudhari, Pratik},
  booktitle={International Conference on Machine Learning},
  pages={3367--3376},
  year={2020},
  organization={PMLR}
}


@article{liu2021towards,
  title={Towards out-of-distribution generalization: A survey},
  author={Liu, Jiashuo and Shen, Zheyan and He, Yue and Zhang, Xingxuan and Xu, Renzhe and Yu, Han and Cui, Peng},
  journal={arXiv preprint arXiv:2108.13624},
  year={2021}
}

@article{ye2021towards,
  title={Towards a theoretical framework of out-of-distribution generalization},
  author={Ye, Haotian and Xie, Chuanlong and Cai, Tianle and Li, Ruichen and Li, Zhenguo and Wang, Liwei},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={23519--23531},
  year={2021}
}

@article{liu2022omnigrok,
  title={Omnigrok: Grokking beyond algorithmic data},
  author={Liu, Ziming and Michaud, Eric J and Tegmark, Max},
  journal={arXiv preprint arXiv:2210.01117},
  year={2022}
}

@article{he2023machine,
  title={Machine-learning mathematical structures},
  author={He, Yang-Hui},
  journal={International Journal of Data Science in the Mathematical Sciences},
  volume={1},
  number={01},
  pages={23--47},
  year={2023},
  publisher={World Scientific}
}

@article{gukov2021learning,
  title={Learning to unknot},
  author={Gukov, Sergei and Halverson, James and Ruehle, Fabian and Su{\l}kowski, Piotr},
  journal={Machine Learning: Science and Technology},
  volume={2},
  number={2},
  pages={025035},
  year={2021},
  publisher={IOP Publishing}
}

@article{baek2024geneft,
  title={GenEFT: Understanding Statics and Dynamics of Model Generalization via Effective Theory},
  author={Baek, David D and Liu, Ziming and Tegmark, Max},
  journal={arXiv preprint arXiv:2402.05916},
  year={2024}
}

@article{mu2020compositional,
  title={Compositional explanations of neurons},
  author={Mu, Jesse and Andreas, Jacob},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={17153--17163},
  year={2020}
}

@article{hupkes2020compositionality,
  title={Compositionality decomposed: How do neural networks generalise?},
  author={Hupkes, Dieuwke and Dankers, Verna and Mul, Mathijs and Bruni, Elia},
  journal={Journal of Artificial Intelligence Research},
  volume={67},
  pages={757--795},
  year={2020}
}

@article{gurnee2023language,
  title={Language models represent space and time},
  author={Gurnee, Wes and Tegmark, Max},
  journal={arXiv preprint arXiv:2310.02207},
  year={2023}
}

@article{hernandez2023linearity,
  title={Linearity of relation decoding in transformer language models},
  author={Hernandez, Evan and Sharma, Arnab Sen and Haklay, Tal and Meng, Kevin and Wattenberg, Martin and Andreas, Jacob and Belinkov, Yonatan and Bau, David},
  journal={arXiv preprint arXiv:2308.09124},
  year={2023}
}

@article{zhang2021survey,
  title={A survey on neural network interpretability},
  author={Zhang, Yu and Ti{\v{n}}o, Peter and Leonardis, Ale{\v{s}} and Tang, Ke},
  journal={IEEE Transactions on Emerging Topics in Computational Intelligence},
  volume={5},
  number={5},
  pages={726--742},
  year={2021},
  publisher={IEEE}
}

@article{bereska2024mechanistic,
  title={Mechanistic Interpretability for AI Safety--A Review},
  author={Bereska, Leonard and Gavves, Efstratios},
  journal={arXiv preprint arXiv:2404.14082},
  year={2024}
}

@inproceedings{hewitt2019structural,
  title={A structural probe for finding syntax in word representations},
  author={Hewitt, John and Manning, Christopher D},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4129--4138},
  year={2019}
}

@article{pimentel2020information,
  title={Information-theoretic probing for linguistic structure},
  author={Pimentel, Tiago and Valvoda, Josef and Maudslay, Rowan Hall and Zmigrod, Ran and Williams, Adina and Cotterell, Ryan},
  journal={arXiv preprint arXiv:2004.03061},
  year={2020}
}

@inproceedings{dalvi2019one,
  title={What is one grain of sand in the desert? analyzing individual neurons in deep nlp models},
  author={Dalvi, Fahim and Durrani, Nadir and Sajjad, Hassan and Belinkov, Yonatan and Bau, Anthony and Glass, James},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={6309--6317},
  year={2019}
}

@article{bubeck2023sparks,
  title={Sparks of artificial general intelligence: Early experiments with gpt-4},
  author={Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and others},
  journal={arXiv preprint arXiv:2303.12712},
  year={2023}
}

@article{abdou2021can,
  title={Can language models encode perceptual structure without grounding? a case study in color},
  author={Abdou, Mostafa and Kulmizev, Artur and Hershcovich, Daniel and Frank, Stella and Pavlick, Ellie and S{\o}gaard, Anders},
  journal={arXiv preprint arXiv:2109.06129},
  year={2021}
}

@article{li2021implicit,
  title={Implicit representations of meaning in neural language models},
  author={Li, Belinda Z and Nye, Maxwell and Andreas, Jacob},
  journal={arXiv preprint arXiv:2106.00737},
  year={2021}
}

@article{tegmark2023provably,
  title={Provably safe systems: the only path to controllable agi},
  author={Tegmark, Max and Omohundro, Steve},
  journal={arXiv preprint arXiv:2309.01933},
  year={2023}
}

@article{michaud2024quantization,
  title={The quantization model of neural scaling},
  author={Michaud, Eric and Liu, Ziming and Girit, Uzay and Tegmark, Max},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@techreport{claude_3_tech_report,
  title={The Claude 3 Model Family: Opus, Sonnet, Haiku},
  author={Anthropic},
  year={2024},
  institution={Anthropic}
}

@article{gemini_tech_report,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{gpt_4_tech_report,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@misc{ wiki:kennedy,
  author = {{Kennedy Family Tree}},
  title = "Kennedy Family Tree --- {W}ikipedia{,} The Free Encyclopedia",
  year = "2023",
  url = "https://en.wikipedia.org/wiki/Template:Kennedy_family_tree",
  note = "[Online; accessed 22-May-2024]"
}


@article{huh2024platonic,
  title={The platonic representation hypothesis},
  author={Huh, Minyoung and Cheung, Brian and Wang, Tongzhou and Isola, Phillip},
  journal={arXiv preprint arXiv:2405.07987},
  year={2024}
}

@article{bansal2021revisiting,
  title={Revisiting model stitching to compare neural representations},
  author={Bansal, Yamini and Nakkiran, Preetum and Barak, Boaz},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={225--236},
  year={2021}
}

@article{sucholutsky2023getting,
  title={Getting aligned on representational alignment},
  author={Sucholutsky, Ilia and Muttenthaler, Lukas and Weller, Adrian and Peng, Andi and Bobu, Andreea and Kim, Been and Love, Bradley C and Grant, Erin and Achterberg, Jascha and Tenenbaum, Joshua B and others},
  journal={arXiv preprint arXiv:2310.13018},
  year={2023}
}

@article{lad2024remarkable,
  title={The Remarkable Robustness of LLMs: Stages of Inference?},
  author={Lad, Vedang and Gurnee, Wes and Tegmark, Max},
  journal={arXiv preprint arXiv:2406.19384},
  year={2024}
}

@article{dalrymple2024towards,
  title={Towards Guaranteed Safe AI: A Framework for Ensuring Robust and Reliable AI Systems},
  author={Dalrymple, David and Skalse, Joar and Bengio, Yoshua and Russell, Stuart and Tegmark, Max and Seshia, Sanjit and Omohundro, Steve and Szegedy, Christian and Goldhaber, Ben and Ammann, Nora and others},
  journal={arXiv preprint arXiv:2405.06624},
  year={2024}
}

@article{michaud2024opening,
  title={Opening the AI black box: program synthesis via mechanistic interpretability},
  author={Michaud, Eric J and Liao, Isaac and Lad, Vedang and Liu, Ziming and Mudide, Anish and Loughridge, Chloe and Guo, Zifan Carl and Kheirkhah, Tara Rezaei and Vukeli{\'c}, Mateja and Tegmark, Max},
  journal={arXiv preprint arXiv:2402.05110},
  year={2024}
}

@article{liu2022towards,
  title={Towards understanding grokking: An effective theory of representation learning},
  author={Liu, Ziming and Kitouni, Ouail and Nolte, Niklas S and Michaud, Eric and Tegmark, Max and Williams, Mike},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={34651--34663},
  year={2022}
}

@article{engels2024not,
  title={Not All Language Model Features Are Linear},
  author={Engels, Joshua and Liao, Isaac and Michaud, Eric J and Gurnee, Wes and Tegmark, Max},
  journal={arXiv preprint arXiv:2405.14860},
  year={2024}
}

@article{marks2023geometry,
  title={The geometry of truth: Emergent linear structure in large language model representations of true/false datasets},
  author={Marks, Samuel and Tegmark, Max},
  journal={arXiv preprint arXiv:2310.06824},
  year={2023}
}

@article{heinzerling2024monotonic,
  title={Monotonic representation of numeric properties in language models},
  author={Heinzerling, Benjamin and Inui, Kentaro},
  journal={arXiv preprint arXiv:2403.10381},
  year={2024}
}

@inproceedings{pennington2014glove,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={1532--1543},
  year={2014}
}

@inproceedings{ma2015using,
  title={Using Word2Vec to process big text data},
  author={Ma, Long and Zhang, Yanqing},
  booktitle={2015 IEEE International Conference on Big Data (Big Data)},
  pages={2895--2897},
  year={2015},
  organization={IEEE}
}

@inproceedings{drozd2016word,
  title={Word embeddings, analogies, and machine learning: Beyond king-man+ woman= queen},
  author={Drozd, Aleksandr and Gladkova, Anna and Matsuoka, Satoshi},
  booktitle={Proceedings of coling 2016, the 26th international conference on computational linguistics: Technical papers},
  pages={3519--3530},
  year={2016}
}

@article{nanda2023emergent,
  title={Emergent linear representations in world models of self-supervised sequence models},
  author={Nanda, Neel and Lee, Andrew and Wattenberg, Martin},
  journal={arXiv preprint arXiv:2309.00941},
  year={2023}
}

@article{li2022emergent,
  title={Emergent world representations: Exploring a sequence model trained on a synthetic task},
  author={Li, Kenneth and Hopkins, Aspen K and Bau, David and Vi{\'e}gas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
  journal={arXiv preprint arXiv:2210.13382},
  year={2022}
}

@inproceedings{wang2014knowledge,
  title={Knowledge graph embedding by translating on hyperplanes},
  author={Wang, Zhen and Zhang, Jianwen and Feng, Jianlin and Chen, Zheng},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={28},
  number={1},
  year={2014}
}

@inproceedings{lin2015learning,
  title={Learning entity and relation embeddings for knowledge graph completion},
  author={Lin, Yankai and Liu, Zhiyuan and Sun, Maosong and Liu, Yang and Zhu, Xuan},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={29},
  number={1},
  year={2015}
}

@inproceedings{trouillon2016complex,
  title={Complex embeddings for simple link prediction},
  author={Trouillon, Th{\'e}o and Welbl, Johannes and Riedel, Sebastian and Gaussier, {\'E}ric and Bouchard, Guillaume},
  booktitle={International conference on machine learning},
  pages={2071--2080},
  year={2016},
  organization={PMLR}
}

@article{cao2024knowledge,
  title={Knowledge graph embedding: A survey from the perspective of representation spaces},
  author={Cao, Jiahang and Fang, Jinyuan and Meng, Zaiqiao and Liang, Shangsong},
  journal={ACM Computing Surveys},
  volume={56},
  number={6},
  pages={1--42},
  year={2024},
  publisher={ACM New York, NY}
}

@inproceedings{jiang2024look,
  title={Look Ahead Text Understanding and LLM Stitching},
  author={Jiang, Junlin Julian and Li, Xin},
  booktitle={Proceedings of the International AAAI Conference on Web and Social Media},
  volume={18},
  pages={751--760},
  year={2024}
}

@article{brown2023understanding,
  title={Understanding the inner workings of language models through representation dissimilarity},
  author={Brown, Davis and Godfrey, Charles and Konz, Nicholas and Tu, Jonathan and Kvinge, Henry},
  journal={arXiv preprint arXiv:2310.14993},
  year={2023}
}

@article{li2019visual,
  title={Visual to text: Survey of image and video captioning},
  author={Li, Sheng and Tao, Zhiqiang and Li, Kang and Fu, Yun},
  journal={IEEE Transactions on Emerging Topics in Computational Intelligence},
  volume={3},
  number={4},
  pages={297--312},
  year={2019},
  publisher={IEEE}
}

@article{shi2023learning,
  title={Learning video-text aligned representations for video captioning},
  author={Shi, Yaya and Xu, Haiyang and Yuan, Chunfeng and Li, Bing and Hu, Weiming and Zha, Zheng-Jun},
  journal={ACM Transactions on Multimedia Computing, Communications and Applications},
  volume={19},
  number={2},
  pages={1--21},
  year={2023},
  publisher={ACM New York, NY}
}

@inproceedings{iashin2020multi,
  title={Multi-modal dense video captioning},
  author={Iashin, Vladimir and Rahtu, Esa},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops},
  pages={958--959},
  year={2020}
}

@article{hendel2023context,
  title={In-context learning creates task vectors},
  author={Hendel, Roee and Geva, Mor and Globerson, Amir},
  journal={arXiv preprint arXiv:2310.15916},
  year={2023}
}

@article{park2024geometry,
  title={The Geometry of Categorical and Hierarchical Concepts in Large Language Models},
  author={Park, Kiho and Choe, Yo Joong and Jiang, Yibo and Veitch, Victor},
  journal={arXiv preprint arXiv:2406.01506},
  year={2024}
}

@article{park2024iclr,
  title={ICLR: In-Context Learning of Representations},
  author={Park, Core Francisco and Lee, Andrew and Lubana, Ekdeep Singh and Yang, Yongyi and Okawa, Maya and Nishi, Kento and Wattenberg, Martin and Tanaka, Hidenori},
  journal={arXiv preprint arXiv:2501.00070},
  year={2024}
}

@article{novak2018sensitivity,
  title={Sensitivity and generalization in neural networks: an empirical study},
  author={Novak, Roman and Bahri, Yasaman and Abolafia, Daniel A and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1802.08760},
  year={2018}
}

@article{li2024geometry,
  title={The geometry of concepts: Sparse autoencoder feature structure},
  author={Li, Yuxiao and Michaud, Eric J and Baek, David D and Engels, Joshua and Sun, Xiaoqing and Tegmark, Max},
  journal={arXiv preprint arXiv:2410.19750},
  year={2024}
}

@article{luo2021learning,
  title={Learning with smooth Hinge losses},
  author={Luo, JunRu and Qiao, Hong and Zhang, Bo},
  journal={Neurocomputing},
  volume={463},
  pages={379--387},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{li2024deepspeed,
  title={Deepspeed data efficiency: Improving deep learning model quality and training efficiency via efficient data sampling and routing},
  author={Li, Conglong and Yao, Zhewei and Wu, Xiaoxia and Zhang, Minjia and Holmes, Connor and Li, Cheng and He, Yuxiong},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={16},
  pages={18490--18498},
  year={2024}
}

@article{wang2024patch,
  title={Patch diffusion: Faster and more data-efficient training of diffusion models},
  author={Wang, Zhendong and Jiang, Yifan and Zheng, Huangjie and Wang, Peihao and He, Pengcheng and Wang, Zhangyang and Chen, Weizhu and Zhou, Mingyuan and others},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}
