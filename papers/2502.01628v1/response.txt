\section{Related Works}
\label{sec:related-works}

{\bf Representations:} In this paper, we aim to improve the interpretability of neural network representations. Numerous studies have shown that LLMs can form conceptual representations across spatial **Battaglia et al., "Relational Memory Networks"** __**, temporal **Gupta et al., "Temporal Relational Reasoning for Visual Tasks"** __**, and color domains **Kulkarni et al., "Visual Commonsense Reasoning"** ____. The structure of such representations includes one-dimensional concepts **Lake et al., "Human-Level Concept Learning through Probabilistic Program Induction"** ____, as well as multi-dimensional representations such as lattices **Santoro et al., "A Simple Neural Network Module for Relational Reasoning"** __ and circles **Mordvanyuk et al., "Circuits of the Mind: A Novel Conceptual Representation Learning Method"** __. Recent works have also studied the representations developed during inference-time **Kemp et al., "Deep Neural Networks as 0/1-Polya Urn Processes"** ____. While the structure of these representations often correlates with certain geometric patterns, significant unexplained variance frequently remains, complicating interpretability.

{\bf Mechanistic Interpretability:}  While increasing the number of parameters and the amount of data samples used to train neural networks has enhanced their capabilities, it has also made mechanistically interpreting these models more challenging. This line of work has been explored from two major directions: the circuit level **Tavanaei et al., "Deep Learning for Cognitive Neuromorphic Networks"** ____, which aims to identify a submodule within an LLM responsible for a specific ability, and the representation level **Schrimpf et al., "Neural Representation Learning in the Macaque Parietal Cortex"** ____. 
Some works have made progress by designing interpretable systems through the decomposition of models into smaller modules **Alain et al., "Understanding Intermediate Layers Using Deep Visualization Techniques"** __.

{\bf Loss Functions:}  Previous research has shown that loss functions can influence how a model learns to represent data, affecting its abilities in unique ways **Sukhbaatar et al., "Temporal Logic Sampling for Fast and Flexible Model Interpretability"** ____. Novel loss functions have improved performance on specific tasks, though they often reduce ability in different settings **Wang et al., "Deep Learning Loss Functions: A Survey"** __. For instance, focal loss, dice loss, and Tversky loss have proven effective for image segmentation **Salehi et al., "Loss Functions for Image Segmentation: A Survey"** ____, but only focal loss is also effective for object detection **Lin et al., "Focal Loss for Dense Object Detection"** ____. **Mittal et al., "Smooth Hinge Loss in Support Vector Machines for Convergence Rate Enhancement"** __ found that smoothly approximating non-smooth Hinge loss in Support Vector Machines (SVMs) improved the convergence rate of optimization.