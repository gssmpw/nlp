%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{comment}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\usepackage{bm}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Harmonic Loss Trains Interpretable AI Models}

\begin{document}

\twocolumn[
\icmltitle{Harmonic Loss Trains Interpretable AI Models}
% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{David D. Baek}{equal,yyy}
\icmlauthor{Ziming Liu}{equal,yyy}
\icmlauthor{Riya Tyagi}{yyy}
\icmlauthor{Max Tegmark}{yyy}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Massachusetts Institute of Technology, Cambridge, MA, USA}

\icmlcorrespondingauthor{David D. Baek}{dbaek@mit.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
In this paper, we introduce \textbf{harmonic loss} as an alternative to the standard cross-entropy loss for training neural networks and large language models (LLMs). Harmonic loss enables improved interpretability and faster convergence, owing to its scale invariance and finite convergence point by design, which can be interpreted as a class center. We first validate the performance of harmonic models across algorithmic, vision, and language datasets. Through extensive experiments, we demonstrate that models trained with harmonic loss outperform standard models by:
(a) enhancing interpretability,
(b) requiring less data for generalization, and
(c) reducing grokking. Moreover, we compare a GPT-2 model trained with harmonic loss to the standard GPT-2, illustrating that the harmonic model develops more interpretable representations. Looking forward, we believe harmonic loss has the potential to become a valuable tool in domains with limited data availability or in high-stakes applications where interpretability and reliability are paramount, paving the way for more robust and efficient neural network models.
\end{abstract}



\section{Introduction}

In recent years, machine learning models have gained significant popularity, profoundly impacting various aspects of daily life. Consequently, it has become increasingly important to thoroughly understand the behavior of neural networks. One particularly intriguing characteristic of neural networks is their ability to generalize -- empirical evidence shows that neural networks can perform well on unseen data not explicitly encountered during training \citep{novak2018sensitivity}. This remarkable ability stems from the networksâ€™ capacity to learn generalizable representations and algorithms through training.

However, current models face three key challenges when it comes to generalization:

{\bf (1) Lack of interpretability:} Neural networks often lack interpretability. This opacity limits our understanding of how these models arrive at their conclusions, which is a critical issue in high-stakes applications like healthcare, finance, and autonomous systems. While mutiple research efforts have advanced our insight into the inner workings of LLMs \citep{bereska2024mechanistic}, we are still far from fully explaining their outputs. Ultimately, we believe it is crucial to design systems that are interpretable by design. Without interpretability, it becomes challenging to diagnose errors, ensure fairness, or build trust in a model's decisions.

{\bf (2) Low data efficiency:}
Generalization often requires vast and diverse training data. This raises a critical question: can models generalize effectively with less data? This issue is especially relevant in domains where data availability is scarce, such as rare disease diagnosis, low-resource language processing, or specialized scientific fields like materials science or drug discovery. Previous approaches for improving neural network generalization include efficient data sampling \citep{li2024deepspeed} and modifications to the training procedure to accelerate training \citep{wang2024patch}. However, these methods focus on optimizing existing training procedures rather than addressing the core issues in model design.

{\bf (3) Delayed generalization (grokking):}  
Models sometimes experience a phenomenon known as ``grokking,'' \cite{power2022grokking,liu2021towards} where there is a noticeable delay between the convergence of the training loss and the convergence of the test loss. This gap is problematic because: (i) it complicates determining the optimal point to stop training in order to achieve generalization, and (ii) it necessitates extended computation time and resources to continue training until grokking occurs.

As the saying goes, ``The devil is in the details.'' We attribute these three challenges in part to the widespread use of cross-entropy loss (for classification) and propose  \textbf{harmonic loss} as an alternative. Harmonic loss has two desirable mathematical properties that enable faster convergence and improved interpretability: (1) scale invariance, and (2) a finite convergence point, which can be interpreted as a class center. Through a comprehensive set of experiments, we demonstrate that models trained with harmonic loss outperform standard models in terms of reducing grokking, requiring less data for generalization, and enhancing interpretability. Furthermore, we compare a GPT-2 model trained with harmonic loss to the standard GPT-2 and show that the harmonic model develops more interpretable representations.

The remainder of this paper is organized as follows: In Section \ref{sec:related-works}, we review the relevant literature. Section \ref{sec:harmonic-loss} introduces the principles underlying harmonic loss and explains why it is preferable to cross-entropy loss in terms of generalization capabilities. Section \ref{sec:toy-exp} details a comprehensive set of experiments on algorithmic datasets, demonstrating that models trained with harmonic loss consistently outperform standard models. In Section \ref{sec:mnist}, we demonstrate the performance of harmonic models on the vision task of MNIST digit classification. In Section \ref{sec:gpt2-exp}, we extend our analysis to large models, illustrating that the advantages of harmonic loss also hold at scale. Finally, we conclude the paper in Section \ref{sec:conclusion}.

\section{Related Works}
\label{sec:related-works}

{\bf Representations:} In this paper, we aim to improve the interpretability of neural network representations. Numerous studies have shown that LLMs can form conceptual representations across spatial \citep{gurnee2023language}, temporal \citep{li2021implicit}, and color domains \citep{abdou2021can}. The structure of such representations includes one-dimensional concepts \citep{gurnee2023language, marks2023geometry, heinzerling2024monotonic, park2024geometry}, as well as multi-dimensional representations such as lattices \citep{michaud2024opening,li2024geometry} and circles \citep{liu2022towards, engels2024not}. Recent works have also studied the representations developed during inference-time \cite{park2024iclr}. While the structure of these representations often correlates with certain geometric patterns, significant unexplained variance frequently remains, complicating interpretability.

{\bf Mechanistic Interpretability:}  While increasing the number of parameters and the amount of data samples used to train neural networks has enhanced their capabilities, it has also made mechanistically interpreting these models more challenging. This line of work has been explored from two major directions: the circuit level \citep{michaud2024opening, olah2020circuits, olsson2022context, templeton2024claude}, which aims to identify a submodule within an LLM responsible for a specific ability, and the representation level \citep{liu2022omnigrok, ding2024survival, zhong2024clock}. 
Some works have made progress by designing interpretable systems through the decomposition of models into smaller modules \citep{olah2020circuits, liu2023seeing}.

{\bf Loss Functions:}  Previous research has shown that loss functions can influence how a model learns to represent data, affecting its abilities in unique ways \citep{li2024enhancing, bosco2024cardio}. Novel loss functions have improved performance on specific tasks, though they often reduce ability in different settings \citep{bom2023wind, seber2024protein}. For instance, focal loss, dice loss, and Tversky loss have proven effective for image segmentation \citep{sudre2017dice, demir2023topo, sal2017tversky}, but only focal loss is also effective for object detection \citep{lin2017focal}. \citet{luo2021learning} found that smoothly approximating non-smooth Hinge loss in Support Vector Machines (SVMs) improved the convergence rate of optimization.


%CE, MSE, Hinge loss, Focal Loss,  BIMT perhaps


\section{Harmonic Loss}
\label{sec:harmonic-loss}
\begin{figure*}
    \centering
    \includegraphics[width=0.99\linewidth]{figures/ce_vs_harmonic.pdf}
    \vskip -0.1in
    \caption{Cross-entropy loss versus harmonic loss (ours). (a) Definitions. Cross-entropy loss leverages the inner product as the similarity metric, whereas the harmonic loss uses Euclidean distance. (b) Toy case 1 with two points (classes). Both the harmonic loss and the $l_2$ weight norm converge faster for the harmonic loss. (c) Toy case 2 with five points (classes). Harmonic loss can pick out the red point in the middle. By contrast, the cross-entropy loss cannot, since the red point is not linearly separable from other points. The weight matrices are also more interpretable with harmonic loss than with cross-entropy loss.}
    \label{fig:ce-harmonic}
\end{figure*}

We first review cross-entropy loss and present the harmonic loss, visualized in Figure~\ref{fig:ce-harmonic} (a). Denote the unembedding matrix as $\bm{W}\in\mathbb{R}^{N\times V}$ ($N$ is the embedding dimension, $V$ is the vocabulary size), and the penultimate representation (the representation prior to the unembedding matrix) as $\bm{x}\in\mathbb{R}^{N}$.

{\bf Cross-entropy loss:} Logits $\bm{y}$ are defined as the matrix-vector multiplication, i.e., $\bm{y}=\bm{W}^T\bm{x}\in\mathbb{R}^V$ (ignoring biases), or 
\begin{equation}
y_i = \bm{w}_i\cdot\bm{x},
\end{equation}
where $\bm{w}_i$ is the $i^{\rm th}$ column of $\bm{W}$.  Probability $\bm{p}$ can be obtained by applying softmax to $\bm{y}$, i.e., 

\begin{equation}
p_i={\rm SoftMax}(\bm{y})_i\equiv\frac{{\rm exp}(y_i)}{\sum_j {\rm exp}(y_j)}. 
\end{equation}
Suppose the real class label is $c$, then loss $\ell = -{\rm log}\ p_c$. For notational simplicity, we call a linear layer combined with the cross-entropy loss a \textit{cross-entropy layer}.

{\bf Harmonic loss:} The \textit{harmonic logit} $\bm{d}$ is the $l_2$ distance between $\bm{w}_i$ and $\bm{x}$, i.e., 
\begin{equation}
d_i=||\bm{w}_i-\bm{x}||_2.\label{eq:harmonic-logit}
\end{equation}
We interpret $\bm{w}_i$ as keys and $\bm{x}$ as a query, so smaller $d_i$ means a higher probability of $p_i$. We define \textit{harmonic max} (\textit{harmax}) as 
\begin{equation}
    p_i = {\rm HarMax}(\bm{d})_i \equiv \frac{1/d_i^n}{\sum_{j}1/d_j^n},
\end{equation}
where $n$ (\textit{harmonic exponent}) is a hyperparameter that controls the heavy-tailedness of the probability distribution. If the true class label is $c$, then loss $\ell=-{\rm log}\ p_c$. For notational simplicity, we call a layer combined with the harmonic loss the \textit{harmonic layer}.
% hmm we've set n = embd_dim typically
Since the last step of both losses is the same ($\ell = -{\rm log}\ p$), comparing their values is meaningful. They only differ in the ways of computing probabilities from representations~\footnote{Note that when we say ``cross-entropy loss,'' we do not only refer to $\ell=-{\rm log}\ p$, but rather refer to the whole pipeline including penultimate representation, logit, probability, and loss.}.

A reasonable choice of $n$ is $n\sim\sqrt{D}$, where $D$ represents the intrinsic dimensionality of the underlying data. In LLMs, $D$ could be approximated as $D\approx d_{\rm embed}$, where $d_{\rm embed}$ is the embedding dimension. This approximation arises from considering an embedding initialized from a $D$-dimensional Gaussian distribution. The squared distance between two points, normalized by the number of dimensions $D$, is on the order of $1 \pm O(1/\sqrt{D})$. To ensure that the harmonic distance $\left[1 \pm O(1/\sqrt{D})\right]^n$ remains constant as we scale $D$, we require $n \sim \sqrt{D}$, since $\lim_{x\rightarrow \infty} (1+x^{-1})^x = e$.


{\bf Toy cases:} To provide intuition about what advantages the harmonic loss has over the cross-entropy loss, we consider two toy cases in 2D, as shown in Figure~\ref{fig:ce-harmonic} (b)(c). In each toy case, we train the cross-entropy layer and the harmonic layer with the Adam optimizer. {\bf Toy case 1}: $\bm{x}_1=(1,1)$ and $\bm{x}_2=(-1,-1)$ belong to two different classes. The harmonic layer produces a faster loss decrease, because the harmonic loss only requires $d_i\to 0$ (converging point is finite) to get $p_i\to 1$. By contrast, cross-entropy loss requires $y_i\to \infty$ (converging point is infinite) to get $p_i\to 1$. The harmonic loss already produces a $l_2$ weight norm that plateaus to a constant, while the cross-entropy loss leads to increasing $l_2$, diverging towards infinity. {\bf Toy case 2}: There are 5 points in 2D, each of which belong to a different class. In particular, the red point $(0,0)$ is surrounded by the other four points, i.e., cannot be linearly separated. The cross-entropy layer indeed cannot perform well on this task, manifested by a high loss plateau. By contrast, the harmonic layer can drive the loss down to machine precision. Similar to case 1, the harmonic layer has a plateauing $l_2$ while the cross-entropy layer has an ever-growing $l_2$. We also observe that the weights of the harmonic layer correspond to $\bm{x}$, which is more interpretable than the weights of the cross-entropy layer.


{\bf Benefits of harmonic loss:} From these two toy cases, we understand the advantages of harmonic loss: (1) \emph{nonlinear separability}: in case 2, the red dot can be classified correctly even though it is not linearly separable. (2) \emph{fast convergence}: The fact that the converging point is finite leads both to faster loss decay, and plateauing (non-diverging) $l_2$. (3) \emph{scale invariance}: Harmonic loss is scale-invariant, i.e., $d_i\to \alpha d_i$ leaves $p_i$ (hence loss) invariant, whereas $y_i\to \alpha y_i$ would produce a different cross-entropy loss. (4) \emph{interpretability}: the weight vectors correspond to class centers. 

The rest of the paper explores the use of harmonic loss in various applications: algorithmic tasks in Section~\ref{sec:toy-exp}, MNIST in Section~\ref{sec:mnist}, and language modeling in Section~\ref{sec:gpt2-exp}.

\section{Algorithmic Datasets}
\label{sec:toy-exp}

Algorithmic tasks are good benchmarks for network interpretability since they are well-defined mathematically. However, training neural networks on these tasks is non-trivial due to the observation of grokking (delayed generalization)~\cite{power2022grokking} and the existence of multiple algorithms~\cite{zhong2024clock}, etc. We will show that harmonic models can learn better representations, are more data-efficient, and experience less grokking.

\subsection{Models and Datasets}

\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=.75\textwidth]{figures/rep_plots.pdf}}
\caption{Visualization of the top two principal components of the embeddings in synthetic experiments. The title of each subplot shows the explained variance by the first two principal components. Each row corresponds to a pair of a dataset and a model, while each column represents the embeddings from different training runs with varying seeds. Groups of consecutive two rows belong to the same dataset, with models arranged in the order: \{Standard MLP, Harmonic MLP\}. The datasets are ordered as follows: \{In-Context Learning, Genealogy Learning, Equivalence Classes, Modular Addition, and Permutation Groups\}.  X and Y axis spans are equal.}
\label{fig:rep-vis}
\end{center}
\vskip -0.2in
\end{figure*}

\textbf{Models:} We compare four models:

\begin{enumerate}
    \item \textbf{Standard MLP}: Tokens are embedded into 16-dimensional embeddings, which are then concatenated and used as the input. The model consists of two hidden layers with widths of 100 and 16, respectively. The SiLU activation function is used.
    \item \textbf{Standard Transformer}: Tokens are embedded into a 16-dimensional embedding, with a learnable positional embedding added. The input passes through two transformer decoder layers, each comprising two attention heads and an MLP with a hidden dimension of 64.
    \item \textbf{Harmonic MLP}: Standard MLP with an harmonic unembedding layer of exponent $n=1$.
    \item \textbf{Harmonic Transformer}: Standard Transformer with an harmonic unembedding layer of exponent $n=1$.
\end{enumerate}

We trained the MLP models for 7000 epochs and the transformers for 10000 epochs. For all four models, we used the AdamW optimizer with a learning rate of $2\times 10^{-3}$, a weight decay of $10^{-2}$, and an $L_2$ regularization on the embeddings with strength $0.01$.

\textbf{Datasets:}  We trained the four models above using the following five datasets, and analyzed their performance as well as the resulting representations:

\begin{enumerate}
    \item \textbf{In-Context Learning}: In a 5$\times$5 integer lattice, given three points on the lattice, the model is trained to predict the fourth point that would form a parallelogram with the others. This task exemplifies in-context reasoning in LLMs, mirroring the classic \emph{man:woman::king:queen} analogy by requiring the model to complete the relational pattern such as `man is to woman as king is to queen' based on the given context.
    \item \textbf{Modular Addition}: Given two integers $x$ and $y$, the model is trained to predict $(x+y)\;\textrm{mod}\; 31$.
    \item \textbf{Equivalence Classes}: Given two integers $0\leq x,y < 40$, the model is trained to predict if $x\equiv y\; \textrm{mod}\; 5$.
    
    \item \textbf{Genealogy Learning}: In a complete binary tree with 127 nodes, given a subject and a relation, the model is trained to predict the corresponding object. The relation can be one of the following: parent, grandparent, or sibling.
    \item \textbf{Permutation Composition}: Given two permutations $x$ and $y$ in $S_4$, the model is trained to predict $ x \circ y.$ On this dataset, we trained standard and harmonic transformers with an $L_2$ regularization of 0.005, as we found this configuration led to more complete training. 
    
\end{enumerate}


\subsection{Representation Faithfulness}


Figure \ref{fig:rep-vis} shows the plot of the top two principal components of the models' embeddings for MLP tasks. A complete visualization is available in Figure \ref{fig:rep-vis}. We show the embedding visualization of transformers in Appendix \ref{sec:full-vis}. Overall, harmonic loss representations are cleaner and more organized than their cross-entropy counterparts. We found near-perfect circle representations for the modular addition task, a clear tower-like structure for tree learning, and neat clusters for permutation composition.


We examine the representations task by task:

\emph{(1) In-context learning:} We observe that the representations obtained from standard models are either imperfect lattices or exhibit unexplained variance in higher dimensions, whereas harmonic models almost always perfectly ($100\%$) recover the underlying 2D lattice structure regardless of the random seed.

\emph{(2) Modular addition:} Harmonic MLPs consistently recover a purely 2D circular representation in almost all runs, whereas the standard MLP often fails to identify the circular structure. While the harmonic transformer has a similar success rate to the standard transformer in constructing circles, the explained variance captured by the first two principal components is generally much higher, indicating that harmonic models discover more compact representations with fewer uninterpretable components.


\emph{(3) Equivalence classes:} Both standard and harmonic models are able to identify the underlying groups with high variance. However, we note that standard models' representation tends to be more ``elongated", or not \emph{completely} grouped, compared to its harmonic counterpart. This could be attributed to the fact that cross-entropy loss does not have an incentive to reduce irrelevant variations to zero.

\emph{(4) Genealogy learning:} Harmonic MLP is the only model that successfully recovers the underlying tree representation.

\emph{(5) Permutation composition:} The harmonic MLP generally produces better-separated clusters. A particularly clean representation that appears multiple times contains 6 clusters of 4 permutations, where each cluster is a coset of the subgroup $\langle e, (12)(34), (13)(24), (14)(23)\rangle$ or one of its conjugates. In the harmonic transformer, permutations commonly organize into 4 clusters that are cosets of $\langle e, (13),(14),(34), (134), (143)\rangle$ or one of its conjugates, subgroups isomorphic to $S_3$ (one element, in this case $2$, never permutes).

Figure \ref{fig:algo-exp}(a) further demonstrates that harmonic representations tend to be more compact than standard models, with fewer uninterpretable components. In particular, harmonic models trained for in-context learning achieve 100\% explained variance using only the first two principal components.

% reference small representations figure


% \begin{figure*}[htbp]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=.95\textwidth]{figures/fig_fvu.pdf}}
% \caption{Cumulative Explained Variance as a function of the Principal Component Index for synthetic experiments. The representations of harmonic models tend to be more compact and closely aligned with the underlying 2D structure of the data.}
% \label{fig:fvu}
% \end{center}
% \vskip -0.2in
% \end{figure*}





\subsection{Data Efficiency in Training}



\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=.73\textwidth]{figures/ev_plot.pdf}}
\centerline{\includegraphics[width=.73\textwidth]{figures/data_eff_plot.pdf}}
\centerline{\includegraphics[width=.73\textwidth]{figures/grokking_plot.pdf}}
\caption{(a) Cumulative explained variance as a function of principal components (median over 20 seeds).  Harmonic representations are more compact than standard counterparts. (b) Test Accuracy as a function of Training Fraction. Harmonic models generalize faster with less data than standard counterparts. (c) Epochs to Test Accuracy $>$ 0.9 vs Epochs to Train Accuracy $>$ 0.9 for 20 consecutive times. $y=x$ line represents no grokking, where train and test accuracy improve simultaneously. Points closer to the y-axis indicate a greater degree of grokking. Results from 20 different random seeds are plotted, and the runs that were not able to achieve 90\% accuracy were omitted.}
\label{fig:algo-exp}
\end{center}
\vskip -0.2in
\end{figure*}
Humans can learn from surprisingly few samples, but neural networks usually require much more data to learn. As a result, we want models that can learn efficiently from data. Figure \ref{fig:algo-exp}(b) shows the test accuracy as a function of train data fraction for our synthetic experiments, indicating how much data is necessary in order for the model to be generalizable. We observe that harmonic models require comparable or much less amount of data to generalize, compared to their cross-entropy counterparts. Such improvement is especially notable for in-context learning, where harmonic models generalize nearly immediately.




\subsection{Reduced Grokking}

% To study grokking, we plot To reduce outlirs


Grokking refers to the phenomenon of delayed generalization~\cite{power2022grokking}: for example, it takes $10^3$ steps to reach perfect accuracy on the training data, but it takes $10^5$ steps to generalize to the test data. Grokking is a pathological phenomenon that we want to avoid~\cite{liu2022omnigrok}. We find that harmonic loss overall reduces grokking, as seen in Figure \ref{fig:algo-exp}(c). Points on the \(y=x\) line represent models which trained without grokking, with train and test accuracy improving together. This improvement is particularly evident in learning modular addition and permutation composition: while the standard MLP exhibits severe grokking, most data points for the harmonic MLP lie much closer to the $y=x$ line.




\subsection{Case Study: Modular Addition}

\begin{figure}[tb]
    \centering
    \includegraphics[width=.95\linewidth]{figures/circle_case_study.pdf}
    \caption{Case study on modular addition. Standard MLP trained for modular addition without weight decay often fails to generalize. Generalization is only achieved with the addition of strong weight decay; however, (a) significant grokking occurs, and (b) while the first two principal components form an approximate circle, they explain far less than the total variance. In contrast, the harmonic model trained for modular addition generalizes quickly without grokking. Moreover, the embedding forms a perfect 2D circle. EV in the plot represents the explained variance by the first two principal components of the embedding.}
    \label{fig:circle-case-study}
\end{figure}

In this section, we study modular addition as a case study and analyze why the harmonic MLP encourages more interpretable representations and better generalization compared to the standard MLP. The standard MLP trained for modular addition without weight decay often fails to generalize, as shown in Figure \ref{fig:circle-case-study}. Generalization is only achieved with the addition of strong weight decay; however, (a) significant grokking occurs, as depicted in Figure \ref{fig:circle-case-study}, and (b) while the first two principal components form an approximate circle, they explain far less than the total variance, leaving significant unexplained variance. In contrast, the harmonic model trained for modular addition generalizes quickly without grokking. Furthermore, the embedding forms a perfect circle, as shown in Figure \ref{fig:circle-case-study}.

The better formation of a circle and improved generalization in harmonic MLP can be attributed to the properties of harmonic loss, as explained in Section \ref{sec:harmonic-loss}. To drive the probability to 1, the standard cross-entropy loss requires driving the representation to infinity -- \emph{i.e.}, making the logit infinite. In contrast, harmonic loss achieves this by driving the harmonic logit to zero, which is easily accomplished by learning $\bm{w}_i = \bm{x}$ in Equation \ref{eq:harmonic-logit}. The existence of such a finite converging point results in (a) faster convergence, (b) better generalization, and (c) more interpretable representations.


\section{MNIST Experiments}\label{sec:mnist}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/mnist_standard_weights.pdf}
    \includegraphics[width=\linewidth]{figures/mnist_harmonic_weights.pdf}
    \caption{Visualization of model weights trained for MNIST. Yellow cells show values less than $0.01$. Both models achieved $\approx92.5\%$ test accuracy.}
    \label{fig:mnist_weights}
\end{figure}

For vision tasks, convolutional neural networks are shown to be (at least somewhat) interpretable by demonstrating ``edge detectors'', ``wheel detectors'', etc.~\cite{olah2020zoom}. However, fully connected networks do not appear to be interpretable (e.g., see Figure~\ref{fig:mnist_weights} top). In this section, we demonstrate that the harmonic loss can lead to a more interpretable network for the MNIST dataset. 

As a proof of concept, we choose our setup to be as simple as possible. Thus, we compare 1-layer neural networks trained using cross-entropy loss and harmonic loss. The input images are first flattened and passed through a $784\times 10$ linear layer to obtain the logits. The models were trained with a batch size of 64, a learning rate of 0.001, and for 10 epochs, achieving a 92.50\% test accuracy for cross-entropy loss and 92.49\% test accuracy for harmonic loss. We now analyze the weights learned by the neural network.

Figure \ref{fig:mnist_weights} verifies that the weights in the model trained with harmonic loss are highly interpretable. Consistent with its core principle, we observe that the weights align with the class centers, which, in this case, correspond to images representing each number. Additionally, most peripheral pixels have weights that are almost exactly zero, in contrast to the model trained with cross-entropy loss. The latter, by design, lacks an incentive to reduce the irrelevant background weights to zero.

\section{GPT2 Experiments}
\label{sec:gpt2-exp}
\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
%\centerline{\includegraphics[width=\textwidth]{figures/gpt2_ploss_scatter_5pc.png}}
\includegraphics[width=0.3\textwidth]{figures/GPT2_loss.pdf}
\includegraphics[width=0.6\textwidth]
{figures/gpt2_fraction_5pc.png}
\centerline{\includegraphics[width=0.9\textwidth]{figures/gpt2_para_present-past.pdf}}
\vskip -0.05in
\caption{GPT2 experiments, trained on OpenWebText for 10k steps. (Top left) loss curves. Harmonic GPT achieves a slightly lower loss compared to the standard GPT. (Top right) accumulated distribution function with respect to parallelogram loss, for twelve function-vector tasks. The Harmonic GPT consistently shows lower parallelogram losses (i.e., better parallelograms). (Bottom) Parallelograms (1st and 2nd principal component) with quality ranked in descending order from left to right. The Harmonic GPT tends to produce parallelograms that are more `rectangular', while standard GPT produces flat `parallelograms'.}
\label{fig:gpt2}
\end{center}
\vskip -0.2in
\end{figure*}

Many mechanistic interpretability works have been dedicated to understanding large language models. For example, probing and attribution methods are good post hoc analysis tools. Despite their (partial) success, these tools are not creating interpretable models in the first place but are trying to find needles in the haystack. We argue that it would be nicer if we could pre-train the language models to be more interpretable. By using harmonic loss in training, we can produce a language model that can ``grow" crystal-like representations, while having comparable performance with a standard one (trained with the cross-entropy loss).

We pre-train a GPT-2 small model (128M, based on NanoGPT) on OpenWebText. The embedding matrix and the unembedding matrix are tied (share the same weights). We use 8 V100 GPUs, choose block size 1024, batch size 480 blocks. We use the Adam Optimizer with $\beta_1=0.9$, $\beta_2=0.95$. For the harmonic loss, we choose $n=\sqrt{768}\approx28$, following the discussion on harmonic exponent in Section \ref{sec:harmonic-loss}. For standard (harmonic) GPT, we use a linear warmup learning rate schedule for 2k (1k) steps to maximum learning rate $6\times 10^{-4}$ ($6\times 10^{-3}$), and a cosine decay schedule from 2k to 10k, ending at lr $3\times 10^{-5}$ ($3\times 10^{-4}$). As shown in Figure~\ref{fig:gpt2} top left, Harmonic GPT shows faster converging initially (partially due to larger learning rates), and converges to similar performance in the end (at 10k steps). The final validation losses are 3.159 (standard) and 3.146 (harmonic). From training loss curves, harmonic GPT also seems to have smaller fluctuations. This suggests the effectiveness of the harmonic loss on real-world models.

To testify the interpretability of the learned embeddings, we take twelve function-vector tasks from~\cite{todd2023function}. Each dataset contains many input-output pairs that have a certain relation. For example, the ``present-past" dataset contains pairs like: jump-jumped, fasten-fastened, win-won, etc. To construct parallelograms, we can draw two different pairs from the dataset, obtaining quadruples like (jump, jumped, fasten, fastened) which are expected to form parallelograms. Each word is tokenized into tokens; if multiple tokens are obtained, we use the last token. We project token embeddings onto the first two principal components. The quadruple $(i,j,m,n)$ has 2D PC embeddings $(\bm{E}_i,\bm{E}_j,\bm{E}_m,\bm{E}_n)$; we define the parallelogram loss $l_{\rm para}$ to be
\begin{equation}
    l_{\rm para} = \|\bm{E}_i+\bm{E}_n - \bm{E}_j - \bm{E}_m\|/\sigma,
\end{equation}
where $\sigma=\sqrt{\frac{1}{V}\sum_{k=1}^V\|\bm{E}_k\|^2}$ is a scale factor that normalizes the loss ($\bm{E}_k\to a\bm{E}_k$ leaves $l_{\rm para}$ invariant). We obtain 10000 quadruples, measuring the parallelogram qualities by computing their parallelogram losses. We plot their cumulated distribution function in Figure~\ref{fig:gpt2} top right: for every task, the harmonic GPT produces lower parallelogram loss (better parallelograms) than standard GPT. We show the parallelograms obtained in the present-past task in  Figure~\ref{fig:gpt2} bottom. The parallelograms are ranked with quality in descending order from left to right. The harmonic GPT tends to produce visually appealing parallelograms that are more `rectangular', while standard GPT produces flat `parallelograms'. 





\section{Conclusions}
\label{sec:conclusion}

In this paper, we introduced harmonic loss as an alternative to the standard cross-entropy loss for training neural networks and large language models (LLMs). We found that models trained with harmonic loss outperform standard models by: (a) reducing grokking, (b) requiring less data for generalization, and (c) improving interpretability. We also compared a GPT-2 model trained with harmonic loss to the standard GPT-2, illustrating that the harmonic loss-trained model develops more interpretable representations. Further study is needed to explore the scalability and applicability of our findings to even larger models.



% \section*{Accessibility}
% Authors are kindly asked to make their submissions as accessible as possible for everyone including people with disabilities and sensory or neurological differences.
% Tips of how to achieve this and what to pay attention to will be provided on the conference website \url{http://icml.cc/}.

% \section*{Software and Data}

% If a paper is accepted, we strongly encourage the publication of software and data with the
% camera-ready version of the paper whenever appropriate. This can be
% done by including a URL in the camera-ready copy. However, \textbf{do not}
% include URLs that reveal your institution or identity in your
% submission for review. Instead, provide an anonymous URL or upload
% the material as ``Supplementary Material'' into the OpenReview reviewing
% system. Note that reviewers are not required to look at this material
% when writing their review.

% Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include acknowledgements.  Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.

\section*{Impact Statement}

This paper presents work whose goal is to enhance the interpretability of machine learning systems. Our harmonic loss approach enables a deeper understanding of model behavior, thereby improving the trustworthiness of machine learning systems.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{icml2025_main}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Full Representation Visualization}
\label{sec:full-vis}
Figure \ref{fig-app:rep-vis} shows the visualization of representations for all models and datasets.

\begin{figure*}[htbp]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=.8\textwidth]{figures/rep_plots_appendix.pdf}}
\caption{Visualization of the top two principal components of the embeddings in synthetic experiments. The title of each subplot shows the explained variance by the first two principal components. Each row corresponds to a pair of a dataset and a model, while each column represents the embeddings from different training runs with varying seeds. Groups of four rows belong to the same dataset, with models arranged in the order: \{Standard MLP, Harmonic MLP, Standard Transformer, Harmonic Transformer\}. The datasets are ordered as follows: \{In-Context Learning, Genealogy Learning, Equivalence Classes, Modular Addition, and Permutation Groups\}.}
\label{fig-app:rep-vis}
\end{center}
\vskip 0.2in
\end{figure*}
% \section{You \emph{can} have an appendix here.}

\newpage
\section{Identifying Coset Structure in Permutation Representations}


To explore the coset structure in permutation representations of $S_4$, we began by enumerating its subgroups. Using this enumeration, we computed all possible left and right cosets of each subgroup in $S_4$, yielding 28 distinct left cosets and 28 distinct right cosets.

Among these cosets, two pairs are equivalent, since we consider two of the four normal subgroups of $S_4$: the alternating group $A_4$ and the Klein-4 group. To focus on meaningful structures, the trivial subgroup and the entire group were excluded from further analysis.

The coset partitions were then compared using the silhouette score, a metric for evaluating the quality of clustering. This comparison helped identify the partition with the most structured coset organization, which is likely the structure that the model has captured during training. We then color the representation according to the best-clustered partition, with each coset being a different color.









% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
