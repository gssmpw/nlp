\section{Related Works}
\label{sec:related-works}

{\bf Representations:} In this paper, we aim to improve the interpretability of neural network representations. Numerous studies have shown that LLMs can form conceptual representations across spatial ____, temporal ____, and color domains ____. The structure of such representations includes one-dimensional concepts ____, as well as multi-dimensional representations such as lattices ____ and circles ____. Recent works have also studied the representations developed during inference-time ____. While the structure of these representations often correlates with certain geometric patterns, significant unexplained variance frequently remains, complicating interpretability.

{\bf Mechanistic Interpretability:}  While increasing the number of parameters and the amount of data samples used to train neural networks has enhanced their capabilities, it has also made mechanistically interpreting these models more challenging. This line of work has been explored from two major directions: the circuit level ____, which aims to identify a submodule within an LLM responsible for a specific ability, and the representation level ____. 
Some works have made progress by designing interpretable systems through the decomposition of models into smaller modules ____.

{\bf Loss Functions:}  Previous research has shown that loss functions can influence how a model learns to represent data, affecting its abilities in unique ways ____. Novel loss functions have improved performance on specific tasks, though they often reduce ability in different settings ____. For instance, focal loss, dice loss, and Tversky loss have proven effective for image segmentation ____, but only focal loss is also effective for object detection ____. ____ found that smoothly approximating non-smooth Hinge loss in Support Vector Machines (SVMs) improved the convergence rate of optimization.


%CE, MSE, Hinge loss, Focal Loss,  BIMT perhaps