@article{abdou2021can,
  title={Can language models encode perceptual structure without grounding? a case study in color},
  author={Abdou, Mostafa and Kulmizev, Artur and Hershcovich, Daniel and Frank, Stella and Pavlick, Ellie and S{\o}gaard, Anders},
  journal={arXiv preprint arXiv:2109.06129},
  year={2021}
}

@article{bom2023wind,
  title={Hybrid wind speed forecasting using ICEEMDAN and transformer model with novel loss function},
  author={Bommidi, Bala Saibabu and Teeparthi, Kiran and Kosana, Vishalteja},
  journal={Energy},
  volume={265},
  pages={126383},
  year={2023},
  publisher={Elsevier}
}

@inproceedings{bosco2024cardio,
  title={Echocardiographic Image Segmentation with Vision Transformers: A Comparative Analysis of Different Loss Functions},
  author={Bosco, Edoardo and Magenes, Giovanni and Matrone, Giulia},
  booktitle={2024 IEEE International Symposium on Medical Measurements and Applications (MeMeA)},
  pages={1--6},
  year={2024},
  organization={IEEE}
}

@inproceedings{demir2023topo,
  title={Topology-Aware Focal Loss for 3D Image Segmentation},
  author={Demir, Andac and Massaad, Elie and Kiziltan, Bulent},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={580--589},
  year={2023}
}

@article{ding2024survival,
  title={Survival of the Fittest Representation: A Case Study with Modular Addition},
  author={Ding, Xiaoman Delores and Guo, Zifan Carl and Michaud, Eric J and Liu, Ziming and Tegmark, Max},
  journal={arXiv preprint arXiv:2405.17420},
  year={2024}
}

@article{engels2024not,
  title={Not All Language Model Features Are Linear},
  author={Engels, Joshua and Liao, Isaac and Michaud, Eric J and Gurnee, Wes and Tegmark, Max},
  journal={arXiv preprint arXiv:2405.14860},
  year={2024}
}

@article{gurnee2023language,
  title={Language models represent space and time},
  author={Gurnee, Wes and Tegmark, Max},
  journal={arXiv preprint arXiv:2310.02207},
  year={2023}
}

@article{heinzerling2024monotonic,
  title={Monotonic representation of numeric properties in language models},
  author={Heinzerling, Benjamin and Inui, Kentaro},
  journal={arXiv preprint arXiv:2403.10381},
  year={2024}
}

@article{li2021implicit,
  title={Implicit representations of meaning in neural language models},
  author={Li, Belinda Z and Nye, Maxwell and Andreas, Jacob},
  journal={arXiv preprint arXiv:2106.00737},
  year={2021}
}

@article{li2024enhancing,
  title={Enhancing hydrological extremes prediction accuracy: Integrating diverse loss functions in Transformer models},
  author={Li, Xue and Sun, Qi-Liang and Zhang, Yanfei and Sha, Jian and Zhang, Man},
  journal={Environmental Modelling \& Software},
  volume={177},
  pages={106042},
  year={2024},
  publisher={Elsevier}
}

@article{li2024geometry,
  title={The geometry of concepts: Sparse autoencoder feature structure},
  author={Li, Yuxiao and Michaud, Eric J and Baek, David D and Engels, Joshua and Sun, Xiaoqing and Tegmark, Max},
  journal={arXiv preprint arXiv:2410.19750},
  year={2024}
}

@article{lin2017focal,
  title={Focal Loss for Dense Object Detection},
  author={Lin, T},
  journal={arXiv preprint arXiv:1708.02002},
  year={2017}
}

@article{liu2022omnigrok,
  title={Omnigrok: Grokking beyond algorithmic data},
  author={Liu, Ziming and Michaud, Eric J and Tegmark, Max},
  journal={arXiv preprint arXiv:2210.01117},
  year={2022}
}

@article{liu2022towards,
  title={Towards understanding grokking: An effective theory of representation learning},
  author={Liu, Ziming and Kitouni, Ouail and Nolte, Niklas S and Michaud, Eric and Tegmark, Max and Williams, Mike},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={34651--34663},
  year={2022}
}

@article{liu2023seeing,
  title={Seeing is believing: Brain-inspired modular training for mechanistic interpretability},
  author={Liu, Ziming and Gan, Eric and Tegmark, Max},
  journal={Entropy},
  volume={26},
  number={1},
  pages={41},
  year={2023},
  publisher={MDPI}
}

@article{luo2021learning,
  title={Learning with smooth Hinge losses},
  author={Luo, JunRu and Qiao, Hong and Zhang, Bo},
  journal={Neurocomputing},
  volume={463},
  pages={379--387},
  year={2021},
  publisher={Elsevier}
}

@article{marks2023geometry,
  title={The geometry of truth: Emergent linear structure in large language model representations of true/false datasets},
  author={Marks, Samuel and Tegmark, Max},
  journal={arXiv preprint arXiv:2310.06824},
  year={2023}
}

@article{michaud2024opening,
  title={Opening the AI black box: program synthesis via mechanistic interpretability},
  author={Michaud, Eric J and Liao, Isaac and Lad, Vedang and Liu, Ziming and Mudide, Anish and Loughridge, Chloe and Guo, Zifan Carl and Kheirkhah, Tara Rezaei and Vukeli{\'c}, Mateja and Tegmark, Max},
  journal={arXiv preprint arXiv:2402.05110},
  year={2024}
}

@article{olah2020circuits,
  title={Zoom in: An introduction to circuits},
  author={Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  journal={Distill},
  volume={5},
  number={3},
  pages={e00024--001},
  year={2020}
}

@article{olsson2022context,
  title={In-context learning and induction heads},
  author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and others},
  journal={Transformer Circuits Thread},
  year={2022}
}

@article{park2024geometry,
  title={The Geometry of Categorical and Hierarchical Concepts in Large Language Models},
  author={Park, Kiho and Choe, Yo Joong and Jiang, Yibo and Veitch, Victor},
  journal={arXiv preprint arXiv:2406.01506},
  year={2024}
}

@article{park2024iclr,
  title={ICLR: In-Context Learning of Representations},
  author={Park, Core Francisco and Lee, Andrew and Lubana, Ekdeep Singh and Yang, Yongyi and Okawa, Maya and Nishi, Kento and Wattenberg, Martin and Tanaka, Hidenori},
  journal={arXiv preprint arXiv:2501.00070},
  year={2024}
}

@inproceedings{sal2017tversky,
  title={Tversky loss function for image segmentation using 3D fully convolutional deep networks},
  author={Salehi, Seyed Sadegh Mohseni and Erdogmus, Deniz and Gholipour, Ali},
  booktitle={International workshop on machine learning in medical imaging},
  pages={379--387},
  year={2017},
  organization={Springer}
}

@article{seber2024protein,
  title={Predicting O-GlcNAcylation Sites in Mammalian Proteins with Transformers and RNNs Trained with a New Loss Function},
  author={Seber, Pedro},
  journal={arXiv preprint arXiv:2402.17131},
  year={2024}
}

@inproceedings{sudre2017dice,
  title={Generalised dice overlap as a deep learning loss function for highly unbalanced segmentations},
  author={Sudre, Carole H and Li, Wenqi and Vercauteren, Tom and Ourselin, Sebastien and Jorge Cardoso, M},
  booktitle={Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support: Third International Workshop, DLMIA 2017, and 7th International Workshop, ML-CDS 2017, Held in Conjunction with MICCAI 2017, Qu{\'e}bec City, QC, Canada, September 14, Proceedings 3},
  pages={240--248},
  year={2017},
  organization={Springer}
}

@misc{templeton2024claude,
  title={Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet. Transformer Circuits Thread},
  author={Templeton, Adly and Conerly, Tom and Marcus, Jonathan and Lindsey, Jack and Bricken, Trenton and Chen, Brian and Pearce, Adam and Citro, Craig and Ameisen, Emmanuel and Jones, Andy and others},
  journal={Transformer Circuits Thread},
  year={2024}
}

@article{zhong2024clock,
  title={The clock and the pizza: Two stories in mechanistic explanation of neural networks},
  author={Zhong, Ziqian and Liu, Ziming and Tegmark, Max and Andreas, Jacob},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

