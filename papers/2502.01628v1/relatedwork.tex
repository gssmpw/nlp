\section{Related Works}
\label{sec:related-works}

{\bf Representations:} In this paper, we aim to improve the interpretability of neural network representations. Numerous studies have shown that LLMs can form conceptual representations across spatial \citep{gurnee2023language}, temporal \citep{li2021implicit}, and color domains \citep{abdou2021can}. The structure of such representations includes one-dimensional concepts \citep{gurnee2023language, marks2023geometry, heinzerling2024monotonic, park2024geometry}, as well as multi-dimensional representations such as lattices \citep{michaud2024opening,li2024geometry} and circles \citep{liu2022towards, engels2024not}. Recent works have also studied the representations developed during inference-time \cite{park2024iclr}. While the structure of these representations often correlates with certain geometric patterns, significant unexplained variance frequently remains, complicating interpretability.

{\bf Mechanistic Interpretability:}  While increasing the number of parameters and the amount of data samples used to train neural networks has enhanced their capabilities, it has also made mechanistically interpreting these models more challenging. This line of work has been explored from two major directions: the circuit level \citep{michaud2024opening, olah2020circuits, olsson2022context, templeton2024claude}, which aims to identify a submodule within an LLM responsible for a specific ability, and the representation level \citep{liu2022omnigrok, ding2024survival, zhong2024clock}. 
Some works have made progress by designing interpretable systems through the decomposition of models into smaller modules \citep{olah2020circuits, liu2023seeing}.

{\bf Loss Functions:}  Previous research has shown that loss functions can influence how a model learns to represent data, affecting its abilities in unique ways \citep{li2024enhancing, bosco2024cardio}. Novel loss functions have improved performance on specific tasks, though they often reduce ability in different settings \citep{bom2023wind, seber2024protein}. For instance, focal loss, dice loss, and Tversky loss have proven effective for image segmentation \citep{sudre2017dice, demir2023topo, sal2017tversky}, but only focal loss is also effective for object detection \citep{lin2017focal}. \citet{luo2021learning} found that smoothly approximating non-smooth Hinge loss in Support Vector Machines (SVMs) improved the convergence rate of optimization.


%CE, MSE, Hinge loss, Focal Loss,  BIMT perhaps