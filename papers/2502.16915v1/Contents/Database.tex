\section{Database Construction and Analysis}
In this section, we will describe the database construction and analysis in detail.
\begin{figure}[b]
    \centering
    \includegraphics[width = 0.48\textwidth]{Figs/pie.pdf}
    \caption{The Pie Chart of our used Prompt, which contains 11 challenge categories and 12 scene categories.}
    \label{fig:pie}
\end{figure}
\begin{figure*}[t]
    \centering
    \subfigure[3D assets generated by the prompt: ``a harp without any strings'']{\begin{minipage}[t]{\linewidth}
                \centering
                \includegraphics[width = 0.98\linewidth]{Figs/gallery-004.pdf}
                \end{minipage}}
                
    \subfigure[3D assets generated by the prompt: ``a pair of brown suede shoes'']{\begin{minipage}[t]{\linewidth}
                \centering
                \includegraphics[width = 0.98\linewidth]{Figs/gallery-012.pdf}
                \end{minipage}}
                
    \caption{Sample 3D assets from the AIGC-T23DAQA database, generated by Dreamfusion \cite{poole2022dreamfusion}, LatentNerf \cite{metzer2023latent}; Magic3D \cite{lin2023magic3d}, Prolificdreamer \cite{wang2024prolificdreamer}; SJC\cite{wang2023score}, TextMesh \cite{tsalicoglou2023textmesh} with the same input prompt respectively. (a) 3D assets generated by the prompt ``a harp without any strings''. (b) 3D assets generated by the prompt ``a pair of brown suede shoes''. This clearly shows that the visual quality of assets generated by different models varies greatly.}
    \label{fig:gallery}
    \vspace{-0.5cm}
\end{figure*}
\vspace{-15pt}
\subsection{Prompt Selection}
Compared to AIGC IQA and VQA databases, constructing text-to-3D asset quality assessment database mainly faces two difficulties: 1) The process of generating 3D asset from text is currently time-consuming, typically requiring 1 to 6 hours to generate one 3D asset. 2) The subjective experiment for evaluating generated 3D asset is also time-consuming, since subjects need to observe from whole directions and assess from multiple perspectives. Therefore, our constructed database is a enormous contribution to the field. First of all, meticulous prompts selection is important for text-to-3D asset quality assessment database construction. The selected prompts need to cover a wide range of real user inputs with a relatively small pool. PartiPrompts \cite{yu2022scaling} comprises 1600 varied English prompts designed to comprehensively assess and test the limits of text-to-image synthesis models. Following previous research \cite{wang2023aigciqa2023} we extracted 170 prompts from PartiPrompts, spanning 11 challenge categories and 12 scene categories. The distribution of selected scene and challenge categories is depicted in the pie chart of Fig. \ref{fig:pie}, which manifests that the prompts in our dataset exhibit a high level of scene diversity and encompass a broad spectrum of challenges.
\vspace{-15pt}
\subsection{3D Asset Generation}

To ensure asset diversity, AIGC-T23DAQA database contain six representative text-to-3D asset generation models. These current models typically comprise a 2D image generation module and a 3D asset representation module. When compared to other generation models, the diffusion model delivers exceptional results, establishing itself as the preferred foundational module for generating 2D images within these methodologies. For the 3D asset representation module, a variety of approaches are employed, including NeRF, Instatn-ngp, \textit{etc}. Dreamfusion \cite{poole2022dreamfusion} utilizes mip-NeRF 360 for 3D asset representation, while LatentNerf \cite{metzer2023latent} opts for vanilla NeRF. SJC \cite{wang2023score} employs voxel radiance fields to represent 3D asset, thereby enhancing the speed of the generation process. Conversely, TextMesh  \cite{tsalicoglou2023textmesh}, Magic3D \cite{lin2023magic3d}, and Prolificdreamer \cite{wang2024prolificdreamer} adopt a coarse-to-fine strategy. They commence with coarse 3D asset representations, using vanilla NeRF and Instatn-ngp, respectively, and subsequently refine the differentiable mesh into a fine representation. 
\begin{figure}[!t]
    \centering
    \subfigure[Prompt: ``an ostrich''.]{\begin{minipage}[t]{0.48\linewidth}
                \centering
                \includegraphics[width = 0.94\linewidth]{Figs/q0.pdf}
                \end{minipage}}
    \subfigure[Prompt: ``a comic about a boy and a tiger''.]{\begin{minipage}[t]{0.48\linewidth}
                \centering
                \includegraphics[width = 0.94\linewidth]{Figs/q1.pdf}
                \end{minipage}}
                
    \subfigure[Prompt: ``a fish without eyes''.]{\begin{minipage}[t]{0.48\linewidth}
                \centering
                \includegraphics[width = 0.94\linewidth]{Figs/a0.pdf}
                \end{minipage}}
    \subfigure[Prompt: ``a large present with a red ribbon to the left of a Christmas tree''.]{\begin{minipage}[t]{0.48\linewidth}
                \centering
                \includegraphics[width = 0.94\linewidth]{Figs/a1.pdf}
                \end{minipage}}
                
    \subfigure[Prompt: ``a robot cooking''.]{\begin{minipage}[t]{0.48\linewidth}
                \centering
                \includegraphics[width = 0.94\linewidth]{Figs/c0.pdf}
                \end{minipage}}
    \subfigure[Prompt: ``a bundle of blue and yellow flowers in a vase''.]{\begin{minipage}[t]{0.48\linewidth}
                \centering
                \includegraphics[width = 0.94\linewidth]{Figs/c1.pdf}
                \end{minipage}}
    \caption{Illustration of the differences between the three dimensions of quality ,authenticity, and text-3D correspondence. In each subfigure, the images in the top row are significantly better than the that in bottom row in terms of two perspectives, while similar or worse in terms of another perspective. (a) and (b) show examples that the authenticity and correspondence scores of the top images are higher, while the quality is similar. (c) and (d) show examples that the quality and correspondence scores of the top images are higher, while the authenticity is similar or lower. (e) and (f) show examples that the quality and authenticity scores of the top images are higher, while the correspondence is similar or lower. }
    \label{fig:xx}
    \vspace{-0.5cm}
\end{figure}
The generation process of text-to-3D asset was executed using open-source code \cite{threestudio2023} with default weights and configurations, resulting in a collection of 1020 instances (170 prompts × 6 models) of text-to-3D assets. Some examples of the 3D assets generated by the six text-to-3D asset generation models are illustrated in Fig.  \ref{fig:gallery}. Subsequently, we discarded 51 instances of failed asset generation, defined as cases where the entire spatial domain remained empty after-generation. Due to computational constraints, it is hard to render a generated 3D asset in real-time and evaluate it. Thus, we followed the method used in \cite{zhang2023eep} and projected the 3D asset into videos then conducted evaluation. This manipulation yielded 969 360-degree surround projection videos centered on the generated text-to-3D asset. Each video consists of comprised 120 frames with a resolution of 512 × 512 pixels and cumulative a total duration of 4 seconds. These projection videos were used for the subsequent subjective experiment.

\vspace{-10pt}
\subsection{Subjective Experiment}
\begin{figure}[!t]
    \centering
    \subfigure[Quality]{\begin{minipage}[t]{\linewidth}
                \centering
                \includegraphics[width = 0.98\linewidth]{Figs/assessment-perspectives-q.pdf}
                \end{minipage}}
                
    \subfigure[Authenticity]{\begin{minipage}[t]{\linewidth}
                \centering
                \includegraphics[width = 0.98\linewidth]{Figs/assessment-perspectives-a.pdf}
                \end{minipage}}

    \subfigure[Correspondence]{\begin{minipage}[t]{\linewidth}
                \centering
                \includegraphics[width = 0.98\linewidth]{Figs/assessment-perspectives-c.pdf}
                \end{minipage}}
    \caption{Illustration of the text-to-3d assets from the perspectives of quality, authenticity, and text-asset correspondence. The examples of good, fair, and poor quality are depicted in the first to third rows of (a). The examples illustrating good, fair, and poor authenticity are displayed in the first to third rows of (b). (c) showcases examples of good, fair, and poor correspondence generated by prompts ``a harp without any strings'', ``a knight holding a long sword'', and ``A cartoon tiger face''. }
    \label{fig:ap}
    \vspace{-0.4cm}
\end{figure}
To collect human visual preferences for text-to-3D assets, we further conducted a subjective evaluation experiment. As highlighted in prior AI generated asset quality assessment studies \cite{wang2023aigciqa2023, yang2024aigcoiqa2024}, the degradations of AI generated asset are significantly different from human captured or created asset, which need to be evaluated from multiple perception perspectives. \textcolor{black}{Based on traditional 3D quality assessment, which evaluates texture, color, and other visual quality attributes of the 3D asset, we selected the ``quality'' dimension for evaluation. Similar to AI-generated image and video quality assessment, in addition to assessing the visual quality of the 3D asset, we also need to evaluate its authenticity and correspondence to the text prompt. Therefore, we selected the dimensions of ``authenticity'' and ``correspondence''.} Hence, in this paper, we propose to evaluate human visual preferences for text-to-3D assets from three perspectives, including quality, authenticity, and text-asset correspondence. Fig. \ref{fig:xx} shows the differences between the selected three dimensions, which further manifests the importance, and significance of evaluating text-to-3D assets from multiple perspectives.
\textcolor{black}{Before each subject conducts the subjective experiment, we give a detailed instruction to subjects, which includes explaining to the subject the differences between ``quality'', ``authenticity'' and ``correspondence'' and showing examples of different degrees of each dimension. The ``quality" is the visual quality attribution of 3D asset including texture, color, integrity, etc, while the ``authenticity" refers to whether the 3D asset is consistent with the real world that the subject knows. The ``correspondence" is the alignment between the 3D asset and the input prompt text.} Then, participants were instructed to give their preference scores of text-to-3D assets based on the surrounding 360-degree projection videos. The first dimension for evaluating text-to-3D asset is ``quality'', which mainly evaluates the perception attributes including texture, color, integrity, details \textit{etc.}, analogous to traditional 3D models. Fig. \ref{fig:ap} (a) shows examples of the generated 3D asset with different ``quality'' levels. The second dimension for evaluating text-to-3D asset is ``authenticity'', which evaluates the perception attributes including unrealistic textures, shapes, \textit{etc.} It should be noted that compared to the authenticity attribute generally used in AIGC IQA, the degradation of the authenticity attribute for generated text-to-3D asset generally comes from the unrealistic or inconsistent multiple views. Fig. \ref{fig:ap} (b) shows examples of the generated 3D asset with different ``authenticity'' levels. Similar to AIGC IQA, and AIGC VQA methodologies, the correspondence between text, and 3D asset serves as another critical criterion in assessing text-to-3D asset quality, referred to as ``text-3D asset correspondence''. Fig. \ref{fig:ap} (c) shows examples of the generated 3D asset with different ``correspondence'' levels.
\begin{figure}[t]
    \centering
    \includegraphics[width = 0.48\textwidth]{Figs/gui.png}
    \caption{The illustration of the subjective assessment interface. The subject can evaluate their preferences of the text-to-3D assets, and record the quality, authenticity, correspondence scores with the scroll bars on the right.}
    \label{fig:gui}
    \vspace{-0.5cm}
\end{figure}

\begin{figure*}[t]
    \centering
    \subfigure[Quality MOS distribution.]{\begin{minipage}[t]{0.32\linewidth}
                \centering
                \includegraphics[width = 0.94\linewidth]{Figs/mosz_Quality.pdf}
                \end{minipage}}
    \subfigure[Authenticity MOS distribution.]{\begin{minipage}[t]{0.32\linewidth}
                \centering
                \includegraphics[width = 0.94\linewidth]{Figs/mosz_Authenticity.pdf}
                \end{minipage}}
    \subfigure[Correspondence MOS distribution.]{\begin{minipage}[t]{0.32\linewidth}
                \centering
                \includegraphics[width = 0.94\linewidth]{Figs/mosz_Correspondence.pdf}
                \end{minipage}}
    \caption{Distributions of the MOSs from the perspectives of quality, authenticity, and correspondence, respectively. These distributions exhibiting proposed T23DAQA database cover a wide range in terms of all perspectives. }
    \label{fig:mos}
    \vspace{-0.5cm}
\end{figure*}

We conducted the subjective experiment following the guidance in ITU-R BT.500-13 \cite{other:itu}. The experimental environment was arranged to simulate a typical indoor home setting with standard lighting conditions. The projection videos of text-to-3D asset, accompanied by the corresponding prompts, were presented randomly on a monitor with a resolution of $1920 \times 1080$. The interface, depicted in Fig. \ref{fig:gui}, facilitated viewer interaction, enabling navigation through previous, next, and replay options for the projection videos of the generated 3D asset. Additionally, three sliders ranging from 0 to 5, with a minimum interval of 0.1, were provided for participants to assign scores for quality, authenticity, and correspondence. 17 subjects (10 males and 7 females) participated in the subjective experiment, all possessing normal or corrected-to-normal vision. Each participant received detailed experimental instructions prior to engaging in the subjective evaluation. We divided the conversation of each participant in the subjective experiment into three subsets. For each participant, the database were randomly divided into three subsets, which are used in three subjective tests respectively. Each test lasted around one hour, followed by a 10-20 minutes break in between, and then the next test was performed.
\vspace{-15pt}
\subsection{Data Processing}

We followed the instructions of ITU \cite{other:itu} to conduct the outlier detection and subject rejection. \textcolor{black}{Specifically, for each evaluation dimension, we calculate the kurtosis of the raw subjective quality ratings for each generated 3D asset to determine whether the data follows a Gaussian or non-Gaussian distribution. For Gaussian distributions, a raw score is considered an outlier if it lies more than 2 standard deviations (std) from the mean. For non-Gaussian distributions, a score is deemed as an outlier if it is more than $\sqrt{20}$ standard deviations from the mean. Any subject whose evaluations exceed a 3\% outlier rate in any dimension is excluded from the analysis.} As a result, no subjects were rejected and the rejection ratio is 3\% for all ratings. Subsequently, we converted the raw ratings of the remaining valid subjective scores into Z-scores, which were then linearly scaled to the range of $\left [ 0,100 \right ]$. The final MOS is computed as follows:
\begin{equation}
z_{ij} = \frac{m_{ij}-\mu_{i}}{\sigma_{i}}, \quad z_{ij}^{'} = \frac{100\times(z_{ij} + 3)}{6}
\end{equation}

\begin{equation}
\text{MOS}_{j} = \frac{1}{N} z_{ij}^{'}
\end{equation}
where $m_{ij}$ is the subjective score given by the $i$-th subject to the $j$-th text-to-3D asset, $\mu_{i}$ and $\sigma_{i}$ is the mean score and the standard deviation given by the $i$-th subject respectively, $N$ is the total number of subjects. 
% \begin{figure}[t]
%     \centering
%     \includegraphics[width = 0.48\textwidth]{Figs/assessment-perspectives .pdf}
%     \caption{Illustration of the T23DCs from the perspectives of quality, authenticity, and text-content correspondence. The examples of good, fair, and poor quality are depicted in the first to third rows of (a). The examples illustrating good, fair, and poor authenticity are displayed in the first to third rows of (b). (c) showcases examples of good, fair, and poor correspondence generated by prompts such as "a harp without any strings", "a knight holding a long sword", and "A cartoon tiger face".}
%     \label{fig:ap}
% \end{figure}
\vspace{-15pt}
\subsection{Subjective Data Analysis}
Although a large number of text-to-3D asset generation models have been developed in recent years, the corresponding works that specifically analyze and compare their generation performance are lacking. Considering that the generation quality of the text-to-3D asset is influenced by multiple factors such as prompts, algorithms, \textit{etc}, which leads to diverse perceptual quality and affects the user experience, based on the established AIGC-T23DAQA database, we conduct an in-depth analysis for the collected MOSs from multiple perspectives as follows.

Fig. \ref{fig:mos} demonstrates the distribution of MOS values obtained from subjective experiments. It can be observed that the correspondence distribution surpasses both the quality and authenticity distributions, suggesting that the current generation models learn more towards correspondence while ignoring the quality and authenticity attributes. The reason for this phenomenon is that the current T23DA method utilizes text-to-image models to constrain the correspondence between images rendered from different perspectives and text. These text-to-image models are trained on a large number of text-image pairs and perform well in text-image correspondence, ensuring good correspondence between generated 3D asset and text; However, the text-generated image model cannot guarantee the geometric texture consistency of three-dimensional objects from different perspectives, resulting in the strange geometric shapes and floaters in generated 3D asset. As a result, the quality and authenticity of the generated 3D assets are poorer than those of correspondence. To enhance the overall user preferences in the future, it is more important to improve the quality and authenticity attributes for the generated 3D assets.

Fig. \ref{fig:moscompare} (a) compares the human preference MOSs for different models, including Dreamfusion \cite{poole2022dreamfusion}, LatentNerf \cite{metzer2023latent}; Magic3D \cite{lin2023magic3d}, Prolificdreamer \cite{wang2024prolificdreamer}; SJC\cite{wang2023score}, TextMesh \cite{tsalicoglou2023textmesh}. Fig. \ref{fig:moscompare} (b) compares the human preference MOSs for different prompt length. Prompt length is divided into six intervals on average, with 1-6 on the x-axis representing interval numbers from short to long. We can find from it that: 1) The 3D assets generated by different text-to-3D generation models have significantly different perceptual preferences, and even with the same input prompt, the quality, authenticity, and correspondence vary greatly across different text-to-3D asset methods. Models including Prolificdreamer \cite{wang2024prolificdreamer}, Magic3D \cite{lin2023magic3d}, and Prolificdreamer \cite{wang2024prolificdreamer} exhibit the best quality, authenticity, and correspondence respectively. The reasons for the subjective score differences among different models: From Figure 9 in the manuscript, it can be seen that the best quality, authenticity, and correspondence are Prolificdreamer, Magic3D, and Prolificdreamer respectively. Prolificdreamer uses variational score distillation to instead of score distillation sampling which used in other methods and solve the problems of over-saturation, over-smoothing, and low-diversity. So the Prolificdreamer has better quality and correspondence. Magic3D uses coarse-to-fine strategy to generate 3D asset and a sparse 3D hash grid structure to represent 3D asset, which can reduce the generation of floaters, making generated 3D asset more authenticity. 2) When the prompt is short (1 \& 2), the model is easy to generate high quality, authenticity, and correspondence 3D assets, However, as prompt length increases (3, 4 \& 5), text-to-3D generation models may struggle to meet the requirements of human preferences and the entire prompt, resulting in a decline in the quality, authenticity, and correspondence scores. Finally, when the prompt length is extreme long, the explicit descriptions make the quality, authenticity scores higher, while the correspondence scores are still lower than the prompt length of 1 \& 2. The reasons for subjective score differences in different prompt lengths: When the prompt length is short, the generated 3D asset is less constrained by the text, making it easy to achieve better text asset correspondence. However, as the length increases, the text-asset correspondence decreases; When the prompts are too long, a more detailed description can help the models generate better textures and geometry, resulting in better authenticity and quality.




\begin{figure}[ht]
    \centering

    \subfigure[]{\begin{minipage}[t]{0.9\linewidth}
                \centering
                \includegraphics[width = 0.93\linewidth]{Figs/catplot.pdf}
                \end{minipage}}

    \subfigure[]{\begin{minipage}[t]{0.9\linewidth}
                \centering
                \includegraphics[width = 0.93\linewidth]{Figs/promptplot.pdf}
                \end{minipage}}
                
    \caption{Illustration of the impact of different models and prompt lengths on the perceptual quality of T23DAs respectively. (a) shows the subjective quality, authenticity, and correspondence score of T23DAs with different methods including Dreamfusion, LatentNerf, Magic3D, Prolificdreamer, SJC, and TextMesh respectively. (b) shows the subjective quality, authenticity, and correspondence score of T23DAs with different prompt lengths. Prompt length is divided into six intervals on average, with 1-6 on the x-axis representing interval numbers from short to long.}
    \label{fig:moscompare}
    \vspace{-0.6cm}
\end{figure}
