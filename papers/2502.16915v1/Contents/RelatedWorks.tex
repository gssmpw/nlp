
\section{Related Work}
\subsection{Text-to-3D Asset Generation}
In recent years, many 3D asset generation methods have been proposed, drawing inspiration from advancements in AI-based 2D image generation works. Early explorations in 3D generation \cite{henzler2019escaping} have leveraged generative adversarial network (GAN) algorithms, such as 3DGAN, to produce 3D models from probability space. The seminal work DreamFusion \cite{poole2022dreamfusion} have pioneered the utilization of pre-trained 2D text-to-image models for text-to-3D transformation via differentiable rendering. Their key methodology, score distillation sampling (SDS), involves uniformly sampling from the parameter space of pre-trained diffusion models to obtain gradients aligned with given text prompts. Building upon this foundation, Magic3D \cite{lin2023magic3d} have further enhanced the quality and efficiency of 3D asset generation through a two-step approach. Prolificdreamer \cite{wang2024prolificdreamer}, SJC\cite{wang2023score}, LatentNerf \cite{metzer2023latent} and TextMesh \cite{tsalicoglou2023textmesh} have optimized 3D asset generation by improving the representation of 3D assets and improving SDS. These works generally employ volunteers to conduct pairwise comparisons of results from different methods to ascertain the visual quality of generated 3D asset, underscoring the pressing need for a quality assessment algorithm tailored to generated 3D asset.

\begin{table*}[t]
\caption{Summary of the existing AIGC Quality Assessment databases and AIGC-T23DAQA database. The numbers in parentheses of score type represent the dimensions of the subjective experimental annotations.\label{tab:summary}}
\centering
\begin{tabular}{c|c|c|c|c|c|c|c|c}
\toprule
Type                           & \multicolumn{1}{c|}{Dataset} & \multicolumn{1}{c|}{Contents}         & \multicolumn{1}{c|}{Prompts} & \multicolumn{1}{c|}{Models} & \multicolumn{1}{c|}{Annotators} & \multicolumn{1}{c|}{Ratings} & \multicolumn{1}{c|}{Score type}   & \multicolumn{1}{c}{Public Available} \\ \hline
\multirow{8}{*}{Text-To-Image} & Pick-a-pic\cite{kirstain2024pick}        & 500,000         & 35,000  & 3  & -  & 500,000   & Preference         & $\checkmark$ \\
                               & HPS\cite{wu2023human}                    & 98,807         & 25,205  & 1  & -  & 98,807    & Preference         & $\checkmark$    \\
                               & ImageReward\cite{xu2024imagereward}      & 136,892 & 8,878   & 1  & -  & 136,892   & Seven Point Likert & $\checkmark$ \\
                               & AGIQA-1K\cite{zhang2023perceptual}       & 10,80                & 540     & 2  & 22 & 23,760    & MOS                & $\checkmark$   \\
                               & AGIQA-3K\cite{10262331}                  & 2,982             & 497     & 6  & 21 & 125,244   & MOS(2)             & $\checkmark$   \\
                               & AGIQA-20K\cite{li2024aigiqa}             & 20,000                & 20,000  & 15 & 21 & 420,000   & MOS                & $\checkmark$  \\
                               & AIGCIQA2023\cite{wang2023aigciqa2023}    & 2,400            & 100     & 6  & 28 & 201,600   & MOS(3)             & $\checkmark$  \\ 
                               & AIGCOIQA2024\cite{yang2024aigcoiqa2024}  & 300            & 25     & 5  & 20 & 18,000   & MOS(3)             & $\checkmark$  \\ \hline
\multirow{5}{*}{Text-To-Video} & Chivileva's\cite{chivileva2023measuring} & 1,005             & 201     & 5  & 24 & 48,240    & MOS(2)             & $\checkmark$     \\
                               & EvalCrafter\cite{liu2023evalcrafter}     & 3,500             & 700     & 7  & 3  & 73,500    & MOS(5)             & $\checkmark$  \\
                               & Vbench\cite{huang2023vbench}             & 6,984         & 1,746   & 4  & -  & 209,520   & Preference         & $\checkmark$  \\
                               & FETV\cite{liu2023fetv}                   & 2,476             & 619     & 3  & 3  & 11,142    & MOS(2)             & $\checkmark$  \\
                               & T2VQA-DB\cite{kou2024subjective}         & 1,000                & 1,000   & 9  & 27 & 27,000    & MOS                & $\checkmark$  \\ \hline
Text-To-3D                     & Ours                                     & 969             & 170     & 6  & 17 & 49,419    & MOS(3)             & $\checkmark$   \\
\bottomrule
\end{tabular}
\vspace{-0.5cm}
\end{table*}
\vspace{-15pt}
\subsection{3D Quality Assessment}
% important
% MQA PCQA
% Model-based Projection-based
3D asset quality assessment can be used to choose or optimize 3D assets, and contribute to VR \cite{duan2024quick, duan2023attentive, duan2018perceptual} and AR \cite{duan2022augmented, duan2022saliency} applications. Currently, most 3D asset quality assessment studies mainly research the mesh quality assessment (MQA) and point cloud quality assessment (PCQA) problems, as mesh and point cloud formats represent common structures of 3D models. According to the quality feature extraction methods, the MQA methods can be divided into two main categories, including: model-based approaches and projection-based approaches. Model-based methods \cite{m1, tian-color} typically compute local features at the vertex level and global color features from texture images, subsequently aggregating these features into the quality score. However, projection-based methods \cite{zhang2023gms} need to first generate projection images from the mesh, then utilize mature 2D IQA or 3D video quality assessment (VQA) tools to predict mesh quality scores. Similarly, PCQA methods can also be categorized into model-based and projection-based methods. However, due to the discrepancy in data storage between point clouds and meshes, model-based PCQA methods \cite{tian2017geometric, torlig2018novel} typically extract geometry features from point-wise gradient vector distances and color features from point-wise color attributes. Projection-based PCQA methods \cite{zhang2022treating} generally follow projection-based MQA methods,  which extracts features from the projected images of point clouds by 2D IQA and 3D VQA tools to predict quality scores.

Model-based methods does not exist information loss during evaluating but demand considerable computational resources due to the complexity of high-fidelity point clouds or meshes, while projection-based approaches relying on mature 2D IQA or 3D VQA tools have lower computational complexity, but the performance may be influenced by the selection of viewpoints. To mitigate the variability inherent in viewpoint selection, several studies \cite{zhang2022treating, fan2022no} advocate for employing multiple projections, significantly enhancing accuracy compared to single-projection approaches. Compared to traditional 3D quality assessment tasks and methods \cite{zhang2022no, liu2021pqa, chai2024plain}, our proposed database and method focus on text-to-3d asset, which is more potential and challenge.
% Model-based methods does not exist information loss during evaluating but demand considerable computational resources due to the complexity of high-fidelity point clouds or meshes, while projection-based approaches relying on mature 2D IQA or 3D VQA tools have lower computational complexity, but the performance may be influenced by the selection of viewpoints. To mitigate the variability inherent in viewpoint selection, several studies advocate for employing multiple projections, significantly enhancing accuracy compared to single-projection approaches. Compared to traditional 3D content quality assessment tasks and methods, our proposed database and method focus on text-to-3d content, which is more potential and challenge.

\vspace{-15pt}
\subsection{AIGC Image and Video Quality Assessment}
With the success of diffusion models in image generation tasks, numerous text-to-image methods have emerged. Concurrently, to evaluate the quality of AI-generated images (AIGI), several AGI databases and AI-generated image quality assessment (IQA) methods have surfaced. These databases can be classified into two main types including coarse-grained and fine-grained. Coarse-grained databases such as HPS\cite{wu2023human} and Pick-A-Pic \cite{kirstain2024pick} generally gather paired image comparison results or series image selection results for images generated by Stable Diffusion or other text-to-image models, as subjective evaluation results. In contrast, fine-grained databases like AGIQA-20K \cite{li2024aigiqa} and AIGCIQA2023 \cite{wang2023aigciqa2023} generally conducts subjective quality rating experiments from multiple perspectives to evaluate human preferences for AIGIs. For objective AIGI quality assessment, IS \cite{gulrajani2017improved} and FID \cite{heusel2017gans} have long been adopted to evaluate the fidelity of a collection of generated images. Recently, numerous specialized algorithms for evaluating AIGIs have emerged \cite{xu2024imagereward, wu2023human}. These algorithms typically leverage contrastive language-image pre-training (CLIP) \cite{radford2021learning} to extract text-image features and utilize classical classification backbone networks to extract image perception features\cite{wang2024understanding}, which are then fused to predict preference scores.

Recently, OpenAI's video generation model Sora has demonstrated the ability to generate one-minute high-fidelity videos, drawing public attention to the task of text-generated videos. Recently, some quality assessment studies for AI-generated videos (AIGV) have also been conducted. Chivileva et al. \cite{chivileva2023measuring} have proposed a dataset comprising 1,005 videos generated by 5 text-to-video models, with quality assessment performed by 24 annotators to provide subjective scores. Similarly, Kou et al. \cite{kou2024subjective} have established the expansive text-to-video quality assessment database (T2VQA-DB), consisting of 10,000 videos generated by 9 different text-to-video models, each accompanied by its corresponding MOS. Text-to-video quality assessment algorithms typically combine NR-VQA and 2D AIGIQA methods to predict text-to-video quality. An overview of current AIGC quality assessment databases have been give in Table \ref{tab:summary}.


% Text 2 3D
% \subsection{Metrics for T23C Generation}
% previous work usually use 
% FID vote for model
% so we create a dataset
% To ensure content diversity, our T23DCQA-DB dataset contain six representative text to 3D content models: Prolificdreamer \cite{wang2024prolificdreamer}, Dreamfusion \cite{poole2022dreamfusion}, Magic3D \cite{lin2023magic3d}, SJC\cite{wang2023score}, LatentNerf \cite{metzer2023latent} and TextMesh \cite{tsalicoglou2023textmesh}.