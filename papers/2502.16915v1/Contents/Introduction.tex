\section{Introduction}
% \IEEEPARstart{T}{his} file is intended to serve as a ``sample article file''
% for IEEE journal papers produced under \LaTeX\ using \cite{chetouani2021deep}

\IEEEPARstart{T}{he} 3D asset generation has long been an important task in the field of computer vision (CV) and artificial intelligence (AI), which pursues high-quality and automatic 3D model or view synthesis \cite{henzler2019escaping, tsalicoglou2023textmesh}. The recent advances in text-to-image generation via diffusion models have spurred the development of numerous text-to-3D asset generation methodologies, exemplified by works including Dreamfusion \cite{poole2022dreamfusion}, Prolificdreamer \cite{wang2024prolificdreamer}, \textit{etc}. However, the text-to-3D asset synthesis is influenced by various factors such as prompts, and techniques, leading to diverse perceptual qualities that directly impact user experience. Consequently, there is a crucial need for a subjective-consistent quality assessment framework to evaluate text-to-3D assets. However, existing quality assessment models fail to adequately address this task. As shown in Fig. \ref{fig:thumbnail}, on one hand, distortions introduced by text-to-3D asset generation models, including unreal structures, unreasonable components, discontinuous views, are significantly different from those encountered in traditional 3D asset, which invalidates traditional quality assessment methods. On the other hand, conventional quality assessment models do not take the alignment between text and 3D asset into consideration, which is a pivotal evaluation aspect for text-to-3D assets.

\begin{figure}[t]
    \centering
    \includegraphics[width = 0.48\textwidth]{Figs/thumbnail.pdf}
    \caption{Illustration of the difference between traditional 3d asset and AI generated 3d asset, whose perceptual quality are affected by different attributes.}
    % \captionsetup{justification=centering}
    \label{fig:thumbnail}
    \vspace{-0.5cm}
\end{figure}

Current text-to-3D asset generation models generally uses image fidelity evaluation metrics such as Inception Score (IS) \cite{gulrajani2017improved} and Fréchet Inception Distance (FID) \cite{heusel2017gans} to assess the quality of text-to-3D assets. However, these metrics cannot evaluate the fidelity, quality and text-image correspondence of a single generated image. Moreover, previous quality assessment metrics designed for natural images, omnidirectional images, natural videos, user generated videos, point clouds, meshes \textit{etc.}\cite{zhou2024blind,chen2024dynamic,sun2022a,wu2023dover}, may not generalize well for assessing text-to-3D assets. There are two main reasons for this: 1) Previous quality assessment methods can only predict the quality aspect of the generated asset, while ignoring the authenticity aspect and the association between the prompt and the generated 3D asset; 2) The distortions existed in text-to-3D asset, such as floating artifacts and multiple similar planes, are different from the common distortions existed in traditional 3D models, such as noise and compression, which makes most traditional quality assessment methods unable to generalize well to assess the quality of T23DA. Several text-to-3D generation works also conduct the user studies, which let volunteers choose the better generated 3D assets, to validate the effectiveness of a generation framework. However, user studies are time consuming and inefficient, which further strengths the importance of developing an objective perception evaluation algorithm for text-to-3D assets. In the same time, T23DAQA has many potential applications in real-world scenarios: 1) it has the potential to be used to optimize the perceptual quality of generated 3D asset as a loss function in the training of a text-to-3D asset model. 2) Nowadays, many T23DA companies have emerged to assist game designers and filmmakers in creating 3D asset, such as Genie \cite{genie} and Meshy\cite{meshy}. However, the generated 3D asset is not always excellent and usually requires to select the best 3D asset from multiple generated results. T23DAQA can automatically filter out generated 3D asset with better perceptual quality.
% The distortions existed in text-to-3D content, such as floating artifacts and multiple overlapping planes, differ from the typical distortions found in traditional 3D models, such as noise and compression.
\begin{figure*}[t]
    \centering
    \includegraphics[width = 0.98\textwidth]{Figs/spotlight.pdf}
    % \caption{An Overview of the established AIGC-T23DAQA database and the proposed T23DAQA method. AIGC-T23DAQA database is the first and the largest text-to-3d assets quality assessment database. The proposed method achieves the state-of-the-art performance in evaluating the perceptual attributes of text-to-3d assets.}?
    \caption{\color{black}{An Overview of the established AIGC-T23DAQA database and the proposed T23DAQA method. AIGC-T23DAQA database is the first and the largest text-to-3d assets quality assessment database. This database encompasses 969 validated 3D assets generated from 170 prompts via 6 popular text-to-3D asset generation models, and corresponding subjective quality ratings. In addition, we popose a T23DAQA method to predict the text-to-3D asset quality from three aspects: shape, texture, and correspondence. The proposed method achieves the state-of-the-art performance in evaluating the perceptual attributes of text-to-3d assets.}}
    \label{fig:spotlight}
    \vspace{-0.5cm}
\end{figure*}
As shown in Fig. \ref{fig:spotlight}, in order comprehensively and accurately evaluate text-to-3D assets, we conduct both subjective and objective assessment studies in this work. Firstly, we establish the largest-scale subjective text-to-3D asset quality assessment dataset to date, named AIGC-T23DAQA database. This dataset comprises 969 text-to-3D assets generated by six distinct text-to-3D methods using 170 text prompts. Based on the generated 3D assets, a subjective experiment is conducted to collect the quality, authenticity, and text-asset correspondence ratings, respectively, which are further processed to obtain mean opinion scores (MOSs).  To the best of our knowledge, this is the first database for text-to-3D generated asset evaluation from multiple perspectives.
Based on the established AIGC-T23DAQA database, we propose a novel model equipped with multi-modality foundation models for better text-to-3D asset quality assessment, which is named as T23DAQA model. Since the most of recent text-to-3D asset generation methods employ neural radiation fields (NeRF) to represent 3D asset, which are stored in multilayer perceptrons (MLPs) or voxels. The NeRF-based representation generally lacks explicit 3D models, which poses challenges for 3D quality assessment. So our proposed method is a projection-based quality evaluation algorithm that extracts perceptual quality features from three aspects, including: shape, texture, and text-asset correspondence, and then fuse the extracted features to predict quality, authenticity, and text-asset correspondence scores. Based on the constructed AIGC-T23DAQA database, we establish a benchmark for it including many SOTA quality assessment methods and validate the effectiveness of the proposed method on this benchmark. Experimental results demonstrate that our proposed method achieves the best performance compared to these state-of-the-art methods for evaluating text-to-3D assets, which manifests the superiority of the proposed model.

% 一定要说和之前工作的区别 以及可以benchmark 其他t23d模型
% blue2black
\textcolor{black}{In summary, the motivation of conducting this work is that there are many text-based 3D asset generation methods and corresponding generated assets, however, the existing quality assessment algorithms cannot well evaluate the performance of these models and the quality of the generated assets. As the first text-to-3D asset quality assessment work, the proposed database can be used to develop corresponding models, which can be used to benchmark text-to-3D generation methods, select generated 3D assets with better quality and help optimize text-to-3D models,\textit{etc.}} This paper makes the following contributions:
\begin{itemize}
    \item We construct so far the largest text-to-3D assets quality assessment database, named AIGC-T23DAQA database, and to the best of our knowledge, this is the first work that tries to study human preferences for AI-based text-to-3D assets from multiple perspectives.
    
    \item We propose a novel projection-based evaluator for better text-to-3D asset quality assessment, termed T23DAQA model, which leverages a 3D encoder, two 2D encoders, and multi-modality foundation models to extract features encompassing 3D shape, texture, and text-asset correspondence to predict human preference scores.
    
    \item Comprehensive experimental results demonstrate that our proposed method surpasses existing state-of-the-art NR-IQA, NR-VQA, NR-MQA, NR-PCQA, LMMQA, T2IQA, T2VQA models, and text-image alignment methods, affirming its efficacy in measuring the perceptual quality of text-to-3D assets. Furthermore, the ablation experiments validate the effectiveness of the proposed module.
    
\end{itemize}

