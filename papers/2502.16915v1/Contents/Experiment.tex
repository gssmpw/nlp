\section{Experimental Validation}
This section begins with a detailed outline of the experimental protocol, followed by an assessment of the performance of both conventional perception methods and the proposed approach on the AIGC-T23DAQA database. These perception models include traditional NR-IQA, NR-VQA, NR-MQA, NR-PCQA, LMMQA, T2IQA, T2VQA and alignment methods. Subsequently, we undertake ablation studies to illustrate the robustness and effectiveness of the proposed methodology.

\begin{table*}[tbph]

\caption{Performance results of learning-based metrics on our AIGC-T23DAQA database. [Key: {\bf\textcolor{red}{Best}, \bf\textcolor{blue}{Second Best}}]}
\label{tab:training}
\centering
\scalebox{1.0}{
\begin{tabular}{c|ll|rrr|rrr|rrr}
\toprule
\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}} Index \end{tabular}} & \multicolumn{2}{c|}{\textbf{Dimension}}  & \multicolumn{3}{c|}{Authenticity}  & \multicolumn{3}{c|}{Correspondence} & \multicolumn{3}{c}{Quality}                                                 \\ \cline{2-12}
 & \multicolumn{1}{c|}{\textbf{Type}}              & \multicolumn{1}{c|}{\textbf{Metric}} & \multicolumn{1}{c}{SRCC} & \multicolumn{1}{c}{KRCC} & \multicolumn{1}{c|}{PLCC} & \multicolumn{1}{c}{SRCC} & \multicolumn{1}{c}{KRCC} & \multicolumn{1}{c|}{PLCC} & \multicolumn{1}{c}{SRCC} & \multicolumn{1}{c}{KRCC} & \multicolumn{1}{c}{PLCC} \\ \hline
A & \multicolumn{1}{l|}{\multirow{10}{*}{\begin{tabular}[c]{@{}l@{}} NR-IQA \end{tabular}}}  & Resnet-18\cite{he2016deep} & 0.5114 & 0.3618 & 0.5267 & 0.5652 & 0.4027 & 0.6240 & 0.6970 & 0.5166 & 0.7004 \\
B & \multicolumn{1}{l|}{} & Resnet-34\cite{he2016deep} & 0.5688 & 0.4047 & 0.5846 & 0.5794 & 0.4181 & 0.6325 & 0.7122 & 0.5288 & 0.7104 \\
C & \multicolumn{1}{l|}{} & Resnet-50\cite{he2016deep} & 0.4657 & 0.3354 & 0.4961 & 0.4750 & 0.3364 & 0.5629 & 0.6441 & 0.4772 & 0.6624 \\
D & \multicolumn{1}{l|}{} & Swin-T\cite{liu2021swin} & 0.5934 & 0.4273 & 0.6241 & 0.6360 & 0.4669 & 0.6951 & 0.7515 & 0.5734 & 0.7678 \\
E & \multicolumn{1}{l|}{} & Swin-S\cite{liu2021swin} &  0.6263 & 0.4541 & 0.6478 & 0.6434 & 0.4817 & 0.6983 & \textcolor{blue}{0.7652} & 0.5869 & \textcolor{blue}{0.7820} \\
F & \multicolumn{1}{l|}{} & Swin-B\cite{liu2021swin} & 0.6197 & 0.4483 & 0.6415 & 0.6431 & 0.4776 & 0.6995 & 0.7617 & 0.5844 & 0.7774 \\
G & \multicolumn{1}{l|}{} & Swin-L\cite{liu2021swin} & 0.6069 & 0.4396 & 0.6323 & 0.6539 & 0.4850 & 0.7132 & 0.7592 & 0.5810 & 0.7714 \\ 
H & \multicolumn{1}{l|}{} & CNNIQA\cite{kang2014convolutional} & 0.4281 & 0.2969 & 0.4332 & 0.5562 & 0.3932 & 0.6104 & 0.6776 & 0.4954 & 0.6658 \\
I & \multicolumn{1}{l|}{} & StairIQA\cite{sun2023blind} & 0.5002 & 0.3579 & 0.5375 & 0.4635 & 0.3360 & 0.5773 & 0.6373 & 0.4804 & 0.6715 \\ 
J & \multicolumn{1}{l|}{} & HyperIQA\cite{Su_2020_CVPR} & 0.6069 & 0.4396 & 0.6323 & 0.6539 & 0.4850 & 0.7132 & 0.7592 & 0.5810 & 0.7714 \\ 
\hline
K & \multicolumn{1}{l|}{\multirow{9}{*}{\begin{tabular}[c]{@{}l@{}}NR-VQA\end{tabular}}} & MC3-18\cite{tran2018closer} & 0.5702 & 0.4090 & 0.5948 & 0.6203 & 0.4554 & 0.6623 & 0.7421 & 0.5631 & 0.7528 \\
L & \multicolumn{1}{l|}{} & R2P1D-18\cite{tran2018closer} & 0.5864 & 0.4168 & 0.5903 & 0.6134 & 0.4520 & 0.6726 & 0.7423 & 0.5613 & 0.7474 \\
M & \multicolumn{1}{l|}{} & R3D-18\cite{tran2018closer} & 0.5869 & 0.4141 & 0.5951 & 0.5962 & 0.4327 & 0.6626 & 0.7430 & 0.5608 & 0.7466 \\
N & \multicolumn{1}{l|}{} & Swin3D-T\cite{liu2022video} & 0.6190 & 0.4521 & 0.6433 & 0.6517 & 0.4885 & 0.7034 & 0.7556 & 0.5842 & 0.7752 \\
O & \multicolumn{1}{l|}{} & Swin3D-S\cite{liu2022video} & 0.6317 & 0.4641 & 0.6517 & 0.6394 & 0.4795 & 0.7030 & 0.7579 & 0.5846 & 0.7768 \\
P & \multicolumn{1}{l|}{} & Swin3D-B\cite{liu2022video} & 0.6181 & 0.4502 & 0.6447 & 0.6294 & 0.4707 & 0.6973 & 0.7544 & 0.5809 & 0.7757 \\
Q &\multicolumn{1}{l|}{} & SimpleVQA\cite{sun2022a} & 0.6072 & 0.4545 & 0.6404 & 0.6102 & 0.4627 & 0.6971 & 0.7539 & \textcolor{blue}{0.5872} & 0.7712\\
R &\multicolumn{1}{l|}{} & Fast-VQA\cite{wu2022fastquality} & 0.6457 & 0.4690 & 0.6501 & 0.6477 & 0.4816 & 0.7071 & 0.7621 & 0.5813 & 0.7747 \\
S &\multicolumn{1}{l|}{} & DOVER\cite{wu2023dover} & \textcolor{blue}{0.6534} & \textcolor{blue}{0.4745} & \textcolor{blue}{0.6627} & \textcolor{blue}{0.6791} & 0.4954 & 0.7059 & 0.7508 & 0.5805 & 0.7708 \\ \hline
T & \multicolumn{1}{l|}{\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}NR-MQA\end{tabular}}} & NR-SVR\cite{abouelaziz2016no} & 0.3479 & 0.2637 & 0.4769 & 0.3163 & 0.3473 & 0.4921 & 0.5134 & 0.3904 & 0.5375 \\
U &\multicolumn{1}{l|}{} & NR-GRNN\cite{abouelaziz2016curvature} & 0.5613 & 0.3875 & 0.5336 & 0.4065 & 0.3025 & 0.5581 & 0.6052 & 0.4703 & 0.6074 \\\hline
V & \multicolumn{1}{l|}{\multirow{3}{*}{\begin{tabular}[c]{@{}l@{}}NR-PCQA\end{tabular}}} & 3D-NSS\cite{zhang2022no} & 0.3075 & 0.2190 & 0.3062 & 0.3969 & 0.2763 & 0.3094 & 0.5919 & 0.3937 & 0.5112 \\
W &\multicolumn{1}{l|}{} & ResSCNN\cite{liu2023point} & 0.4901 & 0.2445 & 0.4194 & 0.5965 & 0.3024 & 0.6785 & 0.6098 & 0.4961 & 0.6741 \\
X &\multicolumn{1}{l|}{} & IT-PCQA\cite{yang2022no} & 0.5663 & 0.3442 & 0.5950 & 0.4124 & 0.3551 & 0.5849 & 0.6405 & 0.4978 & 0.6797 \\ \hline
Y &\multicolumn{1}{l|}{\multirow{3}{*}{\begin{tabular}[c]{@{}l@{}}\textcolor{black}{T2IQA}\end{tabular}}} & \textcolor{black}{MA-AGIQA} \cite{wang2024large} & 0.6307 & 0.4558 & 0.6369 & 0.5965 & 0.4303 & 0.6713 &  0.7603 & 0.5767 & 0.7434 \\
Z &\multicolumn{1}{l|}{} & \textcolor{black}{MoE-AGIQA} \cite{yang2024moe} & 0.6386 & 0.4592 & 0.6396 & 0.6673 & 0.4999 & 0.6857 & 0.7350 & 0.5685 & 0.7454 \\
AA &\multicolumn{1}{l|}{} & \textcolor{black}{CLIP-AGIQA} \cite{tang2025clip}  & 0.6373 & 0.4689 & 0.6531 & 0.6739 & \textcolor{blue}{0.5057} & \textcolor{blue}{0.7197} & 0.7428 & 0.5683 & 0.7548 \\
\hline
AB &\multicolumn{1}{l|}{\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}\textcolor{black}{T2VQA}\end{tabular}}} & \textcolor{black}{T2VQA} \cite{kou2024subjective} & 0.6317 & 0.4365 & 0.6289 & 0.6489 & 0.4644 & 0.6704 & 0.7319 & 0.5526 & 0.7378 \\
AC &\multicolumn{1}{l|}{} & \textcolor{black}{TriVQA} \cite{qu2024exploring} & 0.6357 & 0.4588 & 0.6364 & 0.6353 & 0.4505 & 0.6717 & 0.7291 & 0.5331 & 0.7228 \\
\hline
AD & \multicolumn{1}{l|}{} & Proposed & \textcolor{red}{0.6728} & \textcolor{red}{0.4909} & \textcolor{red}{0.6840} & \textcolor{red}{0.7000} & \textcolor{red}{0.5157} & \textcolor{red}{0.7297} & \textcolor{red}{0.7853} & \textcolor{red}{0.5987} & \textcolor{red}{0.7828} \\
\bottomrule 
\end{tabular}
}
\vspace{-0.5cm}
\end{table*}
\vspace{-15pt}
\subsection{Experiment Protocol}

1) Baseline Algorithms:
In our evaluation, we incorporate a selection of representative NR-IQA, NR-VQA, NR-MQA, NR-PCQA algorithms, LMMQA, T2IQA, T2VQA and alignment methods as benchmarks for comparative analysis. These baseline methods encompass:
\begin{itemize}
    \item General NR-IQA methods: We test 20 baseline IQA methods categorized into two groups, including: traditional NR-IQA models and deep neural network (DNN) based NR-IQA models. For traditional NR-IQA, the selection models comprises NIQE \cite{mittal2012making}, ILNIQE \cite{zhang2015feature}, BRISQUE \cite{mittal2012no}, QAC \cite{xue2013learning}, FISBLIM \cite{gu2013fisblim}, BMPRI \cite{min2018blind}, BMPRI \cite{min2017blind}, BPRI-PSS \cite{min2017blind}, BPRI-LSSs \cite{min2017blind}, and BPRI-LSSn \cite{min2017blind}. In the realm of DNN-based NR-IQA, we consider Resnet-18 \cite{he2016deep}, Resnet-34 \cite{he2016deep}, Resnet-50 \cite{he2016deep}, Swin-T \cite{liu2021swin}, Swin-S \cite{liu2021swin}, Swin-B \cite{liu2021swin}, Swin-L \cite{liu2021swin}, CNNIQA \cite{kang2014convolutional}, HyperIQA \cite{sun2023blind}, and StairIQA \cite{sun2023blind}. These metrics represent widely used NR-IQA methodologies applied in practical applications.
    
    \item General NR-VQA methods: We test 9 baseline VQA methods on the constructed database including MC3-18\cite{tran2018closer}, R2P1D-18 \cite{tran2018closer}, R3D-18 \cite{tran2018closer}, Swin3D-T \cite{liu2022video}, Swin3D-S \cite{liu2022video}, Swin3D-B \cite{liu2022video}, SimpleVQA \cite{sun2022a}, Fast-VQA \cite{wu2022fastquality}, and DOVER \cite{wu2023dover}. These metrics serve as prevalent NR-VQA measures utilized in practical scenarios such as video coding and enhancement.

    \item General NR 3D quality assessment methods: We test 5 baseline 3DQA methods including NR-SVR \cite{abouelaziz2016no}, NR-GRNN \cite{abouelaziz2016curvature}, 3D-NSS \cite{zhang2022no}, ResSCNN \cite{liu2023point}, and IT-PCQA \cite{yang2022no}.
    
    \item Alignment methods: We select 5 baseline alignment methods: CLIPScore\cite{hessel2021clipscore}, HPS \cite{wu2023human}, ImageReward \cite{xu2024imagereward}, PickScore \cite{kirstain2024pick}, and ViCLIP \cite{wang2023internvid}. The first four metrics facilitate image-to-text alignment, and ViCLIP is tailored for video-to-text alignment applications.

    \textcolor{black}{\item LMMQA, T2IQA and T2VQA methods: We selected Q-align\cite{wu2023q}, T2I-Scorer\cite{wu2024t2i}, and VQAScore\cite{wu2024t2i} as representatives of LMMQA methods. Meanwhile, T2IQA and T2VQA mthods selecte MA-AGIQA\cite{wang2024large}, MoE-AGIQA\cite{yang2024moe}, CLIP-AGIQA\cite{tang2025clip} and T2VQA\cite{kou2024subjective}, TriVQA\cite{qu2024exploring} respectively.   }
    
\end{itemize}


2) Experimental and settings: 
For traditional NR-IQA and alignment methods and LMMQA, our evaluation encompasses the entire AIGC-T23DAQA database. For each projection video, these metrics predict scores for individual frames and derive the final prediction results by averaging these scores. However, for ViCLIP, the entire video is directly utilized to predict the final score.
As for CNN-based NR-IQA and general NR-VQA methods, we undertake fine-tuning on our AIGC-T23DAQA database. For SimpleVQA, Fast-VQA, DOVER, T2IQA, and T2VQA, we evaluate their performance using the provided open-source implementation. For the remaining algorithms, each projected video is segmented into an average of 12 segments. During training, one frame is randomly sampled from each segment, resulting in a total of 12 frames used as input. During testing, the first frame from each segment is selected. For IQA algorithms, the average score of the selected frames is computed as the final result.
Following the settings used in previous works \cite{wu2022fastquality, wu2023dover}, we partition the AIGC-T23DAQA database into training and test sets at a ratio of 4:1. Additionally, we conduct 10 random splits of the dataset and average the results to ensure unbiased performance comparison. We use the Adam optimizer with the initial learning rate set as $1e^{-4}$ and set the batch size as 4. The training process is stopped after 50 epochs. The resolution of input frames is rescaled to $224 \times 224$. 
\textcolor{black}{The image and text encoders used for text-image alignment feature extraction are from CLIP \cite{radford2021learning}. The 3D encoder for shape feature extraction is Swin3D-S\cite{liu2022video}, initialized with weights pretrained on the Kinetics dataset \cite{kay2017kinetics}. The 2D encoders for texture feature extraction are Swin-S\cite{liu2021swin}, initialized with weights pretrained on the ImageNet-1K dataset \cite{deng2009imagenet}.}
For NR-MQA and NR-PCQA, We export the generated 3D assets to mesh models using the Marching Cubes algorithm and convert the exported mesh models to point clouds using MeshLab. In the same time, we test several NR-MQA and NR-PCQA metrics on our proposed database.

% \cite{deng2009imagenet, kay2017kinetics, radford2021learning}
3) Evaluation Criteria:
To evaluate the predictive accuracy of quality metrics, we employ three widely recognized global indicators, including Spearman’s Rank-Order Correlation Coefficient (SRCC), Kendall’s Rank-Order Correlation Coefficient (KRCC), and PLCC for assessing prediction monotonicity. Recognizing the potential presence of nonlinear mapping characteristics between objective scores and subjective scores, we apply score alignment by mapping the predicted values using the five-parameter logistic function, following the standard practice recommended in prior research \cite{sheikh2006statistical}:
\begin{equation}
\hat{Y}=\beta_{1}\left(0.5-\frac{1}{1+e^{\beta_{2}\left(Y-\beta_{3}\right)}}\right)+\beta_{4} Y+\beta_{5},
\end{equation}
where $\{\beta_{i} \mid i=1,2, \ldots, 5\}$ represent the parameters for fitting, $Y$ and $\hat{Y}$ stand for predicted and fitted scores respectively.

% \begin{figure*}[t]
%     \centering
%     \includegraphics[width = 0.83\textwidth]{Figs/Appendix-a.pdf}
%     \caption{The selected 3D contents using the random selection method and using our proposed assessment metric. The left part is selected randomly. In the score under the image, ``a", ``c", ``q" represents authenticity, text-content correspondence, and quality respectively.}
%     \label{fig:appendixa}
%     \vspace{-0.4cm}
% \end{figure*}

% \begin{figure*}[!h]
%     \centering
%     \includegraphics[width = 0.83\textwidth]{Figs/Appendix-c.pdf}
%     \caption{The generated 3D contents without/with our method as a loss. The left part is generated contents without our method. In the score under the image, ``a", ``c", ``q" represents authenticity, text-content correspondence, and quality respectively.}
%     \label{fig:appendixc}
%     \vspace{-0.5cm}
% \end{figure*}

\begin{table*}[tbph]

\caption{Ablation study on AIGC-T23DAQA database. [Key: {\bf\textcolor{red}{Best}, \bf\textcolor{blue}{Second Best}}] The a-c represents the use of only text-image alignment feature extraction, texture feature extraction, and shape feature extraction module respectively, d-f represents not using the shape feature extraction, texture feature extraction, and text-image alignment feature extraction module, g uses all modules.}
\label{tab:ablation}
\centering
\scalebox{1.0}{
\begin{tabular}{c|rrr|rrr|rrr}
\toprule
% \multicolumn{2}{c|}{\textbf{Dimension}}  & \multicolumn{3}{c|}{Authenticity}  & \multicolumn{3}{c|}{Correspondence} & \multicolumn{3}{c}{Quality}                                                 \\ \hline
% \multicolumn{1}{c|}{\textbf{Type}}              & \multicolumn{1}{c|}{\textbf{Metric}} & \multicolumn{1}{c}{SRCC} & \multicolumn{1}{c}{KRCC} & \multicolumn{1}{c|}{PLCC} & \multicolumn{1}{c}{SRCC} & \multicolumn{1}{c}{KRCC} & \multicolumn{1}{c|}{PLCC} & \multicolumn{1}{c}{SRCC} & \multicolumn{1}{c}{KRCC} & \multicolumn{1}{c}{PLCC} \\ \hline
% \multicolumn{1}{l|}{\multirow{7}{*}{\begin{tabular}[c]{@{}l@{}} NR-IQA \end{tabular}}}  & Resnet-18\cite{he2016deep} & 0.5114 & 0.3618 & 0.5267 & 0.5652 & 0.4027 & 0.6240 & 0.6970 & 0.5166 & 0.7004 \\
% Resnet-34\cite{he2016deep} & 0.5688 & 0.4047 & 0.5846 & 0.5794 & 0.4181 & 0.6325 & 0.7122 & 0.5288 & 0.7104 \\
\textbf{Dimension}  & \multicolumn{3}{c|}{Authenticity}  & \multicolumn{3}{c|}{Correspondence} & \multicolumn{3}{c}{Quality} \\ \hline
\textbf{Model} & \multicolumn{1}{c}{SRCC} & \multicolumn{1}{c}{KRCC} & \multicolumn{1}{c|}{PLCC} & \multicolumn{1}{c}{SRCC} & \multicolumn{1}{c}{KRCC} & \multicolumn{1}{c|}{PLCC} & \multicolumn{1}{c}{SRCC} & \multicolumn{1}{c}{KRCC} & \multicolumn{1}{c}{PLCC} \\ \hline

a & 0.6414 & 0.4654 & 0.6572 & 0.6805 & 0.5005 & 0.7073 & 0.7538 & 0.5683 & 0.7492 \\
b & 0.5882 & 0.4288 & 0.6092 & 0.6114 & 0.4477 & 0.6699 & 0.7467 & 0.5672 & 0.7539 \\
c & 0.5762 & 0.4162 & 0.6099 & 0.6070 & 0.4522 & 0.6852 & 0.7281 & 0.5544 & 0.7469 \\
d & 0.6369 & 0.4608 & 0.6504 & 0.6884 & 0.5080 & 0.7137 & 0.7627 & 0.5747 & 0.7565 \\
e & \textcolor{blue}{0.6665} & \textcolor{blue}{0.4853} & \textcolor{blue}{0.6792} & \textcolor{blue}{0.6997} & \textcolor{blue}{0.5082} & \textcolor{blue}{0.7277} & \textcolor{blue}{0.7766} & \textcolor{blue}{0.5891} & \textcolor{blue}{0.7729} \\
f & 0.5406 & 0.3962 & 0.5533 & 0.5790 & 0.4352 & 0.6301 & 0.6979 & 0.5315 & 0.7041 \\
g & \textcolor{red}{0.6728} & \textcolor{red}{0.4909} & \textcolor{red}{0.6840} & \textcolor{red}{0.7000} & \textcolor{red}{0.5157} & \textcolor{red}{0.7297} & \textcolor{red}{0.7853} & \textcolor{red}{0.5987} & \textcolor{red}{0.7828} \\
\bottomrule
\end{tabular}
}
\vspace{-0.5cm}
\end{table*}


\begin{figure*}[ht]
    \centering

    \subfigure[]{\begin{minipage}[t]{0.32\linewidth}
                \centering
                \includegraphics[width = 0.98\linewidth]{Figs/st-a.pdf}
                \end{minipage}}
    \subfigure[]{\begin{minipage}[t]{0.32\linewidth}
                \centering
                \includegraphics[width = 0.98\linewidth]{Figs/st-c.pdf}
                \end{minipage}}
    \subfigure[]{\begin{minipage}[t]{0.32\linewidth}
                \centering
                \includegraphics[width = 0.98\linewidth]{Figs/st-q.pdf}
                \end{minipage}}
                            
    \caption{The results of statistical tests on the AIGC-T23DAQA database. A black/white block indicates that the row method is inferior/superior to the column method, while a gray block signifies that there is no statistical difference between the row and column methods. The methods are identified by the same index as in Table \ref{tab:training}. }
    \label{fig:compare}
    \vspace{-0.5cm}
\end{figure*}
\vspace{-15pt}
\subsection{Experimental Results and Discussion}

Table \ref{tab:zero-shot} shows the performance results of various traditional NR-IQA methods and alignment methods on the established AIGC-T23DAQA database. From the results, we can get the following observation and conclusions:  1) Traditional NR-IQA methods exhibit relative poor performance. This is because NR-IQA methods predict image perception quality through handcrafted natural image texture features, which have a low correlation with the perception quality of generated 3D assets. 2) The quality of generated 3D asset is more correlated with traditional NR-IQA methods than authenticity and text 3D asset correspondence, which manifests that the authenticity and text-asset correspondence are two unique factors significantly different with the quality.  3) Alignment methods achieve commendable results due to the strong correlation between the generated 3D assets and prompts. Hence, employing a text-image alignment model can significantly enhance prediction accuracy. 4) Predicting the text-3d asset correspondence of generated 3D asset can assist in predicting its authenticity and quality. Table \ref{tab:training} showcases the performance results of different DNN-based NR-IQA methods, NR-VQA methods, NR-MQA methods, NR-PCQA methods and our proposed method on the proposed AIGC-T23DAQA database. The observations and conclusions are summarized as follows. 1) Our proposed method surpasses all baselines in terms of SRCC, KRCC, and PLCC, which demonstrates the effectiveness of our proposed method. 2) Overall, NR-VQA methods outperform NR-IQA methods, primarily due to their ability to extract 3D shape features from projection videos of generated 3D assets. Moreover, to gain further insight into the performance of the proposed method, we also conduct a significance-statistic test. \textcolor{black}{3) The performance of NR-MQA and NR-PCQA methods is worse than that of NR-VQA. The main reason for this is than the generated 3D asset utilizes implicit representations, such as occupancy fields or signed distance functions. Converting these to explicit representations (meshes or point clouds) will introduce distortions and loss of detail. This conversion process may adversely affect the quality assessment, as NR-MQA and NR-PCQA are sensitive to such distortions.} Our experiment setup follows the same procedure outlined in \cite{sheikh2006statistical} and evaluates the significance of the correlation between the predicted quality, authenticity, and correspondence scores and the subjective ratings. All possible pairs of models are tested and the results are displayed in Fig. \ref{fig:compare}. The results reveal that our method is significantly better than the other 10 NR-IQA methods and 9 NR-VQA methods.
% 3)  It can be observed that using the mesh models or point clouds directly to predict quality scores does not yield good prediction results because of the experimental subjects annotate the projection videos of generated 3D content and there has a gap of perceived quality between generated 3D contents and mesh models or point clouds.

\vspace{-13pt}
\subsection{Ablation Study}
To demonstrate the effectiveness of each module in our proposed method, we further conduct ablation experiments, and the results are presented in Table \ref{tab:ablation}. The ``a-c'' denote the utilization of only the text-image alignment feature extraction, texture feature extraction, and shape feature extraction modules respectively, while ``d-f'' represents the absence of shape feature extraction, texture feature extraction, and text-image alignment feature extraction modules respectively. The ``g'' configuration employs all modules. From the results, we draw the following conclusions. 1) All three proposed modules are effective for boosting the performance, while the text-image alignment feature extraction module playing the most significant role. This prominence can be attributed to the strong relationship between text-to-3d assets and their prompts, enabling the prompts to substantially contribute to predicting the perception quality of text-to-3d assets. \textcolor{black}{For the traditional 3D quality assessment, the focus is primarily on the aspects such as geometry and texture quality. While the T23DAQA need to not only assess the geometry and texture quality of generated 3D assets, but also a comprehensive evaluation of the alignment between text and 3D assets, encompassing semantic consistency, style matching. Therefore, the text-image alignment feature is the most important feature.} 2) The 3D shape feature extraction module and texture feature extraction module can effectively extract perceptually relevant features from the projected video and front and back projected images, respectively. Consequently, the two modules can enhance the accuracy of quality, authenticity, and correspondence prediction.
\vspace{-13pt}

% \subsection{Application Validation of the Proposed T23DCQA}
% \subsubsection{Selecting generated 3D content using our T23DCQA method}
% To prove that our method can select generated 3D content with better perceptual quality, we first generate 4 sets of 3D content using magic3d \cite{lin2023magic3d} with the same prompt. Then, we perform the selection by random selection and T23DCQA selection. The 3D content contents selected by random selection and T23DCQA selection are shown in Fig. \ref{fig:appendixa} (b) and (c), respectively. It can be observed that our model can help to choose 3D content with higher quality, authenticity, and text-content correspondence.


% \subsubsection{Helping compare generated contents}
% Current text-to-3D generation works generally conduct user study to compare the performance of two models, which are time consuming. Our method can be used to compare generation contents and can be applied as an alternative method for user study. To validate that our method is alignment with human subjective preferences in visual comparison, we further conduct a user study. Following recent T23DC research, we select 5 prompts and generate 75 3D contents using 5 different T23D generation methods with 3 distinct random seeds. Based on these contents, we randomly select 50 pairs of 3D content ensuring each pair is generated under the same prompt, and then ask 10 volunteers to select the better one based on quality, authenticity, and text-content correspondence, respectively, resulting in 1,500 pairwise comparisons. We then apply our method to the same pairs and calculate its accuracy with the human choices. The results of this study are presented in Table \ref{userstudy}. It can be observed that our method has good alignment with human perception in visual comparison.
% \begin{table}[]
%     \centering
%     \caption{Results of the alignment with human choices.}
%     \scalebox{1.2}{
%     \begin{tabular}{c|c|c|c}
%          \toprule
%          Dimension & Authenticity & Correspondence & Quality \\ \hline
%          Accuracy & 81.4\% & 83.2\% & 87.2\% \\
%          \bottomrule
%     \end{tabular}
%     }
%     \label{userstudy}
%     \vspace{-0.5cm}
% \end{table}
% % For completeness, we follow previous works [20, 4] and conduct a user study by comparing ProlificDreamer with DreamFusion [34], Magic3D [20] and Fantasia3D [4] under 15 prompts, 5 prompts for each baseline. Since none of the baselines have released their codes, we can only use the figures from their papers, which limits the number of results from baselines for us to compare. So we currently only compare under 15 prompts. The volunteers are shown the generated results of our ProlificDreamer and baselines and asked to choose the better one in terms of fidelity, details and vividness. We collect results from 109 volunteers, yielding 1635 pairwise comparisons. The results are shown in Table 3. Our method outperforms all of the baselines.

% \subsubsection{Optimizing generation model by our method}
% In order to validate that our method can help text-to-3D content algorithms generate better 3D content,  we incorporate our T23DCQA model as an additional loss function during the text-to-3D content training process. The results generated without or with our method are shown in Fig. \ref{fig:appendixc} (b) (c), respectively. It can be observed that the 3D content generated using our method has better quality, authenticity, and text-content correspondence.





% \begin{table*}[!t]
% \centering
% \vspace{-3pt}
% \caption{Performance comparision of the state-of-the-art NR-IQA models on the evaluation of human preferences for AI generated omnidirectional images from the perspectives of quality, comfortability and correspondence. The best performances are marked in \textcolor{red}{RED} and the second-best performances are marked in \textcolor{blue}{BLUE}.}
% \vspace{2pt}
% \label{1}
% \setlength{\tabcolsep}{1.3em}
% \scalebox{0.9}{
% \begin{tabular}{l|c c c|c c c|c c c}
% \toprule
% Dimension & \multicolumn{3}{c|}{Authenticity} & \multicolumn{3}{c|}{Correspondence} & \multicolumn{3}{c}{Quality}\\
% \midrule
% Model&SRCC&KRCC&PLCC&SRCC&KRCC&PLCC&SRCC&KRCC&PLCC\\
% \midrule
% NIQE\cite{mittal2012making} &0.1025 & 0.0671 & 0.0936 & -0.1273 & -0.0881 & -0.1362 & -0.0261 & -0.0212 & 0.0047 \\
% ILNIQE\cite{zhang2015feature} &0.1802&0.1197&0.0933&0.2665&0.1823&0.2483&0.1756&0.1175&0.0023\\
% BRISQUE\cite{mittal2012no} &0.1802&0.1197&0.0933&0.2665&0.1823&0.2483&0.1756&0.1175&0.0023\\
% QAC\cite{xue2013learning} & 0.2472 & 0.1671 & 0.2589 & 0.0567 & 0.0373 & 0.0432 & 0.2198 & 0.1496 & 0.2153 \\
% FISBLIM\cite{gu2013fisblim} &0.1802&0.1197&0.0933&0.2665&0.1823&0.2483&0.1756&0.1175&0.0023\\
% BMPRI\cite{min2018blind} & 0.0629 & 0.0436 & 0.0885 & 0.0408 & 0.0282 & 0.0771 & 0.0486 & 0.034 & 0.0767 \\
% BPRI-PSS\cite{min2017blind} &0.1802&0.1197&0.0933&0.2665&0.1823&0.2483&0.1756&0.1175&0.0023\\
% BPRI-LSSs\cite{min2017blind} &0.1802&0.1197&0.0933&0.2665&0.1823&0.2483&0.1756&0.1175&0.0023\\
% BPRI-LSSn\cite{min2017blind} &0.1802&0.1197&0.0933&0.2665&0.1823&0.2483&0.1756&0.1175&0.0023\\
% \midrule
% CLIPScore\cite{hessel2021clipscore} & 0.4812 & 0.3324 & 0.4877 & 0.6053 & 0.4280 & 0.6340 & 0.5765 & 0.4057 & 0.5639 \\
% HPS\cite{wu2023human} & 0.4393 & 0.3002 & 0.4211 & 0.5638 & 0.3922 & 0.5574 & 0.5876 & 0.4170 & 0.5718 \\
% ImageReward\cite{xu2024imagereward} & 0.5119 & 0.3588 & 0.4759 & 0.7104 & 0.5187 & 0.6553 & 0.6585 & 0.4752 & 0.6106 \\
% PickScore\cite{kirstain2024pick} & 0.4782 & 0.3335 & 0.5002 & 0.5396 & 0.3812 & 0.5423 & 0.5796 & 0.4115 & 0.5682 \\
% ViCLIP\cite{wang2023internvid} & 0.4815 & 0.3327 & 0.4938 & 0.6529 & 0.4670 & 0.6682 & 0.6235 & 0.4449 & 0.6168 \\
% \midrule
% MC3-18\cite{kirstain2024pick} & 0.4782 & 0.3335 & 0.5002 & 0.5396 & 0.3812 & 0.5423 & 0.5796 & 0.4115 & 0.5682 \\
% R2P1D-18\cite{kirstain2024pick} & 0.4782 & 0.3335 & 0.5002 & 0.5396 & 0.3812 & 0.5423 & 0.5796 & 0.4115 & 0.5682 \\
% R3D-18\cite{kirstain2024pick} & 0.4782 & 0.3335 & 0.5002 & 0.5396 & 0.3812 & 0.5423 & 0.5796 & 0.4115 & 0.5682 \\
% Swin3D-T\cite{kirstain2024pick} & 0.4782 & 0.3335 & 0.5002 & 0.5396 & 0.3812 & 0.5423 & 0.5796 & 0.4115 & 0.5682 \\
% Swin3D-S\cite{kirstain2024pick} & 0.4782 & 0.3335 & 0.5002 & 0.5396 & 0.3812 & 0.5423 & 0.5796 & 0.4115 & 0.5682 \\
% Swin3D-B\cite{kirstain2024pick} & 0.4782 & 0.3335 & 0.5002 & 0.5396 & 0.3812 & 0.5423 & 0.5796 & 0.4115 & 0.5682 \\

% SimpleVQA\cite{kirstain2024pick} & 0.4782 & 0.3335 & 0.5002 & 0.5396 & 0.3812 & 0.5423 & 0.5796 & 0.4115 & 0.5682 \\
% Fast-VQA\cite{kirstain2024pick} & 0.4782 & 0.3335 & 0.5002 & 0.5396 & 0.3812 & 0.5423 & 0.5796 & 0.4115 & 0.5682 \\
% DOVER\cite{kirstain2024pick} & 0.4782 & 0.3335 & 0.5002 & 0.5396 & 0.3812 & 0.5423 & 0.5796 & 0.4115 & 0.5682 \\

% % \midrule
% \bottomrule
% \end{tabular}
% }
% \vspace{-6pt}
% \end{table*}


% \begin{table*}[tbph]

% \caption{Performance results on T23DCQA-DB using training-based metrics. [Key: {\bf\textcolor{red}{Best}, \bf\textcolor{blue}{Second Best}}]}
% \label{tab:training}
% \centering
% \scalebox{1.0}{
% \begin{tabular}{ll|rrr|rrr|rrr}
% \toprule
% \multicolumn{2}{c|}{\textbf{Dimension}}  & \multicolumn{3}{c|}{Authenticity}  & \multicolumn{3}{c|}{Correspondence} & \multicolumn{3}{c}{Quality}                                                 \\ \hline
% \multicolumn{1}{c|}{\textbf{Type}}              & \multicolumn{1}{c|}{\textbf{Metric}} & \multicolumn{1}{c}{SRCC} & \multicolumn{1}{c}{KRCC} & \multicolumn{1}{c|}{PLCC} & \multicolumn{1}{c}{SRCC} & \multicolumn{1}{c}{KRCC} & \multicolumn{1}{c|}{PLCC} & \multicolumn{1}{c}{SRCC} & \multicolumn{1}{c}{KRCC} & \multicolumn{1}{c}{PLCC} \\ \hline
% \multicolumn{1}{l|}{\multirow{7}{*}{\begin{tabular}[c]{@{}l@{}} NR-IQA \end{tabular}}}  & Resnet-18\cite{he2016deep} & 0.5114 & 0.3618 & 0.5267 & 0.5652 & 0.4027 & 0.6240 & 0.6970 & 0.5166 & 0.7004 \\
% \multicolumn{1}{l|}{} & Resnet-34\cite{he2016deep} & 0.5688 & 0.4047 & 0.5846 & 0.5794 & 0.4181 & 0.6325 & 0.7122 & 0.5288 & 0.7104 \\
% \multicolumn{1}{l|}{} & Resnet-50\cite{he2016deep} & 0.4657 & 0.3354 & 0.4961 & 0.4750 & 0.3364 & 0.5629 & 0.6441 & 0.4772 & 0.6624 \\
% \multicolumn{1}{l|}{} & Swin-T\cite{liu2021swin} & 0.5934 & 0.4273 & 0.6241 & 0.6360 & 0.4669 & 0.6951 & 0.7515 & 0.5734 & 0.7678 \\
% \multicolumn{1}{l|}{} & Swin-S\cite{liu2021swin} &  0.6263 & 0.4541 & 0.6478 & 0.6434 & 0.4817 & 0.6983 & 0.7652 & 0.5869 & 0.7820 \\
% \multicolumn{1}{l|}{} & Swin-B\cite{liu2021swin} & 0.6197 & 0.4483 & 0.6415 & 0.6431 & 0.4776 & 0.6995 & 0.7617 & 0.5844 & 0.7774 \\
% \multicolumn{1}{l|}{} & Swin-L\cite{liu2021swin} & 0.6069 & 0.4396 & 0.6323 & 0.6539 & 0.4850 & 0.7132 & 0.7592 & 0.5810 & 0.7714 \\ 
% \multicolumn{1}{l|}{} & CNNIQA\cite{kang2014convolutional} & 0.4281 & 0.2969 & 0.4332 & 0.5562 & 0.3932 & 0.6104 & 0.6776 & 0.4954 & 0.6658 \\
% \multicolumn{1}{l|}{} & StairIQA\cite{sun2023blind} & 0.5002 & 0.3579 & 0.5375 & 0.4635 & 0.3360 & 0.5773 & 0.6373 & 0.4804 & 0.6715 \\ 
% \multicolumn{1}{l|}{} & HyperIQA\cite{Su_2020_CVPR} & 0.6069 & 0.4396 & 0.6323 & 0.6539 & 0.4850 & 0.7132 & 0.7592 & 0.5810 & 0.7714 \\ 
% \hline
% \multicolumn{1}{l|}{\multirow{6}{*}{\begin{tabular}[c]{@{}l@{}}NR-VQA\end{tabular}}} & MC3-18\cite{tran2018closer} & 0.5702 & 0.4090 & 0.5948 & 0.6203 & 0.4554 & 0.6623 & 0.7421 & 0.5631 & 0.7528 \\
% \multicolumn{1}{l|}{} & R2P1D-18\cite{tran2018closer} & 0.5864 & 0.4168 & 0.5903 & 0.6134 & 0.4520 & 0.6726 & 0.7423 & 0.5613 & 0.7474 \\
% \multicolumn{1}{l|}{} & R3D-18\cite{tran2018closer} & 0.5869 & 0.4141 & 0.5951 & 0.5962 & 0.4327 & 0.6626 & 0.7430 & 0.5608 & 0.7466 \\
% \multicolumn{1}{l|}{} & Swin3D-T\cite{liu2022video} & 0.6190 & 0.4521 & 0.6433 & 0.6517 & 0.4885 & 0.7034 & 0.7556 & 0.5842 & 0.7752 \\
% \multicolumn{1}{l|}{} & Swin3D-S\cite{liu2022video} & 0.6317 & 0.4641 & 0.6517 & 0.6394 & 0.4795 & 0.7030 & 0.7579 & 0.5846 & 0.7768 \\
% \multicolumn{1}{l|}{} & Swin3D-B\cite{liu2022video} & 0.6181 & 0.4502 & 0.6447 & 0.6294 & 0.4707 & 0.6973 & 0.7544 & 0.5809 & 0.7757 \\
% % \multicolumn{1}{l|}{} & SimpleVQA\cite{kirstain2024pick} & 0.4782 & 0.3335 & 0.5002 & 0.5396 & 0.3812 & 0.5423 & 0.5796 & 0.4115 & 0.5682 \\
% % \multicolumn{1}{l|}{} & Fast-VQA\cite{kirstain2024pick} & 0.4782 & 0.3335 & 0.5002 & 0.5396 & 0.3812 & 0.5423 & 0.5796 & 0.4115 & 0.5682 \\
% % \multicolumn{1}{l|}{} & DOVER\cite{kirstain2024pick} & 0.4782 & 0.3335 & 0.5002 & 0.5396 & 0.3812 & 0.5423 & 0.5796 & 0.4115 & 0.5682 \\
% \multicolumn{1}{l|}{} & Proposed & 0.6728 & 0.4909 & 0.6840 & 0.7000 & 0.5157 & 0.7297 & 0.7853 & 0.5987 & 0.7828 \\
% \bottomrule
% \end{tabular}
% }
% \end{table*}
