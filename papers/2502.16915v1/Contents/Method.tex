\section{Proposed Method}

\begin{figure*}[t]
    \centering
    \includegraphics[width = 0.94\textwidth]{Figs/framework.pdf}
    \caption{Illustration of our proposed T23DAQA method, which is divided into two stages. In the first stage, circular projection views are captured from the text-to-3D generated assets and then concatenated to form a video sequence. In the second stage, three distinct modules are employed to extract shape features, texture features, and text-image alignment features, respectively. Then these features are fused together to regress into quality, authenticity, and text-asset correspondence scores for comprehensive evaluation.}
    \label{fig:framework}
    \vspace{-0.5cm}
\end{figure*}

In this section, we introduce the architecture of the proposed text-to-3D asset quality assessment (T23DAQA) model in detail, as shown in Fig. \ref{fig:framework}. It is divided into two stages. In the first stage, we capture circular projections for the text-to-3D assets, and concatenate the projection views into videos. In the second stage, we first use the shape feature extraction module, texture feature extraction module, and text-image correspondence feature extraction module to extract the features related to human preferences respectively, and then fuse these features to regress into quality, authenticity, and text-asset correspondence scores for evaluation.
\vspace{-15pt}
\subsection{Projection Process}
                            Our T23DAQA model first represent the 3D asset into videos for the subsequent evaluation.
                            The reasons of choosing projection videos as the format to predict human preferences for 3D assets are given as follows. 1) text-to-3D asset generation methods usually adopt neural radiation field to represent the 3D asset, which is indirectly stored in MLP or voxel. This has resulted in a lack of a unified format for the generated 3D asset, making it difficult to be evaluated by 3D quality assessment methods. 2) The projection-based 3D quality assessment methods can be adapted to all kinds of 3D models, not only for the generated 3D asset but also for point cloud, mesh, voxel, \textit{etc}, since they infer the visual quality via the rendered projections. As shown in Fig. \ref{fig:framework}, we move the camera around the the generated 3D asset, then obtain a projection sequence and select $\mathbf{K}$ frames from it for subsequent processing, Given a text-to-3D asset $\mathbf{O}$, the projection process can be described as:

\begin{equation}
\begin{aligned}
     &\mathbf{P}  = R(\mathbf {O}), \\
    \mathbf{P}  = \{&\mathcal{P}_{k}|k =1,\cdots, \mathbf{K}\},
\end{aligned}
\end{equation}
\textcolor{black}{where $\mathbf{P}$ represents the set of select projection frames and $R(\cdot)$ stands for the rendering process, which determines the color of each pixel by calculating the density and color integral of the intersection of the ray passing through each pixel with the asset.}
\vspace{-13pt}
\subsection{Shape Feature Encoder}
The shape feature encoder is aimed to extract the 3D shape features of text-to-3d asset from the projection videos. Due to the use of implicit neural radiation fields to represent 3D asset, the shape of the T23DA is usually relatively smooth, and some may have floaters, which greatly affects the quality and authenticity of the T23DA. Therefore, we use a Swin3D-s\cite{liu2021swin} as the 3D shape encoder to extract the 3D shape feature from the projection video. This process can be represented as:
\begin{equation}
     f_{s}  =  E_{s}(\mathbf{P}),
\end{equation}
where $E_{s}$ and $f_{s}$ represents the projection video encoder and the obtained 3D shape features respectively.
\vspace{-13pt}
\subsection{Texture Feature Encoder}

The texture feature encoder is aimed to extract the texture feature of the text-to-3d asset from the image dimension, which represents the material and physical properties of the text-to-3d asset. If the texture feature is incompatible with the shape of 3D asset, the quality and authenticity of the T23DA are low. In order to extract the overall texture feature, we utilize Swin Transformer-small (Swin-s)\cite{liu2021swin} as the front projection image encoder and the back projection image encoder to extract the texture features. This process can be formulated as:
\begin{equation}
     f_{t}  = F_{t}( E_{t}^{f}(\mathcal{P}_{1}) , E_{t}^{b}(\mathcal{P}_{1+\frac{N}{2}}) ),
\end{equation}
% where $E_{t}^{f}$ and $E_{t}^{b}$ represent the front projection image encoder, the back projection image encoder. $\mathcal{P}_{1}$ and $\mathcal{P}_{1+\frac{N}{2}}$ indicate the front and back projection image. $F_{t}$ is the texture feature fusion module. $f_{t}$ means the obtained texture features.
\textcolor{black}{where $E_{t}^{f}$ and $E_{t}^{b}$ denote the encoders for the front and back projection images, respectively. $\mathcal{P}_{1}$ and $\mathcal{P}_{1+\frac{N}{2}}$ represent the front and back projection images. $F_{t}$ corresponds to the texture feature fusion module, while $f_{t}$ signifies the extracted texture features.}
\vspace{-15pt}
\subsection{Text-image Alignment Encoder}
The text-image alignment encoder is used to extract the text-image alignment feature. Following previous works, we use the pre-trained CLIP \cite{radford2021learning} image encoder $E_{ci}$ as the projection video frame encoder and the text encoder $E_{ct}$ as the prompt encoder. The two features extracted by these two encoders are fused to the alignment feature by alignment fusion module $F_{c}$. This process can be expressed as:

\begin{equation}
\begin{aligned}
     f_{c}^{i}  = E_{c}^{i} (\mathcal{P}_{1}) &, f_{c}^{t}  = E_{c}^{t}(W), \\
     f_{c}  = F_{c}& ( f_{c}^{i} , f_{c}^{t} ),
\end{aligned}
\end{equation}
% where $\mathcal{P}_{1}$ and $W$ represents the front projection image and prompt respectively. $f_{c}^{i}$, $f_{c}^{t}$, and $f_{c}$ stands for the obtained image feature, prompt feature, and text-image alignment feature respectively. In the training stage, we freeze the weights of the projection video frame encoder $E_{c}^{i}$ and the prompt encoder $E_{c}^{t}$, only train alignment fusion module $F_{c}$.
\textcolor{black}{where $\mathcal{P}_{1}$ denotes the front projection image, and $W$ represents the prompt. The features $f_{c}^{i}$, $f_{c}^{t}$, and $f_{c}$ correspond to the image feature, prompt feature, and text-image alignment feature, respectively. During the training phase, the weights of the projection video frame encoder $E_{c}^{i}$ and the prompt  encoder $E_{c}^{t}$ are frozen, while only the alignment fusion module $F_{c}$ is trained.}



\begin{table*}[t]

\caption{Performance results of traditional handcrafted perceptual quality metrics and alignment metrics on our AIGC-T23DAQA database. [Key: {\bf\textcolor{red}{Best}, \bf\textcolor{blue}{Second Best}}]}
\label{tab:zero-shot}
\centering
\scalebox{1.0}{
\begin{tabular}{ll|rrr|rrr|rrr}
\toprule
\multicolumn{2}{c|}{\textbf{Dimension}}  & \multicolumn{3}{c|}{Authenticity}  & \multicolumn{3}{c|}{Correspondence} & \multicolumn{3}{c}{Quality}                                                 \\ \hline
\multicolumn{1}{c|}{\textbf{Type}}              & \multicolumn{1}{c|}{\textbf{Metric}} & \multicolumn{1}{c}{SRCC} & \multicolumn{1}{c}{KRCC} & \multicolumn{1}{c|}{PLCC} & \multicolumn{1}{c}{SRCC} & \multicolumn{1}{c}{KRCC} & \multicolumn{1}{c|}{PLCC} & \multicolumn{1}{c}{SRCC} & \multicolumn{1}{c}{KRCC} & \multicolumn{1}{c}{PLCC} \\ \hline
\multicolumn{1}{l|}{\multirow{10}{*}{\begin{tabular}[c]{@{}l@{}} NR-IQA \end{tabular}}}  & NIQE\cite{mittal2012making} & 0.1534 & 0.1270 & 0.1708 & 0.1272 & 0.0881 & 0.1755 & 0.0256 & 0.0209 & 0.1408 \\
\multicolumn{1}{l|}{} & ILNIQE\cite{zhang2015feature} & 0.0670 & 0.0556 & 0.0545 & 0.2764 & 0.1939 & 0.3092 & 0.2385 & 0.1727 & 0.2838 \\
\multicolumn{1}{l|}{} & BRISQUE\cite{mittal2012no} & 0.1224 & 0.0831 & 0.1461 & 0.1244 & 0.0852 & 0.1422 & 0.0884 & 0.0605 & 0.1100 \\
\multicolumn{1}{l|}{} & QAC\cite{xue2013learning} & 0.2472 & 0.1671 & 0.2662 & 0.1114 & 0.0921 & 0.0938 & 0.2198 & 0.1496 & 0.2415 \\
\multicolumn{1}{l|}{} & FISBLIM\cite{gu2013fisblim} & 0.1507 & 0.1049 & 0.1902 & 0.1297 & 0.0931 & 0.2015 & 0.3816 & 0.2764 & 0.4049 \\
\multicolumn{1}{l|}{} & BMPRI\cite{min2018blind} & 0.0680 & 0.0562 & 0.0740 & 0.0408 & 0.0282 & 0.1247 & 0.0486 & 0.0340 & 0.1063 \\
\multicolumn{1}{l|}{} & BPRI\cite{min2017blind} & 0.1322 & 0.0907 & 0.1752 & 0.0064 & 0.0041 & 0.1553 & 0.1354 & 0.0928 & 0.1655 \\
\multicolumn{1}{l|}{} & BPRI-PSS\cite{min2017blind} & 0.1296 & 0.0875 & 0.2508 & 0.1182 & 0.0792 & 0.3154 & 0.1866 & 0.1241 & 0.3436 \\
\multicolumn{1}{l|}{} & BPRI-LSSs\cite{min2017blind} & 0.0333 & 0.0220 & 0.0702 & 0.1089 & 0.0750 & 0.1465 & 0.0425 & 0.0277 & 0.0968 \\
\multicolumn{1}{l|}{} & BPRI-LSSn\cite{min2017blind} & 0.1345 & 0.1114 & 0.1332 & 0.2624 & 0.1855 & 0.3490 & 0.3136 & 0.2209 & 0.3551 \\ \hline
\multicolumn{1}{l|}{\multirow{5}{*}{\begin{tabular}[c]{@{}l@{}}Alignment\end{tabular}}} & CLIPScore\cite{hessel2021clipscore} & 0.4812 & 0.3324 & 0.5107 & 0.6053 & 0.4280 & 0.6584 & 0.5765 & 0.4057 & 0.5806 \\
\multicolumn{1}{l|}{} & HPS\cite{wu2023human} & 0.4393 & 0.3002 & 0.4589 & 0.5638 & 0.3922 & 0.5977 & 0.5876 & 0.4170 & 0.5856 \\
\multicolumn{1}{l|}{} & ImageReward\cite{xu2024imagereward} & \textcolor{blue}{0.5119} & \textcolor{blue}{0.3588}& \textcolor{blue}{0.5161} & \textcolor{red}{0.6604} & \textcolor{red}{0.4887} & \textcolor{red}{0.7027} & \textcolor{blue}{0.6585} & \textcolor{blue}{0.4752} & \textcolor{blue}{0.6469} \\
\multicolumn{1}{l|}{} & PickScore\cite{kirstain2024pick} & 0.4782 & 0.3335 & 0.5054 & 0.5396 & 0.3812 & 0.5792 & 0.5796 & 0.4115 & 0.5902 \\
\multicolumn{1}{l|}{} & ViCLIP\cite{wang2023internvid} & 0.4815 & 0.3327 & 0.5122 & \textcolor{blue}{0.6529} & \textcolor{blue}{0.4670}& \textcolor{blue}{0.6919} & 0.6235 & 0.4449 & 0.6304 \\ \hline
\multicolumn{1}{l|}{\multirow{3}{*}{\begin{tabular}[c]{@{}l@{}}\textcolor{black}{LMMQA}\end{tabular}}} & \textcolor{black}{Q-align} \cite{wu2023q} & 0.2339 & 0.1605 & 0.2941 & 0.1441 & 0.0997 & 0.2002 & 0.3906 & 0.2724 & 0.4302 \\
\multicolumn{1}{l|}{} & \textcolor{black}{T2I-Scorer} \cite{wu2024t2i} & \textcolor{red}{0.5449} & \textcolor{red}{0.3834} & \textcolor{red}{0.5567} & 0.4908 & 0.3411 & 0.5022 & \textcolor{red}{0.6771} & \textcolor{red}{0.4957} & \textcolor{red}{0.6835} \\
\multicolumn{1}{l|}{} & \textcolor{black}{VQAScore} \cite{lin2025evaluating} & 0.3701 & 0.2548 & 0.3849 & 0.5451 & 0.3805 & 0.5381 & 0.4373 & 0.3015 & 0.4390 \\

\bottomrule
\end{tabular}
}
\vspace{-0.5cm}
\end{table*}
\vspace{-13pt}
\subsection{Feature Fusion and Quality Regression}
The previous three modules extract the 3d shape, texture, and text-image alignment feature of the text-to-3D asset respectively. Finally, we concatenate these features to obtain the perception quality features $f$ for the text-to-3D asset:
\begin{equation}
     f  = \text{concatenate}(f_{c},f_{t},f_{f}),
\end{equation}

After extracting perception quality features through the designated feature extraction modules, we then map these features to preference scores using a regression module. In this model, we utilize a MLP as the regression module, due to its simplicity and effectiveness in terms of model complexity. The MLP architecture consists of three fully connected layers, with 1024 neurons in the first layer, 128 neurons in the second layer, and 3 neurons in the output layer. Consequently, through this process, we are able to derive quality, authenticity, and text-asset correspondence scores as follows:

\begin{equation}
     [\hat{Q}_{q},\hat{Q}_{a},\hat{Q}_{c}]  = F_{f}(f),
\end{equation}
where $F_{f}$ denotes the function of the three FC layers. $\hat{Q}_{q}$, $\hat{Q}_{a}$and $\hat{Q}_{c}$ are the predicted quality, authenticity, and text-image correspondence scores respectively.
\vspace{-13pt}
\subsection{Loss function}

% In accordance with \cite{wu2022fastquality, li2020norm}, we employ differentiable Pearson Linear Correlation Coefficient (PLCC) and rank loss functions. PLCC serves as a prevalent criterion for assessing the correlation between sequences, while rank loss aids in enhancing the model's ability to discern the relative quality of videos. The composite loss function is formulated as follows:
% \begin{equation}
% L = L_{plcc} + \lambda \cdot L_{rank}, 
% \end{equation}
% Here, $\lambda$ denotes a hyper-parameter for balancing, which is set to 0.3 during the training phase.

In accordance with \cite{wu2022fastquality, li2020norm}, we employ linearity loss and monotonicity loss functions. The linearity loss function is used to force the predicted quality scores close to the quality labels, which can be regarded as Mean Squared Error loss with z-score normalization. We need to normalize the predicted scores vectors $\hat{Q}$ and ground truth label vectors $Q$ to obtain $\hat{S}$ and $S$ respectively.
The linearity loss can be described as:
\begin{equation}
\begin{aligned}
    L_{\text{lin}} =& \frac{((\hat{S} - S)^{2} + (\sum (\hat{S} * S)*\hat{S} - S)^{2})}{2}, \\
\end{aligned}
\end{equation}

while rank loss aids in enhancing the model's ability to discern the relative quality of projection videos, which can be formalized as:
\begin{equation}
L_{\text{rank}} = \sum \max((\hat{Q} - Q)sgn(\hat{Q} - Q), 0),
\end{equation}
where $sgn(\cdot)$ denotes the sign function. The composite loss function is formulated as follows:
\begin{equation}
L = \sum_{i} L_{\text{lin}} + \lambda \cdot L_{\text{rank}} \quad i \in \{ q, a, c\}, 
\end{equation}
Here, $\lambda$ denotes a hyper-parameter for balancing, which is set to 0.3 during the training phase. $q, a, c$ represent quality, authenticity, and text-asset correspondence respectively. 


