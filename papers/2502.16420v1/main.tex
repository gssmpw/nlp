\documentclass[11pt, a4paper, logo, onecolumn]{preprint}

\usepackage[all]{hypcap}
\usepackage{xspace}
\usepackage[capitalize,noabbrev]{cleveref}
\bibliographystyle{plainnat}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{lipsum}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{algorithm}
\usepackage{algpseudocode}




%%IMPORTS from SR

\usepackage{url}
\usepackage{float}
\usepackage{tikz}
\usepackage{tabularx}
\usepackage{array}
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\usepackage{multirow} 
\usepackage{makecell}
\usepackage{subcaption}
\usepackage{footnote}
\usepackage{hyperref}
\newcommand{\pcc}{\,cm$^{-3}$}
\usepackage{amsmath}
\newcommand{\todo}[1]{\textcolor{red}{TODO #1}}


\makeatletter
\def\mathcolor#1#{\@mathcolor{#1}}
\def\@mathcolor#1#2#3{%
  \protect\leavevmode
  \begingroup
    \color#1{#2}#3%
  \endgroup
}
\makeatother

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{AnyDexGrasp: General Dexterous Grasping for Different Hands with Human-level Learning Efficiency}
\runningtitle{AnyDexGrasp: General Dexterous Grasping for Different Hands with Human-level Learning Efficiency}
% \titlenote{Produces the permission block, and copyright information}

% Remove these if they are not needed 
% \keywords{Maximum Entropy, Reinforcement Learning, Robotics}
% \paperurl{https://arxiv.org/pdf/1801.01290.pdf}

% Use the internally issued paper ID, if there is one
\reportnumber{} % Leave blank if n/a 

% Assign your own date to the report. 
% Can comment out if not needed or leave blank if n/a.
\renewcommand{\today}{Nov 2024} 

\author[1]{Hao-Shu Fang}
\author[1]{Hengxu Yan}
\author[1]{Zhenyu Tang}
\author[1]{Hongjie Fang}
\author[1]{Chenxi Wang}
\author[2]{Cewu Lu}

% Affiliations *must* come after the declaration of \author[]

\affil[1]{Department of Computer Science, Shanghai Jiao Tong University}
\affil[2]{School of Artificial Intelligence, Shanghai Jiao Tong University}


\email{ \href{mailto:fhaoshu@gmail.com}{fhaoshu@gmail.com}, \{hengxuyan, tang\_zhenyu, galaxies, wcx1997, lucewu\}@sjtu.edu.cn}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\hyphenation{pre-print}

\begin{abstract}
We introduce an efficient approach for learning dexterous grasping with minimal data, advancing robotic manipulation capabilities across different robotic hands. Unlike traditional methods that require millions of grasp labels for each robotic hand, our method achieves high performance with human-level learning efficiency: only hundreds of grasp attempts on 40 training objects. The approach separates the grasping process into two stages: first, a universal model maps scene geometry to intermediate contact-centric grasp representations, independent of specific robotic hands. Next, a unique grasp decision model is trained for each robotic hand through real-world trial and error, translating these representations into final grasp poses. Our results show a grasp success rate of 75-95\% across three different robotic hands in real-world cluttered environments with over 150 novel objects, improving to 80-98\% with increased training objects. This adaptable method demonstrates promising applications for humanoid robots, prosthetics, and other domains requiring robust, versatile robotic manipulation. Project website: \href{https://graspnet.net/anydexgrasp/}{https://graspnet.net/anydexgrasp/}.
% The evolution of various mechanical designs in robotic hands continually drives advancements in dexterous grasping and manipulation. Learning visually guided grasping in cluttered environments across diverse robotic hands, without relying on extensive data collection and annotation, is a compelling yet challenging goal. Current learning systems often necessitate a large-scale dataset with annotated grasp poses or an extensive number of grasp trials, for each specific multi-fingered hand, which hinders their practical implementation. This paper proposes an efficient methodology for directly learning dexterous grasping in real-world settings, achieved through merely hundreds of grasp attempts on a limited set of 40 training objects. Our approach divides the raw observation-to-grasp decision mapping into two distinct sub-problems. The first step is to map the geometry of a scene to a set of intermediate contact-centric grasp representations, facilitated by a universally trained model that is agnostic to robotic hands. Subsequently, the second step trains a specific grasp decision model for each robotic hand through real-world trial and error, mapping the contact-centric grasp representation to final grasp poses. Notably, for the first time, we demonstrate a remarkable 75\% to 95\% real-world grasp success rate for three different robotic hands in cluttered environments on over 150 unseen testing objects, training on only 40 objects within hundreds of grasp attempts. These success rates further improve to 80\% to 98\% when we increase the number of training objects and real-world grasp attempts. Additionally, our method shows robust performance with challenging objects, such as deformable textiles and adversarial items. This adaptability and robustness highlight the potential applicability of our pipeline across various domains, including humanoid robots and prosthetic hands, laying a solid foundation for diverse real-world applications. 
\end{abstract}

\begin{document}

\maketitle

\section*{Summary}
A general framework for learning visually guided dexterous grasping across various robotic hands, achieving human-level learning efficiency and robust performance in cluttered environments.

\input{sections/intro}
\input{sections/results}
\input{sections/discussion}
\input{sections/methods}



\input{sections/ack}

\newpage
\balance

\bibliography{main}



\newpage


\appendix

\input{sections/supp}

\end{document}
