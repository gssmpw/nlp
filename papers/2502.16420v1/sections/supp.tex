\section*{\Large \textbf{Supplementary Methods}}

\subsection*{Query 6D Grasp Pose for CGR }
When we map a CGR to grasp pose, we first calculate the antipodal grasp representation of the CGR. Given a CGR
\begin{align}
r = \bigg\{ & (d_{\alpha_i}, \theta_{\alpha_i})_j \, \bigg| \, \alpha_i = 0, \frac{2\pi}{N}, \ldots, 2\pi-\frac{2\pi}{N}, \, j = 1, 2, \ldots, M; \mathbf{R}_{3d},\mathbf{t}_{3d} \bigg\},
\end{align}
the antipodal grasp representation is calculated by
\begin{align}
\nonumber s = \bigg\{ & (w_{\alpha_i}, \mu_{\alpha_i})_j  \bigg|  \alpha_i = 0, \frac{2\pi}{N}, \ldots, \pi-\frac{2\pi}{N}, \, j = 1, 2, \ldots, M;  \mathbf{R_{3d}}, \mathbf{t_{3d}} \bigg\},
\label{grasprep_sup}
\end{align}  
where  $w_{\alpha_i} = 2\times\max(d_{\alpha_i}, d_{\alpha_i+\pi})$ and  $\mu_{\alpha_i} = \max(\tan(\theta_{\alpha_i}), \tan(\theta_{\alpha_i+\pi}))$. After we obtained $s$, we choose the $\alpha_i$ and $j$ that has the maximum antipodal grasp score:
\[
(\alpha_i^*, j^*) = \underset{\alpha_i, j}{\operatorname{arg\,max}} \, (\mu_{\alpha_i})_j.
\]
We add the rotation $\alpha_i^*$ and translation corresponds to the $j^*$-th section along approach direction to $\mathbf{R_{3d}}$ and $\mathbf{t_{3d}}$:
\begin{align}
\nonumber \mathbf{R_{g}} &= \mathbf{R_{3d}} \cdot \mathbf{R}_z(\alpha_i^*),\\
\nonumber \mathbf{t_{g}} &= \mathbf{t_{3d}} + \mathbf{d}(j^*) \cdot \mathbf{R_{3d}} \cdot \mathbf{R}_z(\alpha_i^*) \cdot \mathbf{z},
\end{align}
where:
\[
\mathbf{R}_z(\alpha_i^*) = \begin{bmatrix} \cos(\alpha_i^*) & -\sin(\alpha_i^*) & 0 \\ \sin(\alpha_i^*) & \cos(\alpha_i^*) & 0 \\ 0 & 0 & 1 \end{bmatrix}, 
\]
$\mathbf{d}(\cdot)$ is a function that maps the index $j$ of the section to its actual depth along the approach direction (maps $\{1,2,3,4,5\}$ to $\{0.005\text{m}, 0.01\text{m}, 0.02\text{m}, 0.03\text{m}, 0.04\text{m}\}$), and:
\[
\mathbf{z} = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}.
\]
The updated rotation $\mathbf{R_{g}}$ and translation $\mathbf{t_{g}}$ are the 6D grasp pose that corresponds to this CGR. 

\subsection*{Details of Grasp Decision Model}
Each grasp decision sub-model is learned by a neural network. It takes a contact-centric grasp representation as input and outputs a score ranging from 0 to 1 to indicate whether the corresponding grasp candidate would be successful. The input size is $2\times5\times48=480$, which is composed of distances and normal angles on 5 sections along 48 in-plane rotations.

The network comprises seven fully connected layers with a skip connection for improving robustness. Each intermediate layer consists of a fully connected layer with 1024 neurons, a batch normalization layer, and a ReLU activation function. The output of the second intermediate layer is also forwarded to the fifth intermediate layer with a skip connection. Networks for different grasp types are trained separately. We employ a loss function defined as: 
\begin{equation}\label{loss}
L = -\frac{1}{\mathbf{Z}}\sum_{z=1}^\mathbf{Z}y_{z}\log(p_{z}).
\end{equation}
In this equation, $L$ is the loss, $y$ denotes the binary label of whether the real robot trial and error succeeded or not, and $p$ represents the predicted grasp success probability by the network. $\mathbf{Z}$ denotes the batch size.

\subsection*{Local Geometry Sampling for Grasp Coverage Analysis}
The local geometries are cropped using 3D boxes defined by valid antipodal grasp poses. To obtain the grasp candidates, each object is voxel-downsampled to get grasp points in uniform distributions. $V$ approach directions are sampled on the grasp point. $A$ inplane rotation angles are sampled uniformly for each direction. On the training objects, we set $V$=100 and $A$=12 for the dense set, and $V$=50 and $A$=6 for the sparse set, respectively. In these two cases, the average numbers of local geometries for each training object are around 1M and 4M. For testing objects in the EGAD dataset, we set $V$=100 and $A$=12.

\section*{\Large \textbf{Supplementary Text}}

\subsection*{Training Object Collection}
The 144 training objects are collected from supermarkets and grocery stores, which is extended from the 40 training objects collected in GraspNet-1Billion~\cite{fang2023robust}. The principle of choosing objects is that they have a roughly different shape or some local geometries from other objects, and they are chosen by authors heuristically. We provide the 3D scanned models of the objects to support reproducible research. Figure~\ref{fig:train_obj} shows an overview of the training object.

\subsection*{Grasp Types for Different Hands} The index number for each grasp type of different robotic hands is given in Figure~\ref{fig:index}.

\subsection*{Principal Closing Axis for Different Grasp Types} We illustrate the principal closing axis for different grasp types in Figure~\ref{fig:principal}. The $x$-axis (in red) in the local coordinate frame is the approach direction and the $y$-axis (in green) is the principal closing axis of the hand. The two-finger gripper (in blue) is the corresponding antipodal grasp pose for each grasp type.

\section*{\Large \textbf{Supplementary Movies}}
We believe that presenting the complete process of our robotic grasping experiments can provide valuable insights into potential improvements for the grasping system. Additionally, it is essential to demonstrate the systemâ€™s robustness, which requires running it for an extended period. Therefore, we recorded the entire grasping process, retaining all original content without cuts, but with speed adjustments to keep the video at a reasonable length. The grasping process for each robotic hand lasts over 3 hours, with the total time across all three hands exceeding 15 hours. We applied a 20x speed-up for the collision detection phase and a 2x speed-up for the grasp execution phase. Even after these adjustments, the resulting videos still exceed 6 hours in length. Consequently, we have hosted the videos on YouTube, with the links provided below:
\begin{itemize}
    \item \hypertarget{movie_s1}{Movie S1} - Grasping with 3-finger DH-3 hand on daily objects, after training on 144 objects: \\\url{https://youtu.be/GGBesshyfxk}
    \item \hypertarget{movie_s2}{Movie S2} - Grasping with 4-finger Allegro hand on daily objects, after training on 144 objects: \\\url{https://youtu.be/HkrvWm_TTGo}
    \item \hypertarget{movie_s2}{Movie S3} - Grasping with 5-finger Inspire hand on daily objects, after training on 144 objects: \\\url{https://youtu.be/3Om7G8nMJPg}
    \item \hypertarget{movie_s4}{Movie S4} - Grasping with 3-finger DH-3 hand on adversarial objects, after training on 144 objects: \\\url{https://youtu.be/GGBesshyfxk?t=1837}
    \item \hypertarget{movie_s5}{Movie S5} - Grasping with 4-finger Allegro hand on adversarial objects, after training on 144 objects: \\\url{https://youtu.be/E7i3pqxA4RM}
    \item \hypertarget{movie_s6}{Movie S6} - Grasping with 5-finger Inspire hand on adversarial objects, after training on 144 objects: \\\url{https://youtu.be/o6LQwRgu82s}
    \item \hypertarget{movie_s7}{Movie S7} - Grasping with 3-finger DH-3 hand on daily and adversarial objects, after training on 40 objects:  \url{https://youtu.be/--5wIHfPoZs}
    \item \hypertarget{movie_s8}{Movie S8} - Grasping with 4-finger Allegro hand on daily objects, after training on 40 objects: \\\url{https://youtu.be/uhaC8NORqm4}
    \item \hypertarget{movie_s9}{Movie S9} - Grasping with 4-finger Allegro hand on adversarial objects, after training on 40 objects: \\\url{https://youtu.be/5pN6BYOH4xw}
    \item \hypertarget{movie_s10}{Movie S10} - Grasping with 5-finger Inspire hand on daily objects, after training on 40 objects: \\\url{https://youtu.be/GQDLTVjXPQk} 
    \item \hypertarget{movie_s11}{Movie S11} - Grasping with 5-finger Inspire hand on adversarial objects, after training on 40 objects: \\\url{https://youtu.be/B7qc7qRw4ss} 
\end{itemize}

\clearpage

\renewcommand{\thefigure}{S\arabic{figure}}

\setcounter{figure}{0}

\begin{figure*}
    \centering
    \includegraphics[width=1\textwidth]{images/train_obj.pdf}
    \caption{\textbf{Training object set.} The set $\mathbb{L}$ with 144 training objects are enclosed by the orange rectangle, the set $\mathbb{S}$ with 40 training objects are enclosed by the blue rectangle, and the set $\mathbb{T}$ with 30 training objects are enclosed by the green rectangle. Their CAD models are available upon request.
    }
    \label{fig:train_obj}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.8\textwidth]{images/hand_new_fig_supp_1.pdf}
    \caption{\textbf{Grasp type numbering.} \textbf{(A)}, \textbf{(B)} and \textbf{(C)} give the index numbers of different grasp types for the three-finger, four-finger, and five-finger hands, respectively.
    }
    \label{fig:index}
\end{figure*}


\begin{figure*}
    \centering
    \includegraphics[width=0.8\textwidth]{images/hand_new_fig_supp_2.pdf}
    \caption{\textbf{Identifying principal closing axis.} We show the designated principal closing axis and corresponding antipodal grasp pose for each predefined grasp type. \textbf{(A)}, \textbf{(B)} and \textbf{(C)} shows the results for the three hands we used.
    }
    \label{fig:principal}
\end{figure*}