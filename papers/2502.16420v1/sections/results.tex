

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{images/fig1.pdf}
    \caption{\textbf{The overview of our method.} \textbf{(A)}: Our method consists of two steps. The first step is to train a representation model on partial-view point cloud. The training set only consists of 40 objects. The second step would fix the representation model, and train a grasp decision model that takes the grasp-centric contact representation as input and outputs the grasp success score, based on hundreds of real-world trial-and-error attempts. The grasp algorithm is tested thoroughly on hundreds of unseen objects. \textbf{(B)}: Illustration of contact-centric grasp representation. A local geometry is discrete into several tangent planes along the approach direction of a robotic hand. Each tangent surface is transformed into the polarized coordinate frame of the robotic hand. The shape of the surface is encoded into discretized points and normal representation in the polar coordinate. \textbf{(C)}: Our experiments are also carried out on a three-finger hand and a five-finger hand and demonstrate excellent performance.}
    \label{fig:teaser}
\end{figure}


\section{Preliminary}
\subsection{Formulation}

A multi-finger grasp pose $g$ is formally defined as:
\begin{equation}\label{multi-finger definition1}
    g = [\mathbf{R}\ \mathbf{t}\ \mathbf{q}],
\end{equation}
where $\mathbf{R} \in \mathbb{R}^{3\times3}$ represents the robotic hand's rotation, $\mathbf{t} \in \mathbb{R}^{3\times1}$ denotes the hand's translation, and $\mathbf{q} \in \mathbb{R}^{n\times1}$ characterizes the joint configuration of a $n$-DoF multi-finger hand. The goal of the grasp pose detection problem is to predict a set of grasp poses from a scene perception. Conventionally, data-driven methods have employed a single network $f(\cdot)$ to map the partial point cloud of the scene $\mathcal{P} \in \mathbb{R}^{k\times3}$ to a set of candidate poses, $\mathbf{G}=\left\{g_i\right\}_{i=1}^{|\mathbf{G}|}$. In contrast, our approach decouples the mapping into two distinct steps: a state embedding step and a grasp decision step.


In the state embedding step, we extract a collection of contact-centric grasp representations from the partial point cloud $\mathcal{P}$. This is achieved by using a hand-agnostic representation model $\Phi(\cdot)$, which generates the scene representation $\mathcal{R}$:
\begin{equation}
\mathcal{R} = \Phi(\mathcal{P}),
\end{equation}
where $\mathcal{R} = \left\{r_j\right\}_{j=1}^{|\mathcal{R}|}$ is a set of contact-centric grasp representations.

The grasp decision step consists of two distinct procedures: a mapping process that converts contact-centric grasp representations into a set of candidate grasp poses (referred to as grasp candidates) and a quality estimation process for each candidate. For each grasp representation $r_j$, we generate a set of grasp candidates based on the specific robotic hand. A hand-dependent mapping function $\mathcal{K}(\cdot)$ takes a grasp representation $r_j$ and a hand specification $h$ as input, and output $\mathbf{G}_j$:
\begin{equation}
\mathbf{G}_j = \mathcal{K}(r_j, h),
\end{equation}
where $\mathbf{G}_j = \left\{g^{(i)}_j\right\}_{i=1}^{|\mathbf{G}_j|}$ denotes the set of grasp candidates for each $r_j$.

To estimate the quality of a grasp, we use a hand-dependent grasp decision model $\Psi(\cdot)$, which predicts the probability of success $\beta$ given a grasp representation $r_j$, a grasp pose $g^{(i)}_j$, and a hand specification $h$:
\begin{equation}
\beta = \Psi(r_j, g^{(i)}_j, h).
\end{equation}

\textbf{Objective:} Our goal is to find a set of grasp poses $\mathbf{G}^*$ that maximizes the grasp success rate given a desired number of grasp poses $K$:

\begin{align}
\mathbf{G}^* & =  \argmax_{\mathbf{G} \subset \bigcup_j \mathbf{G}_j, \; |\mathbf G|=K}\mathbb{E}_{r_j\in \mathcal{R},\; g_j^{(i)}\in \mathcal{K}(r_j, h) \cap \mathbf{G}}[\Psi(r_j, g_j^{(i)}, h)].
\end{align}

Figure~\ref{fig:teaser}A shows the pipeline of our methodology.




\subsection{Contact-centric Grasp Representation} \label{antipodal_representaiton}
We initiate our approach with the development of a contact-centric grasp representation. Initially, consider a 2D object, we can represent it as a set comprising surface points and their corresponding normals:
\begin{equation}
r_{2d} = \{(p_i,n_i) \mid i=1,2,\cdots,N\}.
\end{equation}
In this representation, $p_i$ denotes the position of a surface point, and $n_i$ represents the normal vector associated with that surface point. For clarity, the object's surface is discretized into $N$ bins.
 
For the task of grasp pose detection, it is common to represent the object shape in a local coordinate frame~\cite{gpd,mousavian2019graspnet}, as the classification of grasp quality depends primarily on the geometry within a localized area. This step, referred to as canonicalization, equips the representation with SE(3)-equivalent property and makes subsequent learning easier. For the 2D example, when we employ a polar coordinate system and sample the pole coordinate $\mathbf{t}_{2d}$ and the polar axis $\mathbf{R}_{2d}$, the discrete object shape representation is refactored accordingly. In this system, a surface point $p_i$ is represented by an angle $\alpha_i$ from the polar axis and a distance $d_i$ from the pole. Additionally, the surface normal is encoded as the angle between the normal $n_i$ and $\alpha_i$:
\begin{align}
r_{2d} = \bigg\{ (\alpha_i, d_i, \theta_i) \, \mid \, & i = 1, 2, \ldots, N, \, p'_i = \mathbf{R}_{2d}(p_i - \mathbf{t}_{2d}), \, n'_i = \mathbf{R}_{2d}n_i, \nonumber \\
& \alpha_{i} = \frac{p'_{i}}{\|p'_{i}\|},  d_{i} = \|p'_{i}\|, \nonumber \\
& \theta_{i} = \arccos\left(\frac{\alpha_{i} \cdot n'_{i}}{\|\alpha_{i}\|\|n'_{i}\|}\right) ; \mathbf{R}_{2d},  \mathbf{t}_{2d} \bigg\}.
\end{align}

A benefit of adopting a polar coordinate system is that the in-plane rotation angles $\{\alpha_{i}\}$ can be uniformly sampled across the polar angle range, resulting in constant values for $\{\alpha_{i}\}$ across different representations. Therefore, we move $\{\alpha_{i}\}$ to the right side of the set notation to make the representation more compact. Since the values of $d_i$ and $\theta_i$ depend on $\alpha_{i}$, we rewrite them as $d_{\alpha_{i}}$ and $\theta_{\alpha_{i}}$:
\begin{align} 
r_{2d} = \bigg\{ (d_{\alpha_i}, \theta_{\alpha_i}) \mid \alpha_i = 0, \frac{2\pi}{N}, \ldots, \frac{2\pi (N-1)}{N} ; \mathbf{R}_{2d},  \mathbf{t}_{2d} \bigg\}. 
\end{align}

Extending this representation to a real-world 3D object and a 3D coordinate system with rotation $ \mathbf{R}_{3d}$ and translation $\mathbf{t}_{3d}$ involves decoupling the object's geometry along a chosen axis and composing multiple 2D representations.  By selecting a specific axis in the 3D coordinate system (\textit{e.g.}, the $z$-axis), we discretize the object along this axis into $M$ sections. Each section corresponds to a cross-sectional slice of the object at a particular coordinate along the axis.

Within each cross-sectional slice, the same polar coordinate system is employed as in the 2D case. We apply the same angular sampling and the local geometry is represented in terms of distance and normal angle at each sampled angle $\alpha_i$. The 3D representation is then formulated as:
\begin{align}
r_{3d} = \bigg\{ & (d_{\alpha_i}, \theta_{\alpha_i})_j \, \bigg| \, \alpha_i = 0, \frac{2\pi}{N}, \ldots, \frac{2\pi (N-1)}{N}, \, j = 1, 2, \ldots, M; \mathbf{R}_{3d},\mathbf{t}_{3d} \bigg\}
\label{eqn_r3d}
\end{align} 

In the following sections, we use $r$ as a shorthand for $r_{3d}$. In Figure~\ref{fig:teaser}B, we illustrate the process of representing a 3D geometry in the contact-centric representation format within a robotic hand's local coordinate frame.

\subsection{Robotic Hands and Grasp Types} 


\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{images/hand_new_fig.pdf}
    \caption{\textbf{Illustration of the predefined grasp types for three robotic hands.} The types are categorized by the number of fingers involved in the grasping procedure. Some types can be categorized into multiple taxonomies defined in previous literature~\cite{feix2015grasp} when the grasping depths differ.}
    \label{fig:hand_pose}
\end{figure}

In our experiments, we utilize three distinct robotic hands:
\begin{itemize}
    \item DH-3: A three-finger robotic hand comprises 4 degrees of freedom and 2 motors, operating in an underactuated manner.
    \item Allegro: A four-finger robotic hand comprises 16 degrees of freedom and 16 motors, designed for full actuation.
    \item Inspire: A five-finger robotic hand equipped with 12 degrees of freedom and 6 motors, operating in an underactuated manner.
\end{itemize}
These robotic hands represent a variety of applications, including industrial tasks, dexterous manipulation, and underactuated prosthetic hand functionalities.

One challenge with dexterous grasping is the complexity introduced by the high degrees of freedom in these robotic hands, which creates a vast joint configuration space. However, when humans grasp objects, we typically rely on only a small subset of these configurations, which can be categorized into specific taxonomies~\cite{cutkosky1989grasp,feix2015grasp}. To address this complexity and make grasp pose detection more manageable, we discretize the continuous joint configurations of the multi-fingered hands into a finite set of predefined grasp types. This is represented as $\mathbf{q} \in \{\mathbf{q}_1, ..., \mathbf{q}_c\}$, where $c$ denotes the total number of grasp types specific to each hand.

For the three-finger hand, we discretize the entire joint space into several bins, while for the four- and five-finger hands, we select grasp types from the human grasp taxonomy that can be executed by these dexterous robotic hands. The predefined grasp types are illustrated in Figure~\ref{fig:hand_pose}. While this approach simplifies the grasp pose detection process, it still provides sufficient flexibility for subsequent manipulation tasks.

It is important to note that these grasp types serve as anchor poses prior to contact. Once the hand reaches its target position, it undergoes a closure process, where the fingers progressively move toward each other until the forces exerted on the finger joints reach predefined limits.


\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{images/train-test-obj-2-robot.pdf}
    \caption{\textbf{Experimental setup.} \textbf{(A)}: Platform setting of our dexterous grasping experiments. \textbf{(B)}: Illustration of our 40 training objects and 150 testing objects. The testing objects are much more diverse than the training objects, including deformable and adversarial objects not presented in the training set.}
    \label{fig:objects}
\end{figure}

\subsection{Overview of Experiments}
To assess the performance of our multi-finger grasping model, we established a real-world experimental platform. Our hardware setup includes a UR5 robotic arm and an Intel RealSense D415 camera, positioned at the robot's end-effector. The initial camera pose is vertical to the table and is approximately 60 cm above it. Figure~\ref{fig:objects}A illustrates the setup of our robotic platform.

We first learn a general hand-agnostic representation model based on an offline annotated, large-scale dataset. Once the representation model is learned, we use the predicted contact-centric grasp representation as a new state space for the problem of multi-finger grasping. For each robotic hand, we can learn grasping in the real world directly through trial and error. We start with thousands of trial-and-error grasp attempts, and gradually reduce the number to hundreds of attempts in later experiments to demonstrate the efficiency of our learning paradigm. We also vary the number of training objects from 144 to 40 and even 30, to verify the generalization ability of our grasp system. 

To assess the multi-finger grasp performance thoroughly, we construct a comprehensive real-world test set, featuring objects commonly encountered in everyday life. These objects encompass diverse shapes, materials, and textures and are categorized into hardware, food, textile, household, toy, and adversarial items. The test set comprises nearly 150 objects ranging in size from $2.5 \times 2.5 \times 2.5$ cm$^3$ to $8 \times 8 \times 5$ cm$^3$.

During real-world testing, objects from each category are randomly placed on a table in a cluttered way, and the robots attempt to grasp all the objects and clear the table. This process is repeated twice for accuracy. We also establish a baseline that aligns the principal closing axis of the grasp types with antipodal grasp poses, followed by collision detection, to compare with our proposed method. The success rate is determined by dividing the number of successful grasp attempts by the total number of grasp attempts.

Ultimately, our grasp system is successfully evaluated on three different robotic hands, where the whole system is trained on a limited dataset comprising merely 40 objects and hundreds of grasp attempts, and tested on a broader spectrum of 150 previously unseen objects. Notably, it represents a pioneering achievement in the literature where a grasping algorithm is evaluated on a significantly larger set of objects than those included in its training dataset. The final training and testing objects are illustrated in Figure~\ref{fig:objects}B for reference.


\section{Results}
\subsection{Learning Dexterous Grasping in the Real World}
Based on the contact-centric grasp representations output by a trained representation model, we first employ a training set of 144 objects to train the grasp decision model. Approximately 1,000 grasp samples are collected for each grasp type of the DH-3 and Inspire Hand, and 200 grasp samples are collected for each grasp type of the Allegro Hand, forming the basis for our learning process. The amount of training objects and grasp samples would be gradually reduced in later sections to verify the effectiveness of our method.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{images/main_results.pdf}
    \caption{\textbf{Success rates on the testing set after training on abundant real-world data.} \textbf{(A)}: The averaged and detailed success rates of the DH-3 hand on five object categories commonly encountered in our daily activities. \textbf{(B)}: The averaged and detailed success rates of the Allegro hand. \textbf{(C)}: The averaged and detailed success rates of the Inspire hand. \textbf{(D)}: The success rates on the adversarial objects of three robotic hands.}
    \label{fig:main}
\end{figure}

\subsubsection*{Dexterous Grasping on Daily Objects}

We systematically evaluate the success rates of our approach on testing objects from the first five categories commonly encountered in our daily activities. The average success rates achieved by the three distinct robotic hands are 97\%, 78\%, and 83\%, respectively. Movies \hyperlink{movie_s1}{S1}, \hyperlink{movie_s2}{S2} and \hyperlink{movie_s3}{S3} record the grasping process. In contrast, the success rates of the baseline method using heuristic sampling and collision detection reach only 66\%, 51\%, and 58\%. A detailed breakdown of success rates for each object category is presented in Figure~\ref{fig:main}A. Compared to the baseline method, the substantial improvements across this extensive test set demonstrate the effectiveness of our proposed representation and approach.

Several noteworthy points are hereby highlighted. Firstly, the 3-finger gripper attains an average success rate of 97\% across over 100 real-world objects, surpassing even the performance of previous state-of-the-art parallel-gripper algorithm~\cite{fang2023anygrasp}. Secondly, for deformable objects within the textile and food categories, the grasp success rates across different grippers show no significant degradation. In some cases, they even slightly outperform the overall success rate, despite the absence of explicit training on deformable objects. This observation emphasizes the remarkable generalization capacity of data-driven methods. We observe that deformable objects tend to comply with the gripper during the grasping process, making them easier to be successfully grasped.

Regarding grasping speed, our system takes an average of 0.5 second to generate 200 grasp poses in a cluttered scene. Additionally, an extra collision detection step utilizing scene partial point cloud and hand mesh is performed. It takes 20 seconds on our CPU using Open3D library~\cite{Zhou2018}. Although this step could be accelerated through advanced collision detection technology or hardware acceleration, this aspect falls outside the scope of this paper.

\subsubsection*{Dexterous Grasping on Adversarial Objects}

In addition to daily objects, we extend our methodâ€™s evaluation to more challenging adversarial objects. These objects encompass 13 human-selected items from DexNet~\cite{dexnet2} and 49 program-generated objects from EGAD! evaluation set~\cite{morrison2020egad}, characterized by distinct shapes and varying grasp difficulties. Prior literature shows a performance degradation of parallel grasping on adversarial versus daily objects~\cite{fang2023anygrasp}. To the best of our knowledge, this is the first comprehensive evaluation of a multi-finger grasping algorithm on adversarial objects in real-world scenarios.

Success rates for the three distinct robotic hands are reported in Figure~\ref{fig:main}B, where our system achieves 99\%, 82\%, and 79\% success rates, respectively. Movies \hyperlink{movie_s4}{S4}, \hyperlink{movie_s5}{S5} and \hyperlink{movie_s6}{S6} record the grasping process.  In contrast, the baseline method achieves success rates of 72\%, 54\%, and 59\%. Remarkably, the performance on adversarial objects is on par with daily objects for all the robotic hands, highlighting the promising generalization ability of our dexterous grasping system. It surprises us since previous results from parallel grippers show a dramatic performance degradation. We presume that the additional fingers can improve the grasping ability, and the adversarial objects designed for parallel grippers do not pose significant challenges in multi-finger cases. 

In the subsequent sections, unless otherwise stated, we proceed to conduct experiments on all 150 objects including daily ones and adversarial ones.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{images/efficiency_analysis.pdf}
    \caption{\textbf{Success rates on the testing set after training on reduced real-world data.} \textbf{(A)}: We reduce the training object number from 144 to 40 and 30 respectively and test the success rates on different categories of objects. \textbf{(B)}:  With 40 training objects, we reduce the data from around 1000 trials per grasp type to 100 trials and 50 trials respectively. \textbf{(C) and (D)}: When reducing the training data on fewer training objects and fewer grasp attempts, success rates on both Allegro hand and Inspire hand only decrease slightly, showing good generalization ability and high learning efficiency of our method.}
    \label{fig:efficiency}
\end{figure}

\subsection{Reducing Real-World Training Burden}
In previous experiments, we collected a considerable volume of real-world training data, approximately 1,000 trials per grasp type, on the full set of 144 training objects. In this section, we aim to alleviate the demands of real-world training by assessing the model's performance under reduced object and trial conditions. For the sake of simplicity, our evaluation concentrates on the 3-finger hand in this section.

We initiate this exploration by reducing the number of objects utilized in real-world trial-and-error attempts. Two smaller sets consisting of 40 and 30 objects were adopted for our experiments. The details of the training object sets are given in Materials and Method. To ensure a fair comparison with the original object set, we collect an equivalent number of around 1,000 grasp samples for each grasp type, ensuring the convergence of the grasp decision model.

Figure~\ref{fig:efficiency}A presents the experimental results. As the training object count reduces from 144 to 40, the grasp success rate achieves 96.7\% on all testing objects, representing only a marginal decrease of 1.1\%. Such subtle performance degradation, given a nearly 3/4 reduction in training objects, showcases the robustness of our approach.  A further reduction to 30 training objects results in an overall grasp success of 95.1\%. This translates to a further decrease of 1.6\%, yet the performance remains notably promising. Our experiments demonstrate that, with proper learning methods, the thousands of training objects adopted in previous systems\cite{dexnet4, wang2023dexgraspnet} are not necessary. 



Since the performance degradation when reducing the object set from 144 to 40 and from 40 to 30 is comparable, while reducing the training object set from 40 to 30 does not significantly lower the training burden, we opt to proceed with the 40-object training set for subsequent experiments.

We then explore the impact of reducing the number of trials and errors for each grasp type. On the 40 training objects, we reduce trials and errors from approximately 1,000 attempts per grasp type to 100 attempts and 50 attempts, respectively. Figure~\ref{fig:efficiency}B presents the real robot testing results. When training with 100 trials per grasp type, the success rate reaches 94.5\% on average on all objects. We show the whole grasping process in Movie \hyperlink{movie_s7}{S7}. This success rate is strikingly high given the limited number of real-world training samples. Previous literature~\cite{xu2023unidexgrasp,liu2023dexrepnet} often required millions of grasp attempts in simulation to achieve grasping proficiency. Further reducing the trials to 50 attempts per grasp type yields a success rate of 93.1\% on all objects. These results demonstrate the high learning efficiency of our method, which requires only a small number of grasp attempts for convergence. In our following experiments, given the already high efficiency of 100 trials per grasp type, we adopt this setting for learning.

\subsection{Dexterous Grasp Learning with 40 Objects and 100 Attempts}
In the previous section, we demonstrated the robust grasping policy acquired by the 3-finger gripper through a significantly limited amount of training data and real-world attempts. In this section, we extend the validation of such a learning paradigm to the other two robotic hands utilized in this study.

We directly assess the performance of training using 40 objects with 100 trials for each grasp type on the four-finger and five-finger hands. Depending on the number of grasp types, the total real-world training samples amount to 1,000 and 800 for these two robotic hands, respectively. This significantly reduced volume of real-world training samples, nearly 1/10 of the original experiments, presents a territory in grasp learning that is unexplored by previous work.

Figure~\ref{fig:efficiency}C and Figure~\ref{fig:efficiency}D display the detailed success rates of real-world experiments. The average success rates stand at 75\%, and 77\% for all objects. The grasping process is recorded in Movies \hyperlink{movie_s8}{S8}, \hyperlink{movie_s9}{S9}, \hyperlink{movie_s10}{S10} and \hyperlink{movie_s11}{S11}. It's striking that the success rates show minimal decreases compared to the original performance. This observation demonstrates the substantial learning efficiency enabled by our methodology. Such proficiency allows diverse robotic hands to acquire dexterous grasping ability in real-world settings. 

Notably, this efficiency surpasses that observed in human infants, who typically require months of practice to develop visually guided grasping skills. The grasp success rates for human infants reach 61.9\% at 8 months old~\cite{domellof2015infant}, which involves thousands of practice attempts starting at 4 months old~\cite{newell1989task}. It is noteworthy that our grasping results are achieved based solely on visual perception, with no tactile feedback.

\subsection{Influence of Grasp Types}


\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{images/type_analysis_new2.pdf}
    \caption{\textbf{Analysis of the influence by grasp type.} \textbf{(A)} A breakdown analysis of grasp success rates on different grasp types for each robotic hand. \textbf{(B)} The selected frequency of different grasp types for each robotic hand during testing. \textbf{(C)} Grasping success rates when using different portions of grasp types for the allegro hand.}
    \label{fig:grasp_type}
\end{figure}

\subsubsection*{Accuracy of Different Grasp Poses}

In the above experiments, we have shown that our method can enable efficient grasp learning with high success rates. Here we further analyze the success rates of each robotic hand with a detailed breakdown according to their respective grasp types. For clarity purposes, we number each grasp type, as illustrated in Figure~\ref{fig:index}. The results trained on 40 objects and 100 grasp attempts per grasp type are adopted for analysis, as depicted in Figure~\ref{fig:grasp_type}A. For each hand, we can see that different grasp types have different difficulties in dealing with grasping. Usually, the success rates after learning are dramatically higher than the baseline method. However, there also exist some exceptions. For example, the grasp type 1 of the five-finger Inspire hand after training yields a close success rate to the baseline. After inspection, we found that this grasp type was selected fewer times after the training. We anticipated that other grasp types might be more confident to grasp if the objects can be grasped by multiple types, which leaves some hard cases for this grasp type.

\subsubsection*{Distribution of Grasp Types}
A natural question that arises is whether the system learned by our method can demonstrate a variety of grasp types. From the example above, it is possible that the system may achieve high success rates by favoring one or two grasp types while ignoring diversity. To address this, we analyze how frequently each grasp type is selected during testing to verify whether our system indeed learns diverse grasp poses. To quantify the frequency of each grasp type, we normalize by dividing the number of grasp attempts for each type by the total number of grasp attempts across all types.

To establish a baseline, we examine the frequency of grasp types obtained by the baseline method, reflecting the inherent frequency determined solely by collision detection. Grasp types prone to collision with the scene naturally constitute a smaller fraction among all types. This baseline grasp type frequency serves as a reference for natural distribution. The top row of Figure~\ref{fig:grasp_type}B illustrates the grasp type frequency for different robotic hands. Notably, the three-finger hand exhibits a balanced distribution, whereas the four- and five-finger hands display more unbalanced distributions. This discrepancy arises from the fact that the fingertips of the three-finger hand consistently point in the same direction along the approach vector, resulting in similar collision situations across different types. Conversely, the four- and five-finger hands exhibit types with greater variance, including some that are prone to collision with the scene.

Then, we present the frequency of grasp pose after employing our learned system. The statistics are given in the second row of Figure~\ref{fig:grasp_type}B. For the three-finger hand, type 3 presents an increasing ratio among all grasp types. The reason is that this grasp type presents a higher success rate, and usually has a higher grasp quality score than other grasp types. However, the other three grasp types are also frequently selected. For the four- and five-finger hand, the grasp frequency is similar to the baseline method. These results affirm that our learned system adeptly captures diverse grasp poses, achieving high success rates without compromising grasp diversity.

\subsubsection*{Reducing Grasp Types}
Another question for multi-finger grasping is whether employing multiple grasp types is necessary, given the argument that a single power grasp might be sufficient for good results. However, we argue that incorporating multiple types enhances flexibility, particularly when faced with cluttered scenarios. To prove that, we conducted a targeted experiment to compare grasping outcomes with varying numbers of grasp types. Specifically, we employed the best grasp model trained with 144 objects for the Allegro hand, initially defined with 10 grasp types. In our experiment, we compared the original model with two modified versions that use fewer grasp types. The first version was limited to a single grasp type, specifically the one that achieved the highest overall success rate across all types. The second version used a subset of the five most effective grasp types, chosen based on their individual success rates. For simplicity, the evaluation focused exclusively on adversarial objects due to the performance similarity with that on the entire object set. The resulting success rates are detailed in Figure~\ref{fig:grasp_type}C.

The original method, employing 10 grasp types, achieved an 80.3\% success rate on the test set. In contrast, utilizing only a single grasp type led to a reduction in the grasp success rate to 67.3\%. Employing five grasp types performed better, resulting in a success rate of 77.6\%, but is still inferior to the original method. Our experimental results show that increasing the number of grasp types can improve overall grasp success rates. One reason for this improvement is that a greater variety of grasp types provides more flexibility, enabling the hand to better adapt to different object shapes, sizes, and orientations. Additionally, using multiple grasp types can increase tolerance for collisions, allowing the hand to adjust its grasping strategy based on spatial constraints, particularly in cluttered environments. It is noteworthy that, on the other hand, our results also reveal that the benefits derived from further adding grasp types would eventually saturate. Thus, it is reasonable to adopt diverse yet limited grasp types, which optimizes both grasp success rates and learning efficiency.
