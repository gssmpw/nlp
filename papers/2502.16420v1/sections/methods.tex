
\section{Materials and Method}


\subsection{Baseline Method}
Here we introduce our baseline method of multi-finger grasping. Currently, our community can achieve human-level robotic grasping with a parallel-jaw gripper~\cite{fang2023anygrasp}. An intuitive approach for multi-finger grasping is to mimic the behavior of parallel grasping. Thus, we propose a baseline method that discovers the principal closing axis of a robotic hand and aligns it with a parallel grasp pose. First, for each grasp type of a robotic hand, we manually designate its principal closing axis, which is the primary direction along which the fingers converge when the hand closes to grasp an object. Then, given a parallel grasp pose and a grasp type of a hand, we can align the multi-finger hand's principal closing axis to the parallel grasp pose. Previous literature~\cite{fan2019optimization,fan2018real} also explored similar ways to initialize a multi-finger grasp. In Figure~\ref{fig:principal} we illustrate the alignment example. 

When grasping with a selected robotic hand, we first generate multiple high-score antipodal grasp poses for a single-view point cloud using the AnyGrasp library~\cite{fang2023anygrasp}. Then, for each antipodal grasp pose, we align the robotic hand configured in all grasp types with the antipodal pose. It means that for each antipodal grasp pose, we would have multiple multi-finger grasp candidates with different types. For all of the multi-finger grasp candidates across the scene, we run a collision detection based on the partial-view point cloud and robotic hand model. We select the grasp type assigned with the highest antipodal grasp score for the remaining grasp candidates without collision. If multiple grasp candidates have the same grasp score, we randomly select one as the final grasp pose.
 
\subsection{Algorithm Details}
Next, we introduce the details of our algorithm, which consists of three steps: learning the representation model, mapping from representation to grasp pose, and learning the grasp decision model.
\subsubsection*{Representation Model} \label{repre_model}
Our representation model takes a partial-view point cloud as input and generates the contact-centric grasp representation $r$ for different rotation $\mathbf{R}_{3d}$ and translation $\mathbf{t}_{3d}$ across the scene. It may seem initially challenging to establish the representation model, given that this representation demands full surface information, and the $r$ needs to be predicted for SE(3) space across the scene. However, recent advancements in grasp pose detection algorithms have successfully learned the mapping from partial-view point clouds to antipodal grasp poses across the scene, unveiling the feasibility of learning the mapping from partial-view point clouds to the proposed intermediate representation. Specifically, prior works, such as graspnet-baseline~\cite{graspnet} and GSNet~\cite{wang2021graspness}, have predicted the gripper opening widths and antipodal scores for discretized rotation $\mathbf{R}_{3d}$ and translation $\mathbf{t}_{3d}$ across the scene:   
\begin{align}
s = \bigg\{ & (w_{\alpha_i}, \mu_{\alpha_i})_j  \bigg|  \alpha_i = 0, \frac{2\pi}{N}, \ldots, \pi-\frac{2\pi}{N}, \, j = 1, 2, \ldots, M;  \mathbf{R_{3d}}, \mathbf{t_{3d}} \bigg\},
\label{grasprep}
\end{align}  
where $w_{\alpha_i} = 2\times\max(d_{\alpha_i}, d_{\alpha_i+\pi})$ and  $\mu_{\alpha_i} = \max(\tan(\theta_{\alpha_i}), \tan(\theta_{\alpha_i+\pi}))$ are the gripper opening width and antipodal grasp quality metric defined in~\cite{graspnet}. This representation shares a structural resemblance with our contact-centric grasp representation in Equation~\eqref{eqn_r3d}. Thus, we opt to build our representation model upon the GSNet~\cite{wang2021graspness} architecture.
 
When predicting the representation, it is intractable to account for every possible \(\mathbf{R}_{3d}\) and \(\mathbf{t}_{3d}\) in continuous space. Previous work~\cite{graspnet,wang2021graspness} addressed this by selecting orientation \(\mathbf{R}_{3d}\) from 300 discretized directions, voxelizing the scene, and selecting only \(\mathbf{t}_{3d}\) that lies on object surfaces. However, the total number of resulting combinations still remains quite large. In~\cite{wang2021graspness}, a metric called “graspness” is proposed as a heuristic to bias sampling towards \(\mathbf{t}_{3d}\) and \(\mathbf{R}_{3d}\) values that have a higher probability of generating successful grasp poses. This metric includes two components: “point-wise graspness” and “view-wise graspness.” Point-wise graspness is calculated by counting the ratio of high-score antipodal grasp poses among all poses at a given \(\mathbf{t}_{3d}\), while view-wise graspness counts this ratio among all grasp poses at a given \(\mathbf{R}_{3d}\) for a sampled \(\mathbf{t}_{3d}\). These two scores are learned jointly within the grasp pose detection network and guide sampling during inference.

In this work, since we aim to train a hand-agnostic representation model, we define a new “graspness” score for each \(r\) to indicate its suitability for subsequent grasping across different robotic hands. Intuitively, for a point \((\alpha_i, d_{i}, \theta_i)\), a robotic hand achieves better contact when \(\theta_i\) is small, meaning the surface normal \(n_i\) is opposite to the contact direction (assuming the robotic finger approaches towards the polar pole of the local coordinate frame). Additionally, geometries with many high-score antipodal grasp poses tend to be easier for dexterous hands to grasp. Thus, we define “graspness” in this paper as the sum of \(\theta_i\) values below a threshold and the number of antipodal grasp poses in \(r\). This definition helps the model reduce candidates for representation prediction within a scene without significantly affecting accuracy.

Similar to GSNet, our representation model consists of three cascaded modules. Firstly, a Minkowski Engine~\cite{choy20194d} backbone takes the single-view point cloud as input, encodes their geometric features, and outputs a computed feature vector for each input point. Then a multi-layer-perception (MLP) takes the features of each point and generates a point-wise graspness heatmap. We sample 1024 seed points with high graspness, and forward these points to another MLP block. It outputs the view-wise graspness scores for 300 approach directions towards each seed point respectively. We then select the direction with the highest graspness score for each point, group the features with cylinder grouping~\cite{fang2020graspnet} along that direction and forward the grouped features for each point through a final MLP block. This final layer outputs $r$ for $N=48$ in-plane rotations and $M=5$ grasp depths, which are 0.005m, 0.01m, 0.02m, 0.03m and 0.04m respectively.


\subsubsection*{Mapping from Representation to Multi-Finger Grasp Candidates}
After we obtain the representation $r$ at different positions across the scene, we link them with different grasp types of a robotic hand to generate multi-finger grasp candidates. In theory, since we have predicted the contact information, we can already generate suitable multi-finger grasp candidates through optimization~\cite{miller2004graspit,liu2021synthesizing}. However, for simplicity, we follow the same technique adopted in the baseline method to generate multi-finger grasp candidates. Such a design also facilitates fair comparison with the baseline method and shows how our grasp decision network improves the grasping ability.

For each predicted CGR with the form of Equation~\eqref{eqn_r3d}, we calculate the corresponding antipodal grasp representation defined in Equation~\eqref{grasprep}. Then the CGRs with top-500 antipodal grasp scores are selected. These representations are associated with different multi-finger grasp candidates following the same procedure of the baseline method. After this process, we query the orientation $\mathbf{R}_g$ and translation $\mathbf{t}_g$ of the multi-finger grasp candidates associated with the CGRs (more details in supplementary material). Together with the associated grasp types, we map the CGRs to multi-finger grasp candidates.





\subsubsection*{Learning Multi-Finger Grasping} \label{model_architecture}
For each sampled grasp candidate $g_i$, we learn a mapping from its corresponding CGR $r_i$ to grasp success probability. This mapping is approximated through the grasp decision model, using training data collected via trial and error: 
\begin{equation}
\nonumber\alpha = \Psi(r_i, g_i, h).
\end{equation}

In practice, we  train different decision models for different robotic hands, denoted as  $\Psi_h(r_i, g_i)$. Since the grasp types of each hand are discretized, we further decompose the classification of different grasp types into different sub-models:
\[
\nonumber\Psi_h(r_i, g_i) = \sum_{\mathbf{q}} \mathbb{I}(g_i, \mathbf{q}) \, \Psi_{h,\mathbf{q}}(r_i, \mathbf{R}_g, \mathbf{t}_g),
\]
where $\mathbb{I}(g_i, \mathbf{q})$ is an indicator function that is 1 when $g_i$ matches the grasp type $\mathbf{q}$ and 0 otherwise. Since $\mathbf{R}_g$ and \(\mathbf{t}_g\) are functions of \(r_i\), we can simplify the input to the sub-models by removing \(\mathbf{R}_g\) and \(\mathbf{t}_g\). Thus, \(\Psi_{h,\mathbf{q}}(r_i, \mathbf{R}_g, \mathbf{t}_g)\) can be reformulated as \(\Psi_{h,\mathbf{q}}(r_i)\), where the computation of \(\mathbf{R}_g\) and \(\mathbf{t}_g\) is implicit in the model. We empirically found that using different sub-models for different grasp types gives better performance. The input to the model consists of the CGR of a selected grasp candidate. The model's output is a score of whether the selected grasp would be successful. Details of the model is given in supplementary material. For simplicity, we regard the combination of all sub-models for each robotic hand as a single model and still refers to it as the grasp decision model.

\subsubsection*{Detection Post-Processing}
\label{detection post processing}
Following the grasp decision model's output, we select grasp poses with high-quality scores, typically exceeding 0.9. Collision detection is then performed by voxelizing the pre-shaped multi-finger hand and examining intersections between the hand voxels and the scene point cloud using the Open3D library. The final grasp pose is chosen from those grasp poses without collision with the scene, with the highest grasp quality score.




\subsection{Training Environment}

\definecolor{darkgrey}{rgb}{0.65, 0.65, 0.65}
\begin{algorithm}[!t]
\caption{Multi-finger Grasping Data Collection for Robotic Hand $h$}\label{alg:anyhand-learn}

\textbf{Input:} the expected size $K$ of the grasp dataset.

\textbf{Output:} the collected grasp dataset $\mathbf{G}$.

\begin{algorithmic}[1]
\State $\mathbf{G} \leftarrow \emptyset$
\While{$|\mathbf{G}| < K$}
\State The \textbf{\textit{robot}} moves to the ready pose
\State $\mathcal{P} \leftarrow \textbf{\textit{camera}}.\text{perception}$ \Comment{\textcolor{darkgrey}{capture RGBD images and transform into point cloud}}
\State $\mathcal{R} \leftarrow \Phi (\mathcal{P})$ \Comment{\textcolor{darkgrey}{generate scene representation from the point cloud}} %Generate intermediate representation for $\mathcal{P}^t$
\State Sample a CGR $r \in \mathcal{R}$ in the scene
\State Sample a grasp type $\mathbf{q} \in \{\mathbf{q}_1,...,\mathbf{q}_c\}$
\State $[\mathbf{R}_g\ \mathbf{t}_g]\leftarrow\text{compute\_grasp\_pose}(r)$
\Comment{\textcolor{darkgrey}{Map CGR to the grasp pose}}
\If{$\text{collision\_detection}([\mathbf{R}_g\ \mathbf{t}_g\ \mathbf{q}], \mathcal{P}; h)$} \\\Comment{\textcolor{darkgrey}{Check if the multi-finger grasp pose will collide with the scene point cloud}}
\State \textbf{continue}
\EndIf
\State The \textbf{\textit{robotic hand}} executes the multi-finger grasp pose $[\mathbf{R}\ \mathbf{t}\ \mathbf{q}]$
\State Record the grasp result $S$ \Comment{\textcolor{darkgrey}{Collect trial-and-error results}}
\State $\mathbf{G} \leftarrow \mathbf{G} \cup \{\left<r, \mathbf{R}_g, \mathbf{t}_g, \mathbf{q}, S\right>\}$
\EndWhile \\
\Return the collected grasp dataset $\mathbf{G}$
\end{algorithmic}
\label{alg_datacollection}
\end{algorithm}

\subsubsection*{Training Object Set}
Our experiments involve three different training object sets, the larger dataset $\mathbb{L}$ contains 144 objects, the smaller one $\mathbb{S}$ contains 40 objects, and the tiniest one $\mathbb{T}$ contains 30 objects. $\mathbb{L}$ is the training set collected in AnyGrasp~\cite{fang2023anygrasp}. $\mathbb{S}$ encompasses the 40 training objects featured in the original GraspNet-1Billion dataset, and $\mathbb{T}$ includes 30 randomly selected objects from $\mathbb{L}$. In Figure~\ref{fig:train_obj} we detail the three training object sets.

\subsubsection*{Data Annotation and Collection}


To facilitate the training of our representation model, we re-annotate the GraspNet-1Billion dataset. The training set consists of 100 scenes made up of 40 objects. Each scene includes 256 RGBD images, each of which can be transformed into a single-view point cloud. Instead of the original antipodal grasp representation (illustrated in Equation~\eqref{grasprep}), we annotate the contact-centric grasp representation as per Equation~\eqref{eqn_r3d} for the 100 training scenes.

Our process begins by voxelizing the 3D mesh of each training object with a resolution of 0.005 m. We collect all points on the voxelized object surface, denoted as \(\{\mathbf{t}_{3d}^{(i)}\}\), where \(i\) indexes each individual surface point. For each surface point \(\mathbf{t}_{3d}^{(i)}\), we sample 300 approach directions \(\{\mathbf{R}_{3d}^{(j)}\}\), where \(j\) indexes the sampled directions. We then compute the CGR \(r\) for each combination of \(\mathbf{t}_{3d}^{(i)}\) and \(\mathbf{R}_{3d}^{(j)}\). This computation relies on the complete mesh of the object. The computed CGRs are then projected from each training object to the training scenes based on the object’s 6D pose provided in the original dataset.

After generating the CGRs for each scene, we apply a simple post-processing step to verify grasp feasibility. For each CGR, we check whether a cylindrical region extending backward along the approach direction collides with the tabletop or other objects in the scene. If a collision is detected, we set the CGR to a zero vector, indicating it is not a viable grasp candidate. This post-processing step helps reduce the likelihood of robotic hand collisions within the scene.

To train the grasp decision model, we collect grasping data by trial and error. Previously, most of the grasp attempts related to multi-finger grasping were collected within a simulation environment. Nevertheless, significant gaps may arise due to the inherent differences between the simulation and real environments. Thus, in this paper, we directly collect grasping data in a real-world environment.

We provide an overview of the complete data collection pipeline, summarized in Algorithm~\ref{alg_datacollection}. Initially, we randomly place objects on the table. We then run the representation model to generate dense contact-centric grasp representations for the scene. We sample a CGR and a grasp type of the robotic hand, and map the CGR to a multi-finger grasp pose. Collision detection is performed to ensure that the grasp pose does not collide with the scene. If no collision happens, we execute the grasp process. During this process, we record whether the grasp is successful and store the necessary information in the dataset. 


\subsubsection*{Training Details}
For the representation model, the input point clouds are down-sampled with a voxel size of 0.005m. In practice, we set the parameters of the 3D representation $N$ and $M$ in Equation~\eqref{eqn_r3d} to 48 and 5 respectively. The model is trained on the re-annotated GraspNet-1Billion dataset using one Nvidia A100 GPU with Adam optimizer~\cite{kingma2014adam} and an initial learning rate of 0.001. The learning rate follows a descent strategy and we adopt ``poly'' policy with $power=0.9$ for learning rate decay. The model is trained from scratch with a batch size of 4. For data augmentation, we randomly flip the scene horizontally and randomly rotate the points by Uniform$[-30^\circ,30^\circ]$ around the $z$-axis (in the camera coordinate frame). We also randomly translate the points by Uniform$[-0.2\text{m},0.2\text{m}]$ in the $x$- or $y$-axis and Uniform$[-0.1\text{m},0.2\text{m}]$ in the $z$-axis.

For the grasp decision model, since we have a relatively limited amount of collected data, our model is trained for only 20 epochs to avoid overfitting. We leverage the Adam optimizer~\cite{kingma2014adam}. The learning rate follows a segmented descent strategy starting from 0.0001, and the batch size $\mathbf{Z}$ is set to 128 to optimize training efficiency. Since the network is quite small, we train the model on a laptop with NVIDIA 1650 GPU.


\subsection{Experimental Procedure}
In each experiment, we randomly distribute objects from different categories in the robot workspace. During the grasping process, the partial-view point cloud captured by the camera is fed into our representation model. When collecting training data, we follow the procedure in Algorithm 1. During testing, we first choose 100 CGRs from the outcome of the representation model, which has the top-100 antipodal grasp scores. These CGRs are mapped to multi-finger grasp candidates, and given the number of predefined types for each robotic hand, the total number of multi-finger grasp candidates varies (\textit{e.g.}, we define 4 grasp types for the three-finger hand, thus it has 400 grasp candidates). These grasp candidates are fed into our grasp decision model. The grasp candidates with the top-200 grasp quality scores then undergo collision detection post-processing. The grasp pose that passes collision detection and has the highest grasp score is selected as the final multi-finger grasp pose in the camera's coordinate system. It is subsequently converted into the world coordinate system and sent to the UR5 robot through socket communication. The UR5's embedded motion planner navigates it to the grasp pose, where we set a waypoint 10 cm backward from the final grasp along the approach direction to avoid collision during movement. Simultaneously, the robotic hand is configured to the selected grasp type. After the robot arm reaches the target pose, the robotic hand closes the fingers until the grasping force reaches a predefined limit. The robot then lifts the object and moves it to the top of the bin and drops the object. The experiment concludes with manually recording whether the robotic hand successfully move the object to target position.
