\section{Introduction}
Grasping, as a fundamental problem of prehensile manipulation, holds significant importance in robotics. Over the past decades, diverse mechanical structures for robotic hands have been developed. Visually guided dexterous grasping is in high demand to enable robots to interact effectively with their environments. This ability also plays a crucial role in the context of intelligence. Throughout human evolution, early humans developed the capability for precise grip~\cite{doi:10.1126/science.1261735}, which enabled tool use and is believed to have facilitated the evolution of the human species~\cite{almecija2010early, kivell2015evidence}. From the perspectives of both advancing robotics and promoting embodied intelligence, it is essential to design a learning framework that efficiently equips different robotic hands with visually guided dexterous grasping capabilities.

To make such a grasping system practically useful, it should use a single commodity camera, observe environments with cluttered objects, handle perception noise and generate a set of dexterous grasp poses that can be selected by subsequent tasks. Due to the challenges of the problem, early research focused on generating dexterous grasp poses given a single, complete object mesh, utilizing either analytical~\cite{miller2004graspit, rosales2011synthesizing, liu2021synthesizing, liu2020deep} or learning-based approaches~\cite{li2023gendexgrasp}. The idea is to decouple the grasping system into 6D pose estimation and grasp poses generation based on the object CAD model. However, the requirement for the object mesh limits its ability to handle new object shapes.

It is challenging to detect grasp poses for unseen objects based on partial-view perception. Some recent methodologies pursue mesh completion using partial point clouds~\cite{lundell2021ddgc, wei2022dvgg, wei2024learning}, followed by grasps generation on the complete mesh. However, the error introduced by perception noise and mesh completion often results in inaccurate grasp analysis.  An increasing amount of research has attempted to learn the mapping from raw partial observation to grasp poses within a single network. Due to the highly nonlinear property of this mapping, extensive training data is required. Two data sources are commonly adopted: human grasping demonstrations~\cite{gupta2016learning, christen2019guided, qin2022dexmv, mandikal2022dexvip, wei2024learning, shaw2024learning} or data from simulated environments\cite{brahmbhatt2019contactgrasp, corona2020ganhand, grady2021contactopt,li2023gendexgrasp, wang2023dexgraspnet, lum2024dextrahg,
singh2025dextrahrgb}. However, both methods have their limitations. The former approach struggles to accurately capture human hand gestures and is confined to robotic hands resembling human anatomy. The latter requires substantial effort to build the simulation environment, annotate grasp poses by analytical models or trial and error, and transfer algorithms from simulation to the real world. These challenges limit current grasping systems to simple scenarios, typically involving a single object at a time from a limited set.  No prior work demonstrates robust grasping in cluttered environments from partial-view perception in the real world.

Most critically, even if these challenges are overcome, the policy obtained with substantial efforts is only suitable for a specific robotic hand each time. The  end-to-end learning paradigm implicitly encodes the information about hand kinematic structure, relevant state information and grasp quality in the weights, making it difficult for models to share computation between different hands. Consequently, we need to repeat the tedious data generation and policy training pipeline for each hand.

We identified two main bottlenecks for efficiently learning visually guided dexterous grasping for different robotic hands: the requirements for extensive training data for each hand, and the inability to share  computation across different hands. These bottlenecks arise from attempting to learn the mapping from raw observation to grasp poses with an end-to-end network. In this paper, we revisit this paradigm. We hypothesize that if there exists a low-dimensional intermediate state space that encapsulates grasp information, then the mapping from this state space to grasp poses can be learned more efficiently than the original mapping, requiring less training data. Moreover, if such a state space is transferable across different robotic hands, it could be shared without the need to retrain a state estimator each time. Note that such a state space should not require object knowledge during inference, in order to generalize to unseen objects.


Recognizing this potential, we aimed to identify such a state space. For the grasping problem, the robot needs to decide its grasp forces on each finger, based on the grasp matrix, surface normals of contact points and friction coefficient~\cite{dai2018synthesis}. From visual perception, the information we can extract is the positions and normals of potential contact points, where positions are linked to the grasp matrix and normals determine the orientations of friction cones. Based on this observation, we introduce a novel intermediate representation for multi-finger grasping, referred to as the Contact-centric Grasp Representation (CGR), which encapsulates contact information on the object's surface and possesses SE(3)-equivalent property.

Based on this representation, we present AnyDexGrasp, a novel methodology that can effectively learn dexterous grasping for different hands on a modest set of training objects. In this method, the multi-finger grasp detection problem that maps raw perception to grasp poses is divided into two steps. In the first step, we train a general representation model that maps single-view partial observations to contact-centric grasp representations. A large-scale dataset is annotated to train this model. After training, it can be applied to different hands without fine-tuning. In the second step, we map the contact-centric grasp representations to a set of grasp proposals through a hand-specific mapping, and then learn a hand-specific classifier to evaluate each grasp proposal. This classifier takes a contact-centric grasp representation and a grasp proposal as input and maps them to the probability of grasp success. The training data is collected by real-world trial and error.  We empirically observed that this mapping is significantly easier to learn, requiring merely hundreds of trial-and-error attempts. It dramatically reduces the cost of real-world learning and allows our approach to work for different types of robotic hands efficiently.

We evaluate the effectiveness of our method using three different robotic hands, each featuring three to five fingers. Our system is first trained on 144 objects, with approximately 2,000 to 8,000 grasp attempts, depending on the robotic hand. On a diverse set of 150 previously unseen objects, including deformable and adversarial items, our approach achieves an average grasp success rate ranging from 80\% to 98\% across different hands.  Notably, this performance is achieved in cluttered scenarios, demonstrating the effectiveness of our approach.

In addition we explore further reductions in training samples required for our grasp learning paradigm. We limit the training objects to 40 and reduce the grasp attempts to approximately 400 to 1,000 depending on the robotic hand. Even with this limited amount of training data, our system consistently achieves grasp success rates ranging from 75\% to 95\% during real-world testing. Notably, our experiments also highlight the potential for further reductions of training samples, with the ability to decrease the training object number to 30 and the total grasp attempts to 200 for a three-finger hand, without decreasing the grasp performance by a large margin.  Such learning efficiency allows robots to master visually guided grasping in a matter of hours in the real world, surpassing the learning efficiency of human infants.

We conduct a series of analyses to clarify why our two step learning method is so efficient. In the first step, we perform a geometry coverage analysis, showcasing that by scaling up data in the correct dimension, the local geometries on just 40 objects can effectively cover a wide range of unseen objects. This explains the generalization capabilities with a small number of training objects. In the second step, we provide various perspectives illustrating how our proposed contact-centric grasp representation serves as a robust state space for grasp decision, which allows the model to learn from just hundreds of real-world trial-and-error attempts.

This paper represents a significant step toward the efficient realization of dexterous robotic grasping, with the potential to revolutionize various applications, from advanced humanoid robots to prosthetic hands.