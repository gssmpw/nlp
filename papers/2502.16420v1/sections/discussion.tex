
\section{Discussion}

\subsection{Efficiency Analysis}
It is surprising to see that our grasp system can be learned for different hands so efficiently. Previous work for multi-finger grasping usually requires thousands of objects or millions of grasp samples~\cite {wang2023dexgraspnet, lum2024dextrahg,
singh2025dextrahrgb}. In the deep learning era, it seems to be an underlying rule that we need to train a robot system on as many objects as possible to have good generalization ability. However, the satisfactory performance of our system breaks this intuition. What are the key factors for our method to learn so efficiently and generalize so well? There are two aspects of learning efficiency in our system, the first is that we only need 40 objects and the second is that we only need hundreds of trials for each hand. Here we discuss how our method achieves efficiency in these two aspects.


\subsubsection*{Representation Model}
Representation plays a crucial role in our system for real-world learning, as it must map different geometries into a contact-centric grasp representation. How can the system, trained on only 40 objects, generalize to hundreds of unseen objects? We address this by conducting a geometry coverage analysis, revealing that \textbf{scaling up data along the right dimension} is key to improving the model's generalization ability.

Our representation model takes a scene point cloud as input and outputs contact-centric grasp representations (CGRs). To train this model, we need a dataset containing scene point clouds with annotated CGRs across various geometries. Since CGRs depend on local geometry, a representative dataset must include diverse local geometries to effectively train deep networks. While many researchers intuitively attempt to collect more training objects to achieve this, our geometry coverage analysis demonstrates that more objects do not necessarily lead to richer local geometries.

We begin by defining the local geometry used in our analysis. Specifically, the representation network operates in a partial observation scenario, where it infers contact positions and normals on unobserved surfaces based on the observed geometry. Although each normal and contact point is predicted independently, we consider the minimal continuous components of local patches, enclosed by the simplest form of grasping—an antipodal grasp—as a foundational element in our analysis for consistency.


\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{images/coverage.pdf}
    \caption{\textbf{Geometry coverage analysis.} \textbf{(A)}: The colored objects are our training objects and the gray objects are in the EGAD! object set. The surfaces highlighted in green and connected by a dotted line have similar local geometries. We see that although the training object and testing object have very different overall shapes, we can find local geometries on them that are pretty similar. \textbf{(B)}: The local geometry coverage curves on the testing set given different choices of scaling up the training set. The $x$-axis denotes the ID of each testing object, and the $y$-axis denotes the number of covered local geometries on each testing object. An example is given where the 1000-th testing object has around 250 covered local geometries. We only draw 3 of them for illustration.
    }
    \label{fig:coverage}
\end{figure}

Next, we define geometry coverage in the analysis. Given a training and testing object set, a local geometry on a test object is considered "covered" if it closely resembles a local geometry from the training object set. We define similarity by a chamfer distance smaller than 1mm, with examples illustrated in Figure~\ref{fig:coverage}A. For a training dataset, we can assess the diversity of local geometries by counting the number of covered geometries on test objects.

In practice, when constructing a training dataset, we need to generate labels for a fixed number of local geometries selected from the training objects, constrained by computational resources. There are two possible dimensions  along which to collect more local geometries: increase the number of training objects or increase the sample density on each training object. To assess which dimension is more effective for increasing local geometry diversity, our analysis is conducted as follows. We collect two training object sets: $\mathbb{S}$, with 40 objects, and $\mathbb{L}$, with 144 objects. For each object set, we sample 1 million and 4 million local geometries from each object on average, respectively (sampling details given in supplementary material). This combination results in four different training datasets. The testing object set for the coverage analysis is the EGAD! test set, which contains over 2000 complex, program-generated objects. We sample around 400 local geometries on each test object and evaluate if they are covered. Figure~\ref{fig:coverage}B shows the number of covered geometries on each test object, considering different training sets and sampling densities. 

Surprisingly, we found that increasing the number of training objects does not significantly increase the coverage rate on the test set. However, increasing the sampling density of local geometries per object leads to a dramatic increase in coverage—\textit{even when the total number of sampled geometries is similar to increasing the object count}. This result demonstrates that increasing sample density for each training object is far more impactful than increasing the number of objects.

Based on this analysis, we prioritize scaling up the label density of CGRs on each training object, rather than increasing the number of training objects, when constructing our dataset. By training on over a billion CGRs, our model has learned to map local geometries to grasp representations effectively, thereby enhancing its ability to generalize to novel objects.


\subsubsection*{Grasp Decision Model}
In the previous section, we discussed how our representation model can generalize well to novel scenes despite being trained on only 40 objects. Now, we turn to the grasp decision model and discuss why it can learn grasp success from just hundreds of trial-and-error attempts. Here, we highlight a few possible reasons.

First, the representation captures all the relevant information about force closure that can be extracted from vision. For a point-to-plane contact problem, the force-closure condition must satisfy the following criteria~\cite{dai2018synthesis}:
\begin{equation}
\begin{aligned}
     &Gf = 0, \\
     &GG^\top > \epsilon I_{6\times6},\\
     &f_i^\top n_i > \frac{1}{\sqrt{\mu^2+1}}|f_i|,
\label{eqn:forceclosure}
\end{aligned}
\end{equation}
where $f$ is the vector of contact forces acting at each contact point, $G$ is the grasp matrix determined by the positions of the contact points, and $n_i$ represents the surface normal at the $i$-th contact point. The latter two parameters are the only aspects that a vision model can estimate, and are generated by our representation model. The grasp decision model needs only to learn whether the forces $f$ exerted by the gripper for different grasp types can satisfy Equation~\eqref{eqn:forceclosure}, given a friction coefficient $\mu$. Although the friction coefficient is unknown, the model tends to learn an average behavior from the training set.

Second, the representation is compact. Instead of dealing with high-dimensional data like images or point clouds, we reduce the input to a 1D vector that represents the shape. This compactness simplifies the mapping from input to grasp quality, making it easier for the grasp decision model to learn.

\subsection{Method Positioning}

The evolution of visually guided dexterous grasping methodologies within robotics has developed two prominent paradigms: the 6D pose estimation paradigm and the end-to-end grasp learning paradigm. The former relies on the precise estimation of an object's 6D pose and then calculates the hand pose accordingly. It can transfer across different robotic hands easily, but requires prior knowledge of the object's model. On the other hand, the end-to-end grasp learning models do not require explicit object knowledge, yet the trained models lack transferability across different robotic hands.

Our proposed approach explores a middle ground between these two paradigms, which ombining the advantages of both. By developing a contact-centric grasp representation that encapsulates the scene's contact information, we eliminate the need for an object's model beforehand. The CGR preserves critical information pertinent to grasp quality, endowing our system with adaptability and applicability across different morphologies of robotic hands. Moreover, by eliminating the need for an accurate kinematic model, which was frequently used in previous work learned in simulation~\cite{xu2023unidexgrasp,wan2023unidexgrasp++}, our method is suitable for soft hand grasp learning.


\subsection{Integration with Tactile Sensors}
Future directions for research entail expanding the scope of the contact-centric grasp representation model to include a wider array of tactile and sensory information, enabling a more comprehensive understanding of object manipulation. Tactile sensors encapsulate rich information concerning contact positions and contact point normals, mirroring the fundamental attributes of our representation model. This alignment highlights the potential for our method to work well with tactile sensing technology. By leveraging this alignment, the incorporation of tactile sensors can augment the current representation, further refining the contact-centric information available for decision-making in our learning paradigm. 



