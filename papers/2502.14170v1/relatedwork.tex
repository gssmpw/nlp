\section{Related Work}
\label{sec:related_work}

Blockchain-based approaches to FL have garnered significant attention for their potential to enhance decentralization, transparency, and trust in decentralized learning systems. Our work aligns with the growing research at the intersection of blockchain and FL~\cite{issa2023blockchain}, focusing on leveraging blockchain to enhance scalability and fairness, particularly for resource-intensive applications like training LLMs. Below, we discuss some of the closest related works to the work presented in this paper.

% Blockchain-Based Decentralized Federated Learning

The Blockchain-Based Decentralized Federated Learning Framework with Committee Consensus (BFLC)~\cite{li2020blockchain} eliminates reliance on a central server by using blockchain for global model storage and local model update exchange. The committee consensus mechanism reduces computational overhead and mitigate malicious attacks. In contrast, our framework employs a hybrid incentive mechanism to ensure fairness and scalability. It addresses challenges such as the high computational demands of large-scale models in decentralized settings by leveraging off-chain aggregation alongside efficient on-chain operations.

% Privacy and Differential Privacy in Federated Learning

The Differentially Private Blockchain-Based Vertical Federated Learning (DP-BBVFL) framework~\cite{tran2024adifferentially} introduces differential privacy to protect embeddings stored on the blockchain, ensuring privacy in vertical FL scenarios. Unlike DP-BBVFL, which focuses on embedding aggregation in settings with disjoint feature spaces, our work targets horizontal FL, where clients share data with the same feature space but different sample distributions. By aggregating model updates with a hybrid incentive mechanism, our approach ensures fairness and promotes meaningful contributions across participants.

% Incentive Mechanisms for Reliable Federated Learning

Kang et al.~\cite{kang2019incentive} introduce a reputation-based worker selection scheme using a multiweight subjective logic model to evaluate reliability and trustworthiness. Their blockchain-based incentive mechanism integrates reputation with contract theory to motivate high-quality participation. While this approach focuses on trust and reputation, our framework emphasizes the scalability of FL, balancing alignment-based rewards and fairness checks, and enabling efficient training for large-scale models.

% Federated Learning in IoT Environments

The Blockchain and Federated Learning for Privacy-Preserved Data Sharing in Industrial IoT framework~\cite{lu2019blockchain} integrates FL into the consensus process of a permissioned blockchain, allowing computational resources used for consensus to contribute to model training. While this industrial IoT-focused approach addresses privacy and resource utilization, our framework is designed for FL, emphasizing scalability and fairness through hybrid incentives and off-chain processing, making it suitable for diverse applications beyond industrial settings.

% Decentralized Machine Learning and Incentives

DeepChain~\cite{weng2019deepchain} introduces a blockchain-integrated framework for distributed deep learning, employing a protocol-level incentive mechanism to enforce correct participant behavior and mitigate malicious attacks. Unlike DeepChain’s protocol-level integration, our work focuses on application-level hybrid incentives, addressing fairness and scalability in federated training for resource-intensive models.

% Collaborative AI and Public Blockchains
The Shareable Updatable Model (SUM) framework~\cite{harris2019decentralized,harris2020analysis} proposes a decentralized methodology for collaboratively building datasets and hosting models on public blockchains, employing financial and gamified incentives. While SUM focuses on public blockchains and gamified incentives for collaborative construction of models, our framework emphasizes federated techniques for training models, offering scalability and fairness in heterogeneous environments.

Finally, the Swarm Learning  framework~\cite{hp_swarm_learning} utilizes blockchain-based peer-to-peer networking for decentralized machine learning, ensuring privacy by keeping raw data localized and complying with regulations. While swarm learning achieves transparency and equitable collaboration, it lacks explicit incentive mechanisms to ensure meaningful contributions or penalize malicious behavior, a gap that our hybrid incentive mechanism addresses by combining alignment-based rewards and fairness checks.

By addressing the scalability challenges of FL and integrating a hybrid incentive mechanism, our framework advances the capabilities of blockchain-based FL for large-scale, resource-intensive applications, distinguishing itself from prior works focused on specific FL paradigms or application domains.

% \subsection{SUM Framework}
% The SUM framework, proposed by Microsoft Research, offers a blockchain-based system explicitly designed for decentralized collaborative AI with structured incentives. The architecture provides modularity, allowing several machine learning models to integrate with diverse incentive mechanisms. A notable feature is the \textit{self-assessment mechanism}, where data contributors must deposit funds alongside their submissions. Deposits are refunded if the submitted data aligns with the evolving model over time, introducing accountability for data quality ~\cite{microsoft_research_2023}.

% To stimulate regular activity, gamification strategies grant badges and points for verified contributions, while monetary reward mechanisms based on prediction markets provide financial incentives to improve model accuracy. This structured incentivization makes the SUM framework robust in managing fairness and encouraging high-quality participation ~\cite{microsoft_research_2023}.

% On the other hand, the SUM framework relies on external resources such as predefined test datasets and funding pools for monetary rewards, which may limit its applicability in resource-constrained scenarios. Nevertheless, it presents a comprehensive approach to incentivize meaningful contributions and maintain fairness in decentralized learning systems ~\cite{microsoft_research_2023}.

% \subsection{Comparison and Integration Potential}
% Swarm Learning and the SUM framework represent two complementary methods for decentralized collaboration in machine learning. SL offers superior privacy and decentralization but lacks explicit incentivization mechanisms ~\cite{hp_swarm_learning}. Conversely, the SUM framework excels in providing structured rewards and penalties, though it relies on external infrastructures for its incentivization mechanisms ~\cite{microsoft_research_2023}.

% Merging their strengths—SL's privacy-preserving operations with SUM's robust incentivization strategies—could yield a more comprehensive and efficient decentralized learning system. Such integration would address the deficiencies of each framework, ensuring trust, scalability, fairness, and efficiency. This aligns with the motivation for robust decentralized federated learning systems that leverage blockchain-enabled smart contracts to foster meaningful contributions and fairness in collaborative AI ~\cite{hp_swarm_learning, microsoft_research_2023}.