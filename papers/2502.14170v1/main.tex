% Venue: https://federated-learning.org/fl@fm-www-2025/
\documentclass[sigconf,nonacm]{acmart}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\copyrightyear{2025}
\acmYear{2025}
\setcopyright{acmlicensed}\acmConference[WWW Companion '25]{Companion
Proceedings of the ACM Web Conference 2025}{April 28-May 2, 2025}{Sydney,
NSW, Australia}
\acmBooktitle{Companion Proceedings of the ACM Web Conference 2025 (WWW
Companion '25), April 28-May 2, 2025, Sydney, NSW, Australia}
\acmDOI{10.1145/3701716.3717649}
\acmISBN{979-8-4007-1331-6/2025/04}


\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{cleveref}
\usepackage{multirow}
\usepackage{array}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
% \pagestyle{plain} 
% \settopmatter{printccs=true, printfolios=true} 

\usepackage{flushend}
\usepackage{balance}

% \acmConference[FL@FM-TheWebConf'25]{International Workshop on Federated Foundation Models for the Web}{April 29, 2025}{Sydney, Australia}

\acmYear{2025}
\acmISBN{}
\acmDOI{}

\title{Blockchain-based Framework for Scalable and Incentivized Federated Learning}
% \title{Decentralized Federated Learning via Blockchain Smart Contracts and Incentive Mechanisms}

\author{Bijun Wu}
\affiliation{%
  \institution{Rensselaer Polytechnic Institute}
  \city{Troy}
  \country{USA}
}
\email{wub8@rpi.edu}

\author{Oshani Seneviratne}
\orcid{0000-0001-8518-917X}
\affiliation{%
  \institution{Rensselaer Polytechnic Institute}
  \city{Troy}
  \state{NY}
  \country{USA}
}
\email{senevo@rpi.edu}

\renewcommand{\shortauthors}{Wu and Seneviratne}
\renewcommand{\shorttitle}{Scalable and Incentivized Federated Learning}

\begin{document}

\begin{abstract}

Federated Learning (FL) enables collaborative model training without sharing raw data, preserving privacy while harnessing distributed datasets. However, traditional FL systems often rely on centralized aggregating mechanisms, introducing trust issues, single points of failure, and limited mechanisms for incentivizing meaningful client contributions. These challenges are exacerbated as FL scales to train resource-intensive models, such as large language models (LLMs), requiring scalable, decentralized solutions.
This paper presents a blockchain-based FL framework that addresses these limitations by integrating smart contracts and a novel hybrid incentive mechanism. The framework automates critical FL tasks, including client registration, update validation, reward distribution, and maintaining a transparent global state. The hybrid incentive mechanism combines on-chain alignment-based rewards, off-chain fairness checks, and consistency multipliers to ensure fairness, transparency, and sustained engagement. We evaluate the framework through gas cost analysis, demonstrating its feasibility for different scales of federated learning scenarios.

% Traditional Federated Learning (FL) frameworks rely on centralized aggregation mechanism, which are vulnerable to single points of failure, privacy risks, and manipulation. We present an innovative decentralized FL system leveraging blockchain technology to replace these centralized mechanisms with blockchain-based smart contracts. These smart contracts manage the aggregation, validation, and incentivization of clients in a transparent and secure manner. Building upon the initial design, this paper introduces a hybrid incentive mechanism that integrates alignment-based rewards with periodic off-chain fairness checks to promote meaningful contributions, consistent participation, and fairness in a scalable manner.
\end{abstract}

% \begin{CCSXML}
% <ccs2012>
%    <concept>
%        <concept_id>10010147.10010178.10010213</concept_id>
%        <concept_desc>Computing methodologies~Distributed computing methodologies</concept_desc>
%        <concept_significance>500</concept_significance>
%    </concept>
%    <concept>
%        <concept_id>10002951.10003227.10003233.10010519</concept_id>
%        <concept_desc>Information systems~Data analytics</concept_desc>
%        <concept_significance>300</concept_significance>
%    </concept>
% </ccs2012>
% \end{CCSXML}

% \ccsdesc[500]{Computing methodologies~Distributed computing methodologies}
% \ccsdesc[300]{Information systems~Data analytics}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010178</concept_id>
       <concept_desc>Computing methodologies~Artificial intelligence</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010178.10010219</concept_id>
       <concept_desc>Computing methodologies~Distributed artificial intelligence</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10003752.10010070.10010071</concept_id>
       <concept_desc>Theory of computation~Machine learning theory</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010919</concept_id>
       <concept_desc>Computing methodologies~Distributed computing methodologies</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10002978</concept_id>
       <concept_desc>Security and privacy</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10010520.10010521.10010537.10010540</concept_id>
       <concept_desc>Computer systems organization~Peer-to-peer architectures</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10002951</concept_id>
       <concept_desc>Information systems</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010257.10010282</concept_id>
       <concept_desc>Computing methodologies~Learning settings</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010405</concept_id>
       <concept_desc>Applied computing</concept_desc>
       <concept_significance>100</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Artificial intelligence}
\ccsdesc[500]{Computing methodologies~Distributed artificial intelligence}
\ccsdesc[300]{Theory of computation~Machine learning theory}
\ccsdesc[500]{Computing methodologies~Distributed computing methodologies}
\ccsdesc[300]{Security and privacy}
\ccsdesc[500]{Computer systems organization~Peer-to-peer architectures}
\ccsdesc[300]{Information systems}
\ccsdesc[500]{Computing methodologies~Learning settings}
\ccsdesc[100]{Applied computing}

\keywords{Federated Learning, Blockchain, Incentive Mechanisms, Decentralization, Smart Contracts}

\maketitle


\section{Introduction}
\label{sec:introduction}

Federated Learning (FL)~\cite{mcmahan2017communication} has emerged as a transformative paradigm for distributed machine learning, enabling clients to collaboratively train models without sharing raw data. By preserving data privacy and supporting collaborative training, FL has shown promise for applications across diverse domains~\cite{yang2019federated}. However, traditional FL systems face critical limitations that hinder their scalability, reliability, and fairness~\cite{kairouz2021advances,li2020federated}.

First, most FL systems rely on centralized mechanisms to aggregate and validate contributions. This centralization introduces trust issues, as clients must rely on the centralized aggregator to act without bias, manipulation, or misuse of data~\cite{zhang2024}. Second, centralized architectures create single points of failure, making the system vulnerable to outages, attacks, and bottlenecks that compromise reliability~\cite{sani2024}. Finally, as FL systems scale, particularly with the growing adoption of foundation models, existing approaches lack robust incentive mechanisms to encourage meaningful contributions or penalize malicious or low-quality updates that could lead to fairness and performance degradation over time.

These challenges highlight the need for decentralized and transparent FL systems that enhance trust, resilience, and incentivization. 

Inspired by Swarm Learning~\cite{hp_swarm_learning}, a decentralized machine learning framework, our work draws on blockchain-based coordination and tamper-proof state synchronization. Swarm Learning replaces the centralized aggregation mechanism with decentralized nodes and integrates blockchain technology to ensure transparent and immutable synchronization. Trusted execution environments (TEEs) are used to secure data and model parameters. However, Swarm Learning lacks explicit incentive mechanisms to ensure meaningful contributions, limiting its applicability in competitive or heterogeneous environments.

\subsection*{Contributions}
We propose a blockchain-based FL framework that integrates a novel hybrid incentive mechanism. Our framework utilizes smart contracts~\cite{buterin2014next} to automate critical FL tasks, including client registration, update validation, reward distribution, and maintenance of a transparent global state. A hybrid incentive mechanism ensures fairness and scalability by combining:
\begin{enumerate}
    \item \textbf{On-Chain Alignment-Based Rewards:} Evaluate client contributions in real-time to promote high-quality updates.
    \item \textbf{Off-Chain Fairness Checks:} Leverage decentralized storage to ensure equitable reward distribution over time while minimizing blockchain costs.
    \item \textbf{Consistency Multipliers:} Reward sustained, high-quality participation across multiple rounds for long-term engagement.
\end{enumerate}

Our contributions address key challenges in trust, scalability, and incentivization, paving the way for scalable FL systems capable of handling resource-intensive applications. Notably, training Large Language Models (LLMs) demands substantial computational resources—a challenge that is further amplified in decentralized settings, where blockchain constraints such as limited on-chain capacity and high gas costs complicate resource management. Our framework is designed to mitigate these issues by efficiently balancing off-chain and on-chain operations.

\medskip

The rest of the paper is organized as follows. In \Cref{sec:related_work}, we review related work in blockchain-based FL, highlighting existing gaps and complementary approaches. \Cref{sec:architecture} describes the proposed system architecture, focusing on the integration of blockchain and FL. \Cref{sec:incentive_mechanism} details the hybrid incentive mechanism design, including its components and operational flow. In \Cref{sec:evaluation}, we present empirical evaluations of gas consumption under varying model parameter sizes, demonstrating the scalability and efficiency of the framework. \Cref{sec:discussion} outlines limitations of the current work and several future directions. Finally, \Cref{sec:conclusion} concludes the paper with a short summary.


% \subsection{Motivation}
% Federated Learning (FL)~\cite{mcmahan2017communication} has emerged as a promising paradigm for distributed machine learning, allowing clients to collaboratively train models without sharing raw data. However, traditional FL systems face significant limitations:
% \begin{itemize}
%     \item \textbf{Trust Issues:} Centralized servers are often required to aggregate and validate contributions, necessitating trust from clients. This centralization introduces potential risks of bias, manipulation, or misuse of data~\cite{zhang2024}.
%     \item \textbf{Single Points of Failure:} The reliance on a single aggregation server creates a bottleneck, leaving the system vulnerable to outages, attacks, and other failures~\cite{sani2024}.
%     \item \textbf{Inefficient Incentives:} FL systems lack robust mechanisms to encourage meaningful contributions or penalize malicious or low-quality updates. This undermines fairness and system performance over time~\cite{microsoft_research_2023}.
% \end{itemize}

% These challenges highlight the need for decentralized and transparent FL systems that ensure trust, resilience, and robust incentivization mechanisms.

% \subsection{Inspired by Swarm Learning}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\linewidth]{figures/swarm-learning.png}
%     \caption{Architecture of Swarm Learning. Clients collaborate through a private blockchain, which manages synchronization, privacy, and security.}
%     \Description{This figure shows the architecture of Swarm Learning. It highlights multiple local nodes training machine learning models locally while interacting with a private blockchain for secure synchronization and parameter merging.}
%     \label{fig:swarm_learning_architecture}
% \end{figure}

% This work draws inspiration from Swarm Learning, a decentralized machine learning framework developed by Hewlett-Packard (HPE)~\cite{hp_swarm_learning}. Swarm Learning eliminates the need for centralized aggregation by integrating blockchain technology to synchronize global states and ensure tamper-proof coordination. Its key components include:
% \begin{itemize}
%     \item \textbf{Decentralized Nodes:} Each node independently trains models on local data and shares parameters securely within a network.
%     \item \textbf{Blockchain Integration:} Blockchain technology replaces centralized servers, enabling transparent and immutable synchronization across participants.
%     \item \textbf{Trusted Execution Environments (TEEs):} TEEs provide a secure environment for computation, ensuring data and model parameters remain private.
% \end{itemize}

% \subsection{Proposed Approach}
% To address the challenges in traditional federated learning (FL) systems, this paper introduces a blockchain-based smart contract controller with an integrated incentive mechanism. The proposed approach removes the reliance on centralized servers, offering a decentralized and transparent solution for federated learning. The key features of the system include:

% \begin{itemize}
%     \item \textbf{Smart Contract Controller:} Blockchain-based smart contracts serve as the core of the system, automating client registration, update validation, reward distribution, and maintaining a transparent global state.
%     \item \textbf{Hybrid Incentive Mechanism:} A novel incentive mechanism combines:
%         \begin{itemize}
%             \item On-chain alignment-based rewards to evaluate the quality of client contributions in real-time.
%             \item Off-chain fairness checks leveraging IPFS to ensure equitable reward distribution over time without incurring excessive blockchain costs.
%             \item Consistency multipliers to reward sustained high-quality participation across multiple rounds ~\cite{zhao2023}.
%         \end{itemize}
%     \item \textbf{Efficiency and Scalability:} The system employs efficient aggregation strategies like FedAvg to reduce computational overhead while ensuring model convergence. Large model parameters are offloaded to decentralized storage solutions such as IPFS, making the system scalable for large language models (LLMs) and other resource-intensive applications~\cite{ye2024}.
%     \item \textbf{Transparency and Security:} Blockchain provides an immutable ledger for recording contributions and rewards, ensuring fairness and enabling participants to verify outcomes independently~\cite{hutson2024}.
% \end{itemize}

% This approach balances computational efficiency, scalability, and fairness, making it suitable for diverse federated learning scenarios. By addressing critical gaps in trust, reward mechanisms, and scalability, the system advances the capabilities of decentralized FL systems.

\section{Related Work}
\label{sec:related_work}

Blockchain-based approaches to FL have garnered significant attention for their potential to enhance decentralization, transparency, and trust in decentralized learning systems. Our work aligns with the growing research at the intersection of blockchain and FL~\cite{issa2023blockchain}, focusing on leveraging blockchain to enhance scalability and fairness, particularly for resource-intensive applications like training LLMs. Below, we discuss some of the closest related works to the work presented in this paper.

% Blockchain-Based Decentralized Federated Learning

The Blockchain-Based Decentralized Federated Learning Framework with Committee Consensus (BFLC)~\cite{li2020blockchain} eliminates reliance on a central server by using blockchain for global model storage and local model update exchange. The committee consensus mechanism reduces computational overhead and mitigate malicious attacks. In contrast, our framework employs a hybrid incentive mechanism to ensure fairness and scalability. It addresses challenges such as the high computational demands of large-scale models in decentralized settings by leveraging off-chain aggregation alongside efficient on-chain operations.

% Privacy and Differential Privacy in Federated Learning

The Differentially Private Blockchain-Based Vertical Federated Learning (DP-BBVFL) framework~\cite{tran2024adifferentially} introduces differential privacy to protect embeddings stored on the blockchain, ensuring privacy in vertical FL scenarios. Unlike DP-BBVFL, which focuses on embedding aggregation in settings with disjoint feature spaces, our work targets horizontal FL, where clients share data with the same feature space but different sample distributions. By aggregating model updates with a hybrid incentive mechanism, our approach ensures fairness and promotes meaningful contributions across participants.

% Incentive Mechanisms for Reliable Federated Learning

Kang et al.~\cite{kang2019incentive} introduce a reputation-based worker selection scheme using a multiweight subjective logic model to evaluate reliability and trustworthiness. Their blockchain-based incentive mechanism integrates reputation with contract theory to motivate high-quality participation. While this approach focuses on trust and reputation, our framework emphasizes the scalability of FL, balancing alignment-based rewards and fairness checks, and enabling efficient training for large-scale models.

% Federated Learning in IoT Environments

The Blockchain and Federated Learning for Privacy-Preserved Data Sharing in Industrial IoT framework~\cite{lu2019blockchain} integrates FL into the consensus process of a permissioned blockchain, allowing computational resources used for consensus to contribute to model training. While this industrial IoT-focused approach addresses privacy and resource utilization, our framework is designed for FL, emphasizing scalability and fairness through hybrid incentives and off-chain processing, making it suitable for diverse applications beyond industrial settings.

% Decentralized Machine Learning and Incentives

DeepChain~\cite{weng2019deepchain} introduces a blockchain-integrated framework for distributed deep learning, employing a protocol-level incentive mechanism to enforce correct participant behavior and mitigate malicious attacks. Unlike DeepChain’s protocol-level integration, our work focuses on application-level hybrid incentives, addressing fairness and scalability in federated training for resource-intensive models.

% Collaborative AI and Public Blockchains
The Shareable Updatable Model (SUM) framework~\cite{harris2019decentralized,harris2020analysis} proposes a decentralized methodology for collaboratively building datasets and hosting models on public blockchains, employing financial and gamified incentives. While SUM focuses on public blockchains and gamified incentives for collaborative construction of models, our framework emphasizes federated techniques for training models, offering scalability and fairness in heterogeneous environments.

Finally, the Swarm Learning  framework~\cite{hp_swarm_learning} utilizes blockchain-based peer-to-peer networking for decentralized machine learning, ensuring privacy by keeping raw data localized and complying with regulations. While swarm learning achieves transparency and equitable collaboration, it lacks explicit incentive mechanisms to ensure meaningful contributions or penalize malicious behavior, a gap that our hybrid incentive mechanism addresses by combining alignment-based rewards and fairness checks.

By addressing the scalability challenges of FL and integrating a hybrid incentive mechanism, our framework advances the capabilities of blockchain-based FL for large-scale, resource-intensive applications, distinguishing itself from prior works focused on specific FL paradigms or application domains.

% \subsection{SUM Framework}
% The SUM framework, proposed by Microsoft Research, offers a blockchain-based system explicitly designed for decentralized collaborative AI with structured incentives. The architecture provides modularity, allowing several machine learning models to integrate with diverse incentive mechanisms. A notable feature is the \textit{self-assessment mechanism}, where data contributors must deposit funds alongside their submissions. Deposits are refunded if the submitted data aligns with the evolving model over time, introducing accountability for data quality ~\cite{microsoft_research_2023}.

% To stimulate regular activity, gamification strategies grant badges and points for verified contributions, while monetary reward mechanisms based on prediction markets provide financial incentives to improve model accuracy. This structured incentivization makes the SUM framework robust in managing fairness and encouraging high-quality participation ~\cite{microsoft_research_2023}.

% On the other hand, the SUM framework relies on external resources such as predefined test datasets and funding pools for monetary rewards, which may limit its applicability in resource-constrained scenarios. Nevertheless, it presents a comprehensive approach to incentivize meaningful contributions and maintain fairness in decentralized learning systems ~\cite{microsoft_research_2023}.

% \subsection{Comparison and Integration Potential}
% Swarm Learning and the SUM framework represent two complementary methods for decentralized collaboration in machine learning. SL offers superior privacy and decentralization but lacks explicit incentivization mechanisms ~\cite{hp_swarm_learning}. Conversely, the SUM framework excels in providing structured rewards and penalties, though it relies on external infrastructures for its incentivization mechanisms ~\cite{microsoft_research_2023}.

% Merging their strengths—SL's privacy-preserving operations with SUM's robust incentivization strategies—could yield a more comprehensive and efficient decentralized learning system. Such integration would address the deficiencies of each framework, ensuring trust, scalability, fairness, and efficiency. This aligns with the motivation for robust decentralized federated learning systems that leverage blockchain-enabled smart contracts to foster meaningful contributions and fairness in collaborative AI ~\cite{hp_swarm_learning, microsoft_research_2023}.

\section{System Architecture}
\label{sec:architecture}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/system-architecture.png}
    \Description{The figure illustrates the interaction between clients performing local training, the smart contract for registration, aggregation, validation, and incentives, the private Ethereum-based blockchain network for disseminating model update results and eventually the rewards to the clients participating in the training process, and the off-chain storage for fairness checks and alignment score computations.}
    \caption{System Architecture.}
    \label{fig:system_architecture}
\end{figure}

The proposed system architecture integrates blockchain technology with FL to create a decentralized and transparent model update framework. 
\Cref{fig:system_architecture} illustrates the interactions within the system, including clients performing local training, the smart contract managing registration, aggregation, validation, and incentives, and the private Ethereum-based blockchain network disseminating model updates and distributing rewards. Additionally, the framework leverages the Interplanetary File System (IPFS) for storing the alignment score computations for efficient fairness checks to ensure transparency and equitable participation in the training process.

The architecture comprises three interdependent components that together facilitate decentralized training, validation, and incentive distribution:

\begin{itemize}
\item \textbf{Private Blockchain Network:}
A private Ethereum-based blockchain forms the backbone of the system, ensuring immutable and transparent recording of critical data such as client contributions, rewards, and fairness evaluations. We employ a private blockchain as we envision such collaborative training taking place in a consortium-based environment, where participants belong to a trusted group. 
Deploying the system on a private blockchain ensures controlled access, reduces transaction fees, and minimizes latency compared to public blockchain networks.
However, the proposed methodology is flexible and can also be adapted for public, permissionless blockchain settings if required. 

\item \textbf{Smart Contracts:}  
Smart contracts automate key processes, ensuring trustless interactions between participants. They handle the following tasks:
\begin{itemize}
    \item \textbf{Registration and Staking:} Clients register on the blockchain and stake tokens as a commitment to meaningful participation.
    \item \textbf{Update Validation:} Submitted updates are validated for alignment with the global model, filtering out malicious or low-quality contributions. This includes \emph{Off-Chain Fairness Checks}, which aggregate contributions over multiple rounds, are performed off-chain to reduce blockchain computational overhead. Only the final fairness results are stored on-chain for transparency.
    \item \textbf{Aggregation:} The validated updates are aggregated using an efficient strategy to update the global model. We leverage the FedAvg algorithm~\cite{mcmahan2017communication}, which ensures that aggregation is computationally lightweight, reducing blockchain gas costs while maintaining model convergence.
    \item \textbf{Reward Distribution:} Rewards are allocated based on the alignment of updates, fairness evaluations, and participation consistency.
\end{itemize}

\item \textbf{Clients:}  
Participating clients perform local training on private datasets. They submit gradient updates to the smart contract while preserving data privacy, contributing to the global model without exposing sensitive information.

\end{itemize}

% By replacing the traditional centralized aggregation mechanism with blockchain-based smart contracts, the architecture addresses key challenges such as trust, fairness, and scalability. 

% \section{System Architecture}

% The architecture as seen in \Cref{fig:system_architecture} leverages blockchain technology to securely and efficiently implement decentralized federated learning. Below, we outline the core components and their interactions.

% \subsection{Core Components}
% The system comprises three main components that work together to facilitate training, validation, and reward distribution:

% \begin{itemize}
%     \item \textbf{Blockchain Network:}
%     A private blockchain is used to host the smart contract, ensuring secure and immutable storage for critical data such as client contributions, rewards, and fairness checks. The private nature of the blockchain reduces transaction fees and network latency compared to public blockchains.

%     \item \textbf{Smart Contracts:}
%     Smart contracts automate key federated learning tasks, including:
%     \begin{itemize}
%         \item Registration and Staking: Clients register to participate and stake tokens as a commitment to meaningful contributions.
%         \item Update Validation: Validates client updates to filter out malicious or low-quality contributions.
%         \item Aggregation: Combines client updates using a simple and efficient averaging strategy.
%         \item Reward Distribution: Allocates rewards based on alignment scores, periodic fairness checks, and consistency bonuses.
%     \end{itemize}

%     \item \textbf{Clients:}
%     Participating devices train models locally on private data and submit updates (gradients) to the blockchain. These devices, which can include IoT devices and smartphones, maintain data privacy while contributing to the global model.
% \end{itemize}

% \subsection{System Workflow}
% The workflow consists of the following steps:

% \begin{enumerate}[label=\arabic*.]
%     \item \textbf{Registration and Staking:}
%     Clients register on the blockchain and stake tokens to discourage malicious participation.

%     \item \textbf{Local Training:}
%     Each client trains a model locally on its private dataset using the global model as the initial state.

%     \item \textbf{Update Submission:}
%     Clients compute gradient updates and submit them to the smart contract, along with metadata (e.g., the size of their dataset).

%     \item \textbf{Validation:}
%     The smart contract verifies the quality of submitted updates by checking their alignment with the global model.

%     \item \textbf{Aggregation:}
%     The system aggregates updates using the \textbf{Federated Averaging (FedAvg)} strategy:
%     \[
%     \mathbf{g}_{\text{global}} = \sum_{i=1}^{N} \frac{n_i}{N} \mathbf{g}_i
%     \]
%     Here:
%     \begin{itemize}
%         \item $\mathbf{g}_i$: Gradient update from client $i$.
%         \item $n_i$: Number of data samples contributed by client $i$.
%         \item $N$: Total number of samples across all clients.
%     \end{itemize}
%     FedAvg ensures efficient aggregation and reduces computational complexity, which helps lower gas costs.

%     \item \textbf{Reward Distribution:}
%     The smart contract calculates alignment scores for each client and distributes rewards. Periodic fairness checks and consistency multipliers ensure long-term equity in the system.
% \end{enumerate}

% \subsection{Design Optimizations}
% Several design choices are made to enhance the system’s scalability and efficiency:

% \begin{itemize}
%     \item \textbf{FedAvg Aggregation:}
%     By using a weighted averaging approach, the system avoids computationally expensive operations on-chain, reducing gas costs significantly.

%     \item \textbf{Off-Chain Fairness Checks:}
%     Periodic fairness checks are computed off-chain, minimizing the blockchain’s computational burden. Only the results are stored on-chain for transparency.

%     \item \textbf{Private Blockchain:}
%     Deploying the system on a private blockchain reduces transaction fees and network congestion while maintaining security.
% \end{itemize}

% \subsection{Scalability and Privacy}
% The system is designed to address the needs of large-scale federated learning networks while protecting client privacy:
% \begin{itemize}
%     \item \textbf{Scalability:}
%     The lightweight nature of FedAvg and off-chain computations ensures that the system can scale to thousands of participants without performance degradation.

%     \item \textbf{Privacy:}
%     Clients’ raw data remains on their local devices, complying with privacy regulations such as GDPR. The system only exchanges gradient updates, ensuring sensitive data is never exposed.
% \end{itemize}



\section{Hybrid Incentive Mechanism Design}
\label{sec:incentive_mechanism}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/hybrid-incentive-mechanism.png}
    \Description{This figure depicts the hybrid incentive mechanism design for FL. It highlights the allocation of rewards through alignment-based rewards, periodic fairness checks, and consistency multipliers.}
    \caption{Allocation of Rewards.}
    \label{fig:hybrid_incentive_mechanism}
\end{figure}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/sequence-diagram.png}
    \Description{This figure shows a sequence diagram for the hybrid incentive mechanism. It details the flow of contributions, alignment-based rewards performed every round, and periodic fairness and consistency checks performed every N rounds.}
    \caption{Sequence Diagram for Hybrid Incentive Mechanism Design.}
    \label{fig:hybrid_incentive_sequence}
\end{figure*}

\begin{algorithm}
\caption{Hybrid Incentive Mechanism}
\label{alg:hybrid_incentive_mechanism}
\KwIn{Client updates \( \mathbf{g}_i \) for \( i \in \{1, 2, \dots, N\} \)}
\KwOut{Reward distribution for each client}

\SetAlgoLined
\textbf{Initialization:} Deploy smart contract; initialize global model \( \mathbf{g}_{\text{global}} \).

\ForEach{round $r$}{
    \textbf{Step 1: Update Submission:} \\
    \Indp Clients submit their gradient updates \( \mathbf{g}_i \) and metadata (e.g., \( n_i \)) to the smart contract.\\
    \Indm
    \textbf{Step 2: Alignment-Based Rewards:} \\
    \Indp Compute alignment scores \( S_i \) for each client \( i \) using:
    \[
    S_i = \left( \mathbf{g}_i \cdot \mathbf{g}_{\text{global}} \right) \cdot \frac{n_i}{N}
    \]
    Distribute rewards to clients with positive alignment scores.\\
    \Indm
    \textbf{Step 3: Model Aggregation:} \\
    \Indp Update the global model \( \mathbf{g}_{\text{global}} \) using FedAvg:
    \[
    \mathbf{g}_{\text{global}} = \sum_{i=1}^{N} \frac{n_i}{N} \mathbf{g}_i
    \]
    \Indm
}

\If{round $r$ is a multiple of fairness interval $N$}{
    \textbf{Step 4: Fairness Evaluation:} \\
    \Indp Compute cumulative scores \( C_i \) for each client \( i \):
    \[
    C_i = \sum_{r=1}^{R} S_{i,r}
    \]
    Store \( C_i \) values on IPFS and record the CID and integrity hash on the blockchain.\\
    \Indm
}

\textbf{Step 5: Reward Adjustment:} \\
\Indp Compute final rewards \( \text{Reward}_i \) for each client \( i \):
\[
\text{Reward}_i = S_i \cdot (1 + \alpha \cdot C)
\]
Distribute adjusted rewards to clients.\\
\Indm
\end{algorithm}

The hybrid incentive mechanism integrates three complementary components to address fairness, efficiency, and scalability challenges in decentralized FL systems as shown in \Cref{fig:hybrid_incentive_mechanism} and further illustrated in Algorithm \ref{alg:hybrid_incentive_mechanism}. 
These components ensure equitable rewards while promoting consistent and meaningful participation across multiple training rounds. 
\Cref{fig:hybrid_incentive_sequence} details the flow of contributions where the alignment-based rewards performed every round and periodic fairness and consistency checks performed every N rounds.

% \begin{itemize}
%     \item \textbf{Alignment-Based Rewards (On-Chain):} Rewards are calculated each round based on the alignment of a client’s updates with the aggregated global model direction, incentivizing high-quality contributions.
%     \item \textbf{Periodic Fairness Checks (Off-Chain):} Contributions are aggregated over multiple rounds to ensure proportional reward distribution, promoting long-term fairness without imposing excessive blockchain costs.
%     \item \textbf{Consistency Multipliers (On-Chain):} Sustained participation and consistently meaningful contributions are incentivized through additional reward adjustments, encouraging reliable client engagement.
% \end{itemize}

\subsection{Alignment-Based Rewards}
At the core of the incentive mechanism is the alignment-based reward system, which operates entirely on-chain. By leveraging smart contracts, the system ensures transparency and automation in evaluating client contributions. For each training round, the following steps are performed:
\begin{itemize}
    \item \textbf{Submission and Validation:} Clients submit updates (gradients) which are validated to filter out malicious or low-quality contributions.
    \item \textbf{Alignment Score Calculation:} Each client’s update is assessed based on its alignment with the aggregated global model direction.
\end{itemize}

The alignment score for client \(i\) is computed as:
\[
S_{i} = \left(\mathbf{g}_i \cdot \mathbf{g}_{\text{global}}\right) \cdot \frac{n_i}{N}
\]
where \(\mathbf{g}_i\) is the gradient submitted by client \(i\), \(\mathbf{g}_{\text{global}}\) is the aggregated gradient from all client updates, \(n_i\) is the number of data samples contributed by client \(i\), and \(N\) is the total number of data samples across all clients.

Using Shapley values~\cite{shapley1951notes}, we compute the contribution of client $i$ by evaluating their impact on the improvement of the global model across all possible subsets of clients. 
The Shapley value accounts for the improvement in model performance when client $i$ participates in a subset of clients, and the fairness of contributions, particularly for clients with smaller datasets or specialized data that might disproportionately benefit the global model.
Therefore, this adjustment ensures that rewards are distributed not only based on dataset size and alignment but also on the strategic importance of each client’s contribution to the overall training process. 

This above methodology ensures that clients are rewarded proportionally based on both the quality of their contributions and the scale of their data involvement. Misaligned or malicious updates are either penalized or ignored.
% 
The alignment-based rewards are implemented via Solidity smart contracts, incorporating the following functionalities:
\begin{itemize}
    \item \textbf{Event Tracking:} Real-time updates on alignment scores through an \texttt{AlignmentScoresUpdated} event.
    \item \textbf{Reward Allocation:} Automated reward distribution to clients with positive alignment scores.
    \item \textbf{Gas Efficiency:} Computations are optimized to be lightweight to minimize on-chain resource usage.
\end{itemize}

\subsection{Periodic Fairness Checks via Cumulative Scoring}
To complement the short-term focus of alignment-based rewards, periodic fairness checks are conducted off-chain to ensure equitable distribution over longer periods. These checks operate as follows:
\begin{enumerate}[label=\arabic*.]
    \item \textbf{Cumulative Contribution Calculation:} Over \(R\) rounds, each client’s cumulative score \(C_i\) is computed as:
    \[
    C_i = \sum_{r=1}^{R} S_{i,r}
    \]
    where \(S_{i,r}\) is the alignment score for client \(i\) in round \(r\).
    
    \item \textbf{Data Storage and Transparency:} Cumulative scores are stored on the InterPlanetary File System (IPFS) for verifiability. The corresponding Content Identifier (CID) is recorded on-chain.
    
    \item \textbf{Integrity Validation:} A Keccak-256 hash of client contributions is generated for verification:
    \[
    H = \text{Keccak256}(C_i \ \forall i \in N)
    \]
    
    \item \textbf{On-Chain Updates:} The CID, cumulative scores, and integrity hash are updated on-chain via the smart contracts, as a tamper-proof record of fairness evaluations.
\end{enumerate}

The fairness checks bring several benefits:
\begin{itemize}
    \item \textbf{Transparency:} Verifiable cumulative data is accessible to all participants via IPFS.
    \item \textbf{Efficiency:} Off-chain computations reduce the blockchain’s computational load.
    \item \textbf{Fairness:} Aggregated rewards mitigate short-term biases, ensuring equitable treatment over time.
\end{itemize}

\subsection{Consistency Multipliers}
Consistency multipliers encourage sustained and meaningful participation by adjusting rewards based on client engagement across multiple rounds. The final reward for client \(i\) is calculated as:
\[
\text{Reward}_{i} = S_i \cdot (1 + \alpha \cdot C)
\]
where \(S_i\) is the alignment-based score of client \(i\), \(\alpha\) is the scaling factor for the consistency multiplier, and \(C\) is the proportion of rounds in which client \(i\) participated.

This formulation ensures that clients who consistently contribute high-quality updates are appropriately rewarded for long-term engagement.


% \section{Hybrid Incentive Mechanism Design}

% \subsection{Overview}
% The hybrid incentive mechanism integrates three components to ensure fairness, efficiency, and scalability:
% \begin{itemize}
%     \item \textbf{Alignment-Based Rewards (On-Chain):} Rewards are calculated each round based on the alignment of a client’s updates with the aggregated model direction.
%     \item \textbf{Periodic Fairness Checks (Off-Chain):} Aggregated contributions over several rounds are used to determine proportional rewards in periodic intervals.
%     \item \textbf{Consistency Multipliers (On-Chain):} Clients are incentivized for sustained participation and consistently meaningful contributions.
% \end{itemize}

% \subsection{Main Reward System: Alignment-Based Rewards}
% The alignment-based reward system is managed entirely on-chain, leveraging smart contracts for transparency and automation. For each round, the smart contract processes:
% \begin{itemize}
%     \item Client-submitted updates, validated for quality.
%     \item Calculation of alignment scores for each client based on the gradient similarity with the aggregated global model.
% \end{itemize}

% The alignment score for client $i$ is computed as:
% \[
% S_{i} = \left(\mathbf{g}_i \cdot \mathbf{g}_{\text{global}}\right) \cdot \frac{n_i}{N}
% \]
% where:
% \begin{itemize}
%     \item $\mathbf{g}_i$: Gradient submitted by client $i$.
%     \item $\mathbf{g}_{\text{global}}$: Aggregated gradient of all client updates.
%     \item $n_i$: Number of data samples contributed by client $i$.
%     \item $N$: Total number of samples across all clients.
% \end{itemize}

% Clients with high alignment scores are rewarded proportionally, while misaligned contributions are either penalized or ignored.

% \subsubsection*{Implementation Details}
% The Solidity smart contract tracks the following:
% \begin{itemize}
%     \item Events: For real-time tracking of alignment scores using the `AlignmentScoresUpdated` event.
%     \item Reward Allocation: Rewards are distributed to clients with positive alignment scores directly on-chain, ensuring transparency.
%     \item Gas Efficiency: By focusing only on alignment-based metrics per round, the computation remains lightweight and scalable.
% \end{itemize}

% \subsection{Periodic Fairness Checks: Cumulative Scoring}
% While alignment-based rewards handle short-term contributions, fairness checks ensure that clients are rewarded proportionally over time. These checks are performed every $N$ rounds to reduce computational overhead. The process includes:

% \begin{enumerate}[label=\arabic*.]
%     \item \textbf{Cumulative Contributions:} For each client $i$, cumulative scores $C_i$ are calculated as:
%     \[
%     C_i = \sum_{r=1}^{R} S_{i,r}
%     \]
%     where $S_{i,r}$ is the alignment score of client $i$ in round $r$ and $R$ is the total number of rounds considered.
    
%     \item \textbf{Data Storage on IPFS:} The Python script saves cumulative scores to IPFS for verifiability. The resulting Content Identifier (CID) is recorded on-chain for transparency.
    
%     \item \textbf{Integrity Hashing:} To ensure data integrity, a Keccak-256 hash of client data and scores is computed:
%     \[
%     H = \text{Keccak256}(\text{JSON}(C_i \ \forall i \in N))
%     \]
    
%     \item \textbf{On-Chain Updates:} The CID, cumulative scores, and integrity hash are submitted to the smart contract via the `updateFairnessData` function, ensuring rewards reflect aggregated contributions.
% \end{enumerate}

% \subsubsection*{Advantages}
% \begin{itemize}
%     \item Transparency: All cumulative data is stored on IPFS, allowing clients to verify fairness.
%     \item Efficiency: Off-chain computation reduces blockchain costs.
%     \item Fairness: Rewards are proportional to long-term contributions, addressing potential imbalances in alignment-based rewards.
% \end{itemize}

% \subsection{Consistency Multipliers}
% Consistency multipliers encourage sustained participation by rewarding clients who contribute meaningfully across multiple rounds. The final reward for client $i$ is computed as:
% \[
% \text{Reward}_{i} = S_i \cdot (1 + \alpha \cdot C)
% \]
% where:
% \begin{itemize}
%     \item $S_i$: Alignment-based score of client $i$.
%     \item $\alpha$: Scaling factor for the consistency multiplier.
%     \item $C$: Fraction of rounds in which client $i$ participated.
% \end{itemize}

% \subsection{Implementation Workflow}
% Figure~\ref{fig:hybrid_incentive_mechanism} illustrates the hybrid incentive mechanism design, showing the flow from client contributions to reward distribution:
% \begin{enumerate}[label=\arabic*.]
%     \item Clients submit updates, and alignment scores are calculated on-chain.
%     \item Every $N$ rounds, fairness checks are triggered, and cumulative scores are computed off-chain.
%     \item Fairness data is saved on IPFS, and the results are updated on-chain via smart contracts.
%     \item Rewards are distributed based on alignment, fairness adjustments, and consistency bonuses.
% \end{enumerate}



\begin{table*}[t]
\centering
\caption{Gas Costs by Parameter Size}
\label{tab:gas_costs}
\begin{tabular}{|p{0.05\textwidth}|>{\raggedleft\arraybackslash}p{0.15\textwidth}|>{\raggedleft\arraybackslash}p{0.15\textwidth}|>{\raggedleft\arraybackslash}p{0.15\textwidth}|>{\raggedleft\arraybackslash}p{0.15\textwidth}|>{\raggedleft\arraybackslash}p{0.15\textwidth}|}
\hline
\textbf{Param Size} & \textbf{Registration \& Staking (per client)} & \textbf{Submit Model (per client)} & \textbf{Aggregate Models} & \textbf{Update Validation} & \textbf{Distribute Rewards} \\ \hline
10                  & 45,373                                          & 393,262                                & 499,660                       & 512,769                        & 219,961                         \\ \hline
100                 & 45,373                                          & 2,403,817                              & 3,891,311                     & 2,153,970                      & 219,961                         \\ \hline
1,000               & 45,373                                          & 22,866,722                             & 37,893,125                    & 18,609,485                     & 219,961                         \\ \hline
10,000              & 45,373                                          & 229,065,242                            & 386,438,410                   & 187,515,227                    & 219,961                         \\ \hline
100,000             & 45,373                                          & 2,447,670,138                          & 4,724,606,105                 & 2,311,631,243                  & 219,961                         \\ \hline
\end{tabular}
\end{table*}


% \section{Evaluation}
% \label{sec:evaluation}

% In decentralized systems like blockchain-based FL, gas consumption is a critical metric that quantifies the computational and storage costs of executing smart contract functions. Gas costs directly impact the economic feasibility and scalability of the system, particularly when dealing with large-scale models and a high number of participants. 
% In this evaluation, we simulate the execution of key smart contract functions—such as registration, staking, model submission, aggregation, update validation, and reward distribution—under varying model parameter sizes. This provides insights into the system’s efficiency and its suitability for resource-intensive tasks like training foundation models in federated settings.

% The relationship between gas units and actual dollar costs depends on the gas price (in gwei) and the current exchange rate of ETH to USD. The total cost is calculated as 
% \(\text{Cost in USD} = \text{Gas Units} \times \text{Gas Price} \times \text{ETH to USD Exchange Rate}\). Gas units also correlate with computational time, as they quantify the complexity of executing smart contract operations; higher gas units typically indicate longer computation times due to more resource-intensive tasks. However, since this evaluation is conducted in a simulation environment using the "Foundry" tool\footnote{https://book.getfoundry.sh} to deploy a private Ethereum blockchain, we only report gas units. These values provide a close estimate of the computational costs but may differ from real deployments on the Ethereum mainnet due to variations in network congestion, gas prices, and token valuation. In practice, the likely deployment path for this framework would involve a private consortium blockchain, where economic costs are less significant compared to computational time, which becomes the primary factor for evaluating feasibility.

% The following deployment metrics establish a baseline for understanding the overhead of initializing the smart contract. \begin{itemize}
%     \item Deployment Cost: 2,371,244 gas
%     \item Deployment Size: 10,667 bytes
% \end{itemize}

% The gas consumption results in \Cref{tab:gas_costs} provide empirical evidence of the scalability challenges associated with aggregating large models entirely on-chain in blockchain-based FL settings. While operations for smaller models (e.g., 10 or 100 parameters) incur manageable gas costs, the gas requirements grow exponentially with larger parameter sizes for \emph{Submit Models}, \emph{Aggregate Models}, and \emph{Update Validation}. For instance, models with 100,000 parameters incur gas costs that are prohibitively expensive, demonstrating the impracticality of such operations in real-world deployments.
% However, \emph{Registration \& Staking} and \emph{Distribute Rewards} remain constant because these operations are independent of the model's parameter size and can be efficiently handled using smart contracts.

% To address these challenges and enable scalable and cost-effective FL for large models batch processing is a feasible optimization, i.e., submissions and computations can be split into smaller chunks, such as 10,000 parameters per batch, to mitigate the exponential increase in gas consumption for large models.

% Alternatively, performing aggregation and update validation off-chain significantly reduces gas costs. Only the final results, such as a cryptographic hash, are submitted on-chain for verification, ensuring efficiency and integrity.

\section{Evaluation}
\label{sec:evaluation}

Evaluating gas consumption in decentralized systems like blockchain-based FL quantifies the computational and storage costs of executing smart contract functions. Gas costs directly influence the economic feasibility and scalability of the system, particularly when scaling to large models and a high number of participants. We evaluated key smart contract operations, including registration, staking, model submission, aggregation, update validation, and reward distribution, under varying model parameter sizes. The results provide insights into the system’s efficiency and its applicability for resource-intensive tasks like training foundation models in federated settings.

\subsection{Gas Costs and Simulation Setup}

The relationship between gas units and actual dollar costs is determined by the gas price (in gwei), the gas price, and the current exchange rate of ETH to USD. 
% The total cost can be expressed as:  
% \[
% \text{Cost in USD} = \text{Gas Units} \times \text{Gas Price} \times \text{ETH to USD Exchange Rate}.
% \]
Gas units also correlate with computational time, as they measure the complexity of smart contract operations; higher gas units typically indicate longer computation times due to more resource-intensive tasks. 

In this evaluation, we use the "Foundry" tool\footnote{\url{https://book.getfoundry.sh}} to deploy a private Ethereum blockchain. Since this is a simulation environment, we only report gas units as a proxy for computational costs. While these values provide an accurate estimate of resource requirements, they may differ from real-world deployments on the Ethereum mainnet due to variations in gas prices, network congestion, and token valuations. 

In practical settings, this framework is more likely to be deployed on a private Ethereum-based consortium blockchain, where the focus shifts from economic costs to computational efficiency as the primary metric for evaluating feasibility. This particular setup minimizes financial overhead while ensuring that the system’s performance meets the demands of real-world applications.

\subsection{Baseline Deployment Metrics}

The following deployment metrics establish a baseline for understanding the overhead associated with initializing the smart contract:
\begin{itemize}
    \item \textbf{Deployment Cost:} 2,371,244 gas
    \item \textbf{Deployment Size:} 10,667 bytes
\end{itemize}

\subsection{Gas Consumption Analysis}

The gas consumption results in \Cref{tab:gas_costs} highlight the scalability challenges of aggregating large models entirely on-chain in blockchain-based FL settings. For smaller models, such as those with 10 or 100 parameters, gas costs for operations like \emph{Submit Models}, \emph{Aggregate Models}, and \emph{Update Validation} are manageable. However, as the model size increases, gas costs grow exponentially, making such operations impractical for models with 100,000 parameters or more. This exponential growth underscores the limitations of on-chain processing for large-scale FL.

In contrast, operations such as \emph{Registration \& Staking} and \emph{Distribute Rewards} incur constant gas costs regardless of the model size. This is because these operations are independent of the parameter size and are efficiently implemented using smart contracts.

The results demonstrate that while on-chain operations are feasible for small to moderately sized models, alternative approaches like batch processing and off-chain computations are essential for scaling to resource-intensive models like LLMs. These optimizations ensure that blockchain-based FL can remain cost-effective and efficient while supporting the training of high-complexity models in decentralized environments.

% \subsection{Heterogeneous Data}
% Heterogeneous data can cause biases in updates. This is mitigated through:
% \begin{itemize}
%     \item Weighted alignment scores to balance sample size effects.
%     \item Periodic fairness checks to ensure specialized contributors are not under-rewarded.
% \end{itemize}

% \subsection{Alignment Gaming}
% To counteract alignment gaming, the following measures are implemented:
% \begin{itemize}
%     \item Penalize updates with consistently negative alignment scores~\cite{zhao2023}.
%     \item Adjust alignment metrics to incorporate model accuracy improvements.
% \end{itemize}

% \subsection{Scalability}
% Scalability is addressed through:
% \begin{itemize}
%     \item Off-chain Shapley computations to reduce blockchain gas costs.
%     \item Efficient aggregation techniques for large networks~\cite{microsoft_research_2023}.
% \end{itemize}

\section{Discussion}
\label{sec:discussion}

Our findings underscore the importance of aligning model size with the capabilities of the underlying blockchain infrastructure to achieve practical and scalable FL. Given the observed scalability limitations as noted in \Cref{tab:gas_costs}, smaller LLMs may be better suited for FL settings when using on-chain aggregation. 

\subsection{Limitations}

Although the current evaluation offers a comprehensive analysis of gas costs associated with deploying, training, and updating the proposed blockchain-based federated learning framework, it does not fully address the performance evaluation of the incentive and reward mechanisms. Specifically, the on-chain alignment-based rewards, off-chain fairness checks, and consistency multipliers—key components of the framework—remain unevaluated in terms of their impact on fairness, transparency, and participant engagement. These mechanisms play a critical role in incentivizing meaningful contributions and maintaining long-term system participation, particularly in heterogeneous and competitive environments. A thorough analysis of their operational efficiency, scalability, and effectiveness is essential to assess the framework's overall feasibility in real-world deployments. 

\subsection{Challenges and Mitigation Strategies}

Building on the identified limitations, this section outlines key challenges faced by the proposed blockchain-based FL framework and the mitigation strategies designed to address them. These challenges, including scalability, heterogeneous data distributions, and alignment gaming, require tailored solutions especially to ensure the framework’s efficiency, fairness, and robustness in real-world deployments.

\subsubsection{Scalability}
\begin{itemize}
    \item \textbf{Batch Processing:} Model submissions and computations can be split into smaller chunks, such as 10,000 parameters per batch. This mitigates the exponential increase in gas consumption for large models while preserving the integrity of the aggregation process.
    
    \item \textbf{Off-Chain Aggregation and Validation:} Performing resource-intensive operations like aggregation and update validation off-chain significantly reduces gas costs. Only the final results, such as cryptographic hashes, are submitted on-chain for verification. This approach ensures computational efficiency while maintaining the trust and transparency inherent in blockchain-based systems.
    
    \item \textbf{Efficient Aggregation Techniques for Large Networks:} Employing advanced aggregation methods, such as sparse updates or quantized model representations, can significantly reduce computational overhead and the volume of data transmitted. These techniques ensure that the framework can scale effectively while maintaining model performance.
\end{itemize}

\subsubsection{Heterogeneous Data}
\begin{itemize}
    \item \textbf{Weighted Alignment Scores:} To address biases caused by varying dataset sizes among clients, alignment scores are weighted based on the sample size contributed by each client. This ensures that smaller datasets do not disproportionately affect the global model's training process.
    
    \item \textbf{Periodic Fairness Checks:} Specialized contributors with unique or highly valuable data may risk being under-rewarded in conventional systems. Periodic fairness checks ensure equitable reward distribution by assessing long-term contributions and adjusting for any imbalances.
\end{itemize}

\subsubsection{Alignment Gaming}
\begin{itemize}
    \item \textbf{Penalty for Negative Alignment Scores:} To discourage malicious behavior, updates with consistently negative alignment scores are penalized~\cite{zhao2023}. This ensures that clients who provide harmful or low-quality updates are deterred from gaming the system.
    
    \item \textbf{Accuracy-Adjusted Alignment Metrics:} The alignment metrics are refined to incorporate improvements in model accuracy. This adjustment ensures that client contributions are evaluated not only on alignment with the global gradient but also on their impact on the model's overall performance.
\end{itemize}

% \Cref{tab:challenges_mitigation} outlines key challenges faced in blockchain-based FL systems and the corresponding mitigation strategies. These challenges include issues arising from heterogeneous data distributions, potential gaming of alignment metrics, and scalability constraints inherent in blockchain operations. The proposed mitigation strategies address these limitations through a combination of weighted scoring, fairness checks, off-chain computations, and efficient aggregation techniques, ensuring robustness, fairness, and scalability in decentralized FL environments.

% \begin{table}[t]
% \centering
% \caption{Challenges and Mitigation Strategies}
% \label{tab:challenges_mitigation}
% \begin{tabular}{|p{0.3\columnwidth}|p{0.6\columnwidth}|}
% \hline
% \textbf{Challenge}        & \textbf{Mitigation Strategies}                                                                                       \\ \hline
% \multirow{2}{*}{Heterogeneous Data} 
%     & Weighted alignment scores to balance sample size effects. \\ \cline{2-2}
%     & Periodic fairness checks to ensure specialized contributors are not under-rewarded. \\ \hline
% \multirow{2}{*}{Alignment Gaming} 
%     & Penalize updates with consistently negative alignment scores~\cite{zhao2023}. \\ \cline{2-2}
%     & Adjust alignment metrics to incorporate model accuracy improvements. \\ \hline
% \multirow{2}{*}{Scalability} 
%     & Off-chain computations to reduce blockchain gas costs. \\ \cline{2-2}
%     & Efficient aggregation techniques for large networks~\cite{microsoft_research_2023}. \\ \hline
% \end{tabular}
% \end{table}

\subsection{Future Work}

Future efforts will focus on addressing several critical aspects to further enhance the proposed blockchain-based FL framework.

First, optimizing computational resources for LLMs and other models with large parameter settings is a key priority. This can be achieved by offloading large parameter aggregation to decentralized storage solutions, such as IPFS, in trustful settings, thereby reducing the computational and storage burden on the blockchain. However, when such models must be aggregated on a public permissionless blockchain, additional strategies are required to address scalability and security concerns.
In such scenarios, aggregation can be performed in a batched manner, where only hashed summaries or intermediate results of smaller parameter chunks are recorded on-chain. This reduces the size of on-chain transactions while preserving transparency and verifiability. 

Second, improving reward scaling mechanisms is essential to ensure fairness and efficiency across a wide range of FL scenarios. 
real-world FL scenarios with heterogeneous data distributions and client behaviors.
This includes comprehensive testing in environments with diverse client behaviors and varying data contributions to address discrepancies in reward distribution.
We plan to simulate real-world FL scenarios with diverse client behaviors and measuring the gas costs and computational overhead associated with each incentive mechanism. Additionally, empirical studies could explore the trade-offs between the economic costs of running these mechanisms and their impact on participation rates, fairness, and model performance, providing a more comprehensive assessment of the framework. This expanded evaluation would further validate the practicality of the proposed hybrid incentive mechanism and identify optimization opportunities for large-scale deployments.

Finally, establishing a standardized benchmark for evaluating FL architectures integrated with the proposed mechanism will be valuable for comparing different architectures and guiding future optimizations.

\section{Conclusion}
\label{sec:conclusion}

This paper introduces a blockchain-based framework that tackles key challenges in traditional FL systems, including trust, fairness, and scalability. By leveraging blockchain technology and smart contracts, the framework automates critical operations such as client registration, update validation, reward distribution, and global state maintenance, eliminating the need for centralized aggregation mechanisms. The hybrid incentive mechanism—combining on-chain alignment-based rewards, off-chain fairness checks, and consistency multipliers—ensures equitable participation, sustained engagement, and efficient resource utilization, making the framework robust and adaptable to heterogeneous environments.

Empirical evaluations validate the framework’s feasibility, demonstrating that it is well-suited for decentralized and collaborative training scenarios, particularly for models with moderate parameter sizes. The gas efficiency results underscore the practicality of the framework in addressing the computational and economic constraints associated with blockchain-based systems. 

As machine learning continues to scale, with foundation models and other resource-intensive architectures at the forefront, the framework proposed in this paper offers a promising path forward by enabling equitable participation and contributions in decentralized training. Finally, by addressing core challenges in trust, scalability, and fairness, this work represents a critical step toward realizing efficient, decentralized AI solutions that are both practical and impactful for real-world applications.

% The proposed system addresses critical gaps in traditional FL systems, including trust, incentive design, and scalability. Empirical evaluations demonstrate its feasibility for resource-intensive applications while ensuring fairness and transparency. By advancing the integration of blockchain with federated learning, our work contributes a robust, decentralized framework capable of scaling to real-world, heterogeneous environments.

% Future work includes:
% \begin{itemize}
%     \item Optimizing computational resources for large language models (LLMs) and other models with large parameter settings. This can be achieved by offloading large parameters to decentralized storage solutions such as IPFS. By keeping parameters off-chain, the system reduces the computational and storage burden on the blockchain, making it more feasible for models with high complexity~\cite{ye2024}.
%     \item Debugging and synchronizing the system to address reward scaling issues, ensuring that rewards remain fair and efficient across a wide range of federated learning scenarios~\cite{sani2024}.
%     \item Establishing a benchmark for model evaluation to assess the performance, efficiency, and scalability of different federated learning architectures integrated with the proposed mechanism. This will provide a standardized comparison and guide future optimizations.
% \end{itemize}

\section*{Code Availability}

The source code for the blockchain-based FL aggregator, including the Solidity smart contracts and Python scripts for off-chain computations, is available on our GitHub repository at \url{https://github.com/brains-group/OpenFedLLM/tree/Bijun-SmartContract/smart_contract}.

% \begin{itemize}
%     \item Repository: \textcolor{blue}{\url{https://github.com/brains-group/OpenFedLLM}}
%     \item Branch: \texttt{Bijun-SmartContract}
% \end{itemize}

% Contributions, bug reports, and suggestions are welcome through the repository's issue tracker or pull requests.

\balance

\bibliographystyle{ACM-Reference-Format}
\bibliography{reference}



% \section*{Appendix}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\linewidth]{SUM.png}
%     \caption{Architecture of the SUM Framework. Contributors interact with an incentive mechanism, data handler, and model for collaborative training.}
%     \Description{This figure depicts the architecture of the SUM Framework. Contributors submit data to a collaborative trainer that interacts with an incentive mechanism, a data handler, and a model for predictions and updates.}
%     \label{fig:sum_framework_architecture}
% \end{figure}

\end{document}
