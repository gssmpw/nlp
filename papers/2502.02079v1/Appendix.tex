\onecolumn
\allowdisplaybreaks

\section{Clustering Of Neural Dueling Bandits (CONDB) Algorithm}
\label{app:sec:condb:algo}
Here we provide the complete statement of our CONDB algorithm.

\begin{algorithm*}[h] 
\caption{Clustering Of Neural Dueling Bandits (CONDB)}
\label{algo:neural:dueling:bandits}
	\begin{algorithmic}[1]
    \STATE {\bf Input:} $f(T_{i,t}) \triangleq \frac{\beta_T + B \sqrt{\frac{\lambda}{\kappa_\mu}} + 1}{\sqrt{2\tilde{\lambda}_x T_{i,t}}}$, regularization parameter $\lambda>0$, confidence parameter $\beta_T \triangleq \frac{1}{\kappa_\mu} \sqrt{ \widetilde{d} + 2\log(u/\delta)}$.
    $\phi(\bx) = \frac{1}{\sqrt{m_{\text{NN}}}}g(\bx;\btheta_0)$ where $\btheta_0$ denotes the NN parameters at initialization.
    \STATE {\bf Initialization:} 
$\bV_0=\bV_{i,0} = \frac{\lambda}{\kappa_\mu} \mathbf{I}$ , $\hat\btheta_{i,0}=\bzero$, $\forall{i \in \mathcal{U}}$, a complete Graph $G_0 = (\mathcal{U},E_0)$ over $\mathcal{U}$.\alglinelabel{algo line: init}
		\FOR{$t= 1, \ldots, T$}
            \STATE Receive the index of the current user $i_t\in\mathcal{U}$, and the current feasible arm set $\cX_t$;\label{user serve neural}
            \STATE Find the connected component $\overline C_t$ for user $i_t$ in the current graph $G_{t-1}$ as the current cluster; \alglinelabel{detection:neural}

            \STATE Train the neural network using $\{(\bx_{s,1}, \bx_{s,2}, y_s)\}_{s\in[t-1], i_s\in \overline C_t}$ by minimizing the following loss function:
            \begin{equation}
                \overline{\btheta}_t=\arg\min_{\btheta} 
                - \frac{1}{m} \sum_{s\in[t-1]\atop i_s\in \overline C_t}\left( y_s\log\mu\left( h(\bx_{s,1};\btheta) - h(\bx_{s,2};\btheta) \right) + (1-y_s)\log\mu\left(h(\bx_{s,2};\btheta) - h(\bx_{s,1};\btheta)\right) \right) + \frac{\lambda}{2}\norm{\btheta - \btheta_0}_2^2;
            \end{equation}\alglinelabel{algo line: common theta:neural}
            
            \STATE Calculate the aggregated information matrix for cluster $\overline C_t$: $\bV_{t-1}=\bV_0+\sum_{s\in[t-1]\atop i_s\in \overline C_t} (\phi(\bx_{s,1}) - \phi(\bx_{s,2}))(\phi(\bx_{s,1}) - \phi(\bx_{s,2}))^\top$. \alglinelabel{algo line: common matrix:neural}
            \STATE Choose the first arm  $\bx_{t,1} = \arg\max_{\bx\in\mathcal{X}_t} h(\bx;\overline{\btheta}_t)$; \alglinelabel{algo line: choose x1:neural}
            \STATE Choose the second arm $\bx_{t,2} = \arg\max_{\bx\in\mathcal{X}_t} h(\bx;\overline{\btheta}_t) + \left( \beta_T + B\sqrt{\frac{\lambda}{\kappa_\mu}} + 1 \right) \norm{\left(\phi(\bx) - \phi(\bx_{t,1})\right)}_{\bV_{t-1}^{-1}}$; \alglinelabel{algo line: choose x2:neural}
		\STATE Observe the preference feedback: $y_t = \mathbbm{1}(\bx_{t,1}\succ \bx_{t,2})$, and update history: $\mathcal{D}_t=\{i_s, \bx_{s,1}, \bx_{s,2}, y_s\}_{s=1,\ldots,t}$;\alglinelabel{algo line: feedback:neural}
        \STATE Train the neural network using all data for user $i_t$: $\{(\bx_{s,1}, \bx_{s,2}, y_s)\}_{s\in[t], i_s = i_t}$ by minimizing the following loss function:\alglinelabel{algo line: update it:neural}
            \begin{equation}
                \hat{\btheta}_{i_t,t}=\arg\min_{\btheta} 
                - \frac{1}{m_{\text{NN}}} \sum_{s\in[t-1]\atop i_s = i_t}\left( y_s\log\mu\left( h(\bx_{s,1};\btheta) - h(\bx_{s,2};\btheta) \right) + (1-y_s)\log\mu\left(h(\bx_{s,2};\btheta) - h(\bx_{s,1};\btheta)\right) \right)+ \frac{\lambda}{2}\norm{\btheta - \btheta_0}_2^2;
                \label{eq:loss:func:individial}
            \end{equation}
            keep the estimations of other users unchanged;
            \STATE Delete the edge $(i_t,\ell)\in E_{t-1}$ if
            \begin{equation}
                \sqrt{m_{\text{NN}}}\norm{\hat\btheta_{i_t,t}-\hat\btheta_{\ell,t}}_2>f(T_{i_t,t})+f(T_{\ell,t})
            \end{equation} \alglinelabel{algo line: delete:neural}
		\ENDFOR
	\end{algorithmic}
\end{algorithm*}





\section{Proof of Theorem \ref{thm: linear regret bound}}
\label{app: proof linear}
First, we prove the following lemma.
\begin{lemma}\label{lemma:concentration:theta}
With probability at least $1-\delta$ for some $\delta\in(0,1)$, at any $t\in[T]$:
\begin{equation}
    \norm{\hat{\btheta}_{i,t}-\btheta^{j(i)}}_2\leq\frac{\sqrt{\lambda \kappa_\mu}+\sqrt{2\log(u/\delta)+d\log(1+T_{i,t}\kappa_\mu/d\lambda)}}{\kappa_\mu\sqrt{\lambda_{\text{min}}(\bV_{i,t-1})}}, \forall{i\in\mathcal{U}}\label{l2 norm difference bound}\,,
\end{equation}
where $\bV_{i,t-1}=\frac{\lambda}{\kappa_\mu} \mathbf{I}+\sum_{s\in[t-1]\atop i_s=i}(\phi(\bx_{s,1}) - \phi(\bx_{s,2}))(\phi(\bx_{s,1}) - \phi(\bx_{s,2}))^\top$, and $T_{i,t}$ denotes the number of rounds of seeing user $i$ in the first $t$ rounds.
\end{lemma}
\begin{proof}
    First, we prove the following result. 
    
    For a fixed user $i$, with probability at least $1-\delta$ for some $\delta\in(0,1)$, at any $t\in[T]$:
\begin{equation}
    \norm{\hat{\btheta}_{i,t}-\btheta^{j(i)}}_{\bV_{i,t-1}}\leq\frac{\sqrt{\lambda \kappa_\mu}+\sqrt{2\log(1/\delta)+d\log(1+4T_{i,t}\kappa_\mu/d\lambda)}}{\kappa_\mu}\label{V norm difference bound}\,,
\end{equation}
Recall that $f_i(\bx)=\btheta_i^\top\phi(\bx)$. In iteration $s$, define $\widetilde{\phi}_s = \phi(\bx_{s,1}) - \phi(\bx_{s,2})$.
And we define $\widetilde{f}_{i,s} = f_i(\bx_{s,1}) - f_i(\bx_{s,2}) =\btheta_i^{\top} \widetilde{\phi}_s$.

For any $\btheta_{f^\prime} \in\mathbb{R}^{d}$, define 
\[
G_{i,t}(\btheta_{f^\prime}) = \sum_{s\in[t-1]:\atop i_s=i}\left(\mu(\btheta_{f^\prime}^\top\widetilde{\phi}_s ) - \mu(\btheta_i^\top\widetilde{\phi}_s) \right) \widetilde{\phi}_s  + \lambda \btheta_{f'}.
\]
For $\lambda'\in(0, 1)$, setting $\btheta_{\bar{f}} = \lambda' \btheta_{f^\prime_1} + (1 - \lambda')\btheta_{f^\prime_2}$.
and using the mean-value theorem, we get:
\begin{align}
    \label{eqn:glb}
    G_{i,t}(\btheta_{f^\prime_1}) - G_{i,t}(\btheta_{f^\prime_2}) &= \left[\sum_{s\in[t-1]:\atop i_s=i} \nabla\mu(\btheta_{\bar{f}}^\top\widetilde{\phi}_s)\widetilde{\phi}_s \widetilde{\phi}_s^\top + \lambda \mathbf{I} \right](\btheta_{f^\prime_1} - \btheta_{f^\prime_2}) & \left( \btheta_i\text{ is constant} \right)  \nonumber \\
\end{align}
Define $\bM_{i,t-1} = \left[\sum_{s\in[t-1]:\atop i_s=i}\nabla\mu(\btheta_{\bar{f}}^\top\widetilde{\phi}_s)\widetilde{\phi}_s \widetilde{\phi}_s^\top + \lambda \mathbf{I} \right]$, and recall that $\bV_{i,t-1} = \sum_{s\in[t-1]:\atop i_s=i} \widetilde{\phi}_s \widetilde{\phi}_s^\top + \frac{\lambda}{\kappa_\mu} \mathbf{I}$.
Then we have that $\bM_{i,t-1} \succeq \kappa_\mu \bV_{i,t-1}$ and that $\bV^{-1}_{i,t-1} \succeq \kappa_\mu \bM^{-1}_{i,t-1}$, where we use the notation $\bM\succeq \bV$ to denote that $\bM-\bV$ is a positive semi-definite matrix. Then we have

\begin{align*}
    \norm{G_{i,t}(\hat\btheta_{i,t})-\lambda\btheta_i}_{\bV_{i,t-1}^{-1}}^2 &=  \norm{G_{i,t}(\btheta_i) - G_t(\hat\btheta_{i,t})}_{\bV_{i,t-1}^{-1}}^2 = \norm{\bM_{i,t-1} (\btheta_i - \hat\btheta_{i,t})}_{\bV_{i,t-1}^{-1}}^2 & \left( G_{i,t}(\btheta_i) = \lambda\btheta_i\text{ by definition} \right)\\
    & = (\btheta_i - \hat\btheta_{i,t})^{\top} \bM_{i,t-1} \bV_{i,t-1}^{-1} \bM_{i,t-1} (\btheta_i - \hat\btheta_{i,t})\\
    &\geq (\btheta_i - \hat\btheta_{i,t})^{\top} \bM_{i,t-1} \kappa_\mu \bM_{i,t-1}^{-1} \bM_{i,t-1} (\btheta_i - \hat\btheta_{i,t})\\
    & = \kappa_\mu(\btheta_i - \hat\btheta_{i,t})^{\top} \bM_{i,t-1} (\btheta_i - \hat\btheta_{i,t})\\
    & \geq \kappa_\mu(\btheta_i - \hat\btheta_{i,t})^{\top} \kappa_\mu \bV_{i,t-1} (\btheta_i - \hat\btheta_{i,t})\\
    & = \kappa_\mu^2 (\btheta_i - \hat\btheta_{i,t})^{\top} \bV_{i,t-1} (\btheta_i - \hat\btheta_{i,t})\\
    & = \kappa_\mu^2 \norm{\btheta_i - \hat\btheta_{i,t}}^2_{\bV_{i,t-1}}  & \left(\text{as } ||\bx||_{\bA}^2 = \bx^\top \bA \bx \right)
\end{align*}
The first inequality is because $\bV^{-1}_{i,t-1} \succeq \kappa_\mu \bM^{-1}_{i,t-1}$, and the second inequality follows from $\bM_{i,t-1} \succeq \kappa_\mu \bV_{i,t-1}$.

Note that $\frac{\kappa_\mu}{\lambda} \mathbf{I}\succeq\bV_{i,t-1}$, which allows us to show that
\begin{equation}
\begin{split}
\norm{\lambda \btheta_i}_{\bV_{i,t-1}^{-1}} = \lambda \sqrt{ \btheta_i^{\top} \bV_{i,t-1}^{-1} \btheta_i} \leq \lambda \sqrt{ \btheta_i^{\top} \frac{\kappa_\mu}{\lambda} \btheta_i} \leq \sqrt{\lambda\kappa_\mu} \norm{\btheta_i}_2 \leq \sqrt{\lambda\kappa_\mu}.
\end{split}
\end{equation}
Using the two equations above, we have that
\begin{equation}
\begin{split}
\norm{\btheta_i - \hat{\btheta}_{i,t}}_{\bV_{i,t-1}} \le \frac{1}{\kappa_\mu} \norm{G_{i,t}(\hat{\btheta}_{i,t}) - \lambda \btheta_i}_{\bV_{i,t-1}^{-1}} &\leq \frac{1}{\kappa_\mu} \norm{G_{i,t}(\hat{\btheta}_{i,t})}_{\bV_{i,t-1}^{-1}} + \frac{1}{\kappa_\mu}\norm{\lambda \btheta_i}_{\bV_{i,t-1}^{-1}} \\
&\leq \frac{1}{\kappa_\mu} \norm{G_{i,t}(\hat{\btheta}_{i,t})}_{\bV_{i,t-1}^{-1}} + \sqrt{\frac{\lambda}{\kappa_\mu}}
\end{split}
\end{equation}


Then, let $f^i_{t,s}=\hat\btheta_{i,t}^\top\tilde\phi_s$, we have:
\begin{align*}
\frac{1}{\kappa_\mu^2} \norm{G_{i,t}(\hat\btheta_{i,t})}_{\bV_{i,t-1}^{-1}}^2 &= \frac{1}{\kappa_\mu^2} \norm{\sum_{s\in[t-1]:\atop i_s=i} (\mu(\hat\btheta_{i,t}^\top \widetilde{\phi}_s ) - \mu(\btheta_i^\top \widetilde{\phi}_s) ) \widetilde{\phi}_s + \lambda \hat\btheta_{i,t}}_{\bV_{i,t-1}^{-1}}^2 & \left(\text{by definition of } G_{i,t}(\hat\btheta_{i,t}) \right) \\
    &= \frac{1}{\kappa_\mu^2} \norm{\sum_{s\in[t-1]:\atop i_s=i} (\mu(f^i_{t,s}) - \mu(\widetilde{f}_{i,s}) ) \widetilde{\phi}_s + \lambda \hat\btheta_{i,t}}_{\bV_{i,t-1}^{-1}}^2 & \left(\text{see definitions of } f^i_{t,s}\, \text{and } \widetilde{f}_{i,s} \right) \\
    &= \frac{1}{\kappa_\mu^2} \norm{\sum_{s\in[t-1]:\atop i_s=i} (\mu(f^i_{t,s}) - (y_s - \epsilon_s) ) \widetilde{\phi}_s + \lambda \hat\btheta_{i,t}}_{\bV_{i,t-1}^{-1}}^2 & \left(\text{as } y_s = \mu(\widetilde{f}_{i,s}) + \epsilon_s \text{if } i_s=i \right) \\
    &= \frac{1}{\kappa_\mu^2} \norm{\sum_{s\in[t-1]:\atop i_s=i} \left(\mu(f^i_{t,s}) - y_s\right) \widetilde{\phi}_s + \sum_{s\in[t-1]:\atop i_s=i}\epsilon_s \widetilde{\phi}_s  + \lambda \hat\btheta_{i,t}}_{\bV_{i,t-1}^{-1}}^2 \\
    &\leq \frac{1}{\kappa_\mu^2} \norm{\sum_{s\in[t-1]:\atop i_s=i}\epsilon_s \widetilde{\phi}_s}_{\bV_{i,t-1}^{-1}}^2.
\end{align*}
The last step holds due to the following reasoning. Recall that $\hat\btheta_{i,t}$ is computed using MLE by solving the following equation:
    \begin{equation}
         \hat{\btheta}_{i_t,t} = \arg\min_{\btheta} \Bigg[ - \sum_{\substack{s \in [t-1] \\ i_s = i_t}} \bigg( y_s \log \mu\big(\btheta^\top [\phi(\bx_{s,1}) - \phi(\bx_{s,2})]\big)
        + (1 - y_s) \log \mu\big(\btheta^\top [\phi(\bx_{s,2}) - \phi(\bx_{s,1})]\big) \bigg) + \frac{\lambda}{2} \|\btheta\|_2^2 \Bigg].
    \end{equation}
Setting its gradient to $0$, the following is satisfied:
\begin{equation}
    \sum_{s\in[t-1]:\atop i_s=i} \left(\mu\left( \hat\btheta_{i,t}^{\top} \widetilde{\phi}_s \right) - y_s\right) \widetilde{\phi}_s + \lambda \hat\btheta_{i,t} = 0,
\end{equation}
which is used in the last step.

Now we have
\begin{equation}
    \label{eqn:parUB}
    \frac{1}{\kappa_\mu^2} \norm{G_{i,t}(\hat\btheta_{i,t})}_{\bV_{i,t-1}^{-1}}^2 \le \frac{1}{\kappa_\mu^2} \norm{\sum_{s\in[t-1]:\atop i_s=i}\epsilon_s \widetilde{\phi}_s}_{\bV_{i,t-1}^{-1}}^2.
\end{equation}

Denote $\bV \triangleq \frac{\lambda}{\kappa_\mu} \mathbf{I}$.
Note that the sequence of observation noises $\{\epsilon_s\}$ is $1$-sub-Gaussian.

Next, we can apply Theorem 1 from \cite{abbasi2011improved}, to obtain
\begin{equation}
\norm{\sum_{s\in[t-1]:\atop i_s=i}\epsilon_s \widetilde{\phi}_s}_{\bV_{i,t-1}^{-1}}^2 \leq 2\log\left( \frac{\det(\bV_{i,t-1})^{1/2}}{\delta\det(\bV)^{1/2}} \right),
\end{equation}
which holds with probability of at least $1-\delta$.

Next, based on our assumption that $\norm{\widetilde{\phi}_s}_2 \leq 2$, according to Lemma 10 from \cite{abbasi2011improved}, we have that
\begin{equation}
\det(\bV_{i,t-1}) \leq \left( \lambda/\kappa_\mu + 4T_{i,t} / d \right)^d\,,
\end{equation}
where $T_{i,t}$ denotes the number of rounds of serving user $i$ in the first $t$ rounds.
Therefore, 
\begin{equation}
\sqrt{\frac{\det{\bV_{i,t-1}}}{\det(V)}} \leq \sqrt{\frac{\left( \lambda/\kappa_\mu + 4T_{i,t} / d \right)^d}{(\lambda/\kappa_\mu)^d}} = \left( 1 + 4T_{i,t}\kappa_{\mu}/(d\lambda) \right)^{\frac{d}{2}}
\label{eq:upper:bound:det:Vt:V}
\end{equation}
This gives us
\begin{equation}
\norm{\sum_{s\in[t-1]:\atop i_s=i}\epsilon_s \widetilde{\phi}_s}_{\bV_{i,t-1}^{-1}}^2 \leq 2\log\left( \frac{\det(\bV_{i,t-1})^{1/2}}{\delta\det(V)^{1/2}} \right) \leq 2\log(1/\delta) + d\log\left( 1 + 4T_{i,t}\kappa_{\mu}/(d\lambda) \right)
\label{eq:upper:bound:proof:theta:interm}
\end{equation}

Then, with the above reasoning, we have that with probability at least $1-\delta$ for some $\delta\in(0,1)$, at any $t\in[T]$:
\begin{equation}
    \norm{\hat{\btheta}_{i,t}-\btheta^{j(i)}}_{\bV_{i,t-1}}\leq\frac{\sqrt{\lambda \kappa_\mu}+\sqrt{2\log(1/\delta)+d\log(1+4T_{i,t}\kappa_\mu/d\lambda)}}{\kappa_\mu}\,,
\end{equation}

Taking a union bound over $u$ users, we have that with probability at least $1-\delta$ for some $\delta\in(0,1)$, at any $t\in[T]$:
\begin{equation}
    \norm{\hat{\btheta}_{i,t}-\btheta^{j(i)}}_{\bV_{i,t-1}}\leq\frac{\sqrt{\lambda \kappa_\mu}+\sqrt{2\log(u/\delta)+d\log(1+4T_{i,t}\kappa_\mu/d\lambda)}}{\kappa_\mu}\label{V norm difference bound union}\,, \forall i\in \mathcal{U}.
\end{equation}
Then we have that with probability at least $1-\delta$ for all $t\in[T]$ and all $i\in\mathcal{U}$
\begin{align}
    \norm{\hat{\btheta}_{i,t}-\btheta^{j(i)}}&\leq \frac{\norm{\hat{\btheta}_{i,t}-\btheta^{j(i)}}_{\bV_{i,t-1}}}{\sqrt{\lambda_{\text{min}(\bV_{i,t-1})}}}\notag\\
    &\leq \frac{\sqrt{\lambda \kappa_\mu}+\sqrt{2\log(u/\delta)+d\log(1+4T_{i,t}\kappa_\mu/d\lambda)}}{\kappa_\mu{\sqrt{\lambda_{\text{min}(\bV_{i,t-1})}}}}.
\end{align}
\end{proof}

Then, we prove the following lemma, which gives a sufficient time $T_0$ for the COLDB algorithm to cluster all the users correctly with high probability.

\begin{lemma}\label{T0 lemma}
    With the carefully designed edge deletion rule, after 
\begin{equation*}
    \begin{aligned}
        T_0&\triangleq 16u\log(\frac{u}{\delta})+4u\max\{
        \frac{128d}{\kappa_\mu^2\tilde{\lambda}_x\gamma^2}\log(\frac{u}{\delta}),\frac{16}{\tilde{\lambda}_x^2}\log(\frac{8ud}{\tilde{\lambda}_x^2\delta})\}\\
        &=O\bigg(u\left( \frac{d}{\kappa_\mu^2\tilde{\lambda}_x\gamma^2}+\frac{1}{\tilde{\lambda}_x^2}\right)\log \frac{1}{\delta}\bigg)
    \end{aligned}
\end{equation*}
rounds, with probability at least $1-3\delta$ for some $\delta\in(0,\frac{1}{3})$, COLDB can cluster all the users correctly.
\end{lemma}
\begin{proof}
    Then, with the item regularity assumption stated in Assumption \ref{assumption3}, Lemma J.1 in \cite{wang2024onlinea}, together with Lemma 7 in \cite{li2018online}, and applying a union bound, with probability at least $1-\delta$, for all $i\in\mathcal{U}$, at any $t$ such that $T_{i,t}\geq\frac{16}{\tilde{\lambda}_x^2}\log(\frac{8ud}{\tilde{\lambda}_x^2\delta})$, we have:
\begin{equation}
    \lambda_{\text{min}}(\bV_{i,t})\geq2\tilde{\lambda}_x T_{i,t}\,.
    \label{min eigen}
\end{equation}
Then, together with Lemma \ref{lemma:concentration:theta}, we have: if $T_{i,t}\geq\frac{16}{\tilde{\lambda}_x^2}\log(\frac{8ud}{\tilde{\lambda}_x^2\delta})$, then with probability $\geq 1-2\delta$, we have:
\begin{align}
    \norm{\hat{\btheta}_{i,t}-\btheta^{j(i)}}
    &\leq \frac{\sqrt{\lambda \kappa_\mu}+\sqrt{2\log(u/\delta)+d\log(1+4T_{i,t}\kappa_\mu/d\lambda)}}{\kappa_\mu{\sqrt{\lambda_{\text{min}(\bV_{i,t-1})}}}}\notag\\
    &\leq \frac{\sqrt{\lambda \kappa_\mu}+\sqrt{2\log(u/\delta)+d\log(1+4T_{i,t}\kappa_\mu/d\lambda)}}{\kappa_\mu{\sqrt{2\tilde{\lambda}_x T_{i,t}}}}\notag\,.
\end{align}

Now, let
\begin{equation}
    \frac{\sqrt{\lambda \kappa_\mu}+\sqrt{2\log(u/\delta)+d\log(1+4T_{i,t}\kappa_\mu/d\lambda)}}{\kappa_\mu{\sqrt{2\tilde{\lambda}_x T_{i,t}}}}<\frac{\gamma}{4}\,,
\end{equation}
Let $\lambda \kappa_\mu\leq2\log(u/\delta)+d\log(1+4T_{i,t}\kappa_\mu/d\lambda)$, which typically holds ($\kappa_\mu$ is typically very small), we can get
\begin{equation}
        \frac{2\log(u/\delta)+d\log(1+4T_{i,t}\kappa_\mu/d\lambda)}{2\kappa_\mu^2{\tilde{\lambda}_x T_{i,t}}}<\frac{\gamma^2}{64}\,,
\end{equation}
and a sufficient condition for it to hold is
\begin{align}
     \frac{2\log(u/\delta)}{2\kappa_\mu^2{\tilde{\lambda}_x T_{i,t}}}<\frac{\gamma^2}{128}\label{condition1}
\end{align}
and 
\begin{equation}
    \frac{d\log(1+4T_{i,t}\kappa_\mu/d\lambda)}{2\kappa_\mu^2{\tilde{\lambda}_x T_{i,t}}}<\frac{\gamma^2}{128}\,.\label{condition2}
\end{equation}
Solving Eq.(\ref{condition1}), we can get
\begin{equation}
    T_{i,t}\geq \frac{128\log(u/\delta)}{\kappa_\mu^2\tilde{\lambda}_x\gamma^2}\,.
\end{equation}
Following Lemma 9 in \cite{li2018online}, we can get the following sufficient condition for Eq.(\ref{condition2}):
\begin{equation}
    T_{i,t}\geq \frac{128d}{\kappa_\mu^2\tilde\lambda_x\gamma^2}\log(\frac{512}{\lambda\kappa_\mu\tilde{\lambda}_x\gamma^2})\,.
\end{equation}
Let $u/\delta\geq 512/\lambda\kappa_\mu\tilde{\lambda}_x\gamma^2$, which is typically held. Then, combining all together, we have that if
\begin{equation}
    T_{i,t}\geq\max\{\frac{128d}{\kappa_\mu^2\tilde{\lambda}_x\gamma^2}\log(\frac{u}{\delta}),\frac{16}{\tilde{\lambda}_x^2}\log(\frac{8ud}{\tilde{\lambda}_x^2\delta})\}, \forall i\in \mathcal{U}\,, \label{condition final}
\end{equation}
then with probability at least $1-2\delta$, we have
\begin{equation}
    \norm{\hat{\btheta}_{i,t}-\btheta^{j(i)}}<\gamma/4, \forall i\in\mathcal{U}\,.
\end{equation}
By Lemma 8 in \cite{li2018online}, and Assumption \ref{assumption2} of user arrival uniformness, we have that for all
\begin{equation*}
    \begin{aligned}
        T_0&\triangleq 16u\log(\frac{u}{\delta})+4u\max\{
        \frac{128d}{\kappa_\mu^2\tilde{\lambda}_x\gamma^2}\log(\frac{u}{\delta}),\frac{16}{\tilde{\lambda}_x^2}\log(\frac{8ud}{\tilde{\lambda}_x^2\delta})\}\\
        &=O\bigg(u\left( \frac{d}{\kappa_\mu^2\tilde{\lambda}_x\gamma^2}+\frac{1}{\tilde{\lambda}_x^2}\right)\log \frac{1}{\delta}\bigg)\,,
    \end{aligned}
\end{equation*}
the condition in Eq.(\ref{condition final}) is satisfied with probability at least $1-\delta$.

Therefore we have that for all $t\geq T_0$, with probability $\geq 1-3\delta$:
\begin{equation}
    \norm{\hat{\btheta}_{i,t}-\btheta^{j(i)}}_2<\frac{\gamma}{4}\,,\forall{i\in\mathcal{U}}\,.
    \label{final condition}
\end{equation}
Finally, we only need to show that with $\norm{\hat{\btheta}_{i,t}-\btheta^{j(i)}}_2<\frac{\gamma}{4}\,,\forall{i\in\mathcal{U}}$, the algorithm can cluster all the users correctly. First, when the edge $(i,l)$ is deleted, user $i$ and user $j$ must belong to different \gtclusters{}, i.e., $\norm{\btheta_i-\btheta_l}_2>0$. This is because by the deletion rule of the algorithm, the concentration bound, and triangle inequality
\begin{align}
   &\norm{\btheta_i-\btheta_l}_2=\norm{\btheta^{j(i)}-\btheta^{j(l)}}_2\notag\\
   &\geq\norm{\hat{\btheta}_{i,t}-\hat{\btheta}_{l,t}}_2-\norm{\btheta^{j(l)}-\hat{\btheta}_{l,t}}_2-\norm{\btheta^{j(i)}-\hat{\btheta}_{i,t}}_2\notag\\
   &\geq\norm{\hat{\btheta}_{i,t}-\hat{\btheta}_{l,t}}_2-f(T_{i,t})-f(T_{l,t})>0 \,.
\end{align}
Second, we can show that if $\norm{\btheta_i-\btheta_l}>\gamma$, meaning that user $i$ and user $l$ are not in the same \gtcluster, COLDB will delete the edge $(i,l)$ after $T_0$. This is because
\begin{align}
    \norm{\hat\btheta_{i,t}-\hat{\btheta}_{l,t}}&\geq \norm{\btheta_i-\btheta_l}-\norm{\hat{\btheta}_{i,t}-\btheta^{j(i)}}_2-\norm{\hat{\btheta}_{l,t}-\btheta^{j(l)}}_2\notag\\
    &>\gamma-\frac{\gamma}{4}-\frac{\gamma}{4}\notag\\
    &=\frac{\gamma}{2}>f(T_{i,t})+f(T_{l,t})\,,
\end{align}
which will trigger the edge deletion rule to delete edge $(i,l)$. Combining all the reasoning above, we can finish the proof.
\end{proof}

Then, we prove the following lemmas for the cluster-based statistics.
\begin{lemma}\label{lemma:concentration:theta cluster}
With probability at least $1-4\delta$ for some $\delta\in(0,1/4)$, at any $t\geq T_0$:
\begin{equation}
    \norm{\overline \btheta_t-\btheta_{i_t}}_{\bV_{t-1}}\leq\frac{\sqrt{\lambda \kappa_\mu}+\sqrt{2\log(u/\delta)+d\log(1+4T\kappa_\mu/d\lambda)}}{\kappa_\mu}\label{l2 norm difference bound for cluster}\,.
\end{equation}
\end{lemma}
\begin{proof}
First, by Lemma \ref{T0 lemma}, we have that with probability at least $1-3\delta$, all the users are clustered correctly, i.e., $\overline{C}_t=C_{j(i_t)}, \forall t\geq T_0$.  
Recall that $f_i(\bx)=\btheta_i^\top\phi(\bx)$. In iteration $s$, define $\widetilde{\phi}_s = \phi(\bx_{s,1}) - \phi(\bx_{s,2})$.
And we define $\widetilde{f}_{i,s} = f_i(\bx_{s,1}) - f_i(\bx_{s,2}) =\btheta_i^{\top} \widetilde{\phi}_s$.

For any $\btheta_{f^\prime} \in\mathbb{R}^{d}$, define 
\[
G_t(\btheta_{f^\prime}) = \sum_{s\in[t-1]:\atop i_s\in\overline{C}_t}\left(\mu(\btheta_{f^\prime}^\top\widetilde{\phi}_s ) - \mu(\btheta_{i_t}^\top\widetilde{\phi}_s) \right) \widetilde{\phi}_s  + \lambda \btheta_{f'}.
\]
For $\lambda'\in(0, 1)$, setting $\btheta_{\bar{f}} = \lambda' \btheta_{f^\prime_1} + (1 - \lambda')\btheta_{f^\prime_2}$.
and using the mean-value theorem, we get:
\begin{align}
    \label{eqn:glb}
    G_t(\btheta_{f^\prime_1}) - G_t(\btheta_{f^\prime_2}) &= \left[\sum_{s\in[t-1]:\atop i_s\in\overline{C}_t} \nabla\mu(\btheta_{\bar{f}}^\top\widetilde{\phi}_s)\widetilde{\phi}_s \widetilde{\phi}_s^\top + \lambda \mathbf{I} \right](\btheta_{f^\prime_1} - \btheta_{f^\prime_2})\nonumber \\
\end{align}
Define $\bM_{t-1} = \left[\sum_{s\in[t-1]:\atop i_s\in\overline{C}_t}\nabla\mu(\btheta_{\bar{f}}^\top\widetilde{\phi}_s)\widetilde{\phi}_s \widetilde{\phi}_s^\top + \lambda \mathbf{I} \right]$, and recall that $\bV_{t-1} = \sum_{s\in[t-1]:\atop i_s\in\overline{C}_t} \widetilde{\phi}_s \widetilde{\phi}_s^\top + \frac{\lambda}{\kappa_\mu} \mathbf{I}$.
Then we have that $\bM_{t-1} \succeq \kappa_\mu \bV_{t-1}$ and that $\bV^{-1}_{t-1} \succeq \kappa_\mu \bM^{-1}_{t-1}$. Then we have
% It is easy to verify that $M_t V^{-1/2} \geq M'_t V^{-1/2}$.

\begin{align*}
    \norm{G_t(\overline{\btheta}_t) - \lambda \btheta_{i_t}}_{\bV_{t-1}^{-1}}^2 &=  \norm{G_t(\btheta_{i_t}) - G_t(\overline{\btheta}_t)}_{\bV_{t-1}^{-1}}^2 = \norm{\bM_{t-1} (\btheta_{i_t} - \overline{\btheta}_t)}_{\bV_{t-1}^{-1}}^2 & \left( G_t(\btheta_{i_t}) = \lambda \btheta_{i_t} \text{ by definition} \right)\\
    & = (\btheta_{i_t} - \overline{\btheta}_t)^{\top} \bM_{t-1} \bV_{t-1}^{-1} \bM_{t-1} (\btheta_{i_t} - \overline{\btheta}_t)\\
    &\geq (\btheta_{i_t} - \overline{\btheta}_t)^{\top} \bM_{t-1} \kappa_\mu \bM_{t-1}^{-1} \bM_{t-1} (\btheta_{i_t} - \overline{\btheta}_t)\\
    & = \kappa_\mu(\btheta_{i_t} - \overline{\btheta}_t)^{\top} \bM_{t-1} (\btheta_{i_t} - \overline{\btheta}_t)\\
    & \geq \kappa_\mu(\btheta_{i_t} - \overline{\btheta}_t)^{\top} \kappa_\mu \bV_{t-1} (\btheta_{i_t} - \overline{\btheta}_t)\\
    & = \kappa_\mu^2 (\btheta_{i_t} - \overline{\btheta}_t)^{\top} \bV_{t-1} (\btheta_{i_t} - \overline{\btheta}_t)\\
    & = \kappa_\mu^2 \norm{\btheta_{i_t} - \overline{\btheta}_t}^2_{\bV_{t-1}}  & \left(\text{as } ||\bx||_{\bA}^2 = \bx^\top \bA \bx \right)
\end{align*}
The first inequality is because $\bV^{-1}_{t-1} \succeq \kappa_\mu \bM^{-1}_{t-1}$, and the second inequality follows from $\bM_{t-1} \succeq \kappa_\mu \bV_{t-1}$.

Note that $\frac{\kappa_\mu}{\lambda} \mathbf{I}\succeq\bV_{t-1}$, which allows us to show that
\begin{equation}
\begin{split}
\norm{\lambda \btheta_{i_t}}_{\bV_{t-1}^{-1}} = \lambda \sqrt{ \btheta_{i_t}^{\top} \bV_{t-1}^{-1} \btheta_{i_t}} \leq \lambda \sqrt{ \btheta_{i_t}^{\top} \frac{\kappa_\mu}{\lambda} \btheta_{i_t}} \leq \sqrt{\lambda\kappa_\mu} \norm{\btheta_{i_t}}_2 \leq \sqrt{\lambda\kappa_\mu}.
\end{split}
\end{equation}
Using the two equations above, we have that
\begin{equation}
\begin{split}
\norm{\btheta_{i_t} - \overline{\btheta}_t}_{\bV_{t-1}} \le \frac{1}{\kappa_\mu} \norm{G_t(\overline{\btheta}_t) - \lambda \btheta_{i_t}}_{\bV_{t-1}^{-1}} &\leq \frac{1}{\kappa_\mu} \norm{G_t(\overline{\btheta}_t)}_{\bV_{t-1}^{-1}} + \frac{1}{\kappa_\mu}\norm{\lambda \btheta_{i_t}}_{\bV_{t-1}^{-1}} \\
&\leq \frac{1}{\kappa_\mu} \norm{G_t(\overline{\btheta}_t)}_{\bV_{t-1}^{-1}} + \sqrt{\frac{\lambda}{\kappa_\mu}}
\end{split}
\end{equation}

Then, let $\overline f_{t,s}=\overline{\btheta}_t^\top\tilde\phi_s$, we have:
\begin{align*}
\frac{1}{\kappa_\mu^2} \norm{G_t(\overline{\btheta}_t)}_{\bV_{t-1}^{-1}}^2
    &\leq \frac{1}{\kappa_\mu^2} \norm{\sum_{s\in[t-1]:\atop i_s\in\overline{C}_t} (\mu(\overline{\btheta}_t^\top \widetilde{\phi}_s ) - \mu(\btheta_{i_t}^\top \widetilde{\phi}_s) ) \widetilde{\phi}_s + \lambda \overline{\btheta}_t}_{\bV_{t-1}^{-1}}^2  & \left(\text{by definition of } G_t(\overline{\btheta}_t) \right) \\
    &= \frac{1}{\kappa_\mu^2} \norm{\sum_{s\in[t-1]:\atop i_s\in\overline{C}_t} (\mu(\overline f_{t,s}) - \mu(\widetilde{f}_{i_t,s}) ) \widetilde{\phi}_s + \lambda \overline{\btheta}_t}_{\bV_{t-1}^{-1}}^2 
    & \left(\text{see definitions of } \overline f_{t,s}\, \text{and } \widetilde{f}_{i,s} \right) \\
    &= \frac{1}{\kappa_\mu^2} \norm{\sum_{s\in[t-1]:\atop i_s\in\overline{C}_t} (\mu(\overline f_{t,s}) - (y_s - \epsilon_s) ) \widetilde{\phi}_s + \lambda \overline{\btheta}_t}_{\bV_{t-1}^{-1}}^2  \notag\\
    & \left(y_s = \mu(\widetilde{f}_{i_t,s}) + \epsilon_s \text{if } i_s=i_t, \text{and} i_s=i_t, \forall i_s\in \overline{C}_t, \forall t\geq T_0 \right) \\
    &= \frac{1}{\kappa_\mu^2} \norm{\sum_{s\in[t-1]:\atop i_s\in\overline{C}_t} \left(\mu(\overline f_{t,s}) - y_s\right) \widetilde{\phi}_s + \sum_{s\in[t-1]:\atop i_s\in\overline{C}_t}\epsilon_s \widetilde{\phi}_s  + \lambda \overline{\btheta}_t}_{\bV_{t-1}^{-1}}^2 \\
    &\leq \frac{1}{\kappa_\mu^2} \norm{\sum_{s\in[t-1]:\atop i_s\in\overline{C}_t}\epsilon_s \widetilde{\phi}_s}_{\bV_{t-1}^{-1}}^2.
\end{align*}
The last step holds due to the following reasoning. Recall that $\overline{\btheta}_t$ is computed using MLE by solving the following equation:
    \begin{equation}
        \overline{\btheta}_t=\arg\min_{\btheta} - \sum_{s\in[t-1]\atop i_s\in \overline C_t}\left( y_s\log\mu\left({\btheta}^{\top}\left[\phi(\bx_{s,1}) - \phi(\bx_{s,2})\right]\right) + (1-y_s)\log\mu\left({\btheta}^{\top}\left[\phi(\bx_{s,2}) - \phi(\bx_{s,1})\right]\right) \right) + \frac{1}{2}\lambda\norm{\btheta}_2^2.
    \end{equation}
Setting its gradient to $0$, the following is satisfied:
\begin{equation}
    \sum_{s\in[t-1]:\atop i_s\in\overline{C}_t} \left(\mu\left( \overline{\btheta}_t^{\top} \widetilde{\phi}_s \right) - y_s\right) \widetilde{\phi}_s + \lambda \overline{\btheta}_t = 0,
\end{equation}
which is used in the last step.

Now we have
\begin{equation}
    \label{eqn:parUB}
    \norm{\btheta_{i_t} - \overline{\btheta}_t}_{\bV_{t-1}} \le \frac{1}{\kappa_\mu} \norm{\sum_{s\in[t-1]:\atop i_s\in\overline{C}_t}\epsilon_s \widetilde{\phi}_s}_{\bV_{t-1}^{-1}}  + \sqrt{\frac{\lambda}{\kappa_\mu}}.
\end{equation}

Denote $\bV \triangleq \frac{\lambda}{\kappa_\mu} \mathbf{I}$.
Note that the sequence of observation noises $\{\epsilon_s\}$ is $1$-sub-Gaussian.

Next, we can apply Theorem 1 from \cite{abbasi2011improved}, to obtain
\begin{equation}
\norm{\sum_{s\in[t-1]:\atop i_s\in\overline{C}_t}\epsilon_s \widetilde{\phi}_s}_{\bV_{t-1}^{-1}}^2 \leq 2\log\left( \frac{\det(\bV_{t-1})^{1/2}}{\delta\det(\bV)^{1/2}} \right),
\end{equation}
which holds with probability of at least $1-\delta$.

Next, based on our assumption that $\norm{\widetilde{\phi}_s}_2 \leq 2$, according to Lemma 10 from \cite{abbasi2011improved}, we have that
\begin{equation}
\det(\bV_{t-1}) \leq \left( \lambda/\kappa_\mu + 4T / d \right)^d\,.
\end{equation}

Therefore, 
\begin{equation}
\sqrt{\frac{\det{\bV_{t-1}}}{\det(V)}} \leq \sqrt{\frac{\left( \lambda/\kappa_\mu + 4T / d \right)^d}{(\lambda/\kappa_\mu)^d}} = \left( 1 + 4T\kappa_{\mu}/(d\lambda) \right)^{\frac{d}{2}}
\label{eq:upper:bound:det:Vt:V}
\end{equation}
This gives us
\begin{equation}
\norm{\sum_{s\in[t-1]:\atop i_s\in\overline{C}_t}\epsilon_s \widetilde{\phi}_s}_{\bV_{t-1}^{-1}}^2 \leq 2\log\left( \frac{\det(\bV_{t-1})^{1/2}}{\delta\det(V)^{1/2}} \right) \leq 2\log(1/\delta) + d\log\left( 1 + 4T\kappa_{\mu}/(d\lambda) \right)
\label{eq:upper:bound:proof:theta:interm}
\end{equation}

Combining all together, we have with probability at least $1-4\delta$ for some $\delta\in(0,1/4)$, at any $t\geq T_0$:
\begin{equation}
    \norm{\overline \btheta_t-\btheta_{i_t}}_{\bV_{t-1}}\leq\frac{\sqrt{\lambda \kappa_\mu}+\sqrt{2\log(u/\delta)+d\log(1+4T\kappa_\mu/d\lambda)}}{\kappa_\mu}\,.
\end{equation}\end{proof}
Then, we prove the following lemma with the help of Lemma \ref{lemma:concentration:theta cluster}.
\begin{lemma}
    \label{lemma:ucb:diff}
For any iteration $t\geq T_0$, for all $\bx,\bx'\in\mathcal{X}_t$, with probability of at least $1-4\delta$, we have 
\[
|\left(f_{i_t}(\bx) - f_{i_t}(\bx')\right) - \overline\btheta_t^{\top}\left( \phi(\bx) - \phi(\bx') \right)| \leq \frac{\beta_T}{\kappa_\mu}\norm{\phi(\bx) - \phi(\bx')}_{\bV_{t-1}^{-1}}\,,
\]
where $\beta_T=\sqrt{\lambda \kappa_\mu}+\sqrt{2\log(u/\delta)+d\log(1+4T\kappa_\mu/d\lambda)}$.
\end{lemma}
\begin{proof}
\begin{equation}
\begin{split}
|\left(f_{i_t}(\bx) - f_{i_t}(\bx')\right) - \overline\btheta_t^{\top}\left( \phi(\bx) - \phi(\bx') \right)| &= |\btheta_{i_t}^{\top} \left[(\phi(\bx) - \phi(\bx')\right] - \overline\btheta_t^{\top}\left[ \phi(\bx) - \phi(\bx') \right]|\\
&= | \left(\btheta_{i_t} - \overline\btheta_t\right)^{\top} \left[\phi(\bx) - \phi(\bx') \right]|\\
&\leq \norm{\btheta_{i_t} - \overline\btheta_t}_{\bV_{t-1}} \norm{\phi(\bx) - \phi(\bx')}_{\bV_{t-1}^{-1}}\\
&\leq \frac{\beta_T}{\kappa_\mu} \norm{\phi(\bx) - \phi(\bx')}_{\bV_{t-1}^{-1}},
\end{split}
\end{equation}
in which the last inequality follows from Lemma \ref{lemma:concentration:theta cluster}.
\end{proof}

We also prove the following lemma to upper bound the summation of squared norms which will be used in proving the final regret bound.
\begin{lemma}
With probability at least $1-4\delta$, we have
\label{lemma:concentration:square:std}
\[
\sum^T_{t=T_0}\mathbb{I}\{i_t\in C_j\}\norm{\phi(\bx_{t,1}) - \phi(\bx_{t,2})}_{\bV_{t-1}^{-1}}^2 \leq 2 d\log \left( 1 + 4T \kappa_{\mu}/(d\lambda) \right)\,, \forall j\in[m]\,,
\]
\end{lemma}
where $\mathbb{I}$ denotes the indicator function.
\begin{proof}
We denote $\widetilde{\phi}_t = \phi(\bx_{t,1}) - \phi(\bx_{t,2})$.
Recall that we have assumed that $\norm{\phi(\bx_{t,1}) - \phi(\bx_{t,2})}_2 \leq 2$.
It is easy to verify that $\bV_{t-1} \succeq \frac{\lambda}{\kappa_\mu} I$ and hence $\bV_{t-1}^{-1} \preceq \frac{\kappa_\mu}{\lambda}I$.
Therefore, we have that $\norm{\widetilde{\phi}_t}_{\bV_{t-1}^{-1}}^2 \leq \frac{\kappa_\mu}{\lambda} \norm{\widetilde{\phi}_t}_{2}^2 \leq \frac{4\kappa_\mu}{\lambda}$. We choose $\lambda$ such that $\frac{4\kappa_\mu}{\lambda} \leq 1$, which ensures that $\norm{\widetilde{\phi}_t}_{\bV_{t-1}^{-1}}^2 \leq 1$.
Our proof here mostly follows from Lemma 11 of \cite{abbasi2011improved} and Lemma J.2 of \cite{wang2024onlinea}. To begin with, note that $x\leq 2\log(1+x)$ for $x\in[0,1]$. Denote $\bV_{t,j}=\sum_{s\in[t-1]:\atop i_s\in C_j} \widetilde{\phi}_s \widetilde{\phi}_s^\top + \frac{\lambda}{\kappa_\mu} \mathbf{I}$. Then we have that 
\begin{equation}
\begin{split}
\sum^T_{t=T_0}\mathbb{I}\{i_t\in C_j\}\norm{\widetilde{\phi}_t}_{\bV_{t-1}^{-1}}^2 &\leq \sum^T_{t=T_0} 2\log\left(1 + \mathbb{I}\{i_t\in C_j\}\norm{\widetilde{\phi}_t}_{\bV_{t-1}^{-1}}^2\right)\\
&= 2 \left( \log\det V_{T,j} - \log\det V \right)\\
&= 2 \log \frac{\det V_{T,j}}{\det V}\\
&\leq 2\log \left( \left( 1 + 4T \kappa_{\mu}/(d\lambda) \right)^{d} \right)\\
&= 2 d\log \left( 1 + 4T \kappa_{\mu}/(d\lambda) \right).
\end{split}
\end{equation}
The second inequality follows the same reasoning as \eqref{eq:upper:bound:det:Vt:V}.
This completes the proof.
\end{proof}

Now we are ready to prove Theorem \ref{thm: linear regret bound}.
First, we have
\begin{equation}
    R_T=\sum_{t=1}^T r_t\leq T_0 +\sum_{t=T_0}^T r_t\,,
\end{equation}
where we use that the reward at each round is bounded by 1.

Then, we only need to upper bound the regret after $T_0$. By Lemma \ref{T0 lemma}, we know that with probability at least $1-4\delta$, the algorithm can cluster all the users correctly, $\overline{C}_t=C_{j(i_t)}$, and the statements of all the above lemmas hold. We have that for any $t\geq T_0$:
\begin{equation}
\begin{split}
r_t &= f_{i_t}(\bx^*_t) - f_{i_t}(\bx_{t,1}) + f_{i_t}(\bx^*_t) - f_{i_t}(x_{t,2})\\
&\stackrel{(a)}{\leq} \overline\btheta_t^\top \left( \phi(\bx^*_t) - \phi(\bx_{t,1}) \right) + \frac{\beta_T}{\kappa_\mu}\norm{\phi(\bx^*_t) - \phi(\bx_{t,1})}_{\bV_{t-1}^{-1}} +  \overline\btheta_t^\top \left( \phi(\bx^*_t) - \phi(\bx_{t,2}) \right) + \frac{\beta_T}{\kappa_\mu}\norm{\phi(\bx^*_t) - \phi(\bx_{t,2})}_{\bV_{t-1}^{-1}}\\
&= \overline\btheta_t^\top \left( \phi(\bx^*_t) - \phi(\bx_{t,1}) \right) + \frac{\beta_T}{\kappa_\mu}\norm{\phi(\bx^*_t) - \phi(\bx_{t,1})}_{\bV_{t-1}^{-1}} + \\
&\qquad \overline\btheta_t^\top \left( \phi(\bx^*_t) - \phi(\bx_{t,1}) \right) + \overline\btheta_t^\top \left( \phi(\bx_{t,1}) - \phi(\bx_{t,2}) \right) + \frac{\beta_T}{\kappa_\mu}\norm{\phi(\bx^*_t) - \phi(\bx_{t,1}) + \phi(\bx_{t,1}) - \phi(\bx_{t,2})}_{\bV_{t-1}^{-1}}\\
&\stackrel{(b)}{\leq} 2 \overline\btheta_t^\top \left( \phi(x^*) - \phi(\bx_{t,1}) \right) + 2 \frac{\beta_T}{\kappa_\mu}\norm{\phi(x^*) - \phi(\bx_{t,1})}_{\bV_{t-1}^{-1}} + \\
&\qquad \overline\btheta_t^\top \left( \phi(\bx_{t,1}) - \phi(\bx_{t,2}) \right) + \frac{\beta_T}{\kappa_\mu}\norm{\phi(\bx_{t,1}) - \phi(\bx_{t,2})}_{\bV_{t-1}^{-1}}\\
&\stackrel{(c)}{\leq} 2 \overline\btheta_t^\top \left( \phi(\bx_{t,2}) - \phi(\bx_{t,1}) \right) + 2 \frac{\beta_T}{\kappa_\mu}\norm{\phi(\bx_{t,2}) - \phi(\bx_{t,1})}_{\bV_{t-1}^{-1}} + \\
&\qquad \overline\btheta_t^\top \left( \phi(\bx_{t,1}) - \phi(\bx_{t,2}) \right) + \frac{\beta_T}{\kappa_\mu}\norm{\phi(\bx_{t,1}) - \phi(\bx_{t,2})}_{\bV_{t-1}^{-1}}\\
&\leq \overline\btheta_t^\top \left( \phi(\bx_{t,2}) - \phi(\bx_{t,1}) \right) + 3 \frac{\beta_T}{\kappa_\mu}\norm{\phi(\bx_{t,2}) - \phi(\bx_{t,1})}_{\bV_{t-1}^{-1}} \\
&\stackrel{(d)}{\leq} 3 \frac{\beta_T}{\kappa_\mu}\norm{\phi(\bx_{t,1}) - \phi(\bx_{t,2})}_{\bV_{t-1}^{-1}} \\
\end{split}
\label{eq:upper:bound:inst:regret}
\end{equation}
Step $(a)$ follows from Lemma \ref{lemma:ucb:diff}. Step $(b)$ makes use of the triangle inequality.
Step $(c)$ follows from the way in which we choose the second arm $\bx_{t,2}$: $\bx_{t,2} = \arg\max_{x\in\mathcal{X}_t} \overline\btheta_t^\top \left( \phi(x) - \phi(\bx_{t,1}) \right) + \frac{\beta_T}{\kappa_\mu}\norm{\phi(x) - \phi(\bx_{t,1})}_{\bV_{t-1}^{-1}}$.
Step $(d)$ results from the way in which we select the first arm: $\bx_{t,1} = \arg\max_{x\in\mathcal{X}_t}\overline\btheta_t^\top \phi(x)$.

Then we have
\begin{align}
    \sum_{t=T_0}^T r_t &\leq 3 \frac{\beta_T}{\kappa_\mu}\sum_{t=T_0}^T\norm{\phi(\bx_{t,1}) - \phi(\bx_{t,2})}_{\bV_{t-1}^{-1}}\notag\\
    &=3 \frac{\beta_T}{\kappa_\mu}\sum_{t=T_0}^T\sum_{j\in[m]}\mathbb{I}\{i_t\in C_j\}\norm{\phi(\bx_{t,1}) - \phi(\bx_{t,2})}_{\bV_{t-1}^{-1}}\notag\\
    &\leq 3 \frac{\beta_T}{\kappa_\mu}\sqrt{\sum_{t=T_0}^T\sum_{j\in[m]}\mathbb{I}\{i_t\in C_j\}\sum_{t=T_0}^T\sum_{j\in[m]}\mathbb{I}\{i_t\in C_j\}\norm{\phi(\bx_{t,1}) - \phi(\bx_{t,2})}_{\bV_{t-1}^{-1}}^2}\notag\\
    &\leq 3 \frac{\beta_T}{\kappa_\mu}\sqrt{T\cdot m\cdot 2 d\log \left( 1 + 4T \kappa_{\mu}/(d\lambda) \right)}\,,
\end{align}
where in the second inequality we use the Cauchy-Swarchz inequality, and in the last step we use $\sum_{t=T_0}^T\sum_{j\in[m]}\mathbb{I}\{i_t\in C_j\}\leq T$ and Lemma \ref{lemma:concentration:square:std}.

Therefore, finally, we have with probability at least $1-4\delta$
\begin{align}
    R_T & \leq T_0+ 3 \frac{\beta_T}{\kappa_\mu}\sqrt{T\cdot m\cdot 2 d\log \left( 1 + 4T \kappa_{\mu}/(d\lambda) \right)}\notag\\
    &\leq O(u(\frac{d}{\kappa_\mu^2\tilde\lambda_x \gamma^2}+\frac{1}{\tilde\lambda_x^2})\log T+\frac{1}{\kappa_\mu}d\sqrt{mT})\notag\\
        &=O(\frac{1}{\kappa_\mu}d\sqrt{mT})\,,
\end{align}


\section{Proof of Theorem \ref{thm: neural regret bound}}
\label{app: proof neural}

\subsection{Auxiliary Definitions and Explanations}
\label{app:subsec:aux:defs}

\paragraph{Denifition of the NTK matrix $\mathbf{H}_j$ for cluster $j$.}
Recall that we use $T_j$ to denote the total number of iterations in which the users in cluster $j$ are served.
For cluster $j$, let $\{x_{(i)}\}_{i=1}^{T_j K}$ be a set of all $T_j \times K$ possible arm feature vectors: $\{x_{t,a}\}_{1\le t \le T_j, 1\le a \le K}$, where $i = K(t-1) + a$. 
Firstly, we define $\mathbf{h}_t = [f^j(x_{(i)})]_{i=1,\ldots,T_j K}^{\top}$, i.e., $\mathbf{h}_t$ is the $T_j K$-dimensional vector containing the reward function values of the arms corresponding to cluster $j$.
Next, define 
$$
\widetilde{\mathbf{H}}_{p,q}^{(1)} = \mathbf{\Sigma}_{p,q}^{(1)} = \langle x_{(p)}, x_{(q)}  \rangle, \\
\mathbf{A}_{p,q}^{(l)} =\begin{pmatrix}
	\mathbf{\Sigma}_{p,q}^{(l)} & \mathbf{\Sigma}_{p,q}^{(l)} &\\
	\mathbf{\Sigma}_{p,q}^{(l)} &\mathbf{\Sigma}_{q,q}^{(l)} &
\end{pmatrix},
$$
$$
\mathbf{\Sigma}_{p,q}^{(l+1)} = 2\mathbb{E}_{(u,v)\sim\mathcal{N}(0,\mathbf{A}_{p,q}^{(l)} )}[\max\{u,0\}\max\{v,0\}],
$$
$$
\widetilde{\mathbf{H}}_{p,q}^{(l+1)} = 2\widetilde{\mathbf{H}}_{p,q}^{(l)}\mathbb{E}_{(u,v)\sim\mathcal{N}(0,\mathbf{A}_{p,q}^{(l)} )}[\mathbbm{1}(u \ge 0)\mathbbm{1}(v \ge 0)] + \mathbf{\Sigma}_{p,q}^{(l+1)}.
$$
With these definitions, the NTK matrix for cluster $j$ is then defined as $\mathbf{H}_j = (\widetilde{\mathbf{H}}^{(L)} + \mathbf{\Sigma}^{(L)})/2$.

\paragraph{The Initial Parameters $\btheta_0$.}
Next, we discuss how the initial parameters $\btheta_0$ are obtained.
We adopt the same initialization method from \citet{zhang2020neural,zhou2020neural}.
Specifically, for each $l=1,\ldots,L-1$, let 
$\mathbf{W}_l=\left(
\begin{array}{cc} 
  \mathbf{W} & \mathbf{0} \\ 
  \mathbf{0} & \mathbf{W} 
\end{array} 
\right)$
in which every entry of $\mathbf{W}$ is independently and randomly sampled from $\mathcal{N}(0, 4/m_{\text{NN}})$, and choose $\mathbf{W}_L=(\mathbf{w}^{\top},-\mathbf{w}^{\top})$ in which every entry of $\mathbf{w}$ is independently and randomly sampled from $\mathcal{N}(0,2/m_{\text{NN}})$.

\paragraph{Justifications for Assumption \ref{assumption:main:neural}.}
The last assumption in Assumption \ref{assumption:main:neural}, together with the way we initialize $\theta_0$ as discussed above, ensures that the initial output of the NN is $0$: $h(x;\theta_0)=0,\forall x\in\mathcal{X}$.
The assumption of $x_{j}=x_{j+d/2}$ from Assumption \ref{assumption:main:neural} is a mild assumption which is commonly adopted by previous works on neural bandits \cite{zhou2020neural,zhang2020neural}. 
To ensure that this assumption holds, for any arm $x$, we can always firstly normalize it such that $||x|| = 1$, and then construct a new context $x' = (x^\top,x^\top)^\top/\sqrt{2}$ to satisfy this assumption \cite{zhou2020neural}.


\subsection{Proof}
\label{app:subsec:proof:neural:real:proof}

To begin with, we first list the specific conditions we need for the width $m_{\text{NN}}$ of the NN:
\begin{equation}
	\begin{split}
	&m_{\text{NN}} \geq C T^4K^4 L^6\log(T^2K^2 L/\delta) / \lambda_0^4,\\
	&m_{\text{NN}}(\log m)^{-3} \geq C \kappa_\mu^{-3} T^{8} L^{21} \lambda^{-5} ,\\
	&m_{\text{NN}}(\log m_{\text{NN}})^{-3} \geq C \kappa_\mu^{-3} T^{14} L^{21} \lambda^{-11} L_\mu^6,\\
	&m_{\text{NN}}(\log m_{\text{NN}})^{-3} \geq C T^{14} L^{18} \lambda^{-8},
	\end{split}
	\label{eq:conditions:on:m}
\end{equation}
for some absolute constant $C>0$.
To ease exposition, we express these conditions above as 
$m_{\text{NN}} \geq \text{poly}(T, L, K, 1/\kappa_\mu, L_\mu, 1/\lambda_0, 1/\lambda, \log(1/\delta))$.
% }

In our proof here, we use the gradient of the NN  at $\btheta_0$ to derive the feature mapping for the arms, i.e., we let $\phi(\bx) = g(\bx;\btheta_0) / \sqrt{m_{\text{NN}}}$.
We use $\hat{\btheta}_{i,t}$ to denote the paramters of the NN after training in iteration $t$ (see Algorithm \ref{algo:neural:dueling:bandits}).

We use the following lemma to show that for every cluster $j\in\mathcal{C}$, its reward function $f^j$ can be expressed as a linear function with respect to the initial gradient $g(\bx;\btheta_0)$.
\begin{lemma}[Lemma B.3 of \cite{zhang2020neural}]
\label{lemma:linear:utility:function}
As long as the width $m$ of the NN is large enough:
\[
	m_{\text{NN}} \geq C_0 T^4K^4 L^6\log(T^2K^2 L/\delta) / \lambda_0^4,
\]
then for all clusters $j\in[m]$, with probability of at least $1-\delta$, there exits a $\btheta^j_{f}$ such that 
\[ 
	f^j(\bx) = \langle g(\bx;\btheta_0), \btheta^j_{f} - \btheta_0 \rangle, \qquad \sqrt{m_{\text{NN}}} \norm{\btheta^j_{f} - \btheta_0}_2 \leq \sqrt{2\mathbf{h}_j^{\top} \mathbf{H}_j^{-1} \mathbf{h}_j} \leq B.
\]
for all $\bx\in\mathcal{X}_{t}$, $t\in[T]$ with $i_t\in C_{j}$.
\end{lemma}

Lemma \ref{lemma:linear:utility:function} is the formal statement of Lemma \ref{lemma:linear:utility:function:informal} from Sec.~\ref{subsec:problem:setting:neural}.
Note that the constant $B$ is applicable to all $m$ clusters.

The following lemma converts our assumption about cluster separation (Assumption \ref{assumption:gap:neural:bandits}) into the difference between the linearized parameters for different clusters.
\begin{lemma}
\label{lemma:neural:gap:theta}
If users $i$ and $l$ belong to different clusters, then we have that
\[
\sqrt{m_{\text{NN}}} \norm{\btheta_{f,i} - \btheta_{f,l}} \geq \gamma'.
\]
\end{lemma}
\begin{proof}
To begin with, Lemma \ref{lemma:linear:utility:function} tells us that
\begin{equation}
\begin{split}
|f_i(\bx) - f_l(\bx)| = | \langle g(\bx;\btheta_0),  \btheta_{f,i} - \btheta_{f,l}\rangle | \leq \norm{g(\bx;\btheta_0)} \norm{\btheta_{f,i} - \btheta_{f,l}}.
\end{split}
\end{equation}
This leads to
\begin{equation}
\begin{split}
\norm{\btheta_{f,i} - \btheta_{f,l}} \geq \frac{|f_i(\bx) - f_l(\bx)|}{\norm{g(\bx;\btheta_0)}} \geq \frac{\gamma'}{\sqrt{m_{\text{NN}}}} ,
\end{split}
\end{equation}
in which we have made use of Assumption \ref{assumption:gap:neural:bandits} and our assumption that $\frac{1}{m_{\text{NN}}}\langle g(\bx;\btheta_0), g(\bx;\btheta_0) \rangle \leq 1$ in the last inequality.
This completes the proof.
\end{proof}

The following lemma shows that for every user, the output of the NN trained using its own local data can be approximated by a linear function.
\begin{lemma}
\label{lemma:bound:approx:error:linear:nn:duel:individual}
    Let $\varepsilon'_{m_{\text{NN}},t} \triangleq C_2 m_{\text{NN}}^{-1/6}\sqrt{\log m_{\text{NN}}} L^3 \left(\frac{t}{\lambda}\right)^{4/3}$ where $C_2>0$ is an absolute constant.
    Then
    \[
   		|\langle g(\bx;\btheta_0), \hat{\btheta}_{i,t} - \btheta_0 \rangle - h(\bx;\hat{\btheta}_{i,t}) | \leq \varepsilon'_{m_{\text{NN}},t}, \,\,\, \forall t\in[T], \bx,\bx'\in\mathcal{X}_t.
    \]
\end{lemma}
\begin{proof}
This lemma can be proved following a similar line of proof as Lemma 1 from \citet{verma2024neural}.
Here the $t$ in $\varepsilon'_{m_{\text{NN}},t}$ can in fact be replaced by $T_{i,t} \leq t$, however, we have simply used its upper bound $t$ for simplicity.
\end{proof}

\begin{lemma}
\label{lemma:conf:ellip:neural}
Let $\beta_T \triangleq \frac{1}{\kappa_\mu} \sqrt{ \widetilde{d} + 2\log(u/\delta)}$.
Assuming that the conditions on $m_{\text{NN}}$ from \cref{eq:conditions:on:m} are satisfied.
With probability of at least $1-\delta$, we have that
\[
	\sqrt{m_{\text{NN}}} \norm{\btheta_{f,i} - \hat{\btheta}_{i,t}}_{2} \leq  \frac{\beta_T + B \sqrt{\frac{\lambda}{\kappa_\mu}} + 1}{\sqrt{\lambda_{\min}(\bV_{i,t-1})}}, \qquad \forall t\in[T].
\]
where $\bV_{i,t-1}=\frac{\lambda}{\kappa_\mu} \mathbf{I}+\sum_{s\in[t-1]\atop i_s=i}(\phi(\bx_{s,1}) - \phi(\bx_{s,2}))(\phi(\bx_{s,1}) - \phi(\bx_{s,2}))^\top$, $\phi(\bx) = \frac{1}{\sqrt{m_{\text{NN}}}} g(\bx;\btheta_0)$, and $T_{i,t}$ denotes the number of rounds of seeing user $i$ in the first $t$ rounds.
\end{lemma}
\begin{proof}
In iteration $t$, for any user $i\in\mathcal{U}$, the user leverages its current history of observations $\{(\bx_{s,1}, \bx_{s,2}, y_s)\}_{s\in[t-1], i_s = i}$ to train the NN by minimizing the loss function (\eqref{eq:loss:func:individial}), to obtain the NN parameters $\hat{\btheta}_{i,t}$.
Note that the NN has been trained when the most recent observation in $\{(\bx_{s,1}, \bx_{s,2}, y_s)\}_{s\in[t-1], i_s = i}$ was collected, i.e., the last time when user $i$ was encountered.
Of note, according to Lemma \ref{lemma:linear:utility:function}, the latent reward function of user $i$ can be expressed as $f_i(\bx) = \langle g(\bx;\btheta_0), \btheta_{f,i} - \btheta_0 \rangle$.
Therefore, from the perspective of each individual user $i$, the user is faced with a \emph{neural dueling bandit} problem instance.
As a result, we can modifying the proof of Lemma 6 from \citet{verma2024neural} to show that
with probability of at least $1-\delta$,
\[
	\sqrt{m_{\text{NN}}} \norm{\btheta_{f,i} - \hat{\btheta}_{i,t}}_{\bV_{i,t-1}} \leq  \beta_T + B \sqrt{\frac{\lambda}{\kappa_\mu}} + 1, \qquad \forall t\in[T], i\in\mathcal{U}.
\]
Here in our definition of $\beta_T \triangleq \frac{1}{\kappa_\mu} \sqrt{ \widetilde{d} + 2\log(u/\delta)}$, we have replaced the error probability $\delta$ (from \citet{verma2024neural}) by $\delta/u$ to account for the use of an extra union bound over all $u$ users.

This allows us to show that
\begin{equation}
\begin{split}
\sqrt{m_{\text{NN}}} \norm{\btheta_{f,i} - \hat{\btheta}_{i,t}}_2 &\leq \frac{\sqrt{m_{\text{NN}}} \norm{\btheta_{f,i} - \hat{\btheta}_{i,t}}_{\bV_{i,t-1}}}{\sqrt{\lambda_{\min}(\bV_{i,t-1})}}\\
&\leq \frac{\beta_T + B \sqrt{\frac{\lambda}{\kappa_\mu}} + 1}{\sqrt{\lambda_{\min}(\bV_{i,t-1})}}
\end{split}
\end{equation}
This completes the proof.
\end{proof}


\begin{lemma}\label{T0 lemma neural}
    With the carefully designed edge deletion rule in Algorithm \ref{algo:neural:dueling:bandits}, after 
\begin{equation*}
    \begin{aligned}
        T_0&\triangleq 16u\log(\frac{u}{\delta})+4u \max\left\{\frac{32 \left( \widetilde{d} + 2\log(u/\delta)\right)}{\tilde{\lambda}_x \gamma^2 \kappa_\mu^2},  \frac{16}{\tilde{\lambda}_x^2}\log(\frac{24u d m^2(L-1)}{\tilde{\lambda}_x^2\delta}) \right\}\\
        &= O\left(u \left( \frac{ \widetilde{d}}{\kappa_\mu^2\tilde{\lambda}_x \gamma^2} + \frac{1}{\tilde{\lambda}_x^2} \right)\log(\frac{1}{\delta}) \right),
    \end{aligned}
\end{equation*}
rounds, with probability at least $1-3\delta$ for some $\delta\in(0,\frac{1}{3})$, CONDB can cluster all the users correctly.
\end{lemma}
\begin{proof}
Recall that we use $p = dm_{\text{NN}} + m_{\text{NN}}^2(L-1) + m_{\text{NN}}$ to denote the total number of parameters of the NN.
Similar to the proof of Lemma \ref{T0 lemma}, with the item regularity assumption stated in Assumption \ref{assumption3}, Lemma J.1 in \cite{wang2024onlinea}, together with Lemma 7 in \cite{li2018online} (note that when using these technical results, we use $g(\bx;\btheta)/\sqrt{m_{\text{NN}}}$ as the feature vector to replace the original feature vector of $\bx$), and applying a union bound, with probability at least $1-\delta$, for all $i\in\mathcal{U}$, at any $t$ such that $T_{i,t}\geq\frac{16}{\tilde{\lambda}_x^2}\log(\frac{8up}{\tilde{\lambda}_x^2\delta})$, we have:
\begin{equation}
    \lambda_{\text{min}}(\bV_{i,t})\geq2\tilde{\lambda}_x T_{i,t}\,.
    \label{min eigen}
\end{equation}
Note that compared with the proof of \ref{T0 lemma}, in the lower bound on $T_{i,t}$ here, we have replaced the dimension $d$ by $p$. This has led to a logarithmic dependence 
on the width $m_{\text{NN}}$ of the NN.
To simplify the exposition, using the fact that $p \geq 3dm_{\text{NN}}^2(L-1)$, we replace this condition on $T_{i,t}$ by a slightly stricter condition: $T_{i,t}\geq\frac{16}{\tilde{\lambda}_x^2}\log(\frac{8u \times 3dm_{\text{NN}}^2(L-1)}{\tilde{\lambda}_x^2\delta}) = \frac{16}{\tilde{\lambda}_x^2}\log(\frac{24u d m_{\text{NN}}^2(L-1)}{\tilde{\lambda}_x^2\delta})$.

Then, together with Lemma \ref{lemma:conf:ellip:neural}, we have: if 
$T_{i,t}\geq\frac{16}{\tilde{\lambda}_x^2}\log(\frac{8u \times 3dm_{\text{NN}}^2(L-1)}{\tilde{\lambda}_x^2\delta})$, 
then with probability $\geq 1-2\delta$, we have:
\begin{align}
    \sqrt{m_{\text{NN}}} \norm{\hat{\btheta}_{i,t}-\btheta^{j(i)}}
    &\leq \frac{\beta_T + B \sqrt{\frac{\lambda}{\kappa_\mu}} + 1}{\sqrt{\lambda_{\min}(\bV_{i,t-1})}} \leq \frac{\beta_T + B \sqrt{\frac{\lambda}{\kappa_\mu}} + 1}{\sqrt{2\tilde{\lambda}_x T_{i,t}}}\notag\,.
\end{align}

Now, let
\begin{equation}
    \frac{\beta_T + B \sqrt{\frac{\lambda}{\kappa_\mu}} + 1}{\sqrt{2\tilde{\lambda}_x T_{i,t}}}<\frac{\gamma}{4}\,,
\end{equation}

Note that in Algorithm \ref{algo:neural:dueling:bandits}, we have defined the funciton $f$ as 
\begin{equation}
f(T_{i,t}) \triangleq \frac{\beta_T + B \sqrt{\frac{\lambda}{\kappa_\mu}} + 1}{\sqrt{2\tilde{\lambda}_x T_{i,t}}}
\end{equation}
This immediately leads to
\begin{equation}
\sqrt{m_{\text{NN}}} \norm{\hat{\btheta}_{i,t}-\btheta^{j(i)}} \leq f(T_{i,t}) < \frac{\gamma}{4}.
\end{equation}

For simplicity, now let $B \sqrt{\frac{\lambda}{\kappa_\mu}} + 1 \leq \beta_T$ which is typically satisfied. This allows us to show that
\begin{equation}
    T_{i,t} > \frac{32\beta_T^2}{\tilde{\lambda}_x \gamma^2} = \frac{32 \left(\frac{1}{\kappa_\mu} \sqrt{ \widetilde{d} + 2\log(u/\delta)}\right)^2}{\tilde{\lambda}_x \gamma^2} = \frac{32 \left( \widetilde{d} + 2\log(u/\delta)\right)}{\tilde{\lambda}_x \gamma^2 \kappa_\mu^2}.
\label{condition final neural}
\end{equation}

Combining both conditions on $T_{i,t}$ together, we have that
\begin{equation}
T_{i,t}\geq \max\left\{\frac{32 \left( \widetilde{d} + 2\log(u/\delta)\right)}{\tilde{\lambda}_x \gamma^2 \kappa_\mu^2},  \frac{16}{\tilde{\lambda}_x^2}\log(\frac{24u d m_{\text{NN}}^2(L-1)}{\tilde{\lambda}_x^2\delta}) \right\}
\end{equation}

By Lemma 8 in \cite{li2018online} and Assumption \ref{assumption2} of user arrival uniformness, we have that for all
\begin{equation*}
    \begin{aligned}
        T_0&\triangleq 16u\log(\frac{u}{\delta})+4u \max\left\{\frac{32 \left( \widetilde{d} + 2\log(u/\delta)\right)}{\tilde{\lambda}_x \gamma^2 \kappa_\mu^2},  \frac{16}{\tilde{\lambda}_x^2}\log(\frac{24u d m_{\text{NN}}^2(L-1)}{\tilde{\lambda}_x^2\delta}) \right\}\\
        &= O\left(u \left( \frac{ \widetilde{d}}{\kappa_\mu^2\tilde{\lambda}_x \gamma^2} + \frac{1}{\tilde{\lambda}_x^2} \right)\log(\frac{1}{\delta}) \right),
    \end{aligned}
\end{equation*}
the condition in Eq.(\ref{condition final neural}) is satisfied with probability at least $1-\delta$.

Therefore we have that for all $t\geq T_0$, with probability $\geq 1-3\delta$:
\begin{equation}
    \sqrt{m_{\text{NN}}}\norm{\hat{\btheta}_{i,t}-\btheta^{j(i)}}_2<\frac{\gamma}{4}\,,\forall{i\in\mathcal{U}}\,.
\end{equation}
Finally, we show that as long as the condition $\sqrt{m_{\text{NN}}}\norm{\hat{\btheta}_{i,t}-\btheta^{j(i)}}_2<\frac{\gamma}{4}\,,\forall{i\in\mathcal{U}}$, our algorithm can cluster all the users correctly.

First, we show that when the edge $(i,l)$ is deleted, user $i$ and user $j$ must belong to different \gtclusters{}, i.e., $\norm{\btheta_{f,i}-\btheta_{f,l}}_2>0$. 
This is because by the deletion rule of the algorithm, the concentration bound, and triangle inequality
\begin{align}
   &\sqrt{m_{\text{NN}}}\norm{\btheta_{f,i}-\btheta_{f,l}}_2=\sqrt{m_{\text{NN}}}\norm{\btheta^{j(i)}-\btheta^{j(l)}}_2\notag\\
   &\geq \sqrt{m_{\text{NN}}}\norm{\hat{\btheta}_{i,t}-\hat{\btheta}_{l,t}}_2 - \sqrt{m_{\text{NN}}}\norm{\btheta^{j(l)}-\hat{\btheta}_{l,t}}_2 - \sqrt{m_{\text{NN}}}\norm{\btheta^{j(i)}-\hat{\btheta}_{i,t}}_2\notag\\
   &\geq \sqrt{m_{\text{NN}}}\norm{\hat{\btheta}_{i,t}-\hat{\btheta}_{l,t}}_2-f(T_{i,t})-f(T_{l,t})>0 \,.
\end{align}
Second, we can show that if 
$|f_i(\bx) - f_l(\bx)| \geq \gamma',\forall \bx\in\mathcal{X}$,
meaning that user $i$ and user $l$ are not in the same \gtcluster, CONDB will delete the edge $(i,l)$ after $T_0$.
Note that when user $i$ and user $l$ are not in the same \gtcluster, Lemma \ref{lemma:neural:gap:theta} tells us that $\sqrt{m_{\text{NN}}} \norm{\btheta_{f,i} - \btheta_{f,l}} \geq \gamma'$.
Then we have that
\begin{align}
    \sqrt{m_{\text{NN}}}\norm{\hat\btheta_{i,t}-\hat{\btheta}_{l,t}}&\geq \sqrt{m_{\text{NN}}}\norm{\btheta_{f,i}-\btheta_{f,l}}- \sqrt{m_{\text{NN}}}\norm{\hat{\btheta}_{i,t}-\btheta^{j(i)}}_2-\sqrt{m_{\text{NN}}}\norm{\hat{\btheta}_{l,t}-\btheta^{j(l)}}_2\notag\\
    &>\gamma-\frac{\gamma}{4}-\frac{\gamma}{4}\notag\\
    &=\frac{\gamma}{2}>f(T_{i,t})+f(T_{l,t})\,,
\end{align}
which will trigger the edge deletion rule to delete edge $(i,l)$. 
This completes the proof.
\end{proof}

Then, we prove the following lemmas for the cluster-based statistics.
\begin{lemma}\label{lemma:concentration:theta cluster:neural}
Assuming that the conditions on $m$ from \cref{eq:conditions:on:m} are satisfied.
With probability at least $1-4\delta$ for some $\delta\in(0,1/4)$, at any $t\geq T_0$:
\[
	\sqrt{m_{\text{NN}}} \norm{\btheta_{f,i_{t}} - \overline{\btheta}_{t}}_{\bV_{t-1}} \leq  \beta_T + B \sqrt{\frac{\lambda}{\kappa_\mu}} + 1, \qquad \forall t\in[T].
\]
\end{lemma}
\begin{proof}
To begin with, note that by Lemma \ref{T0 lemma neural}, we have that with probability of at least $1-3\delta$, all users are clustered correctly, i.e., $\overline{C}_t=C_{j(i_t)}, \forall t\geq T_0$.
Note that according to our Algorithm \ref{algo:neural:dueling:bandits}, in iteration $t$, we select the pair of arms using all the data collected by all users in cluster $\overline{C}_t$.
That is, $\overline{\btheta}_{t}$ represents the NN parameters trained using the data from all users in the cluster $\overline{C}_t$ (i.e., $\{(\bx_{s,1}, \bx_{s,2}, y_s)\}_{s\in[t-1], i_s\in \overline C_t}$), and $\bV_t$ also contains the data from all users in this cluster $\overline{C}_t$.
Therefore, in iteration $t$, we are effectively following a neural dueling bandit algorithm using $\{(\bx_{s,1}, \bx_{s,2}, y_s)\}_{s\in[t-1], i_s\in \overline C_t}$ as the current observation history.
This allows us to leverage the proof of Lemma 6 from \citet{verma2024neural} to complete the proof.
\end{proof}

\begin{lemma}
\label{lemma:bound:approx:error:linear:nn:duel}
    Let $\varepsilon'_{m_{\text{NN}},t} \triangleq C_2 m_{\text{NN}}^{-1/6}\sqrt{\log m_{\text{NN}}} L^3 \left(\frac{t}{\lambda}\right)^{4/3}$ where $C_2>0$ is an absolute constant.
    Then
    \[
   		|\langle g(\bx;\btheta_0) -g(\bx';\btheta_0), \overline{\btheta}_t - \btheta_0 \rangle - (h(\bx;\overline{\btheta}_t) - h(\bx';\overline{\btheta}_t)) | \leq  2\varepsilon'_{m_{\text{NN}},t}, \,\,\, \forall t\in[T], \bx,\bx'\in\mathcal{X}_t.
    \]
\end{lemma}
\begin{proof}
This lemma can be proved following a similar line of proof as Lemma 1 from \citet{verma2024neural}.
\end{proof}

\begin{lemma}
	\label{thm:confBound:neural}  
    Let $\delta\in(0,1)$, $\varepsilon'_{m_{\text{NN}},t} \doteq C_2 m_{\text{NN}}^{-1/6}\sqrt{\log m_{\text{NN}}} L^3 \left(\frac{t}{\lambda}\right)^{4/3}$ for some absolute constant $C_2>0$.
    As long as $m_{\text{NN}} \geq \text{poly}(T, L, K, u, 1/\kappa_\mu, L_\mu, 1/\lambda_0, 1/\lambda, \log(1/\delta))$, then with probability of at least $1-\delta$, at any $t\geq T_0$,
    \[
        |\left[f_{i_t}(\bx) - f_{i_t}(\bx')\right] - \left[h(\bx;\overline{\btheta}_t) - h(\bx';\overline{\btheta}_t)\right]| \leq \nu_T \sigma_{t-1}(\bx, \bx') + 2\varepsilon'_{m_{\text{NN}},t},
    \]
    for all $\bx,\bx'\in\mathcal{X}_t, t\in[T]$. 
\end{lemma}
\begin{proof}
	Denote $\phi(\bx) = \frac{1}{\sqrt{m_{\text{NN}}}} g(\bx;\btheta_0)$.
	Recall that \cref{lemma:linear:utility:function} tells us that $f_{i_t}(\bx) = \langle g(\bx;\btheta_0), \btheta_{f,i_t} - \btheta_0 \rangle=\langle \phi(\bx), \btheta_{f,i_t} - \btheta_0 \rangle$ for all $\bx\in\mathcal{X}_t,t\in[T]$.
	To begin with, for all $\bx,\bx'\in\mathcal{X}_t,t\in[T]$ we have that
		\begin{equation}
		\begin{split}
			|&f_{i_t}(\bx) - f_{i_t}(\bx') - \langle g(\bx;\btheta_0) - g(\bx';\btheta_0), \overline{\btheta}_t - \btheta_0 \rangle| \\
			&= |\langle g(\bx;\btheta_0) - g(\bx';\btheta_0), \btheta_{f,i_t} - \theta_0 \rangle - \langle g(\bx;\btheta_0) - g(\bx';\btheta_0), \overline{\btheta}_t - \btheta_0 \rangle|\\
			&= |\langle g(\bx;\btheta_0) - g(\bx';\btheta_0), \btheta_{f,i_t} - \overline{\btheta}_t \rangle  \rangle|\\
			&= |\langle  \phi(\bx)-\phi(\bx'), \sqrt{m_{\text{NN}}}\left( \btheta_{f,i_t} - \overline{\btheta}_t\right) \rangle  |\\
			&\leq \norm{\left(\phi(\bx)-\phi(\bx')\right)}_{\bV_{t-1}^{-1}} \sqrt{m_{\text{NN}}}\norm{\btheta_{f,i_t} - \overline{\btheta}_t}_{\bV_{t-1}}\\
			&\leq \norm{\left(\phi(\bx)-\phi(\bx')\right)}_{\bV_{t-1}^{-1}} \left( \beta_T + B \sqrt{\frac{\lambda}{\kappa_\mu}} + 1 \right),
		\end{split}
		\label{eq:diff:between:func:and:linear:approx:dueling}
		\end{equation}
	in which we have used Lemma \ref{lemma:concentration:theta cluster:neural} in the last inequality.
	Now making use of the equation above and \cref{lemma:bound:approx:error:linear:nn:duel}, we have that 
	\begin{equation}
    \begin{split}
		|f_{i_t}(\bx) - f_{i_t}(\bx') &- (h(\bx;\btheta_t) - h(\bx';\btheta_t))| \\
		&= | f_{i_t}(\bx) - f_{i_t}(\bx') - \langle g(\bx;\btheta_0) - g(\bx';\btheta_0), \overline{\btheta}_t - \btheta_0 \rangle \\
		&\qquad\qquad\qquad + \langle g(\bx;\btheta_0) - g(\bx';\btheta_0), \overline{\btheta}_t - \btheta_0 \rangle - (h(\bx;\overline{\btheta}_t) - h(\bx';\overline{\btheta}_t)) |\\
        &\leq | f_{i_t}(\bx) - f_{i_t}(\bx') - \langle g(\bx;\btheta_0) - g(\bx';\btheta_0), \overline{\btheta}_t - \btheta_0 \rangle | \\
		&\qquad\qquad\qquad + |\langle g(\bx;\btheta_0) - g(\bx';\btheta_0), \overline{\btheta}_t - \btheta_0 \rangle - (h(\bx;\overline{\btheta}_t) - h(\bx';\overline{\btheta}_t)) |\\
		&\leq \norm{\frac{1}{\sqrt{m_{\text{NN}}}}\left(\phi(\bx)-\phi(\bx')\right)}_{\bV_{t-1}^{-1}} \left( \beta_T + B \sqrt{\frac{\lambda}{\kappa_\mu}} + 1 \right) + 2\varepsilon'_{m_{\text{NN}},t}.\\
    \end{split}
    \end{equation}
	
	This completes the proof.
\end{proof}

We also prove the following lemma to upper bound the summation of squared norms which will be used in proving the final regret bound.
\begin{lemma}
With probability at least $1-4\delta$, we have
\label{lemma:concentration:square:std:neural}
\[
\sum^T_{t=T_0}\mathbb{I}\{i_t\in C_j\} \norm{\phi(\bx_{t,1}) - \phi(\bx_{t,2})}_{\bV_{t-1}^{-1}}^2 \leq 16 \widetilde{d}\,, \forall j\in[m]\,,
\]
\end{lemma}
where $\mathbb{I}$ denotes the indicator function.
\begin{proof}
We denote $\widetilde{\phi}_t = \phi(\bx_{t,1}) - \phi(\bx_{t,2})$.
Note that we have defined $\phi(\bx) = \frac{1}{\sqrt{m_{\text{NN}}}}g(\bx;\btheta_0)$.
Here we assume that $\norm{\phi(\bx_{t,1}) - \phi(\bx_{t,2})}_2 = \frac{1}{\sqrt{m_{\text{NN}}}}\norm{g(\bx_{t,1};\btheta_0) - g(\bx_{t,2};\btheta_0)}_{2} \leq 2$.
Replacing $2$ by an absolute constant $c_0$ would only change the final regret bound by a constant factor, so we omit it for simplicity.

It is easy to verify that $\bV_{t-1} \succeq \frac{\lambda}{\kappa_\mu} I$ and hence $\bV_{t-1}^{-1} \preceq \frac{\kappa_\mu}{\lambda}I$.
Therefore, we have that $\norm{\widetilde{\phi}_t}_{\bV_{t-1}^{-1}}^2 \leq \frac{\kappa_\mu}{\lambda} \norm{\widetilde{\phi}_t}_{2}^2 \leq \frac{4\kappa_\mu}{\lambda}$. We choose $\lambda$ such that $\frac{4\kappa_\mu}{\lambda} \leq 1$, which ensures that $\norm{\widetilde{\phi}_t}_{\bV_{t-1}^{-1}}^2 \leq 1$.
Our proof here mostly follows from Lemma 11 of \cite{abbasi2011improved} and Lemma J.2 of \cite{wang2024onlinea}. To begin with, note that $x\leq 2\log(1+x)$ for $x\in[0,1]$. Denote $\bV_{t,j}=\sum_{s\in[t-1]:\atop i_s\in C_j} \widetilde{\phi}_s \widetilde{\phi}_s^\top + \frac{\lambda}{\kappa_\mu} \mathbf{I}$. Then we have that 
\begin{equation}
\begin{split}
\sum^T_{t=T_0}\mathbb{I}\{i_t\in C_j\} \norm{\widetilde{\phi}_t}_{\bV_{t-1}^{-1}}^2 &\leq \sum^T_{t=T_0} 2\log\left(1 + \mathbb{I}\{i_t\in C_j\} \norm{\widetilde{\phi}_t}_{\bV_{t-1}^{-1}}^2\right)\\
&\leq 16 \log\det\left(\frac{\kappa_\mu}{\lambda}\mathbf{H}' + \mathbf{I}\right) \\
&\triangleq 16 \widetilde{d}.
\end{split}
\end{equation}
The second inequality follows from the proof in Section A.3 from \citet{verma2024neural}.
This completes the proof.
\end{proof}



Now we are ready to prove Theorem \ref{thm: neural regret bound}.
To begin with, we have that
$
    R_T=\sum_{t=1}^T r_t\leq T_0 +\sum_{t=T_0}^T r_t$.

Then, we only need to upper-bound the regret after $T_0$. By Lemma \ref{T0 lemma neural}, we know that with probability at least $1-4\delta$, the algorithm can cluster all the users correctly, $\overline{C}_t=C_{j(i_t)}$, and the statements of all the above lemmas hold. We have that for any $t\geq T_0$:
    
To simplify exposion here, we denote $\beta_T' \triangleq \beta_T + B \sqrt{\lambda / \kappa_\mu} + 1$.
\begin{equation}
\begin{split}
r_t &= f_{i_t}(\bx^*_t) - f_{i_t}(\bx_{t,1}) + f_{i_t}(\bx^*_t) - f_{i_t}(x_{t,2})\\
&\stackrel{(a)}{\leq} \langle g(\bx^*_t;\btheta_0) - g(\bx_{t,1};\btheta_0), \overline\btheta_t-\btheta_0 \rangle + \beta_T' \norm{\phi(\bx^*_t) - \phi(\bx_{t,1})}_{\bV_{t-1}^{-1}} + \\
&\qquad \langle g(\bx^*_t;\btheta_0) - g(\bx_{t,2};\btheta_0), \overline\btheta_t-\btheta_0 \rangle + \beta_T'\norm{\phi(\bx^*_t) - \phi(\bx_{t,2})}_{\bV_{t-1}^{-1}}\\
&= \langle g(\bx^*_t;\btheta_0) - g(\bx_{t,1};\btheta_0), \overline\btheta_t-\btheta_0 \rangle + \beta_T' \norm{\phi(\bx^*_t) - \phi(\bx_{t,1})}_{\bV_{t-1}^{-1}} + \\
&\qquad \langle g(\bx^*_t;\btheta_0) - g(\bx_{t,1};\btheta_0), \overline\btheta_t-\btheta_0 \rangle + \langle g(\bx_{t,1};\btheta_0) - g(\bx_{t,2};\btheta_0), \overline\btheta_t-\btheta_0 \rangle + \\
&\qquad \beta_T'\norm{\phi(\bx^*_t) - \phi(\bx_{t,1}) + \phi(\bx_{t,1}) - \phi(\bx_{t,2})}_{\bV_{t-1}^{-1}}\\
&\stackrel{(b)}{\leq} 2 \langle g(\bx^*_t;\btheta_0) - g(\bx_{t,1};\btheta_0), \overline\btheta_t-\btheta_0 \rangle + 2 \beta_T' \norm{\phi(\bx^*_t) - \phi(\bx_{t,1})}_{\bV_{t-1}^{-1}} + \\
&\qquad \langle g(\bx_{t,1};\btheta_0) - g(\bx_{t,2};\btheta_0), \overline\btheta_t-\btheta_0 \rangle + \beta_T'\norm{\phi(\bx_{t,1}) - \phi(\bx_{t,2})}_{\bV_{t-1}^{-1}}\\
&\stackrel{(c)}{\leq} 2 h(\bx^*_t;\overline{\btheta}_t) - 2 h(\bx_{t,1};\overline{\btheta}_t) + 4\varepsilon'_{m_{\text{NN}},t} + 2 \beta_T' \norm{\phi(\bx^*_t) - \phi(\bx_{t,1})}_{\bV_{t-1}^{-1}} + \\
&\qquad h(\bx_{t,1};\overline{\btheta}_t) - h(\bx_{t,2};\overline{\btheta}_t) + 2\varepsilon'_{m_{\text{NN}},t} + \beta_T'\norm{\phi(\bx_{t,1}) - \phi(\bx_{t,2})}_{\bV_{t-1}^{-1}}\\
&\stackrel{(d)}{\leq} 2 h(\bx_{t,2};\overline{\btheta}_t) - 2 h(\bx_{t,1};\overline{\btheta}_t) + 2 \beta_T' \norm{\phi(\bx_{t,2}) - \phi(\bx_{t,1})}_{\bV_{t-1}^{-1}} + \\
&\qquad h(\bx_{t,1};\overline{\btheta}_t) - h(\bx_{t,2};\overline{\btheta}_t) + 6\varepsilon'_{m_{\text{NN}},t} + \beta_T'\norm{\phi(\bx_{t,1}) - \phi(\bx_{t,2})}_{\bV_{t-1}^{-1}}\\
&= h(\bx_{t,2};\overline{\btheta}_t) - h(\bx_{t,1};\overline{\btheta}_t) + 3 \beta_T' \norm{\phi(\bx_{t,1}) - \phi(\bx_{t,2})}_{\bV_{t-1}^{-1}} + 6\varepsilon'_{m_{\text{NN}},t}\\
&\stackrel{(e)}{\leq} 3 \beta_T' \norm{\phi(\bx_{t,1}) - \phi(\bx_{t,2})}_{\bV_{t-1}^{-1}} + 6\varepsilon'_{m_{\text{NN}},t}\\
\end{split}
\label{eq:upper:bound:inst:regret:neural}
\end{equation}

Step $(a)$ follows from Equation \ref{eq:diff:between:func:and:linear:approx:dueling}, step $(b)$ results from the triangle inequality, step $(c)$ has made use of Lemma \ref{lemma:bound:approx:error:linear:nn:duel}.
Step $(d)$ follows from the way in which we choose the second arm $\bx_{t,2}$: $\bx_{t,2} = \arg\max_{\bx\in\mathcal{X}_t} h(\bx;\overline{\btheta}_t) + \left( \beta_T + B\sqrt{\frac{\lambda}{\kappa_\mu}} + 1 \right) \norm{\left(\phi(\bx) - \phi(\bx_{t,1})\right)}_{\bV_{t-1}^{-1}}$.
Step $(e)$ results from the way in which we select the first arm: $\bx_{t,1} = \arg\max_{\bx\in\mathcal{X}_t} h(\bx;\overline{\btheta}_t)$.
            

Then we have
\begin{align}
    \sum_{t=T_0}^T r_t &\leq 3 \beta_T' \sum_{t=T_0}^T\norm{\phi(\bx_{t,1}) - \phi(\bx_{t,2})}_{\bV_{t-1}^{-1}} + 6T\varepsilon'_{m_{\text{NN}},T} \notag \\
    &=3 \beta_T'\sum_{t=T_0}^T\sum_{j\in[m]}\mathbb{I}\{i_t\in C_j\}\norm{\phi(\bx_{t,1}) - \phi(\bx_{t,2})}_{\bV_{t-1}^{-1}}  + 6T\varepsilon'_{m_{\text{NN}},T}\notag\\
    &\leq 3 \beta_T'\sqrt{\sum_{t=T_0}^T\sum_{j\in[m]}\mathbb{I}\{i_t\in C_j\}\sum_{t=T_0}^T\sum_{j\in[m]}\mathbb{I}\{i_t\in C_j\}\norm{\phi(\bx_{t,1}) - \phi(\bx_{t,2})}_{\bV_{t-1}^{-1}}^2} + 6T\varepsilon'_{m_{\text{NN}},T}\notag\\
    &\leq 3 \beta_T' \sqrt{T\cdot m\cdot 16 \widetilde{d}} + 6T\varepsilon'_{m_{\text{NN}},T}\\
    &\leq 12 \beta_T' \sqrt{T\cdot m\cdot \widetilde{d}} + 6T\varepsilon'_{m_{\text{NN}},T}\,,
\end{align}
where in the second inequality we use the Cauchy-Swarchz inequality, and in the last step we use $\sum_{t=T_0}^T\sum_{j\in[m]}\mathbb{I}\{i_t\in C_j\}\leq T$ and Lemma \ref{lemma:concentration:square:std:neural}.
It can be easily verified that as long as the conditions on $m$ specified in \cref{eq:conditions:on:m} are satisfied (i.e., as long as the NN is wide enough), we have that 
$6T\varepsilon'_{m_{\text{NN}},T} \leq 1$.

Recall that $\beta_T' \triangleq \beta_T + B \sqrt{\lambda / \kappa_\mu} + 1$ and $\beta_T \triangleq \frac{1}{\kappa_\mu} \sqrt{ \widetilde{d} + 2\log(u/\delta)}$.
Therefore, finally, we have with probability at least $1-4\delta$
\begin{align}
    R_T & \leq T_0+ 12 (\beta_T + B \sqrt{\lambda / \kappa_\mu} + 1) \sqrt{T\cdot m\cdot \widetilde{d}} + 1\notag\\
    &\leq O\left(u(\frac{\widetilde{d}}{\kappa_\mu^2\tilde\lambda_x \gamma^2}+\frac{1}{\tilde\lambda_x^2})\log T+\left(\frac{\sqrt{\widetilde{d}}}{\kappa_\mu} + B\sqrt{\frac{\lambda}{\kappa_\mu}}\right)\sqrt{\widetilde{d}mT} \right)\notag\\
        &=O\left(\left(\frac{\sqrt{\widetilde{d}}}{\kappa_\mu} + B\sqrt{\frac{\lambda}{\kappa_\mu}}\right)\sqrt{\widetilde{d}mT} \right)\,.
\end{align}

