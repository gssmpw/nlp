\section{Experimental Results}
\label{sec:experiments}

We use both synthetic and real-world experiments to evaluate the performance of our COLDB and CONDB algorithms.
For both algorithms, we compare them with their corresponding single-user variant as the baseline. Specifically, for COLDB, we compare it with the baseline of LDB\_IND, which refers to Linear Dueling Bandit (Independent) \cite{ICML22_bengs2022stochastic}, meaning running independent classic linear dueling bandit algorithms for each user separately; similarly, for CONDB, we compare it with NDB\_IND, which stands for Neural Dueling Bandit (Independent) \cite{verma2024neural}.

\paragraph{COLDB.}
Our experimental settings mostly follow the designs from the works on clustering of bandits \cite{wang2024onlinea,10.5555/3367243.3367445}.
In our synthetic experiment for COLDB, we design a setting with linear reward functions: $f_i(\bx)=\btheta_i^{\top} \bx$.
We choose $u=200$ users, $K=20$ arms and a feature dimension of $d=20$, and construct two settings with $m=2$ and $m=5$ groundtruth clusters, respectively.
In the experiment with the MovieLens dataset \cite{harper2015movielens}, we follow the experimental setting from \citet{wang2024onlinea}, a setting with $200$ users.
Same as the synthetic experiment, we choose the number of arms in every round to be $K=20$ and let the input feature dimension be $d=20$. We construct a setting with $m=5$ clusters.
We repeat each experiment for three independent trials and report the mean $\pm$ standard error.

Fig.~\ref{fig:exp:linear} plots the cumulative regret of our COLDB and the baseline of LDB\_IND.
The results show that our COLDB algorithm significantly outperforms the baseline of LDB\_IND in both the synthetic and real-world experiments. Moreover, Fig.~\ref{fig:exp:linear} (a) demonstrates that when $m=2$ (i.e., when a larger number of users belong to the same cluster on average), the performance of our COLDB is improved, which is \emph{consisent with our theoretical results} (Sec.~\ref{subsec:theory:linear}).
\begin{figure}[t]
     \centering
     \begin{tabular}{cc}
        \hspace{-3mm} \includegraphics[width=0.52\linewidth]{figures/COLDB/synthetic_60noise_user200d20m2+5L20_1001_3.pdf} &\hspace{-5mm} 
         \includegraphics[width=0.52\linewidth]{figures/COLDB/realworld_60noise_user200d20m5L20_1001_3.pdf} \\
         {\hspace{-3mm}(a) Synthetic} & {(b) MovieLens} 
     \end{tabular}
     \caption{
     Experimental results for our COLDB algorithm with a linear reward function.
     }
     \label{fig:exp:linear}
\end{figure}
\begin{figure}[h]
     \centering
     \begin{tabular}{cc}
        \hspace{-3mm} \includegraphics[width=0.52\linewidth]{figures/CONDB_cumulative_regret.pdf} &\hspace{-5mm} 
         \includegraphics[width=0.52\linewidth]{figures/Real_World_CONDB_cumulative_regret.pdf} \\
         {\hspace{-3mm}(a) Synthetic} & {(b) MovieLens} 
     \end{tabular}
     \caption{
     Experimental results for our CONDB algorithm with a non-linear (square) reward function.
     }
     \label{fig:exp:neural}
\end{figure}

\paragraph{CONDB.}
We also construct both a synthetic and real-world experiment to evaluate our CONDB algorithm.
Most of the experimental settings are the same as those of the COLDB algorithm described above. The major difference is that instead of using linear reward functions, here we adopt a non-linear reward function, i.e., a square function: $f_i(\bx)=(\btheta_i^{\top} \bx)^2$. 
The results in this setting are plotted in Fig.~\ref{fig:exp:neural}.
Our CONDB algorithm achieves significantly smaller cumulative regrets than the baseline algorithm of NDB\_IND in both the synthetic and real-world experiments. Moreover, Fig.~\ref{fig:exp:neural} (a) shows that the performance of our CONDB is improved when a larger number of users are in the same cluster on average, i.e., when $m=2$.
These results demonstrate the potential of our CONDB algorithm to excel in problems with complicated non-linear reward functions.
