\vspace{-1.5mm}
\section{Related Work}
\vspace{-1.5mm}
Our work is closely related to: online clustering of bandits (CB), dueling bandits, and neural bandits.

\subsection{Clustering of Bandits}
The concept of clustering bandits (CB) was first introduced in \cite{gentile2014online}, where a graph-based approach was proposed for solving the problem. In subsequent work, \cite{li2016collaborative} explored the incorporation of collaborative effects among items to aid in the clustering of users. Further extending this idea, \cite{li2018online} tackled the CB problem in the context of cascading bandits, where feedback is provided through random prefixes. Another direction of this research, presented in \cite{10.5555/3367243.3367445}, investigates the scenario where users have varying arrival frequencies. In \cite{liu2022federated}, a federated setting for CB is proposed, which addresses both privacy concerns and the communication overhead in distributed environments. More recently, two papers by \cite{wang2024onlinea} and \cite{wang2024onlineb} examine the design of robust CB algorithms in the presence of model mis-specifications and adversarial data corruptions, respectively.

All these works in CB assume the agent recommends a single arm per round, with a real-valued reward reflecting user satisfaction. However, this does not apply to scenarios such as large language models seeking user preference feedback to improve the model, where users provide binary feedback comparing two responses. To the best of our knowledge, this paper is the first to consider dueling binary feedback in the CB problem.

\subsection{Dueling Bandits and Neural Bandits}
Dueling bandits has been receiving growing attention over the years since its introduction \cite{ICML09_yue2009interactively,ICML11_yue2011beat,JCSS12_yue2012k} due to the prevelance of preference or relative feedback in real-world applications.
Many earlier works on dueling bandits have focused on MAB problems with a finte number of arms \cite{WSDM14_zoghi2014relative,ICML14_ailon2014reducing,ICML14_zoghi2014relative,COLT15_komiyama2015regret,ICML15_gajane2015relative,UAI18_saha2018battle,AISTATS19_saha2019active,ALT19_saha2019pac,AISTATS22_saha2022exploiting,ICML23_zhu2023principled}.
More recently, contextual dueing bandits, which model the reward function using a parametric function of the features of the arms, have attracted considerable attention \cite{NeurIPS21_saha2021optimal,ALT22_saha2022efficient,ICML22_bengs2022stochastic,arXiv23_di2023variance,arXiv24_li2024feelgood,verma2024neural}.

To apply MABs to complicated real-world applications with non-linear reward functions, neural bandits have been proposed which use a neural network to model the reward function \cite{zhou2020neural,zhang2020neural}.
Recently, we have witnessed a significant growing interest in further improving the theoretical and empirical performance of neural bandits and applying it to solve real-world problems \cite{xu2020neural,kassraie2021neural,gu2021batched,nabati2021online,lisicki2021empirical,ban2021ee,ban2021convolutional,jia2021learning,nguyen2021offline,zhu2021pure,kassraie2022graph,salgia2022provably,dai2022sample,hwang2023combinatorial,qi2023graph,qi2024meta}.
In particular, the work of \citet{ban2024meta} has adopted a neural network as a meta-learner for adapting to users in different clusters within the framework of clustering of bandits, and the work of \citet{verma2024neural} has combined neural bandits with dueling bandits.
