\section{Introduction}

The contextual multi-armed bandit (MAB) is a widely used method in real-world applications requiring sequential decision-making under uncertainty, such as recommendation systems, computer networks, among others \cite{li2010contextual}.
In a contextual MAB problem, a user faces a set of $K$ arms (i.e., context vectors) in every round, selects one of these $K$ arms, and then observes a corresponding numerical reward \cite{lattimore2020bandit}.
In order to select the arms to maximize the cumulative reward (or equivalently minimize the cumulative regret), we often need to consider the trade-off between the \emph{exploration} of the arms whose unknown rewards are associated with large uncertainty and \emph{exploitation} of the available observations collected so far.
To carefully handle this trade-off, we often model the reward function using a surrogate model, such as a linear model \cite{chu2011contextual} or a neural network \cite{zhou2020neural}.

Some important applications of contextual MAB, such as recommendation systems, often involve a large number (e.g., in the scale of millions) of users, which opens up the possibility of further improving the performance of contextual MAB via user collaboration.
To this end, the method of \emph{online Clustering of Bandits} (CB) has been proposed, which adaptively partitions the users into a number of clusters and leverages the collaborative effect of the users in the same cluster to achieve improved performance \cite{gentile2014online,wang2024onlinea,10.5555/3367243.3367445}.

Classical CB algorithms usually require an absolute real-valued numerical reward as feedback for each arm \cite{wang2024onlinea}. However, in some crucial applications of contextual MAB, it is often more realistic and reliable to request the users for \emph{preference feedback}.
For example, in recommendation systems, it is often preferable to recommend a pair of items to a user and then ask the user for relative feedback (i.e., which item is preferred) \cite{JCSS12_yue2012k}.
As another example, contextual MAB has been successfully adopted to optimize the input prompt for large language models (LLMs), which is often referred to as \emph{prompt optimization} \cite{lin2024prompt,lin2023instinct}.
In this application, instead of requesting an LLM user for a numerical score as feedback, it is more practical to show the user a pair of LLM responses generated by two candidate prompts and ask the user which response is preferred \cite{lin2024prompt,verma2024neural}.

A classical and principled approach to account for preference feedback in contextual MAB is the framework of contextual \emph{dueling bandit} 
\cite{NeurIPS21_saha2021optimal,ICML22_bengs2022stochastic,ALT22_saha2022efficient,arXiv24_li2024feelgood}.
In every round of contextual dueling bandits, a pair of arms are selected, after which a binary observation is collected reflecting which arm is preferred.
However, classical dueling bandit algorithms are not able to leverage the collaboration of multiple users, which leaves significant untapped potential to further improve the performance in these applications involving preference feedback.
In this work, we bring together the merits of both approaches, and hence introduce the first \emph{clustering of dueling bandit} algorithms, enabling multi-user collaboration in scenarios involving preference feedback.

We firstly proposed our \emph{Clustering Of Linear Dueling Bandits} (COLDB) algorithm (Sec.~\ref{subsec:algo:coldb}), which assumes that the latent reward function of each user is a linear function of the context vectors (i.e., the arm features). In addition, to handle challenging real-world scenarios with complicated non-linear reward functions, we extend our COLDB algorithm to use a \emph{neural network to model the reward function}, hence introducing our \emph{Clustering Of Neural Dueling Bandits} (CONDB) algorithm (Sec.~\ref{subsec:algo:condb}).
Both algorithms adopt a graph to represent the estimated clustering structure of all users, and adaptively update the graph to iteratively refine the estimate. After receiving a user in every round, our both algorithms firstly assign the user to its estimated cluster, and then leverage the data from all users in the estimated cluster to learn a linear model (COLDB) or a neural network (CONDB), which is then used to select a pair of arms for the user to query for preference feedback. After that, we update the reward function estimate for the user based on the newly observed feedback, and then update the graph to remove its connection with users who are estimated to belong to a different cluster.

We conduct rigorous theoretical analysis for both our COLDB and CONDB algorithms, and our theoretical results demonstrate that the regret upper bounds of both algorithms are sub-linear and that a larger degree of user collaboration (i.e., when a larger number of users belong to the same cluster on average) leads to theoretically guaranteed improvement (Sec.~\ref{sec:theory}).
In addition, we also perform both synthetic and real-world experiments to demonstrate the practical advantage of our algorithms and the benefit of user collaboration in contextual MAB problems with preference feedback (Sec.~\ref{sec:experiments}).
