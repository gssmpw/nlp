\section{ Mathematical Derivations}

\subsection{Deriving the \ourMethod{} Objective}
\label{subsec:derive_longpo}
In this section, we will derive the reward function of our \ourMethod{} objective in~\Cref{eq:longpo_reward} by incorporating short-to-long constraint in~\Cref{eq:adjust_constraint}. Starting from RLHF objective in~\Cref{eq:RL} with short-to-long constraint, we have
\begin{equation}
\max_{\pi}  \mathbb{E}_{\xl\sim \mathcal{D}, y\sim \pi}\bigl[r(\xl, y)\bigr] - \beta\mathbb{D}_{\textrm{KL}}\bigl[\pi(y|\xl)||\pishort(y|\xs)\bigr]
\end{equation}
Following the DPO derivation process~\citep{Rafailov2023DirectPO}, we have:
\begin{align}\label{eq:RL_proof}
\max_{\pi}  \mathbb{E}_{(\xl, \xs) \sim \mathcal{\hat{D}^{\text{SL}}}, y\sim \pi(y|\xl)}&\bigl[r(\xl, y)\bigr] - \beta\mathbb{D}_{\textrm{KL}}\bigl[\pi(y|\xl)\mid\mid\pishort(y|\xs)\bigr] \nonumber\\
&=\max_{\pi}  \mathbb{E}_{(\xl, \xs) \sim \mathcal{\hat{D}^{\text{SL}}}}\mathbb{E}_{y\sim \pi(y|\xl)}\left[r(\xl, y) - \beta\log\frac{\pi(y|\xl)}{\pishort(y|\xs)}\right] \nonumber\\&=
\min_{\pi}  \mathbb{E}_{(\xl, \xs) \sim \mathcal{\hat{D}^{\text{SL}}}}\mathbb{E}_{y\sim \pi(y|\xl)}\left[\log\frac{\pi(y|\xl)}{\pishort(y|\xs)} - \frac{1}{\beta}r(\xl, y)\right] \nonumber\\ &=
\min_{\pi}  \mathbb{E}_{(\xl, \xs) \sim \mathcal{\hat{D}^{\text{SL}}}}\mathbb{E}_{y\sim \pi(y|\xl)}\left[\log\frac{\pi(y|\xl)}{\frac{1}{Z(\xl, \xs)}\pishort(y|\xs)\exp\left(\frac{1}{\beta}r(\xl, y)\right)} - \log Z(\xl, \xs)\right],
\end{align}
where we have partition function:
\begin{equation*}
Z(\xl, \xs) = \sum_{y}\pishort(y|\xs)\exp\left(\frac{1}{\beta}r(\xl, y)\right).
\end{equation*}
The partition function is only related to $\xl$, $\xs$, and original short-context LLM $\pishort$.
% We can now define
% \begin{equation*}
%     \pi^*(y|x) = \frac{1}{Z(\xl, \xs)}\pishort(y|\xs)\exp\left(\frac{1}{\beta}r(\xl, y)\right),
% \end{equation*}
% which is a valid probability distribution as $\pi^*(y|x)\geq 0$ for all $y$ and $\sum_{y}\pi^*(y|x)=1$. Since $Z(\xl, \xs)$ is not a function of $y$, we can then re-organize the final objective in Eq \ref{eq:RL_proof} as:
% \begin{align}
% \min_{\pi}  \mathbb{E}_{(\xl, \xs) \sim \mathcal{\hat{D}^{\text{SL}}}}\left[\mathbb{E}_{y\sim \pi(y|\xl)}\left[\log\frac{\pi(y|x)}{\pi^*(y|x)}\right] - \log Z(\xl)\right]=\\
% \min_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{D}_{\text{KL}}(\pi(y|x)\mid\mid\pi^*(y|x)) - \log Z(\xl)\right]
% \end{align}
% Now, since $Z(\xl, \xs)$ does not depend on $\pi$, the minimum is achieved by the policy that minimizes the first KL term. Gibbs' inequality tells us that the KL-divergence is minimized at 0 if and only if the two distributions are identical. 
Hence we have the optimal solution following~\citet{Rafailov2023DirectPO}:
\begin{equation}
\pi^*(y|\xl) = \frac{1}{Z(\xl, \xs)}\pishort(y|\xs)\exp\left(\frac{1}{\beta}r(\xl, y)\right).
\end{equation}
The optimal reward function would be derived:
\begin{equation}\label{eq:main_eq_restated}
    r^*(\xl,y) =\beta \log \frac{\pi^*(y|\xl)}{\pishort(y|\xs)} + \beta \log Z(\xl, \xs).
\end{equation}


% Following the DPO objective under the Bradley-Terry  model, we have
% \begin{equation}\label{eq:BT_restated}
%     p^*(y_1\succ y_2|\xl)=\frac{\exp\left(r^*(\xl, y_1)\right)}{\exp\left(r^*(\xl, y_1)\right) + \exp\left(r^*(\xl, y_2)\right)}.
% \end{equation}
We thus have:
\begin{align*}
    p^*(y_1\succ y_2|\xl)&=\frac{\exp\left(r^*(\xl, y_1)\right)}{\exp\left(r^*(\xl, y_1)\right) + \exp\left(r^*(\xl, y_2)\right)} \\
    &=\frac{\exp\left(\beta \log \frac{\pi^*(y_1|\xl)}{\pishort(y_1|\xs)} + \beta \log Z(\xl, \xs)\right)}{\exp\left(\beta \log \frac{\pi^*(y_1|\xl)}{\pishort(y_1|\xs)} + \beta \log Z(\xl, \xs)\right) + \exp\left(\beta \log \frac{\pi^*(y_2|\xl)}{\pishort(y_2|\xs)} + \beta \log Z(\xl, \xs)\right)}\\ &=
    \frac{1}{1+\exp\left(\beta \log \frac{\pi^*(y_2|\xl)}{\pishort(y_2|\xs)}-\beta \log \frac{\pi^*(y_1|\xl)}{\pishort(y_1|\xs)}\right)} \\&= \sigma\left(\beta \log \frac{\pi^*(y_1|\xl)}{\pishort(y_1|\xs)} - \beta \log \frac{\pi^*(y_2|\xl)}{\pishort(y_2|\xs)}\right).
\end{align*}
By optimizing the $\pitheta$ towards the optimal policy $\pi^*$, we finally access the objective of \ourMethod{} in~\Cref{eq:longpo}.


% \subsection{}

% \begin{equation}
%     \begin{aligned}
% \constraint &= \beta\mathbb{D}_{\textrm{KL}}\bigl[\pitheta(y\mid \xl)\mid \mid \piref(y\mid \xl)\bigr] \\
% &= \beta\mathbb{D}_{\textrm{KL}}\bigl[\pitheta(y\mid \xl)\mid \mid \piref^\ast(y\mid \xl)\bigr] \quad \text{(since $\piref = \piref^\ast$)} \\
% &= \beta\mathbb{D}_{\textrm{KL}}\bigl[\pitheta(y\mid \xl)\mid \mid \piref^\ast(y\mid \xs)\bigr] \\
% &\quad \text{(from Eq. \ref{eq:equal_kl}, $\mathbb{D}_{\textrm{KL}}\bigl[\piref^\ast(y\mid \xl)\mid \mid \piref^\ast(y\mid \xs)\bigr] = 0$, so $\piref^\ast(y\mid \xl) = \piref^\ast(y\mid \xs)$)} \\
% &= \beta\mathbb{D}_{\textrm{KL}}\bigl[\pitheta(y\mid \xl)\mid \mid \pishort(y\mid \xs)\bigr] \\
% &\quad \text{(from Eq. \ref{eq:equal_kl}, $\mathbb{D}_{\textrm{KL}}\bigl[\piref^\ast(y\mid \xs)\mid \mid \pishort(y\mid \xs)\bigr] = 0$, so $\piref^\ast(y\mid \xs) = \pishort(y\mid \xs)$)} \\
% &= \constraint^\prime
% \end{aligned}

% \end{equation}


% The last line is the per-instance loss in Equation~\ref{eq:optimum_model}.


\section{Experimental Details}

\subsection{Data Construction Details}
\label{subsec:data_construct}
We prompt the Mistral-7B-Instruct-v0.2 to generate instructions with decode parameters of temperature $T=0.7$ and $p=0.9$. The prompt of Self-Instruct to generate an instruction pool is shown in~\Cref{fig:prompt}. For generating the corresponding responses, we directly concatenate the short or long context with corresponding instructions and adopt the greedy decoding to maintain the deterministic behaviour of LLMs. As shown in~\Cref{fig:rewards}, the model would tend to prefer the high-quality chosen response and deviate from the low-quality rejected response over long context, hence improve the long-context capabilities.

\subsection{Evaluation details}
\label{subsec:eval_details}
On long-context benchmarks InfiniteBench and RULER, we evaluate our models and all baselines following the settings in the original benchmarks. For short-context evaluation, we utilize the lm-evaluaton-harness framework~\citep{eval-harness} and following the evaluation settings in~\citep{open-llm-leaderboard-v1}: 5-shots for MMLU, 25-shots for ARC-C, 10-shots for Hellaswag, and 5-shots for Winogrande. We use GPT-4-Turbo-1106-Preview as the judge for MT-Bench and LongBench-Chat evaluation.

\subsection{More Training Details}
\label{subsec:train_details}
Leveraging the DeepSpeed-Ulysses sequence parallel framework, we train the Mistral-7B/Qwen2.5-7B with a sequence length of 128K on an 8$\times$A800 80GB, achieving a throughput of 4,401 tokens per second. For sequence lengths of 256K and 512K, the models are trained on a 16$\times$A800 80GB, yielding throughputs of 4,120 tokens per second and 2,744 tokens per second, respectively.
To facilitate a comparison with standard LLM alignment methods, we train Mistral-7B using SFT and DPO utilizing the same short-to-long preference data of \ourMethod{}. For DPO training, we apply the same settings as LongPO outlined in~\Cref{subsec:train_setup}, but excluding the short-to-long constraint of \ourMethod{} introduced in~\Cref{subsec:short_to_long_kl}. Since SFT cannot utilize paired responses within preference data, we train it using only the chosen responses provided alongside long context inputs. The hyperparameters for SFT remain unchanged, except for an increase in the learning rate to 2e-5.





\begin{figure}[!t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/prompt.png}
    \caption{The prompt for generating instruction pool.}
    \label{fig:prompt}
\end{figure}


\begin{figure}[!t]
    \centering
    \subfloat[The rewards for chosen response during training.]{
        \includegraphics[width=0.48\textwidth, trim={50 300 80 300}, clip]{figures/chosen_reward.pdf}
        \label{fig:figure1}
    }
    \hfill
    \subfloat[The rewards for rejected response during training.]{
        \includegraphics[width=0.48\textwidth, trim={80 300 50 300}, clip]{figures/rejected_reward.pdf}
        \label{fig:figure2}
    }
    \caption{The chosen and rejected rewards during the training of Mistral-7B-\ourMethod-128K.}
    \label{fig:rewards}
\end{figure}




\begin{table}[!t]
\centering
\caption{Full results on 13 tasks of RULER benchmark. The \textbf{bold} values denote the average score of 13 tasks in RULER over various context lengths.}
\label{tab:ruler_all}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l l c c c c c c c}
\toprule
Model & Category & 4k & 8k & 16k & 32k & 64k & 128k & AVG \\
\midrule
\multirow{5}{*}{Qwen2.5-7B-Instruct} 
& NIAH & 99.69 & 98.45 & 97.82 & 95.24 & 74.56 & 26.86 & 82.10 \\
& VT & 99.88 & 99.72 & 96.24 & 96.44 & 81.44 & 6.84 & 80.09 \\
& AGG & 92.52 & 89.78 & 92.08 & 81.93 & 62.48 & 28.23 & 74.50 \\
& QA & 71.00 & 65.30 & 64.00 & 58.70 & 46.80 & 19.99 & 54.30 \\
& AVG (13 tasks) & 94.19 & 92.11 & 91.61 & 87.66 & 68.96 & 24.47 & \textbf{76.50} \\
\addlinespace
\hdashline
\addlinespace
\multirow{5}{*}{Qwen2.5-7B-LongPO-128K} 
& NIAH & 99.64 & 98.97 & 97.80 & 95.54 & 94.80 & 88.15 & 95.82 \\
& VT & 99.96 & 99.92 & 96.12 & 86.24 & 78.20 & 77.80 & 89.71 \\
& AGG & 95.50 & 86.12 & 91.75 & 82.56 & 66.31 & 49.81 & 78.67 \\
& QA & 70.00 & 64.00 & 62.70 & 57.70 & 53.00 & 49.00 & 59.40 \\
& AVG (13 tasks) & 94.47 & 91.69 & 91.34 & 87.00 & 82.71 & 75.43 & \textbf{87.11} \\

\midrule
\multirow{5}{*}{Mistral-7B-LongPO-128K} 
& NIAH & 99.43 & 98.64 & 98.09 & 97.84 & 95.82 & 91.44 & 96.88 \\
& VT & 99.40 & 99.16 & 98.08 & 96.36 & 92.80 & 93.12 & 96.49 \\
& AGG & 88.31 & 82.91 & 92.23 & 72.775 & 46.305 & 46.79 & 71.55 \\
& QA & 71.10 & 70.15 & 66.60 & 65.80 & 61.00 & 54.20 & 64.81 \\
& AVG (13 tasks) & 93.36 & 91.88 & 92.35 & 88.94 & 82.61 & 78.97 & \textbf{88.02} \\
\addlinespace
\hdashline
\addlinespace
\multirow{5}{*}{Mistral-7B-LongPO-256K} 
& NIAH & 99.16 & 97.79 & 98.02 & 97.76 & 96.53 & 91.54 & 96.80 \\
& VT & 99.40 & 99.20 & 97.96 & 97.72 & 94.21 & 93.52 & 97.00 \\
& AGG & 87.40 & 76.59 & 89.03 & 72.20 & 45.17 & 44.47 & 69.14 \\
& QA & 71.50 & 69.50 & 66.70 & 64.30 & 60.80 & 56.40 & 64.87 \\
& AVG (13 tasks) & 93.11 & 90.28 & 91.81 & 88.68 & 82.95 & 79.04 & \textbf{87.65} \\
\addlinespace
\hdashline
\addlinespace
\multirow{5}{*}{Mistral-7B-LongPO-512K} 
& NIAH & 99.19 & 97.78 & 98.06 & 97.69 & 96.62 & 94.36 & 97.28 \\
& VT & 99.44 & 99.16 & 98.04 & 97.80 & 95.92 & 94.52 & 97.48 \\
& AGG & 87.56 & 76.71 & 88.95 & 72.70 & 44.93 & 44.51 & 69.22 \\
& QA & 71.40 & 69.50 & 66.40 & 64.50 & 60.60 & 57.10 & 64.92 \\
& AVG (13 tasks) & 93.14 & 90.29 & 91.78 & 88.75 & 83.07 & 80.97 & \textbf{88.00} \\
\bottomrule
\end{tabular}%
}
\end{table}


% \begin{table}[!t]
% \centering
% \begin{tabular}{l|ccccc}
% \hline
% Model & MMLU & ARC-C & Hellaswag & Winogrande & Avg \\
% \hline
% Mistral-7B-Instruction-v0.2 & 59.15 & 59.26 & 83.2 & 78.4 & 70.00 \\
% Mistral-7B-LongPO-128K & 59.99 & 59.34 & 82.99 & 78.53 & 70.21 \\
% Mistral-7B-LongPO-256K-EXP & 59.43 & 60.58 & 83.14 & 78.14 & 70.32 \\
% Mistral-7B-LongPO-512K-EXP & 59.51Í & 60.58 & 82.87 & 77.66 & 70.16 \\
% Qwen2.5-7B-Instruct & 74.28 & 67.15 & 81.41 & 74.66 & 74.38 \\
% Qwen2.5-7B-LongPO-128K & 73.64 & 65.70 & 80.82 & 74.98 & 73.79 \\
% \hline
% \end{tabular}
% \caption{Performance on short-context tasks.}
% \label{tab:short-performance}
% \end{table}


\begin{table}[!t]
\centering
\caption{Performance on short-context tasks.}
\begin{tabular}{l|cccccc}
\hline
Model & MMLU & ARC-C & Hellaswag & Winogrande & MT-Bench \\
\hline
Mistral-7B-Instruct-v0.2 & 59.15 & 59.26 & 83.2 & 78.4 & 6.34 \\
Mistral-7B-LongPO-128K & 59.99 & 59.34 & 82.99 & 78.53 & 6.35 \\
Mistral-7B-LongPO-256K & 59.47 & 60.28 & 83.14 & 78.14 & 6.38 \\
Mistral-7B-LongPO-512K & 59.51 & 60.58 & 82.87 & 77.66 & 6.34 \\
Qwen2.5-7B-Instruct & 74.28 & 67.15 & 81.41 & 74.66 & 7.30 \\
Qwen2.5-7B-LongPO-128K & 73.64 & 65.70 & 80.82 & 74.98 & 7.62 \\
\hline
\end{tabular}
\label{tab:short-performance}
\end{table}




% \subsection{Case Study: Review the Paper}


% \begin{tcolorbox}[title=Mistral-7B-\ourMethod{}-128K]
% The paper introduces Short-to-Long Preference Optimization (\ourMethod), a method for aligning Large Language Models (LLMs) with long-context capabilities without using external data. By transferring preferences learned from short contexts, it optimizes long-context performance while maintaining short-context abilities. The results show that \ourMethod{} outperforms traditional approaches, offering a more efficient solution for long-context LLM development.

% \textbf{Strengths:}
% \begin{itemize}
%     \item \textbf{Innovativeness:} The concept of self-evolving LLMs through internal preference data is innovative. It addresses a significant problem in the field, which is the difficulty of obtaining high-quality, long-context annotated data.
%     \item \textbf{Technical Soundness:} The proposed method is technically sound, with a clear mathematical formulation and a logical approach to optimizing long-context performance without degrading short-context proficiency.
%     \item \textbf{Broader Impact:} If successful, LongPO could reduce the reliance on large amounts of human annotation, making it easier and more cost-effective to develop LLMs with robust long-context understanding.
% \end{itemize}

% \textbf{Weaknesses:}
% \begin{itemize}
%     \item \textbf{Scalability:} The paper could discuss how LongPO scales with model size and context length beyond the experiments conducted. Would the gains be consistent as we move to even larger models and contexts?
%     \item \textbf{Practicality:} It's unclear how practical LongPO is in real-world scenarios. The paper could provide more details on computational costs and the feasibility of applying LongPO at scale.
% \end{itemize}

% Overall, this is a strong paper that makes a clear contribution to the field. With some additional work to address the scalability and practicality concerns,

% \end{tcolorbox}


% \begin{tcolorbox}[title=Mistral-7B-Instruct-v0.2]


% The paper introduces LongPO, a novel method aimed at enhancing the performance of Large Language Models (LLMs) in long-context scenarios by leveraging their existing short-context capabilities. The authors argue that superior short-context LLMs may underperform in long-context scenarios due to insufficient alignment, and propose a solution that doesn't rely on extensive human annotation.

% \textbf{Strengths:}
% \begin{itemize}
%     \item \textbf{Clear Problem Statement:}
% The paper clearly identifies the gap between short-context and long-context alignments in large language models (LLMs).
%     \item \textbf{Practical Implications:}
% The method shows promise in achieving long-context alignment without extensive resource use, making it potentially more efficient than existing methods.
%     \item \textbf{Experimental Results:}
% The results indicate that \ourMethod significantly outperforms naive SFT and DPO, retaining short-context performance effectively.
% \end{itemize}

% \textbf{Weaknesses:}
% \begin{itemize}
%     \item \textbf{Comparative Analysis:}
% More detailed comparisons with existing state-of-the-art methods, possibly including nuanced pros and cons, would provide a clearer competitive landscape.
%     \item \textbf{Generalization:}
% The generalizability of the method across different languages and tasks is not thoroughly explored, which could limit its applications.
% \end{itemize}


% \end{tcolorbox}