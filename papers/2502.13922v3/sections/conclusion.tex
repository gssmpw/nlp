\section{Conclusion and Discussion}
% In this work, we propose \ourMethod{}, a novel long-context alignment method that enables
In this work, we propose \ourMethod{}, a novel long-context alignment method that enables LLMs to effectively transfer their short-context capabilities to long-context scenarios. Our approach addresses key challenges in long-context alignment by leveraging intrinsic model knowledge, eliminating the need for external long-context annotated data. \ourMethod{} is built on short-to-long preference data, comprising paired responses for the same instruction given a long context and relevant shortened chunk, respectively. By steering the policy model to learn from the discrepancies within these paired responses, \ourMethod{} facilitates the transfer of established capabilities from short to long contexts. In addition, \ourMethod{} incorporates a short-to-long constraint using KL divergence, that effectively preserve short-context performance during training.
Experimental results demonstrate that \ourMethod{} significantly improves long-context performance across various tasks, outperforming existing alignment methods and even surpassing more sophisticated models. Importantly, this improvement is achieved without sacrificing short-context proficiency.
The success of \ourMethod{} highlights the potential of leveraging internal model knowledge for alignment tasks, opening new avenues for efficient adaptation of LLMs to diverse context lengths. 

% \gc{However, our efforts with \ourMethod{} on larger LLMs are hampered by limited resources and underdeveloped long-context training infrastructures. This constrains our ability to scale \ourMethod{} with increased self-generated data.
% In the future, we plan to extend \ourMethod{} to a wider range of LLMs at various scales, as well as explore other modalities like video, which urgently require long-context input support.}

