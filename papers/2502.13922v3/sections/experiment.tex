\section{Experimental Setup}
\label{sec:expermental_setup}

\subsection{Training Setup}
\label{subsec:train_setup}
\paragraph{Data Curation Details.}
We curate the short-to-long preference data based on a long-context corpus sampled from the Book and ArXiv subsets of Long-Data-Collection\footnote{\url{https://huggingface.co/datasets/togethercomputer/Long-Data-Collections}}, and the GitHub subset of RedPajama~\citep{together2023redpajama}. For a specific target length (e.g., 128K tokens), we filter the corpus to include only documents that are shorter than this length but longer than 64K tokens. 
% This process yields a corpus of 48K documents of 128K tokens, 22K documents of 256K tokens, and 5K documents of 512K tokens. 
Each long document is then segmented into chunks of up to 32K tokens, with a maximum of 4 randomly-sampled chunks retained per document. For instruction generation, we prompt short-context models to generate 4 instructions per document, from which we randomly select one for further use. After filtering undesired (censored and repetitive) responses, we collect 45K, 16K, 2.5K multi-turn instruction-response samples of 128K, 256K, and 512K tokens for Mistral-7B, and 32K samples of 128K tokens for Qwen2.5-7B. More details are listed in~\Cref{subsec:data_construct}.


\paragraph{Training Details.} 
We extend the context length of Mistral-7B and Qwen2.5-7B using our \ourMethod{} on short-to-long preference data specifically generated by models themselves. The training process begins with utilizing Mistral-7B/Qwen2.5-7B to generate data with a length of 128K  and extend the context length to 128K. To investigate the scalability, we utilize the resulting Mistral-7B-LongPO-128K to generate data with lengths of 256K/512K and further extend the context length to 256K/512K.
We leverage Deepspeed-Ulysses~\citep{Jacobs2023DeepSpeedUS} for sequence parallelism and employ Flash Attention~\citep{dao2022flashattention,dao2023flashattention2} for efficient computation. All models are optimized using the Adam optimizer~\citep{KingmaB14} with a learning rate of 5e-7. We set the margin $\beta$ in~\Cref{eq:longpo_mt} to 0.1 and the weighting factor $\lambda$ in~\Cref{eq:final_loss} to 0.01. RoPE $\theta$ is set as 1e7 and the batch size is set as 8.
More details are listed in~\Cref{subsec:train_details}.


\subsection{Evaluation Benchmarks}
We assess both the long- and short-context capabilities of our models against baselines. The long-context evaluation utilizes the following benchmarks:
\begin{itemize}
    \item \textbf{$\infty$Bench}~\citep{zhang-etal-2024-bench}. We evaluate all models on three tasks in this benchmark: summarization (En.Sum), long-book question answering (En.QA), and multi-choice question-answering (En.MC). The evaluation length is beyond 100K.
    \item \textbf{RULER}~\citep{hsieh2024ruler}. This benchmark comprises four types of synthetic tasks across variable sequence lengths (4K to 128K): Needle-in-a-haystack (NIAH) retrieval, Multi-hop Tracing with Variable Tracking (VT), Aggregation, and Question Answering (QA). 
    We exclude the Aggregation tasks, which involve word frequency counting within the context, since they present challenges in word counting beyond mere long-context capabilities that current LLMs still struggle in.    
    % We ignore the Aggregation type in our evaluation since it involves counting word frequency in context, which current LLMs still struggle in.
    \item \textbf{LongBench-Chat}~\citep{bai2024longalign}. This benchmark assesses instruction-following abilities over long contexts (10K to 100K tokens), employing GPT-4-128K as an impartial judge to evaluate model-generated responses. We filter out the English samples for fair comparison across different models.
\end{itemize}
For short-context evaluation, we employ MMLU~\citep{hendrycks2021measuring}, ARC-C~\citep{Clark2018ThinkYH}, Hellaswag~\citep{Zellers2019HellaSwagCA} and Winogrande~\citep{sakaguchi2019winogrande} for assessing the general language understanding and reasoning capabilities, and MT-Bench~\citep{Zheng2023JudgingLW} for assessing instruction-following capability. More details are listed in~\Cref{subsec:eval_details}.

% To comprehensively evaluate the short-context capabilities, we employ widely-recognized tasks

% four widely used short-context tasks: MMLU, ARC-C, TruthfulQA for assessing the general capabilities, and MT-Bench for short-context instruction-following capabilities.


\subsection{Baselines}

We train our \ourMethod{} on Mistral-7B-Instruct-v0.2 (denoted as Mistral-7B) and Qwen2.5-7B-Instruct (denoted as Qwen2.5-7B), comparing them against a range of powerful LLMs including GPT-4-128K, Qwen2-72B-Instruct~\citep{yang2024qwen2technicalreport}, LLaMA-3.1-70B, LLaMA-3.1-8B, GLM-4-9B-Chat, GLM-4-9B-Chat-1M, LWM-Text-Chat-1M~\citep{liu2023world}, and Yarn-Mistral-7b-128k~\citep{peng2023yarn}. Additionally, we establish baselines using Mistral-7B trained with conventional SFT and DPO on the same dataset used by \ourMethod{}.

For short-context evaluation, we primarily compare the performance of naive LLMs against their counterparts post-trained with SFT, DPO, and \ourMethod{} on our synthetic data. To provide a more comprehensive comparison, we also include two series of open-source long-context language models: GLM-4-9B-Chat versus GLM-4-9B-Chat-1M, and LWM-Text-Chat-128k versus LWM-Text-Chat-1M. This allows us to assess the effectiveness of our \ourMethod{} to maintain the short-context performance during long-context alignment, comparing with baselines utilizing various strategies.




% the primary comparison is conducted between naive LLM and the counterpart post-trained with SFT, DPO, and \ourMethod{} on our synthetic data, respectively. To include external baselines, we further include two series of open source long-context LLM: GLM-4-9B-Chat vs GLM-4-9B-Chat-1M and LWM-Text-Chat-128k vs LWM-Text-Chat-1M. \gcc{Consider Film-7B.}



% \paragraph{Long-Context} Film-7B, longalign-7B, longalign-13B, lwm, claude gpt4 










\section{Results and Analyses}
\label{sec:experiment}
% In this section, we present the performance of our \ourMethod{} against baselines, highlighting the superior performance of \ourMethod{} in both long and short contexts (\Cref{subsec:main_results}). We then provide in-depth analyses...

In this section, we demonstrate the exceptional effectiveness of \ourMethod{} through two types of comparisons: (1) comparison with naive SFT and DPO trained on identical models and datasets;  (2) comparison with SOTA long-context LLMs. 
% Both comparisons are conducted on both long-context and short-context benchmarks.





\subsection{Comparison with SFT and DPO}
\label{subsec:internal_comparison}

We first compare \ourMethod{} with conventional SFT and DPO using identical LLM (Mistral-7B). All models are trained on equivalent self-generated datasets, as detailed in~\Cref{subsec:train_setup}. Given the inability of SFT to leverage preference data, we apply it to the instructions paired with chosen responses.

\paragraph{\ourMethod{} exhibits superior performance over SFT and DPO.}
The experimental results, illustrated in~\Cref{table:long_context_results}, reveal consistent and substantial performance gains (10 to 20+ points) of \ourMethod{} over SFT and DPO across a diverse range of long-context tasks. Crucially, as depicted in~\Cref{fig:short_performance}, \ourMethod{} maintains robust short-context performance compared with original short-context LLMs (59.99 vs 59.15 on MMLU), whereas SFT and DPO exhibit notable degradation in short-context scenarios after long-context alignment process.

The performance disparity between \ourMethod{} and SFT can be attributed to the explicit integration of short-to-long preference in \ourMethod{}, which is either absent or merely implicit in the chosen responses utilized by SFT. 
% This key distinction underscores the importance of preserving and transferring short-context capabilities to long-context scenarios.
While both \ourMethod{} and DPO leverage the proposed short-to-long preference data, the pivotal difference lies in the short-to-long constraint introduced in~\Cref{subsec:short_to_long_kl}. The marked performance gaps between \ourMethod{} and DPO, observed across both long- and short-context tasks, highlight the effectiveness of the proposed constraint for successfully mitigating the problematic limitations in DPO and retaining the short-context performance during long-context training. More ablations are detailed in~\Cref{subsec:ablation}.

% the implicit limitations present in DPO, resulting in enhanced performance across the spectrum of context lengths.

% \gc{These findings underscore the capacity of \ourMethod{} to effectively empower LLMs with superior long-context capabilities while preserving crucial short-context performance, offering a more holistic and balanced approach to long-context alignment.}

\begin{table}[!t]
    \centering
    \caption{Long-Context Performance of our \ourMethod{} compared with baselines. Higher is better for all metrics. Results marked with $\flat$ are evaluated by ourselves, while other results of baselines are sourced from the original benchmarks. Full results on RULER are listed in~\Cref{tab:ruler_all}.}
    \resizebox{0.94\textwidth}{!}{
    \renewcommand\arraystretch{1.2}
    \Huge
    % \rowcolors{2}{gray!10}{white}
\begin{tabular}{l|c|cccccccccccc}
\toprule
Model            & Train/Claimed  &  & \multicolumn{4}{c}{$\infty$Bench} &  & \multicolumn{4}{c}{RULER}     &  & LongBench- \\ \cline{4-7} \cline{9-12}
                 & Length &  & En.Sum   & En.QA  & En.MC     & AVG.  &  & NIAH  & VT    & QA    & AVG.  &  & Chat (EN)  \\
GPT-4-128K       & 128K      &  & 14.73  & 22.44   & 67.25  & 34.81 &  & 95.4  & 99.9  & 70.3  & 88.53 &  & 8.40       \\
Qwen2-72B        & 128K   &  & 24.32$^\flat$  & 7.03$^\flat$   & 72.05$^\flat$  & 34.47$^\flat$       &  & 88.6  & 95.7  & 66.7  & 83.67 &  &  7.72$^\flat$          \\
LLaMA 3.1-70B    & 128K   &  & 33.55$^\flat$  & 36.08$^\flat$   & 69.00$^\flat$  & 46.21$^\flat$ &  & 96.1  & 93.2  & 67.8  & 85.7  &  &  6.67$^\flat$      \\
LLaMA 3.1-8B     & 128K   &  & 28.06$^\flat$  & 30.47$^\flat$   & 58.08$^\flat$  & 38.87$^\flat$ &  & 97.93 & 91.4  & 64.7  & 84.68 &  & 6.22$^\flat$           \\
GLM-4-9B         & 128K   &  & 14.84$^\flat$  & 9.51$^\flat$    & 67.25$^\flat$  & 30.53$^\flat$ &  & 96.51$^\flat$ & 97.3$^\flat$  & 64.8$^\flat$0 & 86.20$^\flat$ &  & 5.67$^\flat$       \\
GLM-4-9B-1M      & 1M     &  & 28.3   & 9.7     & 68.6   & 35.53 &  & 98.2  & 99.4  & 69.4  & 89.0  &  & 5.03$^\flat$       \\
LWM-7B-1M        & 1M     &  & 4.33$^\flat$     & 0.0$^\flat$     & 3.06$^\flat$    &  2.46$^\flat$  &  & 87.20  & 57.5  & 56.4  & 67.03 &  & 1.25$^\flat$       \\
YaRN-Mistral-7B  & 128K   &  & 9.09   & 9.55    & 27.95  & 15.53 &  & 63.4  & 36.1  & 25.9  & 41.8  &  & -          \\ \hdashline
Mistral-7B       & 32K    &  & 22.13  & 4.93    & 14.41  & 13.82 &  & 72.60  & 74.40  & 52.2  & 66.4  &  & 4.10       \\
- SFT            & 128K   &  & 23.44  & 13.45   & 53.21  & 30.03 &  & 88.73 & 79.64 & 51.08 & 73.15 &  & 4.25       \\
- DPO            & 128K   &  & 15.21  & 10.34   & 48.14  & 25.56 &  & 74.25 & 72.36 & 50.24 & 65.62 &  & 4.08       \\
\rowcolor[HTML]{ECF4FF} 
- LongPO (iter1) & 128K   &  & 27.05  & 23.51   & 67.25  & 39.27 &  & 96.88 & 96.49 & 64.81 & 86.06 &  & 5.42       \\
\rowcolor[HTML]{ECF4FF} 
- LongPO (iter2) & 256K   &  & 28.16  & 24.43   & 66.35  & 39.65 &  & 96.80 & 97.0  & 64.87 & 86.22 &  & 5.48       \\ 
\rowcolor[HTML]{ECF4FF} 
- LongPO (iter3) & 512K  &  & 29.10  & 27.85   & 66.67  & 41.21 &  & 97.28 & 97.48  & 64.92 & 86.56 &  & 5.80       \\ 
\hdashline
% Qwen 2.5-1.5B       & 32K    &  & 17.18  & 6.22    & 24.25  & 15.88  &  & 73.78  & 88.59  & 42.1  & 68.16  &  & 5.00       \\
% \rowcolor[HTML]{ECF4FF} 
% - LongPO (iter1) & 128K   &  & 23.64  & 14.49   & 48.47  & 28.87 &  & 86.19 & 88.7  & 43.47 &  &  & 5.22       \\
Qwen2.5-7B       & 128K    &  & 22.89  & 6.08    & 52.4  & 27.12  &  & 82.1  & 80.09  & 54.30  & 72.16   &  & 5.80       \\
\rowcolor[HTML]{ECF4FF} 
- LongPO (iter1) & 128K   &  & 32.06  & 17.32   & 72.05  & 40.48 &  & 95.81 & 89.71  & 59.4 &  81.64 &  & 5.75       \\

\bottomrule
\end{tabular}
}
    \label{table:long_context_results}
\end{table}


\begin{figure}[!t]
\centering
\includegraphics[width=0.98\textwidth]{figures/short_performance.pdf}
\vspace{-0.5em}
\caption{The margins of the short-context performance of Mistral-7B-\ourMethod{} and baselines relative to corresponding base model. GLM and LWM refer to the margins of GLM-9B-1M and LWM-7B-1M over GLM-9B-128K and LWM-7B-128K, respectively. MT-Bench metrics ($\in$[0, 10]) are linearly scaled to [0, 100] for better comparability across tasks. See numerical results in~\Cref{tab:short-performance}.}
\vspace{-1.0em}
\label{fig:short_performance}
\end{figure}

\subsection{Comparison with SOTA Long-Context LLMs}
\label{subsec:external_comparison}

To further substantiate the efficacy of \ourMethod{}, we conducted an extensive comparison between our \ourMethod{}-trained Mistral-7B and leading long-context LLMs across varying model scales.

\paragraph{\ourMethod{} demonstrates exceptional competitiveness at similar scale.}

As detailed in~\Cref{table:long_context_results}, \ourMethod{} demonstrates formidable competitiveness in terms of models at similar scales. For example, Mistral-7B-\ourMethod{} significantly outperforms some established long-context models, including LWM-7B and YaRN-Mistral, across all long-context tasks in $\infty$Bench and RULER. Remarkably, Mistral-7B-LongPO-128K surpasses GLM-4-9B (39.27 vs. 30.53 on $\infty$Bench and 86.06 vs. 86.20 on RULER), although the latter is training on manually annotated long-context data spanning up to 128K sequence length. 
% \gc{In addition, \ourMethod{} significantly improves the long-context instruction-following performance of Mistral-7B on LongBench-Chat. However, the instruction-following abilities also heavily rely on the short-context alignment, making Mistral-7B-\ourMethod{} underperform superior LLMs.}
Moreover, GLM-4-9B-1M, an extension of GLM-4-9B trained on contexts up to 1M tokens, demonstrates slightly superior performance than \ourMethod{} on the RULER benchmark. However, these performance gains come at the costs of \textit{degenerated short-context performance} (0.41 on MMLU) and \textit{long-context instruction-following capability} (0.64 on LongBench-Chat (EN)) as illustrated in~\Cref{fig:short_performance}. Notably, our models still outperform GLM-4-9B-1M on $\infty$Bench even trained with substantially shorter sequences.
These results underscore the exceptional efficiency of \ourMethod{} in transferring performance from short to long contexts through self-evolution, thereby circumventing the need for extensive manual annotation.




\paragraph{Long-context annotation is not sufficient.} The superiority of our approach is particularly evident in the En.QA task within $\infty$Bench, which involves complex free-form question answering over extensive book-length contexts. In this challenging task, our models surpass both GLM-4-9B and GLM-4-9B-1M by substantial margins (10+ points). The inherent difficulty of such task, which poses challenges even for human annotators, highlights the limitations of relying solely on manually annotated long-context data. By effectively transferring short-context capabilities to long-context scenarios, \ourMethod{} demonstrates superior scalability and efficacy across diverse and intricate tasks.

\paragraph{Superior LLMs Yet to Dominate Long-Context Scenarios}

When benchmarked against leading models such as GPT-4-128K, our \ourMethod{}-trained models still exhibit comparable or even superior long-context performance (e.g., Mistral-7B-LongPO-128K of 39.27 vs. GPT-4-128K of 34.81 on $\infty$Bench), despite being based on significantly smaller Mistral-7B. This observation reveals that even the most advanced LLMs have not yet achieved the same level of dominance in long-context scenarios as they have in short-context tasks. This performance gap can be attributed primarily to the scarcity of high-quality, large-scale long-context training data. The dearth of such data is particularly impactful for larger LLMs, given the established scaling laws in language model training. This finding underscores the potential of \ourMethod{} for enhanced performance without the requirement for externally annotated long-context datasets.


% We first train identical LLMs (Mistral-7B and LLaMA 3.1-8B) with \ourMethod{}, SFT, and DPO on the same self-generated datasets as mentioned in~\Cref{subsec:train_setup}. As SFT cannot utilize the preference pair data, we apply SFT to the instructions with the chosen responses.

% \paragraph{\ourMethod{} outperforms naive SFT and DPO.}
% As shown in~\Cref{table:long_context_results}, the experimental results demonstrate that \ourMethod{} can consistently empower LLMs with much better performance than SFT and DPO across various long-context tasks. In~\Cref{fig:short_performance}, more importantly, \ourMethod{} always maintains the short-context performance, while SFT and DPO tends to achieve significant short-context performance drops during long-context alignments.

% Compared to SFT, the superior performance of \ourMethod{} can be attributed to its explicit incorporation of short-to-long preference, which is lost or only implicitly present in the chosen responses utilized by SFT. As DPO is also trained on our proposed short-to-long preference data, the key difference between \ourMethod{} and DPO here is the short-to-long constraint introduced in~\Cref{subsec:short_to_long_kl}. The long- and short- context performance gaps between \ourMethod{} and DPO highlight that our short-to-long constraint effectively addresses the implicitly problematic constraint in DPO, hence improving both performances from short to long.


% \subsection{External Comparison with SOTA Long-Context LLMs}
% \label{subsec:external_comparison}
% We further compare our \ourMethod-trained models (Mistral-7B and LLaMA3.1-8B) with SOTA long-context LLMs across various scales, to establish the place of ...


% \paragraph{Long-context annotation is not sufficient.}
% The results, presented in~\Cref{table:long_context_results}, showcase the competitiveness of our approach. In terms of performance on smaller models, Mistral-7B-\ourMethod{} significantly outperforms other long-context models like LWM-7B and YaRN-Mistral. Notably, our model achieves comparable performance to GLM-4-9B, despite the latter being trained on manually annotated long-context data up to 128K seq length. 
% This highlights the efficiency of our \ourMethod{} to transfer performance from short to long via self-generated data.
% More specifically, our models excel on the En.QA task in $\infty$Bench, which involves free-form QA over long books. In this task, our models exceed both GLM-4-9B and GLM-4-9B-1M by significant margins. This task is particularly challenging, even for humans, resulting in the inabilities to annotate such long-context data. This demonstrates the limitations of relying solely on long-context annotations. Our \ourMethod{}, by transferring short-context capabilities to long contexts, proves more scalable and effective for diverse and complex tasks.

% \paragraph{Dominant LLMs Yet to Conquer Long-Context Scenarios}

% When compared to superior models like GPT-4 and LLaMA3.1-70B, our \ourMethod{}-trained models demonstrate comparable or better long-context performance, despite being trained on much smaller LLMs. This observation suggests that even top-tier LLMs have not yet dominated long-context scenarios as they have in short-context tasks. This may be attributed to the scarcity of high-quality, large-scale long-context data scale, which is particularly crucial for larger LLMs given the scaling laws.










% This performance gap may be attributed to two main factors: (1) The scarcity of high-quality, large-scale long-context data, which is particularly crucial for larger LLMs given the scaling laws; and (2) The effectiveness of \ourMethod{} in internally transferring short-context performance to long contexts, thus circumventing the limitations of data scarcity.

% These findings underscore the potential of \ourMethod{} as a powerful and efficient approach for enhancing long-context capabilities in language models, offering a promising direction for future research and development in this area.


% \subsubsection{Implications and Advantages of \ourMethod{}}

% The strong performance of \ourMethod{} across various comparisons highlights several key advantages:

% \textbf{Efficient Training:} By internally transferring short-context performance to long contexts, \ourMethod{} circumvents the need for large-scale, high-quality long-context data, which is particularly beneficial for larger LLMs.

% \textbf{Cost-Effective:} Our approach eliminates the need for expensive long-context annotations, making it more accessible and scalable.

% \textbf{Versatility:} The ability to perform well on diverse and complex tasks demonstrates the method's adaptability to various long-context scenarios.

% \textbf{Competitive Performance:} Even when compared to larger models or those trained on manually annotated data, \ourMethod{} shows comparable or superior performance, especially on challenging tasks.

% These results underscore the potential of \ourMethod{} as a powerful and efficient approach for enhancing long-context capabilities in language models, offering a promising direction for future research and development in this area.





% \paragraph{\ourMethod{} Largely Improves the Long-Context Performance.} We first demonstrate the exceptional long-context performance of \ourMethod{} by two-side comparison: (1) \textit{internal comparison}, where we compare the long-context performance improvement of identical LLMs trained with naive SFT, DPO, and our \ourMethod{}; (2) \textit{external comparison}, where we compare Mistral-7B and LLaMA3.1-8B trained upon \ourMethod{} with a variety of SOTA long-context LLMs across small and large scales.

% For internal comparison, as shown in~\Cref{table:long_context_results}, \ourMethod{} yields significantly larger performance gains than naive SFT and DPO across the board. Note that the models are trained on the same data (SFT is applied to chosen responses). Therefore, the performance gains demonstrate the effective of short-to-long preference, which is explicitly embedded in \ourMethod{} but lost (or implicitly) embedded in SFT and DPO.


% For external comparison, when comparing to exceptional long-context LLMs,  models trained with \ourMethod{} still hold competitiveness. For smaller LLMs, as illustrated in~\Cref{table:long_context_results}, Mistral-7B-\ourMethod{} surpassing LWM-7B and YaRN-Mistral by large margins. Note that GLM-4-9B involves annotating long-context data up to 128K lengths, while our \ourMethod{} trained with self-generated data can still achieve on par long-context performance. Specifically, our models surpasses both GLM-4-9B and GLM-4-9B-1M on the En.QA task in $\infty$Bench by large margins. The task involves free-form QA over long books, which is challenging even for humans. Therefore, the prior from long-context annotation for such tasks may be limited, leading to the poor performance. While our \ourMethod{} trained on self-generated data to transfer the short-context capabilities to long context, which not only saves the costs for long-context annotation, but also be more scalable for more diverse and complex tasks. In addition, when comparing to the superior GPT-4 and LLaMA3.1-70B, \ourMethod{} still demonstrate on par or better long-context performance even trained with much smaller LLM.
% This indicates that these superior LLMs have not dominated the long-context scenarios as they do in short-context scenarios. This may be caused by the lack of large-scale long-context data that is of high quality, which is more eager for larger LLMs considering the scaling law. While \ourMethod{} internally transfers the short-context performance to long context, hence circumventing such limitations.



% As shown in~\Cref{table:long_context_results}, \ourMethod{} can empower Mistral-7B-Instruct-v0.2 and LLaMA-3.1-8B-Instruct with significantly improved performance on long-context benchmarks. Comparing to naive SFT and DPO, \ourMethod{}


% when trained with \ourMethod{}, Mistral-7B-Instruct-v0.2 and LLaMA-3.1-8B-Instruct can achieve significantly improved performance on long-context benchmarks. More specifically, our \ourMethod{} can empower LLMs of 7/8B parameters with long-context capabilities (e.g., summarization and long-book QA) of GPT-4-128K level. When comparing to smaller LLMs at similar scale, 


\begin{figure}[!t]
\centering
\includegraphics[width=0.96\textwidth]{figures/ablation.pdf}
% \vspace{-2.0em}
\caption{Long- and short-context performance comparison among \ourMethod{}, SFT on chosen responses (\textbf{SFT-Chosen}), SFT on rejected responses (\textbf{SFT-Rejected}), DPO, and SFT on chosen responses with short-to-long constraint (\textbf{SFT-Chosen-Constraint}). }
\vspace{-0.7em}
\label{fig:ablation}
\end{figure}








% \begin{table}[!t]
%     \centering
%     % \begin{tabular}{c|c}
%     %      &  \\
%     %      & 
%     % \end{tabular}
%     \caption{A Table to display the short-context and instruction-following performance.}
%     \label{tab:my_label}
% \end{table}



% \begin{figure}[!t]
%     \centering
%     % \includegraphics[width=0.5\linewidth]{}
%     \caption{A figure to display the reward during optimization.}
%     \label{fig:enter-label}
% \end{figure}



% \begin{figure}[!t]
%     \centering
%     % \includegraphics[width=0.5\linewidth]{}
%     \caption{A figure to show case studies.}
%     \label{fig:enter-label}
% \end{figure}

% \textcolor{red}{\FiveStar}
\subsection{Ablation Studies}
\label{subsec:ablation}
We conduct comprehensive ablation studies to investigate the efficacy of components in \ourMethod{}:

% \paragraph{Short-to-long preference contributes to the long-context capabilities.} The core of \ourMethod{} is to learn the short-to-long preference between chosen and rejected responses given short and long contexts, respectively. To investigate the effectiveness of the proposed preference, we compare \ourMethod{} with naive SFT on chosen (SFT-Chosen) and rejected (SFT-Rejected) responses, respectively. Note that the SFT-Chosen implicitly incorporates the short-context preference as it utilizes the responses given the short context, while the preference is totally removed in SFT-Rejected. In terms of long-context performance (RULER-NIAH), as depicted in~\Cref{fig:ablation}, our \ourMethod{} significantly surpasses all SFT variants across the training procedure, indicating that the  short-to-long preference is more effective to improve the long-context capabilities.



% \paragraph{Short-to-long constraint largely retains short-context capabilities.} As shown in~\Cref{fig:ablation}, when removing the short-to-long constraint in \ourMethod{}, the DPO with short-to-long preference still performs significantly worse across both long- and short-context tasks. Specifically, the short-context capabilities drastically degenerate during initial training. However, even when empower the naive SFT with our proposed short-to-long constraint without explicit short-to-long preference, the short-context performance remains consistent with the original LLMs after long-context alignment. This demonstrates the effectiveness of our short-to-long constraint for maintaining the short-context capabilities.




\paragraph{Effectiveness of short-to-long preference.} The core of \ourMethod{} is learning the short-to-long preference between chosen and rejected responses given short and long contexts, respectively. To evaluate this component's effectiveness, we compare \ourMethod{} with two baseline methods: SFT on chosen responses (SFT-Chosen) and on rejected responses (SFT-Rejected). SFT-Chosen implicitly incorporates short-context preference, while SFT-Rejected entirely omits it. As illustrated in~\Cref{fig:ablation}, \ourMethod{} consistently outperforms both SFT variants in long-context performance (RULER-NIAH) throughout the training process. This substantial improvement underscores the efficacy of our short-to-long preference approach in enhancing long-context capabilities.

\paragraph{Effectiveness of short-to-long constraint.} To assess the impact of our short-to-long constraint, we compare \ourMethod{} with DPO upon short-to-long preference that removes this constraint. As evident in~\Cref{fig:ablation}, the unconstrained DPO demonstrates markedly inferior performance throughout the training process, both in long- and short-context tasks. Notably, short-context capabilities degrade rapidly in DPO during the initial training. Conversely, when we apply our short-to-long constraint to naive SFT without explicit short-to-long preference, the model maintains short-context performance on par with the original LLMs, even after long-context alignment. These results demonstrate the crucial role of our short-to-long constraint in preserving short-context capabilities.


\paragraph{Impact of NLL loss.} 
We investigate the effect of incorporating a negative log-likelihood (NLL) loss over long context input and chosen response in~\Cref{eq:final_loss} during \ourMethod{} training. As shown in~\Cref{fig:ablation}, removing the NLL loss significantly degrades the long-context performance of \ourMethod{} across the training procedure. Specifically, the convergence of training for long-context performance becomes slower. This demonstrates the crucial role of NLL loss in enhancing long-context capabilities without resorting to continual training on long data.

% \paragraph{Synergistic effect of preference and constraint.} Our ablation studies reveal that the combination of short-to-long preference and constraint yields the best overall performance. While the preference primarily drives improvements in long-context tasks, the constraint ensures the retention of short-context capabilities. This synergy allows \ourMethod{} to achieve superior performance across varying context lengths, highlighting the importance of both components in our approach.


% \paragraph{Ablation on loss weight}


% \paragraph{Case Study.}

