\section{\ourMethod: Short-to-Long Preference Optimization}
\label{sec:method}





Motivated by the challenges of data annotation and performance balance during long-context alignment, we introduce the Short-to-\textbf{Long} \textbf{P}reference \textbf{O}ptimization (\textbf{\ourMethod}), to effectively empowers a short-context LLM self-evolve to a long-context counterpart while preserving its original short-context capabilities. 
The foundation of \ourMethod{} lies in the transfer of capabilities deeply ingrained during short-context alignment to long-context scenarios by learning from short-to-long preference (\Cref{subsec:short_to_long_p}). Additionally, \ourMethod{} incorporates a short-to-long constraint based on the KL divergence between short- and long-context models during training, to maintain the short-context performance in a simple yet effective way (\Cref{subsec:short_to_long_kl}). In~\Cref{subsec:self_evolve}, we present the details of curating short-to-long preference data without external guidance and self-evolving long context training process using \ourMethod.



% Since the human values have been deeply ingrained during short-context alignment, the first key of \ourMethod{} is to transfer these established values to long context by learning from short-to-long preference (\Cref{subsec:short_to_long_p}). Subsequently, \ourMethod{} incorporates a short-to-long constraint based on the KL divergence between short- and long-context model distribution during training, to lossesly maintain the short-context performance in a simple yet effective way (\Cref{subsec:short_to_long_kl}). In~\Cref{subsec:self_evolve}, we demonstrate the details of the short-to-long data construction and self-evolving long-context alignment process using \ourMethod.

% proximity

% As the short-context performance tend to degrade during long-conte


% our goal is to derive an effective method to align a short-context LLM to evolve to long context. Since the human preference has been deeply ingrained during short-context alignment,

% align short-context LLM to long using model itself. Our key insight is any short-context LLM is already a weak long-context counterpart. Hence, 


% Human values and preferences, deeply ingrained during short-
% context alignment, can be effectively transferred to longer contexts without requiring additional
% external information. Therefore, the core of LongPO is to learn from short-to-long preference data,
% which comprises paired responses generated by the short-context model for identical instructions,
% but given long contexts and corresponding compressed short-context counterparts, respectively. The
% discrepancies between each paired response reveal preferences cultivated during short-context align-
% ment that may be diminished in under-aligned long-context scenarios. By optimizing the model
% towards these preferences in long contexts, we effectively bring forward the capabilities and val-
% ues established during short-context alignment. 


% In this section,

% In this section, we first introduce the 


% Our approach first assumes access to a well-aligned short-context LLM, and a long-context corpus to build the self-generated question-answering data. Note that we hand-craft


% seeks to extend the context length  to produce exceptional long-context performance while maintaining both short-context performance and instruction-following capability.


% a small amount of long-context pertaining corpus that might 



% \subsection{Problem Overview}

% In practice, annotating rather amount of long-context alignment data by human or proprietary models is expensive and even impractical. Therefore, our approach solely assumes access to a well-aligned short-context LLM, with a long-context plain corpus. Note that the long-context corpus can refrain from crafting, and can be sampled and collected from existing pretraining corpus of LLMs.

% \paragraph{Initialization.} Annotating a substantial amount of long-context alignment data manually or through proprietary models is expensive and even impractical. Consequently, our approach relies solely on access to a well-aligned short-context LLM in conjunction with a long-context plain corpus. Note that the long-context corpus need not be meticulously crafted, as it can be sampled and extracted from existing pretraining corpora of LLMs.

% \paragraph{Problem Definition.} improve the long-context performance via model itself while maintain the performance.
% Self-evolving, introduce DPO and then we have other two goals: prefer the short-context and strengthen the long-context, long-context learning from short-context preference.



% Due to the expensive annotation for 

% Our approach first assumes access to a well-aligned short-context LLM, and a long-context corpus to build the self-generated question-answering data. Note that we hand-craft




% A instruction-following and short-context LLM, some corpus that may be included in LLM's pretraining data.

\subsection{Learning from Short-to-Long Preference}
\label{subsec:short_to_long_p}


As outlined in~\Cref{sec:preliminary}, aligning LLMs with human preference typically relies on datasets comprising ranked responses to identical prompts or instructions. However, in long-context scenarios, constructing such datasets becomes impractical due to the extensive effort required for annotation. To circumvent the external data annotation, we leverage the \textit{short-to-long preference} to internally transfer capabilities well-established in the short-context alignment of LLMs to long-context counterpart.

Concretely, we assume access solely to a short-context LLM $\pishort$ that has been well aligned. Given a long input $\xl = [\cl; \il]$ where $\cl$ is the long context and $\il$ is the instruction, we can acquire the response $\yl \!\sim\! \pishort(y \mid \xl)$ by conditioning on the entire context. Due to the limitations of $\pishort$ in handling long contexts, $\yl$ is likely to be of lower quality.


We then hypothesize an ideal extractor $\mathcal{F}$ that can rigorously identify and extract all essential information $\cs$ within $\cl$ relevant to addressing $\il$:
\begin{equation}
\label{eq:extractor}
    \cs = \mathcal{F}(\cl, \il).
\end{equation}
By querying the instruction $\il$ based on $\cs$, we obtain a new answer $\ys \sim \pishort(y \mid \xs)$, where $\xs = [\cs; \il]$. 
As $\cs$ is a shortened context for $\il$, the well-aligned short-context model $\pishort$ should be capable of producing a high-quality answer that aligns with human preferences.
% utilizing the short-to-long preference, to bring forward the short-context intention to long-context, hence internally achieving the long-context alignment.

% to propose the short-to-long preference implicitly transfer human preferences learned

Intuitively, $\ys$ can serve as a high-quality answer even when giving the whole long context, as its conditioned context is self-contained for instruction $\il$. Hence, we definite the short-to-long preference distribution $p^{\text{SL}}$ based on Bradley-Terry (BT) model following~\Cref{eq:bt_model}:
\begin{equation}
\label{eq:stl_bt_model}
    p^{\text{SL}}(\ys \succ \yl \mid \xl)= \sigma(r(\xl, \ys)- r(\xl, \yl)).
\end{equation}
We now steer a policy model $\pitheta$ (initialized with $\pishort$) to follow the preference distribution $p^{\text{SL}}$, forming the \ourMethod{} objective:
\begin{equation}
    \label{eq:longpo_init}
        \mathcal{L}_{\text{\ourMethod}}(\pi_\theta; \pi_{\text{ref}}) = - \mathbb{E}_{(\xs, \xl, \ys, \yl) \sim \mathcal{D}^{\text{SL}}}\left[\sigma(r_\theta(\xl, \ys)- r_\theta(\xl, \yl))\right],
\end{equation}
where $\mathcal{D}^{\text{SL}}$ is the short-to-long preference data consisting of quadruples $\left(\xs, \xl, \ys, \yl\right)$.
This objective encourages the policy model to consistently accommodate the well-aligned short-context preference while deviating the under-aligned long-context preference. Therefore, \ourMethod{} internally transfers preferences from short to long contexts without requiring external supervision, effectively addressing the challenge of long-context data annotation.


% DPO aims to align the policy model with preferences embedded in paired answers $(\ys, \yl)$, where $\ys$ and $\yl$ represent the preferred (winning) and dispreferred (losing) responses to a given instruction. However, injecting human preference into answer pairs based on long contexts is often impractical. To address this challenge, we introduce the \textit{short-to-long preference}, which implicitly transfers human preferences learned during short-context alignment to long-context scenarios.





% Our approach leverages a short-context LLM $\pishort$ to generate paired answers that serve as long-context preference data. This process involves the following steps:

% \begin{enumerate}
%     \item Given a long input $\xl = [\cl; \il]$, where $\cl$ is the long context and $\il$ is the instruction, we hypothesize an ideal extractor $\mathcal{F}$ that can identify and extract all essential information $\cs$ within $\cl$ relevant to addressing $\il$:
%     \begin{equation}
%         \cs = \mathcal{F}(\cl, \il).
%     \end{equation}
%     \item We then apply the instruction $\il$ to $\cs$ to obtain an answer $\ys \sim \pishort(y \mid \xs)$, where $\xs = [\cs; \il]$. As $\cs$ is a shortened context, the well-aligned short-context model $\pishort$ should be capable of producing a high-quality answer that aligns with human preferences.

%     \item Conversely, due to the model's limitations in handling long contexts, the answer $\yl \sim \pishort(y \mid \xl)$ generated by conditioning on the entire long context is likely to be of lower quality and less preferred.

%     \item We can naturally obtain paired answers by regarding $\ys$ and $\yl$ as the preferred and dispreferred answers, respectively.
% \end{enumerate}


% \subsection{Short-to-Long Constraints}
% Recall that long-context alignment usually leads to a inbalance between long- and short-context performance. This issue may be alleviated by carefully ablating the long and short data scale and mixing proportion across various lengths between original and target context lengths. This would consume many ... In our \ourMethod, we identify that the \textit{shifting} of models during long-context alignment may attribute to the improper (or missing) constraint.

% The objective of DPO implies a KL divergence, $\beta\mathbb{D}_{\textrm{KL}}\bigl[\pitheta(y\mid x)\mid \mid \piref(y\mid x)\bigr]$, which serves as a constraint to avoid the deviation too far from the reference model. Specifically, the constraint $\constraint$   for long input $\xl$ would be
% \begin{equation}
%     \constraint = \beta\mathbb{D}_{\textrm{KL}}\bigl[\pitheta(y\mid \xl)\mid \mid \piref(y\mid \xl)\bigr].
% \end{equation}
% Note that the reference model is usually the short-context model $\pishort$ itself and is not capable of harnessing the long context well. This indicates a problematic reference distribution $\piref(y\mid \xl)$, which resulting the deviation from short-context model distribution. To address this issue, we propose the short-to-long constraint leveraging the quadruples in~\Cref{subsec:short_to_long_p}. Recall that $\xs$ has all the information required in $\xl$ to access a satisfactory response. Therefore, for an ideal reference model that is capable of handling context lengths from short to long, we should have
% \begin{equation}
%     \mathbb{D}_{\textrm{KL}}\bigl[\piref(y\mid \xl)\mid \mid \piref(y\mid \xs)\bigr] = \mathbb{D}_{\textrm{KL}}\bigl[\piref(y\mid \xs)\mid \mid \pishort(y\mid \xs)\bigr] = 0.
% \end{equation}
% Here, we derive the adjusted constraint
% \begin{equation}
%     \constraint = \beta\mathbb{D}_{\textrm{KL}}\bigl[\pitheta(y\mid \xl)\mid \mid \pishort(y\mid \xs)\bigr].
% \end{equation}
% This constraint indicates the policy model $\pitheta$ on long context should not deviate from the short-context model $\pishort$ by giving



\subsection{Short-to-Long Constraint}
\label{subsec:short_to_long_kl}
Long-context alignment often leads to an imbalance between long- and short-context performance. While this issue can be mitigated by carefully calibrating the scale and mixing proportion of long and short data across various context lengths, such an approach is resource-intensive and time-consuming. Moreover, an excessive incorporation of short-context data may inadvertently lead to insufficient long-context alignment. In \ourMethod{}, we recognize that the degradation in short-context performance during long-context alignment may be attributed to an improper (or missing) constraint in current alignment methods.

Specifically, the RLHF and DPO objectives (implicitly) include a KL divergence term, $\beta\mathbb{D}_{\textrm{KL}}\bigl[\pitheta(y\mid x)\mid \mid \piref(y\mid x)\bigr]$, which serves as a constraint to prevent excessive deviation from the reference model in~\Cref{eq:RL}. For a long input $\xl$, this constraint $\mathcal{C}$ is expressed as:
\begin{equation}
\label{eq:ideal_kl}
    \constraint = \beta\mathbb{D}_{\textrm{KL}}\bigl[\pitheta(y\mid \xl)\mid \mid \piref(y\mid \xl)\bigr].
\end{equation}
However, the reference model is typically the short-context model $\pishort$ itself, which is not adept at handling long contexts. This results in a problematic reference distribution $\piref(y\mid \xl)$, leading to undesired deviation from the short-context model distribution.

To address this issue, we propose a \textit{short-to-long constraint} leveraging the quadruples introduced in~\Cref{eq:longpo_init}. Recall that $\xs$ contains all the essential information from $\xl$ required to generate a satisfactory response, $\pishort$ can serve as a proficient reference model conditioned on $\xs$. While for an ideal reference model $\piref^\ast$ capable of handling context lengths from short to long, we should have:
\begin{equation}
\label{eq:equal_kl}
    \mathbb{D}_{\textrm{KL}}\bigl[\piref^\ast(y\mid \xl)\mid \mid \piref^\ast(y\mid \xs)\bigr] = \mathbb{D}_{\textrm{KL}}\bigl[\piref^\ast(y\mid \xs)\mid \mid \pishort(y\mid \xs)\bigr] = 0,
\end{equation}
namely $\piref^\ast(y\mid \xl)$ and $\pishort(y\mid \xs)$ are identical distribution following Gibbsâ€™ inequality.
We hence derive an adjusted short-to-long constraint between short-context reference model and ``long-context'' policy model given contexts of different lengths:
\begin{equation}
\label{eq:adjust_constraint}
    \constraint^\prime = \beta\mathbb{D}_{\textrm{KL}}\bigl[\pitheta(y\mid \xl)\mid \mid \pishort(y\mid \xs)\bigr].
\end{equation}

This refined constraint ensures that the policy model $\pitheta$ operating on long contexts does not deviate significantly from the short-context model $\pishort$ when provided with the essential information. By enforcing this constraint, we aim to preserve the short-context performance during long-context alignment, thereby addressing the imbalance issue in a more principled manner.

By incorporating the short-to-long constraint in~\Cref{eq:dpo_reward}, we have a refined reward function for long input $\xl$ (following derivation in~\Cref{subsec:derive_longpo}):
\begin{equation}
\label{eq:longpo_reward}
r_\theta^{\text{\ourMethod}}(\xl, y) = \beta \log \frac{\pi_\theta(y \mid \xl)}{\pishort\left(y \mid \xs\right)} + \beta \log Z(\xl, \xs),
\end{equation}
where $\xs$ is extracted from $\xl$ as illustrated in~\Cref{eq:extractor}.
Hence we access the \ourMethod{} objective:
\begin{equation}
    \label{eq:longpo}
    \begin{aligned}
        \mathcal{L}_{\text{LongPO}}(\pitheta; \pishort) &= - \mathbb{E}_{(\xs, \xl, \ys, \yl) \sim \mathcal{D}^{\text{SL}}}\left[\sigma(r^{\text{\ourMethod}}_\theta(\xl, \ys)- r^{\text{\ourMethod}}_\theta(\xl, \yl))\right] \\
        &= - \mathbb{E}_{(\xs, \xl, \ys, \yl) \sim \mathcal{D}^{\text{SL}}}\left[ \log \sigma \left( \beta \log \frac{\pitheta(\ys \mid \xl)}{\pishort(\ys \mid \xs)} - \beta \log \frac{\pitheta(\yl \mid \xl)}{\pishort(\yl \mid \xs)}\right) \right].
    \end{aligned}
\end{equation}



% This approach allows us to implicitly transfer human preferences learned in short-context scenarios to long-context settings, thereby addressing the practical limitations of direct human preference injection in long-context scenarios.



% \subsection{Learning from Short-to-Long Preference}
% As outlined in~\Cref{subsec:dpo}, DPO aims to align the policy model with preferences embedded in paired answers $(\ys, \yl)$, where $\ys$ and $\yl$ denote the preferred (winning) and dispreferred (losing) responses to a given instruction. However, it is impractical to inject human preference to a pair of answers based on a long context.  We thus introduce the \textit{short-to-long preference} to implicitly transfer the human preference learned during short-context alignment to long context.

% Given a long input $\xl \meq \left[\cl; \il \right]$ where $\cl$ is the long context and $\il$ is the instruction, 
% our goal is to utilize a short-context LLM $\pishort$ to generate the paired answers, but as long-context preference data. To facilitate this, we suppose there is an ideal extractor $\mathcal{F}$, that given a long context $\cl$ and a instruction $\il$, it can identify and extract all essential information $\cs$ within $\cl$ that is pertinent to addressing $\il$, namely
% \begin{equation}
%     \cs = \mathcal{F}\left(\cl, \il \right).
% \end{equation}
% By apply the instruction $\il$ upon $\cs$, we can acquire an answer $\ys \!\sim\! \pishort \left(y \mid \xs \right) $ where $\xs \meq \left[\cs; \il \right]$. As $\cs$ is yet a shorted context, the well-aligned short-context model $\pishort$ should be capable of dropping high-quality answer that matches human preference. However, due to the unsatisfying long-context performance, the answer $\yl \!\sim\! \pishort \left(y \mid \xl \right) $ by conditioning on the entire long context should be low-quality and less preferred. Hence, we can naturally access the paired answers by regarding $\ys$ and $\yl$ as the preferred and disprefered answers, respectively.

% Hence, we can naturally acquire a high-quality answer $\ys \!\sim\! \pishort \left(y \mid \xs \right) $ where $\xs \meq \left[\cs; \il \right]$. As 

% Similarly, $\yl \!\sim\! \pishort \left(y \mid \xl \right) $ by conditioning on the entire long context. As the $\pishort$ cannot yield satisfying long-context performance, $\yl$ is naturally low-quality and should be less preferred than $\ys$. Hence, we can access to the pair of answer $\left(\yl, \ys \right)$.




% \subsection{Regrading Short-Context LLM as Longer}
% \subsection{Deriving the LongPO Objective}

% % Short-context LLM is secretly a long-context reference model

% As outlined in~\Cref{subsec:dpo}, DPO aims to align the policy model $\pitheta$ with preferences embedded in pair answers $(\ys, \yl)$. Specifically, \citet{yuan2024selfrewarding} proposed that the pair answers can be generated and ranked by the policy model itself,  hence model can evolve from weak to strong upon its own preference without data annotation. As short-context LLMs have demonstrated the length extrapolation capabilities, intuitively, a well-aligned short-context LLM is capable of serving as an under-aligned long-context LLM.
% Inspired by this, we seek to reframe the long-context alignment challenge as a self-evolving preference optimization problem.



% Given a long input $\xl \meq \left[\cl; \il \right]$ where $\cl$ is the long context and $\il$ is the instruction, 
% our goal is to utilize a short-context LLM $\pishort$ to generate the paired answers, but as long-context preference data.

% the naive DPO in~\Cref{eq:dpo} would optimize the model $\pitheta$ via preference loss upon the pair answers $(\ys, \yl)$. This objective of DPO is to align the policy more closely to the preference in $(\ys, \yl)$.

% % \citet{yuan2024selfrewarding} propose that the pair answers can be generated and ranked by model itself, hence model can evolve from weak to strong upon its own preference without data annotation. 

% % Such ``self-rewarding" is utilized based on an aligned (but not )
% As short-context LLMs have demonstrated the length extrapolation capabilities, intuitively, a well-aligned short-context LLM is capable of serving as an under-aligned long-context LLM. Therefore, the long-context alignment can be transformed as a weak-to-strong improvement problem upon long context.


% % the LLM should be potentially to self-evolve from short to long. Due to the 

% $(\ys, \yl) \!\sim\! \pishort(y \mid \xl)$. This in
% % \begin{equation}
% %     \label{eq:naive_longdpo}
% %     \mathcal{L}_{\text{DPO}}(\pitheta; \piref) = - \mathbb{E}_{(\xl, \highlight{\ys}, \highlight{\yl}) \sim \mathcal{D}}\left[ \log \sigma \left( \beta \log \frac{\pitheta(\highlight{\ys} \mid \xl)}{\highlight{\piref}(\highlight{\ys} \mid \xl)} - \beta \log \frac{\pitheta(\highlight{\yl} \mid \xl)}{\highlight{\piref}(\highlight{\yl} \mid \xl)}\right) \right].
% % \end{equation}
% As highlighted, however, there is a lack of high-quality paired answers \((\highlight{\ys}, \highlight{\yl})\) for \(\xl\) in the absence of annotations. Additionally, the short-context model fails to serve as an appropriate reference model $\highlight{\piref}$ due to its limited ability to effectively handle long input \(\xl\).




% To bridge the gap, we introduce the Short-to-\textbf{Long} \textbf{P}reference \textbf{O}ptimization (\ourMethod{}), seeking to ... We follow the intuition that any short-context LLM $\pishort$ is potentially a rudimentary long-context model, hence the answer $\yl \!\sim\! \pishort \left(y \mid \xl \right) $ is low-quality for long input.
% Consequently, our objective is to effectively propagate its inherent short-context capabilities across longer contexts. To facilitate this, we suppose there is a powerful extractor $\mathcal{F}$, that given a long context $\cl$ and a instruction $\il$, it can identify and extract all essential information $\cs$ within $\cl$ that is pertinent to addressing $\il$, namely
% \begin{equation}
%     \cs = \mathcal{F}\left(\cl, \il \right).
% \end{equation}
% As $\cs$ is yet a shorted context, the well-aligned short-context model $\pishort$ should be capable of harnessing it. Hence, we can naturally acquire a high-quality answer $\ys \!\sim\! \pishort \left(y \mid \xs \right) $ where $\xs \meq \left[\cs; \il \right]$.
% % spread its own short-context capabilities over long context.

% We now access to the pair of answer $\left(\yl, \ys \right)$.


\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{figures/longpo_refined.png}
    \caption{The procedure of generating short-to-long preference data from step 1 to 7.}
    \label{fig:data_generation}
\end{figure}

\subsection{Self-Evolving}
\label{subsec:self_evolve}
\paragraph{Initialization.} \ourMethod{} relies solely on access to a well-aligned short-context LLM, i.e., $\pishort$, in conjunction with a long-context plain corpus. Note that the long-context corpus need not be meticulously crafted, as it can be sampled and extracted from existing pretraining corpora of LLMs.



% \paragraph{Short-to-Long Preference Data Construction.}
% Recall that the short-to-long preference data in~\Cref{subsec:short_to_long_p} is based on an ideal extractor that losslessly retrieves all information related to the instruction in a long context. That said, current retrieval modules cannot meet such requirements. To build such short-to-long preference data, we follow two steps:
% \begin{itemize}
%     \item For a long document $\cl$, we reversely prompt the short-context LLM $\pishort$ to generate instructions $\il$ based on a shortening chunk $\cs$ using Self-Instruct~\citep{wang-etal-2023-self-instruct}. Note that $\cs$ can be randomly sampled from $\cl$ and self-contained for $\il$ without the requirement of retrieval. In addition, to keep the diversity of instructions, we would generate a instruction pool $\mathbb{I}$ first and then sample $\il \sim \mathbb{I}$.
%     \item We then prompt the $\pishort$ to generate the chosen response $\ys$ and rejected response $\yl$ based on the short context $\xs$ and long context $\xl$, respectively.
% \end{itemize}


\paragraph{Construction of short-to-long preference data.}
The construction of short-to-long preference data $\mathcal{D}^{\text{SL}}$ introduced in~\Cref{subsec:short_to_long_p} assumes an impractical extractor capable of retrieving essential information from long contexts for each instruction. To satisfy this hypothesis, we reversely prompt $\pishort$ to generate instructions for shortened chunks within long documents. This ensures that the short context information is self-contained for instructions. Concretely, our data construction process involves two steps as displayed in~\Cref{fig:data_generation}:
\begin{enumerate}
    \item \textbf{Instruction Generation.} 
    % We employ the Self-Instruct~\citep{wang-etal-2023-self-instruct} strategy using a short-context Language Model ($\pishort$) to generate a diverse pool of instructions $\mathbb{I}$. 
    For each long document $\cl$, we randomly sample a shortened chunk $\cs$ and prompt the $\pishort$ to generate an instruction via the Self-Instruct~\citep{wang-etal-2023-self-instruct}. To ensure the diversity of instructions, the model is prompted to generate an instruction pool first and then we randomly sample an instruction $\il$ from this pool.
    \item \textbf{Response Generation.} Using the generated instruction $\il$, we prompt $\pishort$ to produce two responses: a chosen response $\ys \!\sim\! \pishort(y\mid \xs)$ based on the short context $\xs$, and a rejected response $\yl \!\sim\! \pishort(y\mid \xl)$ derived from the long context $\xl$. 
    % \gc{This step simulates the preference between short and long context responses, allowing us to study the behavior of the model in different context lengths.}
\end{enumerate}


% This methodology enables us to create a dataset that reflects the theoretical short-to-long preferences discussed in \Cref{subsec:short_to_long_p}, while accounting for real-world constraints in information retrieval and processing.




\paragraph{Iterative self-evolving training with \ourMethod.}
\ourMethod{} employs an iterative process to extend LLM context length. Initially, a short-context LLM $\pishort$ generates short-to-long preference data for documents of length $L^1$. The resulting model after \ourMethod{} training, now capable of handling $L^1$ contexts, then serves as the new ``short-context LLM'' for the next iteration, generating data for an extended length $L^2$. This process repeats, progressively increasing context length capacity.


As there are multiple short chunks $\cs \!=\! \{\cs^i\}_{i=1}^{n}$ within a long document $\cl$, we collect the instruction-response triples $(\il^i, \ys^i, \yl^i)$ for each chunk within identical long document, to form a multi-turn dataset $\mathcal{\hat{D}}^{\text{SL}}$. We then aggregate the probabilities across all turns to produce a multi-turn \ourMethod{} objective:
\begin{equation}
    \label{eq:longpo_mt}
        \mathcal{L}_{\text{LongPO}}^{\text{MT}}(\pi_\theta; \pishort)
        = - \mathbb{E}_{(\xs, \xl, \ys, \yl) \sim \mathcal{\hat{D}}^{\text{SL}}}\left[ \log \sigma \left( \beta \log \frac{\sum_{i=1}^n \pi_\theta(\ys^i \mid \xl^i)}{\sum_{i=1}^n\pishort(\ys^i \mid \cs^i)} - \beta \log \frac{\sum_{i=1}^n\pi_\theta(\yl^i \mid \xl^i)}{\sum_{i=1}^n\pishort(\yl^i \mid \cs^i)}\right) \right],
\end{equation}
where $\xs\!=\!\{[\cs^i; \il^i]\}_{i=1}^n, \xl\!=\!\{[\cl; \il^i]\}_{i=1}^n, \ys\!=\!\{\ys^i\}_{i=1}^n, \text{and } \yl\!=\!\{\yl^i\}_{i=1}^n$. LLMs trained with \ourMethod{} do not necessarily involve continual training before, which may lead to instability when processing long contexts. To address this issue and stabilize the training process, we incorporate a continual training objective following~\citet{pang2024iterativereasoningpreferenceoptimization}. Specifically, we add the negative log-likelihood (NLL) loss over entire long chosen sequences $S_{\text{L}} \!=\! [\xl; \{\il^{i};\ys^{i}\}_{i=1}^n]$ to \ourMethod{} objective. Thus, our final training objective is:
\begin{equation}
\label{eq:final_loss}
    \mathcal{L}_\theta = \lambda \cdot \mathcal{L}_{\text{LongPO}}^{\text{MT}}(\pi_\theta; \pishort) + \mathcal{L}_\text{NLL}(\pitheta; S_{\text{L}}) = \lambda \cdot \mathcal{L}_{\text{LongPO}}^{\text{MT}}(\pi_\theta; \pishort) + \frac{\pitheta(S_{\text{L}})}{\left| S_{\text{L}} \right|}.
\end{equation}