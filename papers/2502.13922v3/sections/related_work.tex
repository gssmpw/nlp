\section{Related Work}

% \paragraph{Alignment of LLMs.} Aligning Large Language Models (LLMs) with human preferences and values has been crucial to uncovering the capabilities of LLMs from large-scale pretraining. The typical alignment starts with Supervised Finetuning (SFT) on annotated instruction-response pairs. Subsequently, the LLMs are aligned to follow human intention via Reinforcement learning from human feedback (RLHF), which involves reward model training and policy optimization. To facilitate the RLHF training, DPO and its variants are proposed to circumvent the training of reward model and learn preference from human-ranked response pairs. All alignment methods heavily rely on human-annotated data, however, the annotation for long-context data is both challenging and unreliable, as human may be not as reliable annotator as they are for short-context data.

% \paragraph{Long-context extending of LLMs.}
% To extend the context length of short-context LLMs, previous approaches involve scaling the rotary position embedding~\citep{su2022roformer} and then continual training on a small amount of long documents~\citep{chen2023extending, peng2023yarn, rozière2023code,Chen2023CLEXCL}. In addition, \citet{jin2024llm, an2024trainingfree} proposed hierarchical/chunked attention to extend the context length of LLMs without further training. However, these methods remain limited in practical tasks. Recently, \citet{dubey2024llama3herdmodels} proposes continual pretraining on large-scale long-context corpus (800B tokens) and mixing 0.1\% long-context data during SFT to further enhance the long-context capabilities. \citet{Zeng2024ChatGLMAF} utilized human-annotated long-context data to apply SFT and DPO for aligning long-context LLMs. However, these methods involve large-scale training or human annotation on long-context data, which is extremely expensive. 


\paragraph{Alignment of LLMs.} 
Aligning Large Language Models (LLMs) with human preferences and values has been crucial to unlocking their full potential from large-scale pretraining. The typical alignment process begins with Supervised Finetuning (SFT) on annotated instruction-response pairs. This is followed by Reinforcement Learning from Human Feedback (RLHF), which aligns LLMs more closely with human intentions through reward model training and policy optimization \citep{christiano2017deep,Ouyang2022TrainingLM, Bai2022ConstitutionalAH,stiennon2020learning}. To streamline RLHF training, Direct Preference Optimization (DPO)~\citep{Rafailov2023DirectPO} and its variants~\citep{Ethayarajh2024KTOMA, Azar2023AGT, Pang2024IterativeRP, Hong2024ORPOMP, Meng2024SimPOSP} have been proposed, eliminating the need for explicit reward model training by learning preferences directly from human-ranked response pairs. While these alignment methods have shown significant success, they heavily rely on human-annotated data. This reliance becomes problematic for long-context data, where human annotation is both challenging and potentially less reliable.

\paragraph{Long-context extending of LLMs.}
Extending the context length of LLMs has been approached through various methods. Some techniques involve scaling the rotary position embedding \citep{su2022roformer} followed by continual training on a small corpus of long documents \citep{chen2023extending, peng2023yarn, rozière2023code, Chen2023CLEXCL}. Alternative approaches, such as those proposed by \citet{jin2024llm, an2024trainingfree}, introduce hierarchical or chunked attention mechanisms to extend context length without additional training. However, these methods often involve limitations in practical applications. Recent advancements include the work of \citet{dubey2024llama3herdmodels}, who proposed continual pretraining on a massive long-context corpus (800B tokens) and incorporating a small fraction (0.1\%) of long-context data during SFT to enhance long-context capabilities. \citet{Zeng2024ChatGLMAF} utilizes human-annotated long-context data for SFT and DPO to align long-context LLMs. Despite their effectiveness, these methods require either extensive training or human annotation of long-context data, making them prohibitively expensive and lack scalability.


% \paragraph{Self-Evolving LLMs.} Recently, LLMs have been found to be capable of evolving from weak to strong on self-augumented data. \citet{Yuan2024SelfRewardingLM,liu2024directlargelanguagemodel} propose iterative training model-generated responses ranked by LLM-as-a-Judge prompting to improve model itself. \citet{li2024selfalignmentinstructionbacktranslation} introduces instruction backtranslation to produce self-augumenting data. Our method first considers the self-evolution property of LLMs for the short to long view, that utilizes the property to produce long-context LLMs without external annotation.
% LLM-as-a-Judge prompting.

\paragraph{Self-Evolving LLMs.} 
Recent works~\citep{Yuan2024SelfRewardingLM,liu2024directlargelanguagemodel,li2024selfalignmentinstructionbacktranslation} have unveiled the remarkable capability of Large Language Models (LLMs) to evolve from relatively weak to significantly stronger performance through self-augmented data. \citet{Yuan2024SelfRewardingLM, liu2024directlargelanguagemodel}  leverage iterative training on model-generated responses, ranked by LLM-as-a-Judge~\citep{Zheng2023JudgingLW} prompting, to enhance model itself. \citet{li2024selfalignmentinstructionbacktranslation} introduces the instruction backtranslation to produce self-augmenting data that further enhances model capabilities. Our work first extends the self-evolution property to the context length, to develop long-context LLMs without relying on external annotations.


