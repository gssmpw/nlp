% \section{Introduction}



% Large Language Models (LLMs) have made significant strides in aligning with human values, intentions, and behaviors, through various techniques such as Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Reinforcement Learning from Human Feedback (RLHF). However, these alignment methods predominantly cater to short-context scenarios, leaving the adaptation to long-context situations largely unexplored. 

% The development of long-context LLMs typically involves continual training on unsupervised long documents, 
% with notably less or no emphasis on tailored long-context alignment strategies compared to their short-context counterparts. However, \citet{dubey2024llama3herdmodels} suggest that relying solely on short-context data for alignment can lead to substantial degradation in long-context capabilities established during pretraining, underscoring the importance of dedicated long-context alignment.

% Long-context alignment presents two primary challenges.
% First, the data annotation for long contexts is a formidable and resource-intensive task. Human annotation becomes increasingly impractical as context length grows. While leveraging advanced LLMs like GPT-4 to generate synthetic long-context data is theoretically possible, this approach is prohibitively expensive and lacks scalability. Furthermore, the seemingly straightforward solution of concatenating existing short SFT datasets has been shown to yield unsatisfactory long-context performance~\citep{liu2023world}. Second, maintaining a balance between short- and long-context performance during alignment is non-trivial. As illustrated in~\Cref{fig:comparion_overview}, while long-context SFT and DPO on synthetic data improve long-context performance, they often come at the cost of diminished short-context and instruction-following capabilities.



% In this work, we introduce the Short-to-\textbf{Long} \textbf{P}reference \textbf{O}ptimization (\textbf{\ourMethod}), to enable LLM self-evolve to long context without relying on externally annotated data or compromising short-context capabilities. 
% We posit that human values and preferences, deeply ingrained during short-context alignment, can be effectively transferred to longer contexts without requiring additional external information.
% Therefore, the core of \ourMethod{} is to learn from short-to-long preference data, which comprises paired responses generated by the short-context model for identical instructions, but given long contexts and corresponding compressed short-context counterparts, respectively. The discrepancies between each paired response reveal preferences cultivated during short-context alignment that may be diminished in under-aligned long-context scenarios. By optimizing the model towards these preferences in long contexts, we effectively bring forward the capabilities and values established during short-context alignment. Furthermore, we recognize that the decline in short-context performance during long-context alignment may stem from an implicitly improper Kullback-Leibler (KL) constraint. To mitigate this issue, \ourMethod{} incorporates a short-to-long KL constraint, guiding the model to minimize deviation from its original short-context behavior even when giving the long context during training. We found that this straightforward constraint largely enhances the retention of short-context performance post-alignment with long contexts.
% % preserve the short-context performance after long-context alignment.

% We apply \ourMethod{} to train the Mistral-7B-Instruct-v0.2 and LLaMA 3.1-8B using self-generated short-to-long preference data, without additionally continual unsupervised training on long documents. The experimental results demonstrate that \ourMethod{}, as a long-context alignment method, largely outperforms naive SFT and DPO in both long- and short-context tasks. Notably, \ourMethod{} fully retain the performance of short-context LLMs before and after long-context alignment, whereas SFT and DPO  yield substantial performance degradation. In terms of long-context tasks, \ourMethod-trained LLMs are comparable with superior long-context LLMs at various scales, despite the latter often involving extensive continual training on hundreds of billions of tokens and labor-intensive long-context data annotation. These findings underscore the efficacy of our proposed method in addressing the challenges of long-context alignment while simultaneously preserving short-context capabilities, offering a more efficient and balanced approach to the development of long-context LLMs.






\section{Introduction}




% Remarkable capabilities of Large Language Models (LLMs) have been uncovered through large-scale pretraining and subsequent alignment with human intentions. The alignment helps to explore the capabilities of LLMs gained during pre-training, through various approaches such as Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Reinforcement Learning from Human Feedback (RLHF). 
% % However, these alignment methods predominantly cater to short-context scenarios, leaving the adaptation to long-context situations largely unexplored.  
% To replicate the success achieved in short-context scenarios, the development of long-context LLMs typically involves continuous unsupervised training and alignment on long context, similar to that for short-context LLMs. 


% However, as illustrated in~\Cref{fig:comparion_overview}], even dominant LLMs like GPT-4, which excel in short-context tasks, may underperform in long-context tasks compared to models with significantly fewer parameters and less advanced short-context capabilities. 
% This discrepancy may be attributed to the insufficient long-context alignment compared to the short-context counterpart. 
% % as the continual long-context training can be efficiently accomplished with minimum steps~\citep{Fu2024DataEF}.
% The alignment of LLMs hinges on annotated responses to diverse instructions. As context length expands, however, human annotation becomes increasingly impractical. While leveraging advanced LLMs like GPT-4 to generate synthetic long-context data is conceivable, this approach is prohibitively expensive and lacks scalability. This issue is further exacerbated for larger LLMs, which may require more data to converge out of scaling laws. Furthermore, the seemingly straightforward solution of concatenating existing short SFT datasets has been shown to yield unsatisfactory long-context performance~\citep{liu2023world}. 
% In addition, to maintain a balance between short- and long-context performance, the long-context alignment usually incorporates a substantial proportion of short samples. For example, the LLaMA-3.1 series includes merely 0.1\% long-context data during long-context SFT, based on comprehensive ablations. This may lead to insufficient alignment that fails to fully unlock the long-context capabilities of LLMs.




% First, the alignment relies heavily on annotated responses to diverse instructions. As context length expands, however, human annotation becomes increasingly impractical and cost-prohibitive. While leveraging advanced LLMs like GPT-4 to generate synthetic long-context data is conceivable, this approach lacks scalability and remains prohibitively expensive, especially for larger LLMs that may require more data to converge out of scaling laws. Moreover, the seemingly straightforward solution of concatenating existing short SFT datasets has been shown to yield unsatisfactory long-context performance~\citep{liu2023world}. Second, to maintain a balance between short- and long-context performance, long-context alignment typically incorporates a substantial proportion of short samples. For instance, the LLaMA-3.1 seriesinclude more than 99\% short-context data during long-context SFT based on empirical ablations. This limited exposure may result in insufficient alignment that fails to fully unlock the long-context capabilities of LLMs.


% compared to models with significantly fewer parameters and less advanced short-context capabilities.

% Insufficient long-context alignment can lead to substantial degradation in long-context capabilities established during pre-training~\citep{dubey2024llama3herdmodels}. As illustrated in~\Cref{fig:comparion_overview}, even dominant models like GPT-4, which excel in short-context tasks, may underperform in long-context scenarios compared to models with significantly fewer parameters and less advanced short-context capabilities. 

% Standard alignment for long-context LLMs presents two primary challenges. First, the alignment relies heavily on annotated responses to diverse instructions. As context length expands, however, human annotation becomes increasingly impractical and cost-prohibitive. While leveraging advanced LLMs like GPT-4 to generate synthetic long-context data is conceivable, this approach lacks scalability and remains prohibitively expensive, especially for larger LLMs that may require more data to converge out of scaling laws. Moreover, the seemingly straightforward solution of concatenating existing short SFT datasets has been shown to yield unsatisfactory long-context performance~\citep{liu2023world}. Second, to maintain a balance between short- and long-context performance, long-context alignment typically incorporates a substantial proportion of short samples. For instance, the LLaMA-3.1 seriesinclude more than 99\% short-context data during long-context SFT based on empirical ablations. This limited exposure may result in insufficient alignment that fails to fully unlock the long-context capabilities of LLMs\lx{Note: better rewrite this paragraph}.


Recent advancements in Large Language Models (LLMs) have revealed remarkable capabilities through extensive pretraining and subsequent alignment with human intentions. The alignment process, including methods such as Supervised Fine-Tuning (SFT)~\citep{wei2022finetuned}, Direct Preference Optimization (DPO)~\citep{Rafailov2023DirectPO}, and Reinforcement Learning from Human Feedback (RLHF)~\citep{christiano2017deep, Ouyang2022TrainingLM, stiennon2020learning}, has effectively unleashed the potential of LLMs acquired during pretraining to achieve desired behaviors. 

Although off-the-shelf alignment methods have made significant strides in short-context settings, their application to long-context situations remains challenging~\citep{bai2024longalign}. First, the scarcity of high-quality, long-context annotated data poses a significant hurdle. Human annotation becomes impractical and less-reliable as context length increases~\citep{dubey2024llama3herdmodels}, while synthetic data generation using advanced LLMs lacks scalability and remains resource-intensive. Moreover, simply concatenating existing short-context datasets has been shown to yield unsatisfactory long-context performance~\citep{liu2023world}.
Second, long-context alignment methods grapple with the balance between preserving short-context proficiency and cultivating long-context capabilities~\citep{liu2023world}.
For instance, the LLaMA-3.1 series incorporate merely 0.1\% long-context data with over 99\% short-context data during alignment to maintain the short-context performance~\citep{liu2023world}. This limited exposure to natural long-context data may result in insufficient alignment, potentially blocking the intrinsic long-context capabilities in LLMs.

The challenges of long-context alignment suggest that the full potential of LLMs may remain untapped for long-context tasks. As illustrated in~\Cref{fig:comparion_overview}, even superior models such as GPT-4, which excel in short-context tasks, unexpectedly underperform in long-context scenarios.
Interestingly, despite the much stronger short-context capabilities, GPT-4 is still inferior to LLaMA3.1-8B on long-context tasks. This disparity underscores the need for more effective long-context alignment methods to fully unleash the intrinsic power of LLMs across variable context lengths.

In this work, we posit that the capabilities, deeply ingrained during short-context pretraining and alignment, can be effectively transferred to longer contexts without external guidance. To this end, we introduce Short-to-\textbf{Long} \textbf{P}reference \textbf{O}ptimization (\textbf{\ourMethod}), to steer long-context alignment by injecting internal short-context preferences into long-context scenarios. Specifically, we propose to construct the preference data pairs by prompting the short-context LLM (e.g., Mistral-Instruct) with two inputs: (1) a long input comprising an instruction over a long document and,  (2) a short input with the identical instruction over the relevant shortened chunk within the same document. We then designate the responses to short and long inputs as chosen and rejected responses, respectively. The short-to-long preference, i.e., the discrepancies between each paired response, reveal the capabilities and potentials cultivated during short-context alignment that may be diminished in under-aligned long-context scenarios. 
In order to bring forward the established capabilities, \ourMethod{} is utilized to optimize the model towards short-to-long preferences using DPO-style objectives upon long contexts. Furthermore, to maintain the short-context performance, we incorporate a \textit{short-to-long constraint} in \ourMethod{} by applying Kullback-Leibler (KL) divergence between the response distributions to short and long inputs, respectively. This constraint, inspired by the KL constraint in RLHF~\citep{Ouyang2022TrainingLM,stiennon2020learning}, guides the policy model to minimize the deviation from its short-context output distribution when giving the long context during training. We found that this straightforward constraint largely enhances the retention of short-context performance after the long-context alignment.
% By incorporating this straightforward constraint, our \ourMethod{} largely enhances the retention of short-context performance post-alignment with long contexts.
\begin{figure}[!t]
    \centering
    % \includegraphics[width=0.5\linewidth]{}
    % \caption{A comparison between SFT, DPO, and LongPO.}
    \includegraphics[width=0.86\linewidth]{figures/overview_figure1.pdf}
    \vspace{-0.3em}
    \caption{The comparison of long-context ($\infty$Bench) and short-context (MMLU) performance among GPT-4-128K and smaller LLMs.}
    \label{fig:comparion_overview}
\end{figure}

% recognize that the decline in short-context performance during long-context alignment may implicitly stem from an improper Kullback-Leibler (KL) constraint. To mitigate this issue, \ourMethod{} incorporates a short-to-long KL constraint, guiding the policy model to minimize deviation from its short-context output distribution when giving the long context during training. We found that this straightforward constraint largely enhances the retention of short-context performance post-alignment with long contexts.
% preserve the short-context performance after long-context alignment.

We apply LongPO to Mistral-7B-Instruct-v0.2~\citep{Jiang2023Mistral7} and Qwen2.5-7B-Instruct, while iteratively extending their context lengths up to 512K, with the self-generated short-to-long preference data only.
% using self-generated short-to-long preference data from 128K to 256K, without additionally continual unsupervised training on long documents. 
The experimental results demonstrate that \ourMethod{}, as a long-context alignment method,
surpasses naive SFT and DPO by large margins (over 10 points) in both long- and short-context tasks. Notably, \ourMethod{} fully retains the performance of short-context LLMs after long-context alignment, whereas SFT and DPO  yield substantial performance degradation (10$\sim$20 points on most tasks). In terms of long-context performance, \ourMethod{} largely improves the Mistral-7B-Instruct-v0.2 by 25.45 points on $\infty$Bench. Specifically, as depicted in~\Cref{fig:comparion_overview}, the resulting model is comparable with superior long-context LLMs at various scales (e.g., Mistral-7B-LongPO-128K of 39.27 vs. GPT-4-128K of 34.81 on $\infty$Bench), despite the latter often involving extensive continual training on hundreds of billions of tokens~\citep{dubey2024llama3herdmodels} or labor-intensive long-context data annotation~\citep{Zeng2024ChatGLMAF}. These findings underscore the efficacy of our proposed method in addressing the challenges of long-context alignment while simultaneously preserving short-context capabilities, offering a more efficient and balanced approach to the development of long-context LLMs.






% Long-context alignment presents two primary challenges.
% First, the data annotation for long contexts is a formidable and resource-intensive task. Human annotation becomes increasingly impractical as context length grows. While leveraging advanced LLMs like GPT-4 to generate synthetic long-context data is theoretically possible, this approach is prohibitively expensive and lacks scalability. Furthermore, the seemingly straightforward solution of concatenating existing short SFT datasets has been shown to yield unsatisfactory long-context performance~\citep{liu2023world}. Second, maintaining a balance between short- and long-context performance during alignment is non-trivial. As illustrated in~\Cref{fig:comparion_overview}, while long-context SFT and DPO on synthetic data improve long-context performance, they often come at the cost of diminished short-context and instruction-following capabilities.