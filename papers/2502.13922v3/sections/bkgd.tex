\section{Preliminaries}
\label{sec:preliminary}
In this section, we introduce two key methods for aligning language models with human preferences: Reinforcement Learning from Human Feedback (RLHF, \Cref{subsec:rlhf}) and Direct Preference Optimization (DPO, \Cref{subsec:dpo}).

\subsection{RLHF}
\label{subsec:rlhf}
Reinforcement Learning from Human Feedback (RLHF)~\citep{Ouyang2022TrainingLM, stiennon2020learning} aims to optimize the policy model $\pitheta$ to maximize rewards while maintaining proximity to a reference policy $\piref$. Formally, the objective is
\begin{equation}\label{eq:RL}
\max_{\pi_{\theta}}  \mathbb{E}_{x\sim \mathcal{D}, y\sim \pi_{\theta}(y \mid x)}\bigl[r_{\phi}(x, y)\bigr] - \beta\mathbb{D}_{\textrm{KL}}\bigl[\pi_{\theta}(y\mid x)\mid \mid \piref(y\mid x)\bigr],
\end{equation}
where $r_{\phi}$ is the reward model that has been trained on ranked responses to reflect human preference, $\beta$ is a hyper-parameter controlling the deviation from reference policy, and $\mathbb{D}_{\textrm{KL}}$ denotes the Kullback-Leibler divergence. Typically, both $\pitheta$ and $\piref$ are initialized with identical model.



\subsection{DPO}
\label{subsec:dpo}
Considering the instability and difficulty of RLHF training, DPO~\citep{Rafailov2023DirectPO} offers an alternative approach by reparameterizing the reward function $r$ that incorporates the optimal policy:  
\begin{equation}
\label{eq:dpo_reward}
r(x,y) = \beta \log \frac{\pi_\theta(y \mid x)}{\pi_{\text{ref}}(y \mid x)} + \beta \log Z(x),
\end{equation}
where $Z(x)$ is the partition function. DPO assumes access to preference data $\mathcal{D}$, which consists of paired responses $\left(y_w, y_l\right)$ to an instruction $x$. Specifically, the $y_w$ and $y_l$ represent the preferred (winning) and dispreferred (losing) responses, respectively, based on human preference. Inspired by the Bradley-Terry (BT) theory that models the preference distribution $p^*$ by
\begin{equation}
\label{eq:bt_model}
    p^*(y_w\succ y_l \mid x)= \sigma(r(x, y_w)- r(x, y_l)),
\end{equation}
where $\sigma$ is the sigmoid function.
DPO derives the preference optimization objective for the policy model $\pitheta$ as
\begin{equation}
    \label{eq:dpo}
    \begin{aligned}
        \mathcal{L}_{\text{DPO}}(\pi_\theta; \pi_{\text{ref}}) &= - \mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}}\left[\sigma(r_\theta(x, y_w)- r_\theta(x, y_l))\right]\\
        &= - \mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}}\left[ \log \sigma \left( \beta \log \frac{\pi_\theta(y_w \mid x)}{\pi_{\text{ref}}(y_w \mid x)} - \beta \log \frac{\pi_\theta(y_l \mid x)}{\pi_{\text{ref}}(y_l \mid x)}\right) \right].
    \end{aligned}
\end{equation}

% By incorporating this reward formulation into the Bradley-Terry (BT) ranking objective, $p(y_w \succ y_l \mid x) = \sigma \left( r(x, y_w) - r(x, y_l) \right)$, DPO expresses the probability of preference data with the policy model rather than the reward model, yielding the following objective:
% \begin{equation}
%     \label{eq:dpo}
%     \mathcal{L}_{\text{DPO}}(\pi_\theta; \pi_{\text{ref}}) = - \mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}}\left[ \log \sigma \left( \beta \log \frac{\pi_\theta(y_w \mid x)}{\pi_{\text{ref}}(y_w \mid x)} - \beta \log \frac{\pi_\theta(y_l \mid x)}{\pi_{\text{ref}}(y_l \mid x)}\right) \right],
% \end{equation}
% where $(x, y_w, y_l)$ are preference pairs consisting of the prompt, the winning response, and the losing response from the preference dataset $\mathcal{D}$.

