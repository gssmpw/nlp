\section{Related Work}
\label{2}
\subsection{Reinforcement and Imitation Learning in Vehicle Control}
In recent years, researchers have enhanced vehicle control performance through diverse approaches and methodologies \cite{kiran2021deep,li2024iss}. This includes mimicking human driver behavior \cite{sun2023benchmark,gao2019comparison} and mastering intricate, precise policies via reinforcement learning \cite{patel2024enhancing,chen2023risk}.

In \cite{gangopadhyay2022safe}, a method called S2RL is proposed. This method uses Control Barrier Functions (CBFs) and Control Lyapunov Functions (CLFs) to guide the training of the vehicle, ensuring that the learned policy maintains optimal behavior while meeting safety and stability requirements. In \cite{manikandan2023ad}, the authors built the DDQN framework by introducing a dual network architecture, proposing the Dueling Double Deep Q-Network (D3DQN). The D3DQN has shown good performance in both simulation and real-time testing, effectively navigating vehicles around temporary obstacles and pedestrians. However, these methods typically require explicit reward signals to guide the vehicle's learning to optimize its driving performance. In contrast, our proposed RFRLF method can guide the vehicle to effectively learn driving policies without providing manually designed reward signals. In \cite{gong2024beyond}, a Life-long Policy Learning (LLPL) framework is proposed. This framework combines Imitation Learning (IL) with Life-long Learning (LLL), providing vehicles with a path tracking control policy that can continuously learn and adapt to new environments, significantly improving the learning efficiency and adaptability of the policy. In \cite{zhou2021exploring}, a method based on imitation learning is proposed, which effectively improve the planning and control performance of the vehicle by combining data augmentation, task loss, attention mechanisms, and post-processing planners, achieving robust driving by imitating human driver behavior. However, these methods all require explicit action information in expert data to guide the vehicle's learning of driving policies and optimize performance. In contrast, our proposed RFRLF method learn effective driving policies without providing explicit action information.

\subsection{Reinforcement Learning with No Rewards}
In the field of reinforcement learning, when the reward signal is completely missing, the learning process of the agent faces great challenges. To address this challenge, the authors of \cite{ma2024explorllm} propose a method called ExploRLLM, which uses large language models (e.g., GPT-4) to generate common sense reasoning and combines it with the empirical learning ability of reinforcement learning. Specifically, the base model is used to obtain basic strategies, efficient representations, and exploration strategies. By guiding the agent's exploration process through natural language instructions, ExploRLLM can reduce the dependence on explicit reward signals to a certain extent, thereby improving learning efficiency and strategy quality. In \cite{pathak2017curiosity}, the authors propose a curiosity-based exploration mechanism that uses self-supervised learning to predict the consequences of the agent's own actions. Specifically, the agent uses a self-supervised inverse dynamics model to learn the visual feature space, and then uses this feature space to predict state changes caused by the current state and executed actions. In a non-reward environment, curiosity can serve as an intrinsic reward signal, motivating the agent to explore the environment and learn skills that may be useful in the future. In \cite{dawood2023handling}, the authors propose a method that combines Model Predictive Control (MPC) with reinforcement learning to handle non-reward issues. In this method, MPC serves as a source of experience, providing demonstrations to the agent during RL training, thus helping the agent learn how to navigate in non-reward environments. This method successfully improve the agent's learning efficiency and success rate in non-reward environments and reduce the number of collisions and timeouts. In summary, the above methods require manually design of rewards or intrinsic rewards. In contrast, our proposed RFRLF method avoids the manual design of rewards and instead cultivates the decision-making ability of the agent through other mechanisms, thus still demonstrating excellent policy when the design of a reward function is challenging.

\subsection{Control method based on supervised learning of neural network}
It is important to note that although the proposed RFRLF framework incorporates state prediction errors as supervisory signals during training, it fundamentally adheres to the reinforcement learning paradigm. Traditional supervised learning methods primarily rely on static input-output mapping relationships, with their optimization objectives limited to minimizing immediate prediction errors. In contrast, the RFRLF framework generates trajectories through dynamic interaction with the environment and optimizes policies based on the long-term reachability of target states. This interaction-based learning mode preserves the advantages of reinforcement learning in exploration and long-term optimization while avoiding dependence on artificially designed reward functions, which fundamentally distinguishes it from traditional supervised learning methods.

At the implementation level, neural network-based supervised control methods \cite{fei2017adaptive,zhang2020near} typically employ a dual neural network structure. For instance, the dual adaptive neural network control method proposed in \cite{fei2017adaptive}, designed for dynamic control of nonholonomic constrained mobile robots, requires the error between the reference trajectory and current state as input for one of its controllers, and the entire closed-loop control process depends on target state information. In comparison, our proposed RFRLF method uses state errors only as a loss function during the training phase, with the policy network taking only the agent's current state information as input, enabling operation without reliance on expert states during practical application. Furthermore, some traditional control methods require explicit target action information to construct loss functions. For example, \cite{zhang2020near} needs to use MPC to obtain action annotations. For the scenario of learning actions from videos that this paper focuses on, action annotations are difficult to obtain. The RFRLF method, however, can learn effective control policies without any action annotation information, significantly expanding its range of applications.