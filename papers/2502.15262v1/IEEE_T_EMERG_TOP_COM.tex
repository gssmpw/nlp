\documentclass[journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{array}
\pdfoutput=1
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage[colorlinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue]{hyperref}
\usepackage{booktabs}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{balance}
\begin{document}
\title{Towards a Reward-Free Reinforcement Learning Framework for Vehicle Control}


\author{Jielong Yang, Daoyuan Huang}

%\markboth{Journal of \LaTeX\ Class Files,~Vol.~18, No.~9, September~2020}%
%{How to Use the IEEEtran \LaTeX \ Templates}

\maketitle
\begin{abstract}
Reinforcement learning plays a crucial role in vehicle control by guiding agents to learn optimal control strategies through designing or learning appropriate reward signals. However, in vehicle control applications, rewards typically need to be manually designed while considering multiple implicit factors, which easily introduces human biases. Although imitation learning methods does not rely on explicit reward signals,  they necessitate high-quality expert actions, which are often challenging to acquire. To address these issues, we propose a reward-free reinforcement learning framework (RFRLF). This framework directly learns the target states to optimize agent behavior through a target state prediction network (TSPN) and a reward-free state-guided policy network (RFSGPN), avoiding the dependence on manually designed reward signals. Specifically, the policy network is learned via minimizing the differences between the predicted state and the expert state. Experimental results demonstrate the effectiveness of the proposed RFRLF in controlling vehicle driving, showing its advantages in improving learning efficiency and adapting to reward-free environments.
\end{abstract}

\begin{IEEEkeywords}
reward-free, state prediction, reinforcement learning, vehicle control.
\end{IEEEkeywords}


\section{Introduction}\label{1}
\IEEEPARstart{I}{n} the field of vehicle control, although traditional control methods such as Model Predictive Control perform well in certain scenarios, they often struggle to achieve ideal control effects when facing dynamically changing environments \cite{10225497,gheisarnejad2022adaptive,10186728}. In contrast, Reinforcement Learning (RL), as a learning-based decision-making method, can learn the optimal policy through the interaction between the agent and the environment \cite{Mnih2013PlayingAW,10020015,zhang2024reinforcement}. The advantage of RL lies in its ability to adapt to the uncertainty and dynamic changes of the environment, learning effective behavioral policies in specific environments through a continuous trial-and-error process \cite{mnih2015human}. Moreover, RL algorithms can automatically adjust and optimize control parameters through the interaction between the agent and the environment \cite{li2024autopilot}. Therefore, researchers are actively exploring various reinforcement learning algorithms and their applications in vehicle control \cite{10547481,shi2023optimal}.

In recent years, reinforcement learning has achieved many remarkable accomplishments in the field of vehicle driving control \cite{chen2023milestones,ye2021survey,grigorescu2020survey}. However, traditional reinforcement learning algorithms, such as the MAD-ARL method \cite{sharif2022adversarial} and the PETS-MPPI method \cite{frauenknecht2023data}, typically require explicit reward signals to guide the vehicle in learning driving policies. Yet, the design of driving rewards for vehicles must consider many implicit factors, making it difficult to define an appropriate reward function for RL \cite{wang2024deep,abouelazm2024review}. Furthermore, in some real-world scenarios, reward signals may be very sparse or inaccessible \cite{yang2020adaptive,booher2024cimrl}.

In existing work, Kamienny et al. \cite{kamienny2020learning} propose the Import algorithm, which regularizes the training of recurrent neural network strategies by using informative policies. Import effectively reduces the reliance on explicit rewards, but it still needs to use artificially selected task description information to learn the reward function and then guide the learning of the agent. Zintgraf et al. \cite{zintgraf2019varibad} propose the Varibad algorithm, a Bayesian adaptive deep reinforcement learning method based on meta-learning. It uses a variational auto-encoder (VAE) to learn a low-dimensional random latent representation of the task, enabling the agent to effectively explore and exploit in an unknown environment, but it not be able to learn effective strategies for environments where rewards are completely missing. Hejna et al. \cite{hejna2024inverse} propose an Inverse Preference Learning (IPL) algorithm aimed at addressing some issues in traditional preference RL methods by eliminating the need for explicit reward functions. IPL enhances learning stability by simultaneously learning an implicit reward that aligns with expert preferences and the optimal policy. Although IPL does not require an explicit reward function, it still necessitates the human design of an implicit reward for optimization, and the specific form of the implicit reward limits its flexible application. In the case of sparse rewards, Liu et al. \cite{liu2021decoupling} propose the Dream algorithm, which learns to automatically identify task-relevant information by leveraging goals and restores this information by learning an exploration objective. It performs well in handling sparse reward issues but requires additional policies to ensure effective exploration when rewards are completely missing. 

Additionally, when rewards are completely unknown and reward functions are challenging to design, imitation learning emerges as a viable alternative. Imitation learning observes and learns from the behavior of experts, directly extracting behavioral patterns from demonstration data, thus bypassing the reliance on explicit reward signals. Xiao et al. \cite{xiao2023imitation} propose an imitation learning method that enables agents to acquire high-quality behavioral policies by imitating expert behaviors and learning from experience distributions rather than reward functions. Generative Adversarial Imitation Learning (GAIL) \cite{ho2016generative} draws inspiration from Generative Adversarial Network (GAN) \cite{goodfellow2014generative}, training a discriminator to distinguish between expert trajectories and policy-generated trajectories while optimizing the policy to deceive the discriminator. However, imitation learning typically relies on high-quality expert demonstration data, which includes not only state transitions but also the corresponding action information. In some practical applications, obtaining such action information can be challenging because it may involve privacy protection issues or confidentiality of expertise. Generative Adversarial Imitation from Observation (GAIfO) \cite{torabi2018generative}, as a representative method of Imitation from Observation (IfO) \cite{torabi2019recent}, builds upon GAIL and requires only expert state trajectories as input, without the need for expert actions. However, its training process requires fine-tuning of the balance between the two components, making it prone to training instability or even collapse. We propose a novel framework that incorporates a target state prediction network, allowing the policy network to be optimized gradually without the need to simultaneously handle the dynamic balance in adversarial training, thereby improving learning efficiency and stability.

Inspired by this, we propose a reward-free vehicle control policy learning method, which updates policies using predicted states and expert states, avoiding reliance on manually designed reward functions. Through this approach, the agent focuses on transitioning from the current state to the target state, rather than maximizing manually designed rewards through trial and error, thereby improving learning efficiency and reducing the influence of improper reward function design. However, this reward-free learning also presents new challenges. The agent needs to employ alternative methods to evaluate and optimize its behavior while maintaining the direction of learning in the absence of immediate reward feedback.

Therefore, driven by the insights mentioned above, we propose the RFRLF framework. The RFRLF is mainly divided into two parts: the Target State Prediction Network (TSPN) and the Reward-Free State-Guided Policy Network (RFSGPN). The TSPN aims to predict the state of the environment at next time step based on the agent's current state and the actions it performs, thereby providing target state predictions for the optimization of subsequent policy networks. The RFSGPN aims to make optimized action decisions without relying on manually designed reward signals. Without the TSPN, the interaction between the agent and the environment  cannot perform backpropagation when training the RFSGPN. The TSPN, combined with the state information provided by expert data, provides a loss for policy optimization for the RFSGPN. By training the policy network using a loss function based on state representation, there is no need to depend on manually designed reward signals. Moreover, RFRLF is particularly suitable for situations where expert actions are unknown. By learning target states rather than specific actions, RFRLF enables agents to optimize their behavioral policies even in the absence of expert action guidance, through expert state information. Experimental results demonstrate the effectiveness of the proposed framework in controlling vehicles.

Compared to existing research, the main contributions of this work are summarized as follows: 
\begin{enumerate}{}{}
	\item{We propose a novel reward-free reinforcement learning framework (RFRLF). RFRLF can optimize agent behavior by solely observing records (e.g., videos) of expert demonstrations without the need for known rewards or the design of implicit rewards.}
	\item{We innovatively combine a TSPN with a RFSGPN, enabling efficient policy optimization without the need to design a reward function, thus mitigating performance issues that may arise from improper reward design.}
	\item{This method is particularly suitable for situations where expert actions are unknown, allowing agents to learn effective policies through imitation learning even in the absence of action guidance.}
\end{enumerate}

The rest of this paper is organized as follows. Section \ref{2} will review the research progress in related fields. Section \ref{3} provides a detailed introduction to our proposed reward-free reinforcement learning framework RFRLF, including the design and implementation of the TSPN and the RFSGPN. Section \ref{4} will present our experimental setup, comparison results with baseline methods, and sensitivity analysis of key parameters. Section \ref{5} will summarize the main findings of this study and provide perspectives on future research directions.

\section{Related Work}\label{2}
\subsection{Reinforcement and Imitation Learning in Vehicle Control}
In recent years, researchers have enhanced vehicle control performance through diverse approaches and methodologies \cite{kiran2021deep,li2024iss}. This includes mimicking human driver behavior \cite{sun2023benchmark,gao2019comparison} and mastering intricate, precise policies via reinforcement learning \cite{patel2024enhancing,chen2023risk}.

In \cite{gangopadhyay2022safe}, a method called S2RL is proposed. This method uses Control Barrier Functions (CBFs) and Control Lyapunov Functions (CLFs) to guide the training of the vehicle, ensuring that the learned policy maintains optimal behavior while meeting safety and stability requirements. In \cite{manikandan2023ad}, the authors built the DDQN framework by introducing a dual network architecture, proposing the Dueling Double Deep Q-Network (D3DQN). The D3DQN has shown good performance in both simulation and real-time testing, effectively navigating vehicles around temporary obstacles and pedestrians. However, these methods typically require explicit reward signals to guide the vehicle's learning to optimize its driving performance. In contrast, our proposed RFRLF method can guide the vehicle to effectively learn driving policies without providing manually designed reward signals. In \cite{gong2024beyond}, a Life-long Policy Learning (LLPL) framework is proposed. This framework combines Imitation Learning (IL) with Life-long Learning (LLL), providing vehicles with a path tracking control policy that can continuously learn and adapt to new environments, significantly improving the learning efficiency and adaptability of the policy. In \cite{zhou2021exploring}, a method based on imitation learning is proposed, which effectively improve the planning and control performance of the vehicle by combining data augmentation, task loss, attention mechanisms, and post-processing planners, achieving robust driving by imitating human driver behavior. However, these methods all require explicit action information in expert data to guide the vehicle's learning of driving policies and optimize performance. In contrast, our proposed RFRLF method learn effective driving policies without providing explicit action information.

\subsection{Reinforcement Learning with No Rewards}
In the field of reinforcement learning, when the reward signal is completely missing, the learning process of the agent faces great challenges. To address this challenge, the authors of \cite{ma2024explorllm} propose a method called ExploRLLM, which uses large language models (e.g., GPT-4) to generate common sense reasoning and combines it with the empirical learning ability of reinforcement learning. Specifically, the base model is used to obtain basic strategies, efficient representations, and exploration strategies. By guiding the agent's exploration process through natural language instructions, ExploRLLM can reduce the dependence on explicit reward signals to a certain extent, thereby improving learning efficiency and strategy quality. In \cite{pathak2017curiosity}, the authors propose a curiosity-based exploration mechanism that uses self-supervised learning to predict the consequences of the agent's own actions. Specifically, the agent uses a self-supervised inverse dynamics model to learn the visual feature space, and then uses this feature space to predict state changes caused by the current state and executed actions. In a non-reward environment, curiosity can serve as an intrinsic reward signal, motivating the agent to explore the environment and learn skills that may be useful in the future. In \cite{dawood2023handling}, the authors propose a method that combines Model Predictive Control (MPC) with reinforcement learning to handle non-reward issues. In this method, MPC serves as a source of experience, providing demonstrations to the agent during RL training, thus helping the agent learn how to navigate in non-reward environments. This method successfully improve the agent's learning efficiency and success rate in non-reward environments and reduce the number of collisions and timeouts. In summary, the above methods require manually design of rewards or intrinsic rewards. In contrast, our proposed RFRLF method avoids the manual design of rewards and instead cultivates the decision-making ability of the agent through other mechanisms, thus still demonstrating excellent policy when the design of a reward function is challenging.

\subsection{Control method based on supervised learning of neural network}
It is important to note that although the proposed RFRLF framework incorporates state prediction errors as supervisory signals during training, it fundamentally adheres to the reinforcement learning paradigm. Traditional supervised learning methods primarily rely on static input-output mapping relationships, with their optimization objectives limited to minimizing immediate prediction errors. In contrast, the RFRLF framework generates trajectories through dynamic interaction with the environment and optimizes policies based on the long-term reachability of target states. This interaction-based learning mode preserves the advantages of reinforcement learning in exploration and long-term optimization while avoiding dependence on artificially designed reward functions, which fundamentally distinguishes it from traditional supervised learning methods.

At the implementation level, neural network-based supervised control methods \cite{fei2017adaptive,zhang2020near} typically employ a dual neural network structure. For instance, the dual adaptive neural network control method proposed in \cite{fei2017adaptive}, designed for dynamic control of nonholonomic constrained mobile robots, requires the error between the reference trajectory and current state as input for one of its controllers, and the entire closed-loop control process depends on target state information. In comparison, our proposed RFRLF method uses state errors only as a loss function during the training phase, with the policy network taking only the agent's current state information as input, enabling operation without reliance on expert states during practical application. Furthermore, some traditional control methods require explicit target action information to construct loss functions. For example, \cite{zhang2020near} needs to use MPC to obtain action annotations. For the scenario of learning actions from videos that this paper focuses on, action annotations are difficult to obtain. The RFRLF method, however, can learn effective control policies without any action annotation information, significantly expanding its range of applications.

\section{Methodology}\label{3}

\begin{figure*}[!t]
	\centering
	\includegraphics[width=0.9\textwidth]{Overall_framework_diagram.png}
	\caption{The architecture of our reward-free reinforcement learning framework, which consists of two parts. In the first part, we collect state-action pairs and train a target state prediction network. In the second part, we train the reward-free state-guided policy network using the predicted states provided by the target state prediction network along with the state data provided by the expert, while freezing the target state prediction network.}
	\label{fig:Overall_framework}
\end{figure*}

This chapter provides a detailed introduction to the reward-free reinforcement learning framework (RFRLF) we propose, as shown in Fig. \ref{fig:Overall_framework}. The motivation of this framework stems from addressing the challenge of dependency on reward signals in traditional reinforcement learning. In practical applications, crafting effective reward functions is often difficult and time-consuming, especially in environments where rewards are sparse or ambiguous. To tackle this, we divide the framework into two complementary modules: the Target State Prediction Network (TSPN) and the Reward-Free State-Guided Policy Network (RFSGPN). The TSPN predicts the next state based on the agent's current state and actions, providing a clear target information for the policy network. The RFSGPN utilizes the target state predictions and expert state information to optimize the agent's policy, enabling effective learning in environments without explicit rewards. These two modules work in tandem; the TSPN supplies the necessary target information, and the policy network uses this information to refine actions, thereby reducing reliance on manually designed reward signals and enhancing adaptability and learning efficiency in complex environments. Distinguished from neural network-based supervised control methods \cite{fei2017adaptive,zhang2020near}, the proposed method offers the following advantages: 
\begin{enumerate}{}{}
	\item{It uses expert states to compute state errors as loss functions only during the training phase, requiring only the agent's current state information during practical application, without dependence on expert states.}
	\item{It can learn effective control policies without action annotation information, greatly expanding its application scope, particularly suitable for scenarios such as learning actions from videos.}
\end{enumerate}
In the following sections of this chapter, we will delve into the specific implementations of these two modules and how they collectively address the challenges (i.e.,dependence on manually designed rewards or expert actions) in reinforcement learning.


\subsection{Target State Prediction Network (TSPN)}
The design of the TSPN stems from an in-depth understanding of environmental dynamics, aiming to predict the state at the next time step by modeling the interactions between the agent and the environment, thereby providing predictions of target states for subsequent policy optimization. This approach circumvents the reliance on manually designed reward signals in traditional reinforcement learning, enabling the policy network to directly learn how to achieve target states. This network uses random interaction data $(s_t,a_t)$ from the agent and the environment to predict the target state $s_{t+1}^{pre}$, and the predicted target state is used alongside the next state $s_{t+1}$ from the interaction data to optimize the prediction network. The optimized prediction network is then used to train the RFSGPN, thereby assisting the agent in learning the policy. This part of the content will be introduced in the following section \hyperref[sec:reward-free state-guided policy network]{III-B}.

To provide the TSPN with continuous and diverse state-action pair observation data, we design a data collector and a data processor. As shown in Fig. \ref{fig:Overall_framework}, the data collector uses an untrained policy network to interact randomly with the environment to obtain observational data. During this process, a series of state-action-next state triplet data generated are saved to the data buffer. Subsequently, the data processor processes the data in the data buffer to ensure that the extracted observation sequences are continuous and do not contain end flags. This process ensures the quality of the training data for the TSPN, enabling it to learn on a continuous and diverse state-action sequence.

%\begin{figure*}[!t]
%	\centering
%	\includegraphics[width=0.9\textwidth]{Figure/Diagram of the target state prediction network.png}
%	\caption{The structure of the TSPN. In these networks, the input state is first processed by the input layer and then passed through the downsampling layer to extract key state features. Further, the action information is fused into these state features through the action injection layer. Finally, through the upsampling operation and the output layer, the network generates a prediction of the target state.}
%	\label{fig:Target_prediction_network}
%\end{figure*}

Based on the high-quality training data, the TSPN takes the current state and action information as inputs and outputs the prediction of the state at the next time step. As shown in Fig. \ref{fig:Target_prediction_network}, the TSPN is composed of multiple modules, including the input embedding layer, downsampling layer, upsampling layer, action injection layer, and the output layer. These modules not only effectively capture the features of the current state but also consider the agent's decisions and actions through the action injection layer, enabling the network to better capture the correlation between the previous and subsequent states as well as the actions. In the following sections, we will provide a detailed introduction to these modules.

The input embedding layer uses a convolutional neural network to embed the input state frames. We have selected appropriate kernel sizes and strides to ensure that key information in the state frames can be effectively extracted, thereby improving the accuracy of predictions \cite{zhou2020one}. We design two downsampling layers that reduce the dimension of features obtained after embedding through multiple convolutions and instance normalization operations, reducing computational complexity and memory consumption and allowing the network to consider contextual information over a larger range \cite{yu2020research,li2023efficient}. Specifically, after the input state $s_t$ is embedded and downsampled, the resulting feature representation is:
\begin{equation}
	\begin{gathered}
		h_0(t)=\text{ReLU}(W_{in}s_t+b_m),\\
		h_1(t)=\text{InstanceNorm}(\text{ReLU}(W_{c1}h_0(t)+b_{c1})),\\
		h_2(t)=\text{InstanceNorm}(\text{ReLU}(W_{c2}h_1(t)+b_{c2})),
	\end{gathered}
\end{equation}

\begin{figure*}[!t]
	\centering
	\includegraphics[width=0.9\textwidth]{Diagram_of_the_target_state_prediction_network.png}
	\caption{The structure of the TSPN. In these networks, the input state is first processed by the input layer and then passed through the downsampling layer to extract key state features. Further, the action information is fused into these state features through the action injection layer. Finally, through the upsampling operation and the output layer, the network generates a prediction of the target state.}
	\label{fig:Target_prediction_network}
\end{figure*}

where $W_{in}$, $b_{in}$, $W_{c1}$, $b_{c1}$, $W_{c2}$ and $b_{c2}$ are the weight matrices and bias vectors of the network, respectively. On this basis, the action injection layer injects action information into the feature representation, enabling the network to make state predictions based on action information. In reinforcement learning tasks, the next state depends not only on the current state but also on the actions taken \cite{song2022action}. Therefore, injecting action information during the downsampling and upsampling processes of the network can help the network better understand the impact of actions on state changes, thereby improving the accuracy of predictions. The action injection layer converts the action vector into a mask through a fully connected layer and performs element-wise multiplication and addition operations with the state features. The specific process is as follows:
\begin{equation}
	\begin{aligned}
		m_1(t) &= \sigma(W_{a1}a_t),\\
		h_2^{'}(t) &= h_2(t) \odot m_1(t),\\
		m_2(t) &= W_{a2}a_t,\\
		h_2^{''}(t) &= h_2^{'}(t) + m_2(t),
	\end{aligned}
\end{equation}
where $a_t$ is the current action, $W_{a1}$, $W_{a2}$ are the weight matrices of the action injection layer, $m_1(t)$, $m_2(t)$ are the generated masks, $\sigma$ is the \text{sigmoid} function, and $\odot$ represents element-wise multiplication. Subsequently, the upsampling layer upsamples the feature representation through multiple deconvolutions and instance normalization operations, generating a prediction result of the same size as the original input. We designed two upsampling layers to map the features after action injection layer, $h_2^{''}(t)$, back to a higher-dimensional representation $h_1^{'}(t)$ and the final embedded representation $h_0^{'}(t)$ in sequence:
\begin{equation}
	\begin{gathered}
		h_1^{'}(t)=\text{InstanceNorm}(\text{ReLU}(W_{u1}h_2^{''}(t)+b_{u1})),\\
		h_0^{'}(t)=\text{InstanceNorm}(\text{ReLU}(W_{u2}h_1^{'}(t)+b_{u2})).
	\end{gathered}
\end{equation}
Finally, the output layer uses a convolutional neural network to map the upsampled feature representation to the predicted result of the next state. This design allows the network to directly output pixel-wise prediction results, which is essential for image prediction tasks \cite{moosavi2021driving}. The predicted state expression after the output layer is:
\begin{equation}
	s_{t+1}^{pre}=W_{out}h_0^{'}(t)+b_{out}.
\end{equation}

During the training process, our goal is to make the network's prediction of the next state $s_{t+1}^{pre}$ as close as possible to the actual next state $s_{t+1}$. We choose to use the mean squared loss function to train the network. Specifically, the definition of the loss function is as follows:
\begin{equation}
	\mathcal{L} =\frac{1}{N}\sum_{i=1}^{N}(s_{t+1}^{pre}-s_{t+1})^2,
\end{equation}
%\begin{equation}
%	Loss=-\frac{1}{N}\sum_{i=1}^{N}\sum_{c=1}^{C}s_{i,c}\log(s_{i,c}^{pre}).
%\end{equation}
where $N$ is the batch size.

It is worth noting that when the input to the TSPN consists of numerical data, convolutional layers are not required for prediction. Therefore, in this case, we replace the original convolutional and deconvolutional layers with linear layers. Additionally, the instance normalization function is replaced by a layer normalization function, which is more appropriate for linear layers.

In summary, the TSPN processes the interaction data between the agent and the environment to generate predictions of future states. These predictions serve not only to refine the network itself but also provide crucial target state guidance for the policy network. In the following section \hyperref[sec:reward-free state-guided policy network]{III-B}, we will explore how to use these predicted states in combination with expert state data to learn the RFSGPN.

\subsection{Reward-Free State-Guided Policy Network (RFSGPN)}\label{sec:reward-free state-guided policy network}
In practice, explicit reward signals are often difficult to obtain. Even when reward signals can be acquired, their effectiveness may be compromised by noise, delay, or design complexity, which in turn affects the learning of the policy \cite{yang2020adaptive}, \cite{chen2021self}. To address this issue, we design a RFSGPN. The motivation of this network is to use the predicted state as a substitute supervision signal, directly providing the agent with learning objectives. By leveraging predicted target states and state information provided by experts, the network continuously guide the learning process without the need for manually designed rewards and in the absence of immediate reward feedback, achieving effective policy optimization.

Next, we introduce the design and implementation of the RFSGPN. The RFSGPN consists of multiple convolutional layers, batch normalization layers, and fully connected layers. The main role of the convolutional layers is to extract useful features from the input states, which is crucial for understanding the environment and making decisions. To more effectively process features, we design a feature extraction network and obtain the extracted features:
\begin{equation}
	f_t=Q(s_t),
	\label{eq:Q}
\end{equation}
where $s_t$ is the current input state, initialized as the provided initial state, and subsequent states are obtained as the next state after interaction with the environment. $Q(\cdot)$ is a feature extraction network composed of three convolutional layers. After each convolutional layer, there is a batch normalization layer to improve the learning ability and stability of the network \cite{zhang2022traffic}. When the input of RFSGPN is numerical data, it is not suitable to use convolutional layers to extract features. we change the convolution layers in \eqref{eq:Q} to linear layers. Subsequently, the fully connected layer transforms the features extracted by the convolutional layers into a mapping of the action space:
\begin{equation}
	\begin{gathered}
		h_t=\text{ReLU}(W_1f_3+b_1),\\
		q_t=\text{ReLU}(W_2h_t+b_2),
	\end{gathered}
\end{equation}
%\begin{equation}
%	\begin{array}{c}
	%		h_t=ReLU(W_1f_3+b_1),\\
	%        q_t=ReLU(W_2h_t+b_2),
	%	\end{array}
%\end{equation}
%\begin{align}
%	h_t=ReLU(W_1f_3+b_1),\\
%	q_t=ReLU(W_2h_t+b_2),
%\end{align}
where $W_1$ and $W_2$ are the weight matrices of the fully connected layer, $b_1$ and $b_2$ are the bias terms, $q_t$ is the output action probability distribution. In order to ensure that the output of the network matches the size of the action space, we set the number of output nodes equal to the size of the action space. Finally, the action probability distribution of the network output can be expressed as
\begin{equation}
	\pi(a_t\mid s_t)=\mathrm{softmax}(q_t).
\end{equation}
In this way, the network can generate a probability distribution for action selection based on the extracted features.

To optimize the policy network in the absence of reward signals, we adopt a new approach. In this situation without manually designed reward signals, selecting the action with the highest probability can make the policy more stable and reduce fluctuations caused by uncertainty. For the continuous action space, we discretize it and put the discretized actions into the discrete action pool. Then, we obtain the index $i_t$ corresponding to the maximum action by the following method, and then obtain the corresponding action $a_t$ from the discrete action pool:
\begin{equation}
	\begin{gathered}
		i_t=\mathop{\arg\max}\pi(a_t\mid s_t),\\
		a_t=\text{ActionPool}[i_t],
	\end{gathered}
\end{equation}
%\begin{equation}
%	a_t=\mathop{\arg\max}\limits_{a}\pi(a_t\mid s_t).
%\end{equation}
where \text{ActionPool} is discrete action pool. This action $a_t$ serves two purposes. First, the action is used to interact with the real environment to obtain the next state $s_{t+1}$. At next step $t+1$, this new state replaces $s_t$ in \eqref{eq:Q}. The initial state of each learning step uses the real state output by the environment, rather than the predicted state, which helps correct errors in the prediction model during policy learning. Second, the action is injected into the TSPN with frozen parameters to predict the next state:
%\begin{equation}
%	s_{t+1}^{pre}=TargetNet_{frozen}(s_t,a_t),
%\end{equation}
\begin{equation}
	s_{t+1}^{pre}=\text{TargetNet}_{\text{frozen}}(s_t,a_t),
\end{equation}
where $\text{TargetNet}_{\text{frozen}}$ represents the TSPN with frozen parameters. When training the RFSGPN, direct interaction with the environment will not be able to effectively backpropagate without the TSPN. Instead of using manually designed reward signal for optimizing the policy network, we use the difference between the next state from expert data $s^{exp}_{t+1}$ and the predicted next state $s^{pre}_{t+1}$ as our loss. We employ the following mean squared error loss function:
\begin{equation}
	\mathcal{L} =\frac{1}{N}\sum_{i=1}^{N}(s_{t+1}^{pre}-s_{t+1}^{exp})^2,
\end{equation}
%\begin{equation}
%	\begin{split}
	%		Loss = &\ \frac{1}{N}\sum_{i=1}^{N} \max\big(CrossEntropy(s_{t+1,i}^{pre}, s_{t+1,i}^{exp}), \\
	%		&\ \text{clip}\big) - \text{clip}.
	%	\end{split}
%\end{equation}
where $N$ is the batch size. In this way, our network training process leverage the advantage of TSPN in state prediction accuracy and advantage of RFSGPN in learning policy, achieving effective learning without manually designing reward signals. Hence, our method is application to environments where reward signals are difficult to design or missing.

Additionally, for the RFSGPN of numerical data, it is not suitable to use convolutional layers to extract features, so in terms of structure, we change the original convolutional layers to linear layers.

\section{Experiments and Discussion}\label{4}
This section elaborates on the experimental setup of our method, covering key steps such as data collection, data processing, and network training. Our method is benchmarked against two existing reward-free and two reward-based reinforcement learning methods in the Carla and Autocar environments. Additionally, a sensitivity analysis of key parameters further validates the robustness and generalization capability of our approach. The experiments are conducted to answer the following questions:
\begin{itemize}{}{}
	\item{Can RFRLF effectively optimize the agent’s behavior without manually designing reward signals?}
	\item{When using only expert state information, can the joint TSPN and the RFSGPN enable the agent to learn effective policies?}
	\item{How do different parameter settings affect the performance of the algorithm, and can it maintain its stability under different training conditions?}
\end{itemize}
%enumerate
%list
%itemize

\subsection{Experimental Setup}
In this part, we introduce our methods for data collection and processing. Subsequently, we describe the training methodology for the network. 

\subsubsection{Data Collection}
The data collection is executed by a data collector equipped with an initialized policy network. It engages in random interactions with the environment, collecting observational data and predicting actions through an untrained, initialized policy agent. The purpose of this process is to gather a series of state-action-next state triplets, which will be used for the training of TSPN.

During the data collection process, we use a temperature parameter to adjust the randomness of the actions selected by the agent. We set the temperature parameter to 1.0 to maintain a balance between exploration and exploitation. The setting of this parameter is an important experimental factor, which we study in detail in the subsequent section \hyperref[sec:Parameter sensitivity analysis]{IV-D}.

\subsubsection{Data processing}
Data processing is carried out after the data collection is completed. We designe a data processor that extracts data of the set batch size from the collected data buffer. During the data processing, the data processor ensures that the extracted observational sequences do not contain end flags. The design of this mechanism is to ensure the quality of the training data, enabling the network to learn on meaningful state sequences. 

\subsubsection{Network Training}
In our framework, training is divided into two stages: independent training of the TSPN and subsequent training of the RFSGPN, with the latter being performed on the basis of fixing the parameters of the former.

During the training of the TSPN, we use Scheduled Sampling techniques and the Adafactor optimization algorithm. Scheduled Sampling is employed to produce the input to the network by determining, through a probability $\epsilon$ , whether to use the actual state sequence or the network's own predictive output. We initially set $\epsilon$ to a value close to 1 and gradually begin to decay it within the first two iterations; in the subsequent iterations, we set $\epsilon$ to 0.5. The purpose of this setting is to allow the network's input to be the real state sequence at the beginning of training, but as training progresses, the network's input gradually transitions from the real state sequence to the network's own prediction results, which helps to improve the network's stability. We use mean squared error loss function, training the network by optimizing this loss function.

During the training of the RFSGPN, we employ the Adafactor optimization algorithm along with the mean square error loss function for parameter update. Specifically, with the TSPN parameters fixed, the agent starts from the initial state, predicts the action and interacts with the environment to obtain the next state, and sends the action to the TSPN to obtain the next predicted state. The mean square error loss function is used to calculate the error between the predicted state and the expert state, and the parameters of the RFSGPN are updated by the Adafactor algorithm.

\subsection{Baselines}
In our experiments, we compare the proposed RFRLF method with four different baseline methods: Import \cite{kamienny2020learning}, Varibad \cite{zintgraf2019varibad}, IPL \cite{hejna2024inverse}, and Dream \cite{liu2021decoupling}.

Import: An algorithm proposed by Kamienny et al. \cite{kamienny2020learning}. While not completely reward-free, Import reduces reliance on explicit rewards by using informed policies to regularize the training of recurrent neural network policies.

Varibad: A Bayesian adaptive deep reinforcement learning method based on meta-learning proposed by Zintgraf et al. \cite{zintgraf2019varibad} uses a Variational Auto-Encoder (VAE) to learn a low-dimensional stochastic latent representation of tasks. Through meta-learning and modeling of task uncertainty, the agent can effectively explore and exploit in an environment with unknown rewards.

IPL: A reward-free algorithm proposed by Hejna et al. \cite{hejna2024inverse}, designed to learn policies through preference-based reinforcement learning without the need to explicitly learn a reward function. IPL utilizes offline preference data and optimizes a parameter-efficient algorithm to directly learn policies from human feedback.

Dream: A meta reinforcement learning framework proposed by Liu et al. \cite{liu2021decoupling}. This framework avoids the problem of local optimum by optimizing exploration and exploitation separately, allowing the agent to learn quickly on new tasks. Notably, it does not depend on explicit reward signals, thereby regarded as a reward-free reinforcement learning approach.

\subsection{Comprehensive Environmental Experiment Analysis}
\subsubsection{Carla Environment Experiment Analysis}
The environment is created by Cai et al. \cite{cai2020high} in the high-fidelity driving simulator Carla \cite{dosovitskiy2017carla}, which provides a highly realistic virtual environment for training and evaluating autonomous driving policies. The map is generated by RoadRunner \cite{vectorzero2019roadrunner}, a road and environment creation software used for vehicle simulation. Our goal is to ensure that the vehicle travels forward without collision along a small number of reference trajectories within the map's lanes. The entire training and testing process is conducted in Carla.

\textbf{States and Actions:} The state space of the environment is defined as follows:
\begin{equation}
	S=\{\delta,\tau,e_y,\dot{e}_y,e_{\varphi},\dot{e}_{\varphi},e_{\beta},\dot{e}_{\beta},e_{vx},\dot{e}_{vx},e_{vy},\dot{e}_{vy},T\},
\end{equation}
where $\delta$ is the steering angle of the vehicle; $\tau$ is the throttle size; $e_y$ is the cross track error, defined as the perpendicular distance of the vehicle from the reference track; $e_{\varphi}$ is the heading angle error, which is the difference between heading angle of the vehicle and the desired heading angle; $e_{\beta}$ is the slip angle difference between the vehicle and the reference trajector; $e_{vx}$ and $e_{vy}$ are the longitudinal and lateral velocity errors, respectively; $\dot{e}_y, \dot{e}_{\varphi}, \dot{e}_{\beta}, \dot{e}_{vx}$ and $\dot{e}_{vy}$ are the time derivatives of $e_y, e_{\varphi}, e_{\beta}, e_{vx}$ and $e_{vy}$, respectively; $T$ contains ten $(x,y)$ positions and slip angle in the reference trajectories ahead. Thus, the dimension of $S$ is 42. The action space is defined as follows:
\begin{equation}
	A=\{\delta,\tau\},
\end{equation}
here,  we limit the vehicle's steering angle $\delta$ and throttle $\tau$ to the range of [-0.8, 0.8] and [0.6, 1], respectively.

\textbf{Rewards:} We define the reward as \eqref{eq:r}, which is the product of the vehicle speed and the weighted sum of the partial rewards:
\begin{equation}
	r=v(40r_{e_y}+40r_{e_{\varphi}}+20r_{e_{\beta}}),
	\label{eq:r}
\end{equation}
where $r_{e_y}$, $r_{e_{\varphi}}$, $r_{e_{\beta}}$ are defined as follows:
%\begin{equation}
%	\begin{gathered}
	%		r_{e_y}=e^{-k_1e_y},\\
	%		r_{e_{\varphi}},r_{e_{\beta}}=f(x)=
	%		\begin{cases}
		%			e^{-k_1|x|}& \text{if } |x|<90^{\circ}\\
		%			-e^{-k_2(180^{\circ}-x)}& \text{if } x\geq90^{\circ}\\
		%			-e^{-k_2(180^{\circ}+x)}& \text{otherwise}
		%		\end{cases}.
	%	\end{gathered}
%\end{equation}
\begin{equation}
	\begin{gathered}
		r_{e_y}=e^{-k_1e_y},\\
		r_{e_{\varphi}}=
		\begin{cases}
			e^{-k_1|e_{\varphi}|}& |e_{\varphi}|<90^{\circ}\\
			-e^{-k_2(180^{\circ}-e_{\varphi})}& e_{\varphi}\geq90^{\circ}\\
			-e^{-k_2(180^{\circ}+e_{\varphi})}& e_{\varphi}\leq90^{\circ}
		\end{cases},\\
		r_{e_{\beta}}=
		\begin{cases}
			e^{-k_1|e_{\beta}|}& |e_{\beta}|<90^{\circ}\\
			-e^{-k_2(180^{\circ}-e_{\beta})}& e_{\beta}\geq90^{\circ}\\
			-e^{-k_2(180^{\circ}+e_{\beta})}& e_{\beta}\leq90^{\circ}
		\end{cases}.
	\end{gathered}
\end{equation}

%%%双栏三线表
%\begin{table}[htbp]
%	\centering
%	\caption{Evaluation Results of Different Methods in the Carla Environment Experiment, Including Maximum, Average, and IQM Metrics.}
%	\label{Table1}
%	\begin{tabular}{p{1.5cm}<{\centering}p{1.2cm}<{\centering}p{1.2cm}<{\centering}p{1.2cm}<{\centering}p{1.2cm}<{\centering}}
%		\toprule
%		\textbf{Method} & \textbf{Max Value} & \textbf{Mean Value} & \textbf{Original IQM} & \textbf{Normal IQM} \\
%		\midrule
%		Ours & 3.97$\times$10\textsuperscript{5} & 3.29$\times$10\textsuperscript{5} & 3.64$\times$10\textsuperscript{5} & 0.91 \\[0.2cm]
%		IPL(2023) & 3.20$\times$10\textsuperscript{5} & 2.16$\times$10\textsuperscript{5} & 2.56$\times$10\textsuperscript{5} & 0.79 \\[0.2cm]
%		Dream(2020) & 1.47$\times$10\textsuperscript{5} & 3.86$\times$10\textsuperscript{4} & 3.47$\times$10\textsuperscript{4} & 0.16 \\[0.2cm]
%		Import(2020) & 1.32$\times$10\textsuperscript{5} & 3.93$\times$10\textsuperscript{4} & 3.41$\times$10\textsuperscript{4} & 0.18 \\[0.2cm]
%		Varibad(2019) & 1.33$\times$10\textsuperscript{5} & 3.07$\times$10\textsuperscript{4} & 2.62$\times$10\textsuperscript{4} & 0.15 \\
%		\bottomrule
%	\end{tabular}
%\end{table}
%
%\begin{figure}[!t]
%	\centerline{\includegraphics[width=3.15in,height=1.98in]{Figure/Diagram of environmental drift test results.png}}
%	\caption{Test results of different methods on the Carla environment.}
%	\label{fig:Carla environmental test results}
%\end{figure}

\noindent Among them, $k_1$ and $k_2$ take the values of 0.5 and 0.1, respectively. Although we have defined rewards, they are only used to verify the effectiveness of our method in the final testing phase and are not utilized during the training process. Additionally, when the vehicle collides with an obstacle, reaches the destination, or is more than 15 meters away from the track, 'done' becomes true, ending the current episode.

\textbf{Evaluation:} Here, we assess and compare the performance of our method with four baseline methods. We evaluate the performance of the agent under each method for 200 episodes and present the testing performance of different methods on this task in Table \ref{Table1}. Correspondingly, Fig. \ref{fig:Carla environmental test results} shows the test results for each episode during the testing period. In Table \ref{Table1}, in addition to the maximum and average values, we also use the Inter-Quartile Mean (IQM) and the normalized IQM as evaluation metrics. These two indicators can effectively reduce the impact of extreme values on the average, thereby more accurately reflecting the central tendency of the test results. Table \ref{Table1} and Fig. \ref{fig:Carla environmental test results} indicate that, overall, RFRLF (Ours) has surpassed existing baselines in terms of maximum, average, and IQM metrics.

%%双栏三线表
\begin{table}[htbp]
	\centering
	\caption{Evaluation Results of Different Methods in the Carla Environment Experiment, Including Maximum, Average, and IQM Metrics.}
	\label{Table1}
	\begin{tabular}{p{1.5cm}<{\centering}p{1.2cm}<{\centering}p{1.2cm}<{\centering}p{1.2cm}<{\centering}p{1.2cm}<{\centering}}
		\toprule
		\textbf{Method} & \textbf{Max Value} & \textbf{Mean Value} & \textbf{Original IQM} & \textbf{Normal IQM} \\
		\midrule
		Ours & 3.97$\times$10\textsuperscript{5} & 3.29$\times$10\textsuperscript{5} & 3.64$\times$10\textsuperscript{5} & 0.91 \\[0.2cm]
		IPL(2023) & 3.20$\times$10\textsuperscript{5} & 2.16$\times$10\textsuperscript{5} & 2.56$\times$10\textsuperscript{5} & 0.79 \\[0.2cm]
		Dream(2020) & 1.47$\times$10\textsuperscript{5} & 3.86$\times$10\textsuperscript{4} & 3.47$\times$10\textsuperscript{4} & 0.16 \\[0.2cm]
		Import(2020) & 1.32$\times$10\textsuperscript{5} & 3.93$\times$10\textsuperscript{4} & 3.41$\times$10\textsuperscript{4} & 0.18 \\[0.2cm]
		Varibad(2019) & 1.33$\times$10\textsuperscript{5} & 3.07$\times$10\textsuperscript{4} & 2.62$\times$10\textsuperscript{4} & 0.15 \\
		\bottomrule
	\end{tabular}
\end{table}

\begin{figure}[!t]
	\centerline{\includegraphics[width=3.15in,height=1.98in]{Diagram_of_environmental_drift_test_results.png}}
	\caption{Test results of different methods on the Carla environment.}
	\label{fig:Carla environmental test results}
\end{figure}

%%%单栏三线表
%\begin{table*}[htbp]
%	\centering
%	\caption{Evaluation Results of Different Methods in the Carla Environment Experiment, Including Maximum, Average, and IQM Metrics.}
%	\label{Table1}
%	\begin{tabular}{p{2cm}<{\centering}p{2cm}<{\centering}p{2cm}<{\centering}p{2cm}<{\centering}p{2cm}<{\centering}}
	%		\toprule
	%		\textbf{Method} & \textbf{Max Value} & \textbf{Mean Value} & \textbf{Original IQM} & \textbf{Normal IQM} \\
	%		\midrule
	%		Ours & 3.97$\times$10\textsuperscript{5} & 3.29$\times$10\textsuperscript{5} & 3.64$\times$10\textsuperscript{5} & 0.91 \\[0.2cm]
	%		IPL(2023) & 3.20$\times$10\textsuperscript{5} & 2.16$\times$10\textsuperscript{5} & 2.56$\times$10\textsuperscript{5} & 0.79 \\[0.2cm]
	%		Dream(2020) & 1.47$\times$10\textsuperscript{5} & 3.86$\times$10\textsuperscript{4} & 3.47$\times$10\textsuperscript{4} & 0.16 \\[0.2cm]
	%		Import(2020) & 1.32$\times$10\textsuperscript{5} & 3.93$\times$10\textsuperscript{4} & 3.41$\times$10\textsuperscript{4} & 0.18 \\[0.2cm]
	%		Varibad(2019) & 1.33$\times$10\textsuperscript{5} & 3.07$\times$10\textsuperscript{4} & 2.62$\times$10\textsuperscript{4} & 0.15 \\
	%		\bottomrule
	%	\end{tabular}
%\end{table*}

%\begin{table}
%	\begin{center}
	%		\caption{Evaluation Results of Different Methods in the Carla Environment Experiment, Including Maximum, Average, and IQM Metrics.}
	%		\label{Table1}
	%		\begin{tabular}{| c | c | c | c | c |}
		%			\hline
		%			\textbf{Method} & \textbf{Max Value} & \textbf{Mean Value} & \textbf{Original IQM} & \textbf{Normal IQM} \\
		%			\hline
		%			Ours & 3.97$\times$10\textsuperscript{5} & 3.29$\times$10\textsuperscript{5} & 3.64$\times$10\textsuperscript{5} & 0.91 \\ 
		%			\hline
		%			IPL(2023) & 3.20$\times$10\textsuperscript{5} & 2.16$\times$10\textsuperscript{5} & 2.56$\times$10\textsuperscript{5} & 0.79 \\
		%			\hline
		%			Dream(2020) & 1.47$\times$10\textsuperscript{5} & 3.86$\times$10\textsuperscript{4} & 3.47$\times$10\textsuperscript{4} & 0.16 \\
		%			\hline 
		%			Import(2020) & 1.32$\times$10\textsuperscript{5} & 3.93$\times$10\textsuperscript{4} & 3.41$\times$10\textsuperscript{4} & 0.18 \\
		%			\hline 
		%			Varibad(2019) & 1.33$\times$10\textsuperscript{5} & 3.07$\times$10\textsuperscript{4} & 2.62$\times$10\textsuperscript{4} & 0.15 \\
		%			\hline
		%		\end{tabular}
	%	\end{center}
%\end{table}
From the results, it can be seen that the Import, Varibad and Dream algorithms do not achieve ideal performance. Although the Import algorithm reduces the dependence on the reward signal to a certain extent by utilizing the information of the task description, it is difficult to find an effective strategy based on this alone when the reward signal is completely missing. The Varibad algorithm involves a complex VAE model and meta-learning process, which may not be able to adapt to the reward-free environment and the autonomous driving scenario that requires rapid response, resulting in decision delays and failure to learn effective strategies. In the absence of such a reward signal, the Dream algorithm may require additional information or policies for effective exploration and learning. In addition, although IPL can simultaneously learn implicit rewards and optimal policies that meet the expert's preferences, and can improve the learning process by collecting feedback data, it has a relatively good effect in this task, but expert preferences still introduce human errors. Compared with these methods, the RFRLF framework we proposed shows significant advantages. RFRLF optimizes the behavior of the agent and guides it to learn an effective policy without manually designing reward signals by combining the TSPN and the RFSGPN.

\subsubsection{Autocar Environment Experiment Analysis}
This environment is created by Maxime Ellerbach \cite{ellerbach2019rl} and is designed to simulate real-world driving scenarios. As shown on the left side of Fig. \ref{fig:Autocar environment picture}, in this environment, the agent drives from a first-person perspective with a limited field of view and is unable to obtain global information. The agent can only observe a small area in front and around it, which makes the decision-making process more complicated. As shown on the right side of Fig. \ref{fig:Autocar environment picture}, the main goal of the agent (indicated by the red dot) is to avoid collisions with road boundaries or other obstacles while maintaining safety, while driving as far forward as possible to increase the driving distance.

\begin{figure*}[!t]
	\centering
	\includegraphics[width=0.8\textwidth]{Autocar_environment.png}
	\caption{A sequence diagram of state-action pairs (time from left to right) for some vehicles when turns left, turns right, and goes straight  and a map of the environment. On the left side of the figure, the upper row shows the first-person perspective (i.e., states) of the vehicle during driving, and the lower row shows the actions taken by the vehicle in these states. On the right side of the figure, a complete map of the driving environment is shown, showing the approximate position and path of the vehicle in the simulated environment.}
	\label{fig:Autocar environment picture}
\end{figure*}

\textbf{States and Actions:} The state consists of three-dimensional image data, which reflects the observation of the vehicle in its limited field of view. The agent can perform three basic actions: turn left, go straight, and turn right. The left side of Fig. \ref{fig:Autocar environment picture} shows the timing diagram of several groups of vehicles taking corresponding actions in different states. Through these images, we can observe how the agent chooses appropriate actions to avoid collisions and optimize the driving path when facing different environmental conditions. For example, in a state where a turn is required, the agent may choose to turn left or right; while on a wide straight road, it may choose to go straight.

\textbf{Rewards:} If the vehicle is moving forward, the reward is the current speed; if the vehicle is turning, the reward is half of the current speed (to prevent excessive turning); if the vehicle is making a half-circle turn, the reward is -45; if the vehicle goes off the track or has a collision, the reward is -150, and the episode ends. Like the previous experiment, although we have defined rewards, they are only used to verify the effectiveness of our method in the final testing phase and are not used in the training process.

\textbf{Evaluation:} Here, we assess and compare the performance of our proposed RFRLF method against four baseline methods. In our experiments, we evaluate the performance of  the agent under each method in 200 episodes. The test results show that the RFRLF method achieves the same performance as the IPL algorithm, Varibad algorithm, and Import algorithm among the four baseline algorithms, all achieving a test reward of 414. This score is significantly higher than the rewards obtained by the Dream algorithm.

The results are due to two reasons. First, the Dream algorithm has limited feature extraction capabilities when processing image data, making it difficult to extract sufficiently accurate features from it and unable to adapt well to the nuances of the environment. In contrast, the TSPN in our proposed RFRLF framework is able to extract state features from image data and combine the corresponding action features with state features to predict the next state. In addition, RFSGPN has a specialized feature extraction network that can effectively process environmental state features, thereby better understanding the environment and adjusting policies. Second, although the Dream algorithm performs well in processing sparse rewards, it has difficulty effectively balancing exploration and exploitation in environments where rewards are completely lacking, probably because it requires sparse rewards to evaluate the utility of its exploration. In contrast, the policy optimization process of RFRLF does not dependent on manually designed reward feedback, but is instead guided by the loss function of the state representation, providing a more stable supervision signal for the learning process in a reward-free environment.

\subsection{Performance Evaluation of Target State Prediction Network}
The TSPN is a key component of the RFRLF framework, which aims to predict the next state of the environment. This TSPN provides prediction values for the RFSGPN, which is compared with the state information of the expert to help the agent learn effective policies in the reward-free environment. This section takes the Autocar environment task as an example to evaluate the performance of the TSPN and show the accuracy of its prediction.

We train the TSPN for 10 epochs. The specific network configuration and hyperparameters are as follows: the size of the state frame is 3×159×255, the batch size is 16, and the learning rate scheduler is set to decay the learning rate after every 5,000 steps, with a decay coefficient of 0.5. In order to verify the performance improvement of the target state prediction network, we re-collect test data after every 5 epochs of training, and use this data to test the TSPN, comparing the next state $s^{pre}_{t+1}$ predicted by the network with the actual next state $s_{t+1}$. Fig. \hyperref[fig:Comparison of prediction model performance]{5} shows the effect of the network after training. It can be clearly seen from the figure that as the training progresses, the prediction accuracy of the network is significantly improved. In particular, in Fig. \hyperref[fig:Comparison of prediction model performance]{5 (b)}, the predicted state generated by the TSPN is very close to the actual state, indicating that the network has been able to effectively learn the relationship between the current state and the action, and predict the dynamic changes of the environment.

%We train the TSPN for 10 epochs. The specific network configuration and hyperparameters are as follows: the size of the state frame is 3×159×255, the batch size is 16, and the learning rate scheduler is set to decay the learning rate after every 5,000 steps, with a decay coefficient of 0.5. In order to verify the performance improvement of the target state prediction network, we re-collect test data after every 5 epochs of training, and use this data to test the TSPN, comparing the next state $s^{pre}_{t+1}$ predicted by the network with the actual next state $s_{t+1}$. Fig. \hyperref[fig:Comparison of prediction model performance]{5 (a)} shows the effect of the network after 5 epochs of training, and Fig. \hyperref[fig:Comparison of prediction model performance]{5 (b)} shows the effect of the network after 10 epochs of training. It can be clearly seen from the figure that as the training progresses, the prediction accuracy of the network is significantly improved. In particular, in Fig. \hyperref[fig:Comparison of prediction model performance]{5 (b)}, the predicted state generated by the TSPN is very close to the actual state, indicating that the network has been able to effectively learn the relationship between the current state and the action, and predict the dynamic changes of the environment.

%\begin{figure}[!t]
%	\centerline{\includegraphics[width=3.15in,height=1.98in]{Figure/Comparison of prediction model performance after five epochs}}
%	\caption{Prediction performance of the TSPN after five epochs of training.}
%	\label{fig:five epochs}
%\end{figure}
%
%\begin{figure}[!t]
%	\centerline{\includegraphics[width=3.15in,height=1.98in]{Figure/Comparison of prediction model performance after ten epochs}}
%	\caption{Prediction performance of the TSPN after ten epochs of training.}
%	\label{fig:ten epochs}
%\end{figure}

%\begin{figure}[!t]
%	\centering
%	\begin{minipage}{0.45\textwidth}
%		\centering
%		\includegraphics[width=\linewidth]{Figure/Comparison of prediction model performance after five epochs}
%		{\footnotesize (a) Prediction performance of the TSPN after 5 epochs of training.}
%	\end{minipage}
%	\hfill
%	\begin{minipage}{0.45\textwidth}
%		\centering
%		\includegraphics[width=\linewidth]{Figure/Comparison of prediction model performance after ten epochs}
%		{\footnotesize (b) Prediction performance of the TSPN after 10 epochs of training.}
%	\end{minipage}
%	\caption{Prediction performance of the TSPN after training.}
%	\label{fig:Comparison of prediction model performance}
%\end{figure}

\begin{figure}[!t]
	\centering
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=\linewidth]{Comparison_of_prediction_model_performance_after_five_epochs}
		{\footnotesize (a)}
	\end{minipage}
	\hfill
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=\linewidth]{Comparison_of_prediction_model_performance_after_ten_epochs}
		{\footnotesize (b)}
	\end{minipage}
	\caption{Prediction performance of the TSPN after training. (a) Prediction performance of the TSPN after 5 epochs of training. (b) Prediction performance of the TSPN after 10 epochs of training.}
	\label{fig:Comparison of prediction model performance}
\end{figure}

The key reason why the TSPN can achieve high-precision predictions lies in the action injection layer in its design. By injecting action information into the state features, the network is able to capture the impact of actions on state changes. In addition, the upsampling and downsampling modules not only effectively preserve the spatial resolution of the features, but also enable the network to capture richer contextual information, thereby further improving the prediction accuracy.

In general, the TSPN performed well in the experiment and is able to accurately predict the next state of the environment, which is crucial for the RFRLF framework. Accurate state prediction provides a reliable foundation for the training of the RFSGPN, allowing the RFSGPN to optimize the behavior of the agent and learn effective policies even in the absence of manually designed reward signals.

\subsection{Parameter Sensitivity Analysis}\label{sec:Parameter sensitivity analysis}
In the Carla environment, we implement 5 comparison schemes by setting various hyperparameters and making changes to components. In Table \ref{Table2}, ``Partial Freeze (PF)" indicates freezing some layers (input embedding layer and downsampling layer) of the TSPN. This is to analyze the impact of free adjustment of other layers on performance while retaining some feature extraction capabilities. In order to analyze the impact of temperature parameter ``$T$=1.0" and scheduled sampling rate ``$\epsilon$=0.5" on the RFRLF method, we also set $T$ in \{0.5,2.0\} and $\epsilon$ in \{0.3,0.7\} , where $T$ is the temperature parameter during data collection and $\epsilon$ is the scheduled sampling rate after the TSPN training epoch is greater than 2. These parameters are selected to observe the impact of smaller and larger temperatures and sampling rates on model exploration and training stability.

%%双栏三线表
\begin{table}[htbp]
	\centering
	\caption{Evaluation Results of Different Agents, Including Maximum, Average, and IQM Metrics.}
	\label{Table2}
	\begin{tabular}{p{1.5cm}<{\centering}p{1.2cm}<{\centering}p{1.2cm}<{\centering}p{1.2cm}<{\centering}p{1.2cm}<{\centering}}
		\toprule
		\textbf{Method} & \textbf{Max Value} & \textbf{Mean Value} & \textbf{Original IQM} & \textbf{Normal IQM} \\
		\midrule
		RFRLF & 3.97$\times$10\textsuperscript{5} & 3.29$\times$10\textsuperscript{5} & 3.64$\times$10\textsuperscript{5} & 0.91 \\ [0.2cm]
		T=0.5 & 3.75$\times$10\textsuperscript{5} & 3.12$\times$10\textsuperscript{5} & 3.43$\times$10\textsuperscript{5} & 0.90 \\[0.2cm]
		T=2.0 & 3.89$\times$10\textsuperscript{5} & 3.33$\times$10\textsuperscript{5} & 3.67$\times$10\textsuperscript{5} & 0.93 \\[0.2cm]
		$\epsilon=0.3$ & 3.77$\times$10\textsuperscript{5} & 3.16$\times$10\textsuperscript{5} & 3.48$\times$10\textsuperscript{5} & 0.91 \\[0.2cm]
		$\epsilon=0.7$ & 3.93$\times$10\textsuperscript{5} & 3.26$\times$10\textsuperscript{5} & 3.57$\times$10\textsuperscript{5} & 0.88 \\[0.2cm]
		PF & 3.91$\times$10\textsuperscript{5} & 3.25$\times$10\textsuperscript{5} & 3.57$\times$10\textsuperscript{5} & 0.90 \\
		\bottomrule
	\end{tabular}
\end{table}


%%%%单栏三线表
%\begin{table*}[htbp]
%	\centering
%	\caption{Evaluation Results of Different Agents, Including Maximum, Average, and IQM Metrics.}
%	\label{Table2}
%	\begin{tabular}{p{2cm}<{\centering}p{2cm}<{\centering}p{2cm}<{\centering}p{2cm}<{\centering}p{2cm}<{\centering}}
	%			\toprule
	%			\textbf{Method} & \textbf{Max Value} & \textbf{Mean Value} & \textbf{Original IQM} & \textbf{Normal IQM} \\
	%			\midrule
	%			RFRLF & 3.97$\times$10\textsuperscript{5} & 3.29$\times$10\textsuperscript{5} & 3.64$\times$10\textsuperscript{5} & 0.91 \\ [0.2cm]
	%			T=0.5 & 3.75$\times$10\textsuperscript{5} & 3.12$\times$10\textsuperscript{5} & 3.43$\times$10\textsuperscript{5} & 0.90 \\[0.2cm]
	%			T=2.0 & 3.89$\times$10\textsuperscript{5} & 3.33$\times$10\textsuperscript{5} & 3.67$\times$10\textsuperscript{5} & 0.93 \\[0.2cm]
	%			$\epsilon=0.3$ & 3.77$\times$10\textsuperscript{5} & 3.16$\times$10\textsuperscript{5} & 3.48$\times$10\textsuperscript{5} & 0.91 \\[0.2cm]
	%			$\epsilon=0.7$ & 3.93$\times$10\textsuperscript{5} & 3.26$\times$10\textsuperscript{5} & 3.57$\times$10\textsuperscript{5} & 0.88 \\[0.2cm]
	%			PF & 3.91$\times$10\textsuperscript{5} & 3.25$\times$10\textsuperscript{5} & 3.57$\times$10\textsuperscript{5} & 0.90 \\[0.2cm]
	%			SO & 3.02$\times$10\textsuperscript{5} & 2.52$\times$10\textsuperscript{5} & 2.80$\times$10\textsuperscript{5} & 0.92 \\
	%			\bottomrule
	%		\end{tabular}
%\end{table*}

Overall, the performance of RFRLF is comparable to the baseline with altered hyperparameters and outperforms the baseline with partial component structure changes in terms of Mean Value and IQM. Specifically, by comparing with the ``PF" baseline, it is found that after freezing part of the TSPN, the network performance does not change much. This may be because the frozen input embedding layer and downsampling layer effectively retain the network's ability to extract input features, while allowing other unfrozen layers to still be adjusted and optimized during training, thereby ensuring the stability of the overall performance.

In the temperature parameter analysis, the ``$T$=2.0" baseline performs best. This is because when training the TSPN, a higher temperature coefficient enhances exploration, enabling the agent to explore more states, thereby providing the TSPN with richer data. However, as the temperature coefficient increases, instability also increases, which means that the $T$ should not be too high, and a balance needs to be found between exploration and stability. Therefore, ``$T$=1.0" as the default value is a suitable compromise. In addition, by comparing the ``$T$=0.5", ``$T$=2.0", ``$\epsilon$=0.3", and ``$\epsilon$=0.7" baselines, we find that hyperparameters have a certain impact on the performance of RFRLF, but in general, RFRLF is not very sensitive to changes in hyperparameters. This shows that RFRLF has strong robustness within a certain range and can adapt to different hyperparameter settings while maintaining good performance.

\section{Conclusion}\label{5}
This paper presents a novel Reward-Free Reinforcement Learning Framework (RFRLF), aimed at optimizing the agent's policy without manually designed reward signals and without explicit action information. RFRLF consists of two main modules: the Target State Prediction Network (TSPN) and the Reward-Free State-Guided Policy Network (RFSGPN). The TSPN predicts the target state based on the agent's current state and the actions it performs. The RFSGPN uses the output information of the TSPN and expert demonstration data that does not contain action and reward information to optimize the agent's policy.

Experimental results show that RFRLF demonstrates superior performance compared to various reward-based and reward-free reinforcement learning methods in the Carla and Autocar environments. Our framework has surpassed baseline methods in terms of maximum value, average value, and IQM metrics, proving its effectiveness for vehicle control in reward-free environments.

Although RFRLF performs well in simulation environments, its performance still needs to be optimized in the real world to cope with more uncertainties. In the future, the application and development of RFRLF in real environments will be further promoted by introducing dynamic environmental factors and more efficient optimization algorithms.

\bibliographystyle{IEEEtran}
\bibliography{Huang} 

\end{document}


