@misc{huEKANEquivariantKolmogorovArnold2024a,
  title = {{{EKAN}}: {{Equivariant Kolmogorov-Arnold Networks}}},
  shorttitle = {{{EKAN}}},
  author = {Hu, Lexiang and Wang, Yisen and Lin, Zhouchen},
  year = {2024},
  month = oct,
  number = {arXiv:2410.00435},
  eprint = {2410.00435},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.00435},
  urldate = {2025-01-31},
  abstract = {Kolmogorov-Arnold Networks (KANs) have seen great success in scientific domains thanks to spline activation functions, becoming an alternative to Multi-Layer Perceptrons (MLPs). However, spline functions may not respect symmetry in tasks, which is crucial prior knowledge in machine learning. Previously, equivariant networks embed symmetry into their architectures, achieving better performance in specific applications. Among these, Equivariant Multi-Layer Perceptrons (EMLP) introduce arbitrary matrix group equivariance into MLPs, providing a general framework for constructing equivariant networks layer by layer. In this paper, we propose Equivariant Kolmogorov-Arnold Networks (EKAN), a method for incorporating matrix group equivariance into KANs, aiming to broaden their applicability to more fields. First, we construct gated spline basis functions, which form the EKAN layer together with equivariant linear weights. We then define a lift layer to align the input space of EKAN with the feature space of the dataset, thereby building the entire EKAN architecture. Compared with baseline models, EKAN achieves higher accuracy with smaller datasets or fewer parameters on symmetry-related tasks, such as particle scattering and the three-body problem, often reducing test MSE by several orders of magnitude. Even in non-symbolic formula scenarios, such as top quark tagging with three jet constituents, EKAN achieves comparable results with EMLP using only \$26{\textbackslash}\%\$ of the parameters, while KANs do not outperform MLPs as expected.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/alesiani/Zotero/storage/VVGEWFM4/Hu et al. - 2024 - EKAN Equivariant Kolmogorov-Arnold Networks.pdf;/Users/alesiani/Zotero/storage/3UL6UH68/2410.html}
}


@misc{yangKolmogorovArnoldTransformer2024,
  title = {Kolmogorov-{{Arnold Transformer}}},
  author = {Yang, Xingyi and Wang, Xinchao},
  year = {2024},
  month = sep,
  number = {arXiv:2409.10594},
  eprint = {2409.10594},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.10594},
  urldate = {2025-01-30},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},

}


@misc{somvanshiSurveyKolmogorovArnoldNetwork2024,
  title = {A {{Survey}} on {{Kolmogorov-Arnold Network}}},
  author = {Somvanshi, Shriyank and Javed, Syed Aaqib and Islam, Md Monzurul and Pandit, Diwas and Das, Subasish},
  year = {2024},
  month = nov,
  number = {arXiv:2411.06078},
  eprint = {2411.06078},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.06078},
  urldate = {2024-11-29},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
}



@article{Finzi2021,
  author       = {Marc Finzi and
                  Max Welling and
                  Andrew Gordon Wilson},
  title        = {A Practical Method for Constructing Equivariant Multilayer Perceptrons
                  for Arbitrary Matrix Groups},
  journal      = {CoRR},
  volume       = {abs/2104.09459},
  year         = {2021},
  url          = {https://arxiv.org/abs/2104.09459},
  eprinttype    = {arXiv},
  eprint       = {2104.09459},
  timestamp    = {Mon, 26 Apr 2021 17:25:10 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2104-09459.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{glorot2011deep,
  title={Deep sparse rectifier neural networks},
  author={Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  booktitle={Proceedings of the fourteenth international conference on artificial intelligence and statistics},
  pages={315--323},
  year={2011},
  organization={JMLR Workshop and Conference Proceedings}
}

@article{chmiela2023accurate,
  title={Accurate global machine learning force fields for molecules with hundreds of atoms},
  author={Chmiela, Stefan and Vassilev-Galindo, Valentin and Unke, Oliver T and Kabylda, Adil and Sauceda, Huziel E and Tkatchenko, Alexandre and M{\"u}ller, Klaus-Robert},
  journal={Science Advances},
  volume={9},
  number={2},
  pages={eadf0873},
  year={2023},
  publisher={American Association for the Advancement of Science}
}

@article{chmiela2017machine,
  title={Machine learning of accurate energy-conserving molecular force fields},
  author={Chmiela, Stefan and Tkatchenko, Alexandre and Sauceda, Huziel E and Poltavsky, Igor and Sch{\"u}tt, Kristof T and M{\"u}ller, Klaus-Robert},
  journal={Science advances},
  volume={3},
  number={5},
  pages={e1603015},
  year={2017},
  publisher={American Association for the Advancement of Science}
}

@misc{villarScalarsAreUniversal2023,
  title = {Scalars Are Universal: {{Equivariant}} Machine Learning, Structured like Classical Physics},
  shorttitle = {Scalars Are Universal},
  author = {Villar, Soledad and Hogg, David W. and {Storey-Fisher}, Kate and Yao, Weichi and {Blum-Smith}, Ben},
  year = {2023},
  month = feb,
  number = {arXiv:2106.06610},
  eprint = {2106.06610},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.06610},
  urldate = {2024-11-26},
  abstract = {There has been enormous progress in the last few years in designing neural networks that respect the fundamental symmetries and coordinate freedoms of physical law. Some of these frameworks make use of irreducible representations, some make use of high-order tensor objects, and some apply symmetry-enforcing constraints. Different physical laws obey different combinations of fundamental symmetries, but a large fraction (possibly all) of classical physics is equivariant to translation, rotation, reflection (parity), boost (relativity), and permutations. Here we show that it is simple to parameterize universally approximating polynomial functions that are equivariant under these symmetries, or under the Euclidean, Lorentz, and Poincar{\textbackslash}'e groups, at any dimensionality \$d\$. The key observation is that nonlinear O(\$d\$)-equivariant (and related-group-equivariant) functions can be universally expressed in terms of a lightweight collection of scalars -- scalar products and scalar contractions of the scalar, vector, and tensor inputs. We complement our theory with numerical examples that show that the scalar-based method is simple, efficient, and scalable.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematical Physics,Mathematics - Mathematical Physics,Statistics - Machine Learning},
  file = {/Users/alesiani/Zotero/storage/2XPB7QW2/Villar et al. - 2023 - Scalars are universal Equivariant machine learning, structured like classical physics.pdf;/Users/alesiani/Zotero/storage/I2ZIEYP2/2106.html}
}


@article{ostrandDIMENSIONMETRICSPACESa,
  Title = {{{Dimension of Metric Spaces and Hilbert}}'{{S Problem}} 13},
  author = {Ostrand, Phillip A},
  langid = {english},
  year      = {1965},
  keywords = {No DOI found},
  file = {/Users/alesiani/Zotero/storage/6EM9QUC8/Ostrand - DIMENSION OF METRIC SPACES AND HUBERT'S PROBLEM 13.pdf}
}


@article{kreinovichNORMALFORMSFUZZY1996,
  title = {{{Normal Forms For Fuzzy Logic}} --- {{An Application Of Kolmogorov}}'{{S Theorem}}},
  author = {Kreinovich, Vladik and Nguyen, Hung T. and Sprecher, David A.},
  year = {1996},
  month = aug,
  journal = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
  volume = {04},
  number = {04},
  pages = {331--349},
  issn = {0218-4885, 1793-6411},
  doi = {10.1142/S0218488596000196},
  urldate = {2025-01-24},
  abstract = {This paper addresses mathematical aspects of fuzzy logic. The main results obtained in this paper are:             1. the introduction of a concept of normal form in fuzzy logic using hedges;             2. using Kolmogorov's theorem, we prove that all logical operations in fuzzy logic have normal forms;             3. for min-max operators, we obtain an approximation result similar to the universal approximation property of neural networks.},
  langid = {english},
  file = {/Users/alesiani/Zotero/storage/LGQ75HNG/Kreinovich et al. - 1996 - NORMAL FORMS FOR FUZZY LOGIC — AN APPLICATION OF KOLMOGOROV’S THEOREM.pdf}
}


@article{epistemicNNs,
  author    = {Ian Osband and
               Zheng Wen and
               Mohammad Asghari and
               Morteza Ibrahimi and
               Xiyuan Lu and
               Benjamin Van Roy},
  title     = {Epistemic Neural Networks},
  journal   = {CoRR},
  volume    = {abs/2107.08924},
  year      = {2021},
  url       = {https://arxiv.org/abs/2107.08924},
  eprinttype = {arXiv},
  eprint    = {2107.08924},
  timestamp = {Thu, 22 Jul 2021 11:14:11 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2107-08924.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}









% Encoding: UTF-8
% Edit By Wenjie Liu 2021/5/26


@book{PetersenZech2024,
  title     = {{Mathematical Theory of Deep Learning}},
  author    = {Philipp Petersen and Jakob Zech},
  year      = {2024},
  publisher = {arXiv},
  note      = {arXiv:2407.18384 [cs.LG]},
}

@article{Igelnik2003,
  author    = {Boris Igelnik and Neel Parikh},
  title     = {Kolmogorov's Spline Network},
  journal   = {IEEE Transactions on Neural Networks},
  volume    = {14},
  number    = {4},
  pages     = {725--733},
  year      = {2003},
}




@article{Sternfeld1985,
  author = {Sternfeld, Y.},
  title = {Dimension, superposition of functions and separation of points, in compact metric spaces},
  journal = {Israel Journal of Mathematics},
  volume = {50},
  pages = {13--53},
  year = {1985},
}


@article{YHattori1993,
  author       = {Yasunao Hattori},
  title        = {Dimension and superposition of bounded continuous functions on locally compact, separable metric spaces},
  journal      = {Topology and its Applications},
  volume       = {54},
  number       = {1–3},
  pages        = {123--132},
  year         = {1993},
  month        = {December},
  publisher    = {Elsevier},
}



@article{Kolmogorov1956,
  author       = {Andrey Kolmogorov},
  title        = {On the representation of continuous functions of several variables by superpositions of continuous functions of a smaller number of variables},
  journal      = {Proceedings of the USSR Academy of Sciences},
  volume       = {108},
  year         = {1956},
  pages        = {179--182},
  note         = {English translation: Amer. Math. Soc. Transl., 17: Twelve Papers on Algebra and Real Functions (1961), pp. 369--373}
}

@article{Arnold1957a,
  author       = {Vladimir Arnold},
  title        = {On functions of three variables},
  journal      = {Proceedings of the USSR Academy of Sciences},
  volume       = {114},
  year         = {1957},
  pages        = {679--681},
  note         = {English translation: Amer. Math. Soc. Transl., 28: Sixteen Papers on Analysis (1963), pp. 51--54}
}


@article{Arnold1957b,
  author       = {Vladimir Arnold},
  title        = {On the representation of continuous functions of three variables as superpositions of continuous functions of two variables},
  journal      = {Doklady Akademii Nauk SSSR},
  volume       = {114},
  number       = {4},
  year         = {1957},
  pages        = {679--681},
  language     = {Russian},
  note         = {Available on SpringerLink}
}

@article{Kolmogorov1957,
  author       = {Andrey Kolmogorov},
  title        = {On the representation of continuous functions of several variables as superpositions of continuous functions of one variable and addition},
  year         = {1957},
  note         = {English translation: Amer. Math. Soc. Transl., 28: Sixteen Papers on Analysis (1963)}
}


@article{Freedman2024proof,
  title={The Proof of {Kolmogorov-Arnold} May Illuminate Neural Network Learning},
  author={Freedman, Michael H},
  journal={arXiv preprint arXiv:2410.08451},
  year={2024}
}


@article{Demko1977superposition,
  title={A superposition theorem for bounded continuous functions},
  author={Demko, Stephen},
  journal={Proceedings of the American Mathematical Society},
  volume={66},
  number={1},
  pages={75--78},
  year={1977}
}


@article{Doss1977superposition,
  title={A superposition theorem for unbounded continuous functions},
  author={Doss, Raouf},
  journal={Transactions of the American Mathematical Society},
  volume={233},
  pages={197--203},
  year={1977}
}

@article{Park1991universal,
  title={Universal approximation using radial-basis-function networks},
  author={Park, Jooyoung and Sandberg, Irwin W},
  journal={Neural Computation},
  volume={3},
  number={2},
  pages={246--257},
  year={1991},
  publisher={MIT Press}
}

@article{Montanelli2020error,
  title={{Error bounds for deep ReLU networks using the Kolmogorov--Arnold superposition theorem}},
  author={Montanelli, Hadrien and Yang, Haizhao},
  journal={Neural Networks},
  volume={129},
  pages={1--6},
  year={2020},
  publisher={Elsevier}
}

@article{laczkovich2021superposition,
  title={{A superposition theorem of Kolmogorov type for bounded continuous functions}},
  author={Laczkovich, Mikl{\'o}s},
  journal={Journal of Approximation Theory},
  volume={269},
  pages={105609},
  year={2021},
  publisher={Elsevier}
}


@book{Bader2013,
  author    = {Michael Bader},
  title     = {{Space-Filling Curves: An Introduction with Applications in Scientific Computing}},
  year      = {2013},
  publisher = {Springer},
  address   = {Berlin, Heidelberg},
  isbn      = {978-3-642-31045-3},
}


@article{Ismayilova2024kolmogorov,
  title={{On the Kolmogorov neural networks}},
  author={Ismayilova, Aysu and Ismailov, Vugar E},
  journal={Neural Networks},
  volume={176},
  pages={106333},
  year={2024},
  publisher={Elsevier}
}

@article{Ismailov2024kart_uar_nn,
  title={Addressing Common Misinterpretations of {KART} and {UAT} in Neural Network Literature},
  author={Vugar Ismailov},
  journal={arXiv preprint arXiv:2408.16389},
  year={2024}
}


@book{Lorentz1996,
  author    = {Lorentz, G. G. and Golitschek, M. and Makovoz, Y.},
  title     = {Constructive Approximation},
  series    = {Grundlehren der Mathematischen Wissenschaften},
  volume    = {304},
  publisher = {Springer},
  address   = {Berlin},
  year      = {1996},
  isbn      = {978-3-642-64669-3},
}

@article{Kahane1975,
  author    = {Kahane, J.-P.},
  title     = {{Sur le th\'eor\`eme de superposition de Kolmogorov}},
  journal   = {Journal of Approximation Theory},
  volume    = {13},
  pages     = {229--234},
  year      = {1975},
}



@article{zhang2024structured,
  title={Structured and Balanced Multi-component and Multi-layer Neural Networks},
  author={Zhang, Shijun and Zhao, Hongkai and Zhong, Yimin and Zhou, Haomin},
  journal={arXiv preprint arXiv:2407.00765},
  year={2024},
}


@article{Lai2024optimal,
  title= {The optimal rate for linear {KB}-splines and {LKB}-splines approximation of high dimensional continuous functions and its application},
  author    = {Lai, Ming-Jun and Shen, Zhaiming},
  journal   = {arXiv preprint arXiv:2401.03956},
  year      = {2024},
}

@article{Girosi1989,
  author    = {Federico Girosi and Tomaso Poggio},
  title     = {Representation Properties of Networks: Kolmogorov's Theorem Is Irrelevant},
  journal   = {Neural Computation},
  volume    = {1},
  number    = {4},
  pages     = {465--469},
  year      = {1989},
}

@article{Vitushkin1954,
  author    = {Vitushkin, A. G.},
  title     = {{On Hilbert's Thirteenth Problem}},
  journal   = {Doklady Akademii Nauk SSSR},
  volume    = {95},
  pages     = {701--704},
  year      = {1954}
}



@inproceedings{Koppen2002,
  author    = {K\"oppen, Mario},
  title     = {On the Training of a Kolmogorov Network},
  booktitle = {ICANN 2002: International Conference on Artificial Neural Networks},
  series    = {Lecture Notes in Computer Science},
  volume    = {2415},
  pages     = {474--479},
  year      = {2002},
  publisher = {Springer},
}




@article{Siegel2023112084,
title = {{Greedy training algorithms for neural networks and applications to PDEs}},
journal = {Journal of Computational Physics},
volume = {484},
pages = {112084},
year = {2023},
issn = {0021-9991},
author = {Jonathan W. Siegel and Qingguo Hong and Xianlin Jin and Wenrui Hao and Jinchao Xu},
}


@article{Zeinhofer2023unified,
  title={A unified framework for the error analysis of physics-informed neural networks},
  author={Zeinhofer, Marius and Masri, Rami and Mardal, Kent-Andr{\'e}},
  journal={arXiv preprint arXiv:2311.00529},
  year={2023}
}

@article{Cybenko1989,
  title={Approximation by superpositions of a sigmoidal function},
  author={Cybenko, George},
  journal={Mathematics of Control, Signals, and Systems},
  volume={2},
  number={4},
  pages={303--314},
  year={1989},
  publisher={Springer},
}

@article{Hornik1991approximation,
  title={Approximation capabilities of multilayer feedforward networks},
  author={Hornik, Kurt},
  journal={Neural Networks},
  volume={4},
  number={2},
  pages={251--257},
  year={1991},
  publisher={Elsevier}
}

@article{Barron1993universal,
  title={Universal Approximation Bounds for Superpositions of a Sigmoidal Function},
  author={Barron, Andrew R.},
  journal={IEEE Transactions on Information Theory},
  volume={39},
  number={3},
  pages={930--945},
  year={1993},
  publisher={IEEE}
}

@article{Kourkova1991,
  author    = {K{$\dot {\rm u}$}rkov{\'a}, Vera},
  title     = {Kolmogorov's Theorem Is Relevant},
  journal   = {Neural Computation},
  volume    = {3},
  number    = {4},
  pages     = {617--622},
  year      = {1991},
}



@article{Leshno1993,
    author = {M. Leshno and V. Y. Lin and A. P. Pinkus and S. S. Schocken},
    title = {Multilayer feedforward networks with a nonpolynomial activation function can approximate any function},
    journal = {Neural Networks},
    volume = {6},
    number = {9},
    pages = {861--867},
    year = {1993},
    publisher = {Elsevier}
}

@phdthesis{Sprecher1963dissertation,
  author       = {Sprecher, D.},
  title        = {Ph.D. Dissertation},
  school       = {University of Maryland},
  year         = {1963}
}


@article{Lorentz1962metric,
  author       = {G. G. Lorentz},
  title        = {Metric Entropy, Widths, and Superpositions of Functions},
  journal      = {The American Mathematical Monthly},
  volume       = {69},
  number       = {6},
  pages        = {469--485},
  year         = {1962},
  publisher    = {Mathematical Association of America},
}


@book{Lorentz1966approximation,
  title     = {{Approximation of Functions}},
  author    = {Lorentz, G. G.},
  year      = {1966},
  publisher = {Holt, Rinehart and Winston, Inc.}
}


@article{Sprecher1965structure,
  title={On the structure of continuous functions of several variables},
  author={Sprecher, David A},
  journal={Transactions of the American Mathematical Society},
  volume={115},
  pages={340--355},
  year={1965}
}


@article{Sprecher1996numerical,
  title={{A numerical implementation of Kolmogorov's superpositions}},
  author={Sprecher, David A},
  journal={Neural Networks},
  volume={9},
  number={5},
  pages={765--772},
  year={1996},
  publisher={Elsevier}
}

@article{Hattori1993,
  author    = {Hattori, Y.},
  title     = {Dimension and Superposition of Bounded Continuous Functions on Locally Compact, Separable Metric Spaces},
  journal   = {Topology and its Applications},
  volume    = {54},
  number    = {1--3},
  pages     = {123--132},
  year      = {1993},
}


@article{Feng2011,
  author    = {Feng, Z. and Gartside, P.},
  title     = {Spaces with a Finite Family of Basic Functions},
  journal   = {Bulletin of the London Mathematical Society},
  volume    = {43},
  number    = {1},
  pages     = {26--32},
  year      = {2011},
}


@article{Sprecher1997numerical,
  title={{A numerical implementation of Kolmogorov's superpositions II}},
  author={Sprecher, David A},
  journal={Neural Networks},
  volume={10},
  number={3},
  pages={447--457},
  year={1997},
  publisher={Elsevier}
}

@book{Sprecher2017algebra,
  title={{From Algebra to Computational Algorithms: Kolmogorov and Hilbert's Problem 13}},
  author={Sprecher, David A},
  year={2017},
  publisher={Docent Press}
}

@article{Demb2021note,
  title={{A note on computing with Kolmogorov Superpositions without iterations}},
  author={Demb, Robert and Sprecher, David},
  journal={Neural Networks},
  volume={144},
  pages={438--442},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{HechtNielsen1987,
  author    = {Hecht-Nielsen, Robert},
  title     = {Kolmogorov's Mapping Neural Network Existence Theorem},
  booktitle = {Proceedings of the IEEE First International Conference on Neural Networks},
  volume    = {III},
  pages     = {11--13},
  year      = {1987},
  publisher = {IEEE},
  address   = {Piscataway, NJ},
  location  = {San Diego, CA}
}



@Inbook{Brattka2007,
author="Brattka, Vasco",
editor="Charpentier, {\'E}ric
and Lesne, Annick
and Nikolski, Nikola{\"i} K.",
title="From Hilbert's 13th Problem to the theory of neural networks: constructive aspects of Kolmogorov's Superposition Theorem",
bookTitle="Kolmogorov's Heritage in Mathematics",
year="2007",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="253--280",
}

@article{Braun2009constructive,
  title={{On a constructive proof of Kolmogorov’s superposition theorem}},
  author={Braun, J{\"u}rgen and Griebel, Michael},
  journal={Constructive approximation},
  volume={30},
  pages={653--675},
  year={2009},
  publisher={Springer}
}


@phdthesis{braun2009application,
  title={{An Application of Kolmogorov's Superposition Theorem to Function Reconstruction in Higher Dimensions}},
  author={Braun, J{\"u}rgen},
  year={2009},
  school={Universit{\"a}ts-und Landesbibliothek Bonn}
}

@book{Khavinson1997best,
  author       = {Semen Ya. Khavinson},
  title        = {Best Approximation by Linear Superpositions (Approximate Nomography)},
  translator   = {D. Khavinson},
  series       = {Translations of Mathematical Monographs},
  volume       = {159},
  year         = {1997},
  publisher    = {American Mathematical Society},
  address      = {Providence, RI},
  isbn         = {978-0-8218-0422-3},
}



@article{schmidt2021kolmogorov,
  title={{The Kolmogorov--Arnold representation theorem revisited}},
  author={Schmidt-Hieber, Johannes},
  journal={Neural Networks},
  volume={137},
  pages={119--126},
  year={2021},
  publisher={Elsevier}
}


@article {Ainsworth-Song21,
    AUTHOR = {Ainsworth, M. and Dong, J.},
     TITLE = {{Galerkin neural networks: A framework for approximating
              variational equations with error control}},
   JOURNAL = {SIAM J. Sci. Comput.},
  FJOURNAL = {SIAM Journal on Scientific Computing},
    VOLUME = {43},
      YEAR = {2021},
    NUMBER = {4},
     PAGES = {A2474--A2501},
}

@article {Wang-Xie24,
    AUTHOR = {Wang, Y.F. and Xie, H.H.},
     TITLE = {Computing multi-eigenpairs of high-dimensional eigenvalue
              problems using tensor neural networks},
   JOURNAL = {J. Comput. Phys.},
  FJOURNAL = {Journal of Computational Physics},
    VOLUME = {506},
      YEAR = {2024},
}

@article{Demko1977,
  author    = {Demko, S.},
  title     = {A Superposition Theorem for Bounded Continuous Functions},
  journal   = {Proceedings of the American Mathematical Society},
  volume    = {66},
  number    = {1},
  pages     = {75--78},
  year      = {1977},
}




























@article{log-likelihood-1981,
author = {John W. Pratt},
title = {Concavity of the Log Likelihood},
journal = {Journal of the American Statistical Association},
volume = {76},
number = {373},
pages = {103--106},
year = {1981},
publisher = {Taylor \& Francis},
doi = {10.1080/01621459.1981.10477613},
URL = { https://www.tandfonline.com/doi/abs/10.1080/01621459.1981.10477613 },
eprint ={https://www.tandfonline.com/doi/pdf/10.1080/01621459.1981.10477613}
}

@article{karniadakis2021physics,
  title={Physics-informed machine learning},
  author={Karniadakis, George Em and Kevrekidis, Ioannis G and Lu, Lu and Perdikaris, Paris and Wang, Sifan and Yang, Liu},
  journal={Nature Reviews Physics},
  volume={3},
  number={6},
  pages={422--440},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{eriksson2019turbo-bo,
  title={Scalable global optimization via local Bayesian optimization},
  author={Eriksson, David and Pearce, Michael and Gardner, Jacob and Turner, Ryan D and Poloczek, Matthias},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}



@misc{osband2018rpn,
      title={Randomized Prior Functions for Deep Reinforcement Learning}, 
      author={Ian Osband and John Aslanides and Albin Cassirer},
      year={2018},
      eprint={1806.03335},
      archivePrefix={arXiv},
      primaryClass={id='stat.ML' full_name='Machine Learning' is_active=True alt_name=None in_archive='stat' is_general=False description='Covers machine learning papers (supervised, unsupervised, semi-supervised learning, graphical models, reinforcement learning, bandits, high dimensional inference, etc.) with a statistical or theoretical grounding'}
}

@article{py-pde,
    Author = {David Zwicker},
    Doi = {10.21105/joss.02158},
    Journal = {Journal of Open Source Software},
    Number = {48},
    Pages = {2158},
    Publisher = {The Open Journal},
    Title = {py-pde: A Python package for solving partial differential equations},
    Url = {https://doi.org/10.21105/joss.02158},
    Volume = {5},
    Year = {2020}
}


@misc{sorokin2021interferobot,
      title={Interferobot: aligning an optical interferometer by a reinforcement learning agent}, 
      author={Dmitry Sorokin and Alexander Ulanov and Ekaterina Sazhina and Alexander Lvovsky},
      year={2021},
      eprint={2006.02252},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}


@incollection{qEI,
  title={Kriging is well-suited to parallelize optimization},
  author={Ginsbourger, David and Le Riche, Rodolphe and Carraro, Laurent},
  booktitle={Computational intelligence in expensive optimization problems},
  pages={131--162},
  publisher={Springer},
  year={2010},
}

@article{
kim2022deep,
title={Deep Learning for Bayesian Optimization of Scientific Problems with High-Dimensional Structure},
author={Samuel Kim and Peter Y Lu and Charlotte Loh and Jamie Smith and Jasper Snoek and Marin Solja{\v{c}}i{\'c}},
journal={Transactions on Machine Learning Research},
year={2022},
url={https://openreview.net/forum?id=tPMQ6Je2rB}
}

@misc{astudillo2019bayesian,
      title={Bayesian Optimization of Composite Functions}, 
      author={Raul Astudillo and Peter I. Frazier},
      year={2019},
      eprint={1906.01537},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}


@article{Pickering2022,
  title = {Discovering and forecasting extreme events via active learning in neural operators},
  volume = {2},
  ISSN = {2662-8457},
  url = {http://dx.doi.org/10.1038/s43588-022-00376-0},
  DOI = {10.1038/s43588-022-00376-0},
  number = {12},
  journal = {Nature Computational Science},
  publisher = {Springer Science and Business Media LLC},
  author = {Pickering,  Ethan and Guth,  Stephen and Karniadakis,  George Em and Sapsis,  Themistoklis P.},
  year = {2022},
  month = dec,
  pages = {823–833}
}

@misc{kirsch2021practical,
      title={A Practical \& Unified Notation for Information-Theoretic Quantities in ML}, 
      author={Andreas Kirsch and Yarin Gal},
      year={2021},
      eprint={2106.12062},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@article{gp_bandits_2009,
  author       = {Niranjan Srinivas and
                  Andreas Krause and
                  Sham M. Kakade and
                  Matthias W. Seeger},
  title        = {Gaussian Process Bandits without Regret: An Experimental Design Approach},
  journal      = {CoRR},
  volume       = {abs/0912.3995},
  year         = {2009},
  url          = {http://arxiv.org/abs/0912.3995},
  eprinttype    = {arXiv},
  eprint       = {0912.3995},
  timestamp    = {Mon, 13 Aug 2018 16:45:57 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-0912-3995.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{li2021_FNO,
      title={Fourier Neural Operator for Parametric Partial Differential Equations}, 
      author={Zongyi Li and Nikola Kovachki and Kamyar Azizzadenesheli and Burigede Liu and Kaushik Bhattacharya and Andrew Stuart and Anima Anandkumar},
      year={2021},
      eprint={2010.08895},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@ARTICLE{2020SciPy-NMeth,
  author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
            Haberland, Matt and Reddy, Tyler and Cournapeau, David and
            Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
            Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
            Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
            Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
            Kern, Robert and Larson, Eric and Carey, C J and
            Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
            {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
            Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
            Harris, Charles R. and Archibald, Anne M. and
            Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
            {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
            Computing in Python}},
  journal = {Nature Methods},
  year    = {2020},
  volume  = {17},
  pages   = {261--272},
  adsurl  = {https://rdcu.be/b08Wh},
  doi     = {10.1038/s41592-019-0686-2},
}


@article{kissas2022_LOCA,
  title={Learning operators with coupled attention},
  author={Kissas, Georgios and Seidman, Jacob H and Guilhoto, Leonardo Ferreira and Preciado, Victor M and Pappas, George J and Perdikaris, Paris},
  journal={The Journal of Machine Learning Research},
  volume={23},
  number={1},
  pages={9636--9698},
  year={2022},
  publisher={JMLRORG}
}

@article{yang2022scalable,
  title={Scalable uncertainty quantification for deep operator networks using randomized priors},
  author={Yang, Yibo and Kissas, Georgios and Perdikaris, Paris},
  journal={Computer Methods in Applied Mechanics and Engineering},
  volume={399},
  pages={115399},
  year={2022},
  publisher={Elsevier}
}

@article{vaswani2017attention-is-all-you-need,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@misc{dao2022flashattentionfastmemoryefficientexact,
      title={FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness}, 
      author={Tri Dao and Daniel Y. Fu and Stefano Ermon and Atri Rudra and Christopher Ré},
      year={2022},
      eprint={2205.14135},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2205.14135}, 
}

@misc{rebain2022attention,
      title={Attention Beats Concatenation for Conditioning Neural Fields}, 
      author={Daniel Rebain and Mark J. Matthews and Kwang Moo Yi and Gopal Sharma and Dmitry Lagun and Andrea Tagliasacchi},
      year={2022},
      eprint={2209.10684},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@book{RasmussenW06_gps4ml,
  added-at = {2020-07-17T00:00:00.000+0200},
  author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  biburl = {https://www.bibsonomy.org/bibtex/2670a576a21065048f7ddede17e09b6b4/dblp},
  ee = {https://www.worldcat.org/oclc/61285753},
  interhash = {72c030472023000e0bdeeb06081c3764},
  intrahash = {670a576a21065048f7ddede17e09b6b4},
  isbn = {026218253X},
  keywords = {dblp},
  pages = {I-XVIII, 1-248},
  publisher = {MIT Press},
  series = {Adaptive computation and machine learning},
  timestamp = {2020-07-24T00:45:17.000+0200},
  title = {Gaussian processes for machine learning.},
  year = 2006
}


@inproceedings{rahaman2019spectral,
  title={On the spectral bias of neural networks},
  author={Rahaman, Nasim and Baratin, Aristide and Arpit, Devansh and Draxler, Felix and Lin, Min and Hamprecht, Fred and Bengio, Yoshua and Courville, Aaron},
  booktitle={International conference on machine learning},
  pages={5301--5310},
  year={2019},
  organization={PMLR}
}

@article{wang2021eigenvector,
  title={On the eigenvector bias of Fourier feature networks: From regression to solving multi-scale PDEs with physics-informed neural networks},
  author={Wang, Sifan and Wang, Hanwen and Perdikaris, Paris},
  journal={Computer Methods in Applied Mechanics and Engineering},
  volume={384},
  pages={113938},
  year={2021},
  publisher={Elsevier}
}

@article{daulton2020differentiable,
  title={Differentiable expected hypervolume improvement for parallel multi-objective Bayesian optimization},
  author={Daulton, Samuel and Balandat, Maximilian and Bakshy, Eytan},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9851--9864},
  year={2020}
}

@article{wang2016parallel,
  title={Parallel Bayesian global optimization of expensive functions},
  author={Wang, Jialei and Clark, Scott C and Liu, Eric and Frazier, Peter I},
  journal={arXiv preprint arXiv:1602.05149},
  year={2016}
}

@article{di2023neural,
  title={Neural operator prediction of linear instability waves in high-speed boundary layers},
  author={Di Leoni, Patricio Clark and Lu, Lu and Meneveau, Charles and Karniadakis, George Em and Zaki, Tamer A},
  journal={Journal of Computational Physics},
  volume={474},
  pages={111793},
  year={2023},
  publisher={Elsevier}
}


@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{tancik2020fourfeat,
    title={Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains},
    author={Matthew Tancik and Pratul P. Srinivasan and Ben Mildenhall and Sara Fridovich-Keil and Nithin Raghavan and Utkarsh Singhal and Ravi Ramamoorthi and Jonathan T. Barron and Ren Ng},
    journal={NeurIPS},
    year={2020}
}

@inproceedings{dreifuerst2021optimizing,
  title={Optimizing coverage and capacity in cellular networks using machine learning},
  author={Dreifuerst, Ryan M and Daulton, Samuel and Qian, Yuchen and Varkey, Paul and Balandat, Maximilian and Kasturia, Sanjay and Tomar, Anoop and Yazdan, Ali and Ponnampalam, Vish and Heath, Robert W},
  booktitle={ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={8138--8142},
  year={2021},
  organization={IEEE}
}


@article{bliznyuk2008bayesian,
  title={Bayesian calibration and uncertainty analysis for computationally expensive models using optimization and radial basis function approximation},
  author={Bliznyuk, Nikolay and Ruppert, David and Shoemaker, Christine and Regis, Rommel and Wild, Stefan and Mugunthan, Pradeep},
  journal={Journal of Computational and Graphical Statistics},
  volume={17},
  number={2},
  pages={270--294},
  year={2008},
  publisher={Taylor \& Francis}
}

@article{maus2023joint,
  title={Joint Composite Latent Space Bayesian Optimization},
  author={Maus, Natalie and Lin, Zhiyuan Jerry and Balandat, Maximilian and Bakshy, Eytan},
  journal={arXiv preprint arXiv:2311.02213},
  year={2023}
}


@inproceedings{maas2013leakyrelu,
  title={Rectifier nonlinearities improve neural network acoustic models},
  author={Maas, Andrew L and Hannun, Awni Y and Ng, Andrew Y and others},
  booktitle={Proc. icml},
  volume={30-1},
  pages={3},
  year={2013},
  organization={Atlanta, GA}
}

@article{ament2024unexpected,
  title={Unexpected improvements to expected improvement for bayesian optimization},
  author={Ament, Sebastian and Daulton, Samuel and Eriksson, David and Balandat, Maximilian and Bakshy, Eytan},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{li2022soft,
  title={Soft actuators for real-world applications},
  author={Li, Meng and Pal, Aniket and Aghakhani, Amirreza and Pena-Francesch, Abdon and Sitti, Metin},
  journal={Nature Reviews Materials},
  volume={7},
  number={3},
  pages={235--249},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@article{shafer2008conformal-prediction,
  title={A tutorial on conformal prediction.},
  author={Shafer, Glenn and Vovk, Vladimir},
  journal={Journal of Machine Learning Research},
  volume={9},
  number={3},
  year={2008}
}

@article{2012vanishinggradient,
  author       = {Razvan Pascanu and
                  Tom{\'{a}}s Mikolov and
                  Yoshua Bengio},
  title        = {Understanding the exploding gradient problem},
  journal      = {CoRR},
  volume       = {abs/1211.5063},
  year         = {2012},
  url          = {http://arxiv.org/abs/1211.5063},
  eprinttype    = {arXiv},
  eprint       = {1211.5063},
  timestamp    = {Mon, 28 Dec 2020 11:31:02 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1211-5063.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@article{liu1989lbfgs,
  title={On the limited memory BFGS method for large scale optimization},
  author={Liu, Dong C and Nocedal, Jorge},
  journal={Mathematical programming},
  volume={45},
  number={1},
  pages={503--528},
  year={1989},
  publisher={Springer}
}


@misc{jin2022mionet,
      title={MIONet: Learning multiple-input operators via tensor product}, 
      author={Pengzhan Jin and Shuai Meng and Lu Lu},
      year={2022},
      eprint={2202.06137},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@ARTICLE{chenchen1995,
  author={Tianping Chen and Hong Chen},
  journal={IEEE Transactions on Neural Networks}, 
  title={Universal approximation to nonlinear operators by neural networks with arbitrary activation functions and its application to dynamical systems}, 
  year={1995},
  volume={6},
  number={4},
  pages={911-917},
  keywords={Neural networks;Nonlinear dynamical systems;Computer networks;Kernel;Sufficient conditions;Polynomials;Integral equations;Mathematics;Sun;H infinity control},
  doi={10.1109/72.392253}}


@article{kovachki2023neural_operator,
  title={Neural operator: Learning maps between function spaces with applications to PDEs},
  author={Kovachki, Nikola and Li, Zongyi and Liu, Burigede and Azizzadenesheli, Kamyar and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={89},
  pages={1--97},
  year={2023}
}


@article{wang2022improved,
  title={Improved architectures and training algorithms for deep operator networks},
  author={Wang, Sifan and Wang, Hanwen and Perdikaris, Paris},
  journal={Journal of Scientific Computing},
  volume={92},
  number={2},
  pages={35},
  year={2022},
  publisher={Springer}
}


@article{wang2021learning,
  title={Learning the solution operator of parametric partial differential equations with physics-informed DeepONets},
  author={Wang, Sifan and Wang, Hanwen and Perdikaris, Paris},
  journal={Science advances},
  volume={7},
  number={40},
  pages={eabi8605},
  year={2021},
  publisher={American Association for the Advancement of Science}
}

@Article{         harris2020numpy,
 title         = {Array programming with {NumPy}},
 author        = {Charles R. Harris and K. Jarrod Millman and St{\'{e}}fan J.
                 van der Walt and Ralf Gommers and Pauli Virtanen and David
                 Cournapeau and Eric Wieser and Julian Taylor and Sebastian
                 Berg and Nathaniel J. Smith and Robert Kern and Matti Picus
                 and Stephan Hoyer and Marten H. van Kerkwijk and Matthew
                 Brett and Allan Haldane and Jaime Fern{\'{a}}ndez del
                 R{\'{i}}o and Mark Wiebe and Pearu Peterson and Pierre
                 G{\'{e}}rard-Marchant and Kevin Sheppard and Tyler Reddy and
                 Warren Weckesser and Hameer Abbasi and Christoph Gohlke and
                 Travis E. Oliphant},
 year          = {2020},
 month         = sep,
 journal       = {Nature},
 volume        = {585},
 number        = {7825},
 pages         = {357--362},
 doi           = {10.1038/s41586-020-2649-2},
 publisher     = {Springer Science and Business Media {LLC}},
 url           = {https://doi.org/10.1038/s41586-020-2649-2}
}

@Article{Hunter2007matplotlib,
  Author    = {Hunter, J. D.},
  Title     = {Matplotlib: A 2D graphics environment},
  Journal   = {Computing in Science \& Engineering},
  Volume    = {9},
  Number    = {3},
  Pages     = {90--95},
  abstract  = {Matplotlib is a 2D graphics package used for Python for
  application development, interactive scripting, and publication-quality
  image generation across user interfaces and operating systems.},
  publisher = {IEEE COMPUTER SOC},
  doi       = {10.1109/MCSE.2007.55},
  year      = 2007
}


@misc{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.3.13},
  year = {2018},
}

@misc{flax2020github,
  author = {Jonathan Heek and Anselm Levskaya and Avital Oliver and Marvin Ritter and Bertrand Rondepierre and Andreas Steiner and Marc van {Z}ee},
  title = {{F}lax: A neural network library and ecosystem for {JAX}},
  url = {http://github.com/google/flax},
  version = {0.8.2},
  year = {2023},
}

@inproceedings{pytorch_2024,
author = {Ansel, Jason and Yang, Edward and He, Horace and Gimelshein, Natalia and Jain, Animesh and Voznesensky, Michael and Bao, Bin and Bell, Peter and Berard, David and Burovski, Evgeni and Chauhan, Geeta and Chourdia, Anjali and Constable, Will and Desmaison, Alban and DeVito, Zachary and Ellison, Elias and Feng, Will and Gong, Jiong and Gschwind, Michael and Hirsh, Brian and Huang, Sherlock and Kalambarkar, Kshiteej and Kirsch, Laurent and Lazos, Michael and Lezcano, Mario and Liang, Yanbo and Liang, Jason and Lu, Yinghai and Luk, CK and Maher, Bert and Pan, Yunjie and Puhrsch, Christian and Reso, Matthias and Saroufim, Mark and Siraichi, Marcos Yukio and Suk, Helen and Suo, Michael and Tillet, Phil and Wang, Eikan and Wang, Xiaodong and Wen, William and Zhang, Shunting and Zhao, Xu and Zhou, Keren and Zou, Richard and Mathews, Ajit and Chanan, Gregory and Wu, Peng and Chintala, Soumith},
booktitle = {29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS '24)},
doi = {10.1145/3620665.3640366},
month = apr,
publisher = {ACM},
title = {{PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation}},
url = {https://pytorch.org/assets/pytorch2-2.pdf},
year = {2024}
}

@misc{tensorflow_2015,
author = {Abadi, Martín and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jozefowicz, Rafal and Jia, Yangqing and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mané, Dan and Schuster, Mike and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viégas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
doi = {10.5281/zenodo.4724125},
license = {Apache-2.0},
month = nov,
title = {{TensorFlow, Large-scale machine learning on heterogeneous systems}},
year = {2015}
}

@misc{li2023multiresolution,
      title={Multi-Resolution Active Learning of Fourier Neural Operators}, 
      author={Shibo Li and Xin Yu and Wei Xing and Mike Kirby and Akil Narayan and Shandian Zhe},
      year={2023},
      eprint={2309.16971},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{wang2022boreview,
      title={Recent Advances in Bayesian Optimization}, 
      author={Xilu Wang and Yaochu Jin and Sebastian Schmitt and Markus Olhofer},
      year={2022},
      eprint={2206.03301},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{botorch,
  author       = {Maximilian Balandat and
                  Brian Karrer and
                  Daniel R. Jiang and
                  Samuel Daulton and
                  Benjamin Letham and
                  Andrew Gordon Wilson and
                  Eytan Bakshy},
  title        = {BoTorch: Programmable Bayesian Optimization in PyTorch},
  journal      = {CoRR},
  volume       = {abs/1910.06403},
  year         = {2019},
  url          = {http://arxiv.org/abs/1910.06403},
  eprinttype    = {arXiv},
  eprint       = {1910.06403},
  timestamp    = {Wed, 16 Oct 2019 16:25:53 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1910-06403.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{ganaie2022deep-ensemble-review,
  title={Ensemble deep learning: A review},
  author={Ganaie, Mudasir A and Hu, Minghui and Malik, Ashwani Kumar and Tanveer, Muhammad and Suganthan, Ponnuthurai N},
  journal={Engineering Applications of Artificial Intelligence},
  volume={115},
  pages={105151},
  year={2022},
  publisher={Elsevier}
}

@article{abdar2021-deep-uq-review,
  title={A review of uncertainty quantification in deep learning: Techniques, applications and challenges},
  author={Abdar, Moloud and Pourpanah, Farhad and Hussain, Sadiq and Rezazadegan, Dana and Liu, Li and Ghavamzadeh, Mohammad and Fieguth, Paul and Cao, Xiaochun and Khosravi, Abbas and Acharya, U Rajendra and others},
  journal={Information fusion},
  volume={76},
  pages={243--297},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{valdenegro2022deeper,
  title={A deeper look into aleatoric and epistemic uncertainty disentanglement},
  author={Valdenegro-Toro, Matias and Mori, Daniel Saromo},
  booktitle={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  pages={1508--1516},
  year={2022},
  organization={IEEE}
}

@inproceedings{Seitzer2022PitfallsOfUncertainty,
  title = {On the Pitfalls of Heteroscedastic Uncertainty Estimation with Probabilistic Neural Networks},
  author = {Seitzer, Maximilian and Tavakoli, Arash and Antic, Dimitrije and Martius, Georg},
  booktitle = {International Conference on Learning Representations},
  month = apr,
  year = {2022},
  url = {https://openreview.net/forum?id=aPOpXlnV1T},
  month_numeric = {4}
}

@misc{nvidia-2021-mcmc-bnn,
  author={NVIDIA},
  title={Bayesian neural network (BNN) training using MCMC},
  year={2021},
  url={https://github.com/NVIDIA/mcmc-bnn-example},
}

@article{cybenko1989approximation,
  title={Approximation by superpositions of a sigmoidal function},
  author={Cybenko, George},
  journal={Mathematics of control, signals and systems},
  volume={2},
  number={4},
  pages={303--314},
  year={1989},
  publisher={Springer}
}

@article{kuurkova1992kolmogorov,
  title={Kolmogorov's theorem and multilayer neural networks},
  author={K{\u u}rkov\'a, V{\v e}ra},
  journal={Neural networks},
  volume={5},
  number={3},
  pages={501--506},
  year={1992},
  publisher={Elsevier}
}

@article{kononenko1989bayesian-nn,
  title={Bayesian neural networks},
  author={Kononenko, Igor},
  journal={Biological Cybernetics},
  volume={61},
  number={5},
  pages={361--370},
  year={1989},
  publisher={Springer}
}

@inproceedings{blundell2015weight-uncertainty-in-nn,
  title={Weight uncertainty in neural network},
  author={Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
  booktitle={International conference on machine learning},
  pages={1613--1622},
  year={2015},
  organization={PMLR}
}

@misc{fort2020deepensembleslosslandscape,
      title={Deep Ensembles: A Loss Landscape Perspective}, 
      author={Stanislav Fort and Huiyi Hu and Balaji Lakshminarayanan},
      year={2020},
      eprint={1912.02757},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1912.02757}, 
}
@article{baysean-calibration-2008,
 ISSN = {10618600},
 URL = {http://www.jstor.org/stable/27594307},
 abstract = {We present a Bayesian approach to model calibration when evaluation of the model is computationally expensive. Here, calibration is a nonlinear regression problem: given a data vector Y corresponding to the regression model f(β), find plausible values of β. As an intermediate step, Y and f are embedded into a statistical model allowing transformation and dependence. Typically, this problem is solved by sampling from the posterior distribution of β given Y using MCMC. To reduce computational cost, we limit evaluation of f to a small number of points chosen on a high posterior density region found by optimization. Then, we approximate the logarithm of the posterior density using radial basis functions and use the resulting cheap-to-evaluate surface in MCMC. We illustrate our approach on simulated data for a pollutant diffusion problem and study the frequentist coverage properties of credible intervals. Our experiments indicate that our method can produce results similar to those when the true "expensive" posterior density is sampled by MCMC while reducing computational costs by well over an order of magnitude.},
 author = {Nikolay Bliznyuk and David Ruppert and Christine Shoemaker and Rommel Regis and Stefan Wild and Pradeep Mugunthan},
 journal = {Journal of Computational and Graphical Statistics},
 number = {2},
 pages = {270--294},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd., Institute of Mathematical Statistics, Interface Foundation of America]},
 title = {Bayesian Calibration and Uncertainty Analysis for Computationally Expensive Models Using Optimization and Radial Basis Function Approximation},
 urldate = {2023-12-18},
 volume = {17},
 year = {2008}
}


@article{Sukumar2022exact-bdry-pinns,
   title={Exact imposition of boundary conditions with distance functions in physics-informed deep neural networks},
   volume={389},
   ISSN={0045-7825},
   url={http://dx.doi.org/10.1016/j.cma.2021.114333},
   DOI={10.1016/j.cma.2021.114333},
   journal={Computer Methods in Applied Mechanics and Engineering},
   publisher={Elsevier BV},
   author={Sukumar, N. and Srivastava, Ankit},
   year={2022},
   month=feb, pages={114333} }

@misc{xie2022neural-fields,
      title={Neural Fields in Visual Computing and Beyond}, 
      author={Yiheng Xie and Towaki Takikawa and Shunsuke Saito and Or Litany and Shiqin Yan and Numair Khan and Federico Tombari and James Tompkin and Vincent Sitzmann and Srinath Sridhar},
      year={2022},
      eprint={2111.11426},
      archivePrefix={arXiv},
      primaryClass={id='cs.CV' full_name='Computer Vision and Pattern Recognition' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers image processing, computer vision, pattern recognition, and scene understanding. Roughly includes material in ACM Subject Classes I.2.10, I.4, and I.5.'}
}
@misc{mildenhall2020nerf,
      title={NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis}, 
      author={Ben Mildenhall and Pratul P. Srinivasan and Matthew Tancik and Jonathan T. Barron and Ravi Ramamoorthi and Ren Ng},
      year={2020},
      eprint={2003.08934},
      archivePrefix={arXiv},
      primaryClass={id='cs.CV' full_name='Computer Vision and Pattern Recognition' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers image processing, computer vision, pattern recognition, and scene understanding. Roughly includes material in ACM Subject Classes I.2.10, I.4, and I.5.'}
}

@article{wang2024bridging,
  title={Bridging Operator Learning and Conditioned Neural Fields: A Unifying Perspective},
  author={Wang, Sifan and Seidman, Jacob H and Sankaran, Shyam and Wang, Hanwen and Pappas, George J and Perdikaris, Paris},
  journal={arXiv preprint arXiv:2405.13998},
  year={2024}
}

@article{Lu_2021_deeponet,
	doi = {10.1038/s42256-021-00302-5},
  
	url = {https://doi.org/10.1038%2Fs42256-021-00302-5},
  
	year = 2021,
	month = {mar},
  
	publisher = {Springer Science and Business Media {LLC}},
  
	volume = {3},
  
	number = {3},
  
	pages = {218--229},
  
	author = {Lu Lu and Pengzhan Jin and Guofei Pang and Zhongqiang Zhang and George Em Karniadakis},
  
	title = {Learning nonlinear operators via {DeepONet} based on the universal approximation theorem of operators},
  
	journal = {Nature Machine Intelligence}
}

@article{fourier-deeponet-2023,
title = {Fourier-DeepONet: Fourier-enhanced deep operator networks for full waveform inversion with improved accuracy, generalizability, and robustness},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {416},
pages = {116300},
year = {2023},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2023.116300},
url = {https://www.sciencedirect.com/science/article/pii/S0045782523004243},
author = {Min Zhu and Shihang Feng and Youzuo Lin and Lu Lu},
keywords = {Full waveform inversion, Deep learning, Fourier-DeepONet, Benchmark datasets, Generalizability, Robustness},
abstract = {Full waveform inversion (FWI) infers the subsurface structure information from seismic waveform data by solving a non-convex optimization problem. Data-driven FWI has been increasingly studied with various neural network architectures to improve accuracy and computational efficiency. Nevertheless, the applicability of pre-trained neural networks is severely restricted by potential discrepancies between the source function used in the field survey and the one utilized during training. Here, we develop a Fourier-enhanced deep operator network (Fourier-DeepONet) for FWI with the generalization of seismic sources, including the frequencies and locations of sources. Specifically, we employ the Fourier neural operator as the decoder of DeepONet, and we utilize source parameters as one input of Fourier-DeepONet, facilitating the resolution of FWI with variable sources. To test Fourier-DeepONet, we develop three new and realistic FWI benchmark datasets (FWI-F, FWI-L, and FWI-FL) with varying source frequencies, locations, or both. Our experiments demonstrate that compared with existing data-driven FWI methods, Fourier-DeepONet obtains more accurate predictions of subsurface structures in a wide range of source parameters. Moreover, the proposed Fourier-DeepONet exhibits superior robustness when handling data with Gaussian noise or missing traces and sources with Gaussian noise, paving the way for more reliable and accurate subsurface imaging across diverse real conditions.}
}

@article{LIN2023-b-deeponet,
title = {B-DeepONet: An enhanced Bayesian DeepONet for solving noisy parametric PDEs using accelerated replica exchange SGLD},
journal = {Journal of Computational Physics},
volume = {473},
pages = {111713},
year = {2023},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2022.111713},
url = {https://www.sciencedirect.com/science/article/pii/S0021999122007768},
author = {Guang Lin and Christian Moya and Zecheng Zhang},
keywords = {Parametric PDE, Deep operator network, Noisy data, Bayesian method, Langevin diffusion, Replica exchange},
abstract = {The Deep Operator Network (DeepONet) is a neural network architecture used to approximate operators, including the solution operator of parametric PDEs. DeepONets have shown remarkable approximation ability. However, the performance of DeepONets deteriorates when the training data is polluted with noise, a scenario that occurs in practice. To handle noisy data, we propose a Bayesian DeepONet based on replica exchange Langevin diffusion (reLD). Replica exchange uses two particles. The first particle trains a DeepONet to exploit the loss landscape and make predictions. The other particle trains a different DeepONet to explore the loss landscape and escape local minima via swapping. Compared to DeepONets trained with state-of-the-art gradient-based algorithms (e.g., Adam), the proposed Bayesian DeepONet greatly improves the training convergence for noisy scenarios and accurately estimates the uncertainty. To further reduce the high computational cost of the reLD training of DeepONets, we propose (1) an accelerated training framework that exploits the DeepONet's architecture to reduce its computational cost up to 25% without compromising performance and (2) a transfer learning strategy that accelerates training DeepONets for PDEs with different parameter values. Finally, we illustrate the effectiveness of the proposed Bayesian DeepONet using four parametric PDE problems.}
}

@article{fno-deformed-2023,
  author  = {Zongyi Li and Daniel Zhengyu Huang and Burigede Liu and Anima Anandkumar},
  title   = {Fourier Neural Operator with Learned Deformations for PDEs on General Geometries},
  journal = {Journal of Machine Learning Research},
  year    = {2023},
  volume  = {24},
  number  = {388},
  pages   = {1--26},
  url     = {http://jmlr.org/papers/v24/23-0064.html}
}
@article{wen2022u-fno,
  title={U-FNO—An enhanced Fourier neural operator-based deep-learning model for multiphase flow},
  author={Wen, Gege and Li, Zongyi and Azizzadenesheli, Kamyar and Anandkumar, Anima and Benson, Sally M},
  journal={Advances in Water Resources},
  volume={163},
  pages={104180},
  year={2022},
  publisher={Elsevier}
}
@article{model-parallel-fno-2023,
title = {Model-parallel Fourier neural operators as learned surrogates for large-scale parametric PDEs},
journal = {Computers \& Geosciences},
volume = {178},
pages = {105402},
year = {2023},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2023.105402},
url = {https://www.sciencedirect.com/science/article/pii/S0098300423001061},
author = {Thomas J. Grady and Rishi Khan and Mathias Louboutin and Ziyi Yin and Philipp A. Witte and Ranveer Chandra and Russell J. Hewett and Felix J. Herrmann},
keywords = {Operator learning, Deep learning, Learned surrogates, Model parallelism, Multiphase flow, Carbon capture sequestration},
abstract = {Fourier neural operators (FNOs) are a recently introduced neural network architecture for learning solution operators of partial differential equations (PDEs), which have been shown to perform significantly better than comparable deep learning approaches. Once trained, FNOs can achieve speed-ups of multiple orders of magnitude over conventional numerical PDE solvers. However, due to the high dimensionality of their input data and network weights, FNOs have so far only been applied to two-dimensional or small three-dimensional problems. To remove this limited problem-size barrier, we propose a model-parallel version of FNOs based on domain-decomposition of both the input data and network weights. We demonstrate that our model-parallel FNO is able to predict time-varying PDE solutions of over 2.6 billion variables on Perlmutter using up to 512 A100 GPUs and show an example of training a distributed FNO on the Azure cloud for simulating multiphase CO2 dynamics in the Earth’s subsurface.}
}

@misc{kingma2017adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{gelu,
      title={Gaussian Error Linear Units (GELUs)}, 
      author={Dan Hendrycks and Kevin Gimpel},
      year={2020},
      eprint={1606.08415},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}



@misc{seidman2022nomad,
      title={NOMAD: Nonlinear Manifold Decoders for Operator Learning}, 
      author={Jacob H. Seidman and Georgios Kissas and Paris Perdikaris and George J. Pappas},
      year={2022},
      eprint={2206.03551},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{smith2023predictionoriented,
      title={Prediction-Oriented Bayesian Active Learning}, 
      author={Freddie Bickford Smith and Andreas Kirsch and Sebastian Farquhar and Yarin Gal and Adam Foster and Tom Rainforth},
      year={2023},
      eprint={2304.08151},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{wilson2018maximizing,
      title={Maximizing acquisition functions for Bayesian optimization}, 
      author={James T. Wilson and Frank Hutter and Marc Peter Deisenroth},
      year={2018},
      eprint={1805.10196},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{gonzález2015batch_penalizer,
      title={Batch Bayesian Optimization via Local Penalization}, 
      author={Javier González and Zhenwen Dai and Philipp Hennig and Neil D. Lawrence},
      year={2015},
      eprint={1505.08052},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}




@techreport{settles2009active,
  added-at = {2011-03-25T11:05:49.000+0100},
  author = {Settles, Burr},
  biburl = {https://www.bibsonomy.org/bibtex/211a6f820b613ae8cacc5cccfe41f6b38/beate},
  institution = {University of Wisconsin--Madison},
  interhash = {d21ffc0eaffcf51e86e81779fe2b22c2},
  intrahash = {11a6f820b613ae8cacc5cccfe41f6b38},
  keywords = {active-learning literature-review spam-detection survey},
  number = 1648,
  timestamp = {2011-03-25T11:05:49.000+0100},
  title = {Active Learning Literature Survey},
  type = {Computer Sciences Technical Report},
  url = {http://axon.cs.byu.edu/~martinez/classes/778/Papers/settles.activelearning.pdf},
  year = 2009
}



@book{evans1991measure,
  title={Measure Theory and Fine Properties of Functions},
  author={Evans, L.C. and Gariepy, R.F.},
  isbn={9780849371578},
  lccn={91030277},
  series={Studies in Advanced Mathematics},
  url={https://books.google.com/books?id=HZONacVPGlMC},
  year={1991},
  publisher={Taylor \& Francis}
}


@misc{filos2019systematic,
      title={A Systematic Comparison of Bayesian Deep Learning Robustness in Diabetic Retinopathy Tasks}, 
      author={Angelos Filos and Sebastian Farquhar and Aidan N. Gomez and Tim G. J. Rudner and Zachary Kenton and Lewis Smith and Milad Alizadeh and Arnoud de Kroon and Yarin Gal},
      year={2019},
      eprint={1912.10481},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}


@article{Esteva2017dermatologist,
  title = {Dermatologist-level classification of skin cancer with deep neural networks},
  volume = {542},
  ISSN = {1476-4687},
  url = {http://dx.doi.org/10.1038/nature21056},
  DOI = {10.1038/nature21056},
  number = {7639},
  journal = {Nature},
  publisher = {Springer Science and Business Media LLC},
  author = {Esteva,  Andre and Kuprel,  Brett and Novoa,  Roberto A. and Ko,  Justin and Swetter,  Susan M. and Blau,  Helen M. and Thrun,  Sebastian},
  year = {2017},
  month = jan,
  pages = {115–118}
}


@misc{huang2020autonomous,
      title={Autonomous Driving with Deep Learning: A Survey of State-of-Art Technologies}, 
      author={Yu Huang and Yue Chen},
      year={2020},
      eprint={2006.06091},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@inproceedings{2011-high-dim-noisy-regression,
 author = {Loh, Po-ling and Wainwright, Martin J},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity},
 url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/ab541d874c7bc19ab77642849e02b89f-Paper.pdf},
 volume = {24},
 year = {2011}
}

@article{muthukumar2020harmless-noisy-regression,
  title={Harmless interpolation of noisy data in regression},
  author={Muthukumar, Vidya and Vodrahalli, Kailas and Subramanian, Vignesh and Sahai, Anant},
  journal={IEEE Journal on Selected Areas in Information Theory},
  volume={1},
  number={1},
  pages={67--83},
  year={2020},
  publisher={IEEE}
}

@article{psaros2023uncertainty-in-sciml,
  title={Uncertainty quantification in scientific machine learning: Methods, metrics, and comparisons},
  author={Psaros, Apostolos F and Meng, Xuhui and Zou, Zongren and Guo, Ling and Karniadakis, George Em},
  journal={Journal of Computational Physics},
  volume={477},
  pages={111902},
  year={2023},
  publisher={Elsevier}
}

@book{murphy2023probabilistic,
  title={Probabilistic machine learning: Advanced topics},
  author={Murphy, Kevin P},
  year={2023},
  publisher={MIT press}
}


@INPROCEEDINGS{huber2008,
  author={Huber, Marco F. and Bailey, Tim and Durrant-Whyte, Hugh and Hanebeck, Uwe D.},
  booktitle={2008 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems}, 
  title={On entropy approximation for Gaussian mixture random vectors}, 
  year={2008},
  volume={},
  number={},
  pages={181-188},
  doi={10.1109/MFI.2008.4648062}}

@incollection{bnn-survey-2020,
	doi = {10.1007/978-3-030-42553-1_3},
  
	url = {https://doi.org/10.1007%2F978-3-030-42553-1_3},
  
	year = 2020,
	publisher = {Springer International Publishing},
  
	pages = {45--87},
  
	author = {Ethan Goan and Clinton Fookes},
  
	title = {Bayesian Neural Networks: An Introduction and Survey},
  
	booktitle = {Case Studies in Applied Bayesian Data Science}
}

@article{wang2019aleatoric,
  title={Aleatoric uncertainty estimation with test-time augmentation for medical image segmentation with convolutional neural networks},
  author={Wang, Guotai and Li, Wenqi and Aertsen, Michael and Deprest, Jan and Ourselin, S{\'e}bastien and Vercauteren, Tom},
  journal={Neurocomputing},
  volume={338},
  pages={34--45},
  year={2019},
  publisher={Elsevier}
}

@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}

@misc{gal2016dropout,
      title={Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning}, 
      author={Yarin Gal and Zoubin Ghahramani},
      year={2016},
      eprint={1506.02142},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{maddox2021bayesian,
  title={Bayesian optimization with high-dimensional outputs},
  author={Maddox, Wesley J and Balandat, Maximilian and Wilson, Andrew G and Bakshy, Eytan},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={19274--19287},
  year={2021}
}

@article{neural-testbed-2021,
  author    = {Ian Osband and
               Zheng Wen and
               Seyed Mohammad Asghari and
               Vikranth Dwaracherla and
               Botao Hao and
               Morteza Ibrahimi and
               Dieterich Lawson and
               Xiuyuan Lu and
               Brendan O'Donoghue and
               Benjamin Van Roy},
  title     = {Evaluating Predictive Distributions: Does Bayesian Deep Learning Work?},
  journal   = {CoRR},
  volume    = {abs/2110.04629},
  year      = {2021},
  url       = {https://arxiv.org/abs/2110.04629},
  eprinttype = {arXiv},
  eprint    = {2110.04629},
  timestamp = {Thu, 21 Oct 2021 16:20:08 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2110-04629.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{BHOURI2023116428,
title = {Scalable Bayesian optimization with randomized prior networks},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {417},
pages = {116428},
year = {2023},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2023.116428},
url = {https://www.sciencedirect.com/science/article/pii/S0045782523005522},
author = {Mohamed Aziz Bhouri and Michael Joly and Robert Yu and Soumalya Sarkar and Paris Perdikaris},
keywords = {Constrained Bayesian optimization, Multi-fidelity Bayesian optimization, Monte Carlo approximation, Parallel acquisition functions, Sequential decision making, Deep ensembles},
abstract = {Several fundamental problems in science and engineering consist of global optimization tasks involving unknown high-dimensional (black-box) functions that map a set of controllable variables to the outcomes of an expensive experiment. Bayesian Optimization (BO) techniques are known to be effective in tackling global optimization problems using a relatively small number objective function evaluations, but their performance suffers when dealing with high-dimensional outputs. To overcome the major challenge of dimensionality, here we propose a deep learning framework for BO and sequential decision making based on bootstrapped ensembles of neural architectures with randomized priors. Using appropriate architecture choices, we show that the proposed framework can approximate functional relationships between design variables and quantities of interest, even in cases where the latter take values in high-dimensional vector spaces or even infinite-dimensional function spaces. In the context of BO, we augmented the proposed probabilistic surrogates with re-parameterized Monte Carlo approximations of multiple-point (parallel) acquisition functions, as well as methodological extensions for accommodating black-box constraints and multi-fidelity information sources. We test the proposed framework against state-of-the-art methods for BO and demonstrate superior performance across several challenging tasks with high-dimensional outputs, including a constrained multi-fidelity optimization task involving shape optimization of rotor blades in turbo-machinery.}
}


@misc{deep-ensambles-2016,
  doi = {10.48550/ARXIV.1612.01474},
  
  url = {https://arxiv.org/abs/1612.01474},
  
  author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@misc{batchbald-2019,
  doi = {10.48550/ARXIV.1906.08158},
  
  url = {https://arxiv.org/abs/1906.08158},
  
  author = {Kirsch, Andreas and van Amersfoort, Joost and Gal, Yarin},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {BatchBALD: Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{og_pinns-1994,
author = {Dissanayake, M. W. M. G. and Phan-Thien, N.},
title = {Neural-network-based approximations for solving partial differential equations},
journal = {Communications in Numerical Methods in Engineering},
volume = {10},
number = {3},
pages = {195-201},
doi = {https://doi.org/10.1002/cnm.1640100303},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cnm.1640100303},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cnm.1640100303},
abstract = {Abstract A numerical method, based on neural-network-based functions, for solving partial differential equations is reported in the paper. Using a ‘universal approximator’ based on a neural network and point collocation, the numerical problem of solving the partial differential equation is transformed to an unconstrained minimization problem. The method is extremely easy to implement and is suitable for obtaining an approximate solution in a short period of time. The technique is illustrated with the aid of two numerical examples.},
year = {1994}
}

@article{raissi_pinns-2019,
title = {Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
journal = {Journal of Computational Physics},
volume = {378},
pages = {686-707},
year = {2019},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2018.10.045},
url = {https://www.sciencedirect.com/science/article/pii/S0021999118307125},
author = {M. Raissi and P. Perdikaris and G.E. Karniadakis},
keywords = {Data-driven scientific computing, Machine learning, Predictive modeling, Runge–Kutta methods, Nonlinear dynamics},
abstract = {We introduce physics-informed neural networks – neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge–Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction–diffusion systems, and the propagation of nonlinear shallow-water waves.}
}

@article{krishnapriyan2021characterizing,
  title={Characterizing possible failure modes in physics-informed neural networks},
  author={Krishnapriyan, Aditi and Gholami, Amir and Zhe, Shandian and Kirby, Robert and Mahoney, Michael W},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={26548--26560},
  year={2021}
}

@article{daw2022rethinking,
  title={Rethinking the importance of sampling in physics-informed neural networks},
  author={Daw, Arka and Bu, Jie and Wang, Sifan and Perdikaris, Paris and Karpatne, Anuj},
  journal={arXiv preprint arXiv:2207.02338},
  year={2022}
}


@article{wang2023expert-pinns,
  title={An expert's guide to training physics-informed neural networks},
  author={Wang, Sifan and Sankaran, Shyam and Wang, Hanwen and Perdikaris, Paris},
  journal={arXiv preprint arXiv:2308.08468},
  year={2023}
}

@article{wang2024piratenets,
  title={PirateNets: Physics-informed Deep Learning with Residual Adaptive Networks},
  author={Wang, Sifan and Li, Bowen and Chen, Yuhan and Perdikaris, Paris},
  journal={arXiv preprint arXiv:2402.00326},
  year={2024}
}

@article{wang2024respecting,
  title={Respecting causality for training physics-informed neural networks},
  author={Wang, Sifan and Sankaran, Shyam and Perdikaris, Paris},
  journal={Computer Methods in Applied Mechanics and Engineering},
  volume={421},
  pages={116813},
  year={2024},
  publisher={Elsevier}
}


@article{wang-pinn-ntk-2022,
title = {When and why PINNs fail to train: A neural tangent kernel perspective},
journal = {Journal of Computational Physics},
volume = {449},
pages = {110768},
year = {2022},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2021.110768},
url = {https://www.sciencedirect.com/science/article/pii/S002199912100663X},
author = {Sifan Wang and Xinling Yu and Paris Perdikaris},
keywords = {Physics-informed neural networks, Spectral bias, Multi-task learning, Gradient descent, Scientific machine learning},
abstract = {Physics-informed neural networks (PINNs) have lately received great attention thanks to their flexibility in tackling a wide range of forward and inverse problems involving partial differential equations. However, despite their noticeable empirical success, little is known about how such constrained neural networks behave during their training via gradient descent. More importantly, even less is known about why such models sometimes fail to train at all. In this work, we aim to investigate these questions through the lens of the Neural Tangent Kernel (NTK); a kernel that captures the behavior of fully-connected neural networks in the infinite width limit during training via gradient descent. Specifically, we derive the NTK of PINNs and prove that, under appropriate conditions, it converges to a deterministic kernel that stays constant during training in the infinite-width limit. This allows us to analyze the training dynamics of PINNs through the lens of their limiting NTK and find a remarkable discrepancy in the convergence rate of the different loss components contributing to the total training error. To address this fundamental pathology, we propose a novel gradient descent algorithm that utilizes the eigenvalues of the NTK to adaptively calibrate the convergence rate of the total training error. Finally, we perform a series of numerical experiments to verify the correctness of our theory and the practical effectiveness of the proposed algorithms. The data and code accompanying this manuscript are publicly available at https://github.com/PredictiveIntelligenceLab/PINNsNTK.}
}

@inproceedings{
sankaran2022pinn-batch-size,
title={On the impact of larger batch size in the training of Physics Informed Neural Networks},
author={Shyam Sankaran and Hanwen Wang and Leonardo Ferreira Guilhoto and Paris Perdikaris},
booktitle={The Symbiosis of Deep Learning and Differential Equations II},
year={2022},
url={https://openreview.net/forum?id=THCvohg1RV}
}

@article{li2021pinn-neural-operator,
  title={Physics-informed neural operator for learning partial differential equations},
  author={Li, Zongyi and Zheng, Hongkai and Kovachki, Nikola and Jin, David and Chen, Haoxuan and Liu, Burigede and Azizzadenesheli, Kamyar and Anandkumar, Anima},
  journal={ACM/JMS Journal of Data Science},
  year={2021},
  publisher={ACM New York, NY}
}

@article{lu2022epinet-robustness,
  title={Robustness of epinets against distributional shifts},
  author={Lu, Xiuyuan and Osband, Ian and Asghari, Seyed Mohammad and Gowal, Sven and Dwaracherla, Vikranth and Wen, Zheng and Van Roy, Benjamin},
  journal={arXiv preprint arXiv:2207.00137},
  year={2022}
}

@misc{guilhoto2024neon,
      title={Composite Bayesian Optimization In Function Spaces Using NEON -- Neural Epistemic Operator Networks}, 
      author={Leonardo Ferreira Guilhoto and Paris Perdikaris},
      year={2024},
      eprint={2404.03099},
      archivePrefix={arXiv},
      primaryClass={id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'}
}

@misc{stochastic-batch-2021,
  doi = {10.48550/ARXIV.2106.12059},
  
  url = {https://arxiv.org/abs/2106.12059},
  
  author = {Kirsch, Andreas and Farquhar, Sebastian and Atighehchian, Parmida and Jesson, Andrew and Branchaud-Charron, Frederic and Gal, Yarin},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Stochastic Batch Acquisition for Deep Active Learning},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@misc{BALD-2011,
  doi = {10.48550/ARXIV.1112.5745},
  
  url = {https://arxiv.org/abs/1112.5745},
  
  author = {Houlsby, Neil and Huszár, Ferenc and Ghahramani, Zoubin and Lengyel, Máté},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Bayesian Active Learning for Classification and Preference Learning},
  
  publisher = {arXiv},
  
  year = {2011},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{sitzmann2020siren,
  author    = {Vincent Sitzmann and
               Julien N. P. Martel and
               Alexander W. Bergman and
               David B. Lindell and
               Gordon Wetzstein},
  title     = {Implicit Neural Representations with Periodic Activation Functions},
  journal   = {CoRR},
  volume    = {abs/2006.09661},
  year      = {2020},
  url       = {https://arxiv.org/abs/2006.09661},
  eprinttype = {arXiv},
  eprint    = {2006.09661},
  timestamp = {Tue, 23 Jun 2020 17:57:22 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2006-09661.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@article{adaptive_grad_clip-2021,
  author       = {Andrew Brock and
                  Soham De and
                  Samuel L. Smith and
                  Karen Simonyan},
  title        = {High-Performance Large-Scale Image Recognition Without Normalization},
  journal      = {CoRR},
  volume       = {abs/2102.06171},
  year         = {2021},
  url          = {https://arxiv.org/abs/2102.06171},
  eprinttype    = {arXiv},
  eprint       = {2102.06171},
  timestamp    = {Thu, 18 Feb 2021 15:26:00 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2102-06171.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@misc{he2024uq-survey,
      title={A Comprehensive Survey on Uncertainty Quantification for Deep Learning}, 
      author={Wenchong He and Zhe Jiang},
      year={2024},
      eprint={2302.13425},
      archivePrefix={arXiv},
      primaryClass={id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'}
}


@article{guilhoto2017mcmc,
  title={Applying markov chains to monte carlo integration},
  author={Guilhoto, Leonardo Ferreira},
  journal={The University of Chicago Research Experience for Undergraduates},
  year={2017}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Kolmogorov papers %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{hilbert2000mathematical,
  title={Mathematical problems},
  author={Hilbert, David},
  journal={Bulletin of the American Mathematical Society},
  volume={37},
  number={4},
  pages={407--436},
  year={2000}
}

@book{kolmogorov1961representation,
  title={On the representation of continuous functions of several variables by superpositions of continuous functions of a smaller number of variables},
  author={Kolmogorov, Andrei Nikolaevich},
  year={1961},
  publisher={American Mathematical Society}
}

@inproceedings{vitushkin1964proof,
  title={A proof of the existence of analytic functions of several variables not representable by linear superpositions of continuously differentiable functions of fewer variables},
  author={Vitushkin, Anatoliy Georgievich},
  booktitle={Dokl. Akad. Nauk SSSR},
  volume={156-1258-1261},
  pages={3},
  year={1964}
}

@article{sprecher1965structure_whattodo,
  title={On the structure of continuous functions of several variables},
  author={Sprecher, David A},
  journal={Transactions of the American Mathematical Society},
  volume={115},
  pages={340--355},
  year={1965}
}

@article{sprecher1996numerical_remove,
  title={A numerical implementation of Kolmogorov's superpositions},
  author={Sprecher, David A},
  journal={Neural networks},
  volume={9},
  number={5},
  pages={765--772},
  year={1996},
  publisher={Elsevier}
}

@incollection{sprecher2013new-computational-algorithm,
  title={Kolmogorov superpositions: A new computational algorithm},
  author={Sprecher, David},
  booktitle={Efficiency and scalability methods for computational intellect},
  pages={219--245},
  year={2013},
  publisher={IGI Global}
}

@article{braun2009constructive_remove2,
  title={On a constructive proof of Kolmogorov’s superposition theorem},
  author={Braun, J{\"u}rgen and Griebel, Michael},
  journal={Constructive approximation},
  volume={30},
  pages={653--675},
  year={2009},
  publisher={Springer}
}

@book{sprecher2017kolmogorov-survey-book,
  title={From Algebra to Computational Algorithms: Kolmogorov and Hilbert's Problem 13},
  author={Sprecher, David A},
  year={2017},
  publisher={Docent Press}
}

@inproceedings{hecht1987kolmogorov,
  title={Kolmogorov’s mapping neural network existence theorem},
  author={Hecht-Nielsen, Robert},
  booktitle={Proceedings of the international conference on Neural Networks},
  volume={3},
  pages={11--14},
  year={1987},
  organization={IEEE press New York, NY, USA}
}

@article{laczkovich2021superposition-remove,
title = {A superposition theorem of Kolmogorov type for bounded continuous functions},
journal = {Journal of Approximation Theory},
volume = {269},
pages = {105609},
year = {2021},
issn = {0021-9045},
doi = {https://doi.org/10.1016/j.jat.2021.105609},
url = {https://www.sciencedirect.com/science/article/pii/S0021904521000721},
author = {Miklós Laczkovich},
keywords = {Kolmogorov superposition theorem},
abstract = {Let C(Rn) denote the set of real valued continuous functions defined on Rn. We prove that for every n≥2 there are positive numbers λ1,…,λn and continuous functions ϕ1,…,ϕm∈C(R) with the following property: for every bounded and continuous f∈C(Rn) there is a continuous function g∈C(R) such that f(x)=∑q=1mg∑p=1nλpϕq(xp) for every x=(x1,…,xn)∈Rn. Consequently, every f∈C(Rn) can be obtained from continuous functions of one variable using compositions and additions.}
}

@article{kolmorogov_network-1993,
    author = {Lin, Ji-Nan and Unbehauen, Rolf},
    title = "{On the Realization of a Kolmogorov Network}",
    journal = {Neural Computation},
    volume = {5},
    number = {1},
    pages = {18-20},
    year = {1993},
    month = {01},
    issn = {0899-7667},
    doi = {10.1162/neco.1993.5.1.18},
    url = {https://doi.org/10.1162/neco.1993.5.1.18},
    eprint = {https://direct.mit.edu/neco/article-pdf/5/1/18/812463/neco.1993.5.1.18.pdf},
}

@InProceedings{training-kolmogorov-2022,
author="K{\"o}ppen, Mario",
editor="Dorronsoro, Jos{\'e} R.",
title="On the Training of a Kolmogorov Network",
booktitle="Artificial Neural Networks --- ICANN 2002",
year="2002",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="474--479",
abstract="The Kolmogorov theorem gives that the representation of continuous and bounded real-valued functions of n variables by the superposition of functions of one variable and addition is always possible. Based on the fact that each proof of the Kolmogorov theorem or its variants was a constructive one so far, there is the principal possibility to attain such a representation. This paper reviews a procedure for obtaining the Kolmogorov representation of a function, based on an approach given by David Sprecher. The construction is considered in more detail for an image function.",
isbn="978-3-540-46084-8"
}


@misc{liu2024kan,
      title={KAN: Kolmogorov-Arnold Networks}, 
      author={Ziming Liu and Yixuan Wang and Sachin Vaidya and Fabian Ruehle and James Halverson and Marin Soljačić and Thomas Y. Hou and Max Tegmark},
      year={2024},
      eprint={2404.19756},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{yu2024kanmlpfairercomparison,
      title={KAN or MLP: A Fairer Comparison}, 
      author={Runpeng Yu and Weihao Yu and Xinchao Wang},
      year={2024},
      eprint={2407.16674},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.16674}, 
}



%%% PINN KANs references


@article{SHUKLA2024117290,
title = {A comprehensive and FAIR comparison between MLP and KAN representations for differential equations and operator networks},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {431},
pages = {117290},
year = {2024},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2024.117290},
url = {https://www.sciencedirect.com/science/article/pii/S0045782524005462},
author = {Khemraj Shukla and Juan Diego Toscano and Zhicheng Wang and Zongren Zou and George Em Karniadakis},
keywords = {Scientific machine learning, Kolmogorov–Arnold networks, Physics-informed neural networks, Operator networks, Partial differential equations},
abstract = {Kolmogorov–Arnold Networks (KANs) were recently introduced as an alternative representation model to MLP. Herein, we employ KANs to construct physics-informed machine learning models (PIKANs) and deep operator models (DeepOKANs) for solving differential equations for forward and inverse problems. In particular, we compare them with physics-informed neural networks (PINNs) and deep operator networks (DeepONets), which are based on the standard MLP representation. We find that although the original KANs based on the B-splines parameterization lack accuracy and efficiency, modified versions based on low-order orthogonal polynomials have comparable performance to PINNs and DeepONet, although they still lack robustness as they may diverge for different random seeds or higher order orthogonal polynomials. We visualize their corresponding loss landscapes and analyze their learning dynamics using information bottleneck theory. Our study follows the FAIR principles so that other researchers can use our benchmarks to further advance this emerging topic.}
}

@misc{wang2024kolmogorovarnoldinformedneural-remove,
      title={Kolmogorov Arnold Informed neural network: A physics-informed deep learning framework for solving forward and inverse problems based on Kolmogorov Arnold Networks}, 
      author={Yizheng Wang and Jia Sun and Jinshuai Bai and Cosmin Anitescu and Mohammad Sadegh Eshaghi and Xiaoying Zhuang and Timon Rabczuk and Yinghua Liu},
      year={2024},
      eprint={2406.11045},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.11045}, 
}

@misc{howard2024finitebasiskolmogorovarnoldnetworks,
      title={Finite basis Kolmogorov-Arnold networks: domain decomposition for data-driven and physics-informed problems}, 
      author={Amanda A. Howard and Bruno Jacob and Sarah H. Murphy and Alexander Heinlein and Panos Stinis},
      year={2024},
      eprint={2406.19662},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.19662}, 
}


@misc{shuai2024physicsinformedkolmogorovarnoldnetworkspower-remove,
      title={Physics-Informed Kolmogorov-Arnold Networks for Power System Dynamics}, 
      author={Hang Shuai and Fangxing Li},
      year={2024},
      eprint={2408.06650},
      archivePrefix={arXiv},
      primaryClass={eess.SY},
      url={https://arxiv.org/abs/2408.06650}, 
}

@misc{rigas2024adaptivetraininggriddependentphysicsinformed,
      title={Adaptive Training of Grid-Dependent Physics-Informed Kolmogorov-Arnold Networks}, 
      author={Spyros Rigas and Michalis Papachristou and Theofilos Papadopoulos and Fotios Anagnostopoulos and Georgios Alexandridis},
      year={2024},
      eprint={2407.17611},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.17611}, 
}


@misc{patra2024physicsinformedkolmogorovarnoldneural,
      title={Physics Informed Kolmogorov-Arnold Neural Networks for Dynamical Analysis via Efficent-KAN and WAV-KAN}, 
      author={Subhajit Patra and Sonali Panda and Bikram Keshari Parida and Mahima Arya and Kurt Jacobs and Denys I. Bondar and Abhijit Sen},
      year={2024},
      eprint={2407.18373},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.18373}, 
}


@article{Koenig_2024,
   title={KAN-ODEs: Kolmogorov–Arnold network ordinary differential equations for learning dynamical systems and hidden physics},
   volume={432},
   ISSN={0045-7825},
   url={http://dx.doi.org/10.1016/j.cma.2024.117397},
   DOI={10.1016/j.cma.2024.117397},
   journal={Computer Methods in Applied Mechanics and Engineering},
   publisher={Elsevier BV},
   author={Koenig, Benjamin C. and Kim, Suyong and Deng, Sili},
   year={2024},
   month=dec, pages={117397} }


@misc{qian2024investigatingkanbasedphysicsinformedneural,
      title={Investigating KAN-Based Physics-Informed Neural Networks for EMI/EMC Simulations}, 
      author={Kun Qian and Mohamed Kheir},
      year={2024},
      eprint={2405.11383},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.11383}, 
}




@article{girosi1989representation,
  title={Representation properties of networks: Kolmogorov's theorem is irrelevant},
  author={Girosi, Federico and Poggio, Tomaso},
  journal={Neural Computation},
  volume={1},
  number={4},
  pages={465--469},
  year={1989},
  publisher={MIT Press}
}

@article{he2023deep,
  title={Deep neural networks and finite elements of any order on arbitrary dimensions},
  author={He, Juncai and Xu, Jinchao},
  journal={arXiv preprint arXiv:2312.14276},
  year={2023}
}
@article{he2018relu,
  title={ReLU deep neural networks and linear finite elements},
  author={He, Juncai and Li, Lin and Xu, Jinchao and Zheng, Chunyue},
  journal={arXiv preprint arXiv:1807.03973},
  year={2018}
}
@article{he2023optimal,
  title={On the optimal expressive power of relu dnns and its application in approximation with kolmogorov superposition theorem},
  author={He, Juncai},
  journal={arXiv preprint arXiv:2308.05509},
  year={2023}
}


@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}


@article{poluektov2023new,
  title={A new iterative method for construction of the Kolmogorov-Arnold representation},
  author={Poluektov, Michael and Polar, Andrew},
  journal={arXiv preprint arXiv:2305.08194},
  year={2023}
}


@article{ismayilova2024kolmogorov,
  title={On the Kolmogorov neural networks},
  author={Ismayilova, Aysu and Ismailov, Vugar E},
  journal={Neural Networks},
  pages={106333},
  year={2024},
  publisher={Elsevier}
}


@article{song2018optimizing,
  title={Optimizing kernel machines using deep learning},
  author={Song, Huan and Thiagarajan, Jayaraman J and Sattigeri, Prasanna and Spanias, Andreas},
  journal={IEEE transactions on neural networks and learning systems},
  volume={29},
  number={11},
  pages={5528--5540},
  year={2018},
  publisher={IEEE}
}



@article{wang2024multi,
  title={Multi-stage neural networks: Function approximator of machine precision},
  author={Wang, Yongji and Lai, Ching-Yao},
  journal={Journal of Computational Physics},
  pages={112865},
  year={2024},
  publisher={Elsevier}
}

@article{sun2021discerning,
  title={Discerning decision-making process of deep neural networks with hierarchical voting transformation},
  author={Sun, Ying and Zhu, Hengshu and Qin, Chuan and Zhuang, Fuzhen and He, Qing and Xiong, Hui},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17221--17234},
  year={2021}
}


@article{zaheer2017deep,
  title={Deep sets},
  author={Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Russ R and Smola, Alexander J},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{polar2021deep,
  title={A deep machine learning algorithm for construction of the Kolmogorov--Arnold representation},
  author={Polar, Andrew and Poluektov, Michael},
  journal={Engineering Applications of Artificial Intelligence},
  volume={99},
  pages={104137},
  year={2021},
  publisher={Elsevier}
}


@article{igelnik2003kolmogorov,
  title={Kolmogorov's spline network},
  author={Igelnik, Boris and Parikh, Neel},
  journal={IEEE transactions on neural networks},
  volume={14},
  number={4},
  pages={725--733},
  year={2003},
  publisher={IEEE}
}

@article{siegel2023optimal,
  title={Optimal approximation rates for deep ReLU neural networks on Sobolev and Besov spaces},
  author={Siegel, Jonathan W},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={357},
  pages={1--52},
  year={2023}
}

@article{devore1989optimal,
  title={Optimal nonlinear approximation},
  author={DeVore, Ronald A and Howard, Ralph and Micchelli, Charles},
  journal={Manuscripta mathematica},
  volume={63},
  pages={469--478},
  year={1989},
  publisher={Springer}
}
@article{devore1993wavelet,
  title={Wavelet compression and nonlinear n-widths.},
  author={DeVore, Ronald A and Kyriazis, George and Leviatan, Dany and Tikhomirov, Vladimir M},
  journal={Adv. Comput. Math.},
  volume={1},
  number={2},
  pages={197--214},
  year={1993}
}
@article{yarotsky2017error,
  title={Error bounds for approximations with deep ReLU networks},
  author={Yarotsky, Dmitry},
  journal={Neural Networks},
  volume={94},
  pages={103--114},
  year={2017},
  publisher={Elsevier}
}
@article{siegel2024sharp,
  title={Sharp Lower Bounds on the Manifold Widths of Sobolev and Besov Spaces},
  author={Siegel, Jonathan W},
  journal={arXiv preprint arXiv:2402.04407},
  year={2024}
}
@article{bartlett2019nearly,
  title={Nearly-tight VC-dimension and pseudodimension bounds for piecewise linear neural networks},
  author={Bartlett, Peter L and Harvey, Nick and Liaw, Christopher and Mehrabian, Abbas},
  journal={Journal of Machine Learning Research},
  volume={20},
  number={63},
  pages={1--17},
  year={2019}
}
@article{horowitz2007rate,
  title={Rate-optimal estimation for a general class of nonparametric regression models with unknown link functions},
  author={Horowitz, Joel L and Mammen, Enno},
  year={2007}
}

@article{schmidt2020nonparametric,
  title={Nonparametric regression using deep neural networks with ReLU activation function},
  author={Schmidt-Hieber, Johannes},
  year={2020}
}

@article{kohler2021rate,
  title={On the rate of convergence of fully connected deep neural network regression estimates},
  author={Kohler, Michael and Langer, Sophie},
  journal={The Annals of Statistics},
  volume={49},
  number={4},
  pages={2231--2249},
  year={2021},
  publisher={Institute of Mathematical Statistics}
}

@article{montanelli2020error,
  title={Error bounds for deep ReLU networks using the Kolmogorov--Arnold superposition theorem},
  author={Montanelli, Hadrien and Yang, Haizhao},
  journal={Neural Networks},
  volume={129},
  pages={1--6},
  year={2020},
  publisher={Elsevier}
}

@article{lin2017does,
  title={Why does deep and cheap learning work so well?},
  author={Lin, Henry W and Tegmark, Max and Rolnick, David},
  journal={Journal of Statistical Physics},
  volume={168},
  pages={1223--1247},
  year={2017},
  publisher={Springer}
}

@misc{lu2024revisiting,
      title={Revisiting Neural Networks for Continual Learning: An Architectural Perspective}, 
      author={Aojun Lu and Tao Feng and Hangjie Yuan and Xiaotian Song and Yanan Sun},
      year={2024},
      eprint={2404.14829},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{chen2023exponentially,
  title={Exponentially convergent multiscale finite element method},
  author={Chen, Yifan and Hou, Thomas Y and Wang, Yixuan},
  journal={Communications on Applied Mathematics and Computation},
  pages={1--17},
  year={2023},
  publisher={Springer}
}
@article{ma2023unified,
  title={A unified framework for multiscale spectral generalized FEMs and low-rank approximations to multiscale PDEs},
  author={Ma, Chupeng},
  journal={arXiv preprint arXiv:2311.08761},
  year={2023}
}
@inproceedings{zhang2021multiscale,
  title={Multiscale invertible generative networks for high-dimensional Bayesian inference},
  author={Zhang, Shumao and Zhang, Pengchuan and Hou, Thomas Y},
  booktitle={International Conference on Machine Learning},
  pages={12632--12641},
  year={2021},
  organization={PMLR}
}
@article{xu2017algebraic,
  title={Algebraic multigrid methods},
  author={Xu, Jinchao and Zikatanov, Ludmil},
  journal={Acta Numerica},
  volume={26},
  pages={591--721},
  year={2017},
  publisher={Cambridge University Press}
}


@article{sitzmann2020implicit,
  title={Implicit neural representations with periodic activation functions},
  author={Sitzmann, Vincent and Martel, Julien and Bergman, Alexander and Lindell, David and Wetzstein, Gordon},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={7462--7473},
  year={2020}
}


@incollection{leni2013kolmogorov,
  title={The kolmogorov spline network for image processing},
  author={Leni, Pierre-Emmanuel and Fougerolle, Yohan D and Truchetet, Fr{\'e}d{\'e}ric},
  booktitle={Image Processing: Concepts, Methodologies, Tools, and Applications},
  pages={54--78},
  year={2013},
  publisher={IGI Global}
}


@article{lai2021kolmogorov,
  title={The kolmogorov superposition theorem can break the curse of dimensionality when approximating high dimensional functions},
  author={Lai, Ming-Jun and Shen, Zhaiming},
  journal={arXiv preprint arXiv:2112.09963},
  year={2021}
}


@article{lin1993realization,
  title={On the realization of a Kolmogorov network},
  author={Lin, Ji-Nan and Unbehauen, Rolf},
  journal={Neural Computation},
  volume={5},
  number={1},
  pages={18--20},
  year={1993},
  publisher={MIT Press}
}


@inproceedings{koppen2002training,
  title={On the training of a Kolmogorov Network},
  author={K{\"o}ppen, Mario},
  booktitle={Artificial Neural Networks—ICANN 2002: International Conference Madrid, Spain, August 28--30, 2002 Proceedings 12},
  pages={474--479},
  year={2002},
  organization={Springer}
}

@article{sprecher2002space,
  title={Space-filling curves and Kolmogorov superposition-based neural networks},
  author={Sprecher, David A and Draghici, Sorin},
  journal={Neural Networks},
  volume={15},
  number={1},
  pages={57--67},
  year={2002},
  publisher={Elsevier}
}

@article{kolmogorov,
  author  = "A.N. Kolmogorov",
  title   = "On the Representation of continuous functions of several variables as superpositions of continuous functions of a smaller number of variables.",
  journal = "Dokl. Akad. Nauk",
  year    = 1956,
  volume  = "108",
  number  = "2",
}


@article{braun2009constructive_remove1,
  title={On a constructive proof of Kolmogorov’s superposition theorem},
  author={Braun, J{\"u}rgen and Griebel, Michael},
  journal={Constructive approximation},
  volume={30},
  pages={653--675},
  year={2009},
  publisher={Springer}
}


@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}


@article{hornik1989multilayer,
  title={Multilayer feedforward networks are universal approximators},
  author={Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  journal={Neural networks},
  volume={2},
  number={5},
  pages={359--366},
  year={1989},
  publisher={Elsevier}
}


@article{cybenko1989approximation,
  title={Approximation by superpositions of a sigmoidal function},
  author={Cybenko, George},
  journal={Mathematics of control, signals and systems},
  volume={2},
  number={4},
  pages={303--314},
  year={1989},
  publisher={Springer}
}


@book{haykin1994neural,
  title={Neural networks: a comprehensive foundation},
  author={Haykin, Simon},
  year={1994},
  publisher={Prentice Hall PTR}
}


@inproceedings{
mundhenk2021symbolic,
title={Symbolic Regression via Deep Reinforcement Learning Enhanced Genetic Programming Seeding},
author={Terrell N. Mundhenk and Mikel Landajuela and Ruben Glatt and Claudio P. Santiago and Daniel faissol and Brenden K. Petersen},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=tjwQaOI9tdy}
}


@article{udrescu2020ai2,
  title={AI Feynman 2.0: Pareto-optimal symbolic regression exploiting graph modularity},
  author={Udrescu, Silviu-Marian and Tan, Andrew and Feng, Jiahai and Neto, Orisvaldo and Wu, Tailin and Tegmark, Max},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={4860--4871},
  year={2020}
}


@article{udrescu2020ai,
  title={AI Feynman: A physics-inspired method for symbolic regression},
  author={Udrescu, Silviu-Marian and Tegmark, Max},
  journal={Science Advances},
  volume={6},
  number={16},
  pages={eaay2631},
  year={2020},
  publisher={American Association for the Advancement of Science}
}


@article{dugan2020occamnet,
  title={OccamNet: A Fast Neural Model for Symbolic Regression at Scale},
  author={Dugan, Owen and Dangovski, Rumen and Costa, Allan and Kim, Samuel and Goyal, Pawan and Jacobson, Joseph and Solja{\v{c}}i{\'c}, Marin},
  journal={arXiv preprint arXiv:2007.10784},
  year={2020}
}


@article{martius2016extrapolation,
  title={Extrapolation and learning equations},
  author={Martius, Georg and Lampert, Christoph H},
  journal={arXiv preprint arXiv:1610.02995},
  year={2016}
}


@article{cranmer2023interpretable,
  title={Interpretable machine learning for science with PySR and SymbolicRegression. jl},
  author={Cranmer, Miles},
  journal={arXiv preprint arXiv:2305.01582},
  year={2023}
}


@misc{gplearn,
  title = {GPLearn},
  howpublished = {\url{https://github.com/trevorstephens/gplearn}},
  note = {Accessed: 2024-04-19}
}

@article{Dubckov2011EureqaSR,
  title={Eureqa: software review},
  author={Ren{\'a}ta Dubc{\'a}kov{\'a}},
  journal={Genetic Programming and Evolvable Machines},
  year={2011},
  volume={12},
  pages={173-178},
  url={https://api.semanticscholar.org/CorpusID:36698573}
}

@article{meunier2010modular,
  title={Modular and hierarchically modular organization of brain networks},
  author={Meunier, David and Lambiotte, Renaud and Bullmore, Edward T},
  journal={Frontiers in neuroscience},
  volume={4},
  pages={7572},
  year={2010},
  publisher={Frontiers}
}


@article{kolb1998brain,
  title={Brain plasticity and behavior},
  author={Kolb, Bryan and Whishaw, Ian Q},
  journal={Annual review of psychology},
  volume={49},
  number={1},
  pages={43--64},
  year={1998},
  publisher={Annual Reviews 4139 El Camino Way, PO Box 10139, Palo Alto, CA 94303-0139, USA}
}


@inproceedings{kemker2018measuring,
  title={Measuring catastrophic forgetting in neural networks},
  author={Kemker, Ronald and McClure, Marc and Abitino, Angelina and Hayes, Tyler and Kanan, Christopher},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  year={2018}
}


@article{kirkpatrick2017overcoming,
  title={Overcoming catastrophic forgetting in neural networks},
  author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and others},
  journal={Proceedings of the national academy of sciences},
  volume={114},
  number={13},
  pages={3521--3526},
  year={2017},
  publisher={National Acad Sciences}
}


@article{cunningham2023sparse,
  title={Sparse autoencoders find highly interpretable features in language models},
  author={Cunningham, Hoagy and Ewart, Aidan and Riggs, Logan and Huben, Robert and Sharkey, Lee},
  journal={arXiv preprint arXiv:2309.08600},
  year={2023}
}


@article{elhage2022solu,
   title={Softmax Linear Units},
   author={Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Nanda, Neel and Henighan, Tom and Johnston, Scott and ElShowk, Sheer and Joseph, Nicholas and DasSarma, Nova and Mann, Ben and Hernandez, Danny and Askell, Amanda and Ndousse, Kamal and Jones, Andy and Drain, Dawn and Chen, Anna and Bai, Yuntao and Ganguli, Deep and Lovitt, Liane and Hatfield-Dodds, Zac and Kernion, Jackson and Conerly, Tom and Kravec, Shauna and Fort, Stanislav and Kadavath, Saurav and Jacobson, Josh and Tran-Johnson, Eli and Kaplan, Jared and Clark, Jack and Brown, Tom and McCandlish, Sam and Amodei, Dario and Olah, Christopher},
   year={2022},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2022/solu/index.html}
}


@article{liu2023seeing,
  title={Seeing is believing: Brain-inspired modular training for mechanistic interpretability},
  author={Liu, Ziming and Gan, Eric and Tegmark, Max},
  journal={Entropy},
  volume={26},
  number={1},
  pages={41},
  year={2023},
  publisher={MDPI}
}


@inproceedings{
zhong2023the,
title={The Clock and the Pizza: Two Stories in Mechanistic Explanation of Neural Networks},
author={Ziqian Zhong and Ziming Liu and Max Tegmark and Jacob Andreas},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=S5wmbQc1We}
}


@inproceedings{
nanda2023progress,
title={Progress measures for grokking via mechanistic interpretability},
author={Neel Nanda and Lawrence Chan and Tom Lieberum and Jess Smith and Jacob Steinhardt},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=9XFSbDPmdW}
}


@article{elhage2022toy,
  title={Toy models of superposition},
  author={Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and others},
  journal={arXiv preprint arXiv:2209.10652},
  year={2022}
}


@inproceedings{
wang2023interpretability,
title={Interpretability in the Wild: a Circuit for Indirect Object Identification in {GPT}-2 Small},
author={Kevin Ro Wang and Alexandre Variengien and Arthur Conmy and Buck Shlegeris and Jacob Steinhardt},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=NpsVSN6o4ul}
}



@article{meng2022locating,
  title={Locating and editing factual associations in GPT},
  author={Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17359--17372},
  year={2022}
}



@article{olsson2022context,
  title={In-context learning and induction heads},
  author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and others},
  journal={arXiv preprint arXiv:2209.11895},
  year={2022}
}


@article{song2024resource,
  title={A Resource Model For Neural Scaling Law},
  author={Song, Jinyeop and Liu, Ziming and Tegmark, Max and Gore, Jeff},
  journal={arXiv preprint arXiv:2402.05164},
  year={2024}
}


@inproceedings{
michaud2023the,
title={The Quantization Model of Neural Scaling},
author={Eric J Michaud and Ziming Liu and Uzay Girit and Max Tegmark},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=3tbTw2ga8K}
}


@article{bahri2021explaining,
  title={Explaining neural scaling laws},
  author={Bahri, Yasaman and Dyer, Ethan and Kaplan, Jared and Lee, Jaehoon and Sharma, Utkarsh},
  journal={arXiv preprint arXiv:2102.06701},
  year={2021}
}


@article{hestness2017deep,
  title={Deep learning scaling is predictable, empirically},
  author={Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, Gregory and Jun, Heewoo and Kianinejad, Hassan and Patwary, Md Mostofa Ali and Yang, Yang and Zhou, Yanqi},
  journal={arXiv preprint arXiv:1712.00409},
  year={2017}
}

@inproceedings{
gordon2021data,
title={Data and Parameter Scaling Laws for Neural Machine Translation},
author={Mitchell A Gordon and Kevin Duh and Jared Kaplan},
booktitle={ACL Rolling Review - May 2021},
year={2021},
url={https://openreview.net/forum?id=IKA7MLxsLSu}
}


@article{henighan2020scaling,
  title={Scaling laws for autoregressive generative modeling},
  author={Henighan, Tom and Kaplan, Jared and Katz, Mor and Chen, Mark and Hesse, Christopher and Jackson, Jacob and Jun, Heewoo and Brown, Tom B and Dhariwal, Prafulla and Gray, Scott and others},
  journal={arXiv preprint arXiv:2010.14701},
  year={2020}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@book{petersen2006riemannian,
  title={Riemannian Geometry},
  author={Petersen, P.},
  isbn={9780387294032},
  lccn={97005786},
  series={Graduate Texts in Mathematics},
  url={https://books.google.com/books?id=9cekXdo52hEC},
  year={2006},
  publisher={Springer New York}
}

@article{Gukov:2020qaj,
    author = "Gukov, Sergei and Halverson, James and Ruehle, Fabian and Su\l{}kowski, Piotr",
    title = "{Learning to Unknot}",
    eprint = "2010.16263",
    archivePrefix = "arXiv",
    primaryClass = "math.GT",
    reportNumber = "CALT-2020-046. CERN-TH-2020-179",
    doi = "10.1088/2632-2153/abe91f",
    journal = "Mach. Learn. Sci. Tech.",
    volume = "2",
    number = "2",
    pages = "025035",
    year = "2021"
}

@misc{kauffman2020rectangular,
      title={Rectangular knot diagrams classification with deep learning}, 
      author={L. H. Kauffman and N. E. Russkikh and I. A. Taimanov},
      year={2020},
      eprint={2011.03498},
      archivePrefix={arXiv},
      primaryClass={math.GT}
}

@misc{gukov2023searching,
      title={Searching for ribbons with machine learning}, 
      author={Sergei Gukov and James Halverson and Ciprian Manolescu and Fabian Ruehle},
      year={2023},
      eprint={2304.09304},
      archivePrefix={arXiv},
      primaryClass={math.GT}
}

@article{hughes2020neural,
  title={A neural network approach to predicting and computing knot invariants},
  author={Hughes, Mark C},
  journal={Journal of Knot Theory and Its Ramifications},
  volume={29},
  number={03},
  pages={2050005},
  year={2020},
  publisher={World Scientific}
}

@article{Craven:2020bdz,
    author = "Craven, Jessica and Jejjala, Vishnu and Kar, Arjun",
    title = "{Disentangling a deep learned volume formula}",
    eprint = "2012.03955",
    archivePrefix = "arXiv",
    primaryClass = "hep-th",
    doi = "10.1007/JHEP06(2021)040",
    journal = "JHEP",
    volume = "06",
    pages = "040",
    year = "2021"
}

@article{Craven:2022cxe,
    author = "Craven, Jessica and Hughes, Mark and Jejjala, Vishnu and Kar, Arjun",
    title = "{Illuminating new and known relations between knot invariants}",
    eprint = "2211.01404",
    archivePrefix = "arXiv",
    primaryClass = "math.GT",
    month = "11",
    year = "2022"
}

@article{Ruehle:2020jrk,
    author = "Ruehle, Fabian",
    title = "{Data science applications to string theory}",
    doi = "10.1016/j.physrep.2019.09.005",
    journal = "Phys. Rept.",
    volume = "839",
    pages = "1--117",
    year = "2020"
}

@book{he2023machine,
  title={Machine Learning in Pure Mathematics and Theoretical Physics},
  author={He, Y.H.},
  isbn={9781800613690},
  lccn={2022058911},
  series={G - Reference,Information and Interdisciplinary Subjects Series},
  url={https://books.google.com/books?id=6a5gzwEACAAJ},
  year={2023},
  publisher={World Scientific}
}


@article{Gukov:2024aaa,
	author = {Gukov, Sergei and Halverson, James and Ruehle, Fabian},
	doi = {10.1038/s42254-024-00709-0},
	id = {Gukov2024},
	isbn = {2522-5820},
	journal = {Nature Reviews Physics},
	title = {Rigor with machine learning from field theory to the Poincar{\'e}conjecture},
	url = {https://doi.org/10.1038/s42254-024-00709-0},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1038/s42254-024-00709-0}
}


@article{davies2021advancing,
  title={Advancing mathematics by guiding human intuition with AI},
  author={Davies, Alex and Veli{\v{c}}kovi{\'c}, Petar and Buesing, Lars and Blackwell, Sam and Zheng, Daniel and Toma{\v{s}}ev, Nenad and Tanburn, Richard and Battaglia, Peter and Blundell, Charles and Juh{\'a}sz, Andr{\'a}s and others},
  journal={Nature},
  volume={600},
  number={7887},
  pages={70--74},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{poggio2020theoretical,
  title={Theoretical issues in deep networks},
  author={Poggio, Tomaso and Banburski, Andrzej and Liao, Qianli},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={48},
  pages={30039--30045},
  year={2020},
  publisher={National Acad Sciences}
}

@article{michaud2023precision,
  title={Precision machine learning},
  author={Michaud, Eric J and Liu, Ziming and Tegmark, Max},
  journal={Entropy},
  volume={25},
  number={1},
  pages={175},
  year={2023},
  publisher={MDPI}
}


@article{sharma2020neural,
  title={A neural scaling law from the dimension of the data manifold},
  author={Sharma, Utkarsh and Kaplan, Jared},
  journal={arXiv preprint arXiv:2004.10802},
  year={2020}
}


@article{xu2015nonlinear,
  title={Nonlinear material design using principal stretches},
  author={Xu, Hongyi and Sin, Funshing and Zhu, Yufeng and Barbi{\v{c}}, Jernej},
  journal={ACM Transactions on Graphics (TOG)},
  volume={34},
  number={4},
  pages={1--11},
  year={2015},
  publisher={ACM New York, NY, USA}
}


@inproceedings{aziznejad2019deep,
  title={Deep spline networks with control of Lipschitz regularity},
  author={Aziznejad, Shayan and Unser, Michael},
  booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={3242--3246},
  year={2019},
  organization={IEEE}
}


@article{bohra2020learning,
  title={Learning activation functions in deep (spline) neural networks},
  author={Bohra, Pakshal and Campos, Joaquim and Gupta, Harshit and Aziznejad, Shayan and Unser, Michael},
  journal={IEEE Open Journal of Signal Processing},
  volume={1},
  pages={295--309},
  year={2020},
  publisher={IEEE}
}
@article{cho2024separable,
  title={Separable physics-informed neural networks},
  author={Cho, Junwoo and Nam, Seungtae and Yang, Hyunmo and Yun, Seok-Bae and Hong, Youngjoon and Park, Eunbyung},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{zhang2022neural,
  title={Neural network architecture beyond width and depth},
  author={Zhang, Shijun and Shen, Zuowei and Yang, Haizhao},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={5669--5681},
  year={2022}
}


@article{fakhoury2022exsplinet,
  title={ExSpliNet: An interpretable and expressive spline-based neural network},
  author={Fakhoury, Daniele and Fakhoury, Emanuele and Speleers, Hendrik},
  journal={Neural Networks},
  volume={152},
  pages={332--346},
  year={2022},
  publisher={Elsevier}
}


@article{goyal2019learning,
  title={Learning activation functions: A new paradigm for understanding neural networks},
  author={Goyal, Mohit and Goyal, Rajan and Lall, Brejesh},
  journal={arXiv preprint arXiv:1906.09529},
  year={2019}
}

@article{bingham2022discovering,
  title={Discovering parametric activation functions},
  author={Bingham, Garrett and Miikkulainen, Risto},
  journal={Neural Networks},
  volume={148},
  pages={48--65},
  year={2022},
  publisher={Elsevier}
}


@article{ramachandran2017searching,
  title={Searching for activation functions},
  author={Ramachandran, Prajit and Zoph, Barret and Le, Quoc V},
  journal={arXiv preprint arXiv:1710.05941},
  year={2017}
}


@article{agarwal2021neural,
  title={Neural additive models: Interpretable machine learning with neural nets},
  author={Agarwal, Rishabh and Melnick, Levi and Frosst, Nicholas and Zhang, Xuezhou and Lengerich, Ben and Caruana, Rich and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={4699--4711},
  year={2021}
}

@article{schmidt2021kolmogorov,
  title={The Kolmogorov--Arnold representation theorem revisited},
  author={Schmidt-Hieber, Johannes},
  journal={Neural networks},
  volume={137},
  pages={119--126},
  year={2021},
  publisher={Elsevier}
}


@article{poggio2022deep,
  title={How deep sparse networks avoid the curse of dimensionality: Efficiently computable functions are compositionally sparse},
  author={Poggio, Tomaso},
  journal={CBMM Memo},
  volume={10},
  pages={2022},
  year={2022}
}
@inproceedings{kolmogorov1957representation,
  title={On the representation of continuous functions of many variables by superposition of continuous functions of one variable and addition},
  author={Kolmogorov, Andrei Nikolaevich},
  booktitle={Doklady Akademii Nauk},
  volume={114},
  pages={953--956},
  year={1957},
  organization={Russian Academy of Sciences}
}
@book{de1978practical,
  title={A practical guide to splines},
  author={De Boor, Carl},
  volume={27},
  year={1978},
  publisher={springer-verlag New York}
}

@article{lu2021learning,
  title={Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators},
  author={Lu, Lu and Jin, Pengzhan and Pang, Guofei and Zhang, Zhongqiang and Karniadakis, George Em},
  journal={Nature machine intelligence},
  volume={3},
  number={3},
  pages={218--229},
  year={2021},
  publisher={Nature Publishing Group UK London}
}

@article{raissi2019physics,
  title={Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
  author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George E},
  journal={Journal of Computational physics},
  volume={378},
  pages={686--707},
  year={2019},
  publisher={Elsevier}
}

@article{karniadakis2021physics,
  title={Physics-informed machine learning},
  author={Karniadakis, George Em and Kevrekidis, Ioannis G and Lu, Lu and Perdikaris, Paris and Wang, Sifan and Yang, Liu},
  journal={Nature Reviews Physics},
  volume={3},
  number={6},
  pages={422--440},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{yu2018deep,
  title={The deep Ritz method: a deep learning-based numerical algorithm for solving variational problems},
  author={Yu, Bing and others},
  journal={Communications in Mathematics and Statistics},
  volume={6},
  number={1},
  pages={1--12},
  year={2018},
  publisher={Springer}
}

@article{li2021physics,
  title={Physics-informed neural operator for learning partial differential equations},
  author={Li, Zongyi and Zheng, Hongkai and Kovachki, Nikola and Jin, David and Chen, Haoxuan and Liu, Burigede and Azizzadenesheli, Kamyar and Anandkumar, Anima},
  journal={ACM/JMS Journal of Data Science},
  year={2021},
  publisher={ACM New York, NY}
}

@article{kovachki2023neural,
  title={Neural operator: Learning maps between function spaces with applications to pdes},
  author={Kovachki, Nikola and Li, Zongyi and Liu, Burigede and Azizzadenesheli, Kamyar and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={89},
  pages={1--97},
  year={2023}
}

@article{li2020fourier,
  title={Fourier neural operator for parametric partial differential equations},
  author={Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2010.08895},
  year={2020}
}





@article{maust2022fourier,
  title={Fourier continuation for exact derivative computation in physics-informed neural operators},
  author={Maust, Haydn and Li, Zongyi and Wang, Yixuan and Leibovici, Daniel and Bruno, Oscar and Hou, Thomas and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2211.15960},
  year={2022}
}

@article{ME_biddle2010predicted,
  title={Predicted mobility edges in one-dimensional incommensurate optical lattices: An exactly solvable model of Anderson localization},
  author={Biddle, J and Sarma, S Das},
  journal={Physical review letters},
  volume={104},
  number={7},
  pages={070601},
  year={2010},
  publisher={APS}
}

@article{ME_ganeshan2015nearest,
  title={Nearest neighbor tight binding models with an exact mobility edge in one dimension},
  author={Ganeshan, Sriram and Pixley, JH and Sarma, S Das},
  journal={Physical review letters},
  volume={114},
  number={14},
  pages={146601},
  year={2015},
  publisher={APS}
}

@article{ME_wang2020one,
  title={One-dimensional quasiperiodic mosaic lattice with exact mobility edges},
  author={Wang, Yucheng and Xia, Xu and Zhang, Long and Yao, Hepeng and Chen, Shu and You, Jiangong and Zhou, Qi and Liu, Xiong-Jun},
  journal={Physical Review Letters},
  volume={125},
  number={19},
  pages={196604},
  year={2020},
  publisher={APS}
}

@article{ME_an2021interactions,
  title={Interactions and mobility edges: Observing the generalized aubry-andr{\'e} model},
  author={An, Fangzhao Alex and Padavi{\'c}, Karmela and Meier, Eric J and Hegde, Suraj and Ganeshan, Sriram and Pixley, JH and Vishveshwara, Smitha and Gadway, Bryce},
  journal={Physical review letters},
  volume={126},
  number={4},
  pages={040603},
  year={2021},
  publisher={APS}
}

@article{ME_duthie2021self,
  title={Self-consistent theory of mobility edges in quasiperiodic chains},
  author={Duthie, Alexander and Roy, Sthitadhi and Logan, David E},
  journal={Physical Review B},
  volume={103},
  number={6},
  pages={L060201},
  year={2021},
  publisher={APS}
}

@article{ME_wang2021duality,
  title={Duality between two generalized Aubry-Andr{\'e} models with exact mobility edges},
  author={Wang, Yucheng and Xia, Xu and Wang, Yongjian and Zheng, Zuohuan and Liu, Xiong-Jun},
  journal={Physical Review B},
  volume={103},
  number={17},
  pages={174205},
  year={2021},
  publisher={APS}
}

@article{ME_zhou2023exact,
  title={Exact new mobility edges between critical and localized states},
  author={Zhou, Xin-Chi and Wang, Yongjian and Poon, Ting-Fung Jeffrey and Zhou, Qi and Liu, Xiong-Jun},
  journal={Physical Review Letters},
  volume={131},
  number={17},
  pages={176401},
  year={2023},
  publisher={APS}
}

@article{vaidya2023reentrant,
  title={Reentrant delocalization transition in one-dimensional photonic quasicrystals},
  author={Vaidya, Sachin and J{\"o}rg, Christina and Linn, Kyle and Goh, Megan and Rechtsman, Mikael C},
  journal={Physical Review Research},
  volume={5},
  number={3},
  pages={033170},
  year={2023},
  publisher={APS}
}

@article{anderson1958absence,
  title={Absence of diffusion in certain random lattices},
  author={Anderson, Philip W},
  journal={Physical review},
  volume={109},
  number={5},
  pages={1492},
  year={1958},
  publisher={APS}
}

@article{thouless1972relation,
  title={A relation between the density of states and range of localization for one dimensional random systems},
  author={Thouless, David J},
  journal={Journal of Physics C: Solid State Physics},
  volume={5},
  number={1},
  pages={77},
  year={1972},
  publisher={IOP Publishing}
}

@article{abrahams1979scaling,
  title={Scaling theory of localization: Absence of quantum diffusion in two dimensions},
  author={Abrahams, Elihu and Anderson, PW and Licciardello, DC and Ramakrishnan, TV},
  journal={Physical Review Letters},
  volume={42},
  number={10},
  pages={673},
  year={1979},
  publisher={APS}
}

@article{segev2013anderson,
  title={Anderson localization of light},
  author={Segev, Mordechai and Silberberg, Yaron and Christodoulides, Demetrios N},
  journal={Nature Photonics},
  volume={7},
  number={3},
  pages={197--204},
  year={2013},
  publisher={Nature Publishing Group UK London}
}

@article{john1987strong,
  title={Strong localization of photons in certain disordered dielectric superlattices},
  author={John, Sajeev},
  journal={Physical review letters},
  volume={58},
  number={23},
  pages={2486},
  year={1987},
  publisher={APS}
}

@article{lahini2009observation,
  title={Observation of a localization transition in quasiperiodic photonic lattices},
  author={Lahini, Yoav and Pugatch, Rami and Pozzi, Francesca and Sorel, Marc and Morandotti, Roberto and Davidson, Nir and Silberberg, Yaron},
  journal={Physical review letters},
  volume={103},
  number={1},
  pages={013901},
  year={2009},
  publisher={APS}
}

@article{vardeny2013optics,
  title={Optics of photonic quasicrystals},
  author={Vardeny, Z Valy and Nahata, Ajay and Agrawal, Amit},
  journal={Nature photonics},
  volume={7},
  number={3},
  pages={177--187},
  year={2013},
  publisher={Nature Publishing Group UK London}
}

@article{de2016absence,
  title={Absence of many-body mobility edges},
  author={De Roeck, Wojciech and Huveneers, Francois and M{\"u}ller, Markus and Schiulaz, Mauro},
  journal={Physical Review B},
  volume={93},
  number={1},
  pages={014203},
  year={2016},
  publisher={APS}
}

@article{li2015many,
  title={Many-body localization and quantum nonergodicity in a model with a single-particle mobility edge},
  author={Li, Xiaopeng and Ganeshan, Sriram and Pixley, JH and Sarma, S Das},
  journal={Physical review letters},
  volume={115},
  number={18},
  pages={186601},
  year={2015},
  publisher={APS}
}

@article{lagendijk2009fifty,
  title={Fifty years of Anderson localization},
  author={Lagendijk, Ad and Tiggelen, Bart van and Wiersma, Diederik S},
  journal={Physics today},
  volume={62},
  number={8},
  pages={24--29},
  year={2009},
  publisher={AIP Publishing}
}


@article{Arnold1957,
	author    = {Arnold, V. I.},
	title     = {On functions of three variables},
	journal   = {Doklady Akademii Nauk SSSR},
	volume    = {114},
	pages     = {679-681},
	year      = {1957}
}

@article{Kolmogorov1957,
	author    = {Kolmogorov, A. N.},
	title     = {On the representation of continuous functions of several variables by superpositions of continuous functions of one variable and addition},
	journal   = {Doklady Akademii Nauk SSSR},
	volume    = {114},
	pages     = {953-956},
	year      = {1957}
}

@article{Liu2024,
	author    = {Liu, J. and others},
	title     = {Kolmogorov-Arnold Networks for Symbolic Regression and Time Series Prediction},
	journal   = {Journal of Machine Learning Research},
	volume    = {25},
	number    = {2},
	pages     = {95-110},
	year      = {2024}
}

@article{Dhiman2024,
	author    = {Dhiman, P.},
	title     = {Applications of Kolmogorov-Arnold Networks in Hyperspectral Image Classification},
	journal   = {Remote Sensing},
	volume    = {16},
	number    = {2},
	pages     = {205-220},
	year      = {2024}
}

@article{Cheon2024,
	author    = {Cheon, J.},
	title     = {Improving Computational Efficiency in Convolutional Kolmogorov-Arnold Networks},
	journal   = {Neural Computing and Applications},
	volume    = {37},
	number    = {1},
	pages     = {15-30},
	year      = {2024}
}

@article{Lin1993,
	author    = {Lin, J.-N. and Unbehauen, R.},
	title     = {On the realization of a Kolmogorov Network},
	journal   = {Neural Computation},
	volume    = {5},
	number    = {1},
	pages     = {18-20},
	year      = {1993}
}

@book{Menger1932,
	author    = {Menger, K.},
	title     = {Kurventheorie},
	publisher = {Teubner},
	address   = {Leipzig},
	year      = {1932}
}

@article{Diaconis1984,
	author    = {Diaconis, P. and Shahshahani, M.},
	title     = {On nonlinear functions of linear combinations},
	journal   = {SIAM Journal on Scientific Computing},
	volume    = {5},
	number    = {1},
	pages     = {175-191},
	year      = {1984}
}

@article{Braun2009,
	author    = {Braun, J. and Griebel, M.},
	title     = {On a constructive proof of Kolmogorov's superposition theorem},
	journal   = {Constructive Approximation},
	volume    = {30},
	number    = {3},
	pages     = {653-675},
	year      = {2009}
}

@misc{SpringerLink2023,
	author    = {SpringerLink},
	title     = {On functions of three variables},
	year      = {2023},
	note      = {Retrieved from \url{https://link.springer.com/article/10.1007/BF01213206}}
}

@article{Xu2024,
	author    = {Xu, L. and others},
	title     = {Time-Kolmogorov-Arnold Networks and Multi-Task Kolmogorov-Arnold Networks for Time Series Prediction},
	journal   = {Journal of Time Series Analysis},
	volume    = {45},
	number    = {3},
	pages     = {200-220},
	year      = {2024}
}

@article{Afzalaghaei2024,
	author    = {Afzalaghaei, M. and Kiamari, M.},
	title     = {rKAN: Rational Function Based Kolmogorov-Arnold Networks for Enhanced Regression and Classification},
	journal   = {Journal of Machine Learning Research},
	volume    = {25},
	number    = {3},
	pages     = {123-145},
	year      = {2024}
}

@article{Arnold1957a,
	author    = {Arnold, V. I.},
	title     = {On the representation of continuous functions of three variables by the superposition of continuous functions of two variables},
	journal   = {Mathematical Notes},
	volume    = {54},
	number    = {5},
	pages     = {235-236},
	year      = {1957}
}

@article{Babuska1978,
	author    = {Babuska, I. and Rheinboldt, W. C.},
	title     = {Error estimates for adaptive finite element computations},
	journal   = {SIAM Journal on Numerical Analysis},
	volume    = {15},
	number    = {4},
	pages     = {736-754},
	year      = {1978}
}

@article{Bozorgasl2024,
	author    = {Bozorgasl, H. and others},
	title     = {Enhancing Model Interpretability and Performance with Wavelet-Based Kolmogorov-Arnold Networks (Wav-KAN)},
	journal   = {IEEE Transactions on Neural Networks and Learning Systems},
	volume    = {35},
	number    = {1},
	pages     = {50-65},
	year      = {2024}
}

@article{Courant1928,
	author    = {Courant, R. and Friedrichs, K. and Lewy, H.},
	title     = {On the partial difference equations of mathematical physics},
	journal   = {Mathematische Annalen},
	volume    = {100},
	number    = {1},
	pages     = {32-74},
	year      = {1928}
}

@article{Cranmer2020,
	author    = {Cranmer, K. and others},
	title     = {Discovering Symbolic Models from Deep Learning with Inductive Biases},
	journal   = {Advances in Neural Information Processing Systems},
	volume    = {33},
	pages     = {17429-17442},
	year      = {2020}
}

@article{Huang2024,
	author    = {Huang, Y.},
	title     = {Improving Robustness of Deep Neural Networks with KAN-based Adversarial Training (KAT)},
	journal   = {IEEE Transactions on Artificial Intelligence},
	volume    = {9},
	number    = {4},
	pages     = {130-145},
	year      = {2024}
}

@article{Inzirillo2024,
	author    = {Inzirillo, M. and Genet, S.},
	title     = {Path Signature Integration in Kolmogorov-Arnold Networks for Time Series Analysis},
	journal   = {Journal of Time Series Analysis},
	volume    = {45},
	number    = {2},
	pages     = {100-115},
	year      = {2024}
}

@article{Jamali2024,
	author    = {Jamali, A. and others},
	title     = {Advances in Kolmogorov-Arnold Networks for Data Fitting and PDE Solving},
	journal   = {Journal of Computational Physics},
	volume    = {423},
	number    = {1},
	pages     = {145-160},
	year      = {2024}
}

@article{Kiamari2024,
	author    = {Kiamari, M. and others},
	title     = {Graph Kolmogorov-Arnold Networks: A Novel Approach for Node Classification},
	journal   = {IEEE Transactions on Neural Networks and Learning Systems},
	volume    = {35},
	number    = {6},
	pages     = {1450-1465},
	year      = {2024}
}

@article{Schmidt2009,
	author    = {Schmidt, M. and Lipson, H.},
	title     = {Distilling Free-Form Natural Laws from Experimental Data},
	journal   = {Science},
	volume    = {324},
	number    = {5923},
	pages     = {81-85},
	year      = {2009}
}

@article{Seydi2024,
	author    = {Seydi, M. and others},
	title     = {Enhancing Hyperspectral Image Classification with Wavelet-Based Kolmogorov-Arnold Networks},
	journal   = {IEEE Geoscience and Remote Sensing Letters},
	volume    = {21},
	number    = {4},
	pages     = {300-315},
	year      = {2024}
}

@article{Tikhonov1963,
	author    = {Tikhonov, A. N.},
	title     = {Solution of incorrectly formulated problems and the regularization method},
	journal   = {Soviet Mathematics Doklady},
	volume    = {4},
	pages     = {1035-1038},
	year      = {1963}
}

@article{Kundu2024,
	author    = {Kundu, Akash and Sarkar, Aritra and Sadhu, Abhishek},
	title     = {Kanqas: Kolmogorov-Arnold Network for Quantum Architecture Search},
	journal   = {arXiv preprint arXiv:2406.17630},
	year      = {2024},
	url       = {https://arxiv.org/abs/2406.17630}
}

@article{Wang2024,
	author    = {Wang, H. and others},
	title     = {SpectralKAN: Spatial-Spectral Kolmogorov-Arnold Networks for Hyperspectral Image Classification},
	journal   = {IEEE Transactions on Geoscience and Remote Sensing},
	volume    = {62},
	number    = {4},
	pages     = {500-515},
	year      = {2024}
}

@article{Li2024,
	author    = {Chenxin Li and Xinyu Liu and Wuyang Li and Cheng Wang and Hengyu Liu and Yixuan Yuan},
	title     = {U-KAN Makes Strong Backbone for Medical Image Segmentation and Generation},
	journal   = {arXiv preprint arXiv:2406.02918v2},
	year      = {2024},
	note      = {Version 2, last revised 6 Jun 2024},
	url       = {https://arxiv.org/abs/2406.02918v2}
}

@article{Galitsky2024,
	author    = {B. A. Galitsky},
	title     = {Kolmogorov-Arnold Network for Word-Level Explainable Meaning Representation},
	journal   = {Preprints},
	year      = {2024},
	note      = {Retrieved from https://www.preprints.org/manuscript/202405.1981},
	url       = {https://www.preprints.org/manuscript/202405.1981}
}

@article{Liu2024a,
	author    = {Ziming Liu and Pingchuan Ma and Yixuan Wang and Wojciech Matusik and Max Tegmark},
	title     = {KAN 2.0: Kolmogorov-Arnold Networks Meet Science},
	journal   = {arXiv preprint arXiv:2408.10205},
	year      = {2024},
	url       = {https://arxiv.org/abs/2408.10205}
}

@article{Alter2024,
	author    = {Tal Alter and Raz Lapid and Moshe Sipper},
	title     = {On the Robustness of Kolmogorov-Arnold Networks: An Adversarial Perspective},
	journal   = {arXiv preprint arXiv:2408.13809},
	year      = {2024},
	url       = {https://arxiv.org/abs/2408.13809}
}

@misc{zeng2024kanversusmlpirregular,
	title     = {KAN versus MLP on Irregular or Noisy Functions},
	author    = {Chen Zeng and Jiahui Wang and Haoran Shen and Qiao Wang},
	year      = {2024},
	eprint    = {2408.07906},
	archivePrefix = {arXiv},
	primaryClass  = {cs.LG},
	url       = {https://arxiv.org/abs/2408.07906}
}

@misc{sasse2024evaluatingfederatedkolmogorovarnoldnetworks,
	title     = {Evaluating Federated Kolmogorov-Arnold Networks on Non-IID Data},
	author    = {Arthur Mendonça Sasse and Claudio Miceli de Farias},
	year      = {2024},
	eprint    = {2410.08961},
	archivePrefix = {arXiv},
	primaryClass  = {cs.LG},
	url       = {https://arxiv.org/abs/2410.08961}
}

@misc{shukla2024comprehensivefaircomparisonmlp,
	title     = {A comprehensive and FAIR comparison between MLP and KAN representations for differential equations and operator networks},
	author    = {Khemraj Shukla and Juan Diego Toscano and Zhicheng Wang and Zongren Zou and George Em Karniadakis},
	year      = {2024},
	eprint    = {2406.02917},
	archivePrefix = {arXiv},
	primaryClass  = {cs.LG},
	url       = {https://arxiv.org/abs/2406.02917}
}

@misc{pourkamalianaraki2024kolmogorovarnoldnetworkslowdataregimes,
	title     = {Kolmogorov-Arnold Networks in Low-Data Regimes: A Comparative Study with Multilayer Perceptrons},
	author    = {Farhad Pourkamali-Anaraki},
	year      = {2024},
	eprint    = {2409.10463},
	archivePrefix = {arXiv},
	primaryClass  = {cs.LG},
	url       = {https://arxiv.org/abs/2409.10463}
}

@misc{guo2024kanvsmlpoffline,
	title     = {KAN v.s. MLP for Offline Reinforcement Learning},
	author    = {Haihong Guo and Fengxin Li and Jiao Li and Hongyan Liu},
	year      = {2024},
	eprint    = {2409.09653},
	archivePrefix = {arXiv},
	primaryClass  = {cs.LG},
	url       = {https://arxiv.org/abs/2409.09653}
}

@misc{xu2024kaneffectiveidentifyingtracking,
	title     = {Are KAN Effective for Identifying and Tracking Concept Drift in Time Series?},
	author    = {Kunpeng Xu and Lifei Chen and Shengrui Wang},
	year      = {2024},
	eprint    = {2410.10041},
	archivePrefix = {arXiv},
	primaryClass  = {cs.LG},
	url       = {https://arxiv.org/abs/2410.10041}
}

@misc{genet2024tkantemporalkolmogorovarnoldnetworks,
	title     = {TKAN: Temporal Kolmogorov-Arnold Networks},
	author    = {Remi Genet and Hugo Inzirillo},
	year      = {2024},
	eprint    = {2405.07344},
	archivePrefix = {arXiv},
	primaryClass  = {cs.LG},
	url       = {https://arxiv.org/abs/2405.07344}
}

@misc{inzirillo2024sigkansignatureweightedkolmogorovarnoldnetworks,
	title     = {SigKAN: Signature-Weighted Kolmogorov-Arnold Networks for Time Series},
	author    = {Hugo Inzirillo and Remi Genet},
	year      = {2024},
	eprint    = {2406.17890},
	archivePrefix = {arXiv},
	primaryClass  = {cs.LG},
	url       = {https://arxiv.org/abs/2406.17890}
}

@misc{xu2024kolmogorovarnoldnetworkstimeseries,
	title     = {Kolmogorov-Arnold Networks for Time Series: Bridging Predictive Power and Interpretability},
	author    = {Kunpeng Xu and Lifei Chen and Shengrui Wang},
	year      = {2024},
	eprint    = {2406.02496},
	archivePrefix = {arXiv},
	primaryClass  = {cs.LG},
	url       = {https://arxiv.org/abs/2406.02496}
}

@misc{bandyopadhyay2024kolmogorovarnoldneuralnetworkshighentropy,
	title     = {Kolmogorov-Arnold Neural Networks for High-Entropy Alloys Design},
	author    = {Yagnik Bandyopadhyay and Harshil Avlani and Houlong L. Zhuang},
	year      = {2024},
	eprint    = {2410.08452},
	archivePrefix = {arXiv},
	primaryClass  = {cond-mat.mtrl-sci},
	url       = {https://arxiv.org/abs/2410.08452}
}

@misc{polomolina2024monokancertifiedmonotonickolmogorovarnold,
	title     = {MonoKAN: Certified Monotonic Kolmogorov-Arnold Network},
	author    = {Alejandro Polo-Molina and David Alfaya and Jose Portela},
	year      = {2024},
	eprint    = {2409.11078},
	archivePrefix = {arXiv},
	primaryClass  = {cs.LG},
	url       = {https://arxiv.org/abs/2409.11078}
}

@misc{montanelli2020errorboundsdeeprelu,
	title     = {Error bounds for deep ReLU networks using the Kolmogorov--Arnold superposition theorem},
	author    = {Hadrien Montanelli and Haizhao Yang},
	year      = {2020},
	eprint    = {1906.11945},
	archivePrefix = {arXiv},
	primaryClass  = {math.NA},
	url       = {https://arxiv.org/abs/1906.11945}
}

@misc{cheon2024demonstratingefficacykolmogorovarnoldnetworks,
	title     = {Demonstrating the Efficacy of Kolmogorov-Arnold Networks in Vision Tasks},
	author    = {Minjong Cheon},
	year      = {2024},
	eprint    = {2406.14916},
	archivePrefix = {arXiv},
	primaryClass  = {cs.CV},
	url       = {https://arxiv.org/abs/2406.14916}
}

@misc{tran2024exploringlimitationskolmogorovarnoldnetworks,
	title     = {Exploring the Limitations of Kolmogorov-Arnold Networks in Classification: Insights to Software Training and Hardware Implementation},
	author    = {Van Duy Tran and Tran Xuan Hieu Le and Thi Diem Tran and Hoai Luan Pham and Vu Trung Duong Le and Tuan Hai Vu and Van Tinh Nguyen and Yasuhiko Nakashima},
	year      = {2024},
	eprint    = {2407.17790},
	archivePrefix = {arXiv},
	primaryClass  = {cs.LG},
	url       = {https://arxiv.org/abs/2407.17790}
}

@misc{poeta2024benchmarkingstudykolmogorovarnoldnetworks,
	title     = {A Benchmarking Study of Kolmogorov-Arnold Networks on Tabular Data},
	author    = {Eleonora Poeta and Flavio Giobergia and Eliana Pastor and Tania Cerquitelli and Elena Baralis},
	year      = {2024},
	eprint    = {2406.14529},
	archivePrefix = {arXiv},
	primaryClass  = {cs.LG},
	url       = {https://arxiv.org/abs/2406.14529}
}

@misc{yang2024endowinginterpretabilityneuralcognitive,
	title     = {Endowing Interpretability for Neural Cognitive Diagnosis by Efficient Kolmogorov-Arnold Networks},
	author    = {Shangshang Yang and Linrui Qin and Xiaoshan Yu},
	year      = {2024},
	eprint    = {2405.14399},
	archivePrefix = {arXiv},
	primaryClass  = {cs.LG},
	url       = {https://arxiv.org/abs/2405.14399}
}

@misc{hassan2024bayesiankolmogorovarnoldnetworks,
	title     = {Bayesian Kolmogorov Arnold Networks ($Bayesian_KANs$): A Probabilistic Approach to Enhance Accuracy and Interpretability},
	author    = {Masoud Muhammed Hassan},
	year      = {2024},
	eprint    = {2408.02706},
	archivePrefix = {arXiv},
	primaryClass  = {cs.LG},
	url       = {https://arxiv.org/abs/2408.02706}
}

@misc{knottenbelt2024coxkankolmogorovarnoldnetworksinterpretable,
	title     = {CoxKAN: Kolmogorov-Arnold Networks for Interpretable, High-Performance Survival Analysis},
	author    = {William Knottenbelt and Zeyu Gao and Rebecca Wray and Woody Zhidong Zhang and Jiashuai Liu and Mireia Crispin-Ortuzar},
	year      = {2024},
	eprint    = {2409.04290},
	archivePrefix = {arXiv},
	primaryClass  = {cs.LG},
	url       = {https://arxiv.org/abs/2409.04290}
}

@misc{amouri2024enhancingintrusiondetectioniot,
	title     = {Enhancing Intrusion Detection in IoT Environments: An Advanced Ensemble Approach Using Kolmogorov-Arnold Networks},
	author    = {Amar Amouri and Mohamad Mahmoud Al Rahhal and Yakoub Bazi and Ismail Butun and Imad Mahgoub},
	year      = {2024},
	eprint    = {2408.15886},
	archivePrefix = {arXiv},
	primaryClass  = {cs.CR},
	url       = {https://arxiv.org/abs/2408.15886}
}

@misc{vacarubio2024kolmogorovarnoldnetworkskanstime,
	title     = {Kolmogorov-Arnold Networks (KANs) for Time Series Analysis},
	author    = {Cristian J. Vaca-Rubio and Luis Blanco and Roberto Pereira and Màrius Caus},
	year      = {2024},
	eprint    = {2405.08790},
	archivePrefix = {arXiv},
	primaryClass  = {eess.SP},
	url       = {https://arxiv.org/abs/2405.08790}
}

@misc{tang20243dukanimplementationmultimodal,
	title     = {3D U-KAN Implementation for Multi-modal MRI Brain Tumor Segmentation},
	author    = {Tianze Tang and Yanbing Chen and Hai Shu},
	year      = {2024},
	eprint    = {2408.00273},
	archivePrefix = {arXiv},
	primaryClass  = {eess.IV},
	url       = {https://arxiv.org/abs/2408.00273}
}

@misc{jahin2024kacqdcnnuncertaintyawareinterpretablekolmogorovarnold,
	title     = {KACQ-DCNN: Uncertainty-Aware Interpretable Kolmogorov-Arnold Classical-Quantum Dual-Channel Neural Network for Heart Disease Detection},
	author    = {Md Abrar Jahin and Md. Akmol Masud and M. F. Mridha and Zeyar Aung and Nilanjan Dey},
	year      = {2024},
	eprint    = {2410.07446},
	archivePrefix = {arXiv},
	primaryClass  = {cs.LG},
	url       = {https://arxiv.org/abs/2410.07446}
}

@misc{lee2024hippokanefficientkanmodel,
	title     = {HiPPO-KAN: Efficient KAN Model for Time Series Analysis},
	author    = {SangJong Lee and Jin-Kwang Kim and JunHo Kim and TaeHan Kim and James Lee},
	year      = {2024},
	eprint    = {2410.14939},
	archivePrefix = {arXiv},
	primaryClass  = {cs.LG},
	url       = {https://arxiv.org/abs/2410.14939}
}

@misc{zhou2024kanadtimeseriesanomaly,
	title     = {KAN-AD: Time Series Anomaly Detection with Kolmogorov-Arnold Networks},
	author    = {Quan Zhou and Changhua Pei and Fei Sun and Jing Han and Zhengwei Gao and Dan Pei and Haiming Zhang and Gaogang Xie and Jianhui Li},
	year      = {2024},
	eprint    = {2411.00278},
	archivePrefix = {arXiv},
	primaryClass  = {cs.LG},
	url       = {https://arxiv.org/abs/2411.00278}
}

@misc{mahara2024dawnkanimagetoimagei2i,
	title     = {The Dawn of KAN in Image-to-Image (I2I) Translation: Integrating Kolmogorov-Arnold Networks with GANs for Unpaired I2I Translation},
	author    = {Arpan Mahara and Naphtali D. Rishe and Liangdong Deng},
	year      = {2024},
	eprint    = {2408.08216},
	archivePrefix = {arXiv},
	primaryClass  = {cs.CV},
	url       = {https://arxiv.org/abs/2408.08216}
}

@misc{ning2024kandark,
	title     = {KAN See In the Dark},
	author    = {Aoxiang Ning and Minglong Xue and Jinhong He and Chengyun Song},
	year      = {2024},
	eprint    = {2409.03404},
	archivePrefix = {arXiv},
	primaryClass  = {cs.CV},
	url       = {https://arxiv.org/abs/2409.03404}
}

@misc{yu2024residualkolmogorovarnoldnetworkenhanced,
	title     = {Residual Kolmogorov-Arnold Network for Enhanced Deep Learning},
	author    = {Ray Congrui Yu and Sherry Wu and Jiang Gui},
	year      = {2024},
	eprint    = {2410.05500},
	archivePrefix = {arXiv},
	primaryClass  = {cs.CV},
	url       = {https://arxiv.org/abs/2410.05500}
}

@misc{xu2024fourierkangcffourierkolmogorovarnoldnetwork,
	title     = {FourierKAN-GCF: Fourier Kolmogorov-Arnold Network -- An Effective and Efficient Feature Transformation for Graph Collaborative Filtering},
	author    = {Jinfeng Xu and Zheyu Chen and Jinze Li and Shuo Yang and Wei Wang and Xiping Hu and Edith C. -H. Ngai},
	year      = {2024},
	eprint    = {2406.01034},
	archivePrefix = {arXiv},
	primaryClass  = {cs.IR},
	url       = {https://arxiv.org/abs/2406.01034}
}

@misc{decarlo2024kolmogorovarnoldgraphneuralnetworks,
	title     = {Kolmogorov-Arnold Graph Neural Networks},
	author    = {Gianluca De Carlo and Andrea Mastropietro and Aris Anagnostopoulos},
	year      = {2024},
	eprint    = {2406.18354},
	archivePrefix = {arXiv},
	primaryClass  = {cs.LG},
	url       = {https://arxiv.org/abs/2406.18354}
}

@misc{park2024cfkankolmogorovarnoldnetworkbasedcollaborative,
	title     = {CF-KAN: Kolmogorov-Arnold Network-based Collaborative Filtering to Mitigate Catastrophic Forgetting in Recommender Systems},
	author    = {Jin-Duk Park and Kyung-Min Kim and Won-Yong Shin},
	year      = {2024},
	eprint    = {2409.05878},
	archivePrefix = {arXiv},
	primaryClass  = {cs.IR},
	url       = {https://arxiv.org/abs/2409.05878}
}

@misc{abueidda2024deepokandeepoperatornetwork,
	title     = {DeepOKAN: Deep Operator Network Based on Kolmogorov Arnold Networks for Mechanics Problems},
	author    = {Diab W. Abueidda and Panos Pantidis and Mostafa E. Mobasher},
	year      = {2024},
	eprint    = {2405.19143},
	archivePrefix = {arXiv},
	primaryClass  = {cs.CE},
	url       = {https://arxiv.org/abs/2405.19143}
}

@misc{wang2024kolmogorovarnoldinformedneural,
	title     = {Kolmogorov Arnold Informed neural network: A physics-informed deep learning framework for solving forward and inverse problems based on Kolmogorov Arnold Networks},
	author    = {Yizheng Wang and Jia Sun and Jinshuai Bai and Cosmin Anitescu and Mohammad Sadegh Eshaghi and Xiaoying Zhuang and Timon Rabczuk and Yinghua Liu},
	year      = {2024},
	eprint    = {2406.11045},
	archivePrefix = {arXiv},
	primaryClass  = {cs.LG},
	url       = {https://arxiv.org/abs/2406.11045}
}

@misc{shuai2024physicsinformedkolmogorovarnoldnetworkspower,
	title     = {Physics-Informed Kolmogorov-Arnold Networks for Power System Dynamics},
	author    = {Hang Shuai and Fangxing Li},
	year      = {2024},
	eprint    = {2408.06650},
	archivePrefix = {arXiv},
	primaryClass  = {eess.SY},
	url       = {https://arxiv.org/abs/2408.06650}
}

@misc{ss2024chebyshevpolynomialbasedkolmogorovarnoldnetworks,
	title     = {Chebyshev Polynomial-Based Kolmogorov-Arnold Networks: An Efficient Architecture for Nonlinear Function Approximation},
	author    = {Sidharth SS and Keerthana AR and Gokul R and Anas KP},
	year      = {2024},
	eprint    = {2405.07200},
	archivePrefix = {arXiv},
	primaryClass  = {cs.LG},
	url       = {https://arxiv.org/abs/2405.07200}
}

@misc{mostajeran2024epickanselastoplasticityinformedkolmogorovarnold,
	title     = {EPi-cKANs: Elasto-Plasticity Informed Kolmogorov-Arnold Networks Using Chebyshev Polynomials},
	author    = {Farinaz Mostajeran and Salah A Faroughi},
	year      = {2024},
	eprint    = {2410.10897},
	archivePrefix = {arXiv},
	primaryClass  = {cond-mat.mtrl-sci},
	url       = {https://arxiv.org/abs/2410.10897}
}

@misc{bozorgasl2024wavkanwaveletkolmogorovarnoldnetworks,
	title     = {Wav-KAN: Wavelet Kolmogorov-Arnold Networks},
	author    = {Zavareh Bozorgasl and Hao Chen},
	year      = {2024},
	eprint    = {2405.12832},
	archivePrefix = {arXiv},
	primaryClass  = {cs.LG},
	url       = {https://arxiv.org/abs/2405.12832}
}

@misc{nehma2024leveragingkansenhanceddeep,
	title     = {Leveraging KANs For Enhanced Deep Koopman Operator Discovery},
	author    = {George Nehma and Madhur Tiwari},
	year      = {2024},
	eprint    = {2406.02875},
	archivePrefix = {arXiv},
	primaryClass  = {cs.LG},
	url       = {https://arxiv.org/abs/2406.02875}
}

@misc{kashefi2024pointnetkanversuspointnet,
	title     = {PointNet with KAN versus PointNet with MLP for 3D Classification and Segmentation of Point Sets},
	author    = {Ali Kashefi},
	year      = {2024},
	eprint    = {2410.10084},
	archivePrefix = {arXiv},
	primaryClass  = {cs.CV},
	url       = {https://arxiv.org/abs/2410.10084}
}

@misc{liu2024baseflowidentificationexplainableai,
	title     = {Baseflow identification via explainable AI with Kolmogorov-Arnold networks},
	author    = {Chuyang Liu and Tirthankar Roy and Daniel M. Tartakovsky and Dipankar Dwivedi},
	year      = {2024},
	eprint    = {2410.11587},
	archivePrefix = {arXiv},
	primaryClass  = {cs.LG},
	url       = {https://arxiv.org/abs/2410.11587}
}

@misc{cheon2024kolmogorovarnoldnetworksatelliteimage,
	title     = {Kolmogorov-Arnold Network for Satellite Image Classification in Remote Sensing},
	author    = {Minjong Cheon},
	year      = {2024},
	eprint    = {2406.00600},
	archivePrefix = {arXiv},
	primaryClass  = {cs.CV},
	url       = {https://arxiv.org/abs/2406.00600}
}

@misc{yu2024exploringkolmogorovarnoldnetworksrealistic,
	title     = {Exploring Kolmogorov-Arnold networks for realistic image sharpness assessment},
	author    = {Shaode Yu and Ze Chen and Zhimu Yang and Jiacheng Gu and Bizu Feng},
	year      = {2024},
	eprint    = {2409.07762},
	archivePrefix = {arXiv},
	primaryClass  = {cs.CV},
	url       = {https://arxiv.org/abs/2409.07762}
}

@misc{wang2024cestkankolmogorovarnoldnetworkscest,
	title     = {CEST-KAN: Kolmogorov-Arnold Networks for CEST MRI Data Analysis},
	author    = {Jiawen Wang and Pei Cai and Ziyan Wang and Huabin Zhang and Jianpan Huang},
	year      = {2024},
	eprint    = {2406.16026},
	archivePrefix = {arXiv},
	primaryClass  = {physics.med-ph},
	url       = {https://arxiv.org/abs/2406.16026}
}

@misc{yang2024kolmogorovarnoldtransformer,
	title     = {Kolmogorov-Arnold Transformer},
	author    = {Xingyi Yang and Xinchao Wang},
	year      = {2024},
	eprint    = {2409.10594},
	archivePrefix = {arXiv},
	primaryClass  = {cs.LG},
	url       = {https://arxiv.org/abs/2409.10594}
}

@misc{inzirillo2024gatedresidualkolmogorovarnoldnetworks,
	title     = {A Gated Residual Kolmogorov-Arnold Networks for Mixtures of Experts},
	author    = {Hugo Inzirillo and Remi Genet},
	year      = {2024},
	eprint    = {2409.15161},
	archivePrefix = {arXiv},
	primaryClass  = {cs.LG},
	url       = {https://arxiv.org/abs/2409.15161}
}

@misc{gao2024convergencestochasticgradientdescent,
	title     = {On the Convergence of (Stochastic) Gradient Descent for Kolmogorov--Arnold Networks},
	author    = {Yihang Gao and Vincent Y. F. Tan},
	year      = {2024},
	eprint    = {2410.08041},
	archivePrefix = {arXiv},
	primaryClass  = {cs.LG},
	url       = {https://arxiv.org/abs/2410.08041}
}

@misc{wang2024expressivenessspectralbiaskans,
	title     = {On the expressiveness and spectral bias of KANs},
	author    = {Yixuan Wang and Jonathan W. Siegel and Ziming Liu and Thomas Y. Hou},
	year      = {2024},
	eprint    = {2410.01803},
	archivePrefix = {arXiv},
	primaryClass  = {cs.LG},
	url       = {https://arxiv.org/abs/2410.01803}
}

@misc{aghaei2024rkanrationalkolmogorovarnoldnetworks,
	title     = {rKAN: Rational Kolmogorov-Arnold Networks},
	author    = {Alireza Afzal Aghaei},
	year      = {2024},
	eprint    = {2406.14495},
	archivePrefix = {arXiv},
	primaryClass  = {cs.LG},
	url       = {https://arxiv.org/abs/2406.14495}
}

@misc{qiu2024relukannewkolmogorovarnoldnetworks,
	title     = {ReLU-KAN: New Kolmogorov-Arnold Networks that Only Need Matrix Addition, Dot Multiplication, and ReLU},
	author    = {Qi Qiu and Tao Zhu and Helin Gong and Liming Chen and Huansheng Ning},
	year      = {2024},
	eprint    = {2406.02075},
	archivePrefix = {arXiv},
	primaryClass  = {cs.LG},
	url       = {https://arxiv.org/abs/2406.02075}
}

@misc{reinhardt2024sinekankolmogorovarnoldnetworksusing,
	title     = {SineKAN: Kolmogorov-Arnold Networks Using Sinusoidal Activation Functions},
	author    = {Eric A. F. Reinhardt and P. R. Dinesh and Sergei Gleyzer},
	year      = {2024},
	eprint    = {2407.04149},
	archivePrefix = {arXiv},
	primaryClass  = {cs.LG},
	url       = {https://arxiv.org/abs/2407.04149}
}

@misc{moradzadeh2024ukanunboundkolmogorovarnoldnetwork,
	title     = {UKAN: Unbound Kolmogorov-Arnold Network Accompanied with Accelerated Library},
	author    = {Alireza Moradzadeh and Lukasz Wawrzyniak and Miles Macklin and Saee G. Paliwal},
	year      = {2024},
	eprint    = {2408.11200},
	archivePrefix = {arXiv},
	primaryClass  = {cs.LG},
	url       = {https://arxiv.org/abs/2408.11200}
}

@misc{so2024higherorderrelukanshrkanssolvingphysicsinformed,
	title     = {Higher-order-ReLU-KANs (HRKANs) for solving physics-informed neural networks (PINNs) more accurately, robustly and faster},
	author    = {Chi Chiu So and Siu Pang Yung},
	year      = {2024},
	eprint    = {2409.14248},
	archivePrefix = {arXiv},
	primaryClass  = {cs.NE},
	url       = {https://arxiv.org/abs/2409.14248}
}

@misc{yang2024activationspaceselectablekolmogorovarnold,
	title     = {Activation Space Selectable Kolmogorov-Arnold Networks},
	author    = {Zhuoqin Yang and Jiansong Zhang and Xiaoling Luo and Zheng Lu and Linlin Shen},
	year      = {2024},
	eprint    = {2408.08338},
	archivePrefix = {arXiv},
	primaryClass  = {cs.LG},
	url       = {https://arxiv.org/abs/2408.08338}
}
@misc{qiu2024powermlpefficientversionkan,
	title={PowerMLP: An Efficient Version of KAN}, 
	author={Ruichen Qiu and Yibo Miao and Shiwen Wang and Lijia Yu and Yifan Zhu and Xiao-Shan Gao},
	year={2024},
	eprint={2412.13571},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2412.13571}, 
}
@misc{kim2024cikanconstraintinformedkolmogorovarnold,
	title={CIKAN: Constraint Informed Kolmogorov-Arnold Networks for Autonomous Spacecraft Rendezvous using Time Shift Governor}, 
	author={Taehyeun Kim and Anouck Girard and Ilya Kolmanovsky},
	year={2024},
	eprint={2412.03710},
	archivePrefix={arXiv},
	primaryClass={eess.SY},
	url={https://arxiv.org/abs/2412.03710}, 
}
@misc{mohan2024kanscomputervisionexperimental,
	title={KANs for Computer Vision: An Experimental Study}, 
	author={Karthik Mohan and Hanxiao Wang and Xiatian Zhu},
	year={2024},
	eprint={2411.18224},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2411.18224}, 
}
@misc{han2024kanface,
	title={KAN See Your Face}, 
	author={Dong Han and Yong Li and Joachim Denzler},
	year={2024},
	eprint={2411.18165},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2411.18165}, 
}
@misc{cang2024kanworkexploringpotential,
	title={Can KAN Work? Exploring the Potential of Kolmogorov-Arnold Networks in Computer Vision}, 
	author={Yueyang Cang and Yu hang liu and Li Shi},
	year={2024},
	eprint={2411.06727},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2411.06727}, 
}
@misc{aghaomidi2024ecgsleepnetdeeplearningbasedcomprehensive,
	title={ECG-SleepNet: Deep Learning-Based Comprehensive Sleep Stage Classification Using ECG Signals}, 
	author={Poorya Aghaomidi and Ge Wang},
	year={2024},
	eprint={2412.01929},
	archivePrefix={arXiv},
	primaryClass={cs.AI},
	url={https://arxiv.org/abs/2412.01929}, 
}
@misc{deng2025mvketrchestctreport,
	title={MvKeTR: Chest CT Report Generation with Multi-View Perception and Knowledge Enhancement}, 
	author={Xiwei Deng and Xianchun He and Jiangfeng Bao and Yudan Zhou and Shuhui Cai and Congbo Cai and Zhong Chen},
	year={2025},
	eprint={2411.18309},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2411.18309}, 
}
@misc{agrawal2024kanmambafusionnetredefiningmedical,
	title={KAN-Mamba FusionNet: Redefining Medical Image Segmentation with Non-Linear Modeling}, 
	author={Akansh Agrawal and Akshan Agrawal and Shashwat Gupta and Priyanka Bagade},
	year={2024},
	eprint={2411.11926},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2411.11926}, 
}
@misc{chen2024usingstructuralsimilaritykolmogorovarnold,
	title={Using Structural Similarity and Kolmogorov-Arnold Networks for Anatomical Embedding of 3-hinge Gyrus}, 
	author={Minheng Chen and Chao Cao and Tong Chen and Yan Zhuang and Jing Zhang and Yanjun Lyu and Xiaowei Yu and Lu Zhang and Tianming Liu and Dajiang Zhu},
	year={2024},
	eprint={2410.23598},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2410.23598}, 
}








@misc{Blealtan_2024, type={Python}, title={Blealtan/efficient-kan}, rights={MIT}, url={https://github.com/Blealtan/efficient-kan}, abstractNote={An efficient pure-PyTorch implementation of Kolmogorov-Arnold Network (KAN).}, author={Blealtan}, year={2024}, month=nov }
 @article{Braun_Griebel_2009a, title={On a Constructive Proof of Kolmogorov’s Superposition Theorem}, volume={30}, rights={http://www.springer.com/tdm}, ISSN={0176-4276, 1432-0940}, DOI={10.1007/s00365-009-9054-2}, number={3}, journal={Constructive Approximation}, author={Braun, Jürgen and Griebel, Michael}, year={2009}, month=dec, pages={653–675}, language={en} }
 @article{Braun_Griebel_2009b, title={On a Constructive Proof of Kolmogorov’s Superposition Theorem}, volume={30}, rights={http://www.springer.com/tdm}, ISSN={0176-4276, 1432-0940}, DOI={10.1007/s00365-009-9054-2}, abstractNote={Kolmogorov showed in [14] that any multivariate continuous function can be represented as a superposition of one–dimensional functions, i.e.}, number={3}, journal={Constructive Approximation}, author={Braun, Jürgen and Griebel, Michael}, year={2009}, month=dec, pages={653–675}, language={en} }
 @inproceedings{Goldblum_Finzi_Rowan_Wilson_2024, title={Position: The No Free Lunch Theorem, Kolmogorov Complexity, and the Role of Inductive Biases in Machine Learning}, ISSN={2640-3498}, url={https://proceedings.mlr.press/v235/goldblum24a.html}, abstractNote={No free lunch theorems for supervised learning state that no learner can solve all problems or that all learners achieve exactly the same accuracy on average over a uniform distribution on learning problems. Accordingly, these theorems are often referenced in support of the notion that individual problems require specially tailored inductive biases. While virtually all uniformly sampled datasets have high complexity, real-world problems disproportionately generate low-complexity data, and we argue that neural network models share this same preference, formalized using Kolmogorov complexity. Notably, we show that architectures designed for a particular domain, such as computer vision, can compress datasets on a variety of seemingly unrelated domains. Our experiments show that pre-trained and even randomly initialized language models prefer to generate low-complexity sequences. Whereas no free lunch theorems seemingly indicate that individual problems require specialized learners, we explain how tasks that often require human intervention such as picking an appropriately sized model when labeled data is scarce or plentiful can be automated into a single learning algorithm. These observations justify the trend in deep learning of unifying seemingly disparate problems with an increasingly small set of machine learning models.}, booktitle={Proceedings of the 41st International Conference on Machine Learning}, publisher={PMLR}, author={Goldblum, Micah and Finzi, Marc Anton and Rowan, Keefer and Wilson, Andrew Gordon}, year={2024}, month=jul, pages={15788–15808}, language={en} }
 @article{Guilhoto_Perdikaris_2024, title={Deep Learning Alternatives of the Kolmogorov Superposition Theorem}, url={http://arxiv.org/abs/2410.01990}, DOI={10.48550/arXiv.2410.01990}, abstractNote={This paper explores alternative formulations of the Kolmogorov Superposition Theorem (KST) as a foundation for neural network design. The original KST formulation, while mathematically elegant, presents practical challenges due to its limited insight into the structure of inner and outer functions and the large number of unknown variables it introduces. Kolmogorov-Arnold Networks (KANs) leverage KST for function approximation, but they have faced scrutiny due to mixed results compared to traditional multilayer perceptrons (MLPs) and practical limitations imposed by the original KST formulation. To address these issues, we introduce ActNet, a scalable deep learning model that builds on the KST and overcomes many of the drawbacks of Kolmogorov’s original formulation. We evaluate ActNet in the context of Physics-Informed Neural Networks (PINNs), a framework well-suited for leveraging KST’s strengths in low-dimensional function approximation, particularly for simulating partial differential equations (PDEs). In this challenging setting, where models must learn latent functions without direct measurements, ActNet consistently outperforms KANs across multiple benchmarks and is competitive against the current best MLP-based approaches. These results present ActNet as a promising new direction for KST-based deep learning applications, particularly in scientific computing and PDE simulation tasks.}, note={arXiv:2410.01990}, number={arXiv:2410.01990}, publisher={arXiv}, author={Guilhoto, Leonardo Ferreira and Perdikaris, Paris}, year={2024}, month=oct }
 @article{Ji_Hou_Zhang_2024, title={A Comprehensive Survey on Kolmogorov Arnold Networks (KAN)}, url={http://arxiv.org/abs/2407.11075}, DOI={10.48550/arXiv.2407.11075}, abstractNote={Through this comprehensive survey of Kolmogorov-Arnold Networks (KAN), we have gained a thorough understanding of its theoretical foundation, architectural design, application scenarios, and current research progress. KAN, with its unique architecture and ﬂexible activation functions, excels in handling complex data patterns and nonlinear relationships, demonstrating wide-ranging application potential. While challenges remain, KAN is poised to pave the way for innovative solutions in various ﬁelds, potentially revolutionizing how we approach complex computational problems.}, note={arXiv:2407.11075 [cs]}, number={arXiv:2407.11075}, publisher={arXiv}, author={Ji, Tianrui and Hou, Yuntian and Zhang, Di}, year={2024}, month=dec, language={en} }
 @misc{Lai_Shen_2021, title={The Kolmogorov Superposition Theorem can Break the Curse of Dimensionality When Approximating High Dimensional Functions}, url={https://arxiv.org/abs/2112.09963v4}, abstractNote={We explain how to use Kolmogorov Superposition Theorem (KST) to break the curse of dimensionality when approximating a dense class of multivariate continuous functions. We first show that there is a class of functions called $K$-Lipschitz continuous in $C([0,1]^d)$ which can be approximated by a special ReLU neural network of two hidden layers with a dimension independent approximation rate $O(n^{-1})$ with approximation constant increasing quadratically in $d$. The number of parameters used in such neural network approximation equals to $(6d+2)n$. Next we introduce KB-splines by using linear B-splines to replace the K-outer function and smooth the KB-splines to have the so-called LKB-splines as the basis for approximation. Our numerical evidence shows that the curse of dimensionality is broken in the following sense: When using the standard discrete least squares (DLS) method to approximate a continuous function, there exists a pivotal set of points in $[0,1]^d$ with size at most $O(nd)$ such that the rooted mean squares error (RMSE) from the DLS based on the pivotal set is similar to the RMSE of the DLS based on the original set with size $O(n^d)$. In addition, by using matrix cross approximation technique, the number of LKB-splines used for approximation is the same as the size of the pivotal data set. Therefore, we do not need too many basis functions as well as too many function values to approximate a high dimensional continuous function $f$.}, journal={arXiv.org}, author={Lai, Ming-Jun and Shen, Zhaiming}, year={2021}, month=dec, language={en} }
 @article{Li_2024, title={Kolmogorov-Arnold Networks are Radial Basis Function Networks}, url={http://arxiv.org/abs/2405.06721}, DOI={10.48550/arXiv.2405.06721}, abstractNote={This short paper is a fast proof-of-concept that the 3-order B-splines used in Kolmogorov-Arnold Networks (KANs) can be well approximated by Gaussian radial basis functions. Doing so leads to FastKAN, a much faster implementation of KAN which is also a radial basis function (RBF) network.}, note={arXiv:2405.06721}, number={arXiv:2405.06721}, publisher={arXiv}, author={Li, Ziyao}, year={2024}, month=may }
 @article{Schmidt-Hieber_2021, title={The Kolmogorov–Arnold representation theorem revisited}, volume={137}, ISSN={08936080}, DOI={10.1016/j.neunet.2021.01.020}, journal={Neural Networks}, author={Schmidt-Hieber, Johannes}, year={2021}, month=may, pages={119–126}, language={en} }
 @article{SS_AR_R_KP_2024, title={Chebyshev Polynomial-Based Kolmogorov-Arnold Networks: An Efficient Architecture for Nonlinear Function Approximation}, url={http://arxiv.org/abs/2405.07200}, DOI={10.48550/arXiv.2405.07200}, abstractNote={Accurate approximation of complex nonlinear functions is a fundamental challenge across many scientific and engineering domains. Traditional neural network architectures, such as Multi-Layer Perceptrons (MLPs), often struggle to efficiently capture intricate patterns and irregularities present in high-dimensional functions. This paper presents the Chebyshev Kolmogorov-Arnold Network (Chebyshev KAN), a new neural network architecture inspired by the Kolmogorov-Arnold representation theorem, incorporating the powerful approximation capabilities of Chebyshev polynomials. By utilizing learnable functions parametrized by Chebyshev polynomials on the network’s edges, Chebyshev KANs enhance flexibility, efficiency, and interpretability in function approximation tasks. We demonstrate the efficacy of Chebyshev KANs through experiments on digit classification, synthetic function approximation, and fractal function generation, highlighting their superiority over traditional MLPs in terms of parameter efficiency and interpretability. Our comprehensive evaluation, including ablation studies, confirms the potential of Chebyshev KANs to address longstanding challenges in nonlinear function approximation, paving the way for further advancements in various scientific and engineering applications.}, note={arXiv:2405.07200}, number={arXiv:2405.07200}, publisher={arXiv}, author={SS, Sidharth and AR, Keerthana and R, Gokul and KP, Anas}, year={2024}, month=jun }

 @article{Liu_Wang_Vaidya_Ruehle_Halverson_Soljačić_Hou_Tegmark_2024, title={KAN: Kolmogorov-Arnold Networks}, url={http://arxiv.org/abs/2404.19756}, DOI={10.48550/arXiv.2404.19756}, abstractNote={Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes (“neurons”), KANs have learnable activation functions on edges (“weights”). KANs have no linear weights at all -- every weight parameter is replaced by a univariate function parametrized as a spline. We show that this seemingly simple change makes KANs outperform MLPs in terms of accuracy and interpretability. For accuracy, much smaller KANs can achieve comparable or better accuracy than much larger MLPs in data fitting and PDE solving. Theoretically and empirically, KANs possess faster neural scaling laws than MLPs. For interpretability, KANs can be intuitively visualized and can easily interact with human users. Through two examples in mathematics and physics, KANs are shown to be useful collaborators helping scientists (re)discover mathematical and physical laws. In summary, KANs are promising alternatives for MLPs, opening opportunities for further improving today’s deep learning models which rely heavily on MLPs.}, note={arXiv:2404.19756 [cs]}, number={arXiv:2404.19756}, publisher={arXiv}, author={Liu, Ziming and Wang, Yixuan and Vaidya, Sachin and Ruehle, Fabian and Halverson, James and Soljačić, Marin and Hou, Thomas Y. and Tegmark, Max}, year={2024}, month=jun }
