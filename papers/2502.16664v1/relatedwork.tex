\section{Related Works}
\paragraph{Machine Learning Interatomic Potentials and Equivariant Architectures} 
% \tm{Feels somewhat long.}
Machine learning interatomic potentials (MLIPs) 
have emerged as powerful tools for modeling interatomic interactions in molecular and materials systems, offering a computationally efficient alternative to traditional ab initio methods. Architectures like Schnet \cite{schutt2017schnet} use continuous-filter convolutional layers to capture local atomic environments and message passing, enabling accurate predictions of molecular properties. To further enhance physical expressivity, $E(3)$-equivariant architectures \cite{thomas2018tensor} have been developed, which respect the symmetries of Euclidean space (rotations, translations, and reflections) by design. These models, such as Tensor Field Networks \cite{thomas2018tensor} and NequIP \cite{batzner2022nequip}, ensure that predictions (i.e. energy and forces) are invariant or equivariant to transformations in 3D space, making them highly data-efficient for tasks like force field prediction in molecular dynamics. 
MACE \cite{Batatia_Kovács_Simm_Ortner_Csányi_2023} is a higher-order equivariant message-passing network that enhances force field accuracy and efficiency by leveraging multi-body interactions. 
E(n)-equivariant GNNs (EGNNs) \cite{Satorras_Hoogeboom_Welling_2022} implement a higher-order representation while maintaining equivariance to rotations, translations, and permutations. 
Irreducible Cartesian Tensor Potential (ICTP)  \cite{Zaverkin_Alesiani_Maruyama_Errica_Christiansen_Takamoto_Weber_Niepert_2024} introduces irreducible Cartesian tensors for equivariant message passing, offering computational advantages over spherical harmonics in the small tensor rank regime. Tensor field networks \cite{Thomas_Smidt_Kearnes_Yang_Li_Kohlhoff_Riley_2018} and Equiformer \cite{Liao_Smidt_2023} use spherical harmonics as bases for tensors. While SO3krates \cite{Frank_Unke_Müller_Chmiela_2024a} combines sparse equivariant representations with transformers to balance accuracy and speed.
Additionally, equivariant Clifford networks \cite{ruheCliffordGroupEquivariant2023b}
extend this framework by incorporating geometric algebra to build equivariant models. 
Equivariant representations mitigate cumulative errors in molecular dynamics  \cite{Unke_Chmiela_Sauceda_Gastegger_Poltavsky_Schütt_Tkatchenko_Müller_2021},  while 
directional message passing with spherical harmonics improves angular dependency modeling as implemented in DimeNet \cite{Gasteiger_Groß_Günnemann_2022}.  
Equivariant or invariant architectures enhance data efficiency, accuracy, and physical consistency in tasks where input symmetries (e.g., rotation, reflection, translation) dictate output invariance or equivariance.
While these advancements have significantly improved the accuracy and efficiency of MLIPs for applications in chemistry, physics, and materials science, the advantage of KAN architecture has not yet been explored, we thus take a fundamental step in this direction with our study. 

\paragraph{KAN Architectures}
Kolmogorov-Arnold Networks (KANs) are inspired by the Kolmogorov-Arnold representation theorem, which provides a theoretical foundation for approximating multivariate functions using univariate functions and addition. Early work by Hecht-Nielsen (1987) \cite{hecht1987kolmogorov} introduced one of the first neural network architectures based on this theorem, demonstrating its potential for efficient function approximation. 
\cite{lai2021kolmogorov} study the approximation capability of KST-based models in high dimensions and how they could potentially break the curse of dimension \cite{poggio2022deep}. 
\cite{ferdausKANICEKolmogorovArnoldNetworks2024} propose to combine  Convolutional Neural Networks (CNNs) with Kolmogorov Arnold Network (KAN) principles.
Additionally, 
\cite{yangKolmogorovArnoldTransformer2024}
explored the integration of KAN principles into transformer models, achieving improvements in efficiency for sequence modeling tasks. 
\cite{huEKANEquivariantKolmogorovArnold2024a} propose EKAN, an approximation method for incorporating matrix group equivariance into KANs. While these studies highlight the versatility of KAN architectures in adapting to various neural network frameworks, the extension to physical and geometrical symmetries has not been fully considered.

\paragraph{Application of KAN}
KANs have been applied to a range of machine learning tasks, particularly in scenarios requiring efficient function approximation. For instance, Kůrková (1991) \cite{kurkova1991kolmogorov} demonstrated the effectiveness of KANs in high-dimensional regression problems, where traditional neural networks often struggle with scalability. In the natural language processing domain, \cite{Galitsky2024} utilized KAN for word-level explanations.  
Furthermore, 
\cite{decarlo2024kolmogorovarnoldgraphneuralnetworks}
applied KANs to graph-based learning tasks, showing that their hybrid models could achieve state-of-the-art results in graph classification and node prediction. 
% survey: \cite{somvanshiSurveyKolmogorovArnoldNetwork2024} 
KAN has been used as a function approximation to solve PDE \cite{wang2024kolmogorovarnoldinformedneural,shukla2024comprehensivefaircomparisonmlp} for both forward and backward problems with highly complex boundary and initial conditions.
\cite{aghaei2024rkanrationalkolmogorovarnoldnetworks} extends KAN with rational polynomials basis to regression and classifications problems. \cite{Seydi2024} explores using Wavelet as basis functions to model hyper-spectral data. KANs have been extended to model time-series \cite{xu2024kolmogorovarnoldnetworkstimeseries,inzirillo2024sigkansignatureweightedkolmogorovarnoldnetworks} to dynamically adapt to temporal data. 
While these, and other \cite{somvanshiSurveyKolmogorovArnoldNetwork2024},  applications highlight the practical utility of KANs in solving complex real-world problems, a significant class of molecular applications remains overlooked. 

\paragraph{Theoretical Work on KAN}
The theoretical foundations of Kolmogorov–Arnold Networks (KANs) are rooted in the Kolmogorov–Arnold representation theorem, established by Andrey Kolmogorov  \citet{kolmogorov1957representation} and later refined by Vladimir Arnold \citet{arnold1959functions}. 
Building upon this foundation, David Sprecher \citet{sprecher1965structure} and George Lorentz \citet{lorentz1976approximation} provided constructive algorithms to implement the theorem, enhancing its applicability in computational contexts. 
Recent theoretical advancements have addressed challenges in training KANs, such as non-smooth optimization landscapes. Researchers have proposed various techniques to improve the stability and convergence of KAN training, including regularization methods \cite{Braun2009constructive} like dropout and weight decay, as well as optimization strategies involving adaptive learning rates, while \cite{igelnik2003kolmogorov} have proposed using cubic spline as activation and internal function for efficient approximation.  
These contributions have been instrumental in bridging the gap between the mathematical foundations of KANs and their practical implementation in machine learning.
However, training with energies requires fitting highly non-linear functions. In this work, we demonstrate how extending the KAN architecture enhances the learning capacity of KAT-based models.