@article{Balla_Mishra-Sharma_Cuesta-Lazaro_Jaakkola_Smidt_2024, 
title={A Cosmic-Scale Benchmark for Symmetry-Preserving Data Processing}, url={http://arxiv.org/abs/2410.20516}, DOI={10.48550/arXiv.2410.20516}, abstractNote={Efficiently processing structured point cloud data while preserving multiscale information is a key challenge across domains, from graphics to atomistic modeling. Using a curated dataset of simulated galaxy positions and properties, represented as point clouds, we benchmark the ability of graph neural networks to simultaneously capture local clustering environments and long-range correlations. Given the homogeneous and isotropic nature of the Universe, the data exhibits a high degree of symmetry. We therefore focus on evaluating the performance of Euclidean symmetry-preserving ($E(3)$-equivariant) graph neural networks, showing that they can outperform non-equivariant counterparts and domain-specific information extraction techniques in downstream performance as well as simulation-efficiency. However, we find that current architectures fail to capture information from long-range correlations as effectively as domain-specific baselines, motivating future work on architectures better suited for extracting long-range information.}, note={arXiv:2410.20516 [cs]}, number={arXiv:2410.20516}, publisher={arXiv}, author={Balla, Julia and Mishra-Sharma, Siddharth and Cuesta-Lazaro, Carolina and Jaakkola, Tommi and Smidt, Tess}, year={2024}, month=oct 
}

@article{Batatia_Benner_Chiang_Elena_Kovács_Riebesell_Advincula_Asta_Avaylon_Baldwin_2024, title={A foundation model for atomistic materials chemistry}, url={http://arxiv.org/abs/2401.00096}, DOI={10.48550/arXiv.2401.00096}, abstractNote={Machine-learned force fields have transformed the atomistic modelling of materials by enabling simulations of ab initio quality on unprecedented time and length scales. However, they are currently limited by: (i) the significant computational and human effort that must go into development and validation of potentials for each particular system of interest; and (ii) a general lack of transferability from one chemical system to the next. Here, using the state-of-the-art MACE architecture we introduce a single general-purpose ML model, trained on a public database of 150k inorganic crystals, that is capable of running stable molecular dynamics on molecules and materials. We demonstrate the power of the MACE-MP-0 model - and its qualitative and at times quantitative accuracy - on a diverse set problems in the physical sciences, including the properties of solids, liquids, gases, chemical reactions, interfaces and even the dynamics of a small protein. The model can be applied out of the box and as a starting or “foundation model” for any atomistic system of interest and is thus a step towards democratising the revolution of ML force fields by lowering the barriers to entry.}, note={arXiv:2401.00096 [physics]}, number={arXiv:2401.00096}, publisher={arXiv}, author={Batatia, Ilyes and Benner, Philipp and Chiang, Yuan and Elena, Alin M. and Kovács, Dávid P. and Riebesell, Janosh and Advincula, Xavier R. and Asta, Mark and Avaylon, Matthew and Baldwin, William J. and Berger, Fabian and Bernstein, Noam and Bhowmik, Arghya and Blau, Samuel M. and Cărare, Vlad and Darby, James P. and De, Sandip and Pia, Flaviano Della and Deringer, Volker L. and Elijošius, Rokas and El-Machachi, Zakariya and Falcioni, Fabio and Fako, Edvin and Ferrari, Andrea C. and Genreith-Schriever, Annalena and George, Janine and Goodall, Rhys E. A. and Grey, Clare P. and Grigorev, Petr and Han, Shuang and Handley, Will and Heenen, Hendrik H. and Hermansson, Kersti and Holm, Christian and Jaafar, Jad and Hofmann, Stephan and Jakob, Konstantin S. and Jung, Hyunwook and Kapil, Venkat and Kaplan, Aaron D. and Karimitari, Nima and Kermode, James R. and Kroupa, Namu and Kullgren, Jolla and Kuner, Matthew C. and Kuryla, Domantas and Liepuoniute, Guoda and Margraf, Johannes T. and Magdău, Ioan-Bogdan and Michaelides, Angelos and Moore, J. Harry and Naik, Aakash A. and Niblett, Samuel P. and Norwood, Sam Walton and O’Neill, Niamh and Ortner, Christoph and Persson, Kristin A. and Reuter, Karsten and Rosen, Andrew S. and Schaaf, Lars L. and Schran, Christoph and Shi, Benjamin X. and Sivonxay, Eric and Stenczel, Tamás K. and Svahn, Viktor and Sutton, Christopher and Swinburne, Thomas D. and Tilly, Jules and Oord, Cas van der and Varga-Umbrich, Eszter and Vegge, Tejs and Vondrák, Martin and Wang, Yangshuai and Witt, William C. and Zills, Fabian and Csányi, Gábor}, year={2024}, month=mar 
}

@article{Batatia_Kovács_Simm_Ortner_Csányi_2023, title={MACE: Higher Order Equivariant Message Passing Neural Networks for Fast and Accurate Force Fields}, url={http://arxiv.org/abs/2206.07697}, DOI={10.48550/arXiv.2206.07697}, abstractNote={Creating fast and accurate force fields is a long-standing challenge in computational chemistry and materials science. Recently, several equivariant message passing neural networks (MPNNs) have been shown to outperform models built using other approaches in terms of accuracy. However, most MPNNs suffer from high computational cost and poor scalability. We propose that these limitations arise because MPNNs only pass two-body messages leading to a direct relationship between the number of layers and the expressivity of the network. In this work, we introduce MACE, a new equivariant MPNN model that uses higher body order messages. In particular, we show that using four-body messages reduces the required number of message passing iterations to just two, resulting in a fast and highly parallelizable model, reaching or exceeding state-of-the-art accuracy on the rMD17, 3BPA, and AcAc benchmark tasks. We also demonstrate that using higher order messages leads to an improved steepness of the learning curves.}, note={arXiv:2206.07697 [stat]}, number={arXiv:2206.07697}, publisher={arXiv}, author={Batatia, Ilyes and Kovács, Dávid Péter and Simm, Gregor N. C. and Ortner, Christoph and Csányi, Gábor}, year={2023}, month=jan }

@article{Batzner_Musaelian_Sun_Geiger_Mailoa_Kornbluth_Molinari_Smidt_Kozinsky_2022, title={E(3)-Equivariant Graph Neural Networks for Data-Efficient and Accurate Interatomic Potentials}, volume={13}, ISSN={2041-1723}, DOI={10.1038/s41467-022-29939-5}, abstractNote={This work presents Neural Equivariant Interatomic Potentials (NequIP), an E(3)-equivariant neural network approach for learning interatomic potentials from ab-initio calculations for molecular dynamics simulations. While most contemporary symmetry-aware models use invariant convolutions and only act on scalars, NequIP employs E(3)-equivariant convolutions for interactions of geometric tensors, resulting in a more information-rich and faithful representation of atomic environments. The method achieves state-of-the-art accuracy on a challenging and diverse set of molecules and materials while exhibiting remarkable data efficiency. NequIP outperforms existing models with up to three orders of magnitude fewer training data, challenging the widely held belief that deep neural networks require massive training sets. The high data efficiency of the method allows for the construction of accurate potentials using high-order quantum chemical level of theory as reference and enables high-fidelity molecular dynamics simulations over long time scales.}, note={arXiv:2101.03164 [physics]}, number={1}, journal={Nature Communications}, author={Batzner, Simon and Musaelian, Albert and Sun, Lixin and Geiger, Mario and Mailoa, Jonathan P. and Kornbluth, Mordechai and Molinari, Nicola and Smidt, Tess E. and Kozinsky, Boris}, year={2022}, month=may, pages={2453} }

@article{Frank_Unke_Müller_Chmiela_2024a, title={A Euclidean transformer for fast and stable machine learned force fields}, volume={15}, ISSN={2041-1723}, DOI={10.1038/s41467-024-50620-6}, abstractNote={Abstract                            Recent years have seen vast progress in the development of machine learned force fields (MLFFs) based on ab-initio reference calculations. Despite achieving low test errors, the reliability of MLFFs in molecular dynamics (MD) simulations is facing growing scrutiny due to concerns about instability over extended simulation timescales. Our findings suggest a potential connection between robustness to cumulative inaccuracies and the use of equivariant representations in MLFFs, but the computational cost associated with these representations can limit this advantage in practice. To address this, we propose a transformer architecture called              SO3krates              that combines sparse equivariant representations (              Euclidean variables              ) with a self-attention mechanism that separates invariant and equivariant information, eliminating the need for expensive tensor products.              SO3krates              achieves a unique combination of accuracy, stability, and speed that enables insightful analysis of quantum properties of matter on extended time and system size scales. To showcase this capability, we generate stable MD trajectories for flexible peptides and supra-molecular structures with hundreds of atoms. Furthermore, we investigate the PES topology for medium-sized chainlike molecules (e.g., small peptides) by exploring thousands of minima. Remarkably,              SO3krates              demonstrates the ability to strike a balance between the conflicting demands of stability and the emergence of new minimum-energy conformations beyond the training data, which is crucial for realistic exploration tasks in the field of biochemistry.}, number={1}, journal={Nature Communications}, author={Frank, J. Thorben and Unke, Oliver T. and Müller, Klaus-Robert and Chmiela, Stefan}, year={2024}, month=aug, pages={6539}, language={en} }

@article{Frank_Unke_Müller_Chmiela_2024b, title={A Euclidean transformer for fast and stable machine learned force fields}, volume={15}, ISSN={2041-1723}, DOI={10.1038/s41467-024-50620-6}, abstractNote={Abstract
Recent years have seen vast progress in the development of machine learned force fields (MLFFs) based on ab-initio reference calculations. Despite achieving low test errors, the reliability of MLFFs in molecular dynamics (MD) simulations is facing growing scrutiny due to concerns about instability over extended simulation timescales. Our findings suggest a potential connection between robustness to cumulative inaccuracies and the use of equivariant representations in MLFFs, but the computational cost associated with these representations can limit this advantage in practice. To address this, we propose a transformer architecture called              SO3krates              that combines sparse equivariant representations (              Euclidean variables              ) with a self-attention mechanism that separates invariant and equivariant information, eliminating the need for expensive tensor products.              SO3krates              achieves a unique combination of accuracy, stability, and speed that enables insightful analysis of quantum properties of matter on extended time and system size scales. To showcase this capability, we generate stable MD trajectories for flexible peptides and supra-molecular structures with hundreds of atoms. Furthermore, we investigate the PES topology for medium-sized chainlike molecules (e.g., small peptides) by exploring thousands of minima. Remarkably,              SO3krates              demonstrates the ability to strike a balance between the conflicting demands of stability and the emergence of new minimum-energy conformations beyond the training data, which is crucial for realistic exploration tasks in the field of biochemistry.}, number={1}, journal={Nature Communications}, author={Frank, J. Thorben and Unke, Oliver T. and Müller, Klaus-Robert and Chmiela, Stefan}, year={2024}, month=aug, pages={6539}, language={en} }
@article{Gao_Günnemann_2022, title={Ab-Initio Potential Energy Surfaces by Pairing GNNs with Neural Wave Functions}, url={http://arxiv.org/abs/2110.05064}, DOI={10.48550/arXiv.2110.05064}, abstractNote={Solving the Schr"odinger equation is key to many quantum mechanical properties. However, an analytical solution is only tractable for single-electron systems. Recently, neural networks succeeded at modeling wave functions of many-electron systems. Together with the variational Monte-Carlo (VMC) framework, this led to solutions on par with the best known classical methods. Still, these neural methods require tremendous amounts of computational resources as one has to train a separate model for each molecular geometry. In this work, we combine a Graph Neural Network (GNN) with a neural wave function to simultaneously solve the Schr"odinger equation for multiple geometries via VMC. This enables us to model continuous subsets of the potential energy surface with a single training pass. Compared to existing state-of-the-art networks, our Potential Energy Surface Network PESNet speeds up training for multiple geometries by up to 40 times while matching or surpassing their accuracy. This may open the path to accurate and orders of magnitude cheaper quantum mechanical calculations.}, note={arXiv:2110.05064 [cs]}, number={arXiv:2110.05064}, publisher={arXiv}, author={Gao, Nicholas and Günnemann, Stephan}, year={2022}, month=mar }
@article{Gasteiger_Groß_Günnemann_2022, title={Directional Message Passing for Molecular Graphs}, url={http://arxiv.org/abs/2003.03123}, DOI={10.48550/arXiv.2003.03123}, abstractNote={Graph neural networks have recently achieved great successes in predicting quantum mechanical properties of molecules. These models represent a molecule as a graph using only the distance between atoms (nodes). They do not, however, consider the spatial direction from one atom to another, despite directional information playing a central role in empirical potentials for molecules, e.g. in angular potentials. To alleviate this limitation we propose directional message passing, in which we embed the messages passed between atoms instead of the atoms themselves. Each message is associated with a direction in coordinate space. These directional message embeddings are rotationally equivariant since the associated directions rotate with the molecule. We propose a message passing scheme analogous to belief propagation, which uses the directional information by transforming messages based on the angle between them. Additionally, we use spherical Bessel functions and spherical harmonics to construct theoretically well-founded, orthogonal representations that achieve better performance than the currently prevalent Gaussian radial basis representations while using fewer than 1/4 of the parameters. We leverage these innovations to construct the directional message passing neural network (DimeNet). DimeNet outperforms previous GNNs on average by 76% on MD17 and by 31% on QM9. Our implementation is available online.}, note={arXiv:2003.03123 [cs]}, number={arXiv:2003.03123}, publisher={arXiv}, author={Gasteiger, Johannes and Groß, Janek and Günnemann, Stephan}, year={2022}, month=apr }
@article{Geiger_Smidt_2022, title={e3nn: Euclidean Neural Networks}, url={http://arxiv.org/abs/2207.09453}, DOI={10.48550/arXiv.2207.09453}, abstractNote={We present e3nn, a generalized framework for creating E(3) equivariant trainable functions, also known as Euclidean neural networks. e3nn naturally operates on geometry and geometric tensors that describe systems in 3D and transform predictably under a change of coordinate system. The core of e3nn are equivariant operations such as the TensorProduct class or the spherical harmonics functions that can be composed to create more complex modules such as convolutions and attention mechanisms. These core operations of e3nn can be used to efficiently articulate Tensor Field Networks, 3D Steerable CNNs, Clebsch-Gordan Networks, SE(3) Transformers and other E(3) equivariant networks.}, note={arXiv:2207.09453 [cs]}, number={arXiv:2207.09453}, publisher={arXiv}, author={Geiger, Mario and Smidt, Tess}, year={2022}, month=jul }
@article{Manolache_Tantaru_Niepert_2024, title={MolMix: A Simple Yet Effective Baseline for Multimodal Molecular Representation Learning}, url={http://arxiv.org/abs/2410.07981}, DOI={10.48550/arXiv.2410.07981}, abstractNote={In this work, we propose a simple transformer-based baseline for multimodal molecular representation learning, integrating three distinct modalities: SMILES strings, 2D graph representations, and 3D conformers of molecules. A key aspect of our approach is the aggregation of 3D conformers, allowing the model to account for the fact that molecules can adopt multiple conformations-an important factor for accurate molecular representation. The tokens for each modality are extracted using modality-specific encoders: a transformer for SMILES strings, a message-passing neural network for 2D graphs, and an equivariant neural network for 3D conformers. The flexibility and modularity of this framework enable easy adaptation and replacement of these encoders, making the model highly versatile for different molecular tasks. The extracted tokens are then combined into a unified multimodal sequence, which is processed by a downstream transformer for prediction tasks. To efficiently scale our model for large multimodal datasets, we utilize Flash Attention 2 and bfloat16 precision. Despite its simplicity, our approach achieves state-of-the-art results across multiple datasets, demonstrating its effectiveness as a strong baseline for multimodal molecular representation learning.}, note={arXiv:2410.07981 [cs]}, number={arXiv:2410.07981}, publisher={arXiv}, author={Manolache, Andrei and Tantaru, Dragos and Niepert, Mathias}, year={2024}, month=oct }
@article{Nguyen_Lukashina_Nguyen_Le_Nguyen_Ho_Peters_Sonntag_Zaverkin_Niepert_2024, title={Structure-Aware E(3)-Invariant Molecular Conformer Aggregation Networks}, url={http://arxiv.org/abs/2402.01975}, DOI={10.48550/arXiv.2402.01975}, abstractNote={A molecule’s 2D representation consists of its atoms, their attributes, and the molecule’s covalent bonds. A 3D (geometric) representation of a molecule is called a conformer and consists of its atom types and Cartesian coordinates. Every conformer has a potential energy, and the lower this energy, the more likely it occurs in nature. Most existing machine learning methods for molecular property prediction consider either 2D molecular graphs or 3D conformer structure representations in isolation. Inspired by recent work on using ensembles of conformers in conjunction with 2D graph representations, we propose $mathrm{E}$(3)-invariant molecular conformer aggregation networks. The method integrates a molecule’s 2D representation with that of multiple of its conformers. Contrary to prior work, we propose a novel 2D-3D aggregation mechanism based on a differentiable solver for the Fused Gromov-Wasserstein Barycenter problem and the use of an efficient conformer generation method based on distance geometry. We show that the proposed aggregation mechanism is $mathrm{E}$(3) invariant and propose an efficient GPU implementation. Moreover, we demonstrate that the aggregation mechanism helps to significantly outperform state-of-the-art molecule property prediction methods on established datasets.}, note={arXiv:2402.01975 [cs]}, number={arXiv:2402.01975}, publisher={arXiv}, author={Nguyen, Duy M. H. and Lukashina, Nina and Nguyen, Tai and Le, An T. and Nguyen, TrungTin and Ho, Nhat and Peters, Jan and Sonntag, Daniel and Zaverkin, Viktor and Niepert, Mathias}, year={2024}, month=aug }
@article{Satorras_Hoogeboom_Welling_2022, title={E(n) Equivariant Graph Neural Networks}, url={http://arxiv.org/abs/2102.09844}, DOI={10.48550/arXiv.2102.09844}, abstractNote={This paper introduces a new model to learn graph neural networks equivariant to rotations, translations, reflections and permutations called E(n)-Equivariant Graph Neural Networks (EGNNs). In contrast with existing methods, our work does not require computationally expensive higher-order representations in intermediate layers while it still achieves competitive or better performance. In addition, whereas existing methods are limited to equivariance on 3 dimensional spaces, our model is easily scaled to higher-dimensional spaces. We demonstrate the effectiveness of our method on dynamical systems modelling, representation learning in graph autoencoders and predicting molecular properties.}, note={arXiv:2102.09844 [cs]}, number={arXiv:2102.09844}, publisher={arXiv}, author={Satorras, Victor Garcia and Hoogeboom, Emiel and Welling, Max}, year={2022}, month=feb }
@article{Schütt_Kindermans_Sauceda_Chmiela_Tkatchenko_Müller_2017, title={SchNet: A continuous-filter convolutional neural network for modeling quantum interactions}, url={http://arxiv.org/abs/1706.08566}, DOI={10.48550/arXiv.1706.08566}, abstractNote={Deep learning has the potential to revolutionize quantum chemistry as it is ideally suited to learn representations for structured data and speed up the exploration of chemical space. While convolutional neural networks have proven to be the first choice for images, audio and video data, the atoms in molecules are not restricted to a grid. Instead, their precise locations contain essential physical information, that would get lost if discretized. Thus, we propose to use continuous-filter convolutional layers to be able to model local correlations without requiring the data to lie on a grid. We apply those layers in SchNet: a novel deep learning architecture modeling quantum interactions in molecules. We obtain a joint model for the total energy and interatomic forces that follows fundamental quantum-chemical principles. This includes rotationally invariant energy predictions and a smooth, differentiable potential energy surface. Our architecture achieves state-of-the-art performance for benchmarks of equilibrium molecules and molecular dynamics trajectories. Finally, we introduce a more challenging benchmark with chemical and structural variations that suggests the path for further work.}, note={arXiv:1706.08566 [stat]}, number={arXiv:1706.08566}, publisher={arXiv}, author={Schütt, Kristof T. and Kindermans, Pieter-Jan and Sauceda, Huziel E. and Chmiela, Stefan and Tkatchenko, Alexandre and Müller, Klaus-Robert}, year={2017}, month=dec }
@article{Schütt_Tkatchenko_Müller_2018, title={Learning representations of molecules and materials with atomistic neural networks}, url={http://arxiv.org/abs/1812.04690}, DOI={10.48550/arXiv.1812.04690}, abstractNote={Deep Learning has been shown to learn efficient representations for structured data such as image, text or audio. In this chapter, we present neural network architectures that are able to learn efficient representations of molecules and materials. In particular, the continuous-filter convolutional network SchNet accurately predicts chemical properties across compositional and configurational space on a variety of datasets. Beyond that, we analyze the obtained representations to find evidence that their spatial and chemical properties agree with chemical intuition.}, note={arXiv:1812.04690 [physics]}, number={arXiv:1812.04690}, publisher={arXiv}, author={Schütt, Kristof T. and Tkatchenko, Alexandre and Müller, Klaus-Robert}, year={2018}, month=dec }
@article{Unke_Chmiela_Sauceda_Gastegger_Poltavsky_Schütt_Tkatchenko_Müller_2021, title={Machine Learning Force Fields}, volume={121}, ISSN={0009-2665, 1520-6890}, DOI={10.1021/acs.chemrev.0c01111}, abstractNote={In recent years, the use of Machine Learning (ML) in computational chemistry has enabled numerous advances previously out of reach due to the computational complexity of traditional electronic-structure methods. One of the most promising applications is the construction of ML-based force fields (FFs), with the aim to narrow the gap between the accuracy of ab initio methods and the efficiency of classical FFs. The key idea is to learn the statistical relation between chemical structure and potential energy without relying on a preconceived notion of fixed chemical bonds or knowledge about the relevant interactions. Such universal ML approximations are in principle only limited by the quality and quantity of the reference data used to train them. This review gives an overview of applications of ML-FFs and the chemical insights that can be obtained from them. The core concepts underlying ML-FFs are described in detail and a step-by-step guide for constructing and testing them from scratch is given. The text concludes with a discussion of the challenges that remain to be overcome by the next generation of ML-FFs.}, note={arXiv:2010.07067 [physics]}, number={16}, journal={Chemical Reviews}, author={Unke, Oliver T. and Chmiela, Stefan and Sauceda, Huziel E. and Gastegger, Michael and Poltavsky, Igor and Schütt, Kristof T. and Tkatchenko, Alexandre and Müller, Klaus-Robert}, year={2021}, month=aug, pages={10142–10186} }
@article{Zaverkin_Alesiani_Maruyama_Errica_Christiansen_Takamoto_Weber_Niepert_2024, title={Higher-Rank Irreducible Cartesian Tensors for Equivariant Message Passing}, url={http://arxiv.org/abs/2405.14253}, DOI={10.48550/arXiv.2405.14253}, abstractNote={The ability to perform fast and accurate atomistic simulations is crucial for advancing the chemical sciences. By learning from high-quality data, machine-learned interatomic potentials achieve accuracy on par with ab initio and first-principles methods at a fraction of their computational cost. The success of machine-learned interatomic potentials arises from integrating inductive biases such as equivariance to group actions on an atomic system, e.g., equivariance to rotations and reflections. In particular, the field has notably advanced with the emergence of equivariant message passing. Most of these models represent an atomic system using spherical tensors, tensor products of which require complicated numerical coefficients and can be computationally demanding. Cartesian tensors offer a promising alternative, though state-of-the-art methods lack flexibility in message-passing mechanisms, restricting their architectures and expressive power. This work explores higher-rank irreducible Cartesian tensors to address these limitations. We integrate irreducible Cartesian tensor products into message-passing neural networks and prove the equivariance and traceless property of the resulting layers. Through empirical evaluations on various benchmark data sets, we consistently observe on-par or better performance than that of state-of-the-art spherical and Cartesian models.}, note={arXiv:2405.14253 [cs]}, number={arXiv:2405.14253}, publisher={arXiv}, author={Zaverkin, Viktor and Alesiani, Francesco and Maruyama, Takashi and Errica, Federico and Christiansen, Henrik and Takamoto, Makoto and Weber, Nicolas and Niepert, Mathias}, year={2024}, month=nov }
@article{Zaverkin_Holzmüller_Steinwart_Kästner_2021, title={Fast and Sample-Efficient Interatomic Neural Network Potentials for Molecules and Materials Based on Gaussian Moments}, volume={17}, ISSN={1549-9618, 1549-9626}, DOI={10.1021/acs.jctc.1c00527}, abstractNote={Artificial neural networks (NNs) are one of the most frequently used machine learning approaches to construct interatomic potentials and enable efficient large-scale atomistic simulations with almost ab initio accuracy. However, the simultaneous training of NNs on energies and forces, which are a prerequisite for, e.g., molecular dynamics simulations, can be demanding. In this work, we present an improved NN architecture based on the previous GM-NN model [V. Zaverkin and J. K"astner, J. Chem. Theory Comput. 16, 5410-5421 (2020)], which shows an improved prediction accuracy and considerably reduced training times. Moreover, we extend the applicability of Gaussian moment-based interatomic potentials to periodic systems and demonstrate the overall excellent transferability and robustness of the respective models. The fast training by the improved methodology is a pre-requisite for training-heavy workflows such as active learning or learning-on-the-fly.}, note={arXiv:2109.09569 [physics]}, number={10}, journal={Journal of Chemical Theory and Computation}, author={Zaverkin, Viktor and Holzmüller, David and Steinwart, Ingo and Kästner, Johannes}, year={2021}, month=oct, pages={6658–6670} }
@article{Thomas_Smidt_Kearnes_Yang_Li_Kohlhoff_Riley_2018, title={Tensor field networks: Rotation- and translation-equivariant neural networks for 3D point clouds}, url={http://arxiv.org/abs/1802.08219}, DOI={10.48550/arXiv.1802.08219}, abstractNote={We introduce tensor field neural networks, which are locally equivariant to 3D rotations, translations, and permutations of points at every layer. 3D rotation equivariance removes the need for data augmentation to identify features in arbitrary orientations. Our network uses filters built from spherical harmonics; due to the mathematical consequences of this filter choice, each layer accepts as input (and guarantees as output) scalars, vectors, and higher-order tensors, in the geometric sense of these terms. We demonstrate the capabilities of tensor field networks with tasks in geometry, physics, and chemistry.}, note={arXiv:1802.08219 [cs]}, number={arXiv:1802.08219}, publisher={arXiv}, author={Thomas, Nathaniel and Smidt, Tess and Kearnes, Steven and Yang, Lusann and Li, Li and Kohlhoff, Kai and Riley, Patrick}, year={2018}, month=may }
@article{Liao_Smidt_2023, title={Equiformer: Equivariant Graph Attention Transformer for 3D Atomistic Graphs}, url={http://arxiv.org/abs/2206.11990}, DOI={10.48550/arXiv.2206.11990}, abstractNote={Despite their widespread success in various domains, Transformer networks have yet to perform well across datasets in the domain of 3D atomistic graphs such as molecules even when 3D-related inductive biases like translational invariance and rotational equivariance are considered. In this paper, we demonstrate that Transformers can generalize well to 3D atomistic graphs and present Equiformer, a graph neural network leveraging the strength of Transformer architectures and incorporating SE(3)/E(3)-equivariant features based on irreducible representations (irreps). First, we propose a simple and effective architecture by only replacing original operations in Transformers with their equivariant counterparts and including tensor products. Using equivariant operations enables encoding equivariant information in channels of irreps features without complicating graph structures. With minimal modifications to Transformers, this architecture has already achieved strong empirical results. Second, we propose a novel attention mechanism called equivariant graph attention, which improves upon typical attention in Transformers through replacing dot product attention with multi-layer perceptron attention and including non-linear message passing. With these two innovations, Equiformer achieves competitive results to previous models on QM9, MD17 and OC20 datasets.}, note={arXiv:2206.11990 [cs]}, number={arXiv:2206.11990}, publisher={arXiv}, author={Liao, Yi-Lun and Smidt, Tess}, year={2023}, month=feb }