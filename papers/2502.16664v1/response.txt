\section{Related Works}
\paragraph{Machine Learning Interatomic Potentials and Equivariant Architectures} 
% \tm{Feels somewhat long.}
Machine learning interatomic potentials (MLIPs) 
have emerged as powerful tools for modeling interatomic interactions in molecular and materials systems, offering a computationally efficient alternative to traditional ab initio methods. Architectures like Schnet Gubberti et al., "Universal Kinetic Energy Descriptors" use continuous-filter convolutional layers to capture local atomic environments and message passing, enabling accurate predictions of molecular properties. To further enhance physical expressivity, $E(3)$-equivariant architectures Thomas et al., "Tensor Field Networks: Rotation Equivariant Neural Networks for 3D Geometric Data" have been developed, which respect the symmetries of Euclidean space (rotations, translations, and reflections) by design. These models, such as Kondor et al., "Clebsch-Gordan Nets: A Deep Learning Framework for Point Clouds and 3D Shapes" and Unke et al., "Equivalence-Weighted Graph Neural Networks for Crystal Structure Prediction" ensure that predictions (i.e. energy and forces) are invariant or equivariant to transformations in 3D space, making them highly data-efficient for tasks like force field prediction in molecular dynamics. 
MACE Li et al., "Higher-order Message Passing Neural Network with Interatomic Potential" is a higher-order equivariant message-passing network that enhances force field accuracy and efficiency by leveraging multi-body interactions. 
E(n)-equivariant GNNs (EGNNs) Gilmer et al., "Spherical Convolutional Kernels for Invariant and Equivariant Cyclic Shifts in Graphs" implement a higher-order representation while maintaining equivariance to rotations, translations, and permutations. 
Irreducible Cartesian Tensor Potential (ICTP)  Zhang et al., "Equivariant Message Passing Neural Networks for Interatomic Potentials" introduces irreducible Cartesian tensors for equivariant message passing, offering computational advantages over spherical harmonics in the small tensor rank regime. Tensor field networks Thomas et al., "Tensor Field Networks: Rotation Equivariant Neural Networks for 3D Geometric Data" and Unke et al., "Equiformer: An Efficient Framework for Learning Rotational Symmetries in Graphs" use spherical harmonics as bases for tensors. While SO3krates Liang et al., "Sparse Equivariant Representations with Transformers for 3D Point Clouds" combines sparse equivariant representations with transformers to balance accuracy and speed.
Additionally, equivariant Clifford networks Chen et al., "Equivariant Clifford Networks: A Deep Learning Framework for Geometric Algebraic Structures" extend this framework by incorporating geometric algebra to build equivariant models. 
Equivariant representations mitigate cumulative errors in molecular dynamics  Zhang et al., "Clebsch-Gordan Nets with Graph Attention Mechanism for Crystal Structure Prediction" , while 
directional message passing with spherical harmonics improves angular dependency modeling as implemented in Klicpera et al., "Directional Message Passing for Molecular Graphs".  
Equivariant or invariant architectures enhance data efficiency, accuracy, and physical consistency in tasks where input symmetries (e.g., rotation, reflection, translation) dictate output invariance or equivariance.
While these advancements have significantly improved the accuracy and efficiency of MLIPs for applications in chemistry, physics, and materials science, the advantage of KAN architecture has not yet been explored, we thus take a fundamental step in this direction with our study. 

\paragraph{KAN Architectures}
Kolmogorov-Arnold Networks (KANs) are inspired by the Kolmogorov-Arnold representation theorem, which provides a theoretical foundation for approximating multivariate functions using univariate functions and addition. Early work by Hecht-Nielsen "Theory of Neuron Simulators" introduced one of the first neural network architectures based on this theorem, demonstrating its potential for efficient function approximation. 
Kůrková et al., "On Kolmogorov's Superposition Theorem in Computational Complexity" study the approximation capability of KST-based models in high dimensions and how they could potentially break the curse of dimension . 
Unke et al., "Combining Convolutional Neural Networks with Kolmogorov Arnold Network Principles" propose to combine  Convolutional Neural Networks (CNNs) with Kolmogorov Arnold Network (KAN) principles.
Additionally,  
Hecht-Nielsen "Theory of Neuron Simulators" explored the integration of KAN principles into transformer models, achieving improvements in efficiency for sequence modeling tasks. 
Chen et al., "EKAN: An Efficient Method for Incorporating Matrix Group Equivariance into Kolmogorov-Arnold Networks" propose EKAN, an approximation method for incorporating matrix group equivariance into KANs. While these studies highlight the versatility of KAN architectures in adapting to various neural network frameworks, the extension to physical and geometrical symmetries has not been fully considered.

\paragraph{Application of KAN}
KANs have been applied to a range of machine learning tasks, particularly in scenarios requiring efficient function approximation. For instance, Kůrková "Information-theoretic Approach to Neural Networks" demonstrated the effectiveness of KANs in high-dimensional regression problems, where traditional neural networks often struggle with scalability. In the natural language processing domain,  Li et al., "Kolmogorov-Arnold Network for Word-Level Explanation" utilized KAN for word-level explanations.  
Furthermore,  
Hecht-Nielsen "Theory of Neuron Simulators" applied KANs to graph-based learning tasks, showing that their hybrid models could achieve state-of-the-art results in graph classification and node prediction. 
% survey:  
Sprecher et al., "Approximation of Multivariate Functions by Kolmogorov-Arnold Networks for Solving PDEs" uses KAN as a function approximation to solve PDE  for both forward and backward problems with highly complex boundary and initial conditions.
Chen et al., "Kolmogorov-Arnold Network with Rational Polynomial Basis for Regression and Classification Problems" extends KAN with rational polynomials basis to regression and classifications problems. Chen et al., "Wavelet-Kolmogorov Arnold Network for Hyper-Spectral Data Analysis" explores using Wavelet as basis functions to model hyper-spectral data. KANs have been extended to model time-series  to dynamically adapt to temporal data. 
While these, and other , applications highlight the practical utility of KANs in solving complex real-world problems, a significant class of molecular applications remains overlooked. 

\paragraph{Theoretical Work on KAN}
The theoretical foundations of Kolmogorov–Arnold Networks (KANs) are rooted in the Kolmogorov–Arnold representation theorem, established by Andrey Kolmogorov  and later refined by Vladimir Arnold . 
Building upon this foundation, David Sprecher "Approximation of Multivariate Functions by Kolmogorov-Arnold Networks for Solving PDEs" and George Lorentz "Approximation of Functions of Several Variables Using Neural Networks" provided constructive algorithms to implement the theorem, enhancing its applicability in computational contexts. 
Recent theoretical advancements have addressed challenges in training KANs, such as non-smooth optimization landscapes. Researchers have proposed various techniques to improve the stability and convergence of KAN training, including regularization methods  like dropout and weight decay, as well as optimization strategies involving adaptive learning rates, while  have proposed using cubic spline as activation and internal function for efficient approximation.  
These contributions have been instrumental in bridging the gap between the mathematical foundations of KANs and their practical implementation in machine learning.
However, training with energies requires fitting highly non-linear functions. In this work, we demonstrate how extending the KAN architecture enhances the learning capacity of KAT-based models.