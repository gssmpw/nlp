\section{Related Works}
\paragraph{Machine Learning Interatomic Potentials and Equivariant Architectures} 
% \tm{Feels somewhat long.}
Machine learning interatomic potentials (MLIPs) 
have emerged as powerful tools for modeling interatomic interactions in molecular and materials systems, offering a computationally efficient alternative to traditional ab initio methods. Architectures like Schnet ____ use continuous-filter convolutional layers to capture local atomic environments and message passing, enabling accurate predictions of molecular properties. To further enhance physical expressivity, $E(3)$-equivariant architectures ____ have been developed, which respect the symmetries of Euclidean space (rotations, translations, and reflections) by design. These models, such as Tensor Field Networks ____ and NequIP ____, ensure that predictions (i.e. energy and forces) are invariant or equivariant to transformations in 3D space, making them highly data-efficient for tasks like force field prediction in molecular dynamics. 
MACE ____ is a higher-order equivariant message-passing network that enhances force field accuracy and efficiency by leveraging multi-body interactions. 
E(n)-equivariant GNNs (EGNNs) ____ implement a higher-order representation while maintaining equivariance to rotations, translations, and permutations. 
Irreducible Cartesian Tensor Potential (ICTP)  ____ introduces irreducible Cartesian tensors for equivariant message passing, offering computational advantages over spherical harmonics in the small tensor rank regime. Tensor field networks ____ and Equiformer ____ use spherical harmonics as bases for tensors. While SO3krates ____ combines sparse equivariant representations with transformers to balance accuracy and speed.
Additionally, equivariant Clifford networks ____
extend this framework by incorporating geometric algebra to build equivariant models. 
Equivariant representations mitigate cumulative errors in molecular dynamics  ____,  while 
directional message passing with spherical harmonics improves angular dependency modeling as implemented in DimeNet ____.  
Equivariant or invariant architectures enhance data efficiency, accuracy, and physical consistency in tasks where input symmetries (e.g., rotation, reflection, translation) dictate output invariance or equivariance.
While these advancements have significantly improved the accuracy and efficiency of MLIPs for applications in chemistry, physics, and materials science, the advantage of KAN architecture has not yet been explored, we thus take a fundamental step in this direction with our study. 

\paragraph{KAN Architectures}
Kolmogorov-Arnold Networks (KANs) are inspired by the Kolmogorov-Arnold representation theorem, which provides a theoretical foundation for approximating multivariate functions using univariate functions and addition. Early work by Hecht-Nielsen (1987) ____ introduced one of the first neural network architectures based on this theorem, demonstrating its potential for efficient function approximation. 
____ study the approximation capability of KST-based models in high dimensions and how they could potentially break the curse of dimension ____. 
____ propose to combine  Convolutional Neural Networks (CNNs) with Kolmogorov Arnold Network (KAN) principles.
Additionally, 
____
explored the integration of KAN principles into transformer models, achieving improvements in efficiency for sequence modeling tasks. 
____ propose EKAN, an approximation method for incorporating matrix group equivariance into KANs. While these studies highlight the versatility of KAN architectures in adapting to various neural network frameworks, the extension to physical and geometrical symmetries has not been fully considered.

\paragraph{Application of KAN}
KANs have been applied to a range of machine learning tasks, particularly in scenarios requiring efficient function approximation. For instance, Kůrková (1991) ____ demonstrated the effectiveness of KANs in high-dimensional regression problems, where traditional neural networks often struggle with scalability. In the natural language processing domain, ____ utilized KAN for word-level explanations.  
Furthermore, 
____
applied KANs to graph-based learning tasks, showing that their hybrid models could achieve state-of-the-art results in graph classification and node prediction. 
% survey: ____ 
KAN has been used as a function approximation to solve PDE ____ for both forward and backward problems with highly complex boundary and initial conditions.
____ extends KAN with rational polynomials basis to regression and classifications problems. ____ explores using Wavelet as basis functions to model hyper-spectral data. KANs have been extended to model time-series ____ to dynamically adapt to temporal data. 
While these, and other ____,  applications highlight the practical utility of KANs in solving complex real-world problems, a significant class of molecular applications remains overlooked. 

\paragraph{Theoretical Work on KAN}
The theoretical foundations of Kolmogorov–Arnold Networks (KANs) are rooted in the Kolmogorov–Arnold representation theorem, established by Andrey Kolmogorov  ____ and later refined by Vladimir Arnold ____. 
Building upon this foundation, David Sprecher ____ and George Lorentz ____ provided constructive algorithms to implement the theorem, enhancing its applicability in computational contexts. 
Recent theoretical advancements have addressed challenges in training KANs, such as non-smooth optimization landscapes. Researchers have proposed various techniques to improve the stability and convergence of KAN training, including regularization methods ____ like dropout and weight decay, as well as optimization strategies involving adaptive learning rates, while ____ have proposed using cubic spline as activation and internal function for efficient approximation.  
These contributions have been instrumental in bridging the gap between the mathematical foundations of KANs and their practical implementation in machine learning.
However, training with energies requires fitting highly non-linear functions. In this work, we demonstrate how extending the KAN architecture enhances the learning capacity of KAT-based models.