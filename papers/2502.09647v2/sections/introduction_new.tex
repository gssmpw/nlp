



\section{Introduction}
\label{section:intro}



The landscape of large language models (LLMs) is rapidly evolving, with modern architectures capable of generating text from vast contexts. Recent advances have led to a significant increase in context window sizes, with Llama 3 \citep{dubey2024llama}, DeepSeekv3 \citep{liu2024deepseek}, and Gemini \citep{team2024gemini} supporting windows of at least 128k. 
However, long context modeling still poses significant challenges \citep{hsieh2024ruler} in terms of both accuracy  and  the substantial cost of processing long contexts in terms of memory and run-time compute. 
In spite of their importance, our current comprehension of the attention mechanism in long-context tasks remains incomplete. This work aims to address some of these knowledge gaps.

Despite the overwhelming complexity of state-of-the-art models, certain simple behaviors in the attention mechanism are strikingly consistent. In particular, many forms of sparse behaviors have been consistently observed, and exploited by numerous methods for efficient inference (see Section~\ref{sec:relatedworks}). 
Among them, \citet{xiao2023efficient} showed that even when computing the attention only using tokens close to the current token plus initial ``sink'' tokens, as illustrated in Figure~\ref{fig:gaussian},  the model is still capable of generating fluent text. We refer to these tokens as local window, and always implicitly include the   initial tokens as they play a crucial role as an attention ``sink'' (see also \citet{chen2024magicpig,gu2024attention,sun2024massive}). 

However, such a local window approximation, if applied to every attention head simultaneously, necessarily harms the capabilities of LLMs to retrieve and process long-context information (see e.g., \citet{xiao2024duoattention}).
Instead, to overcome such limitations, we aim to identify the heads whose output can be well-approximated via a local window attention, and apply the approximation to those only. If a head can be approximated via a local approximation, we call it a \textbf{local head}, and otherwise it is a \textbf{long-context head}. 
In particular, we ask:     Which heads can be approximated using a local window with minimal impact on downstream task performance?

 \begin{figure*}[ht]
\centering
\begin{subfigure}[b]{\linewidth}
\centering
        \includegraphics[width=1.0\linewidth]{plots/Figure1.pdf}\\
        \begin{subfigure}[b]{\linewidth}
    \end{subfigure}
\end{subfigure}
    \vspace{-1cm}
     \caption{
    \small{ 
    \textbf{Attention sparsity and its impact on efficiency.} 
    \textit{Left:} Attention scores are split into \textit{bulk} ($A^{\text{bulk}}$) for distant tokens and \textit{local window} ($A^{\text{local}}$) for nearby ones. A head is considered local if most of its attention mass falls within the local window. The static criterion pre-assigns local heads, while the adaptive oracle query-dependently compares bulk and local contributions but is computationally expensive. Our approximation models $A^{\text{bulk}}$ using a Gaussian distribution for efficiency.
    \textit{Middle:} Oracle-based classification with $\tau = 0.6$ (see Figure~\ref{fig:compare-approx} for the threshold) reveals three types of heads: consistently local (heads labeled more than $95\%$ of the times as local), often long-context (less than $50\%$), and varying, which switch behavior dynamically.
    \textit{Right:} Comparison of three methods: Static (green) removes a fixed fraction of heads, the adaptive oracle (blue) dynamically selects heads but is costly, and our adaptive method (purple) achieves near-oracle performance with significantly lower cost. As sparsity increases, static pruning degrades performance, while our adaptive method remains robust.
    These results show that \textit{most attention heads do not need to attend to the entire context}, enabling significant efficiency gains with \textit{query-adaptive} head classification.} }
    \label{fig:gaussian}
\end{figure*}



Two approaches to this problem can be distinguished:
  \textit{Static} criteria label the heads -- local vs long-context --  once for all queries, while \textit{query-adaptive} criteria change the labels from query to query.  Static criteria, as used by \citet{xiao2024duoattention,tang2024razorattention}, have the advantage that all key-value pairs (except for the few in the local window) of local heads can be discarded, thus saving memory. While recent works \citep{wu2024retrieval,tang2024razorattention,hong2024token} 
 provide some evidence that a \textit{fixed} small subset of the heads are particularly relevant for processing long-context information, the following question remains unclear:
 \begin{center}
     \textit{How much sparsity (measured as the average percentage of local heads) can we gain using query-adaptive criteria compared to static criteria?}
     \end{center} 


\paragraph{Contribution 1.} We present an extensive analysis comparing a query-adaptive oracle criterion, which selects local heads independently for each token, with static criteria. We make two observations: first, we find that static criteria can label up to 60\% of the heads as local heads without impacting downstream task evaluations, which confirms the intuition from \citep{wu2024retrieval}. Nevertheless, we find that a query-adaptive oracle criterion allows to label a substantially higher percentage of heads as local heads (up to 90\%) without sacrificing performance (see Figure~\ref{fig:gaussian}).


Unfortunately, the oracle requires the computation of the full attention scores. This leads to the following question:
\begin{center}
    \textit{ For each query, can we determine which heads are long-context and which are local without computing the full attention scores?}
\end{center}

The relevance of this question is twofold: on one hand, answering it helps guide further research toward developing more compute-efficient attention mechanisms. On the other hand, it advances our understanding of the inner workings of attention mechanisms, which is central to mechanistic interpretability (see, e.g., \citet{olsson2022context}). 

\paragraph{Contribution 2.} We address this question by proposing a novel query-adaptive attention criterion (QAdA) based on second-order statistics of the attention scores (briefly summarized in Figure~\ref{fig:gaussian}).
Our experiments on three families of LLMs, Llama \citep{dubey2024llama}, Qwen \citep{bai2023qwen} and Mistral \citep{jiang2023mistral} applied to a variety of standard long-context benchmarks, as well as hard  reasoning tasks embedded in long-context prompts, show that this relatively simple criterion allows to efficiently identify long-context heads: our method increased sparsity at a smaller loss in downstream performance than oracle static approaches. 
Along with our other experiments, it sheds light onto certain simple behaviors of attention heads in long-context settings. 


 


