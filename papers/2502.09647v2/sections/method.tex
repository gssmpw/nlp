






\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{plots/four_possibilities.png}
    \caption{
        Examples of attention score distributions for each possible outcome with 
        $\thrsapprox = \thrsoracle = 0.6$ with the oracle criterion as ground truth. 
        We show histograms of scores from the \textbf{local window} $\mathcal{I}$ (\textcolor{brown}{brown}) 
        and the \textbf{bulk complement} $[T] \setminus \mathcal{I}$ (\textcolor{gray}{gray}), 
        along with the bulk Gaussian approximation (black dashed line). 
        The annotations above each plot indicate the values taken by the statistics used for the oracle criterion 
        and the adaptive criterion.
    }
    \label{fig:scores}
\end{figure*}

\section{Preliminaries}
\label{sec:setting}
 We consider decoder-only transformer models \citep{vaswani2017attention},  consisting of $L$-layers each containing one attention  and one feed-forward block, using the rotary positional encoding (RoPE, \citet{su2024roformer}),  which is commonly used in state-of-the-art open source LLMs, e.g., Llama3 \citep{dubey2024llama}, Qwen \citep{bai2023qwen} or Gemma \citep{team2024gemma}. 
During inference, when predicting the next token, every single attention head  takes as input a vector of (already rotated) queries $q\in \mathbb R^{1\times d}$  and the (updated and rotated) cached key-value pairs $K, V \in \mathbb R^{T\times d}$, with sequence length $T$, and returns the weighted average of the values:
\begin{equation}
\label{eq:softmax}
    o = \text{softmax}(s) V \quad \text{with scores}\quad s= q K^\top/ \sqrt{d}
\end{equation}


\paragraph{Local window approximation.} 
We are interested in long-context settings, where $T$ is large. For a given query and attention head, one can restrict the head's attention to a \textit{local window}: instead of computing the head's attention scores with respect to each of the $T$ keys, only the attention scores corresponding to the first $T_{\text{sink}}$ input tokens (i.e. those closest to the start of the sequence) and the last $T_{\text{local}} - T_{\text{sink}}$ tokens are computed (as illustrated in Figure~\ref{fig:gaussian}) and used to produce the output, where $T_{\text{local}}, T_{\text{sink}} \in \mathbb{N}$ are fixed parameters. Though they may not contain particularly relevant information, the first $ T_{\text{sink}}$ tokens are included to serve as ``attention sink'' tokens, in line with the observations from \citet{xiao2023efficient}.
To summarize it more formally, we call $\mathcal I := \{1,\ldots, T_{\text{sink}}\} \cup \{T -  T_{\text{local}} + T_{\text{sink}}+1, \ldots, T\} \subset [T] $ the set of local indices, and the output of an attention head restricted to a local window is equal to $o_{\text{local}} = \text{softmax}(s_{\mathcal I}) V_{\mathcal I}, $ with $s_{\mathcal I} = q K_{\mathcal I}^\top /\sqrt{d}$. 








 \paragraph{Query-adaptive oracle criterion} 
 To determine which heads are local, we need to define a criterion that makes a decision for each query. 
 We call the heads labeled by the criterion  \textbf{local head} (for a given input token) and the others \textbf{long-context head}.
Assuming that we have access to all scores, a natural way to define such a criterion is to compare the mass of attention scores from the local window $\mathcal I$ to some threshold. That is, given a threshold $\thrsoracle$, an attention head $h$, and its associated attention scores $s_i = q K_i^\top/\sqrt{d}, i\in [T]$, we define the \textbf{(query-adaptive) oracle criterion}  $\oraclecriterion$ which takes the head's scores $s$ as input:
\begin{equation}
\label{eq:oracle}
    \oraclecriterion(s) = \indicator{ \frac{\sum_{i\in \mathcal I} \exp(s_i)}{\sum_{i \in \mathcal I} \exp(s_i)  +\sum_{i \notin \mathcal I} \exp(s_i) } \geq \thrsoracle}.
\end{equation}
If the criterion is satisfied for a given query, that is, if $\oraclecriterion=1$, the head \textit{mostly attends} to tokens from the \textit{local window}, and we call it a \textit{local} head. On the other hand, if  $\oraclecriterion=0$, the head assigns at least $1-\thrsoracle$ attention mass to tokens from the global context, and we call it \textit{long-context}. Note that our oracle criterion requires the computation of all the head's attention scores--as such, it is a tool of analysis, but it cannot be used as a practical way to increase compute efficiency.





















\section{Method}
\label{sec:method}
Given that many attention heads swing between being local and being long-context depending on the input token (as illustrated in Figure~\ref{fig:gaussian} and further observed in Section~\ref{sec:downstream_analysis}), how can we identify local heads in a query-adaptive manner while only computing the attentions scores from the local window? Intuitively, we want a criterion that can distinguish between the two following cases:

\begin{itemize}
    \item \textit{Case 1 (long-context head):} The scores from the local window follow the same distribution as the remaining scores (second plot in Figure~\ref{fig:scores}), and thus tokens from the local window cannot make up for most of the mass. 
    
    \item \textit{Case 2 (local head):} The scores from local tokens are significantly ``out-of-distribution'' on the right-sided tail (first plot in Figure~\ref{fig:scores}). While this does not guarantee that the attention head assigns most of the mass to those tokens, as there might be outliers in the distribution of the non-local scores (third plot in Figure~\ref{fig:scores}),
    this motivates us to label the head as a local head.
\end{itemize}


But how can we efficiently distinguish between the two cases? The key insight is that a Gaussian approximation for the keys, which in turn yields a  Gaussian approximation for the scores (black dashed line in Figure~\ref{fig:scores}), provides a good approximation for deciding what is ``in-distribution'' (Case 1) and what is ``out-of-distribution'' (Case 2). Such an approximation in turn allows us to construct an efficient approximate version of the oracle criterion from Equation~\eqref{eq:oracle}, that we call \textbf{query-adaptive attention (QAdA)}.




\begin{figure*}[t]
\begin{lstlisting}[language=Python, caption=Query-adaptive attention (QAdA) with local window approximation, label={lst:adaptive_attention}],float=th]
def adaptive_attention(q, k, v, mean_k, cov_k, Tl=128, log_thrs=0.6):
    mean_s = einsum('bhnd,hd->bhn', q, mean_k), 
    var_s = einsum('bhnd,hde,bhne->bhn', q, cov_k, q)
    numerator = lse(q @ k[:,:, local_indices]/sqrt(d), dim=-1)
    log_bulk = log(seq_len - window_size) + var_s / 2 + mean_s
    denominator = lse(stack([numerator, log_bulk]),dim=0)
    mask = numerator - denominator > log(log_thrs)
    out[mask], out[!mask] = local_attn_(q, k, v,  mask), dense_attn_(q, k, v,  !mask)
    return out
\end{lstlisting}
\end{figure*}


\subsection{Query-adaptive criterion}
The computational bottleneck in the oracle criterion from Equation~\eqref{eq:oracle} arises from the un-normalized mass $A^{\text{bulk}} := \sum_{i \notin \mathcal I} \exp(s_i)$ of the tokens from the bulk (see Figure~\ref{fig:gaussian}).  Let $\nu^{\text{bulk}}$ be the \textit{empirical distribution} of the keys $k_i^\top$, $i \in [T] \setminus \mathcal I$ and let $\tbulk=T- T_{\text{local}}$. We can write the un-normalized mass as an expectation over $\nu^{\text{bulk}}$:
\begin{equation}
    A^{\text{bulk}}= \tbulk~\mathbb E_{k^\top \sim \nu^{\text{bulk}}} \exp\left(\frac{qk_i^\top }{\sqrt{d}}\right). 
\end{equation}
The main idea behind our method is to now approximate $\nu^{\text{bulk}}$ by a product of Gaussians distributions with some mean $\mu_K$ and covariance $\Sigma_K$ (defined in Section~\ref{sec:pipeline}):



\begin{equation}
    \nu^{\text{bulk}} \approx \left(\mathcal  N(\mu_K, \Sigma_K)  \right)^{\tbulk}.
\end{equation}
Such an approximation clearly does not apply at the level of individual keys. Indeed, according to the Gaussian approximation, all keys should be identically distributed. However, this is definitely not the case as any two distinct keys store different positional information. Nevertheless, when averaged over the keys, we can hope that on a macro distribution level the approximation is accurate. More precisely, we propose to approximate:
\begin{align}
    \underset{k^\top \sim \nu^{\text{bulk}}}{\mathbb E} \exp\left(\frac{qk^\top}{\sqrt{d}}\right) &\approx \underset{k^\top \sim \mathcal N(\mu_K, \Sigma_K)}{ \mathbb{E}} \exp\left(\frac{q k^\top}{\sqrt{d}}\right). \label{eq:gaussianapprox}
\end{align}
In fact, the RHS can be computed in closed form. Indeed, we note that $\exp(q k^\top/\sqrt{d})$ follows a log-normal distribution:
\begin{align}
        \underset{k^\top \sim \mathcal N(\mu_K, \Sigma_K)}{ \mathbb{E}} \exp\left(\frac{q k_i^\top}{\sqrt{d}}\right) &=   \underset{s\sim \mathcal N(\mu_s, \sigma^2_s)}{ \mathbb{E}} \exp(s)  \nonumber\\
        &= \exp(\mu_s + \sigma_s^2/2)
        \label{eq:approx}
\end{align}
with $\mu_s = q \mu_K^\top/\sqrt{d}$ and $\sigma^2_s = q \Sigma_K q^\top /d$ the mean and variance of the scores. 
Assuming that we have access to the mean $\mu_K$ and covariance $\Sigma_K$ statistics (see Section~\ref{sec:moments}), we can therefore compute an approximation of $ A^{\text{bulk}}$ in constant run-time wrt.~$T$! 

 In summary, given the moments $\mu_K$ and $\Sigma_K$, the query $q$ and  the  scores $s_i$ obtained from the local keys $k_i$, $i \in \mathcal I$, we propose to approximate  the oracle criterion in Equation~\eqref{eq:oracle} via the following query-adaptive criterion (QAdA) with $A^{\text{local}} = \sum_{i\in \mathcal I} \exp(s_i)$:
\begin{align}
 c^h_{\text{approx}}(s)=
        \indicator{ \frac{A^{\text{local}}}{ A^{\text{local}} +\tbulk \exp\left(\mu_s +  \sigma_s^2/2 \right) } 
        \geq \thrsapprox}
        \label{eq:adaptive_criterion}
\end{align}





\subsection{Computing $\mu_K$ and $\Sigma_K$}
\label{sec:moments}
 
\underline{\textit{Option 1 (current prompt):}} After pre-filling and before generation, we can compute the moment statistics from the current KV-cache. That is, we compute $\mu_K = \frac{1}{T^{\text{bulk}}}\sum_{i \in [T] \setminus \mathcal I} K_i $ and $\Sigma_K =\frac{1}{T^{\text{bulk}}}\sum_{ i \in [T] \setminus \mathcal I} K_i K_i^\top - \mu_K \mu_K^\top $. As a result, the moment statistics capture information from the keys contained in the bulk. 
A key point to note is that while the definition of $\mu_K$ involves a sum over all the bulk tokens, computing $\mu_{K}$ does \textbf{not} cost $O(Td)$ operations per token, as it can be updated at each step during decoding for a cost of $O(d)$ operations by using the fact that $\mu_{K, T+1} =  \frac{1}{T^{\text{bulk}} + 1}\sum_{i \in [T+1] \setminus \mathcal I} K_i = 
\frac{T^{\text{bulk}}}{T^{\text{bulk}} + 1}\mu_{K, T} + \frac{K_{T+1}}{T^{\text{bulk}} + 1}$.
The same applies to $\Sigma_k$ (for an update cost of  $O(d^2)$ operations).

\underline{\textit{Option 2 (other prompt):}} Maybe surprisingly, we show in Section~\ref{sec:ablations}  and Appendix~\ref{sec:additional_exps} that we obtain more robust performances by computing the mean $\mu_K$ and covariance $\Sigma_K$ from keys generated from a \textit{different prompt} of similar length. We refer the reader to Appendix~\ref{sec:keys} for additional details. While such an approach may appear counter-intuitive, we hypothesize that $\mu_K$ and $\Sigma_K$ benefit from reflecting a ``generic distribution of keys'', rather than that of the current prompt. While the underlying reasons for this remain unclear, this intuition is supported by the fact that we show in Section~\ref{sec:ablations} that using a \textit{random words} prompt yields robust performance.  While the distribution of keys becomes independent of the current prompt, query-dependency still persists as inner product involves the query.























\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{plots/figure3.pdf}
    \caption{\small Comparison of QAdA against the adaptive and static oracles on the RULER benchmark.  
\textit{Left:} For Llama 3-8B, we show the average performance (top) over the selected RULER 16k tasks as a function of the average sparsity for varying thresholds $\tau$, along with the worst-case performance drop (\%) compared to the baseline performance among the selected tasks.  
\textit{Middle and Right:} Average performance and worst-case drop for a fixed sparsity level of 0.85 across three model families—Llama, Mistral, and Qwen—on RULER 8k (center) and RULER 16k (right).  
Our adaptive criterion consistently matches or outperforms the static oracle criterion, and in some cases (e.g., Mistral), even achieves performance comparable to the adaptive oracle.}
    \label{fig:compare-approx}
\end{figure*}



\subsection{Summary of inference pipeline and run-time complexity}
\label{sec:pipeline}

We describe how our adaptive criterion can be applied in practice by decoding LLMs and explain how this can lead to decreased run-time complexity.

Before starting generation,  we calculate the moment statistics $\mu_K$ and $\Sigma_K$. Then, during decoding, before computing the attention output for a layer, we update the moment statistics $\mu_K$ and $\Sigma_K$ and apply the query-adaptive criterion to every head in the layer, thus labeling a subset of them as local heads. We approximate the output of those using a local window, and compute the output of the others the usual way. We summarize the procedure in Listing~\ref{lst:adaptive_attention}.



\begin{table}[t]
    \centering
    \begin{tabular}{lll}
        \toprule
        criterion & criterion comp. & attention comp. \\
        \midrule
        none&  -  & $O(Td)$ \\
        oracle &  $O(Td)$ & $O((1-\rho) Td +  \rho T_{\text{local}}d)$ \\
        QAdA &  $O(T_{\text{local}}d +d^2)$  &  $O( (1-\rho) Td + \rho T_{\text{local}}d)$  \\
        \bottomrule
    \end{tabular}
        \caption{\small Run-time complexity of the oracle and adaptive criterion, as well as the cost of computing the resulting approximate attention. $\rho$ is the fraction approximated by a local window of size $T_{\text{local}}$. }\vspace{-0.2in}
    \label{tab:complexity}
\end{table}

Unlike the oracle criterion from Equation~\eqref{eq:oracle}, our query-adaptive criterion achieves a constant run-time complexity in $T$ assuming that $T_{\text{local}} \ll T$. 
Moreover, let $\rho$ be the fraction of times a head has been labeled as local head and $d$ be the head dimension: then the average cost of computing the next token using the (approximated) attention mechanism  is $O((1-\rho)  Td + \rho  T_{\text{local}}d)$, as opposed to the $O(Td)$ operations required by the standard attention mechanism. These computations are summarized in Table~\ref{tab:complexity}.



















\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{plots/individual/Longlegend.pdf}
    \begin{subfigure}[b]{0.24\linewidth}
        \centering
\includegraphics[width=\linewidth]{plots/Long.pdf}
        \caption{LongBench Performance}
        \label{fig:longbench}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
        \centering
\includegraphics[width=\linewidth]{plots/Longdrop.pdf}
        \caption{LongBench Max Drop}
        \label{fig:longbenchmin}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
        \centering\includegraphics[width=\linewidth]{plots/mbpp_60.pdf}
        \caption{Long-Context MBPP}
        \label{fig:mbpp}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\linewidth}
        \centering\includegraphics[width=\linewidth]{plots/gsm8k_60.pdf}
        \caption{Long-Context GSM8k}
        \label{fig:gsm8k}
    \end{subfigure}
        \begin{subfigure}[b]{0.64\linewidth}\centering
        \includegraphics[width=0.8\linewidth]{plots/ablations/mean_legend.pdf}
        \begin{subfigure}[b]{0.49\linewidth}
        \includegraphics[width=\linewidth]{plots/ablations/vt.pdf}
                            
        \caption{varying prompts, ``vt'' task}
        \label{fig:prompts}
        \end{subfigure}
        \begin{subfigure}[b]{0.49\linewidth}
        \includegraphics[width=\linewidth]{plots/ablations/fwe.pdf}
        \caption{varying prompts, ``fwe'' task}
        \label{fig:promptsfwe}
        \end{subfigure}
    \end{subfigure}
\begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/ablations/seqlen_legend.pdf}
        \includegraphics[width=\linewidth]{plots/ablations/seqlen.pdf}
        \caption{varying seq. lengths}
        \label{fig:seqlen_prompt}
        \end{subfigure}

    \caption{\small \textbf{Top row:} Similar to Figure~\ref{fig:compare-approx}, we show the average  performance for the LongBench benchmark, the pass@1 score for the MBPP task and the f1-score for the GSM8k task. 
    \textbf{Bottom row:}   Ablations for the content of the prompt (e-f) and the length of the prompt (g) used to generate the mean $\mu_K$ and covariance $\Sigma_K$ for the adaptive criterion from Section~\ref{sec:method}.  We show the normalized performance as a function of sparsity  (e) for the  ``vt'' task and (f) for the ``fwe'' task and (g) averaged over the RULER $8$k tasks,  respectively.}    
\end{figure*}





\section{Evaluation on downstream tasks}
\label{sec:downstream}

\subsection{Experimental Setting}
\label{sec:expsetting}





\paragraph{Datasets.}
We evaluate on the two standard long-context benchmarks, RULER \citep{hsieh2024ruler} and LongBench \citep{bai2024longbench}. We also propose long-context variants of  GSM8k \citep{cobbe2021training} and MBPP \citep{austin2021program}, where we  ``hide''  informative few-shot examples in a long-context prompt containing roughly $\approx 10k$ tokens. We refer the reader to Appendix~\ref{sec:apx-exp-details} for further details. 



\paragraph{Models.}
Our default model is the instruction fine-tuned \textit{Llama 3-8B} model.
We also use the two models
 \textit{Mistral-7B-Instruct-v0.2} and  \textit{Qwen2-7B-Instruct} as provided by \textit{HuggingFace}.
    To account for longer contexts, we set our models' RoPE parameter to $\theta = 2'000'000$, which is approximately the value from the NTK-aware interpolation \citep{peng2023ntk} for a context length of $32k$. 
 For all evaluations, we choose a temperature of $0$, i.e.~use the greedy decoding strategy. We always let $T_{\text{local}} = 128$ and use the first $T_{\text{init}}=16$ tokens as ``attention sink'' tokens, leaving $112$ tokens from the neighorhood closest to the current token (or sliding window).




\paragraph{Methods}
We implement the query-adaptive \textbf{oracle} criterion (Equation~\ref{eq:oracle}), alongside with two query-independent static criteria, \textbf{static oracle} and \textbf{static RULER}. The static method, for a fixed sparsity threshold of $\alpha$ (we ablate over intervals of 5\%), permanently labels as local the $\alpha$ percentage of heads that were most often labeled as local by the oracle criterion on prompts from the RULER tasks. 
The oracle static method, for a fixed sparsity threshold of $\alpha$, labels as local the $\alpha$ of heads that are most often labeled as local by the oracle criterion on the prompts of the processed task. See Appendix~\ref{sec:apx-exp-details} for further details.

We implement QAdA from Section~\ref{sec:method} for four choices of prompts (see Section~\ref{sec:moments}): The current prompt, described as Option 1 in Section~\ref{sec:moments}, and three variants of Option 2:  randomly sampled independent words from Wikipedia (\textit{random words prompt}), concatenated Wikipedia extracts (\textit{wiki prompt}), and repetitions of single word (\textit{single word prompt}). Only the statistics $\mu_K$ and $\Sigma_K$ generated from  the current prompt contain information about the prompt, while the others are agnostic to the current prompt. Our ablation in Subsection~\ref{sec:ablations} suggest that Option 2 (random words prompt) yields the most robust performance. 

\paragraph{Metrics}
We use the standard metrics for evaluation provided by the corresponding benchmarks, which we refer to as the \textbf{performance}. For the LongBench benchmark, we compute the average normalized performance \textbf{(avg. norm. performance)}, which is obtained by dividing the performance by the performance of the standard full attention model. We always plot the performance as a function of the \textbf{sparsity}, that is the average percentage of heads labeled as local heads, and thus approximated by a local window. For both our adaptive, as well as the static criteria, the sparsity almost directly translates into a reduction of FLOPs used by the attention mechanism (minus a small constant overhead to compute the local scores). 









\begin{figure*}
    \centering    
        \begin{subfigure}[b]{0.29\linewidth}
        \centering
\includegraphics[width=\linewidth]{plots/legend_timewise_approx_fwe.pdf}\\
\includegraphics[width=\linewidth]{plots/timewise_approx_fwe.pdf}
        \caption{query-wise adaptivity}
        \label{fig:timewise-fwe}
    \end{subfigure}
\begin{subfigure}[b]{0.34\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/tau.pdf}
        \caption{oracle vs QAdA}
        \label{fig:threshold}
    \end{subfigure}
\begin{subfigure}[b]{0.34\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/tau-adaptive_.pdf}
        \caption{sparsity of QAdA by tasks}
        \label{fig:threshold_per_task}
    \end{subfigure}
    \caption{\small 
    \textbf{a)} The mean and standard deviation of the fraction of heads labeled as local heads as a function of time-steps for prompts from the ``fwe'' task. 
   \textbf{ b)}  The average sparsity and standard deviation as a function of the threshold $\tau$ for Llama 3-8B over the RULER 8k and 16k, as well as the LongBench tasks. The annotations show the mean and standard deviation of the normalized performances (with $1$ being the performance of the standard dense attention). 
\textbf{c)} The average sparsities as a function of the threshold $\tau$, similar to those shown in b), are presented for each task, specifically for the QAdA criterion. Additionally, we present the average sparsity for a context-independent task. This task does not require context to be solved, and we observe that QAdA labels significantly more heads as local heads for the same threshold.} 
\end{figure*}








\vspace{-0.1in}
\subsection{Performance on RULER and LongBench}
\label{sec:downstream_analysis}
\paragraph{Oracle gains over static.} We begin by comparing the adaptive oracle criterion against the  static oracle criterion. We observe significant gains in performance across all models on the RULER benchmark in Figure~\ref{fig:compare-approx}, both in terms of the average performance, as well as the worst-case performance drop. The same observation also holds for the experiments on the LongBench benchmark in  Figure~\ref{fig:longbench},\ref{fig:longbenchmin}. For instance, for the Llama model we see a 20\% increase in sparsity on the RULER tasks (from $\approx 70\%$ to $\approx 90\%$) and a $\approx 5-10\%$ increase on LongBench tasks at fixed performance level. These results underline the potential gains that are achievable by adaptive criteria for selecting attention heads over static ones.  



\paragraph{QAdA outperforms static.}
We observe that our efficient adaptive criterion significantly outperforms the static criterion  on the RULER task for sequence lengths of $8$k in Figure~\ref{fig:compare-approx}, and also for lengths $16$k for the Llama model. 
Moreover, our adaptive criterion matches the performance of the oracle static criterion and even slightly outperforms it on LongBench in Figure~\ref{fig:longbench} and Mistral on RULER $16$k. The only situation where we see performance drops compared to the static method is for Qwen on RULER $16$k, where the score of the baseline model is itself very low. These results demonstrate that our criterion is capable of exploiting the query-adaptivity of attention heads. 



\paragraph{Outperforming the standard dense attention with Qwen} Finally, we observe in Figure~\ref{fig:compare-approx} that both the oracle adaptive criterion and our adaptive criterion surpass the baseline performance of the standard full attention for Qwen on RULER $8k$ (see Figure~\ref{fig:qwen9k} in the Appendix).
These gains are even more visible for the oracle criterion on RULER $16$k, where we find an average performance increase of more than 15 points for a sparsity of $0.85$.  
It is also worth noting that these gains are made possible by a query-adaptive approach and do not occur for static methods. These improvements highlight the fact that in long-context settings, models may attend to unnecessary parts of the context, which the query-adaptive criterion can effectively prune. Consequently, in such settings, the query-adaptive criterion can provide benefits beyond computational efficiency, also leading to enhanced performance.




\subsection{Performance on reasoning and code tasks }
While both the RULER and LongBench benchmarks require only short answers (sometimes less than 20 tokens), we also wonder how well our method is capable of selecting the ``right'' heads in challenging reasoning and code generation tasks, where the expected answers tend to be longer. 
We propose two long-context variants of the GSM8k and MBPP tasks (we provide examples in the Appendix) where we hide a few relevant few-shot examples in a mostly irrelevant long prompt. As instruction fine-tuned models do not require few-shot COT examples for solving the tasks, we instead use the pre-trained version of Llama 3-8B which heavily relies on these examples. 




We show in Figure~\ref{fig:mbpp} and Figure~\ref{fig:gsm8k} the performances on the long-context variants of the two tasks as a function of sparsity. We again observe that our adaptive criterion yields robust performance, outperforming the static criteria. Particularly striking are the gains for the long-context MBPP task, where both the oracle criterion and our query-adaptive criterion let us approximate almost all heads as local heads (more than 95\%), while the performance of the static approaches significantly decreases beyond $80\%$ sparsity.


\subsection{Ablations over moment statistics}
\label{sec:ablations}
In this section, we present  ablations for the choice of the prompt used to generate the  mean $\mu_K$ and variance $\Sigma_K$ statistics, as described in Section~\ref{sec:moments}.


\paragraph{Prompt.} We ablate in Figure~\ref{fig:prompts}-\ref{fig:promptsfwe} over the content of the prompts used to generate the moments statistics. We show the curves only for the two illustrative RULER tasks ``variable tracing'' (``vt''), that has a highly repetitive structure,  and ``frequent word extraction'' (``fwe'').
Maybe surprisingly, we find for the ``vt'' task that the best performance is attained when using randomly sampled words, while repetitively using the same words results in the worst performance. Moreover, using the exact moments (i.e., \textit{current prompt}) also results in very poor performance. This is not the case for the ``fwe'' task, where using the current prompt achieves the best performance. We believe that the failure on the ``vt'' task is explained by the repetitive structure of the prompt, which resembles the structure of the repeated single word prompt that also yields very poor performance. 
 In summary, we find that although using  ``current prompt'' can sometimes yield strong performance (``fwe'') task, it is not robust to the choice of task. In contrast,  ``random words prompt'' using a distinct dataset yields more robust performance. We present additional related experiments in Figure~\ref{fig:ablations-vt-extra} in the Appendix.
 


\paragraph{Sequence Length.}
We compare in Figure~\ref{fig:seqlen_prompt} the performances of our query-adaptive method using Option 2 (random words prompt) for different lengths of the prompt used to generate the  mean $\mu_K$ and covariance $\Sigma_K$.  We show the average normalized performance across all RULER  $8$k tasks.
We see drastic drops in performance when the prompt used to compute the statistics gets longer than the length of the actual prompt (that is $\approx 8100$ tokens long), whereas performance is surprisingly robust to variations for shorter sequence lengths. This dependence to the length of the random words prompt suggests that while the statistics $\mu_K$ and $\Sigma_K$ do not contain any information about the task (as we use random words), they nevertheless contain positional information critical for the criterion to identify the right set of local heads.













\section{Discussion: Adaptivity to contexts}
\label{sec:rec}
We saw in the previous section that QAdA is capable of selecting relevant heads for solving the corresponding long-context tasks. In this section, we investigate which heads are selected by the model, and to what extent the model selects heads based on the context. Besides prompts from the RULER and LongBench tasks, we also study the behavior on a \textit{context-independent} task where. More precisely, we take the context from the ``qa-2'' task from the RULER benchmark but replace the question with: \textit{Can you describe LLMs in a few sentences?}. To solve this task, the model does not need to attend on the context, and we show that the model indeed labels more heads as local heads. This shows that the model is capable of \textit{adapting to the context}. 


\paragraph{Query-wise sparsity.} As a first question, we investigate whether QAdA is capable of changing the sparsity (average fraction of heads labeled as local heads) on a query-wise basis. We provide an illustrative example in Figure~\ref{fig:timewise-fwe}, showing the average percentage of heads chosen by both the oracle and the adaptive criterion as a function of the time-step (query). We choose the "fwe" task, for which all responses to the prompts follow exactly the same pattern, and plot the mean and standard deviation as a function of the index of the generated token.
We observe that the trend of our adaptive criterion aligns closely with the trend of the oracle criterion, and both vary strongly from token to token.

\paragraph{Sparsity vs. Threshold.} We further plot in Figure~\ref{fig:threshold} the average sparsity and the standard deviation of QAdA and the oracle criterion as a function of the threshold $\tau$.
We make two findings: first, that QAdA closely follows the sparsity of the adaptive oracle criterion but tends to label slightly more heads as long-context. Second, that the standard deviation of the average sparsity (with respect to different tasks) is non-negligible, meaning that the sparsity can vary from task to task. This indicates that our adaptive criterion effectively adjusts the level of sparsity and is capable of adapting to "difficult" tokens. Indeed, we further show in Figure~\ref{fig:threshold_per_task} the average sparsities for each task for QAdA. We also plot in green the average sparsity when asking the model to generate a response for a task that does not require any knowledge from the context. As we can see, the QAdA uses significantly fewer heads as long-context heads for this task than for the other tasks at the same threshold.



\paragraph{Distribution of local heads across layers.}
Finally, in Figure~\ref{fig:heatmap} and Figure~\ref{fig:heatmap_independent}, we show the average percentage of times each head has been labeled as long-context for the RULER tasks and the context-independent tasks. For the RULER tasks, which require the model to look at the entire context, we see that both criteria show matching patterns and long-context heads occur across all layers. This demonstrates that our adaptive criterion successfully identifies long-context heads across all layers. Moreover, for the context-independent task, we see that while the first layer still attends to the full context, all layers are essentially always approximated by the local windows.


  










\section{Related works}
\label{sec:relatedworks}
There is an overwhelming body of work studying and exploiting sparsity in attention heads. We refer the reader to \citep{wan2023efficient,zheng2024attention} for surveys and only discuss the most directly related works here. 


\begin{figure*}
    \centering    
        \begin{subfigure}[b]{0.49\linewidth}
        \centering
\includegraphics[width=\linewidth]{plots/layersheatmap.pdf}
        \caption{RULER tasks}
        \label{fig:heatmap}
    \end{subfigure}
        \begin{subfigure}[b]{0.49\linewidth}
        \centering
\includegraphics[width=\linewidth]{plots/layersheatmap_sparse.pdf}
        \caption{context-independent task}
        \label{fig:heatmap_independent}
    \end{subfigure}
    \caption{\small  We show for both the oracle adaptive and the adaptive criterion the \% of times each head has been labeled as long-context head averaged over a) the six RULER 8k tasks with  $\thrsoracle=\thrsapprox=0.6$ and b) the context-independent task based on the ``qa-2'' task from RULER.} 
    \label{fig:heatmaps}
\end{figure*}

\paragraph{Static classification of heads } 
\citet{wu2024retrieval} showed that a few attention heads, called ``retrieval heads,'' are particularly critical in retrieving long-context information, with multiple follow-up works \citep{tang2024razorattention,hong2024token,xiao2024duoattention,cai2024pyramidkv,he2025task}. Most related to this paper is \citet{xiao2024duoattention}, who also proposed dividing the heads into long-context and local heads. All these methods statically assign labels to the heads before generation. They do so by analyzing attention patterns on selected tasks, or, as done in \citep{xiao2024duoattention}, learn the assignment using gradient descent. Our paper crucially differs from these works as we explore the \textit{query-adaptive} nature of attention heads to their changing contexts and do not require an additional dataset to label the heads.

\paragraph{Query-adaptive sparsity.}
Similar to this paper, there is an entire line of research that exploits query-dependent sparsity in some way. For instance, numerous works propose efficient approximations that retrieve per head the subset of tokens with the highest scores \citep{tang2024quest,ribar2023sparq,chen2021scatterbrain,sun2024shadowkv}. For context, multiple works also propose static variants that select the tokens for all queries \citep{zhang2023h2o,li2024snapkv,oren2024transformers}. These works are complementary to this paper. More related to this paper is the approach taken by \citep{liu2023deja,akhauri2024shadowllm}, where a classifier is trained to dynamically predict which attention heads can be ``dropped.'' The classifier takes as input the residual of an earlier layer and thus also adapts to the changing contexts. However, our paper crucially differs in two ways: first, we do not rely on any additional dataset for labeling the heads, nor do we require training an additional classifier. Second, we also distinguish between local and long-context heads, and do not simply drop heads.






\section{Limitations}
This paper highlights the query-adaptive nature of attention heads in the way they retrieve long-context information, and provides a second order statistics-based test for locality. However, we do not test and provide a highly optimized implementation compatible with flash attention, and we do not showcase real run-time gains. This was out of scope for the current work and is an exciting area for future research.






\section{Conclusions }
Our first key finding shows that the attention head exhibits two distinct behaviors: local- it attends to local tokens and long-context- it attends to tokens beyond local tokens. This behavior is query-dependent, and perhaps surprisingly, a simple test QAdA (Query-Adaptive Attention) based on the second-order statistics of the keys and local scores is quite effective in predicting this behavior. We tested the efficacy of QAdA through state-of-the-art models such as Llama, Qwen, and Mistral (7 to 8 billion parameters) and various important long-context benchmarks, including RULER and Longbench. Through rigorous ablations, we present a deeper understanding of the inner workings of the test and the attention mechanism.














