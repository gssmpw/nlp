
\documentclass[]{fairmeta}


\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} %

\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}


\usepackage{tcolorbox}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\newcommand{\kd}[1]{\textcolor{blue}{kd: #1}}
\newcommand{\ca}[1]{\textcolor{red}{CA: #1}}
\newcommand{\viv}[1]{\textcolor{red}{VC: #1}}
\definecolor{myred}{HTML}{CB4154}

\usepackage{amsmath}
\usepackage{bbm}
\usepackage{booktabs}
\newcommand{\indicator}[1]{\mathbbm{1}\left\lbrace #1\right\rbrace}
\newcommand{\thrsoracle}{\tau_{\text{oracle}}}
\newcommand{\thrsapprox}{\tau_{\text{approx}}}
\newcommand{\thrsstatic}{\tau_{\text{static}}}

\newcommand{\tbulk}{T^{\text{bulk}}}

\newcommand{\oraclecriterion}{c^h_{\text{oracle}}}


\usepackage{listings}
\usepackage{xcolor}
\definecolor{keyword}{rgb}{0.75, 0.13, 0.13}
\definecolor{comment}{rgb}{0.25, 0.5, 0.35}
\definecolor{string}{rgb}{0.6, 0.1, 0.1}
\usepackage{listings}
\usepackage{xcolor}







\definecolor{codeblue}{rgb}{0.13, 0.13, 0.75}
\definecolor{codegray}{rgb}{0.5, 0.5, 0.5}
\definecolor{codepurple}{rgb}{0.58, 0.0, 0.82}
\definecolor{backcolour}{rgb}{0.98, 0.98, 0.98}

\definecolor{functionblue}{RGB}{67,110,238}    %
\definecolor{variablegreen}{RGB}{102,153,0}    %
\definecolor{torchfunc}{RGB}{255,140,0}        %
\definecolor{outputpurple}{RGB}{147,112,219}   %
\definecolor{keywordcolor}{RGB}{0,119,170}     %


\definecolor{lightgreen}{RGB}{76,175,80}     %
\definecolor{darkgreen}{RGB}{46,125,50}      %
\definecolor{torchfunc}{RGB}{255,140,0}      %
\definecolor{outputpurple}{RGB}{147,112,219} %

\usepackage{listings}
\usepackage{xcolor}

\definecolor{backcolour}{rgb}{0.99,0.99,0.99}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codeblue}{rgb}{0.0,0.0,0.5}
\definecolor{codepurple}{rgb}{0.58,0.0,0.82}
\definecolor{orange}{rgb}{1.0,0.5,0.0}

\lstset{
    language=Python,
    backgroundcolor=\color{backcolour},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=true,
    breaklines=true,
    captionpos=b,
    commentstyle=\color{codegray}\itshape,
    keepspaces=true,
    keywordstyle=\color{codeblue}\bfseries,
    numbers=left,
    numbersep=5pt,
    numberstyle=\tiny\color{codegray},
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    stringstyle=\color{codepurple},
    tabsize=4,
    frame=single,
    rulecolor=\color{black},
    emphstyle=\color{orange},
    emph={einsum, lse, stack, local_attn_, dense_attn_, sqrt, log},
    morekeywords={def, return}
}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}



\usepackage[textsize=tiny]{todonotes}


\title{Unveiling Simplicities of Attention:\\ Adaptive Long-Context Head Identification}

\author[2,*]{Konstantin Donhauser}
\author[1]{Charles Arnal}
\author[1]{Mohammad Pezeshki}
\author[1]{Vivien Cabannes}
\author[1]{David Lopez-Paz}
\author[1]{Kartik Ahuja}

\affiliation[1]{FAIR at Meta}
\affiliation[2]{ETH Zurich}

\contribution[*]{Work done at Meta}

\abstract{
The ability to process long contexts is crucial for many natural language processing tasks, yet it remains a significant challenge. While substantial progress has been made in enhancing the efficiency of attention mechanisms, there is still a gap in understanding how attention heads function in long-context settings. In this paper, we observe that while certain heads consistently attend to local information only, others swing between attending to local and long-context information depending on the query. This raises the question: can we identify which heads require long-context information to predict the next token accurately? We demonstrate that it's possible to predict which heads are crucial for long-context processing using only local keys. The core idea here is to exploit a simple model for the long-context scores via second moment approximations. These findings unveil simple properties of attention in the context of long sequences, and open the door to potentially significant gains in efficiency.
}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{subcaption}

\date{\today}
\correspondence{First Author at \email{konstantin.donhauser@ai.ethz.ch}}


\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\textbf{#1}}
\usepackage{amsmath}



\begin{document}

\maketitle


\input{sections/introduction_new}
\input{sections/related_work}
\input{sections/method}





\bibliography{paper}
\bibliographystyle{assets/plainnat}


\appendix
\onecolumn





\section{Additional Experimental Details}
\label{sec:apx-exp-details}
In this section we present additional details for the experiments.



\paragraph{Additional details for the methods }
The best way to select the ``right'' subset of attention heads for the static criterion is still widely understudied. In particular, it poses  the fundamental challenge of which dataset should be chosen to select the heads in advance. Since we are primarily interested in how much query-adaptivity helps to improve, we compare against a \textbf{static oracle} criterion, that uses the prompts for evaluation to decide which heads are sued as static heads. Moreover, we also implement \textbf{static RULER}, using the prompts from the RULER task. We present additional ablations for the choice of the static criterion in Figure~\ref{fig:staticablations}.
Similar to \citet{wu2024retrieval,tang2024razorattention}, we measure head patterns in a synthetic retrieval task, and select heads via the following  simple \textbf{static criterion}: 
\begin{itemize}
    \item \textit{Step 1}: Generate responses for selected prompts using full attention (for LongBench, GSM8k and MBPP tasks) or the approximate attention from the oracle criterion with $\thrsoracle =0.6$ (RULER tasks). Compute the percentage of times each head is labeled as local window by the oracle criterion from Equation~\eqref{eq:oracle} with threshold $\thrsstatic$.
\item
\textit{Step 2}: Calculate the $(1-\alpha)$-quantile of these percentages across all heads $h$. Label heads below the threshold as \textit{long-context} ($c^h_{\text{static}} = 0$) and those above as \textit{local} ($c^h_{\text{static}} = 1$). These labels are query-independent.
\end{itemize}

We further refer the reader to Appendix~\ref{sec:keys} for how we compute the moments used by  \textbf{QAdA}, for which we devote an entire section. 

\paragraph{Choices for thresholds} We ablate over the various thresholds $\thrsoracle, \thrsapprox \in $ (0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99, 0.995), as well as 
$\alpha \in $ (0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6) with $\thrsstatic=0.6$. We ran additional ablations in Figure~\ref{fig:static} for $\thrsstatic$ confirming that the choice $\thrsstatic=0.6$ yields robust performance across all tasks. 


\paragraph{RULER tasks} The RULER benchmark \citep{hsieh2024ruler} consists of a collection of synthetic tasks with varying prompt sizes. These tasks are designed to challenge the model's capabilities in processing long-context information.
We choose the two Q/A tasks, ``qa-1'' and ``qa-2'', the two aggregation tasks: common words extraction ``cwe'' and frequent words extraction ``fwe'', the variable tracing task ``vt'', and the multiquery needle-in-a-haystack task ``niah''. Especially, the two aggregation tasks ``fwe'' and ``cwe'' are known to be difficult baselines for achieving accuracy using efficient sparse attention mechanisms (see the discussion in \citet{chen2024magicpig}).

\paragraph{LongBench tasks} The LongBench benchmark contains a selection of challenging real-world and synthetic tasks, including single-doc QA, multi-doc QA, summarization, and few-shot learning.
We use a selection of tasks from the LongBench dataset for which the standard model achieves at least decent scores. We evaluate on the tasks: (Single-Document QA): ``qasper'', ``multifieldqa-en'', ``multifieldqa-zh'', ``narrativeqa''; (Multi-Document QA): ``2wikimqa'', ``musique'', ``hotpotqa''; (Summarization): ``qmsum'', ``vcsum''; and (Few-shot Learning): ``triviaqa''.





\begin{figure*}[t]
    \centering    
        \centering
            \includegraphics[width=\linewidth]{plots/static/mean_legend.pdf}

                \begin{subfigure}[b]{0.47\linewidth}
\includegraphics[width=\linewidth]{plots/spearman.pdf}

        \caption{ Spearman rank correlation of heads}
        \end{subfigure}
                \begin{subfigure}[b]{0.52\linewidth}
\includegraphics[width=\linewidth]{plots/static/mean.pdf}
    
        \caption{ Ablation over datasets for static criterion}
        \label{fig:static}
        \end{subfigure}
    \caption{\small \textbf{a)}  The Spearman rank correlation of the attention heads ordered by the fraction of times labeled as Local Heads by the oracle criterion with $\tau=0.6$. We see a high correlation among all tasks. b) Ablations for the static criterion using different datasets (LongBench, RULER and specific RULER task, called oracle) and threshold $\thrsstatic$ to label the heads. We use Llama3-8B on RULER 8k.} 
    \label{fig:staticablations}
\end{figure*}



\paragraph{Long-context GSM8k and MBPP datasets}

In addition to the two standard benchmarks, RULER and LongBench, we also construct our own long-context tasks based on the reasoning task GSM8k \citep{cobbe2021training} and the code-generation task MBPP \citep{austin2021program}. We use the standard evaluation protocol, but instead of using only the ``correct'' few-shot examples, we select 55 few-shot examples in the same format generated from the SQUAD \citep{rajpurkar2016squad} dataset, as well as 5 actual few-shot examples (highlighted in green). We provide fragments of the example prompts below. The resulting context lengths are $\approx 10k$ for GSM8k and $\approx 11k$ for MBPP.

For these two tasks, we always use the pre-trained Llama3-8B parameter model \citep{dubey2024llama}, instead of the instruction fine-tuned variant. The reason for choosing the pre-trained model is that the instruction fine-tuned model can solve these tasks without the need for few-shot examples, while the pre-trained model crucially depends on few-shot examples. Since these examples are hidden in a long context, the task becomes challenging, and the model requires retrieving information from tokens far away in order to achieve high accuracy on the task.






\section{Computing the moment statistics}
\label{sec:keys}
We discuss in this section more formally how we obtain the moment statistics as sketched in Section~\ref{sec:moments}.



\begin{figure*}
    \centering    

\begin{subfigure}[b]{0.24\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/truepositive.pdf}
        \caption{oracle vs adaptive}
        \label{fig:accuracy}
    \end{subfigure}
        \begin{subfigure}[b]{0.24\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/recall/summary.pdf}
        \caption{recall of aggregation}
        \label{fig:recalla}
    \end{subfigure}
        \begin{subfigure}[b]{0.24\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/recall/qa.pdf}
        \caption{recall of Q/A}
        \label{fig:recallb}
    \end{subfigure}
        \begin{subfigure}[b]{0.24\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/recall/retrieval.pdf}
        \caption{recall of retrieval}
        \label{fig:recallc}
    \end{subfigure}

    
    \caption{\small \textbf{a)}  Accuracy and fraction of true/false  positives/negatives for the 10\% quantiles of the heads (labeled as local heads) for the adaptive criterion with  $\thrsoracle=\thrsapprox=0.6$ on the  RULER benchmark with sequence length 8k. 
    \textbf{b,c,d)} The recall values of long-context heads selected by the oracle criterion for various thresholds $\thrsoracle$ when using the static and adaptive oracle criteria as a function of the average sparsity (percentage of local heads). We adjust the thresholds $\alpha$ (with $\thrsstatic = \thrsoracle$) and $\thrsapprox$ to achieve matching sparsity levels. Annotations indicate the specific oracle thresholds $\thrsoracle$.  We use Llama3-8B on RULER 8k.} 
    \vspace{-0.2in}
\end{figure*}


\paragraph{Option 1 (current prompt):} In this case, after pre-filling, we compute the moment statistics for each head as described in Section~\ref{sec:moments}. Note that for grouped-query attention \citep{ainslie2023gqa}, as used by Llama, we naturally use the same moments for each query in the group since these heads share the same keys. During generation, we keep the moment statistics fixed and do not update them after predicting each token. This is because we always generate sequences of length less than $256$, so updating the statistics has only a limited influence. However, when generating long sequences consisting of thousands of tokens, we would expect that updating the moments during generation becomes beneficial for performance.




\paragraph{Option 2 (other prompt):} In this case, we perform a single forward pass using one of the three choices as prompts: \textit{random word prompt}, which simply permutes words from a Wikipedia article (including the HTML syntax); \textit{wiki prompt}, where we concatenate Wikipedia articles; and \textit{single words prompt}, where we repeat the word "observation." As we showed in Section~\ref{sec:ablations}, the content of the prompt is not important as long as there is enough "diversity." However, we found that the length of the sequence is crucial. Therefore, we store all keys from the forward pass of this prompt. During generation, when predicting the next tokens for a given prompt, we load the keys from the specific \textit{other prompt} and generate the moments using the first $T-1024$ keys, where $T$ is the sequence length of the current prompt. The reason for choosing minus $1024$ is because, as we saw in Figure~\ref{fig:seqlen_prompt}, the performance is robust to keys generated from shorter prompts than the actual sequence but suffers significantly in performance for longer ones. As an alternative implementation, one could also pre-compute the moments for lengths of fixed intervals and load the corresponding moment after pre-filling before starting the generation.














\section{Recall of Attention Heads}
\label{sec:recall}
In this section, we analyze how well our adaptive criterion from Section~\ref{sec:method} can recall the heads selected by the oracle criterion; in other words, how effectively it serves as a proxy for the oracle. We always use the current prompt (Option 1) to generate the moment statistics. 



\paragraph{Accuracy}

We generate responses using standard dense attention and store the scores used to compare the two criteria using the current prompt to generate the moments. For each task, we group the heads into $10\%$ quantiles based on the percentage of times the oracle criterion has been satisfied. For each quantile (averaged over the six selected RULER tasks), we show the fraction of true positives, true negatives, false positives, and false negatives, where a true positive means that both the oracle and adaptive criteria labeled a head as a local head.

We find that the adaptive criterion always correctly identifies the top $50\%$ of the heads that are consistently local heads. Moreover, we find even higher accuracies for the lower quantiles where heads vary between local and long-context. Interestingly, we see that the false negative rate is much lower than the false positive rate for these heads. As a result, the adaptive criterion selects fewer heads than the oracle criterion. This observation is counter-intuitive to the observations made in Section~\ref{sec:rec}, where we observed that our adaptive criterion tends to select more heads than the oracle criterion for the same threshold. The explanation here is that in this section we compare the criterion on scores obtained when using standard full attention. This is necessary to allow a direct comparison between the two criteria. In contrast, in Section~\ref{sec:rec} we compare the average sparsity when using the approximate attention that approximates all labeled heads by a local window.




\paragraph{Recall of long-context heads.} We further compare our adaptive criterion  with the oracle  static criterion in their ability to identify long-context heads selected by the oracle criterion. 
We show in Figure~\ref{fig:recalla}-\ref{fig:recallc} the recall value of long-context heads selected by the oracle criterion for different oracle thresholds $\thrsoracle$ as a function of the sparsity (fraction of heads labeled as local heads by the oracle criterion). 
To allow for a direct comparison between static and adaptive, we  choose $\thrsapprox$, resp. quantile $\alpha$ (with $\thrsstatic = \thrsoracle$), such that the average sparsity is the same as the one of the oracle criterion. We plot the curves for all (selected)  RULER tasks, and find that our test achieves consistently a higher recall value than the oracle static assignment (except for the ``vt'' task, for which the \textit{current prompt} choice for the moments breaks down, as discussed in Section~\ref{sec:ablations}).







\begin{table}[t]
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
Method & all & top 20\% & top 10\% \\
& $\mu \pm \sigma$ & $\mu \pm \sigma$ & $\mu \pm \sigma$ \\
\midrule\midrule
 & \multicolumn{3}{c}{RULER 8k task ``fwe''} \\ 
\midrule\midrule
Log error & $0.41 \pm 0.58$ & $0.50 \pm 0.98$ & $0.57 \pm 1.27$ \\
Dist. local & $3.44 \pm 1.73$ & $1.78 \pm 1.38$ & $1.54 \pm 1.23$ \\
Gaussian opt. & $0.15 \pm 0.18$ & $0.14 \pm 0.21$ & $0.15 \pm 0.25$ \\
\midrule\midrule
 & \multicolumn{3}{c}{RULER 8k task ``Q/A-2''} \\
\midrule\midrule
Log error & $0.37 \pm 0.52$ & $0.63 \pm 0.75$ & $0.74 \pm 0.83$ \\
Dist. local & $2.80 \pm 1.55$ & $1.17 \pm 0.98$ & $1.29 \pm 1.08$ \\
Gaussian opt.  & $0.18 \pm 0.22$ & $0.25 \pm 0.34$ & $0.29 \pm 0.40$ \\
\bottomrule
\end{tabular}
\caption{The mean and standard deviation for the terms  log difference  $|\log A^{\text{bulk}} - (\log(T^{\text{bulk}}) + \mu_s + \sigma_s^2/2)|$ (Log error) and $|\log A^{\text{bulk}} - \log A^{\text{local}}|$ (Dist. local) for all heads (first column) and the 20\% and 10\% percentiles of heads most often labeled as local heads by the oracle criterion with $\thrsoracle=0.6$. We further show the ``Log error'' when replacing the scores by i.i.d.~Gaussian samples instead with matching mean and variance. This indicates the achievable error assuming that the Gaussian approximation holds true.  We use Llama3-8B on RULER 8k.}\label{tab:comparison}
\vspace{-0.1in}
\end{table}





\section{Discussion: Gaussian Approximation}

 \label{apx:gaussian}
In this section, we further discuss the Gaussian approximation exploited  by our criterion in Section~\ref{sec:method}. We divide the discussion into multiple paragraphs.  

\paragraph{Approximatin error} We wonder what is the approximation error arising from Equation~\eqref{eq:gaussianapprox}. 
We show in Table~\ref{tab:comparison}  the average log difference  $|\log A^{\text{bulk}} - (\log(T^{\text{bulk}}) + \mu_s + \sigma_s^2/2)|$  (first row)  between the un-normalized mass of the bulk and our Gaussian approximation from Equation~\eqref{eq:gaussianapprox}.  Taking the exponent, we find that the Gaussian approximation is typically off by a factor of $\approx 2-5$, and thus clearly imprecise. In comparison, in the third row, we show the same statistics, when replacing the scores by i.i.d~samples from a Gaussian distribution with matching mean and variance. This error captures the ``optimal'' error given that Gaussian actually holds. As we can see, this error is significantly smaller. 

Nevertheless, we are effectively interested in whether the Gaussian assumption suffices to make an accurate prediction on whether the head is a local or long-context head. To that end, we also compare in the second row the average log difference  $|\log A^{\text{bulk}} -  \log A^{\text{local}}|$. Indeed, if this distance is much larger than the average log error arising form the Gaussian approximation, we expect our criterion to nevertheless be accurate. As we observe, this is the case. Taking again the exponent, we  find that the $A^{\text{bulk}} $ and $A^{\text{local}}$ typically differ by  factors around $\approx 15-50$. Interestingly, however, we see that the gap becomes more narrow when only considering the top 20\% (resp. 10\%) of heads most frequently selected by the oracle criterion as long-context heads. Finally, we also show the average standard deviation. 














\section{Additional Experiments}

\label{sec:additional_exps}

\paragraph{Ablations for the choice of the prompts}
We show in Figure~\ref{fig:ablations-vt-extra} the plots for the other RULER tasks for the ablations for the choice of the prompt in Figures~\ref{fig:prompts},\ref{fig:promptsfwe} in Section~\ref{sec:ablations}. 



\paragraph{Performances for individual tasks}
We showed in Figures~\ref{fig:compare-approx} and \ref{fig:longbench} the aggregated performances over the tasks. For completeness, we further show in Figures~\ref{fig:llama8k}-\ref{fig:longappendix} the performances for the individual tasks. We further also show the performance of QAdA (current prompt). Interestingly, we observe that the using the random words prompt (Option 2) for generating the keys overwhelmingly often outperforms the use of the current prompt (Option 1). We leave an explanation for this intriguing finding as a task for future work. 











\begin{figure*}[t]
    \centering    
        \centering
        \includegraphics[width=0.8\linewidth]{plots/ablations/mean_legend.pdf}

        \begin{subfigure}[b]{0.24\linewidth}
        \includegraphics[width=\linewidth]{plots/ablations/qa_1.pdf}
                            
        \caption{ ``qa-1'' task}
        \end{subfigure}
        \begin{subfigure}[b]{0.24\linewidth}
        \includegraphics[width=\linewidth]{plots/ablations/qa_2.pdf}
                            
        \caption{ ``qa-2'' task}
        \end{subfigure}
            \begin{subfigure}[b]{0.24\linewidth}
        \includegraphics[width=\linewidth]{plots/ablations/niah_multiquery.pdf}
                            
        \caption{ ``niah'' task}
        \end{subfigure}
            \begin{subfigure}[b]{0.24\linewidth}
        \includegraphics[width=\linewidth]{plots/ablations/cwe.pdf}
                            
        \caption{ ``cwe'' task}
        \end{subfigure}
        

    \caption{\small  Ablations for varying prompts. Same as Figure~\ref{fig:prompts} and \ref{fig:promptsfwe} for the additional RULER $8$k tasks using Llama 3-8B.} 
    \label{fig:ablations-vt-extra}
\end{figure*}




















\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{plots/individual/llama38192.pdf}
    \caption{ Performances for individual tasks for RULER $8$k using Llama-3 8B as in Figure~\ref{fig:compare-approx}}
    \label{fig:llama8k}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{plots/individual/llama316384.pdf}
    \caption{ Performances for individual tasks for RULER $16$k using Llama-3 8B as in Figure~\ref{fig:compare-approx}}
    \label{fig:llama16k}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{plots/individual/mistralai8192.pdf}
    \caption{ Performances for individual tasks for RULER $8$k using Mistral-7B as in Figure~\ref{fig:compare-approx}}
    \label{fig:mistral8k}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{plots/individual/mistralai16384.pdf}
    \caption{ Performances for individual tasks for RULER $16$k using Mistral-7B as in Figure~\ref{fig:compare-approx}}
    \label{fig:mistral16k}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{plots/individual/Qwen8192.pdf}
    \caption{ Performances for individual tasks for RULER $8$k using Qwen-7B as in Figure~\ref{fig:compare-approx}}
    \label{fig:qwen9k}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{plots/individual/Qwen16384.pdf}
    \caption{ Performances for individual tasks for RULER $16$k using Qwen-7B as in Figure~\ref{fig:compare-approx}}
    \label{fig:qwen16k}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{plots/individual/LONG.pdf}
    \caption{\small Performances for individual tasks for LongBench as in Figure~\ref{fig:longbench}}
    \label{fig:longappendix}
\end{figure}


















\begin{figure*}
\begin{tcolorbox}[
    title=Example Prompt for long-context MBPP,
    width=\textwidth,
    colback=white,
    left=5pt,
    right=5pt,
    top=5pt,
    bottom=5pt
]
[...]\\
Q: Due to extreme variation in elevation, great variation occurs in the climatic conditions of Himachal . The climate varies from hot and subhumid tropical in the southern tracts to, with more elevation, cold, alpine, and glacial in the northern and eastern mountain ranges. The state has areas like Dharamsala that receive very heavy rainfall, as well as those like Lahaul and Spiti that are cold and almost rainless. Broadly, Himachal experiences three seasons: summer, winter, and rainy season. Summer lasts from mid-April till the end of June and most parts become very hot (except in the alpine zone which experiences a mild summer) with the average temperature ranging from 28 to 32 °C (82 to 90 °F). Winter lasts from late November till mid March. Snowfall is common in alpine tracts (generally above 2,200 metres (7,218 ft) i.e. in the higher and trans-Himalayan region).\\
What is the climate like?\\
A: varies from hot and subhumid tropical  The answer is varies from hot and subhumid tropical.\\\\
\textcolor{darkgreen}{
Q: James decides to buy a new bed and bed frame.  The bed frame is \$75 and the bed is 10 times that price.  He gets a deal for 20\% off.  How much does he pay for everything?\\
A: The bed cost 75*10=\$750\\
So everything cost 750+75=\$825\\
He gets 825*.2=\$165 off\\
So that means he pays 825-165=\$660 The answer is 660.}
\\\\
\textcolor{darkgreen}{
Q: Liz sold her car at 80\% of what she originally paid. She uses the proceeds of that sale and needs only \$4,000 to buy herself a new \$30,000 car. How much cheaper is her new car versus what she originally paid for her old one?\\
A: If Liz needs only \$4,000 to buy a new \$30,000 car, that means she has \$30,000-\$4,000=\$26,000 from the proceeds of selling her old car\\
If she sold her car at 80\% of what she originally paid for and sold it for \$26,000 then she originally paid \$26,000/80\% = \$32,500 for her old car\\
If she paid \$32,500 for her old car and the new one is \$30,000 then, the new one is \$32,500-\$30,000 = \$2,500 cheaper The answer is 2500.}\\\\
Q: Unlike in multicellular organisms, increases in cell size (cell growth) and reproduction by cell division are tightly linked in unicellular organisms. Bacteria grow to a fixed size and then reproduce through binary fission, a form of asexual reproduction. Under optimal conditions, bacteria can grow and divide extremely rapidly, and bacterial populations can double as quickly as every 9.8 minutes. In cell division, two identical clone daughter cells are produced. Some bacteria, while still reproducing asexually, form more complex reproductive structures that help disperse the newly formed daughter cells. Examples include fruiting body formation by Myxobacteria and aerial hyphae formation by Streptomyces, or budding. Budding involves a cell forming a protrusion that breaks away and produces a daughter cell.\\
What are produced in cell division?\\
A: two identical clone daughter cells  The answer is two identical clone daughter cells.\\\\
\textcolor{darkgreen}{
Q: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for \$2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\\
A:}
\end{tcolorbox}
\end{figure*}



\begin{figure*}

\begin{tcolorbox}[
    title=Example Prompt for long-context GSM8k,
    width=\textwidth,
    colback=white,
    left=5pt,
    right=5pt,
    top=5pt,
    bottom=5pt
]
[...]
You are an expert Python programmer, and here is your task: Due to extreme variation in elevation, great variation occurs in the climatic conditions of Himachal . The climate varies from hot and subhumid tropical in the southern tracts to, with more elevation, cold, alpine, and glacial in the northern and eastern mountain ranges. The state has areas like Dharamsala that receive very heavy rainfall, as well as those like Lahaul and Spiti that are cold and almost rainless. 
Broadly, Himachal experiences three seasons: summer, winter, and rainy season. Summer lasts from mid-April till the end of June and most parts become very hot (except in the alpine zone which experiences a mild summer) with the average temperature ranging from 28 to 32 °C (82 to 90 °F). Winter lasts from late November till mid March. Snowfall is common in alpine tracts (generally above 2,200 metres (7,218 ft) i.e. in the higher and trans-Himalayan region).\\
What is the climate like? Your code should pass these tests:
\\
empty
\\
{[BEGIN]}\\
varies from hot and subhumid tropical\\
{[DONE]}
\\\\
\textcolor{darkgreen}{
You are an expert Python programmer, and here is your task: Write a function to find the similar elements from the given two tuple lists. Your code should pass these tests:\\
assert similar\_elements((3, 4, 5, 6),(5, 7, 4, 10)) == (4, 5)\\
assert similar\_elements((1, 2, 3, 4),(5, 4, 3, 7)) == (3, 4)\\
assert similar\_elements((11, 12, 14, 13),(17, 15, 14, 13)) == (13, 14)\\\\
{[BEGIN]}\\
def similar\_elements(test\_tup1, test\_tup2):\\
  res = tuple(set(test\_tup1) \& set(test\_tup2))\\
  return (res) \\
{[DONE]}}\\\\
You are an expert Python programmer, and here is your task: Unlike in multicellular organisms, increases in cell size (cell growth) and reproduction by cell division are tightly linked in unicellular organisms. Bacteria grow to a fixed size and then reproduce through binary fission, a form of asexual reproduction. Under optimal conditions, bacteria can grow and divide extremely rapidly, and bacterial populations can double as quickly as every 9.8 minutes. In cell division, two identical clone daughter cells are produced. Some bacteria, while still reproducing asexually, form more complex reproductive structures that help disperse the newly formed daughter cells. Examples include fruiting body formation by Myxobacteria and aerial hyphae formation by Streptomyces, or budding. Budding involves a cell forming a protrusion that breaks away and produces a daughter cell.\\
What are produced in cell division? Your code should pass these tests:\\
empty
\\
{[BEGIN]}\\
two identical clone daughter cells\\
{[DONE]}\\\\
\textcolor{darkgreen}{
You are an expert Python programmer, and here is your task: Write a python function to remove first and last occurrence of a given character from the string. Your code should pass these tests:\\
assert remove\_Occ("hello","l") == "heo"\\
assert remove\_Occ("abcda","a") == "bcd"\\
assert remove\_Occ("PHP","P") == "H"\\\\
{[BEGIN]}\\}

\end{tcolorbox}
\end{figure*}


\end{document}
