%

\section{Appendix Notation and Definitions}
We often use the shorthand $(a)_+ \defeq \max(a,0)$ as well as the shorthand $\k(\xset,\xset)$ to represent the matrix $(\k(\x_i,\x_j))_{i,j=1}^n$. 
In addition, for each kernel $\k$, we let $\rkhs$ and $\knorm{\cdot}$ represent the associated reproducing kernel Hilbert space (RKHS) and RKHS norm, so that $\ball_{\kernel}=\{ f\in\rkhs : \knorm{f} \leq 1\}$ and define
\begin{talign}
(\Pin - \Qout)\k \defeq \frac{1}{\nin}\sum_{x\in\xin} \k(\x,\cdot) - \frac{1}{\nout}\sum_{x\in\xout} \k(\x,\cdot).
\end{talign}
%
We also relate our definition of a sub-Gaussian thinning algorithm (\cref{def:alg-subg}) to several useful notions of sub-Gaussianity.
%
\begin{definition}[\tbf{Sub-Gaussian vector}]\label{def:vector-subg}
We say that a random vector $\diff \in \R^n$ is \emph{$(\K,\subg)$-sub-Gaussian on an event $\event$} if $\K$ is SPSD and $\subg>0$ satisfies 
\begin{talign}\label{eq:vector-subg}
    \Esubarg{\event}{\exp(\bu^\top \K \diff)} \leq \exp(\frac{\subg^2}{2} \cdot \bu^\top \K \bu)
    \qtext{for all}
    \bu \in \reals^n.
\end{talign}
If, in addition, the event has probability $1$, we say that $\w$ is \emph{$(\K,\subg)$-sub-Gaussian}.
\end{definition}
%
Notably, a thinning algorithm is $(\K,\subg,\delta)$-sub-Gaussian if and only if its associated vector $\pin-\qout$ is $(\K,\subg)$-sub-Gaussian on an event $\event$ of probability at least $1-\delta/2$.

%
\begin{definition}[\tbf{Sub-Gaussian function}]\label{def:function-subg}
For a kernel $\kernel$, %
we say that a random function $\fsubg\in \rkhs$ is \emph{$(\kernel,\subg)$-sub-Gaussian on an event $\event$} if $\subg > 0$ satisfies
\begin{talign}\label{eq:function-subg}
    \Esubarg{\event}{\exp(\inner{f}{\fsubg}_{\kernel})} \leq \exp(\frac{\subg^2}{2}\cdot \knorm{f}^2)
    \qtext{for all}
    f \in\rkhs.
\end{talign}
If, in addition, the event has probability $1$, we say that $\fsubg$ is \emph{$(\kernel,\subg)$-sub-Gaussian}.
\end{definition}
Our next two lemmas show that for finitely-supported signed measures like $\Pin-\Qout$, this notion of functional sub-Gaussianity is equivalent to the prior notion of vector sub-Gaussianity, allowing us to use the two notions interchangeably. 
%
%
Hereafter, we say that $\k$ generates a SPSD matrix $\K$ if $\k(\xset,\xset) = \K$. 

\begin{lemma}[\tbf{Functional sub-Gaussianity implies vector sub-Gaussianity}]
\label{lem:funct_subg_vector_subg}
In the notation of \cref{def:alg-subg}, if $(\Pin - \Qout)\kernel$ is $(\kernel,\subg)$-sub-Gaussian on an event $\event$ and $\kernel$ generates $\K$, then the vector $\pin - \qout$ is $(\K,\subg)$-sub-Gaussian on $\event$.
%
\end{lemma}
%

%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
\begin{proof}
%
Suppose $(\Pin - \Qout)\kernel$ is $(\kernel,\subg)$-sub-Gaussian on an event $\event$, fix a vector $\bu\in \reals^n$, and define the function
\begin{talign}
    f_{\bu} \defeq \sumn u_i \kernel(\cdot, x_i) \in \rkhs.
\end{talign}
By the reproducing property, 
\begin{talign}\label{eq:hnorm-of-fu}
    \bu^\top \K (\pin -\qout) = \inner{f_{\bu}}{(\Pin-\Qout)\kernel}_{\kernel} \qtext{and} \knorm{f_{\bu}}^2 = \bu^\top \K \bu.
\end{talign}
Invoking the representations \cref{eq:hnorm-of-fu} and the functional sub-Gaussianity condition \cref{eq:function-subg} we therefore obtain
\begin{talign}
    \Esubarg{\event}{\exp(\bu^\top \K(\pin-\qout)} &= \Esubarg{\event}{\exp(\inner{f_{\bu}}{(\Pin-\Qout)\kernel}_{\kernel})} 
    \leq \exp(\knorm{f_{\bu}}^2 \cdot \frac{\subg^2}{2}) 
    = \exp(\bu^\top \K \bu \cdot \frac{\subg^2}{2}),
\end{talign}
so that $\pin-\qout$ is $(\K,\subg)$-sub-Gaussian on the event $\event$ as claimed.
\end{proof}

%

\begin{lemma}[\tbf{Vector sub-Gaussianity implies functional sub-Gaussianity}]
\label{lem:vector_subg_funct_subg}
In the notation of \cref{def:alg-subg}, if $\pin - \qout$ is $(\K,\subg)$-sub-Gaussian on an event $\event$ and $\kernel$ generates $\K$, then $(\Pin - \Qout)\kernel$ is $(\kernel,\subg)$-sub-Gaussian on $\event$.
\end{lemma}
\begin{proof}
Suppose $\pin-\qout$ is $(\K,\subg)$-sub-Gaussian on an event $\event$, fix a function $f\in \rkhs$, and consider the set 
\begin{talign}
\Lset \defeq \braces{f_{\bu} \defeq \sum_{i=1}^n u_i \kernel(\cdot,x_i) : \bu \in \reals^n}.
\end{talign} 
Since $\Lset$ is a closed linear subspace of $\rkhs$, we can decompose $f$ as $f  = f_{\bu} + f_\perp$,
where $\bu\in\Rn$ and $f_\perp$ is orthogonal to $\Lset$ \citep[Theorem 12.4]{rudin1991functional},
%
so that 
\begin{talign}\label{eq:knorm-decomposition}
    \knorm{f}^2 = \knorm{f_{\bu}}^2 + \knorm{f_\perp}^2\qtext{and} \knorm{f_{\bu}}^2 = \bu^\top \K \bu.
\end{talign}
Invoking the orthogonality of $f_\perp$ and $(\Pin - \Qout)\kernel\in \Lset$, the reproducing property representations \cref{eq:hnorm-of-fu}, and the vector sub-Gaussianity condition \cref{eq:vector-subg}, we find that
\begin{talign}
    \Esubarg{\event}{\exp(\inner{f}{(\Pin-\Qout)\kernel}_{\kernel})} 
    &= \Esubarg{\event}{\exp(\inner{f_{\bu} + f_\perp}{(\Pin - \Qout) \kernel}_{\kernel})} 
    = \Esubarg{\event}{\exp(\bu^\top \K (\pin - \qout)})\\
    &\leq \exp(\bu^\top \K \bu \cdot \frac{\subg^2}{2}) 
    \sless{\cref{{eq:knorm-decomposition}}} \exp(\knorm{f}^2 \cdot \frac{\subg^2}{2}),
\end{talign}
so that $(\Pin-\Qout)\kernel$ is $(\kernel,\subg)$-sub-Gaussian on the event $\event$ as claimed.
\end{proof}


We end our discussion about the versions of sub-Gaussianity considered above by presenting the standard fact about the additivity of sub-Gaussianity parameters under summation of independent sub-Gaussian random vectors, adapted to our setting.

\begin{lemma}[\tbf{Vector sub-Gaussian additivity}]\label{lem:K_sub_gsn_additivity}
    Suppose that, for each $j\in [m]$, 
    $\Delta_j\in\reals^n$ is $(\mbf K,\subg_j)$ on an event $\event[j]$ given $\Delta_{1:(j-1)}\defeq (\Delta_1,\ldots,\Delta_{j-1})$ and $\event[\leq j-1]\defeq \bigcap_{i=1}^{j-1}\event[i]$. 
    Then $\sum_{j=1}^m \Delta_j$ is $(\mbf K, (\sum_{j=1}^m \subg_j^2)^{1/2})$-sub-Gaussian on $\event[\leq m]$.
    %
    \end{lemma}
    \begin{proof}
    Let $\event[\leq s] = \bigcap_{j=1}^s\event[j]$ for each $s\in [m]$.
    We prove the result for $\mc Z_s = \sum_{i=1}^s \Delta_j$ by induction on $s\in [m]$. 
    The result holds for the base case of $s=1$ by assumption. For the inductive case, suppose the result holds for $s\in [m-1]$. Fixing $\bu\in \R^n$, we may apply the tower property, our conditional sub-Gaussianity assumption, and our inductive hypothesis in turn to conclude
    \begin{talign}
        \Earg{\exp(\inner{\bu}{\K \sum_{j=1}^{s+1} \Delta_j})\indic{\event[\leq s+1]}} &= \Earg{\exp(\inner{\bu}{\K \sum_{j=1}^{s} \Delta_j})\indic{\event[\leq s]} \Earg{\exp(\inner{\bu}{\Delta_{s+1}})\indic{\event[s+1]} \mid \Delta_{1:s},\event[\leq s]} } \\
        &\leq \Earg{\exp(\inner{\bu}{\K \sum_{j=1}^{s} \Delta_j})\indic{\event[\leq s]}} \exp\parenth{\frac{\subg_{s+1}^2}{2}\cdot \bu^\top \K \bu}
        \leq \exp\big( \frac{\sum_{j=1}^{s+1} \subg_j^2}{2} \cdot \bu^\top \K \bu\big).
    \end{talign}
    Hence, $\mc Z_{s+1}$ is $(\K,(\sum_{j=1}^{s+1} \subg_j^2)^{1/2})$-sub-Gaussian on $\event[\leq s+1]$, and the proof is complete.
    \end{proof}



%



%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


