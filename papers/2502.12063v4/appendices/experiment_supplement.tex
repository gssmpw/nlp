\section{Supplementary Experiment Details}
\label{sec:experiment_supplement}

\subsection{Approximating attention experiment}
\label{app:attention_details}

The experiment of \cref{sec:att-experiment} was carried out using 
Python 3.12.9,  
PyTorch 2.8.0.dev20250407+cu128  
\citep{paszke2019pytorch}, and an Ubuntu 22.04.5 LTS server with an AMD EPYC 7V13 64-Core Processor, 220 GB RAM, and a single NVIDIA A100 GPU (80 GB memory, CUDA 12.8, driver version 570.124.04). 
%
For reference, attention layer 1 has $(n,d)=(3136,64)$ and attention layer 2 has $(n,d)=(784,64)$.
For each layer and each of the first $50$ ImageNet 2012 validation set batches of size $64$, we measured the time required to complete a forward pass through the layer using CUDA events following $10$ warm-up batches to initialize the GPU.
\cref{tab:imagenet-configs} provides the hyperparameter settings for each attention approximation in \cref{tab:imagenet-acc-time}. 
The settings and implementations for all methods other than Thinformer were provided by \citet{zandieh2023kdeformer}, and our experiment code builds on their open-source repository \url{https://github.com/majid-daliri/kdeformer}. 
%

\begin{table}[h!]
    \centering
    \caption{\tbf{Configurations for the attention approximation methods of \cref{tab:imagenet-acc-time}.}}
    \begin{tabular}{ccc}
        \toprule
        {\bf Attention Algorithm} & {\bf Layer 1 Configuration} & {\bf Layer 2 Configuration}
        \\\midrule
        \multirow{1}{*}{\bf Performer} & \verb|num_features=49| & \verb|num_features=12| 
        \\[1mm]
        \multirow{2}{*}{\bf Reformer} & \verb|bucket_size=49| & \verb|bucket_size=12| \\
        & \verb|n_hashes=2| & \verb|n_hashes=2|
        \\[1mm]
        \multirow{2}{*}{\bf ScatterBrain} & \verb|local_context=49| & \verb|local_context=12| \\
        & \verb|num_features=48| & \verb|num_features=6|
        \\[1mm]
        \multirow{2}{*}{\bf KDEformer} & \verb|sample_size=64| & \verb|sample_size=56| \\
        & \verb|bucket_size=32| & \verb|bucket_size=32|
        \\[1mm]
        \multirow{1}{*}{\bf Thinformer (Ours)} & \verb|g=2| & \verb|g=4|
        \\
        \bottomrule
    \end{tabular}
    \label{tab:imagenet-configs}
\end{table}

%
\subsection{Faster SGD training experiment}
\label{app:sgd_details}
The experiment of \cref{sec:theory-practice-gap} was carried out using Python 3.10, PyTorch 2.0.1, a Rocky Linux 8.9 server with 64 CPU cores (Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60GHz), and a NVIDIA A100 GPU (40 GB memory, CUDA 12.4, driver version 550.54.15). 

Technically, the CD-GraB: SBW algorithm requires an a priori upper bound on the maximum Euclidean norm $b_{\max}$ of any stochastic gradient that it will encounter.  
To conduct our experiment, we first estimate $b_{\max}$ by calculating the maximum gradient Euclidean encountered across $10$ epochs of running SGD with $\khsgd$ reordering.
One would typically not choose to carry out such a two-step procedure in practice, but the experiment serves to demonstrate that the CD-GraB: SBW leads to overly conservative performance even if reasonable upper bound is known in advance.

 
The settings and implementation for both random reshuffling (RR) and CD-GraB: Greedy were those used in the original logistic regression on mortgage application experiment of  \citet{cooper2023coordinatingdistributedexampleorders}.
Our experiment code builds on the open-source CD-GraB repository \url{https://github.com/GarlGuo/CD-GraB}.
As in \citet{cooper2023coordinatingdistributedexampleorders}, optimization was carried out with a learning rate of $\alpha=0.01$, datapoints were loaded in batches of size $16$, and stochastic gradients were reordered for each datapoint individually.

%
\subsection{Cheap two-sample testing experiment}
\label{app:testing_details}

The experiment of \cref{sub:ctt_experiments} was carried out using Python 3.10.15, PyTorch 2.5.0, and a Rocky Linux 8.9 server with an AMD EPYC 9454 48-Core Processor, 100 GB RAM, and a single NVIDIA H100 GPU (80 GB memory, CUDA 12.5, driver version 555.42.02). Each test is run with replication count $\numperm=100$, nominal level $\alpha=0.05$, and failure probability $\delta=0.5$.
The neural network $\phi$ was trained exactly as in \citet{liu2020learning} (with learning rate $5\times10^{-5}$ and batch size equal to the full training sample size), and
runtime measurements exclude the time required to train $\phi$.
Our experiment code builds on the open-source deep kernel testing 
(\url{https://github.com/fengliu90/DK-for-TST}) and Compress Then Test
(\url{https://github.com/microsoft/goodpoints}) repositories.