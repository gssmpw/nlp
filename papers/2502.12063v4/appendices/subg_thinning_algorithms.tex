%

\section{Proof of \cref{tab:subg_thinning_algorithms}: Sub-Gaussian Thinning Examples}\label{app:subg_thinning_algorithms}
\newcommand{\ininfnorm}[1]{\norm{#1}_{\infty,\mrm{in}}}

This section provides supplementary details for each of the sub-Gaussian thinning algorithms of \cref{tab:subg_thinning_algorithms}.
%
\subsection{\textsc{Subsampling}}
\label{sub:subsampling}
\subsubsection{\pcref{prop:uniform-subsampling}}
\label{proof:uniform-subsampling}
We begin by computing the first and second moments of $\qout$:
$\E[\qout] = \pin$ and
\begin{talign}
\E[\qout\qout^\top] 
    =
\frac{1}{\nout}\diag(\pin) + \frac{\nin(\nout-1)}{\nout(\nin-1)}(\pin\pin^\top - \frac{1}{\nin}\diag(\pin))
    =
\frac{1}{\nout}(\frac{\nin-\nout}{\nin-1})\diag(\pin)
    +
\frac{\nin(\nout-1)}{\nout(\nin-1)} \pin\pin^\top.
\end{talign}
Hence,
\begin{talign}
\E[\mmd^2_{\K}(\pin, \qout)]
    &= 
\pin\tp\K\pin
    - 
2 \pin\tp \K \E[\qout]
    + 
\E[\qout\tp\K\qout]
    =
\tr(\K\E[\qout\qout^\top])
    - 
\pin\tp \K \pin \\
    &=
\frac{1}{\nout}(\frac{\nin-\nout}{\nin-1})(\tr(\K \diag(\pin)) -\pin\tp\K\pin)
    =
\frac{1}{\nout}(\frac{\nin-\nout}{\nin-1}) C_{\K}.
\label{eq:uniform-mmd}
\end{talign}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

To derive the second advertised result, we note that
\begin{talign}
\E[\indnorm^2]
    &\geq
\max_{i\in\ind}
\E[(\e_i^\top\K(\pin-\qout))^2]
    =
\max_{i\in\ind} \E[\mmd^2_{\K\e_i\e_i^\top\K}(\pin,\qout)]
\end{talign}
and invoke the initial result \cref{eq:uniform-mmd} to conclude.
%
%
%
%
%
%
%
%
%
%
%
%
%
%


\subsubsection{Sub-Gaussianity of subsampling}

\begin{proposition}[Sub-Gaussianity of uniform subsampling]\label{prop:uniformsubg} 
For any SPSD $\K \in \reals^{n\times n}$, uniform subsampling (without replacement) is a $(\K,\subg,0)$-sub-Gaussian thinning algorithm with 
\begin{talign}\label{eq:uniformsubg}
    \subg \defeq \frac{\sqrt{\Kmax}}{\sqrt{\nout}}. %
\end{talign}
\end{proposition}

\begin{proof}
\newcommand{\xstar}{x^\star}
%
Fix any vector $\bu\in\reals^n$, and  
let $J_1, \dots, J_{\nout}$ 
%
be the random indices in $[n]$ selected by uniform subsampling.
Since $\bu\tp\K(\pin - \qout) = \frac{1}{\nout}\sum_{i=1}^{\nout} \bu\tp\K(\pin - \e_{J_i})$ is an average of mean-centered scalars drawn without replacement and satisfying
\begin{talign}
|\bu\tp\K\e_{J_i}|
\leq \sqrt{\bu\tp\K\bu} \sqrt{\e_{J_i}\tp\K\e_{J_i}}
\leq \sqrt{\Kmax} \sqrt{\bu\tp\K\bu}
\qtext{with probability $1$}
\end{talign}
by Cauchy-Schwarz, Thm.~4 and equations (1.8) and (4.16) of \citet{hoeffding1994probability} imply that 
\begin{talign}
\E[\exp(\bu\tp\K(\pin - \qout))]
    \leq
\exp(\frac{\Kmax}{2\nout} \bu\tp\K\bu).
\end{talign}
\end{proof}

%
%
%
%
%
%
%
%
%
%
%
%
%


%


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
\input{appendices/rkhd}

\input{appendices/khcompressd}

%
\subsection{\gsthin}
\label{sub:gs_thin}
The section introduces and analyzes the Gram-Schmidt Thinning algorithm (\gsthin, \cref{algo:gs_thin}). 
%
%
%
%
%
%
%
\gsthin repeatedly divides an input sequence in half using, \gshalve (\cref{algo:gs_halve}), a symmetrized and kernelized version of the Gram-Schmidt (GS) Walk of \citet{bansal2018gram}.
%
%
We will present two different implementations of \gshalve: a quartic-time implementation (\cref{algo:gs_halve}) based on the GS Walk description of \citet{bansal2018gram} and a cubic-time implementation based on local updates to the matrix inverse (\cref{algo:gs_halve_cubic}).
While both the algorithms lead to the same output given the same source of randomness, we present the original implementation\footnote{\label{footnote:alg_equiv} Towards making this equivalence clear, \cref{algo:gs_halve} has been expressed with the same variables that \cref{algo:gs_halve_cubic} uses. \cref{algo:gs_halve} can be slightly simplified if it were to be considered independently.} for conceptual clarity and the optimized implementation for improved runtime.
Throughout, for a matrix $\mbf Q$ and vector $\bu$, we use the notation $\mbf Q_{\ind\times\mc{J}}$ 
and $\bu_{\ind}$
to represent the submatrix $(\mbf Q_{ij})_{i\in\ind,j\in\mc{J}}$ and subvector $(\bu_{i})_{i\in\ind}$.


%
%
%
%
%
%
%




\begin{algorithm2e}[]
\caption{\gsthin: Gram-Schmidt Thinning}%
\label{algo:gs_thin}
\small{
    \KwIn{point sequence  $\xin=(\x_i)_{i = 1}^{\nin}$, kernel $\kernel$, output size $\nout \in \nin / 2^\naturals$, $\halve\in \braces{\gshalve, \gshalvecubic}$}
  \BlankLine
  // Repeatedly divide coreset size in half \\
  $m \gets \log_2(\nin/\nout)$ \\ 
  \lFor{$\ell=1, 2, \ldots, m$}
    {%
    $\xin \gets \halve(\xin, \kernel)$
  }
  \KwRet{\textup{$\xout\defeq\xin$, coreset of size} $\nout=\nin/2^m$}{} 
  } 
\end{algorithm2e}

\SetKwFunction{proctwo}{\texttt{kernel\_gs\_walk}}

\begin{algorithm2e}[]
\caption{\gshalve: Gram-Schmidt Halving}%
\label{algo:gs_halve}
\small{
    \KwIn{point sequence $\xin = \parenth{x_i}_{i=1}^{\nin}$ with even $\nin$, kernel $\kernel$} 
    \BlankLine
    %
    %
    %
    
    $\xout\gets \braces{}$ %
    \quad // Initialize empty coreset \\[1mm]
    // Select one point to keep from each consecutive pair using  kernelized GS Walk  \\
    $\bz \gets $ \proctwo{$\xin$} \\
    \For{$i=1,\dots,\nin/2$}
    {
        \eIf{$\bz_i = 1$}
        {
            $\xout.\texttt{append}(x_{2i-1})$
        }
        {
            $\xout.\texttt{append}(x_{2i})$
        }
    }
    \KwRet{$\xout$\textup{, coreset of size $\nin/2$}}
    
    \hrulefill\\
    \SetKwProg{myproc}{function}{}{}
     \myproc{\proctwo{$(\x_i)_{i=1}^{\nin}$}:}{
        $t \gets 1$; \quad $\bz_t \gets (0,0,\ldots,0) \in \R^{\nin/2} $ \quad\quad\quad\quad// Initialize fractional assignment vector \\
            $\mc A \gets [\nin/2] $  \quad\quad\quad\quad // Initialize  set of active coordinates \\
    $p \sim \mc A $ \quad\quad\quad\quad // Select a pivot uniformly at random \\

    %
    \While{$\bz_t \notin \braces{\pm 1}^{\nin/2}$}
    {   
        $\mc A'  \gets \mc A  \,\backslash\, \big\{ \min \big( \braces{i\in [\nin/2]: \abss{\bz_{ti}} = 1} \,\backslash\, ( [\nin/2] \,\backslash\, \mc  A ) \big)  \big\}  $ \\   // Update set of active coordinates by removing smallest index set to $\pm 1 $\\
        \eIf{$p\notin \mc A'$}
        {
            $p' \sim \Unif(\mc A')$ \quad // Select a new pivot from $\mc A'$ uniformly at random
        }{$p' \gets p$}
        %
        %
        %
        %
        %
        // Compute step direction in which to update fractional assignment vector \\
        $\bu_{t} \gets \argmin_{\bu \in \R^{\nin/2}} \bu^\top \mbf Q \bu$ subject to $\bu_{p'}=1$ and $\bu_i=0$ for all $i\notin \mc A'$, \\ 
        \quad where $\mbf Q\in \R^{(\nin/2)\times (\nin/2)}$ has entries $\mbf Q_{ij} \defeq \kernel(x_{2i-1},x_{2j-1}) + \kernel(x_{2i},x_{2j} ) - \kernel(x_{2i-1},x_{2j}) - \kernel(x_{2i},x_{2j-1}) $ \\ 
        %
        $\delta^{+} \gets \abss{\max \Delta}$ and $\delta^- \gets \abss{\min \Delta}$, where $\Delta = \braces{\delta\in \R: \bz_{t} + \delta\bu_{t} \in [-1,+1]^{\nin/2}}$ \qquad// Select candidate step sizes\\
        $\delta_t \gets \delta^+$ with probability $\delta^-/(\delta^+ + \delta^-)$; otherwise $\delta_t \gets -\delta^-$ \qquad// Choose step size and sign at random \\
        $\bz_{t+1} \gets \bz_{t} + \delta_t \bu_{t}$ \qquad // Update fractional assignments \\
        $t\gets t+1$; \quad
        $\mc A \gets \mc A'$;\quad
        $ p \gets p'$  \\  
    }
     }
     \KwRet{$\bz_t$\textup{, sign vector in $\{\pm1\}^{\nin/2}$}}
} 
\end{algorithm2e}

\SetKwFunction{proccubic}{\texttt{kernel\_gs\_walk\_cubic}}

\begin{algorithm2e}[]
\caption{\gshalvecubic: Gram-Schmidt Halving with cubic runtime}
\label{algo:gs_halve_cubic}
\small{
    \KwIn{point sequence $\xin = \parenth{x_i}_{i=1}^{\nin}$ with even $\nin$, kernel $\kernel$ with positive definite  $\k(\xin,\xin)$} 
    \BlankLine
    $\xout\gets \braces{}$ %
    \quad // Initialize empty coreset \\[1mm]
    // Select one point to keep from each consecutive pair using  kernelized GS Walk  \\
    $\bz \gets $ \proccubic{$\xin$} \\
    \For{$i=1,\dots,\nin/2$}
    {
        \eIf{$\bz_i = 1$}
        {
            $\xout.\texttt{append}(x_{2i-1})$
        }
        {
            $\xout.\texttt{append}(x_{2i})$
        }
    }
    \KwRet{$\xout$\textup{, coreset of size $\nin/2$}}

    \hrulefill\\
    \SetKwProg{myproc}{function}{}{}
     \myproc{\proccubic{$\parenth{x_i}_{i=1}^{\nin}$}:}{
        $t \gets 1$; \quad $\bz_t \gets (0,0,\ldots,0) \in \R^{\nin/2} $ \quad\quad\quad\quad// Initialize fractional assignment vector \\
       $\mc A \gets [\nin/2] $  \quad\quad\quad\quad // Initialize  set of active coordinates \\
    $p \sim \mc A $ \quad\quad\quad\quad // Select pivot uniformly at random \\
 
    %
    $\mbf Q \gets (\kernel(x_{2i-1},x_{2j-1}) + \kernel(x_{2i},x_{2j} ) - \kernel(x_{2i-1},x_{2j}) - \kernel(x_{2i},x_{2j-1}))_{i,j=1}^{\nin/2}$ %
    \quad // Form paired difference kernel matrix\\
    $  \mbf C \gets (\mbf Q_{ \mc A \backslash \{p\} \times \mc A \backslash \{p\}   } )^{-1}$ \\%\quad\quad\quad\quad // Requires invertible $\mbf Q$  \\   
    %
    \While{$\bz_t \notin \braces{\pm 1}^{\nin/2}$}
    {
        $\mc A'  \gets \mc A  \,\backslash\, \big\{ \min \big( \braces{i\in [\nin/2]: \abss{\bz_{ti}} = 1} \,\backslash\, ( [\nin/2] \,\backslash\, \mc  A ) \big)  \big\}  $ \\   // Update set of active coordinates by removing smallest index set to $\pm 1 $\\
        \eIf{$p\notin \mc A'$}
        {
            $p' \sim \Unif(\mc A')$ \quad // Select a new pivot from $\mc A'$ uniformly at random
        }{$p' \gets p$}
         $\mc A_1 \gets \mc A \,\backslash\, \{p\}  $ \\ $\mc A_2 \gets \mc A' \,\backslash\, \{ p' \} $. \\
        $i \gets \mc A_1 \backslash \mc A_2  $  // Choose $i$ as the (unique) index that was removed from the active coordinates \\[1mm] 
         // Compute $(\mbf{Q}_{\mc A_2   \times \mc A_2})^{-1} $ using block matrix inversion and the Sherman-Morrison formula  \\[1mm]   
        $\mbf D \gets \mbf C_{ \mc A_2 \times A_2  } $ \\[1mm] %
        $ \mbf C \gets \mbf D - \frac{\mbf D \mbf Q_{ \mc A_2 \times \{ i\} } \mbf Q_{\{i\} \times \mc A_2 } \mbf D}{ \mbf Q_{ ii } + \mbf Q_{\{i\} \times \mc A_2 } \mbf D \mbf Q_{ \mc A_2 \times \{ i\} }}$  \quad\quad\quad\quad \\[1mm] %
        // Compute step direction in which to update fractional assignment vector \\
       Compute $\bu_t $ as $  (\bu_t)_{\mc A_2 }  = - \mbf C  \mbf Q_{ \mc A_2 \times \{p'\}  }  $ , $\bu_{tp'} = 1$, and $\bu_{ti} = 0 $ for $i \notin \mc A'$    \\[1mm] 
        %
        %

        $\delta^{+} \gets \abss{\max \Delta}$ and $\delta^- \gets \abss{\min \Delta}$, where $\Delta = \braces{\delta\in \R: \bz_{t} + \delta\bu_{t} \in [-1,+1]^{\nin/2}}$ \quad// Select candidate step sizes \\ 
        $\delta_t \gets \delta^+$ with probability $\delta^-/(\delta^+ + \delta^-)$; otherwise $\delta_t \gets -\delta^-$ \quad// Choose step size and sign at random \\
        $\bz_{t+1} \gets \bz_{t} + \delta_t \bu_{t}$ \quad // Update fractional assignments \\
        $t\gets t+1$; \quad
        $\mc A \gets \mc A'$; \quad  
        $ p \gets p'$  \\  
    }
     }
       \KwRet{$\bz_t$\textup{, sign vector in $\{\pm1\}^{\nin/2}$}}
} 
\end{algorithm2e}




%
Our first result, proved in \cref{proof:gs_thin}, shows that \gsthin is a sub-Gaussian thinning algorithm.
%
\begin{proposition}[\tbf{\gsthin sub-Gaussianity}]\label{prop:gs_thin}
For $\K$ generated by $\k$, 
\gsthin (\cref{algo:gs_thin}) is a $(\mkernel,\subg,0)$-sub-Gaussian thinning algorithm with parameter
%
\begin{talign}\label{eq:gs_thin_parameter}
    \subg \defeq \frac{2}{\sqrt 3} \frac{\sqrt{\maxnorm{\K}}}{\nout}.
\end{talign}
\end{proposition}
%
Our second result, proved in \cref{proof:quartic-GS}, shows that \gsthin with the \gshalve implementation has $O(\nin^4)$ runtime. 
%

\begin{proposition}[\tbf{Runtime of \gsthin with \gshalve}]
\label{prop:gs_thin_runtime_1}
The runtime of \gsthin with implementation \gshalve (\cref{algo:gs_halve}) is $\bigO{\nin^4}$. 
%
\end{proposition}
%

Our third result, proved in \cref{proof:gs_halve_agreement}, establishes the equivalence between \gshalve and \gshalvecubic.
More precisely, we show that the sequence of partial assignment vectors generated by \proctwo{$\cdot$} of \cref{algo:gs_halve} and \proccubic{$\cdot$} of \cref{algo:gs_halve_cubic} are identical given identical inputs, an invertible induced kernel matrix, and an identical source of randomness.
%

    \begin{proposition}[\textbf{Agreement of \gshalve and \gshalvecubic}]\label{lem:gs_halve_agreement}
        %
        Let $\bz_1,\bz_2,\dots$ be the fractional assignment sequence generated by \proctwo{$(\x_i)_{i=1}^{\nin}$} in \cref{algo:gs_halve} and $\bz_1',\bz_2',\dots$ be the fractional assignment sequence generated by \proccubic{$(\x_i)_{i=1}^{\nin}$} in \cref{algo:gs_halve_cubic} with an identical source of randomness. 
        If the pairwise difference matrix
        \begin{talign}
        \mbf Q \defeq (\kernel(x_{2i-1},x_{2j-1}) + \kernel(x_{2i},x_{2j}) - \kernel(x_{2i-1},x_{2j}) - \kernel(x_{2i},x_{2j-1}))_{i,j\in [\nin/2]} 
    \end{talign}
    is positive definite, then $\bz_t = \bz_t'$ for all $t$.
    \end{proposition}

%
Our fourth result, proved in \cref{proof:cubic-GS}, shows that \gsthin with the \gshalvecubic implementation has $O(\nin^3)$ runtime.

%
\begin{proposition}[\tbf{Runtime of \gsthin with \gshalvecubic}] \label{prop:gs_thin_runtime_2}
    The runtime of \gsthin with implementation \gshalvecubic (\cref{algo:gs_halve_cubic}) is $\bigO{\nin^3}$.
\end{proposition}
%



\subsubsection{\pcref{prop:gs_thin}}
\label{proof:gs_thin}

Our first lemma bounds the sub-Gaussian constant of \gshalve (\cref{algo:gs_halve}).

%
\begin{lemma}[\tbf{\gshalve sub-Gaussianity}]\label{lem:gs_halve}
In the notation of \cref{def:thinning_algo}, consider the input and output vectors $\pin,\qout\in\reals^n$ of \gshalve (\cref{algo:gs_halve}) for $\xset\supseteq \xin$ with $|\xset| = n \geq \nin$.
If $\K = \k(\xset,\xset)$, 
%
then $\pin - \qout$ is $(\mbf K, \subg)$-sub-Gaussian with
\begin{talign}\label{eq:gs_halve_nu}
    \subg \defeq \frac{2\maxnorm{\K}^{1/2}}{\nin} = \frac{\maxnorm{\K}^{1/2}}{\nout}.
\end{talign}
\end{lemma}
\begin{proof}
Since $\K$ is SPSD, there exists a matrix $\bPhi\in\reals^{n\times d}$ such that $\K = \bPhi \bPhi^\top$. Let $\mbf B\in \R^{d \times (\nin/2)}$ be the matrix with entries
\begin{talign}
    \mbf B_{j,i} \defeq \bPhi_{2i-1,j} - \bPhi_{2i,j} \qtext{for}~i\in[\nin/2]
    \qtext{and} j \in [d].
\end{talign}
%
%
Note that, for each $i\in[\nin/2]$, 
%
\begin{talign} \label{eq:Bi_two_norm}
    \sum_{j\in[d]}\mbf B_{j,i}^2 = \mbf{K}_{2i-1,2i-1} + \mbf{K}_{2i,2i} - \mbf{K}_{2i-1,2i} - \mbf{K}_{2i,2i-1} \leq 4\maxnorm{\mbf K}.
\end{talign}
Hence, by \citet[Thm.~6.6]{harshaw2024balancing}, $\frac{1}{\nin}\mbf B \z$ is $(\ident,\subg)$-sub-Gaussian where $\ident$ is the identity matrix in $\reals^{d\times d}$.
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
Now fix any $\bu \in \Rd$. 
Since $\frac{1}{\nin}\mbf B \z = -\bPhi^\top(\pin-\qout)$ by construction,  %
\begin{talign}\label{eq:gs_halve-1}
\Earg{ \exp\parenth{\bu^\top\K(\pin-\qout) }}
    \leq
\Earg{ \exp\parenth{-\inner{\bPhi^\top \bu}{\frac{1}{\nin}\mbf B \z}}} 
    \leq 
\exp\parenth{\frac{ \subg^2}{2} \cdot \twonorm{\bPhi^\top \bu}^2}  
    = 
\exp\parenth{\frac{\subg^2}{2} \cdot \bu^\top \mbf K \bu}.
\end{talign}
%
%
%
%
%
\end{proof}

%
%
%
%
%
Now, for $\ell\in[m]$, let $\mbi p_\ell \in \R^n$ denote the output probability vector produced by the $\ell$-th call to \gshalve. %
Defining $\mbi p_0 \defeq \pin$ and $\qout \defeq \mbi p_m$, we have
\begin{talign}
    \pin - \qout = \sum_{i=1}^m \Delta_i,\qtext{for}~ \Delta_i\defeq \mbi p_{i-1} - \mbi p_{i}
    \qtext{for} i\in[m].
\end{talign}
%
%

By \cref{lem:gs_halve}, each $\mbi p_{i-1} - \mbi p_{i}$ is $(\mbf K,\frac{2\maxnorm{\mbf K}^{1/2} }{\nin/2^{i-1}})$-sub-Gaussian conditional on $(\Delta_1, \ldots,\Delta_{i-1})$. 
Applying \cref{lem:K_sub_gsn_additivity} to the sequence $\parenth{\Delta_j}_{j=1}^m$, we find that $\pin - \qout$ is $(\K,\subg)$-sub-Gaussian with parameter
\begin{talign}
    \subg = \parenth{\sum_{j=1}^m \frac{4\maxnorm{\K}}{(\nin/2^{j-1})^2} }^{1/2} = \frac{2\maxnorm{\K}^{1/2}}{\nin} \parenth{\sum_{j=1}^m 4^j}^{1/2} \leq \frac{\maxnorm{\K}^{1/2}}{\nin} \sqrt{\frac{4}{3} 4^m}. 
    %
\end{talign}
Simplifying the above using the fact that $\nout = \nin/2^m$ yields our desired result \cref{eq:gs_thin_parameter}.



\subsubsection{\pcref{prop:gs_thin_runtime_1}} \label{proof:quartic-GS}

    We essentially reproduce the argument from \citet{bansal2018gram} for the runtime of the \gshalve algorithm in our kernelized context.

    The main computational cost of \gshalve is the execution of the \proctwo{$\cdot$} subroutine in \cref{algo:gs_halve}.
    The number of iterations in while loop for $\bz_t$ is at most $\nin/2$. 
    This is due to the fact that in each iteration, at least one new variable is set to $ \left\{ \pm 1 \right\} $.
    Further, in each iteration, the main computational cost is the computation of 
    \begin{talign}
        \bu_{t} \gets \argmin_{\bu \in \R^{\nin/2}} \bu^\top \mbf Q \bu
    \end{talign}
    under the constraints  that $\bu_p=1$ and $\bu_i=0$ for all $i\notin \mc A$. 
    Since this can be implemented in $\bigO{\nin^3}$ time  using standard convex optimization techniques, 
    %
    %
    %
    %
%
\gshalve has total runtime  
\begin{talign}\label{eq:gs_halve_runtime}
    r_{\mrm H}(\ell) \leq C\ell^4
\end{talign}
for an input sequence of size $\ell$ and a constant $C$ independent of $\ell$.
Now, note that \gsthin calls \gshalve iteratively on inputs of size $\nin 2^{-i}$ for $i=0,1,\ldots, m-1 $ where $m = \log_2(\nin/\nout)$. 
Thus, \gsthin has runtime
\begin{talign}
    \sum_{i=0}^{m-1} r_{\mrm{H}} (\nin/2^i) \leq \sum_{i=0}^{m-1} C(\nin/2^i)^4 = 
    %
    \bigO{\nin^4}.
\end{talign}

%
\subsubsection{\pcref{lem:gs_halve_agreement}}\label{proof:gs_halve_agreement}
        %
    %
    %
    
    %
    %
    %
    %
    We want to reason that any round of partial coloring leads to the same output across the two algorithms.
    Fix any fractional assignment update round. 
    %
    Recall that $\mc A_1 = \mc A \backslash \{p\}  $ and $\mc A_2 = \mc A' \,\backslash\, \{ p' \} $. 
    These represent the active set coordinates without the pivot before and after the update respectively.
     
    The main difference between \cref{algo:gs_halve_cubic,algo:gs_halve} is in the computation of the step direction $\bu_t$, which is the solution of the program
    %
    \begin{talign}
        \bu_{t} \gets \argmin_{\bu \in \R^n} \bu^\top \mbf Q \bu
        \qtext{subject to} \bu_{p'}=1 
        \qtext{and}
        \bu_i=0
        \qtext{for all}
        i\notin \mc A'.
    \end{talign}
    $\bu_t$ has a closed form with entries
    %
    \begin{talign}
        (\bu_{t} )_{ \mc A_2 } = - (\mbf{Q}_{\mc A_2 \times \mc A_2})^{-1} \cdot \mbf{Q}_{\mc A_2 \times  \{p'\}  }.
    \end{talign}
    %
    %
    Note that the invertibility of $\mbf{Q}_{\mc A_2 \times \mc A_2 }$ follows from the positive-definiteness of $\bQ$, as, for any $\w\in\R^{|\mc A_2|}$,
    \begin{align}
        \w^{\top}\mbf{Q}_{\mc A_2 \times \mc A_2 } \w = \tilde{\w}^{\top}\mbf{Q} \tilde{\w} > 0 
    \end{align}
    for a second vector $\tilde{\w}$ with $\tilde{\w}_{\mc A_2} = \w$ and all other entries equal to zero.
    %
    %
    %
    Therefore, to compute $\bu_t$, it suffices to keep track of the inverse of $\mbf Q_{\mc A_2 \times \mc A_2}$ as $ \mc A' $ across iterations.


    %
    %
 
    %
    %
    %
    Let $i$ be the unique element in $\mc A_1  \backslash \mc A_2  $. 
    Writing $\bQ_{\mc A_1 \times \mc A_1 }$ in block form, we have
    \begin{talign}
        \mbf Q_{\mc A_1 \times \mc A_1 } = \begin{bmatrix}
            \mbf Q_{\mc A_2  \times \mc A_2  } & \mbf Q_{\mc A_2 \times \{i\}   } \\
            \mbf Q_{ \{i\} \times \mc A_2  } & \mbf Q_{ ii }
        \end{bmatrix}.
    \end{talign}    
    By block matrix inversion  \citep[see, e.g.,][Thm. 2]{block_matrix}, the leading size $|\Aset_2|\times|\Aset_2|$ principal submatrix of $(\mbf Q_{\mc A_1 \times \mc A_1 })^{-1}$ equals
    \begin{talign} \label{eq:block_inv}
    \D \defeq
    \left(\mbf Q_{\mc A_2  \times \mc A_2  } - \frac{ \mbf Q_{ \mc A_2 \times \{ i\} } \mbf Q_{\{i\} \times \mc A_2 }    }{ \mbf Q_{ ii  }  }  \right)^{-1}.
        %
        %
        %
        %
    \end{talign}
    %
    %
    Thus, by the Sherman-Morrison formula \citep{sherman_morrison},   
    \begin{talign} 
        (\mbf Q_{ \mc A_2 \times A_2 })^{-1} = \left( \mbf D^{-1} + \frac{ \mbf Q_{ \mc A_2 \times \{ i\} } \mbf Q_{\{i\} \times \mc A_2 }    }{ \mbf Q_{ ii   }  }  \right)^{-1}
        =
        \mbf D - \frac{\mbf D \mbf Q_{ \mc A_2 \times \{ i\} } \mbf Q_{\{i\} \times \mc A_2 } \mbf D}{ \mbf Q_{ ii } + \mbf Q_{\{i\} \times \mc A_2 } \mbf D \mbf Q_{ \mc A_2 \times \{ i\} }}.
        \label{eq:block_inv_3}
    \end{talign}


    %
    %
    %
    %
    %
    %
    %
    %
    Hence, if we already have access to a matrix $\bC = (\mbf Q_{ \mc A_1 \times \mc A_1 })^{-1}$, we can compute $\mbf D$ by dropping the row and column of $\bC$ corresponding to $i$ and then compute $(\mbf Q_{ \mc A_2 \times \mc A_2 })^{-1}$ using \cref{eq:block_inv_3}.   
    Since in \cref{algo:gs_halve_cubic} we begin by  explicitly computing the inverse of $\mbf Q_{\mc A' \times \mc A'}$,
    the update step in \cref{algo:gs_halve_cubic} maintains the required inverse
    and thus its partial assignment updates match those of  \cref{algo:gs_halve}.
    %


%
\subsubsection{\pcref{prop:gs_thin_runtime_2} } \label{proof:cubic-GS}
We begin by establishing the runtime of \proccubic{$\cdot$}.

    \begin{lemma}[\textbf{Running time of \proccubic{$\cdot$} }]\label{lem:gs_halve_cubic}
        The routine \proccubic{$\cdot$} runs in $\bigO{ \ell^3}$ time given a point sequence of size $\ell$.
    \end{lemma}
    \begin{proof}
    First, the initialization of $\bC$ costs $O(\ell^3)$ time using standard matrix inversion algorithms. 
        Second, the number of iterations in the while loop is at most $\ell/2$ since, in each iteration, at least one new variable is assigned a permanent sign in $ \left\{ \pm 1 \right\} $.
        In each while loop iteration, the main computational costs are the update of $ \mbf C $ and the computation of the step direction $\bu_t$, both of which cost $O(\ell^2)$ time using standard matrix-vector multiplication.  
        %
        %
        Hence, together, all while loop iterations cost $O(\ell^3)$ time.  
        %
    \end{proof}



%
%
%

%
%
%
%
%


%
%
       
%


    Given the above lemma, we have that \gshalvecubic, on input of size $\ell$, has a running time 
\begin{talign}
    r_{\mrm{H}}(\ell) \leq C\ell^3
\end{talign}
for some $C$ independent of $\ell$.
When used in \gsthin this yields the runtime
\begin{talign}
    \sum_{i=0}^{m-1} r_{\mrm{H}} (\nin/2^i) = \sum_{i=0}^{m-1} C(\nin/2^i)^3 = 
    %
    \bigO{\nin^3}.
\end{talign}




    %
    %
    %
    %

   


    %
    %
    %
    %
    %
    %
    %
    %

%
\subsection{\gscompress} 
\label{sub:gs_compress}
This section introduces and analyzes the new \gscompress algorithm (\cref{algo:gs_compress}) which combines the \compress meta-algorithm of \citet{shetty2022distributioncompressionnearlineartime} with the \gshalvecubic halving algorithm (\cref{algo:gs_halve_cubic}). The following result bounds the sub-Gaussian constant and runtime of \gscompress.

%
\begin{algorithm2e}[t]
    \caption{\gscompress: Compress with \gshalvecubic halving}
    \label{algo:gs_compress}
    \SetAlgoLined
      \DontPrintSemicolon
\small{
  \KwIn{point sequence $\xin=(\x_i)_{i=1}^{\nin}$, kernel $\kernel$, $\nout\in\sqrt{\nin}\cdot 2^{\naturals}$}
\BlankLine
$\ossymb \gets \log_2(\nout/\sqrt{\nin})$ 
\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad// identify \osname \\
\BlankLine
\SetKwProg{myproc}{function}{}{}
\myproc{\proccompress{$\cset$}:}{
    \lIf{  $\abss{\cset} = 4^{\ossymb}$ }{
        \KwRet{ $\cset $ }{}
    }  
    %
        Partition $\cset$ into four arbitrary subsequences $ \{ \cset_i \}_{i=1}^4 $ each of size $\abss{\cset}/4$ \\ 
        \For{$i=1, 2, 3, 4$}
        {$ \wtil{ \cset_i } \leftarrow$ \proccompress{$\cset_i$} \ \qquad\qquad\qquad\qquad\qquad\qquad\quad\ \  // return coresets of size $2^{\ossymb} \cdot \sqrt{\frac{\abss{\cset}}{4}}$
        }
        %
        %
        $ \wtil{\cset} \gets\textsc{Concatenate}( \wtil{\cset}_1, \wtil{\cset}_2,\wtil{\cset}_3, \wtil{\cset}_4)$;
        \quad $\ell \gets 2 \cdot 2^{\ossymb} \cdot \sqrt{\abss{\cset}}$ 
        \quad // coreset of size   $\ell$ \\
        \KwRet{ \textup{  $\gshalvecubic(\wtil{\cset},  \kernel)$ 
        \qquad\qquad\qquad\qquad\qquad\quad \!// coreset of size $ 2^{\ossymb} \sqrt{\abss{\cset}}$} }       
}
\BlankLine
    \KwRet{ \proccompress{$\xin$} \qquad\qquad\qquad\ \qquad\qquad\qquad\qquad\qquad\!\textup{// coreset of size $\nout = 2^{\ossymb} \sqrt{\nin}$}}{}
}
\end{algorithm2e}
%

\begin{proposition}[\tbf{\gscompress sub-Gaussianity and runtime}]\label{prop:gs_thin_compress}
If $\K$ is generated by $\k$, then 
\gscompress is $(\K,\subg,0)$-sub-Gaussian with
\begin{talign}
    \subg \defeq \frac{1}{\nout} \sqrt{\log_2(\nout) \maxnorm{\K}}.
    %
\end{talign}
Moreover, \gscompress has an $\bigO{\nout^3}$ runtime.
\end{proposition}
%
\begin{proof}

By \cref{lem:gs_halve} and \cref{lem:gs_halve_agreement}, \gshalvecubic is $(\K,\subg_{\mrm H}(\ell))$-sub-Gaussian for an input point sequence of size $\ell$ and $\subg_{\mrm H}(\ell) = 2\sqrt{\maxnorm{K}}/\ell$.
Hence, by \cref{lem:vector_subg_funct_subg}, \gshalvecubic is also $\subg_{\mrm H}(\ell)$ $f$-sub-Gaussian in the sense of \citet[Def.~2]{shetty2022distributioncompressionnearlineartime} for each $f\in\rkhs$.  
By \citet[Rmk.~2]{shetty2022distributioncompressionnearlineartime}, \gscompress is therefore $f$-sub-Gaussian with parameter
\begin{talign}
    \subg &\leq \sqrt{\log_2(\nin/\nout)} \subg_{\mrm H}(2\nout) 
    \leq \sqrt{\log_2(\nout)} \frac{\maxnorm{\K}^{1/2}}{\nout} 
\end{talign}
for each $f\in\rkhs$.
Hence, \cref{lem:funct_subg_vector_subg} implies that \gscompress is a $(\K,\subg,0)$-sub-Gaussian thinning algorithm.

Furthermore, \citet[Thm.~1]{shetty2022distributioncompressionnearlineartime} implies that \gscompress has a runtime of 
\begin{talign}
    \sum_{i=0}^{\log_2( \nin / (2 \nout)) } 4^i \cdot r_{\mrm H}(2\nout 2^{-i}).
\end{talign}
where the \gshalvecubic runtime $r_{\mrm H}(\ell) \leq C\ell^3$ for $C$ independent of the input size $\ell$ by  \cref{lem:gs_halve_cubic}.
Therefore, the \gscompress runtime is bounded by 
\begin{talign}
    \sum_{i=0}^{\log_2( \nin / (2 \nout)) } 4^i \cdot (2\nout)^3 2^{-3i} = O(  \nout^3). 
\end{talign}
\end{proof}
%
\begin{remark}[\tbf{\compress with \gshalve}]
If the \gshalve implementation were used in place of \gshalvecubic, parallel reasoning would yield an  $\bigO{\nout^4}$ runtime for \gscompress.
\end{remark}

