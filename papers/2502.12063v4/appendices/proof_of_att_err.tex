\section{\pcref{att-err}}\label{proof:att-err}
Throughout we will make use of the convenient representation
\begin{talign}
\That
    =
\hDinv\ha\V
    \sstext{for}
\indout 
    &\defeq
\{ i \in [n] : (\augkey_i,\augval_i)\in\xout\},
    \sstext{}
\ha 
    \defeq 
\frac{n}{\nout}(\exp(\frac{\inner{\query_i}{\key_j}}{\sqrt{d}})\indic{j\in\indout})_{i,j=1}^n,
    \sstext{and}
\hD
    \defeq
\ha\boldone_n. 
\label{eq:ahat}
\end{talign}


Our proof makes use of three lemmas.
The first, proved in \cref{proof:att-err-decomposition}, bounds the approximation error for the attention matrix $\T$ in terms of the approximation error for $\A\V$ and $\A\boldone_n$.

%
\begin{lemma}[Decomposing attention approximation error]\label{att-err-decomposition}
In the notation of \cref{algo:thinformer,eq:ahat},
\begin{talign}
\maxnorm{\hdav-\dav}
    \leq
\min\big(\maxnorm{(\frac{1}{n}\D)^{-1}}, \maxnorm{(\frac{1}{n}\hD)^{-1}}\big)
(\frac{1}{n}\maxnorm{\hav - \av}
    +
\frac{1}{n}\infnorm{\A\boldone_n-\ha\boldone_n}
\maxnorm{\V}).
\end{talign}
\end{lemma}
%

The second, proved in \cref{proof:kms-bounds-att}, bounds the approximation error for $\A\V$ and $\A\boldone_n$ in terms of the KMS \cref{eq:kms} for a specific choice of attention kernel matrix.

%
\begin{lemma}[KMS bound on attention approximation error]\label{kms-bounds-att}
Instantiate the notation of \cref{algo:thinformer,eq:ahat} and define the query set
\begin{talign}
\xp 
    \defeq 
\{\x_{i+nj}\defeq (\augquery_i, \e_{j}^{d+1}) : i \in [n], j \in [d+1]\}
    \qtext{where}
\augquery_i 
    \defeq
\query_i/d^{\quarter}
\end{talign} 
and $\e_{j}^{d+1}$ is the $j$-th standard basis vector in $\reals^{d+1}$.
If $\Katt \defeq \katt(\xset,\xset)$ for $\xset \defeq \xp\cup\xin$, then
\begin{talign}
\max\big(\frac{1}{n} \maxnorm{(\hat{\mbf A}- \mbf A)\V},
\frac{1}{n} \infnorm{(\hat{\mbf A}- \mbf A)\boldone_n}\maxnorm{\V}\big)
    =
\attindnorm
    \qtext{for}
\ind \defeq [n(d+1)].
\end{talign}
\end{lemma}
%

Our third lemma, proved in \cref{proof:att-parameters}, bounds the size of key parameters of the thinned attention problem.
%
\begin{lemma}[Thinned attention problem parameters]\label{att-parameters}
Instantiate the notation of \cref{kms-bounds-att}, and define 
$R\defeq \max_{i\in[n]}\max(\twonorm{\query_i},\twonorm{\key_i})$.
Then, for all $i,j\in\ind$ and $l\in\supp{\pin}$, 
\begin{talign}
&\maxnorm{(\frac{1}{n}\D)^{-1}}
    \leq 
\exp(\frac{R^2}{\sqrt{d}}),
    \quad 
\max_{\x\in\xin}\sqrt{\katt(\x,\x)}
    \leq
\exp(\frac{R^2}{2\sqrt{d}})\sqrt{\rownorm{\V}^2+\maxnorm{\V}^2},
\\
&R_\ind 
    \defeq
\max_{i\in\ind}\twonorm{\x_i}
    \leq 
\sqrt{\frac{R^2}{\sqrt{d}}+1},
    \quad
D_{\ind} 
    \defeq
\max_{i\in\ind}\sqrt{\Katt[,ii]}
    \leq
\exp(\frac{R^2}{2\sqrt{d}}),
\\
&\rank{\X_\ind} 
    \leq
d+1
    \qtext{for}
\X_\ind
    \defeq
[\x_i]_{i\in\ind}^\top,
    \qtext{and}\\
&|\Katt[,il] -\Katt[,jl]| \leq L_{\Katt} \twonorm{\x_i-\x_j} 
    \qtext{for} 
L_{\Katt}
    \defeq
\exp(\frac{R^2}{\sqrt{d}})\sqrt{\frac{R^2}{\sqrt{d}} + 2}\maxnorm{\V}.
\end{talign}
\end{lemma}

Now instantiate the notation of \cref{kms-bounds-att}, and define the coefficient 
\begin{talign}
c
    \defeq
2\sqrt{2}\left(32\sqrt{\frac{2}{3}\,(d+1)\log(3e^2(\frac{R^2}{\sqrt{d}} + 2)\maxnorm{\V})}
    +
\sqrt{2\log(8)}(1+\frac{32}{\sqrt{3}})\right).
\end{talign}
Together,  
\cref{att-parameters}, the KMS quality bound of \cref{thm:subg_low_rank_gen_kernel}, and the \khcompresshalf sub-Gaussian constant $\subg$ of \cref{khcompressd-sub-gaussian} imply that, with probability at least $\half$, 
\begin{talign}
&\attindnorm
    \leq 
\frac{c}{2\sqrt{2}}\exp(\frac{R^2}{\sqrt{d}})\sqrt{\rownorm{\V}^2+\maxnorm{\V}^2}\frac{\sqrt{\log_2(\nout)\log({8\nout \log_2\frac{\nin}{\nout}})}}{\nout}.
\end{talign}
Hence, by \cref{att-err-decomposition,kms-bounds-att}, with probability at least $\half$,
\begin{talign}
\maxnorm{\hdav-\dav}
    &\leq 
\frac{c}{\sqrt{2}}\exp(\frac{2R^2}{\sqrt{d}})\sqrt{\rownorm{\V}^2+\maxnorm{\V}^2}\frac{\sqrt{\log_2(\nout)\log({8\nout \log_2\frac{\nin}{\nout}})}}{\nout} \\
    &\leq 
c\exp(\frac{2R^2}{\sqrt{d}})\rownorm{\V}\frac{\sqrt{\log_2(\nout)\log({8\nout \log_2\frac{\nin}{\nout}})}}{\nout}.
\end{talign}

%
\subsection{\pcref{att-err-decomposition}}\label{proof:att-err-decomposition}
By the triangle inequality, we have
\begin{talign}
\maxnorm{\hdav-\dav}
    \leq
\maxnorm{\hdav-\hddav} 
    +
\maxnorm{\hddav-\dav}.
\end{talign}
We bound the first term on the right-hand side using the submultiplicativity of the max norm under diagonal rescaling:
\begin{talign}
\maxnorm{\hdav-\hddav} 
    \leq
\maxnorm{\hD^{-1}}\maxnorm{\hav - \av}
    =
\maxnorm{(\frac{1}{n}\hD)^{-1}}\frac{1}{n}\maxnorm{\hav - \av}.
\end{talign}
To bound the second term we use the same submultiplicativity property and the fact that each entry of $\dav$ is the average of values in $\V$:
\begin{talign}
\maxnorm{\hddav-\dav}
    &=
\maxnorm{\hDinv(\D-\hD)\dav}
    \leq
\maxnorm{\hDinv}
\maxnorm{\D-\hD}
\maxnorm{\dav} \\
    &=
\maxnorm{(\frac{1}{n}\hD)^{-1}}
\frac{1}{n}\infnorm{\A\boldone_n-\ha\boldone_n}
\maxnorm{\V}.
\end{talign}
An identical argument reversing the roles of $(\D,\A)$ and $(\hD,\ha)$ yields the second bound.
%
\subsection{\pcref{kms-bounds-att}}\label{proof:kms-bounds-att}
Define the augmented value matrix $\augV=[\V,\maxnorm{\V}\boldone_n]\in\reals^{d+1}$.
By the definition of $\Katt$ and $\ha$,
%
%
%
%
%
%
%
%
\begin{talign}
\attindnorm
    =
\max_{i\in[n],j\in[d+1]}
|\sum_{\ell\in[n]}
\A_{i\ell} \augV_{\ell j}
(\pin-\qout)_\ell|
    =
\frac{1}{n}\infnorm{(\A-\ha)\augV\e_j^d}
    =
\frac{1}{n}\maxnorm{(\A-\ha)\augV}.
\end{talign}


%
\subsection{\pcref{att-parameters}}\label{proof:att-parameters}
First, by the Cauchy-Schwarz inequality and the nonnegativity of $\D=\A\boldone_n$ we have
\begin{talign}
\maxnorm{(\frac{1}{n}\D)^{-1}}
    =
\frac{1}{\min_{i\in[n]}\frac{1}{n}\sum_{j\in[n]}\A_{ij}}
    \leq
\frac{1}{\min_{i\in[n],j\in[n]}\exp(\frac{\inner{\query_i}{\key_j}}{\sqrt{d}})}
    \leq
\frac{1}{\min_{i\in[n],j\in[n]}\exp(\frac{-\twonorm{\query_i}\twonorm{\key_j}}{\sqrt{d}})}
    \leq
\exp(\frac{R^2}{\sqrt{d}}).
\end{talign}
Second, the  $\max_{\x\in\xin}\sqrt{\katt(\x,\x)}$ inequality follows as
\begin{talign}
\katt((\augkey_i,\augval_i),(\augkey_i,\augval_i))
    =
\exp(\frac{\twonorm{\key_i}^2}{\sqrt{d}})(\twonorm{\val_i}^2+\maxnorm{\V}^2)
    \leq
\exp(\frac{R^2}{\sqrt{d}})(\rownorm{\V}^2+\maxnorm{\V}^2).
\end{talign}
Third, the $R_\ind$ inequality follows as 
\begin{talign}
\twonorm{(\augquery_i,\e_j^{d+1})}
    =
\sqrt{\twonorm{\augquery_i}^2+1}
    \leq
\sqrt{\frac{R^2}{\sqrt{d}}+1}
    \qtext{for all}
i\in[n],j\in[d+1].
\end{talign}
Fourth, the $D_\ind$ inequality follows as 
\begin{talign}
\max_{i\in\ind}\Katt[,ii]
    =
\max_{i\in[n]}\exp(\frac{\twonorm{\query_i}^2}{\sqrt{d}})
    \leq
\exp(\frac{R^2}{\sqrt{d}}).
\end{talign}
Fifth, the rank inequality follows as $\x_i\in\reals^{d+1}$ for $i\in\ind$.
Finally, the Lipschitz inequality follows as, for any $i,k,l\in[n]$ and $j,m\in[d+1]$,
\begin{talign}
&|\exp(\frac{\inner{\query_i}{\key_l}}{\sqrt{d}})\inner{\e_j^{d+1}}{\augval_l} -  \exp(\frac{\inner{\query_k}{\key_l}}{\sqrt{d}})\inner{\e_m^{d+1}}{\augval_l}| \\
    &\leq
\exp(\frac{\inner{\query_i}{\key_l}}{\sqrt{d}})|\augval_{lj}-\augval_{lm}|
    +
|\exp(\frac{\inner{\query_i}{\key_l}}{\sqrt{d}})-\exp(\frac{\inner{\query_k}{\key_l}}{\sqrt{d}})||\augval_{lm}| \\
    &\leq 
\exp(\frac{\twonorm{\query_i}\twonorm{\key_l}}{\sqrt{d}})\twonorm{\e_j^{d+1}-\e_m^{d+1}}\frac{|\augval_{lj}-\augval_{lm}|}{\sqrt{2}}
    +
\exp(\frac{\max(\twonorm{\query_i},\twonorm{\query_k})\twonorm{\key_l}}{\sqrt{d}})|\frac{\inner{\query_i-\query_k}{\key_l}}{\sqrt{d}}||\augval_{lm}| \\
    &\leq
\exp(\frac{R^2}{\sqrt{d}})\twonorm{\e_j^{d+1}-\e_m^{d+1}}\frac{|\augval_{lj}-\augval_{lm}|}{\sqrt{2}}
    +
\exp(\frac{R^2}{\sqrt{d}})\frac{\twonorm{\query_i-\query_k} R}{\sqrt{d}}|\augval_{lm}| \\
    &\leq
\exp(\frac{R^2}{\sqrt{d}})\twonorm{\e_j^{d+1}-\e_m^{d+1}}\sqrt{2}\maxnorm{\V}
    +
\exp(\frac{R^2}{\sqrt{d}})\frac{\twonorm{\query_i-\query_k} R}{\sqrt{d}}\maxnorm{\V} \\
    &\leq
\exp(\frac{R^2}{\sqrt{d}})\sqrt{\frac{R^2}{\sqrt{d}} + 2}\maxnorm{\V}\twonorm{(\augquery_i,e_j^{d+1})-(\augquery_k,\e_m^{d+1})} 
\end{talign}
by the triangle inequality, multiple applications of Cauchy-Schwarz, and the mean-value theorem applied to $x\mapsto e^x$.
%

%

%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%

%
%
%
%
%
%
%
%

%
%
%
%



%

%
%
%
%
%

%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%

%
%
%
%
%
%
%

%

%
%
%
%
%
%

