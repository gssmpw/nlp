%
\section{\pcref{thm:convergence}}\label{proof:convergence}

Our proof makes use of three intermediate results.
The first, inspired by \citet[Thm.~10]{harvey2014near} and \citet[Lem.~1]{cooper2023coordinatingdistributedexampleorders}, relates the quality of the ordering produced by \cref{algo:reordering} to the quality of the thinning. 
%
\begin{lemma}[Quality of thinned reordering]
\label{lem:thinned-reordering-quality}
The output of thinned reordering (\cref{algo:reordering}) satisfies
\begin{talign}
\max_{j\in[n]}\twonorm{\sum_{i=1}^j \x_{\perm_{k+1}(\perm_{k}^{-1}(i))}^k}
    \leq
\half \max_{j\in[n]}\twonorm{\sum_{i=1}^j \x_{i}^k}
    +
\half \max_{j\in[n]}\twonorm{\sum_{i=1}^j \eps_i^k\x_{i}^k}
    +
\twonorm{\sum_{i=1}^n \x_i^k}
\end{talign}
where $\perm_k^{-1}$ is the inverse permutation of $\perm_k$ 
and $\eps_i^k \defeq 2(\indic{\x_i^k \in \xout^k}-1)$.
\end{lemma}
\begin{proof}
Fix any $\jstar \in \argmax_{j\in[n]}\twonorm{\sum_{i=1}^j \x_{\perm_{k+1}(\perm_{k}^{-1}(i))}^k}$. 
If $\jstar \leq n/2$, then 
\begin{talign}
2\twonorm{\sum_{i=1}^{\jstar} \x_{\perm_{k+1}(\perm_{k}^{-1}(i))}^k}
    \leq 
2\max_{j\in[n]}\twonorm{\sum_{i=1}^j \indic{\eps_i^k = 1} \x_{i}^k}
    \leq
\max_{j\in[n]}\twonorm{\sum_{i=1}^j \x_i^k}
    +
\max_{j\in[n]}\twonorm{\sum_{i=1}^j \eps_i^k\x_i^k}
\end{talign}
by the triangle inequality.
Similarly, if $\jstar > n/2$, then,
\begin{talign}
2(\twonorm{\sum_{i=1}^{\jstar} \x_{\perm_{k+1}(\perm_{k}^{-1}(i))}^k}
    -
\twonorm{\sum_{i=1}^n \x_i^k})
    &\leq
2\twonorm{\sum_{i > \jstar} \x_{\perm_{k+1}(\perm_{k}^{-1}(i))}^k}
    \leq 
2\max_{j\in[n]}\twonorm{\sum_{i=1}^j \indic{\eps_i^k = -1} \x_{i}^k} \\
    &\leq
\max_{j\in[n]}\twonorm{\sum_{i=1}^j \x_i^k}
    +
\max_{j\in[n]}\twonorm{-\sum_{i=1}^j \eps_i^k\x_i^k}.
\end{talign}
\end{proof}
%


The second, a mild adaptation of \citet[Thms.~2 and 3]{cooper2023coordinatingdistributedexampleorders}, bounds the convergence rate of SGD with thinned reordering in terms of the thinning quality.

\begin{theorem}[Convergence of SGD with thinned reordering]
\label{thm:onlinedm}
Suppose that, for all $i\in[n]$ and $\w,\v\in\reals^d$, 
\begin{talign}
&\twonorm{\grad f_i(\w ) - \grad f(\w)}^2    
    \le 
\sigma^2 
    \qtext{and} 
\twonorm{\grad f_i(\w ) - \grad f_i(\mbi v ) } 
    \leq 
L \twonorm{\w  - \v}
\end{talign}
and that SGD \cref{eq:sgd} with thinned reordering (\cref{algo:reordering})  satisfies the \emph{prefix discrepancy bound }
\begin{talign}\label{eq:prefix-discrepancy-bound}
\max_{j\in[n]}
\twonorm{\sum_{i=1}^j \eps_i^k\x_i^k} \le 2 \tilde{A} \max_{i\in[n]}\twonorm{\x_i^k - \xbar^k} 
\qtext{for}
\eps_i^k \defeq 2(\indic{\x_i^k \in \xout^k}-1),
\quad
\bar{\x}^k \defeq \frac{1}{n}\sum_{i=1}^n \x_i^k, 
\end{talign}
and each epoch $k\in[K]$. 
Then the step size setting
\begin{talign}
\alpha 
	= 
\min\left\{\frac{1}{16 L (2n + \tilde{A})}, \left(\frac{4 F_1}{42 L^2 \sigma^2\tilde{A}^2 n K + 18L^2  n^3 \sigma^2}\right)^{1/3}\right\} 
%
%
    \qtext{with}
F_1
	\defeq 
f(\w_1) - \fstar
    \qtext{and}
\fstar\defeq\inf_{\v\in\Rd} f(\v)
\end{talign}
yields the convergence bound
\begin{talign}
\frac{1}{K}\sum_{k=1}^{K}\norm{ \nabla f(\w_k) }^2  
	&\le 
\frac{9 (F_1 L\sigma \tilde{A})^{2/3}}{(n K)^{2/3}} + \frac{(72 F_1 L\sigma)^{2/3} + 64F_1 L (2 + \tilde{A}/(n))}{K}.
%
%
%
%
%
%
%
%
%
%
\end{talign}
If, in addition, $f$ satisfies the $\mu$-Polyak-≈Åojasiewicz (PL) condition, 
\begin{talign}
&\mu(f(\w)-\fstar)
    \leq 
\frac{1}{2} \twonorm{\grad f(\w)}^2
\qtext{for all}
%
%
\w \in\Rd, 
\end{talign}
and the number of epochs satisfies
\begin{talign}
K \ge 10 + \frac{1}{\mu}32 L(2+\tilde{A}/n)\tilde{W}%
	\qtext{for}
\tilde{W} 
	\defeq 
W_0(K^2n^2C_3)
	\qtext{and}
C_3 
	\defeq
\frac{(F_1+\sigma^2/L)\mu^2}{224L^2\sigma^2\tilde{A}^2},
\end{talign}
where $W_0$ denotes the Lambert W function, 
then the step size setting
$\alpha 
	= 
\frac{2\tilde{W}}{Kn\mu}$ 
%
%
yields the convergence bound 
\begin{talign}
f(\w_{K}) - \fstar 
    \leq
\frac{1}{(n K)^2}\left(\frac{(F_1 + L^2\sigma^2)\tilde{W}}{C_3} + \frac{112L^2\sigma^2\tilde{A}^2{\tilde{W}}^2}{\mu^3}\right).
%
%
%
\end{talign}
\end{theorem}
\begin{proof}
The proof is identical to that of \citet[Thms.~2 and 3]{cooper2023coordinatingdistributedexampleorders} with $m=1$ worker 
once each instance of $\infnorm{\cdot}$ is replaced with $\twonorm{\cdot}$, 
each instance of $L_{2,\infty}$ is replaced with $L$, 
%
each instance of $T$ is replaced with $K$, 
and 
\cref{lem:thinned-reordering-quality} is substituted for \citet[Lem.~1]{cooper2023coordinatingdistributedexampleorders}. 
\end{proof}


The final result uses \cref{thm:subg_low_rank_gen_kernel} to bound the prefix discrepancy of \khlind. 
%
\begin{lemma}[\khlind prefix discrepancy]\label{khlind-prefix-discrepancy}
Fix any epoch $k\in[K]$.  
With probability at least $1-\frac{\delta}{2}-\delta'$, 
thinned reordering (\cref{algo:reordering}) with \khlind satisfies 
the \emph{prefix discrepancy bound} \cref{eq:prefix-discrepancy-bound} with 
\begin{talign}
\tilde{A} 
    =
\sqrt{\log (\frac{2n(\log(n/2)+1)}{\delta})
\brackets{e^2\,\epsrank[\eps_k](\X^k)+e\log(\frac{n}{\delta'})}+ 1}
\end{talign}
for
$\X^k \defeq [\x_1^k,\dots,\x_n^k]^\top$,
$\eps_k \defeq {\max_{i\in[n]}\twonorm{\x_i^k-\xbar^k}}/{\sqrt{n}},$
and
$\xbar^k \defeq \frac{1}{n}\sum_{i=1}^n\x_i^k.$
\end{lemma}
%
\begin{proof}
Define 
%
$\xset^k = \{\x_1^k,\dots,\x_n^k\}$,
$c = 2\max_{i\in[n]}\twonorm{\x_i^k - \xbar^k}$,
and $r = \epsrank[\eps_k](\X^k)$.
%
For any $j\in[n]$, we can write
\begin{talign}
\twonorm{\sum_{i=1}^j \eps_i^k\x_i^k}
	=
\twonorm{\sum_{i=1}^j \x_i^k - \sum_{i=1}^j \indic{\x_i^k\in\xoutj^k}\x_i^k}
	=
2j\twonorm{(\X^k)^\top(\pin^j - \qout^j)}
	=
2j\mmd_{\X^k(\X^k)^\top}(\pin^j, \qout^j)  
\end{talign}
where $\pin^j$ and $\qout^j$ are the empirical distributions over $\xinj^k = (\x_i^k)_{i=1}^j$ and $\xoutj^k = \{ \x_i^k \in \xout^k : i \in [j]\}$.

Since \khlind is an online algorithm that assigns signs $(\eps_i^k,\eps_{i+1}^k=1-\eps_i^k)$ to the points $(\x_i^k,\x_{i+1}^k)$ sequentially, we can view $\xoutj^k$ as the output of \khlind applied to $\xinj^k$ with $\nout= \frac{j}{2}$ and the linear kernel $\kernel(\x,\y)=\inner{\x}{\y}$ for each $j\in[n]$.  Therefore, we may invoke the established \khlind sub-Gaussian constants $\subg_j$ of \cref{khlind-sub-gaussian}, \cref{thm:subg_low_rank_gen_kernel}, the union bound, and the definition of $\eps$-rank (\cref{def:eps-rank}) to deduce that 
\begin{talign}
\max_{j\in[n]} \twonorm{\sum_{i=1}^j \eps_i^k\x_i^k}^2
    &\leq 
\max_{j\in[n]} 4j^2\subg_j^2 \brackets{e^2r+e\log(\frac{n}{\delta'})}+ \sig_{r+1}(\X^k)^2\frac{4j^2}{j} 
    \leq 
c^2 \tilde{A}^2
\end{talign}
with probability at least $1-\frac{\delta}{2}-\delta'$.
\end{proof}
%

\cref{thm:convergence} now follows directly from \cref{thm:onlinedm} and \cref{khlind-prefix-discrepancy} applied to \khsgd with $\delta'=\frac{1}{4K}$ and a union bound over epochs.

%

%
%
%
%
%
%
%
%
%
%
%

%

%
%
%

%