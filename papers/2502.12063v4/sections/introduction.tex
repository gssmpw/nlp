%

%
\section{Introduction}
\label{sec:intro}
%

%

%

%

%

This work is about thinning,  %
finding a small set of representative points to accurately summarize a larger dataset. 
State-of-the-art thinning %
techniques %
provably improve upon uniform subsampling %
but only for restricted classes of kernel-based quality measures %
and with pessimistic dependence on the data dimension \citep[see, e.g.,][]{harvey2014near,phillips2020near,alweiss2020discrepancyminimizationselfbalancingwalk,dwivedi2024kernel,dwivedi2021generalized,shetty2022distributioncompressionnearlineartime,li2024debiased}. 
%
We introduce a new analysis for sub-Gaussian thinning algorithms that applies to any kernel and shows that one can efficiently identify a better-than-uniform set of representative points whenever the 
kernel or data matrix 
%
is nearly low-rank.
This opens the door to a variety of impactful applications including approximate dot-product attention in transformers, accelerated stochastic gradient training, and distinguishing distributions with deep kernels in near-linear time.

%
%
%
%
%

\paragraph{Notation.}
%
%
For each $n\in\naturals$ and $a,b\in\reals$,  we define $[n] \defeq \{1,\dots,n\}$, $a \wedge b \defeq \min(a,b)$, and $a \vee b \defeq \max(a,b)$.
We let $\specnorm{\A}$, $\maxnorm{\A}$, and $\rownorm{\A}$ respectively represent the maximum singular value, absolute entry, and row Euclidean norm of a matrix $\A$ and let $\lam_{r}(\K)$ denote the $r$-th largest eigenvalue of a suitable matrix $\K$. 
%
We also define the Euclidean norm balls
$\ball^m 
    \defeq 
\{ \bu \in\reals^{m} : \twonorm{\bu} \leq 1\}$ 
and $\ball^m(R)\defeq R\,\ball^m$ for each $m\in\naturals$ and $R>0$.
For an event $\event$ and an integrable random variable $X$, we define $\Esub{\event}[X] \defeq \E[X\cdot\indic{\event}]$.
We write $a_n \leq \Otilde(b_n)$ to mean $a_n \leq b_n \polylog(n)$.
