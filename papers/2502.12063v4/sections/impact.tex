\section*{Impact Statement}

This work introduced a new analysis of thinning algorithms that adapts to low-rank structures.
We exploited this adaptivity to design fast algorithms with strong quality guarantees for  three key applications in machine learning: dot-product attention in Transformers, stochastic gradient training in optimization, and deep kernel testing for distinguishing distributions.
More broadly, our techniques provide a general framework for reducing computational resource use in machine learning. Such tools have the potential to reduce energy costs and environmental harms from model training, inference, and evaluation and to improve accessibility in resource-constrained settings, all while provably maintaining high quality. %
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%

%
%
%

%
%
%
%
