%
\section{Faster SGD Training} %
\label{sec:SGD}
To train a machine learning model parameterized by a vector $\w\in\Rd$, a standard approach is to minimize the empirical risk $f(\w) \defeq \frac{1}{n}\sum_{i=1}^n f_i(\w)$ using stochastic gradient descent (SGD) updates,%
\begin{talign}
%
%
%
%
&\w^{k+\frac{i}{n}}
= \w^{k+\frac{i-1}{n}} - \alpha \grad f_{\perm_k(i)}(\w^{k+\frac{i-1}{n}}), \label{eq:sgd}
\end{talign}
for each epoch $k\in[K]$ and datapoint $i\in[n]$.
Here, $\alpha > 0$ is a step size, each $f_i$ is a datapoint-specific loss function, and $\perm_k$ is a permutation of $[n]$ representing the order in which datapoints are processed in the $k$-th epoch.

\begin{algorithm2e}[htb]
\caption{Thinned Reordering}
\label{algo:reordering}
\small{
\KwIn{Stochastic gradients $(\x_i^k \defeq \grad f_{\perm_k(i)}(\w^{k+\frac{i-1}{n}}))_{i=1}^n$, prior ordering $\perm_k$, thinning algorithm \alg}
\BlankLine
// Select half of points using linear kernel \\[2pt]
$\xout^k \gets \alg(\xin=(\x_i^k)_{i=1}^n, \nout=\frac{n}{2}, \k(\x,\y)=\inner{\x}{\y})$ 
    \ \ \\[2pt]
$\lfront \gets [];\quad \lback \gets []$ 
    \qquad\ \ \ // Initialize empty start and end lists \\[2pt] %
\For{$i=1, \ldots, n$}
{$\lfront.\texttt{append}(\perm_k(i))$ if $\x_i^k\in\xout^k$ else  $\lback.\texttt{prepend}(\perm_k(i))$} 
\vspace{2pt}
\KwRet{\textup{$\perm_{k+1} = \concat(\lfront, \lback)$}}
\vspace{2pt}
}
\end{algorithm2e}
\begin{figure*}[tb]
    {\centering
    %
    %
    %
        \includegraphics[width=.5\textwidth]{figures/HMDA-epoch.pdf}  %
    %
    %
    %
    %
    %
    %
        \includegraphics[width=.5\textwidth]{figures/HMDA-second.pdf}  %
    %
    %
}
    \caption{
    \tbf{Train and test convergence trajectories for mortgage  classification with reordered SGD variants.} We display mean values $\pm 1$ standard deviation across $5$ random seeds. See \cref{sec:theory-practice-gap} for more details.}
    \label{fig:sgd-epoch-time}
\end{figure*}

Typically, one selects the orderings $\perm_k$ uniformly at random, but recent work has demonstrated faster convergence using non-uniform, adaptively selected orderings.
Specifically, \citet{lu2023grab,cooper2023coordinatingdistributedexampleorders} show that any sufficiently accurate thinning algorithm 
%
%
%
%
can be efficiently transformed into a reordering rule that improves the convergence rate of SGD by a substantial $\Otilde(n^{-1})$ factor.
Their approach, distilled in \cref{algo:reordering}, uses an elegant  construction of \citet[Thm.~10]{harvey2014near} to translate a high-quality thinning of stochastic gradients into a higher-quality reordering. 
%
%
However, these prior studies leave two problems unaddressed. 



First, while the established convergence rates of \citet{lu2023grab} nearly match the minimax lower bounds for permuted SGD algorithms  \citep[Thm.~4.5]{cha2023tighterlowerboundsshuffling}, a multiplicative gap of size $\Theta(d)$ remains in the worst case.
This led \citet{cha2023tighterlowerboundsshuffling} to declare, ``It is an open problem whether there exists a permutation-based SGD algorithm that gives a dimension-free upper bound while maintaining the same dependency on other factors.'' 

Second, \citet{lu2023grab} carry out their analysis using the self-balancing walk (SBW) thinning algorithm of \citet{alweiss2020discrepancyminimizationselfbalancingwalk} but find its overhead to be too high in practice.  Hence, in all experiments they instead employ a greedy thinning algorithm that often works well in practice but is not covered by their analysis. %
%

%
\subsection{Bridging the dimension gap}\label{sec:dimension-gap}
%
%
%
To address the first problem, 
%
%
%
we derive a new guarantee for SGD with \khlin reordering that  replaces the typical $\Theta(d)$ penalty with a soft notion of rank.
%
\begin{definition}[$\eps$-rank]\label{def:eps-rank}
The \emph{$\eps$-rank}, $\epsrank(\X)$, of a matrix $\X$ is the number of singular values greater than $\eps$.
%
%
%
%
%
%
%
\end{definition}

%
%
%
%
%

%
%
%
%
%

%

\begin{theorem}[\textbf{\khlin-SGD convergence}]\label{thm:convergence}
Suppose that, for all $i\in[n]$ and $\w,\v\in\reals^d$, 
\begin{talign}
&\twonorm{\grad f_i(\w ) - \grad f(\w)}^2    
    \le 
\sig^2,
%
%
%
\\
&\twonorm{\grad f_i(\w ) - \grad f_i(\mbi v ) } 
    \leq 
L \twonorm{\w  - \v}, 
    \qtext{and}  \\
&f(\w)-\fstar
    \leq 
\frac{1}{2\mu} \twonorm{\grad f(\w)}^2
\qtext{for}
\fstar\defeq\inf_{\v\in\Rd} f(\v).
\end{talign}
Then SGD \cref{eq:sgd} with \khsgd reordering (\cref{algo:reordering}) and step size $\alpha$ given in \cref{proof:convergence}
%
%
%
%
%
%
%
%
%
satisfies, with probability at least $\half$, 
\vspace{-.5\baselineskip}
\begin{talign}
\textstyle f(\w_K)-\fstar
    \leq 
\Otilde(\frac{r}{n^2 K^2})
    \qtext{for}\qquad\qquad\ \ 
\\[5pt]
r 
    \defeq
\max_{k\in[K]}\epsrank[\eps_k](\X^k),
    %
    \quad
\X^k \defeq [\x_1^k,\dots,\x_n^k]^\top,
    \quad
    \\
\textstyle\eps_k \defeq {\max_{i\in[n]}\twonorm{\x_i^k-\xbar^k}}/{\sqrt{n}},
    \sstext{and}
\xbar^k \defeq \frac{1}{n}\sum_{i=1}^n\x_i^k.
\end{talign}
\end{theorem}
%
The proof of \cref{thm:convergence} in \cref{proof:convergence} simply uses \cref{thm:subg_low_rank_gen_kernel} to bound the thinning quality of \khsgd and then adapts the prior SGD analysis of \citet{cooper2023coordinatingdistributedexampleorders}. 
Notably, the standard practice of \emph{random reshuffling}, i.e., SGD with uniform reordering, can only guarantee a significantly slower $\Omega(\frac{1}{nK^2})$ rate under these assumptions \citep[Thm.~2]{rajput2020closing}, while 
\citet[Thm.~4]{lu2023grab} implies a similar but dimension-dependent  $\Otilde(\frac{d}{n^2 K^2})$   rate for SBW reordering. 
%
\cref{thm:convergence} shows that this dimension dependence can be avoided whenever the gradient update matrices $\X^k$ are low-rank, or, more generally, whenever they are $\eps=O(1/\sqrt{n})$-approximable by low-rank matrices.

%
\subsection{Bridging the theory-practice gap}\label{sec:theory-practice-gap}
Two criticisms levied by \citet{lu2023grab} against the SBW algorithm were the need to estimate the maximum Euclidean norm of any possible gradient vector in advance and the need to tune its free hyperparameter. 
\khsgd has neither of these drawbacks as it automatically adapts to the scale of each input and has no hyperparameters to tune.
Moreover, with a linear kernel, \khsgd can be run online in $O(nd)$ time. 
Hence, \khsgd is a promising substitute for the greedy thinning of \citet{lu2023grab,cooper2023coordinatingdistributedexampleorders}. 
Indeed, when we recreate the Home Mortgage Disclosure Act logistic regression experiment of \citet{cooper2023coordinatingdistributedexampleorders} with a single worker (\cref{fig:sgd-epoch-time}), we find that 
%
\khlin-SGD strongly outperforms the standard practice of random reshuffling (RR) and the theoretically justified but overly conservative CD-GraB: SBW variant.
In addition, \khlin-SGD 
matches the state-of-the-art test accuracy of CD-GraB: Greedy and lags only slightly in terms of training convergence.
%
%
%
See 
\url{https://github.com/microsoft/khsgd}
for PyTorch code replicating this experiment %
and \cref{app:sgd_details} for supplementary experiment details.



%
%
%
%
%

%

%

%

%

%

%
%
%
%
%

%
%

%

%

%





%



%
%

%

%

%


%


%

%
%
%
%
%



%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
     
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
        
%
%
%
%
%
%
%