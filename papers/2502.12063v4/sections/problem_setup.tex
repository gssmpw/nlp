%

%
\section{Sub-Gaussian Thinning}
\begin{table*}[t]
    \centering
    \caption{\tbf{Examples of $\mbi{(\K,\subg,\delta)}$-sub-Gaussian thinning algorithms.} 
    %
    For input size $\nin$, output size $\nout\geq \sqrt{\nin}$, %
    and $\Kmax=1$ 
    we report each 
    %
    sub-Gaussian parameter $\subg$ and runtime up to constants independent of $(\nin,\nout,\delta,\K)$.
    }
    \resizebox{\textwidth}{!}{
    %
    \begin{tabular}{ccc ccc}
    \toprule
    \bf{Algorithm}
    
    & \Centerstack{\bf \subsampling \\
    \tiny{\cref{prop:uniformsubg} } }
    
    & \Centerstack{\bf $\khd$ \\ \tiny{\cref{khd-sub-gaussian}} }
    
    & \Centerstack{\bf $\khcompress(\delta)$ \\ \tiny{\cref{khcompressd-sub-gaussian}}}
    
    & \Centerstack{\bf \gsthin \\ 
    \tiny{\cref{prop:gs_thin}}}
    
    & \Centerstack{\bf \gscompress  \\ \tiny{\cref{prop:gs_thin_compress}} } 
    \\[1mm]
    
    \midrule
    
    \Centerstack{\bf Sub-Gaussian \\ \bf parameter $\nu$} 
    & 
    %
    {\large$\frac{1}{\sqrt{\nout}}$}
    & {\large $\frac{\sqrt{\log(\nout/\delta)}}{\nout}$} 
    & {\large$\frac{\sqrt{\log(\nout)\log(\nout/\delta)}}{\nout}$ }
    & {\large$\frac{1}{\nout}$ }
    &{\large$\frac{ \sqrt{\log(\nout)}}{\nout}$} 
    \\[3mm]
    
    \Centerstack{\bf Runtime}
    & $\nout$
    & $\nin^2$ 
    & $\nout^2$ %
    & $\nin^3$
    %
    & $\nout^{3}$
    \\[1mm]
    
    \bottomrule
    \end{tabular}
    }
    \label{tab:subg_thinning_algorithms}
\end{table*}
%

Consider a fixed collection of $\nin$ input points $\xin$ belonging to a potentially larger universe of datapoints $\xset\defeq\{\x_1,\dots,\x_n\}$.
The aim of a thinning algorithm is to select $\nout$ points from $\xin$ that together accurately summarize $\xin$. 
This is formalized by the following definition.

%

%
%
%
%
%
%
%
%
%
%
%
%

\begin{definition}[\tbf{Thinning algorithms}]
\label{def:thinning_algo}
    %
    %
A \emph{thinning algorithm} \alg takes as input $\xin$ and returns a possibly random subset $\xout$ of size $\nout$. 
We denote the input and output empirical distributions by $\Pin \defeq \frac1\nin \sum_{\x\in\xin} \dirac_{\x}$  and $\Qout \defeq \frac1\nout \sum_{\x\in\xout}\dirac_{\x}$ and 
define the induced probability vectors $\pin, \qout\in\simplex$ over the indices $[n]$ by
    \begin{talign}
        \pin[\textup{in}, i] = \frac{\indic{\x_i\in\xin}}{\nin}
        \sstext{and}
        \qout[\textup{out}, i] = \frac{\indic{\x_i\in\xout}}{\nout}
        \sstext{for all} i \in [n].
    \end{talign}
    When $\xset\subset\real^d$, we use $\X \defeq [\x_1, \ldots, \x_n]\tp  \in \real^{n\times d}$ to denote the input point matrix so that
    \begin{talign}
    %
    \E_{\x\sim\Pin}[\x] = \X\tp \pin
    \qtext{and}
    %
    \E_{\x\sim\Qout}[\x] = 
    %
    %
    %
    \X\tp\qout.
\end{talign}
%
%
%
%
%
%
%
%
%
%
%
%
%
\end{definition}

%
%
We will make use of two common measures of summarization quality.\\[\baselineskip]
%

\begin{definition}[\tbf{Kernel MMD and max seminorm}]\label{def:mmd}
     Given two distributions $\mu, \nu$ and a reproducing kernel $\kernel$ \citep[Def.~4.18]{Steinwart2008SupportVM}, the associated kernel \emph{maximum mean discrepancy (MMD)} is the worst-case difference in means for functions in the unit ball $\ball_{\kernel}$ of the associated reproducing kernel Hilbert space:
    \begin{talign}
        \mmd_{\kernel}(\nu, \mu) &\defeq \sup_{f\in\ball_{\kernel}} |\E_{\x\sim \mu} f(\x) - \E_{\x\sim \nu} f(\x)|. 
\end{talign}
When $\mu = \Pin$ and $\nu=\Qout$ as in \cref{def:thinning_algo} and $\mbf K \defeq (\kernel(\x_i,\x_j))_{i, j=1}^n \in \real^{n \times n}$ denotes the induced kernel matrix, then the MMD can be expressed as a Mahalanobis distance between $\pin$ and $\qout$:
\begin{talign}
\mmd_{\kernel}(\Pin, \Qout) 
    &= \sqrt{(\pin-\qout)\tp\mkernel(\pin-\qout)} \\
    &\defeq \mmd_{\mkernel}(\pin, \qout).
    \label{eq:mmd_maha}
\end{talign}
For any indices $\ind\subseteq [n]$, we further define the \emph{kernel max seminorm (KMS)}
\begin{talign}
     \indnorm \defeq \max_{i\in\ind} |\e_i^\top \K (\pin-\qout)|.
    \label{eq:kms}
\end{talign}
\end{definition}

Notably, when the input points lie in $\real^d$ and $\kernel(\x_i,\x_j)$ is the linear kernel $\inner{\x_i}{\x_j}$ (so that $\mkernel = \X\X\tp$), MMD measures the Euclidean discrepancy in datapoint means between the input and output distributions:
\begin{talign}
    \mmd_{\mkernel}(\pin, \qout) = \twonorm{\X\tp\pin -\X\tp\qout}.
\end{talign}

%
A common strategy for bounding the error of a thinning algorithm is to establish its sub-Gaussianity.
%
\begin{definition}[\tbf{Sub-Gaussian thinning algorithm}]\label{def:alg-subg}
    We write $\alg \in \ksubge$ and say $\alg$ is \emph{$(\K,\subg,\delta)$-sub-Gaussian}, if $\alg$ is a thinning algorithm, $\K$ is a symmetric positive semidefinite (SPSD) matrix, $\subg >0$, $\delta\in[0, 1)$, and there exists an event $\mc E$ with probability at least $1-\delta/2$ such that, the input and output probability vectors satisfy
    \begin{talign}
    \Esub{\event}[\exp\parenth{\angles{\bu,\K(\pin-\qout)}}] \leq \exp\big(\frac{\subg^2}{2}  \bu^\top \K\bu\big),  \forall \bu \in \Rn.
\end{talign}
\end{definition}
%
%
%
Here, the sub-Gaussian parameter $\subg$ controls the summarization quality of the thinning algorithm, and
%
we see from \cref{tab:subg_thinning_algorithms} 
that a variety of practical thinning algorithms are $(\K,\subg,\delta)$-sub-Gaussian for varying levels of $\subg$. 
%


 
%
%
\subsection{Examples of sub-Gaussian thinning algorithms}
Perhaps the simplest sub-Gaussian thinning algorithm is \emph{uniform subsampling}: by \cref{prop:uniformsubg}, selecting $\nout$ points from $\xin$ uniformly at random (without replacement) is $(\K,\subg,0)$-sub-Gaussian with $\subg = {\sqrt{\Kmax}/\sqrt{\nout}}$. 
Unfortunately, uniform subsampling suffers from relatively poor summarization quality.
As we prove in \cref{proof:uniform-subsampling}, its root-mean-squared MMD and KMS are both $\Omega(1/\sqrt{\nout})$ meaning that $\nout=10000$ points are needed to achieve $1\%$ relative error.

%
%
%
%

%
%
%
%
%
%
%
%
%
%

%
%

%
%
\begin{proposition}[\tbf{Quality of uniform subsampling}]\label{prop:uniform-subsampling} 
%
For any $\ind\subseteq [n]$, 
a uniformly subsampled thinning %
   %
   %
   %
   %
   %
satisfies
\begin{talign}
 %
\E[\mmd^2_{\K}(\pin, \qout)]
    &= 
%
\frac{1}{\nout}\textfrac{\nin-\nout}{\nin-1}\, C_{\K}
%
%
   %
   %
   %
   %
\qtext{and}\\
\E[\indnorm^2]
    &\geq
\frac{1}{\nout}\textfrac{\nin-\nout}{\nin-1}\, \max_{i\in\ind} 
 C_{\K\e_i\e_i^\top\K}
\end{talign}
for any SPSD $\K$ with $C_{\K} \defeq \sum_{i=1}^n\pin[\textup{in},i]\K_{ii} - \pin\tp\K\pin$.
   %
\end{proposition}

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

Fortunately, uniform subsampling is not the only sub-Gaussian thinning algorithm available.  
For example, the Kernel Halving (\khd) algorithm of \citet{dwivedi2024kernel} provides a substantially smaller  sub-Gaussian parameter, $\nu = O({\sqrt{\log(\nout/\delta)}}{/\nout})$, at the cost of $\nin^2$ runtime, 
while the $\khcompress(\delta)$ algorithm of \citet[Ex.~3]{shetty2022distributioncompressionnearlineartime} delivers $\subg=O({\sqrt{\log(\nout)\log(\nout/\delta)}}{/\nout})$ in only $\nout^2$ time.  
We derive simplified versions of these algorithms with identical sub-Gaussian constants in \cref{sub:khd,sub:khcompress} and a linear-kernel variant (\khlind) with $\nin d$ runtime in \cref{sub:khlind}.
To round out our set of examples, we show in \cref{proof:gs_thin} that two new thinning algorithms based on the Gram-Schmidt walk of \citet{bansal2018gram} yield even smaller $\subg$ at the cost of increased runtime.  
We call these algorithms Gram-Schmidt Thinning 
(\gsthin) and \gscompress.


%
\section{Low-rank Sub-Gaussian Thinning}\label{sec:low-rank}
One might hope that the improved sub-Gaussian constants of \cref{tab:subg_thinning_algorithms} would also translate into improved quality metrics.  
Our main result, proved in \cref{proof:subg_low_rank_gen_kernel}, shows that this is indeed the case whenever the inputs are approximately low-rank. 


\newcommand{\lowrankerrorbound}{Low-rank sub-Gaussian thinning}%
\begin{theorem}[\tbf{\lowrankerrorbound}]\label{thm:mmd-kernel-compression}
Fix any $\delta'\in(0, 1)$, $r \leq n$, and $\ind\subseteq [n]$. 
If $\alg \in \ksubge$, then the following bounds hold individually with probability at least $1-\delta/2-\delta'$:
\begin{talign}
\mmd^2_{\mkernel}(\pin, \qout)  
    &\leq 
\subg^2 
\brackets{e^2r+e\log(\frac{1}{\delta'})}\\ %
&+ \lambda_{r+1}(\frac{1}{\nout} -\frac{1}{\nin}) 
\qtext{and}
\label{eq:mmd_bound}\\
\indnorm
    &\leq     \subg\inddiam\sqrt{2\log(\frac{2|\ind|}{\delta'})}.
    \label{eq:ind_bound}
%
%
%
\end{talign}
Here, $\lambda_{j}$  denotes the $j$-th largest eigenvalue of $\mkernel$, $\lambda_{n+1}\defeq 0$, and 
$D_{\ind} \defeq \max_{i\in\ind}\sqrt{\K_{ii}}$.
%

Suppose that, in addition, 
$\xset\subset\reals^d$ and 
%
$|\K_{il} -\K_{jl}| \leq L_\K \twonorm{\x_i-\x_j}$ for some $L_\K > 0$ and all $i,j\in\ind$ and $l\in\supp{\pin}$.
Then, with probability at least $1-\delta/2-\delta'$,
\begin{talign}
&\indnorm
    \leq 
\subg\inddiam\sqrt{2\log(4/\delta')}(1+\frac{32}{\sqrt{3}})
 \\
    &\ \ \qquad+ 
\subg \inddiam\, 32\sqrt{\frac{2}{3}\,\rank{\X_\ind}\log(\frac{3e^2R_\ind L_{\K}}{\inddiam^2 \wedge (R_\ind L_{\K})})}
\label{eq:ind_bound_lipschitz}
\end{talign}
for $R_\ind \defeq \max_{i\in\ind}\twonorm{\x_i}$
and $\X_\ind \defeq [\x_i]_{i\in\ind}^\top$.
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
\label{thm:subg_low_rank_gen_kernel}
\end{theorem}

%
%

Let us unpack the three components of this result.
First, \cref{thm:subg_low_rank_gen_kernel} provides 
a high-probability
%
$O(\subg \sqrt{\log( |\ind|)})$  bound \cref{eq:ind_bound} on the KMS for any kernel and any sub-Gaussian thinning algorithm on any space. 
In particular, the non-uniform algorithms of \cref{tab:subg_thinning_algorithms} all enjoy $O(\log(\nout)\sqrt{\log(|\ind|)}/\nout)$ KMS, a significant improvement over the $\Omega(1/\sqrt{\nout})$ KMS of uniform subsampling.
Second, \cref{thm:subg_low_rank_gen_kernel} 
provides a refined $O(\subg \sqrt{\rank{\X_\ind}\log(R_\ind L_{\K})})$ bound \cref{eq:ind_bound_lipschitz} on KMS for datapoints in $\reals^d$.
For bounded data, this  trades an explicit dependence on the number of query points $|\ind|$ for a rank factor that is never larger (and sometimes significantly smaller) than $d$. 
%
We will make use of these results when approximating dot-product attention in \cref{sec:attn}.

Finally, \cref{thm:subg_low_rank_gen_kernel} provides an $O(\subg \sqrt{r} + \sqrt{\lam_{r+1}/\nout})$ high-probability bound on kernel MMD, where the approximate rank parameter $r$ can be freely optimized.
When $\K = (\k(\x_i, \x_j))_{i,j=1}^n$ is generated by a finite-rank kernel $\k$, like a linear kernel $\inner{\x_i}{\x_j}$, a polynomial kernel $(1+\inner{\x_i}{\x_j})^p$, or a random Fourier feature kernel \citep{rahimi2007random}, this guarantee becomes $O(\subg)$ and improves upon uniform subsampling whenever $\subg = o(1/\sqrt{\nout})$.
In this case, the non-uniform algorithms of \cref{tab:subg_thinning_algorithms} all enjoy $O(\log(\nout)/\nout)$ MMD, a significant improvement over the $\Omega(1/\sqrt{\nout})$ MMD of uniform subsampling.
We will revisit this finite-rank setting when studying stochastic gradient acceleration strategies in \cref{sec:SGD}.

More generally, \cref{thm:subg_low_rank_gen_kernel} guarantees improved MMD even for full-rank $\K$, provided that the eigenvalues of $\K$ decay sufficiently rapidly.  
For example, optimizing over the approximate rank parameter $r$ yields an $O(\subg \log^{p/2}(\nout) )$ bound under exponential eigenvalue decay $\lam_{r+1} = O(n e^{-c r^{1/p}})$ and an $O(\subg^{\frac{p}{p+1}} (\frac{n}{\nout})^{\frac{1}{2(p+1)}})$ bound under polynomial eigenvalue decay $\lam_{r+1} = O(n/ r^{p})$. 
Fortunately, some of the most commonly-used kernels  generate kernel matrices with rapid eigenvalue decay.

For example, the popular Gaussian kernel on $\Rd$, %
\begin{talign}\label{eq:gaussian_kernel}
\textsc{Gauss}(\eta):\ 
    \kernel(x,y) = \exp(-\eta \statictwonorm{x-y}^2)\stext{for} \eta > 0,
\end{talign}
 generates $\K = (\k(\x_i, \x_j))_{i,j=1}^n$ satisfying 
\begin{talign}\label{eq:gsn_lambda_ball}
    \lambda_{r+1} \leq n e^{-\frac{d}{2e} r^{1/d} \log\parenth{\frac{d r^{1/d}}{4 e^2 \eta R^2}}}\sstext{for} (2e)^d \leq r < n
\end{talign}
 whenever 
 $\Xset\subset\ball^d(R)$ %
 \citep[Thm.~3]{altschuler2019massivelyscalablesinkhorndistances}.
 Combined with \cref{thm:subg_low_rank_gen_kernel}, this fact immediately yields an MMD guarantee for each algorithm in \cref{tab:subg_thinning_algorithms}. We present a representative guarantee for \khd.
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


%
%
%
%
%
%

%

%
%
%
%
%

%

%

%

%
%
%
%
%

%

%
%
%

%
%

%

%
%
%
%

%
%



%
%
%
%
%
%
%

%

%

%
%
%
%

%
\newcommand{\Rin}{R_{\textup{in}}}
\begin{corollary}[\tbf{Gaussian MMD of \kh}]\label{cor:gaussian_mmd}
If $\xin \subset \ball^d (R)$ for $R>0$, then $\khd$ with $\kernel=\textsc{Gauss}(\eta)$, %
and $n=\nin$ delivers
\begin{talign}\label{eq:gaussian_mmd_ball}
&\mmd_{\mkernel}^2(\pin,\qout) 
    \leq \\ 
&\quad O \bigl( \frac{\log({\nout}{/\delta})}{\nout^2} 
\big(\big(\frac{\log(\nout)\vee(R^2\eta)}{d}\big)^{d}
%
+ \log(\frac{1}{\delta'})\big)\bigr)
\end{talign}
with probability at least $1-\delta/2-\delta'$. 
%
%
%
\end{corollary}

The proof in \cref{proof:gaussian_mmd} provides a fully explicit 
and easily computed bound on the Gaussian MMD. 
Under the same assumptions, the  distinct analysis of 
\citet[Thm.~2, Prop.~3]{dwivedi2021generalized} provides a squared MMD bound of size 
$\Theta\big(\frac{\log(\nout/\delta)}{\nout^2}(\frac{\log^{d+1}(\nout)R^d\eta^{d/2}}{(\log\log(\nout))^d}+ \log(\frac{1}{\delta'}))\big)$. 
%
%
Notably, \cref{cor:gaussian_mmd} improves upon this best known \khd guarantee whenever the datapoint radius $R=O(\log \nout)$, a property that holds almost surely for any bounded, sub-Gaussian, or  subexponential data sequence \citep[see][Prop.~2]{dwivedi2024kernel}.

%
%
%
%
%
%

\citet[Thm.~4]{altschuler2019massivelyscalablesinkhorndistances} additionally showed that
\begin{talign}\label{eq:gsn_lambda_manifold}
    \lambda_{r+1} \leq n e^{- cr^{{2}{/(5d^\star)}}}\qtext{for} 1 \leq r < n
\end{talign}
for a constant $c$ independent of $\Xset$ when $\Xset$ belongs to a smooth compact manifold of dimension $d^\star < d$.
In this case, our low-rank analysis %
yields adaptive MMD guarantees that scale with the potentially much smaller intrinsic dimension $d^\star$.
We use \cref{thm:subg_low_rank_gen_kernel} to prove the first such intrinsic-dimension guarantee for \khd in \cref{proof:gaussian_mmd_manifold}.

%
%

\begin{corollary}[\tbf{Intrinsic Gaussian MMD of \kh}]
\label{cor:gaussian_mmd_manifold}
If $\xin$ lies on a smooth manifold $ \Omega \subset \ball^d$ of dimension $d^\star < d$ (\cref{assum:manifold}), then $\khd$ with $\kernel=\textsc{Gauss}(\eta)$, 
%
and $n=\nin$ delivers
\begin{talign}\label{eq:gaussian_mmd_manifold}
\mmd_{\mkernel}^2(\pin,\qout) 
    \leq 
O\big( \frac{ \log(\frac{\nout}{\delta})}{\nout^2} \big( (\frac{\log(\nout)}{c})^{\frac{5  d^\star}{2}} \!+\log(\frac{1}{\delta'})\big)\big)
\end{talign}
with probability at least $1-
\frac{\delta}{2}-\delta'$ for $c$ independent of $\xin$.
\end{corollary}
%

In \cref{sec:ctt}, we will use \cref{cor:gaussian_mmd,cor:gaussian_mmd_manifold} to establish new guarantees for distinguishing distributions in near-linear time.
%