\section{Approximating Attention}\label{sec:attn}
Dot-product attention lies at the heart of the Transformer neural network architecture that has revolutionized natural language processing, computer vision, and speech recognition over the last decade \citep{vaswani2017attention,dosovitskiy2021an,dong2018speech-transformer}.
Given a collection of query, key, and value vectors $(\query_i,\key_i,\val_i)_{i=1}^n$ each in $\Rd$, dot-product attention computes the \emph{softmax matrix}
\begin{talign}
\label{def:att}
&\T 
    \defeq 
\attention((\query_i)_{i=1}^n, 
           (\key_j, \val_j)_{j=1}^n) 
    \defeq
\Dinv \A \V  \\
    &\text{for}\  
\A_{ij} 
    \defeq 
\exp(\frac{\inner{\query_i}{\key_j}}{\sqrt{d}}),
\D = \diag(\A\boldone_n),
    \stext{and}
\V_{ij}
    \defeq
\val_{ij}.
\end{talign}
While attention has enjoyed unprecedented success in capturing long-range dependencies amongst datapoints, its computation is expensive, requiring $\Theta(d\,n^2)$ time to construct and multiply the matrix $\A$. 
This quadratic-time bottleneck has inspired a plethora of practical approximate attention mechanisms \citep[e.g.,][]{kitaev2020reformer,choromanski2021rethinking,chen2021scatterbrain}, but, to our knowledge, only two guarantee accurate reconstruction of the softmax matrix $\T$ \citep{zandieh2023kdeformer,han2024hyperattention}.\footnote{A third remarkable work \citep{alman2024fast} establishes upper and lower bounds for attention approximation but without a practical implementation.}  In this section, we design a new fast attention approximation based on sub-Gaussian thinning and derive guarantees that improve upon the prior art.


%
\subsection{Thinning attention in theory}
\begin{algorithm2e}[htb]
\caption{Thinformer}
\label{algo:thinformer}
\small{
  \KwIn{%
  Queries, keys, and values $(\query_i,\key_i,\val_i)_{i=1}^n$ in $\Rd$, $\nout$
  %
  }
  %
  %
  %
  \BlankLine
// Define key-value attention kernel\\[2pt]
    $\katt((\augkey,\augval),(\augkey',\augval')) \defeq \exp\big(\inner{\augkey}{\augkey'}\big)\inner{\augval}{\augval'}$  
    \BlankLine
    // Thin augmented key-value pairs using $\katt$ \\[2pt]
    %
    $v_{\max} \gets \displaystyle\max_{i\in[n]}\infnorm{\val_{i}}$;\ \  $(\augkey_i,\augval_i)_{i=1}^n \gets ({\key_i}{/d^{\quarter}},(\val_i, v_{\max}))_{i=1}^n$\\[2pt] %
    $\xout \gets \khcompresshalf(\xin=(\augkey_i,\augval_i)_{i=1}^n, \katt, \nout)$ \\
    
    %
    \BlankLine
    // Return exact attention on selected key-value subset 
    \BlankLine
    \KwRet{$\That \defeq \attention\big((\query_i)_{i=1}^n, \{(\key,\val): (\augkey,\augval)\in\xout\}\big)$}
    \BlankLine   
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
}
\end{algorithm2e}

\cref{algo:thinformer} summarizes our new \emph{Thinformer} module. 
At its heart is a new key-value attention kernel $\katt$ that mimics the special structure of the softmax matrix $\T$.
\cref{algo:thinformer} uses the attention kernel and a high-quality thinning algorithm, \khcompresshalf, to subselect key-value pairs and then computes exact attention \cref{def:att} for the key-value subset. 
In total, this requires only $O(d\,\nout^2)$ time to run \khcompresshalf and $O(d\,n\,\nout)$ time to compute $\attention$ with $n$ queries and $\nout$ key-value pairs. %
In contrast, computing the exact softmax matrix $\T$ with standard matrix multiplication requires $\Theta(d\,n^2)$ time.
Our next result, proved in \cref{proof:att-err}, shows that \cref{algo:thinformer} also admits a strong quality guarantee for approximating  $\T$.
%
\newcommand{\tablelineskip}{3.5mm}
\newcommand{\tabletopskip}{-4mm}
\begin{table}[tb]
    \centering
    %
    \caption{\tbf{Practical approximations with  guarantees.} %
    For each approximation $\That\in\reals^{n\times d}$ to the softmax matrix $\T$ \cref{def:att}, 
    %
    we report, up to a constant factor, the best  worst-case error guarantee for  $\maxnorm{\That-\T}$ given $O(d\,n^{1+a})$ running time
    and 
    $\gamma$-bounded \cref{eq:gamma-bounded} queries and keys. %
    %
    %
    %
    Here, the ratio ${\opnorm{\V}}{/\rownorm{\V}}$ lies in $[1,\sqrt{n}]$ and
    $\tau = 0.173+o(1)$.
    %
    \label{tab:att-assumptions-bound}}
    %
    %
     {
    \begin{tabular}{cc}
        \toprule
        \Centerstack{\bf Approximation}
         %
         & \Centerstack{\bf Guarantee}
         \\\midrule\\[\tabletopskip] 
         \Centerstack{\bf Thinformer} 
         %
         & %
         $\frac{n^{2\gamma}\sqrt{d\log(n \maxnorm{\V})}\log n}{n^{a}}\cdot\rownorm{\V}$ %
         \\[\tablelineskip]
         \Centerstack{\textbf{KDEformer}}%
         %
         %
         & %
         $\frac{n^{2\gamma+\frac{\tau}{2} (1 + \frac{\gamma}{2})}}{n^{a/2}}\cdot\specnorm{\V}$ %
         \\[\tablelineskip]
         %
         \Centerstack{\textbf{HyperAttention}}%
         %
         %
         %
         %
         %
         & $\frac{n^{\frac{17\gamma}{3}} (\log n)^{\frac{1}{6}}}{n^{a/6}}\cdot\specnorm{\V}$%
         %
         %
         %
         \\[2mm]
         %
         %
         %
         %
         %
         %
         %
         %
         %
         %
         %
         \bottomrule
    \end{tabular}%
    }       
\end{table}
\begin{table*}[t]
\caption{\tbf{Quality of T2T-ViT attention approximations on ImageNet.} 
We report mean Top-$1$ accuracy $\pm1$ standard deviation across five random seeds and mean forward pass runtime $\pm1$ standard deviation across $50$ batches of $64$ images.}%
    \label{tab:imagenet-acc-time}
    \centering
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %

    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %

    %
    \begin{tabular}{cccc}
\toprule
\textbf{Attention Algorithm} & \Centerstack{\bf Top-1 Accuracy (\%)} & \Centerstack{\bf Layer 1 Runtime (ms)} & \Centerstack{\bf Layer 2 Runtime (ms)} \\
\midrule
\Centerstack{\bf Exact} & 82.55 ± 0.00 & 18.48 ± 0.12 & 1.40 ± 0.01 \\[1mm]
\Centerstack{\bf Performer} & 80.56 ± 0.30 & 2.54 ± 0.01 & 0.60 ± 0.01 \\[1mm]
\Centerstack{\bf Reformer} & 81.47 ± 0.06 & 7.84 ± 0.03 & 1.53 ± 0.01 \\[1mm]
\Centerstack{\bf KDEformer} & 82.00 ± 0.07 & 5.39 ± 0.03 & 2.28 ± 0.03 \\[1mm]
\Centerstack{\bf Scatterbrain} & 82.05 ± 0.08 & 6.86 ± 0.02 & 1.55 ± 0.03 \\[1mm]
\Centerstack{\bf Thinformer (Ours)} & 82.18 ± 0.05 & 2.06 ± 0.01 & 0.54 ± 0.00 \\
\bottomrule
\end{tabular}
\end{table*}
%
\begin{theorem}[\tbf{Quality of Thinformer}]\label{att-err}
With probability at least $\half$, Thinformer (\cref{algo:thinformer}) yields   
    %
    %
\begin{talign}
&\maxnorm{\That - \T} 
    \leq \!
\frac{c\exp(\frac{2R^2}{\sqrt{d}})\rownorm{\V}\sqrt{\log_2(\nout)\log({12\nout \log_2\frac{\nin}{\nout}})}}{\nout}
\end{talign}
for $c\defeq \frac{128}{\sqrt{3}}\sqrt{(d+1)\log(3e^2(\frac{R^2}{\sqrt{d}} + 2)\maxnorm{\V})}
    +
\sqrt{\log(8)}(4+\frac{128}{\sqrt{3}})$
and $R\defeq\max_{i\in[n]}\max(\twonorm{\key_i},\twonorm{\query_i})$.
\end{theorem}
%
To put this result into context, let us compare with the existing guarantees for practical attention approximation, 
%
summarized in \cref{tab:att-assumptions-bound}.
%
%
Under the $\gamma$-boundedness assumption, 
\begin{talign}\label{eq:gamma-bounded}
\max_{i\in[n]} \max(\twonorm{\key_i}^2,\twonorm{\query_i}^2) \leq \gamma \sqrt{d} \log n, 
\end{talign}
%
the KDEformer approximation $\Tkde$ \citep[Cor.~3.6]{zandieh2023kdeformer} with $\tau = 0.173+o(1)$, 
the HyperAttention approximation $\Thyp$  \citep[Thm.~1]{han2024hyperattention} with no masking, 
and the Thinformer approximation $\Tthin$  
guarantee 
%
%
%
\begin{talign}
\maxnorm{\Tkde-\T} 
    &\leq
O(\frac{n^{2\gamma+\frac{\tau}{2} (1 + \frac{\gamma}{2})}}{n^{a/2}}\cdot\specnorm{\V})
    \\
\maxnorm{\Thyp-\T} &\leq O\big(\frac{n^{\frac{17\gamma}{3}} (\log n)^{\frac{1}{6}}}{n^{a/6}}\cdot\specnorm{\V}\big)\\
\maxnorm{\Tthin-\T} &\leq O\big(\frac{n^{2\gamma}\sqrt{d\log(n \maxnorm{\V})}\log n}{n^{a}}\cdot\rownorm{\V})
\end{talign}
with $O(dn^{1+a})$ runtime and probability at least $\half$.
The Thinformer guarantee exhibits four improvements over its predecessors. 
First, it establishes a significantly faster error decay rate ($n^{-a}$ versus $n^{-a/2}$ or $n^{-a/6}$) for a given subquadratic runtime $n^{1+a}$. 
%
%
%
%
Second, it reduces the  dependence on the error inflation factor $\gamma$. 
%
Third, like the HyperAttention guarantee, it eliminates all dependence on the KDEformer penalty parameter $\tau$. 
Finally, it reduces  dependence on the value matrix by a factor of $\frac{\opnorm{\V}}{\rownorm{\V}} \in [1, \sqrt{n}]$.  

%
%
%
%

Put otherwise, with bounded $\rownorm{\V}$, $\Tthin$ can provide consistent (i.e., $\maxnorm{\Tthin-\T}\to0$ as $n\to\infty$) subquadratic estimation  whenever $\gamma$ is bounded away from $1/2$ and guarantee, for example, $O(\frac{1}{\sqrt{n}})$ error in $\Otilde(d n^{\frac{3}{2}+2\gamma})$ time. 
In contrast, the $\Tkde$ and $\Thyp$ bounds require quadratic runtime to guarantee $O(\frac{1}{\sqrt{n}})$ error in the best case (${\opnorm{\V}}=O(1)$) and cannot guarantee consistent subquadratic estimation in the worst case (${\opnorm{\V}}=\Omega(\sqrt{n})$). 
%


%
\subsection{Thinning attention in practice}\label{sec:att-experiment}
To gauge the practical effectiveness of \cref{algo:thinformer}, we recreate the benchmark Tokens-To-Token Vision Transformer (T2T-ViT) experiment of \citet{zandieh2023kdeformer}. 
In this experiment, attention approximations are scored on their ImageNet classification accuracy and computational expense when used as drop-in replacements for the two most expensive attention layers in a pretrained T2T-ViT neural network \citep{yuan2021tokens}. 
Using the exact implementations and settings provided by \citet{zandieh2023kdeformer}, we benchmark our PyTorch implementation of Thinformer against exact attention and four leading attention approximations: Performer \citep{choromanski2021rethinking}, Reformer \citep{kitaev2020reformer}, ScatterBrain \citep{chen2021scatterbrain}, and KDEformer.
In \cref{tab:imagenet-acc-time}, we find that Thinformer provides the highest Top-$1$ accuracy on the ImageNet 2012 validation set \citep{ILSVRC15}, while running faster than all of the alternatives. 
%
The final attention call of Thinformer can also be combined with optimized attention implementations like FlashAttention~\citep{dao2022flashattention,dao2024flashattention} to further reduce the time and memory footprint.
%
%
We provide PyTorch code replicating this experiment at  \url{https://github.com/microsoft/thinformer} and supplementary experiment details in  \cref{app:attention_details}.
%

%
%
%

%

%
%

%
%
%
%

%

%

%
%
%
%

%

%
%
%
%
%
%

%

%

%
%
%

%


%
%


%

%
%
%
%
%

%

%
    
%
%
%
%
%
%


%

%

%
%
%
%
%
%
%
%

%
%
%
%
%

%
%
%

%

%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%

%

%
%
%

%

%
 
%
%
%

%






%
%
%
%
%
%
%
%
%
%
%
%
%
%