\section{Cheap Two-Sample Testing}\label{sec:ctt}
%
\newcommand{\mout}{m_{\textup{out}}}
\newcommand{\dembd}{d_{\mrm{embd}}}
\newcommand{\eventbin}[1][b]{\mc E_{#1}}
\newcommand{\distX}{\P}
\newcommand{\distY}{\Q}%
\newcommand{\inflation}{\mbb R}
\newcommand{\distXin}{\distX_{\textup{in}}}
\newcommand{\distYin}{\distY_{\textup{in}}}
\newcommand{\xs}[1][]{\mc{X}_{#1}}
\newcommand{\ys}[1][]{\mc{Y}_{#1}}
\newcommand{\zs}[1][]{\mc{Z}_{#1}}
A core task in statistics and machine learning is to determine whether two datasets are drawn from the same underlying distribution.  In this two-sample testing problem, we observe independent samples $\xs\defeq(\x_i)_{i=1}^m$ and $\ys\defeq(\y_j)_{j=1}^n$ from the unknown distributions $\distX$ and $\distY$ respectively, and we seek to accept or reject the null hypothesis that $\distX = \distY$.  
%
%
Standard kernel MMD tests 
tackle this task 
%
by computing the empirical  MMD
\begin{talign}
\mmd_{\k}(\distXin, \distYin)
    \stext{for}
\distXin,\distYin
    \defeq 
\frac{1}{m}\sum_{\x\in \xs}\!\dirac_{\x},
\frac{1}{n}\sum_{\y\in \ys}\!\dirac_{\y}
\end{talign}
for an appropriate kernel $\k$ 
and rejecting the null hypothesis whenever $\mmd_{\k}(\distXin, \distYin)$ is sufficiently large \citep{gretton2012kernel}.
Such tests are prized both for their broad applicability and for their high discriminating \emph{power}, that is, their probability of rejecting the null when $\P\neq\Q$. 
A standard way to summarize the power properties of a test is through its \emph{detectable separation rate}.
%
%
%
\begin{definition}[Detectable separation rate]\label{def:separation-rate}
%
We say a two-sample test has \emph{detectable separation rate} $\eps_{\kernel,m,n}$ if, for any detection probability $1-\beta\in (0,1)$, there exists a constant $c_{\k,\beta}>0$ such that the test has power at least $1-\beta$ of rejecting the null whenever $\mmd_\k(\distX,\distY) \geq c_{\k,\beta} \cdot \eps_{\kernel,m,n}$.
\end{definition}
%
Standard MMD tests can detect distributional differences on the order of 
$\eps_{\kernel,m,n} = \frac{1}{\sqrt{\min(m,n)}}$ 
\citep[Cor.~9]{gretton2012kernel},  
and this detectable separation rate is known to be the best possible for MMD tests \citep[Prop.~2]{domingoenrich2023compresstestpowerfulkernel} 
and minimax optimal for translation invariant kernels \citep[Thm.~8]{kim2023differentially}.
However, standard MMD tests also suffer from the $\Theta((m+n)^2)$ time burden of computing the empirical MMD.
Recently, \citet{domingoenrich2023compresstestpowerfulkernel} showed that one can improve scalability while preserving power by compressing $\distXin$ and $\distYin$ using a high-quality thinning algorithm.  However, their analysis applies only to a restricted class of distributions and kernels and exhibits a pessimistic dimension dependence on $\reals^d$.  Here, we offer a new analysis of their Compress Then Test approach that applies to any bounded kernel on any domain and, as an application, develop the first non-asymptotic power guarantees for testing with learned deep neural network kernels.

%
%
%
%
%

%
%
%
%
%

%
%
%

%
\subsection{Low-rank analysis of Compress Then Test}
\begin{algorithm2e}[h]
\caption{\cttname (\ctt)}%
\label{algo:ctt}
\SetAlgoLined
  \DontPrintSemicolon
\small{
  \KwIn{Samples ($\xs$, $\ys$),  
  \# coresets $\sblock$,
  \osname $\ossymb$,
  kernel $\k$,
  failure probability~$\delta$,  \# replicates $\numperm$, level $\alpha$} 
  \BlankLine
  Partition $\xs$ into $\sblock_m =  \frac{\sblock m}{m+n}$ equal-sized bins 
  $ ( \xs^{(i)} )_{i=1}^{\sblock_m}$ \\
  Partition $\ys$ into $\sblock_n =  \frac{\sblock n}{m+n}$ equal-sized bins 
  $ ( \ys^{(i)} )_{i=1}^{\sblock_n}$ \\
  
  \BlankLine
  // Identify coreset of size
  $\nout=2^\ossymb\sqrt{\frac{m+n}{\sblock}}$
  for each bin\\
  %
  %
  \lFor{$i=1, \dots, \sblock_m$}
  {$\distX^{(i)}_{\tout} \leftarrow \ktcompressd( \xs^{(i)}, \ossymb, \k)$}
  \lFor{$i=1, \dots, \sblock_n$}{$\distY^{(i)}_{\tout} \leftarrow \ktcompressd( \ys^{(i)}, \ossymb, \k)$}

    \BlankLine
    // Compute \tmmd test statistic \\

\makebox[\linewidth]{$M_{\numperm+1} \gets 
\mmd_{\k}(\frac{1}{s_m}\sum_{i=1}^{s_m}\distX^{(i)}_{\tout},\frac{1}{s_n}\sum_{i=1}^{s_n}\distY^{(i)}_{\tout})$ 
%
\hfill\refstepcounter{equation}\llap{(\theequation)} \label{tmmd}} \\[.2\baselineskip]
%
    \BlankLine
    // Simulate null by randomly permuting the $\sblock$ coresets $\numperm$ times \\ %
    \For{$b=1,\dots,\numperm$}
    {
    %
    $(\distX^{(i)}_{\tout,b})_{i=1}^{s_m}, (\distY^{(i)}_{\tout,b})_{i=1}^{s_n}\gets \textsc{Permute}((\distX^{(i)}_{\tout})_{i=1}^{s_m}, (\distY^{(i)}_{\tout})_{i=1}^{s_n})$
    
    $M_b \!\gets\! 
    \mmd_{\k}(\frac{1}{s_m}\sum_{i=1}^{s_m}\distX^{(i)}_{\tout,b},\frac{1}{s_n}\sum_{i=1}^{s_n}\distY^{(i)}_{\tout,b})$
    %
    %
    }
    
    \BlankLine
    // Threshold test statistic\\
    $R \gets$ position of $M_{\numperm+1}$ in an increasing ordering of $(M_b)_{b=1}^{\numperm+1}$ with ties broken uniformly at random
    
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    \textbf{return Reject} with prob.\  
        $\min(1, \max(0,R-(1-\alpha)(\numperm+1)))$}%
    %
\end{algorithm2e}

\cref{algo:ctt} details the Compress Then Test (\ctt) approach of \citet[Alg.~1]{domingoenrich2023compresstestpowerfulkernel}. %
Given a coreset count $\sblock\geq 2$, a \osname $\ossymb\ge0$, and a nominal level $\alpha\in(0,1)$, \ctt divides $\xs$ and $\ys$ into datapoint bins of size $\nin\defeq \frac{m+n}{\sblock}$, 
thins each bin down to size $\nout \defeq 2^\ossymb \sqrt{\nin}$ using \ktcompressd (a refinement of \khcompressd detailed in \cref{app:ktcompress}), and uses the thinned coresets to cheaply approximate 
$\mmd_{\k}(\distXin, \distYin)$ and permuted versions thereof. 
%
\citet[(8)]{domingoenrich2023compresstestpowerfulkernel} showed that the total runtime of \ctt is dominated by 
\begin{talign}
    \bigO{4^{\ossymb} (m+n) (\sblock + \log_4\parenth{\frac{m+n}{\sblock} - \ossymb})},
\end{talign}
kernel evaluations, yielding a near-linear $\bigO{(m+n)\log^c(m+n)}$ time algorithm whenever  $\sblock=\bigO{\log_4(m+n)}$ and $\ossymb \leq c \log_4 \log(m+n)$.
Moreover, Prop.~1 of \citet{domingoenrich2023compresstestpowerfulkernel} ensures that \ctt has probability at most $\alpha$ of falsely rejecting the null hypothesis. 

Our next, complementary result shows that \ctt 
%
%
also matches the detectable separation rate of standard MMD tests up to an inflation factor $\error/2^\ossymb$ depending on the \osname $\ossymb$.
%
%
%
%
%
%
%

\begin{theorem}[\tbf{Low-rank analysis of \ctt power}]\label{thm:ctt_power}
Suppose the parameters of \ctt (\cref{algo:ctt}) satisfy $m\leq n$,
\begin{talign}
\sblock_m \geq \frac{32}{9} \log(\frac{2e}{\gamma}), 
\qtext{and}
\delta= \min(\frac{\wtil \beta}{6}, (\frac{\wtil \beta}{2})^{1/\floor{\alpha(\numperm+1)}} \frac{\alpha}{30 e \sblock})
\end{talign}
%
%
%
for $\wtil \beta \defeq \frac{\beta}{1+\beta/2}$
and 
$\gamma \defeq \frac{\alpha}{4e} (\frac{\Tilde \beta}{4})^{1/\floor{\alpha(\numperm+1)}}$. 
Then \ctt has detectable separation rate (\cref{def:separation-rate}) 
\begin{talign}
\eps_{\kernel,m,n} = (1 +\error/2^\ossymb)/ \sqrt m,
\end{talign}
%
where $\error^2$ denotes the $(1-\frac{\wtil\beta}{20\sblock_n})$-th quantile of 
%
\begin{align}\label{eq:error-inflation-factor-simplified}
%
    &\textstyle\errorhat^2 \defeq \log(\frac{m+n}{s}) \log(\frac{n}{\wtil\beta})\, \cdot \\
    & \min_{r\leq 2\nout}\textstyle \bigl\{ \infnorm{\k} r \log(\frac{n}{\wtil\beta}) + (\lambda_{r+1}(\K)+\lambda_{r+1}(\K')) \nout\bigr\}.
\end{align}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
for
$\K\defeq(\k(\x_i,\x_j))_{i,j=1}^m$,  $\K'\defeq(\k(\y_i,\y_j))_{i,j=1}^n$, 
and 
$\infnorm{\k} \defeq \sup_{x,y\in \supp{\distX+\distY}}\abss{\k(x,y)}$. 

%
%
%
%

%
%
%
%
%
%


%
%
%
%
%
%

%
%
%
%
%
%
\end{theorem}
%

The proof in \cref{proof:ctt_power} combines the low-rank sub-Gaussian error bounds of \cref{thm:subg_low_rank_gen_kernel} with the generic compressed power analysis of \citet[App.~B.1]{domingoenrich2023compresstestpowerfulkernel} to yield power guarantees for  bounded kernels on any domain.
%
Notably, when $\rank{\K}$ and $\rank{\K'}$ are bounded or, more generally, $\polylog(n)$ one can choose the compression level $\ossymb=\Theta(\log_4\log(m+n))$ to exactly match the optimal quadratic-time detectable separation rates with a near-linear time \ctt test.
Moreover, the inflation factors remain well-controlled whenever the induced kernel matrices exhibit rapid eigenvalue decay. 

As a concrete example, consider the learned deep neural network kernel of 
\citet{liu2020learning},
\newcommand{\kdeep}{\k_{\textup{deep}}}
\begin{talign}\label{eq:deep_kernel}
    \kdeep(\x,\y) \defeq \brackets{(1-\epsilon) \kappa(\phi(\x),\phi(\y)) + \epsilon} q(\x,\y),
\end{talign}
where $\phi: \reals^d \to \reals^{\dembd}$ is a pretrained neural network, 
$q$ and $\kappa$ are $\textsc{Gauss}(\eta)$ 
%
kernels \cref{eq:gaussian_kernel} on $\Rd$ and $\reals^{\dembd}$ respectively, 
and $\eps\in(0,1)$.
%
%
%
%
This deep kernel generates full-rank kernel matrices \citep[Prop.~5]{liu2020learning} but
%
induces exponential eigenvalue decay due to its decomposition as a mixture of Gaussian kernels. 
Hence, as we show in \cref{proof:ctt_power_deep_kernel}, 
\ctt 
with $\kdeep$,  
$\ossymb=\Theta(\log_4\log(m+n))$, and sub-Gaussian inputs matches the detection quality of a quadratic-time MMD test in near-linear time.

%
 %
%
%
%
%

\begin{corollary}[\tbf{Power of deep kernel \ctt}]\label{cor:ctt_power_deep_kernel}
Instantiate the assumptions of \cref{thm:ctt_power} with $\k=\kdeep$ \cref{eq:deep_kernel}.
If the inputs $(\phi(\x_1), \x_1,\phi(\y_1), \y_1)$ are \emph{sub-Gaussian}, that is, 
\begin{talign}\label{eq:subexp-dist}
\E[e^{c\statictwonorm{(\phi(\x_1), \x_1,\phi(\y_1), \y_1)}^2}]<\infty
\end{talign}
for some $c>0$, %
then \ctt satisfies the conclusions of \cref{thm:ctt_power} with $d'\defeq \dembd + d$ and 
\begin{talign}
%
\error[\kdeep]
=
O(\log^{\frac{d'}{2}+\frac{3}{2}}(\frac{n}{\wtil \beta})).
\end{talign}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
\end{corollary}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
Moreover, when the input and neural features lie on smooth compact manifolds \citep[as, e.g., in][]{zhu2018ldmnet}, the error inflation of \ctt adapts to the smaller intrinsic manifold dimension, enabling an improved trade-off between runtime and detection power. See \cref{proof:ctt_power_deep_kernel_manifold} for our proof.
%
%

\begin{corollary}[\tbf{Power of deep manifold kernel \ctt}]
\label{cor:ctt_power_deep_kernel_manifold}
Under the assumptions of \cref{cor:ctt_power_deep_kernel}, if $\x_1$, $\y_1$, $(\x_1,\phi(\x_1))$,  and $(\y_1,\phi(\y_1))$ belong to smooth compact manifolds (\cref{assum:manifold}) with dimension $d^\star <d'$ then 
\ctt satisfies the conclusions of \cref{thm:ctt_power} with 
\begin{talign}
%
\error[\kdeep]
=
O(\log^{\frac{5d^\star}{4} + \frac{3}{2}}(\frac{n}{\wtil \beta})).
\end{talign}
%
%
%
%
%
%
%
%
%
%
%
%
\end{corollary}
\cref{cor:ctt_power_deep_kernel,cor:ctt_power_deep_kernel_manifold} follow from explicitly bounding the eigenvalues of the generated deep kernel matrices as in \cref{eq:gsn_lambda_ball,eq:gsn_lambda_manifold}. 
One could alternatively bound the compression error of \ktcompressd using the covering number approach of \citet[Thm.~2, Prop.~3]{dwivedi2021generalized}. 
In the setting of \cref{cor:ctt_power_deep_kernel}, the argument of \cref{proof:ctt_power_deep_kernel} combined with this distinct analysis would yield an alternative error inflation factor $\terror[\kdeep]/2^{\ossymb}$ with worse dimension dependence,
\begin{talign}
\terror[\kdeep]
=
\Theta(\log^{\frac{3d'}{4}+2}(\frac{n}{\wtil \beta})),
\end{talign}
and without known adaptivity to an intrinsic manifold dimension.

%
\subsection{Powerful deep kernel testing in near-linear time} %
\label{sub:ctt_experiments}

\begin{figure}[t!]%
    \begin{center}
        \includegraphics[width=\linewidth]{figures/power.pdf}
    \end{center}
    
    \caption{\tbf{Time-power trade-off curves for detecting Higgs bosons with deep kernel MMD tests.} We plot mean values $\pm 1$ standard error across $1000$ independent trials with level $\alpha=0.05$ and  $\numperm=100$ permutations.}
    
    \label{fig:tst-power-runtime}
\end{figure}

%

%


%
%

%
%

To evaluate the practical utility of deep kernel \ctt, we follow the Higgs mixture experiment of \citet[Sec.~5]{domingoenrich2023compresstestpowerfulkernel} and use the deep kernel training procedure of \citet[Tab.~1]{liu2020learning}.
%
Here, the aim is to distinguish a Higgs boson signal process $\P$ from a background process $\Q$ given $m=n=16384$ observations,  $d=2$ particle-detector features, and a five-layer fully-connected neural network $\phi$ with softplus activations and embedding dimension $\dembd = 20$.

\cref{fig:tst-power-runtime} compares the time-power trade-off curves induced by three fast kernel testing approaches to this problem: \subsampling, a standard wild-bootstrap MMD test \citep{chwialkowski2014wild} that simply evaluates empirical $\mmd_{\kdeep}$ using $\nout=m_{\tout}$ uniformly subsampled points; \textsc{W-Block}, a wild-bootstrap test that averages $\frac{n}{B}$ subsampled squared $\mmd_{\kdeep}$  estimates based on $\nout=m_{\tout} =B$ points \citep{zaremba2013b}; and \ctt with $\sblock=32$ bins and varying $\ossymb$.
We find that the \ctt curve uniformly dominates that of the alternative methods and matches the power of an exact MMD test (\subsampling with $\nout=n$) in a fraction of the time. 
%
See \url{https://github.com/microsoft/deepctt} for PyTorch code replicating this experiment and \cref{app:testing_details} 
for supplementary experiment details.