%%%%%%%% ICML 2025 ExAMPLE LATEx SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
% \usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
% \newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage{algpseudocode}
% \usepackage[noalgorithmic]{algorithm}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% \usepackage{algorithm}
% \usepackage[noend]{algorithmic}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

\include{configs}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\theoremstyle{definition}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\usepackage{notation}
\usepackage{researchpack}

\newcommand{\guy}[1]{\textcolor{blue}{\textbf{[Guy: #1]}}}
\newcommand{\anji}[1]{\textcolor{purple}{\textbf{[Anji: #1]}}}
\newcommand{\xuejie}[1]{\textcolor{orange}{\textbf{[Xuejie: #1]}}}
\newcommand{\yitao}[1]{\textcolor{red}{\textbf{[Yitao: #1]}}}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Tractable Transformers}

\begin{document}

\twocolumn[
\icmltitle{Tractable Transformers for Flexible Conditional Generation}

% It is OKAY to include author information, even for blind
% submissions: the style ufile will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Anji Liu}{equal,ucla,stuttgart}
\icmlauthor{Xuejie Liu}{equal,peking,pku}
\icmlauthor{Dayuan Zhao}{yuanpei}
\icmlauthor{Mathias Niepert}{stuttgart}
\icmlauthor{Yitao Liang}{peking}
\icmlauthor{Guy Van den Broeck}{ucla}
\end{icmlauthorlist}

\icmlaffiliation{ucla}{Department of Computer Science, University of California, Los Angeles}
\icmlaffiliation{peking}{Institute for Artificial Intelligence, Peking University}
\icmlaffiliation{yuanpei}{Yuanpei College, Peking University}
\icmlaffiliation{stuttgart}{Institute for Artificial Intelligence, University of Stuttgart}
\icmlaffiliation{pku}{School of Intelligence Science and Technology, Peking University}

\icmlcorrespondingauthor{Anji Liu}{liuanji@cs.ucla.edu}
\icmlcorrespondingauthor{Yitao Liang}{yitaol@pku.edu.cn}
\icmlcorrespondingauthor{Guy Van den Broeck}{guyvdb@cs.ucla.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.
\begin{abstract}
Non-autoregressive (NAR) generative models are valuable because they can handle diverse conditional generation tasks in a more principled way than their autoregressive (AR) counterparts, which are constrained by sequential dependency requirements. Recent advancements in NAR models, such as diffusion language models, have demonstrated superior performance in unconditional generation compared to AR models (e.g., GPTs) of similar sizes. However, such improvements do not always lead to improved conditional generation performance. We show that a key reason for this gap is the difficulty in \emph{generalizing to conditional probability queries unseen during training}. As a result, strong unconditional generation performance does not guarantee high-quality conditional generation. This paper proposes Tractable Transformers (Tracformer), a Transformer-based generative model that is more robust to different conditional generation tasks. Unlike existing models that rely solely on global contextual features derived from full inputs, Tracformers incorporate a sparse Transformer encoder to capture both local and global contextual information. This information is routed through a decoder for conditional generation. Empirical results demonstrate that Tracformers achieve state-of-the-art conditional generation performance on text modeling compared to recent diffusion and AR model baselines.
\end{abstract}

\section{Introduction}

Generative AI has emerged as a transformative paradigm for solving machine learning tasks. Its core strength lies in the ability of modern deep generative models to learn complex and high-dimensional data distributions. Autoregressive (AR) models such as GPTs \citep{brown2020language} are among the most well-developed generative models, demonstrating exceptional performance in modeling discrete data such as language \citep{dubey2024llama,reid2024gemini} and protein sequences \citep{shin2021protein,trinquier2021efficient}. However, despite their expressiveness and scalability, AR models are not best suited for many conditional generation tasks such as DNA imputation and protein sequence infilling \citep{TransformerHLAImputation,hawkins2023getting,alamdari2023protein} due to their inherent (autoregressive) sequential dependencies \citep{kaddour2023challenges}. 
% \guy{this is too strong of a claim; tons of production models for code completion are using AR models for text infilling. They just train for the task and put both prefix and suffix into the prompt.} \xuejie{have added DNA/protein task citation}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{Figs/fig-ar-cond-ppl.pdf}
    \vspace{-1.6em}
    \caption{Zero-shot conditional perplexity ($\downarrow$) of AR (GPT-2 \citep{radford2019language}) and NAR models (SEDD \citep{lou2023discrete}, MDLM \citep{sahoo2024simple}) models conditioned on different proportions of prefix given sequences of length 128. Existing NAR models perform worse than GPT-2 despite having comparable unconditional perplexity on sequences of length 1024. Their performance gets worse when provided with more context.
    % \guy{pink dots are very hard to see}
    % \guy{why does SEDD on wiki and SEDD + MDLM on 1BW not start at the same point as GPT-2 for 0 percent prefix provided?}
    }
    \label{fig:ar-cond-ppl}
    \vspace{-1.6em}
\end{figure}

In contrast, non-autoregressive (NAR) generative models are inherently more flexible, as they can condition on arbitrary contexts. This flexibility makes them particularly well-suited for conditional generation tasks, such as code editing \citep{liu2024non} and DNA imputation \citep{stark2024dirichlet,li2024discdiff,dasilva2024dna}. 
% \xuejie{have added DNA diffusion citation} 
Recent advances in discrete diffusion \citep{lou2023discrete,shi2024simplified} and semi-autoregressive models \citep{chen2024diffusion} have further improved the performance of NAR models across various domains. Notably, SoTA diffusion language models outperform GPT-2 in terms of unconditional perplexity on several benchmarks \citep{sahoo2024simple}.

However, we observe a significant performance drop when using SoTA diffusion language models for conditional generation tasks. As shown in \cref{fig:ar-cond-ppl}, we evaluate the conditional perplexity (\ie the perplexity of the corresponding conditional likelihood) of two SoTA diffusion language models, SEDD \citep{lou2023discrete} and MDLM \citep{sahoo2024simple} by providing different fractions of the prefix of length-128 sequences from WikiText103 \citep{merity2022pointer} and 1BW \citep{chelba2013one}. Despite both models having comparable or better unconditional perplexity than GPT-2 on length-1024 sequences, their conditional perplexity is significantly worse. Moreover, their performance degrades further as the length of the provided prefix context increases.

We attribute this performance discrepancy to the inability of existing NAR models to generalize effectively to conditional queries unseen during training. To address this issue, we propose \textbf{Tractable Transformers (Tracformers)}, an NAR generation model designed to handle diverse conditional generation tasks more robustly. A key insight is to learn local features that enhance generalization across different queries. Specifically, unlike existing models that rely solely on global features from all input tokens, Tracformers use a novel sparse encoder to learn features at multiple context levels, which are then processed by a decoder for conditional generation. As shown in \cref{fig:ar-cond-ppl}, Tracformer achieves better conditional perplexity than other NAR models.

Empirical results on text generation show that Tracformer achieves consistently better conditional generation performance compared to existing architectures such as BERT \citep{devlin2018bert} and BART \citep{lewis2020bart}. Further, Tracformer beats state-of-the-art (SoTA) diffusion language models on zero-shot conditional generation tasks, which are conditional variants of the zero-shot perplexity tasks used to evaluate GPT-2 \citep{radford2019language}. In addition to proposing an NAR model architecture, we emphasize the importance of directly evaluating the conditional generation performance of NAR models.

% \guy{another benefit of AR is that they can be scaled up much more efficiently than NAR. There are no billion+ parameter NAR models because you need kv-caching? I guess it's best not to talk about this?}

\section{Background}

In this section, we first introduce key concepts of sequence modeling and the distinctions between autoregressive (AR) and non-autoregressive (NAR) approaches (Sec.~\ref{sec:seq_modeling}). We then describe the Transformer architecture, which serves as the backbone of many modern sequence modeling frameworks, highlighting its key components and their roles in both AR and NAR paradigms (Sec.~\ref{sec:transformers}). We use uppercase letters (e.g., \(X_t\)) to represent random variables and lowercase letters (e.g., \(x_t\)) for their assignments.

\subsection{Sequence Modeling}
\label{sec:seq_modeling}

Given a sequence of $T$ categorical variables $\X:= \{X_t\}_{t=1}^{T}$, sequence modeling aims to capture their joint distribution. Autoregressive (AR) modeling achieves this by factorizing the joint probability of $\x$ using the chain rule of probability:
\begin{align}
    \!\!\!\! \mathrm{Pr}(\x) \!=\!  \mathrm{Pr}(x_1)\cdot  \mathrm{Pr}(x_2|x_1) \cdots \mathrm{Pr}(x_T | x_1, \dots, x_{T-1}).
    \label{eq:ar}
\end{align}
By learning each of the $T$ conditional distributions with a neural network, AR models such as GPT \citep{radford2019language, brown2020language, openai2023chatgpt} and State Space Models \citep{gu2023mamba,fu2022hungry} achieved state-of-the-art performance in modeling high-dimensional sequences like text and protein \citep{nguyen2024hyenadna}.

% This factorization simplifies the modeling process by using a sequence of conditional distributions to capture token dependencies (i.e., next-token prediction), thereby enabling efficient and scalable training. Consequently, AR models, such as GPT \citep{radford2019language, brown2020language, openai2023chatgpt}, have become the foundation of tasks like text generation. 

However, AR modeling requires the context to be provided contiguously at the beginning, limiting its applicability for tasks that demand more flexible context handling, such as DNA imputation and protein sequence infilling. 

% \guy{I think protein sequence infilling is the only real-world problem here. The infilling task is a little artificial because it assumes you know exactly how many tokens are missing in the middle.}\xuejie{fixed}

In contrast, non-autoregressive (NAR) modeling seeks to learn conditional distributions given arbitrary contexts.  Formally, for a subset of context variables $\X_C$ with $C \!\subset\! [T]$, NAR models aim to encode the conditional distribution $\Pr (\X_{R} \given \x_{C})$ of the remaining variables $\X_R$, where $R \!:=\! [T] \backslash C$. Here $[T]$ denotes the set of positive integers up to $T$. Compared to AR models, the flexibility of NAR models makes them adaptable to a broader range of conditional generation tasks \citep{li2022diffusion,han2023ssd}.

% let $\X_C$ represent an arbitrary context variable set associated with a given task, where $C \subset [T]$, and let $\X_R$ denote the target variable set, defined as $R:= [T] \backslash C$, the complement of $C$. \anji{We need to define $[\cdot]$. Maybe at the end of the first paragraph of this section.} NAR models aim to estimate $\Pr(\x_R \given \x_C)$, relaxing the strict sequential dependencies inherent in AR models. \anji{Dependencies in a distribution are independent of how we represent it. An autoregressive factorization can still encode any dependency. The limitation is that we are restricted to querying the learned distribution in certain ways.} This relaxation allows NAR models to capture more flexible conditional dependencies, making them more adaptable for a broader range of conditioned generation tasks.

% However, these models are restricted to conditioning only on preceding tokens and generating text in a left-to-right manner, which limits their applicability to tasks requiring more flexible conditional generation beyond the constraints of \cref{eq:ar} 

% Many downstream tasks, such as code completion, text infilling, and dialogue repair \citep{li2020pal}, demand the ability to generate tokens conditioned on arbitrary contexts rather than strictly sequential dependencies. 
 
 
 % \anji{To define $\mathbf{X}_C$ formally, we should add a formal notation like $C \subset [T]$.} and $\mathbf{x}_R$ denotes the remaining variable set, where $\mathbf{x}_C \cap \mathbf{x}_R = \emptyset$ and $\mathbf{x}_C \cup \mathbf{x}_R = \mathbf{x}$ \anji{We can instead say: and $\mathbf{X}_{C}$ ($C := [T] \backslash R$) is its complement.}. 

\subsection{Transformer Models}
\label{sec:transformers}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{Figs/fig-nar-challenges.pdf}
    \vspace{-1.6em}
    \caption{\textbf{Different conditional queries do not respect the same joint distribution in NAR models.} (a) The log-likelihoods (LLs) of the sequence ``The cat is cute'' differ depending on whether SEDD is queried in the forward order or the reverse order. (b) Histogram of the difference between the highest and the lowest LLs of 2,000 length-5 sequences from WikiText103 when queried in all possible orders.}
    \label{fig:nar-challenges}
    \vspace{-0.6em}
\end{figure*}

The Transformer \cite{vaswani2017attention} architecture is commonly used to implement generative models for both AR and NAR generation. Given a sequence of tokens \(\{x_t\}_{t=1}^T\), the model initially maps each token $x_{t}$ into a continuous embedding $\h_t^0 \in \mathbb{R}^d$, where \(d\) denotes the dimensionality of the embedding space. The embeddings are then iteratively transformed through a stack of \(L\) Transformer blocks:
    \begin{align*}
        \forall l \in \{1, \dots, L\}, \quad \h_{1:T}^l = \texttt{Block}_l(\h_{1:T}^{l-1}),
    \end{align*}
\noindent where \(\h_t^l\) represents the embedding at position \(t\) in layer \(l\) and $\h_{1:T}^l := \{\h_t^l\}_{t=1}^{T}$ denotes all embeddings in the layer.

% Each Transformer block is composed of two key components: a multi-head self-attention mechanism and a feed-forward neural network (FFN) (we omit layer normalization for clarity). The transformation in each block can be expressed as:
% \begin{equation}
% \h_t^l = \text{MLP}\big(\text{Attn}(\h_{1:T}^{l-1})\big),
% \end{equation}
% The self-attention operations \text{Attn} play an important role in capturing contextual dependencies across the entire sequence, allowing the model to aggregate information dynamically and effectively. 
% To ensure adherence to appropriate token dependencies during training and inference, autoregressive (AR) models typically use a causal attention mask, which restricts each token to attending only to preceding tokens: \anji{I feel the following equation is not enough to explain what are causal attention masks. We can say at a high level how attention works: there is a score function applied to every pair of embeddings (\eg $s_{ij}^{l} := \mathrm{Score} (\h_{i}^{l-1}, \h_{j}^{l-1})$); then the scores are masked and normalized (softmax), then the output of Attn is $\sum_{j} s_{ij}^{l} \cdot f (h_{j}^{l-1})$, where $f$ is xxx. Here we can have some notations on the attention mask that can be referred to when describing the encoder.}
% \[
% \mathbf{h}_t^l = \text{MLP}\big(\text{Attn}(\mathbf{h}_{1:t-1}^{l-1})\big).
% \]
% In comparison, non-autoregressive (NAR) models do not modify the model structure but instead employ input masking, where missing tokens \(\mathbf{x}_R\) are replaced with special placeholder tokens (e.g., [MASK]). The attention mechanism remains unchanged, attending to all positions in the input sequence.

Omitting design details such as the use of layer normalization and residual connections, each Transformer block consists of two modules: a feed-forward neural network ($\texttt{FFN}$) and an attention module ($\texttt{Attn}$), which can be expressed as $\h_{1:T}^l = \texttt{FFN}_{l} \big ( \texttt{Attn}_{l} (\h_{1:T}^{l-1}) \big )$.

% For clarity, layer normalization and residual connections are omitted in this description. The transformation within each block can be expressed as:
% \begin{equation}
% \h_t^l = \text{MLP}\big(\text{Attn}(\h_{1:T}^{l-1})\big),
% \end{equation}

While the FFN is applied independently to embeddings at each position $t$, the attention module captures dependencies between different token variables. Specifically, a learnable function $\texttt{score}_{l}$ first computes a score value $s_{t,t'}^{l} \!:=\! \texttt{score}_{l} (\h_{t}^{l-1}, \h_{t'}^{l-1})$ for each pair of token positions $(t, t')$ using their respective embeddings. The scores are then normalized with the softmax function:
    \begin{align*}
        \forall t, t' \in \{1,\dots,T\}, \quad \hat{s}_{t,t'}^{l} = \frac{m_{t,t'}^{l} \cdot \exp (s_{t,t'}^{l})}{\sum_{t''} m_{t,t''}^{l} \cdot \exp (s_{t,t''}^{l})},
    \end{align*}
\noindent where $\bfm_{l} := \{m_{t,t'}^{l}\}_{t,t'} \in \{0,1\}^{T \times T}$ is a mask that specifies the allowed dependencies between token positions.

% The self-attention mechanism \(\text{Attn}\) is pivotal for capturing contextual dependencies across the entire sequence, enabling the model to aggregate information dynamically and effectively. Specifically, to compute attention, each input hidden state \(\h_t^{l-1}\) is first projected into query, key, and value representations:
% \[
% \q_t^{l-1} = W^q \h_t^{l-1}, \quad \kk_t^{l-1} = W^k \h_t^{l-1}, \quad \vv_t^{l-1} = W^v \h_t^{l-1},
% \]
% where \(W^q, W^k, W^v \in \mathbb{R}^{d \times d}\) are learnable weight matrices for the query, key, and value projections, respectively.

% Next, the attention scores are computed for every pair of tokens. To ensure numerical stability and improve gradient flow, the scores are scaled by \(\sqrt{d}\) additionally:
% \[
% \mathrm{score}(t, t') := \frac{\q_t^{l-1} \cdot \kk_{t'}^{l-1}}{\sqrt{d}}.
% \]


% The scores are then normalized using the softmax function, optionally incorporating an attention mask \(m_{tt'}\):
% \[
% \hat{\mathrm{score}} (t, t') = \frac{\exp(m_{tt'} \cdot \mathrm{score}(t, t'))}{\sum_{\hat{t}} \exp(m_{t\hat{t}} \cdot \mathrm{score}(t, \hat{t}))}.
% \]

The normalized scores $\{\hat{s}_{t,t'}^{l}\}_{t,t'}$ are then used to weigh the embeddings, with the weighted sum of these embeddings forming the final output of the attention module. Specifically, the output at position $t$ is computed as:
    \begin{align*}
        \texttt{Attn}_{l} (\h_{1:T}^{l-1})_{t} = \sum_{t'} \hat{s}_{t,t'}^{l} \cdot g_{l} (\h_{t'}^{l-1}),
    \end{align*}
\noindent where $g_{l} \!:\! \R^{d} \!\rightarrow\! \R^{d}$ is a learnable mapping. When implementing AR models with Transformers, causal attention masks are used to ensure each token only receives context from previous tokens:
    \begin{align}
        \forall l \in \{1, \dots, L\}, \quad m_{t,t'}^{l} = \begin{cases}
            1 & \text{if~} t' \leq t, \\
            0 & \text{otherwise}.
        \end{cases}
        \label{eq:ar-attn-mask}
    \end{align}
The output $\h_{t}^{L}$ of the final layer at position $t$ is transformed to predict the conditional distribution $\Pr (X_t \given \x_{1:t-1})$ (cf. Eq.~(\ref{eq:ar})) through a learnable mapping. 

In contrast, NAR modeling is commonly achieved by setting unobserved input tokens to a special placeholder token $\texttt{<MASK>}$, while configuring the attention mask to allow full visibility of all inputs \citep{devlin2018bert,lou2023discrete,sahoo2024simple}. Specifically, given contexts $\x_{C}$, all tokens in $\X_{R}$ with $R := [T] \backslash C$ are assigned the $\texttt{<MASK>}$ token, and the final hidden embedding $\h_{t}^{L}$ is used to capture $\Pr (X_t \given \x_{C})$ for every $t \in R$.

\section{Query Generalization in NAR Models}
\label{sec:query-generalization}

As shown in \cref{fig:ar-cond-ppl}, the strong unconditional generation performance of SoTA NAR models does not ensure high-quality conditional generation, as different conditional queries from the same model do not align with a consistent joint distribution. For example, for any text distribution, the log-likelihood (LL) of the sequence $\x_{1:4} := \text{``The~cat~is~cute''}$ should remain the same regardless of how we break down the joint probability into conditional probabilities following the chain rule. However, as shown in \cref{fig:nar-challenges}(a), the LLs computed with SEDD \citep{lou2023discrete} in the forward and the reverse order are different, which indicates that the model does not follow the same underlying joint distribution when prompted with different conditional queries. \cref{fig:nar-challenges}(b) further illustrates the prevalence of such inconsistency by measuring the gap between the highest and lowest LLs of 2,000 length-5 sequences from WikiText103 \citep{merity2022pointer}, evaluated across all possible orders.

% Due to the inconsistency between outputs of different conditional probability queries, NAR models need to generalize well to queries unseen in training.\guy{previous sentence does not make sense to me. It is saying that because of the lack of generalization, models need to generalize.} 
As discussed in \cref{sec:transformers}, tokens not given as evidence to NAR models are often represented by a special $\texttt{<MASK>}$ token. In such cases, generalization to new queries means generalizing to inputs with different $\texttt{<MASK>}$ distribution.

Existing models use different mask strategies during training, which means they are trained to predict certain types of conditional queries, \ie $\Pr (\cdot \given \x_{C})$ for specific subsets $C \!\subset\! [T]$. For instance, discrete diffusion models \citep{austin2021structured} and BERT \citep{devlin2018bert} sample $C$ uniformly at random while T5 \citep{raffel2020exploring} sample random spans of variables. However, since it is infeasible to cover all conditional queries during training, NAR models have to achieve robust query generalization to excel at conditional generation.

% However, we argue that unconditional likelihood/perplexity is insufficient to indicate the conditional generation performance of the model since the answers to different conditional queries \emph{do not respect the same underlying distribution}. For example, \anji{fig}

% State-of-the-art NAR models have demonstrated impressive performance in unconditional generation tasks, even out-performing AR models of similar size in terms of unconditional perplexity \citep{luo2021score,sahoo2024simple,shi2024simplified}. However, this success does not always extend to tasks that require effective conditional generation.

% We argue that a key cause of this performance discrepancy is the challenge NAR models face in generalizing to \emph{conditional probability queries unseen in training}. Specifically, NAR models such as discrete diffusion models \citep{austin2021structured,sun2022score} and semi-autoregressive models \citep{ghazvininejad2020semi,chen2024diffusion} are trained to predict certain types of conditional queries, \eg $\Pr (\cdot \given \x_{C})$ for specific subsets $C \subset [T]$. For example, discrete diffusion sample $C$ uniformly at random while BERT \citep{devlin2018bert} and T5 \citep{raffel2020exploring} sample random spans of variables. However, downstream tasks may ask the model to answer drastically different conditional queries. For instance, text infilling necessitates generating continuous chunks of text to complete missing sections, whereas code editing primarily involves making precise modifications to small fragments of code.

% This is a unique challenge in NAR modeling since in AR models, we always ask the model the same set of conditional probability queries (\ie the distribution of $X_{t}$ given the values of all previous variables $\x_{1:t-1}$).

% \anji{Add discussions on the teaser results...} We empirically validate the hypothesis by ...

% We argue that a key challenge lies in the model's capability to generalize from the training task to diverse generation tasks, including both unconditional generation and conditional generation with varying contexts and masking strategies.

% \anji{It is hard for the readers to grasp the goal of this paragraph.} \anji{We can start by saying, SoTA NAR models have very good unconditional generation performance, but that does not seem to lead to good downstream performance. Then refer to the teaser results to show what we found.} Unconditional generation involves producing a sequence of tokens from scratch without any prior context. In contrast, conditional generation is associated with a given context, where the nature of the context depends on the applied masking strategy. Common masking strategies include short-span masking and long-span masking. Short-span masking provides sparse and fragmented context with short masked spans. For instance, BERT employs a uniform random masking strategy, masking individual tokens independently, while T5 masks contiguous spans of tokens with an average length of 3 tokens. Long-span masking, on the other hand, involves a more continuous and coherent context, often spanning a substantial portion of the sequence (e.g., half the sequence length). Many downstream tasks, such as prompt-based text generation and text infilling, can be framed as long-span masked conditional generation tasks.


% our experiments show that discrete diffusion models struggle to generalize effectively across varying conditional generation tasks. Specifically, as shown in \xuejie{figure}, we evaluate the conditional perplexity of a SOTA discrete diffusion model under various masking strategies and observe a significant performance drop when transitioning from short-span masking to long-span masking. We hypothesize that this is because the uniform denoising strategy used during training primarily exposes the model to fragmented contexts, leading it to struggle with more continuous contexts during inference. More importantly, the current architectural design fails to generalize effectively across different masking types.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{Figs/fig-tgpt-overview.pdf}
    \vspace{-1.8em}
    \caption{\textbf{Tractable Transformers (Tracformers)} use an encoder to learn feature embeddings (\ie $\h_{t}^{\text{enc}, l}$) with context (denoted as scope on the left) sizes ranging from $1$ to $2^{L}$, where $L$ is the number of layers. The encoder features are then fed to a decoder for conditional generation. Each decoder layer contains a feed-forward neural network and a cross-attention layer to collect information from corresponsing encoder features. Attention layers in both the encoder and the decoder have special sparse patterns (see Secs.~\ref{sec:encoder}~and~\ref{sec:decoder}).}
    \label{fig:tgpt-overview}
    \vspace{-0.8em}
\end{figure*}

\section{Tractable Transformers}
\label{sec:tgpt}

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.9\linewidth]{Figs/stride_attn.png}
%     \caption{Multi-Granularity Self-Attention}
%     \label{fig:stride_attn}
% \end{figure}

The standard Transformer architecture introduced in \cref{sec:transformers} is highly expressive and scalable thanks to its attention modules that enable information transformation between arbitrary pairs of tokens. However, this global attention mechanism can hinder generalization to unseen conditional queries due to its sensitivity to changes in the distribution of the $\texttt{<MASK>}$ token. In contrast, restricting the context of features $\h_{t}^{l}$ to local neighborhoods (\ie makes $\h_{t}^{l}$ only depends on a small subset of variables) improves robustness across different conditional queries since local features are invariant to changes of mask tokens outside their respective context windows. However, using local features significantly restricts the model's expressiveness.

We propose an encoder-decoder architecture that effectively leverages both local and global features to achieve robustness against query changes while preserving the expressiveness of global context modeling. As shown in \cref{fig:tgpt-overview}, the encoder learns feature embeddings with exponentially increasing context lengths in different layers by a sparse attention mechanism. The encoder embeddings are then fed to a cross-attention-only decoder Transformer, which independently predicts the conditional probability of each token. In the following, we introduce the backbone structure of the encoder (Sec.~\ref{sec:encoder}) and the decoder (Sec.~\ref{sec:decoder}).

\subsection{Multi-Scope Encoder}
\label{sec:encoder}

Following \cref{sec:transformers}, we define $\h_{t}^{\text{enc}, l}$ as the feature embeddings at position $t$ of the $l$-th encoder layer (Fig.~\ref{fig:tgpt-overview}). Define the \emph{variable scope} (or \emph{scope}) $\phi_{t}^{l} \subseteq [T]$ of $\h_{t}^{\text{enc}, l}$ as the set of variables that contribute to the computation of $\h_{t}^{\text{enc}, l}$. For example, with full attention, the scope of every embedding spans the entire input sequence: $\forall t \text{~and~} l, \phi_{t}^{l} = [T]$; when using causal attention masks in \cref{eq:ar-attn-mask}, we have $\phi_{t}^{l} = [t]$ for every $l$ and every $t$.

The encoder layers implement a Multi-Scope Self-Attention (MSSA) mechanism such that the size of the variable scopes (\ie $\abs{\phi_{t}^{l}}$) grows exponentially with the layer index $l$. This enables earlier layers to capture fine-grained local semantics, while later layers encode broader contextual information and more abstract representations. \cref{fig:tgpt-overview} provides an example multi-scope encoder with base $2$. Specifically, the scope of each embedding $\h_{t}^{\text{enc},l}$ is the set of variables whose distance from the left of $X_t$ is smaller than $2^{l}$:
    \begin{align}
        \phi_{t}^{l} = \{t' : t' \geq 1, 0 \leq t - t' < 2^{l}\}.
        \label{eq:example-scope}
    \end{align}
MSSAs can be implemented by applying sparse attention masks to a standard attention module as introduced in \cref{sec:transformers}. Specifically, the base-2 scope pattern in \cref{fig:tgpt-overview} can be achieved using the following attention masks for each $l \in [L]$, and $t,t' \in [T]$:
    \begin{align}
        \quad m_{t,t'}^{\text{enc}, l} = \begin{cases}
            1 & \text{if~} t - 2^{l-1} \leq t' \leq t, \\
            0 & \text{otherwise}.
        \end{cases}
        \label{eq:sp-attn-mask}
    \end{align}
Moreover, we note that only $\bigO (T)$ $1$s in every attention mask $\bfm_{l}^{\text{enc}} \!\in\! \{0,1\}^{T \times T}$ are needed to get the desired exponentially increasing scope pattern. For example, when the base is $2$, instead of attending to every token in the range $t \!-\! 2^{l-1} \!\leq\! t' \!\leq\! t$ (Eq.~(\ref{eq:sp-attn-mask})), we only need to attend to the two tokens at the boundary, \ie $m_{t,t'}^{\text{enc}, l} \!=\! 1$ when $t' = t$ or $t' = t - 2^{l-1}$, to obtain the desired variable scope in \cref{eq:example-scope}.\footnote{To see this, we have in this case $\forall t \text{~and~} l, \phi_{t}^{l} \!:=\! \phi_{t-2^{l - 1}}^{l-1} \!\cup\! \phi_{t}^{l-1} \!=\! \{t' : t' \!\geq\! 0, t \!-\! t' \!<\! 2^{l}\}$, which matches \cref{eq:example-scope}.} This enables linear time (\wrt sequence length) implementations of the encoder, effectively avoiding the quadratic computation overhead of standard Transformers.

To control this computational complexity, we define a hyperparameter $N_{\text{max}}$ ($N_{\text{max}} \geq 2$) that specifies the maximum number of embeddings a given token can attend to using the attention mask $\bfm_{l}^{\text{enc}}$. Specifically, if the range $t \!-\! 2^{l-1} \!\leq\! t' \!\leq\! t$ in \cref{eq:sp-attn-mask} contains no more than $N_{\text{max}}$ embeddings, all embeddings in the range are attended to (\ie the corresponding mask values are $1$). Otherwise, the attention mask selects exactly $N_{\text{max}}$ embeddings from this range by sample linearly and round to the nearest integer.

To ensure the existence of encoder embeddings with full context (\ie its variable scope is $[T]$), we choose the number of layers $L$ to be greater than or equal to $\log_{2} T$.

% To implement MGSA, we apply an attention masking strategy defined as follows:
% \[
% m_{tt'} =
% \begin{cases}
% 1 & \text{if } \max(0, t - 2^{l-2}) \leq t' \leq t, \\
% 0 & \text{otherwise.}
% \end{cases}
% \]
% This masking mechanism restricts each encoder token $\h_t^{\text{enc},l}$ to attend only to tokens within its defined context window:
% \[
% C^{\text{enc},l}_t = [t - 2^{l-2}, t].
% \]
% Consequently, the variable scope of $\h_t^{\text{enc},l}$ is given by:
% \[
% VS(\h_t^{\text{enc},l}) = X_{t - 2^{l-1}:t}.
% \]
% For $L > \log_2 T$, the variable scope of the final encoder layer encompasses the entire input sequence, ensuring that global context is fully encoded.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{Figs/fig-tgpt-gen-paradigms.pdf}
    \caption{\textbf{Tracformers for contextual AR (CAR) and arbitrary-context (AC) generation.} In both cases, we use a prefix encoder and a suffix encoder to capture prefix and suffix information for every token, respectively. Model parameters of the two encoders are shared. The decoder receives information from features generated by both encoders. Specifically, the decoder attention masks ensure the decoder only acquires information of preceding (resp. succeeding) variables from the prefix (resp. suffix) encoder. During training, $\texttt{<MASK>}$ tokens are given to both encoders if the model is trained for AC generation. When training Tracformer for CAR generation, the prefix encoder always receives all inputs as preceding tokens $\x_{1:t-1}$ are assumed to be given when sampling $x_t$.}
    \label{fig:tgpt-gen-paradigms}
    \vspace{-0.6em}
\end{figure*}

\subsection{Decoder}
\label{sec:decoder}

% To address this, we introduce a decoder that aggregates multi-granularity features (with varying variable scopes) using cross-attention. This ensures the model retains the capacity to encode rich and comprehensive contextual information while enabling adaptability to diverse tasks by controlling the connection patterns between encoder and decoder.

Recall from the beginning of \cref{sec:tgpt} that the decoder combines local and global features computed in the encoder to predict distributions. As illustrated in \cref{fig:tgpt-overview}, the decoder consists of $L$ Transformer blocks, each containing a feed-forward neural network (FFN) and a cross-attention layer. Formally, given inputs $\h_{1:T}^{\text{dec}, l-1}$ to the $l$-th decoder block, the outputs $\h_{1:T}^{\text{dec},l}$ are computed as
    \begin{align*}
        \h_{1:T}^{\text{dec},l} = \texttt{FFN}_{l} \big ( \texttt{CrossAttn}_{l} (\h_{1:T}^{\text{dec}, l-1}, \h_{1:T}^{\text{enc},L-l+1}) \big ),
    \end{align*}
\noindent where in the cross-attention operation, each decoder embedding $\h_{t}^{\text{dec}, l-1}$ ($\forall t \!\in\! [T]$) attends to the outputs $\h_{1:T}^{\text{enc},L-l+1}$ of the $(L \!-\! l \!+\! 1)$-th encoder layer. Intuitively, the decoder first cross-attend to encoder embeddings that encode global information (\ie those with large scope sizes) before focusing on embeddings encoding local information (\ie those with small scope sizes). This progressive refinement mirrors human language generation, where high-level intent is first structured before being articulated into detailed expressions.

Attention masks are used in cross-attention layers to ensure each decoder embedding only depends on desired input variables. For example, when given the context $\x_{C}$, since the $t$-th decoder embedding $\h_{t}^{\text{dec}, l}$ is used to predict $\Pr (X_{t} \given \x_{C})$, $\h_{t}^{\text{dec}, l}$ should not attend to encoder embeddings whose variable scope contain variables not in $X_{C}$. 
% \guy{I don't get this part. To predict Xt, of course you need to know what the context XC is...}\xuejie{fixed}

Formally, define $m_{t,t'}^{\text{dec}, l}$ as the mask deciding whether the $t$-th embedding of the input to the $l$-th decoder block (\ie $\h_{t}^{\text{dec}, l-1}$) can attend to the $t'$-th embedding in the corresponding encoder layer (\ie $\h_{t'}^{\text{enc}, L-l+1}$). 

Different generation paradigms can be achieved using specific decoder masks $\bfm_{\text{dec}}^{l} \!:=\! \{ m_{t,t'}^{\text{dec}, l}\}_{t,t'}$ For example, to achieve AR modeling with Tracformers, we set $m_{t,t'}^{\text{dec}, l} \!=\! 1$ if $t' \!<\! t$ and zero out all remaining mask entries. Additionally, we observe that attending to fewer encoder tokens in the initial decoder layers has little impact on the performance. Therefore, to improve computational efficiency, we scale the sparsity of the cross-attention in proportion to the scope length of the corresponding encoder layer. Formally, the decoder attention mask is defined as:
    \begin{align}
        m_{t,t'}^{\text{dec},l} = \begin{cases}
            1 & \text{if~} t' < t \text{~and~} t' \equiv t \!-\! 1 \, (\mathrm{mod}\, 2^{L-l+1}), \\
            0 & \text{otherwise}.
        \end{cases}
        \label{eq:dec-mask}
    \end{align}

\subsection{Tractable Transformers for NAR Generation}
\label{sec:nar_generation}

In this section, we demonstrate how to apply Tracformers to two common NAR generation paradigms---contextual autoregressive generation and arbitrary-context generation. 

% Note that our model is versatile and can be adapted for other generation paradigms, with various possible modifications depending on the specific paradigm. We defer a detailed discussion of these adaptations to \cref{appx:tt-nar-adapt}.

\boldparagraph{Contextual AR Generation.} Contextual AR (CAR) generation refers to a scenario in which, when predicting a variable $X_t$, the model has access to all preceding tokens $\x_{1:t-1}$ and a subset of future tokens. This paradigm is used when an arbitrary context $\x_{C}$ is provided, and the model is tasked with autoregressively sampling all remaining tokens.

To implement Tracformer for CAR generation, we use two encoders, a \emph{prefix encoder} and a \emph{suffix encoder}, which capture information from preceding and succeeding tokens, respectively. As shown in \cref{fig:tgpt-gen-paradigms}(a), the prefix encoder processes the original sequence during training since preceding tokens are always available in CAR generation. In contrast, the suffix encoder observes only the tokens in a chosen context set $\x_{C}$, while the remaining tokens are represented by the $\texttt{<MASK>}$ token.

The prefix encoder uses the sparse attention mask described in \cref{sec:encoder} (\ie the sparse version of Eq.~(\ref{eq:sp-attn-mask})), where the scope of each feature $\h_{t}^{\text{enc},l}$ includes the $2^{l}$ preceding variables, including $X_{t}$. We use similar attention masks in the suffix encoder such that the scope of each feature $\h_{t}^{\text{enc},l}$ covers the $2^{l}$ succeeding variables starting from $X_{t}$. See \cref{appx:model-details} for a formal description. We use the masks defined in \cref{eq:dec-mask} for cross-attention between the decoder and the prefix encoder, as the attended features contain only prefix token information. Analogously, the suffix encoder uses the following cross-attention masks to ensure the decoder receives only suffix information from it:
    \begin{align}
        m_{t,t'}^{\text{dec},l} = \begin{cases}
            1 & \text{if~} t' > t \text{~and~} t' \equiv t \!+\! 1 \, (\mathrm{mod}\, 2^{L-l+1}), \\
            0 & \text{otherwise}.
        \end{cases}
        \label{eq:dec-mask-suffix}
    \end{align}
As illustrated in \cref{fig:tgpt-gen-paradigms}(a), given context $\x_{C}$, the $t$-th output feature embedding of the decoder is used to predict the distribution $\Pr (X_{t} \given \x_{C_t})$, where $C_t \!:=\! C \!\cup [t\!-\!1]$. The overall training loss for the CAR generation model is
    \begin{align}
        L(\params) \!=\! - \expectation_{\x \sim \data, C \sim \calP_{C}} \!\! \left [ \sum_{t \not\in C} \log \Pr\nolimits_{\params} (x_{t} \given \x_{C_{t}}) \right ] \!\!,
        \label{eq:car-eq}
    \end{align}
\noindent where $\params$ is the set of learnable parameters, $\data$ is a dataset, and $\calP_{C}$ is a mask strategy used to sample the context set $C$.

% As the prefix context is captured by the encoder (Sec.~\ref{sec:encoder}) and used by the decoder (Sec.~\ref{sec:decoder}), we use another suffix encoder to capture suffix context. Despite sharing the same parameters, there are two key differences with the original encoder, which we call the prefix encoder in the following.

% First, while all tokens are provided to the prefix encoder since all preceding tokens are observable in CAR generation, we input $\texttt{<MASK>}$ to the suffix encoder if a token is not provided in the context. 

% Next, the scope of each embedding $\h_{t}^{\text{enc},l}$ in the suffix encoder is defined to be the set of variables whose distance from the right (in contrast to the left for the prefix encoder) of $X_t$ is smaller than $2^{l}$. Analogous to \cref{eq:sp-attn-mask}, the attention mask of the suffix encoder is defined as
%     \begin{align*}
%         \quad m_{t,t'}^{\text{enc}, l} = \begin{cases}
%             1 & \text{if~} t \leq t' \leq t + 2^{l-1}, \\
%             0 & \text{otherwise}.
%         \end{cases}
%     \end{align*}
% We use the same trick described in \cref{sec:encoder} to cap the maximum number of embeddings each feature embedding can attend to.

% Finally, analogous to \cref{eq:dec-mask}, the cross-attention layers in the decoder additionally attend to feature embeddings with suffix context from the suffix encoder.

\boldparagraph{Arbitrary-Context Generation.} In arbitrary-context (AC) generation, the model is tasked to predict the distribution of $X_t$ given arbitrary context, which can be used to generate missing tokens in an arbitrary order. Analogous to \cref{eq:car-eq}, the objective of AC generation task is 
    \begin{align}
        L(\params) \!=\! - \expectation_{\x \sim \data, C \sim \calP_{C}} \!\! \left [ \sum_{t \not\in C} \log \Pr\nolimits_{\params} (x_{t} \given \x_C) \right ] \!\!,
        \label{eq:ac-eq}
    \end{align}
As shown in \cref{fig:tgpt-gen-paradigms}(b), AC generation paradigm can be implemented by the same model described for CAR generation, with the only difference that inputs to both the prefix encoder and the suffix encoder use the mask token if a token does not exist in the context. Correspondingly, the $t$-th output feature embedding predicts the distribution $\Pr (X_{t} \given \x_{c})$.

\boldparagraph{Training and Inference Efficiency.} 
Thanks to its sparse attention modules in both the encoder and the decoder, Tracformers enjoy efficient training and inference. In particular, KV-caching \citep{pope2023efficiently} can be used to amortize inference cost. See \cref{appx:efficiency} for a detailed discussion.

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{Figs/fig-ac-car-results.pdf}
    \vspace{-1.8em}
    \caption{Validation conditional perplexity ($\downarrow$) of CAR/AC generation tasks with varying masking strategies, evaluated on the WikiText103 validation set with a sequence length of 1024. (a) and (b) show CAR generation with span lengths sampled from \(\text{Geometric}(\mu=50)\) and \(\text{Geometric}(\mu=10)\), respectively. (c) and (d) correspond to AC generation with the same span length distributions as in CAR. Across all mask ratios and span lengths, Tracformer consistently outperforms all baselines, demonstrating strong generalization capabilities.}
    \label{fig:ac-car-results}
    \vspace{-1.0em}
\end{figure*}

% Let $C_t^{\text{dec},l}$ denote the indices of encoder tokens that $\h_t^{\text{dec},l}$ can attend to. The \textit{variable scope} of $\h_t^{\text{dec},l}$ is then defined as the union of the variable scopes of the encoder tokens it attends to:
% \[
% VS(\h_t^{\text{dec},l}) := \bigcup_{t' \in C_t^{\text{dec},l}} VS(\h_{t'}^{\text{enc},L-l+1}).
% \]

% It is inefficient and impractical to expose the model to all possible masking types during training. To address this, we propose a new foundation architecture for sequence modeling, \textit{Tractable Transformer}, designed to generalize more effectively across diverse generation tasks. The first key component of our design is a novel attention mechanism, \textit{Multi-Granularity Self-Attention} (MGSA), illustrated in \cref{fig:stride_attn}. While standard Transformer architectures are effective at encoding global context, we argue that the model should also capture relatively local contexts.\anji{This should be the first sentence.} Since the local context around a masked region often remains consistent across different masking types, local context features are inherently more invariant and robust, making them critical for improving generalization across conditional generation tasks. MGSA leverages this by combining global and local contexts, encoding dependencies at multiple granularities to enhance the model’s ability to generalize.

% However, naively replacing traditional dense attention with the sparse structure in \cref{fig:stride_attn} may compromise the model’s expressiveness. Therefore, the second design choice of the Tractable Transformer is a novel encoder-decoder architecture. The encoder leverages the proposed MGSA mechanism to extract local-to-global context features. The decoder then uses purely cross-attention to aggregate the multi-granularity features extracted by the encoder, ensuring the model maintains its capacity to encode rich and comprehensive contextual information.

% Next, we provide a formal and detailed description of the Tractable Transformer architecture, including both the encoder and decoder design along with its adaptability to different generation tasks:

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.9\linewidth]{Figs/tgpt_ar.png}
%     \caption{TGPT for Autoregressive Generation}
%     \label{fig:tgpt_ar}
% \end{figure}

% \textbf{Encoder}\quad
% The encoder of the Tractable Transformer consists of $L$ stacked layers, each comprising a Multi-Granularity Self-Attention (MGSA) sub-layer and a feed-forward sub-layer. In MGSA, each layer has a limited context window size, meaning that $\h_t^{\enc,l}$  can only attend to tokens within its defined context window. Let $\text{CW-E}_t^l$ \anji{Maybe use one letter instead of CW-E for clarity.} denote the set of token indices that $\h_t^{\enc,l}$ can attend to, then we have:

% \[
% \h_t^{\enc,l} = \text{MLP}\big(\text{Attn}(\h^{\enc,l-1}_t, \forall t \in \text{CW-E}_t^l) \big).
% \]

% As illustrated in \cref{fig:tgpt_nar}, the context window length grows exponentially with the layer depth, enabling shallow layers to focus on fine-grained, local semantics, while deeper layers capture broader contextual information and abstract representations. 

% The \textit{variable scope} of $\h_t^{\enc,l}$ is defined as the input variable set that $\h_t^{\enc,l}$ can directly or indirectly attend to.\anji{I think we should define this first. First, define variable scope, then describe the desired variable scope. Finally, say how we achieve it with an attention mask.} 


% Specifically, by setting $\text{CW-E}_t^l = [t - 2^{l-2}, t]$, the variable scope becomes: $VS(\h_t^{\enc,l}) = X_{t-2^{l-1}:t}$. For $L > \log_2 T$, the variable scope of the final encoder layer covers the entire input sequence, ensuring the global context can be encoded.

% \textbf{Decoder}\quad
% As previously discussed, the decoder aggregates multi-granularity features (w.r.t. different context window length) extracted by the encoder using cross-attention. Therefore, it is also composed of $L$ stacked layers, where each layer contains a cross-attention sub-layer and a feed-forward sub-layer. As shown in \cref{fig:tgpt_ar}, earlier decoder layers focus on global and abstract features from the encoder, while later layers refine these features into precise, context-specific semantic representations. This progressive refinement mirrors human language generation, where high-level intent is structured before being articulated into detailed expressions.

% Let $C^{\text{dec}}_t^l$ denote the indices of encoder tokens that $\h_t^{\dec,l}$ can attend to. Then we have:

% \[
% \h_t^{\dec,l}= \text{MLP}\big(\text{Attn}(\h^{\enc,L-l+1}_t,\forall t \in C^{\text{dec}}_t^l)\big).
% \]

% The \textit{variable scope} of $\h_t^{\dec,l}$ is defined as the union of the variable scopes of the encoder tokens it attends to: $VS(\h_t^{\dec,l}):= \cup_{ t \in C^{\text{dec}}_t^l } VS(\h_t^{\enc,L-l+1})$

% \textbf{Generative Adaption}\quad
% To adapt the Tractable Transformer for various generation tasks, we control the variable scope of each decoder token by adjusting $C^{\text{dec},l}_t$, i.e., the connection pattern between the encoder and decoder. Specifically:
% \begin{itemize}
%     \item For AR modeling, \(C^{\text{dec,l}}_t\) for all \(l \in [L]\) is restricted to \([t]\), ensuring causal dependency.
%     \item For NAR tasks, where \(\Pr(\mathbf{x}_R \given \mathbf{x}_C)\) is modeled, \(C^{\text{dec},l}_t\) is adjusted to ensure:
%     \[
%     VS(\mathbf{h}_t^{\text{dec},l}) = X_C, \, \forall l \in [L].
%     \]
% \end{itemize}
% Further details  can be found in \xuejie{appx}.

% \subsection{Bidirectional Modeling Enables TGPT to Fill in the Blanks}


% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.9\linewidth]{Figs/tgpt_nar.png}
%     \caption{TGPT for Contextual Autoregressive Generation}
%     \label{fig:tgpt_nar}
% \end{figure}



% Autoregressive (AR) models like GPT have demonstrated exceptional performance across a wide range of natural language processing tasks. However, the inherent limitation of unidirectional left-to-right generation constrains their applicability to non-autoregressive (NAR) tasks that require filling in missing tokens within arbitrary contexts. These NAR tasks include critical downstream applications such as code completion, text infilling, dialogue repair, and data imputation. To address this challenge, TGPT introduces bidirectional modeling by leveraging both left-to-right and right-to-left encoders, allowing the model to handle NAR tasks effectively by capturing bidirectional contextual information.

% \textbf{Bidirectional Model Architecture}\quad
% TGPT incorporates a novel architecture by integrating a right-to-left encoder alongside the traditional left-to-right encoder. This additional encoder is structurally analogous to the left-to-right encoder, employing the same multi-grained self-attention mechanism. However, the token attention direction is reversed, allowing $t_k^i$ (the $k^{\text{th}}$ token in the $i^{\text{th}}$ layer) to attend to $t_k^{i-1}$ and $t_{k+s_i}^{i-1}$ for $i \in \{2, ..., N\}$. This configuration ensures that the receptive field grows hierarchically, covering relevant tokens from both directions.

% The decoder is enhanced to accommodate inputs from both encoders. Through cross-attention, the decoder tokens in each layer attend to layer outputs from both the left-to-right and right-to-left encoders. Following the previous definition of the receptive field, we define the \textbf{left receptive field} of any decoder token as the receptive field w.r.t. the left-to-right encoder, and \textbf{right receptive field} w.r.t. the right-to-left encoder. This bidirectional modeling enables TGPT to effectively integrate information from both past and future contexts, as illustrated in Figure \cref{fig:tgpt_nar}.


% \textbf{Contextual Autoregressive Generation}\quad
% Contextual AR generation (CAR) is an effective strategy for addressing NAR tasks. In this paradigm, the input comprises corrupted texts containing missing tokens, which the model sequentially fills using autoregressive generation based on the surrounding context.

% To train TGPT for CAR tasks, the following procedure is designed:

% \begin{enumerate}
% \item \textbf{Left-to-Right Encoder Input}: The input to the left-to-right encoder is the original text sequence $\{x_1, x_2, ..., x_n\}$, ensuring that the encoder generates representations conditioned on preceding tokens.
% \item \textbf{Right-to-Left Encoder Input}: For the right-to-left encoder, the input sequence is masked sequence $\{\hat{x}_1, \hat{x}_2, ..., \hat{x}_n\}$, with corrupted tokens replaced by a special [MASK] token. This prevents the model from accessing future corrupted tokens during encoding.
% \item \textbf{Decoder Adjustments}: During training, the decoder token’s left receptive field is still constrained to be $\{x_1, x_2, ..., x_{k-1}\}$ as AR decoding, while the right receptive field is the masked sequence variables: $\{\hat{x}_{k+1}, \hat{x}_{k+2}, ..., \hat{x}_n\}$.
% \item \textbf{Loss Function}: The model minimizes the combined negative log-likelihood of correctly predicting missing tokens using both encoders' representations.
% \end{enumerate}


% During inference, TGPT operates autoregressively, filling in blanks within the corrupted text. By employing bidirectional modeling, TGPT captures the full context to generate coherent predictions, which can naturally handle tasks requiring contextual awareness in both directions and bridge the gap between AR and CAR paradigms.

\section{Related Work}

\boldparagraph{NAR Modeling Techniques.}
BERT \citep{devlin2018bert,warner2024smarter} is one of the first Transformer models designed for NAR generation. It proposes to use a special mask token to indicate unknown tokens and task the model to predict them given the observed tokens. Built on top of this mask prediction principle, discrete diffusion models \citep{austin2021structured} improve NAR generation performance by designing better learning objectives \citep{campbell2022continuous,lou2023discrete,sahoo2024simple} and mask strategies \citep{shi2024simplified}. Instead of recovering sequences from mask tokens, some discrete diffusion models learn to recover from uniformly sampled sequences \citep{lou2023discrete}. Another thread of work incorporates autoregressive or semi-autoregressive biases to the denoising process of diffusion models, intending to combine the expressiveness of autoregressive modeling and the ability to perform NAR generation \citep{chen2024diffusion,han2023ssd}.

\boldparagraph{Architectures for NAR Modeling.}
Decoder-only transformers with full attention are the most widely adopted architecture for NAR modeling. Many SoTA discrete diffusion models use these models. Additionally, bidirectional autoregressive modeling, exemplified by models like BART \citep{lewis2020bart} and MASS \citep{song2019mass}, represents an intermediate approach that incorporates bidirectional context while preserving the left-to-right autoregressive generation process. \citet{sun2023score} developed a Transformer-based architecture for a subclass of discrete diffusion models. \citet{liu2025discrete} and \citet{xu2025energy} combine diffusion models with other deep generative models, such as AR models and energy-based models.

\section{Experiment}

% \guy{it's likely that someone will ask about modern bert?}

In this section, we aim to empirically evaluate Tracformer’s effectiveness in both conditional and unconditional generation. Specifically, our experiments are designed to answer two key questions: (i) How does Tracformer compare to other NAR architectures in terms of conditional generation performance? (ii) Can Tracformer scale effectively and outperform existing SoTA generative models in both conditional and unconditional tasks? To this end, we conduct two sets of experiments: In \cref{sec:arch_comparison}, we compare Tracformer with a range of NAR architectures on WikiText \citep{merity2022pointer}, LAMBADA \citep{paperno2016lambada}, and One Billion Words (1BW) \citep{chelba2013one} datasets to evaluate its performance across diverse conditional queries. In \cref{sec:scaling_tracformer}, we scale Tracformer to OpenWebText \citep{Gokaslan2019OpenWeb} and benchmark it against SoTA discrete diffusion models, focusing on zero-shot conditional and unconditional performance. These experiments comprehensively evaluate Tracformer’s advantages and its potential to serve as a more effective backbone for NAR generation.

\subsection{Comparison of Architectures for NAR Modeling}
\label{sec:arch_comparison}
Recall from \cref{sec:tgpt} that we propose Tracformer’s encoder-decoder architecture as an effective approach to leveraging both local and global features, enabling robust performance across varying queries. To empirically validate this, we compare Tracformer with baseline models employing different transformer architectures on identical NAR modeling tasks, focusing on their conditional generalization capabilities.


\boldparagraph{Baselines.} As discussed in \cref{sec:nar_generation}, Tracformer supports two distinct NAR generation modes: contextual AR (CAR) generation and arbitrary-context (AC) generation, each requiring different baseline comparisons. For CAR generation, we use BART \citep{lewis2020bart} as a baseline, as it represents a widely adopted encoder-decoder architecture for CAR tasks. BART’s encoder captures bidirectional global context, while its autoregressive decoder generates outputs sequentially via cross-attention. This fundamental design principle is shared by many CAR models, such as MASS \citep{song2019mass} and PALM \citep{bi2020palm}, making BART a strong representative baseline.  

\begin{table}[t]
    \centering
    \caption{Evaluation of CAR text infilling performance using the MAUVE and BERT Score with different mask ranges. Higher scores indicate better performance. Tracformer consistently outperforms BART under all conditions.}
    \label{tab:wiki-car-infilling}
    \vspace{0.1em}

    \renewcommand{\arraystretch}{1.1}
    \setlength{\tabcolsep}{0.24em}
    \centering
    \scalebox{0.78}{
    \begin{tabular}{cc@{\hspace{0.3em}}c@{\hspace{0.3em}}cc@{\hspace{0.3em}}c@{\hspace{0.3em}}c}
        \toprule
        \multirow{2}{*}[-0.3em]{Mask ranges} & \multicolumn{2}{c}{MAUVE ($\uparrow$)} & \multicolumn{2}{c}{BERT Score ($\uparrow$)} \\
        \cmidrule(lr){2-3}
        \cmidrule(lr){4-5}
        & Tracformer & BART & Tracformer & BART  \\
        \cmidrule(lr){1-1}
        \cmidrule(lr){2-5}
        $\scalebox{0.78}{[0.25,0.75]}$ & \textbf{0.960} & 0.951 & \textbf{0.464} & 0.414 \\
        $\scalebox{0.78}{[0.5,1.0]}$ & \textbf{0.114} & 0.016 & \textbf{0.488} & 0.386 \\
        $\scalebox{0.78}{[0.1,0.4] \& [0.6,0.9]}$ & \textbf{0.931} & 0.889 & \textbf{0.370} & 0.306 \\
        $\scalebox{0.78}{[0,0.4] \& [0.5,0.8]}$ & \textbf{0.946} & 0.940 & \textbf{0.274} & 0.216 \\
        $\scalebox{0.78}{[0,0.25] \& [0.75,1.0]}$ & \textbf{0.177} & 0.063 & \textbf{0.457} & 0.401 \\
        $\scalebox{0.78}{[0,0.1] \& [0.2,0.5] \& [0.7,1.0]}$ & \textbf{0.107} & 0.023 & \textbf{0.286} & 0.208 \\
        \bottomrule
    \end{tabular}}
    \vspace{-1.2em}
\end{table}

For AC generation, we compare Tracformer against two baselines: (i) BERT \citep{devlin2018bert}, which is the predominantly used backbone for modern diffusion models, and (ii) BERT-bidir, a variant of BERT that incorporates both forward and reverse AR encoders using dense self-attention. These baselines provide a contrast between standard bidirectional encoding and autoregressive modeling in AC generation tasks (see Appx.~\ref{appx:exp-wiki} for details). 

% Moreover, we choose BERT as well as its variant as the baseline for AC generation because this task is equivalent to training diffusion models for one-step prediction, and  predominantly use BERT-family models as their backbone.

\boldparagraph{Training Setup.} For the CAR task, Tracformer and BART are both trained using the CAR objective defined in \cref{eq:car-eq}. During training, we set the mask strategy (\ie $\calP_{C}$) to sample spans whose lengths follow a geometric distribution with mean $\mu = 50$ and apply a total mask ratio of 50\% (see Appx.~\ref{appx:span-mask}).
% The masking strategy $\calP_{C}$, used to sample context in each iteration, is detailed in \cref{appx:span-mask}. We sample the span length from a geometric distribution with $\mu = 50$ and adopt a total mask ratio of 50\% for training. 
All models are trained on the WikiText103, LAMBADA, and 1BW. We present the results of WikiText103 in the paper and defer other results to \cref{appx:exp-eval-details}. The experiment setup for the AC generation task follows the CAR task, where the training objective is defined in \cref{eq:ac-eq}. See \cref{appx:exp-wiki} for more details.


\boldparagraph{Empirical Insights from the CAR Generation Results.} 
We evaluate both models' generalization capabilities on CAR tasks through two specific tests: ratio generalization and span generalization. For ratio generalization evaluation, the span masking strategy remains consistent with the training setup, where the span length is sampled from \(\text{Geometric}(\mu=50)\), but the total mask ratio is varied between 0.1 and 0.9. \cref{fig:ac-car-results}(a) shows the conditional perplexity (PPL) for both models on the WikiText103 validation set. The results exhibit a U-shape, where performance improves as the mask ratio approaches the training mask ratio. Tracformer consistently outperforms BART across all mask ratios. The performance gap between the two models is small near the training mask ratio, but BART's PPL increases as the mask ratio deviates further, whereas Tracformer maintains robust generalization performance. 

\begin{table}[t]
    \centering
    \caption{Zero-shot conditional perplexity on WikiText103 and 1BW using six fixed prompts. Tracformer operates in the CAR generation mode, while the conditional PPLs of SEDD and MDLM are computed following the derivation in \cref{appx:conditional-elbo}. Tracformer consistently outperforms state-of-the-art discrete diffusion models SEDD and MDLM across various masking ranges.}
    \label{tab:owt-fixed-infilling}
    \vspace{0.1em}

    \renewcommand{\arraystretch}{1.1}
    \setlength{\tabcolsep}{0.4em}
    \centering
    \scalebox{0.76}{
    \begin{tabular}{cc@{\hspace{0.32em}}c@{\hspace{0.32em}}cc@{\hspace{0.32em}}c@{\hspace{0.32em}}c}
        \toprule
        \multirow{2}{*}[-0.3em]{Mask ranges} & \multicolumn{3}{c}{WikiText103} & \multicolumn{3}{c}{1BW} \\
        \cmidrule(lr){2-4}
        \cmidrule(lr){5-7}
        & Tracformer & SEDD & MDLM & Tracformer & SEDD & MDLM \\
        \cmidrule(lr){1-1}
        \cmidrule(lr){2-7}
        $\scalebox{0.78}{[0.25,0.75]}$ & \textbf{29.38} & 37.83 & 30.36 & \textbf{41.98} & 53.72 & 55.19 \\
        $\scalebox{0.78}{[0.1,0.4] \& [0.6,0.9]}$ & \textbf{29.67} & 37.17 & 30.55 & \textbf{38.52} & 49.38 & 49.48 \\
        $\scalebox{0.78}{[0,0.4] \& [0.5,0.8]}$ & \textbf{34.73} & 43.86 & 35.84 & \textbf{45.56} & 58.22 & 58.41 \\
        $\scalebox{0.78}{[0,0.25] \& [0.75,1]}$ & \textbf{37.73} & 46.65 & 40.11 & \textbf{49.14} & 66.96 & 66.16 \\
        $\scalebox{0.78}{[0.2,0.3] \& [0.4,0.6]}$ & 24.31 & 28.31 & \textbf{22.97} & \textbf{31.76} & 38.76 & 38.97 \\
        $\scalebox{0.78}{[0.1,0.9]}$ & \textbf{33.88} & 44.92 & 36.69 & \textbf{43.98} & 57.96 & 59.14 \\
        \bottomrule
    \end{tabular}}
    \vspace{-0.8em}
\end{table}

To evaluate span generalization performance, we alter the mean span lengths of the adopted span masking strategy during inference, resulting in a mismatch $\calP_{C}$ between training and evaluation. \cref{fig:ac-car-results}(b) illustrates the conditional PPL when the mean span length is reduced to 10. As expected, shorter span lengths reduce the overall task difficulty, resulting in lower PPL for all models. However, Tracformer demonstrates significantly better performance improvements compared to BART, highlighting its ability to generalize effectively to unseen masking patterns. We also include \(\text{Geometric}(\mu=3)\) span mask results in \cref{fig:ac-car-results2}(a) of \cref{appx:exp-eval-details}, where a similar trend is observed.


% \begin{table}[h]
%     \centering
%     \caption{....\xuejie{CAR infilling performance}}
%     \label{tab:wiki-car-infilling}
%     \vspace{0.1em}

%     \renewcommand{\arraystretch}{1.1}
%     \setlength{\tabcolsep}{0.24em}
%     \centering
%     \scalebox{0.78}{
%     \begin{tabular}{cc@{\hspace{0.3em}}c@{\hspace{0.3em}}cc@{\hspace{0.3em}}c@{\hspace{0.3em}}c}
%         \toprule
%         \multirow{2}{*}[-0.3em]{Mask ranges} & \multicolumn{3}{c}{MAUVE} & \multicolumn{3}{c}{BERT Score} \\
%         \cmidrule(lr){2-4}
%         \cmidrule(lr){5-7}
%         & Tracformer & BART & SSD-LM & Tracformer & BART & SSD-LM \\
%         \cmidrule(lr){1-1}
%         \cmidrule(lr){2-7}
%         $\scalebox{0.78}{[0.25,0.75]}$ & &  &&  &  & \\
%         $\scalebox{0.78}{[0.5,1.0]}$ & &  &&  &  & \\
%         $\scalebox{0.78}{[0.1,0.4] \& [0.6,0.9]}$ & &  &&  &  & \\
%         $\scalebox{0.78}{[0,0.4] \& [0.5,0.8]}$ & &  &&  &  & \\
%         $\scalebox{0.78}{[0,0.25] \& [0.75,1.0]}$ & &  &&  &  & \\
%         \multirow{2}{*}[0.0em]{\makecell{$\scalebox{0.78}{[0,0.1] \& [0.2,0.5]}$ \\ $\scalebox{0.78}{\& [0.7,1.0]}$}} & \multirow{2}{*}[0.0em]{\textbf{-}} & \multirow{2}{*}[0.0em]{-} & \multirow{2}{*}[0.0em]{-} & \multirow{2}{*}[0.0em]{\textbf{-}} & \multirow{2}{*}[0.0em]{-} & \multirow{2}{*}[0.0em]{-} \\
%         & \\
%         \bottomrule
%     \end{tabular}}
%     \vspace{-0.4em}
% \end{table}



Beyond perplexity, we assess the quality of generated text in CAR tasks using six fixed mask ranges (\ie text infilling). Metrics such as MAUVE \citep{pillutla2021mauve} and BERT Score \citep{Zhang2020BERTScore} are used to quantify the similarity between the generated and original text. See \cref{appx:exp-eval-details} for further evaluation details. \cref{tab:wiki-car-infilling} presents the results, where Tracformer consistently achieves higher scores than all baselines, demonstrating Tracformer's ability to produce high-quality text under diverse conditions.





\boldparagraph{Empirical Insights from the AC Generation Results.} 
As shown in \cref{fig:ac-car-results}(c) and (d), Tracformer achieves significantly lower conditional perplexity than both baselines across all mask ratios and span lengths, demonstrating superior generalization performance consistently. 
% These results highlight the advantages of the adopted multi-scope attention as well as the meticulously designed encoder-decoder structure. 
Additionally, we conduct an ablation study on Tracformer's multi-scope attention, with a detailed analysis provided in \cref{appx:ablation}.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{Figs/fig-owt-rand-cond-ppl.pdf}
    \vspace{-1.8em}
    \caption{Conditional perplexity under varied masking strategies, evaluating ratio-generalization and span-generalization. We evaluate two different span length distributions: $\text{Geometric}(\mu=10)$ and $\text{DLogistic}(\mu=15, \sigma=3)$. Tracformer demonstrates superior conditional generalization capabilities compared to SEDD and MDLM across different masking strategies.}
    \label{fig:owt-rand-cond-ppl}
    \vspace{-0.8em}
\end{figure*}

% As discussed in \cref{sec:nar_generation}, the AC generation paradigm enables generating missing tokens in an arbitrary order. However, \cref{fig:ar-cond-ppl} shows that SoTA discrete diffusion models struggle to generalize across different conditional queries, likely due to their reliance on BERT-like NAR backbones. Tracformer, with its demonstrated robustness and generalization capabilities, holds promise as a better backbone for diffusion models, enabling improved performance in conditional generation tasks.  


\subsection{Benchmarking Against SoTA NAR Models}
\label{sec:scaling_tracformer}

In this section, we test the scalability of Tracformer by training it on the OpenWebText \citep{Gokaslan2019OpenWeb} dataset and comparing its performance against various SoTA generative models of similar scale, including GPT-2 \citep{radford2019language} and SoTA discrete diffusion models \citep{lou2023discrete,sahoo2024simple,austin2021structured,gulrajani2024likelihood}. Tracformer is trained using the CAR generation objective defined in \cref{eq:car-eq}, with similar masking strategies $\calP_{C}$ to \cref{sec:arch_comparison} (see Appx.~\ref{appx:owt-exp-details}). For evaluation, we benchmark zero-shot conditional and unconditional performance on multiple datasets.


For conditional generation, we compare Tracformer with SoTA discrete diffusion models, SEDD and MDLM. \cref{tab:owt-fixed-infilling} presents the zero-shot conditional PPL results using fixed masking strategies, where Tracformer consistently outperforms both baselines across all mask ranges. To further assess generalization, we evaluate varied masking strategies, with \cref{fig:owt-rand-cond-ppl} reporting results from ratio-generalization and span-generalization tests. Specifically, we consider two span length distributions: $\text{Geometric}(\mu \!=\! 10)$ and $\text{DLogistic}(\mu \!=\! 15, \sigma \!=\! 3)$. On 1BW, Tracformer consistently outperforms both SEDD and MDLM across all mask ratios. On WikiText103, MDLM shows a slight advantage at low mask ratios, but as the masking ratio increases, Tracformer achieves the best performance, demonstrating stronger generalization under more challenging conditions.

\begin{table}[t]
\centering
\caption{Model size, number of non-embedding parameters, and number of trained tokens for the models evaluated in this paper. Tracformer has a smaller full model size and fewer non-embedding parameters compared to SEDD and MDLM.}
\scalebox{0.72}{
\begin{tabular}{lccc}
\toprule
Model & Full Model Size & \#Non-Embed Params & \#Trained Tokens \\ 
\cmidrule(lr){1-1}
\cmidrule(lr){2-4}
GPT2           & 124M                     & 85M                             & -                     \\
SEDD          & 169M                     & 90M                             & 210B                     \\ 
MDLM          & 169M                     & 90M                             & 524B                     \\

Tracformer    & 109M                     & 79M                             & 295B                     \\
\bottomrule
\end{tabular}}
\label{tab:model_size}
\vspace{-1.0em}
\end{table}

Notably, despite its smaller model size (Tab.~\ref{tab:model_size}), Tracformer not only excels in fixed-mask conditional PPL (Tab.~\ref{tab:owt-fixed-infilling}) but also adapts better to diverse masking distributions.

\begin{table}[t]
    \centering
    \caption{Zero-shot unconditional perplexity across various datasets. Tracformer, despite its smaller size, achieves competitive or superior performance compared to larger models.}
    \label{tab:owt-fixed-uncond}
    \vspace{0.1em}

    \renewcommand{\arraystretch}{1.1}
    \setlength{\tabcolsep}{0.40em}
    \centering
    \scalebox{0.76}{
    \begin{tabular}{cc@{\hspace{0.46em}}c@{\hspace{0.46em}}c@{\hspace{0.46em}}c@{\hspace{0.46em}}c}
        \toprule
        Model & WikiText103 & WikiText2 & Lambada & PTB & 1BW \\
        \cmidrule(lr){1-1}
        \cmidrule(lr){2-6}
        GPT-2 (124M) & 41.60 & 42.32 & \textbf{45.04} & 138.43 & 75.20 \\
        D3PM (169M) & 75.16 & 77.28 & 93.47 & 200.82 & 138.92 \\
        PLAID (169M) & 50.86 & 51.80 & 57.28 & 142.60 & 91.12 \\
        SEDD (169M) & 40.62 & 41.84 & 50.92 & 114.24 & 79.29 \\
        MDLM (169M) & \textbf{37.01} & \textbf{36.75} & 48.46 & \textbf{96.40} & 67.94 \\
        Tracformer (109M) & 43.27 & 43.82 & 58.10 & 166.10 & \textbf{51.34} \\
        \bottomrule
    \end{tabular}}
    \vspace{-1.2em}
\end{table}

For unconditional generation, \cref{tab:owt-fixed-uncond} reports the zero-shot unconditional PPL across various datasets. Tracformer remains highly competitive, achieving results comparable to or better than larger models. We additionally include text samples generated by Tracformer in \cref{appx:text-samples} for both conditional and unconditional generation. These findings further reinforce our earlier conclusion: while SoTA NAR diffusion models exhibit strong unconditional PPL, their inability to generalize effectively to conditional queries unseen during training limits their broader applications. Tracformer, with its robust performance in both conditional and unconditional tasks, demonstrates its potential as a scalable and versatile generative model. 

\section{Conclusion and Limitations}

We propose Tracformer, a Transformer-based architecture for flexible and generalizable conditional generation. Through extensive experiments, we demonstrate that Tracformer’s multi-scope attention mechanism and specialized encoder-decoder design enable robust conditional generation performance. However, due to resource limitations, we only train Tracformer at the GPT-2 (base) scale. While our current results already establish Tracformer as a highly promising NAR architecture, future work will focus on scaling the model further to fully explore its potential. Moreover, given that AC generation closely aligns with one-step diffusion training, Tracformer could serve as a strong backbone for modern diffusion models. Future research will investigate this direction, leveraging Tracformer’s capabilities to enhance diffusion-based generative frameworks.


% \subsection{\xuejie{[optional] exps beyond language modeling}}

% \subsection{Context-conditioned AR Generation}
% \begin{table}[h]
% \centering
% \begin{tabular}{lccccccccc}
%     \toprule
%     \multirow{2}{*}[-0.2em]{Model} & \multicolumn{3}{c}{Long Span Mask} & \multicolumn{3}{c}{Short Span Mask} & \multicolumn{3}{c}{Uniform Mask} \\
%     \cmidrule(lr){2-4}
%     \cmidrule(lr){5-7}
%     \cmidrule(lr){8-10}
%     ~ & 0.15 & 0.25 & 0.5 & 0.15 & 0.25 & 0.5 & 0.15 & 0.25 & 0.5\\
%     \midrule
%     BART(340M) & 21.33 & 20.89 & 21.10 & 22.11 & 22.06 & 21.45 & 22.09 & 21.89 & 21.93\\
%     TGPT(318M) & 19.99 & 19.80 & 19.95 & 13.26 & 13.58 & 14.48 & 8.83 & 9.81 & 12.62\\
%     TGPT(bf16) & 20.75 & 20.38 & 20.75 & 13.81 & 14.25 &  15.19  & 9.50  & 10.44 & 13.38\\
%     \bottomrule
% \end{tabular}
% \caption{Conditional PPL on WikiText103 Test Set (seq$\_$length=1024, no overlap, 270 samples). Long Span Mask (mean$\_$span$\_$length=50). Short Span Mask (T5-style, mean$\_$span$\_$length=3). Uniform Mask(BERT-style). Mask ratio (0.15,0.25,0.5)}
% \end{table}



% \begin{table}[h]
% \centering
% \begin{tabular}{lccccccccc}
%     \toprule
%     \multirow{2}{*}[-0.2em]{Model} & \multicolumn{3}{c}{Long Span Mask} & \multicolumn{3}{c}{Short Span Mask} & \multicolumn{3}{c}{Uniform Mask} \\
%     \cmidrule(lr){2-4}
%     \cmidrule(lr){5-7}
%     \cmidrule(lr){8-10}
%     ~ & MAUVE & BLUE & BERT & MAUVE & BLUE & BERT & MAUVE & BLUE & BERT\\
%     \midrule
%     BART(340M) & 97.4 & 0.53 & 0.49 & MAUVE & BLUE & BERT & / & 0.32 & 0.19\\
%     TGPT(318M) & 98.1 & 0.54 & 0.51 & MAUVE & BLUE & BERT & / & 0.39 & 0.37\\
%     \bottomrule
% \end{tabular}
% \caption{Evaluation of the generated texts (seq$\_$length=1024, mask ratio = 0.5, no overlap, 960$\times$1 samples). Long Span Mask (mean$\_$span$\_$length $\geq$ 50). Short Span Mask (T5-style, mean$\_$span$\_$length=3). Uniform Mask (BERT-style)}
% \end{table}




% TODO:
% \begin{itemize}
%     \item for AR generation, there is no significant difference among different mask ratios. So maybe we can fix mask ratio = 0.5 and draw a ppl curve over different mean$\_$span$\_$length.
%     \item The convergence speed/training efficiency of TGPT is better.
%     \item check missing at random/missing completely at random related works
%     \item train BART/TGPT with Uniform or Short Span mask to check performance.
% \end{itemize}


% \begin{table}[h]
% \centering
% \begin{tabular}{lccccc}
%     \toprule
%     Mask ranges & LAMBADA & WikiText2 & PTB & WikiText103 & 1BW\\
%     \midrule
%     (0.25,0.75) & 0.1 & 0.2 & 0.3 & 0.4 \\
%     (0.5, 1.0) & 0.1 & 0.2 & 0.3 & 0.4  \\
%     (0, 0.25) \& (0.75,1) & 0.1 & 0.2 & 0.3 & 0.4 \\
%     (0.1, 0.4) \& (0.6,0.9) & 0.1 & 0.2 & 0.3 & 0.4  \\
%     \bottomrule
% \end{tabular}
% \caption{Zero-shot conditional PPL on a variety of datasets}
% \end{table}


% \begin{table}[h]
% \centering
% \begin{tabular}{lccccc}
%     \toprule
%     Model & LAMBADA & WikiText2 & PTB & WikiText103 & 1BW\\
%     \midrule
%     TGPT(310M) & 0.1 & 0.2 & 0.3 & 0.4 & 0.5  \\
%     GPT2-Medium & 0.1 & 0.2 & 0.3 & 0.4 & 0.5  \\
%     SEDD-Medium & 0.1 & 0.2 & 0.3 & 0.4 & 0.5  \\
%     \bottomrule
% \end{tabular}
% \caption{Zero-shot unconditional PPL on a variety of datasets}
% \end{table}


% \begin{table}[h]
% \centering
% \begin{tabular}{lcccc}
%     \toprule
%     \multirow{2}{*}[-0.2em]{Mask ranges} & \multicolumn{4}{c}{Model} \\
%     \cmidrule(lr){2-5}
%     ~ & GPT2 & SEDD(32) & SEDD(64) & TGPT\\
%     \midrule
%     (0.25,0.75) & 0.1 & 0.2 & 0.3 & 0.4 \\
%     (0.5, 1.0) & 0.1 & 0.2 & 0.3 & 0.4  \\
%     (0, 0.25) \& (0.75,1) & 0.1 & 0.2 & 0.3 & 0.4 \\
%     (0.1, 0.4) \& (0.6,0.9) & 0.1 & 0.2 & 0.3 & 0.4  \\
%     \bottomrule
% \end{tabular}
% \caption{Evaluation of text infilling performance using the MAUVE score}
% \end{table}





% \subsection{NAR Denoising Performance}

% \begin{table}[h]
% \centering
% \begin{tabular}{lccccccccc}
%     \toprule
%     \multirow{2}{*}[-0.2em]{Model} & \multicolumn{3}{c}{Long Span Mask} & \multicolumn{3}{c}{Short Span Mask} & \multicolumn{3}{c}{Uniform Mask} \\
%     \cmidrule(lr){2-4}
%     \cmidrule(lr){5-7}
%     \cmidrule(lr){8-10}
%     ~ & 0.15 & 0.25 & 0.5 & 0.15 & 0.25 & 0.5 & 0.15 & 0.25 & 0.5\\
%     \midrule
%     BERT(Large) & 21.33 & 20.89 & 21.10 & 22.11 & 22.06 & 21.45 & 22.09 & 21.89 & 21.93\\
%     TGPT(318M) & 19.99 & 19.80 & 19.95 & 13.26 & 13.58 & 14.48 & 8.83 & 9.81 & 12.62\\
%     TGPT(bf16) & 20.75 & 20.38 & 20.75 & 13.81 & 14.25 &  15.19  & 9.50  & 10.44 & 13.38\\
%     \bottomrule
% \end{tabular}
% \caption{Conditional PPL on WikiText103 Test Set (seq$\_$length=1024, no overlap, 270 samples). Long Span Mask (mean$\_$span$\_$length=50). Short Span Mask (T5-style, mean$\_$span$\_$length=3). Uniform Mask(BERT-style). Mask ratio (0.15,0.25,0.5)}
% \end{table}


\section*{Impact Statement}

% Authors are \textbf{required} to include a statement of the potential 
% broader impact of their work, including its ethical aspects and future 
% societal consequences. This statement should be in an unnumbered 
% section at the end of the paper (co-located with Acknowledgements -- 
% the two may appear in either order, but both must be before References), 
% and does not count toward the paper page limit. In many cases, where 
% the ethical impacts and expected societal implications are those that 
% are well established when advancing the field of Machine Learning, 
% substantial discussion is not required, and a simple statement such 
% as the following will suffice:

This paper presents work whose goal is to advance the field of 
Machine Learning. There are many potential societal consequences 
of our work, none which we feel must be specifically highlighted here.

\section*{Acknowledgements}

This work was funded in part by the National Science and Technology Major Project (2022ZD0114902), the DARPA ANSR program under award FA8750-23-2-0004, the DARPA CODORD program under award HR00112590089, the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy - EXC 2075 – 390740016, NSF grant \#IIS-1943641, and gifts from Adobe Research and Amazon. We acknowledge the support of the Stuttgart Center for Simulation Science (SimTech). MN thanks IMPRS-IS (International Max Planck Research School for Intelligent Systems) for the support.





% The above statement can be used verbatim in such cases, but we 
% encourage authors to think about whether there is content which does 
% warrant further discussion, as this statement will be apparent if the 
% paper is later flagged for ethics review.


\bibliography{refs}
\bibliographystyle{icml2025}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIx
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
% \section{You \emph{can} have an appendix here.}

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.

\section{Additional Details of Tracformers}
\label{appx:model-details}

This section introduces the design details of Tracformers.

\boldparagraph{Sparse Attention Masks of the Encoder.}
As described in \cref{sec:nar_generation}, a prefix encoder and a suffix encoder are used for both CAR and AC generation. Specifically, the attention mask for each layer is the sparsified version of \cref{eq:sp-attn-mask} such that each token attends to at most $N_{\text{max}}$ tokens. Formally, the attention map $\bfm_{\text{enc}}^{l}$ of layer $l \in [L]$ in the prefix encoder is given by (denote $a = t - 2^{l-1}, d = \left\lfloor \frac{2^{l-1}}{N_{\text{max}}-1} \right\rfloor, b = t$):
    \begin{align*}
        m_{t,t'}^{\text{enc},l} = \begin{cases}
            1 & \text{if~} t' \in S_{t}^{l}, \\
            0 & \text{otherwise},
        \end{cases}
        \; \text{~where~}
        S_{t}^{l} = 
        \begin{cases} 
            \{t' \in \mathbb{Z} \mid t - 2^{l-1} \leq t' \leq t, t' \geq 1\}, & \text{if } 2^{l-1} \leq N_{\text{max}}, \\
            \{a, a + d, a + 2d, \dots, b\} \cap [T], & \text{if } 2^{l-1} > N_{\text{max}}.
        \end{cases}
    \end{align*}
As shown in \cref{sec:nar_generation}, each feature $\h_{t}^{\text{enc},l}$ in the suffix encoder covers the $2^{l}$ succeeding variables starting from $X_{t}$:
    \begin{align*}
        \phi_{t}^{l} = \{t' : t' \leq T, 0 \leq t' - t < 2^{l}\},
    \end{align*}
\noindent which is analogous to \cref{eq:example-scope}. Similar to the prefix encoder, we use the following sparse attention mask to implement the suffix encoder (denote $a = t, d = \left\lfloor \frac{2^{l-1}}{N_{\text{max}}-1} \right\rfloor, b = t + 2^{l-1}$):
    \begin{align*}
        m_{t,t'}^{\text{enc},l} = \begin{cases}
            1 & \text{if~} t' \in S_{t}^{l}, \\
            0 & \text{otherwise},
        \end{cases}
        \; \text{~where~}
        S_{t}^{l} = 
        \begin{cases} 
            \{t' \in \mathbb{Z} \mid t \leq t' \leq t + 2^{l-1}, t' \leq T\}, & \text{if } 2^{l-1} \leq N_{\text{max}}, \\
            \{a, a + d, a + 2d, \dots, b\} \cap [T], & \text{if } 2^{l-1} > N_{\text{max}}.
        \end{cases}
    \end{align*}

\boldparagraph{Sparse Attention Masks of the Encoder.}
As discussed in \cref{sec:nar_generation}, the decoder cross-attends to features in both the prefix encoder and the suffix encoder, using the mask in \cref{eq:dec-mask,eq:dec-mask-suffix}, respectively.

\boldparagraph{Encoder Blocks.}
Each encoder block contains an attention layer and an FFN following the standard design. Skip connections and layer normalization layers are used.
    \begin{align*}
        \hat{\h}_{1:T}^{\text{enc},l} & = \h_{1:T}^{\text{enc},l-1} + \texttt{Attn}_{l} ( \texttt{LayerNorm}_{l,1} (\h_{1:T}^{\text{enc},l-1}) ), \\
        \h_{1:T}^{\text{enc},l} & = \hat{\h}_{1:T}^{\text{enc},l} + \texttt{FFN}_{l} ( \texttt{LayerNorm}_{l,2} (\hat{\h}_{1:T}^{\text{enc},l}) ).
    \end{align*}
In the attention layers, we adopt the Rotary positional encoding \citep{su2024roformer} to encode information about relative positions between feature embeddings. This is widely adopted in NAR models such as discrete diffusion models \citep{lou2023discrete,sahoo2024simple}.

For the FFN, we follow the design of GPT-2 and use a two-layer fully connected neural network with GeLU \citep{hendrycks2016gaussian} activation. The input and output dimensions are both the embedding dimension $d$ and the latent dimension size is $4 \times d$.

\boldparagraph{Decoder Blocks.}
Each decoder block consists of a cross-attention layer and an FFN:
    \begin{align*}
        \hat{\h}_{1:T}^{\text{dec},l} & = \h_{1:T}^{\text{dec},l-1} + \texttt{CrossAttn}_{l} ( \texttt{LayerNorm}_{l,1} (\h_{1:T}^{\text{dec},l-1}), \texttt{LayerNorm}_{l,2} (\h_{1:T}^{\text{enc},L-l+1}) ), \\
        \h_{1:T}^{\text{dec},l} & = \hat{\h}_{1:T}^{\text{dec},l} + \texttt{FFN}_{l} ( \texttt{LayerNorm}_{l,3} (\hat{\h}_{1:T}^{\text{dec},l}) ).
    \end{align*}
Similar to the encoder, we also adopt the Rotary positional encoding in cross-attention layers. We set the input features to the decoder to zero (\ie $\h_{1:T}^{\text{dec},0} = \mathbf{0}$). The FFNs are the same as in the encoder blocks.

\section{Comparison of NAR Architectures in Small-Scale Experiments}
\subsection{Training Mask Strategy}
\label{appx:span-mask}

The span masking strategy $\calP_{C}$ used in our experiments is implemented using the following algorithm: 

\begin{algorithm}[H]
\caption{Span Masking Strategy}
\label{alg:span_mask}
\begin{algorithmic}[1]
\State \textbf{Input:} Sequence length $L$, mask probability $p$, mean span length $m$, distribution type (\text{Geometric} or \text{DLogistic}), $\sigma$
\State Initialize $num\_to\_mask = \max(1, \text{round}(p \cdot L))$, $num\_masked = 0$, $blank\_ids = []$, $spans = []$
\While{$num\_masked < num\_to\_mask$}
    \State Sample a random $start$ index in $[0, L)$
    \If{$start$ overlaps with an existing span in $spans$}
        \State \textbf{continue}
    \EndIf
    \If{distribution == \text{Geometric}}
        \State Sample $span\_length \sim \text{Geometric}(\mu=m)$ \Comment{Geometric distribution with mean $\mu$}
    \ElsIf{distribution == \text{DLogistic}}
        \State Sample $span\_length \sim \text{DLogistic}(\mu=m,\sigma=\sigma)$ \Comment{Dlogistic distribution with mean $\mu$ and std $\sigma$}
    \EndIf
    \State $end = \min(start + span\_length, L)$
    \If{Overlap with existing spans detected}
        \State \textbf{continue}
    \EndIf
    \State Append $(start, end)$ to $spans$, update $blank\_ids$ and $num\_masked$
\EndWhile
\If{$num\_masked > num\_to\_mask$}
    \State Trim the last span to ensure exact masking ratio
\EndIf
\State \textbf{Return:} $spans, blank\_ids$
\end{algorithmic}
\end{algorithm}


Following \cref{eq:car-eq}, the CAR training process involves applying the span masking strategy to corrupt input sequences and training the model to predict the original tokens based on the provided context. Specifically, at each training iteration, the generate\_span\_mask function is used to create masked spans within the input sequence. The masking process ensures that 50\% of the tokens (based on a mask probability of 0.5) are selected in spans, where the span length is sampled from either a \texttt{Geometric} or \texttt{DLogistic} distribution. For training in \cref{sec:arch_comparison}, we use the \texttt{Geometric} distribution with a mean span length of $\mu=50$. The corrupted input sequence, along with a mask indicating the positions of the masked tokens, is then passed to the model. The model is trained to minimize the negative log-likelihood of the original tokens at the masked positions, conditioning on the unmasked tokens as context.


\subsection{Model and Training Configurations}
\label{appx:exp-wiki}

\begin{table}[h]
\centering
\begin{minipage}{0.48\textwidth}
\centering
\caption{Training hyperparameters for CAR and AC tasks.}
\label{tab:training-hyperparams}
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
\multicolumn{2}{l}{\textit{Training Setup}} \\
Sequence length & 1024 tokens \\
Batch size & 256 \\
Training steps & 30,000 \\
\midrule
\multicolumn{2}{l}{\textit{Optimizer}} \\
Optimizer & AdamW \\
$\beta_1$ & 0.9 \\
$\beta_2$ & 0.95 \\
Weight decay & 0.1 \\
\midrule
\multicolumn{2}{l}{\textit{Learning Rate Schedule}} \\
Initial learning rate & $6 \times 10^{-4}$ \\
Final learning rate & $6 \times 10^{-5}$ \\
Learning rate decay & Cosine schedule \\
Warmup steps & 1,000 \\
\bottomrule
\end{tabular}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
\centering
\caption{Hyperparameters of Tracformer for the Small-scale Experiments.}
\label{tab:tracformer-hyperparams}
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
\multicolumn{2}{l}{\textit{Structure Parameters}} \\
Block size & 1024 \\
Number of encoder layers & 10 \\
Number of decoder layers & 10 \\
Max attended tokens ($N_{\text{max}}$) & 16 \\
\midrule
\multicolumn{2}{l}{\textit{Attention Parameters}} \\
Number of attention heads & 9 \\
Embedding dimension ($d$) & 576 \\
\midrule
\multicolumn{2}{l}{\textit{Other Parameters}} \\
Dropout rate & 0.1 \\
\bottomrule
\end{tabular}
\end{minipage}
\end{table}

\boldparagraph{Training Hyperparameters.}
For both CAR and AC training tasks, the sequence length is set to 1024 tokens, with a batch size of 256. The models are optimized using AdamW with $\beta_1=0.9$, $\beta_2=0.95$, and a weight decay of 0.1. The initial learning rate is set to $6 \times 10^{-4}$ and follows a cosine decay schedule, with 1,000 warmup steps to stabilize the early training phase. The final learning rate is $6 \times 10^{-5}$. Training is conducted for 30,000 steps. 

% \begin{table}[t]
% \centering

% \end{table}

\begin{table}[t]
\centering
\caption{Comparison of the model size for small-scale experiments.}
\label{tab:wiki-model-size}
\begin{tabular}{lc}
\toprule
\textbf{Model} & \textbf{Full Model Size} \\
\midrule
Tracformer & 109M \\
BART & 138M \\
BERT & 149M \\
BERT-bidir & 170M \\
\bottomrule
\end{tabular}
\end{table}

\boldparagraph{Hyperparameters of Tracformer.}
As listed in \cref{tab:tracformer-hyperparams}, for both CAR and AC training tasks, Tracformer is implemented with a 10-layer encoder-decoder architecture, maintaining a block size (\ie maximum sequence length) of 1024 tokens. It utilizes sparse multi-scope attention with a constraint of 16 attended tokens per step, allowing for efficient context aggregation while keeping computational cost manageable. The decoder operates with a maximum stride of 1024 tokens, ensuring global context encoding. The model is configured with 9 attention heads and an embedding dimension of 576. A dropout rate of 0.1 is applied to mitigate overfitting during training.

\boldparagraph{Hyperparameters of the Baseline Models.}
As the CAR baseline, BART adopts an 8-layer encoder-decoder architecture with 9 attention heads and an embedding dimension of 576. This leads to a total of 138M parameters.

For the AC generation experiments, we use BERT as the encoder-only baseline with full attention, employing 10 layers, 12 attention heads, and an embedding dimension of 768. This configuration ensures a fair comparison by maintaining a similar model size to Tracformer. Additionally, we include BERT-bidir, which encodes bidirectional context using two separate encoders: a classical forward AR encoder with causal attention and a reverse AR encoder. While structurally similar to Tracformer’s prefix and suffix encoders, BERT-bidder relies on dense causal attention and lacks the sparse multi-scope self-attention mechanism that enhances Tracformer’s efficiency in handling conditional generation. BERT-bidder is configured with 12 layers, 12 attention heads, an embedding dimension of 768, and a dropout rate of 0.1. After encoding, the final-layer features from both encoders are concatenated and passed through a two-layer MLP to produce the final representation, allowing the model to integrate information from both directions before prediction. The MLP consists of two fully connected layers ($\text{config.n\_embd}$ is the embedding size of the model):
    \begin{align*}
    \text{mlp} = \text{nn.ModuleList}([
        & \text{nn.Linear}(\text{config.n\_embd} \times 2, \text{config.n\_embd} \times 4), \\
        & \text{GELU()}, \\
        & \text{nn.Linear}(\text{config.n\_embd} \times 4, \text{config.n\_embd}), \\
        & \text{GELU()}
    ])
    \end{align*}
\cref{tab:wiki-model-size} shows the parameter count of the adopted baseline models.

\subsection{Additional Evaluation Details and Results}
\label{appx:exp-eval-details}

\boldparagraph{Conditional PPL under Short Span Masking.}
\cref{fig:ac-car-results2} contains complementary results of \cref{fig:ac-car-results}.

\begin{figure*}[h]
\centering
\includegraphics[width=0.6\textwidth]{Figs/fig-ac-car-results2.pdf}
\caption{Conditional perplexity (PPL) evaluated on the WikiText-103 validation set. The span length follows a \(\text{Geometric}(\mu=3)\) distribution, while the mask ratio varies between 0.1 and 0.9. The sequence length is set to 1024, which aligns with the training setup. (a) Results for the CAR task. (b) Results for the AC task. Tracformer consistently outperforms all baselines.}
\label{fig:ac-car-results2}
\end{figure*}

\boldparagraph{Metrics for CAR Text Infilling.}
For the CAR text infilling task, a sequence length of 128 is used. To compute MAUVE and BERT scores, we evaluate 1,000 text sequences from the WikiText-103 validation set across all methods. Each prompt generates five samples, resulting in 5,000 generated sequences.

\begin{table}[t]
    \centering
    \caption{CAR Text infilling performance evaluated on 1BW and Lambada Datasets.}
    \label{tab:multi-dataset-infilling}
    \vspace{0.1em}

    \renewcommand{\arraystretch}{1.1}
    % \setlength{\tabcolsep}{0.24em}
    \centering
    \scalebox{0.86}{
    \begin{tabular}{ccccc}
        \toprule
        \multirow{2}{*}{Mask ranges} & \multicolumn{2}{c}{MAUVE ($\uparrow$)} & \multicolumn{2}{c}{BERT Score ($\uparrow$)} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5}
        & Tracformer & BART & Tracformer & BART  \\
        \midrule
        \multicolumn{5}{c}{1BW Dataset} \\
        \midrule
        $\scalebox{0.78}{[0.25,0.75]}$ & 0.974 & \textbf{0.975} & \textbf{0.471} & 0.446 \\
        $\scalebox{0.78}{[0.50,1.00]}$ & \textbf{0.421} & 0.384 & \textbf{0.478} & 0.476 \\
        $\scalebox{0.78}{[0.10,0.40] \& [0.60,0.90]}$ & \textbf{0.992} & 0.979 & \textbf{0.377} & 0.334 \\
        $\scalebox{0.78}{[0.00,0.40] \& [0.50,0.80]}$ & \textbf{0.987} & 0.977 & \textbf{0.272} & 0.230 \\
        $\scalebox{0.78}{[0.00,0.25] \& [0.75,1.00]}$ & \textbf{0.379} & 0.375 & \textbf{0.450} & 0.425 \\
        $\scalebox{0.78}{[0.00,0.10] \& [0.20,0.50] \& [0.70,1.00]}$ & 0.346 & \textbf{0.364} & \textbf{0.270} & 0.231 \\
        \midrule
        \multicolumn{5}{c}{Lambada Dataset} \\
        \midrule
        $\scalebox{0.78}{[0.25,0.75]}$ & 0.897 & \textbf{0.913} & \textbf{0.301} & 0.289 \\
        $\scalebox{0.78}{[0.50,1.00]}$ & \textbf{0.024} & 0.016 & \textbf{0.344} & 0.320 \\
        $\scalebox{0.78}{[0.10,0.40] \& [0.60,0.90]}$ & \textbf{0.714} & 0.693 & \textbf{0.178} & 0.153 \\
        $\scalebox{0.78}{[0.00,0.40] \& [0.50,0.80]}$ & 0.804 & \textbf{0.839} & \textbf{0.077} & 0.056 \\
        $\scalebox{0.78}{[0.00,0.25] \& [0.75,1.00]}$ & \textbf{0.037} & 0.024 & \textbf{0.322} & 0.309 \\
        $\scalebox{0.78}{[0.00,0.10] \& [0.20,0.50] \& [0.70,1.00]}$ & \textbf{0.023} & 0.014 & \textbf{0.068} & 0.041 \\
        \bottomrule
    \end{tabular}}
    \vspace{-0.4em}
\end{table}

\boldparagraph{CAR Text Infilling Results on Other Datasets.}
Text-infilling performance on the 1BW and LAMBADA datasets are given in \cref{tab:multi-dataset-infilling}.


\section{Details of Large-Scale Experiments on WebText/OpenWebText}
\label{appx:owt-exp-details}

\subsection{Training Mask Strategy}

To scale Tracformer to OpenWebText while balancing both conditional and unconditional generation performance, we adopt a \textbf{mixed masking strategy}. This approach integrates contextual autoregressive (CAR) training with an autoregressive (AR) objective by varying the masking strategy probabilistically during training.

At each training iteration, we select from the following masking strategies:
\begin{itemize}
    \item \textbf{Unconditional training (30\%)}: No context is provided, and the model learns in a fully autoregressive (AR) manner, simulating traditional language modeling.
    \item \textbf{High-span masking (20\%)}: A high masking probability (85\%) is applied, forcing the model to rely on limited observed tokens to reconstruct missing content.
    \item \textbf{Moderate-span masking (50\%)}: A standard masking probability (50\%) is used, following the CAR objective.
\end{itemize}

The \texttt{generate\_span\_mask} function, as defined in \cref{alg:span_mask}, is used when applying high- and moderate-span masking strategies to ensure structured token corruption. However, in unconditional training, no span masking is applied, effectively reducing the task to standard autoregressive modeling. This hybrid approach allows Tracformer to retain strong unconditional generation capabilities while improving its ability to generalize to arbitrary conditional queries.

The formalized procedure for the mixed masking strategy is presented in \cref{alg:mixed_mask}.



\begin{algorithm}[H]
\caption{Mixed Masking Strategy for OpenWebText Training}
\label{alg:mixed_mask}
\begin{algorithmic}[1]
\State \textbf{Input:} Input sequence $\mathbf{x}$ of length $L=1024$, mask token ID $m_{id}$
\State Sample $r \sim \text{Uniform}(0,1)$
\If{$r < 0.3$}  \Comment{Unconditional training (AR)}
    \State $\mathbf{x}_{corrupt} = m_{id}$ (fully masked)
\ElsIf{$r < 0.5$}  \Comment{High-span masking (CAR)}
    \State $\text{spans}, \text{blank\_ids} = \text{generate\_span\_mask}(L=1024, p=0.85, m=50, \text{distribution type}=\text{Geometric})$
\Else \Comment{Moderate-span masking (CAR)}
    \State $\text{spans}, \text{blank\_ids} = \text{generate\_span\_mask}(L=1024, p=0.5, m=50, \text{distribution type}=\text{Geometric})$
\EndIf
\State $\mathbf{x}_{corrupt} = \mathbf{x}.\text{clone}()$
\State $\mathbf{x}_{corrupt}[\text{blank\_ids}] = m_{id}$
\State \textbf{Return:} $\mathbf{x}_{corrupt}$
\end{algorithmic}
\end{algorithm}



\subsection{Model and Training Configuration}

\boldparagraph{Tracformer.}
Details of the architecture and optimization procedure for Tracformer are detailed in \cref{tab:tracformer-hyperparams-owt}.

\begin{table}[h]
\centering
\caption{Hyperparameters of Tracformer for the large-scale (OpenWebText) Experiments}
\label{tab:tracformer-hyperparams-owt}
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
\multicolumn{2}{l}{\textit{Structure Parameters}} \\
Block size & 1024 \\
Number of encoder layers & 10 \\
Number of decoder layers & 10 \\
Max attended tokens ($N_{\text{max}}$) & 32 \\
\midrule
\multicolumn{2}{l}{\textit{Attention Parameters}} \\
Number of attention heads & 9 \\
Embedding dimension ($d$) & 576 \\
\midrule
\multicolumn{2}{l}{\textit{Other Parameters}} \\
Dropout rate & 0.0 \\
\midrule
\multicolumn{2}{l}{\textit{Optimization Parameters}} \\
Initial learning rate & 6e-4 \\
Terminal learning rate & 6e-5 \\
Learning rate schedule & cosine \\
\# warmup steps & 2,000 \\
\# training steps & 600,000 \\
Batch size & 480 \\
Weight decay & 1e-1 \\
Adam betas & $(0.9, 0.95)$ \\
Gradient clipping maximum norm & 1.0 \\
\bottomrule
\end{tabular}
\end{table}

\boldparagraph{SEDD.} 
We use the SEDD-small model with 169M parameters (including 90M non-embedding parameters) trained on OpenWebText. The model is accessed through HuggingFace: 
 \url{https://huggingface.co/louaaron/sedd-small}. Following the original paper \cite{lou2023discrete}, we adopt the log-linear noise schedule and the absorbing mask forward noising process.

\boldparagraph{MDLM.}
We use the MDLM model with 169M parameters (including 90M non-embedding parameters) trained on OpenWebText. The model is accessed through HuggingFace: \url{https://huggingface.co/kuleshov-group/mdlm-owt}. Following the original paper \cite{sahoo2024simple}, we adopt the log-linear noise schedule.

\boldparagraph{GPT.}
We use the GPT-2 small model with 124M parameters (including 85M non-embedding parameters). The model is obtained from HuggingFace: \url{https://huggingface.co/openai-community/gpt2}.

\subsection{Evaluation Metrics}

For all conditional generation experiments in \cref{sec:scaling_tracformer}, we use sequences of length 128 following prior work \citep{lou2023discrete,han2023ssd,gu2022diffusionlm}. Additionally, we exclude EOS tokens when evaluating conditional perplexity since the focus of these tasks is the ability of different models to generate coherent text given prompt texts.

\section{Additional Ablation Studies}
\label{appx:ablation}

\begin{table}[h]
    \centering
    \caption{Ablation results for the CAR generation task. \textit{Tracformer w/ full encoder} is an ablation model where the multi-scope encoder in the original Tracformer is replaced with dense self-attention while maintaining the encoder-decoder structure. The table presents results for two span length distributions: $\text{Geometric}(\mu=50)$ and $\text{Geometric}(\mu=10)$, evaluated under different mask ratios.}
    \label{tab:ablation-car}
    \renewcommand{\arraystretch}{1.1}
    \begin{tabular}{cccccccccc}
        \toprule
        & \multicolumn{9}{c}{Mask Ratio} \\
        \cmidrule(lr){2-10}
        Model & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.9 \\
        \midrule
        & \multicolumn{9}{c}{$\text{Geometric}(\mu=50)$} \\
        \cmidrule(lr){2-10}
        Tracformer & 445.46 & 467.03 & 478.13 & 475.65 & 459.17 & 460.53 & 447.84 & 449.77 & 564.32 \\
        Tracformer w/ full encoder & 443.65 & 462.90 & 475.25 & 473.85 & 458.77 & 462.30 & 452.11 & 455.23 & 574.73 \\
        \midrule
        & \multicolumn{9}{c}{$\text{Geometric}(\mu=10)$} \\
        \cmidrule(lr){2-10}
        Tracformer & 165.07 & 178.77 & 181.89 & 178.79 & 185.38 & 193.40 & 219.13 & 288.91 & 509.35 \\
        Tracformer w/ full encoder & 160.39 & 174.64 & 177.88 & 176.36 & 183.01 & 191.53 & 217.69 & 288.45 & 519.73 \\
        \bottomrule
    \end{tabular}
    \vspace{-0.4em}
\end{table}



% \begin{table}[h]
%     \centering
%     \caption{\xuejie{need captions}}
%     \label{tab:ablation-nar-50}
%     \renewcommand{\arraystretch}{1.1}
%     % \setlength{\tabcolsep}{0.40em}
%     \centering
%     % \scalebox{0.76}{
%     \begin{tabular}{cccccccccc}
%         \toprule
%         Mask prob & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.9 \\
%         \midrule
%         Tracformer & 445.46 & 467.03 & 478.13 & 475.65 & 459.17 & 460.53 & 447.84 & 449.77 & 564.32 \\
%         Tracformer w/ full encoder & 443.65 & 462.90 & 475.251 & 473.85 & 458.77 & 462.30 & 452.11 & 455.23 & 574.73 \\
%         \bottomrule
%     \end{tabular}% }
%     \vspace{-0.4em}
% \end{table}

% \begin{table}[h]
%     \centering
%     \caption{\xuejie{need captions}}
%     \label{tab:ablation-nar-10}
%     \renewcommand{\arraystretch}{1.1}
%     % \setlength{\tabcolsep}{0.40em}
%     \centering
%     % \scalebox{0.76}{
%     \begin{tabular}{cccccccccc}
%         \toprule
%         Mask prob & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.9 \\
%         \midrule
%         Tracformer & 165.07 & 178.77 & 181.89 & 178.79 & 185.38 & 193.40 & 219.13 & 288.91 & 509.35 \\
%         Tracformer w/ full encoder & 160.39 & 174.64 & 177.88 & 176.36 & 183.01 & 191.53 & 217.69 & 288.45 & 519.73 \\
%         \bottomrule
%     \end{tabular}% }
%     \vspace{-0.4em}
% \end{table}


To further evaluate the impact of Tracformer's \textit{multi-scope attention}, we conduct an ablation study by replacing it with \textit{full causal attention}. In this modified architecture, the prefix encoder employs \textit{classical dense causal self-attention}, while the suffix encoder uses \textit{reverse causal attention}, where each token attends to all subsequent tokens within the layer. Apart from this modification, the encoder-decoder structure remains unchanged, ensuring a fair comparison.

Tables \ref{tab:ablation-car} present the AC generation performance under two different masking strategies: \textit{Geometric(50)} with a mean span length of 50, aligned with the training mask length, and \textit{Geometric(10)} with a mean span length of 10, which deviates from the training distribution. The results show that at \textbf{low mask ratios} (i.e., when more context is available), both models achieve comparable performance, with the ablation model exhibiting a slight advantage. However, as the \textbf{mask ratio increases} and the task becomes more challenging, Tracformer with \textit{multi-scope attention} significantly outperforms the ablation model, demonstrating superior generalization under high-uncertainty conditions. 
% This suggests that while dense attention may help in easier scenarios where ample context is provided, \textit{multi-scope attention} plays a crucial role in handling more complex masked sequence reconstructions.

Another key observation is that replacing multi-scope attention with full attention introduces a notable \textbf{efficiency drop}. The ablation model incurs higher computational costs due to the dense attention mechanism, making it less scalable compared to Tracformer. This highlights the practical advantage of Tracformer's sparse \textit{multi-scope self-attention}, which achieves a favorable balance between performance and efficiency.

Finally, it is worth noting that despite replacing multi-scope attention with full attention, the ablation model still \textbf{significantly outperforms the previous baseline, BERT-bidir}, which has a similar number of parameters. This underscores the importance of Tracformer's specialized \textit{encoder-decoder design}, beyond just its attention mechanism. The architectural improvements introduced in Tracformer contribute substantially to its ability to handle AC generation tasks, making it a more effective approach compared to conventional NAR models.

% Overall, these findings confirm that while full attention may provide slight advantages in low-difficulty settings, \textit{multi-scope attention} is crucial for ensuring robust generalization in more challenging masked generation tasks. Furthermore, the results reinforce that Tracformer’s unique encoder-decoder structure, independent of its attention mechanism, is a key factor in achieving state-of-the-art performance.


\section{Training and Inference Efficiency of Tracformers}
\label{appx:efficiency}

\boldparagraph{Training Efficiency.}
As shown in \cref{sec:encoder} and \cref{appx:model-details}, given a Tracformer with $L$ layers, input length $T$, and $N_{\text{max}}$, the computation cost of the encoder is $\bigO (L \cdot T \cdot N_{\text{max}})$, which is linear \wrt $T$. We note that a custom kernel implementation is needed to fully take advantage of this sparse attention and leave it to future work.

According to the decoder attention mask (Eqs.~(\ref{eq:dec-mask})~and~(\ref{eq:dec-mask-suffix})), the number of tokens required to attend to is exponentially smaller for initial decoder layers. Suppose $L = \bigO(\log T)$, then the total computation cost for the decoder is
    \begin{align*}
        \underbrace{\sum_{l=1}^{L} \bigO (T \cdot 2^{l})}_{\text{Cross~attention~layers}} + \underbrace{\bigO (T \cdot L)}_{\text{FFNs}} = \bigO(T^2 + T \cdot L).
    \end{align*}
This improves upon the cost $\bigO (T^2 \cdot L)$ in classic decoder-only Transformers.

\boldparagraph{Inference Efficiency.}
We give an example of using KV-caching techniques in Tracformers designed for CAR generation (\ie Fig.~\ref{fig:tgpt-gen-paradigms}(a)). Given context $\x_{C}$, we first run the suffix encoder for all tokens by feeding the $\texttt{<MASK>}$ token to every token $t \not\in C$. We then autoregressively decode the remaining tokens following the order $\x_{1}, \x_{2}, \dots, \x_{T}$. When decoding $\x_{t}$, we first compute the $(t\!-\!1)$-th feature in each layer of the prefix encoder (\ie $\{\h_{t-1}^{\text{enc},l}\}_{l=0}^{L}$). In the attention layers, we use KV caches for preceding tokens to avoid re-computing their features. Then, we compute the $t$-th feature in each layer of the decoder (\ie $\{\h_{t}^{\text{dec},l}\}_{l=0}^{L}$) to decode $\x_{t}$. This is possible because we have already computed features $\{\h_{1:t-1}^{\text{enc},l}\}_{l=0}^{L}$ in the prefix encoder and all features in the suffix encoder. Since there is no self-attention layer in the decoder, KV caching is not needed.


\section{Text Samples from Tracformer}
\label{appx:text-samples}
\subsection{Unconditional Generation}
The following are randomly selected unconditional text samples generated by a Transformer model with a sequence length of 128:

\begin{quote}
    in the fishing hole outburst at or before the time that the whale had successfully spawned. Indeed, a relatively close encounter may have emboldened the whale to pull the mylar through the sand and shelled it, as she was able to in this example. Interestingly, the flattening events which resulted in the. Whale cutluring was caused by the thermal spark that resulted from the spin and instability of the high (or low) pressure, thus inducing the ripening state. The more the thermal increase of the high (or low) temperature was offset from the opposite direction along seismic quadrants. Interestingly, the evaporation rates at the ...
\end{quote}

\begin{quote}
     the Antiquarian League of Massachusetts (ASML) works with the Massachusetts League of New Hampshire Bar Associations to enforce the law through nicety and compromise. Unlike the League of Massachusetts Missouri, and its members are independent, their committees are almost entirely autonomous, with sole jurisdiction solely on the matters of labor rights for inmates. They keep offsite their legal animosity to the state, only restricting their manifestations to marking weeks in advance.And, in an unusual turn, they hold committees to run stretchin, direct sensitivity exercises of a ceremonies-like kind. These bundles of letters in the verse, in order to mark the time during which a batch ...
\end{quote}

\begin{quote}
    The 90-day player initiative on June 2 also launched across all seven parks and recreational areas in the National Park system in Central Sierras. There, children along with the government hamstrung the operation, an action hoped to be a long-term solution to the larger problem of ecological loss along the Central Sierras. The USDA estimated that 20 275-acre sites on the Hudson River basin are almost colliding, forming a “very difficult interplay of life and conflict.” They include the East Bay Schuylkill River (230 versus the 81-acre Wyoming Standard Ditch), the ...
\end{quote}

\begin{quote}
    was directed against the convict institution. Augustine said the parish sincerely believed the practice was against the 100 U.S. Statute. In fact if that is the practice, we would not call it a crime that was committed in Phillippines – we were talking about out of state institutions. And we don’t do that. So if the confession was confessionally formed it is in the whole, full church confessional as we described it.”$\backslash$n$\backslash$nAgain, the most surprising day of the probation hearing was the fact the single author testified the confession was genuine.$\backslash$n$\backslash$nThe principal tried to inscribe the patriarch, ...
\end{quote}

\begin{quote}
     I still see themes that clouds up uncertainty about the work of first Y.$\backslash$n$\backslash$nInterestingly, note how Elsa feels an inability to see how far to go on her journey in those citymarish dark castles. Darkness isn’t the right word for her solution to the riddle – the life of an urban teen slowly dissolving behind the clouds, every shadow suddenly overwhelmed or broken, only to be replaced by a more believable and understandable life. The crowding of darkness adds to the disorder. Of course she doesn’t expect Snow to go though, and Elsa will have to find her own answers as to how far ...
\end{quote}

\begin{quote}
     TalkTalk Studios production. “We’re stellar!”!$\backslash$n$\backslash$nAt the time of the movie’s release, there were quite a few references to a 5.0 rating and 60\% top ballerinas, plus amusing examples of mixed reviews and negative news coverage. “We really appealed to the social media revolution because I think that’s really got to get people excited and care about their content: whether they like it or not,” producer Jill Mullin has explained.$\backslash$n$\backslash$nA downhill slope has been one of the biggest tickets for Sony’s likes-on-rumours
\end{quote}


\subsection{Conditional Generation}
The following are randomly selected conditional text samples generated by a Transformer model with a sequence length of 128. The prompt texts are bolded and in blue:

\begin{quote}
    \textcolor{blue}{track-two diplomacy between U.S. and Soviet officials and nuclear scientists, which helped lead to the Comprehensive Test Ban Treaty. MacArthur grantees also helped develop} real alternatives to the IAEA, such as providing uranium to New Zealand. The centerpiece was the launch of a satellite called WUSA.$\backslash$n$\backslash$nFinances of the IAEA declined. The international community supports the cessation of proliferation, nuclear nonproliferation, and proliferation reduction, and it has improved the coordination of \textcolor{blue}{security policy on issues like nonproliferation and arms control.But more recently, they have borne the brunt of its economic decline, enduring lower wages}
\end{quote}

\begin{quote}
    \textcolor{blue}{and to ascertain its peaceful means of production.Mugabe has ruled Zimbabwe since independence from Britain in 1980 but faced an unprecedented challenge in Saturday's elections because of} the unrest across his country.$\backslash$n$\backslash$nFrom newly jailed opposition leader Eldion Oliverine for the Republic of Zimbabwe to the conservative populist Robert Vibert for the ruling party, the verdict could mark a “new beginning” for Zimbabwe, which had once been its own country.$\backslash$n$\backslash$nThe verdict applies to \textcolor{blue}{brain cancer, O. Wayne Corley, senior shareholder in the McNair Law Firm, announced.Another question that seems never to have been raised is that}
\end{quote}

\begin{quote}
    Powerton Historic District. Nearly 1 million square feet may be taken care of by the Virginia Historical Commission, say officials, but the built-design preferred by the Lee administration would have led to \textcolor{blue}{up to six months.Thousands of jobs are threatened, and towns will lose part of their economic base.OK, so} today is time for some Aliens Interviews: Episode 5 – Gruff Ed Beeton interview.$\backslash$n$\backslash$nAs you might expect but a few comments are here:$\backslash$n$\backslash$nTerm Ear Ham Mick \textcolor{blue}{Deputy Assistant to the President, Mr. Ballentine was Special Assistant to the President for Legislative Affairs, where he focused on energy}
\end{quote}

\begin{quote}
     full-time practice for the Dynamo with one of their available options to make his debut from the bench is looking likely to be the more familiar striker. The way players are used to it is \textcolor{blue}{not the "main factor," Roden said.If there are rights, it is too little.He had been in} San Antonio since the winter of 2010 as a civil servant with a pension to return to Brazil.$\backslash$n$\backslash$n"Brazil is the kind of economy that opened up variations in economic conditions and strengthened the \textcolor{blue}{key refinery centers," said Jim Rouiller, meteorologist with private weather forecaster Planalytics.Headquartered in Basking}
\end{quote}

\begin{quote}
    \textcolor{blue}{at a wavelength of 13 cm show no evidence for water ice} (Fig. 16(j)). The middle crust of the pixel is depressurised by around 0.3 Å before a crystalline bath and column. The software eliminates any liquid after residual \textcolor{blue}{ice, down to the image resolution of 10 m per pixel.$\backslash$n On November 15, 2008, a 34-kg}, 5.2-pound airborne "mysterious satellite" appears on high resolution radar screen at altimeter branches at the Pangaea solar array observatory on Oak Flat, California, with a radar \textcolor{blue}{altimeter, video imaging system, and a mass spect}
\end{quote}

\begin{quote}
    \textcolor{blue}{ramuros, were destroyed but after the war, reconstruction took place.$\backslash$n In 1948, President Elpidio Quiroga set in motion a program that could reshape Philippine} history. The concept was called “The Bilateral Relations with \textcolor{blue}{Manila, created in 1938 by former President Manuel L. Quezon, which was named after him. The move ended any implementation} first created by then-President Marcos.$\backslash$n$\backslash$nIn 1945, the Bilateral Relations obscure evolved into inter-provincial relations. Manila and Bilateral Relations were also linked to the Bilateral Relations.$\backslash$n$\backslash$nThe Bilateral relations were based on strategic, tactical
\end{quote}

\begin{quote}
    is difficult to make sense of. Most researchers think that terrestrial vertebrae are made of tendon, but there are several proposed hypotheses.1 Two proposed vertebrae, which sometimes don't \textcolor{blue}{such as Sclerothorax and Eryops that may have been at least partly terrestrial also have long neural spines on top of their vertebrae that would have stabilized the spine} and required reattachment to form a non-spine rest part.3 Another speculation involving the placement of vertebrae in a notch below the flexic muscles on the back of the spine would seem a more plausible explanation.3 Another proposed vertebrae
\end{quote}

\section{Proof of Conditional NELBO for MDLM}
\label{appx:conditional-elbo}

In this section, we generalize the unconditional negative evidence lower-bound (NELBO) derivation in \citet{sahoo2024simple} to the conditional case. The resultant formula is then used to compute the conditional perplexity of baseline discrete diffusion models in various settings. First, we derive the expression for conditional NELBO. We then examine and simplify the various terms within the conditional NELBO. Finally, we present the final expression for the conditional NELBO.

This proof builds upon and extends the results presented in \citet{sahoo2024simple}. Equations from the original paper are frequently referenced, and readers are encouraged to refer to the original paper for a deeper understanding of the foundational concepts and equations discussed.

\subsection{Derivation of the Conditional NELBO Expression}

For a sequence of $L$ tokens, let $F = \{1, 2, \dots, L\}$ denote the set of all indices in the sequence. Let $S \subset F$ be the set containing all and only the indices of the given tokens. In this proof, we denote the entire sequence as $\x^F$ (equivalent to $\x_{1:L}$ in \citet{sahoo2024simple}), the given tokens as $\x^S$, and the unknown tokens as $\x^{F - S}$. Following the definitions in \citet{sahoo2024simple}, we define $s(i) = (i - 1) / T$ and $t(i) = i / T$, and omit the $i$ from $t(i)$ and $s(i)$.

Using the aforementioned notations, we can express the conditional negative log-likelihood as $-\log p_\theta(\x^{F-S} | \x^S)$. Now, we step into the details of deriving the conditional NELBO:
{\allowdisplaybreaks
\begin{align}
    & - \log p_\theta (\x^{F - S} | \x^S), & \nonumber \\
    & = - \log \int p_\theta (\x^{F - S}, \z_{t(0)}^F, \dots, \z_{t(T)}^F | \x^S) d\z_{t(0)}^F \dots d\z_{t(T)}^F, & \nonumber \\
    & = - \log \int q(\z_{t(0)}^F, \dots, \z_{t(T)}^F | \x^F) \frac{p_\theta (\x^{F - S}, \z_{t(0)}^F, \dots, \z_{t(T)}^F | \x^S)}{q(\z_{t(0)}^F, \dots, \z_{t(T)}^F | \x^F)} d\z_{t(0)}^F \dots d\z_{t(T)}^F, & \nonumber \\
    & \leq - \int q(\z_{t(0)}^F, \dots, \z_{t(T)}^F | \x^F) \log \frac{p_\theta(\x^{F - S}, \z_{t(0)}^F, \dots, \z_{t(T)}^F | \x^S)}{q(\z_{t(0)}^F, \dots, \z_{t(T)}^F | \x^F)} d\z_{t(0)}^F \dots d\z_{t(T)}^F, & \nonumber \\
    & = - \E_{\z^F \sim q(\cdot|\x^F)} \Bigg[\log \frac{p_\theta (\x^{F - S}, \z_{t(0)}^F, \dots, \z_{t(T)}^F | \x^S)}{q(\z_{t(0)}^F, \dots, \z_{t(T)}^F | \x^F)}\Bigg], \label{supp:mdlm:derive:z^F} \\
    & = - \E_{\z^F} \Bigg[\log \frac{p_\theta (\x^{F - S} | \z_{t(0)}^F, \x^S) p_\theta (\z_{t(T)}^F | \x^S) \prod_{i=1}^T p_\theta (\z_{s}^F | \z_{t}^F, \x^S)}{q(\z_{t(0)}^F | \x^F) \prod_{i=1}^T q(\z_{t}^F | \z_{s}^F)} \Bigg] \; \text{(note~that~} s \text{~(resp.~} t \text{)~denotes~} s(i) \text{~(resp.~} t(i) \text{))}, & \nonumber \\
    & = \E_{\z^F} \Bigg[ - \log p_\theta (\z_{t(T)}^F | \x^S) + \sum_{i=1}^T \log \frac{q(\z_{t}^F | \z_{s}^F)}{p_\theta (\z_{s}^F | \z_{t}^F, \x^S)} + \log \frac{q(\z_{t(0)}^F | \x^F)}{p_\theta (\x^{F - S} | \z_{t(0)}^F, \x^S)} \Bigg], & \nonumber \\
    & = \E_{\z^F} \Bigg[ - \log p_\theta (\z_{t(T)}^F | \x^S) + \sum_{i=1}^T \log \frac{q(\z_{s}^F | \z_{t}^F, \x^F)}{p_\theta (\z_{s}^F | \z_{t}^F, \x^S)} \nonumber \\
    & \hspace{4.14cm} + \sum_{i=1}^T \log \frac{q(\z_{t}^F | \x^F)}{q(\z_{s}^F | \x^F)} + \log \frac{q(\z_{t(0)}^F | \x^F)}{p_\theta (\x^{F - S} | \z_{t(0)}^F, \x^S)} \Bigg], \label{supp:mdlm:derive:q_decompose} \\
    & = \E_{\z^F} \Bigg[ - \log p_\theta (\z_{t(T)}^F | \x^S) + \sum_{i=1}^T \log \frac{q(\z_{s}^F | \z_{t}^F, \x^F)}{p_\theta (\z_{s}^F | \z_{t}^F, \x^S)} + \log \frac{q(\z_{t(T)}^F | \x^F)}{q(\z_{t(0)}^F | \x^F)} + \log \frac{q(\z_{t(0)}^F | \x^F)}{p_\theta (\x^{F - S} | \z_{t(0)}^F, \x^S)} \Bigg], & \nonumber \\
    & = \E_{\z^F} \Bigg[ \log \frac{q(\z_{t(T)}^F | \x^F)}{p_\theta (\z_{t(T)}^F | \x^S)} + \sum_{i=1}^T \log \frac{q(\z_{s}^F | \z_{t}^F, \x^F)}{p_\theta (\z_{s}^F | \z_{t}^F, \x^S)} - \log p_\theta (\x^{F - S} | \z_{t(0)}^F, \x^S) \Bigg], & \nonumber \\
    & = \E_{\z^F} \Bigg[ \underbrace{- \log p_\theta (\x^{F - S} | \z_{t(0)}^F, \x^S)}_{\begin{array}{c}{\mathcal{L}_{\text{recons}}}\end{array}} + \underbrace{\sum_{i=1}^T \KL [q(\z_{s}^F | \z_{t}^F, \x^F) \| p_\theta (\z_{s}^F | \z_{t}^F, \x^S)]}_{\begin{array}{c}{\mathcal{L}_{\text{diffusion}}}\end{array}} \Bigg] & \nonumber \\
    & \hspace{5.01cm} + \underbrace{\KL[q(\z_{t(T)}^F | \x^F) \| p_\theta (\z_{t(T)}^F | \x^S)]}_{\begin{array}{c}{\mathcal{L}_{\text{prior}}}\end{array}}. & \label{supp:mdlm:derive:final}
\end{align}}

In \cref{supp:mdlm:derive:z^F}, $\z^F$ is used to denote $[\z_{t(0)}^F, \dots, \z_{t(T)}^F]$ for simplicity. When deriving \cref{supp:mdlm:derive:q_decompose} from its previous step, we make use of the following factorization: 
\begin{align*}
    q(\z_{t}^F | \z_{s}^F) = q(\z_{t}^F | \z_{s}^F, \x^F) = \frac{q(\z_{s}^F | \z_{t}^F, \x^F) q(\z_{t}^F | \x^F)}{q(\z_{s}^F | \x^F)}.
\end{align*}

\subsection{Simplification of Conditional NELBO Terms}

As assumed in Section 3.5 of \citet{sahoo2024simple}, the forward noising process is applied independently across a sequence, and the denoising process factorizes independently across tokens. These two assumptions can be translated to the following equations:
\begin{align}
    q(\z_{t}^F | \z_{s}^F, \x^F) & = \prod_{\ell=1}^L q(\z_{t}^\ell | \z_{s}^\ell, \x^\ell) \label{supp:mdlm:simplify:forward_ind} \\
    p_\theta(\z_{s}^F | \z_{t}^F) & = \prod_{\ell=1}^L p_\theta(\z_{s}^\ell | \z_{t}^F) \label{supp:mdlm:simplify:backward_ind}
\end{align}

As defined in Equation (7) of \citet{sahoo2024simple}, for $\forall \ell \in F$,
\begin{align}
    p_\theta(\z_{s}^\ell | \z_{t}^F) = q(\z_{s}^\ell | \z_{t}^\ell, \x^\ell = \x_\theta^\ell (\z_{t}^F, t)) \label{supp:mdlm:simplify:ptheta_ori}
\end{align}
Note that the original definition was for the specific case where only a single token is noised and denoised. However, since we are addressing the noising and denoising of entire token sequences, we extend the definition to accommodate sequences of tokens.

Additionally, we define:
\begin{align}
    p_\theta (\z_{s}^\ell | \z_{t}^F, \x^S) = q(\z_{s}^\ell | \z_{t}^\ell, \x^\ell = \x_\theta^\ell ([\z_{t}^{F - S}, \x^S], t)) \label{supp:mdlm:simplify:ptheta_cond} 
\end{align}
where $[\z_{t}^{F - S}, \x^S]$ is a shorthand for $\z_{t}^F$ with $\z_{t}^\ell$ replaced by $\x^\ell$, for $\forall \ell \in S$. In practice, this is easily done by replacing the tokens with indices in $S$ with the corresponding given tokens, while leaving the remaining tokens unchanged before feeding the sequence into the model $\x_\theta$.

According to ``Carry Over Masking'' in the original paper, the model $\x_\theta$ has the property that $\x_\theta^\ell (\z_t^F, t) = \z_t^\ell$ if $\z_t^\ell$ is not masked. Therefore, for $\forall \ell \in S$,
\begin{align}
    p_\theta (\z_{s}^\ell | \z_{t}^F, \x^S) 
    & = q(\z_{s}^\ell | \z_{t}^\ell, \x^\ell = \x_\theta^\ell ([\z_{t}^{F - S}, \x^S], t)) \nonumber \\
    & = q(\z_{s}^\ell | \z_{t}^\ell, \x^\ell) \label{supp:mdlm:simplify:carry_over}
\end{align}
Note that the symbol the $\x^\ell$ is reused: the $\x^\ell$ in the first line represents a random variable, while the $\x^\ell$ in the second line is the $\ell$-th token of the ground truth sequence, which is a given one-hot vector.

As an extension to (\ref{supp:mdlm:simplify:backward_ind}), we further assume that:
\begin{align}
    p_\theta(\z_{s}^F | \z_{t}^F, \x^S) & = \prod_{\ell=1}^L p_\theta(\z_{s}^\ell | \z_{t}^F, \x^S) \label{supp:mdlm:simplify:backward_cond_ind}
\end{align}

Using these equations and the continuation of timesteps (i.e. $T \to \infty$), we can further simplify the terms in (\ref{supp:mdlm:derive:final}).

\subsubsection{Diffusion Loss}

Following Section A.2.3 of the original paper, let $\mathcal{L}_{T} = \E_{t \in \{\frac{1}{T}, \frac{2}{T}, \dots, 1\}} \E_{q(\z_t^F | \x^F)} T \KL [q(\z_{s}^F | \z_{t}^F, \x^F) \| p_\theta (\z_{s}^F | \z_{t}^F, \x^S)]$ denote the diffusion loss. We can simplify it with the following steps:
\begin{align}
    \mathcal{L}_{T} 
    & = \E_{t \in \{\frac{1}{T}, \frac{2}{T}, \dots, 1\}} \E_{q(\z_t^F | \x^F)} T \sum_{\ell=1}^L \KL [q(\z_{s}^\ell | \z_{t}^\ell, \x^\ell) \| p_\theta (\z_{s}^\ell | \z_{t}^F, \x^S)], & \text{\footnotesize Using (\ref{supp:mdlm:simplify:forward_ind}) and (\ref{supp:mdlm:simplify:backward_cond_ind})} \nonumber \\
    & = \E_{t \in \{\frac{1}{T}, \frac{2}{T}, \dots, 1\}} \E_{q(\z_t^F | \x^F)} T \sum_{\ell \in F - S} \KL [q(\z_{s}^\ell | \z_{t}^\ell, \x^\ell) \| p_\theta (\z_{s}^\ell | \z_{t}^F, \x^S)], & \text{\footnotesize Using (\ref{supp:mdlm:simplify:carry_over})} \nonumber \\
    & = \sum_{\ell \in F - S} \E_{t \in \{\frac{1}{T}, \frac{2}{T}, \dots, 1\}} \E_{q(\z_t^F | \x^F)} \left [ T \cdot \KL [q(\z_{s}^\ell | \z_{t}^\ell, \x^\ell) \| p_\theta (\z_{s}^\ell | \z_{t}^F, \x^S)] \right ], & \nonumber 
\end{align}
which is essentially the sum of the losses for each unknown token.

In the appendix of \citet{sahoo2024simple}, the derivation is focused on the case where only a single token is involved. But since we decomposed the diffusion loss of the sequence into the sum of single-token losses, now we are able to use the results from the original paper, as long as we can address the difference between $p_\theta (\z_{s}^\ell | \z_{t}^F, \x^S)$ in this derivation and $p_\theta(\z_s | \z_t)$ in the original paper.

According to Equation (15) from the original paper, we can derive from \cref{supp:mdlm:simplify:ptheta_cond} that:
\begin{align}
    p_\theta (\z_{s}^\ell | \z_{t}^F, \x^S) = q(\z_{s}^\ell | \z_{t}^\ell, \x^\ell = \x_\theta^\ell ([\z_{t}^{F - S}, \x^S], t)) = \cat \Bigg( \z_s^\ell; \frac{Q_{t|s} \z_t^\ell \odot Q_s^\top \x_\theta^\ell ([\z_{t}^{F - S}, \x^S], t))}{{\z_t^\ell}^\top Q_t^\top \x_\theta^\ell ([\z_{t}^{F - S}, \x^S], t))} \Bigg). \nonumber
\end{align}

Thus, the KL divergence term in the diffusion loss can be expressed as:
\begin{align}
    \KL [q(\z_{s}^\ell | \z_{t}^\ell, \x^\ell) \| p_\theta (\z_{s}^\ell | \z_{t}^F, \x^S)] = \KL \Bigg[ q(\z_{s}^\ell | \z_{t}^\ell, \x^\ell) || \cat \Bigg( \z_s^\ell; \frac{Q_{t|s} \z_t^\ell \odot Q_s^\top \x_\theta^\ell ([\z_{t}^{F - S}, \x^S], t))}{{\z_t^\ell}^\top Q_t^\top \x_\theta^\ell ([\z_{t}^{F - S}, \x^S], t))} \Bigg) \Bigg], \label{mdlm:supp:simplify:kl_cond}
\end{align}
while for the single-token case as in the original paper, the KL divergence term $\KL [q(\z_s^\ell | \z_t^\ell, \x^\ell) || p_\theta(\z_s^\ell | \z_t^\ell)]$ is:
\begin{align}
    \KL [q(\z_s^\ell | \z_t^\ell, \x^\ell) || p_\theta(\z_s^\ell | \z_t^\ell)] = \KL \Bigg[ q(\z_s^\ell | \z_t^\ell, \x^\ell) || \cat \Bigg( \z_s; \frac{Q_{t|s} \z_t^\ell \odot Q_s^\top \x_\theta^\ell (\z_t^\ell, t))}{{\z_t^\ell}^\top Q_t^\top \x_\theta^\ell (\z_t^\ell, t))} \Bigg) \Bigg]. \label{mdlm:supp:simplify:kl_single}
\end{align}
In \cref{mdlm:supp:simplify:kl_single}, the index $\ell \in F - S$ is added for notational consistency between \cref{mdlm:supp:simplify:kl_cond} and \cref{mdlm:supp:simplify:kl_single}. As we can see, the sole difference is that in \cref{mdlm:supp:simplify:kl_single}, $\x_\theta^\ell ([\z_{t}^{F - S}, \x^S], t)$ is substituted with $\x_\theta^\ell (\z_t^\ell, t)$. Consequently, as long as the former shares the same properties as the latter, the results derived for the single-token case still applies.

When simplifying the diffusion loss, only 3 properties are required of $\x_\theta^\ell (\z_t^\ell, t)$ in the original paper: 

\begin{enumerate}
    \item The output of $\x_\theta^\ell (\z_t^\ell, t)$ is a probability distribution over all categories, i.e. $\langle \1, \x_\theta^\ell(\z_t^\ell, t) \rangle = 1$.
    \item ``Zero Masking Probabilities'' i.e. $\langle \x_\theta^\ell (\z_t^\ell, t), {\mathbf m} \rangle = 0$.
    \item ``Carry Over Unmasking'' i.e. $\x_\theta^\ell (\z_t^\ell, t) = \x^\ell \text{ when }\z_t^\ell = \x^\ell$.
\end{enumerate}

which all happens to be properties that MDLM models share, and therefore also holds for $\x_\theta^\ell ([\z_{t}^{F - S}, \x^S], t)$. Thus, by substituting $\x_\theta(\z_t, t)$ with $\x_\theta^\ell ([\z_{t}^{F - S}, \x^S], t)$, we can apply the results from the single-token case and derive the following equation from Eqn. 42 of the original paper:
\begin{align}
    \mathcal{L}_{T} = \sum_{\ell \in F - S} \E_{t \in \{\frac{1}{T}, \frac{2}{T}, \dots, 1\}} \E_{q(\z_t^F | \x^F)} T \Bigg[ \frac{\alpha_t - \alpha_s}{1 - \alpha_t} \log \langle \x_\theta^\ell([\z_t^{F - S}, \x^S], t), \x^\ell \rangle \Bigg].
\end{align}

\subsubsection{Prior Loss}

According to Equations (37) and (38) in the original paper, we can easily derive that:
\begin{align}
    q(\z_{t(T)}^F = \M | \x^F) = 1, \nonumber \\
    p_\theta(\z_{t(T)}^F = \M | \x^S) = 1, \nonumber
\end{align}
where $\M = [\mathbf{m}, \mathbf{m}, \dots, \mathbf{m}] \in \mathbb{R}^{K \times L}$, and $K$ is the number of categories.

Thus, $q(\z_{t(T)}^F | \x^F)$ and $p_\theta(\z_{t(T)}^F | \x^S)$ are identical distributions where $\z_{t(T)}^F$ has a probability of 1 of being an entire sequence of masked tokens, resulting in a prior loss of 0:
\begin{align}
    \mathcal{L}_{\text{prior}} = \KL[q(\z_{t(T)}^F | \x^F) \| p_\theta (\z_{t(T)}^F | \x^S)] = 0. \label{supp:mdlm:l_prior}
\end{align}

\subsubsection{Continuous time}

For the continuous-time case, similar to Equation (44) from the original paper, we can derive:
\begin{align}
    \mathcal{L}_{\text{diffusion}}^\infty = \lim_{T \to \infty} \mathcal{L}_{T} = \sum_{\ell \in F - S} \E_{t \sim \mathcal{U}[0, 1], q(\z_t^F | \x^\ell)} 
    \left[\frac{\alpha'_{t}}{1 - \alpha_t} \log \langle \x_\theta^\ell([\z_t^{F - S}, \x^S], t), \x^\ell \rangle \right]. \label{supp:mdlm:l_diffusion}
\end{align}

According to Equation (45) from the original paper, under continuous time, we also have $\z_{t(0)}^\ell = \x^\ell$. Thus we have the following:
\begin{align}
    \mathcal{L}_{\text{recons}} 
    & = \E_{q(\z_{t(0)}^F | \x^F)} [- \log p_\theta(\x^{F - S} | \z_{t(0)}^F, \x^S)], & \nonumber \\
    & = \E_{q(\z_{t(0)}^F | \x^F)} [- \log p_\theta(\x^{F - S} | \z_{t(0)}^F = \x^F, \x^S)], & \text{\footnotesize Using $\z_{t(0)}^\ell = \x^\ell$} \nonumber \\
    & = \E_{q(\z_{t(0)}^F | \x^F)} [- \log \prod_{\ell \in F - S} \langle \x_\theta^\ell([\z_{t(0)}^{F - S} = \x^{F - S}, \x^S], t(0)), \x^\ell \rangle], & \nonumber \\
    & = \E_{q(\z_{t(0)}^F | \x^F)} [- \log \prod_{\ell \in F - S} \langle \x^\ell, \x^\ell \rangle], & \label{supp:mdlm:l_recons_carry} \\
    & = \E_{q(\z_{t(0)}^F | \x^F)} [- \log \prod_{\ell \in F - S} 1], & \nonumber \\
    & = 0, & \label{supp:mdlm:l_recons}
\end{align}
where deriving \cref{supp:mdlm:l_recons_carry} from its previous step makes use of the ``Carry Over Masking'' property, \ie $\x_\theta^\ell([\z_t^{F - S} = \x^{F - S}, \x^S], t) = \x^\ell$.

\subsubsection{Conditional NELBO}
Finally, using \cref{supp:mdlm:l_prior,supp:mdlm:l_diffusion,supp:mdlm:l_recons}, we arrive at the final expression for conditional NELBO:
\begin{align}
    \mathcal{L}_{\text{NELBO}} & = \mathcal{L}_{\text{recons}} + \mathcal{L}_{\text{diffusion}}^\infty + \mathcal{L}_{\text{prior}}, \nonumber \\
    & = \sum_{\ell \in F - S} \E_{q, t} 
    \left[\frac{\alpha'_{t}}{1 - \alpha_t} \log \langle \x_\theta^\ell([\z_t^{F - S}, \x^S], t), \x^\ell \rangle \right]. \nonumber
\end{align}

% \section{Adapting TTs for Various NAR Generation Paradigms}
% \label{appx:tt-nar-adapt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
