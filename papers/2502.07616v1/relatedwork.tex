\section{Related Work}
\boldparagraph{NAR Modeling Techniques.}
BERT \citep{devlin2018bert,warner2024smarter} is one of the first Transformer models designed for NAR generation. It proposes to use a special mask token to indicate unknown tokens and task the model to predict them given the observed tokens. Built on top of this mask prediction principle, discrete diffusion models \citep{austin2021structured} improve NAR generation performance by designing better learning objectives \citep{campbell2022continuous,lou2023discrete,sahoo2024simple} and mask strategies \citep{shi2024simplified}. Instead of recovering sequences from mask tokens, some discrete diffusion models learn to recover from uniformly sampled sequences \citep{lou2023discrete}. Another thread of work incorporates autoregressive or semi-autoregressive biases to the denoising process of diffusion models, intending to combine the expressiveness of autoregressive modeling and the ability to perform NAR generation \citep{chen2024diffusion,han2023ssd}.

\boldparagraph{Architectures for NAR Modeling.}
Decoder-only transformers with full attention are the most widely adopted architecture for NAR modeling. Many SoTA discrete diffusion models use these models. Additionally, bidirectional autoregressive modeling, exemplified by models like BART \citep{lewis2020bart} and MASS \citep{song2019mass}, represents an intermediate approach that incorporates bidirectional context while preserving the left-to-right autoregressive generation process. \citet{sun2023score} developed a Transformer-based architecture for a subclass of discrete diffusion models. \citet{liu2025discrete} and \citet{xu2025energy} combine diffusion models with other deep generative models, such as AR models and energy-based models.