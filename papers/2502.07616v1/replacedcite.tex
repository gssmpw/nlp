\section{Related Work}
\boldparagraph{NAR Modeling Techniques.}
BERT ____ is one of the first Transformer models designed for NAR generation. It proposes to use a special mask token to indicate unknown tokens and task the model to predict them given the observed tokens. Built on top of this mask prediction principle, discrete diffusion models ____ improve NAR generation performance by designing better learning objectives ____ and mask strategies ____. Instead of recovering sequences from mask tokens, some discrete diffusion models learn to recover from uniformly sampled sequences ____. Another thread of work incorporates autoregressive or semi-autoregressive biases to the denoising process of diffusion models, intending to combine the expressiveness of autoregressive modeling and the ability to perform NAR generation ____.

\boldparagraph{Architectures for NAR Modeling.}
Decoder-only transformers with full attention are the most widely adopted architecture for NAR modeling. Many SoTA discrete diffusion models use these models. Additionally, bidirectional autoregressive modeling, exemplified by models like BART ____ and MASS ____, represents an intermediate approach that incorporates bidirectional context while preserving the left-to-right autoregressive generation process. ____ developed a Transformer-based architecture for a subclass of discrete diffusion models. ____ and ____ combine diffusion models with other deep generative models, such as AR models and energy-based models.