% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}
\usepackage{amsmath}
% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{tabularx}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{soul}
% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{array}

\newcommand{\review}[1]{\textcolor{blue}{#1}}
\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}}
\newcommand{\name}{MorphNLI}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{MorphNLI: A Stepwise Approach to Natural Language Inference Using Text Morphing }

\author{Vlad-Andrei Negru$^{1}$, Robert Vacareanu$^{1,2}$, Camelia Lemnaru$^{1}$, \\ \textbf{Mihai Surdeanu$^{2}$, Rodica Potolea$^{1}$ }\\\\
$^1$Department of Computer Science, Technical University of Cluj-Napoca, Cluj-Napoca, Romania  \\
	$^2$Department of Computer Science, University of Arizona, Tucson, USA \\ 
		\{vlad.negru, camelia.lemnaru, rodica.potolea\}@cs.utcluj.ro, \\\{rvacareanu, msurdeanu\}@arizona.edu  \\
}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{Vlad-Andrei Negru \\
%   Technical University of Cluj-Napoca \\ Cluj-Napoca, Romania \\
%   \texttt{vlad.negru@cs.utcluj.ro} \\\And
%   Robert Vacareanu \\
%   University of Arizona, \\ Tucson, USA  \\
%   \texttt{rvacareanu@arizona.edu} \\\And
%   Camelia Lemnaru \\\And
%   Mihai Surdeanu \\\And
%   Rodica Potolea \\
%   }

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\newcommand{\significant}[1]{$#1^\ast$}
\newcommand{\significantbf}[1]{$\textbf{#1}^\ast$}
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\newcolumntype{F}[1]{%
    >{\raggedright\arraybackslash\hspace{0pt}}p{#1}}%

% Right-align
\newcolumntype{Z}{>{\raggedleft\arraybackslash}X}

\begin{document}
\maketitle
\begin{abstract}
We introduce MorphNLI, a modular step-by-step approach to natural language inference (NLI). When classifying the premise-hypothesis pairs into \{\textit{entailment}, \textit{contradiction}, \textit{neutral}\}, we use a language model to generate the necessary edits to % atomically % ms: I don't like the word "atomic" here. What does it mean exactly? Did we prove it (don't think so)
incrementally
transform (i.e., \textit{morph}) the premise into the hypothesis.
Then, using an off-the-shelf NLI model we track how the entailment progresses with these atomic changes, aggregating these intermediate labels into a final output.
We demonstrate the advantages of our proposed method particularly in realistic cross-domain settings, where our method always outperforms strong baselines, with improvements up to 12.6\% (relative). % better than the baselines. % or always outperforms the baselines, with an average improvement of X%
    Further, our proposed approach is explainable as the atomic edits can be used to understand the overall NLI label.

% \todo{Robert: I plan to revisit this.}
\end{abstract}

\section{Introduction}
\begin{figure*}[h]
    \centering
    \includegraphics[width=1\textwidth]{figures/first_page_image_wide_new.png}
    \vspace{-4mm}
    \caption{Natural language inference example where both a state-of-the-art encoder-decoder model -- BART (left) and a LLM -- GPT-4o (middle) predict the incorrect label. Our approach  (right) incrementally morphs the premise into the hypothesis, which 
    decomposes the inference process into several simpler steps. This allows %our method
    it to generate the correct label, which is also associated with an intuitive explanation that falls naturally from the morphing steps. In contrast, both the encoder-decoder model and the LLM produce the incorrect label. The LLM's explanation suggests overfitting on annotation artifacts from SNLI, which assumes coreference between participants and concepts in the two texts~\cite{jiang-marneffe-2022-investigating}.
    % \todo{Maybe entailment with green, neutral with yellow, and contradiction with red?} % ms: done
    }
    \vspace{-4mm}
    \label{fig:example}
\end{figure*}

Natural Language Inference (NLI), i.e., the task that determines whether a text hypothesis is true, false, or undetermined given a text premise \cite{condoravdi-etal-2003-entailment, Dagan2005ThePR, bowman2015large}, is an important building block of many applications such as question answering, summarization, and dialogue systems, where understanding the logical connection between different pieces of information is essential \cite{yin-etal-2019-benchmarking, sainz-etal-2021-label, sainz-etal-2022-textual}. % \todo{R: cite some app with NLI papers here}. 
Despite the fact that NLI has received significant attention lately \cite{Raffel2019ExploringTL, Jiang2019SMARTRA, Sun2020SelfExplainingSI, Wang2021EntailmentAF}, 
several analyses have indicated that neural NLI methods fail to capture important semantic features of logic such as monotonicity, and more granular aspects like negation, universal vs. existential quantifiers, and concept modifiers~\cite{rozanova-etal-2022-decomposing,akoju-etal-2023-synthetic}. Other significant limitations of current models are caused by task artifacts that oversimplify the NLI problem~\cite{williams-etal-2018-broad,jiang-marneffe-2022-investigating}. Large Language Models (LLMs) are prone to contamination~\cite{golchin2024time,contindex}, which causes overfitting on these task artifacts (see Section \ref{sec:discussion}). LLMs also tend to ``not say what they think'' \cite{turpin2024language}, which reduces the quality and faithfulness of their explanations. 

To address the above drawbacks, we propose a {\em cautious} NLI strategy that decomposes the NLI decision into several simpler and more explainable steps. Specifically, our approach: (a) incrementally transforms the premise into the hypothesis using text morphing~\cite{huang2018text}; (b) applies an off-the-shelf NLI model on each morphing iteration; and (c) aggregates the individual NLI labels into an overall label for the given premise-hypothesis pair. We call our method \name. Figure~\ref{fig:example} provides a walk-through example of our approach, contrasted with a state-of-the-art encoder-decoder model and an LLM. The advantages of our direction are two fold. First, it performs better out of domain because its individual, smaller decisions reduce the chance of overfitting. Second, it naturally produces an explainable reasoning chain that traces the morphing transformations. 

Our approach is inspired by Natural Logic (NL)~\cite{maccartney2009extended,maccartney2014natural} but is more flexible. First, rather than relying on a formal alignment algorithm between premise and hypothesis, which continues to be a pain point in the development of NL systems~\cite{krishna2022proofver}, we use a more nimble morphing algorithm \cite{huang2018text} that is trained on synthetic data. Second, instead of using the seven NL logic operators and a relatively complex finite-state automaton to aggregate them, we rely just on the three standard NLI labels (entailment, contradiction, neutral) and 
on a straightforward, robust aggregation decision that performs well in practice: pick the first non-entailment label in the sequence of NLI decisions.

The contributions of our paper are:
{\flushleft {\bf (1)}} We introduce \name, a modular approach for NLI that combines text morphing with neural NLI. Our method does not require any additional supervision, i.e., the text morphing model is trained using synthetic data; the neural NLI engine is an off-the-shelf model.
{\flushleft {\bf (2)}} We evaluate our proposed method in multiple scenarios, including two cross-domain settings: from MNLI~\cite{williams-etal-2018-broad} to SICK~\cite{sick}, and from SICK to MNLI. Our empirical evaluation indicates that \name\ outperforms other state-of-the-art NLI models in all cross-domain experiments. Further, morphing improves the decisions of GPT-4o in the SICK dataset, further highlighting that LLMs do not capture well the semantics of logic~\cite{rozanova-etal-2022-decomposing,akoju-etal-2023-synthetic}.
{\flushleft {\bf (3)}} We perform a qualitative analysis of the explanations generated by \name, and show that they are  better than GPT-4o's on SICK, despite the fact that our model sizes are orders of magnitude smaller. However, both NLI performance and explanation quality of \name\ are worse on MNLI, which we suspect is due to the LLM's contamination with the MNLI dataset.

\begin{figure*}[th!]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/architecture_shortened.png}
    \vspace{-1mm}
    \caption{Training (top) and inference (bottom) for \name, including synthetic data generation for morphing. For the teacher model we use GPT-4; for the student model we use GPT-4o-mini.} 
    % \todo{V: none of the notations in this fig (e.g., $M_{ICL}$, P, H are explained in text. I suggest you replace them with natural language descriptions in the fig.} \todo{+1}, \todo{We can save some space by making this figure slightly less tall.}
    \label{fig:archtecture}
    \vspace{-4mm}
\end{figure*}

\section{Related work}
\input{latex/related_work}

\section{Approach}

Our proposed method, \name, uses a modular step-by-step approach for natural language inference (NLI). At a high level, \name\ operates in three steps: (a) the premise is incrementally converted into the hypothesis through a sequence of small atomic edits that we call {\em morphisms} (see subsection~\ref{sec:morphisms}); (b) an NLI engine is applied to generate NLI labels for each pair of texts in the sequence of transformations; and (c) these labels are aggregated into an overall NLI label for the original premise/hypothesis pair. This is beneficial for two reasons. First, the differences between a premise and a hypothesis are gradually broken into multiple sentences, which makes the task easier for an NLI engine and less prone to overfitting. Second, the trace resulting from the atomic edits can be used as a rationale for the final label, making the method more explainable.
% We propose a new approach called \name\ that simplifies the NLI task, by decomposing the problem into multiple sequential logic reasoning steps. This benefits the current state-of-the-art approaches as the logical differences between a premise and a hypothesis are gradually broken into multiple sentences, making the task easier for an NLI engine. Moreover, due to the smaller changes in text and meaning, the explainability of the final solution is enhanced.

Figure \ref{fig:archtecture} shows the overall architecture of our pipeline. The first module presents the training of the morphism model, where we use In-Context Learning (ICL) with an LLM as a teacher model to generate a synthetic dataset labeled with morphisms. After a filtering step, we use this dataset for fine-tuning a student model for morphism generation. At inference time, we use the student model for generating the morphisms and an NLI prediction model for generating labels. The labels are then aggregated into one final prediction. 
We detail all these components below. 

\subsection{The text morphing task}
\label{sec:morphisms}

Before describing these components,  
we define the morphism generation task, similar in nature with the work of \citet{huang2018text}.
Formally, this task is the process of changing one initial sentence (i.e., premise) into a destination sentence (i.e., hypothesis) through a series of 
% atomic steps -- % ms: they are not necessarily atomic...
morphing operations. These operations are similar to the steps used in computing the Levenshtein distance: 

\begin{enumerate}
    \vspace{-2mm}
    \item Replace - (\textit{replace}, <old\_text>, <new\_text>)
    \vspace{-3mm}
    \item Remove - (\textit{remove}, <text>)
    \vspace{-3mm}
    \item Insert - (\textit{insert}, <text>)
    \vspace{-2mm}
\end{enumerate}

There are three important differences between our morphing and Levenshtein distance. First, our morphing operations 
operate at word/phrase granularity rather than characters. 
Second, our transformations are encouraged to preserve the syntactic structure of the source sentence (see subsection~\ref{subsec:training}).
Third, morphisms are generated using an LLM rather than an edit distance algorithm.
% However, while the Levenshtein distance regards changes done at the character level, we follow the changes done at the syntactic level. Thus, the operations would work on groups of words without breaking the syntactic boundaries. 

Morphing a premise into the corresponding hypothesis results in a finite sequence  $\mathbf{M}$ of sentences (morphisms), where each sentence $\mathrm{M}_i$ is the result of applying a morph operation on the previous sentence $\mathrm{M}_{i-1}$. The first sentence in this sequence is the premise and the last is the hypothesis.



\subsection{Training the morphism model}
\label{subsec:training}

One of our key contributions is training a morphism generation model with minimal supervision. 
The only supervision we require is: (a) a dataset of premise/hypothesis pairs with the associated NLI labels; and (b) a small pool of sentence pairs annotated with morphisms. 
To generate synthetic training data for morphisms we use an LLM with ICL (the teacher model). This LLM is coupled with a deterministic filter that increases the quality of the generated data. Using this data, we fine-tune a smaller LLM (the student model) to generate morphisms during inference.

% We use Large Language Models (LLMs) to generate morphisms for a given pair of sentences. We reckon that a mere ICL approach using tens of annotated examples is not sufficient to handle complex syntactic relations that may arise during the morphing process. A superior method in our case is to fine-tune a generative model on morphing data and let it find the underlying morphing rules and special cases from the data presented. The major problem for this approach is the absence of annotated data.

\paragraph{%Morphism 
Morphing teacher model and ICL selection} \mbox{} \\
Given the complexity of the task and the nonexistence of a dataset labeled with morphisms, we steer the design of our method towards ICL. 
% model towards knowledge distillation. Thus, a student model much smaller in size would be fine-tuned on the examples generated by a teacher model. To generate the synthetic morphism dataset, we use ICL on the teacher model. 
Our ICL pool contains 40 pairs of premises and hypotheses, humanly annotated with intermediate sentences and corresponding morph operations. When generating the morphisms for a pair of sentences, we select the 12 closest examples from the pool of 40 to be used in the prompt. These examples are selected based on the cosine similarity with the input premise and hypothesis, computed on the embeddings generated by a Sentence-BERT \cite{reimers-2019-sentence-bert}.

The generation of the morphisms is driven by a Chain-of-Thought \cite{Chain-of-Thought} prompt, where we ask the teacher model to output the morph operations before generating each intermediate sentence. The input prompt also contains formal rules for the morphing task, encouraging the LLM to preserve the syntactic structure of the source sentence, and forcing a strict order for the morph operations: first apply \textit{replace} operations, then \textit{remove} operations, and lastly \textit{insert} operations. We empirically found that enforcing the operations in this order improves the quality of the overall results. The complete prompt and examples of generated training morphisms are included in the appendix.
%\todo{V: add the prompt and a couple of example of training morphisms to the appendix}

\paragraph{Morphism filter} \mbox{} \\
The synthetically annotated morphisms undergo a series of filtering steps for ensuring their quality. First, for obvious reasons, we filter out the examples where no intermediate sentences were generated (we called these examples {\em lazy morphisms}).

Second, we consider only the examples with intermediate sentences that are longer than either the premise or the hypothesis. We call the phenomenon where some intermediate sentences are too short {\em short morphisms}. This phenomenon may bring faulty reasoning processes, as some intermediate sentences may be formed by removing word groups from the initial sentence that may be necessary for future downstream NLI steps. Figure~\ref{fig:short_morphism} shows an example of this situation.
% Then, in order to form the following sentence, new information is added, that may (or may not) interfere with the previously removed word groups. As a result, a contradiction relation could be seen as an entailment followed by a neutral relation, as depicted in Figure \ref{fig:short_morphism}. % ms: i didn't understand this text. rephrased above.
In order to limit these cases, we removed all short morphisms from the generated data. 

Last but not least, we keep only examples where the overall predicted NLI label is identical to the gold label for the given premise/hypothesis pair. Our hypothesis is that morphisms that yield the correct overall label are more likely to be correct. An initial investigation of the generated data validated our hypothesis. 
To generate individual NLI labels, i.e., between $M_{i-1}$ and $M_i$, we used a BART-large NLI classifier fine-tuned on SNLI, MNLI and FEVER; we aggregated these labels using the aggregation function described below.

\begin{figure}[hp]
    \centering
    \includegraphics[width=\linewidth]{figures/short_morphism.png}
    \caption{Example of a short morphism for sentence $M_2$. The information about the context of the action (``on green grass'') is lost when $M_2$ is generated. A similar context is then added in $M_3$ (``outside''), yielding a faulty neutral prediction because the connection ``on green grass'' $\rightarrow$ ``outside'' is lost.} 
    \label{fig:short_morphism}
    \vspace{-5mm}
\end{figure}

\paragraph{Morphing student model} \mbox{} \\
Using the remaining synthetic data, we fine-tune a smaller LLM as the morphism student model. We used GPT-4o-mini.

\subsection{Modular reasoning using morphisms}

During inference, \name\ operates in 4 steps:
{\flushleft {\bf (1) Voice normalization (VN):}} We observed that the sequential nature of morphing operations proves to be too rigid when there is a change of voice between the premise and hypothesis, as Figure~\ref{fig:voice_correction} shows. To address this, we normalize the premise and the hypothesis to active voice using a smaller language model. 


{\flushleft {\bf (2) Morphing:}} We use the above morphism student model to generate the transformations between the premise and hypothesis.

{\flushleft {\bf (3) Generating individual NLI decisions:}} We use an existing NLI classifier to generate the individual NLI labels between every ($M_{i-1}$, $M_i$) pair of sentences capturing a morphing transformation (see Figure~\ref{fig:example} for an example).

{\flushleft {\bf (4) Aggregating NLI decisions:}} An aggregation function is then used to combine the sequence of NLI labels into an overall label for the given premise/hypothesis pair. To this end, we use a simple heuristic: if all individual NLI labels are {\em entailment}, then the overall label is  {\em entailment}; otherwise the overall label is set to be the first (left-most) individual label that is not {\em entailment}. For example, in Figure~\ref{fig:example} the first non-entailment label is {\em neutral}, which becomes the overall prediction for the example in the figure. In initial experiments, we experimented with aggregating labels using the Natural Logic fine state automaton~\cite{maccartney2014natural,krishna2022proofver}, but have observed that this more formal automaton does not translate well to our more flexible setting. In contrast, our heuristic performed better and is efficient, as it does not require substantial additional processing overhead.

% ms: redundant
%\paragraph{Morphing student model and NLI prediction} \mbox{} \\
%We design a modular architecture, where the natural language inference task is decoupled from the morphism generation task. For generating the intermediary sentences, we leverage on the knowledge gained from the student model during the fine-tuning process on the synthetically generated data. In the classification process, we use encoder-based models that were already fine-tuned and achieved competitive results on popular NLI datasets. These classification models would provide the set of labels $\mathrm{L}_{s}$, that follow the logic reasoning provided by the morphisms  $\mathrm{M}_{s}$.

%\paragraph{Aggregation function} \mbox{} \\
%An aggregation function is then used to reduce the set of labels $\mathrm{L}_{s}$ into one final label for the NLI task. We use a first non-entailment logic, having as the result the first label in the morphing process that is not an entailment. The result of the aggregation process is entailment if and only if all the relations between consecutive morphisms are entailments. \review{We use this simple aggregation method due to its efficiency, as it does not require substantial additional processing overhead.}

% ms: why is this at the end?
%\paragraph{Voice normalization} \mbox{} \\
%We complete the modular reasoning pipeline with the pre-processing step of voice normalization. The sequential nature of morph operations proves to be too rigid when there is a change of voice between the premise and hypothesis, as we can see in Figure \ref{fig:voice_correction}. To address this, we normalize the premise and the hypothesis to active voice using a smaller language model. 

\begin{figure}[hp]
    \centering
    \includegraphics[width=1\linewidth]{figures/voice_correction_wide.png}
    \caption{Example of morphisms with no voice correction. Due to the difficulties caused by the change from passive to active voice between premise and hypothesis, the morphing model  ``hallucinates'' inner sentences.}
    \label{fig:voice_correction}
    \vspace{-5mm}
\end{figure}


\section{Experimental results}
\subsection{Datasets used}
% \todo{Vlad: write about the datasets used SNLI/MNLI/SICK. Explain the task artifacts in SNLI. See the 2nd page in the MNLI paper and \cite{jiang-marneffe-2022-investigating} for a nice summary of these artifacts. See also Fig 1.}
% Our experiments are focused around three popular datasets from the sphere of natural logic: SNLI, MNLI and SICK. Both consist of sentence pairs that are annotated for the NLI task. 

We evaluate the NLI performance of \name\ using two datasets: Multi-Genre Natural Language Inference (MNLI) \cite{williams-etal-2018-broad} and Sentences Involving Compositional Knowledge (SICK)~\cite{sick}. MNLI covers 10 genres of written and spoken English and contains fairly complex natural language. SICK contains artificially-generated premise/hypothesis pairs, which were created using a formal set of logic rules that follow syntactic and lexical transformations. As such, SICK exhibits different challenges from MNLI, assessing the ability of NLI models to comprehend complex logic and compositional structures. Considering these differences, these two datasets are a good selection for both in-domain (ID) and out-of-domain (OOD) evaluations. That is, in addition of training and testing in each dataset, we evaluate \name\ when the underlying NLI engine is trained on the other dataset.

We did not use the Stanford Natural Language Inference (SNLI) corpus \cite{bowman2015large} for the NLI evaluations because, as some of its original authors noticed, it ``is not sufficiently demanding to serve as an effective benchmark''~\cite{williams-etal-2018-broad}. SNLI ignores important phenomena such as temporal reasoning, compositionality of logic, and they make simplifying coreference assumptions, i.e., that the participants and concepts mentioned in the premise and hypothesis are the same~\cite{williams-etal-2018-broad,jiang-marneffe-2022-investigating}. 

However, to minimize any potential overfitting, we fine-tune the morphing engine using premise/hypothesis pairs from SNLI (see next subsection). Thus, our morphing component can be seen as always being evaluated out-of-domain. 

% ms: tried to rewrite this to be more focused, addressing what we did and did not do
% The Stanford Natural Language Inference (SNLI) corpus \cite{bowman2015large} consists of approximately 470k humanly annotated sentence pairs. It is considered a point of reference in the NLI area, multiple state-of-the-art solutions being trained and tested against it. In our approach we use the SNLI validation set to generate the synthetic morphisms used for fine-tuning the student model. We resolve to testing our design on other datasets because of the limitations in difficulty and several artifacts of SNLI. Being formed from image captions, the dataset contains short and simple descriptions of scenes, as mentioned by the authors of MNLI \cite{williams-etal-2018-broad}. Moreover, there may be inconsistencies in classifying the examples from a purely natural language point of view, since the labels given by the annotators were influenced by the images presented.  

% Multi-Genre Natural Language Inference (MNLI) \cite{williams-etal-2018-broad} corpus gives an alternative, being formed from ten genres of written and spoken English. Since it captures more language complexity, we use the mismatched variants of the validation and test datasets in developing and testing our solution. 

%Sentences Involving Compositional Knowledge (SICK) \cite{sick} has artificially generated premise/hypothesis pairs, from image captions. The pairs are generated using a formal set of rules that follow syntactic and lexical transformations. The dataset exhibits different challenges, assessing NLI models in the ability to comprehend complex semantic relationships and compositional structures within sentences. \review{Considering this, SICK is a good alternative for testing, complementing the complex, genre-spanning nature of MNLI.}




\subsection{Experimental settings}

%The LLMs used for the teacher and student model are from the GPT-4 family. The synthetic dataset containing morphisms is generated from the SNLI validation dataset (\textasciitilde10.000 examples of premise/hypothesis pairs), using GPT-4-turbo. After the filtering step, we remain with 3027 pairs labeled with morphisms for fine-tuning. We split these in two sets, one for training (2127 examples) and one for validation (900 examples). The rest of the examples that were filtered out are later used for testing purposes, to compare our knowledge distillation approach against the simple ICL approach. 

The LLMs used for the teacher and student morphing models are both from the GPT-4 family. Details on the models' identifiers are present in Appendix \ref{sec:models_used}, together with experiments using another LLM from the Llama family. The synthetic data set that contains morphisms is generated from the SNLI validation dataset (\textasciitilde10,000 premise/hypothesis pairs) using GPT-4-turbo. After the filtering step (see Section \ref{subsec:training}), we are left with 3,027 pairs labeled with morphisms for fine-tuning. These are split into two sets: one for training (2,127 examples) and one for validation (900 examples). The remaining filtered-out examples are later used to compare the fine-tuning approach with simple ICL for morphing. Our %original 
preliminary experiments indicated that fine-tuning outperforms ICL, both in terms of overall performance and model efficiency. For this reason, all experiments described later in this section use a morphing model fine-tuned on the above training data. Moreover, the fine-tuned model proves to be more expressive, with a much lower rate of lazy morphisms (1,575 compared to 4,375 in the case of ICL), having %slightly more 
a slight increase in the number of short morphisms (674 compared to 557 in the case of ICL).

% GPT-4o-mini is used as the morphing student model as well as for the voice normalization step. % \todo{V: asta spui deja la sec 3, mai punem si aici? V: Ar trebui lasat doar intr-un loc. Aici specificam si de normalization model (care nu e specificat in sectiunea 3) } % ms: asta e important. un pic de repetitie e Ok

For the individual NLI decisions we experimented with state-of-the-art NLI prediction models from two different families: encoder-decoder using BART and encoder-only using RoBERTa (large versions).\footnote{The sources of the models are presented in the appendix.} %The testing of our method is done 

% We evaluate \name\ on two different datasets, SICK and MNLI, assessing both in-domain (ID) and out-of-domain (OOD) performance.  % ms: this belongs in the previous subsection
%as we conduct our analysis on two dimensions, in domain and out-of-domain, depending on the dataset used for fine-tuning the classification model.

% \begin{table}

% \begin{center}
% \resizebox{1\linewidth
% }{!}{
% \input{tables/SICK}
% }
% \end{center}
% \caption{\name\ accuracy on the SICK dataset using two NLI engines: RoBERTa and BART. We compare our results against the two ``vanilla'' NLI models, i.e., without using text morphing. For the out-of-domain results, we train the respective NLI model on SNLI, MNLI and FEVER.}
% \label{tab:SICK}
% \end{table}

% \begin{table}

% \begin{center}
% \resizebox{1\linewidth
% }{!}{
% \input{tables/MNLI}
% }
% \end{center}
% \caption{\name\ accuracy on the MNLI dataset using two NLI engines: RoBERTa and BART. We compare our results against the two ``vanilla'' NLI models, i.e., without using text morphing. For the out-of-domain results, we train the respective NLI model on SICK.}
% \label{tab:MNLI}
% \end{table}

\subsection{Results}
\label{sec:results}

Table \ref{tab:SICK} shows the accuracy of MorphNLI on the SICK test dataset. We report the results with and without voice normalization, with two different NLI engines (RoBERTa and BART), which are trained both ID and OOD. We compare the performance of our approach to the same NLI model applied directly to the original premise/hypothesis pair, i.e., without morphing (referred to as ``vanilla'').
We draw several observations from this table. First, all models perform better ID than OOD, which indicates a certain degree of overfitting. 
Second, \name\ shows a slight drop in ID performance, which we attribute to the fact that the NLI models were not trained on incremental transformations (see the next subsection for a more detailed analysis). Most importantly, \name\ outperforms the ``vanilla'' NLI model in all four OOD configurations, with an improvement as large as 1.74\% for BART with voice normalization. This is an encouraging result, as it validates our hypothesis: that modular NLI improves domain transfer.
% These results show a slight drop in performance for the in-domain cases, both for RoBERTa and BART. When considering the out-of-domain tests, however, our approach outperforms the vanilla NLI models, with a maximum of 1.74\% for BART with voice normalization. % ms: tried to rephrase in a more exciting way :)

\begin{table}

\begin{center}
\resizebox{1\linewidth
}{!}{
\input{tables/SICK}
}
\end{center}
\vspace{-2mm}
\caption[cap]{\name\ accuracy on the SICK dataset using two NLI engines: RoBERTa and BART. We compare our results against the two ``vanilla'' NLI models, i.e., without using text morphing. 
VN indicates voice normalization. For OOD, we use the RoBERTa models trained on MNLI, and BART models trained on SNLI, MNLI and FEVER.\footnotemark}
\label{tab:SICK}
\vspace{-3mm}
\end{table}

\footnotetext{We empirically observed that this model outperforms an MNLI-only trained BART.}

Table \ref{tab:MNLI} shows the same behaviour for the MNLI test dataset. Here the OOD enhancements are more considerable. For example, we observe an increase of 5.29\% for RoBERTa and 5.92\% for BART, both in settings with no voice normalization. While the voice normalization proved beneficial for the SICK dataset, for all the scenarios tested, for MNLI we see a decline in accuracy when applying it (see the next subsection for a more detailed discussion). 

\begin{table}

\begin{center}
\resizebox{1\linewidth
}{!}{
\input{tables/MNLI}
}
\end{center}
\vspace{-2mm}
\caption{\name\ accuracy on the MNLI dataset, under the same settings as Table \ref{tab:SICK}. For OOD, we use models trained on SICK.}
\label{tab:MNLI}
\vspace{-3mm}
\end{table}

% The results in both tables show that our pipeline is better suited for cross-domain scenarios, outperforming the vanilla state-of-the-art models in this context. We also note the low accuracy in the out-of-domain context, compared to in domain, showing the lack of transfer capabilities of these NLI models to other domains. % ms: now redundant with the new text above

To understand if our approach is compatible with LLMs, we evaluate the performance of our pipeline in another setting, 
in which we use two LLMs (GPT-4o and GPT-4o-mini) as the NLI engines. These results are presented in Table \ref{tab:gpt}. 
Despite their massive size and their extensive training, these LLMs still benefit from morphing on the SICK dataset. This underlines previous observations that LLMs do not capture well the semantics of logic, which is a key focus in SICK~\cite{rozanova-etal-2022-decomposing, akoju-etal-2023-synthetic}. However, we do not observe a similar improvement on MNLI. One potential explanation for such an effect is the potential contamination of these LLMs with the MNLI dataset (see next subsection for a longer discussion).

\begin{table}[htbp]

\begin{center}
\resizebox{1\linewidth
}{!}{
\input{tables/gpt}
}
\end{center}
\vspace{-2mm}
\caption{GPT-4o and GPT-4o-mini NLI accuracies, with and without text morphing.} %and without (``vanilla'').}
\label{tab:gpt}
\vspace{-6mm}
\end{table}



\subsection{Discussion}
\label{sec:discussion}

To further understand \name's behavior we answer below three important research questions.

% ms: am schimbat subsectiunile in intrebari ca sa fie f clar ce facem --- cl: excelent puse accentele acum! si imi plac mai mult intrebarile ca titlu

\subsubsection{What is the quality of \name's explanations?}
\label{ssec:manual}

To get a better understanding of how the explanations generated via our modular approach compare to those generated by LLMs (GPT-4o and Llama 3.1 8B), we performed a manual evaluation on a random sample from both MNLI and SICK datasets. We selected 20 instances from each development set, distributed as follows: 5 instances where the NLI model's predictions are correct both with and without morphing, 5 where both predictions are incorrect, 5 where morphing improves the NLI prediction, and 5 where it worsens the prediction. The NLI model used here was RoBERTa, fine-tuned in-domain on each dataset. Four human evaluators awarded a score between 0 and 2, as follows: 2 indicates the explanation is correct; 1 indicates the explanation is partially correct, i.e.,  it contains correct elements, but it misses required information or includes extraneous elements; and 0 indicates the explanation is completely incorrect.

We computed Cohenâ€™s Kappa inter-annotator agreement across all six pairs of evaluators from the four annotators. For the MNLI dataset, we calculated an average Kappa agreement of 34\%, which indicates fair agreement. Considering the complexity of the task, i.e., evaluators had to evaluate both the correctness of each morphism and the NLI label produced at each step, we consider this a respectable result. Even more encouragingly, the maximum agreement between two annotators was 57\%, which falls on the high end of moderate agreement, touching on substantial. This suggests that the agreement can be improved with more training. Similarly, on the SICK dataset, the average agreement is 67\% (substantial agreement), with a maximum of 91\% (almost perfect). These higher scores highlight that despite the complexity of the task, annotators were trained to perform high-quality annotations.

We evaluated: (i) the overall explanation quality with our modular approach; (ii) the quality of the GPT-4o reasoning process and (iii) the quality of the morphisms generated via our approach (we asked the evaluators to discard the NLI label, and reason based on the morphisms alone). Table \ref{tab:manual} presents the percentage scores average from the four annotators. %and their standard deviations.

\begin{table}[htbp]
\begin{center}
\resizebox{1\linewidth
}{!}{
\input{tables/manual_new}
}
\end{center}
\vspace{-3mm}
\caption{Average percentage scores for the quality of the explanations produced via morphing, compared with the GPT-4o and Llama 3.1 8B explanations. We also assessed the quality of the morphing process alone -- i.e., whether a human evaluator could infer the correct NLI label from the morphisms.}
\label{tab:manual}
\vspace{-3mm}
\end{table}

Our approach delivers uniform explanation quality across the two dataset samples (MNLI and SICK) and the overall quality is considerably larger than that of Llama 3.1 8B, despite the latter model's much larger size.
The quality of the GPT-4o explanations is significantly better for MNLI than for SICK, where the explanations via morphisms are superior. An interesting phenomenon reported by the annotators was related to the potential overfitting of the GPT-4o reasoning. In 6 out of the 20 examples sampled from SICK, the model incorrectly assumed that premise and hypothesis refer to the same situation, i.e., the participants and the concepts mentioned are the same between premise and hypothesis. See Figure~\ref{fig:GPT_overfit_example} for a simple example. This task artifact exists in the SNLI dataset \cite{bowman2015large}, where both premise and hypothesis are annotated given a single image \cite{jiang-marneffe-2022-investigating}, but it has been removed in more recent datasets such as SICK and MNLI. However, since SNLI has been publicly available for approximately a decade it is likely that it ``leaked'' into the GPT-4o training data, which learned this task artifact. As a consequence, the LLM mistakingly labeled these pairs as contradiction and produced completely incorrect explanations.
For MNLI, this phenomenon was not as strong, which is likely due to another contamination: GPT models are known to have been contaminated with the MNLI development partition~\cite{contindex}. The large difference between MNLI and SICK GPT-4o explanation scores supports this hypothesis.



\begin{figure}[htbp]
\vspace{-2mm}
\begin{center}
\resizebox{1\linewidth
}{!}{
\input{tables/overfit_example}
}
\end{center}
\vspace{-3mm}
\caption{Misbehavior %example
of GPT-4o related to the artifacts from SNLI. 
The underlined font highlights the explanation fragments that are not correct with respect to the italic text in premise/hypothesis. Here the model incorrectly assumes that the dog in the premise being the same as the dog in the hypothesis. }
\label{fig:GPT_overfit_example}
\vspace{-3mm}
\end{figure}

\begin{figure*}[!h]
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/no_morph.png}
        \subcaption{LIME explanation, no morphing}
        \label{fig:lime-1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/morph1.png}
        \subcaption{LIME explanation,  morphing step 1}
        \label{fig:lime-2}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/morph2.png}
        \subcaption{LIME explanation,  morphing step 2}
        \label{fig:lime-3}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        % \vspace{-10mm}
        \centering
        \includegraphics[width=1\linewidth]{figures/morph3.png}
        \subcaption{LIME explanation,  morphing step 3}
        \label{fig:lime-4}
    \end{subfigure}
    \caption{LIME analysis of the predictions of the in-domain RoBERTa NLI model for a premise-hypothesis pair from MNLI, without morphing (a) and in subsequent morphing steps (b--d). In (b) the operation is (\textit{replace}, ``we just sit around and reminisce'', ``It is nice to talk about fond memories''); in (c) the operation is (\textit{replace}, ``go back to the time when we were growing up'', ``from childhood''); in (d) the operation is (\textit{remove}, ``Lot of times'').}
    \label{fig:lime}
    \vspace{-5mm}
\end{figure*}

A second observation from this analysis is that the off-the-shelf NLI models likely exhibit some degree of overfitting as well. Specifically, they have been trained on the original premise/hypothesis pairs and underperform on our (simpler) incremental inference steps. For instance, the NLI model fails to correctly interpret semantic information in short textual snippets (e.g., understanding that "grazing a field" implies the field has "grass," or that "sandy land" implies "desert").
This explains the large difference between the morphism-only explanations (which do not use an NLI model) and the full \name\ system. In contrast, GPT-4o ---being a significantly larger model--- is able to grasp these semantic aspects. However, its explanations are sometimes long, convoluted, and repetitious, whereas the explanations provided via morphing are concise and straightforward.

% \begin{figure*}[!h]
%     \begin{minipage}[l]{1.0\columnwidth}
%         \centering
%         \includegraphics[width=1\linewidth]{figures/no_morph.png}
%         \subcaption{LIME explanation, no morphing}
%         \label{fig:lime-1}
%     \end{minipage}
%     \hfill{}
%     \begin{minipage}[r]{1.0\columnwidth}
%         \centering
%         \includegraphics[width=1\linewidth]{figures/morph1.png}
%         \subcaption{LIME explanation,  morphing step 1}
%         \label{fig:lime-2}
%     \end{minipage}
%     \begin{minipage}[l]{1.0\columnwidth}
%         \centering
%         \vspace{10px}
%         \includegraphics[width=1\linewidth]{figures/morph2.png}
%         \subcaption{LIME explanation,  morphing step 2}
%         \label{fig:lime-3}
%     \end{minipage}
%     \hfill{}
%     \begin{minipage}[r]{1.0\columnwidth}
%         \centering
%          \vspace{10px}
%         \includegraphics[width=1\linewidth]{figures/morph3.png}
%         \subcaption{LIME explanation,  morphing step 3}
%         \label{fig:lime-4}
%     \end{minipage}
%     \caption{LIME analysis performed on the predictions of the in-domain RoBERTa NLI model for a premise-hypothesis pair from MNLI, without morphing (a) and in subsequent morphing steps (b--d). In (b) the operation is (\textit{replace}, ``we just sit around and reminisce'', ``It is nice to talk about fond memories''); in (c) the operation is (\textit{replace}, ``go back to the time when we were growing up'', ``from childhood''); in (d) the operation is (\textit{remove}, ``Lot of times'').}
%     \label{fig:lime}
%     \vspace{-5mm}
% \end{figure*}




% \begin{figure*}[!h]
%     \begin{minipage}[l]{1.0\columnwidth}
%         \centering
%         \includegraphics[width=1\linewidth]{figures/no_morph.png}
%         \subcaption{LIME explanation, no morphing}
%         \label{fig:lime-1}
%     \end{minipage}
%     \hfill{}
%     \begin{minipage}[r]{1.0\columnwidth}
%         \centering
%         \includegraphics[width=1\linewidth]{figures/morph1.png}
%         \subcaption{LIME explanation,  morphing step 1}
%         \label{fig:lime-2}
%     \end{minipage}
%     \begin{minipage}[l]{1.0\columnwidth}
%         \centering
%         \vspace{10px}
%         \includegraphics[width=1\linewidth]{figures/morph2.png}
%         \subcaption{LIME explanation,  morphing step 2}
%         \label{fig:lime-3}
%     \end{minipage}
%     \hfill{}
%     \begin{minipage}[r]{1.0\columnwidth}
%         \centering
%          \vspace{10px}
%         \includegraphics[width=1\linewidth]{figures/morph3.png}
%         \subcaption{LIME explanation,  morphing step 3}
%         \label{fig:lime-4}
%     \end{minipage}
%     \caption{LIME analysis performed on the predictions of the in-domain RoBERTa NLI model for a premise-hypothesis pair from MNLI, without morphing (a) and in subsequent morphing steps (b--d). In (b) the operation is (\textit{replace}, ``we just sit around and reminisce'', ``It is nice to talk about fond memories''); in (c) the operation is (\textit{replace}, ``go back to the time when we were growing up'', ``from childhood''); in (d) the operation is (\textit{remove}, ``Lot of times'').}
%     \label{fig:lime}
%     \vspace{-5mm}
% \end{figure*}



\subsubsection{What are \name's common errors?}
\label{ssec:error_analysis}
To identify where most errors occur within the \name\ pipeline, we randomly sampled 20 errors from each of the two development sets (SICK/MNLI) and manually analyzed these examples. We discovered that: 45\% (SICK) and 50\% (MNLI) errors were caused by the NLI model (in-domain RoBERTa).
As indicated above, this is likely a form of overfitting due to the NLI model's original training data, which did not contain text pairs similar to our incremental transformations. This suggests that valuable future work would be to fine-tune an NLI model that is morphing aware.
The second most common error type were faulty morphisms: 45\% (SICK) and 20\% (MNLI).
This observation indicates that our morphing would probably benefit from more fine-tuning. Lastly,
5\% (SICK) and 30\% (MNLI) were a result of poor voice normalization. MNLI, which contains longer and more complex statements, potentially with several predicates, suffers from this problem more. This suggests that identifying first which verb is the sentence's main predicate might improve voice normalization.
All in all, this analysis indicates that \name's errors are caused by issues of local components, which can be potentially addressed, and are not a limitation of the overall direction.



\subsubsection{Are \name's decisions more interpretable?}
\label{ssec:lime}
To gain a better insight on how morphing improves the prediction process of the (independent) NLI model, we conducted several analyses using LIME \cite{ribeiro2016whyitrustyou} on examples from both SICK and MNLI, where we compare a ``vanilla'' NLI model (in-domain RoBERTa) with \name\ using the same NLI engine.
As anticipated, providing the NLI model with incremental changes helps it focus on more semantically relevant words. For example, Figure~\ref{fig:lime-1} shows that, without morphing, the inference model mistakenly predicts the pair as being \textit{neutral} and the focus is on words without strong relation to the sentence pair's meaning (``go,'' ``it,'' ``were,'' etc.); with morphing (Figures~\ref{fig:lime-2} --  \ref{fig:lime-4}), in each subsequent step, the model correctly and more confidently identifies all three transitions as \textit{entailment}, and focuses on more semantically relevant words (``reminisce'' and ``fond;'' ``growing up'' and ``childhood'').\footnote{Although the overall semantics are still not perfect: while the first three phrases are associated with the {\em entailment} label, ``childhood'' is associated with {\em non-entailment}.}

\subsubsection{Lexical sensitivity between the premises and hypotheses}
\label{ssec:lexical_sensitivity}

During our experiments we noticed an inverse correlation between the quality of the morphing process and the syntactic/lexical differences between the premises and hypotheses. For example, where the two share little to no lexical similarities (i.e., often found in MNLI), the morphing operations tend to consider larger textual groups, affecting the performance. On the other hand, the more similar the premise and hypothesis are, the more the morphing operations follow clear logical steps. To a large extent, this carries over to examples with larger syntactic/lexical differences. However, too many lexical differences between the premise and hypothesis hurt multiple NLI techniques, and MorphNLI is no exception. Nevertheless, our approach is less sensitive to lexical differences out-of-domain, indicating a lower degree of overfitting. This phenomenon is further detailed in the Appendix \ref{sec:appendix_lexical}.


\subsubsection{Importance of the filtering stage}

As described in section \ref{subsec:training}, we included filters to remove low-quality data, i.e., examples with no inner sentences (lazy morphisms) and examples with inner sentences that are shorter than both the premise and hypothesis (short morphisms). By filtering these examples before fine-tuning, we significantly reduce the number of short morphisms at inference time, while maintaining a low number of lazy morphisms. In our experiments, for a sample of roughly 5,500 examples during inference, the initial morphing mechanism predicted 26\% lazy and 32\% short. After introducing our filtering mechanism, the percentage of lazy morphisms had a slight increase to 28\%, while the percentage of short morphisms dropped considerably to 12\%.

\label{ssec:filtering}

\section{Conclusions}
In this paper, we proposed \name\ -- a modular step-by-step approach for natural language inference.
Our method uses a language model to generate atomic edits that progressively transform (i.e., morph) the premise into the hypothesis. We then track how these atomic edits impact the entailment between successive sentences, aggregating these intermediate labels into a final answer (see Figure~\ref{fig:example}). We hypothesized that typical NLI models can better handle examples where the two sentences are lexically close (i.e., they differ only by an atomic edit)
Our results confirm that our proposed approach is more robust, outperforming traditional NLI models in all cross-domain settings investigated.
Furthermore, our proposed method is explainable. The sequence of intermediate edits together with their corresponding individual NLI labels can be used to explain the overall prediction. 



\section*{Limitations}

Our work focuses on the task of Natural Language Inference. Although the text morphing process proves to be beneficial in the context of logical reasoning, its applicability in other reasoning tasks is still to be tested. Moreover, we cannot offer an assurance on the level of generalizability of our method. For our experiments, we designed the morphism generation as a general task, as the fine-tuning data is constructed from a different domain than the testing data (SNLI vs. SICK/MNLI). However, we do not know if this generalization is constrained on data specific to the NLI task. 

In the development of our solution, for the morphism generation task, we have experimented mostly with LLMs from the GPT family. We are unsure if our pipeline may have different behavior for other proprietary LLMs or for much larger LLMs, as we are using a fine-tuned version of GPT-4o-mini. Also, being a closed source model, we do not know if the LLM was previously trained towards this objective of text morphing. Further, it is hard to accurately predict the level of contamination of the model with the test datasets, and what influence it has on the morphism generation. 

The morphing process is evaluated only on English. We have no assurance that the same techniques could apply on other languages for multilingual models or if it follows only the particularities of the English language.  

\section*{Acknowledgment}

The other authors thank Robert Vacareanu for providing the idea for this research and contributing with valuable insights in the development process.

% The other authors thank <anonymous> for providing the idea for this research and contributing with valuable insights in the development process.


The first author was partially supported by a private scholarship grant with id 31638/04.10.2023, given by the company Electrolux. 

% The first author was partially supported by a private scholarship grant with id <anonymous>, given by the company <anonymous>. 

\section*{Ethics}

We have extensively utilized off-the-self LLMs and NLI models, which may contain hidden biases. However, our approach makes inference more explicit, so these potential biases are more likely to be exposed during the morphing process. 


We mostly used closed models, however our costs are reduced. We believe that this study does not exclude any communities from the point of view of the associated cost. 





% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{latex/acl_latex}
\newpage
\appendix
%
% add appendixes here
%

\section{Morphism generation examples}
\label{sec:appendix_examples}

Figure \ref{fig:prompt_example} presents the prompt that we have used for the generation of the synthetic dataset labeled with morphisms. The first part of the prompt consists on giving basic rules on the morphing task such as specifying what is the input and the desired output, the maximum number of intermediary sentences and the operations used (with their structure). The next part presents in more detail information about each operation and how they should be performed. This part was developed through prompt engineering, analyzing the systematic mistakes that the model was making in the generation process. Then, we give 12 examples of morphisms, together with the morph operations. Finally, the premise and hypothesis are given. Figure \ref{fig:ICL_example} presents a humanly annotated example in the prompt. The output structure of the LLM follows the structure of the ICL example. An example of morphism generated by the student model (fine-tuned GPT-4o-mini) is presented in Figure \ref{fig:output_example}. 


Figure \ref{fig:explanation_prompt} shows the prompt used for generating the GPT-4o and Llama 3.1 8B explanations that were compared against \name\ explanations.

\begin{figure*}
\begin{center}
\resizebox{1\textwidth
}{!}{
\input{tables/prompt_example}
}
\end{center}
\caption{Prompt example for morphism generation.}
\label{fig:prompt_example}
\end{figure*}

\begin{figure}[!h]
\begin{center}
\resizebox{1\linewidth
}{!}{
\input{tables/ICL_example}
}
\end{center}
\vspace{-1mm}
\caption{Example of a manually annotated morphism from the ICL pool.}
\label{fig:ICL_example}
\end{figure}

\begin{figure}[!h]
\begin{center}
\resizebox{1\linewidth
}{!}{
\input{tables/output_example}
}
\end{center}
\vspace{-1mm}
\caption{Example of a morphism as in the LLM output. The premise is "A white man is walking a dog through brown water with difficulty".}
\label{fig:output_example}
\end{figure}

\begin{figure}[!h]
\begin{center}
\resizebox{1\linewidth
}{!}{
\input{tables/explanations_example}
}
\end{center}
\vspace{-1mm}
\caption{The prompt used for generating the GPT-4o and Llama 3.1 8B explanations.}
\label{fig:explanation_prompt}
\end{figure}


\section{Results on validation datasets}
\label{sec:appendix_dev_dataset}

Tables \ref{tab:SICK_val} and \ref{tab:MNLI_val} present the results on the validation datasets of SICK and MNLI. We observe the same behaviour as the one described in Section \ref{sec:results} on the test datasets. Our method significantly outperforms the state-of-the-art models in the OOD setting, with an increase of 1.21\% for RoBERTa and 4.84\% for BART in the case of SICK, and 3.10\% for RoBERTa and 4.59\% for BART in the case of MNLI without voice normalization. 

\begin{table}[!h]
\begin{center}
\resizebox{1\linewidth
}{!}{
\input{tables/SICK_val}
}
\end{center}
\vspace{-1mm}
\caption{\name\ accuracy on the SICK validation dataset, using two NLI engines: RoBERTa and BART. We compare our results against the two ``vanilla'' NLI models, i.e., without using text morphing. For OOD, we use the RoBERTa models trained on MNLI, and BART models trained on SNLI, MNLI and FEVER.}
\label{tab:SICK_val}
\end{table}

\begin{table}[!h]
\begin{center}
\resizebox{1\linewidth
}{!}{
\input{tables/MNLI_val}
}
\end{center}
\vspace{-1mm}
\caption{\name\ accuracy on the MNLI validation dataset, under the same settings as Table \ref{tab:SICK_val}. For the OOD results, we train the respective NLI model on SICK.}
\label{tab:MNLI_val}
\end{table}

\section{NLI models and LLMs used}
\label{sec:models_used}

Throughout our study, we used NLI classifiers in order to generate the individual labels between each pair of sentences and to provide a comparison baseline. Table \ref{tab:nli_models} shows the NLI models used, together with the dataset they were fine-tuned on and their source (Hugging Face path\footnote{https://huggingface.co/}). As we could not find an off-the-shelf BART-large model fine-tuned on SICK, we fine-tuned a version of our own on the train split. We have used a learning rate of 1e-4 with 500 warm-up steps and batch size of 32 for 5 epochs. Cross Entropy is used as loss function, together with AdamW as optimizer. 

\begin{table*}[!ht]
\begin{center}
\resizebox{1\textwidth
}{!}{
\input{tables/nli_models}
}
\end{center}
\caption{The NLI models used for classification. }
\label{tab:nli_models}
\end{table*}


As mentioned in the article, we used various LLMs of the GPT-4 family. Here, we provide the identifiers of these models to increase reproducibility:
\begin{itemize}
    \item GPT-4o (labeling, explanations): gpt-4o-2024-08-06
    \item GPT-4 (teacher model): gpt-4-0125-preview
    \item GPT-4o-mini (student model, voice normalization, labeling): gpt-4o-mini-2024-07-18
\end{itemize}

The total budget for synthetically annotating morphisms using ICL, fine-tuning the student model, making the ablation studies, and testing the performance of our model was approximately 350\$.


It is important to mention that our method does not rely on proprietary models (GPT family). At the beginning of our research, we experimented with GPT-4, Claude 3 and Llama 3.1, and chose GPT-4 as it performed slightly better in the morphing generation process.

To verify whether our overall results hold with an open-weight LLM, we conducted an experiment in which we took a small sample size from SICK (50 examples) and generated morphisms with both GPT-4o-mini and Llama-3.1-70b-Instruct (using in-context learning examples). Then we labeled the morphisms using a RoBERTa based NLI model for both in-domain and out-of-domain scenarios. We present the results in Table \ref{tab:llama}. We observe that the results are reasonably similar. That is, the GPT model outperforms Llama for the in-domain case, but Llama is superior for the out-of-domain case. This small experiment yields promising insights into the applicability of our method to LLMs from other families.

\begin{table}[!ht]
\begin{center}
\resizebox{1\linewidth
}{!}{
\input{tables/Llama_appendix}
}
\end{center}
\vspace{-1mm}
\caption{\name\ accuracy on a small sample from SICK, using GPT-4o-mini or Llama-3.1-70b-Instruct for generating the morphisms. The NLI modules used are RoBERTa based. }
\label{tab:llama}
\end{table}

\section{Lexical sensitivity}
\label{sec:appendix_lexical}

We wanted to see how the performance of our approach varies considering the lexical difference between the premise and hypothesis. We measured the accuracy as the similarity between the hypothesis and the premise varies, and as the word difference varies. For the similarity, we used a sentence transformer (all-MiniLM-L6-v2) and measured the cosine similarity between premise and hypothesis. For the word difference, we computed the difference in words between the lemmatized premise and hypothesis. The results are presented in Figure \ref{fig:lexical}. We see that for both scenarios and datasets, \name\ is less sensitive to lexical differences, especially out-of-domain. We observe that in the case of word difference, the in-domain vanilla approach is not affected by a large difference. We consider this a clear sign of overfitting, especially as the rest of the methods have a drop in performance. This experiment further shows that our approach is less prone to overfitting and outperforms the vanilla models in out-of-domain scenarios. 

\begin{figure*}[!h]
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/MNLI_Similarity.png}
        \subcaption{MNLI Similarity}
        \label{fig:MNLI_sim}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/SICK_Similarity.png}
        \subcaption{SICK Similarity}
        \label{fig:SICK_sim}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/MNLI_Word_Difference.png}
        \subcaption{MNLI Word Difference}
        \label{fig:MNLI_wd}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.5\textwidth}
        % \vspace{-10mm}
        \centering
        \includegraphics[width=1\linewidth]{figures/SICK_Word_Difference.png}
        \subcaption{SICK Word Difference}
        \label{fig:SICK_wd}
    \end{subfigure}
    \caption{\name\ sensitivity to lexical difference in the premise and hypothesis pair. We can see that our model is less sensitive in the out-of-domain scenario and less prone to overfitting. }
    \label{fig:lexical}
    \vspace{-5mm}
\end{figure*}

\end{document}




