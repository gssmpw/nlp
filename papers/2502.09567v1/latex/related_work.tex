% Related Work
Our work draws inspiration from Natural Logic \cite{Lakoff1970LinguisticsAN}, which is a form of reasoning aiming to draw logic inferences by operating directly over linguistic structures. 
% For example, 
Over the years, this has been implemented for natural language processing in various forms \cite{MacCartney2007NaturalLF, Krishna2021ProoFVerNL, rozanova-etal-2022-decomposing, feng-etal-2022-neuro, korakakis-vlachos-2023-improving}. \citet{MacCartney2007NaturalLF} introduced one of the first computational models for natural logic, which has been subsequently extended and improved in follow up work \cite{MacCartney2008ModelingSC, MacCartney2009AnEM}. 
Natural logic can be useful beyond natural language inference, for tasks such as commonsense reasoning \cite{angeli-manning-2014-naturalli}, fact verification \cite{krishna2022proofver, Strong2024ZeroShotFV}, or polarity tracking \cite{hu-moss-2018-polarity}. 
One drawback of natural logic is that it is too strict. For example, natural logic cannot readily accommodate paraphrases or temporal reasoning. Our proposed approach relaxes the strict requirements of natural logic formalism, relying instead on text morphing \cite{huang2018text} and off-the-shelf NLI models. 

Our work is also related to explainable NLI \cite[ inter alia]{camburu2018explainablenli, Thorne2019GeneratingTE, camburu-etal-2020-make}. Importantly, in our proposed approach, the explanations are guaranteed to be faithful \cite{Kumar2020NILEN}, as they are constructed based on the atomic edits produced by the morphing model.

Tangentially, our proposed approach resembles work on modeling edit processes \cite{guu-etal-2018-generating, awasthi-etal-2019-parallel, reid-neubig-2022-learning, reid2023diffuser}. Very relevant is the work on text morphing \cite{huang2018text}, which we repurpose to generate atomic edits to transform the premise into the hypothesis. % We are perhaps the first to demonstrate downstream application of this task.

We also leverage off-the-shelf NLI models to produce the final label. We refer the interested reader to the survey of \citet{Storks2019RecentAI}. Specifically, we use transformer-based NLI models \cite{Vaswani2017AttentionIA, Devlin2019BERTPO, Liu2019RoBERTaAR, Lewis2019BARTDS}, typically trained on a mixture of NLI datasets \cite{sick, bowman2015large, williams-etal-2018-broad}. 

% see https://nlp.stanford.edu/~manning/papers/natlog-cm.pdf and https://logic.pku.edu.cn/ann_attachments/zz_zzzslides.pdf (slide 57)


% For example, \citet{MacCartney2007NaturalLF} .


% Natural Logic \cite{MacCartney2007NaturalLF, angeli-manning-2014-naturalli, Krishna2021ProoFVerNL, Strong2024ZeroShotFV, korakakis-vlachos-2023-improving, rozanova-etal-2022-decomposing, feng-etal-2022-neuro}.

% Other NLI \cite{Stacey2023AtomicIF, bowman2015large}.

% Explainable NLI \cite{camburu2018explainablenli} and explanations in general \cite{camburu-etal-2020-make}.


% Text Morphing \cite{huang2018text}.

% Other works (modelling edits) \cite{awasthi-etal-2019-parallel, reid-neubig-2022-learning, guu-etal-2018-generating, reid2023diffuser}.