\section{Introduction} \label{sec:intro}

% transformers
% 
% 
Large language models (LLMs)—and the transformer architecture they're based on \citep{vaswani2023attentionneed}—revolutionized countless different fields, from machine translation and search engines to DNA analysis and protein research \citep{devlin2018bert, khattab2020colbert, ji2021dnabert, chandra2023transformer}. 
% 
% 
% why we need interpretability
% 
% 
As they evolve, disrupting their previous generation's benchmarks, transformers' internal workings remain largely unknown. Given their widespread presence in daily life and work, significant research efforts have been devoted to fully describing transformer-based computations. 
% 
% 
% mechanistic interpretability
% 
% 
Mechanistic interpretability \cite{bereska2024mechanisticinterpretabilityaisafety} is a research field that aims to link a model’s components to specific output features. 
% 
% 
% limiti degli approcci precedenti
% 
% 
While exciting results have been achieved in this area \citep{nanda2023progressmeasuresgrokkingmechanistic, elhage2022superposition}, they remain limited to toy models trained for ad-hoc tasks, far from the LLMs used for real-world applications. \chris{Exand the gap in the research} \newline
% 
% 
% il nostro spazio di ricerca
% 
% 
This work develops interpretability tools that are \textit{architecture and scale agnostic}. Moreover, we examine \textit{real-world LLMs}, including Llama \citep{touvron2023llamaopenefficientfoundation}, Gemma \citep{gemmateam2024gemmaopenmodelsbased} and GPT \citep{radford2018improving} with different parameter counts, ranging from 1 to 9 billions, \textit{without training or fine-tuning} any of them. \chris{off-the shelf, frozen model checkpoints}

Other recent work \citep{sun2025idiosyncrasieslargelanguagemodels} examined fully trained LLMs by inspecting the tokens they generate in inference, and found that different models tend to produce `characteristic tokens'. Hence, given a generated text, the authors could attribute it to a particular LLM with high accuracy.\newline
This work, instead, shows that it is possible to identify a particular LLM architecture by inspecting its intermediate predictions with Shannon entropy. Moreover, we show that it is also possible to \textit{characterize the computational `task'} (e.g.~ generative, semantic, syntactic) the model is performing by analysing the Shannon entropy of these internal states. Surprisingly, we found that the these same quantities identify with a high degree of accuracy correct and wrong answers given by LLMs on multiple choice questions from the MMLU dataset \cite{hendrycks2021measuringmassivemultitasklanguage}.

Previous research employed information theory in LLMs. For example, entropy was used to stabilize the training procedure \cite{10.5555/3618408.3620117} and evaluate ``compression'' in LLMs \cite{wei2024differanknovelrankbasedmetric}, while mutual information was used to study chain-of-thought \cite{ton2024understandingchainofthoughtllmsinformation}. \newline
This work, instead, prototypes the use of information theory, and entropy in particular, as an interpretability tool for LLMs. In particular, we achieve this \textit{without training any probes}, but by simply using the already trained LLM's decoder.

\subsubsection*{Summary of Contributions}
Here, we outline a summary of our contributions. 
\begin{enumerate}
    \item We show that the Shannon entropy of the intermediate predictions identifies the LLM architecture family performing the computations \ref{sec:exp-fingerprint}
    \item We show that the Shannon entropy of the intermediate predictions identifies the type of the computational task that the LLM is performing \ref{sec:exp-ts}
    \item We show that the Shannon entropy of the intermediate predictions identifies correct and wrong answers generated by LLMs on multiple choice questions \ref{sec:exp-tm}
\end{enumerate}
Crucially, we achieve all these on real-world LLMs with different sizes up to 9B parameters without any training or fine-tuning, but only using the already trained LLM's decoder.


\fra{
Scaletta Intorduction (scrivo una proposta ma modificatela, solo che direi ti tenerci una scaletta per ordine)

\begin{itemize}
    \item I transformers soon fichi e tutti li usano (forse trivial per ora lo saltiamo)
    \item per question di sicurezza e per svilupparli ulteriormente è important capire il loro funzionamento
    \item mechanistic interpretability/logit lens
    \item previous tools have narrow applications and requires access to the full model; inoltre c'erano degli studi che identificavano gli llm dai token
    \item il nostro spazio di ricerca è nella mechanistic interpretability per tanti modelli diversi, senza accesso al full model ma solo ai logits a ogni layer; espandiamo la ricerca basata sui singoli token, non solo passando ai logits per ogni layer ma facendo diversi zoom: dall'identificare i modelli che hanno uno di tre profili caratteristici, a identificare il task, a identificare la correttezza della risposta a un prompt di un dato task, mostrando come l'entropia possa essere una sorta di microscopio dei processi di informazione del transformer
    \item information theory
\end{itemize}

}

