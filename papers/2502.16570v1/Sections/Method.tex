\section{Method} \label{sec:method}
The aim of our framework is to find and characterize the information-theoretic signature of transformer computations. \entropylens's pipeline comprises three steps and is described in Figure \ref{fig:architecture}.

\paragraph{Notation} We denote the input sentence comprising tokens $t_1,\dots, t_N$ by $S=(t_i)_{i=1}^N$. Then, $x_j^i$ denotes the activations of the token $t_j$ after block $i$ for $j\in\{1,\dots,N\}$ and $i\in\{1,\dots,L\}$. Since our analysis focuses on the logits extracted from the intermediate layers of the transformer, it will be useful to distinguish between \textit{normalized} and \textit{unnormalized} logits. We define $W:=\softmax\circ\, D$ and $y_j^i:=W(x_j^i)$ the normalized logits of the token $t_j$'s activations after layer $i$.


The core of our methodology is to analyze the entropy of the generated tokens' intermediate representations $y_j^i$. These vectors are probability distributions, as they are the output of a $\softmax$. To obtain a single quantity that summarizes the information they contain, we compute their entropy $H(y_j^i)$. For one generated token, we can consider the entropy of all of its intermediate predictions $H(y_j^i)$ for $i\in\{1,\dots,L\}$. This leads us to the definition of entropy profile:

\begin{definition}[Entropy profile]\label{def:entropy-profile}
    Let $h_j^i=H(y_j^i)$ be the entropy of the intermediate representation of token $t_j$ after block $i$ and residual connection. The entropy profile of the next generated token is defined as
    \begin{equation}
        h_N = \bigoplus_i h_N^i
    \end{equation}
    where $\bigoplus$ denotes any aggregation function.
\end{definition}
In our experiments, we set $\bigoplus$ to be concatenation, so that $h_N=(h_N^1,\dots,h_N^L)^\top$, but other choices are possible. The extraction of entropy profiles is the step 1 of our pipeline.

Then, we fix the number of tokens that the LLM is required to generate, $T$ and repeat the same procedure for each of them, leading us to the next definition:

\begin{definition}[Aggregated entropy profile] \label{def:aggregated-entropy-profile}
Let $h_{N+t}$ be the entropy profiles according to Definition \ref{def:entropy-profile} for $t\in\{0,\dots,T-1\}$, i.e.~ the entropy profile of each token generated sequentially by a transformer. The aggregated entropy profile of the next $T$ generated tokens is defined as 
\begin{equation}
    h_{[N:T]} = \bigotimes_{t=0}^{T-1} h_{N+t}
\end{equation}
where $\bigotimes$ denotes any aggregation function. 
\end{definition}

Note that $\bigotimes$ in Definition \ref{def:aggregated-entropy-profile} need not be the same as $\bigoplus$ defined in Definition \ref{def:entropy-profile}. In our experiments, we set both of them to be concatenation, so that $h_{[N:T]}$ is the matrix with $h_{N+t}$ as columns, that is $(h_{[N:T]})_{t}^i=h_{N+t}^i$ for $i\in\{1,\dots,L\}$ and $t\in\{0,\dots,T-1\}$. The aggregation of entropy profiles is the step 2 of our framework.

The last step of our framework is classification, where we feed the aggregated entropy profile to a classifier $\classifier$ to determine whether it contains sufficient information to identify a particular `entity'. 

In our experiments, we examine whether aggregated entropy profiles identify model family (Section \ref{sec:exp-fingerprint}), task type (Section \ref{sec:exp-ts}), and correct and wrong answers to multiple choice questions (Section \ref{sec:exp-tm}) in LLMs. Additionally, we apply the same pipeline to ViTs showing the flexibility of the proposed methodology (Section \ref{sec:exp-vit}).  %from the MMLU dataset. 
In our experiments, we take $\classifier$ to be a k-NN classifier. Classification of the aggregated entropy profiles is the step 3 our framework.
%\chris{Ho tolto "simple" a KNN perché simple non vuol dire niente, tuttavia vorrei comunque giustificare tale scelta. Possiamo dire che ci consente di osservare in modo più diretto quanto/quale segnale arriva in quanto basato interamente tra le distanza(?) Troppo supercazzola?}

%To understand the internal mechanisms of a transformer, we analyze how different layers process information in different ways. At the output of each block, we read the hidden representation vector, unembed it, and apply the softmax. For each of the resulting distributions, we calculate the entropy. We study how the entropy changes between consecutive blocks of the transformer. With the pipeline defined above, we obtain an entropy value for each block and each generated token. We then concatenate the entropies of multiple consecutive tokens within a fixed window. This results in a vector with entropy values for each block and each token, representing how each block processes information by increasing or decreasing token entropy. Depending on the analysis, we average these values—while keeping the division by blocks—across different prompts belonging to the same task. Once the entropy signature is generated, we analyze it using a simple classifier such as a K-NN.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\linewidth]{Figures/antani_arch_1.png}
    %\caption{(Left) Pipeline for calculating the entropy of each layer: the hidden representation output from each block is converted into token probabilities, as with \texttt{logit-lens}, and then their entropies are computed. (Right) How the entropies are processed to obtain a task classification.}
    \caption{\entropylens's pipeline. (Left) A diagram representing a transformer architecture: hidden representations are converted into intermediate predictions with $W$ before calculating their entropy $H$. (Right) A diagram representing our framework: step 1: entropy profile extraction, step 2: entropy profile aggregation and step 3: classification.}
    \label{fig:architecture}
\end{figure}


