\section{Background} \label{sec:bg}

\subsection{Information Theory}
The main information-theoretic quantity used in our study is \textit{entropy}. Given a discrete\footnote{Although entropy can be naturally extended to the continuous case with probability \textit{density} functions, we restrict ourselves to the discrete case as it is the most relevant to our study.} random variable $X$ with outcomes $x_i$ and probability mass function $p$, the Shannon entropy $H$ of $X$ is defined as 
\begin{equation}
    H(X) = - \sum_ip(x_i)\log p(x_i) = \mathbb{E}[-\log p(X)]
\end{equation}
Shannon proved that this function is the only one—up to a scalar multiplication—that satisfies intuitive properties for measuring `disorder'. These include being maximal for a uniform distribution, minimal for the limit of a Kronecker delta function, and ensuring that $H(A, B) \leq H(A) + H(B)$ for every possible event $A$ and $B$. The same function already existed in continuous form in physics, where it linked the probabilistic formalism of statistical mechanics with the more phenomenological framework of thermodynamics, where the term `entropy' was originally coined. \newline
Next, we study the entropy of vocabulary predictions—a quantity that is maximal when the prediction assigns equal probability to all tokens, minimal when it assigns zero probability to all but one token, and takes intermediate values when probability is distributed across multiple tokens, consistent with the previously mentioned properties. 

\subsection{The Transformer}
\paragraph{Architecture}
\begin{wrapfigure}{r}{0.3\textwidth} % "r" per destra, "l" per sinistra
    \vspace{-0.4cm}
    \centering
    \includegraphics[width=0.75\linewidth]{Figures/TransformerBlock.png}
    \caption{Structure of a generic Transformer block.}%, composed of layer norms, attention mechanisms, and a multi-layer perceptron (MLP).}
    \label{fig:transf_block}
    \vspace{-0.4cm}
\end{wrapfigure}
The transformer \citep{vaswani2023attentionneed} is a deep learning architecture widely applied in language modelling with LLMs \citep{brown2020languagemodelsfewshotlearners} and computer vision \citep{dosovitskiy2021imageworth16x16words}. Transformer computations happen through \textit{transformer blocks} and \textit{residual connections}, as exemplified in Figure \ref{fig:architecture}. While various design choices are possible, blocks are usually a composition of layer normalization \citep{zhang2019rootmeansquarelayer}, attention, and multi layer perceptrons (MLPs), as shown in Fig. \ref{fig:transf_block}. Residual connections, instead, sum the output of the layer $i-1$ to the output of the layer $i$.   

Inside a single transformer block, the information flows both \textit{horizontally} and \textit{vertically}. The former, enabled by the attention mechanism, allows the token representations to interact with each other. In a language modelling task, for example, this is useful to identify which parts of the input sequence—the sentence prompt—should influence the next token prediction and quantify by how much. The latter vertical information flow allows the representation to evolve and encode different meanings or concepts. Usually, the dimension of the latent space is the same for each block in the transformer. The embedding spaces where these computations take place are generally called the \textit{residual stream}.

\paragraph{Computation schema} 
LLMs are trained to simply predict the next token in a sentence. That is, given a sentence prompt $S$ with tokens $t_1,\dots,t_N$, the transformer encodes each token with a linear encoder $E$. Throughout the residual stream, the representation $x_N$ of the token $t_N$ evolves into the representation of the token $t_{N+1}$, which is then decoded back into token space via a linear decoder $D$, sometimes set to $E^\top$, tying the embedding weights and the decoder. Finally, the logits—the output of $D$—are normalized with $\softmax$ to represent a probability distribution over the vocabulary. We summarize this operation with the function $W:=\softmax\circ\,D$.

In formal terms, information processing can be expressed using the encoder, decoder, Transformer block $f$, and residual connection:
\begin{align}
    x_j^0 & = E(t_j)\\
    x_j^i &  = f^i(x_j^{i-1}) + x_j^{i-1}  \\
    y_j^{i} &  = W(x_j^i) 
\end{align}
where $j\in\{1,\dots, N\}$ ranges over the number of tokens in the prompt and $i\in\{0,\dots,L\}$ ranges over the number of layers. Hence, $x_j^i$ represents the activations of token $t_j$ after layer $i$. 

% Functionally, the transformer is a function $f$ that can be recursively defined in terms of its blocks and residual connections:
% \begin{align}
%     x_j^0 &= f^0(t_j) := E(t_j)\\
%     x_j^i &= f^i(x_j) + x_j^{i-1}  \\
%     x_j^{L+1} &= W(x_j^L)
% \end{align}
% where $j\in\{1,\dots, N\}$ ranges over the number of tokens in the prompt and $i\in\{0,\dots,L\}$ ranges over the number of layers. Hence, $x_j^i$ represents the activations of token $t_j$ after layer $i$.

\subsubsection{Instruct Models}
Training a Large Language Model (LLM) requires vast amounts of data and is generally divided into multiple phases. 

\textbf{Pretraining}: The model is exposed to large datasets through self-supervised tasks, such as next-token prediction or similar variants. This phase helps the model learn a broad range of general knowledge.

\textbf{Fine-tuning}: This phase teaches the model to generate more useful and coherent responses. Two main strategies are used:
\textit{Chat}: The model is trained on structured conversations between a user and the model, with clearly defined roles.
\textit{Instruct}: The model learns from simple commands, without a predefined dialogue structure.

\textbf{RLHF} (optional): Some models undergo Reinforcement Learning from Human Feedback (RLHF) to further refine their responses based on human preferences.

For our experiments, we used off-the-shelf models without RLHF to analyze information processing in a less biased transformer version. We also focused on Instruct models instead of Chat models for two reasons: 1. the Instruct strategy aligns better with our experimental setup 2.
Instruct models are more flexible and often preferred for practical applications.