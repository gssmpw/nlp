\section{Introduction} \label{sec:intro}
% transformers are cool
% 
% 
Transformer-based architectures \citep{vaswani2023attentionneed} are widely employed as state of the art models in several fields, from machine translation and search engines to DNA
analysis and protein research \citep{devlin2018bert,khattab2020colbert,ji2021dnabert,chandra2023transformer}. Their declination in language modeling of large corpora is referred to as large language models (LLMs), and in computer vision as vision transformers (ViTs). Despite their success and ubiquity, transformers' inner workings remain largely unknown,
% 
% 
% why we need interpretability
% 
% 
resulting in unpredictable behaviour \citep{wei2022emergentabilitieslargelanguage} and reliability concerns \citep{schroeder2025trustllmjudgmentsreliability, Huang_2025}.
% 
% 
% mechanistic interpretability
% 
% 

Therefore, considerate research efforts are devoted to transformer-based architectures interpretability, mostly focusing on LLMs \citep{nanda2022transformerlens, bereska2024mechanisticinterpretabilityaisafety, elhage2021mathematical, elhage2022superposition} and ViTs \citep{chefer2021transformerinterpretabilityattentionvisualization}. 
% 
% 
% limiti degli approcci precedenti
% 
% 
While exciting results have been achieved in this area, they remain limited to toy models and simplified setups, both very different from real use-case conditions. Moreover, these methods often require training a set of probes \citep{nostalgebraist2020, belrose2023elicitinglatentpredictionstransformers} or full models on ad-hoc tasks \citep{nanda2023progressmeasuresgrokkingmechanistic}, making them architecture or even model specific and computationally expensive. 
% 
% 
% Why these limitations matter
% 
% 
These limitations restrict the scope and usability of current methodologies, rendering them unsuitable for off-the-shelf or large-scale transformer-based architectures.
% 
% 
% our selling points
% 
% 

Hence, we develop \entropylens, a scalable framework to address these limitations. Our methodology is architecture agnostic and applicable to frozen off-the-shelf large-scale transformers. In our experiments, we consider both LLMs, including Llama \citep{touvron2023llamaopenefficientfoundation}, Gemma \citep{gemmateam2024gemmaopenmodelsbased}, GPT \citep{radford2018improving} 
up to 9B parameters, and ViTs \citep{wu2020visual} and data-efficient image transformers (DeiTs) \citep{touvron2021training, rw2019timm}.
%\chris{lascerei perdere la footnote} 
%\footnote{Although in our experiments we considered architectures up to 9B parameters, our method does not have a model size upper bound.}. 
In particular, we analyze the evolution of the generated tokens' Shannon entropy after each intermediate block in the residual stream, as described in Section \ref{sec:method}. In Section \ref{sec:exp}, we show that these quantities constitute an information-theoretic signature of which and how the computation is performed. Finally, our work paves the way for several potential research directions involving information theory for LLM interpretability, as outlined in Section \ref{sec:conclusion}.


Our contributions are as follows:
\begin{enumerate}
    \item We develop a scalable model agnostic methodology grounded in information theory for frozen off-the-shelf large-scale transformer architectures interpretability (Section \ref{sec:method}).
    \item We analyze the Shannon entropy of LLMs' generated tokens' intermediate predictions, showing that these quantities (1) identify the model family that generated them (2) identify the `task type' of the prompt (3) correlate with the correctness of LLM generated answers to multiple choice questions (Section \ref{sec:exp}).%LLM generated answers to multiple choice questions from the MMLU dataset \citep{hendrycks2021measuringmassivemultitasklanguage} (Section \ref{sec:exp}).
    \item We demonstrate that the same framework is adaptable to domains outside of language modeling. In particular, we apply it to computer vision to show that the Shannon entropy of ViTs' intermediate predictions identify the model family that generated them (Section \ref{sec:exp}), similarly to the LLMs case. 
\end{enumerate}

% \rick{
% Struttura:
% \begin{itemize}
%     \item Transformers are cool
%     \item Why we need interpretability of transformers
%     \item Current ways to do interpretability
%     \item Limitations of current methodologies
%     \item Why these limitations matter
%     \item Why our approach is better
%     \item Summary
% \end{itemize}

% }


% \fra{
% Scaletta Intorduction (scrivo una proposta ma modificatela, solo che direi ti tenerci una scaletta per ordine)

% \begin{itemize}
%     \item I transformers soon fichi e tutti li usano (forse trivial per ora lo saltiamo)
%     \item per question di sicurezza e per svilupparli ulteriormente è important capire il loro funzionamento
%     \item mechanistic interpretability/logit lens
%     \item previous tools have narrow applications and requires access to the full model; inoltre c'erano degli studi che identificavano gli llm dai token
%     \item il nostro spazio di ricerca è nella mechanistic interpretability per tanti modelli diversi, senza accesso al full model ma solo ai logits a ogni layer; espandiamo la ricerca basata sui singoli token, non solo passando ai logits per ogni layer ma facendo diversi zoom: dall'identificare i modelli che hanno uno di tre profili caratteristici, a identificare il task, a identificare la correttezza della risposta a un prompt di un dato task, mostrando come l'entropia possa essere una sorta di microscopio dei processi di informazione del transformer
%     \item information theory
% \end{itemize}

% }

