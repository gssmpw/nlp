\section{Related Work} \label{sec:related}
% \paragraph{Lenses and Early Exit in LLMs}
\paragraph{Lenses in LLMs} Mechanistic interpretability \citep{bereska2024mechanisticinterpretabilityaisafety} aims to provide a precise description and prediction of transformer-based computations. Common tools in the field are \textit{lenses}, which are a broad class of probes deployed in intermediate steps of the residual stream. For example, \texttt{logit-lens} \citep{nostalgebraist2020} uses the model's decoder function to decode the intermediate activations in the vocabulary space. \texttt{tuned-lens} \citep{belrose2023elicitinglatentpredictionstransformers} refines this technique by training a different affine probe at each layer, instead of only using the pretrained model's decoder function. Building on the Transformer-Lens library \citep{nanda2022transformerlens}, we propose \entropylens, which employ \texttt{logit-lens} to study and characterize LLMs' computations via their decoded version with information theory. %Moreover, recent research provided empirical evidence that stopping inference at intermediate layers, or `early-exit', is a natural capability of LLMs \citep{shan2024early} and showed that it is possible to use the insights derived thereof to speed up training and inference of LLMs \citep{Elhoushi_2024}. \fra{per me si può anche togliere l'ultima frase e chiamare questa sottosezione "Lenses in LLMs"} \rick{Va bene, l'avevo messa perché mi avevate linkato i due lavori lì, però ci sta toglierla}

\paragraph{Transformers' Circuits}
Another approach to mechanistic interpretability aims to identify and understand the specific sub-computations, or \texttt{circuits}, in a neural network \citep{saphra2024mechanistic}. \cite{olah2020zoom} pioneered this approach, introducing the concept of circuits and demonstrating their existence in small models through manual analysis. In transformers, circuits are hypothesized to act as agents that read from and write to the residual stream, which acts as a form of memory. This has been demonstrated in a simplified transformer model composed only of attention blocks (without MLPs) \citep{elhage2021mathematical}. However, there is evidence that full transformers exhibit similar behavior. This evidence—linked to the concept of \texttt{superposition}, the idea that models can represent more features than the available dimensions by compressing multiple features into one \citep{elhage2022superposition}—is supported by studies on sparse autoencoders, which demonstrate the ability to decompose representations into simpler components \citep{bricken2023monosemanticity}. \citet{conmy2023towards} developed a toolkit to facilitate mechanistic interpretability, offering techniques like activation patching and weight factorization. Building upon this, \cite{conmy2023automatedcircuit} explored automated circuit discovery methods, addressing the challenge of scaling analysis to larger models. These works collectively emphasize the importance of understanding the concrete computational steps within LLMs, moving beyond superficial observations to reveal the underlying mechanisms.

\paragraph{Information Theory in Transformers} Information theory has been studied both in connection to the training phase of LLMs and their interpretability. For example, a collapse in attention entropy has been linked to training instabilities \citep{10.5555/3618408.3620117} and matrix entropy was employed to evaluate ``compression'' in LLMs \citep{wei2024differanknovelrankbasedmetric}. Additionally, mutual information was used to study the effectiveness of the chain-of-thought mechanism \citep{ton2024understandingchainofthoughtllmsinformation}. Our work, instead, shifts the focus on the vocabulary's natural domain. Through \entropylens, we use information theory to study the evolution of entropy of the intermediate layers' decoded logits.

