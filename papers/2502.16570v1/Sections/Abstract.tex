\begin{abstract}
    Transformer models have revolutionized fields from natural language processing to computer vision, yet their internal computational dynamics remain poorly understoodâ€”raising concerns about predictability and robustness. In this work, we introduce \entropylens, a scalable, model-agnostic framework that leverages information theory to interpret frozen, off-the-shelf large-scale transformers. By quantifying the evolution of Shannon entropy within intermediate residual streams, our approach extracts computational signatures that distinguish model families, categorize task-specific prompts, and correlate with output accuracy. We further demonstrate the generality of our method by extending the analysis to vision transformers. Our results suggest that entropy-based metrics can serve as a principled tool for unveiling the inner workings of modern transformer architectures.
\end{abstract}
