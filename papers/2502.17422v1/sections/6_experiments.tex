\section{ViCrop Method Analysis}
\label{sec:experiments}

In this section, we apply our proposed visual cropping methods to two open-source SOTA MLLMs, InstructBLIP (Vicuna-7B)~\citep{instructblip} and LLaVA-1.5 (Vicuna-7B)~\citep{llava1.5}. We evaluate their effectiveness in improving the perception of smaller visual concepts on 4 detail-sensitive datasets (TextVQA
\footnote{$^\dagger$In TextVQA evaluation, we do not provide externally extracted OCR tokens to the MLLM since we want to measure its true perception, this differs from the setup in the original paper. See more discussion in~\cref{app:imp}.}
~\citep{textvqa}, V$^*$~\citep{v-star}, POPE~\citep{li2023pope}, DocVQA~\citep{docvqa}), and their ability to maintain performance on larger visual concepts in 3 general-purpose datasets containing mostly large objects (GQA~\citep{hudson2019gqa}, AOKVQA~\citep{schwenk2022okvqa}, VQAv2~\citep{goyal2017vqav2}). InstructBLIP uses the hyper-parameters $N=16, m=15, k=2$ and input image resolution of $224\times 224$. LLaVA-1.5 uses $N=24, m=14$ and input image resolution of $336\times 336$.  When reporting accuracy, we compute VQA-score\footnote{https://visualqa.org/evaluation.html} for all benchmarks except GQA. For GQA, we compute accuracy using the official code.\footnote{https://cs.stanford.edu/people/dorarad/gqa/evaluate.html}. See~\cref{app:imp,app:dataset,app:zeroshot_instruction} for further details about implementation, datasets, and prompts.


\begin{figure*}[t]
    \centering
    \includegraphics[trim=0 0 0 0, clip, width=\textwidth]{figures/method_case.pdf}
    \caption{Examples of \rel{} helping MLLMs correct their mistakes (cyan-colored bounding box shows cropped region by \rel{}; zoom-in insets are displayed for better readability).}
    \label{fig:crop_examples}
\end{figure*}

\input{sections/tab_main_results}

\textbf{ViCrop Improves VQA Accuracy.} In \cref{fig:crop_examples}, we show a few examples of the ViCrop helping the MLLM correct itself (more examples in~\cref{app:examples}), and in~\cref{tab:main_result}, we report the accuracy of the proposed ViCrop methods on the VQA benchmarks. We observe that all methods significantly improve the accuracy of the original MLLMs (\emph{no cropping}) on detail-sensitive benchmarks, without requiring any training, while maintaining the MLLMs' performance on benchmarks with larger visual concepts. Thus, the accuracy gain on fine details (most notably in TextVQA and V$^*$) does not seem to come at the cost of accuracy on larger visual details and relations. We also observe that the accuracy gain for LLaVA-1.5 is more substantial than for InstructBLIP. This can be explained by the fact that InstructBLIP only trains its connector and not its backbone LLM during tuning---the LLM does not adapt to use the image tokens, rather the image tokens are adapted to optimally prompt the LLM---and therefore the LLM cannot effectively use the additional (cropped) image tokens provided through visual cropping. Nonetheless, the results show that ViCrop can be effectively applied to different MLLMs, and is a promising inference-time solution for mitigating the perception limitation observed in~\cref{sec:human_crop}.

\input{sections/tab_layer_res_ablation}

\textbf{Ablation Study on the Choice of Layer.}
To understand the importance of the choice of an informative layer for \rel{} and \gra{} (as discussed in~\cref{sec:vicrop}), in~\cref{tab:layer_res} we compare the accuracy of these methods when simply taking the average of all layers in ${A_{rel}}$ and $\tilde{A}_{si}$, respectively, on TextVQA. We observe that \rel{} is robust to this choice and \gra{} declines about $3.5$ percentage points in accuracy. Importantly, both methods still improve the MLLMs' accuracy even when using the layer average, suggesting that averaging is a suitable choice in the absence of any data for selecting a layer.

\textbf{Ablation Study on the High-Resolution ViCrop.} In~\cref{sec:vicrop}, we proposed a two-stage strategy for processing the very high-resolution images in the V$^*$ benchmark. To see how effective this strategy is, in~\cref{tab:layer_res} we compare the accuracy of ViCrop methods with and without this high-resolution strategy on V$^*$. We observe that while this strategy is very beneficial to LLaVA-1.5, it declines the performance of \gra{} and \pgra{} for InstructBLIP. However, all methods, with and without this strategy, still improve the MLLMs' accuracy.

\textbf{ViCrop with External Tools.}
In addition to the internal ViCrop methods, we also considered the use of external off-the-shelf models to find the region of interest in an image for visual cropping. Specifically, we utilized SAM~\citep{kirillov2023segment}, YOLO~\citep{yolo}, and CLIP~\citep{clip} to find the most relevant part of an image to a given question (details of these external ViCrop methods are provided in~\cref{app:external}). In~\cref{tab:externals_time}, we compare the accuracy of external ViCrop methods to the internal methods on TextVQA. While external models are also effective in improving the accuracy of MLLMs, they are weaker than all the proposed internal ViCrop methods, thus we did not explore them further.

\input{sections/tab_externals_time}

\textbf{Inference Time Overhead.}
In~\cref{tab:externals_time}, we report the average inference-time overhead of the proposed visual cropping methods on GPU (NVIDIA RTX A6000) and CPU (Intel(R) Gold 5317 CPU @ 3.00GHz) and compare with the per-answer-token processing time of the MLLMs.
We see that all proposed methods (except SAM) are reasonably fast (1 to 2 seconds overhead on GPU). For example, computing the visual cropping with \rel{} takes the time of generating only 5 tokens by the MLLM. \textbf{Note that our methods’ time overhead will not scale with the number of answer tokens and is constant regardless of how long the answer is} because our external methods do not need any answer token, and internal methods only need the starting answer token (see~\cref{sec:vicrop}). In contrast, MLLMs’ inference time scales approximately linearly with the number of answer tokens.
