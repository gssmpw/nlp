\section{MLLMs' Sensitivity to the Size of Visual Concepts}

\input{sections/tab_bbox_size}

\label{sec:human_crop}
In this section, our goal is to quantitatively study our qualitative observations in~\cref{fig:crop_fig} that MLLMs struggle with describing small visual details in images. To that end, we consider the TextVQA dataset, in which for each question we can find the image ground-truth bounding box that contains the correct textual answer. We partition its validation set into three groups based on the relative size of the ground-truth bounding box $S = \frac{A_{bb}}{A_{total}}$, where $A_{bb}$ denotes the area of the ground-truth bounding box, and $A_{total}$ the total area of the image: 1) $S<0.005$ (\texttt{small}) consisting of 773 question-image pairs, 2) $0.005\leq S<0.05$ (\texttt{medium}) consisting of 2411 question-image pairs, and 3) $S\geq 0.05$ (\texttt{large}) consisting of 1186 question-image pairs. We chose TextVQA for this study because it contains a significant number of questions about small visual concepts, and textual answers are harder for MLLMs to guess from other side information in the image (compared to object types and attributes).

\textbf{Sensitivity Study.} If a model's perception is not sensitive to the size of visual concepts, we expect it to have similar accuracy in all three partitions.  In~\cref{tab:bbox_size}, we observe that the accuracy of all MLLMs declines as the ground-truth bounding box becomes relatively smaller (right to left on the \emph{no~cropping} rows). BLIP-2 and InstructBLIP are not trained on TextVQA (\ie, are zero-shot models), and their accuracy declines by $24$ and $23$ absolute percentage points between the \texttt{large} and \texttt{small} partitions, respectively. LLaVA-1.5 and Qwen-VL are trained on the training set of TextVQA, yet, their accuracy also declines by $11$ and $12$ between the \texttt{large} and \texttt{small} partitions, respectively. Lastly, even the most recent commercial GPT-4o, with an unknown training set that might include all of TextVQA, is suffering from a $7$ percentage point decline in accuracy between small and medium partitions. These findings suggest that MLLMs have a bias against perceiving smaller visual concepts.


\textbf{Intervention Study.} The perceptual limitation we observed above might be merely correlated with size. To study whether this limitation is causally related to size, we conduct an intervention study where we provide the MLLMs with visually cropped images based on the ground-truth bounding boxes, denoted as \hc{}. More specifically, for each image-question pair and each MLLM, we crop the smallest square-shaped region containing the ground-truth bounding box from the image, and resize it to the input image resolution of the MLLM (the square-shaped cropping prevents extreme deformation of the cropped image when resizing). The cropped image is then provided to the MLLM in addition to the original image-question pair (see more details in~\cref{fig:methods}). We observe in~\cref{tab:bbox_size} that \hc{} significantly improves the accuracy of all MLLMs on the \texttt{small} and \texttt{medium} partitions, and to a lesser extent on the \texttt{large} partition. These findings show that the perception limitation is indeed caused by the size of the visual concepts, and that visual cropping can be a promising direction to mitigate this limitation.