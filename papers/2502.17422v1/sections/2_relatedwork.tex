\section{Related Works}
\label{sec:related_works}

\textbf{Multimodal Large Language Models~(MLLMs).}
MLLMs are foundation models capable of handling diverse language and vision tasks. These models fall into two categories: \emph{end-to-end pretrained} and \emph{modular pretrained}. End-to-end models process joint image-language data through architectures such as dual-encoder~\citep{clip}, fusion-encoder~\citep{li2021align-before-fuse}, encoder-decoder~\citep{cho2021unifying}, and unified transformer~\citep{wang2022image-as-foreign-lang}, using objectives like image-text matching, contrastive learning, and masked language modeling. Modular pretrained models, which dominate recent state-of-the-art approaches, avoid costly full pretraining by adapting existing components: 
BLIP2~\citep{li2023blip} and InstructBLIP~\citep{instructblip} train a Transformer-based connector between a frozen pretrained ViT~\citep{vit} image encoder and a frozen LLM, which transforms ViT output tokens into a fixed set of image tokens in the input space of the LLM; Qwen-VL~\citep{qwen-vl}, similarly uses a fixed-length token connector (a single cross-attention layer), but trains both the connector and the LLM; LLaVA~\citep{llava} and LLaVA-1.5~\citep{llava1.5} instead use a linear projection and a two-layer MLP as their connectors, respectively, and train both.
Our work will contribute to a better understanding of the perception limitations of MLLM and improve their perception scalably and without training, offering orthogonal benefits to existing approaches.

\textbf{Visual Localization Methods.}
Dedicated visual localization methods, such as YOLO~\citep{yolo}, SAM~\citep{kirillov2023segment}, and GLIP~\citep{glip}, rely heavily on dense spatial annotations for identifying salient image regions. Native approaches, such as Grad-CAM~\citep{gradcam}, localize regions by analyzing gradients from classifier decisions without spatial supervision. Prior work adapts Grad-CAM to BLIP~\citep{blip} leveraging its dedicated image-text similarity computation neural network called the Image-Text Matching network~\citep{pnpvqa,pnpvqa2}. In this work, we derived a more general way for localizing the attention of MLLMs to images, not relying on the specific BLIP architecture. Several recent works have explored ways to improve the visual localization capability of MLLMs for visual question answering, including chain-of-thought~\citep{visualcot,llavaplus}, tool-using~\citep{v-star}, and visual programming approaches~\citep{suris2023vipergpt,visualprogramming}. In contrast, we demonstrate that MLLMs can often effectively localize the visual subject of a question in their internal states, and propose training-free methods to leverage their internal states for improving their visual perception.

\textbf{Visual Perception Limitations in MLLMs.} The difficulty of answering questions about small objects in images has been observed by several prior and concurrent works~\citep{vicrop1,perlim,liu2024llavanext,v-star}, which have explored mitigating solutions based on high-resolution fine-tuning~\citep{liu2024llavanext,navit,qwen2vl}, multi-agent pipelines~\citep{v-star}, and use of visual cropping~\citep{vicrop1}. In this work, we provide more extensive evidence for this difficulty, establish its causal effect on MLLMs' performance, and show that it is rooted in a failure to observe small visual details as opposed to a failure to locate small objects. Several works have also shown that MLLMs suffer from object hallucination~\citep{li2023pope,rlhfv}. Furthermore, \citet{perlim} have shown visual blind spots in MLLMs---i.e., locations on the image where the MLLMs' perception degrades---as well as their sensitivity to visual quality, presence of visual distractors in the image, and even local object location perturbations.