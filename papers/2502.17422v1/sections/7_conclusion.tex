\section{Conclusion}
In this work, we qualitatively and quantitatively showed that there exists a perception bias against small visual details in widely-used MLLMs. Then we found that MLLMs often know where to look even if they fail to answer the question, indicating that the bias toward small visual details is rooted in a perception limitation rather than a localization limitation. To mitigate this limitation, we proposed multiple automatic visual localization methods as scalable and training-free solutions based on models' internal dynamics while answering the visual questions. Through evaluation of multiple multimodal benchmarks, we showed that our method can significantly improve MLLMsâ€™ accuracy without requiring any training, especially in detail-sensitive scenarios. Our findings suggest that MLLMs should be used with caution in detail-sensitive applications, and that visual cropping/localization with the model's own knowledge is a promising direction to enhance their performance.

\textbf{Limitations and Future Work.}
The proposed ViCrop methods do not enhance all types of questions equally. We have observed that questions concerning relations and counting are particularly difficult for ViCrop methods to help answer. This is expected as the proposed ViCrop can only focus on one region in the image. We leave extending ViCrop to focus on multiple regions simultaneously for future work. Another limitation of the proposed methods is their time overhead and the addition of visual tokens. While the overhead is reasonable (a few seconds), we believe it can be significantly optimized as an inference-time mechanism, for example by utilizing lower precision, and weight quantization. Furthermore,
Matryoshka Query Transformer (MQT)~\citep{matryoshka} enables MLLMs to have varying visual context size during inference. In our current results, we have shown that our methods can work with two different MLLMs with distinct visual context sizes, so it seems entirely possible that our method can still work with varying visual context size under MQT, which can further reduce our computational cost through rescaling the cropped image. We leave these inference cost optimizations to future works. Lastly, we have observed that the proposed methods tend to have some complementary benefits, and therefore exploring ways to combine them (for example based on the prediction uncertainty) is also a very interesting direction for future research.