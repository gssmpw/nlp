\section{Do MLLMs Know Where to Look?}

\label{sec:where_to_look}
The limitation in perceiving small visual concepts can have two primary reasons: 1) they are hard to locate in the larger image, and 2) their small details are hard to perceive correctly. In~\cref{fig:crop_fig}, we observed that the MLLM's incorrect answer may contain partially correct information, hinting that it might know where to look in the image. In this section, we quantitatively study that observation to answer whether MLLMs' sensitivity to size is rooted in a perception limitation or a localization limitation. To that end, we first utilize the attention maps computed inside the Transformer layers of an MLLM to quantify its spatial attention over the image and then compare the total amount of this attention inside the ground-truth bounding box to other bounding boxes of the same size.

\textbf{MLLMs' Setup.} The considered MLLMs process a given image-question pair $(x,q)$ in four steps (depicted in~\cref{fig:methods}): 1)~the image is divided into $N\times N$ non-overlapping patches and processed by the ViT image encoder into $N\times N$ output tokens; 2)~the ViT output tokens are transformed into the input space of the backbone LLM---by either an MLP (LLaVA-1.5) or a Transformer connector (BLIP-2, InstructBLIP and Qwen-VL)---which we refer to as image tokens; 3)~the image tokens are then prepended to the question tokens and a predefined starting answer token, and fed to the LLM; 4) the LLM is sampled auto-regressively following the starting answer token (we use greedy sampling).


\textbf{Quantifying MLLMs' Spatial Attention over the Image.} We first measure how important each image token is to the MLLM's decision (\emph{answer-to-token attention}) by extracting the softmax cross-attention of the starting answer token to all image tokens in all layers of the backbone LLM, resulting in $A_{st}(x,q) \in \mathbb{R}^{L \times H \times 1 \times T}$, where $L, H$ are the number of layers and heads-per-layer in the LLM, and $T$ is the number of image tokens provided to the LLM. We then take the average over attention heads to arrive at the answer-to-token attention $\hat{A}_{st}(x,q) = \frac{1}{H}\sum_{h=1}^H A_{st}(x,q)$.
Next, we measure how important each image region is to each image token (\emph{token-to-image attention}). For the MLLMs that use a Transformer connector to resample ViT output tokens into a fixed number of image tokens (BLIP-2, InstructBLIP and Qwen-VL), we extract the softmax cross-attention of each image token to all ViT output tokens in all layers of the connector, resulting in $A_{ti} \in \mathbb{R}^{L_c \times H_c \times T \times N^2}$, where $L_c, H_c$ are the number of layers and heads-per-layer in the connector, $T$ the number of learnable query tokens (that become input image tokens to the LLM), and $N^2$ the number of image patches of the ViT image encoder. We then take the average over attention heads to arrive at the token-to-image attention $\hat{A}_{ti}(x) = \frac{1}{H_c}\sum_{h=1}^{H_c} A_{ti} (x)$. For LLaVA-1.5 which uses an MLP to transform ViT output tokens to image tokens, we set $\hat{A}_{ti}(x)$ to the identity tensor.
Finally, we compute the \emph{answer-to-image attention} by computing the tensor product of the answer-to-token and token-to-image attention, resulting in $A_{si}(x,q)\in \mathbb{R}^{L\times L_c \times 1 \times N^2}$ where $A^{mk}_{si}(x,q) = \hat{A}_{st}^m(x,q) \hat{A}_{ti}^k(x)$ (superscripts $m$ and $k$ denote layer indices on the LLM and the connector, respectively).

\begin{figure*}[t!]
    \includegraphics[width=\linewidth]{figures/motivation_case.pdf}
    \caption{Examples of MLLMs knowing where to look despite answering incorrectly. The right panel in each example displays relative attention to the image (defined in~\cref{sec:where_to_look}) of one layer in the MLLM.}
    \label{fig:motivation_case}
    \vspace{-1.5em}
\end{figure*}

\begin{figure*}[b!]
    \centering
    \includegraphics[trim=0 10 0 0, clip, width=0.99\textwidth]{figures/com.pdf}
    \caption{MLLMs' attention ratio across all layers (average with $95\%$ CI over TextVQA). The attention ratio measures how significantly the MLLM is attending to the ground-truth bounding box (defined in~\cref{sec:where_to_look}). We observe that it is greater than 1 in most layers, showing that the MLLMs know where to look in the image even when they fail to answer correctly.}
    \label{fig:where_to_look}
\end{figure*}

\textbf{Relative Attention.} One issue with using the softmax cross-attention is that not all highly attended tokens are semantically relevant to the input question. For example, recent work has observed that Transformers may use several tokens as registers to aggregate global information~\citep{registers}. To emphasize semantically relevant attention, we propose to normalize the answer-to-image attention of an image-question pair $(x,q)$ by its value on a generic instruction $q'$. Specifically, we consider a fixed instruction $q'= $``Write a general description of the image.'', and compute \textbf{relative attention} as $A_{rel}(x,q) = \frac{A_{si}(x,q)}{A_{si}(x,q')}$ under element-wise division. \cref{fig:motivation_case} shows examples of relative attention for LLaVA-1.5 and InstructBLIP ($A^{mk}_{rel}$ at layers $m=14,k=0$ and $m=15, k=2$, respectively).

\textbf{Do MLLMs Know Where to Look?}
Equipped with relative attention, we now return to our question of whether MLLMs have a localization limitation or perception limitation. To that end, we consider the validation set of TextVQA again. For each image-question pair, we first compute the relative attention. We then define \textbf{attention ratio} as the ratio of the total (sum) relative attention inside the answer ground-truth bounding box to its average across all bounding boxes of the same size as the ground-truth on the image. This ratio provides a measure of how significantly the MLLM is attending to the ground-truth bounding box (in the sense of Markov's inequality).
In~\cref{fig:where_to_look}, we plot the average (with $95\%$ confidence interval) of the attention ratio, over the validation set of TextVQA for all layers in the considered MLLMs. The horizontal axis shows the combined layer index $l = m + k \times L$ for $m \in \{0\dots L-1\}$ spanning the number of cross-attention layers in the backbone LLM, and $k \in \{0\dots L_c-1\}$ spanning the number of cross-attention layers in the connector (BLIP-2: $L=24, L_c=6$; InstructBLIP: $L=32, L_c=6$; Qwen-VL: $L=32, L_c=1$; LLaVA-1.5: $L=32, L_c=1$).
In all MLLMs, we observe a significantly larger than 1 attention ratio in most layers, suggesting that the models are attending significantly to the ground-truth bounding box region on the image. Intriguingly, the models show similarly strong attention to the correct region regardless of whether they can answer the question correctly or incorrectly. These observations show that the MLLMs tend to know where to look, even when they answer incorrectly.