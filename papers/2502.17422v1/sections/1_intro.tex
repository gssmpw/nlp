\section{Introduction}
\label{sec:intro}
Multimodal large language models (MLLMs)~\citep{gpt4o,gemini,claude,qwen2vl,llavaov,kimi,r1v} have greatly progressed the state of multimodal reasoning and planning, and are rapidly being integrated into various downstream applications, ranging from robotics~\citep{llara,spatialvlm}, biomedicine~\citep{llavamed}, autonomous driving~\citep{drivegpt4,zhang2023study} to visual mathematical reasoning~\citep{gllava,mavis,euclid} and even food recipe generation~\citep{fire}. Given the rapidly growing application of MLLMs, especially in critical domains such as biomedicine and security, it is crucial to study the limitations of their visual perception to elucidate the potential risks that may affect their downstream applications.

To motivate the limitation that will be the focus of this work, we start by presenting three revealing visual question answering examples in~\cref{fig:crop_fig}, in which we ask a popular MLLM BLIP-2 (FlanT5$_\mathrm{XL}$)~\citep{li2023blip} to identify an object's presence or type in each image as we vary the size of the object.
In the absence of any prior evidence, we might reasonably expect the MLLM's answer to be invariant to the size of the object, because of the MLLM's large representational capacity and pretraining on a wide variety of images containing objects of various sizes.
To the contrary, in~\cref{fig:crop_fig} (left), we observe that initially the model does not recognize the existence of a small street sign and assigns a lower probability to the correct answer; however, zooming into the image (via more focused visual cropping) towards the street sign gradually increases the probability assigned to the correct answer, suggesting that the model gradually perceives more and more relevant details of the street sign. In~\cref{fig:crop_fig} (middle), we observe further evidence of this difficulty in perceiving small details: the model initially predicts \emph{white} as the type of the bird, but when we zoom into the image towards the bird, without changing the question in any way, we observe that the model gradually assigns higher probability to the correct bird type of \emph{egret}. This suggests that the model was not making a semantic error of misunderstanding what \emph{type} means, rather it was unable to perceive sufficient details to discriminate \emph{egret} from other \emph{white} birds, which is mitigated by visual cropping. Similarly, in~\cref{fig:crop_fig} (right), we observe that the modelâ€™s initial answer is not entirely irrelevant (``ama'' compared to the correct answer ``moma''), suggesting that the model knows where to look based on the question but cannot accurately perceive the actual word, which is again mitigated by visual cropping.

In this work, we will study the limitation observed in~\cref{fig:crop_fig} extensively, elucidate its cause, and propose potential solutions to mitigate its consequences. In~\cref{sec:human_crop}, we quantitatively show that there indeed exists a difficulty in perceiving small visual concepts across various widely-used MLLMs. Our findings are consistent with prior works on evaluating the text-image matching in vision-language joint embedding models, which have observed a reverse correlation between visual object size in images and the text-image matching score~\citep{vlchecklist}, but we further establish a causal connection between visual concept size and MLLMs' perception ability through an intervention study.
In~\cref{sec:where_to_look}, we study whether the MLLMs' difficulty in perceiving small visual concepts stems from a difficulty in perceiving visual details, or from a difficulty in locating the concept due to its small size. We quantitatively show that MLLMs consistently know where to look, even when they fail to answer the question correctly.
In~\cref{sec:vicrop}, we propose three automatic visual cropping methods---leveraging the attention maps and gradients of the MLLM itself---as scalable and training-free solutions to the visual perception limitation. Finally, in~\cref{sec:experiments}, we apply our proposed methods to two popular MLLMs and evaluate them on seven visual question answering (VQA) benchmarks, showing their efficacy in enhancing MLLMs' accuracy, especially on detail-sensitive benchmarks.

\begin{figure*}[t]
    \centering
    \includegraphics[trim=0 0 0 0, clip, width=\textwidth]{figures/vicrop_motivation.pdf}
    \caption{The effect of visual cropping on the probability of answers predicted by BLIP-2 FlanT5$_\mathrm{XL}$ zero-shot VQA model. The x-axis labels are indices to the respective cropped images displayed under each plot that the model sees at each step. The model gradually finds the correct answer.}
    \label{fig:crop_fig}
\end{figure*}