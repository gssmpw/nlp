\section{Automatic Visual Cropping (ViCrop)}
\label{sec:vicrop}

\begin{figure*}[b]
    \centering
    \includegraphics[trim=0 0 0 0, clip, width=0.8\textwidth]{figures/vicrop_methods.pdf}
    \caption{Illustration of the proposed visual cropping approach applied to two MLLMs.}
    \label{fig:methods}
\end{figure*}

We observed in~\cref{sec:where_to_look} that the sensitivity of MLLMs to visual concept size is primarily a perception limitation (rather than a localization limitation). Therefore, one solution to mitigate this limitation is to simply train MLLMs with a larger number of image patches while maintaining per-patch resolution (hence increasing the image resolution of MLLMs). However, increasing the input image resolution by a factor of $\alpha$, increases the number of ViT input patches (and output tokens) from $N^2$ to $\alpha^2 N^2$, which in turn increases the softmax attention computation complexity on the order of $\alpha^4 N^4$. Given that this solution is not scalable for current Transformer-based MLLMs, we choose to explore an alternative solution that \textbf{does not require any training and is scalable to any image resolution}. We note that several concurrent works have explored the first direction of \textit{training} MLLMs with higher resolution image patches~\citep{li2024mini-gemini,sun2024parrot,li2024monkey,mckinzie2024mm1,xu2024llava-uhd,luo2024feast}, and notably LLaVA-Next~\citep{liu2024llavanext} has achieved the VQA state-of-the-art in several VQA benchmarks at the time of writing. We believe our work is parallel to these efforts in the following sense: rather than training higher and higher resolution MLLMs to enable them to see all resolutions (which is inevitably upper bounded), we explore how to smartly adjust the input image towards what an MLLM already can see without any additional training. We provide evidence showing that our training-free method can provide orthogonal benefits to the training-based methods in~\cref{app:llavanext,app:seal}.

Inspired by our findings that MLLMs tend to know where to look (\cref{sec:where_to_look}) and that visual cropping can mitigate the perception limitation (\cref{sec:human_crop}), in this section we construct three automatic visual cropping methods in order to mitigate the perception limitation of MLLMs. These methods seek to use the internal information of an MLLM itself---in the form of attention maps and gradients---to find the approximate region of interest in images (\ie, the region containing the subject of a question), and then to zoom into that region via visual cropping.
One potential drawback of visual cropping is that some questions might need to have a global view of the image. To address this issue, we utilize the fact that MLLMs typically convert the image into a series of tokens. This allows us to directly extend the original image tokens by concatenating the visually cropped image tokens, as illustrated in~\cref{fig:methods}. We use this concatenation approach when applying all our methods to MLLMs.

\textbf{Relative Attention ViCrop (\rel{}).} In this method, we directly compute the relative attention $A_{rel}(x,q)$ defined in~\cref{sec:where_to_look} for each image-question pair $(x,q)$. We then select a target layer (in LLM and connector) based on a small held-out set of samples in TextVQA and use its relative attention as the importance map for visual cropping (discussed below). We ablate on the choice of layer in~\cref{sec:experiments}.

\textbf{Gradient-Weighted Attention ViCrop (\gra{}).} The relative attention runs an additional generic instruction through the MLLM to normalize the answer-to-image attention and emphasize semantically relevant attention. As an alternative that does not require a second forward pass, we consider using the gradients to normalize attention, because the gradient of the model's decision with respect to an attention score shows how sensitive the decision is to changes in that attention, hence how semantically relevant the attention is for answering the question.
To get a differentiable representation of the model's decision, we consider the logarithm of the maximum output probability at the starting answer token, $v = \log \text{softmax}(z(x,q))_{t^*} \in \mathbb{R}$, where $z \in \mathbb{R}^{D}$ is the output logit of the LLM at the starting answer position, $D$ the vocabulary size, and $t^* = \argmax_t z_t$. Then, following our notation in~\cref{sec:where_to_look}, we can compute the gradient-weighted versions of answer-to-token attention $\tilde{A}_{st}(x,q) = A_{st}(x,q) \odot \sigma(\nabla_{A_{st}}v(x,q))$ and token-to-image attention $\tilde{A}_{ti}(x,q) = A_{ti}(x) \odot \sigma(\nabla_{A_{ti}}v(x,q))$, where $\odot$ is element-wise product and $\sigma(w)=\max(0, w)$. We remove negative gradients because they correspond to tokens that if attended to will reduce the model certainty, hence often distracting locations~\cite{gradcam}. Finally, we compute the gradient-weighted answer-to-image attention as the following tensor product $\tilde{A}_{si}(x,q) = \tilde{A}_{st}(x,q) \otimes \tilde{A}_{ti}(x,q) \in \mathbb{R}^{L \times L_c \times 1 \times N^2}$. We will select the same target layer identified in~\rel{} from $\tilde{A}_{si}(x,q)$ as the importance map for visual cropping.

\textbf{Input Gradient ViCrop (\pgra{}).} In this method, we seek to find the relevant regions on the image directly using the gradient of the MLLM's decision with respect to the input image. Compared to the previous attention-based methods, \pgra{} is more versatile because it does not rely on the Transformer-based architecture. Specifically, for each image-question pair $(x,q)$, we will compute $G(x,q) = \lVert \nabla_x v(x,q) \lVert_2$, where $v(x,q)$ is the logarithm of the maximum output probability of the LLM at the starting answer token (as defined in \gra{} above), and the L2-norm is taken over the image channel dimension. However, gradients sometimes show high values in entirely constant-color regions (\eg, blue skies). Given that these non-edge regions do not contain any visual details, we will explicitly diminish them in $G$. To that end, we first apply a $3\times 3$-size Gaussian high-pass filter to the image, followed by a median filter of the same size to reduce salt-and-pepper noise. The resulting filtered image is then thresholded at its spatial median value to become a binary mask and is element-wise multiplied by $G$. Finally, the edge-emphasized $G$ is spatially average-pooled into the $N\times N$ patches of the MLLM to become an importance map for visual cropping.

\textbf{Bounding Box Selection for Visual Cropping.}
To convert the importance map (from each of the methods described above) to a bounding box, we use sliding windows of different sizes inspired by object detection literature~\cite{yolo}. Specifically, for each MLLM, we define a set of windows with sizes equal to a multiple of the input image resolution of the MLLM. The multiples are in $\{1, 1.2, \dots 2\}$. We slide each window over the image with a stride of 1 and find its best position where the sum of the importance map inside the window is maximized. After selecting the best position per window, we choose the window whose internal sum has the largest difference from the average internal sum of its adjacent positions. This latter step is a heuristic to avoid choosing too small or too large windows (notice that in both cases, moving the window slightly left/right or up/down will not change its internal sum significantly). The chosen window is then cropped from the image, resized to the input image resolution of the MLLM, and provided to the MLLM in addition to the image-question pair.

\textbf{High-Resolution Visual Cropping.} In one of the benchmarks we consider in this work, V$^*$~\cite{v-star}, the images are of very high resolution (always more than 1K) and consequently, the resized input image provided to the MLLM might completely lose the visual concept of interest for a question. To mitigate this, on this benchmark, we employ a two-stage strategy. In the first stage, we divide images into non-overlapping blocks of smaller than $1024\times 1024$ with an aspect ratio close to 1, compute the importance map separately for each block according to the ViCrop methods, and then re-attach the blocks back together. In the second stage, we find the bounding box for visual cropping on this re-attached importance map exactly as described before and provide the original image-question pair with the resized cropped image to the MLLM.
