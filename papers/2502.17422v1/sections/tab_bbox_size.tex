\begin{table*}[b!]
\caption{Sensitivity of the accuracy of MLLMs to the size of visual concepts in TextVQA. As the relative visual size of the answer decreases (right to left in each row), we observe a decline in the accuracy of the original models (no cropping) in answering questions, whereas visual cropping (\hc) significantly improves accuracy on smaller objects.}
\label{tab:bbox_size}
\centering
\begin{tabular}{llccc}
\toprule
\multirow{2}{*}{Model} & \multirow{2}{*}{Method} & \multicolumn{3}{c}{Answer Bbox Size ($S$)}\\
\cmidrule(lr){3-5}
& & \texttt{small} & \texttt{medium} & \texttt{large} \\
\midrule

\multirow{2}{*}{\makecell[c]{BLIP-2 (FlanT5$_\mathrm{XL}$)}} & no cropping 
& 12.13 & 19.57 & 36.32 \\ & \hc 
& 55.76 & 52.02 & 45.73 \\ \midrule

\multirow{2}{*}{\makecell[c]{InstructBLIP (Vicuna-7B)}} & no cropping 
& 21.79 & 30.58 & 45.30 \\ & \hc 
& 69.60 & 61.56 & 53.39 \\ \midrule

\multirow{2}{*}{\makecell[c]{LLaVA-1.5 (Vicuna-7B)}} & no cropping 
& 39.38 & 47.74 & 50.65 \\ & \hc 
& 69.95 & 65.36 & 56.96 \\ \midrule

\multirow{2}{*}{\makecell[c]{Qwen-VL (Qwen-7B)}} & no cropping 
& 56.42 & 65.09 & 68.60 \\ & \hc 
& 70.35 & 75.49 & 71.05 \\ \midrule

\multirow{2}{*}{GPT-4o} & no cropping 
& 65.76 & 72.81 & 69.17 \\ & \hc 
& 75.63 & 81.32 & 71.72 \\

\bottomrule
\end{tabular}
\end{table*}