\section{Implementation Details}
\label{app:imp}
We use \textit{python 3.10.6, transformers 4.29.1 and torch 2.1.2} for all the experiments. Our environment consists of an Intel(R) Gold 5317 CPU @ 3.00GHz with 48 cores and 756 GB of RAM, and we utilize NVIDIA RTX A6000 GPUs for our experiments. We use the huggingface implementations of all studied MLLMs with the recommended hyper-parameters according to the respective papers. For GPT-4o, we use the official public API, which is available at the time of submission.

Regarding the evaluation setting of the TextVQA dataset in~\cref{tab:main_result}, our setting is slightly different from the one used by the LLaVA-1.5 original paper~\cite{llava1.5}. They report accuracy on TextVQA by using externally extracted OCR tokens to enrich its text prompt. This is a text-specific trick that essentially out-sources the perception of text to an external OCR model. This text-specific trick is not mentioned in their paper or supplementary material, but see their clarification in response to a GitHub issue here: \url{https://github.com/haotian-liu/LLaVA/issues/515#issuecomment-1763779341}. In contrast, we treat TextVQA the same as any other vision dataset in our experiments, that is, we do not provide any OCR extracted tokens to MLLMs when applying them to TextVQA (only image and question, in the evaluation prompt format specified in their respective papers). This results in a slightly lower accuracy compared to the one reported in the original paper, but instead, this number shows the true perception ability of LLaVA-1.5 on TextVQA, not confounded by the ability of an external OCR model. For completeness, we also measured TextVQA accuracy in the presence of OCR tokens, which results in $59.8$ for LLaVA-1.5 without any visual cropping, and $63.95$ with \rel{}, showing that our proposed visual cropping can still be beneficial even when OCR tokens are provided to the MLLM.


\section{Dataset Statistics}
\label{app:dataset}
In this section, we present the details of the datasets used for evaluation in this paper. We report the average height and weight of the images in the dataset. We also report the number of images and questions in each dataset. For VQAv2, we run our experiment on a random 50K subset of the official validation set. We use the entire validation set in all other datasets.


\begin{table}[h]
\caption{Average width ($\bar{W}$) and height ($\bar{H}$) of images, number of images, and number of questions on all datasets.}

\centering
\begin{tabular}{lccccccc}
\toprule
 & V$^{*}$ & DocVQA & TextVQA & POPE & AOKVQA & GQA & VQAv2 \\
\midrule
$\bar{W}$  & 2246 & 1776 & 954 & 584 & 581 & 578 & 577\\ 
$\bar{H}$ & 1582 & 2084 & 818 & 478 & 480 & 482 & 485\\ 
\# Images & 191 & 1286 & 3166 & 500 & 1122 & 398 & 14206 \\
\# Questions & 191 & 5349 & 5000 & 8910 & 1145 & 10781 & 50000 \\

\bottomrule
\end{tabular}
\label{tab:example}
\end{table}

For our analysis presented in Table \ref{tab:bbox_size} and Figure \ref{fig:where_to_look}, we focused on TextVQA dataset, which includes bounding box annotations for OCR-detected text within images. However, this dataset does not specify which bounding boxes correspond to the regions where answers are located, necessitating a manual annotation process.
The TextVQA dataset comprises 5000 questions and 3166 images. We manually annotated these question-image pairs, ensuring accurate bounding boxes over \textbf{all the regions} of interest where the answers could be found.
This manual annotation process was essential for our analysis, allowing us to provide precise and reliable ground-truth data for the study.
Given that some questions were associated with multiple bounding boxes in their corresponding images, we undertook a filtering process to isolate the question-image pairs. This effort resulted in a refined set of 4370 question-image pairs, where there is only one instance of the subject of the question in the image. For example, if the question is ``what type of drink is sold here?'' and there are two different cans of drinks in the image, we remove this image-question pair.


\section{Prompt Format for Zero-shot Inference}
\label{app:zeroshot_instruction}

In this section, we provide details about the prompt format used in models for zero-shot inference. We use a different prompt format for LLaVA and InstructBLIP which we adapt from the original papers, as shown below.


\begin{tcolorbox}
\textbf{LLaVA-1.5}\\

\texttt{<image>
USER:\{question\} Answer the question using a single word or phrase.
ASSISTANT:}
\end{tcolorbox}


\begin{tcolorbox}
\textbf{InstructBLIP} \\ 

\texttt{<image> Question:\{question\} Short Answer:}
\end{tcolorbox}

\section{Orthogonal Benefits to LLaVA-NeXT}
\label{app:llavanext}
We apply our proposed \rel{} visual cropping method to an additional newer MLLM -- LLaVA-NeXT~\citep{liu2024llavanext} current SOTA in several VQA benchmarks -- that has support for higher-resolution compared to LLaVA-1.5. In~\cref{tab:llavanext}, we observe that our method can still boost the MLLM's performance, without requiring any training. This provides further evidence for the generalizability of our proposed visual cropping and its orthogonal benefits to training MLLMs with higher image patch resolution.

\begin{table}[h]
\caption{Orthogonal benefits of visual cropping when applied to LLaV-NeXT that is trained to adapt to processing high-resolution images.}

\centering
\begin{tabular}{lcc}
\toprule
Model & TextVQA & V$^{*}$ \\
\midrule
LLaVA-NeXT (Mistral-7B) & 65.17 & 58.11\\
LLaVA-NeXT (Mistral-7B) + \rel{} & 68.65 & 61.78\\

\bottomrule
\end{tabular}
\label{tab:llavanext}
\end{table}




\section{Comparison with the V* method (SEAL)}
\label{app:seal}
The V* method (SEAL)~\citep{v-star} proposes a multi-agent fine-tuning approach to enhance the ability of an underlying MLLM to answer questions about small visual concepts.
However, SEAL requires substantial training and finetuning of several neural networks, whereas our methods are completely training-free, so a direct comparison would not be fair. Nonetheless, to provide an idea of how our method compares to SEAL in an “as-is” fashion (i.e. if a user just wants to pick one method as-is off-the-shelf), we report the accuracy of SEAL compared to LLaVA-1.5+\rel{} in~\cref{tab:seal_comparison}. We observe that our method outperforms SEAL except on the V* benchmark. We think this might be because SEAL is designed and tuned specifically toward high-resolution images in its V* benchmark. We also note that the inference time of SEAL is slower than our method (4.44s compared to 1.88s on average per question, tested on the same random 100 TextVQA samples with one A6000 GPU).
That being said, we note that our methods and SEAL can both help enhance MLLMs, and our methods can be integrated into SEAL or other multi-agent pipelines.

\begin{table}[h] 
\small
\caption{Performance comparison between our~\rel{} applied on LLaVA-1.5 and SEAL~\citep{v-star} across multiple vision-language benchmarks.} 
\centering 
\begin{tabular}{lccccccc} 
\toprule Model & TextVQA & V$^*$ & POPE & DocVQA & AOKVQA & GQA & VQAV2 \\
\midrule SEAL  & 36.30 & 75.30 & 82.40 & 5.31 & 55.34 & 50.18 & 65.35 \\
LLaVA-1.5+\rel{}  & 55.17 & 62.30 & 87.25 & 19.63 & 60.66 & 60.97 & 76.29 \\
\bottomrule 
\end{tabular} 
\label{tab:seal_comparison} 
\end{table}

%%% EXTERNALS
\section{External Tools ViCrop}
\label{app:external}

In this section, we present three automatic question-guided localization methods based on popular off-the-shelf vision-based models, namely CLIP~\cite{clip}, YOLO~\cite{yolo}, and SAM~\cite{kirillov2023segment}. These three methods utilize external vision-based knowledge for the localization process through multimodal encoding, object detection, and semantic segmentation, respectively. See~\cref{tab:externals_time} for their results compared to internal ViCrop methods.

\textbf{CLIP ViCrop.} The intuition of this method is to progressively refine the image towards the region of highest relevance to a given question using CLIP~\cite{clip}. CLIP consists of an image encoder and a text encoder, which are trained on a large dataset of image-caption pairs to map each image (caption) close to its caption (image) and far from all other captions (images). The result is an aligned shared space where various images can be directly compared with various texts. To find the region of interest, given an image-question pair, we first crop the image from the four sides (top, bottom, left, and right) at a cropping ratio of 0.9 to produce four overlapping cropped images. We then use CLIP to assess the semantic similarity between these cropped images and the question. The highest-scoring crop is chosen as the input for the next iteration. This process is repeated for 20 iterations, and the cropped image with the highest CLIP similarity to the question is selected for visual cropping.

\textbf{YOLO ViCrop.} Instead of a progressive approach to finding the region of interest, in this method we select candidate regions based on a state-of-the-art object detection method: YOLOv8~\citep{Jocher_YOLO_by_Ultralytics_2023} pretrained on COCO~\cite{lin2014mscoco}. Using YOLO, we filter out regions that contain no salient objects -- \ie, regions for which CLIP could mistakenly assign high similarity. More concretely, for each question-image pair, we first use YOLO to collect bounding boxes for all predicted objects with confidence higher than 0.25 (the recommended default).\footnote{https://docs.ultralytics.com/modes/predict} Then, for each predicted bounding box, we crop its corresponding image and compute its similarity to the question using CLIP. Finally, the bounding box with the highest similarity score is selected as the region of interest for visual cropping.

\textbf{SAM ViCrop.} A limitation of YOLO is that it only provides bounding boxes corresponding to a fixed number of object classes. To overcome this issue, we use the segment anything model (SAM)~\cite{kirillov2023segment}, which has shown state-of-the-art zero-shot segmentation performance. SAM can provide an extensive set of segmentation masks for each image, thus providing a more granular set of salient candidate regions compared to YOLO. More concretely, for each image-question pair, we feed the image into SAM, which provides an extensive set of segmentation masks corresponding to all objects and object parts. Then, we translate these masks into bounding boxes by computing the smallest bounding box that covers each segmentation mask. Finally, the bounding box with the highest CLIP similarity to the question is selected as the region of interest for visual cropping.

Finally, for each method, we crop the smallest covering square (so that the cropped image is not deformed when resized to the input resolution of the MLLM), and provide it to the MLLM in addition to the original image-question pair (as depicted in~\cref{fig:methods}).

\clearpage
\section{Additional Examples on Model's Predictions}
\label{app:examples}


\begin{figure*}[h]
    \centering
    \includegraphics[trim=0 0 0 0, clip, width=0.99\textwidth]{figures/appendix_vstar.pdf}
    \caption{Success (first 3) and failure (last) examples of LLaVA-1.5 (\rel{}) on the V$^*$ benchmark (cyan-colored bounding box shows cropped region by \rel{}; zoom-in insets are displayed for better readability).}
    \label{fig:example_llava_vstar}
\end{figure*}


\begin{figure*}[t]
    \centering
    \includegraphics[trim=0 0 0 0, clip, width=0.99\textwidth]{figures/appendix_textvqa1.pdf}
    \caption{Success (first 9) and failure (last 6) examples of LLaVA-1.5 (\rel{}) on the TextVQA benchmark (cyan-colored bounding box shows cropped region by \rel{}).}
    \label{fig:example_llava_textvqa}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[trim=0 0 0 0, clip, width=0.99\textwidth]{figures/appendix_textvqa2.pdf}
    \caption{Success (first 9) and failure (last 6) examples of InstructBLIP (\rel{}) on the TextVQA benchmark (cyan-colored bounding box shows cropped region by \rel{}).}
    \label{fig:example_instructblip_textvqa}
\end{figure*}
