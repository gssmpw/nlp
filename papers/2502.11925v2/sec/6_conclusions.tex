\section{Conclusions}\label{sec:conclusions}

In this paper, we address the challenge of multimodal content generation on multimodal attributed graphs (MMAGs). 
To this end, we propose a graph-enhanced multimodal large language model, \Ours, designed with the following components:
(1) A personalized PageRank-based sampling strategy to extract informative neighbors from the graph, effectively mitigating the challenge of graph size explosion;
(2) A transformation mechanism that encodes graph information as sequences, employing either linearization or deep graph encoding with a hierarchical aligner, thereby addressing the non-Euclidean nature of graphs and hierarchical modality dependencies;
(3) Dual inference modes supporting both sequential and parallel inference to alleviate inference dependency issues.
We conduct comprehensive experiments on MMAGs within art and e-commerce domains, demonstrating the effectiveness of our approach against strong baseline methods. 
Future work includes extending MLLMs for discriminative tasks on MMAGs and capturing the complex heterogeneous relations between texts and images within these graphs.
