\clearpage
\setcounter{page}{1}
\maketitlesupplementary

\section{Limitations}
In our current approach, we treat the graph as homogeneous, simplifying all nodes and edges into a single type. However, real-world graphs often consist of multiple node and edge types, each with unique semantic meanings. Future research could address this limitation by extending GraphGPT-o to heterogeneous graphs, allowing for richer and more nuanced representations of complex structures.

\section{Ethical Considerations}
GraphGPT-o presents a new method for improving the structural understanding of MLLMs through graph-based alignment. This approach seeks to tackle current issues in MLLMs, such as the uncontrolled generation of unsuitable content and susceptibility to adversarial attacks. Although GraphGPT-o provides enhancements, it still depends on the MLLM foundation, making it subject to these inherent limitations. Ethical concerns, like the potential for misuse, unintended generation of inappropriate content, and exposure to adversarial manipulation, need careful attention when deploying GraphGPT-o in practical applications.




\section{Experiment settings.}
For training, we randomly sampled 40,000 nodes from each original dataset. For testing, we randomly selected 50 nodes and its related neighbors from the rest of the dataset.

In the implementation of GraphGPT-o, we utilize DreamLLM as the pre-trained backbone. Within the Graph Hierarchical Tokenization module, the learnable tokens, as well as all self-attention and cross-attention layers, are randomly initialized. We employ a pre-trained CLIP encoder as the fixed image and text encoder, with an additional MLP to resolve dimensional discrepancies.


\begin{table}[h!]
\centering
\caption{Hyper-parameter configuration for model training.}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Parameter}             & \textbf{ART500K} & \textbf{Beauty} & \textbf{Baby} \\ \hline
learning rate                      & 1e-5            & 1e-5           & 1e-5              \\ \hline
Batch size per GPU             & 1               & 1              & 1                 \\ \hline
warmup ratio          & 3e-3                & 3e-3              & 3e-3                  \\ \hline
Epochs                         & 1               & 1              & 1                 \\ \hline
loss weight of lm                     & 1              & 1             & 1                \\ \hline
loss weight of vm                  & 5             & 5            & 5               \\ \hline
\end{tabular}
\label{tab:hyperparams}
\end{table}

\section{More Experiment Results.}
We demonstrate more cases generated by DreamLLM and \Ours with comparision with the ground truth.

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figs/examples.pdf}
    \caption{More cases for qualitative evaluation. Our method exhibits better consistency with the ground truth by better utilizing the graph information
from neighboring nodes}
    \label{fig:enter-label}
\end{figure*}


