\section{Methodology}
\label{sec:methodology}
In this section, we present our \Ours framework, a novel approach for generating image-text pairs on MMAGs using multimodal LLMs (MLLMs). 
We begin by introducing graph information into MLLMs in Section \ref{sec:graph-mllm}.
Next, we describe a personalized PageRank-based graph sampling strategy in Section~\ref{sec:sampling}, addressing the \textit{Graph Size Explosion} challenge.
In Section~\ref{sec:graph-tokenization}, we propose graph linearization strategies and develop a hierarchical graph aligner to address the \textit{Non-Euclidean Nature} of graphs and capture \textit{Hierarchical Modality Dependency} in MMAGs.
Finally, in Section~\ref{sec:inference-strategy}, we explore different generation strategies to manage \textit{Inference Dependency} across modalities.

% Next, we explore various generation strategies for the inference stage in Section~\ref{sec:inference-strategy}. 
% Finally, in Section~\ref{sec:graph-alignment}, we present the \Ours framework, which effectively aligns graph information with image-text pair information.

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figs/pipeline.pdf}
    \caption{The overall framework of the proposed \Ours is as follows. Given a target node in a multimodal attribute graph (MMAG), we begin by using personalized PageRank for neighbor sampling. These sampled neighboring nodes are then fed into a Hierarchical Multimodal Aligner, which aligns text, image, and graph structure data. Each modality of a node is initially encoded and fused through multiple self-attention and cross-attention layers to produce multimodal node tokens. Subsequently, the tokens are processed by a graph structure Q-former, ultimately serving as inputs to the Multimodal LLM.}
    \label{fig:pipeline}
\end{figure*}


\subsection{Multimodal LLM on MMAGs}\label{sec:graph-mllm}
\textbf{DreamLLM.} The proposed \Ours is built upon DreamLLM~\cite{dong2024dreamllmsynergisticmultimodalcomprehension}, an MLLM capable of comprehension and generation on both text and image modalities.
To be specific, DreamLLM represents both texts and images as tokens and conducts encoding and generation in an autoregressive fashion:
\begin{equation}
\mathcal{L}^{\text{DreamLLM}}_{\text{MLLM}} = -\mathbb{E}_{t} \left[ \log p\left(x^{\text{WI}}_t \mid \bmx^{\text{WI}}_{<t} \right) \right],
\end{equation}
where $\bmx^{\text{WI}}=\{x^{\text{WI}}_t\}^T_{t=1}$ is an interleaved sequence containing both word tokens $\bm{w}=\{ w_i\}^N_{i=1}$ and image tokens $\bm{I}=\{I_k\}^K_{k=1}$.
The generated image tokens are decoded into images with a Stable Diffusion \cite{rombach2022high} trained by the following objective:
\begin{equation}
\mathcal{L}^{\text{DreamLLM}}_{\text{SD}} = \mathbb{E}_{t, \epsilon}\left[ \left| \epsilon - \epsilon_{\theta}\left(z_t; C^{\text{WI}}_{\text{DreamLLM}}, t \right) \right|^2 \right],
\end{equation}
where $C^{\text{WI}}_{\text{DreamLLM}}$ represents the condition incorporating information from both text and image modalities.

\vspace{0.1in}
\noindent \textbf{\Ours: introducing graph signals into MLLMs.}
In the context of MMAGs, generating image and text content for a node $v_i$ requires utilizing semantic information from the surrounding nodes within the \textit{graph} structure.
Therefore, we introduce an auxiliary set of graph tokens $\bm{g}_{v_i}=\{ g_j\}^M_{j=1}$ as input to the MLLM in addition to the text and image tokens:
\begin{equation}
\mathcal{L}^{\text{\Ours}}_{\text{MLLM}} = -\mathbb{E}_{t} \left[ \log p\left(x^{\text{WIG}}_t \mid \bmx^{\text{WIG}}_{<t} \right) \right],
\end{equation}
where $\bmx^{\text{WIG}}=\{x^{\text{WIG}}_t\}^T_{t=1}$ is an interleaved sequence containing both word tokens $\bm{w}$, image tokens $\bm{I}$ and graph tokens $\bm{g}$.
The image decoder Stable Diffusion \cite{rombach2022high} is then trained by the following objective:
\begin{equation}
\mathcal{L}^{\text{\Ours}}_{\text{SD}} = \mathbb{E}_{t, \epsilon}\left[ \left| \epsilon - \epsilon_{\theta}\left(z_t; C^{\text{WIG}}_{\text{\Ours}}, t \right) \right|^2 \right],
\end{equation}
where $C^{\text{WIG}}_{\text{\Ours}}$ represents the condition incorporating information from text, image, and graph modalities.

% The proposed \Ours is built upon an MLLM capable of processing interleaved text and image as inputs and outputs. 
% % Each modality, including both text and images, is encoded into tokens, and the model generates new tokens in an auto-regressive manner, allowing it to effectively learn the relationships between different modalities. 
% We utilize DreamLLM~\cite{dong2024dreamllmsynergisticmultimodalcomprehension} as the default backbone, where images are encoded using a CLIP image encoder. 
% These embeddings are then aggregated into a series of specialized "dream tokens" that are subsequently fed into a Stable Diffusion model for image decoding. 
% The synthesized images are then fed back into the MLLM for enhanced comprehension, facilitating a richer interplay between text and image representations.

% The training objective combines next-token prediction with image generation via Stable Diffusion, aiming to enhance the model's understanding of both text and image features. This objective is defined as follows:
% \begin{equation}
% \mathcal{L}_{\text{NTP}}(\mathbf{Prompt}(\mathbf{T}, \mathbf{I})) = -\mathbb{E}_{s} \left[ \log p\left(T_s \mid \mathbf{T}_{< s}, \mathbf{I}_{< s}\right) \right]
% \end{equation}
% where \(\mathcal{L}_{\text{NTP}}\) represents the next-token prediction loss, \(\mathbf{T}\) denotes the text tokens, and \(\mathbf{I}\) denotes the image tokens. The variable \(s\) represents the step index, indicating that each token is predicted conditionally based on all preceding tokens from both modalities. The model learns to predict the next text token given the preceding text and image tokens, thereby facilitating multimodal comprehension.

% \begin{equation}
% \mathcal{L}_{\text{SD}}(\mathbf{I}, \mathbf{DTs}) = \mathbb{E}\left[ \left| \epsilon - \epsilon_{\theta}\left(\text{Enc}(\mathbf{I}), t, \mathbf{DTs}\right) \right|^2 \right]
% \end{equation}
% where \(\mathcal{L}_{\text{SD}}\) is the Stable Diffusion loss, \(\mathbf{I}\) represents the input image, \(\mathbf{DTs}\) are the dream tokens generated from text and image representations, \(\epsilon\) is the noise to be estimated, and \(\epsilon_{\theta}\) is the noise predicted by the model parameterized by \(\theta\). The encoder function \(\text{Enc}(\cdot)\) extracts latent features from the input image, while \(t\) represents the timestep in the diffusion process. This loss helps in generating high-quality images by minimizing the difference between the predicted and actual noise in the diffusion process.


% \subsection{Graph Prompt Design}
% \label{sec:prompt-design}
\subsection{Personalized PageRank Neighbor Sampling.}\label{sec:sampling}
A simple approach to obtain $\bm{g}_{v_i}$ is to encode the entire local subgraph of $v_i$ within $\mathcal{G}$.
However, this becomes impractical as the subgraph size grows exponentially with each additional hop, resulting in excessively long context sequences.
Additionally, irrelevant or extraneous information in the local subgraph could misguide the model.
To overcome this, inspired by \cite{gasteiger2022predictpropagategraphneural}, we utilize personalized PageRank (PPR) to selectively gather information for constructing $\bm{g}_{v_i}$ from a graph structure perspective.

To be specific, PPR \cite{haveliwala2002topic} utilizes the graph structure to produce a ranking score, $P_{i,j}$, for each node $v_j$ relative to a target node $v_i$. A higher score $P_{i,j}$ indicates a stronger “similarity” or relevance between nodes $v_i$ and $v_j$. We represent the PPR scores across all nodes with the PPR matrix $\bmP\in \mathbf{R}^{n\times n}$, where each row $P_{i,:}$ corresponds to the PPR vector for node $v_i$. The PPR matrix $\bmP$ is computed by solving the following equation:
\begin{gather}\label{eq:ppr}
    \bmP = \beta \hat{\bmA}\bmP + (1-\beta)\bmI.
\end{gather}
where $\beta$ is the reset probability governing the random walk in PPR and $\hat{\bmA}$ is the normalized adjacency matrix.
Using the PPR matrix, we define the PPR-based graph neighborhood nodes $N(v_i)$ to calculate $\bm{g}_{v_i}$ as the top $K$ most relevant neighbors, obtained by maximizing the sum of PPR scores for the selected neighbors:
\begin{gather}
   N(v_i) = \argmax_{N(v_i)\subset\mathcal{V}, |N(v_i)|=K} \sum_{v_j\in N(v_i)} P_{i, j}.
\end{gather}
This selection ensures that $N(v_i)$ captures the nodes most similar to $v_i$, based on their PPR scores.

% To construct the prompt \(\mathbf{Prompt}(\mathbf{T}, \mathbf{I})\), we first determine which nodes to include from the graph. 
% Instead of relying on traditional $k$-hop sub-graph sampling, which may include irrelevant or extraneous nodes, we use Personalized PageRank (PPR) to control the number of nodes and select the most structurally relevant ones. 
% Personalized PageRank allows us to assign higher importance to nodes based on their structural relationship to the target, ensuring that the sub-graph contains the most contextually significant nodes. 
% The importance score for each node \(v\) can be calculated using the following formula:
% \begin{equation}
% PPR(v) = (1 - \alpha) \sum_{u \in N(v)} \frac{PPR(u)}{d(u)} + \alpha p(v)
% \end{equation}
% where \(\alpha\) is the teleport probability (typically set to a value between 0 and 1), \(N(v)\) represents the set of neighbors of node \(v\), \(d(u)\) is the degree of node \(u\), and \(p(v)\) is the personalized preference vector that assigns higher probabilities to target nodes. 
% This approach ensures that nodes with a higher personalized PageRank score are selected, providing a more focused and contextually relevant sub-graph for prompt construction. 
% Unlike methods that utilize node attributes for sub-graph sampling, we face the challenge that the attribute of the target (i.e., center node) is unknown. 
% Therefore, the graph structure becomes our only criterion for node selection, allowing us to prioritize nodes that are highly interconnected and likely to contribute valuable contextual information for prompt generation.

\subsection{Multimodal Graph as Sequence}\label{sec:graph-tokenization}

After obtaining $N(v_i)$, the problem is how to extract meaningful graph representations from it. 
Given that \Ours takes sequential data as input, this involves tokenizing $N(v_i)$ into sequence $\bm{g}_{v_i}$. Previous studies ~\cite{zhu2024investigating, Liu2024graphprompter,instructglm} have explored methods for inputting text-attributed graphs as sequences into LLMs, but handling multimodal attributed graphs presents greater complexity.
In this section, we explore two ways to achieve this including (1) Linearization: simply linearizing the textual and image features in $N(v_i)$ into a sequence, and (2) Hierarchical Aligner: a hierarchical graph encoder to obtain deep representations as tokens for $N(v_i)$.

\subsubsection{Graph Linear Tokenization}

We first discuss tokenizing $N(v_i)$ with simple sequence linearization. This involves designing rules $\text{Linearize}(\cdot)$ to transform textual and image features in $N(v_i)$ into $\bm{g}_{v_i}$:
\begin{gather}
    \bm{g}_{v_i} = \text{Linearize}(N(v_i))
\end{gather}
Given that $N(v_i)$ is a set of nodes and each $v_j\in N(v_i)$ is associated with both text information $d_{v_j}$ and image information $p_{v_j}$, the design of the linearization rule should consider three factors: (1) modality choice; (2) modality order and (3) number of neighbors, which are discussed as follows:

% When constructing an instruction prompt for generation tasks using a sub-graph of nodes, each linked to an image-text pair, it is crucial to consider the \textbf{neighbor modality choices and order}, and \textbf{number of neighbors} in the prompt. These factors directly impact the prompt's clarity and effectiveness. We examine each below:

\vspace{0.05in}
\noindent
\textbf{Modality choice.} Depending on the graph, it is possible that presenting only texts $\{d_{v_j}| v_j\in N(v_i)\}$ or only images $\{d_{v_j}| v_j\in N(v_i)\}$ or both of them could benefit the multimodal content generation on MMAGs.
% For each node, deciding whether to include the image, text, or both is essential. While including both provides richer context, task-specific or computational constraints may favor using just one modality if it is more relevant to the task.

\vspace{0.05in}
\noindent
\textbf{Modality order.} Given that we have both text and image modality, it is flexible to adjust the order of different information, including (1) all images first, followed by texts, (2) all texts first, followed by images, and (3) interleaving image and text for each node $v_j \in N(v_i)$.
% The sequence of information is another key decision. Options include inputting all images first, followed by texts, or interleaving image and text for each node. The chosen order affects the model’s understanding and coherence of the prompt.

\vspace{0.05in}
\noindent
\textbf{Number of neighbors.} $N(v_i)$ is a list of nodes ranked by PPR score. Including more neighbors $v_j\in N(v_i)$ into $\bm{g}_{v_i}$ could potentially add more information but at the same time increase noise.
% Choosing the number of neighbors in the prompt affects context richness and length. While more neighbors offer broader context, too many can dilute focus. Selecting a few, highly relevant neighbors can optimize clarity and coherence.

In Section \ref{sec:res-linear}, we conduct systematic experiments on how different design choices affect the model performance.
After the design choice is given, $\{d_{v_j}| v_j\in N(v_i)\}$ are tokenized with text tokenizer and $\{d_{v_j}| v_j\in N(v_i)\}$ are tokenized with the pretrained CLIP encoder \cite{radford2021clip} similar to \cite{dong2024dreamllmsynergisticmultimodalcomprehension}. 

\subsubsection{Graph Hierarchical Tokenization}

Although linearization offers a solution for graph tokenization, it fails to capture hierarchical modality dependencies in MMAGs.
To be specific, at the node level, the combined information from associated text and image data contributes to a richer semantic representation of individual nodes. At the subgraph level, features synthesized from node-level semantics, alongside the local graph structure, enable a more comprehensive contextual understanding, thereby enhancing the generation of target nodes.
To this end, we design a hierarchical aligner $\mathcal{F}(\cdot)$ with a node feature Q-Former $\phi(\cdot)$ and a graph structure Q-Former $\psi(\cdot)$ to capture the node-level and subgraph-level modality dependency respectively:
\begin{gather}
    \bm{g}_{v_i} = \mathcal{F}(N(v_i)) = \psi(\{\phi(v_j) |v_j\in N(v_i)\})
\end{gather}

% \label{sec:graph-alignment}
% \subsubsection{Modality Fusion Q-Former}
\paragraph{Node Feature Q-Former.} It is proposed to learn node representations for $v_j\in N(v_i)$ considering the node-level modality dependency.
% To achieve alignment between text and image modalities, we introduce the Modality Fusion Q-Former, denoted as , which is designed to learn effective representations by integrating relevant text and image information for each neighboring node. 
As shown in Figure~\ref{fig:pipeline}, the Q-Former comprises two core Transformer \cite{vaswani2017attention} modules motivated by \cite{li2023blip}: (1) a self-attention module that facilitates deep information exchange between node text features and image features; (2) a cross-attention module that compresses node feature into a fixed number of representations.
% \begin{itemize}
% \item \textit{\textbf{Self-Attention}}: This module enables interactions among the hidden states within a single node, facilitating the capture of dependencies across text and image tokens.
% \item \textit{\textbf{Cross-Attention}}: This module applies attention to nodes in the neighborhood , weighted according to text guidance, thereby capturing dependencies between text data and neighboring image representations.
% \end{itemize}

% Let $\mathbf{H}^{(t)}_{v_j}\in \mathcal{R}^{d\times l}$ denote the hidden states calculated by the $t$-th Node Feature Q-Former layer.

The associated text $d_{v_j}$ and image $p_{v_j}$ of a node $v_j\in N(v_i)$ are first transformed into token representations $\bm{w}_{v_j}$ and $\bm{I}_{v_j}$ with text tokenizer and pretrained CLIP encoder respectively, which are then concatenated to form the initial input embedding:
\begin{equation}
\mathbf{H}^{(0)}_{v_j} = \left[ \bm{w}_{v_j} ;
\bm{I}_{v_j} \right] \in \mathbb{R}^{d \times (|d_{v_j}| + |p_{v_j}|)}
\end{equation}


% \noindent
% The text input is encoded into token embeddings, while the image input is also transformed into corresponding token embeddings. These text and image tokens are then concatenated to serve as input to the Q-Former. 

% \noindent
% For each node $\mathbf{{v_i}}$, the text data  is transformed into token embeddings:
% \begin{equation}
% \mathbf{X}_{d_{v_i}} = \left[ \bm{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_{|{d_{v_i}}|} \right] \in \mathbb{R}^{d \times |{d_{v_i}}|},
% \end{equation}
% where $\mathbf{x}_k$ represents the embedding for each token and $\mathbf{|d{v_i}}$ is the number of tokens in the text data.

% \noindent
% Similarly, each image  is encoded into token embeddings:
% \begin{equation}
% \mathbf{Z}_{p_{v_i}} = \left[ \mathbf{z}_1, \mathbf{z}_2, \ldots, \mathbf{z}_n \right] \in \mathbb{R}^{d \times n},
% \end{equation}
% where  $\mathbf{z}_k$ represents an image token embedding, and  is the number of tokens derived from the image data.

% \noindent
% The concatenated text and image tokens form the initial input embedding for node :
% \begin{equation}
% \mathbf{H}^{(0)}_{\mathcal{G}(v_i)} = \left[ \mathbf{X}_{d_{v_i}}
% \mathbf{Z}_{p_{v_i}} \right] \in \mathbb{R}^{d \times (|d_{v_i}| + n)}
% \end{equation}
% where  encapsulates both text and image information.


% \noindent
% Within each layer  of the Q-Former, a multi-head self-attention (MHA) mechanism is applied to the concatenated tokens to enable intra-node interactions, capturing dependencies across the text and image tokens:
The self-attention Transformer layers are designed to perform text and image modality information exchange calculated by:
\begin{equation}
\mathbf{H}^{(t)}_{v_j} = \text{Trans}_{\text{SAT}} \left( q, k, v=\mathbf{H}^{(t-1)}_{v_j} \right)
\end{equation}

% \noindent
Following $L_1$ self-attention Transformer layers, a cross-attention Transformer layer is applied, extracting the core feature into a fixed number of representations:
\begin{equation}
\mathbf{H}_{v_j} = \text{Trans}_{\text{CAT}} \left(q=\bm{Q}_V; k, v=\mathbf{H}^{(L_1)}_{v_j} \right)
\end{equation}
where $\bm{Q}_V\in \mathcal{R}^x$ is a node-level information aggregation soft prompt. The final representation $\mathbf{H}_{v_j}$ is leveraged as modality fused node feature representation.



% where the cross-attention process aggregates image embeddings from the neighborhood , utilizing text data to guide this integration.

% \noindent
% Finally, the multimodal representation  for each node is obtained as:
% \begin{equation}
% h_M(\mathcal{G}(v_i)) = \mathbf{H}^{(L)}_{\mathcal{G}(v_i)}
% \end{equation}
% where  represents the output of the Q-Former after  layers, capturing both intra-node text-image relationships and inter-node image dependencies to yield a comprehensive multimodal representation for each node.

% \subsubsection{Graph Structure Q-Former}
\paragraph{Graph Structure Q-Former.}
It is designed to aggregate the local context semantics inside $N(v_i)$, capturing the subgraph level modality dependency.
Similar to node feature Q-Former, graph structure Q-Former also contains two core Transformer modules:
(1) a self-attention module that enables deep information integration inside the local subgraph;
(2) a cross-attention module that aggregates the local semantics into a fixed number of representations.

% The Graph Structure Q-Former builds on the Modality Fusion Q-Former outputs, incorporating graph structure to aggregate multimodal representations from neighboring nodes. By leveraging graph topology, it enhances each node's representation by explicitly capturing structural relationships between nodes.

The node representations $\mathbf{H}_{v_j}$ for $v_j\in N(v_i)$ obtained from the node feature Q-Former are concatenated and serve as the initial inputs to the graph structure Q-Former:
\begin{equation} 
\mathbf{G}^{(0)}_{N(v_i)} = \left[{\mathbf{H}_{v_j} \ | \ v_j \in N(v_i)} \right] 
\end{equation} 

The self-attention Transformer layers are then applied to conduct deep information fusion between nodes inside the local subgraph:
\begin{equation}
\mathbf{G}^{(t)}_{N(v_i)} = \text{Trans}_{\text{SAT}} \left( q, k, v=\mathbf{G}^{(t-1)}_{N(v_i)} \right)
\end{equation}

% Following $L_2$ self-attention Transformer layers, a cross-attention Transformer layer is designed to 
After the $L_2$ self-attention Transformer layers, a cross-attention Transformer layer is designed to compress essential local graph features into a fixed set of representations:
\begin{equation}
\mathbf{G}_{N(v_i)} = \text{Trans}_{\text{CAT}} \left(q=\bm{Q}_G; k, v=\mathbf{G}^{(L_2)}_{N(v_i)} \right)
\end{equation}
where $\bm{Q}_G\in \mathcal{R}^x$ is a subgraph-level information aggregation soft prompt. 
The final representation is leveraged as graph token representations which are inputted into the MLLM: $\bm{g}_{v_i}=\mathbf{G}_{N(v_i)}$.



% \noindent
% The initial input to the graph structure Q-Former is the multimodal representation of the neighboring nodes $\mathcal{N}(v)$ from the Modality Fusion Q-Former, denoted by $\mathbf{H}^{(L)}_{\mathcal{G}(v_i)}$ for $v_i \in \mathcal{N}(v)$. The goal is to aggregate the information across these nodes, incorporating structural dependencies.

% The aggregated input representation for node ${v}$ is defined as: 
% \begin{equation} 
% \mathbf{G}^{(0)}_{\mathcal{G}(v_i)} = \left[{\mathbf{H}^{(L)}_{\mathcal{G}(v_i)} \ | \ v_i \in \mathcal{N}(v)} \right] 
% \end{equation} 
% where $\mathbf{H}^{(L)}_{\mathcal{G}(v_j)}$ are the neighboring node representations concatenated to form the initial aggregated input.

% \noindent Within each layer $t$ of the Graph Structure Q-Former, a multi-head self-attention (MHA) mechanism is applied to capture intra-node dependencies, enabling each node to contextualize its representation with respect to the information aggregated from its neighbors: 
% \begin{equation} 
% \mathbf{G'}^{(t)}_{\mathcal{G}(v_i)} = \text{MHA}_{\text{SAT}} \left( \mathbf{G}^{(t-1)}_{\mathcal{G}(v_i)} \right)
% \end{equation}

% Following the self-attention step, a cross-attention mechanism is used to allow nodes to explicitly attend to specific neighbors, guided by a series of learnable tokens to capture structural information. In particular, these learnable tokens are initialized to match the number of neighboring nodes $K$, which is a hyperparameter representing the fixed number of neighbors considered for each node. Each learnable token essentially acts as a query that attends to the multimodal representation of neighboring nodes to aggregate information effectively.

% The cross-attention mechanism aggregates information from neighboring nodes using learnable graph tokens (LGT): 
% \begin{equation} 
% \mathbf{G}^{(t)}_{\mathcal{G}(v_i)} = \text{MHA}_{\text{CAT}} \left( \mathbf{G'}^{(t)}_{\mathcal{G}(v_i)}, \mathbf{LGT}_k \right)
% \end{equation} 
% where $k$ is a hyperparemeter that represent the number of target nodes after aggregating. These learnable tokens incorporate structural properties, allowing the cross-attention module to selectively focus on specific neighbors based on their relevance. 

% The final representation for each node, capturing both multimodal information and graph structure, is obtained after $T$ layers of the Graph Structure Q-Former: 
% \begin{equation} 
% h_G({\mathcal{G}}_(v_i)) = \mathbf{G}^{(T)}_{\mathcal{G}(v_i)}
% \end{equation} 

% where $h_G({\mathcal{G}}_(v_i))$ represents the comprehensive representation of node $v_i$, effectively integrating both the intra-node multimodal relationships from the Modality Fusion Q-Former and the inter-node dependencies from the Graph Structure Q-Former.

% \noindent 
% \textbf{Summary.} This hierarchical approach, comprising both the Modality Fusion Q-Former and the Graph Structure Q-Former, enables the model to capture rich representations that encompass both the local multimodal context and the broader graph structure. This combination ultimately provides a powerful means for performing downstream generation, by leveraging both the content of nodes and their structural relationships.


\subsection{Inference Strategy}\label{sec:inference-strategy}
Given the inherent interdependence between textual and visual features ($d_{v_i}$ and $p_{v_i}$) within a node $v_i$ and the joint objectives of generating both image and text, the order of inference across these modalities plays a crucial role.
To this end, we propose two strategies: (1) sequential inference and (2) parallel inference.

% In the inference phase, GraphGPT-o employs various strategies to generate multimodal content from prompts containing both text and images. These strategies enable effective interplay between modalities, enriching the generated output.

\vspace{0.05in}
\noindent
\textbf{Sequential Inference.} 
The proposed framework employs a sequential dual-generation process, in which one modality is generated first and subsequently serves as a conditioning factor for the generation of the other modality.
Specifically, this approach enables us to generate text  $d_{v_i}$ by optimizing $p(d_{v_i}|g_{v_i})$ and then generate the corresponding image $p_{v_i}$ by maximizing $p(p_{v_i}|g_{v_i}, d_{v_i})$.
Alternatively, we can initiate generation with the image $p_{v_i}$ by maximizing $p(p_{v_i}|g_{v_i})$ and then produce the text $d_{v_i}$ by optimizing $p(d_{v_i}|g_{v_i}, p_{v_i})$.
This sequential conditioning strategy ensures that the second generation step is contextually anchored in the outcome of the first, potentially enhancing coherence and consistency across modalities.

% In the sequential strategy, one modality (either text or image) is generated first, forming the foundation for subsequent generation of the other modality. This ensures that the final output is grounded in a coherent representation of the initial content.

% If the image is generated first, the prompt guides the generation of an initial image, which is then encoded into a sequence of image tokens capturing its visual features:
% \begin{equation}
%     \mathbf{Z}_{v_i} = \mathbf{Enc}(\mathbf{Dec}(\mathbf{I}_{v_i})) = \left[ \mathbf{z}_1, \mathbf{z}_2, \ldots, \mathbf{z}_n \right] \in \mathbb{R}^{d \times n}
% \end{equation}
% These image tokens, along with the original prompt, serve as input to generate text tokens using an auto-regressive model:
% \begin{equation}
%     P(T) = \prod_{k=1}^{m} P(T_k \mid T_1, \ldots, T_{k-1}, \mathbf{Z}_{v_i}, \text{prompt}(v_i))
% \end{equation}
% This approach enriches text generation with visual context, ensuring a coherent alignment between generated images and text.

% Alternatively, text may be generated first from the prompt, establishing a detailed narrative:
% \begin{equation}
%     P(T) = \prod_{k=1}^{m} P(T_k \mid T_1, \ldots, T_{k-1}, \text{prompt}(v_i))
% \end{equation}
% The generated text is then used to guide the image synthesis, forming the conditioning input for image generation:
% \begin{equation}
%     C(\mathbf{I}_{v_i}) = \prod_{t=1}^{n} P(\mathbf{I}_t \mid T, \text{prompt}(v_i), \mathbf{I}_1, \mathbf{I}_2, \ldots, \mathbf{I}_{t-1})
% \end{equation}
% This ensures that visual content closely aligns with the generated text, maintaining consistency and contextual accuracy.

\noindent
\textbf{Parallel Inference.} 
The framework is designed to enable simultaneous dual generation of text and image by jointly optimizing $p(d_{v_i}|g_{v_i})$ and $p(p_{v_i}|g_{v_i})$.
This concurrent generation approach allows the production of $d_{v_i}$ and $p_{v_i}$ to proceed independently, mitigating the risk of error propagation from one modality serving as a conditional input for the other. 
Consequently, this parallel optimization strategy can reduce dependency on sequential conditioning, enhancing robustness in the generation process.

% In this way, the generation of $d_{v_i}$ and $p_{v_i}$ are independent, potentially preventing the first step generation error from being propagated as the condition for the second step generation.

% In parallel generation, text and image are generated simultaneously from the prompt without explicit dependencies between them:
% \begin{equation}
%     P(T, \mathbf{I}_{v_i}) = P(T \mid \text{prompt}(v_i)) \cdot P(\mathbf{I}_{v_i} \mid \text{prompt}(v_i))
% \end{equation}
% Here, both modalities are conditioned independently on the prompt, providing distinct but complementary representations. This approach is ideal when generating diverse perspectives on the prompt, with the image and text highlighting different aspects of the content.

% \noindent
% \textbf{Summary.} Each strategy offers unique strengths, whether sequential or parallel, allowing GraphGPT-o to flexibly adapt to the needs of specific tasks. The choice of strategy depends on whether prioritizing visual or textual coherence is crucial, or whether balanced, complementary multimodal content is desired.



\begin{comment}
\subsection{Inference Strategy}
\label{sec:inference-strategy}
In the inference phase, \Ours employs different strategies to generate multimodal content from a prompt containing both text and images. These strategies are designed to enhance the interplay between modalities, leading to richer multimodal outputs.

\noindent
\textbf{Image-First Generation Strategy.} This strategy initiates with an image generated from a given prompt containing textual and visual information. The generated image then serves as the foundation for the subsequent text generation, enriching the textual output by leveraging visual context to provide enhanced coherence and detail.

For a node \( v_i \), the conditional tokens for image generation are calculated first as follows:
\begin{equation}
    C(\mathbf{I}_{v_i}) = \prod_{t=1}^{n} P(\mathbf{I}_t \mid \text{prompt}(v_i), \mathbf{I}_1, \mathbf{I}_2, \ldots, \mathbf{I}_{t-1})
\end{equation}
where $C(\mathbf{I}_{v_i})$ represents the conditional tokens for image generation at node $v_i$ and $P(\cdot)$ is the auto-regressive process of generating the \( t \)-th image token conditioned on the prompt $\text{prompt}(v_i)$ and all previous image conditional tokens.



After obtaining the conditional tokens $C(\mathbf{I}_{v_i})$ for the image conditioned on the given $\text{prompt}(v_i)$, we utilize a decoder to synthesize the final image $\mathbf{I}_{v_i}$. This initial image is crafted based on visual and textual cues from the prompt, setting a concrete visual context that guides the subsequent stages.

The generated image $\mathbf{I}_{v_i}$ is then processed by an encoder, converting it into a sequence of image tokens. These tokens capture essential visual features and details embedded in the image, transforming them into a format suitable for text generation.
\begin{equation}
\mathbf{Z}_{v_i} = \mathbf{Enc}(\mathbf{Dec}(\mathbf{I}_{v_i})) = \left[ \mathbf{z}_1, \mathbf{z}_2, \ldots, \mathbf{z}_n \right] \in \mathbb{R}^{d \times n}
\end{equation}
The image tokens, alongside the original $\text{prompt}(v_i)$, are used as input to an auto-regressive model. By sequentially feeding the prompt and image tokens, the model generates text tokens one by one. This approach ensures that each generated token is influenced by both the visual content of the image and the information provided in the prompt, leading to a more coherent and contextually aligned text output.
\begin{equation}
    P(T) = \prod_{k=1}^{m} P(T_k \mid T_1, \ldots, T_{k-1}, \mathbf{Z}_{v_i}, \text{prompt}(v_i))
\end{equation}

This method enhances the text generation process by grounding it in a visual framework, fostering a closer alignment between generated images and the resulting text.

\noindent
\textbf{Text-First Generation Strategy.} Alternatively, the model can prioritize generating a textual representation from the prompt before using it to guide image generation. This approach ensures the text is complete and coherent, providing a rich basis for subsequent image synthesis.

For a node , the text tokens are first generated from the prompt using an auto-regressive model:
\begin{equation}
P(T) = \prod_{k=1}^{m} P(T_k \mid T_1, \ldots, T_{k-1}, \text{prompt}(v_i))
\end{equation}
where $P(T)$ represents the probability distribution over the generated text tokens $T_1, T_2, \ldots, T_m$, conditioned on the prompt $\text{prompt}(v_i)$. This ensures the text output is coherent, capturing the detailed aspects of the prompt.

Once the text $T$ is generated, it serves as additional supervision for image generation. The prompt, together with the generated text tokens, forms the conditioning input for the image generation process. The conditional tokens for image generation are computed as follows:
\begin{equation}
C(\mathbf{I}_{v_i}) = \prod_{t=1}^{n} P(\mathbf{I}_t \mid T, \text{prompt}(v_i), \mathbf{I}_1, \mathbf{I}_2, \ldots, \mathbf{I}_{t-1})
\end{equation}
This approach allows the generated image to align closely with the detailed descriptions provided in the text, leading to a highly consistent and contextually accurate multimodal output.

The text-first strategy is particularly effective in scenarios where generating a detailed and comprehensive textual description is crucial before synthesizing corresponding visual elements. By setting the textual context first, the subsequent image generation is well-guided, resulting in images that closely match the rich semantics of the text.

\noindent
\textbf{Parallel Generation Strategy.} In the parallel generation approach, both image and text are generated simultaneously from the prompt without direct dependencies between them.

Mathematically, the probability distributions for both the image and the text generation can be represented as:
\begin{equation}
P(T, \mathbf{I}_{v_i}) = P(T \mid \text{prompt}(v_i)) \cdot P(\mathbf{I}_{v_i} \mid \text{prompt}(v_i))
\end{equation}
Here, the text tokens $T$ and the image $\mathbf{I}_{v_i}$ are independently generated, both conditioned on the same prompt $\text{prompt}(v_i)$. This simultaneous generation ensures that each modality is independently influenced by the prompt, allowing for complementary but distinct outputs.

The parallel generation strategy is suitable when the image and text are intended to provide different perspectives on the prompt. For example, the image might capture one visual aspect while the text elaborates on another, resulting in a richer multimodal representation. This independence allows both modalities to focus on different features or elements of the input prompt, providing a more diverse and potentially insightful multimodal output.

\noindent
\textbf{Summary.} Overall, each strategy has distinct advantages depending on whether text, image, or balanced multimodal content is prioritized in response to the prompt. The choice of strategy can be tailored to the requirements of the specific task or application scenario, leveraging the strengths of each approach to generate coherent, contextually relevant, and engaging multimodal content.
\end{comment}



