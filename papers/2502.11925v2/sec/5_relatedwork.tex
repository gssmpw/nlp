\section{Related Work}\label{sec:relatedwork}

\subsection{Large Language Models on Graphs}
Large Language Models (LLMs) have driven substantial progress in graph learning applications \cite{jin2024large, ren2024survey}. Graph data, on the one hand, can be utilized directly to train LLMs \cite{chen2024graphwiz, zhang2023graph}. For instance, models like Heterformer \cite{jin2023heterformer} and Edgeformer \cite{ge2022edgeformer} introduce graph-enhanced Transformer architectures, positioning them as foundational models for graph-based LLMs. GraphGPT \cite{tang2024graphgpt} leverages graph structural data via graph instruction tuning, facilitating robust generalization across supervised and zero-shot graph learning tasks. LLaGA~\cite{chen2024llagalargelanguagegraph} employs a parameter-free GNN and incorporates the graph structure based on the order of node tokens. Similarly, InstructGraph \cite{wang2024instructgraph} employs a structured format verbalizer to encode graph data, enhancing LLMs in tasks requiring graph reasoning and generation. GraphAdapter \cite{huang2023can} incorporates GNNs as efficient adapters for LLMs, while GAugLLM \cite{fang2024gaugllm} advances self-supervised learning through augmented node features generated by an MoE module, effectively bridging textual and graph structures. UniGLM\cite{fang2024uniglmtrainingunifiedlanguage} uses structure information to build positive sample pairs in contrastive learning framework to train a unified text encoder. 
On the other hand, graph data can be utilized as external knowledge in a plug-and-play manner with LLMs \cite{jin2024graph, mavromatis2024gnn}. For example, Graph Chain-of-Thought \cite{jin2024graph} proposes an iterative framework that enables LLMs to reason, interact, and operate effectively on graphs. GNN-RAG \cite{mavromatis2024gnn} introduces a retrieval-augmented generation framework \cite{lewis2020retrieval}, using a GNN retriever to extract knowledge from graph data. Despite these advances, existing research has primarily focused on graphs with textual attributes, leaving multimodal attributed graphs underexplored.

% However, these approaches do not bridge the gap between MMAG and MLLM.


% In graph reasoning, LLMs address the challenge of interpreting complex relational structures by integrating with graph learning models or transforming graph data into language representations \cite{chen2024graphwiz,zhang2023graph}. 
% GraphLLM \cite{chai2023graphllm} enhances this by allowing LLMs to directly reason over graph data, moving beyond the Graph2Text approach. 
% Similarly, GraphText \cite{zhao2023graphtext} leverages LLMs' natural language capabilities to perform graph reasoning as text generation tasks by converting graph structures into syntax-based text sequences. 

% Graph learning with large language models (LLMs) has brought significant advancements in understanding complex relational data \cite{jin2024large,ren2024survey}. 
% Recent works integrating graph learning with LLM to build a general graph foundation model have emerged in multiple domains. 
% One for All (OFA)\cite{liu2023one} leverages text-attributed graphs and a graph prompting paradigm to generalize graph learning across graph tasks and domains, enabling in-context learning and robust performance without fine-tuning. 
% In node classification, LLMs enrich graph structure representations to better identify nodes in complex networks \cite{sun2023large,pan2024distilling}. 
% Graph LLM either improves graph learning tasks with LLM's text interpretation ability or improves LLM's reasoning ability with graph structural knowledge. 

\subsection{Multimodal Large Language Models}
Multimodal Large Language Models (MLLMs) have advanced the field by enabling unified multimodal understanding and generation within a single autoregressive framework \cite{yin2023surveymllm, zhang2024mm}. In terms of multimodal comprehension, models like Flamingo \cite{alayrac2022flamingo} process visual data interleaved with text, utilizing a gated cross-attention layer to encode inputs and produce free-form textual output. BLIP-2 \cite{li2023blip} introduces the Q-Former architecture, which maps images into a hidden space aligned with text tokens in LLMs, while LLaVA \cite{liu2024llava} simplifies this framework further with a projector and explores instruction tuning within the multimodal domain. Despite these advancements, current MLLMs primarily emphasize text generation and lack the capability to synthesize multimodal outputs (\textit{e.g.}, images).
To address this, DreamLLM \cite{dong2024dreamllmsynergisticmultimodalcomprehension} integrates an LLM backbone with a diffusion model to enable image generation as a multimodal output. Emu2 \cite{sun2024generative} scales this architecture to 37B parameters, demonstrating strong multimodal in-context learning and the ability to handle complex tasks requiring real-time reasoning, such as visual prompting and object-grounded generation. Chameleon \cite{team2024chameleon} proposes a stable training strategy from the ground up, featuring an alignment process and architectural parameterization tailored to early-fusion, token-based, mixed-modal settings. Nevertheless, most existing approaches overlook the relational dynamics between text and images, limiting their applicability to multimodal content generation tasks on multimodal attributed graphs (MMAGs).

% Multimodal Large Language Models (MLLMs) enable multimodal understanding and generation in a unified autoregressive architecture \cite{yin2023surveymllm,zhang2024mm}.
% From the multimodal understanding perspective, Flamingo \cite{alayrac2022flamingo} takes visual data interleaved with text as input, encodes them with a gated xattention layer, and produces free-form text as output.
% BLIP-2 \cite{li2023blip} introduces Q-Former architecture which projects images into a hidden space aligned with text tokens in LLMs.
% LLaVA \cite{liu2024llava} further simplifies the framework with a projector and explores instruction tuning in the multimodal space.
% However, these MLLMs mainly focus on text generation and lack the capability to synthesize multimodal outputs (\textit{e.g.}, images).
% To this end, DreamLLM \cite{dong2024dreamllmsynergisticmultimodalcomprehension} is proposed which connects an LLM backbone with a diffusion model as the image decoder.
% Emu2 \cite{sun2024generative} further scales up the MLLM to 37B and achieves strong multimodal in-context learning abilities, even emerging to solve tasks that require on-the-fly reasoning, such as visual prompting and object-grounded generation.
% Chameleon \cite{team2024chameleon} further outlines a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting.
% However, most existing works neglect the relational information between texts and images and cannot be directly applied to multimodal content generation on MMAGs.

% Tool-based MLLMs also demonstrate capabilities in handling multimodal tasks, as exemplified by Chameleon \cite{team2024chameleon}, which extends multimodal abilities by incorporating external tools for compositional reasoning. 
% DreamLLM \cite{dong2024dreamllmsynergisticmultimodalcomprehension} directly models language and image posteriors from raw multimodal data, allowing MLLMs to generate interleaved text and image content without relying on external extractors. 


% GPT-4 \cite{achiam2023gpt4} marked a milestone by enabling LLMs to accept both text and image inputs, while MiniGPT-4 \cite{zhu2023minigpt4} demonstrated the potential of aligning a frozen visual encoder with a language encoder using a single projection layer. 


% Multimodal Large Language Models (MLLMs) combine the strengths of LLMs and modality-specific foundation models to enhance cross-modal understanding and generation \cite{yin2023surveymllm,zhang2024mm}. 
% The earliest modality alignment method, CLIP \cite{radford2021clip}, aligns two modalities through contrastive learning. 
% Early efforts to integrate vision and language understanding began with  and .
% GPT-4 \cite{achiam2023gpt4} marked a milestone by enabling LLMs to accept both text and image inputs, while MiniGPT-4 \cite{zhu2023minigpt4} demonstrated the potential of aligning a frozen visual encoder with a language encoder using a single projection layer. 
% Kosmos-2 \cite{peng2023kosmos} further advanced multimodal learning by incorporating grounded text-visual understanding. 
% The field progressed with Monkey \cite{li2024monkey}, which improves scene understanding through high-resolution image processing. 
% Tool-based MLLMs also demonstrate capabilities in handling multimodal tasks, as exemplified by Chameleon \cite{lu2024chameleon}, which extends multimodal abilities by incorporating external tools for compositional reasoning. 
% DreamLLM \cite{dong2024dreamllmsynergisticmultimodalcomprehension} directly models language and image posteriors from raw multimodal data, allowing MLLMs to generate interleaved text and image content without relying on external extractors. 
% Inspired by cognitive science, V* \cite{wu2023vstar} introduces a unique visual search mechanism that uses language model knowledge to selectively focus on relevant image regions. 
% Despite these advancements, challenges persist in enabling MLLMs to comprehend and generate content in MMAG.
