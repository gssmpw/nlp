\section{Introduction}
\label{sec:intro}

Multimodal Large Language Models (MLLMs) \cite{dong2024dreamllmsynergisticmultimodalcomprehension,liu2024visual,sun2024generative,li2023blip} have made significant progress in recent years, allowing the comprehension and generation of diverse data modalities including text and images. 
However, in real-world scenarios, there exists a pervasive \textit{graph-structured relationships} between texts and images.
Such graph-structured relationship can be described as ``\textit{Multimodal Attributed Graphs}'' (MMAGs)~\cite{peng2024learningmultimodalgraphssurvey,jin2024instructg2isynthesizingimagesmultimodal,zhu2024multimodalgraphbenchmark}, where nodes are associated with image and text information.
For example, the artwork graph~\cite{mao2019visual} is composed of nodes that include images (pictures) and text (titles), with edges representing shared genres and authorship. This structure uniquely represents each artwork in relation to thousands of others within the graph, providing a context that extends beyond simple language descriptions or image references.
While MLLMs have demonstrated outstanding comprehension and generation capability for text and image data, it is questionable how they could utilize the structural information on MMAGs.

In this context, we formulate the problem of \textit{multimodal content generation} on MMAGs which tasks MLLMs with producing both a textual description and an accompanying image for a new node based on the graph connectivity and node attributes.
This task focuses on generating text-image pairs for a node from MMAGs, reflecting a wide range of practical applications. 
For example, generating an image and a text for a product node linked to others through co-purchase edges in an e-commerce MMAG is equivalent to recommending~\cite{deldjoo2024reviewmodernrecommendersystems,liu2024multibehaviorgenerativerecommendation} potential future products to users.
Likewise, creating an image and a title for a virtual artwork node in the art MMAG is comparable to creating virtual artwork~\cite{huang2022drawartdreamdiverse,epstein2023art} that reflects the subtle styles of various artists and genres.

However, directly adopting MLLMs on MMAGs for multimodal content generation presents several challenges:
(1) \textit{Graph Size Explosion}: Although MMAGs provide substantial context for image and text generation, inputting the entire local subgraph structure to a model is impractical due to the exponential increase in size with additional hops, leading to excessively long context sequences.
(2) \textit{Non-Euclidean Nature}: Unlike texts or images, which follow linear structures, graphs are inherently non-Euclidean with complex topologies~\cite{bronstein2017geometric}, making them challenging to feed into MLLMs. 
(3) \textit{Hierarchical Modality Dependency}: At the node level, complementary information from associated text and image data enhances the semantic understanding of individual nodes. At the subgraph level, integrated features derived from node text/image semantics and local graph structure enable a more nuanced understanding of the subgraph's context for target node generation.
(4) \textit{Inference Dependency}: Due to the intrinsic interdependence between text and image features within a node, as well as the dual objectives of image and text generation, the order of inference across these modalities is critical.

To address these challenges, we introduce \Ours, a multimodal large language model tailored for comprehensive understanding and creation within MMAGs. Our approach features several key contributions:
(1) We develop a personalized PageRank-based graph sampling method to extract relevant subgraph information, effectively mitigating the \textit{Graph Size Explosion} issue.
(2) We investigate various design approaches for graph linearization, adapting its \textit{Non-Euclidean Nature} to fit a sequential MLLM processing paradigm.
(3) We construct a hierarchical graph aligner, incorporating a node-level modality fusion Q-Former and a graph structure Q-Former to capture \textit{Hierarchical Modality Dependency} within MMAGs.
(4) We explore different inference strategies, including sequential and parallel generation, to address \textit{Inference Dependency} across modalities in MMAGs.
With adaptive graph prompt designs and specialized alignment techniques, \Ours achieves effective comprehension and content generation in MMAGs, overcoming key challenges related to graph topology and multimodal attribute integration.

The primary contributions of this paper are as follows:
\begin{itemize}
\item \textbf{Problem Formulation and Benchmarking.} We formally define the task of multimodal content generation from MMAGs and introduce three real-world benchmark datasets across domains such as art and e-commerce to support this task.
\item \textbf{Proposed Methodology.} We introduce \Ours, a multimodal large language model designed to effectively encode graph structures for concurrent image and text generation.
\item \textbf{Experiments and Evaluation.} We perform comprehensive experiments and evaluations, demonstrating that \Ours achieves significant improvements over baseline models.
\end{itemize}

% This challenge highlights the synergy between structured graph data and multimodal outputs, demonstrating the capability of MMAGs to generate rich, context-aware text and visuals that can enhance user experience and drive informed purchasing decisions in e-commerce platforms.

% This advancement has expanded the range of applications of multi-modality, enabling models to tackle complex, multimodal tasks with remarkable efficiency. Despite these achievements, one crucial modality remains underexplored: graphs.

% Graphs, composed of nodes representing entities and edges signifying relationships, are a powerful framework for capturing structured and relational information. They are extensively used across domains like social networks, knowledge graphs, and biological systems. In the multimodal scenario, each node can be enriched with data from various modalities, such as text, images, and numerical information, forming what are known as Multimodal Attributed Graphs (MMAGs)~\cite{jin2024instructg2isynthesizingimagesmultimodal}. MMAGs excel in not only depicting relationships between entities but also capturing the interactions between different modalities within each node. For instance, a node might contain textual attributes describing an entity while simultaneously linking to visual data, such as an image. This dual-layer structure, which combines entity-to-entity relationships with cross-modal connections, enables graphs to capture both the structural relationships between entities and the complex interactions among different data types within those entities. This comprehensive capability makes graphs particularly suited to modeling the interconnected nature of multimodal information.

% In this context, we introduce the multimodal attributed node generation challenge, which tasks multimodal generative models with producing both a textual description and an accompanying image based on the graph structure and node attributes. This task focuses on generating text-image pairs for a node from MMAGs, reflecting a wide range of practical applications. For example, in an e-commerce MMAG, a node could represent a product, with connections to various attributes such as customer reviews, product specifications, and images. The task would be to generate a detailed product description (text) along with a corresponding product image that accurately reflects the product's features and appeal. This challenge highlights the synergy between structured graph data and multimodal outputs, demonstrating the capability of MMAGs to generate rich, context-aware text and visuals that can enhance user experience and drive informed purchasing decisions in e-commerce platforms.

% However, incorporating MMAGs into the MLLM framework presents several challenges. \textbf{First}, Unlike text or images, which follow linear structures, graphs are inherently non-Euclidean with complex topologies, making them challenging for traditional language models. \textbf{Second} the relative importance of each modality—whether text, images, or graphs—varies by context, complicating their effective integration for multi-modal graph analysis. The challenge lies in aligning image, text, and graph structures within the MLLM hidden space. \textbf{Third,} generation order during inference across different modalities can affect their interdependencies, making it difficult to determine the optimal sequence for effective analysis.

% Therefore, we propose GraphGPT-o to bridge the gap between pre-trained MLLMs and MMAGs. By incorporating specialized alignment techniques and adaptive prompt designs, GraphGPT-o effectively comprehends and generates content based on MMAGs, addressing the inherent challenges of graph topology and multimodal attribute integration. Our main contributions are as follows:
% \begin{itemize}
% \item \textbf{Formulation and Benchmark.} We formulate the problem of integrating MMAGs into MLLMs and establish a comprehensive benchmark dataset for evaluation.
% \item \textbf{Prompt Design and Inference Setting.} We develop innovative prompt designs that facilitate the MLLMs' understanding of graph structures and node attributes.
% \item \textbf{Hierarchical Aligner for MMAGs.}  We introduce a novel hierarchical aligner module that adapts pre-trained MLLMs to handle MMAGs effectively. 
% \item \textbf{Experiments and Evaluation.} We conduct extensive experiments and evaluations to demonstrate the effectiveness of GraphGPT-o, showing significant improvements over baseline models in tasks involving graph-based text and image generation.
% \end{itemize}