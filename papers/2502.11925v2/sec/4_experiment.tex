\section{Experiment}
\label{sec:experiment}

\subsection{Experimental Setups}
\noindent\textbf{Datasets.} We conduct experiments on three multimodal attributed graphs from distinct domains: ART500K, Amazon-Baby, and Amazon-Beauty. The ART500K dataset represents artworks, where nodes correspond to individual pieces, and edges indicate relationships such as shared authorship or genre. The Amazon datasets, comprising Amazon-Baby and Amazon-Beauty, represent product graphs. Here, nodes denote products, while edges capture co-view relationships. Each node in these graphs is enriched with a title and an image.

\noindent\textbf{Metrics.} To thoroughly assess the comprehension and generation capabilities of \Ours on multimodal attributed graphs, our evaluation focuses on two key aspects:

\begin{itemize} 
\item The quality of the synthesized image and text, and how well they align. 
\item The text/image correspondence between synthesized nodes and the conditioned sub-graphs.
\end{itemize}

To evaluate the quality of the synthesized outputs, we use CLIP (CLIP-I2) scores to compare the synthesized images with the ground truth images, assessing image generation quality. We also measure the perplexity of the generated text to evaluate its coherence. Additionally, we calculate the CLIP (CLIP-IT) score of generated image-text pairs to assess image-text alignment.

To evaluate alignment with the conditioned sub-graph, we calculate the KL divergence (KL-DV) between the distributions of the neighbor nodes and generated node image-text CLIP scores.

\subsection{Graph Linear Tokenization}
\label{sec:res-linear}
In this section, we study the quantitative results with graph linear tokenization, which are presented in Table~\ref{table-prompt}, from which we observe the following:
\newline \noindent \textbf{(1) Node Modality Integration.} Utilizing both modalities together generally improves model performance, indicating that integrating multiple information sources leads to a more comprehensive understanding of the data.
\newline \noindent \textbf{(2) Node Modality Order.} The order in which the modalities are processed does not consistently or significantly affect model performance.
\newline \noindent \textbf{(3) Inference Strategy.} Generating the image first typically enhances the quality of the synthesized image but may reduce text quality, whereas starting with text generation results in the lowest KL-DV score.


\begin{table*}[h!]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{c|c|c|cccc|cccc|cccc} 
\toprule
\multirow{2}{*}{\textbf{Modality}} & \multirow{2}{*}{\textbf{Order}} & \multirow{2}{*}{\textbf{Inference}} & \multicolumn{4}{c|}{\textbf{ART500K}} & \multicolumn{4}{c|}{\textbf{Beauty}} & \multicolumn{4}{c}{\textbf{Baby}} \\ 
\cline{4-15}
 &  &  & \textbf{CLIP-I2} & \textbf{Perplexity} & \textbf{CLIP-IT} & \textbf{KL-DV} & \textbf{CLIP-I2} & \textbf{Perplexity} & \textbf{CLIP-IT} & \textbf{KL-DV} & \textbf{CLIP-I2} & \textbf{Perplexity} & \textbf{CLIP-IT} & \textbf{KL-DV} \\ 
\hline\hline
\multirow{3}{*}{Text-only} & \multirow{3}{*}{Text-first} & Text-first & 65.83 & 163.3 & 22.66 & 4.65 & 55.49 & 193.8 & 24.6 & 10.72 & \textbf{78.89} & 328.3 & 17.88 & 2.51 \\
 &  & Image-first & 65.31 & 619.9 & \underline{24.54} & 5.04 & 65.56 & 668.5 & \textbf{25.64} & 14.72 & \underline{75.36} & 819.8 & 23.84 & 1.32 \\
 &  & Parallel & 65.31 & 158.5 & 16.37 & 9.96 & 65.56 & 206.9 & 19.99 & 22.31 & \underline{75.36} & 253.5 & 18.74 & 6.99 \\ 
\hline\hline
\multirow{3}{*}{Image-only} & \multirow{3}{*}{Image-first} & Text-first & 73.08 & 130.4& 19.53 & \underline{0.33} & 62.25 & \underline{124.5} & 9.49 & 18.55 & 72.40 & 155.5 & 27.8 & \textbf{0.73} \\
 &  & Image-first & 75.55 & 177.7 & 12.18 & 9.85 & \textbf{67.61} & 266.8 & 20.85 & 14.83 & 75.22 & \textbf{130.1} & 10.62 & 2.54 \\
 &  & Parallel & 75.55 & 460.8 & 22.66 & 5.76 & \textbf{67.61} & \textbf{108.8} & \underline{25.48} & 10.82 & 75.22 & 178.8 & 21.76 & 1.14 \\ 
\hline\hline
\multirow{9}{*}{Text-Image} & \multirow{3}{*}{Text-first} & Text-first & 71.15 & 555.7 & 18.86 & 0.49 & 60.5 & 514.7 & 20.84 & \textbf{9.85} & 67.98 & 402.3 & 27.59 & 1.93 \\
 &  & Image-first & \textbf{79.26} & \textbf{117.7} & 20.15 & 2.78 & \underline{66.89} & 379.7 & 6.78 & 10.64 & 59.27 & \underline{153.3} & 8.78 & 16.07 \\
 &  & Parallel & 79.26 & 737.7 & 22.56 & 3.82 & \underline{66.89} & 407.3 & 19.50 & 10.65 & 59.27 & 839.2 & 14.62 & 7.43 \\  
\cmidrule{2-15}
 & \multirow{3}{*}{Image-first} & Text-first & 74.14 & 217.3 & 23.81 & \textbf{0.19} & 62.19 & 259.3 & 22.83 & \underline{10.27} & 71.38 & 325.4 & \textbf{33.31} & 5.86 \\
 &  & Image-first & \underline{77.81} & 437.8 & 19.32 & 3.19 & 60.55 & 353.7 & 14.32 & 24.77 & 65.48 & 242.2 & 9.62 & 1.15 \\
 &  & Parallel & 77.81 & 219.8 & 22.18 & 2.97 & 60.55 & 207.1 & 22.51 & 22.21 & 65.48 & 169.9 & 23.42 & \underline{0.79} \\ 
\cmidrule{2-15}
 & \multirow{3}{*}{Interleaved} & Text-first & 68.40 & 315.7 & 18.57 & 0.70 & 64.70 & 310.8 & 25.5 & 10.39 & 71.71 & 522.5 & \underline{31.86} & \underline{0.79} \\
 &  & Image-first & 77.71 & \underline{117.9} & 20.84 & 2.66 & 64.64 & 346.5 & 6.79 & 10.63 & 62.62 & 572.5 & 21.98 & 0.88 \\
 &  & Parallel & 77.71 & 402.8 & \textbf{28.41} & 2.68 & 64.64 & 354.8 & 24.69 & 18.38 & 62.62 & 572.5 & 13.95 & 7.72 \\ 
\bottomrule
\end{tabular}}
\caption{Evaluation Results for Different Modalities and Orders on ART500K, Amazon-Beauty, and Amazon-Baby Datasets}
\label{table-prompt}
\end{table*}






\subsection{Graph Hierarchical Tokenization}
\label{sec:res-Hiera}
\subsubsection{Quantitative Evaluation.}
In this section, we compare the results of the original DreamLLM \cite{dong2024dreamllmsynergisticmultimodalcomprehension} and Chameleon \cite{team2024chameleon}, and DreamLLM fine-tuned with graph linear tokenization prompts named \Ours (Hard), and trained with an additional hierarchical aligner module named \Ours (soft). The default prompt setting for training and inference utilizes both modalities, with text-first in the instruction and generating text-first during inference. The results are shown in Table~\ref{table-main}, from which we can observe that \Ours(soft) outperforms baselines in most cases, especially aligns better with the golden sub-graph.


\begin{table*}[h!]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{c|cccc|cccc|cccc} 
\toprule
\multirow{2}{*}{\textbf{Model}} & \multicolumn{4}{c|}{\textbf{ART500K}} & \multicolumn{4}{c|}{\textbf{Beauty}} & \multicolumn{4}{c}{\textbf{Baby}} \\ 
\cline{2-13}
 & \textbf{CLIP-I2} & \textbf{Perplexity} & \textbf{CLIP-IT} & \textbf{KL-DV} & \textbf{CLIP-I2} & \textbf{Perplexity} & \textbf{CLIP-IT} & \textbf{KL-DV} & \textbf{CLIP-I2} & \textbf{Perplexity} & \textbf{CLIP-IT} & \textbf{KL-DV} \\ 
 \hline    
\cmidrule{1-13}
\multirow{1}{*}
{Janus} & 59.32 & 351.2 & 21.43 & 7.59& 42.52 & 415.6 & 17.8 & 1.45 & 52.81 & 324.5 & 25.6 & 2.43\\
\cmidrule{1-13}
\multirow{1}{*}
{Emu3} & 62.11 & 257.5 & 20.07 & 9.83 & 45.82 & 398.3 & 24.2 & 5.96 & 58.8 & 374.5 & 23.7 & 3.55 \\
\cmidrule{1-13}
\multirow{1}{*}{Chameleon} & 61.19 & 228.3 & 23.87 & 4.18 & 43.73 & 180.9 & 16.6 & 22.80 & 45.20 & 144.7 & 0.56 & 1.87 \\ 
\cmidrule{1-13}
\multirow{1}{*}{DreamLLM} & 71.15 & 555.7 & 18.86 & 0.4882 & 60.5 & 514.7 & 20.84 & 9.85 & 67.98 & 402.3 & 27.59 & 1.92 \\
\cmidrule{1-13}
\multirow{1}{*}{\Ours(Hard)} & \textbf{77.62} & 347.4 & 18.58 & 0.9377 & 57.99 & \textbf{107.9} & 24.66 & 12.75 & 68.23 & 124.8 & 20.24 & 1.39  \\
\cmidrule{1-13}
\multirow{1}{*}{ \Ours (Soft)} & 72.64 & \textbf{59.8} & \textbf{25.63} & \textbf{0.4327} & \textbf{63.46} & 285.0 & \textbf{27.38} & \textbf{5.82} & \textbf{74.77} & \textbf{103.2} & \textbf{31.14} & \textbf{0.23} \\
\bottomrule
\end{tabular}}
\caption{Results for different backbones on ART500K, Amazon-Beauty, and Amazon-Baby Datasets}
\label{table-main}
\end{table*}


\subsubsection{Qualitative Evaluation.}
We performed a qualitative evaluation by randomly selecting several generated cases, and comparing them with ground-truth, DreamLLM, and ChatGPT-4o. The results are presented in Figure 4, which includes sampled neighbor images and text from the graph alongside the ground truth images and text. These findings show that \Ours generates images that align closer with the contextual information derived from the golden sub-graph, while DreamLLM and ChatGPT- 4o stick to one style and fail to adapt based on the input. 

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figs/quality.pdf}
    \caption{Qualitative evaluation. Our method exhibits better consistency with the ground truth by better utilizing the graph information from neighboring nodes.}
    \label{fig:enter-label}
\end{figure*}
\subsection{Ablation Study}
First, we evaluate the effectiveness of our \textbf{hierarchical aligner module} by individually removing the node feature Q-former and the graph structure Q-former. The results, presented in Table~\ref{table-ablation-module}, demonstrate that both modules contribute significantly to overall performance. Removing the graph structure causes a substantial increase in the KL-DV score while excluding the node features results in a higher perplexity for text.

\begin{table}
\centering
\resizebox{\linewidth}{!}{
    \begin{tabular}{c!{\vrule width \lightrulewidth}c!{\vrule width \lightrulewidth}c!{\vrule width \lightrulewidth}c!{\vrule width \lightrulewidth}c} 
    \toprule
    & CLIP-I2 & Perplexity & CLIP-IT & KL-DV \\ 
    \hline\hline
    \Ours w/o GSQ & 61.44 & \textbf{59.67} & \textbf{26.96} & 9.1406 \\ 
    \midrule
    \Ours w/o NFQ & {72.60} & 71.96 & {26.74} & 2.5050 \\ 
    \midrule
    \Ours & \textbf{72.64} & {59.80} & 25.63 & \textbf{0.4327} \\
    \bottomrule
    \end{tabular}
}
\caption{The impact of different modules in \Ours. w/o GSQ means without graph structure Q-Former and w/o NFQ means without node feature Q-Former.}
\label{table-ablation-module}
\end{table}

To further evaluate the effect of our hierarchical aligner module, a \textbf{GNN module} is used to replace it and the results are shown in Table ~\ref{table-ablation-GNN}, which shows that graph structure q-former is much better.

\begin{table}
\centering
\resizebox{\linewidth}{!}{
    \begin{tabular}{c!{\vrule width \lightrulewidth}c!{\vrule width \lightrulewidth}c!{\vrule width \lightrulewidth}c!{\vrule width \lightrulewidth}c} 
    \toprule
    & CLIP-I2 & Perplexity & CLIP-IT & KL-DV \\ 
    \hline\hline
    \Ours with GNN & 65.53   & 599.1      & 22.5    & 4.83 \\ 
    \midrule
    \Ours with GSQ & \textbf{71.15}   & \textbf{555.7}      & \textbf{18.86}   & \textbf{0.49} \\ 
    \bottomrule
    \end{tabular}
}
\caption{The impact of using GNN or graph structure Q-former (GSQ) for structure information learning in \Ours on ART500K dataset.}
\label{table-ablation-GNN}
\end{table}

We then assess the impact of our \textbf{Personalized PageRank sampling} method. From Figure~\ref{fig-sample-strategy}, it can be observed that our proposed Personalized PageRank sampling strategy effectively captures neighbors that contribute most to the ground truth in terms of texture, artistic style, and visual consistency. This results in a generated image that more closely resembles the ground-truth image's detailed patterns and overall aesthetic.
\begin{figure*}
    \centering
    \includegraphics[width=0.7\linewidth]{figs/sample-ablaiton.pdf}
    \caption{The impact of sampling strategies. Our proposed personalized PageRank sampling strategy leads to better image-text pair.}
    \label{fig-sample-strategy}
\end{figure*}


\subsection{Other Studys}
\textbf{Study of generation with partial node feature guidance.} We further conduct a study on the performance of \Ours with additional node text guidance or node image guidance. From Figure~\ref{fig-partial-guidance} we can see that the style and the character information is well-captured.

\begin{figure}[h!]
    \centering
    \includegraphics[width=1.0\linewidth]{figs/partial.pdf}
    \caption{Study of \Ours generation with auxiliary node feature guidance: either image or text.}
    \label{fig-partial-guidance}
\end{figure}

\noindent
\textbf{Study on the impact of number of neighbors.} Figure~\ref{fig-neighbors-art} shows that incorporating information from more neighbors can improve performance, but an excessive number may introduce noise, potentially hindering results.
\begin{figure}[h!]
\centering
\includegraphics[width=1.0\linewidth]{figs/art-tf-bo-1-5.pdf}
    \caption{Study on the different number of neighbors on ART500K dataset.}
    \label{fig-neighbors-art}
\end{figure}