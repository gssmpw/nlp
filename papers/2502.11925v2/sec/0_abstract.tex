\begin{abstract}
The rapid development of Multimodal Large Language Models (MLLMs) has enabled the integration of multiple modalities, including texts and images, within the large language model (LLM) framework.
However, texts and images are usually interconnected, forming a multimodal attributed graph (MMAG).
It is underexplored how MLLMs can incorporate the relational information (\textit{i.e.}, graph structure) and semantic information (\textit{i.e.,} texts and images) on such graphs for multimodal comprehension and generation.
In this paper, we propose \Ours, which supports omni-multimodal understanding and creation on MMAGs.
We first comprehensively study linearization variants to transform semantic and structural information as input for MLLMs.
Then, we propose a hierarchical aligner that enables deep graph encoding, bridging the gap between MMAGs and MLLMs.
Finally, we explore the inference choices, adapting MLLM to interleaved text and image generation in graph scenarios. 
Extensive experiments on three datasets from different domains demonstrate the effectiveness of our proposed method. Datasets and codes will be open-sourced upon acceptance.
% However, the incorporation of graph-based modalities remains relatively unexplored. Graphs, which represent relationships between entities, possess unique structural and relational information that can enhance language models in understanding complex data. 
% In this paper, we address this gap by introducing a novel multimodal dataset tailored for graph data. 
% We propose innovative methods for graph prompt design, enabling seamless integration of graph data into LLMs. 
% Additionally, we present a specialized hierarchical aligner to further processing graph information as input for LLM architectures. 
% Our work lays the groundwork for incorporating graph data into multimodal models and demonstrates how such data can enhance model comprehension and reasoning capabilities.
\end{abstract}


