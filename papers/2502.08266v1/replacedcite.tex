\section{Related Work}
\label{sec:related-work}
Despite the growing body of research on hate speech detection models, the literature lacks a thorough examination of the annotation process and the challenges associated with it. The process of annotation is already challenging due to many considerations such as what to do when the intent is covert or when the hate discourse is carried in an image. The subjective nature of hate speech further complicates the issue, as it can leads to disagreements among annotators. These disagreements can significantly impact the quality of the dataset and, consequently, the performance of  models trained with it.

____ evaluated three different annotation schemes on a hate speech corpus to enhance data reliability: binary, rating scales, and best-worst scaling (BWS). Although rating scales and best-worst scaling are more costly annotation methods, their experimental results indicate that these approaches are valuable for improving hate speech detection. 
%
____ proposed a hierarchical annotation scheme for hate speech aims to  focus on objective criteria rather than subjective categories or perceived degree of hatefulness. Their approach, used in the MaNeCo corpus, involves assessing the sentiment of the post (positive, negative, or neutral), identifying the target (individual or group), and categorizing the nature of the negative attitude, such as derogatory terms, insults, or threats, with a specific focus on whether the content incites violence. 
In contrast, ____ hypothesized that offensive content identification should be tailored to the individual. Therefore, they introduced two new perspectives: group-based and individual perception. Accordingly, they trained transformer-based models adjusted to personal and group agreements. They achieved the best performance in the individuality scenario—where annotations were provided by individuals based on their own perception of offensive content.
%, with an accuracy score of 89.0\%. 

____ introduced methods to measure annotator bias in abusive language datasets, focusing on identifying different perspectives on abusive language. They divided the annotators into groups based on their pessimistic and optimistic ratings, then created separate datasets for each group, consisting of documents annotated by all groups. The labels for these documents were determined by the majority vote of each group's annotators. The results showed that the classifier trained on annotations from the optimistic group performed best on its own test set (87.5\%) but worst on the pessimistic test set (64.5\%). This study highlights that bias is significantly influenced by annotators' subjective perceptions and the complexity of the annotation task. 
%
Similarly, ____ investigated the reasons behind disagreements in hate speech annotation using a mixed-method approach that included expert interviews, concept mapping exercises, and self-reporting from 170 annotators. Their findings revealed that individual differences among annotators, such as age and personality, influence their labeling decisions.
%
____ proposed a crowdsourcing framework for annotating hate speech that enables participants to register by providing their profile details, including name, age, nationality, date of birth, and location. They found that workers struggled to determine whether a comment was harmful or harmless based solely on the comment itself. To make accurate judgments, they needed to view the original post, related replies, and any associated images. Consequently, their research suggests that when designing tasks for crowdsourcing platforms, it is crucial to include relevant images and context-specific information alongside the post to improve annotation accuracy.
 
Moreover, ____ proposed two contrasting paradigms or data annotation, descriptive and prescriptive, to manage  annotator subjectivity in the annotation process. They reported that agreement is very low in the descriptive approach (Fleiss’ $k$ = 0.20), while agreement is significantly higher (Fleiss’ $k$ = 0.78) in the prescriptive approach.
Similarly, ____ adopted a perspectivist approach, categorizing elements as acceptable, inappropriate, offensive, or violent. They reported that reliable annotators disagree in about 20\% of cases. While the model’s performance aligns with the overall agreement among annotators, they reported a need for improvement for accurately detecting the minority class (violent).

Adding to this, ____ proposed a descriptive approach that categorizes hate speech into five distinct discursive categories, considering in particular tweets targeting Jews. %They annotated a sample dataset of 1,050 tweets. T
They also emphasized the importance of leveraging the complete Twitter conversations within the corpus, rather than focusing solely on the content of individual tweets. They argue that a reply that expresses a strong agreement to a hateful post may only be considered as hate speech when the context of the preceding posts is taken into account. 
%
In a related study, ____ investigated how providing context affects the quality of manual annotation for hate speech detection in online comments. By comparing annotations with and without context on the same dataset, they found that context significantly improves annotation quality, especially for replies. They showed that annotations are more consistent when context is available and highlighted that replies are harder to annotate consistently compared to comments.

____ demonstrated how challenging it is to accurately assess offensiveness when the targeted group is small or under-represented among annotators. This important observation indicates that majority voting can overlook important differences in how various groups perceive statements. For example, with the statement "women should just stay in the kitchen", four men might find it non-offensive, while one man and two women consider it offensive. This disparity highlights the difficulty in evaluating offensiveness across diverse demographic perspectives.
____ analyzed various datasets, detailing the purpose for which each dataset was created, the methods of data collection, and the annotation guidelines used. Their analysis revealed a lack of a standardized definition of abusive language, which frequently results in inconsistent annotations. Consequently, the main conclusion of their work is a call for a consistent definition of abusive language in research, including related concepts such as hate speech, aggression, and cyber-bullying. They emphasize that annotation guideline authors should adhere to these definitions to produce consistently annotated datasets, which can serve as benchmarks for future analyses.

____ introduced a sexism and misogyny dataset composed of approximately 8,000 comments from an Austrian newspaper's online forum, written in Austrian German with some dialectal and English elements. They used both prescriptive and descriptive approaches for annotation: the prescriptive approach specified what content should be labeled as sexist, while the descriptive approach allowed annotators to personally assess and rate the severity of sexism on a scale from 0 (not sexist) to 4 (highly sexist). They measured a large rate of disagreement among annotators, especially on estimating the fine-grained degree of sexism. %They achieved the best results on the binary problem (sexist yes/no) with the accuracy of 76.7\% and M-F1 score of 74.1\%. 

____ revealed that disagreements in annotation are often not due to errors, but are instead caused by multiple valid interpretations, particularly concerning boundaries, labels, or the presence of argumentation. These findings highlight the importance of analyzing disagreement more thoroughly, beyond just relying on inter-annotator agreement (IAA) measures. The research shows that not all disagreements in argumentation datasets are the same, and many should be seen as variations in perspective rather than true disagreements. To better understand the nature of these disagreements, IAA measures alone are insufficient, and a more detailed examination of the data, along with specific methodologies, is necessary. 

____ investigated presence of annotator biases (including gender, race, religion, and disability) in hate speech detection using both both GPT-3.5 and GPT-4o. They used prompts such as: "You are an annotator with the gender FEMALE. Annotate the following text as ‘Hateful’ or ‘Not Hateful’ with no explanation: [Text]." Their study highlights the risks of biases emerging when LLMs are directly employed for annotation tasks. 
%Is LLM defined by you - above? 


%please add a line or two here how related work relates to our work .... e.g. Some work has been done in .... or some researchers have highlighted the issues with .... we will also contribute to this research by .... cite ....  etc 

Many of these studies have highlighted the various challenges in hate speech annotation, such as the impact of annotator bias, subjective interpretations, and the need for context in annotations. Our research builds on this body of work by focusing specifically on handling annotation disagreements in hate speech detection. 
We aim to address the gaps  in existing research, by providing well-defined strategies in combining annotations with possibly multiple labels.
%such as improving the consistency and reliability of annotations through novel methods for managing disagreements. 
%BY novel methods degil aslinda 
%This will contribute to more accurate and robust hate speech detection models, enhancing the overall quality of annotated datasets.

%\newpage %olsun burada - see how much nicer it looks when not split
% So: hocam  mecburen  \newpage kaldiridm cunku LLM introduction ekledim biraz asagiya kaydi
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%