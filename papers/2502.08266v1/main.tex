\documentclass{article}

\usepackage{arxiv}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}		% Can be removed after putting your text content
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}

%------------------
\usepackage{amsmath}
\usepackage{comment}
\usepackage{soul}
%\usepackage{emoji}
\usepackage{orcidlink}
\usepackage{tikz}
\usepackage{amssymb}
%\usepackage[skip=0pt plus1pt, indent=14pt]{parskip}
\usepackage{caption}
\captionsetup[]{justification= justified, singlelinecheck=false}
\ifpdf%
\usepackage{epstopdf}%
\else%
\fi
\usepackage{indentfirst} 
\usepackage{multirow}
\usepackage{academicons}
\usepackage{orcidlink}
\usepackage{dsfont}
\usepackage{subcaption}

\usepackage{enumitem}
\setlist[enumerate,1]{start=0}

\setlength{\parindent}{0.4 cm}
\pagenumbering{gobble}

\setlength{\parskip}{3pt}
\renewcommand{\shorttitle}{}
\renewcommand{\arraystretch}{1.3}
\renewcommand{\baselinestretch}{1.1}

\hypersetup{
  colorlinks   = true, %Colours links instead of ugly boxes
  urlcolor     = blue, %Colour for external hyperlinks
  linkcolor    = blue, %Colour of internal links
  citecolor    = blue %Colour of citations
}
%------------------


\title{Dealing with Annotator Disagreement in \\ Hate Speech Classification}

%\date{September 9, 1985}	% Here you can change the date presented in the paper title
%\date{} 					% Or removing it

\author{Somaiyeh Dehghan\thanks{Corresponding author} \ $^{1,2}$ \orcidlink{0000-0002-5011-5821}, Mehmet Umut Sen$^{1,2}$ \orcidlink{0000-0002-8810-0502}, Berrin Yanikoglu$^{1,2}$ \orcidlink{0000-0001-7403-7592} \\
 $^{1}$Faculty of Engineering and Natural Sciences, Sabanci University, Istanbul, Turkey 34956 \\ $^{2}$Center of Excellence in Data Analytics (VERIM), Sabanci University, Istanbul, Turkey 34956 \\ \{somaiyeh.dehghan, umut.sen, berrin\}@sabanciuniv.edu \\}
 



% Uncomment to remove the date
\date{}

% Uncomment to override  the `A preprint' in the header
%\renewcommand{\headeright}{\textit{Preprint submitted to Cambridge University Press}}
%\renewcommand{\undertitle}{Technical Report}
%\renewcommand{\shorttitle}{\textit{arXiv} Template}
%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={A template for the arxiv style},
pdfsubject={},
pdfauthor={Somaiyeh Dehghan, Mehmet Umut Sen, Berrin Yanikoglu},
%pdfkeywords={First keyword, Second keyword, More},
}

\begin{document}
\maketitle

\begin{abstract}
Hate speech detection is a crucial task, especially on social media, where harmful content can spread quickly. Implementing machine learning models to automatically identify and address hate speech is essential for mitigating its impact and preventing its proliferation. The first step in developing an effective hate speech detection model is to acquire a high-quality dataset for training.  Labeled data is foundational for most natural language processing tasks, but categorizing hate speech is difficult due to the diverse and often subjective nature of hate speech, which can lead to varying interpretations and disagreements among annotators. This paper examines strategies for addressing annotator disagreement, an issue that has been largely overlooked. In particular, we evaluate different approaches to deal with annotator disagreement  regarding hate speech classification in Turkish tweets, based on a fine-tuned BERT model. Our work highlights the importance of the problem and provides state-of-art benchmark results for detection and understanding of hate speech in online discourse. 
\end{abstract}


% keywords can be removed
%\keywords{First keyword \and Second keyword \and More}

\section*{Disclaimer:} Some examples in this work include offensive language, hate speech, and profanity due to the nature of the study. These examples do not reflect the authors' opinions. We aim for this work to aid in detecting and preventing the spread of such harmful content and violence against minorities.

\section{Introduction}
Hate speech detection plays a vital role in maintaining a safe and respectful environment, especially on social media platforms. To achieve accurate automatic hate speech detection, it is crucial to have a sufficient amount of well-labeled training data. Large language models (LLMs) such as BERT \citep{Devlin2019} have demonstrated state-of-the-art performance in many NLP tasks, including hate speech detection. These models rely heavily on high-quality, accurately labeled datasets to train effectively. Therefore, ensuring that the training data is both fair and precise is essential for leveraging the full potential of these advanced models. However, earlier research on hate speech detection often lacks clarity in detailing their annotation processes, which can impact the quality of the datasets used for training.

% LLM yukairyal ekledim
%Hate speech detection plays a vital role in maintaining a safe and respectful environment, especially on social media platforms. In automatic hate speech detection, having a sufficient amount of accurately labeled training data is a critical factor for the success of any model. However, earlier research on hate speech detection tends to be somewhat unclear when it comes to detailing their annotation process. 

Many tasks in natural language processing (NLP) are subjective, meaning there can be a variety of valid perspectives on what the appropriate data labels should be. This is particularly true for tasks like hate speech detection, where individuals often hold differing opinions on what content should be labeled as hateful \citep{Talat2016, Salminen2019, Davani2021}. 
While certain viewpoints are more commonly accepted than others, there is no absolute, objective standard.
%
%Hate speech annotation is particularly challenging due to its subjective nature. While annotators often agree when labeling discourse that contains explicit threats or slurs against a target group as hate speech, they frequently differ in their classification of more subtle discriminatory language (e.g., "Refugees should not receive government assistance"). 
%
Additionally, some tweets may contain multiple intensities of hate within one tweet, ranging from discrimination to more severe forms such as cursing, further complicating the annotation process.
%as these nuances can lead to varying interpretations among annotators.

Since annotators might have subjective disagreements regarding labels (e.g., for hate speech), it is up to the dataset creators to decide how to handle these disagreements and establish the true labels. The issue is often addressed with one of two approaches:
(1) keeping only the samples where there is agreement among annotators, (2) selecting the samples where the majority opinion is clear. However, for cases of disagreement where the majority is not obvious, researchers must decide whether to use methods like expert adjudication, averaging annotations, or considering the context in more detail to determine the most accurate label.

%BY: Yani sadece propose yapmıyoruz, study ediyoruz, we shine light at the problem
%Given the challenge of subjective disagreements among annotators in labeling data, particularly for tasks like hate speech detection, in this study, we propose a novel approach for extracting the true label from such annotations. Our method explores various strategies to determine the most accurate label when faced with differing opinions. Specifically, we investigate the effectiveness of taking the maximum, minimum, random selection, and mean of the annotators' labels. Additionally, we introduce weighted versions of these strategies—weighted max, min, random, and mean—to account for the varying levels of confidence or reliability among annotators. This approach aims to systematically address the issue of annotator disagreement and provide a more reliable method for establishing true labels in subjective labeling tasks.

Recognizing the challenge of subjective disagreements among annotators, especially in tasks such as hate speech detection, we shed light on this issue and propose a novel method for deriving the true label from such annotations. Towards this end, we explore various strategies to determine the most accurate label when faced with differing opinions. 
Our analysis includes methods such as taking the maximum, minimum, random selection, and mean of the annotators' labels. Additionally, we examine weighted versions of these strategies—weighted max, min, random, and mean—to account for the varying levels of confidence or reliability among annotators. Through this approach, we aim to better understand and address the issue of annotator disagreement, ultimately providing a more reliable method for establishing true labels in subjective labeling tasks.

%what we need here is the simplest approach - e.g. taking all data where there is agreement (olabilir mi?)
%veya taking all data where the majority is obvious.... Yani burada amac once modeli basit bir data üzerinde tanıtmak, 
%dual-contrastive vs vs, sonra bu weightedMin/Max işlerine girmek
%Öneriniz var mı? <Mesela two-class üstünde sadece majority vote ( [0 0 2] => 0 alsak....



Our main contributions are as follows:
\begin{itemize}
  %Ya diğerleri de var ya contribute diyemiyoruz
  %\item We contribute a new dataset that covers five distinct topics—immigrants and refugees, the Israel-Palestine conflict, anti-Greek sentiment in Turkey, LGBTQ+, and religion/race/ethnicity—specifically tailored for hate speech detection in Turkish, providing valuable resources for further research in this domain.
  \item  We address the critical issue of annotator disagreement in the labeling of hate speech data and conduct a comprehensive evaluation of various strategies to manage the issue, even when considering multiple labels.
  \item We assess the effectiveness of these  strategies, along with potential issues and important observations, contributing to the development of more fair and reliable detection models.
  \item We show that annotating based on perceived hate speech strength—where the annotator provides judgments without following guidelines—can be a slightly inferior but faster alternative to annotation using detailed guidelines. This approach can also be utilized in building an ensemble model.
  %We show that annotating with the perceived hate speech strength-which is what the annotator is asked without following guidelines- can be a slightly inferior but fast alternative to annotating using detailed guidelines, that can also be used in building an ensemble model.
  \item We demonstrate state-of-art results in hate speech detection and classification in Turkish tweets, based on fine-tuning a pretrained BERT model.
\end{itemize}
 
  %This analysis offers insights into how different interpretations can be systematically addressed to improve labeling consistency.
  %asagıda "which" = "annot. disagr." oluyor ama annotation is challenging, not annot. disagr. 
  %which is a significant challenge due to the subjective nature of the task.

%bazisi genis zaman bazisi gecmis zaman olmamali!
The paper is organized as follows: In Section \ref{sec:related-work}, we review the existing literature on hate speech annotation, including works that report on the difficulty of the problem. In Section \ref{sec:our-dataset}, we introduce the topics of our used hate speech dataset. 
%
Section \ref{sec:annotation-process} provides a brief overview of our annotation process, including few main issues by the annotators, resulting in an iteratively improved guideline.
In Section \ref{sec:methodology}, we present alternative approaches to the annotator disagreement problem --ranging from only taking agreed-upon samples to weighted majority voting with tie-breaking--   and the architecture of our transformer-based models for hate speech classification and strength prediction.
Section \ref{sec:experiment} presents our experimental results, highlighting the performance variations and possible reasons. Finally, Section \ref{sec:conclusion} summarizes our findings and outlines future research directions.

%\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work} \label{sec:related-work}
Despite the growing body of research on hate speech detection models, the literature lacks a thorough examination of the annotation process and the challenges associated with it. The process of annotation is already challenging due to many considerations such as what to do when the intent is covert or when the hate discourse is carried in an image. The subjective nature of hate speech further complicates the issue, as it can leads to disagreements among annotators. These disagreements can significantly impact the quality of the dataset and, consequently, the performance of  models trained with it.

\citet{Poletto2019} evaluated three different annotation schemes on a hate speech corpus to enhance data reliability: binary, rating scales, and best-worst scaling (BWS). Although rating scales and best-worst scaling are more costly annotation methods, their experimental results indicate that these approaches are valuable for improving hate speech detection. 
%
\citet{Assimakopoulos2020} proposed a hierarchical annotation scheme for hate speech aims to  focus on objective criteria rather than subjective categories or perceived degree of hatefulness. Their approach, used in the MaNeCo corpus, involves assessing the sentiment of the post (positive, negative, or neutral), identifying the target (individual or group), and categorizing the nature of the negative attitude, such as derogatory terms, insults, or threats, with a specific focus on whether the content incites violence. 
In contrast, \citet{Kocon2021} hypothesized that offensive content identification should be tailored to the individual. Therefore, they introduced two new perspectives: group-based and individual perception. Accordingly, they trained transformer-based models adjusted to personal and group agreements. They achieved the best performance in the individuality scenario—where annotations were provided by individuals based on their own perception of offensive content.
%, with an accuracy score of 89.0\%. 

\citet{Wich2021} introduced methods to measure annotator bias in abusive language datasets, focusing on identifying different perspectives on abusive language. They divided the annotators into groups based on their pessimistic and optimistic ratings, then created separate datasets for each group, consisting of documents annotated by all groups. The labels for these documents were determined by the majority vote of each group's annotators. The results showed that the classifier trained on annotations from the optimistic group performed best on its own test set (87.5\%) but worst on the pessimistic test set (64.5\%). This study highlights that bias is significantly influenced by annotators' subjective perceptions and the complexity of the annotation task. 
%
Similarly, \citet{Sang2022} investigated the reasons behind disagreements in hate speech annotation using a mixed-method approach that included expert interviews, concept mapping exercises, and self-reporting from 170 annotators. Their findings revealed that individual differences among annotators, such as age and personality, influence their labeling decisions.
%
\cite{Meedin2022} proposed a crowdsourcing framework for annotating hate speech that enables participants to register by providing their profile details, including name, age, nationality, date of birth, and location. They found that workers struggled to determine whether a comment was harmful or harmless based solely on the comment itself. To make accurate judgments, they needed to view the original post, related replies, and any associated images. Consequently, their research suggests that when designing tasks for crowdsourcing platforms, it is crucial to include relevant images and context-specific information alongside the post to improve annotation accuracy.
 
Moreover, \citet{Rottger2022} proposed two contrasting paradigms or data annotation, descriptive and prescriptive, to manage  annotator subjectivity in the annotation process. They reported that agreement is very low in the descriptive approach (Fleiss’ $k$ = 0.20), while agreement is significantly higher (Fleiss’ $k$ = 0.78) in the prescriptive approach.
Similarly, \citet{Novak2022} adopted a perspectivist approach, categorizing elements as acceptable, inappropriate, offensive, or violent. They reported that reliable annotators disagree in about 20\% of cases. While the model’s performance aligns with the overall agreement among annotators, they reported a need for improvement for accurately detecting the minority class (violent).

Adding to this, \citet{Ron2023} proposed a descriptive approach that categorizes hate speech into five distinct discursive categories, considering in particular tweets targeting Jews. %They annotated a sample dataset of 1,050 tweets. T
They also emphasized the importance of leveraging the complete Twitter conversations within the corpus, rather than focusing solely on the content of individual tweets. They argue that a reply that expresses a strong agreement to a hateful post may only be considered as hate speech when the context of the preceding posts is taken into account. 
%
In a related study, \citet{Ljubesic2023} investigated how providing context affects the quality of manual annotation for hate speech detection in online comments. By comparing annotations with and without context on the same dataset, they found that context significantly improves annotation quality, especially for replies. They showed that annotations are more consistent when context is available and highlighted that replies are harder to annotate consistently compared to comments.

\citet{Fleisig2023} demonstrated how challenging it is to accurately assess offensiveness when the targeted group is small or under-represented among annotators. This important observation indicates that majority voting can overlook important differences in how various groups perceive statements. For example, with the statement "women should just stay in the kitchen", four men might find it non-offensive, while one man and two women consider it offensive. This disparity highlights the difficulty in evaluating offensiveness across diverse demographic perspectives.
\citet{Seemann2023} analyzed various datasets, detailing the purpose for which each dataset was created, the methods of data collection, and the annotation guidelines used. Their analysis revealed a lack of a standardized definition of abusive language, which frequently results in inconsistent annotations. Consequently, the main conclusion of their work is a call for a consistent definition of abusive language in research, including related concepts such as hate speech, aggression, and cyber-bullying. They emphasize that annotation guideline authors should adhere to these definitions to produce consistently annotated datasets, which can serve as benchmarks for future analyses.

\citet{Krenn2024} introduced a sexism and misogyny dataset composed of approximately 8,000 comments from an Austrian newspaper's online forum, written in Austrian German with some dialectal and English elements. They used both prescriptive and descriptive approaches for annotation: the prescriptive approach specified what content should be labeled as sexist, while the descriptive approach allowed annotators to personally assess and rate the severity of sexism on a scale from 0 (not sexist) to 4 (highly sexist). They measured a large rate of disagreement among annotators, especially on estimating the fine-grained degree of sexism. %They achieved the best results on the binary problem (sexist yes/no) with the accuracy of 76.7\% and M-F1 score of 74.1\%. 

\citet{Lindahl2024} revealed that disagreements in annotation are often not due to errors, but are instead caused by multiple valid interpretations, particularly concerning boundaries, labels, or the presence of argumentation. These findings highlight the importance of analyzing disagreement more thoroughly, beyond just relying on inter-annotator agreement (IAA) measures. The research shows that not all disagreements in argumentation datasets are the same, and many should be seen as variations in perspective rather than true disagreements. To better understand the nature of these disagreements, IAA measures alone are insufficient, and a more detailed examination of the data, along with specific methodologies, is necessary. 

\citet{Das2024} investigated presence of annotator biases (including gender, race, religion, and disability) in hate speech detection using both both GPT-3.5 and GPT-4o. They used prompts such as: "You are an annotator with the gender FEMALE. Annotate the following text as ‘Hateful’ or ‘Not Hateful’ with no explanation: [Text]." Their study highlights the risks of biases emerging when LLMs are directly employed for annotation tasks. 
%Is LLM defined by you - above? 


%please add a line or two here how related work relates to our work .... e.g. Some work has been done in .... or some researchers have highlighted the issues with .... we will also contribute to this research by .... cite ....  etc 

Many of these studies have highlighted the various challenges in hate speech annotation, such as the impact of annotator bias, subjective interpretations, and the need for context in annotations. Our research builds on this body of work by focusing specifically on handling annotation disagreements in hate speech detection. 
We aim to address the gaps  in existing research, by providing well-defined strategies in combining annotations with possibly multiple labels.
%such as improving the consistency and reliability of annotations through novel methods for managing disagreements. 
%BY novel methods degil aslinda 
%This will contribute to more accurate and robust hate speech detection models, enhancing the overall quality of annotated datasets.

%\newpage %olsun burada - see how much nicer it looks when not split
% So: hocam  mecburen  \newpage kaldiridm cunku LLM introduction ekledim biraz asagiya kaydi
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Dataset} \label{sec:our-dataset}

We use a dataset containing 11,021 samples to date, across 5 topics in Turkish as given below. The dataset is part of an ongoing project, to be made public in the near future at the end of the project. The first set of tweets collected within the project has been already shared in \citep{Arin2023, Gökçe2024, Dehghan2024a, Dehghan2024b}. The topics covered in the dataset are as follows: 


\vspace{3pt}
\paragraph{\textbf{Immigrants and Refugees in Turkey}} 
In recent years, the civil wars in Syria and Afghanistan have led countless immigrants and refugees from these countries to seek refuge in Turkey. According to the latest statistics, approximately 3.7 million Syrians and around 300,000 Afghans have settled in Turkey. 
While public opinion was initially welcoming during the early stages of the refugee crisis, the challenges posed by the large influx of asylum seekers and the widespread misconception that refugees receive rights not granted to Turkish citizens have fueled growing negative sentiments toward them. Consequently, this has led to an increase in hate speech directed at them on social networks. This topic includes 2,281 tweets.

%\vspace{3pt}
\paragraph{\textbf{Israel-Palestine Conflict}} 
%, 
The Israel-Palestine conflict, which began in the mid-20th century, remains one of the world's most enduring disputes,
%cok yorum yazmamalıyız - ilkinde var cunku herkes bilmiyor diye
%Despite long-standing efforts toward peace and reconciliation, the conflict persists, 
with pro-Israeli and pro-Palestinian groups holding sharply opposing views. 
As the situation frequently escalates into warfare, it continues to be a highly debated topic in Turkey, which has a generally pro-Palestine views.
This topic includes 2,873 tweets.

%\vspace{3pt}
\paragraph{\textbf{Anti-Greek Sentiment in Turkey}} 
Anti-Hellenism, or Hellenophobia (commonly known as Anti-Greek sentiment), refers to hatred and prejudice against Greeks, the Hellenic Republic, and Greek culture. Since the Treaty of Lausanne, Turkey and Greece have been in conflict over the sovereignty of the Aegean islands, territorial waters, flight zones, and the rights of their respective minorities. In the summer of 2022, Greece began to increase its military presence on the islands, which heightened the rhetoric between politicians in Ankara and Athens, as well as tensions between the two populations, especially as the Turkish elections approached. This topic includes 2,033 samples.

\paragraph{\textbf{Religion/Race/Ethnicity (Alevis, Armenians, Arabs, Kurds, and Jews)}} In addition to the above four topics, people sometimes directly attack ethnic groups or religions with hate speech. Hate speech against groups like Alevis, Armenians, Arabs, Jews, Kurds, and Romans often stems from historical, political, and social factors, as well as media and propaganda that create and perpetuate stereotypes, prejudices, and discrimination. These attacks are often rooted in deep-seated biases and misconceptions that have been reinforced over generations, leading to further marginalization and social division. This topic includes 3,135 samples.

\paragraph{\textbf{LGBTQ+}} In Turkey and many Muslim-majority countries, opposition to LGBTQ+ individuals often stems from deeply rooted cultural, religious, and social beliefs. Islam, the predominant religion in these regions, traditionally views homosexuality as sinful and contrary to religious teachings. These views are further reinforced by conservative cultural norms that emphasize traditional family structures and gender roles. In Turkey, where these beliefs are prevalent, LGBTQ+ individuals often face societal rejection, discrimination, and hostility. Additionally, political discourse and government policies in some Muslim countries frequently position LGBTQ+ rights as being in opposition to national values, further fueling negative sentiments and hate speech against the LGBTQ+ community. This topic includes 699 samples.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Annotation Process} \label{sec:annotation-process}

We use a prescriptive annotation strategy for classifying tweets into different hate speech categories and a descriptive one for indicating the perceived  degree (strength) of hate speech on a scale of $[0, 10]$.  
The annotation guidelines  were refined over time to reduce ambiguities when annotating the tweets into different categories. The degree of hate speech was asked independently of the category, as a second measure and to study its correlation with hate speech classes.

Tweets were assigned to three annotators in batches of 50 using Label Studio\footnote{https://labelstud.io/}
and annotators were asked to label each tweet one by one according to the guidelines.
%
The following six categories were decided for a comprehensive labelling:
\vspace{3pt}
\begin{enumerate}
   \item \textbf{No Hate Speech:} Tweet does not contain hate speech. 
   %None-hate olmaz ki! None olur, No-HS olur, ....
   
   \item \textbf{Exclusion/discriminatory discourse:} These are discourses in which a community is seen as negatively different from the dominant group as to the benefit, rights and freedoms in the society. For instance, the expressions "Suriyeliler oy kullanmasın" (Syrians should not vote) and "Kürtçe eğitim kabul edilemez" (Kurdish education is unacceptable) are considered discriminatory discourse and hate speech.
   %, and "Gayrimüslimler milletvekili olmasın" (Non-Muslims should not be members of parliament) 
   
  \item \textbf{Exaggeration, Generalization, Attribution, Distortion:} These are discourses that draw larger conclusions and inferences from 
  individual events or situations;  manipulate real data by distorting it; or attribute individual events to the whole identity based on their agents.  For example, "Eşcinsel sapkınlar dehşet saçıyor" (Perverted homosexuals are spreading terror), "Suriyeliler gına getirdi" (I am fed up from Syrians), after  individual and isolated incidents.

  \item \textbf{Symbolization:} These are discourses in which an element of identity itself is used as an element of insult, hatred or humiliation and the identity is symbolized in such manners (e.g., "Ermeni gibi konuştular" (They spoke like Armenians)). 
   %"Yunan artığına Atatürk cevabı" (Ataturk's response to the Greek surplus), 
   %, "Bizi Eurovision’da Yahudi mi temsil edecek?", "Will the Jew represent us at Eurovision?". % Eli'@in paperinde vardi .çıkardım
  
  \item \textbf{Swearing, Insult, Defamation, Dehumanization:} These are discourses that include direct profanity, insult, contempt towards a community, or insults by characterizing them with actions or adjectives specific to non-human beings, e.g., 
  %"Küstah Rum’a Gözdağı" (Intimidation to the Arrogant Greek), 
   %"Danimarkalı itler iftar basıp, Kur’an yaktı" (Danish dogs raided iftar and burned the Quran), 
   "Barbar ve ahlaksız Fransızlar" (Barbaric and immoral French). 

  \item \textbf{Threat of Enmity, War, Attack, Murder, or Harm:} These are discourses that include expressions about a community that are hostile, evoke war or express a desire to harm the identity in question, e.g., 
  %"Haydut Rumlar Ateşle Oynuyor" (Rogue Greeks Playing with Fire), 
  "Mültecilerin ölmesini istiyorum" (I want the refugees to die).
\end{enumerate}

To form the guidelines and ensure that  annotators follow a consistent set of rules, we iteratively refined the guidelines to resolve ambiguities and conflicts in the annotation process. For instance, we have added more examples to the classes to eliminate confusion and clarified the guidelines about what to do in ambiguous situations such as when a tweet contains hate speech towards multiple groups or when it contains covert hate speech. 

Nonetheless, hate speech annotation is a difficult task due to its relatively subjective nature. While annotators typically agree on discourse that includes swear words or threats towards a target group, they quite often disagree on how to classify discriminatory speech. 
When each tweet can be assigned to multiple categories (e.g. swear and threat) and there are multiple annotators, the resulting annotations are often not in agreement. This is called \textit{annotator disagreement}. Figure \ref{fig:senario} illustrates the three tweet annotations by three annotators who have assigned one or two labels (classes 0-5) per tweet.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{images/senario.png}
    \caption{Three tweets are annotated by one or more labels by three annotators, showing agreement, clear majority, and no clear majority cases in order. }
    \label{fig:senario}
\end{figure}

Researchers often address the issue of annotator disagreement by discarding samples for which there are disagreements, with the goal of obtaining data without label noise. In addition to losing a large portion of data, this results in overly optimistic model results since difficult cases are left out. Another common approach to deal with annotator disagreements is to have a second annotation phase where annotators are expected to come to a consensus. However, this is a lengthy process and consensus may not always be possible. 
%
Other research select the final label ("gold standard"), but  offer scant details on how it  is determined from the initial annotations.
In this work, we explore different approaches for handling annotator disagreement and selecting the final label, and evaluate resulting performances.

In addition, we derive a 4-class and 2-class categorizations from the initial 6-class labels. The two-class categorization is commonly performed in  literature, whereas the 4-class categorization is proposed in this work, where we group symbolization and exaggeration categories together, and swearing and threat categories as another,
as explained in Section \ref{sec:mv-setting}. %5.1.3. %buraya label eklemek iyi olur. 
This categorization reduces annotator disagreement due to the smaller number of classes; but more importantly, classes show increased hate speech strength. %, as shown in Figure 2.
%One reason for using the 4-class classification is that annotators generally  have a difficult time separating the contain exclusion speech.

%
%In the final guideline used for labeling the data, annotators were instructed to select more than one category when a tweet contains different types of hate speech (e.g. swear and threat) and to indicate hate speech even if it is covert, as long as they understand it from background knowledge. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodology} \label{sec:methodology}
In this section, we first discuss annotator disagreement and our proposed handling strategies (\ref{sec:5.1}). Then, we present our transformer-based model for hate speech classification and hate strength prediction (\ref{sec:5.2}). Finally, we analyze the relationship between classes and strengths (\ref{sec:5.3}).
%The results section shows the performance results with the different strategies and the transformer model.

%----------------------------------
\subsection{Handling Annotator Disagreement} \label{sec:5.1}

%In many studies, researchers require that the data is labeled with agreement, so they may discard data for which there is no consensus or at least a majority label, or  a second stage of annotation focuses on resolving differences among annotators by refining the guidelines and through discussions.

%\textbf{Requiring Agreement:} 
Many works in the literature %\citep{Krenn2024, Braun2024} İlki Braun mu?
seek agreement among annotators by either using only the subset of the collected data where there is agreement \citep{Braun2024}  or by requiring a second phase of annotation so that the annotators can reach a consensus \citep{Krenn2024}. 
%
%---------------------------------------------------
%\textbf{Using the Majority/Mean Label:} 
Other studies handle disagreements by selecting the final label (gold standard) through majority voting. In this case, each independent annotation counts as a vote, and the annotation that receives the most votes is selected as the gold standard. However, research in more objective fields, such as the medical domain, generally agrees that majority voting is not an ideal method for resolving annotation disagreements.
\citet{Sudre2019} recommend retaining the labels from individual annotators to preserve the disagreements, whereas \citet{Campagner2021} proposed different methods for consolidating conflicting labels into a single groundtruth, namely corrected majority, probabilistic overwhelming majority, and fuzzy possibilistic three-way.

While the issue of annotator disagreement is very prevalent, there are very few studies that explore the effects of including disagreeing annotations.
\citet{Prabhakaran2021} suggest that dataset developers should consider including annotator-level labels, but do not provide specific guidance on how to handle disagreement in the annotation process. 
In contrast, \citep{Jamison2015} recommend that the best strategy is to exclude instances with low agreement from the training data.

%\citet{Prabhakaran2021} suggest that dataset developers should consider including annotator-level labels but do not provide specific guidance on handling disagreement in the annotation process.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{\textbf{Majority Voting for Multi-Label Annotations}} \label{sec:5.1.1}
\vspace{6pt} 

Most prior research that consolidates multiple annotations into single class labels focuses on data where each annotator selects only one category. In contrast, our work addresses multi-label annotations, allowing each annotator to select multiple categories. %Some examples for this annotation scheme are given at Figure-\ref{fig:senario}. 

In multi-label annotation scheme, we are given a set of classes from each annotator. Let $L$ be the set of classes, i.e., $L=\{0, 1, 2, 3, 4, 5\}$ for our 6-class setting and let $A_k(x) \subseteq L$ be the annotation (set of labels) 
of data-point $x$ by the annotator $k$. Then, we define the majority-voting (MV) result of $x$ as follows:
\begin{equation}
M(x) = \left\{ l \;\;:\;\; c_l(x) = \max_{l'} c_{l'}(x) \right\}
\end{equation}

where $c_l(x)$ is the number of times label-$l$ is selected by the annotators:

\begin{equation}
c_l(x) = \sum_k \mathds{1} \left( l \in A_k(x) \right )
\end{equation}

where $\mathds{1} \left( l \in A_k(x) \right )$ is $1$ if $l \in A_k(x)$ and $0$ otherwise. The result $M(x)$ of the majority-voting is a set of classes and there are more than one element in this set in the case there is equality among classes. This set contains a single class in the case there is a clear winner of the Majority Voting. We define three different scenarios, not mutually exclusive, based on annotator agreements and Majority Voting results:

\begin{itemize}
    \item \textbf{Agreement}: Each annotator selects the same single class: 
    $|A_k(x)|=1 \; \forall k$ and $A_i(x)=A_j(x) \; \forall i,j$. An example for the agreement scenario is as follows: $A_1(x)=\{1\}$, $A_2(x)=\{1\}$, $A_3(x)=\{1\}$ where class-1 is the "Exclusion/discriminatory discourse" as described in Section \ref{sec:annotation-process}.
    %
    \item \textbf{Clear majority}: The majority voting set contains a single class: $|M(x)|=1$. This scenario also contains examples of the above "agreement" scenario. An example for this scenario is as follows: $A_1(x)=\{0\}$, $A_2(x)=\{3,4\}$, $A_3(x)=\{4,5\}$ where the majority voting set is $M(x)=\{4\}$ since count of the class-4 is $c_4(x)=2$ and it is the maximum.
    %
    \item \textbf{No clear majority}: There is an equality between some classes, i.e. the Majority Voting set may contain multiple classes: $|M(x)|\geq1$. This scenario contains all the examples. An example for which majority voting is not clear is as follows: $A_1(x)=\{4,5\}$, $A_2(x)=\{1,5\}$, $A_1(x)=\{2,4\}$ where the Majority Voting set is $M(x)=\{4,5\}$.
\end{itemize}

%Bunu ekledim asagiya: Furthermore, in this case, there must be a single selection from each annotator, so tweets involving multiple classes are also filtered out.

% Umut ama herkes 2 secim yapsa da majority oluyor - neden single class??

% Olmuyor Hocam asagida sizin ornekteki gibi yani (0,1) (0,1)

% ben merak ediyorsam okuyan da edecektir - buralara biraz ornek iyi olur, her yerde agreemnt diyoruz, ben simdi ilk defa farkettim (0,1) (0,1) diyen 2 annot. bile olsa agreement olmayacak burada 

% Hocam zaten |a_k(x)|=1 o yuzden dedik, yani herkes multiple secerse majority clear olmuyor

%Yukaridaki "single" lari Fig 1e ref. ile aciklaman lazim, yani birinde annot single (agreement da) digerinde secilen single (majority)- o kafa karistiryr. Ornek verebilirsin uzatmak da istiyoruz ya

Some other examples for such scenarios are given at Figure \ref{fig:senario}. Note that class indices which are in the range [0, 5] are described at Section \ref{sec:annotation-process}.

\subsubsection{\textbf{Strategies When Majority Vote Is Not Clear}} \vspace{6pt}

If majority is not clear, a usual resolution is to pick a random label from the set $M(x)$ or discard that data-point from the training set. In this work, since we assume classes are ordered, we propose other solutions for the not-clear majority scenario: minimum ($y_{min}$), maximum ($y_{max}$) and the mean ($y_{mean}$). The definitions are as follows:

\begin{equation}
    y_{min}(x) = \min M(x)
\end{equation}

\begin{equation}
    y_{max}(x) = \max M(x)
\end{equation}

\begin{equation}
y_{mean}(x) = round \left(\frac{1}{|M(x)|} \sum_{l\in M(x)} l \right)
\end{equation}

where $round$ is the rounding function which maps rational numbers to the closest integer. 

We also propose a weighted majority voting where weights are inversely proportional to the number of classes an annotator chooses:

\begin{equation}
c_l^{(w)}(x) = \sum_k \frac{1}{|A_k(x)|} \mathds{1} \left( l \in A_k(x) \right )
\end{equation}

Then, the weighted majority voting set is found as follows:

\begin{equation}
M^{(w)}(x) = \left\{ l \;\;:\;\; c_l^{(w)}(x) = \max_{l'} c^{(w)}_{l'}(x) \right\}
\end{equation}

This weighted majority voting setting decreases the number of data-points for which Majority Voting is not clear. For example, for the datapoint with the annotations $A_1(x)=\{1,2\}$, $A_2(x)=\{2\}$, $A_3(x)=\{1,4\}$; regular majority voting set is $M(x)=\{1,2\}$ whereas the weighted majority voting set is $M^{(w)}(x)=\{2\}$. 

%Bir satir daha yazarsan (ornek vs) bu newpage e de gerek kalmaz :)

% \newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsubsection{\textbf{Majority Voting for the Four-Class and the Two-Class Settings}} 
\subsubsection{\textbf{Deriving the Four-Class and the Two-Class Labels}}\vspace{6pt} \label{sec:mv-setting}

In the four-class setting, we first map the six-class labels to the corresponding four-class labels, then calculate the counts of four-class labels before taking the majority vote:

\begin{equation}
c_l^{(4)}(x) = \sum_k \sum_{l'\in A_k(x)} \mathds{1} \left(l=r_4(l') \right)
\end{equation}

where $\mathds{1} \left(l=r_4(l') \right)$ is $1$ if $l=r_4(l')$, $0$ otherwise and $r_4$ is the reduction mapping from the six-class setting to the four-class setting:
\begin{equation*}
r_4(l) = \begin{cases}
0 & l=0 \\
1 & l=1 \\
2 & l=2 \;\;\text{or}\;\; l=3 \\
3 & l=4 \;\;\text{or}\;\; l=5
\end{cases}
\end{equation*}

For two-class setting we follow the same approach, only with the following mapping function:
\begin{equation*}
r_2(l) = \begin{cases}
0 & l=0 \\
1 & \text{otherwise}
\end{cases}
\end{equation*}

Then the counts are calculated as follows:

\begin{equation}
c_l^{(2)}(x) = \sum_k \sum_{l'\in A_k(x)} \mathds{1} \left(l=r_2(l') \right)
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Classification and Regression Models} \label{sec:5.2}
We design our classification model using transfer learning, incorporating a single layer on top of the BERT \citep{Devlin2019} 2019) encoder to predict hate speech categories or strengths. BERT, being a state-of-the-art model, has demonstrated success across numerous NLP tasks, including in our previous works \citep{Beyhan2022, Arin2023, Dehghan2024a, Dehghan2024b}, where it has consistently shown strong performance.

We use BERTurk\footnote{https://huggingface.co/dbmdz/bert-base-turkish-uncased} checkpoint from Huggingface Transformer package and  apply fine-tuning for 10 epochs. We reserve 5\% of the train data as validation and select the model with the best validation performance --accuracy for the classification  and MSE loss for the regression tasks. During training, we use a learning rate of $5 \times 10^{-6}$, batch-size of $4$ and apply weight-decay regularization with a parameter of $0.01$.

For the classification problem, we use the cross-entropy loss:
\begin{equation}
    L_{CE} = -\sum_{i=1}^{N} y_ilog(\hat y_i)
\end{equation}
where $y_i$ is the target value for the $i^{th}$ input and $\hat y_i$ is the prediction.

\vspace{3pt}
For regression problem, we use the MSE loss (Mean Squared Loss) to compute strength of hate in the scale of $\{0, 10\}$
\begin{equation}
        L_{MSE} = \frac{1}{N}\sum_{i=1}^{N} (y_i- \hat y_i)^2
\end{equation}

where $y_i$ and $\hat y_i$ are the desired values and predicted values, respectively.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\newpage
\subsection{\textbf{Analyzing the Relation Between Classes and Strengths}}
\label{sec:5.3}

As previously noted, during data preparation, annotators were asked to select both a class and a strength value ranging from 0 to 10 to represent the severity of hate speech in each tweet. This class-strength relationship allows us to obtain the strength distributions for each class. 

%dash kullanmayın ltf Figure- gibi
Figure \ref{fig:class_dist6} illustrates these distributions within the original 6-class setting. 
%We observe that the distribution for the class 'Discriminatory Speech' (class-1) is skewed to the left, while the distributions for the classes 'Swearing, Insult, Defamation, Dehumanization' (numbered class-4) and 'Threat of Enmity, War, Attack, Murder, or Harm' (numbered class-5) are skewed to the right. 
%Additionally, the distributions for the classes 'Symbolization' (numbered 2) and 'Exaggeration, Generalization, Attribution, Distortion' (numbered 3) are symmetrical. This observation supports the rationale for grouping classes 2 with 3 and classes 4 with 5 in the 4-class setting. 
%Umutcum bu main observation olmalı
The first observation we can derive from the class-strength distributions is the considerable variability in the assigned strength scores. 
Specifically, for all classes except class-0 ("No Hate Speech"), the strength values span the entire range from 0 to 10. This wide range of scores highlights the inherent difficulty and subjectivity involved in categorizing tweets into specific hate-speech classes. The variability suggests that annotators may perceive and interpret the severity of hate speech differently, leading to a broad distribution of scores even within the same class. This subjectivity not only poses a challenge for the annotators in consistently assigning strength scores but also complicates the task for models attempting to accurately classify hate speech. %The variability within each class underscores the nuanced nature of hate speech and the complexities involved in its detection and categorization.

We also note that the class "Discriminatory Speech" (class-1) 
is often labelled as no-hate speech. Indeed, classifying discriminatory speech as hate speech is one of the  most argued points, by annotators. 

Finally, the mean strength scores for classes 2 and 3 (4.94 and 5.01) are also similar, as well as the means of classes 4 and 5 (5.81 and 6.26). 
This observation supports the rationale for grouping classes 2 with 3 and classes 4 with 5 in the 4-class setting. 
Distributions for the resulting 4-class setting are improved in the sense that class means are more separated, as depicted in Figure \ref{fig:class_dist4}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[h]
\label{fig:dists}
\centering
\begin{subfigure}[b]{1\textwidth}
   \includegraphics[width=1\linewidth]{images/cats_degree_dist02}
   \caption{Strength distributions for the 6-class setting. Means are as follows: [0.15, 4.59, 4.94, 5.01, 5.81, 6.26]}
   \label{fig:class_dist6} 
\end{subfigure}

\begin{subfigure}[b]{1\textwidth}
    \centering
   \includegraphics[width=0.66\linewidth]{images/cats4_degree_dist02_02}
   \caption{Strength distributions for the 4-class setting. Means are as follows: [0.15, 4.59, 4.97, 6.01]}
   \label{fig:class_dist4}
\end{subfigure}
\caption{Relation of classes with strength scale (a) 6-class and (b) 4-class problems. Vertical dashed lines represent the mean of the distributions.}
%Distributions of strength scales for different categories (b) Means of strength scales for different categories.}
%\vspace{3pt}
\end{figure}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \newpage
\section{Experiments} \label{sec:experiment}

We have implemented three methods to handle annotator disagreements (Experiments 1-3) and evaluated  the performance of the classification model trained with the data obtained in each case. For each experiment, the data was divided into an 80-20\% split for training and testing, respectively. The performance on the test set was evaluated using macro F1 and accuracy scores.
%
In the fourth experiment, we trained and evaluated a regression model using the hate speech strength indicated by annotators. It should be noted that each annotator was allowed to indicate multiple classes, but only a single strength value.


\textbf{Experiment 1:} We only kept the tweets for which there is consensus (i.e. all annotators have picked the same single category). Using the terminology defined in Section 5, this corresponds to  $|a_k(x)|=1 \; \forall k$ and $a_i(x)=a_j(x) \; \forall i,j$ where $a_k(x)$ is the set of label selections from annotator $k$ for the tweet $x$.
%No need? and $a_i(x)=a_j(x) \; \forall i,j$.). 

Note that this approach filtered out many tweets and resulted in the smallest dataset. 
Furthermore, as annotators in general agree about the no hate speech class, there are almost 4-fold samples in that category. Furthermore, in this case, there must be a single selection from each annotator, so tweets involving multiple classes are also filtered out.

\textbf{Experiment 2:} In addition to the above tweets, we also kept the tweets where the majority vote is clear (see \ref{sec:5.1.1}).
%: $|m(x)|=1$. 
This increases both the total number of tweets kept and  the proportion of Hate category (where there is less consensus) among the data that is kept. 

\textbf{Experiment 3:} In this experiment, we used all the data, including those from the first and second experiments, as well as the data where the majority is not clearly defined. The disagreement rates are approximately 10\%, 12\%, and 13\% for the 2-class, 4-class, and 6-class problems, respectively.
%The number of disagreements is approximately 10\%, 12\%, and 13\% for the 2-class, 4-class, and 6-class problems, respectively.

\textbf{Experiment 4:} In this experiment, we trained a model to predict the hate speech strength, using as label only the annotated strength ranging from 0 to 10, where a score of 0 indicates the absence of hate speech, and any score above 0 quantifies the severity of the hate speech. 
More specifically, for each tweet, we calculated the mean of the annotated strength scores and subsequently trained a regression model using the Mean Squared Error (MSE) loss function.

To assess the classification performance of the regression-based model, we transformed the mean strength scores into binary labels: tweets with a mean score above 0.5 were labeled as "hate", while those with a score of 0.5 or below were labeled as "no-hate". During the testing phase, the predicted binary classes were determined by applying a threshold to the scores.

We conducted experiments on different subsets of the data, comparing results from subsets with full annotator agreement to those including all data, to examine the impact of excluding data points lacking consensus among annotators. To identify cases of agreement among annotators, we first converted the strength annotations into auxiliary binary values: a score of 0 was mapped to 0, while scores in the range of [1, 10] were mapped to 1. We then selected tweets for which all annotators assigned the same binary value. 
Additionally, we trained a classification-based model to provide a comparative baseline.

%oldu sanki simdi - yani bu bolumn strength uzerinden gittigini yazmak istedim
% To evaluate the classification performance of the regression-based model, we converted the mean strength scores into auxiliary binary labels: tweets with a mean score greater than 0.5 were labeled as 'hate' while those with a score of 0.5 or lower were labeled as 'no-hate'. 
% On the test phase, we find the predicted binary classes by thresholding the scores. 
% % We then trained a binary classifier with these labels, 
% %We then obtained binary class predictions by thresholding the final scores 
% We conducted the experiments on different data subsets (agreements vs. all) to analyze the effect of excluding data which are not all agreed by the annotators.
% In addition, we also trained a classification-based model for comparison.
%Umut burada regresion model sonucunu classif. olarak reinterpret mi ettin, yoksa binary label aldin saniyorum 
%ona gore benim texti degistir ltf. yani we converted predicted scores degil galiba - tamam
%binary classification yerine binary label yazmak lazim - yukarida oyle duzelttim

%cabuk yaz Somayeh cikacam birazdan!! Yukardaki dogru ise daha anlatmayin - ne yaptigini anladim saniyorum
% Hocam, 1-10 strength'leri 0.5 thr ile binary label'lari buldum -> 
% evet ama bin. classification demistin oradan kafam karisti 
%We then trained a binary classifier yazdim yukarida 
% Bir de classifier da egittim (Tablodaki ilk satir) ama asil deneylerde regresyon yaptim 1-10 ile (
%benim cikmam lazim - tamam yukarida yazdiklarim dogru ise daha anlatmayin ltf - anlasildi bence
% lutfen yollandigindan emin olun tmm
% simdi daha iyi oldu sanki?
% hayir ya - strength aldin, ondan binary labela gectin (yukarida yazili)
% sonra da o labellar ile train ettin - benim yazdigi dogru - ayrica final score vs net degil

% Hocam o classification model'i (tablodaki ilk row), regresyon modellerinde normal regresyon egittim 1-10 ile, test kisminda skorlari bulduktan sonra threshold ile ne yaptin???... ??????
% ee tabi ki Umut - son haline bak ltf. regresyon ilk paragraf, classif. da karisiklik

% oyyy - bu kadar karmasik anlatilir - neyse - nasil anlatiyorsan - netlestir ama

% yani regresyon mse ile ve labellar elimizde
% classifi. da binarized strength ile - ve ayni labellara mi karsilastiriyorsun - orijinal labellara karsilastiriyorsun galiba..... sorun orada - neyse, makul sekilde coz ve yolla 
% Ayni label'lar

% bence burada classifier'dan hic bahsetmeyelim Hocam, asagida sonuclarda daha ayrintili anlatiyoruz zaten
% Tamam Hocam - burada anlatmamak ok ama asagida sonuclarda ne label, classification nasil yapildi VE nasil degerlendirildi - net degil!

% Tamam Hocam netlestirip yazacagim
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[]
\centering
\small%\scriptsize
\caption{Results of hate speech classification model (BERTurk) for different strategies to handle disagreement among annotators on 2-class, 4-class, and 6-class problems. Bold and underlined results indicate the best and second-best results in each experiment.}
\label{tab:classification-result}
\begin{tabular}{|c|l|cccccc|}
\hline
\multirow{3}{*}{Experiment} & \multicolumn{1}{c|}{\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Majority voting strategy\\ (multi-label annotation)\end{tabular}}} & \multicolumn{6}{c|}{Classification Problem}           \\ \cline{3-8} 
 & \multicolumn{1}{c|}{}    & \multicolumn{2}{c|}{6-class}     & \multicolumn{2}{c|}{4-class}     & \multicolumn{2}{c|}{2-class} \\ \cline{3-8} 
& \multicolumn{1}{c|}{}   & M-F1 & \multicolumn{1}{c|}{Acc.} & M-F1 & \multicolumn{1}{c|}{Acc.} & M-F1  & Acc.         \\ \hline
E1    & \multicolumn{1}{c|}{} 
& \textbf{70.70}    & \multicolumn{1}{c|}{\textbf{97.67}}    & \textbf{84.37}    & \multicolumn{1}{c|}{\textbf{97.08}}    & \textbf{91.10}       & \textbf{94.50}            \\ \hline
\multirow{2}{*}{E2}         
& Simple  
& 61.47    & \multicolumn{1}{c|}{79.57}    & 73.36    & \multicolumn{1}{c|}{80.30}    & 83.94     & 83.97            \\
& Weighted  
& \textbf{62.08}    & \multicolumn{1}{c|}{77.81}    & 71.56    & \multicolumn{1}{c|}{78.79}    & \textbf{84.66}             & \textbf{84.76}            \\ \hline
\multirow{8}{*}{E3}         
& Max   
& \textbf{56.96}    & \multicolumn{1}{c|}{70.24}    
& \textbf{69.66}    & \multicolumn{1}{c|}{74.19}    
& 80.78             & 80.78            \\
& Weighted Max 
& 55.12    & \multicolumn{1}{c|}{70.19}    
& 68.13    & \multicolumn{1}{c|}{73.92}    
& 81.73             & 81.73           \\
%
& Min  
& \ul{56.29}    & \multicolumn{1}{c|}{\ul{75.32}}
& 66.97    & \multicolumn{1}{c|}{\ul{77.01}}    
& \textbf{82.57}      & \textbf{83.00}           \\
& Weighted Min      
& 51.90    & \multicolumn{1}{c|}{\textbf{75.46}}    
& \ul{68.52}    & \multicolumn{1}{c|}{\textbf{77.55}}    
& \ul{82.09}    & \ul{82.59}            \\
%
& Random 
& 55.97    & \multicolumn{1}{c|}{70.96}    
& 67.83    & \multicolumn{1}{c|}{74.42}    
& 80.83             & 80.87            \\
& Weighted Random     
& 52.56    & \multicolumn{1}{c|}{71.64}    
& 66.66    & \multicolumn{1}{c|}{74.55}    
& 81.34             & 81.59            \\
%
& Mean   
& 55.03    & \multicolumn{1}{c|}{70.14}    
& 66.38    & \multicolumn{1}{c|}{72.87}    
& \textbf{82.57}        & \textbf{83.00}            \\
& Weighted Mean    
& 54.14    & \multicolumn{1}{c|}{70.24}    
& 65.73    & \multicolumn{1}{c|}{72.96} 
& \ul{82.09}             & \ul{82.59}            \\ \hline
\end{tabular}
\vspace{12pt}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[]
\centering
\small %«scriptsize
\caption{Confusion matrices of best models for different scenarios in 2-class problem.}
\label{tab:conf-2class}
\begin{tabular}{cccccccccccccc}
&    & \multicolumn{2}{c}{Predicted Label}
&    &   &  & \multicolumn{2}{c}{Predicted Label} &      &                    &    & \multicolumn{2}{c}{Predicted Label}   \\
&     & 0       & 1    &      &      &    & 0     & 1    &   &                &     & 0       & 1     \\ \cline{3-4} \cline{8-9} \cline{13-14} 
\multirow{2}{*}{\rotatebox[origin=c]{90}{True Label}} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{555}   & \multicolumn{1}{c|}{14}                    &   & \multirow{2}{*}{\rotatebox[origin=c]{90}{True Label}} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{925}    & \multicolumn{1}{c|}{154}              &                      & \multirow{2}{*}{\rotatebox[origin=c]{90}{True Label}} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{1086}               & \multicolumn{1}{c|}{143}               \\ \cline{3-4} \cline{8-9} \cline{13-14} 
& \multicolumn{1}{c|}{1} & \multicolumn{1}{c|}{25}                    & \multicolumn{1}{c|}{116}                   &                      &           & \multicolumn{1}{c|}{1} & \multicolumn{1}{c|}{159}                            & \multicolumn{1}{c|}{789}                             &                      &     & \multicolumn{1}{c|}{1} & \multicolumn{1}{c|}{231}                & \multicolumn{1}{c|}{741}               \\ \cline{3-4} \cline{8-9} \cline{13-14} 
&     &     &     &     &       &     &    &    &  &  &  &   &  \\
\multicolumn{1}{l}{}        & \multicolumn{1}{l}{}   & \multicolumn{1}{l}{}                       & \multicolumn{1}{l}{}                       & \multicolumn{1}{l}{} & \multicolumn{1}{l}{}        & \multicolumn{1}{l}{}   & \multicolumn{1}{l}{}                                  & \multicolumn{1}{l}{}                                 & \multicolumn{1}{l}{} & \multicolumn{1}{l}{}        & \multicolumn{1}{l}{}   & \multicolumn{1}{l}{}                    & \multicolumn{1}{l}{}                   \\
 &                        & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}Experiment 1\\ (Agreements)\end{tabular}} &                      &                             &                        & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}Experiment 2\\ (Agreements + \\ Majority is clear)\end{tabular}} &                      &                             &                        & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}Experiment 3\\ (All)\end{tabular}}
\end{tabular}
\vspace{6pt}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[]
\centering
\small
\caption{Confusion matrices of best models for different scenarios in 4-class problem.}
\label{tab:conf-4class}
\begin{tabular}{cccccccccccccccccccc}
&    & \multicolumn{4}{c}{Predicted Label}                                    &    &    &    & \multicolumn{4}{c}{Predicted Label}  
&    &    &    & \multicolumn{4}{c}{Predicted Label}      \\
&    & 0   & 1   & 2      & 3     &      &     & 
& 0  & 1   & 2   & 3    &   &     &      & 0    & 1    & 2    & 3  
\\ \cline{3-6} \cline{10-13} \cline{17-20} 
\multirow{4}{*}{\rotatebox[origin=c]{90}{True Label}} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{546} & \multicolumn{1}{c|}{0}  & \multicolumn{1}{c|}{1} & \multicolumn{1}{c|}{4}  &                      & \multirow{4}{*}{\rotatebox[origin=c]{90}{True Label}} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{1030}  & \multicolumn{1}{c|}{15}   & \multicolumn{1}{c|}{27}   & \multicolumn{1}{c|}{50}  &                      & \multirow{4}{*}{\rotatebox[origin=c]{90}{True Label}} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{1199} & \multicolumn{1}{c|}{29}  & \multicolumn{1}{c|}{44}  & \multicolumn{1}{c|}{55}  \\ \cline{3-6} \cline{10-13} \cline{17-20} 
& \multicolumn{1}{c|}{1} & \multicolumn{1}{c|}{0}   & \multicolumn{1}{c|}{44} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0}  &                      &                             & \multicolumn{1}{c|}{1} & \multicolumn{1}{c|}{14}    & \multicolumn{1}{c|}{176}  & \multicolumn{1}{c|}{3}    & \multicolumn{1}{c|}{13}  &                      &                             & \multicolumn{1}{c|}{1} & \multicolumn{1}{c|}{34}   & \multicolumn{1}{c|}{205} & \multicolumn{1}{c|}{14}  & \multicolumn{1}{c|}{16}  \\ \cline{3-6} \cline{10-13} \cline{17-20} 
& \multicolumn{1}{c|}{2} & \multicolumn{1}{c|}{6}   & \multicolumn{1}{c|}{0}  & \multicolumn{1}{c|}{6} & \multicolumn{1}{c|}{0}  &                      &                             & \multicolumn{1}{c|}{2} & \multicolumn{1}{c|}{94}    & \multicolumn{1}{c|}{7}    & \multicolumn{1}{c|}{118}  & \multicolumn{1}{c|}{24}  &                      &                             & \multicolumn{1}{c|}{3} & \multicolumn{1}{c|}{115}  & \multicolumn{1}{c|}{11}  & \multicolumn{1}{c|}{127} & \multicolumn{1}{c|}{29}  \\ \cline{3-6} \cline{10-13} \cline{17-20} 
& \multicolumn{1}{c|}{3} & \multicolumn{1}{c|}{8}   & \multicolumn{1}{c|}{0}  & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{19} &                      &                             & \multicolumn{1}{c|}{3} & \multicolumn{1}{c|}{78}    & \multicolumn{1}{c|}{5}    & \multicolumn{1}{c|}{43}   & \multicolumn{1}{c|}{197} &                      &                             & \multicolumn{1}{c|}{4} & \multicolumn{1}{c|}{101}  & \multicolumn{1}{c|}{6}   & \multicolumn{1}{c|}{40}  & \multicolumn{1}{c|}{176} \\ \cline{3-6} \cline{10-13} \cline{17-20} 
&                        &                          &                         &                        &                         &                      &                             &                        &                            &                           &                           &                          &                      &                             &                        &                           &                          &                          &                          \\
\multicolumn{1}{l}{}        & \multicolumn{1}{l}{}   & \multicolumn{4}{c}{\begin{tabular}[c]{@{}c@{}}Experiment 1\\ (Agreements)\end{tabular}}               & \multicolumn{1}{l}{} & \multicolumn{1}{l}{}        & \multicolumn{1}{l}{}   & \multicolumn{4}{c}{\begin{tabular}[c]{@{}c@{}}Experiment 2\\ (Agreements + \\ Majority is clear)\end{tabular}} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{}        & \multicolumn{1}{l}{}   & \multicolumn{4}{c}{\begin{tabular}[c]{@{}c@{}}Experiment 3\\ (All)\end{tabular}}                           
\end{tabular}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Classification Results} \label{sec:result}

The results corresponding to the Experiments 1-4 are given in Tables 1-4.  
%
We first observe that our results are on par with state-of-art \citep{Arin2023, Gökçe2024} as discussed in Section \ref{sec:comparison}.
%with 83\% accuracy and 84.66 macro F1 score in the two-class case. 
%with dedigin kimin souclar i belli degil
%For the 4- and 6-class problems the performance drops as expected, but they are also on par with the literature.
%
When we consider what happens with respect to different annotator disagreement handling strategies, we first see that there is a  significant performance drop between Experiment 1 and 2. For the two-class case, macro F1 score drops from 91.10\% to 83.94\%, as shown in Table \ref{tab:classification-result}, with corresponding confusion matrices shown in Table \ref{tab:conf-2class}.
This performance drop highlights the fact that taking only the data samples for which there is agreement among annotators eliminates more challenging samples and result in higher performance.  
%\newpage

When we consider Experiment 3, dealing with all of the data with multiple annotators and labels, we see that the overall best strategy appears to be taking the minimum label when tie-breaking,  across the three classification tasks with 6-, 4- and 2- classes.
%
However, there isn't a clearly winning strategy with respect to the weighted versus unweighted approaches in Table \ref{tab:classification-result}. We observed that the two approaches obtained similar results, with unweighted approach performing slightly better. 

%\newpage
When considering confusion matrices in  Table \ref{tab:conf-4class}, we see that the classification model works quite well, and that the problem gets more and more complex when including more data with disagreements.
Additionally, the model has successfully classified all 44 instances of discriminatory speech (class-one) in the case where only tweets with fully agreeing labels were used (See Experiment 1 in Table \ref{tab:conf-4class}); however this is not the case in the more realistic scenarios, highlighting the challenge introduced by more ambiguous cases, regarding this class. On the other hand, the model failed to identify all other classes of hate speech (i.e., symbolization/exaggeration, swearing/threat) in Experiment 1. 
%(where all samples had agreement). 
%This further points to the fact that distinguishing these types of hate speech is challenging for both the model and the annotators.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Regression and Classification Results Using Hate Speech Strength } \label{sec:regresult}
The results of the fourth experiment using the 0-to-10 strength labels are presented in Table \ref{tab:regression}.  

In the regression experiments, we first trained the model using the mean of the annotated strength scores, which range from 0 to 10, with the Mean Squared Error (MSE) loss function. Then, to obtain the predicted labels, we applied the optimal threshold determined on the validation set, which is the threshold that maximizes accuracy on that set.
%
We evaluated the two-class classification with the binary class labels obtained by converting the mean strength scores using a threshold of 0.5, as described in Section \ref{sec:experiment}. We also trained a classification-based model using the Cross Entropy Loss function using these binary class labels.

We first observe that the regression-based model outperforms the classification-based model (79.42 vs 78.33). This finding suggests that the strength labels contain valuable information that enhances the model's ability to distinguish between hate and non-hate speech more effectively.

Another observation is that considering only the samples where there is agreement  results in a very high performance (89.80\% accuracy) in the regression approach. 
However, when tested with the  more realistic scenario of  unfiltered test data, the model  performs worse in comparison to the one trained with all data (77.88\% vs 78.44\% F1 score). This result suggests not filtering out any data.% in search of annotator agreement samples.
%highlights the overly optimistic results obtained when considering only the data for which there is agreement.

Finally, we obtained an ensemble model of the classification and regression-based models by combining the scores:
\begin{equation}
    S=\alpha S_{r} + (1-\alpha) S_{c}
\end{equation}
where $S_r$ and $S_c$ are the scores of regression and classification models respectively. 
The value for the parameter $\alpha$  is determined on a validation set and is set to
$\alpha=0.93$. Note that we normalized the scores to be in the range $[0, 1]$ before the ensemble. 

\begin{table}[h]
    \centering
\caption{Regression and classification using the hate speech strength on a 0-10 scale. }
    \small% scriptsize
    \begin{tabular}{|c|c|c|c|c|c|}
    \hline
    Modelling-type & Train-data & Test-data & MSE & M-F1 (\%) & Acc. (\%) \\
    \hline
    \multirow{3}{*}{Regression} & Agreements & Agreements & 2.71 & 89.57 & 89.80\\
     & Agreements & All & 3.69 & 77.88 & 78.91\\
    & All & All & 2.55 & {78.44} & {79.42}\\
    \hline
        Classification & All & All & - & 77.07  & 78.33  \\
    \hline
    Ensemble & All & All & - & \textbf{79.00} & \textbf{79.96} \\
    \hline
    \end{tabular}

    \label{tab:regression}
\end{table}

The fact that the ensemble performs better than both the regression and the classification models (Table \ref{tab:regression} last row) supports that there is complementary information in these models that can be leveraged by an ensemble.

%\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Comparison to Literature} \label{sec:comparison}

We have compiled various literature results on hate speech classification in Turkish, as shown in Table \ref{tab:comparison}. 
While our performance results are not directly comparable to any results in the literature, 
two competitions on the topic used earlier versions of this dataset. 
The winner of the  SIU2023-NST competition \citep{Arin2023} obtained 76.87\% F1 score on the 2-class classification for a subset of the Refugee dataset. 
%
Similarly, in the HSD-2Lang competition, \citep{Gökçe2024} achieved a macro F1 score of 69.64\%, while \citep{Dehghan2024b} attained a macro F1 score of 79.49\%, both evaluated on a combined test set of three topics (Anti-Refugees, Israel-Palestine conflict, and Anti Greek sentiment in Turkey).
Our model outperformed these results, achieving a macro F1 score of 82.57\% on the extended version of the same combined test set, which also included the LGBTQ+ topic, demonstrating superior performance in hate speech classification across all four topics.

%BY Buralarda aslinda Arin et al daki sistemleri cite etmek lazim, ama neyse simdilik

\begin{table}[!th]
\centering
\small
\caption{Literature results for hate speech classification in Turkish.}
\label{tab:comparison}
\begin{tabular}{|l|cccccccc|}
\hline
\multicolumn{1}{|c|}{\multirow{3}{*}{Model}} & \multicolumn{8}{c|}{Classification Problem}                                                                                                 \\ \cline{2-9} 
\multicolumn{1}{|c|}{}                       & \multicolumn{2}{c|}{2-class}       & \multicolumn{2}{c|}{4-class}       & \multicolumn{2}{c|}{5-class}       & \multicolumn{2}{c|}{6-class} \\ \cline{2-9} 
\multicolumn{1}{|c|}{}                       & M-F1  & \multicolumn{1}{c|}{Acc.}  & M-F1  & \multicolumn{1}{c|}{Acc.}  & M-F1  & \multicolumn{1}{c|}{Acc.}  & M-F1          & Acc.         \\ \hline
\citep{Beyhan2022}                          & 65.54 & \multicolumn{1}{c|}{71.06} & -     & \multicolumn{1}{c|}{-}     & 60.98 & \multicolumn{1}{c|}{71.74} & -             & -            \\ \hline
\citep{Arin2023}      & 76.87 & \multicolumn{1}{c|}{-} & -     & \multicolumn{1}{c|}{-}     & 57.58 & \multicolumn{1}{c|}{-}     & -             & -            \\ \hline
\citep{Gökçe2024}       & 69.64 & \multicolumn{1}{c|}{-}     & -     & \multicolumn{1}{c|}{-}     & -     & \multicolumn{1}{c|}{-}     & -             & -            \\ \hline
\citep{Dehghan2024b}                        & 79.49 & \multicolumn{1}{c|}{83.81} & -     & \multicolumn{1}{c|}{-}     & 54.99 & \multicolumn{1}{c|}{80.56} & -             & -            \\ \hline
Ours (this study)                            & \textbf{82.57} & \multicolumn{1}{c|}{83.00} & 68.52 & \multicolumn{1}{c|}{77.55} & -     & \multicolumn{1}{c|}{-}     & 56.29         & 75.32        \\ \hline
\end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion and Future Work} \label{sec:conclusion}

Disagreements in hate speech annotation are often overlooked in the literature. Our work aims to highlight the problem and describe and evaluate alternatives to find the final label, even in  in the case of multi-label multi-annotator labelling process. 

%Ilk sey agreement in cok yuksek oldugu ama realistic olmaldigi. 
Our results show that when we consider only samples with agreeing annotations, the problem is much simplified, as evidenced by high accuracy (94.50\%) compared to more realistic scenarios.
When we include all samples with a clear majority label, most of the data is kept and the result is 84.76\%.
Finally, when all samples are kept, the annotator disagreement management becomes important, as results range from 80.78\% to 83.00\%, with the best method (taking the minimum label in case of tie) achieving 83\%.
%and the performance drop to 83\%
%Our model achieved an accuracy of 84.76\% when using only the samples with agreement annotations and samples with a clear majority, and 83.0\% when 
%The slight decrease in accuracy indicates that addressing disagreements is feasible, and it is better not to ignore them so easily.

%Bu 2. point olamaz - onemsiz birsey - tek satırda gecistirin sayı vermeden
%As we realizes that the samples with disagreement or clear majority label, are about 1,100-1,800. Therefore, we suggest re-annotating these samples through a meta-reviewer process, as this represents a relatively small portion (10-16\%) of the dataset. This approach will help ensure consistency and accuracy in the annotations, ultimately enhancing the reliability of the hate speech detection model.

%BY Daha iyi yazılmalı - "results show that ...." cok uzamıs 
%Additionally, the results show that the accuracy of regression on hate strength is 78.33\%, while the accuracy of classification on categories is 83.0\%. This suggests that classification performs better, likely because the guideline provided annotators with more detailed explanations and examples for each class. In contrast, for strength, the guideline only asked annotators to rate the hatefulness of the text on a scale from 1 to 10, without as much detailed guidance. Therefore, it can be concluded that by incorporating a clear definition of hate speech, along with detailed categories and examples in the annotation guideline, a richer dataset with fewer disagreements can be achieved. 

Additionally, the results indicate that the accuracy of regression on hate strength is 78.33\%, while the accuracy of classification on categories is 83.0\%. This suggests that classification on categories performs better, likely because the guideline provided annotators with more detailed explanations and examples for each class. In contrast, for hate strength, the guideline only asked annotators to rate the text's hatefulness on a scale from 1 to 10, without much  guidance. Therefore, it can be concluded that incorporating a clear definition of hate speech, along with detailed categories and examples in the annotation guidelines, can lead to a richer dataset with fewer disagreements. On the other hand, it is also interesting to see that the annotator perception about the hate speech strength can be used to train a pretty successful model. 

We also realized that the samples with disagreement constitute a relatively small portion (10-13\%) of the dataset. Therefore, as a straightforward approach, re-annotating these samples through a meta-reviewer process can be an option and will help ensure consistency and accuracy in the annotations, ultimately enhancing the reliability of the hate speech detection model.

No matter how detailed and precise the guideline is, subjectivity cannot be entirely avoided \citep{Sang2022, Wan2023}. For instance, as mentioned in \citep{Fleisig2023}, the phrase "A woman's place is in the kitchen",  is considered as hateful by some women, while some men or women do \textit{not} consider it as hate speech. As it is impossible to include all such cases in the guidelines, we believe that annotator disagreements will always exist and need to be handled clearly by researchers.

% personal outlook (optimistic or pessimistic)

%Good
As future work, it would be valuable to explore the impact of annotators’ gender, demographic backgrounds, ethnicity, education level, personal outlook (optimistic or pessimistic), and personal biases on the quality of hate speech annotations. By examining how these factors influence the perception and labeling of hate speech, researchers could develop more comprehensive annotation guidelines and methods that account for these variations. This could lead to even more reliable datasets, ultimately enhancing the accuracy of hate speech detection models.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\newpage
\section*{Acknowledgments}
This work was supported by the EU project “Utilizing Digital Technology for Social Cohesion, Positive Messaging and Peace by Boosting Collaboration, Exchange and Solidarity" (EuropeAid/170389/DD/ACT/Multi), carried out by the Hrant Dink Foundation.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{thebibliography}{}
\input{references.bbl}
\end{thebibliography}

\end{document}

\begin{thebibliography}{}

\bibitem[Arın et al., 2023]{Arin2023}
\textbf{Arın I., Işık Z., Kutal S., Dehghan S., Özgür A., and Yanıkoğlu B.} (2023). SIU2023-NST - Hate Speech Detection Contest. 31. IEEE Conference on Signal Processing and Communications Applications, Istanbul.

\bibitem[Assimakopoulos et al., 2020]{Assimakopoulos2020}
\textbf{Assimakopoulos S., Muskat R. V., Plas L. V., and Gatt A.} (2020). Annotating for Hate Speech: The MaNeCo Corpus and Some Input from Critical Discourse Analysis. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 5088–5097, Marseille, France. European Language Resources Association.

\bibitem[Beyhan et al., 2022]{Beyhan2022}
\textbf{Beyhan F., Çarık B., Arın I., Terzioğlu A., Yanıkoğlu B., and Yeniterzi R.} (2022). A Turkish Hate Speech Dataset and Detection System. In Proceedings of the 13th Conference on Language Resources and Evaluation (LREC 2022), pp. 4177–4185. 
%\url{https://aclanthology.org/2022.lrec-1.443.pdf}

\bibitem[Braun, 2024]{Braun2024}
\textbf{Braun D.} (2024). I beg to differ: how disagreement is handled in the annotation of legal machine learning data sets. Artif Intell Law 32, 839–862. %\url{https://doi.org/10.1007/s10506-023-09369-4}

\bibitem[Campagner et al., 2021]{Campagner2021}
\textbf{Campagner A., Ciucci D., and Svensson CM.} (2021). Ground truthing from multi-rater labeling with threeway decision and possibility theory. Inf Sci 545:771–790. %\url{https://doi.org/10.1016/j.ins.2020.09.049}

\bibitem[Das et al., 2024]{Das2024}
\textbf{Das A., Zhang Z., Jamshidi F., Jain V., Chadha A.,  Raychawdhary N., Sandage M., Pope L., Dozier G., and Seals C.} (2024). Investigating Annotator Bias in Large Language Models for Hate Speech Detection, arXiv:2406.11109v2

%\bibitem[\protect\citeauthoryear{Davani}{Davani}{2021}]{Davani2021}
\bibitem[Davani et al, 2021]{Davani2021}
\textbf{Davani A. M., Atari M., Kennedy  B., and Dehghani M.} (2021). Hate speech classifiers learn human-like social stereotypes. 	arXiv:2110.14839.

\bibitem[Dehghan and Yanikoglu, 2024a]{Dehghan2024a}
\textbf{Dehghan S. and Yanikoglu B.} (2024a). Evaluating ChatGPT’s Ability to Detect Hate Speech in Turkish Tweets. In Proceedings of the 7th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE 2024), pages 54–59, St. Julians, Malta. Association for Computational Linguistics.

\bibitem[Dehghan and Yanikoglu, 2024b]{Dehghan2024b}
\textbf{Dehghan S. and Yanikoglu B.} (2024b). Multi-domain Hate Speech Detection Using Dual Contrastive Learning and Paralinguistic Features. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 11745–11755, Torino, Italia. ELRA and ICCL.

\bibitem[Devlin et al., 2019]{Devlin2019}
\textbf{Devlin J., Chang M.W., Lee K., and Toutanova K.} (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume~1, pp. 4171-4186. %\url{https://doi.org/10.18653/v1/N19-1423}.

\bibitem[Fleisig et al., 2023]{Fleisig2023}
\textbf{Fleisig E., Abebe R., and Klein D.} (2023). When the Majority is Wrong: Modeling Annotator Disagreement for Subjective Tasks. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 6715–6726, Singapore. Association for Computational Linguistics.

\bibitem[Jamison and Gurevych, 2015]{Jamison2015}
\textbf{Jamison E. and Gurevych, I.} (2015). Noise or additional information? leveraging crowdsource annotation item agreement for natural language tasks. In: Proceedings of the 2015 conference on empirical methods in natural language processing. Association for Computational Linguistics, Lisbon, Portugal, pp 291–297. %\url{https://doi.org/10.18653/v1/D15-1035}

\bibitem[Krenn et al., 2024]{Krenn2024}
\textbf{Krenn B., Petrak J.,  Kubina M., and Burger C.} (2024). GERMS-AT: A Sexism/Misogyny Dataset of Forum Comments from an Austrian Online Newspaper. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 7728–7739, Torino, Italia. ELRA and ICCL.

\bibitem[Kocon et al., 2021]{Kocon2021}
\textbf{Kocoń J., Figas A.,  Gruza M., Puchalska D.,  Kajdanowicz T., and Kazienko P.} (2021). Offensive, aggressive, and hate speech analysis: From data-centric to human-centered approach, Information Processing and Management, Volume 58, Issue 5, 102643, ISSN 0306-4573.
%\url{https://doi.org/10.1016/j.ipm.2021.102643}.

\bibitem[Lindahl, 2024]{Lindahl2024}
\textbf{Lindahl A.} (2024). Disagreement in Argumentation Annotation. In Proceedings of the 3rd Workshop on Perspectivist Approaches to NLP (NLPerspectives) @ LREC-COLING 2024, pages 56–66, Torino, Italia. ELRA and ICCL.

\bibitem[Ljubešić et al., 2023]{Ljubesic2023}
\textbf{Ljubešić N, Mozetič I, and Novak P. K.} (2023). Quantifying the impact of context on the quality of manual hate speech annotation. Natural Language Engineering. 2023;29(6):1481-1494. %doi:10.1017/S1351324922000353

\bibitem[Meedin et al., 2022]{Meedin2022}
\textbf{Meedin N., Caldera M., Perera S., and Perera I.} (2022). A Novel Annotation Scheme to Generate Hate Speech Corpus through Crowdsourcing and Active Learning. (IJACSA) International Journal of Advanced Computer Science and Applications, Vol. 13, No. 11.


\bibitem[Novak et al., 2022]{Novak2022}
\textbf{Novak P. K. , Scantamburlo T., Pelicon A., Cinelli M., Mozetič I., and Zollo F.} (2022). Handling Disagreement in Hate Speech Modelling. In: Ciucci, D., et al. Information Processing and Management of Uncertainty in Knowledge-Based Systems. IPMU 2022. Communications in Computer and Information Science, vol 1602. Springer, Cham. 
%\url{https://doi.org/10.1007/978-3-031-08974-9\_54}

\bibitem[Poletto et a., 2019]{Poletto2019}
\textbf{Poletto F.,  Basile V., Bosco C., Patti V., and Stranisci M. A.} (2019). Annotating Hate Speech: Three Schemes at Comparison, Italian Conference on Computational Linguistics.
%url={https://api.semanticscholar.org/CorpusID:204901524}


\bibitem[Prabhakaran et al., 2021]{Prabhakaran2021}
\textbf{Prabhakaran V., Mostafazadeh Davani A., and Diaz M.} (2021). On releasing annotator-level labels and information in datasets. In: Proceedings of the Joint 15th linguistic annotation workshop (LAW) and 3rd designing meaning representations (DMR) workshop. Association for Computational Linguistics, Punta Cana, Dominican Republic, pp 133–138. %\url{https://doi.org/10.18653/v1/2021.law-1.14}

\bibitem[Ron et al., 2023]{Ron2023}
\textbf{Ron G.,  Levi E., Oshri O., and Shenhav S.} (2023). Factoring Hate Speech: A New Annotation Framework to Study Hate Speech in Social Media. In The 7th Workshop on Online Abuse and Harms (WOAH), pages 215–220, Toronto, Canada. Association for Computational Linguistics.

\bibitem[Rottger et al., 2022]{Rottger2022}
\textbf{Rottger P.,  Vidgen B., Drik H., and Pierrehumbert J.} (2022). Two Contrasting Data Annotation Paradigms for Subjective NLP Tasks. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 175–190, Seattle, United States. Association for Computational Linguistics.

\bibitem[Salminen et al., 2019]{Salminen2019}
\textbf{Salminen J.,  Almerekhi H., Kamel A. M., Jung S-G., and Jansen B. J.} (2019). Online hate ratings vary by extremes: A statistical analysis. In Proceedings of the 2019 Conference on Human Information Interaction and Retrieval, CHIIR ’19, page 213–217, New York, NY, USA. Association for Computing Machinery.

\bibitem[Sang and Stanton, 2022]{Sang2022}
\textbf{Sang Y. and Stanton J.} (2022). The Origin and Value of Disagreement Among Data Labelers: A Case Study of Individual Differences in Hate Speech Annotation. In: Smits, M. (eds) Information for a Better World: Shaping the Global Future. iConference 2022. Lecture Notes in Computer Science(), vol 13192. Springer, Cham. %https://doi.org/10.1007/978-3-030-96957-8_36

\bibitem[Schmidt and Wiegand, 2017]{Schmidt2017}
\textbf{Schmidt A. and Wiegand M.} (2017). A survey on hate speech detection using natural language processing. In Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media, pages 1–10, Valencia. Association for Computational Linguistics.

\bibitem[Seemann et al., 2023]{Seemann2023}
\textbf{Seemann N., Lee YS., Höllig J., and Geierhos M.} (2023). The problem of varying annotations to identify abusive language in social media content. Natural Language Engineering. 2023;29(6):1561-1585. %doi:10.1017/S1351324923000098

\bibitem[Sudre et al., 2019]{Sudre2019}
\textbf{Sudre CH., Anson BG., and Ingala S.} (2019). Let’s agree to disagree: learning highly debatable multirater labelling. In: Shen D, Liu T, Peters TM et al. (eds) Medical image computing and computer assisted intervention—MICCAI 2019. Springer, Cham, pp 665–673.

\bibitem[Talat, 2016]{Talat2016}
\textbf{Talat Z.} (2016). Are you a racist or am I seeing things? Annotator influence on hate speech detection on Twitter. In Proceedings of the First Workshop on NLP and Computational Social Science, pages 138–142, Austin, Texas. Association for Computational Linguistics.

\bibitem[Uludoğan et al., 2024]{Gökçe2024}
\textbf{Uludoğan G., Dehghan S., Arin I., Erol E., Yanikoglu B., and Özgür A.} (2024). Overview of the Hate Speech Detection in Turkish and Arabic Tweets (HSD-2Lang) Shared Task at CASE 2024. In Proceedings of the 7th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE 2024), pages 229–233, St. Julians, Malta. Association for Computational Linguistics.

\bibitem[Wan et al, 2023]{Wan2023}
\textbf{Wan R., Kim J., and Kang D.} (2023). Everyone's Voice Matters: Quantifying Annotation Disagreement Using Demographic Information. arXiv:2301.05036.

\bibitem[Wich et al., 2021]{Wich2021}
\textbf{Wich M., Widmer C., Hagerer G., and Groh G.} (2021). Investigating Annotator Bias in Abusive Language Datasets. Proceedings of Recent Advances in Natural Language Processing, pages 1515–1525, Sep 1–3. %https://doi.org/10.26615/978-954-452-072-4_170


\end{thebibliography}


%\bibliographystyle{unsrtnat}
%\bibliography{references}  %%% Uncomment this line and comment out the ``thebibliography'' section below to use the external .bib file (using bibtex) .


%%% Uncomment this section and comment out the \bibliography{references} line above to use inline references.
% \begin{thebibliography}{1}

% 	\bibitem{kour2014real}
% 	George Kour and Raid Saabne.
% 	\newblock Real-time segmentation of on-line handwritten arabic script.
% 	\newblock In {\em Frontiers in Handwriting Recognition (ICFHR), 2014 14th
% 			International Conference on}, pages 417--422. IEEE, 2014.

% 	\bibitem{kour2014fast}
% 	George Kour and Raid Saabne.
% 	\newblock Fast classification of handwritten on-line arabic characters.
% 	\newblock In {\em Soft Computing and Pattern Recognition (SoCPaR), 2014 6th
% 			International Conference of}, pages 312--318. IEEE, 2014.

% 	\bibitem{hadash2018estimate}
% 	Guy Hadash, Einat Kermany, Boaz Carmeli, Ofer Lavi, George Kour, and Alon
% 	Jacovi.
% 	\newblock Estimate and replace: A novel approach to integrating deep neural
% 	networks with existing applications.
% 	\newblock {\em arXiv preprint arXiv:1804.09028}, 2018.

% \end{thebibliography}



