\section{Related work}
\noindent \textbf{Parameter Estimation in Markov Random Fields.}  A large body of work has focused on parameter estimation under the one-shot learning paradigm (see, e.g., \cite{Chatterjee07c, BM18, DDK19, DDP20, GM20, DDDVK21, MSB}), particularly for Ising-like models in statistical physics and for dependent regression models in statistics. In this work, we follow a similar approach by establishing the consistency of the maximum pseudo-likelihood estimator.  Earlier studies (e.g., \cite{Gidas88, CometsGidas91, Comets92, GeyerThompson92}) have also explored parameter estimation in Markov random fields using the maximum likelihood estimator.

Before our work, the papers~\cite{BR21,GKK24} were the first to study one-shot learning in hard-constrained models. In particular, the hardcore model analysed in \cite{BR21} can be viewed as a weighted monotone $2$-SAT model, and one natural extension of the hardcore model to $k$-uniform hypergraphs corresponds to the class of weighted monotone $k$-SAT models—a special case of the weighted $k$-SAT models that we consider. Because a typical assignment in these monotone formulas possesses $\Omega(n)$ flippable variables, the pseudo-likelihood estimator remains consistent across all parameter regimes, and no phase transition is expected. 
The weighted $k$-SAT problem was analysed in \cite{GKK24}, where the authors derived both a consistency condition and an impossibility condition, though a substantial gap remained between them. By tightening the bounds on both ends, our work considerably narrows this gap, nearly closing it entirely.

\noindent\textbf{Related Works in Structural Learning/Testing.}
An alternative direction in learning Markov Random Fields involves estimating the interaction matrix between variables—a question originally posed by \cite{ChowLiu68}. For the Ising model, this problem has been extensively studied (see, e.g., \cite{Bresler15, VMLC16, BGS17} and the references therein), and subsequent work has extended the results to higher-order models \cite{KilvansMeka17, HKM17, GMM24}. Recent work \cite{ZhangKKW20, DDDVK21, GM24} has also considered the joint learning of both structure and parameters. Moreover, \cite{SanthanamWainwright12} establishes the information-theoretic limits on what any learner can achieve, and similar analyses have been conducted for hardcore models \cite{BGS14a, BGS14b}.
While some approaches in this line of work require multiple independent samples, as noted in \cite{DDDVK21}, it is also possible to reduce learning with $O(1)$ samples to a class of special cases within one-shot learning. 
Related problems in one-shot testing for Markov random fields have also been studied in \cite{BreslerNagaraj18,DDK18,MukherjeeMukherjeeYuan18,BBCSV20,BCSV21}.