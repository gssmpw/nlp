% WACV 2025 Paper Template
% based on the WACV 2024 template, which is
% based on the CVPR 2023 template (https://media.icml.cc/Conferences/CVPR2023/cvpr2023-author_kit-v1_1-1.zip) with 2-track changes from the WACV 2023 template (https://github.com/wacv-pcs/WACV-2023-Author-Kit)
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt, letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review,algorithms]{wacv}      % To produce the REVIEW version for the algorithms track
%\usepackage[review,applications]{wacv}      % To produce the REVIEW version for the applications track
%\usepackage{wacv}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{wacv} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\wacvPaperID{} % *** Enter the WACV Paper ID here
\def\confName{WACV}
\def\confYear{2025}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Hardware-Friendly Static Quantization Method for Video Diffusion Transformers}

\author{Sanghyun Yi\\
California Institute of Technology\\
Pasadena, CA\\
{\tt\small syi@caltech.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Qingfeng Liu\\
Samsung Semiconductor Inc.\\
San Diego, CA\\
{\tt\small qf.liu@samsung.com}
\and
Mostafa El-Khamy\\
Samsung Semiconductor Inc.\\
San Diego, CA\\
{\tt\small mostafa.e@samsung.com}
}
\maketitle

\begin{abstract}
Diffusion Transformers for video generation have gained significant
research interest since the impressive performance of SORA. Efficient deployment of such generative-AI models on GPUs has been demonstrated with dynamic quantization. 
However, resource-constrained devices cannot support dynamic quantization, and need static quantization of the models for their efficient deployment on AI processors.
In this paper, we propose a novel method for the post-training quantization of OpenSora\cite{opensora}, a Video Diffusion Transformer, without relying on dynamic quantization techniques. Our approach employs static quantization, achieving video quality comparable to FP16 and dynamically quantized ViDiT-Q methods, as measured by CLIP, and VQA metrics. 
In particular, we utilize per-step calibration data to adequately provide a post-training statically quantized model for each time step, incorporating channel-wise quantization for weights and tensor-wise quantization for activations. 
By further applying the smooth-quantization technique, we can obtain high-quality video outputs with the statically quantized models. 
Extensive experimental results demonstrate that static quantization can be a viable alternative to dynamic quantization for video diffusion transformers, offering a more efficient approach without sacrificing performance.
\end{abstract}

%\begin{IEEEkeywords}
%video diffusion model, static quantization, diffusion transformer
%\end{IEEEkeywords}

\section{Introduction}
\label{sec:intro}
The use of generative AI for video creation, especially through diffusion techniques, has gained traction for various applications, ranging from content creation to complex simulation environments. As diffusion models become more common, enhancing their efficiency and reducing size for use in limited-resource settings becomes essential. One commonly employed approach for neural network compression is post-training quantization (PTQ). This method allows for reducing model size and improving inference speed without requiring model retraining, making it particularly attractive for large and compute-intensive models.
PTQ has been effective in optimizing large language models (LLMs) by lowering bitwidth while keeping performance of floating-point models \cite{gholami2021surveyquantizationmethodsefficient,zhu2024surveymodelcompressionlarge}. 

Despite such progress, quantization for video diffusion transformers (DiT) \cite{opensora} remains challenging due to their complexity. A recent work, Video Difussion Transformer Quantization (ViDiT-Q), suggests using dynamic quantization for these models \cite{zhao2024vidit}. However, this approach requires calculating quantization parameters online during inference, which complicates hardware-level optimization, such as those for NPUs and Mobile System on Chip (SoC) \cite{park2022multi}.

To address this challenge, we propose static quantization for DiT models, calculating quantization parameters only during a calibration phase for use in inference. Our method includes aggregated static quantization aggregating activation from all denoising steps to estimate a single set of quantization parameters that is used across denoising steps, and time-step-wise (TSW) static quantization estimating quantization parameters for each time step. Both approaches involves Channel-Wise (CW) quantization for weights, Tensor-Wise (TW) for activations, Aggregated or Time-step-wise Smooth Quantization (ASQ or TSQ).
Our static approaches match the performance of the original FP16 STDiT model (Spatial-Temporal DiT or OpenSora \cite{opensora}) and the dynamic quantization method from ViDiT-Q \cite{zhao2024vidit} in W8A8 quantization, with robust results under various precision levels.

For example, as shown in Figure~\ref{fig1}, our aggregated static quantization solution of using CW, TW and ASQ (*+ASQ) showed comparable visual quality with dynamic quantization and FP16 model. Our another proposal, TSW static quantization method using CW, TW and TSQ (*+TSQ+TSW) showed best alignment between prompt and generated videos.

\section{Related Works}
\label{sec:rw}
\textbf{PTQ in Transformers}
Numerous PTQ methods have been developed to meet the demands of transformer-based language models in industry \cite{zhao2023surveylargelanguagemodels}. For example, Activation-aware Weight Quantization (AWQ) quantizes weights based on the saliency of the corresponding activations and scales the weights per channel to minimize quantization errors \cite{lin2024awqactivationawareweightquantization}. Other methods also take into account the activation outliers in quantizing weights \cite{dettmers2023spqrsparsequantizedrepresentationnearlossless, lee2024owqoutlierawareweightquantization}.  Advancements in LLM weight quantization have even reached sub-2-bit quantization, utilizing ternary numbers for weights \cite{wang2023bitnetscaling1bittransformers, ma2024era1bitllmslarge}. Additionally, various methods have been proposed for PTQ of both weights and activations, with the primary challenge being the diverse distribution across activation channels. For instance, Smooth Quantization (SQ) smooths the weights and activation by scaling the channels, and Reorder-based Post-training Quantization (RPTQ) cluster the channels of weights and activation based on the similarity and quantizes per cluster \cite{xiao2024smoothquantaccurateefficientposttraining, yuan2023rptqreorderbasedposttrainingquantization}. 

A series of PTQ methods for Vision Transformer (ViT) was also suggested such as using mixed precision, attention weight order preserving quantization, and using multiple uniform quantization or log transformations to manage non-Gaussian activation distribution, \cite{liu2021posttraining, yuan2024ptq4vitposttrainingquantizationvision, lin2023fqvitposttrainingquantizationfully}.

\textbf{PTQ in Diffusion Models}
However, the recurrent nature of diffusion models, which rely on the output of previous steps to sample the input for subsequent denoising steps, have limited the direct application of PTQ methods from other domains to diffusion model quantization. A key obstacle is the significant variation in activation distribution across time steps \cite{shang2023posttrainingquantizationdiffusionmodels}. Early work in PTQ for diffusion models mitigated this issue by generating calibration sets from the denoising process, with a focus on sampling across time steps to preserve output quality \cite{shang2023posttrainingquantizationdiffusionmodels, li2023qdiffusionquantizingdiffusionmodels,liu2024enhanceddistributionalignmentposttraining}. Other approaches involved using a neural network module to determine the time intervals for quantization and predict quantization parameters per interval or having different precision for different time steps and progressive calibration \cite{so2023temporaldynamicquantizationdiffusion, he2023ptqdaccurateposttrainingquantization,tang2024posttrainingquantizationtexttoimagediffusion}. Another study suggested a method using SQ, mixed precision across layers and focusing on the final time step in calibration for quantizing latent diffusion models \cite{yang2023efficientquantizationstrategieslatent}.

\textbf{PTQ in Diffusion Transformers}
Recent advancements in video generation have increasingly adopted transformer models in denoising, such as Sora by OpenAI \cite{videoworldsimulators2024}. However, effective methods for quantizing Diffusion Transformer (DiT) models remain limited due to their unique characteristics compared to LLMs and CNN-based diffusion models. For example, unlike LLMs, DiT models experience significant variation across tokens, making the direct application of LLM quantization methods challenging \cite{zhu2024surveymodelcompressionlarge, zhao2024vidit}. Additionally, the activation distribution in DiT models varies significantly across time steps and between forward paths with or without conditional information from prompts \cite{zhao2024vidit}. The ViDiT-Q framework introduced techniques for quantizing DiT models, utilizing both path-wise dynamic quantization based on prompts and token-wise dynamic quantization for activations \cite{zhao2024vidit}. Despite its advantages, the framework requires real-time calculation of quantization parameters during inference, which complicates hardware optimizations, particularly for NPUs. Our methods avoid this problem by using static quantization while achieving the performance of the original model.

%To address this challenge, we propose a method to statically quantize DiT models by calculating quantization parameters during a calibration phase and using these fixed parameters during inference. Our method involves channel-wise quantization for weights, tensor-wise quantization for activations, smooth quantization, and estimating quantization parameters for each time step. Using this static quantization approach, we achieved performance comparable to the original FP16 model and the dynamic quantization method from ViDiT-Q \cite{zhao2024vidit} in W8A8 quantization for the STDiT (OpenSora \cite{opensora}) model.

\begin{figure}[t]
	\centering
	% \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
	\includegraphics[width=0.7\linewidth]{figures/1.png}
	\caption{Example videos from various quantization methods
	}
	\label{fig1}
\end{figure}

\begin{figure*}[t]
	\centering
	% \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
	\includegraphics[width=1\linewidth]{figures/video_consecutive_frames_visualization.png}
	\caption{Example frames from generated videos for each quantization method and the corresponding prompt.
	}
	\label{fig_frames}
\end{figure*}


\section{Our Method}

\begin{figure}[!htb]
	\centering
	% \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
	\includegraphics[width=0.7\linewidth]{figures/0.png}
	\caption{\textbf{Overview of the proposed method} Upper left figure shows the architecture of STDiT. The upper right figure shows the change of activation distribution  across time steps from STDiT. Lower figures summarized the components of our method: Channel-Wise (CW) quantization for weights, Tensor-Wise (TW) quantization for activations, Aggregated or Time-step-wise Smooth Quantization (ASQ, TSQ), and Time-Step-Wise (TSW) static quantization for each diffusion step. The linear layers in the attention and feed forward layers in the transformer blocks (orange-colored) of STDiT were quantized. $N$ is the number of transformer blocks (28 in STDiT v1.0). $\it{\bf{X}}$ is the activation and $\it{\bf{W}}$ is the weight matrix where $T$ is the number of tokens, $C_I$ and $C_O$ is the input and output channel sizes and $t$ is the number of denoising time steps.
	}
	\label{fig0}
\end{figure}

The proposed methods, aggregated static quantization and time-step-wise static quantization consist of three different components to quantize the transformer denoiser of DiT models: channel-wise (CW) weight quantization, tensor-wise (TW) activation quantization and aggregated of time-step-wise smooth quantization (ASQ or TSQ). Each component's will be explained as aggregated static quantization version first and the time-step-wise static quantization version will be discussed in the next subsection. Using these methods, linear layers of the spatial self-attention, temporal self-attention, prompt cross-attention, and pointwise feed forward layers in the transformer blocks were quantized (See the upper left plot of Figure \ref{fig0}).

\subsection{Aggregated static quantization}
The first component of our method involves channel-wise quantization of the weight matrix to mitigate quantization errors arising from channel-wise (CW) variance \cite{zhao2024vidit}. The minimum and maximum values for each channel in weights were extracted and stored. Then the minimum and maximum were used to calculate bin sizes ($\Delta W_i$) and zero points ($z_{W_i}$) channel-wise using the following equation \ref{eq1}.
\begin{equation}\label{eq1}
\begin{split}
\Delta W_i = \frac{\max(W_i)-\min(W_i)}{2^b}, \quad
z_{W_i} = \frac{\min(W_i)}{\Delta W_i},\\ 
b\text{ is the bit width and } i\text{ is the channel index.}
\end{split}
\end{equation}
Thus the bin sizes and zero points for CW quantization of a weight matrix have $[1\times C_O]$ dimensions where $C_O$ is the output channel size (see the middle row of Figure \ref{fig0}). 

The second component is tensor-wise (TW) quantization of the activation matrix. While dynamic token-wise quantization is widely used for transformer models, it is not feasible to estimate statistics to cover the variance of each token activation during inference in a static manner due to the heterogeneity across inference samples. Instead, the simplest method, which involves estimating the minimum and maximum values of activations tensor-wise, was utilized. Therefore, the bin sizes ($\Delta X$) and zero points ($z_X$) for TW quantization of a activation matrix are scalar values (see the middle row of Figure \ref{fig0}).

Next, as the third component, we explored variants of smooth quantization method to address the quantization difficulty difference across different channels \cite{xiao2024smoothquantaccurateefficientposttraining}. The smooth quantization we applied in aggregated static quantization method is aggregated smooth quantization (ASQ). In ASQ, the maximum absolute values of weights for each channel were extracted ($\max(|W_{i})|$) to calculate the smooth quantization scaling term ($s_{i}$) for each channel $i$. Then, the maximum absolute values of activations for each channel were averaged across the batch ($\max(|X_{i}|)$). The maximum absolute values of activations for the entire calibration set were then calculated using a running average with a momentum of 0.95, aggregating information across time steps. The final scaling term for smooth quantization was determined using equation \ref{eq2}, where $\alpha\in[0,1]$ is a hyperparameter that needs to be tuned. Therefore, in ASQ, a single scaling term was obtained for each channel, which was used for the entire denoising steps.

\begin{equation}\label{eq2}
\begin{split}
Y = \left(X \cdot \text{diag}(s)^{-1}\right) \cdot \left(\text{diag}(s) \cdot W\right) = \hat{X} \cdot \hat{W} \\
s_{i} = \frac{\max(|X_{i}|)^\alpha}{\max(|W_{i}|)^{1-\alpha}}, \quad i\text{ is the channel index}
\end{split}
\end{equation}

As a result, the scaling term ($s_{i}$) has the size of $[C_I\times 1]$ where the $C_I$ is the input channel size (see the middle row of Figure \ref{fig0}). When the ASQ was applied, CW weight quantization and TW activation quantization was applied to the smoothed $\hat{X}$ and $\hat{W}$, instead of the raw $X$ and $W$. Therefore, applying ASQ and do TW activation quantization can be viewed as a method that handles channel-wise variance of the activation without doing CW quantization on the activation matrix.

\subsection{Time-step-wise static quantization}

One of our main contributions is a method that estimates quantization parameters for individual denoising time steps during the calibration stage only, enabling time-step-wise (TSW) static quantization. Unlike previously suggested dynamic quantization method that estimates quantization parameters for each denoising time step during the inference \cite{zhao2024vidit}, our TSW static quantization estimates the parameters during the calibration stage only, and use the estimated parameters without any updates for the inference, facilitating efficient hardware-level optimization.

Thus TSW static quantization can handle the time-step-wise variance in activation distributions (See the upper right plot in Figure \ref{fig0}) without additional calculation during the inference. The essential of implementing the static TSW quantization is to use the calibration set to estimate the parameter statistics for each denoising step. 

When the TSW static quantization is applied, the bin sizes ($\Delta X$) and zero points ($z_X$) for the TW quantization have $[1\times t]$ dimensions. The smooth quantization scaling term ($s_i$) has the $[C_I \times t]$ dimension where $t$ is the number of denoising time steps, which we called Time-step-wise Smooth Quantization, or TSQ. The CW quantization stays same as it quantizes weight not activations. See the last row of Figure \ref{fig0}). During inference, these fixed, pre-estimated statistics are used without further calculations or updates.

TSW static quantization can also be applied in a coarser manner, where quantization parameters are estimated not for each time step but across time ranges that group multiple denoising steps. For instance, estimating parameters over the entire denoising process corresponds to a single time-range static quantization. Dividing the process into two time ranges (e.g., early and late stages) and estimating parameters for each range leads to a two-time-range static quantization. The effects of different levels of granularity in static quantization parameter estimation are further discussed in Section \ref{tsw}.

\begin{table}[!htb]
	\centering
	\caption{Quantization methods on the open-sora prompt set}
	\label{tab1}
    \small % Makes the font size smaller
    \setlength{\tabcolsep}{2pt} % Reduce padding between columns
	\begin{tabular}{l | l | c | c | c | c %| c | c
	}
		Method & Bit (W/A) & CLIPSIM & CLIP-temp & VQA-a & VQA-t %& IQA-a & IQA-t
		\\
		\hline
		FP16 \cite{opensora}& 16/16 & 0.1950 & 0.9982 & 54.2730 & 49.9179 %& 4.8132 & 4.6001
		\\
        Dynamic \cite{zhao2024vidit} & 8/8 & 0.1960 & 0.9982 & 53.3998 & 49.4671 %& 4.8219 & 4.5962
        \\
        \hline
        CW+TW (*) & 8/8 & 0.1931 & 0.9966 & 44.2729 & 38.6906 %& 4.5347 & 4.3631
        \\
        *+TSW & 8/8 & 0.1931 & 0.9972 & 42.4221 & 40.8720 %& 4.6094 & 4.4304
        \\
        %*+LC+TSW & 8/8 & 0.1910 & 0.9986 & 14.2270 & 16.6479 %& 4.2340 & 4.6134
        %\\
        *+ASQ & 8/8 & 0.1926 & \bf0.9988 & \bf52.9969 & \bf49.9545 %& 4.8030 & 4.6242
        \\
        *+TSQ+TSW & 8/8 & \bf0.1967 & 0.9987 & 47.5200 & 39.0468 %& \bf4.8104 & \bf4.6884
        \\
		\hline
	\end{tabular}
    \\
	\centering
	\caption{Quantization methods on the UCF-101} 
    \small % Makes the font size smaller
    \setlength{\tabcolsep}{2pt} % Reduce padding between columns
	\label{tab2}
	\begin{tabular}{l | l | c | c | c | c %| c | c
	}
		Method & Bit (W/A) & CLIPSIM & CLIP-temp & VQA-a & VQA-t %& IQA-a & IQA-t
		\\
		\hline
		FP16 \cite{opensora}& 16/16 & 0.2058 & 0.9970 & 26.3483 & 34.5356 %& 4.5385 & 5.0782
		\\
        Dynamic\cite{zhao2024vidit} & 8/8 & 0.2049 & 0.9968 & 25.6435 & 34.6162 %& 4.5249 & 5.1175
        \\
        \hline
        CW+TW (*) & 8/8 & 0.2033 & 0.9955 & 18.1882 & 22.8154 %& 4.3827 & 4.8845
        \\
        *+TSW & 8/8 & 0.2025 & 0.9958 & 17.9768 & 24.7684 %& 4.4000 & 4.9351
        \\
        *+ASQ & 8/8 & 0.2048 & 0.9975 & \bf24.6499 & \bf31.9683 %& \bf4.5345 & \bf5.1199
        \\
        *+TSQ+TSW & 8/8 & \bf0.2084 & \bf0.9986 & 10.2957 & 17.4171 %& 4.3674 & 5.1182
        \\
		\hline
	\end{tabular}
    \\
	\centering
	\caption{Robustness test of the proposed methods}
    \small % Makes the font size smaller
    \setlength{\tabcolsep}{2pt} % Reduce padding between columns
	\label{tab3}
	\begin{tabular}{l | l | c | c | c | c %| c | c
	}
		Bit (W/A) & Method & CLIPSIM & CLIP-temp & VQA-a & VQA-t %& IQA-a & IQA-t
		\\
		\hline\hline
  FP16 \cite{opensora}& - & 0.1950 & 0.9982 & 54.2730 & 49.9179 %& 4.8132 & 4.6001
  \\
  \hline
    4/4  & *+TSQ+TSW & 0.1569 & 0.9967 & 0.0257 & 0.0008 %& 3.9653 & 4.6572
    \\
      & *+ASQ & 0.1538 & \bf0.9974 & 0.0171 & 0.0009 %& 4.0158 & 4.5555
      \\
      & Dynamic \cite{zhao2024vidit} & \bf0.1786 & 0.9729 & \bf2.7089 & \bf0.4849 \\
      \hline
    4/6  & *+TSQ+TSW & \bf0.1909 & 0.9981 & \bf4.8082 & \bf5.4387 %& 4.0776 & 4.1705
    \\
    & *+ASQ & 0.1838 & \bf0.9983 & 0.0557 & 0.0686 %& 3.8255 & 4.5854
    \\
    & Dynamic \cite{zhao2024vidit} & 0.1817 & 0.9976 & 0.1681 & 0.2958 \\
    \hline
    6/6  & *+TSQ+TSW & 0.1864 & 0.9981 & 9.2497 & 9.2459 %& 4.2499 & 4.1549
    \\
    & *+ASQ & 0.1829 & \bf0.9986 & 0.0961 & 0.0781 %& 3.8122 & 4.5334
    \\
    & Dynamic \cite{zhao2024vidit} & \bf0.1881 & 0.9981 & \bf18.9117 & \bf15.1275 \\
    \hline
    4/8  & *+TSQ+TSW & \bf0.1922 & \bf0.9994 & 24.5604 & \bf24.0194 %& 4.6170 & 4.5706
    \\
    & *+ASQ & 0.1849 & 0.9984 & \bf26.9354 & 15.8466 %& 4.5022 & 4.2895
    \\
    & Dynamic \cite{zhao2024vidit} & 0.1849 & 0.9986 & 0.0404 & 0.3072 \\
    \hline
    6/8  & *+TSQ+TSW & \bf0.1962 & 0.9987 & \bf36.0467 & \bf40.6366 %& 4.5335 & 4.6191
    \\
    & *+ASQ & 0.1910 & \bf0.9991 & 25.6601 & 17.4476 %& 4.4841 & 4.4779
    \\
    & Dynamic \cite{zhao2024vidit} & 0.1903 & 0.9990 & 28.2134 & 21.8238 \\
    \hline
    8/8  & *+TSQ+TSW & \bf0.1967 & 0.9987 & 47.5200 & 39.0468 %& 4.8104 & 4.6884
    \\
    & *+ASQ & 0.1926 & \bf0.9988 & \bf53.9969 & \bf49.9545 %& 4.8030 & 4.6242
    \\
    & Dynamic \cite{zhao2024vidit} & 0.1960 & 0.9982 & 53.3998 & 49.4671 \\
    \hline
    4/16 & *+TSQ+TSW & \bf0.1911 & \bf0.9996 & 28.3527 & 12.5568 %& 4.6650 & 4.4838
    \\
    & *+ASQ & 0.1895 & 0.9988 & \bf31.6746 & \bf35.4823 %& 4.4511 & 4.5720
    \\
    & Dynamic \cite{zhao2024vidit} & 0.1871 & 0.9972 & 0.0237 & 0.0240 \\
    \hline
    6/16 & *+TSQ+TSW & \bf0.1960 & \bf0.9996 & 26.2750 & 17.4801 %& 4.6757 & 4.5786
    \\
    & *+ASQ & 0.1959 & 0.9989 & \bf44.6240 & \bf38.0148 %& 4.6417 & 4.6155
    \\
    & Dynamic \cite{zhao2024vidit} & 0.1854 & 0.9983 & 8.3704 & 10.7042 \\
    \hline
    8/16 & *+TSQ+TSW & 0.1953 & \bf0.9988 & 46.8446 & 38.9204 %& 4.7489 & 4.6439
    \\
    & *+ASQ & \bf0.1963 & 0.9982 & \bf53.2460 & \bf51.9194 %& 4.8196 & 4.6157
    \\
    & Dynamic \cite{zhao2024vidit} & 0.1871 & 0.9981 & 24.0312 & 26.8858 \\
    \hline\hline
    Average & *+TSQ+TSW & \bf0.1891 & \bf0.9986 & 24.8537 & 20.8162 %& \bf4.4826 & 4.5074
    \\
    & *+ASQ & 0.1856 & 0.9985 & \bf26.1451 & \bf23.2014 %& 4.3728 & \bf4.5410
    \\
    & Dynamic \cite{zhao2024vidit} & 0.1866 & 0.9953 & 15.0964 & 13.9022 \\
		\hline
	\end{tabular}
\end{table}

%-------------------------------------------------------------------------
\begin{figure*}[t]
	\centering
	% \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
	\includegraphics[width=0.8\linewidth]{figures/2.png}
	\caption{Robustness comparison between the CW+TW+ASQ (Aggr) and the CW+TW+TSQ+TSW (TSW). Additional examples in the supplemental material.}
	\label{fig2}
\end{figure*}

%\begin{figure*}[t]
%	\centering
	% \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%	\includegraphics[width=1\linewidth]{figures/3.png}
%	\caption{Robustness test examples from the CW+TW+ASQ model
%	}
%	\label{fig3}
%\end{figure*}

%------------------------------------------------------------------------
\begin{table}[!htb]
	\caption{Granular TSW quantization on the open-sora prompt set}
	\label{tab5}
    \centering
	\begin{tabular}{l | c | c | c | c %| c | c
	}
		Method & CLIPSIM & CLIP-temp & VQA-a & VQA-t %& IQA-a & IQA-t
		\\
		\hline
 *+ASQ (=1TR)  & 0.1926 & 0.9988 & 52.9969 & 49.9545 %& 4.8030 & 4.6242
 \\
    *+TSQ+2TR  & 0.1960 & 0.9981 & 42.2731 & 43.7837 %& 4.7081 & 4.4828
    \\
    *+TSQ+4TR  & 0.1957 & 0.9978 & 43.3432 & 46.1740 %& 4.6421 & 4.6014
    \\
    *+TSQ+10TR  & 0.1946 & 0.9978 & 40.5217 & 42.3603 %& 4.4233 & 5.0389
    \\
    *+TSQ+TSW (=20TR)  & 0.1967 & 0.9987 & 47.5200 & 39.0468 %& 4.8104 & 4.6884
    \\
		\hline
	\end{tabular}
    \\
    \centering
	\caption{Granular TSW quantization on the UCF-101}
	\label{tab6}
	\begin{tabular}{l | c | c | c | c %| c | c
	}
		Method & CLIPSIM & CLIP-temp & VQA-a & VQA-t %& IQA-a & IQA-t
		\\
		\hline
 *+ASQ (=1TR)  & 0.2048 & 0.9975 & 24.6499 & 31.9683 %& 4.5345 & 5.1199
 \\
    *+TSQ+2TR  & 0.2039 & 0.9966 & 18.0531 & 23.2528 %& 4.5379 & 4.9975
    \\
    *+TSQ+4TR  & 0.2054 & 0.9967 & 18.3887 & 26.8416 %& 4.4233 & 5.0389
    \\
    *+TSQ+10TR  & 0.2063 & 0.9967 & 14.2008 & 22.1882 %& 4.4233 & 5.0389
    \\
    *+TSQ+TSW (=20TR)  & 0.2084 & 0.9986 & 10.2957 & 17.4171 %& 4.3674 & 5.1182
    \\
		\hline
	\end{tabular}
\end{table}


\section{Experiment}
\subsection{Implementation Details and Experimental Settings}
We evaluated the performance of our methods on the STDiT v1.0 using various bitwidths and evaluation settings \cite{opensora}. For quantization, we adopted the min-max quantization scheme, where the quantization parameters for actions were estimated using a calibration set. This calibration set was generated using example prompts to create a calibration dataset for ViDiT-Q, consisting of 10 prompts \cite{zhao2024vidit}. 
%We also tested larger calibration (LC) set selected from WebVid-10M which was used as a training set for STDiT \cite{Bain21}. About 600 prompts from WebVid-10M were embedded using CLIP-ViT-L14 and clustered into 600 groups using K-Means clustering \cite{radford2021learningtransferablevisualmodels}. Then the prompts closest to each centroid were selected as the calibration set. 
For evaluation, we utilized CLIPSIM and CLIP-temp, both scaled from 0 to 1, to measure text-video alignment and temporal semantic consistency of the generated videos \cite{wu2021godivageneratingopendomainvideos, esser2023structurecontentguidedvideosynthesis}. The CLIPSIM is the average CLIP cosine similarity between the prompt CLIP embedding and each frame's CLIP embedding. The CLIP-temp is calculated by averaging across CLIP cosine similarities between CLIP embeddings of consecutive two frames. To assess video quality, we used VQA-aesthetic and VQA-technical scores, scaled from 0 to 100, to evaluate high-level aesthetics and low-level technical soundness \cite{wu2023exploringvideoqualityassessment}. The VQA scores are automatically evaluated by neural networks trained on large video datasets, which were annotated by humans for aesthetic appeal and technical quality. For the evaluations, implementations in the EvalCrafter were used \cite{liu2024evalcrafterbenchmarkingevaluatinglarge}. %Additionally, we employed IQA-aesthetic and IQA-technical, scaled from 1 to 10, for image evaluation, supplementing the VQA metrics by calculating scores for each frame and averaging them across frames \cite{Talebi_2018}. 
The experiments were conducted on UCF-101 \cite{soomro2012ucf101dataset101human} and OpenSora\cite{opensora} datasets, on a single A100 GPU using 20 denoising steps and an IDDPM scheduler with a CFG scale of 7.0. The $\alpha$ parameter for SmoothQuant was grid-searched from 0.1 to 1.0 in 0.1 increments, chosen based on the CLIPSIM score. For dynamic quantization, $\alpha = 0.625$ (from ViDiT-Q) was used; for CW+TW+ASQ, $\alpha = 0.4$; and for CW+TW+TSQ+TSW, $\alpha = 0.2$. 

\subsection{Main Results and Ablation Studies}

First, we compared our STDiT quantization approaches (*+ASQ and *+TSQ+TSW) with previous methods (Tables \ref{tab1} and \ref{tab2}). Our methods showed superior performance across various metrics. Specifically, *+TSQ+TSW excelled in producing semantically aligned videos. For example, frames from the generated videos in Figure \ref{fig_frames} show that our *+TSQ+TSW method most accurately captured the vibrant characteristics of the city of Burano (see also Figure \ref{fig1}). The aggregated method, *+ASQ achieved video quality comparable to ViDiT-Q's dynamic quantization according to VQA metrics. 

Intuitively, it might be thought that aggregated static quantization would perform worse than TSW static quantization. However, we observed on-par or even superior performance from the *+ASQ models for a subset of metrics. This is attributed to the fact that when ASQ (also TSQ) is applied, within-a-channel activity variations are smoothed out, allowing the maximum absolute activation to be normalized by $max(|X_i|)^\alpha$, which ensures that the maximum absolute value of the smoothed activation is less than 1, bringing the range of activations within $[-1, 1]$ regardless of the denoising steps, and making TSW less necessary. 

Related this analysis, the ablation study further confirmed the importance of applying ASQ and TSQ for the best performance static quantization (Tables \ref{tab1} and \ref{tab2}).

However, while not using TSQ but using ASQ does not lead to significant failures—due to the activation distribution remaining within the range of the calibrated parameters—it can result in less precise quantization. This imprecision arises because the activation range for some time steps may only cover subset of $[-1, 1]$. Consequently, we can imagine *+ASQ is more sensitive to changes in bit-widths than *+TSQ+TSW, which will be discussed in the next section \ref{tsw}.

%However, using the larger calibration (LC) set didn't improve the performance much so we didn't explore this direction further. 

\subsection{Method Robustness Across Different Bit-Widths}\label{tsw}
We assessed the robustness of *+TSQ+TSW and *+ASQ across various bitwidths using the open-sora prompt set and compared them to the dynamic quantization used in ViDiT-Q for W8A8 quantization \cite{zhao2024vidit}. Table \ref{tab3} shows that both static quantization methods outperformed dynamic quantization across a broader bitwidth range. The averaged score across bitwidth in Table \ref{tab3} revealed that the *+TSQ+TSW model generated videos with better semantic alignment with the prompt and temporal consistency, while the *+ASQ model produced videos with more aesthetic appeal and less noise and distortion. 

Furthermore, qualitative analysis showed that *+TSQ+TSW generated recognizable videos in wider ranges of bit-width than the *+ASQ which confirming our analysis in the previous section (see Figure \ref{fig2}). 


\subsection{Effect of Time-Step-Wise Quantization}
We further explored the impact of time-step-wise quantization by grouping time steps into several time ranges (TR), estimating quantization parameters separately for each group. For instance, the 2TR method grouped the time steps into two ranges and estimated quantization parameters for each range, while the 4TR method did so for four separate time ranges. Therefore, for example, the SQ scaling term will have $[C_i\times 2]$ or $[C_i\times 4]$ dimensions for 2TR or 4TR methods. As shown in Tables \ref{tab5} and \ref{tab6}, implementing time-step-wise quantization improved semantic adherence to the prompt, as measured by CLIP, but compromised video quality metrics (VQA-a and VQA-t) and the model size. This trade-off is evident in the qualitative analysis of the generated videos (see the last two columns of Figure \ref{fig1} and Figure \ref{fig2}).

\subsection{Cross-Dataset Evaluation}
Although calibration was performed using the OpenSora prompt set from the ViDiT-Q paper, we validated our model’s generalization by evaluating it on UCF-101. The results in Tables \ref{tab2} and \ref{tab6} demonstrate that our static quantization approach maintains competitive performance, validating its cross-dataset generalization capability.

\subsection{Temporal Efficiency}
We evaluated the execution time of different quantization methods by measuring the time taken to generate 48 videos using the OpenSora prompt set on an NVIDIA A100 GPU. As shown in Table \ref{tab7}, both CW+TW+ASQ and CW+TW+TSQ+TSW showed significantly reduced inference time compared to the dynamic quantization approach.

\begin{table}[t]
\centering
\caption{Execution time for generating 48 videos using OpenSora prompts on an A100 GPU}
\label{tab7}
    \begin{tabular}{c|c}
        \toprule
        Method & Execution time (s)\\
        \midrule
        Dynamic & 2259 \\
        CW+TW+ASQ & 1942 \\
        CW+TW+TSQ+TSW & 1919 \\
        \bottomrule
    \end{tabular}
\end{table}


\section{Conclusion}

Modern Mobile SoC hardware lacks support for any form of dynamic quantization due to its inability to calculate activation or weight statistics at runtime \cite{park2022multi}. Our proposed static quantization schemes provide a viable solution for deployment on these devices, as they eliminate the need for online statistic computation, enabling simpler hardware design with lower power and area requirements.

This paper introduces two static quantization methods—aggregated and time-step-wise—that match the performance of previously proposed dynamic quantization for DiT models. By incorporating channel-wise weight quantization, tensor-wise activation quantization and smooth quantization, either aggregated or time-step-wise, our approach successfully maintains the performance of the original STDiT (Open-Sora) model.

The proposed methods could broaden the use of diffusion-based generative models in mobile AI devices, though limitations at narrow bit-widths suggest room for future enhancement. Additionally, assigning different quantized models for each time step means intelligent processing to swap models across time steps is needed to hide the latency in loading new model parameters. Balancing between fine-grained time step quantization and grouping time steps into ranges, while considering hardware constraints, will be crucial for optimizing the method's practical application.

\subsection{Supplemental material}
Please refer to the provided slides for videos of the figures.


%%%%%%%%% REFERENCES
{\small
\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{videoworldsimulators2024}
Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh.
\newblock Video generation models as world simulators.
\newblock 2024.

\bibitem{dettmers2023spqrsparsequantizedrepresentationnearlossless}
Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh.
\newblock Spqr: A sparse-quantized representation for near-lossless llm weight compression, 2023.

\bibitem{esser2023structurecontentguidedvideosynthesis}
Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis.
\newblock Structure and content-guided video synthesis with diffusion models, 2023.

\bibitem{gholami2021surveyquantizationmethodsefficient}
Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael~W. Mahoney, and Kurt Keutzer.
\newblock A survey of quantization methods for efficient neural network inference, 2021.

\bibitem{he2023ptqdaccurateposttrainingquantization}
Yefei He, Luping Liu, Jing Liu, Weijia Wu, Hong Zhou, and Bohan Zhuang.
\newblock Ptqd: Accurate post-training quantization for diffusion models, 2023.

\bibitem{lee2024owqoutlierawareweightquantization}
Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, and Eunhyeok Park.
\newblock Owq: Outlier-aware weight quantization for efficient fine-tuning and inference of large language models, 2024.

\bibitem{li2023qdiffusionquantizingdiffusionmodels}
Xiuyu Li, Yijiang Liu, Long Lian, Huanrui Yang, Zhen Dong, Daniel Kang, Shanghang Zhang, and Kurt Keutzer.
\newblock Q-diffusion: Quantizing diffusion models, 2023.

\bibitem{lin2024awqactivationawareweightquantization}
Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han.
\newblock Awq: Activation-aware weight quantization for llm compression and acceleration, 2024.

\bibitem{lin2023fqvitposttrainingquantizationfully}
Yang Lin, Tianyu Zhang, Peiqin Sun, Zheng Li, and Shuchang Zhou.
\newblock Fq-vit: Post-training quantization for fully quantized vision transformer, 2023.

\bibitem{liu2024enhanceddistributionalignmentposttraining}
Xuewen Liu, Zhikai Li, Junrui Xiao, and Qingyi Gu.
\newblock Enhanced distribution alignment for post-training quantization of diffusion models, 2024.

\bibitem{liu2024evalcrafterbenchmarkingevaluatinglarge}
Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, and Ying Shan.
\newblock Evalcrafter: Benchmarking and evaluating large video generation models, 2024.

\bibitem{liu2021posttraining}
Zhenhua Liu, Yunhe Wang, Kai Han, Wei Zhang, Siwei Ma, and Wen Gao.
\newblock Post-training quantization for vision transformer.
\newblock In A. Beygelzimer, Y. Dauphin, P. Liang, and J.~Wortman Vaughan, editors, {\em Advances in Neural Information Processing Systems}, 2021.

\bibitem{ma2024era1bitllmslarge}
Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping Wang, Jilong Xue, and Furu Wei.
\newblock The era of 1-bit llms: All large language models are in 1.58 bits, 2024.

\bibitem{shang2023posttrainingquantizationdiffusionmodels}
Yuzhang Shang, Zhihang Yuan, Bin Xie, Bingzhe Wu, and Yan Yan.
\newblock Post-training quantization on diffusion models, 2023.

\bibitem{so2023temporaldynamicquantizationdiffusion}
Junhyuk So, Jungwon Lee, Daehyun Ahn, Hyungjun Kim, and Eunhyeok Park.
\newblock Temporal dynamic quantization for diffusion models, 2023.

\bibitem{soomro2012ucf101dataset101human}
Khurram Soomro, Amir~Roshan Zamir, and Mubarak Shah.
\newblock Ucf101: A dataset of 101 human actions classes from videos in the wild, 2012.

\bibitem{tang2024posttrainingquantizationtexttoimagediffusion}
Siao Tang, Xin Wang, Hong Chen, Chaoyu Guan, Zewen Wu, Yansong Tang, and Wenwu Zhu.
\newblock Post-training quantization for text-to-image diffusion models with progressive calibration and activation relaxing, 2024.

\bibitem{wang2023bitnetscaling1bittransformers}
Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi Wu, and Furu Wei.
\newblock Bitnet: Scaling 1-bit transformers for large language models, 2023.

\bibitem{wu2021godivageneratingopendomainvideos}
Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo Sapiro, and Nan Duan.
\newblock Godiva: Generating open-domain videos from natural descriptions, 2021.

\bibitem{wu2023exploringvideoqualityassessment}
Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou, Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi Lin.
\newblock Exploring video quality assessment on user generated contents from aesthetic and technical perspectives, 2023.

\bibitem{xiao2024smoothquantaccurateefficientposttraining}
Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han.
\newblock Smoothquant: Accurate and efficient post-training quantization for large language models, 2024.

\bibitem{yang2023efficientquantizationstrategieslatent}
Yuewei Yang, Xiaoliang Dai, Jialiang Wang, Peizhao Zhang, and Hongbo Zhang.
\newblock Efficient quantization strategies for latent diffusion models, 2023.

\bibitem{yuan2023rptqreorderbasedposttrainingquantization}
Zhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang, Yuzhang Shang, Guangyu Sun, Qiang Wu, Jiaxiang Wu, and Bingzhe Wu.
\newblock Rptq: Reorder-based post-training quantization for large language models, 2023.

\bibitem{yuan2024ptq4vitposttrainingquantizationvision}
Zhihang Yuan, Chenhao Xue, Yiqi Chen, Qiang Wu, and Guangyu Sun.
\newblock Ptq4vit: Post-training quantization for vision transformers with twin uniform quantization, 2024.

\bibitem{zhao2024vidit}
Tianchen Zhao, Tongcheng Fang, Enshu Liu, Wan Rui, Widyadewi Soedarmadji, Shiyao Li, Zinan Lin, Guohao Dai, Shengen Yan, Huazhong Yang, et~al.
\newblock Vidit-q: Efficient and accurate quantization of diffusion transformers for image and video generation.
\newblock {\em arXiv preprint arXiv:2406.02540}, 2024.

\bibitem{zhao2023surveylargelanguagemodels}
Wayne~Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen.
\newblock A survey of large language models, 2023.

\bibitem{opensora}
Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You.
\newblock Open-sora: Democratizing efficient video production for all, March 2024.

\bibitem{zhu2024surveymodelcompressionlarge}
Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang.
\newblock A survey on model compression for large language models, 2024.

\bibitem{park2022multi}
Park, \emph{et al.}, 
``A multi-mode 8k-MAC HW-utilization-aware neural processing unit with a unified multi-precision datapath in 4-nm flagship mobile SoC,'' 
\emph{IEEE Journal of Solid-State Circuits}, 58.1 (2022): 189-202.
\end{thebibliography}


\end{document}
