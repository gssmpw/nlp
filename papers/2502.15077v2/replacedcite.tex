\section{Related Works}
\label{sec:rw}
\textbf{PTQ in Transformers}
Numerous PTQ methods have been developed to meet the demands of transformer-based language models in industry ____. For example, Activation-aware Weight Quantization (AWQ) quantizes weights based on the saliency of the corresponding activations and scales the weights per channel to minimize quantization errors ____. Other methods also take into account the activation outliers in quantizing weights ____.  Advancements in LLM weight quantization have even reached sub-2-bit quantization, utilizing ternary numbers for weights ____. Additionally, various methods have been proposed for PTQ of both weights and activations, with the primary challenge being the diverse distribution across activation channels. For instance, Smooth Quantization (SQ) smooths the weights and activation by scaling the channels, and Reorder-based Post-training Quantization (RPTQ) cluster the channels of weights and activation based on the similarity and quantizes per cluster ____. 

A series of PTQ methods for Vision Transformer (ViT) was also suggested such as using mixed precision, attention weight order preserving quantization, and using multiple uniform quantization or log transformations to manage non-Gaussian activation distribution, ____.

\textbf{PTQ in Diffusion Models}
However, the recurrent nature of diffusion models, which rely on the output of previous steps to sample the input for subsequent denoising steps, have limited the direct application of PTQ methods from other domains to diffusion model quantization. A key obstacle is the significant variation in activation distribution across time steps ____. Early work in PTQ for diffusion models mitigated this issue by generating calibration sets from the denoising process, with a focus on sampling across time steps to preserve output quality ____. Other approaches involved using a neural network module to determine the time intervals for quantization and predict quantization parameters per interval or having different precision for different time steps and progressive calibration ____. Another study suggested a method using SQ, mixed precision across layers and focusing on the final time step in calibration for quantizing latent diffusion models ____.

\textbf{PTQ in Diffusion Transformers}
Recent advancements in video generation have increasingly adopted transformer models in denoising, such as Sora by OpenAI ____. However, effective methods for quantizing Diffusion Transformer (DiT) models remain limited due to their unique characteristics compared to LLMs and CNN-based diffusion models. For example, unlike LLMs, DiT models experience significant variation across tokens, making the direct application of LLM quantization methods challenging ____. Additionally, the activation distribution in DiT models varies significantly across time steps and between forward paths with or without conditional information from prompts ____. The ViDiT-Q framework introduced techniques for quantizing DiT models, utilizing both path-wise dynamic quantization based on prompts and token-wise dynamic quantization for activations ____. Despite its advantages, the framework requires real-time calculation of quantization parameters during inference, which complicates hardware optimizations, particularly for NPUs. Our methods avoid this problem by using static quantization while achieving the performance of the original model.

%To address this challenge, we propose a method to statically quantize DiT models by calculating quantization parameters during a calibration phase and using these fixed parameters during inference. Our method involves channel-wise quantization for weights, tensor-wise quantization for activations, smooth quantization, and estimating quantization parameters for each time step. Using this static quantization approach, we achieved performance comparable to the original FP16 model and the dynamic quantization method from ViDiT-Q ____ in W8A8 quantization for the STDiT (OpenSora ____) model.

\begin{figure}[t]
	\centering
	% \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
	\includegraphics[width=0.7\linewidth]{figures/1.png}
	\caption{Example videos from various quantization methods
	}
	\label{fig1}
\end{figure}

\begin{figure*}[t]
	\centering
	% \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
	\includegraphics[width=1\linewidth]{figures/video_consecutive_frames_visualization.png}
	\caption{Example frames from generated videos for each quantization method and the corresponding prompt.
	}
	\label{fig_frames}
\end{figure*}