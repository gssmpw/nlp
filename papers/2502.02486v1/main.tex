% !TEX TS-program = pdflatex
\documentclass{article}

%\input{myheaders.tex}

%\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{fullpage}
\usepackage{amsmath,amsfonts,amssymb, amsthm,thmtools,thm-restate, bm}
\usepackage{mathrsfs, mathtools}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[ruled,linesnumbered]{algorithm2e}
\newcommand\mycommfont[1]{\foot\documentclass{article}


\notesize\ttfamily\textcolor{blue}{#1}}
%\SetCommentSty{mycommfont}
\usepackage{xspace}
\usepackage{natbib}
\usepackage{bm}
\usepackage{accents}
\usepackage{tablefootnote}

\usepackage[dvipsnames, svgnames, x11names,table]{xcolor}
\usepackage[colorlinks,
            linkcolor=RoyalBlue,
            anchorcolor=RoyalBlue,
            citecolor=RoyalBlue
            ]{hyperref}
\usepackage{cleveref}
\usepackage{pifont}  % for \ding commands

% Define commands for checkmark and x-mark
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\usepackage{makecell}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{assumption}{Assumption}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\theoremstyle{definition}
\newtheorem{example}{Example}
\newtheorem{remark}{Remark}
\newtheorem{fact}{Fact}


\newcommand{\cA}{{\mathcal A}}
\newcommand{\cX}{\ensuremath{\mathcal{X}}}
\newcommand{\cD}{{\mathcal D}}
\newcommand{\cF}{\ensuremath{\mathcal{F}}}
\newcommand{\cR}{\ensuremath{\mathcal{R}}}
\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\cG}{\ensuremath{\mathcal{G}}}
\renewcommand{\P}{\ensuremath{\mathbb{P}}}
\newcommand{\B}{\ensuremath{\mathbb{B}}}
\newcommand{\cM}{\ensuremath{\mathcal{M}}}
\newcommand{\cN}{\ensuremath{\mathcal{N}}}
\newcommand{\cV}{\ensuremath{\mathcal{V}}}
\newcommand{\cS}{\ensuremath{\mathcal{S}}}
\newcommand{\cB}{\ensuremath{\mathcal{B}}}


\newcommand{\vect}{{\mathrm{vec}}}
\DeclareMathOperator*{\rE}{{\mathbb E}}
\newcommand{\rV}{{\mathbb V}}
\newcommand{\rG}{{\mathbb G}}
\DeclareMathOperator*{\argmax}{\text{argmax}}
\DeclareMathOperator*{\argmin}{\text{argmin}}
\DeclareMathOperator*{\argsup}{\text{argsup}}
\DeclareMathOperator*{\arginf}{\text{arginf}}
\newcommand{\epoch}{m}
\newcommand{\one}{\ensuremath{\mathbf{1}}}
\newcommand{\tr}{\ensuremath{\text{Tr}}}
\newcommand{\logdet}{\ensuremath{\text{logdet}}}
\newcommand{\inner}[2]{\ensuremath{\left\langle #1, #2 \right\rangle}}
\newcommand{\err}[2]{\ensuremath{\texttt{err}(#1\rightarrow #2)}}
\newcommand{\extend}{\ensuremath{\varphi}}

\usepackage{eqparbox}
%\renewcommand{\algorithmiccomment}[1]{\hspace*{\fill}\mbox{\textcolor{blue}{$\triangleright$ #1}}}

\newcommand{\cP}{\ensuremath{\mathcal{P}}}
\newcommand{\Rhat}{\ensuremath{\widehat{R}}}
\newcommand{\fhat}{\ensuremath{\widehat{f}}}
\newcommand{\order}{O}
\newcommand{\otil}{\ensuremath{\widetilde{\order}}}
\newcommand{\reg}{\ensuremath{\text{Reg}}}
\newcommand{\regh}{\ensuremath{\widehat{\text{Reg}}}}
\newcommand{\Rtrunc}{\ensuremath{\overline{R}}}
\newcommand{\BE}{\ensuremath{{\mathcal E}_B}}
\newcommand{\trace}{{\mathrm{trace}}}

\newcommand{\tpi}{{\tilde{\pi}}}
\newcommand{\tOmega}{{\tilde{\Omega}}}

\newcommand{\dref}{\ensuremath{P_0}}
\newcommand{\cW}{\ensuremath{\mathcal{W}}}
\newcommand{\cT}{\ensuremath{\mathcal{T}}}
\newcommand{\cZ}{\ensuremath{\mathcal{Z}}}
\renewcommand{\cG}{\ensuremath{\mathcal{G}}}
\newcommand{\Ehat}{\ensuremath{\widehat{\rE}}}
\newcommand{\sigmah}{\ensuremath{\widehat{\sigma}}}
\newcommand{\betah}{\ensuremath{\widehat{\beta}}}
\newcommand{\epsilonh}{\ensuremath{\widehat{\epsilon}}}
\newcommand{\dro}{\ensuremath{\text{DRO}}\xspace}

\newcommand{\pt}{{\tilde{t}}}
\newcommand{\pp}{{\tilde{p}}}
\newcommand{\pZ}{{\tilde{Z}}}
\newcommand{\hatd}{{\hat{\delta}}}
\newcommand{\hatc}{{\hat{Z}}}
\newcommand{\dmu}{\ensuremath{\Delta_{\mu}}}
\newcommand{\ds}{\ensuremath{\Delta_S}}
\newcommand{\br}{{\mathrm{br}}}
\newcommand{\dc}{{\mathrm{dc}}}
\newcommand{\regret}{{\mathrm{Regret}}}
\newcommand{\rR}{{\mathbb R}}
\newcommand{\tx}{{\tilde{x}}}
\newcommand{\Deltah}{\ensuremath{\hat{\Delta}}}

\newcommand{\loss}{\Omega}
\newcommand{\noise}{\epsilon}
\newcommand{\term}{\mathcal{T}}
\newcommand{\event}{\ensuremath{\mathcal{E}}}
\newcommand{\Sigmat}{\ensuremath{\widetilde{\Sigma}}}

\newcommand{\KL}{\mathrm{KL}}
\newcommand{\tv}{\mathrm{TV}}

\newcommand{\hellinger}{{D_H}}
\newcommand{\ind}{\ensuremath{\bm 1}}
\newcommand{\epsplan}{\ensuremath{\epsilon_{\text{plan}}}}
\renewcommand{\wr}{\mathrm{wr}}

\newcommand{\tw}{{\tilde{w}}}
\newcommand{\tphi}{{\tilde{\phi}}}
\newcommand{\med}{\mathrm{med}}
\newcommand{\mypar}[1]{\noindent\textbf{#1}}
\newcommand{\normal}{\ensuremath{\mathcal{N}}}
\newcommand{\logm}{\ensuremath{\omega}}
\newcommand{\deff}{\ensuremath{d_{\mathrm{eff}}}}

\newcommand{\gen}{{\pi_{\mathrm{gen}}}}
\renewcommand{\dim}{\ensuremath{\mathrm{dim}}}

%% yujia created the following
\newcommand{\E}{\mathbb{E}}
\newcommand{\calD}{\mathcal{D}}
\newcommand{\calC}{\mathcal{C}}
\newcommand{\norm}[1]{\left\|{#1}\right\|} % A norm with 1 argument
\newcommand{\calE}{\mathcal{E}}
\newcommand{\defeq}{:=}
\newcommand{\calF}{\mathcal{F}}
\newcommand{\Mt}{\mathsf{M}}
\newcommand{\D}{\mathsf{D}}
\newcommand{\bsigma}{\bar{\sigma}}
\newcommand{\vTheta}{\varTheta}
\newcommand{\frob}{\mathsf{F}}
\newcommand{\hV}{\hat{V}}% optimistic V-value
\newcommand{\mI}{\mathbf{I}}
% \newcommand{\dim}{\mathcal{Dim}}
\newcommand{\calI}{\mathcal{I}}
\newcommand{\calH}{\mathcal{H}}
\newcommand{\calG}{\mathcal{G}}
\newcommand{\ctn}{\mathsf{Catoni}}
\newcommand{\bmu}{\bar{\mu}}
\newcommand{\1}{\mathbf{1}}
\newcommand{\calT}{\mathcal{T}}
\newcommand{\calN}{\mathcal{N}}
\newcommand{\eps}{\epsilon}
\newcommand{\tf}{\tilde{f}}
\newcommand{\calA}{\mathcal{A}}
\newcommand{\tilh}{\tilde{h}}
\newcommand{\Lg}{L_\mathcal{G}}
\newcommand{\bxi}{\bar{\xi}}
\newcommand{\tg}{\tilde{g}}
\newcommand{\tsigma}{\tilde{\sigma}}
\newcommand{\bbeta}{\bar{\beta}}
\newcommand{\calS}{\mathcal{X}}
\newcommand{\calZ}{\mathcal{Z}}
\newcommand{\Z}{\mathbb{Z}}
\renewcommand{\tphi}{\tilde{\phi}}
\newcommand{\utf}{\underaccent{\tilde}{f}}
\newcommand{\lf}{\check{f}}
\newcommand{\utphi}{\underaccent{\tilde}{\phi}}
\newcommand{\tpsi}{\tilde{\psi}}
\newcommand{\bfun}{\bar{f}}
\newcommand{\bSigma}{\bar{\Sigma}}
\newcommand{\calW}{\mathcal{W}}
\renewcommand{\d}[1]{\underaccent{\dot}{#1}}
%\renewcommand{\ud}[1]{\accent{\dot}{#1}}
\newcommand{\chat}[1]{\dot{\hat{#1}}}
\newcommand{\ct}[1]{\dot{\tilde{#1}}}
\newcommand{\cbar}[1]{\dot{\bar{#1}}}
\newcommand{\mmu}{\bm{\mu}}
\newcommand{\mtheta}{\bm{\theta}}

\newcommand{\sensample}{\textsc{SenSample}}
\newcommand{\oracle}{\mathcal{B}}
\newcommand{\calTo}{\calT_{\mathrm{o}}}
\newcommand{\calToo}{\calT_{\mathrm{oo}}}
\newcommand{\xid}{\xi_{\mathrm{dif}}}
\newcommand{\lin}{\mathsf{lin}}

% Yujia added while writing paper

\newcommand{\calM}{\mathcal{M}}
\newcommand{\calP}{\mathcal{P}}
\newcommand{\calX}{\mathcal{X}}
\newcommand{\calR}{\mathcal{R}}

\newcommand{\Par}[1]{\left(#1\right)}
\newcommand{\Brack}[1]{\left[#1\right]}
\newcommand{\Brace}[1]{\left\{#1\right\}}

\newcommand{\calNb}{\mathcal{N}_b}

\newcommand{\thres}{\mathrm{trs}}
\newcommand{\sag}{\mathrm{sag}}
\newcommand{\epsc}{\eps_\mathrm{b}}
\newcommand{\epscov}{\eps_\mathrm{c}}
\newcommand{\epsb}{\eps_\mathrm{b}}
\newcommand{\Explr}{\mathcal{R}^{\exp}}
\newcommand{\calV}{\mathcal{V}}
\newcommand{\calQ}{\mathcal{Q}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\hbeta}{\hat{\beta}}
\newcommand{\hf}{\hat{f}}
\newcommand{\hVar}{\widehat{\mathrm{Var}}}
\newcommand{\Evar}{\calE_{\mathrm{var}}}
\newcommand{\Econv}{\calE_{\mathrm{conv}}}
\newcommand{\bVar}{\overline{\mathrm{Var}}}


%\newtagform{blue}{\color{red}(}{)}


%% make comments
\usepackage[textsize=scriptsize,textwidth=2cm]{todonotes}
\newcommand{\yujia}[1]{{\bf \color{purple} Yujia: #1}}
\newcommand{\tz}[1]{{\bf \color{violet} TZ: #1}}
\newcommand{\alekh}[1]{\textcolor{blue}{\textbf{AA:} #1}}
\newcommand{\chenlu}[1]{\textcolor{teal}{\textbf{Chenlu:} #1}}
\newcommand{\alg}{VO$Q$L\xspace}
\newcommand{\alglong}{Variance-weighted Optimistic $Q$-Learning\xspace}


% % remove comments
% \renewcommand{\yujia}[1]{{}}
% \renewcommand{\tz}[1]{{}}
% \renewcommand{\alekh}[1]{{}}

\title{Catoni Contextual Bandits are Robust to Heavy-tailed Rewards}
\author{
Chenlu Ye\thanks{Correspondence to Chenlu Ye}\thanks{University of Illinois Urbana-Champaign; e-mail: {\tt chenluy3@illinois.edu}}
\qquad
Yujia Jin\thanks{OpenAI (Work done during an internship at Google); e-mail: {\tt yujiajin@stanford.edu}}
\qquad
Alekh Agarwal\thanks{Google Research; e-mail: {\tt alekhagarwal@google.com}}
\qquad
Tong Zhang\thanks{University of Illinois Urbana-Champaign; e-mail: {\tt tongzhang@tongzhang-ml.org}}
}
\date{}
\usepackage{cancel}

\begin{document}

\maketitle

\begin{abstract}
Typical contextual bandit algorithms assume that the rewards at each round lie in some fixed range $[0, R]$, and their regret scales polynomially with this reward range $R$. However, many practical scenarios naturally involve heavy-tailed rewards or rewards where the worst-case range can be substantially larger than the variance. In this paper, we develop an algorithmic approach building on Catoni's estimator from robust statistics, and apply it to contextual bandits with general function approximation. When the variance of the reward at each round is known, we use a variance-weighted regression approach and establish a regret bound that depends only on the cumulative reward variance and logarithmically on the reward range $R$ as well as the number of rounds $T$. For the unknown-variance case, we further propose a careful peeling-based algorithm and remove the need for cumbersome variance estimation. With additional dependence on the fourth moment, our algorithm also enjoys a variance-based bound with logarithmic reward-range dependence. Moreover, we demonstrate the optimality of the leading-order term in our regret bound through a matching lower bound.
\end{abstract}

\section{Introduction}
Minimax optimal regret bounds in the worst-case over problem instances for contextual bandit learning are relatively well-understood in the literature, both using policy-based approaches in the agnostic case, and regression-based approaches in the realizable case. A variety of algorithms attain these bounds in both settings, and the minimax optimality implies that the bounds are unimprovable in general. When the expected reward of each action is realizable using some function class $\cF$ available to the learner, this optimal regret scales as $O(R \sqrt{T d_{\cF} \ln N_{\cF}})$, where $R$ is the range of the rewards, $T$ is the number of rounds, $d_{\cF}$ is a complexity notion for $\cF$, such as the eluder dimension \citep{russo2013eluder}, and $N_{\cF}$ is the covering number of $\cF$. 

However, this worst-case behavior arises only when the rewards span their entire range $[0, R]$ with a significant probability, a phenomenon not typical in practice. Even for a common case of binary rewards in $\{0, R\}$ for instance, the expected reward is often relatively close to $0$ in common click/no-click style recommendation settings with low clickthrough rates. Consequently, the expectation, variance and even higher moments of the reward are much smaller than the worst-case range. More generally, rewards with heavier tails naturally arise when considering waiting times in wireless communication networks \citep{nair2013fundamentals}, stock prices in financial markets \citep{cont2001empirical,hull2012risk}, or value returns for online advertising \citep{choi2020online,jebarajakirthy2021mobile}. In this paper, we study the design of contextual bandit algorithms that can leverage such structures to have regret guarantees dependent polynomially on the reward variance, with only a mild logarithmic scaling with the range parameter $R$.

\begin{table}[t]
\label{tbl:results}
\footnotesize
\centering
\caption{Comparison between different algorithms for stochastic contextual bandits, where $d$ denotes the dimension for linear function approximation, $d_\cF, \tilde{d}_\cF$ capture the complexity of the function space $\cF$ used for reward estimation, $T$ is the number of rounds, $\sigma_t$ is the variance of the observed reward at round $t$, $\sigma$ is a uniform bound on reward variance ($\sigma_t\le\sigma$ for all $t\in[T]$), $R$ is the range of rewards, and $N_\cF$ is the covering number for function class $\cF$. $\widetilde{O}$ omits terms logarithmic in $T$ and $R$.}
\vspace{5pt}
\label{tab:regret-comparison}
\begin{tabular}{cccc}
\toprule
\textbf{Algorithm} & \textbf{Function Type} 
                   & \textbf{Known Variances} 
                   & \textbf{Regret Bound}\\
\midrule
\makecell{Weighted OFUL$+$ \\ \citep{zhou2022computationally} }
    & Linear
    & \cmark 
    & $\widetilde{O}\bigl(d \sqrt{\sum_{t\in[T]}\sigma_t^2} + d{\textcolor{red} R}\bigr)$  \\[0.35cm]
\makecell{Heavy-OFUL \citep{huang2024tackling} \tablefootnote{\citet{huang2024tackling} consider a more general setting, where the $1+\epsilon$-th moment of the reward is upper bounded for some $\epsilon\in(0,1]$, and incur a dependence in terms of this moment along with additional $poly(T)$ terms. Since our work only considers bounded variance, we present their result with \citep{livariance} together, as the two results are identical for the case of $\epsilon=1$.} \\ AdaOFUL \citep{livariance}}
    & Linear 
    & \cmark
    & $\widetilde{O}\big(d\sqrt{\sum_{t\in[T]}\sigma_t^2}\big)$ \\[0.35cm]
OLS \citep{pacchiano2024second}
    & Non-linear 
    & \cmark
    & $\widetilde{O}\big(\sigma\sqrt{d_\cF\ln N_\cF} + {\textcolor{red} R}d_{\cF}\ln N_\cF\big)$ \\
\rowcolor{blue!15} 
Catoni-OFUL (Theorem \ref{th:known_var}) 
    & Non-linear 
    & \cmark 
    & $\widetilde{O}\big(\sqrt{\sum_{t\in[T]}\sigma_t^2 \cdot d_\cF\ln N_\cF} + d_\cF \ln N_\cF\big)$ \\
\midrule
SAVE \citep{zhao2023variance} 
    & Linear 
    & \xmark 
    & $\widetilde{O}\bigl(d \sqrt{\sum_{t\in[T]}\sigma_t^2} + d{\textcolor{red} R} \bigr)$ \\
DistUCB \citep{wang2024more} \tablefootnote{DistUCB relies on estimating the full reward distribution rather than just the mean, and hence requires a stronger realizability assumption on the function class to capture this distribution.}
    & Non-linear 
    & \xmark
    & $\widetilde{O}\big(\sqrt{\sum_{t\in[T]}\sigma_t^2 \cdot \tilde{d}_\cF\ln N_\cF} + {\textcolor{red} R}\tilde d_{\cF}\ln N_\cF\big)$ \\
\makecell[c]{Unknown-Variance OLS \\ \citep{pacchiano2024second}}
    & Non-linear 
    & \xmark 
    & $\widetilde{O}\bigl(d_\cF\sqrt{\sum_{t\in[T]}\sigma_t^2 \cdot\ln N_\cF} + {\textcolor{red} R}d_{\cF}\ln N_\cF \bigr)$ \\
\rowcolor{blue!15}
VACB (Theorem \ref{th:unknown_var})
    & Non-linear 
    & \xmark 
    & $\widetilde{O}\bigl(d_\cF\sqrt{\sum_{t\in[T]}\sigma_t^2 \cdot\ln N_\cF} + d_{\cF}(\ln N_\cF)^{3/4} \bigr)$ \\
\bottomrule
\end{tabular}
\end{table}

Such variance-based regret bounds have received significant attention recently, under the topic of \emph{robustness to heavy-tailed rewards}.
\citet{huang2024tackling,livariance} study Huber regression and design variance-weighted regression-based approaches for linear contextual bandits with known variance, and show that their algorithms achieve a variance-based $\widetilde{O}\big(d\sqrt{\sum_{t\in[T]}\sigma_t^2}\big)$ regret bound, where $d$ is the dimension for the linear function, thus avoiding a dependence on range $R$. They also study Markov Decision Processes (MDPs) with linear function approximation under heavy-tailed rewards with unknown variance, and use the linearity of both expected rewards and variances in linear MDPs to design a weighted regression algorithm relying on variance estimation. %the estimation of variances at each round in the variance weighting. Their algorithms obtain an $\widetilde{O}\big(d\sqrt{H \sum_{t\in[t]}\E_{\hat{\pi}_t} [\sum_{h=1}^H (\sigma_t^h)^2]} + Hd^{7/3}T^{1/4}\big)$, where $H$ is the number of transition steps, $\sigma_t^h$ is the variance for the reward and state transition at round $t$ and step $h$ for the trajectory induced by the learned policy $\hat{\pi}_t$. To ensure the performance of variance estimation, this approach assumes the realizability for the noise variance. This means that besides the existence of a function class that approximates the mean reward, we also require the noise variance be approximated by a function class, which is restrictive in realistic scenarios since the noise distribution may vary through different rounds and not a function of state-action pairs $(x,a)$. 
To the best of our knowledge, these works heavily rely on the linear function structure and are hard to extend to the non-linear setting. The general question of designing a robust contextual bandit algorithm under the heavy-tailed reward (or a reward with a large range) for general function approximation is still lacking in the literature.

A different line of work called distributional RL estimates the full reward distribution \citep{wang2024more,wang2024central,wang2024model} under the unknown variance case to achieve variance-based regret bounds with general function approximation. However, their focus is on replacing the $T$-based scaling with the cumulative variance and still incurs a polynomial dependence on $R$. Additionally, the distributional approach requires the stronger modeling assumption that the full reward distribution, rather than just the expected reward is realizable.

There are some works considering the unknown variance case for contextual bandits without realizability conditions for the noise \citep{zhang2021improved,kim2021improved,zhao2023variance,pacchiano2024second}. %\alekh{Is this accurate? Also, we should not go back and forth between MDP and contextual bandits consantly. Can we just stick to CB in this paragraph?}.
Particularly, the most relevant ones to our work for the unknown-variance setting are \citet{zhao2023variance,pacchiano2024second}. \citet{zhao2023variance} develop a peeling approach for the unknown variance case without variance estimation in linear settings, and \citet{pacchiano2024second} extend this technique to general function approximation. Nevertheless, all of these algorithms have an $O(dR)$ or $O(R d_\cF \ln N_\cF)$ term in the regret bound.  We summarize the key results from the prior literature in Table~\ref{tbl:results} to better contextualize our results, and defer additional related works to Appendix \ref{ssec:Additional Related Works}.


\subsection{Our contributions}
This work considers a different route for robustness to heavy-tailed rewards, building on the well-studied Catoni's mean estimator from the robust statistics literature.  We design a contextual bandit (CB) algorithm that uses the Catoni mean as a robust device for constructing a regression error estimator for the \textbf{excess loss}, given some function class $\cF$ for predicting the expected reward. Using the variance-dependent concentration of the Catoni estimator, we conduct a careful analysis of our algorithm and show that its regret scales as $\widetilde{O}\big(\sqrt{\sum_{t\in[T]}\sigma_t^2 \cdot d_\cF\ln N_\cF} + d_\cF \ln N_\cF\big)$, when the reward variance $\sigma_t$ is known at each round $t$. 

Since reward variance information is seldom available in practice, we refine our approach for cases with unknown variances by employing a multi-level uncertainty estimation for the expected rewards of a carefully chosen subset of actions. For this approach, we obtain regret guarantees dependent on the fourth moment of the reward, while still maintaining a logarithmic scaling in $R$. Formally, the regret scales as $\widetilde{O}\bigl(d_\cF\sqrt{\sum_{t\in[T]}\sigma_t^2 \cdot\ln N_\cF} + d_{\cF}(\ln N_\cF)^{3/4} \bigr)$. Notably, our method does not rely on some other function class to help predict the per-round variance as a function. Instead, we estimate a robust averaged variance quantity, and show that it approximates the averaged true variance up to logarithmic factors in $R$. 

Overall, our results significantly improve the state-of-the-art in variance-aware regret guarantees, that are amenable to practical reward structures. We summarize our results relative to the most relevant prior literature in Table~\ref{tbl:results}.



\section{Preliminary}
\paragraph{Notations.} For any integer $n$, we use the short-hand notation $[n]=\{1,\ldots,n\}$, and define $x_{[n]}=\{x_1,\ldots,x_n\}$. We use $\widetilde{O}$ to omit terms logarithmic in $T$ and $R$. The comprehensive tables of notations are provided in Appendix \ref{ssec:table}.

We consider a contextual bandit problem over $T$ rounds of interactions between an agent and the environment. At each round $t\in[T]$, the environment generates a decision set $\cX_t\in\cX$, where each element $x\in\cX_t$ is a candidate action for the agent. After observing $\cX_t$, the agent plays an action $x_t\in\cX_t$ and observes the reward $y_t = f^\star(x_t)+\eta_t$. Particularly, this setting subsumes classic contextual bandit where an action $a_t\in\mathcal{A}$ is chosen upon observing a context $z_t$ at round $t$, since we can always set $\cX_t = \{z_t\times \mathcal{A}\}$. We make the standard boundedness assumptions that 
\begin{equation*}
\begin{aligned}
    |\eta_t|\le R,~ \E\eta_t = 0,~ \E \eta_t^2\le \sigma_t^2.
\end{aligned}
\end{equation*}
We assume access to a function class $\calF~:~\cX\to[-R, R]$ such that $f^\star\in\calF$. For a function class $\calF$, we recall the standard definitions of $\epsilon$-cover and covering number (see e.g., \citet{wainwright2019high, TZ23-lt}) as follows.

\begin{definition}[$\upsilon$-cover and covering number]\label{def:cover}
Given a function class $\cF$, for each $\upsilon > 0$, a $\upsilon$-cover of $\cF$ with respect to $\norm{\cdot}_\infty$, denoted by $\mathcal{C}(\cF, \upsilon)$, satisfies that for any $f \in \cF$, we can find $f' \in \mathcal{C}(\cF, \upsilon)$ such that $\norm{f - f'}_\infty \leq \upsilon$. The $\upsilon$-covering number, denoted as $N(\upsilon, \cF)$, is the smallest cardinality of such a $\calC(\cF, \upsilon)$. 
\end{definition}

We assume that the function class $\cF$ consists of bounded functions, that is, $|f(x)|\le L_f$ for all $f\in\cF$ and $x\in\cX$. The variances $\sigma_t$ at each time step $t$ are not necessarily known. The (pseudo-) regret is defined as 
\begin{align*}
    R_T=\E \sum_{t\in[T]}\Big[\max_{x\in\calX_t}f^\star(x)-f^\star(x_t)\Big].
\end{align*}
To describe the structure of the general function class, we define the following (eluder dimension) quantities \citep{gentile2022achieving,russo2013eluder} as
\begin{definition}[Eluder dimension~\citep{gentile2022achieving}]\label{def:eluder}
Given a sequence of ordered actions $X=(x_1,x_2,\cdots, x_T)\in\calX_1\times\calX_2\times\cdots\times\calX_T$ and a function class $\calF$, let the eluder coefficients be
\begin{align*}
    &D^2_\calF(x,\bsigma;x_{[t-1]}, \bsigma_{[t-1]}) \defeq  \sup_{f_1,f_2\in\calF}\frac{\left(f_1(x)-f_2(x)\right)^2/\bsigma^2}{\sum_{i\in[t-1]}\left(f_1(x_i)-f_2(x_i)\right)^2/\bsigma_i^2+\lambda},\\
    &D_\calF(x;x_{[t-1]}, \bsigma_{[t-1]}) := D_\calF(x,1;x_{[t-1]}, \bsigma_{[t-1]}).
\end{align*}
Then we define the eluder dimension as:
\begin{align*}
    &\dim(\calF,X,\bsigma_{[T]}) \defeq\sum_{i=1}^T \min\left(1,D^2_\calF(x_i,\bsigma_i;x_{[i-1]}, \bsigma_{[i-1]})\right),\\
    &\dim_{\alpha,T}(\calF) \defeq\max_{X,\bsigma_{[T]}:|X|=T, \bsigma_1,\ldots,\bsigma_t\ge \alpha}\dim(\calF,X,\bsigma_{[T]}).
\end{align*}
\end{definition}
The weighted eluder coefficient $D_{\calF}^2$ describes at each time step $t$, how much the in-sample error can bound the out-of-sample error. We can illustrate the eluder quantities with linear function approximation. If the function class $\calF$ is embedded into a linear mapping $\calF=\{\theta^{\top}\phi(\cdot,\cdot):~\theta\in\R^d,~\|\theta\|_2\le B\}$, and we define the covariance matrix $\Sigma_t=\sum_{i\in[t]}x_ix_i^{\top}/\bsigma_i^2$, the weighted eluder coefficient can be simplified as
\begin{equation}\label{eq:b}
\begin{aligned}
    &D^2_\calF(x,\bsigma;x_{[t-1]}, \bsigma_{[t-1]})\\ 
    &\quad= \sup_{\theta_1,\theta_2\in\R^d}\frac{((\theta_1-\theta_2)^{\top}\phi(x)/\bsigma)^2}{\sum_{i\in[t-1]}((\theta_1-\theta_2)^{\top}\phi(x_i)/\bsigma_i)^2} \le \big\|\frac{\phi(x)}{\bsigma}\big\|^2_{\Sigma_{t-1}},
\end{aligned}
\end{equation}
where the inequality applies Cauchyâ€“Schwarz inequality. Hence, the eluder coefficient reduces to how much a direction is explored in the linear case.

The summation of eluder coefficients over $T$ time steps is the eluder dimension. The (weighted) eluder coefficients and the eluder dimension are broadly used in general function approximation \citep{zhang2023mathematical,ye2023corruption,agarwal2023vo,zhao2023nearly}. For the linear case in $d$-dmiensions, when all the $\bsigma,\bsigma_{[t-1]}$ are $1$, the $\dim_{1, T}(\cF)$ can be bounded in terms of $d\ln d$ \citep{zhang2023mathematical,agarwal2023vo}. When the weights are larger than $\alpha$, we can regard $\phi'(x)=\phi(x)/\bsigma$ as the new feature representation and bound the $\dim_{\alpha, T}(\cF)$ via the elliptical potential lemma \citep{abbasi2014online}.
%\alekh{I think we should probably cite the paper with Yujia or Tong's book for the linear example and just state that when the $\sigma$'s are all $1$, then the $\dim(\alpha, T)(\cF)$ can be bounded in terms of $d\ln d$ for linear functions in $d$-dimensions, and can be related to weighted versions of the standard elliptical potential for general $\sigma$ sequences in the linear case.} \alekh{Looks good, please replace the earlier text with this.}


\section{Bandits with Known Variance}
In this section, we present upper and lower bounds, when the per-round variance of each action is known to the learner.

\subsection{Lower Bound}
We start with a minimax lower bound for the class of multi-armed bandit problems where the variance of each action's reward is known to the learner.
\begin{theorem}\label{th:lower_bound}
    For any integer $T>0$, there exists a contextual bandit problem such that any $\pi=\{\pi_t\}_{t=1}^T$ will incur regret at least $\Omega(\sqrt{\E\sum_{t=1}^T\sigma_t^2})$, where $\{\sigma_t=\Var_{x_t\sim\pi_t}[y_t]\}_{t=1}^T$ and the expectation is jointly over any randomness in the environment as well as the algorithm.
\end{theorem}
In other words, the theorem states that the regret of any contextual bandit algorithm scales with the square root of the sum of the variances of the rewards for its chosen actions. That is, it rules out a regret bound which scales solely as the variance of the reward of the optimal policy's actions.

The detailed proof is deferred to Appendix \ref{apss:pf_lower_bound}. The intuition is to construct two bandit instances, each with two arms $\{x_1, x_2\}$. In the first instance, the arm $x_1$ has a deterministic reward while $x_2$ has a higher expected reward, but with a large variance. In the second instance, $x_2$ has a smaller mean reward. Clearly, the optimal action $x_1$ has a variance of zero in the first instance, but any algorithm needs sufficiently many draws of $x_2$ as well to distinguish between the two instances. In the sequel, we will present a matching upper bound for our robust estimator.

\subsection{Upper Bound for Known Variance}
\paragraph{Catoni Estimator}
We first introduce $\ctn$ estimator. This is a robust estimator proposed by~\citet{audibert2011robust}(see also \citep{lugosi2019mean}) to estimate random variables with bounded variance and unbounded range. Following \citet[section 2.2]{lugosi2019mean}, to estimate $t^{-1}\sum_{i\in[t]}\E Z_i$, we first define a function  
\begin{align*}
    \Psi(x) = \begin{cases}
    \log(1+x+x^2/2)\quad\text{if}~x\ge0,\\
    -\log(1-x+x^2/2)\quad\text{if}~x<0.
    \end{cases}
\end{align*}
Then for some parameter $\theta > 0$, $\ctn_\theta(\{Z_i\}_{i\in[t]})$ is the unique zero of the antisymmetric increasing function
\begin{equation}
\label{eq:catoni}
\begin{aligned}
    f(x;\{Z_i\}_{i\in[t]}, \theta) := \sum_{i\in[t]}\Psi(\theta(Z_i-x)).
\end{aligned}
\end{equation}

We first provide the following result about the concentration properties of the $\ctn$ estimator, which we use in various places to prove why our design of confidence sets in the sequel algorithms.

\begin{lemma}[Informal]\label{coro:catoni}
Let $Z_t$ be a random variable adapted to filtration the $\calH_t$, with a uniform bound $|Z_t|\le R$, $\E[Z_i|\calH_{i-1}] = \mu_i$, $\sum_{i\in[t]} \E\left[\left(Z_i-\mu_i\right)^2|\calH_{i-1}\right] \le V$ for some fixed $V$. Let $\bmu\defeq t^{-1}\sum_{i\in[t]}\mu_i$. Let $\theta\in[a, A]$ be a parameter, for some constants $a, A$ independent of $Z_i$. For an appropriate $\epsilon$ and any large enough $t$, 
with probability at least $1-2\delta$ we have uniformly for all $\theta \in [a, A]$:
\begin{equation*}
{
\begin{aligned}
\left|\ctn_\theta(\{Z_i\}_{i\in[t]})-\bmu\right|
\le& \frac{\theta\left(V+\sum_{i\in[t]}\left(\mu_i-\bmu\right)^2\right)}{t}+\frac{4 \iota_0^2}{\theta t}+\frac{\epsilon}{t},
\end{aligned}}
\end{equation*}
where $\iota_0$ contains log terms and is given in Appendix \ref{apss:Concentration Inequality for Catoni Estimator}.
\end{lemma}
This inequality differs from the prior concentration results for the Catoni estimator as it is uniform for all $\theta \in [a, A]$. In the sequel, we use this flexibility to choose $\theta$ based on the samples. The formal version of the lemma and the proof are deferred to Appendix \ref{apss:Concentration Inequality for Catoni Estimator}.
 
\paragraph{Algorithm}
\begin{algorithm}[t]
\caption{Catoni-OFUL}\label{alg:Catoni-VOFUL}
\small
\begin{algorithmic}
    \STATE \textbf{Input:} Parameter $\alpha>0$, $\delta$ and $\hbeta_t$ for each $t\in[T]$.
    \FOR{t=1,2,\ldots,T}
        \STATE Pick action $x_t=\argmax_{x\in\calX_t}\max_{f\in\calF_{t-1}} f(x)$;
        \STATE Observe the reward $y_t$;
        \STATE Let $\bsigma_t = \max\left(\alpha,\sigma_t, \sqrt{4\iota(\delta)L_f D_{\calF_{t-1}}(x_t; x_{[t-1]},\bsigma_{[t-1]})}\right)$;
        \STATE Estimate $\hf_t$ in \eqref{def:catoni-esti};
        \STATE Construct confidence set 
        \begin{align*}
            \calF_t \defeq \Big\{f\in\calF_{t-1}:\sum_{i\in[t]}\frac{1}{\bsigma_i^2}\left(f(x_i)-\hat{f}_t(x_i)\right)^2\le \hbeta_t^2\Big\};
        \end{align*}        
    \ENDFOR
\end{algorithmic}
\end{algorithm}

By incorporating the Catoni estimator into the Optimism in the Face of Uncertainty Learning (OFUL)\citep{abbasi2011improved}, we propose the Catoni-OFUL approach in Algorithm \ref{alg:Catoni-VOFUL}. Given failure probabilities $\delta$ and confidence parameters $\hbeta_t$, the algorithm chooses the action $x_t$ with the highest optimistic reward by maximizing across all functions in a confidence set $\cF_t$, as in the standard OFUL approach. 

The key difference lies in the construction of a robust confidence set based on Catoni's mean estimator. We first define a per-sample weight $\bsigma_t$
as the maximum of a parameter $\alpha$, the variance $\sigma_t$ of the reward of $x_t$, and an uncertainty term based on the eluder coefficient $D_{\calF_{t-1}}(x_t; x_{[t-1]},\bsigma_{[t-1]})$. 

Then, we define a robust estimator of $f^\star$, given the data, as the solution to the following saddle-point problem: 
\begin{align}\label{def:catoni-esti}
    \hf_t =& \argmin_{\hf\in\calF}\max_{f'\in\calF} L_t(\hf,f') := \sum_{i\in[t]}\frac{1}{\bsigma_i^2}(f'(x_i) - \hf(x_i))^2 + 2t\ctn_{\theta_t(\hf,f')}(\{Z_i(\hf,f')\}_{i\in[t]}),
\end{align}
%\alekh{We should use the full function class in the algorithm and the cover in the analysis. We cannot implement algorithms over covering sets.}
where we use the notation $Z_i(f,f') := \bsigma_i^{-2}(f(x_i)-f'(x_i)) (f'(x_i)-y_i)$, $\iota(\delta)$ scales as $\widetilde{O}(\sqrt{\log(1/\delta)})$ and is specified in Table \ref{tab:notation_known}, and the parameter $\theta_t(\hf,f')$ is also specified in Table \ref{tab:notation_known}.
%\alekh{We should not give the actual defn of $\theta$ here. This is not important and is anyways relying on other undefined notation. We can just say that it is a parameter which is specified in the appendix.}

To understand this definition, we observe that $L_t(f,f')$ is a robust sample-based estimator of the true excess risk:
\begin{equation}
\begin{aligned}
    R_t(f,f') :=& \sum_{i\in[t]}\frac{1}{\bsigma_i^2}\E_i[(f(x_i)-y_i)^2] - \E_i[(f'(x_i)-y_i)^2]\\
    =& \sum_{i\in[t]} \frac{1}{\bsigma_i^2} \E_i\bigg[ (f(x_i)-f'(x_i))^2 + 2\underbrace{(f(x_i)-f'(x_i))(f'(x_i)-y_i)}_{\mathcal{I}_i} \bigg],\label{eq:R_t}
\end{aligned}
\end{equation}
where the expectation $\E_i$ is taken with respect to the noise $\eta_i$. Since $\mathcal{I}_i$ is the only term that depends on the heavy-tailed noise, it is approximated by the $\theta$-robust Catoni estimator $\ctn_{\theta}(\{Z_i(f,f')\}_{i\in[t]})$ in $L_t(f,f')$. Then, we include all the $f\in\calF_{t-1}$ that have a small weighted squared loss to $\hf_t$ in the confidence set $\calF_t$.

\begin{remark}
    Since the min-max optimization in \eqref{def:catoni-esti} can be hard to solve, we provide an alternative (Algorithm \ref{alg:Catoni-OFUL with Candidate Set} in Appendix \ref{aps:Construct Two Confidence Sets}), where we construct a candidate set first similar to the confidence set, and then choose an estimator from the candidates randomly. This approach can improve the optimization efficiency and ensure the same regret bound as Theorem \ref{th:known_var}.
\end{remark}

% \begin{remark}
%     since this general design principle is not so clear from prior linear specific works. We should also discuss the connection with them once we finish setting up the algorithm. xxx
% \end{remark}

%\alekh{I would simplify by saying that for appropriate choices of the parameters $\alpha, \lambda, \upsilon$ and $\hbeta$, we have}
\begin{theorem}[Informal]\label{th:known_var}
Under Algorithm~\ref{alg:Catoni-VOFUL} with appropriate choices of the parameters $\alpha, \lambda, \upsilon$ and $\hbeta$,
with probability $1-2\delta$, we can bound the regret by 
\begin{align*}
    R_T =& \widetilde{O}\Big(L_f\sqrt{\sum_{t\in[T]}\sigma_t^2\cdot \dim_{\frac{1}{\sqrt{T}}, T}(\calF)\cdot\log \calN(\calF,\upsilon)} +L_f\cdot \log \calN(\calF,\upsilon)\cdot \dim_{\frac{1}{\sqrt{T}},T}(\calF)\Big).
\end{align*}
\end{theorem}
The formal version of the theorem and appropriate choices of the hyperparameters are provided in Appendix \ref{apss:pf_th1}. The variance dependence in our theorem matches the lower bound in Theorem \ref{th:lower_bound}. Specifically, for the deterministic case where $\sigma_t=0$ for all $t\in[T]$, the bound is reduced to $\widetilde{O}(\log \calN(\calF,\upsilon)\cdot \dim_{\frac{1}{\sqrt{T}},T}(\calF))$, and in the worst case where $\sigma_t=\Theta(1)$ for all $t\in[T]$, the bound becomes $\widetilde{O}(\sqrt{T\dim_{\frac{1}{\sqrt{T}}, T}(\calF)\cdot\log \calN(\calF,\upsilon)})$. We note that the bound depends only polylogarithmically on $R$, improving upon most prior results as observed in Table~\ref{tbl:results}.

\subsection{Proof Sketch}
To illustrate the intuition clearly, we ignore the covering number in this subsection, and assume that the function space $\calF$ is finite. The detailed proof considers an infinite function space and uses the uniform covering number. The novelty of the proof lies in the following two parts.

\paragraph{Part I: Concentration of excess loss}
Recall that for any $f,f'\in\calF$, we formulate the excess loss $L_t(f,f')$ to estimate the excess loss $R_t(f,f')$ in \eqref{eq:R_t} under heavy-tailed noise. Here the conditional expectation of the variable $Z_i(f,f')$ is $\E[Z_i|x_i]=(f(x_i)-f'(x_i))(f'(x_i)-f^\star(x_i))/\bsigma_i^2$. Since the standard Hoeffding's inequality leads to the error dependent on the uniform noise bound $R$, which can be extremely large in our setting, we can utilize the robustness of the Catoni estimator via Lemma \ref{coro:catoni} to obtain the following lemma.
\begin{lemma}\label{lm:conv_ctn}
    For all large enough time steps $t$ and two fixed $f,f'\in\calF_{t-1}$, with a proper choice of parameters $\alpha$, $\hbeta_t$ and $\epsilon$, we have with probability at least $1-\delta/N^2$,
    \begin{equation}
    {
    \begin{aligned}
    \Big|L_t(f,f') - R_t(f,f') \Big| &= 2\Big|t\ctn_{\theta_t(f,f')}(\{Z_i(f,f')\}_{i\in[t]}) - \sum_{i\in[t]}\E[Z_i|x_i] \Big|\notag\\
    &\le \frac{1}{2}V_t(f,f') + \frac{1}{3}\hbeta_t^2,
    \end{aligned}}
    \end{equation}
    where $V_t(f,f')=\sum_{i\in[t]}(f(x_i)-f'(x_i)))^2/\bsigma_i^2$.
\end{lemma}
The values of the parameters are shown in Lemma \ref{lm:conv_ctn_formal}, and the proof is deferred to Appendix \ref{apss:pf_th1}. Importantly, $\hbeta_t^2$ in the lemma above only has $\log R$ dependence. %\alekh{I don't think that the notation $V_t$ is introduced anywhere, we neeed to define it at least informally.}

\paragraph{Part II: Sharpness of the confidence set} Next, we show that for our choice of $\hbeta_t$, the true function $f^\star\in\calF_t$ with a high probability for $t\in[T]$ that are appropriately large. To prove this, we define $L_t(f):=\max_{f'\in\calF} L_t(f,f')$, apply Lemma \ref{lm:conv_ctn} with $f=\hf_t$, and take a minimum over $f\in\calF$ on both sides of the inequality to get the following result.
\begin{lemma}\label{lm:L_f(f)}
Under the conditions of Lemma \ref{lm:conv_ctn}, we have for all large enough $t\in[T]$ with probability at least $1-\delta$,
\begin{align*}
    L_t(\hf_t) \ge&  \min_{f'\in\calF} \Big\{V_t(\hf_t,f^\star) - V_t(f',f^\star) - \frac{1}{2}V_t(\hf_t,f') - \frac{1}{3}\hbeta_t^2\Big\}\\
    =& \frac{2}{3}V_t(\hf_t,f^\star) - \frac{1}{3}\hbeta_t^2,
\end{align*}
where the minimizer for $f'$ is $f'_{\min} = \frac{2}{3}f^\star + \frac{1}{3}\hf_t$.
\end{lemma}
The proof is deferred to Appendix \ref{apss:pf_th1}. An analogous argument also yields an upper bound $L_t(f^\star) \leq \hbeta_t^2/3$, as shown in Appendix~\ref{apss:pf_th1}. Furthermore, since $\hf_t$ is the minimizer of $L_t(\cdot)$, we have
\begin{align*}
    0 \ge L_t(\hf_t) - L_t(f^\star) \ge \frac{2}{3}V_t(\hf_t,f^\star) - \frac{2}{3}\hbeta_t^2,
\end{align*}
which leads to $f^\star\in\calF_t$.

Ultimately, if the event $f^\star\in\calF_t$ for a large enough $t$ happens, the regret can be bounded by using the definition of $\hbeta_t$ and the definition of the eluder dimension. Since this part is standard, we defer the details to Appendix \ref{apss:pf_th1}.


\section{Bandits with Unknown Variance}
In this section, we generalize to the case where the noise variance $\E\eta_t^2$ for any $t\in[T]$ is unknown. In addition to the assumption that for any $f\in\calF$, $\|f\|_{\infty}\le L_f$, and $\|f^{\star}\|_{\infty}\in[0,1]$, the following condition for the noise variance is required.
\begin{assumption}\label{ass:var_peel}
    For each time step $t\in[T]$, the noise $\eta_t$ satisfies that there exist positive constants $\sigma_{\eta}$ and $c_{\eta}$ such that $\E[\eta_t|\calF_t]=0,~\E\eta_t^2=\sigma_t^2\le \sigma_{\eta}^2$ and $\Var[\eta_t^2|\calF_t] \le c_{\eta} \Var[\eta_t|\calF_t]$.
\end{assumption}


\subsection{Algorithm}

\begin{algorithm}[t]
\small
\caption{Variance-Agnostic Catoni Bandit}
\label{alg:Peeling}
\begin{algorithmic}[1]
    \STATE \textbf{Input:} Parameter $\gamma>0$, $L=\lceil \log_2(1/\gamma) \rceil$, $l_\star=\lceil \log_2(1076\iota'(\delta)) \rceil$.
    \STATE Initialize the estimators for all layers: $\lambda^l\leftarrow 2^{-2l},~\hbeta_0^l\leftarrow 2^{-l+1},~\Psi_0^l\leftarrow\emptyset$ for all $l\in[l_\star,L]$.
    \FOR{t=1,\ldots,T}
        \STATE Observe $\cX_t$, and initialize $\calX_t^1\leftarrow\cX_t,~l\leftarrow l_\star$.
        \WHILE{$x_t$ is not specified}
            \IF{$D_t^l(x)\le\gamma$ for all $x\in\cX_t^l$}
                \STATE Choose $x_t,f_{t-1}^l \leftarrow\argmax_{x\in\cX_t^l,f\in\calF_{t-1}^l} f(x)$ \STATE Observe $y_t$.
                \STATE \textbf{Break.}
                %\alekh{We should break after this, right?}
            \ELSIF{$D_t^l(x)\le 2^{-l}$ for all $x\in\cX_t^l$}
                \STATE Update $\cX_t^{l+1} \leftarrow \{x\in\cX_t^l \mid \hf_{t-1}^l(x) \ge \max_{x\in\cX_t^l} \hf_{t-1}^l(x) - 2^{-l+1}\hbeta_{t-1}^l\}$.
            \ELSE
                \STATE Choose $x_t\in\cX_t^l$ such that $D_t^l(x_t)> 2^{-l}$ and observe $y_t$.
                \STATE Update $w_t \leftarrow 2^l D_t^l(x_t)$.
                \STATE Update the index sets: $\Psi_{t}^l \leftarrow \Psi_{t-1}^l \cup \{t\}$ and $\Psi_{t}^{l'}\leftarrow \Psi_{t-1}^{l'}$ for $l'\ne l$.%\alekh{Should we have $\Psi_{t+1}$ on the left of assignments?}
                \STATE Optimize $\hat{f}_t^l$ as in \eqref{def:catoni_esti_peel}, and choose the confidence set $\calF_t^l$ defined in \eqref{eq:confidence_set_peel}.
            \ENDIF
            \STATE Update $l\leftarrow l+1$.
        \ENDWHILE
        \STATE For $l\in[L]$ s.t. $\Psi_t^l=\Psi_{t-1}^l$, $\hat{f}_t^l \leftarrow \hat{f}_{t-1}^l,~\calF_t^l \leftarrow\calF_{t-1}^l$.
    \ENDFOR
\end{algorithmic}
\end{algorithm}

Since variances $\sigma_t$ are unknown, traditional variance weighting techniques necessitate an accurate estimation for the noise variance at each time step \citep{huang2024tackling, livariance}. To circumvent the complicated variance estimation, we adapt the SupLinUCB-type \citep{chu2011contextual} algorithm with adaptive variance-aware exploration from \citet{zhao2023variance} to propose Variance-Agnostic Catoni Bandit (VACB) in Algorithm \ref{alg:Peeling}, where we split the contexts $\{x_t\}_{t\in[T]}$ into $L$ subsets according to their uncertainty. For each level $l\in[L]$, let $\Psi_t^l$ denote the set of time indexes within $[t]$ when the estimator update happens. Specifically, we use the following short-hand notation of uncertainty with respect to history information in $\Psi_t^l$: for any $x\in\cX$,
\begin{equation}
{
\begin{aligned}\label{eq:uncertainty}
    D_t^l(x) = \sup_{f,f'\in\calF_{t-1}^l} \frac{|f(x)-f'(x)|}{\sqrt{\sum_{i\in\Psi_{t-1}^l}(f(x_i)-f'(x_i))^2/w_i^2 + \lambda^l}}.
\end{aligned}}
\end{equation}
At each time step $t$, starting from $l=l_\star$, if there exists a decision $x\in\cX_t^l$ with sufficiently large uncertainty $D_t^l(x_t)> 2^{-l}$,
%\alekh{Do we use $\calF$ or $\calF_t^l$ in the uncertainty here?} \alekh{The $\cX_t^l$ and $D_t^l$ notations look very similar. I would use a different symbol like $\mathcal{X}_t^l$ for the decision set.}
this decision will be chosen; otherwise, all the actions $x\in\cX_t^l$ that are far from the optimal reward $\max_{x\in\cX_t^l}\hf_{t-1}^l(x)$ are eliminated, and the remaining actions compose the decision set $\cX_t^{l+1}$ at the next level. The process does not stop until (a) there exists an action with large uncertainty; (b) or the uncertainty of all the remaining decisions is small ($D_t^l(x_t)\le\gamma$ for all $X_t\in\cX_t^l$). If case (a) happens, we will construct the estimation for the current layer. Specifically, for each level $l$, the variance estimator
$\hVar_t^l$ uses plug-in:
\begin{equation}
\label{eq:def_hatvar}
{
\begin{aligned}
\hVar_t^l :=& t\ctn_{\theta_\Var^{t,l}}\Big(\Big\{\frac{1}{w_i^2}(y_i-\hf_
{t-1}^l(x_i))^2\Big\}_{i\in\Psi_t^l}\Big) + \hat{b}_t^l,
\end{aligned}}
\end{equation}
where the detailed choice of bonus $\hat{b}_t^l$, the parameter $\theta_\Var^{t,l}$ and $\iota'(\delta)$ are provided in Table \ref{tab:notation_unknown}.
Then, the function estimation follows Algorithm \ref{alg:Catoni-VOFUL}:
\begin{equation}
\label{def:catoni_esti_peel}
\begin{aligned}
     \hf_t^l =& \argmin_{\hf\in\calF_{t-1}^l}\max_{f'\in\calF_{t-1}^l} L_t^l(\hf,f')
     := \sum_{i\in\Psi_t^l}\frac{1}{w_i^2}(f'(x_i) - \hf(x_i))^2 + 2t\ctn_{\theta'_t(\hf,f')}(\{Z_i(\hf,f')\}_{i\in\Psi_t^l}),
\end{aligned}
\end{equation}
where $Z_i(f,f') := (f(x_i)-f'(x_i)) (f'(x_i)-y_i)/w_i^2$, the parameter $\theta'_t(f,f')$ is given in Table \ref{tab:notation_unknown}. Essentially, the weight $w_i$ can substitute the per-round variance in normalizing the loss $L_t^l$, as we will show in Lemma \ref{lm:variance} that $\hVar_t^l$ can be upper and lower bounded by the true variance $\sum_{i\in\Psi_t^l}\sigma_i^2/w_i^2$ up to additive and multiplicative constants. Furthermore, since $\hVar_t^l$ appears in the Catoni-mean's concentration (Lemma~\ref{coro:catoni}) for $t^{-1}\sum_{i\in\Psi_t^l} Z_i(f, f')$, we see that normalizing the losses with $w_i$ results in variance-aware concentration just like the known variance case.

For the two parameters require knowledge of the cumulative variance, we substitute the true summation with the optimistic variance estimator $\hVar_t^l$: one is $\theta'(f,f')$ defined above, and the other is $\hbeta_t^l$, which is iteratively computed:
\begin{equation}\label{eq:hbeta_tl}
{
\begin{aligned}
(\hbeta_t^l)^2 = \Theta\Big((\iota'(\delta))^22^{-2l}\hVar_t^l + \iota'(\delta)2^{-2l} + \Delta_{\upsilon} + \lambda^l\Big),
\end{aligned}}
\end{equation}
where the specific value of $\hbeta_t^l$ and $\Delta_\upsilon$ is provided in Table \ref{tab:notation_unknown}, and $\Delta_\upsilon$ is a small term depending on the parameter $\upsilon$ for the $\upsilon$-cover in Definition \ref{def:cover}.

In summary, our algorithm needs to estimate only an aggregate variance instead of estimating the per-round variance exactly, as we would require for applying a variance-weighted directly in the agnostic setting. This requires access to another function class that can model variances, which we cleanly avoid. Finally, we define the confidence set
\begin{equation}\label{eq:confidence_set_peel}
{
\begin{aligned}
    \calF_t^l & \defeq \Big\{f\in\calF_{t-1}^l:\sum_{i\in\Psi_t^l}\frac{1}{w_i^2}\left(f(x_i)-\hat{f}_t^l(x_i)\right)^2 + \lambda^l\le (\hbeta_t^l)^2\Big\}.
\end{aligned}}
\end{equation}


\subsection{Analysis}
\begin{theorem}[Informal]\label{th:unknown_var}
Suppose that Assumption \ref{ass:var_peel} holds. With appropriate choices of $\gamma,~\iota'(\delta),~\upsilon$ and $\hbeta_t^l$, if $T$ is large enough, with probability at least $1-3\delta$, we can bound the regret of Algorithm \ref{alg:Peeling} by
\begin{align*}
R_T =& \widetilde{O}\bigg(L_f\Big(\sum_{t\in[T]}\sigma_t^2 \cdot \log \calN(\calF,\upsilon)\Big)^{1/2} \cdot \dim_{1,T}(\calF) + L_f\dim_{1,T}(\calF)(\log \calN(\calF,\upsilon))^{3/4}(\sqrt{c_{\eta}} + \sigma_{\eta})\bigg).
\end{align*}
\end{theorem}
This theorem provides a variance-dependent upper bound when variances are unknown, which matches the upper bound when variances are observed (Theorem \ref{th:known_var}) up to a slightly worse dependence on the eluder dimension. The higher order dimension term arises in the analysis of the peeling technique. 

When compared to the upper bound $\widetilde{O}(d\sqrt{\sum_{t\in[T]}\sigma_t^2} + d^{3.5}T^{1/4})$ \citep{livariance} for the linear setting with dimension $d$ and unknown variance, our bound gets rid of the dependence on $T^{1/4}$, which means that our bound is still optimal up to the dimension when the sum of variances is small: $\sum_{t\in[T]}\sigma_t^2 = o(\sqrt{T})$. We give more intuition on why our algorithm admits this sharper bound in the proof sketch below, with details deferred to Appendix \ref{aps:pf_unknown_var}. 

\paragraph{Proof Sketch} 
The main challenges for the variance-agnostic algorithm are: (I) how to obtain the concentration inequalities when the weights are not based on the noise variance; (II) how to make accurate substitutions for the sum of variance $\sum_{i\in[t]}\sigma_i^2/w_i^2$ in the parameters; and (III) how to deal with the regret of each level $l$. The insight of solving challenges (I) and (III) basically follows previous work \citep{zhao2023variance,pacchiano2024second}, but for (II), because of the heavy-tailed setting, our contribution is designing the robust Catoni variance estimator, and demonstrating the estimator almost has the same order as the true average-variance $\sum_i\sigma_i^2/w_i^2$ up to constants with logarithmic dependence on the reward range $R$.  We address these challenges in the following three parts, respectively. 

\paragraph{Part I: Average variance bound for concentration} 
In this part, we study the concentration of the excess loss for each level. For clearer illustration, we omit level $l$ when there is no confusion and denote $D_i=D_i^l(x_i)$ for short. Distinct from the known variance case where one directly takes the variance $\sigma_t$ as weights to derive an upper bound for the variance of $Z_i(f,f')$, we start with an alternate bound in terms of the weights $w_i$:
\begin{equation*}
{
\begin{aligned}
    S_t &:= \sum_{i\in\Psi_t} \Var[Z_i(f,f')]\\
    &= \sum_{i\in\Psi_t}\E\Big[\frac{1}{w_i^2}(f(x_i)-f'(x_i))^2(f^\star(x_i)-y_i)^2 \,\Big|\, x_i\Big]\\
    &\le \sum_{i\in\Psi_t}\frac{(f(x_i)-f'(x_i))^2}{w_i^2} \cdot \frac{\sigma_i^2}{w_i^2}.
\end{aligned}}
\end{equation*}
When the variance is known, and $w_i = \sigma_i$ as in Algorithm~\ref{alg:Catoni-VOFUL}, the second term in the final inequality is uniformly equal to $1$.

When the variance is unknown, we can no longer weight the variances, instead, we uniformly bound the first term and aggregate the second term as shown below:
\begin{equation*}
{
\begin{aligned}
    S_t \le& \max_{i\in\Psi_t} \frac{(f(x_i)-f'(x_i))^2}{w_i^2} \cdot \sum_{i\in\Psi_t}\frac{\sigma_i^2}{w_i^2}\\
    \le& \underbrace{\max_{i\in\Psi_t} \frac{D_i^2}{w_i^2}\cdot \Big(\sum_{\tau\in[i-1]}\frac{(f(x_\tau)-f'(x_\tau))^2}{w_\tau^2}+\lambda\Big)}_{\displaystyle\text{Uniform~bound}~\le 2^{-2l}\cdot4\hbeta_{t-1}^2} \cdot \sum_{i\in\Psi_t}\frac{\sigma_i^2}{w_i^2},
\end{aligned}}
\end{equation*}
where the first inequality uses the definition of $D_i$, and the uniform bound holds because $D_i/w_i\le2^{-l}$ at level $l$ from Algorithm \ref{alg:Peeling}. Also $f,f'\in\calF_{t-1}$ implies that
\begin{equation*}
{
\begin{aligned}
    \sum_{\tau\in[i-1]}\frac{(f(x_\tau)-f'(x_\tau))^2}{w_\tau^2} \le& 2\sum_{\tau\in[t-1]}\frac{(f(x_\tau)-\hf_{t-1}(x_\tau))^2}{w_\tau^2} + 2\sum_{\tau\in[t-1]}\frac{(f'(x_\tau)-\hf_{t-1}(x_\tau))^2}{w_\tau^2}\\
    \le& 4\hbeta_{t-1}^2.
\end{aligned}}
\end{equation*}
Here the first inequality applies the Cauchy-Schwarz inequality, and the second inequality follows from the definition of $\cF_{t-1}$.

Therefore, by not requiring a uniform bound on the closeness of the variances and the weights, we can successfully derive the concentration inequality for $L_t(f,f') - R_t(f,f')$.
\begin{lemma}\label{lm:concen_ctn_peel_0}
    Under Assumption \ref{ass:var_peel} and Algorithm \ref{alg:Peeling}, we have $\sup_{i\in\Psi_t}D_i/w_i \le2^{-l}:=\rho$, $w_i\ge 1$. Then, for a large enough $t\in[T]$ and for any $f,f'\in\calF_{t-1}$, with probability at least $1-\delta/TL$,
    \begin{equation*}
    {
    \begin{aligned}
        &\Big|L_t(f,f') - R_t(f,f') \Big|
       \le\frac{1}{2}V_t(f,f') +\frac{1}{2}\hbeta_{t-1}^2 + O\bigg((\iota'(\delta))^2\rho^2 \Big(\frac{\big(\sum_{i\in\Psi_t}\sigma_i^2/w_i^2\big)^2}{\hVar_t} + \hVar_t\Big)+\Delta_{\upsilon}\bigg).
    \end{aligned}}
    \end{equation*}
\end{lemma}
The more involved version of this lemma and the proof is presented in Appendix \ref{apss:th:unknown_var}. Note that the variance estimator $\hVar_t$ appears on the right-hand side of the inequality above because the parameter $\theta'_t(\hf,f')$ cannot be directly set in terms of $\sum_{i\in\Psi_t}\sigma_i^2/w_i^2$, and we instead use the surrogate $\hVar_t$. The $\hVar_t$ dependence will be eliminated after demonstrating the close relationship between $\hVar_t$ and $\sum_{i\in\Psi_t}\sigma_i^2/w_i^2$ in the next part.

\paragraph{Part II: Accuracy of variance estimation.} For any $t\in[t]$, let $V_i(\hf_{t-1}) = (y_i-\hf_{t-1}(x_i))^2/w_i^2$. The conditional expectation of this term is 
\begin{align*}
    \E[V_i(\hf_{t-1})|x_i]=(\sigma_i^2 + (f^{\star}(x_i)-\hf_{t-1}(x_i))^2)/w_i^2.
\end{align*}
Then, by using Lemma \ref{coro:catoni} and using an argument similar to Part I, we can control the concentration error
\begin{equation*}
{
\begin{aligned}
    &\Big|t\ctn_{\theta_\Var^{t}}\big(\big\{V_i(\hf_{t-1})\big\}_{i\in\Psi_t}\big) - \sum_{i\in\Psi_t}\E[V_i(\hf_{t-1})|x_i]\Big|.
\end{aligned}
}
\end{equation*}
Hence, it suffices to bound the gap between the variance and the expectation:
\begin{equation*}
{
\begin{aligned}
    &\Big|\sum_{i\in\Psi_t}\E[V_i(\hf_{t-1})|x_i] - \sum_{i\in\Psi_t} \frac{\sigma_i^2}{w_i^2}\Big|\\
    &= \sum_{i\in\Psi_t}\frac{(f^{\star}(x_i)-\hf_{t-1}(x_i))^2}{w_i^2} \\
    &= \sum_{i\in\Psi_t}\frac{(\hf_{t-1}(x_i)-f^\star(x_i))^2}{w_i^2} + \frac{(\hf_{t-1}(x_t)-f^\star(x_t))^2}{w_t^2}\\
    &\le \hbeta_{t-1}^2 + \frac{D_t^2}{w_t^2}\cdot\hbeta_{t-1}^2 \le (1+2^{-2l})\hbeta_{t-1}^2,
\end{aligned}
}
\end{equation*}
where the first inequality uses $f^\star\in\calF_{t-1}$, the definition of $D_i$ and $D_i/w_i=2^{-l}$. %\alekh{Should we use $\rho^2$ for consistency? Also, why does the first term in the inequality sum over [t-1] instead of $\Psi{t-1}^l$?}

Recalling the definition~\eqref{eq:def_hatvar} of $\hVar_t$, we derive the following accuracy guarantee of this estimate compared with the true aggregated weighted variance:
\begin{lemma}\label{lm:variance}
Under Algorithm \ref{alg:Peeling} and the condition that $f^\star\in\cF_{t-1}^l$, when $2^l$ is large enough, we have with probability at least $1-2\delta$ for all large enough $t\in[T]$,
    \begin{equation}
    {
    \begin{aligned}
     &\sum_{i\in\Psi_t} \frac{\sigma_i^2}{w_i^2} \le 2\hVar_t,\\
     &\hVar_t \le \frac{3}{2}\sum_{i\in\Psi_t} \frac{\sigma_i^2}{w_i^2} + O\Big(\iota'(\delta)(\sigma_\eta^2+c_\eta) + \Delta_{\upsilon}+\lambda\Big).\label{eq:var}
    \end{aligned}
    }
    \end{equation}
\end{lemma}

\paragraph{Part III: Bounding the regret for each level $l$.}
Conditioning on the high-probability events, we can show that for any time step $t\in\Psi_T^l$, the true optimal decision $x_t^\star=\argmax_{x\in\calX_t}f^\star(x)$ remains in the candidate set $\calX_t^l$ during the level-wise elimination in Algorithm \ref{alg:Peeling}, where $l$ is the level from which $x_t$ arises. By the definition of $\cX_t^l$, we know that $x_t$ cannot be far from $x_t^\star$, thus we can demonstrate the following lemma.
\begin{lemma}\label{lm:regret_l}
Under Assumption \ref{ass:var_peel} and Algorithm \ref{alg:Peeling}, if $f^\star\in\cF_{t-1}^l$ and \eqref{eq:var} hold for all large enough $t,l$, then, for all large enough $l$ and $t\in\Psi_T^l$, we have:
\begin{align*}
    x_t^\star \in\cX_t^l,
\end{align*}
    and the regret at the $l$-th level is bounded by
    $$
    \sum_{t\in\Psi_T^l:t\ge \Theta(1)} (f^\star(x_t^\star) - f^\star(x_t) )\le 2^{-l+3}\hbeta_T^{l-1}\cdot|\Psi_T^l|.
    $$
\end{lemma}
The details and the proof of the lemma are deferred to Appendix \ref{apss:th:unknown_var}.
% We demonstrate in Lemma \ref{lm:regret_l} that for any time step $t\in\Psi_T^l$, the true optimal decision $x_t^\star=\argmax_{x\in\calX_t}f^\star(x)$ remains in the candidate set $\calX_t^l$ during the level-wise elimination in Algorithm \ref{alg:Peeling}, where $l$ is the level from which $x_t$ arises. Thus, we get from Algorithm \ref{alg:Peeling} that $D_t^{l-1}(x_t^*)\le 2^{-l+1},~ D_t^{l-1}(x_t)\le 2^{-l+1}$, and 
% \begin{equation*}
% {
% \begin{aligned}
%      \hf_{t-1}^{l-1}(x_t^*) - \hf_{t-1}^{l-1}(x_t) \le 2^{-l+2}\hbeta_t^{l-1},
% \end{aligned}
% }
% \end{equation*}
% which imply that
% \begin{equation*}
% {\footnotesize
% \begin{aligned}
%     &f^\star(x_t^*) - f^\star(x_t)\\
%     &\quad\le \hf_{t-1}^{l-1}(x_t^*) - \hf_{t-1}^{l-1}(x_t) + |f^\star(x_t^*) - \hf_{t-1}^{l-1}(x_t^*)|\\
%     &\qquad + |f^\star(x_t) - \hf_{t-1}^{l-1}(x_t)|\\
%     &\quad\le 2^{-l+2}\hbeta_t^{l-1} + D_t^{l-1}(x_t^*)\hbeta_t^{l-1} + D_t^{l-1}(x_t)\hbeta_t^{l-1}\\
%     &\quad\le 2^{-l+3}\hbeta_t^{l-1}.
% \end{aligned}
% }
% \end{equation*}
Hence, it suffices to bound the size of $\Psi_T^l$. We get via $D_t^l(x_t)/w_t=2^{-l}$ that
\begin{equation*}
{
\begin{aligned}
   |\Psi_T^l| =& 2^{2l}\cdot\sum_{i\in\Psi_{T,l}} \frac{(D_t^l(x_i))^2}{w_i^2} \le 2^{2l}\dim_{1,T}(\calF).
\end{aligned}
}
\end{equation*}
Ultimately, by combining the results above the choice of $\hbeta_t^{l-1}$ in \eqref{eq:hbeta_tl}, we can obtain the bound for each $l$. The final regret is obtained by summing the regret for level $l=l_*,\ldots,L$.


\section{Conclusion}
In this work, we consider contextual bandits under heavy-tailed rewards (rewards with a large range $R$) with general function approximation. The key novelty of our approach is the application of Catoni's mean estimator for non-linear settings based on the observation that excess loss estimation is the correct object to robustify. For the known-variance case, the Catoni-OFUL algorithm combines the adaptive Catoni estimator and the variance-weighted optimization. The algorithm enjoys a variance-based regret bound with only polynomial dependence on $R$. When the per-round variance is unknown, our proposed variance-agnostic Catoni bandit algorithm carefully peels the samples based on their uncertainty and utilizes a plug-in estimator for the sum of variances. The algorithm also obtains a variance-based bound depending on $R$ logarithmically, but has a worse dependence on the eluder dimension. Improving this is left as a future direction. We also provide a lower bound to show that our regret bounds are optimal in the leading-order term.

For the future work, since the Catoni estimator is a general device from robust statistics, it might also be useful to investigate if it enables us to handle other forms of noise, such as adversarial corruption \citep{he2022nearlyb,ye2023corruption,ye2024corruption}. Additionally, while we obtain information-theoretic results in this paper, the algorithms are not easy to implement, both because OFUL-style algorithms are always tricky due to the version space structure, and the function-dependent choice of $\theta$ in the way we invoke the Catoni estimator makes things even harder. It would also be interesting to extend the results to general MDPs. %There are also questions of dimension-dependence even in the linear case.


\section{Acknowledgment}
Chenlu Ye and Tong Zhang are partially supported by an NSF IIS grant No. 2416897.

\bibliography{myrefs}
\bibliographystyle{apalike}

\newpage
\appendix


\section{Notation Table and Additional Related Works}
\subsection{Notation Table}\label{ssec:table}
To improve the readability of this paper, we provide Tables \ref{tab:notation_known} and \ref{tab:notation_unknown} for the notations used in this paper.
 
\begin{table}[h]
\centering 
\begin{tabular}{|c|c|}
\hline
\textbf{Notation} & \textbf{Description} \\
\hline
$[n]$ & $\{1,\ldots,n\}$\\
$R$ & The range for the noise $\eta_t$, $\forall t\in[T]$\\
$\sigma_t$ & The variance for the noise $\eta_t$\\
$L_f$ & The range for any function $f\in\cF$\\
$N$& The $\upsilon$-covering number for the reward function class $\cF$\\
\makecell{$D_\calF(x,\sigma;x_{[t-1]}, \sigma_{[t-1]})$, \\ $D^2_\calF(x;x_{[t-1]}, \sigma_{[t-1]})$} & The eluder coefficients in Definition \ref{def:eluder}\\
\makecell{$\dim(\calF,X,\sigma)$, \\ $\dim_{\alpha,T)}(\calF)$} & The eluder dimension in Definition \ref{def:eluder}\\
$\ctn_\theta(\{Z_i\}_{i\in[t]})$ & Catoni estimator defined in \eqref{eq:catoni}\\
$\iota_0^2$ & $4\log\left(\frac{48R(1+2AR)t^2}{\min(1,a)\epsilon^2\delta}\log(A/a)\right)$\\
$\iota(\delta)$ & $\Theta(\sqrt{\log(RL_fTN/\delta)})$\\
$\iota(\delta)$ & $\sqrt{\log(720R^2L_f^3T^3N^2T^5/\delta)}$\\
$\theta_t(f,f')$ & $\frac{2\iota
(\delta)}{\sqrt{\sum_{i\in[t]}(f(x_i)-f'(x_i))^2/\bsigma_i^2\cdot\Big(1+(2\iota
(\delta))^{-1}\sqrt{\hbeta_{t-1}^2+\lambda}\Big) + \epsilon^2}}$\\
$\hbeta_t$ & The confidence radius, $\Theta(\sqrt{\log(R L_f \calN(\calF,\upsilon)T/\delta)})$\\
$\alpha$ & $1/\sqrt{T}$\\
$\lambda$ & $\Theta(1)$\\
$\upsilon$ & $O(1/L_f^{12}R^4T^{10})$\\
\hline
\end{tabular}
\caption{The Table of Notations for the Known Variance Case.}
\label{tab:notation_known}
\end{table}

\begin{table}[h]
\centering 
\begin{tabular}{|c|c|}
\hline
\textbf{Notation} & \textbf{Description} \\
\hline
$\sigma_\eta^2$ & The union bound for $\sigma_t^2$, $\forall~t\in[T]$\\
$c_\eta$ & The union bound of the ratio: $\Var[\eta_t^2|\calF_t] \le c_{\eta} \Var[\eta_t|\calF_t]$\\
$\Psi_t^l$ & The set of time steps when the update happens for level $l$ in Algorithm \ref{alg:Peeling}\\
$\lambda^l$ & The prameter in the uncertainty $2^{-2l}$ for $l\in[L]$\\
$l_\star$ & $\lceil \log_2(1076\iota'(\delta)) \rceil$\\
$D_t^l(x)$ & $\sup_{f,f'\in\calF_{t-1}^l} \frac{|f(x)-f'(x)|}{\sqrt{\sum_{i\in\Psi_{t-1}^l}(f(x_i)-f'(x_i))^2/w_i^2 + \lambda^l}}$\\
$\hat{b}_t^l$ & $14\iota'(\delta)(2\sigma_\eta^2+c_\eta) + 43\Delta_{\upsilon}+268\lambda^l$\\
$\theta_\Var^{t,l}$ & $(4(2\sigma_\eta^2 + c_{\eta}+L_f^2 +  2^{-2l+4}\cdot(\hbeta^l_{t-1})^2))^{-1}$\\
$\Delta_\upsilon$ & $\Theta(\mathrm{Poly}(L_fR\upsilon T))$\\
$\iota'(\delta)$ & $\Theta\Big(\sqrt{\log\Big(RL_f(\sigma_\eta^2+c_\eta+\Delta_\upsilon+\lambda^l)NLT/\delta\Big)}\Big)$\\
$\theta_t^l(f,f')$ & $\frac{\iota'(\delta)}{\sqrt{2^{-2l}(\hbeta_{t-1}^l)^2(\hVar_t^l+V_t^l(f,f')) + 2^{-4l}}}$\\
$(\hbeta_t^l)^2$ & The confidence radius, $2880(\iota'(\delta))^22^{-2l}\hVar_t^l + 60\iota'(\delta)2^{-2l} +12\Delta_{\upsilon,2} +2\lambda^l$\\
$\Delta_\upsilon$ & $\Theta\big(L_f\upsilon T^2 + L_f^4R^2\iota'(\delta)\upsilon^{0.5}T^{3.5} + L_f^3R^{1.5}\upsilon^{0.25}T^{1.25} + R^3L_f^3\upsilon T + \sqrt{RL_f\upsilon}T\big)$\\
\hline
\end{tabular}
\caption{The Table of Notations for the Unknown Variance Case.}
\label{tab:notation_unknown}
\end{table}

\subsection{Additional Related Works}\label{ssec:Additional Related Works}
\paragraph{Variance-weighted regression.}
Variance-weighted regression has been studied for light-tailed noises for both contextual bandits and Markov Decision Processes (MDPs) with linear and general function approximation. Specifically, \citet{zhou2021nearly,zhou2022computationally} apply variance-weighted regression to obtain second-order bounds for linear contextual bandits under the known variance case. They also use the weighting technique for linear mixture MDPs under unknown variance case, where they assume that the variance can be realized by a linear function class. Similar weighted regression also appears in MDPs with linear and general function approximation to achieve the optimal regret bound \citep{agarwal2023vo,he2023nearly,zhao2023nearly}, and in the adversarial corruption settings to make the algorithm robust to adversarial attacks \citep{he2022nearlyb,ye2023corruption,ye2024corruption,ye2024towards}.

\paragraph{Heavy-tailed rewards in bandits and RL.} The topic of \emph{robustness to heavy-tailed rewards} has received a considerable amount of attention recently. \citet{bubeck2013bandits} are the first to study heavy-tailed rewards in multi-armed bandits. More generally, robust mean estimators \citep{lugosi2019mean} such as median-of-means, truncated mean and Catoni's mean have been applied to linear contextual bandits \citep{medina2016no,shao2018almost,xue2020nearly,huang2024tackling,livariance}.


\section{Proofs for the Known Variance Setting}\label{aps:Proofs for the Unknown Variance Setting}

\subsection{Proof for the Lower Bound}\label{apss:pf_lower_bound}
\begin{proof}[Proof of Theorem \ref{th:lower_bound}]
For any $0\le\sigma\le1/2$, $0\le \eps \le \sigma/2$ and $R> \sqrt{3}$, define three distributions
\begin{align*}
P_{\sigma} = \sigma(1+R^{-1}),
\quad
P_{\sigma,\epsilon}^+ = \left\{
\begin{array}{cc}
    2\sigma, & \text{w.p.}~\frac{\sigma+\epsilon}{2\sigma}, \\
    2\sigma R, & \text{w.p.}~\frac{\sigma+\epsilon}{2\sigma R^2},\\
    0, & \text{w.p.}~1-\frac{(\sigma+\epsilon)(1+R^{-2})}{2\sigma},
\end{array}\right.
\quad
P_{\sigma,\epsilon}^- = \left\{
\begin{array}{cc}
    2\sigma, & \text{w.p.}~\frac{\sigma-\epsilon}{2\sigma}, \\
    2\sigma R, & \text{w.p.}~\frac{\sigma-\epsilon}{2\sigma R^2},\\
    0, & \text{w.p.}~1-\frac{(\sigma-\epsilon)(1+R^{-2})}{2\sigma},
\end{array}\right.
\end{align*}
We have the means
$$
\mu_{\sigma} = \sigma(1+R^{-1}),\qquad \mu_{\sigma,\eps}^+ = (\sigma+\eps)(1+R^{-1}),\qquad \mu_{\sigma,\eps}^- = (\sigma-\eps)(1+R^{-1}),
$$
and variance
\begin{equation}\label{eq:variances}
\begin{aligned}
    &V_{\sigma} = 0,\\
&V_{\sigma,\eps}^+ = (\sigma+\epsilon)(4\sigma - (1+R^{-1})^2\sigma - (1+R^{-1})^2\epsilon) \le 6\sigma^2,\\
&V_{\sigma,\eps}^- = (\sigma-\epsilon)(4\sigma - (1+R^{-1})^2\sigma + (1+R^{-1})^2\epsilon)\le 2\sigma^2. 
\end{aligned}
\end{equation}
Thus, the rewards induced by the last two distributions $P_{\sigma,\epsilon}^+,P_{\sigma,\epsilon}^-$ have large $L_1$ norm ($2\sigma R$) and bounded variances.

Furthermore, we have
\begin{align*}
    \KL(P_{\sigma,\eps}^-\|P_{\sigma,\eps}^+) =& \frac{\sigma-\eps}{2\sigma}\log \frac{\sigma-\eps}{\sigma+\eps} + \frac{\sigma-\eps}{2\sigma R^2}\log \frac{\sigma-\eps}{\sigma+\eps} + \Big(1-\frac{(\sigma-\eps)(1+R^{-2})}{2\sigma}\Big)\log\frac{\sigma+\eps-\frac{\sigma-\eps}{R^2}}{\sigma-\eps-\frac{\sigma+\eps}{R^2}}\\
    \le& (1+R^{-2})\frac{\sigma-\epsilon}{2\sigma}\log\frac{\sigma-\epsilon}{\sigma+\epsilon} + (1+R^{-2})\frac{\sigma+\epsilon}{2\sigma}\log\frac{\sigma+\epsilon}{\sigma-\epsilon}\\
    \le& 4(1+R^{-2})\frac{\epsilon^2}{\sigma^2}.
\end{align*}

Fix a policy $\pi$. Now, we construct two $2$-armed bandits and let the context space $\mathcal{X}=\emptyset$. For the first bandit $B_1$, the reward of the first arm $R_1(a_1)\sim P_{\sigma}$, and the reward of the second arm $R_1(a_2)\sim P_{\sigma,\epsilon}^-$. Thus, the first arm $a_1$ is the optimal arm for $B_1$. For the second bandit, we have $R_2(a_1)\sim P_{\sigma}$, and $R_2(a_2)\sim P_{\sigma,\epsilon}^+$, and the second arm is the optimal arm for $B_2$ but with a large variance. For $i=1,2$, let $\P_i$ denote the distribution generated by the bandit environment $B_i$, and let $\E_i$ denote the expectations under $\P_i$. Then, we have
$$
\E_1[R_T] \ge \P_1(N_T(1)\le T/2)\cdot \frac{T\eps}{2},\quad \E_{2}[R_T] \ge \P_2(N_T(1)\ge T/2)\cdot \frac{T\eps}{2}.
$$
Then, by Bretagnolle-Huber inequality, we have
\begin{align*}
    \E_1[R_T] + \E_{2}[R_T] \ge \frac{T\eps}{2}\big(\P_1(N_T(1)\le T/2) + \P_{2}(N_T(1)> T/2)\big)
    \ge \frac{T\eps}{4}\exp(-\KL(\P_1\|P_{2})).
\end{align*}
We also have
$$
\KL(\P_1\|\P_{2}) = \E_1[N_T(2)] \KL(P_{\sigma,\eps}^-\|P_{\sigma,\eps}^+) \le \frac{4(1+R^{-2})T\eps^2}{\sigma^2}.
$$
Thus, we have
$$
\E_1[R_T] + \E_{2}[R_T] \ge \frac{T\eps}{4}\exp\Big(-\frac{4(1+R^{-2}) T\eps^2}{\sigma^2}\Big)
$$
By choosing $\eps=\sqrt{\sigma^2/4(1+R^{-2})T}$, we have
\begin{align}\label{eq:lower_bound0}
    \max_i \{\E_i[R_T]\} = \Omega(\sqrt{\sigma^2 T})
    .    
\end{align}
Since the variance accumulates only when the arm $x_2$ is pulled,
\begin{align*}
&T \ge \E_1[N_T(2)] = \E_1\Big[\sum_{t=1}^T I(x_t=x_2)\Big] \ge \frac{\E_1[\sum_{t=1}^T\sigma_t^2]}{6\sigma^2},\\
&T \ge \E_2[N_T(2)] = \E_2\Big[\sum_{t=1}^T I(x_t=x_2)\Big] \ge \frac{\E_2[\sum_{t=1}^T\sigma_t^2]}{2\sigma^2},
\end{align*}
where the last inequalities for both lines uses \eqref{eq:variances}. Then, we have
$$
T\ge \max\Big\{\frac{\E_1[\sum_{t=1}^T\sigma_t^2]}{6\sigma^2}, \frac{\E_2[\sum_{t=1}^T\sigma_t^2]}{2\sigma^2}\Big\},
$$
which together with \eqref{eq:lower_bound0} imply that
$$
\max_{i\in\{1,2\}} \{\E_i[R_T]\} = \Omega\bigg(\sqrt{\max\Big\{\E_1\sum_{t=1}^T\sigma_t^2, \E_2\sum_{t=1}^T\sigma_t^2\Big\}}\bigg).
$$
\end{proof}

\subsection{Proof for Theorem \ref{th:known_var}}\label{apss:pf_th1}

\begin{theorem}[Formal version of Theorem \ref{th:known_var}]\label{th:known_var_formal}
Under Algorithm~\ref{alg:Catoni-VOFUL} with the parameter $\alpha=1/\sqrt{T}$, $\lambda=\Theta(1)$, $\upsilon=O(1/L_f^{12}R^4T^{10})$ and
\begin{align}
    \hbeta_t = \Theta(\sqrt{\log(R L_f \calN(\calF,\upsilon)T/\delta)}),\label{eq:def-beta-ctn}
\end{align}
with probability $1-2\delta$, we can bound the regret by 
\begin{align*}
    R_T =& \widetilde{O}\Big(L_f\sqrt{\sum_{t\in[T]}\sigma_t^2\cdot \dim_{\frac{1}{\sqrt{T}}, T}(\calF)\cdot\log \calN(\calF,\upsilon)}\\
    &\qquad +L_f\cdot \log \calN(\calF,\upsilon)\cdot \dim_{\frac{1}{\sqrt{T}},T}(\calF)\Big).
\end{align*}
\end{theorem}

\paragraph{Notations} In the following analysis, we use the short-hand notation for any $f,f'\in\calF$
$$
V_t(f,f'):= \sum_{i\in[t]}\frac{(f(x_i)-f'(x_i))^2}{\bsigma_i^2}.
$$
Recall that we define the excess loss and expected loss: for any $f,f'\in\calF$
\begin{align*}
    L_t(f,f') &= \sum_{i\in[t]}\frac{1}{\bsigma_i^2}(f'(x_i) - f(x_i))^2
    + 2t\ctn_{\theta_t(f,f')}(\{Z_i(f,f')\}_{i\in[t]}),\\
    R_t(f,f') &= \sum_{i\in[t]} \frac{1}{\bsigma_i^2} \E_i\big[ (f(x_i)-f'(x_i))^2 + 2(f(x_i)-f'(x_i))(f'(x_i)-y_i) \big]\\
    &= \sum_{i\in[t]} \frac{1}{\bsigma_i^2} \big[ (f(x_i)-f'(x_i))^2 + 2(f(x_i)-f'(x_i))(f'(x_i)-f^\star(x_i)) \big],
\end{align*}
where we define
\begin{align*}
    &Z_i(f,f') = \frac{1}{\bsigma_i^2}(f(x_i)-f'(x_i))(f'(x_i)-y_i),\\
    &\theta_t(f,f') = \frac{2\iota
(\delta)}{\sqrt{V_t(f,f')\Big(1+(2\iota
(\delta))^{-1}\sqrt{\hbeta_{t-1}^2+\lambda}\Big) + \epsilon^2}},\\
    &\iota(\delta) = \sqrt{\log\left(\frac{720R^2L_f^3N^2T^5}{\delta}\right)}.
\end{align*}
We also use the short-hand notation for the covering number $N:=\calN(\calF,\upsilon)$.

\paragraph{Part I: Concentration of excess loss}
To begin with, we focus on proving the concentration between $L_t(f,f')$ and $R_t(f,f')$. We first consider two fixed functions $f,f'\in\calF$.

\begin{lemma}[Formal Version of Lemma \ref{lm:conv_ctn}]\label{lm:conv_ctn_formal}
    For each time step $t\ge 3\iota^2(\delta)$ and two fixed $f,f'\in\calF_{t-1}$, if we take $\alpha=1/\sqrt{T}$ and $\epsilon=1$, we have with probability at least $1-\delta/N^2$,
    \begin{align*}
        \Big|L_t(f,f') - R_t(f,f') \Big|
        =& 2\Big|t\ctn_{\theta_t(f,f')}(\{Z_i(f,f')\}_{i\in[t]}) - \sum_{i\in[t]}\frac{1}{\bsigma_i^2}(f(x_i)-f'(x_i))(f'(x_i)-f^\star(x_i)) \Big|\\
        \le&\frac{1}{2}V_t(f,f') + 16\iota(\delta)(1+\frac{\sqrt{\lambda}}{2}) + 32\iota^2(\delta) + 5\iota(\delta) + \frac{1}{6}\hbeta_{t-1}^2.
    \end{align*}
\end{lemma}
\begin{proof}
We first compute the expectation of $Z_i(f,f')$ as
\begin{align*}
    \mu_i(f,f') = \frac{1}{\bsigma_i^2}(f(x_i)-f'(x_i))(f'(x_i)-f^\star(x_i)),
\end{align*}
and bound the sum of variance by
\begin{align*}
    \sum_{i\in[t]} \Var[Z_i(f,f')] =& \sum_{i\in[t]}\E\Big[\frac{1}{\bsigma_i^2}(f(x_i)-f'(x_i))^2(f^\star(x_i)-y_i)^2\Big]\\
    \le& \sum_{i\in[t]}\frac{(f(x_i)-f'(x_i))^2}{\bsigma_i^2}=V_t(f,f').
\end{align*}
We can also bound $\theta_t(f,f')\in[a,A]$ by choosing
\begin{align*}
    A = \frac{\iota(\delta)}{\epsilon},\qquad a = \frac{\iota(\delta)}{\sqrt{8L_f^2t/\alpha^2+\epsilon^2}}.
\end{align*}
Hence, given choice of $\alpha = 1/\sqrt{T}$ and $\epsilon = 1$, we have
\begin{align*}
    \log\left(\frac{48R(1+2AR)t^2}{\min(1,a)\epsilon^2\cdot(\delta/N^2T)}\log(A/a)\right) \le \log\left(\frac{720R^2L_f^3T^4}{\delta/N^2T}\right) \le \iota^2(\delta)
\end{align*}
Thus, for any time step
$$
t\ge 6\iota^2(\delta) \ge 4\iota^2(\delta) + 2 \log\left(\frac{48R(1+2AR)t^2}{\min(1,a)\epsilon^2\cdot(\delta/N^2T)}\log(A/a)\right),
$$
by using Lemma \ref{coro:catoni} with $\alpha=1/\sqrt{T}$ and $\epsilon=1$, we
have with probability at least $1-\delta/N^2T$,
\begin{align}\label{eq:aaa}
    & \Big|t\ctn_{\theta_t(f,f')}(\{Z_i(f,f')\}_{i\in[t]}) - \sum_{i\in[t]}\frac{1}{\bsigma_i^2}(f(x_i)-f'(x_i))(f'(x_i)-f^\star(x_i)) \Big| \notag\\
    \le& \theta_t(f,f') \Big(V_t(f,f') + \sum_{i\in[t]}\frac{1}{\bsigma_i^4}(f(x_i)-f'(x_i))^2(f'(x_i)-f^\star(x_i))^2 \Big) + \frac{4\iota^2(\delta)}{\theta_t(f,f')} + \epsilon \notag\\
    \le& \theta_t(f,f')V_t(f,f')\Big(1 + \max_{i\in[t]}\frac{1}{\bsigma_i^2}(f'(x_i)-f^\star(x_i))^2 \Big) + \frac{4\iota^2(\delta)}{\theta_t(f,f')} + \epsilon \notag\\
    \le& \theta_t(f,f')V_t(f,f')\Big(1 + \frac{1}{2\iota(\delta)}\cdot\sqrt{\hbeta^2_{t-1}+\lambda} \Big) + \frac{4\iota^2(\delta)}{\theta_t(f,f')} + \epsilon,
\end{align}
where the last inequality uses the definition of the weight $\bsigma_i^2\ge 4\iota(\delta)L_fD_f(x_i,x_{[i-1]},\bsigma_{[i-1]})$ and $f,f'\in\calF_{t-1
}\subset\calF_{i-1}$ to get for any $i\in[t]$
\begin{align*}
    \frac{1}{\bsigma_i^2}(f'(x_i)-f^\star(x_i))^2 \le& \frac{L_f}{\bsigma_i^2}\cdot\sup_{f,f'\in\calF_{i-1}}\frac{|f'(x_i)-f^\star(x_i)|}{\sqrt{\sum_{k\in[i-1]}\frac{1}{\bsigma_k^2}(f'(x_k)-f^\star(x_k))^2+\lambda}} \cdot \sqrt{\sum_{k\in[i-1]}\frac{1}{\bsigma_k^2}(f'(x_k)-f^\star(x_k))^2+\lambda}\\
    \le& \frac{1}{4\iota(\delta)}\sqrt{2\sum_{k\in[t-1]}\frac{1}{\bsigma_k^2}(f'(x_k)-\hf_{t-1}(x_k))^2 + 2\sum_{k\in[i-1]}\frac{1}{\bsigma_k^2}(f^\star(x_k)-\hf_{t-1}(x_k))^2 + \lambda}\\
    \le& \frac{1}{2\iota(\delta)}\cdot\sqrt{\hbeta^2_{t-1}+\lambda},
\end{align*}
where the second inequality uses the Cauchy-Schwartz inequality, and the last inequality uses the definition of $\calF_{i-1}$.
Via the choice of 
$$
\theta_t(f,f') = \frac{2\iota
(\delta)}{\sqrt{V_t(f,f')\Big(1+(2\iota
(\delta))^{-1}\sqrt{\hbeta_{t-1}^2+\lambda}\Big) + \epsilon^2}},
$$
we get the right-hand side of \eqref{eq:aaa} is upper-bounded by
\begin{align*}
    & 4\iota(\delta)\sqrt{V_t(f,f')\Big(1+(2\iota
(\delta))^{-1}\sqrt{\hbeta_{t-1}^2+\lambda}\Big) + \epsilon^2} + \epsilon\\
    \le& 4\iota(\delta)\sqrt{1+\frac{\sqrt{\lambda}}{2}}\cdot\sqrt{V_t(f,f')} + 2\sqrt{2\iota(\delta)}\cdot\sqrt{V_t(f,f')\hbeta_{t-1}} + 5\iota(\delta)\epsilon \\
    \le& \frac{1}{4}V_t(f,f') + 16\iota(\delta)(1+\frac{\sqrt{\lambda}}{2}) + \frac{1}{4}V_t(f,f') + 96\iota^2(\delta) + \frac{1}{6}\hbeta_{t-1}^2 + 5\iota(\delta)\\
    =& \frac{1}{2}V_t(f,f') + 16\iota(\delta)(1+\frac{\sqrt{\lambda}}{2}) + 96\iota^2(\delta) + 5\iota(\delta) + \frac{1}{6}\hbeta_{t-1}^2,
\end{align*}
which concludes the proof by taking the union bound over $t\in[T]$.
\end{proof}


\begin{lemma}\label{lm:ctn_lipshitz}
    For any $f,f'\in\calF$, there exist two $f_\upsilon,f_\upsilon'\in\calF_\upsilon$ such that $\|f-f_\upsilon\|_\infty\le\upsilon$, $\|f'-f_\upsilon'\|_\infty\le\upsilon$. Then, we have
    \begin{align*}
    &\left|\ctn_{\theta_t(f,f')}(\{Z_i(f,f')\}_{i\in[t]}) - \ctn_{\theta_t(f_\upsilon,f_\upsilon')}(\{Z_i(f_\upsilon,f_\upsilon')\}_{i\in[t]})\right|\\
    &\qquad\le \frac{360L_f^4R^2\iota(\delta)\sqrt{\upsilon t}}{\alpha^4\epsilon^2} + \frac{90L_f^3R^{1/2}(\upsilon t)^{1/4}}{\iota(\delta)\alpha^{5/2}\epsilon^{1/2}}.
\end{align*}
\end{lemma}
\begin{proof}
From the definitions of $Z_i(f,f')$, we have $Z_i(f,f')\le 2L_f(2L_f+R)/\alpha^2$, and
\begin{align*}
    \left|Z_i(f,f') - Z_i(f_\upsilon,f_\upsilon')\right| &\le \left|\frac{1}{\bsigma_i^2}(f(x_i)-f'(x_i))(f'(x_i)-y_i) - \frac{1}{\bsigma_i^2}(f_\upsilon(x_i)-f_\upsilon'(x_i))(f_\upsilon'(x_i)-y_i)\right|\\
    &\le \left|\frac{1}{\bsigma_i^2}\big(f(x_i)-f_\upsilon(x_i)-f'(x_i)+f_\upsilon'(x_i)\big)(f'(x_i)-y_i) - \frac{1}{\bsigma_i^2}\big(f_\upsilon(x_i)-f_\upsilon'(x_i)\big)(f_\upsilon'(x_i)-f(x_i))\right|\\
    &\le \frac{6L_f\upsilon}{\alpha^2}.
\end{align*}
From the definitions $\theta_t(f,f')$, we get $\theta_t(f,f')\le \iota(\delta)/\epsilon$ and
\begin{align*}
    \left|\theta_t(f,f') - \theta_t(f_\upsilon,f_\upsilon')\right| &\le \left|\frac{\iota(\delta)}{\sqrt{\sum_{i\in[t]}(f(x_i)-f'(x_i))^2/\bsigma_i^2 + \epsilon^2}} - \frac{\iota(\delta)}{\sqrt{\sum_{i\in[t]}(f_\upsilon(x_i)-f_\upsilon'(x_i))^2/\bsigma_i^2 + \epsilon^2}}\right|\\
    &\le \frac{\iota(\delta)}{\epsilon}\cdot\sqrt{\sum_{i\in[t]}\frac{1}{\bsigma_i^2}\left[(f(x_i)-f'(x_i))^2 - (f_\upsilon(x_i)-f_\upsilon'(x_i))^2\right]}\\
    &\le \frac{\iota(\delta)}{\epsilon\alpha}\cdot\sqrt{8L_f\upsilon t}.
\end{align*}
Combining the two inequalities above, we have
\begin{align*}
    \Delta &= \frac{1}{t}\sum_{i\in[t]}\theta_t(f,f')\left|Z_i(f,f') - Z_i(f_\upsilon,f_\upsilon')\right| + \frac{6L_f(2L_f+R)}{\alpha^2}\cdot\left|\theta_t(f,f') - \theta_t(f_\upsilon,f_\upsilon')\right| \\
    &\le \frac{\iota(\delta)}{\epsilon}\cdot\frac{6L_f\upsilon}{\alpha^2} + \frac{6L_f(2L_f+R)}{\alpha^2}\cdot\frac{\iota(\delta)}{\epsilon\alpha}\cdot\sqrt{8L_f\upsilon t}\\
    &\le \frac{60L_f^{2.5}R\iota(\delta)\sqrt{\upsilon t}}{\alpha^3\epsilon}.
\end{align*}
Then, by invoking Lemma \ref{lem:catoni-helper}, we deduce that
\begin{align*}
    &\left|\ctn_{\theta_t(f,f')}(\{Z_i(f,f')\}_{i\in[t]}) - \ctn_{\theta_t(f_\upsilon,f_\upsilon')}(\{Z_i(f_\upsilon,f_\upsilon')\}_{i\in[t]})\right|\\
    &\qquad \le \frac{1+2\theta_t(f,f') \cdot 2L_f(2L_f+R)/\alpha^2}{\theta_t(f,f')}\cdot\Delta + \sqrt{\frac{2\Delta}{(\theta_t(f,f'))^2}}\\
    &\qquad\le \frac{360L_f^4R^2\iota(\delta)\sqrt{\upsilon t}}{\alpha^4\epsilon^2} + \frac{90L_f^3R^{1/2}(\upsilon t)^{1/4}}{\iota(\delta)\alpha^{5/2}\epsilon^{1/2}}.
\end{align*}

\end{proof}

Then, it follows the analysis for any $f,f'\in\calF$ which uses the uniform cover.
\begin{lemma}\label{lm:conv_ctn_2}
    If we take $\upsilon=O(1/L_f^{12}R^4T^{10}),~\alpha=1/\sqrt{T}$, $\epsilon=1$, and $\hbeta_t=\Theta(\iota(\delta)(1 + \lambda^{1/4}))$. For any $f,f'\in\calF$ and any $t\ge 3\iota^2(\delta)$, with probability at least $1-\delta$,
    \begin{align*}
        \Big|L_t(f,f') - R_t(f,f') \Big|
        =& 2\Big|t\ctn_{\theta_t(f,f')}(\{Z_i(f,f')\}_{i\in[t]}) - \sum_{i\in[t]}\frac{1}{\bsigma_i^2}(f(x_i)-f'(x_i))(f'(x_i)-f^\star(x_i)) \Big|\\
        \le& \frac{1}{2}V_t(f,f') + \frac{1}{3}\hbeta_t^2.
    \end{align*}
\end{lemma}
\begin{proof}
For any $f,f'\in\calF$, there exist two $f_\upsilon,f_\upsilon'\in\calF_\upsilon$ such that $\|f-f_\upsilon\|_\infty\le\upsilon$, $\|f'-f_\upsilon'\|_\infty\le\upsilon$. by invoking Lemma \ref{lm:ctn_lipshitz} with $\alpha=1/\sqrt{T},\epsilon=1$, we have
    \begin{align*}
    &\left|\ctn_{\theta_t(f,f')}(\{Z_i(f,f')\}_{i\in[t]}) - \ctn_{\theta_t(f_\upsilon,f_\upsilon')}(\{Z_i(f_\upsilon,f_\upsilon')\}_{i\in[t]})\right|\\
    &\qquad\le 360L_f^4R^2\iota(\delta)\upsilon^{0.5}T^{2.5} + \frac{90L_f^3R^{0.5}\upsilon^{0.25}T^{1.5}}{\iota(\delta)} := \Delta_c.
\end{align*}
Additionally, we get
\begin{align*}
    &\Big|R_t(f,f') - R_t(f_\upsilon,f_\upsilon') \Big|\\
    &\qquad= \sum_{i\in[t]}\frac{1}{\bsigma_i^2}\left| \E_i[(f(x_i)-y_i)^2] - \E_i[(f'(x_i)-y_i)^2] - E_i[(f_\upsilon(x_i)-y_i)^2] + \E_i[(f_\upsilon'(x_i)-y_i)^2] \right|\\
    &\qquad\le 12L_f\upsilon T^2.
\end{align*}
Thus, by using Lemma \ref{lm:conv_ctn} with a union bound over $f_\upsilon,f_\upsilon'\in\calF_\upsilon$, we have with probability at least $1-\delta$,
\begin{align*}
    &\Big|L_t(f,f') - R_t(f,f') \Big|\\
    &\qquad= \Big|L_t(f,f') - L_t(f_\upsilon,f_\upsilon') + L_t(f_\upsilon,f_\upsilon') - R_t(f_\upsilon,f_\upsilon') + R_t(f_\upsilon,f_\upsilon') - R_t(f,f') \Big|\\
    &\qquad\le \Big|L_t(f_\upsilon,f_\upsilon') - R_t(f_\upsilon,f_\upsilon')\Big| + 18L_f\upsilon T^2 + T\Delta_c\\
    &\qquad\le \frac{1}{2}V_t^2(f_\upsilon,f'_\upsilon) + 16\iota(\delta)\Big(1+\frac{\sqrt{\lambda}}{2}\Big) + 96\iota^2(\delta) + 5\iota(\delta) + \frac{1}{6}\hbeta_{t-1}^2 + 18L_f\upsilon T^2 + T\Delta_c\\
    &\qquad\le \frac{1}{2}V_t(f,f') + 4L_f\upsilon T + 16\iota(\delta)\Big(1+\frac{\sqrt{\lambda}}{2}\Big) + 96\iota^2(\delta) + 5\iota(\delta) + \frac{1}{6}\hbeta_{t-1}^2 + 18L_f\upsilon T^2 + T\Delta_c\\
    &\qquad\le \frac{1}{2}V_t(f,f') + \frac{1}{3}\hbeta_t^2,
\end{align*}
where the last inequality holds since $\hbeta_{t-1}$ satisfies that
$$
\hbeta_{t-1}^2 \ge 6\left(16\iota(\delta)\Big(1+\frac{\sqrt{\lambda}}{2}\Big) + 96\iota^2(\delta) + 5\iota(\delta)
 + 24L_f\upsilon T^2 + T\Delta_c\right).
$$
\end{proof}


\paragraph{Part II: Sharpness of the confidence set}
\begin{lemma}[Formal version of Lemma \ref{lm:L_f(f)}]\label{lm:L_f(f)_formal}
If we take $\upsilon=O(1/L_f^{12}R^4T^{10}),~\alpha=1/\sqrt{T}$, $\epsilon=1$, and under Algorithm \ref{alg:Catoni-VOFUL} with
    $\hbeta_t=\Theta(\iota(\delta)(1 + \lambda^{1/4}))$, we have for all large enough $t\in[T]$ with probability at least $1-\delta$,
\begin{align*}
    L_t(\hf_t) \ge& \frac{2}{3}V_t(\hf_t,f^\star) - \frac{1}{3}\hbeta_t^2,\\
    L_t(f^\star) \le& \frac{1}{3}\hbeta_t^2,
\end{align*}
where the minimizer for $f'$ is $f'_{\min} = \frac{2}{3}f^\star + \frac{1}{3}\hf_t$.
\end{lemma}
\begin{proof}[Proof of Lemma \ref{lm:L_f(f)}]
By invoking Lemma \ref{lm:conv_ctn_2} with taking minimum over $f'\in\calF$ on the both sides of the inequality and $f=\hf_t$, we have with probability at least $1-\delta$
$$
\max_{f'\in\calF_{t-1}}L_t(\hf_t,f') \ge \max_{f'\in\calF_{t-1}} \Big\{R_t(\hf_t,f') - \frac{1}{2}V_t(\hf_t,f') -\frac{1}{3}\hbeta_t^2\Big\},
$$
which implies that
\begin{align*}
    L_t(\hf_t) \ge&  \max_{f'\in\calF_{t-1}} \Big\{V_t(\hf_t,f^\star) - V_t(f',f^\star) - \frac{1}{2}V_t(\hf_t,f') - \frac{1}{3}\hbeta_t^2\Big\}\\
    =& \max_{f'\in\calF_{t-1}} \Big\{-\frac{3}{2}\sum_{i\in[t]}\frac{(f'(x_i)-\frac{2}{3}f^\star(x_i) - \frac{1}{3}\hf_t(x_i))^2}{w_i^2} + \frac{2}{3}V_t(\hf_t,f^\star) - \frac{1}{3}\beta_t^2\Big\}\\
    =& \frac{2}{3}V_t(\hf_t,f^\star) - \frac{1}{3}\hbeta_t^2,
\end{align*}
where the minimizer is $f'_{\max} = \frac{2}{3}f^\star + \frac{1}{3}\hf_t$.

Additionally, by using Lemma \ref{lm:conv_ctn} with taking minimum over $f'\in\calF$ on the both sides of the inequality and $f=f^\star$, we have with probability at least $1-\delta$
\begin{align*}
    L_t(f^\star) \le& \max_{f'\in\calF_{t-1}} \Big\{R_t(f^\star,f') + \frac{1}{2}V_t(f',f^\star) + \frac{1}{3}\hbeta_t^2\Big\}\\
    =&\max_{f'\in\calF_{t-1}} \Big\{-V_t(f',f^\star) + \frac{1}{2}V_t(f',f^\star) + \frac{1}{3}\hbeta_t^2\Big\}\\
    \le&  \frac{1}{3}\hbeta_t^2,
\end{align*}
where the minimizer of $f'=f^\star$.
\end{proof}

\begin{lemma}\label{lm:confidence_set}
    If we take $\upsilon=O(1/L_f^{12}R^4T^{10}),~\alpha=1/\sqrt{T}$, $\epsilon=1$, and under Algorithm \ref{alg:Catoni-VOFUL} with
    $\hbeta_t=\Theta(\iota(\delta)(1 + \lambda^{1/4}))$, with probability at least $1-\delta$, we have $f^\star\in\cF_t$.
\end{lemma}
\begin{proof}
We use the notation $ L_t(f):=\max_{f'\in\calF} L_t(f,f')$, and recall that
\begin{align*}
    R_t(f,f') =& \sum_{i\in[t]} \frac{1}{w_i^2}\E_i\left[(f(x_i)-f^\star(x_i) + f^\star(x_i)-y_i)^2 - (f'(x_i)-f^\star(x_i)+f^\star(x_i)-y_i)^2\right]\\
    =& \sum_{i\in[t]} \frac{1}{w_i^2}\E_i\left[(f(x_i)-y_i)^2+(f^\star(x_i)-y_i)^2 - (f'(x_i)-y_i)^2-(f^\star(x_i)-y_i)^2\right]\\
    =& \sum_{i\in[t]} \frac{1}{w_i^2}\E_i\left[(f(x_i)-y_i)^2 - (f'(x_i)-y_i)^2\right]\\
    =& V_t(f,f^\star) - V_t(f',f^\star).
\end{align*}

Since $\hf_t = \argmin_{f\in\calF_{t-1}}L_t(f)$, we invoke Lemma \ref{lm:L_f(f)_formal} to get
\begin{align*}
    0 \ge L_t(\hf_t) - L_t(f^\star) \ge \frac{2}{3}V_t(\hf_t,f^\star) - \frac{2}{3}\hbeta_t^2,
\end{align*}
which means that
\begin{align*}
    V_t(\hf_t,f^\star) = \sum_{i\in[t]}\frac{1}{\bsigma_i^2}(\hf_t(x_i) - f^\star(x_i))^2 \le \hbeta_t^2.
\end{align*}
\end{proof}


\paragraph{Part III: Bounding the regret conditioning on good events.}
We now recall the definition that $\calT\defeq \{t\in[T]: t> 7\iota^2(\delta_{n,T})\}$, we further denote the good events $\calE_0 = \{f^\star\in\cap_{t\in\calT}\calF_t\}$. 

\begin{proof}[Proof of Theorem \ref{th:known_var}]\label{aaa}
Conditioning on both good events $\calE_0$, we use the notation $f_t(x)=\argmax_{f\in\calF_{t-1}} f(x)$ can bound the regret of $t\in\calT$ by
\begin{align*}
    &\max_{x\in\cX_t}f^\star(x) - f^\star(x_t)\\
    &\qquad\le \max_{x\in\cX_t}f_t(x) - f^\star(x_t) \le f_t(x_t) - f^\star(x_t)\\
    &\qquad\le \bsigma_t D_{\calF_{t-1}}(x_t,\bsigma_t;x_{[t-1]},\bsigma_{[t-1]})\cdot\sqrt{\sum_{i\in[t-1]}\frac{1}{\bsigma_i^2}\left(f_t(x_i)-\hf_{t-1}(x_i) + \hf_{t-1}(x_i) - f^\star(x_i)\right)^2+\lambda}\\
    &\qquad\le \bsigma_t D_{\calF_{t-1}}(x_t,\bsigma_t;x_{[t-1]},\bsigma_{[t-1]})\cdot\sqrt{2\sum_{i\in[t-1]}\frac{1}{\bsigma_i^2}\left(f_t(x_i)-\hf_{t-1}(x_i)\right)^2 + 2\sum_{i\in[t-1]}\frac{1}{\bsigma_i^2}\left(\hf_{t-1}(x_i) - f^\star(x_i)\right)^2 + \lambda}\\
    &\qquad\le 2\bsigma_t D_{\calF_{t-1}}(x_t,\bsigma_t;x_{[t-1]},\bsigma_{[t-1]})\cdot\hbeta_{t-1},
\end{align*}
where the second inequality follows from $X_t=\argmax_{x\in\calX_t}f_t(x)$, the second inequality uses the definition of uncertainty $D_{\cF_{t-1}}$, and the last inequality holds due to $f^\star,\hf_{t-1}\in\cF_{t-1}$.

Combining this with the range bound that $\|f^\star\|_\infty\le L_f$, the cumulative regret is bounded by
\begin{align}\label{eq:regret-summation-catoni}
R_T & = \sum_{t\in[T]}(\max_{x\in\cX_t}f^\star(x) - f^\star(x_t))\notag\\
&\le 2L_f\cdot7\iota^2(\delta)+2L_f\sum_{t-1\in\calT}\min\left(1,\bsigma_tD_{\calF_{t-1}}(x_t,\bsigma_t; x_{[t-1]},\bsigma_{[t-1]})\hbeta_{t-1}\right).
\end{align}

To finally bound the regret, we bound the second term in RHS of $R_T$ expression in~\eqref{eq:regret-summation-catoni} respectively. These steps mainly follow Lemma 4.4 in~\cite{zhou2022computationally}. We can decompose the terms by considering $\calI_1  = \{t-1\in\calT|D_{\calF_{t-1}}(x_t,\bsigma_t; x_{[t-1]},\bsigma_{[t-1]})\ge1\}$ and $\calI_2 = \{t-1\in\calT, t\notin \calI_1\}$.

For the first set, we bound its size naively by
\[
|\calI_1| \le \sum_{t\in\calI_1} \min\left(1, D^2_{\calF_{t-1}}(x_t,\bsigma_t; x_{[t-1]},\bsigma_{[t-1]})\right)\le  \dim_{\alpha, T}(\calF).
\]

For the second set, we bound the summation of terms of interest contraining on $\calI_2$ by 
\begin{align*}
    & \sum_{t\in\calI_2}\bsigma_t\sqrt{\hbeta^2_{t-1}+\lambda}\cdot D_{\calF_{t-1}}(x_t,\bsigma_t; x_{[t-1]},\bsigma_{[t-1]})\\ & \hspace{2em} \le  \sum_{t\in\calI_2, \bsigma_t = \sigma_t~\text{or}~\alpha}\bsigma_t\sqrt{\hbeta^2_{t-1}+\lambda}\cdot D_{\calF_{t-1}}(x_t,\bsigma_t; x_{[t-1]},\bsigma_{[t-1]})\\
    & \hspace{4em} +\sum_{t\in\calI_2, \bsigma_t = \sqrt{4\iota(\delta_{n,t,l})L_fD_{\calF_{t-1}}(x_t; x_{[t-1]},\bsigma_{[t-1]})} }\bsigma_t\sqrt{\hbeta^2_{t-1}+\lambda}\cdot D_{\calF_{t-1}}(x_t,\bsigma_t; x_{[t-1]},\bsigma_{[t-1]})\\
    & \hspace{2em} \stackrel{(i)}{\le} \sum_{t\in[T]} \left(\sigma_t+\alpha\right)\hbeta_{t-1}\cdot D_{\calF_{t-1}}(x_t,\bsigma_t;x_{[t-1]},\bsigma_{[t-1]})+ \sum_{t\in[T]}32L_f\iota(\delta_{n,t,l})\hbeta_{t-1}\cdot D^2_{\calF_{t-1}}(x_t,\bsigma_t;x_{[t-1]},\bsigma_{[t-1]})\\
    & \hspace{2em} \stackrel{(ii)}{\le} \sqrt{2\sum_{t\in[T]}\hbeta_{t-1}^2(\sigma_t^2+\alpha^2)}\sqrt{\dim_{\alpha,T}(\calF)}+16L_f\iota(\delta_{n,t,l})\max_{t\in[T]}\hbeta_{t-1}\cdot\dim_{\alpha,T}(\calF).
\end{align*}
Here for $(i)$ we use the condition for each distinct set and for $(ii)$ we use Cauchy-Schwarz inequality for the first term and the definition of $\dim_\alpha$ for both terms.

Consequently plugging these back in~\eqref{eq:regret-summation-catoni} and take supremum over $x:|x|=T$, we conclude that with probability at least $1-2\delta$,
\begin{align*}
    R_T & = O\left(L_f\cdot\iota^2(\delta_{n,T})+ L_f\dim_{\alpha,T}(\calF)+L_f^2\cdot \iota(\delta_{n,t,l})\cdot\max_{t\in[T]}\hbeta_{t-1}\cdot\dim_{\alpha,T}(\calF)\right.\\
    &\hspace{2em} \left.L_f+\sqrt{\sum_{t\in[T]}\hbeta_{t-1}^2\left(\sigma_t^2+\alpha^2\right)}\cdot\sqrt{\dim_{\alpha,T}(\calF)}\right)\\
    & = \widetilde{O}\left(L_f\cdot \log \calN\left(\calF,\upsilon \right)\cdot \dim_{\frac{1}{\sqrt{T}},T}(\calF)+L_f\sqrt{\sum_{t\in[T]}\sigma_t^2}\cdot\sqrt{\dim_{\frac{1}{\sqrt{T}}, T}(\calF)\cdot\log \calN\left(\calF,\upsilon \right)}\right),
\end{align*}
where for the last inequality we pick $\lambda = \Theta(1)$, $\alpha = 1/\sqrt{T}$ and $\upsilon = O(1/L_f^{12}R^4T^{10})$.
\end{proof}


\section{Proofs for Unknown Variance}\label{aps:pf_unknown_var}

\subsection{Proof of Theorem \ref{th:unknown_var}}\label{apss:th:unknown_var}

\begin{theorem}[Formal Version of Theorem \ref{th:unknown_var}]\label{th:unknown_var_formal}
Suppose that Assumption \ref{ass:var_peel} holds. Under Algorithm \ref{alg:Peeling} with $\gamma=1/(\sigma_\eta T^{3/2})$ and $\iota'(\delta)=\Theta(\sqrt{\log\Big(RL_f(\sigma_\eta^2+c_\eta+\Delta_\upsilon+\lambda^l)NLT/\delta\Big)})$, if $T\ge 14(\iota'(\delta))^2$, with probability at least $1-3\delta$, we can bound the regret by
\begin{align*}
R_T =& \widetilde{O}\bigg(L_f\Big(\sum_{t\in[T]}\sigma_t^2 \cdot \log \calN(\calF,\upsilon)\Big)^{1/2} \cdot \dim_{1,T}(\calF)\\
&\quad + L_f\dim_{1,T}(\calF)(\log \calN(\calF,\upsilon))^{3/4}(\sqrt{c_{\eta}} + \sigma_{\eta})\bigg).
\end{align*}
\end{theorem}

\paragraph{Notations} In the following analysis, we use the short-hand notation for any $f,f'\in\calF$
$$
V_t^l(f,f'):= \sum_{i\in\Psi_t^l}\frac{(f(x_i)-f'(x_i))^2}{w_i^2}.
$$
Recall that we define the excess loss and expected loss: for each $t\in[T],~l\in[L]$, and any $f,f'\in\calF$
\begin{align*}
    L_t^l(f,f') &= \sum_{i\in\Psi_t^l}\frac{1}{w_i^2}(f'(x_i) - f(x_i))^2
    + 2t\ctn_{\theta_t(f,f')}(\{Z_i(f,f')\}_{i\in\Psi_t^l}),\\
    R_t^l(f,f') &= \sum_{i\in\Psi_t^l} \frac{1}{w_i^2} \E_i\big[ (f(x_i)-f'(x_i))^2 + 2(f(x_i)-f'(x_i))(f'(x_i)-y_i) \big]\\
    &= \sum_{i\in\Psi_t^l} \frac{1}{w_i^2} \big[ (f(x_i)-f'(x_i))^2 + 2(f(x_i)-f'(x_i))(f'(x_i)-f^\star(x_i)) \big],
\end{align*}
where we define
\begin{align*}
    &Z_i(f,f') = \frac{1}{w_i^2}(f(x_i)-f'(x_i))(f'(x_i)-y_i),\\
    &\theta_t^l(f,f') = \frac{\iota'(\delta)}{\sqrt{2^{-2l}(\hbeta_{t-1}^2+\lambda)(\bVar_t^l+V_t^l(f,f')) + \epsilon^2}},\\
    &\iota'(\delta) = \Theta\Big(\sqrt{\log\Big(RL_f(\sigma_\eta^2+c_\eta+\Delta_\upsilon+\lambda^l)NLT/\delta\Big)}\Big).
\end{align*}

The proof is decomposed into four main parts. 

In the following parts, we will show that the following two events both hold with a high probability:
\begin{equation}\label{def:two_events}
\begin{aligned}
    &\Evar^t:=\left\{\sum_{i\in\Psi_t^l}\frac{\sigma_i^2}{w_i^2} \le 2\hVar_t^l,~\text{for}~l\in[L],~2^l\ge 1076\iota'(\delta)\right\},\\
    &\Econv^t \defeq \left\{f^\star\in\calF_t^l,~\text{for}~l\in[L],~2^l\ge 1076\iota'(\delta)\right\}.
\end{aligned}
\end{equation}
We will prove the events hold conditioned on each other sequentially for $t=\Theta(1),\ldots,T$. We also use the short-hand notation for the covering number $N:=\calN(\calF,\upsilon)$.

\paragraph{Part I: Concentration of excess loss} First of all, we also need to prove the concentration of excess loss for each $l\in[L]$. %In this part, we do the analysis assuming that the variance estimation in the weights is correct enough: $\sum_{i\in[t]}\frac{\sigma_i^2}{w_i^2} \le 2\hVar_t$. 

In the following lemma, for conciseness, we neglect level $l$ in the concentration analysis. Later, we will apply the result for each $l\in[L]$.

\begin{lemma}[Formal version of \ref{lm:concen_ctn_peel_0}]\label{lm:concen_ctn_peel_0_formal}
Under Assumption \ref{ass:var_peel}, given $\{x_i,y_i,w_i\}_{i\in[t]}$, we define for $i\in[t-1]$, $\cF_i=\{f\in\cF_{i-1}:~\sum_{l\in[i]}(f(x_l)-\hf_i(x_i))^2/w_l^2+\lambda \le \hbeta_i^2\}$, and $D_i=\sup_{f,f'\in\calF_{i-1}}\frac{|f(x_i)-f'(x_i)|}{\sqrt{\sum_{l=1}^{i-1}(f(x_l)-f'(x_l))^2/w_i^2 + \lambda}}$. Suppose that $\sup_{i\in[t]}\frac{D_i}{w_i}\le\rho$, $w_i\ge 1$, $
(\iota'(\delta))^2 \ge \Theta\Big(\log\Big(RL_f(\sigma_\eta^2+c_\eta+\Delta_\upsilon+\lambda^l)NLT/\delta\cdot \hbeta_{t-1}\Big)\Big)
$ and $\rho\le\min\{1,1/(16\sqrt{3}\iota'(\delta))\}$.

Then, for the time step $t$ such that the following event happens
$
\calE_t = \Big\{t\ge 4(\iota'(\delta))^2\frac{\sum_{i\in[t]}\sigma_i^2/w_i^2}{\hVar_t}+6(\iota'(\delta))^2\Big\},
$
and for any $f,f'\in\calF_{t-1}$, if we take $\epsilon=\rho^2$, with probability at least $1-\delta/TL$,
    \begin{align*}
        \Big|L_t(f,f') - R_t(f,f') \Big|
       \le&\frac{1}{2}V_t(f,f') + \frac{1}{3}\left(\frac{1}{2}\hbeta_{t-1}^2 + 6\bigg(48(\iota'(\delta))^2\rho^2 \left(\frac{\big(\sum_{i\in[t]}\sigma_i^2/w_i^2\big)^2}{\hVar_t} + \hVar_t\right) + 5\iota'(\delta)\rho^2+ \Delta_{\upsilon,2}\bigg)\right),
    \end{align*}
    where
$
\hVar_t = t\ctn_{\theta_\Var^{t}}\Big(\Big\{\frac{1}{w_i^2}(y_i-\hf_
{t-1}(x_i))^2\Big\}_{i\in[t]}\Big)+ 14\iota'(\delta)(2\sigma_\eta^2+c_\eta) + 43\Delta_{\upsilon}+268\lambda, 
$
and $
\Delta_{\upsilon,2} = \Theta(L_f\upsilon T^2 + L_f^4R^2\iota'(\delta)\upsilon^{0.5}T^{3.5} + L_f^3R^{1.5}\upsilon^{0.25}T^{1.25})
$, $\theta_\Var^t = (4(2\sigma_\eta^2 + c_{\eta}+L_f^2 +  16\rho^2\cdot\hbeta_{t-1})^2)^{-1}$.
\end{lemma}
\begin{proof}
At each time step $t\in[T]$, for two fixed $f,f'\in\cF_{t-1}$, we first compute the expectation of $Z_i(f,f')$ as
\begin{align*}
    \mu_i(f,f') = \frac{1}{w_i^2}(f(x_i)-f'(x_i))(f'(x_i)-f^\star(x_i)).
\end{align*}
Additionally, we deduce that
\begin{align*}
    \frac{(f(x_i)-f'(x_i))^2}{w_i^2} 
    \le& \frac{1}{w_i^2}\cdot \sup_{f,f'\in\cF_{i-1}}\frac{|f(x_i)-f'(x_i)|^2}{\sum_{l\in[i-1]}(f(x_l)-f'(x_l))^2/w_l^2+\lambda}\cdot \left(\sum_{l\in[i-1]}\frac{(f(x_l)-f'(x_l))^2}{w_l^2}+\lambda\right)\\
    \le& \frac{D_i^2}{w_i^2}\cdot \left(2\sum_{l\in[t-1]}\frac{(f(x_l)-\hf_{t-1}(x_l))^2}{w_l^2} + 2\sum_{l\in[i-1]}\frac{(f'(x_l)-\hf_{t-1}(x_l))^2}{w_l^2} + \lambda\right)\\
    \le& 4\rho^2\hbeta_{t-1}^2,
\end{align*}
where the first inequality uses $f,f'\in\cF_{t-1}\subset\cF_{i-1}$, the second inequality uses the definition of $D_i$ and the Cauchy-Schwarz inequality, and the last inequality follows from $D_i/w_i\le\rho$ and $f,f'\in\cF_{t-1}$. Thus, we bound the sum of variance by
\begin{align*}
    \sum_{i\in[t]} \Var[Z_i(f,f')] =& \sum_{i\in[t]}\E\Big[\frac{1}{w_i^2}(f(x_i)-f'(x_i))^2(f^\star(x_i)-y_i)^2 \,\Big|\, x_i\Big]\\
    \le& \sum_{i\in[t]}\frac{(f(x_i)-f'(x_i))^2}{w_i^2} \cdot \frac{\sigma_i^2}{w_i^2}\\
    \le& 4\rho^2\hbeta_{t-1}^2\sum_{i\in[t]}
\frac{\sigma_i^2}{w_i^2}.
\end{align*}
Similarly, we can bound the sum of $\mu_i^2$ by
\begin{align*}
    \sum_{i\in[t]}\mu_i^2(f,f') \le& \sum_{i\in[t]}\frac{1}{w_i^4}(f(x_i)-f'(x_i))^2(f'(x_i)-f^\star(x_i))^2\\
    \le& V_t(f,f') \max_{i\in[t]}\frac{(f'(x_i)-f^\star(x_i))^2}{w_i^2}\\
    \le& V_t(f,f') \max_{i\in[t]} \frac{D_i^2}{w_i^2}\cdot \left(\sum_{l\in[t-1]}(f'(x_l)-f^\star(x_l))^2/w_l^2+\lambda\right)\\
    \le& V_t(f,f')\cdot 4\rho^2\hbeta_{t-1}^2.
\end{align*}
We can also get the upper and lower bound of $\theta_t(f,f')$:
\begin{align*}
    \theta_t(f,f') = \frac{\iota'(\delta)}{\sqrt{\rho^2\hbeta_{t-1}^2(\hVar_t+V_t(f,f')) + \epsilon^2}}\le \frac{\iota'(\delta)}{\epsilon} :=A.
\end{align*}
Besides, by using Claim A.14 from \citet{wagenmaker2022first}, we know that
\begin{align*}
    \ctn_{\theta_\Var^{t}}\Big(\Big\{\frac{1}{w_i^2}(y_i-\hf_
{t-1}(x_i))^2\Big\}_{i\in[t]}\Big) \le \max_{i\in[t]} \frac{1}{w_i^2}(y_i-\hf_t(x_i))^2 \le \max_{i\in[t]} \big(2\eta_i^2 + (f^\star(x_i) - \hf_t(x_i))^2\big) \le 2R^2+4L_f^2,
\end{align*}
which indicates the lower bound of $\theta_t(f,f')$:
\begin{align*}
    \theta_t(f,f')\ge \frac{\iota'(\delta)}{\sqrt{(T(2R^2+8L_f^2)+17\iota'(\delta)(2\sigma_\eta^2+c_\eta) + 43\Delta_{\upsilon}+268\lambda^l)\rho^2\hbeta_{t-1}^2+\rho^4}} := a
\end{align*}
Hence, given choice of $\epsilon = \rho^2$, we have
\begin{align*}
    &\log\left(\frac{48R(1+2AR)t^2}{\min(1,a)\epsilon^2\cdot(\delta/N^2TL)}\log(A/a)\right)\\
    &\qquad\le \log\left(\frac{144R^2T^2}{\rho^2\delta/N^2TL}\cdot\Big(\frac{A}{a}\Big)^2\right)\\
    &\qquad\le \log\left(\frac{144R^2T^2}{\rho^2\delta/N^2TL}\cdot\Big((T(2R^2+8L_f^2)+17\iota'(\delta)(2\sigma_\eta^2+c_\eta) + 43\Delta_{\upsilon}+268\lambda^l)\rho^2\hbeta_{t-1}^2+\rho^4\Big)\right)\\
    &\qquad\le (\iota'(\delta))^2,
\end{align*}
where the last inequality holds since
$$
(\iota'(\delta))^2 \ge \Theta\Big(\log\Big(RL_f(\sigma_\eta^2+c_\eta+\Delta_\upsilon+\lambda^l)NLT/\delta\cdot \hbeta_{t-1}\Big)\Big).
$$
Thus, since the following condition holds for time step
$$
\calE_t = \left\{t\ge 4(\iota'(\delta))^2\frac{\sum_{i\in[t]}\frac{\sigma_i^2}{w_i^2}}{\hVar_t}+6(\iota'(\delta))^2\right\},
$$
we have
$$
t\ge 4(\iota'(\delta))^2\frac{\sum_{i\in[t]}\frac{\sigma_i^2}{w_i^2}+V_t(f,f')}{\hVar_t+V_t(f,f')} + 2 \log\left(\frac{48R(1+2AR)t^2}{\min(1,a)\epsilon^2\cdot(\delta/N^2TL)}\log(A/a)\right),
$$
by combining the results above and using Lemma \ref{coro:catoni} with 
$\epsilon=\rho^2$ and the choice of 
$\theta_t(f,f')$,
we have with probability at least $1-\delta/N^2TL$,
\begin{align}\label{eq:aac}
    & \Big|t\ctn_{\theta_t(f,f')}(\{Z_i(f,f')\}_{i\in[t]}) - \sum_{i\in[t]}\frac{1}{w_i^2}(f(x_i)-f'(x_i))(f'(x_i)-f^\star(x_i)) \Big| \notag\\
    &\qquad\le \theta_t(f,f') \Big(\sum_{i\in[t]} \Var[Z_i(f,f')] + \sum_{i\in[t]}\mu_i^2(f,f')\Big) + \frac{4(\iota'(\delta))^2}{\theta_t(f,f')} + \rho^2 \notag\\
    &\qquad\le \frac{\iota'(\delta)}{\sqrt{\rho^2\hbeta_{t-1}^2(\hVar_t+V_t(f,f')) + \epsilon^2}} \cdot 4\rho^2\hbeta_{t-1}^2\left(\sum_{i\in[t]} \frac{\sigma_i^2}{w_i^2} + V_t(f,f')\right)\notag\\
    &\qquad\qquad + 4\iota'(\delta)\sqrt{\rho^2\hbeta_{t-1}^2(\hVar_t+V_t(f,f')) + \epsilon^2} + 1 \notag\\
    &\qquad\le 4\iota'(\delta)\rho\hbeta_{t-1} \left(\frac{\sum_{i\in[t]}\sigma_i^2/w_i^2}{\sqrt{\hVar_t}} + \sqrt{\hVar_t} + 2\sqrt{V_t(f,f')}\right) + 4\iota'(\delta)\rho^2 + \rho^2 \notag\\
    &\qquad\le \frac{1}{12}\hbeta_{t-1}^2 + 48(\iota'(\delta))^2\rho^2 \left(\frac{\big(\sum_{i\in[t]}\sigma_i^2/w_i^2\big)^2}{\hVar_t} + \hVar_t + 4V_t(f,f')\right) + 5\iota'(\delta)\rho^2\notag\\
    &\qquad\le \frac{1}{4}V_t(f,f') + \frac{1}{12}\hbeta_{t-1}^2 + 48(\iota'(\delta))^2\rho^2 \left(\frac{\big(\sum_{i\in[t]}\sigma_i^2/w_i^2\big)^2}{\hVar_t} + \hVar_t\right) + 5\iota'(\delta)\rho^2,
\end{align}
where the fourth inequality uses the Cauchy-Schwarz inequality and $\lambda = O(1)$, and the last inequality holds due to the condition that $\rho\le1/16\sqrt{3}\iota'(\delta)$.

Then, for any $f,f'\in\cF_{t-1}$, there exist two $f_\upsilon,f_\upsilon'\in\calF_\upsilon$ such that $\|f-f_\upsilon\|_\infty\le\upsilon$, $\|f'-f_\upsilon'\|_\infty\le\upsilon$. Similar to Lemma \ref{lm:ctn_lipshitz}, we have
    \begin{align*}
    &\left|\ctn_{\theta_t(f,f')}(\{Z_i(f,f')\}_{i\in[t]}) - \ctn_{\theta_t(f_\upsilon,f_\upsilon')}(\{Z_i(f_\upsilon,f_\upsilon')\}_{i\in[t]})\right|\\
    &\qquad\le 360L_f^4R^2\iota'(\delta)\upsilon^{0.5}T^{0.5} + 90L_f^3R^{0.5}\upsilon^{0.25}T^{0.25}:= \Delta_{\upsilon,1}.
\end{align*}
Additionally, we get
\begin{align*}
    &\Big|R_t(f,f') - R_t(f_\upsilon,f_\upsilon') \Big|\\
    &\qquad= \sum_{i\in[t]}\frac{1}{\bsigma_i^2}\left| \E_i[(f(x_i)-y_i)^2] - \E_i[(f'(x_i)-y_i)^2] - E_i[(f_\upsilon(x_i)-y_i)^2] + \E_i[(f_\upsilon'(x_i)-y_i)^2] \right|\\
    &\qquad\le 12L_f\upsilon T^2.
\end{align*}
Thus, by using \eqref{eq:aac} with a union bound over $f_\upsilon,f_\upsilon'\in\calF_\upsilon$, we have with probability at least $1-\delta/TL$,
\begin{align*}
    &\Big|L_t(f,f') - R_t(f,f') \Big|\\
    &\qquad= \Big|L_t(f,f') - L_t(f_\upsilon,f_\upsilon') + L_t(f_\upsilon,f_\upsilon') - R_t(f_\upsilon,f_\upsilon') + R_t(f_\upsilon,f_\upsilon') - R_t(f,f') \Big|\\
    &\qquad\le \Big|L_t(f_\upsilon,f_\upsilon') - R_t(f_\upsilon,f_\upsilon')\Big| + 24L_f\upsilon T^2 + T\Delta_{\upsilon,1}\\
    &\qquad\le \frac{1}{4}V_t(f,f') + \frac{1}{12}\hbeta_{t-1}^2 + 48(\iota'(\delta))^2\rho^2 \left(\frac{\big(\sum_{i\in[t]}\sigma_i^2/w_i^2\big)^2}{\hVar_t} + \hVar_t\right) + 5\iota'(\delta)\rho^2 + \Delta_{\upsilon,2}\\
    &\qquad\le \frac{1}{4}V_t(f,f') + \frac{1}{6}\beta_t^2,
\end{align*}
where we define $\Delta_{\upsilon,2}=24L_f\upsilon T^2 + T\Delta_{\upsilon,1}$ the last inequality holds due to the definition of $\beta_t$.
\end{proof}


\begin{lemma}\label{lm:concen_ctn_peel_1}
% Under Assumption \ref{ass:var_peel}, given $\{x_i,y_i,w_i\}_{i\in[t]}$, we define $D_i=\sup_{f,f'\in\calF_{i-1}}\frac{|f(x_i)-f'(x_i)|}{\sqrt{\sum_{l=1}^{i-1}(f(x_l)-f'(x_l))^2/w_i^2 + \lambda}}$. Suppose that $\sup_{i\in[t]}\frac{D_i}{w_i^2}\le\rho$, $w_i\ge 1$, let
% $$
% \beta_t^2 := \frac{392}{63}\Big(\frac{1}{16}\sum_{i\in[t]}\frac{\sigma_i^2}{w_i^2} + 64^2L_f^2(\iota'(\delta))^4\rho^2 + \frac{\lambda}{4} + \Delta_\upsilon\Big),
% $$
% where $
% \Delta_{\upsilon} = 24L_f\upsilon T^2 + 360L_f^4R^2\iota'(\delta)\upsilon^{0.5}T^{3.5} + 90L_f^3R^{0.5}\upsilon^{0.25}T^{2.5}.
% $
Under the conditions of Lemma \ref{lm:concen_ctn_peel_0}, assume that $f^\star\in\cF_{t-1}$, and the estimator $\hf_t$ is
$$
\hf_t = \argmin_{\hf\in\calF_{t-1}}\max_{f'\in\calF_{t-1}} L_t(\hf,f') := \sum_{i\in[t]}\frac{1}{\bsigma_i^2}(f'(x_i) - \hf(x_i))^2
+ 2t\ctn_{\theta_t(\hf,f')}(\{Z_i(\hf,f')\}_{i\in[t]}).
$$
Then, for the time step $t$ such that the following event happens
$$
\calE_t = \left\{t\ge 4(\iota'(\delta))^2\frac{\sum_{i\in[t]}\frac{\sigma_i^2}{w_i^2}}{\hVar_t}+6(\iota'(\delta))^2\right\},
$$
and all $l\in[L]$, if we take $\epsilon=2^{-2l}$, with probability at least $1-\delta/TL$,
    $$
    \sum_{i\in[t]}\frac{(f^\star(x_i)-\hf_t(x_i))^2)}{w_i^2}+\lambda \le \frac{1}{2}\hbeta_{t-1}^2 + 6\bigg(48(\iota'(\delta))^2\rho^2 \left(\frac{\big(\sum_{i\in[t]}\sigma_i^2/w_i^2\big)^2}{\hVar_t} + \hVar_t\right) + 5\iota'(\delta)\rho^2 + \Delta_{\upsilon,2}\bigg)+\lambda:= \beta_t^2.
    $$
\end{lemma}
\begin{proof}
Let $L_t(f):=\max_{f'\in\calF} L_t(f,f')$ and
$$
(\beta_t')^2=\frac{1}{2}\hbeta_{t-1}^2 + 6\bigg(48(\iota'(\delta))^2\rho^2 \left(\frac{\big(\sum_{i\in[t]}\sigma_i^2/w_i^2\big)^2}{\hVar_t} + \hVar_t\right) + 5\iota'(\delta)\rho^2 + \Delta_{\upsilon,2}\bigg)
$$
Thus, by invoking Lemma \ref{lm:concen_ctn_peel_0} with taking minimum over $f'\in\calF_{t-1}$ on the both sides of the inequality and $f=\hf_t$, we have with probability at least $1-\delta$
\begin{align*}
    L_t(\hf_t) \ge& \max_{f'\in\calF_{t-1}} \Big\{R_t(\hf_t,f') - \frac{1}{2}V_t(\hf_t,f') - \frac{1}{3}(\beta_t')^2\Big\}\\
    \ge&  \max_{f'\in\calF_{t-1}} \Big\{V_t(\hf_t,f^\star) - V_t(f',f^\star) - \frac{1}{2}V_t(\hf_t,f') - \frac{1}{3}(\beta_t')^2\Big\}\\
    =& \max_{f'\in\calF_{t-1}} \Big\{-\frac{3}{2}\sum_{i\in[t]}\frac{(f'(x_i)-\frac{2}{3}f^\star(x_i) - \frac{1}{3}\hf_t(x_i))^2}{w_i^2} + \frac{2}{3}V_t(\hf_t,f^\star) - \frac{1}{3}(\beta_t')^2\Big\}\\
    =& \frac{2}{3}V_t(\hf_t,f^\star) - \frac{1}{3}(\beta_t')^2,
\end{align*}
where we take $f' = \frac{2}{3}f^\star + \frac{1}{3}\hf_t$.

Additionally, by using Lemma \ref{lm:conv_ctn} with taking minimum over $f'\in\calF$ on the both sides of the inequality and $f=f^\star$, we have with probability at least $1-\delta$
\begin{align*}
    L_t(f^\star) \le& \max_{f'\in\calF_{t-1}} \Big\{R_t(f^\star,f') 
    +\frac{1}{2}V_t(f^\star,f') + \frac{1}{3}(\beta_t')^2\Big\}\\
    =&\max_{f'\in\calF_{t-1}} \Big\{-V_t(f',f^\star) + \frac{1}{2}V_t(f',f^\star) + \frac{1}{3}(\beta_t')^2\Big\}\\
    \le& \frac{1}{3}(\beta_t')^2,
\end{align*}
where we take $f'=f^\star$.

Since $\hf_t = \argmin_{f\in\calF_{t-1}}L_t(f)$, we have
\begin{align*}
    0 \ge L_t(\hf_t) - L_t(f^\star) \ge \frac{2}{3}V_t(\hf_t,f^\star) - \frac{2}{3}(\beta_t')^2,
\end{align*}
which means that
\begin{align*}
    V_t(\hf_t,f^\star)+\lambda = \sum_{i\in[t]}\frac{1}{\bsigma_i^2}(\hf_t(x_i) - f^\star(x_i))^2+\lambda \le (\beta_t')^2 +\lambda=\beta_t^2.
\end{align*}
\end{proof}


\paragraph{Part II: Event $\Evar^{t,l}$ holds with high probability.} 
In this part, we focus on the relationship between the sum of true variance $\sum_{i\in[t]}\sigma_i^2/w_i^2$ and the estimation $\hVar_t$ conditioned on $\Econv^{t-1}$. Recall that we define the events $\Evar^t$ and $\Econv^t$ in \eqref{def:two_events}. First, we will show that event $\Evar^t$ conditioned on $\Econv^{t-1}$ holds with high probability.

First of all, we provide a lemma for the $\upsilon$-cover.
\begin{lemma}\label{lm:ctn_lipshitz_var}
    For any $f\in\calF$, there exist $f_\upsilon\in\calF_\upsilon$ such that $\|f-f_\upsilon\|_\infty\le\upsilon$. Then, we have
    \begin{align*}
    &\left|\ctn_{\theta'}\left(\left\{\frac{1}{w_i^2}(y_i-f(x_i))^2\right\}_{i\in[t]}\right) - \ctn_{\theta'}\left(\left\{\frac{1}{w_i^2}(y_i-f_\upsilon(x_i))^2\right\}_{i\in[t]}\right)\right|\\
    &\qquad\le 4(R+2L_f)^3\upsilon + 2\sqrt{(R+2L_f)\upsilon}.
\end{align*}
\end{lemma}
\begin{proof}
We have 
$$
\frac{1}{w_i^2}(y_i-f(x_i))^2 \le 2R^2+8L_f^2,
$$
and
\begin{align*}
    \left|\frac{1}{w_i^2}(y_i-f(x_i))^2 - \frac{1}{w_i^2}(y_i-f_\upsilon(x_i))^2\right| &\le \left|(f_\upsilon(x_i)-f(x_i))(2y_i-f(x_i)-f_\upsilon(x_i))\right|\\
    &\le \upsilon(2R+4L_f).
\end{align*}
From the definitions $\theta_\Var^t$, we get 
$$
\theta_\Var^t = (2(36L_f\rho\hbeta_{t-1} + c_{\eta} + 2\sigma^2_{\eta}) + \epsilon)^{-1} \le 1/\epsilon
$$
Combining the two inequalities above, we have
\begin{align*}
    \frac{1}{t}\sum_{i\in[t]}\theta_\Var^t\left|\frac{1}{w_i^2}(y_i-f(x_i))^2 - \frac{1}{w_i^2}(y_i-f_\upsilon(x_i))^2\right| \\
    &\le \frac{1}{\epsilon}\cdot\upsilon(2R+4L_f).
\end{align*}
Then, by invoking Lemma \ref{lem:catoni-helper} with $\Delta=\frac{1}{\epsilon}\cdot\upsilon(2R+4L_f)$ and taking $\epsilon=\rho^2$, we deduce that
\begin{align*}
    &\left|\ctn_{\theta_\Var^t}(\{Z_i(f,f')\}_{i\in[t]}) - \ctn_{\theta_t(f_\upsilon,f_\upsilon')}(\{Z_i(f_\upsilon,f_\upsilon')\}_{i\in[t]})\right|\\
    &\qquad \le \frac{1+2\theta_\Var^t \cdot 2R^2+8L_f^2}{\theta_\Var^t}\cdot\upsilon(2R+4L_f) + \sqrt{\frac{2\upsilon(2R+4L_f)}{(\theta_\Var^t)^2}}\\
    &\qquad\le 4(R+2L_f)^3\upsilon + 2\sqrt{(R+2L_f)\upsilon}.
\end{align*}

\end{proof}



\begin{lemma}\label{lm:variance_peel}
Under the same condition as Lemma \ref{lm:concen_ctn_peel_0}, and  assuming that $f^\star\in\cF_{t-1}$, we use the Catoni estimator
\begin{align*}
\bVar_t(\hf_t) = t\ctn_{\theta_\Var^t}\left(\left\{\frac{1}{w_i^2}(y_i-\hf_{t-1}(x_i))^2\right\}_{i\in[t]}\right),
\end{align*}
where $\theta_\Var^t = (12(2\sigma_\eta^2 + c_{\eta}+L_f^2 + 16\rho^2\cdot\hbeta^2_{t-1}))^{-1}$, and suppose that $\rho\le 1/(1076\iota'(\delta))$ and $\hbeta_{t-1}^2\le 4320(\iota'(\delta))^2\rho^2\sum_{i=1}^{t-1} \frac{\sigma_i^2}{w_i^2} + 16\times 5660(\iota'(\delta))^32^{-2l}(2\sigma_\eta^2+c_\eta)+120\iota'(\delta)\rho^2+26\Delta_\upsilon+4\lambda$, where $\Delta_\upsilon$ is defined in Table \ref{tab:notation_unknown}.

Then, for the time step $t$ such that the following event happens
$$
\calE_t = \left\{t\ge 4(\iota'(\delta))^2\frac{\sum_{i\in[t]}\frac{\sigma_i^2}{w_i^2}}{\hVar_t}+6(\iota'(\delta))^2\right\},
$$
we have with probability $1-2\delta/TL$
\begin{align*}
    \left|\bVar_t(\hf_t) - \frac{1}{2}\sum_{i=1}^t \frac{\sigma_i^2}{w_i^2}\right| \le 14\iota'(\delta)(2\sigma_\eta^2+c_\eta) + 43\Delta_{\upsilon}+268\lambda.
\end{align*}
where $\Delta_{\upsilon}=
\Theta(L_f\upsilon T^2 + L_f^4R^2\iota'(\delta)\upsilon^{0.5}T^{3.5} + L_f^3R^{1.5}\upsilon^{0.25}T^{1.25} + R^3L_f^3\upsilon T + \sqrt{RL_f\upsilon}T)$.
\end{lemma}
\begin{proof}
For a fixed $f\in\cF_{t-1}$, let
$$
V_i(f) = \frac{1}{w_i^2}\left(y_i-f(x_i)\right)^2 = \frac{1}{w_i^2}\left(\eta_i^2 + (f^{\star}(x_i)-f(x_i))^2 + 2\eta_i(f^{\star}(x_i)-f(x_i))\right).
$$
We know that
$$
|V_i(f)| \le R^2 + 8L_f^2+2\upsilon^2:=R'.
$$
We can calculate the conditional mean of $Z_i$:
\begin{align*}
\mu_i(f) = \frac{1}{w_i^2}\left(\sigma_i^2 + (f^{\star}(x_i)-f(x_i))^2\right),
\end{align*}
and the sum of variance of $Z_i$:
\begin{align*}
\sum_{i=1}^t\Var[V_i(f)] =& \sum_{i=1}^t \frac{1}{w_i^4} \E\left[(\eta_i^2-\sigma_i^2)^2 + 4\eta_i^2(f^{\star}(x_i)-f(x_i))^2\right]\\
\le& \sum_{i=1}^t \frac{1}{w_i^4} \sigma_i^2\left(c_{\eta} + 4(f^{\star}(x_i)-f(x_i))^2\right)\\
\le& \sum_{i=1}^t \frac{\sigma_i^2}{w_i^2} \cdot \left(c_{\eta} + 16\rho^2\cdot\hbeta^2_{t-1}\right),
\end{align*}
where the second inequality holds since 
\begin{align}\label{eq:aae}
    \frac{1}{w_i^2}(f^\star(x_i)-f(x_i))^2 \le& \frac{1}{w_i^2}\cdot\sup_{f,f'\in\calF_{i-1}}\frac{|f(x_i)-f'(x_i)|^2}{\sum_{k\in[i-1]}\frac{1}{\bsigma_k^2}(f(x_k)-f'(x_k))^2+\lambda} \cdot \left(\sum_{k\in[i-1]}\frac{1}{\bsigma_k^2}(f(x_k)-f^\star(x_k))^2+\lambda\right)\notag\\
    \le& \rho^2\left(2\sum_{k\in[t-1]}\frac{1}{\bsigma_k^2}(f(x_k)-\hf_{t-1}(x_k))^2 + 2\sum_{k\in[i-1]}\frac{1}{\bsigma_k^2}(f^\star(x_k)-\hf_{t-1}(x_i))^2 + \lambda\right).\notag\\
    \le& 4\rho^2\cdot\hbeta^2_{t-1},
\end{align}
where the second inequality uses the Cauchy-Schwartz inequality, and the last inequality uses the definition of $\calF_{i-1}$.

Then, we have
\begin{align*}
\sum_{i=1}^t \mu_i^2(f) \le& \sum_{i=1}^t \frac{2}{w_i^4}\big(\sigma_i^4 + (f^{\star}(x_i)-f(x_i))^4\big)\\
\le& 2\sum_{i=1}^t\frac{\sigma_i^4}{w_i^4} + V_t(f,f^\star)\cdot\max_{i\in[t]}\frac{f^{\star}(x_i)-f(x_i))^2}{w_i^2}\\
\le& 2\sigma_{\eta}^2\sum_{i=1}^t\frac{\sigma_i^2}{w_i^2} + V_t(f,f^\star)\cdot 4\rho^2\cdot\hbeta^2_{t-1},
\end{align*}
where the first inequality uses the Cauchy-Schwarz inequality, the last inequality holds due to Assumption \ref{ass:var_peel} and \eqref{eq:aae}.

Then, since from $\theta_\Var^t = (12(2\sigma_\eta^2 + c_{\eta}+L_f^2 + 16\rho^2\cdot\hbeta^2_{t-1}))^{-1}$ we have
\begin{align*}
    &(\theta_{\Var}^t)^2\left(\sum_{i=1}^t\Var[V_i(f)] + \sum_{i=1}^t(\mu_i-\bar{\mu})^2\right)\\
    &\qquad\le (\theta_{\Var}^t)^2\left(\sum_{i=1}^t \frac{\sigma_i^2}{w_i^2} \cdot \left(2\sigma_\eta^2 + c_{\eta} + 16\rho^2\hbeta^2_{t-1}\right) + V_t(f,f^\star)\cdot 4\rho^2\hbeta^2_{t-1}\right)\\
    &\qquad\le (\theta_{\Var}^t)^2\left(t\sigma_\eta^2\left(2\sigma_\eta^2 + c_{\eta} + 16\rho^2\hbeta^2_{t-1}\right) + t\cdot4L_f^2\cdot 4\rho^2\hbeta^2_{t-1}\right)\\
    &\qquad\le \frac{t}{6},
\end{align*}
by the choice of $\iota'(\delta)$ and the condition $\calE_t$ holds, we get
\begin{align*}
    t=& \frac{t}{6} + \frac{5t}{6} \ge (\theta_{\Var}^t)^2\Big(\sum_{i=1}^t\Var[V_i(f)] + \sum_{i=1}^t(\mu_i-\bar{\mu})^2\Big) + 2\log(NTL/\delta).
\end{align*}
Therefore, we can apply Lemma \ref{lem:concentration-catoni} and with a union bound over the covering set of $\cF_{t-1}$, which is denoted as $\cF_{t-1,\upsilon}$, to obtain with probability at least $1-\delta/TL$, for any $f\in\cF_{t-1,\upsilon}$ %\alekh{Should the LHS below have $f$ or $\hf_t$? Also, should the first term involve $\sum_i\Var(V_i(f))$} 
\begin{align}\label{eq:aaf}
\big|\bVar_t(f) - \sum_{i=1}^t\mu_i(f)\big| \le& \theta_{\Var}^t\Big(\sum_{i=1}^t\Var[V_i(f)] + \sum_{i=1}^t\mu^2_i(f)\Big) + 2\frac{\log(NTL/\delta)}{\theta_\Var^t} \notag\\
\le& \theta_{\Var}^t\left(\sum_{i=1}^t \frac{\sigma_i^2}{w_i^2} \cdot \left(2\sigma_\eta^2 + c_{\eta} + 16\rho^2\hbeta^2_{t-1}\right) + V_t(f,f^\star)\cdot 4\rho^2\hbeta^2_{t-1}\right) + 2\frac{\log(NTL/\delta)}{\theta_\Var^t} .
\end{align}

Then, for the estimator $\hf_{t-1}$, there exists a $f_\upsilon\in\cF_{t-1,\upsilon}$ such that $\|\hf_{t-1}-f_\upsilon\|_\infty \le\upsilon$. By invoking Lemma \ref{lm:ctn_lipshitz_var}, we have
\begin{align*}
    &\left|\ctn_{\theta_\Var^t}\left(\left\{\frac{1}{w_i^2}(y_i-\hf_{t-1}(x_i))^2\right\}_{i\in[t]}\right) - \ctn_{\theta_\Var^t}\left(\left\{\frac{1}{w_i^2}(y_i-f_\upsilon(x_i))^2\right\}_{i\in[t]}\right)\right|\\
    &\qquad\le 4(R+2L_f)^3\upsilon + 2\sqrt{(R+2L_f)\upsilon}.
\end{align*}
Also, we can get
\begin{align*}
    \big|\sum_{i=1}^t\mu_i(\hf_{t-1}) - \sum_{i=1}^t\mu_i(f_\upsilon)\big| \le& 4L_f\upsilon T.
\end{align*}
Combining the results above and by the choice of $\theta_\Var^t$, we obtain that with probability at least $1-\delta$,
\begin{align*}
    &\big|\bVar_t(\hf_{t-1}) - \sum_{i=1}^t\mu_i(\hf_{t-1})\big|\\
    &\qquad\le \big|\bVar_t(f_\upsilon) - \sum_{i=1}^t\mu_i(f_\upsilon)\big| + T(4(R+2L_f)^3\upsilon + 2\sqrt{(R+2L_f)\upsilon}) + 4L_f\upsilon T\\
    &\qquad\le \theta_{\Var}^t\left(\sum_{i=1}^t \frac{\sigma_i^2}{w_i^2} \cdot \left(2\sigma_\eta^2 + c_{\eta} + 16\rho^2\hbeta^2_{t-1}\right) + V_t(f_\upsilon,f^\star)\cdot 4\rho^2\hbeta^2_{t-1}\right) + 2\frac{\iota'(\delta)}{\theta_\Var^t} + \Delta_{\upsilon,3}\\
    &\qquad\le \theta_{\Var}^t\left(\sum_{i=1}^t \frac{\sigma_i^2}{w_i^2} \cdot \left(2\sigma_\eta^2 + c_{\eta} + 16\rho^2\hbeta^2_{t-1}\right) + (V_t(\hf_{t-1},f^\star)+4L_f\upsilon)\cdot 4\rho^2\hbeta^2_{t-1}\right)+ 2\frac{\iota'(\delta)}{\theta_\Var^t}+ \Delta_{\upsilon,3}
\end{align*}
where $\Delta_{\upsilon,3}=4(R+2L_f)^3\upsilon T + 2\sqrt{(R+2L_f)\upsilon}T + 4L_f\upsilon T$.
Further, we have 
$$
\sum_{i=1}^t\mu_i(\hf_{t-1}) - \sum_{i=1}^t \frac{\sigma_i^2}{w_i^2} = V_t(\hf_{t-1},f^\star),
$$
and 
\begin{align*}
    V_t(\hf_{t-1},f^\star) =& \sum_{i\in[t-1]}\frac{(\hf_{t-1}(x_i)-f^\star(x_i))^2}{w_i^2} + \frac{(\hf_{t-1}(x_t)-f^\star(x_t))^2}{w_t^2}\\
    \le& \hbeta_{t-1}^2 + \frac{D_t^2}{w_t^2}\cdot\hbeta_{t-1}^2\\
    \le& (1+\rho^2)\hbeta_{t-1}^2,
\end{align*}
where the first inequality uses the definition of $D_t$ and $f^\star\in\cF_{t-1}$. Combining the results above and the value of $\theta_\Var^t = (12(2\sigma_\eta^2 + c_{\eta} + 16\rho^2\hbeta^2_{t-1}))^{-1}$ leads to
\begin{align*}
&\big|\bVar_t(\hf_t) - \sum_{i=1}^t \frac{\sigma_i^2}{w_i^2}\big|\\
&\qquad\le \theta_{\Var}^t\left(\sum_{i=1}^t \frac{\sigma_i^2}{w_i^2} \cdot \left(2\sigma_\eta^2 + c_{\eta} + 16\rho^2\hbeta^2_{t-1}\right) + ((1+\rho^2)\hbeta_{t-1}^2+4L_f\upsilon T)\cdot 4\rho^2\hbeta^2_{t-1}\right) + 2\frac{\iota'(\delta)}{\theta_\Var^t} + \Delta_{\upsilon,3} + (1+\rho^2)\hbeta_{t-1}^2\\
&\qquad\le \frac{1}{4}\sum_{i=1}^t \frac{\sigma_i^2}{w_i^2} + \frac{1+\rho^2}{16}\hbeta_{t-1}^2 + \frac{1}{4}L_f\upsilon T + 8\iota'(\delta)(2\sigma_\eta^2+c_\eta) + 64\rho^2\hbeta_{t-1}^2+\Delta_{\upsilon,3}+(1+\rho^2)\hbeta_{t-1}^2\\
&\qquad\le \frac{1}{4}\sum_{i=1}^t \frac{\sigma_i^2}{w_i^2} + 67\Big(4320(\iota'(\delta))^2\rho^2\sum_{i=1}^{t-1} \frac{\sigma_i^2}{w_i^2} + 16\times 5660(\iota'(\delta))^32\rho^2(2\sigma_\eta^2+c_\eta)+120\iota'(\delta)\rho^2+26\Delta_{\upsilon,2}+4\lambda\Big) + \frac{1}{4}L_f\upsilon T +
\Delta_{\upsilon,3}\\
&\qquad\le \frac{1}{2}\sum_{i=1}^t \frac{\sigma_i^2}{w_i^2} + 67\Big(16\times 5660(\iota'(\delta))^3\rho^2(2\sigma_\eta^2+c_\eta)+26\Delta_{\upsilon,2}+4\lambda\Big) + 8\iota'(\delta)(2\sigma_\eta^2+c_\eta) + \Delta_\upsilon\\
&\qquad\le \frac{1}{2}\sum_{i=1}^t \frac{\sigma_i^2}{w_i^2} + 14\iota'(\delta)(2\sigma_\eta^2+c_\eta) + 43\Delta_{\upsilon}+268\lambda,
\end{align*}
where the second inequality uses $\rho\le 1$, and the third inequality uses the value of $\hbeta_{t-1}^2$, the last second inequalities holds since we know from $\rho\le 1/(1076\iota'(\delta))$ that $67\times4320(\iota'(\delta))^2\rho^2\le 1/4$, and the last inequality also holds due to $67\times4\times12^3(\iota'(\delta))^2\rho^2\le 2$,
and we define $\Delta_{\upsilon}=67\times26\Delta_{\upsilon,2}+\Delta_{\upsilon,3}+\frac{1}{4}L_f\upsilon T = \Theta(L_f\upsilon T^2 + L_f^4R^2\iota'(\delta)\upsilon^{0.5}T^{3.5} + L_f^3R^{1.5}\upsilon^{0.25}T^{1.25} + R^3L_f^3\upsilon T + \sqrt{RL_f\upsilon}T)$, which concludes the proof.
There is a fixable error. Now, I change the plug-in in $\hVar_t$ from $\hf_t$ to $\hf_{t-1}$, and change the analysis above, so now the upper and lower bound is reasonable.
%\alekh{Shouldn't the upper and lower bounds involve 7/6 and 5/6 on the $\sigma_i^2/w_i^2$ term respectively?}
\end{proof}

\begin{lemma}[Formal version of Lemma \ref{lm:variance}]
\label{lm:E_var}
Recall the definition of the variance estimation from \eqref{eq:def_hatvar}:
\begin{align*}
\hVar_t^l =& \bVar_t^l+ 14\iota'(\delta)(2\sigma_\eta^2+c_\eta) + 43\Delta_{\upsilon}+268\lambda^l,
\end{align*}
where $\theta_\Var^{t,l} = (4(2\sigma_\eta^2 + c_{\eta}+L_f^2 +  2^{-2l+4}\cdot\hbeta^2_{t-1}))^{-1}$ and $\Delta_{\upsilon}=\Theta(L_f\upsilon T^2 + L_f^4R^2\iota'(\delta)\upsilon^{0.5}T^{3.5} + L_f^3R^{1.5}\upsilon^{0.25}T^{1.25} + R^3L_f^3\upsilon T + \sqrt{RL_f\upsilon}T)$.
Then, conditioned on $\Econv^{t-1}$, when $2^l\ge 1076\iota'(\delta)$ we have with probability at least $1-2\delta$ for all $t\ge 14(\iota'(\delta))^2$,
\begin{align*}
    &\frac{1}{2}\sum_{i\in\Psi_t^l} \frac{\sigma_i^2}{w_i^2} \le \hVar_t^l \le \frac{3}{2}\sum_{i\in\Psi_t^l} \frac{\sigma_i^2}{w_i^2} + 2\Big(14\iota'(\delta)(2\sigma_\eta^2+c_\eta) + 43\Delta_{\upsilon}+268\lambda^l\Big),
\end{align*}
which implies that $\cup_{t\in[T]:t\ge 14(\iota'(\delta))^2}\Evar^t$ holds.
\end{lemma}
\begin{proof}[Proof of Lemma \ref{lm:variance}]
If we suppose that $\calE_t$ happens, since $\Econv^{t-1}$ holds true, we can apply Lemma \ref{lm:variance_peel} to each $l\in[L]$ satisfying that $2^l\ge 1076\iota'(\delta)$ with $\rho=2^{-l}$ and obtain with probability $1-2\delta$,
\begin{align*}
     \left|\bVar_t(\hf_t) - \frac{1}{2}\sum_{i=1}^t \frac{\sigma_i^2}{w_i^2}\right| \le 14\iota'(\delta)(2\sigma_\eta^2+c_\eta) + 43\Delta_{\upsilon}+268\lambda^l,
\end{align*}
which indicates the desired result according to the definition of $\hVar_t^l$.
%\alekh{Why do you get to drop the remaining terms? They are positive.}\chenlu{Now this proof also changes accordingly}

Furthermore, we show that when $t\ge 14(\iota'(\delta))^2$, $\calE_t$ holds true.
\begin{align*}
    t &\ge 14(\iota'(\delta))^2 = 4(\iota'(\delta))^2\times 2 + 6(\iota'(\delta))^2\\
    &\ge 4(\iota'(\delta))^2\frac{\sum_{i\in\Psi_t^l}\frac{\sigma_i^2}{w_i^2}}{\bVar_t^l}+6(\iota'(\delta))^2,
\end{align*}
where the second inequality uses $\frac{1}{2}\sum_{i\in\Psi_t^l} \frac{\sigma_i^2}{w_i^2} \le \hVar_t^l$.
\end{proof}



\paragraph{Part III: Sharpness of the confidence set}
\begin{lemma}\label{lm:confidence_set_peel}
Conditioned on $\Evar^t$, if we take $\upsilon=O(1/L_f^{12}R^6(\iota'(\delta))^2T^7)$, $\epsilon=2^{-2l}$, and take the confidence parameter as \eqref{eq:hbeta_tl}:
\begin{align*}
    (\hbeta_t^l)^2 = 2880(\iota'(\delta))^22^{-2l}\hVar_t^l + 60\iota'(\delta)2^{-2l} +12\Delta_{\upsilon,2} +2\lambda^l,
\end{align*}
where $\Delta_\upsilon=\Theta\big(L_f\upsilon T^2 + L_f^4R^2\iota'(\delta)\upsilon^{0.5}T^{3.5} + L_f^3R^{1.5}\upsilon^{0.25}T^{1.25} + R^3L_f^3\upsilon T + \sqrt{RL_f\upsilon}T\big)$. 
We have
$$
(\hbeta_t^l)^2 \le 4320(\iota'(\delta))^22^{-2l}\sum_{i\in\Psi_t^l} \frac{\sigma_i^2}{w_i^2} + 16\times 5660(\iota'(\delta))^3 2^{-2l}(2\sigma_\eta^2+c_\eta)+120\iota'(\delta)2^{-2l}+26\Delta_\upsilon+4\lambda^l.
$$

Additionally, with probability at least $1-\delta$, the following event occurs: 
$$\cup_{t\in[T]:t\ge 3(\iota'(\delta))^2}\Econv^t =\left\{f^\star\in\calF_t^l,~\text{for}~l\in[L],~2^l\ge 1076\iota'(\delta)\right\}.$$
\end{lemma}
\begin{proof}
By invoking Lemma \ref{lm:concen_ctn_peel_1} for $l$ with $\rho=2^{-l}$, we get with probability at least $1-\delta$,
\begin{align}\label{eq:aag}
    &\sum_{i\in\Psi_t^l}\frac{(f^\star(x_i)-\hf_t^l(x_i))^2)}{w_i^2}+\lambda^l\notag\\
    &\qquad\le \frac{1}{2}(\hbeta_{t-1}^l)^2 + 6\bigg(48(\iota'(\delta))^22^{-2l}\Big(\frac{\big(\sum_{i\in\Psi_t^l}\sigma_i^2/w_i^2\big)^2}{\hVar_t^l} + \hVar_t^l\Big) + 5\iota'(\delta)2^{-2l} +\Delta_{\upsilon,2}\bigg)+\lambda^l.
\end{align}

Then, since Corollary \ref{lm:E_var} implies that with probability at least $1-2\delta$,
\begin{align}\label{eq:aaj}
    &\frac{1}{2}\sum_{i\in\Psi_t^l} \frac{\sigma_i^2}{w_i^2} \le \hVar_t^l \le \frac{3}{2}\sum_{i\in\Psi_t^l} \frac{\sigma_i^2}{w_i^2} + 2\Big(14\iota'(\delta)(2\sigma_\eta^2+c_\eta) + 43\Delta_{\upsilon}+268\lambda^l\Big),
\end{align}
%\alekh{Again, don't we have some negative terms in the denominator when you lower bound $\bVar_t^l$?}
we get with probability at least $1-2\delta$,
\begin{align*}
    \frac{\big(\sum_{i\in\Psi_t^l}\sigma_i^2/w_i^2\big)^2}{\hVar_t^l} + \hVar_t^l \le 5\hVar_t^l.
\end{align*}
Hence, we deduce that
\begin{align*}                          
    &\sum_{i\in\Psi_t^l}\frac{(f^\star(x_i)-\hf_t^l(x_i))^2)}{w_i^2}+\lambda^l\notag\\
    &\qquad\le \frac{1}{2}(\hbeta_{t-1}^l)^2 + 6\bigg(48(\iota'(\delta))^22^{-2l}5\hVar_t^l + 5\iota'(\delta)2^{-2l} +\Delta_{\upsilon,2}\bigg)+\lambda^l\\
    &\qquad\le \frac{1}{2}(\hbeta_{t-1}^l)^2 + 1440(\iota'(\delta))^22^{-2l}\hVar_t^l + 30\iota'(\delta)2^{-2l} +6\Delta_{\upsilon,2} +\lambda^l\\
    &\qquad\le (\hbeta_t^l)^2,
\end{align*}
where the last inequality uses the definition of $\hbeta_t^l$.

Moreover, we define
$$
B_\beta^l = 16\times 5660(\iota'(\delta))^32^{-2l}(2\sigma_\eta^2+c_\eta)+120\iota'(\delta)2^{-2l}+26\Delta_\upsilon+4\lambda^l,
$$
and deduce from \eqref{eq:aaj} that
\begin{align*}
    (\hbeta_t^l)^2 =& 2880(\iota'(\delta))^22^{-2l}\hVar_t^l + 60\iota'(\delta)2^{-2l} +12\Delta_{\upsilon,2} +2\lambda^l\\
    \le&4320(\iota'(\delta))^22^{-2l}\sum_{i\in\Psi_t^l} \frac{\sigma_i^2}{w_i^2} + 5660(\iota'(\delta))^22^{-2l}\Big(14\iota'(\delta)(2\sigma_\eta^2+c_\eta) + 43\Delta_{\upsilon}+268\lambda^l\Big)+ 60\iota'(\delta)2^{-2l} +12\Delta_{\upsilon,2} +2\lambda^l\\
    \le& 4320(\iota'(\delta))^22^{-2l}\sum_{i\in\Psi_t^l} \frac{\sigma_i^2}{w_i^2} + \frac{1}{2}B_\beta^l + \frac{1}{2}B_\beta^l\\
    =& 4320(\iota'(\delta))^22^{-2l}\sum_{i\in\Psi_t^l} \frac{\sigma_i^2}{w_i^2} + 16\times 5660(\iota'(\delta))^32^{-2l}(2\sigma_\eta^2+c_\eta)+120\iota'(\delta)2^{-2l}+26\Delta_\upsilon+4\lambda^l,
\end{align*}
where the last inequality holds since $2^l\ge 1076\iota'(\delta)$.
\end{proof}


\paragraph{Part IV: Bounding the regret conditioning on good events.}
Recall the notation for the eluder coefficient for each layer %\alekh{Worth recalling here that $w$ is the sequence $w_1,...,w_t$.}:
\begin{align*}
    &D_t^l(x) = \sup_{f,f'\in\calF_{t-1}^l} \frac{|f(x)-f'(x)|}{\sqrt{\sum_{i\in\Psi_{t-1}^l}\frac{1}{w_i^2}(f(x_i)-f'(x_i))^2 + \lambda^l}}.
\end{align*}

\begin{lemma}\label{lm:regret_l_formal}
Let $l_\star=\lceil \log_2(1076\iota'(\delta)) \rceil$.
    Under Assumption \ref{ass:var_peel} and Algorithm \ref{alg:Peeling}, if $\cup_{t\ge14(\iota'(\delta))^2}\Econv^t$ happens, then, for all $l\in[l_\star,L]$, $t\in\Psi_T^l$, and $X_t^*=\argmax_{x\in\cX_t}f^\star(x)\in\cX_t^l$, and the regret at the $l$-th level is bounded by
    $$
    \sum_{t\in\Psi_T^l:t\ge 14(\iota'(\delta))^2} (f^\star(x_t^*) - f^\star(x_t) )\le 2^{-l+3}\hbeta_T^{l-1}|\Psi_T^l|.
    $$
\end{lemma}
\begin{proof}[Proof of Lemma \ref{lm:regret_l}]
First according to Algorithm \ref{alg:Peeling}, we will prove that for all $t\in\Psi_T^l$, $X_t^*\in\cX_t^l$ by induction from $l_\star$ to $l$, where $l$ is the level from which $x_t$ arises, and note that Algorithm \ref{alg:Peeling} starts from level $l_\star$. %\alekh{Is this for all $l$ larger than the lower bound in the lemma statement?}, where we have from Lemma \ref{lm:confidence_set_peel} that
$$
(\hbeta_{t-1}^l)^2 = \Theta\bigg((\iota'(\delta))^22^{-2l}\sum_{i\in\Psi_{t-1}^l}\sigma_i^2/w_i^2 + (\iota'(\delta))^32^{-2l}(\sigma_\eta^2+c_\eta) + \Delta_\upsilon\bigg).
$$

Assume that $X_t^*\in\cX_t^{l_0}$ for some $l_0\in[l_\star,l-1]$ and $\cX_t^{l_0+1}$ exists. Since $\Psi_t^{l_0+1}$ exists only if $D_t^{l_0}(x)\le 2^{-l_0}$ for all $x\in\cX_t^{l_0}$. Then, we denote $X_t^{l_0}=\argmax_{x\in\cX_t^{l_0}}\hf_{t-1}^{l_0}(x)$ and deduce that
\begin{align*}
    \hf_{t-1}^{l_0}(x_t^*) - \hf_{t-1}^{l_0}(x_t^{l_0}) \ge& f^\star(x_t^*) - f^\star(x_t^{l_0}) - \left|\hf_{t-1}^{l_0}(x_t^*) - f^\star(x_t^*)\right| - \left|\hf_{t-1}^{l_0}(x_t^{l_0}) - f^\star(x_t^{l_0})\right|\\
    \ge& - D_t^{l_0}(x_t^*)\hbeta_{t-1}^l - D_t^{l_0}(x_t^*)\hbeta_{t-1}^l\\
    \ge& -2^{-l_0+1}\hbeta_{t-1}^l,
\end{align*}
where the second inequality uses $X_t^*=\argmax_{x\in\cX_t}f^\star(x)$, and the last inequality holds by using  $D_t^{l_0}(x_t^*)\le 2^{-l_0},~D_t^{l_0}(x_t^{l_0})\le 2^{-l_0}$. Therefore, from the definition of $\cX_t^{l_0+1}=\{x\in\cX_t^{l_0} \mid \hf_{t-1}^{l_0}(x) \ge \max_{x\in\cX_t^{l_0}} \hf_{t-1}^{l_0}(x) - 2^{-l_0+1}\hbeta_{t-1}^{l-1}\}$, we obtain that $X_t^*\in\cX_t^{l_0+1}$. Hence, via induction, we can prove that $X_t^*\in\cX_t^l$.

Next, for the $l_0+1\le l$ since $X_t^*,X_t\in\cX_t^l$, we know that %\alekh{We are only consider $l \leq l_t$ where $l_t$ is the level from which $x_t$ arises?} \chenlu{Yes.}
\begin{align}\label{eq:aah}
    \hf_{t-1}^{l-1}(x_t^*) - \hf_{t-1}^{l-1}(x_t) \le 2^{-l+2}\hbeta_t^{l-1},
\end{align}
and 
\begin{align}\label{eq:aai}
    D_t^{l-1}(x_t^*)\le 2^{-l+1},\quad D_t^{l-1}(x_t)\le 2^{-l+1}.
\end{align}
Thus, we derive that
\begin{align*}
    f^\star(x_t^*) - f^\star(x_t) \le& \hf_{t-1}^{l-1}(x_t^*) - \hf_{t-1}^{l-1}(x_t) + \left|f^\star(x_t^*) - \hf_{t-1}^{l-1}(x_t^*)\right| + \left|f^\star(x_t) - \hf_{t-1}^{l-1}(x_t)\right|\\
    \le& 2^{-l+2}\hbeta_t^{l-1} + D_t^{l-1}(x_t^*)\hbeta_t^{l-1} + D_t^{l-1}(x_t)\hbeta_t^{l-1}\\
    \le& 2^{-l+2}\hbeta_t^{l-1} + 2^{-l+1}\hbeta_t^{l-1} + 2^{-l+1}\hbeta_t^{l-1}\\
    =& 2^{-l+3}\hbeta_t^{l-1},
\end{align*}
which implies that
\begin{align*}
    \sum_{t\in\Psi_T^l:t\ge 14(\iota'(\delta))^2} (f^\star(x_t^*) - f^\star(x_t) )\le & \sum_{t\in\Psi_T^l}2^{-l+3}\hbeta_t^{l-1}\\
    \le& 2^{-l+3}\hbeta_T^{l-1}|\Psi_T^l|.
\end{align*}
\end{proof}

Now, we provide the proof for the main theorem.
\begin{proof}[Proof of Theorem \ref{th:unknown_var}]
Recall the definition of two good events
\begin{align*}
     &\Evar^t:=\left\{\sum_{i\in\Psi_t^l}\frac{\sigma_i^2}{w_i^2} \le 2\hVar_t^l,~\text{for}~l\in[L],~2^l\ge 1076\iota'(\delta)\right\},\\
    &\Econv^t \defeq \left\{f^\star\in\calF_t^l,~\text{for}~l\in[L],~2^l\ge 1076\iota'(\delta)\right\}.
\end{align*}
By invoking Lemma \ref{lm:confidence_set_peel} and \ref{lm:E_var}, we have
$$
\P\left(\cup_{t\in[T]:t\ge 3(\iota'(\delta))^2}(\Econv^t\cap\Evar^t)\right) \ge 1-3\delta.
$$
In the remaining proof, we suppose that $\cup_{t\in[T]:t\ge 3(\iota'(\delta))^2}(\Econv^t\cap\Evar^t)$ holds. Let $l_\star$ be the minimum $l\in[L]$ such that $2^l\ge\max\{18c_\beta^2(\iota'(\delta))^2, 6\times48^2L_f(\iota'(\delta))^2\}$.

We can decompose the regret into three parts
\begin{align*}
R_T =& \underbrace{\sum_{l=l_\star}\sum_{t\in\Psi_T^l} (f^{\star}(x_t^*) - f^{\star}(x_t))}_{I_1} + \underbrace{\sum_{l=l_\star+1}^{L}\sum_{t\in\Psi_T^l} (f^{\star}(x_t^*) - f^{\star}(x_t))}_{I_2} + \underbrace{\sum_{t\in[T]\setminus\cup_{l\in[L]}\Psi_T^l} (f^{\star}(x_t^*) - f^{\star}(x_t))}_{I_3},
\end{align*}
where we use the short notation $X_t^*=\argmax_{x\in\cX_t}f^\star(x)$.
For the term $I_1$, we have
\begin{align*}
I_1 \le& 2l_\star L_f |\Psi_T^{l_\star}|\\
\le& 2l_\star L_f 2^{2l_\star}\cdot \sum_{t\in\Psi_{T,l_\star}} \frac{(D_t^{l_\star}(x_t))^2}{w_t^2}\\
\le& L_f l_\star 2^{2l_\star+1} \dim_{1,T}(\calF) = \widetilde{O}(L_f \log(N)\cdot\dim_{1,T}(\calF)),
\end{align*}
where the second inequality holds due to $2^{-l_\star}= D_t^{l_\star}(x_t)/w_t$ from Algorithm \ref{alg:Peeling}, and the last inequality follows from the fact that $2^{l_\star}=\Theta(L_f(\iota'(\delta))^2)$.

For the term $I_2$, we invoke Lemma \ref{lm:regret_l} to get
\begin{align*}
I_2 \le& \sum_{l\ge l_\star+1}\hbeta_T^l 2^{-l+3}|\Psi_T^l| \le 8L_f\sum_{l\ge l_\star}\hbeta_T^l 2^{-l}\cdot 2^{2l}\sum_{i\in\Psi_{T,l}} \min\Big\{1, \frac{(D_t^l(x_i))^2}{w_i^2}\Big\}\\
\le& 8L_f\sum_{l\ge l_\star+1}\hbeta_T^l 2^{l} \dim_{1,T}(\calF)\\
=& \widetilde{O}\left(L_f\sqrt{\sum_{t\in[T]}\sigma_t^2 \cdot \log N}\cdot \dim_{1,T}(\calF) + L_f(\log N)^{3/4}\dim_{1,T}(\calF)(\sqrt{c_{\eta}} + \sigma_{\eta})\right),
\end{align*}
where the last inequality holds since we know from Lemma \ref{lm:confidence_set_peel} that $\iota'(\delta)=\Theta\Big(\sqrt{\log\Big(RL_f(\sigma_\eta^2+c_\eta+\Delta_\upsilon+\lambda^l)NLT/\delta\Big)}\Big)$ and
$$
\hbeta_t^l = \Theta\bigg(\iota'(\delta)2^{-l}\sqrt{\sum_{i\in\Psi_t^l}\sigma_i^2/w_i^2} + (\iota'(\delta))^{3/2}2^{-l}(\sigma_\eta+\sqrt{c_\eta}) + \sqrt{\Delta_\upsilon}\bigg).
$$
For the term $I_3$, we have
\begin{align*}
I_3 \le& \sum_{t\in[T]\setminus\cup_{l\in[L]}\Psi_T^l} (f_{t-1}^{l_t}(x_t)-f^{\star}(x_t))\\
\le& \sum_{t\in[T]\setminus\cup_{l\in[L]}\Psi_T^l} \hbeta_t^{l_t}\cdot \gamma\\
=& \widetilde{O}\left(T\cdot \sigma_\eta\sqrt{T}\cdot \frac{1}{\sigma_\eta T^{3/2}}\right),
\end{align*}
where the first inequality follows from the peeling rule that $D_{\calF}^{t,l}(x_t)\le \gamma$ for $t\in[T]\setminus\cup_{l\in[L]}\Psi_T^l$, and the second inequality uses the upper bound of $\hbeta_t^{l_t}$ and the $\gamma=1/\sigma_\eta T^{3/2}$.

Combining all three terms leads us to the eventual result.
\end{proof}


\section{Another Algorithm: Construct Two Confidence 
Sets}\label{aps:Construct Two Confidence 
Sets}
In this section, we develop a substitution for the Catoni estimator by constructing another candidate set and picking one estimator out of the set instead of solving the min-max optimization as \eqref{def:catoni-esti}. We use the known variance case to present the idea and result. Additionally, for simplicity, we consider the finite function space $\cF$ with cardinality $N$ in this section. By standard analysis for the union bound over the cover set, we can extend the analysis to infinite function space with finite covering number as in Appendix \ref{apss:pf_th1} and \ref{apss:th:unknown_var}.

\paragraph{Notations.}
Recall the probability parameter as follows:
\begin{align}\label{def:delta-ctn}
    \delta_{n,t} \defeq \frac{\delta}{N(T+1)},~~\delta_t \defeq \sum_{n\in[N]}\delta_{n,t} = \frac{\delta} {T+1}.
\end{align}
Note again we have $\sum_{n\in[N], t\in[T]}\delta_{n,t} = \sum_{t\in[T]} \delta_t \le \delta$.

We also define the following logarithmic factor throughout the analysis that
\begin{align*}
    \iota\left(\delta_{n,t}\right) = \sqrt{\log\left(\frac{\sqrt{21}\cdot288\cdot L_f^2R^2T^{3.5}}{\delta_{n,t}}\right)} \ge  \sqrt{\log\left(\frac{96R(1+2R/\epsilon)T^2}{\min(1,\alpha^2/\sqrt{20T}L_f^2)\epsilon^2\delta_{n,t}}\right)},
\end{align*}
where for the last inequality we choose $\alpha = 1/\sqrt{T}$, $\epsilon=1$ and use the assumption that $R\ge1$, $L_f\ge1$ without loss of generality.

\begin{algorithm}
\caption{Catoni-OFUL with Candidate Set}\label{alg:Catoni-OFUL with Candidate Set}
\begin{algorithmic}
    \STATE \textbf{Input:} Parameter $\delta_{n,t}$ and $\hbeta_t$ for each $t\in[T]$.
    \FOR{t=1,2,\ldots,T}
        \STATE Pick action $x_t$ according to $\max_{f\in\calF_{t-1},x\in\calD_t} f(x)$;
        \STATE Observe the reward $y_t$;
        \STATE Let the weight $\bsigma_t = \max\left(\sigma_t, \alpha, 4\sqrt{2\iota(\delta_{n,t})L_f\cdot D_\calF(x_t; x_{[t-1]},\bsigma_{[t-1]})}\right)$;
        \STATE Construct $\hat{\calF}_t$ as defined in \eqref{def:catoni_can_set} and pick any $\hat{f}_t \in \hat{\calF}_t$;
        \STATE Construct confidence set 
        $$
            \calF_t \defeq \Big\{f\in\calF:\sum_{i\in[t]}\frac{1}{\bsigma_i^2}\left(f(x_i)-\hat{f}_t(x_i)\right)^2\le \hbeta_t^2\Big\};
        $$
        
    \ENDFOR
\end{algorithmic}
\end{algorithm}

Suppose that for each $t\in[T]$, the upper bound of the noise variance $\sigma_t^2$ is known. We now consider the following VOFUL algorithm tailored to this nonlinear function class setup. After specifying parameters $\delta_{n,t}$ and $\hbeta_t$ for $t\in[T]$, and the weight 
$$
\bsigma_t = \max\left(\sigma_t, \alpha, 4\sqrt{2\iota(\delta_{n,t})L_f\cdot D_\calF(x_t; x_{[t-1]},\bsigma_{[t-1]})}\right)
$$
depending on the variance $\sigma_t$ and eluder coefficient $D_{\calF}$, 
we define the candidate set for the estimator as
\begin{align}\label{def:catoni_can_set}
    \hat{\calF}_t & \defeq  \bigg\{\hat{f}\in\calF: \min_{f\in\calF}\sum_{i\in[t]}\frac{1}{\bsigma_i^2}\left(f(x_i)-\hat{f}(x_i)\right)^2+2t\cdot \ctn_{\theta(f, \hat{f})}(Z_1,\cdots, Z_t)\ge -\frac{1}{4}\hbeta_t^2 \bigg\},
\end{align}
where $Z_i(f,\hf) = \frac{1}{\bsigma_i^2}(f(x_i)-\hat{f}(x_i)) (\hat{f}(x_i)-y_i)$ and
$$
\theta_t(f,\hf) = \sqrt{\frac{ \iota^2\left(\delta_{n,t}\right)}{\sum_{i\in[t]}\frac{1}{\bsigma_i^2}\left(f(x_i)-\hat{f}(x_i)\right)^2+\sum_{i\in[t]}\frac{2}{\bsigma_i^4}\left(f(x_i)-\hat{f}(x_i)\right)^4+\epsilon^2}}.
$$
This candidate set selects robust estimations for the true function $f^\star$, and we will prove in the sequel that the $f^\star$ belongs to $\hat{\calF}_t$.
Then, we choose any function $\hf_t$ from $\hat{\calF}_t$ and further construct the confidence set $\calF_t$ with a small weighted square error. We will demonstrate that $f^\star\in\calF_t$. Based on the principle of optimism in the face of uncertainty, we choose the greedy function $\hf_t\in\calF_t$ and the greedy action $X_t\in\calD_t$.

\begin{theorem}\label{th:known_var_can}
Under Algorithm~\ref{alg:Catoni-OFUL with Candidate Set} with the parameter $\delta_{n,t}=\delta/N(T+1)$, $\iota\left(\delta_{n,t}\right) = \sqrt{\log\left(\sqrt{21}\cdot288\cdot L_f^2R^2T^{3.5} / \delta_{n,t}\right)}$, and
\begin{align}
    \hbeta_t & \defeq \Big[\Big(8\Big(8\cdot 13^4+2\cdot 13^2+13\Big)\Big)^{1/2} + 13\sqrt{2} \lambda^{1/4}\Big]\iota(\delta_{n,t}),\label{eq:def-beta-ctn_can}
\end{align}
with probability $1-2\delta$, we can bound the regret by 
\[R_T = \widetilde{O}\Big(L_f\cdot \log N\cdot \dim_{\frac{1}{\sqrt{T}},T}(\calF)+L_f\Big(\sum_{t\in[T]}\sigma_t^2\Big)^{1/2} \cdot\sqrt{\dim_{\frac{1}{\sqrt{T}}, T}(\calF)\cdot\log N}\Big).\]
\end{theorem}

We now divide the argument into the following three parts. In the analysis, we omit $(f,f')$ in $Z_i$ and $\theta$ when there is no confusion.

\paragraph{Part I: With high probability $
1-\delta$, all the sets $\hat{\calF}_t$ are non-empty for each $t\in[\widetilde{O}(1), T]$.}

\begin{lemma}\label{lem:ctn-1_can}
For any iteration $t\in[T], t\ge 7\iota^2(\delta_{n,T})$ and the set $\hat{\calF}_t$ as constructed in~\eqref{def:catoni_can_set}, we have with probability at least $1-\delta$, $\cap_{t\in[T], t\ge 7\iota^2(\delta_{n,T})}\hat{\calF}_t \neq \emptyset$.
\end{lemma}
\begin{proof}
It suffices to show that for each $t$, we have $\P(f^\star\notin\hat{\calF}_t)\le \delta_t$. When $f^\star\notin\hat{\calF}_t$, there exists some $f^n\in\calF$ such that 
\begin{align*}
\sum_{i\in[t]}\frac{1}{\bsigma_i^2}\left(f^n(x_i)-f^\star(x_i)\right)^2+2t\cdot \ctn_\theta\left(\left\{\frac{1}{\bsigma_i^2}\left(f^n(x_i)-f^\star(x_i)\right)\left(f^\star(x_i)-y_i\right)\right\}_{i\in[t]}\right)< -\frac{1}{4}\hbeta_t^2,
\end{align*}
which implies that
\begin{align*}
t\cdot\ctn_\theta\left(\left\{\frac{1}{\bsigma_i^2}\left(f^n(x_i)-f^\star(x_i)\right)\left(f^\star(x_i)-y_i\right)\right\}_{i\in[t]}\right)<\frac{-\frac{1}{4}\hbeta_t^2-\sum_{i\in[t]}\frac{1}{\bsigma_t^2}\left(f^n(x_i)-f^\star(x_i)\right)^2}{2}.
\end{align*}

Now we bound the probability that the above inequality happens. We use the notation for any $f,f'\in\calF$
\[Z_i(f,f') = \frac{1}{\bsigma_i^2}\left(f(x_i)-f'(x_i)\right)\left(f'(x_i)-y_i\right), \]
which is short-notated as $Z_i$ when no confusion arises. 

We apply~\Cref{lem:concentration-catoni} to
$Z_i(f^n,f^*)$. The variable $Z_i$ has conditional mean $\mu_i = \E Z_i = 0$, and sum of conditional variance 
\begin{align*}
&\sum_{i\in[t]} \E \frac{1}{\bsigma_i^4}\left(f^n(x_i)-f^\star(x_i)\right)^2\left(f^\star(x_i)-y_i\right)^2\\
&\qquad\le \sum_{i\in[t]}\frac{1}{\bsigma_i^2}\left(f^n(x_i)-f^\star(x_i)\right)^2\\
&\qquad:= V(f^n,f^*),
\end{align*}
where the inequality uses the fact that $\E \left(f^\star(x_i)- y_i\right)^2 = \sigma_i^2\le \bsigma_i^2$. We can also bound $\theta$ by definition $\theta\in[a,A]$ where 
\[A = \iota(\delta_{n,t,l})/\epsilon~~\text{and}~~a=\frac{\iota(\delta_{n,t,l})}{\sqrt{20L_f^4t/\alpha^4+\epsilon^2}}.\]
Thus, we have $\log\left(\frac{48R(1+2AR)t^2}{\min(1,a)\epsilon^2\delta_{n,t}}\log(A/a)\right) \le 3\iota^2(\delta_{n,t,l})$ given choice of $\alpha = 1/\sqrt{T}$ and $\epsilon = 1$, and thus for any iteration 
\begin{align*}
t & \ge 7\iota^2(\delta_{n,t,l})\ge \iota^2(\delta_{n,t,l})+6\iota^2(\delta_{n,t,l}) \ge \iota^2(\delta_{n,t,l})+2\log\left(\frac{48R(1+2AR)t^2}{\min(1,a)\epsilon^2\delta_{n,t}}\log(A/a)\right),\end{align*}
by choice of $\theta =  \sqrt{\frac{\iota^2(\delta_{n,t,l})}{\sum_{i\in[t]}\frac{1}{\bsigma_i^2}\left(f^n(x_i)-f^\star(x_i)\right)^2+\sum_{i\in[t]}\frac{2}{\bsigma_i^4}\left(f^n(x_i)-f^\star(x_i)\right)^4+\epsilon^2}}$, with probability at least $1-\delta_{n,t}$, we have from Lemma \ref{coro:catoni} that
\begin{align*}
& t\cdot \left|\ctn_\theta(\{Z_i\}_{i\in[t]})\right|\\
&\hspace{2em} \le \iota(\delta_{n,t,l})\frac{V+\sum_{i\in[t]}(\mu_i-\bar{\mu})^2}{\sqrt{V + \sum_{i\in[t]}\frac{2}{\bar{\sigma}_i^4}(f^n(x_i)-\tilde{f}^\star(x_i))^4 + \epsilon^2}} + 12\iota(\delta_{n,t,l})\left(\sqrt{V + \sum_{i\in[t]}\frac{2}{\bar{\sigma}_i^4}(f^n(x_i)-\tilde{f}^\star(x_i))^4 + \epsilon^2}\right)\\
&\hspace{4em}+ \epsilon + t\bar{\mu}\\
&\hspace{2em} \stackrel{(o)}\le 13\sqrt{V\cdot\left(1+\max_{i\in[t]}\frac{2}{\bsigma_i^2}\left(f^n(x_i)-f^\star(x_i)\right)^2\right)}\cdot\iota(\delta_{n,t,l})+12\iota(\delta_{n,t,l})\epsilon+\epsilon\\
& \hspace{2em} \stackrel{(i)}{\le}
13\sqrt{V\cdot \left(1+\max_{i\in[t]}\frac{4L_f}{\bsigma_i^2}D_{\calF}(x_i; x_{[i-1]},\bsigma_{[i-1]})\sqrt{\sum_{k\in[i-1]}\frac{1}{\bsigma_k^2}\left(f^n(x_k)-f^\star(x_k)\right)^2+\lambda}\right)}\iota(\delta_{n,t,l})\\
& \hspace{2em} \stackrel{(ii)}{\le} 13\iota(\delta_{n,t,l})\sqrt{\left(1+\frac{\sqrt{\lambda}}{8}\right)V}+\frac{13}{2\sqrt{2}}V^{3/4}\left(\iota^2(\delta_{n,t,l})\right)^{1/4}\\
& \hspace{2em} \stackrel{(iii)}{\le} \frac{V}{4}+169\left(1+\frac{\sqrt{\lambda}}{8}\right)\iota^2(\delta_{n,t,l})+\frac{V}{4}+13^4\iota^2(\delta_{n,t,l})\\
& \hspace{2em} \stackrel{(iv)}{\le} \frac{\frac{1}{4}\hbeta_t^2+\sum_{i\in[t]}\frac{1}{\bsigma_i^2}\left(f^n(x_i)-f^\star(x_i)\right)^2}{2},
\end{align*}
where we use $(o)$ that $\sqrt{a+b}
\le \sqrt{a}+\sqrt{b}$, and the inequality that $\log\left(\frac{48R(1+2AR)t^2}{\min(1,a)\epsilon^2\delta_{n,t}}\log(A/a)\right) \le 3\iota^2(\delta_{n,t,l})$; $(i)$ the range assumption that $|f(\cdot)|\le L_f$, choice of $\epsilon=1$ and definition of $D_\alpha$; $(ii)$ the choice of $\bsigma_i^2\ge 32L_fD_{\calF}(x_i; x_{[i-1]}, \bsigma_{[i-1]})\iota(\delta_{n,t,l})$ and $\sqrt{a+b}\le \sqrt{a}+\sqrt{b}$; $(iii)$ triangle inequality that $\sqrt{ab}\le \frac{a}{2c}+\frac{c\cdot b}{2}$ where we let $a = V$, $b =  169\iota^2(\delta_{n,t,l})$, $c = 2$ and similarly $a^{3/4}b^{1/4}\le \frac{a}{c}+c^3 b$ where we let $a=V$, $b=\iota^2(\delta_{n,t,l})$, and $c = 13\sqrt{2}$; and finally $(iv)$ the definition of $\hbeta_t$ in~\eqref{eq:def-beta-ctn_can} so that $\hbeta_t^2\ge 8\left(13^4+13^2\right)\iota^2(\delta_{n,t,l})+13^2\sqrt{\lambda}\iota^2(\delta_{n,t,l})$.

This implies that 
\[
\P\left(\sum_{i\in[t]}\frac{1}{\bsigma_t^2}\left(f^n(x_i)-f^\star(x_i)\right)^2+2t\cdot \ctn_\theta\left(\left\{\frac{1}{\bsigma_i^2}\left(f^n(x_i)-f^\star(x_i)\right)\left(f^\star(x_i)-y_i\right)\right\}_{i\in[t]}\right)< -\frac{1}{4}\hbeta_t^2\right)\le \delta_{n,t}.
\]

Thus taking a union bound on all $f = f^n\in\calF$ and $t\in[T], t\ge 7\iota^2(\delta_{n,T})$, we can conclude that with probability at least $1-\delta$ (where $\delta = \sum_{n\in[N],t\in[T]}\delta_{n,t}$) for all iteration $7\iota^2(\delta_{n,T}) \le t\le T$, one has $f^\star\in\hat{\calF}_t$, i.e. $\cap_{t\in[T], t\ge 7\iota^2(\delta_{n,T})}\hat{\calF}_t\neq\emptyset$.
\end{proof}


\paragraph{Part II: With high probability $1-\delta$, $f^\star\in\calF_t$ for all $t\in[\widetilde{O}(1),T]$.}

We first provide the following lemma.

\begin{lemma}\label{lem:ctn-2_can}
For any function $f^n\in\calF$, let $Z_i = \frac{1}{\bsigma_i^2}\left(f^\star(x_i)-f^n(x_i)\right)\left(f^n(x_i)-y_i\right)$ and $\theta$ as defined in~\eqref{def:catoni_can_set}, we have  for any $t\ge 7\iota^2(\delta_{n,T})$ with probability $1-\delta_{n,t}$,
\[
t\cdot \left| \ctn_\theta(Z_1,\cdots, Z_t)+\frac{1}{t}\sum_{i\in[t]}\frac{1}{\bsigma_i^2}\left(f^\star(x_i)-f^n(x_i)\right)^2\right|\le \frac{1}{4}\sum_{i\in[t]}\frac{1}{\bsigma_i^2}\left(f^\star(x_i)-f^n(x_i)\right)^2+\frac{\hbeta_t^2}{8}.
\]
\end{lemma}

\begin{proof}
In order to apply the concentration inequality in~\Cref{lem:concentration-catoni}, we first bound the following
\begin{align*}
    V= \sum_{i\in[t]}\E\left[(Z_i-\mu_i)^2|\calH_{i-1}\right] = \sum_{i\in[t]} \frac{1}{\bsigma_i^2}\left(f^\star(x_i)-f^n(x_i)\right)^2,
\end{align*}
and further
\begin{align*}
    \sum_{i\in[t]}\left(\mu_i-\bmu\right)^2\le \sum_{i\in[t]}\mu_i^2 = \sum_{i\in[t]}\frac{1}{\bsigma_i^4}\left(f^\star(x_i)-f^n(x_i)\right)^2\left(f^n(x_i)-f^\star(x_i)\right)^2\le \sum_{i\in[t]}\frac{2}{\bsigma_i^4}\left(f^\star(x_i)-f^n(x_i)\right)^4.
\end{align*}
Now by choice of $\theta = \sqrt{\frac{\iota^2(\delta_{n,t,l})}{\sum_{i\in[t]}\frac{1}{\bsigma_i^2}\left(f^n(x_i)-f^\star(x_i)\right)^2+\sum_{i\in[t]}\frac{2}{\bsigma_i^4}\left(f^n(x_i)-f^\star(x_i)\right)^4+\epsilon^2}}$%\todo{for this choice, not trying to optimize for constants}
, we will have 
with probability $1-\delta_{n,t}$, for any $t\ge 7\iota^2(\delta_{n,T})$, it holds that 
\begin{align*}
& t\cdot\left|\ctn_\theta(\{Z_i\}_{i\in[t]})-\frac{1}{t}\sum_{i\in[t]}\frac{1}{\bsigma_i^2}\left(f^\star(x_i)-f^n(x_i)\right)\left(f^n(x_i)-f^\star(x_i)\right)\right|\\
& \hspace{1em} \le 13\sqrt{\left(\sum_{i\in[t]}\frac{1}{\bsigma_i^2}\left(f^\star(x_i)-f^n(x_i)\right)^2\right)\cdot\left(1+\max_{i\in[t]}\frac{2}{\bsigma_i^2}\left(f^\star(x_i)-f^n(x_i)\right)^2\right)}\iota(\delta_{n,t,l})+13\iota(\delta_{n,t,l}).
\end{align*}

We now proceed to bound $\max_{i\in[t]}\frac{2}{\bsigma_i^2}\left(f^n(x_i)-f^\star(x_i)\right)^2$, we assume $|f^n(x)-f^\star(x)|\le 2L_f$ for any $x\in\calX$, consequently we have
\begin{align*}
\max_{i\in[t]}\frac{2}{\bsigma_i^2}\left(f^n(x_i)-f^\star(x_i)\right)^2 & \le \max_{i\in[t]}\frac{4L_f}{\bsigma_i^2}\sqrt{\left(f^n(x_i)-f^\star(x_i)\right)^2}\\
& \le \max_{i\in[t]}\frac{4L_f}{\bsigma_i^2}D_\calF(x_i; x_{[i-1]},\bsigma_{[i-1]})\sqrt{\sum_{k\in[i]}\frac{1}{\bsigma_k^2}\left(f^n(x_k)-f^\star(x_k)\right)^2+\lambda}\\
& \le \frac{1}{8\iota(\delta_{n,t,l})}\sqrt{\sum_{i\in[t]}\frac{1}{\bsigma_i^2}\left(f^n(x_k)-f^\star(x_k)\right)^2+\lambda}\\
& \le \frac{1}{8\iota(\delta_{n,t,l})}\sqrt{\sum_{i\in[t]}\frac{1}{\bsigma_i^2}\left(f^n(x_k)-f^\star(x_k)\right)^2}+\frac{\sqrt{\lambda}}{8},
\end{align*}
where for the last inequality we use the fact that $\bsigma_i^2\ge 32L_f\iota(\delta_{n,t,l})D_\calF^2(x_i; x_{[i-1]},\bsigma_{[i-1]})$ by the choice of $\bsigma$.

Plugging this back we can conclude that 
\begin{align*}
& t\cdot\left|\ctn_\theta(\{Z_i\}_{i\in[t]})+\frac{1}{t}\sum_{i\in[t]}\frac{1}{\bsigma_i^2}\left(f^\star(x_i)-f^n(x_i)\right)^2\right|-13\iota(\delta_{n,t,l})\\
& \hspace{1em} \le 13\sqrt{\left(1+\frac{\sqrt{\lambda}}{8}\right)}\cdot\sqrt{\sum_{i\in[t]}\frac{1}{\bsigma_i^2}\left(f^\star(x_i)-f^n(x_i)\right)^2}\iota(\delta_{n,t,l})+\frac{13}{2\sqrt{2}}\left(\sum_{i\in[t]}\frac{1}{\bsigma_i^2}\left(f^n(x_t)-f^\star(x_i)\right)^2\right)^{3/4}\left(\iota^2(\delta_{n,t,l})\right)^{1/4}\\
& \hspace{1em}\le \frac{1}{8}\sum_{i\in[t]}\frac{1}{\bsigma_i^2}\left(f^n(x_t)-f^\star(x_i)\right)^2+2\cdot13^2\iota^2(\delta_{n,t,l})\left(1+\frac{\sqrt{\lambda}}{8}\right)+\frac{1}{8}\sum_{i\in[t]}\frac{1}{\bsigma_i^2}\left(f^n(x_t)-f^\star(x_i)\right)^2+8\cdot 13^4\iota^2(\delta_{n,t,l})\\
& \implies ~~t\cdot\left|\ctn_\theta(\{Z_i\}_{i\in[t]})+\frac{1}{t}\sum_{i\in[t]}\frac{1}{\bsigma_i^2}\left(f^\star(x_i)-f^n(x_i)\right)^2\right|\le \frac{1}{4}\sum_{i\in[t]}\frac{1}{\bsigma_i^2}\left(f^n(x_t)-f^\star(x_i)\right)^2+\frac{1}{8}\hbeta_t^2,
\end{align*}
where for the last inequality we use the definition of $\hbeta_t$ so that $\hbeta_t^2\ge 8(8\cdot 13^4+2\cdot 13^2+13)\iota^2(\delta_{n,t,l})+2\cdot 13^2\sqrt{\lambda}\iota^2(\delta_{n,t,l})$. This concludes the proof of lemma.
\end{proof}

\begin{corollary}\label{coro:ctn-2_can}
With high probability $1-\delta$ where $\delta = \sum_{n\in[N], t\in[T]}\delta_{n,t}$, we have $\tilde{f}^\star\in\calF_t$ for all $t\in[T-1]$ satisfying $t\ge 7\iota^2(\delta_{n,T})$. 
\end{corollary}
\begin{proof}
We bound the probability by a union bound argument. Let $\calT\defeq\{t\in[T-1]:t\ge 7\iota^2(\delta_{n,T})\}$ and $Z_i'=\frac{1}{\bsigma_i^2}\left(f^\star(x_i)-f^n(x_i)\right)\left(f^n(x_i)-y_i\right)$, we have
\begin{align*}
    & \P\left(\exists~t,  7\iota^2(\delta_{n,T})\le t\le T, f^\star\notin \calF_t\right) \le  \sum_{n\in[N], t\in\calT}\P\left(\hat{f}_t = f^n, \sum_{i\in[t]}\frac{1}{\bsigma_i^2}\left(f^\star(x_i)-f^n(x_i)\right)^2> \hbeta_t^2\right)\\
    & \hspace{1em} \le \sum_{n\in[N], t\in\calT}\P\left(\begin{array}{r@{}l}
   \sum_{i\in[t]} & \frac{1}{\bsigma_i^2}\left(f^\star(x_i)-f^n(x_i)\right)^2+2t\cdot \ctn_\theta\left(\left\{Z_i'\right\}_{i\in[t]}\right)\ge -\frac{1}{4}\hbeta_t^2\\
   \sum_{i\in[t]} & \frac{1}{\bsigma_i^2}\left(f^\star(x_i)-f^n(x_i)\right)^2> \hbeta_t^2
  \end{array}\right)\\
  & \hspace{1em}= \sum_{n\in[N],t\in\calT}\P\left(  
  \begin{array}{r@{}l}
   \sum_{i\in[t]} & \frac{1}{\bsigma_i^2}\left(f^\star(x_i)-f^n(x_i)\right)^2\le \frac{1}{4}\hbeta_t^2+2t\cdot\left| \ctn_\theta\left(\left\{Z_i'\right\}_{i\in[t]}\right)+\frac{1}{t}\sum_{i\in[t]}\frac{1}{\bsigma_i^2}\left(f^\star(x_i)-f^n(x_i)\right)^2\right|\\
   \sum_{i\in[t]} & \frac{1}{\bsigma_i^2}\left(f^\star(x_i)-f^n(x_i)\right)^2> \hbeta_t^2
  \end{array}
  \right)\\
  & \hspace{1em} \le \sum_{n\in[N], t\in\calT}\P\left(t\cdot\left|\ctn_\theta\left(\left\{Z_i'\right\}_{i\in[t]}\right)+\frac{1}{t}\sum_{i\in[t]}\frac{1}{\bsigma_i^2}\left(f^\star(x_i)-f^n(x_i)\right)^2\right|>\frac{1}{4}\sum_{i\in[t]}\frac{1}{\bsigma_i^2}\left(f^\star(x_i)-f^n(x_i)\right)^2+\frac{1}{8}\hbeta_t^2\right)\\
  & \hspace{1em} \le \sum_{n\in[N], t\in\calT}\delta_{n,t} \le \sum_{n\in[N], t\in[T]} \delta_{n,t} \le   \delta.
\end{align*}
\end{proof}


\paragraph{Part III: Bounding the regret conditioning on good events.}
We now recall the definition that $\calT\defeq \{t\in[T]: t\ge 7\iota^2(\delta_{n,T})\}$, we further denote the good events $\calE = \{\cap_{t\in\calT}\hat{\calF}_t\neq \emptyset\}$ and $\calE' = \{\tilde{f}^\star\in\cap_{t\in\calT}\calF_t\}$. 

\begin{proof}[Proof of Theorem \ref{th:known_var_can}]
Conditioning on both good events $\calE\cap\calE'$, we let $f^{n_t}$ be the function maximizer in set $\calF_{t-1}$ we pick at step $t$ and can bound the regret by
\begin{align*}
    R_T & = \sum_{t\in[T]}r_t = \sum_{t\in[T]} \left(f^\star(x_t^\star)-f^\star(x_t)\right)\le  2L_f\left(7\iota^2(\delta_{n,T})+2\right)+ \sum_{t-1\in\calT}\left(f^{n_t}(x_t)-f^\star(x_t)\right)\\
    & \le 2L_f\left(7\iota^2(\delta_{n,T})+2\right)+\sum_{t-1\in\calT}\left(\left|f^{n_t}(x_t)-\hf_{t-1}(x_t)\right|+\left|\hf_{t-1}(x_t)-f^\star(x_t)\right|\right)\\
    & \stackrel{(i)}{\le} O\left( L_f\cdot\iota^2(\delta_{n,T})\right) +\sum_{t-1\in\calT}\left[ \bsigma_t D_{\calF}(x_t,\bsigma_t;x_{[t-1]},\bsigma_{[t-1]})\cdot \left(\sqrt{\sum_{i\in[t-1]}\frac{1}{\bsigma_i^2}\left(f^{n_t}(x_i)-\hf_{t-1}(x_i)\right)^2+\lambda}\right.\right.\\
    & \hspace{15em} \left.\left. +\sqrt{\sum_{i\in[t-1]}\frac{1}{\bsigma_i^2}\left(f^{\star}(x_i)-\hf_{t-1}(x_i)\right)^2+\lambda}\right)\right]\\
    & \stackrel{(ii)}{\le} O\left( L_f\cdot\iota^2(\delta_{n,T})\right)+ \sum_{t-1\in\calT}2\bsigma_tD_\calF(x_t,\bsigma_t; x_{[t-1]},\bsigma_{[t-1]})\cdot\left(\sqrt{\hbeta_{t-1}^2+\lambda}\right),
\end{align*}
where we use $(i)$ the definition of $D_{\alpha}$ for each $t\in[T]$ and $(ii)$ the definition of $ \calF_{t-1}$ and that $f^{n_t}, f^\star\in\calF_{t-1}$ conditioning on $\calE$ and $\calE'$.

Combining this with the range bound that $[0,1]$ of each individual reward one may receive by assumption, one can conclude that 
\begin{equation}\label{eq:regret-summation-catoni_can}
R_T\le O\left( L_f\cdot\iota^2(\delta_{n,T})\right)+2L_f\sum_{t-1\in\calT}\min\left(1,\bsigma_tD_\calF(x_t,\bsigma_t; x_{[t-1]},\bsigma_{[t-1]})\sqrt{\hbeta_{t-1}^2+\lambda}\right).
\end{equation}

To finally bound the regret, we bound the second term in RHS of $R_T$ expression in~\eqref{eq:regret-summation-catoni_can} respectively. These steps mainly follow Lemma 4.4 in~\cite{zhou2022computationally}. We can decompose the terms by considering $\calI_1  = \{t-1\in\calT|D_\calF(x_t,\bsigma_t; x_{[t-1]},\bsigma_{[t-1]})\ge1\}$ and $\calI_2 = \{t-1\in\calT, t\notin \calI_1\}$.

For the first set, we bound its size naively by
\[
|\calI_1| \le \sum_{t\in\calI_1} \min\left(1, D^2_\calF(x_t,\bsigma_t; x_{[t-1]},\bsigma_{[t-1]})\right)\le  \dim_{\alpha, T}(\calF).
\]

For the second set, we bound the summation of terms of interest contraining on $\calI_2$ by 
\begin{align*}
    & \sum_{t\in\calI_2}\bsigma_t\sqrt{\hbeta^2_{t-1}+\lambda}\cdot D_\calF(x_t,\bsigma_t; x_{[t-1]},\bsigma_{[t-1]})\\ & \hspace{2em} \le  \sum_{t\in\calI_2, \bsigma_t = \sigma_t~\text{or}~\alpha}\bsigma_t\sqrt{\hbeta^2_{t-1}+\lambda}\cdot D_\calF(x_t,\bsigma_t; x_{[t-1]},\bsigma_{[t-1]})\\
    & \hspace{4em} +\sum_{t\in\calI_2, \bsigma_t = 4\sqrt{2\iota(\delta_{n,t,l})L_fD_\calF(x_t; x_{[t-1]},\bsigma_{[t-1]})} }\bsigma_t\sqrt{\hbeta^2_{t-1}+\lambda}\cdot D_\calF(x_t,\bsigma_t; x_{[t-1]},\bsigma_{[t-1]})\\
    & \hspace{2em} \stackrel{(i)}{\le} \sum_{t\in[T]} \left(\sigma_t+\alpha\right)\sqrt{\hbeta_{t-1}^2+\lambda}\cdot D_\calF(x_t,\bsigma_t;x_{[t-1]},\bsigma_{[t-1]})+ \sum_{t\in[T]}32L_f\iota(\delta_{n,t,l})\sqrt{\hbeta_{t-1}^2+\lambda}\cdot D^2_\calF(x_t,\bsigma_t;x_{[t-1]},\bsigma_{[t-1]})\\
    & \hspace{2em} \stackrel{(ii)}{\le} \sqrt{2\sum_{t\in[T]}(\hbeta_{t-1}^2+\lambda)(\sigma_t^2+\alpha^2)}\sqrt{\dim_{\alpha,T}(\calF)}+16L_f\iota(\delta_{n,t,l})\max_{t\in[T]}\sqrt{\hbeta_{t-1}^2+\lambda}\cdot\dim_{\alpha,T}(\calF).
\end{align*}
Here for $(i)$ we use the condition for each distinct set and for $(ii)$ we use Cauchy-Schwarz inequality for the first term and the definition of $\dim_\alpha$ for both terms.

Consequently plugging these back in~\eqref{eq:regret-summation-catoni_can} and take supremum over $x:|x|=T$, we conclude that with probability at least $1-2\delta$, %\todo{on the high level the first term is due to the fact that we don't have any regret control on the first $\log$ number of steps}
\begin{align*}
    R_T & = O\left(L_f\cdot\iota^2(\delta_{n,T})+ \dim_{\alpha,T}(\calF)+L_f\cdot \iota(\delta_{n,t,l})\cdot\max_{t\in[T]}\sqrt{\hbeta_{t-1}^2+\lambda}\cdot\dim_{\alpha,T}(\calF)\right.\\
    &\hspace{2em} \left.+L_f\sqrt{\sum_{t\in[T]}\left(\hbeta_{t-1}^2+\lambda\right)\left(\sigma_t^2+\alpha^2\right)}\cdot\sqrt{\dim_{\alpha,T}(\calF)}\right)\\
    & = \widetilde{O}\left(L_f\cdot \log \calN\left(\calF,\frac{1}{L_fT^2} \right)\cdot \dim_{\frac{1}{\sqrt{T}},T}(\calF)+L_f\sqrt{\sum_{t\in[T]}\sigma_t^2}\cdot\sqrt{\dim_{\frac{1}{\sqrt{T}}, T}(\calF)\cdot\log \calN\left(\calF,\frac{1}{L_fT^2} \right)}\right),
\end{align*}
where for the last inequality we pick $\lambda = \Theta(1)$, $\alpha = 1/\sqrt{T}$. %\todo{due to the not very smart union bound on $\alpha$, we have additional logarithmic dependence on $R$. actually we may be able to improve it to $\log\log R$ if we can show $\ctn_\alpha$ is monotonic in $\alpha$, when fixing data $\{Z_i\}_{i\in[t]}$, but I am unsure if this is true in general.}

\end{proof}


\section{Auxiliary Proofs}
\subsection{Concentration Inequality for Catoni Estimator}\label{apss:Concentration Inequality for Catoni Estimator}
\begin{lemma}[Concentration for $\ctn$ estimator, cf. Lemma 13 in~\cite{wei2020taking}]\label{lem:concentration-catoni}
Let $Z_t$ be random variable adapted to filtration $\calH_t$, suppose $\E[Z_i|\calH_{i-1}] = \mu_i$, $\sum_{i\in[t]} \E\left[\left(Z_i-\mu_i\right)^2|\calH_{i-1}\right] \le V$ for some fixed $V$. Let $\bmu\defeq \frac{1}{t}\sum_{i\in[t]}\mu_i$, for some fixed parameter $\theta>0$, we have for any $t\ge \theta^2\left(V+\sum_{i\in[t]}(\mu_i-\bmu)^2\right)+2\log(1/\delta)$, with probability at least $1-2\delta$,
\[
\left|\ctn_\theta(\{Z_i\}_{i\in[t]})-\bmu\right| \le \frac{\theta\left(V+\sum_{i\in[t]}\left(\mu_i-\bmu\right)^2\right)}{t}+\frac{2\log(1/\delta)}{\theta t}.
\]
\end{lemma}

We provide the following lemma used in proving~\Cref{coro:catoni}
\begin{lemma}[Sensitivity of $\ctn$ estimator, cf. Lemma A.13 of~\cite{wagenmaker2022first}] \label{lem:catoni-helper}
Consider some fixed $Z=\{Z_i\}_{i\in[t]}$, $\tilde Z=\{\tilde Z_i\}_{i\in[t]}$ satisfying $|Z_i|\le R$, $|\tilde Z_i|\le R$ for all $i\in[t]$, and some fixed $\theta>0$, $\tilde{\theta}>0$. Then, assuming that
$$
\Delta := \frac{1}{t}\sum_{i\in[t]}\theta|Z_i-\tilde{Z}_i| + 3R|\theta - \tilde{\theta}| \le \frac{1}{18}\min\{1,\theta^2R^2\},
$$
we will have
\[
\left|\ctn_\theta(\{Z_i\}_{i\in[t]})-\ctn_{\tilde\theta}(\{\tilde Z_i\}_{i\in[t]})\right|\le \frac{1+2\theta R}{\theta}\Delta + \sqrt{\frac{2\Delta}{\theta^2}}.
\]
\end{lemma}


\begin{lemma}[Formal version of Lemma \ref{coro:catoni}]\label{formal_coro:catoni}
Let $Z_t$ be a random variable adapted to filtration $\calH_t$ with a uniform bound $|Z_t|\le R$, $\E[Z_i|\calH_{i-1}] = \mu_i$, $\sum_{i\in[t]} \E\left[\left(Z_i-\mu_i\right)^2|\calH_{i-1}\right] \le V$ for some fixed $V$. Let $\bmu\defeq \frac{1}{t}\sum_{i\in[t]}\mu_i$. For any parameter $\theta\in[a, A]$ and given $\epsilon\le 24R(1+2AR)t^2$, if $t\ge \theta^2(V+\sum_{i\in[t]}(\mu_i-\bmu)^2)+2\log(\frac{48R(1+2AR)t^2}{\min(1,a)\epsilon^2\delta}\log(A/a))$,
with probability at least $1-2\delta$,
\begin{equation*}
{
\begin{aligned}
\left|\ctn_\theta(\{Z_i\}_{i\in[t]})-\bmu\right|
\le \frac{\theta\left(V+\sum_{i\in[t]}\left(\mu_i-\bmu\right)^2\right)}{t}
+\frac{4\iota_0^2}{\theta t}+\frac{\epsilon}{t},
\end{aligned}}
\end{equation*}
where
$$
\iota_0^2 = 4\log\left(\frac{48R(1+2AR)t^2}{\min(1,a)\epsilon^2\delta}\log(A/a)\right).
$$
\end{lemma}
\begin{proof}[Proof of Lemma \ref{coro:catoni}]
    For any $\epsilon>0$, set $\kappa = \frac{\epsilon^2\cdot \min(1,a)}{24R(1+2AR)t^2}\le 1$, we consider a set $\calA = \{\theta = (1+\kappa)^j\cdot a \mid (1+\kappa)^j\cdot a\in[a,A], j\ge 0\}$, it is immediate to see $|\calA| \le \frac{48R(1+2AR)t^2}{\min(1,a)\epsilon^2}\log(A/a)$. Now for any $\theta\in\calA$, we have by~\Cref{lem:concentration-catoni} that with probability $1-2\delta/|\calA|$, we have for any $t\ge \theta^2\left(V+\sum_{i\in[t]}(\mu_i-\bmu)^2\right)+2\log(|\calA|/\delta)$,
    \[
    \left|\ctn_\theta(\{Z_i\}_{i\in[t]})-\bmu\right| \le \frac{\theta\left(V+\sum_{i\in[t]}\left(\mu_i-\bmu\right)^2\right)}{t}+\frac{2\log(|\calA|/\delta)}{\theta t}.
    \]
    Thus by taking a union bound over $\theta\in\calA$, we have with probability $1-2\delta$, for all $\theta\in\calA$, it holds that for any $t\ge \theta^2\left(V+\sum_{i\in[t]}(\mu_i-\bmu)^2\right)+2\log(|\calA|/\delta)$, 
    \[
    \left|\ctn_\theta(\{Z_i\}_{i\in[t]})-\bmu\right| \le \frac{\theta\left(V+\sum_{i\in[t]}\left(\mu_i-\bmu\right)^2\right)}{t}+\frac{2\log(|\calA|/\delta)}{\theta t}.
    \]
    Thus, for any $\theta'\in[a,A]$, we have there exists some $\theta\in\calA$ such that the above bound holds true and $\frac{|\theta'-\theta|}{\theta}\le \kappa\le \min\left(\frac{\epsilon}{2\cdot 3R(1+2AR)t},\frac{a\cdot\epsilon^2}{24Rt^2}\right)$. Now by triangle inequality we can conclude that for any $\theta\in[a,A]$, 
    \begin{align*}
        |\ctn_\theta - \bar{\mu}| & \le  |\ctn_{\theta_0}-\bar{\mu}|+|\ctn_\theta - \ctn_{\theta_0}|\\
        & \le \frac{\theta_0\left(V+\sum_{i\in[t]}\left(\mu_i-\bmu\right)^2\right)}{t}+\frac{2\log(|\calA|/\delta)}{\theta_0 t}+\frac{\epsilon}{t}\\
        & \le \frac{\theta\left(V+\sum_{i\in[t]}\left(\mu_i-\bmu\right)^2\right)}{t}+\frac{4\log(|\calA|/\delta)}{\theta t}+\frac{\epsilon}{t},
    \end{align*}
    where for the second inequality we use~\Cref{lem:catoni-helper}.
\end{proof}

\end{document}