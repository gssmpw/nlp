\begin{figure*}[tp]
  \centering
  \includegraphics[width=\linewidth]{figs/overview.pdf}
  % \vspace{-8pt}
\caption{\textbf{Unmix-CLIP Overview.} Given image and label names in the dataset, CLIP extracts image and text features, which are further processed by respective projectors to embed into a disentangled space while preserving local image information. To reduce mutual feature information (MFI) between class features we propose MFI loss that enforces the self-similarity matrix of projected text features to approximate an identity matrix, effectively reducing inter-class feature dependencies (Sec.\ref{sec: Local Features - MFI Reduction}). We propagate the separation in the text features to image space by aligning the image and separated text features using a multi-label recognition setup (Sec.\ref{sec:Global Features - MLR}). Following \cite{dualcoop} and as detailed in Sec. \ref{sec:Global Features - MLR}, we aggregate the projected image and text features to obtain predicted logits. The predicted logits are trained with ground truth labels using the widely used asymmetric loss (ASL) \cite{asl}. Our training loss combines the ASL and MFI loss; the only trainable components are the projectors. We freeze both CLIP encoders and projectors during inference for MLR and downstream tasks such as ZS3.}
% wyw: align Input Image with "Input Image" by central align.
% wyw: The outer surronding blocks of negative/positive logits in the bottom right hand should be removed. N should be in math font.
%samyak: done!
\label{fig:Overview}
\end{figure*}

 % Our approach consists of three main components introduced across different sections: (1) Feature extraction and projection (Sec 3.1): CLIP's frozen encoders extract initial features from both image and text inputs, which are then refined through learnable projectors. The text input uses fixed dual prompts, while the image features preserve local spatial information. (2) Feature disentanglement (Sec 3.2): We introduce a Mutual Feature Information (MFI) loss that encourages the self-similarity matrix of projected text features to approximate an identity matrix, effectively reducing inter-class feature dependencies. (3) Multi-label recognition (Sec 3.3): The disentangled text features guide the image feature learning through a multi-label recognition setup, where projected features are aggregated to produce classification logits. These logits are trained using Asymmetric Loss (ASL) against ground truth labels. During inference, both CLIP encoders and projectors remain frozen, enabling efficient deployment for multi-label recognition and zero-shot tasks.
