% \subsection{Experimental Setup}

\subsection{Datasets and Metrics}

\textbf{Pre-training with MLR:} We evaluate the MLR performance using mean-Average Precision (mAP) on the following datasets:\\
\textbf{COCO-14} \cite{coco} contains 80 classes across diverse categories with 82,081 training and 40,504 validation images. Following recent works \cite{dualcoop,MLR-GCN,PositiveCoOp}, we train on the training set and evaluate on the validation set. \\
% 
\textbf{VOC2007} \cite{pascal-voc} is another widely used MLR dataset containing 20 classes with 9,963 images. Following \cite{dualcoop,MLR-GCN,PositiveCoOp}, we use the train-val set for training and the test set for evaluation. 



\textbf{Zero-Shot Semantic Segmentation (ZS3):} We use image and text projectors trained on the COCO-14 dataset and evaluate ZS3 using the mIoU metric on the following datasets: \\
\textbf{PASCAL VOC 2012} \cite{pascal-voc} includes segmentation masks for the 20 classes in VOC2007.  Following works \cite{clip_surgery,gem_walid}, we evaluate on the validation set.\\
% 
\textbf{PASCAL Context} \cite{context} extends PascalVOC to 59 classes, 30 of which were unseen during our pre-training. These additional classes provide dense annotations for the whole scene. We evaluate on the test set, comprising 5,104 images.\\
% 
\textbf{COCO-2017} \cite{coco} includes segmentation masks for the 80 classes in COCO-14.  Following recent works \cite{clip_surgery,gem_walid}, we evaluate on the validation set.
% 
% \textbf{COCO-stuff} \cite{coco-stuff} extends COCO-17 to 172 classes (80 Things, 91 Stuff and 1 Unlabeled) providing denser segmentation maps for COCO-2017. 
% 
% 
\subsection{Implementation Details}
We use CLIP's \cite{clip} original pre-trained encoder weights for all our experiments and keep them frozen. Consistent with MLR literature, we use a ResNets-based visual encoder (RN-101) and the standard transformer for text encoding. We conduct all experiments on a single RTX A4000 GPU.\\
During the pre-training stage with the MLR setup (Sec.\ref{sec:Global Features - MLR}), we follow the settings and hyperparameters from recent works \cite{dualcoop,MLR-GCN,PositiveCoOp}. This includes resizing images to $448$, applying Cutout \cite{cutout} and RandAugment \cite{randaug} for augmentation. 
% 
Our projectors ($h_{\phi}$) are implemented as multi-layer perceptrons (MLPs). Specifically, the image projector follows a [512 $\rightarrow$ 256] architecture, while the text projector is designed as [512 $\rightarrow$ 384 $\rightarrow$ 256] with batch normalization and ReLU. We train both projectors with stochastic gradient descent (SGD) using an initial learning rate of 0.002, which is reduced by cosine annealing. We train the Unmix-CLIP setup (ASL + MFI loss) for 50 epochs with a batch size of 32. We follow \cite{dualcoop,MLR-GCN,PositiveCoOp}, and use ASL hyperparameters in Eq. \ref{eq:ASL} as $\gamma_- = 2$, $\gamma_+ = 1$ and $\delta$ = 0.05. We set $\lambda$ = 0.2 and $\alpha$ = $7\mathrm{e}{-5}$ when pre-trained with COCO-14 in equation \ref{eq:mfi}. 

For Zero-Shot Semantic Segmentation, we adopt the v-v attention described in \cite{clip_surgery} that prevents inversion of activation commonly observed in CLIP. We then add our pre-trained projectors to CLIP.  To obtain the semantic segmentation map, we compute the cosine similarity between locally projected image features ($\mathbf{z'}$) and projected text features for all classes in the dataset. We use the text template "A photo of a \{classname\}." Lastly, we use bilinear interpolation to upsample the segmentation mask to the input image size.

\input{tables/zs3}
\subsection{Results}
\label{sec: Results}
\textbf{Multi-Label Recognition.}
We primarily compare Unmix-CLIP with other SOTA VLM-based MLR approaches. In Table.\ref{tab:MLR performance}, we present a detailed comparison of the performance (mAP) and the number of training parameters required by each method on the VOC2007 \cite{pascal-voc} and COCO-14 \cite{coco} datasets. Consistent with previous works \cite{dualcoop,dualcoop++,MLR-GCN,PositiveCoOp,scpnet}, we use the ResNet-101 architecture. For VOC 2007, we observe that our performance is competitive with DualCoOp++\cite{dualcoop++} and requires the same number of parameters. However, on the more challenging COCO-14 dataset, Unmix-CLIP outperforms DualCoOp++ while requiring only one-third of the parameters. 
% 

\textbf{Zero-Shot Semantic Segmentation.}
We categorize our comparisons into two main groups. The first group includes approaches that use local annotations (segmentation masks, etc.) to fine-tune the network \cite{SPNet, ZS3Net, CLIP-ES}. The second comparison is with training-free approaches \cite{clip, clip_surgery}. Our approach is closer to the training-free methods, as it does not use any form of local annotations. It is important to note that our comparisons are limited to approaches that use ResNet-based architectures due to the following reasons: (1) The MFI phenomenon is observed explicitly in ResNet-based CLIP models, whereas ViT-based models have better localization properties as they benefit from the use of patches during CLIP's pre-training. (2) ViT-based methods \cite{gem_walid,clip_surgery} have been explicitly adapted for ViT architectures, they do show analysis with ResNet-based models. Additionally, other methods incorporate external information by leveraging large vision models \cite{proxyclip,samclip,CLIP-dino}. Our results are summarized in Table \ref{tab:ZS3_performance}. Following prior works \cite{SPNet, ZS3Net, CLIP-ES, clip_surgery}, we report mIoU values for VOC 2012 by including the background as a class. We use a threshold of 0.85 to identify the background, as suggested in \cite{gem_walid}. Our approach outperforms CLIP Surgery by 18.5 mIoU and CLIP-VV by 3.4 mIoU on VOC 2012. For COCO-14, we report results both with and without the background class. When including the background, our method surpasses CLIP Surgery and CLIP-VV by 9.7 mIoU and 2.8 mIoU, respectively. Without the background, we achieve gains of 14.9 mIoU and 2.3 mIoU. Additionally, we evaluate the VOC Context dataset, which contains 30 unseen classes not used during our pre-training. Although our model is not explicitly trained to reduce MFI between these classes (it is designed to minimize MFI among COCOâ€™s 80 classes), our approach still outperforms CLIP Surgery. These results demonstrate that our projectors preserve the open-vocabulary capabilities of CLIP. We further show qualitative results for open-vocabulary tasks in the Supplementary (Fig. \ref{fig:openvocab}).

\input{tables/zs_mlr}

\textbf{Segmentation-driven Zero-Shot Multi-Label Recognition (ZS-MLR).} We leverage the segmentation capabilities of Unmix-CLIP to reformulate the multi-label recognition problem into a single-label recognition problem, a domain more suitable for CLIP. Specifically, we use two predictions: global and local. We pass the input image directly through CLIP to obtain its global predictions. However, as discussed in Sec. \ref{sec: Introduction}, these predictions are often dominated by more prominent objects in the image, ignoring smaller objects, which leads to poor zero-shot MLR performance. To address this limitation, we introduce local predictions. We segment the image into multiple regions (ideally corresponding to individual objects) using Unmix-CLIP. Each segment is then processed independently through CLIP, and the predictions from all segments are aggregated. Finally, we combine the global and local predictions to obtain the zero-shot scores for the image.
% 
We evaluate the ZS-MLR performance on the VOC2007 and COCO-14 datasets, with results presented in Table \ref{tab:zero_shot_mlr}. The results demonstrate that our method provides meaningful information (segments) to improve CLIP zero-shot capabilities.


