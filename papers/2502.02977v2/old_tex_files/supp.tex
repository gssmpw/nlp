% \newpage
% \newpage

\[
\mathcal{L}_i = \underbrace{I(T_i; Z_i)}_{\text{Compression}} - \beta_1 \underbrace{I(Z_i; Y_i)}_{\text{Relevance}} + \beta_2 \sum_{i \neq j} \underbrace{I(Z_i; Y_j)}_{\text{Disentanglement}}.
\]

$T_i$: Text (prompt with label name)

$Z_i$: Text features corresponding to text $T_i$

$Y_i$: Mapping / Output corresponding to each text $T_i$. 


Compression: Minimize the mutual information between $T_i$ and $Z_i$, we want to express the text $T_i$ in the most compressed form $Z_i$, which provides the best results for our task. 

Relevance: Maximize mutual information between $Z_i$ and $Y_i$, that is the information in $T_i$ to predict $Y_i$. 

Disentanglement: Making sure that $T_i$ has less mutual information about other labels


% \[
% \text{TP} \rightarrow Z_0 \cdots Y_P \\
% \phantom{.} \hspace{0.5cm} Y_N
% \]


%%%%%%%%%%%%%



\subsection*{Objective Function: MFI Loss}
\label{Supp:info-bottleneck}
We define the Information Bottleneck (IB) objective as:

\begin{equation} \mathcal{IB} = \mathbb{I}(Z_i, T_i) - \beta \left[\mathbb{I}(Z_i, Y_i) - \sum_{j \neq i} \mathbb{I}(Z_i, Y_j) \right]
\label{eq:it_bottleneck} \end{equation}



Next, we express the mutual information in terms of entropy using:
\begin{equation} \mathbb{I}(A; B) = \mathbb{H}(A) - \mathbb{H}(A | B),
\label{eq:mutual_info} \end{equation}
where $\mathbb{H}(A)$ is the marginal entropy of $A$, and $\mathbb{H}(A|B)$ is the conditional entropy of $A$ given $B$.

Substituting this relation into the IB objective in Equation \ref{eq:it_bottleneck}, the objective can be expressed as a combination of entropy terms:
\begin{align}
\mathcal{IB} = &\; (1 - \beta + \sum_{j \neq i} \beta) \mathbb{H}(Z_i) + \beta \left[ \mathbb{H}(Z_i | Y_i) - \sum_{j \neq i}\mathbb{H}(Z_i | Y_j) \right] \nonumber \\
&\; - \beta \, \mathbb{H}(Z_i | T_i)
\label{eq:entropy_form}
\end{align}

Now, in our case, we know the VLM, meaning that given an input ($T_i$), the  output ($Z_i$) is deterministic and hence the entropy term 
$\mathbb{H}(Z_i | T_i)$ = 0. 

Additionally, we know the whole set of inputs (text labels for a dataset) $T_i$, the output of VLM $Z_i$ is also deterministic and hence its entropy $\mathbb{H}(Z_i)$ = 0.

Removing the two deterministic terms from eq. \ref{eq:entropy_form}, the objective simplifies to:
\begin{equation} \mathcal{IB} \propto \mathbb{H}(Z_i | Y_i) - \sum_{j \neq i} \mathbb{H}(Z_i | Y_j).
\label{eq:final_simplified} \end{equation}

In order to further simplify the problem we assume $Z$ follows a Gaussian distribution. The entropy of a Gaussian distribution is given by:
\begin{equation} \mathbb{H}(Z) = \frac{1}{2} \log \left|\mathbf{C}\right| + \text{const},
\label{eq:gaussian_entropy} \end{equation}
where $\mathbf{C}$ is the covariance matrix of $Z_i$. Since the constant term does not affect the optimization, it can be ignored. Thus, the objective reduces to optimizing the determinant of the covariance matrix $\mathbf{C}$. In practice, we observe that optimizing the covariance matrix instead of its determinant leads to more stable training and state-of-the-art results.

The IT Bottleneck objective in Equation \ref{eq:final_simplified} becomes:
\begin{equation} \mathcal{IB} = \mathbf{C}_{Z_i | Y_i} - \sum_{j \neq i} \mathbf{C}_{Z_i | Y_j},
\label{eq:gaussian_formulation} \end{equation}

The optimization of these two components in eq. \ref{eq:gaussian_formulation} is equivalent to optimizing $\mathcal{L}{\text{on-diag}}$ and $\mathcal{L}{\text{off-diag}}$ in eq. \ref{eq:total_loss}



% \begin{figure*}
%   \centering
%   \includegraphics[width=\linewidth]{figs/openvocab.png}
%   % \vspace{-8pt}
%   \caption{\textbf{Qualitative comparison - Open Vocabulary.} EDIT THIS,THE CURRENT CAPTIN IS FROM ANOTHER PAPER Here we show the segmentation
% results on various unseen classes, including
% fine-grained classes such as cars in different colors/imagery properties, celebrities,
% and animation characters. .}
%   \label{fig:openvocab}
% \end{figure*}

\input{figs/open_vocab}

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{figs/miou_map.png}
  % \vspace{-8pt}
  \caption{ \textbf{mAP vs mIoU} .}
  \label{fig: mAP_mIoU}
  %wyw: should use the following commands in Python to get a better font.
  %wyw: matplotlib.rc('text', usetex = True)
% fig = plt.figure(figsize = (13, 20))
% ax = plt.axes()
% plt.rcParams.update({'lines.linewidth': 2})
% plt.rcParams.update({'lines.markersize': 8})
% plt.rcParams.update({'lines.markeredgewidth': 1})
% plt.rcParams["font.family"] = "Times New Roman"
\end{figure}