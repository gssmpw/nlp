\begin{abstract}
Vision-language models (VLMs) demonstrate impressive capabilities in coarse-grained tasks like image classification and retrieval. However, these models face significant challenges in fine-grained tasks that require localized understanding. To better understand these challenges, we comprehensively analyzed CLIP features, revealing an important limitation: the semantic features are highly correlated. Specifically, the features of a class encode information about other classesâ€”a phenomenon we refer to as mutual feature information (MFI). This mutual information becomes evident when we query a specific class and unrelated objects are activated along with the target class. 
% 
To address this limitation, we propose Unmix-CLIP, a novel framework that reduces MFI between class features through our proposed MFI loss. This loss explicitly separates text features by projecting them such that inter-class similarity is zero. To achieve a corresponding separation in image features, we use a multi-label recognition (MLR) setup to align the image features with the disentangled text features. This ensures that both text and image features are disentangled and aligned across domains, allowing for improved feature separation in downstream tasks. We demonstrate the effectiveness of Unmix-CLIP through extensive evaluations on multi-label recognition and zero-shot semantic segmentation (ZS3). In MLR, our method demonstrates competitive performance on the VOC2007 dataset and surpasses state-of-the-art approaches on the COCO-14 dataset while using fewer training parameters. Additionally, Unmix-CLIP consistently outperforms existing ZS3 methods on COCO and VOC.

\end{abstract}