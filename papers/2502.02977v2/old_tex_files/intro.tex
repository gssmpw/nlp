\label{sec: Introduction}
\input{figs/teaser}

% CLIP \cite{clip} is a widely used vision-language model (VLM) trained on 400 million image-text pairs (WIT-400M) to learn transferable visual representations. These representations perform exceptionally well in coarse-grained tasks such as single-label image recognition  \cite{coop}, visual question answering \cite{vqa}, and action recognition \cite{action_recog}. However, CLIP's training objective depends on global image-text alignment and struggles to maintain performance in tasks that require localized understanding, including multi-label recognition (MLR) \cite{mlmc,huang2024radiology} and semantic segmentation \cite{seg}. This limitation motivates a deeper analysis of CLIP's feature representations, where we discover significant interference between class features that hinders fine-grained understanding.

Vision-language models (VLMs) have emerged as powerful tools for understanding visual content through natural language supervision. CLIP \cite{clip} is trained on 400 million image-text pairs (WIT-400M), enabling it to achieve remarkable performance in coarse-grained visual understanding tasks such as image classification \cite{coop}, image retrieval \cite{retrival}, and visual question answering \cite{vqa}. However, these models struggle with fine-grained tasks that require localized understanding, leading to significant performance degradation in multi-label recognition (MLR) \cite{mlmc,huang2024radiology} and semantic segmentation \cite{seg}. While previous work has attributed this to architectural choices \cite{ViT_need_register} or training objectives \cite{tagclip,dong2023maskclip}, our analysis reveals a more fundamental issue: the entanglement of semantic features in CLIP's representation space.

% wyw: the current logics is: CLIP is widely used -> CLIP has limitations -> limitations's reason is not known. In this logic chain, the first part and the latter ones have no connections, could we have the following logics? Clip is not limitations -> this causes a lot of problems in CLIP's applications -> improving this limitations is difficult
% 
% To examine CLIP's representations we focus on the ResNet \cite{resnet} architecture. 
% wyw: Is the Resnet architecture the current most popular ones? This selection needs to be justified. Or, we may not mention this selection to avoid the attacks from reviewers.
% For our analysis, we leverage two aspects of ResNet-based CLIP models: First, we remove the final spatial pooling operation, which aggregates spatial features and harms localized information. Second, we leverage text in the joint vision-language space to identify the image regions CLIP attends to, allowing us to examine the areas of focus and potential misalignments in the model.
% 
% wyw: Not start with As shown in... The sotry should look like this: we do .... analysis. We find that .... We visualize an example in .... This demonstrates that ....

We systematically analyze CLIP's features and identify two key factors contributing to this limitation. First, the spatial pooling operation in the final layer, although effective for global tasks, discards essential localized information necessary for fine-grained understanding. Second, and more importantly, we discover significant interference between class features in the joint vision-language space, a phenomenon we term mutual feature information (MFI). The mutual information becomes apparent during class-specific queries: regions corresponding to unrelated objects are consistently activated alongside the target class. For instance, as illustrated in Fig. \ref{fig:teaser}, regions containing 'dog' and 'horse' also activate when we query' person.' This activation pattern correlates strongly with high similarity scores between class text features (0.84 for person-dog and 0.80 for person-horse), indicating substantial feature entanglement in CLIP's representation space.


% We analyze CLIP features by focusing on two key aspects of models. First, we remove the final spatial pooling operation, which aggregates spatial features and harms localized information. Second, we use text in the joint vision-language space to identify the image regions CLIP attends to by computing the cosine similarity between text and local image features. Our analysis shows that when querying a specific class in the image, multiple regions corresponding to other unqueried classes are also activated. For example, in Fig. \ref{fig:teaser}, when we query the class 'person,' it also activates regions corresponding to the classes 'dog' and 'horse.' We quantify this effect by calculating the similarity between CLIP’s class text features, referred to as class similarity in Fig. \ref{fig:teaser}. The high similarity scores (0.84 for person-dog and 0.80 for person-horse) indicate strong correlations between these features. A broader visualization (Fig. \ref{fig:seperation}) further confirms that this effect is consistent across many classes. This analysis demonstrates the interference between class features, meaning that the features of a class have information that is mutually shared by others, we refer to this as mutual feature information (MFI). The presence of MFI in class features helps explain why unqueried regions in the image are activated, making it less effective for fine-grained tasks.

To address this fundamental limitation, we introduce Unmix-CLIP, a novel framework that disentangles class features in vision-language models. We draw inspiration from the Barlow redundancy reduction principle \cite{barlow} and extend the concept to the vision-language domain. Unlike previous attempts that focus on architectural modifications \cite{clip_surgery,gem_walid} or prompt engineering \cite{dualcoop,PositiveCoOp} to adapt VLMs for fine-grained tasks, Unmix-CLIP directly targets the root cause by minimizing mutual feature information (MFI) between class representations while preserving task-relevant information. We achieve this with a carefully designed MFI loss. This loss explicitly disentangles text features by projecting them to achieve zero inter-class similarity. To achieve a similar separation in image features, we align them with the disentangled text features using a multi-label recognition framework. The MFI loss separates text features, while the MLR loss aligns image and text features. As a result, we obtain disentangled features that align across the image and text domains, leading to improved separation in downstream tasks.

% Based on our analysis of CLIP features, we propose Unmix-CLIP, a framework specifically designed to reduce the mutual information between class features. Inspired by the Barlow redundancy reduction principle \cite{barlow}, recently highlighted in the context of neural networks by \cite{barlowtwins}, Unmix-CLIP aims to minimize redundancy while preserving essential information. In our work, this redundancy refers to the mutual feature information (MFI) shared among the class features.
% Unmix CLIP is inspired from Barlow redundancy reduction principle \cite{barlow}, which was recently highlighted in \cite{barlowtwins} in context of neural networks. The principle states that "sensory relays recode sensory messages so that their redundancy is reduced but comparatively little information is lost". In our work, this redundancy is the mutual feature information (MFI) between class features. 
%  

% Unmix-CLIP uses frozen CLIP to extract image and text features. These features are then projected into a new space to reduce mutual information between class features. To achieve this, we introduce an MFI loss that explicitly reduces the inter-class similarity of text features while preserving high intra-class self-similarity of features to prevent feature collapse. This explicit separation in text features implicitly propagates to image features during the training (image-text alignment) for the multi-label recognition (MLR) task.  For MLR, we obtain evidence logits for each class by computing the similarity between projected local image features and class-specific text features. These local logits are then aggregated to produce the final image-level predictions. We jointly train Unmix-CLIP with the asymmetric loss \cite{asl} (for the MLR task) and our MFI loss.

We train Unmix-CLIP on 80 classes of the COCO-14 \cite{coco} dataset. We evaluate our approach across two tasks: (1) multi-label recognition (MLR), and (2) zero-shot semantic segmentation (ZS3). For MLR, we use the COCO-14  and VOC2007 \cite{pascal-voc} datasets. For ZS3, we use VOC2012 \cite{pascal-voc} and COCO-17 \cite{coco} for seen classes, while VOC Context \cite{context} provides 59 classes, 30 of which are unseen during pre-training. Our approach matches SOTA performance on VOC and outperforms SOTA approaches on the challenging COCO-14 dataset, using only one-third of their training parameters. For ZS3, Unmix-CLIP surpasses SOTA VLMs across all three datasets, demonstrating that reducing mutual feature information (MFI) benefits both tasks. To expand our evaluation, we assess Unmix-CLIP's segmentation capabilities by applying them to MLR objects. We reframe this task as single-label recognition. We combine the segment-level and whole-image results to obtain zero-shot MLR predictions. 
We note that segmenting objects provides additional information on top of global image features.
%
%\textbf{Our Contributions:} 
The main contributions of this work are:
\begin{itemize}
    \item We identify a critical challenge in adapting VLMs for fine-grained tasks: mutual information between class features (MFI) degrades fine-grained task performance 
    % \vspace{-3pt}
    \item To address this challenge, we propose Unmix-CLIP, a framework that adapts CLIP features for fine-grained tasks by reducing MFI. At its core lies our proposed MFI loss, which explicitly disentangles text features and guides the disentanglement of image features
    % \vspace{-3pt}
    \item We show that Unmix-CLIP achieves SOTA performance in zero-shot semantic segmentation and outperforms or matches SOTA results in multi-label recognition using significantly fewer training parameters. Furthermore, Unmix-CLIP as an object segmenter enhances CLIP’s zero-shot MLR performance
\end{itemize}