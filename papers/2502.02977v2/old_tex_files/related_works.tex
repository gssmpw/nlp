\input{figs/seperation}

%\subsection{Recoding Information}
\noindent {\bf Recoding information.}
Shannon proposed that optimal information transmission involves designing codes with minimum entropy \cite{shannon1948mathematical}. Barlow extended this idea to neuroscience, suggesting that sensory systems recode information to reduce redundancy with minimal loss \cite{barlow}. This principle has since been applied to many recent works, including image compression \cite{balle2016end} and self-supervised learning \cite{barlowtwins}. Our mutual-information reduction is inspired by the formulation of Barlow Twins \cite{barlowtwins}. We leverage this concept to reduce mutual information among class features in VLMs.

\vspace{1mm}
%\subsection{Vision-Language Models for Fine-grained Tasks}
\noindent {\bf Vision-Language Models for Fine-grained Tasks.}
Vision-language models (VLMs) trained with contrastive losses \cite{clip, align} are challenging to adapt for fine-grained tasks due to two reasons: (1) their reliance on global feature aggregation, which ignores local information. (2) Using the softmax operation in their training loss biases them toward single-object settings.

{\em Recognition.} 
Early efforts to adapt VLMs for recognition centered on learning prompts as classifiers for visual features \cite{coop}. These methods were extended to multi-label settings by learning multiple prompts for each class \cite{dualcoop,dualcoop++,PositiveCoOp}. Building on this, subsequent works incorporated co-occurrence information to make predictions interdependent \cite{scpnet,MLR-GCN}. In contrast, our approach does not rely on prompt learning or co-occurrence modeling during pre-training. Furthermore, our features are adaptable to tasks beyond recognition.

{\em Localization.} Early approaches addressed localization by training image segmentation models and using VLMs to label the segmented regions \cite{sam}. Later methods introduced pre-training setups that combined vision-language alignment with mask distillation to enhance localization \cite{dong2023maskclip}. Recent works adapted features for localization without additional training by leveraging the spatial properties preserved in the value projection of CLIPâ€™s transformer-style aggregation \cite{maskclip}. CLIP Surgery \cite{clip_surgery} identified consistent noisy activations across classes and reduced them by subtracting average features from class-specific features \cite{clip_surgery}, though the cause of these activations remains unclear. GEM generalized this concept to vision transformers \cite{gem_walid}. We use the finding that value projection preserves spatial information. We further improve value projection by disentangling class features during pre-training.

\input{figs/overview}