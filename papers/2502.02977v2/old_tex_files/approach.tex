% In this section, we introduce Unmix-CLIP, a framework to adapt representations of CLIP for fine-grained tasks such as multi-label recognition (MLR) and zero-shot semantic segmentation (ZS3). As detailed in Sec. \ref{sec: Introduction} and illustrated in Fig. \ref{fig:teaser}, mutual feature information (MFI) shared between class (object) features negatively impacts the performance of fine-grained tasks. To mitigate the issue of MFI, we propose the MFI loss ($\mathcal{L}_{\text{MFI}}$), which disentangles the features of each class, effectively reducing the mutual feature information between classes. We incorporate the MFI loss into our MLR setup, resulting in a joint framework called Unmix-CLIP. Specifically, our framework is trained for recognition tasks using the widely adopted Asymmetric Loss (ASL) $\mathcal{L}_{\text{ASL}}$ \cite{asl} while simultaneously reducing the MFI using MFI loss. In our experiments, we rely solely on ground truth labels for MLR and \textbf{do not} use any form of local annotation, such as segmentation masks. The following section is organized as follows: (1)  Feature Extraction Setup, (2) MFI Reduction, and (3) Image-Text Alignment with MLR. Refer to Fig. \ref{fig:Overview} for an overview of our approach.

\textbf{Formulation:}
% Given a multi-label dataset \(\mathcal{D}\) containing images \(\{\mathbf{x}_{i}\}_{i=1}^{|\mathcal{D}|}\) and \(N\) class labels \(\{C_j\}_{j=1}^N\), each image \(\mathbf{x}_i\) can contain objects belonging to one or more of these $N$ classes. Additionally, we use CLIP (\( f_\theta \)) as our VLM, parameterized by weights \(\theta\), consisting of an image encoder (\(f_{\theta,\text{img}}\)) and a text encoder (\(f_{\theta,\text{text}}\)), as our feature extractors. CLIP (\( f_\theta \)) is kept frozen throughout all experiments.

Given a multi-label dataset \(\mathcal{D}\), where \(\mathcal{D} = \{\mathbf{x}_i\}_{i=1}^{|\mathcal{D}|}\) consists of images \(\mathbf{x}_i\) and \(N\) class labels \(\{C_j\}_{j=1}^N\), each image \(\mathbf{x}_i\) can contain objects belonging to one or more of these \(N\) classes. Additionally, we use CLIP (\( f_\theta \)), parameterized by weights \(\theta\), consisting of an image encoder (\(f_{\theta,\text{img}}\)) and a text encoder (\(f_{\theta,\text{text}}\)) for feature extraction. Throughout all experiments, we keep the parameters of CLIP (\( f_\theta \)) frozen, including both the image and text encoders.

\textbf{Motivation:}

We observe that mutual information among class features in CLIP is detrimental to fine-grained task performance. To investigate this phenomenon, we analyze CLIP's features by focusing on two key aspects: (1) spatial preservation in the visual feature maps and (2) the relationship between class features in the joint vision-language space.

We first remove CLIP's final spatial pooling layer to examine local features. When querying an image (local features) for a specific class (text class features), we find that multiple unrelated regions are consistently activated. Figure\ref{fig:teaser} shows that querying for 'person' not only highlights the person regions but also activates areas containing dogs and horses. This suggests that CLIP's features for different classes share substantial information.

To quantify this feature entanglement, we analyze the similarity between class text features across multiple datasets (VOC \cite{pascal-voc}, COCO \cite{coco}, and Context \cite{context}). Since CLIP learns a joint embedding space, text feature similarities directly reflect the model's ability to distinguish between classes. As illustrated in Fig.\ref{fig:seperation}, we consistently observe high similarity values between different classes. Specifically, the similarity reaches 0.84 for person-dog pairs and 0.80 for person-horse pairs, far exceeding what one would expect from their semantic relationships. Extending this analysis across various datasets (Table \ref{tab:mfi_reduction}), We observe high average feature similarities of 0.77 in VOC, 0.69 in COCO, and 0.75 in Context, indicating that this is a universal limitation of CLIP's features space. This feature entanglement fundamentally affects CLIP's ability to perform fine-grained tasks. When features intended to represent one class encode significant information about other classes, the model struggles to make precise discrimination necessary for tasks like multi-label recognition and semantic segmentation.

To address this limitation, we propose a framework that reduces mutual information between class features while preserving task-essential semantics. Our approach consists of three components: (1) Feature extraction and Projection, where we extract CLIP features and project them into a disentangled space (Sec.\ref{sec:Feature Extraction and Projection}),  (2) MFI Loss, a novel approach for disentangling text features (Sec.\ref{sec: Local Features - MFI Reduction}), and (3) An MLR setup that aligns and propagates text disentanglement to image features (Sec.\ref{sec:Global Features - MLR}).



\subsection{Feature Extraction and Projection}
\label{sec:Feature Extraction and Projection}
We use CLIP as our feature extractor. Its image encoder (\(f_{\theta,\text{img}}\)) performs spatial pooling in the final layer, aggregating features from local regions into a \(d\)-dimensional vector for the input image \(x_i\). However, this pooling step removes spatial details, making it unsuitable for fine-grained tasks where localization is essential. To preserve class-specific information across local regions, we remove the final pooling layer. The encoder now outputs ($\mathbf{x}_{i}$) is ${f}_{\theta, img}(\mathbf{x_{i}}) = \mathbf{z}_i \in \mathbb{R}^{H \times W \times d}$ , where \(H\) and \(W\) are the spatial dimensions. The text encoder remains unchanged. We use a fixed pair of positive and negative (\( \mathbf{txt_{i,+}} \), \( \mathbf{txt_{j,-}} \)) prompts for each class \( j \) as input to the text encoder. The positive prompt indicates the presence of the class in a local region, while the negative prompt indicates its absence. Passing these prompts through the text encoder produces ${f}_{\theta, text}(\mathbf{txt_{i}}) = \mathbf{t}_i \in \mathbb{R}^{d}$.
% 


The extracted features (image ($\mathbf{z}_i$), text ($\mathbf{t}_i $)) lie in CLIP's original feature space and are not suitable for fine-grained tasks as discussed in Sec.\ref{sec: Introduction}.  To address this, we introduce learnable projectors ($ h_\phi :  h_{\phi,\text{img}} $ and $ h_{\phi,\text{text}} $), parameterized by weights $ \phi $. These projectors map the image ($ \mathbf{z}_i $) and text ($ \mathbf{t}_i $) features from their original space ($ d $-dim) to a new disentangled space ($ d' $-dim), making them suitable for fine-grained tasks. The image projector transforms $ \mathbf{z}_i \to \mathbf{z'}_i $ ($ \mathbb{R}^{H \times W \times d} \to \mathbb{R}^{H \times W \times d'} $) while preserving the spatial dimensions ($H ,W$). The text projector maps $ \mathbf{t}_i \to \mathbf{t'}_i $ ($ \mathbb{R}^{d} \to \mathbb{R}^{d'} $).

\subsection{MFI Loss}
\label{sec: Local Features - MFI Reduction}
We design the projected feature space to reduce mutual feature information (MFI) between class features. Reducing MFI requires obtaining individual class features, as MFI represents the shared information between these individual features. Separating image features into individual class features is non-trivial because multiple classes often co-occur in an image, which leads to mixed features that make class-wise feature isolation difficult. Object segmentation models could assist by extracting features from segmented regions, but these models add significant complexity. In contrast, text class features are inherently independent because they are derived from separate class names or prompts inputted to the text encoder. This independence directly gives us individual text class features. We leverage this property of text features and apply MFI reduction to them first.

We propose the MFI reduction loss to minimize the mutual information between class text features. This loss is applied to the projected text features ($\mathbf{t'}$) as follows:
%
\begin{equation}
\label{eq:mfi}
\mathcal{L}_{\text{MFI}} = \underbrace{\sum_{i=1} \left( \mathbf{S}_{ii} - 1 \right)^2}_{\text{Collapse Prevention}} + \lambda \underbrace{\sum_{i=1} \sum_{\substack{j=1 \\ j \neq i}} \mathbf{S}_{ij}^2}_{\text{MFI Reduction}}
\end{equation}
were $\mathbf{S}$ is the self-similarity matrix obtained from $\mathbf{t'}$.
\[
\mathbf{S}_{ij} = \frac{\mathbf{t'}_i \mathbf{t'}_j^\top}{\|\mathbf{t'}_i\| \|\mathbf{t'}_j\|}, \quad \forall i, j 
\]
% 
\( \mathbf{t'}_i, \mathbf{t'}_j \) are the \( i \)-th and \( j \)-th column vectors of \( \mathbf{t'} \) (i.e., \( \mathbf{t'}_i, \mathbf{t'}_j \in \mathbb{R}^{d'} \)) and  \( \|\mathbf{t'}_i\| \) is the \( L_2 \)-norm of \( \mathbf{t}_i \). Here, \( \lambda \) is the hyperparameter that addresses the imbalance in the loss arising from the larger number of MFI reduction terms in \( \mathbf{S} \) comparison to the collapse prevention terms.

The MFI loss minimizes the inter-class similarity \( \mathbf{S}_{ij} \) (\( i \neq j \)) while simultaneously preserving high intra-class \( \mathbf{S}_{ii} \) to prevent feature collapse. Our MFI loss builds upon ideas from Barlow Twins \cite{barlowtwins}. However, unlike Barlow Twins, which reduces cross-correlation between pairs of embeddings to learn representations, the MFI loss focus on reducing mutual information within a single set of text embeddings through self-similarity. We provide detailed proof of the loss function's connection to the Information Bottleneck principle is supplementary material Sec.\ref{Supp:info-bottleneck}.

With these disentangled text features, we explain the separation of image features in the next section.

\input{tables/MLR}
\subsection{Image-Text Alignment with MLR}
\label{sec:Global Features - MLR}

\textbf{MLR Formulation}: MLR task involves identifying the subset of classes \(\mathcal{C}_i \subseteq \{C_1, C_2, \ldots, C_N\}\) associated with the image \(\mathbf{x}_i\). The goal is to learn a mapping function \(g: \mathbf{x}_i \rightarrow \{-1, 1\}^N\), that maps input images to \(1\) if the class is present and \(-1\) if the class is absent in the image. 


We train our model to recognize multiple objects in images by learning to align projected image features with disentangled text features. For each location $(h,w)$ in the projected image features $(\mathbf{z'}_i)$, we detect the presence or absence of a class $j$, by computing the cosine similarity with positive text features ($\mathbf{t'_{j,+}}$) and negative text features ($\mathbf{t'_{j,-}}$). A higher similarity with the positive text features indicates the presence of the class, while a higher similarity with the negative text features indicates its absence. We aggregate these similarity scores from local regions to produce logits $\mathbf{p_{i}}$ for the image, following \cite{dualcoop, MLR-GCN, PositiveCoOp}. We train the setup with the widely used Asymmetric Loss function (ASL) \cite{asl}, which addresses the significant imbalance between negative and positive examples in a multi-label recognition dataset. The ASL loss is given by:
% 
\begin{align}
    \mathcal{L}_{ASL}(p_{i}^{j}) = 
    \begin{cases} 
        \left(1 - p_{i}^{j}\right)^{\gamma_{+}} \log \left(p_{i}^{j}\right), & \text{if } y_{i}^{j} = 1, \\
        \left(p_{i, \delta}^{j}\right)^{\gamma_{-}} \log \left(1 - p_{i, \delta}^{j}\right), & \text{else }
    \end{cases} 
\label{eq:ASL}
\end{align}
% 
where $p_{i}^{j}$ represents the corresponding prediction associated with label $y_{i}^{j}$, $i$ represents the image and $j$ represents the class.  $p_{i, \delta}^{j} = \max(\hat{y} - \delta, 0)$, with $\delta$ representing the shifting parameter defined in ASL.
% 

\textbf{Training:}
Our training objective is composed of two components: (1) mutual feature information loss that enforces the separation between class text features and (2)  Asymmetric loss function \cite{asl} designed for MLR, that aligns the image features with disentangled text features to obtain predictions for an image.
% 
\begin{equation}
 \mathcal{L}_{\text{Unmix-CLIP}} =  \mathcal{L}_{\text{ASL}} + \alpha \mathcal{L}_{\text{MFI}}
\end{equation}
where 
\( \alpha \) controls the relative importance of two objectives.