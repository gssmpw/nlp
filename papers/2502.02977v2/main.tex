%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{pifont}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{helvet} 
\usepackage{mathptmx}



% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage{amsfonts}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{adjustbox}
\usepackage{colortbl}
\usepackage{chngcntr}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\usepackage[normalem]{ulem} % For strikethrough
\usepackage{makecell}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{}

\begin{document}

\twocolumn[
\icmltitle{Disentangling CLIP Features for Enhanced Localized Understanding}
%\textcolor{red}{OR} Unmix-CLIP:Enhancing Fine-grained Visual Understanding through Mutual Information Reduction}
% Alternatives: 
% (1)Feature Disentanglement in CLIP: Enhancing Fine-grained Visual Understanding through Mutual Information Reduction 
% (2)Unmixing CLIP's Feature Space: A Mutual Information Approach to Fine-grained Visual Tasks
% (3)Feature-Disentangled CLIP: Breaking the Mutual Information Barrier in Vision-Language Models
% (4)Unmix-CLIP:

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance, which is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Samyak Rawlekar}{yyy}
% \icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
\icmlauthor{Yujun Cai}{sch}
\icmlauthor{Yiwei Wang}{comp}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
\icmlauthor{Ming-Hsuan Yang}{comp,temp}
%\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
\icmlauthor{Narendra Ahuja}{yyy}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Department of Electrical and Computer Engineering, University of Illinois Urbana-Champaign, USA}
\icmlaffiliation{comp}{UC Merced, USA}
\icmlaffiliation{sch}{ University of Queensland, Brisbane, Australia}
\icmlaffiliation{temp}{Yonsei University, South Korea}

\icmlcorrespondingauthor{Samyak Rawlekar}{samyakr2@illinois.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution

%% Uncomment this during camera-ready
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Vision-language models (VLMs) demonstrate impressive capabilities in coarse-grained tasks like image classification and retrieval. However, they struggle with fine-grained tasks that require localized understanding. To investigate this weakness, we comprehensively analyze CLIP features and identify an important issue: semantic features are highly correlated. Specifically, the features of a class encode information about other classes, which we call mutual feature information (MFI). This mutual information becomes evident when we query a specific class and unrelated objects are activated along with the target class. 
% 
To address this issue, we propose Unmix-CLIP, a novel framework designed to reduce MFI and improve feature disentanglement. We introduce MFI loss, which explicitly separates text features by projecting them into a space where inter-class similarity is minimized. To ensure a corresponding separation in image features, we use multi-label recognition (MLR) to align the image features with the separated text features. This ensures that both image and text features are disentangled and aligned across modalities, improving feature separation for downstream tasks. For the COCO-14 dataset, Unmix-CLIP reduces feature similarity by 24.9\%. We demonstrate its effectiveness through extensive evaluations of MLR and zero-shot semantic segmentation (ZS3). In MLR, our method performs competitively on the VOC2007 and surpasses SOTA approaches on the COCO-14 dataset, using fewer training parameters. Additionally, Unmix-CLIP consistently outperforms existing ZS3 methods on COCO and VOC.
%
\end{abstract}

\section{Introduction}
\label{sec: Introduction}
% \input{figs/teaser}

\begin{figure}[t]
    \centering
    \renewcommand{\arraystretch}{1} % Adjust row spacing
    \setlength{\tabcolsep}{1pt} % Adjust column spacing
    \begin{tabular}{c c c c }
        & Input Image & Query: Person & \hspace{1.75pt} Class Similarity \\ 
        \rotatebox{90}{\hspace{32pt}CLIP} 
        & 
        \includegraphics[width=0.32\linewidth]{figs/teaser/T1.png} & 
        \includegraphics[width=0.32\linewidth]{figs/teaser/vis_map_clip.png} & 
        {\includegraphics[width=0.30\linewidth]{figs/teaser/sim_map_clip.png}} \\ 
        \rotatebox{90}{\hspace{32pt}Ours} 
        & 
        \includegraphics[width=0.32\linewidth]{figs/teaser/T1.png} & 
        \includegraphics[width=0.32\linewidth]{figs/teaser/vis_map_ours.png} &  
        {\includegraphics[width=0.30\linewidth]{figs/teaser/sim_map_ours.png}} \\ 
        % \rotatebox{90}{\hspace{12pt}Ours}
        
    \end{tabular}
      \caption{\textbf{Comparison of Activated Regions.} When queried for the 'person' class (middle column, highlighted in red), CLIP shows activation in unqueried regions (dogs and horses), while our method maintains focus on the person. The rightmost column displays cosine similarities between class features, showing that reducing the inter-class similarity (person-dog: 0.84 $\rightarrow$ 0.42, person-horse: 0.80 $\rightarrow$ 0.28)  results in features that are suitable for fine-grained tasks. } 
  \label{fig:teaser}
\end{figure}


Vision-language models (VLMs) have emerged as powerful tools for understanding visual content through natural language supervision. CLIP \cite{clip}, trained on 400 million image-text pairs (WIT-400M), achieves remarkable performance in coarse-grained visual understanding tasks such as image classification \cite{coop}, image retrieval \cite{retrival}, and visual question answering \cite{vqa}. However, these models struggle with fine-grained tasks that require localized understanding, leading to significant performance degradation in multi-label recognition (MLR) \cite{mlmc,huang2024radiology} and semantic segmentation \cite{seg}. While previous work has attributed these limitations to architectural choices \cite{ViT_need_register,maskclip,clip_surgery} or training objectives \cite{tagclip,dong2023maskclip}, our analysis reveals a more fundamental issue: the entanglement of semantic features in CLIP's feature space.

We systematically analyze CLIP's features and identify two key factors contributing to this issue. First, the spatial pooling operation in the final layer, although effective for global tasks, discards essential localized information necessary for fine-grained understanding. Second, and more importantly, we discover significant interference between class features in the joint vision-language space, which we term mutual feature information (MFI). The mutual information becomes apparent during class-specific queries: regions corresponding to unrelated objects are consistently activated alongside the target class. For example, as illustrated in \Cref{fig:teaser}, regions containing 'dog' and 'horse' also activate when we query the class 'person.' This activation pattern strongly correlates with the high similarity scores between class text features (0.84 for person-dog and 0.80 for person-horse), indicating substantial feature entanglement in CLIP's representation space.


To address this fundamental limitation, we introduce Unmix-CLIP, a novel framework that disentangles class features in vision-language models. Drawing inspiration from the redundancy reduction principle \cite{barlow} in neuroscience, we extend this concept to the vision-language domain. While previous approaches have focused on architectural modifications \cite{maskclip,clip_surgery,gem_walid} or prompt engineering \cite{dualcoop,PositiveCoOp} to adapt VLMs for fine-grained tasks, Unmix-CLIP directly targets the root cause by minimizing MFI between class representations while preserving task-relevant information. We achieve this through a carefully designed MFI loss function that explicitly disentangles text features by projecting them to minimize inter-class similarity. To achieve a similar separation in image features, we align them with the projected text features using a multi-label recognition framework. The joint training using MFI loss (separates text features) and MLR loss (aligns text and image features) results in disentangled features that align across the image and text domains, leading to improved separation in semantic features.
% 

We train Unmix-CLIP on 80 classes from the COCO-14 \cite{coco} dataset and evaluate its performance on two fine-grained tasks: multi-label recognition (MLR) and zero-shot semantic segmentation (ZS3). For MLR evaluation, we use the COCO-14  and VOC2007 \cite{pascal-voc} datasets. For ZS3, we use VOC2012 \cite{pascal-voc} and COCO-17 \cite{coco} for seen classes, and VOC Context \cite{context} provides 59 classes, 30 of which are unseen during pre-training. Our experimental results demonstrate that Unmix-CLIP achieves competitive performance on VOC and outperforms state-of-the-art (SOTA) methods on the challenging COCO-14 dataset, using only one-third of their training parameters. For ZS3, Unmix-CLIP surpasses SOTA VLMs on datasets with seen classes, demonstrating that reducing mutual feature information (MFI) is crucial for fine-grained tasks.
% 
To further assess its segmentation capabilities, we apply Unmix-CLIP to segment objects in the images, recasting the task as single-label recognition, a task more suitable for CLIP.  We combine the segment-level and whole-image results to obtain zero-shot MLR predictions. Segmenting objects provides complementary information on top of global image features.
The main \textbf{contributions} of this work are:
\begin{itemize}
    \item We identify a critical challenge in adapting VLMs for fine-grained tasks: mutual information between class features (MFI) degrades fine-grained task performance 
    % \vspace{-15pt}
    \item To address this challenge, we propose Unmix-CLIP, a framework that adapts CLIP features for fine-grained tasks by reducing MFI. At its core lies our proposed MFI loss, which explicitly disentangles text features and guides the disentanglement of image features
    % \vspace{-10pt}
    \item We show that Unmix-CLIP outperforms SOTA multi-label recognition methods in challenging settings using significantly fewer training parameters. Additionally, it outperforms zero-shot semantic segmentation methods. Moreover, as an object segmenter, Unmix-CLIP enhances CLIP’s zero-shot MLR performance.
\end{itemize}


\section{Related Work}

% \input{figs/seperation}
\begin{figure*}[tp]
    \centering
    \renewcommand{\arraystretch}{1} % Adjust row spacing
    \setlength{\tabcolsep}{1pt} % Adjust column spacing
    \begin{tabular}{c c c c c}
       \hspace{20pt}VOC (Seen) & \hspace{15pt}COCO (Seen) & \hspace{15pt} Context (Partial) & \hspace{17pt}Context (Unseen) & \multirow{2}{*}{\vspace{-210pt} \includegraphics[width=0.1\linewidth, height=0.425\linewidth, keepaspectratio]{figs/ICML_Sep/colorbar.png}} \\
        \includegraphics[width=0.225\linewidth]{figs/ICML_Sep/voc.png} & 
        \includegraphics[width=0.225\linewidth]{figs/ICML_Sep/coco.png} & 
        \includegraphics[width=0.225\linewidth]{figs/ICML_Sep/context.png} &
        \includegraphics[width=0.225\linewidth]{figs/ICML_Sep/context_no_coco.png} \\ 

         \includegraphics[width=0.225\linewidth]{figs/ICML_Sep/voc_ours.png} & 
         \includegraphics[width=0.225\linewidth]{figs/ICML_Sep/coco_ours.png}& 
         \includegraphics[width=0.225\linewidth]{figs/ICML_Sep/context_ours.png} &  
         \includegraphics[width=0.225\linewidth]{figs/ICML_Sep/context_no_coco_ours.png} \\  
    \end{tabular}
    \caption{\textbf{Class Feature Similarity Analysis.} Comparison of class-text feature similarities between CLIP (top row) and our method (bottom row) across four datasets: VOC, COCO, Context, and Context (unseen). The heatmaps show cosine similarity between class text features, where darker blue indicates higher similarity. As the reduced off-diagonal similarity values show, our method achieves higher class feature separation. This improved class separation suggests better discrimination capabilities.} 
    \label{fig:seperation}
\end{figure*}

\noindent {\bf Recoding information.}
Shannon proposed that optimal information transmission involves designing codes with minimum entropy \cite{shannon1948mathematical}. The redundancy reduction principle extended this idea to neuroscience, suggesting that sensory systems recode information to reduce redundancy with minimal loss \cite{barlow}. This principle has since been applied to many recent works, including image compression \cite{balle2016end} and more popularly in representation learning \cite{infonce,contrast1,barlowtwins,contrast2,contrast3,contrast4}. While our loss function shares structural similarities with representation learning methods (a similarity and contrastive term), our method differs as follows: (1) Instead of learning features from scratch, we refine learned features (reducing MFI). (2) We do not rely on augmentation-based learning or batch processing. (3) Unlike contrastive methods that require paired embeddings, our approach operates on a fixed set of text embeddings. Most importantly, our objective is not generic feature learning but targeted feature modification to enhance task-specific utility.

\vspace{1mm}
%\subsection{Vision-Language Models for Fine-grained Tasks}
\noindent {\bf Vision-Language Models for Fine-grained Tasks.}
Vision-language models (VLMs) trained with contrastive losses \cite{clip, align} are challenging to adapt for fine-grained tasks due to two reasons: (1) their reliance on global feature aggregation, which ignores local information. (2) Using the softmax operation in their training loss biases them toward single-object settings.

{\em Recognition.} 
Early efforts to adapt VLMs for recognition centered on learning prompts as classifiers for visual features \cite{coop}. These methods were extended to multi-label settings by learning multiple prompts for each class \cite{dualcoop,dualcoop++,PositiveCoOp}. Subsequent works incorporated co-occurrence information to make predictions interdependent \cite{scpnet,MLR-GCN}. In contrast, our approach does not rely on prompt learning or co-occurrence modeling during pre-training. Furthermore, our features are adaptable to tasks beyond multi-label recognition.

{\em Localization.} Early approaches addressed localization by training image segmentation models and using VLMs to label the segmented regions \cite{sam}. Later methods introduced pre-training setups that combined vision-language alignment with mask distillation to enhance localization \cite{dong2023maskclip}. Recent works adapted features for localization without additional training by leveraging the spatial properties preserved in the value projection of CLIP’s transformer-style aggregation \cite{maskclip}. CLIP Surgery \cite{clip_surgery} identified consistent noisy activations across classes and reduced them by subtracting average features from class-specific features \cite{clip_surgery}, though the cause of these activations remains unclear. GEM generalized this concept to vision transformers \cite{gem_walid}. We use the finding that value projection preserves spatial information. We further improve value projection by disentangling class features.

% \input{figs/overview}
\begin{figure*}[tp]
  \centering
  \includegraphics[width=\linewidth]{figs/op.pdf}
  % \vspace{-8pt}
\caption{\textbf{Unmix-CLIP Overview.} Given image and label names in the dataset, CLIP extracts image and text features, which are further processed by respective projectors to embed into a disentangled space while preserving local image information. To reduce mutual feature information (MFI) between class features, we propose MFI loss that enforces the self-similarity matrix of projected text features to approximate an identity matrix, effectively reducing inter-class feature dependencies (\Cref{sec: Local Features - MFI Reduction}). We propagate the separation in the text features to image space by aligning the image and separated text features using a multi-label recognition setup (\Cref{sec:Global Features - MLR}). Following \cite{dualcoop} and as detailed in \Cref{sec:Global Features - MLR}, we aggregate the projected image and text features to obtain predicted logits. The predicted logits are trained with ground truth labels using the widely used asymmetric loss (ASL) \cite{asl}. Our training loss combines the ASL and MFI loss; the only trainable components are the projectors. We freeze both CLIP encoders and projectors during inference for multi-label recognition and downstream tasks such as zero-shot semantic segmentation.}
\label{fig:Overview}
\end{figure*}
% 

\section{Unmix-CLIP} 
\label{sec:method}
Given a multi-label dataset \(\mathcal{D}\), where \(\mathcal{D} = \{\mathbf{x}_i\}_{i=1}^{|\mathcal{D}|}\) consists of images \(\mathbf{x}_i\) and \(N\) class labels \(\{C_j\}_{j=1}^N\), each image \(\mathbf{x}_i\) can contain objects belonging to one or more of these \(N\) classes. Additionally, we use CLIP (\( f_\theta \)), parameterized by weights \(\theta\), consisting of an image encoder (\(f_{\theta,\text{img}}\)) and a text encoder (\(f_{\theta,\text{text}}\)) for feature extraction. Throughout all experiments, we keep the parameters of CLIP (\( f_\theta \)) frozen, including both the image and text encoders.

Since the mutual information among class features present in CLIP is detrimental to fine-grained task performance, we analyze CLIP's features from 
this perspective. Specifically, we focus on two key aspects: (1) spatial preservation in the visual feature maps and (2) the relationship between class features in the joint vision-language space.

Towards (1), we remove CLIP's final spatial pooling layer to preserve local information in feature maps. We then evaluate class-wise activations by computing the similarity between local visual and text features. For (2), we find that querying an image for a specific class consistently activates unrelated regions. \Cref{fig:teaser} shows that querying for 'person' highlights the person regions and activates areas containing dogs and horses. This suggests that CLIP's features for different classes share substantial information.

To quantify this feature entanglement, we analyze the similarity between class text features across multiple datasets (VOC \cite{pascal-voc}, COCO \cite{coco}, and Context \cite{context}). Since CLIP learns a joint embedding space, text feature similarities directly reflect the model's ability to distinguish between classes. As illustrated in \Cref{fig:seperation} (top-row), we consistently observe high similarity values between different classes. Specifically, the similarity reaches 0.84 for person-dog pairs and 0.80 for person-horse pairs, far exceeding what one would expect from their semantic relationships. Extending this analysis across various datasets (\Cref{tab:mfi_reduction}), We observe high average feature similarities of 0.77 in VOC, 0.69 in COCO, and 0.75 in Context, indicating that this is a universal limitation of CLIP's features space. This feature entanglement fundamentally affects CLIP's ability to perform fine-grained tasks. When features intended to represent one class encode significant information about other classes, the model struggles to make precise discrimination necessary for tasks like multi-label recognition and semantic segmentation.

To address this limitation, we propose a framework that reduces mutual information between class features while preserving task-essential semantics. Our approach consists of three components: (1) Feature extraction and Projection, where we extract CLIP features and project them into a disentangled space (\Cref{sec:Feature Extraction and Projection}),  (2) Defining novel MFI Loss for disentangling text features (\Cref{sec: Local Features - MFI Reduction}), and (3) Performing MLR to align image features to the disentangled text features  (\Cref{sec:Global Features - MLR}).



\subsection{Feature Extraction and Projection}
\label{sec:Feature Extraction and Projection}
We use CLIP as our feature extractor. Its image encoder (\(f_{\theta,\text{img}}\)) performs spatial pooling in the final layer, aggregating features from local regions into a \(d\)-dimensional vector for the input image \(x_i\). However, this pooling step removes spatial details, making it unsuitable for fine-grained tasks where localization is essential. We remove the final pooling layer to preserve class-specific information across local regions. Then the encoder output for input ($\mathbf{x}_{i}$) is ${f}_{\theta, img}(\mathbf{x_{i}}) = \mathbf{z}_i \in \mathbb{R}^{H \times W \times d}$ , where \(H\) and \(W\) are the spatial dimensions. The text encoder remains unchanged. We use a fixed pair of positive and negative (\( \mathbf{txt_{j,+}} \), \( \mathbf{txt_{j,-}} \)) prompts for each class \( j \) as input to the text encoder. The positive prompt indicates the presence of the class in a local region, while the negative prompt indicates its absence. Passing these prompts through the text encoder produces ${f}_{\theta, text}(\mathbf{txt_{i}}) = \mathbf{t}_i \in \mathbb{R}^{d}$.

The extracted features (image ($\mathbf{z}_i$), text ($\mathbf{t}_i $)) lie in CLIP's original feature space and are not suitable for fine-grained tasks as discussed in \Cref{sec: Introduction}.  To address this, we introduce learnable projectors ($ h_\phi :  h_{\phi,\text{img}} $ and $ h_{\phi,\text{text}} $), parameterized by weights $ \phi $. These projectors map the image ($ \mathbf{z}_i $) and text ($ \mathbf{t}_i $) features from their original space ($ d $-dim) to a new disentangled space ($ d' $-dim), making them suitable for fine-grained tasks. The image projector transforms $ \mathbf{z}_i \to \mathbf{z'}_i $ ($ \mathbb{R}^{H \times W \times d} \to \mathbb{R}^{H \times W \times d'} $) while preserving the spatial dimensions ($H ,W$). The text projector maps $ \mathbf{t}_i \to \mathbf{t'}_i $ ($ \mathbb{R}^{d} \to \mathbb{R}^{d'} $).

\subsection{MFI Loss}
\label{sec: Local Features - MFI Reduction}
We design the projected feature space to reduce mutual feature information (MFI) between class features. Reducing MFI requires obtaining individual class features, as MFI represents the shared information between these individual features. Separating image features into individual class features is non-trivial because multiple classes often co-occur in an image. This leads to mixed features that make class-wise feature isolation difficult. Object segmentation models could assist by extracting features from segmented regions, but these models add significant complexity. In contrast, text class features are inherently independent because they are derived from separate class names or prompts inputted to the text encoder. This independence directly gives us individual text class features. We leverage this property of text features and apply MFI reduction to them.

We propose the MFI reduction loss to minimize the mutual information between class text features. This loss is applied to the projected text features ($\mathbf{t'}$) as follows:
\begin{equation}
\label{eq:mfi}
\mathcal{L}_{\text{MFI}} = \underbrace{\sum_{i=1} \left( \mathbf{S}_{ii} - 1 \right)^2}_{\text{Collapse Prevention}} + \lambda \underbrace{\sum_{i=1} \sum_{\substack{j=1 \\ j \neq i}} \mathbf{S}_{ij}^2}_{\text{MFI Reduction}}
\end{equation}
where $\mathbf{S}$ is the self-similarity matrix obtained from $\mathbf{t'}$. Here, $\mathbf{S}$ is defined by 
\[
\mathbf{S}_{ij} = \frac{\mathbf{t'}_i \mathbf{t'}_j^\top}{\|\mathbf{t'}_i\| \|\mathbf{t'}_j\|}, \quad \forall i, j 
\]
where \( \mathbf{t'}_i, \mathbf{t'}_j \) are the \( i \)-th and \( j \)-th column vectors of \( \mathbf{t'} \) (i.e., \( \mathbf{t'}_i, \mathbf{t'}_j \in \mathbb{R}^{d'} \)) and  \( \|\mathbf{t'}_i\| \) is the \( L_2 \)-norm of \( \mathbf{t}_i \). In this formulation, \( \lambda \) is the hyperparameter that addresses the imbalance in the loss arising from the larger number of MFI reduction terms in \( \mathbf{S} \) compared to the collapse prevention terms.

The MFI loss minimizes the inter-class similarity \( \mathbf{S}_{ij} \) (\( i \neq j \)) while simultaneously preserving high intra-class \( \mathbf{S}_{ii} \) to prevent feature collapse. We provide detailed proof of the loss function's connection to the Information Bottleneck principle in supplementary material \Cref{Supp:info-bottleneck}.

\begin{table*}[tp]
\centering
\caption{\textbf{Comparison on multi-label recognition (MLR).} We compare the performance (mAP) and training efficiency (number of parameters) of our approach with SOTA VLM-based MLR methods on VOC2007 and COCO-14 datasets. Our approach is competitive with SOTA on VOC2007, and on the challenging COCO dataset, it outperforms SOTA while requiring only one-third of the parameters. \textcolor{red}{red} and \textcolor{blue}{\underline{blue}} indicate the best and the second best performance.}
\small
\begin{tabular}{ccccc}
\toprule
\textbf{Methods}               & \multicolumn{2}{c}{\textbf{VOC2007}}       & \multicolumn{2}{c}{\textbf{COCO-14}}       \\  \cmidrule(lr){2-3} \cmidrule(lr){4-5}
                               & \textbf{\# Params($\downarrow$)} &  \textbf{mAP($\uparrow$)} & \textbf{\# Params ($\downarrow$)} & \textbf{mAP($\uparrow$)} \\ \midrule
DualCoOp \cite{dualcoop}       &  0.3M           &  94.2          &   1.3M            &   83.6       \\
SCPNet \cite{scpnet}           &  -              &  94.3          &   3.4M            &   84.4       \\
TAI-DPT \cite{TaI-DPT}         & $>$ 0.3M        & -              &   $>$1.3M         &   84.5       \\
DualCoOp++ \cite{dualcoop++}   &  \textcolor{blue}{\underline{0.4M}}           &  \textcolor{red}{94.9} &   1.5M            &   \textcolor{blue}{\underline{85.1}}       \\
MLR-GCN \cite{MLR-GCN}         &  0.3M           &  94.4          &   1.3M            &    -         \\
PositiveCoOp \cite{PositiveCoOp}    &  \textcolor{red}{0.2M}          &  94.4          &  \textcolor{blue}{\underline{0.8M}}           &   84.7        \\
\midrule
Ours                           &  \textcolor{blue}{\underline{0.4M}}          &  \textcolor{blue}{\underline{94.8}}  &  \textcolor{red}{0.4M}   &   \textcolor{red}{85.3}        \\ \bottomrule
\end{tabular}

\label{tab:MLR performance}
\end{table*}



\subsection{Image-Text Alignment with MLR}
\label{sec:Global Features - MLR}

\textbf{MLR Formulation.} MLR task involves identifying the subset of classes \(\mathcal{C}_i \subseteq \{C_1, C_2, \ldots, C_N\}\) associated with the image \(\mathbf{x}_i\). The goal is to learn a mapping function \(g: \mathbf{x}_i \rightarrow \{-1, 1\}^N\), that maps input images to \(1\) if the class is present and \(-1\) if the class is absent in the image. 


We train our model to recognize multiple objects in images by learning to align projected image features and text features. For each location $(h,w)$ in the projected image features $(\mathbf{z'}_i)$, we detect the presence or absence of a class $j$, by computing the cosine similarity with positive text features ($\mathbf{t'_{j,+}}$) and negative text features ($\mathbf{t'_{j,-}}$). A higher similarity with the positive text features indicates the presence of the class, while a higher similarity with the negative text features indicates its absence. We aggregate these similarity scores from local regions to produce logits $\mathbf{p_{i}}$ for the image, following \cite{dualcoop, MLR-GCN, PositiveCoOp}. We train the setup with the widely used Asymmetric Loss function (ASL) \cite{asl}, which addresses the significant imbalance between negative and positive examples in a multi-label recognition dataset. The ASL loss is given by:
% 
\begin{align}
    \mathcal{L}_{ASL}(p_{i}^{j}) = 
    \begin{cases} 
        \left(1 - p_{i}^{j}\right)^{\gamma_{+}} \log \left(p_{i}^{j}\right), & \text{if } y_{i}^{j} = 1, \\
        \left(p_{i, \delta}^{j}\right)^{\gamma_{-}} \log \left(1 - p_{i, \delta}^{j}\right), & \text{else }
    \end{cases} 
\label{eq:ASL}
\end{align}
% 
where $p_{i}^{j}$ represents the corresponding prediction associated with label $y_{i}^{j}$, $i$ represents the image and $j$ represents the class.  $p_{i, \delta}^{j} = \max(\hat{y} - \delta, 0)$, with $\delta$ representing the shifting parameter defined in ASL.
% 

\textbf{Training.}
Our training objective is composed of two components: (1) mutual feature information loss that enforces the separation between class text features and (2)  Asymmetric loss function \cite{asl}, designed for MLR that aligns the image features and text features to obtain predictions for an image.
% 
\begin{equation}
 \mathcal{L}_{\text{Unmix-CLIP}} =  \mathcal{L}_{\text{ASL}} + \alpha \mathcal{L}_{\text{MFI}}
\end{equation}
where 
\( \alpha \) controls the relative importance of the two objectives.


\section{Experiments}
Here we describe the datasets, evaluation metrics, implementation details, and performance analysis for multi-label recognition and zero-shot semantic segmentation.

\subsection{Datasets and Metrics}

1) Pre-training with MLR: We evaluate the MLR performance using mean-Average Precision (mAP) on the following datasets:\\
\textbf{COCO-14} \cite{coco} contains 80 classes across diverse categories with 82,081 training and 40,504 validation images. Following recent works \cite{dualcoop,MLR-GCN,PositiveCoOp}, we train on the training set and evaluate on the validation set. \\
% 
\textbf{VOC2007} \cite{pascal-voc} is another widely used MLR dataset containing 20 classes with 9,963 images. Following \cite{dualcoop,MLR-GCN,PositiveCoOp}, we use the train-val set for training and the test set for evaluation. 


2) Zero-Shot Semantic Segmentation (ZS3): We use image and text projectors trained on the COCO-14 dataset and evaluate ZS3 using the mIoU metric on the following datasets: \\
\textbf{PASCAL VOC 2012} \cite{pascal-voc} includes segmentation masks for the 20 classes in VOC2007.  Following works \cite{clip_surgery,gem_walid}, we evaluate on the validation set.\\
% 
\textbf{PASCAL Context} \cite{context} extends PascalVOC to 59 classes, 30 of which were unseen during our pre-training. These additional classes provide dense annotations for the whole scene. We evaluate the test set, comprising 5,104 images.\\
% 
\textbf{COCO-2017} \cite{coco} includes segmentation masks for the 80 classes in COCO-14.  Following\cite{clip_surgery,gem_walid}, we evaluate the validation set.
% 
% 
\subsection{Implementation Details}
We use CLIP's \cite{clip} original pre-trained encoder weights for all our experiments and keep them frozen. Consistent with popular MLR and ZS3 literature, we use a ResNets-based visual encoder (RN-101) and the standard transformer for text encoding \cite{dualcoop,scpnet,dualcoop++,PositiveCoOp,MLR-GCN,TaI-DPT,clip_surgery,CLIP-ES}. We conduct all experiments on a single RTX A4000 GPU.

During the pre-training stage with the MLR setup (\Cref{sec:Global Features - MLR}), we follow the settings and hyperparameters from recent works \cite{dualcoop,MLR-GCN,PositiveCoOp}. This includes resizing images to $448$, applying Cutout \cite{cutout} and RandAugment \cite{randaug} for augmentation. 
% 
Our projectors ($h_{\phi}$) are implemented as multi-layer perceptrons (MLPs). Specifically, the image projector follows a [512 $\rightarrow$ 256] architecture, while the text projector is designed as [512 $\rightarrow$ 384 $\rightarrow$ 256] with batch normalization and ReLU. We train both projectors with stochastic gradient descent (SGD) using an initial learning rate of 0.002, which is reduced by cosine annealing. We train the Unmix-CLIP setup (ASL + MFI loss) for 50 epochs with a batch size of 32. We follow \cite{dualcoop,MLR-GCN,PositiveCoOp}, and use ASL hyperparameters in \Cref{eq:ASL} as $\gamma_- = 2$, $\gamma_+ = 1$ and $\delta$ = 0.05. We set $\lambda$ = 0.2 and $\alpha$ = $7\mathrm{e}{-5}$ when pre-trained with COCO-14 in \Cref{eq:mfi}. 

For Zero-Shot Semantic Segmentation, we adopt the v-v attention described in \cite{clip_surgery} that prevents inversion of activation commonly observed in CLIP. We then add our pre-trained projectors to CLIP.  To obtain the segmentation mask, we compute the cosine similarity between locally projected image features ($\mathbf{z'}$) and projected text features for all classes in the dataset. We use the text template "A photo of a \{classname\}." Lastly, we use bilinear interpolation to upsample the segmentation mask to the input image size.

% \input{tables/zs3}
\begin{table*}[tp]
    \centering
    \caption{\textbf{Comparison on zero-shot semantic segmentation (ZS3).} We compare Unmix-CLIP with other SOTA baselines across three semantic segmentation datasets using the mIoU metric. The "Dataset" column details the pre-training dataset and the type of annotations used. The abbreviations are as follows: Loc Ann. + FT: local annotations and fine-tuning, SM: segmentation mask, IT: image-text, IC: image classes, Bkgd: include background class, No Bkgd: ignore background class, MR: MFI Reduction(\%), \textcolor{red}{red} and \textcolor{blue}{\underline{blue}} indicate the best and the second best performance}
    \small
    \begin{tabular}{cccccccc}
        \toprule
        \textbf{Method} & \textbf{Loc Ann.} & \multicolumn{2}{c}{\textbf{Dataset}} & \textbf{VOC12} & \multicolumn{2}{c}{\textbf{COCO-17}}  & \textbf{Context} \\ \cmidrule(lr){3-4} \cmidrule(lr){5-5} \cmidrule(lr){6-7}
           \textbf{Arch: RN-101}& \textbf{+ FT} & Pre-training & Ann & Bkgd &Bkgd  & No Bkgd  &  \\ \midrule
         SPNet\cite{SPNet}  & \ding{51} & COCO, VOC, Context & SM & 15.6 & - & - & 4 \\ 
         ZS3Net\cite{ZS3Net} & \ding{51} & VOC, Context & SM & 17.7 & - & - & 7 \\ 
         CLIP-ES\cite{CLIP-ES}  & \ding{51} & WIT, COCO-Stuff & IT,IC & 75 & - & - & - \\ \midrule
         CLIP\cite{clip} & \ding{55} & WIT-400M & IT & 14.1 & 3.9 & 5.6 & 4.1 \\ 
         CLIPSurgery\cite{clip_surgery} & \ding{55} & WIT-400M & IT & 17.5 & 13.0 & 22.9 & 11 \\ 
         CLIP-VV\cite{clip_surgery} & \ding{55} & WIT-400M & IT & \textcolor{blue}{\underline{32.6}}  & \textcolor{blue}{\underline{19.9}}  & \textcolor{blue}{\underline{35.5}} &  \textcolor{red}{15.5} \\ 
         \midrule
         Ours (MR = 24.9) & \ding{55} & WIT-400M, COCO & IT,IC  & \textcolor{red}{36} & \textcolor{red}{22.7}  & \textcolor{red}{37.8}  & \textcolor{blue}{\underline{12.9}}  \\ \bottomrule 
    \end{tabular}
    
\label{tab:ZS3_performance}
\end{table*}



\subsection{Results}
\label{sec: Results}
\textbf{Multi-Label Recognition.}
We primarily compare Unmix-CLIP with other SOTA VLM-based MLR approaches. In \Cref{tab:MLR performance}, we present a detailed comparison of the performance (mAP) and the number of training parameters required by each method on the VOC2007 \cite{pascal-voc} and COCO-14 \cite{coco} datasets. For VOC 2007, we observe that our performance is competitive with DualCoOp++\cite{dualcoop++} and requires the same number of parameters. However, on the more challenging COCO-14 dataset, Unmix-CLIP outperforms DualCoOp++ while requiring only one-third of the training parameters. 
% 

\textbf{Zero-Shot Semantic Segmentation.}
We categorize our comparisons into two main groups. The first group includes approaches that use local annotations (segmentation masks, etc.) to fine-tune the network \cite{SPNet, ZS3Net, CLIP-ES}. The second comparison is with training-free approaches \cite{clip, clip_surgery}. Our approach is closer to the training-free methods, as it does not use any form of local annotations.

Our results are summarized in \Cref{tab:ZS3_performance}. Following prior works \cite{SPNet, ZS3Net, CLIP-ES, clip_surgery}, we report mIoU values for VOC 2012 by including the background as a class. We use a threshold of 0.85 to identify the background, as suggested in \cite{gem_walid}. Our approach outperforms CLIP Surgery by 18.5 mIoU and CLIP-VV by 3.4 mIoU on VOC 2012. For COCO-14, we report results both with and without the background class. When including the background, our method surpasses CLIP Surgery and CLIP-VV by 9.7 mIoU and 2.8 mIoU, respectively. Without the background, we achieve gains of 14.9 mIoU and 2.3 mIoU. Additionally, we evaluate the VOC Context dataset, which contains 30 unseen classes not used during our pre-training. Although our model is not explicitly trained to reduce MFI between these classes (it is designed to minimize MFI among COCO’s 80 classes), our approach still outperforms CLIP Surgery. These results demonstrate that our projectors preserve some of the open-vocabulary capabilities of CLIP. We show qualitative results for open-vocabulary tasks in Supplementary \Cref{fig:openvocab}.

% \input{tables/zs_mlr}
\begin{table}[tp]
\centering
\caption{ \textbf{Comparison on Zero-shot Multi-Label Recognition (ZS-MLR).}  We segment objects from images using Unmix-CLIP and improve CLIP's zero-shot multi-label recognition capabilities by integrating predictions from segmented objects and the entire image.}
\small
\begin{tabular}{cccc}
\toprule
\textbf{Dataset} & \textbf{Backbone} & \textbf{CLIP (mAP)} & \textbf{Ours (mAP)} \\ \midrule
\multirow{2}{*}{VOC2007}  & RN 101            & 78.73         & \textbf{80.71}         \\ 
                         & RN 50             & 76.20         & \textbf{79.87}         \\ \midrule
\multirow{2}{*}{COCO-14} & RN 101            & 50.10         & \textbf{52.00}         \\ 
                         & RN 50             & 47.30         & \textbf{50.15}            \\ \bottomrule
\end{tabular}
\label{tab:zero_shot_mlr}
\end{table}


\textbf{Segmentation-driven Zero-Shot Multi-Label Recognition (ZS-MLR).} We leverage the segmentation capabilities of Unmix-CLIP to reformulate the multi-label recognition problem into a single-label recognition problem, a domain more suitable for CLIP. Specifically, we use two predictions: global and local. We pass the input image directly through CLIP to obtain its global predictions. However, as discussed in \Cref{sec: Introduction}, these predictions are often dominated by more prominent objects in the image, ignoring smaller objects, which leads to poor zero-shot MLR performance. 
To address this limitation, we introduce local predictions. We segment the image into multiple regions (ideally corresponding to individual objects) using Unmix-CLIP. Each segment is then processed independently through CLIP, and the predictions from all segments are aggregated. Finally, we combine the global and local predictions to obtain the zero-shot scores for the image.
% 
We evaluate the ZS-MLR performance on the VOC2007 and COCO-14 datasets, with results presented in \Cref{tab:zero_shot_mlr}. The results demonstrate that our method provides meaningful information (segments) to improve CLIP zero-shot capabilities.

\section{Analysis}

\input{figs/ZS3}

\begin{figure}[tp]
  \centering
  \includegraphics[width=\linewidth]{figs/sep_miou_mAP.png}
  % \vspace{-18pt}
  \caption{\textbf{Performance vs. MFI Reduction.} Performance of Multi-Label Recognition (MLR, measured by mAP) and Zero-Shot Semantic Segmentation (ZS3, measured by mIoU) on COCO as a function of MFI reduction. As class feature separation increases (i.e., MFI decreases), the model performs better on both tasks.}
  \label{fig:mAP_mIOU_MFI_red}
  % \vspace{-50pt}
\end{figure}

\textbf{Feature Disentanglement.} 
We pre-train Unmix-CLIP on COCO-14, which contains 80 classes. As shown in \Cref{sec: Results}, our approach improves performance even on datasets with previously unseen classes, such as VOC Context. We analyze this improvement by comparing MFI reduction across four datasets: VOC2012 (20 seen classes), COCO-2017 (80 seen classes), Context (59 partially seen classes), and a Context subset (30 unseen classes from COCO-2017). \Cref{fig:seperation} shows the self-similarity matrices of class text features from CLIP and Unmix-CLIP, demonstrating the class feature disentanglement. \Cref{tab:mfi_reduction} quantifies the MFI reduction through the difference in average inter-class similarity between CLIP and Unmix-CLIP. 
%
Our framework effectively disentangles representations for both seen and unseen classes, leading to performance gains.

\input{tables/MFI_reduction}



\textbf{Feature Disentanglement Impact}. \Cref{fig:mAP_mIOU_MFI_red} shows how MFI reduction improves performance in multi-label recognition on COCO-14 dataset and zero-shot semantic segmentation on the COCO-2017 dataset. We observe that as MFI decreases, the performance of both tasks improves.

\input{tables/loss_ablation}

\section{Conclusions}
In conclusion, this work advances our understanding of CLIP features by identifying and addressing a fundamental challenge in their localized understanding. 
We first show that reducing mutual information is critical for fine-grained recognition tasks. 
Motivated by this, we introduce Unmix-CLIP, a novel approach to project CLIP features into a disentangled space by combining our proposed MFI loss and the asymmetric loss for MLR. Our experimental results demonstrate that reducing feature entanglement through Unmix-CLIP significantly enhances the model's ability to perform fine-grained tasks. This improvement is particularly evident in two challenging tasks: multi-label recognition (MLR) and zero-shot semantic segmentation (ZS3). These findings highlight the importance of feature disentanglement in vision-language models and provide a promising direction for future research in improving the localized understanding capabilities of CLIP-based architectures.
% 
A limitation of our approach is its reduced capability in zero-shot open-vocabulary segmentation. We constrain some of CLIP's broader semantic capabilities by optimizing feature disentanglement for COCO dataset classes. Training on substantially larger datasets could help mitigate this limitation while preserving the benefits of our feature disentanglement approach.


\bibliography{main}
\bibliographystyle{icml2025}

\newpage
\pagebreak

\newpage
\pagebreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% APPENDIX %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix
% \onecolumn
\section{Objective Function: MFI Loss}
\label{Supp:info-bottleneck}

This section establishes a connection between MFI loss and the Information Bottleneck (IB) principle \cite{tishby2015deep}. As described in \Cref{sec: Local Features - MFI Reduction}, MFI loss explicitly reduces the mutual information between text features to obtain disentangled features. 

\subsection{Information Bottleneck (IB) Objective}
\textbf{Formulation.}  Let, $T_i$ represent the input text (i.e., the prompt with the class label), and let \( Z_i \) be the extracted features from the CLIP text encoder \cite{clip}. The output is represented by \( Y_i \), indicating the class associated with \( Z_i \). 

As we show in \Cref{sec:method}, mutual information exists between text (class) features \( Z_i \), i.e., each class feature contains information about multiple classes rather than only its corresponding class \( Y_i \).
Our goal is to enforce a one-to-one mapping where \( Z_i \) retains information only about \( Y_i \) while discarding information about all other classes \( Y_j \) (\( j \neq i \)). 

This aligns naturally with the IB principle, which formulates an optimal trade-off between minimizing the information \( Z_i \) retains from \( T_i \) and maximizing the information it preserves for the target class \( Y_i \). We extend the IB principle to reduce explicit information about all other classes. We express this formally as:  


\begin{equation} \mathbf{IB} = \mathbf{I}(Z_i, T_i) + \beta \left[\mathbf{I}(Z_i, Y_i) - \sum_{j \neq i} \mathbf{I}(Z_i, Y_j) \right]
\label{eq:it_bottleneck} \end{equation}

where \( \mathbf{I} \) represents mutual information. Here, 

1. \( I(Z_i; T_i) \) ensures that \( Z_i \) take only the information form $T_i$ that is needed to map to $Y_i$.  \\
2. \( I(Z_i; Y_i) \) preserves discriminative class information. \\  
3. \( \sum_{j \neq i} I(Z_i; Y_j) \) reduces information in $Z_i$ that map to $Y_j$ where $j \neq i$

\subsection{Connection to MFI Loss}
To minimize IB, we first express mutual information in terms of entropy:
% 
\begin{equation} \mathbf{I}(A; B) = \mathbf{H}(A) - \mathbf{H}(A | B),
\label{eq:mutual_info} \end{equation}
where $\mathbf{H}(A)$ is the marginal entropy of $A$, and $\mathbf{H}(A|B)$ is the conditional entropy of $A$ given $B$.

Substituting this into the IB objective \Cref{eq:it_bottleneck}:
\begin{align}
\mathbf{IB} = &\; (1 + \beta - \sum_{j \neq i} \beta) \mathbf{H}(Z_i) - \beta \left[ \mathbf{H}(Z_i | Y_i) - \sum_{j \neq i}\mathbf{H}(Z_i | Y_j) \right] \nonumber \\
&\; - \, \mathbf{H}(Z_i | T_i)
\label{eq:entropy_form}
\end{align}


Since the CLIP text encoder is deterministic, the entropy term \( \mathbf{H}(Z_i | T_i) = 0 \). Also, given that text inputs are predefined (i.e., class names in the dataset), \( Z_i \) is deterministic, implying \( \mathbf{H}(Z_i) = 0 \). This simplifies the IB objective to:

\begin{equation} \mathbf{IB} \propto -\mathbf{H}(Z_i | Y_i) + \sum_{j \neq i} \mathbf{H}(Z_i | Y_j).
\label{eq:final_simplified} \end{equation}

\begin{figure}[tp]
  \centering
  \includegraphics[width=0.7\linewidth]{figs/supp_proof.pdf}
  % \vspace{-8pt}
  \caption{The Information Bottleneck principle is applied for feature disentanglement. Given an input text \( T_i \), the text encoder of CLIP \cite{clip} generates features \( Z_i \), which encode information about the output classes \( Y_i \). Our objective is to ensure that \( Z_i \) retains only the information necessary to map to its corresponding class \( Y_i \) while minimizing its information about other classes \( Y_j \) (\( j \neq i \))  }
  \label{fig:supp_proof}
\end{figure}

Assuming $Z$ follows a Gaussian distribution, its entropy is given by:
\begin{equation} \mathbf{H}(Z) = \frac{1}{2} \log \left|\mathbf{C}\right| + \text{const},
\label{eq:gaussian_entropy} \end{equation}
where $\mathbf{C}$ is the covariance matrix of $Z_i$. Since the constant term does not affect the optimization, we optimize the determinant of the covariance matrix $\mathbf{C}$. In practice, we optimize the covariance matrix. Thus, minimizing IB reduces the covariance between class features, ensuring they are independent.  


The IB objective in \Cref{eq:final_simplified} becomes:
\begin{equation} \mathbf{IB} \propto - \mathbf{C}_{Z_i | Y_i} + \sum_{j \neq i} \mathbf{C}_{Z_i | Y_j},
\label{eq:gaussian_formulation} \end{equation}

Minimizing the IB objective is equivalent to minimizing the MFI Loss. Specifically, maximizing $\mathbf{C}_{Z_i | Y_i}$ is equivalent to collapse prevention term and minimizing $\sum_{j \neq i} \mathbf{C}_{Z_i | Y_j}$ is our MFI reduction term in the following equation:

\begin{equation}
\mathcal{L}_{\text{MFI}} = \underbrace{\sum_{i=1} \left( \mathbf{S}_{ii} - 1 \right)^2}_{\text{Collapse Prevention}} + \lambda \underbrace{\sum_{i=1} \sum_{\substack{j=1 \\ j \neq i}} \mathbf{S}_{ij}^2}_{\text{MFI Reduction}}
\end{equation}

\input{figs/open_vocab}

\begin{figure*}
  \centering
  \includegraphics[width=\linewidth]{figs/mIoU_vs_mAP.png}
  % \vspace{-8pt}
  \caption{\textbf{mAP vs mIoU}. Performance comparison of zero-shot semantic segmentation (mIoU) for VOC2012, COCO 2017 with and without the background, and VOC Context as a function of multi-label recognition (mAP) performance on the COCO-14 dataset. A general trend: higher MLR performance positively correlates with segmentation results.}
  \label{fig: mAP_mIoU}

\end{figure*}

\end{document}
