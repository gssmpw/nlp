\begin{abstract}
Early Exit (EE) techniques have emerged as a means to reduce inference latency in Deep Neural Networks (DNNs). The latency improvement and accuracy in these techniques crucially depend on the criteria used to make exit decisions. We propose a new decision criterion \algo{}
where exit classifiers are treated as experts and aggregate their confidence scores. The confidence scores are aggregated only if neighbouring experts are consistent in prediction as the samples pass through them, thus capturing their ensemble effect. A sample exits when the aggregated confidence value exceeds a threshold. The threshold is set using the error rates of the intermediate exits aiming to surpass the performance of conventional DNN inference. Experimental results on the COCO dataset for Image captioning and GLUE datasets for various language tasks demonstrate that our method enhances the performance of state-of-the-art EE methods, achieving improvements in speed-up by a factor $1.5\times$ to $2.1\times$. When compared to the final layer, its accuracy is comparable in harder Image Captioning and improves in the easier language tasks. The source code for this work is publicly available at  
\noindent{\href{https://github.com/Div290/BEEM1/tree/main}{\faGithub}} .

% In early exits, the classifiers attached at each layer can be treated as experts, and if their predictions are consistent, their confidence scores can be accumulated to obtain an ensemble effect. We propose a novel model named Boosting Early Exit DNN Performance with Multiple Classifiers (\algo{}) to improve the performance of early exit DNNs.  \algo{} aggregates weighted confidence from the successive exit layers that agree in predictions. . \algo{} assigns weights to exits based on their performance or the cost required to obtain inference from them. We experiment on BERT/ALBERT models for NLP tasks and encoder-decoder models for image captioning. 
\end{abstract}