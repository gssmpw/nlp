
\section{Experiments}
In this section, we provide the details of the experiments performed in this work.

{\bf Datasets:}
We evaluate our approach using the GLUE benchmark datasets \citep{wang2019glue}. Our assessments encompass diverse tasks, such as sentiment classification using the Stanford Sentiment Treebank (SST-2), Natural Language Inference (NLI) tasks with Multi-Genre Natural Language Inference (MNLI), Question Natural Language Inference (QNLI), and Recognizing Textual Entailment (RTE). For Paraphrase Similarity Matching, we include Microsoft Research Paraphrase Matching (MRPC) and Quora Question Pairs (QQP), while Linguistic Acceptability is measured using The Corpus of Linguistic Acceptability (CoLA). In instances where datasets comprise multiple units, we report the arithmetic mean. We exclude the WNLI task,  following previous works \citep{devlin2018bert, zhu2021leebert, zhou2020bert}. For captioning, we use the COCO \citep{lin2014microsoft} dataset.



{\bf Baselines:}
We compare against the vanilla DNN exiting and other techniques that speed up DNN inference. The baselines are as follows:


\textbf{1) Final layer:} The final layer of the DNN model, referred to as the "final layer" in Table \ref{tab: results1}.

\textbf{2) Reducing layers:} We use only the first 9 layers of the DNN model with a single output layer, denoted as DNN-9L. This serves as a performance lower bound since it employs no EE techniques.

\textbf{3) Early-exit models:}
DeeBERT \citep{xin2020deebert} and ElasticBERT \citep{liu2021towards}: Use fixed confidence thresholds for early exits.
FastBERT \citep{liu2020fastbert}: Uses a self-distillation framework to train intermediate exits.
PABEE \citep{zhou2020bert} and LeeBERT \citep{zhu2021leebert}: uses prediction stability, with LeeBERT incorporating knowledge distillation. ZTW \citep{sun2021early}: combines the output probability outputs across all the layers and trains the weights provided as additional parameters for training. 
PCEEBERT \citep{zhang2022pcee}: Combines confidence and patience metrics, similar to PABEE.
MuE \citep{tang2023you}: Uses hidden representation similarity for early exits, applied to the BERT-base model.
PALBERT model \citep{balagansky2022palbert}: State-of-the-art methods that face adaptation challenges due to training dataset bias. PALBERT uses Lambda layers \citep{banino2021pondernet}. JEI-DNN \citep{regol2023jointly} performs exiting using a gating mechanism where it learns a probability distribution over all exits and decidesto exitg based on that. DeeCAP \citep{fei2022deecap} is specifically for image captioning that uses an imitation network to mimic the behaviour of the decoder model.

We utilized the codebases of existing methods to get the results, all the results were obtained using the hyperparameters given in their available codes. Note that for the encoder-decoder model, we extend the ideas of DeeBERT, FastBERT, PABEE, and LeeBERT to the decoder of the backbone.
\begin{table*}[]
\centering
\small
\begin{tabular}{ccccccccccc}
\hline
\textbf{Model/Data}               & \multicolumn{2}{c}{\textbf{SST-2}}    & \multicolumn{2}{c}{\textbf{MNLI}}      & \multicolumn{2}{c}{\textbf{RTE}}      & \multicolumn{2}{c}{\textbf{QNLI}}     & \multicolumn{2}{c}{\textbf{QQP}}      \\ \hline
                         & Acc          & Speed      & Acc           & Speed      & Acc          & Speed      & Acc          & Speed      & Acc          & Speed      \\ \hline
\multicolumn{11}{c}{\textit{Dev set}}                                                                                                                                                         \\ \hline
ALBERT                   & 92.4         & 1.00x             & 84.5          & 1.00x             & 77.9         & 1.00x             & 91.3         & 1.00x             & 90.6         & 1.00x             \\
ALBERT-9L                & -1.6         & 1.33x          & -3.2          & 1.33x          & -2.5         & 1.33x          & -2.7         & 1.33x          & -1.5         & 1.33x          \\ \hline
DeeBERT                  & -2.3         & 1.72x          & -2.9          & 1.65x          & -3.1         & 1.78x          & -1.9         & 1.57x          & -2.5         & 1.81x          \\
ElasticBERT              & -2.1         & 1.75x          & -2.3          & 1.71x          & -2.7         & 1.81x          & -1.7         & 1.66x          & -2.1         & 1.78x          \\
FastBERT                 & -1.1         & 1.85x          & -0.3          & 1.61x          & -0.2         & 1.79x          & -0.8         & 1.71x          & -0.3         & 1.88x          \\
PABEE                    & -0.1         & 1.87x          & -0.5          & 1.85x          & -0.7         & 1.64x          & -0.6         & 1.81x          & -0.2         & 1.68x          \\
ZTW                      & -0.2         & 1.64x          & -0.3          & 1.67x          & +0.2         & 1.63x          & -0.3         & 1.75x          & -0.1         & 1.71x          \\
PCEEBERT                  & +0.1         & 1.24x          & 0.0          & 1.31x          & +0.3          & 1.27x          & -0.1         & 1.21x          & +0.1         & 1.37x          \\

LeeBERT                  & 0.0            & 1.78x          & -0.2          & 1.74x          & -0.1         & 1.59x          & +0.1         & 1.79x          & -0.2         & 1.97x          \\
PALBERT                  & -0.4         & 1.54x          & -0.8          & 1.61x          & +0.3          & 1.45x          & -0.2         & 1.59x          & -0.1         & 1.63x          \\

JEI-DNN                    & -0.1        & 1.77x          & +0.1          & 1.67x          & 0.0            & 1.35x          & -0.1         & 1.43x          & +0.2         & 1.57x          \\
\hline
\algo{}-C                    & 0.0         & 1.71x          & +0.1          & \textbf{2.03x}          & +0.4           & 1.79x          & 0.0         & 1.90x          & 0.0         & 1.93x          \\
\algo{}-A & \textbf{+0.4} & \textbf{1.98x} & \textbf{+0.3}  & 1.96x & \textbf{+0.7} & \textbf{1.89x} & \textbf{+0.2} & \textbf{1.92x} & \textbf{+0.5} & \textbf{2.09x} \\ \hline
\multicolumn{11}{c}{\textit{Test set}}                                                                                                                                                        \\ \hline
ALBERT                   & 92.3         & 1.00x             & 84.2          & 1.00x             & 72.1         & 1.00x             & 90.9         & 1.00x             & 80.1         & 1.00x             \\ \hline
ZTW        &  -0.4    &     1.61x       &  
      -0.5            &      1.52x    
   &      +0.1      &         1.64x   & -0.1       &       1.59x   &    -0.5  &     1.81x
         \\
LeeBERT                  & -0.5         & 1.79x          & -0.9          & 1.88x         & 0.0            & 1.68x          & -0.4         & 1.72x          & -0.3        & 1.86x          \\
PALBERT                  & -0.3         & 1.49x          & -1.1          & 1.72x          & +0.2          & 1.27x          & -0.4         & 1.51x          & -0.3         & 1.50x           \\
JEI-DNN                    & -0.1         & 1.35x          & -0.7          & 1.59x          & 0.0            & 1.36x          & -0.2         & 1.39x          & 0.0            & 1.47x          \\
\hline
\algo{}-C                    & -0.2         & \textbf{1.98x}          & -0.4          & 1.95x          & +0.1            & 1.74x          & {+0.1}         & 1.81x          & +0.1            & \textbf{1.97x}          \\
\algo{}-A & \textbf{+0.4}   & 1.91x & \textbf{-0.3} & \textbf{2.06x} & \textbf{+0.6} & \textbf{1.77x} & \textbf{+0.5} & \textbf{1.88x} & \textbf{+0.2} & 1.95x \\ \hline
\end{tabular}
\caption{Main results: This table compares \algo{} against all the state-of-the-art early exiting baselines. We report the accuracy (Acc in \%) and Speed-up (Speed).}
\label{tab: results1}
% \vspace{-0.5cm}
\end{table*}
\begin{table}  % Adjust the position and width of the table
\centering
\small
\begin{tabular}{ccccc}
\hline
\textbf{Model/Data}      & \multicolumn{2}{c}{\textbf{RTE}} & \multicolumn{2}{c}{\textbf{CoLA}} \\ \hline
                         & Acc             & Speed       & Acc             & Speed        \\ \hline
BERT                     & 69.3            & 1.00x       & 57.8            & 1.00x        \\
BERT-9L                  & -1.8            & 1.33x       & -2.1            & 1.33x        \\ \hline
DeeBERT                  & -2.5            & 1.47x       & -1.5            & 1.21x        \\
ElasticBERT              & -2.2            & 1.52x       & -1.2            & 1.18x        \\
FastBERT                 & -0.8            & 1.44x       & -0.2            & 1.24x        \\
PABEE                    & -1.1            & 1.62x       & -0.1            & 1.16x        \\
ZTW                      & -0.7            & 1.52x       & -0.5            & 1.48x        \\
LeeBERT                  & -0.6            & 1.60x       & -0.1            & 1.28x        \\
PALBERT                  & -0.5            & 1.32x       & -0.6            & 1.19x        \\
JEI-DNN                    & -0.2            & 1.30x       & -0.3            & 1.18x        \\
\hline
\algo{}-C                & -0.1            & 1.63x       & +0.0            & 1.30x        \\
\algo{}-A                & \textbf{+0.2}   & \textbf{1.70x} & \textbf{+0.3}  & \textbf{1.49x} \\ \hline
\end{tabular}
\caption{Results on the BERT backbone on the GLUE datasets. We report accuracy (in $\%$).}
\label{tab: results2}
\vspace{-0.5cm}
\end{table}

\subsection{Experimental setup}
Our experiments are conducted on a single NVIDIA RTX 2070 GPU, The runtimes are given below. 

\textbf{Training.} For the training phase, we augment the pre-trained BERT/ALBERT model with a linear output layer after each intermediate layer to serve as an exit point. We conduct a grid search over batch sizes of $\{8, 16, 32\}$ and learning rates \{2e-5, 3e-5, 4e-5, 5e-5\} using the Adam \citep{kingma2014adam} optimizer. 

Incorporating an early-stopping mechanism, we select the best model based on the validation set. These parameters are fixed to 16 batch size and 3e-5 learning rate for the encoder-decoder backbone. 
The training time has an average GPU runtime of around $10$ hours on a dataset, with the COCO dataset exhibiting the highest runtime ($\sim 26$ hours). 

\textbf{Inference:} Following the previous methodology on input-adaptive inference \citep{teerapittayanon2016branchynet, kaya2019shallow}, the inference is performed on a per-instance basis, setting the batch size to 1. This aligns with scenarios where low latency is critical, such as processing individual requests from different users \citep{schwartz2020right}. The reported values represent the median results from 5 runs with different seeds as small datasets such as CoLA and RTE have high variance in performance. For performing inference, the average runtime was $<20$ minutes for NLP datasets. For COCO dataset the runtime was $5$ hours on the Karpathy test split.



\textbf{Metric.} We report the speed-up ratio as a metric for measuring time reduction to remain consistent with the previous methods. Speed-up could be defined as:
$\frac{\sum_{i = 1}^L L\times n_i}{\sum_{i = 1}^L i\times n_i}$
where $n_i$ are the number of samples exiting from the $i$th layer. For the image captioning task $n_i$ is the number of words exiting from the $i$th layer. This metric could be interpreted as the increase in speed of the model as compared to the naive (AL)BERT model. This metric can be converted to expected time reduction rate.

In Table \ref{tab: results1} and \ref{tab: results2}, we present results wherein classifiers are assigned weights based on the cost of each classifier denoted as \algo{}-C and where the weights are set using the accuracy on the validation set, we denote it by \algo{}-A. 

\begin{table*}[]
\centering
\small
\begin{tabular}{cccccccc}
\hline
\textbf{Models/Metric }       & \textbf{BLEU-1}        & \textbf{BLEU-4}        & \textbf{METEOR}        & \textbf{CIDEr}          & \textbf{SPICE}         & \textbf{ROUGE-L}       & \textbf{Speedup }          \\ \hline
Final-Exit    & 82.5          & 42.3          & 32.2          & 147.1          & 26.7          & 61.3          & 1.00x              \\ 
Decoder-9L       & 76.5          & 37.1          & 29.3          & 134.8          & 23.2          & 57.9          & 1.33x          \\ \hline
DeeBERT       & 70.1          & 32.3          & 26.9          & 110.2          & 20.9          & 50.7          & 1.35x          \\
ElasticBERT       & 71.4          & 32.8          & 27.6          & 114.6          & 21.4          & 51.6          & 1.37x          \\
PABEE         & 72.7          & 33.9          & 27.9          & 115.6          & 21.9          & 52.3          & 1.30x          \\
FastBERT       & 75.0          & 35.6          & 28.2          & 119.5          & 22.1          & 53.7          & 1.42x          \\
LeeBERT       & 77.3          & 38.7          & 29.4          & 129.2          & 23.0          & 55.9          & 1.39x          \\
DeeCap        & 77.5          & 39.2          & 29.9          & 132.8          & 23.2          & 56.9          & 1.60x          \\
MuE           & 79.3          & 40.5          & 30.9          & 139.4          & 24.9          & 59.7          & 1.64x          \\ \hline
\textbf{\algo-C} & 81.8 & 41.5 & 31.7 & 145.1 & 25.9 & 60.1 & \textbf{1.71x} \\
\textbf{\algo-A} & \textbf{82.4} & \textbf{42.1} & \textbf{32.0} & \textbf{146.5} & \textbf{26.3} & \textbf{60.9} & 1.67x \\ \hline
\end{tabular}
\caption{Results showing that \algo{} outperforms the other baselines on test split of COCO dataset.}
\label{tab: results_cap}
\end{table*}

\section{Results}\label{sec: results}
In this section, we highlight and discuss the key findings of our work. Tables \ref{tab: results1} and \ref{tab: results2} present the results when ALBERT and BERT serve as the backbone models, respectively. \algo{} consistently outperforms all previous baselines by a significant margin. A major observation is a notable enhancement in \algo{} as compared to the performance of (AL)BERT models, except for a minor setback on the MNLI dataset. The improvement in accuracy by \algo{} may be attributed to the thresholds being chosen after solving the constraint optimization \ref{eq: optimize} exclusively on the validation dataset. Table \ref{tab: results_cap} shows results on the COCO dataset and observes significant improvements by using \algo{}.

The substantial accuracy drop observed in DeeBERT and ElasticBERT results from a direct comparison with entropy, neglecting the information utilized by preceding classifiers. Conversely, PABEE, LeeBERT, FastBERT, and ETFEE employ patience-based early exit criteria, posing a stringent criterion for exiting. ZTW is one of the works that weights the classifier and utilizes the ensemble techniques but suffers from poo generalization as weights are learned restricting better generalization as well as adding complexity. JEI-DNN uses the gating mechanism to decide whether to exit and does not utilize the information of multiple available classifiers. Similar is the case with DeeCAP and MuE, and for image captioning, they do not perform any knowledge distillation, further reducing the performance. These methods do not account for the confidence available at each exit, assigning them equal weight irrespective of their varying confidence level prediction. This lack of consideration impacts the adaptiveness of early exit models.

\algo{}-C and \algo{}-A are the two variants of our proposed method. In the results, we can observe that \algo{}-A consistently outperforms \algo{}-C for all the datasets in terms of accuracy and most of the datasets in terms of speed-up. This gain for \algo{}-A could be attributed to the assumption in \algo{}-C that the cost of exits (experts) was set by assuming that it is directly proportional to the accuracy but this is not true due to the overthinking issue. Still \algo{}-C performs better than previous baselines on the datasets in which the overthinking issue is minimal. The main advantage of \algo{}-C is that since the thresholds are fixed in our setup, we can still tune the cost $\lambda$ based on the speed-up (see section \ref{sec: lambda}) needed which is unavailable in \algo{}-A.


% \algo{} considers prediction consistency and observes the prediction quality as well as the quality of the employed classifier. This is achieved by considering the confidence available at each exit and appropriately weighing each classifier. This approach enhances the adaptiveness of early exit models, contributing to a more effective speed-up. Additionally, we set thresholds using classifier properties, further improving accuracy.


\section{Ablation study and Analysis}
In this section, we provide the results of our method on (AL)BERT large models.
In the Appendix, we perform an analysis of the behaviour of parameters $\alpha$ and $\lambda$ (see Appendix \ref{sec: alpha}, \ref{sec: lambda}). This analysis shows that our methods not only have better performance but also better models for the accuracy-efficiency trade-off i.e., the drop in accuracy of \algo{} was lower when speedup increases as compared to others.








% Please add the following required packages to your document preamble:
% \usepackage{multirow}




\subsection{PLM size}
\begin{wraptable}{r}{0.59\textwidth}  % Adjust the position and width of the table
\centering
\small
\begin{tabular}{ccccccc}
\hline
\textbf{Data} & \multicolumn{2}{c}{\textbf{RTE}} & \multicolumn{2}{c}{\textbf{CoLA}} & \multicolumn{2}{c}{\textbf{QQP}} \\ \hline
                       & Acc             & Spd             & Acc             & Spd              & Acc             & Spd             \\ \hline
AB-L                   & 80.5            & 1.00x           & 60.9            & 1.00x            & 91.1            & 1.00x           \\
Our-A                  & +1.8            & 2.04x           & +1.3            & 2.85x            & +0.1            & 3.33x           \\ \hline
B-L                    & 70.9            & 1.00x           & 64.3            & 1.00x            & 91.2            & 1.00x           \\
Our-A                  & +0.5            & 1.81x           & +0.9            & 1.71x            & +0.3            & 2.51x           \\ \hline
\end{tabular}
\caption{This table provides results on the large variants of (AL)BERT models compared with \algo{}-A. AB-L is ALBERT-Large and B-L is BERT-Large.}
\label{tab:large_models}
\end{wraptable}


In Table \ref{tab:large_models}, we analyze \algo{}'s performance on ALBERT-Large models, each with 24 layers.

Our results show a significant acceleration in processing speed, especially for larger models, due to their inherent overparameterization. This efficiency gain underscores \algo{}'s potential for optimizing large architectures.

Furthermore, \algo{} notably improves accuracy by mitigating overthinking, where models focus on irrelevant features. This issue is more pronounced in larger models, making \algo{} particularly effective. Our findings demonstrate that \algo{} enhances performance and speedup for large-scale transformer-based PLMs, becoming increasingly effective with larger model sizes.








\subsection{Choice of thresholds}
In table \ref{tab:fixing_thresholds}, we compare results when the thresholds are chosen based on the equation \ref{eq: optimize} and when the thresholds are set using the vanilla method i.e. best-performing on the validation set. We can observe that, there is a significant increase in the performance in both ALBERT models attributed to the choice of thresholds made by equation \ref{eq: optimize}. Observe that setting thresholds by solving the equation can improve both speedup as well as accuracy, this is as the equation finds the smallest threshold that can improve the accuracy from the final layer. The thresholds are set such that each of them perform equivalent to the final layer.





\begin{table}[]  % Adjust the position and width of the table
\centering
\small
\begin{tabular}{cccccc}
\hline
\textbf{Method}               & \textbf{}  & \multicolumn{2}{c}{\textbf{SST-2}} & \multicolumn{2}{c}{\textbf{QQP}} \\ \hline
\multicolumn{2}{c}{Our method}             & Acc             & Spd            & Acc            & Spd           \\ \hline
\multirow{2}{*}{Base}  & w/o fix & 92.4            & 1.81x             & 90.4           & 2.05x           \\
                              & w fix   & 92.6            & 1.98x             & 91.1           & 2.09x           \\ \hline
\multirow{2}{*}{Large} & w/o fix & 93.1            & 2.19x             & 91.1           & 2.95x           \\
                              & w fix   & 93.4            & 2.31x             & 91.3           & 3.33x           \\ \hline
\end{tabular}
\caption{This table compares the setting of thresholds based on the best-performing threshold on the validation dataset (w/o fix) and fixing the threshold after solving equation \ref{eq: optimize} (w fix) on ALBERT-Base/Large models.}
\label{tab:fixing_thresholds}
% \vspace{-0.19cm}
\end{table}