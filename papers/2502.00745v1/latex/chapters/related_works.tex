\section{Related work}
Early exit methods are applied for various tasks such as image classification, image captioning and NLP tasks to reduce the computational resources and inference latency. 

\noindent\textbf{Early exits in Image tasks:}
For image classification tasks, BranchyNet \citep{teerapittayanon2016branchynet} uses classification entropy at each attached exit to decide whether to infer the sample at the side branch based on the entropy of prediction.
Shallow-deep \citep{kaya2019shallow} and MSDNet \citep{huang2017multi} improve upon BranchyNet by effectively choosing the thresholds based on the confidence distribution.
Similar architectures \citep{pacheco2021calibration, dai2020epnet} split the NN to be deployed on edge and cloud. SEE \citep{wang2019see} work in service outage scenarios. FlexDNN \citep{fang2020flexdnn} and Edgent \citep{li2019edge} focus mainly on the most appropriate Neural Network (NN) depth. Other works such as Dynexit \citep{wang2019dynexit} focus on deploying the multi-exit NN in hardware. It trains and deploys the NN on Field Programmable Gate Array (FPGA) hardware while Paul \textit{et al.} \citep{kim2020low} explains that implementing a multi-exit NN on FPGA board reduces inference time and energy consumption. ZTW \citep{sun2021early} uses a combination of probability distribution to decide to exit and the combination is learned during training reducing its generalization capabilities. JEI-DNN \citep{regol2023jointly} on the other hand uses a gating mechanism for inference where which gate will be opened for a sample is learned during training.
In a parallel vein, the MuE, DeeCap and CapEEN \citep{tang2023you, fei2022deecap, bajpai2024capeen} model employ a distinctive approach to apply early exits to image captioning. DeeCap only applies to the decoder, while  MuE applies to the encoder and the decoder. CapEEN makes the exiting process more robust to noisy images.

\noindent \textbf{Early exit in PLMs:} 
Multiple approaches have been proposed to effectively apply early exits to PLMs and solve multiple NLP tasks \citep{liu2021elasticbert, ACL2020_Deebert, zhou2020bert, banino2021pondernet, balagansky2022palbert, sun2022simple, ji2023early}. DeeBERT~\citep{ACL2020_Deebert}, ElasticBERT~\citep{liu2021elasticbert} and BERxiT \citep{xin2021berxit}. BERxiT proposes an efficient fine-tuning strategy for the BERT model with attached exits. DeeBERT is obtained by training the exit points attached before the last module to the BERT backbone separately. In contrast, ElasticBERT is obtained by training all the exit points attached to the BERT backbone jointly. PABEE \citep{zhou2020bert} is another multi-exit model that makes the exit decision based on the stability of the predictions after different exits. LeeBERT \citep{zhu2021leebert} proposed a self-distillation framework that has similar exiting criteria as PABEE. ETFEE \citep{ji2023early} adds an adapter on top of the transformer layers and an (Entangled Frame) ETF classifier to make intermediate exits learn better. CeeBERT \citep{bajpai2024ceebert} and \citep{bajpai2024dadee} propose multiple methods to adapt early exits to various domains in an unsupervised setup. \cite{bajpai2023splitee, bajpai2024splitee} utilize early exits for distributed inference setup.
 

Our approach differs from past works as 1) Unlike previous studies, \algo{} utilizes the ensemble learning principles by treating each exit as an expert. 2) Our work proposes an early exiting method that utilizes each expert based on its strengths.  3) We also provide a method to set the thresholds using the error rates of the exit classifiers to perform better than the final classifiers.