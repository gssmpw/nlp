\section{Conclusion}
In conclusion, our study introduces a novel framework BEEM designed to enhance the efficiency, robustness, and adaptability of early exiting strategies in DNNs. By leveraging multiple exit classifiers, where each exit is treated as an `expert', and their outputs are combined to create an ensemble effect. Our approach considers both prediction confidence and patience, leading to improved performance and reduced latency, particularly advantageous in scenarios with strict latency requirements. Additionally, we propose a method for threshold selection, further enhancing the effectiveness of our approach. We also perform theoretical analysis to provide deep insights into our method. We experimentally validate that the speed-up observed was $1.5\times$ - $2.1\times$ for various NLP and image captioning tasks.

\section{Limitations}
While the performance of our method is better than the final layer for NLP tasks, it takes a hit for difficult tasks such as image captioning. It happens as the thresholds are being set on the validation dataset that might not generalize well on the test dataset i.e., the solution to the optimization problem \ref{eq: optimize} might not work for the test dataset. However, as our objective is to minimize the performance loss, \algo{} effectively does that and performs better than all the existing early exit models and is comparable to the final layer of DNNs with a large improvement in inference speed.


\section*{Acknowledgements}
Divya Jyoti Bajpai is supported by the Prime Ministerâ€™s Research Fellowship (PMRF), Govt. of India.  Manjesh K. Hanawal thanks funding support from SERB, Govt. of India, through the Core Research Grant (CRG/2022/008807) and MATRICS grant (MTR/2021/000645), and DST-Inria Targeted Programme. 



% \section{Reproducibility Statement}
% The experimental results can be reproduced using the link \url{https://anonymous.4open.science/r/BEEM1-639C/README.md} for the NLP tasks and for the image captioning tasks, the results could be obtained using the repository \url{https://anonymous.4open.science/r/CapEEN-3D40/README.md}.

% 
% The weights $w_i$ are very crucial parts of our method and are learned based on the accuracy of the validation dataset or cost attached with each classifier to get inference from them. However, it might happen that the validation dataset is not a very good representative of the test dataset. In these cases, the model might take minor hits in terms of accuracy as compared to the final classifier of the PLM during testing. Also, the setting of thresholds also has the assumption that the validation dataset is representative of the test dataset. However, in most real-world scenarios, the validation dataset is carefully chosen to be representative of the test dataset to ensure that the model's performance on the validation set is indicative of its performance on unseen data, hence our method fits in well for most cases.