\section{Introduction}
% Pre-trained language models (PLMs) such as ELMo \cite{peters1802deep} BERT \cite{devlin2018bert}, GPT \cite{radford2019language} XLNet \cite{yang2019xlnet}, RoBERTa \cite{liu2019roberta}, and ALBERT \cite{lan2019albert} have showcased remarkable performance across various language tasks, including text classification and natural language inference. Their effectiveness stems from pre-training on extensive corpora followed by fine-tuning on task-specific labeled datasets.
Transformer-based models \citep{ devlin2018bert, radford2019language,cornia2020meshed, luo2021dual,li2022blip, li2023blip} have set new benchmarks in performance across diverse tasks and domains through their prowess in capturing semantic information and dependencies using attention mechanisms \citep{vaswani2017attention}. However, the sheer scale and intricate structure of these models pose a challenge, particularly in terms of inference speed, limiting their practicalities in resource-constrained scenarios. Also, these models are susceptible to overthinking issues \citep{zhou2020bert, zhu2021leebert} which degrades their performance in terms of accuracy and inference speed. 

% However, they suffer from the following issues:

% \noindent
% {\bf Resource intensive:} These models require considerable computational resources characterized by millions or even billions of parameters and pose challenges related to increased memory usage and higher inference latencies. This limitation restricts their deployment in resource-constraint scenarios.

% \noindent
% {\bf Overthinking:} Existing studies \cite{fan2019reducing, michel2019sixteen, zhou2020bert} indicate that PLMs exhibit a high degree of overparameterization, and are susceptible to the `overthinking' issue \cite{kaya2019shallow}. This phenomenon suggests that, for `easy' input samples, accurate predictions can be achieved using the shallow representations from the initial layers. In contrast, the representations in the final layer may become distracted by overly complex or irrelevant features, hindering generalization. This issue not only results in a drop in accuracy but also wastes computation and restricts the overall model's ability to generalize.

To address these challenges, various techniques have been proposed, including direct network pruning \citep{zhu2017prune, fan2019reducing,michel2019sixteen}, knowledge distillation \citep{sun2019patient, sanh2019distilbert, jiao2019tinybert}, quantization methods \citep{zhang2020ternarybert, bai2020binarybert, kim2021bert}, and adaptive inference \citep{zhou2020bert, xin2020deebert, geng2021romebert, liu2020fastbert}.
%Notably, adaptive inference has gained significant attention, given that real-world datasets typically consist of samples with different difficulty levels, adaptive inference offers the advantage of processing easier samples using only the initial layers of the PLM. This approach effectively accelerates inference time while preserving performance. Moreover, adaptive inference mitigates the overthinking problem by dynamically adjusting to the input's difficulty level. 
Early Exit (EE) methods \citep{teerapittayanon2016branchynet, zhou2020bert, fei2022deecap} is one of the adaptive inference methods where intermediate classifiers (exits) are added after every layer. The difficulty of the sample is determined using confidence in the prediction, and the sample is inferred early based on the confidence score exceeding a pre-defined threshold. The confidence score becomes a crucial part of the inference process and decides the sample hardness.  


EE strategies either perform confidence-based exiting \citep{xin2020deebert} or a patience-based exiting \citep{zhou2020bert} depending on the prediction consistencies treating each classifier equally. Recently EEIC \citep{sun2021early} decided on exiting based on majority voting between the exits. This method also treats each classifier equally. These methods either consider the confidence of individual exits or utilize the predictions made by exits to define the confidence scores. For instance, in Fig~\ref{fig: Main figure}, an input sample is currently processed till the third exit; for confidence-based exiting, it checks the confidence at the third exit, ignoring all the information gathered from previous exits. The patience-based exiting requires predictions of all exits to be consistent if it wants to make an exit. %It can be a hard criterion to meet! 
Also, prediction from the first classifier is treated as important as the third, which should not be the case as deeper layers have more information. Similar is the case with majority voting, where all the classifiers are treated equally. They also do not utilize the confidence available from previous layers, thus discarding available information. 

This necessitates the requirement of an EE strategy that can utilize the information available at the exits to effectively mitigate the overthinking issue while speeding up the inference. Also, the existing methods do not offer any viewpoint or make strong assumptions, e.g., all the layers have the same error rate, which makes them less desirable. 

We present an EE mechanism named \algo{}: \underline{B}oosting Performance of \underline{E}arly \underline{E}xit DNNs
using \underline{M}ulti-Exit Classifiers as Experts motivated by ensemble learning \citep{dong2020survey},  to improve performance of EE DNNs. We treat each intermediate exit classifier as an expert that outputs confidence values on the labels for each input.  This confidence score is then weighted based on the expert's accuracy in predictions or the associated prediction cost, i.e., higher weights to deeper exits and vice versa. By treating each exit as an expert, \algo{} ensures that the model leverages the strengths of each exit and does not discard the scores of the previous layers if their predictions are in agreement. To determine if a sample can exit at the $i$th classifier, we accumulate weighted confidences of the immediate previous layers whose predictions are in agreement with the $i$th classifier. In case of disagreement with the immediate previous layer, the confidence score resets to the score of the current exit, ignoring past aggregated values. This score is subsequently compared to a predefined threshold for EE decisions. 

% Overall, \algo{}, incorporates ensemble learning principles to improve the efficiency and adaptability of early exit models.

% Crucially, our method ensures that computation performed by previous classifiers is not wasted; instead, it incorporates them into the confidence score if they exhibit consistency in prediction 
% with previous classifiers. The core idea revolves around harnessing the confidence scores of exits that may lack sufficient confidence for a standalone prediction but can enhance the confidence of subsequent exits when predictions align consistently (See Fig.~\ref{fig: Main figure} for visualization).  

The exit decision at each layer is based on a cumulative confidence score exceeding a threshold value. The thresholds play a pivotal role as they offer a means to model the trade-off between accuracy and latency. In Section \ref{sec: thresholds}, we introduce a novel approach to determine the threshold values for different exits by converting the problem of choosing thresholds to a simple linear program. We utilize the error rate of exits to set the threshold values, forcing the exit classifier to perform better than the final classifier of DNNs. In Section \ref{sec: theore_analy}, we also perform a theoretical analysis to derive a condition based on the error rate of the intermediary layer under which \algo{} performs better than the vanilla DNN inference.

We experiment with widely adopted Pre-Trained Language Models (PLMs) and encoder-decoder models to perform experiments on GLUE \citep{wang2019glue} and COCO dataset \citep{lin2014microsoft}. We show that \algo{} outperforms all the previous EE methods in terms of speed as well as accuracy. \algo{} increases the inference speed by $1.5\times$ - $2.1\times$ with accuracy close to the final layer. For easier NLP tasks such as sentiment analysis, \algo{} even outperforms the final layer in terms of accuracy.

\begin{figure*}
    \centering
    \includegraphics[scale = 0.43]{latex/figures/Expert_based_exiting.pdf}
    \caption{Comparison between (a) DeeBERT, which uses the confidence available at each exit as the metric or deciding early inference (set to 0.9), (b) PABEE, which uses the consistency in prediction as the confidence metric (set to 2) and (c) \algo{} that uses the weighted confidence $S_i$ (weights = $[0.1, 0.2, \ldots, 1.2]$) and threshold $\alpha = 0.2$. In \algo{}, by appropriately considering information from previous classifiers, a correct prediction is made early which was not the case with others.}
    \label{fig: Main figure}
    \vspace{-0.39cm}
\end{figure*}

\indent
In summary, our contributions are as follows:
\begin{itemize}
    \item We propose new criteria to make EE decisions in DNNs. It combines the confidence score of the intermediary exit classifier to produce an ensemble effect to make PLMs more efficient, robust, and adaptive.

    \item We provide a method to set threshold values in BEEM by analyzing the error rates of exit classifiers (Section \ref{sec: thresholds}). This not only helps \algo{} achieve better speed-up but also improves accuracy compared to inference at the last layer. We also derive a condition under which this performance is guaranteed. 

    \item Extensive evaluation showed speed-up improvement in both GLUE and COCO datasets. For the GLUE dataset, accuracy also improves with speedup due to a reduction in the overthinking issues of the DNNs.

    
\end{itemize}