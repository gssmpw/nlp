% \newpage
\section{Appendix}
\label{sec:appendix}
\subsection{Proof of Theorem \ref{thm:theorem 1}}\label{sec: proof}
\textbf{Proof.} For simplicity, we prove it for the binary classification case. 
For the samples that are not inferred at intermediate exits, the misclassification probability will remain the same with or without \algo{}. Therefore, we only need to consider the case when the sample exits early and misclassified. We denote the probability that the sample exits early from exit $t$ as $p_t^{{stop}}$ and the probability that the exiting sample is misclassified as $p_t^{{misc}}$. We next look for conditions under which $\frac{p_t^{misc}}{p_t^{stop}}<p$, i.e., the fraction of samples early exiting being misclassified is less the error rate of the final classifier.  

Let us consider two random variables $X_t$ and $Y_t$ where $X_t=1$ if the sample exits at $t$th classifier else 0 and $Y_t = 1$ if the prediction at $t$th classifier is correct else 0. Now the probability that a sample exits at the $t$th classifier could be written as:
\begin{equation}
P(X_t = 1) = P(S_1<\alpha, \ldots S_{t-1}<\alpha, S_t\geq\alpha)    
\end{equation}

Let $q_t^{min} = \min\{q_1, q_2, \ldots, q_{t-1}\}$ and $q_t^{max} = \{q_1, q_2, \ldots, q_{t-1}\}$. We denote the prediction of classifier $t$ as $\hat{y}_t$. Note that $P(y^{*}=\hat{y}_t)$ = $1-q_t$. We define a counter $Y_t$ as:
\begin{equation}
\label{eq: Reward1}
    \mathbf{c}_t = \left\{
        \begin{array}{ll}
             1 & \textit{if} \quad \hat{y}_{t-1} = \hat{y}_t \\
             0 & \textit{if} \quad \hat{y}_{t-1} \neq \hat{y}_t
        \end{array}
    \right.
\end{equation}


We define $\mathbf{C}_t=\sum_{i=1}^{t} \mathbf{c}_i$. Note that $\mathbf{C}_t$ monitors the number of times the prediction has been changed till the $t$th classifier. 


\begin{multline}
p_t^{{stop}} = P(X_t = 1) =\\ \sum_{c=0}^{t}P(X_t = 1|\mathbf{C}_t= c)P(\mathbf{C}_t=c)
\end{multline}

%First, we consider only the probability that the model consecutively outputs the same prediction from the first exit classifier. 
We denote $P(X_t = 1|\mathbf{C}_t = c)$ as $A_{t}^{c}$. We have 
$$p_t^{stop} = P(X_t=1)= \sum_{c = 0}^{t}A_t^{c}P(\mathbf{C}_t=c).$$

By Total Probability law, we can say that
\begin{multline}
P(X_t= 1) = P(X_t = 1|mc)P(mc)\\+P(X_t = 1|cc)P(cc)
\end{multline}

where $mc$ is misclassified and $cc$ is correctly classified. The ratio is now:

\begin{multline}
    \frac{p_t^{misc}}{p_t^{stop}} =\\ \frac{P(X_t=1|mc)P(mc)}{P(X_t=1|mc)P(mc)+P(X_t  = 1|cc)P(cc)}<p
\end{multline}

Simplifying this, we have,

$$\frac{P(X_t = 1|cc).P(cc)}{P(X_t = 1|mc).P(mc)}>\frac{1}{p}-1$$

For $P(X_t = 1|mc)$, from the definition of the confidence score \ref{eq: Reward1}, we observe that the highest probability of exiting at $t$th layer with misclassification is if all the previous predictions are consistent, i.e.,  $S_t$ will be highest when all the previous exits misclassified. Hence we have 

$$P(X_t = 1|mc)< tA_t^{0}q_1q_2\ldots q_t<tA_t^{0}(q_t^{max})^{t-1}q_t$$

For $P(X_t = 1|cc)$, we observe that again by definition of the confidence score, we can lower bound it, since the lowest probability of exiting with correct classification at $t$th classifier will be when all the previous classifiers had a misclassification and then at $t$th classifier the prediction reversed. Hence, we have that

\begin{multline}
P(X_t = 1|cc)>\\ tA_t^{1}q_1q_2\ldots (1-q_t)>tA_t^{1}(q_t^{min})^{t-1}(1-q_t)
\end{multline}

Now we have inequality as 

$$\frac{tA_t^{1}q_{min}^{t-1}(1-q_t)}{tA_t^{0}q_{max}^{t-1}q_t}>\frac{1}{p}-1$$

We get the desired result by simplifying the above inequality for $q_t$. We denote the constant term $\frac{A_t^1}{A_t^{0}}$ as $a_t$. Hence, we finally show with $b_t = \frac{q_t^{max}}{q_t^{min}}$:

\begin{equation}
    q_t<\frac{a_t}{a_t+((1/p-1)b_t^{t-1})}
\end{equation}

where $a_t$ and $b_t$ are constant for each $t$.

This concludes the proof.


\subsection{Analysis of thresholds $\alpha$}\label{sec: alpha}
In table \ref{tab:fixing_thresholds}, we compare \algo{}-A where thresholds are set by optimizing Eq~ \ref{eq: optimize} against the case where threshold values are chosen using the validation dataset and are constant for all exits. 
We provide the comparison on (AL)BERT-base as well as large models. We can observe a significant difference in accuracy and speedup. The improvement in accuracy and speedup is explained by the formulation of the optimization problem in Sec. \ref{sec: thresholds}.



Also, in Fig~ \ref{fig:alpha trade-off}, we plot the accuracy-speedup trade-off curves. We can observe that the change rate of decrease of the accuracy of \algo{} by increasing speed-up is smaller as compared to previous baselines. This extra stability by \algo{} could be attributed to its characteristic of confirming the predictions from multiple exits (experts). Note that Fig~ \ref{fig:alpha trade-off} does not assume the thresholds are fixed and varies them to show the stability of the approach.


\subsection{Analysis of cost $\lambda$ and \# parameters}\label{sec: lambda}
In \algo{}-C, we introduce weights representing costs associated with utilizing exits. Figure \ref{fig:lambda_trade-off} illustrates how altering these costs influences accuracy and speed-up trade-offs. As the cost attributed to each exit classifier increases, we observe a slight decline in accuracy accompanied by a more significant enhancement in speed-up. This hyperparameter thus offers a mechanism for modeling the trade-off between accuracy and speed-up, particularly as the thresholds $\alpha$ remain fixed.
It's worth noting that adjusting the value of $\lambda$ impacts the quantity $q_{\alpha^t}$ defined in Section \ref{sec: thresholds}. Consequently, modifications to $\lambda$ may induce changes in the values of $\alpha^t$. 

Note that BERT-base/Large has 110/340 Million Parameters with exits and ALBERT-base/Large has 13/19 Million parameters with exits.

\begin{figure*}
    \centering
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{latex/figures/acc_compare.pdf}
        % \caption{BLEU-1 score for COCO with model BLIP-2-ViT-L-OPT\textsubscript{2.7B}}.
        \caption{Accuracy-speedup trade-off for $\alpha$.}
        \label{fig:alpha trade-off}
    \end{subfigure}
    \begin{subfigure}{0.53\textwidth}
        \includegraphics[width=\textwidth]{latex/figures/Expert_cost_lambda.pdf}
        % \caption{VQA accuracy on the VQAv2 dataset with BLIP-2-ViT-L-FlanT5\textsubscript{XL}}
        \caption{Trade-off by varying $\lambda$.}
        \label{fig:lambda_trade-off}
    \end{subfigure}
    % \caption{Left: Speed and accuracy trade-off by varying hyperparameter $\alpha$. Right: Accuracy Speedup by varying the cost parameter $\lambda$}
    % \label{fig: diff_cmp_and_speed}
\end{figure*}


% Then it can be rewritten as:

% $\sum_{c = 0}^t A_t^c P(C_t=c) q_t+\sum_{c = 0}^t A_t^c P(C_t=c) (1-q_t)$.

% According to the defined score $S_i$, we observe that the lowest chances of exiting will be when the prediction for a sample is consistent since $(t-1)$th layer and then changes at the $t$th. This will be the smallest as the probability of flipping scores will be smallest at the $t$th layer while others are consistent. 

% \noindent Hence, we can write that:

% \noindent $P(X_t=1, misc)<t A_t^{1} (1-q_t)^{t-1}q_t$

% \noindent Similarly,

% \noindent $P(X_t=1, corr)<t A_t^{1} (q_t)^{t-1}(1-q_t)$

% \noindent Finally we have,

% \noindent $p_{stop}\leq\sum_{c=0}^{t} P(C_t=c) t A_t^1((1-q_t)^{t-1}q_t+(q_t)^{t-1}(1-q_t))$

% \noindent Now we have to upper bound the $p_{misc}$, we can write as 

% \noindent 
% $p_{misc} = P(X_t=1, misc)$

% Now if there is a misclassification at the final layer, and based on the definition of the score, we observe that the most likely chances for misclassification will be when all the previous layers have misclassified the sample.

% \noindent Hence, $p_{misc}$ can be upper bounded by

%  \noindent $p_{misc} < t A_t^{0} \sum_{c=0}^{t} P(C_t=c) q_t^t$.

% Finally, we can replace the bounds in the numerator and denominator, hence the inequality looks like this:

% $\frac{A_t^{0}q_t^{t}}{A_t^{1}(1-q_t)^{t-1}q_t+A_t^{1}(q_t)^{t-1}(1-q_t)}<p$








% We next lower bound $p_{{stop}}$ by considering only the event $C_t=0$, i.e., the model consecutively outputs the same prediction
% %Since $\mathbf{C}_t = 0$ in these cases, we get the lower bound on $p_{stop}$ as:

% $$p_{stop} \geq A_t^{0}P(\mathbf{C}_t = 0)$$

% We can 

% The event $C_t=0$ can occur when all the exits make the same prediction, irrespective of whether the prediction is correct or not. Hence we have  $P(\mathbf{C}_t = 0) = q_1q_2\cdots q_t+(1-q_1) \cdots (1-q_t)$.

% $$p_{stop}>A_{t}^{0}(q_{min}^{t}+(1-q_{max})^{t})$$ 

% \textcolor{red}{$$p_{stop}>A_{t}^{0}q_{t}(q_{m}^{t-1}+(1-q_{M})^{t-1})$$}

% Then, we need to find an upper bound on $p_{misc}$, we know that 

% $$p_{misc} = P(X_t = 1,miss)$$
% $$p_{misc} = \sum_{c=1}^{t}P(X_t = 1, mis| C_t = c)P(C_t = c)$$
% $$=\sum_{c=1}^{t}P(X_t = 1| C_t = c)P(mis| C_t = c, X_t=1)P(C_t = c)$$
% $$\leq A_t^0\sum_{c=1}^{t}P(mis| C_t = c, X_t=1)P(C_t = c)$$

% if  more of the immediate previous layers make a consistent prediction, the accumulated confidence will be higher and the probability of exit is also higher, i.e., 
% $P(X_t = 1|\mathbf{C}_t = c)$ is decreasing in $c$. Since there is a misclassification at the $t$th classifier, and consistency in predictions which makes the probability  $P(\mathbf{C}_t = 0) = q_1q_2\cdots q_t$. The final upper bound could be written as:

% % \textcolor{red}{$$p_{misc}<A_t^{0}q_t\sum_{c = 0}^{t-1}q_{max}^{i}(1-q_{min})^{t-1-i}$$}


% $$p_{misc}<A_{t}^{0}tq_1q_2\cdots q_t$$

% \textcolor{red}{$$p_{misc}<A_{t}^{0}q_t^(t)q_{max}^{t-1}$$}

% Finally, we can get the desired condition by solving the below inequality:
% $$\frac{A_{t}^{0}tq^{t}}{A_{t}^{0}q^{t}+A_{t}^{0}(1-q)^{t}}<p$$

% \textcolor{red}{$$\frac{A_{t}^{0}(t-1)q_{max}^{t-1}}{A_{t}^{0}q_{min}^{t-1}+A_{t}^{0}(1-q_{max})^{t-1}}<p$$}

% By solving this, we get the desired condition as $q< \frac{1}{(1+(1/p-1))^{1/t}}$.



