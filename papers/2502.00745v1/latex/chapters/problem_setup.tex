\section{Problem setup}
We start with a pre-trained DNN and attach exit classifiers after each layer. We provide details on training the PLMs for language tasks and the encoder-decoder backbone for image captioning.

\subsection{PLMs}
\textbf{Training:} \algo{} requires the training of exit classifiers that provide predictions based on their respective layer outputs. Let $\mathcal{D}$ denote the dataset distribution with the label class $\mathcal{C}$ employed for backbone training. Given an input sample $(x, y) \sim \mathcal{D}$, the loss for $i$th exit is calculated as:
\begin{equation}
    \mathcal{L}_i(\theta) = \mathcal{L}_{CE}(f_i(x, \theta), y) +KL(p_i, p_L)
\end{equation}
where $f_i(x, \theta)$ represents the output of the exit attached at the $i$-th layer, $\theta$ denotes the set of all learnable parameters, $\mathcal{L}_{CE}$ denotes the cross-entropy loss and $KL$ is the KL-divergence loss used to additionally train the exits with soft labels from the final layer. KL divergence (Kullback-Leibler divergence) is used in knowledge distillation because it measures how well one probability distribution (the student model's predictions) approximates another (the teacher model's predictions). In the context of knowledge distillation, KL divergence serves as a key component to transfer "soft knowledge" from the teacher to the student. Let $p_i = \mathcal{P}_i(c|x)$ denote the probability distribution over the set of output classes at the $i$th layer, where $\mathcal{P}_i(c|x)$ denotes the estimated probability that $x$ belongs to class $c$. We simultaneously optimize parameters for all exit classifiers, following the methodology proposed by \cite{kaya2019shallow}. The loss function is defined as $\mathcal{L} = \frac{\sum_{i = 1}^{L} i\mathcal{L}_i}{\sum_{i = 1}^{L} i}$, considering the weighted average to account for the relative inference cost of each exit classifier where $L$ denotes the number of layers in the model. Note that the importance of KL-divergence loss is well-explained in \cite{zhu2021leebert}. Following this training, the model is ready for inference.

\textbf{Inference:} We illustrate the inference process of \algo{} in Fig.~\ref{fig: Main figure}.  As the input instance $x$ goes through layers $1, 2, \ldots L$ sequentially, the classifier attached to that layer predicts a class label distribution. For $i$th exit classifier, let $C_i$ denote the confidence in the estimate at the $i$th exit. We define $C_i$ as the maximum of the estimated probability class, i.e., $C_i:=\max_{c\in \mathcal{C}}{\mathcal{P}}_i(c|x)$. We denote $\hat{y}_i = \arg \max_{c\in \mathcal{C}}{\mathcal{P}}_i(c|x)$, the prediction of $i$th exit. Based on the confidence scores we define a weighted confidence score, denoted $S_i$ as:
\begin{equation}
\label{eq: Reward}
    S_i = \left\{
        \begin{array}{ll}
            S_{i-1}+w_i C_i & \textit{if} \quad \hat{y}_{i-1} = \hat{y}_i \\
            w_i C_i & \textit{if} \quad \hat{y}_{i-1} \neq \hat{y}_i
        \end{array}
    \right.
\end{equation}
The inference process halts when $S_i \geq \alpha$, where $\alpha$ represents a predefined threshold, and exits with label $\hat{y}_i$. Otherwise, the sample is processed in the next layer and the process completes. If this condition is never met at any exit classifiers, a label is assigned by the classifier at the final layer. This allows a sample to exit the backbone early if the condition is satisfied, avoiding traversal through all layers.



\subsection{Encoder-Decoder Models}
\textbf{Encoder-decoder model:} For the image captioning task where the objective is to generate a caption for an input image, we start with a pre-trained encoder and decoder model. We use the Swin Transformer \citep{liu2021swin} as an encoder and GPT-2 \citep{radford2019language} as a decoder. We attach exits to the decoder of the backbone. The backbone is trained using cross entropy and KL-divergence loss where loss for $i$th exit could be written as:
\begin{eqnarray*}
    \mathcal{L}_i(\theta) = \sum_{t=1}^{T}(\mathcal{L}_{CE}(f_i(x, \theta, y_{1:t-1}), y_t) +KL(p_t^i, p_t^L)),
\end{eqnarray*}
where $\theta$ is the collection of all the parameters, $x$ is the input image, $T$ is the caption length, $y_{1:T}$ is the ground-truth caption. $p_t^i$ is the probability vector on the vocabulary $\mathcal{V}$ for $i$th exit. Its $v$th component could be written as $p_t^i(v) = \mathcal{P}_i(v|y_{1:t-1}, x; \theta)$ where $\mathcal{P}_i$ is the probability distribution output over $\mathcal{V}$ by the $i$th exit. Note that $L$ here is the number of layers in the decoder. Similarly, we define $p_t^L$ as the probability vector for the final decoder layer. The overall loss across all the exits is the same as for the PLM training.

\textbf{Caption inference:} We predict the caption in an autoregressive manner. This entails making a token-by-token prediction for a given image. In this case, the confidence could be formulated as $C_i = \max_{v\in \mathcal{V}}\mathcal{P}_i(v|\hat{y}_{1:t-1}, x; \theta)$ where $\mathcal{P}_i$ is same as defined above and $\hat{y}_{1:t-1}$ is the predicted caption till $(t-1)$th word. A token will be predicted at which exit is decided by equation \ref{eq: Reward}. For an input image $x$, we start the caption with begin of the sentence token and then the inference process stops after the end of the sentence token is predicted. 

In Figure \ref{fig: Main figure}, confidence-based early exit methods like DeeBERT and ElasticBERT, relying on softmax scores, tend to be overly confident toward a single class and classifier. Such methods also face the consequences of ignoring information obtained from previous classifiers as they progress to the next one. This limitation is addressed by patience-based approaches like PABEE, which decide to exit when predictions show consistency across multiple classifiers. However, patience-based methods treat each classifier equally and underutilize valuable information available in terms of prediction scores, which affects the adaptability of the model.

Contrarily, \algo{} captures the confidence available at each exit and assigns weights to each classifier based on its accuracy or cost. It takes into account the consistency in predictions by reducing the score to the current classifier's weighted confidence when predictions are inconsistent. This unique approach in \algo{} incorporates both patience and confidence to make predictions. Note that the confidence score $S_i$ given in equation \ref{eq: Reward} can predict hard samples early as if the predictions of initial classifiers are consistent but with low confidence, the summed-up $S_i$ score makes them exit early as shown in Figure \ref{fig: Main figure}. Also, it effectively mitigates errors arising from a single classifier while considering the confidence in predictions and weighing them based on their performance.

\subsection{Assigning weights}
In this section, we provide methods to set the weights for the exits.

\noindent 1) \textbf{Cost vector:} First, we consider weights as the cost of getting inference from the exit classifier where the cost could be in the form of $w_i =  \lambda i$ where $\lambda$ is the processing cost of the one exit and since the layers are identical, the cost is a multiple of $\lambda$ for deeper layers. 

\noindent 2) \textbf{Accuracy:} We can also consider the weights as the accuracy of each classifier. The accuracy could be calculated on a validation dataset. This will provide weights to exits depending on how much accurate a particular expert (exit) is. 

Note that the major difference between the existing methods is the cost-based weights have a task to reduce the overall cost while sacrificing some accuracy while the accuracy-based methods will focus more on accuracy. Note that using accuracy-based weights can also improve the efficiency that comes because of overthinking issues. As in the accuracy-based, we know the true capability of each exit.

% The main difference between the two methods is if we take weights as the cost then each exit will get uniform weights in increasing order while if we take the accuracy even deeper classifiers might get lower weight than the initial ones as accuracy might be lower for deeper classifiers due to overthinking in some cases.

% \subsection{Theoretical analysis}
% It could be observed that \algo{} can reduce the inference latency. The question is then left under what conditions, it can also improve accuracy. We conduct theoretical analysis on the model's accuracy with and without \algo{} and come up with simplified conditions under which \algo{} can perform better than the vanilla (AL)BERT models. For simplicity, we consider the binary classification tasks and conclude that:

% \begin{theorem}\label{thm:theorem 1}
% Consider an early exit PLM with $L$ layers. Let $p$ denote the error rate of the final classifier and the error rate of intermediate exit classifiers be \textcolor{red}{atmost?} $q$ such that $q<\frac{1}{(1+(1/p-1))^{1/t}}$ holds for all exit layers $t=1,2,\ldots, L-1$. Then, the error rate of \algo{} is better than $p$.
% \end{theorem}
% \noindent The proof can be found in the Appendix \ref{}.

% Observe that this theorem gives an upper bound on the error rates of intermediate exits. If the above condition is satisfied for a model and a dataset then we can guarantee that \algo{} will improve upon the vanilla PLM.
% Also, note that the above condition could be easily satisfied. For instance, if $t=2$, and $p=0.1$, then the final condition will be $q<0.33$ for the $2$nd exit classifier, which is satisfied for most of the datasets.
% Even though we have assumed the error rate of all the exits to be the same (as assumed in previous methods PABEE and MSDNet) to get the simplified condition, observe that the condition established in the theorem above itself suggests that the upper bounds for the error rates can be relaxed for deeper exits.

\subsection{Choice of thresholds $\alpha$}\label{sec: thresholds}
We can choose the threshold values in two ways, one way is to choose the best-performing threshold on the validation set, and the other is based on forcing error rates to be smaller than the error rate of the final classifier.

\noindent 1) \textbf{Classical method:} We choose the search space for threshold $\alpha\in S =  \{0.3, 0.6, 0.9, 1.2, 1.5\}$. The values of $w_i\in [0,1]$, $C_i\in [0,1]$ imply that $S_i \leq L$ i.e. the score at any exit layer $i$ cannot be greater than the number of layers $L$ as $S_i$ is a multiple of two values between $0$ and $1$, it becomes very small and is added almost $L$ times. We choose the best-performing threshold on the validation set in terms of accuracy.

\noindent 2) \textbf{Using error rates:}
% If we place the final classifier's accuracy and the exit classifier's location, we can find out the exact upper bound on the error rate of an exit classifier. The above analysis is made on the error rates of the internal classifiers and it is suggested that if we can bound the error rates below a certain threshold, then we can improve upon the accuracy of the final classifier. However, we can not increase or decrease the error rates of the classifiers. We need to have an alternative metric that can be tuned according to the bound. We use the false positive rate i.e., the number of samples that exited due to threshold $\alpha$ and misclassified can be used as a substitute for the error rate and can be used to set the thresholds such that the above condition holds in a softer sense. 
Let us consider that $c_{misc}^t$ represents the number of samples that exit at $t$th classifier with a misclassification, $c_{stop}^t$ represents the number of samples that exit at the $t$th classifier. Note the $c_{stop} = \sum_{j=1}^{n}\mathbbm{1}_{\{C_j\geq\alpha\}}$ can also be considered as the coverage of the $t$th classifier and $c_{misc}^t = \frac{\sum_{j =1}^{n}\mathbbm{1}_{\{\hat{y}_j \neq y|C_j\geq\alpha\}}}{c_{stop}}$, where $n$ is the total number of samples in the dataset. $p$ is the error rate of the final classifier, then we observe that our algorithm will perform better than final layer if $c_{misc}^t/c_{stop}^t<p$ for every exit classifier $t$. The above condition tells that the fraction of the samples that have exited and misclassified (i.e., error rate) at $t$th classifier should be less than the error rate of the final classifier. If the above condition is satisfied for all the exits then we are guaranteed that \algo{} can outperform the final classifier of the PLM. The objective is to maximize speedup while satisfying the above condition.  

Note that the error rate depends on the threshold $\alpha$, a higher value of the threshold will lower the error rate as then samples with higher confidence will exit reducing the chance of misclassification.
Observe that, we can find the threshold value $\alpha_t$ for the $t$th classifier such that the condition $c_{misc}^t/c_{stop}^t<p$ is satisfied on the validation set.
We define $q_{\alpha_{t}}$ as the error rate associated with the threshold $\alpha_t$ for the $t$th classifier. We can set the threshold by solving the optimization problem.
\begin{equation}\label{eq: optimize}
\begin{aligned}
& \underset{\alpha_t\in S}{\text{minimize}}
& & \alpha_t \\
& \text{subject to}
& & q_{\alpha_t} \leq p,
\end{aligned}
\end{equation}
where the set $S = \{0.5, 1, \ldots, 5, L\}$ is the search space for the thresholds. Note that we have added $L$ in the search space so that the problem always remains feasible. By solving Eq. \ref{eq: optimize}, our method finds the optimal threshold that has maximum speedup while performing better than the final layer. Also, observe that the above problem has small computational complexity as the minimization is over a very small finite set. 

\subsection{Theoretical analysis}\label{sec: theore_analy}
\begin{theorem}\label{thm:theorem 1}
Consider an early exit PLM with $L$ layers. Let $p$ denote the error rate of the final classifier and the error probability of $i$th exit classifiers be $q_i$ such that $q_i<\frac{a_i}{a_i+((1/p-1)b_i^{i-1})}$ holds for all exit layers $i=1,2,\ldots, L-1$ where $a_i$ and $b_i$ are constants for a given exit $i$. Then, the error probability of \algo{} is better than $p$ i.e., it performs better than the final layer.
\end{theorem}

The proof of the theorem is given in the Appendix \ref{sec: proof}. Note that the above theorem proves the general condition for better performance of \algo{} and does not depend on the threshold values $\alpha$. $a_i$ denotes the ratio of the probability of exiting with one change in prediction to the probability of exiting with zero changes in prediction till $i$th classifier and $b_i = \frac{q_i^{max}}{q_i^{min}}$ where $q_i^{max} = \max\{q_1, \ldots, q_i\}$ while $q_i^{min} = \min\{q_1, \ldots, q_i\}$. Observe that as we move deeper into the backbone the bound becomes tighter which makes sense as deeper layers are more likely to be accurate. Also, the bound is inversely proportional to the error rate of the final layer, if the error rate of the final layer is smaller, then the bound gets tighter. Previous method \cite{zhou2020bert} had a very strong assumption while providing a similar condition for their method, it assumed that all the classifiers have the same error rate which is not true. If we impose the same condition the bound simplifies to $q_i<\frac{a_i}{a_i+((1/p-1))}$ which is a more simplified and stronger bound than PABEE \citep{zhou2020bert}.


