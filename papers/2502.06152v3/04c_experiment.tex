 \mvspace{-2mm}
\section{Experiment III: Information-based Local Explanation on Recidivism Prediction}
\label{exp3}
 \mvspace{-2mm}

We apply our framework to a recidivism prediction task, where the decision-maker decides whether to release a defendant.
We demonstrate using the framework to augment SHAP.


\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/explanation_arxiv.png}
    \mvspace{-6mm}
    \caption{(A) Summary of SHAP explanation and (B) summary of ILIV-SHAP explanation, where each point represents an instance from the test set. (C-F) Waterfall plots of applying SHAP and ILIV-SHAP on instance 4 and instance 18. The Waterfall plot starts from the prior expected prediction (or ILIV of the prior expected prediction), and then adds features one at a time until reaching the current model output (or ILIV of the current model output). See further comparisons between SHAP and ILIV-SHAP in \Cref{app:iliv-shap}.}
    \mvspace{-4mm}
    \label{fig:explanation}
\end{figure*}
 \mvspace{-2mm}
\subsection{Data and Model}
 \mvspace{-2mm}
We use the COMPAS dataset~\citep{angwin2022machine}, which contains 11,758 defendants with associated features capturing demographics, information about their current charges, and their prior criminal history.
We merge the dataset with the experimental data from \citet{lin2020limits} to obtain human decisions, in the form of recidivism decisions on a subset of instances from \citet{angwin2022machine}'s experiment on laypeople. 
Human decisions were elicited on a 30-point probability scale.
Merging the two datasets produces 9,269 instances (defendants).
For the AI model, we trained an extreme gradient boosting (XGBoost) model~\citep{chen2016xgboost} on a training set with 6,488 instances, achieving an AUC of $0.84$ on a hold-out test set with 2,781 instances. 
We round the model predictions to the same 30-point probability scale available to study participants in \citet{lin2020limits}.


We use the Brier score as the payoff function, which is commonly used in previous studies on recidivism~\citep{green2019disparate, lin2020limits, fogliato2021impact}.
The binary payoff-related state $\payoffstatevalue$ indicates whether the defendant gets rearrested within two years.
We use the features of the defendant contained in the COMPAS dataset as the signals in our demonstration: demographic features (age, sex, and race), information about the current
charge (type of offense and whether it is a misdemeanor or a felony), prior criminal history (e.g., past number of arrests and charges for several offense categories), and the predicted score from the COMPAS system\footnote{\url{https://doc.wi.gov/Pages/AboutDOC/COMPAS.aspx}}.
We take the empirical distribution of the hold-out test set as the data-generating process, which defines the joint distribution of state, signals, human decisions, and AI predictions.
\mvspace{-4mm}
% \paragraph{Constructing signals.}
%We first construct the signals that can be used to calculate the information value in the realization of the AI prediction on an instance.
% Denote the XGBoost model's predictions as a random variable $Y^{pred} = f(X)$, where $X$ denotes the random variable of data features and $f(\cdot)$ denotes the model's predictive function.
% We discretize the model predictions into $20$ bins by breaks: $\{\hat{y}^{pred}_0 = 0, \hat{y}^{pred}_1 = 0.05, \ldots, \hat{y}^{pred}_{20} = 0.95, \hat{y}^{pred}_{21} = 1\}$, where any $f(x)$ fulfilling $\hat{y}^{pred}_{i} \leq f(x) < \hat{y}^{pred}_{i + 1}$ will be labeled by bin id $i$.
% We construct $\localsig_{f(x)}$ as $\mathbb{1}[Y^{pred} = f(x)]$ for every $x$ and use it to calculate the realization-indicated agent-complementary information value ($\ILIV$) of $Y^{pred}$ on instances $x$ (\Cref{def:iliv}).
% We generate the information-based explanations by ILIV-SHAP in this demonstration.

% We use SHAP~\citep{lundberg2017unified} (one of the most successful saliency-based explanations \jessica{this should be said when SHAP is mentioned in definitions rather than here}) to construct the local explanation in our demonstration. 
% Instead of inputing the model prediction $f(x)$, we use $ILIV^{\dgp, S}(f(x); \actionvar^{\text{Human}})$ as the objective function of SHAP.
% The orignal SHAP (which takes as input a model prediction) captures the marginal contribution of each feature to the model prediction. Taking the $\ILIV$ as input instead, SHAP outputs the marginal contribution of each feature to the total possible human-complementary information value of the model prediction.%, indicating information that the human lacks but AI has.
% Intuitively, when a feature makes a large relative contribution to the information value of the model prediction, it means that the model is extracting information from that feature (or other features with equivalent information to it).
% Otherwise, when a feature makes a small relative contribution, it means that it makes the model ignore information in other features that has higher information value for human decision-makers.
% \jessica{I don't think the model loses information. maybe you mean 'Having access to that feature }


%\begin{figure}
%    \centering
%    \includegraphics[width=\linewidth]{figures/explanation_icml_2025_jh.png}
%    \vspace{-7mm}
%    \caption{[placeholder]
%    }
 %   \label{fig:explanation_aggregate}
%\end{figure}

 \mvspace{-3mm}
\subsection{Results}
 \mvspace{-2mm}
\paragraph{SHAP explanations do not communicate the complementary information of AI predictions.}
Specifically, the changes in information value of AI predictions caused by marginalizing out features, i.e., ILIV-SHAP scores, are not predictable from the changes in AI predictions, i.e., SHAP scores, and the feature values.
\Cref{fig:explanation} (A) and (B) compare the distribution of feature importance scores from SHAP to those from ILIV-SHAP.
The scores of \texttt{prior\_counts} (the number of prior criminal charges), \texttt{age}, and \texttt{decile\_score} (the score from the COMPAS system), which are three features with the largest scores in both SHAP and ILIV-SHAP, show different correlations with the feature values in SHAP and ILIV-SHAP.
For instance, in \Cref{fig:explanation}(A), most of the low values (blue dots) of \texttt{prior\_counts} have a negative SHAP score, and most of the high values (red dots) have a positive SHAP score.
This indicates that when the value of \texttt{prior\_counts} is low, marginalizing out \texttt{prior\_counts} from the features leads to a higher expected model prediction than not marginalizing out, and vice versa.
However, this correlation does not hold for ILIV-SHAP.
In \Cref{fig:explanation}(B), the high values of \texttt{prior\_counts} appear on both the left side (below zero) and the right side (above zero), indicating having low ILIV-SHAP scores and high ILIV-SHAP scores at the same time.
This mismatch between the distributions of SHAP scores and ILIV-SHAP scores implies that the impact of features on the information value cannot be directly inferred (at least monotonically) from their impact on model output and their values.
% shows that the change of human-complementary information value in model prediction caused by not marginalizing out \textit{prior counts} does not necessarily result in more information value of model prediction than marginalizing out \textit{prior counts}.
% indicating that the changes in AI predictions caused by the low values of \textit{prior counts} contribute decision-relevant information that human decisions lack, while 
% Though SHAP highlights the features that are important for the AI prediction, ILIV-SHAP can directly indicate whether the changes caused by features can lead to more informative AI predictions. 


% We also find that even though some features correlate strongly with AI predictions, they do not necessarily contribute more decision-relevant information to AI predictions. \jessica{more than what?}
% For example, in the second and third rows in \Cref{fig:explanation}(A), \textit{age} of the defendants and \textit{decile score} predicted by the COMPAS both correlate with \jessica{the} SHAP value. \st{in a clear way.}
% The blue dots \jessica{in the age plot} indicate young defendants, and most of them \jessica{most of the young defendants? doesn't make sense} lead to a positive change in AI predictions, while decile scores work oppositely.
% In contrast, \Cref{fig:explanation}(B) reveals that neither age nor decile score correlates strongly with ILIV-SHAP values.
%Therefore, a saliency-based explanation alone may overlook features that, while not drastically changing the AIâ€™s estimate, hold important complementary information for humans.

 \mvspace{-1mm}
\paragraph{ILIV-SHAP can augment SHAP on communicating the complementary information of AI predictions.}
We argue that ILIV-SHAP can supplement saliency-based explanations to provide users with a sense of how the AI predictions can help improve their decisions.
With ILIV-SHAP, users can check whether the prediction contains decision-relevant information by looking at the total $\ILIV$ of the AI prediction, which indicates the maximum improvement in expected payoff that could be achieved by incorporating this prediction. 
\Cref{fig:explanation}(D) and (F) show two different cases, where the AI predictions contain substantial human-complementary information value in (D) (shown by the distance between the tail and the head of the Waterfall plot) but very little information in (F).


ILIV-SHAP also tells the user whether an AI model's prediction encodes useful complementary information from a particular feature.
In \Cref{fig:explanation}(E), the defendant's prior count record (\texttt{prior\_counts=1}) ranks second among features that impact the AI prediction ($-0.139$).
Upon seeing this, users might be interested in investigating how the AI uses \texttt{prior\_counts=1}.
However, \Cref{fig:explanation}(F) shows that \texttt{prior\_counts=1} contributes marginally to the decision-relevant information in AI predictions (about $+0.007$).
Thus, with ILIV-SHAP, users can see that even if \texttt{prior\_counts=1} changes the AI prediction significantly, the AI does not predict more accurately as a result of it.
