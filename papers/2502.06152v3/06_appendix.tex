\newpage
\appendix
\onecolumn

\section{The Combinatorial Nature of the Value of Signals}
\label{app:shapley}

We model the information value of a single signal over the existing information in agent decisions.
When decision-makers are provided with multiple signals, they might use them in combination.
Therefore, our definition of information value in \Cref{def:aciv} may overlook the signals' value in combination with other signals.
Signals can be complemented~\citep{chen2016informational}, i.e, they contain no information value by themselves but a considerable value when combined with other signals.
For example, two signals $\basicsig_1$ and $\basicsig_2$ ight be uniformly random bits and the state $\payoffstatevalue=\basicsig_1 \oplus \basicsig_2$, the XOR of $\basicsig_1$ and
$\basicsig_2$.
In this case, neither of the signals offers information value on its own, but knowing both leads to the maximum payoff.
To consider this complementation between signals, we use the Shapley value $\shapleyval$ \citep{shapley1953value} to interpret the contribution to information gain of each basic signal.
The Shapley value calculates the average of the marginal contribution of a basic signal $\basicsigsp_i$ in every combination of signals.
\begin{equation}
\label{eq:shapley_infogain}
    \shapleyval%^{\actionvar^b}
    (\basicsig_i) = \frac{1}{n} \sum_{\sig \subseteq \{\sig_1, \ldots, \sig_n\} / \{\basicsig_i\}} {(n - 1)\choose |\sig|}^{-1} (\ACIV(\sig \cup \{\basicsig_i\}; \actionvar^b) - ACIV(\sig; \actionvar^b))
\end{equation}
The Shapley value suggests how much information value of the basic signal is unexploited by the human decision-maker on average in all combinations.

The following algorithm provides a polynomial-time approximation of the Shapley value of $\ACIV$. Under the assumption of submodularity, it orders the signals the same as the Shapley value.

% \textbf{Algorithm 1}
        
%         \textbf{Input}: $\dgp^b$, $\score$, $\{\basicsig_1, \ldots, \basicsig_n\}$, $\actionvar^b$

%         \textbf{Output}: $\{\phi_1, \ldots, \phi_n\}$ for unexploited information value in $\{\basicsig_1, \ldots, \basicsig_n\}$
        
        \begin{algorithm}[H]
        \caption{Greedy algorithm for marginal gain of $\ACIV$}\label{alg:cap}
        \begin{algorithmic}[1]
        \STATE $V^* = \{\actionvar^b\}$
        \STATE $\Phi^* = \{\}$
        \FOR{$i=1$ to $n$}
        \STATE $\phi'_j = \ACIV(\basicsig_j; V^*) \ for \ each \ j$
        \STATE $j^* = \arg \max_{j \ s.t. \ \basicsig_j \notin V^*} \phi'_j$
        \STATE $\phi_{j^*} = \max_{j \ s.t. \ \basicsig_j \notin V^*} \phi'_j$
        \STATE $add \ \basicsig_{j^*} \ to \ V^*$
        \STATE $add \ \phi_{j^*} \ to \ \Phi^*$
        \ENDFOR
        \STATE $output \ \phi_{j^*}$
        % \RETURN $\{\phi_1, \ldots, \phi_n\}$
        \end{algorithmic}
        \label{alg:seq}
        \end{algorithm}
\newpage

\section{Embeddings for High-dimensional Signals in Decision Problem}
\label{app:high-dimensional}

Suppose the data-generating process is estimated from a set of observations $\{(\payoffstatevalue_i, \sigval_i)\}_{i=1}^T$, i.e., 
\begin{equation}
\label{eq:hatdgp}
    \hat{\dgp}(\payoffstatevalue, \sigval) = \frac{1}{T} \sum_{i=1}^{T} \mathbb{1}_{\payoffstatevalue = \payoffstatevalue_i} \cdot \mathbb{1}_{\sigval = \sigval_i}
\end{equation}

In this case, the sample size $T$ that is required to get a good estimate of DGP is the exponential of the number of dimensions of the signal $\sigval$.
The high-dimensional signals, such as images and text, that are usual in human-AI decision problems are not computationally feasible to be quantified under our framework.
We propose a systematic procedure to pick the optimal reduction model to represent the high-dimensional signal space in a low-dimensional embedding space.

The optimal reduction model and its generated embeddings should fulfill two properties.
First, the generated signals should be as informative as possible for the decision task, such that they maintain the maximum information value contained in the original signals.
Second, the derived DGP that is implied by the embedding signals should be feasible to be estimated from the observed dataset.
We quantify the above objectives by a framework based on Train/Test split.

\begin{algorithm}[H]
\caption{Embedding high-dimensional signals in decision problems}\label{alg:cap}
\begin{algorithmic}[1]
\STATE Input: Observed dataset $\{(\payoffstatevalue_i, \sigval_i)\}_{i=1}^T$, Candidate model generator $C$.
\STATE $R^* = 0$
\STATE Split the dataset into a training set and a testing set.
\REPEAT
\STATE Generate a candidate model $c$ from $C$.
\STATE Generate embedding $\sigval^c_i = c(\sigval_i)$ on each data point of both sets.
\STATE Estimate the DGP $\hat{\dgp}$ implied by the new signals $\sigval^c_i$ by \Cref{eq:hatdgp}.
\STATE Calculate rational decision rule: \\
\quad \quad $d^r(\sigval^c) = \arg\max_{\action \in \actionspace}\expect[\payoffstatevalue \sim \dgp(\payoffstatevalue | \sigval^c)]{\score(\action, \payoffstatevalue)}$.
\STATE Calculate training performance $R_{training} = \expect[(\sigval^c, \payoffstatevalue) \sim training\_set]{\score(d^r(\sigval^c), \payoffstatevalue)}$.
\STATE Calculate testing performance $R_{testing} = \expect[(\sigval^c, \payoffstatevalue) \sim testing\_set]{\score(d^r(\sigval^c), \payoffstatevalue)}$.
\IF{$R_{training} - R_{testing} > \tau$}
\STATE go to step 4.
\ENDIF
\IF{$R^* < R_{training}$}
\STATE $c^* = c$.
\STATE $R^* = R_{training}$.
\ENDIF
\UNTIL{no more new candidate model in $C$ or $R^*$ converages}

\STATE Output: $c^*$
% \RETURN $\{\phi_1, \ldots, \phi_n\}$
\end{algorithmic}
\label{alg:seq}
\end{algorithm}

The threshold $\tau$ represents the tolerance in the exchangeability between the signal-state tuples in the training set and the testing set. 
When the candidate model space is infinite, the optimal reduction model can still be found if the candidate space is a lattice that is defined by the informativeness of their generated embeddings, e.g., a class of embedding models with different parameter p in the dropout layer.

\newpage

\section{Robustness Analysis in Experiment I}

\begin{figure*}[!h]
    \centering
    \includegraphics[width=\linewidth]{figures/robust_analysis.png}
    \caption{Robust analysis for experiment I on all V-shaped payoff functions. The kink $\mu$ is shown on the \textit{x}-axis. Each subplot displays the difference between the $\ACIV$ on the row model over human decisions and the $\ACIV$ on the column model over human decisions. A positive value (colored in blue) at $\kink$ indicates the model on the row contains more informative than the model on the column under the evaluation of V-shaped scoring rule with kink $\kink$. The subplots are symmetric along the diagonal, e.g., $(1,2)$ subplot and $(2, 1)$ subplot display the same distribution with opposite signs.}
    \label{fig:robust-analysis}
\end{figure*}
\newpage
\section{More examples of ILIV-SHAP}
\label{app:iliv-shap}


\begin{figure}[h!]
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/explanation_images/Model_instance0.pdf}
         \caption{SHAP on instance 0}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/explanation_images/InfoModel_instance0.pdf}
         \caption{ILIV-SHAP on instance 0}
     \end{subfigure}
\end{figure}


\begin{figure}[h!]
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/explanation_images/Model_instance1.pdf}
         \caption{SHAP on instance 1}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/explanation_images/InfoModel_instance1.pdf}
         \caption{ILIV-SHAP on instance 1}
     \end{subfigure}
\end{figure}


\begin{figure}[h!]
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/explanation_images/Model_instance2.pdf}
         \caption{SHAP on instance 2}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/explanation_images/InfoModel_instance2.pdf}
         \caption{ILIV-SHAP on instance 2}
     \end{subfigure}
\end{figure}




% \begin{figure}[h!]
% \centering     %%% not \center
% \subfigure[SHAP on instance 1]{\includegraphics[width=0.4\linewidth]{figures/explanation_images/Model_instance1.pdf}}
% \subfigure[ILIV-SHAP on instance 1]{\includegraphics[width=0.4\linewidth]{figures/explanation_images/InfoModel_instance1.pdf}}
% \end{figure}


% \begin{figure}[h!]
% \centering     %%% not \center
% \subfigure[SHAP on instance 2]{\includegraphics[width=0.4\linewidth]{figures/explanation_images/Model_instance2.pdf}}
% \subfigure[ILIV-SHAP on instance 2]{\includegraphics[width=0.4\linewidth]{figures/explanation_images/InfoModel_instance2.pdf}}
% \end{figure}



% \begin{figure}[h!]
% \centering     %%% not \center
% \subfigure[SHAP on instance 3]{\includegraphics[width=0.4\linewidth]{figures/explanation_images/Model_instance3.pdf}}
% \subfigure[ILIV-SHAP on instance 3]{\includegraphics[width=0.4\linewidth]{figures/explanation_images/InfoModel_instance3.pdf}}
% \end{figure}


% \begin{figure}[h!]
% \centering     %%% not \center
% \subfigure[SHAP on instance 4]{\includegraphics[width=0.4\linewidth]{figures/explanation_images/Model_instance4.pdf}}
% \subfigure[ILIV-SHAP on instance 4]{\includegraphics[width=0.4\linewidth]{figures/explanation_images/InfoModel_instance4.pdf}}
% \end{figure}


% \begin{figure}[h!]
% \centering     %%% not \center
% \subfigure[SHAP on instance 5]{\includegraphics[width=0.4\linewidth]{figures/explanation_images/Model_instance5.pdf}}
% \subfigure[ILIV-SHAP on instance 5]{\includegraphics[width=0.4\linewidth]{figures/explanation_images/InfoModel_instance5.pdf}}
% \end{figure}
