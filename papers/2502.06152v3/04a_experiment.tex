
 \mvspace{-2mm}
\section{Experiment II: Behavioral Analysis on Deepfake Detection}
\label{exp1}
 \mvspace{-2mm}
We apply our framework to analyze a deepfake video detection task~\citep{dolhansky2020deepfake}, where participants are asked to judge whether a video was created by generative AI, including with the assistance of an AI model.

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/groh_et_al.png}
    \mvspace{-7mm}
    \caption{Information value calculated under our framework in the information model defined by the experiment of \citet{groh2022deepfake}. Basic signals include the seven video level features and three types of agent decisions. The baseline on the left represents the expected payoff given no information, i.e., $\mathrm{R}^{\dgp, \score}(\emptyset)$, and the benchmark on the right represents the expected payoff given all available information, i.e., $\mathrm{R}^{\dgp, \score}(\{\basicsig_1,\ldots,\basicsig_n, \actionvar^{\text{Human}}, \actionvar^{\text{Human-AI}}, \actionvar^{\text{AI}}\})$. All the payoffs are calculated by $\mathrm{R}^{\dgp, \score}(\cdot)$, where $\cdot$ is the signal on the \textit{y}-axis.
    % The normalization factor is taken as the information gain of all signals, i.e., $Z^{\sig_1, \ldots, \sig_n} = \shapleyval(\{\sig_1, \ldots, \sig_n\})^{-1}$. \jessica{can we describe the scale somewhere, like does the axis limit mark the maximum possible attainable information gain?} \jessica{Information is spelled wrong in the labels above each plot}
    }
    \mvspace{-6mm}
    \label{fig:analysis_groh}
\end{figure}
 \mvspace{-2mm}
\subsection{Data and Model}
  \mvspace{-2mm}
We define the information model on the experiment data of \citet{groh2022deepfake}. 
Non-expert participants (n=5,524) were recruited through Prolific and asked to examine the videos. 
They reported their decisions in two rounds.
They first reviewed the video and reported an \textit{initial} decision ($\actionvar^{\text{Human}}$) without access to the AI model. Then, in a second round, they were provided with the recommendation ($\actionvar^{\text{AI}}$) of a multitask cascaded convolutional neural network~\citep{zhang2016joint}, with estimated $65\%$ accuracy on holdout data, and chose whether to change their initial decision. 
This produced a \textit{final} decision ($\actionvar^{\text{Human-AI}}$).
Both decisions were elicited as a percentage indicating how confident the participant was that the video was a deepfake, measured in $1\%$ increments: $\action \in \{0\%, 1\%, \ldots, 100\%\}$.
We round the predictions from the AI model to the same 100-scale probability scale available to study participants.

We use the Brier score as the payoff function, 
with the binary payoff-related state: $\payoffstatevalue\in \{0,1\} = \{\text{genuine}, \text{fake}\}$.
This choice differs from the mean absolute error used by \citet{groh2022deepfake}, but again we use the quadratic score because it is a proper scoring rule where truthfully reporting the belief maximizes the score.

% Under this payoff function, participants achieved a payoff of $0.73$ and the AI achieved a payoff of $0.85$. \jessica{Previous sentence belongs in results}


We identify a set of features that were implicitly available to all three agents (human, AI, and human-AI). Because the video signal is high dimensional, we make use of seven video-level features using manually coded labels by \citet{groh2022deepfake}: graininess, blurriness, darkness, presence of a flickering face, presence of two people, presence of a floating distraction, and the presence of an individual with dark skin, all of which are labeled as binary indictors.
These are the basic signals in our framework. 
We estimate the data-generating process $\dgp$ using the realizations of signals, state, first-round human decisions, AI predictions, and second-round human-AI decisions.
The no-information bound is the same as \Cref{exp2} while the full-information bound is defined as the expected payoff of a rational DM who has access to all video-level features and three agents' decisions.


 \mvspace{-2mm}
\subsection{Results}
 \mvspace{-2mm}

\paragraph{How much decision-relevant information do agents' decisions offer?}
We first compare the information value of the AI predictions to that of the human decisions in the first round (without AI assistance).
\Cref{fig:analysis_groh}(a) shows that \textbf{\textcolor{aidecision}{AI predictions}} provide about $65\%$ of the total possible information value over the no-information baseline, while \textbf{\textcolor{humandecision}{human decisions}} only provide about $15\%$.
Because the no information baseline, $0.75$, is equivalent to a random guess drawn from $\text{Bernoulli}(0.5)$, human decisions are only weakly informative for the problem.


We next consider the \textbf{\textcolor{humanaidecision}{human-AI decisions}}. %observe that the human-AI team decisions still leave a lot of room for improvement compared to the information value in AI predictions.
Given that the AI predictions contain a significant portion of the total possible information value, we might hope that when participants have access to the AI predictions, their performance will be close to the full information baseline.
However, the information value of the \textbf{\textcolor{humanaidecision}{human-AI decisions}} only achieves a small proportion of the total possible information value ($30\%$). % compared to the information value contained in AI predictions ($65\%$ full information value).
This is consistent with the findings of \citet{guo2024decision} that humans are bad at distinguishing when AI predictions are correct.


\paragraph{How much additional decision-relevant information do the available features offer over agents' decisions?}
To understand what information might improve human decisions, we assess the $\ACIV$s of different signals over different agents. This describes the additional information value in the signal after conditioning on the existing information in the agents' decisions.
As shown on the fifth row in \Cref{fig:analysis_groh}, the presence of a flickering face offers larger $\ACIV$ over human decisions than over AI predictions, meaning that human decisions could improve by a greater amount if they were to incorporate this information. 
Meanwhile, as shown on the fourth row in \Cref{fig:analysis_groh}, the presence of an individual with dark skin offers larger $\ACIV$ over AI predictions than over human decisions, suggesting that humans make greater use of this information.
This suggests that the AI and human rely on differing information to make their initial predictions, where the AI relies more on information associated with the presence of a flickering face while human participants rely more on information associated with the presence of an individual with dark skin.
% However, because the information model we studied contained only seven video-level features, the suggestions we can give for potentially improving human decisions are limited.
% For example, there is no single signal that can be combined with human decisions to get a comparable payoff to AI predictions in these seven video-level features. \jessica{do you  mean no single signal?}
% \jessica{talk about features over human-AI decisions here}

By comparing the $\ACIV$s of different signals over human decisions and human-AI decisions, we also find that simply displaying AI predictions to humans did not lead to the AI-assisted humans exploiting the observed signals in their decisions.
As shown in \Cref{fig:analysis_groh}, with the assistance of AI, the $\ACIV$s of all signals over the human-AI teams' decisions do not change much compared to the $\ACIV$s over human decisions, with the exception of a slight improvement in the presence of a flickering face.
This finding further confirms the hypothesis that humans are simply relying on AI predictions without processing the information contained in them.
% (note that the $\ACIV$ over human decisions shows significant difference on same signal from the $\ACIV$ over AI predictions \jessica{i dont understand why this parenthetical is here, how does it belong in this sentence})


% We first compare how participants without AI assistance and the AI use information in the deepfake detection task.
% Specifically, we calculate the Shapley value of information gain, $\shapleyval^{\actionvar^H}(i)$ for participants and $\shapleyval^{\actionvar^{AI}}(i)$ for AI, as shown in \Cref{eq:shapley_infogain}, for each video-level feature $\sig_i$.
% % We normalize by the Shapley value of information gain from all signals into $[0, 1]$ (i.e., $Z^{\sig_1, \ldots, \sig_n} = \shapleyval(\{\sig_1, \ldots, \sig_n\})^{-1}$).
% The information gain over behavioral decisions reflects the information value of signals that are not fully redundant with the information in the behavioral decisions.

% First, we observe that, in general, the information gain of the features over the AI decisions are lower than those for human decisions. 
% There are several exceptions, such as the presence of an individual with dark skin.
% This suggests that, overall, the information in features is better exploited by AI than by participants. 
% More specifically, we find that the AI uses certain features much more effectively than participants. 
% For instance, the presence of a flickering face offers the least information gain over AI decisions among all the features, whereas it is the feature that offers the largest information gain over human decisions.
% This suggests that one way to improve the current human-AI performance is to help the participants better exploit the information that AI exploits well but participants did not.
% % For example, an explanation could be used to illustrate the AI's decision rule on the flickering face signal to help human better exploit that. \jessica{what would this look like? Can we necessarily do this?}
% Second, we find that the AI relies on less sensitive information compared to participants. 
% For example, AI uses the presence of an individual with dark skin the least among all features, while for participants it is the second most important feature.

% \vspace{-2mm}
% \paragraph{Unaided human v.s. human-AI team.}
% We assess the information gain after participants are presented with AI recommendations in the deepfake detection task. 
% We calculate the Shapley value of information gain $\shapleyval^{\actionvar^{HAI}}(i)$ for human participants with AI recommendations relative to without. 
% % In the context of our approach, this entails treating the AI recommendation as an additional signal, alongside the seven video-level features.
% We find that simply displaying the AI's predictions to participants does not necessarily help them better exploit the potential value of information that they exploited poorly without access to the AI. 
% For example, even though the information gain of the presence of a flickering face is reduced when presenting participants with AI predictions relative to without, the AI’s much smaller gain for the signal implies participants could still use it much more effectively.
% This suggests that better interventions (e.g.,\ explanations for AI predictions) may be needed to help people better incorporate some signals. 
% % First, we observe that although participants do not make full use of the total possible information provided by AI recommendations, 
% % \jessica{have we set up what the max attainable info gain is? i assume you say this because they could be getting more gain, but is 1 the limit?} 
% % presenting the AI recommendation improves human use of information contained in video-level features that the AI uses. 
% % As shown in \Cref{fig:analysis_groh}, among features the information gain from AI recommendations is the largest, indicating a large potential gap to improve the use of the AI recommendation.
% % Notably, we see a dramatic drop in the information gain of the "flickering face" feature compared to human decisions made without AI recommendation, which implies that human makes better use of the information in the "flickering face" feature when AI is presented.
% Second, for the signals that the AI does not exploit well, offering the AI predictions does not necessarily reduce participants' usage of that information.
% For example, for the signal denoting the presence of an individual with dark skin, we did not see a significant improvement on the information gain over human-AI decisions compared to the gain over human decisions.
% Both of these findings suggest that simply displaying AI predictions may not change people's usage of information and improve their decision quality.
% Other interventions (such as\ explanations) should be explored to improve the use of potentially valuable by humans in human-AI collaborations. 

