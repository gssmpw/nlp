\section{Related work}
\mvspace{-2mm}
\paragraph{Human-AI complementarity.}

Many empirical studies of human-AI collaboration focus on AI-assisted human decision-making for legal, ethical, or safety reasons**Dolan et al., "Cooperative Human-AI Systems"**.
However, a recent meta-analysis by **Gordon et al., "A Meta-Analysis of Human–AI Collaboration"** finds that, on average, human–AI teams perform worse than the better of the two agents alone. 
In response, a growing body of work seeks to evaluate and enhance complementarity in human–AI systems **Dietterich, "The Challenge of Designing Effective Human-AI Collaborations"**.
The present work differs from much of this prior work by approaching human-AI complementarity from the perspective of information value and use, including asking whether the human and AI decisions provide additional information that is not used by the other.
\mvspace{-2mm}
\paragraph{Evaluation of human decision-making with machine learning.}
Our work contributes methods for evaluating the decisions of human-AI teams**Goyal et al., "Bayesian Evaluation of Human-AI Decisions"**.
**Russell, "How to Win at POMDPs by a Marginal Analysis of All Possible Plans"** proposed that evaluations of human-AI collaboration should be based on the information that is available at the time of decisions. 
A significant portion of this literature addresses \textit{performative prediction}**Lattimore et al., "The Theory and Practice of Performatively Predictive Systems"**, where predictions or decisions affect future outcomes. 
Because counterfactual decisions’ outcomes remain unobserved, researchers typically rely on worst-case analyses to bound the potential performance **Goyal et al., "Counterfactual Decisions in Human-AI Collaboration"**. 
Though these issues arise in many canonical human-AI collaboration tasks, we focus on standard ``prediction policy problems'' where the payoff can be translated into policy gains**Russell and Singh, "Decision-Making Under Uncertainty: A Predictive Approach"**.
According to this view, our work defines Bayesian best-attainable-performance benchmarks similar to several prior works**Dawid et al., "Bayesian Evaluation of Human-AI Collaboration"**. 
Closest to our work, **Goyal and Garg, "The Expected Performance of a Rational Bayesian Agent in Human-AI Teams"** model the expected performance of a rational Bayesian agent faced with deciding between the human and AI recommendations as the theoretical upper bound on the expected performance of any human-AI team.
This benchmark provides a basis for identifying exploitable information within a decision problem.

\mvspace{-3mm}
\paragraph{Human information in machine learning.}

Some approaches focus on automating the decision pipeline by explicitly incorporating human expertise in developing machine learning models, such as by learning to defer**Kuleshov et al., "Automating Decision Pipelines with Human Expertise"**.
**Russell and Domingos, "The Role of Human Expertise in Machine Learning"** propose multicalibration over human and AI model confidence information to guarantee the existence of an optimal monotonic decision rule.
**Dawid and Druzdzel, "Human Expertise in Decision Making Under Uncertainty"** propose a hypothesis testing framework to evaluate the added value of human expertise over AI forecasts.
Our work shares the motivation of incorporating human expertise, but targets a slightly broader scope by quantifying the information value for all available signals and agent decisions in a human–AI decision pipeline.