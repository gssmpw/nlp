\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
% \usepackage{neurips_2024}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
    \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx} 
\usepackage{mathbbol}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{soul}
\usepackage{enumitem}

\newcommand{\jessica}[1]{\textcolor{red}{Jessica: #1}}
\newcommand{\ziyang}[1]{\textcolor{blue}{Ziyang: #1}}
\newcommand{\yifan}[1]{\textcolor{purple}{[Yifan: #1]}}
\newcommand{\jason}[1]{\textcolor{orange}{[Jason: #1]}}

\title{Unexploited Information Value in Human-AI Collaboration}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Ziyang Guo \\
  Department of Computer Science\\
  Northwetsern University\\
  Evanston, IL, 60208 \\
  \texttt{ziyang.guo@northwestern.edu} \\
  % examples of more authors
  \And
  Yifan Wu \\
  Department of Computer Science \\
  Northwetsern University\\
  Evanston, IL, 60208 \\
  \texttt{yifan.wu@u.northwestern.edu} \\
  \And
  Jason Hartline \\
  Department of Computer Science \\
  Northwetsern University\\
  Evanston, IL, 60208 \\
  \texttt{hartline@northwestern.edu} \\
  \And
  Jessica Hullman \\
  Department of Computer Science \\
  Northwetsern University\\
  Evanston, IL, 60208 \\
  \texttt{jhullman@northwestern.edu} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\input{notation}


\begin{document}


\maketitle


\begin{abstract}

Humans and AIs are often paired on decision tasks with the expectation of achieving \textit{complementary performance} -- where the combination of human and AI outperforms either one alone. 
However, how to improve performance of a human-AI team is often not clear without knowing more about what particular information and strategies each agent employs. 
In this paper, we propose a model based in statistical decision theory to analyze human-AI collaboration from the perspective of what information could be used to improve a human or AI decision. 
We demonstrate our model on a deepfake detection task to investigate seven video-level features by their unexploited value of information.
We compare the human alone, AI alone and human-AI team and offer insights on how the AI assistance impacts people's usage of the information and what information that the AI exploits well might be useful for improving human decisions.

\end{abstract}


\section{Introduction}

As the performance of artificial intelligence (AI) models makes remarkable advances, workflows in which humans and AIs collaborate have been sought for important decisions in medicine, finance, and other domains. 
Designing for human involvement is critical.
While an AI model can usually make predictions with higher accuracy than the average human when the two use similar information~\citep{aegisdottir2006meta,grove2000clinical,meehl1954clinical}, in some cases, a human must retain final control over the decision for liability reasons.
When humans have access to additional information over the AI, there is the potential for a human-AI collaboration to achieve \textit{complementary performance}, i.e., better performance than either the human or AI alone.
% \jessica{are we restricting our discussion here to model-assisted human decisions? I can't tell, but we say 'the human with the AI can perform better' - why not say 'combining the input of both DMs can perform better'?}
For example, a physician may have access to additional information that may not be captured in tabular electronic health records or other structured data \citep{alur2024distinguishing}.
Others argue that human theory-based causal logic can contribute knowledge that AI data-based predictions can not learn from historic data \citep{felintheory}.

However, evidence supporting complementary performance between humans and AI is limited, with many studies showing that human-AI teams often underperform AI alone in tasks~\citep{buccinca2020proxy, bussone2015role, green2019principles, jacobs2021machine, lai2019human, vaccaro2019effects, kononenko2001machine}.
Thus, numerous empirical studies attempt to explore design strategies and conditions under which complementary human-AI performance can be achieved.
For example, some find that complementary performance is more likely to be obtained when the AI and human have comparable ability~\citep{bansal2021does}.
Other studies focus on improving the workflow~\citep{buccinca2021trust, fogliato2021impact} and information display~\citep{bussone2015role, fok2023search}.
% such as cognitive forcing functions~\cite{buccinca2021trust}, explanations~\cite{bussone2015role}, and out-of-distribution data points~\cite{liu2021understanding}.
% While complementary performance is more likely to be seen when the AI and human have comparable ability~\cite{bansal2021does}, it remains unclear what factors lead to complementary performance in real-world decision-making tasks. \jessica{last part is kind of vague ... what do you mean 'what factors'. Maybe something like 'remains unclear what information each type of agent brings to the interaction and how this information is combined'}
% Numerous empirical studies attempt to explore the conditions that might improve human-AI collaboration, such as cognitive forcing functions~\cite{buccinca2021trust}, explanations~\cite{bussone2015role}, and out-of-distribution data points~\cite{liu2021understanding}. \jessica{how does OOD data improve collaboration? maybe you mean to say something like 'explore design strategiess and conditions under which complementary human-AI performance can be achieved' Or you could say under which human-AI performance is better, but I still don't see how they can do better when data is OOD versus IID}


Most analyses of human behavior in human-AI collaboration to date focus on the \textit{performance} of human-AI teams or each individually, without considering the potential for available \textit{information} to improve the decisions.
However, knowing how much more effectively decision-relevant information might be \textit{used} by both agents paves the way to opportunities to improve human use of available information, such as through the design of new explanations or human-AI workflows.
A decision-theoretic conceptual framework by \cite{guo2024decision} upper bounds the possible performance of an AI-human team using the expected score of a rational Bayesian agent faced with deciding between human and AI recommendations. 
This approach provides a basis for identifying informational ``opportunities'' within a decision problem.

We propose an informativeness analysis framework based on decision theory, which looks into the value of information provided by a predictive model to decision-makers.
We also find that such informativeness analysis can be used to answer the fundamental questions that motivate human-AI collaboration: how can we improve human-AI complementarity?
Based on decision theory, our framework offers a new perspective on studying human-AI collaboration: how much additional value an AI model offers to a decision task compared to the ``existing'' information?
By ``existing'' information, we mean the information that is already displayed to decision makers or even revealed in the outcome decisions.
Consider the case of deploying a computer vision model to help doctors diagnose heart abnormality from radiology images for following tests or treatments (e.g., \citet{tang2020automated}).
While the predicted risk being calibrated is important, how much information the predictions provide to doctors also matters to improve the quality of their diagnosis.
The doctors might already have accessible information, such as reports from a radiologist or the patient's medical history, before having AI predictions, so it is also important to ask whether the AI extracts ``complementary'' information from radiology images.

We demonstrate our framework on three decision-making tasks where the AI models are well-developed as assistants to human decision maker: deepfake detection~\citep{dolhansky2020deepfake, groh2022deepfake}, chest X-raying diagnosis~\citep{rajpurkar2018deep, johnson2019mimic} and sentiment analysis~\citep{ni2019justifying, hartmann2023}.
We show different usage of our framework along the pipeline of human-AI collaboration workflow, from offering new epistemic insights to giving actionable suggestions on human-AI collaboration pipeline.
In the first demonstration, we empirically compare the ``unexploited'' information value in each signals by human and AI.
We reveal ...
In the second demonstration, we show another application of the information analysis framework, to compare the complementary information offered by each model.
We find that while the models have similar accuracy, some models actually domiate the others on informativeness, i.e., they offer more complementary information than the others under the evaluation of every proper scoring rule.
In the last demonstration, we try to answer a harder question: how to design the signals shown to human in human-AI decision making?
We show that ...

% In this paper, we present a method for identifying unexploited information value by a human and an AI in a human-AI collaboration.
% We use the notion of information gain (the marginal value that one piece of information provides over another) to capture the overlap between the information contained in human decisions (or AI predictions) and  
% the contextual information available to the human. 
% % \jessica{since our analysis depends on comparing between human and AI use, maybe we want to mention AI here too}
% % This allows us to separately investigate the use of different pieces of displayed information without needing to repeatedly gather new observations of human behavior.
% A larger information gain identifies a higher unexploited value of information that a contextual signal offers which might improve decisions.
% We use the Shapley value \citep{shapley1953value} to quantify the contribution of each basic element of information to the overall information value contained in the human decisions.
% We demonstrate our methodology on a deepfake video detection task \citep{groh2022deepfake}.
% Through a comaprision between the information gain over human decisions and AI predictions, we find that participants failed to make use of the considerable information value of some signals that the AI exploited effectively, highlighting the potential for improvement in human decisions.
% We also find that simply displaying the AI predictions did not necessarily help participants improve on their usage of the information, which suggests the need for further improvements such as explanations of the AI's decision rule on those unexploited signals.


\section{Model Setup}
%\jessica{Edit if needed: Information can be considered valuable \textit{for a decision problem} to the extent that it is expected to improve the payoff of an idealized agent on the task.}
Information can be considered valuable to a decision-maker to the extent that it is possible in theory to incorporate it in their decisions to improve performance.
% \jessica{that sentence is hard to parse... how does information gain value in improving decisions? Do you mean something like 'Information can be considered valuable to a decision maker to the extent that they incorporate it in their decisions?' (But that might imply that we can say with our method what info they incorporate, and im not sure we can claim that)} 
Our approach analyzes the expected marginal payoff gain from ideal use of additional information over ideal use of the existing information in human decisions in decision tasks. 
In this section, we define the model and notations for this approach, including a decision problem and associated information structure, following prior decision-theoretic frameworks for studying decisions from statistical information~\citep{wu2023rational,guo2024decision,hullman2024decision}.
Then we define how a rational decision maker would act given a signal and a decision problem with an associated information structure.
Using the rational decision maker as a tool, we show how to investigate the information encoded in behavioral decisions.
%\jessica{May want to add a sentence or two here to give the reader some intuition for our approach. E.g., Our approach relies on analysis of the marginal gain ... }

%\ziyang{Merge infomration structure and decision-making problem into one section}

\paragraph{Decision Problem} A decision problem consists of three key elements, a payoff-relevant state, a decision, and a payoff function. We illustrate with an example of a weather decision. 
\begin{itemize}[wide]
\vspace{-2mm}
    \item A payoff-relevant state $\payoffstatevalue$ from a space $\payoffstatespace$. For example,\ $\payoffstatevalue \in \payoffstatespace =  \{0, 1\} = \{\text{no rain}, \text{rain}\}$.
    \item A decision $\action$ from the decision space $\actionspace$ characterizing the decision-maker (DM)'s choice. For example,\ $\action\in \actionspace = \{0, 1\} = \{\text{not take umbrella}, \text{take umbrella}\}$.
    \item The payoff function $\score: \actionspace\times\payoffstatespace\to\mathbb{R}$, used to assess the quality of a decision given a realization of the state, e.g., $\score(\action = 0, \payoffstatevalue = 0) = 0, \score(\action = 0, \payoffstatevalue = 1) = -100, \score(\action = 1, \payoffstatevalue = 0) = -50, \score(\action = 1, \payoffstatevalue = 1) = 0$, which punishes the DM for selecting an action that does not match the weather. 
\end{itemize}

\paragraph{Information Model} 
We cast the information available to a DM as a signal defined within an information structure. %We define the data-generating process, the distribution over signals and states.
We use the definition of an information structure in \cite{blackwell1951comparison}. 
The information structure has two elements:
\begin{itemize}[wide]
\vspace{-2mm}
    \item \textit{Signals}. There are $n$ ``basic signals'' represented as random variables $\basicsig_1, \ldots, \basicsig_n$, from the signal spaces $\basicsigsp_1, \ldots, \basicsigsp_n$. These represent information obtained by a decision-maker, e.g., $\basicsig_1 = \{\text{cloudy}, \text{not cloudy}\}$, $\basicsig_2\in \{0, \ldots, 100\}$ for temprature Celsius, etc. 
    % We write $k_i = |\basicsigsp_i|$ as the size of the signal space of the basic signal $i$, $\basicsig_i$ as the random variable for basic signal $i$, and $\basicsigval_{ij_i}\in \basicsigsp_i$ as the $j_i$th realized value of the $\basicsig_i$ ($j_i\leq k_i$).

    
    % E.g.\ observable features about the weather $\{\sig_1, \sig_2, \ldots\} = \{\text{temperature}, \text{cloud level}, \dots\}$. 
    % In addition to the basic signals, there are also other signals that \st{intuitively} represent the combination of basic signals.
    The decision maker observes a signal, which is a combination of the basic signals, represented as a set $\sig \subseteq 2^{\{\basicsig_1, \dots, \basicsig_n\}}$. For example,\ a signal $\sig = \{\basicsig_1, \basicsig_2\}$ observed by the decision maker might consist of cloudiness $\basicsig_1$ and the temperature $\basicsig_2$ of the day. Given a signal composed of $m$ basic signals, we write the realization of $\sig$ as $\sigval = (\basicsigval_{j_1}, \dots, \basicsigval_{j_{m}})$, where the realizations are sorted by the index of the basic signals and $\basicsigval_{j_i} \in \basicsigsp_{j_i}$.
    The union $\sig$ of two signals $\sig_1, \sig_2$ takes the set union, i.e., $\sig = \sig_1\cup\sig_2$.
    We will slightly abuse notation $\sig$ to represent the random variable of a signal.
   % Formally, a signal $\sigval$ is the combination of a subset of $\{\sigval_1, \ldots, \sigval_n\}$ and it can take values in the space $\sig \subseteq \sig_1 \times \ldots \times \sig_n$.
    % We use $\sig \subseteq \{\sig_1, \ldots, \sig_n\}$ to represent a policy to display signals, i.e., which combination of signals is shown to DM, and use $\sigval$ and $\sig$ to represent the realized value and space of signals for the corresponding policy $\sig$.
    % Any subscripted $\sigval_i$ always refers to a base signal, while $\sigval$ may in general be any signal.
    
    \item \textit{Data-generating process}. A data-generating process is a joint distribution $\dgp\in \Delta(\basicsigsp_1 \times \ldots \times \basicsigsp_n \times\payoffstatespace)$ over the basic signals and the payoff-relevant state. However, the DM may only observe a subset $\sig$ of the $n$ basic signals. Conditioning on receiving a signal $\sig = \sigval$, the DMs who know the data-generating process is able to infer the Bayesian posterior $\Pr[\payoffstatevalue|\sigval]$ of the state, thus improving their payoff. % \jessica{should we mention that this DM has the prior?} 
    Slightly abusing the notation, we will write $\dgp(\sigval, \payoffstatevalue)$ as the marginal
     probability of the signal realized to be $\sigval$ and the state being $\payoffstatevalue$ with expectation over unobserved signals.
\end{itemize}
%Slightly abusing notations, we write $\dgp(\payoffstatevalue)$ as the prior probability of the state $\payoffstatevalue$. 

%There is a payoff-related uncertain state $\payoffstate$ of interest to the decision-maker, e.g., $\payoffstate \in \{0, 1\} = \{\text{no rain}, \text{rain}\}$.
%There are also $n$ signals, $\sig_1, \ldots, \sig_n$, modeled as random variables. 
%These signals represent the information displayed to the decision-maker, e.g., whether it is cloudy $\sig_i \in \{0, 1\}$.
%An information structure $\infostructure$ is given by $\payoffstate$, $\sig_1$, $\ldots$, $\sig_n$ and a data generating process $\dgp \in \Delta(\payoffstate \times \sig_1 \times \ldots \times \sig_n)$, which describes the joint distribution between state and signals.

% We use lower-case $\payoffstatevalue, \sigval_1, \ldots, \sigval_n$ to refer to the outcomes generated under $\dgp$.
% Given a realization $\sigval_i$ of $\sig_i$, the probability that $\payoffstate = \payoffstatevalue$ conditioned on $\sig_i = \sigval_i$ can be obtained \jessica{by Bayesian updating the} prior: $\Pr[\payoffstate = \payoffstatevalue | \sig_i = \sigval_i] = \dgp(\payoffstatevalue, \sigval_i) / \dgp(\sigval_i)$.
% Similarly, we can also define the probability of $\payoffstate$ conditioned on realizations of multiple signals.
\subsection{Rational Decision Maker}
We suppose a rational DM who knows the data-generating process, observes a signal realization, updates their prior to arrive at posterior beliefs, and then chooses a decision to maximize their expected payoff based on the posterior belief. 
Formally, the rational DM's expected payoff given a (set of) signals $\sig$ is
\[
\mathrm{R}%^{\dgp, \score}
(\sig)
= \expect[\sigval \sim \dgp]{\max_{\action \in \actionspace}\expect[\payoffstatevalue \sim \Pr(\payoffstatevalue | \sigval)]{\score(\action, \payoffstatevalue)}}
\]
We use $\emptyset$ to represent a null signal, such that $\mathrm{R}(\emptyset)$ is the expected payoff of a Bayesian rational DM who has no access to a signal but only uses their prior belief to make decisions.
In this case, the Bayesian rational DM will take the best fixed action and their expected payoff is 
\[
\mathrm{R}%^{\dgp, \score}
(\emptyset) 
= \max_{\action \in \actionspace} \expect[\payoffstatevalue \sim \pi]{\score(\action, \payoffstatevalue)}
\]
Given a set of signals $\sig_1$ and a ground set of signals $\sig_2$, we can define the \textit{information gain} from $\sig_1$ over $\sig_2$, the payoff improvement of $\sig_1$ over the payoff obtainable from $\sig_2$. %We define the difference between $\mathrm{R}^{\dgp, \score}(\sig)$ and $\mathrm{R}^{\dgp, \score}(\emptyset)$ as the \textit{information gain} in $\sig$, which represents the maximum expected improvement of payoff by displaying $\sig$ compared to when no information provided.
\begin{equation}
\infoval(\sig_1; \sig_2) = \mathrm{R}%^{\dgp, \score}
(\sig_1\cup\sig_2) - \mathrm{R}%^{\dgp, \score}
(\sig_2).
\end{equation}
% We isolate the average contribution of a basic signal $\sig_i$ to the value of information of all combination of signals using Shapley value:
% \[
%     \shapleyinfoval(i) = \frac{1}{n} Z^{\sig_1, \ldots, \sig_n} \sum_{\sig \subseteq \{\sig_1, \ldots, \sig_n\} / \{\sig_i\}} {(n - 1)\choose |\sig|}^{-1} (\infoval(\sig \vee \{\sig_i\}) - \infoval(\sig))
% \]
% where $Z^{\sig_1, \ldots, \sig_n}$ is a normalization factor.
% In our analysis, we take $\infoval(\{\sig_1, \ldots, \sig_n\})$ to rescale $\shapleyinfoval(i)$ into $[0, 1]$ in our analysis.


% \begin{itemize}
%     \item \textbf{Rational Baseline} is the expected payoff of a rational DM without access to additional information of the payoff-relevant state. Without information, the rational DM only knows the prior from the data-generating process and selects the best fixed action. 
%     \begin{equation*}
%        \mathrm{R}^{\dgp, \score}(\sig) = \max_{\action \in \actionspace} \expect[\payoffstatevalue \sim \pi(\payoffstatevalue)]{\score(\action, \payoffstatevalue)}
%     \end{equation*}
%     \item \textbf{Rational Benchmark} of information $\sig$. \yifan{To finish}
%     \begin{equation*}
%     \mathrm{R}^{\dgp, \score}(\sig) = \expect[\sigval \sim \dgp(\sig)]{\max_{\action \in \actionspace}\expect[\payoffstatevalue \sim \Pr(\payoffstatevalue | \sig = \sigval)]{\score(\action, \payoffstatevalue)}}
% \end{equation*}
%     \item \textbf{Value of Information} $\sig$
%     \begin{equation*}
%     \infoval(\sig) = \mathrm{R}^{\dgp, \score}(\sig) - \mathrm{R}^{\dgp, \score}(\sig)
% \end{equation*}
% \end{itemize}

%Specifically, given $\payoffstate$ following the data generating process $\dgp$, the decision that maximizes the expected payoff is $\arg \max_{\action \in \actionspace} \expect[\payoffstatevalue \sim \pi(\payoffstatevalue)]{\score(\action, \payoffstatevalue)}$.
%We suppose a Bayesian rational DM who knows $\dgp$ and will first observe a signal $\sig = \sigval$, then update to the posterior $\Pr[\payoffstate = \payoffstatevalue | \sig = \sigval]$, and then choose a decision to maximize the expected payoff based on the posterior belief. 
% Formally, their expected payoff given $\dgp$, $\score$ and $\sig$ is


%We write $\mathrm{R}^{\dgp, \score}(\sig)$ as a function taking as input $\sig$ after fixing a decision task (i.e., information structure $\dgp$ and scoring rule $\score$).
% We use $\emptyset$ ro represent a null signal, such that $\mathrm{R}^{\dgp, \score}(\emptyset)$ is the expected payoff of a Bayesian rational DM who has no access to a signal but only uses their prior belief to make decisions.
% Note that the signal $\sig$ can also be a subset of $\sig_1, \ldots, \sig_n$, such that $\mathrm{R}^{\dgp, \score}(\sig)$ can represent the expected payoff of a Bayesian rational DM given a combination of signals.
% We define the difference between $\mathrm{R}^{\dgp, \score}(\sig)$ and $\mathrm{R}^{\dgp, \score}(\emptyset)$ as the \textit{value of information} in $\sig$, which represents the maximum expected improvement of payoff by displaying $\sig$ compared to when no information provided.
% \begin{equation}
%     \infoval(\sig) = \mathrm{R}^{\dgp, \score}(\sig) - \mathrm{R}^{\dgp, \score}(\emptyset)
% \end{equation}


\subsection{Information in Behavioral Decisions}
We use the term ``behavioral DM'' for a human who makes the decision in a decision-making problem after observing the signals.
The intuition behind our approach is that any information that is used by behavioral DMs should eventually reveal itself through variation in their behaviors.
Therefore, the information value in behavioral decisions can be recovered by offering the behavioral decisions as a signal to the Bayesian rational DM, which is equivalent to the information gain from behavioral decisions over a null signal.
Similarly, we can look to the information gain from different signals over the behavioral decisions alone to test how useful the signals are beyond the information revealed in the behavioral decisions.
% Intuitively, any information that is used by behavioral DMs should eventually reveal itself through variation in their behaviors. \jessica{not sure how to interpret the prev sentence - what are you trying to say that hasn't been said already?}
% \jessica{maybe add another sentence here saying that we then want to test how much the information value changes as we add signals}


We model the decisions of a behavioral DM as a random variable $\actionvar^b$ from the action space $\actionspace$, which follows the distribution $\dgp^b \in \Delta(\payoffstatespace \times \basicsigsp_1 \times \ldots \times \basicsigsp_n \times \actionspace)$ -- the joint behavior of the human correlated with the state and signals.
% , i.e., the information used by behavioral DMs to make decisions. \jessica{slightly weird to call it information value, since we defined that as a score difference}
The Bayesian rational DM knows the joint distribution $\dgp^b$. After observing human decisions, the rational DM updates to a posterior and selects the decision that maximizes their expected payoff.
Their expected payoff is given by the function:
\[
 \mathrm{R}%^{\dgp, \score}
 (\actionvar^b) = \expect[\action^b \sim \dgp^b]{\max_{\action \in \actionspace}\expect[\payoffstatevalue \sim \Pr(\payoffstatevalue | \actionvar^b = \action^b)]{\score(\action, \payoffstatevalue)}}
\]
% \jessica{why not just pi, why pi(D)? seems redundant given how you define pi b above}
%Similarly, we define $\infoval(\actionvar^b) = \mathrm{R}^{\dgp, \score}(\actionvar^b) - \mathrm{R}^{\dgp, \score}(\emptyset)$ as the information value in behavioral decisions, which is equivalent to the information value that behavioral DMs used. 
% Note that $\mathrm{R}%^{\dgp, \score}
% (\actionvar^b)$ is always higher than $\expect[\action^b, \payoffstatevalue \sim \dgp^b(\payoffstatevalue, \actionvar^b)]{\score(\action^b, \payoffstatevalue)}$, and the difference between $\mathrm{R}^{\dgp, \score}(\actionvar^b)$ and $\expect[\action^b, \payoffstatevalue \sim \dgp^b(\payoffstatevalue, \actionvar^b)]{\score(\action^b, \payoffstatevalue)}$ represents the behavioral error beyond information used by behavioral DMs (e.g., optimization error in \cite{wu2023rational}).

\paragraph{Information Gain of Signals Over Behavioral Decisions}
We seek to identify signals that can potentially improve
behavioral decisions by analyzing their expected information gain $\infoval(\sig; \actionvar^b)$, the improvement in payoff expected from having the signal $\sig$ over only having the behavioral action $\actionvar^b$. % over behavioral signals $\actionvar^b$. 
If the information gain of a signal is low over having only the behavioral decisions, this means either that the behavioral DM has already exploited the information, or that the information value to the decision problem of the signal is low.
% the information provided by the signal is reflected in the behavioral decision. \jessica{but the next sentence says there's two explanations, one of which does not mean the info is reflected in the behavioral decision}
% This means either that the behavioral DM has already exploited the information, or that the information value to the decision problem of the signal is low.
If, however, the information gain of a signal is high, then in theory the behavioral DM can improve their payoff by incorporating the signal's information in their decision making. 
%Informational substitutes~\cite{chen2016informational} capture the diminishing marginal value of information. 
%Intuitively, when two signals are similarly correlated with the payoff-relevant state, the marginal value of one signal decreases in the presence of the other, and the two signals are considered substitutes.
% \jessica{should we verbally give intuition for information substitute and complement? (if space allows?)}
% In our analysis, we use the concept of informational substitutes to measure the overlap between behavioral information and the information contained in signals, i.e., when a signal offers very small marginal value to the behavioral decisions, then the behavioral DM is using information in that signal for their decisions.
% Formally, we define the \textit{information gain} by the behavioral DMs from signal $\sig$ as:
% \begin{equation}
%     \mathrm{I}^{\actionvar^b}(\sig) = 
%     % \frac{
%     \infoval(\actionvar^b \vee \sig) - \infoval(\actionvar)
%     % }{\infoval(\actionvar^b) + \infoval(\sig)}
% \end{equation}
%Information gain $\mathrm{I}^{\actionvar^b}(\sig)$ captures how much information in $\sig$ does the behavioral DM use.
%For example, when the behavioral DM relies more on a signal to make their decisions, $\actionvar^b$ and $\sig$ are more correlated, such that they offer less marginal value of information to each other.
%We use Shapley values to quantify the average contribution of a single signal $\sig_i$ to the information gain by behavioral DMs:

However, the information value of a basic signal may be overlooked if its value in combination with other signals is not considered.
Signals can be complemented~\citep{chen2016informational}, i.e, they contain no information value by themselves but a considerable value when combined with other signals.
For example, two signals $\basicsig_1$ and $\basicsig_2$ ight be uniformly random bits and the state $\payoffstatevalue=\basicsig_1 \oplus \basicsig_2$, the XOR of $\basicsig_1$ and
$\basicsig_2$.
In this case, neither of the signals offers information value on its own but knowing both can lead to the maximum payoff.
To consider this complementation between signals, we use the Shapley value $\shapleyval$ \citep{shapley1953value} to interpret the contribution to information gain of each basic signal.
The Shapley value calculates the average of the marginal contribution of a basic signal $\basicsigsp_i$ in every combination of signals.
\begin{equation}
\label{eq:shapley_infogain}
    \shapleyval%^{\actionvar^b}
    (\basicsig_i) = \frac{1}{n} \sum_{\sig \subseteq \{\sig_1, \ldots, \sig_n\} / \{\basicsig_i\}} {(n - 1)\choose |\sig|}^{-1} (\infoval(\sig \cup \{\basicsig_i\}; \actionvar^b) - \infoval(\sig; \actionvar^b))
\end{equation}
% \jessica{equation is hard to understand - if you have space give more intuition for what this is saying}
The Shapley value suggests how much information value of the basic signal is unexploited by the behavioral DM on average in all combinations.

\paragraph{Information Gain of Behavioral Decisions Over Signals}
We analyze the additional information contained in behavioral decisions beyond what is contained in the other available signals by examining the information gain of the behavioral decision over the signal, denoted as $\infoval(\actionvar^b; \sig)$. 
Suppose the set of all signals formalized in the data-generating process are $\overline{\sig} = \{\basicsig_1, \ldots, \basicsig_n\}$.
$\infoval(\actionvar^b; \overline{\sig})$ captures the value of information that is reflected in behavioral decisions beyond the signals formalized by the data-generating process.

Our framework also offers a way to assess whether humans bring addition relevant information over an AI model for a decision task.
Denote the AI predictions and human behavioral decisions as random variables $\actionvar^{AI}$ and $\actionvar^{H}$.
$\infoval(\actionvar^{H}; \actionvar^{AI})$ gives the value of additional information that is reflected in human decisions beyond AI predictions for the decision task. 
% Then we can simply do a hypothesis test on information gain $\infoval(\actionvar^{H}; \actionvar^{AI})$ to test whether human uses additional information in the decision task.
% \[H_0: \infoval(\actionvar^{H}; \actionvar^{AI}) \leq 0\]

% $\infoval^{\dgp, \score, \actionvar^b, \sig}$ is a normalization factor.
% In our analysis, we take the information value in all signals as the normalization factor, i.e., $\infoval^{\dgp, \score, \actionvar^b, \sig} = \mathrm{R}^{\dgp, \score}(\sig_1 \vee \ldots \vee \sig_n) - \mathrm{R}^{\dgp, \score}(\emptyset)$ (\cite{wu2023rational}).

\section{Experiment}

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{groh_et_al_no_shapley.png}
%     \caption{Marginal information gain and value of information of basic signals, ordered in decreasing of $\shapleyval(i)$. The normalization factor is taken as the value of information of all signals, i.e., $Z^{\sig_1, \ldots, \sig_n} = \infoval(\sig_1, \ldots, \sig_n)^{-1}$.}
%     \label{fig:analysis_groh}
% \end{figure}
We apply our model to a deepfake video detection task studied by \cite{groh2022deepfake}, where participants are asked to judge whether a video is genuine or has been manipulated by neural network models. 
They are given access to predictions from a computer vision model that achieved an accuracy score of $65\%$ on 4,000 videos in heldout data.
Participants first review the video and report an initial decision. Then, in a second round, they are told the AI's recommendation and choose whether to change their initial decision.
Participants are asked to report their belief that the video is fake in $1\%$ increments: $\action \in \{0\%, 1\%, \ldots, 100\%\}$.

We use the Brier score as the payoff function in our model: $\score(\payoffstatevalue, \action) = 1 - (\payoffstatevalue - \action)^2$, 
with the binary payoff-related state: $\payoffstatevalue\in \{0,1\} = \{\text{genuine}, \text{fake}\}$.
We construct the basic signals in our model by the seven video-level features proposed by \citet{groh2022deepfake}: graininess, blurriness, darkness, presence of a flickering face, presence of two people, presence of a floating distraction, and the presence of an individual with dark skin, all of which are hand-labeled as binary indictors.
We estimate the data-generating process using the realizations of signals, state and behavioral decisions in the experiment data of \citet{groh2022deepfake}.

We show the results in \autoref{fig:analysis_groh}, where each distribution shows the distribution of the information gain of the signal over behavioral decisions.
The signals are on the $y$ axis and behavioral decisions are encoded by different colors.
The information gain is on the payoff scale, which is bounded by $[0,1]$, where $1$ means the signal can improve the payoff of a rational decision maker who performs as badly as possible (defined by the scoring rule, e.g., $0$ payoff in Brier score) to a rational decision maker who achieves the maximum payoff (e.g., $1$ payoff in Brier score).

% \ziyang{Conclude about the method we studied.}

% \ziyang{expand with more datasets}
% \ziyang{suppose human decision is only a funciton of information in signals. the information gain is }

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{groh_et_al.png}
    \caption{Information gain over the decisions of human-AI team, AI and unaided human.
    % The normalization factor is taken as the information gain of all signals, i.e., $Z^{\sig_1, \ldots, \sig_n} = \shapleyval(\{\sig_1, \ldots, \sig_n\})^{-1}$. \jessica{can we describe the scale somewhere, like does the axis limit mark the maximum possible attainable information gain?} \jessica{Information is spelled wrong in the labels above each plot}
    }
    \label{fig:analysis_groh}
    \vspace{-4mm}
\end{figure}

\paragraph{Unaided human v.s. AI.}
We first compare how participants without AI assistance and the AI use information in the deepfake detection task.
Specifically, we calculate the Shapley value of information gain, $\shapleyval^{\actionvar^H}(i)$ for participants and $\shapleyval^{\actionvar^{AI}}(i)$ for AI, as shown in \autoref{eq:shapley_infogain}, for each video-level feature $\sig_i$.
% We normalize by the Shapley value of information gain from all signals into $[0, 1]$ (i.e., $Z^{\sig_1, \ldots, \sig_n} = \shapleyval(\{\sig_1, \ldots, \sig_n\})^{-1}$).
The information gain over behavioral decisions reflects the information value of signals that are not fully redundant with the information in the behavioral decisions.

First, we observe that, in general, the information gain of the features over the AI decisions are lower than those for human decisions. 
There are several exceptions, such as the presence of an individual with dark skin.
This suggests that, overall, the information in features is better exploited by AI than by participants. 
More specifically, we find that the AI uses certain features much more effectively than participants. 
For instance, the presence of a flickering face offers the least information gain over AI decisions among all the features, whereas it is the feature that offers the largest information gain over human decisions.
This suggests that one way to improve the current human-AI performance is to help the participants better exploit the information that AI exploits well but participants did not.
% For example, an explanation could be used to illustrate the AI's decision rule on the flickering face signal to help human better exploit that. \jessica{what would this look like? Can we necessarily do this?}
Second, we find that the AI relies on less sensitive information compared to participants. 
For example, AI uses the presence of an individual with dark skin the least among all features, while for participants it is the second most important feature.

\vspace{-2mm}
\paragraph{Unaided human v.s. human-AI team.}
We assess the information gain after participants are presented with AI recommendations in the deepfake detection task. 
We calculate the Shapley value of information gain $\shapleyval^{\actionvar^{HAI}}(i)$ for human participants with AI recommendations relative to without. 
% In the context of our approach, this entails treating the AI recommendation as an additional signal, alongside the seven video-level features.
We find that simply displaying the AI's predictions to participants does not necessarily help them better exploit the potential value of information that they exploited poorly without access to the AI. 
For example, even though the information gain of the presence of a flickering face is reduced when presenting participants with AI predictions relative to without, the AIâ€™s much smaller gain for the signal implies participants could still use it much more effectively.
This suggests that better interventions (e.g.,\ explanations for AI predictions) may be needed to help people better incorporate some signals. 
% First, we observe that although participants do not make full use of the total possible information provided by AI recommendations, 
% \jessica{have we set up what the max attainable info gain is? i assume you say this because they could be getting more gain, but is 1 the limit?} 
% presenting the AI recommendation improves human use of information contained in video-level features that the AI uses. 
% As shown in \autoref{fig:analysis_groh}, among features the information gain from AI recommendations is the largest, indicating a large potential gap to improve the use of the AI recommendation.
% Notably, we see a dramatic drop in the information gain of the "flickering face" feature compared to human decisions made without AI recommendation, which implies that human makes better use of the information in the "flickering face" feature when AI is presented.
Second, for the signals that the AI does not exploit well, offering the AI predictions does not necessarily reduce participants' usage of that information.
For example, for the signal denoting the presence of an individual with dark skin, we did not see a significant improvement on the information gain over human-AI decisions compared to the gain over human decisions.
Both of these findings suggest that simply displaying AI predictions may not change people's usage of information and improve their decision quality.
Other interventions (such as\ explanations) should be explored to improve the use of potentially valuable by humans in human-AI collaborations. 


\section{Discussion}

In this paper, we propose an approach to investigating information value in the context of a human and AI paired on a decision task.
We identify the unexploited value of information contained in available signals by quantifying the information gain (as the increase in the expected payoff of a rational decision maker) of the signals over the behavioral decisions.
We also identify the information value reflected in the behavioral decisions that is not contained in the signals.
% By demonstrating the model on a deepfake video detection task, we learn what information is encoded in human and AI decisions versus what information could still be gained.
% \jessica{say something more descriptive, like 'we learn what information is encoded in human and AI decisions versus what information could still be gained}.
This approach makes it possible to identify signals that could be used to improve behavioral decisions through further interventions.  
For example, explanations might be designed to focus attention on signals whose information gain is low over the AI predictions but high over the human decisions.
% Moreover, knowing which information is redundant with that contained in human decisions can inform the identification of proper benchmarks to use in the evaluation of the performance of human-AI complementarity.
% \cite{guo2024decision} use the value of information in all signals to benchmark the complementary performance, though a better benchmark would consider how much value of information is processed by human.

Our work has limitations. 
First, the problem of complementation between signals is still not fully addressed.
Though we develop a model using Shapley value to calculate the marginal contribution of signals in all combinations, we only account for the complementation and substitution between observed signals.
%Future work should assess whether extra information complements the observed signals and how large would that change to our quantification of information gain.
Second, our methodology investigates information value in terms of the improvement that an ideal decision maker can achieve given that information.
% Future work can also investigate how decision makers use the extend of information given that the information is revealed in their decisions, such that can build the connection between information gain and decision performance.

\begin{ack}
We thank \citet{groh2022deepfake}, who provided their
data for demonstration in this paper.
We thank the annonymous reviewers for feedback.
\end{ack}

\bibliographystyle{plainnat}
\bibliography{ref}

\end{document}