 \mvspace{-2mm}
\section{Experiment I: Model Comparison on Chest Radiograph Diagnosis}
\label{exp2}
 \mvspace{-2mm}
We apply our framework to a well-known cardiac dysfunction diagnosis task~\citep{rajpurkar2018deep, tang2020automated, shreekumar2025x}.
We demonstrate how our framework can be used in model evaluation for analyzing how much complementary information value a set of possible AI models offers to the radiology reports written by experts.
%We demonstrate this analysis using a fixed payoff function as in Experiment 1, as well as a robustness analysis on all V-shaped scoring rules. The latter allows us to identify a Blackwell order characterizing the value of some AI models, i.e., some models dominate the others in all payoff functions.


\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/mimic_iv.png}
    \mvspace{-4mm}
    \caption{Information value of all deep-learning models calculated under our framework. The first row represents the human-alone decisions (without considering any AI predictions as additional signals). The other rows are the combinations of the human-alone decisions and the AI predictions from different pre-trained models. We list the AI predictions alone to show the AI-complementary information value offered by human decisions.}
    \mvspace{-6mm}
    \label{fig:mimic-iv-analysis}
\end{figure}
 \mvspace{-2mm}
\subsection{Data and Model}
 \mvspace{-2mm}
%We apply our framework to a well-known cardiac dysfunction diagnosis task~\citep{rajpurkar2018deep, tang2020automated, shreekumar2025x}.
We use data from the MIMIC dataset~\citep{goldberger2000physiobank}, which contains anonymized electronic health records from Beth Israel Deaconess Medical Center (BIDMC), a large teaching hospital in Boston, Massachusetts, affiliated with Harvard Medical School.
Specifically, we utilize chest x-ray images and radiology reports from the MIMIC-CXR database~\citep{johnson2019mimic} merged with patient and visit information from the broader MIMIC-IV database~\citep{johnson2023mimic}.
The payoff-related state, cardiac dysfunction $\payoffstatevalue\in \{0,1\}$, is coded based on two common tests, the NT-proBNP and the troponin, using the age-specific cutoffs from \citet{mueller2019heart} and \citet{heidenreich20222022}.
We label the radiology reports by a rule-based tool~\citep{irvin2019chexpert} and use the labels as the human decisions (without AI assistance) in the diagnosis task to solve the problem of computational feasibility with high-dimensional textual reports.
The labels are represented by the symptoms as positive, negative, or uncertain, i.e., $\action \in \{+, ?, -\}$.
% \footnote{The three symbols represent the encoding we use for signal construction, not an assertion of how radiologists communicate. \jessica{what does this mean? confusing}}.
We fine-tuned five deep-learning models on the cardiac dysfunction diagnosis task, VisionTransformer~\citep{alexey2020image}, SwinTransformer~\citep{liu2021swin}, ResNet~\citep{he2016deep}, Inception-v3~\citep{szegedy2016rethinking}, and DenseNet~\citep{huang2017densely}.
Our training set contains 12,228 images, and the validation set contains 6,115 images.
On a hold-out test set with 12,229 images, the AUC achieved by the five models is: DenseNet with $0.77$, Inception v3 with $0.76$, ResNet with $0.77$, SwinTransformer with $0.78$, and VisionTransformer with $0.80$.

We consider Brier score, a.k.a., quadratic score, as the payoff function: $\score(\payoffstatevalue, \action) = 1 - (\payoffstatevalue - \action)^2$. The scale of the quadratic score is $[0, 1]$ and a random guess ($\action \sim \text{Bernoulli}(0.5)$) achieves $0.75$ payoff. We use the quadratic score instead of the mean absolute error that is usually used in cardiac dysfunction diagnosis task because the quadratic score is a proper scoring rule where truthfully reporting the belief maximizes the payoff\footnote{We prefer a proper scoring rule so that the rational decision-maker’s strategy is to reveal their true belief, ensuring that the signal’s information value accurately reflects its role in forming beliefs.}. We also conduct a robust analysis considering various V-shaped payoff functions with different kinks on a discretized grid of $[0, 1]$ with a step of $0.01$.
% Under the evaluation of Brier score, human decisions achieve a payoff of $0.73 \pm 0.007$. \jessica{Among AI models}, VisionTransformer achieves payoff as $0.83 \pm 0.003$, ResNet achieves payoff as $0.82 \pm 0.003$, DenseNet achieves payoff as $0.82 \pm 0.003$, Inception v3 achieves payoff as $0.82 \pm 0.003$, and SwinTransformer achieves payoff as $0.83 \pm 0.003$.
We use the hold-out test set to estimate the data-generating process, which defines the joint distribution of state, human decisions, and AI models' predictions.

We construct the scale of performances by a no-information bound and a full-information bound.
The no-information bound is $\mathrm{R}^{\dgp, \score}(\emptyset)$, the baseline as we define the information value.
The full-information bound is defined as the expected payoff of a rational DM who has access to all signals, human label from radiology report and predictions from five AI models.
We do not consider the radiology images as a basic signal in this task because of the problem of computational feasibility with high-dimensional signals.
We use the five vision models' predictions as embeddings of the original images. See more detail on the algorithm of choosing the optimal embeddings in \Cref{app:high-dimensional}.

 \mvspace{-2mm}
\subsection{Results}
 \mvspace{-2mm}

\paragraph{Can the AI models complement human judgment?} 
We first analyze the agent-complementary information values in \Cref{fig:mimic-iv-analysis}, using the Brier score as the payoff function.
We find that all AI models provide complementary information value to the aforementioned human judgment.
% This highlights the same takeaways as \autoref{exp1}, that the AI model has considerable potential to improve human decisions. \jessica{omit prev sentence -- don't mention if we haven't gotten to it yet}
As shown in \Cref{fig:mimic-iv-analysis} (comparison between \textcolor{humanaidecision}{$\mathrm{R}^{\dgp, \score}(\actionvar^{\text{Human}} \cup \actionvar^{\text{AI}})$} and \textcolor{humandecision}{$\mathrm{R}^{\dgp, \score}(\actionvar^{\text{Human}})$}), all AI models capture at least $20\%$ of the total available information value (across all AI model and human decisions) that is not exploited by human decisions. This motivates deploying an AI to assist humans in this scenario.


In the other direction, the human decisions also provide complementary information to all AI models, comparing \textcolor{humandecision}{$\mathrm{R}^{\dgp, \score}(\actionvar^{\text{Human}})$} with \textcolor{aidecision}{$\mathrm{R}^{\dgp, \score}(\actionvar^{\text{AI}})$} in \Cref{fig:mimic-iv-analysis}.
This observation might inspire, for example, further investigation of the information the humans can access to that is not represented in AI training data.

\paragraph{Which AI model offers the most decision-relevant information over human judgments?} 
\Cref{fig:mimic-iv-analysis} shows that VisionTransformer contains slightly higher information value than the other models, and Inception v3 contains slightly lower information value than the other models.
We assess the stability of VisionTransformer's superiority over the other AI models across many possible losses to test if there is a Blackwell ordering of models.
By \Cref{prop: blackwell-V-test}, we test the payoff of models on all V-shaped scoring rules, shown in \Cref{fig:robust-analysis}.
Across all the V-shaped payoff functions, we find that VisionTransformer is Blackwell more informative and Inception v3 is Blackwell less informative than all other models. 
The VisionTransformer achieves a higher information value on all V-shaped scoring rules, implying a higher information value on all decision problems.
% \st{: 1) While accuracy may rank models in one way, there may exist decision problems where the order between models is reversed; 2) while two models may seem comparable in accuracy, one can be more informative than the other for all decision problems, which is robustly good.} \jessica{none of the previous text was helpful}
% In V-shaped payoff functions, the kink $\mu$ represents the threshold on beliefs that the rational decision-maker would use to change their decision, \jessica{why aren't we explaining this before we talk about mu above?} such that larger $\mu$ means that the decision-makers will prefer $\action = 1$ more, and the closer $\mu$ is to $0.5$, the \jessica{more indifferent the decision-maker is}. \st{harder would it be for the decision-makers to prefer $\action = 0$ or $\action = 1$.}
% The distributions plotted in \Cref{fig:robust-analysis} show that the information contained in VisionTransformer becomes more valuable in theory to human decision-makers when the decision task becomes harder (where the kink $\kink$ is closer to $0.5$).
% This analysis highlights the insufficiency of accuracy-based model comparisons to account for 
% Just because we cannot distinguish some models by accuray (e.g., VisionTransformer and SwinTransformer) does not mean that can always be more informative than another conditioned on the existing information in human decisions under every payoff functions.


