 \mvspace{-2mm}
\section{Discussion and Limitations}
 \mvspace{-2mm}

We propose a decision-theoretic framework for assigning value to information in human-AI decision-making.
Our methods quantify the additional information value of any signals over an agent's decisions, and can be applied to any combination of artificial or human agent judgments. % (or signals) that are available in an AI-assisted decision workflow, and extend our work to a robustness analysis for all possible decision problems.
The three demonstrations we provide show how quantified information value can drive model selection, empirical evaluation, and explanation design.
These are just a few of many possible use cases. 
For example, explanation approaches could be compared in terms of their information value to human decisions. 
Information value analysis could also drive elicitation of human private signals upon identifying that human decisions contain AI-complementary information.
% , or decision rules to further improve the pairing, such as informing humans which AI prediction agrees with their knowledge and which disagree. \jessica{I don't understand what 'or decision rules ... such as informing humans ...' is saying}
New explanation strategies based on ILIV-SHAP could more directly integrate visualized information value with conventional depictions of feature importance.  

It is important to note that when using ACIV to understand complementary information over human decisions, it cannot be interpreted as definitively establishing that particular signals were \textit{used} by the human. 
It is always possible that the human has other private signals offering equivalent information to a feature being analyzed.
This raises a question of whether it is definitive about use if it is an AI as the agent instead of a human.
%For example, when the $ACIV$ says that the information value in a signal can be substitued by the human decisions, it is possible human is relying on other signals (or even unobservable private signals) that contain equivalent information.
% \st{Our framework cannot account for identifying the private signals used by human decisions, even though they might have a strong correlation with the payoff-related state and agent decisions.} 
However, quantifying the value of observed information is an important first step toward learning about information that may exist beyond an explicit problem definition.


Our framework quantifies the best-attainable performance improvement from integrating signals in decisions. This does not necessarily mean that highlighting or otherwise emphasizing those signals for humans will lead to them performing better. However, the basis of our framework in Bayesian decision theory means that it provides a theoretical basis that can be adapted to support comparisons to human behavior to drive learning~\citep{hullman2021designing}. 
For example, if we suspect a human decision-maker uses AI predictions and their own predictions strictly monotonically, we could constrain the rational decision-maker to only make monotonic decisions with AI prediction and their own predictions.


% \ziyang{Note that this does not necessarily mean that the human is using that specific feature information, as it is possible they have access to private signals that contain equivalent information.} 