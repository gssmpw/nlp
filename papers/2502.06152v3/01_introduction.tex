
\section{Introduction}
\mvspace{-2mm}
As the performance of artificial intelligence (AI) for domain-specific predictions improves, workflows in which human and AI models are combined to make decisions in medicine, finance, and law, among other domains. 
Often, statistical models can be developed that exceed the accuracy of human experts on average~\citep{aegisdottir2006meta,grove2000clinical,meehl1954clinical}. 
However, whenever humans have access to additional information over the AI, there is potential to achieve \textit{complementary performance} by pairing the two, i.e., better performance than either the human or AI alone.
For example, a physician may have access to additional information that may not be captured in tabular electronic health records or other structured data \citep{alur2024distinguishing}. 


Evidence of complementary performance between humans and AI nonetheless remains limited, with many studies showing that human-AI teams underperform an AI alone~\citep{buccinca2020proxy, bussone2015role, green2019principles, jacobs2021machine, lai2019human, vaccaro2019effects, kononenko2001machine}.
A solid understanding of such results is limited by the fact that most analyses of human-AI decision-making focus on ranking the performance of human-AI teams or each individually using measures based on post hoc decision accuracy~\citep{passi2022overreliance}. This approach is problematic for several reasons. First, it does not account for the best achievable performance based on the information available at the time of the decision~\citep{kleinberg2015prediction, guo2024decision, rambachan2024identifying}. 
Second, it cannot provide insight into the 
potential for available information to improve decision performance, making it difficult to design interventions that improve the team's performance. 


In contrast, identifying information complementarities that contribute to the maximum achievable decision performance of a human and AI model---such as when one of the agents has access to information not contained in the other's judgments, or has not fully integrated information available in the environment into their judgments---provides actionable information for improving the decision pipeline. For example, the degree to which a model provides information that complements human judgments can guide AI model selection. 
Evidence that human experts possess decision-relevant information over AI predictions motivates further data collection to improve the AI model. 
Evidence that model predictions contain decision-relevant information that humans do not exploit can guide the design of explanations highlighting complementary information.


We contribute a decision-theoretic framework for characterizing the value of information available within an AI-assisted decision workflow.
In our framework, information is considered valuable to a decision-maker to the extent that it is possible, in theory, to incorporate it into their decisions to improve performance. 
Specifically, our approach analyzes the expected marginal payoff gain from best case (Bayes rational) use of additional information over best case use of the information already encoded in agent decisions for a given decision problem. % in decision tasks. 
The intuition behind this approach is that any information that is used by the agents will eventually reveal itself through variation in their decisions. 
We identify the value of the information in agent (human, AI, or human-AI) decisions by offering them as a signal to a Bayesian rational decision-maker. While we focus on combinations of humans and AI agents, the methods we propose can be applied to any combination of agents.

%Consequently, we can assess the potential added value of additional information signals over their decisions. %, comparing information value across different agents.



We introduce two metrics for evaluating information value in human-AI collaboration.
The first---global human-complementary information value---calculates the value of a new piece of information to an agent over all of its possible realizations among all instances.
%It is useful for evaluation questions where it is natural to consider performance over a distribution of instances, such as comparing multiple AI models to determine which can provide the most useful additional information over human judgments, or assessing whether offering AI predictions helped humans use available information more effectively. 
The second---instance-level human-complementary information value---identifies opportunities for decision-makers to better use instance-level information.
%This instance-level view inspires new AI explanation techniques that reveal how specific data features influence the value of information for an individual prediction.
% \jessica{Don't need to say this here} \st{Finally, we offer a robustness analysis that orders signals according to their global (or instance-level) human-complementary information value for all possible decision problems.}

We demonstrate the framework by applying the metrics to three decision-making tasks where AI models serve as human decision-making assistants\footnote{Code to replicate our experiments is available at \url{https://osf.io/p2qzy/?view_only=ec06600d06cd4e59bb6051f992e54c08}}: chest X-ray diagnosis~\citep{rajpurkar2018deep, johnson2019mimic}, deepfake detection~\citep{dolhansky2020deepfake, groh2022deepfake}, and recidivism prediction~\citep{angwin2022machine, dressel2018accuracy}. 
First, we demonstrate the framework's utility in \textit{model selection} by evaluating how well different AI models complement human decision-makers, showing how even among models with similar accuracy, some models strictly offer more \textit{complementary} information than others across decision problems.
Next, we use the framework to empirically evaluate how providing AI assistance (alongside instance-level features) helps humans exploit available information for decision-making. 
% We find that simply displaying model outputs to human decision-makers does not significantly improve their ability to integrate available information; instead, they often simply rely on the AI without leveraging their own expertise. 
Lastly, we demonstrate use of the framework to design explanations by extending SHAP~\citep{lundberg2017unified} to highlight the portion of an AI’s explanation that complements human information.
%We show that the conventional “marginal contribution” approach of SHAP does not address the information value that human lacks, and instead propose an approach that uses the human-complementary information value as the objective function of the explanation technique to attribute each signal’s contribution directly to human's information need. 

\mvspace{-3mm}