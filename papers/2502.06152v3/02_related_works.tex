\section{Related work}
\mvspace{-2mm}
\paragraph{Human-AI complementarity.}

Many empirical studies of human-AI collaboration focus on AI-assisted human decision-making for legal, ethical, or safety reasons~\citep{bo2021toward, boskemper2022measuring, bondi2022role, schemmer2022meta}.
However, a recent meta-analysis by \citet{vaccaro2024combinations} finds that, on average, human–AI teams perform worse than the better of the two agents alone. 
In response, a growing body of work seeks to evaluate and enhance complementarity in human–AI systems \citep{bansal2021does, bansal2019updates, bansal2021most, wilder2021learning, rastogi2023taxonomy, mozannar2024effective}.
The present work differs from much of this prior work by approaching human-AI complementarity from the perspective of information value and use, including asking whether the human and AI decisions provide additional information that is not used by the other.
\mvspace{-2mm}
\paragraph{Evaluation of human decision-making with machine learning.}
Our work contributes methods for evaluating the decisions of human-AI teams~\citep{kleinberg2015prediction, kleinberg2018human, lakkaraju2017selective, mullainathan2022diagnosing,  rambachan2024identifying, guo2024decision, ben2024does, shreekumar2025x}.
\citet{kleinberg2015prediction} proposed that evaluations of human-AI collaboration should be based on the information that is available at the time of decisions.
% \jessica{can omit:} A significant portion of this literature addresses \textit{performative prediction}~\citep{perdomo2020performative}, where predictions or decisions affect future outcomes. 
% Because counterfactual decisions’ outcomes remain unobserved, researchers typically rely on worst-case analyses to bound the potential performance \citep{rambachan2024identifying, ben2024does}. 
% Though these issues arise in many canonical human-AI collaboration tasks, we focus on standard ``prediction policy problems'' where the payoff can be translated into policy gains~\citep{kleinberg2015prediction}.
According to this view, our work defines Bayesian best-attainable-performance benchmarks similar to several prior works~\citep{guo2024decision, wu2023rational,agrawal2020scaling, fudenberg2022measuring}. 
Closest to our work, \citet{guo2024decision} model the expected performance of a rational Bayesian agent faced with deciding between the human and AI recommendations as the theoretical upper bound on the expected performance of any human-AI team.
This benchmark provides a basis for identifying exploitable information within a decision problem.

\mvspace{-3mm}
\paragraph{Human information in machine learning.}

Some approaches focus on automating the decision pipeline by explicitly incorporating human expertise in developing machine learning models, such as by learning to defer~\citep{mozannar2024show, madras2018predict, raghu2019algorithmic, keswani2022designing, keswani2021towards, okati2021differentiable}.
\citet{corvelo2023human} propose multicalibration over human and AI model confidence information to guarantee the existence of an optimal monotonic decision rule.
\citet{alur2023auditing} propose a hypothesis testing framework to evaluate the added value of human expertise over AI forecasts.
Our work shares the motivation of incorporating human expertise, but targets a slightly broader scope by quantifying the information value for all available signals and agent decisions in a human–AI decision pipeline.

