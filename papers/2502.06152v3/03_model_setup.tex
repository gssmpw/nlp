\mvspace{-3mm}
\section{Methodology}

% \jessica{Also, we should be clearly defining what information our approach assumes as input, and what the output is (at least at a high level)}
\mvspace{-2mm}
Our framework takes as input a decision problem associated with an information model and outputs the value of information of any available signals to any agent, conditioning on the existing information in their decisions within a Bayesian decision theoretic framework.
Our framework provides two separate functions to quantify the value of information: one globally across the data-generating process, and one locally in a realization drawn from the data-generating process.
We also introduce a robust analysis approach to information order, which enables us to compare the agent-complementary information in signals for all possible decision problems.
% \jessica{not sure why some things are italized ... for example why is realization not italicized when it first appears? should globally and locally be italicized instead of data-generating process? could probably just remove all italics or only use italics for things we will define specifically}

% In this section, we define the basis of this approach, including a decision problem and associated information structure, following prior decision-theoretic frameworks for studying decisions from statistical information~\citep{wu2023rational,guo2024decision,hullman2024decision}.
% Then we define how a rational decision-maker would act given \st{a signal and} \jessica{such} a decision problem and associated information structure, and use rational \jessica{behavior within the problem} \st{decision-maker ,as a tool  we show how} to \st{investigate} \jessica{characterize} the information encoded in behavioral decisions.
%\jessica{May want to add a sentence or two here to give the reader some intuition for our approach. E.g., Our approach relies on analysis of the marginal gain ... }

%\ziyang{Merge infomration structure and decision-making problem into one section}
\mvspace{-4mm}
\paragraph{Decision Problem.} A decision problem consists of three key elements. We illustrate with an example of a weather decision. 
\mvspace{-2mm}
\begin{itemize}[wide]
    \mvspace{-2mm}
    \item A payoff-relevant state $\payoffstatevalue$ from a space $\payoffstatespace$. For example,\ $\payoffstatevalue \in \payoffstatespace =  \{0, 1\} = \{\text{no rain}, \text{rain}\}$.
    \mvspace{-3mm}
    \item A decision $\action$ from the decision space $\actionspace$ characterizing the decision-maker (DM)'s choice. For example,\ $\action\in \actionspace = \{0, 1\} = \{\text{not take umbrella}, \text{take umbrella}\}$.
    \mvspace{-2mm}
    \item A payoff function $\score: \actionspace\times\payoffstatespace\to\mathbb{R}$, used to assess the quality of a decision given a realization of the state. For example, $\score(\action = 0, \payoffstatevalue = 0) = 0, \score(\action = 0, \payoffstatevalue = 1) = -100, \score(\action = 1, \payoffstatevalue = 0) = -50, \score(\action = 1, \payoffstatevalue = 1) = 0$, which punishes the DM for selecting an action that does not match the weather. 
\end{itemize}

In decision problems corresponding to prediction tasks, the decision space is a probabilistic belief over the state space, i.e., $\actionspace = \Delta(\payoffstatespace)$.
For such problems, a payoff function is said to be a \textit{proper} scoring rule if the optimal action is to predict the true distribution, i.e., $p = \arg \max_{\action \in \actionspace} \expect[\payoffstatevalue \sim p]{\score(\action, \payoffstatevalue)}$.
% Therefore, given a proper scoring rule a DM who maximizes the expected payoff will truthfully report their belief.
For any decision problem with payoff function $\score: \actionspace\times\payoffstatespace\to\mathbb{R}$, there is an equivalent proper scoring rule $\hat{\score}: \Delta(\payoffstatespace)\times\payoffstatespace\to\mathbb{R}$ defined by choosing the optimal decision under the reported
belief. Formally,
\begin{equation}
\label{eq:properscoring}
    \hat{\score}(p, \payoffstatevalue) = \score(\arg \max_{\action \in \actionspace} \expect[\payoffstatevalue \sim p]{\score(\action, \payoffstatevalue)}, \payoffstatevalue).
\end{equation}

\Cref{eq:properscoring} shows a reduction from the payoff function $\score$ to the proper scoring rule $\hat{\score}$, i.e., any decision $p$ under $\hat{\score}$ represents a decision $\action$ in $\score$ that best-responds to the distribution $p$.
Therefore, the best-attainable performance defined in the proper scoring rule $\hat{\score}$ is equivalent to the best-attainable performance defined in any payoff function $\score$ that can be reduced to $\hat{\score}$.
Throughout our framework, we prefer proper scoring rules over non-proper scoring rules, since the best-attainable performance defined in the former implies the best-attainable performance in the latter.

\mvspace{-4mm}
\paragraph{Information Model.} 
We cast the information available to a DM as a signal defined within an information model.
We use the definition of an information model in \citet{blackwell1951comparison}. 
The information model can be represented by a \textit{data-generating model} with a set of \textit{signals}.
\begin{itemize}[wide]
    \mvspace{-3mm}
    \item \textit{Signals}. There are $n$ ``basic signals'' represented as random variables $\basicsig_1, \ldots, \basicsig_n$, from the signal spaces $\basicsigsp_1, \ldots, \basicsigsp_n$. Basic signals represent information available to a decision-maker, e.g., $\basicsigsp_1 = \{\text{cloudy}, \text{not cloudy}\}$, $\basicsigsp_2\in \{0, \ldots, 100\}$ for temperature in Celsius, etc. 
    % We write $k_i = |\basicsigsp_i|$ as the size of the signal space of the basic signal $i$, $\basicsig_i$ as the random variable for basic signal $i$, and $\basicsigval_{ij_i}\in \basicsigsp_i$ as the $j_i$th realized value of the $\basicsig_i$ ($j_i\leq k_i$).
    % E.g.\ observable features about the weather $\{\sig_1, \sig_2, \ldots\} = \{\text{temperature}, \text{cloud level}, \dots\}$. 
    % In addition to the basic signals, there are also other signals that \st{intuitively} represent the combination of basic signals.
    The decision-maker observes a signal, which is a subset of the basic signals, $\sig \subseteq 2^{\{\basicsig_1, \dots, \basicsig_n\}}$. 
    Specifically, we use $\sig = \{\basicsig_{j_1}, \ldots, \basicsig_{j_k}\}$ for a signal having $k$ basic signals and denote the signal space as $\sigsp = \basicsigsp_{j_1} \times \ldots \times \basicsigsp_{j_k}$.
    For example,\ a signal representing a combination of two basic signals $\sig = \{\basicsig_1, \basicsig_2\}$ observed by the decision-maker might consist of cloudiness $\basicsig_1$ and the temperature $\basicsig_2$ of the day. Given a signal composed of $m$ basic signals, we write the realization of $\sig$ as $\sigval = (\basicsigval_{j_1}, \dots, \basicsigval_{j_{k}})$, where the realizations $\basicsigval_{j_i} \in \basicsigsp_{j_i}$ are sorted by the index of the basic signals $j_i \in [n]$.
    The union $\sig$ of two signals $\sig_1, \sig_2$ takes the set union, i.e., $\sig = \sig_1\cup\sig_2$.
    % Though $\sig$ is initially defined as a set of random variables, we will slightly abuse notation $\sig$ to represent a random variable that is drawn from the joint distribution of the basic signals in it.
    % \jessica{I'm finding this part really confusing - eg we use capital V to refer to a signal, then lower case v}
     \mvspace{-1mm}
    \item \textit{Data-generating process}. A data-generating process is a joint distribution $\dgp\in \Delta(\basicsigsp_1 \times \ldots \times \basicsigsp_n \times\payoffstatespace)$ over the basic signals and the payoff-relevant state. $\dgp$ can be viewed as the combination of two distributions: the prior distribution of the state $\Pr[\payoffstatevalue]$ and the signal-generating distribution $\Pr[\sigval | \payoffstatevalue]$ defining the conditional distribution of signals. The DM may only observe a subset $\sig$ of the $n$ basic signals. We can define the Bayesian posterior belief upon receiving a signal $\sig = \sigval$ from the data-generating model as
     \mvspace{-1mm}
    \[\dgp(\payoffstatevalue| \sigval) := \Pr[\payoffstatevalue|\sigval]=\frac{\dgp(\sigval, \payoffstatevalue)}{\dgp(\payoffstatevalue)}\]
    % Conditioning on receiving a signal $\sig = \sigval$, \jessica{a DM} \st{the DMs} who knows the data-generating process is able to infer the Bayesian posterior $\Pr[\payoffstatevalue|\sigval]$ of the state, thus improving their payoff. % \jessica{should we mention that this DM has the prior?} 
 \mvspace{-4mm}

    \noindent where we slightly abuse notation to write $\dgp(\sigval, \payoffstatevalue)$ as the marginal
     probability of the signal realized to be $\sigval$ and the state being $\payoffstatevalue$ with expectation over other signals.
\end{itemize}

The choice of basic signals directly impacts how many observed samples are required to get a good estimate of the data-generating process.
When high-dimensional signals such as images or text are used, it may not be computationally feasible to estimate the data-generating process.
In such cases, one can preprocess the high-dimensional signals to get low-dimensional representations which (ideally) capture any important structure. 
These lower-dimensional signals can be defined by humans, such as when concepts are identified and then used to label the high-dimensional signals (e.g., images or parts of images). 
Alternatively, they can be defined algorithmically, by strategically applying dimensionality reduction to generate low-dimensional embeddings.
We introduce an algorithm for identifying an ``optimal'' reduction model in decision problems in \Cref{app:high-dimensional}.
%Another way is to let human label the concepts in high-dimensional signals, which generates interpretable low-dimensional signals.
We demonstrate these two methods in \Cref{exp2} and \Cref{exp1} respectively.

% \ziyang{several ways to do this. Use the algorithm to find low-dimensional but you can interpret these. Or let human label concepts. (This is what we've done in our study). Mention when we want to just use AI model's predictions to represent. }

%Slightly abusing notations, we write $\dgp(\payoffstatevalue)$ as the prior probability of the state $\payoffstatevalue$. 

%There is a payoff-related uncertain state $\payoffstate$ of interest to the decision-maker, e.g., $\payoffstate \in \{0, 1\} = \{\text{no rain}, \text{rain}\}$.
%There are also $n$ signals, $\sig_1, \ldots, \sig_n$, modeled as random variables. 
%These signals represent the information displayed to the decision-maker, e.g., whether it is cloudy $\sig_i \in \{0, 1\}$.
%An information structure $\infostructure$ is given by $\payoffstate$, $\sig_1$, $\ldots$, $\sig_n$ and a data generating process $\dgp \in \Delta(\payoffstate \times \sig_1 \times \ldots \times \sig_n)$, which describes the joint distribution between state and signals.

% We use lower-case $\payoffstatevalue, \sigval_1, \ldots, \sigval_n$ to refer to the outcomes generated under $\dgp$.
% Given a realization $\sigval_i$ of $\sig_i$, the probability that $\payoffstate = \payoffstatevalue$ conditioned on $\sig_i = \sigval_i$ can be obtained \jessica{by Bayesian updating the} prior: $\Pr[\payoffstate = \payoffstatevalue | \sig_i = \sigval_i] = \dgp(\payoffstatevalue, \sigval_i) / \dgp(\sigval_i)$.
% Similarly, we can also define the probability of $\payoffstate$ conditioned on realizations of multiple signals.
\mvspace{-4mm}
\paragraph{Information value.}
Our framework quantifies the value of information in a signal $\sig$ as the expected payoff improvement of an idealized agent who has access to $\sig$ in addition to some baseline information set.
% \jessica{this whole section could benfeit from a few more sentences early in the subsections to reiterate what we are tryin to achieve, or even phrases. E.g., here it seems we want to quantify the information value of some signal, relative to another signal. Give the reader more 'sign posts' to help remind them why we are setting up different concepts the way we are. Can be simple as adding a phrase like 'To quantify the vaue of the information is some set of signals' to the beginning of the first sentence. Its easy to lose the point currently}
We suppose a rational Bayesian DM who knows the prior probability of the state and conditional distribution of signals (i.e., the data-generating process), observes a signal realization, updates their prior to arrive at posterior beliefs, and then chooses a decision to maximize their expected payoff given their posterior belief. 
Formally, given a decision task with payoff function $\score$ and an information model $\dgp$, the rational DM's expected payoff given a (set of) signal(s) $\sig$ is
\mvspace{-1mm}
\begin{equation}
\mathrm{R}^{\dgp, \score}
(\sig)
= \expect[(\sigval, \payoffstatevalue) \sim \dgp]{\score(\action^r(\sigval), \payoffstatevalue)}
\end{equation}
\mvspace{-4mm}
\noindent where $\action^r(\cdot): \sigsp \rightarrow \actionspace$ denotes the decision rule adopted by the rational DM.
\begin{equation}
\label{eq:rationalDM}
    \action^r(\sigval) = \arg \max_{\action\in\actionspace} \expect[\payoffstatevalue \sim \dgp(\payoffstatevalue|\sigval)]{\score(\action, \payoffstatevalue)}
\end{equation}

We use $\emptyset$ to represent a null signal, such that $\mathrm{R}^{\dgp, \score}(\emptyset)$ is the expected payoff of a Bayesian rational DM who only uses the prior distribution to make decisions.
In this case, the Bayesian rational DM will take the best fixed action under the prior, and their expected payoff is:
\mvspace{-1mm}
\begin{equation}
\label{eq:baseline}
\mathrm{R}^{\dgp, \score}
(\emptyset) 
= \max_{\action \in \actionspace} \expect[\payoffstatevalue \sim \pi]{\score(\action, \payoffstatevalue)}
\end{equation}
\mvspace{-4mm}

This baseline defines the maximum expected payoff that can be achieved with no information.
Bayesian decision theory quantifies the information value of $\sig$ by the payoff improvement of $\sig$ over the payoff obtained without information.
% Given a set of signals $\sig_1$ and a ground set of signals $\sig_2$ (which could be the null signal $\emptyset$), we can define the \textit{information gain} from $\sig_1$ over $\sig_2$, the payoff improvement of $\sig_1$ over the payoff obtainable from $\sig_2$.
% \begin{equation}
% \infoval^{\dgp, \score}(\sig_1; \sig_2) = \mathrm{R}^{\dgp, \score}
% (\sig_1\cup\sig_2) - \mathrm{R}^{\dgp, \score}
% (\sig_2).
% \end{equation}


\begin{definition}
Given a decision task with payoff function $\score$ and an information model $\dgp$, the information value of $\sig$ is defined as
\mvspace{-1mm}\begin{equation}
    \IV^{\dgp, \score}(\sig) = \mathrm{R}^{\dgp, \score}
(\sig) - \mathrm{R}^{\dgp, \score}
(\emptyset)
\end{equation}
\end{definition}
\mvspace{-2mm}
We adopt the same idea to define the agent-complementary information value in our framework.

% $IV$ reflects the marginal information offered by $\sig$ over $\emptyset$.
% In human-AI collaboration, we may especially be interested in the complementary information offered by a signal (e.g., AI prediction and explanation) over human information.
% In that case, $IV$ can be defined as \[IV^{\dgp, \score}(\sig) = \infoval^{\dgp, \score}(\sig; \actionvar^b)\]where $\actionvar^b$ is a random variable for human decisions, which we defined in the following section.
\mvspace{-2mm}
\subsection{Agent-Complementary Information Value}
\mvspace{-2mm}

With the above definitions, it is possible to measure the additional value that new signals can provide over the information already captured by an agent’s decisions. Here, \textit{agent} may refer to a human, an AI system, or a human–AI team.
The intuition behind our approach is that any information that is used by decision-makers should eventually reveal itself through variation in their decisions.
We recover the information value in agent decisions by offering the decisions as a signal to the Bayesian rational DM.
We model the agent decisions as a random variable $\actionvar^b$ from the action space $\actionspace$, which follows a joint distribution $\dgp \in \Delta(\basicsigsp_1 \times \ldots \times \basicsigsp_n \times \payoffstatespace \times  \actionspace)$ with the state and signals.
The expected payoff of a Bayesian rational DM who knows $\dgp$ is given by the function:
 \mvspace{-1mm}
\[
\mathrm{R}^{\dgp, \score}
(\actionvar^b)
= \expect[(\action^b, \payoffstatevalue) \sim \dgp]{\score(\action^r(\action^b), \payoffstatevalue)}
\]
% \[
%  \mathrm{R}%^{\dgp, \score}
%  (\actionvar^b) = \expect[\action^b \sim \dgp^b]{\max_{\action \in \actionspace}\expect[\payoffstatevalue \sim \Pr(\payoffstatevalue | \actionvar^b = \action^b)]{\score(\action, \payoffstatevalue)}}
% \]
 \mvspace{-5mm}
 
% Similarly, we can assess the potential for other available information to improve agent decisions by quantifying the information gain from different signals (such as instance feature information or AI predictions) over the agent decisions alone. 
We seek to identify signals $\sig$ that can potentially improve agent decisions by analyzing the information value in the combined signal $\actionvar^b \cup \sig$ over the information value in $\actionvar^b$, which we define as the agent-complementary information value.
\begin{definition}
\label{def:aciv}
Given a decision task with payoff function $\score$ and an information model $\dgp$, we define the agent-complementary information value ($\ACIV$) of $\sig$ on agent decisions $\actionvar^b$ as 
\mvspace{-1mm}\begin{equation}
    \ACIV^{\dgp, \score}(\sig; \actionvar^b) = \mathrm{R}^{\dgp, \score}(\actionvar^b \cup \sig) - \mathrm{R}^{\dgp, \score}(\actionvar^b)
\end{equation}
\end{definition}
\mvspace{-3mm}

If the $\ACIV$ of a signal $\sig$ is small relative to the baseline (\ref{eq:baseline}), this means either that the information value of $\sig$ to the decision problem is low (e.g., it is not correlated with $\payoffstatevalue$), or that the agent has already exploited the information in $\sig$ (e.g., the agent relies on $\sig$ or equivalent information to make their decisions such that their decisions correlate with $\payoffstatevalue$ in the same way as $\sig$ correlates with $\payoffstatevalue$).
If, however, the $\ACIV$ of $\sig$ is large relative to the baseline, then at least in theory, the agent can improve their payoff by incorporating $\sig$ in their decision making.

 \mvspace{-1mm}
Furthermore, $\ACIV$ can reveal complementary information between different types of agents. 
For instance, if we view AI predictions as $\sig$ and treat human decisions as the agent signal $\actionvar^b$, a large $\ACIV$ indicates that AI predictions add considerable value beyond what humans alone achieve. In the reverse scenario, if human decisions serve as $\sig$ and AI predictions are $\actionvar^b$, we can measure how much humans can contribute over the information captured in the AI predictions. We demonstrate uses of $\ACIV$ in \Cref{exp2} and \Cref{exp1}.

 \mvspace{-2mm}
\subsection{Instance-level Agent-complementary Information Value}
 \mvspace{-2mm}
$\ACIV$ quantifies the value of the decision-relevant information in a signal $\sig$ across all possible realizations defined by the data-generating model.
To provide analogous instance-level quantification of information value, we define Instance-Level agent-complementary Information Value ($\ILIV$) to quantify the value of the decision-relevant information encoded by a single realization of the signal.
% Instance-level Agent-Complementary Information Value ($\ILIV$) evaluates the additional information contributed by a single realization of a signal rather than the entire joint distribution.
% $\ACIV$ can be useful in cases such as evaluation/comparison of models and empirical analysis of whether AI's assistance can help humans improve the information value in their decisions.
This finer-grained view makes it possible to analyze how much an agent can benefit in theory from better incorporating instance-level information in their decision.

Given a realization of signal $\sigval = \{\basicsigval_{j_1}, \ldots, \basicsigval_{j_k}\}$, we want to know the maximum expected payoff gain from gaining access to $\sigval$ on the instances where $\sigval$ is realized over the existing information encoded in agent decisions. Intuitively, this captures how much ``room'' there is for specific decisions to be improved through better use of the signal.
To calculate instance-level information value, we rely on the performance of the Bayesian rational agent on the conditional distribution $\dgp(\basicsigval_1, \ldots, \basicsigval_n, \payoffstatevalue, \action^b | \sigval)$ instead of the whole distribution $\dgp(\basicsigval_1, \ldots, \basicsigval_n, \payoffstatevalue, \action^b)$.
% We use a Bayesian rational DM to quantify $\ILIV$ in a similar way to $\ACIV$.
% Instead of evaluating on the distribution $\dgp(\sigval, \payoffstatevalue)$, $\ILIV$ evaluates the expected payoff of the rational DM on the distribution indicated by the instance $\dgp(\payoffstatevalue | \sigval)$.  \jessica{whats the intuition here? the explanation doesn't provide much of a sense of why this is useful}
Formally, given a decision task with payoff function $\score$ and information model $\dgp$, the expected payoff of the rational DM given signal $\sig = \sigval$ on instances where $\sig = \sigval$ is
\begin{equation}
\mathrm{r}^{\sigval, \dgp, \score}(\sigval) = \expect[\payoffstatevalue \sim \dgp(\payoffstatevalue | \sigval)]{\score(\action^r(\sigval), \payoffstatevalue)}
\end{equation}
% \noindent where $\action^r(\cdot)$ is defined in \Cref{eq:rationalDM}. 
\noindent where $\action^r(\sigval)$ is the Bayesian optimal decision on receiving  $\sigval$ as defined in \Cref{eq:rationalDM}.
If we consider the agent decisions in addition to the realization $\sigval$, the rational DM's expected payoff on instances where $\sig = \sigval$ can be written as
\begin{equation}
\mathrm{r}^{\sigval, \dgp, \score}(\sigval; \actionvar^b) = \expect[(\action^b, \payoffstatevalue) \sim \dgp(\action^b, \payoffstatevalue | \sigval)]{\score(\action^r(\sigval \cup \action^b), \payoffstatevalue)}
\end{equation}

\begin{definition}
\label{def:RIIV}
Given a decision task with payoff function $\score$ and an information model $\dgp$, we define the instance-level agent-complementary information value ($\ILIV$) of signal $\sig = \sigval$ on instances where $\sig = \sigval$ as:
 \mvspace{-1mm}
\begin{equation}
    \ILIV^{\sigval, \dgp, \score}(\sigval; \actionvar^b) = \mathrm{r}^{\sigval, \dgp, \score}(\sigval; \actionvar^b) - \mathrm{r}^{\sigval, \dgp, \score}(\emptyset; \actionvar^b)
\end{equation}
\end{definition}
 \mvspace{-4mm}
\noindent where $\mathrm{r}^{\sigval, \dgp, \score}(\emptyset; \actionvar^b)$ represents the rational DM's expected payoff on instances where $\sig = \sigval$ with only the exisiting information encoded in agent decisions.
Taking the expectation of $\ILIV$ over $\sig$ recovers the global agent-complementary information value ($\ACIV$), i.e.,
\begin{equation}
    \ACIV^{\dgp, \score}(V; \actionvar^b) = \expect[\sigval \sim \dgp(\sigval)]{\ILIV^{\sigval, \dgp, \score}(\sigval; \actionvar^b)}\nonumber
\end{equation}

\subsubsection{ILIV-SHAP Information-based Explanation}

We apply $\ILIV$ to define an \textit{information-based} explanation technique (ILIV-SHAP) that extends the canonical SHAP feature saliency explanation of a model's prediction. ILIV-SHAP communicates how the data features lead to changes in the information value of AI predictions. While traditional SHAP summarizes the average contribution of each feature to a specific prediction over the baseline (prior) prediction, 
ILIV-SHAP summarizes the average contribution of each feature to the decision-relevant information value contained in the prediction. 

% We extend the SHAP algorithms to calculate the effect scores in ILIV-SHAP. 
Suppose a model $f$ that takes as input $m$ features and outputs a real number.
Given an instance $\mathbf{x} = (x_1, \ldots, x_m)$, the importance of one feature $x_i$ to the model output $f(\mathbf{x})$ is encoded by the expected difference of model outputs when $x_i$ is marginalized out.
Specifically, this is quantified by $f(\mathbf{x}) - \expect{f(X) | X_{-i} = \mathbf{x}_{-i}}$, where $X_{-i}$ denotes all features except $X_i$.
Considering the interaction between features, SHAP~\citep{lundberg2017unified} uses the Shapley value to quantify the importance scores averaged on different combinations of features:
\begin{equation*}
    \phi_i(f, \mathbf{x}) = \sum_{\mathbf{x}' \subseteq \mathbf{x}} \frac{|\mathbf{x}'|!(m - |\mathbf{x}'| - 1)!}{m!} [g_f(\mathbf{x}') - g_f(\mathbf{x}' \backslash x_i)]
\end{equation*}
\noindent where $g_f(\mathbf{x}')$ denotes the expected model output conditioned on $\mathbf{x}'$, i.e., $\expect{f(X)| X' = \mathbf{x}'}$ for any $\mathbf{x}' \subseteq \mathbf{x}$. The scores $\phi_i(f, \mathbf{x})$ output by SHAP construct an explanation model for a model output, which quantifies the expected counterfactual change in the model output caused by the feature $x_i$.

ILIV-SHAP extends SHAP to give an explanation model on how features impact the decision-relevant information value of an individual model output.

\begin{definition}[ILIV-SHAP]
\label{def:shap}
Given a model $f$ and data features $\mathbf{x} = (x_1, \ldots, x_m)$, the importance score of the \textit{i}-th feature by ILIV-SHAP is 
\begin{equation*}
    \phi_i^{\ILIV}(f, \mathbf{x}) = \sum_{\mathbf{x}' \subseteq \mathbf{x}} \frac{|\mathbf{x}'|!(m - |\mathbf{x}'| - 1)!}{m!} [\ILIV^{f(\mathbf{x}), \dgp, \score}(g_f(\mathbf{x}'); \actionvar^b) - \ILIV^{f(\mathbf{x}), \dgp, \score}(g_f(\mathbf{x}' \backslash x_i); \actionvar^b)]
\end{equation*}
\end{definition}
\noindent where $\ILIV^{f(\mathbf{x}), \dgp, \score}(g_f(\mathbf{x}'); \actionvar^b)$ denotes a counterfactual evaluation of $\ILIV$, which quantifies the expected payoff gain from additionally knowing $g_f(\mathbf{x}')$ on the instances where $f(\mathbf{x})$ is realized. 
This counterfactual version of $\ILIV$ is guaranteed to achieve the maximum at $\mathbf{x}'$ where $f(\mathbf{x}) = g_f(\mathbf{x}')$, i.e., the features missing from $\mathbf{x}'$ have no impact on $f(\mathbf{x})$.

The explanation model offered by ILIV-SHAP is grounded in the following two properties.
First, $\phi^{\ILIV}_i(f, \mathbf{x})$ are consistent with the extent to which feature $x_i$ contributes to the information value in $f(\mathbf{x})$.
Specifically, for any two models $f$ and $f'$ and any $\mathbf{x}' \subseteq \mathbf{x}$, if $x_i$ contributes more to the information value in $f'$ than the information value in $f$, i.e., $\ILIV^{f'(\mathbf{x}), \dgp, \score}(g_{f'}(\mathbf{x}'); \actionvar^b) - \ILIV^{f'(\mathbf{x}), \dgp, \score}(g_{f'}(\mathbf{x}' \backslash x_i); \actionvar^b) \geq \ILIV^{f(\mathbf{x}), \dgp, \score}(g_{f}(\mathbf{x}');\actionvar^b) - \ILIV^{f(\mathbf{x}), \dgp, \score}(g_{f}(\mathbf{x}' \backslash x_i); \actionvar^b)$, then $\phi^{\ILIV}_i(f', \mathbf{x}) \geq \phi^{\ILIV}_i(f, \mathbf{x})$.
This property of ILIV-SHAP follows the consistency property of SHAP~\citep{lundberg2017unified}.
Second, summing up the scores $\phi^{\ILIV}_i(f, \mathbf{x})$ recovers the information value of model output, $\ILIV^{f(\mathbf{x}), \dgp, \score}(f(\mathbf{x}); \actionvar^b)$.
Formally, for any $f$ and any $\mathbf{x}$, $\ILIV^{f(\mathbf{x}), \dgp, \score}(f(\mathbf{x}); \actionvar^b) = \ILIV^{f(\mathbf{x}), \dgp, \score}(\expect{f(X)}; \actionvar^b) + \sum_{i = 1}^m \phi^{\ILIV}_i(f, \mathbf{x})$.
We demonstrate use of ILIV-SHAP in \Cref{exp3}.


% The information value of the realization $\sig=\sigval$ should be quantified relatively to a counterfactual realization $\sig = \sigval^+$. \jessica{is this not v? can we notate it like that?} Formally, given a decision task with payoff function $\score$ and an information model $\dgp$, the rational DM's expected payoff given the counterfactual signal $\sig = \sigval^+$ on instances where $\sig = \sigval$ is
% \[
% \mathrm{r}^{\sigval, \dgp, \score}(\sigval^+; \sig') = \expect[(\sigval', \payoffstatevalue) \sim \dgp(\sigval', \payoffstatevalue | \sigval)]{\score(\action^r((\sigval^+, \sigval')), \payoffstatevalue)}
% \]

% \begin{proposition}
% \label{prop:counterfactual}
%     For any $\sigval^+ \in \sigsp$, 
%     \[\mathrm{r}^{\sigval, \dgp, \score}(\sigval; \sig') \geq \mathrm{r}^{\sigval, \dgp, \score}(\sigval^+; \sig')\]
% \end{proposition}

% The proof of \Cref{prop:counterfactual} is followed by the fact that any payoff function $\score$ can induce a proper scoring rule $\hat{\score}: \distover{\payoffstatespace} \times \payoffstatespace \rightarrow \mathbb{R}$.


% ILIV-SHAP takes as input a decision task with payoff function $\score$, a model $f$, an instance with features $\mathbf{x} = (x_1, \ldots, x_m)$ and a set of realizations of model predictions, human decisions and state.
% We denote the model predictions by $\action^{f} = f(\mathbf{x})$ and the human decisions by $\action^b$.
% % $[(f(\mathbf{x})_1, \action^b_1, \payoffstatevalue_1), \ldots, (f(\mathbf{x})_m, \action^b_m, \payoffstatevalue_m)]$.
% The construction of ILIV-SHAP proceeds in three steps: 1) Estimate the information model $\dgp(\action^f, \action^b, \payoffstatevalue)$ from the observed realizations, 2) Calculate the factual $\ILIV^{\action^f, \dgp, \score}(\action^f; \actionvar^b)$ and the counterfactual $\ILIV^{\action^f, \dgp, \score}({\action^f}'; \actionvar^b)$ for any ${\action^f}' \neq \action^f$, and 3) Run the SHAP algorithm \citep{lundberg2017unified} to estimate the effect score $\phi^{\ILIV}_i$ for each feature $x_i$ with the objective function as $\ILIV$.

% \begin{algorithm}[tb]
%    \caption{ILIV-SHAP}
%    \label{alg:iliv-shap}
% \begin{algorithmic}
%    \STATE {\bfseries Input:} Payoff function $\score$, model $f$, data features $\mathbf{x}$, a set of realizations $[(\action^f_t, \action^b_t, \payoffstatevalue_t)]_{t=1}^T$
%    \STATE Estimate the empirical distribution $\dgp(\action^f, \action^b, \payoffstatevalue) = \frac{1}{T}\sum_{t=1}^T \mathbb{1}_{\action^f_t = \action^f}\cdot\mathbb{1}_{\action^b_t = \action^b}\cdot\mathbb{1}_{\payoffstatevalue_t = \payoffstatevalue}$, $\forall \action^f, \action^b, \payoffstatevalue$.
%    \STATE Set current signal $\action^f_0 = f(\mathbf{x})$.
%    \STATE Construct function $\ILIV^{\action^f_0, \dgp, \score}(f(\cdot); \actionvar^b)$.
%    \STATE Run SHAP algorithm on function $\ILIV^{\action^f_0, \dgp, \score}(f(\cdot); \actionvar^b)$ to generate $(\phi^{\ILIV}_1, \ldots, \phi^{\ILIV}_m)$.
% \end{algorithmic}
% \end{algorithm}

% Vanilla SHAP defines a \textit{saliency-based} explanation with a set of effect variables $\phi_i$ representing the influence of the realization of basic signal $\basicsig_i = \basicsigval_i$ on the model output $f(\sigval)$, where $\sigval = (\basicsigval_1, \ldots, \basicsigval_n)$ and $f: \sigsp \rightarrow \mathbb{R}$.
% When $\phi_i$ is positive, it means that the realization of $\basicsig_i = \basicsigval_i$ leads the model prediction $f(\sigval)$ to increase $\phi_i$ on the expectation over other basic signals.
% We let $Y$ denote the random variable for model output, $Y = f(\sig)$ for a predictive model $f(\cdot): \sigsp \rightarrow \mathbb{R}$.
% SHAP~\citep{lundberg2017unified} defines a \textit{saliency-based} explanation with a set of \textit{effect variables} $\phi_i$ that represents the influence of the basic signal $\basicsig_i$ on $f(\cdot)$.
% \citet{lundberg2017unified} show that the following set of $\phi$ fulfills properties of \textit{local accuracy}, \textit{missingness} and \textit{consistency}:

% $\sigval' \backslash \basicsigval_i$ denotes removing $\basicsigval_i$ from $\sigval'$.

% \jessica{don't start a sentence with symbols} $(\phi_1^{\ILIV}, \ldots, \phi_m^{\ILIV})$ construct a feasible explanation model that fulfills the properties of \textit{local accuracy}, \textit{missingness} and \textit{consistency} \citep{lundberg2017unified}.
% When $\phi_i^{\ILIV}$ is relatively large, it means that the feature $x_i$ helps the model $f(\cdot)$ extract decision-relevent information that \jessica{complements} $\actionvar^b$. \jessica{Need to convey this intuition we are going for way earlier}
% Otherwise, when $\phi_i^{\ILIV}$ makes a small relative contribution, it means that \jessica{the signal realization} \st{it} makes the model ignore information in that feature \jessica{not sure how to interpret this - makes model ignore information - what's the mechanism behind that?} or the information extracted by the model is already contained in human decisions. 
% \jessica{I'm having trouble following this}

% \Cref{prop:locacc-shap,prop:missingness-shap,prop:consistency-shap} show that the effect variables derived under SHAP fulfill \textit{local accuracy}, \textit{missingness} and \textit{consistency}, which are the basic desirable properties in model explanations.

% \begin{proposition}[Local accuracy, \citealt{lundberg2017unified}]
% \label{prop:locacc-shap}
% For any $f$ and $\sigval \in \sigsp$,
% \begin{equation*}
%     f(\sigval) = \expect{f(\sig)} + \sum_{i=1}^{n} \phi_i(f, \sigval)
% \end{equation*}
% \end{proposition}

% \begin{proposition}[Missingness, \citealt{lundberg2017unified}]
% \label{prop:missingness-shap}
% For any $f$ and $\sigval \in \sigsp$, if $g_{f}(\sigval) = g_{f}(\sigval \backslash \basicsigval_i)$, then $\phi_i(f, \sigval) =0$.
% \end{proposition}

% \begin{proposition}[Consistency, \citealt{lundberg2017unified}]
% \label{prop:consistency-shap}
% For any $\sigval \in \sigsp$ and two models $f$ and $f'$, if for any $\sigval' \subseteq \sigval$,
% \begin{equation*}
%     g_{f'}(\sigval') - g_{f'}(\sigval' \backslash \basicsigval_i) \geq g_{f}(\sigval') - g_{f}(\sigval' \backslash \basicsigval_i)
% \end{equation*}
% then $\phi_i(f', \sigval) \geq \phi_i(f, \sigval)$.
% \end{proposition}

% Extending to the information value, RIIV-SHAP uses $\ILIV^{\dgp, \score}(f(\cdot); \actionvar^b)$ instead of $f(\cdot)$ as the objective function.
% \begin{proposition}[RIIV-SHAP]
% \label{prop:RIIV-shap}
% An explanation model whose effective variable defined as
% \begin{equation*}
%     \phi_i(\ILIV^{\dgp, \score}(f(\cdot); \actionvar^{b}), \sigval)
% \end{equation*}
% where $\phi_i(\cdot, \cdot)$ takes the same form as \Cref{def:shap}, fulfill properties of local accuracy, missingness and consistency for $\ILIV$ of $f(\cdot)$ on $\sigval$.
% \end{proposition}
%  \mvspace{-2mm}

% For example, if $\localsig_\sigval$ indicates whether an AI predicts a particular probability, then $\ILIV$ can serve as an objective in a SHAP-based explanation. 
% Rather than just quantifying changes in the AI’s raw predictions, this approach clarifies how each feature contributes to the additional information value, helping convey when and why the human should trust (or question) a single AI prediction. We demonstrate this use case in \Cref{exp3}.
 \mvspace{-2mm}

\subsection{Robustness Analysis of Information Order}
 \mvspace{-2mm}
%The definition of a decision task requires the identification of a payoff function that evaluates the decisions against the realization of the payoff-related state.
%However, 
Our approach assumes the specification of a decision problem on which agents' decisions and use of information are evaluated. However, 
ambiguity around the appropriate decision problem, and in particular, the appropriate scoring rule, is not uncommon in human-AI decision settings. Ambiguity can arise as a result of challenges in eliciting utility functions and/or variance in these functions across decision-makers or groups of instances; for example, doctors penalize certain false negative results differently when diagnosing younger versus older patients~\citep{mclaughlin2022algorithmic}.
Blackwell's comparison of signals \citep{blackwell1951comparison} is an appropriate tool for addressing ambiguity about the payoff function, as it defines a (set of) signal $\sig_1$ as \textit{more informative} than $\sig_2$ if $\sig_1$ has a higher information value on all possible decision problems. 
We identify this partial order by decomposing the space of decision problems via a basis of proper scoring rules\footnote{For rational DMs, any decision problem can be represented by an equivalent proper scoring rule in \Cref{eq:properscoring}, such that the partial order defined via proper scoring rules also applies to the corresponding decisin tasks.}~\citep{li2022optimization, kleinberg2023u}.

\begin{definition}[Blackwell Order of Information]
    A signal $\sig_1$ is Blackwell more informative than $\sig_2$ if $\sig_1$ achieves a higher best-attainable payoff on any decision problems:
    \begin{equation*}
        \mathrm{R}^{\dgp, \score}
(\sig_1)\geq \mathrm{R}^{\dgp, \score}
(\sig_2), \forall \score
    \end{equation*}
\end{definition}
 \mvspace{-4mm}

The Blackwell order is evaluated over all possible decision problems, which cannot be tested directly.
Fortunately, we only need to test over all proper scoring rules since any decision problem can be represented by an equivalent proper scoring rule, and the space of proper scoring rules can be characterized by a set of V-shaped scoring rules.
A V-shaped scoring rule is parameterized by the kink of the piecewise-linear utility function.


% \begin{definition}
% Given an information model $\dgp$, we define the worst-case information value of $\sig$ as \[WCIV^{\dgp}(\sig) = \inf_{\score}IV^{\dgp, \score}(\sig)\]
% \end{definition}

\begin{definition}(V-shaped scoring rule)
 \label{def:V-shaped score}
 A V-shaped scoring rule with kink $\kink\in (0, \frac{1}{2}]$ is defined as    \begin{equation}
      \score_{\kink} (\action, \payoffstatevalue) = \left\{\begin{array}{cc}
      \frac{1}{2} -\frac{1}{2}\cdot \frac{\payoffstatevalue - \kink}{1-\kink}  &  \text{if }\action\leq \kink\\
        \frac{1}{2} +\frac{1}{2}\cdot \frac{\payoffstatevalue - \kink}{1-\kink}    & \text{else},
      \end{array}
      \right.\nonumber
   \end{equation}

When $\kink'\in (\frac{1}{2}, 1)$, the V-shaped scoring rule can be symmetrically defined by $\score_{\kink'} = \score_{1-\kink'}(1-\pred, \payoffstatevalue)$.
\end{definition}

 \mvspace{-2mm}

Intuitively, the kink $\kink$ represents the threshold belief where the decision-maker switches between two actions.
Larger $\mu$ means that the decision-makers will prefer $\action = 1$ more. The closer $\mu$ is to $0.5$, the more indifferent the decision-maker is to $\action = 0$ or $\action = 1$.

\Cref{prop: blackwell-V-test} shows that if $\sig_1$ achieves a higher information value on the basis of V-shaped proper scoring rules than $\sig_2$, then $\sig_1$ is Blackwell more informative than $\sig_2$. \Cref{prop: blackwell-V-test} follows from the fact that any best-responding payoff can be linearly decomposed into the payoff on V-shaped scoring rules. 

\begin{proposition}[\citealt{hu2024predict}]
\label{prop: blackwell-V-test}
If \(\forall \kink\in (0, 1)\) \begin{equation*}
    \mathrm{R}^{\dgp, \score_\kink}
(\sig_1)\geq \mathrm{R}^{\dgp, \score_\kink}
(\sig_2),
\end{equation*}
then $\sig_1$ is Blackwell more informative than $\sig_2$.
\end{proposition}

 \mvspace{-2mm}

Extending this to agent-complementary information value, we say that $\sig_1$ offers a higher complementary value than $\sig_2$ under the Blackwell order if 
\[\ACIV^{\dgp, \score_\kink}(\sig_1; \actionvar^b) \geq \ACIV^{\dgp, \score_\kink}(\sig_2; \actionvar^b), \forall \kink\in(0,1)\]
This definition allows us to rank signals (or sets of signals) without needing to commit to a specific payoff function. 
We present a use case in \Cref{exp2}.
