\section{Discussion}

\subsection{Post-Hoc Interpretability in Deep RL}

Post hoc interpretability in Deep Reinforcement Learning (DRL) is an increasingly important field, with methods such as saliency maps already being used to visualize agent behaviour \cite{Greydanus2017VisualizingAU}, debug learned concepts \cite{Jaderberg2018HumanlevelPI, Hilton2020UnderstandingRV}, and inform sampling strategies to improve efficiency \cite{Bertoin2022LookWY}. Other approaches analyse agent behaviour by querying interaction data \cite{Sequeira2019InterestingnessEF} or by visualising pattern prototypes \cite{Ragodos2022ProtoXEA,Aliciolu2024UseBA}.  More extensive efforts have focused on interpreting well-known chess engines like AlphaZero \cite{McGrath_2022,lovering2022evaluation,hammersborg2023information,schut2023bridging,jenner2024evidencelearnedlookaheadchessplaying,Poupart2024ContrastiveSA} and Stockfish \cite{palsson2023unveiling}, providing valuable insights into learned strategies. Ongoing efforts are also focused on exposing the key mechanisms behind planning, especially with games as a testbed \cite{Jenner2024EvidenceOL,Taufeeque2024PlanningIA,Guei2024InterpretingTL,Chung2024PredictingFA}.

Other post hoc methods, like policy distillation into interpretable models, often referred to as model extraction, have also been a central focus. Techniques such as DAGGER \cite{Ross2010ARO} and VIPER \cite{Bastani2018VerifiableRL} leverage imitation learning to simplify policies. However, these methods struggle to scale effectively when applied globally to complex models, limiting their applicability to large-scale systems.


\subsection{Interpretability in MADRL}

Interpretability in MADRL is an evolving field with several promising approaches. Shapley values have been widely applied to analyse individual agent contributions, providing a robust theoretical framework for evaluating each agentâ€™s influence on team performance \cite{Heuillet2021CollectiveEA,Wang2021SHAQIS,Mahjoub2023EfficientlyQI}. Diversity measures of agent policies have also emerged as a valuable tool for understanding agent behaviour, revealing distinctions between individual strategies and their roles in collective dynamics \cite{Khlifi2023OnDF}.

Similarly to XRL, policy extraction techniques, such as VIPER \cite{Bastani2018VerifiableRL}, have been extended to leverage MADRL training to distil interpretable policies from complex models \cite{Milani2022MAVIPERLD}. Furthermore, predicting high-level concepts instead of actions offers a novel pathway to intrinsically interpretable models, aligning model outputs with human-understandable abstractions \cite{Zabounidis2023ConceptLF}. These advancements highlight the growing potential of interpretability methods in uncovering insights into multi-agent behaviour and learning processes.

\subsection{Limits of Intrinsically Interpretable Models}

Intrinsically interpretable models, whether obtained by design or post hoc extraction, have long been a dominant paradigm in agent interpretability research, relying on predefined, transparent model architectures. Design frameworks like XAg \cite{rodriguez2024explainable}, concept bottlenecks \cite{Poeta2023ConceptbasedEA}, learning skills with decision trees \cite{Wen2024SkillTreeES}, or learning modularised agents \cite{Cloud2024GradientRM}, aim to embed interpretability directly into model structures. However, such approaches face challenges in scalability and flexibility, particularly in multi-agent settings or with complex DRL models like the latest pre-trained world models \cite{Reed2022AGA,Yang2023FoundationMF, alonso2024diffusionworldmodelingvisual, Bruce2024GenieGI}. The rigidity of design-based interpretability often compromises performance and fails to capture emergent behaviours, highlighting the need for alternative approaches that can adapt to the complexity and scale of modern systems \cite{Madsen2024InterpretabilityNA}. New hybrid paradigms like Wrapper Boxes \cite{Su2023InterpretableBD}, might be required to overcome those limitations.
