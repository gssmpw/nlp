\section{Advocating Direct Interpretability}
\label{sec:advocate}

Direct methods offer a significant advantage in their applicability to models during and after training, enabling developers to analyse and interpret complex systems without requiring architectural changes. This flexibility makes them particularly suitable for MADRL systems compared to intrinsic methods that might be challenging to scale with several agents. Figure~\ref{fig:xmadrl} outlines speculative research directions and methodologies that can enhance systems understanding at different levels, from individual agents to the overall training process.


\subsection{Single-Agent Challenges}

To understand agents trained using MADRL, we can study each agent independently.
Methods drawn from XRL and general interpretability are thus directly applicable to tackle single-agent challenges.

\paragraph{Biases identification:}eliciting models' biases learned during training. In order to debug those "Clever Hans"\footnote{Cognitive bias that was learned due to spurious correlations, see \cite{Lapuschkin2019UnmaskingCH}.}, it is possible to use feature importance techniques, described in Section~\ref{sec:background_interp}. Previous work \cite{Lapuschkin2019UnmaskingCH} showed that this debugging could be semi-automated by combining LRP \cite{Bach2015OnPE} with spectral clustering \cite{Luxburg2007ATO}.  While these methods are relatively established in XRL, further improvements tailored to MADRL could offer more context-specific explanations, e.g., by comparing different agents' perceptions.

\paragraph{Policy distillation,} converting a model into a simpler one, can be achieved by training a new smaller model \cite{Rusu2015PolicyD}, or by extracting intrinsically interpretable models \cite{Ross2010ARO,Bastani2018VerifiableRL}, even for MADRL \cite{Milani2022MAVIPERLD}. Yet, these distillation methods are computationally expensive. Recent works proposed network compression based on interpretability, using weights relevance \cite{Yeom2019PruningBE} or circuit analysis
\cite{Pochinkov2024DissectingLM}.

\paragraph{Decision decomposition:}
could be achieved by internally decomposing an agent's decision into functional modules or representations. This methodology was proven efficient to elicit the algorithms behind certain capabilities, like addition or modular addition \cite{Quirke2023UnderstandingAI,Nanda2023ProgressMF}. Future work could focus on extracting different circuits using ACDC \cite{conmy2023automated} to analyse simple shared actor-critic architectures, e.g., to extract the actor subnetwork.


\paragraph{Policy edition,}an essential aspect to regain control over DNNs. Indeed, being able to edit a trained policy is essential to remove biases, unwanted associations or dangerous behaviours without needing to retrain the model.
In this respect, direct interpretability is perfectly suited for the task with methods leveraging CAVs \cite{Dreyer2023FromHT} or causal
tracing \cite{Meng2022LocatingAE}.

\subsection{Multi-Agent Challenges}

Interpretability could be a powerful tool for automating the oversight of systems involving multiple agents.
Indeed, such systems become more complex through inter-agent interactions, coordination strategies, and emergent behaviours.

\paragraph{Team identification:} grouping together agents with similar roles or policies. This is particularly interesting to reduce the complexity of MAS by having fewer agents to train or could be an avenue to extend the mean-field framework \cite{Yang2018MeanFM}. Previous work showed that selective parameter-sharing can be based on latent spaces \cite{Christianos2021ScalingMR}. Further improvements could consider dynamic teams throughout learning by analysing mixing networks \cite{Rashid2018QMIXMV}, e.g., by partitioning the positive weights using NMF \cite{Paatero1994PositiveMF}, or using other prototype methods like SAE \cite{Cunningham2023SparseAF}.

\paragraph{Agent contribution,}or agent credit assignment, is a well-known challenge introduced by MAS. Shapley values theoretically give the individual agent contributions \cite{Shapley1988AVF}, and thus can be computed using SHAP or equivalent methods \cite{Lundberg2017AUA,Heuillet2021CollectiveEA,Wang2021SHAQIS}. Yet, as it can be expensive to compute, it might be beneficial to explore other versatile methods like LRP \cite{Bach2015OnPE}, e.g., by designing specific relevance propagation rules.

\paragraph{Communication monitoring} In settings with natural language communication between agents, leveraging LLMs or pre-trained models can enable a seamless integration \cite{Zhu2022ASO}. Yet, these models are highly opaque and would benefit from interpretability, offering an avenue to supervise and interpret conversations. Applications could make use of feature importance methods, like AttnLRP \cite{Achtibat2024AttnLRPAL}, to spot key information used in the agent prediction. 

\paragraph{Communication decoding}
For learned communication analyses, it becomes harder and might be reduced to finding patterns or comparing and aligning latent spaces to spot similar messages between agents.
In order to uncover how agents derive meaning from these interactions, causal interventions might 
yields interesting hindsights \cite{Kramar2024AtPAE}. 


\paragraph{Swarm coordination:} an inherent challenge of MAS that becomes increasingly complex as the number of agents scales. Fortunately, modern direct interpretability offers means to control models using methods from representation engineering \cite{zou2023representation}, like activation steering \cite{rimsky2023steering}. The latter method has proven useful to control an agent's policy by favouring different goals \cite{Mini2023UnderstandingAC}.
Further application to MADRL could improve swarm coordination by enhancing traits like cooperativeness or better distributing goals among agents, e.g., by alternating resource collection among sites and agents.


\subsection{Training Process Challenges}
Training multiple agents simultaneously demands more computing power and can lead to learning instabilities.
Therefore, it is crucial to better understand the training process of MADRL at different levels by improving learning efficiency and ensuring robustness. 

\paragraph{State analysis.}In order to model complex environments, one can train world models \cite{Bruce2024GenieGI}, later used by an agent \cite{Hafner2023MasteringDD}. 
The condensed latent representation obtained can be analysed \cite{Ivanitskiy2023StructuredWR}
with tools like the tuned lens \cite{Belrose2023ElicitingLP}. This framework offers a better view of the transition function, which could help guide the agents towards unbiased training if analysed thoroughly.

\paragraph{Reward decomposition:} often achieved by learning separate value functions aggregated afterwards \cite{Seijen2017HybridRA,Juozapaitis2019ExplainableRL}. 
To avoid arbitrary decompositions, one could rely on local backpropagation methods like LRP or CRP \cite{Bach2015OnPE,Achtibat2022FromAM}, enabling the discovery of concepts that can later clarify the influence of the reward on the learning process of a policy.
Further improvements could consider generating an adaptative curriculum \cite{Jiang2020PrioritizedLR}, prioritizing the concepts to learn.

\paragraph{Priority sampling,} a staple method in RL that improves sample efficiency \cite{Schaul2015PrioritizedER}. 
Also, in RL, interpretability was proven efficient to prioritize the important pixels for a visual policy by means of a consistency loss \cite{Bertoin2022LookWY}. Such a framework could be extended to compute importance over multiple inputs, creating a metric for better eliciting shared critical training samples.  

\paragraph{Learning dynamics:} trying to understand the agents throughout training, e.g., by observing the trained policies. Yet, it becomes more complicated as the number of agents scales and requires automated methods beyond observing policies. A widely used method to detect learned concepts in a model is to train linear probes
\cite{alain2018understanding}, which gave valuable insights for the analysis of AlphaZero networks \cite{McGrath_2022}.
By monitoring each agent, it would be possible to gain a more nuanced understanding of the swarm development and track the emergence or disappearance of certain capabilities. 


\paragraph{Experience sharing:} a method introduced to scale MADRL by improving sample efficiency \cite{Christianos2020SharedEA}. Further improvements shared the data selectively according to exploration metrics \cite{Gerstgrasser2023SelectivelySE}. Yet, this framework is missing a key point: you might want to select agents that share their experience similarly to parameter sharing \cite{Christianos2021ScalingMR}. A naive method could be to cluster experiences based on some latent representation of the different agents, enabling efficient knowledge sharing \cite{zou2023representation}.


% \begin{table}[t]
%  \begin{center}
%    % \tabcolsep = 2\tabcolsep
%    \resizebox{\linewidth}{!}{
%    \begin{tabular}{lll}
%    \toprule
%    \textbf{Category} & \textbf{Application}   & \textbf{Related Works} \\
%    \midrule
%    \multirow[c]{3}{*}{Single-agent interpretability} &
%    Biases identification & \cite{Bach2015OnPE,Lapuschkin2019UnmaskingCH}  \\
%    & Policy distillation & \cite{Yeom2019PruningBE,Pochinkov2024DissectingLM} \\
%    & Decision decomposition & \cite{conmy2023automated} \\
%    & Policy edition & \cite{Dreyer2023FromHT,Meng2022LocatingAE} \\
%    \midrule
%    \multirow[c]{5}{*}{Multi-agent interpretability} &
%    Team identification & \cite{Christianos2021ScalingMR} \\
%    & Agent contribution & \cite{Heuillet2021CollectiveEA, Bach2015OnPE} \\
%    & Communication monitoring & \cite{Achtibat2024AttnLRPAL} \\
%    & Communication decoding & \cite{Kramar2024AtPAE} \\
%    & Swarm coordination & \cite{rimsky2023steering} \\
%    \midrule
%    \multirow[c]{5}{*}{Training process interpretability} 
%    & State analysis & \cite{Cunningham2023SparseAF} \\
%    & Reward decomposition & \cite{Achtibat2022FromAM} \\
%    & Priority sampling & \cite{Schaul2015PrioritizedER,Bertoin2022LookWY} \\
%    & Learning dynamics & \cite{alain2018understanding,McGrath_2022} \\
%    & Experience sharing & \cite{Christianos2020SharedEA, Gerstgrasser2023SelectivelySE,Christianos2021ScalingMR, zou2023representation} \\
%    \bottomrule
%    \end{tabular}
%    }
% \caption{Categorisation of potential research tracks in MADRL that might benefit from direct interpretability.} \label{tab:advocate}
%  \end{center}
% \end{table}


