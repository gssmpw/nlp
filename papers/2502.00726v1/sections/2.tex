\section{Background}

\subsection{Multi-Agent Deep Reinforcement Learning}
 A typical system consists of the following components: agents, an environment, and a training algorithm, as depicted in Figure~\ref{fig:madrl_system}. Formally, we consider a system with $N$ agents, each indexed by $i \in \{1, \dots, N\}$. At each time step, the agent $i$ is presented with an observation $o_i$ and produces an action $a_i$. For the sake of generality, we included a possible communication channel $c_i$, seeing that it is increasingly used \cite{Zhu2022ASO}. In principle, we can extend the definition of communication to include the most common MADRL methods like parameter sharing \cite{Gupta2017CooperativeMC,Chu2017ParameterSD}, which can be seen as a form of latent space communication. Finally, the training algorithm provides feedback $\nabla_i$ to each agent.

Training algorithms in MADRL can be centralized, decentralized, or hybrid. Centralized training uses the joint action $a=(a_1,...,a_N)$ and the state $s$, which can be understood as an observation augmented by information at training time \cite{Lambrechts2023InformedPL}, and consists of applying classical RL to multi-agent problems like for AplhaStar \cite{Mathieu2023AlphaStarUL}. While decentralized training restricts each agent to local observations $o_i$, possibly including a local reward $r_i$, see IDQN \cite{Tampuu2015MultiagentCA} or IPPO \cite{Yu2021TheSE}. Hybrid approaches, such as centralized training with decentralized execution, leverage global information during training but allow agents to act independently using only local observations during execution, see VDN \cite{Sunehag2017ValueDecompositionNF}, QMIX \cite{Rashid2018QMIXMV}, MADPG \cite{Lowe2017MultiAgentAF} or MAPPO \cite{Yu2021TheSE}. Here, we consider agents based on DNNs; therefore, the feedbacks $\nabla_i$ are gradients of a loss $\ell$. Depending on the training algorithm, this loss can be a function of the reward $r$, the state $s$, the actions $a_i$, the observations $o_i$ and the communications $c_i$. For simplicity, we didn't include those dependencies in Figure~\ref{fig:madrl_system}.
 
\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/MADRL.pdf}
    \caption{Schema of a simplified view of MADRL systems. At each time step, the agent $i$ receives the initial observation $o_i$, complemented by potential communications $c_i$ and produces an action $a_i$. The agent learns throughout training by the means of gradients $\nabla_i$. }
    \label{fig:madrl_system}
\end{figure}


\subsection{Direct Interpretability of DNNs}
\label{sec:background_interp}

We now present an overview of the modern methods widely used to interpret DNNs in Computer Vision (CV) and Natural Language Processing (NLP). As these domains heavily relied on pre-trained models \cite{Simonyan2014VeryDC,He2015DeepRL, Radford2018ImprovingLU}, direct post-hoc methods have dominated the research landscape, providing key hindsight without altering models' architectures.


\paragraph{Feature importance.} Typical methods used in CV to understand convolutional networks involve visualising important pixels, i.e. saliency maps, \cite{Zeiler2013VisualizingAU,Selvaraju2016GradCAMVE}. Other methods compute importance by perturbing the input \cite{Covert2020ExplainingBR}, using the gradients \cite{Radford2015UnsupervisedRL,Selvaraju2016GradCAMVE,Shrikumar2016NotJA, Smilkov2017SmoothGradRN} or locally decomposing relevance \cite{Montavon2015ExplainingNC,Bach2015OnPE}. Recent works in NLP focus on the Transformer architecture and its attention mechanism \cite{Vaswani2017AttentionIA}, providing token-level insights \cite{Wiegreffe2019AttentionIN,Achtibat2024AttnLRPAL}. 


\paragraph{Prototypes:} a class of methods that creates explanations based on characteristic samples. In CV, it is common to analyse neurons using activation maximisation to create pre-images \cite{Mahendran2015VisualizingDC}, or find related images \cite{Chen2020ConceptWF}. Prototypes can be of various forms like perturbed images \cite{Ribeiro2018AnchorsHM}, cropped images \cite{Dreyer2023UnderstandingT} or latent space vector \cite{alain2018understanding,kim2018interpretability}. Recent works based on sparse autoencoders were able to elicit interpretable features in LLMs, i.e., prototypes \cite{Cunningham2023SparseAF}.

\paragraph{Latent manipulation:} techniques that further extend the interpretability of concepts and features by exploring the internal representations learned by models. These methods were introduced in CV with \cite{kim2018interpretability}, later derived as the field of representation engineering \cite{zou2023representation}. Such latent features enable locating, editing, erasing or decoding models' knowledge \cite{Meng2022LocatingAE,belrose2023leace, Ghandeharioun2024PatchscopesAU}, but causally modify or analyse the produced outputs \cite{rimsky2023steering, Kramar2024AtPAE}.

\paragraph{Circuit analysis:} provides a more granular understanding of model internals by examining pathways and dependencies between models' components, usually neurons or attention heads. Circuits were first discovered in CNNs \cite{Olah2020ZoomIA} before being formalised for Transformers \cite{elhage2021mathematical}.  These circuits revealed peculiar models' components that learned precise mechanisms like induction \cite{Olsson2022IncontextLA}. Using specific datasets, relevant circuits can be automatically discovered \cite{conmy2023automated}. More recent works focus on larger models' components at the layer scale \cite{Dunefsky2024TranscodersFI}.


% \begin{table}[H]
%  \begin{center}
%    % \tabcolsep = 2\tabcolsep
%    \begin{tabular}{ll}
%    \toprule
%    \textbf{Methodology} & \textbf{Related Works} \\
%    \midrule
%    Feature Importance & \cite{Zeiler2013VisualizingAU,Selvaraju2016GradCAMVE, Lundberg2017AUA, Bach2015OnPE,Radford2015UnsupervisedRL, Covert2020ExplainingBR, Montavon2015ExplainingNC, Achtibat2024AttnLRPAL, Smilkov2017SmoothGradRN, Wiegreffe2019AttentionIN} \\
%    %Ribeiro2016WhySI
%    %Katz2024BackwardLP
%    Prototypes   & \cite{Ribeiro2018AnchorsHM,Achtibat2022FromAM, Chen2020ConceptWF, Mahendran2015VisualizingDC, alain2018understanding,Cunningham2023SparseAF} \\
%    %Dreyer2023FromHT
%    %bills2023language
%    %Dar2022AnalyzingTI
%    Latent Manipulations & \cite{kim2018interpretability,Meng2022LocatingAE,zou2023representation,rimsky2023steering,belrose2023leace,Kramar2024AtPAE,Ghandeharioun2024PatchscopesAU} \\
%    Circuit Analysis          & \cite{Olah2020ZoomIA,elhage2021mathematical,Olsson2022IncontextLA,conmy2023automated, Dunefsky2024TranscodersFI}\\
%    \bottomrule
%    \end{tabular}
% \caption{Categorisation of modern direct interpretability methods drawn from CV and NLP domains.} \label{tab:interp_methods}
%  \end{center}
% \end{table}



