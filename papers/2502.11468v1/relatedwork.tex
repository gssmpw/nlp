\section{Related Works}
\label{sec2}
\subsection{Generative Model}\label{sec2.1}
Generating real-like images has been one of the essential topics in computer vision. In the last decade, some important works have been proposed according to various philosophical ideas, typical as using adversarial networks \cite{Goodfellow:NIPS2014}, approximating the actual distribution by variational auto-encoder\cite{Kingma:ICLR2014}, and applying direct mapping between source and target distributions\cite{Rezende:ICML2015}. More details about these generative models can be referred to \cite{Xia:TPAMI2023,DBLP:Tschannen2018,Ivan:NF}. These outstanding works and their varieties significantly promote vital studies of image translation. Primarily, we are inspired by the ideas of \cite{Liu:NIPS2017,Liu:NIPS2016,Yusuf:CMSNET}, which generate the translated images from a shared latent space using a weight-sharing constraint. Unlike them, which try to generate multimodal outputs, our method aims to produce deterministic output depending on the paired images.

\subsection{Image-to-Image Translation}\label{sec2.2}
Pix2Pix\cite{Isola:CVPR2017} and Cycle-GAN\cite{Zhu:ICCV2017} will be briefly introduced and taken as the basis of supervision and semantic preservation. Because almost all works use GAN loss as a part of their objective, we will focus on the extra loss and constraints.

Pix2Pix uses the paired images to train the model, and the $L1$ loss between translated and actual images is taken as an extra constraint. This compels translated images in the same way as the target, and skip connections of the generator enhance this effect at the pixel level. In addition, the noise vector is a dropout, so its capacity to produce diverse outputs is limited. Aiming at the above questions, Cycle-GAN uses domain-set images instead of paired images to train two translation models jointly. The cycle loss is exploited to incentivize mapping an individual image to the desired output.

Considering combining both advantages of Pix2Pix and Cycle-GAN, Cycada\cite{Hoffman:ICML2018} uses paired images as input and explores cycle-consistent adversarial networks. Moreover, it uses an extra classification task to aid semantic preservation. Unlike Cycle-GAN, UNIT\cite{Liu:NIPS2017} also uses the cycle translation but divides its networks into the parts of VAE and GAN and uses a weight-sharing constraint between two VAE models in their high representation layers. In addition to the loss of VAE and GAN, an extra cycle loss based on KL divergence is computed to ensure the latent space alignment. However, according to the reports of UNIT, the sharing weights contribute little to the performance compared to using cycle-consistence alone. According to our experiments, it is mainly because the images from two domains may deviate too much in latent space. Moreover, UNIT appends one noise vector to a feature vector extracted by the encoder to produce diverse outputs. Discarding the sharing weights constraint of UNIT, MUNIT\cite{Huang:ECCV2018} uses two networks of content and style encoder to produce multimodal outputs, and the corresponding loss items of content and style are used as extra constraints. In addition, more techniques, such as attention\cite{Mejjati:NIPS2018}, adaptive normalization\cite{DBLP:Kim2019}, disentangled representations\cite{Lee:DRITplusplus}, and marginal prior knowledge\cite{You:BCCGAN}, are used to improve translation performance.

In general, the above works show that cycle consistency is helpful for individual images to maintain their semantics. However, it often becomes invalid, as \cite{Fu:CVPR2019,Jia:ICCV2021,Yang:CVPR2020,Gong:GAN} reported. The reason may be that cycle consistency constraints cannot fully alleviate the difference between two distributions of images or domains. Moreover, diversity and semantic preservation are contrary goals for unsupervised image translation. Many works\cite{Zhu:ICCV2017,Liu:NIPS2017,Huang:ECCV2018,Mejjati:NIPS2018,DBLP:Kim2019,Lee:DRITplusplus,You:BCCGAN} try to achieve both goals concurrently. Semantic preservation is essential for applications such as remote sensing and needs further research. Unlike the above methods, our method tries to generate the deterministic output and let its semantics be the same as the source.

\subsection{Style Transfer}\label{sec2.3}
Image-to-image translation has strong relations with style transfer. The work of \cite{Gatys:CVPR2016} firstly applied CNN to style transfer, and then many works focused on normalization layers to carry out style transfer. AdaIN\cite{Huang:ICCV2017} and its varieties are often integrated into image translation\cite{Huang:ECCV2018,DBLP:Kim2019,Park:CVPR2019,Jiang:ECCV2020}. In MUNIT\cite{Huang:ECCV2018}, style encoders extract style features of a specific style image and apply them to the content image in AdaIN. Aiming at the question of the normalization layers of 'wash away' semantic information, SPADE\cite{Park:CVPR2019} integrates AdaIN into the Pix2Pix model. It uses a CNN network to extract style information from a pre-defined mask. In \cite{Jiang:ECCV2020}, the style and content features from the style and images are applied in each layer to implement the feature adaptive normalization and denormalization, respectively. These methods fit for the translation of RS images. However, balancing the style and content features in translated images is difficult. Experiments show that strange textures often haunt the transferred images, and some areas are changed.

\subsection{Semantic Robust Image Translation}\label{sec2.4}
Although more attention is paid to producing real-like multimodal outputs in image translation fields, preserving semantics still attracts lots of attention. Unlike Cycle-GAN, which uses cycle consistency, Distance-GAN\cite{Benaim:NIPS2017} uses an extra loss of absolute difference between the distances of paired images in each domain. If there is no access to paired images, an alternative of self-distance can be employed\cite{Benaim:NIPS2017}. Distance-GAN is especially fit for one-sided domain mapping. In \cite{Fu:CVPR2019}, some geometry transformations are applied to the images. The original image and its transformations are all translated into the target domain. Extra loss named geometry consistency is computed between the translated image and its inverse transformations. Like \cite{Benaim:NIPS2017}, it is also used for one-sided domain mapping and performs well in preserving semantics. However, good performance is at the cost of extra models. Each transformation needs one discriminator. Inspired by visual psychophysics, \cite{Yang:CVPR2020} integrates the phase consistency constraints into the domain mapping for semantic preservation. However, our experiments show that only using the phase-consistency constraint in RS images may lead to artifacts in the faked image. In addition, \cite{Jia:ICCV2021} studies the question of semantic robustness under the frame of contrastive learning\cite{Park:ECCV2020}, and semantic robustness is acquired by making the semantics of translated output invariant to slight feature space variations of the inputs.

Our method is different from the above works in three aspects:
\begin{itemize}
\item We aim at unsupervised domain mapping with access to paired images. In order to fully utilize this advantage of paired images, SRUIT exploits the shared-weights constraint. 
\item A cross-cycle consistency constraint ensures semantic preservation for the paired images from the same land location and shares latent space. 
\item Our method jointly trains target and source domain generators and demands no other networks and tasks as help.
\end{itemize}