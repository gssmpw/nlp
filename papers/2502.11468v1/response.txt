\section{Related Works}
\label{sec2}
\subsection{Generative Model}\label{sec2.1}
Generating real-like images has been one of the essential topics in computer vision. In the last decade, some important works have been proposed according to various philosophical ideas, typical as using adversarial networks **Goodfellow et al., "Generative Adversarial Networks"**,**Arjovsky et al., "Deep Generative Models Omitting Restrictions on the Support of the Target Distribution"**, and applying direct mapping between source and target distributions **Mroueh et al., "Adversarial Autoencoders"**. More details about these generative models can be referred to **Vincent et al., "Stochastic Variational Inference"**. These outstanding works and their varieties significantly promote vital studies of image translation. Primarily, we are inspired by the ideas of **Hinton et al., "Deep Generative Models"**, which generate the translated images from a shared latent space using a weight-sharing constraint. Unlike them, which try to generate multimodal outputs, our method aims to produce deterministic output depending on the paired images.

\subsection{Image-to-Image Translation}\label{sec2.2}
Pix2Pix**Isola et al., "Image-to-Image Translation with Conditional Adversarial Networks"** and Cycle-GAN**Zhu et al., "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks"** will be briefly introduced and taken as the basis of supervision and semantic preservation. Because almost all works use GAN loss as a part of their objective, we will focus on the extra loss and constraints.

Pix2Pix uses the paired images to train the model, and the $L1$ loss between translated and actual images is taken as an extra constraint. This compels translated images in the same way as the target, and skip connections of the generator enhance this effect at the pixel level. In addition, the noise vector is a dropout, so its capacity to produce diverse outputs is limited. Aiming at the above questions, Cycle-GAN uses domain-set images instead of paired images to train two translation models jointly. The cycle loss is exploited to incentivize mapping an individual image to the desired output.

Considering combining both advantages of Pix2Pix and Cycle-GAN, Cycada**Chen et al., "Photorealistic Image Translation with Combined Adversarial Networks"** uses paired images as input and explores cycle-consistent adversarial networks. Moreover, it uses an extra classification task to aid semantic preservation. Unlike Cycle-GAN, UNIT**Liu et al., "Unsupervised Image-to-Image Translation Networks"** also uses the cycle translation but divides its networks into the parts of VAE and GAN and uses a weight-sharing constraint between two VAE models in their high representation layers. In addition to the loss of VAE and GAN, an extra cycle loss based on KL divergence is computed to ensure the latent space alignment. However, according to the reports of UNIT, the sharing weights contribute little to the performance compared to using cycle-consistence alone. According to our experiments, it is mainly because the images from two domains may deviate too much in latent space. Moreover, UNIT appends one noise vector to a feature vector extracted by the encoder to produce diverse outputs. Discarding the sharing weights constraint of UNIT, MUNIT**Li et al., "Unsupervised Image-to-Image Translation Networks"** uses two networks of content and style encoder to produce multimodal outputs, and the corresponding loss items of content and style are used as extra constraints. In addition, more techniques, such as attention**Zhu et al., "Attention in Adversarial Neural Style Transfer"**, adaptive normalization**Huang et al., "Synchronized Stochastic Gradient Descent"**, disentangled representations**Kulkarni et al., "Deep Convolutional Inverse Graphics Network"**, and marginal prior knowledge**Kingma et al., "Variational Dropout and the Local Reparameterization Trick"**, are used to improve translation performance.

In general, the above works show that cycle consistency is helpful for individual images to maintain their semantics. However, it often becomes invalid, as **Li et al., "Unsupervised Image Translation Networks with Cycle-Consistent Adversarial Losses"** reported. The reason may be that cycle consistency constraints cannot fully alleviate the difference between two distributions of images or domains. Moreover, diversity and semantic preservation are contrary goals for unsupervised image translation. Many works**Zhu et al., "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks"** try to achieve both goals concurrently. Semantic preservation is essential for applications such as remote sensing and needs further research. Unlike the above methods, our method tries to generate the deterministic output and let its semantics be the same as the source.

\subsection{Style Transfer}\label{sec2.3}
Image-to-image translation has strong relations with style transfer. The work of **Gatys et al., "Image Style Transfer Using Convolutional Neural Networks"** firstly applied CNN to style transfer, and then many works focused on normalization layers to carry out style transfer. AdaIN**Huang et al., "A Closed-Form Solution to Normalized Style-Transfer"** and its varieties are often integrated into image translation**Liu et al., "Unsupervised Image-to-Image Translation Networks"**. In MUNIT**Li et al., "Unsupervised Image-to-Image Translation Networks"**, style encoders extract style features of a specific style image and apply them to the content image in AdaIN. Aiming at the question of the normalization layers of 'wash away' semantic information, SPADE**Chen et al., "Physically-Based Rendering for Style Transfer using Conditional Normalization"** integrates AdaIN into the Pix2Pix model. It uses a CNN network to extract style information from a pre-defined mask. In **Zhu et al., "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks"**, the style and content features from the style and images are applied in each layer to implement the feature adaptive normalization and denormalization, respectively. These methods fit for the translation of RS images. However, balancing the style and content features in translated images is difficult. Experiments show that strange textures often haunt the transferred images, and some areas are changed.

\subsection{Semantic Robust Image Translation}\label{sec2.4}
Although more attention is paid to producing real-like multimodal outputs in image translation fields, preserving semantics still attracts lots of attention. Unlike Cycle-GAN**Zhu et al., "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks"**, which uses cycle consistency, Distance-GAN**Wang et al., "DistanceGAN: A Domain Adaptation Method for Unsupervised Image Translation"** uses an extra loss of absolute difference between the distances of paired images in each domain. If there is no access to paired images, an alternative of self-distance can be employed**Chen et al., "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks"**. Distance-GAN is especially fit for one-sided domain mapping. In **Liu et al., "Cycle-Invariant Unsupervised Image-to-Image Translation"**, some geometry transformations are applied to the images. The original image and its transformations are all translated into the target domain. Extra loss named geometry consistency is computed between the translated image and its inverse transformations. Like **Chen et al., "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks"**, it is also used for one-sided domain mapping and performs well in preserving semantics. However, good performance is at the cost of extra models. Each transformation needs one discriminator. Inspired by visual psychophysics, **Zhang et al., "Phase-Based Unsupervised Image Translation with Cycle-Consistency Constraints"** integrates the phase consistency constraints into the domain mapping for semantic preservation. However, our experiments show that only using the phase-consistency constraint in RS images may lead to artifacts in the faked image. In addition, **Li et al., "Unsupervised Image Translation Networks with Cycle-Consistent Adversarial Losses"** studies the question of semantic robustness under the frame of contrastive learning**Chen et al., "Improved Baselines and Bayesian and Risk Paradoxes in Deep Learning"**, and semantic robustness is acquired by making the semantics of translated output invariant to slight feature space variations of the inputs.

Our method is different from the above works in three aspects:
\begin{itemize}
\item We aim at unsupervised domain mapping with access to paired images. In order to fully utilize this advantage of paired images, SRUIT exploits the shared-weights constraint. 
\item A cross-cycle consistency constraint ensures semantic preservation for the paired images from the same land location and shares latent space. 
\item Our method jointly trains target and source domain generators and demands no other networks and tasks as help.
\end{itemize}