%Version 3.1 December 2024
% See section 11 of the User Manual for version history
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%=========================================================================================%%
%% the documentclass is set to pdflatex as default. You can delete it if not appropriate.  %%
%%=========================================================================================%%

%%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove Numbered in the optional parenthesis. 
%%The option is available for: sn-basic.bst, sn-chicago.bst%  
 
%%\documentclass[pdflatex,sn-nature]{sn-jnl}% Style for submissions to Nature Portfolio journals
%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
\documentclass[pdflatex,sn-mathphys-num]{sn-jnl}% Math and Physical Sciences Numbered Reference Style
%%\documentclass[pdflatex,sn-mathphys-ay]{sn-jnl}% Math and Physical Sciences Author Year Reference Style
%%\documentclass[pdflatex,sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[pdflatex,sn-vancouver-num]{sn-jnl}% Vancouver Numbered Reference Style
%%\documentclass[pdflatex,sn-vancouver-ay]{sn-jnl}% Vancouver Author Year Reference Style
%%\documentclass[pdflatex,sn-apa]{sn-jnl}% APA Reference Style
%%\documentclass[pdflatex,sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style

%%%% Standard Packages
%%<additional latex packages if required can be included here>

\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%


%%%%

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

%% as per the requirement new theorem styles can be included as shown below
\theoremstyle{plain}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%

\theoremstyle{thmstylethree}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}

\title[Article Title]{Maximum Entropy Reinforcement Learning with Diffusion Policy }

%%=============================================================%%
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% \author*[1,2]{\fnm{Joergen W.} \spfx{van der} \sur{Ploeg} 
%%  \sfx{IV}}\email{iauthor@gmail.com}
%%=============================================================%%

\author[1,2]{\fnm{Xiaoyi} \sur{Dong}}\email{dongxiaoyi2022@ia.ac.cn}

\author[1,3,4]{\fnm{Jian} \sur{Cheng}}\email{jcheng@nlpr.ia.ac.cn}

\author*[1,4]{\fnm{Xi Sheryl} \sur{Zhang}}\email{sheryl.zhangxi@gmail.com}

\affil[1]{\orgdiv{
Institute of Automation}, \orgname{Chinese Academy of Sciences}}

\affil[2]{\orgdiv{School of Artificial Intelligence}, \orgname{University of Chinese Academy of Sciences}}

\affil[3]{\orgdiv{School of Future Technology}, \orgname{University of Chinese Academy of Sciences}}

\affil[4]{\orgname{ AiRiA}}

%%==================================%%
%% Sample for unstructured abstract %%
%%==================================%%

\abstract{The Soft Actor-Critic (SAC) algorithm with a Gaussian policy has become a mainstream implementation for realizing the Maximum Entropy Reinforcement Learning (MaxEnt RL) objective, which incorporates entropy maximization to encourage exploration and enhance policy robustness. While the Gaussian policy performs well on simpler tasks, its exploration capacity and potential performance in complex multi-goal RL environments are limited by its inherent unimodality.  In this paper, we employ the diffusion model, a powerful generative model capable of capturing complex multimodal distributions, as the policy representation to fulfill the MaxEnt RL objective, developing a method named \textit{MaxEnt RL with Diffusion Policy} (MaxEntDP). Our method enables efficient exploration and brings the policy closer to the optimal MaxEnt policy. Experimental results on Mujoco benchmarks show that MaxEntDP outperforms the Gaussian policy and other generative models within the MaxEnt RL framework, and performs comparably to other state-of-the-art diffusion-based online RL algorithms. Our code is available at \hyperlink{https://github.com/diffusionyes/MaxEntDP}{https://github.com/diffusionyes/MaxEntDP}.}

%%================================%%
%% Sample for structured abstract %%
%%================================%%

% \abstract{\textbf{Purpose:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Methods:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Results:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Conclusion:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.}

\keywords{Diffusion models, online reinforcement learning, maximum entropy reinforcement learning, soft actor-critic}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle

\section{Introduction}
\label{submission}
Reinforcement Learning (RL) has emerged as a powerful paradigm for training intelligent agents to make decisions in complex control tasks \cite{silver2016mastering,mnih2015human,kaufmann2023champion,kiran2021deep,ibarz2021train}. Traditionally, RL focuses on maximizing the expected cumulative reward, where the agent selects actions that yield the highest return in each state \cite{sutton1999reinforcement}. However, this approach often overlooks the inherent uncertainty and variability of real-world environments, which can lead to suboptimal or overly deterministic policies. To address these limitations, Maximum Entropy Reinforcement Learning (MaxEnt RL) incorporates entropy maximization into the standard RL objective, encouraging exploration and improving robustness during policy learning \cite{toussaint2009robot,ziebart2010modeling,haarnoja2017reinforcement}.

The Soft Actor-Critic (SAC) algorithm \cite{haarnoja2018soft} is an effective method for achieving the MaxEnt RL objective, which alternates between policy evaluation and policy improvement to progressively refine the policy. With high-capacity neural network approximators and suitable optimization techniques, SAC can provably converge to the optimal MaxEnt policy within the chosen policy set. The choice of policy representation in SAC is crucial, as it influences the exploration behavior during training and determines the proximity of the candidate policies to the optimal MaxEnt policy. In complex multi-goal RL tasks, where multiple feasible behavioral modes exist, the commonly used Gaussian policy typically explores only a single mode, which can cause the agent to get trapped in a local optimum and fail to approach the optimal MaxEnt policy that captures all possible behavioral modes.

In this paper, we propose using diffusion models \cite{sohl2015deep,song2019generative,ho2020denoising,song2021score}, a powerful generative model, as the policy representation within the SAC framework. This allows for the exploration of all promising behavioral modes and facilitates convergence to the optimal MaxEnt policy. Diffusion models transform the original data distribution into a tractable Gaussian by progressively adding Gaussian noise, which is known as the forward diffusion process. After training a neural network to predict the noise added to the noisy samples, the original data can be recovered by solving the reverse diffusion process with the noise prediction network. While several generative models, e.g., variational autoencoders \cite{kingma2013auto}, generative adversarial networks \cite{goodfellow2020generative}, and normalizing flows \cite{rezende2015variational} could serve as the policy representation, we choose diffusion models due to their balance between expressiveness and inference speed, achieving remarkable performance with affordable training and inference costs.

However, integrating diffusion models into the SAC framework presents two key challenges: 1) How to train a diffusion model to approximate the exponential of the Q-function in the policy improvement step? 2) How to compute the log probability of the diffusion policy when evaluating the soft Q-function? To address the first challenge, we analyze the training target of the noise prediction network in diffusion models and propose a Q-weighted Noise Estimation method. For the second challenge, we introduce a numerical integration technique to approximate the log probability of the diffusion model. We evaluate the effectiveness of our approach on Mujoco benchmarks. The experimental results demonstrate that our method outperforms the Gaussian policy and other generative models within the MaxEnt RL framework, and performs comparably to other state-of-the-art diffusion-based online RL algorithms.


\section{Preliminary}

\subsection{Maximum Entropy Reinforcement Learning}
In this paper, we focus on policy learning in continuous action spaces. We consider a Markov Decision Process (MDP) defined by the tuple \((\mathcal{S}, \mathcal{A}, p, r, \rho_0, \gamma)\), where \(\mathcal{S}\) represents the state space, \(\mathcal{A}\) is the continuous action space, \(p: \mathcal{S} \times \mathcal{S} \times \mathcal{A} \to [0, +\infty]\) is the probability density function of the next state \(\boldsymbol{s}_{t+1}\ \in \mathcal{S}\) given the current state \(\boldsymbol{s}_t\ \in \mathcal{S}\) and the action \(\boldsymbol{a}_t\ \in \mathcal{A}\), \(r: \mathcal{S} \times \mathcal{A} \to [r_{\min}, r_{\max}]\) is the bounded reward function, \(\rho_0: \mathcal{S} \to [0, +\infty]\) is the distribution of the initial state \(\boldsymbol{s}_0\) and \(\gamma \in [0, 1]\) is the discount factor. The marginals of the trajectory distribution induced by a policy \(\pi(\boldsymbol{a}_t | \boldsymbol{s}_t)\) are denoted as \(\rho_{\pi}(\boldsymbol{s}_t, \boldsymbol{a}_t)\).

The standard RL aims to learn a policy that maximizes the expected cumulative reward. To encourage stochastic policies, Maximum Entropy RL augments this objective by incorporating the expected entropy of the policy:
\begin{equation}
    J(\pi) = \sum_{t=0}^{\infty}  \gamma^{t} \mathbb{E}_{(\boldsymbol{s}_t, \boldsymbol{a}_t) \sim \rho_{\pi}} \left[  r(\boldsymbol{s}_t, \boldsymbol{a}_t) + \beta \mathcal{H}(\pi(\cdot | \boldsymbol{s}_t)) \right],
\end{equation}
where \(\mathcal{H}(\pi(\cdot | \boldsymbol{s}_t)) = \mathbb{E}_{\boldsymbol{a}_t \sim \pi(\cdot | \boldsymbol{s}_t)} \left[ -\log \pi(\boldsymbol{a}_t | \boldsymbol{s}_t) \right]\), and \(\beta\) is the temperature parameter that controls the trade-off between the entropy and reward terms. A higher value of \(\beta\) drives the optimal policy to be more stochastic, which is advantageous for RL tasks requiring extensive exploration. In contrast, the standard RL objective can be seen as the limiting case where \(\beta \to 0\).

\subsection{Soft Actor Critic}
The optimal maximum entropy policy can be derived by applying the Soft Actor-Critic (SAC) algorithm \cite{haarnoja2018soft}. In this subsection, we will briefly introduce the framework of SAC, and the relevant proofs are provided in Appendix \ref{appendix::SAC}. The SAC algorithm utilizes two parameterized networks, \( Q_{\theta} \) and \( \pi_{\phi} \), to model the soft Q-function and the policy, where \( \theta \) and \( \phi \) represent the parameters of the respective networks. These networks are optimized by alternating between policy evaluation and policy improvement.

In the policy evaluation step, the soft Q-function of the current policy \( \pi_{\phi} \) is learned by minimizing the soft Bellman error:
\begin{equation}
\label{soft Bellman error}
    L(\theta) = \mathbb{E}_{(\boldsymbol{s},\boldsymbol{a}) \sim \mathcal{D}} \left[ \frac{1}{2} \left( Q_{\theta}(\boldsymbol{s},\boldsymbol{a}) - \hat{Q}(\boldsymbol{s},\boldsymbol{a}) \right)^2 \right],
\end{equation}
where \( \mathcal{D} \) is the replay buffer, and the target value
$ \hat{Q}(\boldsymbol{s},\boldsymbol{a}) = r(\boldsymbol{s}, \boldsymbol{a}) + \gamma \mathbb{E}_{\boldsymbol{s}' \sim p, \boldsymbol{a}' \sim \pi_{\phi}} \left[ Q_{\theta}(\boldsymbol{s}',\boldsymbol{a}') - \beta \log \pi_{\phi}(\boldsymbol{a}'|\boldsymbol{s}') \right]$. 

In the policy improvement step, the old policy \( \pi_{{\phi}_k} \) is updated towards the exponential of the new Q-function, whose soft value is guaranteed higher than the old policy. However, the target policy may be too complex to be exactly represented by any policy within the parameterized policy set \( \Pi = \{\pi_{\phi} | \phi \in \Phi \} \), where $\Phi$ is the parameter space of the policy. Therefore, the new policy is obtained by projecting the target policy onto the policy set \( \Pi \) based on the Kullback-Leibler divergence:
\begin{equation}
\label{equation::SAC policy loss}
    L(\phi) = \text{D}_{\text{KL}} \left( \pi_{\phi}(\cdot | \boldsymbol{s}) \ \middle\| \ \frac{\exp(\frac{1}{\beta}Q_{\theta}(\boldsymbol{s}, \cdot))}{Z_{\theta}(\boldsymbol{s})} \right).
\end{equation}

\begin{theorem}
\label{theorem::soft policy iteration}
    \textbf{(Soft Policy Iteration)} In the tabular setting, let $L(\theta_k)=0$ and $L(\phi_k)$ be minimized for each $k$. Repeated application of policy evaluation and policy improvement, i.e., $k \to \infty$, $\pi_{\phi_k}$ will converge to a policy $\pi^*$ such that $Q^{ \pi^{*}}(\boldsymbol{s}, \boldsymbol{a}) \geq Q^{ \pi}(\boldsymbol{s}, \boldsymbol{a})$ for all $\pi \in \Pi$ and $(\boldsymbol{s}, \boldsymbol{a}) \in \mathcal{S} \times \mathcal{A}$ with $|\mathcal{A}| < \infty$.
\end{theorem}

Theorem \ref{theorem::soft policy iteration} suggests that if the Bellman error can be reduced to zero and the policy loss is minimized at each optimization step, the soft actor-critic algorithm will converge to the optimal maximum entropy policy within the policy set \(\Pi\). This indicates that the choice of the policy set $\Pi$ significantly affects the performance of the soft actor-critic algorithm. Specifically, a more expressive policy class will yield a policy closer to the optimal MaxEnt policy. Inspired by this intuition, we employ the diffusion model to represent the policy, as it is highly expressive and well-suited to capture the complex multimodal distribution \cite{chi2023diffusion,wangdiffusion,chenoffline,ajayconditional}.

\subsection{Diffusion Models}
Diffusion models are powerful generative models. Given an unknown data distribution \( p(\boldsymbol{x}_0) \), which is typically a mixture of Dirac delta measures over the training dataset, diffusion models transform this data distribution into a tractable Gaussian distribution by progressively adding Gaussian noise \cite{ho2020denoising}. In the context of a Variance-Preserving (VP) diffusion process \cite{ho2020denoising,song2021score}, the transition from the original sample \( \boldsymbol{x}_0 \) at time \( t = 0 \) to the noisy sample \( \boldsymbol{x}_t \) at time \( t \in [0, 1] \) follows the distribution:
\begin{equation}
    p(\boldsymbol{x}_t|\boldsymbol{x}_0) = \mathcal{N}(\boldsymbol{x}_t | \sqrt{\sigma(\alpha_t)} \boldsymbol{x}_0, \sigma(-\alpha_t) \boldsymbol{I}),
\end{equation}
where \( \alpha_t \) represents the log of the Signal-to-Noise Ratio (SNR) at time \( t \), and \( \sigma(\cdot) \) is the sigmoid function. \( \alpha_t \) determines the amount of noise added at each time and is referred to as the noise schedule of a diffusion model. Denote the marginal distribution of \( \boldsymbol{x}_t \) as \( p(\boldsymbol{x}_t) \). The noise schedule should be designed to ensure that \( p(\boldsymbol{x}_1 | \boldsymbol{x}_0) \approx p(\boldsymbol{x}_1) \approx \mathcal{N}(\boldsymbol{x}_1 | \boldsymbol{0}, \boldsymbol{I}) \), and that \( \alpha_t \) is strictly decreasing w.r.t. \( t \). Then, starting from \( \boldsymbol{x}_1 \sim \mathcal{N}(\boldsymbol{x}_1 | \boldsymbol{0}, \boldsymbol{I}) \), the original data samples can be recovered by reversing the diffusion process from \( t = 1 \) to \( t = 0 \).
For sample generation, we can also employ the following probability flow ordinary differential equation (ODE) that shares the same marginal distribution with the diffusion process \cite{song2021score}:
\begin{equation}
\label{PFODE}
    \frac{\text{d}\boldsymbol{x}_t}{\text{d}t} = f(t)\boldsymbol{x}_t - \frac{1}{2} g^2(t) \nabla_{\boldsymbol{x}_t} \log p(\boldsymbol{x}_t),
\end{equation}
where \( f(t) = \frac{1}{2} \frac{\text{d} \log \sigma(\alpha_t)}{\text{d}t} \), \( g^2(t) = -\frac{\text{d} \log \sigma(\alpha_t)}{\text{d}t} \), and \( \nabla_{\boldsymbol{x}_t} \log p(\boldsymbol{x}_t) \), known as the score function, is the only unknown term. Consequently, diffusion models train a neural network \( \boldsymbol{\epsilon}_\phi(\boldsymbol{x}_t, \alpha_t) \) to approximate the scaled score function \( -\sqrt{\sigma(-\alpha_t)} \nabla_{\boldsymbol{x}_t} \log p(\boldsymbol{x}_t) \).
The training loss \( L(\phi) \) is defined as:
\begin{align}
    L(\phi)
    &= \mathbb{E}_{t, \boldsymbol{x}_t} \left[ w_t\left\| \boldsymbol{\epsilon}_\phi(\boldsymbol{x}_t, \alpha_t) + \sqrt{\sigma(-\alpha_t)} \nabla_{\boldsymbol{x}_t} \log p(\boldsymbol{x}_t) \right\|^2_2 \right]\\
    &= \mathbb{E}_{t, \boldsymbol{x}_0, \boldsymbol{\epsilon}} \left[w_t \left\| \boldsymbol{\epsilon}_\phi(\boldsymbol{x}_t, \alpha_t) - \boldsymbol{\epsilon} \right\|^2_2 \right] + C
\end{align}
where \(\boldsymbol{x}_0 \sim  p(\boldsymbol{x}_0)\), \( \boldsymbol{\epsilon} \sim \mathcal{N}(\boldsymbol{0},  \boldsymbol{I}) \), \(t \sim \mathcal{U}([0,1])\), \( \boldsymbol{x}_t = \sqrt{\sigma(\alpha_t)} \boldsymbol{x}_0 + \sqrt{\sigma(-\alpha_t)} \boldsymbol{\epsilon} \), \(w_t\) is a weighting function and usually set to \(w_t \equiv 1\), and \( C \) is a constant independent of \( \phi \). In this setup, the network \( \boldsymbol{\epsilon}_\phi(\boldsymbol{x}_t, \alpha_t) \) target at predicting the expectation of noise added to the noisy sample \( \boldsymbol{x}_t \), and is therefore called the noise prediction network.
Minimizing the loss function \( L(\phi) \) results in the following relationship:
\begin{equation}
    \nabla_{\boldsymbol{x}_t} \log p(\boldsymbol{x}_t) = -\frac{\boldsymbol{\epsilon}_\phi(\boldsymbol{x}_t, \alpha_t)}{\sqrt{\sigma(-\alpha_t)}}.
\end{equation}
Then we can solve the probability flow ODE in Equation \ref{PFODE} with the assistance of existing ODE solvers \cite{ho2020denoising,songdenoising,lu2022dpm,karras2022elucidating,zheng2023dpm} to generate data samples.


\section{Methodology}
In the soft actor-critic algorithms, Gaussian policies have become the most widely used class of policy representation due to their simplicity and efficiency. Although Gaussian policies perform well in relatively simple single-goal RL environments, they often struggle with more complex multi-goal tasks.

Consider a typical RL task that involves multiple behavior modes. The most efficient solution is to explore all behavior modes until one obviously outperforms the others. However, this exploration strategy is difficult to achieve with Gaussian policies. In the training process of a soft actor-critic algorithm with Gaussian policies, minimizing the K-L divergence between the Gaussian policy and the exponential of the Q-function—which is often multimodal in multi-goal tasks—tends to push the Gaussian policy to allocate most of the probability mass to the action region with the highest Q value \cite{chenscore}. Consequently, other promising action regions with slightly lower Q values will be neglected, which may cause the agent to become stuck at a local optimal policy.

However, an efficient exploration strategy can be achieved by replacing the Gaussian policy with a more expressive policy representation class. If accurately fitting the multimodal target policy (i.e., the exponential of the Q-function), the agent will explore all high-return action regions at a high probability, thus reducing the risk of converging to a local optimum. Moreover, recall that when the assumptions on loss optimization are met, the soft actor-critic algorithm is guaranteed to converge to the optimal maximum entropy policy within the chosen policy class. Therefore, with sufficient network capacity and appropriate optimization techniques, we can obtain the true optimal maximum entropy policy, as long as the selected policy representation class is expressive enough to capture it.

The above analysis emphasizes the importance of applying an expressive policy class to achieve efficient exploration as well as a higher performance upper bound. Since diffusion models have demonstrated remarkable performance in capturing complex multimodal distributions, we adopt them to represent the policy within the soft actor-critic framework. However, integrating a diffusion-based policy into the soft actor-critic algorithm presents several challenges:
(1) In the policy improvement step, the new diffusion policy is updated to approximate the exponential of the Q-function. However, existing methods for training diffusion models rely on samples from the target distribution, which are unavailable in this case.
(2) In the policy evaluation step, computing the soft Q-function requires access to the probability of the diffusion policy.  Nevertheless, diffusion models implicitly model data distributions by estimating their score functions, making it intractable to compute the exact probability.

The remainder of this section addresses these challenges and describes how to incorporate diffusion models into the soft actor-critic algorithm for efficient policy learning. We first propose the Q-weighted Noise Estimation approach to fit the exponential of the Q-function in Section \ref{Q-weighted Noise Estimation}, then introduce a method for probability approximation in diffusion policies in Section \ref{Probability Estimation}, and finally present the complete algorithm in Section \ref{MaxEnrRLDP}. We name this method MaxEntDP because it can fulfill the MaxEnt RL objective with diffusion policies.

\subsection{Q-weighted Noise Estimation}
\label{Q-weighted Noise Estimation}
Given a Q-function $Q(\boldsymbol{s},\boldsymbol{a})$, below we will analyze how to train a noise prediction network $\boldsymbol{\epsilon}_{\phi}$ in the diffusion model to approximate the target distribution:
\begin{equation}
\label{target distribution}
    \pi(\boldsymbol{a}|\boldsymbol{s})=\frac{\exp(\frac{1}{\beta}Q(\boldsymbol{s},\boldsymbol{a}))}{Z(\boldsymbol{s})}.
\end{equation}
Omitting the state in the condition for simplicity and following the symbol convention of diffusion models, we rewrite $\pi(\boldsymbol{a}|\boldsymbol{s})$ as $p(\boldsymbol{a}_0)$. The transition from the original action samples $\boldsymbol{a}_0$ at time $t=0$ to the noisy actions $\boldsymbol{a}_t$ at time $t \in [0,1]$ is defined as:
\begin{equation}
\label{transition distribution}
    p(\boldsymbol{a}_t|\boldsymbol{a}_0) = \mathcal{N}(\boldsymbol{a}_t | \sqrt{\sigma(\alpha_t)} \boldsymbol{a}_0, \sigma(-\alpha_t) \boldsymbol{I})
\end{equation}
Note that the symbol $t$ stands for the time of diffusion models if not specified. 

The marginal distribution of noisy actions $\boldsymbol{a}_t$ at time $t$ is denoted by $p(\boldsymbol{a}_t)$. To sample from $p(\boldsymbol{a}_0)$, we need to estimate the score function $\nabla_{\boldsymbol{a}_t} \log p(\boldsymbol{a}_t)$ at each intermediate time $t$ during the diffusion process. The score function can be reformulated as:
\begin{equation}
\label{score function}
    \nabla_{\boldsymbol{a}_t} \log p(\boldsymbol{a}_t) = \mathbb{E}_{p(\boldsymbol{a}_0|\boldsymbol{a}_t)}\left[\nabla_{\boldsymbol{a}_t} \log p(\boldsymbol{a}_t|\boldsymbol{a}_0)\right],
\end{equation}
which is an expectation with respect to the conditional distribution $p(\boldsymbol{a}_0|\boldsymbol{a}_t)$, a.k.a. the reverse transition distribution of the diffusion process. If samples from $p(\boldsymbol{a}_0)$ are available, as is often the case in the application scenarios of diffusion models \cite{saharia2022photorealistic,ho2022video,chi2023diffusion,xu2023dream3d,huang2023make}, we can first sample original actions $\boldsymbol{a}_0 \sim p(\boldsymbol{a}_0)$, and then sample noisy actions $\boldsymbol{a}_t \sim p(\boldsymbol{a}_t|\boldsymbol{a}_0)$ to obtain several sample pairs following the joint distribution $p(\boldsymbol{a}_0, \boldsymbol{a}_t)$. Then for a fixed noisy action $\boldsymbol{a}_t$, the corresponding $\boldsymbol{a}_0$ will conform the conditional distribution $p(\boldsymbol{a}_0|\boldsymbol{a}_t)$, which can serve as Monte Carlo samples to estimate the expectation in Equation \ref{score function}. Conversely, in the context of the soft actor-critic algorithm, we lack samples from the target distribution $p(\boldsymbol{a}_0)$ but instead have access to a Q-function. Therefore, we must establish the relationship between the conditional distribution $p(\boldsymbol{a}_0|\boldsymbol{a}_t)$ and the Q-function. 

\begin{lemma} 
\label{lemma::decomposition}
\textbf{(Decomposition of the Reverse Transition Distribution)}
    The conditional distribution $ p(\boldsymbol{a}_0|\boldsymbol{a}_t)$ can be decomposed as 
\begin{equation}
    p(\boldsymbol{a}_0|\boldsymbol{a}_t) \propto \exp(\frac{1}{\beta}Q(\boldsymbol{a}_0)) 
     \mathcal{N}(\boldsymbol{a}_0 | \frac{1}{\sqrt{\sigma(\alpha_t)}} \boldsymbol{a}_t, \frac{\sigma(-\alpha_t)}{\sigma(\alpha_t)} \boldsymbol{I})
\end{equation}    
\end{lemma}

 The proof is provided in Appendix \ref{appendix::condition distribution}. Lemma \ref{lemma::decomposition} demonstrates that the conditional distribution $p(\boldsymbol{a}_0|\boldsymbol{a}_t)$ can be seen as a Gaussian distribution of $\boldsymbol{a}_0$ weighted by the exponential of the Q-function. Sampling from the Gaussian distribution is straightforward, we can apply importance sampling \cite{bishop2006patterm} to estimate the expectation in Equation \ref{score function}.

\begin{theorem}
    \textbf{(Importance Sampling Estimate for the Score Function)} The score function can be estimated by
\begin{align}
    \nabla_{\boldsymbol{a}_t} \log p(\boldsymbol{a}_t) \approx \frac{1}{\sqrt{\sigma(-\alpha_t)}} \cdot\frac{1}{K} \sum_{i=1}^K w(\boldsymbol{a}_0^i) \boldsymbol{\epsilon}^i,
\end{align}
where $\boldsymbol{\epsilon}^1, \dots, \boldsymbol{\epsilon}^K \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol{I})$, $\boldsymbol{a}_0^i=\frac{1}{\sqrt{\sigma(\alpha_t)}} \boldsymbol{a}_t + \frac{\sqrt{\sigma(-\alpha_t)}}{\sqrt{\sigma(\alpha_t)}}\boldsymbol{\epsilon}^i$ and the importance ratio $w(\boldsymbol{a}_0)=\frac{\exp(\frac{1}{\beta}Q(\boldsymbol{a}_0))}{Z(\boldsymbol{a}_t)}$ with $Z(\boldsymbol{a}_t)$ being the normalizing constant of $p(\boldsymbol{a}_0|\boldsymbol{a}_t)$.

\end{theorem}
 
 The derivation is detailed in Appendix \ref{appendix::importance sampling}. Although this importance sampling estimate is unbiased, it exhibits high variance when the variance of the Q-function is large. 
To address this issue, we employ the weighted importance sampling approach \cite{bishop2006patterm} to reduce variance and stabilize the training process.

\begin{theorem}
    \textbf{(Weighted Importance Sampling Estimate for the Score Function)} The score function can be estimated by 
\begin{align}
\hspace{-1em}
    \nabla_{\boldsymbol{a}_t} \log p(\boldsymbol{a}_t)
    &\approx \frac{1}{\sqrt{\sigma(-\alpha_t)}} \cdot \sum_{i=1}^K \frac{w(\boldsymbol{a}_0^i)}{\sum_{j=1}^K w(\boldsymbol{a}_0^j)} \boldsymbol{\epsilon}^i\\
    \label{equation::weighted noise}
    &=\frac{1}{\sqrt{\sigma(-\alpha_t)}} \sum_{i=1}^K \textnormal{softmax}(\frac{1}{\beta}Q(\boldsymbol{a}_0^{1:K}))_i\boldsymbol{\epsilon}^i,
\end{align}
where $\textnormal{softmax}(\frac{1}{\beta}Q(\boldsymbol{a}_0^{1:K}))_i=\frac{\exp(\frac{1}{\beta}Q(\boldsymbol{a}_0^i))}{\sum^K_{j=1}\exp(\frac{1}{\beta}Q(\boldsymbol{a}_0^j))}$.
\end{theorem}
The normalizing constant $Z(\boldsymbol{a}_t)$ is canceled out in Equation \ref{equation::weighted noise}, eliminating the need for its explicit computation. Since the bias of the weighted importance sampling method decreases as the number of Monte Carlo samples increases, a larger value of $K$ is preferred in practice given adequate computation budgets. 

Then the training target of the noise prediction network is 
\begin{align}
    \boldsymbol{\epsilon}^*(\boldsymbol{a}_t, \alpha_t)
    &=-\sqrt{\sigma(-\alpha_t)}\nabla_{\boldsymbol{a}_t} \log p(\boldsymbol{a}_t)\\
    \label{equation::noise target}
    &\approx - \sum_{i=1}^K \text{softmax}(\frac{1}{\beta}Q(\boldsymbol{a}_0^{1:K}))_i\boldsymbol{\epsilon}^i,
\end{align}
This target can be interpreted as a weighted sum of noise, with the weights being the exponential of the Q-value. Consequently, we refer to this method as Q-weighted Noise Estimation for training the noise prediction network. The overall training loss is
\begin{equation}
\label{equation::policy training loss}
L(\phi)=\mathbb{E}_{p(\boldsymbol{a}_t)}\left[\parallel\boldsymbol{\epsilon}_{\phi}(\boldsymbol{a}_t, \alpha_t)-\boldsymbol{\epsilon}^*(\boldsymbol{a}_t, \alpha_t)\parallel^2_2\right]
\end{equation}
While the true distribution of noisy actions $p(\boldsymbol{a}_t)$ may be inaccessible, we can substitute it with other distributions with full support, as the loss will still be minimized for each $\boldsymbol{a}_t$ given sufficient network capacity.

We briefly compare our method with two previous approaches that approximate the exponential of a given function $Q(a)$. The QSM method \cite{psenkalearning} estimates the score function as $\nabla_{\boldsymbol{a}_t} \log p(\boldsymbol{a}_t) \approx \nabla_{\boldsymbol{a}_t} \
\frac{1}{\beta}Q(\boldsymbol{a}_t)$. This approximation requires $p(\boldsymbol{a}_t)\propto \exp (\frac{1}{\beta}Q(\boldsymbol{a}_t))$, which is true only when the time $t$ is close to $0$. Therefore, the score function estimation in QSM is imprecise for most values of $t$. Another method iDEM \cite{akhounditerated} proposes $\nabla_{\boldsymbol{a}_t} \log p(\boldsymbol{a}_t)\approx \frac{1}{\sqrt{\sigma(\alpha_t)}} \sum_{i=1}^K \text{softmax}(\frac{1}{\beta}Q(\boldsymbol{a}_0^{1:K}))_i \nabla_{\boldsymbol{a}_0^i} \frac{1}{\beta} Q(\boldsymbol{a}_0^i)$, and the derivation is included in Appendix \ref{appendix::iDEM} for completion. Although the expressions of iDEM and our method appear similar and both can approach the true score function as $K \to \infty$, our method does not require computing the gradient of the Q-function, which is more computationally efficient, especially when the Q-function is evaluated on a neural network. Furthermore, the experiments in Section \ref{section::cmp_eval} demonstrate that the variance of the score estimation in our method is significantly lower than the other two methods that rely on gradient computation, leading to a more stable training process.

\subsection{Probability Approximation of Diffusion Policy}
\label{Probability Estimation}
Diffusion models approximate the desired distributions by estimating their score function. Although this implicit modeling enhances the expressiveness of the model, enabling it to approximate any distribution with a differentiable probability density function, it also introduces challenges in computing the exact likelihood of the distribution.

Previous study \cite{konginformation,wu2024your} proved that the log-likelihood of $p(\boldsymbol{a}_0)$ can be written exactly as an expression that depends only on the true noise prediction target, i.e.,
\begin{equation}
\label{equation::exact probability}
    \log p(\boldsymbol{a}_0)= c - \frac{1}{2} \int_{-\infty}^{+\infty}\mathbb{E}_{\boldsymbol{\epsilon} }\left[\parallel \boldsymbol{\epsilon} - \boldsymbol{\epsilon}^*(\boldsymbol{a}_t, \alpha_t) \parallel^2_2 \right]\text{d}\alpha_t
\end{equation}
where $c=-\frac{d}{2}\log (2\pi e)+\frac{d}{2} \int_{-\infty}^{+\infty} \sigma(\alpha_t) \text{d}\alpha_t$ with $d$ being the dimension of $\boldsymbol{a}_0$, $\boldsymbol{\epsilon} \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol{I})$, $\boldsymbol{a}_t = \sqrt{\sigma(\alpha_t)} \boldsymbol{a}_0 + \sqrt{\sigma(-\alpha_t)} \boldsymbol{\epsilon}$, and $\boldsymbol{\epsilon}^*(\boldsymbol{a}_t, \alpha_t)=-\sqrt{\sigma(-\alpha_t)}\nabla_{\boldsymbol{a}_t} \log p(\boldsymbol{a}_t)$ is the training target of the noise prediction network. 

\begin{corollary} \textbf{(The Exact Probability of Diffusion Policy)}
    Let $\boldsymbol{\epsilon}_{\phi}$ be a well-trained noise prediction network, i.e., it can induce a probability density function $p_{\phi}(\boldsymbol{a}_0)$ satisfying $\boldsymbol{\epsilon}_{\phi}(\boldsymbol{a}_t, \alpha_t)=-\sqrt{\sigma(-\alpha_t)}\nabla_{\boldsymbol{a}_t} \log p_{\phi}(\boldsymbol{a}_t)$, then 
\begin{equation}
    \log p_{\phi}(\boldsymbol{a}_0)= c - \frac{1}{2} \int_{-\infty}^{+\infty}\mathbb{E}_{\boldsymbol{\epsilon} }\left[\parallel \boldsymbol{\epsilon} - \boldsymbol{\epsilon}_{\phi}(\boldsymbol{a}_t, \alpha_t) \parallel^2_2 \right]\text{d}\alpha_t
\end{equation}
\end{corollary}
This corollary can be inferred from Equation \ref{equation::exact probability}. However, this expression is intractable because both the integral in $c$ and the integral of the noise prediction error diverge, with only their difference converging \cite{konginformation}. We attempt to approximate the integral using numerical integration techniques. However, we observe that using the log SNR as the integration variable results in a high variance, as it spans from $-\infty$ to $+\infty$. Therefore, we instead utilize $\sigma(\alpha_t)$ with a narrower integration domain of $[0,1]$.

\begin{theorem}
    \textbf{(The Probability Approximation of Diffusion Policy)} The log probability of diffusion policy can be approximated by
\begin{equation}
\label{equation::numeracal integral}
    \log p_{\phi}(\boldsymbol{a}_0)\approx c' + \frac{1}{2}\sum_{i=1}^T w_{t_i} \left( d \cdot \sigma(\alpha_{t_i}) - \tilde{\boldsymbol{\epsilon}}_{\phi}(\boldsymbol{a}_{t_i}, \alpha_{t_i})\right)
\end{equation}
where $c'=-\frac{d}{2}\log (2\pi e)$, $t_{0:T}$ are uniformly spaced timesteps in $[t_{\text{min}},t_{\text{max}}]$, $w_{t_i}=\frac{\sigma(\alpha_{t_{i-1}}) - \sigma(\alpha_{t_{i}})}{\sigma(\alpha_{t_i})\sigma(-\alpha_{t_i})}$ is the weight at $t_i$, $\tilde{\boldsymbol{\epsilon}}_{\phi}(\boldsymbol{a}_{t_i}, \alpha_{t_i})=\frac{1}{N}\sum^N_{j=1}\parallel \boldsymbol{\epsilon}^j - \boldsymbol{\epsilon}_{\phi}(\boldsymbol{a}_{t_i}^j, \alpha_{t_i}) \parallel^2_2$ is the noise prediction error estimation at $t_i$.
\end{theorem}
 The detailed derivation is provided in Appendix \ref{appendix::numerial integration}.

\subsection{MaxEnt RL with Diffusion Policy}
\label{MaxEnrRLDP}
After addressing the critical challenges in training and probability estimation for the diffusion policy, we present the complete algorithm for achieving the MaxEnt RL objective with a diffusion policy. Our approach is based on the soft actor-critic framework. We utilize two neural networks: $Q_{\theta}(s,a)$ to model the Q-function, and $\boldsymbol{\epsilon}_{\phi}(\boldsymbol{a}_t, \alpha_t, \boldsymbol{s})$ to model the noise prediction network for the diffusion policy $\pi_{\phi}(\boldsymbol{a}_0|\boldsymbol{s})$.

The training process alternates between policy evaluation and policy improvement. In the policy evaluation step, the Q-network is trained by minimizing the soft Bellman error, as defined in Equation \ref{soft Bellman error}. Here, the actions $a' \sim \pi_{\phi}(\cdot|\boldsymbol{s'})$ are sampled by solving the probability flow ODE in Equation \ref{PFODE} with the noise prediction network $\boldsymbol{\epsilon}_{\phi}(\boldsymbol{a}_t, \alpha_t, \boldsymbol{s})$, and the log probality $\log \pi_{\phi}(\cdot|\boldsymbol{s})$ is approximated using Equation \ref{equation::numeracal integral}. In the policy improvement step, the noise prediction network is optimized using the loss function in Equation \ref{equation::policy training loss}\footnote{The minimizers of Equation \ref{equation::policy training loss} and \ref{equation::SAC policy loss} will be equal when the exponential of the Q-function can be exactly expressed by the chosen policy set, so the capacity of the noise prediction network is preferred to be large if allowed.}, with the training target computed in Equation \ref{equation::noise target}. The pseudocode for our method is presented in Algorithm \ref{algorithm::MaxEntDP}.

\begin{algorithm}[tb]
\caption{MaxEnt RL with Diffusion Policy}
\label{algorithm::MaxEntDP}
\begin{algorithmic}[1]
\State Initialize critic networks $Q_{\theta_1}$, $Q_{\theta_2}$, and the noise prediction network $\boldsymbol{\epsilon}_{\phi}$ with random parameters $\theta_1, \theta_2, \phi$.
\State Initialize target networks $\theta'_1 \leftarrow \theta_1, \theta'_2 \leftarrow \theta_2$
\State Initialize replay buffer $\mathcal{D}$
\For{each iteration}
    \For{each sampling step}
    \State Sample $\boldsymbol{a} \sim \pi_{\phi}(\cdot|\boldsymbol{s})$ according to Equation \ref{PFODE}
    \State Step environment: $\boldsymbol{s'}, r$  $\leftarrow$ $\text{env}(\boldsymbol{a})$
    \State Store $(\boldsymbol{s}, \boldsymbol{a}, r, \boldsymbol{s}')$ in $\mathcal{D}$
    \EndFor
    \For{each update step}
    \State Sample $B$ transitions $(\boldsymbol{s}, \boldsymbol{a}, r, \boldsymbol{s}')$ from $\mathcal{D}$
    \State Sample $\boldsymbol{a}' \sim \pi_{\phi}(\cdot|\boldsymbol{s}')$ according to Equation \ref{PFODE}
    \State Compute $\log\pi_{\phi}(\boldsymbol{a}'|\boldsymbol{s}')$ using Equation \ref{equation::numeracal integral}
    \State Compute the target Q-value: $
        \hat{Q}(\boldsymbol{s},\boldsymbol{a}) = r(\boldsymbol{s}, \boldsymbol{a}) + \gamma  \left( \min_{i=1,2}Q_{\theta_i}(\boldsymbol{s}',\boldsymbol{a}') - \beta \log \pi_{\phi}(\boldsymbol{a}'|\boldsymbol{s}') \right). 
    $
    \State Update critics: $\theta_{i} = \arg\min_{\theta_i}\frac{1}{B}\sum (Q_{\theta_i}(\boldsymbol{s}, \boldsymbol{a}) - \hat{Q}(\boldsymbol{s}, \boldsymbol{a}))^2$
    \State Sample $t \sim \mathcal{U}([t_{\text{min}}, t_{\text{max}}])$ and the noisy action $\boldsymbol{a}_t \sim \mathcal{N}(\boldsymbol{a}_t| \sqrt{\sigma(\alpha_t)}\boldsymbol{a}, \sigma(-\alpha_t)\boldsymbol{I})$
    \State Estimate $\boldsymbol{\epsilon}^{*}(\boldsymbol{a}_t, \alpha_t, \boldsymbol{s})$ with Equation \ref{equation::noise target} 
    \State Update the noise prediction network: $\phi=\arg \min_{\phi}\frac{1}{B}\sum \parallel \boldsymbol{\epsilon}_{\phi}(\boldsymbol{a}_t, \alpha_t, \boldsymbol{s}) - \boldsymbol{\epsilon}^{*}(\boldsymbol{a}_t, \alpha_t, \boldsymbol{s})\parallel^2_2$
    \State Updtae target networks: $\theta_{i} \leftarrow \tau \theta_{i} + (1 - \tau)\theta_{i}$
    \EndFor
\EndFor
% \vskip -0.2in
\end{algorithmic}
\end{algorithm}

In addition, we adopt several techniques to improve the training and inference of our method:

\textbf{Truncated Gaussian Noise Distribution for Bounded Action Space.} In RL tasks with bounded action spaces, the Q-function is undefined outside the action space. To avoid evaluating Q-values for illegal actions, the noise distribution in Equation \ref{equation::noise target} is modified from a standard Gaussian to a truncated standard Gaussian. This modification still generates samples according to the Gaussian function, but all samples are bounded in the specified range.

\textbf{Action Selection for inference.} Previous studies \cite{chao2024maximum,wangdiffusion,maodiffusion,chenaligning} have found that a deterministic policy typically outperforms its stochastic counterpart during testing. Consequently, we employ an action selection technique to further refine the policy after training. Specifically, $M$ action candidates are sampled from the diffusion policy, and the action with the highest Q-value is selected to interact with the RL environment.

\section{Related Work}

\textbf{MaxEnt RL}
A variety of approaches have been proposed to achieve the MaxEnt RL objective. SQL \cite{haarnoja2017reinforcement} introduces soft Q-learning to learn the optimal soft Q-function and trains an energy-based model using the amortized Stein variational gradient method to generate actions according to the exponential of the optimal soft Q-function. SAC \cite{haarnoja2018soft} presents the soft actor-critic algorithm, which iteratively improves the policy towards a higher soft value, and provides an implementation using Gaussian policies. MEow \cite{chao2024maximum} employs energy-based normalizing flows as unified policies to represent both the actor and the critic, simplifying the training process for MaxEnt RL. This paper highlights the importance of policy representation within the MaxEnt RL framework: a more expressive policy representation enhances exploration and facilitates closer convergence to the optimal MaxEnt policy. Diffusion models, which are more expressive than Gaussian distributions and energy-based normalizing flows and easier to train and sample than energy-based models, present an ideal policy representation that effectively balances expressiveness and the complexity of training and inference.

% \subsection{Diffusion Policies for Offline RL}
% Due to its strong expressivity in modeling complex multi-modal distributions, the diffusion model has been widely adopted as a policy representation to address the limitations of the commonly used Gaussian policy.

\textbf{Diffusion Policies for Offline RL.} Offline RL attempts to learn a well-performing policy from a pre-collected dataset. Collected by multiple policies, the offline datasets may exhibit high skewness and multi-modality. Diffusion Policy \cite{chi2023diffusion} trains a diffusion model to approximate the multi-modal expert behavior by behavior cloning. To optimize the policy for higher performance, Diffusion-QL \cite{wangdiffusion} combines the diffusion loss with Q-value loss evaluated on the generated actions, CEP \cite{lu2023contrastive} trains a separate guidance network using Q-function to guide the actions to regions with high Q values, and EDA \cite{chenaligning} employs direct preference optimization to align the diffusion policy with Q-function. To improve the training and inference speed of diffusion policy, EDP \cite{kang2024efficient} adopts action approximation and efficient ODE sampler DPM-solver for action generation, and CPQL \cite{chen2024boosting} utilizes the consistency policy \cite{song2023consistency}, a one-step diffusion policy. Due to the lack of online samples, the above approaches require staying close to the behavior policy to prevent out-of-distribution actions whose performances are unpredictable. However, in this paper, we focus on online RL, where online interactions are accessible to correct the errors in value evaluation. Therefore, different techniques should be developed to employ diffusion models in online RL.

\textbf{Diffusion Policies for Online RL.} In online RL, a key challenge lies in balancing exploration and exploitation. Previous studies \cite{psenkalearning,yang2023policy,ding2024diffusion,wang2024diffusion} apply expressive diffusion models as policy representations to promote the exploration of the state-action space. QSM \cite{psenkalearning} fits the exponential of the Q-function by training a score network to approximate the action gradient of the Q-function. DIPO \cite{yang2023policy} improves the actions by applying the action gradient of the Q-function and clones the improved actions. QVPO \cite{ding2024diffusion} weights the diffusion loss with the Q-value, assigning probabilities to actions that are linearly proportional to the Q-value. DACER \cite{wang2024diffusion} optimizes the Q-value loss to generate actions with high Q values and adds extra noise to the generated actions to keep a constant policy entropy. Unlike previous approaches, we employ the MaxEnt RL objective to encourage exploration and enhance policy robustness. Similar to QSM, we train the diffusion model to fit the exponential of the Q-function. However, our Q-weighted noise estimation method is more accurate and stable. Furthermore, we include policy entropy when computing the Q-function, which can further promote exploration. 


\section{Experiments}
In this section, we conduct experiments to address the following questions:
(1) Can MaxEntDP effectively learn a multi-modal policy in a multi-goal task? 
(2) Does the diffusion policy outperform the Gaussian policy and other generative models within the MaxEnt RL framework? 
(3) How does performance vary when replacing the Q-weighted Noise Estimation method with competing approaches, such as QSM and iDEM? 
(4) How does MaxEntDP compare to other diffusion-based online RL algorithms? (5) Does the MaxEnt RL objective benefit policy training? 

\subsection{A Toy Multi-goal Environment}
In this subsection, we use a 2-D multi-goal environment \cite{haarnoja2017reinforcement} to demonstrate the effectiveness of MaxEntDP. In this environment, the agent is a 2-D point mass trying to reach one of four symmetrically placed goals. The state and action are position and velocity, respectively. And the reward is a penalty for the velocity and distance from the closest goal. Under the MaxEnt RL objective, the optimal policy is to choose one goal randomly and then move toward it. Figure \ref{fig::toy} shows the trajectories generated by the diffusion policy during the training process. We can see that MaxEntDP can effectively explore the state-action space and learn a multi-modal policy that approaches the optimal MaxEnt policy.

\begin{figure}[ht]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.6\columnwidth]{pics/toy_example/all_merged.pdf}}
\caption{The generated trajectories during the training process. From left to right are trajectories generated after 2k, 4k, and 6k training steps. The goals are denoted by the red points.}
\label{fig::toy}
\end{center}
\vskip -0.4in
\end{figure}

\subsection{Comparative Evaluation }
\label{section::cmp_eval}
\textbf{Policy Representations.} To reveal the superiority of applying diffusion models as policy representations to achieve the MaxEnt RL objective, we compare the performance of MaxEntDP on Mujoco benchmarks \cite{todorov2012mujoco} with other algorithms. Our chosen baseline algorithms include SAC \cite{chao2024maximum}, MEow \cite{chao2024maximum}, and TD3 \cite{fujimoto2018addressing}. SAC and MEow are two methods to pursue the same MaxRnt RL objective using Gaussian policy and energy-based normalizing flow policy, and TD3 provides a contrast to the deterministic policy. Figure \ref{fig::cmp_generative} shows that MaxEntDP surpasses (a-d) or matches (e-f) baseline algorithms on all tasks, and its evaluation variance is much smaller than other algorithms. This result indicates that the combination of MaxEntRL and diffusion policy effectively balances exploration and exploitation, enabling rapid convergence to a robust and high-performing policy.

\begin{figure*}[t]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{pics/comparison/comparison_generative.pdf}}
\caption{Learning curves on Mujoco benchmarks. The solid lines are the means and the shaded regions represent the standard deviations over five runs.}
\label{fig::cmp_generative}
\end{center}
\vskip -0.3in
\end{figure*}

\textbf{Diffusion Model Training Methods.}
In this subsection, we demonstrate the advantages of the proposed Q-weighted Noise Estimation method (QNE) on training diffusion models, compared to two competing methods, QSM and iDEM. We replace the QNE module with QSM and iDEM to observe performance differences. As shown in Figure \ref{fig::ablation_QNE}(a), the performance of QSM and iDEM improves initially but then fluctuates after reaching a high level. This may be due to both methods relying on the gradient computation of the Q-function to estimate the score function. When the Q-value is large, its gradient typically varies much across different actions, leading to a high variance in score function estimation for QSM and iDEM, as illustrated in Figure \ref{fig::ablation_QNE}(b). This increased variance causes instability in the training of the noise prediction network. In contrast, QNE exhibits significantly lower variance, and its performance improves steadily throughout the training process.

\begin{figure}[ht]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.6\columnwidth]{pics/ablation/ablation_QNE.pdf}}
\caption{Comparison between QNE and two competing methods, QSM and iDEM on HalfCheetah-v3 benchmark. (a) Learning curves. (b) Standard deviations of target noise (a.k.a. the scaled score function) estimation computed on a batch of noisy actions.}
\label{fig::ablation_QNE}
\end{center}
\vskip -0.4in
\end{figure}


\textbf{Diffusion-based Online RL Algorithms.}
We also compare MaxEntDP with state-of-the-art diffusion-based online RL algorithms: QSM, DIPO, QVPO, and DACER. These algorithms adopt different techniques to seek a balance between exploration and exploitation. Since the performances of different exploration strategies depend quite a lot on the characteristics of the RL tasks, none of the competing methods performs consistently well on all tasks, as shown by Figure \ref{fig::comparison_diffusion}. In contrast, our MaxEntDP outperforms or performs comparably to the top method on each task, showing consistent sample efficiency and stability.

\begin{figure}[t]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.7\columnwidth]{pics/comparison/comparison_diffusion.pdf}}
\caption{Learning curves of diffusion-based online RL algorithms.}
\label{fig::comparison_diffusion}
\end{center}
\vskip -0.4in
\end{figure}

\subsection{Ablation analysis}
In addition, we analyze the function of the MaxEnt RL objective by removing the probability approximation module in MaxEntDP. After doing this, we compute the original Q-function rather than the soft Q-function in the policy evaluation step. As shown in Figure \ref{fig::ablation_noent}, the performance decreases and exhibits greater variance after excluding policy entropy in the Q-function. This implies that the MaxEnt RL objective can benefit policy learning: it not only encourages the action distribution at the current step to be more stochastic (by fitting the exponential of Q-function), but also encourages transferring to the states with higher policy entropy (by computing the soft Q-function). Therefore, the MaxEnt RL objective shows a stronger exploration ability of the whole state-action space, leading to an efficient and stable training process.

\begin{figure}[ht]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.7\columnwidth]{pics/ablation/ablation_noent.pdf}}
\caption{The learning curve of MaxEntDP with and without entropy in Q-function computation.}
\label{fig::ablation_noent}
\end{center}
\vskip -0.3in
\end{figure}

\section{Conclusion}
This paper proposes MaxEntDP, a method to achieve the MaxEnt RL objective with diffusion policies. Compared to the Gaussian policy, the diffusion policy shows stronger exploration ability and expressiveness to approach the optimal MaxEnt policy. To address challenges in applying diffusion policies, we propose Q-weighted noise estimation to train the diffusion model and introduce the numerical integration technique to approximate the probability of diffusion policy. Experiments on Mujoco benchmarks demonstrate that MaxEntDP outperforms Gaussian policy and other generative models within the MaxEnt RL framework, and performs comparably to other diffusion-based online RL algorithms.

\textbf{Limitations and Future Work.} Since different RL tasks require varying levels of exploration, we adjust the temperature coefficient for each task and keep it fixed during training. Future work will explore how to automatically adapt this parameter, making MaxEntDP easier to apply in real-world applications. 

\backmatter

\section*{Impact Statement}

This paper focuses on achieving the MaxEnt RL objective, which is particularly effective for reinforcement learning tasks that require extensive exploration or policy robustness. Beyond advancing RL, our proposed Q-weighted noise estimation and numerical integration techniques address two fundamental issues in diffusion models: fitting the exponential of a given energy function and computing exact
probabilities. These two modules can be seamlessly integrated into diffusion-based studies that involve these issues.

\bibliography{sn-bibliography}% common bib file
%% if required, the content of .bbl file can be included here once bbl is generated
%%\input sn-article.bbl

\newpage
\begin{appendices}

\section{Theoretic proofs.}

\subsection{Proofs for Soft Actor-Critic algorithm}
\label{appendix::SAC}

Our proof is based on the tabular setting, i.e., $|\mathcal{S}| < \infty$, $|\mathcal{A}| < \infty$ and the replay buffer $\mathcal{D}$ covers all $(\boldsymbol{s},\boldsymbol{a})\in |\mathcal{S}| \times |\mathcal{A}|$.

The soft Q-function of policy $\pi$ is defined as:
\begin{equation}
    Q^{\pi}(\boldsymbol{s}_t,\boldsymbol{a}_t)=r(\boldsymbol{s}_t,\boldsymbol{a}_t)+\mathbb{E}_{\rho_{\pi}} \left[\sum_{l=1}^{\infty}\gamma^l(r(\boldsymbol{s}_{t+l}, \boldsymbol{a}_{t+l})-\beta\log \pi(\boldsymbol{a}_{t+l}|\boldsymbol{s}_{t+l}))\right],
\end{equation}
which satisfies:
\begin{align}
    \nonumber
    Q^{\pi}(\boldsymbol{s}_t,\boldsymbol{a}_t)&=r(\boldsymbol{s}_t,\boldsymbol{a}_t)+\mathbb{E}_{\rho_{\pi}} \left[\sum_{l=1}^{\infty}\gamma^l(r(\boldsymbol{s}_{t+l}, \boldsymbol{a}_{t+l})-\beta\log \pi(\boldsymbol{a}_{t+l}|\boldsymbol{s}_{t+l}))\right]\\
    &=r(\boldsymbol{s}_t,\boldsymbol{a}_t)+\mathbb{E}_{\rho_{\pi}} \left[-\gamma \beta\log \pi(\boldsymbol{a}_{t+1}|\boldsymbol{s}_{t+1}) +\gamma r(\boldsymbol{s}_{t+1}, \boldsymbol{a}_{t+1}) + \sum_{l=2}^{\infty}\gamma^l(r(\boldsymbol{s}_{t+l}, \boldsymbol{a}_{t+l})-\beta\log \pi(\boldsymbol{a}_{t+l}|\boldsymbol{s}_{t+l}))\right]\\
    &=r(\boldsymbol{s}_t,\boldsymbol{a}_t)+\gamma\mathbb{E}_{\rho_{\pi}} \left[-\beta\log \pi(\boldsymbol{a}_{t+1}|\boldsymbol{s}_{t+1}) + r(\boldsymbol{s}_{t+1}, \boldsymbol{a}_{t+1})+\sum_{l=1}^{\infty}\gamma^l(r(\boldsymbol{s}_{t+1+l}, \boldsymbol{a}_{t+1+l})-\beta\log \pi(\boldsymbol{a}_{t+1+l}|\boldsymbol{s}_{t+1+l}))\right]\\
    \label{equation::soft_bellman_equation}
    &=r(\boldsymbol{s}_t,\boldsymbol{a}_t) + \gamma\mathbb{E}_{\boldsymbol{s}_{t+1} \sim p, \boldsymbol{a}_{t+1} \sim \pi}\left[-\beta\log \pi(\boldsymbol{a}_{t+1}|\boldsymbol{s}_{t+1}) +Q^{\pi}(\boldsymbol{s}_{t+1},\boldsymbol{a}_{t+1})\right].
\end{align}

Equation \ref{equation::soft_bellman_equation} is called the soft Bellman equation. 

\begin{lemma}
\label{lemma::soft_policy_evaluation}
    \textbf{(Soft Policy Evaluation)} \textit{$Q_{\theta}$ converges to the soft Q-function of $\pi_{\phi}$ as $L(\theta) \to 0$.}
\end{lemma}

\begin{proof}
    Define the soft Bellman operator $\mathcal{T}^{\pi}$ as:
\begin{equation}
    \mathcal{T}^{\pi}Q(s,a)=r(\boldsymbol{s}, \boldsymbol{a}) + \gamma \mathbb{E}_{\boldsymbol{s}' \sim p, \boldsymbol{a}' \sim \pi} \left[ Q(\boldsymbol{s}',\boldsymbol{a}') - \beta \log \pi(\boldsymbol{a}'|\boldsymbol{s}') \right].
\end{equation}

For two Q-function $Q$ and $Q'$, we have
\begin{align}
    \left|\mathcal{T}^{\pi}Q(\boldsymbol{s},\boldsymbol{a})-\mathcal{T}^{\pi}Q'(\boldsymbol{s},\boldsymbol{a})\right|
    &=\left|\gamma\mathbb{E}_{\boldsymbol{s}' \sim p, \boldsymbol{a}' \sim \pi} \left[Q(\boldsymbol{s}',\boldsymbol{a}')-Q'(\boldsymbol{s}',\boldsymbol{a}')\right]\right|\\
    &\leq \gamma \mathbb{E}_{\boldsymbol{s}' \sim p, \boldsymbol{a}' \sim \pi} \left[\left| Q(\boldsymbol{s}',\boldsymbol{a}')-Q'(\boldsymbol{s}',\boldsymbol{a}')\right |\right]\\
    &\leq \gamma \max_{(\boldsymbol{s}',\boldsymbol{a}')}\left|Q(\boldsymbol{s}',\boldsymbol{a}')-Q'(\boldsymbol{s}',\boldsymbol{a}')\right|\\
    &=\gamma \parallel Q-Q' \parallel_{\infty}
\end{align}
Then
\begin{align}
    \parallel\mathcal{T}^{\pi}Q-\mathcal{T}^{\pi}Q'\parallel_{\infty} 
    &\leq\gamma \parallel Q-Q' \parallel_{\infty},
\end{align}
which proves that the soft Bellman operator $\mathcal{T}^{\pi}$ is a contraction. It has a unique fixed point $Q^{\pi}$. When Q-function loss $L(\theta)=0$, the Q-function $Q_{\theta}$ satisfies the soft Bellman equation $Q_{\theta}(\boldsymbol{s},\boldsymbol{a})=r(\boldsymbol{s}, \boldsymbol{a}) + \gamma \mathbb{E}_{\boldsymbol{s}' \sim p, \boldsymbol{a}' \sim \pi_{\phi}} \left[ Q_{\theta}(\boldsymbol{s}',\boldsymbol{a}') - \beta \log \pi_{\phi}(\boldsymbol{a}'|\boldsymbol{s}') \right]$ for all $(\boldsymbol{s},\boldsymbol{a})\in |\mathcal{S}| \times |\mathcal{A}|$, indicating that $Q_{\theta}$ converges to the true soft Q-function of $\pi_{\phi}$.
\end{proof}

\begin{lemma}
\label{lemma::soft_policy_improvement}
    \textbf{(Soft Policy Improvement)} \textit{Let \( \pi_{\phi_k} \in \Pi\) and assume $Q_{\theta}=Q^{\pi_{\phi_k}}$ after soft policy evaluation. If $\pi_{\phi_{k+1}}$ is the minimizer of the loss defined in Equation \ref{equation::SAC policy loss}, then $Q^{ \pi_{\phi_{k+1}}}(\boldsymbol{s}, \boldsymbol{a}) \geq Q^{ \pi_{\phi_k}}(\boldsymbol{s}, \boldsymbol{a})$ for all $(\boldsymbol{s}, \boldsymbol{a}) \in \mathcal{S} \times \mathcal{A}$ with $|\mathcal{A}| < \infty$. }
\end{lemma}

\begin{proof}
Since the new policy $\pi_{\phi_{k+1}}$ is the minimizer of the loss defined in Equation \ref{equation::SAC policy loss}, it holds that 
\begin{align}
    \pi_{\phi_{k+1}}(\cdot|\boldsymbol{s}) 
    & = \arg \min_{\pi \in \Pi} \text{D}_{\text{KL}} \left( \pi(\cdot | \boldsymbol{s}) \ \middle\| \ \frac{\exp(\frac{1}{\beta}Q_{\theta}(\boldsymbol{s}, \cdot))}{Z_{\theta}(\boldsymbol{s})} \right)\\
    &= \arg \min_{\pi \in \Pi} \text{D}_{\text{KL}} \left( \pi(\cdot | \boldsymbol{s}) \ \middle\| \ \frac{\exp(\frac{1}{\beta}Q^{\pi_{\phi_k}}(\boldsymbol{s}, \cdot))}{Z^{\pi_{\phi_k}}(\boldsymbol{s})} \right)\\
    &=\arg \min_{\pi \in \Pi} \left\{\mathbb{E}_{\boldsymbol{a}\sim \pi(\cdot|\boldsymbol{s})}\left[\log \pi(\boldsymbol{a}|\boldsymbol{s}) - \frac{1}{\beta}Q^{\pi_{\phi_k}}(\boldsymbol{s}, \boldsymbol{a})+\log Z^{\pi_{\phi_k}}(\boldsymbol{s})\right]\right\}\\
    &=\arg \min_{\pi \in \Pi} \left\{\mathbb{E}_{\boldsymbol{a}\sim \pi(\cdot|\boldsymbol{s})}\left[\log \pi(\boldsymbol{a}|\boldsymbol{s}) - \frac{1}{\beta} Q^{\pi_{\phi_k}}(\boldsymbol{s}, \boldsymbol{a})\right] + \log Z^{\pi_{\phi_k}}(\boldsymbol{s})\right\}\\
    \label{equation::eq_new_old}
    &=\arg \min_{\pi \in \Pi} \left\{\mathbb{E}_{\boldsymbol{a}\sim \pi(\cdot|\boldsymbol{s})}\left[\log \pi(\boldsymbol{a}|\boldsymbol{s}) - \frac{1}{\beta} Q^{\pi_{\phi_k}}(\boldsymbol{s}, \boldsymbol{a})\right] \right\}.
\end{align}

Since \( \pi_{\phi_k} \in \Pi\), we have

\begin{equation}
\label{equation::new_old}
    \mathbb{E}_{\boldsymbol{a}\sim \pi_{\phi_{k+1}}(\cdot|\boldsymbol{s})}\left[\log \pi_{\phi_{k+1}}(\boldsymbol{a}|\boldsymbol{s}) - \frac{1}{\beta} Q^{\pi_{\phi_k}}(\boldsymbol{s}, \boldsymbol{a})\right] \leq \mathbb{E}_{\boldsymbol{a}\sim \pi_{\phi_{k}}(\cdot|\boldsymbol{s})}\left[\log \pi_{\phi_{k}}(\boldsymbol{a}|\boldsymbol{s}) - \frac{1}{\beta} Q^{\pi_{\phi_k}}(\boldsymbol{s}, \boldsymbol{a})\right].
\end{equation}

According the soft Bellman equation, the Q-function of $\pi_{\phi_k}$ satisfies 

\begin{align}
    Q^{\pi_{\phi_k}}(\boldsymbol{s}_{t},\boldsymbol{a}_{t})
    &=r(\boldsymbol{s}_{t},\boldsymbol{a}_{t}) + \gamma\mathbb{E}_{\boldsymbol{s}_{t+1} \sim p,\boldsymbol{a}_{t+1} \sim\pi_{\phi_k}}\left[Q^{\pi_{\phi_k}}(\boldsymbol{s}_{t+1},\boldsymbol{a}_{t+1}) -\beta\log \pi_{\phi_k}(\boldsymbol{a}_{t+1}|\boldsymbol{s}_{t+1})\right]\\
    &\leq r(\boldsymbol{s}_{t},\boldsymbol{a}_{t}) + \gamma\mathbb{E}_{\boldsymbol{s}_{t+1} \sim p,\boldsymbol{a}_{t+1} \sim\pi_{\phi_{k+1}}}\left[Q^{\pi_{\phi_k}}(\boldsymbol{s}_{t+1},\boldsymbol{a}_{t+1}) -\beta\log \pi_{\phi_{k+1}}(\boldsymbol{a}_{t+1}|\boldsymbol{s}_{t+1})\right]\\
    &= r(\boldsymbol{s}_{t},\boldsymbol{a}_{t})+ \gamma\mathbb{E}_{\boldsymbol{s}_{t+1} \sim p,\boldsymbol{a}_{t+1} \sim\pi_{\phi_{k+1}}} \left[r(\boldsymbol{s}_{t+1},\boldsymbol{a}_{t+1}) -\beta\log \pi_{\phi_{k+1}}(\boldsymbol{a}_{t+1}|\boldsymbol{s}_{t+1})\right]
    \\& \hspace{0.64in}+\gamma^2\mathbb{E}_{\boldsymbol{s}_{t+1} \sim p,\boldsymbol{a}_{t+1} \sim\pi_{\phi_{k+1}}, \boldsymbol{s}_{t+2} \sim p,\boldsymbol{a}_{t+2} \sim\pi_{\phi_k}}\left[Q^{\pi_{\phi_k}}(\boldsymbol{s}_{t+2},\boldsymbol{a}_{t+2}) -\beta\log \pi_{\phi_k}(\boldsymbol{a}_{t+2}|\boldsymbol{s}_{t+2})\right]\\
    &\vdots \\
    \label{equation::Q_new_old}
    &\leq Q^{\pi_{\phi_{k+1}}}(\boldsymbol{s}_{t},\boldsymbol{a}_{t}),
\end{align}
which is proved by repeatedly expanding $Q^{\pi_{\phi_{k}}}$ using the soft Bellman equation and applying Equation \ref{equation::new_old}. Then the proof for Lemma \ref{lemma::soft_policy_improvement} is completed.
\end{proof}

\begin{theorem}
% \label{theorem::soft policy iteration}
    \textbf{(Soft Policy Iteration)} In the tabular setting, let $L(\theta_k)=0$ and $L(\phi_k)$ be minimized for each $k$. Repeated application of policy evaluation and policy improvement, i.e., $k \to \infty$, $\pi_{\phi_k}$ will converge to a policy $\pi^*$ such that $Q^{ \pi^{*}}(\boldsymbol{s}, \boldsymbol{a}) \geq Q^{ \pi}(\boldsymbol{s}, \boldsymbol{a})$ for all $\pi \in \Pi$ and $(\boldsymbol{s}, \boldsymbol{a}) \in \mathcal{S} \times \mathcal{A}$ with $|\mathcal{A}| < \infty$.
\end{theorem}

\begin{proof}
According to Lemma \ref{lemma::soft_policy_evaluation} and \ref{lemma::soft_policy_improvement}, when $L(\theta_k)=0$ and $L(\phi_k)$ be minimized for each $k$, we have $\forall k, Q^{ \pi_{\phi_{k+1}}} \geq Q^{ \pi_{\phi_k}}$. This indicates that the sequence $Q^{ \pi_{\phi_k}}$ is monotonically increasing. Furthermore, the Q-function is bounded since both the reward and entropy are bound. Therefore, when $k \to \infty$, the policy sequence converges to some $\pi^*$. Below we will prove that $\pi^*$ is the optimal MaxEnt policy within $\pi$.

Since $\pi^*$ has already converged, it satisfies 
\begin{equation}
    \pi^*(\cdot|\boldsymbol{s})=\arg \min_{\pi \in \Pi} \left\{\mathbb{E}_{\boldsymbol{a}\sim \pi(\cdot|\boldsymbol{s})}\left[\log \pi(\boldsymbol{a}|\boldsymbol{s}) - \frac{1}{\beta}Q^{\pi^*}(\boldsymbol{s}, \boldsymbol{a})\right] \right\}
\end{equation}
following Equation \ref{equation::eq_new_old}. Then for all $\pi \in \Pi$, it holds that
\begin{equation}
    \mathbb{E}_{\boldsymbol{a}\sim \pi^*(\cdot|\boldsymbol{s})}\left[\log \pi^*(\boldsymbol{a}|\boldsymbol{s}) - \frac{1}{\beta} Q^{\pi^*}(\boldsymbol{s}, \boldsymbol{a})\right] \leq \mathbb{E}_{\boldsymbol{a}\sim \pi(\cdot|\boldsymbol{s})}\left[\log \pi(\boldsymbol{a}|\boldsymbol{s}) - \frac{1}{\beta} Q^{\pi^*}(\boldsymbol{s}, \boldsymbol{a})\right].
\end{equation}
Using the same iterative argument as in the proof of Equation \ref{equation::Q_new_old}, we can derive $Q^{ \pi^{*}}(\boldsymbol{s}, \boldsymbol{a}) \geq Q^{ \pi}(\boldsymbol{s}, \boldsymbol{a})$ for all $(\boldsymbol{s}, \boldsymbol{a}) \in \mathcal{S} \times \mathcal{A}$. Consequently, $\pi^*$ is the optimal MaxEnt policy within $\Pi$. The proof is completed.
\end{proof}


\subsection{The decomposition of the condition distribution $p(\boldsymbol{a}_0|\boldsymbol{a}_t)$}
\label{appendix::condition distribution}

According to the Bayesian rule, the conditional distribution $p(\boldsymbol{a}_0|\boldsymbol{a}_t)$ satisfies:
\begin{align}
    p(\boldsymbol{a}_0|\boldsymbol{a}_t) 
    &= \frac{p(\boldsymbol{a}_0) p(\boldsymbol{a}_t|\boldsymbol{a_0})}{p(\boldsymbol{x}_t)} \\
    & \propto p(\boldsymbol{a}_0) p(\boldsymbol{a}_t|\boldsymbol{a_0})\\
    \label{joint probability decomposition}
    & \propto \exp(\frac{1}{\beta}Q(\boldsymbol{a}_0)) \mathcal{N}(\boldsymbol{a}_t | \sqrt{\sigma(\alpha_t)} \boldsymbol{a}_0, \sigma(-\alpha_t) \boldsymbol{I}),
\end{align}
where Equation \ref{joint probability decomposition} is derived by substituting Equation \ref{target distribution} and \ref{transition distribution}. For the same $\boldsymbol{a}_0$ and $\boldsymbol{a}_t$, the probability density of $\boldsymbol{a}_t$ following the Gaussian distribution $\mathcal{N}(\boldsymbol{a}_t | \sqrt{\sigma(\alpha_t)} \boldsymbol{a}_0, \sigma(-\alpha_t) \boldsymbol{I})$ is equal to the probability density of $\boldsymbol{a}_0$ following the Gaussian distribution $\mathcal{N}(\boldsymbol{a}_0 | \frac{1}{\sqrt{\sigma(\alpha_t)}} \boldsymbol{a}_t, \frac{\sigma(-\alpha_t)}{\sigma(\alpha_t)} \boldsymbol{I})$ up to a constant to compensate for the scale difference between the two random variables. Then we have
 \begin{equation}
 \label{eqution::condition distribution}
     p(\boldsymbol{a}_0|\boldsymbol{a}_t) \propto \exp(\frac{1}{\beta}Q(\boldsymbol{a}_0)) \mathcal{N}(\boldsymbol{a}_0 | \frac{1}{\sqrt{\sigma(\alpha_t)}} \boldsymbol{a}_t, \frac{\sigma(-\alpha_t)}{\sigma(\alpha_t)} \boldsymbol{I}).
\end{equation}

\subsection{Estimating the score function with importance sampling}
\label{appendix::importance sampling}
The score function satisfies
\begin{align}
     \nabla_{\boldsymbol{a}_t} \log p(\boldsymbol{a}_t) 
     &= \mathbb{E}_{p(\boldsymbol{a}_0|\boldsymbol{a}_t)}\left[\nabla_{\boldsymbol{a}_t} \log p(\boldsymbol{a}_t|\boldsymbol{a}_0)\right]\\
    &=\mathbb{E}_{\boldsymbol{a}_0 \sim \mathcal{N}(\boldsymbol{a}_0 | \frac{1}{\sqrt{\sigma(\alpha_t)}} \boldsymbol{a}_t, \frac{\sigma(-\alpha_t)}{\sigma(\alpha_t)} \boldsymbol{I})}\left[\frac{p(\boldsymbol{a}_0|\boldsymbol{a}_t)}{\mathcal{N}(\boldsymbol{a}_0 | \frac{1}{\sqrt{\sigma(\alpha_t)}} \boldsymbol{a}_t, \frac{\sigma(-\alpha_t)}{\sigma(\alpha_t)} \boldsymbol{I})}\nabla_{\boldsymbol{a}_t} \log p(\boldsymbol{a}_t|\boldsymbol{a}_0)\right] \\
    \label{equation::for_iDEM_start}
    &=\mathbb{E}_{\boldsymbol{a}_0 \sim \mathcal{N}(\boldsymbol{a}_0 | \frac{1}{\sqrt{\sigma(\alpha_t)}} \boldsymbol{a}_t, \frac{\sigma(-\alpha_t)}{\sigma(\alpha_t)} \boldsymbol{I})}\left[w(\boldsymbol{a}_0)\nabla_{\boldsymbol{a}_t} \log p(\boldsymbol{a}_t|\boldsymbol{a}_0)\right]\\
    \label{equation::expectation_a0}
    &=\mathbb{E}_{\boldsymbol{a}_0 \sim \mathcal{N}(\boldsymbol{a}_0 | \frac{1}{\sqrt{\sigma(\alpha_t)}} \boldsymbol{a}_t, \frac{\sigma(-\alpha_t)}{\sigma(\alpha_t)} \boldsymbol{I})}\left[-w(\boldsymbol{a}_0)\frac{\boldsymbol{a}_t-\sqrt{\sigma(\alpha_t)}\boldsymbol{a}_0}{\sigma(-\alpha_t)}\right],
\end{align}
where the importance ratio $w(\boldsymbol{a}_0)=\frac{\exp(\frac{1}{\beta}Q(\boldsymbol{a}_0))}{Z(\boldsymbol{a}_t)}$ with $Z(\boldsymbol{a}_t)=\int \exp(\frac{1}{\beta}Q(\boldsymbol{a}_0)) \mathcal{N}(\boldsymbol{a}_0 | \frac{1}{\sqrt{\sigma(\alpha_t)}} \boldsymbol{a}_t, \frac{\sigma(-\alpha_t)}{\sigma(\alpha_t)} \boldsymbol{I}) \text{d}\boldsymbol{a}_0$ being the normalizing constant of $p(\boldsymbol{a}_0|\boldsymbol{a}_t)$. Let $\boldsymbol{a}_0 = \frac{1}{\sqrt{\sigma(\alpha_t)}} \boldsymbol{a}_t + \frac{\sqrt{\sigma(-\alpha_t)}}{\sqrt{\sigma(\alpha_t)}}\boldsymbol{\epsilon}$, then Equation \ref{equation::expectation_a0} can be rewritten as
\begin{align}
    \nabla_{\boldsymbol{a}_t} \log p(\boldsymbol{a}_t) 
     &= \frac{1}{\sqrt{\sigma(-\alpha_t)}} \cdot \mathbb{E}_{\boldsymbol{\epsilon} \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol{I})}\left[w(\boldsymbol{a}_0)\boldsymbol{\epsilon}\right]\\
     &\approx \frac{1}{\sqrt{\sigma(-\alpha_t)}} \cdot\frac{1}{K} \sum_{i=1}^K w(\boldsymbol{a}_0^i) \boldsymbol{\epsilon}^i,
\end{align}
where $\boldsymbol{\epsilon}^1, \dots, \boldsymbol{\epsilon}^K \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol{I})$, $\boldsymbol{a}_0^i=\frac{1}{\sqrt{\sigma(\alpha_t)}} \boldsymbol{a}_t + \frac{\sqrt{\sigma(-\alpha_t)}}{\sqrt{\sigma(\alpha_t)}}\boldsymbol{\epsilon}^i$.

\subsection{The derivation of the iDEM method}
\label{appendix::iDEM}
We provide the derivation of the iDEM method to demonstrate the difference and relationship between Q-weighted noise estimation and iDEM. Our derivation is equivalent to the official proof of iDEM although in a different way. 

Since $\nabla_{\boldsymbol{a}_0}\log \mathcal{N}(\boldsymbol{a}_0 | \frac{1}{\sqrt{\sigma(\alpha_t)}} \boldsymbol{a}_t, \frac{\sigma(-\alpha_t)}{\sigma(\alpha_t)} \boldsymbol{I})=\sqrt{\sigma(\alpha_t)}\cdot\frac{\boldsymbol{a}_t-\sqrt{\sigma(\alpha_t)}\boldsymbol{a}_0}{\sigma(-\alpha_t)}$ and $\nabla_{\boldsymbol{a}_t} \log p(\boldsymbol{a}_t|\boldsymbol{a}_0)=-\frac{\boldsymbol{a}_t-\sqrt{\sigma(\alpha_t)}\boldsymbol{a}_0}{\sigma(-\alpha_t)}$, it holds that

\begin{equation}
\label{equation::gradiant_relation}
    \nabla_{\boldsymbol{a}_t} \log p(\boldsymbol{a}_t|\boldsymbol{a}_0)=-\frac{1} {\sqrt{\sigma(\alpha_t)}} \nabla_{\boldsymbol{a}_0}\log \mathcal{N}(\boldsymbol{a}_0 | \frac{1}{\sqrt{\sigma(\alpha_t)}} \boldsymbol{a}_t, \frac{\sigma(-\alpha_t)}{\sigma(\alpha_t)} \boldsymbol{I}).
\end{equation}

Substitute Equation \ref{equation::gradiant_relation} into Equation \ref{equation::for_iDEM_start}, we have 
\begin{align}
    \nabla_{\boldsymbol{a}_t} \log p(\boldsymbol{a}_t) 
    &= -\frac{1} {\sqrt{\sigma(\alpha_t)}} \mathbb{E}_{\boldsymbol{a}_0 \sim \mathcal{N}(\boldsymbol{a}_0 | \frac{1}{\sqrt{\sigma(\alpha_t)}} \boldsymbol{a}_t, \frac{\sigma(-\alpha_t)}{\sigma(\alpha_t)} \boldsymbol{I})}\left[w(\boldsymbol{a}_0)\nabla_{\boldsymbol{a}_0}\log \mathcal{N}(\boldsymbol{a}_0 | \frac{1}{\sqrt{\sigma(\alpha_t)}} \boldsymbol{a}_t, \frac{\sigma(-\alpha_t)}{\sigma(\alpha_t)} \boldsymbol{I})\right]\\
    \label{equation::crosspoint}
    &=-\frac{1} {\sqrt{\sigma(\alpha_t)}}\int w(\boldsymbol{a}_0)\nabla_{\boldsymbol{a}_0} \mathcal{N}(\boldsymbol{a}_0 | \frac{1}{\sqrt{\sigma(\alpha_t)}} \boldsymbol{a}_t, \frac{\sigma(-\alpha_t)}{\sigma(\alpha_t)} \boldsymbol{I}) \text{d} \boldsymbol{a}_0.
\end{align}
After applying the integration by parts formula, Equation \ref{equation::crosspoint} can be expanded to
\begin{align}
    \nabla_{\boldsymbol{a}_t} \log p(\boldsymbol{a}_t) 
    &=\frac{1} {\sqrt{\sigma(\alpha_t)}}\int \nabla_{\boldsymbol{a}_0} w(\boldsymbol{a}_0)\cdot \mathcal{N}(\boldsymbol{a}_0 | \frac{1}{\sqrt{\sigma(\alpha_t)}} \boldsymbol{a}_t, \frac{\sigma(-\alpha_t)}{\sigma(\alpha_t)} \boldsymbol{I}) \text{d} \boldsymbol{a}_0.
\end{align}
Since $w(\boldsymbol{a}_0)=\frac{\exp(\frac{1}{\beta}Q(\boldsymbol{a}_0))}{Z(\boldsymbol{a}_t)}$, it satisfies that $\nabla_{\boldsymbol{a}_0} w(\boldsymbol{a}_0)=w(\boldsymbol{a}_0) \nabla_{\boldsymbol{a}_0}\frac{1}{\beta}Q(\boldsymbol{a}_0)$. Then we have
\begin{align}
    \nabla_{\boldsymbol{a}_t} \log p(\boldsymbol{a}_t)
    &=\frac{1} {\sqrt{\sigma(\alpha_t)}}\int w(\boldsymbol{a}_0)\nabla_{\boldsymbol{a}_0}\frac{1}{\beta}Q(\boldsymbol{a}_0)\cdot \mathcal{N}(\boldsymbol{a}_0 | \frac{1}{\sqrt{\sigma(\alpha_t)}} \boldsymbol{a}_t, \frac{\sigma(-\alpha_t)}{\sigma(\alpha_t)} \boldsymbol{I}) \text{d} \boldsymbol{a}_0\\
    \label{equation::weighted_gradient}
    &=\frac{1} {\sqrt{\sigma(\alpha_t)}} \mathbb{E}_{\boldsymbol{a}_0 \sim \mathcal{N}(\boldsymbol{a}_0 | \frac{1}{\sqrt{\sigma(\alpha_t)}} \boldsymbol{a}_t, \frac{\sigma(-\alpha_t)}{\sigma(\alpha_t)} \boldsymbol{I})}\left[w(\boldsymbol{a}_0)\nabla_{\boldsymbol{a}_0}\frac{1}{\beta}Q(\boldsymbol{a}_0)\right]
\end{align}
Equation \ref{equation::weighted_gradient} appears similar to Equation \ref{equation::expectation_a0}, except that the random variable in the expectation transfers from Q-weighted noise to Q-weighted gradient. Utilizing the same weighted importance sampling method as Q-weighted noise, the score function can be estimated by
\begin{align}
    \nabla_{\boldsymbol{a}_t} \log p(\boldsymbol{a}_t)
    &\approx \frac{1}{\sqrt{\sigma(\alpha_t)}} \cdot \sum_{i=1}^K \frac{w(\boldsymbol{a}_0^i)}{\sum_{j=1}^K w(\boldsymbol{a}_0^j)} \nabla_{\boldsymbol{a}_0^i} \frac{1}{\beta} Q(\boldsymbol{a}_0^i)\\
    &= \frac{1}{\sqrt{\sigma(\alpha_t)}} \sum_{i=1}^K \text{softmax}(\frac{1}{\beta}Q(\boldsymbol{a}_0^{1:K}))_i \nabla_{\boldsymbol{a}_0^i} \frac{1}{\beta} Q(\boldsymbol{a}_0^i).
\end{align}

\subsection{Probabity Approximation of Diffusion Policy using Numerical Integration Techniques}
\label{appendix::numerial integration}
We use numerical integration techniques to estimate the following integral:
\begin{equation}
    \log p_{\phi}(\boldsymbol{a}_0)= c - \frac{1}{2} \int_{-\infty}^{+\infty}\mathbb{E}_{\boldsymbol{\epsilon} }\left[\parallel \boldsymbol{\epsilon} - \boldsymbol{\epsilon}_{\phi}(\boldsymbol{a}_t, \alpha_t) \parallel^2_2 \right]\text{d}\alpha_t,
\end{equation}
where $c=-\frac{d}{2}\log (2\pi e)+\frac{d}{2} \int_{-\infty}^{+\infty} \sigma(\alpha_t) \text{d}\alpha_t$ with $d$ being the dimension of $\boldsymbol{a}_0$, $\boldsymbol{\epsilon} \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol{I})$, $\boldsymbol{a}_t = \sqrt{\sigma(\alpha_t)} \boldsymbol{a}_0 + \sqrt{\sigma(-\alpha_t)} \boldsymbol{\epsilon}$. 
First, we replace the integral variable for $\sigma(\alpha_t)$ as it has a narrow integration domain of $[0,1]$:
\begin{equation}
    \log p_{\phi}(\boldsymbol{a}_0)= -\frac{d}{2}\log (2\pi e) + \frac{1}{2} \int_{0}^{1} \left(d \cdot \sigma(\alpha_t) - \mathbb{E}_{\boldsymbol{\epsilon} }\left[\parallel \boldsymbol{\epsilon} - \boldsymbol{\epsilon}_{\phi}(\boldsymbol{a}_t, \alpha_t) \parallel^2_2 \right]\right)\frac{\text{d} \sigma(\alpha_t)}{\sigma(\alpha_t)\sigma(-\alpha_t)}.
\end{equation}
In practice, we calculate the integral between $[\sigma(\alpha_{t_{\text{max}}}), \sigma(\alpha_{t_{\text{min}}})]$ for numerical stability, where in our experiments, $t_{\text{min}}=1e-3$ and $t_{\text{max}}=0.9946$. Obtain $T+1$ discrete timesteps by setting $t_{i} = t_{\text{min}} + \frac{i}{T} (t_{\text{max}} - t_{\text{min}})$, $i=0,1,\dots, T$. Then the integration domain of $[\sigma(\alpha_{t_{\text{max}}}), \sigma(\alpha_{t_{\text{min}}})]$ is split into $T$ intervals, where the range of the $i\text{-th}$ segment is $[\sigma(\alpha_{t_{i}}), \sigma(\alpha_{t_{i-1}})]$. Using the left-hand endpoints to represent the function value of each interval, the integral can be approximated by
\begin{equation}
    \log p_{\phi}(\boldsymbol{a}_0)\approx -\frac{d}{2}\log (2\pi e) + \frac{1}{2}\sum_{i=1}^T \left(d \cdot \sigma(\alpha_{t_i}) - \mathbb{E}_{\boldsymbol{\epsilon} }\left[\parallel \boldsymbol{\epsilon} - \boldsymbol{\epsilon}_{\phi}(\boldsymbol{a}_{t_i}, \alpha_{t_i}) \parallel^2_2 \right]\right)\frac{ \sigma(\alpha_{t_{i-1}})- \sigma(\alpha_{t_{i}})}{\sigma(\alpha_{t_i})\sigma(-\alpha_{t_i})}.
\end{equation}
Estimating the noise predicting error $\mathbb{E}_{\boldsymbol{\epsilon} }\left[\parallel \boldsymbol{\epsilon} - \boldsymbol{\epsilon}_{\phi}(\boldsymbol{a}_{t_i}, \alpha_{t_i}) \parallel^2_2 \right]$ using Monte Carlo samples, we have 
\begin{equation}
    \label{eqaution::complete integral}
    \log p_{\phi}(\boldsymbol{a}_0)\approx -\frac{d}{2}\log (2\pi e) + \frac{1}{2}\sum_{i=1}^T \left(d \cdot \sigma(\alpha_{t_i}) - \frac{1}{N}\sum^N_{j=1}\parallel \boldsymbol{\epsilon}^j - \boldsymbol{\epsilon}_{\phi}(\boldsymbol{a}_{t_i}^j, \alpha_{t_i}) \parallel^2_2 \right)\frac{ \sigma(\alpha_{t_{i-1}})- \sigma(\alpha_{t_{i}})}{\sigma(\alpha_{t_i})\sigma(-\alpha_{t_i})},
\end{equation}
where $\boldsymbol{\epsilon}^1,\dots,\boldsymbol{\epsilon}^N \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol{I})$, $\boldsymbol{a}_{t_i}^j = \sqrt{\sigma(\alpha_{t_i})} \boldsymbol{a}_0 + \sqrt{\sigma(-\alpha_{t_i})} \boldsymbol{\epsilon}^j$. The equation \ref{eqaution::complete integral} can be short for 
\begin{equation}
    \log p_{\phi}(\boldsymbol{a}_0)\approx c' + \frac{1}{2}\sum_{i=1}^T w_{t_i} \left( d \cdot \sigma(\alpha_{t_i}) - \tilde{\boldsymbol{\epsilon}}_{\phi}(\boldsymbol{a}_{t_i}, \alpha_{t_i})\right)
\end{equation}
where $c'=-\frac{d}{2}\log (2\pi e)$, $w_{t_i}=\frac{\sigma(\alpha_{t_{i-1}}) - \sigma(\alpha_{t_{i}})}{\sigma(\alpha_{t_i})\sigma(-\alpha_{t_i})}$ is the weight at $t_i$, $\tilde{\boldsymbol{\epsilon}}_{\phi}(\boldsymbol{a}_{t_i}, \alpha_{t_i})=\frac{1}{N}\sum^N_{j=1}\parallel \boldsymbol{\epsilon}^j - \boldsymbol{\epsilon}_{\phi}(\boldsymbol{a}_{t_i}^j, \alpha_{t_i}) \parallel^2_2$ is the noise prediction error estimation at $t_i$.

% \subsection{Pseudocode of MaxEnt RL with Diffusion Policy}
% \begin{algorithm}
% \label{algorithm::MaxEntDP}
% \caption{MaxEnt RL with Diffusion Policy}
% \begin{algorithmic}[1]
% \State Initialize critic networks $Q_{\theta_1}$, $Q_{\theta_2}$, and the noise prediction network $\boldsymbol{\epsilon}_{\phi}$ with random parameters $\theta_1, \theta_2, \phi$.
% \State Initialize target networks $\theta'_1 \leftarrow \theta_1, \theta'_2 \leftarrow \theta_2$
% \State Initialize replay buffer $\mathcal{D}$
% \FOR{each iteration}
%     \FOR{each sampling step}
%     \State Sample $\boldsymbol{a} \sim \pi_{\phi}(\cdot|\boldsymbol{s})$ according to Equation \ref{PFODE}
%     \State Step environment $\boldsymbol{s'}, r$  $\leftarrow$ $\text{env}(\boldsymbol{a})$
%     \State Store $(\boldsymbol{s}, \boldsymbol{a}, r, \boldsymbol{s}')$ in $\mathcal{D}$
%     \ENDFOR
%     \FOR{each update step}
%     \State Sample a batch of B transitions $(\boldsymbol{s}, \boldsymbol{a}, r, \boldsymbol{s}')$ from $\mathcal{D}$
%     \State Sample the next action $\boldsymbol{a}' \sim \pi_{\phi}(\cdot|\boldsymbol{s}')$ according to Equation \ref{PFODE}
%     \State Compute the log probability $\log\pi_{\phi}(\boldsymbol{a}'|\boldsymbol{s}')$ using Equation \ref{equation::numeracal integral}
%     \State Compute the target Q-value: $
%         \hat{Q}(\boldsymbol{s},\boldsymbol{a}) = r(\boldsymbol{s}, \boldsymbol{a}) + \gamma  \left( \min_{i=1,2}Q_{\theta_i}(\boldsymbol{s}',\boldsymbol{a}') - \beta \log \pi_{\phi}(\boldsymbol{a}'|\boldsymbol{s}') \right). 
%     $
%     \State Update critics: $\theta_{i} = \arg\min_{\theta_i}\frac{1}{B}\sum (Q_{\theta_i}(\boldsymbol{s}, \boldsymbol{a}) - \hat{Q}(\boldsymbol{s}, \boldsymbol{a}))^2$
%     \State Sample $t \sim \mathcal{U}([t_{\text{min}}, t_{\text{max}}])$ and sample the noisy action $\boldsymbol{a}_t \sim \mathcal{N}(\boldsymbol{a}_t| \sqrt{\sigma(\alpha_t)}\boldsymbol{a}, \sigma(-\alpha_t)\boldsymbol{I})$
%     \State Estimate the target noise $\boldsymbol{\epsilon}^{*}(\boldsymbol{a}_t, \alpha_t, \boldsymbol{s})$ with Equation \ref{equation::noise target} 
%     \State Update the noise prediction network: $\phi=\arg \min_{\phi}\frac{1}{B}\sum \parallel \boldsymbol{\epsilon}_{\phi}(\boldsymbol{a}_t, \alpha_t, \boldsymbol{s}) - \boldsymbol{\epsilon}^{*}(\boldsymbol{a}_t, \alpha_t, \boldsymbol{s})\parallel^2_2$
%     \State Updtae target networks: $\theta_{i} \leftarrow \tau \theta_{i} + (1 - \tau)\theta_{i}$
%     \ENDFOR

% \ENDFOR
% \end{algorithmic}
% \end{algorithm}

\section{Experimental Details.}

\subsection{Hyperparameter Analysis}

\begin{figure}[ht]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.7\columnwidth]{pics/ablation/ablation_parameter.pdf}}
\caption{Learning curves of different parameter settings on HalfCheetah-v3 benchmark. (a) Testing different numbers of samples $K$ for Q-weighted noise estimation. (b) Testing different numbers of samples $N$ for probability approximation. (c) Testing different diffusion steps $T$. (d) Testing different candidate numbers $M$ for action selection.}
\label{fig::ablation_parameter}
\end{center}
\vskip -0.3in
\end{figure}

In this subsection, we analyze the effect of different hyperparameter settings on the performance:

\begin{itemize}
    \item \textbf{Sample Number for Q-weighted Noise Estimation.} The Q-weighted noise estimation can be seen as a weighted importance sampling method to estimate the training target of the noise prediction network. With more samples, the estimation will be more accurate and less varied, which benefits the training of diffusion policy. This is consistent with the observation in Figure \ref{fig::ablation_parameter}(a) that the performance will be better with a larger $K$. We select $K=500$ since it can obtain good performance and cause relatively small computation costs.
    \item \textbf{Sample Number for Probability Approximation.} For probability approximation of diffusion policy, several Monte Carlo samples are utilized to estimate the noise prediction error at each diffusion timestep. This sample number is also preferred to be large for higher accuracy and less variance. The performance of different sample numbers $N$ is shown in Figure \ref{fig::ablation_parameter}(b). We set $N=50$ after trading off performance and computation efficiency.
    \item \textbf{Diffusion Steps.} Due to the discretization error of ODE solvers, the actual distribution of generated actions may be different from the diffusion policy induced by the noise prediction network. Therefore, when the diffusion steps $T$ is small, the non-negligible discretization error will disrupt the training process and lead to a performance drop. As shown in Figure \ref{fig::ablation_parameter}(c), the performance is higher with larger $T$. We choose $T=20$ as the default setting for a balance between performance and computation efficiency.
    \item \textbf{Candidate Number for Action Selection.} By selecting the action with the highest Q-value among several action candidates, the action selection technique can further improve the performance of the diffusion policy when testing. Figure \ref{fig::ablation_parameter}(d) demonstrates that a larger number of action candidates will result in a better performance. Consequently, we set $M=10$ by default. 
    \item \textbf{Temperature Coefficient.} The temperature coefficient $\beta$, which determines the exploration strength, is an important parameter in the MaxEnt RL framework. Since the difficulty and reward scales vary across different tasks, different $\beta$ need to be set for different tasks. We sweep over $[0.01, 0.02, 0.05, 0.1, 0.2]$ to find the optimal setting for each task, displaying the results in Figure \ref{fig::ablation_temp}. The temperature coefficient selected for each task is listed in Table \ref{tab::temp}.
\end{itemize}

\begin{figure}[ht]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{pics/ablation/ablation_temp.pdf}}
\caption{Learning curves of different temperature coefficients on Mujoco benchmarks.}
\label{fig::ablation_temp}
\end{center}
\vskip -0.3in
\end{figure}

\begin{table}[t]
\caption{The temperature coefficients adapted for each benchmark.}
\label{tab::temp}
\vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
\begin{tabular}{cc}
\toprule
Benchmark & Temperature coefficient \\
\midrule
Ant-v3 & 0.05 \\
HalfCheetah-v3 & 0.2 \\
Hopper-v3 & 0.05 \\
Humanoid-v3 & 0.02 \\
Swimmer-v3 & 0.01 \\
Walker2d-v3 & 0.01 \\
\bottomrule
\end{tabular}
% \end{small}
% \end{center}
\vskip -0.1in
\end{table}

\subsection{Hyperparameters settings}
All experiments in this paper are conducted on a GPU of Nvidia GeForce RTX 3090 and a CPU of AMD EPYC 7742. Our implementation of SAC, MEow, TD3, QSM, DACER, QVPO, and DIPO follows their official codes: \hyperlink{https://github.com/toshikwa/soft-actor-critic.pytorch}{https://github.com/toshikwa/soft-actor-critic.pytorch}, \hyperlink{https://github.com/ChienFeng-hub/meow}{https://github.com/ChienFeng-hub/meow}, \hyperlink{https://github.com/sfujim/TD3}{https://github.com/sfujim/TD3}, \hyperlink{https://github.com/Alescontrela/score_matching_rl}{https://github.com/Alescontrela/score\_matching\_rl}, \hyperlink{https://github.com/happy-yan/DACER-Diffusion-with-Online-RL}{https://github.com/happy-yan/DACER-Diffusion-with-Online-RL}, \hyperlink{https://github.com/wadx2019/qvpo}{https://github.com/wadx2019/qvpo}, and \hyperlink{https://github.com/BellmanTimeHut/DIPO}{https://github.com/BellmanTimeHut/DIPO}. The shared hyperparameters of all algorithms are listed in Table  \ref{tab::parameter}\footnote{When comparing with other diffusion-based algorithms, MaxEntDP uses 3-layer MLPs as the actor and critic networks following the default settings of these algorithms. In other experiments, 2-layer MLPs are used as they can already attain good performance.}.

\begin{table}[t]
\caption{The shared hyperparameters of all algorithms.}
\label{tab::parameter}
\vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
\begin{tabular}{lcccccccc}
\toprule
Hyperparameter & MaxEntDP & SAC & MEow & TD3 & QSM & DACER &  QVPO & DIPO  \\
\midrule
Batch size & 256 & 256 & 256 & 256 & 256 & 256 & 256 & 256 \\
Discount factor $\gamma$ & 0.99 & 0.99 & 0.99 & 0.99 & 0.99 & 0.99 & 0.99 & 0.99 \\
Target smoothing coefficient $\tau$ & 0.005 & 0.005 & 0.005 & 0.005 & 0.005 & 0.005 & 0.005 & 0.005 \\
No. of hidden layers & 2 & 2 & 2 & 2 & 3 & 3 & 3 & 3 \\
No. of hidden nodes & 256 & 256 & 256 & 256 & 256 & 256 & 256 & 256 \\
Actor learning rate & 3e-4 & 3e-4 & 3e-4 & 3e-4 & 3e-4 & 3e-4 & 3e-4 & 3e-4 \\
Critic learning rate & 3e-4 & 3e-4 & 3e-4 & 3e-4 & 3e-4 & 3e-4 & 3e-4 & 3e-4 \\
Activation & mish & relu & relu & relu & mish & mish & mish & mish \\
Replay buffer size & 1e6 & 1e6 & 1e6 & 1e6 & 1e6 & 1e6 & 1e6 & 1e6 \\
Diffusion steps & 20 & N/A & N/A & N/A & 20 & 20 & 20 & 20 \\
Action candidate number & 10 & N/A & N/A & N/A & N/A & N/A & 32 & N/A \\
\bottomrule
\end{tabular}
% \end{small}
% \end{center}
\vskip -0.1in
\end{table}

\subsection{Training time}
The training time for all algorithms is presented in Table \ref{tab::training_time}. Leveraging the computation efficiency of JAX \cite{frostig2018compiling} and the parallel processing capabilities of GPU, MaxEntDP demonstrates high training efficiency compared to competing methods, only behind TD3 and QSM. This highlights its advantage for real-world applications that require high computation efficiency.

\begin{table}[t]
\caption{The comparison of training time on HalfCheetah-v3 benchmark.}
\label{tab::training_time}
\vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
\begin{tabular}{lcccc}
\toprule
Algorithm (2-layer MLP) & MaxEntDP (jax) & SAC & MEow & TD3\\
\midrule
Training time (h) & 3.9 & 4.6 & 11 & 1.7 \\
\bottomrule
\end{tabular}

\vskip 0.1in

\begin{tabular}{lccccc}
\toprule
Algorithm (3-layer MLP) & MaxEntDP (jax) & QSM (jax) & DACER (jax) & QVPO & DIPO\\
\midrule
Training time (h) & 5.5 & 1.9 & 5.9 & 22.6 & 55.6 \\
\bottomrule
\end{tabular}
% \end{sc}
% \end{small}
% \end{center}
\vskip -0.1in
\end{table}

%%=============================================%%
%% For submissions to Nature Portfolio Journals %%
%% please use the heading ``Extended Data''.   %%
%%=============================================%%

%%=============================================================%%
%% Sample for another appendix section			       %%
%%=============================================================%%

%% \section{Example of another appendix section}\label{secA2}%
%% Appendices may be used for helpful, supporting or essential material that would otherwise 
%% clutter, break up or be distracting to the text. Appendices can consist of sections, figures, 
%% tables and equations etc.

\end{appendices}

%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%
%%===========================================================================================%%


\end{document}
