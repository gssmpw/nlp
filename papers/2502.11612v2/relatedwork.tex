\section{Related Work}
\textbf{MaxEnt RL}
A variety of approaches have been proposed to achieve the MaxEnt RL objective. SQL \cite{haarnoja2017reinforcement} introduces soft Q-learning to learn the optimal soft Q-function and trains an energy-based model using the amortized Stein variational gradient method to generate actions according to the exponential of the optimal soft Q-function. SAC \cite{haarnoja2018soft} presents the soft actor-critic algorithm, which iteratively improves the policy towards a higher soft value, and provides an implementation using Gaussian policies. MEow \cite{chao2024maximum} employs energy-based normalizing flows as unified policies to represent both the actor and the critic, simplifying the training process for MaxEnt RL. This paper highlights the importance of policy representation within the MaxEnt RL framework: a more expressive policy representation enhances exploration and facilitates closer convergence to the optimal MaxEnt policy. Diffusion models, which are more expressive than Gaussian distributions and energy-based normalizing flows and easier to train and sample than energy-based models, present an ideal policy representation that effectively balances expressiveness and the complexity of training and inference.

% \subsection{Diffusion Policies for Offline RL}
% Due to its strong expressivity in modeling complex multi-modal distributions, the diffusion model has been widely adopted as a policy representation to address the limitations of the commonly used Gaussian policy.

\textbf{Diffusion Policies for Offline RL.} Offline RL attempts to learn a well-performing policy from a pre-collected dataset. Collected by multiple policies, the offline datasets may exhibit high skewness and multi-modality. Diffusion Policy \cite{chi2023diffusion} trains a diffusion model to approximate the multi-modal expert behavior by behavior cloning. To optimize the policy for higher performance, Diffusion-QL \cite{wangdiffusion} combines the diffusion loss with Q-value loss evaluated on the generated actions, CEP \cite{lu2023contrastive} trains a separate guidance network using Q-function to guide the actions to regions with high Q values, and EDA \cite{chenaligning} employs direct preference optimization to align the diffusion policy with Q-function. To improve the training and inference speed of diffusion policy, EDP \cite{kang2024efficient} adopts action approximation and efficient ODE sampler DPM-solver for action generation, and CPQL \cite{chen2024boosting} utilizes the consistency policy \cite{song2023consistency}, a one-step diffusion policy. Due to the lack of online samples, the above approaches require staying close to the behavior policy to prevent out-of-distribution actions whose performances are unpredictable. However, in this paper, we focus on online RL, where online interactions are accessible to correct the errors in value evaluation. Therefore, different techniques should be developed to employ diffusion models in online RL.

\textbf{Diffusion Policies for Online RL.} In online RL, a key challenge lies in balancing exploration and exploitation. Previous studies \cite{psenkalearning,yang2023policy,ding2024diffusion,wang2024diffusion} apply expressive diffusion models as policy representations to promote the exploration of the state-action space. QSM \cite{psenkalearning} fits the exponential of the Q-function by training a score network to approximate the action gradient of the Q-function. DIPO \cite{yang2023policy} improves the actions by applying the action gradient of the Q-function and clones the improved actions. QVPO \cite{ding2024diffusion} weights the diffusion loss with the Q-value, assigning probabilities to actions that are linearly proportional to the Q-value. DACER \cite{wang2024diffusion} optimizes the Q-value loss to generate actions with high Q values and adds extra noise to the generated actions to keep a constant policy entropy. Unlike previous approaches, we employ the MaxEnt RL objective to encourage exploration and enhance policy robustness. Similar to QSM, we train the diffusion model to fit the exponential of the Q-function. However, our Q-weighted noise estimation method is more accurate and stable. Furthermore, we include policy entropy when computing the Q-function, which can further promote exploration.