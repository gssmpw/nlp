\section{Related Work}
\subsection{Unsafe Prompts Detection via External APIs or Tools}
To moderate unsafe prompts, technical vendors have engaged in developing moderation APIs, such as Azure AI Content Safety \citep{azure_ai_content_safety_2024}, Perspective API \citep{perspective_api_2024}, OpenAI Moderation API \citep{openai_moderation_2024}, Baidu Text Moderation \citep{baidu_text_censoring} and Alibaba Content Moderation \citep{aliyun_image_audit}. These moderation APIs typically employ a hybrid architecture that integrates rule-based filtering mechanisms for explicit content detection with machine learning models trained on various safety-related datasets. Meanwhile, the scope of application for these moderation APIs differs. For instance, the Perspective API \citep{perspective_api_2024} is primarily focused on analyzing the presence of harmful or offensive language, such as detecting toxicity or insults within text. In contrast, the OpenAI Moderation API \citep{openai_moderation_2024} is designed for content moderation of language model outputs, specifically assessing whether the generated content violates OpenAI's defined usage policies.


Additionally, external tools are often employed to detect unsafe prompts in LLMs. For instance, Detoxify \citep{Detoxify} is an open-source toxic comment detection tool that includes three key functionalities: toxic comment classification, detection of unintended bias in toxic comments, and multilingual toxic comment classification. HateBERT \citep{caselli-etal-2021-hatebert} is a bert-based model trained as a tool for detecting abusive language, with training data collected from controversial communities on Reddit. 

While these APIs and tools provide valuable moderation services, they often require customized engineering efforts to support their functionality.

\subsection{Guardrail Models for Unsafe Prompt Detection}
Recent advancements in detecting unsafe prompts in LLMs have predominantly focused on fine-tuning-based methodologies. The Llama Guard series \citep{inan2023llama, dubey2024llama3herdmodels} address this challenge by training language models on extensive annotated datasets to assess whether given input prompts or model responses might violating predefined safety policies. 

The latest Llama Guard 3 Vision \citep{chi2024llama}, extends this capability to multi-modal scenarios, enabling safety checks for both text and image inputs. Furthermore, ShieldLM \citep{DBLP:conf/emnlp/ZhangLMZLKSSSWH24} enhances the safety risk detection performance and decision transparency by providing explanatory rationales through data-augmented fine-tuning. In addition, Code Shield \citep{inan2023llama} is designed to help developers reduce the likelihood of generating potentially unsafe code. It filters out unsafe code during the inference phase, thereby effectively mitigating related risks and ensuring the safety of code execution. NeMo Guardrails \citep{rebedea2023nemo} employs a dialogue management-inspired runtime mechanism that empowers developers to augment Large Language Model (LLM) as guardrails using language instructions. 

Recently, GuardAgent \citep{xiang2024guardagent} was introduced as a guardrail agent aimed at ensuring that the inputs or outputs of an LLM agent comply with specific safety policies. The guradrail agent functions by initially analyzing the guard requests to create a task plan, followed by the generation and subsequent execution of guardrail script based on the formulated plan. 

Unlike these data-driven LLM guardrail models, our proposed method performs unsafe prompt detection using only few safe and unsafe prompts. It is efficient in terms of both data and computation, while also achieving strong performance. 

\subsection{Gradient-Based Analysis of LLMs}
One primary objective of gradient analysis is to examine how input features contribute to model outputs. For example, \citet{shrikumar2016not} propose a method that evaluates the contribution of each input feature to the modelâ€™s output by computing the gradient of the output with respect to (wrt) the input features and performing an element-wise multiplication with the input values. This approach is straightforward and computationally efficient. Furthermore, Layer-wise Relevance Propagation (LRP) \citep{bach2015pixel} starts from the model's output and propagates ``relevance'' scores backward layer by layer until reaching the input layer, allowing for fine-grained feature attribution. Moreover, Axiomatic Attribution \citep{pmlr-v70-sundararajan17a} leverages integrated gradients to compute the attribution of input features to the predictive output grounded in two fundamental axioms: sensitivity and implementation invariance, further enhancing the feature attribution performance.

Additionally, there are gradient-based analysis methods tailored for specific model architectures, such as GradCAM \citep{selvaraju2017grad}, which generates class-specific heatmaps by leveraging the gradients of the target class wrt the final convolutional layer, highlighting the important regions in the input image that contribute to the model's decision.  Recently, GradSafe \citep{xie2024gradsafe} propose a gradient based method to assess the safety of a given prompt by measuring the directional similarity (cosine similarity) between the gradients of the given prompt and those of the unsafe reference prompts.

Unlike GradSafe \citep{xie2024gradsafe} that restricts its analysis to the gradient direction similarity, our proposed method uses the gradient co-occurrence scores to accommodate unsigned similarity, therefore analyzing the overall patterns of the gradients. Our approach mitigates the directional bias, leading to enhanced performance in unsafe prompt detection.