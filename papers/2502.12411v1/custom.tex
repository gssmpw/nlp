% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
% \usepackage[review]{acl}
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% 新增用于algorithm-begin
% \usepackage[linesnumbered,ruled,vlined]
% \usepackage[ruled,vlined]{algorithm2e}
% \DontPrintSemicolon
% % Define pseudocode formatting
% \renewcommand{\KwSty}[1]{\textnormal{\textcolor{blue!90!black}{\ttfamily\bfseries #1}}\unskip}
% \renewcommand{\ArgSty}[1]{\textnormal{\ttfamily #1}\unskip}
% \SetKwComment{Comment}{\color{green!50!black}// }{}
% \renewcommand{\CommentSty}[1]{\textnormal{\ttfamily\color{green!50!black}#1}\unskip}
% \newcommand{\assign}{\leftarrow}
% \newcommand{\var}{\texttt}
% \newcommand{\FuncCall}[2]{\texttt{\bfseries #1(#2)}}
% \SetKwProg{Function}{function}{}{}
% \renewcommand{\ProgSty}[1]{\texttt{\bfseries #1}}
% 新增用于algorithm-end

% \usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{amsmath}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usepackage{float}



\title{Gradient Co-occurrence Analysis for Detecting Unsafe Prompts in Large Language Models}


\newcommand*{\samethanks}[1][\value{footnote}]{\footnotemark[#1]}

\author{Jingyuan Yang\textsuperscript{1,2}, Bowen Yan\textsuperscript{3}, Rongjun Li\textsuperscript{2}, Ziyu Zhou\textsuperscript{2}, Xin Chen\textsuperscript{4}, Zhiyong Feng\textsuperscript{1,}\thanks{Corresponding Author} \and Wei Peng\textsuperscript{2,}\samethanks  \\
\textsuperscript{1}College of Intelligence and Computing, Tianjin University \\
\textsuperscript{2}IT Innovation and Research Center, Huawei Technologies \\
\textsuperscript{3}Artificial Intelligence Academy, Beijing University of Posts and Telecommunications \\
\textsuperscript{4}IT Platform Dept 1, Huawei Technologies \\
\{yangjingyuan2, lirongjun3, zhouziyu8, chenxin247, peng.wei1\}@huawei.com\\ yanbowen@bupt.edu.cn, zyfeng@tju.edu.cn}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
Unsafe prompts pose significant safety risks to large language models (LLMs). Existing methods for detecting unsafe prompts rely on data-driven fine-tuning to train guardrail models, necessitating  significant data and computational resources. In contrast, recent few-shot gradient-based methods emerge, requiring only few safe and unsafe reference prompts. A gradient-based approach identifies unsafe prompts by analyzing consistent patterns of the gradients of safety-critical parameters in LLMs. Although effective, its restriction to directional similarity (cosine similarity) introduces ``directional bias'', limiting its capability to identify unsafe prompts. To overcome this limitation, we introduce \textbf{GradCoo}, a novel gradient co-occurrence analysis method that expands the scope of safety-critical parameter identification to include unsigned gradient similarity, thereby reducing the impact of ``directional bias'' and enhancing the accuracy of unsafe prompt detection. Comprehensive experiments on the widely-used benchmark datasets ToxicChat and XStest demonstrate that our proposed method can achieve state-of-the-art (SOTA) performance compared to existing methods. Moreover, we confirm the generalizability of \textbf{GradCoo} in detecting
unsafe prompts across a range of LLM base models with various sizes and origins. 



%This method assesses the safety of a given prompt by measuring the directional similarity (cosine similarity) between the gradients of the given prompt and those of the unsafe reference prompts. Although this method has shown some effectiveness, we argue that the gradient directional similarity measure suffers from directional bias, as it emphasizes the similarity in gradients direction while overlooking the overall gradients patten similarly. To address this challenge, we propose \textbf{GradCoo} a novel gradient co-occurrence analysis method for detecting unsafe prompts. This approach utilizes a gradient co-occurrence score to account for the overall similarity of the gradient patterns, thereby mitigating the impact of directional bias and improving the performance of unsafe prompt detection. Comprehensive experiments on the widely-used benchmark datasets ToxicChat and XStest demonstrates that our proposed method can achieve SOTA performance compared to existing methods, confirming its effectiveness. 
% Furthermore, our proposed method exhibits generalizability and can be extended to other tasks such as fact hallucination detection.  It also achieves significant results on this task, demonstrating its potential for broader applicability.
\end{abstract}

\section{Introduction}
\label{sec:intro}
\begin{figure}[ht]
  \centering
\includegraphics[width=1\linewidth]{imgs/Figure1.pdf}
  \caption{A scenario depicting gradients grouping under the influence of directional bias. Safe, unsafe reference prompts (prompts A, B) are illustrated in yellow and red respectively. We use light green to represent input prompt to be classified (prompt C). (a) Incorrect grouping of gradients of prompts A and C due to directional bias; (b) Eliminating directional bias produces desirable grouping of gradients of prompts A and C. The gradients of a safe prompt (A), an unsafe prompt (B) and the prompt to be classified (C) are illustrated as triangles with yellow, red and light green color in an LLM's representation space.} 
  % However, the gradient direction similarity method leads to incorrect prediction due to the direction differences for certain gradients (sign mismatch). }
  % We call this problem \textbf{direction bias} in this paper. In contrast, our method can capture the similarity of overall gradients patterns, thereby avoiding this issue.}
  \label{cos_logic}
\end{figure}

%The development of large language models (LLMs) \cite{achiam2023gpt, bai2023qwen} has revolutionized the field of NLP. However, unsafe prompts have introduced significant safety risks that hinder the broader application of LLMs.  For example, unsafe prompts can guide LLMs to generate content that is typically prohibited or restricted, leading to the production of toxic and harmful material.

%Current mainstream methods for detecting unsafe prompts can be broadly classified into two categories: one based on external moderation APIs, and the other on data-driven fine-tuning approaches. External APIs primarily leverage a combination of rules and various machine learning techniques for unsafe prompt detection \citep{azure_ai_content_safety_2024,perspective_api_2024, openai_moderation_2024}. In contrast, data-driven methods involve collecting large amount of safety-relevant dataset to train guardrails models for unsafe prompts detection \cite{inan2023llama, dubey2024llama3herdmodels,azaria2023internal}. However, these methods often require extensive dataset curation or significant computational resources, making them cost-prohibitive. Recently, \citet{xie2024gradsafe} introduced GradSafe, a few-shot gradient-based method for detecting unsafe prompts in LLMs. This approach relies on the finding that gradients of an LLM's loss for unsafe prompts, when paired with compliance responses, exhibit high directional similarity (cosine similarity). In contrast, gradients corresponding to safe prompts show lower directional similarity. While this method has demonstrated effectiveness, we contend that the gradient directional similarity measure is prone to directional bias, as it focuses on directional similarity while neglecting the similarity of the overall gradients patterns. For example, as depicted in Figure \ref{cos_logic}, the overall gradients patterns of $Grad_{pred}$ and $Grad_{unsafe}$ are closer than $Grad_{safe}$. However, the gradient direction similarity method leads to incorrect predictions due to the gradient direction bias. In contrast, our method places greater emphasis on the overall similarity of gradient patterns, thereby avoiding this issue.

%In this paper, we propose \textbf{GradCoo} a novel method for detecting unsafe prompts using gradient co-occurrence analysis with few safe and unsafe prompts. Specifically, we first construct the safe and unsafe gradient references from few safe and unsafe prompts respectively. Then, we aggregate the gradient co-occurrence scores of the given prompts with the safe and unsafe gradient references across each model component (e.g., attention heads or MLPs) to classify unsafe prompts. Comprehensive experiments on unsafe prompt detection datasets, including ToxicChat and XStest, demonstrate the effectiveness of our method. Our method also achieves significant results on the fact hallucination detection task, underscoring its generalizability for other tasks. The contributions of our paper can be summarized as follows:

%\begin{itemize}
    %\item We propose a novel gradient co-occurrence analysis method for detecting unsafe prompts in LLMs, requiring only few safe and unsafe prompts. Extensive experiments demonstrate its effectiveness and show that it can achieve the SOTA performance compared to existing methods.
    %\item Our proposed method is generic and can also achieves significant results in the fact hallucination detection task, underscoring its potential for broader applicability.
%\end{itemize}

The use of unsafe prompts poses significant risks, limiting the widespread adoption of large language models (LLMs). For example, these prompts can be exploited by malicious actors to generate prohibited or restricted content, enabling the development of substances for mass destruction. Current mainstream methods for detecting unsafe prompts fall into two categories: external moderation APIs and data-driven fine-tuning approaches. The former primarily leverages a combination of rules and various machine learning techniques to identify unsafe prompts \citep{azure_ai_content_safety_2024,perspective_api_2024, openai_moderation_2024}. In contrast, data-driven methods involve collecting large amounts of safety-relevant datasets to train guardrail models for detecting unsafe signals \cite{inan2023llama, dubey2024llama3herdmodels,azaria2023internal}. However, these methods are often cost-prohibitive due to need for large and curated datasets and significant computational resources for model training. Recently, \citet{xie2024gradsafe} propose GradSafe, a gradient-based method for detecting unsafe prompts in LLMs in a low resource setting based on few prompting examples. This approach identifies unsafe prompts by investigating consistent patterns of the gradients of safety-critical parameters in LLMs. It has been observed that gradients of an LLM's loss for unsafe prompts (when paired with compliance responses) exhibit high directional similarity (cosine similarity). In contrast, gradients corresponding to safe prompts show a lower directional similarity to those of their unsafe counterparts. While this method has demonstrated effectiveness in detecting unsafe prompts, using cosine similarity to benchmark gradients of safety-critical parameters  may be error-prone due to the presence of ``directional bias''. As depicted in Figure \ref{cos_logic}, GradSafe tends to over-weigh gradients with the same sign (or direction), i.e., prompts A and C under the influence of directional bias, therefore potentially neglecting gradients for prompting pairs with strong unsigned similarity (prompts B and C) and leading to inaccurate grouping.  

%we contend that the gradient directional similarity measure is prone to directional bias, as it focuses on directional similarity while neglecting the similiarty of the overall gradients patterns. For example, as depicted in Figure \ref{cos_logic}, the overall gradient patterns of $Grad_{pred}$ and $Grad_{unsafe}$ are closer than $Grad_{safe}$. However, the gradient direction similarity method leads to incorrect predictions due to the gradient direction bias. In contrast, our method places greater emphasis on the overall similarity of gradient patterns, thereby avoiding this issue.


% we argue that gradient co-occurrence score offers more fundamental clues for identifying unsafe prompts in LLMs compared to gradient directional similarity. 

In this paper, we propose \textbf{GradCoo}, a novel method for detecting unsafe prompts based on gradient co-occurrence analysis to accommodate a broader scope of safety-critical parameters with unsigned gradient similarity. Specifically, we first construct the safe and unsafe gradient references from few safe and unsafe prompts respectively. Then, we produce the overall unsafe classification score by aggregating the component-level (e.g., attention heads or MLPs) gradient co-occurrence scores of the given prompts wrt the safe and unsafe gradient references to classify unsafe prompts. Comprehensive experiments on unsafe prompt detection datasets, including ToxicChat and XStest, demonstrate the effectiveness of our method. 
% Our method also achieves significant results on the LLM hallucination detection task, underscoring its generalizability across safety-relevant detection tasks. 

The contributions of our paper can be summarized as follows:

\begin{itemize}
    \item We propose a novel gradient co-occurrence analysis method for detecting unsafe prompts in LLMs, requiring only few safe and unsafe prompts. Our extensive experiments show that it outperforms existing methods, achieving state-of-the-art (SOTA) results in unsafe prompt detection;
    \item \textbf{GradCoo} is a general approach for detecting unsafe prompts across a range of LLM base models with various sizes and origins. 
\end{itemize}


\section{Related Work}
\subsection{Unsafe Prompts Detection via External APIs or Tools}
To moderate unsafe prompts, technical vendors have engaged in developing moderation APIs, such as Azure AI Content Safety \citep{azure_ai_content_safety_2024}, Perspective API \citep{perspective_api_2024}, OpenAI Moderation API \citep{openai_moderation_2024}, Baidu Text Moderation \citep{baidu_text_censoring} and Alibaba Content Moderation \citep{aliyun_image_audit}. These moderation APIs typically employ a hybrid architecture that integrates rule-based filtering mechanisms for explicit content detection with machine learning models trained on various safety-related datasets. Meanwhile, the scope of application for these moderation APIs differs. For instance, the Perspective API \citep{perspective_api_2024} is primarily focused on analyzing the presence of harmful or offensive language, such as detecting toxicity or insults within text. In contrast, the OpenAI Moderation API \citep{openai_moderation_2024} is designed for content moderation of language model outputs, specifically assessing whether the generated content violates OpenAI's defined usage policies.


Additionally, external tools are often employed to detect unsafe prompts in LLMs. For instance, Detoxify \citep{Detoxify} is an open-source toxic comment detection tool that includes three key functionalities: toxic comment classification, detection of unintended bias in toxic comments, and multilingual toxic comment classification. HateBERT \citep{caselli-etal-2021-hatebert} is a bert-based model trained as a tool for detecting abusive language, with training data collected from controversial communities on Reddit. 

While these APIs and tools provide valuable moderation services, they often require customized engineering efforts to support their functionality.

\subsection{Guardrail Models for Unsafe Prompt Detection}
Recent advancements in detecting unsafe prompts in LLMs have predominantly focused on fine-tuning-based methodologies. The Llama Guard series \citep{inan2023llama, dubey2024llama3herdmodels} address this challenge by training language models on extensive annotated datasets to assess whether given input prompts or model responses might violating predefined safety policies. 

The latest Llama Guard 3 Vision \citep{chi2024llama}, extends this capability to multi-modal scenarios, enabling safety checks for both text and image inputs. Furthermore, ShieldLM \citep{DBLP:conf/emnlp/ZhangLMZLKSSSWH24} enhances the safety risk detection performance and decision transparency by providing explanatory rationales through data-augmented fine-tuning. In addition, Code Shield \citep{inan2023llama} is designed to help developers reduce the likelihood of generating potentially unsafe code. It filters out unsafe code during the inference phase, thereby effectively mitigating related risks and ensuring the safety of code execution. NeMo Guardrails \citep{rebedea2023nemo} employs a dialogue management-inspired runtime mechanism that empowers developers to augment Large Language Model (LLM) as guardrails using language instructions. 

Recently, GuardAgent \citep{xiang2024guardagent} was introduced as a guardrail agent aimed at ensuring that the inputs or outputs of an LLM agent comply with specific safety policies. The guradrail agent functions by initially analyzing the guard requests to create a task plan, followed by the generation and subsequent execution of guardrail script based on the formulated plan. 

Unlike these data-driven LLM guardrail models, our proposed method performs unsafe prompt detection using only few safe and unsafe prompts. It is efficient in terms of both data and computation, while also achieving strong performance. 

\subsection{Gradient-Based Analysis of LLMs}
One primary objective of gradient analysis is to examine how input features contribute to model outputs. For example, \citet{shrikumar2016not} propose a method that evaluates the contribution of each input feature to the model’s output by computing the gradient of the output with respect to (wrt) the input features and performing an element-wise multiplication with the input values. This approach is straightforward and computationally efficient. Furthermore, Layer-wise Relevance Propagation (LRP) \citep{bach2015pixel} starts from the model's output and propagates ``relevance'' scores backward layer by layer until reaching the input layer, allowing for fine-grained feature attribution. Moreover, Axiomatic Attribution \citep{pmlr-v70-sundararajan17a} leverages integrated gradients to compute the attribution of input features to the predictive output grounded in two fundamental axioms: sensitivity and implementation invariance, further enhancing the feature attribution performance.

Additionally, there are gradient-based analysis methods tailored for specific model architectures, such as GradCAM \citep{selvaraju2017grad}, which generates class-specific heatmaps by leveraging the gradients of the target class wrt the final convolutional layer, highlighting the important regions in the input image that contribute to the model's decision.  Recently, GradSafe \citep{xie2024gradsafe} propose a gradient based method to assess the safety of a given prompt by measuring the directional similarity (cosine similarity) between the gradients of the given prompt and those of the unsafe reference prompts.

Unlike GradSafe \citep{xie2024gradsafe} that restricts its analysis to the gradient direction similarity, our proposed method uses the gradient co-occurrence scores to accommodate unsigned similarity, therefore analyzing the overall patterns of the gradients. Our approach mitigates the directional bias, leading to enhanced performance in unsafe prompt detection.

\section{Method}
\begin{figure*}[ht]
  \centering
  \includegraphics[width=1\linewidth]{imgs/Figure2.pdf}
  \caption{The flowchart of our proposed Gradient Co-occurrence method contains two main steps. (1). The first step extracts the safe and unsafe parameter gradients by computing the gradients from safe/unsafe reference prompts and removing corresponding directional and magnitude biases. (2). The second step aggregate the gradients' co-occurrence scores to determine the safety of the input prompt.}
  \label{fig:safe_hallu_detect}
\end{figure*}

As illustrated in Figure \ref{fig:safe_hallu_detect}, our proposed method comprises two main steps. In the first step, we first extract safe and unsafe parameter gradients by computing the gradients of an LLM's loss for prompts paired with compliance responses such as ``Sure'', followed by the approach of \citet{xie2024gradsafe}. Next, we slice the parameter gradients at the component level (e.g., attention head, MLP) and then remove both directional and magnitude biases of the sliced gradients to construct safe gradient and unsafe gradient references. In the second step, we compute the gradients co-occurrence scores between the gradients of the given prompt and the safe/unsafe gradient references across all model components. These scores are then aggregated to determine the safety of the given input prompt.


\subsection{Safe and Unsafe Gradient References Construction}
To calculate the safe and unsafe gradient references, we first need a set of safe and unsafe prompt reference texts. We use the same safe/unsafe prompt reference texts as \citet{xie2024gradsafe} to ensure a fair comparison. After feeding in safe/unsafe prompt reference texts, we obtain the generated responses and compute the loss wrt the predefined compliance responses such as ``Sure''.  Backpropagation is performed to produce the parameter gradients for all model parameters. We use the averaged safe/unsafe parameter gradients from these two pairs of safe/unsafe prompts as the safe/unsafe parameter gradient references $G_s$ and $G_u$ respectively. 

As mentioned in Section \ref{sec:intro}, directly using these safe/unsafe parameter gradients introduces a directional bias. Additionally, for safe/unsafe reference texts with different literal expressions, the corresponding loss values may vary. This results in magnitude bias for gradients during backpropagation. It is also noted that, due to GPU memory limitations, we are unable to use the safe/unsafe gradient references for computation across all model parameters. 

To mitigate these biases while adhering to GPU memory constraints, we first partition the model parameter gradients according to their respective model components. Within each component gradients, we apply the standard deviation normalization and the absolute value function to reduce the directional and magnitude biases. We define the safe/unsafe gradients as $g^c_s \in G_s$ and $g^c_u \in G_u$, where $g^c_s$ and $g^c_u$ represent the safe/unsafe gradients corresponding to model component $c$.


\subsection{Gradient Co-occurrence Scores Aggregation for Unsafe Prompts Detection}
To determine whether a given prompt is safe or unsafe, we first compute the gradients $g^c_p \in G_p$ for the given prompt in the same manner as the safe/unsafe gradient references. Next, we calculate the gradient co-occurrence scores between the $g^c_p \in G_p$ with the  safe/unsafe gradient references $g^c_s \in G_s$ and $g^c_u \in G_u$ respectively. Subsequently, we compute a relative unsafe score for each model component $c \in C$.

After that, we aggregate these scores across model components to obtain an overall unsafe classification score. The given prompt with a score above a predefined threshold is classified as unsafe, otherwise it was regarded as safe. The specific procedure is detailed in Algorithm \ref{algo_dict}.

\input{algorithm_1}

{\linespread{1.1}
\begin{table*}[t]
  \centering
  \begin{tabular}{lcccccccc}
    \hline
     & \multicolumn{4}{c}{\textbf{ToxicChat}} & \multicolumn{4}{c}{\textbf{XSTest}} \\
    \cmidrule(r){2-5} 
    \cmidrule(r){6-9} 
    & AUPRC&P&R& F1 & AUPRC&P&R&F1\\
    \hline
    OpenAI Moderation API  &0.604 &\cellcolor{gray!45}0.815&0.145&0.246 &0.779  &0.878&0.430&0.577\\
    Perspective API  &0.487  &0.614&0.148&0.238&0.713  &0.835&0.330&0.473\\
    Azure API  &-  &0.559&0.634&0.594 &-   &0.673&0.700&0.686 \\
    GPT-4  &-  &0.475&\cellcolor{gray!45}0.831&0.604 &-   &\cellcolor{gray!15}0.878&\cellcolor{gray!15}0.970&\cellcolor{gray!45}0.921\\
    Llama2-7B-Chat  &-  &0.241&\cellcolor{gray!15}0.822&0.373 &-  &0.509&\cellcolor{gray!45}0.990&0.672 \\
    Llama Guard  &0.635 &0.744&0.396&0.517 &0.889   & 0.813&0.825&0.819\\
    GradSafe  &\cellcolor{gray!15}0.755&\cellcolor{gray!15}0.753&0.667&\cellcolor{gray!15}0.707 &\cellcolor{gray!15}0.936  &0.856&0.950&0.900 \\
    \hline
    GradCoo  &\cellcolor{gray!45}\textbf{0.789} & 0.714   &0.779   & \cellcolor{gray!45}0.745  &\cellcolor{gray!45}\textbf{0.955}   &\cellcolor{gray!45}0.889  & 0.925  & \cellcolor{gray!15}0.907 \\
    \hline

  \end{tabular} 
  \caption{Main experimental results on the unsafe prompt detection task. Dark-colored squares represent the highest value for the evaluation metric, while light-colored squares represent the second highest. The AUPRC metric is additionally bolded to signify its role as the primary evaluation metric. Baseline method results are taken from \citep{xie2024gradsafe}.}
  \label{main_result_safety}
\end{table*}


\section{Main Experiments}
\subsection{Datasets and Evaluation Metric}
To ensure comparison fairness, we adopt the same test datasets and evaluation metrics as GradSafe \citep{xie2024gradsafe}. Specifically, two datasets are used: the ToxicChat dataset \citep{lin2023toxicchat}, which consists of 10,166 toxicity-annotated prompts derived from user interactions, split into training and testing sets, with the official test set ToxicChat-1123 used for evaluation, and XSTest \citep{rottger2023xstest}, a test suite containing 250 safe prompts across 10 types and 200 corresponding crafted unsafe prompts. 

For evaluation metrics, we primarily use the Area Under the Precision-Recall Curve (AUPRC). Additionally, to ensure a comprehensive evaluation, we also report precision (P), recall (R), and F1 score (F1), consistent with prior work \cite{xie2024gradsafe, inan2023llama}.

\subsection{Baselines}
We select three types of unsafe prompt detection methods as baselines, including moderation API tools, guardrail models, and a gradient-based method. 

For the moderation API tools and guardrail models, following \cite{xie2024gradsafe}, we chose well-known moderation API tools, including the OpenAI Moderation API \citep{openai_moderation_2024}, Perspective API \citep{perspective_api_2024}, and Azure AI Content Safety API \citep{azure_ai_content_safety_2024}. 

Additionally, we use LLMs as guardrail models, applying GPT-4 \cite{achiam2023gpt} and Llama2-7B-Chat \cite{touvron2023llama} with a safety assessment prompt for unsafe prompt detection. We also utilize Llama Guard \citep{inan2023llama}, which is fine-tuned on a large safety-related dataset based on the Llama2-7B-Chat model \citep{touvron2023llama} for unsafe prompt detection.

For the gradient-based method, we employ GradSafe \cite{xie2024gradsafe}, which utilizes few safe and unsafe prompts to detect unsafe prompts. It calculates the gradient direction similarity between a given prompt and the gradients of safe/unsafe prompts to evaluate the safety of the prompt.

\subsection{Main Experimental Results}
As shown from the results in Table ~\ref{main_result_safety}, our proposed method achieves the SOTA performance compared to the mainstream baseline methods, significantly improving unsafe prompt detection performance. Specifically, our method outperforms the best baseline, GradSafe, by 3.4\% and 1.9\% in AUPRC on the ToxicChat and XSTest datasets. 

Compared to the moderation API baselines, our method outperforms  the best-performing Azure API with an F1 score improvement of 15.1\% and 22.1\% on the corresponding two datasets, respectively. 

In comparison to guardrail models, our approach demonstrates a substantial improvement in F1 score on both two test datasets, outperforming Llama2-7B-Chat and the Llama Guard significantly, which use the same backbone model. Note that, to ensure a fair comparison to these baselines, we use the Llama2-7B-Chat as the backbone model.

From these results, it is evident that our approach, using only few safe and unsafe prompts, surpasses several commercial moderation APIs. Additionally, on the same backbone model, our method outperforms both the prompt engineering-based Llama2-7B-Chat and the Llama Guard, which fine-tunes on a large set of safety data. Furthermore, compared to GradSafe, which utilizes gradient direction similarity, our proposed gradient co-occurrence analysis method demonstrates a clear advantage.


% \begin{table*}[ht]
%   \centering
%   \resizebox{0.6\textwidth}{!}{
%     \begin{tabular}{lllcccccccc}
%       % \hline
%       % \multicolumn{11}{c}{\textbf{Hallucination Detection}} \\
%       \hline
%       & & & \multicolumn{4}{c}{Movies} & \multicolumn{4}{c}{GCI} \\
%       \cmidrule(r){4-7} 
%       \cmidrule(r){8-11} 
%       & & & \multicolumn{2}{c}{AUC} & \multicolumn{2}{c}{B-ACC} & \multicolumn{2}{c}{AUC} & \multicolumn{2}{c}{B-ACC} \\
%       \hline
%       SBERT-cosine& & & \multicolumn{2}{c}{0.586} & \multicolumn{2}{c}{0.516} & \multicolumn{2}{c}{0.957} & \multicolumn{2}{c}{0.548} \\
%       ADA-cosine& &  & \multicolumn{2}{c}{0.770} & \multicolumn{2}{c}{0.501} & \multicolumn{2}{c}{0.950} & \multicolumn{2}{c}{0.820} \\
%       SelfCheckGPT& & & \multicolumn{2}{c}{0.820} & \multicolumn{2}{c}{0.634} & \multicolumn{2}{c}{\cellcolor{gray!15}0.963} & \multicolumn{2}{c}{0.927} \\
%       InterrogateLLM& & & \multicolumn{2}{c}{\cellcolor{gray!15}0.823} & \multicolumn{2}{c}{\cellcolor{gray!15}0.750} & \multicolumn{2}{c}{0.959} & \multicolumn{2}{c}{\cellcolor{gray!15}0.958} \\
%       \hline
%       GradCoo& & & \multicolumn{2}{c}{\cellcolor{gray!45}0.986} & \multicolumn{2}{c}{\cellcolor{gray!45}0.914} & \multicolumn{2}{c}{\cellcolor{gray!45}0.973} & \multicolumn{2}{c}{\cellcolor{gray!45}0.968} \\
%       \hline
%     \end{tabular}
%   }
%   \caption{Main experimental results on the fact hallucination detection task. Same as Table~\ref{main_result_safety}, dark-colored squares represent the highest value for the indicator, while light-colored squares represent the second highest. The AUC metric is additionally bolded to signify its role as the primary evaluation metric. Baseline results are taken from \citep{yehuda2024search}.}
%   \label{main_result_hallu}
% \end{table*}

% \section{Generalization Experiment for Fact Hallucination Detection}
% To validate the generalizability of our proposed method, we conduct an evaluation on the fact hallucination detection task. In this task, we calculate the safe gradient reference using the prompt with the truthful response. The unsafe gradient reference is constructed using the prompt with the hallucinated response. The given prompt gradient reference is constructed using the prompt with the corresponding LLM's response. The corresponding safe/unsafe reference prompt texts in this task can be found in Appendix \ref{appendix:A}. Similar to the unsafe prompt detection experiments, we also use two paris of safe/unsafe reference texts.

% \subsection{Datasets and Evaluation Metric}
% To ensure a fair comparison, we adopted the same test dataset and evaluation metrics as in \cite{yehuda2024search}. Specifically, the Movies dataset \cite{rounakbanik_the_movies_dataset} and the Global Country Information (GCI) dataset \cite{nelgiriyewithana_countries_of_the_world_2023} are used for fact hallucination detection. The Movies dataset consists of 3000 samples, each containing a movie title, release year, and corresponding cast information. The task is to predict the cast of a movie based on its title and release year. The GCI dataset includes information on 181 countries, such as their names, capitals, GDP, and more. The task is to predict a country's capital based on its name. Both datasets utilize Area Under the Curve (AUC) and Balanced Accuracy (B-ACC) metrics for evaluation, with the AUC serving as the primary metric.

% \subsection{Baselines}
% We compare our method with four baselines on the Movies and GCI datasets. Specifically, the SBERT-cosine \citep{reimers2019sentence} and ADA-cosine \citep{achiam2023gpt} methods compute hallucination likelihood by calculating the cosine similarity between prompt-response embeddings, differing only in their backbone models (SBERT vs. OpenAI’s ada-002). SelfCheckGPT \citep{manakul2023selfcheckgpt} detects fact hallucinations by generating multiple responses via sampling and evaluating the consistency among these generated responses and the target response. InterrogateLLM \citep{yehuda2024search} generates diverse queries based on the answer and the original query, then calculates the consistency between the reconstructed queries and the original query to detect fact hallucinations. For fairness in comparison, SelfCheckGPT, InterrogateLLM, and our method all utilize Llama2-7B-chat model \citep{touvron2023llama} as backbone model.


% \subsection{Experimental Results}
% As shown in Table~\ref{main_result_hallu}, our method significantly outperforms the best baseline, InterrogateLLM. Specifically, on the Movies dataset, our approach achieves a 16.3\% improvement in AUC and a 16.4\% improvement in B-ACC. Similarly, on the GCI dataset, our method also delivers superior results. These findings underscore the generalizability of our proposed method, demonstrating its potential to achieve substantial performance improvements on other domain tasks.

\section{Ablation Study}
\subsection{Effects of Magnitude and Directional Bias Mitigation}
We evaluate the effectiveness of our proposed method in mitigating directional \& magnitude gradient biases by testing the following scenarios: (1) impact of the normalization operation (norm) to mitigate magnitude bias; (2) impact of the absolute value function (abs) to mitigate directional bias, and (3) the effect of eliminating both operations. The experiments are conducted on the XSTest datasets.

It can be observed in Table \ref{tab:movies} that removing either the normalization operation or the absolute value function operation leads to a significant performance drop. 

Specifically, on the XStest, removing the normalization operation cause a decrease of 38.4\% in AUPRC, while removing the absolute value operation lead to a 38.0\% decrease in AUPRC, respectively. These experimental results demonstrate the critical role of the directional \& magnitude gradient bias mitigation operations in \textbf{GradCoo} to  distinguish the patterns of safe and unsafe gradients.

{\linespread{1.1}
\begin{table}[ht]
  \centering
  \resizebox{0.48\textwidth}{!}{ % 按宽度缩放表格
  \begin{tabular}{lcccc}
  \hline 
    & \multicolumn{4}{c}{\textbf{XSTest}} \\
  \cmidrule(r){2-5} 
    & \textbf{AUPRC} & \textbf{P} & \textbf{R} & \textbf{F1} \\
    \hline
   GradCoo & 0.955 & 0.889 & 0.925 & 0.907 \\
   w/o norm & 0.571 & 0.607 & 0.786 & 0.685 \\
   w/o abs & 0.575 & 0.633 & 0.749 & 0.685 \\
   w/o abs \& norm & 0.585 & 0.640 & 0.738 & 0.685 \\
  \hline
  \end{tabular}
  }
  \caption{Effects of magnitude and directional bias mitigation operations of \textbf{GradCoo} on the XSTest dataset.}
  \label{tab:movies}
\end{table}

% \begin{table}[ht]
%   \centering
%   % \resizebox{0.5\textwidth}{!}{ % 按宽度缩放表格
%   \begin{tabular}{p{3.5cm}cc}
%   \hline
%     & \multicolumn{2}{c}{\textbf{GCI}} \\
%   \cmidrule(r){2-3} 
%     & \textbf{AUC} & \textbf{B-ACC} \\
%     \hline
%    GradCoo & 0.973 & 0.968 \\
%    w/o norm & 0.887 & 0.911 \\
%    w/o abs & 0.900 & 0.921 \\
%    w/o abs \& norm & 0.880 & 0.872 \\
%   \hline
%   \end{tabular}
%   % }
%   \caption{Ablation study results for reducing magnitude and directional bias on the GCI dataset.}
%   \label{tab:gci}
% \end{table}


\section{Effects of Numbers of Safe/Unsafe Reference Pairs}
\label{Influence_k}
%To evaluate the impact of varying the number of safe/unsafe reference texts on the performance of our proposed method, we conduct experiments on the XStest dataset for unsafe prompt detection, testing with 1 to 6 pairs of safe/unsafe reference texts. 

We investigate the effect of the number of safe/unsafe reference texts on the performance of our method in the XSTest dataset for unsafe prompt detection. We experiment with 1 to 6 pairs of reference texts and further assess the model's sensitivity to reference text selection by sampling five times from the pool for each pair. 
%Additionally, to assess the model's sensitivity to the selection of different safe/unsafe reference texts, we sample five times from the safe/unsafe reference pool for each of the 1 to 6 pairs of reference texts and evaluate the sensitivity 

By calculating the standard deviation of the results across these five samplings, we quantify the sensitivity. The safe/unsafe reference pool for prompt detection is the same as in \citet{xie2024gradsafe}.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            width=0.45\textwidth,
            height=0.23\textheight,
            xlabel={Number of Safe/Unsafe Reference Prompts},
            xmin=0.8, 
            xmax=6.2,
            xtick={1,2,3,4,5,6},
            grid=major,
            grid style={dashed},
            title={XStest},
            ylabel={AUPRC with Error Bars},
            ymin=0.88, 
            ymax=0.99,
            ytick={0.88,0.90,0.92,0.94,0.96,0.98},
            legend pos=south east,
        ]
        \addplot[
            color=red,
            mark=square,
            error bars/.cd,
            y dir=both,
            y explicit
        ]
        coordinates {
            (1,0.948) +- (0,0.009) 
            (2,0.952) +- (0,0.014)
            (3,0.961) +- (0,0.012)
            (4,0.966) +- (0,0.015)
            (5,0.967) +- (0,0.013)
            (6,0.968) +- (0,0.007)
        };
        \end{axis}
    \end{tikzpicture}
    \caption{Performance Variation on the XSTest Dataset with Varying Numbers of Safe/Unsafe Reference Pairs.}
    \label{fig:xstest}
\end{figure}

As shown in Figure ~\ref{fig:xstest}, the performance is slightly improved as the number of safe/unsafe reference texts increases, but the overall trend is stable. This indicates that a small number of safe/unsafe reference texts (e.g., two pairs) are sufficient to capture the patterns of safe/unsafe gradient dynamics. Figure ~\ref{fig:xstest} further demonstrates that our method can achieve strong performance in a low-resource setting with a limited number of safe and unsafe reference prompts. Meanwhile, the standard deviation (std) remains low with values below 1\%, demonstrating our method is robust against prompting variations \citep{wang-etal-2024-assessing}.




% \begin{figure}[ht] % Second figure: GCI (Only AUC)
%     \centering
%     \begin{tikzpicture}
%         \begin{axis}[
%             width=0.45\textwidth, % 图宽度
%             height=0.2\textheight, % 图高度
%             xlabel={Number of Safe/Unsafe Reference Prompts},
%             xmin=0.8, 
%             xmax=6.2,
%             xtick={1,2,3,4,5,6},
%             grid=major,
%             grid style={dashed},
%             title={GCI},
%             ylabel={AUC with Error Bars},
%             ymin=0.90, ymax=1.00,
%             ytick={0.90,0.92,0.94,0.96,0.98,1.00},
%             legend pos=south east,
%         ]
%         \addplot[
%             color=blue,
%             mark=square,
%             error bars/.cd,
%             y dir=both,
%             y explicit
%         ]
%         coordinates {
%             (1,0.959) +- (0,0.009) 
%             (2,0.971) +- (0,0.014)
%             (3,0.973) +- (0,0.012)
%             (4,0.974) +- (0,0.013)
%             (5,0.9745) +- (0,0.009)
%             (6,0.976) +- (0,0.006)
%         };
%         \end{axis}
%     \end{tikzpicture}
%     \caption{Performance Variation on the GCI Dataset with Varying Numbers of Safe/Unsafe Reference Pairs.}
%     \label{fig:gci_auc}
% \end{figure}

\section{Effects of Our Method on Different LLM Base Models}
\label{abtest_base_model}
We evaluate the effects of our method on different base LLM models using the XSTest dataset, which includes the Llama-2-7B-Chat \citep{inan2023llama}, Qwen-2.5-7B-Instruct \citep{yang2024qwen2}, and Gemma-7B-It models \citep{team2024gemma}. We assess the task performance improvement of our method by using the base models as baselines, serving as guardrail models.

Following \citet{xie2024gradsafe}, we use the same prompt format in our baseline models for evaluation: \textbf{``You are a helpful assistant. Provide your safety assessment for this user prompt: \{\{input prompt\}\}  You must only answer with `safe' or `unsafe'.''}. The evaluation metric employed is the F1 Score. We do not use the AUPRC metric because the involved prompt engineering-based method outputs only binary labels (`safe' or `unsafe'), without providing a specific score for threshold selection.


As illustrated in Figure \ref{fig:diff_model}, the baseline models employing LLMs as guardrails exhibit moderate performance in detecting unsafe prompts, achieving F1 scores ranging from 61.5\% to 76.0\%.  However, the application of our proposed method to the same base models yields significant performance improvements, substantially outperforming the prompt engineering baselines.  Specifically, we observe F1 score increases ranging from 11\% to 33\% across different base models, demonstrating its generalizability to various base models.

% 从图Figure~\ref{fig:diff_model}中可以发现, 只用LLM as guradrail 模型效果比较一般, unsafe prompt detection的F1 Score是从61.5\%到76.0\%. 但是利用我们

% Figure~\ref{fig:diff_model} shows that our method outperforms the prompt engineering baselines, with F1 score improvements ranging from 11\% to 33\% across different base models, demonstrating its generalizability to various base models.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\linewidth,height=0.7\linewidth]{imgs/diff_model2.png}
    \caption{The effects of our method across different base models. The test dataset is XSTest.}
    \label{fig:diff_model}
\end{figure}

\section{Effects of Our Method on Different Sizes of LLMs}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\linewidth]{imgs/scaling2.png}
    \caption{The performance of our method on models of different size scales. The selected models are from the Qwen-2.5-Instruct series, ranging from 0.5B to 14B, and the test dataset is XSTest.}
    \label{fig:scaling}
\end{figure}

To validate whether our proposed method is effective across different sizes of LLMs, we conduct experiments on various model sizes of the Qwen-2.5-Instruct series \citep{yang2024qwen2}, including 0.5B, 1.5B, 3B, and 7B for the prompt safety detection task.

Additionally, to quantify the improvement our method brings to the task, we use the LLMs themselves as the guardrail model, serving as the baselines.

As the results shown in Figure \ref{fig:scaling}, our method outperforms the baseline approach across all model sizes. As the model size increases, the improvement becomes more pronounced, with the performance increase reaching 11.5\% at 7B and 14.3\% at 14B, respectively. 

Additionally, it is worth noting that the effect is not significant on LLMs of sizes 0.5B, 1.5B, and 3B. This may indicate that smaller base models lack capacity to distinguish the gradient pattern between `safe' and `unsafe' prompts.



\section{Effects of Different Compliance Responses}
% 为了验证不同的Compliance Responses 是否会影响我们提出的方法效果, We examine the impact of different compliance responses on performance of our method on the unsafe prompt detection task using the XStest dataset. These Compliance Responses are created by 给定初始的种子Compliance Response `Sure', 之后我们放到GPT-4当中利用指令生成了另外10个Compliance Responses, 之后我们随机抽取了四个作为我们用来评测的Compliance Responses.
To evaluate the influence of varying compliance responses on the efficacy of our proposed methodology, we conducted an examination of its performance on the unsafe prompt detection task. This evaluation utilized the XSTest dataset. 

Initially, a seed compliance response `Sure' was provided. Subsequently, leveraging the capabilities of GPT-4\footnote{https://openai.com/index/gpt-4/}, we employed instructional prompts to generate ten additional compliance responses. From this set, we randomly selected six responses for the purpose of our comparative assessment.

{\linespread{1.2}
\begin{table}[ht]\small
  \centering
  \resizebox{0.5\textwidth}{!}{ % 根据页面宽度缩放表格
  \begin{tabular}{lc}
  \hline
   \textbf{Compliance Responses} & \textbf{AUPRC} \\
    \hline
   `Sure' & 0.955  \\
    `Yes' & 0.950  \\
    `Sure, I would be delighted to assist you.'  &  0.940  \\
    `Of course, I will help you.' & 0.960 \\ 
    `Absolutely, I'm happy to help.' & 0.955 \\
    `Yes, I'm happy to provide assistance.' & 0.956 \\
  \hline
  \end{tabular}
  }
  \caption{The performance of our method on different compliance responses for unsafe prompt detection. The test dataset is XSTest.}
  \label{diff_response}
\end{table}

As demonstrated in Table \ref{diff_response}, our method exhibits consistent performance across various compliance responses, achieving a mean AUPRC of 0.952 with a standard deviation of 0.0069. These results further validate the robustness of our approach. 

Furthermore, the AUPRC score is marginally higher when the response is 'Of course, I will help you' compared to the response 'Sure' used in the paper. This suggests that optimizing compliance response selection to construct a more effective safe/unsafe gradient reference may further enhance performance. It is also worth noting that although there is a obvious differences in the length of our sampled compliance responses, the corresponding AUPRC scores do not exhibit significant differences.


\section{Conclusion}
In this paper, we propose a novel unsafe prompt detection method \textbf{GradCoo}  based on gradient co-occurrence analysis, which requires only a small set of safe and unsafe reference prompts. By mitigating gradient direction bias,
GradCoo accommodates unsigned similarity, therefore analyzing the overall patterns of the gradients more accurately, leading to significant performance enhancement in unsafe prompt detection.  Comprehensive experiments on the widely-used benchmark datasets ToxicChat and XSTest demonstrate that our method can achieve state-of-the-art (SOTA) performance in unsafe prompt detection task compared to existing methods. Moreover, we confirm the generalizability of our method in detecting unsafe prompts across of a diverse range of LLMs baseline with various sizes and origins in comprehensive experiments. 

Building upon the promising results achieved by GradCoo, future work will explore several avenues.  Initially, we will focus on expanding the scope of GradCoo to encompass more complex forms of unsafe content. This includes investigating multimodal prompts, such as combinations of images and text, as well as prompts designed to elicit harmful model behaviors beyond simple text generation, such as code execution or agent actions.  

Furthermore, a crucial direction for future work involves developing a deeper theoretical understanding of the correlation between gradient co-occurrence patterns and the characteristics of unsafe prompts. This will entail analyzing the mathematical properties of the gradient space and investigating how various types of unsafe content are manifested within it. Concurrently, we aim to develop explainability techniques for GradCoo, providing insights into the rationale behind a prompt being flagged as unsafe. This explainability will be valuable for both model developers and end-users, potentially informing the development of more robust defense mechanisms against unsafe prompts.


\section*{Limitations}
While our method demonstrates significant performance improvements in both unsafe prompt detection, several limitations remain. First, our method relies on gradient computation, which introduces additional computational overhead compared to a single forward pass in guardrail models, particularly for extremely large models. Second, although we have conducted experiments with other LLMs, the effects of our method on multimodal models or broader tasks require further investigations, as the unsafe gradient patterns may not exhibit similar characteristics in such settings. Third, while our empirical results demonstrate the effectiveness of GradCoo, a deeper theoretical understanding of the correlation between these safe and unsafe gradient patterns is needed.


\section*{Ethical Impact}
The aim of this paper is to explore novel methods to safeguard large language models against potential harms arising from unsafe prompts. We employ existing unsafe prompt detection benchmark datasets in the experiment, thereby mitigating the introduction of new safety risks. We believe that our approach promotes advancements in unsafe prompt detection methods, thereby benefiting the AI safety community.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}
\end{document}
