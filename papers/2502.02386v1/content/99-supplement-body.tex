\section{Parameter estimation in synthetic HCM hypergraphs.}
For consistency, we applied the same convergence criterion across all experiments in the paper. Specifically, every 100 steps, we calculated the difference between the current and previous $\eta$ values. If the percentage change was less than $1\%$, the loop was terminated. As examples, in \Cref{fig:accuracy} we show the errors in the parameter estimates as obtained after different numbers of steps of the algorithm for two synthetic hypergraphs.


\begin{figure*}[h!]
    \centering
    \includegraphics[scale=0.55]{fig/FigS1.pdf}
    \caption{Estimation of parameters in two synthetic hypergraphs with 10k hyperedges %(random seed set to be $0$) 
    each. The convergence criterion, tested every 100 steps, was reached at 1400 steps for each of them. 
    We show the absolute difference $\abs{\hat{\eta} - \eta}$ between the estimate and true value of $\eta$. 
    We also show the Kullback--Leibler divergence of the estimates of $\vbeta$ and $\vgamma$ from the true values.    
    The $\beta$ and $\gamma$ here are generated with a random Poisson distribution, truncated at value $10$, with means $\mu_{\vbeta}$ and $\mu_{\vgamma}$ indicated in the plot legends. 
    Note the logarithmic scale of the vertical axis.}
    \label{fig:accuracy}
\end{figure*}


\begin{table*}[h!]
    \centering
     \input{fig/parameter-table}
    \caption{Details of parameters retrieved through stochastic expectation maximization with batch size 30 for each hypergraph. We set both the length of $\beta$ and $\gamma$ to be equivalent to the largest edge size in the corresponding real-world hypergraph.}
    \label{tab:parameter-values}
\end{table*}
    


\section{Reconstruction of Hypergraph Properties from Generative Models}

In \autoref{fig:Mean_sizes}, we compare the mean edge size and mean degree between hypergraphs generated from estimated parameters (listed in \autoref{tab:parameter-values}) and the original data, for a variety of real-world hypergraphs.
\begin{figure*}[h!]
    \centering
    \includegraphics[scale=0.55]{fig/MEANS_scatter_log.pdf}
    \caption{Mean edge sizes and mean node degrees compared between fit \model instances and empirical data.
    Note the log-log scale.}
    \label{fig:Mean_sizes}
\end{figure*}


We perform a similar analysis for synthetic datasets, comparing different hypergraph properties, including face edit simplicity, which is a more localized concept indicating the number of subfaces needed to be added to the hypergraph to render a specific face a simplex.  
We observe minimal differences in degree assortativity and edit simplicity between the \model and ER hypergraphs in \autoref{fig:four-properties-supp}. However, discrepancies arise in the clustering coefficient and face edit simplicity. Particularly, we see the face edit simpliciality for ER hypergraphs dropped persistently over time, which is very different behavior from the \model (except in the case when $\eta=0.7$ and $\mu_{\novelnodedist}=1.3$, and even in that case the \model face edit simpliciality is orders of magnitude larger than for the corresponding ER hypergraphs). As was shown in \cite{landrySimplicialityHigherorderNetworks2024}, face edit simpliciality is usually higher than $10^{-2}$ for real-world hypergraphs, and thus the real-world replication of the ER model is much worse than that of our \model, which keeps steadily above $10^{-2}$ for all cases studied. 

For the clustering coefficient, interestingly, we see in \autoref{fig:four-properties-supp} that the values from the ER and \model are almost the same when $\mu_{\novelnodedist}$ is small. However, when $\mu_{\novelnodedist} = 1.3$, the clustering coefficient quickly drops for the ER hypergraphs, but stays steady for the HCM model, which has values close to the real world hypergraph clustering coefficients described in \cite{gallagherClusteringCoefficientsProtein2013}. These properties of our \model with various parameter sets demonstrate the ability of the model to generate realistic-seeming hypergraphs, despite its small number of parameters. 

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=1\linewidth]{fig/FigM2_four_property_plot.pdf} 
    \caption{Hypergraph properties including top-2 degree assortativity, clustering coefficient, edit simpliciality, and face edit simpliciality from different generative models. Solid lines show values for data generated by the \model. Dashed lines are for Erd\H{o}s-R\'enyi hypergraphs generated with the same expected edge-size. The horizontal axis ranges from $t=0$ to $10$k, showing how the properties change with the numbers of steps in the growing hypergraphs.}
    \label{fig:four-properties-supp}
\end{figure*}

In \Cref{fig:additionalM4}, we repeat the experiment shown in  \Cref{fig:four-properties} for \data{email-enron} on other datasets, to demonstrate different exhibited behaviors across these other datasets, computing the assortativity, clustering coefficient, edit simpliciality, and intersection sizes for each dataset and the corresponding \model, ER and PA models.

\begin{figure*}[!ht]
\centering
   \begin{subfigure}[b]{1\textwidth}
   \includegraphics[width=1\textwidth]{fig/FigM4_email-eu.pdf}
   \caption{\texttt{email-eu}}
   \label{fig:additionalM4a} 
\end{subfigure}
\\
\begin{subfigure}[b]{1\textwidth}
   \includegraphics[width=1\textwidth]{fig/FigM4_ndc-classes.pdf}
   \caption{\texttt{ndc-classes}}
   \label{fig:additionalM4b} 
\end{subfigure}
\\
\begin{subfigure}[b]{1\textwidth}
   \includegraphics[width=1\textwidth]{fig/FigM4_ndc-substances.pdf}
   \caption{\texttt{ndc-substances}}
   \label{fig:additionalM4c} 
\end{subfigure}
\caption{Structural properties of \data{email-eu}, \data{ndc-classes}, and \data{ndc-substances}. We show that these datasets have exhibited behaviors distinct from those of \data{email-enron}, shown in \Cref{fig:four-properties} in the main text, highlighting that assortativity is highly dependent on the underlying structure of the real-world hypergraph. We also note that HCM is the only model that demonstrates non-diminishing edge intersection sizes across the different datasets.}
    \label{fig:additionalM4}
\end{figure*}




\section{Asymptotic Properties}



\subsection{Power-law degree distribution} \label{sec:power-law-tails}


We now give a heuristic argument that the degree distribution of our model has an asymptotic power-law tail, with an exponent that depends on the model parameters. 
Our argument is based on rate equations, generalizing an argument by Mitzenmacher \cite{mitzenmacherBriefHistoryGenerative2004} in the context of preferential attachment dyadic graphs. 

Fix $d > 1$. 
Let $\at{p}{t}_d$ be the proportion of nodes which have degree $d$ at time $t$. 
The total number of such nodes is $\at{n}{t} \at{p}{t}_d$. 
In timestep $t+1$, the change in the number of these nodes is $\at{\Delta}{t+1}_d = \at{n}{t+1} \at{p}{t+1}_d - \at{n}{t} \at{p}{t}_d$. 
We now estimate $\at{\delta}{t+1}_d = \mathbb{E}\brackets{\at{\Delta}{t+1}_d}$ in expectation. 
The probability of an individual node being selected in the edge-sampling step is proportional to its degree $d$ in the current hypergraph $\at{\hypergraph}{t}$, since there are $d$ edges which could be selected which contain that node. 
Normalizing, the probability that a given node selected in the edge-sampling step has degree $d$ is $\frac{d}{\mean{\at{d}{t}}}\at{p}{t}_d$, where $\mean{\at{d}{t}}$ is the mean degree at time $t$. 
When a node of degree $d$ is selected in the edge-sampling step it becomes a node of degree $d+1$, while when a node of degree $d-1$ is selected it becomes a node of degree $d$. 
The expected number of nodes so selected is $1 + \eta\paren{\mean{\at{k}{t}}-1}$, where $\mean{\at{k}{t}}$ is the mean edge size at time $t$. 
Thus, the expected number of degree $d$ nodes created through the edge-sampling step is $\frac{1 + \eta\paren{\mean{\at{k}{t}}-1}}{\mean{\at{d}{t}}}\brackets{\paren{d-1}\at{p}{t}_{d-1} - d\at{p}{t}_d}$. 
Through similar reasoning, the expected number of degree $d$ nodes created through the process of extant node addition is $\mu_{\extantnodedist}\brackets{\at{p}{t}_{d-1} - \at{p}{t}_d}$.
Since we have assumed $d > 1$ but novel node addition can only create nodes of degree $1$, only these two processes contribute. 
Our expected rate equation is then 
\begin{align}
    \at{n}{t+1} \at{p}{t+1}_d - \at{n}{t} \at{p}{t}_d &=  \frac{1 + \eta\paren{\mean{\at{k}{t}}-1}}{\mean{\at{d}{t}}}\brackets{\paren{d-1}\at{p}{t}_{d-1} - d\at{p}{t}_d}  + \mu_{\extantnodedist}\brackets{\at{p}{t}_{d-1} - \at{p}{t}_d}\;. \label{eq:rate-equation}
\end{align}

We now assume stationarity of the degree distribution and the edge size sequence. 
At stationarity, we must have $\at{p}{t+1}_d = \at{p}{t}_d = p_d$ for some constant $p_d$, as well as $\mean{\at{k}{t}} = \mean{k}$ and $\mean{\at{d}{t}} = \mean{d}$ for constants $\mean{k}$ and $\mean{d}$. 
We also have that $\mean{d} = \frac{m}{n}\mean{k} = \frac{\mean{k}}{\mu_{\novelnodedist}}$, where we have used the fact that $\mu_{\novelnodedist}$ nodes are added per edge in each timestep. 
Finally, in expectation, $\at{n}{t+1} - \at{n}{t} = \mu_{\novelnodedist}$.
At stationarity and in expectation, our approximate compartmental equation now reads 
\begin{align}
    \mu_{\novelnodedist} p_d = a\brackets{\paren{d-1}p_{d-1} - dp_d} + \mu_{\extantnodedist}\brackets{p_{d-1} - p_d}\;, \label{eq:stationary-rate-equation}
\end{align}
where we have defined $a = \frac{1 + \eta\paren{\mean{k}-1}}{\mean{d}}$. 
Rearrangement yields 
\begin{align}
    \frac{p_d}{p_{d-1}} &= \frac{a(d-1) + \mu_{\extantnodedist}}{ad + \mu_{\extantnodedist} + \mu_{\novelnodedist}} \\ 
    &= 1 - \frac{a + \mu_{\novelnodedist}}{ad + \mu_{\extantnodedist} + \mu_{\novelnodedist}}\;.
\end{align}
Since we are interested in the tails of the degree distribution, we allow $d$ to grow large, yielding 
\begin{align}
    \frac{p_d}{p_{d-1}} &\approx 1 - \frac{a + \mu_{\novelnodedist}}{a}\frac{1}{d} \\ 
    &\approx \paren{\frac{d-1}{d}}^{1 + \frac{\mu_{\novelnodedist}}{a}}\;. 
\end{align}
Unfolding the recurrence yields, for large $d$, a power-law tail, $p_d\sim d^{-\zeta}$, with exponent $\zeta = 1 + \frac{\mu_{\novelnodedist}}{a}$.
Inserting explicit formulae for $a$, $\mean{k}$ (\cref{eq:mean-edge-size}) and $\mean{d}$ (\cref{eq:mean-degree}), 
we can express this exponent explicitly in terms of the model parameters using the formulas for $a$, $\mean{k}$, and $\mean{d}$, obtaining 
\begin{align}
    \zeta = 1 + \frac{1 - \eta + \mu_{\extantnodedist} + \mu_{\novelnodedist}}{1 - \eta(1 - \mu_{\extantnodedist} - \mu_{\novelnodedist})}\;. \label{eq:power-law-exponent}
\end{align}


\subsection{Edge-size distribution} \label{sec:edge-sizes}

We describe the entries of the matrix $\mathbf{W}$ described in the main text. 
Each entry $w_{ij}$ of $\mathbf{W}$ represents the conditional probability 
\begin{align}
    w_{ij} = \prob{\abs{e} = i \given \abs{f} = j}\;,
\end{align}
where $f$ is the edge sampled in the edge-selection step of \Cref{alg:model} and $e$ is the final edge formed.  
We explicitly construct these probabilities conditional on the events that $\ell+1$ nodes are selected in the edge-sampling step and that $h$ nodes are selected in the extant node addition step. 
With these conditional probabilities, the law of total probability then yields 
\begin{align}
    w_{ij} = \prob{\abs{e} = i \given \abs{f} = j} 
           = \sum_{\ell = 0}^{j-1} \sum_{h = 0}^{i - \ell} \binom{j-1}{\ell} \eta^{\ell} (1-\eta)^{j - \ell - 1} \extantnodepar_h \novelnodepar_{i - \ell - h}\;.
\end{align}
The Perron eigenvector of this matrix gives the stationary distribution of edge sizes in the model. 
In practice, it is necessary to select a finite size for the matrix $\mathbf{W}$, which amounts to artificially setting $w_{ij} = 0$ for $i$ and $j$ sufficiently large. 


% \begin{figure*}[!ht]
%     \centering \includegraphics[width=1\linewidth]{fig/ndc-substances_summarize_color.pdf}
%     \caption{The average edge intersection size over time, comparing the \emph{ndc-substances} hypergraph, our model as fit to this data, the corresponding Erd\H{o}s-R\'enyi hypergraph, and the corresponding Preferential Attachment hypergraph. Notably, larger intersection sizes exist, but for readability, we are only showing up to $k = 7$ here.}
%     \label{fig:intersection}
% \end{figure*}


\subsection{Asymptotic properties of intersection sizes} \label{sec:intersection-sizes}

We now develop a probabilistic argument supporting the following claim: 
\begin{clm*}
    Let $r_{ijk}$ be the expected proportion of pairs of the $m$ edges, relative to the total number of pairs $\binom{m}{2}$, which have sizes $i$ and $j$ with intersection size $k$,  with the expectation taken with respect to our proposed model and with the edge of size $j$ appearing before the edge of size $i$. 
    %\pjm{A couple paragraphs down from here, we additionally require the first two indices are the succeeding and preceding edges of the pair. But this doesn't change the expected proportion, does it? Or does it confuse the $i=j$ cases?}
    Then, there exist constants $q_{ijk} \geq 0$ such that: 
    \begin{align}
        r_{ijk} = \begin{cases}
            q_{ijk} + o(1) &\quad k = 0 \\ 
            m^{-1}q_{ijk} + O(m^{-2}) &\quad k \geq 1\;. 
        \end{cases}
    \end{align}
    Furthermore, the scalars $q_{ijk}$ can be approximated as the solution of an eigenvector problem $\mathbf{q} = \mathbf{C}\mathbf{q}$, where $\mathbf{q}$ collects the scalars $q_{ijk}$ in flattened form and the matrix $\mathbf{C}$ is a function of the model parameters $\alpha$, $\vbeta$, and $\vgamma$.
\end{clm*}
%Several assumptions and instances of casual reasoning are present in the argument. 
%My suspicion is that a fully formal proof is probably more trouble than it's worth. 
%That said, there are also likely typos and mistakes! 
%I'll feel confident in the result when the argument does not contain obvious errors and when we can test the result experimentally. 


Throughout this section, we fix the timestep $t$. 
We use the shorthand $z \eqdef \at{z}{t}$ and $z' \eqdef \at{z}{t+1}$ for any time-dependent quantities $z$. 
We also say that $f(m) \doteq g(m)$ if $\lim_{m\rightarrow \infty} \frac{f(m)}{g(m)} = 1$, where $m$ is the number of edges in $\cH$. 
We write $g \prec e$ if $e$ was added to $\cH$ after $g$. 


Let $R_{ijk} \eqdef \prob[t]{\abs{e} = i, \abs{g} = j, \abs{e\cap g} = k \given g \prec e}$, where 
the probability is computed with respect to the empirical distribution of $\at{\cE}{t}$. 
The quantity $R_{ijk}$ corresponds to a sampling process in which we uniformly select a pair of edges; arrange them in descending order according to the timestep in which they were sampled; label the first (later) edge $e$ and the second (earlier) edge $g$; and then check whether $\abs{e} = i$, $\abs{g} = j$, and $\abs{e\cap g} = k$.
We will study the limiting behavior of $r_{ijk} \eqdef \E{R_{ijk}} $ as $t \rightarrow \infty$. 
Let $m = \at{m}{t}$ and assume that one edge is added every timestep. 
Let $P_{ijk} \eqdef \binom{m}{2} R_{ijk}$ be the corresponding number of ordered pairs of edges of sizes $\abs{e} = i$ and $\abs{g} = j$ with intersection size $k$, ordered so that $g \prec e$.
Let $p_{ijk} = \E{P_{ijk}}$.

We first write down a compartmental update for $P_{ijk}$ when a single edge $e$ is added. 
Assume that $e$ is sampled from edge $f$ in the edge-sampling step. 
Let $Z_{eg, ijk}$ be the indicator of the event that the new edge $e$ has size $i$, a previously existing edge $g$ has size $j$, and the intersection size of $e$ and $g$ has size $k$. 
Then, the compartmental update for $P_{ijk}$ reads
\begin{align}
    P'_{ijk} = P_{ijk} + \sum_{g \in \cE} Z_{eg, ijk}\;. 
\end{align}
It is useful to condition on whether $g = f$, the edge that was sampled in generating $e$: 
\begin{align}
    P'_{ijk} = P_{ijk} + Z_{ef, ijk} + \sum_{g \in \cE\setminus f} Z_{eg, ijk}\;. 
\end{align}
Computing expectations gives 
\begin{align}
    p'_{ijk} = p_{ijk} + z_{ef, ijk} + \sum_{g \in \cE\setminus f} z_{eg, ijk}\;, 
\end{align}
where we have defined $z_{eg, ijk} = \E{Z_{eg, ijk}}$.
Since $z_{eg, ijk}$ only depends on edge $g$ through its size $j$ whenever $g\neq f$, let us write $y_{ijk} \eqdef z_{eg, ijk}$. 
Similarly, we will use the simplifying notation $z_{ijk} \eqdef z_{ef,ijk}$. 
Our expected compartmental update becomes 
\begin{align}
    p'_{ijk} = p_{ijk} + z_{ijk} + (m-1) y_{ijk}\;. \label{eq:compartmental-1}
\end{align}
We aim to close \cref{eq:compartmental-1} approximately by expressing $z_{ijk}$ and $y_{ijk}$ in terms of $r_{ijk}$. 

Let us first consider $z_{ijk}$. 
The probability that an edge $f$ of size $j$ is selected uniformly at random for sampling can be written %, in the long-time limit, 
in terms of $r_{ijk}$ by total probability, summing appropriately over the possible sizes of some other edge $\tilde{f}\neq f$ and conditioning on whether $\tilde{f}$ appears before or after $f$, noting that $\prob{f \prec \tilde{f}} = \prob{\tilde{f} \prec f} = \frac{1}{2}$ in the absence of any information about the time step corresponding to edge $f$, giving
\begin{align}
    r_j &\eqdef \prob{\abs{f}=j} \\
    &= \sum_{\ell h} \prob{\abs{\tilde{f}} = \ell, \abs{f} = j, \abs{f\cap \tilde{f}} = h \given f \prec \tilde{f}} \prob{f \prec \tilde{f}} \\
    &\quad+ \sum_{\ell h}\prob{\abs{f} = j, \abs{\tilde{f}} = \ell, \abs{f\cap \tilde{f}} = h \given \tilde{f} \prec f} \prob{\tilde{f} \prec f} \\
    &= \frac{1}{2}\sum_{\ell h} \paren{r_{\ell jh } + r_{j \ell h }}\;.
\end{align}

Given $\abs{f}=j$, the probability that the newly-formed edge $e$ has size $i$ and that its intersection with $f$ has size $k$ is then 
\begin{align}
    b_{ik|j} &\eqdef \prob{\abs{e} = i, \abs{e \cap f} = k \given \abs{f} = j}  \\ 
    &= \prob{\abs{e \cap f} = k \given \abs{f} = j} \prob{\abs{e} = i \given  \abs{e \cap f} = k,  \abs{f} = j} \\ 
    &= \prob{\abs{e \cap f} = k \given \abs{f} = j} \prob{\abs{e} = i \given  \abs{e \cap f} = k}\\ %\tag{$\abs{e} \perp \abs{f} | \abs{e\cap f}$.}\\ 
    &= \binom{j-1}{k-1}\eta^{k-1}(1-\eta)^{j-k} \sum_{x = 0}^{i-k} \beta_x \gamma_{i - k - x}\;, \label{eq:bikj}
\end{align}
where the third line follows from the second because, for large graphs, the role of the sampled edge $f$ in determining the properties of the new edge $e$ is fully captured by their intersection.
Importantly, this expression does not depend on $m$ in the long-time limit, since $m$ is large enough so that the number of nodes $n$ in the hypergraph is at least $j - k$. We also note that we assume $\beta_0<1$. 
We therefore have 
%\begin{align}
%    x_{ijk} &= b_{ik|j} r_j
%            = \frac{1}{2} b_{ik|j} \sum_{\ell, h: h \leq \ell} \paren{r_{\ell jh } + r_{j \ell h }}\;. 
%\end{align}
\begin{align}
    z_{ijk} &= b_{ik|j} r_j
            = \frac{1}{2} b_{ik|j} \sum_{\ell h} \paren{r_{\ell jh } + r_{j \ell h }}\;.  \label{eq:zijk}
\end{align}


We now study $y_{ijk}$. 
Let us condition on $\abs{f} = \ell$, $\abs{f \cap g} = h$, and the relative temporal order $f \prec g$ v.\ $g \prec f$.
Noting again that $\prob{f \prec g} = \prob{g \prec f} = \frac{1}{2}$, we have
\begin{align}
    y_{ijk} &\eqdef \prob{\abs{e} = i, \abs{g} = j, \abs{e \cap g} = k}  \\
            &= \sum_{\ell h} \prob{g \prec f} \underbrace{\prob{\abs{f} = \ell, \abs{g} = j, \abs{f \cap g} = h\given g \prec f}}_{=r_{\ell j h}} \underbrace{\prob{\abs{e} = i,  \abs{e \cap g} = k \given\abs{f} = \ell, \abs{g} = j,  \abs{f \cap g} = h, g \prec f}}_{\eqdef a^\succ_{ik|\ell j h}}  \\ 
            &\quad+ \sum_{\ell h} \prob{f \prec g} \underbrace{\prob{\abs{g} = j, \abs{f} = \ell,  \abs{f \cap g} = h\given f \prec g}}_{=r_{ j \ell h}} \underbrace{\prob{\abs{e} = i,  \abs{e \cap g} = k \given \abs{g} = j,  \abs{f} = \ell,  \abs{f \cap g} = h, f \prec g}}_{\eqdef a^\prec_{ik|j\ell h}} \\ 
            &= \frac{1}{2}\sum_{\ell h} \paren{r_{\ell j h} a^\succ_{ik|\ell jh} + r_{j \ell h} a^\prec_{ik|j\ell h}}\;,
\end{align}
where the superscript $\{\succ,\prec\}$ on the $a$ coefficients indicates whether $e$ originates as a copy of the succeeding or preceding edge in the pair, respectively. In our calculation of these $a$ coefficients below, they will be equivalent at the level of the present approximation. However, it will be important to note that, unlike $b_{ik|j}$, these $a$ coefficients depend on $m$. 
For notational compactness, let $I^\succ_{\ell jh}$ be the event $\braces{\abs{f} = \ell, \abs{g} = j,  \abs{f \cap g} = h, g \prec f}$ and $I^\prec_{j\ell h}$ be the event $\braces{\abs{g} = j,  \abs{f} = \ell,  \abs{f \cap g} = h, f \prec g}$, again denoting whether the distinguished edge $f$ to be copied is the succeeding or preceding edge in the pair (and the index on the event $I$ continuing our convention of listing first the size of the succeeding edge, then the size of the preceding edge, and lastly the size of their intersection). 
%Unlike $b_{ik|j}$, $a_{ik|j\ell h}$ these $a$ coefficients depend on $m$. 
%For notational compactness, let $I_{\ell jh}$ be the event  $\braces{\abs{g} = j, \abs{f} = \ell, \abs{f \cap g} = h, f \prec g}$. 
%Then, 
%\begin{align}
%    a_{ik|\ell j h} \eqdef \prob{\abs{e} = i, \abs{e \cap g} = k \given I_{\ell j h}}\;.  
%\end{align}

Under this notation, we have
\begin{equation}
    a^*_{ik|\ell j h} \eqdef \prob{\abs{e} = i, \abs{e \cap g} = k \given I^*_{\ell j h}}
\end{equation}
where the $*$ is either $\succ$ or $\prec$ to distinguish the two cases.
Let $S(e)$ be the number of nodes in $e$ formed by the edge-sampling step.
Then, $\abs{e \cap f} = S(e)$. 
We expand $a^*_{ik|\ell j h}$ by conditioning on $s = S(e)$, writing 
\begin{align}
    a^*_{ik|\ell j h} &\eqdef \prob{\abs{e} = i, \abs{e\cap g} = k \given I^*_{\ell j h}} 
    = \sum_{s} \prob{S(e) = s \given I^*_{\ell j h}} \prob{\abs{e} = i, \abs{e\cap g} = k \given S(e) = s , I^*_{\ell j h}}  \nonumber\\ 
    &= \sum_{s} \prob{S(e) = s \given I^*_{\ell j h}} \prob{\abs{e\cap g} = k \given S(e) = s , I^*_{\ell j h}} \prob{\abs{e} = i \given  S(e) = s ,  \abs{e\cap g} = k , I^*_{\ell j h}}  \;, \label{eq:a-star-1}
\end{align}
where we take note to observe that the sum over possible values of $s=S(e)$ here ranges from $0$ to $\min(|e|,|g|)$, where $|g|=j$ when calculating $a^\succ_{ik|\ell j h}$ and $|g|=\ell$ when calculating $a^\prec_{ik|\ell j h}$.

Introducing additional notation,  
%If edge $e$ is formed through an HCM step, 
let $S(e,g)$ be the number of nodes in $e$ \emph{formed by the edge-sampling step} that are also elements of edge $g$. 
Under our model definition, this is equivalent to  $S(e,g)= \abs{e \cap f \cap g}$. 
Similarly, let $X(e, g)$ be the number of nodes in $e$ \emph{formed by the extant node addition step} that are also elements of edge $g$.
Abusing notation, we also let $X(e)$ denote the total number of nodes in $e$ formed through the extant node addition step, regardless of whether they intersect with any other edges. 
Then, for any edge $g \prec e$, we have $\abs{e \cap g} = S(e,g) + X(e,g)$.
We note that under the HCM step process, $S(e,g)$ is independent of $X(e,g)$ and $S(e)$ is independent of $X(e)$.



We can now further condition our expression for $a^*_{ik|\ell j h}$ in \cref{eq:a-star-1} on $X(e)$, writing 
\begin{align}
    a^*_{ik|\ell j h} 
    &= \sum_{s} \prob{S(e) = s \given I^*_{\ell j h}} \times\nonumber\\
    &\qquad\sum_{x} \prob{X(e) = x}  \prob{\abs{e\cap g} = k \given S(e) = s , X(e) = x, I^*_{\ell j h}} \prob{\abs{e} = i \given  S(e) = s ,  \abs{e\cap g} = k , X(e) = x, I^*_{\ell j h}} \\ 
    &= \sum_{s} \prob{S(e) = s \given I^*_{\ell j h}} \times\nonumber\\
    &\qquad\sum_{x} \gamma_x  \prob{\abs{e\cap g} = k \given S(e) = s , X(e) = x, I^*_{\ell j h}} \prob{\abs{e} = i \given  \abs{e\cap g} = k, S(e) = s ,  X(e) = x, I^*_{\ell j h}} \\ 
    &= \sum_{s} \underbrace{\prob{S(e) = s \given I^*_{\ell j h}}}_{t^{(1)}_{s|\ell jh}} \sum_{x} \gamma_x  \underbrace{\prob{\abs{e\cap g} = k \given S(e) = s , X(e) = x, I^*_{\ell j h}}}_{t^{(2)}_{k|sx\ell jh}} \underbrace{\prob{\abs{e} = i \given S(e) = s, X(e) = x}}_{t^{(3)}_{i|sx}}\;. \label{eq:a}
\end{align}
In the second line we have used the definition of $\vgamma$. In the third line we have used the fact that, conditional on $S(e)$ and $X(e)$, the size of $e$ is independent of the sizes of $f$, $g$, $f\cap g$, and $e\cap g$, because $S(e)$ and $X(e)$ specify the size of $e$ except for the novel nodes, which cannot intersect any other edges. 
We have also named the resulting terms, which we now proceed to compute. 

There are two relatively simple terms. First,
\begin{align}
    t^{(1)}_{s|\ell j h} = \prob{S(e) = s \given I^*_{\ell j h}} = \binom{\ell-1}{s-1}\eta^{s-1}(1-\eta)^{\ell-s}\;, \label{eq:t1}
\end{align}
since this is simply the probability of selecting a total of $s$ nodes from $f$ (which has size $\ell$) to form $e$ during the edge-sampling step.
Next, the term $t^{(3)}_{i|sx}$ is simply the probability of sampling $i - s - x$ from the novel node distribution: 
\begin{align}
    t^{(3)}_{i|sx} = \beta_{i-s-x}\;. \label{eq:t3}
\end{align}

The more complicated term is $t^{(2)}_{k|sx\ell jh}$. 
We condition on the value of $S(e,g)$:
\begin{align}
    t^{(2)}_{k|sx\ell jh} &\eqdef \prob{\abs{e\cap g} = k \given S(e) = s, X(e) = x, I^*_{\ell j h}} \\ 
    &= \sum_{\sigma} \prob{\abs{e\cap g} = k \given S(e) = s, X(e) = x, S(e,g) = \sigma, I^*_{\ell j h}} \prob{S(e,g) = \sigma \given S(e) = s, X(e) = x, I^*_{\ell j h}} \\ 
    &= \sum_{\sigma} \underbrace{\prob{\abs{e\cap g} = k \given  X(e) = x, S(e,g) = \sigma, I^*_{\ell j h}}}_{t^{(4)}_{k|x\sigma \ell jh}} \underbrace{\prob{S(e,g) = \sigma \given S(e) = s,  I^*_{\ell j h}}}_{t^{(5)}_{\sigma | s \ell jh}} \;.
\end{align}
In the third line we have used two simplifications: first, $e\cap g$ depends on $e\cap f$ only through $e \cap f \cap g$, which is described by $S(e, g)$. 
Second, $S(e,g)$ is independent of $X(e)$, since $X(e)$ specifies the nodes in $e$ that are not in $e\cap f$.
We have also again named several terms which we will analyze further. 
First, $t^{(4)}_{k|x\sigma \ell jh}$ is the probability that $k - \sigma$ extant nodes are added to $e$ that are also added to $g$. 
There are $x$ total extant nodes added, $j - \sigma$ candidate extant nodes in $g$, and $n - \ell$ total candidate extant nodes. 
This probability is hypergeometric: 
\begin{align}
    t^{(4)}_{k|x\sigma \ell jh} &\eqdef \prob{\abs{e\cap g} = k \given  X(e) = x, S(e,g) = \sigma, I^*_{\ell j h}}  \\ 
    &= \mathrm{HyperGeometric}(k - \sigma; x, j - \sigma, n - \ell)  \\ 
    &= \frac{\binom{j - \sigma}{k - \sigma}\binom{n - \ell - j + \sigma}{x - k + \sigma}}{\binom{n-\ell}{x}}\;. \label{eq:t4}    
\end{align}
Unlike most of the other terms we have studied, this term includes a dependence on the extensive quantity $n$, which can be re-expressed (in expectation) in terms of $m$.
Let us parse the asymptotics of this term up to order $m^{-1}$. 
When $k < \sigma$, $t_{k|x\sigma\ell jh} = 0$. 
When $k = \sigma$, this expression simplifies in the asymptotic limit to 
\begin{align}
    t^{(4)}_{k|xk\ell jh} = \frac{\binom{n - \ell - j + k}{x}}{\binom{n-\ell}{x}} \doteq 1\;.
\end{align}
When $k = \sigma + 1$, we have 
\begin{align}
    t^{(4)}_{k|x\sigma\ell jh} = \frac{\binom{j-\sigma}{1} \binom{n-\ell - j + \sigma}{x-1}}{\binom{n-\ell}{x}} \doteq (j-\sigma) \frac{{(n-\ell - j + \sigma)}^{x-1}}{(x-1)!} \frac{x!}{(n -\ell)^x} \doteq x(j-\sigma)n^{-1}\;. 
\end{align}
Recalling that $\mean{d}n = \mean{k}{m}$ and that $\frac{\mean{k}}{\mean{d}} = \mu_{\vbeta}$, we find that, when $k = \sigma+1$,  
\begin{align}
    t^{(4)}_{k|x\sigma\ell jh} = x(j-\sigma)\mu_{\vbeta} m^{-1}\;.
\end{align}
A similar calculation shows that, if $k > \sigma+1$, then $t^{(4)}_{k|x\sigma\ell jh} = O(m^{-2})$.
We therefore conclude 
\begin{align}
    t^{(4)}_{k|x\sigma\ell jh}  &\doteq \begin{cases}
        0 &\quad k < 0 \text{ or } k > j\\ 
        1 &\quad k = \sigma \\ 
        x(j-\sigma)\mu_{\vbeta} m^{-1} &\quad k = \sigma+1 \\ 
        O(m^{-2}) &\quad k > \sigma+1\;.
    \end{cases}
\end{align}

Next, $t^{(5)}_{\sigma|s\ell jh}$ is the probability that, among $s$ nodes added to $e$ through the edge-sampling step, a total of $\sigma$ of them are also elements of $g$.
This probability is also hypergeometric: we require $\sigma$ successful draws in $s$ total draws from a population of $\ell$ nodes in $f$ containing $\abs{f \cap g} = h$ ``successful'' nodes which are also elements of $g$. 
This gives 
\begin{align}
    t^{(5)}_{\sigma|s\ell jh} &\eqdef \prob{S(e,g) = \sigma \given S(e) = s, I^*_{\ell j h}}  \\ 
    &= \mathrm{HyperGeometric}(\sigma; s, h, \ell)  \\ 
    &= \frac{\binom{h}{\sigma}\binom{\ell - h}{s - \sigma}}{\binom{\ell}{s}}\;. \label{eq:t5}
\end{align}

These expressions then give us an approximation for $t^{(2)}_{k|sx\ell jh}$: we separate out the cases $\sigma = k$ and $\sigma = k-1$, yielding
\begin{align}
    t^{(2)}_{k|sx\ell jh} &= \prob{\abs{e\cap g} = k \given S(e) = s, X(e) = x, I^*_{\ell j h}}  \\
    &= \sum_{\sigma} t^{(4)}_{k|x\sigma\ell jh} t^{(5)}_{\sigma|s\ell jh}  \\
    &\doteq t^{(4)}_{k|xk\ell jh} t^{(5)}_{k|s\ell jh} + t^{(4)}_{k|x(k-1)\ell jh} t^{(5)}_{k|s\ell jh} + O(m^{-2})  \\
    &= \frac{\binom{h}{k}\binom{\ell - h}{s - k}}{\binom{\ell}{s}} + \frac{\binom{h}{k-1}\binom{\ell - h}{s - k+1}}{\binom{\ell}{s}} x(j - k + 1)\mu_{\vbeta} m^{-1} + O(m^{-2})\\ 
    &\eqdef w^{(1)}_{k|s\ell h} + w^{(2)}_{k|sx\ell jh}m^{-1} + O(m^{-2})\;, \label{eq:t2}
\end{align}
where 
\begin{align}
    w^{(1)}_{k|s\ell h} \eqdef \frac{\binom{h}{k}\binom{\ell - h}{s - k}}{\binom{\ell}{s}} \quad \text{and} \quad 
    w^{(2)}_{k|sx\ell jh} \eqdef \frac{\binom{h}{k-1}\binom{\ell - h}{s - k+1}}{\binom{\ell}{s}} x(j - k + 1)\mu_{\vbeta}\;. \label{eq:w1-and-w2}
\end{align}
This expression says that, to form an intersection of size $k$ with edge $g$, we either need to pick $k$ nodes from $g$ during the edge-sampling step, or $k-1$ nodes from $g$ during the edge-sampling step together with one additional node from the extant node addition step, with other possibilities being much less likely. 
Importantly, $w^{(1)}_{k|s\ell h} = 0$ iff $k \geq h+1$, while $w^{(2)}_{k|sx\ell jh} = 0$ iff $k \geq h+2$.
In particular, $w^{(2)}_{1|sx\ell j0} \geq 0$. 
Furthermore, $w^{(2)}_{k|sx\ell jh} = 0$ if $k = 0$. 

To sum up these calculations, we can insert our findings into \eqref{eq:a}:  
\begin{align}
    a^*_{ik|\ell jh } &\doteq \sum_{s} t^{(1)}_{s|\ell jh} \sum_{x} \gamma_x  t^{(2)}_{k|sx\ell jh} t^{(3)}_{i|sx} \\ 
    &\doteq\sum_{s} t^{(1)}_{s|\ell jh} \sum_{x} \gamma_x \paren{w^{(1)}_{k|s\ell h} + w^{(2)}_{k|sx\ell jh}m^{-1} + O(m^{-2})} t^{(3)}_{i|sx}  \\
    &\doteq \phi_{ik|\ell jh} + m^{-1}\psi_{ik|\ell jh} \;, 
\end{align}
where 
\begin{align}
    \phi_{ik|\ell jh} &\eqdef \sum_{s} t^{(1)}_{s|\ell jh} \sum_{x} \gamma_x w^{(1)}_{k|s\ell h} t^{(3)}_{i|sx} \quad \text{and} \quad 
    \psi_{ik|\ell jh} \eqdef \sum_{s} t^{(1)}_{s|\ell jh} \sum_{x} \gamma_x w^{(2)}_{k|sx\ell jh} t^{(3)}_{i|sx}\;.
\end{align}
We note that, since  $w^{(2)}_{k|sx\ell jh} = 0$ if $k = 0$, it is also the case that $\psi_{ik|\ell jh} = 0$ if $k = 0$.
We also have that $\psi_{ik|\ell jh} = 0$ if $k \geq h + 2$ per the argument above. 
Our computations above imply that $\phi_{ik|\ell jh} = 0$ if $k > h$ or $k > \ell$, and $\psi_{ik|\ell jh} = 0$ if $k > h+1$,  $k > \ell$, or $k = 0$. 

Our approximate compartmental update now reads 
\begin{align}
    p'_{ijk} &\doteq p_{ijk} + {\frac{1}{2}}b_{ik|j}\sum_{\ell h} \paren{r_{\ell j h} + r_{j\ell h}} + \frac{m-1}{2} \sum_{\ell h} \brackets{ \paren{\phi_{ik|\ell j h} + m^{-1} \psi_{ik|\ell j h}} r_{\ell j h} + \paren{\phi_{ik|j \ell h} + m^{-1} \psi_{ik|j \ell h}} r_{j \ell h}} \\ 
    &\doteq p_{ijk} + {\frac{1}{2}}b_{ik|j}\sum_{\ell h} \paren{r_{\ell j h} + r_{j\ell h}} + \frac{1}{2}\sum_{\ell h} \paren{\psi_{ik|\ell j h} r_{\ell jh} + \psi_{ik|j \ell h}r_{j \ell h}} + \frac{m-1}{2}\sum_{\ell h} \paren{\phi_{ik|\ell j h}r_{\ell j h} + \phi_{ik|j \ell h}r_{j\ell h}} \;. \label{eq:compartmental-linearized}
\end{align}

\subsubsection{Asymptotic Behavior of $\mathbf{r_{ijk}}$}

We now aim to use the compartmental update \cref{eq:compartmental-1}, along with the formulae in \cref{eq:zijk,eq:t1,eq:t3,eq:t2} to study the asymptotic behavior of $r_{ijk}$ as $m$ grows large.  

Let us assume that, at stationarity,  
\begin{align}
    r_{ijk} \doteq m^{-\lambda_k} q_{ijk} \label{eq:asymptotics}
\end{align}
for all $i, j, k$ where $q_{ijk}$ is a nonnegative constant independent of $m$, and $\lambda_k$ depends only on $k$ but not on $i$ or $j$. 
We assume that $q_{ijk} > 0$ when $k \leq i \land j$, provided that $i$ and $j$ are edge sizes supported by the model.  
Our aim is to determine the values of $\lambda_k$ and $q_{ijk}$. 
Our strategy is to substitute \cref{eq:asymptotics} into \cref{eq:compartmental-linearized} and then determine the values of $\lambda_k$ and $q_{ijk}$. 

Substituting \cref{eq:asymptotics} into \cref{eq:compartmental-linearized}, along with $p'_{ijk} = \binom{m+1}{2}r_{ijk}$ and $p_{ijk} = \binom{m}{2}r_{ijk}$ gives 
\begin{align}
    \binom{m+1}{2}(m+1)^{-\lambda_k}q_{ijk} &\doteq \binom{m}{2}m^{-\lambda_k}q_{ijk} + \frac{1}{2}b_{ik|j} \sum_{h\leq \ell} m^{-\lambda_h} (q_{\ell jh} + q_{j\ell h}) + \frac{1}{2}\sum_{\ell, h } m^{-\lambda_h}\paren{\psi_{ik|\ell j h} q_{\ell jh} + \psi_{ik|j \ell h}q_{j \ell h}} \nonumber\\
        &\quad  + \frac{m-1}{2}\sum_{\ell, h} m^{-\lambda_h} \paren{q_{\ell j h} \phi_{ik|\ell jh} + q_{j \ell h} \phi_{ik|j\ell h}}\;.
\end{align}
We now move $\binom{m}{2}m^{-\lambda_k}q_{ijk}$ to the left side and simplify 

%\pc{I have revised here and below. The primary innovation is the correction of the calculation immediately below, which introduces a new coefficient $c_k$ into the calculation. The practical role of $c_k$ is that it is going to cancel all of the $\frac{1}{2}$'s in the calculation when $k \geq 1$. I think it all works but please check!!!}
\begin{align}
    \binom{m+1}{2}(m+1)^{-\lambda_k}q_{ijk} - \binom{m}{2}m^{-\lambda_k}q_{ijk} &= q_{ijk}\brackets{\frac{m(m+1)}{2}(m+1)^{-\lambda_k} - \frac{m(m-1)}{2}m^{-\lambda_k}} \\ 
    &= \frac{q_{ijk}}{2}\brackets{m(m+1)^{1-\lambda_k} - (m-1)m^{1-\lambda_k}} \\ 
    &= \frac{m^{1-\lambda_k} q_{ijk}}{2} \brackets{m\paren{\frac{m+1}{m}}^{1-\lambda_k} - (m-1)} \\ 
    &= \frac{m^{2-\lambda_k} q_{ijk}}{2} \brackets{\paren{\frac{m+1}{m}}^{1-\lambda_k} - \frac{m-1}{m}} \\ 
    &= \frac{m^{2-\lambda_k} q_{ijk}}{2} \brackets{1 + \frac{1}{m}(1-\lambda_k) +  o\paren{\frac{1}{m}} - 1 + \frac{1}{m}} \\ 
    &= \frac{m^{2-\lambda_k} q_{ijk}}{2} \brackets{(2-\lambda_k)\frac{1}{m} + o\paren{\frac{1}{m}}} \\ 
    &\doteq \frac{m^{2-\lambda_k} q_{ijk}}{2} \brackets{(2-\lambda_k)\frac{1}{m}} \\ 
    &= \frac{(2-\lambda_k)m^{1-\lambda_k} q_{ijk}}{2} \\ 
    &\triangleq c_k m^{1-\lambda_k} q_{ijk}\;,
\end{align}
where we have defined $c_k = \frac{2-\lambda_k}{2}$.
We assume throughout that $\lambda_k \neq 2$.
We then have 
\begin{align}
    c_k m^{1-\lambda_k} q_{ijk} &\doteq \frac{1}{2}b_{ik|j} \sum_{h\leq \ell} m^{-\lambda_h} (q_{\ell jh} + q_{j\ell h}) + \frac{1}{2}\sum_{\ell, h} m^{-\lambda_h}\paren{\psi_{ik|\ell j h} q_{\ell jh} + \psi_{ik|j \ell h}q_{j \ell h}} \nonumber\\ 
    &\quad + \frac{m-1}{2}\sum_{\ell, h} m^{-\lambda_h} \paren{q_{\ell j h} \phi_{ik|\ell jh} + q_{j \ell h} \phi_{ik|j\ell h}} \\ 
    &\doteq \frac{1}{2}b_{ik|j} \sum_{h\leq \ell} m^{-\lambda_h} (q_{\ell jh} + q_{j\ell h}) + \frac{1}{2}\sum_{\ell, h} m^{-\lambda_h}\paren{\psi_{ik|\ell j h} q_{\ell jh} + \psi_{ik|j \ell h}q_{j \ell h}} \nonumber\\ 
    &\quad + \frac{1}{2}\sum_{\ell, h} m^{1-\lambda_h} \paren{q_{\ell j h} \phi_{ik|\ell jh} + q_{j \ell h} \phi_{ik|j\ell h}}\;,
\end{align}
yielding 
\begin{align}
    q_{ijk} &\doteq \frac{1}{2c_k}b_{ik|j} \sum_{h \leq \ell} m^{\lambda_k-\lambda_h-1} (q_{\ell jh} + q_{j\ell h}) + \frac{1}{2c_k}\sum_{\ell, h} m^{\lambda_k-\lambda_h-1}\paren{\psi_{ik|\ell j h} q_{\ell jh} + \psi_{ik|j \ell h}q_{j \ell h}} \nonumber\\ 
    &\quad + \frac{1}{2c_k}\sum_{\ell, h} m^{\lambda_k-\lambda_h} \paren{q_{\ell j h} \phi_{ik|\ell jh} + q_{j \ell h} \phi_{ik|j\ell h}} \label{eq:asymptotics-3}
\end{align}
provided that $\lambda_k \neq 2$. 

We now determine the values of $\lambda_k$. 
We first consider $k = 0$. 
In this case, $b_{ik|j} = 0$ and $\psi_{ik|\ell jh} = 0$ for all $i$ and $j$. 
This simplifies \cref{eq:asymptotics-3} to 
\begin{align}
    q_{ij0} \doteq \frac{1}{2c_0}\sum_{\ell, h} m^{\lambda_0-\lambda_h} \paren{q_{\ell j h} \phi_{i0|\ell jh} + q_{j \ell h} \phi_{i0|j\ell h}}\;.
\end{align}
The requirement that $q_{ij0}$ be a constant implies that either (a) $a_{i0|\ell jh}=0$ and $a_{i0|\ell jh}=0$ or (b) $\lambda_0 \leq \lambda_h$ for all $h \geq 0$. 
Case (a) occurs only when no novel nodes are added to the hypergraph ($\beta_0 = 1$). %\footnote{Throughout the remainder of this section we disregard this case.} 
For the remainder of this section, we will assume that this is not the case. 
In case (b), the normalization requirement 
\begin{align}
    1 &= \sum_{ijk} r_{ijk} = \sum_{ijk} m^{-\lambda_k}q_{ijk}\;.
\end{align}
% \begin{align}
%     1 &= \sum_{ijk} r_{ijk} - \sum_{ik}r_{iik} \\ 
%       &= \sum_{ijk} m^{-\lambda_k}q_{ijk} - \sum_{ik} m^{-\lambda_k}q_{iik}\;.
% \end{align}
implies that $\lambda_k = 0$ for at least one choice of $k$.
It follows that $\lambda_0 = 0$. 

Furthermore, since $b_{ik|j} > 0$ whenever $k \leq i\land j$, \cref{eq:asymptotics-3} implies that $\lambda_k - \lambda_h - 1 \leq 0$. 
Choosing $h = 0$ implies that $\lambda_k \leq 1$ for all $k$.
We now show that, under our assumptions, $\lambda_k = 1$ for all $k \geq 1$. 
Fix $k = \argmin_{h \geq 1}\lambda_h$. 
Consider the exponent of $m$ in the first two terms of \cref{eq:asymptotics-3}, which is $\lambda_k - \lambda_h - 1$ as $h$ ranges.  
Let us first assume that these terms do not vanish as $m\rightarrow \infty$. 
This requires that $\lambda_k = 1 + \lambda_{h^*}$ for at least one choice  $h = h^*$.
If $h^* \geq 1$, we may repeat this argument to find $h^{**}$ such that $\lambda_{h^*} = 1 + \lambda_{h^{**}}$. 
But then, $\lambda_k = 2 + \lambda_{h^{**}}$, which contradicts the requirement that $\lambda_k - \lambda_{h^{**}} - 1 \leq 0$. 
Therefore, we must have $h^* = 0$, from which it follows that $\lambda_k = 1$. 

So far, we have shown that, for any $k$ such that the first two terms of \cref{eq:asymptotics-3} do not vanish, we must have $\lambda_k = 1$.
We will now show that if the two terms of \cref{eq:asymptotics-3} do vanish for some $k = k^*$, then $q_{ijh} = 0$ for all $i$,  $j$, and $h \geq k^*$.
Since the sum defining the first two terms includes $h = 0$, vanishing of the first two terms would imply that $\lambda_{k^*} < 1$. 
In order for the second term to remain bounded, we must also have $\lambda_h \geq \lambda_{k^*}$ for all $h \geq k^*$, with at least one $h\geq k^*$ such that $\lambda_h = \lambda_{k^*}$.
Indeed, the second term of \cref{eq:asymptotics-3} can be maximized by setting $\lambda_{h} = \lambda_{k^*}$ for all $h \geq {k^*}$.
This gives the approximate linear system 
\begin{align}
    q_{ijk} \doteq \frac{1}{2c_k} \sum_{\ell, h}  \paren{q_{\ell j h} \phi_{ik|\ell jh} + q_{j \ell h} \phi_{ik|j\ell h}} \quad \forall k \geq k^*\;. \label{eq:linear-system-contradiction}
\end{align}
Recalling that $\psi_{ik|\ell jh} = 0$ whenever $k > h$, we find that this system is closed in the entries $q_{ijk}$ such that $h \geq {k^*}$, and implies that the vector $\mathbf{q}$ is an eigenvector with eigenvalue 1 of the matrix $\mathbf{C}$ whose action on $\mathbf{q}$ is defined by \cref{eq:linear-system-contradiction}. 
This, however, is impossible, since $\phi_{ik|\ell jh} \geq 0$ for all $i, k, \ell, j, h$, $\sum_{\ell, h }\phi_{ik|\ell jh} = 1$, and $c_k \geq \frac{1}{2}$, which means that $\frac{1}{c_k}\sum_{\ell, h }\phi_{ik|\ell jh} < 1$ and therefore $\frac{1}{2c_k}\sum_{\ell, h }\brackets{\phi_{ik|\ell jh} + \phi_{ik|j \ell h}} < 1$.
This implies that the spectral radius of $\mathbf{C}$ is strictly less than 1, so the only solution to the system is $\mathbf{q} = \mathbf{0}$.
This contradicts the assumption that $q_{ijk} > 0$ for all $i$, $j$, and $k\geq k^*$.
We conclude that the first term of \cref{eq:asymptotics-3} does not vanish for any $k$.

Summarizing, \emph{under our assumptions}, 
\begin{align}
    \lambda_k = \begin{cases} 
        0 &\quad k = 0 \\ 
        1 &\quad k \geq 1\;.
    \end{cases}
\end{align}
To complete our asymptotic analysis, it is necessary to describe the values of $q_{ijk}$. 
We proceed from \cref{eq:asymptotics-3}. 
When $k = 0$, $\lambda_k = 0$ and $c_k = 1$. We then have 
\begin{align}
    q_{ij0} &\doteq \frac{1}{2}\sum_{\ell, h\geq 0} m^{\lambda_0-\lambda_h} \paren{q_{\ell j h} \phi_{i0|\ell jh} + q_{j \ell h} \phi_{i0|j\ell h}} \\ 
    &= \frac{1}{2}\brackets{\sum_{\ell} \paren{q_{\ell j 0} \phi_{i0|\ell j0} + q_{j \ell 0} \phi_{i0|j\ell 0}} + \sum_{\ell, h\geq 1} m^{-1} \paren{q_{\ell j h} \phi_{i0|\ell jh} + q_{j \ell h} \phi_{i0|j\ell h}}}  \\
    &\doteq \frac{1}{2}\sum_{\ell} \paren{q_{\ell j 0} \phi_{i0|\ell j0} + q_{j \ell 0} \phi_{i0|j\ell 0}}\;. \label{eq:system-1}
\end{align}
%PJM commented out because it is duplicative of the k\geq 1 arguments that follow
% Next, when $k = 1$, we have 
% \begin{align}
%     q_{ij1} &\doteq \frac{1}{2}b_{i1|j} \sum_{h\leq \ell} m^{\lambda_1-\lambda_h-1} (q_{\ell jh} + q_{j\ell h}) +\frac{1}{2}\sum_{\ell, h \leq k} m^{\lambda_1-\lambda_h-1}\paren{\psi_{i1|\ell j h} q_{\ell jh} + \psi_{i1|j \ell h}q_{j \ell h}} \nonumber\\
%     &\quad + \frac{1}{2}\sum_{\ell, h\geq 1} m^{\lambda_1-\lambda_h} \paren{q_{\ell j h} \phi_{i1|\ell jh} + q_{j \ell h} \phi_{i1|j\ell h}}  \\ 
%     &= \frac{1}{2}b_{i1|j} \sum_{h\leq \ell} m^{-\lambda_h} (q_{\ell jh} + q_{j\ell h}) +\frac{1}{2}\sum_{\ell, h \leq k} m^{-\lambda_h}\paren{\psi_{i1|\ell j h} q_{\ell jh} + \psi_{i1|j \ell h}q_{j \ell h}} \nonumber\\
%     &\quad + \frac{1}{2}\sum_{\ell, h\geq 1} m^{1-\lambda_h} \paren{q_{\ell j h} \phi_{i1|\ell jh} + q_{j \ell h} \phi_{i1|j\ell h}} \\ 
%     &\doteq \frac{1}{2}b_{i1|j} \sum_{\ell}  (q_{\ell j0} + q_{j\ell 0}) +\frac{1}{2}\sum_{\ell} \paren{\psi_{i1|\ell j 0} q_{\ell j0} + \psi_{i1|j \ell 0}q_{j \ell 0}} +  \frac{1}{2}\sum_{\ell, h\geq 1} \paren{q_{\ell j h} \phi_{i1|\ell jh} + q_{j \ell h} \phi_{i1|j\ell h}}\;.
% \end{align}
Next, when $k \geq 1$, $\lambda_k = 1$ and $c_k = \frac{1}{2}$. We then have 
\begin{align}
    q_{ijk} &\doteq \frac{1}{2c_k}b_{ik|j} \sum_{h\leq \ell} m^{\lambda_k-\lambda_h-1} (q_{\ell jh} + q_{j\ell h}) + \frac{1}{2c_k}\sum_{\ell, h \leq k} m^{\lambda_k-\lambda_h-1}\paren{\psi_{ik|\ell j h} q_{\ell jh} + \psi_{ik|j \ell h}q_{j \ell h}} \nonumber\\
    &\quad + \frac{1}{2c_k}\sum_{\ell, h\geq k} m^{\lambda_k-\lambda_h} \paren{q_{\ell j h} \phi_{ik|\ell jh} + q_{j \ell h} \phi_{ik|j\ell h}} \\ 
    &\doteq b_{ik|j} \sum_{h\leq \ell} m^{\lambda_k-\lambda_h-1} (q_{\ell jh} + q_{j\ell h}) + \sum_{\ell, h \leq k} m^{\lambda_k-\lambda_h-1}\paren{\psi_{ik|\ell j h} q_{\ell jh} + \psi_{ik|j \ell h}q_{j \ell h}} \nonumber\\
    &\quad + \sum_{\ell, h\geq k} m^{\lambda_k-\lambda_h} \paren{q_{\ell j h} \phi_{ik|\ell jh} + q_{j \ell h} \phi_{ik|j\ell h}} \\ 
    &= b_{ik|j} \sum_{h\leq \ell} m^{-\lambda_h} (q_{\ell jh} + q_{j\ell h}) + \sum_{\ell, h \leq k} m^{-\lambda_h}\paren{\psi_{ik|\ell j h} q_{\ell jh} + \psi_{ik|j \ell h}q_{j \ell h}} \nonumber\\
    &\quad + \sum_{\ell, h\geq k}  \paren{q_{\ell j h} \phi_{ik|\ell jh} + q_{j \ell h} \phi_{ik|j\ell h}} \\ 
    &\doteq b_{ik|j} \sum_{\ell} (q_{\ell j0} + q_{j\ell 0}) + \sum_{\ell} \paren{\psi_{ik|\ell j 0} q_{\ell j0} + \psi_{ik|j \ell 0}q_{j \ell 0}} + \sum_{\ell, h\geq k} \paren{q_{\ell j h} \phi_{ik|\ell jh} + q_{j \ell h} \phi_{ik|j\ell h}}\;.
\end{align}
In the case $k=1$, this becomes 
\begin{align}
    q_{ij1} &\doteq b_{i1|j} \sum_{\ell}  (q_{\ell j0} + q_{j\ell 0}) +\sum_{\ell} \paren{\psi_{i1|\ell j 0} q_{\ell j0} + \psi_{i1|j \ell 0}q_{j \ell 0}} +  \sum_{\ell, h\geq 1} \paren{q_{\ell j h} \phi_{i1|\ell jh} + q_{j \ell h} \phi_{i1|j\ell h}} \label{eq:system-k1}\;,
\end{align}
while the $k>1$ case simplifies further, using the fact that
$\psi_{ik|\ell jh} = 0$ for $k \geq h+2$, 
to give
\begin{align}
    q_{ijk} &\doteq b_{ik|j} \sum_{\ell} (q_{\ell j0} + q_{j\ell 0}) + \sum_{\ell, h\geq k}  \paren{q_{\ell j h} \phi_{ik|\ell jh} + q_{j \ell h} \phi_{ik|j\ell h}} \label{eq:system-2} \quad\mathrm{for}\quad k>1\;.
\end{align}
Jointly, \cref{eq:system-1,eq:system-k1,eq:system-2} define an approximate linear system for $q_{ijk}$: 
\begin{align}
    \mathbf{q} = \mathbf{C} \mathbf{q}\;,
\end{align}
where the entries of $\mathbf{C}$ are defined to appropriately conform to the entries of a (vectorized) $\mathbf{q}$. 
We note that $\mathbf{C}$ has nonnegative entries, so $\mathbf{q}$ has to be its Perron eigenvector. 
In principle, writing down $\mathbf{C}$ and finding the Perron eigenvector would be sufficient to determine $\mathbf{q}$. 

\subsection{Computational Challenges}

In the experiment shown in the main text, we tracked edge sizes and intersections up to size $12$, resulting in a matrix $\mathbf{C}$ of size $12^3 \times 12^3$.
Experimentally, we found that the LAPACK solver (accessed through the \texttt{numpy.linalg.eig} function in Python) was able to accurately find the leading eigenpair for this matrix for some but not all parameter combinations, with larger values of $\eta$ especially leading to convergence issues. 
Our experimental evidence suggests that this was indeed a solver issue rather than an issue with our proposed approximation scheme, in that the numerically obtained solution in such cases was demonstrably not an eigenvalue. 
Because we considered the results such as those shown in the main text to constitute sufficient evidence of the correctness of our approximation scheme, we did not pursue other solvers or otherwise attempt to solve the system for all parameter combinations.


\section{Descriptions of Alternative Models} \label{sec:alternative-models}

\subsection{Growing Hypergraph Erd\H{o}s-R\'enyi model}

We list the steps that we take for growing hypergraphs in an Erd\H{o}s-R\'enyi manner in a framework similar to what we did for HCM in the main text. We emphasize that our approach here is only one way of generating hypergraphs that generalize Erd\H{o}s-R\'enyi networks and Preferential Attachment networks; this particular approach was chosen to maintain a roughly consistent edge size with the \model at each time step. Recall that at each time step of the \model, we seed the new edge $e$ with nodes from existing edge $f$ (first uniformly sampling one node from $f$, and then adding each other node IID with probability $\eta$) --- we denote this positive integer as $\alpha$. The \model step also includes $g \sim \extantnodedist$ extant nodes and $b \sim \novelnodedist$ novel nodes. Denoting the newly formed edge as $\at{e}{t+1}$, our corresponding Erd\H{o}s-R\'enyi generalization proceeds as follows.

\begin{enumerate}
    \item \textbf{Extant node sampling}: Following the above notation, we select $\alpha + g$ nodes drawn uniformly at random without replacement from $\at{\nodeset}{t}$ to initiate $\at{e}{t+1}$.
    \item \textbf{Novel node addition}: We next sample $\hat{b}$ from a Poisson distribution with mean $b$ (so that there is some randomness in this number compared to the \model) and add $\hat{b}$ novel nodes to $\at{e}{t+1}$.
\end{enumerate}
After forming $\at{e}{t+1}$, we have an Erd\H{o}s-R\'enyi update $\at{\hypergraph}{t+1} = (\at{\nodeset}{t} \cup \braces{\at{e}{t+1}}, \at{\edgeset}{t} \cup {\at{e}{t+1}})$. 

\subsection{Hypergraph Preferential Attachment Model}

We do not employ the model of \cite{avinRandomPreferentialAttachment2019} due to lack of available code for simulation or inference. Instead, the generalization of preferential attachment we use here takes the different numbers from the \model step, using the two different numbers of extant nodes to mix preferential and uniform selection. Note that the \textbf{Novel node addition} step below is exactly the same as described for Erd\H{o}s-R\'enyi above.
\begin{enumerate}
    \item \textbf{Degree based sampling}: We select $\alpha$ nodes drawn with probability proportional to their degree without replacement from $\at{\nodeset}{t}$ and name this set $\alpha'$.
    \item \textbf{Extant node sampling}: We select $g$ nodes drawn uniformly at random without replacement from $\at{\nodeset}{t} \setminus \alpha'$ to initiate $\at{e}{t+1}$.
    \item \textbf{Novel node addition}: We next sample $\hat{b}$ from a Poisson distribution with mean $b$ (so that there is some randomness in this number compared to the \model) and add $\hat{b}$ novel nodes to $\at{e}{t+1}$. 
\end{enumerate}
After forming $\at{e}{t+1}$, we have a Preferential Attachment update $\at{\hypergraph}{t+1} = (\at{\nodeset}{t} \cup \braces{\at{e}{t+1}}, \at{\edgeset}{t} \cup {\at{e}{t+1}})$. 




