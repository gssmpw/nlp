We propose a generative model of temporally-evolving hypergraphs in which hyperedges form via noisy copying of previous hyperedges. 
Our proposed model reproduces several stylized facts from many empirical hypergraphs, is learnable from data, and defines a likelihood over a complete hypergraph rather than ego-based or other sub-hypergraphs. 
Analyzing our model, we derive descriptions of node degree, edge size, and edge intersection size distributions in terms of the model parameters. 
We also show several features of empirical hypergraphs which are and are not successfully captured by our model. 
We provide a scalable stochastic expectation maximization algorithm with which we can fit our model to hypergraph data sets with millions of nodes and edges. 
Finally, we assess our model on a hypergraph link prediction task, finding that an instantiation of our model with just 11 parameters can achieve competitive predictive performance with large neural networks. 
