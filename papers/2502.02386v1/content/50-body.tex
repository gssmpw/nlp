\dropcap{M}any complex systems are composed of simple components participating in multi-way interactions. 
Such systems --- often called \emph{higher-order networks} \cite{bickWhatAreHigherOrder2023a} to distinguish them from networks composed of only pairwise interactions --- have received much attention in recent years. 
Higher-order networks preserve richer structural information about a system and therefore admit a broader array of measures, dynamics, and algorithms than their pairwise counterparts \cite{battistonNetworksPairwiseInteractions2020,battiston2021physics,pmlr-v97-chitra19a}. 
Higher-order networks are frequently represented as either hypergraphs or simplicial complexes \cite{bacciniWeightedSimplicialComplexes2022}. 
Whether a given system demands a higher-order representation, and which one to choose, are subtle modeling questions \cite{torresWhyHowWhen2021} driven by both the data analysis techniques to be used and the structure of the data itself. 


Hypergraphs are among the most flexible data structures for representing higher-order networks. 
In hypergraphs, each multi-way interaction is represented by an edge consisting of a set of nodes of arbitrary finite size. 
While simplicial complex representations require that every subset of every edge is also present in the data, hypergraphs require no such stipulation. 
Empirically, many hypergraphs come close to satisfying this subset inclusion criterion, but many others do not \cite{landrySimplicialityHigherorderNetworks2024}. 
While many hypergraph data sets include only the memberships of nodes in edges, other data sets may include node attributes \cite{badalyanHypergraphsNodeAttributes2023}, directedness or generalized node-edge roles \cite{gallo1993directed,chodrowAnnotatedHypergraphsModels2020}, and temporal information about the arrival or duration of interactions \cite{leeTHyMeTemporalHypergraph2021,myersTopologicalAnalysisTemporal2023}. 
The latter case is often described under the heading of \emph{temporal hypergraphs}, and is our modeling focus in this paper. 
Some examples of systems naturally represented by temporal hypergraphs include group socializing, email communication, and scholarly collaboration \cite{neuhauserConsensusDynamicsTemporal2021,sahasrabuddheModellingNonlinearConsensus2021,cencettiTemporalPropertiesHigherorder2021}.


A common aim in modeling temporal hypergraphs is link prediction, first popularized for dyadic graphs \cite{liben-nowellLinkpredictionProblemSocial2007}: given observations of the hypergraph up to a specified time, we aim to predict which possible new edges are likely to form in the future. 
The link prediction problem in hypergraphs has a wide range of applications: 
in collaboration networks, predicting future collaborations among multiple entities can aid in resource allocation and project planning \cite{zhangHypergraphModelSocial2010}; 
in biological systems, forecasting interactions among a set of molecules can contribute to drug discovery and understanding molecular processes \cite{tavakoliRxnHypergraphHypergraph2022}; 
in logistics and supply chain management, predicting future connections in a hypergraph can optimize the flow of goods and resources \cite{suoExploringEvolutionaryMechanism2018}.
The problem of link prediction in hypergraphs was popularized by Benson et al.~\cite{bensonSimplicialClosureHigherorder2018} and has since received treatment from a wide range of approaches \cite{lizotteHypergraphReconstructionUncertain2023,youngHypergraphReconstructionNetwork2021}, with 
techniques including linear discriminative models \cite{bensonSimplicialClosureHigherorder2018}, neural discriminative models \cite{yadatiNHPNeuralHypergraph2020}, statistical generative models \cite{ruggeriCommunityDetectionLarge2023}, and mechanistic generative models \cite{bensonSequencesSets2018,avinRandomPreferentialAttachment2019,giroirePreferentialAttachmentHypergraph2022,giroirePreferentialAttachmentHypergraph2022a}. 

We take a mechanistic approach to the modeling of temporal hypergraphs. 
Our modeling framework is motivated by a simple, fundamental difference between hypergraphs and dyadic graphs. 
In dyadic graphs, all edges contain two nodes (ignoring self-loops). 
Any pair of edges can therefore intersect on zero, one, or two nodes. 
The number of two-edge motifs \cite{miloNetworkMotifsSimple2002} in undirected graphs (in which self-loops are disallowed but multiedges are allowed) is therefore three. 
In contrast, in hypergraphs, an edge of size $i$ may intersect with another edge of size $j$ on a set of any size $k \leq \min\{i,j\}$. 
The number of possible two-edge motifs in a hypergraph \cite{leeHypergraphMotifsConcepts2020,lotitoHigherorderMotifAnalysis2022} with maximum edge-size $\bar{k}$ is therefore $O(\bar{k}^3)$, since the sizes of two edges, as well as the size of the intersection can all be distinguished. 
A wide range of intersection behavior is observed in empirical hypergraph data \cite{leeHowHyperedgesOverlap2021}, at rates much higher than explained by simple null models \cite{chodrowConfigurationModelsRandom2020}. 


Several generative models have been proposed which produce large hypergraph edge intersections \cite{leeSurveyHypergraphMining2024}. 
The model of Lee, Choe, and Shin \cite{leeHowHyperedgesOverlap2021} reproduces several empirical intersection patterns in a hypergraph with specified edge-size and node degree distributions. 
Because these features of the data are specified in advance, the model is statistically generative but does not model mechanistic evolution in growing hypergraphs. 
The Correlated Repeated Unions (CRU) model of Benson, Kumar, and Tomkins \cite{bensonSequencesSets2018} is an explicitly mechanistic and temporal hypergraph model: arriving edges are formed as unions of noisy copies of previous edges. 
Copy models have a rich history in network science, serving, for example, as an early alternative to preferential attachment as a mechanistic explanation for heavy tails in the degree sequences of dyadic graphs \cite{kleinbergWebGraphMeasurements1999,soleModelLargeScaleProteome2002,vazquezModelingProteinInteraction2003,newman2018networks}. 
The CRU model is designed to generate hypergraphs in which the same subsets of nodes appear in multiple edges. 
This model, however, is designed for subsampled hypergraphs which may be viewed as single ``sequences of sets.''
Applying the model to a full hypergraph dataset requires the researcher to subsample the data into one or more such sequences; 
this limits the model's ability to describe hypergraphs in which seemingly disparate sequences may merge, and also prevents the researcher from evaluating a model likelihood on the complete data. 
Roh et al.~\cite{rohGrowingHypergraphsPreferential2023} details the degree distribution and edge-size distribution for a model of growing hypergraphs with preferential linking in which the evolution of the hypergraph is based entirely on adding new nodes to current edges and nodes. 
Our proposed model is perhaps most similar to the hypergraph preferential attachment models proposed by Avin et al.~\cite{avinRandomPreferentialAttachment2019} and Giroire et al.~\cite{giroirePreferentialAttachmentHypergraph2022} involve several types of updates to the hypergraph, such as isolated vertex addition and multiple types of edge addition. 
In each timestep, one of these actions is selected and performed. 
In contrast, our model involves a single, multipart update step. 

Our proposed \Model (\model) employs a simple edge-copying growth mechanism. 
In each timestep, a new edge is formed as a noisy copy of an existing edge that was formed at a prior time, supplemented with nodes existing elsewhere in the hypergraph and novel nodes added after copying. 
In Methods, we describe the model update and derive asymptotic descriptions of the edge-size distribution, node degree distribution, and edge intersection rates. 
The generative nature of this \model also provides a principled statistical framework for fitting and evaluating model fit to data. 
However, because we do not observe the identity of the edge which is (noisily) copied in each timestep, direct optimization of the model likelihood is intractable. 
Instead, we develop a stochastic expectation-maximization algorithm \cite{cappeOnLineExpectationMaximization2009} for the inference task. 
In Results, we use stochastic expectation-maximization to fit our model to 27 empirical hypergraphs of sizes spanning several orders of magnitude.  
We also evaluate the fitted model on a hyperedge prediction task, finding competitive predictive performance benchmarking against neural network methods despite the extremely low-dimensional parameter space of the model. 
We close with discussion and suggestions for model generalizations. 
     
   
\section*{Methods}

\subsection*{\Model (\model)} \label{sec:model}

\begin{figure*}[!ht]
% \captionsetup[subfigure]{justification=Centering}

\begin{subfigure}[t]{0.3\textwidth}
    \includegraphics[width=\textwidth]{fig/toy_1.pdf}
    \caption{Current hypergraph $\at{\cH}{t}$.}
\end{subfigure}\hspace{\fill} % maximize horizontal separation
\begin{subfigure}[t]{0.3\textwidth}
    \includegraphics[width=\textwidth]{fig/toy_2.pdf}
    \caption{\textbf{Edge selection}: An edge $f$ is selected uniformly at random (u.a.r.)\ from $\at{\edgeset}{t}$ and a node $v_0$ is selected u.a.r.\ from $f$ to seed the new edge $\at{e}{t+1}$.}
\end{subfigure}\hspace{\fill}
\begin{subfigure}[t]{0.3\textwidth}
    \includegraphics[width=\textwidth]{fig/toy_3.pdf}
    \caption{\textbf{Edge sampling}: The remaining nodes $v \in f \setminus \braces{v_0}$ are each selected i.i.d.\ with probability $\eta$ for inclusion in $\at{e}{t+1}$.}
\end{subfigure}

\bigskip % more vertical separation
\begin{subfigure}[t]{0.3\textwidth}
    \includegraphics[width=\textwidth]{fig/toy_4.pdf}
    \caption{\textbf{Extant node addition}: A number $g \sim \extantnodedist$ is sampled. Then, a set of $g$ distinct nodes is sampled u.a.r.\ from $\at{\nodeset}{t} \setminus f$ and added to $\at{e}{t+1}$.}
\end{subfigure}\hspace{\fill} % maximize horizontal separation
\begin{subfigure}[t]{0.3\textwidth}
    \includegraphics[width=\textwidth]{fig/toy_5.pdf}
    \caption{\textbf{Novel node addition}: A number $b \sim \novelnodedist$ is sampled. Then, a set of $b$ distinct nodes is created and added to both $\at{\nodeset}{t+1}$ and $\at{e}{t+1}$.}
\end{subfigure}\hspace{\fill}
\begin{subfigure}[t]{0.3\textwidth}
    \includegraphics[width=\textwidth]{fig/toy_6.pdf}
    \caption{Edge $\at{e}{t+1}$ is added to $\edgeset^{(t+1)}$.}
\end{subfigure}

\caption{Schematic illustration of the edge generation process for our \Model (\model).}
\label{fig:toy-model}
\end{figure*}

Our proposed model generates a sequence of growing hypergraphs. 
At each discrete timestep $t$, let $\at{\hypergraph}{t} = (\at{\nodeset}{t}, \at{\edgeset}{t})$ be a hypergraph with node set $\at{\nodeset}{t}$ and edge set $\at{\edgeset}{t}$.\footnote{We note that the discrete timestep here is merely a notational convenience for adding edges and could alternatively be interpreted as indexing the different times at which new edges are added.} 
Each hyperedge $e \in \at{\edgeset}{t}$ is a named set of nodes $e \subseteq \at{\nodeset}{t}$.
Multiedges --- in which two distinctly named edges are equal as sets --- are permitted. 
The degree of node $v$ in hypergraph $\at{\hypergraph}{t}$ is $\at{d}{t}_v = \sum_{e \in \at{\edgeset}{t}} \one{v \in e}$, the number of edges containing $v$ in $\at{\edgeset}{t}$.


We consider only a single method of updating the hypergraph from time $t$ to time $t+1$, which consists of the following four substeps: 
\begin{enumerate}
    \item \textbf{Edge selection}: a single edge $f$ is selected uniformly at random (u.a.r.) from  $\at{\edgeset}{t}$.
    Then, one node $v_0$ is selected u.a.r.\ from $f$ to seed the new edge $\at{e}{t+1}$.
    \item \textbf{Edge sampling}: the remaining nodes $v \in f \setminus \braces{v_0}$ are each included in $\at{e}{t+1}$ independently with probability $\eta \in [0,1]$.
    \item \textbf{Extant node addition}: a nonnegative integer $g$ is sampled from a distribution which we express as a probability vector $\extantnodedist$. 
    We require that $\extantnodedist$ be an element of $\simplex^{\bar{k}}$, where $\simplex^{\bar{k}}$ is the set of probability vectors indexed $0$ through $\bar{k}$.
    The choice of $\bar{k}$ gives the largest possible number of extant nodes which can be added in a single update step.\footnote{It is also possible to use a parametric model (e.g.\ a Poisson distribution) to determine the number of extant nodes to add. 
    This would reduce the number of parameters at the cost of making the model somewhat less flexible.}  
    With a slight abuse of notation, we write $g\sim \extantnodedist$ to denote this sampling process.  
    Then, $g$ distinct nodes are selected u.a.r.\ from $\at{\nodeset}{t}$ and added to $\at{e}{t+1}$.
    \item \textbf{Novel node addition}: a nonnegative integer $b \sim \novelnodedist$ is sampled, where again $\novelnodedist$ is a probability vector, and $b$ distinct nodes are created and added to $\at{e}{t+1}$.
\end{enumerate}
After forming $\at{e}{t+1}$, we define the updated hypergraph $\at{\hypergraph}{t+1} = (\at{\nodeset}{t} \cup \braces{\at{e}{t+1}}, \at{\edgeset}{t} \cup {\at{e}{t+1}})$. 
\Cref{alg:model} gives a formalized summary of the model, and \Cref{fig:toy-model} gives a schematic graphical illustration. 
The parameters of the model are the copy probability $\eta$, the extant node distribution $\extantnodedist$, and the novel node distribution $\novelnodedist$. 
For notational compactness, we will use the vector $\params = (\eta, \extantnodedist, \novelnodedist)$ to refer to the concatenation of these three parameters. 
The total number of scalar parameters of the model is $2\bar{k} + 1$. 


% Note that we have three different sources of nodes contributing to the new edge $\tilde{e}$: the nodes kept from $f$, the nodes not in $f$ collected from the rest of the hypergraph, and the nodes newly added to the hypergraph. 

% Note also in the above description of the $\eta$ probability that we first select a single node from $\hat{e}$ to seed the new edge $\tilde{e}$, and then each of the other nodes in $\hat{e}$ are copied to $\tilde{e}$ at random, IID with probability $\eta$. This seeding by a single randomly selected node ensures that the added edge $\tilde{e}$ is non-empty. 

% Our resulting growing hypergraph model, which we call the \Model (\model) model, results from iterative application of the random single-time update step $H(H^{(t)}, \eta, \extantnodedist, \novelnodedist)$ specified in \Cref{alg:model}, which is guaranteed to add a non-empty edge at each timestep. \pc{different name for the update, various figure revisions, use $\theta$ to absorb all the parameters when conveniently possible.}

\begin{algorithm}[!ht]
    \caption{\Model (\model) update step}\label{alg:model}
    \begin{algorithmic}
    \Require $\at{\hypergraph}{t} = \paren{\at{\nodeset}{t}, \at{\edgeset}{t}}$, $\eta \in [0,1]$, $\extantnodedist \in \simplex^{\bar{k}}$, $\novelnodedist \in \simplex^{\bar{k}}$
    \State $e \gets \emptyset$
    \State Sample $f \sim \mathrm{Uniform}\paren{\at{\edgeset}{t}}$ \Comment{Edge selection}
    \State Sample $v_0 \sim \mathrm{Uniform}\paren{f}$ \Comment{Edge sampling}
    \State $\at{e}{t+1} \gets e\cup \braces{v_0}$ 
    \For{node $v \in f\setminus v_0$}
    \State With probability $\eta$, $\at{e}{t+1} \gets \at{e}{t+1} \cup \braces{v}$   
    \EndFor
    \State Sample $g \sim \extantnodedist$ \Comment{Extant node addition}
    %\State Sample $V \sim \mathrm{Uniform}\paren{{\at{\nodeset}{t}\setminus f}\atop{g}}$ 
    \State Sample $V \sim \mathrm{Uniform}\paren{\genfrac{}{}{0pt}{2}{\at{\nodeset}{t}\setminus f}{g}}$ 
    \State $\at{e}{t+1} \gets \at{e}{t+1} \cup V$
    \State Sample $b \sim \novelnodedist$  \Comment{Novel node addition}
    \State $n \gets \abs{\at{\nodeset}{t}}$
    \State $\nodeset^{(t+1)} \gets \at{\nodeset}{t} \cup \braces{v_{n+1}, \ldots, v_{n+b}}$ 
    \State $\at{e}{t+1} \gets \at{e}{t+1} \cup \braces{v_{n+1}, \ldots, v_{n+b}}$
    \State $\edgeset^{(t+1)} \gets \at{\edgeset}{t} \cup \at{e}{t+1}$
    \State \Return{$\at{\hypergraph}{t+1}=\paren{\at{\nodeset}{t+1}, \at{\edgeset}{t+1}}$}
    \end{algorithmic}
\end{algorithm}



\subsection*{Asymptotic degree and edge-size distributions} 



We now derive several asymptotic properties of our proposed \model. 
We first derive the asymptotic mean edge size, as well as a linear system describing the complete edge size distribution.  
Let $\mean{k}$ be the mean edge size in $\at{\hypergraph}{t}$.
We will compute this mean under an assumption of stationarity.
Each time an edge is constructed, there are in expectation $1 + \eta(\mean{k}-1)$ nodes sampled in the edge sampling step, $\mu_{\extantnodedist}$ nodes sampled in the extant node addition step, and $\mu_{\novelnodedist}$ nodes sampled in the novel node addition step, where $\mu_{\cdot}$ denotes the mean of the corresponding distribution. 
At stationarity, we therefore have the self-consistent equation 
\begin{align}
    \mean{k} = 1 + \eta(\mean{k}-1) + \mu_{\extantnodedist} + \mu_{\novelnodedist}\;, \label{eq:mean-edge-size-eq}
\end{align}
from which it follows that, provided $\eta < 1$, 
\begin{align}
    \mean{k} = \frac{1 - \eta + \mu_{\extantnodedist} + \mu_{\novelnodedist}}{1-\eta}\;. \label{eq:mean-edge-size}
\end{align}
When $\eta = 1$, the edge size is nonstationary and grows arbitrarily large, as reflected in the divergence of \cref{eq:mean-edge-size}. 

To describe the edge size distribution, let $\mathbf{W} \in \R^{\bar{k}\times \bar{k}}$ be the matrix whose entry $w_{ij}$ gives the probability that the produced edge $\at{e}{t+1}$ has size $i$ given that the sampled edge $f$ has size $j$. 
As we show in \Cref{sec:edge-sizes}, the entries of $\mathbf{W}$ have closed-form expressions in terms of the parameter vector $\params$: 
\begin{align}
    w_{ij} = \sum_{\ell = 0}^{j-1} \sum_{h = 0}^{i - \ell} \binom{j-1}{\ell} \eta^{\ell} (1-\eta)^{j - \ell - 1} \extantnodepar_h \novelnodepar_{i - \ell - h}\;. \label{eq:edge-size-distribution-matrix}
\end{align}
The stationary distribution of edge sizes under our model is then given by the Perron eigenvector of the matrix $\mathbf{W}$. 
We give several examples of computing the stationary distribution of edge sizes for synthetic and empirical data sets using \cref{eq:edge-size-distribution-matrix} in \Cref{fig:degree-edge-size}. 


Turning now to the degree distribution, we first calculate the mean degree $\mean{d}$. 
In any hypergraph of $n$ nodes and $m$ edges, it holds that  ${n}\mean{d} = {m \mean{k}}$. 
At large $t$, in expectation $m/n \approx {1}/{\mu_{\novelnodedist}}$, since on average there are $\mu_{\novelnodedist}$ nodes added per new edge. 
Provided that $\eta <1$ (so that $\mean{k}$ is finite), we therefore have 
\begin{align}
    \mean{d} = \frac{\mean{k}}{\mu_{\novelnodedist}} = \frac{1 - \eta + \mu_{\extantnodedist} + \mu_{\novelnodedist}}{\mu_{\novelnodedist}(1-\eta)}\;. \label{eq:mean-degree}
\end{align}

Furthermore, the tails of the degree distribution are power-law with an exponent that depends on the model parameters. 
In \Cref{sec:power-law-tails} we follow a derivation by Mitzenmacher \cite{mitzenmacherBriefHistoryGenerative2004} to argue that, for large $d$, the proportion $p_d$ of nodes with degree $d$ is approximated, for large $d$, by the power law $p_d \propto d^{-\zeta}$, where the exponent $\zeta$ is given by 
\begin{align}
    \zeta = 1 + \frac{1 - \eta + \mu_{\extantnodedist} + \mu_{\novelnodedist}}{1 - \eta(1 - \mu_{\extantnodedist} - \mu_{\novelnodedist})}\;. \label{eq:power-law-exponent}
\end{align}
An intuitive explanation for the occurrence of a power law in this context is that the edge selection and sampling steps of the \model choose nodes in proportion to the number of edges in which those nodes are present; i.e.\ their degree. 
Degree-proportional sampling is a classical mechanism for generating power-law degree distributions \cite{barabasiEmergenceScalingRandom1999,kleinbergWebGraphMeasurements1999}. 
We show examples of the power law exponent predicted by our model in comparison to synthetic and empirical data sets in  \Cref{fig:degree-edge-size}. 

 
\subsection*{Asymptotic edge intersection sizes}

It is possible to compute the exact asymptotic structure of the densities of pairwise intersections in our proposed model. 
We express this structure in terms of the joint distribution 
\begin{align}
    r_{ijk} \eqdef \rho \paren{\abs{e} = i, \abs{f} = j, \abs{e\cap f} = k \given | e\succ f}\;, 
\end{align}
where $e\succ f$ indicates that edge $e$ appears later than edge $f$.
We present a probabilistic argument in \Cref{sec:intersection-sizes} for a claim describing the asymptotic structure of the intersection sizes in our model.
Under this claim, if $\beta_0 < 1$, there exist constants $q_{ijk}$ such that, as the number of edges $m$ grows large: 
% that can be computed as the entries of the unique largest eigenvector of a linear operator $\mathbf{A}$ determined by the parameters $\eta$, $\extantnodedist$, and $\novelnodedist$ such that: 
\begin{itemize}
    \item The intersection sizes $r_{ijk}$ are related to the constants $q_{ijk}$ as 
    \begin{align}
        r_{ijk} = \begin{cases}
            q_{ijk} + O \paren{m^{-1}} &\quad k = 0 \\
            m^{-1}q_{ijk} + O\paren{m^{-2}} &\quad \text{otherwise\;.}
        \end{cases} \label{eq:intersection-asymptotics}
    \end{align}
    \item The constants $q_{ijk}$ can be computed as the entries of the unique nonnegative eigenvector of a matrix $\mathbf{C}$ determined by the parameters $\eta$, $\extantnodedist$, and $\novelnodedist$. 
\end{itemize}
These asymptotics are illustrated in \Cref{fig:intersection-asymptotics} on a large synthetic hypergraph simulated according to the \model{}. 
We show the proportion $r_{k} = \sum_{ij}r_{ijk}$ of pairs of edges of any size intersecting on a set of size $k$ over time (left) and at the final timestep (right), and compare these to the asymptotic predictions of \cref{eq:intersection-asymptotics}.
The predictions agree closely with simulated values, matching both the $m^{-1}$ scaling and the predicted intercepts. 

\begin{figure}[!ht]
    \centering \includegraphics[width=1\linewidth]{fig/intersection-asymptotics-2-panel.png}
    \caption{Illustration of the edge intersection asymptotics given by \cref{eq:intersection-asymptotics}. 
    We simulated an \model for $10^6$ timesteps with parameters $\eta = 0.3$, $\vbeta$ uniform on $\braces{1,2}$, and $\vgamma$ uniform on $\braces{0,1}$.
    (Left): Scaling of intersection size densities $r_k$ as a function of timestep $t$. 
    Dotted lines give empirical intersection rates. 
    Solid lines for $k \geq 1$ give the predicted scaling $m^{-1} \sum_{ij}q_{ijk}$, with $q_{ijk}$ computed as the entries of the leading eigenvector for a matrix $\mathbf{C}$ determined by the model parameters.
    Note the log-log axes. 
    (Right): Proportion $r_k$ of pairs of edges intersecting on a set of size $k$ at the end of our simulation after $10^6$ timesteps, compared with predictions obtained from \cref{eq:intersection-asymptotics}. 
    } 
    \label{fig:intersection-asymptotics}
\end{figure}


\subsection*{Other asymptotic properties}

Our \model displays several other asymptotic structural properties which differ simpler models of evolving hypergraphs (\Cref{fig:four-properties}). 
We fit the \model to the \data{email-enron} dataset using the stochastic expectation-maximization (SEM) algorithm described in the following section. 
We simulated a synthetic hypergraph generated according to the model. 
We also simulated two other synthetic hypergraphs: one generated according to a temporal Erd\H{o}s-R\'enyi-type model (ER) that replicates the edge-size distribution of the fitted \model, as well as a preferential attachment-type model (PA) which also replicates the power law degree distribution.
Details on these two alternative models can be found in \cref{sec:alternative-models}.  
We measured four structural properties of the empirical \data{email-enron} hypergraph and the three synthetic hypergraphs: uniform degree assortativity \cite{chodrowConfigurationModelsRandom2020}, clustering coefficient \cite{gallagherClusteringCoefficientsProtein2013}, edit simpliciality \cite{landrySimplicialityHigherorderNetworks2024}, and edge intersections \cite{landrySimplicialityHigherorderNetworks2024} (values shown in \autoref{fig:four-properties}). 
% The degree assortativity for hypergraphs represents a generalization of the standard Spearman rank assortativity coefficient (detailed explanation is provided in Chodrow et al.~\cite{chodrowConfigurationModelsRandom2020}). The clustering coefficient is defined as the clustering coefficient of the unweighted pairwise projection of the hypergraph, as outlined in Gallagher et al.~\cite{gallagherClusteringCoefficientsProtein2013}. The edit simpliciality refers to the minimal number of additional edges required to transform a hypergraph into a simplicial complex, and this concept is elaborated in detail in Landry et al.~\cite{landrySimplicialityHigherorderNetworks2024}. 
% We then grow a selection of hypergraphs with varying parameter values and observe the discrepancies between them. 

In the case of \data{email-enron}, we observe that the behavior of the \model is quite distinct from that of the ER and PA models. 
The \model replicates the degree assortativity less well than either the ER or PA models and replicates the clustering coefficient roughly as well as the ER model. 
For edit simpliciality and intersection sizes, the \model does not quantitatively capture the correct measurement, but does qualitatively express that these quantities are not asymptotically vanishing, in contrast to the predictions by both the ER and PA models. 
We show the same experiment on several other datasets in  \Cref{fig:additionalM4}. 




\begin{figure*}[!ht]
    \centering
    \includegraphics[width=1\linewidth]{fig/FigM4_email-enron.pdf} 
    \caption{
    Structural properties of the \data{email-enron} temporal hypergraph and three synthetic models. We note that assortativity is highly dataset-dependent, and the behavior of different models changes drastically with different datasets. Specifically, for edge intersection size, we observe that only \model has demonstrated non-diminishing intersection sizes over time steps, as shown in the same experiment on several other example datasets in \Cref{fig:additionalM4}.
        }
    \label{fig:four-properties}
\end{figure*}






% \subsection*{Intersection Decay}



% We now formalize the idea that some hypergraph models produce large intersections while others produce small ones. 
% Let $\braces{\at{H}{t}}_{t = 1}^\infty$ be a sequence of hypergraphs. 
% For each $t$, let edges $\at{E}{t}$ and $\at{F}{t}$ be uniformly sampled without replacement from $\at{H}{t}$. 
% Let $\at{K}{t} = \abs{\at{E}{t}\cap \at{F}{t}}$. 
% Let $g:\cH \mapsto \R$ be a function with the property that $g(\at{H}{t})\rightarrow 0$ almost surely as $t$ grows large.  
% We say that the sequence $\braces{\at{H}{t}}_{t = 1}^\infty$ has \emph{vanishing $k$-intersections with rate $g$} if 
% \begin{align}
%     \rho\left(\at{K}{t} = k\right) \in \Theta(g(\at{H}{t}))
% \end{align}
% with high probability as $t$ grows large. 

% \begin{thm}\label{thm:large-intersections}
%     Provided that $\eta > 0$, our proposed model has vanishing $k$-intersections with rate $g(H) = n^{-1}$ for any $k$. 
% \end{thm}

% Consider a temporal Erd\"os-R\'enyi model in a new edge is formed by selecting $\mathrm{Poisson}(a) + 1$ nodes uniformly at random from $H$, and combining these with $\mathrm{Poisson}(b)$ novel nodes which are added to $H$. 
% \begin{thm}\label{thm:small-intersections}
%     Assume that $b > 0$. Then, temporal Erd\"os-R\'enyi has vanishing $k$-intersections with rate $g(H)  = n^{-k}$ for any $k$. 
% \end{thm}










\subsection*{Inference via Stochastic Expectation Maximization (SEM)} \label{sec:inference}

We use maximum-likelihood estimation to learn the parameters $\noderetentionprob$, $\extantnodedist$, and $\novelnodedist$ of our proposed \model from a dataset containing time-stamped hyperedges. 
The \model has the structure of a latent-variable model: the edge $\at{e}{t+1}$ that was added in timestep $t+1$ is observed, but the edge $f$ that was sampled in the edge-selection step to generate $\at{e}{t+1}$ is not. 
This structure lends itself naturally to optimization via the expectation-maximization (EM) algorithm \cite{dempster1977maximum}.
%Consider an edge $\at{e}{t+1}$ added in timestep $t+1$. 
Let $\at{F}{t+1}$ be the true but unobserved edge that was sampled in the edge-selection step. %to form $\at{e}{t+1}$.
Given the observation of $\at{e}{t+1}$ and some current estimate $\params$ of the parameters, the probability that the true $\at{F}{t+1}$ was edge $f$ is 
\begin{align*}
    \prob{\at{F}{t+1} = f \given \at{e}{t+1} ; \params} &= \frac{\prob{\at{e}{t+1}, \at{F}{t+1} = f; \params}}{\prob{\at{e}{t+1}; \params}} \\ 
    &= \frac{\prob{\at{e}{t+1} , \at{F}{t+1} = f; \params}}{\sum_{f'}\prob{\at{e}{t+1} ,  \at{F}{t+1} = f'; \params}}\; \\ 
    &= \frac{\prob{\at{e}{t+1} \given \at{F}{t+1} = f; \params}}{\sum_{f'}\prob{\at{e}{t+1} \given  \at{F}{t+1} = f'; \params}}\;,
\end{align*}
where the final line follows because the selection of $\at{F}{t+1} = f$ in the HCM is uniform over the edge set $\at{\edgeset}{t}$.
The probabilities appearing in this final expression are determined by \Cref{alg:model} and can be computed in closed form. 
The general EM algorithm applied to the single observation of $\at{e}{t+1}$ proceeds by maximizing the expectation of the log-likelihood with respect to a new parameter estimate, with the expectation taken with respect to $\prob{\at{F}{t+1} = f \given \at{e}{t+1} ; \params}$, which we now abbreviate $\at{p}{t+1}(f; \params)$ for notational compactness. 
In our case, this maximization problem has a closed-form solution in terms of the vector $\mathbf{s}$ of expected sufficient statistics of the distributions involved in sampling. 
This vector has components 
\begin{align}
    s_1        &= \sum_{f}\at{p}{t+1}(f; \params) \abs{f\cap \at{e}{t+1}}\;, \label{eq:expected-ss-1}\\ 
    s_2        &= \sum_{f}\at{p}{t+1}(f; \params) \abs{f\setminus \at{e}{t+1}}\;, \label{eq:expected-ss-2}\\ 
    s_{3,\ell} &= \sum_{f}\at{p}{t+1}(f; \params) \one{\abs{\at{e}{t+1} \setminus \at{\nodeset}{t}} = \ell}\;. \label{eq:expected-ss-3}\\ 
    s_{4,\ell} &= \sum_{f}\at{p}{t+1}(f; \params) \one{\abs{\paren{\at{e}{t+1} \setminus f} \cap \at{\nodeset}{t}} = \ell} \;. \label{eq:expected-ss-4}
\end{align}
Once $\mathbf{s}$ is computed, the maximum likelihood estimates for the parameters are 
\begin{align}
    \hat{\noderetentionprob} = \frac{s_1-1}{s_1 + s_2-1}\;, \quad 
    \hat{\extantnodepar}_\ell = s_{3,\ell}\;, \quad 
    \hat{\novelnodepar}_\ell = s_{4,\ell}\;. \label{eq:m-step}
\end{align}
Importantly, we are guaranteed that $s_1\geq 1$  by the requirement of \Cref{alg:model} that $\abs{f\cap \at{e}{t+1}}\geq 1$ for all $f$ with $\at{p}{t+1}(f; \params)>0$.

In standard, full-batch EM, we would compute averages of the expected sufficient statistics across the entire sequence of new edges, and then use \cref{eq:m-step} to form new parameter updates. 
We would then repeat this process until convergence to a local maximum of the likelihood function, which is guaranteed by standard theory. 
For our setting, however, the full-batch EM algorithm is infeasible due to the computational cost of forming the distributions $\at{p}{t+1}(f; \params)$, 
requiring summation over all $t$ edges that arrived prior to time $t+1$. 
This sum thus includes order $m^2$ terms for \emph{each} timestep, where $m$ is the number of edges in the hypergraph, which  
is computationally prohibitive for hypergraphs with $m\gtrsim 10^4$ on modern personal computers.

We therefore instead consider a stochastic variant of EM \cite{cappeOnLineExpectationMaximization2009}. 
Starting from an arbitrary initial vector $\hat{\mathbf{s}}(0)$ of estimates of the expected sufficient statistics, we update with an exponentially moving average. 
In each algorithmic step $\tau$ of stochastic EM, we sample an edge $e$ uniformly at random. 
We then construct the vector $\mathbf{s}({\tau})$ of expected sufficient statistics from the observation of according to \cref{eq:expected-ss-1,eq:expected-ss-2,eq:expected-ss-3,eq:expected-ss-4}. 
Next, we update $\hat{\mathbf{s}}(\tau)$ according to
\begin{align}
    \hat{\mathbf{s}}(\tau+1) = \paren{1-\rho(\tau)}\hat{\mathbf{s}}(\tau) + \rho(\tau) \mathbf{s}(\tau). 
\end{align}
Here, $\rho(\tau)$ is a learning schedule that determines the rate of update in the estimate of the expected sufficient statistics.
The running estimate $\hat{\params}(\tau)$ of the parameter vector $\params$ is obtained from \cref{eq:m-step}, using $\mathbf{\hat{s}}(\tau)$ in place of $\mathbf{s}$. 
We use a learning schedule $\rho(\tau) = \tau^{-1}$ and terminate the algorithm when the relative change in the estimate of $\eta$ between the most recent 100-step windows falls below $10^{-2}$.






\section*{Results} \label{sec:results}

\subsection*{SEM-\model on empirical data}
We used SEM to fit our \model to 27 empirical hypergraphs provided by the XGI package for Python \cite{landryXGIPythonPackage2023}. 
Clock-times to convergence ranged from 1.6 seconds in the case of the \data{diseasome} dataset (516 nodes and 903 edges) to $2.9\times 10^4$ seconds (8 hours) for the \data{threads-stack-overflow} dataset ($2.7\times 10^6$ nodes and $1.1 \times 10^7$ edges). 
We give convergence times and more detailed descriptions of learned parameters in \Cref{tab:parameter-values}.  

\begin{figure*}%[h]
    \centering
    \includegraphics[scale=0.55]{fig/PARAMETERS.pdf}
    \caption{
    (Best viewed in color). Visual summary of parameters obtained by SEM fits of our \model to empirical hypergraphs. 
    We show $\eta$ and the expectations $\mu_{\vbeta} = \sum_{i} i\beta_i$ and $\mu_{\vgamma} = \sum_i i \gamma_i$.
    We use linear scale for $\eta$ and log scale for $\mu_{\vbeta}$ and $\mu_{\vgamma}$. 
    When fitting the model using SEM, The default length $\bar{k}$ of $\vbeta$ and $\vgamma$ is set to be equal to the largest edge size in the corresponding empirical hypergraphs. 
    Coauthorship datasets are shown in orange; biological datasets in pink; information datasets in green, and social interaction datasets in blue. 
    }
    \label{fig:parameters}
\end{figure*}

\Cref{fig:parameters} summarizes the parameters obtained by SEM fits of our \model to empirical hypergraphs.
We group the datasets into four broad categories: coauthorship, biological, information, and social interaction. 
The social interaction networks in our dataset have very high rates $\eta$ of edge-copying, along with relatively low rates of extant or novel node addition. 
Coauthorship networks tend to display lower rates of edge-copying and higher rates of extant and novel node addition. 
Biological and information networks display less clear patterns, with different networks in these categories displaying very different estimated parameter values. 
On the whole, most datasets exhibit seemingly small values of $\mu_{\extantnodedist}$, consistently below one, indicating low rates of adding extant nodes elsewhere in the hypergraph to the edge being copied. The \data{kaggle} graph, however, stands out as an outlier among the larger hypergraphs, introducing a substantial number of new nodes at each timestep.   



\begin{figure*}[!ht]
    \centering
    \includegraphics[width=1\linewidth]{fig/degree-and-edge-sizes.png}
    \caption{Degree distributions and edge-size distributions for one synthetic HCM with $10^6$ edges and three empirical datasets: \data{ndc-classes}, \data{email-enron}, and \data{tags-ask-ubuntu}.
    In the top row, dashed lines indicate the exponent of the power-law describing the asymptotic degree distribution using parameters inferred via SEM and \cref{eq:power-law-exponent}. 
    %The horizontal and vertical axes are logarithmic, and the limits of both axes vary by dataset.
    In the bottom row, the dashed curve gives the modeled asymptotic edge-size distribution using inferred parameters to compute the matrix $\mathbf{W}$ described by \cref{eq:edge-size-distribution-matrix} and its associated Perron eigenvector. 
    %The vertical axis is logarithmic, and the limits of both axes vary by dataset. 
    For the synthetic hypergraph (first column), the true parameters are used to construct the approximations and no inference is performed.
    The \data{ndc-classes} and \data{email-enron} edge-size distributions have been truncated for visualization purposes. 
    }
    \label{fig:degree-edge-size}
\end{figure*}


As one form of validation, we compare the actual degree and edge-size distributions of several datasets to the asymptotic descriptions provided by \cref{eq:edge-size-distribution-matrix,eq:power-law-exponent}, using the parameters obtained by SEM.
We show this comparison in \Cref{fig:degree-edge-size}. 
We show the slope corresponding to the power-law exponent for the degree-distribution of the \model as well as the complete modeled distribution of edge sizes.  
We find rough qualitative agreement in both cases, despite the small number of parameters that the \model uses to describe each dataset. 
This appears true despite the considerable variety of network structures shown. 
Different empirical hypergraphs may exhibit quite different edge-size distributions: some, like \data{congress-bills}, have a small number of nodes but very large edges (see \autoref{tab:lp_results} below for sizes of each dataset), while others, like \data{tags-ask-ubuntu}, have much larger numbers of nodes but much smaller edges. 
The parameters from the SEM-\model fit approximate the degree and edge-size distributions for these very different empirical hypergraphs. 



% Additionally, we observe that both the degree distribution and edge-size distribution of empirical hypergraphs closely align with those predicted using the parameters obtained by SEM. 
% For example, while we observe fluctuations in these distributions in \autoref{fig:degree-edge-size}, overall the slopes of the degree distributions and the shapes of the edge-size distributions predicted from the SEM-obtained parameters remain relatively close to the actual distributions. 


% The SEM implemented here is efficient and can process large hypergraphs like \data{threads-stack-overflow}, which has over $2$ million nodes and $11$ million edges, showing its scalability for studying larger hypergraphs. 
% This efficiency allows us to examine larger datasets, such as those representing online threads with millions of hyperedges. In these cases, we find that smaller $\eta$ coincides with larger $\mu_{\novelnodedist}$, as seen in \autoref{fig:Parameters}. We also note that most datasets exhibit seemingly small values of $\mu_{\extantnodedist}$, consistently below one, indicating low rates of adding extant nodes elsewhere in the hypergraph to the edge being copied. However, the \data{kaggle} graph stands out as an outlier among the larger hypergraphs, introducing a substantial number of new nodes at each timestep.   


%Another important property for a realistic hypergraph model is the edge intersection size distribution. This property has been extensively discussed in state-of-the-art literature concerning hypergraph motifs \cite{bensonSequencesSets2018, leeHowHyperedgesOverlap2021, chodrowConfigurationModelsRandom2020}. The authors of \cite{leeHowHyperedgesOverlap2021} emphasized the wide range of intersection sizes in empirical hypergraphs, while \cite{chodrowConfigurationModelsRandom2020} demonstrated that intersection sizes tend to be much higher in empirical hypergraphs compared to simple null models.

% These distinctive characteristics in empirical networks contribute to their unique positions and types, underscoring the diversity in their structural properties.




\subsection*{Link prediction on empirical networks}

\begin{table*}
    \centering
    % \resizebox{\textwidth}{!}{%
    \input{fig/prediction-table.tex}
    % }
    
    \caption{
        Link prediction on the empirical hypergraphs provided by the XGI package for Python \cite{landryXGIPythonPackage2023}. 
        The number of nodes $n$, number of edges $m$, and maximum edge size $\bar{k}$ are shown for each dat set. 
        We compute area under the receiver operating characteristic (AUC) and recall scores for models trained on timestamped sequences (t) and random sequences of edges, using $20\%$ of the total data in both cases. 
        ``NA'' indicates that timestamps were not supplied for the dataset, making it impossible to perform link prediction under the (t) condition. 
        AUC and recall scores are computed using either the remaining $80\%$ of data or $10^5$ edges sampled uniformly at random (in the case of large datasets) as positive examples, then generating an equal number of negative examples, and forming predictions on the examples as described in the main text. 
        We did not obtain link prediction results for two of the datasets due to computational limitations in the evaluation of the marginal likelihood. 
        We set the length of $\extantnodedist$ and $\novelnodedist$ to be equal to the largest edge size for each empirical hypergraph for all runs.
        }
    \label{tab:lp_results}
\end{table*}
    
At any given time, the \model assigns a probability that any given candidate edge will indeed be formed in the next timestep. 
We can therefore perform link prediction using the \model by predicting that high-probability candidates will be formed and that low-probability candidates will not.

A challenge in most link prediction contexts is the absence of true negative samples for training and evaluation. 
Because our model ``training'' is simply the SEM algorithm described above, we do not need negative samples for training; we do, however, still need them for evaluation. 
We therefore form a collection of negative examples by sampling non-realized edges in order to match the degree distribution and edge-size distribution of each empirical network. 
To do this, we draw the edge size for each created negative edge set of nodes according to the same empirical edge-size distribution of the (positive) edges while sampling nodes to be included in the negative set based on the node degree distribution. 
To maintain balanced classes, we generate as many negative examples as there are positive examples. 

In each dataset, we use $20\%$ of the observed (positive) edges to perform SEM. 
We fit our model using two distinct types of training data. 
When timestamped data is available, we take the \emph{first} $20\%$ of edges as training data. 
However, for each dataset, we also fit on a uniformly random subset of $20\%$ of the training data, which is not temporally contiguous. 
We use this strategy in order to assess the effectiveness of our model, which has explicit temporal structure, for settings in which no empirical timestamps are available. 
To assess the models trained under each condition, we combine the positive and negative examples and compute the probability of each candidate being realized under the \model. 
We consider a positive prediction to be a candidate with modeled probability above the median. 
From the model's predictions, we compute area under the receiver-operating characteristic curve (AUC) and recall scores. 
These scores are shown in \Cref{tab:lp_results}, with the (t) columns indicating the use of temporal information for model fitting. 

We use a maximum of $10^5$ edges as positive examples for evaluation; for temporally trained models we use edges immediately following the training set, while in non-temporally trained models we use up to $10^5$ edges sampled uniformly at random. 
There were two extremely dense datasets (\data{dawn} and \data{tags-stack-overflow}) in which we were able to perform inference via SEM but not able to perform link prediction due to the computational expense of computing the marginal log-likelihood of a candidate edge. 
In such hypergraphs, the number of candidate edges $f$ which could have generated a new edge $e$ is very large, leading to many terms in the marginal log-likelihood which must be computed. 

Perhaps surprisingly considering that our \model relies heavily on temporal information, the link prediction results from random draws of edges ignoring the temporal timestamp information are in many cases as good or better than the predictions obtained from using the temporal ordering supplied with the data. 
Indeed, our model was especially successful on many social interaction datasets for which no timestamps are supplied with the data (bottom rows of \Cref{tab:lp_results}). 
Despite the small number of parameters in the \model, we are able to achieve AUCs over $0.85$ in 14 out of 25 datasets for which we were able to perform link prediction. 



We now compare the predictive performance of our \model to that of two neural network methods: neural network methods, Neural Hypergraph Link Prediction (NHP) \cite{yadatiNHPNeuralHypergraph2020} and the Logical Hyperlink Predictor (LHP) \cite{yangLHPLogicalHypergraph2023}.
The NHP study \cite{yadatiNHPNeuralHypergraph2020} shows results on 5 datasets and the LHP study \cite{yangLHPLogicalHypergraph2023} shows results on 4 datasets. 
We test our \model on the three of these datasets that appear in both papers, replicating the experimental setup from these studies as closely as possible. 
We compute AUC and F1 scores for link prediction, as well as training time, and compare our results to the published results for NHP and LHP.
To ensure comparability, we follow the same training setup: we use a uniformly random $20\%$ of hyperedges for training and use the rest for testing. 
We also follow the same approach for generating negative examples: for each positive hyperedge, we create a corresponding negative sample by randomly replacing half of the nodes in the edge set with edges from the rest of the hypergraph. 
Because these datasets are relatively small, we set both $\extantnodedist$ and $\novelnodedist$ to a maximum length of $\bar{k} = 5$ during training. 
Combined with $\eta$, this gives our model a total of 11 scalar parameters. 
The neural network models are much larger: while we do not have exact published parameter counts, information in the LHP paper gives at least $9.8 \times 10^5$ scalar parameters.

We observe in \Cref{tab:benchmarking} that our \model performs competitively with the two neural network models: on the iJO1366 dataset it substantially outperforms both; on the iAF1260b dataset it performs better than NHP and worse than LHP; and on the USPTO dataset it performs worse than both NHP and LHP. 
We conjecture that the comparatively poor performance of our model reflects in part the small maximum edge size of this data set compared to the other two. 
Although our training times are typically larger than those reported for LHP, we note that LHP was trained on a TITAN RTX GPU while our model was trained on X. H.'s personal laptop, which possesses an 11th Gen Intel i7-11800H processor with 8 cores and 16 logical processors and 16GB of RAM. 



% We observe that, among the three datasets under comparison in \autoref{tab:benchmarking}, LHP outperforms NHP in all cases, while \model attains comparable (but slightly higher) scores to LHP on one, and performs much better on another, but registers lower scores on the third. 
% In comparing these results, we note that NHP and LHP can both be used for directed and undirected hypergraphs, whereas the \model as formulated here only works for undirected hypergraphs. 
% Of possible interest, noting the training time in \autoref{tab:benchmarking}, we observe a reversal in the prediction accuracy and time complexity between LHP and \model: specifically, \model performed worse on the \data{USPTO} dataset but required less training time, whereas LHP was most accurate on \data{USPTO} after taking much longer to train compared to the other datasets. We hypothesize that this discrepancy may be due to this hypergraph having more nodes than edges. Indeed, we notice a tendency for \model to underperform in such cases where edge intersection sizes are relatively small.


%We also note that changing the fraction of edges used for model fitting leads to large variations in the convergence speeds and accuracies observed across the three datasets; indeed, if we allow $80\%$ of the hypergraph to be observed by the model for retrieving the parameters, the AUC scores we obtain using the \model are all above $0.9$. 



\begin{table*}
    \centering

    \resizebox{\textwidth}{!}{%
        \input{fig/link-prediction-results.tex}
        }

    \caption{
        Link prediction results of our \model against two neural network hypergraph link prediction methods, NHP \cite{yadatiNHPNeuralHypergraph2020} and LHP \cite{yangLHPLogicalHypergraph2023}. 
        The iAF1260b and iJO1366 datasets are metabolic reaction hypergraphs while USPTO is an organic reactions hypergraphdataset. 
        Negative edges are generated according to the description and code for LHP. 
        Since none of these datasets are temporal hypergraphs, results are calculated from $20\%/80\%$ training-testing splits over 10 independent randomized trials. 
        We set the lengths of both $\extantnodedist$ and $\novelnodedist$ to be $5$ for all runs. 
        The reported AUC and F1 scores for LHP and NHP are taken from Table 4 of the LHP paper \cite{yangLHPLogicalHypergraph2023}. 
        The original NHP paper presents lower AUC scores compared to those reported in the LHP paper, possibly due to randomization; the higher of the two reported scores were used here. 
        The times to train the models (``T'') are also taken from the LHP paper (Table 7), which used an NVIDIA TITAN RTX GPU. 
        Time for fitting the \model parameters was measured on X. H.'s personal computer, which possesses a a 11th Gen Intel i7-11800H that has 8 cores and 16 logical processors and 16GB of RAM. 
}
\label{tab:benchmarking}
\end{table*}












\section*{Discussion} \label{sec:discussion}

    We proposed the \Model, a simple model of hypergraph evolution based on a noisy edge-copying mechanism. 
    This model is mechanistic, interpretable, and analytically tractable. 
    In addition to several analytic descriptions of the model's behavior, we also provide a scalable stochastic expectation maximization algorithm for fitting the model to empirical data.
    We find in \Cref{tab:benchmarking} that our 11-parameter model is competitive on link prediction tasks with neural network models containing hundreds of thousands of parameters.
    
    One direction of future work concerns modeling of recombination of edges. 
    In our \model, new edges are formed from a noisy copy of a single prior edge. 
    It may be of interest to allow edges to to form from multiple prior edges; such a process might model, for example, the formation of broad collaborations from multiple research groups. 
    A version of this idea is explored in the context of sequences of sets by   Benson et al. \cite{bensonSequencesSets2018}. 
    Incorporating such a mechanism into the \model would significantly complicate the inference problem, since the set of possible generators of each edge would grow large. 
    Fitting such a model to data might require more sophisticated computational techniques that could be of independent interest. 
    Another direction of future work involves the incorporation of hypergraph metadata. 
    In a hypergraph with node attributes, for example, one might model an edge sampling step in which the node $v_0$ sampled from the selected edge $e$ acts as a ``leader'' in the next edge; other edges in $e$ could be more likely to be copied into the next edge if they share attributes with $v_0$.
    
    
    
    It is often claimed that the widespread availability of large data sets and computational power sufficient to train deep models has made theory and simple models obsolete in predictive tasks \cite{anderson2008end}.
    We take our results to suggest a continuing role for simple, interpretable stochastic models in the study of complex systems, even in the age of deep learning. 



