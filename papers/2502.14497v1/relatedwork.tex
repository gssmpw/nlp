\section{Literature Review}
\vspace{-0.1cm}

\subsection{Text-based Forecasting}

Text-based forecasting explores the predictive power of textual data such as news articles \cite{peng-jiang-2016-leverage, 10.1145/3533018, wang2024newsforecast, rahimikia2024r}, social media \cite{10.1145/2542182.2542190, iso-etal-2016-forecasting}, or corporate reports \cite{kogan-etal-2009-predicting}. By extracting linguistic features such as keywords \cite{iso-etal-2016-forecasting}, sentiment \cite{math10132156}, raw embedding representations \cite{sawhney-etal-2020-deep} or high-density embedding clusters \cite{drinkall-etal-2025-forecasting, drinkall2025financialregression}, researchers have shown that it is possible to enhance traditional forecasting models using text-derived information. The findings of these papers imply there is some causal interaction between textual data and the economy. Yet to our knowledge, no papers have explored the bidirectional nature of this relationship; understanding the effect that markets have on news discourse. 
\vspace{-0.1cm}

\subsection{Diachronic Shifts}
\label{sec:diachronic}

Diachronic analysis \cite{10.1145/3672393} examines how linguistic phenomena change over time. The process begins by aggregating time-stamped information to create time-specific representations. The aggregation process can be done using averaging \cite{10.1007/978-3-030-72610-2_13} or clustering \cite{10.1145/3366424.3382186}, and is considered a design choice dependent on what one is measuring; with averaging suiting cases which aim to detect the dominant shifts. Diachronic shifts tend to be measured using a distance metric between the point-in-time representations. There are several distance metrics that are used in the literature: Cosine Distance (CD) \cite{martinc-etal-2020-leveraging, horn-2021-exploring}, Average Pairwise Distance (APD) \cite{giulianelli-etal-2020-analysing, kudisov-arefyev-2022-black}, Hausdorf Distance (HD) \cite{Wang2020} etc. The standard distance metric is CD -- which we use in this paper -- since APD is more commonly used for polysemy detection \cite{keidar-etal-2022-slangvolution} and HD is sensitive to outliers \cite{Wang2020}. The reciprocal of CD, Cosine Similarity, is widely used in other NLP applications as a robust and efficient similarity metric \cite{thongtan-phienthrakul-2019-sentiment, reimers-gurevych-2019-sentence, yamagiwa-etal-2025-revisiting}. 
\vspace{-0.1cm}

\subsection{Causality}
\label{sec:causality}
% \vspace{-0.1cm}

\subsubsection{Causality in Time-series}
\label{sec:caus_in_TS}

Causality in time-series can be inferred using methods like transfer entropy \cite{barnett2009granger}, convergent cross mapping \cite{ye2015distinguishing} and dynamic Bayesian networks \cite{ghahramani1997learning}; these techniques model non-linear effects but are poor at isolating individual lag contributions. Granger causality \cite{702ab909-8cb1-3c30-a5f1-ab4517d6cf1c} tests whether past values of one variable can help to predict the future value of another, enabling analysis of individual lag contributions. It does not imply “causation” in a philosophical sense but identifies temporal precedence and predictability. If the inclusion of past values of variable $X$ significantly improves the prediction of variable $Y$, then $X$ is said to "Granger cause" $Y$. Non-linear regression models can be used in a Granger causality test to model non-linear relationships \cite{marinazzo2008kernel}.
\vspace{-0.1cm}

\subsubsection{Causality in Text}
\label{sec:cause_in_text}

There are three main research areas that explore "causal" interactions in NLP. Firstly, autoregressive generation which underpins modern generative LLMs is sometimes referred to as "causal language modelling" \cite{Radford2018, Radford2019, Brown2020}, but this area is not related to the identification of causal relationships with exogenous data. Secondly, causal relation extraction \cite{ning-etal-2018-joint, dasgupta-etal-2018-automatic-extraction, Yang2021ASO} identifies causal relationships between clauses. The final causal NLP research area which is of interest to this paper creates a link between text and observable phenomena in the real world. Text-derived features can be used in causal inference where text is the treatment variable - what causes the outcome - for a temporally static outcome. Past work has found success in medical records \cite{Zeng2021UncoveringIP}, politics \cite{fong-grimmer-2016-discovery}, and mental health \cite{zhang2020quantifying}. This research direction is mature and has developed solutions that encode text with topic models \cite{roberts2020adjusting} as well as text embeddings \cite{Veitch2019AdaptingTE}. However, there has been little work exploring the predictive causal relationship between text and time-series information. Existing methods do not link changes in an LLM-derived semantic space with observable time-series, instead focusing on keywords \cite{balashankar-etal-2019-identifying, Maisonnave2022CausalGE}, lexicons, and LDA topics \cite{kang-etal-2017-detecting}, making our framework novel. Further to this, no techniques measure the temporal causality of observable time-series on text.
\vspace{-0.1cm}