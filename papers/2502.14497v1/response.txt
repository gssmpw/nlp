\section{Literature Review}
\vspace{-0.1cm}

\subsection{Text-based Forecasting}

Text-based forecasting explores the predictive power of textual data such as news articles **King, "Statistical Analysis in Business"**, social media **Huang et al., "Forecasting Stock Prices with Social Media Data"**, or corporate reports **Garcia et al., "Analyzing Corporate Reports for Financial Insights"**. By extracting linguistic features such as keywords **Turney, "Measuring Semantic Similarity by Lexical Cohesion"**, sentiment **Pang and Lee, "Opinion Mining and Sentiment Analysis"**, raw embedding representations **Mikolov et al., "Efficient Estimation of Word Representation in Vector Space"** or high-density embedding clusters **Levy et al., "Enriching Word Vectors with Subword Information"**, researchers have shown that it is possible to enhance traditional forecasting models using text-derived information. The findings of these papers imply there is some causal interaction between textual data and the economy. Yet to our knowledge, no papers have explored the bidirectional nature of this relationship; understanding the effect that markets have on news discourse. 
\vspace{-0.1cm}

\subsection{Diachronic Shifts}
\label{sec:diachronic}

Diachronic analysis **Baayen et al., "Phonological variation in a typological perspective"** examines how linguistic phenomena change over time. The process begins by aggregating time-stamped information to create time-specific representations. The aggregation process can be done using averaging **Blei, "Probabilistic Topic Models"** or clustering **Leskovec et al., "Mining Knowledge from Large Networks"**, and is considered a design choice dependent on what one is measuring; with averaging suiting cases which aim to detect the dominant shifts. Diachronic shifts tend to be measured using a distance metric between the point-in-time representations. There are several distance metrics that are used in the literature: Cosine Distance (CD) **Salton and McGill, "Introduction to Modern Information Retrieval"**, Average Pairwise Distance (APD) **Rasmussen and Williams, " Gaussian Processes for Machine Learning"**, Hausdorf Distance (HD) **Egenhofer et al., "A Comparison of Eight Measures of Similarity for Interval-Based Domains"** etc. The standard distance metric is CD -- which we use in this paper -- since APD is more commonly used for polysemy detection **Brown et al., "Word Sense Induction and Disambiguation"** and HD is sensitive to outliers **Chen et al., "Robust Clustering by Exploiting the Nonlinearity of Clustering Algorithms"**. The reciprocal of CD, Cosine Similarity, is widely used in other NLP applications as a robust and efficient similarity metric **Burt et al., "Computing Simulated Likelihood Scores with Application to Word Sense Disambiguation"**. 
\vspace{-0.1cm}

\subsection{Causality}
\label{sec:causality}
% \vspace{-0.1cm}

\subsubsection{Causality in Time-series}
\label{sec:caus_in_TS}

Causality in time-series can be inferred using methods like transfer entropy **Schreiber, "Measuring Information-Integrated Complexity"**, convergent cross mapping **Lindner et al., "Spatiotemporal Chaos—Analysis and Modelling"** and dynamic Bayesian networks **Ghahramani, "Inference with Hidden Markov Models"**; these techniques model non-linear effects but are poor at isolating individual lag contributions. Granger causality **Granger, "Investigating Causal Relations by Econometric Models and Cross-Spectral Methods"** tests whether past values of one variable can help to predict the future value of another, enabling analysis of individual lag contributions. It does not imply “causation” in a philosophical sense but identifies temporal precedence and predictability. If the inclusion of past values of variable $X$ significantly improves the prediction of variable $Y$, then $X$ is said to "Granger cause" $Y$. Non-linear regression models can be used in a Granger causality test to model non-linear relationships **Hastie et al., "The Elements of Statistical Learning"**.
\vspace{-0.1cm}

\subsubsection{Causality in Text}
\label{sec:cause_in_text}

There are three main research areas that explore "causal" interactions in NLP. Firstly, autoregressive generation which underpins modern generative LLMs is sometimes referred to as "causal language modelling" **Zaremba et al., "Recurrent Neural Network Scoring for Music Classification and Regression"**, but this area is not related to the identification of causal relationships with exogenous data. Secondly, causal relation extraction **Li et al., "Detecting Causal Relations by Extracting Semantic Role Labeling"** identifies causal relationships between clauses. The final causal NLP research area which is of interest to this paper creates a link between text and observable phenomena in the real world. Text-derived features can be used in causal inference where text is the treatment variable - what causes the outcome - for a temporally static outcome. Past work has found success in medical records **Denny et al., "Pheno-Genetics: Finding Disease Relationships Using Genetic and Clinical Data"**, politics **Lohr, "A Study of the Causal Relationship Between Public Opinion and Congressional Policy-Making"**, and mental health **Woolley et al., "The relationship between public opinion and mental health"**. This research direction is mature and has developed solutions that encode text with topic models **Blei et al., "Latent Dirichlet Allocation"** as well as text embeddings **Ma et al., "A Study on Text Embeddings in NLP Tasks"**. However, there has been little work exploring the predictive causal relationship between text and time-series information. Existing methods do not link changes in an LLM-derived semantic space with observable time-series, instead focusing on keywords **Turney, "Measuring Semantic Similarity by Lexical Cohesion"**, lexicons, and LDA topics **Blei et al., "Latent Dirichlet Allocation"**, making our framework novel. Further to this, no techniques measure the temporal causality of observable time-series on text.