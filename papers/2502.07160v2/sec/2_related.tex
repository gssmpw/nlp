\section{Related Work}
\label{sec:relate_work}
\subsection{Learned Image Compression} LIC \cite{jpegai, mlic2023, Cheng2020} using neural networks \cite{Hyperprior2018, ma2019image, li2024nas} has shown superior performance over traditional methods like JPEG \cite{JPEG2000}, VVC \cite{vvc}. One most popular LIC frameworks is based on VAE. In the encoder, the input image is encoded into a dense latent feature, which is quantized by rounding operations for efficient transmission. Then the decoder reconstructs the output image based on the dequantized latent feature. At ultra-low bitrates, the information loss caused by the universal rounding quantization is too damaging to be recovered by the decoder, resulting in severe artifacts like blurriness, noises, blocky effects, \textit{etc.}

\subsection{Image Compression by Generative Models}
Generative models such as GAN and diffusion models have been used for image compression in recent years. 

\noindent \textbf{VQ-codebook-based image compression.} 
GAN-assisted VQ-codebook methods have shown advantages over traditional LICs, especially in perceptual quality \cite{WACV2024, vqganmasiwei,muckley2023improving}. A discrete codebook maps the encoded image latent into a transmitted integer indices map instead of the conventional rounding-quantized latent, enabling lower bitrate and greater resilience to network fluctuations. The decoder retrieves the vector-quantized (VQ) latent from the shared codebook according to the indices map and then reconstructs the image. To improve the visual quality and fidelity of the reconstruction from the VQ-based generator, the GAN discriminator is either applied on the pixel level \cite{WACV2024, vqganmasiwei} or further extended to the indices map \cite{muckley2023improving}. However, the VQ-codebook captures general image priors and often deviates from individual image details. As a result, although visually appealing, the reconstructed images usually present significant pixel-level distortions (\textit{e.g.}, poor PSNR).

% \subsubsection{Image compression with diffusion models}
\noindent \textbf{Image compression with diffusion models.}
DMs have surpassed GAN in many vision tasks. Based on VAE and latent diffusion, latent diffusion models (LDMs) can be directly applied to image compression with minor changes. For example, hyperpriors extracted from the input are used as conditional information for the multi-step denoising process to recover a denoised latent for reconstruction \cite{yang2024lossy}. The performance suffers at ultra-low bitrates as the noisy initialization requires relatively accurate denoising guidance. Also, many steps ($>$15) are usually required, causing severe computation latency. The problem may be alleviated by exploiting the strong generative ability of pre-trained LDM like Stable Diffusion (SD), where the rounding-quantized latent is refined by inverse diffusion \cite{relic2024lossy} with adaptive denoising steps. However, the severe memory burden of pre-trained SD models (often with $>$1.3B parameters) is impractical for compression.  

\noindent \textbf{Hybrid dual-stream image compression.}
A dual-stream LIC framework has been proposed recently, 
which combines the VQ-codebook-based compression and conventional LIC and takes advantage of the resulting synergy at ultra-low bitrates. For example, HybridFlow \cite{lu2024hybridflow} uses pre-trained conventional LIC models to provide fidelity information, which assists the VQ-codebook-based stream in both indices prediction and generative reconstruction. The dual-stream framework aims to achieve a balanced reconstruction quality between fidelity and perception at ultra-low bitrates. However, conventional LIC methods are directly equipped into the entire system without any adaptation. The quality of the assisting fidelity information still suffers from the common rounding quantization issue of general LIC. Inspired by the dual-stream framework, we use DMs in this paper to effectively provide complementary input-specific fidelity information to boost performance further.

\subsection{Dense-Vector-based Vision Model} For convolution-based vision models \cite{ gong2022reverse, cavigelli2017cas, li2023less}, an input image is commonly encoded into a latent feature $L$ as a 3D tensor. As transformer blocks gain popularity in vision models, it has been shown that a highly dense 1D vector $V$ is quite powerful to serve as conditional guidance for various downstream tasks. In general, $V$ carries input-adaptive information learned for specific tasks, which conditionally modify $L$ to improve performance over individual inputs. For instance, RCG \cite{RCG2023} uses a global guidance $V$ as a condition for image generation. DiffIR \cite{xia2023diffir} uses a joint embedding $V$ between the ground-truth and degraded inputs to provide ground-truth information for guiding image restoration. In this work, we incorporate such a dense vector $V$ to provide complementary input-specific fidelity information for improved reconstruction.
\vspace{-0.2em}