\section{Experiments}
\label{sec:exp}

\noindent \textbf{Datasets.} Our HDCompression model is trained on ImageNet-1k \cite{imagenet}, with 1000 categories. In alignment with HybridFlow \cite{lu2024hybridflow}, performance is evaluated over three benchmarks: Kodak \cite{kodak}, CLIC 2020 test set \cite{CLIC2020} and Tecnick dataset \cite{tecknick}. 

\noindent \textbf{Model configurations.}
Training images are cropped into $256\!\times\!256$ patches. To achieve approximately 0.025 bpp using pre-trained MLIC models \cite{mlic2023} as the Base LIC, which typically provide a minimum of 0.1 bpp, input patches for the LIC stream are further downsized to $128 \times 128$ via bilinear interpolation. Both streams have the same downsampling factor $m = n = 16$. The lightweight diffusion modules use a 4-step DDPM scheduler. We unify the loss weights as follows: $w_{1} = 1.2$ for $L_{1}$ pixel loss, $w_{2} = 0.8$ for AlexNet-based LPIPS perceptual loss, and $w_{3} = 0.12$ for UNet-based pixel-wise discriminator GAN loss. 

\noindent \textbf{Compared baselines.} 
We compare with several representative image compression methods: 1) Traditional hand-crafted VVC \& BPG; 2) MLIC \cite{mlic2023}, a single-streamed conventional LIC method; 3) VQGAN \cite{esser2021taming} \& Fine-tuned VQGAN \cite{VQPeking}, single-streamed VQ-codebook-based methods; and 4) HybridFlow \cite{lu2024hybridflow}, a dual-stream framework that straightforwardly combines pre-trained LIC with VQ-codebook-based stream. To obtain ultra-low bitrates, we set $\textbf{QP}$ ranging in $[45,51]$ for VVC \& BPG, and downsize the input image (1/2 width and height) for MLIC.

\noindent \textbf{Evaluation metrics.} 
We evaluate commonly used PSNR and LPIPS \cite{LPIPS}. PSNR measures pixel-level distortion and LPIPS assesses the visual quality. 

\subsection{Quantitative Results}
HDCompression has the same bpp as HybridFlow, which operates over $[0.025, 0.065]$ bpp range, spanning from fully masked indices map (lowest quality) to unmasked indices map (highest quality). As shown in Fig.~\ref{fig:kodak_data}, traditional handcrafted VVC \& BPG and conventional MLIC outperform codebook-based methods over PSNR, due to their learning target of minimizing pixel-level distortions. However, codebook-based methods perform significantly better for perceptual LPIPS. The distortion-driven focus gives artificially inflated PSNR, which omits image details and prefers overly smoothed regions. In contrast, single-stream codebook-based methods emphasize LPIPS, neglecting fidelity to the original image, resulting in $>$4 dB PSNR drop compared with traditional methods at the same bpp.

HybridFlow attempts to balance PSNR and LPIPS, which increases PSNR by about 3 dB compared with single-stream codebook-based methods while offering better LPIPS. Our HDCompression further improves LPIPS through diffusion models, providing visually more pleasing reconstruction, and meanwhile maintaining a stable PSNR curve. With the increase of bpp, the generative stream offers more ground-truth information to compensate for the conventional LIC stream and retain only important general fidelity information.
Overall, our HDCompression achieves approximately 26\% LPIPS improvement compared to HybridFlow while preserving the same level of PSNR, providing a better balance between fidelity and perceptual quality under ultra-low-bitrate conditions.

\begin{figure*}[htbp]
\centering    \includegraphics[width=\linewidth]   {paper_img/qualitative_final_s.png}
    \vspace{-1em}
    \caption{Qualitative Comparison of our method to the baselines. "1\_4" mask strategy ($75\%$ mask ratio on $\textbf{d}_{vq}$) is utilized to maintain around 0.035 bpp within the similar range of the compared baselines. Zoom in for better visualization.}
    \vspace{-1.5em}   \label{fig:qualitative_compare}
\end{figure*}

\begin{figure}[htbp]
\centering
    \includegraphics[width=1\linewidth]
    {paper_img/enhancement.png}
    \vspace{-1em}
    \caption{Impact of hybrid-diffusion modules. \textbf{(a)}: Visual improvement of DRV-based enhancement module over base LIC and effect of VQ-correction merging. \textbf{(b)}: Increase of the token-prediction accuracy via DRV-based mask predictor and quantitative improvements from DRV-based enhancement module across various datasets.}
    \vspace{-1.5em}
  \label{fig:visualization_self}
\end{figure}

\subsection{Qualitative results} 
As shown in Fig.~\ref{fig:qualitative_compare}, HDCompression makes more realistic and sharper image reconstructions compared to other baseline methods. Specifically, VVC and MLIC suffer from significant blurs for heavy rounding quantization. To maintain pixel-wise fidelity, they generate highly smoothed color blocks that are perceptually unpleasant. The single-stream VQGAN fabricates inauthentic details in sensitive regions, \textit{e.g.}, the star-shaped details in the first row.  HybridFlow partially addresses these problems but still suffers from excessive smoothing and detail loss due to the direct utilization of low-quality LIC as assistive information. Our HDCompression effectively resolves such issues with more effective dual-stream fusion, significantly surpassing the reconstruction quality of HybridFlow. Additionally, HDCompression mitigates boundary effects compared to VQGAN and HybridFlow, making block-wise fragmentation less noticeable.

\subsection{Ablation Study on Hybrid-Diffusion Modules}
We conduct an ablation study against the dual-stream HybridFlow to investigate the impact of our diffusion modules.

\subsubsection{DRV-based enhancement for the LIC stream}
We compare our LIC stream output ${\hat{\textbf{x}}}_{lic}$ of incorporating the enhancement module against the original output ${\hat{\textbf{x}}}$ of the pre-trained MLIC. As illustrated in Fig.~\ref{fig:visualization_self} (a), the enhancement module improves the quality of the original LIC output ${\hat{\textbf{x}}}$, particularly by reducing blurs. The enhancement module significantly enhances LPIPS of ${\hat{\textbf{x}}}$ while maintaining almost the same PSNR across various datasets in Fig.~\ref{fig:visualization_self} (b).

\subsubsection{DRV-based transformer for mask prediction}
We compare the logit-wise token prediction loss (Eq.~\ref{eq:tokenlogitloss}) of the DRV-based transformer mask predictor against the naive transformer mask predictor in HybridFlow without using DRV. As shown in Fig.~\ref{fig:visualization_self} (b) where the indices map is masked by "1\_4" masking schedule (75\% masking ratio), the prediction loss is dropped by 18.5\% on average by using DRV in the transformer encoder, leading to 15\% accuracy improvement in token prediction on average. Thus, more ground-truth indices are recovered to provide more specific details to the generative stream that might be neglected by the LIC stream.

\subsubsection{VQ correction for dual-stream merging} Even though the enhancement module effectively improves the quality of the LIC stream as stated above, the poor quality of the original LIC output still results in the loss of details, systematic noise artifacts, \textit{etc.}, the missing details of the watch and the blocky sea surface in Fig.~\ref{fig:visualization_self} (a). It is difficult for the enhancement module to recover the significant information loss from the rounding quantization at ultra-low bitrates. When the generative VQ-based information is merged with the enhanced LIC stream, details are further appended and the artifacts are largely removed in the final output. By the high-frequency-friendly generative information infused from the generative stream, the merging process sharpens the enhanced LIC output, making it more visually pleasing to the human eye.
