\section{Methodology}
\label{sec:method}

\begin{figure*}[t]
    \includegraphics[width=\linewidth]
    {paper_img/framework_final.png}
    \vspace{-1em}
    \caption{System Overview. We sample 2 Dense Representative Vectors (\textbf{DRVs}) by Denoising Networks (\textbf{DNs}) conditioned on the base LIC output $\hat{\textbf{x}}$. These DRVs serve as global guidance for enhancing fidelity and mask prediction. The enhanced LIC output $\hat{\textbf{x}}_{lic}$ further infuses fidelity information into the mask predictor and VQ Decoder in the generative stream.}
    \vspace{-1.5em}
    \label{fig:system_overview}
\end{figure*}

\iffalse
\begin{figure*}[t]
    \includegraphics[width=\linewidth]
    {paper_img/transformer_predictor.png}
    \caption{token predictor. \textcolor{red}{I don't think this is necessary, especially when we do not have enough space. Put the figure in supplementary if you want }}
    \label{fig:token_predictor}
\end{figure*}

\begin{figure*}[t]
    \includegraphics[width=\linewidth]
    {paper_img/vq_correct.png}
    \caption{vq correct. \textcolor{red}{same as above, I don't think this figure is necessary}}
    \label{fig:vq_correct}
\end{figure*}
\fi

As shown in Fig.~\ref{fig:system_overview}, the proposed HDCompression approach has two main data streams: a generative stream and a conventional LIC stream.  

\subsection{The LIC Stream}

For general LIC, an input image $\textbf{x}\in\mathbb{R}^{H\!\times\!W\! \times\!3}$ is first encoded into a latent $\textbf{y}\in\mathbb{R}^{\frac{H}{m}\!\times\!\frac{W}{m}\!\times\!c}$ by an LIC encoder. Then $\textbf{y}$ is rounding quantized into $\textbf{y}_{q}$ for easy transmission, and a LIC decoder reconstructs the output image ${\hat{\textbf{x}}}$ from received $\textbf{y}_{q}$. The downsampling factor $m$ and latent dimension $c$ determine  $\textbf{y}$'s size, \textit{e.g.}, a larger $\textbf{y}$ that has more representative capacity, gives better reconstruction but consumes more bits. 

In the dual-stream framework, the LIC stream provides the fidelity information to the final reconstruction. At ultra-low bitrates, such information quality severely suffers due to the large rounding loss. In this work, instead of merely relying on pre-trained LIC models, we introduce a DRV-diffusion-based enhancement module to improve the fidelity of information from the LIC stream. Following the DRV-based vision model, we leverage a DRV to carry ground-truth information from the current input $\textbf{x}$ to enhance the fidelity of the LIC stream. 
In detail, the structure of the DRV-based enhancement module is inspired by DiffIR \cite{xia2023diffir}, where a DRV $\textbf{v}_{gt}$ containing ground-truth information is fused into Restormer via cross-attention in transformer blocks.  However, $\textbf{v}_{gt}$ is too heavy to transfer for ultra-low-bitrate compression. Therefore, we propose to regenerate a DRV in the decoder by utilizing a joint-embedding DRV vector $\textbf{v}_{joint_{E}}$:
\begin{equation}
    \textbf{v}_{joint_{E}} = E_{L}(\textbf{x}, \hat{\textbf{x}}),
\end{equation}
where $E_{L}$ is a DRV extractor that takes the concatenation of $\textbf{x}$ and $\hat{\textbf{x}}$ as input and generates $\textbf{v}_{joint_{E}}$ as the DRV in the joint space between $\textbf{x}$ and $\hat{\textbf{x}}$. We sample DRV $\hat{\textbf{v}}_{joint_{E}}$ in the decoder via a lightweight diffusion model that is conditioned on the decompressed output $\hat{\textbf{x}}$ of the pre-trained LIC. This better constraints the denoising process to generate the output to be consistent with the content of the original $\textbf{x}$.

Specifically, the forward diffusion process on $\textbf{v}_{joint_{E}}$ with $T$ total steps can be described as:
\begin{equation}
    \textbf{v}_{joint_{E},T} = \sqrt{\bar{\alpha}_T} \mathbf{v}_{joint_{E}} + \sqrt{1 - \bar{\alpha}_T} \mathbf{z}_T, \label{eqn:diffusion_drv1}
\end{equation}
where \(\mathbf{z}_T \sim \mathcal{N}(\mathbf{0}, \mathbf{I})\) is the Gaussian noise. \(\!\bar{\alpha}_T\) is accumulated dot product of pre-defined intensity factors $\beta$: 
\begin{equation}
    \bar{\alpha}_T = \prod\nolimits_{s=1}^{T} (1 - \beta_s). \label{eqn:diffusion_drv2}
\end{equation}
For inference, $\hat{\textbf{v}}_{joint_{E}}$ is sampled from a noisy initialization with a denoising network $\epsilon_\theta$, assisted by a conditioning vector $\textbf{c}$ via formula:
\small
\begin{equation}
\!\!\!\!\!\!\!\hat{\mathbf{v}}_{joint_{E}, t-1}\!=\!\frac{1}{\sqrt{\alpha_t}} \left(\!\hat{\mathbf{v}}_{joint_{E}, t}\!-\!\frac{1\!-\!\alpha_t}{\sqrt{1 \!-\!\bar{\alpha}_t}} \epsilon_\theta(\hat{\mathbf{v}}_{joint_{E},t}, t, \textbf{c})\! \right)\!,\!\label{eqn:diffusion_drv} 
\end{equation}
where $t \in [0,T]$ is the iterative denoising process, $\alpha_t = 1 - \beta_{t}$. Vector
$\textbf{c}$ has the same shape as $\textbf{v}_{joint_{E}}$, and is extracted from ${\hat{\textbf{x}}}$ by a DRV extractor $E_{C}$ that is forked from $E_{L}$ but has a modified input dimension to take only ${\hat{\textbf{x}}}$ as input.
Since our DRV only needs to provide complementary fidelity information instead of modeling entire image structures, a simple denoising network $\epsilon_\theta$ consisting of 4 ResMLP blocks with a 4-step sampling scheduler is used, largely reducing computation and memory requirements compared with conventional UNet-based SD denoising.
$\hat{\textbf{v}}_{joint_{E}}$ is then embedded into the Restormer \cite{zamir2022restormer} for latent enhancement via additional cross-attention blocks inserted into the inner transformer blocks where $\hat{\textbf{v}}_{joint_{E}}$ serves as $key$ and $value$. Finally, the enhanced latent is fed into the LIC decoder to generate the final output image ${\hat{\textbf{x}}}_{lic}$ from the LIC stream, serving as the interactive baseline with the codebook-based generative stream for fidelity infusion.


\subsection{The Generative Stream}
In general, this stream encodes image $\textbf{x}$ into a discrete indices map via a codebook-based representation. First, a VQ-encoder $\textbf{E}_{vq}$ encodes $\textbf{x}$ into a latent representation $\textbf{y}_{vq} \in \mathbb{R}^{\frac{H}{n}\times\frac{W}{n}\times C_{vq}}$ with the downsampling factor of $n$. Then $\textbf{y}_{vq}$ is further mapped into an indices map $\textbf{d}_{vq} \in \mathbb{R}^{\frac{H}{n} \times \frac{W}{n}}$ via a learned codebook with $K$ codewords $\textbf{C} = \{ c_{k} \in \mathbb{R}^{1\times C_{vq}}\}_{k=0}^{K}$. Each vector $y_{ij} \in \mathbb{R}^{1 \times C_{vq}}$ ($i \in [0, \frac{H}{n}], j \in [0, \frac{W}{n}]$) is mapped to the nearest codeword $c_{k} \in \textbf{C}$ by:\vspace{-1em}

\begin{equation}
    {\arg\min}_{k} \ \ \| c_{k} - y_{ij} \|.
\end{equation}
Transmitting whole $\textbf{d}_{vq}$ can be too costly for ultra-low-bitrate scenarios. For example for $K$=1024 and $n$=16, the bpp is about $10/256 \!\!\approx\!\!0.04$, which is close to the upper bound of ultra-low bitrate range ($<$0.05). Thus, compression-friendly binary masks have been used \cite{lu2024hybridflow} to transmit only a masked portion of $\textbf{d}_{vq}$: $\textbf{d}_{masked} = \textbf{m} \circ \textbf{d}_{vq}$.

Then in the decoding stage, an indices map $\hat{\textbf{d}}_{vq}$ is recovered from $\textbf{d}_{masked}$ by masked index prediction through a token-based encoder-decoder transformer $\textbf{T}$. A codebook-based latent ${\hat{\textbf{y}}}_{vq}$ can be retrieved from the shared learned codebook $\textbf{C}$ based on indices ${\hat{\textbf{d}}}_{vq}$, which is used to reconstruct the output image by a VQ-decoder $\textbf{D}_{vq}$.
In our generative stream, the baseline ${\hat{\textbf{x}}}_{lic}$ from the LIC stream is used in two different ways to provide fidelity information: for indices map prediction, and for conditioned pixel decoding with VQ-latent correction.

\begin{figure}[t]
\centering
    \includegraphics[width=\linewidth]
    {paper_img/vq_prediction.png}
    \vspace{-1em}
    \caption{DRV $\hat{\textbf{v}}_{joint_P}$ embedding process in $\textbf{T}$.}
    \vspace{-1.5em}
    \label{fig:transformer_detail}
\end{figure}

\subsubsection{LIC-assisted indices map prediction}
The token-based encoder-decoder transformer $\textbf{T}$ takes as input a token-embedding $\textbf{emb}_{P}$ with length of $\frac{HW}{n^2}\!+\!1$ by $\{emb_{token_0}, emb_{id_0}, emb_{mask}, emb_{id_3}, \cdots, emb_{id_{\frac{HW}{n^2}}}\}$, where $mask$ is the masked index to predict and $emb_{mask}$ is the embedding vector of $mask$; $id_{i}$ is the unmasked ground-truth index and $emb_{id_i}$ is its embedding vector; and $token_0$ is a  class token originally designed for class-based generation and $emb_{token_0}$ is the embedding vector of $token_0$. HybridFlow \cite{lu2024hybridflow} ignores $token_0$ by using a fake class label without actual meaning. However, as shown in MAGE \cite{li2023mage}, instead of fake `dummy' embedding, global priors can be fused into masked locations for improved prediction. Therefore, we fuse information from ${\hat{\textbf{x}}}$ into $\textbf{emb}_{P}$, which serves as image-aware global embedding for better prediction. 

Specifically, similar to the case of the DRV extractors $E_L$ in the LIC stream, a DRV extractor $E_{P}$ extracts $\textbf{v}_{joint_{P}}$ from the concatenation of $\textbf{x}$ and $\hat{\textbf{x}}$.
$\textbf{v}_{joint_{P}}$ is then fused into the token-embedding 
% $\textbf{emb}_P = \{\textbf{v}_{joint_{P}}, emb_{id_0}, emb_{mask}, \cdots, emb_{id_{\frac{HW}{n^2}}}\}.$
\begin{equation}
    \textbf{emb}_P = \{\textbf{v}_{joint_{P}}, emb_{id_0}, emb_{mask}, \cdots, emb_{id_{\frac{HW}{n^2}}}\}.
\end{equation}
Based on $\textbf{emb}_{P}$, the transformer encoder computes a token encoding as 
% $\textbf{enc}_{P} = \{ enc_{\mathbf{v}}, enc_{id_0}, enc_{mask}, \cdots, enc_{id_{\frac{HW}{n^2}}} \},$
\begin{equation}
    \textbf{enc}_{P} = \{ enc_{\mathbf{v}}, enc_{id_0}, enc_{mask}, \cdots, enc_{id_{\frac{HW}{n^2}}} \},
\end{equation}
where $enc_{\mathbf{v}}$, $enc_{id_i}$, and $enc_{mask}$ correspond to the encoded DRV, encoded $id_i$, and encoded $mask$ respectively. To fully utilize the encoded global information from DRV, we replace all $enc_{mask}$ with $enc_{\mathbf{v}}$ in $\textbf{enc}_{P}$ before feeding it to the transformer decoder, so that all masked locations have global priors for better token prediction.

\begin{figure}[t]
\centering
    \includegraphics[width=\linewidth]
    {paper_img/vq_correct.png}
    \vspace{-1em}
    \caption{VQ Correction Module for dual-stream merging.}
    \vspace{-1.5em}
    \label{fig:vq_detail}
\end{figure}

Fig.~\ref{fig:transformer_detail} illustrates the detailed process of DRV $\textbf{v}_{joint_P}$-guided masked token prediction using $\textbf{T}$. Note that the 2D VQ indices map $\textbf{d} \in \mathbb{R}^{\frac{H}{m} \times \frac{W}{m}}$ is first flattened into a sequence before being fed into $\textbf{T}$ in transformer.
Since LIC-assisted Indices Map Prediction is a form of conditional generation and high-level global priors are already fused into the masked locations via the $\textbf{v}{joint_P}$ embedding, we find it beneficial to further constrain the prediction output to more closely align with the ground truth. This is achieved by integrating lower-level fidelity features from the enhanced LIC output $\hat{\textbf{x}}_{lic}$ into the transformer decoder. Specifically, inspired by the “Prediction Assistance” design in HybridFlow \cite{lu2024hybridflow}, we use a forked VQ Encoder $\textbf{E}_{P}$, with the output channel $\textbf{C}_{vq}$ modified to match the hidden dimension $h_{dim}$ of the transformer, to extract a lower-level feature map $\textbf{fm}_{P} \in \mathbb{R}^{\frac{H}{n} \times \frac{W}{n} \times h_{dim}}$ from $\hat{\textbf{x}}_{lic}$. The feature map $\textbf{fm}_{P}$ is then flattened to the shape $\frac{HW}{n^2} \times h_{dim}$ and fed into the cross-attention modules of the transformer decoder, serving as the $key$ and $value$.

To avoid transmitting $\textbf{v}_{joint_{P}}$ and save bits, we use a diffusion process, similar to the approach of using ground-truth infused DRV $\textbf{v}_{gt}$ for enhancing the LIC stream. We sample a $\hat{\textbf{v}}_{joint_{P}}$ in decoder from a noisy initialization, conditioned on a vector $\textbf{c}_{P}$ extracted from $\hat{\textbf{x}}_{lic}$ by following Eq.(\ref{eqn:diffusion_drv1}$\sim$\ref{eqn:diffusion_drv}). The denoising module for generating $\hat{\textbf{v}}_{joint_{P}}$ has the same simple network structure and sampling scheduler as the denoising module for generating $\hat{\textbf{v}}_{joint_{E}}$, and the network for extracting $\textbf{c}_{P}$ from $\hat{\textbf{x}}$ share the same architecture with network $E_C$ for extracting $\textbf{c}$ from $\hat{\textbf{x}}$ in the LIC stream.

\subsubsection{LIC-assisted conditioned pixel decoding} The output image from the LIC stream ${\hat{\textbf{x}}}_{lic}$ provides important fidelity information to the pixel decoding process for reconstructing the final image. We propose an $S$-channel VQ-correction module guided by $\hat{\textbf{x}}_{lic}$ to mitigate the VQ loss caused by inaccurate codebook-entry mapping and introduce an assistive decoder $\textbf{D}_{a}$ to infuse the fidelity information to the VQ-Decoder $\textbf{D}_{vq}$. 

The detailed structure of the VQ-Correction module is shown in Fig.~\ref{fig:vq_detail}, where $\hat{\textbf{y}}_{vq}$ is retrieved in decoder from the codebook using the recovered indices $\hat{\textbf{d}}_{vq}$. It is then fed into $S$-parallel channels, each comprising a $3\!\times\!3$ and $1\!\times\!1$ conv kernel, to generate $S$ derived latents $\textbf{ys}_{1},\ldots,\textbf{ys}_{S}$. Each $\textbf{ys}_{i}\in \mathbb{R}^{\frac{H}{n} \times \frac{W}{n} \times C_{vq}}$ has the same shape as $\hat{\textbf{y}}_{vq}$. Then they are weighted to give a final corrected VQ latent: 
\begin{equation}
\textbf{y}_{correct_{vq}}\!\in\!\mathbb{R}^{\frac{H}{n} \times \frac{W}{n} \times C_{vq}} \!= \!\sum\nolimits_{i=1}^{S} \!\left( \textbf{ys}_{i,\frac{H}{n},\frac{W}{n},C_{vq}} \cdot \textbf{w}_{i,\frac{H}{n},\frac{W}{n}} \right)
\end{equation}
Combining weights $\textbf{w}\!\in\!\mathbb{R}^{S\times\frac{H}{n}\times\frac{W}{n}}$ are extracted from $\hat{\textbf{x}}_{lic}$ by a weight extractor comprising several residual swin transformer blocks (RSTB). $\textbf{w}$ carries input-adaptive information from $\hat{\textbf{x}}_{lic}$ to reduce indices mapping loss in $\textbf{y}_{correct_{vq}}$. 

The assistive decoder $\textbf{D}_{a}$ has the same structure as the VQ-Decoder $\textbf{D}_{vq}$.  $\textbf{D}_{a}$ takes in $\textbf{y}_{correct_{vq}}$ and the feature output after each upsampling layer is element-wisely added to the corresponding layer of $\textbf{D}_{vq}$ via connection links. $\textbf{D}_{vq}$ decodes $\textbf{y}_{correct_{vq}}$ together with additional information from $\textbf{D}_{a}$ to reconstruct the final output image ${\hat{\textbf{x}}}_{final}$. Note that we remove $Softmax()$ 
 to improve module performance during the training phase. Thus, the direct interaction between the $\textbf{w}$ extracted from the enhanced LIC output $\hat{\textbf{x}}_{lic}$ facilitates a more seamless integration of the enhanced LIC feature space into the VQ-based generative feature space. 

It is worth mentioning that our method has the identical bit consumption as the dual-stream HybridFlow \cite{lu2024hybridflow}, since both the ground-truth DRV $\textbf{v}_{gt}$ for LIC stream enhancement and latent $\textbf{v}_{join_{P}}$ for improved indices prediction are reconstructed in the decoder. In other words, by learning diffusion priors, we enhance dual-stream performance without additional transmission overhead. 

\subsection{Training Pipeline} 
Our entire framework is trained through multiple stages to balance training effectiveness and efficiency. 

\subsubsection{Basic flow pre-training} 
The LIC stream uses the pre-trained LIC encoder from MLIC. The VQ-Encoder $\textbf{E}_{vq}$ and the learned visual codebook $\textbf{C}$ use the pre-trained VQGAN model \cite{esser2021taming}. These pre-trained components are designed to either pursue high-fidelity reconstruction or high-quality reconstruction, ensuring the baseline performance of our dual-stream system.

\subsubsection{DRV-based enhancement module training} 
The extractor $\textbf{E}_{L}$ for DRV $\textbf{v}_{joint_{E}}$ and the enhancement module are jointly trained based on the difference between the enhanced $\hat{\textbf{x}}_{lic}$ and the ground-truth $\textbf{x}$, with image loss:
\begin{equation}
    \label{eq: imageloss}
        L_{E} \!=\!w_{1} \!*\!L_{1} \!+\!w_{2}\!*\!L_{P}
        + w_{3}\!*\!L_{G},
\end{equation}
where $L_{1}$, $L_{P}$, and $L_{G}$ are L$_{1}$ pixel loss, perceptual loss via AlexNet, and UNet-based pixel-wise discriminator GAN loss, between ${\hat{\textbf{x}}}_{lic}$ and $\textbf{x}$, weighted by $w_1$, $w_2$, and $w_3$, respectively.
% \vspace{-4pt}
\subsubsection{DRV-based transformer predictor training} 
A binary mask $\textbf{m}_{b}$ is randomly selected among pre-fix mask schedulers and applied to the indices map $\textbf{d}_{vq}$ generated by pre-trained VQ-Encoder $\mathbf{E}_{vq}$  and codebook $\mathbf{C}$. The extractor $\textbf{E}_{P}$ for DRV $\textbf{v}_{joint_{P}}$ and the encoder-decoder transformer $\textbf{T}$ are trained together to predict the masked tokens assisted by $\textbf{v}_{joint_{P}}$, with loss:
\begin{equation}
\label{eq:tokenlogitloss}
L_{T}= -E(\sum \mathrm{log} p(m_{i}|d_{r}, \textbf{v}_{joint_{P}})),
\end{equation}
where $m_{i}$ is the predicted masked tokens and $d_{r}$ is the remaining unmasked indices.
% \vspace{-4pt}
\subsubsection{VQ correction module training} We keep the pre-trained VQ-encoder $\mathbf{E}_{vq}$ and codebook $\mathbf{C}$ frozen and train the VQ-decoder $\mathbf{D}_{vq}$ together with the VQ-correction module and the assistive decoder $\mathbf{D}_a$, based on the difference between the final output $\hat{\textbf{x}}_{final}$ and the ground-truth $\textbf{x}$ with a similar loss function as Eq.~\ref{eq: imageloss} to ensure more stable performance.

\subsubsection{DRV-diffusion module training} The two DRV-diffusion modules, one for enhancing the LIC stream and another for indices map prediction, where each learns an independent 4-step DRV-based diffusion process.
We optimize the mean squared error (MSE) between denoised DRV vector ($\hat{\textbf{v}}_{joint_{E}}$ or $\hat{\textbf{v}}_{joint_{P}}$) and its ground-truth ($\textbf{v}_{joint_{E}}$ or $\textbf{v}_{joint_{P}}$), using pre-trained joint-DRV extractors ($\textbf{E}_{L}$ or $\textbf{E}_{P}$). Instead of focusing on specific steps, our approach minimizes the cumulative loss after the full denoising process, ensuring better DRV reconstruction quality.

\begin{figure*}[t]
\centering \includegraphics[width=\linewidth]
    {paper_img/plot_stats.png}
    \vspace{-1em}
    \caption{Quantitative metrics on Kodak, Tecnick and CLIC2020 test set. \textbf{PSNR} the higher the better. \textbf{LPIPS} the lower the better.}
    \vspace{-1.5em}
    \label{fig:kodak_data}
\end{figure*}