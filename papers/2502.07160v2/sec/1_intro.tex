\section{Introduction}
\label{sec:intro}
\begin{figure*}[t]
    \centering
\includegraphics[width=\linewidth]
    {paper_img/overview_s.pdf}
    % \vspace{-.5em}
    \caption{Visual comparisons of different methods. Bitrates are listed as percentages relative to our method. Traditional hand-crafted VVC and conventional LIC method MLIC present severe blurs, single-streamed VQ-codebook-based VQGAN generates inauthentic details, and HybridFlow has high-frequency artifacts. Our HDCompression retains both fidelity and clarity.}
    \label{fig:general_compare}
\end{figure*}
There has been an explosion of applications requiring transmitting large amounts of image data with limited bandwidth, calling for effective image compression solutions at ultra-low bitrates. Despite decades of research \cite{JPEG2000,vvc,BPG,jpegai}, image compression at ultra-low bitrates remains an ongoing challenge. This is primarily due to the conventional framework of applying heavy quantization at ultra-low bitrates, resulting in significant artifacts. In the conventional framework, an encoder first transforms the input image into a latent feature, either by traditional transformation as in JPEG \cite{JPEG2000} or by neural network models as in learned image compression (LIC) \cite{jpegai}. Then the latent feature is quantized by rounding operations for transmission. A decoder subsequently recovers the output image from the dequantized latent feature using traditional inverse-transformation or neural network models. Therefore, bit reduction occurs during the quantization process. Especially at ultra-low bitrates, intense quantization causes excessive information loss, leading to severe and unpleasant blurriness and noises. Although numerous efforts \cite{jpegai, losslesshyperprior, mlic2023} have been focused on improving the transformation model and prediction of quantization statistics, such artifacts cannot be easily mitigated, as shown in Fig.~\ref{fig:general_compare}. %Therefore, these methods usually operate at \textgreater 0.1 bpp to have a reasonable perceptual quality. 

Besides Variational AudoEncoder (VAE), generative methods such as generative adversarial networks (GANs) \cite{goodfellow2014generative, mirza2014conditional, radford2015unsupervised, isola2017image} and diffusion models \cite{dhariwal2021diffusion, ho2020denoising, rombach2022high, li2024pruning, shen2024} offer promising opportunities to explore alternative frameworks for image compression. Generative approaches learn statistical priors from images, which allows for the synthesis of perceptually realistic high-quality image details from degraded input images. For instance, vector-quantized (VQ) image modeling \cite{van2017neural,esser2021taming} has been recently used for image compression \cite{jia2024generative, vqganmasiwei, WACV2024}, where the learned generative priors serve as visual codewords that span a latent space, enabling images to be mapped into vector-quantized integer indices. 
% \textcolor{red}{Yize (Avoid implying that rounding-based quantization is the sole reason for lower performance in existing LIC methods):}
Thus, the learned VQ latent space provides a refined quantization strategy that retrieves high-quality, information-rich codewords for reconstructing high-realism outputs, which could lead to finer quantization adjustments and effectively avoid the degraded outputs at ultra-low bitrates. However, while the generated outputs are visually appealing to human eyes, the learned generative priors (\textit{i.e.}, codewords) often deviate from authentic image details, bringing about significant pixel-level differences from the original inputs. Thus, most VQ-modeling-based approaches \cite{jia2024generative, vqganmasiwei} primarily address perceptual quality only and operate at extremely low bitrates where poor fidelity may be tolerated, as shown in Fig.~\ref{fig:general_compare}. 

For the practical task of image compression, both content authenticity and visual quality are crucial, even at ultra-low bitrates. However, there is a complex and contradictory relationship between perceptual quality and fidelity \cite{blau2018perception}, making it very challenging for a method to perfect both aspects for general scenarios. Recently, HybridFlow \cite{lu2024hybridflow} has synergized the conventional LIC and the generative VQ-modeling to preserve both fidelity and perceptual quality at ultra-low bitrates (around 0.05 bpp). In HybridFlow, VQ-modeling provides high-realism generation, while conventional LIC offers authentic details from each specific input. However, after incorporating conventional LIC directly into HybridFlow, the quantization issue at ultra-low bitrates still severely impacts the assistive fidelity information quality for indices map prediction and conditional reconstruction. 

To address the issues above and maintain the balance between fidelity and perceptual quality, we propose the Hybrid-Diffusion Compression (HDCompression) approach that effectively exploits both GAN and diffusion models (DM) for ultra-low-bitrate image compression in a dual manner. The diffusion model contributes detailed perceptual features that address high-fidelity requirements, while the VQ-based stream provides structured latent codebooks that enable efficient compression at ultra-low bitrates. Furthermore, instead of directly using pre-trained DM as previous DM-based compression methods \cite{relic2024lossy}, we utilize a dense representative vector (DRV) to mitigate the heavy computation and memory consumption issues. Compared with previous state-of-the-art (SOTA) image compression methods, including traditional VVC, single-stream LIC method MLIC \cite{mlic2023}, single-stream VQ-modeling method \cite{vqganmasiwei}, and dual-stream HybridFlow \cite{lu2024hybridflow}, HDCompression can improve perceptual quality (LPIPS) by 26\% over HybridFlow, while maintaining the same level of fidelity (PSNR), and can reduce artifacts like random or structured noise patterns.  
We highlight our contributions as follows.
\begin{itemize}
    \item We introduce a novel dual-stream framework, HDCompression. The generative stream exploits the power of both VQ-based modeling and diffusion-based latent structure learning, where codebooks provide general image priors for reconstruction and the lightweight diffusion module learns structural priors of joint embedding between high-quality original inputs and low-quality compressed inputs. Based on the compressed inputs, the diffusion module recovers the DRV in decoder to provide input-specific fidelity information without additional transmission, which complements the fidelity information of the conventional LIC stream to enhance the performance of both indices recovery and final reconstruction.  
    
    \item We design an efficient DRV-based lightweight vector-wise diffusion module with a 4-step sampling scheduler to provide complementary fidelity information instead of modeling entire image structures. This approach mitigates the difficulty of obtaining accurate denoising guidance at ultra-low bitrates and reduces computation and memory requirements.
    
    \item We propose two modules to merge the generative stream and the conventional LIC stream. The enhancement module uses the DRV from the generative stream to improve the conventional LIC stream, providing improved fidelity information for the reconstruction. The VQ-codebook latent correction module uses the enhanced conventional LIC stream to reduce VQ loss during indices prediction. 
\end{itemize}


