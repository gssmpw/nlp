\clearpage
\setcounter{page}{1}
\maketitlesupplementary

\begin{figure*}[b]
\centering
    \includegraphics[width=0.93\linewidth]
    {paper_img/additional_2.png}
    \caption{Visual Comparisons on JPEG-AI test set. Zoom in for better visualization.}
    \label{fig:additional_visual}
\end{figure*}

\section{Training Configurations in Detail}
All of the module training workloads are launched on 8 NVIDIA L40s GPUs with 48G GPU memory each. We uniformly employ the AdamW optimizer with a weight decay of 0.05 and betas set to (0.95, 0.99), along with the CosineAnnealing learning rate scheduler. Additionally, mixed precision training is enabled to accelerate the training process and optimize GPU memory usage.

The ImageNet training images are randomly cropped into $256\times 256$ patches as follows:

\texttt{RandomResizedCrop(size=(256,256), scale=(1.0,1.0), ratio=(0.75,1.3333), interpolation=bilinear, antialias=warn).}

\subsection{DRV-based enhancement module \& VQ Correction module} Given that both modules apply the image loss between $\textbf{x}$ \& $\hat{\textbf{x}}_{lic}$ and $\textbf{x}$ \& $\hat{\textbf{x}}_{final}$, the loss weights are the following: $w_{1} = 1.2$ for $L_{1}$ pixel loss, $w_{2} = 0.8$ for LPIPS perceptual loss, and $w_{3} = 0.12$ for GAN loss. This weight configuration strikes a balance between reconstruction fidelity and perceptual quality. The inclusion of the GAN loss, particularly in ultra-low bitrate scenarios with severely limited reference information, effectively improves the sharpness of the reconstructed image, mitigates the regular noise artifacts introduced by the LPIPS loss, and results in more visually pleasing outputs.

The hyperparameters are set as follows: 

batch\_size = 16, base\_learning\_rate = $1e^{-4}$, epochs = 10, warmup\_epochs = 1, gradient\_clip = 5.0.

\subsection{DRV-based transformer predictor} MAGE and HybridFlow conduct the mask predictor training, where a random mask ratio ranging within $[0.5, 1)$ of the truncated normal distribution (mean = 0.55, std = 0.25) is sampled and the ground-truth indices map $\textbf{d}$ would be randomly masked out according to the sampled mask ratio. However, during inference, a regularized mask schedule $\textbf{m}_{b}$ could be chosen among a series of pre-fixed mask schedulers $\textbf{m}$. As MAGE is designed for various downstream tasks, especially unconditioned image generation, directly applying MAGE’s training pipeline to the task-specific ultra-low bitrate image compression would not be a perfect fit and could lead to a misalignment between training design and inference. Thus, we unify the training design and inference via directly sampling a regularized mask schedule $\textbf{m}_{b}$ and applying it to $\textbf{d}$ during training to accurately recover the masked indices in the pre-fixed locations.

The hyperparameters are set as follows:

batch\_size = 32, base\_learning\_rate = $1e^{-4}$, epochs = 40, warmup\_epochs = 8, gradient\_clip = 3.0

\subsection{DRV-diffusion module} We aim at minimizing the gap between fully-denoised DRVs from the random noisy initialization \& the ground-truth DRVs via MSE loss. We register the \textbf{DDPM} module in our implementation as follows:

\texttt{DDPM.denoise\_network = $\epsilon_{\theta}$} consisting of 4 ResMLP blocks,
\texttt{DDPM.condition = $E_{c}$} for extracting conditioned vector $\textbf{c}$ from $\hat{\textbf{x}}$, 
\texttt{DDPM.n\_feats = 64, DDPM.linear\_start = 0.1, DDPM.linear\_end = 0.99, DDPM.timesteps = 4}. 

The hyperparameters are set as follows:

batch\_size = 32, base\_learning\_rate = $5e^{-5}$, epochs = 5, warmup\_epochs = 1, gradient\_clip = 2.0

\section{Additional Qualitative Results}
We provide additional visual comparisons between single-streamed MLIC, single-streamed VQGAN, HybridFlow, and our method, HDCompression, on the JPEG-AI test set. All configurations are aligned with those mentioned in the Experiments section, where the ‘1\_4’ pre-fixed mask schedule (with a 75\% mask ratio) is applied to the VQ indices map $\textbf{d}$, resulting in approximately 0.035 bpp for general image inputs. The other methods are configured to achieve a similar bitrate of around 0.035 bpp to ensure fair comparisons. The additional visual comparisons are presented in Fig.~\ref{fig:additional_visual}, where ‘@bpp’ indicates the overall bitrates used for compression across the entire dataset.
