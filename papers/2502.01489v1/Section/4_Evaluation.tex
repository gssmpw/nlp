\section{Results and Analysis}\label{sec:Results}

\begin{table}
\centering
\caption{Design properties of the analog spline}
\input{FigTab/DesignProp}
\label{tab:designsizes}
\end{table}

\subsection{Simulation Setup}

Simulations were made using the Cadence Spectre simulator. For the subtractive flexible technology, we employ the PragmatIC FlexICs PDK second-generation Helvellyn 2.1.0 technology~\cite{FlexICs}. All simulations are performed with $V_{dd}$ of 1.0V, $V_{ss}$ of -1.0V, temperature of 27ºC and our $V_{in}$ is a slope from -0.5V to 0.5V with a rise up time of 10ms.
In~\autoref{tab:designsizes} we present the sizes of each design used in the spline. We do not add the block of the adder, since its design is the result of combining one inversion and one subtraction block.

Since the range of the ABBs inputs was known from the implementation, we were able to optimize each circuit for this specific range. 
For example, the input of the squaring circuit is always between [-0.5,0.5] with an output range of [0,0.25]. 
In contrast, the multiplication circuit's output range can include negative as well as positive values, requiring a more robust circuit. This explains why the squaring circuit has lower resistor values and a higher number of transistors, and why in some cases, is more convenient to use this circuit to perform multiplications instead of the multiplication circuit.

To compare the hardware cost of the analog spline with the digital, we created a standard cell library using PragmatIC PDK \blue{with a $V_{dd}$ of 1V}, synthesized it using Synopsys Design Compiler S-2021.06. 
For simulation and power analysis, we used VCS T-2022.06 and PrimeTime T-2022.03. 
\blue{Based on~\autoref{eq:quadratic_formula} we developed an 8-bit fixed-point digital spline with 6-bit decimal bits and one signed bit,
%This was done to match the chosen input range of the analog spline.  
implemented with sequential logic \purple{using verilog}. The architecture of the digital spline was determined by the synthesis tool.}

\subsection{Hardware costs}\label{sec:HardwareCosts}

\begin{table}
\centering
\caption{Hardware cost of an analog and digital spline}
\scalebox{1}{\input{FigTab/hardwarecost}}
\vspace{-2ex}\label{tab:analogvsdigital}
\end{table}

\input{FigTab/splinelayout}

In table~\autoref{tab:analogvsdigital}, we compare the hardware cost of the analog spline presented in this work to \blue{a 8-bit digital spline.}
We can see that the analog implementation's area cost is significantly lower than the digital one, achieving a 125$\times$ reduction. In terms of power, the analog implementation is also more efficient than the digital, with a 10.59\% reduction.
The physical implementation of the analog spline is presented in~\autoref{fig:splinelayout}.
As mentioned in the implementation section, the selection of implementing~\autoref{eq:quadratic_formula} and not~\autoref{eq:bezier_second_order}, reduces area up to 46\% and achieves power savings of 45.7\%.

\subsection{Approximation error}

Since the target of our ABBs is to approximate functions, we expect an error from this approximation. 
We also analyze where the error originates. We use the Normalized Mean Percentage Error (NMPE) metric, \blue{since} it quantifies the average error as a percentage of the function’s range, measuring the magnitude and direction of errors. 
This relative measure is particularly useful for evaluating function approximation within normalized signal ranges.

\textit{Error from each ABB}: We first measured the error in each block, as presented in~\autoref{tab:errorABBs}.
A negative NMPE indicates that the approximation tends to underestimate the function, while a positive value suggests that the approximation overestimates it.

\begin{table}
\centering
\caption{Error of each ABB}
\scalebox{1}{\input{FigTab/errorABBs}}
\vspace{-2ex}\label{tab:errorABBs}
\end{table}

\textit{Error of the full spline}: When we integrated all our ABBs together, we observed that the error is not commutative, but in many cases, it compensates. 
We ran three different scenarios with predefined values for $P_{0}$, $P_{1}$ and $P_{2}$, and the maximum error was -7.58\%\orange{, observed in Scenario C.
The results for each scenario, including the values of $P_{0}$, $P_{1}$ and $P_{2}$, along with the corresponding NMPE, are summarized in~\autoref{tab:errorFullSpline}.}.

\begin{table}
\centering
\caption{Error Analysis of the Full Spline Across Different Scenarios}
\scalebox{1}{\input{FigTab/errorFullSpline}}
\label{tab:errorFullSpline}
\end{table}

\input{FigTab/schevslayout}

\textit{Error of the \blue{physical implementation}}: We also need to consider the error introduced by the layout and its parasitic elements. 
In the example with the most error, the NMPE from the schematic simulation was -6.34\% but with the post-layout simulations, this error increased to -7.58\% (\orange{value presented in~\autoref{tab:errorFullSpline}}). 
A comparison between the schematic and post-layout simulations can be seen in~\autoref{fig:schevslayout}.

\input{FigTab/KANerror}

\textit{Impact of Hardware-Induced Error on Function Approximation Using KAN}: To investigate the sensitivity of KAN-based function approximation to hardware errors, we developed a Python script using numpy, matplotlib, and scipy CubicSpline. 
This script simulates a simplified KAN model with one input and multiple basis splines. 
The network was trained to approximate two chosen target functions, $x^2$ and $e^x$, using a fixed range of input values $[-0.5,0.5]$.

The KAN implementation features three basis splines, each represented by a cubic spline. 
A controlled noise factor of -7.58\% was applied to each spline during training to emulate hardware error, introducing a stochastic perturbation that mirrors potential inaccuracies in hardware approximations. 
For comparison, we ran a parallel experiment without noise to observe the baseline performance of the KAN.
The output of these simulations is shown in~\autoref{fig:KANerror}. As we can see, the effect of the hardware error is minimal: \orange{less than 1\% for $x^2$ and less than 5\% for $e^x$}.
It is also important to note that expanding the input range to higher values, such as [-1,1] or [-5,5], significantly reduces the NMPE to below 1\% in the error-induced KAN in both cases.

\blue{\subsection{Discussion of the results}}

This work has several limitations that suggest promising directions for future research. 
First, our focus is limited to approximating second-degree functions, which restricts the range of applications. 
Extending this approach to higher-order functions would increase its applicability. 
\red{For higher precision requirements or more complex function approximations, it would be necessary to either use more splines—thus increasing the KAN network—or to improve and add more ABBs.}

Since this approach relies on approximation, it naturally introduces some degree of error. 
\red{While these results demonstrate the resilience of KAN-based approximation to hardware errors, minimizing hardware error requires a robust design that maintains simplicity and is specifically tailored to our FE implementation.
This presents a trade-off between error reduction and preserving the hardware benefits of analog design. Balancing these factors is critical in ensuring an optimal approximation with minimal complexity.}

Another limitation is that we did not incorporate co-design strategies, which might have enabled more optimized and efficient configurations\red{, increasing accuracy and versatility, as seen in~\cite{Huang:KANAcceletaror:2024,VanDuy:KANCodesign:2024}}.
Lastly, our design operates in the voltage domain, which limits the operating range. 
Future studies might explore shifting to the current domain to extend this range and adapt to broader application needs.

These results highlight the benefits of analog computing for target applications such as ABBs and the implementation of splines to create KAN. Nevertheless, it is important to mention that while the objective of this work is to create an analog function approximation, which provides benefits in area and power savings, it comes at the cost of higher error compared to the digital equivalent. Therefore, digital computing may be a better solution when high precision is mandatory. 

\vspace{2ex}