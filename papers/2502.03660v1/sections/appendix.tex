\appendix
\onecolumn

\section{MLIP Evaluation in Molecular Dynamics Simulation} \label{app:aimd}

\textbf{Ab-Initio Molecular Dynamics (AIMD):} AIMD combines quantum mechanical calculations with classical molecular dynamics to model the dynamics of atoms and molecules. AIMD generally applies a quantum mechanical method, such as DFT or Car-Parrinello Molecular Dynamics (CPMD) \citep{car1985unified}, to compute the electronic energy and atomic forces (the negative gradient of the energy with respect to atom coordinates) which are used to propagate atoms according to classical mechanics. The ability of AIMD toward quantum accurate computation of energy and forces makes it a natural choice for MLIPs deployment shown in a couple of early studies that have indicated limited success \citep{fuforces, bihani2024egraffbench}. AIMD is also a useful method for obtaining correlated training data for MLIPs for real-world applications of materials system under physical conditions, thereby lending itself in enabling device-scale simulations.


\begin{figure}[hbp]
     \vspace{-0.2cm}
    \centering
    \includegraphics[width=0.5\linewidth, trim=0 20 0 20, clip]{figures/Modal_params_plot.png}
    \caption{Training and inference times for different MLIP architectures on the LiPS dataset based on an analysis from \citet{bihani2024egraffbench}. The MLIPs architectures include: MACE \citep{batatia2023foundation}, BotNet \citep{batatia2025design}, Allegro \citep{musaelian2023learning}, Equiformer \citep{liao2023equiformer} and NequiP \citep{batzner20223}, all of which fail to achieve the inference time performance of the classical BKS potential \citep{van1990force}.}
    \label{fig:model_scaling}
\end{figure}


\textbf{MLIP Inference and Training Time Comparison:} \Cref{fig:model_scaling} based on \citet{bihani2024egraffbench} illustrates the inference and training times for several geometric deep learning architectures developed for molecular dynamics (MD) simulations. It is important to note that while these models often have a large number of parameters, leading to higher inference times, certain architectures—such as transformers—are relatively fast during inference. However, transformer-based models require more epochs for training, which increases the overall training cost.


\section{MLIP Evaluation at Large Scale for Diverse Materials} \label{app:sec:mlip-eval}

\begin{figure}
\centering     
\subfigure[Normalized Element Frequency in Log Scale]{\label{fig:a}\includegraphics[width=0.45\textwidth]{figures/Normalised_frequency.png}}
\subfigure[Normalized Element Frequency in Linear Scale]{\label{fig:b}\includegraphics[width=0.45\textwidth]{figures/Normalised_frequency_nolog.png}}
 \vspace{-0.40cm}
\caption{Normalized element frequency of Mptrj \citep{deng2023chgnet} containing DFT simulated and naturally occuring minerals in AMCSD \citep{downs2003american} that are experimentally measured compounds. The linear scale shows the high proportion of Mptrj related to binary and ternary materials. }
\label{fig:amcsd_main}
\end{figure}


Our analysis in \Cref{sec:mat-limits} shows the limited space of coverage of common DFT datasets, such as Mptrj \citep{deng2023chgnet}. The shortage of complex materials can further be augmented by comparing Mptrj to experimental databases, such as real-world experimentally measured minerals from the AMCSD database \citep{downs2003american}. One observation is that real minerals consist of far more components than just binary and ternary crystals as shown in \Cref{fig:amcsd_main}. While the maximum number of elements in Mptrj is 9, AMCSD contains meaningful representation with much larger number of elements, with some containing up to 23 components. This highlights that the interactions between such complex compositions might not be captured by the universal potential trained only on the MPtrj dataset, suggesting that these materials may be considered out of distribution.


\subsection{Considerations for Large-Scale MD Bechmarking}

The utility of MLIPs span a broad application space, from single system inference, to large scale material screening. In the way that computer vision models are just as easily applied to one image as they are to a collection of real time video feeds, and single point weather forecasting is transferable to global scale weather modeling, we would like to be able to apply MLIPs across this range of system scale while maintaining model consistency in with respect to both scientific and engineering considerations. To get to this level of robustness for MLIPs requires many interrelated considerations, starting from base level materials and working towards device scale simulations. In base level materials, the specific system will inform important simulation parameters that underlie the behavior of the model. These simulation parameteres include boundary conditions, data preprocessing pipelines, convergence thresholds, as well as downstream evaluation metrics, all of which are conditioned on the base material. Many of these scientific parameters have concrete computational impacts, and when evaluating systems at scale, can have major implications for simulation throughput. While simulation parameters and compute requirements can be meticulously tuned when evaluating individual systems, fine-tuning quickly becomes intractable with large-scale evaluation of diverse materials. 

Furthermore, different MLIPs may have different computational costs, creating further complexity when comparing MLIPs at large scale. Practically, engineering considerations and compute constraints need to be considered and reported when developing broad MLIP evaluation pipelines. Similar to consistency issues with DFT described in \Cref{sec:dft-limits}, further work is also needed in providing consistency, transparency and reproducibility in MLIP evaluation. For example, most models come with their own \textit{calculator} interface designed to plug into virtual material modeling environments which orchestrate simulations. These calculator implementations are not readily transferable between models, and may include assumptions and nuanced model and data specific transforms, rendering one model's calculator incompatible with other models. This implementation detail makes direct simulation comparisons difficult unless the explicit calculator paradigms are explicitly provided and comparable in their implementation. 

Additionally, MLIP data processing pipelines can have subtle differences that can make reproducing results challenging. Some models may expect fractional coordinates, while others rely on Cartesian, and in the case of GNN's, the graph creation process itself may vary amongst data processing pipelines. When further extended to a framework to framework comparison, common graph libraries such as PyG \citep{Fey/Lenssen/2019} and DGL \citep{wang2019dgl}, and two of the most popular material analysis libraries, pymatgen \citep{ong2013python}, ASE \citep{HjorthLarsen_2017} and LAMMPS \citep{thompson2022lammps}, may lead to divergent results simply due to convention of implemented functions. As such, to evaluate a collection of MLIPs consistently, common frameworks, calculator interfaces, computational constraints, and representation formats should all be considered. Current benchmarks, such as Matbench \citep{riebesell2023matbench}, fall short in properly evaluating the performance of MLIPs in these simulation settings. For example, while many models show similar performance in test metrics such as energy and force MAE, they show great variance when used in basic molecular dynamics tasks such as crystal structure relaxation \citep{gonzales2024benchmarking, bihani2024egraffbench}, showing that strong static benchmarking results do not always correlate with simulation performance. Given the importance evaluation to ML method development, integrating MLIPs into simulations remains critical to enable greater understanding of the underlying materials.


\section{Model \& Data Distillation Methods} \label{app:distillation}


\textbf{Data distillation:} Data distillation in MLIPs represents a critical, yet underaddressed, challenge, wherein strategic configuration selection from large-scale datasets (such as MPTrj, OMat24, Alexandria) can substantially reduce computational training costs. Existing active learning approaches insufficiently address the complex multi-dimensional challenge of identifying representative, information-rich configurations that capture the configurational space's essential physics. Moreover, in active learning approaches such as the full retraining approach in \citep{gonzales2023data}, the computational cost is fundamentally driven by the requirement of retraining the entire model in each iteration, which becomes prohibitively expensive when comprehensive datasets are available a priori. Systematic sampling methodologies must balance configuration diversity, statistical representativeness, and information entropy to enable efficient potential training. The goal is to develop principled strategies that can extract minimal yet maximally informative subsets from massive trajectory databases, potentially reducing computational cost of training by orders of magnitude.


\textbf{Model distillation:} Model distillation is an emerging strategy for scaling MLIPs to device-level simulations through knowledge transfer from large ``teacher'' models to compact ``student'' models exploiting, for instance, energy hessians~\cite{amin2025towards}. This approach enables the development of parameterically efficient models that can potentially surpass their large-scale predecessors in system-specific performance. The core methodology involves systematically compressing the knowledge representation of complex MLIPs into smaller neural network architectures. These distilled models can be further refined on targeted, minimal datasets, leveraging their reduced parameter space to achieve computational efficiency comparable to classical potentials while maintaining high-fidelity predictive capabilities. However, further investigation is required on the distilling strategy, the stability and performance of the consequent MLIP, including its generalizability. 
