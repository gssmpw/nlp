\vspace{-0.2cm}
\section{Computational \& Modeling Considerations} \label{sec:computation}
\vspace{-0.1cm}

While tools exist today to integrate MLIPs into materials simulation codes \citep{gupta2024kusp}, such as ASE \citep{HjorthLarsen_2017} and LAMMPS \citep{thompson2022lammps}, the evaluation of MLIPs on simulation benchmarks remains limited in large part due to inefficient computation \citep{gonzales2024benchmarking}. Some architectures, such as MACE \citep{batatia2023foundation}, have achieved additional acceleration by implementing the model architecture using the Kokkos language \citep{9485033}. While re-implemtation might be feasible in isolated cases, this approach may not be scalable given the dominance of PyTorch for MLIP development and training.  
One promising approach for scaling the training and deployment of MLIPs is the emergence of performant, cross-hardware programming language like JAX \citep{jax2018github} and Triton \citep{tillet2019triton} that encourage community driven development. These langauges have already enabled the development of MLIP-based simulation frameworks \citep{schoenholz2020jax, Hu2020DiffTaichi:, helal2024mess, doerr2021torchmd} along with faster training and inference of MLIP architectures based on scalable primitives \citep{lee2024scaling}.

As shown in \Cref{fig:compute-scales}, hardware and software advancements have reached new levels of maturity, thereby enabling the deployment of higher accuracy methods described in \Cref{sec:materials}. While these advancements show promise for better data generation methods, a significant gap remains for effective MLIP deployment. Like many other computational fields, there are physical effects and interactions that can only be modeled over large spatial and temporal scales---in the case of materials science, logarithmic in the number of atoms \emph{and} the time range. To date, pragmatic modeling choices are made to enable computationally \emph{tractable} but not necessarily realistic simulations to be conducted. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/compute_scaling.png}
    \vspace{-0.90cm}
    \caption{Schematic of length and time scales relevant to materials modeling. Annotations indicate approximate computational requirements; each block region corresponds to the amount of effort required for the corresponding scale of compute within a ``timely'' fashion. Routine and feasible refer to on the order of hours to days, while difficult can extend from weeks to months depending on the scale of distributed computing (e.g., the 2023 Gordon Bell prize submission by \citet{kozinsky2023scaling}). ``Unreasonable'' means technically possible, but practically improbable due to to collective compute and engineering efforts required, and corresponds to the device scale of an Intel 8086--a microprocessor from the late 1970's--assuming a representative ${\sim}$30,000 atoms per transistor.}
    \label{fig:compute-scales}
    \vspace{-0.60cm}
\end{figure}


The majority of the field currently focuses on pure crystalline structures that take full advantage of small unit cells, but fall short of the device-scale regime that requires the explicit treatment of billions of atoms composed into functional components (e.g. transistors), with timescales spanning from femtoseconds (e.g. electric field response) to microseconds or more (e.g. thermal dissipation, diffusion driven reactive chemistry, long-lived excited states). While MLIPs undoubtedly benefit from better time complexity scaling than conventional methods, \Cref{fig:compute-scales} shows that the forefront of atom scaling is still barely within reach of historical computing devices like the Intel 8086 with tens of thousands of transistors composed of hundreds of millions of atoms \citep{kozinsky2023scaling}, let alone modern devices with hundreds of billions. Using the empirical strong scaling results on $10^8$ atoms by \citet{kozinsky2023scaling} with 1024 nodes of Nvidia A100, reaching one millisecond of simulation time would require ${\sim}$105 years of dedicated computation. Thus, achieving the required scale needed to simulate properties of interest in integrated devices would require significant algorithmic and/or hardware improvements: performant kernels hundreds if not thousands of times faster could render at least the temporal aspect ``difficult'', although the atomic scale would remain in question. Hardware and algorithm co-design has enabled breakthroughs before, such as Anton for MD simulations \citep{shaw2021anton}. 

Another interesting direction is on fundamental changes to how MD simulations are performed. Notably, recent efforts such as TimeWarp \cite{klein2024timewarp} and distributional graphnormer \cite{zheng2024predicting}, have adapted generative modeling methods to improve sampling of rare and notable dynamics and minimize nominally wasteful timesteps. We note, however, that these approaches are still in their infancy and have been developed primarily for small molecules and structural biology, and thus have yet to be proved at the scale of hundreds of millions of atoms. Nonetheless, they appear to be highly promising research avenues.

\textbf{Challenge 5.1} \textit{Scalable methods for MLIP deployment on large-scale atomistic systems leveraging tightly coupled hardware/software co-design, and performant differentiable software stacks that drive new MLIP-enabled capabilities.}



