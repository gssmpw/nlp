\section{Related Work}
\subsection{Synthetic Datasets in VQA}
The use of synthetic data in visual question answering (VQA) has gained traction as a means to circumvent the limitations and biases inherent in real-world datasets. Synthetic environments~\cite{hess2013blender, qiu2016unrealcv, sanders2016introduction} allow for controlled manipulation of scene elements, enabling researchers to probe specific reasoning abilities in VQA models without the confounding factors often present in natural imagery. Previous works~\cite{johnson2017clevr, lindstrom2022clevr, li2023super} have utilized synthetic environments to evaluate multimodal models across a range of visual reasoning tasks, typically focusing on 2D spatial relationships and object features. For instance, CLEVR~\cite{johnson2017clevr}, a synthetic dataset widely used in the VQA community, evaluates compositional reasoning by introducing artificially generated scenes with diverse object arrangements. The follow-up work, Super-CLEVR~\cite{li2023super}, incorporates a broader range of out-of-distribution domains. While Super-CLEVR-3D~\cite{wang20243d} represents a significant step forward in evaluating 3D locations, it lacks an assessment of 6D spatial relationships that consider both 3D location and 3D orientation. Our work extends this tradition by creating a synthetic dataset tailored for 6D spatial reasoning, filling a gap in current synthetic datasets that do not address the complete spatial understanding of objects in 3D space.

\subsection{LMM Benchmarks}
Existing benchmarks~\cite{goyal2017making, hudson2019gqa, singh2019towards, yue2024mmmu} for large multimodal models (LMMs) predominantly assess their performance on tasks involving 2D visual question answering and multimodal reasoning. For instance, datasets like VQAv2~\cite{goyal2017making} and GQA~\cite{hudson2019gqa} focus on a model's ability to answer questions based on visual content, while multimodal reasoning datasets, such as MMMU~\cite{yue2024mmmu}, evaluate the model's integration of visual and linguistic information to achieve complex reasoning. However, these benchmarks primarily emphasize visual understanding, leaving a gap in evaluating models' capabilities in spatial contexts. Recent benchmarks~\cite{tong2024cambrian,chen2024spatialvlm,cheng2024spatialrgpt} evaluate spatial understanding, including depth ordering for 2.5D and 3D location assessment. Our proposed benchmark goes beyond them by introducing tasks that challenge LMMs to understand and reason about 3D positions and orientations altogether, setting a new standard for 6D spatial reasoning evaluation in multimodal AI.


\vspace{-0.3em}