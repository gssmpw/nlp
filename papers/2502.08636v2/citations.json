[
  {
    "index": 0,
    "papers": [
      {
        "key": "hess2013blender",
        "author": "Hess, Roland",
        "title": "Blender foundations: The essential guide to learning blender 2.5"
      },
      {
        "key": "qiu2016unrealcv",
        "author": "Qiu, Weichao and Yuille, Alan",
        "title": "Unrealcv: Connecting computer vision to unreal engine"
      },
      {
        "key": "sanders2016introduction",
        "author": "Sanders, Andrew",
        "title": "An introduction to Unreal engine 4"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "johnson2017clevr",
        "author": "Johnson, Justin and Hariharan, Bharath and Van Der Maaten, Laurens and Fei-Fei, Li and Lawrence Zitnick, C and Girshick, Ross",
        "title": "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning"
      },
      {
        "key": "lindstrom2022clevr",
        "author": "Lindstr{\\\"o}m, Adam Dahlgren and Abraham, Savitha Sam",
        "title": "Clevr-math: A dataset for compositional language, visual and mathematical reasoning"
      },
      {
        "key": "li2023super",
        "author": "Li, Zhuowan and Wang, Xingrui and Stengel-Eskin, Elias and Kortylewski, Adam and Ma, Wufei and Van Durme, Benjamin and Yuille, Alan L",
        "title": "Super-CLEVR: A virtual benchmark to diagnose domain robustness in visual reasoning"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "johnson2017clevr",
        "author": "Johnson, Justin and Hariharan, Bharath and Van Der Maaten, Laurens and Fei-Fei, Li and Lawrence Zitnick, C and Girshick, Ross",
        "title": "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "li2023super",
        "author": "Li, Zhuowan and Wang, Xingrui and Stengel-Eskin, Elias and Kortylewski, Adam and Ma, Wufei and Van Durme, Benjamin and Yuille, Alan L",
        "title": "Super-CLEVR: A virtual benchmark to diagnose domain robustness in visual reasoning"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "wang20243d",
        "author": "Wang, Xingrui and Ma, Wufei and Li, Zhuowan and Kortylewski, Adam and Yuille, Alan L",
        "title": "3D-Aware Visual Question Answering about Parts, Poses and Occlusions"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "goyal2017making",
        "author": "Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi",
        "title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering"
      },
      {
        "key": "hudson2019gqa",
        "author": "Hudson, Drew A and Manning, Christopher D",
        "title": "Gqa: A new dataset for real-world visual reasoning and compositional question answering"
      },
      {
        "key": "singh2019towards",
        "author": "Singh, Amanpreet and Natarajan, Vivek and Shah, Meet and Jiang, Yu and Chen, Xinlei and Batra, Dhruv and Parikh, Devi and Rohrbach, Marcus",
        "title": "Towards vqa models that can read"
      },
      {
        "key": "yue2024mmmu",
        "author": "Yue, Xiang and Ni, Yuansheng and Zhang, Kai and Zheng, Tianyu and Liu, Ruoqi and Zhang, Ge and Stevens, Samuel and Jiang, Dongfu and Ren, Weiming and Sun, Yuxuan and others",
        "title": "Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "goyal2017making",
        "author": "Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi",
        "title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "hudson2019gqa",
        "author": "Hudson, Drew A and Manning, Christopher D",
        "title": "Gqa: A new dataset for real-world visual reasoning and compositional question answering"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "yue2024mmmu",
        "author": "Yue, Xiang and Ni, Yuansheng and Zhang, Kai and Zheng, Tianyu and Liu, Ruoqi and Zhang, Ge and Stevens, Samuel and Jiang, Dongfu and Ren, Weiming and Sun, Yuxuan and others",
        "title": "Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "tong2024cambrian",
        "author": "Tong, Shengbang and Brown, Ellis and Wu, Penghao and Woo, Sanghyun and Middepogu, Manoj and Akula, Sai Charitha and Yang, Jihan and Yang, Shusheng and Iyer, Adithya and Pan, Xichen and others",
        "title": "Cambrian-1: A fully open, vision-centric exploration of multimodal llms"
      },
      {
        "key": "chen2024spatialvlm",
        "author": "Chen, Boyuan and Xu, Zhuo and Kirmani, Sean and Ichter, Brain and Sadigh, Dorsa and Guibas, Leonidas and Xia, Fei",
        "title": "Spatialvlm: Endowing vision-language models with spatial reasoning capabilities"
      },
      {
        "key": "cheng2024spatialrgpt",
        "author": "Cheng, An-Chieh and Yin, Hongxu and Fu, Yang and Guo, Qiushan and Yang, Ruihan and Kautz, Jan and Wang, Xiaolong and Liu, Sifei",
        "title": "SpatialRGPT: Grounded Spatial Reasoning in Vision Language Model"
      }
    ]
  }
]