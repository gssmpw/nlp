% CVPR 2025 Paper Template; see https://github.com/cvpr-org/author-kit

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[preprint]{cvpr}      % To produce the REVIEW version
\usepackage{multirow} 
\usepackage{makecell}
\usepackage{array}
% \usepackage[table]{xcolor} % Enable coloring in tables
\definecolor{tablered}{RGB}{255,200,200}
\definecolor{tableblue}{RGB}{200,220,255}
\definecolor{tablepurple}{RGB}{222,210,222}
\definecolor{tableyellow}{RGB}{255, 255, 200}

\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\definecolor{darkgreen}{rgb}{0.2, 0.5, 0.2}

\newcommand{\xmark}{\text{\ding{55}}} % Define \xmark as an "X" symbol
\newcommand{\cmark}{\textcolor{darkgreen}{\ding{51}}} % Green checkmark

% \usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Import additional packages in the preamble file, before hyperref
\input{preamble}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, 
% e.g. with the file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete *.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you should be clear).
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
% \definecolor{cvprblue}{rgb}{0.49,0.74,0.21}
\usepackage[pagebackref,breaklinks,colorlinks,allcolors=cvprblue]{hyperref}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\paperID{5169} % *** Enter the Paper ID here
\def\confName{CVPR}
\def\confYear{2025}

%%%%%%%%% TITLE - PLEASE UPDATE
% \title{SuperCLEVR-6D: A Diagnostic Benchmark for
% 6D Spatially Visual Reasoning}
% \newcommand{\dataset}{PulseCheck\textcolor[RGB]{277,13,13}{4}\textcolor[RGB]{245,171,0}{5}\textcolor[RGB]{15,78,215}{7}}
\newcommand{\dataset}{PulseCheck457}

\usepackage[most]{tcolorbox}
\newcommand{\hypbox}[2]{%
\begin{tcolorbox}[colback=white!98!black,colframe=white!30!black,boxsep=1.1pt,top=6.75pt]%
\vspace{1.75pt}%
\textbf{#1}\\[-0.575em]
\noindent\makebox[\textwidth]{\rule{\textwidth}{0.4pt}}
\\[0.25em]
#2
\end{tcolorbox}
}

\newtcbox{\grayround}[1][]{%
    on line, % Ensures it works inline
    colback=gray!12, % Background color
    colframe=gray!12, % Border color
    % sharp corners=south, % Ensures rounded corners
    arc=1mm, % Adjust corner roundness
    boxrule=0mm, % Border thickness
    % left=2mm, right=2mm, top=1mm, bottom=1mm, % Padding
    left=0.5pt, right=0.5pt, top=0.2pt, bottom=0.2pt, % Padding
    #1 % Pass additional optional arguments
}


\title{\dataset{}: A Diagnostic Benchmark for 6D Spatial Reasoning \\ of Large Multimodal Models}

%%%%%%%%% AUTHORS - PLEASE UPDATE
% \author{First Author\\
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }

% Xingrui Wang, Wufei Ma, Tiezheng Zhang, Celso M de Melo, Jieneng Chen, Alan Yuille

 
\author{%
Xingrui Wang\textsuperscript{$1$}
\enskip Wufei Ma\textsuperscript{$1$}
\enskip Tiezheng Zhang\textsuperscript{$1$}  \\
\enskip Celso M de Melo\textsuperscript{$2$}
\enskip Jieneng Chen\textsuperscript{$1$}
\enskip Alan Yuille\textsuperscript{$1$} \\ 
{\textsuperscript{$1$} Johns Hopkins University    } \enspace 
{\textsuperscript{$2$} DEVCOM Army Research Laboratory} \enspace
% \texttt{\{xwang378, wma27, angtianwang ,ayuille1\}@jhu.edu} \\
% \texttt{chenshuo20@mails.tsinghua.edu.cn}
% \qquad \texttt{akortyle@mpi-inf.mpg.de}
}


\begin{document}
\maketitle
\begin{abstract}
% Although large mutlimodal models (LMMs) have demonstrated exceptional abilities in interpreting visual scenes and reasoning, their capacity for complex and precise spatial reasoning remains unclear. Existing benchmarks primarily assess 2D spatial understanding of objects, neglecting the comprehensive 6D spatial reasoning that requires knowledge of both the 3D positions and 3D orientations of objects. 
%
% To address this gap, we introduce \dataset{}, an unbiased and scalable synthetic dataset rendered in a realistic style, specifically designed to evaluate the 6D spatial reasoning abilities of LMMs. Our dataset universally measure the spatial reasoning as 4 capabilities -- multiple objects, 2D locations, 3D locations and 3D orientations. Based on the  the compositona of each fa  
% offers a diverse range of question types varying in spatial complexity—from basic semantic queries to comprehensive 6D spatial reasoning tasks—ensuring a thorough assessment across different dimensions.
%
% We conduct comprehensive benchmarking of various LMMs architectures to examine how design choices impact 6D spatial reasoning performance, providing valuable insights into which components enhance this capability. Our findings highlight the strengths and limitations of current large multimodal models. Additionally, our benchmark offer guidance for benchmarking LMMs in complex real world, facilitating advancements in practical applications that demand precise spatial awareness.

Although large multimodal models (LMMs) have demonstrated remarkable capabilities in visual scene interpretation and reasoning, their capacity for complex and precise 3-dimensional spatial reasoning remains uncertain. Existing benchmarks focus predominantly on 2D spatial understanding and lack a framework to comprehensively evaluate 6D spatial reasoning across varying complexities.
%
To address this limitation, we present \dataset{}, a scalable and unbiased synthetic dataset designed with \textbf{4} key capability for spatial reasoning: multi-object recognition, 2D location, 3D location, and 3D orientation. We develop a cascading evaluation structure,  constructing \textbf{7} question types across \textbf{5} difficulty levels that range from basic single object recognition to our new proposed complex 6D spatial reasoning tasks.
%
We evaluated various large multimodal models (LMMs) on PulseCheck457, observing a general decline in performance as task complexity increases, particularly in 3D reasoning and 6D spatial tasks. To quantify these challenges, we introduce the Relative Performance Dropping Rate (RPDR), highlighting key weaknesses in 3D reasoning capabilities. Leveraging the unbiased attribute design of our dataset, we also uncover prediction biases across different attributes, with similar patterns observed in real-world image settings.


\end{abstract}  

\section{Introduction}
\label{sec:intro}

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.88\linewidth]{figures/overview-2.pdf}
   \caption{Overview of the \dataset{} benchmark. We define four core capabilities for spatial reasoning. By incorporating these capabilities step-by-step, the benchmark assesses models across five progressive difficulty levels and seven question types, ranging from single-object recognition to advanced 6D spatial relationships and collision prediction. Our evaluation demonstrates a significant performance drop for more advanced questions and highlights the gap between state-of-the-art (SoTA) models and human performance in complex spatial reasoning VQA tasks.}
   % \vspace{-0.1em}
   \label{fig:overview}
\end{figure*}

In recent years, large multimodal models (LMMs) have demonstrated remarkable reasoning capabilities, enabling them to interpret visual scenes from images and articulate their understanding through natural language. These advancements have been evaluated using a variety of benchmarks that primarily emphasize semantic features—such as object categories and appearances, spatial relationships within the 2D image plane~\cite{hu2022promptcap,goyal2017making,hudson2019gqa} (\eg left-right positioning from the camera view), and limited 2.5D features such as camera depth~\cite{tong2024cambrian}. Tasks like image captioning, visual question answering, and object detection have significantly benefited from these evaluations, driving LMMs to new heights in visual understanding.

However, a critical gap remains in assessing the ability of LMMs to comprehend and reason about the full six degrees of freedom (6D) of objects in spatial environments, which includes both 3D positions and orientations. Despite its significance in many real-world applications, such as robotics, autonomous navigation, and augmented reality, there are no available benchmarks for LMMs that evaluate 6D spatial reasoning capabilities.

Building a scalable diagnostic dataset for 6D vision tasks poses considerable challenges due to the extensive annotations required across vast image collections. One strategy is to develop 6D vision question-answering benchmarks on top of existing image datasets. However, real-world image collections often exhibit significant biases in 3D locations and poses, influenced by camera perspective limitations and the prevalence of common object orientations in everyday scenes. For instance, statistics from the SUN-RGBD~\cite{song2015sun} and nuScenes~\cite{caesar2020nuscenes} datasets reveal that over 70\% of objects cluster into a single predominant direction (see \cref{fig:sunrgbd}), limiting the diversity required for comprehensive 6D reasoning evaluation.

% \vspace{-0.2em}
\begin{figure}[!h]
  \centering
   \includegraphics[width=0.85\linewidth]{figures/distribution.pdf}
   \caption{The distribution of 3D poses in representative 6D datasets (a\&b) highlights an \textbf{imbalance} in object orientations. This inspired us to develop a \textbf{balanced} benchmark encompassing all degrees of 3D orientation.}
   \label{fig:sunrgbd}
   % \vspace{-0.5em}
\end{figure}

To overcome these challenges, we introduce an unbiased and scalable synthetic dataset rendered in a realistic style, specifically designed to diagnose the 6D spatial reasoning abilities of LMMs. To comprehensively evaluate diverse aspects of 6D spatial reasoning, we focus on four core capabilities: \textbf{multi-object recognition}, \textbf{2D locations}, \textbf{3D locations}, and \textbf{3D orientation}. Multi-object recognition serves as the foundation of scene understanding, while 2D location represents the simplest way to depict the spatial relationship between objects within the image plane. The 3D location factor extends understanding into three-dimensional space, capturing the depth and distance of objects, which is essential for tasks such as reasoning about occlusions. The 3D orientation introduces object rotation, enabling models to reason about precise 3D poses of objects within a scene.

By progressively incorporating the four core capabilities, we establish five levels of difficulty, resulting in seven distinct question types. These questions span a spectrum of spatial reasoning complexity, ranging from basic semantic identification and 2D spatial relationships to advanced 3D positional understanding and orientation-based queries, culminating in comprehensive 6D spatial reasoning tasks.

Our evaluation measures performance across varied question types and examines how it is affected by different combinations of the four capabilities. This tiered design provides comprehensive benchmarks for spatial reasoning capabilities, highlighting each model's strengths and areas of vulnerability. To the best of our knowledge, this work presents the first comprehensive benchmark designed to evaluate the 6D spatial reasoning capabilities of vision-language models.

Additionally, our dataset's unbiased attribute design allows us to analyze potential biases in model predictions. By systematically exploring model performance under different attributes, we identify tendencies or limitations that may affect real-world applications. Altogether, our benchmark not only provides a foundational tool for evaluating LMMs but also serves as a guide for developing future benchmarks that support models with advanced 6D spatial reasoning.

Our main contributions are threefold:
\begin{enumerate}
    \item \textbf{An unbiased and scalable synthetic dataset}: We present a realistic, unbiased synthetic dataset designed to assess 6D spatial reasoning in LMMs. We define four core capabilities and propose five difficulty levels for spatial reasoning VQA with seven question types.
    \item \textbf{Comprehensive benchmarking of 6D capabilities}: We benchmark various LMMs across all difficulty levels. The performance variations across settings reveal potential limitations or strengths, providing valuable insights into which components should be prioritized to improve the spatial reasoning of LMMs.
    \item \textbf{Bias analysis in model predictions}: Leveraging the unbiased nature of our dataset, we conduct an in-depth bias analysis to explore how LMM predictions may be unbalanced across different attributes, especially 3D-related ones. This reveals potential weaknesses in model predictions, offering insights into limitations that may also appear in real-world scenarios.
\end{enumerate}


\section{Related Work}

\subsection{Synthetic Datasets in VQA}
The use of synthetic data in visual question answering (VQA) has gained traction as a means to circumvent the limitations and biases inherent in real-world datasets. Synthetic environments~\cite{hess2013blender, qiu2016unrealcv, sanders2016introduction} allow for controlled manipulation of scene elements, enabling researchers to probe specific reasoning abilities in VQA models without the confounding factors often present in natural imagery. Previous works~\cite{johnson2017clevr, lindstrom2022clevr, li2023super} have utilized synthetic environments to evaluate multimodal models across a range of visual reasoning tasks, typically focusing on 2D spatial relationships and object features. For instance, CLEVR~\cite{johnson2017clevr}, a synthetic dataset widely used in the VQA community, evaluates compositional reasoning by introducing artificially generated scenes with diverse object arrangements. The follow-up work, Super-CLEVR~\cite{li2023super}, incorporates a broader range of out-of-distribution domains. While Super-CLEVR-3D~\cite{wang20243d} represents a significant step forward in evaluating 3D locations, it lacks an assessment of 6D spatial relationships that consider both 3D location and 3D orientation. Our work extends this tradition by creating a synthetic dataset tailored for 6D spatial reasoning, filling a gap in current synthetic datasets that do not address the complete spatial understanding of objects in 3D space.

\subsection{LMM Benchmarks}
Existing benchmarks~\cite{goyal2017making, hudson2019gqa, singh2019towards, yue2024mmmu} for large multimodal models (LMMs) predominantly assess their performance on tasks involving 2D visual question answering and multimodal reasoning. For instance, datasets like VQAv2~\cite{goyal2017making} and GQA~\cite{hudson2019gqa} focus on a model's ability to answer questions based on visual content, while multimodal reasoning datasets, such as MMMU~\cite{yue2024mmmu}, evaluate the model's integration of visual and linguistic information to achieve complex reasoning. However, these benchmarks primarily emphasize visual understanding, leaving a gap in evaluating models' capabilities in spatial contexts. Recent benchmarks~\cite{tong2024cambrian,chen2024spatialvlm,cheng2024spatialrgpt} evaluate spatial understanding, including depth ordering for 2.5D and 3D location assessment. Our proposed benchmark goes beyond them by introducing tasks that challenge LMMs to understand and reason about 3D positions and orientations altogether, setting a new standard for 6D spatial reasoning evaluation in multimodal AI.


\vspace{-0.3em}
\section{\dataset{}: Comprehensive Spatial Reasoning Benchmark}
\vspace{-0.3em}

To comprehensively evaluate the spatial reasoning capabilities of vision-language models, we introduce a synthetic dataset named \dataset{}. We define four core capabilities for comprehensive spatial reasoning (\cref{sec:4factors}) and generate scenes with full 6D annotations (\cref{sec:scenes}). The full combination of these capabilities form the new 6D spatial relationship VQA tasks (\cref{sec:6dquestions}). 

To thoroughly assess model capabilities, we define five levels of difficulty in vision question answering (VQA) tasks, convering seven question subsets that range from single-object recognition to the newly proposed 6D spatial reasoning tasks (\cref{sec:7questions}). This structured progression in question difficulty is achieved by sequentially introducing new spatial reasoning capabilities, as illustrated in \cref{fig:overview}.

\subsection{Four Capabilities for Spatial Reasoning}
\label{sec:4factors}
We define 4 capabilities for comprehensive spatial understanding in a 3D scene: \textit{multiple objects}, \textit{2D locations}, \textit{3D locations}, and \textit{3D orientations}. 

% \textbf{(1) Multiple objects}. Objects are fundamental components in defining scene structure~\cite{yuille2006vision}. When dealing with fewer objects, large vision-language models have shown promising results in aligning an image’s semantic features with textual descriptions~\cite{radford2021learning,li2023blip}. However, in scenarios with multiple objects, the model must not only recognize each object's individual attributes but also interpret the relationships among them, which introduces additional challenges. In our dataset, we assess the model’s ability in multiple-object reasoning tasks, such as comparison and counting. For example, in \cref{fig:overview}, the image shows two cyan objects, requiring the model to determine whether their colors match and to accurately count the number of cyan objects when queried.

\textbf{(1) Multiple objects}. Objects are fundamental to defining scene structure~\cite{yuille2006vision}. Large vision-language models show strong results in aligning semantic features with text~\cite{radford2021learning,li2023blip}, handling multiple objects from scene becomes the first toward scene understanding. Models must recognize individual attributes and interpret relationships among them. Our dataset evaluates multi-object reasoning, such as comparison and counting. For example, in \cref{fig:overview}, the image shows two cyan objects, requiring the model to verify color similatiry and count cyan objects accurately.


% \textbf{(2) 2D locations}. The 2D spatial relationship plays a crucial role in previous image-based vision question answering tasks~\cite{johnson2017clevr,grunde2021agqa}. In traditional multimodal models~\cite{kamath2021mdetr,mao2019neuro,yi2018neural}, spatial relationships are effectively learned through object detection methods~\cite{ren2016faster,carion2020end}. However, in large-scale vision-language models, explicit location information is often absent from the latent representations. Following \citet{li2023super}, we consider  four spatial relationships (\texttt{left}, \texttt{right}, \texttt{front}, \texttt{back}) between objects in the camera view. Questions utilize these relationships to filter targeted objects, for example, asking about the question ``What is the shape of the object to the left of the chopper? (see \cref{fig:overview}).

\textbf{(2) 2D locations}. 2D spatial relationships are essential in prior image-based VQA tasks~\cite{johnson2017clevr,grunde2021agqa}. In traditional multimodal models~\cite{kamath2021mdetr,mao2019neuro,yi2018neural}, these relationships can be learned through object detection training~\cite{ren2016faster,carion2020end}, However, in large-scale vision-language models, explicit location information is often absent from the latent representations. Following \citet{li2023super}, we examine four spatial relationships (\texttt{left}, \texttt{right}, \texttt{front}, \texttt{back}) between objects from 2D camera views as the first taks of spatial reasoning. Questions use these relations to filter targets, such as “What is the shape of the object to the left of the chopper?” (see \cref{fig:overview}).


% \textbf{(3) 3D Locations}. Beyond 3D orientations, 3D spatial relationships capture depth, distance, and the relative positions of objects within a three-dimensional space. 
% In vision questions answering, the 3D locations is first introduced in \
% \cite{cheng2024spatialrgpt}

% \textbf{(3) 3D locations}. Beyond 2D spatial relationships, 3D locations consider depth or precise positioning of objects within a three-dimensional space, capturing relationships that cannot be represented in 2D alone. This factor is essential for tasks involving occlusions, and complex spatial relationship. In our dataset, we assess models' understanding of 3D locations through questions that require interpreting occlusions or spatial arrangements from different perspectives as introduced in \cite{wang20243d}. For instance, as shown in \cref{fig:overview}, questions might ask about the position of an object in relation to others in 3D space, or whether one object is occluded by others, challenging models to accurately judge relative distances and spatial positions.

\textbf{(3) 3D locations}. Beyond 2D relationships, 3D locations can incorporate depth, capturing spatial relationships that 2D alone cannot represent. This is crucial for tasks involving occlusions and complex spatial arrangements. Our dataset assesses the 3D location understanding abilities of model through questions about occlusions query as in \cite{wang20243d}. For example, as shown in \cref{fig:overview}, questions may ask about an object’s position relative to others in 3D space or whether one object is occluded, challenging models to judge distances and spatial positions accurately.




% \textbf{3D orientations}
% \textbf{(4) 3D Orientations}. The 3D orientation of objects provides a more comprehensive representation of their spatial attributes, extending beyond the limitations of 2D location data. In traditional computer vision tasks, 3D orientations are often derived using methods such as pose estimation~\cite{}, which are critical for applications that require understanding an object’s rotation and alignment in three-dimensional space~\cite{}. However, large-scale vision-language models typically do not include explicit 3D orientation information in their latent representations, making it challenging for these models to reason about an object's pose or orientation.

% In our dataset, we evaluate the model's ability to understand and reason about 3D orientations. Specifically, we focus on object poses, such as rotations and parallel alignments, adding a new aspect of complexity to spatial reasoning. For example, as shown in \cref{fig:overview}, the model might need to identify if a cyan object is oriented parallel to a brown object or if two objects are facing each other. 



\textbf{(4) 3D orientations}. The 3D orientation of objects introduces an additional layer of complexity by incorporating rotational attributes. In current computer vision tasks, 3D orientation is often derived by pose estimation methods. However, large-scale vision-language models generally lack explicit 3D orientation information in their latent representations, potentially making it challenging to accurately infer the object's orientations.
In our dataset, we evaluate models' abilities to understand 3D orientations by focusing on object poses, including their facing direction and alignments with others. For example, as depicted in \cref{fig:overview}, the model may be asked to identify whether a cyan object is facing left, oriented parallel to a brown object, or positioned in a specific spatial relationship — i.e. left, right, front, or behind— from the perspective of the objects themselves.

% \begin{table*}[ht]
% \centering
% \begin{tabular}{l|ccccccc}
% \toprule
% \multicolumn{1}{c|}{\textbf{Difficulty Level}} & \multicolumn{1}{c}{Level 1} & \multicolumn{1}{c}{Level 2} & \multicolumn{1}{c}{Level 3} & \multicolumn{2}{c}{Level 4} & \multicolumn{2}{c}{Level 5} \\
% \midrule
% \textbf{Factor} & Single Object & Multiple Objects & 2D Spatial & 3D Occlusion & 3D Pose & Collisions & 6D Spatial \\
% \midrule
% Multiple Objects & \xmark & \cmark & \cmark & \cmark & \cmark & \cmark & \cmark \\
% 2D Location & \xmark & \xmark & \cmark & \cmark & \cmark & \cmark & \cmark \\
% 3D Location & \xmark & \xmark & \xmark & \cmark & \xmark & \cmark & \cmark \\
% 3D Orientation & \xmark & \xmark & \xmark & \xmark & \cmark & \cmark & \cmark \\
% \bottomrule
% \end{tabular}
% \caption{Comparison of difficulty levels with factors and requirements}
% \end{table*}

\vspace{-0.3em}
\subsection{Scene Rendering}
\label{sec:scenes}
\vspace{-0.3em}

We build upon the generation pipeline from prior VQA datasets \citep{johnson2017clevr,li2023super}. To improve the realism of our rendered dataset, we use real image style environment maps and refined object textures. For benchmarking new 6D spatial understanding ability, we add new 6D spatial relationship annotations among objects. 

\textbf{Background}. To create realistic scene backgrounds, we use HDRI images\footnote{\url{https://polyhaven.com/hdris}} with environment maps and  natural lightings. We selected 224 different outdoor scenes with normal brightness levels without high light contrast by their metadata to maintain visual consistency across images.


\textbf{Objects}. We reconstruct 3D mesh models \citep{li2023super} with realistic textures, introducing more variation and realistic appearances for objects. Similar to \citet{li2023super}, each object in our dataset is defined by distinct attributes and precise spatial locations. The original attributes include \texttt{shape}, \texttt{color}, and \texttt{size}. For benchmarking 6D spatial reasoning, each object also includes a vector specifying its 2D location \texttt{(x, y)}, 3D world coordinates \texttt{(X, Y, Z)}, and \texttt{pose} direction derived from its 3D orientation~\cite{wang20243d}.

\textbf{Relationships}. As shown in \cref{fig:programs} (a.1 to a.3), we define three types of spatial relationships:
%
1. \textit{2D Spatial Relationships}: These describe the relative positioning of objects within the 2D camera view, using terms such as \texttt{left}, \texttt{right}, \texttt{front}, and \texttt{behind}.
%
2. \textit{6D Spatial Relationships}: Extending spatial descriptions into 3D space, these relationships incorporate both the 3D position and orientation of objects to define relative positions from a target object's perspective, including \texttt{left}, \texttt{right}, \texttt{front}, and \texttt{behind}.
%
3. \textit{Collision Relationships}: This relationship describes potential collisions if the target object moves \texttt{forward} or \texttt{backward}. Collision prediction involves assessing the orientation and 3D location of target objects relative to others to determine possible collisions.


% \textbf{Scene rendering}. We follow the same scene structure and rendering pipeline as \citet{li2023super}. We improve the appearance of background and objects with realistic textures~\citep{wang2024compositional} as shown in \cref{fig:overview}.


% \textbf{Spatial relationship}
% \begin{itemize}
%     \item 2D spatial relationship
%     \item 3D spatial relationship
%     \item occlusion
%     \item future collision
% \end{itemize}

\begin{figure}[!h]
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.8\linewidth}{0pt}}
   \includegraphics[width=0.85\linewidth]{figures/programs.pdf}
   \vspace{-0.2em}
    \caption{ (a.1-3) Example of the 2D spatial, 3D spatial and collision with images and questions. (b.1-2) shows the new operation programs we generated for the new 6D spatial reasoning questions.}
    \vspace{-1em}
   \label{fig:programs}
\end{figure}


\vspace{-0.3em}
\subsection{New Questions for 6D Spatial Reasoning}
\label{sec:6dquestions}
\vspace{-0.3em}

Based on the four capabilities defined in \cref{sec:4factors}, we construct new question types for \textbf{6D spatial reasoning} and \textbf{collision prediction}. These questions require an understanding of both 3D location and 3D orientation across multiple objects to generate accurate answers. This level of complexity represents the L5 difficulty level shown in \cref{fig:overview}.

The 6D spatial relationship questions challenge the model to understand the spatial relationship involving both 3D position and orientation from a target object's perspective (shown in \cref{fig:programs} (a.2)). Compared with prior VQA benchmarks focusing on 2D~\cite{johnson2017clevr,li2023super}, we introduce new 3D spatial reasoning operations, \texttt{3D Relate($\cdot$)}, to determine relative positions like \texttt{left}, \texttt{right}, \texttt{front}, and \texttt{behind} within a 3D coordinate space (see \cref{fig:programs} (b.1)). For example, a question might ask, “How many objects are to the right of the red wagon?” requiring the model to filter and count objects based on their orientation and relative 3D position, which would yield different answers compared to similar 2D left and right questions (\cref{fig:programs} (a.1)). This task is particularly challenging as it requires models to integrate 3D spatial relationships while managing occlusions and depth perception—capabilities not typically covered in simpler 2D spatial tasks.

The collision prediction questions, as shown in \cref{fig:programs} (a.3), extend 6D spatial reasoning by introducing motion-based inference. These questions assess the model’s ability to anticipate future states and potential interactions (collisions) based on accurate estimation of 3D location and direction, denoted as \texttt{forward} or \texttt{backward}. For instance, a question might ask, ``Will the red wagon collide with a yellow bus if it moves forward?”. This requires the model to consider the orientation and direction of movement relative to surrounding objects to predict possible outcomes. Such tasks test the model's advanced spatial reasoning about 3D orientation and position, and its capacity to predict accurate trajectories and directions within the future scene.


\subsection{Questions under Other 4 Difficulty Levels}
\label{sec:7questions}

The L5 questions above represent the most challenging cases for 6D spatial reasoning, testing all four capabilities in \cref{sec:4factors}. However, the purpose of this benchmark is to comprehensively diagnose each factor in spatial reasoning. Thus, we design a structured roadmap to evaluate the model's performance across different levels of spatial reasoning, assessing its capabilities in each specific factor.

By composing the four factors of comprehensive spatial reasoning, we define an additional four levels of question difficulty based on the generated scenes, encompassing a total of seven tasks in our benchmark. As the complexity of spatial reasoning increases with the introduction of additional factors, we organize these questions into a structured roadmap, as illustrated in \cref{fig:overview}. Although many questions are reimplementations from previous benchmarks~\cite{li2023super,wang20243d} on our new images, we aim to integrate these into a unified benchmark to provide deeper insights into the model's spatial reasoning capabilities. The composition of the four capabilities at each level, along with comparisons to previous VQA benchmarks, is shown in \cref{tab:levels}.


% From Level 1 to Level 3, questions are generated using selected templates from \cite{li2023super} applied to our newly defined scenes. Level 4 questions are adapted from \cite{wang20243d}. For Level 5, we design novel tasks with new question templates that introduce 6D spatial relationships and collision prediction. By progressively incorporating the four factors, our benchmark enables a detailed evaluation of spatial reasoning capabilities. This approach allows us to assess models by comparing their performance across different question pairs and task levels.

\textbf{Level 1 - single object questions}~\cite{johnson2017clevr,li2023super}. We begin with the questions with single objects. These questions are generated using a filtered set of templates from \citet{li2023super}. We select five templates involving only one object in the question, such as ``What color is the double bus?" in \cref{fig:overview}.
This question type provides a base score for the tested vision-language model. This performance offers insight into how well the model performs within the domain of our generated scenes. When evaluating the model on more complex questions, the comparison with L1 helps to determine whether the decrease of performance are due to challange of spatial reasoning or object recognition.

\textbf{Level 2 - multiple objects questions}~\cite{johnson2017clevr,li2023super}. Level 2 questions extend Level 1 by involving multiple objects. These questions require recognizing each object and understanding the relationships between them, such as matching colors or counting. For example, “Is there another object of the same color as the double bus?” as shown in \cref{fig:overview}. We selected question templates from \citet{li2023super} and applied them to our new images. By adding this level of complexity, we evaluate the model's capacity for reasoning in scenes with multiple entities, which is a critical step toward understanding more intricate spatial relationships in subsequent levels.

\textbf{Level 3 - 2D spatial questions}~\cite{johnson2017clevr,li2023super}. Level 3 questions further introduce 2D spatial relationships among multiple objects, requiring the model to understand the position of objects and their relationships from the 2D camera's perspective. For example, “What shape is the object to the left of the chopper?” as depicted in \cref{fig:overview}.
%
This level serves as the first step toward spatial relationships, challenging the model’s capacity to interpret spatial layouts within the 2D image plane.

\textbf{Level 4 - 3D pose questions}~\cite{wang20243d}. Level 4 questions extend 3D orientation reasoning beyond the 2D spatial relationships in Level 3. Beyond appearance attributes (\texttt{shape}, \texttt{color}, \texttt{size}) or 2D locations, these questions require understanding 3D orientation, including the rotation angle or facing direction of an object from the camera's perspective. For example, “What shape is the object that is parallel to the brown object?” as shown in \cref{fig:overview}. 3D pose questions test the model's ability to interpret scenes with 3D awareness, involving rotation and alignment with other objects.

\textbf{Level 4 - occlusion questions}~\cite{wang20243d}. In addition to 3D orientation, incorporating 3D location presents another challenge beyond the Level 3 questions on 2D spatial relationships. A unique aspect of 3D spatial reasoning from images is occlusion. The occlusion questions in Level 4 require the model to comprehend depth perception and the relative positions of objects in 3D space, determining which object is closer to the camera within overlapping areas. For example, an occlusion question might ask, “Is the red wagon occluded by the yellow bus?” as shown in \cref{fig:overview}. This level evaluates the model's ability to reason about occlusions and depth perception, which are essential for understanding complex spatial relationships in 3D scenes.
\vspace{-1em}

\begin{table}[!h]
\centering
\renewcommand{\arraystretch}{1.5}
\resizebox{1.00\columnwidth}{!}{
\begin{tabular}{c|cccc|c}
% \toprule
\makecell{Question\\ Level}  & \rotatebox{90}{Multi-object}& \rotatebox{90}{2D location} & \rotatebox{90}{3D location} & \rotatebox{90}{3D Orientation} & VQA Benchmarks \\
\midrule
L1 &  &  & & & spatial-agnostic bench (e.g., TextVQA~\cite{singh2019towards}) \\
L2 & $\checkmark$ &  &  & & CLEVR-Math~\cite{lindstrom2022clevr}, CLEVR-Count~\cite{johnson2017clevr} \\
L3 & $\checkmark$ & $\checkmark$ & & &  VQA~\cite{goyal2017making}, GQA~\cite{hudson2019gqa}, CLEVR~\cite{johnson2017clevr}, Super-CLEVR~\cite{li2023super} \\
\hline
L4 (a)  & $\checkmark$ & $\checkmark$ & $\checkmark$ & &  CV-Bench (DepthOrder)~\cite{tong2024cambrian}, Super-CLEVR-3D~\cite{wang20243d}  \\
L4 (b) &  $\checkmark$ & $\checkmark$ & & $\checkmark$ & Super-CLEVR-3D~\cite{wang20243d}  \\
\hline
L5 & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & \textbf{PulseCheck457}   \\
% \bottomrule
\end{tabular}
}
\caption{Comparison of VQA Benchmarks by Factors and Question Levels. Compared with existing VQA benchmarks, \dataset{} includes all four spatial reasoning factors, providing a comprehensive evaluation of 6D spatial understanding.}
\vspace{-0.8em}
\label{tab:levels}
\end{table}


\vspace{-0.7em}
\subsection{Evaluation Protocols}
\label{sec:RPDR}
\vspace{-0.3em}
To better evaluate the model's spatial reasoning capabilities, we propose a structured evaluation protocol that measures the relative capability of models on each factor for spatial reasoning. We define this evaluation metric as the Relative Performance Dropping Rate (RPDR), which assesses the decline in performance after introducing a new factor in the questions. Based on the roadmap in \cref{fig:overview}, the RPDR for each factor is defined as follows:
% \vspace{-0.1em}
\vspace{-0.5em}
{\small
\begin{align*}
    \text{RPDR}_{\text{Multi-Obj.}} &= Acc_{\text{Level 1}} \Rightarrow Acc_{\text{Level 2}} \\ 
    \text{RPDR}_{\text{2D Loc.}} &= Acc_{\text{Level 2}} \Rightarrow Acc_{\text{Level 3}} \\
    \text{RPDR}_{\text{3D Loc.}} &= \text{Avg} \left( 
    \begin{array}{c}
        Acc_{\text{Level 3}} \Rightarrow Acc_{\text{Level 4 (Occ.)}},\\
        Acc_{\text{Level 4 (3D Pose)}} \Rightarrow Acc_{\text{Level 5 (6D)}}
    \end{array}
    \right) \\
   \text{RPDR}_{\text{3D Ori.}} &= \text{Avg} \left( 
    \begin{array}{c}
        Acc_{\text{Level 3}} \Rightarrow Acc_{\text{Level 4 (3D Pose)}},\\
        Acc_{\text{Level 4 (Occ.)}} \Rightarrow Acc_{\text{Level 5 (Col.)}}
    \end{array}
    \right)
\end{align*}}
where $Acc_{\text{Level}}$ denotes the model's accuracy on the corresponding questions and $(Acc_{\text{Q1}} \Rightarrow Acc_{\text{Q2}})=\frac{Acc_{\text{Q2}}}{Acc_{\text{Q1}}}$ denotes the relative performance from $Q1$ to $Q2$. The results of RPDR will be discussed in \cref{sec:RPDR_results}. 


\begin{table*}[!t]
\centering
\small
\begin{tabular}{l|ccccccc}
\toprule
\textbf{Difficulty Level} & \multicolumn{1}{c}{Level 1} & \multicolumn{1}{c}{Level 2} & \multicolumn{1}{c}{Level 3} & \multicolumn{2}{c}{Level 4} & \multicolumn{2}{c}{Level 5} \\
\midrule
\textbf{Questions type} & Single Object & Multi-Obj. & 2D Spatial & Occlusion & 3D Pose & Collisions & 6D Spatial \\
\midrule
Random & 33.05 & 32.77 & 33.47 & 22.04 & 18.99 & 21.02 & 19.41 \\
\midrule
GPT-4o & \cellcolor{tablered}74.46 & \cellcolor{tablered}62.88 & \underline{56.14} & \cellcolor{tablered}48.40 & \underline{42.41} & \underline{38.41} & \underline{37.01} \\
GeminiPro 1.5 & \underline{73.26} & \underline{62.54} & 54.49 & \underline{47.65} & \cellcolor{tablered}43.67& \cellcolor{tablered}41.19 & \cellcolor{tablered}39.36 \\
Claude 3.5 Sonnet & 68.24 & 57.40 & 54.19 & 30.84 & 38.40 & 35.34 & 33.48 \\
\midrule
Qwen2-VL-7B-Inst. & \cellcolor{tableblue}71.96 & \cellcolor{tableblue}61.44 & 55.34 & 27.87 & \cellcolor{tableblue}34.29 & \cellcolor{tableblue}36.58 & 33.75 \\
InternVL2-8B & 58.11 & 58.76 & \cellcolor{tablered} 57.40 & \cellcolor{tableblue}33.25 & 32.32 & 34.30 & \cellcolor{tableblue}34.30 \\
LLaVA-v1.5-7B & 44.87 & 44.72 & 42.20 & 24.34 & 24.55 & 23.63 & 23.86 \\
LLaVA-NeXT-vicuna-7B & 50.72 & 49.47 & 46.01 & 29.68 & 29.35 & 31.95 & 31.95 \\
LLaVA-NeXT-llama3-8B & 52.15 & 49.73 & 45.92 & 30.31 & 29.77 & 32.12 & 32.12 \\
\midrule
PO3D-VQA & 86.46 & 82.55 & 80.64 & 70.49 & 81.40 & 68.12 & 71.06 \\
Human & 89.97 & 86.83 & 84.95 & 82.76 & 84.95 & 81.82 & 79.94 \\
\bottomrule
\end{tabular}
\caption{Performance comparison across all 7 question types under 5 difficulty levels. As task complexity increases from single-object questions to 6D spatial relationships, performance generally decreases in most cases, highlighting the challenges in handling multi-object interactions, 3D orientations, and predictive spatial reasoning. The highest score among all models for each question type is marked in \colorbox{tablered}{red}, and the second-highest scores are \underline{underlined}. The best performance among open-source models is marked in \colorbox{tableblue}{blue}. PO3D-VQA is the state-of-the-art (SOTA) 3D-aware neural symbolic reasoning model as proposed in ~\cite{wang20243d}, and is listed as the potential upper bound of evaluated methods. The comparison with human performance also shows an increasing gap for LLMs as task difficulty increases.}
\label{tab:performance}
\end{table*}

\vspace{-0.4em}
\section{Experiments}
\vspace{-0.3em}

\subsection{Experimental Setup}
\vspace{-0.3em}
We generate 1,000 images to test the performance of large vision-language models. For the seven types of questions, we generate 1000 single-object questions (\textit{L1-Single}), 5,000 multiple-object questions (\textit{L2-Multi-obj.}), 4,995 2D spatial relationship questions (\textit{L3-2D-Spatial}), 3,534 occlusion questions (\textit{L4-Occlusion}), 4,555 3D pose questions (\textit{L4-3D-Pose}), 4,357 collision questions (\textit{L5-Collision}), and 4,995 6D spatial relationship questions (\textit{L5-6D-Spatial}).

We evaluate two series of large vision-language models on \dataset{}: (1) API-based large language models, including GPT-4o~\cite{openai2023gpt4}, Gemini-1.5-Pro~\cite{team2024gemini}, and Claude-3.5-Sonnet~\cite{anthropic2023claude}; and (2) open-source large vision-language models, including Qwen2-VL-7B-Instruct~\cite{qwen2023vl7b}, InternVL2-8B~\cite{intern2023vl2}, llava-v1.5-7b~\cite{liu2023llava}, llava-next-vicuna-7b~\cite{liu2023llavanext}, and llava-next-llama3-8b~\cite{touvron2023llama}. 
% This evaluation allows us to assess the models' spatial reasoning capabilities across multiple levels of complexity and question types, providing a comprehensive comparison of proprietary APIs versus open-source alternatives on \dataset{}.
The details of the model implementation including prompts are listed in Appendix. 

% For human testing, we randomly sample 20 questions for easy question types,
\vspace{-0.3em}
\subsection{Evaluation Results}
\vspace{-0.3em}
In \cref{tab:performance}, we present the performance of various large vision-language models across different levels of difficulty on \dataset{}. The levels range from basic single-object tasks to complex 6D spatial reasoning, with increasing difficulty as more factors are introduced. We compare both API-based models (e.g., GPT-4o, GeminiPro1.5, Claude3-5V) and open-source models (e.g., Qwen2-VL-7B-Instruct, InternVL2-8B, LLaVA series) to assess their spatial reasoning capabilities.

\textbf{Comparison across difficulty levels.} We evaluate model performance across different levels of difficulty in \dataset{}, ranging from basic single-object tasks to complex 6D spatial reasoning. As shown in \cref{tab:performance}, performance generally decreases as the complexity of spatial relationships increases, highlighting the challenges models face when handling multi-object interactions, 3D orientations, and predictive spatial reasoning tasks.

\textbf{Comparison between API models and open-source models.} From \cref{tab:performance}, we observe that API models generally outperform open-source models across all difficulty levels. In 2D spatial relationship questions, InternVL2-8B achieves slightly higher results than the API models, while GPT-4o is only 1\% lower. Qwen2-VL-7B-Instruct achieves comparable results, especially in simpler tasks like single-object and multiple-object reasoning. This analysis highlights the strengths of API models in complex spatial reasoning tasks.

\textbf{Comparison among API models.} API-based models, \ie GPT-4o, GeminiPro1.5, and Claude3-5V, demonstrate strengths across all difficulty levels. For instance, GPT-4o excels in 2D and 3D spatial tasks, while GeminiPro1.5 shows robust performance in collision prediction tasks. This comparison provides insights into the unique capabilities and limitations of each API model in handling spatial reasoning across different levels of complexity.


\vspace{-0.2em}
\subsection{RPDR Analysis for 4 Capabilities}
\label{sec:RPDR_results}
\vspace{-0.2em}

% As To further analyze model performance, we conduct a capability analysis focusing on the four core factors of \dataset{}: multi-object relationships, 2D locations understanding, 3D orientation and 3D location relationships in \cref{tab:spatial_performance}. This analysis allows us to evaluate how well each model handles distinct aspects of spatial reasoning, providing insights into their strengths and areas for improvement in relation to specific spatial tasks.


As outlined in \cref{sec:RPDR}, we use the Relative Performance Dropping Rate (RPDR) to analyze model performance across the four core factors of \dataset{}: multi-object reasoning, 2D location understanding, 3D location comprehension, and 3D orientation interpretation. The RPDR analysis provides a structured approach to quantify the decline in accuracy with the introduction of each new factor, as detailed in \cref{tab:spatial_performance}. Our findings reveal that all models exhibit weaknesses in 3D reasoning. Specifically, GPT-4o and GeminiPro-1.5 perform the worst in 3D orientation tasks, while the other two struggle most with 3D location.


\begin{table}[!h]
\centering
\small
\begin{tabular}{l|cccc}
\toprule
\textbf{Model} & \textbf{Multi.} & \textbf{2D Loc.} & \textbf{3D Ori.} & \textbf{3D Loc.} \\
\midrule
GPT-4o & 84.45 & 89.27 & \red{\textbf{77.46}} & 86.74 \\
GeminiPro1-5 & 85.37 & 87.13 & \red{\textbf{83.28}} & 88.79 \\
Claude3-5V S & 84.12 & 94.41 & 85.43 & \red{\textbf{72.05}} \\
Qwen2-VL-7B & 85.39 & 90.06 & 80.99 & \red{\textbf{74.40}} \\
\bottomrule
\end{tabular}
% \vspace{-0.3em}
\caption{RPDR (\%) analysis on the four spatial reasoning capabilities, where higher values indicate better performance. 
% For each row, we \colorbox{tableyellow}{highlight} the lowest RPDR score among all factors.
For each row, we \red{\textbf{mark}} the lowest RPDR score among all factors of each model to show their weakness.
}
% \vspace{-0.3em}
\vspace{-1em}

\label{tab:spatial_performance}
\end{table}

\vspace{-0.3em}
\subsection{Bias Analysis}
\vspace{-0.2em}
In this section, we analyze prediction biases in large language models (LLMs) regarding their understanding of key attributes, specifically shape, color, size, and pose. Leveraging the unbiased generation of dataset images and questions in \dataset{}, we examine how these models may demonstrate attribute-specific biases in their predictions.

% In this section, we analyze prediction bias in large vision-language models with respect to attribute understanding, specifically in shape, color, size, and pose predictions. Leveraging the unbiased generation of dataset images and questions in \dataset{}, we evaluate how model predictions align with ground truth across these attributes.

We first compute the confusion matrices and distribution statistics for both ground truth and model predictions across each attribute (shape, color, size, and pose). For example, the confusion matrix plots for color and pose attributes of GPT-4o and GeminiPro1.5, together with the distribution of prediction and ground truth label are shown in ~\cref{fig:exp_bias}. We find both GPT-4o and GeminiPro1.5 exhibit bias in both color and pose predictions. These results highlight areas for improvement to reduce bias and enhance predictive reliability across models.

To quantify prediction variability, we calculate the Coefficient of Variation (CV) for each attribute as follows:
% \vspace{-0.42em}
{\small
\begin{equation}
    \text{CV} = \frac{\text{Std}(p_1,\ldots,p_k)}{\text{Mean}(p_1,\ldots,p_k)},
\end{equation}}
where $\text{Std}(\cdot)$ and $\text{Mean}(\cdot)$ are the standard deviation and mean of the probability distribution $p_i$ for each attribute label $i$ (e.g., left," "right" for pose). 

\cref{tab:exp_bias} shows the CV values for shape, color, size, and pose attributes across different models. Lower CV values indicate more consistent predictions with less bias, which reveals significant biases in model predictions across attributes. Higher CV values, especially in color and pose, indicate consistency challenges. 

\begin{figure}[h]
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=0.9\linewidth]{figures/exp_bias.pdf}
\caption{The distribution of \texttt{color} and \texttt{pose} attributes in the L4-3D-Pose task for GPT-4o and Gemini-Pro1.5. Although the ground truth labels are well balanced, the predicted colors and poses for both models are imbalanced, with ``yellow" being the most frequent color and ``front" the most frequent pose.}
   \label{fig:exp_bias}
\end{figure}

\begin{table}[h]
\small
\centering
\begin{tabular}{lccccc}
\toprule
\textbf{Models} & $Avg.\downarrow$ & Shape & Color & Size & Pose\\
\midrule
GPT4o & \cellcolor{tablered}0.485 & 0.623 & 0.521 & \underline{0.043} & 0.752 \\
GeminiPro1.5 & 0.498 & \underline{0.567} & \underline{0.366} & 0.297 & 0.760 \\
Claude3.5V-S & 0.720 & 0.777 & 0.465 & 0.574 & 1.063 \\
Qwen2-VL-7B & 0.545 & 0.868 & 0.558 & 0.067 & \underline{0.688} \\
\midrule
Human & 0.093 & 0.110 & 0.082 & 0.055 & 0.126 \\
% \midrule
% Ground Truth & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
\bottomrule
\end{tabular}
\caption{Coefficient of Variation (CV) for predictions on each attribute in the \textit{L4-3D-Pose} task. We evaluate three API models and one open-source model, calculating the CV across four attributes (shape, color, size, 3D pose) based on the distribution of predicted labels. Lower CV values indicate less bias. The first column shows the average CV across attributes, and GPT4-o has the lowest bias.}
\vspace{-1em}
\label{tab:exp_bias}
\end{table}

\vspace{-0.4em}
\subsection{Comparison to the 3D Symbolic Model}
\vspace{-0.4em}
% As nerual symbolic model achieve the SOTA performanc on many prior synthetic dataset, we implement the 3D aware nerual symbolic model PO3D-VQA~\cite{wang20243d} on our new images domain and all level of questions. We train the PO3D-VQA on another separated 10k images and questions and the performance on the same test dataset in ~\ref{tab:performance} . The direct comparison between 3D neural symbolic model and large language model is unfair because each module of the neural symbolic model (6D pose estimation, attributes classification, and language reasoning) is well trained independently as sub tasks. But we hope this model can shown the upper bound of a data driven machine learning model can achieved on our dataset.

As neural symbolic models achieve state-of-the-art (SOTA) performance on many prior synthetic datasets, we implement the 3D-aware neural symbolic model PO3D-VQA~\cite{wang20243d} on our new image domain and all levels of questions. We train PO3D-VQA on a separate set of 20,000 images and questions and report its performance on the same test dataset in \cref{tab:performance}. 
A direct comparison between the 3D neural symbolic model and large language models is not entirely fair, as each module of the neural symbolic model (6D pose estimation, attribute classification, and language reasoning) is trained independently as a sub-task. However, we include this model to illustrate the upper bound that a data-driven machine learning model can achieve on our dataset.


\vspace{-0.8em}
\subsection{Qualitative Results}
\vspace{-0.5em}

\cref{fig:exp_vis} presents an example of GPT-4o's performance across various spatial reasoning tasks on the same image. GPT-4o correctly answers multi-object, 2D spatial, and occlusion tasks but fails on 3D pose and 6D spatial questions, indicating challenges in 3D orientation understanding in this case.
\vspace{-0.3em}
\begin{figure}[h]
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=0.9\linewidth]{figures/vis.pdf}
   \vspace{-0.5em}
\caption{Example of GPT-4o across different spatial reasoning tasks on the same image with the ground truth answer marked in blue. GPT-4o answers multi-object, 2D spatial, and occlusion tasks correctly (in the green block) but fails on 3D pose and 6D spatial questions (the red block). }
\label{fig:exp_vis}
\end{figure}
\vspace{-0.5em}
\vspace{-0.3em}


 \vspace{-1em}
\section{Extension to Real Dataset}
\vspace{-0.4em}

We extend the setting of 3D questions to real-world image data. Due to limitations in fully annotated attributes such as category, color, 3D orientation, position, and occlusion, we focus on questions from the \textit{L4-Pose} category. Using the scene structure from SUN-RGBD~\cite{song2015sun}, we construct a new subset, \textit{L4-Pose-Real}, which involves only the category and pose attributes. To reduce ambiguity in object localization, we add a red bounding box around target objects, ensuring clarity in identifying categories within the scene.

We provide the question along with the image as input to the LLM. In this example in \cref{fig:exp_real}, GPT-4o provides the correct answer, while Gemini-Pro 1.5 is incorrect. However, the reasoning process reveals that both models rely on common sense knowledge or 2D visibility cues to interpret the 3D orientation.

\begin{figure}[h]
  \centering
   \includegraphics[width=0.9\linewidth]{figures/visulization_real.pdf}
   \vspace{-0.3em}
    \caption{Example of real-world 3D pose reasoning tasks \textit{L4-Pose-Real}. Here we shows the reasoning and answers provided by LLMs where  GPT-4o is correct and Gemini-Pro 1.5 is wrong.  But the reasoning process reveals these two LLMs are both reasoning 3D orientation from the command sense or 2D visibility.}
    \vspace{-1em}
   \label{fig:exp_real}
\end{figure}

% \vspace{-0.5em}
As shown in ~\cref{tab:exp_real}, Gemini Pro 1.5 achieves the highest accuracy (40.50\%) in \textit{L4-Pose-Real}, and the gap between the three API-based models and the open-source model Qwen2-VL-7B-Instruct (26.50\%) is larger. However, all these models perform significantly lower than humans, which have accuracy 92.64\%.  We further analyze the bias of predictions in this real-world question set. Among the models, Claude3-5V Sonnet exhibits the lowest CV (0.374), indicating relatively stable predictions, while Qwen2-VL-7B-Instruct has the highest CV (0.934). The distribution of ground truth and predicted pose and the confusion matrix are shown in \cref{fig:exp_bias_real}.

\begin{table}[ht]
\centering
\small
\begin{tabular}{lcc}
\toprule
% Model & $Accuracy\uparrow$ &  $CV\downarrow$ \\
\textbf{Models} & \textbf{Accuracy (\%)}  $\uparrow$  & \textbf{CV} $\downarrow$ \\
\midrule
GPT-4o & 38.75 & 0.690 \\
GeminiPro1-5 & \cellcolor{tablered}40.50 & 0.762 \\
Claude3-5V Sonnet & 38.75 & \cellcolor{tablered}0.374 \\
Qwen2-VL-7B-Instruct & 26.50 & 0.934 \\
\midrule
Human & 92.64 & 0.197 \\
\bottomrule
\end{tabular}
\vspace{-0.9em}
\caption{Accuracy and Coefficient of Variation (CV) for \textit{L4-Pose-Real} task: Comparison of accuracy and CV for GPT-4o, GeminiPro1.5, Claude3-5V, and Qwen2-VL-7B-Instruct on real-world 3D pose tasks. Human performance is included as a reference.}
\vspace{-1em}
\label{tab:exp_real}
\end{table}




\begin{figure}[h]
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=0.9\linewidth]{figures/exp_bias_real-s.pdf}
   \caption{Distribution of 3D poses for in \textit{L4-Pose-Real} task.}
   \vspace{-1em}
   \label{fig:exp_bias_real}
\end{figure}











\vspace{-0.5em}
\section{Conclusion}
\vspace{-0.7em}
In this work, we introduce \dataset{}, a scalable and unbiased benchmark designed to comprehensively assess 6D spatial reasoning capabilities in LMMs. \dataset{} extends traditional 2D and 3D VQA benchmarks by incorporating complex 6D spatial reasoning tasks, structured across five levels of difficulty. Through evaluations, we observe that current LMMs demonstrate strong performance in basic object recognition and 2D spatial relationships, but struggle significantly with 3D spatial understanding and collision prediction tasks. Additionally, our analysis of prediction biases uncovers attribute-specific inconsistencies that could affect model reliability in real-world applications. We hope \dataset{} will serve as a foundation for future research on 6D spatial reasoning, facilitating the development of robust, spatially aware multimodal models with enhanced spatial reasoning abilities.






% \input{sec/2_formatting}
% \input{sec/3_finalcopy}
{
    \small
    \bibliographystyle{ieeenat_fullname}
    \bibliography{main}
}

% WARNING: do not forget to delete the supplementary pages from your submission 
\appendix
% \clearpage
\setcounter{page}{1}
\setcounter{section}{0}
\renewcommand{\thesection}{\Alph{section}}
\maketitlesupplementary

\section{New programs for 6D spatial reasoning}

As introduced in \cref{sec:6dquestions}, we create two types of new questions: \textbf{6D spatial reasoning} and \textbf{collision prediction}. 

The 6D spatial reasoning questions introduce the new program \grayround{\texttt{3D Relate}}, an extension of \grayround{\texttt{Relate}} from \citet{johnson2017clevr}. It takes two arguments: a direction (\texttt{left}, \texttt{right}, \texttt{front}, or \texttt{behind}) and an object ID specifying the target.  Unlike \grayround{\texttt{Relate}}, which operates in a 2D plane from the camera's view, \grayround{\texttt{3D Relate}} works in 3D space from the object's perspective, as described in \cref{sec:6dquestions}.

For collision prediction, we introduce the new program \grayround{\texttt{Relate Collision}}. This operation expands on the spatial reasoning of \grayround{\texttt{3D Relate}} by incorporating motion-based inference. It assesses whether the movement of a target object, specified with directions like \texttt{forward} or \texttt{backward}, will result in a collision with other objects in the scene. This program enables models to predict interactions and anticipate future states based on both 3D location and orientation, pushing the boundaries of spatial reasoning to include dynamic scenarios.

\section{Examples of Benchmarks}

In this section, we present additional examples from the \dataset{} benchmark. Each example consists of an image accompanied by seven questions spanning five levels of difficulty, as shown in \cref{fig:appendix}.

\begin{figure}[h]
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=0.9\linewidth]{figures/appendix.pdf}
   \caption{\textbf{Examples from the \dataset{} Benchmark.} Each example includes an image and seven questions that test the model across five difficulty levels.}
   \label{fig:appendix}
\end{figure}



\section{Prompts for LLM}

In this section, we present the detailed prompts designed for evaluating the Large Language Models (LLMs) on the \dataset{} benchmark. Each level targets a specific skill set, ranging from identifying single objects to reasoning about spatial relationships and occlusions in 3D space.

The prompts are structured with clear instructions, encouraging the model to first describe the scene comprehensively before formulating the answer. This approach ensures that the reasoning process is explicit and easy for interpretable. The output format is standardized as a JSON object, enabling straightforward evaluation and comparison against expected results. Figures~\ref{fig:prompts_level1}--\ref{fig:prompts_level5-6d} provide detailed examples of the prompts used at each level:

% \textbf{Model Settings.} GPT-4o. GeminiPro1-5. Claude 3.5 Sonnet. Qwen2-VL-7B-Instruction. InternVL2-8B. LLaVA-v1.5-7B. LLaVA-NeXT-vicuna-7B. LLaVA-NeXT-llama3-8B

% \textbf{Prompts}
\begin{figure}[h]
    \centering
    \hypbox{Prompts for Level 1 - Single Object questions}{%
    \textcolor{ForestGreen}{\textbf{System:}} \\
    You are an intelligent chatbot designed to answer questions based on an image. 
    Your task is to analyze the images, identify attributes of the objects and then determine the answer to the question.\\

    INSTRUCTIONS:\\
    Step 1: Identify the objects mentioned in the question, noting each object's shape, color, size and their locations. Objects can have a shape (e.g., 'airliner'), size ('small' or 'large'), color (e.g., 'blue'). The possible colors are: gray, blue, purple, brown, green, cyan, red, and yellow. 

    Step 2: Use the identified details to form a brief reasoning process for answering the question. Based on this reasoning, provide your answer in one of the following formats: (1) a phrase chosen from the available options: Small, Large, Airliner, Dirtbike, Road bike, Tandem bike, Suv, Wagon, Scooter, Mountain bike, Minivan, Sedan, School bus, Fighter, Chopper, Double bus, Truck, Articulated bus, Cruiser, Jet, Utility bike, Regular bus, Biplane, Gray, Blue, Purple, Brown, Green, Cyan, Red, Yellow, Left, Right, Front, Back, (2) an integer between 0-10 for questions like 'How many' or 'What is the number of,' or (3) 'Yes' or 'No' for questions asking 'Is there.'
    
    Output your response using this JSON format:
    \{ 'Reasoning': '\textless reasoning\textgreater', 'Answer': '\textless answer\textgreater'\}
    \\

    \textcolor{RoyalBlue}{\textbf{User:}} \\
    What is the shape of the big yellow thing?\\
    

    \textcolor{Orange}{\textbf{GPT-4o:}} \\
    \{'Reasoning': 'The image contains a green utility bike with a red seat, a small white biplane, a large yellow sedan, and two purple tandem bikes. The large yellow object is a sedan.', 'Answer': 'Sedan'\}
    }
    % \vspace{2pt}
    \caption{\textbf{The prompts used for L1- Single object questions.} In the instruction, we encourage the model to first describe all the objects in the scenes and the reasoning process of answer the question. We format the reply into a dictionary format, where the answer can be decoded and compared with the ground truth answer.}
    \label{fig:prompts_level1}
\end{figure}

\begin{figure}[h]
    \centering
    \hypbox{Prompts for Level 2 - Multiple Objects questions}{%
    \textcolor{ForestGreen}{\textbf{System:}} \\
    You are an intelligent chatbot designed to answer questions based on an image. 
    Your task is to analyze the images, \textbf{identify attributes of multiple objects} and then determine the answer to the question.\\

    INSTRUCTIONS:\\
    Step 1: Identify the objects mentioned in the question, noting each object's shape, color, size and their locations. Objects can have a shape (e.g., 'airliner'), size ('small' or 'large'), color (e.g., 'blue'). The possible colors are: gray, blue, purple, brown, green, cyan, red, and yellow. 

    Step 2: Use the identified details to form a brief reasoning process for answering the question. Based on this reasoning, provide your answer in one of the following formats: (1) a phrase chosen from the available options: Small, Large, Airliner, Dirtbike, Road bike, Tandem bike, Suv, Wagon, Scooter, Mountain bike, Minivan, Sedan, School bus, Fighter, Chopper, Double bus, Truck, Articulated bus, Cruiser, Jet, Utility bike, Regular bus, Biplane, Gray, Blue, Purple, Brown, Green, Cyan, Red, Yellow, Left, Right, Front, Back, (2) an integer between 0-10 for questions like 'How many' or 'What is the number of,' or (3) 'Yes' or 'No' for questions asking 'Is there.'
    
    Output your response using this JSON format:
    { 'Reasoning': '\textless reasoning\textgreater', 'Answer': '\textless answer\textgreater', 'Related Objects': [{'shape': '\textless shape\textgreater', 'size': '\textless size\textgreater', 'color': '\textless color\textgreater'}, ...] }\\

    \textcolor{RoyalBlue}{\textbf{User:}} \\
    Is there any other thing that is the same color as the small utility bike?\\
    
    \textcolor{Orange}{\textbf{GPT-4o:}} \\
    \{'Reasoning': 'The objects in the image are: a small gray SUV, a small gray sedan, a small gray school bus, a small red utility bike, a small red tandem bike, a small red road bike, a small red mountain bike, a small red fighter jet, and a small purple biplane. The small utility bike is red. There are other objects that are also red: the tandem bike, the road bike, the mountain bike, and the fighter jet.', 'Answer': 'Yes'\}
    }
    % \vspace{2pt}
    \caption{\textbf{The prompts used for L2- Multiple object questions.} In the instruction, we encourage the model to first describe all the objects in the scenes and the reasoning process of answer the question. We format the reply into a dictionary format, where the answer can be decoded and compared with the ground truth answer.}
    \label{fig:prompts_level2}
\end{figure}

\begin{figure}[h]
    \centering
    \hypbox{Prompts for Level 3 - 2D Spatial questions}{%
    \textcolor{ForestGreen}{\textbf{System:}} \\
    You are an intelligent chatbot designed to answer questions based on an image. 
    Your task is to analyze the images, \textbf{identify attributes of multiple objects and their spatial relationship from 2D projected camera view}, and then determine the answer to the question.\\

    INSTRUCTIONS:\\
    Step 1: Identify the objects mentioned in the question, noting each object's shape, color, size and their locations. Objects can have a shape (e.g., 'airliner'), size ('small' or 'large'), color (e.g., 'blue'). The possible colors are: gray, blue, purple, brown, green, cyan, red, and yellow. 

    Step 2: Use the identified details to form a brief reasoning process for answering the question. Based on this reasoning, provide your answer in one of the following formats: (1) a phrase chosen from the available options: Small, Large, Airliner, Dirtbike, Road bike, Tandem bike, Suv, Wagon, Scooter, Mountain bike, Minivan, Sedan, School bus, Fighter, Chopper, Double bus, Truck, Articulated bus, Cruiser, Jet, Utility bike, Regular bus, Biplane, Gray, Blue, Purple, Brown, Green, Cyan, Red, Yellow, Left, Right, Front, Back, (2) an integer between 0-10 for questions like 'How many' or 'What is the number of,' or (3) 'Yes' or 'No' for questions asking 'Is there.'
    
    Output your response using this JSON format:
    { 'Reasoning': '\textless reasoning\textgreater', 'Answer': '\textless answer\textgreater', 'Related Objects': [{'shape': '\textless shape\textgreater', 'size': '\textless size\textgreater', 'color': '\textless color\textgreater'}, ...] }\\

    \textcolor{RoyalBlue}{\textbf{User:}} \\
    Is there a gray mountain bike that is to the left of the gray thing that is behind the tiny chopper?
    \\

    \textcolor{Orange}{\textbf{GPT-4o:}} \\
    \{'Reasoning': 'In the image, there are several objects including a gray car, a red car, a white bus, a purple airplane, and several bicycles. The gray car is located towards the left side of the image. To the right of the gray car, there is a red car. There are no brown bicycles to the right of the red car.', 'Answer': 'No'\}
    }
    % \vspace{2pt}
    \caption{\textbf{The prompts used for L3- 2D Spatial questions.} In the instruction, we encourage the model to first describe all the objects in the scenes and the reasoning process of answer the question. We format the reply into a dictionary format, where the answer can be decoded and compared with the ground truth answer.}
    \label{fig:prompts_level3}
\end{figure}

\begin{figure}[h]
    \centering
    \hypbox{Prompts for Level 4 - Occlusion questions}{%
    \textcolor{ForestGreen}{\textbf{System:}} \\
    You are an intelligent chatbot designed to answer questions based on an image. 
    Your task is to analyze the images, \textbf{identify attributes of multiple objects and their occlusions relationships}, and then determine the answer to the question.\\

    INSTRUCTIONS:\\
    Step 1: Identify the objects mentioned in the question, noting each object's shape, color, size and their locations. Objects can have a shape (e.g., 'airliner'), size ('small' or 'large'), color (e.g., 'blue'). The possible colors are: gray, blue, purple, brown, green, cyan, red, and yellow. 

    Step 2: Use the identified details to form a brief reasoning process for answering the question. Based on this reasoning, provide your answer in one of the following formats: (1) a phrase chosen from the available options: Small, Large, Airliner, Dirtbike, Road bike, Tandem bike, Suv, Wagon, Scooter, Mountain bike, Minivan, Sedan, School bus, Fighter, Chopper, Double bus, Truck, Articulated bus, Cruiser, Jet, Utility bike, Regular bus, Biplane, Gray, Blue, Purple, Brown, Green, Cyan, Red, Yellow, Left, Right, Front, Back, (2) an integer between 0-10 for questions like 'How many' or 'What is the number of,' or (3) 'Yes' or 'No' for questions asking 'Is there.'
    
    Output your response using this JSON format:
    { 'Reasoning': '\textless reasoning\textgreater', 'Answer': '\textless answer\textgreater', 'Related Objects': [{'shape': '\textless shape\textgreater', 'size': '\textless size\textgreater', 'color': '\textless color\textgreater'}, ...] }\\

    \textcolor{RoyalBlue}{\textbf{User:}} \\
    What is the size of the object that is occluded by the truck?\\

    \textcolor{Orange}{\textbf{GPT-4o:}} \\
    \{'Reasoning': 'The image contains several objects: a gray sedan, a white school bus, a red tandem bike, a red road bike, a red mountain bike, a purple jet, a gray wagon, and a red cruiser. The truck is occluding a small red road bike.', 'Answer': 'Small'\}
    }
    % \vspace{2pt}
    \caption{\textbf{The prompts used for L4- Occlusion questions.} In the instruction, we encourage the model to first describe all the objects in the scenes and the reasoning process of answer the question. We format the reply into a dictionary format, where the answer can be decoded and compared with the ground truth answer.}
    \label{fig:prompts_level4-occ}
\end{figure}

\begin{figure}[h]
    \centering
    \hypbox{Prompts for Level 4 - 3D Pose questions}{%
    \textcolor{ForestGreen}{\textbf{System:}} \\
    You are an intelligent chatbot designed to answer questions based on an image. 
    Your task is to analyze the images, \textbf{identify attributes of multiple objects and their facing direction in 3D space from the camera view}, and then determine the answer to the question.\\

    INSTRUCTIONS:\\
    Step 1: Identify the objects mentioned in the question, noting each object's shape, color, size, and direction which facing at. Objects can have a shape (e.g., 'airliner'), size ('small' or 'large'), color (e.g., 'blue'), \textbf{and direction ('left', 'right', 'front', 'back') in camera view.} The possible colors are: gray, blue, purple, brown, green, cyan, red, and yellow. 
    
    Step 2: Use the identified details to form a brief reasoning process for answering the question. Based on this reasoning, provide your answer in one of the following formats: (1) a phrase chosen from the available options: Small, Large, Airliner, Dirtbike, Road bike, Tandem bike, Suv, Wagon, Scooter, Mountain bike, Minivan, Sedan, School bus, Fighter, Chopper, Double bus, Truck, Articulated bus, Cruiser, Jet, Utility bike, Regular bus, Biplane, Gray, Blue, Purple, Brown, Green, Cyan, Red, Yellow, Left, Right, Front, Back, (2) an integer between 0-10 for questions like 'How many' or 'What is the number of,' or (3) 'Yes' or 'No' for questions asking 'Is there.'
    
    Output your response using this JSON format:
    { 'Reasoning': '\textless reasoning\textgreater', 'Answer': '\textless answer\textgreater', 'Related Objects': [{'shape': '\textless shape\textgreater', 'size': '\textless size\textgreater', 'color': '\textless color\textgreater', \textbf{'direction': '\textless direction\textgreater'}}, ...] }\\

    \textcolor{RoyalBlue}{\textbf{User:}} \\
    Which direction the suv is facing? \\

    \textcolor{Orange}{\textbf{GPT-4o:}} \\
    {'Reasoning': 'The image contains several objects: a small gray SUV, a small gray wagon, a small white school bus, a small red sedan, a small purple jet, a small red tandem bike, a small red road bike, a small red mountain bike, and a small red utility bike. The SUV is positioned towards the left side of the image.', 'Answer': 'Left'}
    }
    % \vspace{2pt}
    \caption{\textbf{The prompts used for Level 4 - 3D Pose  questions.} In the instruction, we encourage the model to first describe all the objects in the scenes and the reasoning process of answer the question. We format the reply into a dictionary format, where the answer can be decoded and compared with the ground truth answer.}
    \label{fig:prompts_level-pose}
\end{figure}

\begin{figure}[h]
    \centering
    \hypbox{Prompts for Level 5 - Collision questions}{%
    \textcolor{ForestGreen}{\textbf{System:}} \\
    You are an intelligent chatbot designed to answer questions based on an image. 
    Your task is to analyze the images, \textbf{identify attributes of multiple objects and their potential collision given the assumption of moving direction in 3D space}, and then determine the answer to the question.\\

    INSTRUCTIONS:\\
    Step 1: Identify the objects mentioned in the question, noting each object's shape, color, size, and direction which facing at. Objects can have a shape (e.g., 'airliner'), size ('small' or 'large'), color (e.g., 'blue'), \textbf{and direction ('left', 'right', 'front', 'back') in camera view.} The possible colors are: gray, blue, purple, brown, green, cyan, red, and yellow. 
    
    Step 2: Use the identified details to form a brief reasoning process for answering the question. Based on this reasoning, provide your answer in one of the following formats: (1) a phrase chosen from the available options: Small, Large, Airliner, Dirtbike, Road bike, Tandem bike, Suv, Wagon, Scooter, Mountain bike, Minivan, Sedan, School bus, Fighter, Chopper, Double bus, Truck, Articulated bus, Cruiser, Jet, Utility bike, Regular bus, Biplane, Gray, Blue, Purple, Brown, Green, Cyan, Red, Yellow, Left, Right, Front, Back, (2) an integer between 0-10 for questions like 'How many' or 'What is the number of,' or (3) 'Yes' or 'No' for questions asking 'Is there.'
    
    Output your response using this JSON format:
    { 'Reasoning': '\textless reasoning\textgreater', 'Answer': '\textless answer\textgreater', 'Related Objects': [{'shape': '\textless shape\textgreater', 'size': '\textless size\textgreater', 'color': '\textless color\textgreater', \textbf{'direction': '\textless direction\textgreater'}}, ...] }\\

    \textcolor{RoyalBlue}{\textbf{User:}} \\
    There is a small bicycle that is behind the aeroplane; how many tiny purple fighters are on the right side of it? \\


    \textcolor{Orange}{\textbf{GPT-4o:}} \\
    \{'Reasoning': 'In the image, there is a small bicycle behind the airplane. There is one small purple fighter on the right side of the small bicycle.', 'Answer': '1'\}
    }
    % \vspace{2pt}
    \caption{\textbf{The prompts used for Level 5 - Collision questions.} In the instruction, we encourage the model to first describe all the objects in the scenes and the reasoning process of answer the question. We format the reply into a dictionary format, where the answer can be decoded and compared with the ground truth answer.}
    \label{fig:prompts_level-col}
\end{figure}

\begin{figure}[h]
    \centering
    \hypbox{Prompts for Level 5 - 6D Spatial questions}{%
    \textcolor{ForestGreen}{\textbf{System:}} \\
    You are an intelligent chatbot designed to answer questions based on an image. 
    Your task is to analyze the images, \textbf{identify attributes of multiple objects and their spatial relationship from objects' perspective in 3D space}, and then determine the answer to the question.\\

    INSTRUCTIONS:\\
    Step 1: Identify the objects mentioned in the question, noting each object's shape, color, size, and direction which facing at. Objects can have a shape (e.g., 'airliner'), size ('small' or 'large'), color (e.g., 'blue'), \textbf{and direction ('left', 'right', 'front', 'back') in camera view.} The possible colors are: gray, blue, purple, brown, green, cyan, red, and yellow. 
    
    Step 2: Use the identified details to form a brief reasoning process for answering the question. Based on this reasoning, provide your answer in one of the following formats: (1) a phrase chosen from the available options: Small, Large, Airliner, Dirtbike, Road bike, Tandem bike, Suv, Wagon, Scooter, Mountain bike, Minivan, Sedan, School bus, Fighter, Chopper, Double bus, Truck, Articulated bus, Cruiser, Jet, Utility bike, Regular bus, Biplane, Gray, Blue, Purple, Brown, Green, Cyan, Red, Yellow, Left, Right, Front, Back, (2) an integer between 0-10 for questions like 'How many' or 'What is the number of,' or (3) 'Yes' or 'No' for questions asking 'Is there.'
    
    Output your response using this JSON format:
    { 'Reasoning': '\textless reasoning\textgreater', 'Answer': '\textless answer\textgreater', 'Related Objects': [{'shape': '\textless shape\textgreater', 'size': '\textless size\textgreater', 'color': '\textless color\textgreater', \textbf{'direction': '\textless direction\textgreater'}}, ...] }\\

    \textcolor{RoyalBlue}{\textbf{User:}} \\
    There is a aeroplane; how many tiny brown bicycles will it collide if it moving forward? \\

    \textcolor{Orange}{\textbf{GPT-4o:}} \\
    \{'Reasoning': 'The image contains a small purple jet, a small gray sedan, a small gray wagon, a small white school bus, a small red tandem bike, a small red road bike, a small brown road bike, a small brown mountain bike, and a small red sedan. The small purple jet is positioned at the back of the image. If it moves forward, it will collide with the small brown road bike and the small brown mountain bike.', 'Answer': '2'\}
    }
    % \vspace{2pt}
    \caption{\textbf{The prompts used for Level 5 - 6D Spatial questions.} In the instruction, we encourage the model to first describe all the objects in the scenes and the reasoning process of answer the question. We format the reply into a dictionary format, where the answer can be decoded and compared with the ground truth answer.}
    \label{fig:prompts_level5-6d}
\end{figure}






% \section{Bias Analysis}

% \section{More Results on L4-pose-Real}











\end{document}