@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})

@inproceedings{caesar2020nuscenes,
  title={nuscenes: A multimodal dataset for autonomous driving},
  author={Caesar, Holger and Bankiti, Varun and Lang, Alex H and Vora, Sourabh and Liong, Venice Erin and Xu, Qiang and Krishnan, Anush and Pan, Yu and Baldan, Giancarlo and Beijbom, Oscar},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={11621--11631},
  year={2020}
}
@book{sanders2016introduction,
  title={An introduction to Unreal engine 4},
  author={Sanders, Andrew},
  year={2016},
  publisher={AK Peters/CRC Press}
}
@inproceedings{qiu2016unrealcv,
  title={Unrealcv: Connecting computer vision to unreal engine},
  author={Qiu, Weichao and Yuille, Alan},
  booktitle={Computer Vision--ECCV 2016 Workshops: Amsterdam, The Netherlands, October 8-10 and 15-16, 2016, Proceedings, Part III 14},
  pages={909--916},
  year={2016},
  organization={Springer}
}
@book{hess2013blender,
  title={Blender foundations: The essential guide to learning blender 2.5},
  author={Hess, Roland},
  year={2013},
  publisher={Routledge}
}
@inproceedings{yue2024mmmu,
  title={Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi},
  author={Yue, Xiang and Ni, Yuansheng and Zhang, Kai and Zheng, Tianyu and Liu, Ruoqi and Zhang, Ge and Stevens, Samuel and Jiang, Dongfu and Ren, Weiming and Sun, Yuxuan and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9556--9567},
  year={2024}
}
@inproceedings{song2015sun,
  title={Sun rgb-d: A rgb-d scene understanding benchmark suite},
  author={Song, Shuran and Lichtenberg, Samuel P and Xiao, Jianxiong},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={567--576},
  year={2015}
}
@article{tong2024cambrian,
  title={Cambrian-1: A fully open, vision-centric exploration of multimodal llms},
  author={Tong, Shengbang and Brown, Ellis and Wu, Penghao and Woo, Sanghyun and Middepogu, Manoj and Akula, Sai Charitha and Yang, Jihan and Yang, Shusheng and Iyer, Adithya and Pan, Xichen and others},
  journal={arXiv preprint arXiv:2406.16860},
  year={2024}
}
@inproceedings{singh2019towards,
  title={Towards vqa models that can read},
  author={Singh, Amanpreet and Natarajan, Vivek and Shah, Meet and Jiang, Yu and Chen, Xinlei and Batra, Dhruv and Parikh, Devi and Rohrbach, Marcus},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={8317--8326},
  year={2019}
}

@inproceedings{hudson2019gqa,
  title={Gqa: A new dataset for real-world visual reasoning and compositional question answering},
  author={Hudson, Drew A and Manning, Christopher D},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={6700--6709},
  year={2019}
}
@article{lindstrom2022clevr,
  title={Clevr-math: A dataset for compositional language, visual and mathematical reasoning},
  author={Lindstr{\"o}m, Adam Dahlgren and Abraham, Savitha Sam},
  journal={arXiv preprint arXiv:2208.05358},
  year={2022}
}
@inproceedings{goyal2017making,
  title={Making the v in vqa matter: Elevating the role of image understanding in visual question answering},
  author={Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={6904--6913},
  year={2017}
}
@article{yi2019clevrer,
  title={Clevrer: Collision events for video representation and reasoning},
  author={Yi, Kexin and Gan, Chuang and Li, Yunzhu and Kohli, Pushmeet and Wu, Jiajun and Torralba, Antonio and Tenenbaum, Joshua B},
  journal={arXiv preprint arXiv:1910.01442},
  year={2019}
}


@article{chen2022comphy,
  title={Comphy: Compositional physical reasoning of objects and events from videos},
  author={Chen, Zhenfang and Yi, Kexin and Li, Yunzhu and Ding, Mingyu and Torralba, Antonio and Tenenbaum, Joshua B and Gan, Chuang},
  journal={arXiv preprint arXiv:2205.01089},
  year={2022}
}

@article{tung2023physion++,
  author    = {Tung, Fish and Ding, Mingyu and Chen, Zhenfang and Bear,  Daniel M. and Gan, Chuang and Tenenbaum, Joshua B. and Yamins, Daniel L. K. and Fan, Judith and Smith, Kevin A.},
  title     = {Physion++: Evaluating Physical Scene Understanding that 
 Requires Online Inference of Different Physical Properties},
  journal   = {arXiv},
  year      = {2023},
}


@article{baradel2019cophy,
  title={Cophy: Counterfactual learning of physical dynamics},
  author={Baradel, Fabien and Neverova, Natalia and Mille, Julien and Mori, Greg and Wolf, Christian},
  journal={arXiv preprint arXiv:1909.12000},
  year={2019}
}


@article{bakhtin2019phyre,
  title={Phyre: A new benchmark for physical reasoning},
  author={Bakhtin, Anton and van der Maaten, Laurens and Johnson, Justin and Gustafson, Laura and Girshick, Ross},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}


@article{chen2021grounding,
  title={Grounding physical concepts of objects and events through dynamic visual reasoning},
  author={Chen, Zhenfang and Mao, Jiayuan and Wu, Jiajun and Wong, Kwan-Yee Kenneth and Tenenbaum, Joshua B and Gan, Chuang},
  journal={arXiv preprint arXiv:2103.16564},
  year={2021}
}

@inproceedings{ates-etal-2022-craft,
    title = "{CRAFT}: A Benchmark for Causal Reasoning About Forces and in{T}eractions",
    author = "Ates, Tayfun  and
      Ate{\c{s}}o{\u{g}}lu, M.  and
      Yi{\u{g}}it, {\c{C}}a{\u{g}}atay  and
      Kesen, Ilker  and
      Kobas, Mert  and
      Erdem, Erkut  and
      Erdem, Aykut  and
      Goksun, Tilbe  and
      Yuret, Deniz",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.205",
    doi = "10.18653/v1/2022.findings-acl.205",
    pages = "2602--2627"
}




@article{riochet2021intphys,
  title={Intphys 2019: A benchmark for visual intuitive physics understanding},
  author={Riochet, Ronan and Castro, Mario Ynocente and Bernard, Mathieu and Lerer, Adam and Fergus, Rob and Izard, V{\'e}ronique and Dupoux, Emmanuel},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={44},
  number={9},
  pages={5016--5025},
  year={2021},
  publisher={IEEE}
}

@inproceedings{girdhar2020cater,
    title = {{CATER: A diagnostic dataset for Compositional Actions and TEmporal Reasoning}},
    author = {Girdhar, Rohit and Ramanan, Deva},
    booktitle = {ICLR},
    year = 2020
}

@inproceedings{hong2021ptr,
author = {Hong, Yining and Yi, Li and Tenenbaum, Joshua B and Torralba, Antonio and Gan, Chuang},
title = {PTR: A Benchmark for Part-based Conceptual, Relational, and Physical Reasoning},
booktitle = {Advances In Neural Information Processing Systems},
year = {2021}
}


@inproceedings{tang2023intrinsic,
  title={Intrinsic Physical Concepts Discovery with Object-Centric Predictive Models},
  author={Tang, Qu and Zhu, Xiangyu and Lei, Zhen and Zhang, Zhaoxiang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={23252--23261},
  year={2023}
}


@article{melnik2023benchmarks,
  title={Benchmarks for Physical Reasoning AI},
  author={Melnik, Andrew and Schiewer, Robin and Lange, Moritz and Muresanu, Andrei and Saeidi, Mozhgan and Garg, Animesh and Ritter, Helge},
  journal={arXiv preprint arXiv:2312.10728},
  year={2023}
}

@article{bear2021physion,
  title={Physion: Evaluating physical prediction from vision in humans and machines},
  author={Bear, Daniel M and Wang, Elias and Mrowca, Damian and Binder, Felix J and Tung, Hsiao-Yu Fish and Pramod, RT and Holdaway, Cameron and Tao, Sirui and Smith, Kevin and Sun, Fan-Yun and others},
  journal={arXiv preprint arXiv:2106.08261},
  year={2021}
}

@article{patel2022cripp,
  title={CRIPP-VQA: Counterfactual reasoning about implicit physical properties via video question answering},
  author={Patel, Maitreya and Gokhale, Tejas and Baral, Chitta and Yang, Yezhou},
  journal={arXiv preprint arXiv:2211.03779},
  year={2022}
}

@article{wang20243d,
  title={3D-Aware Visual Question Answering about Parts, Poses and Occlusions},
  author={Wang, Xingrui and Ma, Wufei and Li, Zhuowan and Kortylewski, Adam and Yuille, Alan L},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@article{wu2017learning,
  title={Learning to see physics via visual de-animation},
  author={Wu, Jiajun and Lu, Erika and Kohli, Pushmeet and Freeman, Bill and Tenenbaum, Josh},
  journal={Advances in neural information procesing systems},
  volume={30},
  year={2017}
}

@inproceedings{greff2022kubric,
  title={Kubric: A scalable dataset generator},
  author={Greff, Klaus and Belletti, Francois and Beyer, Lucas and Doersch, Carl and Du, Yilun and Duckworth, Daniel and Fleet, David J and Gnanapragasam, Dan and Golemo, Florian and Herrmann, Charles and others},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={3749--3761},
  year={2022}
}

@article{gan2020threedworld,
  title={Threedworld: A platform for interactive multi-modal physical simulation},
  author={Gan, Chuang and Schwartz, Jeremy and Alter, Seth and Mrowca, Damian and Schrimpf, Martin and Traer, James and De Freitas, Julian and Kubilius, Jonas and Bhandwaldar, Abhishek and Haber, Nick and others},
  journal={arXiv preprint arXiv:2007.04954},
  year={2020}
}

@article{lei2018tvqa,
  title={Tvqa: Localized, compositional video question answering},
  author={Lei, Jie and Yu, Licheng and Bansal, Mohit and Berg, Tamara L},
  journal={arXiv preprint arXiv:1809.01696},
  year={2018}
}

@inproceedings{grunde2021agqa,
  title={Agqa: A benchmark for compositional spatio-temporal reasoning},
  author={Grunde-McLaughlin, Madeleine and Krishna, Ranjay and Agrawala, Maneesh},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11287--11297},
  year={2021}
}

@inproceedings{li2023super,
  title={Super-CLEVR: A virtual benchmark to diagnose domain robustness in visual reasoning},
  author={Li, Zhuowan and Wang, Xingrui and Stengel-Eskin, Elias and Kortylewski, Adam and Ma, Wufei and Van Durme, Benjamin and Yuille, Alan L},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14963--14973},
  year={2023}
}

@inproceedings{johnson2017clevr,
  title={Clevr: A diagnostic dataset for compositional language and elementary visual reasoning},
  author={Johnson, Justin and Hariharan, Bharath and Van Der Maaten, Laurens and Fei-Fei, Li and Lawrence Zitnick, C and Girshick, Ross},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2901--2910},
  year={2017}
}

@inproceedings{tapaswi2016movieqa,
  title={Movieqa: Understanding stories in movies through question-answering},
  author={Tapaswi, Makarand and Zhu, Yukun and Stiefelhagen, Rainer and Torralba, Antonio and Urtasun, Raquel and Fidler, Sanja},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4631--4640},
  year={2016}
}

@article{wang2021nemo,
  title={Nemo: Neural mesh models of contrastive features for robust 3d pose estimation},
  author={Wang, Angtian and Kortylewski, Adam and Yuille, Alan},
  journal={arXiv preprint arXiv:2101.12378},
  year={2021}
}

@INPROCEEDINGS{salzmann2011physical,
  author={Salzmann, Mathieu and Urtasun, Raquel},
  booktitle={2011 International Conference on Computer Vision}, 
  title={Physically-based motion models for 3D tracking: A convex formulation}, 
  year={2011},
  volume={},
  number={},
  pages={2064-2071},
  keywords={Three dimensional displays;Tracking;Markov processes;Computational modeling;Mathematical model;Humans;Gravity},
  doi={10.1109/ICCV.2011.6126480}}
@inproceedings{liu2019soft,
  title={Soft rasterizer: A differentiable renderer for image-based 3d reasoning},
  author={Liu, Shichen and Li, Tianye and Chen, Weikai and Li, Hao},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={7708--7717},
  year={2019}
}

@article{battaglia2013simulation,
  title={Simulation as an engine of physical scene understanding},
  author={Battaglia, Peter W and Hamrick, Jessica B and Tenenbaum, Joshua B},
  journal={Proceedings of the National Academy of Sciences},
  volume={110},
  number={45},
  pages={18327--18332},
  year={2013},
  publisher={National Acad Sciences}
}

@inproceedings{majumdar2023openeqa,
  author={Majumdar, Arjun and Ajay, Anurag and Zhang, Xiaohan and Putta, Pranav and Yenamandra, Sriram and Henaff, Mikael and Silwal, Sneha and Mcvay, Paul and Maksymets, Oleksandr and Arnaud, Sergio and others},
  title={{OpenEQA: Embodied Question Answering in the Era of Foundation Models}},
  booktitle={{CVPR}},
  year={2024},
}

@ARTICLE{8017608,
  author={Xue, Hongyang and Zhao, Zhou and Cai, Deng},
  journal={IEEE Transactions on Image Processing}, 
  title={Unifying the Video and Question Attentions for Open-Ended Video Question Answering}, 
  year={2017},
  volume={26},
  number={12},
  pages={5656-5666},
  keywords={Knowledge discovery;Visualization;Adaptation models;Natural languages;Motion pictures;Coherence;Hair;Video question answering;attention model;scene understanding},
  doi={10.1109/TIP.2017.2746267}}

@article{yang2022learningta,
title={Learning to Answer Visual Questions from Web Videos},
author={Antoine Yang and Antoine Miech and Josef Sivic and Ivan Laptev and Cordelia Schmid},
journal={IEEE TPAMI},
year={2022}}

@inproceedings{ma2022robust,
  title={Robust category-level 6d pose estimation with coarse-to-fine rendering of neural features},
  author={Ma, Wufei and Wang, Angtian and Yuille, Alan and Kortylewski, Adam},
  booktitle={European Conference on Computer Vision},
  pages={492--508},
  year={2022},
  organization={Springer}
}

@inproceedings{NEURIPS2023_b783c44b,
 author = {Wang, Xingrui and Ma, Wufei and Li, Zhuowan and Kortylewski, Adam and Yuille, Alan L},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {58717--58735},
 publisher = {Curran Associates, Inc.},
 title = {3D-Aware Visual Question Answering about Parts, Poses and Occlusions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/b783c44ba9adbc30344473dc633b4869-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@article{kadambi2023incorporating,
  title={Incorporating physics into data-driven computer vision},
  author={Kadambi, Achuta and de Melo, Celso and Hsieh, Cho-Jui and Srivastava, Mani and Soatto, Stefano},
  journal={Nature Machine Intelligence},
  volume={5},
  number={6},
  pages={572--580},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{bain2021frozen,
  title={Frozen in time: A joint video and image encoder for end-to-end retrieval},
  author={Bain, Max and Nagrani, Arsha and Varol, G{\"u}l and Zisserman, Andrew},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1728--1738},
  year={2021}
}

@article{schuhmann2022laion,
  title={Laion-5b: An open large-scale dataset for training next generation image-text models},
  author={Schuhmann, Christoph and Beaumont, Romain and Vencu, Richard and Gordon, Cade and Wightman, Ross and Cherti, Mehdi and Coombes, Theo and Katta, Aarush and Mullis, Clayton and Wortsman, Mitchell and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={25278--25294},
  year={2022}
}

@article{wang2022internvideo,
  title={Internvideo: General video foundation models via generative and discriminative learning},
  author={Wang, Yi and Li, Kunchang and Li, Yizhuo and He, Yinan and Huang, Bingkun and Zhao, Zhiyu and Zhang, Hongjie and Xu, Jilan and Liu, Yi and Wang, Zun and others},
  journal={arXiv preprint arXiv:2212.03191},
  year={2022}
}

@article{lin2023video,
  title={Video-llava: Learning united visual representation by alignment before projection},
  author={Lin, Bin and Zhu, Bin and Ye, Yang and Ning, Munan and Jin, Peng and Yuan, Li},
  journal={arXiv preprint arXiv:2311.10122},
  year={2023}
}

@article{xu2024pllava,
  title={PLLaVA: Parameter-free LLaVA Extension from Images to Videos for Video Dense Captioning},
  author={Xu, Lin and Zhao, Yilin and Zhou, Daquan and Lin, Zhijie and Ng, See Kiong and Feng, Jiashi},
  journal={arXiv preprint arXiv:2404.16994},
  year={2024}
}

@article{jesslen2023robust,
  title={Robust 3D-aware object classification via discriminative render-and-compare},
  author={Jesslen, Artur and Zhang, Guofeng and Wang, Angtian and Yuille, Alan and Kortylewski, Adam},
  journal={arXiv preprint arXiv:2305.14668},
  year={2023}
}

@inproceedings{
wang2023daware,
title={3D-Aware Visual Question Answering about Parts, Poses and Occlusions},
author={Xingrui Wang and Wufei Ma and Zhuowan Li and Adam Kortylewski and Alan Yuille},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=AMIJEupsNq}
}


@inproceedings{kyriazis2013physically,
  title={Physically plausible 3d scene tracking: The single actor hypothesis},
  author={Kyriazis, Nikolaos and Argyros, Antonis},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={9--16},
  year={2013}
}

@article{wu2015galileo,
  title={Galileo: Perceiving physical object properties by integrating a physics engine with deep learning},
  author={Wu, Jiajun and Yildirim, Ilker and Lim, Joseph J and Freeman, Bill and Tenenbaum, Josh},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@article{ding2021dynamic,
  title={Dynamic visual reasoning by learning differentiable physics models from video and language},
  author={Ding, Mingyu and Chen, Zhenfang and Du, Tao and Luo, Ping and Tenenbaum, Josh and Gan, Chuang},
  journal={Advances In Neural Information Processing Systems},
  volume={34},
  pages={887--899},
  year={2021}
}

@article{ullman2018learning,
  title={Learning physical parameters from dynamic scenes},
  author={Ullman, Tomer D and Stuhlm{\"u}ller, Andreas and Goodman, Noah D and Tenenbaum, Joshua B},
  journal={Cognitive psychology},
  volume={104},
  pages={57--82},
  year={2018},
  publisher={Elsevier}
}


@article{hamrick2016inferring,
  title={Inferring mass in complex scenes by mental simulation},
  author={Hamrick, Jessica B and Battaglia, Peter W and Griffiths, Thomas L and Tenenbaum, Joshua B},
  journal={Cognition},
  volume={157},
  pages={61--76},
  year={2016},
  publisher={Elsevier}
}


@article{zheng2024contphy,
  title={ContPhy: Continuum Physical Concept Learning and Reasoning from Videos},
  author={Zheng, Zhicheng and Yan, Xin and Chen, Zhenfang and Wang, Jingzhou and Lim, Qin Zhi Eddie and Tenenbaum, Joshua B and Gan, Chuang},
  journal={arXiv preprint arXiv:2402.06119},
  year={2024}
}

@inproceedings{xiang2014beyond,
  title={Beyond pascal: A benchmark for 3d object detection in the wild},
  author={Xiang, Yu and Mottaghi, Roozbeh and Savarese, Silvio},
  booktitle={IEEE winter conference on applications of computer vision},
  pages={75--82},
  year={2014},
  organization={IEEE}
}


@article{greff2021kubric,
    title = {Kubric: a scalable dataset generator}, 
    author = {Klaus Greff and Francois Belletti and Lucas Beyer and Carl Doersch and
              Yilun Du and Daniel Duckworth and David J Fleet and Dan Gnanapragasam and
              Florian Golemo and Charles Herrmann and Thomas Kipf and Abhijit Kundu and
              Dmitry Lagun and Issam Laradji and Hsueh-Ti (Derek) Liu and Henning Meyer and
              Yishu Miao and Derek Nowrouzezahrai and Cengiz Oztireli and Etienne Pot and
              Noha Radwan and Daniel Rebain and Sara Sabour and Mehdi S. M. Sajjadi and Matan Sela and
              Vincent Sitzmann and Austin Stone and Deqing Sun and Suhani Vora and Ziyu Wang and
              Tianhao Wu and Kwang Moo Yi and Fangcheng Zhong and Andrea Tagliasacchi},
    booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    year = {2022},
}

@inproceedings{perez2018film,
  title={Film: Visual reasoning with a general conditioning layer},
  author={Perez, Ethan and Strub, Florian and De Vries, Harm and Dumoulin, Vincent and Courville, Aaron},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  number={1},
  year={2018}
}

@article{coumans2016pybullet,
  title={Pybullet, a python module for physics simulation for games, robotics and machine learning},
  author={Coumans, Erwin and Bai, Yunfei},
  year={2016}
}

@manual{blender,
  title = {Blender - a 3D modelling and rendering package},
  author = {{Blender Online Community}},
  year = {2021},
  note = {Blender Foundation, Stichting Blender Foundation, Amsterdam, The Netherlands},
  url = {http://www.blender.org}
}

@article{ma2023adding,
  title={Adding 3D geometry control to diffusion models},
  author={Ma, Wufei and Liu, Qihao and Wang, Jiahao and Wang, Angtian and Liu, Yaoyao and Kortylewski, Adam and Yuille, Alan},
  journal={arXiv preprint arXiv:2306.08103},
  year={2023}
}


@book{sarkka2023bayesian,
  title={Bayesian filtering and smoothing},
  author={S{\"a}rkk{\"a}, Simo and Svensson, Lennart},
  volume={17},
  year={2023},
  publisher={Cambridge university press}
}

@article{kaushik2024source,
  title={Source-Free and Image-Only Unsupervised Domain Adaptation for Category Level Object Pose Estimation},
  author={Kaushik, Prakhar and Mishra, Aayush and Kortylewski, Adam and Yuille, Alan},
  journal={arXiv preprint arXiv:2401.10848},
  year={2024}
}

@article{huang2024rekep, 
  title = {ReKep: Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robotic Manipulation}, 
  author = {Huang, Wenlong and Wang, Chen and Li, Yunzhu and Zhang, Ruohan and Fei-Fei, Li}, 
  journal = {arXiv preprint arXiv:2409.01652}, 
  year = {2024} 
}

@article{ma2024rethinking,
  title={Rethinking Video-Text Understanding: Retrieval from Counterfactually Augmented Data},
  author={Ma, Wufei and Li, Kai and Jiang, Zhongshi and Meshry, Moustafa and Liu, Qihao and Wang, Huiyu and H{\"a}ne, Christian and Yuille, Alan},
  journal={arXiv preprint arXiv:2407.13094},
  year={2024}
}

@inproceedings{caba2015activitynet,
  title={ActivityNet: A Large-Scale Video Benchmark for Human Activity Understanding},
  author={Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem and Juan Carlos Niebles},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={961--970},
  year={2015}
}

@inproceedings{goyal2017something,
  title={The" something something" video database for learning and evaluating visual common sense},
  author={Goyal, Raghav and Ebrahimi Kahou, Samira and Michalski, Vincent and Materzynska, Joanna and Westphal, Susanne and Kim, Heuna and Haenel, Valentin and Fruend, Ingo and Yianilos, Peter and Mueller-Freitag, Moritz and others},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={5842--5850},
  year={2017}
}

@article{ren2016faster,
  title={Faster R-CNN: Towards real-time object detection with region proposal networks},
  author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={39},
  number={6},
  pages={1137--1149},
  year={2016},
  publisher={IEEE}
}

@article{wu2024star,
  title={Star: A benchmark for situated reasoning in real-world videos},
  author={Wu, Bo and Yu, Shoubin and Chen, Zhenfang and Tenenbaum, Joshua B and Gan, Chuang},
  journal={arXiv preprint arXiv:2405.09711},
  year={2024}
}


@inproceedings{bai2023coke,
  title={CoKe: Contrastive Learning for Robust Keypoint Detection},
  author={Bai, Yutong and Wang, Angtian and Kortylewski, Adam and Yuille, Alan},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={65--74},
  year={2023}
}

@article{yuille2006vision,
  title={Vision as Bayesian inference: analysis by synthesis?},
  author={Yuille, Alan and Kersten, Daniel},
  journal={Trends in cognitive sciences},
  volume={10},
  number={7},
  pages={301--308},
  year={2006},
  publisher={Elsevier}
}

@inproceedings{carion2020end,
  title={End-to-end object detection with transformers},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  booktitle={European conference on computer vision},
  pages={213--229},
  year={2020},
  organization={Springer}
}

@article{wang2024compositional,
  title={Compositional 4D Dynamic Scenes Understanding with Physics Priors for Video Question Answering},
  author={Wang, Xingrui and Ma, Wufei and Wang, Angtian and Chen, Shuo and Kortylewski, Adam and Yuille, Alan},
  journal={arXiv preprint arXiv:2406.00622},
  year={2024}
}

@inproceedings{kamath2021mdetr,
  title={Mdetr-modulated detection for end-to-end multi-modal understanding},
  author={Kamath, Aishwarya and Singh, Mannat and LeCun, Yann and Synnaeve, Gabriel and Misra, Ishan and Carion, Nicolas},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={1780--1790},
  year={2021}
}

@article{mao2019neuro,
  title={The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision},
  author={Mao, Jiayuan and Gan, Chuang and Kohli, Pushmeet and Tenenbaum, Joshua B and Wu, Jiajun},
  journal={arXiv preprint arXiv:1904.12584},
  year={2019}
}

@article{yi2018neural,
  title={Neural-symbolic vqa: Disentangling reasoning from vision and language understanding},
  author={Yi, Kexin and Wu, Jiajun and Gan, Chuang and Torralba, Antonio and Kohli, Pushmeet and Tenenbaum, Josh},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{cheng2024spatialrgpt,
  title={SpatialRGPT: Grounded Spatial Reasoning in Vision Language Model},
  author={Cheng, An-Chieh and Yin, Hongxu and Fu, Yang and Guo, Qiushan and Yang, Ruihan and Kautz, Jan and Wang, Xiaolong and Liu, Sifei},
  journal={arXiv preprint arXiv:2406.01584},
  year={2024}
}




@article{openai2023gpt4,
  title={GPT-4 Technical Report},
  author={OpenAI},
  year={2023},
  journal={OpenAI},
  url={https://openai.com/research/gpt-4}
}

@article{alibaba2023gemini,
  title={Gemini: Vision-Language Model by Alibaba},
  author={Alibaba DAMO Academy},
  year={2023},
  journal={Alibaba DAMO},
  url={https://damo.alibaba.com/gemini}
}

@article{anthropic2023claude,
  title={Claude: An Anthropic Language Model},
  author={Anthropic},
  year={2023},
  journal={Anthropic},
  url={https://www.anthropic.com/index/claude}
}

@article{qwen2023vl7b,
  title={Qwen-VL: A Large Multimodal Model},
  author={Qwen Team},
  year={2023},
  journal={Alibaba Group},
  url={https://github.com/alibaba/qwen-vl}
}

@article{intern2023vl2,
  title={InternVL 2.0: A Vision-Language Model},
  author={Intern Team},
  year={2023},
  journal={Intern Foundation},
  url={https://intern.com/internvl2}
}

@article{liu2023llava,
  title={LLaVA: Large Language and Vision Assistant},
  author={Liu, Haotian and others},
  year={2023},
  journal={LLaVA Project},
  url={https://github.com/haotian-liu/LLaVA}
}

@article{liu2023llavanext,
  title={LLaVA-Next: Extended Large Language and Vision Assistant},
  author={Liu, Haotian and others},
  year={2023},
  journal={LLaVA Project},
  url={https://github.com/haotian-liu/LLaVA-Next}
}

@article{touvron2023llama,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Touvron, Hugo and others},
  year={2023},
  journal={Meta AI},
  url={https://arxiv.org/abs/2302.13971}
}


@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@inproceedings{li2023blip,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  booktitle={International conference on machine learning},
  pages={19730--19742},
  year={2023},
  organization={PMLR}
}

@article{team2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Team, Gemini and Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}

@article{hu2022promptcap,
  title={Promptcap: Prompt-guided task-aware image captioning},
  author={Hu, Yushi and Hua, Hang and Yang, Zhengyuan and Shi, Weijia and Smith, Noah A and Luo, Jiebo},
  journal={arXiv preprint arXiv:2211.09699},
  year={2022}
}


@inproceedings{chen2024spatialvlm,
  title={Spatialvlm: Endowing vision-language models with spatial reasoning capabilities},
  author={Chen, Boyuan and Xu, Zhuo and Kirmani, Sean and Ichter, Brain and Sadigh, Dorsa and Guibas, Leonidas and Xia, Fei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14455--14465},
  year={2024}
}

