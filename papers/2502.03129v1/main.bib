
@book{amaratunga_understanding_2023,
	address = {Berkeley, CA},
	title = {Understanding {Large} {Language} {Models}: {Learning} {Their} {Underlying} {Concepts} and {Technologies}},
	isbn = {9798868800160 9798868800177},
	shorttitle = {Understanding {Large} {Language} {Models}},
	url = {https://link.springer.com/10.1007/979-8-8688-0017-7},
	language = {en},
	urldate = {2024-03-19},
	publisher = {Apress},
	author = {Amaratunga, Thimira},
	year = {2023},
	doi = {10.1007/979-8-8688-0017-7},
	keywords = {llm},
	file = {Amaratunga - 2023 - Understanding Large Language Models Learning Thei.pdf:/Users/zhenqian/Zotero/storage/CL67IMN4/Amaratunga - 2023 - Understanding Large Language Models Learning Thei.pdf:application/pdf},
}

@misc{vaswani_attention_2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2024-03-19},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762 [cs]},
	keywords = {llm},
	annote = {Comment: 15 pages, 5 figures},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/D3JZHYVG/Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/EP3XINNW/1706.html:text/html},
}

@article{radford_improving_nodate,
	title = {Improving {Language} {Understanding} by {Generative} {Pre}-{Training}},
	abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classiﬁcation. Although large unlabeled text corpora are abundant, labeled data for learning these speciﬁc tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative ﬁne-tuning on each speciﬁc task. In contrast to previous approaches, we make use of task-aware input transformations during ﬁne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speciﬁcally crafted for each task, signiﬁcantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
	language = {en},
	author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
	keywords = {llm},
	annote = {not read yet

},
	file = {Radford et al. - Improving Language Understanding by Generative Pre.pdf:/Users/zhenqian/Zotero/storage/RMPSTE5W/Radford et al. - Improving Language Understanding by Generative Pre.pdf:application/pdf},
}

@misc{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	doi = {10.48550/arXiv.1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2024-03-19},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv:1810.04805 [cs]},
	keywords = {llm},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/IBP83SSV/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/9SLKNZZP/1810.html:text/html},
}

@article{ling_deductive_2023,
	title = {Deductive {Verification} of {Chain}-of-{Thought} {Reasoning}},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/72393bd47a35f5b3bee4c609e7bba733-Abstract-Conference.html},
	language = {en},
	urldate = {2024-03-21},
	journal = {Advances in Neural Information Processing Systems},
	author = {Ling, Zhan and Fang, Yunhao and Li, Xuanlin and Huang, Zhiao and Lee, Mingu and Memisevic, Roland and Su, Hao},
	month = dec,
	year = {2023},
	keywords = {cot, rationale generation},
	pages = {36407--36433},
	file = {Full Text PDF:/Users/zhenqian/Zotero/storage/DDHCLSIE/Ling et al. - 2023 - Deductive Verification of Chain-of-Thought Reasoni.pdf:application/pdf},
}

@misc{cobbe_training_2021,
	title = {Training {Verifiers} to {Solve} {Math} {Word} {Problems}},
	url = {http://arxiv.org/abs/2110.14168},
	doi = {10.48550/arXiv.2110.14168},
	abstract = {State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.},
	urldate = {2024-03-23},
	publisher = {arXiv},
	author = {Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
	month = nov,
	year = {2021},
	note = {arXiv:2110.14168 [cs]},
	keywords = {rationale-augmented training, verifier},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/4GBWAQYF/Cobbe et al. - 2021 - Training Verifiers to Solve Math Word Problems.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/4L37YDMN/2110.html:text/html},
}

@misc{hendrycks_measuring_2021,
	title = {Measuring {Mathematical} {Problem} {Solving} {With} the {MATH} {Dataset}},
	url = {http://arxiv.org/abs/2103.03874},
	doi = {10.48550/arXiv.2103.03874},
	abstract = {Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.},
	urldate = {2024-03-23},
	publisher = {arXiv},
	author = {Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
	month = nov,
	year = {2021},
	note = {arXiv:2103.03874 [cs]},
	keywords = {unread},
	annote = {Comment: NeurIPS 2021. Code and the MATH dataset is available at https://github.com/hendrycks/math/},
	annote = {Comment: NeurIPS 2021. Code and the MATH dataset is available at https://github.com/hendrycks/math/},
	annote = {not read yet

},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/5BHMJL6E/Hendrycks et al. - 2021 - Measuring Mathematical Problem Solving With the MA.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/E94QTH6Z/2103.html:text/html},
}

@inproceedings{chiang_semantically-aligned_2019,
	address = {Minneapolis, Minnesota},
	title = {Semantically-{Aligned} {Equation} {Generation} for {Solving} and {Reasoning} {Math} {Word} {Problems}},
	url = {https://aclanthology.org/N19-1272},
	doi = {10.18653/v1/N19-1272},
	abstract = {Solving math word problems is a challenging task that requires accurate natural language understanding to bridge natural language texts and math expressions. Motivated by the intuition about how human generates the equations given the problem texts, this paper presents a neural approach to automatically solve math word problems by operating symbols according to their semantic meanings in texts. This paper views the process of generating equation as a bridge between the semantic world and the symbolic world, where the proposed neural math solver is based on an encoder-decoder framework. In the proposed model, the encoder is designed to understand the semantics of problems, and the decoder focuses on tracking semantic meanings of the generated symbols and then deciding which symbol to generate next. The preliminary experiments are conducted in a dataset Math23K, and our model significantly outperforms both the state-of-the-art single model and the best non-retrieval-based model over about 10\% accuracy, demonstrating the effectiveness of bridging the symbolic and semantic worlds from math word problems.},
	urldate = {2024-03-23},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Chiang, Ting-Rui and Chen, Yun-Nung},
	editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
	month = jun,
	year = {2019},
	keywords = {rationale-augmented training},
	pages = {2656--2668},
	file = {Full Text PDF:/Users/zhenqian/Zotero/storage/VTAE6TS6/Chiang and Chen - 2019 - Semantically-Aligned Equation Generation for Solvi.pdf:application/pdf},
}

@inproceedings{amini_mathqa_2019,
	address = {Minneapolis, Minnesota},
	title = {{MathQA}: {Towards} {Interpretable} {Math} {Word} {Problem} {Solving} with {Operation}-{Based} {Formalisms}},
	shorttitle = {{MathQA}},
	url = {https://aclanthology.org/N19-1245},
	doi = {10.18653/v1/N19-1245},
	abstract = {We introduce a large-scale dataset of math word problems and an interpretable neural math problem solver by learning to map problems to their operation programs. Due to annotation challenges, current datasets in this domain have been either relatively small in scale or did not offer precise operational annotations over diverse problem types. We introduce a new representation language to model operation programs corresponding to each math problem that aim to improve both the performance and the interpretability of the learned models. Using this representation language, we significantly enhance the AQUA-RAT dataset with fully-specified operational programs. We additionally introduce a neural sequence-to-program model with automatic problem categorization. Our experiments show improvements over competitive baselines in our dataset as well as the AQUA-RAT dataset. The results are still lower than human performance indicating that the dataset poses new challenges for future research. Our dataset is available at https://math-qa.github.io/math-QA/},
	urldate = {2024-03-23},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Amini, Aida and Gabriel, Saadia and Lin, Shanchuan and Koncel-Kedziorski, Rik and Choi, Yejin and Hajishirzi, Hannaneh},
	editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
	month = jun,
	year = {2019},
	keywords = {rationale-augmented training},
	pages = {2357--2367},
	file = {Full Text PDF:/Users/zhenqian/Zotero/storage/R95X9R9P/Amini et al. - 2019 - MathQA Towards Interpretable Math Word Problem So.pdf:application/pdf},
}

@misc{zhang_elastic_2022,
	title = {{ELASTIC}: {Numerical} {Reasoning} with {Adaptive} {Symbolic} {Compiler}},
	shorttitle = {{ELASTIC}},
	url = {http://arxiv.org/abs/2210.10105},
	abstract = {Numerical reasoning over text is a challenging task of Artificial Intelligence (AI), requiring reading comprehension and numerical reasoning abilities. Previous approaches use numerical reasoning programs to represent the reasoning process. However, most works do not separate the generation of operators and operands, which are key components of a numerical reasoning program, thus limiting their ability to generate such programs for complicated tasks. In this paper, we introduce the numEricaL reASoning with adapTive symbolIc Compiler (ELASTIC) model, which is constituted of the RoBERTa as the Encoder and a Compiler with four modules: Reasoning Manager, Operator Generator, Operands Generator, and Memory Register. ELASTIC is robust when conducting complicated reasoning. Also, it is domain agnostic by supporting the expansion of diverse operators without caring about the number of operands it contains. Experiments show that ELASTIC achieves 68.96 and 65.21 of execution accuracy and program accuracy on the FinQA dataset and 83.00 program accuracy on the MathQA dataset, outperforming previous state-of-the-art models significantly.},
	urldate = {2024-03-23},
	publisher = {arXiv},
	author = {Zhang, Jiaxin and Moshfeghi, Yashar},
	month = oct,
	year = {2022},
	note = {arXiv:2210.10105 [cs]
version: 2},
	keywords = {rationale-augmented training},
	annote = {Comment: Accepted to NeurIPS 2022},
	file = {arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/4WLHCQEE/2210.html:text/html;Full Text PDF:/Users/zhenqian/Zotero/storage/73MB82BW/Zhang and Moshfeghi - 2022 - ELASTIC Numerical Reasoning with Adaptive Symboli.pdf:application/pdf},
}

@inproceedings{ling_program_2017,
	address = {Vancouver, Canada},
	title = {Program {Induction} by {Rationale} {Generation}: {Learning} to {Solve} and {Explain} {Algebraic} {Word} {Problems}},
	shorttitle = {Program {Induction} by {Rationale} {Generation}},
	url = {https://aclanthology.org/P17-1015},
	doi = {10.18653/v1/P17-1015},
	abstract = {Solving algebraic word problems requires executing a series of arithmetic operations—a program—to obtain a final answer. However, since programs can be arbitrarily complicated, inducing them directly from question-answer pairs is a formidable challenge. To make this task more feasible, we solve these problems by generating answer rationales, sequences of natural language and human-readable mathematical expressions that derive the final answer through a series of small steps. Although rationales do not explicitly specify programs, they provide a scaffolding for their structure via intermediate milestones. To evaluate our approach, we have created a new 100,000-sample dataset of questions, answers and rationales. Experimental results show that indirect supervision of program learning via answer rationales is a promising strategy for inducing arithmetic programs.},
	urldate = {2024-03-24},
	booktitle = {Proceedings of the 55th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Ling, Wang and Yogatama, Dani and Dyer, Chris and Blunsom, Phil},
	editor = {Barzilay, Regina and Kan, Min-Yen},
	month = jul,
	year = {2017},
	keywords = {rationale-augmented training},
	pages = {158--167},
	file = {Full Text PDF:/Users/zhenqian/Zotero/storage/FKXGEHEV/Ling et al. - 2017 - Program Induction by Rationale Generation Learnin.pdf:application/pdf},
}

@inproceedings{wei_finetuned_2021,
	title = {Finetuned {Language} {Models} are {Zero}-{Shot} {Learners}},
	url = {https://openreview.net/forum?id=gEZrGCozdqR},
	abstract = {This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning—finetuning language models on a collection of datasets described via instructions—substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.},
	language = {en},
	urldate = {2024-03-24},
	author = {Wei, Jason and Bosma, Maarten and Zhao, Vincent and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M. and Le, Quoc V.},
	month = oct,
	year = {2021},
	keywords = {instruction tuning},
	file = {Full Text PDF:/Users/zhenqian/Zotero/storage/HRJZP34J/Wei et al. - 2021 - Finetuned Language Models are Zero-Shot Learners.pdf:application/pdf},
}

@misc{sanh_multitask_2022,
	title = {Multitask {Prompted} {Training} {Enables} {Zero}-{Shot} {Task} {Generalization}},
	url = {http://arxiv.org/abs/2110.08207},
	doi = {10.48550/arXiv.2110.08207},
	abstract = {Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks (Brown et al., 2020). It has been hypothesized that this is a consequence of implicit multitask learning in language models' pretraining (Radford et al., 2019). Can zero-shot generalization instead be directly induced by explicit multitask learning? To test this question at scale, we develop a system for easily mapping any natural language tasks into a human-readable prompted form. We convert a large set of supervised datasets, each with multiple prompts with diverse wording. These prompted datasets allow for benchmarking the ability of a model to perform completely held-out tasks. We fine-tune a pretrained encoder-decoder model (Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a wide variety of tasks. The model attains strong zero-shot performance on several standard datasets, often outperforming models up to 16x its size. Further, our approach attains strong performance on a subset of tasks from the BIG-bench benchmark, outperforming models up to 6x its size. All trained models are available at https://github.com/bigscience-workshop/t-zero and all prompts are available at https://github.com/bigscience-workshop/promptsource.},
	urldate = {2024-03-24},
	publisher = {arXiv},
	author = {Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H. and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Scao, Teven Le and Raja, Arun and Dey, Manan and Bari, M. Saiful and Xu, Canwen and Thakker, Urmish and Sharma, Shanya Sharma and Szczechla, Eliza and Kim, Taewoon and Chhablani, Gunjan and Nayak, Nihal and Datta, Debajyoti and Chang, Jonathan and Jiang, Mike Tian-Jian and Wang, Han and Manica, Matteo and Shen, Sheng and Yong, Zheng Xin and Pandey, Harshit and Bawden, Rachel and Wang, Thomas and Neeraj, Trishala and Rozen, Jos and Sharma, Abheesht and Santilli, Andrea and Fevry, Thibault and Fries, Jason Alan and Teehan, Ryan and Bers, Tali and Biderman, Stella and Gao, Leo and Wolf, Thomas and Rush, Alexander M.},
	month = mar,
	year = {2022},
	note = {arXiv:2110.08207 [cs]},
	keywords = {instruction tuning},
	annote = {Comment: ICLR 2022 Spotlight (with extended discussion)},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/NTZLJQBX/Sanh et al. - 2022 - Multitask Prompted Training Enables Zero-Shot Task.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/IDEBJDDF/2110.html:text/html},
}

@misc{lewis_bart_2019,
	title = {{BART}: {Denoising} {Sequence}-to-{Sequence} {Pre}-training for {Natural} {Language} {Generation}, {Translation}, and {Comprehension}},
	shorttitle = {{BART}},
	url = {http://arxiv.org/abs/1910.13461},
	doi = {10.48550/arXiv.1910.13461},
	abstract = {We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance.},
	urldate = {2024-04-16},
	publisher = {arXiv},
	author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
	month = oct,
	year = {2019},
	note = {arXiv:1910.13461 [cs, stat]},
	keywords = {llm},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/EQ8GW77R/Lewis et al. - 2019 - BART Denoising Sequence-to-Sequence Pre-training .pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/3SVGBLL9/1910.html:text/html},
}

@misc{rafailov_direct_2023,
	title = {Direct {Preference} {Optimization}: {Your} {Language} {Model} is {Secretly} a {Reward} {Model}},
	shorttitle = {Direct {Preference} {Optimization}},
	url = {http://arxiv.org/abs/2305.18290},
	doi = {10.48550/arXiv.2305.18290},
	abstract = {While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.},
	urldate = {2024-04-17},
	publisher = {arXiv},
	author = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D. and Finn, Chelsea},
	month = dec,
	year = {2023},
	note = {arXiv:2305.18290 [cs]},
	keywords = {model training},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/E78KUC7W/Rafailov et al. - 2023 - Direct Preference Optimization Your Language Mode.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/MHHYNQ4F/2305.html:text/html},
}

@article{radford_language_nodate,
	title = {Language {Models} are {Unsupervised} {Multitask} {Learners}},
	abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts WebText. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
	language = {en},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	keywords = {llm, gpt2},
	file = {Radford et al. - Language Models are Unsupervised Multitask Learner.pdf:/Users/zhenqian/Zotero/storage/7TDYRTJ5/Radford et al. - Language Models are Unsupervised Multitask Learner.pdf:application/pdf},
}

@misc{touvron_llama_2023,
	title = {{LLaMA}: {Open} and {Efficient} {Foundation} {Language} {Models}},
	shorttitle = {{LLaMA}},
	url = {http://arxiv.org/abs/2302.13971},
	doi = {10.48550/arXiv.2302.13971},
	abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
	urldate = {2024-04-19},
	publisher = {arXiv},
	author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
	month = feb,
	year = {2023},
	note = {arXiv:2302.13971 [cs]},
	keywords = {llm},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/3NMCM9MD/Touvron et al. - 2023 - LLaMA Open and Efficient Foundation Language Mode.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/JDKMX6YK/2302.html:text/html},
}

@misc{kumar_comprehensive_nodate,
	title = {Comprehensive {Overview} of {GPT}, {LLaMA}, and {PaLM} {Large} {Language} {Model} {Families}},
	url = {https://www.linkedin.com/pulse/comprehensive-overview-gpt-llama-palm-large-language-model-7r6tc/},
	urldate = {2024-04-19},
	author = {Kumar, Sanjay},
	keywords = {llm},
	file = {(23) Comprehensive Overview of GPT, LLaMA, and PaLM Large Language Model Families | LinkedIn:/Users/zhenqian/Zotero/storage/BTL7VXWX/comprehensive-overview-gpt-llama-palm-large-language-model-7r6tc.html:text/html},
}

@misc{ai_mistral_2023,
	title = {Mistral {7B}},
	url = {https://mistral.ai/news/announcing-mistral-7b/},
	abstract = {The best 7B model to date, Apache 2.0},
	language = {en-us},
	urldate = {2024-04-19},
	author = {AI, Mistral},
	month = sep,
	year = {2023},
	note = {Section: news},
	keywords = {llm},
	file = {Snapshot:/Users/zhenqian/Zotero/storage/ESQUIGSK/announcing-mistral-7b.html:text/html},
}

@misc{raffel_exploring_2023,
	title = {Exploring the {Limits} of {Transfer} {Learning} with a {Unified} {Text}-to-{Text} {Transformer}},
	url = {http://arxiv.org/abs/1910.10683},
	abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
	urldate = {2024-04-19},
	publisher = {arXiv},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	month = sep,
	year = {2023},
	note = {arXiv:1910.10683 [cs, stat]},
	keywords = {unread},
	file = {arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/G3F9LZR9/1910.html:text/html;Full Text PDF:/Users/zhenqian/Zotero/storage/Y356UISU/Raffel et al. - 2023 - Exploring the Limits of Transfer Learning with a U.pdf:application/pdf},
}

@misc{zhang_pegasus_2020,
	title = {{PEGASUS}: {Pre}-training with {Extracted} {Gap}-sentences for {Abstractive} {Summarization}},
	shorttitle = {{PEGASUS}},
	url = {http://arxiv.org/abs/1912.08777},
	abstract = {Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-training large Transformer-based encoder-decoder models on massive text corpora with a new self-supervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets.},
	urldate = {2024-04-19},
	publisher = {arXiv},
	author = {Zhang, Jingqing and Zhao, Yao and Saleh, Mohammad and Liu, Peter J.},
	month = jul,
	year = {2020},
	note = {arXiv:1912.08777 [cs]},
	keywords = {summarization, llm},
	annote = {Comment: Added results from mixed+stochastic model, test-set overlapping analysis; Code link added; Accepted for ICML 2020. arXiv admin note: text overlap with arXiv:1605.06560, arXiv:1205.2395, arXiv:0902.4351, arXiv:1610.09932, arXiv:nucl-ex/0512029 by other authors},
	file = {arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/WQ9CS5FU/1912.html:text/html;Full Text PDF:/Users/zhenqian/Zotero/storage/AUCKNLZ5/Zhang et al. - 2020 - PEGASUS Pre-training with Extracted Gap-sentences.pdf:application/pdf},
}

@misc{noauthor_what_nodate,
	title = {What {Is} {Instruction} {Tuning}? {\textbar} {IBM}},
	shorttitle = {What {Is} {Instruction} {Tuning}?},
	url = {https://www.ibm.com/topics/instruction-tuning},
	abstract = {Instruction tuning is a technique for fine-tuning large language models (LLMs) to improve model performance on natural language instruction following.},
	language = {en-us},
	urldate = {2024-04-19},
	keywords = {instruction tuning},
	file = {Snapshot:/Users/zhenqian/Zotero/storage/N57K3473/instruction-tuning.html:text/html},
}

@misc{chung_scaling_2022,
	title = {Scaling {Instruction}-{Finetuned} {Language} {Models}},
	url = {http://arxiv.org/abs/2210.11416},
	doi = {10.48550/arXiv.2210.11416},
	abstract = {Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4\% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2\% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.},
	urldate = {2024-04-19},
	publisher = {arXiv},
	author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Castro-Ros, Alex and Pellat, Marie and Robinson, Kevin and Valter, Dasha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},
	month = dec,
	year = {2022},
	note = {arXiv:2210.11416 [cs]},
	keywords = {instruction tuning},
	annote = {Comment: Public checkpoints: https://huggingface.co/docs/transformers/model\_doc/flan-t5},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/MFIKUMLL/Chung et al. - 2022 - Scaling Instruction-Finetuned Language Models.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/SN83538Q/2210.html:text/html},
}

@misc{huang_numhg_2023,
	title = {{NumHG}: {A} {Dataset} for {Number}-{Focused} {Headline} {Generation}},
	shorttitle = {{NumHG}},
	url = {http://arxiv.org/abs/2309.01455},
	doi = {10.48550/arXiv.2309.01455},
	abstract = {Headline generation, a key task in abstractive summarization, strives to condense a full-length article into a succinct, single line of text. Notably, while contemporary encoder-decoder models excel based on the ROUGE metric, they often falter when it comes to the precise generation of numerals in headlines. We identify the lack of datasets providing fine-grained annotations for accurate numeral generation as a major roadblock. To address this, we introduce a new dataset, the NumHG, and provide over 27,000 annotated numeral-rich news articles for detailed investigation. Further, we evaluate five well-performing models from previous headline generation tasks using human evaluation in terms of numerical accuracy, reasonableness, and readability. Our study reveals a need for improvement in numerical accuracy, demonstrating the potential of the NumHG dataset to drive progress in number-focused headline generation and stimulate further discussions in numeral-focused text generation.},
	urldate = {2024-04-20},
	publisher = {arXiv},
	author = {Huang, Jian-Tao and Chen, Chung-Chi and Huang, Hen-Hsen and Chen, Hsin-Hsi},
	month = sep,
	year = {2023},
	note = {arXiv:2309.01455 [cs]},
	keywords = {dataset},
	annote = {Comment: NumEval@SemEval-2024 Dataset},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/6JIFPVSC/Huang et al. - 2023 - NumHG A Dataset for Number-Focused Headline Gener.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/TWY75XUC/2309.html:text/html},
}


@inproceedings{huang-etal-2024-numhg,
    title = "{N}um{HG}: A Dataset for Number-Focused Headline Generation",
    author = "Huang, Jian-Tao  and
      Chen, Chung-Chi  and
      Huang, Hen-Hsen  and
      Chen, Hsin-Hsi",
    editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.1078",
    pages = "12323--12329",
    abstract = "Headline generation, a key task in abstractive summarization, strives to condense a full-length article into a succinct, single line of text. Notably, while contemporary encoder-decoder models excel based on the ROUGE metric, they often falter when it comes to the precise generation of numerals in headlines. We identify the lack of datasets providing fine-grained annotations for accurate numeral generation as a major roadblock. To address this, we introduce a new dataset, the NumHG, and provide over 27,000 annotated numeral-rich news articles for detailed investigation. Further, we evaluate five well-performing models from previous headline-generation tasks using human evaluation in terms of numerical accuracy, reasonableness, and readability. Our study reveals a need for improvement in numerical accuracy, demonstrating the potential of the NumHG dataset to drive progress in number-focused headline generation and stimulate further discussions in numeral-focused text generation.",
}


@misc{zhao_moverscore_2019,
	title = {{MoverScore}: {Text} {Generation} {Evaluating} with {Contextualized} {Embeddings} and {Earth} {Mover} {Distance}},
	shorttitle = {{MoverScore}},
	url = {http://arxiv.org/abs/1909.02622},
	abstract = {A robust evaluation metric has a profound impact on the development of text generation systems. A desirable metric compares system output against references based on their semantics rather than surface forms. In this paper we investigate strategies to encode system and reference texts to devise a metric that shows a high correlation with human judgment of text quality. We validate our new metric, namely MoverScore, on a number of text generation tasks including summarization, machine translation, image captioning, and data-to-text generation, where the outputs are produced by a variety of neural and non-neural systems. Our findings suggest that metrics combining contextualized representations with a distance measure perform the best. Such metrics also demonstrate strong generalization capability across tasks. For ease-of-use we make our metrics available as web service.},
	urldate = {2024-04-20},
	publisher = {arXiv},
	author = {Zhao, Wei and Peyrard, Maxime and Liu, Fei and Gao, Yang and Meyer, Christian M. and Eger, Steffen},
	month = sep,
	year = {2019},
	note = {arXiv:1909.02622 [cs]},
	keywords = {evaluation},
	annote = {Comment: EMNLP19 Camera-Ready},
	file = {arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/MQH3BWC8/1909.html:text/html;Full Text PDF:/Users/zhenqian/Zotero/storage/RJFMGUXF/Zhao et al. - 2019 - MoverScore Text Generation Evaluating with Contex.pdf:application/pdf},
}

@inproceedings{chu_learning_2020,
	address = {Taipei Taiwan},
	title = {Learning to {Generate} {Correct} {Numeric} {Values} in {News} {Headlines}},
	isbn = {978-1-4503-7024-0},
	url = {https://dl.acm.org/doi/10.1145/3366424.3382676},
	doi = {10.1145/3366424.3382676},
	abstract = {Motivated by the significant role of numeric values to convey concise and accurate information in news headlines, we focus the headline generation task on displaying correct numbers. We propose various ways to present the numeric values to the generative model. In the end, we come up with a simple but effective pre-train task to guide the generator to correctly process the values, which outperforms other base models even if the numbers in the headline are newly generated from the article.},
	language = {en},
	urldate = {2024-04-21},
	booktitle = {Companion {Proceedings} of the {Web} {Conference} 2020},
	publisher = {ACM},
	author = {Chu, Jui and Chen, Chung-Chi and Huang, Hen-Hsen and Chen, Hsin-Hsi},
	month = apr,
	year = {2020},
	keywords = {headline generation},
	pages = {17--18},
	file = {Chu et al. - 2020 - Learning to Generate Correct Numeric Values in New.pdf:/Users/zhenqian/Zotero/storage/IL3SR6KY/Chu et al. - 2020 - Learning to Generate Correct Numeric Values in New.pdf:application/pdf},
}

@misc{wang_self-consistency_2023,
	title = {Self-{Consistency} {Improves} {Chain} of {Thought} {Reasoning} in {Language} {Models}},
	url = {http://arxiv.org/abs/2203.11171},
	doi = {10.48550/arXiv.2203.11171},
	abstract = {Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9\%), SVAMP (+11.0\%), AQuA (+12.2\%), StrategyQA (+6.4\%) and ARC-challenge (+3.9\%).},
	urldate = {2024-04-22},
	publisher = {arXiv},
	author = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
	month = mar,
	year = {2023},
	note = {arXiv:2203.11171 [cs]},
	keywords = {inference},
	annote = {Comment: Published at ICLR 2023. V2: added PaLM results; V3: added UL2 results; V4: camera ready version at ICLR 2023},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/WQSQ4HFY/Wang et al. - 2023 - Self-Consistency Improves Chain of Thought Reasoni.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/DX6B3NI2/2203.html:text/html},
}

@misc{zhou_large_2023,
	title = {Large {Language} {Models} {Are} {Human}-{Level} {Prompt} {Engineers}},
	url = {http://arxiv.org/abs/2211.01910},
	doi = {10.48550/arXiv.2211.01910},
	abstract = {By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the "program," optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 19/24 tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts. Please check out our webpage at https://sites.google.com/view/automatic-prompt-engineer.},
	urldate = {2024-04-22},
	publisher = {arXiv},
	author = {Zhou, Yongchao and Muresanu, Andrei Ioan and Han, Ziwen and Paster, Keiran and Pitis, Silviu and Chan, Harris and Ba, Jimmy},
	month = mar,
	year = {2023},
	note = {arXiv:2211.01910 [cs]},
	keywords = {unread},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/PB8Y9ELH/Zhou et al. - 2023 - Large Language Models Are Human-Level Prompt Engin.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/GANMVW63/2211.html:text/html},
}

@misc{dabkowski_large_2023,
	title = {Large language models and (non-)linguistic recursion},
	url = {http://arxiv.org/abs/2306.07195},
	doi = {10.48550/arXiv.2306.07195},
	abstract = {Recursion is one of the hallmarks of human language. While many design features of language have been shown to exist in animal communication systems, recursion has not. Previous research shows that GPT-4 is the first large language model (LLM) to exhibit metalinguistic abilities (Begu{\textbackslash}v\{s\}, D{\textbackslash}k\{a\}bkowski, and Rhodes 2023). Here, we propose several prompt designs aimed at eliciting and analyzing recursive behavior in LLMs, both linguistic and non-linguistic. We demonstrate that when explicitly prompted, GPT-4 can both produce and analyze recursive structures. Thus, we present one of the first studies investigating whether meta-linguistic awareness of recursion -- a uniquely human cognitive property -- can emerge in transformers with a high number of parameters such as GPT-4.},
	urldate = {2024-04-22},
	publisher = {arXiv},
	author = {Dąbkowski, Maksymilian and Beguš, Gašper},
	month = jun,
	year = {2023},
	note = {arXiv:2306.07195 [cs]},
	keywords = {unread},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/5ZR3JZ7Z/Dąbkowski and Beguš - 2023 - Large language models and (non-)linguistic recursi.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/F2YT3888/2306.html:text/html},
}

@inproceedings{akyurek_rl4f_2023,
	address = {Toronto, Canada},
	title = {{RL4F}: {Generating} {Natural} {Language} {Feedback} with {Reinforcement} {Learning} for {Repairing} {Model} {Outputs}},
	shorttitle = {{RL4F}},
	url = {https://aclanthology.org/2023.acl-long.427},
	doi = {10.18653/v1/2023.acl-long.427},
	abstract = {Despite their unprecedented success, even the largest language models make mistakes. Similar to how humans learn and improve using feedback, previous work proposed providing language models with natural language feedback to guide them in repairing their outputs. Because human-generated critiques are expensive to obtain, researchers have devised learned critique generators in lieu of human critics while assuming one can train downstream models to utilize generated feedback. However, this approach does not apply to black-box or limited access models such as ChatGPT, as they cannot be fine-tuned. Moreover, in the era of large general-purpose language agents, fine-tuning is neither computationally nor spatially efficient as it results in multiple copies of the network. In this work, we introduce RL4F (Reinforcement Learning for Feedback), a multi-agent collaborative framework where the critique generator is trained to maximize end-task performance of GPT-3, a fixed model more than 200 times its size. RL4F produces critiques that help GPT-3 revise its outputs. We study three datasets for action planning, summarization and alphabetization and show relative improvements up to 10\% in multiple text similarity metrics over other learned, retrieval-augmented or prompting-based critique generators.},
	urldate = {2024-04-22},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Akyurek, Afra Feyza and Akyurek, Ekin and Kalyan, Ashwin and Clark, Peter and Wijaya, Derry Tanti and Tandon, Niket},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	keywords = {reinforcement learning},
	pages = {7716--7733},
	file = {Full Text PDF:/Users/zhenqian/Zotero/storage/7S4CDKYB/Akyurek et al. - 2023 - RL4F Generating Natural Language Feedback with Re.pdf:application/pdf},
}

@misc{paranjape_art_2023,
	title = {{ART}: {Automatic} multi-step reasoning and tool-use for large language models},
	shorttitle = {{ART}},
	url = {http://arxiv.org/abs/2303.09014},
	doi = {10.48550/arXiv.2303.09014},
	abstract = {Large language models (LLMs) can perform complex reasoning in few- and zero-shot settings by generating intermediate chain of thought (CoT) reasoning steps. Further, each reasoning step can rely on external tools to support computation beyond the core LLM capabilities (e.g. search/running code). Prior work on CoT prompting and tool use typically requires hand-crafting task-specific demonstrations and carefully scripted interleaving of model generations with tool use. We introduce Automatic Reasoning and Tool-use (ART), a framework that uses frozen LLMs to automatically generate intermediate reasoning steps as a program. Given a new task to solve, ART selects demonstrations of multi-step reasoning and tool use from a task library. At test time, ART seamlessly pauses generation whenever external tools are called, and integrates their output before resuming generation. ART achieves a substantial improvement over few-shot prompting and automatic CoT on unseen tasks in the BigBench and MMLU benchmarks, and matches performance of hand-crafted CoT prompts on a majority of these tasks. ART is also extensible, and makes it easy for humans to improve performance by correcting errors in task-specific programs or incorporating new tools, which we demonstrate by drastically improving performance on select tasks with minimal human intervention.},
	urldate = {2024-04-22},
	publisher = {arXiv},
	author = {Paranjape, Bhargavi and Lundberg, Scott and Singh, Sameer and Hajishirzi, Hannaneh and Zettlemoyer, Luke and Ribeiro, Marco Tulio},
	month = mar,
	year = {2023},
	note = {arXiv:2303.09014 [cs]},
	keywords = {rationale generation},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/S9UN9R6F/Paranjape et al. - 2023 - ART Automatic multi-step reasoning and tool-use f.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/FLTI9EBG/2303.html:text/html},
}

@misc{wu_enhancing_2024,
	title = {Enhancing {Large} {Language} {Model} with {Decomposed} {Reasoning} for {Emotion} {Cause} {Pair} {Extraction}},
	url = {http://arxiv.org/abs/2401.17716},
	doi = {10.48550/arXiv.2401.17716},
	abstract = {Emotion-Cause Pair Extraction (ECPE) involves extracting clause pairs representing emotions and their causes in a document. Existing methods tend to overfit spurious correlations, such as positional bias in existing benchmark datasets, rather than capturing semantic features. Inspired by recent work, we explore leveraging large language model (LLM) to address ECPE task without additional training. Despite strong capabilities, LLMs suffer from uncontrollable outputs, resulting in mediocre performance. To address this, we introduce chain-of-thought to mimic human cognitive process and propose the Decomposed Emotion-Cause Chain (DECC) framework. Combining inducing inference and logical pruning, DECC guides LLMs to tackle ECPE task. We further enhance the framework by incorporating in-context learning. Experiment results demonstrate the strength of DECC compared to state-of-the-art supervised fine-tuning methods. Finally, we analyze the effectiveness of each component and the robustness of the method in various scenarios, including different LLM bases, rebalanced datasets, and multi-pair extraction.},
	urldate = {2024-04-22},
	publisher = {arXiv},
	author = {Wu, Jialiang and Shen, Yi and Zhang, Ziheng and Cai, Longjun},
	month = jan,
	year = {2024},
	note = {arXiv:2401.17716 [cs]},
	keywords = {rationale generation},
	annote = {Comment: 13 pages, 5 figures},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/WL4VZDS2/Wu et al. - 2024 - Enhancing Large Language Model with Decomposed Rea.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/NTD76UMA/2401.html:text/html},
}

@misc{liu_brio_2022,
	title = {{BRIO}: {Bringing} {Order} to {Abstractive} {Summarization}},
	shorttitle = {{BRIO}},
	url = {http://arxiv.org/abs/2203.16804},
	doi = {10.48550/arXiv.2203.16804},
	abstract = {Abstractive summarization models are commonly trained using maximum likelihood estimation, which assumes a deterministic (one-point) target distribution in which an ideal model will assign all the probability mass to the reference summary. This assumption may lead to performance degradation during inference, where the model needs to compare several system-generated (candidate) summaries that have deviated from the reference summary. To address this problem, we propose a novel training paradigm which assumes a non-deterministic distribution so that different candidate summaries are assigned probability mass according to their quality. Our method achieves a new state-of-the-art result on the CNN/DailyMail (47.78 ROUGE-1) and XSum (49.07 ROUGE-1) datasets. Further analysis also shows that our model can estimate probabilities of candidate summaries that are more correlated with their level of quality.},
	urldate = {2024-04-22},
	publisher = {arXiv},
	author = {Liu, Yixin and Liu, Pengfei and Radev, Dragomir and Neubig, Graham},
	month = mar,
	year = {2022},
	note = {arXiv:2203.16804 [cs]},
	keywords = {summarization},
	annote = {Comment: ACL 2022},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/YZCG74XC/Liu et al. - 2022 - BRIO Bringing Order to Abstractive Summarization.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/FEWIBH26/2203.html:text/html},
}

@misc{kojima_large_2023,
	title = {Large {Language} {Models} are {Zero}-{Shot} {Reasoners}},
	url = {http://arxiv.org/abs/2205.11916},
	doi = {10.48550/arXiv.2205.11916},
	abstract = {Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding "Let's think step by step" before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7\% to 78.7\% and GSM8K from 10.4\% to 40.7\% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.},
	urldate = {2024-04-29},
	publisher = {arXiv},
	author = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
	month = jan,
	year = {2023},
	note = {arXiv:2205.11916 [cs]},
	keywords = {rationale generation},
	annote = {Comment: Accepted to NeurIPS2022. Our code is available at https://github.com/kojima-takeshi188/zero\_shot\_cot},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/FEZTRC9W/Kojima et al. - 2023 - Large Language Models are Zero-Shot Reasoners.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/SGN5NTX9/2205.html:text/html},
}

@misc{zhang_automatic_2022,
	title = {Automatic {Chain} of {Thought} {Prompting} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2210.03493},
	doi = {10.48550/arXiv.2210.03493},
	abstract = {Large language models (LLMs) can perform complex reasoning by generating intermediate reasoning steps. Providing these steps for prompting demonstrations is called chain-of-thought (CoT) prompting. CoT prompting has two major paradigms. One leverages a simple prompt like "Let's think step by step" to facilitate step-by-step thinking before answering a question. The other uses a few manual demonstrations one by one, each composed of a question and a reasoning chain that leads to an answer. The superior performance of the second paradigm hinges on the hand-crafting of task-specific demonstrations one by one. We show that such manual efforts may be eliminated by leveraging LLMs with the "Let's think step by step" prompt to generate reasoning chains for demonstrations one by one, i.e., let's think not just step by step, but also one by one. However, these generated chains often come with mistakes. To mitigate the effect of such mistakes, we find that diversity matters for automatically constructing demonstrations. We propose an automatic CoT prompting method: Auto-CoT. It samples questions with diversity and generates reasoning chains to construct demonstrations. On ten public benchmark reasoning tasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. Code is available at https://github.com/amazon-research/auto-cot},
	urldate = {2024-04-29},
	publisher = {arXiv},
	author = {Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Smola, Alex},
	month = oct,
	year = {2022},
	note = {arXiv:2210.03493 [cs]},
	keywords = {rationale generation},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/QMNH6R5E/Zhang et al. - 2022 - Automatic Chain of Thought Prompting in Large Lang.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/K3ENA9FF/2210.html:text/html},
}

@misc{shum_automatic_2024,
	title = {Automatic {Prompt} {Augmentation} and {Selection} with {Chain}-of-{Thought} from {Labeled} {Data}},
	url = {http://arxiv.org/abs/2302.12822},
	doi = {10.48550/arXiv.2302.12822},
	abstract = {Chain-of-thought (CoT) advances the reasoning abilities of large language models (LLMs) and achieves superior performance in complex reasoning tasks. However, most CoT studies rely on carefully designed human-annotated rational chains to prompt LLMs, posing challenges for real-world applications where labeled data is available without rational chains. This paper proposes a new strategy, Automate-CoT (Automatic Prompt Augmentation and Selection with Chain-of-Thought), that can bypass human engineering of CoT by automatically augmenting rational chains from a small labeled dataset, and then pruning low-quality chains to construct a candidate pool of machine-generated rationale chains based on the labels. Finally, it selects the optimal combination of several rationale chains from the pool for CoT prompting by employing a variance-reduced policy gradient strategy to estimate the significance of each example. Automate-CoT enables a quick adaptation of the CoT technique to different tasks. Experimental results demonstrate the effectiveness of our method, where competitive results are achieved on arithmetic reasoning (+2.7\%), commonsense reasoning (+3.4\%), symbolic reasoning (+3.2\%), and non-reasoning tasks (+2.5\%). The code is available at https://github.com/SHUMKASHUN/Automate-CoT.},
	urldate = {2024-05-01},
	publisher = {arXiv},
	author = {Shum, KaShun and Diao, Shizhe and Zhang, Tong},
	month = feb,
	year = {2024},
	note = {arXiv:2302.12822 [cs]},
	keywords = {rationale generation},
	annote = {Comment: EMNLP 2023},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/AWRZ8JEY/Shum et al. - 2024 - Automatic Prompt Augmentation and Selection with C.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/G4XE5DAN/2302.html:text/html},
}

@misc{shao_synthetic_2023,
	title = {Synthetic {Prompting}: {Generating} {Chain}-of-{Thought} {Demonstrations} for {Large} {Language} {Models}},
	shorttitle = {Synthetic {Prompting}},
	url = {http://arxiv.org/abs/2302.00618},
	doi = {10.48550/arXiv.2302.00618},
	abstract = {Large language models can perform various reasoning tasks by using chain-of-thought prompting, which guides them to find answers through step-by-step demonstrations. However, the quality of the prompts depends on the demonstrations given to the models, and creating many of them by hand is costly. We introduce Synthetic prompting, a method that leverages a few handcrafted examples to prompt the model to generate more examples by itself, and selects effective demonstrations to elicit better reasoning. Our method alternates between a backward and forward process to generate new examples. The backward process generates a question that match a sampled reasoning chain, so that the question is solvable and clear. The forward process produces a more detailed reasoning chain for the question, improving the quality of the example. We evaluate our method on numerical, symbolic, and algorithmic reasoning tasks, and show that it outperforms existing prompting techniques.},
	urldate = {2024-05-01},
	publisher = {arXiv},
	author = {Shao, Zhihong and Gong, Yeyun and Shen, Yelong and Huang, Minlie and Duan, Nan and Chen, Weizhu},
	month = feb,
	year = {2023},
	note = {arXiv:2302.00618 [cs]},
	keywords = {rationale generation},
	annote = {Comment: Preprint},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/3JAKZVZ3/Shao et al. - 2023 - Synthetic Prompting Generating Chain-of-Thought D.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/FVMB7IR2/2302.html:text/html},
}

@misc{khaki_rs-dpo_2024,
	title = {{RS}-{DPO}: {A} {Hybrid} {Rejection} {Sampling} and {Direct} {Preference} {Optimization} {Method} for {Alignment} of {Large} {Language} {Models}},
	shorttitle = {{RS}-{DPO}},
	url = {http://arxiv.org/abs/2402.10038},
	abstract = {Reinforcement learning from human feedback (RLHF) has been extensively employed to align large language models with user intent. However, proximal policy optimization (PPO) based RLHF is occasionally unstable requiring significant hyperparameter finetuning, and computationally expensive to maximize the estimated reward during alignment. Recently, direct preference optimization (DPO) is proposed to address those challenges. However, DPO often relies on contrastive responses generated from human annotator and alternative LLM, instead of the policy model, limiting the effectiveness of the RLHF. In this paper, we addresses both challenges by systematically combining rejection sampling (RS) and DPO. Our proposed method, RS-DPO, initiates with the development of a supervised fine-tuned policy model (SFT). A varied set of k responses per prompt are sampled directly from the SFT model. RS-DPO identifies pairs of contrastive samples based on their reward distribution. Finally, we apply DPO with the contrastive samples to align the model to human preference. Our experiments indicate that our proposed method effectively fine-tunes LLMs with limited resource environments, leading to improved alignment with user intent. Furthermore, it outperforms existing methods, including RS, PPO, and DPO.},
	language = {en},
	urldate = {2024-05-06},
	publisher = {arXiv},
	author = {Khaki, Saeed and Li, JinJin and Ma, Lan and Yang, Liu and Ramachandra, Prathap},
	month = mar,
	year = {2024},
	note = {arXiv:2402.10038 [cs]},
	keywords = {model training},
	annote = {Comment: 16 pages, 4 figures},
	file = {Khaki et al. - 2024 - RS-DPO A Hybrid Rejection Sampling and Direct Pre.pdf:/Users/zhenqian/Zotero/storage/NCFTPNDE/Khaki et al. - 2024 - RS-DPO A Hybrid Rejection Sampling and Direct Pre.pdf:application/pdf},
}

@article{giarelis_abstractive_2023,
	title = {Abstractive vs. {Extractive} {Summarization}: {An} {Experimental} {Review}},
	volume = {13},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	shorttitle = {Abstractive vs. {Extractive} {Summarization}},
	url = {https://www.mdpi.com/2076-3417/13/13/7620},
	doi = {10.3390/app13137620},
	abstract = {Text summarization is a subtask of natural language processing referring to the automatic creation of a concise and fluent summary that captures the main ideas and topics from one or multiple documents. Earlier literature surveys focus on extractive approaches, which rank the top-n most important sentences in the input document and then combine them to form a summary. As argued in the literature, the summaries of these approaches do not have the same lexical flow or coherence as summaries that are manually produced by humans. Newer surveys elaborate abstractive approaches, which generate a summary with potentially new phrases and sentences compared to the input document. Generally speaking, contrary to the extractive approaches, the abstractive ones create summaries that are more similar to those produced by humans. However, these approaches still lack the contextual representation needed to form fluent summaries. Recent advancements in deep learning and pretrained language models led to the improvement of many natural language processing tasks, including abstractive summarization. Overall, these surveys do not present a comprehensive evaluation framework that assesses the aforementioned approaches. Taking the above into account, the contribution of this survey is fourfold: (i) we provide a comprehensive survey of the state-of-the-art approaches in text summarization; (ii) we conduct a comparative evaluation of these approaches, using well-known datasets from the related literature, as well as popular evaluation scores such as ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-LSUM, BLEU-1, BLEU-2 and SACREBLEU; (iii) we report on insights gained on various aspects of the text summarization process, including existing approaches, datasets and evaluation methods, and we outline a set of open issues and future research directions; (iv) we upload the datasets and the code used in our experiments in a public repository, aiming to increase the reproducibility of this work and facilitate future research in the field.},
	language = {en},
	number = {13},
	urldate = {2024-05-26},
	journal = {Applied Sciences},
	author = {Giarelis, Nikolaos and Mastrokostas, Charalampos and Karacapilidis, Nikos},
	month = jan,
	year = {2023},
	note = {Number: 13
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {summarization},
	pages = {7620},
	file = {Full Text PDF:/Users/zhenqian/Zotero/storage/QEMSD4GY/Giarelis et al. - 2023 - Abstractive vs. Extractive Summarization An Exper.pdf:application/pdf},
}

@misc{jin_hooks_2020,
	title = {Hooks in the {Headline}: {Learning} to {Generate} {Headlines} with {Controlled} {Styles}},
	shorttitle = {Hooks in the {Headline}},
	url = {http://arxiv.org/abs/2004.01980},
	doi = {10.48550/arXiv.2004.01980},
	abstract = {Current summarization systems only produce plain, factual headlines, but do not meet the practical needs of creating memorable titles to increase exposure. We propose a new task, Stylistic Headline Generation (SHG), to enrich the headlines with three style options (humor, romance and clickbait), in order to attract more readers. With no style-specific article-headline pair (only a standard headline summarization dataset and mono-style corpora), our method TitleStylist generates style-specific headlines by combining the summarization and reconstruction tasks into a multitasking framework. We also introduced a novel parameter sharing scheme to further disentangle the style from the text. Through both automatic and human evaluation, we demonstrate that TitleStylist can generate relevant, fluent headlines with three target styles: humor, romance, and clickbait. The attraction score of our model generated headlines surpasses that of the state-of-the-art summarization model by 9.68\%, and even outperforms human-written references.},
	urldate = {2024-05-26},
	publisher = {arXiv},
	author = {Jin, Di and Jin, Zhijing and Zhou, Joey Tianyi and Orii, Lisa and Szolovits, Peter},
	month = may,
	year = {2020},
	note = {arXiv:2004.01980 [cs]},
	keywords = {summarization, headline generation},
	annote = {Comment: ACL 2020},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/2F3NQLDK/Jin et al. - 2020 - Hooks in the Headline Learning to Generate Headli.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/MYYFJ5IC/2004.html:text/html},
}

@misc{jin_comprehensive_2024,
	title = {A {Comprehensive} {Survey} on {Process}-{Oriented} {Automatic} {Text} {Summarization} with {Exploration} of {LLM}-{Based} {Methods}},
	url = {http://arxiv.org/abs/2403.02901},
	doi = {10.48550/arXiv.2403.02901},
	abstract = {Automatic Text Summarization (ATS), utilizing Natural Language Processing (NLP) algorithms, aims to create concise and accurate summaries, thereby significantly reducing the human effort required in processing large volumes of text. ATS has drawn considerable interest in both academic and industrial circles. Many studies have been conducted in the past to survey ATS methods; however, they generally lack practicality for real-world implementations, as they often categorize previous methods from a theoretical standpoint. Moreover, the advent of Large Language Models (LLMs) has altered conventional ATS methods. In this survey, we aim to 1) provide a comprehensive overview of ATS from a ``Process-Oriented Schema'' perspective, which is best aligned with real-world implementations; 2) comprehensively review the latest LLM-based ATS works; and 3) deliver an up-to-date survey of ATS, bridging the two-year gap in the literature. To the best of our knowledge, this is the first survey to specifically investigate LLM-based ATS methods.},
	urldate = {2024-05-26},
	publisher = {arXiv},
	author = {Jin, Hanlei and Zhang, Yang and Meng, Dan and Wang, Jun and Tan, Jinghua},
	month = mar,
	year = {2024},
	note = {arXiv:2403.02901 [cs]},
	keywords = {summarization},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/QXYQ8FAK/Jin et al. - 2024 - A Comprehensive Survey on Process-Oriented Automat.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/Z4NEWNAI/2403.html:text/html},
}

@inproceedings{lin_rouge_2004,
	address = {Barcelona, Spain},
	title = {{ROUGE}: {A} {Package} for {Automatic} {Evaluation} of {Summaries}},
	shorttitle = {{ROUGE}},
	url = {https://aclanthology.org/W04-1013},
	urldate = {2024-05-26},
	booktitle = {Text {Summarization} {Branches} {Out}},
	publisher = {Association for Computational Linguistics},
	author = {Lin, Chin-Yew},
	month = jul,
	year = {2004},
	keywords = {evaluation},
	pages = {74--81},
	file = {Full Text PDF:/Users/zhenqian/Zotero/storage/D7QH9VQI/Lin - 2004 - ROUGE A Package for Automatic Evaluation of Summa.pdf:application/pdf},
}

@article{ji_survey_2023,
	title = {Survey of {Hallucination} in {Natural} {Language} {Generation}},
	volume = {55},
	issn = {0360-0300},
	url = {https://dl.acm.org/doi/10.1145/3571730},
	doi = {10.1145/3571730},
	abstract = {Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation, and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before. In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions, and (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, and machine translation. This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG.},
	number = {12},
	urldate = {2024-05-26},
	journal = {ACM Computing Surveys},
	author = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
	month = mar,
	year = {2023},
	keywords = {hallucination},
	pages = {248:1--248:38},
	file = {Full Text PDF:/Users/zhenqian/Zotero/storage/IAIB3324/Ji et al. - 2023 - Survey of Hallucination in Natural Language Genera.pdf:application/pdf},
}

@misc{hu_lora_2021,
	title = {{LoRA}: {Low}-{Rank} {Adaptation} of {Large} {Language} {Models}},
	shorttitle = {{LoRA}},
	url = {http://arxiv.org/abs/2106.09685},
	abstract = {An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full ﬁne-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example – deploying independent instances of ﬁne-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B ﬁne-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than ﬁnetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deﬁciency in language model adaptation, which sheds light on the efﬁcacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
	language = {en},
	urldate = {2024-05-28},
	publisher = {arXiv},
	author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	month = oct,
	year = {2021},
	note = {arXiv:2106.09685 [cs]},
	keywords = {model training},
	annote = {Comment: Draft V2 includes better baselines, experiments on GLUE, and more on adapter latency},
	file = {Hu et al. - 2021 - LoRA Low-Rank Adaptation of Large Language Models.pdf:/Users/zhenqian/Zotero/storage/YCBUL98Z/Hu et al. - 2021 - LoRA Low-Rank Adaptation of Large Language Models.pdf:application/pdf},
}

@misc{wang_math-shepherd_2024,
	title = {Math-{Shepherd}: {Verify} and {Reinforce} {LLMs} {Step}-by-step without {Human} {Annotations}},
	shorttitle = {Math-{Shepherd}},
	url = {http://arxiv.org/abs/2312.08935},
	doi = {10.48550/arXiv.2312.08935},
	abstract = {In this paper, we present an innovative process-oriented math process reward model called {\textbackslash}textbf\{Math-Shepherd\}, which assigns a reward score to each step of math problem solutions. The training of Math-Shepherd is achieved using automatically constructed process-wise supervision data, breaking the bottleneck of heavy reliance on manual annotation in existing work. We explore the effectiveness of Math-Shepherd in two scenarios: 1) {\textbackslash}textit\{Verification\}: Math-Shepherd is utilized for reranking multiple outputs generated by Large Language Models (LLMs); 2) {\textbackslash}textit\{Reinforcement Learning\}: Math-Shepherd is employed to reinforce LLMs with step-by-step Proximal Policy Optimization (PPO). With Math-Shepherd, a series of open-source LLMs demonstrates exceptional performance. For instance, the step-by-step PPO with Math-Shepherd significantly improves the accuracy of Mistral-7B (77.9{\textbackslash}\%\${\textbackslash}to\$84.1{\textbackslash}\% on GSM8K and 28.6{\textbackslash}\%\${\textbackslash}to\$33.0{\textbackslash}\% on MATH). The accuracy can be further enhanced to 89.1{\textbackslash}\% and 43.5{\textbackslash}\% on GSM8K and MATH with the verification of Math-Shepherd, respectively. We believe that automatic process supervision holds significant potential for the future evolution of LLMs.},
	urldate = {2024-06-27},
	publisher = {arXiv},
	author = {Wang, Peiyi and Li, Lei and Shao, Zhihong and Xu, R. X. and Dai, Damai and Li, Yifei and Chen, Deli and Wu, Y. and Sui, Zhifang},
	month = feb,
	year = {2024},
	note = {arXiv:2312.08935 [cs]},
	keywords = {rl},
	annote = {Comment: Add Step-by-Step reinforcement learning results},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/HIAJKIFQ/Wang et al. - 2024 - Math-Shepherd Verify and Reinforce LLMs Step-by-s.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/ZID4QPAM/2312.html:text/html},
}

@misc{wang_t-sciq_2023,
	title = {T-{SciQ}: {Teaching} {Multimodal} {Chain}-of-{Thought} {Reasoning} via {Mixed} {Large} {Language} {Model} {Signals} for {Science} {Question} {Answering}},
	shorttitle = {T-{SciQ}},
	url = {http://arxiv.org/abs/2305.03453},
	doi = {10.48550/arXiv.2305.03453},
	abstract = {Large Language Models (LLMs) have recently demonstrated exceptional performance in various Natural Language Processing (NLP) tasks. They have also shown the ability to perform chain-of-thought (CoT) reasoning to solve complex problems. Recent studies have explored CoT reasoning in complex multimodal scenarios, such as the science question answering task, by fine-tuning multimodal models with high-quality human-annotated CoT rationales. However, collecting high-quality COT rationales is usually time-consuming and costly. Besides, the annotated rationales are hardly accurate due to the external essential information missed. To address these issues, we propose a novel method termed T-SciQ that aims at teaching science question answering with LLM signals. The T-SciQ approach generates high-quality CoT rationales as teaching signals and is advanced to train much smaller models to perform CoT reasoning in complex modalities. Additionally, we introduce a novel data mixing strategy to produce more effective teaching data samples for simple and complex science question answer problems. Extensive experimental results show that our T-SciQ method achieves a new state-of-the-art performance on the ScienceQA benchmark, with an accuracy of 96.18\%. Moreover, our approach outperforms the most powerful fine-tuned baseline by 4.5\%. The code is publicly available at https://github.com/T-SciQ/T-SciQ.},
	urldate = {2024-07-16},
	publisher = {arXiv},
	author = {Wang, Lei and Hu, Yi and He, Jiabang and Xu, Xing and Liu, Ning and Liu, Hui and Shen, Heng Tao},
	month = dec,
	year = {2023},
	note = {arXiv:2305.03453 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: AAAI 2024},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/VA8WPFT3/Wang et al. - 2023 - T-SciQ Teaching Multimodal Chain-of-Thought Reaso.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/FRBQ3NA2/2305.html:text/html},
}

@misc{stiennon_learning_2022,
	title = {Learning to summarize from human feedback},
	url = {http://arxiv.org/abs/2009.01325},
	doi = {10.48550/arXiv.2009.01325},
	abstract = {As language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task. For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about -- summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR dataset of Reddit posts and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to CNN/DM news articles, producing summaries nearly as good as the human reference without any news-specific fine-tuning. We conduct extensive analyses to understand our human feedback dataset and fine-tuned models We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans. We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want.},
	urldate = {2024-07-16},
	publisher = {arXiv},
	author = {Stiennon, Nisan and Ouyang, Long and Wu, Jeff and Ziegler, Daniel M. and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul},
	month = feb,
	year = {2022},
	note = {arXiv:2009.01325 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: NeurIPS 2020},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/Y67PU9NA/Stiennon et al. - 2022 - Learning to summarize from human feedback.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/25EQ9HRK/2009.html:text/html},
}

@misc{wang_plan-and-solve_2023,
	title = {Plan-and-{Solve} {Prompting}: {Improving} {Zero}-{Shot} {Chain}-of-{Thought} {Reasoning} by {Large} {Language} {Models}},
	shorttitle = {Plan-and-{Solve} {Prompting}},
	url = {http://arxiv.org/abs/2305.04091},
	doi = {10.48550/arXiv.2305.04091},
	abstract = {Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual effort, Zero-shot-CoT concatenates the target problem statement with "Let's think step by step" as an input prompt to LLMs. Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors. To address the missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. To address the calculation errors and improve the quality of generated reasoning steps, we extend PS prompting with more detailed instructions and derive PS+ prompting. We evaluate our proposed prompting strategy on ten datasets across three reasoning problems. The experimental results over GPT-3 show that our proposed zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem. The code can be found at https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.},
	urldate = {2024-07-23},
	publisher = {arXiv},
	author = {Wang, Lei and Xu, Wanyu and Lan, Yihuai and Hu, Zhiqiang and Lan, Yunshi and Lee, Roy Ka-Wei and Lim, Ee-Peng},
	month = may,
	year = {2023},
	note = {arXiv:2305.04091 [cs]},
	keywords = {cot},
	annote = {Comment: ACL 2023},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/49RDVZ4X/Wang et al. - 2023 - Plan-and-Solve Prompting Improving Zero-Shot Chai.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/WSJAJDE6/2305.html:text/html},
}

@misc{wang_element-aware_2023,
	title = {Element-aware {Summarization} with {Large} {Language} {Models}: {Expert}-aligned {Evaluation} and {Chain}-of-{Thought} {Method}},
	shorttitle = {Element-aware {Summarization} with {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2305.13412},
	doi = {10.48550/arXiv.2305.13412},
	abstract = {Automatic summarization generates concise summaries that contain key ideas of source documents. As the most mainstream datasets for the news sub-domain, CNN/DailyMail and BBC XSum have been widely used for performance benchmarking. However, the reference summaries of those datasets turn out to be noisy, mainly in terms of factual hallucination and information redundancy. To address this challenge, we first annotate new expert-writing Element-aware test sets following the "Lasswell Communication Model" proposed by Lasswell (1948), allowing reference summaries to focus on more fine-grained news elements objectively and comprehensively. Utilizing the new test sets, we observe the surprising zero-shot summary ability of LLMs, which addresses the issue of the inconsistent results between human preference and automatic evaluation metrics of LLMs' zero-shot summaries in prior work. Further, we propose a Summary Chain-of-Thought (SumCoT) technique to elicit LLMs to generate summaries step by step, which helps them integrate more fine-grained details of source documents into the final summaries that correlate with the human writing mindset. Experimental results show our method outperforms state-of-the-art fine-tuned PLMs and zero-shot LLMs by +4.33/+4.77 in ROUGE-L on the two datasets, respectively. Dataset and code are publicly available at https://github.com/Alsace08/SumCoT.},
	urldate = {2024-09-10},
	publisher = {arXiv},
	author = {Wang, Yiming and Zhang, Zhuosheng and Wang, Rui},
	month = may,
	year = {2023},
	note = {arXiv:2305.13412 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted by ACL 2023},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/SN7NRIH7/Wang et al. - 2023 - Element-aware Summarization with Large Language Mo.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/9Y4ZEAYF/2305.html:text/html},
}

@inproceedings{ranaldi_aligning_2024,
	address = {St. Julian's, Malta},
	title = {Aligning {Large} and {Small} {Language} {Models} via {Chain}-of-{Thought} {Reasoning}},
	url = {https://aclanthology.org/2024.eacl-long.109},
	abstract = {Chain-of-Thought (CoT) prompting empowersthe reasoning abilities of Large Language Models (LLMs), eliciting them to solve complexreasoning tasks in a step-wise manner. However, these capabilities appear only in models with billions of parameters, which represent an entry barrier for many users who are constrained to operate on a smaller model scale, i.e., Small Language Models (SLMs). Although many companies are releasing LLMs of the same family with fewer parameters, these models tend not to preserve all the reasoning capabilities of the original models, including CoT reasoning.In this paper, we propose a method for aligning and transferring reasoning abilities between larger to smaller Language Models. By using an Instruction-tuning-CoT method, that is, an Instruction-tuning designed around CoT-Demonstrations, we enable the SLMs to generate multi-step controlled reasoned answers when they are elicited with the CoT mechanism. Hence, we instruct a smaller Language Model using outputs generated by more robust models belonging to the same family or not, evaluating the impact across different types of models. Results obtained on question-answering and mathematical reasoning benchmarks show that LMs instructed via the Instruction-tuning CoT method produced by LLMs outperform baselines within both in-domain and out-domain scenarios.},
	urldate = {2024-09-10},
	booktitle = {Proceedings of the 18th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Ranaldi, Leonardo and Freitas, Andre},
	editor = {Graham, Yvette and Purver, Matthew},
	month = mar,
	year = {2024},
	keywords = {cot fine tune},
	pages = {1812--1827},
	file = {Full Text PDF:/Users/zhenqian/Zotero/storage/K6RZJYXW/Ranaldi and Freitas - 2024 - Aligning Large and Small Language Models via Chain.pdf:application/pdf},
}

@misc{narayan_dont_2018,
	title = {Don't {Give} {Me} the {Details}, {Just} the {Summary}! {Topic}-{Aware} {Convolutional} {Neural} {Networks} for {Extreme} {Summarization}},
	url = {http://arxiv.org/abs/1808.08745},
	doi = {10.48550/arXiv.1808.08745},
	abstract = {We introduce extreme summarization, a new single-document summarization task which does not favor extractive strategies and calls for an abstractive modeling approach. The idea is to create a short, one-sentence news summary answering the question "What is the article about?". We collect a real-world, large-scale dataset for this task by harvesting online articles from the British Broadcasting Corporation (BBC). We propose a novel abstractive model which is conditioned on the article's topics and based entirely on convolutional neural networks. We demonstrate experimentally that this architecture captures long-range dependencies in a document and recognizes pertinent content, outperforming an oracle extractive system and state-of-the-art abstractive approaches when evaluated automatically and by humans.},
	urldate = {2024-09-10},
	publisher = {arXiv},
	author = {Narayan, Shashi and Cohen, Shay B. and Lapata, Mirella},
	month = aug,
	year = {2018},
	note = {arXiv:1808.08745 [cs]},
	keywords = {dataset},
	annote = {Comment: 11, 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/NJR36CJF/Narayan et al. - 2018 - Don't Give Me the Details, Just the Summary! Topic.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/7M7VMY3N/1808.html:text/html},
}

@misc{hermann_teaching_2015,
	title = {Teaching {Machines} to {Read} and {Comprehend}},
	url = {http://arxiv.org/abs/1506.03340},
	doi = {10.48550/arXiv.1506.03340},
	abstract = {Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.},
	urldate = {2024-09-10},
	publisher = {arXiv},
	author = {Hermann, Karl Moritz and Kočiský, Tomáš and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
	month = nov,
	year = {2015},
	note = {arXiv:1506.03340 [cs]},
	keywords = {dataset},
	annote = {Comment: Appears in: Advances in Neural Information Processing Systems 28 (NIPS 2015). 14 pages, 13 figures},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/BUESBJSJ/Hermann et al. - 2015 - Teaching Machines to Read and Comprehend.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/B69NZK7V/1506.html:text/html},
}

@misc{magister_teaching_2023,
	title = {Teaching {Small} {Language} {Models} to {Reason}},
	url = {http://arxiv.org/abs/2212.08410},
	doi = {10.48550/arXiv.2212.08410},
	abstract = {Chain of thought prompting successfully improves the reasoning capabilities of large language models, achieving state of the art results on a range of datasets. However, these reasoning capabilities only appear to emerge in models with a size of over 100 billion parameters. In this paper, we explore the transfer of such reasoning capabilities to models with less than 100 billion parameters via knowledge distillation. Specifically, we finetune a student model on the chain of thought outputs generated by a larger teacher model. Our experiments show that the proposed method improves task performance across arithmetic, commonsense and symbolic reasoning datasets. For example, the accuracy of T5 XXL on GSM8K improves from 8.11\% to 21.99\% when finetuned on PaLM-540B generated chains of thought.},
	urldate = {2024-09-14},
	publisher = {arXiv},
	author = {Magister, Lucie Charlotte and Mallinson, Jonathan and Adamek, Jakub and Malmi, Eric and Severyn, Aliaksei},
	month = jun,
	year = {2023},
	note = {arXiv:2212.08410 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/7R7PL9TN/Magister et al. - 2023 - Teaching Small Language Models to Reason.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/VN4IBM97/2212.html:text/html},
}

@misc{zhang_multimodal_2024,
	title = {Multimodal {Chain}-of-{Thought} {Reasoning} in {Language} {Models}},
	url = {http://arxiv.org/abs/2302.00923},
	doi = {10.48550/arXiv.2302.00923},
	abstract = {Large language models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer. However, existing CoT studies have primarily focused on the language modality. We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference. In this way, answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With Multimodal-CoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark. Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https://github.com/amazon-science/mm-cot.},
	urldate = {2024-09-17},
	publisher = {arXiv},
	author = {Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Zhao, Hai and Karypis, George and Smola, Alex},
	month = may,
	year = {2024},
	note = {arXiv:2302.00923 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Published in Transactions on Machine Learning Research},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/AVXPIVCW/Zhang et al. - 2024 - Multimodal Chain-of-Thought Reasoning in Language .pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/9FPPTQ2N/2302.html:text/html},
}

@misc{dettmers_qlora_2023,
	title = {{QLoRA}: {Efficient} {Finetuning} of {Quantized} {LLMs}},
	shorttitle = {{QLoRA}},
	url = {http://arxiv.org/abs/2305.14314},
	doi = {10.48550/arXiv.2305.14314},
	abstract = {We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters{\textasciitilde}(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3\% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.},
	urldate = {2024-09-18},
	publisher = {arXiv},
	author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
	month = may,
	year = {2023},
	note = {arXiv:2305.14314 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: Extended NeurIPS submission},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/YIK4PD7S/Dettmers et al. - 2023 - QLoRA Efficient Finetuning of Quantized LLMs.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/KD9XV2LJ/2305.html:text/html},
}

@misc{wei_chain--thought_2023,
	title = {Chain-of-{Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2201.11903},
	doi = {10.48550/arXiv.2201.11903},
	abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
	urldate = {2024-09-22},
	publisher = {arXiv},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
	month = jan,
	year = {2023},
	note = {arXiv:2201.11903 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/4E7AUCSB/Wei et al. - 2023 - Chain-of-Thought Prompting Elicits Reasoning in La.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/IKYZKTDJ/2201.html:text/html},
}

@misc{ho_large_2023,
	title = {Large {Language} {Models} {Are} {Reasoning} {Teachers}},
	url = {http://arxiv.org/abs/2212.10071},
	doi = {10.48550/arXiv.2212.10071},
	abstract = {Recent works have shown that chain-of-thought (CoT) prompting can elicit language models to solve complex reasoning tasks, step-by-step. However, prompt-based CoT methods are dependent on very large models such as GPT-3 175B which are prohibitive to deploy at scale. In this paper, we use these large models as reasoning teachers to enable complex reasoning in smaller models and reduce model size requirements by several orders of magnitude. We propose Fine-tune-CoT, a method that generates reasoning samples from very large teacher models to fine-tune smaller models. We evaluate our method on a wide range of public models and complex tasks. We find that Fine-tune-CoT enables substantial reasoning capability in small models, far outperforming prompt-based baselines and even the teacher model in many tasks. Additionally, we extend our method by leveraging the teacher model's ability to generate multiple distinct rationales for each original sample. Enriching the fine-tuning data with such diverse reasoning results in a substantial performance boost across datasets, even for very small models. We conduct ablations and sample studies to understand the emergence of reasoning capabilities of student models. Our code implementation and data are available at https://github.com/itsnamgyu/reasoning-teacher.},
	urldate = {2024-09-23},
	publisher = {arXiv},
	author = {Ho, Namgyu and Schmid, Laura and Yun, Se-Young},
	month = jun,
	year = {2023},
	note = {arXiv:2212.10071 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: ACL 2023 camera-ready},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/NIAR7WHX/Ho et al. - 2023 - Large Language Models Are Reasoning Teachers.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/I28SQAJX/2212.html:text/html},
}

@misc{hsieh_distilling_2023,
	title = {Distilling {Step}-by-{Step}! {Outperforming} {Larger} {Language} {Models} with {Less} {Training} {Data} and {Smaller} {Model} {Sizes}},
	url = {http://arxiv.org/abs/2305.02301},
	doi = {10.48550/arXiv.2305.02301},
	abstract = {Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications. In reaction, researchers train smaller task-specific models by either finetuning with human labels or distilling using LLM-generated labels. However, finetuning and distillation require large amounts of training data to achieve comparable performance to LLMs. We introduce Distilling step-by-step, a new mechanism that (a) trains smaller models that outperform LLMs, and (b) achieves so by leveraging less training data needed by finetuning or distillation. Our method extracts LLM rationales as additional supervision for training small models within a multi-task framework. We present three findings across 4 NLP benchmarks: First, compared to both finetuning and distillation, our mechanism achieves better performance with much fewer labeled/unlabeled training examples. Second, compared to few-shot prompted LLMs, we achieve better performance using substantially smaller model sizes. Third, we reduce both the model size and the amount of data required to outperform LLMs; our finetuned 770M T5 model outperforms the few-shot prompted 540B PaLM model using only 80\% of available data on a benchmark, whereas standard finetuning the same T5 model struggles to match even by using 100\% of the dataset. We release the code at: https://github.com/google-research/distilling-step-by-step .},
	urldate = {2024-09-23},
	publisher = {arXiv},
	author = {Hsieh, Cheng-Yu and Li, Chun-Liang and Yeh, Chih-Kuan and Nakhost, Hootan and Fujii, Yasuhisa and Ratner, Alexander and Krishna, Ranjay and Lee, Chen-Yu and Pfister, Tomas},
	month = jul,
	year = {2023},
	note = {arXiv:2305.02301 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Accepted to Findings of ACL 2023},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/753YL2QQ/Hsieh et al. - 2023 - Distilling Step-by-Step! Outperforming Larger Lang.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/HZ5PHQIV/2305.html:text/html},
}

@inproceedings{rajpoot_team_2024,
	address = {Mexico City, Mexico},
	title = {Team {NP}\_PROBLEM at {SemEval}-2024 {Task} 7: {Numerical} {Reasoning} in {Headline} {Generation} with {Preference} {Optimization}},
	shorttitle = {Team {NP}\_PROBLEM at {SemEval}-2024 {Task} 7},
	url = {https://aclanthology.org/2024.semeval-1.103},
	doi = {10.18653/v1/2024.semeval-1.103},
	abstract = {While large language models (LLMs) exhibit impressive linguistic abilities, their numerical reasoning skills within real-world contexts re- main under-explored. This paper describes our participation in a headline-generation challenge by Numeval at Semeval 2024, which focused on numerical reasoning. Our system achieved an overall top numerical accuracy of 73.49\% on the task. We explore the system's design choices contributing to this result and analyze common error patterns. Our findings highlight the potential and ongoing challenges of integrat- ing numerical reasoning within large language model-based headline generation.},
	urldate = {2024-09-29},
	booktitle = {Proceedings of the 18th {International} {Workshop} on {Semantic} {Evaluation} ({SemEval}-2024)},
	publisher = {Association for Computational Linguistics},
	author = {Rajpoot, Pawan and Chukamphaeng, Nut},
	editor = {Ojha, Atul Kr. and Doğruöz, A. Seza and Tayyar Madabushi, Harish and Da San Martino, Giovanni and Rosenthal, Sara and Rosá, Aiala},
	month = jun,
	year = {2024},
	pages = {716--720},
	file = {Full Text PDF:/Users/zhenqian/Zotero/storage/DG6XKEQ3/Rajpoot and Chukamphaeng - 2024 - Team NP_PROBLEM at SemEval-2024 Task 7 Numerical .pdf:application/pdf},
}

@inproceedings{zhao_ncl_nlp_2024,
	address = {Mexico City, Mexico},
	title = {{NCL}\_NLP at {SemEval}-2024 {Task} 7: {CoT}-{NumHG}: {A} {CoT}-{Based} {SFT} {Training} {Strategy} with {Large} {Language} {Models} for {Number}-{Focused} {Headline} {Generation}},
	shorttitle = {{NCL}\_NLP at {SemEval}-2024 {Task} 7},
	url = {https://aclanthology.org/2024.semeval-1.40},
	doi = {10.18653/v1/2024.semeval-1.40},
	abstract = {Headline Generation is an essential task in Natural Language Processing (NLP), where models often exhibit limited ability to accurately interpret numerals, leading to inaccuracies in generated headlines. This paper introduces CoT-NumHG, a training strategy leveraging the Chain of Thought (CoT) paradigm for Supervised Fine-Tuning (SFT) of large language models. This approach is aimed at enhancing numeral perception, interpretability, accuracy, and the generation of structured outputs. Presented in SemEval-2024 Task 7 (task 3): Numeral-Aware Headline Generation (English), this challenge is divided into two specific subtasks. The first subtask focuses on numerical reasoning, requiring models to precisely calculate and fill in the missing numbers in news headlines, while the second subtask targets the generation of complete headlines. Utilizing the same training strategy across both subtasks, this study primarily explores the first subtask as a demonstration of our training strategy. Through this competition, our CoT-NumHG-Mistral-7B model attained an accuracy rate of 94\%, underscoring the effectiveness of our proposed strategy.},
	urldate = {2024-10-03},
	booktitle = {Proceedings of the 18th {International} {Workshop} on {Semantic} {Evaluation} ({SemEval}-2024)},
	publisher = {Association for Computational Linguistics},
	author = {Zhao, Junzhe and Wang, Yingxi and Liang, Huizhi and Rusnachenko, Nicolay},
	editor = {Ojha, Atul Kr. and Doğruöz, A. Seza and Tayyar Madabushi, Harish and Da San Martino, Giovanni and Rosenthal, Sara and Rosá, Aiala},
	month = jun,
	year = {2024},
	pages = {261--269},
	file = {Full Text PDF:/Users/zhenqian/Zotero/storage/L2GKQYZA/Zhao et al. - 2024 - NCL_NLP at SemEval-2024 Task 7 CoT-NumHG A CoT-B.pdf:application/pdf},
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{wolf2019huggingface,
  title={Huggingface's transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  journal={arXiv preprint arXiv:1910.03771},
  year={2019}
}


@inproceedings{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}

@inproceedings{zhang2019bertscore,
  title={BERTScore: Evaluating Text Generation with BERT},
  author={Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q and Artzi, Yoav},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{zhao2019moverscore,
  title={MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance},
  author={Zhao, Wei and Peyrard, Maxime and Liu, Fei and Gao, Yang and Meyer, Christian M and Eger, Steffen},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={563--578},
  year={2019}
}

@inproceedings{lewis-etal-2020-bart,
    title = "{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
    author = "Lewis, Mike  and
      Liu, Yinhan  and
      Goyal, Naman  and
      Ghazvininejad, Marjan  and
      Mohamed, Abdelrahman  and
      Levy, Omer  and
      Stoyanov, Veselin  and
      Zettlemoyer, Luke",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.703",
    doi = "10.18653/v1/2020.acl-main.703",
    pages = "7871--7880",
    abstract = "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.",
}


@misc{rush_neural_2015,
	title = {A {Neural} {Attention} {Model} for {Abstractive} {Sentence} {Summarization}},
	url = {https://arxiv.org/abs/1509.00685v2},
	abstract = {Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines.},
	language = {en},
	urldate = {2024-10-09},
	journal = {arXiv.org},
	author = {Rush, Alexander M. and Chopra, Sumit and Weston, Jason},
	month = sep,
	year = {2015},
	file = {Full Text PDF:/Users/zhenqian/Zotero/storage/2CWV64VH/Rush et al. - 2015 - A Neural Attention Model for Abstractive Sentence .pdf:application/pdf},
}

@inproceedings{tan_enhancing_2024,
	address = {New York, NY, USA},
	series = {{KDD} '24},
	title = {Enhancing {Personalized} {Headline} {Generation} via {Offline} {Goal}-conditioned {Reinforcement} {Learning} with {Large} {Language} {Models}},
	isbn = {9798400704901},
	url = {https://dl.acm.org/doi/10.1145/3637528.3671638},
	doi = {10.1145/3637528.3671638},
	abstract = {Recently, significant advancements have been made in Large Language Models (LLMs) through the implementation of various alignment techniques. These techniques enable LLMs to generate highly tailored content in response to diverse user instructions. Consequently, LLMs have the potential to serve as robust, customizable recommendation systems in the field of content recommendation. However, using LLMs with user individual information and online exploration remains a challenge, which are important perspectives in developing personalized news headline generation algorithms. In this paper, we propose a novel framework to generate personalized news headlines using LLMs with extensive online exploration. The proposed approach involves initially training an offline goal-conditioned policy using supervised learning. Subsequently, online exploration is employed to collect new data for the next training iteration. Results from simulations, experiments, and real-word scenario demonstrate that our framework achieves outstanding performance on established benchmarks and can effectively generate personalized headlines under different reward settings. By treating the LLM as a goal-conditioned agent, the model can perform online exploration by modifying the goals without frequently retraining the model. To the best of our knowledge, this work represents the first investigation into the capability of LLMs to generate customized news headlines with goal-conditioned reinforcement learning via supervised learning within LLMs.},
	urldate = {2024-10-10},
	booktitle = {Proceedings of the 30th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Tan, Xiaoyu and Cheng, Leijun and Qiu, Xihe and Shi, Shaojie and Cheng, Yuan and Chu, Wei and Xu, Yinghui and Qi, Yuan},
	month = aug,
	year = {2024},
	pages = {5762--5772},
	file = {Full Text PDF:/Users/zhenqian/Zotero/storage/NC4AB44E/Tan et al. - 2024 - Enhancing Personalized Headline Generation via Off.pdf:application/pdf},
}


@misc{liu_g-eval_2023,
	title = {G-{Eval}: {NLG} {Evaluation} using {GPT}-4 with {Better} {Human} {Alignment}},
	shorttitle = {G-{Eval}},
	url = {http://arxiv.org/abs/2303.16634},
	doi = {10.48550/arXiv.2303.16634},
	abstract = {The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-Eval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose preliminary analysis on the behavior of LLM-based evaluators, and highlight the potential issue of LLM-based evaluators having a bias towards the LLM-generated texts. The code is at https://github.com/nlpyang/geval},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Liu, Yang and Iter, Dan and Xu, Yichong and Wang, Shuohang and Xu, Ruochen and Zhu, Chenguang},
	month = may,
	year = {2023},
	note = {arXiv:2303.16634 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/zhenqian/Zotero/storage/LCXM8VH5/Liu et al. - 2023 - G-Eval NLG Evaluation using GPT-4 with Better Hum.pdf:application/pdf;Snapshot:/Users/zhenqian/Zotero/storage/Q3X97AUQ/2303.html:text/html},
}
