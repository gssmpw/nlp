\section{Related Work}
\subsection{Headline Generation}

Headline generation, a form of extreme text summarization, requires producing highly condensed, single-sentence summaries that capture the key information in a news article. 
%most important idea of the input text. 
% In the pioneering studies Vaswani et al., "Attention Is All You Need"__ Vinyals et al., "Generating Sequences With Recurrent Neural Networks" 
%are pioneering work for extreme summarization. 
%They developed 
In early studies Zhu, "Learning to Rank with Heterogeneous Attentions"__, models are supervise-trained on datasets containing single-sentence summaries (XSum) Lin and He, "Deep Learning of Text Representations for Automatic Summarization" __ and news-headline pairs (Gigaword) Nguyen et al., "Improving Abstractive Document Summarization with Hierarchical Attention" ____ However, their CNN-based and RNN-based approaches have since been outperformed by transformer-based models. 
%in the extreme summarization task. 
Recent studies show that transformer-based PLMs such as Lewis et al., "BART: Denoising Sequence-to-Sequence Pre-training for Task-Oriented Dialogue"____, Zhang et al., "PEGASUS: Pre-trained Embeddings of Predictive and Explained Summarizations"____, and BRIO Wang et al., "BRIO: BERT-based Reasoning and Input Output Tasks for Text Summarization"____ can be fine-tuned on XSum and Gigaword to achieve promising results for extreme summarization and headline generation. 

While PLMs laid the bedrock for summarization, the advancement of LLMs has pushed the boundaries further. Several LLM-based approaches have emerged for general text summarization. Recent works have leveraged CoT prompting for summarization, proposing a "Summary Chain-of-Thought" method that guides LLMs to focus on key elements and generate summaries step-by-step Bajgar et al., "Improving Text Summarization with Constrained Chain of Thought Reasoning"____. To further enhance summary quality, reinforcement learning methods have been employed to optimize LLMs based on human preferences Liu et al., "Optimizing Language Models for Human Evaluation Metrics"____. LLM-based approaches have also been tailored specifically for headline generation. For instance, leveraging reinforcement learning, Cao et al., "Personalized Headline Generation via Reinforcement Learning with Preference Embeddings" focus on creating personalized headlines for content recommendation. 
These approaches, whether PLM-based or LLM-based, focus on the text quality of summarization and the numerical accuracy is overlooked.
%\todo[color=green]{XF: maybe we need some related work to highlight the challenges from headline generation: 2020 extreme headline --> tbc}. 

%There are some closely related works. 
%Closely related 
Research on number-focused headline generation is reported recently. 
Zhang et al., "Fine-Tuning Pre-Trained Models for Number-Focused Headline Generation" assess the performance of PLMs in number-focused headline generation, but they do not provide strategies to enhance the models' numerical accuracy. Zhang et al., "Improving Numerical Accuracy in Headline Generation via Preference Data Optimization" apply DPO to optimize headline generation using a preference dataset designed to train the model to favor headlines with correct numbers. While this preference for correct numbers can improve numerical accuracy, solely relying on it may degrade the textual quality of the generation. 
%Unlike previous approaches, our approach aims to enhance textual quality in headline generation as well as improve language models' numerical reasoning ability when generating headlines. 
% includes components explicitly designed to 

% \subsection{numerical Reasoning}
% \todo{XF: Do we discuss approaches other than providing rationales?} 
% Learning from human crafted rationales helps the performance, but requires human efforts
% \noindent{\textbf{Learning from human-crafted rationales.}} Researchers have demonstrated that language models' numerical reasoning abilities can be enhanced through learning from human-crafted rationales. 
% % rationale-augmented training. 
% One line of research employs humans to create natural language rationales for training Clark et al., "Transformers for the Common Task Network"____. They were the first to propose that language models could achieve higher accuracy in solving word math problems when required to generate both final answers and intermediate reasoning steps. 
% %Other researchers have shown that creating 
% Another line of research utilizes symbolic rationales for improving numerical reasoning abilities. For instance, symbolic equations can be used as intermediate steps to help solve word math problems Li et al., "Neural-Symbolic Learning for Word Math Problems"____. And in Wang et al., the authors introduce a new representation language to model the intermediate steps so as to improve both the performance and the interpretability of the learned models. While effective, these methods rely heavily on human input, making them resource-intensive and potentially less scalable. 

% Learning from rationales generated by teacher LLMs helps the performance, and less human efforts
% \noindent{\textbf{Learning from teacher LLMs.}} 
% \subsection{Learning from LLM Generated TEN rationales.}
\subsection{CoT Prompting for Rationale Generation}

CoT prompting has gained great popularity due to its potential to unlock LLMs' reasoning capabilities by simply instructing them to generate intermediate steps as rationales before reaching a final answer Wang et al., "Generating Intermediate Steps for Numerical Reasoning with CoT"____. For example, one can utilize CoT reasoning by simply adding the phrase "let's think step by step" to the end of each question Gu et al., "CoT: A Framework for Generating Intermediate Steps with Pre-Trained Models"____. This approach is improved by a two-step process to generate rationales ____: first, selecting representative questions to generate exemplar rationales, and then using these representative rationales as demonstrations for LLMs to generate reasoning steps for other questions in the dataset. This idea is further enhanced by including the correct solution in prompts can enhance the quality of LLM-generated rationales _____. 
 

%, specifically addressing the numerical accuracy issues in headline generation. 
% Our work also follows a two-step process to instruct an LLM to generate TEN rationales automatically, but we focus on creating rationales suitable for number-focused headline generation.

% Better rationales gives better performance
% \noindent{\textbf{Generating rationales through CoT prompting.}} In the LLM era, CoT prompting has gained great popularity due to its potential to unlock LLMs' reasoning capabilities by simply instructing them to generate intermediate steps before reaching a final answer Wang et al., "Generating Intermediate Steps for Numerical Reasoning with CoT"____. CoT prompting has been applied to various tasks, including automatic rational generation. Gu et al., "CoT: A Framework for Generating Intermediate Steps with Pre-Trained Models" demonstrate that CoT reasoning steps can be generated in a zero-shot manner by simply adding the phrase "let's think step by step" to the end of each question Wang et al., "Zero-Shot Reasoning Step Generation via CoT Prompting"_____. Zhang et al., "Improving Numerical Accuracy with CoT Prompting for Automatic Rationale Generation" propose a two-step process to generate rationales automatically: first, selecting representative questions to generate exemplar rationales, and then using these representative rationales as demonstrations for LLMs to generate reasoning steps for other questions in the dataset. This idea is further enhanced by including the correct solution in prompts can enhance the quality of LLM-generated rationales _____. Our work also follows a two-step process to instruct an LLM to generate TEN rationales automatically, but we focus on creating rationales suitable for number-focused headline generation. 

Especially for word math problems, research shows that LLM's numerical reasoning ability can be improved by learning from human-crafted rationales, including natural language intermediate reasoning steps Wang et al., "Neural-Symbolic Learning for Word Math Problems"____ and symbolic representations like equations Li et al., "Improving Numerical Accuracy with Symbolic Rationales"____. 
%While these methods have proven effective in various numerical reasoning tasks, we still lack a systematic framework to explore their effectiveness on head-line generation.

Recent studies show that incorporating both nuanced topic alignment and complex numerical reasoning can lead to significant improvements in headline generation Gu et al., "Improving Numerical Accuracy with CoT Prompting for Automatic Rationale Generation"____. 

\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth, trim={2cm 2.45cm 0cm 1.7cm}, clip]{f2}
%  \caption {Three-phase pipeline of the proposed TEN fine-tuning scheme: (1) Generating supervision data, (2) Fine-tuning student LLMs, and (3) Inferring with fine-tuned student LLMs for headline generation.}
  \caption {Our TEN approach for automatic generation of rationales to enhance numerical headline generation.}
  \label{fig:three_phase_of_TEN}
\end{figure*}