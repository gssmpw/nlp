@inproceedings{amini_mathqa_2019,
	address = {Minneapolis, Minnesota},
	title = {{MathQA}: {Towards} {Interpretable} {Math} {Word} {Problem} {Solving} with {Operation}-{Based} {Formalisms}},
	shorttitle = {{MathQA}},
	url = {https://aclanthology.org/N19-1245},
	doi = {10.18653/v1/N19-1245},
	abstract = {We introduce a large-scale dataset of math word problems and an interpretable neural math problem solver by learning to map problems to their operation programs. Due to annotation challenges, current datasets in this domain have been either relatively small in scale or did not offer precise operational annotations over diverse problem types. We introduce a new representation language to model operation programs corresponding to each math problem that aim to improve both the performance and the interpretability of the learned models. Using this representation language, we significantly enhance the AQUA-RAT dataset with fully-specified operational programs. We additionally introduce a neural sequence-to-program model with automatic problem categorization. Our experiments show improvements over competitive baselines in our dataset as well as the AQUA-RAT dataset. The results are still lower than human performance indicating that the dataset poses new challenges for future research. Our dataset is available at https://math-qa.github.io/math-QA/},
	urldate = {2024-03-23},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Amini, Aida and Gabriel, Saadia and Lin, Shanchuan and Koncel-Kedziorski, Rik and Choi, Yejin and Hajishirzi, Hannaneh},
	editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
	month = jun,
	year = {2019},
	keywords = {rationale-augmented training},
	pages = {2357--2367},
	file = {Full Text PDF:/Users/zhenqian/Zotero/storage/R95X9R9P/Amini et al. - 2019 - MathQA Towards Interpretable Math Word Problem So.pdf:application/pdf},
}

@inproceedings{chiang_semantically-aligned_2019,
	address = {Minneapolis, Minnesota},
	title = {Semantically-{Aligned} {Equation} {Generation} for {Solving} and {Reasoning} {Math} {Word} {Problems}},
	url = {https://aclanthology.org/N19-1272},
	doi = {10.18653/v1/N19-1272},
	abstract = {Solving math word problems is a challenging task that requires accurate natural language understanding to bridge natural language texts and math expressions. Motivated by the intuition about how human generates the equations given the problem texts, this paper presents a neural approach to automatically solve math word problems by operating symbols according to their semantic meanings in texts. This paper views the process of generating equation as a bridge between the semantic world and the symbolic world, where the proposed neural math solver is based on an encoder-decoder framework. In the proposed model, the encoder is designed to understand the semantics of problems, and the decoder focuses on tracking semantic meanings of the generated symbols and then deciding which symbol to generate next. The preliminary experiments are conducted in a dataset Math23K, and our model significantly outperforms both the state-of-the-art single model and the best non-retrieval-based model over about 10\% accuracy, demonstrating the effectiveness of bridging the symbolic and semantic worlds from math word problems.},
	urldate = {2024-03-23},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Chiang, Ting-Rui and Chen, Yun-Nung},
	editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
	month = jun,
	year = {2019},
	keywords = {rationale-augmented training},
	pages = {2656--2668},
	file = {Full Text PDF:/Users/zhenqian/Zotero/storage/VTAE6TS6/Chiang and Chen - 2019 - Semantically-Aligned Equation Generation for Solvi.pdf:application/pdf},
}

@misc{cobbe_training_2021,
	title = {Training {Verifiers} to {Solve} {Math} {Word} {Problems}},
	url = {http://arxiv.org/abs/2110.14168},
	doi = {10.48550/arXiv.2110.14168},
	abstract = {State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.},
	urldate = {2024-03-23},
	publisher = {arXiv},
	author = {Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
	month = nov,
	year = {2021},
	note = {arXiv:2110.14168 [cs]},
	keywords = {rationale-augmented training, verifier},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/4GBWAQYF/Cobbe et al. - 2021 - Training Verifiers to Solve Math Word Problems.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/4L37YDMN/2110.html:text/html},
}

@misc{ho_large_2023,
	title = {Large {Language} {Models} {Are} {Reasoning} {Teachers}},
	url = {http://arxiv.org/abs/2212.10071},
	doi = {10.48550/arXiv.2212.10071},
	abstract = {Recent works have shown that chain-of-thought (CoT) prompting can elicit language models to solve complex reasoning tasks, step-by-step. However, prompt-based CoT methods are dependent on very large models such as GPT-3 175B which are prohibitive to deploy at scale. In this paper, we use these large models as reasoning teachers to enable complex reasoning in smaller models and reduce model size requirements by several orders of magnitude. We propose Fine-tune-CoT, a method that generates reasoning samples from very large teacher models to fine-tune smaller models. We evaluate our method on a wide range of public models and complex tasks. We find that Fine-tune-CoT enables substantial reasoning capability in small models, far outperforming prompt-based baselines and even the teacher model in many tasks. Additionally, we extend our method by leveraging the teacher model's ability to generate multiple distinct rationales for each original sample. Enriching the fine-tuning data with such diverse reasoning results in a substantial performance boost across datasets, even for very small models. We conduct ablations and sample studies to understand the emergence of reasoning capabilities of student models. Our code implementation and data are available at https://github.com/itsnamgyu/reasoning-teacher.},
	urldate = {2024-09-23},
	publisher = {arXiv},
	author = {Ho, Namgyu and Schmid, Laura and Yun, Se-Young},
	month = jun,
	year = {2023},
	note = {arXiv:2212.10071 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: ACL 2023 camera-ready},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/NIAR7WHX/Ho et al. - 2023 - Large Language Models Are Reasoning Teachers.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/I28SQAJX/2212.html:text/html},
}

@misc{hsieh_distilling_2023,
	title = {Distilling {Step}-by-{Step}! {Outperforming} {Larger} {Language} {Models} with {Less} {Training} {Data} and {Smaller} {Model} {Sizes}},
	url = {http://arxiv.org/abs/2305.02301},
	doi = {10.48550/arXiv.2305.02301},
	abstract = {Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications. In reaction, researchers train smaller task-specific models by either finetuning with human labels or distilling using LLM-generated labels. However, finetuning and distillation require large amounts of training data to achieve comparable performance to LLMs. We introduce Distilling step-by-step, a new mechanism that (a) trains smaller models that outperform LLMs, and (b) achieves so by leveraging less training data needed by finetuning or distillation. Our method extracts LLM rationales as additional supervision for training small models within a multi-task framework. We present three findings across 4 NLP benchmarks: First, compared to both finetuning and distillation, our mechanism achieves better performance with much fewer labeled/unlabeled training examples. Second, compared to few-shot prompted LLMs, we achieve better performance using substantially smaller model sizes. Third, we reduce both the model size and the amount of data required to outperform LLMs; our finetuned 770M T5 model outperforms the few-shot prompted 540B PaLM model using only 80\% of available data on a benchmark, whereas standard finetuning the same T5 model struggles to match even by using 100\% of the dataset. We release the code at: https://github.com/google-research/distilling-step-by-step .},
	urldate = {2024-09-23},
	publisher = {arXiv},
	author = {Hsieh, Cheng-Yu and Li, Chun-Liang and Yeh, Chih-Kuan and Nakhost, Hootan and Fujii, Yasuhisa and Ratner, Alexander and Krishna, Ranjay and Lee, Chen-Yu and Pfister, Tomas},
	month = jul,
	year = {2023},
	note = {arXiv:2305.02301 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Accepted to Findings of ACL 2023},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/753YL2QQ/Hsieh et al. - 2023 - Distilling Step-by-Step! Outperforming Larger Lang.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/HZ5PHQIV/2305.html:text/html},
}

@inproceedings{huang-etal-2024-numhg,
    title = "{N}um{HG}: A Dataset for Number-Focused Headline Generation",
    author = "Huang, Jian-Tao  and
      Chen, Chung-Chi  and
      Huang, Hen-Hsen  and
      Chen, Hsin-Hsi",
    editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.1078",
    pages = "12323--12329",
    abstract = "Headline generation, a key task in abstractive summarization, strives to condense a full-length article into a succinct, single line of text. Notably, while contemporary encoder-decoder models excel based on the ROUGE metric, they often falter when it comes to the precise generation of numerals in headlines. We identify the lack of datasets providing fine-grained annotations for accurate numeral generation as a major roadblock. To address this, we introduce a new dataset, the NumHG, and provide over 27,000 annotated numeral-rich news articles for detailed investigation. Further, we evaluate five well-performing models from previous headline-generation tasks using human evaluation in terms of numerical accuracy, reasonableness, and readability. Our study reveals a need for improvement in numerical accuracy, demonstrating the potential of the NumHG dataset to drive progress in number-focused headline generation and stimulate further discussions in numeral-focused text generation.",
}

@misc{kojima_large_2023,
	title = {Large {Language} {Models} are {Zero}-{Shot} {Reasoners}},
	url = {http://arxiv.org/abs/2205.11916},
	doi = {10.48550/arXiv.2205.11916},
	abstract = {Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding "Let's think step by step" before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7\% to 78.7\% and GSM8K from 10.4\% to 40.7\% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.},
	urldate = {2024-04-29},
	publisher = {arXiv},
	author = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
	month = jan,
	year = {2023},
	note = {arXiv:2205.11916 [cs]},
	keywords = {rationale generation},
	annote = {Comment: Accepted to NeurIPS2022. Our code is available at https://github.com/kojima-takeshi188/zero\_shot\_cot},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/FEZTRC9W/Kojima et al. - 2023 - Large Language Models are Zero-Shot Reasoners.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/SGN5NTX9/2205.html:text/html},
}

@misc{lewis_bart_2019,
	title = {{BART}: {Denoising} {Sequence}-to-{Sequence} {Pre}-training for {Natural} {Language} {Generation}, {Translation}, and {Comprehension}},
	shorttitle = {{BART}},
	url = {http://arxiv.org/abs/1910.13461},
	doi = {10.48550/arXiv.1910.13461},
	abstract = {We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance.},
	urldate = {2024-04-16},
	publisher = {arXiv},
	author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
	month = oct,
	year = {2019},
	note = {arXiv:1910.13461 [cs, stat]},
	keywords = {llm},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/EQ8GW77R/Lewis et al. - 2019 - BART Denoising Sequence-to-Sequence Pre-training .pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/3SVGBLL9/1910.html:text/html},
}

@inproceedings{ling_program_2017,
	address = {Vancouver, Canada},
	title = {Program {Induction} by {Rationale} {Generation}: {Learning} to {Solve} and {Explain} {Algebraic} {Word} {Problems}},
	shorttitle = {Program {Induction} by {Rationale} {Generation}},
	url = {https://aclanthology.org/P17-1015},
	doi = {10.18653/v1/P17-1015},
	abstract = {Solving algebraic word problems requires executing a series of arithmetic operations—a program—to obtain a final answer. However, since programs can be arbitrarily complicated, inducing them directly from question-answer pairs is a formidable challenge. To make this task more feasible, we solve these problems by generating answer rationales, sequences of natural language and human-readable mathematical expressions that derive the final answer through a series of small steps. Although rationales do not explicitly specify programs, they provide a scaffolding for their structure via intermediate milestones. To evaluate our approach, we have created a new 100,000-sample dataset of questions, answers and rationales. Experimental results show that indirect supervision of program learning via answer rationales is a promising strategy for inducing arithmetic programs.},
	urldate = {2024-03-24},
	booktitle = {Proceedings of the 55th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Ling, Wang and Yogatama, Dani and Dyer, Chris and Blunsom, Phil},
	editor = {Barzilay, Regina and Kan, Min-Yen},
	month = jul,
	year = {2017},
	keywords = {rationale-augmented training},
	pages = {158--167},
	file = {Full Text PDF:/Users/zhenqian/Zotero/storage/FKXGEHEV/Ling et al. - 2017 - Program Induction by Rationale Generation Learnin.pdf:application/pdf},
}

@misc{liu_brio_2022,
	title = {{BRIO}: {Bringing} {Order} to {Abstractive} {Summarization}},
	shorttitle = {{BRIO}},
	url = {http://arxiv.org/abs/2203.16804},
	doi = {10.48550/arXiv.2203.16804},
	abstract = {Abstractive summarization models are commonly trained using maximum likelihood estimation, which assumes a deterministic (one-point) target distribution in which an ideal model will assign all the probability mass to the reference summary. This assumption may lead to performance degradation during inference, where the model needs to compare several system-generated (candidate) summaries that have deviated from the reference summary. To address this problem, we propose a novel training paradigm which assumes a non-deterministic distribution so that different candidate summaries are assigned probability mass according to their quality. Our method achieves a new state-of-the-art result on the CNN/DailyMail (47.78 ROUGE-1) and XSum (49.07 ROUGE-1) datasets. Further analysis also shows that our model can estimate probabilities of candidate summaries that are more correlated with their level of quality.},
	urldate = {2024-04-22},
	publisher = {arXiv},
	author = {Liu, Yixin and Liu, Pengfei and Radev, Dragomir and Neubig, Graham},
	month = mar,
	year = {2022},
	note = {arXiv:2203.16804 [cs]},
	keywords = {summarization},
	annote = {Comment: ACL 2022},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/YZCG74XC/Liu et al. - 2022 - BRIO Bringing Order to Abstractive Summarization.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/FEWIBH26/2203.html:text/html},
}

@misc{magister_teaching_2023,
	title = {Teaching {Small} {Language} {Models} to {Reason}},
	url = {http://arxiv.org/abs/2212.08410},
	doi = {10.48550/arXiv.2212.08410},
	abstract = {Chain of thought prompting successfully improves the reasoning capabilities of large language models, achieving state of the art results on a range of datasets. However, these reasoning capabilities only appear to emerge in models with a size of over 100 billion parameters. In this paper, we explore the transfer of such reasoning capabilities to models with less than 100 billion parameters via knowledge distillation. Specifically, we finetune a student model on the chain of thought outputs generated by a larger teacher model. Our experiments show that the proposed method improves task performance across arithmetic, commonsense and symbolic reasoning datasets. For example, the accuracy of T5 XXL on GSM8K improves from 8.11\% to 21.99\% when finetuned on PaLM-540B generated chains of thought.},
	urldate = {2024-09-14},
	publisher = {arXiv},
	author = {Magister, Lucie Charlotte and Mallinson, Jonathan and Adamek, Jakub and Malmi, Eric and Severyn, Aliaksei},
	month = jun,
	year = {2023},
	note = {arXiv:2212.08410 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/7R7PL9TN/Magister et al. - 2023 - Teaching Small Language Models to Reason.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/VN4IBM97/2212.html:text/html},
}

@misc{narayan_dont_2018,
	title = {Don't {Give} {Me} the {Details}, {Just} the {Summary}! {Topic}-{Aware} {Convolutional} {Neural} {Networks} for {Extreme} {Summarization}},
	url = {http://arxiv.org/abs/1808.08745},
	doi = {10.48550/arXiv.1808.08745},
	abstract = {We introduce extreme summarization, a new single-document summarization task which does not favor extractive strategies and calls for an abstractive modeling approach. The idea is to create a short, one-sentence news summary answering the question "What is the article about?". We collect a real-world, large-scale dataset for this task by harvesting online articles from the British Broadcasting Corporation (BBC). We propose a novel abstractive model which is conditioned on the article's topics and based entirely on convolutional neural networks. We demonstrate experimentally that this architecture captures long-range dependencies in a document and recognizes pertinent content, outperforming an oracle extractive system and state-of-the-art abstractive approaches when evaluated automatically and by humans.},
	urldate = {2024-09-10},
	publisher = {arXiv},
	author = {Narayan, Shashi and Cohen, Shay B. and Lapata, Mirella},
	month = aug,
	year = {2018},
	note = {arXiv:1808.08745 [cs]},
	keywords = {dataset},
	annote = {Comment: 11, 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/NJR36CJF/Narayan et al. - 2018 - Don't Give Me the Details, Just the Summary! Topic.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/7M7VMY3N/1808.html:text/html},
}

@inproceedings{rajpoot_team_2024,
	address = {Mexico City, Mexico},
	title = {Team {NP}\_PROBLEM at {SemEval}-2024 {Task} 7: {Numerical} {Reasoning} in {Headline} {Generation} with {Preference} {Optimization}},
	shorttitle = {Team {NP}\_PROBLEM at {SemEval}-2024 {Task} 7},
	url = {https://aclanthology.org/2024.semeval-1.103},
	doi = {10.18653/v1/2024.semeval-1.103},
	abstract = {While large language models (LLMs) exhibit impressive linguistic abilities, their numerical reasoning skills within real-world contexts re- main under-explored. This paper describes our participation in a headline-generation challenge by Numeval at Semeval 2024, which focused on numerical reasoning. Our system achieved an overall top numerical accuracy of 73.49\% on the task. We explore the system's design choices contributing to this result and analyze common error patterns. Our findings highlight the potential and ongoing challenges of integrat- ing numerical reasoning within large language model-based headline generation.},
	urldate = {2024-09-29},
	booktitle = {Proceedings of the 18th {International} {Workshop} on {Semantic} {Evaluation} ({SemEval}-2024)},
	publisher = {Association for Computational Linguistics},
	author = {Rajpoot, Pawan and Chukamphaeng, Nut},
	editor = {Ojha, Atul Kr. and Doğruöz, A. Seza and Tayyar Madabushi, Harish and Da San Martino, Giovanni and Rosenthal, Sara and Rosá, Aiala},
	month = jun,
	year = {2024},
	pages = {716--720},
	file = {Full Text PDF:/Users/zhenqian/Zotero/storage/DG6XKEQ3/Rajpoot and Chukamphaeng - 2024 - Team NP_PROBLEM at SemEval-2024 Task 7 Numerical .pdf:application/pdf},
}

@misc{rush_neural_2015,
	title = {A {Neural} {Attention} {Model} for {Abstractive} {Sentence} {Summarization}},
	url = {https://arxiv.org/abs/1509.00685v2},
	abstract = {Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines.},
	language = {en},
	urldate = {2024-10-09},
	journal = {arXiv.org},
	author = {Rush, Alexander M. and Chopra, Sumit and Weston, Jason},
	month = sep,
	year = {2015},
	file = {Full Text PDF:/Users/zhenqian/Zotero/storage/2CWV64VH/Rush et al. - 2015 - A Neural Attention Model for Abstractive Sentence .pdf:application/pdf},
}

@misc{stiennon_learning_2022,
	title = {Learning to summarize from human feedback},
	url = {http://arxiv.org/abs/2009.01325},
	doi = {10.48550/arXiv.2009.01325},
	abstract = {As language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task. For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about -- summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR dataset of Reddit posts and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to CNN/DM news articles, producing summaries nearly as good as the human reference without any news-specific fine-tuning. We conduct extensive analyses to understand our human feedback dataset and fine-tuned models We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans. We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want.},
	urldate = {2024-07-16},
	publisher = {arXiv},
	author = {Stiennon, Nisan and Ouyang, Long and Wu, Jeff and Ziegler, Daniel M. and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul},
	month = feb,
	year = {2022},
	note = {arXiv:2009.01325 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: NeurIPS 2020},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/Y67PU9NA/Stiennon et al. - 2022 - Learning to summarize from human feedback.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/25EQ9HRK/2009.html:text/html},
}

@inproceedings{tan_enhancing_2024,
	address = {New York, NY, USA},
	series = {{KDD} '24},
	title = {Enhancing {Personalized} {Headline} {Generation} via {Offline} {Goal}-conditioned {Reinforcement} {Learning} with {Large} {Language} {Models}},
	isbn = {9798400704901},
	url = {https://dl.acm.org/doi/10.1145/3637528.3671638},
	doi = {10.1145/3637528.3671638},
	abstract = {Recently, significant advancements have been made in Large Language Models (LLMs) through the implementation of various alignment techniques. These techniques enable LLMs to generate highly tailored content in response to diverse user instructions. Consequently, LLMs have the potential to serve as robust, customizable recommendation systems in the field of content recommendation. However, using LLMs with user individual information and online exploration remains a challenge, which are important perspectives in developing personalized news headline generation algorithms. In this paper, we propose a novel framework to generate personalized news headlines using LLMs with extensive online exploration. The proposed approach involves initially training an offline goal-conditioned policy using supervised learning. Subsequently, online exploration is employed to collect new data for the next training iteration. Results from simulations, experiments, and real-word scenario demonstrate that our framework achieves outstanding performance on established benchmarks and can effectively generate personalized headlines under different reward settings. By treating the LLM as a goal-conditioned agent, the model can perform online exploration by modifying the goals without frequently retraining the model. To the best of our knowledge, this work represents the first investigation into the capability of LLMs to generate customized news headlines with goal-conditioned reinforcement learning via supervised learning within LLMs.},
	urldate = {2024-10-10},
	booktitle = {Proceedings of the 30th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Tan, Xiaoyu and Cheng, Leijun and Qiu, Xihe and Shi, Shaojie and Cheng, Yuan and Chu, Wei and Xu, Yinghui and Qi, Yuan},
	month = aug,
	year = {2024},
	pages = {5762--5772},
	file = {Full Text PDF:/Users/zhenqian/Zotero/storage/NC4AB44E/Tan et al. - 2024 - Enhancing Personalized Headline Generation via Off.pdf:application/pdf},
}

@misc{wang_element-aware_2023,
	title = {Element-aware {Summarization} with {Large} {Language} {Models}: {Expert}-aligned {Evaluation} and {Chain}-of-{Thought} {Method}},
	shorttitle = {Element-aware {Summarization} with {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2305.13412},
	doi = {10.48550/arXiv.2305.13412},
	abstract = {Automatic summarization generates concise summaries that contain key ideas of source documents. As the most mainstream datasets for the news sub-domain, CNN/DailyMail and BBC XSum have been widely used for performance benchmarking. However, the reference summaries of those datasets turn out to be noisy, mainly in terms of factual hallucination and information redundancy. To address this challenge, we first annotate new expert-writing Element-aware test sets following the "Lasswell Communication Model" proposed by Lasswell (1948), allowing reference summaries to focus on more fine-grained news elements objectively and comprehensively. Utilizing the new test sets, we observe the surprising zero-shot summary ability of LLMs, which addresses the issue of the inconsistent results between human preference and automatic evaluation metrics of LLMs' zero-shot summaries in prior work. Further, we propose a Summary Chain-of-Thought (SumCoT) technique to elicit LLMs to generate summaries step by step, which helps them integrate more fine-grained details of source documents into the final summaries that correlate with the human writing mindset. Experimental results show our method outperforms state-of-the-art fine-tuned PLMs and zero-shot LLMs by +4.33/+4.77 in ROUGE-L on the two datasets, respectively. Dataset and code are publicly available at https://github.com/Alsace08/SumCoT.},
	urldate = {2024-09-10},
	publisher = {arXiv},
	author = {Wang, Yiming and Zhang, Zhuosheng and Wang, Rui},
	month = may,
	year = {2023},
	note = {arXiv:2305.13412 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted by ACL 2023},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/SN7NRIH7/Wang et al. - 2023 - Element-aware Summarization with Large Language Mo.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/9Y4ZEAYF/2305.html:text/html},
}

@misc{wang_math-shepherd_2024,
	title = {Math-{Shepherd}: {Verify} and {Reinforce} {LLMs} {Step}-by-step without {Human} {Annotations}},
	shorttitle = {Math-{Shepherd}},
	url = {http://arxiv.org/abs/2312.08935},
	doi = {10.48550/arXiv.2312.08935},
	abstract = {In this paper, we present an innovative process-oriented math process reward model called {\textbackslash}textbf\{Math-Shepherd\}, which assigns a reward score to each step of math problem solutions. The training of Math-Shepherd is achieved using automatically constructed process-wise supervision data, breaking the bottleneck of heavy reliance on manual annotation in existing work. We explore the effectiveness of Math-Shepherd in two scenarios: 1) {\textbackslash}textit\{Verification\}: Math-Shepherd is utilized for reranking multiple outputs generated by Large Language Models (LLMs); 2) {\textbackslash}textit\{Reinforcement Learning\}: Math-Shepherd is employed to reinforce LLMs with step-by-step Proximal Policy Optimization (PPO). With Math-Shepherd, a series of open-source LLMs demonstrates exceptional performance. For instance, the step-by-step PPO with Math-Shepherd significantly improves the accuracy of Mistral-7B (77.9{\textbackslash}\%\${\textbackslash}to\$84.1{\textbackslash}\% on GSM8K and 28.6{\textbackslash}\%\${\textbackslash}to\$33.0{\textbackslash}\% on MATH). The accuracy can be further enhanced to 89.1{\textbackslash}\% and 43.5{\textbackslash}\% on GSM8K and MATH with the verification of Math-Shepherd, respectively. We believe that automatic process supervision holds significant potential for the future evolution of LLMs.},
	urldate = {2024-06-27},
	publisher = {arXiv},
	author = {Wang, Peiyi and Li, Lei and Shao, Zhihong and Xu, R. X. and Dai, Damai and Li, Yifei and Chen, Deli and Wu, Y. and Sui, Zhifang},
	month = feb,
	year = {2024},
	note = {arXiv:2312.08935 [cs]},
	keywords = {rl},
	annote = {Comment: Add Step-by-Step reinforcement learning results},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/HIAJKIFQ/Wang et al. - 2024 - Math-Shepherd Verify and Reinforce LLMs Step-by-s.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/ZID4QPAM/2312.html:text/html},
}

@misc{wang_self-consistency_2023,
	title = {Self-{Consistency} {Improves} {Chain} of {Thought} {Reasoning} in {Language} {Models}},
	url = {http://arxiv.org/abs/2203.11171},
	doi = {10.48550/arXiv.2203.11171},
	abstract = {Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9\%), SVAMP (+11.0\%), AQuA (+12.2\%), StrategyQA (+6.4\%) and ARC-challenge (+3.9\%).},
	urldate = {2024-04-22},
	publisher = {arXiv},
	author = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
	month = mar,
	year = {2023},
	note = {arXiv:2203.11171 [cs]},
	keywords = {inference},
	annote = {Comment: Published at ICLR 2023. V2: added PaLM results; V3: added UL2 results; V4: camera ready version at ICLR 2023},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/WQSQ4HFY/Wang et al. - 2023 - Self-Consistency Improves Chain of Thought Reasoni.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/DX6B3NI2/2203.html:text/html},
}

@misc{wang_t-sciq_2023,
	title = {T-{SciQ}: {Teaching} {Multimodal} {Chain}-of-{Thought} {Reasoning} via {Mixed} {Large} {Language} {Model} {Signals} for {Science} {Question} {Answering}},
	shorttitle = {T-{SciQ}},
	url = {http://arxiv.org/abs/2305.03453},
	doi = {10.48550/arXiv.2305.03453},
	abstract = {Large Language Models (LLMs) have recently demonstrated exceptional performance in various Natural Language Processing (NLP) tasks. They have also shown the ability to perform chain-of-thought (CoT) reasoning to solve complex problems. Recent studies have explored CoT reasoning in complex multimodal scenarios, such as the science question answering task, by fine-tuning multimodal models with high-quality human-annotated CoT rationales. However, collecting high-quality COT rationales is usually time-consuming and costly. Besides, the annotated rationales are hardly accurate due to the external essential information missed. To address these issues, we propose a novel method termed T-SciQ that aims at teaching science question answering with LLM signals. The T-SciQ approach generates high-quality CoT rationales as teaching signals and is advanced to train much smaller models to perform CoT reasoning in complex modalities. Additionally, we introduce a novel data mixing strategy to produce more effective teaching data samples for simple and complex science question answer problems. Extensive experimental results show that our T-SciQ method achieves a new state-of-the-art performance on the ScienceQA benchmark, with an accuracy of 96.18\%. Moreover, our approach outperforms the most powerful fine-tuned baseline by 4.5\%. The code is publicly available at https://github.com/T-SciQ/T-SciQ.},
	urldate = {2024-07-16},
	publisher = {arXiv},
	author = {Wang, Lei and Hu, Yi and He, Jiabang and Xu, Xing and Liu, Ning and Liu, Hui and Shen, Heng Tao},
	month = dec,
	year = {2023},
	note = {arXiv:2305.03453 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: AAAI 2024},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/VA8WPFT3/Wang et al. - 2023 - T-SciQ Teaching Multimodal Chain-of-Thought Reaso.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/FRBQ3NA2/2305.html:text/html},
}

@misc{wei_chain--thought_2023,
	title = {Chain-of-{Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2201.11903},
	doi = {10.48550/arXiv.2201.11903},
	abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
	urldate = {2024-09-22},
	publisher = {arXiv},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
	month = jan,
	year = {2023},
	note = {arXiv:2201.11903 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/4E7AUCSB/Wei et al. - 2023 - Chain-of-Thought Prompting Elicits Reasoning in La.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/IKYZKTDJ/2201.html:text/html},
}

@misc{zhang_automatic_2022,
	title = {Automatic {Chain} of {Thought} {Prompting} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2210.03493},
	doi = {10.48550/arXiv.2210.03493},
	abstract = {Large language models (LLMs) can perform complex reasoning by generating intermediate reasoning steps. Providing these steps for prompting demonstrations is called chain-of-thought (CoT) prompting. CoT prompting has two major paradigms. One leverages a simple prompt like "Let's think step by step" to facilitate step-by-step thinking before answering a question. The other uses a few manual demonstrations one by one, each composed of a question and a reasoning chain that leads to an answer. The superior performance of the second paradigm hinges on the hand-crafting of task-specific demonstrations one by one. We show that such manual efforts may be eliminated by leveraging LLMs with the "Let's think step by step" prompt to generate reasoning chains for demonstrations one by one, i.e., let's think not just step by step, but also one by one. However, these generated chains often come with mistakes. To mitigate the effect of such mistakes, we find that diversity matters for automatically constructing demonstrations. We propose an automatic CoT prompting method: Auto-CoT. It samples questions with diversity and generates reasoning chains to construct demonstrations. On ten public benchmark reasoning tasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. Code is available at https://github.com/amazon-research/auto-cot},
	urldate = {2024-04-29},
	publisher = {arXiv},
	author = {Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Smola, Alex},
	month = oct,
	year = {2022},
	note = {arXiv:2210.03493 [cs]},
	keywords = {rationale generation},
	file = {arXiv Fulltext PDF:/Users/zhenqian/Zotero/storage/QMNH6R5E/Zhang et al. - 2022 - Automatic Chain of Thought Prompting in Large Lang.pdf:application/pdf;arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/K3ENA9FF/2210.html:text/html},
}

@misc{zhang_pegasus_2020,
	title = {{PEGASUS}: {Pre}-training with {Extracted} {Gap}-sentences for {Abstractive} {Summarization}},
	shorttitle = {{PEGASUS}},
	url = {http://arxiv.org/abs/1912.08777},
	abstract = {Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-training large Transformer-based encoder-decoder models on massive text corpora with a new self-supervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets.},
	urldate = {2024-04-19},
	publisher = {arXiv},
	author = {Zhang, Jingqing and Zhao, Yao and Saleh, Mohammad and Liu, Peter J.},
	month = jul,
	year = {2020},
	note = {arXiv:1912.08777 [cs]},
	keywords = {summarization, llm},
	annote = {Comment: Added results from mixed+stochastic model, test-set overlapping analysis; Code link added; Accepted for ICML 2020. arXiv admin note: text overlap with arXiv:1605.06560, arXiv:1205.2395, arXiv:0902.4351, arXiv:1610.09932, arXiv:nucl-ex/0512029 by other authors},
	file = {arXiv.org Snapshot:/Users/zhenqian/Zotero/storage/WQ9CS5FU/1912.html:text/html;Full Text PDF:/Users/zhenqian/Zotero/storage/AUCKNLZ5/Zhang et al. - 2020 - PEGASUS Pre-training with Extracted Gap-sentences.pdf:application/pdf},
}

