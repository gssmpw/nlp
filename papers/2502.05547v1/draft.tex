\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
    \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
% \usepackage[preprint]{neurips_2024}
\usepackage[final]{neurips_2024}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage[pdftex]{graphicx}
% \usepackage{fourier}
\counterwithout{figure}{section}
\usepackage[ruled,vlined,linesnumbered,noend]{algorithm2e}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{hyperref}


% \title{HyperGuard: A Hybrid Approach for Privacy-preserving Robust Federated Learning Against Model Poisoning Attacks}
\title{Dual Defense: Enhancing Privacy and Mitigating Poisoning Attacks in Federated Learning}
% \title{Twin Shields: Bolstering Privacy and Countering Poisoning in Federated Learning}





% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Runhua~Xu\\
  %\thanks{Use footnote for providing further information
    % about author (webpage, alternative address)---\emph{not} for acknowledging
    % funding agencies.} \\
  % Department of Computer Science\\
  Beihang University\\
  % Pittsburgh, PA 15213 \\
  \texttt{runhua@buaa.edu.cn} \\
  % examples of more authors
  \And
  Shiqi Gao \\
  Beihang University\\
  \texttt{gaoshiqi@buaa.edu.cn} \\
  \And
  Chao Li \\
  Beijing Jiaotong University\\
  \texttt{li.chao@bjtu.edu.cn} \\
  \AND
  James Joshi \\
  University of Pittsburgh\\
  \texttt{jjoshi@pitt.edu} \\
  \And
  Jianxin Li \\
  Beihang University and Zhongguancun Laboratory\\
  \texttt{lijx@buaa.edu.cn} \\
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \thanks{corresponding author}
}


\begin{document}


\maketitle


\begin{abstract}
Federated learning (FL) is inherently susceptible to privacy breaches and poisoning attacks. To tackle these challenges, researchers have separately devised secure aggregation mechanisms to protect data privacy and robust aggregation methods that withstand poisoning attacks. However, simultaneously addressing both concerns is challenging; secure aggregation facilitates poisoning attacks as most anomaly detection techniques require access to unencrypted local model updates, which are obscured by secure aggregation.
Few recent efforts to simultaneously tackle both challenges offen depend on impractical assumption of non-colluding two-server setups that disrupt FL's topology, or three-party computation which introduces scalability issues, complicating deployment and application.
To overcome this dilemma, this paper introduce a \textbf{D}ual \textbf{D}efense \textbf{Fed}erated learning (\textit{DDFed}) framework.
\textit{DDFed} simultaneously boosts privacy protection and mitigates poisoning attacks, without introducing new participant roles or disrupting the existing FL topology.
\textit{DDFed} initially leverages cutting-edge fully homomorphic encryption (FHE) to securely aggregate model updates, without the impractical requirement for non-colluding two-server setups and ensures strong privacy protection. 
Additionally, we proposes a unique two-phase anomaly detection mechanism for encrypted model updates, featuring secure similarity computation and feedback-driven collaborative selection, with additional measures to prevent potential privacy breaches from Byzantine clients incorporated into the detection process.
We conducted extensive experiments on various model poisoning attacks and FL scenarios, including both cross-device and cross-silo FL. 
Experiments on publicly available datasets demonstrate that \textit{DDFed} successfully protects model privacy and effectively defends against model poisoning threats.

\end{abstract}







\section{Introduction}

% FL Overview - facing issues - two categories
Federated learning (FL)\cite{konecny2016federated} is gaining popularity as a collaborative model training paradigm that provides primary privacy protection by eliminating the need of sharing private training data. 
Based on the participants' scale, FL is typically divided into two categories: cross-silo FL and cross-device FL\cite{kairouz2021advances}. 
Cross-device FL typically involves numerous similar devices, while cross-silo FL usually includes fewer participants like organizations.
Recent studies show that FL mainly confronts two types of threats: privacy risks from curious adversaries attempting to compromise data privacy through methods like membership inference and model inversion attacks, and security risks from Byzantine adversaries looking to damage the final model's integrity with backdoors or by lowering its accuracy \cite{baracaldo2022protecting, nasr2019comprehensive, huang2021evaluating, geiping2020inverting, baruch2019little, bagdasaryan2020backdoor, xie2020fall}.


% How existing studies can solve them
To mitigate privacy risks in FL, researchers have developed a range of techniques to bolster privacy. These encompass differential privacy-based aggregation \cite{wei2020federated}, as well as secure aggregation approaches using homomorphic encryption\cite{zhang2020batchcrypt}, functional encryption\cite{xu2022detrust}, and secure multi-party computation\cite{bonawitz2017practical,zhang2023safelearning}.
Aside from privacy concerns, many studies have proposed strategies to identify and mitigate potentially harmful updates during the model aggregation phase, thereby safeguarding the global model against adversarial attacks. 
Notable Byzantine-resistant aggregation mechanisms encompass the Krum fusion method\cite{blanchard2017machine}, cosine defense aggregation mechanism\cite{sun2019can,yaldiz2023secure}, and median/mean-based strategies like clipping median and trimmed mean strategies \cite{yin2018byzantine}.
Research in these two areas has been conducted separately, and addressing both issues at once continues to be challenging. This difficulty arises because secure aggregation makes it easier for adversarial attacks to occur, as most anomaly detection methods need access to ``unencrypted'' local model updates that secure aggregation protects.



% dilemma issue - few studies
Few recent efforts \cite{yang2023model,hossain2021desmp,jiang2020mitigating,zhang2023safelearning,huang2024vppfl,ma2022shieldfl,dong2023privacy,li2024efficiently} to tackle both challenges simultaneously often depend on differential privacy techniques \cite{yang2023model,hossain2021desmp,jiang2020mitigating,liu2021privacy,guo2024siren}, which can degrade model performance due to added noise, or rely on impractical non-colluded two-server assumption that disrupts FL's topology\cite{zhang2023safelearning,huang2024vppfl,ma2022shieldfl,dong2023privacy,li2024efficiently}, complicating its deployment and application.
In light of these limitations, a critical yet overlooked question is \textit{how to create a straightforward dual defense strategy that simultaneously strengthens privacy protection and mitigates poisoning attacks without introducing new participant roles or altering the single-server multiple-clients structure?}



% our work summarization
To address this dilemma, this paper proposes a \textbf{D}ual \textbf{D}efense approach that simultaneously enhances privacy protection and combats poisoning attacks in \textbf{Fed}erated learning (\textit{DDFed}), without changing the structure of current FL frameworks. 
% \textit{DDFed} leverages cutting-edge cryptographic technology, specifically fully homomorphic encryption (FHE), to securely aggregate model updates in a simple and direct way. 
% Specifically, \textit{DDFed} allows each participant to encrypt their local model updates while the aggregation server performs secure aggregation in the dark, ensuring privacy protection. 
\textit{DDFed} initially leverages cutting-edge cryptographic technology, specifically fully homomorphic encryption (FHE), to securely aggregate model updates without the impractical assumption of non-colluding two-server setups and ensures strong privacy protection by permitting only the aggregation server to perform secure aggregation in the dark.
% In addition, drawing inspiration from existing Byzantine-resistant aggregation methods, \textit{DDFed} incorporates a similarity-based technique to identify malicious models during each training round of FL. 
% Our approach stands out by using optimized cosine-similarity to detect anomalies within FHE-encrypted model updates. It supports complex cosine-similarity operations in the dark and includes countermeasures to protect against privacy breaches by potentially malicious participants.
To tackle the challenge of detecting malicious models within encrypted model updates, \textit{DDFed} introduces a novel two-phase anomaly detection mechanism.
This approach enables cosine similarity computation over encrypted models and incorporates a feedback-driven collaborative selection process, with additional measures to prevent potential privacy breaches from Byzantine clients incorporated into the detection mechanism.
% key contributions
Our main contributions are summarized as follows:
\begin{itemize}
    \item We introduce a dual defense strategy that simultaneously boosts privacy and combats poisoning attacks in federated learning. This is achieved by integrating FHE-based secure aggregation with a mechanism for detecting malicious encrypted models based on similarity.
    \item To effectively detect malicious models in encrypted updates, we propose a novel two-phase anomaly detection mechanism with extra safeguards against potential privacy breaches by Byzantine clients during the detection process. Additionally, we introduce a clipping technique to bolster defenses against diverse poisoning attacks.
    \item We carried out comprehensive experiments on multiple model poisoning attacks and federated learning scenarios, covering both cross-device FL and cross-silo FL. Our experiments with publicly accessible datasets demonstrate \textit{DDFed}'s effectiveness in safeguarding model privacy and robustly defending against model poisoning threats.
\end{itemize}










\section{Related Works}
% The general FL framework inherently protects privacy since it doesn't share raw training data with collaborators \cite{konecny2016federated}. However, studies have shown that adversaries can still compromise user privacy and the global model's performance through poisoning attacks. This has led to the proposal of various secure and robust aggregation mechanisms.


% \paragraph{Privacy-preserving Federated Learning.}
\paragraph{Privacy Risks and Countermeasures in FL}

The fundamental design of FL ensures that all training data stays with its owner, offering basic privacy. However, it still exposes vulnerabilities to inference attacks, which allow adversaries to extract information about the training data used by each party \cite{nasr2019comprehensive, shokri2017membership,baracaldo2022protecting, nasr2019comprehensive, huang2021evaluating, geiping2020inverting}. In some cases, the risk of private information leakage may be unacceptable. Therefore, several defenses have been suggested to mitigate these risks, including differential privacy (DP) and secure aggregation (SA), based on various cryptographic primitives such as (partial) homomorphic encryption \cite{liu2019secure, zhang2020batchcrypt}, threshold Paillier \cite{truex2019hybrid}, functional encryption \cite{xu2019hybridalpha}, and pairwise masking protocols \cite{bonawitz2017practical}. 


% Secure aggregation methods use cryptographic techniques to protect the privacy of inputs, ensuring that an inquisitive or untrusted aggregator cannot view individual model updates. 
% Popular methods include (partial) homomorphic encryption \cite{liu2019secure, zhang2020batchcrypt}, threshold Paillier \cite{truex2019hybrid}, functional encryption \cite{xu2019hybridalpha}, and pairwise masking protocols \cite{bonawitz2017practical}. 
% Conversely, to counter inference attacks targeting the final model or its updates, DP strategies add a precisely calibrated amount of noise through differentially private mechanisms. Although DP offers robust privacy protection, it is well-known for generating models with reduced accuracy due to the added noise. Alternatively, some solutions ingeniously merge DP and SA techniques to maintain strong differential privacy protections while still achieving high model performance \cite{truex2019hybrid}.


% \paragraph{Poisoning Attacks and Defenses.}
\paragraph{Poisoning Risks and Countermeasures in FL.}

Besides privacy inference attacks, FL is also susceptible to poisoning attacks, where adversaries can compromise certain clients and manipulate their data or models to intentionally worsen the global model's performance by introducing corrupted updates during training.
This paper focuses on untargeted model attacks, whose goal is to significantly diminish the effectiveness of the global model through methods such as Inner Product Manipulation (IPM) attack \cite{xie2020fall}, scaling attack\cite{bagdasaryan2020backdoor}, and ``a little is enough" (ALIE) attack \cite{baruch2019little}. 
% Both scaling and ALIE attacks adjust local model updates by multiplying them with various strategic factors, while IPM changes malicious clients' local updates so that the inner product between the true gradient and aggregated updates turns negative.
Several strategies have been developed to counteract the impact of attacks, ensuring they don't compromise model performance. 
These strategies fall into two categories: client-side and server-side defenses. Client-side defenses adjust the local training algorithm with a focus on secure client updates\cite{sun2021fl}, whereas server-side defenses \cite{blanchard2017machine,sun2019can,yaldiz2023secure,yin2018byzantine} either reduce the influence of updates from malicious clients through adjusted aggregation weights or use clustering techniques to aggregate updates from trustworthy clients only.
However, these defense strategies typically operate under the assumption that model updates are not encrypted, which contradicts the objectives of privacy-focused secure aggregation defense strategies.


% Unlike the previously mentioned attacks that deduce private information, FL is also vulnerable to poisoning attacks. 
% In these scenarios, adversaries can compromise certain clients and manipulate their data or models to intentionally worsen the global model's performance by introducing corrupted updates during training. Poisoning attacks are typically categorized into targeted and untargeted types, depending on the attacker’s objectives. 
% This paper focuses on untargeted attacks, whose goal is to significantly diminish the effectiveness of the global model through methods such as Inner Product Manipulation (IPM) attack \cite{xie2020fall}, scaling attack\cite{bagdasaryan2020backdoor}, and ``a little is enough" (ALIE) attack \cite{baruch2019little}. Both scaling and ALIE attacks adjust local model updates by multiplying them with various strategic factors, while IPM changes malicious clients' local updates so that the inner product between the true gradient and aggregated updates turns negative.
% We will employ these attack techniques in our study to evaluate the efficacy of our proposed solution.


% Several strategies have been developed to counteract the impact of attacks, ensuring they don't compromise model performance. These strategies fall into two categories: client-side and server-side defenses. Client-side defenses adjust the local training algorithm with a focus on secure client participation, whereas server-side defenses either reduce the influence of updates from malicious clients through adjusted aggregation weights or use clustering techniques to aggregate updates from trustworthy clients only.
% For instance, Sun et al.\cite{sun2021fl} have demonstrated a method for countering model poisoning by perturbing the model during the local training phase of benign clients.
% Prominent server-side defense methods include Krum fusion technique\cite{blanchard2017machine}, similarity-based aggregation mechanisms\cite{sun2019can,yaldiz2023secure}, and median/mean-oriented approaches such as clipping median and trimmed mean tactics\cite{yin2018byzantine}.
% However, these defense strategies typically operate under the assumption that model updates are not encrypted, which contradicts the objectives of privacy-focused secure aggregation defense strategies.



\paragraph{Private and Robust Federated Learning.}

In privacy-preserving FL, identifying poisoning attacks is harder because of the need to balance local model privacy with the detection of harmful models.
Only a few existing studies like those mentioned in \cite{yang2023model,hossain2021desmp,jiang2020mitigating,huang2024vppfl} employ Byzantine-resilient aggregation through differential-privacy techniques. This approach necessitates a compromise between privacy and model accuracy.
Additionally, recent initiatives have been launched to address this problem through diverse methods by using various secure computation technologies. These include 3PC\cite{dong2023privacy}, which faces scalability limitations; an oblivious random grouping method constrained by its design for partial parameter disclosure\cite{zhang2023safelearning}; and both additive secret sharing\cite{li2024efficiently} and two-trapdoor homomorphic encryption\cite{ma2022shieldfl}, which depend on the impractical assumption of non-colluding dual servers.


% In privacy-preserving FL, identifying poisoning attacks is harder because of the need to balance local model privacy with the detection of harmful models, which involves closely examining each update.
% However, in secure federated learning where user updates are completely hidden, it's not possible to directly use Byzantine-resilient aggregation methods.
% Only a few existing studies like those mentioned in \cite{yang2023model,hossain2021desmp,jiang2020mitigating,huang2024vppfl} employ Byzantine-resilient aggregation through differential-privacy techniques. This approach necessitates a compromise between privacy and model accuracy and usually results in reduced accuracy.

% Recent methods have been developed to tackle this issue from different angles. 
% Dong et al \cite{dong2023privacy} propose a secure three-party computation method, but its design limits the scalability of federated learning. 
% Li et al \cite{li2024efficiently} introduce an additive secret sharing approach, which requires two non-colluding servers—a significant challenge for deployment. 
% Ma et al \cite{ma2022shieldfl} present a two-trapdoor homomorphic encryption method based on the Paillier cryptosystem, yet it also needs a two-server setting without collusion and struggles with inefficiency and impracticality. Zhang et al \cite{zhang2023safelearning} suggest an oblivious random grouping technique limited by its partial parameter disclosure design.













\section{Dual Defense Federated Learning Framework}

% In this section, we elaborate our proposed \textbf{D}ual \textbf{D}efense approach for enhancing privacy preservation and mitigating poisoning attack in \textbf{Fed}erated learning (DDFed).

\subsection{Formulation and Assumption}

\paragraph{Formulation.} 

A typical FL framework involves $m$ clients, $\mathcal{C}_1, ..., \mathcal{C}_m$, and a single aggregation server $\mathcal{A}$. Each client $\mathcal{C}_i$ possesses its own dataset $D_i$. The overarching goal in FL across these $m$ clients is to minimize the global objective function:
\begin{equation}
\min_{\pmb{W}_1, ..., \pmb{W}_m} \frac{1}{m}\sum_{i=1}^{m}\frac{|D_i|}{\sum_{i=1}^{m}|D_i|} L_i(\pmb{W}_i; D_i).
\end{equation}
Here, $L_i$ represents the local loss function for each client's data, and $\pmb{W}_i$ are the local model parameters specific to client $\mathcal{C}_i$. The term $D_i$ refers to the private dataset of client $i$, with $|D_i|$ indicating its size in terms of sample count.
In short, the goal of general FL is to learn an optimal global model $\pmb{W}_{G}$ across $m$ clients. This is achieved by periodically synchronizing the model parameters from all clients using specified fusion algorithms like \textit{FedAvg} and its variants, with the aggregation server $\mathcal{A}$ over several training rounds. 

Due to various malicious activities, including inference attacks that aim to steal private information from legitimate clients and poisoning attacks designed to undermine model integrity by degrading its performance, existing privacy-preserving FL often relies on a secure aggregation mechanism\cite{liu2019secure, zhang2020batchcrypt, truex2019hybrid, xu2019hybridalpha, bonawitz2017practical}. Typically, without loss of generality, during the $t$-th federated learning training round, each client $\mathcal{C}_i$ secures its local model update $\pmb{W}_{i}$ - referred to as $[\![\pmb{W}_i]\!]$ throughout this paper - before transmitting it to the aggregation server. This is achieved by using various privacy-enhancing technologies such as homomorphic encryption and secure multi-party computation.




\paragraph{Threat Assumption.}

\textit{DDFed} tolerates an adversary, capable of corrupting any subset of local clients at a specified ratio $r_{\textsc{attack}}, s.t., r_{\textsc{attack}} < 0.5$, to carry out model poisoning attacks that degrade the global model's performance. 
Additionally, we assume the aggregation server $\mathcal{A}$ is semi-honest (honest-but-curious), meaning it adheres to the protocol but seeks to glean as much private information as possible. Similarly, the compromised clients $\mathcal{C}^{\textsc{Adv}}_{i}$ can conduct privacy inference attacks like those performed by $\mathcal{A}$. In summary, regarding privacy preservation, both the inquisitive $\mathcal{A}$ and the corrupted client subset aim to extract private information from benign clients; however, only the corrupted client subset will also initiate model poisoning attacks to undermine the global model.


\subsection{Framework Details}

\paragraph{Objective of DDFed.}

\textit{DDFed} is designed to bolster privacy protection and mitigate model poisoning attacks seamlessly within the existing FL framework. 
Unlike existing private and robust approaches \cite{yang2023model,hossain2021desmp,jiang2020mitigating,zhang2023safelearning,huang2024vppfl,ma2022shieldfl,dong2023privacy,li2024efficiently} that add new participant roles or depend on differential privacy, which may compromise model performance, \textit{DDFed} maintains effectiveness efficiently.
\textit{DDFed} introduces a dual defense strategy that combines fully homomorphic encryption (FHE) for secure data aggregation with an optimized similarity-based mechanism to detect malicious models, ensuring unparalleled privacy protection and security against model poisoning attacks.

Similarity-based methods are commonly used in existing studies for anomaly detection models \cite{sun2019can,yaldiz2023secure}.
Specifically, it computes the cosine similarity between each local model update of training round $t$ and the global model from the previous round $t-1$:
\begin{equation}
    cos(\alpha_i) = \frac{\langle\pmb{W}^{(t)}_{i}, \pmb{W}^{(t-1)}_{G}\rangle}{\|\pmb{W}^{(t)}_{i}\|_{2}\cdot\|\pmb{W}^{(t-1)}_{G}\|_{2}},
    \label{eq:basic}
\end{equation}
where $\alpha_i$ denotes the angle between global model weights $\pmb{W}^{(t-1)}_{G}$ and local model update $\pmb{W}^{(t)}_{i}$ of client $\mathcal{C}_i$. 
However, existing similarity-based mechanisms \cite{sun2019can,yaldiz2023secure} offer no privacy protection for local model updates, and integrating FHE into them poses significant challenges. These challenges arise from FHE's limitations in performing division and comparison operations, which are essential for identifying benign clients in these methods.


\paragraph{Framework Overview and Training Process.}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{images/framework-v8.png}
    
    \caption{Overview of \textit{DDFed} framework and illustration of a single round \textit{DDFed} training.}
    \label{framework}
    % \vspace{-5mm}
\end{figure}

\figurename~\ref{framework} provides an overview of \textit{DDFed} framework, which includes several clients $\mathcal{C}_1, ..., \mathcal{C}_m$ and a single aggregation server $\mathcal{A}$, consistent with the architecture of most existing FL frameworks.
In the following section, we demonstrate the \textit{DDFed} training process. Due to space limitations, the formal algorithm pseudocode is provided solely in Appendix~\ref{sec:appendix:algorithm}.


Before the FL training begins, each client $\mathcal{C}_i$ is equipped with an FHE key pair $(\textsc{pk}, \textsc{sk})$. During the FL training phase, let's assume that in the $t$-th round, each client $\mathcal{C}_i$ trains a local model $\pmb{W}^{(t)}_i$ (\textcircled{1}) and performs the normalization and encryption as $[\![ \pmb{W}^{(t)}_i ]\!] = \textsc{FHE}.\textsc{Enc}_{\textsc{pk}}(\pmb{W}^{(t)}_i)$ with public key $\textsc{pk}$ (\textcircled{2}).
Upon receiving encrypted local models, $\{[\![ \pmb{W}^{(t)}_{i} ]\!]\}_{i\in[1,...,m]}$, $\mathcal{A}$ starts to detect anomaly model updates over all encrypted local models.
Specifically, $\mathcal{A}$ first extracts the last layer, denoted as $\{[\![ \pmb{W}^{'(t)}_{i} ]\!]\}_{i\in[1,...,m]}$, which remains encrypted (\textcircled{3}), and adds a perturbation $\Delta^{(t)}$ to safeguard against potential privacy attacks by malicious clients.
Next, it retrieves the last layer of the encrypted global model from the previous training round ($[\![ \pmb{W}^{'(t-1)}_{G} ]\!]$), The method for adding perturbations will be discussed in Section~\ref{sec:perturb}.
Then, $\mathcal{A}$ performs secure inner-product between each perturbed $[\![ \pmb{W}^{'(t)}_{i} ]\!]+\Delta^{(t)}$ and $[\![ \pmb{W}^{'(t-1)}_{G} ]\!]$ to derive encrypted similarity score, denoted as $[\![ \pmb{s}^{'(t)} ]\!] = ([\![s^{'(t)}_{1}]\!], ..., [\![s^{'(t)}_{m}]\!])$, and query each client (\textcircled{4}).  
After receiving $[\![ \pmb{s}^{'(t)} ]\!]$, each client $\mathcal{C}_{i}$ decrypts it to obtain the plaintext scores $\pmb{s}^{'(t)}_{i}$.
Subsequently, each client submits their list of similarity scores (\textcircled{5}).
It's important to note that at this stage, malicious clients may tamper with their similarity scores in an attempt to prevent detection of their compromised models.
Since a benign client will honestly and accurately decrypt and select trustworthy clients group via threshold-based filter, and hence their results should be consistent. Therefore, $\mathcal{A}$ uses a majority voting strategy to acquire the final client score list, i.e., the voted $\pmb{s}^{(t)}$ (\textcircled{6}).
Next, $\mathcal{A}$ normalizes $\pmb{s}^{(t)}$ and generates the fusion weight (\textcircled{7}). Here, \textit{DDFed} employs \textit{FedAvg}'s approach by weighting the aggregation according to dataset size proportions in current training round (\textcircled{8}).
Finally, each client $\mathcal{C}_{i}$ receives the aggregated global model $[\![ \pmb{W}^{(t)}_{G} ]\!]$, decrypts it, and initiates the $(t+1)$-th round of \textit{DDFed} training (\textcircled{9}).



\paragraph{Private and Robust Malicious Model Detection.}
As observed in \cite{yaldiz2023secure}, the distribution of local data labels can be more effectively represented in the weights of the last layer than in other layers.
% According to \cite{yaldiz2023secure}, it was observed that the distribution of local data labels can be more effectively represented in the weights of the last layer than in other layers. 
Consequently, \textit{DDFed} employs a similar approach to enhance the efficiency of detecting anomalies, as it requires performing similarity computation on encrypted model updates.
Given that FHE supports only basic mathematical operations, and the similarity-based anomaly model detection mechanism needs complex operations like division (as shown in equation~\ref{eq:basic}),  comparison and sorting operations, \textit{DDFed} breaks it down into two stages: \textit{secure similarity computation} and \textit{feedback-driven collaborative selection}.
In the rest of the paper and during our experimental evaluation, we adhere to the layer section settings described in \cite{yaldiz2023secure}. However, \textit{DDFed} can be easily extended to support strategies for detecting malicious models using full layers. Additional experiments are detailed in Appendix~\ref{sec:appendix:results_cmp_layers} to demonstrate the impact of layer sections on the \textit{DDFed} framework.


\paragraph{\textit{Secure Similarity Computation.}}

To circumvent division operations, \textit{DDFed} necessitates that all clients pre-process their inputs for normalization and shifts the task of comparing similarity scores to the client side. This is because clients possess the FHE private key, allowing them to obtain the similarity score in plaintext.
Formally, we have the following:
\begin{equation}
    [\![cos(\alpha_i)]\!] = \frac{\langle[\![\pmb{W}^{(t)}_{i}]\!] , [\![\pmb{W}^{(t-1)}_{G}]\!] \rangle}{\|[\![\pmb{W}^{(t)}_{i}]\!] \|_{2}\cdot\|[\![\pmb{W}^{(t-1)}_{G}]\!] \|_{2}} = \langle[\![\frac{\pmb{W}^{(t)}_{i}}{\|\pmb{W}^{(t)}_{i}\|_{2}}]\!],  [\![\frac{\pmb{W}^{(t-1)}_{G}}{\|\pmb{W}^{(t-1)}_{G}\|_{2}}]\!]\rangle,
    \label{eq:decompose}
\end{equation}
where each client $\mathcal{C}_{i}$ prepares the $\frac{\pmb{W}^{(t)}_{i}}{\|\pmb{W}^{(t)}_{i}\|_{2}}$ and $\frac{\pmb{W}^{(t-1)}_{G}}{\|\pmb{W}^{(t-1)}_{G}\|_{2}}$ in advance, and then encrypts them using FHE encryption algorithm. 
Next, the aggregation server $\mathcal{S}$ verifies received $[\![\frac{\pmb{W}^{(t-1)}_{G}}{\|\pmb{W}^{(t-1)}_{G}\|_{2}}]\!]$ and perturbs local inputs and conducts secure inner-product computation as follows:
\begin{equation}
    [\![\pmb{s}^{'(t)}]\!] = \langle[\![\frac{\pmb{W}^{(t)}_{i}}{\|\pmb{W}^{(t)}_{i}\|_{2}}]\!] + \Delta^{(t)},  [\![\frac{\pmb{W}^{(t-1)}_{G}}{\|\pmb{W}^{(t-1)}_{G}\|_{2}}]\!]\rangle.
    \label{eq:opt}
\end{equation}


\paragraph{\textit{Motivation of Similarity Score Perturbation}.}
\label{sec:perturb}
% \textit{DDFed} focuses on addressing both privacy and poisoning risks at the same time.
\textit{DDFed} aims to simultaneously address privacy and poisoning risks.
This means it not only considers model poisoning attacks but also prevents adversarial clients from inferring private information from other benign clients by exploiting decrypted similarity scores and previous global models.
To mitigate this privacy risk, \textit{DDFed} improves secure inner-product computation by introducing perturbations into each normalized and encrypted model update.
Specifically, \textit{DDFed} uses $(\varepsilon, \delta)$-differential privacy with a Gaussian mechanism as its method of perturbation, $\Delta^{(t)} = \mathcal{N}(0, \sigma^{2}), \sigma = \frac{\Delta f \sqrt{2 \ln(1.25/\delta)}}{\varepsilon}$, 
% \begin{equation}
%     \Delta^{(t)} = \mathcal{N}(0, \sigma^{2}), \sigma = \frac{\Delta f \sqrt{2 \ln(1.25/\delta)}}{\varepsilon},
% \end{equation}
where $(\varepsilon, \delta)$ represents the parameters of the DP mechanism and $\Delta f$ denotes sensitivity.

It's important to note that our perturbation affects only the anomaly detection phase and does not change the encrypted model updates that are to be aggregated. Consequently, the final aggregated model retains its accuracy, just as it would with a standard aggregation mechanism.
Furthermore, our experiments indicate that the perturbation noise does not affect the effectiveness of anomaly detection. Even at $\varepsilon=0.01$, which offers strong privacy protection, \textit{DDFed} still performs well and delivers good model performance.



\paragraph{\textit{Feedback-driven Collaborative Selection.}}
As shown in the threat model, \textit{DDFed} tolerates less than 50\% malicious clients, indicating that over half of the clients are benign and will execute the steps honestly and correctly as designed. 
\textit{DDFed} employs a feedback-driven collaborative selection approach to filter out potentially malicious models.
Specifically, upon receiving the encrypted $[\![\pmb{s}^{'(t)}]\!]$, each client $\mathcal{C}_{i}$ first decrypts to acquire $\pmb{s}^{'(t)}_{i}$ using the FHE private key $\textsc{sk}$. 
Next, each client $\mathcal{C}_{i}$ independently decrypts the similarity scores, sorts them, and selects trustworthy clients $\pmb{s}^{(t)}_{i}$ for the current training round based on a threshold.
\textit{DDFed} uses only the mean value of similarity scores as its filtering threshold. Subsequent experiments have demonstrated its effectiveness. Additionally, \textit{DDFed} is open and compatible with alternative methods for setting thresholds.
After each client returns their decision on the group of benign clients ($\pmb{s}^{(t)}_{i}$), the aggregation server uses a majority of vote strategy to decide the final aggregation group ($\pmb{s}^{(t)}$) for the current training round.
Next, similar to \textit{FedAvg}, \textit{DDFed} applies a data size-based fusion weight strategy to calculate each client's fusion weight $\pmb{f}^{\pmb{W}^{(t)}}_{\pmb{s}^{(t)}}$ in the aggregation group, where $f^{(t)}_{j}=\frac{|D_j|}{\sum_{j\in\pmb{s}^{(t)}} |D_j|}$. 



\paragraph{FHE-based Secure Aggregation with Clipping.}

\textit{DDFed}'s secure aggregation leverages the FHE cryptosystem, specifically the CKKS instance\cite{cheon2017homomorphic}, which excels in arithmetic operations on encrypted real or complex numbers and stands as one of the most efficient methods for computing with encrypted data.
Formally, the aggregation server performs secure aggregation as $[\![\pmb{W}^{(t)}_{G}]\!] = \langle [\![\pmb{W}^{(t)}]\!], \pmb{f}^{\pmb{W}^{(t)}}_{\pmb{s}^{(t)}}\rangle$.
% \begin{equation}
%     [\![\pmb{W}^{(t)}_{G}]\!] = \langle [\![\pmb{W}^{(t)}]\!], \pmb{f}^{\pmb{W}^{(t)}}_{\pmb{s}^{(t)}}\rangle.
% \end{equation}
Once receiving the aggregated global model $[\![\pmb{W}^{(t)}_{G}]\!]$, each client $\mathcal{C}_{i}$ uses their private key to decrypt it, obtaining the final global model $\pmb{W}^{(t)}_{G}$ in plaintext via the FHE decryption algorithm.
In contrast to current approaches in private and robust FL, \textit{DDFed} uniquely enables each benign client to execute a clipping operation before the next training round. This enhancement is designed to counteract more sophisticated model poisoning attacks that conventional similarity-based methods \cite{sun2019can,yaldiz2023secure}  fail to address, as will be shown in the experiments section.


\subsection{Analysis on Privacy and Robustness}

Based on the threat model discussed earlier, \textit{DDFed} prevents an honest-but-curious aggregation server from potentially inferring private information from accessible model updates. Additionally, it also withstands a subset of local clients, compromised by an adversary, to launch model poisoning attacks and attempt to infer private information from other benign clients during the anomaly model detection phase.

In terms of privacy risks, \textit{DDFed} utilizes FHE primitives to ensure cryptographic-level privacy protection. This means the aggregation server processes each operation without any insight into the model update (in the dark), eliminating any chance of inferring private information from local model updates. Furthermore, to counter potential inferences by corrupted clients exploiting decrypted similarity scores, \textit{DDFed} incorporates a perturbation method where DP noise is added during the secure similarity computation phase.
Due to space limitations, the formal DP-enhanced perturbation analysis is provided solely in Appendix~\ref{sec:appendix:privacy}.

Regarding the risk of poisoning attacks, \textit{DDFed} adopts similarity-based anomaly detection technologies with additional optimizations such as perturbation-based similarity computation and post-aggregation clipping. These enhancements bolster the robustness of its aggregation mechanism.
Our experiments demonstrate that \textit{DDFed} effectively resists a range of continuous poisoning attacks, including IPM, SCALING, and ALIE attacks, which will be elaborated in Section~\ref{sec:experiment}.






\section{Experiments}
\label{sec:experiment}


\subsection{Experimental Setup}


\paragraph{Datasets and Implementation.}

We assessed our proposed \textit{DDFed} framework using publicly available benchmark datasets: MNIST\cite{lecun2010mnist}, a collection of handwritten digits, and Fashion-MNIST (FMNIST)\cite{xiao2017fashion}, which includes images of various clothing items, offering a more challenging and diverse dataset for federated learning tasks.
We create non-iid partitions for all datasets based on previous research \cite{yaldiz2023secure,zhang2023safelearning}, using a default $q$ value of 0.5, where a higher $q$ reflects greater degrees of non-iid.
We assess the framework's performance using a nine-layer CNN model with 225k parameters, secured by the FHE cryptosystem in each training round. This secure aggregation is implemented through TenSEAL library \cite{tenseal2021}.
The experimental \textit{DDFed} is available on the \href{https://github.com/irxyzzz/DualDefense/}{GitHub repository}.



\paragraph{Baselines and Default Setting.}

We compare our proposed method \textbf{\textit{DDFed}} with well-known FL fusion algorithms and robust aggregation methods, including \textbf{\textit{Krum}} \cite{blanchard2017machine}, \textbf{\textit{Cos Defense}} \cite{yaldiz2023secure}, and median/mean-based approaches like \textbf{\textit{Median}}, \textbf{\textit{Clipping Median}}, and \textbf{\textit{Trimmed Mean}} strategies\cite{yin2018byzantine}. 
We exclude baselines such as FLTrust\cite{cao2021fltrust} or RFFL\cite{xu2020reputation} because they require server-side validation data or are incompatible with client sampling, making them impractical for real-world applications. Additionally, we omit secure robust approaches\cite{dong2023privacy,li2024efficiently,ma2022shieldfl,zhang2023safelearning} that depend on complex secure aggregation techniques due to their requirement for additional non-colluding participants, which alters the original structure of the federated learning framework.
Note that the core contribution of this paper is not to propose new model poisoning defense approaches, but to enhance existing popular defenses with privacy features—specifically, server-side similarity-based defenses. Therefore, the experiments aim to evaluate how these privacy-preserving features affect the original defense methods, rather than defending against recent attack techniques and strategies as shown in works like \cite{pillutla2022robust,wangtowards,zhangflip,fang2020local,nguyen2022flame}.



To assess defense performance, we evaluated the proposed work against popular model poisoning attacks: Inner Product Manipulation (\textit{\textbf{IPM}}) attack \cite{xie2020fall}, \textit{\textbf{scaling}} attack\cite{bagdasaryan2020backdoor}, and the "a little is enough" (\textit{\textbf{ALIE}}) attack\cite{baruch2019little}. 
Unless otherwise mentioned, we assume a default attacker ratio of 0.3 among all participants as malicious clients. The attacks commence at the 50th round and persist until training ends. The default FL training involves 10 clients randomly chosen from 100 for each communication round.
Furthermore, we employ a batch size of 64 with each client conducting local training over three epochs per round using an SGD optimizer with a momentum of 0.9 and a learning rate of 0.01. Our \textit{DDFed} implementation's default epsilon ($\varepsilon$) value is set to 0.01 unless specified differently.



\subsection{Performance Evaluation}
\label{sec:exp:results}

\paragraph{Performance of Defense Effectiveness under Various Attacks.}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{images/all_attacks_baselines_cmp.pdf}
    % \vspace{-3mm}
    \caption{Comparison of defense effectiveness across various defense approaches, evaluated on MNIST (top) and FMNIST(bottom), under IPM attack (left), ALIE attack (middle), and SCALING attack (right).}
    \label{fig:cmp_defense_baselines}
    % \vspace{-6mm}
\end{figure}

\figurename~\ref{fig:cmp_defense_baselines} demonstrates the effectiveness of our \textit{DDFed} method compared to baseline methods in countering three prevalent model poisoning attacks, with an attacker ratio set at 0.3. 
The attack commences at the 50th round and continues until training concludes.
% In each training round, 10 out of 100 clients are randomly selected, and from these, 30\% are randomly chosen to carry out model poisoning attacks.
Under the IPM attack scenario, aside from FedAvg, Trimmed Mean, and Clipping Median mechanisms, our approach along with other defense strategies performs well (nearly as model accuracy as without any model poisoning attack) in defending against the IPM attack. The same conclusion also holds true in the ALIE attack.
However, only \textit{DDFed} and Clip Median successfully withstand SCALING attacks with minor and acceptable losses in model performance.
Note that \textit{DDFed} remains robust even when attackers target the system from the start of training. Due to space constraints, we present the defense effectiveness against various cold-start attacks in Appendix~\ref{sec:appendix:results_cold_start_attack}.
In summary, our \textit{DDFed} method achieves the best comprehensive defense performance.





\paragraph{Impact of Attacker Ratio.}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{images/all_attack_ratio_cmp.pdf}
    \caption{Comparison of \textit{DDFed} effectiveness across different attack ratios, evaluated on MNIST (top) and FMNIST (bottom), under IPM attack (left), ALIE attack (middle), and SCALING attack (right).}
    \label{fig:cmp_impact_attack_ratio}
    % \vspace{-5mm}
\end{figure}

To further investigate the impact of attacker ratio in the \textit{DDFed} framework, we conducted experiments with various attacker ratio settings. 
It's important to note that \textit{DDFed} operates under the security assumption that at least half of the participants must be benign (i.e., $r_{attacker} < 0.5$), therefore, in our experiments, the attacker ratio setting is ranged from 0.1 to 0.4.
% In addition, the attack begins at the 50th round and 10 out of 100 clients are randomly selected for each training round.
As shown in \figurename~\ref{fig:cmp_impact_attack_ratio}, the proportion of attackers among all clients does not significantly affect our proposed \textit{DDFed} method. This suggests that it can effectively counter three types of model poisoning attacks. Additionally, we observed that under an ALIE attack scenario, our method may require approximately 10-20 training rounds to recover from the continuous attack, depending on the dataset evaluated.





 
\paragraph{Compatibility with Cross-device and Cross-silo FL Scenarios.}


\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{images/all_parties_cmp.pdf}
    \caption{Comparison of \textit{DDFed} effectiveness across different client numbers, evaluated on MNIST (top) and FMNIST (bottom), under IPM attack (left), ALIE attack (middle), and SCALING attack (right).}
    \label{fig:cmp_parties}
    % \vspace{-5mm}
\end{figure}

To explore how the number of clients affects our \textit{DDFed} framework and to confirm its compatibility with two common federated learning scenarios, i.e., cross-device and cross-silo, we conducted multiple experiments.
These experiments had an attacker ratio fixed at 0.3, with client counts varying from 10 to 100.
In cross-silo FL, client numbers are typically small, often ranging from a few to several dozen; however, for simulating the cross-device FL scenario in our study, we used 100 clients due to their generally larger population.
As illustrated in Figure~\ref{fig:cmp_parties}, our \textit{DDFed} framework effectively defends against all three attacks across various client number settings.
This suggests that the performance of \textit{DDFed} is not significantly affected by the number of clients, indicating its suitability for both cross-silo and cross-device FL scenarios.
Furthermore, a higher number of client settings may result in relatively large fluctuations during training rounds immediately following the attack; however, the model training ultimately converges steadily, unaffected by the continuous attack.




\paragraph{Time Cost of Secure Aggregation.}

\begin{table}[h]
\centering
% \vspace{-3mm}
\caption{Time cost per training round of various defense approaches.}
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
Approaches & \multicolumn{2}{c}{MNIST, IPM attack} & \multicolumn{2}{c}{FMNIST, IPM attack} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
                   & avg (s) & var (s) & avg (s) & var (s) \\
\midrule
% FedAvg (no attack) & 10.49   & 0.01    & 10.33   & 0.03    \\
FedAvg             & 10.26   & 0.07    & 10.47   & 0.01    \\
Krum               & 10.32   & 0.03    & 10.26   & 0.01    \\
Median             & 10.32   & 0.01    & 10.28   & 0.02    \\
Clipping Median    & 10.31   & 0.01    & 10.32   & 0.01    \\
Trimmed Mean       & 10.32   & 0.02    & 10.30   & 0.01    \\
Cos Defense        & 10.25   & 0.01    & 10.26   & 0.02    \\
DDFed (Our Work)   & 12.43   & 0.01    & 12.14   & 0.01    \\
\bottomrule
\end{tabular}

\label{tab:time_cost_ipm}
% \vspace{-5mm}
\end{table}


To assess the additional time cost incurred by integrating FHE-based secure similarity computation and secure aggregation into \textit{DDFed}, we measured the time cost of each training round and compared it with the baseline methods mentioned earlier. 
All experiments were carried out using the default settings described above. 
Due to space constraints, we only present the defense approach's time cost per training round when under an IPM attack and have included further results in Appendix~\ref{sec:appendix:results_time}.

As shown in Table~\ref{tab:time_cost_ipm}, compared to other robust aggregation mechanisms that lack privacy-preserving features, our \textit{DDFed} solution incurs additional time costs due to the integration of FHE-based secure similarity computation and secure aggregation. 
Across experiments on various datasets and under different attacks, our \textit{DDFed} generally requires an extra 2 seconds compared to the usual 10-second training round, resulting in a 20\% increase in time per training round. However, our \textit{DDFed} is capable of defending against model poisoning attacks while also offering strong privacy guarantees.
Note that the time-related experiments were conducted on a MacOS platform with an Apple M2 Max chip and 96GB of memory.






% \begin{figure} 
%     \centering
%     \includegraphics[width=0.7\textwidth]{images/all_epsilon_cmp_acc_p10.pdf}
%     \caption{Impact of hyper-parameter $\epsilon$ of differential privacy based perturbation at secure similarity computation phase in  \textit{DDFed} framework}
%     \label{fig:cmp_epsilon}
% \end{figure}

\paragraph{Impact of Epsilon Setting.} 

To better understand the effect of the hyperparameter $\varepsilon$ setting on \textit{DDFed}'s perturbation-based secure similarity computation phase, we conducted several experiments with different $\varepsilon$ settings, ranging from 0.01 to 0.1. 
Here, we only demonstrate the results from 10 clients here, with additional results in Appendix~\ref{sec:appendix:results_epsilon}.
% These experiments involved 10 clients and an attacker ratio of 0.3 under the IPM attack.

\begin{minipage}{0.33\textwidth}
  As shown in Figure~\ref{fig:cmp_epsilon}, the $\varepsilon$ setting has a negligible impact on performance with the MNIST dataset. However, higher $\varepsilon$ values, which indicate stronger DP protection, cause relatively larger fluctuations in performance on the FMNIST dataset. Therefore, we believe that the optimal $\varepsilon$ setting depends on the specific task at hand and leave it as an open question for future research. 
\end{minipage}
\hspace{0.01\textwidth}
\begin{minipage}{0.66\textwidth}
  \centering
    \includegraphics[width=\textwidth]{images/all_epsilon_cmp_acc_p10.pdf}
    \captionof{figure}{Impact of hyper-parameter $\epsilon$ of differential privacy based perturbation at secure similarity computation phase, evaluated on MNIST (left) and FMNIST (right), under IPM attack.}
    \label{fig:cmp_epsilon}
\end{minipage}%

% Due to space limitations, we only present the settings for client number 10 here and have included additional results in Appendix~\ref{sec:appendix:results_epsilon}.



\subsection{Discussion and Limitation}

To the best of our knowledge, \textit{DDFed} offers a dual defense strategy that simultaneously boosts privacy protection and fights against poisoning attacks in FL, without altering the existing FL framework's architecture. 
\textit{DDFed} utilizes FHE for top-notch privacy, enabling the aggregation server to perform similarity calculations and aggregation without directly accessing model updates.
Additionally, \textit{DDFed} introduces perturbation techniques to block attempts by malicious clients to infer information from similarity scores. 
It further employs similarity-based anomaly detection, enhanced with strategies like perturbation and post-aggregation clipping, to protect against various types of poisoning attacks.
However, \textit{DDFed} has not fully explored two related questions: how can we relax the attacker ratio restriction (i.e., $r_{\textsc{attack}} < 0.5$) while still ensuring effective dual defense? And how can we adapt \textit{DDFed} to more complex FL scenarios, such as dropout and dynamic participant groups? We leave these questions open for future research.
Currently, \textit{DDFed} only enhances existing popular defenses, such as similarity-based strategies with privacy features. Extending \textit{DDFed} to support other or more recent defense strategies remains an open question.







\section{Conclusion}

To tackle the dual challenges of privacy risks and model poisoning in federated learning, we introduce \textit{DDFed}, a comprehensive approach that strengthens privacy protections and counters model poisoning attacks. 
\textit{DDFed} enhances privacy by using an FHE-based secure aggregation mechanism and addresses encrypted poisoned model detection through an innovative secure similarity-based anomaly filtering method.
This method includes secure similarity computation with perturbation and feedback-driven selection process to distinguish safe model updates from potentially harmful ones. 
Our approach has been rigorously tested against well-known attacks on diverse datasets, demonstrating its effectiveness. We believe our work sets a solid foundation for future advancements in secure and robust federated learning.





\acksection

This work is funded by the National Natural Science Foundation of China, under grants No.62302022, No.62225202, No.62202038. 
We sincerely thank the anonymous reviewers for their insightful comments and constructive feedback, which have greatly improved this paper. Their suggestions were invaluable in refining our analysis and presentation, as well as guiding future research questions related to this work.




% References follow the acknowledgments in the camera-ready paper. Use unnumbered first-level heading for
% the references. Any choice of citation style is acceptable as long as you are
% consistent. It is permissible to reduce the font size to \verb+small+ (9 point)
% when listing the references.
% Note that the Reference section does not count towards the page limit.
% \medskip

\bibliographystyle{abbrvnat}
\bibliography{draft}









%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix

\section{Appendix}

\subsection{DDFed Algorithm}
\label{sec:appendix:algorithm}


\begin{algorithm}[]
    \SetAlgoLined
    \caption{\textit{DDFed} Training}
    \label{alg:framework}
    
    \SetKwInput{KwInput}{Input}                % Set the Input
    \SetKwInput{KwOutput}{Output}              % set the Output
    \SetKwProg{Fn}{function}{}{}
    \SetKwRepeat{Do}{do}{while}%
    
    \small
    \KwInput{clients $\{\mathcal{C}_{1}, ..., \mathcal{C}_{m}\}$, each client $\mathcal{C}_{i}$ has its own dataset $D_{i}$; global training round $T$}
    \KwOutput{final global model $\pmb{W}_{G}^{(T)}$.}
    
    Each client initializes with FHE key pair $(\textsc{pk},\textsc{sk})$\;
    aggregation server $\mathcal{A}$ initializes the global model $\pmb{W}_{G}^{(0)}$\;
    \ForEach{training round ${t} \in \{1, ..., T\}$ }{
        \ForEach{client $\mathcal{C}_{i} \in \{\mathcal{C}_{1}, ..., \mathcal{C}_{m}\}$}{
            \If{not initial training round}{
                $\mathcal{C}_i$ receives $[\![\pmb{W}^{(t-1)}_{G}]\!]$ from $\mathcal{A}$ and acquires $\pmb{W}^{(t-1)}_{G} \gets \textsc{FHE.Dec}_{\textsc{sk}}([\![\pmb{W}^{(t-1)}_{G}]\!])$\;
            }
            \If{reaching final training round $T$}{
                \Return final model $\mathcal{M}_{G}^{(m)}$\; 
            }
            \lIf{$\mathcal{C}_{i}$ is benign}{$\mathcal{C}_{i}$ performs clipping on $\pmb{W}^{(t-1)}_{G}$}
            $\mathcal{C}_i$ conducts local training $\pmb{W}^{(t)}_{i} \gets \textsc{Train}(\pmb{W}^{(t-1)}_{G})$\;
            $\mathcal{C}_i$ encrypts local model $[\![\pmb{W}^{(t)}_{i},\pmb{W}^{L,(t)}_{i}]\!] \gets \textsc{FHE.Enc}_{\textsc{PK}}([\![\pmb{W}^{(t-1)}_{G}]\!])$\;
            $\mathcal{C}_i$ sends out $[\![\pmb{W}^{(t)}_{i},\pmb{W}^{L,(t)}_{i}]\!]$ to $\mathcal{A}$\;
        }
        $\mathcal{A}$ waits and collects $\{[\![\pmb{W}^{(t)}_{i},\pmb{W}^{L,(t)}_{i}]\!]\}_{i\in [1,...,m]}$\; 
        $\mathcal{A}$ retrieves $[\![\pmb{W}^{L,(t-1)}_{G}]\!]$ from previous round and prepares perturbation $\Delta^{(t)}$\;
        $\mathcal{A}$ performs $[\![\pmb{s}^{'(t)}]\!] \gets \{\langle[\![\pmb{W}^{L,(t)}_{i}]\!]+\Delta^{(t)},[\![\pmb{W}^{L,(t-1)}_{G}]\!]\rangle\}_{i\in [1,...,m]}$ and sends $[\![\pmb{s}^{'(t)}]\!]$ to $\{\mathcal{C}_{1}, ..., \mathcal{C}_{m}\}$\;
        
        \ForEach{client $\mathcal{C}_{i} \in \{\mathcal{C}_{1}, ..., \mathcal{C}_{m}\}$}{
            $\mathcal{C}_i$ decrypts $\pmb{s}^{'(t)}_{i} \gets \textsc{FHE.Dec}_{\textsc{sk}}([\![\pmb{s}^{'(t)}]\!])$, conducts threshold-based selection and sends back $\pmb{s}^{(t)}_{i}$\;
        }
        $\mathcal{A}$ collects $\{\pmb{s}^{(t)}_{i}\}_{i\int[1,...,m]}$ and selects $\pmb{s}^{(t)}$ via majority of votes strategy\;
        $\mathcal{A}$ generates fusion weights $\pmb{f}^{w}_{\pmb{s}^{(t)}}$ using $\pmb{s}^{(t)}$\;
        $\mathcal{A}$ conducts secure aggregation $[\![\pmb{W}^{(t)}_{G}]\!] \gets \langle [\![\pmb{W}^{(t)}]\!], \pmb{f}^{\pmb{W}^{(t)}}_{\pmb{s}^{(t)}}\rangle$ and sends $[\![\pmb{W}^{(t)}_{G}]\!]$ to   $\{\mathcal{C}_{1}, ..., \mathcal{C}_{m}\}$\;
    }
\end{algorithm}

The \textit{DDFed} algorithm is outlined in Algorithm~\ref{alg:framework}. Assuming, without loss of generality, that at training round $t$, each client receives the aggregated and encrypted global model from the previous round. 
Upon decrypting this global model, benign clients clip it before conducting local training. They then encrypt their local model updates after applying a normalization preprocessing step to aid in detecting similarity-based poisoning attacks.

Once all encrypted model updates are collected from the clients, the aggregation server begins secure similarity computations using the abstracted last layer of these updates. It introduces differential privacy by adding perturbation noise and sends them back to each client for collaborative decryption and selection of benign clients.

Following this, based on majority votes, the aggregation server determines final aggregation groups and calculates fusion weights. Finally, it securely aggregates these with the fusion weights to produce an encrypted global model and concludes that round of federated learning (FL) training.


\subsection{Additional Experimental Results}

\subsubsection{Impact of Epsilon with 100 Clients}
\label{sec:appendix:results_epsilon}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{images/all_epsilon_cmp_acc_p100.pdf}
    \caption{Impact of hyper-parameter $\epsilon$ of differential privacy based perturbation at secure similarity computation phase with client number 100, evaluated on MNIST (left) and FMNIST (right), under IPM attack.}
    \label{fig:cmp_epsilon_extra}
\end{figure}

\figurename~\ref{fig:cmp_epsilon_extra} presents the experimental findings on how different $\varepsilon$ values affect perturbations during the secure similarity computation phase, with experiments focusing on an IPM attack scenario and involving 100 clients. These tests were carried out using the MNIST and Fashion-MNIST (FMNIST) datasets. For both datasets, we explored a range of epsilon values from 0.01 to 0.1, noting that lower epsilon values indicate enhanced privacy through increased noise addition. Initially, all configurations demonstrated high accuracy levels; however, performance fluctuations became evident following the attack. Specifically, the MNIST dataset exhibited a notable decrease in accuracy at certain epsilon settings, while the FMNIST dataset showed more moderate variations in performance. Ultimately, both datasets achieved relatively stable model accuracy.
Determining the optimal $\varepsilon$ setting is task-dependent and remains an area for future investigation.



\begin{table}[h]
\centering
\small
\caption{Time cost per training round of various defense approaches on MNIST and FMNIST datasets under SCALING attack}
\begin{tabular}{@{}lcccc@{}}
\toprule
Approaches & \multicolumn{2}{c}{MNIST, SCALING attack} & \multicolumn{2}{c}{FMNIST, SCALING attack} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
                   & avg (s) & var (s) & avg (s) & var (s) \\
\midrule
FedAvg             & 9.87    & 0.21    & 10.51   & 0.01    \\
Krum               & 9.75    & 0.21    & 10.42   & 0.01    \\
Median             & 9.81    & 0.15    & 10.29   & 0.02    \\
Clipping Median    & 9.73    & 0.11    & 10.19   & 0.01    \\
Trimmed Mean       & 9.76    & 0.22    & 10.24   & 0.01    \\
Cos Defense        & 9.43    & 0.08    & 10.30   & 0.01    \\
DDFed (Our Work)   & 12.15   & 0.03    & 12.25   & 0.18    \\
\bottomrule
\end{tabular}
\label{tab:time_scaling_attack_extra}
\end{table}

\begin{table}[h]
\centering
\small
\caption{Time cost per training round of various defense approaches on MNIST and FMNIST datasets under ALE attack}
\begin{tabular}{@{}lcccc@{}}
\toprule
Approaches & \multicolumn{2}{c}{MNIST, ALE attack} & \multicolumn{2}{c}{FMNIST, ALE attack} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
                   & avg (s) & var (s) & avg (s) & var (s) \\
\midrule
FedAvg             & 10.31   & 0.11    & 10.38   & 0.16    \\
Krum               & 10.19   & 0.10    & 10.06   & 0.08    \\
Median             & 10.19   & 0.05    & 10.15   & 0.04    \\
Clipping Median    & 9.98    & 0.09    & 10.05   & 0.12    \\
Trimmed Mean       & 9.97    & 0.11    & 10.06   & 0.06    \\
Cos Defense        & 9.78    & 0.17    & 10.14   & 0.09    \\
DDFed (Our Work)   & 12.23   & 0.07    & 11.95   & 0.08    \\
\bottomrule
\end{tabular}
\label{tab:time_alie_attack_extra}
\end{table}


\subsubsection{Time Cost of Secure Aggregation on Scaling and ALIE attacks.}
\label{sec:appendix:results_time}

\tablename~\ref{tab:time_scaling_attack_extra} and \tablename~\ref{tab:time_alie_attack_extra}
report additional results on the time cost of each training round taken for various defense strategies against SCALING and ALIE attacks on the MNIST and FMNIST datasets, respectively.
Consistent with the findings presented in Section~\ref{sec:exp:results}, our \textit{DDFed} approach adds only 2 seconds to the usual 10-second training round across multiple experiments, datasets, and attack scenarios, resulting in a 20\% increase in time per round. Despite this slight increase, \textit{DDFed} successfully defends against model poisoning attacks while ensuring robust privacy protection.


\subsubsection{Performance of DDFed Against Cold-Start Model Poisoning Attacks.}
\label{sec:appendix:results_cold_start_attack}

The primary purpose that we initiated the attack at round 50 is to demonstrate the effectiveness of defense mechanisms and clearly show the comparative effects of different defense methods before and after an attack. This setup can also illustrate how various defensive measures impact training convergence and model quality, even without attacks.

\textit{DDFed} is resilient to poisoning attacks from the beginning of training. Our design is not constrained by the attack's initiation round. Supplementary experimental results as reported in \tablename~\ref{tab:cold_start_attacks} on the FMNIST dataset with 100 clients in a non-iid setting support this claim.

\begin{table}[h]
\centering
\small
\caption{Performance of DDFed Against Cold-Start attacks on FMNIST datasets.}
\begin{tabular}{lccc}
\toprule
Approaches &  IPM Attacks & ALIE Attacks & SCALINE Attacks \\
\midrule
FedAvg             & 0       & 10.1    & 0  \\
Krum               & 69.05   & 73.69   & 69.95  \\
Median             & 67.57   & 76.57   & 74.03  \\
Clipping Median    & 61.1    & 73.8    & 75.49  \\
Trimmed Mean       & 0       & 43.29   & 0   \\
Cos Defense        & 81.87   & 82.97   & 81.11  \\
DDFed (Our Work)   & 83.32   & 80.97   & 83.05  \\
\bottomrule
\end{tabular}
\label{tab:cold_start_attacks}
\end{table}




\subsubsection{Impact of Selected Layer Count on Poisoning Model Detection in DDFed.}
\label{sec:appendix:results_cmp_layers}

In the main body of the paper, we use only the last layer for similarity computation because our primary goal is to integrate privacy-preserving functionality into existing poisoning defense strategies rather than optimizing these mechanisms. Our exploration shows that similarity-based methods and their variants provide comprehensive defense effectiveness, robust against various threat scenarios such as server reliance on validation data, types of model poisoning attacks, and the number of compromised clients. Therefore, we selected a typical similarity-based defense strategy (Cosine Defense) as a starting point to enhance privacy-preserving features. Our approach can easily extend to other similarity-based detection variants using full layers for secure similarity computation. As shown in \tablename~\ref{tab:cmp_full_last_layers}, we conducted additional experiments with full-layer secure similarity computation on a larger dataset (CIFAR10) under various attacks.

\begin{table}[h]
\centering
\small
\caption{Comparison of Model Performance and Time Cost Across Different Layer Protection Settings on Evaluating the CIFAR10 Dataset with Setting of 60 Training Rounds.}
\begin{tabular}{@{}lcccccccc@{}}
\toprule
Approaches & \multicolumn{2}{c}{No attack} & \multicolumn{2}{c}{IPM attack} & \multicolumn{2}{c}{ALIE attack} & \multicolumn{2}{c}{SCALINE attack}\\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
                   & Acc & Time(m) &  Acc & Time(m) &  Acc & Time(m) &  Acc & Time(m) \\
\midrule
FedAvg             & 70.16   & 46.23    & 0      & 46.62  & 10   & 46.89 & 0   & 46.48    \\
DDFed (Last Layer) & -       & -        & 70.3   & 50.66  & 64.62   & 51.3 & 69.61   & 51.63    \\
DDFed (Full Layers)& -       & -        & 69.84  & 58.95  & 69.73   & 58.78 & 68.89   & 59.01  \\
\bottomrule
\end{tabular}
\label{tab:cmp_full_last_layers}
\end{table}




\subsection{Differential Privacy}
\label{sec:appendix:privacy}

\subsubsection{Differential Privacy}
Differential privacy is a mathematical framework designed to provide privacy guarantees for individuals in a dataset. The standard definition of differential privacy is as follows:

A randomized algorithm \( \mathcal{M} \) is said to be \((\varepsilon, \delta)\)-differentially private if, for any two adjacent datasets \( D \) and \( D' \) (i.e., datasets differing by only one element), and for any subset of outputs \( S \subseteq \text{Range}(\mathcal{M}) \), the following inequality holds:
\begin{equation}
    \Pr[\mathcal{M}(D) \in S] \leq e^{\varepsilon} \Pr[\mathcal{M}(D') \in S] + \delta
\end{equation}
where \( \varepsilon \) is the privacy budget parameter, which controls the trade-off between privacy and utility. A smaller \( \varepsilon \) indicates stronger privacy. \( \delta \) (delta) is a small probability that accounts for the possibility of the privacy guarantee being violated.

The Gaussian mechanism is a specific method to achieve differential privacy by adding Gaussian noise to the output of a function. The definition of the Gaussian mechanism is as follows:

Given a function \( f \) and any two adjacent datasets \( D \) and \( D' \), the sensitivity of \( f \) is defined as:

\begin{equation}
    \Delta f = \max_{D, D'} \| f(D) - f(D') \|_2
\end{equation}
The Gaussian mechanism adds noise drawn from a Gaussian distribution with mean 0 and standard deviation \( \sigma \), where \( \sigma \) is determined by:

\begin{equation}
\sigma = \frac{\Delta f \sqrt{2 \ln(1.25/\delta)}}{\varepsilon}
\end{equation}
Thus, the Gaussian mechanism is defined as:

\begin{equation}
\mathcal{M}(D) = f(D) + \mathcal{N}(0, \sigma^2)
\end{equation}
where \( \mathcal{N}(0, \sigma^2) \) denotes a Gaussian distribution with mean 0 and variance \( \sigma^2 \). By adding Gaussian noise in this manner, the Gaussian mechanism ensures that the output satisfies \((\varepsilon, \delta)\)-differential privacy.




\subsubsection{Privacy Analysis of Differentially Private Similarity Computation in \textit{DDFed}}
The \textit{DDFed} framework aims to enhance privacy protection and mitigate poisoning attacks within federated learning systems by integrating FHE and a similarity-based anomaly detection system. To further bolster privacy, \textit{DDFed} incorporates DP during the similarity score computation process. This section provides a theoretical analysis of the differential privacy levels maintained by each participant in the \textit{DDFed} framework, specifically focusing on clients during similarity score computation and feedback stages, as well as the aggregation server during model aggregation and similarity score processing.

In the similarity score computation phase, each client normalizes its local model updates before submitting them. To ensure DP, Gaussian noise is added to these normalized updates. By adding Gaussian noise, each client's similarity score computation adheres to \((\varepsilon, \delta)\)-differential privacy, ensuring that the privacy of the client's data is preserved even in the presence of adversaries.

During the feedback phase, clients decrypt the similarity scores and submit their results. Since these scores have already been DP due to the added Gaussian noise, the privacy level remains at \((\varepsilon, \delta)\)-differential privacy. This ensures that even when clients provide feedback, their privacy is not compromised.

In the model aggregation phase, the aggregation server receives encrypted model updates from clients. While FHE inherently provides a high level of security for these crucial parameters, the aggregation server further ensures privacy by applying DP during the similarity score calculation. The server aggregates the encrypted updates without accessing the plaintext data, thereby maintaining the privacy of the individual model updates.

For the similarity score processing phase, the aggregation server handles the scores submitted by clients, which have already been protected using differential privacy. Consequently, the server does not need to apply additional privacy mechanisms during this phase. The DP guarantees provided during the similarity score computation phase by clients are sufficient to protect the overall process.

Based on the analysis, the privacy levels for each client in \textit{DDFed} framework can be summarized as follows. During the similarity score computation phase, clients achieve \((\varepsilon, \delta)\)-differential privacy by adding Gaussian noise to their normalized model updates. During the feedback phase, clients maintain \((\varepsilon, \delta)\)-differential privacy as the similarity scores they submit have already been differential private.

By thoughtfully designing and selecting parameters, the \textit{DDFed} framework can provide robust privacy protection and maintain high model performance. The use of FHE for critical parameters and differential privacy for similarity scores ensures a balanced and comprehensive approach to privacy protection, addressing both security and utility needs effectively.



\subsubsection{Impact of DP on FHE-based Similarity Computation in \textit{DDFed}}

\begin{table}[h]
\centering
\small
\caption{Impact of DP on FHE-based Similarity Detection in DDFed on evaluating CIFAR10 datasets.}
\begin{tabular}{lccc}
\toprule
Approaches &  IPM Attacks & ALIE Attacks & SCALINE Attacks \\
\midrule
DDFed (Simulated)  & 70.21   & 64.3    & 69.82  \\
DDFed (Our Work)   & 70.31   & 64.62   & 69.6  \\
\bottomrule
\end{tabular}
\label{tab:dp_impact}
\end{table}

Generally, the reader may concern about whether $[[x]]+ \Delta$ equals $x + \Delta$, where $x$ is under FHE protection. However, this depends on the precision of the employed FHE schemes. Proving such a statement theoretically may require delving into the specific construction algorithm of the FHE scheme, which is beyond the scope of machine learning-oriented venues.

This paper utilizes CKKS constructions, which natively support high-precision secure computation on floating-point numbers. As a result, adding DP noise to encrypted similarity results does not degrade performance. To validate this, we conducted supplementary experiments on CIFAR10 using a simulated DDFed setup where DP noise was added to non-encrypted parameters. The reported results in \tablename~\ref{tab:dp_impact} support this claim.








\end{document}