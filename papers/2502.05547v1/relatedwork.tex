\section{Related Works}
% The general FL framework inherently protects privacy since it doesn't share raw training data with collaborators \cite{konecny2016federated}. However, studies have shown that adversaries can still compromise user privacy and the global model's performance through poisoning attacks. This has led to the proposal of various secure and robust aggregation mechanisms.


% \paragraph{Privacy-preserving Federated Learning.}
\paragraph{Privacy Risks and Countermeasures in FL}

The fundamental design of FL ensures that all training data stays with its owner, offering basic privacy. However, it still exposes vulnerabilities to inference attacks, which allow adversaries to extract information about the training data used by each party \cite{nasr2019comprehensive, shokri2017membership,baracaldo2022protecting, nasr2019comprehensive, huang2021evaluating, geiping2020inverting}. In some cases, the risk of private information leakage may be unacceptable. Therefore, several defenses have been suggested to mitigate these risks, including differential privacy (DP) and secure aggregation (SA), based on various cryptographic primitives such as (partial) homomorphic encryption \cite{liu2019secure, zhang2020batchcrypt}, threshold Paillier \cite{truex2019hybrid}, functional encryption \cite{xu2019hybridalpha}, and pairwise masking protocols \cite{bonawitz2017practical}. 


% Secure aggregation methods use cryptographic techniques to protect the privacy of inputs, ensuring that an inquisitive or untrusted aggregator cannot view individual model updates. 
% Popular methods include (partial) homomorphic encryption \cite{liu2019secure, zhang2020batchcrypt}, threshold Paillier \cite{truex2019hybrid}, functional encryption \cite{xu2019hybridalpha}, and pairwise masking protocols \cite{bonawitz2017practical}. 
% Conversely, to counter inference attacks targeting the final model or its updates, DP strategies add a precisely calibrated amount of noise through differentially private mechanisms. Although DP offers robust privacy protection, it is well-known for generating models with reduced accuracy due to the added noise. Alternatively, some solutions ingeniously merge DP and SA techniques to maintain strong differential privacy protections while still achieving high model performance \cite{truex2019hybrid}.


% \paragraph{Poisoning Attacks and Defenses.}
\paragraph{Poisoning Risks and Countermeasures in FL.}

Besides privacy inference attacks, FL is also susceptible to poisoning attacks, where adversaries can compromise certain clients and manipulate their data or models to intentionally worsen the global model's performance by introducing corrupted updates during training.
This paper focuses on untargeted model attacks, whose goal is to significantly diminish the effectiveness of the global model through methods such as Inner Product Manipulation (IPM) attack \cite{xie2020fall}, scaling attack\cite{bagdasaryan2020backdoor}, and ``a little is enough" (ALIE) attack \cite{baruch2019little}. 
% Both scaling and ALIE attacks adjust local model updates by multiplying them with various strategic factors, while IPM changes malicious clients' local updates so that the inner product between the true gradient and aggregated updates turns negative.
Several strategies have been developed to counteract the impact of attacks, ensuring they don't compromise model performance. 
These strategies fall into two categories: client-side and server-side defenses. Client-side defenses adjust the local training algorithm with a focus on secure client updates\cite{sun2021fl}, whereas server-side defenses \cite{blanchard2017machine,sun2019can,yaldiz2023secure,yin2018byzantine} either reduce the influence of updates from malicious clients through adjusted aggregation weights or use clustering techniques to aggregate updates from trustworthy clients only.
However, these defense strategies typically operate under the assumption that model updates are not encrypted, which contradicts the objectives of privacy-focused secure aggregation defense strategies.


% Unlike the previously mentioned attacks that deduce private information, FL is also vulnerable to poisoning attacks. 
% In these scenarios, adversaries can compromise certain clients and manipulate their data or models to intentionally worsen the global model's performance by introducing corrupted updates during training. Poisoning attacks are typically categorized into targeted and untargeted types, depending on the attacker’s objectives. 
% This paper focuses on untargeted attacks, whose goal is to significantly diminish the effectiveness of the global model through methods such as Inner Product Manipulation (IPM) attack \cite{xie2020fall}, scaling attack\cite{bagdasaryan2020backdoor}, and ``a little is enough" (ALIE) attack \cite{baruch2019little}. Both scaling and ALIE attacks adjust local model updates by multiplying them with various strategic factors, while IPM changes malicious clients' local updates so that the inner product between the true gradient and aggregated updates turns negative.
% We will employ these attack techniques in our study to evaluate the efficacy of our proposed solution.


% Several strategies have been developed to counteract the impact of attacks, ensuring they don't compromise model performance. These strategies fall into two categories: client-side and server-side defenses. Client-side defenses adjust the local training algorithm with a focus on secure client participation, whereas server-side defenses either reduce the influence of updates from malicious clients through adjusted aggregation weights or use clustering techniques to aggregate updates from trustworthy clients only.
% For instance, Sun et al.\cite{sun2021fl} have demonstrated a method for countering model poisoning by perturbing the model during the local training phase of benign clients.
% Prominent server-side defense methods include Krum fusion technique\cite{blanchard2017machine}, similarity-based aggregation mechanisms\cite{sun2019can,yaldiz2023secure}, and median/mean-oriented approaches such as clipping median and trimmed mean tactics\cite{yin2018byzantine}.
% However, these defense strategies typically operate under the assumption that model updates are not encrypted, which contradicts the objectives of privacy-focused secure aggregation defense strategies.



\paragraph{Private and Robust Federated Learning.}

In privacy-preserving FL, identifying poisoning attacks is harder because of the need to balance local model privacy with the detection of harmful models.
Only a few existing studies like those mentioned in \cite{yang2023model,hossain2021desmp,jiang2020mitigating,huang2024vppfl} employ Byzantine-resilient aggregation through differential-privacy techniques. This approach necessitates a compromise between privacy and model accuracy.
Additionally, recent initiatives have been launched to address this problem through diverse methods by using various secure computation technologies. These include 3PC\cite{dong2023privacy}, which faces scalability limitations; an oblivious random grouping method constrained by its design for partial parameter disclosure\cite{zhang2023safelearning}; and both additive secret sharing\cite{li2024efficiently} and two-trapdoor homomorphic encryption\cite{ma2022shieldfl}, which depend on the impractical assumption of non-colluding dual servers.


% In privacy-preserving FL, identifying poisoning attacks is harder because of the need to balance local model privacy with the detection of harmful models, which involves closely examining each update.
% However, in secure federated learning where user updates are completely hidden, it's not possible to directly use Byzantine-resilient aggregation methods.
% Only a few existing studies like those mentioned in \cite{yang2023model,hossain2021desmp,jiang2020mitigating,huang2024vppfl} employ Byzantine-resilient aggregation through differential-privacy techniques. This approach necessitates a compromise between privacy and model accuracy and usually results in reduced accuracy.

% Recent methods have been developed to tackle this issue from different angles. 
% Dong et al \cite{dong2023privacy} propose a secure three-party computation method, but its design limits the scalability of federated learning. 
% Li et al \cite{li2024efficiently} introduce an additive secret sharing approach, which requires two non-colluding servers—a significant challenge for deployment. 
% Ma et al \cite{ma2022shieldfl} present a two-trapdoor homomorphic encryption method based on the Paillier cryptosystem, yet it also needs a two-server setting without collusion and struggles with inefficiency and impracticality. Zhang et al \cite{zhang2023safelearning} suggest an oblivious random grouping technique limited by its partial parameter disclosure design.