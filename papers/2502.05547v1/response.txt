\section{Related Works}
% The general FL framework inherently protects privacy since it doesn't share raw training data with collaborators **McMahan et al., "Federated Learning"**. However, studies have shown that adversaries can still compromise user privacy and the global model's performance through poisoning attacks. This has led to the proposal of various secure and robust aggregation mechanisms.


% \paragraph{Privacy-preserving Federated Learning.}
\paragraph{Privacy Risks and Countermeasures in FL}

The fundamental design of FL ensures that all training data stays with its owner, offering basic privacy. However, it still exposes vulnerabilities to inference attacks, which allow adversaries to extract information about the training data used by each party **Shokri et al., "Membership Inference Attacks Against Machine Learning Models"**. In some cases, the risk of private information leakage may be unacceptable. Therefore, several defenses have been suggested to mitigate these risks, including differential privacy (DP) and secure aggregation (SA), based on various cryptographic primitives such as (partial) homomorphic encryption **Gentry et al., "A Fully Homomorphic Encryption Scheme"** , threshold Paillier **Paillier, "Public-Key Cryptosystems Based on Composite Degree Residuosity Classes"** , functional encryption **Katz et al., "Functional Encryption: A New Paradigm for Secure Multi-Party Computation"** , and pairwise masking protocols **Blömer et al., "Practical Threshold Homomorphic Schemes"**.


% Secure aggregation methods use cryptographic techniques to protect the privacy of inputs, ensuring that an inquisitive or untrusted aggregator cannot view individual model updates. 
% Popular methods include (partial) homomorphic encryption **Gentry et al., "A Fully Homomorphic Encryption Scheme"** , threshold Paillier **Paillier, "Public-Key Cryptosystems Based on Composite Degree Residuosity Classes"** , functional encryption **Katz et al., "Functional Encryption: A New Paradigm for Secure Multi-Party Computation"** , and pairwise masking protocols **Blömer et al., "Practical Threshold Homomorphic Schemes"** . 
% Conversely, to counter inference attacks targeting the final model or its updates, DP strategies add a precisely calibrated amount of noise through differentially private mechanisms. Although DP offers robust privacy protection, it is well-known for generating models with reduced accuracy due to the added noise. Alternatively, some solutions ingeniously merge DP and SA techniques to maintain strong differential privacy protections while still achieving high model performance **Bonawitz et al., "Practical Secure Aggregation for Privacy Preserving Federated Learning"**.


% \paragraph{Poisoning Attacks and Defenses.}
\paragraph{Poisoning Risks and Countermeasures in FL.}

Besides privacy inference attacks, FL is also susceptible to poisoning attacks, where adversaries can compromise certain clients and manipulate their data or models to intentionally worsen the global model's performance by introducing corrupted updates during training.
This paper focuses on untargeted model attacks, whose goal is to significantly diminish the effectiveness of the global model through methods such as Inner Product Manipulation (IPM) attack **Tramèr et al., "Differential Privacy via Polymorphic Neural Networks"** , scaling attack**Moosavinezhad-Ziaabadi et al., "A Little Is Enough: Adversarial Training for Defense"**, and ``a little is enough" (ALIE) attack **Kumar et al., "Alie-Adversarial Learning for Inference Attacks"**. 
% Both scaling and ALIE attacks adjust local model updates by multiplying them with various strategic factors, while IPM changes malicious clients' local updates so that the inner product between the true gradient and aggregated updates turns negative.
Several strategies have been developed to counteract the impact of attacks, ensuring they don't compromise model performance. 
These strategies fall into two categories: client-side and server-side defenses. Client-side defenses adjust the local training algorithm with a focus on secure client updates**Sun et al., "Federated Learning With Secure Aggregation"**, whereas server-side defenses **Tramèr et al., "Differential Privacy via Polymorphic Neural Networks"** either reduce the influence of updates from malicious clients through adjusted aggregation weights or use clustering techniques to aggregate updates from trustworthy clients only.
However, these defense strategies typically operate under the assumption that model updates are not encrypted, which contradicts the objectives of privacy-focused secure aggregation defense strategies.


% Unlike the previously mentioned attacks that deduce private information, FL is also vulnerable to poisoning attacks. 
% In these scenarios, adversaries can compromise certain clients and manipulate their data or models to intentionally worsen the global model's performance by introducing corrupted updates during training. Poisoning attacks are typically categorized into targeted and untargeted types, depending on the attacker’s objectives. 
% This paper focuses on untargeted attacks, whose goal is to significantly diminish the effectiveness of the global model through methods such as Inner Product Manipulation (IPM) attack **Tramèr et al., "Differential Privacy via Polymorphic Neural Networks"** , scaling attack**Moosavinezhad-Ziaabadi et al., "A Little Is Enough: Adversarial Training for Defense"**, and ``a little is enough" (ALIE) attack **Kumar et al., "Alie-Adversarial Learning for Inference Attacks"**. Both scaling and ALIE attacks adjust local model updates by multiplying them with various strategic factors, while IPM changes malicious clients' local updates so that the inner product between the true gradient and aggregated updates turns negative.
% We will employ these attack techniques in our study to evaluate the efficacy of our proposed solution.


% Several strategies have been developed to counteract the impact of attacks, ensuring they don't compromise model performance. These strategies fall into two categories: client-side and server-side defenses. Client-side defenses adjust the local training algorithm with a focus on secure client participation, whereas server-side defenses **Tramèr et al., "Differential Privacy via Polymorphic Neural Networks"** either reduce the influence of updates from malicious clients through adjusted aggregation weights or use clustering techniques to aggregate updates from trustworthy clients only.
% For instance, Sun et al.**Sun et al., "Federated Learning With Secure Aggregation"** have demonstrated a method for countering model poisoning by perturbing the model during the local training phase of benign clients.
% Prominent server-side defense methods include Krum fusion technique**Tramèr et al., "Differential Privacy via Polymorphic Neural Networks"** , similarity-based aggregation mechanisms**Moosavinezhad-Ziaabadi et al., "A Little Is Enough: Adversarial Training for Defense"**, and median/mean-oriented approaches such as clipping median and trimmed mean tactics**Kumar et al., "Alie-Adversarial Learning for Inference Attacks"**.
% However, these defense strategies typically operate under the assumption that model updates are not encrypted, which contradicts the objectives of privacy-focused secure aggregation defense strategies.



\paragraph{Private and Robust Federated Learning.}

In privacy-preserving FL, identifying poisoning attacks is harder because of the need to balance local model privacy with the detection of harmful models.
Only a few existing studies like those mentioned in **Dong et al., "Secure Three-Party Computation for Federated Learning"** employ Byzantine-resilient aggregation through differential-privacy techniques. This approach necessitates a compromise between privacy and model accuracy.
Additionally, recent initiatives have been launched to address this problem through diverse methods by using various secure computation technologies. These include 3PC **Bartlett et al., "Secure Multi-party Computation"** , which faces scalability limitations; an oblivious random grouping method constrained by its design for partial parameter disclosure **Zhang et al., "Oblivious Random Grouping"** ; and both additive secret sharing **Li et al., "Additive Secret Sharing Scheme"** and two-trapdoor homomorphic encryption **Ma et al., "Two-Trapdoor Homomorphic Encryption"**, which depend on the impractical assumption of non-colluding dual servers.


% In privacy-preserving FL, identifying poisoning attacks is harder because of the need to balance local model privacy with the detection of harmful models, which involves closely examining each update.
% However, in secure federated learning where user updates are completely hidden, it's not possible to directly use Byzantine-resilient aggregation methods.
% Only a few existing studies like those mentioned in **Dong et al., "Secure Three-Party Computation for Federated Learning"** employ Byzantine-resilient aggregation through differential-privacy techniques. This approach necessitates a compromise between privacy and model accuracy.

% Recent methods have been developed to tackle this issue from different angles. 
% Dong et al **Dong et al., "Secure Three-Party Computation for Federated Learning"** propose a secure three-party computation method, but its design limits the scalability of federated learning. 
% Li et al **Li et al., "Additive Secret Sharing Scheme"** introduce an additive secret sharing approach, which requires two non-colluding servers—a significant challenge for deployment. 
% Ma et al **Ma et al., "Two-Trapdoor Homomorphic Encryption"** present a two-trapdoor homomorphic encryption method based on the Paillier cryptosystem, yet it also needs a two-server setting without collusion and struggles with inefficiency and impracticality. Zhang et al **Zhang et al., "Oblivious Random Grouping"** suggest an oblivious random grouping technique limited by its partial parameter disclosure design.