\section{Related Works}
% The general FL framework inherently protects privacy since it doesn't share raw training data with collaborators ____. However, studies have shown that adversaries can still compromise user privacy and the global model's performance through poisoning attacks. This has led to the proposal of various secure and robust aggregation mechanisms.


% \paragraph{Privacy-preserving Federated Learning.}
\paragraph{Privacy Risks and Countermeasures in FL}

The fundamental design of FL ensures that all training data stays with its owner, offering basic privacy. However, it still exposes vulnerabilities to inference attacks, which allow adversaries to extract information about the training data used by each party ____. In some cases, the risk of private information leakage may be unacceptable. Therefore, several defenses have been suggested to mitigate these risks, including differential privacy (DP) and secure aggregation (SA), based on various cryptographic primitives such as (partial) homomorphic encryption ____, threshold Paillier ____, functional encryption ____, and pairwise masking protocols ____. 


% Secure aggregation methods use cryptographic techniques to protect the privacy of inputs, ensuring that an inquisitive or untrusted aggregator cannot view individual model updates. 
% Popular methods include (partial) homomorphic encryption ____, threshold Paillier ____, functional encryption ____, and pairwise masking protocols ____. 
% Conversely, to counter inference attacks targeting the final model or its updates, DP strategies add a precisely calibrated amount of noise through differentially private mechanisms. Although DP offers robust privacy protection, it is well-known for generating models with reduced accuracy due to the added noise. Alternatively, some solutions ingeniously merge DP and SA techniques to maintain strong differential privacy protections while still achieving high model performance ____.


% \paragraph{Poisoning Attacks and Defenses.}
\paragraph{Poisoning Risks and Countermeasures in FL.}

Besides privacy inference attacks, FL is also susceptible to poisoning attacks, where adversaries can compromise certain clients and manipulate their data or models to intentionally worsen the global model's performance by introducing corrupted updates during training.
This paper focuses on untargeted model attacks, whose goal is to significantly diminish the effectiveness of the global model through methods such as Inner Product Manipulation (IPM) attack ____, scaling attack____, and ``a little is enough" (ALIE) attack ____. 
% Both scaling and ALIE attacks adjust local model updates by multiplying them with various strategic factors, while IPM changes malicious clients' local updates so that the inner product between the true gradient and aggregated updates turns negative.
Several strategies have been developed to counteract the impact of attacks, ensuring they don't compromise model performance. 
These strategies fall into two categories: client-side and server-side defenses. Client-side defenses adjust the local training algorithm with a focus on secure client updates____, whereas server-side defenses ____ either reduce the influence of updates from malicious clients through adjusted aggregation weights or use clustering techniques to aggregate updates from trustworthy clients only.
However, these defense strategies typically operate under the assumption that model updates are not encrypted, which contradicts the objectives of privacy-focused secure aggregation defense strategies.


% Unlike the previously mentioned attacks that deduce private information, FL is also vulnerable to poisoning attacks. 
% In these scenarios, adversaries can compromise certain clients and manipulate their data or models to intentionally worsen the global model's performance by introducing corrupted updates during training. Poisoning attacks are typically categorized into targeted and untargeted types, depending on the attacker’s objectives. 
% This paper focuses on untargeted attacks, whose goal is to significantly diminish the effectiveness of the global model through methods such as Inner Product Manipulation (IPM) attack ____, scaling attack____, and ``a little is enough" (ALIE) attack ____. Both scaling and ALIE attacks adjust local model updates by multiplying them with various strategic factors, while IPM changes malicious clients' local updates so that the inner product between the true gradient and aggregated updates turns negative.
% We will employ these attack techniques in our study to evaluate the efficacy of our proposed solution.


% Several strategies have been developed to counteract the impact of attacks, ensuring they don't compromise model performance. These strategies fall into two categories: client-side and server-side defenses. Client-side defenses adjust the local training algorithm with a focus on secure client participation, whereas server-side defenses either reduce the influence of updates from malicious clients through adjusted aggregation weights or use clustering techniques to aggregate updates from trustworthy clients only.
% For instance, Sun et al.____ have demonstrated a method for countering model poisoning by perturbing the model during the local training phase of benign clients.
% Prominent server-side defense methods include Krum fusion technique____, similarity-based aggregation mechanisms____, and median/mean-oriented approaches such as clipping median and trimmed mean tactics____.
% However, these defense strategies typically operate under the assumption that model updates are not encrypted, which contradicts the objectives of privacy-focused secure aggregation defense strategies.



\paragraph{Private and Robust Federated Learning.}

In privacy-preserving FL, identifying poisoning attacks is harder because of the need to balance local model privacy with the detection of harmful models.
Only a few existing studies like those mentioned in ____ employ Byzantine-resilient aggregation through differential-privacy techniques. This approach necessitates a compromise between privacy and model accuracy.
Additionally, recent initiatives have been launched to address this problem through diverse methods by using various secure computation technologies. These include 3PC____, which faces scalability limitations; an oblivious random grouping method constrained by its design for partial parameter disclosure____; and both additive secret sharing____ and two-trapdoor homomorphic encryption____, which depend on the impractical assumption of non-colluding dual servers.


% In privacy-preserving FL, identifying poisoning attacks is harder because of the need to balance local model privacy with the detection of harmful models, which involves closely examining each update.
% However, in secure federated learning where user updates are completely hidden, it's not possible to directly use Byzantine-resilient aggregation methods.
% Only a few existing studies like those mentioned in ____ employ Byzantine-resilient aggregation through differential-privacy techniques. This approach necessitates a compromise between privacy and model accuracy and usually results in reduced accuracy.

% Recent methods have been developed to tackle this issue from different angles. 
% Dong et al ____ propose a secure three-party computation method, but its design limits the scalability of federated learning. 
% Li et al ____ introduce an additive secret sharing approach, which requires two non-colluding servers—a significant challenge for deployment. 
% Ma et al ____ present a two-trapdoor homomorphic encryption method based on the Paillier cryptosystem, yet it also needs a two-server setting without collusion and struggles with inefficiency and impracticality. Zhang et al ____ suggest an oblivious random grouping technique limited by its partial parameter disclosure design.