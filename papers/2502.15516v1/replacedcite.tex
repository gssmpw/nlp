\section{Related Work}
\subsection{Radar Data Representations}
Millimeter-wave radar uses radio waves in the millimeter-wave frequency range (30 GHz to 300 GHz) for environmental sensing. 
Typically, each transmitting antenna (TX) emits a sequence of Frequency Modulated Continuous Waves (FMCW), called chirps, 
with linearly increasing frequency ____. 
Reflected waves from objects are captured by receiving antennas (RX) and processed to determine distance, speed, and angle. 
Traditional 3D radars, with horizontally arranged antennas, 
provide only horizontal detection and lack elevation information. 
In contrast, 4D radars use both horizontal and vertical antenna arrays, 
enabling the measurement of elevation angles as well. 
4D radar data can be represented as ADC signals, radar spectra, or point clouds, 
depending on the processing stage. 
%This section will review the typical 4D millimeter-wave radar signal processing workflow to illustrate how these data representations are generated.


Fig. \ref{radar-pipeline} shows the traditional process of converting 4D millimeter-wave radar signals 
from raw reflected waves to radar point clouds, 
along with the corresponding data format at each step.
\begin{itemize}
	\item Step 1: Each radar TX sends chirps, and each RX receives the reflected signals.
	\item Step 2: The received signal and its corresponding transmitted signal are mixed 
							and passed through a low-pass filter to generate an Intermediate Frequency (IF) signal.
	\item Step 3: The IF signal is converted into raw ADC data by an Analog-to-Digital Converter.
	\item Step 4: Apply FFT to the ADC data along the range and Doppler dimensions to produce a series of Range-Doppler (RD) maps 
						corresponding to different TX-RX pairs.
	\item Step 5 \& 6: One approach is to apply FFT along the azimuth and elevation dimensions of different TX-RX pairs
									to obtain a 4D radar spectrum (Step 5a). 
									Then, in Step 6a, CFAR operation is applied to the four-dimensional data to filter it based on intensity, 
									resulting in a radar point cloud. 
									Another approach is to first apply a CFAR-type algorithm to filter the RD map and generate target cells (Step 5b), 
									and then use Digital Beamforming (DBF) in Step 6b to recover angle information and generate the radar point cloud.
\end{itemize}



\subsection{Camera-Radar Datasets}
Although many public datasets now include millimeter-wave radar data alongside images and LiDAR data, 
these radar datasets contain fewer samples compared to the latter two. 
Most existing datasets, such as NuScenes ____, RADDet ____, Zendar ____, RADIATE ____, and CARRADA ____, 
use traditional 3D radar, 
providing data along the range, Doppler, and azimuth dimensions but lacking elevation information. 
This limitation makes accurate 3D bounding box estimation challenging in 3D object detection.
With the emergence of 4D millimeter-wave radar, 
more public datasets have begun incorporating 4D radar to capture data with elevation information. 

However, most of these datasets provide radar point clouds processed by methods like CFAR, 
such as View-of-Delft ____, Astyx ____, and RadarScenes ____. 
Due to the sparsity of radar signals, we aim to use 4D radar that provides elevation information, 
and the radar data should be in the form of lossless \textit{raw radar data} before CFAR operations, 
including ADC data and radar spectra. 
Among available datasets, RADIal and K-Radar meet these criteria. 
However, RADIal initially only offered 2D annotations, 
and although Liu et al. ____ recently added 3D annotations, 
it remains limited in scope and lacks data under adverse weather conditions. 
Therefore, the K-Radar dataset is the most suitable for this research.

K-Radar contains 35K frames of 4D radar spectrum/tensor (4DRT) with measurements across 
the range, Doppler, azimuth, and elevation dimensions, 
alongside multi-view images, high-resolution LiDAR point clouds, annotated 3D bounding boxes, and other relevant data. 
K-Radar also covers various challenging road structures (e.g., urban, suburban roads, alleyways, and highways) 
and adverse weather conditions (e.g., fog, rain, and snow) ____.


\begin{figure*}[ht]
	\centerline{
		\includegraphics[width=1\textwidth]{figs/our_architecture.png}
	}
	\caption{The overall architecture of our proposed algorithm.}
	\label{our_arch}
\end{figure*}

\subsection{Camera-Radar Fusion based 3D Object Detection}
Previous radar-camera fusion-based 3D object detection algorithms often rely on traditional 3D millimeter-wave radar signals, 
which are typically represented as radar point clouds. 
Methods like CenterFusion ____, RCBEVDet ____, and BEVFusion ____ 
extract radar features similar to LiDAR and then fuse them with image features through concatenation or cross-attention before feeding them into the detection network. 
However, these traditional 3D radar point clouds lack elevation information and lose significant data due to CFAR thresholding, 
which negatively impacts the modelâ€™s detection performance. 
As a result, researchers often prefer high-resolution sensors like cameras and LiDARs for multisensor fusion to achieve better detection performance.

With the emergence of 4D millimeter-wave radars, 
many studies have begun fusing more cost-effective and highly complementary cameras and 4D millimeter-wave radars in 3D object detection algorithms.
Methods such as LXL ____ and RCFusion ____ 
fuse 4D radar point clouds with image features by first pillarizing the radar point clouds to extract BEV features, 
which are then fused with image features in the BEV feature space.
To mitigate the information loss inherent in radar point clouds, 
EchoFusion ____ and DPFT ____ propose fusing raw radar data with camera images. 
However, EchoFusion has high computational resource demands (requiring 4.0 GiB of GPU memory during the inference stage ____), 
while DPFT uses a full 4D tensor as input, which requires significant memory, and is difficult to obtain ____. 
Building on EchoFusion's work, 
our method enhances image features with depth information before radar-camera fusion, 
which can improve 3D object detection performance while reducing network complexity. 
Additionally, we propose that depth information can still be derived from 4D radar spectra even in the absence of a depth sensor.

% ================================================================================================