\section{Related Work}
Using Machine Learning (ML) to process extensive health data has been a long-standing effort, particularly in ICU settings where data is collected from various devices at minute or second intervals \cite{wiens_machine_2018}. Traditional DSS require extensive preprocessing and feature extraction \cite{kong_using_2020}. Recurrent models capture temporal relationships but struggle with sparse, asynchronous ICU data due to fixed temporal resolutions and reliance on predefined variable matrices \cite{ge_interpretable_2018}.

Advancements in Transformers improved the modeling of longitudinal medical data. BEHRT \cite{li_behrt_2020} processes diagnosis codes as tokens within a limited vocabulary, modeling hospital visits as sentences with age encoded in position embeddings and pre-training through Masked Language Modeling (MLM). Building on this, Med-BERT \cite{rasmy_med-bert_2021} and ExMed-BERT \cite{lentzen_exmed-bert_2023} extended the approach by incorporating continuous data and adopting late fusion techniques. Models like Rare-BERT \cite{prakash_rarebert_2021} and life2vec \cite{savcisens_life2vec_2023} further expanded vocabularies to cover broader data types, while ExBEHRT \cite{rupp_exbehrt_2023} and BRLTM \cite{meng_brltm_2021} integrated demographic and lab data through additional embedding layers.

Hierarchical approaches such as Hi-BEHRT \cite{li_hi-behrt_2021} and graph-based methods like GT-BEHRT \cite{poulain_gt-behrt_2023} addressed challenges with large input sequences by summarizing sets of visits before integrating them into patient profiles. STraTS \cite{tipirneni_strats_2022} proposed a triplet embedding scheme for features, values, and time, while DuETT \cite{labach_duett_2023} used dual attention to capture time- and event-based representations.

A key challenge remains the lack of standardized benchmarks to compare. YAIB \cite{van_de_water_yet_2024} provides a reproducible framework for ICU cohort generation and task evaluation, while EHRSHOT \cite{wornow_ehrshot_2023} enables model fine-tuning across contexts.

ICU-BERT advances these efforts by capturing the complexity of ICU data through pre-trained embeddings and modeling temporal information at the embedding level. Unlike earlier approaches, it avoids narrow vocabularies, enabling generalization across datasets and achieving competitive performance on YAIB and DuETT tasks.