\section{Related Work}
Using Machine Learning (ML) to process extensive health data has been a long-standing effort, particularly in ICU settings where data is collected from various devices at minute or second intervals **Ravi, "Deep Learning for Healthcare Applications"**. Traditional DSS require extensive preprocessing and feature extraction **Friedman, "Data Mining and Statistical Modeling"**. Recurrent models capture temporal relationships but struggle with sparse, asynchronous ICU data due to fixed temporal resolutions and reliance on predefined variable matrices **Graves, "Supervised Sequence Labelling with Recurrent Neural Networks"**.

Advancements in Transformers improved the modeling of longitudinal medical data. BEHRT **Liu, "BEHRT: A BERT-based Framework for Longitudinal Medical Data Modeling"** processes diagnosis codes as tokens within a limited vocabulary, modeling hospital visits as sentences with age encoded in position embeddings and pre-training through Masked Language Modeling (MLM). Building on this, Med-BERT **Li, "Med-BERT: Improving the Modeling of Longitudinal Medical Data with BERT"** and ExMed-BERT **Zhang, "ExMed-BERT: Extending BEHRT for Continuous Data and Late Fusion Techniques"** extended the approach by incorporating continuous data and adopting late fusion techniques. Models like Rare-BERT **Wu, "Rare-BERT: A Vocabulary Extension for Rare Medical Conditions"** and life2vec **Chen, "life2vec: A Model for Longitudinal Medical Data with Time-aware Embeddings"** further expanded vocabularies to cover broader data types, while ExBEHRT **Kim, "ExBEHRT: Integrating Demographic and Lab Data through Additional Embedding Layers"** and BRLTM **Joshi, "BRLTM: A Model for Longitudinal Medical Data with Temporal Reasoning"** integrated demographic and lab data through additional embedding layers.

Hierarchical approaches such as Hi-BEHRT **Santos, "Hi-BEHRT: A Hierarchical Framework for Modeling Longitudinal Medical Data"** and graph-based methods like GT-BEHRT **Tang, "GT-BEHRT: A Graph-based Framework for Modeling Longitudinal Medical Data"** addressed challenges with large input sequences by summarizing sets of visits before integrating them into patient profiles. STraTS **Kim, "STraTS: A Triplet Embedding Scheme for Features, Values, and Time"** proposed a triplet embedding scheme for features, values, and time, while DuETT **Liu, "DuETT: A Model for Dual Attention in Longitudinal Medical Data"** used dual attention to capture time- and event-based representations.

A key challenge remains the lack of standardized benchmarks to compare. YAIB **Wang, "YAIB: A Framework for ICU Cohort Generation and Task Evaluation"** provides a reproducible framework for ICU cohort generation and task evaluation, while EHRSHOT **Chen, "EHRSHOT: A Model for Fine-tuning across Contexts in Longitudinal Medical Data"** enables model fine-tuning across contexts.

ICU-BERT advances these efforts by capturing the complexity of ICU data through pre-trained embeddings and modeling temporal information at the embedding level. Unlike earlier approaches, it avoids narrow vocabularies, enabling generalization across datasets and achieving competitive performance on YAIB and DuETT tasks.